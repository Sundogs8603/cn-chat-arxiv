{
    "title": "Long-form evaluation of model editing",
    "abstract": "arXiv:2402.09394v1 Announce Type: new Abstract: Evaluations of model editing currently only use the `next few token' completions after a prompt. As a result, the impact of these methods on longer natural language generation is largely unknown. We introduce long-form evaluation of model editing (\\textbf{\\textit{LEME}}) a novel evaluation protocol that measures the efficacy and impact of model editing in long-form generative settings. Our protocol consists of a machine-rated survey and a classifier which correlates well with human ratings. Importantly, we find that our protocol has very little relationship with previous short-form metrics (despite being designed to extend efficacy, generalization, locality, and portability into a long-form setting), indicating that our method introduces a novel set of dimensions for understanding model editing methods. Using this protocol, we benchmark a number of model editing techniques and present several findings including that, while some methods (R",
    "link": "https://arxiv.org/abs/2402.09394",
    "context": "Title: Long-form evaluation of model editing\nAbstract: arXiv:2402.09394v1 Announce Type: new Abstract: Evaluations of model editing currently only use the `next few token' completions after a prompt. As a result, the impact of these methods on longer natural language generation is largely unknown. We introduce long-form evaluation of model editing (\\textbf{\\textit{LEME}}) a novel evaluation protocol that measures the efficacy and impact of model editing in long-form generative settings. Our protocol consists of a machine-rated survey and a classifier which correlates well with human ratings. Importantly, we find that our protocol has very little relationship with previous short-form metrics (despite being designed to extend efficacy, generalization, locality, and portability into a long-form setting), indicating that our method introduces a novel set of dimensions for understanding model editing methods. Using this protocol, we benchmark a number of model editing techniques and present several findings including that, while some methods (R",
    "path": "papers/24/02/2402.09394.json",
    "total_tokens": 872,
    "translated_title": "长文本评估模型编辑",
    "translated_abstract": "目前，对于模型编辑的评估只使用了提示后的“下几个标记”的完成。因此，这些方法对于更长的自然语言生成的影响大部分是未知的。我们引入了长文本评估模型编辑（LEME）的新颖评估协议，该协议在长篇生成设置中衡量模型编辑的功效和影响。我们的评估协议包括机器评定的调查和与人类评分相关性很好的分类器。重要的是，我们发现我们的评估协议与先前的短文本指标几乎没有关系（尽管设计为扩展功效、泛化性、局部性和可移植性到长文本环境中），这表明我们的方法引入了一组新的维度来理解模型编辑方法。使用这个协议，我们对一些模型编辑技术进行了基准测试，并提出了几个发现，包括一些方法（R",
    "tldr": "长文本评估模型编辑（LEME）是一种新颖的评估协议，用于衡量模型编辑在长篇生成设置中的有效性和影响。这个协议与先前的短文本指标几乎没有关系，引入了一组新的维度来理解模型编辑方法。",
    "en_tdlr": "The Long-form Evaluation of Model Editing (LEME) is a novel evaluation protocol that measures the efficacy and impact of model editing in long-form generative settings. Our protocol has very little relationship with previous short-form metrics, introducing a novel set of dimensions for understanding model editing methods."
}