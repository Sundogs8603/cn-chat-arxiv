{
    "title": "A Sober Look at LLMs for Material Discovery: Are They Actually Good for Bayesian Optimization Over Molecules?",
    "abstract": "Automation is one of the cornerstones of contemporary material discovery. Bayesian optimization (BO) is an essential part of such workflows, enabling scientists to leverage prior domain knowledge into efficient exploration of a large molecular space. While such prior knowledge can take many forms, there has been significant fanfare around the ancillary scientific knowledge encapsulated in large language models (LLMs). However, existing work thus far has only explored LLMs for heuristic materials searches. Indeed, recent work obtains the uncertainty estimate -- an integral part of BO -- from point-estimated, non-Bayesian LLMs. In this work, we study the question of whether LLMs are actually useful to accelerate principled Bayesian optimization in the molecular space. We take a sober, dispassionate stance in answering this question. This is done by carefully (i) viewing LLMs as fixed feature extractors for standard but principled BO surrogate models and by (ii) leveraging parameter-effic",
    "link": "https://arxiv.org/abs/2402.05015",
    "context": "Title: A Sober Look at LLMs for Material Discovery: Are They Actually Good for Bayesian Optimization Over Molecules?\nAbstract: Automation is one of the cornerstones of contemporary material discovery. Bayesian optimization (BO) is an essential part of such workflows, enabling scientists to leverage prior domain knowledge into efficient exploration of a large molecular space. While such prior knowledge can take many forms, there has been significant fanfare around the ancillary scientific knowledge encapsulated in large language models (LLMs). However, existing work thus far has only explored LLMs for heuristic materials searches. Indeed, recent work obtains the uncertainty estimate -- an integral part of BO -- from point-estimated, non-Bayesian LLMs. In this work, we study the question of whether LLMs are actually useful to accelerate principled Bayesian optimization in the molecular space. We take a sober, dispassionate stance in answering this question. This is done by carefully (i) viewing LLMs as fixed feature extractors for standard but principled BO surrogate models and by (ii) leveraging parameter-effic",
    "path": "papers/24/02/2402.05015.json",
    "total_tokens": 951,
    "translated_title": "对于材料发现来说，对LLM的拜占庭优化是否真的有利？一个冷静的观察",
    "translated_abstract": "自动化是当代材料发现的重要基石。贝叶斯优化是这种工作流程的重要组成部分，它使科学家能够将先前的领域知识应用到对大规模分子空间的高效探索中。尽管这样的先前知识可以采用多种形式，但关于大型语言模型（LLMs）中所包含的辅助科学知识已经引起了很大的轰动。然而，迄今为止的研究仅探索了基于启发式材料搜索的LLMs。实际上，最近的研究通过从点估计的非贝叶斯LLMs中获得不确定性估计，这是BO的一个重要组成部分。在这项工作中，我们研究了LLMs是否真的有助于加速在分子空间中的正规贝叶斯优化。我们对这个问题采取了冷静、客观的立场。这是通过仔细地（i）将LLMs视为标准但正规的BO替代模型的固定特征提取器，以及（ii）利用参数效能来实现的。",
    "tldr": "本文研究了LLMs是否真的有助于加速在分子空间中的正规贝叶斯优化。通过将LLMs视为标准但正规的BO替代模型的固定特征提取器，并利用参数效能来实现。",
    "en_tdlr": "This paper investigates whether LLMs are truly helpful in accelerating principled Bayesian optimization in the molecular space, by treating LLMs as fixed feature extractors for standard but principled BO surrogate models and leveraging parameter efficiency."
}