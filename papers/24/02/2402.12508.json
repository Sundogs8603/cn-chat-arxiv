{
    "title": "SDEs for Minimax Optimization",
    "abstract": "arXiv:2402.12508v1 Announce Type: new  Abstract: Minimax optimization problems have attracted a lot of attention over the past few years, with applications ranging from economics to machine learning. While advanced optimization methods exist for such problems, characterizing their dynamics in stochastic scenarios remains notably challenging. In this paper, we pioneer the use of stochastic differential equations (SDEs) to analyze and compare Minimax optimizers. Our SDE models for Stochastic Gradient Descent-Ascent, Stochastic Extragradient, and Stochastic Hamiltonian Gradient Descent are provable approximations of their algorithmic counterparts, clearly showcasing the interplay between hyperparameters, implicit regularization, and implicit curvature-induced noise. This perspective also allows for a unified and simplified analysis strategy based on the principles of It\\^o calculus. Finally, our approach facilitates the derivation of convergence conditions and closed-form solutions for th",
    "link": "https://arxiv.org/abs/2402.12508",
    "context": "Title: SDEs for Minimax Optimization\nAbstract: arXiv:2402.12508v1 Announce Type: new  Abstract: Minimax optimization problems have attracted a lot of attention over the past few years, with applications ranging from economics to machine learning. While advanced optimization methods exist for such problems, characterizing their dynamics in stochastic scenarios remains notably challenging. In this paper, we pioneer the use of stochastic differential equations (SDEs) to analyze and compare Minimax optimizers. Our SDE models for Stochastic Gradient Descent-Ascent, Stochastic Extragradient, and Stochastic Hamiltonian Gradient Descent are provable approximations of their algorithmic counterparts, clearly showcasing the interplay between hyperparameters, implicit regularization, and implicit curvature-induced noise. This perspective also allows for a unified and simplified analysis strategy based on the principles of It\\^o calculus. Finally, our approach facilitates the derivation of convergence conditions and closed-form solutions for th",
    "path": "papers/24/02/2402.12508.json",
    "total_tokens": 844,
    "translated_title": "用于极小化优化的随机微分方程",
    "translated_abstract": "极小化优化问题在过去几年中吸引了很多关注，应用范围从经济学到机器学习。虽然存在针对这类问题的先进优化方法，但在随机场景中描述它们的动态仍然具有挑战性。本文开创性地使用随机微分方程（SDEs）来分析和比较极小化优化器。我们的SDE模型适用于随机梯度下降-上升、随机外推法和随机哈密尔顿梯度下降，可被证明是它们算法对应物的近似，清晰展示了超参数、隐式正则化和隐式曲率诱导噪声之间的相互作用。这种视角还允许基于伊藤微积分原理进行统一简化分析策略。最后，我们的方法有助于推导收敛条件和闭式解。",
    "tldr": "本文开创性地使用随机微分方程分析和比较极小化优化器，展示了超参数、隐式正则化和隐式曲率诱导噪声之间的相互作用，并提供了统一简化的分析策略。"
}