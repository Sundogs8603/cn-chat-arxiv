{
    "title": "Do Membership Inference Attacks Work on Large Language Models?",
    "abstract": "Membership inference attacks (MIAs) attempt to predict whether a particular datapoint is a member of a target model's training data. Despite extensive research on traditional machine learning models, there has been limited work studying MIA on the pre-training data of large language models (LLMs). We perform a large-scale evaluation of MIAs over a suite of language models (LMs) trained on the Pile, ranging from 160M to 12B parameters. We find that MIAs barely outperform random guessing for most settings across varying LLM sizes and domains. Our further analyses reveal that this poor performance can be attributed to (1) the combination of a large dataset and few training iterations, and (2) an inherently fuzzy boundary between members and non-members. We identify specific settings where LLMs have been shown to be vulnerable to membership inference and show that the apparent success in such settings can be attributed to a distribution shift, such as when members and non-members are drawn",
    "link": "https://arxiv.org/abs/2402.07841",
    "context": "Title: Do Membership Inference Attacks Work on Large Language Models?\nAbstract: Membership inference attacks (MIAs) attempt to predict whether a particular datapoint is a member of a target model's training data. Despite extensive research on traditional machine learning models, there has been limited work studying MIA on the pre-training data of large language models (LLMs). We perform a large-scale evaluation of MIAs over a suite of language models (LMs) trained on the Pile, ranging from 160M to 12B parameters. We find that MIAs barely outperform random guessing for most settings across varying LLM sizes and domains. Our further analyses reveal that this poor performance can be attributed to (1) the combination of a large dataset and few training iterations, and (2) an inherently fuzzy boundary between members and non-members. We identify specific settings where LLMs have been shown to be vulnerable to membership inference and show that the apparent success in such settings can be attributed to a distribution shift, such as when members and non-members are drawn",
    "path": "papers/24/02/2402.07841.json",
    "total_tokens": 943,
    "translated_title": "大型语言模型上的成员推断攻击是否奏效？",
    "translated_abstract": "成员推断攻击（MIAs）试图预测特定数据点是否属于目标模型的训练数据。尽管对传统机器学习模型进行了广泛研究，但在大型语言模型（LLMs）的预训练数据上对MIA的研究工作仍有限。我们对在Pile上训练的一系列语言模型（LMs）进行了大规模的MIA评估，参数范围从160M到12B。我们发现，在不同的LLM大小和领域的大多数设置中，MIAs几乎只能比随机猜测稍好。我们进一步分析发现，这种糟糕的性能可以归因于（1）大型数据集和少量训练迭代的组合，以及（2）成员和非成员之间的边界困惑。我们确定了LLMs易受成员推断攻击的特定设置，并表明在这些设置中取得的表面上的成功可以归因于分布的转变，例如当成员和非成员被绘制出来时。",
    "tldr": "这项研究在大规模语言模型上对成员推断攻击进行了评估，发现在大部分设置中，攻击几乎只能比随机猜测稍好，这种糟糕的性能是由于大型数据集和少量训练迭代的组合，以及成员和非成员之间的边界困惑所导致的。"
}