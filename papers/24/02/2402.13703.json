{
    "title": "Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?",
    "abstract": "arXiv:2402.13703v1 Announce Type: new  Abstract: The adaption of multilingual pre-trained Large Language Models (LLMs) into eloquent and helpful assistants is essential to facilitate their use across different language regions. In that spirit, we are the first to conduct an extensive study of the performance of multilingual models on parallel, multi-turn instruction-tuning benchmarks across a selection of the most-spoken Indo-European languages. We systematically examine the effects of language and instruction dataset size on a mid-sized, multilingual LLM by instruction-tuning it on parallel instruction-tuning datasets. Our results demonstrate that instruction-tuning on parallel instead of monolingual corpora benefits cross-lingual instruction following capabilities by up to 4.6%. Furthermore, we show that the Superficial Alignment Hypothesis does not hold in general, as the investigated multilingual 7B parameter model presents a counter-example requiring large-scale instruction-tuning",
    "link": "https://arxiv.org/abs/2402.13703",
    "context": "Title: Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?\nAbstract: arXiv:2402.13703v1 Announce Type: new  Abstract: The adaption of multilingual pre-trained Large Language Models (LLMs) into eloquent and helpful assistants is essential to facilitate their use across different language regions. In that spirit, we are the first to conduct an extensive study of the performance of multilingual models on parallel, multi-turn instruction-tuning benchmarks across a selection of the most-spoken Indo-European languages. We systematically examine the effects of language and instruction dataset size on a mid-sized, multilingual LLM by instruction-tuning it on parallel instruction-tuning datasets. Our results demonstrate that instruction-tuning on parallel instead of monolingual corpora benefits cross-lingual instruction following capabilities by up to 4.6%. Furthermore, we show that the Superficial Alignment Hypothesis does not hold in general, as the investigated multilingual 7B parameter model presents a counter-example requiring large-scale instruction-tuning",
    "path": "papers/24/02/2402.13703.json",
    "total_tokens": 930,
    "translated_title": "调查多语言教学调整：多语模型是否需要多语教学？",
    "translated_abstract": "arXiv:2402.13703v1 公告类型：新摘要：将多语言预训练大型语言模型（LLMs）转化为雄辩而有用的助手对促进它们在不同语言地区的使用至关重要。基于这一精神，我们是第一个对跨多种印欧语言进行大规模研究的研究者，旨在研究多语模型在选择的最常用的印欧语言上的并行、多轮教学调整基准测试的性能。我们系统地研究了语言和教学数据集大小对中型多语言LLM的影响，通过在并行教学调整数据集上进行教学调整。我们的结果表明，在并行教学调整而不是单语语料库上进行教学调整可以使跨语言遵循能力提高多达4.6%。此外，我们表明，表面对齐假设通常不成立，因为所调查的多语7B参数模型是一个反例，需要大规模的教学调整。",
    "tldr": "本研究是第一个对多语模型在不同印欧语言上的性能进行了广泛研究，发现在并行教学调整数据集上进行教学调整可以显著提升跨语言遵循能力，同时提出了对表面对齐假设的质疑"
}