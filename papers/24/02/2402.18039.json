{
    "title": "ResLoRA: Identity Residual Mapping in Low-Rank Adaption",
    "abstract": "arXiv:2402.18039v1 Announce Type: cross  Abstract: As one of the most popular parameter-efficient fine-tuning (PEFT) methods, low-rank adaptation (LoRA) is commonly applied to fine-tune large language models (LLMs). However, updating the weights of LoRA blocks effectively and expeditiously is challenging due to the long calculation path in the original model. To address this, we propose ResLoRA, an improved framework of LoRA. By adding residual paths during training and using merging approaches to eliminate these extra paths during inference, our method can achieve better results in fewer training steps without any extra trainable parameters or inference cost compared to LoRA. The experiments on NLG, NLU, and text-to-image tasks demonstrate the effectiveness of our method. To the best of our knowledge, ResLoRA is the first work that combines the residual path with LoRA. The code of our method is available at https://github.com/microsoft/LMOps/tree/main/reslora .",
    "link": "https://arxiv.org/abs/2402.18039",
    "context": "Title: ResLoRA: Identity Residual Mapping in Low-Rank Adaption\nAbstract: arXiv:2402.18039v1 Announce Type: cross  Abstract: As one of the most popular parameter-efficient fine-tuning (PEFT) methods, low-rank adaptation (LoRA) is commonly applied to fine-tune large language models (LLMs). However, updating the weights of LoRA blocks effectively and expeditiously is challenging due to the long calculation path in the original model. To address this, we propose ResLoRA, an improved framework of LoRA. By adding residual paths during training and using merging approaches to eliminate these extra paths during inference, our method can achieve better results in fewer training steps without any extra trainable parameters or inference cost compared to LoRA. The experiments on NLG, NLU, and text-to-image tasks demonstrate the effectiveness of our method. To the best of our knowledge, ResLoRA is the first work that combines the residual path with LoRA. The code of our method is available at https://github.com/microsoft/LMOps/tree/main/reslora .",
    "path": "papers/24/02/2402.18039.json",
    "total_tokens": 829,
    "translated_title": "ResLoRA：低秩适应中的身份残差映射",
    "translated_abstract": "作为最流行的参数高效微调（PEFT）方法之一，低秩适应（LoRA）通常应用于微调大型语言模型（LLMs）。然而，在原始模型中由于长计算路径，在有效而迅速地更新LoRA块的权重方面存在挑战。为了解决这个问题，我们提出了ResLoRA，这是LoRA的改进框架。通过在训练过程中添加残余路径，并使用合并方法在推断过程中消除这些额外路径，我们的方法可以在较少的训练步骤内取得更好的结果，而与LoRA相比，不需要额外的可训练参数或推断成本。对 NLG、NLU 和文本到图像任务上的实验表明了我们方法的有效性。据我们所知，ResLoRA是首个将残余路径与LoRA结合的工作。我们方法的代码可在 https://github.com/microsoft/LMOps/tree/main/reslora 上获取。",
    "tldr": "ResLoRA提出了在训练中添加残余路径并在推断过程中消除这些额外路径的方法，实现了更好的结果，比LoRA更加高效。",
    "en_tdlr": "ResLoRA introduces the addition of residual paths during training and elimination of these paths during inference, achieving better results more efficiently compared to LoRA."
}