{
    "title": "Are ID Embeddings Necessary? Whitening Pre-trained Text Embeddings for Effective Sequential Recommendation",
    "abstract": "arXiv:2402.10602v1 Announce Type: new  Abstract: Recent sequential recommendation models have combined pre-trained text embeddings of items with item ID embeddings to achieve superior recommendation performance. Despite their effectiveness, the expressive power of text features in these models remains largely unexplored. While most existing models emphasize the importance of ID embeddings in recommendations, our study takes a step further by studying sequential recommendation models that only rely on text features and do not necessitate ID embeddings. Upon examining pretrained text embeddings experimentally, we discover that they reside in an anisotropic semantic space, with an average cosine similarity of over 0.8 between items. We also demonstrate that this anisotropic nature hinders recommendation models from effectively differentiating between item representations and leads to degenerated performance. To address this issue, we propose to employ a pre-processing step known as whiten",
    "link": "https://arxiv.org/abs/2402.10602",
    "context": "Title: Are ID Embeddings Necessary? Whitening Pre-trained Text Embeddings for Effective Sequential Recommendation\nAbstract: arXiv:2402.10602v1 Announce Type: new  Abstract: Recent sequential recommendation models have combined pre-trained text embeddings of items with item ID embeddings to achieve superior recommendation performance. Despite their effectiveness, the expressive power of text features in these models remains largely unexplored. While most existing models emphasize the importance of ID embeddings in recommendations, our study takes a step further by studying sequential recommendation models that only rely on text features and do not necessitate ID embeddings. Upon examining pretrained text embeddings experimentally, we discover that they reside in an anisotropic semantic space, with an average cosine similarity of over 0.8 between items. We also demonstrate that this anisotropic nature hinders recommendation models from effectively differentiating between item representations and leads to degenerated performance. To address this issue, we propose to employ a pre-processing step known as whiten",
    "path": "papers/24/02/2402.10602.json",
    "total_tokens": 861,
    "translated_title": "是否需要ID Embeddings？为了提高序列推荐的有效性，对预训练文本嵌入进行白化处理",
    "translated_abstract": "最近的序列推荐模型将物品的预训练文本嵌入与物品ID嵌入相结合，以实现更优越的推荐性能。尽管它们很有效，但这些模型中文本特征的表现力仍然大多未被探索。大多数现有模型强调推荐中ID嵌入的重要性，而我们的研究更进一步，研究仅依赖文本特征而不需要ID嵌入的序列推荐模型。通过实验检验预训练文本嵌入，我们发现它们存在于一个各向异性语义空间中，项目之间的平均余弦相似度超过0.8。我们还证明，这种各向异性特性阻碍了推荐模型有效区分项目表示，并导致性能退化。为解决这个问题，我们提出采用一个名为白化的预处理步骤",
    "tldr": "深入研究表明，预训练文本嵌入存在各向异性语义空间，其高余弦相似度妨碍了推荐模型对项目表示进行有效区分，我们提出使用白化处理来解决这个问题",
    "en_tdlr": "In-depth study shows that pre-trained text embeddings reside in an anisotropic semantic space, hindering effective differentiation of item representations by recommendation models, we propose using whitening to address this issue."
}