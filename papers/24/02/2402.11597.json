{
    "title": "Multi-Task Inference: Can Large Language Models Follow Multiple Instructions at Once?",
    "abstract": "arXiv:2402.11597v1 Announce Type: new  Abstract: Large language models (LLMs) are typically prompted to follow a single instruction per inference call. In this work, we analyze whether LLMs also hold the capability to handle multiple instructions simultaneously, denoted as Multi-Task Inference. For this purpose, we introduce the MTI Bench(Multi-Task Inference Benchmark), a comprehensive evaluation benchmark encompassing 5,000 instances across 25 tasks. Each task in the MTI Bench involves 2 to 3 sub-tasks. As expected, we first demonstrate that Multi-Task Inference reduces the total inference time by 1.46 times in average since it does not require multiple inference calls. Interestingly, contrary to the expectation that LLMs would perform better when tasks are divided, we find that state-of-the-art LLMs, such as Llama-2-Chat-70B and GPT-4, show up to 7.3% and 12.4% improved performance with Multi-Task Inference compared to Single-Task Inference on the MTI Bench. We release the MTI Bench",
    "link": "https://arxiv.org/abs/2402.11597",
    "context": "Title: Multi-Task Inference: Can Large Language Models Follow Multiple Instructions at Once?\nAbstract: arXiv:2402.11597v1 Announce Type: new  Abstract: Large language models (LLMs) are typically prompted to follow a single instruction per inference call. In this work, we analyze whether LLMs also hold the capability to handle multiple instructions simultaneously, denoted as Multi-Task Inference. For this purpose, we introduce the MTI Bench(Multi-Task Inference Benchmark), a comprehensive evaluation benchmark encompassing 5,000 instances across 25 tasks. Each task in the MTI Bench involves 2 to 3 sub-tasks. As expected, we first demonstrate that Multi-Task Inference reduces the total inference time by 1.46 times in average since it does not require multiple inference calls. Interestingly, contrary to the expectation that LLMs would perform better when tasks are divided, we find that state-of-the-art LLMs, such as Llama-2-Chat-70B and GPT-4, show up to 7.3% and 12.4% improved performance with Multi-Task Inference compared to Single-Task Inference on the MTI Bench. We release the MTI Bench",
    "path": "papers/24/02/2402.11597.json",
    "total_tokens": 909,
    "translated_title": "多任务推断: 大型语言模型能够同时遵循多个指令吗？",
    "translated_abstract": "大型语言模型（LLMs）通常被要求在每次推断调用中遵循单个指令。在这项工作中，我们分析了LLMs是否也具有处理多个指令的能力，称为多任务推断。为此，我们引入了MTI Bench（多任务推断基准），一个包括25个任务的5000个实例的综合评估基准。MTI Bench中的每个任务都涉及2到3个子任务。正如预期的那样，我们首先证明了多任务推断平均降低了1.46倍的总推断时间，因为它不需要多次推断调用。有趣的是，与预期LLMs在任务被划分时表现更好相反，我们发现最先进的LLMs，例如Llama-2-Chat-70B和GPT-4，在MTI Bench上通过多任务推断与单任务推断相比可以获得高达7.3％和12.4％的性能改善。我们发布了MTI Bench。",
    "tldr": "大型语言模型在多任务推断时表现出更高的性能，相比单任务推断平均推断时间减少1.46倍，并且在MTI Bench上显示出最多高达12.4%的性能改善。"
}