{
    "title": "Exploring Learning Complexity for Downstream Data Pruning",
    "abstract": "The over-parameterized pre-trained models pose a great challenge to fine-tuning with limited computation resources. An intuitive solution is to prune the less informative samples from the fine-tuning dataset. A series of training-based scoring functions are proposed to quantify the informativeness of the data subset but the pruning cost becomes non-negligible due to the heavy parameter updating. For efficient pruning, it is viable to adapt the similarity scoring function of geometric-based methods from training-based to training-free. However, we empirically show that such adaption distorts the original pruning and results in inferior performance on the downstream tasks. In this paper, we propose to treat the learning complexity (LC) as the scoring function for classification and regression tasks. Specifically, the learning complexity is defined as the average predicted confidence of subnets with different capacities, which encapsulates data processing within a converged model. Then we",
    "link": "https://arxiv.org/abs/2402.05356",
    "context": "Title: Exploring Learning Complexity for Downstream Data Pruning\nAbstract: The over-parameterized pre-trained models pose a great challenge to fine-tuning with limited computation resources. An intuitive solution is to prune the less informative samples from the fine-tuning dataset. A series of training-based scoring functions are proposed to quantify the informativeness of the data subset but the pruning cost becomes non-negligible due to the heavy parameter updating. For efficient pruning, it is viable to adapt the similarity scoring function of geometric-based methods from training-based to training-free. However, we empirically show that such adaption distorts the original pruning and results in inferior performance on the downstream tasks. In this paper, we propose to treat the learning complexity (LC) as the scoring function for classification and regression tasks. Specifically, the learning complexity is defined as the average predicted confidence of subnets with different capacities, which encapsulates data processing within a converged model. Then we",
    "path": "papers/24/02/2402.05356.json",
    "total_tokens": 813,
    "translated_title": "探索用于下游数据修剪的学习复杂性",
    "translated_abstract": "过度参数化的预训练模型对于有限的计算资源的微调构成了巨大的挑战。一个直观的解决方案是从微调数据集中修剪掉信息较少的样本。提出了一系列基于训练的评分函数来量化数据子集的信息性，但由于参数更新的繁重，修剪成本变得不可忽视。为了高效修剪，将几何方法的相似度评分函数从基于训练的方法适应为无需训练的方法是可行的。然而，我们凭经验证明这种适应扭曲了原始的修剪并导致下游任务表现不佳。在本文中，我们提出将学习复杂性（LC）作为分类和回归任务的评分函数。具体来说，学习复杂性被定义为具有不同容量的子网络的平均预测置信度，它包含了在一个收敛模型中的数据处理。",
    "tldr": "本文提出了一种将学习复杂性作为分类和回归任务的评分函数，以解决在有限计算资源下微调过程中过度参数化模型的问题。",
    "en_tdlr": "This paper proposes using learning complexity as a scoring function for classification and regression tasks to address the problem of over-parameterized pre-trained models during fine-tuning with limited computation resources."
}