{
    "title": "Knowledge Distillation Based on Transformed Teacher Matching",
    "abstract": "arXiv:2402.11148v1 Announce Type: new  Abstract: As a technique to bridge logit matching and probability distribution matching, temperature scaling plays a pivotal role in knowledge distillation (KD). Conventionally, temperature scaling is applied to both teacher's logits and student's logits in KD. Motivated by some recent works, in this paper, we drop instead temperature scaling on the student side, and systematically study the resulting variant of KD, dubbed transformed teacher matching (TTM). By reinterpreting temperature scaling as a power transform of probability distribution, we show that in comparison with the original KD, TTM has an inherent R\\'enyi entropy term in its objective function, which serves as an extra regularization term. Extensive experiment results demonstrate that thanks to this inherent regularization, TTM leads to trained students with better generalization than the original KD. To further enhance student's capability to match teacher's power transformed proba",
    "link": "https://arxiv.org/abs/2402.11148",
    "context": "Title: Knowledge Distillation Based on Transformed Teacher Matching\nAbstract: arXiv:2402.11148v1 Announce Type: new  Abstract: As a technique to bridge logit matching and probability distribution matching, temperature scaling plays a pivotal role in knowledge distillation (KD). Conventionally, temperature scaling is applied to both teacher's logits and student's logits in KD. Motivated by some recent works, in this paper, we drop instead temperature scaling on the student side, and systematically study the resulting variant of KD, dubbed transformed teacher matching (TTM). By reinterpreting temperature scaling as a power transform of probability distribution, we show that in comparison with the original KD, TTM has an inherent R\\'enyi entropy term in its objective function, which serves as an extra regularization term. Extensive experiment results demonstrate that thanks to this inherent regularization, TTM leads to trained students with better generalization than the original KD. To further enhance student's capability to match teacher's power transformed proba",
    "path": "papers/24/02/2402.11148.json",
    "total_tokens": 877,
    "translated_title": "基于转换教师匹配的知识蒸馏",
    "translated_abstract": "作为连接逻辑匹配和概率分布匹配的技术，温度缩放在知识蒸馏（KD）中起着关键作用。传统上，在KD中，温度缩放被应用于教师的logits和学生的logits。受一些最近的研究启发，本文放弃了在学生端的温度缩放，系统地研究了由此产生的KD变体，称为转换教师匹配（TTM）。通过重新解释温度缩放作为概率分布的幂变换，我们表明，与原始的KD相比，TTM在其目标函数中具有固有的Rényi熵项，这充当了额外的正则化项。大量的实验结果表明，由于这种固有的正则化，TTM导致训练良好的学生比原始KD具有更好的泛化能力。",
    "tldr": "通过放弃学生端的温度缩放，本文提出了一种名为转换教师匹配（TTM）的知识蒸馏变体，通过对温度缩放的重新解释，TTM在目标函数中引入了固有的Rényi熵项，从而实现了更好的学生泛化效果。",
    "en_tdlr": "By dropping temperature scaling on the student side, this paper introduces a variant of knowledge distillation called Transformed Teacher Matching (TTM), which incorporates an inherent Rényi entropy term in its objective function through reinterpreting temperature scaling, leading to improved student generalization."
}