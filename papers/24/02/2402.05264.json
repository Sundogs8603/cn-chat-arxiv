{
    "title": "AdaBatchGrad: Combining Adaptive Batch Size and Adaptive Step Size",
    "abstract": "This paper presents a novel adaptation of the Stochastic Gradient Descent (SGD), termed AdaBatchGrad. This modification seamlessly integrates an adaptive step size with an adjustable batch size. An increase in batch size and a decrease in step size are well-known techniques to tighten the area of convergence of SGD and decrease its variance. A range of studies by R. Byrd and J. Nocedal introduced various testing techniques to assess the quality of mini-batch gradient approximations and choose the appropriate batch sizes at every step. Methods that utilized exact tests were observed to converge within $O(LR^2/\\varepsilon)$ iterations. Conversely, inexact test implementations sometimes resulted in non-convergence and erratic performance. To address these challenges, AdaBatchGrad incorporates both adaptive batch and step sizes, enhancing the method's robustness and stability. For exact tests, our approach converges in $O(LR^2/\\varepsilon)$ iterations, analogous to standard gradient descen",
    "link": "https://arxiv.org/abs/2402.05264",
    "context": "Title: AdaBatchGrad: Combining Adaptive Batch Size and Adaptive Step Size\nAbstract: This paper presents a novel adaptation of the Stochastic Gradient Descent (SGD), termed AdaBatchGrad. This modification seamlessly integrates an adaptive step size with an adjustable batch size. An increase in batch size and a decrease in step size are well-known techniques to tighten the area of convergence of SGD and decrease its variance. A range of studies by R. Byrd and J. Nocedal introduced various testing techniques to assess the quality of mini-batch gradient approximations and choose the appropriate batch sizes at every step. Methods that utilized exact tests were observed to converge within $O(LR^2/\\varepsilon)$ iterations. Conversely, inexact test implementations sometimes resulted in non-convergence and erratic performance. To address these challenges, AdaBatchGrad incorporates both adaptive batch and step sizes, enhancing the method's robustness and stability. For exact tests, our approach converges in $O(LR^2/\\varepsilon)$ iterations, analogous to standard gradient descen",
    "path": "papers/24/02/2402.05264.json",
    "total_tokens": 871,
    "translated_title": "AdaBatchGrad: 组合自适应批大小和自适应步长的方法",
    "translated_abstract": "本文提出了一种新颖的随机梯度下降（SGD）的改进方法，称为AdaBatchGrad。这种改进方法无缝地将自适应步长与可调整的批大小相结合。增加批大小和减小步长是紧束SGD收敛区间和减小其方差的已知技术。R. Byrd和J. Nocedal的一系列研究引入了各种测试技术来评估小批量梯度近似的质量，并在每个步骤选择合适的批量大小。采用准确测试的方法观察到在$O(LR^2/\\varepsilon)$迭代内收敛。然而，不准确的测试实现有时导致不收敛和不稳定的性能。为了解决这些挑战，AdaBatchGrad结合了自适应批量和步长的方法，增强了方法的稳健性和稳定性。对于准确测试，我们的方法以$O(LR^2/\\varepsilon)$迭代收敛，类似于标准梯度下降方法。",
    "tldr": "AdaBatchGrad是一种改进的随机梯度下降方法，通过组合自适应批大小和自适应步长来提高方法的稳健性和稳定性。",
    "en_tdlr": "AdaBatchGrad is an improved stochastic gradient descent method that enhances stability and robustness by combining adaptive batch size and adaptive step size."
}