{
    "title": "CompeteSMoE - Effective Training of Sparse Mixture of Experts via Competition",
    "abstract": "Sparse mixture of experts (SMoE) offers an appealing solution to scale up the model complexity beyond the mean of increasing the network's depth or width. However, effective training of SMoE has proven to be challenging due to the representation collapse issue, which causes parameter redundancy and limited representation potentials. In this work, we propose a competition mechanism to address this fundamental challenge of representation collapse. By routing inputs only to experts with the highest neural response, we show that, under mild assumptions, competition enjoys the same convergence rate as the optimal estimator. We further propose CompeteSMoE, an effective and efficient algorithm to train large language models by deploying a simple router that predicts the competition outcomes. Consequently, CompeteSMoE enjoys strong performance gains from the competition routing policy while having low computation overheads. Our extensive empirical evaluations on two transformer architectures a",
    "link": "https://arxiv.org/abs/2402.02526",
    "context": "Title: CompeteSMoE - Effective Training of Sparse Mixture of Experts via Competition\nAbstract: Sparse mixture of experts (SMoE) offers an appealing solution to scale up the model complexity beyond the mean of increasing the network's depth or width. However, effective training of SMoE has proven to be challenging due to the representation collapse issue, which causes parameter redundancy and limited representation potentials. In this work, we propose a competition mechanism to address this fundamental challenge of representation collapse. By routing inputs only to experts with the highest neural response, we show that, under mild assumptions, competition enjoys the same convergence rate as the optimal estimator. We further propose CompeteSMoE, an effective and efficient algorithm to train large language models by deploying a simple router that predicts the competition outcomes. Consequently, CompeteSMoE enjoys strong performance gains from the competition routing policy while having low computation overheads. Our extensive empirical evaluations on two transformer architectures a",
    "path": "papers/24/02/2402.02526.json",
    "total_tokens": 880,
    "translated_title": "CompeteSMoE - 通过竞争实现稀疏专家混合模型的有效训练",
    "translated_abstract": "稀疏专家混合模型（SMoE）为超越增加网络深度或宽度的模型复杂性提供了一种吸引人的解决方案。然而，有效训练SMoE的挑战在于表示崩溃问题，导致参数冗余和有限的表示能力。在本研究中，我们提出了一种竞争机制来解决表示崩溃的基本挑战。通过只将输入路由到具有最高神经响应的专家，我们展示了在温和假设下，竞争享有与最优估计器相同的收敛速率。我们进一步提出了CompeteSMoE，一种通过部署一个简单的路由器来预测竞争结果的有效且高效的大型语言模型训练算法。因此，CompeteSMoE在竞争路由策略方面获得了强大的性能提升，同时具有较低的计算开销。我们在两个Transformer架构上进行了广泛的实证评估。",
    "tldr": "本文提出了CompeteSMoE方法，通过竞争机制解决了稀疏专家混合模型的表示崩溃问题，实现了有效的训练，在大型语言模型上取得了强大性能提升。",
    "en_tdlr": "This paper proposes the CompeteSMoE method, which addresses the representation collapse issue in sparse mixture of experts through a competition mechanism, achieving effective training and strong performance gains in large language models."
}