{
    "title": "Source Identification in Abstractive Summarization",
    "abstract": "Neural abstractive summarization models make summaries in an end-to-end manner, and little is known about how the source information is actually converted into summaries. In this paper, we define input sentences that contain essential information in the generated summary as $\\textit{source sentences}$ and study how abstractive summaries are made by analyzing the source sentences. To this end, we annotate source sentences for reference summaries and system summaries generated by PEGASUS on document-summary pairs sampled from the CNN/DailyMail and XSum datasets. We also formulate automatic source sentence detection and compare multiple methods to establish a strong baseline for the task. Experimental results show that the perplexity-based method performs well in highly abstractive settings, while similarity-based methods perform robustly in relatively extractive settings. Our code and data are available at https://github.com/suhara/sourcesum.",
    "link": "https://arxiv.org/abs/2402.04677",
    "context": "Title: Source Identification in Abstractive Summarization\nAbstract: Neural abstractive summarization models make summaries in an end-to-end manner, and little is known about how the source information is actually converted into summaries. In this paper, we define input sentences that contain essential information in the generated summary as $\\textit{source sentences}$ and study how abstractive summaries are made by analyzing the source sentences. To this end, we annotate source sentences for reference summaries and system summaries generated by PEGASUS on document-summary pairs sampled from the CNN/DailyMail and XSum datasets. We also formulate automatic source sentence detection and compare multiple methods to establish a strong baseline for the task. Experimental results show that the perplexity-based method performs well in highly abstractive settings, while similarity-based methods perform robustly in relatively extractive settings. Our code and data are available at https://github.com/suhara/sourcesum.",
    "path": "papers/24/02/2402.04677.json",
    "total_tokens": 856,
    "translated_title": "摘要自动生成中的源信息识别",
    "translated_abstract": "神经抽象摘要生成模型以端到端的方式生成摘要，但目前对源信息如何转化为摘要的过程知之甚少。本文中，我们将包含生成摘要中基本信息的输入句子定义为“源句子”，并通过分析源句子来研究抽象摘要的生成方式。为此，我们对CNN/DailyMail和XSum数据集中的文档-摘要对进行了系统生成和参考摘要的源句子注释，并制定了自动源句子检测方法并比较了多种方法以建立强基线。实验结果表明，在高度抽象的情况下，基于困惑度的方法表现良好，而基于相似度的方法在相对提取式的情况下表现稳健。我们的代码和数据可以在https://github.com/suhara/sourcesum找到。",
    "tldr": "该论文研究了摘要自动生成中的源信息识别问题，并通过分析源句子的方法来探索抽象摘要的生成方式。他们通过注释源句子和比较多种方法建立了一个强基线，并发现在不同情景下，基于困惑度的方法和基于相似度的方法表现较好。",
    "en_tdlr": "This paper investigates the source identification problem in abstractive summarization and explores the approach of analyzing source sentences to understand how abstractive summaries are generated. They establish a strong baseline by annotating source sentences and comparing multiple methods, finding that perplexity-based methods perform well in highly abstractive settings while similarity-based methods perform robustly in relatively extractive settings."
}