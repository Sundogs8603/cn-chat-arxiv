{
    "title": "Enhancing Robotic Manipulation with AI Feedback from Multimodal Large Language Models",
    "abstract": "arXiv:2402.14245v1 Announce Type: cross  Abstract: Recently, there has been considerable attention towards leveraging large language models (LLMs) to enhance decision-making processes. However, aligning the natural language text instructions generated by LLMs with the vectorized operations required for execution presents a significant challenge, often necessitating task-specific details. To circumvent the need for such task-specific granularity, inspired by preference-based policy learning approaches, we investigate the utilization of multimodal LLMs to provide automated preference feedback solely from image inputs to guide decision-making. In this study, we train a multimodal LLM, termed CriticGPT, capable of understanding trajectory videos in robot manipulation tasks, serving as a critic to offer analysis and preference feedback. Subsequently, we validate the effectiveness of preference labels generated by CriticGPT from a reward modeling perspective. Experimental evaluation of the a",
    "link": "https://arxiv.org/abs/2402.14245",
    "context": "Title: Enhancing Robotic Manipulation with AI Feedback from Multimodal Large Language Models\nAbstract: arXiv:2402.14245v1 Announce Type: cross  Abstract: Recently, there has been considerable attention towards leveraging large language models (LLMs) to enhance decision-making processes. However, aligning the natural language text instructions generated by LLMs with the vectorized operations required for execution presents a significant challenge, often necessitating task-specific details. To circumvent the need for such task-specific granularity, inspired by preference-based policy learning approaches, we investigate the utilization of multimodal LLMs to provide automated preference feedback solely from image inputs to guide decision-making. In this study, we train a multimodal LLM, termed CriticGPT, capable of understanding trajectory videos in robot manipulation tasks, serving as a critic to offer analysis and preference feedback. Subsequently, we validate the effectiveness of preference labels generated by CriticGPT from a reward modeling perspective. Experimental evaluation of the a",
    "path": "papers/24/02/2402.14245.json",
    "total_tokens": 785,
    "translated_title": "利用多模态大语言模型的人工智能反馈增强机器人操作",
    "translated_abstract": "最近，人们开始关注利用大型语言模型（LLMs）来增强决策过程。然而，将由LLMs生成的自然语言文本指令与执行所需的向量化操作对齐，常常需要特定于任务的细节，这是一个重要挑战。为了避免对这种特定于任务的细微之处的需求，受到基于偏好的策略学习方法的启发，我们研究利用多模态LLMs提供自动偏好反馈，仅从图像输入中引导决策。在这项研究中，我们训练了一个名为CriticGPT的多模态LLM，能够理解机器人操作任务中的轨迹视频，作为一个评论员提供分析和偏好反馈。随后，我们从奖励建模的角度验证了CriticGPT生成的偏好标签的有效性。对一种",
    "tldr": "利用多模态大语言模型为机器人操作提供自动偏好反馈，提升决策效果"
}