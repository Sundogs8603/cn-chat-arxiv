{
    "title": "Fairness of Exposure in Online Restless Multi-armed Bandits",
    "abstract": "Restless multi-armed bandits (RMABs) generalize the multi-armed bandits where each arm exhibits Markovian behavior and transitions according to their transition dynamics. Solutions to RMAB exist for both offline and online cases. However, they do not consider the distribution of pulls among the arms. Studies have shown that optimal policies lead to unfairness, where some arms are not exposed enough. Existing works in fairness in RMABs focus heavily on the offline case, which diminishes their application in real-world scenarios where the environment is largely unknown. In the online scenario, we propose the first fair RMAB framework, where each arm receives pulls in proportion to its merit. We define the merit of an arm as a function of its stationary reward distribution. We prove that our algorithm achieves sublinear fairness regret in the single pull case $O(\\sqrt{T\\ln T})$, with $T$ being the total number of episodes. Empirically, we show that our algorithm performs well in the multi",
    "link": "https://arxiv.org/abs/2402.06348",
    "context": "Title: Fairness of Exposure in Online Restless Multi-armed Bandits\nAbstract: Restless multi-armed bandits (RMABs) generalize the multi-armed bandits where each arm exhibits Markovian behavior and transitions according to their transition dynamics. Solutions to RMAB exist for both offline and online cases. However, they do not consider the distribution of pulls among the arms. Studies have shown that optimal policies lead to unfairness, where some arms are not exposed enough. Existing works in fairness in RMABs focus heavily on the offline case, which diminishes their application in real-world scenarios where the environment is largely unknown. In the online scenario, we propose the first fair RMAB framework, where each arm receives pulls in proportion to its merit. We define the merit of an arm as a function of its stationary reward distribution. We prove that our algorithm achieves sublinear fairness regret in the single pull case $O(\\sqrt{T\\ln T})$, with $T$ being the total number of episodes. Empirically, we show that our algorithm performs well in the multi",
    "path": "papers/24/02/2402.06348.json",
    "total_tokens": 938,
    "translated_title": "在线不平衡多臂赌博机中的曝光公平性",
    "translated_abstract": "不平衡多臂赌博机（RMAB）推广了多臂赌博机，其中每个臂展示马尔可夫行为，并根据其过渡动态进行转换。针对RMAB的解决方案存在于离线和在线情况下。然而，它们没有考虑臂之间的拉取分布。研究表明，最优策略会导致不公平，其中一些臂不够暴露。现有的RMAB公平性工作主要集中在离线案例中，这降低了它们在环境大部分不知道的现实场景中的应用。在在线场景中，我们提出了第一个公平的RMAB框架，其中每个臂接收的拉取与其优势成比例。我们将臂的优势定义为其稳态奖励分布的函数。我们证明了我们的算法在单次拉取的公平性遗憾方面实现了次线性的结果$O(\\sqrt{T\\ln T})$，其中$T$是总的尝试次数。经验证明，我们的算法在多次拉取的情况下表现良好。",
    "tldr": "本研究提出了第一个在线的公平RMAB框架，通过将每个臂的拉取与其优势成比例，实现了公平的曝光。算法在单次拉取的公平性遗憾方面取得了次线性的结果$O(\\sqrt{T\\ln T})$。"
}