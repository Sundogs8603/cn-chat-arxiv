{
    "title": "HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual Natural Language Generalization",
    "abstract": "arXiv:2402.16694v1 Announce Type: new  Abstract: Large language models (LLMs) have made significant progress in generating codes from textual prompts. However, existing benchmarks have mainly concentrated on translating English prompts to multilingual codes or have been constrained to very limited natural languages (NLs). These benchmarks have overlooked the vast landscape of massively multilingual NL to multilingual code, leaving a critical gap in the evaluation of multilingual LLMs. In response, we introduce HumanEval-XL, a massively multilingual code generation benchmark specifically crafted to address this deficiency. HumanEval-XL establishes connections between 23 NLs and 12 programming languages (PLs), and comprises of a collection of 22,080 prompts with an average of 8.33 test cases. By ensuring parallel data across multiple NLs and PLs, HumanEval-XL offers a comprehensive evaluation platform for multilingual LLMs, allowing the assessment of the understanding of different NLs. O",
    "link": "https://arxiv.org/abs/2402.16694",
    "context": "Title: HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual Natural Language Generalization\nAbstract: arXiv:2402.16694v1 Announce Type: new  Abstract: Large language models (LLMs) have made significant progress in generating codes from textual prompts. However, existing benchmarks have mainly concentrated on translating English prompts to multilingual codes or have been constrained to very limited natural languages (NLs). These benchmarks have overlooked the vast landscape of massively multilingual NL to multilingual code, leaving a critical gap in the evaluation of multilingual LLMs. In response, we introduce HumanEval-XL, a massively multilingual code generation benchmark specifically crafted to address this deficiency. HumanEval-XL establishes connections between 23 NLs and 12 programming languages (PLs), and comprises of a collection of 22,080 prompts with an average of 8.33 test cases. By ensuring parallel data across multiple NLs and PLs, HumanEval-XL offers a comprehensive evaluation platform for multilingual LLMs, allowing the assessment of the understanding of different NLs. O",
    "path": "papers/24/02/2402.16694.json",
    "total_tokens": 892,
    "translated_title": "HumanEval-XL：面向跨语言自然语言泛化的多语言代码生成基准",
    "translated_abstract": "大型语言模型(LLMs)在从文本提示生成代码方面取得了重大进展。然而，现有的基准主要集中在将英语提示翻译为多语言代码，或者仅限于非常有限的自然语言(NLs)。这些基准忽视了庞大的作为对比的多语言NL到多语言代码的广阔领域，导致了对多语言LLM评估的重大空白。为解决这一问题，我们引入了HumanEval-XL，一个专门设计来解决这一不足的大规模多语言代码生成基准。HumanEval-XL建立了23种NL和12种编程语言(PLs)之间的联系，包括了22,080个提示的集合，平均有8.33个测试用例。通过确保在多个NL和PL之间的并行数据，HumanEval-XL为多语言LLMs提供了全面的评估平台，允许评估对不同NL的理解。",
    "tldr": "HumanEval-XL 是一个面向跨语言自然语言泛化的多语言代码生成基准，建立23种自然语言和12种编程语言的联系，提供了全面的评估平台，弥补了多语言LLM评估的重要空白。",
    "en_tdlr": "HumanEval-XL is a multilingual code generation benchmark for cross-lingual natural language generalization, establishing connections between 23 natural languages and 12 programming languages, providing a comprehensive evaluation platform, and filling a critical gap in multilingual LLM assessment."
}