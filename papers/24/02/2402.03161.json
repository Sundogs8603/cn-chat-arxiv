{
    "title": "Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization",
    "abstract": "In light of recent advances in multimodal Large Language Models (LLMs), there is increasing attention to scaling them from image-text data to more informative real-world videos. Compared to static images, video poses unique challenges for effective large-scale pre-training due to the modeling of its spatiotemporal dynamics. In this paper, we address such limitations in video-language pre-training with an efficient video decomposition that represents each video as keyframes and temporal motions. These are then adapted to an LLM using well-designed tokenizers that discretize visual and temporal information as a few tokens, thus enabling unified generative pre-training of videos, images, and text. At inference, the generated tokens from the LLM are carefully recovered to the original continuous pixel space to create various video content. Our proposed framework is both capable of comprehending and generating image and video content, as demonstrated by its competitive performance across 13",
    "link": "https://arxiv.org/abs/2402.03161",
    "context": "Title: Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nAbstract: In light of recent advances in multimodal Large Language Models (LLMs), there is increasing attention to scaling them from image-text data to more informative real-world videos. Compared to static images, video poses unique challenges for effective large-scale pre-training due to the modeling of its spatiotemporal dynamics. In this paper, we address such limitations in video-language pre-training with an efficient video decomposition that represents each video as keyframes and temporal motions. These are then adapted to an LLM using well-designed tokenizers that discretize visual and temporal information as a few tokens, thus enabling unified generative pre-training of videos, images, and text. At inference, the generated tokens from the LLM are carefully recovered to the original continuous pixel space to create various video content. Our proposed framework is both capable of comprehending and generating image and video content, as demonstrated by its competitive performance across 13",
    "path": "papers/24/02/2402.03161.json",
    "total_tokens": 891,
    "translated_title": "Video-LaVIT：统一的视频语言预训练及解耦的视觉-运动标记化方法",
    "translated_abstract": "鉴于多模态大型语言模型(LLMs)的最新进展，越来越多的关注如何将其从图像-文本数据扩展到更具信息价值的现实世界视频。与静态图像相比，视频在有效的大规模预训练中面临着独特的挑战，原因在于需要对其时空动态进行建模。本文针对视频-语言预训练中的这些限制，提出了一种高效的视频分解方法，将每个视频表示为关键帧和时间运动。然后，使用设计良好的标记器将视觉和时间信息离散化为少量标记，并将其适应于LLM，从而实现对视频、图像和文本的统一生成预训练。在推理过程中，从LLM生成的标记被仔细恢复到原始的连续像素空间，以生成各种视频内容。我们提出的框架既能理解又能生成图像和视频内容，并通过在13个任务上的竞争性表现加以证明。",
    "tldr": "这篇论文介绍了一种统一的视频语言预训练方法，通过解耦的视觉-运动标记化将视频表示为关键帧和时间运动，然后使用统一生成预训练技术来生成各种图像和视频内容。",
    "en_tdlr": "This paper presents a unified video-language pre-training method that represents videos as keyframes and temporal motions using decoupled visual-motional tokenization, enabling the generation of diverse image and video content through unified generative pre-training."
}