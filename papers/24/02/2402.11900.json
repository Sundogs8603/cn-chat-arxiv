{
    "title": "Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models",
    "abstract": "arXiv:2402.11900v1 Announce Type: new  Abstract: Recent work has showcased the powerful capability of large language models (LLMs) in recalling knowledge and reasoning. However, the reliability of LLMs in combining these two capabilities into reasoning through multi-hop facts has not been widely explored. This paper systematically investigates the possibilities for LLMs to utilize shortcuts based on direct connections between the initial and terminal entities of multi-hop knowledge. We first explore the existence of factual shortcuts through Knowledge Neurons, revealing that: (i) the strength of factual shortcuts is highly correlated with the frequency of co-occurrence of initial and terminal entities in the pre-training corpora; (ii) few-shot prompting leverage more shortcuts in answering multi-hop questions compared to chain-of-thought prompting. Then, we analyze the risks posed by factual shortcuts from the perspective of multi-hop knowledge editing. Analysis shows that approximatel",
    "link": "https://arxiv.org/abs/2402.11900",
    "context": "Title: Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models\nAbstract: arXiv:2402.11900v1 Announce Type: new  Abstract: Recent work has showcased the powerful capability of large language models (LLMs) in recalling knowledge and reasoning. However, the reliability of LLMs in combining these two capabilities into reasoning through multi-hop facts has not been widely explored. This paper systematically investigates the possibilities for LLMs to utilize shortcuts based on direct connections between the initial and terminal entities of multi-hop knowledge. We first explore the existence of factual shortcuts through Knowledge Neurons, revealing that: (i) the strength of factual shortcuts is highly correlated with the frequency of co-occurrence of initial and terminal entities in the pre-training corpora; (ii) few-shot prompting leverage more shortcuts in answering multi-hop questions compared to chain-of-thought prompting. Then, we analyze the risks posed by factual shortcuts from the perspective of multi-hop knowledge editing. Analysis shows that approximatel",
    "path": "papers/24/02/2402.11900.json",
    "total_tokens": 808,
    "translated_title": "在大型语言模型的知识编辑中探索多跳事实快捷方式",
    "translated_abstract": "最近的工作展示了大型语言模型（LLMs）在回忆知识和推理方面的强大能力。然而，LLMs在将这两种能力结合到通过多跳事实推理中尚未被广泛探索。本文系统地调查了LLMs利用基于多跳知识的初始和终端实体之间直接连接的快捷方式的可能性。我们首先通过Knowledge Neurons探索了事实快捷方式的存在，揭示出：(i)快捷方式的强度与预训练语料库中初始和终端实体的共现频率高度相关；（ii）少量提示在回答多跳问题时利用更多的快捷方式，相比之下，思维链提示更多。然后，我们从多跳知识编辑的角度分析了事实快捷方式带来的风险。分析表明，大约有",
    "tldr": "本文系统调查了在大型语言模型中利用事实快捷方式进行多跳事实推理的可能性，并分析了这种快捷方式可能带来的风险。"
}