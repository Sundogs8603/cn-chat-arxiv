{
    "title": "One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation",
    "abstract": "arXiv:2402.11683v1 Announce Type: new  Abstract: Evaluation of opinion summaries using conventional reference-based metrics rarely provides a holistic evaluation and has been shown to have a relatively low correlation with human judgments. Recent studies suggest using Large Language Models (LLMs) as reference-free metrics for NLG evaluation, however, they remain unexplored for opinion summary evaluation. Moreover, limited opinion summary evaluation datasets inhibit progress. To address this, we release the SUMMEVAL-OP dataset covering 7 dimensions related to the evaluation of opinion summaries: fluency, coherence, relevance, faithfulness, aspect coverage, sentiment consistency, and specificity. We investigate Op-I-Prompt a dimension-independent prompt, and Op-Prompts, a dimension-dependent set of prompts for opinion summary evaluation. Experiments indicate that Op-I-Prompt emerges as a good alternative for evaluating opinion summaries achieving an average Spearman correlation of 0.70 w",
    "link": "https://arxiv.org/abs/2402.11683",
    "context": "Title: One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation\nAbstract: arXiv:2402.11683v1 Announce Type: new  Abstract: Evaluation of opinion summaries using conventional reference-based metrics rarely provides a holistic evaluation and has been shown to have a relatively low correlation with human judgments. Recent studies suggest using Large Language Models (LLMs) as reference-free metrics for NLG evaluation, however, they remain unexplored for opinion summary evaluation. Moreover, limited opinion summary evaluation datasets inhibit progress. To address this, we release the SUMMEVAL-OP dataset covering 7 dimensions related to the evaluation of opinion summaries: fluency, coherence, relevance, faithfulness, aspect coverage, sentiment consistency, and specificity. We investigate Op-I-Prompt a dimension-independent prompt, and Op-Prompts, a dimension-dependent set of prompts for opinion summary evaluation. Experiments indicate that Op-I-Prompt emerges as a good alternative for evaluating opinion summaries achieving an average Spearman correlation of 0.70 w",
    "path": "papers/24/02/2402.11683.json",
    "total_tokens": 998,
    "translated_title": "一种支配所有的提示：LLMs 用于观点摘要评估",
    "translated_abstract": "使用传统基于参考的度量对观点摘要进行评估很少提供全面的评估，并且已经显示出与人类判断的相关性相对较低。最近的研究表明，使用大型语言模型（LLMs）作为无参考度量的NLG评估，然而，它们在观点摘要评估方面尚未被探索。此外，有限的观点摘要评估数据集阻碍了进展。为了解决这个问题，我们发布了涵盖与观点摘要评估相关的7个维度的SUMMEVAL-OP数据集：流畅性、连贯性、相关性、忠实度、方面覆盖、情感一致性和特异性。我们研究了 Op-I-Prompt，一个独立于维度的提示，以及 Op-Prompts，一个依赖于维度的用于观点摘要评估的提示集。实验证明，Op-I-Prompt 是评估观点摘要的一个很好的替代方案，实现了平均 Spearman 相关性为 0.70。",
    "tldr": "通过释放涵盖观点摘要评估相关七个维度的新数据集SUMMEVAL-OP，研究人员提出了 Op-I-Prompt 作为一种独立于维度的提示方法，以及 Op-Prompts 作为一组依赖于维度的提示，可以表明 Op-I-Prompt 是评估观点摘要的一个很好的替代方案，实现了平均 Spearman 相关性为 0.70。",
    "en_tdlr": "By releasing a new dataset, SUMMEVAL-OP, covering seven dimensions related to the evaluation of opinion summaries, researchers propose Op-I-Prompt as a dimension-independent prompt, as well as Op-Prompts as a dimension-dependent set of prompts. This work demonstrates that Op-I-Prompt emerges as a good alternative for evaluating opinion summaries, achieving an average Spearman correlation of 0.70."
}