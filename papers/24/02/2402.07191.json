{
    "title": "GSINA: Improving Subgraph Extraction for Graph Invariant Learning via Graph Sinkhorn Attention",
    "abstract": "Graph invariant learning (GIL) has been an effective approach to discovering the invariant relationships between graph data and its labels for different graph learning tasks under various distribution shifts. Many recent endeavors of GIL focus on extracting the invariant subgraph from the input graph for prediction as a regularization strategy to improve the generalization performance of graph learning. Despite their success, such methods also have various limitations in obtaining their invariant subgraphs. In this paper, we provide in-depth analyses of the drawbacks of existing works and propose corresponding principles of our invariant subgraph extraction: 1) the sparsity, to filter out the variant features, 2) the softness, for a broader solution space, and 3) the differentiability, for a soundly end-to-end optimization. To meet these principles in one shot, we leverage the Optimal Transport (OT) theory and propose a novel graph attention mechanism called Graph Sinkhorn Attention (G",
    "link": "https://arxiv.org/abs/2402.07191",
    "context": "Title: GSINA: Improving Subgraph Extraction for Graph Invariant Learning via Graph Sinkhorn Attention\nAbstract: Graph invariant learning (GIL) has been an effective approach to discovering the invariant relationships between graph data and its labels for different graph learning tasks under various distribution shifts. Many recent endeavors of GIL focus on extracting the invariant subgraph from the input graph for prediction as a regularization strategy to improve the generalization performance of graph learning. Despite their success, such methods also have various limitations in obtaining their invariant subgraphs. In this paper, we provide in-depth analyses of the drawbacks of existing works and propose corresponding principles of our invariant subgraph extraction: 1) the sparsity, to filter out the variant features, 2) the softness, for a broader solution space, and 3) the differentiability, for a soundly end-to-end optimization. To meet these principles in one shot, we leverage the Optimal Transport (OT) theory and propose a novel graph attention mechanism called Graph Sinkhorn Attention (G",
    "path": "papers/24/02/2402.07191.json",
    "total_tokens": 818,
    "translated_title": "GSINA: 通过图Sinkhorn Attention改进图不变学习中的子图提取",
    "translated_abstract": "图不变学习(GIL)是一种有效的方法，用于在不同分布变化下发现图数据与其标签之间的不变关系，以解决各种图学习任务。最近的GIL研究主要集中在从输入图中提取不变子图，作为规则化策略来提高图学习的泛化性能。然而，这些方法在获取不变子图方面也存在各种限制。本文分析了现有工作的缺点，并提出了提取不变子图的相应原则：1）稀疏性，以过滤掉变异特征；2）软性，以获得更广泛的解空间；和3）可微性，以进行端到端优化。为了在一次操作中满足这些原则，我们利用最优传输(OT)理论，并提出了一种新颖的图注意机制，称为图Sinkhorn Attention（G)",
    "tldr": "本文提出了一种改进的图不变学习方法，通过稀疏性、软性和可微性原则来提取不变子图，从而提高图学习的泛化性能。",
    "en_tdlr": "This paper proposes an improved method for graph invariant learning by extracting invariant subgraphs based on the principles of sparsity, softness, and differentiability, thereby enhancing the generalization performance of graph learning."
}