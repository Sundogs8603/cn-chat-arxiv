{
    "title": "OpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning",
    "abstract": "Trained on massive publicly available data, large language models (LLMs) have demonstrated tremendous success across various fields. While more data contributes to better performance, a disconcerting reality is that high-quality public data will be exhausted in a few years. In this paper, we offer a potential next step for contemporary LLMs: collaborative and privacy-preserving LLM training on the underutilized distributed private data via federated learning (FL), where multiple data owners collaboratively train a shared model without transmitting raw data. To achieve this, we build a concise, integrated, and research-friendly framework/codebase, named OpenFedLLM. It covers federated instruction tuning for enhancing instruction-following capability, federated value alignment for aligning with human values, and 7 representative FL algorithms. Besides, OpenFedLLM supports training on diverse domains, where we cover 8 training datasets; and provides comprehensive evaluations, where we cov",
    "link": "https://arxiv.org/abs/2402.06954",
    "context": "Title: OpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning\nAbstract: Trained on massive publicly available data, large language models (LLMs) have demonstrated tremendous success across various fields. While more data contributes to better performance, a disconcerting reality is that high-quality public data will be exhausted in a few years. In this paper, we offer a potential next step for contemporary LLMs: collaborative and privacy-preserving LLM training on the underutilized distributed private data via federated learning (FL), where multiple data owners collaboratively train a shared model without transmitting raw data. To achieve this, we build a concise, integrated, and research-friendly framework/codebase, named OpenFedLLM. It covers federated instruction tuning for enhancing instruction-following capability, federated value alignment for aligning with human values, and 7 representative FL algorithms. Besides, OpenFedLLM supports training on diverse domains, where we cover 8 training datasets; and provides comprehensive evaluations, where we cov",
    "path": "papers/24/02/2402.06954.json",
    "total_tokens": 946,
    "translated_title": "OpenFedLLM：通过联邦学习在分散的私有数据上训练大规模语言模型",
    "translated_abstract": "在大规模公开可用的数据上训练的大规模语言模型（LLM）在各个领域取得了巨大的成功。然而，更多的数据可以提高性能，但令人担忧的是，高质量的公开数据将在几年内用尽。在本文中，我们提供了对当代LLM的潜在下一步：通过联邦学习在未充分利用的分布式私有数据上进行协作和保护隐私的LLM训练，多个数据所有者共同训练一个共享模型，而不传输原始数据。为了实现这一目标，我们构建了一个简洁、集成和研究友好的框架/代码库，名为OpenFedLLM。它涵盖了用于增强模型遵循指令能力的联邦指令调优、用于与人类价值观对齐的联邦价值对齐以及7个代表性联邦学习算法。此外，OpenFedLLM支持在多领域进行训练，我们涵盖了8个训练数据集；提供全面的评估，我们涵盖了...",
    "tldr": "OpenFedLLM是一个简洁、集成、研究友好的框架/代码库，通过联邦学习在分散的私有数据上实现了大规模语言模型的协作和隐私保护训练，解决了公开数据枯竭的问题。",
    "en_tdlr": "OpenFedLLM is a concise, integrated, and research-friendly framework/codebase that enables collaborative and privacy-preserving training of large language models on decentralized private data via federated learning, addressing the issue of exhaustion of publicly available data."
}