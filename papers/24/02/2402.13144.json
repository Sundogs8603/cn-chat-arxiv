{
    "title": "Neural Network Diffusion",
    "abstract": "arXiv:2402.13144v1 Announce Type: new  Abstract: Diffusion models have achieved remarkable success in image and video generation. In this work, we demonstrate that diffusion models can also \\textit{generate high-performing neural network parameters}. Our approach is simple, utilizing an autoencoder and a standard latent diffusion model. The autoencoder extracts latent representations of a subset of the trained network parameters. A diffusion model is then trained to synthesize these latent parameter representations from random noise. It then generates new representations that are passed through the autoencoder's decoder, whose outputs are ready to use as new subsets of network parameters. Across various architectures and datasets, our diffusion process consistently generates models of comparable or improved performance over trained networks, with minimal additional cost. Notably, we empirically find that the generated models perform differently with the trained networks. Our results en",
    "link": "https://arxiv.org/abs/2402.13144",
    "context": "Title: Neural Network Diffusion\nAbstract: arXiv:2402.13144v1 Announce Type: new  Abstract: Diffusion models have achieved remarkable success in image and video generation. In this work, we demonstrate that diffusion models can also \\textit{generate high-performing neural network parameters}. Our approach is simple, utilizing an autoencoder and a standard latent diffusion model. The autoencoder extracts latent representations of a subset of the trained network parameters. A diffusion model is then trained to synthesize these latent parameter representations from random noise. It then generates new representations that are passed through the autoencoder's decoder, whose outputs are ready to use as new subsets of network parameters. Across various architectures and datasets, our diffusion process consistently generates models of comparable or improved performance over trained networks, with minimal additional cost. Notably, we empirically find that the generated models perform differently with the trained networks. Our results en",
    "path": "papers/24/02/2402.13144.json",
    "total_tokens": 781,
    "translated_title": "神经网络扩散",
    "translated_abstract": "扩散模型在图像和视频生成方面取得了显著成功。在这项工作中，我们展示了扩散模型也可以\\textit{生成表现优异的神经网络参数}。我们的方法很简单，利用了自动编码器和标准的潜在扩散模型。自动编码器提取了部分受训网络参数的潜在表示。然后训练了一个扩散模型来从随机噪声中合成这些潜在参数表示。它生成了新的表示，经过自动编码器的解码器，输出准备用作新的网络参数子集。在各种架构和数据集上，我们的扩散过程始终生成性能与经过训练的网络相当或更好的模型，附加成本极小。值得注意的是，我们在实证研究中发现，生成的模型与经过训练的网络表现出差异。",
    "tldr": "扩散模型能够生成表现优异的神经网络参数，生成的模型在性能上与训练网络相媲美甚至更好，且成本极低。",
    "en_tdlr": "Diffusion models can generate high-performing neural network parameters, producing models of comparable or improved performance over trained networks with minimal additional cost."
}