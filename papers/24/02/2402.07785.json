{
    "title": "HYPO: Hyperspherical Out-of-Distribution Generalization",
    "abstract": "Out-of-distribution (OOD) generalization is critical for machine learning models deployed in the real world. However, achieving this can be fundamentally challenging, as it requires the ability to learn invariant features across different domains or environments. In this paper, we propose a novel framework HYPO (HYPerspherical OOD generalization) that provably learns domain-invariant representations in a hyperspherical space. In particular, our hyperspherical learning algorithm is guided by intra-class variation and inter-class separation principles -- ensuring that features from the same class (across different training domains) are closely aligned with their class prototypes, while different class prototypes are maximally separated. We further provide theoretical justifications on how our prototypical learning objective improves the OOD generalization bound. Through extensive experiments on challenging OOD benchmarks, we demonstrate that our approach outperforms competitive baselines",
    "link": "https://arxiv.org/abs/2402.07785",
    "context": "Title: HYPO: Hyperspherical Out-of-Distribution Generalization\nAbstract: Out-of-distribution (OOD) generalization is critical for machine learning models deployed in the real world. However, achieving this can be fundamentally challenging, as it requires the ability to learn invariant features across different domains or environments. In this paper, we propose a novel framework HYPO (HYPerspherical OOD generalization) that provably learns domain-invariant representations in a hyperspherical space. In particular, our hyperspherical learning algorithm is guided by intra-class variation and inter-class separation principles -- ensuring that features from the same class (across different training domains) are closely aligned with their class prototypes, while different class prototypes are maximally separated. We further provide theoretical justifications on how our prototypical learning objective improves the OOD generalization bound. Through extensive experiments on challenging OOD benchmarks, we demonstrate that our approach outperforms competitive baselines",
    "path": "papers/24/02/2402.07785.json",
    "total_tokens": 831,
    "translated_title": "HYPO：超球面离群分布泛化",
    "translated_abstract": "离群（OOD）泛化对于在现实世界中部署的机器学习模型至关重要。然而，实现这一点可能从根本上具有挑战性，因为它需要学习跨不同领域或环境的不变特征的能力。在本文中，我们提出了一种新颖的框架HYPO（超球面OOD泛化），它能够证明在超球面空间中学习域不变表示。具体而言，我们的超球面学习算法是根据内类变化和间类分离原则进行引导的，确保来自同一类别的特征（跨不同训练领域）与其类别原型紧密对齐，而不同类别的原型之间则被最大化地分离。我们进一步提供了关于我们的原型学习目标如何改善OOD泛化界限的理论证明。通过对具有挑战性的OOD基准的大量实验，我们证明我们的方法优于竞争基准",
    "tldr": "HYPO是一个在超球面空间中学习域不变表示的框架，通过内类变化和间类分离原则的引导，提高了离群泛化性能。",
    "en_tdlr": "HYPO is a framework that learns domain-invariant representations in a hyperspherical space, improving out-of-distribution generalization performance through intra-class variation and inter-class separation principles."
}