{
    "title": "Continuous Unsupervised Domain Adaptation Using Stabilized Representations and Experience Replay",
    "abstract": "We introduce an algorithm for tackling the problem of unsupervised domain adaptation (UDA) in continual learning (CL) scenarios. The primary objective is to maintain model generalization under domain shift when new domains arrive continually through updating a base model when only unlabeled data is accessible in subsequent tasks. While there are many existing UDA algorithms, they typically require access to both the source and target domain datasets simultaneously. Conversely, existing CL approaches can handle tasks that all have labeled data. Our solution is based on stabilizing the learned internal distribution to enhances the model generalization on new domains. The internal distribution is modeled by network responses in hidden layer. We model this internal distribution using a Gaussian mixture model (GMM ) and update the model by matching the internally learned distribution of new domains to the estimated GMM. Additionally, we leverage experience replay to overcome the problem of ",
    "link": "https://arxiv.org/abs/2402.00580",
    "context": "Title: Continuous Unsupervised Domain Adaptation Using Stabilized Representations and Experience Replay\nAbstract: We introduce an algorithm for tackling the problem of unsupervised domain adaptation (UDA) in continual learning (CL) scenarios. The primary objective is to maintain model generalization under domain shift when new domains arrive continually through updating a base model when only unlabeled data is accessible in subsequent tasks. While there are many existing UDA algorithms, they typically require access to both the source and target domain datasets simultaneously. Conversely, existing CL approaches can handle tasks that all have labeled data. Our solution is based on stabilizing the learned internal distribution to enhances the model generalization on new domains. The internal distribution is modeled by network responses in hidden layer. We model this internal distribution using a Gaussian mixture model (GMM ) and update the model by matching the internally learned distribution of new domains to the estimated GMM. Additionally, we leverage experience replay to overcome the problem of ",
    "path": "papers/24/02/2402.00580.json",
    "total_tokens": 831,
    "translated_title": "使用稳定表示和经验回放的连续无监督领域自适应算法",
    "translated_abstract": "我们提出了一种算法来解决在连续学习场景下无监督领域自适应（UDA）问题。主要目标是在只能访问无标签数据的情况下，通过更新基础模型来保持模型在域转移下的泛化能力。虽然已经存在许多UDA算法，但它们通常需要同时访问源域和目标域数据集。相反，现有的连续学习方法可以处理所有具有标签数据的任务。我们的解决方案是通过稳定学习的内部分布来增强模型在新领域上的泛化能力。内部分布是通过隐藏层的网络响应来建模的。我们使用高斯混合模型（GMM）来建模这个内部分布，并通过将新领域的内部学习分布与估计的GMM进行匹配来更新模型。此外，我们利用经验回放来克服用户体验中的问题。",
    "tldr": "本论文提出了一种解决连续学习中无监督领域自适应问题的算法，通过稳定表示和经验回放来增强模型在新领域上的泛化能力。",
    "en_tdlr": "This paper proposes an algorithm for addressing the problem of unsupervised domain adaptation in continuous learning scenarios, enhancing model generalization on new domains through stabilized representations and experience replay."
}