{
    "title": "Metacognitive Retrieval-Augmented Large Language Models",
    "abstract": "arXiv:2402.11626v1 Announce Type: new  Abstract: Retrieval-augmented generation have become central in natural language processing due to their efficacy in generating factual content. While traditional methods employ single-time retrieval, more recent approaches have shifted towards multi-time retrieval for multi-hop reasoning tasks. However, these strategies are bound by predefined reasoning steps, potentially leading to inaccuracies in response generation. This paper introduces MetaRAG, an approach that combines the retrieval-augmented generation process with metacognition. Drawing from cognitive psychology, metacognition allows an entity to self-reflect and critically evaluate its cognitive processes. By integrating this, MetaRAG enables the model to monitor, evaluate, and plan its response strategies, enhancing its introspective reasoning abilities. Through a three-step metacognitive regulation pipeline, the model can identify inadequacies in initial cognitive responses and fixes t",
    "link": "https://arxiv.org/abs/2402.11626",
    "context": "Title: Metacognitive Retrieval-Augmented Large Language Models\nAbstract: arXiv:2402.11626v1 Announce Type: new  Abstract: Retrieval-augmented generation have become central in natural language processing due to their efficacy in generating factual content. While traditional methods employ single-time retrieval, more recent approaches have shifted towards multi-time retrieval for multi-hop reasoning tasks. However, these strategies are bound by predefined reasoning steps, potentially leading to inaccuracies in response generation. This paper introduces MetaRAG, an approach that combines the retrieval-augmented generation process with metacognition. Drawing from cognitive psychology, metacognition allows an entity to self-reflect and critically evaluate its cognitive processes. By integrating this, MetaRAG enables the model to monitor, evaluate, and plan its response strategies, enhancing its introspective reasoning abilities. Through a three-step metacognitive regulation pipeline, the model can identify inadequacies in initial cognitive responses and fixes t",
    "path": "papers/24/02/2402.11626.json",
    "total_tokens": 836,
    "translated_title": "元认知检索增强型大型语言模型",
    "translated_abstract": "由于其在生成事实内容方面的高效性，检索增强生成已经成为自然语言处理中的核心。 传统方法使用单次检索，而最近更倾向于多次检索以执行多跳推理任务。 然而，这些策略受到预定义推理步骤的限制，可能导致响应生成的不准确性。 本文介绍了MetaRAG，一种结合了检索增强生成过程与元认知的方法。 借鉴认知心理学，元认知使实体能够自我反思并批判性评估其认知过程。 通过整合这一点，MetaRAG使模型能够监视、评估和规划其响应策略，增强其内省推理能力。 通过三步元认知调节流程，模型能够识别初始认知响应中的不足之处，并加以修正。",
    "tldr": "本文介绍了MetaRAG，一种结合了检索增强生成过程与元认知的方法，通过元认知调节流程，使模型具有监视、评估和规划其响应策略的能力，增强了其内省推理能力。",
    "en_tdlr": "This paper introduces MetaRAG, an approach that combines the retrieval-augmented generation process with metacognition, enabling the model to monitor, evaluate, and plan its response strategies through a metacognitive regulation pipeline, enhancing its introspective reasoning abilities."
}