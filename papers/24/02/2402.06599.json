{
    "title": "On the Out-Of-Distribution Generalization of Multimodal Large Language Models",
    "abstract": "We investigate the generalization boundaries of current Multimodal Large Language Models (MLLMs) via comprehensive evaluation under out-of-distribution scenarios and domain-specific tasks. We evaluate their zero-shot generalization across synthetic images, real-world distributional shifts, and specialized datasets like medical and molecular imagery. Empirical results indicate that MLLMs struggle with generalization beyond common training domains, limiting their direct application without adaptation. To understand the cause of unreliable performance, we analyze three hypotheses: semantic misinterpretation, visual feature extraction insufficiency, and mapping deficiency. Results identify mapping deficiency as the primary hurdle. To address this problem, we show that in-context learning (ICL) can significantly enhance MLLMs' generalization, opening new avenues for overcoming generalization barriers. We further explore the robustness of ICL under distribution shifts and show its vulnerabil",
    "link": "https://arxiv.org/abs/2402.06599",
    "context": "Title: On the Out-Of-Distribution Generalization of Multimodal Large Language Models\nAbstract: We investigate the generalization boundaries of current Multimodal Large Language Models (MLLMs) via comprehensive evaluation under out-of-distribution scenarios and domain-specific tasks. We evaluate their zero-shot generalization across synthetic images, real-world distributional shifts, and specialized datasets like medical and molecular imagery. Empirical results indicate that MLLMs struggle with generalization beyond common training domains, limiting their direct application without adaptation. To understand the cause of unreliable performance, we analyze three hypotheses: semantic misinterpretation, visual feature extraction insufficiency, and mapping deficiency. Results identify mapping deficiency as the primary hurdle. To address this problem, we show that in-context learning (ICL) can significantly enhance MLLMs' generalization, opening new avenues for overcoming generalization barriers. We further explore the robustness of ICL under distribution shifts and show its vulnerabil",
    "path": "papers/24/02/2402.06599.json",
    "total_tokens": 923,
    "translated_title": "关于多模式大型语言模型的域外泛化能力研究",
    "translated_abstract": "我们通过全面评估在域外场景和特定领域任务下，当前多模式大型语言模型（MLLMs）的泛化边界。我们评估了它们在合成图像、真实世界分布偏移和医学以及分子图像等专业数据集上的零样本泛化能力。实证结果表明，MLLMs在超出常规训练领域的泛化方面存在困难，限制了它们的直接应用而需要进行适应。为了了解性能不可靠的原因，我们对三个假设进行了分析：语义错误解释、视觉特征提取不足和映射不足。结果表明映射不足是主要障碍。为了解决这个问题，我们表明，在上下文学习（ICL）可以显著提升 MLLMs 的泛化能力，为克服泛化障碍开辟了新的道路。我们进一步探索了 ICL 在分布偏移下的鲁棒性，并展示了它的脆弱性。",
    "tldr": "通过对多模式大型语言模型（MLLMs）进行全面评估，研究发现它们在超出训练领域的泛化方面存在困难，主要原因是映射不足。通过上下文学习（ICL）可以显著提升泛化能力，克服泛化障碍。"
}