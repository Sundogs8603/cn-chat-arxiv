{
    "title": "The Impact of Demonstrations on Multilingual In-Context Learning: A Multidimensional Analysis",
    "abstract": "arXiv:2402.12976v1 Announce Type: cross  Abstract: In-context learning is a popular inference strategy where large language models solve a task using only a few labelled demonstrations without needing any parameter updates. Compared to work on monolingual (English) in-context learning, multilingual in-context learning is under-explored, and we lack an in-depth understanding of the role of demonstrations in this context. To address this gap, we conduct a multidimensional analysis of multilingual in-context learning, experimenting with 5 models from different model families, 9 datasets covering classification and generation tasks, and 56 typologically diverse languages. Our results reveal that the effectiveness of demonstrations varies significantly across models, tasks, and languages. We also find that Llama 2-Chat, GPT-3.5, and GPT-4 are largely insensitive to the quality of demonstrations. Instead, a carefully crafted template often eliminates the benefits of demonstrations for some t",
    "link": "https://arxiv.org/abs/2402.12976",
    "context": "Title: The Impact of Demonstrations on Multilingual In-Context Learning: A Multidimensional Analysis\nAbstract: arXiv:2402.12976v1 Announce Type: cross  Abstract: In-context learning is a popular inference strategy where large language models solve a task using only a few labelled demonstrations without needing any parameter updates. Compared to work on monolingual (English) in-context learning, multilingual in-context learning is under-explored, and we lack an in-depth understanding of the role of demonstrations in this context. To address this gap, we conduct a multidimensional analysis of multilingual in-context learning, experimenting with 5 models from different model families, 9 datasets covering classification and generation tasks, and 56 typologically diverse languages. Our results reveal that the effectiveness of demonstrations varies significantly across models, tasks, and languages. We also find that Llama 2-Chat, GPT-3.5, and GPT-4 are largely insensitive to the quality of demonstrations. Instead, a carefully crafted template often eliminates the benefits of demonstrations for some t",
    "path": "papers/24/02/2402.12976.json",
    "total_tokens": 894,
    "translated_title": "示范对多语境学习的影响：多维分析",
    "translated_abstract": "在上下文学习中，示范经常被用作推理策略，在此策略中，大型语言模型仅使用少量标记的示范来解决任务，而无需进行任何参数更新。与单语言（英语）上下文学习的研究相比，多语种上下文学习尚未得到充分探讨，我们缺乏对该环境中示范作用的深入理解。为了填补这一空白，我们对多语境学习进行了多维分析，实验采用了来自不同模型家族的5个模型，涵盖了包括分类和生成任务在内的9个数据集，覆盖了56种类型上不同的语言。我们的结果表明，示范的有效性在模型、任务和语言之间存在显著差异。我们还发现，Llama 2-Chat、GPT-3.5和GPT-4对示范质量的敏感度较低。相反，精心设计的模板往往会消除一些模型对示范的益处。",
    "tldr": "通过多维分析发现，示范的有效性在多语种上下文学习中存在显著差异，其中部分模型对示范质量不敏感，而精心设计的模板可以消除对示范的依赖。",
    "en_tdlr": "Demonstrations vary significantly in effectiveness in multilingual in-context learning, with some models being insensitive to demonstration quality while a carefully crafted template can eliminate the need for demonstrations."
}