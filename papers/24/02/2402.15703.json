{
    "title": "Is Offline Decision Making Possible with Only Few Samples? Reliable Decisions in Data-Starved Bandits via Trust Region Enhancement",
    "abstract": "arXiv:2402.15703v1 Announce Type: cross  Abstract: What can an agent learn in a stochastic Multi-Armed Bandit (MAB) problem from a dataset that contains just a single sample for each arm? Surprisingly, in this work, we demonstrate that even in such a data-starved setting it may still be possible to find a policy competitive with the optimal one. This paves the way to reliable decision-making in settings where critical decisions must be made by relying only on a handful of samples.   Our analysis reveals that \\emph{stochastic policies can be substantially better} than deterministic ones for offline decision-making. Focusing on offline multi-armed bandits, we design an algorithm called Trust Region of Uncertainty for Stochastic policy enhancemenT (TRUST) which is quite different from the predominant value-based lower confidence bound approach. Its design is enabled by localization laws, critical radii, and relative pessimism. We prove that its sample complexity is comparable to that of L",
    "link": "https://arxiv.org/abs/2402.15703",
    "context": "Title: Is Offline Decision Making Possible with Only Few Samples? Reliable Decisions in Data-Starved Bandits via Trust Region Enhancement\nAbstract: arXiv:2402.15703v1 Announce Type: cross  Abstract: What can an agent learn in a stochastic Multi-Armed Bandit (MAB) problem from a dataset that contains just a single sample for each arm? Surprisingly, in this work, we demonstrate that even in such a data-starved setting it may still be possible to find a policy competitive with the optimal one. This paves the way to reliable decision-making in settings where critical decisions must be made by relying only on a handful of samples.   Our analysis reveals that \\emph{stochastic policies can be substantially better} than deterministic ones for offline decision-making. Focusing on offline multi-armed bandits, we design an algorithm called Trust Region of Uncertainty for Stochastic policy enhancemenT (TRUST) which is quite different from the predominant value-based lower confidence bound approach. Its design is enabled by localization laws, critical radii, and relative pessimism. We prove that its sample complexity is comparable to that of L",
    "path": "papers/24/02/2402.15703.json",
    "total_tokens": 938,
    "translated_title": "是否可以仅凭有限样本进行离线决策？通过信任区域增强在数据稀缺赌博机中可靠决策",
    "translated_abstract": "在一个只包含每个臂的单个样本数据集中，一个智能体能从随机多臂老虎机（MAB）问题中学到什么？令人惊讶的是，在这项工作中，我们证明即使在这种数据稀缺的环境中，仍然可能找到一个与最优策略竞争的策略。这为在必须仅依靠少数样本做出关键决策的环境中进行可靠的决策铺平了道路。我们的分析揭示了\\emph{随机策略对于离线决策能够显著优于确定性策略}。专注于离线多臂老虎机，我们设计了一种名为基于不确定性信任区域的随机策略增强（TRUST）的算法，这与主导性价值为基础的较低置信下界方法有很大不同。其设计得益于定位规律、临界半径和相对悲观主义。我们证明其样本复杂度与L的相当。",
    "tldr": "本文展示了即使在数据稀缺的情况下，仍然可能找到一个与最优策略竞争的随机策略，为在仅有少量样本下进行可靠决策铺平了道路。",
    "en_tdlr": "This work demonstrates the possibility of finding a competitive stochastic policy even in data-starved scenarios, paving the way for reliable decision-making with only a handful of samples."
}