{
    "title": "DoRA: Weight-Decomposed Low-Rank Adaptation",
    "abstract": "arXiv:2402.09353v1 Announce Type: new Abstract: Among the widely used parameter-efficient finetuning (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs. However, there still often exists an accuracy gap between these methods and full fine-tuning (FT). In this work, we first introduce a novel weight decomposition analysis to investigate the inherent differences between FT and LoRA. Aiming to resemble the learning capacity of FT from the findings, we propose Weight-Decomposed LowRank Adaptation (DoRA). DoRA decomposes the pre-trained weight into two components, magnitude and direction, for fine-tuning, specifically employing LoRA for directional updates to efficiently minimize the number of trainable parameters. By employing DoRA, we enhance both the learning capacity and training stability of LoRA while avoiding any additional inference overhead. DoRA consistently outperforms LoRA on fine-tuning LLaMA, LLaVA, and VL-B",
    "link": "https://arxiv.org/abs/2402.09353",
    "context": "Title: DoRA: Weight-Decomposed Low-Rank Adaptation\nAbstract: arXiv:2402.09353v1 Announce Type: new Abstract: Among the widely used parameter-efficient finetuning (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs. However, there still often exists an accuracy gap between these methods and full fine-tuning (FT). In this work, we first introduce a novel weight decomposition analysis to investigate the inherent differences between FT and LoRA. Aiming to resemble the learning capacity of FT from the findings, we propose Weight-Decomposed LowRank Adaptation (DoRA). DoRA decomposes the pre-trained weight into two components, magnitude and direction, for fine-tuning, specifically employing LoRA for directional updates to efficiently minimize the number of trainable parameters. By employing DoRA, we enhance both the learning capacity and training stability of LoRA while avoiding any additional inference overhead. DoRA consistently outperforms LoRA on fine-tuning LLaMA, LLaVA, and VL-B",
    "path": "papers/24/02/2402.09353.json",
    "total_tokens": 880,
    "translated_title": "DoRA: 分解权重的低秩适应",
    "translated_abstract": "在广泛使用的参数高效调整（PEFT）方法中，由于避免了额外的推理成本，LoRA及其变种方法因此变得非常流行。然而，这些方法与完全微调（FT）之间仍然存在精度差距。在这项工作中，我们首先引入了一种新颖的权重分解分析方法来研究FT和LoRA之间的内在差异。为了模拟FT的学习能力，我们提出了一种称为DoRA的权重分解低秩适应方法。DoRA将预训练的权重分解为两个组成部分，幅度和方向，并且具体使用LoRA进行方向更新，以有效地减少可训练参数的数量。通过使用DoRA，我们增强了LoRA的学习能力和训练稳定性，同时避免了任何额外的推理开销。在微调LLaMA，LLaVA和VL-B上，DoRA始终优于LoRA。",
    "tldr": "DoRA是一种将预训练权重分解为幅度和方向两个组成部分，并使用LoRA进行方向更新的低秩适应方法，通过增强学习能力和训练稳定性来提高对LLaMA，LLaVA和VL-B的微调性能。",
    "en_tdlr": "DoRA is a low-rank adaptation method that decomposes pretrained weights into magnitude and direction components, using LoRA for directional updates, to enhance learning capacity and training stability for fine-tuning LLaMA, LLaVA, and VL-B."
}