{
    "title": "Measuring Moral Inconsistencies in Large Language Models",
    "abstract": "A Large Language Model~(LLM) is considered consistent if semantically equivalent prompts produce semantically equivalent responses. Despite recent advancements showcasing the impressive capabilities of LLMs in conversational systems, we show that even state-of-the-art LLMs are highly inconsistent in their generations, questioning their reliability. Prior research has tried to measure this with task-specific accuracies. However, this approach is unsuitable for moral scenarios, such as the trolley problem, with no ``correct'' answer. To address this issue, we propose a novel information-theoretic measure called Semantic Graph Entropy~(SGE) to measure the consistency of an LLM in moral scenarios. We leverage ``Rules of Thumb''~(RoTs) to explain a model's decision-making strategies and further enhance our metric. Compared to existing consistency metrics, SGE correlates better with human judgments across five LLMs. In the future, we aim to investigate the root causes of LLM inconsistencies ",
    "link": "https://arxiv.org/abs/2402.01719",
    "context": "Title: Measuring Moral Inconsistencies in Large Language Models\nAbstract: A Large Language Model~(LLM) is considered consistent if semantically equivalent prompts produce semantically equivalent responses. Despite recent advancements showcasing the impressive capabilities of LLMs in conversational systems, we show that even state-of-the-art LLMs are highly inconsistent in their generations, questioning their reliability. Prior research has tried to measure this with task-specific accuracies. However, this approach is unsuitable for moral scenarios, such as the trolley problem, with no ``correct'' answer. To address this issue, we propose a novel information-theoretic measure called Semantic Graph Entropy~(SGE) to measure the consistency of an LLM in moral scenarios. We leverage ``Rules of Thumb''~(RoTs) to explain a model's decision-making strategies and further enhance our metric. Compared to existing consistency metrics, SGE correlates better with human judgments across five LLMs. In the future, we aim to investigate the root causes of LLM inconsistencies ",
    "path": "papers/24/02/2402.01719.json",
    "total_tokens": 974,
    "translated_title": "在大型语言模型中测量道德不一致性",
    "translated_abstract": "如果语义等价的提示产生语义等价的响应，那么大型语言模型(LLM)被认为是一致的。尽管最近的进展展示了LLMs在对话系统中令人印象深刻的能力，但我们表明即使是最先进的LLMs在生成方面也存在高度不一致性，这对它们的可靠性提出了质疑。先前的研究尝试用任务特定的准确度来衡量这一点。然而，这种方法对于没有“正确”答案的道德情景（例如，道路交运问题）是不合适的。为了解决这个问题，我们提出了一种新的信息论度量方法，称为语义图熵（SGE），来衡量LLM在道德情景中的一致性。我们利用“经验法则”（RoTs）来解释模型的决策策略，并进一步增强我们的度量方法。与现有的一致性度量方法相比，SGE与人类判断在五个LLMs上更好地相关。在未来，我们的目标是调查LLM不一致性的根本原因。",
    "tldr": "本研究提出了一种新的信息论度量方法，称为语义图熵（SGE），用于测量道德情景中大型语言模型（LLM）的一致性。与现有的一致性度量方法相比，SGE在五个LLMs上与人类判断更好地相关，为研究LLM不一致性的根本原因提供了新的思路。",
    "en_tdlr": "This study proposes a novel information-theoretic measure called Semantic Graph Entropy (SGE) to measure the consistency of Large Language Models (LLMs) in moral scenarios. Compared to existing metrics, SGE correlates better with human judgments across five LLMs, providing new insights into the root causes of LLM inconsistencies."
}