{
    "title": "Suppressing Pink Elephants with Direct Principle Feedback",
    "abstract": "Existing methods for controlling language models, such as RLHF and Constitutional AI, involve determining which LLM behaviors are desirable and training them into a language model. However, in many cases, it is desirable for LLMs to be controllable \\textit{at inference time}, so that they can be used in multiple contexts with diverse needs. We illustrate this with the \\textbf{Pink Elephant Problem}: instructing an LLM to avoid discussing a certain entity (a ``Pink Elephant''), and instead discuss a preferred entity (``Grey Elephant''). We apply a novel simplification of Constitutional AI, \\textbf{Direct Principle Feedback}, which skips the ranking of responses and uses DPO directly on critiques and revisions. Our results show that after DPF fine-tuning on our synthetic Pink Elephants dataset, our 13B fine-tuned LLaMA 2 model significantly outperforms Llama-2-13B-Chat and a prompted baseline, and performs as well as GPT-4 in on our curated test set assessing the Pink Elephant Problem.",
    "link": "https://arxiv.org/abs/2402.07896",
    "context": "Title: Suppressing Pink Elephants with Direct Principle Feedback\nAbstract: Existing methods for controlling language models, such as RLHF and Constitutional AI, involve determining which LLM behaviors are desirable and training them into a language model. However, in many cases, it is desirable for LLMs to be controllable \\textit{at inference time}, so that they can be used in multiple contexts with diverse needs. We illustrate this with the \\textbf{Pink Elephant Problem}: instructing an LLM to avoid discussing a certain entity (a ``Pink Elephant''), and instead discuss a preferred entity (``Grey Elephant''). We apply a novel simplification of Constitutional AI, \\textbf{Direct Principle Feedback}, which skips the ranking of responses and uses DPO directly on critiques and revisions. Our results show that after DPF fine-tuning on our synthetic Pink Elephants dataset, our 13B fine-tuned LLaMA 2 model significantly outperforms Llama-2-13B-Chat and a prompted baseline, and performs as well as GPT-4 in on our curated test set assessing the Pink Elephant Problem.",
    "path": "papers/24/02/2402.07896.json",
    "total_tokens": 922,
    "translated_title": "使用直接原则反馈抑制“粉色大象”",
    "translated_abstract": "目前的语言模型控制方法，如RLHF和宪法AI，涉及确定LLM行为的可取之处，并将其训练到语言模型中。然而，在许多情况下，希望LLM在推理时是可控制的，这样可以在多种需要的上下文中使用。我们用“粉色大象问题”作为例子：指示LLM避免讨论某个特定实体（“粉色大象”），而是讨论首选实体（“灰色大象”）。我们应用了一种新颖的Constitutional AI简化方法，“直接原则反馈”，它跳过了对响应的排名，直接在批评和修订上使用DPO。我们的结果表明，在我们合成的“粉色大象”数据集上进行DPF微调后，我们的13B微调LLaMA 2模型明显优于Llama-2-13B-Chat和提示基线，并且在评估“粉色大象问题”的精心选择测试集上表现与GPT-4一样好。",
    "tldr": "本研究提出了一种名为“直接原则反馈”的新方法，用于控制语言模型中的LLM行为。通过在批评和修订上直接使用DPO来跳过响应的排名，我们成功地解决了“粉色大象问题”并取得了显著的性能优势。",
    "en_tdlr": "This study proposes a new method called \"Direct Principle Feedback\" to control the behavior of language models. By skipping the ranking of responses and directly using DPO on critiques and revisions, we successfully address the \"Pink Elephant Problem\" and achieve significant performance improvement."
}