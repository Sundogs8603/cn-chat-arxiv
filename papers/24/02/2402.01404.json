{
    "title": "On Measuring Context Utilization in Document-Level MT Systems",
    "abstract": "Document-level translation models are usually evaluated using general metrics such as BLEU, which are not informative about the benefits of context. Current work on context-aware evaluation, such as contrastive methods, only measure translation accuracy on words that need context for disambiguation. Such measures cannot reveal whether the translation model uses the correct supporting context. We propose to complement accuracy-based evaluation with measures of context utilization. We find that perturbation-based analysis (comparing models' performance when provided with correct versus random context) is an effective measure of overall context utilization. For a finer-grained phenomenon-specific evaluation, we propose to measure how much the supporting context contributes to handling context-dependent discourse phenomena. We show that automatically-annotated supporting context gives similar conclusions to human-annotated context and can be used as alternative for cases where human annota",
    "link": "https://rss.arxiv.org/abs/2402.01404",
    "context": "Title: On Measuring Context Utilization in Document-Level MT Systems\nAbstract: Document-level translation models are usually evaluated using general metrics such as BLEU, which are not informative about the benefits of context. Current work on context-aware evaluation, such as contrastive methods, only measure translation accuracy on words that need context for disambiguation. Such measures cannot reveal whether the translation model uses the correct supporting context. We propose to complement accuracy-based evaluation with measures of context utilization. We find that perturbation-based analysis (comparing models' performance when provided with correct versus random context) is an effective measure of overall context utilization. For a finer-grained phenomenon-specific evaluation, we propose to measure how much the supporting context contributes to handling context-dependent discourse phenomena. We show that automatically-annotated supporting context gives similar conclusions to human-annotated context and can be used as alternative for cases where human annota",
    "path": "papers/24/02/2402.01404.json",
    "total_tokens": 901,
    "translated_title": "关于测量文档级机器翻译系统中上下文利用的研究",
    "translated_abstract": "文档级翻译模型通常使用一般的度量标准（如BLEU）进行评估，但这些度量标准无法提供关于上下文的好处的信息。目前关于上下文感知评估的工作，如对比方法，仅仅测量需要上下文以消除歧义的单词的翻译准确度。这样的测量方法无法揭示翻译模型是否正确地使用了支持性上下文。我们提议在基于准确度的评估中补充上下文利用度的度量标准。我们发现扰动分析（比较在提供正确上下文和随机上下文的情况下模型的性能）是一种有效的总体上下文利用度的度量方法。对于更细粒度的现象特定评估，我们提出测量支持性上下文对处理上下文相关话语现象的贡献程度。我们表明自动注释的支持性上下文与人工注释的上下文得出类似的结论，并可作为人工注释无法覆盖的情况下的替代品。",
    "tldr": "提出了一种补充基于准确度评估的上下文利用度度量方法，表明扰动分析是一种有效的总体上下文利用度的度量方法，同时提出了测量支持性上下文对处理上下文相关话语现象贡献程度的方法。",
    "en_tdlr": "Proposed a complementary measure of context utilization to accuracy-based evaluation, found that perturbation analysis is an effective measure of overall context utilization, and introduced a method to measure the contribution of supporting context in handling context-dependent discourse phenomena."
}