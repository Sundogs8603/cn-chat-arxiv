{
    "title": "TransLLaMa: LLM-based Simultaneous Translation System",
    "abstract": "Decoder-only large language models (LLMs) have recently demonstrated impressive capabilities in text generation and reasoning. Nonetheless, they have limited applications in simultaneous machine translation (SiMT), currently dominated by encoder-decoder transformers. This study demonstrates that, after fine-tuning on a small dataset comprising causally aligned source and target sentence pairs, a pre-trained open-source LLM can control input segmentation directly by generating a special \"wait\" token. This obviates the need for a separate policy and enables the LLM to perform English-German and English-Russian SiMT tasks with BLEU scores that are comparable to those of specific state-of-the-art baselines. We also evaluated closed-source models such as GPT-4, which displayed encouraging results in performing the SiMT task without prior training (zero-shot), indicating a promising avenue for enhancing future SiMT systems.",
    "link": "https://arxiv.org/abs/2402.04636",
    "context": "Title: TransLLaMa: LLM-based Simultaneous Translation System\nAbstract: Decoder-only large language models (LLMs) have recently demonstrated impressive capabilities in text generation and reasoning. Nonetheless, they have limited applications in simultaneous machine translation (SiMT), currently dominated by encoder-decoder transformers. This study demonstrates that, after fine-tuning on a small dataset comprising causally aligned source and target sentence pairs, a pre-trained open-source LLM can control input segmentation directly by generating a special \"wait\" token. This obviates the need for a separate policy and enables the LLM to perform English-German and English-Russian SiMT tasks with BLEU scores that are comparable to those of specific state-of-the-art baselines. We also evaluated closed-source models such as GPT-4, which displayed encouraging results in performing the SiMT task without prior training (zero-shot), indicating a promising avenue for enhancing future SiMT systems.",
    "path": "papers/24/02/2402.04636.json",
    "total_tokens": 930,
    "translated_title": "TransLLaMa: 基于LLM的同传翻译系统",
    "translated_abstract": "仅有解码器的大型语言模型（LLM）最近在文本生成和推理方面展示了令人印象深刻的能力。然而，在同时机器翻译（SiMT）方面，它们的应用有限，目前由编码器-解码器变压器主导。本研究表明，在一个包含因果对齐的源句子和目标句子对的小数据集上进行微调后，一个预训练的开源LLM能够通过生成一个特殊的“等待”标记来直接控制输入分段。这消除了独立政策的需要，使LLM能够执行与特定最新基线的BLEU分数相当的英德和英俄SiMT任务。我们还评估了闭源模型，如GPT-4，在没有先前训练（零-shot）的情况下表现出了令人鼓舞的结果，这表明了改进未来SiMT系统的一个有希望的途径。",
    "tldr": "本研究通过对一个小数据集进行微调，展示了一个预训练的开源LLM在同时机器翻译任务中控制输入分段的能力，从而消除了独立政策的需要，并实现了与最新基线相当的BLEU分数。同时，闭源模型GPT-4在零-shot下也显示出了令人鼓舞的结果，为提升未来的SiMT系统提供了有希望的方向。",
    "en_tdlr": "This study demonstrates that a pre-trained open-source LLM can control input segmentation directly, obviating the need for a separate policy, and achieve comparable BLEU scores to state-of-the-art baselines. Additionally, the closed-source model GPT-4 shows promising results in zero-shot simultaneous machine translation, indicating a potential avenue for enhancing future SiMT systems."
}