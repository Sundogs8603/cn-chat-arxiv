{
    "title": "Enhancing Large Language Models with Pseudo- and Multisource- Knowledge Graphs for Open-ended Question Answering",
    "abstract": "arXiv:2402.09911v1 Announce Type: cross  Abstract: Mitigating the hallucinations of Large Language Models (LLMs) and enhancing them is a crucial task. Although some existing methods employ model self-enhancement techniques, they fall short of effectively addressing unknown factual hallucinations. Using Knowledge Graph (KG) enhancement approaches fails to address the generalization across different KG sources and the enhancement of open-ended answer questions simultaneously. To tackle these limitations, there is a framework that combines Pseudo-Graph Generation and Atomic Knowledge Verification proposed. The enhancement of LLM using KG in an open-ended question-answering setting is implemented by leveraging the Pseudo-Graph Generation. Atomic Knowledge Verification utilizes atomic-level knowledge querying and verification to achieve generalizability under different KG sources. Compared to the baseline, this approach yields a minimum improvement of 11.5 in the ROUGE-L score for open-ende",
    "link": "https://arxiv.org/abs/2402.09911",
    "context": "Title: Enhancing Large Language Models with Pseudo- and Multisource- Knowledge Graphs for Open-ended Question Answering\nAbstract: arXiv:2402.09911v1 Announce Type: cross  Abstract: Mitigating the hallucinations of Large Language Models (LLMs) and enhancing them is a crucial task. Although some existing methods employ model self-enhancement techniques, they fall short of effectively addressing unknown factual hallucinations. Using Knowledge Graph (KG) enhancement approaches fails to address the generalization across different KG sources and the enhancement of open-ended answer questions simultaneously. To tackle these limitations, there is a framework that combines Pseudo-Graph Generation and Atomic Knowledge Verification proposed. The enhancement of LLM using KG in an open-ended question-answering setting is implemented by leveraging the Pseudo-Graph Generation. Atomic Knowledge Verification utilizes atomic-level knowledge querying and verification to achieve generalizability under different KG sources. Compared to the baseline, this approach yields a minimum improvement of 11.5 in the ROUGE-L score for open-ende",
    "path": "papers/24/02/2402.09911.json",
    "total_tokens": 863,
    "translated_title": "使用伪和多源知识图增强大型语言模型进行开放式问题回答",
    "translated_abstract": "减轻大型语言模型（LLM）的幻觉并增强它们是一项关键任务。尽管一些现有方法采用了模型自我增强技术，但它们在有效解决未知事实幻觉方面存在不足。使用知识图（KG）增强方法无法同时解决不同KG来源之间的泛化和开放式答案问题的增强。为了解决这些限制，提出了一种结合了伪图生成和原子级知识验证的框架。通过利用伪图生成来实现在开放式问题回答环境中使用KG增强LLM。原子级知识验证利用原子级知识查询和验证来实现在不同KG来源下的泛化能力。与基准相比，该方法在ROUGE-L分数上至少提升了11.5。",
    "tldr": "使用伪和多源知识图对大型语言模型进行增强，以改善其幻觉问题和提高性能。通过结合伪图生成和原子级知识验证的框架，在开放式问题回答环境中使用知识图可以提高ROUGE-L分数至少11.5。"
}