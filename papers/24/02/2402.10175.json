{
    "title": "Unlocking Structure Measuring: Introducing PDD, an Automatic Metric for Positional Discourse Coherence",
    "abstract": "arXiv:2402.10175v1 Announce Type: new  Abstract: Recent large language models (LLMs) have shown remarkable performance in aligning generated text with user intentions across various tasks. When it comes to long-form text generation, there has been a growing interest in generation from a discourse coherence perspective. However, existing lexical or semantic metrics such as BLEU, ROUGE, BertScore cannot effectively capture the discourse coherence. The development of discourse-specific automatic evaluation methods for assessing the output of LLMs warrants greater focus and exploration. In this paper, we present a novel automatic metric designed to quantify the discourse divergence between two long-form articles. Extensive experiments on three datasets from representative domains demonstrate that our metric aligns more closely with human preferences and GPT-4 coherence evaluation, outperforming existing evaluation methods.",
    "link": "https://arxiv.org/abs/2402.10175",
    "context": "Title: Unlocking Structure Measuring: Introducing PDD, an Automatic Metric for Positional Discourse Coherence\nAbstract: arXiv:2402.10175v1 Announce Type: new  Abstract: Recent large language models (LLMs) have shown remarkable performance in aligning generated text with user intentions across various tasks. When it comes to long-form text generation, there has been a growing interest in generation from a discourse coherence perspective. However, existing lexical or semantic metrics such as BLEU, ROUGE, BertScore cannot effectively capture the discourse coherence. The development of discourse-specific automatic evaluation methods for assessing the output of LLMs warrants greater focus and exploration. In this paper, we present a novel automatic metric designed to quantify the discourse divergence between two long-form articles. Extensive experiments on three datasets from representative domains demonstrate that our metric aligns more closely with human preferences and GPT-4 coherence evaluation, outperforming existing evaluation methods.",
    "path": "papers/24/02/2402.10175.json",
    "total_tokens": 822,
    "translated_title": "解锁结构测量：引入PDD，一种用于位置话语连贯的自动评估指标",
    "translated_abstract": "近期，大型语言模型(LLMs)在各种任务中展现了与用户意图对齐生成文本的卓越性能。当涉及长篇文本生成时，从话语连贯的角度出发对生成结果产生了越来越大的兴趣。然而，现有的词汇或语义评估指标如BLEU、ROUGE、BertScore不能有效捕捉话语连贯性。因此，发展针对LLMs生成结果的话语特定的自动评估方法需要更多的关注和探索。在本文中，我们提出了一种新颖的自动评估指标，用于量化两篇长篇文章之间的话语差异。对来自代表性领域的三个数据集进行广泛实验，结果显示我们的指标更接近人类偏好和GPT-4的连贯性评估，优于现有的评估方法。",
    "tldr": "本论文引入了一种新的自动评估指标PDD，用于评估长篇文章之间的话语连贯性，实验证明该指标更接近人类偏好和GPT-4的评估结果。",
    "en_tdlr": "This paper introduces a new automatic metric, PDD, for evaluating the discourse coherence between long-form articles. Experimental results demonstrate that the metric aligns more closely with human preferences and GPT-4 evaluations."
}