{
    "title": "FAST: Factorizable Attention for Speeding up Transformers",
    "abstract": "Motivated by the factorization inherent in the original fast multipole method and the improved fast Gauss transform we introduce a factorable form of attention that operates efficiently in high dimensions. This approach reduces the computational and memory complexity of the attention mechanism in transformers from $O(N^2)$ to $O(N)$. In comparison to previous attempts, our work presents a linearly scaled attention mechanism that maintains the full representation of the attention matrix without compromising on sparsification and incorporates the all-to-all relationship between tokens. We explore the properties of our new attention metric and conduct tests in various standard settings. Results indicate that our attention mechanism has a robust performance and holds significant promise for diverse applications where self-attention is used.",
    "link": "https://arxiv.org/abs/2402.07901",
    "context": "Title: FAST: Factorizable Attention for Speeding up Transformers\nAbstract: Motivated by the factorization inherent in the original fast multipole method and the improved fast Gauss transform we introduce a factorable form of attention that operates efficiently in high dimensions. This approach reduces the computational and memory complexity of the attention mechanism in transformers from $O(N^2)$ to $O(N)$. In comparison to previous attempts, our work presents a linearly scaled attention mechanism that maintains the full representation of the attention matrix without compromising on sparsification and incorporates the all-to-all relationship between tokens. We explore the properties of our new attention metric and conduct tests in various standard settings. Results indicate that our attention mechanism has a robust performance and holds significant promise for diverse applications where self-attention is used.",
    "path": "papers/24/02/2402.07901.json",
    "total_tokens": 861,
    "translated_title": "FAST: 用于加速Transformers的可分解注意力机制",
    "translated_abstract": "在原始的快速多极方法和改进后的快速高斯变换所固有的因子分解的驱动下，我们提出了一种在高维度中高效运行的可分解注意力形式。这种方法将Transformers中的注意力机制的计算和存储复杂度从O(N^2)降低到O(N)。与之前的尝试相比，我们的工作呈现了一个线性缩放的注意力机制，既保持了注意力矩阵的完整表示，又不妥协于稀疏化，并且包含了令牌之间的全互操作关系。我们探索了我们新的注意力度量的属性，并在各种标准设置下进行了测试。结果表明，我们的注意力机制具有稳健的性能，并在使用自我注意力的多样的应用中具有重要的潜力。",
    "tldr": "该论文介绍了一种可以加速Transformers模型的可分解注意力机制，通过引入因子分解形式的注意力，将注意力机制的复杂度从O(N^2)降低到O(N)，并 在维持注意力矩阵完整表示的同时保持稀疏性和所有-所有令牌关系。实验证明该注意力机制具有稳健的性能，并在不同应用中具有重要潜力。"
}