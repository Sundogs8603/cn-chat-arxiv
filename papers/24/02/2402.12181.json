{
    "title": "Revisiting Data Augmentation in Deep Reinforcement Learning",
    "abstract": "arXiv:2402.12181v1 Announce Type: cross  Abstract: Various data augmentation techniques have been recently proposed in image-based deep reinforcement learning (DRL). Although they empirically demonstrate the effectiveness of data augmentation for improving sample efficiency or generalization, which technique should be preferred is not always clear. To tackle this question, we analyze existing methods to better understand them and to uncover how they are connected. Notably, by expressing the variance of the Q-targets and that of the empirical actor/critic losses of these methods, we can analyze the effects of their different components and compare them. We furthermore formulate an explanation about how these methods may be affected by choosing different data augmentation transformations in calculating the target Q-values. This analysis suggests recommendations on how to exploit data augmentation in a more principled way. In addition, we include a regularization term called tangent prop,",
    "link": "https://arxiv.org/abs/2402.12181",
    "context": "Title: Revisiting Data Augmentation in Deep Reinforcement Learning\nAbstract: arXiv:2402.12181v1 Announce Type: cross  Abstract: Various data augmentation techniques have been recently proposed in image-based deep reinforcement learning (DRL). Although they empirically demonstrate the effectiveness of data augmentation for improving sample efficiency or generalization, which technique should be preferred is not always clear. To tackle this question, we analyze existing methods to better understand them and to uncover how they are connected. Notably, by expressing the variance of the Q-targets and that of the empirical actor/critic losses of these methods, we can analyze the effects of their different components and compare them. We furthermore formulate an explanation about how these methods may be affected by choosing different data augmentation transformations in calculating the target Q-values. This analysis suggests recommendations on how to exploit data augmentation in a more principled way. In addition, we include a regularization term called tangent prop,",
    "path": "papers/24/02/2402.12181.json",
    "total_tokens": 791,
    "translated_title": "重新审视深度强化学习中的数据增强",
    "translated_abstract": "最近在基于图像的深度强化学习(DRL)中提出了各种数据增强技术。尽管它们在实证中证明了数据增强对于提高样本效率或泛化的有效性，但并不总是清楚哪种技术应该被优先选择。为了解决这个问题，我们分析了现有方法以更好地理解它们并揭示它们之间的联系。值得注意的是，通过表达这些方法的Q-targets和经验演员/评论家损失的方差，我们可以分析它们不同组成部分的影响并进行比较。我们进一步提出了一个关于如何选择不同的数据增强转换来计算目标Q值可能会影响这些方法的解释。这项分析提出了如何更加原则地利用数据增强的建议。此外，我们还包括了一个称为切线prop的正则化项。",
    "tldr": "重新审视深度强化学习中的数据增强，分析不同方法的影响，提出了如何更加原则地利用数据增强的建议。"
}