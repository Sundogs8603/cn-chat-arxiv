{
    "title": "Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs",
    "abstract": "Natural Language Processing (NLP) research is increasingly focusing on the use of Large Language Models (LLMs), with some of the most popular ones being either fully or partially closed-source. The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers. Several attempts have been made to address this issue, but they are limited to anecdotal evidence and trial and error. Additionally, they overlook the problem of \\emph{indirect} data leaking, where models are iteratively improved by using data coming from users. In this work, we conduct the first systematic analysis of work using OpenAI's GPT-3.5 and GPT-4, the most prominently used LLMs today, in the context of data contamination. By analysing 255 papers and considering OpenAI's data usage policy, we extensively document the amount of data leaked to these models during the first year after the model's release. We report that these models have been g",
    "link": "https://arxiv.org/abs/2402.03927",
    "context": "Title: Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs\nAbstract: Natural Language Processing (NLP) research is increasingly focusing on the use of Large Language Models (LLMs), with some of the most popular ones being either fully or partially closed-source. The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers. Several attempts have been made to address this issue, but they are limited to anecdotal evidence and trial and error. Additionally, they overlook the problem of \\emph{indirect} data leaking, where models are iteratively improved by using data coming from users. In this work, we conduct the first systematic analysis of work using OpenAI's GPT-3.5 and GPT-4, the most prominently used LLMs today, in the context of data contamination. By analysing 255 papers and considering OpenAI's data usage policy, we extensively document the amount of data leaked to these models during the first year after the model's release. We report that these models have been g",
    "path": "papers/24/02/2402.03927.json",
    "total_tokens": 903,
    "translated_title": "泄漏、欺骗、重复：封闭源LLMs中的数据污染和评估不端行为",
    "translated_abstract": "自然语言处理（NLP）研究越来越多地关注使用大型语言模型（LLMs），其中一些最受欢迎的模型是完全或部分封闭源的。对于模型细节，特别是训练数据的缺乏访问权限，使研究人员反复对数据污染提出了担忧。虽然已经进行了一些尝试来解决这个问题，但仅限于个别案例和试错方法。此外，他们忽视了“间接”数据泄漏的问题，即模型通过使用用户提供的数据进行迭代改进。本研究在OpenAI的GPT-3.5和GPT-4使用上进行了首次系统分析，这些是当今最广泛使用的LLMs，并考虑了OpenAI的数据使用政策，详细记录了模型发布后一年内泄露给这些模型的数据量。我们报告了这些模型在主要数据污染方面。",
    "tldr": "该论文研究了封闭源LLMs中的数据污染和评估不端行为。通过对255篇论文的分析和OpenAI的数据使用政策考虑，研究人员发现这些模型在第一年发布后存在泄露数据的问题。"
}