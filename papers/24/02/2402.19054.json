{
    "title": "RobWE: Robust Watermark Embedding for Personalized Federated Learning Model Ownership Protection",
    "abstract": "arXiv:2402.19054v1 Announce Type: cross  Abstract: Embedding watermarks into models has been widely used to protect model ownership in federated learning (FL). However, existing methods are inadequate for protecting the ownership of personalized models acquired by clients in personalized FL (PFL). This is due to the aggregation of the global model in PFL, resulting in conflicts over clients' private watermarks. Moreover, malicious clients may tamper with embedded watermarks to facilitate model leakage and evade accountability. This paper presents a robust watermark embedding scheme, named RobWE, to protect the ownership of personalized models in PFL. We first decouple the watermark embedding of personalized models into two parts: head layer embedding and representation layer embedding. The head layer belongs to clients' private part without participating in model aggregation, while the representation layer is the shared part for aggregation. For representation layer embedding, we emplo",
    "link": "https://arxiv.org/abs/2402.19054",
    "context": "Title: RobWE: Robust Watermark Embedding for Personalized Federated Learning Model Ownership Protection\nAbstract: arXiv:2402.19054v1 Announce Type: cross  Abstract: Embedding watermarks into models has been widely used to protect model ownership in federated learning (FL). However, existing methods are inadequate for protecting the ownership of personalized models acquired by clients in personalized FL (PFL). This is due to the aggregation of the global model in PFL, resulting in conflicts over clients' private watermarks. Moreover, malicious clients may tamper with embedded watermarks to facilitate model leakage and evade accountability. This paper presents a robust watermark embedding scheme, named RobWE, to protect the ownership of personalized models in PFL. We first decouple the watermark embedding of personalized models into two parts: head layer embedding and representation layer embedding. The head layer belongs to clients' private part without participating in model aggregation, while the representation layer is the shared part for aggregation. For representation layer embedding, we emplo",
    "path": "papers/24/02/2402.19054.json",
    "total_tokens": 698,
    "translated_title": "RobWE: Robust Watermark Embedding for Personalized Federated Learning Model Ownership Protection",
    "translated_abstract": "在联邦学习（FL）中，将水印嵌入到模型中已被广泛应用以保护模型所有权。然而，现有方法对于保护个性化联邦学习（PFL）中客户获取的个性化模型所有权是不足的。本文提出了一种名为RobWE的强大水印嵌入方案，以保护PFL中个性化模型的所有权。我们首先将个性化模型的水印嵌入分为两部分：头部层嵌入和表示层嵌入。",
    "tldr": "本文提出了一种名为RobWE的强大水印嵌入方案，以保护个性化联邦学习中个性化模型的所有权。",
    "en_tdlr": "The paper introduces a robust watermark embedding scheme, named RobWE, to protect the ownership of personalized models in Personalized Federated Learning (PFL)."
}