{
    "title": "Fact-and-Reflection (FaR) Improves Confidence Calibration of Large Language Models",
    "abstract": "arXiv:2402.17124v1 Announce Type: new  Abstract: For a LLM to be trustworthy, its confidence level should be well-calibrated with its actual performance. While it is now common sense that LLM performances are greatly impacted by prompts, the confidence calibration in prompting LLMs has yet to be thoroughly explored. In this paper, we explore how different prompting strategies influence LLM confidence calibration and how it could be improved. We conduct extensive experiments on six prompting methods in the question-answering context and we observe that, while these methods help improve the expected LLM calibration, they also trigger LLMs to be over-confident when responding to some instances. Inspired by human cognition, we propose Fact-and-Reflection (FaR) prompting, which improves the LLM calibration in two steps. First, FaR elicits the known \"facts\" that are relevant to the input prompt from the LLM. And then it asks the model to \"reflect\" over them to generate the final answer. Expe",
    "link": "https://arxiv.org/abs/2402.17124",
    "context": "Title: Fact-and-Reflection (FaR) Improves Confidence Calibration of Large Language Models\nAbstract: arXiv:2402.17124v1 Announce Type: new  Abstract: For a LLM to be trustworthy, its confidence level should be well-calibrated with its actual performance. While it is now common sense that LLM performances are greatly impacted by prompts, the confidence calibration in prompting LLMs has yet to be thoroughly explored. In this paper, we explore how different prompting strategies influence LLM confidence calibration and how it could be improved. We conduct extensive experiments on six prompting methods in the question-answering context and we observe that, while these methods help improve the expected LLM calibration, they also trigger LLMs to be over-confident when responding to some instances. Inspired by human cognition, we propose Fact-and-Reflection (FaR) prompting, which improves the LLM calibration in two steps. First, FaR elicits the known \"facts\" that are relevant to the input prompt from the LLM. And then it asks the model to \"reflect\" over them to generate the final answer. Expe",
    "path": "papers/24/02/2402.17124.json",
    "total_tokens": 869,
    "translated_title": "Fact-and-Reflection（FaR）改善大型语言模型的置信校准",
    "translated_abstract": "要使LLM值得信赖，其置信水平应与实际表现良好校准。尽管现在普遍认为LLM的表现在很大程度上受到提示的影响，但提示LLM中的置信校准尚未得到彻底探讨。本文探讨了不同提示策略如何影响LLM的置信校准以及如何改进。我们在问答环境中对六种提示方法进行了大量实验，我们观察到，尽管这些方法有助于改进LLM的预期校准，但也会导致LLM在响应某些实例时过于自信。受人类认知启发，我们提出了Fact-and-Reflection（FaR）提示，它通过两个步骤改善了LLM的校准。首先，FaR从LLM中获取与输入提示相关的已知“事实”。然后要求模型“反思”它们以生成最终答案。",
    "tldr": "提出了Fact-and-Reflection（FaR）提示策略，通过引入已知“事实”并要求模型“反思”，在两个步骤中改进了大型语言模型的置信校准"
}