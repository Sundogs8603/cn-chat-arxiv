{
    "title": "Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning",
    "abstract": "arXiv:2402.18865v1 Announce Type: cross  Abstract: Existing research has shown that large language models (LLMs) exhibit remarkable performance in language understanding and generation. However, when LLMs are continuously fine-tuned on complex and diverse domain-specific downstream tasks, the inference performance on historical tasks decreases dramatically, which is known as a catastrophic forgetting problem. A trade-off needs to be kept between learning plasticity and memory stability. Plenty of existing works have explored strategies like memory replay, regularization and parameter isolation, but little is known about the geometric connection of various adjacent minima in the continual LLMs fine-tuning scenarios. In this work, we investigate the geometric connections of different minima through the lens of mode connectivity, which means different minima can be connected by a low-loss valley. Through extensive experiments, we uncover the mode connectivity phenomenon in the LLMs contin",
    "link": "https://arxiv.org/abs/2402.18865",
    "context": "Title: Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning\nAbstract: arXiv:2402.18865v1 Announce Type: cross  Abstract: Existing research has shown that large language models (LLMs) exhibit remarkable performance in language understanding and generation. However, when LLMs are continuously fine-tuned on complex and diverse domain-specific downstream tasks, the inference performance on historical tasks decreases dramatically, which is known as a catastrophic forgetting problem. A trade-off needs to be kept between learning plasticity and memory stability. Plenty of existing works have explored strategies like memory replay, regularization and parameter isolation, but little is known about the geometric connection of various adjacent minima in the continual LLMs fine-tuning scenarios. In this work, we investigate the geometric connections of different minima through the lens of mode connectivity, which means different minima can be connected by a low-loss valley. Through extensive experiments, we uncover the mode connectivity phenomenon in the LLMs contin",
    "path": "papers/24/02/2402.18865.json",
    "total_tokens": 818,
    "translated_title": "分析和减少参数高效调整中的灾难性遗忘",
    "translated_abstract": "已有研究显示，大型语言模型（LLMs）在语言理解和生成方面表现出色。然而，当LLMs不断在复杂和多样化的特定领域下游任务上进行微调时，对历史任务的推理性能会急剧下降，这被称为灾难性遗忘问题。需要在学习可塑性和记忆稳定性之间保持权衡。已有很多研究探讨了诸如记忆重放、正则化和参数隔离等策略，但在连续的LLMs微调场景中，对各个相邻极小值之间的几何连接知之甚少。在这项工作中，我们通过模式连接的视角调查了不同极小值之间的几何连接，这意味着不同极小值可以通过一个低损失的山谷相连接。通过大量实验，我们揭示了LLMs微调中的模式连接现象。",
    "tldr": "通过模式连接调查了连续微调中不同极小值之间的几何连接，揭示了大型语言模型中的灾难性遗忘问题。",
    "en_tdlr": "Investigated the geometric connections of different minima through mode connectivity in continual fine-tuning, revealing the catastrophic forgetting issue in large language models."
}