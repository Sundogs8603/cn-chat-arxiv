{
    "title": "Generative Cross-Modal Retrieval: Memorizing Images in Multimodal Language Models for Retrieval and Beyond",
    "abstract": "arXiv:2402.10805v1 Announce Type: cross  Abstract: The recent advancements in generative language models have demonstrated their ability to memorize knowledge from documents and recall knowledge to respond to user queries effectively. Building upon this capability, we propose to enable multimodal large language models (MLLMs) to memorize and recall images within their parameters. Given a user query for visual content, the MLLM is anticipated to \"recall\" the relevant image from its parameters as the response. Achieving this target presents notable challenges, including inbuilt visual memory and visual recall schemes within MLLMs. To address these challenges, we introduce a generative cross-modal retrieval framework, which assigns unique identifier strings to represent images and involves two training steps: learning to memorize and learning to retrieve. The first step focuses on training the MLLM to memorize the association between images and their respective identifiers. The latter ste",
    "link": "https://arxiv.org/abs/2402.10805",
    "context": "Title: Generative Cross-Modal Retrieval: Memorizing Images in Multimodal Language Models for Retrieval and Beyond\nAbstract: arXiv:2402.10805v1 Announce Type: cross  Abstract: The recent advancements in generative language models have demonstrated their ability to memorize knowledge from documents and recall knowledge to respond to user queries effectively. Building upon this capability, we propose to enable multimodal large language models (MLLMs) to memorize and recall images within their parameters. Given a user query for visual content, the MLLM is anticipated to \"recall\" the relevant image from its parameters as the response. Achieving this target presents notable challenges, including inbuilt visual memory and visual recall schemes within MLLMs. To address these challenges, we introduce a generative cross-modal retrieval framework, which assigns unique identifier strings to represent images and involves two training steps: learning to memorize and learning to retrieve. The first step focuses on training the MLLM to memorize the association between images and their respective identifiers. The latter ste",
    "path": "papers/24/02/2402.10805.json",
    "total_tokens": 810,
    "translated_title": "生成式跨模态检索：在多模态语言模型中存储图像用于检索及更多应用",
    "translated_abstract": "近期生成式语言模型的进展表明其能够记忆文档中的知识并有效地回答用户查询。在此能力基础上，我们提出了使多模态大型语言模型（MLLMs）能够在其参数内存储和检索图像的方法。给定用户对视觉内容的查询，MLLM被期望能够从其参数中“回忆”相关图像作为响应。实现这一目标面临着显著挑战，其中包括MLLM内置的视觉记忆和视觉检索方案。为解决这些挑战，我们引入了一个生成式跨模态检索框架，该框架为图像分配唯一标识符字符串，并涉及两个训练步骤：学习记忆和学习检索。第一步侧重于训练MLLM记忆图像与其标识符之间的关联。",
    "tldr": "提出了一种生成式跨模态检索框架，在多模态语言模型中实现了存储和检索图像的能力",
    "en_tdlr": "Introduced a generative cross-modal retrieval framework that enables storing and retrieving images within multimodal language models"
}