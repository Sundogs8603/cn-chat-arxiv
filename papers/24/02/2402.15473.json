{
    "title": "Leveraging Domain Knowledge for Efficient Reward Modelling in RLHF: A Case-Study in E-Commerce Opinion Summarization",
    "abstract": "arXiv:2402.15473v1 Announce Type: new  Abstract: Reinforcement Learning from Human Feedback (RLHF) has become a dominating strategy in steering Language Models (LMs) towards human values/goals. The key to the strategy is employing a reward model ({$\\varphi$}) which can reflect a latent reward model with humans. While this strategy has proven to be effective, the training methodology requires a lot of human preference annotation (usually of the order of tens of thousands) to train {$\\varphi$}. Such large-scale preference annotations can be achievable if the reward model can be ubiquitously used. However, human values/goals are subjective and depend on the nature of the task. This poses a challenge in collecting diverse preferences for downstream applications. To address this, we propose a novel methodology to infuse domain knowledge into {$\\varphi$}, which reduces the size of preference annotation required. We validate our approach in E-Commerce Opinion Summarization, with a significant",
    "link": "https://arxiv.org/abs/2402.15473",
    "context": "Title: Leveraging Domain Knowledge for Efficient Reward Modelling in RLHF: A Case-Study in E-Commerce Opinion Summarization\nAbstract: arXiv:2402.15473v1 Announce Type: new  Abstract: Reinforcement Learning from Human Feedback (RLHF) has become a dominating strategy in steering Language Models (LMs) towards human values/goals. The key to the strategy is employing a reward model ({$\\varphi$}) which can reflect a latent reward model with humans. While this strategy has proven to be effective, the training methodology requires a lot of human preference annotation (usually of the order of tens of thousands) to train {$\\varphi$}. Such large-scale preference annotations can be achievable if the reward model can be ubiquitously used. However, human values/goals are subjective and depend on the nature of the task. This poses a challenge in collecting diverse preferences for downstream applications. To address this, we propose a novel methodology to infuse domain knowledge into {$\\varphi$}, which reduces the size of preference annotation required. We validate our approach in E-Commerce Opinion Summarization, with a significant",
    "path": "papers/24/02/2402.15473.json",
    "total_tokens": 895,
    "translated_title": "利用领域知识在RLHF中高效建模奖励：电子商务意见摘要的案例研究",
    "translated_abstract": "从人类反馈中进行强化学习（RLHF）已成为引导语言模型（LMs）朝向人类价值/目标的主导策略。该策略的关键在于使用一个能够反映与人类相关的潜在奖励模型的奖励模型（{$\\varphi$}）。虽然这一策略已被证明是有效的，但训练方法需要大量人类偏好注释（通常数量级为数万）来训练{$\\varphi$}。如果奖励模型可以被普遍使用，这种大规模偏好注释是可以实现的。然而，人类价值/目标是主观的，并且取决于任务的性质。这对于收集下游应用程序的多样化偏好构成挑战。为了解决这个问题，我们提出了一种新颖的方法，将领域知识融入{$\\varphi$}中，从而减少所需注释的大小。我们在电子商务意见摘要中验证了我们的方法，具有显著的",
    "tldr": "提出了一种方法，在RLHF中利用领域知识来降低训练奖励模型所需的大量人类偏好注释数量。",
    "en_tdlr": "Proposing a method to reduce the large amount of human preference annotations required for training reward models in RLHF by leveraging domain knowledge."
}