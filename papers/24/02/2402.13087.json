{
    "title": "How Does Selection Leak Privacy: Revisiting Private Selection and Improved Results for Hyper-parameter Tuning",
    "abstract": "arXiv:2402.13087v1 Announce Type: new  Abstract: We study the problem of guaranteeing Differential Privacy (DP) in hyper-parameter tuning, a crucial process in machine learning involving the selection of the best run from several. Unlike many private algorithms, including the prevalent DP-SGD, the privacy implications of tuning remain insufficiently understood. Recent works propose a generic private solution for the tuning process, yet a fundamental question still persists: is the current privacy bound for this solution tight?   This paper contributes both positive and negative answers to this question. Initially, we provide studies affirming the current privacy analysis is indeed tight in a general sense. However, when we specifically study the hyper-parameter tuning problem, such tightness no longer holds. This is first demonstrated by applying privacy audit on the tuning process. Our findings underscore a substantial gap between the current theoretical privacy bound and the empirica",
    "link": "https://arxiv.org/abs/2402.13087",
    "context": "Title: How Does Selection Leak Privacy: Revisiting Private Selection and Improved Results for Hyper-parameter Tuning\nAbstract: arXiv:2402.13087v1 Announce Type: new  Abstract: We study the problem of guaranteeing Differential Privacy (DP) in hyper-parameter tuning, a crucial process in machine learning involving the selection of the best run from several. Unlike many private algorithms, including the prevalent DP-SGD, the privacy implications of tuning remain insufficiently understood. Recent works propose a generic private solution for the tuning process, yet a fundamental question still persists: is the current privacy bound for this solution tight?   This paper contributes both positive and negative answers to this question. Initially, we provide studies affirming the current privacy analysis is indeed tight in a general sense. However, when we specifically study the hyper-parameter tuning problem, such tightness no longer holds. This is first demonstrated by applying privacy audit on the tuning process. Our findings underscore a substantial gap between the current theoretical privacy bound and the empirica",
    "path": "papers/24/02/2402.13087.json",
    "total_tokens": 914,
    "translated_title": "选择如何泄漏隐私：重新审视私有选择及超参数调整的改进结果",
    "translated_abstract": "我们研究了在超参数调整中保证差分隐私(DP)的问题，这是机器学习中一个关键的过程，涉及从几个运行中选择最佳的过程。与许多私有算法（包括普遍存在的DP-SGD）不同，调整的隐私影响仍然不够了解。最近的研究提出了一个通用的私有解决方案用于调整过程，然而一个根本的问题仍然存在：当前解决方案的隐私界是否紧密？本文对这个问题提出了积极和消极的答案。最初，我们提供的研究证实了当前的隐私分析在一般意义上确实是紧密的。然而，当我们专门研究超参数调整问题时，这种紧密性则不再成立。首先，通过对调整过程进行隐私审计来证明了这一点。我们的研究结果突显了当前理论隐私界与实证之间存在重大差距。",
    "tldr": "本论文探讨了超参数调整中的隐私性问题，发现当前的隐私分析在一般情况下是紧密的，但在特定的超参数调整问题上则不再成立，并通过隐私审计揭示了当前理论隐私界与实证之间的显著差距。",
    "en_tdlr": "This paper investigates the privacy issues in hyper-parameter tuning, finding that the current privacy analysis is tight in general but not in the specific hyper-parameter tuning scenario, and reveals a significant gap between the current theoretical privacy bound and empirical findings through privacy audit."
}