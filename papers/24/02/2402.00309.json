{
    "title": "An Exam-based Evaluation Approach Beyond Traditional Relevance Judgments",
    "abstract": "Current IR evaluation is based on relevance judgments, created either manually or automatically, with decisions outsourced to Large Language Models (LLMs). We offer an alternative paradigm, that never relies on relevance judgments in any form. Instead, a text is defined as relevant if it contains information that enables the answering of key questions. We use this idea to design the EXAM Answerability Metric to evaluate information retrieval/generation systems for their ability to provide topically relevant information.   We envision the role of a human judge to edit and define an exam question bank that will test for the presence of relevant information in text. We support this step by generating an initial set of exam questions. In the next phase, an LLM-based question answering system will automatically grade system responses by tracking which exam questions are answerable with which system responses. We propose two evaluation measures, the recall-oriented EXAM Cover metric, and the",
    "link": "https://arxiv.org/abs/2402.00309",
    "context": "Title: An Exam-based Evaluation Approach Beyond Traditional Relevance Judgments\nAbstract: Current IR evaluation is based on relevance judgments, created either manually or automatically, with decisions outsourced to Large Language Models (LLMs). We offer an alternative paradigm, that never relies on relevance judgments in any form. Instead, a text is defined as relevant if it contains information that enables the answering of key questions. We use this idea to design the EXAM Answerability Metric to evaluate information retrieval/generation systems for their ability to provide topically relevant information.   We envision the role of a human judge to edit and define an exam question bank that will test for the presence of relevant information in text. We support this step by generating an initial set of exam questions. In the next phase, an LLM-based question answering system will automatically grade system responses by tracking which exam questions are answerable with which system responses. We propose two evaluation measures, the recall-oriented EXAM Cover metric, and the",
    "path": "papers/24/02/2402.00309.json",
    "total_tokens": 889,
    "translated_title": "超越传统相关性判断的基于考试的评估方法",
    "translated_abstract": "当前的信息检索评估基于相关性判断，这些判断可以手动或自动创建，并且决策通常被外包给大型语言模型（LLMs）。我们提供了一种替代范式，从不依赖任何形式的相关性判断。相反，如果一段文本包含可以回答关键问题的信息，我们将其定义为相关性。我们利用这个思想设计了EXAM可回答度指标，以评估信息检索/生成系统提供主题相关信息的能力。我们设想一个人类评委的角色是编辑和定义一个考试题库，用于测试文本中相关信息的存在。我们通过生成一个初始的考试题目集来支持这一步骤。在下一个阶段，基于LLM的问答系统将通过跟踪可以回答哪个考试题目来自动评分系统的答案。我们提出了两种评估指标：回忆导向的EXAM覆盖度指标和",
    "tldr": "该论文提出了一种超越传统相关性判断的基于考试的评估方法，不依赖相关性判断，而是根据文本是否包含能回答关键问题的信息来判断相关性。通过设计EXAM可回答度指标和两种评估措施，可以评估信息检索/生成系统的主题相关信息提供能力。",
    "en_tdlr": "This paper proposes an exam-based evaluation approach that goes beyond traditional relevance judgments, defining relevance based on the presence of information that can answer key questions in a text. It introduces the EXAM Answerability Metric and evaluation measures to assess the ability of information retrieval/generation systems to provide topically relevant information."
}