{
    "title": "Unintended Impacts of LLM Alignment on Global Representation",
    "abstract": "arXiv:2402.15018v1 Announce Type: new  Abstract: Before being deployed for user-facing applications, developers align Large Language Models (LLMs) to user preferences through a variety of procedures, such as Reinforcement Learning From Human Feedback (RLHF) and Direct Preference Optimization (DPO). Current evaluations of these procedures focus on benchmarks of instruction following, reasoning, and truthfulness. However, human preferences are not universal, and aligning to specific preference sets may have unintended effects. We explore how alignment impacts performance along three axes of global representation: English dialects, multilingualism, and opinions from and about countries worldwide. Our results show that current alignment procedures create disparities between English dialects and global opinions. We find alignment improves capabilities in several languages. We conclude by discussing design decisions that led to these unintended impacts and recommendations for more equitable ",
    "link": "https://arxiv.org/abs/2402.15018",
    "context": "Title: Unintended Impacts of LLM Alignment on Global Representation\nAbstract: arXiv:2402.15018v1 Announce Type: new  Abstract: Before being deployed for user-facing applications, developers align Large Language Models (LLMs) to user preferences through a variety of procedures, such as Reinforcement Learning From Human Feedback (RLHF) and Direct Preference Optimization (DPO). Current evaluations of these procedures focus on benchmarks of instruction following, reasoning, and truthfulness. However, human preferences are not universal, and aligning to specific preference sets may have unintended effects. We explore how alignment impacts performance along three axes of global representation: English dialects, multilingualism, and opinions from and about countries worldwide. Our results show that current alignment procedures create disparities between English dialects and global opinions. We find alignment improves capabilities in several languages. We conclude by discussing design decisions that led to these unintended impacts and recommendations for more equitable ",
    "path": "papers/24/02/2402.15018.json",
    "total_tokens": 789,
    "translated_title": "LLM对全球表示的意外影响",
    "translated_abstract": "在为面向用户的应用程序部署之前，开发人员通过各种程序（如从人类反馈中学习强化学习（RLHF）和直接偏好优化（DPO））将大型语言模型（LLMs）与用户偏好进行对齐。目前对这些程序的评估侧重于遵循指导、推理和真实性的基准。然而，人类偏好并非普遍，对特定偏好集进行对齐可能会产生意外影响。我们探讨了对三个全球表示维度：英语方言、多语言能力和全球各国意见的影响。我们的结果显示，当前的对齐程序在英语方言和全球意见之间产生差异。我们发现对齐提高了多种语言的能力。最后，我们讨论了导致这些意外影响的设计决策，并为更公平的建议。",
    "tldr": "对大型语言模型（LLMs）进行用户偏好对齐可能会导致英语方言和全球意见之间的差异，但也提高了多种语言的能力。",
    "en_tdlr": "Aligning Large Language Models (LLMs) to user preferences may lead to disparities between English dialects and global opinions, but also improves capabilities in several languages."
}