{
    "title": "FuseMoE: Mixture-of-Experts Transformers for Fleximodal Fusion",
    "abstract": "As machine learning models in critical fields increasingly grapple with multimodal data, they face the dual challenges of handling a wide array of modalities, often incomplete due to missing elements, and the temporal irregularity and sparsity of collected samples. Successfully leveraging this complex data, while overcoming the scarcity of high-quality training samples, is key to improving these models' predictive performance. We introduce ``FuseMoE'', a mixture-of-experts framework incorporated with an innovative gating function. Designed to integrate a diverse number of modalities, FuseMoE is effective in managing scenarios with missing modalities and irregularly sampled data trajectories. Theoretically, our unique gating function contributes to enhanced convergence rates, leading to better performance in multiple downstream tasks. The practical utility of FuseMoE in real world is validated by a challenging set of clinical risk prediction tasks.",
    "link": "https://arxiv.org/abs/2402.03226",
    "context": "Title: FuseMoE: Mixture-of-Experts Transformers for Fleximodal Fusion\nAbstract: As machine learning models in critical fields increasingly grapple with multimodal data, they face the dual challenges of handling a wide array of modalities, often incomplete due to missing elements, and the temporal irregularity and sparsity of collected samples. Successfully leveraging this complex data, while overcoming the scarcity of high-quality training samples, is key to improving these models' predictive performance. We introduce ``FuseMoE'', a mixture-of-experts framework incorporated with an innovative gating function. Designed to integrate a diverse number of modalities, FuseMoE is effective in managing scenarios with missing modalities and irregularly sampled data trajectories. Theoretically, our unique gating function contributes to enhanced convergence rates, leading to better performance in multiple downstream tasks. The practical utility of FuseMoE in real world is validated by a challenging set of clinical risk prediction tasks.",
    "path": "papers/24/02/2402.03226.json",
    "total_tokens": 880,
    "translated_title": "FuseMoE：用于灵活多模态融合的专家混合Transformer",
    "translated_abstract": "随着机器学习模型在关键领域越来越多地处理多模态数据，它们面临处理多种模态的双重挑战，这些模态经常因缺失元素而不完整，以及收集样本的时间不规则性和稀疏性。成功利用这种复杂数据，同时克服高质量训练样本的稀缺性，是提高这些模型预测性能的关键。我们引入了``FuseMoE''，这是一个集成创新门控函数的专家混合框架。FuseMoE旨在整合多种模态，并且在处理缺失模态和不规则采样数据轨迹的情况下非常有效。在理论上，我们独特的门控函数有助于提高收敛速度，在多个下游任务中表现更好。FuseMoE的实际实用性通过一系列具有挑战性的临床风险预测任务得到验证。",
    "tldr": "本论文提出了一种名为FuseMoE的专家混合Transformer框架，通过创新的门控函数实现灵活融合多模态数据，能够有效地处理缺失模态和不规则采样数据，同时改善模型的预测性能，在临床风险预测任务中具有实际应用价值。",
    "en_tdlr": "This paper introduces FuseMoE, a mixture-of-experts framework with an innovative gating function, which effectively integrates diverse modalities, manages scenarios with missing modalities and irregularly sampled data, and improves predictive performance in clinical risk prediction tasks."
}