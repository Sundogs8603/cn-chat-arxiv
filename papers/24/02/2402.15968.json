{
    "title": "CoDream: Exchanging dreams instead of models for federated aggregation with heterogeneous models",
    "abstract": "arXiv:2402.15968v1 Announce Type: cross  Abstract: Federated Learning (FL) enables collaborative optimization of machine learning models across decentralized data by aggregating model parameters. Our approach extends this concept by aggregating \"knowledge\" derived from models, instead of model parameters. We present a novel framework called \\codream, where clients collaboratively optimize randomly initialized data using federated optimization in the input data space, similar to how randomly initialized model parameters are optimized in FL. Our key insight is that jointly optimizing this data can effectively capture the properties of the global data distribution. Sharing knowledge in data space offers numerous benefits: (1) model-agnostic collaborative learning, i.e., different clients can have different model architectures; (2) communication that is independent of the model size, eliminating scalability concerns with model parameters; (3) compatibility with secure aggregation, thus pre",
    "link": "https://arxiv.org/abs/2402.15968",
    "context": "Title: CoDream: Exchanging dreams instead of models for federated aggregation with heterogeneous models\nAbstract: arXiv:2402.15968v1 Announce Type: cross  Abstract: Federated Learning (FL) enables collaborative optimization of machine learning models across decentralized data by aggregating model parameters. Our approach extends this concept by aggregating \"knowledge\" derived from models, instead of model parameters. We present a novel framework called \\codream, where clients collaboratively optimize randomly initialized data using federated optimization in the input data space, similar to how randomly initialized model parameters are optimized in FL. Our key insight is that jointly optimizing this data can effectively capture the properties of the global data distribution. Sharing knowledge in data space offers numerous benefits: (1) model-agnostic collaborative learning, i.e., different clients can have different model architectures; (2) communication that is independent of the model size, eliminating scalability concerns with model parameters; (3) compatibility with secure aggregation, thus pre",
    "path": "papers/24/02/2402.15968.json",
    "total_tokens": 856,
    "translated_title": "CoDream：使用异构模型交换梦想而不是模型进行联合聚合",
    "translated_abstract": "联邦学习（FL）通过聚合模型参数实现机器学习模型在分散数据上的协作优化。我们的方法通过聚合模型产生的“知识”，而不是模型参数来扩展这一概念。我们提出了一个名为 \\codream 的新颖框架，在这个框架中，客户端通过在输入数据空间中使用联合优化来协作优化随机初始化的数据，类似于在FL中优化随机初始化的模型参数。我们的关键见解是，联合优化这些数据可以有效捕获全局数据分布的特性。在数据空间共享知识具有许多好处：（1）与模型无关的协作学习，即不同客户端可以具有不同的模型架构；（2）通信不受模型大小影响，消除了模型参数的可伸缩性问题；（3）与安全聚合兼容，因此可预",
    "tldr": "CoDream提出了一种通过在输入数据空间中协作优化数据来交换知识的框架，实现了模型之间的合作学习，实现了模型架构无关、通信不受模型大小影响、兼容安全聚合的优点。",
    "en_tdlr": "CoDream proposes a framework where knowledge is exchanged by collaboratively optimizing data in the input data space, enabling collaborative learning between models with benefits of model-agnosticism, communication independence from model size, and compatibility with secure aggregation."
}