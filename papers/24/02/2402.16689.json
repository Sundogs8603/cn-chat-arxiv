{
    "title": "Adaptation of Biomedical and Clinical Pretrained Models to French Long Documents: A Comparative Study",
    "abstract": "arXiv:2402.16689v1 Announce Type: new  Abstract: Recently, pretrained language models based on BERT have been introduced for the French biomedical domain. Although these models have achieved state-of-the-art results on biomedical and clinical NLP tasks, they are constrained by a limited input sequence length of 512 tokens, which poses challenges when applied to clinical notes. In this paper, we present a comparative study of three adaptation strategies for long-sequence models, leveraging the Longformer architecture. We conducted evaluations of these models on 16 downstream tasks spanning both biomedical and clinical domains. Our findings reveal that further pre-training an English clinical model with French biomedical texts can outperform both converting a French biomedical BERT to the Longformer architecture and pre-training a French biomedical Longformer from scratch. The results underscore that long-sequence French biomedical models improve performance across most downstream tasks ",
    "link": "https://arxiv.org/abs/2402.16689",
    "context": "Title: Adaptation of Biomedical and Clinical Pretrained Models to French Long Documents: A Comparative Study\nAbstract: arXiv:2402.16689v1 Announce Type: new  Abstract: Recently, pretrained language models based on BERT have been introduced for the French biomedical domain. Although these models have achieved state-of-the-art results on biomedical and clinical NLP tasks, they are constrained by a limited input sequence length of 512 tokens, which poses challenges when applied to clinical notes. In this paper, we present a comparative study of three adaptation strategies for long-sequence models, leveraging the Longformer architecture. We conducted evaluations of these models on 16 downstream tasks spanning both biomedical and clinical domains. Our findings reveal that further pre-training an English clinical model with French biomedical texts can outperform both converting a French biomedical BERT to the Longformer architecture and pre-training a French biomedical Longformer from scratch. The results underscore that long-sequence French biomedical models improve performance across most downstream tasks ",
    "path": "papers/24/02/2402.16689.json",
    "total_tokens": 877,
    "translated_title": "将生物医学和临床预训练模型调整到法语长文档：一项比较研究",
    "translated_abstract": "最近，基于BERT的预训练语言模型已被引入法语生物医学领域。尽管这些模型在生物医学和临床NLP任务上取得了最先进的结果，但它们受到512个令牌的有限输入序列长度的限制，在应用于临床记录时会面临挑战。在本文中，我们提出了三种适用于长序列模型的调整策略的比较研究，利用了Longformer架构。我们对这些模型在涵盖生物医学和临床领域的16个下游任务上进行了评估。我们的研究结果表明，进一步通过法语生物医学文本对英文临床模型进行预训练，可以优于将法语生物医学BERT转换为Longformer架构以及从头开始预训练法语生物医学Longformer。结果强调了长序列法语生物医学模型在大多数下游任务上提高了性能",
    "tldr": "进一步通过法语生物医学文本对英文临床模型进行预训练可以优于其他两种调整策略，结果强调了长序列法语生物医学模型在大多数下游任务上提高了性能",
    "en_tdlr": "Further pre-training an English clinical model with French biomedical texts can outperform other adaptation strategies, highlighting the improved performance of long-sequence French biomedical models on most downstream tasks."
}