{
    "title": "NextLevelBERT: Investigating Masked Language Modeling with Higher-Level Representations for Long Documents",
    "abstract": "arXiv:2402.17682v1 Announce Type: new  Abstract: While (large) language models have significantly improved over the last years, they still struggle to sensibly process long sequences found, e.g., in books, due to the quadratic scaling of the underlying attention mechanism. To address this, we propose NextLevelBERT, a Masked Language Model operating not on tokens, but on higher-level semantic representations in the form of text embeddings. We pretrain NextLevelBERT to predict the vector representation of entire masked text chunks and evaluate the effectiveness of the resulting document vectors on three task types: 1) Semantic Textual Similarity via zero-shot document embeddings, 2) Long document classification, 3) Multiple-choice question answering. We find that next level Masked Language Modeling is an effective technique to tackle long-document use cases and can outperform much larger embedding models as long as the required level of detail is not too high. We make model and code avai",
    "link": "https://arxiv.org/abs/2402.17682",
    "context": "Title: NextLevelBERT: Investigating Masked Language Modeling with Higher-Level Representations for Long Documents\nAbstract: arXiv:2402.17682v1 Announce Type: new  Abstract: While (large) language models have significantly improved over the last years, they still struggle to sensibly process long sequences found, e.g., in books, due to the quadratic scaling of the underlying attention mechanism. To address this, we propose NextLevelBERT, a Masked Language Model operating not on tokens, but on higher-level semantic representations in the form of text embeddings. We pretrain NextLevelBERT to predict the vector representation of entire masked text chunks and evaluate the effectiveness of the resulting document vectors on three task types: 1) Semantic Textual Similarity via zero-shot document embeddings, 2) Long document classification, 3) Multiple-choice question answering. We find that next level Masked Language Modeling is an effective technique to tackle long-document use cases and can outperform much larger embedding models as long as the required level of detail is not too high. We make model and code avai",
    "path": "papers/24/02/2402.17682.json",
    "total_tokens": 873,
    "translated_title": "探究使用更高级别表示的遮蔽语言建模在长文档中的应用 - NextLevelBERT",
    "translated_abstract": "虽然（大型）语言模型在过去几年取得了显著进展，但由于基础注意机制的二次扩展，它们仍然难以合理处理发现在书籍中的长序列。为了解决这个问题，我们提出了NextLevelBERT，这是一个遮蔽语言模型，它不是在标记上操作，而是在文本嵌入的形式中的更高级别语义表示上操作。我们对NextLevelBERT进行预训练，用于预测整个被遮蔽文本块的向量表示，并评估产生的文档向量在三种任务类型上的有效性：1）通过零样本文件嵌入进行语义文本相似性，2）长文档分类，3）多项选择问题回答。我们发现，下一级遮蔽语言建模是一种有效的技术，可以处理长文档用例，并且只要所需的细节水平不太高，就可以超越更大的嵌入模型。我们提供模型和代码 avai",
    "tldr": "提出了NextLevelBERT，通过在更高级别的语义表示上进行遮蔽语言建模，有效处理长文档用例，具有超越更大嵌入模型的性能"
}