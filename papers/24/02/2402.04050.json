{
    "title": "Connecting the Dots: Collaborative Fine-tuning for Black-Box Vision-Language Models",
    "abstract": "With the emergence of pretrained vision-language models (VLMs), considerable efforts have been devoted to fine-tuning them for downstream tasks. Despite the progress made in designing efficient fine-tuning methods, such methods require access to the model's parameters, which can be challenging as model owners often opt to provide their models as a black box to safeguard model ownership. This paper proposes a \\textbf{C}ollabo\\textbf{ra}tive \\textbf{F}ine-\\textbf{T}uning (\\textbf{CraFT}) approach for fine-tuning black-box VLMs to downstream tasks, where one only has access to the input prompts and the output predictions of the model. CraFT comprises two modules, a prompt generation module for learning text prompts and a prediction refinement module for enhancing output predictions in residual style. Additionally, we introduce an auxiliary prediction-consistent loss to promote consistent optimization across these modules. These modules are optimized by a novel collaborative training algor",
    "link": "https://arxiv.org/abs/2402.04050",
    "context": "Title: Connecting the Dots: Collaborative Fine-tuning for Black-Box Vision-Language Models\nAbstract: With the emergence of pretrained vision-language models (VLMs), considerable efforts have been devoted to fine-tuning them for downstream tasks. Despite the progress made in designing efficient fine-tuning methods, such methods require access to the model's parameters, which can be challenging as model owners often opt to provide their models as a black box to safeguard model ownership. This paper proposes a \\textbf{C}ollabo\\textbf{ra}tive \\textbf{F}ine-\\textbf{T}uning (\\textbf{CraFT}) approach for fine-tuning black-box VLMs to downstream tasks, where one only has access to the input prompts and the output predictions of the model. CraFT comprises two modules, a prompt generation module for learning text prompts and a prediction refinement module for enhancing output predictions in residual style. Additionally, we introduce an auxiliary prediction-consistent loss to promote consistent optimization across these modules. These modules are optimized by a novel collaborative training algor",
    "path": "papers/24/02/2402.04050.json",
    "total_tokens": 895,
    "translated_title": "连接点：协作微调黑盒视觉语言模型",
    "translated_abstract": "随着预训练的视觉语言模型（VLMs）的出现，人们在将其用于下游任务时投入了相当大的努力。尽管在设计高效的微调方法方面取得了进展，但这些方法需要访问模型的参数，而对于保护模型所有权，模型所有者通常选择将其作为黑盒提供。本文提出了一种协作微调（CraFT）方法，用于将黑盒VLMs fine-tuning到下游任务中，其中只能访问模型的输入提示和输出预测。CraFT包括两个模块，一个用于学习文本提示的提示生成模块，一个用于以残差方式增强输出预测的预测优化模块。此外，我们引入了一个辅助的预测一致性损失来促进这些模块之间的一致优化。这些模块通过一种新的协作训练算法进行优化。",
    "tldr": "本论文提出了一种协作微调黑盒视觉语言模型的方法，可以在只有输入提示和输出预测的情况下进行微调，提供了一个提示生成模块和一个预测优化模块，并引入了辅助的预测一致性损失进行一致优化。",
    "en_tdlr": "This paper proposes a method for collaborative fine-tuning of black-box vision-language models, allowing fine-tuning with only input prompts and output predictions. It provides a prompt generation module and a prediction refinement module, along with an auxiliary prediction-consistent loss for consistent optimization."
}