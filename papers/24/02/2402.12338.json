{
    "title": "An Adversarial Approach to Evaluating the Robustness of Event Identification Models",
    "abstract": "arXiv:2402.12338v1 Announce Type: cross  Abstract: Intelligent machine learning approaches are finding active use for event detection and identification that allow real-time situational awareness. Yet, such machine learning algorithms have been shown to be susceptible to adversarial attacks on the incoming telemetry data. This paper considers a physics-based modal decomposition method to extract features for event classification and focuses on interpretable classifiers including logistic regression and gradient boosting to distinguish two types of events: load loss and generation loss. The resulting classifiers are then tested against an adversarial algorithm to evaluate their robustness. The adversarial attack is tested in two settings: the white box setting, wherein the attacker knows exactly the classification model; and the gray box setting, wherein the attacker has access to historical data from the same network as was used to train the classifier, but does not know the classifica",
    "link": "https://arxiv.org/abs/2402.12338",
    "context": "Title: An Adversarial Approach to Evaluating the Robustness of Event Identification Models\nAbstract: arXiv:2402.12338v1 Announce Type: cross  Abstract: Intelligent machine learning approaches are finding active use for event detection and identification that allow real-time situational awareness. Yet, such machine learning algorithms have been shown to be susceptible to adversarial attacks on the incoming telemetry data. This paper considers a physics-based modal decomposition method to extract features for event classification and focuses on interpretable classifiers including logistic regression and gradient boosting to distinguish two types of events: load loss and generation loss. The resulting classifiers are then tested against an adversarial algorithm to evaluate their robustness. The adversarial attack is tested in two settings: the white box setting, wherein the attacker knows exactly the classification model; and the gray box setting, wherein the attacker has access to historical data from the same network as was used to train the classifier, but does not know the classifica",
    "path": "papers/24/02/2402.12338.json",
    "total_tokens": 789,
    "translated_title": "一种对事件识别模型鲁棒性进行评估的对抗方法",
    "translated_abstract": "智能机器学习方法正在积极用于事件检测和识别，可实时获得态势感知。然而，这些机器学习算法已被证明容易受到对传入遥测数据的对抗性攻击。本文考虑了一种基于物理的模态分解方法来提取事件分类的特征，并专注于可解释的分类器，包括逻辑回归和梯度提升，以区分两种类型的事件：负载损失和发电损失。然后，对生成的分类器进行对抗算法测试以评估其鲁棒性。对抗攻击在两种情境下进行测试：白盒设置，攻击者完全了解分类模型；灰盒设置，攻击者可以访问与用于训练分类器的相同网络的历史数据，但不知道分类",
    "tldr": "该论文提出了一种基于物理的模态分解方法用于提取事件分类特征，并评估了可解释分类器在面对对抗算法时的鲁棒性。",
    "en_tdlr": "This paper introduces a physics-based modal decomposition method for extracting features for event classification and evaluates the robustness of interpretable classifiers against adversarial attacks."
}