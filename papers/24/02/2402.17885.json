{
    "title": "Independent Learning in Constrained Markov Potential Games",
    "abstract": "arXiv:2402.17885v1 Announce Type: new  Abstract: Constrained Markov games offer a formal mathematical framework for modeling multi-agent reinforcement learning problems where the behavior of the agents is subject to constraints. In this work, we focus on the recently introduced class of constrained Markov Potential Games. While centralized algorithms have been proposed for solving such constrained games, the design of converging independent learning algorithms tailored for the constrained setting remains an open question. We propose an independent policy gradient algorithm for learning approximate constrained Nash equilibria: Each agent observes their own actions and rewards, along with a shared state. Inspired by the optimization literature, our algorithm performs proximal-point-like updates augmented with a regularized constraint set. Each proximal step is solved inexactly using a stochastic switching gradient algorithm. Notably, our algorithm can be implemented independently without",
    "link": "https://arxiv.org/abs/2402.17885",
    "context": "Title: Independent Learning in Constrained Markov Potential Games\nAbstract: arXiv:2402.17885v1 Announce Type: new  Abstract: Constrained Markov games offer a formal mathematical framework for modeling multi-agent reinforcement learning problems where the behavior of the agents is subject to constraints. In this work, we focus on the recently introduced class of constrained Markov Potential Games. While centralized algorithms have been proposed for solving such constrained games, the design of converging independent learning algorithms tailored for the constrained setting remains an open question. We propose an independent policy gradient algorithm for learning approximate constrained Nash equilibria: Each agent observes their own actions and rewards, along with a shared state. Inspired by the optimization literature, our algorithm performs proximal-point-like updates augmented with a regularized constraint set. Each proximal step is solved inexactly using a stochastic switching gradient algorithm. Notably, our algorithm can be implemented independently without",
    "path": "papers/24/02/2402.17885.json",
    "total_tokens": 894,
    "translated_title": "受约束的马尔可夫潜在游戏中的独立学习",
    "translated_abstract": "受限制的马尔可夫游戏提供了一个形式化的数学框架，用于建模多智能体强化学习问题，其中智能体的行为受到约束。本文关注最近引入的一类受约束的马尔可夫潜在游戏。虽然已经提出了用于解决这类受约束游戏的集中式算法，但针对受约束设置的收敛独立学习算法的设计仍然是一个悬而未决的问题。我们提出了一种独立政策梯度算法，用于学习近似的受约束纳什均衡：每个智能体观察到自己的行动和奖励，以及一个共享状态。受到优化文献的启发，我们的算法执行近端点样式的更新，附带有正则化的约束集。每个近端步骤使用随机切换梯度算法进行近似求解。值得注意的是，我们的算法可以独立实现而无需",
    "tldr": "提出了一种独立政策梯度算法，用于学习近似的受约束纳什均衡，通过近端点样式的更新和随机切换梯度算法，解决了受约束的马尔可夫潜在游戏中的独立学习问题",
    "en_tdlr": "Proposed an independent policy gradient algorithm for learning approximate constrained Nash equilibria, addressing the issue of independent learning in constrained Markov Potential Games through proximal-point-like updates and stochastic switching gradient algorithm."
}