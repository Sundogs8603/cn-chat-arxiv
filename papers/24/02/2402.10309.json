{
    "title": "Discrete Probabilistic Inference as Control in Multi-path Environments",
    "abstract": "arXiv:2402.10309v1 Announce Type: new  Abstract: We consider the problem of sampling from a discrete and structured distribution as a sequential decision problem, where the objective is to find a stochastic policy such that objects are sampled at the end of this sequential process proportionally to some predefined reward. While we could use maximum entropy Reinforcement Learning (MaxEnt RL) to solve this problem for some distributions, it has been shown that in general, the distribution over states induced by the optimal policy may be biased in cases where there are multiple ways to generate the same object. To address this issue, Generative Flow Networks (GFlowNets) learn a stochastic policy that samples objects proportionally to their reward by approximately enforcing a conservation of flows across the whole Markov Decision Process (MDP). In this paper, we extend recent methods correcting the reward in order to guarantee that the marginal distribution induced by the optimal MaxEnt RL",
    "link": "https://arxiv.org/abs/2402.10309",
    "context": "Title: Discrete Probabilistic Inference as Control in Multi-path Environments\nAbstract: arXiv:2402.10309v1 Announce Type: new  Abstract: We consider the problem of sampling from a discrete and structured distribution as a sequential decision problem, where the objective is to find a stochastic policy such that objects are sampled at the end of this sequential process proportionally to some predefined reward. While we could use maximum entropy Reinforcement Learning (MaxEnt RL) to solve this problem for some distributions, it has been shown that in general, the distribution over states induced by the optimal policy may be biased in cases where there are multiple ways to generate the same object. To address this issue, Generative Flow Networks (GFlowNets) learn a stochastic policy that samples objects proportionally to their reward by approximately enforcing a conservation of flows across the whole Markov Decision Process (MDP). In this paper, we extend recent methods correcting the reward in order to guarantee that the marginal distribution induced by the optimal MaxEnt RL",
    "path": "papers/24/02/2402.10309.json",
    "total_tokens": 726,
    "translated_title": "离散概率推断作为多路径环境中的控制",
    "translated_abstract": "我们将从离散且结构化分布中采样的问题视为一个顺序决策问题，其目标是找到一种随机策略，使物体在这个顺序过程结束时以某些预定义奖励的比例被采样。本文中，我们扩展了最近纠正奖励的方法，以确保由最佳最大熵强化学习引起的边际分布",
    "tldr": "通过Generative Flow Networks (GFlowNets)学习一个随机策略，以近似实现在整个马尔可夫决策过程（MDP）中流量的守恒，从而解决了多路径生成相同对象的偏倚分布问题。",
    "en_tdlr": "Generative Flow Networks (GFlowNets) learn a stochastic policy that samples objects proportionally to their reward by approximately enforcing a conservation of flows across the whole Markov Decision Process (MDP), addressing the issue of biased distribution when there are multiple ways to generate the same object."
}