{
    "title": "A Primal-Dual Algorithm for Offline Constrained Reinforcement Learning with Low-Rank MDPs",
    "abstract": "Offline reinforcement learning (RL) aims to learn a policy that maximizes the expected cumulative reward using a pre-collected dataset. Offline RL with low-rank MDPs or general function approximation has been widely studied recently, but existing algorithms with sample complexity $O(\\epsilon^{-2})$ for finding an $\\epsilon$-optimal policy either require a uniform data coverage assumptions or are computationally inefficient. In this paper, we propose a primal dual algorithm for offline RL with low-rank MDPs in the discounted infinite-horizon setting. Our algorithm is the first computationally efficient algorithm in this setting that achieves sample complexity of $O(\\epsilon^{-2})$ with partial data coverage assumption. This improves upon a recent work that requires $O(\\epsilon^{-4})$ samples. Moreover, our algorithm extends the previous work to the offline constrained RL setting by supporting constraints on additional reward signals.",
    "link": "https://arxiv.org/abs/2402.04493",
    "context": "Title: A Primal-Dual Algorithm for Offline Constrained Reinforcement Learning with Low-Rank MDPs\nAbstract: Offline reinforcement learning (RL) aims to learn a policy that maximizes the expected cumulative reward using a pre-collected dataset. Offline RL with low-rank MDPs or general function approximation has been widely studied recently, but existing algorithms with sample complexity $O(\\epsilon^{-2})$ for finding an $\\epsilon$-optimal policy either require a uniform data coverage assumptions or are computationally inefficient. In this paper, we propose a primal dual algorithm for offline RL with low-rank MDPs in the discounted infinite-horizon setting. Our algorithm is the first computationally efficient algorithm in this setting that achieves sample complexity of $O(\\epsilon^{-2})$ with partial data coverage assumption. This improves upon a recent work that requires $O(\\epsilon^{-4})$ samples. Moreover, our algorithm extends the previous work to the offline constrained RL setting by supporting constraints on additional reward signals.",
    "path": "papers/24/02/2402.04493.json",
    "total_tokens": 946,
    "translated_title": "离线约束强化学习中一种基本对偶算法在低秩MDPs上的应用",
    "translated_abstract": "离线强化学习旨在通过预先收集的数据集学习一种最大化期望累积奖励的策略。最近，对于低秩MDPs或一般函数逼近的离线强化学习进行了广泛研究，但是现有算法在找到$\\epsilon$-优化策略的样本复杂度为$O(\\epsilon^{-2})$时，要么需要均匀的数据覆盖假设，要么计算效率低下。在本文中，我们提出了一种在折扣无穷时段设置下，用于低秩MDPs的离线强化学习的基本对偶算法。我们的算法是在部分数据覆盖假设下，该设置中第一个样本复杂度达到$O(\\epsilon^{-2})$的计算有效的算法。这优于最近的一项工作，其需要$O(\\epsilon^{-4})$个样本。此外，我们的算法通过支持额外奖励信号的约束，将之前的工作扩展到离线约束强化学习设置中。",
    "tldr": "这篇论文提出了一种在低秩MDPs上的离线约束强化学习算法，该算法通过部分数据覆盖假设实现了更高的计算效率并达到了$O(\\epsilon^{-2})$的样本复杂度。此外，该算法还支持额外奖励信号的约束。",
    "en_tdlr": "This paper proposes a primal-dual algorithm for offline constrained reinforcement learning on low-rank MDPs. The algorithm achieves higher computational efficiency and a sample complexity of $O(\\epsilon^{-2})$ with partial data coverage assumption. Moreover, it extends the previous work to support constraints on additional reward signals."
}