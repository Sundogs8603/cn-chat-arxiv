{
    "title": "Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models",
    "abstract": "arXiv:2402.08955v1 Announce Type: new Abstract: Large language models (LLMs) have performed well on several reasoning benchmarks, including ones that test analogical reasoning abilities. However, it has been debated whether they are actually performing humanlike abstract reasoning or instead employing less general processes that rely on similarity to what has been seen in their training data. Here we investigate the generality of analogy-making abilities previously claimed for LLMs (Webb, Holyoak, & Lu, 2023). We take one set of analogy problems used to evaluate LLMs and create a set of \"counterfactual\" variants-versions that test the same abstract reasoning abilities but that are likely dissimilar from any pre-training data. We test humans and three GPT models on both the original and counterfactual problems, and show that, while the performance of humans remains high for all the problems, the GPT models' performance declines sharply on the counterfactual set. This work provides evide",
    "link": "https://arxiv.org/abs/2402.08955",
    "context": "Title: Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models\nAbstract: arXiv:2402.08955v1 Announce Type: new Abstract: Large language models (LLMs) have performed well on several reasoning benchmarks, including ones that test analogical reasoning abilities. However, it has been debated whether they are actually performing humanlike abstract reasoning or instead employing less general processes that rely on similarity to what has been seen in their training data. Here we investigate the generality of analogy-making abilities previously claimed for LLMs (Webb, Holyoak, & Lu, 2023). We take one set of analogy problems used to evaluate LLMs and create a set of \"counterfactual\" variants-versions that test the same abstract reasoning abilities but that are likely dissimilar from any pre-training data. We test humans and three GPT models on both the original and counterfactual problems, and show that, while the performance of humans remains high for all the problems, the GPT models' performance declines sharply on the counterfactual set. This work provides evide",
    "path": "papers/24/02/2402.08955.json",
    "total_tokens": 894,
    "translated_title": "使用反事实任务评估大型语言模型中类比推理的广泛性",
    "translated_abstract": "大型语言模型（LLMs）在多个推理基准测试中表现出色，包括测试类比推理能力的基准测试。然而，关于它们是否真正进行人类抽象推理，还是依赖于与训练数据中所见相似的较少通用过程的争论一直存在。本研究调查了先前声称LLMs具有类比能力的广泛性（Webb, Holyoak, & Lu, 2023）。我们利用用于评估LLMs的一组类比问题，创建了一组“反事实”变体，即测试相同的抽象推理能力，但很可能与任何预训练资料不同。我们对人类和三个GPT模型在原始问题和反事实问题上进行了测试，并发现，虽然人类的表现对所有问题保持高水平，但GPT模型在反事实集上的表现急剧下降。这项工作提供了证据。",
    "tldr": "本研究通过创建一组反事实问题，评估了大型语言模型中类比推理能力的广泛性。结果表明，尽管人类在所有问题上的表现良好，但GPT模型在反事实集上的表现显著下降。",
    "en_tdlr": "This study investigates the generality of analogy-making abilities in large language models (LLMs) by creating a set of counterfactual problems. The results show a sharp decline in performance for GPT models on the counterfactual set, while humans perform well on all problems."
}