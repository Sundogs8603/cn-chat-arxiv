{
    "title": "Pre-training Differentially Private Models with Limited Public Data",
    "abstract": "arXiv:2402.18752v1 Announce Type: new  Abstract: The superior performance of large foundation models relies on the use of massive amounts of high-quality data, which often contain sensitive, private and copyrighted material that requires formal protection. While differential privacy (DP) is a prominent method to gauge the degree of security provided to the models, its application is commonly limited to the model fine-tuning stage, due to the performance degradation when applying DP during the pre-training stage. Consequently, DP is yet not capable of protecting a substantial portion of the data used during the initial pre-training process.   In this work, we first provide a theoretical understanding of the efficacy of DP training by analyzing the per-iteration loss improvement. We make a key observation that DP optimizers' performance degradation can be significantly mitigated by the use of limited public data, which leads to a novel DP continual pre-training strategy. Empirically, usi",
    "link": "https://arxiv.org/abs/2402.18752",
    "context": "Title: Pre-training Differentially Private Models with Limited Public Data\nAbstract: arXiv:2402.18752v1 Announce Type: new  Abstract: The superior performance of large foundation models relies on the use of massive amounts of high-quality data, which often contain sensitive, private and copyrighted material that requires formal protection. While differential privacy (DP) is a prominent method to gauge the degree of security provided to the models, its application is commonly limited to the model fine-tuning stage, due to the performance degradation when applying DP during the pre-training stage. Consequently, DP is yet not capable of protecting a substantial portion of the data used during the initial pre-training process.   In this work, we first provide a theoretical understanding of the efficacy of DP training by analyzing the per-iteration loss improvement. We make a key observation that DP optimizers' performance degradation can be significantly mitigated by the use of limited public data, which leads to a novel DP continual pre-training strategy. Empirically, usi",
    "path": "papers/24/02/2402.18752.json",
    "total_tokens": 843,
    "translated_title": "使用有限公共数据对有差异隐私的模型进行预训练",
    "translated_abstract": "大型基础模型卓越的性能依赖于大量高质量数据的使用，然而这些数据通常包含需要正式保护的敏感、私人和受版权保护的信息。差分隐私是一种用于衡量模型提供的安全程度的重要方法，然而由于在预训练阶段应用差分隐私会导致性能下降，因此其应用通常仅限于模型微调阶段。因此，差分隐私目前尚不能保护初始预训练过程中使用的大部分数据。在这项工作中，我们首先通过分析每次迭代的损失改进，对差分隐私训练的有效性提供了理论理解。我们观察到，通过使用有限的公共数据，可以显著缓解差分隐私优化器的性能下降，从而引出一种新颖的差分隐私持续预训练策略。在实证方面，通过",
    "tldr": "通过使用有限的公共数据，研究者提出了一种新颖的差分隐私持续预训练策略，以显著缓解优化器性能下降。",
    "en_tdlr": "A novel differentially private continual pre-training strategy is proposed by leveraging limited public data to significantly mitigate optimizer performance degradation."
}