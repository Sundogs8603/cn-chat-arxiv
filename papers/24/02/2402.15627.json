{
    "title": "MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs",
    "abstract": "arXiv:2402.15627v1 Announce Type: new  Abstract: We present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models (LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. We take a full-stack approach that co-designs the algorithmic and system components across model block and optimizer design, computation and communication overlapping, operator optimization, data pipeline, and network performance tuning. Maintaining high efficiency throughout the training process (i.e., stability) is an important consideration in production given the long extent of LLM training jobs. Many hard stability issues only emerge at large scale, and in-depth observability is the key to address them. We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effectiv",
    "link": "https://arxiv.org/abs/2402.15627",
    "context": "Title: MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs\nAbstract: arXiv:2402.15627v1 Announce Type: new  Abstract: We present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models (LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. We take a full-stack approach that co-designs the algorithmic and system components across model block and optimizer design, computation and communication overlapping, operator optimization, data pipeline, and network performance tuning. Maintaining high efficiency throughout the training process (i.e., stability) is an important consideration in production given the long extent of LLM training jobs. Many hard stability issues only emerge at large scale, and in-depth observability is the key to address them. We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effectiv",
    "path": "papers/24/02/2402.15627.json",
    "total_tokens": 890,
    "translated_title": "MegaScale: 将大型语言模型训练扩展到超过10,000个GPU",
    "translated_abstract": "我们介绍了设计、实施和工程经验，构建和部署了MegaScale，一个用于在超过10,000个GPU规模上训练大型语言模型（LLM）的生产系统。在这种规模上训练LLMs会给训练效率和稳定性带来前所未有的挑战。我们采取了一种全栈方法，跨模型块和优化器设计、计算和通信重叠、算子优化、数据管道和网络性能调优，协同设计算法和系统组件。在训练过程中保持高效率（即稳定性）是在生产中的一个重要考虑，考虑到LLM训练作业的长时间跨度。许多艰难的稳定性问题只有在大规模下才会出现，深入的可观察性是解决这些问题的关键。我们开发了一套诊断工具，用于监控系统组件和深层堆栈中的事件，识别根本原因，并得出有效的",
    "tldr": "MegaScale项目介绍了一个用于在超过10,000个GPU规模上训练大型语言模型的生产系统，通过全栈方法协同设计算法和系统组件来解决训练效率和稳定性挑战。",
    "en_tdlr": "The MegaScale project presents a production system for training large language models at the scale of more than 10,000 GPUs, addressing training efficiency and stability challenges through a full-stack approach that co-designs algorithmic and system components."
}