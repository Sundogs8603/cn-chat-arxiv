{
    "title": "Preserving Fairness Generalization in Deepfake Detection",
    "abstract": "arXiv:2402.17229v1 Announce Type: cross  Abstract: Although effective deepfake detection models have been developed in recent years, recent studies have revealed that these models can result in unfair performance disparities among demographic groups, such as race and gender. This can lead to particular groups facing unfair targeting or exclusion from detection, potentially allowing misclassified deepfakes to manipulate public opinion and undermine trust in the model. The existing method for addressing this problem is providing a fair loss function. It shows good fairness performance for intra-domain evaluation but does not maintain fairness for cross-domain testing. This highlights the significance of fairness generalization in the fight against deepfakes. In this work, we propose the first method to address the fairness generalization problem in deepfake detection by simultaneously considering features, loss, and optimization aspects. Our method employs disentanglement learning to ext",
    "link": "https://arxiv.org/abs/2402.17229",
    "context": "Title: Preserving Fairness Generalization in Deepfake Detection\nAbstract: arXiv:2402.17229v1 Announce Type: cross  Abstract: Although effective deepfake detection models have been developed in recent years, recent studies have revealed that these models can result in unfair performance disparities among demographic groups, such as race and gender. This can lead to particular groups facing unfair targeting or exclusion from detection, potentially allowing misclassified deepfakes to manipulate public opinion and undermine trust in the model. The existing method for addressing this problem is providing a fair loss function. It shows good fairness performance for intra-domain evaluation but does not maintain fairness for cross-domain testing. This highlights the significance of fairness generalization in the fight against deepfakes. In this work, we propose the first method to address the fairness generalization problem in deepfake detection by simultaneously considering features, loss, and optimization aspects. Our method employs disentanglement learning to ext",
    "path": "papers/24/02/2402.17229.json",
    "total_tokens": 848,
    "translated_title": "在深度伪造检测中保持公平泛化",
    "translated_abstract": "虽然近年来已经开发出了有效的深度伪造检测模型，然而最近的研究表明，这些模型可能导致在人口统计学群体中（如种族和性别）出现不公平的性能差距。这可能导致特定群体面临不公平的定位或被排除在检测之外，潜在地允许被错误分类的深度伪造篡改公众舆论并破坏对模型的信任。现有的解决这一问题的方法是提供公平的损失函数。 它对于内域评估表现出良好的公平性能，但在跨域测试中无法保持公平性。这突显了在打击深度伪造中公平泛化的重要性。在这项工作中，我们提出了第一种方法，通过同时考虑特征、损失和优化方面，解决深度伪造检测中的公平泛化问题。我们的方法采用解耦学习来扩展",
    "tldr": "本研究提出了第一种方法来解决深度伪造检测中的公平泛化问题，通过同时考虑特征、损失和优化方面，利用解耦学习来实现。"
}