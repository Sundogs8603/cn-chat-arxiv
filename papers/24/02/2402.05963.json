{
    "title": "Frugal Actor-Critic: Sample Efficient Off-Policy Deep Reinforcement Learning Using Unique Experiences",
    "abstract": "Efficient utilization of the replay buffer plays a significant role in the off-policy actor-critic reinforcement learning (RL) algorithms used for model-free control policy synthesis for complex dynamical systems. We propose a method for achieving sample efficiency, which focuses on selecting unique samples and adding them to the replay buffer during the exploration with the goal of reducing the buffer size and maintaining the independent and identically distributed (IID) nature of the samples. Our method is based on selecting an important subset of the set of state variables from the experiences encountered during the initial phase of random exploration, partitioning the state space into a set of abstract states based on the selected important state variables, and finally selecting the experiences with unique state-reward combination by using a kernel density estimator. We formally prove that the off-policy actor-critic algorithm incorporating the proposed method for unique experience",
    "link": "https://arxiv.org/abs/2402.05963",
    "context": "Title: Frugal Actor-Critic: Sample Efficient Off-Policy Deep Reinforcement Learning Using Unique Experiences\nAbstract: Efficient utilization of the replay buffer plays a significant role in the off-policy actor-critic reinforcement learning (RL) algorithms used for model-free control policy synthesis for complex dynamical systems. We propose a method for achieving sample efficiency, which focuses on selecting unique samples and adding them to the replay buffer during the exploration with the goal of reducing the buffer size and maintaining the independent and identically distributed (IID) nature of the samples. Our method is based on selecting an important subset of the set of state variables from the experiences encountered during the initial phase of random exploration, partitioning the state space into a set of abstract states based on the selected important state variables, and finally selecting the experiences with unique state-reward combination by using a kernel density estimator. We formally prove that the off-policy actor-critic algorithm incorporating the proposed method for unique experience",
    "path": "papers/24/02/2402.05963.json",
    "total_tokens": 825,
    "translated_title": "节俭的演员-评论家模型：使用独特经历的高效离线深度强化学习",
    "translated_abstract": "在用于复杂动态系统的无模型控制策略合成中，对回放缓冲器的高效利用在离线演员-评论家强化学习算法中起着重要作用。我们提出了一种实现样本效率的方法，该方法通过在探索过程中选择独特样本并将其添加到回放缓冲器中，旨在减小缓冲器的大小并保持样本的独立同分布（IID）的性质。我们的方法基于在随机探索的初始阶段遇到的经历中选择一组重要的状态变量的重要子集，根据所选重要状态变量将状态空间划分为一组抽象状态，最后通过使用核密度估计器选择具有独特状态-奖励组合的经历。我们严格证明了将所提出的独特经历方法纳入离线演员-评论家算法中的有效性。",
    "tldr": "该方法通过选择独特样本并添加到回放缓冲器中以实现样本效率，在复杂动态系统的无模型控制策略合成中起着重要作用。",
    "en_tdlr": "This approach achieves sample efficiency by selecting unique samples and adding them to the replay buffer, playing a significant role in synthesizing model-free control policies for complex dynamical systems."
}