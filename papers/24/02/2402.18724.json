{
    "title": "Learning Associative Memories with Gradient Descent",
    "abstract": "arXiv:2402.18724v1 Announce Type: cross  Abstract: This work focuses on the training dynamics of one associative memory module storing outer products of token embeddings. We reduce this problem to the study of a system of particles, which interact according to properties of the data distribution and correlations between embeddings. Through theory and experiments, we provide several insights. In overparameterized regimes, we obtain logarithmic growth of the ``classification margins.'' Yet, we show that imbalance in token frequencies and memory interferences due to correlated embeddings lead to oscillatory transitory regimes. The oscillations are more pronounced with large step sizes, which can create benign loss spikes, although these learning rates speed up the dynamics and accelerate the asymptotic convergence. In underparameterized regimes, we illustrate how the cross-entropy loss can lead to suboptimal memorization schemes. Finally, we assess the validity of our findings on small Tr",
    "link": "https://arxiv.org/abs/2402.18724",
    "context": "Title: Learning Associative Memories with Gradient Descent\nAbstract: arXiv:2402.18724v1 Announce Type: cross  Abstract: This work focuses on the training dynamics of one associative memory module storing outer products of token embeddings. We reduce this problem to the study of a system of particles, which interact according to properties of the data distribution and correlations between embeddings. Through theory and experiments, we provide several insights. In overparameterized regimes, we obtain logarithmic growth of the ``classification margins.'' Yet, we show that imbalance in token frequencies and memory interferences due to correlated embeddings lead to oscillatory transitory regimes. The oscillations are more pronounced with large step sizes, which can create benign loss spikes, although these learning rates speed up the dynamics and accelerate the asymptotic convergence. In underparameterized regimes, we illustrate how the cross-entropy loss can lead to suboptimal memorization schemes. Finally, we assess the validity of our findings on small Tr",
    "path": "papers/24/02/2402.18724.json",
    "total_tokens": 834,
    "translated_title": "使用梯度下降学习关联记忆",
    "translated_abstract": "这项工作主要关注存储标记嵌入的外积的一个关联记忆模块的训练动态。我们将这个问题简化为研究一个粒子系统，这些粒子根据数据分布的特性以及嵌入之间的相关性进行交互。通过理论和实验，我们提供了一些见解。在过参数化的情况下，我们获得了“分类边界”的对数增长。然而，我们表明标记频率的不平衡和由相关嵌入导致的内存干扰会导致振荡的瞬态区域。振荡在步长较大时更为明显，这可能导致良性损失峰，尽管这些学习率加速了动态并加速了渐近收敛。在欠参数化的情况下，我们阐明了交叉熵损失如何导致次优的记忆方案。最后，我们评估了我们发现的在小规模Tr上的有效性。",
    "tldr": "该论文研究了一个关联记忆模块的训练动态，通过理论和实验揭示了在过参数化和欠参数化情况下的学习动态和误差特性。",
    "en_tdlr": "This work focuses on the training dynamics of an associative memory module and provides insights into learning dynamics and error characteristics in overparameterized and underparameterized settings."
}