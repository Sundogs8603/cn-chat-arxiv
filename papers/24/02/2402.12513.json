{
    "title": "Induced Model Matching: How Restricted Models Can Help Larger Ones",
    "abstract": "arXiv:2402.12513v1 Announce Type: new  Abstract: We consider scenarios where a very accurate predictive model using restricted features is available at the time of training of a larger, full-featured, model. This restricted model may be thought of as \"side-information\", derived either from an auxiliary exhaustive dataset or on the same dataset, by forcing the restriction. How can the restricted model be useful to the full model? We propose an approach for transferring the knowledge of the restricted model to the full model, by aligning the full model's context-restricted performance with that of the restricted model's. We call this methodology Induced Model Matching (IMM) and first illustrate its general applicability by using logistic regression as a toy example. We then explore IMM's use in language modeling, the application that initially inspired it, and where it offers an explicit foundation in contrast to the implicit use of restricted models in techniques such as noising. We dem",
    "link": "https://arxiv.org/abs/2402.12513",
    "context": "Title: Induced Model Matching: How Restricted Models Can Help Larger Ones\nAbstract: arXiv:2402.12513v1 Announce Type: new  Abstract: We consider scenarios where a very accurate predictive model using restricted features is available at the time of training of a larger, full-featured, model. This restricted model may be thought of as \"side-information\", derived either from an auxiliary exhaustive dataset or on the same dataset, by forcing the restriction. How can the restricted model be useful to the full model? We propose an approach for transferring the knowledge of the restricted model to the full model, by aligning the full model's context-restricted performance with that of the restricted model's. We call this methodology Induced Model Matching (IMM) and first illustrate its general applicability by using logistic regression as a toy example. We then explore IMM's use in language modeling, the application that initially inspired it, and where it offers an explicit foundation in contrast to the implicit use of restricted models in techniques such as noising. We dem",
    "path": "papers/24/02/2402.12513.json",
    "total_tokens": 847,
    "translated_title": "引导模型匹配：受限模型如何帮助更大的模型",
    "translated_abstract": "我们考虑在训练更大、具有完整特征的模型时，是否可以利用受限特征的非常准确的预测模型。这个受限模型可以被视为“辅助信息”，可以通过来自辅助源数据集的详尽数据或在相同数据集上通过施加限制来获得。我们提出了一种方法，将受限模型的知识传递给完整模型，通过使完整模型的上下文受限性能与受限模型的性能对齐。我们将这种方法称为引导模型匹配（IMM），首先通过以逻辑回归为玩具示例来说明其普适性。然后我们探讨了IMM在语言建模中的应用，这也是最初的灵感来源，IMM在这里提供了明确的基础，与在技术中隐式使用受限模型的方法相对应，例如添加噪声。",
    "tldr": "提出了引导模型匹配（IMM）方法，通过使完整模型的性能与受限模型对齐，将受限模型的知识传递给完整模型，具有广泛的应用性。",
    "en_tdlr": "Introduced Induced Model Matching (IMM) method to transfer knowledge of restricted model to full model by aligning the performance of the full model with that of the restricted model, demonstrating its broad applicability."
}