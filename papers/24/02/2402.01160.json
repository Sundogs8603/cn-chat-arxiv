{
    "title": "Truncated Non-Uniform Quantization for Distributed SGD",
    "abstract": "To address the communication bottleneck challenge in distributed learning, our work introduces a novel two-stage quantization strategy designed to enhance the communication efficiency of distributed Stochastic Gradient Descent (SGD). The proposed method initially employs truncation to mitigate the impact of long-tail noise, followed by a non-uniform quantization of the post-truncation gradients based on their statistical characteristics. We provide a comprehensive convergence analysis of the quantized distributed SGD, establishing theoretical guarantees for its performance. Furthermore, by minimizing the convergence error, we derive optimal closed-form solutions for the truncation threshold and non-uniform quantization levels under given communication constraints. Both theoretical insights and extensive experimental evaluations demonstrate that our proposed algorithm outperforms existing quantization schemes, striking a superior balance between communication efficiency and convergence ",
    "link": "https://rss.arxiv.org/abs/2402.01160",
    "context": "Title: Truncated Non-Uniform Quantization for Distributed SGD\nAbstract: To address the communication bottleneck challenge in distributed learning, our work introduces a novel two-stage quantization strategy designed to enhance the communication efficiency of distributed Stochastic Gradient Descent (SGD). The proposed method initially employs truncation to mitigate the impact of long-tail noise, followed by a non-uniform quantization of the post-truncation gradients based on their statistical characteristics. We provide a comprehensive convergence analysis of the quantized distributed SGD, establishing theoretical guarantees for its performance. Furthermore, by minimizing the convergence error, we derive optimal closed-form solutions for the truncation threshold and non-uniform quantization levels under given communication constraints. Both theoretical insights and extensive experimental evaluations demonstrate that our proposed algorithm outperforms existing quantization schemes, striking a superior balance between communication efficiency and convergence ",
    "path": "papers/24/02/2402.01160.json",
    "total_tokens": 871,
    "translated_title": "分布式SGD的截断非均匀量化",
    "translated_abstract": "为了解决分布式学习中的通信瓶颈挑战，我们的工作引入了一种新颖的两阶段量化策略，旨在提高分布式随机梯度下降（SGD）的通信效率。所提出的方法首先使用截断来减轻长尾噪声的影响，然后根据梯度的统计特性进行非均匀量化。我们对量化的分布式SGD进行了全面的收敛性分析，为其性能提供了理论保证。此外，通过最小化收敛误差，我们推导出在给定通信约束下的截断阈值和非均匀量化水平的最优闭式解。理论洞见和广泛的实验评估表明，我们提出的算法优于现有的量化方案，在通信效率和收敛性之间取得了优越的平衡。",
    "tldr": "我们提出了一种截断非均匀量化的分布式SGD方法，用于提高通信效率。我们的方法通过截断来减轻长尾噪声的影响，并根据梯度的统计特性进行非均匀量化。理论分析和实验评估表明，该方法在通信效率和收敛性方面取得了优越的平衡。"
}