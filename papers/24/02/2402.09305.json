{
    "title": "Embracing the black box: Heading towards foundation models for causal discovery from time series data",
    "abstract": "arXiv:2402.09305v1 Announce Type: cross Abstract: Causal discovery from time series data encompasses many existing solutions, including those based on deep learning techniques. However, these methods typically do not endorse one of the most prevalent paradigms in deep learning: End-to-end learning. To address this gap, we explore what we call Causal Pretraining. A methodology that aims to learn a direct mapping from multivariate time series to the underlying causal graphs in a supervised manner. Our empirical findings suggest that causal discovery in a supervised manner is possible, assuming that the training and test time series samples share most of their dynamics. More importantly, we found evidence that the performance of Causal Pretraining can increase with data and model size, even if the additional data do not share the same dynamics. Further, we provide examples where causal discovery for real-world data with causally pretrained neural networks is possible within limits. We arg",
    "link": "https://arxiv.org/abs/2402.09305",
    "context": "Title: Embracing the black box: Heading towards foundation models for causal discovery from time series data\nAbstract: arXiv:2402.09305v1 Announce Type: cross Abstract: Causal discovery from time series data encompasses many existing solutions, including those based on deep learning techniques. However, these methods typically do not endorse one of the most prevalent paradigms in deep learning: End-to-end learning. To address this gap, we explore what we call Causal Pretraining. A methodology that aims to learn a direct mapping from multivariate time series to the underlying causal graphs in a supervised manner. Our empirical findings suggest that causal discovery in a supervised manner is possible, assuming that the training and test time series samples share most of their dynamics. More importantly, we found evidence that the performance of Causal Pretraining can increase with data and model size, even if the additional data do not share the same dynamics. Further, we provide examples where causal discovery for real-world data with causally pretrained neural networks is possible within limits. We arg",
    "path": "papers/24/02/2402.09305.json",
    "total_tokens": 909,
    "translated_title": "embracing the black box: 朝向基于时间序列数据进行因果发现的基础模型",
    "translated_abstract": "来自时间序列数据的因果发现涵盖了许多现有的解决方案，包括基于深度学习技术的方法。然而，这些方法通常不支持深度学习中最常见的范式之一：端到端学习。为了弥补这一差距，我们研究了我们所称之为因果预训练的方法。这种方法旨在以监督的方式学习从多变量时间序列到潜在因果图的直接映射。我们的实证结果表明，在训练和测试时间序列样本共享大部分动力学的情况下，监督式因果发现是可能的。更重要的是，我们发现，即使额外的数据不共享相同的动力学，因果预训练的性能也随着数据和模型规模的增加而增加。此外，我们提供了实例，证明了基于因果预训练神经网络的真实世界数据的因果发现在一定范围内是可能的。",
    "tldr": "本文研究了基于时间序列数据进行因果发现的问题，提出了一种称为因果预训练的方法，通过以监督方式学习从多变量时间序列到潜在因果图的映射，实现了在共享动力学的情况下的监督式因果发现。",
    "en_tdlr": "This paper investigates causal discovery from time series data and proposes a method called Causal Pretraining, which learns a mapping from multivariate time series to underlying causal graphs in a supervised manner, achieving supervised causal discovery under shared dynamics."
}