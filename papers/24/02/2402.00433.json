{
    "title": "Merging Multi-Task Models via Weight-Ensembling Mixture of Experts",
    "abstract": "Merging various task-specific Transformer-based models trained on different tasks into a single unified model can execute all the tasks concurrently. Previous methods, exemplified by task arithmetic, have been proven to be both effective and scalable. Existing methods have primarily focused on seeking a static optimal solution within the original model parameter space. A notable challenge is mitigating the interference between parameters of different models, which can substantially deteriorate performance. In this paper, we propose to merge most of the parameters while upscaling the MLP of the Transformer layers to a weight-ensembling mixture of experts (MoE) module, which can dynamically integrate shared and task-specific knowledge based on the input, thereby providing a more flexible solution that can adapt to the specific needs of each instance. Our key insight is that by identifying and separating shared knowledge and task-specific knowledge, and then dynamically integrating them, ",
    "link": "https://arxiv.org/abs/2402.00433",
    "context": "Title: Merging Multi-Task Models via Weight-Ensembling Mixture of Experts\nAbstract: Merging various task-specific Transformer-based models trained on different tasks into a single unified model can execute all the tasks concurrently. Previous methods, exemplified by task arithmetic, have been proven to be both effective and scalable. Existing methods have primarily focused on seeking a static optimal solution within the original model parameter space. A notable challenge is mitigating the interference between parameters of different models, which can substantially deteriorate performance. In this paper, we propose to merge most of the parameters while upscaling the MLP of the Transformer layers to a weight-ensembling mixture of experts (MoE) module, which can dynamically integrate shared and task-specific knowledge based on the input, thereby providing a more flexible solution that can adapt to the specific needs of each instance. Our key insight is that by identifying and separating shared knowledge and task-specific knowledge, and then dynamically integrating them, ",
    "path": "papers/24/02/2402.00433.json",
    "total_tokens": 815,
    "translated_title": "通过权重集成专家的多任务模型合并",
    "translated_abstract": "将训练在不同任务上的各种特定任务的Transformer模型合并为一个统一的模型，可以同时执行所有任务。以任务算术为例的先前方法已被证明既有效又可扩展。现有方法主要集中在寻找原始模型参数空间内的静态最优解。一个显著的挑战是减轻不同模型参数之间的干扰，这可能会严重削弱性能。在本文中，我们提出了一种方法，将大多数参数合并在一起，同时将Transformer层的MLP扩展为权重集成专家（MoE）模块，该模块可以根据输入动态地整合共享和特定任务的知识，从而提供一个更灵活的解决方案，可以适应每个实例的特定需求。我们的关键见解是通过识别和分离共享知识和特定任务知识，然后动态地整合它们，",
    "tldr": "通过权重集成专家的方法可以将训练在不同任务上的Transformer模型合并为一个统一的模型，通过动态整合共享和特定任务的知识来提供更灵活的解决方案。"
}