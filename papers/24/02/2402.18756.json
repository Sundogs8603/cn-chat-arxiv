{
    "title": "How Much Annotation is Needed to Compare Summarization Models?",
    "abstract": "arXiv:2402.18756v1 Announce Type: new  Abstract: Modern instruction-tuned models have become highly capable in text generation tasks such as summarization, and are expected to be released at a steady pace. In practice one may now wish to choose confidently, but with minimal effort, the best performing summarization model when applied to a new domain or purpose. In this work, we empirically investigate the test sample size necessary to select a preferred model in the context of news summarization. Empirical results reveal that comparative evaluation converges quickly for both automatic and human evaluation, with clear preferences for a system emerging from under 100 examples. The human preference data allows us to quantify how well automatic scores can reproduce preference rankings across a variety of downstream summarization tasks. We find that, while automatic metrics are stable at smaller sample sizes, only some automatic metrics are able to moderately predict model win rates accordi",
    "link": "https://arxiv.org/abs/2402.18756",
    "context": "Title: How Much Annotation is Needed to Compare Summarization Models?\nAbstract: arXiv:2402.18756v1 Announce Type: new  Abstract: Modern instruction-tuned models have become highly capable in text generation tasks such as summarization, and are expected to be released at a steady pace. In practice one may now wish to choose confidently, but with minimal effort, the best performing summarization model when applied to a new domain or purpose. In this work, we empirically investigate the test sample size necessary to select a preferred model in the context of news summarization. Empirical results reveal that comparative evaluation converges quickly for both automatic and human evaluation, with clear preferences for a system emerging from under 100 examples. The human preference data allows us to quantify how well automatic scores can reproduce preference rankings across a variety of downstream summarization tasks. We find that, while automatic metrics are stable at smaller sample sizes, only some automatic metrics are able to moderately predict model win rates accordi",
    "path": "papers/24/02/2402.18756.json",
    "total_tokens": 848,
    "translated_title": "需要多少注释才能比较摘要模型？",
    "translated_abstract": "近代的指导调整模型在文本生成任务（如摘要）中变得非常有能力，并且预计会以稳定的速度发布。实际上，人们现在可能希望在应用于新领域或目的时，自信地选择但又付出最少努力的最佳摘要模型。在这项工作中，我们通过实证研究调查了在新闻摘要环境中选择首选模型所需的测试样本大小。实证结果表明，无论是自动评估还是人工评估，比较评估都会迅速收敛，从不到100个示例中会出现对系统的明确偏好。人类偏好数据使我们能够量化自动分数能够如何复制在各种下游摘要任务中的偏好排名。我们发现，虽然自动指标在较小样本大小时稳定，但只有一些自动指标能够适度预测模型的获胜率。",
    "tldr": "本研究通过实证研究发现，仅需要不到100个例子就能够得出对摘要系统的明确偏好，并且只有一些自动评估指标能够适度预测模型的胜率。",
    "en_tdlr": "This study empirically demonstrates that clear preferences for summarization systems can be obtained with under 100 examples, and only some automatic evaluation metrics are able to moderately predict model win rates."
}