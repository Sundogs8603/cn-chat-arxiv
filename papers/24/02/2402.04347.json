{
    "title": "The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry",
    "abstract": "Linear attentions have shown potential for improving Transformer efficiency, reducing attention's quadratic complexity to linear in sequence length. This holds exciting promise for (1) training linear Transformers from scratch, (2) \"finetuned-conversion\" of task-specific Transformers into linear versions that recover task performance, and (3) \"pretrained-conversion\" of Transformers such as large language models into linear versions finetunable on downstream tasks. However, linear attentions often underperform standard softmax attention in quality. To close this performance gap, we find prior linear attentions lack key properties of softmax attention tied to good performance: low-entropy (or \"spiky\") weights and dot-product monotonicity. We further observe surprisingly simple feature maps that retain these properties and match softmax performance, but are inefficient to compute in linear attention. We thus propose Hedgehog, a learnable linear attention that retains the spiky and monoton",
    "link": "https://arxiv.org/abs/2402.04347",
    "context": "Title: The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry\nAbstract: Linear attentions have shown potential for improving Transformer efficiency, reducing attention's quadratic complexity to linear in sequence length. This holds exciting promise for (1) training linear Transformers from scratch, (2) \"finetuned-conversion\" of task-specific Transformers into linear versions that recover task performance, and (3) \"pretrained-conversion\" of Transformers such as large language models into linear versions finetunable on downstream tasks. However, linear attentions often underperform standard softmax attention in quality. To close this performance gap, we find prior linear attentions lack key properties of softmax attention tied to good performance: low-entropy (or \"spiky\") weights and dot-product monotonicity. We further observe surprisingly simple feature maps that retain these properties and match softmax performance, but are inefficient to compute in linear attention. We thus propose Hedgehog, a learnable linear attention that retains the spiky and monoton",
    "path": "papers/24/02/2402.04347.json",
    "total_tokens": 884,
    "translated_title": "刺猬与豪猪：具有Softmax模仿的表达性线性注意力",
    "translated_abstract": "线性注意力已经显示出提高Transformer效率的潜力，将注意力的二次复杂性降低为与序列长度成线性关系。这对于以下三个方面具有激动人心的前景：（1）从头开始训练线性Transformer，（2）将任务特定的Transformer进行“微调-转换”为线性版本，并恢复任务性能，以及（3）将诸如大型语言模型等Transformer进行“预训练-转换”，以实现下游任务的微调。然而，线性注意力在质量上常常不如标准的softmax注意力。为了弥合这种性能差距，我们发现先前的线性注意力缺乏与良好性能相关的softmax注意力的关键属性：低熵（或“尖峰”）权重和点积单调性。我们进一步观察到一种令人惊讶的简单特征映射，保留了这些属性，并与softmax的表现相匹配，但在线性注意力中计算效率低下。因此，我们提出了Hedgehog，一种可学习的线性注意力，保持了尖峰和单调性。",
    "tldr": "Hedgehog是一种具有Softmax模仿的可学习线性注意力，通过保持尖锐和单调性来弥补线性注意力在质量上的不足。"
}