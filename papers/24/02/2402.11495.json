{
    "title": "URLBERT:A Contrastive and Adversarial Pre-trained Model for URL Classification",
    "abstract": "arXiv:2402.11495v1 Announce Type: cross  Abstract: URLs play a crucial role in understanding and categorizing web content, particularly in tasks related to security control and online recommendations. While pre-trained models are currently dominating various fields, the domain of URL analysis still lacks specialized pre-trained models. To address this gap, this paper introduces URLBERT, the first pre-trained representation learning model applied to a variety of URL classification or detection tasks. We first train a URL tokenizer on a corpus of billions of URLs to address URL data tokenization. Additionally, we propose two novel pre-training tasks: (1) self-supervised contrastive learning tasks, which strengthen the model's understanding of URL structure and the capture of category differences by distinguishing different variants of the same URL; (2) virtual adversarial training, aimed at improving the model's robustness in extracting semantic features from URLs. Finally, our proposed ",
    "link": "https://arxiv.org/abs/2402.11495",
    "context": "Title: URLBERT:A Contrastive and Adversarial Pre-trained Model for URL Classification\nAbstract: arXiv:2402.11495v1 Announce Type: cross  Abstract: URLs play a crucial role in understanding and categorizing web content, particularly in tasks related to security control and online recommendations. While pre-trained models are currently dominating various fields, the domain of URL analysis still lacks specialized pre-trained models. To address this gap, this paper introduces URLBERT, the first pre-trained representation learning model applied to a variety of URL classification or detection tasks. We first train a URL tokenizer on a corpus of billions of URLs to address URL data tokenization. Additionally, we propose two novel pre-training tasks: (1) self-supervised contrastive learning tasks, which strengthen the model's understanding of URL structure and the capture of category differences by distinguishing different variants of the same URL; (2) virtual adversarial training, aimed at improving the model's robustness in extracting semantic features from URLs. Finally, our proposed ",
    "path": "papers/24/02/2402.11495.json",
    "total_tokens": 930,
    "translated_title": "URLBERT：一种用于URL分类的对比和对抗预训练模型",
    "translated_abstract": "arXiv：2402.11495v1 发表类型：跨领域摘要：URL在理解和分类网络内容方面发挥着至关重要的作用，特别是在与安全控制和在线推荐相关的任务中。尽管预训练模型目前在各个领域占据主导地位，但URL分析领域仍缺乏专门的预训练模型。为填补这一空白，本文介绍了URLBERT，这是第一个应用于各种URL分类或检测任务的预训练表示学习模型。我们首先在数十亿个URL的语料库上训练了一个URL标记器，以解决URL数据的标记化问题。此外，我们提出了两种新颖的预训练任务：（1）自监督对比学习任务，通过区分相同URL的不同变体来增强模型对URL结构的理解和对类别差异的捕捉；（2）虚拟对抗训练，旨在提高模型从URL中提取语义特征的鲁棒性。最后，我们提出了",
    "tldr": "URLBERT是第一个专门针对URL分类或检测任务的预训练模型，引入了自监督对比学习和虚拟对抗训练两种新颖的预训练任务，以加强模型对URL结构的理解和提高从URL中提取语义特征的鲁棒性。"
}