{
    "title": "Distillation Enhanced Generative Retrieval",
    "abstract": "arXiv:2402.10769v1 Announce Type: cross  Abstract: Generative retrieval is a promising new paradigm in text retrieval that generates identifier strings of relevant passages as the retrieval target. This paradigm leverages powerful generative language models, distinct from traditional sparse or dense retrieval methods. In this work, we identify a viable direction to further enhance generative retrieval via distillation and propose a feasible framework, named DGR. DGR utilizes sophisticated ranking models, such as the cross-encoder, in a teacher role to supply a passage rank list, which captures the varying relevance degrees of passages instead of binary hard labels; subsequently, DGR employs a specially designed distilled RankNet loss to optimize the generative retrieval model, considering the passage rank order provided by the teacher model as labels. This framework only requires an additional distillation step to enhance current generative retrieval systems and does not add any burden",
    "link": "https://arxiv.org/abs/2402.10769",
    "context": "Title: Distillation Enhanced Generative Retrieval\nAbstract: arXiv:2402.10769v1 Announce Type: cross  Abstract: Generative retrieval is a promising new paradigm in text retrieval that generates identifier strings of relevant passages as the retrieval target. This paradigm leverages powerful generative language models, distinct from traditional sparse or dense retrieval methods. In this work, we identify a viable direction to further enhance generative retrieval via distillation and propose a feasible framework, named DGR. DGR utilizes sophisticated ranking models, such as the cross-encoder, in a teacher role to supply a passage rank list, which captures the varying relevance degrees of passages instead of binary hard labels; subsequently, DGR employs a specially designed distilled RankNet loss to optimize the generative retrieval model, considering the passage rank order provided by the teacher model as labels. This framework only requires an additional distillation step to enhance current generative retrieval systems and does not add any burden",
    "path": "papers/24/02/2402.10769.json",
    "total_tokens": 806,
    "translated_title": "蒸馏增强生成式检索",
    "translated_abstract": "生成式检索是文本检索中的一种新兴范式，通过生成相关段落的标识符字符串作为检索目标。该范式利用强大的生成式语言模型，不同于传统的稀疏或密集检索方法。本研究确定了通过蒸馏进一步增强生成式检索的可行方向，并提出了一个名为DGR的可行框架。DGR利用诸如跨编码器等先进排名模型，在教师角色中提供段落排名列表，捕获段落的不同相关程度，而不是二元硬标签；随后，DGR采用一种特别设计的蒸馏RankNet损失来优化生成式检索模型，考虑教师模型提供的段落排名顺序作为标签。该框架仅需要额外的蒸馏步骤来增强当前的生成式检索系统，并不增加任何负担。",
    "tldr": "通过蒸馏方法增强生成式检索系统，提出了一种名为DGR的框架，利用先进排名模型和蒸馏RankNet损失来优化模型。",
    "en_tdlr": "Enhance generative retrieval systems through distillation method, proposing a framework named DGR that utilizes advanced ranking models and distilled RankNet loss for model optimization."
}