{
    "title": "Graph Out-of-Distribution Generalization via Causal Intervention",
    "abstract": "arXiv:2402.11494v1 Announce Type: new  Abstract: Out-of-distribution (OOD) generalization has gained increasing attentions for learning on graphs, as graph neural networks (GNNs) often exhibit performance degradation with distribution shifts. The challenge is that distribution shifts on graphs involve intricate interconnections between nodes, and the environment labels are often absent in data. In this paper, we adopt a bottom-up data-generative perspective and reveal a key observation through causal analysis: the crux of GNNs' failure in OOD generalization lies in the latent confounding bias from the environment. The latter misguides the model to leverage environment-sensitive correlations between ego-graph features and target nodes' labels, resulting in undesirable generalization on new unseen nodes. Built upon this analysis, we introduce a conceptually simple yet principled approach for training robust GNNs under node-level distribution shifts, without prior knowledge of environment",
    "link": "https://arxiv.org/abs/2402.11494",
    "context": "Title: Graph Out-of-Distribution Generalization via Causal Intervention\nAbstract: arXiv:2402.11494v1 Announce Type: new  Abstract: Out-of-distribution (OOD) generalization has gained increasing attentions for learning on graphs, as graph neural networks (GNNs) often exhibit performance degradation with distribution shifts. The challenge is that distribution shifts on graphs involve intricate interconnections between nodes, and the environment labels are often absent in data. In this paper, we adopt a bottom-up data-generative perspective and reveal a key observation through causal analysis: the crux of GNNs' failure in OOD generalization lies in the latent confounding bias from the environment. The latter misguides the model to leverage environment-sensitive correlations between ego-graph features and target nodes' labels, resulting in undesirable generalization on new unseen nodes. Built upon this analysis, we introduce a conceptually simple yet principled approach for training robust GNNs under node-level distribution shifts, without prior knowledge of environment",
    "path": "papers/24/02/2402.11494.json",
    "total_tokens": 797,
    "translated_title": "通过因果干预实现图形的离群分布泛化",
    "translated_abstract": "离群分布（OOD）泛化在图形学习中引起了越来越多的关注，因为图神经网络（GNN）在分布转移时通常会表现出性能下降。本文从自下而上的数据生成角度出发，通过因果分析揭示了一个关键观察：GNN在OOD泛化中失败的关键在于来自环境的潜在混杂偏差。后者误导模型利用自我图特征和目标节点标签之间的环境敏感相关性，导致在新的未见节点上不良泛化。基于这一分析，我们引入了一个在节点级别分布转移下训练稳健GNN的概念简单而又原则性的方法，而不需要环境的先验知识。",
    "tldr": "GNN在离群分布泛化中的失败关键在于来自环境的潜在混杂偏差，因此引入了一个简单而原则性的方法来训练稳健GNN。",
    "en_tdlr": "The key to the failure of GNN in out-of-distribution generalization lies in the latent confounding bias from the environment, thus introducing a simple yet principled approach to train robust GNNs."
}