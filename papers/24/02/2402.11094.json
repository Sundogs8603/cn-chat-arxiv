{
    "title": "Word Embeddings Revisited: Do LLMs Offer Something New?",
    "abstract": "arXiv:2402.11094v1 Announce Type: new  Abstract: Learning meaningful word embeddings is key to training a robust language model. The recent rise of Large Language Models (LLMs) has provided us with many new word/sentence/document embedding models. Although LLMs have shown remarkable advancement in various NLP tasks, it is still unclear whether the performance improvement is merely because of scale or whether underlying embeddings they produce significantly differ from classical encoding models like Sentence-BERT (SBERT) or Universal Sentence Encoder (USE). This paper systematically investigates this issue by comparing classical word embedding techniques against LLM-based word embeddings in terms of their latent vector semantics. Our results show that LLMs tend to cluster semantically related words more tightly than classical models. LLMs also yield higher average accuracy on the Bigger Analogy Test Set (BATS) over classical methods. Finally, some LLMs tend to produce word embeddings si",
    "link": "https://arxiv.org/abs/2402.11094",
    "context": "Title: Word Embeddings Revisited: Do LLMs Offer Something New?\nAbstract: arXiv:2402.11094v1 Announce Type: new  Abstract: Learning meaningful word embeddings is key to training a robust language model. The recent rise of Large Language Models (LLMs) has provided us with many new word/sentence/document embedding models. Although LLMs have shown remarkable advancement in various NLP tasks, it is still unclear whether the performance improvement is merely because of scale or whether underlying embeddings they produce significantly differ from classical encoding models like Sentence-BERT (SBERT) or Universal Sentence Encoder (USE). This paper systematically investigates this issue by comparing classical word embedding techniques against LLM-based word embeddings in terms of their latent vector semantics. Our results show that LLMs tend to cluster semantically related words more tightly than classical models. LLMs also yield higher average accuracy on the Bigger Analogy Test Set (BATS) over classical methods. Finally, some LLMs tend to produce word embeddings si",
    "path": "papers/24/02/2402.11094.json",
    "total_tokens": 915,
    "translated_title": "重新审视词嵌入：LLMs是否提供新的东西？",
    "translated_abstract": "学习有意义的词嵌入对于训练稳健的语言模型至关重要。最近兴起的大型语言模型（LLMs）为我们提供了许多新的单词/句子/文档嵌入模型。尽管LLMs在各种自然语言处理任务中显示出显着的进步，但仍不清楚性能的提升仅仅是因为规模还是它们生成的底层嵌入与句子-BERT（SBERT）或通用句子编码器（USE）之类的传统编码模型有显著区别。本文通过比较经典词嵌入技术与基于LLM的词嵌入，系统地调查了这个问题，从它们的潜在向量语义方面进行比较。我们的结果显示，LLMs倾向于将语义相关的单词更紧密地聚类在一起，LLMs在Bigger Analogy Test Set（BATS）上的平均准确度也高于经典方法。最后，一些LLMs倾向于产生词嵌入si。",
    "tldr": "该论文系统地比较了经典词嵌入技术和基于LLM的词嵌入，发现LLMs倾向于将语义相关的单词更紧密地聚类在一起，并在Bigger Analogy Test Set（BATS）上具有更高的平均准确度。",
    "en_tdlr": "This paper systematically compares classical word embedding techniques with LLM-based word embeddings, finding that LLMs tend to cluster semantically related words more tightly and have higher average accuracy on the Bigger Analogy Test Set (BATS)."
}