{
    "title": "ToMBench: Benchmarking Theory of Mind in Large Language Models",
    "abstract": "arXiv:2402.15052v1 Announce Type: cross  Abstract: Theory of Mind (ToM) is the cognitive capability to perceive and ascribe mental states to oneself and others. Recent research has sparked a debate over whether large language models (LLMs) exhibit a form of ToM. However, existing ToM evaluations are hindered by challenges such as constrained scope, subjective judgment, and unintended contamination, yielding inadequate assessments. To address this gap, we introduce ToMBench with three key characteristics: a systematic evaluation framework encompassing 8 tasks and 31 abilities in social cognition, a multiple-choice question format to support automated and unbiased evaluation, and a build-from-scratch bilingual inventory to strictly avoid data leakage. Based on ToMBench, we conduct extensive experiments to evaluate the ToM performance of 10 popular LLMs across tasks and abilities. We find that even the most advanced LLMs like GPT-4 lag behind human performance by over 10% points, indicati",
    "link": "https://arxiv.org/abs/2402.15052",
    "context": "Title: ToMBench: Benchmarking Theory of Mind in Large Language Models\nAbstract: arXiv:2402.15052v1 Announce Type: cross  Abstract: Theory of Mind (ToM) is the cognitive capability to perceive and ascribe mental states to oneself and others. Recent research has sparked a debate over whether large language models (LLMs) exhibit a form of ToM. However, existing ToM evaluations are hindered by challenges such as constrained scope, subjective judgment, and unintended contamination, yielding inadequate assessments. To address this gap, we introduce ToMBench with three key characteristics: a systematic evaluation framework encompassing 8 tasks and 31 abilities in social cognition, a multiple-choice question format to support automated and unbiased evaluation, and a build-from-scratch bilingual inventory to strictly avoid data leakage. Based on ToMBench, we conduct extensive experiments to evaluate the ToM performance of 10 popular LLMs across tasks and abilities. We find that even the most advanced LLMs like GPT-4 lag behind human performance by over 10% points, indicati",
    "path": "papers/24/02/2402.15052.json",
    "total_tokens": 873,
    "translated_title": "在大型语言模型中基准测试心灵理论",
    "translated_abstract": "心灵理论（ToM）是指感知和归因自己以及他人的心理状态的认知能力。最近的研究引发了关于大型语言模型（LLMs）是否表现出一种形式的心灵理论的争论。然而，现有的心灵理论评估受到诸如受限范围、主观判断和意外污染等挑战的制约，导致评估不足。为了填补这一空白，我们引入了ToMBench，具有三个关键特征：系统评估框架涵盖社会认知中的8项任务和31项能力，多项选择题格式以支持自动化和无偏见的评估，以及基于双语清单的从头构建，严格避免数据泄漏。基于ToMBench，我们进行了大量实验，评估了10个流行LLMs在任务和能力方面的心灵理论表现。我们发现，即使像GPT-4这样的最先进的LLMs也比人类表现落后超过10个百分点。",
    "tldr": "提出了ToMBench框架，在大型语言模型中进行心灵理论性能评估，发现最先进的模型仍然落后于人类表现超过10%。"
}