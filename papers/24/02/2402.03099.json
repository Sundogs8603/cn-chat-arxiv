{
    "title": "Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases",
    "abstract": "Prompt engineering is a challenging and important task due to the high sensitivity of Large Language Models (LLMs) to the given prompt and the inherent ambiguity of a textual task instruction. Automatic prompt engineering is essential to achieve optimized performance from LLMs. Recent studies have demonstrated the capabilities of LLMs to automatically conduct prompt engineering by employing a meta-prompt that incorporates the outcomes of the last trials and proposes an improved prompt. However, this requires a high-quality benchmark to compare different prompts, which is difficult and expensive to acquire in many real-world use cases. In this work, we introduce a new method for automatic prompt engineering, using a calibration process that iteratively refines the prompt to the user intent. During the optimization process, the system jointly generates synthetic data of boundary use cases and optimizes the prompt according to the generated dataset. We demonstrate the effectiveness of our",
    "link": "https://arxiv.org/abs/2402.03099",
    "context": "Title: Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases\nAbstract: Prompt engineering is a challenging and important task due to the high sensitivity of Large Language Models (LLMs) to the given prompt and the inherent ambiguity of a textual task instruction. Automatic prompt engineering is essential to achieve optimized performance from LLMs. Recent studies have demonstrated the capabilities of LLMs to automatically conduct prompt engineering by employing a meta-prompt that incorporates the outcomes of the last trials and proposes an improved prompt. However, this requires a high-quality benchmark to compare different prompts, which is difficult and expensive to acquire in many real-world use cases. In this work, we introduce a new method for automatic prompt engineering, using a calibration process that iteratively refines the prompt to the user intent. During the optimization process, the system jointly generates synthetic data of boundary use cases and optimizes the prompt according to the generated dataset. We demonstrate the effectiveness of our",
    "path": "papers/24/02/2402.03099.json",
    "total_tokens": 791,
    "translated_title": "基于意图的提示校准：用合成边界情况增强提示优化",
    "translated_abstract": "由于大型语言模型（LLMs）对给定提示的高度敏感性和文本任务指令的固有歧义，提示工程是一项具有挑战性和重要性的任务。通过采用一个包含上次试验结果的元提示并提出改进的提示，最近的研究表明LLMs自动进行提示工程的能力。然而，这需要一个高质量的基准来比较不同的提示，在许多实际应用场景中获取高质量的基准是困难且昂贵的。在这项工作中，我们引入了一种新的自动提示工程方法，使用校准过程来迭代地优化与用户意图相符的提示。在优化过程中，系统联合生成边界用例的合成数据，并根据生成的数据集进行提示优化。我们证明了我们方法的有效性。",
    "tldr": "该论文介绍了一种基于意图的提示校准方法，通过迭代优化和生成合成边界情况数据来改进提示工程，以提高大型语言模型的性能。",
    "en_tdlr": "This paper presents a method for intent-based prompt calibration, which improves prompt engineering by iteratively optimizing and generating synthetic boundary cases, resulting in enhanced performance of Large Language Models (LLMs)."
}