{
    "title": "DOF: Accelerating High-order Differential Operators with Forward Propagation",
    "abstract": "arXiv:2402.09730v1 Announce Type: new  Abstract: Solving partial differential equations (PDEs) efficiently is essential for analyzing complex physical systems. Recent advancements in leveraging deep learning for solving PDE have shown significant promise. However, machine learning methods, such as Physics-Informed Neural Networks (PINN), face challenges in handling high-order derivatives of neural network-parameterized functions. Inspired by Forward Laplacian, a recent method of accelerating Laplacian computation, we propose an efficient computational framework, Differential Operator with Forward-propagation (DOF), for calculating general second-order differential operators without losing any precision. We provide rigorous proof of the advantages of our method over existing methods, demonstrating two times improvement in efficiency and reduced memory consumption on any architectures. Empirical results illustrate that our method surpasses traditional automatic differentiation (AutoDiff)",
    "link": "https://arxiv.org/abs/2402.09730",
    "context": "Title: DOF: Accelerating High-order Differential Operators with Forward Propagation\nAbstract: arXiv:2402.09730v1 Announce Type: new  Abstract: Solving partial differential equations (PDEs) efficiently is essential for analyzing complex physical systems. Recent advancements in leveraging deep learning for solving PDE have shown significant promise. However, machine learning methods, such as Physics-Informed Neural Networks (PINN), face challenges in handling high-order derivatives of neural network-parameterized functions. Inspired by Forward Laplacian, a recent method of accelerating Laplacian computation, we propose an efficient computational framework, Differential Operator with Forward-propagation (DOF), for calculating general second-order differential operators without losing any precision. We provide rigorous proof of the advantages of our method over existing methods, demonstrating two times improvement in efficiency and reduced memory consumption on any architectures. Empirical results illustrate that our method surpasses traditional automatic differentiation (AutoDiff)",
    "path": "papers/24/02/2402.09730.json",
    "total_tokens": 806,
    "translated_title": "DOF: 使用前向传播加速高阶微分算子",
    "translated_abstract": "解决偏微分方程(PDE)的高效方法对于分析复杂的物理系统至关重要。最近利用深度学习来解决PDE的进展显示出重要的潜力。然而，机器学习方法，如物理知识驱动的神经网络（PINN），在处理神经网络参数化函数的高阶导数时面临挑战。受到前向Laplacian的启发，一种加速Laplacian计算的最新方法，我们提出了一种高效的计算框架，Differential Operator with Forward-propagation（DOF），用于计算一般的二阶微分算子而不丢失精度。我们提供了对比现有方法优势的严格证明，证明了我们的方法在效率上提高了两倍，并且在任何架构上减少了内存消耗。实证结果表明，我们的方法超过了传统的自动微分（AutoDiff）方法。",
    "tldr": "DOF是一种高效的计算框架，用于加速高阶微分算子的计算，通过前向传播的方式，不丢失精度，在效率和内存消耗上有着明显的改进。"
}