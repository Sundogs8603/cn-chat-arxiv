{
    "title": "Monitored Markov Decision Processes",
    "abstract": "In reinforcement learning (RL), an agent learns to perform a task by interacting with an environment and receiving feedback (a numerical reward) for its actions. However, the assumption that rewards are always observable is often not applicable in real-world problems. For example, the agent may need to ask a human to supervise its actions or activate a monitoring system to receive feedback. There may even be a period of time before rewards become observable, or a period of time after which rewards are no longer given. In other words, there are cases where the environment generates rewards in response to the agent's actions but the agent cannot observe them. In this paper, we formalize a novel but general RL framework - Monitored MDPs - where the agent cannot always observe rewards. We discuss the theoretical and practical consequences of this setting, show challenges raised even in toy environments, and propose algorithms to begin to tackle this novel setting. This paper introduces a p",
    "link": "https://arxiv.org/abs/2402.06819",
    "context": "Title: Monitored Markov Decision Processes\nAbstract: In reinforcement learning (RL), an agent learns to perform a task by interacting with an environment and receiving feedback (a numerical reward) for its actions. However, the assumption that rewards are always observable is often not applicable in real-world problems. For example, the agent may need to ask a human to supervise its actions or activate a monitoring system to receive feedback. There may even be a period of time before rewards become observable, or a period of time after which rewards are no longer given. In other words, there are cases where the environment generates rewards in response to the agent's actions but the agent cannot observe them. In this paper, we formalize a novel but general RL framework - Monitored MDPs - where the agent cannot always observe rewards. We discuss the theoretical and practical consequences of this setting, show challenges raised even in toy environments, and propose algorithms to begin to tackle this novel setting. This paper introduces a p",
    "path": "papers/24/02/2402.06819.json",
    "total_tokens": 916,
    "translated_title": "监控马尔可夫决策过程",
    "translated_abstract": "在强化学习中，代理通过与环境的交互和接收反馈（数值奖励）来学习执行任务。然而，奖励始终可观察的假设在现实世界的问题中通常不适用。例如，代理可能需要要求人类监督其行为或激活监控系统以接收反馈。甚至可能存在奖励在可观察之前一段时间或在不再给予奖励之后的时间。换句话说，有些情况下，环境根据代理的行为生成奖励，但代理无法观察到这些奖励。在本文中，我们正式定义了一个新颖但通用的强化学习框架 - 监控马尔可夫决策过程(Monitored MDPs)，在此框架中代理并非总是能够观察到奖励。我们讨论了这种设置可能带来的理论和实践上的后果，展示了即使在玩具环境中也会出现的挑战，并提出了算法来开始解决这个新颖的场景。",
    "tldr": "这篇论文介绍了一种新颖且通用的强化学习框架——监控马尔可夫决策过程(Monitored MDPs)。在这个框架中，代理不能始终观察到奖励，提出了算法来解决这个新颖的场景。",
    "en_tdlr": "This paper introduces a novel and general framework for reinforcement learning called Monitored Markov Decision Processes (Monitored MDPs), where the agent cannot always observe rewards. Algorithms are proposed to tackle this novel setting."
}