{
    "title": "Sample, estimate, aggregate: A recipe for causal discovery foundation models",
    "abstract": "Causal discovery, the task of inferring causal structure from data, promises to accelerate scientific research, inform policy making, and more. However, the per-dataset nature of existing causal discovery algorithms renders them slow, data hungry, and brittle. Inspired by foundation models, we propose a causal discovery framework where a deep learning model is pretrained to resolve predictions from classical discovery algorithms run over smaller subsets of variables. This method is enabled by the observations that the outputs from classical algorithms are fast to compute for small problems, informative of (marginal) data structure, and their structure outputs as objects remain comparable across datasets. Our method achieves state-of-the-art performance on synthetic and realistic datasets, generalizes to data generating mechanisms not seen during training, and offers inference speeds that are orders of magnitude faster than existing models.",
    "link": "https://arxiv.org/abs/2402.01929",
    "context": "Title: Sample, estimate, aggregate: A recipe for causal discovery foundation models\nAbstract: Causal discovery, the task of inferring causal structure from data, promises to accelerate scientific research, inform policy making, and more. However, the per-dataset nature of existing causal discovery algorithms renders them slow, data hungry, and brittle. Inspired by foundation models, we propose a causal discovery framework where a deep learning model is pretrained to resolve predictions from classical discovery algorithms run over smaller subsets of variables. This method is enabled by the observations that the outputs from classical algorithms are fast to compute for small problems, informative of (marginal) data structure, and their structure outputs as objects remain comparable across datasets. Our method achieves state-of-the-art performance on synthetic and realistic datasets, generalizes to data generating mechanisms not seen during training, and offers inference speeds that are orders of magnitude faster than existing models.",
    "path": "papers/24/02/2402.01929.json",
    "total_tokens": 839,
    "translated_title": "样本、估计、聚合：因果发现基础模型的一种方法",
    "translated_abstract": "因果发现是从数据中推断因果结构的任务，它可以加速科学研究、指导决策等。然而，现有因果发现算法的每个数据集的特性使它们变得缓慢、需要大量数据并且脆弱。受基础模型的启发，我们提出了一种因果发现框架，其中深度学习模型预训练用于处理在较小的变量子集上运行的经典发现算法的预测。这种方法可以利用以下观察结果：经典算法的输出在小问题上计算速度快，对（边际）数据结构具有信息量，且它们的输出结构作为对象在数据集之间可以进行比较。我们的方法在合成和实际数据集上实现了最先进的性能，可以推广到训练期间未见过的数据生成机制，并且提供比现有模型快几个数量级的推理速度。",
    "tldr": "本文提出一种因果发现框架，通过深度学习模型预训练与经典发现算法的结合，实现了快速、准确地推断因果结构，并在实验中展示了与现有方法相比更好的表现和推理速度。",
    "en_tdlr": "This paper proposes a causal discovery framework that combines deep learning model with classical discovery algorithms for fast and accurate inference of causal structure, achieving better performance and inference speed compared to existing methods in experiments."
}