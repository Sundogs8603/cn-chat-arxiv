{
    "title": "Generalization Properties of Adversarial Training for $\\ell_0$-Bounded Adversarial Attacks",
    "abstract": "We have widely observed that neural networks are vulnerable to small additive perturbations to the input causing misclassification. In this paper, we focus on the $\\ell_0$-bounded adversarial attacks, and aim to theoretically characterize the performance of adversarial training for an important class of truncated classifiers. Such classifiers are shown to have strong performance empirically, as well as theoretically in the Gaussian mixture model, in the $\\ell_0$-adversarial setting. The main contribution of this paper is to prove a novel generalization bound for the binary classification setting with $\\ell_0$-bounded adversarial perturbation that is distribution-independent. Deriving a generalization bound in this setting has two main challenges: (i) the truncated inner product which is highly non-linear; and (ii) maximization over the $\\ell_0$ ball due to adversarial training is non-convex and highly non-smooth. To tackle these challenges, we develop new coding techniques for bounding",
    "link": "https://arxiv.org/abs/2402.03576",
    "context": "Title: Generalization Properties of Adversarial Training for $\\ell_0$-Bounded Adversarial Attacks\nAbstract: We have widely observed that neural networks are vulnerable to small additive perturbations to the input causing misclassification. In this paper, we focus on the $\\ell_0$-bounded adversarial attacks, and aim to theoretically characterize the performance of adversarial training for an important class of truncated classifiers. Such classifiers are shown to have strong performance empirically, as well as theoretically in the Gaussian mixture model, in the $\\ell_0$-adversarial setting. The main contribution of this paper is to prove a novel generalization bound for the binary classification setting with $\\ell_0$-bounded adversarial perturbation that is distribution-independent. Deriving a generalization bound in this setting has two main challenges: (i) the truncated inner product which is highly non-linear; and (ii) maximization over the $\\ell_0$ ball due to adversarial training is non-convex and highly non-smooth. To tackle these challenges, we develop new coding techniques for bounding",
    "path": "papers/24/02/2402.03576.json",
    "total_tokens": 890,
    "translated_title": "$\\ell_0$-有界对抗攻击的对抗训练的泛化性质",
    "translated_abstract": "我们广泛观察到神经网络容易受到小的加性扰动的影响，导致错误分类。本文关注$\\ell_0$-有界对抗攻击，并旨在理论上表征截断分类器的对抗训练性能。此类分类器在高斯混合模型和$\\ell_0$-对抗设置中经验上和理论上表现出强大的性能。本文的主要贡献是证明了一个分布无关的$\\ell_0$-有界对抗扰动的二分类设置的新型泛化界限。在这一设置中推导泛化界限面临两个主要挑战：（i）截断内积高度非线性；（ii）由于对抗训练使得在$\\ell_0$空间上的最大化问题是非凸和高度非光滑的。为了解决这些挑战，我们开发了新的编码技术来界定...",
    "tldr": "本文主要通过研究$\\ell_0$-有界对抗攻击，证明了一个分布无关的对抗训练泛化界限，解决了截断内积的非线性和$\\ell_0$空间上最大化问题的挑战。",
    "en_tdlr": "This paper proves a distribution-independent generalization bound for adversarial training under $\\ell_0$-bounded adversarial attacks, addressing the challenges of non-linearity in truncated inner product and maximization problem in $\\ell_0$ space."
}