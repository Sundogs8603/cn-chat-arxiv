{
    "title": "Improve Cross-Architecture Generalization on Dataset Distillation",
    "abstract": "arXiv:2402.13007v1 Announce Type: new  Abstract: Dataset distillation, a pragmatic approach in machine learning, aims to create a smaller synthetic dataset from a larger existing dataset. However, existing distillation methods primarily adopt a model-based paradigm, where the synthetic dataset inherits model-specific biases, limiting its generalizability to alternative models. In response to this constraint, we propose a novel methodology termed \"model pool\". This approach involves selecting models from a diverse model pool based on a specific probability distribution during the data distillation process. Additionally, we integrate our model pool with the established knowledge distillation approach and apply knowledge distillation to the test process of the distilled dataset. Our experimental results validate the effectiveness of the model pool approach across a range of existing models while testing, demonstrating superior performance compared to existing methodologies.",
    "link": "https://arxiv.org/abs/2402.13007",
    "context": "Title: Improve Cross-Architecture Generalization on Dataset Distillation\nAbstract: arXiv:2402.13007v1 Announce Type: new  Abstract: Dataset distillation, a pragmatic approach in machine learning, aims to create a smaller synthetic dataset from a larger existing dataset. However, existing distillation methods primarily adopt a model-based paradigm, where the synthetic dataset inherits model-specific biases, limiting its generalizability to alternative models. In response to this constraint, we propose a novel methodology termed \"model pool\". This approach involves selecting models from a diverse model pool based on a specific probability distribution during the data distillation process. Additionally, we integrate our model pool with the established knowledge distillation approach and apply knowledge distillation to the test process of the distilled dataset. Our experimental results validate the effectiveness of the model pool approach across a range of existing models while testing, demonstrating superior performance compared to existing methodologies.",
    "path": "papers/24/02/2402.13007.json",
    "total_tokens": 826,
    "translated_title": "改进数据集蒸馏上的跨架构泛化",
    "translated_abstract": "数据集蒸馏是机器学习中一种实用的方法，旨在从现有较大的数据集中创建一个较小的合成数据集。然而，现有的蒸馏方法主要采用基于模型的范式，其中合成数据集继承了特定模型的偏见，限制了其对替代模型的泛化能力。为了应对这一限制，我们提出了一种名为“模型池”的新方法。这种方法在数据蒸馏过程中根据特定概率分布从多样的模型池中选择模型。此外，我们将我们的模型池与已建立的知识蒸馏方法相结合，并将知识蒸馏应用于蒸馏数据集的测试过程。我们的实验结果验证了模型池方法在一系列现有模型上的有效性，同时在测试中表现出优于现有方法的性能。",
    "tldr": "提出一种新的“模型池”方法，通过在数据蒸馏过程中选择多样的模型，结合知识蒸馏方法，并将其应用于蒸馏数据集的测试过程，从而改进数据集蒸馏的跨架构泛化。",
    "en_tdlr": "Propose a novel \"model pool\" methodology by selecting diverse models during data distillation, integrating it with knowledge distillation, and applying it to the testing process of the distilled dataset to improve cross-architecture generalization on dataset distillation."
}