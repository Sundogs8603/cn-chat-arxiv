{
    "title": "Instruction Makes a Difference",
    "abstract": "We introduce Instruction Document Visual Question Answering (iDocVQA) dataset and Large Language Document (LLaDoc) model, for training Language-Vision (LV) models for document analysis and predictions on document images, respectively. Usually, deep neural networks for the DocVQA task are trained on datasets lacking instructions. We show that using instruction-following datasets improves performance. We compare performance across document-related datasets using the recent state-of-the-art (SotA) Large Language and Vision Assistant (LLaVA)1.5 as the base model. We also evaluate the performance of the derived models for object hallucination using the Polling-based Object Probing Evaluation (POPE) dataset. The results show that instruction-tuning performance ranges from 11X to 32X of zero-shot performance and from 0.1% to 4.2% over non-instruction (traditional task) finetuning. Despite the gains, these still fall short of human performance (94.36%), implying there's much room for improveme",
    "link": "https://arxiv.org/abs/2402.00453",
    "context": "Title: Instruction Makes a Difference\nAbstract: We introduce Instruction Document Visual Question Answering (iDocVQA) dataset and Large Language Document (LLaDoc) model, for training Language-Vision (LV) models for document analysis and predictions on document images, respectively. Usually, deep neural networks for the DocVQA task are trained on datasets lacking instructions. We show that using instruction-following datasets improves performance. We compare performance across document-related datasets using the recent state-of-the-art (SotA) Large Language and Vision Assistant (LLaVA)1.5 as the base model. We also evaluate the performance of the derived models for object hallucination using the Polling-based Object Probing Evaluation (POPE) dataset. The results show that instruction-tuning performance ranges from 11X to 32X of zero-shot performance and from 0.1% to 4.2% over non-instruction (traditional task) finetuning. Despite the gains, these still fall short of human performance (94.36%), implying there's much room for improveme",
    "path": "papers/24/02/2402.00453.json",
    "total_tokens": 1033,
    "translated_title": "指令的差异",
    "translated_abstract": "我们介绍了Instruction Document Visual Question Answering (iDocVQA)数据集和Large Language Document (LLaDoc)模型，用于训练语言-视觉（LV）模型进行文档分析和文档图像预测。通常，用于DocVQA任务的深度神经网络是在缺乏指令的数据集上进行训练的。我们表明使用遵循指令的数据集可以提高性能。我们使用最新的Large Language and Vision Assistant (LLaVA)1.5作为基础模型，比较了不同文档相关数据集的性能。我们还使用基于投票的对象探测评估（POPE）数据集评估了导出模型的对象幻觉性能。结果表明，指令调优性能相对于零-shot性能提高了11倍到32倍，并且相对于非指令（传统任务）微调提高了0.1%到4.2%。尽管取得了这些进展，但仍然达不到人类性能（94.36%），这意味着有很大的改进空间。",
    "tldr": "通过引入指令数据集，我们展示了在文档分析和文档图像预测中训练语言-视觉模型的重要性，并表明使用指令数据集可以提高性能。使用Polling-based Object Probing Evaluation (POPE)数据集进行评估，我们发现指令调优性能相对于零-shot性能提高了11倍到32倍，并且相对于非指令微调提高了0.1%到4.2%。尽管如此，仍有很大的改进空间，因为这些性能仍未达到人类水平（94.36%）。",
    "en_tdlr": "By introducing instruction-following datasets, we demonstrate the importance of training language-vision models for document analysis and predictions, and show that using instruction datasets improves performance. Evaluation using the POPE dataset reveals that instruction-tuning performance ranges from 11X to 32X of zero-shot performance and from 0.1% to 4.2% over non-instruction finetuning. However, there is still much room for improvement as these performance levels fall short of human performance (94.36%)."
}