{
    "title": "Code as Reward: Empowering Reinforcement Learning with VLMs",
    "abstract": "Pre-trained Vision-Language Models (VLMs) are able to understand visual concepts, describe and decompose complex tasks into sub-tasks, and provide feedback on task completion. In this paper, we aim to leverage these capabilities to support the training of reinforcement learning (RL) agents. In principle, VLMs are well suited for this purpose, as they can naturally analyze image-based observations and provide feedback (reward) on learning progress. However, inference in VLMs is computationally expensive, so querying them frequently to compute rewards would significantly slowdown the training of an RL agent. To address this challenge, we propose a framework named Code as Reward (VLM-CaR). VLM-CaR produces dense reward functions from VLMs through code generation, thereby significantly reducing the computational burden of querying the VLM directly. We show that the dense rewards generated through our approach are very accurate across a diverse set of discrete and continuous environments, a",
    "link": "https://arxiv.org/abs/2402.04764",
    "context": "Title: Code as Reward: Empowering Reinforcement Learning with VLMs\nAbstract: Pre-trained Vision-Language Models (VLMs) are able to understand visual concepts, describe and decompose complex tasks into sub-tasks, and provide feedback on task completion. In this paper, we aim to leverage these capabilities to support the training of reinforcement learning (RL) agents. In principle, VLMs are well suited for this purpose, as they can naturally analyze image-based observations and provide feedback (reward) on learning progress. However, inference in VLMs is computationally expensive, so querying them frequently to compute rewards would significantly slowdown the training of an RL agent. To address this challenge, we propose a framework named Code as Reward (VLM-CaR). VLM-CaR produces dense reward functions from VLMs through code generation, thereby significantly reducing the computational burden of querying the VLM directly. We show that the dense rewards generated through our approach are very accurate across a diverse set of discrete and continuous environments, a",
    "path": "papers/24/02/2402.04764.json",
    "total_tokens": 920,
    "translated_title": "代码即奖励：用VLM增强强化学习的动力",
    "translated_abstract": "预训练的视觉语言模型(VLMs)能够理解视觉概念，描述并分解复杂任务为子任务，并提供有关任务完成的反馈。本文旨在利用这些能力来支持增强学习(RL)代理的训练。原则上，VLMs非常适合这个目的，因为它们可以自然地分析基于图像的观察结果，并提供关于学习进度的反馈(奖励)。然而，VLMs的推理过程计算代价很高，频繁查询以计算奖励将显著减慢RL代理的训练速度。为了解决这个挑战，我们提出了一种名为“代码即奖励”(VLM-CaR)的框架。VLM-CaR通过代码生成从VLMs生成密集的奖励函数，从而显著减轻了直接查询VLM的计算负担。我们证明通过我们的方法生成的密集奖励在多样的离散和连续环境中都非常准确。",
    "tldr": "本文介绍了一种利用预训练视觉语言模型(VLMs)来支持强化学习(RL)代理训练的框架，通过代码生成从VLMs生成密集奖励函数，减轻了直接查询VLM的计算负担。",
    "en_tdlr": "This paper introduces a framework that utilizes pre-trained Vision-Language Models (VLMs) to support the training of reinforcement learning (RL) agents. By generating dense reward functions through code generation, the computational burden of querying the VLM directly is significantly reduced."
}