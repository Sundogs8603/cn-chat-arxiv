{
    "title": "Is it safe to cross? Interpretable Risk Assessment with GPT-4V for Safety-Aware Street Crossing",
    "abstract": "Safely navigating street intersections is a complex challenge for blind and low-vision individuals, as it requires a nuanced understanding of the surrounding context - a task heavily reliant on visual cues. Traditional methods for assisting in this decision-making process often fall short, lacking the ability to provide a comprehensive scene analysis and safety level. This paper introduces an innovative approach that leverages large multimodal models (LMMs) to interpret complex street crossing scenes, offering a potential advancement over conventional traffic signal recognition techniques. By generating a safety score and scene description in natural language, our method supports safe decision-making for the blind and low-vision individuals. We collected crosswalk intersection data that contains multiview egocentric images captured by a quadruped robot and annotated the images with corresponding safety scores based on our predefined safety score categorization. Grounded on the visual k",
    "link": "https://arxiv.org/abs/2402.06794",
    "context": "Title: Is it safe to cross? Interpretable Risk Assessment with GPT-4V for Safety-Aware Street Crossing\nAbstract: Safely navigating street intersections is a complex challenge for blind and low-vision individuals, as it requires a nuanced understanding of the surrounding context - a task heavily reliant on visual cues. Traditional methods for assisting in this decision-making process often fall short, lacking the ability to provide a comprehensive scene analysis and safety level. This paper introduces an innovative approach that leverages large multimodal models (LMMs) to interpret complex street crossing scenes, offering a potential advancement over conventional traffic signal recognition techniques. By generating a safety score and scene description in natural language, our method supports safe decision-making for the blind and low-vision individuals. We collected crosswalk intersection data that contains multiview egocentric images captured by a quadruped robot and annotated the images with corresponding safety scores based on our predefined safety score categorization. Grounded on the visual k",
    "path": "papers/24/02/2402.06794.json",
    "total_tokens": 841,
    "translated_title": "是否安全过马路？GPT-4V用于安全意识的可解释风险评估",
    "translated_abstract": "对于盲人和视力低下的人来说，安全地通过街道交叉口是一个复杂的挑战，因为它需要对周围环境有细致的理解，而这个任务很大程度上依赖于视觉线索。传统的辅助决策方法往往不够完善，无法提供全面的场景分析和安全级别判断。本文介绍了一种创新的方法，利用大型多模型来解释复杂的过马路场景，相比传统的交通信号识别技术，提供了潜在的进步。我们的方法通过生成安全评分和自然语言场景描述，支持盲人和视力低下人士安全决策。我们收集了由四足机器人捕获的多视角自我中心图像构成的过马路交叉口数据，并根据预先定义的安全评分分类进行了图像标注。",
    "tldr": "本文介绍了使用GPT-4V进行可解释风险评估的方法，该方法通过解释复杂的过马路场景，为盲人和视力低下人士的安全决策提供支持。",
    "en_tdlr": "This paper introduces a method using GPT-4V for interpretable risk assessment, which supports safe decision-making for blind and low-vision individuals by interpreting complex street crossing scenes."
}