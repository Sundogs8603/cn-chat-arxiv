{
    "title": "On the Role of Initialization on the Implicit Bias in Deep Linear Networks",
    "abstract": "Despite Deep Learning's (DL) empirical success, our theoretical understanding of its efficacy remains limited. One notable paradox is that while conventional wisdom discourages perfect data fitting, deep neural networks are designed to do just that, yet they generalize effectively. This study focuses on exploring this phenomenon attributed to the implicit bias at play. Various sources of implicit bias have been identified, such as step size, weight initialization, optimization algorithm, and number of parameters. In this work, we focus on investigating the implicit bias originating from weight initialization. To this end, we examine the problem of solving underdetermined linear systems in various contexts, scrutinizing the impact of initialization on the implicit regularization when using deep networks to solve such systems. Our findings elucidate the role of initialization in the optimization and generalization paradoxes, contributing to a more comprehensive understanding of DL's perf",
    "link": "https://arxiv.org/abs/2402.02454",
    "context": "Title: On the Role of Initialization on the Implicit Bias in Deep Linear Networks\nAbstract: Despite Deep Learning's (DL) empirical success, our theoretical understanding of its efficacy remains limited. One notable paradox is that while conventional wisdom discourages perfect data fitting, deep neural networks are designed to do just that, yet they generalize effectively. This study focuses on exploring this phenomenon attributed to the implicit bias at play. Various sources of implicit bias have been identified, such as step size, weight initialization, optimization algorithm, and number of parameters. In this work, we focus on investigating the implicit bias originating from weight initialization. To this end, we examine the problem of solving underdetermined linear systems in various contexts, scrutinizing the impact of initialization on the implicit regularization when using deep networks to solve such systems. Our findings elucidate the role of initialization in the optimization and generalization paradoxes, contributing to a more comprehensive understanding of DL's perf",
    "path": "papers/24/02/2402.02454.json",
    "total_tokens": 882,
    "translated_title": "关于初始化对深度线性网络中隐式偏差的作用",
    "translated_abstract": "尽管深度学习在实践中取得了成功，但我们对其有效性的理论了解仍然有限。一个显著的悖论是，传统观点不鼓励完美拟合数据，而深度神经网络则被设计成做这件事，然而它们能有效地泛化。本研究着重探索与此现象相关的隐式偏差。已经确定了各种隐式偏差的来源，例如步长、权重初始化、优化算法和参数数量。在这项工作中，我们专注于研究由权重初始化引起的隐式偏差。为此，我们在各种情况下研究了解决欠定线性系统的问题，评估了在使用深度网络解决这些系统时初始化对隐式正则化的影响。我们的研究结果阐明了初始化在优化和泛化悖论中的作用，有助于更全面地理解深度学习的性能。",
    "tldr": "本研究探索了深度学习中隐式偏差的起源，重点研究了由权重初始化引起的隐式偏差。研究结果揭示了初始化对优化和泛化的悖论的作用，为深度学习的性能提供了更全面的理解。"
}