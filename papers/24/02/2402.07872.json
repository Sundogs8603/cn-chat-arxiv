{
    "title": "PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs",
    "abstract": "Vision language models (VLMs) have shown impressive capabilities across a variety of tasks, from logical reasoning to visual understanding. This opens the door to richer interaction with the world, for example robotic control. However, VLMs produce only textual outputs, while robotic control and other spatial tasks require outputting continuous coordinates, actions, or trajectories. How can we enable VLMs to handle such settings without fine-tuning on task-specific data?   In this paper, we propose a novel visual prompting approach for VLMs that we call Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as iterative visual question answering. In each iteration, the image is annotated with a visual representation of proposals that the VLM can refer to (e.g., candidate robot actions, localizations, or trajectories). The VLM then selects the best ones for the task. These proposals are iteratively refined, allowing the VLM to eventually zero in on the best available an",
    "link": "https://arxiv.org/abs/2402.07872",
    "context": "Title: PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs\nAbstract: Vision language models (VLMs) have shown impressive capabilities across a variety of tasks, from logical reasoning to visual understanding. This opens the door to richer interaction with the world, for example robotic control. However, VLMs produce only textual outputs, while robotic control and other spatial tasks require outputting continuous coordinates, actions, or trajectories. How can we enable VLMs to handle such settings without fine-tuning on task-specific data?   In this paper, we propose a novel visual prompting approach for VLMs that we call Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as iterative visual question answering. In each iteration, the image is annotated with a visual representation of proposals that the VLM can refer to (e.g., candidate robot actions, localizations, or trajectories). The VLM then selects the best ones for the task. These proposals are iteratively refined, allowing the VLM to eventually zero in on the best available an",
    "path": "papers/24/02/2402.07872.json",
    "total_tokens": 956,
    "translated_title": "PIVOT: 迭代视觉提示激发可操作知识用于VLMs",
    "translated_abstract": "视觉语言模型（VLMs）显示出在各种任务中的令人印象深刻的能力，从逻辑推理到视觉理解。这为与世界进行更丰富的互动打开了大门，例如机器人控制。然而，VLMs只产生文本输出，而机器人控制和其他空间任务需要输出连续的坐标，动作或轨迹。我们如何在不对任务特定数据进行微调的情况下使VLMs能够处理这种设置呢？在本文中，我们提出了一种新颖的VLMs视觉提示方法，称之为迭代视觉优化提示（PIVOT），将任务视为迭代的视觉问答。在每个迭代中，图像被注释为VLMs可以参考的提案的视觉表示（例如候选机器人动作、定位或轨迹）。然后，VLMs选择最佳的任务。这些提案经过迭代优化，使VLMs最终找到最佳的可用选项。",
    "tldr": "本文介绍了一种名为PIVOT的新颖视觉提示方法，它通过迭代的视觉问答将任务转化为VLMs问题。每个迭代中，图像被标注为VLMs可以参考的视觉表示，并通过优化选择最佳选项。这种方法能够使VLMs进行机器人控制和其他空间任务的输出。"
}