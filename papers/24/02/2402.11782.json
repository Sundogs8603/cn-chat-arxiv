{
    "title": "What Evidence Do Language Models Find Convincing?",
    "abstract": "arXiv:2402.11782v1 Announce Type: new  Abstract: Retrieval-augmented language models are being increasingly tasked with subjective, contentious, and conflicting queries such as \"is aspartame linked to cancer\". To resolve these ambiguous queries, one must search through a large range of websites and consider \"which, if any, of this evidence do I find convincing?\". In this work, we study how LLMs answer this question. In particular, we construct ConflictingQA, a dataset that pairs controversial queries with a series of real-world evidence documents that contain different facts (e.g., quantitative results), argument styles (e.g., appeals to authority), and answers (Yes or No). We use this dataset to perform sensitivity and counterfactual analyses to explore which text features most affect LLM predictions. Overall, we find that current models rely heavily on the relevance of a website to the query, while largely ignoring stylistic features that humans find important such as whether a text ",
    "link": "https://arxiv.org/abs/2402.11782",
    "context": "Title: What Evidence Do Language Models Find Convincing?\nAbstract: arXiv:2402.11782v1 Announce Type: new  Abstract: Retrieval-augmented language models are being increasingly tasked with subjective, contentious, and conflicting queries such as \"is aspartame linked to cancer\". To resolve these ambiguous queries, one must search through a large range of websites and consider \"which, if any, of this evidence do I find convincing?\". In this work, we study how LLMs answer this question. In particular, we construct ConflictingQA, a dataset that pairs controversial queries with a series of real-world evidence documents that contain different facts (e.g., quantitative results), argument styles (e.g., appeals to authority), and answers (Yes or No). We use this dataset to perform sensitivity and counterfactual analyses to explore which text features most affect LLM predictions. Overall, we find that current models rely heavily on the relevance of a website to the query, while largely ignoring stylistic features that humans find important such as whether a text ",
    "path": "papers/24/02/2402.11782.json",
    "total_tokens": 909,
    "translated_title": "语言模型认为哪些证据令人信服？",
    "translated_abstract": "检索增强型语言模型越来越多地被赋予主观、有争议和矛盾的查询任务，如“阿斯巴甜是否与癌症有关”。为了解决这些模糊的查询，我们必须搜索大量网站，并考虑“我认为哪些证据是令人信服的？”。在这项工作中，我们研究了语言模型是如何回答这个问题的。特别是，我们构建了一个名为 ConflictingQA 的数据集，将有争议的查询与一系列包含不同事实（如定量结果）、论证风格（如权威呼声）和答案（是或否）的真实世界证据文档配对。我们使用这个数据集进行敏感性和反事实分析，探讨哪些文本特征最影响语言模型的预测。总体而言，我们发现当前模型在很大程度上依赖网站与查询的相关性，而在很大程度上忽视了人类认为重要的风格特征，比如文本是否是",
    "tldr": "通过构建 ConflictingQA 数据集，并进行敏感性和反事实分析，研究发现当前语言模型在预测时很大程度上依赖于网站与查询的相关性，而忽视了人类认为重要的文本风格特征。",
    "en_tdlr": "By constructing the ConflictingQA dataset and conducting sensitivity and counterfactual analyses, the study found that current language models heavily rely on the relevance of websites to the queries while largely ignoring text stylistic features that humans find important."
}