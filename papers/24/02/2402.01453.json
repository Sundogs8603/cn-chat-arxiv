{
    "title": "The Queen of England is not England's Queen: On the Lack of Factual Coherency in PLMs",
    "abstract": "Factual knowledge encoded in Pre-trained Language Models (PLMs) enriches their representations and justifies their use as knowledge bases. Previous work has focused on probing PLMs for factual knowledge by measuring how often they can correctly predict an object entity given a subject and a relation, and improving fact retrieval by optimizing the prompts used for querying PLMs. In this work, we consider a complementary aspect, namely the coherency of factual knowledge in PLMs, i.e., how often can PLMs predict the subject entity given its initial prediction of the object entity. This goes beyond evaluating how much PLMs know, and focuses on the internal state of knowledge inside them. Our results indicate that PLMs have low coherency using manually written, optimized and paraphrased prompts, but including an evidence paragraph leads to substantial improvement. This shows that PLMs fail to model inverse relations and need further enhancements to be able to handle retrieving facts from th",
    "link": "https://rss.arxiv.org/abs/2402.01453",
    "context": "Title: The Queen of England is not England's Queen: On the Lack of Factual Coherency in PLMs\nAbstract: Factual knowledge encoded in Pre-trained Language Models (PLMs) enriches their representations and justifies their use as knowledge bases. Previous work has focused on probing PLMs for factual knowledge by measuring how often they can correctly predict an object entity given a subject and a relation, and improving fact retrieval by optimizing the prompts used for querying PLMs. In this work, we consider a complementary aspect, namely the coherency of factual knowledge in PLMs, i.e., how often can PLMs predict the subject entity given its initial prediction of the object entity. This goes beyond evaluating how much PLMs know, and focuses on the internal state of knowledge inside them. Our results indicate that PLMs have low coherency using manually written, optimized and paraphrased prompts, but including an evidence paragraph leads to substantial improvement. This shows that PLMs fail to model inverse relations and need further enhancements to be able to handle retrieving facts from th",
    "path": "papers/24/02/2402.01453.json",
    "total_tokens": 927,
    "translated_title": "英国女王并非英格兰的女王：关于PLMs中缺乏事实一致性的问题",
    "translated_abstract": "预训练语言模型（PLMs）中编码的事实知识丰富了其表示，并证明了它们作为知识库的使用合理性。先前的研究侧重于通过衡量PLMs能够在给定主题和关系的情况下正确预测对象实体的频率，以及通过优化查询PLMs时使用的提示来改进事实检索。在本研究中，我们考虑了一个补充性的方面，即PLMs中事实知识的一致性，即在PLMs通过对对象实体的初始预测后，能够多少次预测到主题实体。这超越了评估PLMs所知的程度，而是关注其内部知识状态。我们的结果表明，PLMs在使用手动编写的、优化的和改写的提示时具有较低的一致性，但包含证据段落可以显著提高一致性。这表明，PLMs没有模拟逆关系的能力，需要进一步增强以处理从事实中检索信息的能力。",
    "tldr": "本研究探讨了PLMs中事实知识的一致性问题，结果显示PLMs在使用提示时具有较低的一致性，但通过添加证据段落可以显著提高。这表明PLMs需要进一步增强以处理从事实中检索信息的能力。",
    "en_tdlr": "This study investigates the coherency of factual knowledge in PLMs and finds that they have low coherency when using prompts, but adding an evidence paragraph significantly improves it. This indicates that further enhancements are needed for PLMs to handle fact retrieval effectively."
}