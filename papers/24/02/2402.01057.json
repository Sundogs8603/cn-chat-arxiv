{
    "title": "Expert Proximity as Surrogate Rewards for Single Demonstration Imitation Learning",
    "abstract": "In this paper, we focus on single-demonstration imitation learning (IL), a practical approach for real-world applications where obtaining numerous expert demonstrations is costly or infeasible. In contrast to typical IL settings with multiple demonstrations, single-demonstration IL involves an agent having access to only one expert trajectory. We highlight the issue of sparse reward signals in this setting and propose to mitigate this issue through our proposed Transition Discriminator-based IL (TDIL) method. TDIL is an IRL method designed to address reward sparsity by introducing a denser surrogate reward function that considers environmental dynamics. This surrogate reward function encourages the agent to navigate towards states that are proximal to expert states. In practice, TDIL trains a transition discriminator to differentiate between valid and non-valid transitions in a given environment to compute the surrogate rewards. The experiments demonstrate that TDIL outperforms existin",
    "link": "https://rss.arxiv.org/abs/2402.01057",
    "context": "Title: Expert Proximity as Surrogate Rewards for Single Demonstration Imitation Learning\nAbstract: In this paper, we focus on single-demonstration imitation learning (IL), a practical approach for real-world applications where obtaining numerous expert demonstrations is costly or infeasible. In contrast to typical IL settings with multiple demonstrations, single-demonstration IL involves an agent having access to only one expert trajectory. We highlight the issue of sparse reward signals in this setting and propose to mitigate this issue through our proposed Transition Discriminator-based IL (TDIL) method. TDIL is an IRL method designed to address reward sparsity by introducing a denser surrogate reward function that considers environmental dynamics. This surrogate reward function encourages the agent to navigate towards states that are proximal to expert states. In practice, TDIL trains a transition discriminator to differentiate between valid and non-valid transitions in a given environment to compute the surrogate rewards. The experiments demonstrate that TDIL outperforms existin",
    "path": "papers/24/02/2402.01057.json",
    "total_tokens": 885,
    "translated_title": "专家接近性作为单演示模仿学习的替代奖励",
    "translated_abstract": "本文关注单演示模仿学习（IL），这是一种在获取大量专家演示困难或不可行的实际应用中的实用方法。与 typicIL 设置中具有多个示范不同，单演示IL涉及代理只有一条专家轨迹的访问。我们强调在这种情况下奖励信号稀疏的问题，并提出通过我们提出的基于转换鉴别器的IL（TDIL）方法来缓解这个问题。TDIL是一种基于IRL的方法，旨在通过引入考虑环境动态的更密集的替代奖励函数来解决奖励稀疏性。这个替代奖励函数鼓励代理向靠近专家状态的状态导航。在实践中，TDIL训练一个过渡鉴别器来区分给定环境中的有效和非有效过渡以计算替代奖励。实验表明，TDIL优于现有方法。",
    "tldr": "本文介绍了一种针对单演示模仿学习的新方法TDIL，通过引入基于转换鉴别器的替代奖励函数，鼓励代理向靠近专家状态的状态导航，有效解决了奖励信号稀疏的问题。"
}