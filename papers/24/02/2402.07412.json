{
    "title": "Auxiliary Reward Generation with Transition Distance Representation Learning",
    "abstract": "Reinforcement learning (RL) has shown its strength in challenging sequential decision-making problems. The reward function in RL is crucial to the learning performance, as it serves as a measure of the task completion degree. In real-world problems, the rewards are predominantly human-designed, which requires laborious tuning, and is easily affected by human cognitive biases. To achieve automatic auxiliary reward generation, we propose a novel representation learning approach that can measure the ``transition distance'' between states. Building upon these representations, we introduce an auxiliary reward generation technique for both single-task and skill-chaining scenarios without the need for human knowledge. The proposed approach is evaluated in a wide range of manipulation tasks. The experiment results demonstrate the effectiveness of measuring the transition distance between states and the induced improvement by auxiliary rewards, which not only promotes better learning efficiency",
    "link": "https://arxiv.org/abs/2402.07412",
    "context": "Title: Auxiliary Reward Generation with Transition Distance Representation Learning\nAbstract: Reinforcement learning (RL) has shown its strength in challenging sequential decision-making problems. The reward function in RL is crucial to the learning performance, as it serves as a measure of the task completion degree. In real-world problems, the rewards are predominantly human-designed, which requires laborious tuning, and is easily affected by human cognitive biases. To achieve automatic auxiliary reward generation, we propose a novel representation learning approach that can measure the ``transition distance'' between states. Building upon these representations, we introduce an auxiliary reward generation technique for both single-task and skill-chaining scenarios without the need for human knowledge. The proposed approach is evaluated in a wide range of manipulation tasks. The experiment results demonstrate the effectiveness of measuring the transition distance between states and the induced improvement by auxiliary rewards, which not only promotes better learning efficiency",
    "path": "papers/24/02/2402.07412.json",
    "total_tokens": 778,
    "translated_title": "使用状态转换距离表示学习的辅助奖励生成",
    "translated_abstract": "强化学习在复杂的顺序决策问题中展现出了其优势。在强化学习中，奖励函数对学习性能至关重要，因为它作为任务完成程度的衡量标准。在实际应用中，奖励往往是由人工设计的，需要费时费力的调整，并且容易受到人类认知偏差的影响。为了实现自动的辅助奖励生成，我们提出了一种新颖的表示学习方法，可以衡量状态之间的“转换距离”。在这些表示的基础上，我们引入了一种无需人类知识的辅助奖励生成技术，用于单任务和技能链场景。提出的方法在广泛的操作任务中进行了评估。实验结果表明，测量状态之间的转换距离以及辅助奖励引起的改进有效提高了学习效率。",
    "tldr": "这篇论文提出了一种使用状态转换距离表示学习的辅助奖励生成方法，可以在强化学习中自动生成奖励，提高学习效率和减少人工设计奖励的工作量。"
}