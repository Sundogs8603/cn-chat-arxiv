{
    "title": "The Optimality of Kernel Classifiers in Sobolev Space",
    "abstract": "Kernel methods are widely used in machine learning, especially for classification problems. However, the theoretical analysis of kernel classification is still limited. This paper investigates the statistical performances of kernel classifiers. With some mild assumptions on the conditional probability $\\eta(x)=\\mathbb{P}(Y=1\\mid X=x)$, we derive an upper bound on the classification excess risk of a kernel classifier using recent advances in the theory of kernel regression. We also obtain a minimax lower bound for Sobolev spaces, which shows the optimality of the proposed classifier. Our theoretical results can be extended to the generalization error of overparameterized neural network classifiers. To make our theoretical results more applicable in realistic settings, we also propose a simple method to estimate the interpolation smoothness of $2\\eta(x)-1$ and apply the method to real datasets.",
    "link": "https://rss.arxiv.org/abs/2402.01148",
    "context": "Title: The Optimality of Kernel Classifiers in Sobolev Space\nAbstract: Kernel methods are widely used in machine learning, especially for classification problems. However, the theoretical analysis of kernel classification is still limited. This paper investigates the statistical performances of kernel classifiers. With some mild assumptions on the conditional probability $\\eta(x)=\\mathbb{P}(Y=1\\mid X=x)$, we derive an upper bound on the classification excess risk of a kernel classifier using recent advances in the theory of kernel regression. We also obtain a minimax lower bound for Sobolev spaces, which shows the optimality of the proposed classifier. Our theoretical results can be extended to the generalization error of overparameterized neural network classifiers. To make our theoretical results more applicable in realistic settings, we also propose a simple method to estimate the interpolation smoothness of $2\\eta(x)-1$ and apply the method to real datasets.",
    "path": "papers/24/02/2402.01148.json",
    "total_tokens": 865,
    "translated_title": "Sobolev空间中核分类器的最优性",
    "translated_abstract": "核方法在机器学习中广泛应用，特别是用于分类问题。然而，核分类的理论分析仍然有限。本文研究了核分类器的统计性能。在对条件概率$\\eta(x)=\\mathbb{P}(Y=1\\mid X=x)$作出一些温和的假设后，我们利用核回归理论的最新进展，导出了核分类器分类超额风险的上界。我们还得到了Sobolev空间的极小极大下界，从而证明了所提出分类器的最优性。我们的理论结果可以扩展到超参数化神经网络分类器的泛化误差。为了使我们的理论结果在实际环境中更加适用，我们还提出了一种估计$2\\eta(x)-1$插值平滑度的简单方法，并将该方法应用于实际数据集。",
    "tldr": "本文研究了核分类器在Sobolev空间中的最优性质，并通过对条件概率的假设和核回归理论的应用，导出了核分类器的分类超额风险上界和Sobolev空间的极小极大下界。此外，我们还提出了一种简单方法来估计插值平滑度，并将其应用于实际数据集。",
    "en_tdlr": "This paper investigates the optimality of kernel classifiers in Sobolev space and derives upper bounds on their classification excess risk using assumptions on conditional probability and advances in kernel regression theory. The paper also presents a minimax lower bound for Sobolev spaces and proposes a simple method to estimate interpolation smoothness, which is applied to real datasets."
}