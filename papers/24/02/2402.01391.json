{
    "title": "StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback",
    "abstract": "The advancement of large language models (LLMs) has significantly propelled the field of code generation. Previous work integrated reinforcement learning (RL) with compiler feedback for exploring the output space of LLMs to enhance code generation quality. However, the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective. To tackle these challenges, we introduce StepCoder, a novel RL framework for code generation, consisting of two main components: CCCS addresses the exploration challenge by breaking the long sequences code generation task into a Curriculum of Code Completion Subtasks, while FGO only optimizes the model by masking the unexecuted code segments to provide Fine-Grained Optimization. In addition, we furthermore construct the APPS+ dataset for RL training, which is manually verified to ens",
    "link": "https://rss.arxiv.org/abs/2402.01391",
    "context": "Title: StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback\nAbstract: The advancement of large language models (LLMs) has significantly propelled the field of code generation. Previous work integrated reinforcement learning (RL) with compiler feedback for exploring the output space of LLMs to enhance code generation quality. However, the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective. To tackle these challenges, we introduce StepCoder, a novel RL framework for code generation, consisting of two main components: CCCS addresses the exploration challenge by breaking the long sequences code generation task into a Curriculum of Code Completion Subtasks, while FGO only optimizes the model by masking the unexecuted code segments to provide Fine-Grained Optimization. In addition, we furthermore construct the APPS+ dataset for RL training, which is manually verified to ens",
    "path": "papers/24/02/2402.01391.json",
    "total_tokens": 883,
    "translated_title": "StepCoder: 使用强化学习从编译器反馈中改进代码生成",
    "translated_abstract": "大型语言模型的发展极大地推动了代码生成领域。以前的工作将强化学习与编译器反馈结合起来，探索语言模型的输出空间，以提高代码生成质量。然而，为了满足复杂的人类要求，语言模型生成的代码往往很长，这使得强化学习的探索成为一项挑战。此外，由于单元测试可能无法涵盖复杂代码，使用这些未执行的代码片段来优化语言模型是无效的。为了解决这些问题，我们引入了StepCoder，这是一个用于代码生成的新型强化学习框架，包含两个主要组件：CCCS通过将长序列代码生成任务分解为一系列代码完成子任务来解决探索挑战，而FGO通过屏蔽未执行的代码段来提供细粒度优化，仅优化模型。此外，我们还构建了APPS+数据集用于强化学习训练，该数据集经过手动验证确保质量。",
    "tldr": "StepCoder是一个基于强化学习的代码生成框架，通过将长序列代码生成任务分解为一系列代码完成子任务来解决探索挑战，并使用细粒度优化来提高模型的效果。"
}