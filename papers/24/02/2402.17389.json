{
    "title": "FairBelief - Assessing Harmful Beliefs in Language Models",
    "abstract": "arXiv:2402.17389v1 Announce Type: cross  Abstract: Language Models (LMs) have been shown to inherit undesired biases that might hurt minorities and underrepresented groups if such systems were integrated into real-world applications without careful fairness auditing. This paper proposes FairBelief, an analytical approach to capture and assess beliefs, i.e., propositions that an LM may embed with different degrees of confidence and that covertly influence its predictions. With FairBelief, we leverage prompting to study the behavior of several state-of-the-art LMs across different previously neglected axes, such as model scale and likelihood, assessing predictions on a fairness dataset specifically designed to quantify LMs' outputs' hurtfulness. Finally, we conclude with an in-depth qualitative assessment of the beliefs emitted by the models. We apply FairBelief to English LMs, revealing that, although these architectures enable high performances on diverse natural language processing ta",
    "link": "https://arxiv.org/abs/2402.17389",
    "context": "Title: FairBelief - Assessing Harmful Beliefs in Language Models\nAbstract: arXiv:2402.17389v1 Announce Type: cross  Abstract: Language Models (LMs) have been shown to inherit undesired biases that might hurt minorities and underrepresented groups if such systems were integrated into real-world applications without careful fairness auditing. This paper proposes FairBelief, an analytical approach to capture and assess beliefs, i.e., propositions that an LM may embed with different degrees of confidence and that covertly influence its predictions. With FairBelief, we leverage prompting to study the behavior of several state-of-the-art LMs across different previously neglected axes, such as model scale and likelihood, assessing predictions on a fairness dataset specifically designed to quantify LMs' outputs' hurtfulness. Finally, we conclude with an in-depth qualitative assessment of the beliefs emitted by the models. We apply FairBelief to English LMs, revealing that, although these architectures enable high performances on diverse natural language processing ta",
    "path": "papers/24/02/2402.17389.json",
    "total_tokens": 865,
    "translated_title": "公平信念 - 评估语言模型中的有害信念",
    "translated_abstract": "语言模型（LMs）已被证明存在不良偏见，如果这些系统在没有仔细进行公平审计的情况下集成到现实应用中，可能会伤害少数群体和被忽视的群体。本文提出了FairBelief，一种分析方法，用于捕获和评估信念，即LM可能以不同程度的确信度嵌入的命题，这些命题暗中影响其预测。通过FairBelief，我们利用提示来研究几种最先进的LM在先前被忽视的不同轴上的行为，比如模型规模和可能性，评估对专门设计用于量化LM输出伤害程度的公平数据集的预测。最后，我们对模型发出的信念进行深入的定性评估。我们将FairBelief应用于英语LMs，发现尽管这些架构在各种自然语言处理任务上表现出色，但它们也可能存在有害信念。",
    "tldr": "本文提出了FairBelief，一种用于捕获和评估语言模型中有害信念的分析方法，并通过公平数据集对几种最先进的LM进行评估，发现这些LM可能存在有害信念。",
    "en_tdlr": "This paper introduces FairBelief, an analytical approach to capture and assess harmful beliefs in language models, and evaluates several state-of-the-art LMs using a fairness dataset, revealing the presence of harmful beliefs in these models."
}