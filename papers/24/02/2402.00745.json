{
    "title": "Enhancing Ethical Explanations of Large Language Models through Iterative Symbolic Refinement",
    "abstract": "An increasing amount of research in Natural Language Inference (NLI) focuses on the application and evaluation of Large Language Models (LLMs) and their reasoning capabilities. Despite their success, however, LLMs are still prone to factual errors and inconsistencies in their explanations, offering limited control and interpretability for inference in complex domains. In this paper, we focus on ethical NLI, investigating how hybrid neuro-symbolic techniques can enhance the logical validity and alignment of ethical explanations produced by LLMs. Specifically, we present an abductive-deductive framework named Logic-Explainer, which integrates LLMs with an external backward-chaining solver to refine step-wise natural language explanations and jointly verify their correctness, reduce incompleteness and minimise redundancy. An extensive empirical analysis demonstrates that Logic-Explainer can improve explanations generated via in-context learning methods and Chain-of-Thought (CoT) on challe",
    "link": "https://arxiv.org/abs/2402.00745",
    "context": "Title: Enhancing Ethical Explanations of Large Language Models through Iterative Symbolic Refinement\nAbstract: An increasing amount of research in Natural Language Inference (NLI) focuses on the application and evaluation of Large Language Models (LLMs) and their reasoning capabilities. Despite their success, however, LLMs are still prone to factual errors and inconsistencies in their explanations, offering limited control and interpretability for inference in complex domains. In this paper, we focus on ethical NLI, investigating how hybrid neuro-symbolic techniques can enhance the logical validity and alignment of ethical explanations produced by LLMs. Specifically, we present an abductive-deductive framework named Logic-Explainer, which integrates LLMs with an external backward-chaining solver to refine step-wise natural language explanations and jointly verify their correctness, reduce incompleteness and minimise redundancy. An extensive empirical analysis demonstrates that Logic-Explainer can improve explanations generated via in-context learning methods and Chain-of-Thought (CoT) on challe",
    "path": "papers/24/02/2402.00745.json",
    "total_tokens": 916,
    "translated_title": "通过迭代符号细化提升大型语言模型的伦理解释能力",
    "translated_abstract": "越来越多关于自然语言推理（NLI）的研究集中在大型语言模型（LLMs）及其推理能力的应用和评估上。尽管取得了成功，LLMs仍然容易出现事实错误和解释上的不一致，限制了复杂领域推理的控制性和可解释性。本文关注伦理 NLI，研究混合神经符号技术如何提升LLMs产生的伦理解释的逻辑有效性和一致性。具体地，我们提出了一个名为Logic-Explainer的归纳-演绎框架，将LLMs与外部的逆向推理求解器结合起来，逐步完善自然语言解释，并联合验证其正确性、减少不完整性和冗余。通过大量的实证分析表明，Logic-Explainer可以改进通过环境学习方法和Chain-of-Thought（CoT）生成的解释。",
    "tldr": "本论文提出了一个名为Logic-Explainer的框架，通过迭代符号细化方法增强大型语言模型（LLMs）在伦理NLI中的解释能力。通过集成外部的逆向推理求解器，该框架可以逐步完善自然语言解释，并验证其正确性，减少不完整性和冗余。",
    "en_tdlr": "This paper presents a framework called Logic-Explainer that enhances the explanatory capabilities of Large Language Models (LLMs) in ethical Natural Language Inference (NLI) through iterative symbolic refinement. By integrating an external backward-chaining solver, this framework refines natural language explanations step by step, verifying their correctness and reducing incompleteness and redundancy."
}