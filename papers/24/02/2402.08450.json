{
    "title": "Subgraphormer: Unifying Subgraph GNNs and Graph Transformers via Graph Products",
    "abstract": "In the realm of Graph Neural Networks (GNNs), two exciting research directions have recently emerged: Subgraph GNNs and Graph Transformers. In this paper, we propose an architecture that integrates both approaches, dubbed Subgraphormer, which combines the enhanced expressive power, message-passing mechanisms, and aggregation schemes from Subgraph GNNs with attention and positional encodings, arguably the most important components in Graph Transformers. Our method is based on an intriguing new connection we reveal between Subgraph GNNs and product graphs, suggesting that Subgraph GNNs can be formulated as Message Passing Neural Networks (MPNNs) operating on a product of the graph with itself. We use this formulation to design our architecture: first, we devise an attention mechanism based on the connectivity of the product graph. Following this, we propose a novel and efficient positional encoding scheme for Subgraph GNNs, which we derive as a positional encoding for the product graph. ",
    "link": "https://arxiv.org/abs/2402.08450",
    "context": "Title: Subgraphormer: Unifying Subgraph GNNs and Graph Transformers via Graph Products\nAbstract: In the realm of Graph Neural Networks (GNNs), two exciting research directions have recently emerged: Subgraph GNNs and Graph Transformers. In this paper, we propose an architecture that integrates both approaches, dubbed Subgraphormer, which combines the enhanced expressive power, message-passing mechanisms, and aggregation schemes from Subgraph GNNs with attention and positional encodings, arguably the most important components in Graph Transformers. Our method is based on an intriguing new connection we reveal between Subgraph GNNs and product graphs, suggesting that Subgraph GNNs can be formulated as Message Passing Neural Networks (MPNNs) operating on a product of the graph with itself. We use this formulation to design our architecture: first, we devise an attention mechanism based on the connectivity of the product graph. Following this, we propose a novel and efficient positional encoding scheme for Subgraph GNNs, which we derive as a positional encoding for the product graph. ",
    "path": "papers/24/02/2402.08450.json",
    "total_tokens": 937,
    "translated_title": "Subgraphormer:通过图的乘积将子图GNN和图变换器统一起来",
    "translated_abstract": "在图神经网络(GNNs)领域中，最近出现了两个令人兴奋的研究方向：子图GNN和图变换器。本文提出了一种将两种方法结合起来的架构，称为Subgraphormer，它将子图GNNs的增强表达能力、信息传递机制和聚合方案与图变换器中最重要的注意力和位置编码相结合。我们的方法基于我们揭示的子图GNNs与乘积图之间的新连接，这表明子图GNNs可以被形式化为在图的乘积上操作的消息传递神经网络(MPNNs)。我们使用这个公式来设计我们的架构：首先，我们基于乘积图的连接性设计了一个注意力机制。接着，我们提出了一种新颖高效的子图GNNs位置编码方案，将其推导为乘积图的位置编码。",
    "tldr": "这篇论文提出了一种名为Subgraphormer的架构，通过将子图GNNs和图变换器结合起来，综合了子图GNNs的表达能力和消息传递机制以及图变换器的注意力和位置编码。基于子图GNNs与图的乘积之间的新连接，该方法设计了乘积图上的注意力机制和子图GNNs位置编码方案。",
    "en_tdlr": "This paper presents an architecture called Subgraphormer that integrates Subgraph GNNs and Graph Transformers, combining the expressive power and message-passing mechanisms of Subgraph GNNs with the attention and positional encodings of Graph Transformers. The method utilizes a new connection between Subgraph GNNs and product graphs and designs an attention mechanism and positional encoding scheme based on the product graph."
}