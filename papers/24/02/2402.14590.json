{
    "title": "Scaling Up LLM Reviews for Google Ads Content Moderation",
    "abstract": "arXiv:2402.14590v1 Announce Type: cross  Abstract: Large language models (LLMs) are powerful tools for content moderation, but their inference costs and latency make them prohibitive for casual use on large datasets, such as the Google Ads repository. This study proposes a method for scaling up LLM reviews for content moderation in Google Ads. First, we use heuristics to select candidates via filtering and duplicate removal, and create clusters of ads for which we select one representative ad per cluster. We then use LLMs to review only the representative ads. Finally, we propagate the LLM decisions for the representative ads back to their clusters. This method reduces the number of reviews by more than 3 orders of magnitude while achieving a 2x recall compared to a baseline non-LLM model. The success of this approach is a strong function of the representations used in clustering and label propagation; we found that cross-modal similarity representations yield better results than uni-m",
    "link": "https://arxiv.org/abs/2402.14590",
    "context": "Title: Scaling Up LLM Reviews for Google Ads Content Moderation\nAbstract: arXiv:2402.14590v1 Announce Type: cross  Abstract: Large language models (LLMs) are powerful tools for content moderation, but their inference costs and latency make them prohibitive for casual use on large datasets, such as the Google Ads repository. This study proposes a method for scaling up LLM reviews for content moderation in Google Ads. First, we use heuristics to select candidates via filtering and duplicate removal, and create clusters of ads for which we select one representative ad per cluster. We then use LLMs to review only the representative ads. Finally, we propagate the LLM decisions for the representative ads back to their clusters. This method reduces the number of reviews by more than 3 orders of magnitude while achieving a 2x recall compared to a baseline non-LLM model. The success of this approach is a strong function of the representations used in clustering and label propagation; we found that cross-modal similarity representations yield better results than uni-m",
    "path": "papers/24/02/2402.14590.json",
    "total_tokens": 877,
    "translated_title": "扩展LLM审核以进行Google广告内容管理",
    "translated_abstract": "大型语言模型（LLMs）是内容管理的强大工具，但它们的推理成本和延迟使它们在大型数据集（如Google Ads存储库）上的临时使用成本过高。本研究提出了一种方法，用于扩展LLM审核以在Google Ads中进行内容管理。首先，我们使用启发式方法通过过滤和重复项删除来选择候选项，并为此创建广告群集，我们选择每个群集的一个代表性广告。然后，我们使用LLMs仅审核代表性广告。最后，我们将代表性广告的LLM决策传播回它们的群集。该方法将审核数目减少了3个数量级以上，同时与基线非LLM模型相比实现了2倍的召回率。该方法的成功在很大程度上取决于聚类和标签传播中使用的表示; 我们发现交叉模态相似性表示产生比单一模态更好的结果。",
    "tldr": "本研究提出了一种用于在Google广告中进行内容管理的方法，通过使用LLMs审核代表性广告并将决策传播回其群集，将审核数目减少了3个数量级以上，同时实现了2倍的召回率。",
    "en_tdlr": "This study proposes a method for scaling up LLM reviews for content moderation in Google Ads, which reduces the number of reviews by more than 3 orders of magnitude while achieving a 2x recall compared to a baseline non-LLM model by reviewing representative ads using LLMs and propagating decisions back to their clusters."
}