{
    "title": "Denoising Diffusion via Image-Based Rendering",
    "abstract": "Generating 3D scenes is a challenging open problem, which requires synthesizing plausible content that is fully consistent in 3D space. While recent methods such as neural radiance fields excel at view synthesis and 3D reconstruction, they cannot synthesize plausible details in unobserved regions since they lack a generative capability. Conversely, existing generative methods are typically not capable of reconstructing detailed, large-scale scenes in the wild, as they use limited-capacity 3D scene representations, require aligned camera poses, or rely on additional regularizers. In this work, we introduce the first diffusion model able to perform fast, detailed reconstruction and generation of real-world 3D scenes. To achieve this, we make three contributions. First, we introduce a new neural scene representation, IB-planes, that can efficiently and accurately represent large 3D scenes, dynamically allocating more capacity as needed to capture details visible in each image. Second, we ",
    "link": "https://arxiv.org/abs/2402.03445",
    "context": "Title: Denoising Diffusion via Image-Based Rendering\nAbstract: Generating 3D scenes is a challenging open problem, which requires synthesizing plausible content that is fully consistent in 3D space. While recent methods such as neural radiance fields excel at view synthesis and 3D reconstruction, they cannot synthesize plausible details in unobserved regions since they lack a generative capability. Conversely, existing generative methods are typically not capable of reconstructing detailed, large-scale scenes in the wild, as they use limited-capacity 3D scene representations, require aligned camera poses, or rely on additional regularizers. In this work, we introduce the first diffusion model able to perform fast, detailed reconstruction and generation of real-world 3D scenes. To achieve this, we make three contributions. First, we introduce a new neural scene representation, IB-planes, that can efficiently and accurately represent large 3D scenes, dynamically allocating more capacity as needed to capture details visible in each image. Second, we ",
    "path": "papers/24/02/2402.03445.json",
    "total_tokens": 883,
    "translated_title": "基于图像渲染的去噪扩散",
    "translated_abstract": "生成3D场景是一个具有挑战性的问题，需要合成在三维空间中完全一致的可信内容。最近的方法如神经辐射场在视图合成和3D重构方面表现出色，但由于缺乏生成能力，它们无法合成未观察区域中的可信细节。相反，现有的生成方法通常不能在野外重构具有详细的大规模场景，因为它们使用容量有限的3D场景表示，需要对齐的相机姿态或依赖额外的正则化器。在这项工作中，我们引入了首个能够快速进行详细重构和生成真实世界3D场景的扩散模型。为了实现这一点，我们提出了三个贡献。首先，我们引入了一种新的神经场景表示——IB-planes，可以高效准确地表示大规模3D场景，并根据每张图像中可见的细节动态分配更多容量。其次，我们提供了一种新的扩散建模方法。",
    "tldr": "该论文介绍了一个新的基于图像渲染的去噪扩散模型，可以快速重构和生成真实世界3D场景，通过引入新的神经场景表示方法和扩散建模方法来实现。",
    "en_tdlr": "This paper introduces a new denoising diffusion model based on image-based rendering, which can perform fast reconstruction and generation of real-world 3D scenes by introducing a new neural scene representation and diffusion modeling approach."
}