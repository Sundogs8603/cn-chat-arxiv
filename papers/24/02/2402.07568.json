{
    "title": "Weisfeiler-Leman at the margin: When more expressivity matters",
    "abstract": "The Weisfeiler-Leman algorithm ($1$-WL) is a well-studied heuristic for the graph isomorphism problem. Recently, the algorithm has played a prominent role in understanding the expressive power of message-passing graph neural networks (MPNNs) and being effective as a graph kernel. Despite its success, $1$-WL faces challenges in distinguishing non-isomorphic graphs, leading to the development of more expressive MPNN and kernel architectures. However, the relationship between enhanced expressivity and improved generalization performance remains unclear. Here, we show that an architecture's expressivity offers limited insights into its generalization performance when viewed through graph isomorphism. Moreover, we focus on augmenting $1$-WL and MPNNs with subgraph information and employ classical margin theory to investigate the conditions under which an architecture's increased expressivity aligns with improved generalization performance. In addition, we show that gradient flow pushes the ",
    "link": "https://arxiv.org/abs/2402.07568",
    "context": "Title: Weisfeiler-Leman at the margin: When more expressivity matters\nAbstract: The Weisfeiler-Leman algorithm ($1$-WL) is a well-studied heuristic for the graph isomorphism problem. Recently, the algorithm has played a prominent role in understanding the expressive power of message-passing graph neural networks (MPNNs) and being effective as a graph kernel. Despite its success, $1$-WL faces challenges in distinguishing non-isomorphic graphs, leading to the development of more expressive MPNN and kernel architectures. However, the relationship between enhanced expressivity and improved generalization performance remains unclear. Here, we show that an architecture's expressivity offers limited insights into its generalization performance when viewed through graph isomorphism. Moreover, we focus on augmenting $1$-WL and MPNNs with subgraph information and employ classical margin theory to investigate the conditions under which an architecture's increased expressivity aligns with improved generalization performance. In addition, we show that gradient flow pushes the ",
    "path": "papers/24/02/2402.07568.json",
    "total_tokens": 994,
    "translated_title": "Weisfeiler-Leman在边缘条件下的更高表达力的重要性",
    "translated_abstract": "Weisfeiler-Leman算法（1-WL）是一个被广泛研究的用于图同构问题的启发式算法。最近，该算法在理解传递消息的图神经网络（MPNNs）的表达能力以及作为图核函数方面发挥了重要作用。尽管取得了成功，但1-WL在区分非同构图方面面临挑战，从而导致了更具表达力的MPNN和核架构的发展。然而，增强的表达能力和改进的泛化性能之间的关系仍不清楚。在本文中，我们展示了当通过图同构来观察时，架构的表达能力在解释其泛化性能方面具有有限的洞察力。此外，我们着重在1-WL和MPNN中引入子图信息，并运用经典的边缘理论来研究架构的增强表达能力与改进的泛化性能之间的条件。此外，我们还展示了梯度流如何推动模型学习更丰富的表达能力。",
    "tldr": "研究探讨了1-WL算法在图同构问题中的表达能力和泛化性能之间的关系，发现增强的表达能力对提高泛化性能并不总是有效。此外，通过引入子图信息和经典的边缘理论，探索了更高表达力与改进泛化性能的条件。梯度流也被证明可以促进模型学习更丰富的表达能力。",
    "en_tdlr": "This study investigates the relationship between expressivity and generalization performance of the 1-WL algorithm in the graph isomorphism problem, revealing that enhanced expressivity does not always result in improved generalization. Additionally, by incorporating subgraph information and using classical margin theory, conditions for improved generalization with enhanced expressivity are explored. The influence of gradient flow on learning richer expressivity is also examined."
}