{
    "title": "Maximizing Data Efficiency for Cross-Lingual TTS Adaptation by Self-Supervised Representation Mixing and Embedding Initialization",
    "abstract": "This paper presents an effective transfer learning framework for language adaptation in text-to-speech systems, with a focus on achieving language adaptation using minimal labeled and unlabeled data. While many works focus on reducing the usage of labeled data, very few consider minimizing the usage of unlabeled data. By utilizing self-supervised features in the pretraining stage, replacing the noisy portion of pseudo labels with these features during fine-tuning, and incorporating an embedding initialization trick, our method leverages more information from unlabeled data compared to conventional approaches. Experimental results show that our framework is able to synthesize intelligible speech in unseen languages with only 4 utterances of labeled data and 15 minutes of unlabeled data. Our methodology continues to surpass conventional techniques, even when a greater volume of data is accessible. These findings highlight the potential of our data-efficient language adaptation framework.",
    "link": "https://arxiv.org/abs/2402.01692",
    "context": "Title: Maximizing Data Efficiency for Cross-Lingual TTS Adaptation by Self-Supervised Representation Mixing and Embedding Initialization\nAbstract: This paper presents an effective transfer learning framework for language adaptation in text-to-speech systems, with a focus on achieving language adaptation using minimal labeled and unlabeled data. While many works focus on reducing the usage of labeled data, very few consider minimizing the usage of unlabeled data. By utilizing self-supervised features in the pretraining stage, replacing the noisy portion of pseudo labels with these features during fine-tuning, and incorporating an embedding initialization trick, our method leverages more information from unlabeled data compared to conventional approaches. Experimental results show that our framework is able to synthesize intelligible speech in unseen languages with only 4 utterances of labeled data and 15 minutes of unlabeled data. Our methodology continues to surpass conventional techniques, even when a greater volume of data is accessible. These findings highlight the potential of our data-efficient language adaptation framework.",
    "path": "papers/24/02/2402.01692.json",
    "total_tokens": 1017,
    "translated_title": "通过自监督表示混合和嵌入初始化实现跨语言TTS自适应的最大数据效率",
    "translated_abstract": "本文提出了一种有效的转移学习框架，用于文本到语音系统中的语言自适应，重点是使用最少的标记和未标记数据实现语言自适应。虽然许多工作侧重于减少标记数据的使用，但很少有人考虑尽量减少未标记数据的使用。通过在预训练阶段利用自监督特征，替换细调期间伪标签中的噪声部分，并结合嵌入初始化技巧，我们的方法与传统方法相比利用了更多未标记数据的信息。实验结果表明，我们的框架能够使用仅4个标记数据和15分钟未标记数据合成可理解的未知语言语音。我们的方法即使在更多数据可用的情况下，仍然超过了传统技术。这些发现突显了我们的高效语言自适应框架的潜力。",
    "tldr": "本文提出了一种有效的转移学习框架，用于通过最少的标记和未标记数据实现语言自适应。该方法使用自监督特征进行预训练，在细调期间替换伪标签噪声部分，并结合嵌入初始化技巧，有效利用未标记数据的信息。实验证明，即使在仅有很少的数据情况下，该框架也能合成可理解的未知语言语音，并超越传统技术。这一研究结果展示了该高效语言自适应框架的潜力。"
}