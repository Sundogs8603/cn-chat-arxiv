{
    "title": "MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement Learning for Discrete Prompt Optimization",
    "abstract": "arXiv:2402.11711v1 Announce Type: new  Abstract: RL-based techniques can be used to search for prompts that when fed into a target language model maximize a set of user-specified reward functions. However, in many target applications, the natural reward functions are in tension with one another -- for example, content preservation vs. style matching in style transfer tasks. Current techniques focus on maximizing the average of reward functions, which does not necessarily lead to prompts that achieve balance across rewards -- an issue that has been well-studied in the multi-objective and robust optimization literature. In this paper, we adapt several techniques for multi-objective optimization to RL-based discrete prompt optimization -- two that consider volume of the Pareto reward surface, and another that chooses an update direction that benefits all rewards simultaneously. We conduct an empirical analysis of these methods on two NLP tasks: style transfer and machine translation, each",
    "link": "https://arxiv.org/abs/2402.11711",
    "context": "Title: MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement Learning for Discrete Prompt Optimization\nAbstract: arXiv:2402.11711v1 Announce Type: new  Abstract: RL-based techniques can be used to search for prompts that when fed into a target language model maximize a set of user-specified reward functions. However, in many target applications, the natural reward functions are in tension with one another -- for example, content preservation vs. style matching in style transfer tasks. Current techniques focus on maximizing the average of reward functions, which does not necessarily lead to prompts that achieve balance across rewards -- an issue that has been well-studied in the multi-objective and robust optimization literature. In this paper, we adapt several techniques for multi-objective optimization to RL-based discrete prompt optimization -- two that consider volume of the Pareto reward surface, and another that chooses an update direction that benefits all rewards simultaneously. We conduct an empirical analysis of these methods on two NLP tasks: style transfer and machine translation, each",
    "path": "papers/24/02/2402.11711.json",
    "total_tokens": 831,
    "translated_title": "MORL-Prompt: 离散提示优化的多目标强化学习的实证分析",
    "translated_abstract": "基于RL的技术可以用于搜索提示，将其输入目标语言模型以最大化一组用户指定的奖励函数。然而，在许多目标应用中，自然奖励函数彼此之间存在紧张关系--例如，在风格转移任务中，内容保留与风格匹配之间存在矛盾。当前技术侧重于最大化奖励函数的平均值，这未必会导致取得各种奖励平衡的提示--这个问题在多目标和鲁棒优化文献中得到了深入研究。本文将几种多目标优化技术调整为基于RL的离散提示优化--其中有两种考虑帕累托奖励面积的方法，另外一种选择有益于所有奖励的更新方向。我们在两个NLP任务上对这些方法进行了实证分析：风格转移和机器翻译。",
    "tldr": "本研究将多目标优化技术应用于基于强化学习的离散提示优化，为解决奖励平衡问题提供了新视角。"
}