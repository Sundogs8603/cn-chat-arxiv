{
    "title": "LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A Vision Paper",
    "abstract": "arXiv:2402.15727v1 Announce Type: cross  Abstract: Jailbreaking is an emerging adversarial attack that bypasses the safety alignment deployed in off-the-shelf large language models (LLMs). A considerable amount of research exists proposing more effective jailbreak attacks, including the recent Greedy Coordinate Gradient (GCG) attack, jailbreak template-based attacks such as using \"Do-Anything-Now\" (DAN), and multilingual jailbreak. In contrast, the defensive side has been relatively less explored. This paper proposes a lightweight yet practical defense called SELFDEFEND, which can defend against all existing jailbreak attacks with minimal delay for jailbreak prompts and negligible delay for normal user prompts. Our key insight is that regardless of the kind of jailbreak strategies employed, they eventually need to include a harmful prompt (e.g., \"how to make a bomb\") in the prompt sent to LLMs, and we found that existing LLMs can effectively recognize such harmful prompts that violate ",
    "link": "https://arxiv.org/abs/2402.15727",
    "context": "Title: LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A Vision Paper\nAbstract: arXiv:2402.15727v1 Announce Type: cross  Abstract: Jailbreaking is an emerging adversarial attack that bypasses the safety alignment deployed in off-the-shelf large language models (LLMs). A considerable amount of research exists proposing more effective jailbreak attacks, including the recent Greedy Coordinate Gradient (GCG) attack, jailbreak template-based attacks such as using \"Do-Anything-Now\" (DAN), and multilingual jailbreak. In contrast, the defensive side has been relatively less explored. This paper proposes a lightweight yet practical defense called SELFDEFEND, which can defend against all existing jailbreak attacks with minimal delay for jailbreak prompts and negligible delay for normal user prompts. Our key insight is that regardless of the kind of jailbreak strategies employed, they eventually need to include a harmful prompt (e.g., \"how to make a bomb\") in the prompt sent to LLMs, and we found that existing LLMs can effectively recognize such harmful prompts that violate ",
    "path": "papers/24/02/2402.15727.json",
    "total_tokens": 894,
    "translated_title": "LLMs能够以实用的方式自我防御越狱攻击：一份展望文章",
    "translated_abstract": "越狱是一种新兴的敌对攻击，可以绕过现有的大型语言模型（LLMs）中部署的安全机制。已有大量研究提出了更有效的越狱攻击方法，包括最近的贪婪坐标梯度（GCG）攻击、基于越狱模板的攻击，例如使用“Do-Anything-Now”（DAN），以及多语言越狱。相比之下，防御方面的研究相对较少。本文提出了一种轻量而实用的防御方法，称为SELFDEFEND，可以抵御所有现有的越狱攻击，在越狱提示方面几乎没有延迟，对于正常用户提示也只有微不足道的延迟。我们的主要见解是，无论使用何种越狱策略，最终都需要在发送给LLMs的提示中包含有害提示（例如“如何制造炸弹”），我们发现现有的LLMs可以有效识别违反安全规则的有害提示。",
    "tldr": "本文提出了一种名为SELFDEFEND的轻量级实用防御方法，可以在最小延迟下抵御所有现有的越狱攻击。",
    "en_tdlr": "This paper introduces a lightweight and practical defense method called SELFDEFEND, which can defend against all existing jailbreak attacks with minimal delay."
}