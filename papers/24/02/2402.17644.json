{
    "title": "Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data",
    "abstract": "arXiv:2402.17644v1 Announce Type: cross  Abstract: Quantitative reasoning is a critical skill to analyze data, yet the assessment of such ability remains limited. To address this gap, we introduce the Quantitative Reasoning with Data (QRData) benchmark, aiming to evaluate Large Language Models' capability in statistical and causal reasoning with real-world data. The benchmark comprises a carefully constructed dataset of 411 questions accompanied by data sheets from textbooks, online learning materials, and academic papers. To compare models' quantitative reasoning abilities on data and text, we enrich the benchmark with an auxiliary set of 290 text-only questions, namely QRText. We evaluate natural language reasoning, program-based reasoning, and agent reasoning methods including Chain-of-Thought, Program-of-Thoughts, ReAct, and code interpreter assistants on diverse models. The strongest model GPT-4 achieves an accuracy of 58%, which has a large room for improvement. Among open-source",
    "link": "https://arxiv.org/abs/2402.17644",
    "context": "Title: Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data\nAbstract: arXiv:2402.17644v1 Announce Type: cross  Abstract: Quantitative reasoning is a critical skill to analyze data, yet the assessment of such ability remains limited. To address this gap, we introduce the Quantitative Reasoning with Data (QRData) benchmark, aiming to evaluate Large Language Models' capability in statistical and causal reasoning with real-world data. The benchmark comprises a carefully constructed dataset of 411 questions accompanied by data sheets from textbooks, online learning materials, and academic papers. To compare models' quantitative reasoning abilities on data and text, we enrich the benchmark with an auxiliary set of 290 text-only questions, namely QRText. We evaluate natural language reasoning, program-based reasoning, and agent reasoning methods including Chain-of-Thought, Program-of-Thoughts, ReAct, and code interpreter assistants on diverse models. The strongest model GPT-4 achieves an accuracy of 58%, which has a large room for improvement. Among open-source",
    "path": "papers/24/02/2402.17644.json",
    "total_tokens": 898,
    "translated_title": "LLMs是否具备基于数据的统计和因果推理能力？用数据对先进的定量推理进行基准测试",
    "translated_abstract": "量化推理是分析数据的关键技能，然而对这种能力的评估仍然有限。为了填补这一空白，我们引入了Quantitative Reasoning with Data（QRData）基准测试，旨在评估大型语言模型在统计和因果推理方面与现实世界数据的能力。该基准测试包括一个精心构建的包含来自教科书、在线学习材料和学术论文的数据表的411个问题的数据集。为了比较模型在数据和文本上的定量推理能力，我们还在基准测试中添加了一个包含290个仅文本问题的辅助数据集，即QRText。我们评估了自然语言推理、基于程序推理和代理推理方法，包括Chain-of-Thought、Program-of-Thoughts、ReAct和代码解释器辅助等在各种模型上的表现。最强的模型GPT-4的准确率达到了58％，但仍有很大的改进空间。",
    "tldr": "本研究引入了QRData基准测试，评估了大型语言模型在统计和因果推理方面的能力，结果显示最强模型GPT-4在该测试中准确率为58％，存在改进空间。",
    "en_tdlr": "This study introduces the QRData benchmark to assess the capability of Large Language Models in statistical and causal reasoning with real-world data, where the top model GPT-4 achieves an accuracy of 58%, indicating room for improvement."
}