{
    "title": "How Free is Parameter-Free Stochastic Optimization?",
    "abstract": "We study the problem of parameter-free stochastic optimization, inquiring whether, and under what conditions, do fully parameter-free methods exist: these are methods that achieve convergence rates competitive with optimally tuned methods, without requiring significant knowledge of the true problem parameters. Existing parameter-free methods can only be considered ``partially'' parameter-free, as they require some non-trivial knowledge of the true problem parameters, such as a bound on the stochastic gradient norms, a bound on the distance to a minimizer, etc. In the non-convex setting, we demonstrate that a simple hyperparameter search technique results in a fully parameter-free method that outperforms more sophisticated state-of-the-art algorithms. We also provide a similar result in the convex setting with access to noisy function values under mild noise assumptions. Finally, assuming only access to stochastic gradients, we establish a lower bound that renders fully parameter-free s",
    "link": "https://arxiv.org/abs/2402.03126",
    "context": "Title: How Free is Parameter-Free Stochastic Optimization?\nAbstract: We study the problem of parameter-free stochastic optimization, inquiring whether, and under what conditions, do fully parameter-free methods exist: these are methods that achieve convergence rates competitive with optimally tuned methods, without requiring significant knowledge of the true problem parameters. Existing parameter-free methods can only be considered ``partially'' parameter-free, as they require some non-trivial knowledge of the true problem parameters, such as a bound on the stochastic gradient norms, a bound on the distance to a minimizer, etc. In the non-convex setting, we demonstrate that a simple hyperparameter search technique results in a fully parameter-free method that outperforms more sophisticated state-of-the-art algorithms. We also provide a similar result in the convex setting with access to noisy function values under mild noise assumptions. Finally, assuming only access to stochastic gradients, we establish a lower bound that renders fully parameter-free s",
    "path": "papers/24/02/2402.03126.json",
    "total_tokens": 874,
    "translated_title": "无参随机优化的自由度有多高？",
    "translated_abstract": "我们研究了无参随机优化的问题，探讨了在什么条件下可以存在完全无参的方法：这些方法可以达到与最优调参方法相竞争的收敛速度，而不需要对真实问题参数有很多知识。现有的无参方法只能被视为“部分”无参，因为它们需要对真实问题参数有一些非平凡的知识，比如随机梯度范数的上界、到最小值的距离的上界等。在非凸设置中，我们证明了一个简单的超参数搜索技术可以得到一个完全无参的方法，在性能上超过了更复杂的先进算法。在具有噪声函数值的凸设置下，在较小的噪声假设下，我们也提供了类似的结果。最后，假设只能访问随机梯度，我们建立了一个下界，使得完全无参的方法无法实现。",
    "tldr": "这个论文研究了无参随机优化的问题，提出了一种完全无参的方法，通过简单的超参数搜索技术在非凸和凸设置下都能取得优于先进算法的性能。同时，论文还建立了一个下界，指出完全无参的方法在某些情况下无法实现。",
    "en_tdlr": "This paper investigates the problem of parameter-free stochastic optimization and introduces a fully parameter-free method that outperforms state-of-the-art algorithms using a simple hyperparameter search technique in both non-convex and convex settings. Additionally, a lower bound is established to show the limitations of fully parameter-free methods in certain cases."
}