{
    "title": "REPrune: Channel Pruning via Kernel Representative Selection",
    "abstract": "arXiv:2402.17862v1 Announce Type: cross  Abstract: Channel pruning is widely accepted to accelerate modern convolutional neural networks (CNNs). The resulting pruned model benefits from its immediate deployment on general-purpose software and hardware resources. However, its large pruning granularity, specifically at the unit of a convolution filter, often leads to undesirable accuracy drops due to the inflexibility of deciding how and where to introduce sparsity to the CNNs. In this paper, we propose REPrune, a novel channel pruning technique that emulates kernel pruning, fully exploiting the finer but structured granularity. REPrune identifies similar kernels within each channel using agglomerative clustering. Then, it selects filters that maximize the incorporation of kernel representatives while optimizing the maximum cluster coverage problem. By integrating with a simultaneous training-pruning paradigm, REPrune promotes efficient, progressive pruning throughout training CNNs, avoi",
    "link": "https://arxiv.org/abs/2402.17862",
    "context": "Title: REPrune: Channel Pruning via Kernel Representative Selection\nAbstract: arXiv:2402.17862v1 Announce Type: cross  Abstract: Channel pruning is widely accepted to accelerate modern convolutional neural networks (CNNs). The resulting pruned model benefits from its immediate deployment on general-purpose software and hardware resources. However, its large pruning granularity, specifically at the unit of a convolution filter, often leads to undesirable accuracy drops due to the inflexibility of deciding how and where to introduce sparsity to the CNNs. In this paper, we propose REPrune, a novel channel pruning technique that emulates kernel pruning, fully exploiting the finer but structured granularity. REPrune identifies similar kernels within each channel using agglomerative clustering. Then, it selects filters that maximize the incorporation of kernel representatives while optimizing the maximum cluster coverage problem. By integrating with a simultaneous training-pruning paradigm, REPrune promotes efficient, progressive pruning throughout training CNNs, avoi",
    "path": "papers/24/02/2402.17862.json",
    "total_tokens": 893,
    "translated_title": "REPrune：通过核代表选择进行通道修剪",
    "translated_abstract": "通道修剪被广泛认可为加速现代卷积神经网络（CNNs）的方法。所得到的修剪模型可以立即部署在通用软件和硬件资源上。然而，由于在卷积滤波器这个单元上的大修剪粒度，通常会导致不希望的准确性下降，这是由于在CNNs中决定如何以及在何处引入稀疏性的灵活性不足。在本文中，我们提出了REPrune，一种新颖的通道修剪技术，模拟了核修剪，充分利用了更细但有结构的粒度。REPrune使用凝聚聚类识别每个通道内的相似核。然后，它选择最大化包含核代表的滤波器，同时优化最大聚类覆盖问题。通过与同时训练-修剪范式相结合，REPrune促进了在训练CNNs期间的高效、渐进式修剪，避免了在训练期间的误差传播。",
    "tldr": "REPrune是一种新颖的通道修剪技术，通过模拟核修剪，并结合聚类和滤波器选择，实现了更精细但结构化的修剪粒度，促进了在训练CNNs期间的高效、渐进式修剪。",
    "en_tdlr": "REPrune is a novel channel pruning technique that emulates kernel pruning, fully exploiting the finer but structured granularity, and promotes efficient, progressive pruning throughout training CNNs by integrating clustering and filter selection."
}