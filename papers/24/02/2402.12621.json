{
    "title": "Reflect-RL: Two-Player Online RL Fine-Tuning for LMs",
    "abstract": "arXiv:2402.12621v1 Announce Type: cross  Abstract: As language models (LMs) demonstrate their capabilities in various fields, their application to tasks requiring multi-round interactions has become increasingly popular. These tasks usually have complex dynamics, so supervised fine-tuning (SFT) on a limited offline dataset does not yield good performance. However, only a few works attempted to directly train the LMs within interactive decision-making environments. We aim to create an effective mechanism to fine-tune LMs with online reinforcement learning (RL) in these environments. We propose Reflect-RL, a two-player system to fine-tune an LM using online RL, where a frozen reflection model assists the policy model. To generate data for the warm-up SFT stage, we use negative example generation to enhance the error-correction ability of the reflection model. Furthermore, we designed single-prompt action enumeration and applied curriculum learning to allow the policy model to learn more ",
    "link": "https://arxiv.org/abs/2402.12621",
    "context": "Title: Reflect-RL: Two-Player Online RL Fine-Tuning for LMs\nAbstract: arXiv:2402.12621v1 Announce Type: cross  Abstract: As language models (LMs) demonstrate their capabilities in various fields, their application to tasks requiring multi-round interactions has become increasingly popular. These tasks usually have complex dynamics, so supervised fine-tuning (SFT) on a limited offline dataset does not yield good performance. However, only a few works attempted to directly train the LMs within interactive decision-making environments. We aim to create an effective mechanism to fine-tune LMs with online reinforcement learning (RL) in these environments. We propose Reflect-RL, a two-player system to fine-tune an LM using online RL, where a frozen reflection model assists the policy model. To generate data for the warm-up SFT stage, we use negative example generation to enhance the error-correction ability of the reflection model. Furthermore, we designed single-prompt action enumeration and applied curriculum learning to allow the policy model to learn more ",
    "path": "papers/24/02/2402.12621.json",
    "total_tokens": 877,
    "translated_title": "Reflect-RL：两个玩家在线RL微调语言模型",
    "translated_abstract": "随着语言模型在各个领域展示其能力，将它们应用于需要多轮交互的任务变得越来越受欢迎。这些任务通常具有复杂的动态性，因此仅在有限的离线数据集上进行监督微调（SFT）无法取得良好性能。然而，只有少数研究尝试在交互式决策制定环境内直接对LM进行训练。我们旨在创建一个在这些环境中使用在线强化学习（RL）对LM进行微调的有效机制。我们提出了Reflect-RL，一个两个玩家的系统，使用在线RL对LM进行微调，在此过程中，一个冻结的反射模型辅助策略模型。为了为热身SFT阶段生成数据，我们使用负例生成来增强反射模型的纠错能力。此外，我们设计了单提示动作枚举，并应用了课程学习让策略模型学习更多。",
    "tldr": "提出Reflect-RL，使用在线强化学习来微调语言模型，在这过程中引入了反射模型协助，并采用了负例生成、单提示动作枚举和课程学习来提高性能。",
    "en_tdlr": "Propose Reflect-RL, which fine-tunes language models using online reinforcement learning with the assistance of a frozen reflection model and techniques like negative example generation, single-prompt action enumeration, and curriculum learning to enhance performance."
}