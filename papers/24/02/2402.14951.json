{
    "title": "In-Context Learning of a Linear Transformer Block: Benefits of the MLP Component and One-Step GD Initialization",
    "abstract": "arXiv:2402.14951v1 Announce Type: cross  Abstract: We study the \\emph{in-context learning} (ICL) ability of a \\emph{Linear Transformer Block} (LTB) that combines a linear attention component and a linear multi-layer perceptron (MLP) component. For ICL of linear regression with a Gaussian prior and a \\emph{non-zero mean}, we show that LTB can achieve nearly Bayes optimal ICL risk. In contrast, using only linear attention must incur an irreducible additive approximation error. Furthermore, we establish a correspondence between LTB and one-step gradient descent estimators with learnable initialization ($\\mathsf{GD}\\text{-}\\mathbf{\\beta}$), in the sense that every $\\mathsf{GD}\\text{-}\\mathbf{\\beta}$ estimator can be implemented by an LTB estimator and every optimal LTB estimator that minimizes the in-class ICL risk is effectively a $\\mathsf{GD}\\text{-}\\mathbf{\\beta}$ estimator. Finally, we show that $\\mathsf{GD}\\text{-}\\mathbf{\\beta}$ estimators can be efficiently optimized with gradient f",
    "link": "https://arxiv.org/abs/2402.14951",
    "context": "Title: In-Context Learning of a Linear Transformer Block: Benefits of the MLP Component and One-Step GD Initialization\nAbstract: arXiv:2402.14951v1 Announce Type: cross  Abstract: We study the \\emph{in-context learning} (ICL) ability of a \\emph{Linear Transformer Block} (LTB) that combines a linear attention component and a linear multi-layer perceptron (MLP) component. For ICL of linear regression with a Gaussian prior and a \\emph{non-zero mean}, we show that LTB can achieve nearly Bayes optimal ICL risk. In contrast, using only linear attention must incur an irreducible additive approximation error. Furthermore, we establish a correspondence between LTB and one-step gradient descent estimators with learnable initialization ($\\mathsf{GD}\\text{-}\\mathbf{\\beta}$), in the sense that every $\\mathsf{GD}\\text{-}\\mathbf{\\beta}$ estimator can be implemented by an LTB estimator and every optimal LTB estimator that minimizes the in-class ICL risk is effectively a $\\mathsf{GD}\\text{-}\\mathbf{\\beta}$ estimator. Finally, we show that $\\mathsf{GD}\\text{-}\\mathbf{\\beta}$ estimators can be efficiently optimized with gradient f",
    "path": "papers/24/02/2402.14951.json",
    "total_tokens": 977,
    "translated_title": "一个线性Transformer块的上下文学习：MLP组件和一步GD初始化的优势",
    "translated_abstract": "我们研究了结合线性注意力组件和线性多层感知器（MLP）组件的线性Transformer块（LTB）的上下文学习（ICL）能力。对于具有高斯先验和非零均值的线性回归的ICL，我们表明LTB可以实现几乎贝叶斯最优的ICL风险。相比之下，仅使用线性注意力必须产生不可避免的附加近似误差。此外，我们建立了LTB与具有可学习初始化的一步梯度下降估计器（$\\mathsf{GD}-\\mathbf{\\beta}$）之间的对应关系，从每个$\\mathsf{GD}-\\mathbf{\\beta}$估计器可以通过LTB估计器实现，到最小化类内ICL风险的每个最优LTB估计器实际上是一个$\\mathsf{GD}-\\mathbf{\\beta}$估计器。最后，我们表明$\\mathsf{GD}-\\mathbf{\\beta}$估计器可以通过梯度优化高效地优化。",
    "tldr": "该论文研究了结合线性注意力和线性MLP组件的线性Transformer块在上下文学习中的性能，证明了其在线性回归任务中几乎可以达到贝叶斯最优风险，并且与一步梯度下降估计器有对应关系。",
    "en_tdlr": "This paper explores the performance of a Linear Transformer Block combining linear attention and linear MLP components in in-context learning, demonstrating its near-Bayes optimal risk in linear regression tasks and establishing a correspondence with one-step gradient descent estimators."
}