{
    "title": "Reducing the Cost of Quantum Chemical Data By Backpropagating Through Density Functional Theory",
    "abstract": "Density Functional Theory (DFT) accurately predicts the quantum chemical properties of molecules, but scales as $O(N_{\\text{electrons}}^3)$. Sch\\\"utt et al. (2019) successfully approximate DFT 1000x faster with Neural Networks (NN). Arguably, the biggest problem one faces when scaling to larger molecules is the cost of DFT labels. For example, it took years to create the PCQ dataset (Nakata & Shimazaki, 2017) on which subsequent NNs are trained within a week. DFT labels molecules by minimizing energy $E(\\cdot )$ as a \"loss function.\" We bypass dataset creation by directly training NNs with $E(\\cdot )$ as a loss function. For comparison, Sch\\\"utt et al. (2019) spent 626 hours creating a dataset on which they trained their NN for 160h, for a total of 786h; our method achieves comparable performance within 31h.",
    "link": "https://arxiv.org/abs/2402.04030",
    "context": "Title: Reducing the Cost of Quantum Chemical Data By Backpropagating Through Density Functional Theory\nAbstract: Density Functional Theory (DFT) accurately predicts the quantum chemical properties of molecules, but scales as $O(N_{\\text{electrons}}^3)$. Sch\\\"utt et al. (2019) successfully approximate DFT 1000x faster with Neural Networks (NN). Arguably, the biggest problem one faces when scaling to larger molecules is the cost of DFT labels. For example, it took years to create the PCQ dataset (Nakata & Shimazaki, 2017) on which subsequent NNs are trained within a week. DFT labels molecules by minimizing energy $E(\\cdot )$ as a \"loss function.\" We bypass dataset creation by directly training NNs with $E(\\cdot )$ as a loss function. For comparison, Sch\\\"utt et al. (2019) spent 626 hours creating a dataset on which they trained their NN for 160h, for a total of 786h; our method achieves comparable performance within 31h.",
    "path": "papers/24/02/2402.04030.json",
    "total_tokens": 872,
    "translated_title": "通过反向传播通过密度泛函理论减少量子化学数据的成本",
    "translated_abstract": "密度泛函理论（DFT）准确预测分子的量子化学性质，但复杂度为 $O(N_{\\text{electrons}}^3)$。Sch\\\"utt等人（2019年）通过神经网络（NN）成功地以 1000 倍的速度近似DFT。在扩展到较大分子时面临的最大问题可能是DFT标签的成本。例如，创建PCQ数据集（Nakata＆Shimazaki，2017年）耗费了数年时间，而在一个星期内用于训练后续的NN。DFT通过最小化能量 $E(\\cdot )$ 作为“损失函数”来标记分子。我们通过直接将 $E(\\cdot )$ 作为损失函数来直接训练NN，避免了数据集的创建。相比之下，Sch\\\"utt等人（2019年）花费了626小时创建一个数据集，用该数据集训练他们的NN需要160小时，总共786小时；我们的方法在31小时内实现了可比较的性能。",
    "tldr": "使用神经网络以更快的速度近似密度泛函理论（DFT），通过直接训练NN来绕过创建数据集的问题，大大减少了量子化学数据的成本和时间。",
    "en_tdlr": "Reducing the cost and time of quantum chemical data by approximating Density Functional Theory (DFT) faster using Neural Networks (NN) and bypassing the need to create a dataset."
}