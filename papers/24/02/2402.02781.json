{
    "title": "Dual Knowledge Distillation for Efficient Sound Event Detection",
    "abstract": "Sound event detection (SED) is essential for recognizing specific sounds and their temporal locations within acoustic signals. This becomes challenging particularly for on-device applications, where computational resources are limited. To address this issue, we introduce a novel framework referred to as dual knowledge distillation for developing efficient SED systems in this work. Our proposed dual knowledge distillation commences with temporal-averaging knowledge distillation (TAKD), utilizing a mean student model derived from the temporal averaging of the student model's parameters. This allows the student model to indirectly learn from a pre-trained teacher model, ensuring a stable knowledge distillation. Subsequently, we introduce embedding-enhanced feature distillation (EEFD), which involves incorporating an embedding distillation layer within the student model to bolster contextual learning. On DCASE 2023 Task 4A public evaluation dataset, our proposed SED system with dual knowle",
    "link": "https://arxiv.org/abs/2402.02781",
    "context": "Title: Dual Knowledge Distillation for Efficient Sound Event Detection\nAbstract: Sound event detection (SED) is essential for recognizing specific sounds and their temporal locations within acoustic signals. This becomes challenging particularly for on-device applications, where computational resources are limited. To address this issue, we introduce a novel framework referred to as dual knowledge distillation for developing efficient SED systems in this work. Our proposed dual knowledge distillation commences with temporal-averaging knowledge distillation (TAKD), utilizing a mean student model derived from the temporal averaging of the student model's parameters. This allows the student model to indirectly learn from a pre-trained teacher model, ensuring a stable knowledge distillation. Subsequently, we introduce embedding-enhanced feature distillation (EEFD), which involves incorporating an embedding distillation layer within the student model to bolster contextual learning. On DCASE 2023 Task 4A public evaluation dataset, our proposed SED system with dual knowle",
    "path": "papers/24/02/2402.02781.json",
    "total_tokens": 951,
    "translated_title": "双重知识蒸馏用于高效声音事件检测",
    "translated_abstract": "声音事件检测（SED）对于识别特定声音及其在声学信号中的时间位置至关重要。特别是在设备上的应用中，由于计算资源有限，这变得很具挑战性。为了解决这个问题，我们在本研究中引入了一种新颖的框架，称之为双重知识蒸馏，用于开发高效的SED系统。我们提出的双重知识蒸馏以时序平均知识蒸馏（TAKD）为开端，利用从学生模型参数的时序平均得到的平均学生模型。这使得学生模型能够间接地从预训练的教师模型中学习，确保稳定的知识蒸馏。随后，我们引入了增强嵌入特征蒸馏（EEFD），其中包含在学生模型中引入了嵌入蒸馏层来增强上下文学习。在DCASE 2023任务4A公共评估数据集上，我们的双重知识蒸馏SED系统表现出很好的性能。",
    "tldr": "这项研究提出了一种称为双重知识蒸馏的框架，用于开发高效的声音事件检测系统。这个框架通过时序平均知识蒸馏和增强嵌入特征蒸馏来稳定地蒸馏知识并提升上下文学习能力，在实验中取得了良好的性能。"
}