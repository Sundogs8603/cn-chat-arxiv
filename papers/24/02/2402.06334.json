{
    "title": "ExaRanker-Open: Synthetic Explanation for IR using Open-Source LLMs",
    "abstract": "ExaRanker recently introduced an approach to training information retrieval (IR) models, incorporating natural language explanations as additional labels. The method addresses the challenge of limited labeled examples, leading to improvements in the effectiveness of IR models. However, the initial results were based on proprietary language models such as GPT-3.5, which posed constraints on dataset size due to its cost and data privacy. In this paper, we introduce ExaRanker-Open, where we adapt and explore the use of open-source language models to generate explanations. The method has been tested using different LLMs and datasets sizes to better comprehend the effective contribution of data augmentation. Our findings reveal that incorporating explanations consistently enhances neural rankers, with benefits escalating as the LLM size increases. Notably, the data augmentation method proves advantageous even with large datasets, as evidenced by ExaRanker surpassing the target baseline by 0",
    "link": "https://arxiv.org/abs/2402.06334",
    "context": "Title: ExaRanker-Open: Synthetic Explanation for IR using Open-Source LLMs\nAbstract: ExaRanker recently introduced an approach to training information retrieval (IR) models, incorporating natural language explanations as additional labels. The method addresses the challenge of limited labeled examples, leading to improvements in the effectiveness of IR models. However, the initial results were based on proprietary language models such as GPT-3.5, which posed constraints on dataset size due to its cost and data privacy. In this paper, we introduce ExaRanker-Open, where we adapt and explore the use of open-source language models to generate explanations. The method has been tested using different LLMs and datasets sizes to better comprehend the effective contribution of data augmentation. Our findings reveal that incorporating explanations consistently enhances neural rankers, with benefits escalating as the LLM size increases. Notably, the data augmentation method proves advantageous even with large datasets, as evidenced by ExaRanker surpassing the target baseline by 0",
    "path": "papers/24/02/2402.06334.json",
    "total_tokens": 874,
    "translated_title": "ExaRanker-Open: 使用开源LLMs进行IR的合成解释",
    "translated_abstract": "ExaRanker最近提出了一种训练信息检索(IR)模型的方法，该方法将自然语言解释作为附加标签。该方法解决了有限标记示例的挑战，提高了IR模型的效果。然而，初始结果是基于专有的语言模型，如GPT-3.5，这导致了数据集大小的限制，因为其成本和数据隐私。在本文中，我们介绍了ExaRanker-Open，通过适应和探索开源语言模型来生成解释。该方法已经使用不同的LLMs和数据集大小进行了测试，以更好地理解数据增强的有效贡献。我们的研究结果表明，纳入解释能够稳定提高神经排序器的性能，而LLM的大小越大，收益越大。值得注意的是，即使在大数据集上，数据增强方法也是有优势的，ExaRanker的性能超过目标基线0",
    "tldr": "ExaRanker-Open 是一种使用开源LLMs进行IR的方法，通过适应和探索开源语言模型来生成解释。研究结果表明，纳入解释能够稳定提高神经排序器的性能，而LLM的大小越大，收益越大。",
    "en_tdlr": "ExaRanker-Open is a method for IR using open-source LLMs, which generates explanations through adaptation and exploration of open-source language models. The findings show that incorporating explanations consistently improves the performance of neural rankers, with larger LLMs resulting in greater benefits."
}