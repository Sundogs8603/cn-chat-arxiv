{
    "title": "A Data Generation Perspective to the Mechanism of In-Context Learning",
    "abstract": "In-Context Learning (ICL) empowers Large Language Models (LLMs) with the capacity to learn in context, achieving downstream generalization without gradient updates but with a few in-context examples. Despite the encouraging empirical success, the underlying mechanism of ICL remains unclear, and existing research offers various viewpoints of understanding. These studies propose intuition-driven and ad-hoc technical solutions for interpreting ICL, illustrating an ambiguous road map. In this paper, we leverage a data generation perspective to reinterpret recent efforts and demonstrate the potential broader usage of popular technical solutions, approaching a systematic angle. For a conceptual definition, we rigorously adopt the terms of skill learning and skill recognition. The difference between them is skill learning can learn new data generation functions from in-context data. We also provide a comprehensive study on the merits and weaknesses of different solutions, and highlight the un",
    "link": "https://arxiv.org/abs/2402.02212",
    "context": "Title: A Data Generation Perspective to the Mechanism of In-Context Learning\nAbstract: In-Context Learning (ICL) empowers Large Language Models (LLMs) with the capacity to learn in context, achieving downstream generalization without gradient updates but with a few in-context examples. Despite the encouraging empirical success, the underlying mechanism of ICL remains unclear, and existing research offers various viewpoints of understanding. These studies propose intuition-driven and ad-hoc technical solutions for interpreting ICL, illustrating an ambiguous road map. In this paper, we leverage a data generation perspective to reinterpret recent efforts and demonstrate the potential broader usage of popular technical solutions, approaching a systematic angle. For a conceptual definition, we rigorously adopt the terms of skill learning and skill recognition. The difference between them is skill learning can learn new data generation functions from in-context data. We also provide a comprehensive study on the merits and weaknesses of different solutions, and highlight the un",
    "path": "papers/24/02/2402.02212.json",
    "total_tokens": 889,
    "translated_title": "从数据生成的角度对In-Context Learning机制的解释",
    "translated_abstract": "In-Context Learning（ICL）使大型语言模型（LLM）能够在上下文中学习，在只有少量上下文示例的情况下实现下游泛化，而无需梯度更新。尽管有鼓舞人心的实证成功，ICL的基本机制仍然不清楚，现有研究提供了各种不同观点的理解。这些研究提出了基于直觉和临时技术解决方案来解释ICL，呈现出了一条模糊的路线图。在本文中，我们利用数据生成的视角重新解释最近的研究成果，并展示了流行技术解决方案的潜在广泛应用，从而接近一个系统的角度。我们严格采用技能学习和技能识别的概念定义。它们之间的区别在于技能学习可以从上下文数据中学习新的数据生成函数。我们还对不同解决方案的优势和弱点进行了全面的研究，并强调了其中的不足之处。",
    "tldr": "本文从数据生成的角度重新解释了In-Context Learning（ICL）的机制，并探讨了流行的技术解决方案的潜在应用。对不同解决方案的优劣进行了全面研究，强调了其中的不足之处。",
    "en_tdlr": "This paper offers a data generation perspective to reinterpreting the mechanism of In-Context Learning (ICL), exploring the potential broader usage of popular technical solutions. It provides a comprehensive study on the merits and weaknesses of different solutions, highlighting the limitations."
}