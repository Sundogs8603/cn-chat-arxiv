{
    "title": "Quantifying the Persona Effect in LLM Simulations",
    "abstract": "arXiv:2402.10811v1 Announce Type: new  Abstract: Large language models (LLMs) have shown remarkable promise in simulating human language use and behavior. In this study, we delve into the intersection of persona variables and the capability of LLMs to simulate different perspectives. We find that persona variables can explain <10\\% variance in annotations in existing subjective NLP datasets. Nonetheless, incorporating them via prompting in LLMs provides modest improvement. Persona prompting is most effective on data samples where disagreements among annotators are frequent yet confined to a limited range. A linear correlation exists: the more persona variables influence human annotations, the better LLMs predictions are using persona prompting. However, when the utility of persona variables is low (i.e., explaining <10\\% of human annotations), persona prompting has little effect. Most subjective NLP datasets fall into this category, casting doubt on simulating diverse perspectives in t",
    "link": "https://arxiv.org/abs/2402.10811",
    "context": "Title: Quantifying the Persona Effect in LLM Simulations\nAbstract: arXiv:2402.10811v1 Announce Type: new  Abstract: Large language models (LLMs) have shown remarkable promise in simulating human language use and behavior. In this study, we delve into the intersection of persona variables and the capability of LLMs to simulate different perspectives. We find that persona variables can explain <10\\% variance in annotations in existing subjective NLP datasets. Nonetheless, incorporating them via prompting in LLMs provides modest improvement. Persona prompting is most effective on data samples where disagreements among annotators are frequent yet confined to a limited range. A linear correlation exists: the more persona variables influence human annotations, the better LLMs predictions are using persona prompting. However, when the utility of persona variables is low (i.e., explaining <10\\% of human annotations), persona prompting has little effect. Most subjective NLP datasets fall into this category, casting doubt on simulating diverse perspectives in t",
    "path": "papers/24/02/2402.10811.json",
    "total_tokens": 882,
    "translated_title": "在LLM模拟中量化Persona效应",
    "translated_abstract": "大型语言模型（LLMs）在模拟人类语言使用和行为方面表现出显著的潜力。在这项研究中，我们深入探讨了人物变量与LLMs模拟不同视角的能力的交集。我们发现人物变量可以解释现有主观NLP数据集中<10\\%的注释变异。然而，通过提示在LLMs中加入他们能带来适度的改进。Persona提示在注释者之间存在争议但范围有限的数据样本上效果最好。存在线性相关性：人格变量对人类注释的影响越大，LLMs使用Persona提示的预测就越好。然而，当人物变量的效用较低（即解释人类注释的<10\\%）时，Persona提示几乎没有影响。大多数主观NLP数据集都属于这一类别，对模拟多元视角产生怀疑。",
    "tldr": "本研究探讨了人物变量对LLMs模拟不同视角能力的影响，发现人物变量在现有主观NLP数据集中解释能力有限，但通过提示方式加入可以略微改善模型预测，尤其在存在争议但范围有限的数据样本上效果最好。"
}