{
    "title": "Policy Learning for Off-Dynamics RL with Deficient Support",
    "abstract": "arXiv:2402.10765v1 Announce Type: cross  Abstract: Reinforcement Learning (RL) can effectively learn complex policies. However, learning these policies often demands extensive trial-and-error interactions with the environment. In many real-world scenarios, this approach is not practical due to the high costs of data collection and safety concerns. As a result, a common strategy is to transfer a policy trained in a low-cost, rapid source simulator to a real-world target environment. However, this process poses challenges. Simulators, no matter how advanced, cannot perfectly replicate the intricacies of the real world, leading to dynamics discrepancies between the source and target environments. Past research posited that the source domain must encompass all possible target transitions, a condition we term full support. However, expecting full support is often unrealistic, especially in scenarios where significant dynamics discrepancies arise. In this paper, our emphasis shifts to addres",
    "link": "https://arxiv.org/abs/2402.10765",
    "context": "Title: Policy Learning for Off-Dynamics RL with Deficient Support\nAbstract: arXiv:2402.10765v1 Announce Type: cross  Abstract: Reinforcement Learning (RL) can effectively learn complex policies. However, learning these policies often demands extensive trial-and-error interactions with the environment. In many real-world scenarios, this approach is not practical due to the high costs of data collection and safety concerns. As a result, a common strategy is to transfer a policy trained in a low-cost, rapid source simulator to a real-world target environment. However, this process poses challenges. Simulators, no matter how advanced, cannot perfectly replicate the intricacies of the real world, leading to dynamics discrepancies between the source and target environments. Past research posited that the source domain must encompass all possible target transitions, a condition we term full support. However, expecting full support is often unrealistic, especially in scenarios where significant dynamics discrepancies arise. In this paper, our emphasis shifts to addres",
    "path": "papers/24/02/2402.10765.json",
    "total_tokens": 824,
    "translated_title": "政策学习在支持不足的离线动力学RL中的应用",
    "translated_abstract": "强化学习（RL）可以有效学习复杂的策略。然而，学习这些策略通常需要与环境进行大量的试错交互。在许多现实场景中，由于数据收集的高成本和安全问题，这种方法并不实际。因此，一个常见的策略是将在低成本、快速源模拟器中训练的策略转移到真实世界的目标环境。然而，这个过程存在挑战。无论模拟器多么先进，都不能完美复制真实世界的复杂性，导致源环境和目标环境之间存在动态差异。先前的研究提出，源领域必须包含所有可能的目标转换，这种条件我们称之为完全支持。然而，在预期完全支持往往是不切实际的，特别是在存在重大动态差异的情况下。在本文中，我们的重点转向了解决",
    "tldr": "该论文主要研究在离线动力学强化学习中如何应对源环境和目标环境之间的动态差异挑战。",
    "en_tdlr": "This paper focuses on addressing the challenges of dynamic disparities between the source and target environments in offline dynamic reinforcement learning."
}