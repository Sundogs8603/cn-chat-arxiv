{
    "title": "Time-, Memory- and Parameter-Efficient Visual Adaptation",
    "abstract": "As foundation models become more popular, there is a growing need to efficiently finetune them for downstream tasks. Although numerous adaptation methods have been proposed, they are designed to be efficient only in terms of how many parameters are trained. They, however, typically still require backpropagating gradients throughout the model, meaning that their training-time and -memory cost does not reduce as significantly.   We propose an adaptation method which does not backpropagate gradients through the backbone. We achieve this by designing a lightweight network in parallel that operates on features from the frozen, pretrained backbone. As a result, our method is efficient not only in terms of parameters, but also in training-time and memory usage. Our approach achieves state-of-the-art accuracy-parameter trade-offs on the popular VTAB benchmark, and we further show how we outperform prior works with respect to training-time and -memory usage too. We further demonstrate the train",
    "link": "https://arxiv.org/abs/2402.02887",
    "context": "Title: Time-, Memory- and Parameter-Efficient Visual Adaptation\nAbstract: As foundation models become more popular, there is a growing need to efficiently finetune them for downstream tasks. Although numerous adaptation methods have been proposed, they are designed to be efficient only in terms of how many parameters are trained. They, however, typically still require backpropagating gradients throughout the model, meaning that their training-time and -memory cost does not reduce as significantly.   We propose an adaptation method which does not backpropagate gradients through the backbone. We achieve this by designing a lightweight network in parallel that operates on features from the frozen, pretrained backbone. As a result, our method is efficient not only in terms of parameters, but also in training-time and memory usage. Our approach achieves state-of-the-art accuracy-parameter trade-offs on the popular VTAB benchmark, and we further show how we outperform prior works with respect to training-time and -memory usage too. We further demonstrate the train",
    "path": "papers/24/02/2402.02887.json",
    "total_tokens": 940,
    "translated_title": "时间、内存和参数高效的视觉适应",
    "translated_abstract": "随着基础模型越来越受欢迎，迫切需要高效地对其进行下游任务的微调。虽然已经提出了许多适应方法，但它们仅在训练的参数数量上高效。然而，它们通常仍然需要在整个模型中反向传播梯度，这意味着它们的训练时间和内存成本并没有明显降低。我们提出了一种不通过主干网络反向传播梯度的适应方法。我们通过设计一个轻量级的并行网络，对从冻结的预训练主干网络中提取的特征进行操作来实现这一点。因此，我们的方法不仅在参数方面高效，而且在训练时间和内存使用方面也是高效的。我们的方法在常用的VTAB基准测试上取得了最先进的准确性和参数权衡，并进一步展示了我们在训练时间和内存使用方面超越了先前的工作。",
    "tldr": "这项研究提出了一种时间、内存和参数高效的视觉适应方法，通过设计一个轻量级的并行网络，在不反向传播梯度的情况下对预训练主干网络的特征进行操作，实现了在训练时间和内存使用上的高效。该方法在VTAB基准测试中取得了最先进的准确性和参数权衡，并超越了先前的工作在训练时间和内存使用方面的优势。",
    "en_tdlr": "This research proposes a time-, memory-, and parameter-efficient visual adaptation method that operates on features from a frozen, pretrained backbone without backpropagating gradients, achieving state-of-the-art accuracy-parameter trade-offs and outperforming prior works in terms of training-time and memory usage."
}