{
    "title": "The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)",
    "abstract": "arXiv:2402.16893v1 Announce Type: cross  Abstract: Retrieval-augmented generation (RAG) is a powerful technique to facilitate language model with proprietary and private data, where data privacy is a pivotal concern. Whereas extensive research has demonstrated the privacy risks of large language models (LLMs), the RAG technique could potentially reshape the inherent behaviors of LLM generation, posing new privacy issues that are currently under-explored. In this work, we conduct extensive empirical studies with novel attack methods, which demonstrate the vulnerability of RAG systems on leaking the private retrieval database. Despite the new risk brought by RAG on the retrieval data, we further reveal that RAG can mitigate the leakage of the LLMs' training data. Overall, we provide new insights in this paper for privacy protection of retrieval-augmented LLMs, which benefit both LLMs and RAG systems builders. Our code is available at https://github.com/phycholosogy/RAG-privacy.",
    "link": "https://arxiv.org/abs/2402.16893",
    "context": "Title: The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)\nAbstract: arXiv:2402.16893v1 Announce Type: cross  Abstract: Retrieval-augmented generation (RAG) is a powerful technique to facilitate language model with proprietary and private data, where data privacy is a pivotal concern. Whereas extensive research has demonstrated the privacy risks of large language models (LLMs), the RAG technique could potentially reshape the inherent behaviors of LLM generation, posing new privacy issues that are currently under-explored. In this work, we conduct extensive empirical studies with novel attack methods, which demonstrate the vulnerability of RAG systems on leaking the private retrieval database. Despite the new risk brought by RAG on the retrieval data, we further reveal that RAG can mitigate the leakage of the LLMs' training data. Overall, we provide new insights in this paper for privacy protection of retrieval-augmented LLMs, which benefit both LLMs and RAG systems builders. Our code is available at https://github.com/phycholosogy/RAG-privacy.",
    "path": "papers/24/02/2402.16893.json",
    "total_tokens": 868,
    "translated_title": "探讨检索增强生成（RAG）中的隐私问题的利与弊",
    "translated_abstract": "检索增强生成（RAG）是一种强大的技术，用于促进具有专有和私密数据的语言模型，其中数据隐私是一个关键关注点。尽管大量研究已经证明了大型语言模型（LLMs）存在的隐私风险，RAG技术有可能改变LLM生成的固有行为，从而引发目前尚未充分探讨的新隐私问题。在这项工作中，我们进行了大量的实证研究，并采用了新颖的攻击方法，证明了RAG系统泄露私密检索数据库的脆弱性。尽管RAG带来了检索数据的新风险，我们进一步揭示了RAG可以减少LLM训练数据的泄漏。总体而言，我们在本文中提供了关于检索增强LLMs隐私保护的新见解，这有利于LLMs和RAG系统的构建者。我们的代码可在https://github.com/phycholosogy/RAG-privacy找到。",
    "tldr": "RAG技术有可能改变大型语言模型生成行为，带来新的隐私问题，但同时也可以减少语言模型的训练数据泄漏。",
    "en_tdlr": "RAG has the potential to reshape the generation behavior of large language models, introducing new privacy issues while also mitigating the leakage of training data."
}