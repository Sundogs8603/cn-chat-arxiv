{
    "title": "De-amplifying Bias from Differential Privacy in Language Model Fine-tuning",
    "abstract": "Fairness and privacy are two important values machine learning (ML) practitioners often seek to operationalize in models. Fairness aims to reduce model bias for social/demographic sub-groups. Privacy via differential privacy (DP) mechanisms, on the other hand, limits the impact of any individual's training data on the resulting model. The trade-offs between privacy and fairness goals of trustworthy ML pose a challenge to those wishing to address both. We show that DP amplifies gender, racial, and religious bias when fine-tuning large language models (LLMs), producing models more biased than ones fine-tuned without DP. We find the cause of the amplification to be a disparity in convergence of gradients across sub-groups. Through the case of binary gender bias, we demonstrate that Counterfactual Data Augmentation (CDA), a known method for addressing bias, also mitigates bias amplification by DP. As a consequence, DP and CDA together can be used to fine-tune models while maintaining both ",
    "link": "https://arxiv.org/abs/2402.04489",
    "context": "Title: De-amplifying Bias from Differential Privacy in Language Model Fine-tuning\nAbstract: Fairness and privacy are two important values machine learning (ML) practitioners often seek to operationalize in models. Fairness aims to reduce model bias for social/demographic sub-groups. Privacy via differential privacy (DP) mechanisms, on the other hand, limits the impact of any individual's training data on the resulting model. The trade-offs between privacy and fairness goals of trustworthy ML pose a challenge to those wishing to address both. We show that DP amplifies gender, racial, and religious bias when fine-tuning large language models (LLMs), producing models more biased than ones fine-tuned without DP. We find the cause of the amplification to be a disparity in convergence of gradients across sub-groups. Through the case of binary gender bias, we demonstrate that Counterfactual Data Augmentation (CDA), a known method for addressing bias, also mitigates bias amplification by DP. As a consequence, DP and CDA together can be used to fine-tune models while maintaining both ",
    "path": "papers/24/02/2402.04489.json",
    "total_tokens": 943,
    "translated_title": "通过差分隐私在语言模型微调中消除偏见放大",
    "translated_abstract": "公平性和隐私是机器学习（ML）从业者经常在模型中追求的两个重要价值。公平性旨在减少对社会/人口亚组的模型偏见。然而，差分隐私（DP）机制通过限制任何个体的训练数据对结果模型的影响来保护隐私。可靠的ML的隐私和公平目标之间的权衡对于那些希望解决两者的人来说是一种挑战。我们发现DP在对大型语言模型（LLMs）进行微调时放大了性别、种族和宗教偏见，产生了比没有DP微调的模型更加偏见的模型。我们发现放大的原因是梯度在子组之间的收敛不平衡。通过二元性别偏见的案例，我们证明了对抗事实数据增强（CDA）也能通过DP减少偏见的放大。因此，DP和CDA可以一起用于微调模型，同时保持两者的可靠性。",
    "tldr": "本研究发现，在语言模型的微调过程中，差分隐私放大了性别、种族和宗教方面的偏见。我们通过对抗事实数据增强方法证明了DP可以减少偏见的放大。因此，我们可以使用DP和CDA来微调模型，同时保持可靠性和公平性。",
    "en_tdlr": "This study found that differential privacy amplifies gender, racial, and religious bias in language model fine-tuning. The researchers demonstrated that counterfactual data augmentation can mitigate the bias amplification caused by differential privacy. Therefore, using both differential privacy and counterfactual data augmentation can fine-tune models while maintaining fairness and reliability."
}