{
    "title": "MusicMagus: Zero-Shot Text-to-Music Editing via Diffusion Models",
    "abstract": "Recent advances in text-to-music generation models have opened new avenues in musical creativity. However, music generation usually involves iterative refinements, and how to edit the generated music remains a significant challenge. This paper introduces a novel approach to the editing of music generated by such models, enabling the modification of specific attributes, such as genre, mood and instrument, while maintaining other aspects unchanged. Our method transforms text editing to \\textit{latent space manipulation} while adding an extra constraint to enforce consistency. It seamlessly integrates with existing pretrained text-to-music diffusion models without requiring additional training. Experimental results demonstrate superior performance over both zero-shot and certain supervised baselines in style and timbre transfer evaluations. Additionally, we showcase the practical applicability of our approach in real-world music editing scenarios.",
    "link": "https://arxiv.org/abs/2402.06178",
    "context": "Title: MusicMagus: Zero-Shot Text-to-Music Editing via Diffusion Models\nAbstract: Recent advances in text-to-music generation models have opened new avenues in musical creativity. However, music generation usually involves iterative refinements, and how to edit the generated music remains a significant challenge. This paper introduces a novel approach to the editing of music generated by such models, enabling the modification of specific attributes, such as genre, mood and instrument, while maintaining other aspects unchanged. Our method transforms text editing to \\textit{latent space manipulation} while adding an extra constraint to enforce consistency. It seamlessly integrates with existing pretrained text-to-music diffusion models without requiring additional training. Experimental results demonstrate superior performance over both zero-shot and certain supervised baselines in style and timbre transfer evaluations. Additionally, we showcase the practical applicability of our approach in real-world music editing scenarios.",
    "path": "papers/24/02/2402.06178.json",
    "total_tokens": 841,
    "translated_title": "MusicMagus: 通过扩散模型实现零样本文本到音乐的编辑",
    "translated_abstract": "近年来，文本到音乐生成模型的不断进步为音乐创造力开辟了新的道路。然而，音乐生成通常涉及迭代的改进，如何编辑生成的音乐仍然是一个重大挑战。本文介绍了一种新颖的方法，用于编辑这种模型生成的音乐，实现对特定属性（如风格、情感和乐器）的修改，同时保持其他方面不变。我们的方法将文本编辑转化为潜在空间操纵，并添加额外的约束以确保一致性。它可以无需额外训练与现有的预训练文本到音乐扩散模型无缝集成。实验结果在风格和音色转换评估中显示出优于零样本和某些监督基线的性能。此外，我们展示了我们方法在实际音乐编辑场景中的实用性。",
    "tldr": "本文介绍了一个通过扩散模型实现零样本文本到音乐的编辑的新方法，可以修改生成音乐的特定属性而保持其他方面不变，并展示了其在风格和音色转换方面的优越性能以及在实际音乐编辑场景中的实用性。",
    "en_tdlr": "This paper introduces a novel approach for zero-shot text-to-music editing using diffusion models, enabling modification of specific attributes while maintaining others unchanged. It showcases superior performance in style and timbre transfer evaluations, and practical applicability in real-world music editing scenarios."
}