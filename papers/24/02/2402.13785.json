{
    "title": "Synthesis of Hierarchical Controllers Based on Deep Reinforcement Learning Policies",
    "abstract": "arXiv:2402.13785v1 Announce Type: new  Abstract: We propose a novel approach to the problem of controller design for environments modeled as Markov decision processes (MDPs). Specifically, we consider a hierarchical MDP a graph with each vertex populated by an MDP called a \"room\". We first apply deep reinforcement learning (DRL) to obtain low-level policies for each room, scaling to large rooms of unknown structure. We then apply reactive synthesis to obtain a high-level planner that chooses which low-level policy to execute in each room. The central challenge in synthesizing the planner is the need for modeling rooms. We address this challenge by developing a DRL procedure to train concise \"latent\" policies together with PAC guarantees on their performance. Unlike previous approaches, ours circumvents a model distillation step. Our approach combats sparse rewards in DRL and enables reusability of low-level policies. We demonstrate feasibility in a case study involving agent navigation",
    "link": "https://arxiv.org/abs/2402.13785",
    "context": "Title: Synthesis of Hierarchical Controllers Based on Deep Reinforcement Learning Policies\nAbstract: arXiv:2402.13785v1 Announce Type: new  Abstract: We propose a novel approach to the problem of controller design for environments modeled as Markov decision processes (MDPs). Specifically, we consider a hierarchical MDP a graph with each vertex populated by an MDP called a \"room\". We first apply deep reinforcement learning (DRL) to obtain low-level policies for each room, scaling to large rooms of unknown structure. We then apply reactive synthesis to obtain a high-level planner that chooses which low-level policy to execute in each room. The central challenge in synthesizing the planner is the need for modeling rooms. We address this challenge by developing a DRL procedure to train concise \"latent\" policies together with PAC guarantees on their performance. Unlike previous approaches, ours circumvents a model distillation step. Our approach combats sparse rewards in DRL and enables reusability of low-level policies. We demonstrate feasibility in a case study involving agent navigation",
    "path": "papers/24/02/2402.13785.json",
    "total_tokens": 955,
    "translated_title": "基于深度强化学习策略的分层控制器合成",
    "translated_abstract": "我们提出了一种新颖的方法来解决将环境建模为马尔可夫决策过程（MDPs）的控制器设计问题。具体来说，我们考虑了一个分层MDP，一个由称为“房间”的MDP填充的图形。我们首先应用深度强化学习（DRL）来获得每个房间的低级策略，以适应未知结构的大房间。然后我们应用反应合成来获得一个高级规划者，选择在每个房间执行哪个低级策略。在合成规划者方面的中心挑战是需要对房间进行建模。我们通过开发一个DRL过程来训练简洁的“潜在”策略以及关于其性能的PAC保证来解决这一挑战。与先前方法不同，我们的方法规避了模型提炼步骤。我们的方法对抗DRL中的稀疏奖励，并实现了低级策略的可重用性。我们通过一个涉及代理导航的案例研究证明了可行性。",
    "tldr": "提出了基于深度强化学习策略的分层控制器设计方法，通过训练简洁的“潜在”策略来解决房间建模问题，无需模型提炼步骤，克服了DRL中的稀疏奖励，实现了低级策略的可重用性",
    "en_tdlr": "A method for synthesizing hierarchical controllers based on deep reinforcement learning policies is proposed, tackling room modeling without model distillation step, overcoming sparse rewards in DRL, and enabling reusability of low-level policies."
}