{
    "title": "Memory-Efficient Vision Transformers: An Activation-Aware Mixed-Rank Compression Strategy",
    "abstract": "As Vision Transformers (ViTs) increasingly set new benchmarks in computer vision, their practical deployment on inference engines is often hindered by their significant memory bandwidth and (on-chip) memory footprint requirements. This paper addresses this memory limitation by introducing an activation-aware model compression methodology that uses selective low-rank weight tensor approximations of different layers to reduce the parameter count of ViTs. The key idea is to decompose the weight tensors into a sum of two parameter-efficient tensors while minimizing the error between the product of the input activations with the original weight tensor and the product of the input activations with the approximate tensor sum. This approximation is further refined by adopting an efficient layer-wise error compensation technique that uses the gradient of the layer's output loss. The combination of these techniques achieves excellent results while it avoids being trapped in a shallow local minim",
    "link": "https://arxiv.org/abs/2402.06004",
    "context": "Title: Memory-Efficient Vision Transformers: An Activation-Aware Mixed-Rank Compression Strategy\nAbstract: As Vision Transformers (ViTs) increasingly set new benchmarks in computer vision, their practical deployment on inference engines is often hindered by their significant memory bandwidth and (on-chip) memory footprint requirements. This paper addresses this memory limitation by introducing an activation-aware model compression methodology that uses selective low-rank weight tensor approximations of different layers to reduce the parameter count of ViTs. The key idea is to decompose the weight tensors into a sum of two parameter-efficient tensors while minimizing the error between the product of the input activations with the original weight tensor and the product of the input activations with the approximate tensor sum. This approximation is further refined by adopting an efficient layer-wise error compensation technique that uses the gradient of the layer's output loss. The combination of these techniques achieves excellent results while it avoids being trapped in a shallow local minim",
    "path": "papers/24/02/2402.06004.json",
    "total_tokens": 917,
    "translated_title": "内存高效的视觉Transformer：一种激活感知的混合秩压缩策略",
    "translated_abstract": "随着视觉Transformer（ViTs）在计算机视觉领域不断刷新最新记录，它们在推理引擎上的实际部署往往受到显著的内存带宽和（芯片内）内存占用的限制。本文通过引入一种激活感知的模型压缩方法来解决这一内存限制问题，该方法使用不同层的选择性低秩权重张量近似来减少ViTs的参数数量。关键思想是将权重张量分解为两个参数高效的张量之和，同时将输入激活与原始权重张量的乘积与输入激活与近似张量之和的乘积之间的误差最小化。通过采用有效的逐层误差补偿技术，利用层输出损失的梯度进一步改进了这种近似。这些技术的组合在避免陷入浅层局部最小值的同时取得了优秀的结果。",
    "tldr": "本文提出了一种激活感知的混合秩压缩策略来提高视觉Transformer的内存效率，并通过选择性低秩权重张量近似和层间误差补偿技术来减少参数数量。这种策略避免了浅层局部最小值陷阱，同时取得了优秀的结果。"
}