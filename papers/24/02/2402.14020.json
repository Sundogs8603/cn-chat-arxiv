{
    "title": "Coercing LLMs to do and reveal (almost) anything",
    "abstract": "arXiv:2402.14020v1 Announce Type: cross  Abstract: It has recently been shown that adversarial attacks on large language models (LLMs) can \"jailbreak\" the model into making harmful statements. In this work, we argue that the spectrum of adversarial attacks on LLMs is much larger than merely jailbreaking. We provide a broad overview of possible attack surfaces and attack goals. Based on a series of concrete examples, we discuss, categorize and systematize attacks that coerce varied unintended behaviors, such as misdirection, model control, denial-of-service, or data extraction.   We analyze these attacks in controlled experiments, and find that many of them stem from the practice of pre-training LLMs with coding capabilities, as well as the continued existence of strange \"glitch\" tokens in common LLM vocabularies that should be removed for security reasons.",
    "link": "https://arxiv.org/abs/2402.14020",
    "context": "Title: Coercing LLMs to do and reveal (almost) anything\nAbstract: arXiv:2402.14020v1 Announce Type: cross  Abstract: It has recently been shown that adversarial attacks on large language models (LLMs) can \"jailbreak\" the model into making harmful statements. In this work, we argue that the spectrum of adversarial attacks on LLMs is much larger than merely jailbreaking. We provide a broad overview of possible attack surfaces and attack goals. Based on a series of concrete examples, we discuss, categorize and systematize attacks that coerce varied unintended behaviors, such as misdirection, model control, denial-of-service, or data extraction.   We analyze these attacks in controlled experiments, and find that many of them stem from the practice of pre-training LLMs with coding capabilities, as well as the continued existence of strange \"glitch\" tokens in common LLM vocabularies that should be removed for security reasons.",
    "path": "papers/24/02/2402.14020.json",
    "total_tokens": 875,
    "translated_title": "迫使LLMs执行并揭示（几乎）任何事情",
    "translated_abstract": "最近有研究表明，对大型语言模型（LLMs）的对抗性攻击可以“越狱”该模型以发表有害言论。在这项工作中，我们认为LLMs的对抗性攻击范围远不止于越狱。我们提供了对可能的攻击面和攻击目标的广泛概述。根据一系列具体示例，我们讨论、分类和系统化了一些攻击，这些攻击迫使LLMs展示各种意外行为，如误导、模型控制、拒绝服务或数据提取。我们通过控制实验分析这些攻击，并发现其中许多是由于预训练LLMs具有编码能力的实践，以及常见LLMs词汇中应删除的奇怪“故障”标记的持续存在所导致的。",
    "tldr": "本研究发现对大型语言模型的对抗性攻击不仅仅局限于“越狱”，而包括迫使模型展示各种意外行为，攻击表面和目标广泛。这些攻击源于LLMs的预训练和常见词汇中存在的“故障”标记。",
    "en_tdlr": "This study finds that adversarial attacks on large language models extend beyond \"jailbreaking\" to coercing models to exhibit various unintended behaviors, with a broad spectrum of attack surfaces and goals. These attacks stem from the pre-training of LLMs and the presence of \"glitch\" tokens in common vocabularies."
}