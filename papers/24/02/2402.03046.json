{
    "title": "Open RL Benchmark: Comprehensive Tracked Experiments for Reinforcement Learning",
    "abstract": "In many Reinforcement Learning (RL) papers, learning curves are useful indicators to measure the effectiveness of RL algorithms. However, the complete raw data of the learning curves are rarely available. As a result, it is usually necessary to reproduce the experiments from scratch, which can be time-consuming and error-prone. We present Open RL Benchmark, a set of fully tracked RL experiments, including not only the usual data such as episodic return, but also all algorithm-specific and system metrics. Open RL Benchmark is community-driven: anyone can download, use, and contribute to the data. At the time of writing, more than 25,000 runs have been tracked, for a cumulative duration of more than 8 years. Open RL Benchmark covers a wide range of RL libraries and reference implementations. Special care is taken to ensure that each experiment is precisely reproducible by providing not only the full parameters, but also the versions of the dependencies used to generate it. In addition, O",
    "link": "https://arxiv.org/abs/2402.03046",
    "context": "Title: Open RL Benchmark: Comprehensive Tracked Experiments for Reinforcement Learning\nAbstract: In many Reinforcement Learning (RL) papers, learning curves are useful indicators to measure the effectiveness of RL algorithms. However, the complete raw data of the learning curves are rarely available. As a result, it is usually necessary to reproduce the experiments from scratch, which can be time-consuming and error-prone. We present Open RL Benchmark, a set of fully tracked RL experiments, including not only the usual data such as episodic return, but also all algorithm-specific and system metrics. Open RL Benchmark is community-driven: anyone can download, use, and contribute to the data. At the time of writing, more than 25,000 runs have been tracked, for a cumulative duration of more than 8 years. Open RL Benchmark covers a wide range of RL libraries and reference implementations. Special care is taken to ensure that each experiment is precisely reproducible by providing not only the full parameters, but also the versions of the dependencies used to generate it. In addition, O",
    "path": "papers/24/02/2402.03046.json",
    "total_tokens": 894,
    "translated_title": "开放强化学习基准测试：全面跟踪的实验数据",
    "translated_abstract": "在许多强化学习（RL）论文中，学习曲线是衡量RL算法有效性的有用指标。然而，学习曲线的完整原始数据很少公开。因此，通常需要从头开始重现实验，这可能耗时且容易出错。我们提出了开放式强化学习基准测试，这是一组完全跟踪的强化学习实验，包括通常的数据如一次性回报，以及所有特定算法和系统指标。开放式强化学习基准测试是社区驱动的：任何人都可以下载、使用和贡献数据。到目前为止，已经跟踪了超过25000次运行，累计时间超过8年。开放式强化学习基准测试涵盖了各种强化学习库和参考实现。我们特别注重确保每个实验的精确可再现性，不仅提供完整的参数，还提供用于生成实验的依赖版本。",
    "tldr": "Open RL Benchmark是一组跟踪强化学习实验的数据，并涵盖了广泛的RL库和参考实现。它提供了完整的原始数据，可以用来衡量RL算法的有效性，也可供社区进行下载、使用和贡献。",
    "en_tdlr": "Open RL Benchmark is a set of tracked reinforcement learning experiments with comprehensive data, covering a wide range of RL libraries and reference implementations. It provides complete raw data to measure the effectiveness of RL algorithms and can be downloaded, used, and contributed to by the community."
}