{
    "title": "Preserving Near-Optimal Gradient Sparsification Cost for Scalable Distributed Deep Learning",
    "abstract": "arXiv:2402.13781v1 Announce Type: new  Abstract: Communication overhead is a major obstacle to scaling distributed training systems. Gradient sparsification is a potential optimization approach to reduce the communication volume without significant loss of model fidelity. However, existing gradient sparsification methods have low scalability owing to inefficient design of their algorithms, which raises the communication overhead significantly. In particular, gradient build-up and inadequate sparsity control methods degrade the sparsification performance considerably. Moreover, communication traffic increases drastically owing to workload imbalance of gradient selection between workers.   To address these challenges, we propose a novel gradient sparsification scheme called ExDyna. In ExDyna, the gradient tensor of the model comprises fined-grained blocks, and contiguous blocks are grouped into non-overlapping partitions. Each worker selects gradients in its exclusively allocated partiti",
    "link": "https://arxiv.org/abs/2402.13781",
    "context": "Title: Preserving Near-Optimal Gradient Sparsification Cost for Scalable Distributed Deep Learning\nAbstract: arXiv:2402.13781v1 Announce Type: new  Abstract: Communication overhead is a major obstacle to scaling distributed training systems. Gradient sparsification is a potential optimization approach to reduce the communication volume without significant loss of model fidelity. However, existing gradient sparsification methods have low scalability owing to inefficient design of their algorithms, which raises the communication overhead significantly. In particular, gradient build-up and inadequate sparsity control methods degrade the sparsification performance considerably. Moreover, communication traffic increases drastically owing to workload imbalance of gradient selection between workers.   To address these challenges, we propose a novel gradient sparsification scheme called ExDyna. In ExDyna, the gradient tensor of the model comprises fined-grained blocks, and contiguous blocks are grouped into non-overlapping partitions. Each worker selects gradients in its exclusively allocated partiti",
    "path": "papers/24/02/2402.13781.json",
    "total_tokens": 716,
    "translated_title": "保持接近最优梯度稀疏化成本的可扩展分布式深度学习",
    "translated_abstract": "通信开销是扩展分布式训练系统的一大障碍。梯度稀疏化是一种潜在的优化方法，可以减少通信量而不显著损失模型的保真度。然而，现有的梯度稀疏化方法由于其算法设计低效，从而导致可扩展性差，显著提高了通信开销。为了解决这些挑战，我们提出了一种名为ExDyna的新型梯度稀疏化方案，具有细粒度块和组成非重叠分区的连续块。",
    "tldr": "提出了一种名为ExDyna的新型梯度稀疏化方案，通过细粒度块和分区组合来减少通信开销。",
    "en_tdlr": "Proposed a novel gradient sparsification scheme called ExDyna, reducing communication overhead through fine-grained blocks and partition grouping."
}