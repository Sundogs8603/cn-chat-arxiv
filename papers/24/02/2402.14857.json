{
    "title": "Is the System Message Really Important to Jailbreaks in Large Language Models?",
    "abstract": "arXiv:2402.14857v1 Announce Type: cross  Abstract: The rapid evolution of Large Language Models (LLMs) has rendered them indispensable in modern society. While security measures are typically in place to align LLMs with human values prior to release, recent studies have unveiled a concerning phenomenon named \"jailbreak.\" This term refers to the unexpected and potentially harmful responses generated by LLMs when prompted with malicious questions. Existing research focuses on generating jailbreak prompts but our study aim to answer a different question: Is the system message really important to jailbreak in LLMs? To address this question, we conducted experiments in a stable GPT version gpt-3.5-turbo-0613 to generated jailbreak prompts with varying system messages: short, long, and none. We discover that different system messages have distinct resistances to jailbreak by experiments. Additionally, we explore the transferability of jailbreak across LLMs. This finding underscores the signi",
    "link": "https://arxiv.org/abs/2402.14857",
    "context": "Title: Is the System Message Really Important to Jailbreaks in Large Language Models?\nAbstract: arXiv:2402.14857v1 Announce Type: cross  Abstract: The rapid evolution of Large Language Models (LLMs) has rendered them indispensable in modern society. While security measures are typically in place to align LLMs with human values prior to release, recent studies have unveiled a concerning phenomenon named \"jailbreak.\" This term refers to the unexpected and potentially harmful responses generated by LLMs when prompted with malicious questions. Existing research focuses on generating jailbreak prompts but our study aim to answer a different question: Is the system message really important to jailbreak in LLMs? To address this question, we conducted experiments in a stable GPT version gpt-3.5-turbo-0613 to generated jailbreak prompts with varying system messages: short, long, and none. We discover that different system messages have distinct resistances to jailbreak by experiments. Additionally, we explore the transferability of jailbreak across LLMs. This finding underscores the signi",
    "path": "papers/24/02/2402.14857.json",
    "total_tokens": 933,
    "translated_title": "大型语言模型中的系统消息对越狱是否真的很重要？",
    "translated_abstract": "大型语言模型（LLMs）的快速发展使它们在现代社会中不可或缺。尽管通常会采取安全措施在发布前将LLMs与人类价值观保持一致，但最近的研究揭示了一个令人担忧的现象，被称为\"越狱\"。这个术语指的是当LLMs受到恶意问题提示时产生意外且可能有害的响应。现有研究侧重于生成越狱提示，但我们的研究旨在回答一个不同的问题：系统消息对LLMs中的越狱是否真的很重要？为了回答这个问题，我们在一个稳定的GPT版本gpt-3.5-turbo-0613中进行了实验，生成了具有不同系统消息的越狱提示：短，长和无消息。我们发现不同的系统消息通过实验具有不同的抵抗越狱的能力。此外，我们还探讨了越狱在LLMs之间的可转移性。这一发现强调了系统消息在防止LLMs越狱中的重要性。",
    "tldr": "系统消息在大型语言模型中的越狱过程中起着重要作用，不同系统消息对抵抗越狱具有不同影响，且越狱可能在不同语言模型之间具有可转移性。",
    "en_tdlr": "The system message plays an important role in the jailbreak process in large language models, with different system messages having varying effects on resisting jailbreak, and jailbreaks may be transferable across different language models."
}