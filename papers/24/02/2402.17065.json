{
    "title": "Taming the Tail in Class-Conditional GANs: Knowledge Sharing via Unconditional Training at Lower Resolutions",
    "abstract": "arXiv:2402.17065v1 Announce Type: cross  Abstract: Despite the extensive research on training generative adversarial networks (GANs) with limited training data, learning to generate images from long-tailed training distributions remains fairly unexplored. In the presence of imbalanced multi-class training data, GANs tend to favor classes with more samples, leading to the generation of low-quality and less diverse samples in tail classes. In this study, we aim to improve the training of class-conditional GANs with long-tailed data. We propose a straightforward yet effective method for knowledge sharing, allowing tail classes to borrow from the rich information from classes with more abundant training data. More concretely, we propose modifications to existing class-conditional GAN architectures to ensure that the lower-resolution layers of the generator are trained entirely unconditionally while reserving class-conditional generation for the higher-resolution layers. Experiments on seve",
    "link": "https://arxiv.org/abs/2402.17065",
    "context": "Title: Taming the Tail in Class-Conditional GANs: Knowledge Sharing via Unconditional Training at Lower Resolutions\nAbstract: arXiv:2402.17065v1 Announce Type: cross  Abstract: Despite the extensive research on training generative adversarial networks (GANs) with limited training data, learning to generate images from long-tailed training distributions remains fairly unexplored. In the presence of imbalanced multi-class training data, GANs tend to favor classes with more samples, leading to the generation of low-quality and less diverse samples in tail classes. In this study, we aim to improve the training of class-conditional GANs with long-tailed data. We propose a straightforward yet effective method for knowledge sharing, allowing tail classes to borrow from the rich information from classes with more abundant training data. More concretely, we propose modifications to existing class-conditional GAN architectures to ensure that the lower-resolution layers of the generator are trained entirely unconditionally while reserving class-conditional generation for the higher-resolution layers. Experiments on seve",
    "path": "papers/24/02/2402.17065.json",
    "total_tokens": 874,
    "translated_title": "驯服类别条件GAN中的长尾问题：通过在较低分辨率进行无条件训练进行知识共享",
    "translated_abstract": "尽管对于使用有限训练数据训练生成对抗网络（GANs）进行了广泛的研究，但从长尾训练分布生成图像的技术仍然相当未被探索。在存在不平衡的多类别训练数据时，GANs倾向于偏爱样本更多的类别，导致尾部类别的生成低质量且样本不够多样化。在这项研究中，我们旨在改进使用长尾数据训练类别条件GANs。我们提出了一种简单而有效的知识共享方法，允许尾部类别从训练数据更丰富的类别中借鉴丰富的信息。具体地，我们对现有的类别条件GAN架构进行了修改，以确保生成器的较低分辨率层完全无条件地进行训练，同时将类别条件生成保留给较高分辨率层。在多个实验中进行了实验",
    "tldr": "通过在较低分辨率进行无条件训练，允许长尾类别从信息更丰富的类别中共享知识，以改善长尾数据下类别条件GANs的训练",
    "en_tdlr": "Improving the training of class-conditional GANs with long-tailed data by allowing tail classes to borrow from richer classes through unconditional training at lower resolutions."
}