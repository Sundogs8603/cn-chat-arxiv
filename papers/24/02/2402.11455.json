{
    "title": "LoRA-Flow: Dynamic LoRA Fusion for Large Language Models in Generative Tasks",
    "abstract": "arXiv:2402.11455v1 Announce Type: new  Abstract: LoRA employs lightweight modules to customize large language models (LLMs) for each downstream task or domain, where different learned additional modules represent diverse skills. Combining existing LoRAs to address new tasks can enhance the reusability of learned LoRAs, particularly beneficial for tasks with limited annotated data. Most prior works on LoRA combination primarily rely on task-level weights for each involved LoRA, making different examples and tokens share the same LoRA weights. However, in generative tasks, different tokens may necessitate diverse skills to manage. Taking the Chinese math task as an example, understanding the problem description may depend more on the Chinese LoRA, while the calculation part may rely more on the math LoRA. To this end, we propose LoRA-Flow, which utilizes dynamic weights to adjust the impact of different LoRAs. The weights at each step are determined by a fusion gate with extremely few pa",
    "link": "https://arxiv.org/abs/2402.11455",
    "context": "Title: LoRA-Flow: Dynamic LoRA Fusion for Large Language Models in Generative Tasks\nAbstract: arXiv:2402.11455v1 Announce Type: new  Abstract: LoRA employs lightweight modules to customize large language models (LLMs) for each downstream task or domain, where different learned additional modules represent diverse skills. Combining existing LoRAs to address new tasks can enhance the reusability of learned LoRAs, particularly beneficial for tasks with limited annotated data. Most prior works on LoRA combination primarily rely on task-level weights for each involved LoRA, making different examples and tokens share the same LoRA weights. However, in generative tasks, different tokens may necessitate diverse skills to manage. Taking the Chinese math task as an example, understanding the problem description may depend more on the Chinese LoRA, while the calculation part may rely more on the math LoRA. To this end, we propose LoRA-Flow, which utilizes dynamic weights to adjust the impact of different LoRAs. The weights at each step are determined by a fusion gate with extremely few pa",
    "path": "papers/24/02/2402.11455.json",
    "total_tokens": 827,
    "translated_title": "LoRA-Flow: 大型语言模型在生成任务中的动态LoRA融合",
    "translated_abstract": "LoRA利用轻量级模块定制大型语言模型（LLMs）以适应每个下游任务或领域，在那里不同的学习的额外模块代表不同的技能。结合现有的LoRA来解决新任务可以增强学习的LoRA的可重用性，特别适用于数据有限的任务。大多数先前关于LoRA组合的工作主要依赖于每个涉及的LoRA的任务级别权重，使不同的示例和标记共享相同的LoRA权重。然而，在生成任务中，不同的标记可能需要不同的技能来管理。以中文数学任务为例，理解问题描述可能更依赖于中文LoRA，而计算部分可能更依赖于数学LoRA。为此，我们提出了LoRA-Flow，它利用动态权重来调整不同LoRA的影响。每个步骤的权重由具有极少参数的融合门确定。",
    "tldr": "LoRA-Flow提出了动态权重来调整不同LoRA的影响，以应对生成任务中不同标记所需的多样化技能。",
    "en_tdlr": "LoRA-Flow introduces dynamic weights to adjust the impact of different LoRAs, addressing the diverse skills needed for different tokens in generative tasks."
}