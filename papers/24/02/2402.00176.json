{
    "title": "Adversarial Quantum Machine Learning: An Information-Theoretic Generalization Analysis",
    "abstract": "In a manner analogous to their classical counterparts, quantum classifiers are vulnerable to adversarial attacks that perturb their inputs. A promising countermeasure is to train the quantum classifier by adopting an attack-aware, or adversarial, loss function. This paper studies the generalization properties of quantum classifiers that are adversarially trained against bounded-norm white-box attacks. Specifically, a quantum adversary maximizes the classifier's loss by transforming an input state $\\rho(x)$ into a state $\\lambda$ that is $\\epsilon$-close to the original state $\\rho(x)$ in $p$-Schatten distance. Under suitable assumptions on the quantum embedding $\\rho(x)$, we derive novel information-theoretic upper bounds on the generalization error of adversarially trained quantum classifiers for $p = 1$ and $p = \\infty$. The derived upper bounds consist of two terms: the first is an exponential function of the 2-R\\'enyi mutual information between classical data and quantum embedding,",
    "link": "https://arxiv.org/abs/2402.00176",
    "context": "Title: Adversarial Quantum Machine Learning: An Information-Theoretic Generalization Analysis\nAbstract: In a manner analogous to their classical counterparts, quantum classifiers are vulnerable to adversarial attacks that perturb their inputs. A promising countermeasure is to train the quantum classifier by adopting an attack-aware, or adversarial, loss function. This paper studies the generalization properties of quantum classifiers that are adversarially trained against bounded-norm white-box attacks. Specifically, a quantum adversary maximizes the classifier's loss by transforming an input state $\\rho(x)$ into a state $\\lambda$ that is $\\epsilon$-close to the original state $\\rho(x)$ in $p$-Schatten distance. Under suitable assumptions on the quantum embedding $\\rho(x)$, we derive novel information-theoretic upper bounds on the generalization error of adversarially trained quantum classifiers for $p = 1$ and $p = \\infty$. The derived upper bounds consist of two terms: the first is an exponential function of the 2-R\\'enyi mutual information between classical data and quantum embedding,",
    "path": "papers/24/02/2402.00176.json",
    "total_tokens": 809,
    "translated_title": "对抗性量子机器学习：信息论的泛化分析",
    "translated_abstract": "类似于经典分类器，量子分类器也容易受到扰动其输入的对抗性攻击。一种有希望的对策是采用一个攻击感知或对抗性的损失函数来训练量子分类器。本文研究了针对有界范数白盒攻击进行对抗性训练的量子分类器的泛化特性。具体来说，量子对手通过将输入状态ρ(x)转化为与原始状态ρ(x)在p-Schatten距离上ε接近的状态λ来最大化分类器的损失。在量子嵌入ρ(x)的适当假设下，我们对对抗性训练的量子分类器在p = 1和p = ∞时的泛化误差导出了新颖的信息论上界。导出的上界包含两个项：第一个是经典数据和量子嵌入之间的2-Rényi相互信息的指数函数，",
    "tldr": "本文研究了对抗性训练的量子分类器的泛化特性，并提出了新颖的信息论上界。"
}