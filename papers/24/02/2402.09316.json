{
    "title": "Only My Model On My Data: A Privacy Preserving Approach Protecting one Model and Deceiving Unauthorized Black-Box Models",
    "abstract": "arXiv:2402.09316v1 Announce Type: cross Abstract: Deep neural networks are extensively applied to real-world tasks, such as face recognition and medical image classification, where privacy and data protection are critical. Image data, if not protected, can be exploited to infer personal or contextual information. Existing privacy preservation methods, like encryption, generate perturbed images that are unrecognizable to even humans. Adversarial attack approaches prohibit automated inference even for authorized stakeholders, limiting practical incentives for commercial and widespread adaptation. This pioneering study tackles an unexplored practical privacy preservation use case by generating human-perceivable images that maintain accurate inference by an authorized model while evading other unauthorized black-box models of similar or dissimilar objectives, and addresses the previous research gaps. The datasets employed are ImageNet, for image classification, Celeba-HQ dataset, for ident",
    "link": "https://arxiv.org/abs/2402.09316",
    "context": "Title: Only My Model On My Data: A Privacy Preserving Approach Protecting one Model and Deceiving Unauthorized Black-Box Models\nAbstract: arXiv:2402.09316v1 Announce Type: cross Abstract: Deep neural networks are extensively applied to real-world tasks, such as face recognition and medical image classification, where privacy and data protection are critical. Image data, if not protected, can be exploited to infer personal or contextual information. Existing privacy preservation methods, like encryption, generate perturbed images that are unrecognizable to even humans. Adversarial attack approaches prohibit automated inference even for authorized stakeholders, limiting practical incentives for commercial and widespread adaptation. This pioneering study tackles an unexplored practical privacy preservation use case by generating human-perceivable images that maintain accurate inference by an authorized model while evading other unauthorized black-box models of similar or dissimilar objectives, and addresses the previous research gaps. The datasets employed are ImageNet, for image classification, Celeba-HQ dataset, for ident",
    "path": "papers/24/02/2402.09316.json",
    "total_tokens": 875,
    "translated_title": "只有我的模型在我的数据上：一种保护模型和欺骗未经授权的黑盒模型的隐私保护方法",
    "translated_abstract": "深度神经网络广泛应用于面部识别和医学图像分类等现实世界任务中，隐私和数据保护至关重要。如果不保护图像数据，可能会被利用来推断个人或环境信息。现有的隐私保护方法，如加密，生成的扰动图像即使对人类来说也无法识别。对抗性攻击方法禁止授权利益相关者进行自动推理，限制了商业和广泛适用的实际激励。这项开创性研究通过生成可被人类感知的图像，在经过授权的模型中保持准确的推理，同时规避其他类似或不同目标的未经授权的黑盒模型，并解决了之前的研究空白。所使用的数据集包括ImageNet（用于图像分类）和Celeba-HQ数据集（用于身份识别）。",
    "tldr": "这项开创性研究提出了一种隐私保护方法，通过生成可被人类感知的图像，在经过授权的模型中保持准确的推理，同时规避其他未经授权的黑盒模型的推理，填补了之前的研究空白。",
    "en_tdlr": "This groundbreaking study proposes a privacy preserving approach that generates human-perceivable images to maintain accurate inference by an authorized model while evading inference by other unauthorized black-box models, filling previous research gaps."
}