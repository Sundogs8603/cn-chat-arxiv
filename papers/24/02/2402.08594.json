{
    "title": "Bayesian Multi-Task Transfer Learning for Soft Prompt Tuning",
    "abstract": "Prompt tuning, in which prompts are optimized to adapt large-scale pre-trained language models to downstream tasks instead of fine-tuning the full model parameters, has been shown to be particularly effective when the prompts are trained in a multi-task transfer learning setting. These methods generally involve individually training prompts for each source task and then aggregating them to provide the initialization of the prompt for the target task. However, this approach critically ignores the fact that some of the source tasks could be negatively or positively interfering with each other. We argue that when we extract knowledge from source tasks via training source prompts, we need to consider this correlation among source tasks for better transfer to target tasks. To this end, we propose a Bayesian approach where we work with the posterior distribution of prompts across source tasks. We obtain representative source prompts corresponding to the samples from the posterior utilizing S",
    "link": "https://arxiv.org/abs/2402.08594",
    "context": "Title: Bayesian Multi-Task Transfer Learning for Soft Prompt Tuning\nAbstract: Prompt tuning, in which prompts are optimized to adapt large-scale pre-trained language models to downstream tasks instead of fine-tuning the full model parameters, has been shown to be particularly effective when the prompts are trained in a multi-task transfer learning setting. These methods generally involve individually training prompts for each source task and then aggregating them to provide the initialization of the prompt for the target task. However, this approach critically ignores the fact that some of the source tasks could be negatively or positively interfering with each other. We argue that when we extract knowledge from source tasks via training source prompts, we need to consider this correlation among source tasks for better transfer to target tasks. To this end, we propose a Bayesian approach where we work with the posterior distribution of prompts across source tasks. We obtain representative source prompts corresponding to the samples from the posterior utilizing S",
    "path": "papers/24/02/2402.08594.json",
    "total_tokens": 794,
    "translated_title": "贝叶斯多任务迁移学习用于软提示调整",
    "translated_abstract": "提示调整是一种优化预训练语言模型的方法，通过优化提示来适应下游任务，而不是微调整个模型参数。当这些提示在多任务迁移学习设置下进行训练时，已经证明其特别有效。这些方法通常涉及为每个源任务单独训练提示，然后聚合它们以提供目标任务的提示初始化。然而，这种方法忽视了一些源任务之间可能存在的负面或正面干扰。我们认为，当我们通过训练源提示从源任务中提取知识时，需要考虑源任务之间的相关性，以实现更好的向目标任务的迁移。为此，我们提出了一种基于贝叶斯方法的思路，通过工作在提示在源任务之间的后验分布上。我们利用从后验中提取的样本获得代表性的源提示。",
    "tldr": "本文提出了一种基于贝叶斯方法的多任务迁移学习框架，用于软提示调整。通过考虑源任务之间的相关性，我们可以提高提示在目标任务上的迁移效果。",
    "en_tdlr": "This paper presents a Bayesian multi-task transfer learning framework for soft prompt tuning. By considering the correlation among source tasks, we aim to improve the transfer effectiveness of prompts to target tasks."
}