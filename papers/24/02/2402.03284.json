{
    "title": "Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using Large Language Models",
    "abstract": "Effective interlocutors account for the uncertain goals, beliefs, and emotions of others. But even the best human conversationalist cannot perfectly anticipate the trajectory of a dialogue. How well can language models represent inherent uncertainty in conversations? We propose FortUne Dial, an expansion of the long-standing \"conversation forecasting\" task: instead of just accuracy, evaluation is conducted with uncertainty-aware metrics, effectively enabling abstention on individual instances. We study two ways in which language models potentially represent outcome uncertainty (internally, using scores and directly, using tokens) and propose fine-tuning strategies to improve calibration of both representations. Experiments on eight difficult negotiation corpora demonstrate that our proposed fine-tuning strategies (a traditional supervision strategy and an off-policy reinforcement learning strategy) can calibrate smaller open-source models to compete with pre-trained models 10x their si",
    "link": "https://arxiv.org/abs/2402.03284",
    "context": "Title: Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using Large Language Models\nAbstract: Effective interlocutors account for the uncertain goals, beliefs, and emotions of others. But even the best human conversationalist cannot perfectly anticipate the trajectory of a dialogue. How well can language models represent inherent uncertainty in conversations? We propose FortUne Dial, an expansion of the long-standing \"conversation forecasting\" task: instead of just accuracy, evaluation is conducted with uncertainty-aware metrics, effectively enabling abstention on individual instances. We study two ways in which language models potentially represent outcome uncertainty (internally, using scores and directly, using tokens) and propose fine-tuning strategies to improve calibration of both representations. Experiments on eight difficult negotiation corpora demonstrate that our proposed fine-tuning strategies (a traditional supervision strategy and an off-policy reinforcement learning strategy) can calibrate smaller open-source models to compete with pre-trained models 10x their si",
    "path": "papers/24/02/2402.03284.json",
    "total_tokens": 885,
    "translated_title": "交易，还是不交易（或者谁知道）？使用大型语言模型预测对话中的不确定性",
    "translated_abstract": "有效的对话者考虑他人的不确定目标、信念和情绪。但即使是最佳的人类对话者也无法完美地预测对话的轨迹。语言模型能够多好地表示对话中固有的不确定性？我们提出了FortUne Dial，这是“对话预测”任务的扩展：评估不仅仅以准确度为标准，还采用了对不确定性敏感的度量方法，有效地使个别实例可以放弃。我们研究了语言模型可能表示结果不确定性的两种方式（内部使用分数和直接使用令牌），并提出了改进两种表示的校准的微调策略。对八个困难的谈判语料库进行的实验表明，我们提出的微调策略（一种传统的监督策略和一种离线策略强化学习策略）可以使较小的开源模型校准得上与其尺寸相当的预训练模型。",
    "tldr": "本论文研究了如何用语言模型来表示对话中的不确定性，并提出了改进模型校准的微调策略。实验证明，这些策略可以使较小的模型具备与大型预训练模型相当的性能。",
    "en_tdlr": "This paper investigates how to represent uncertainty in conversations using language models and proposes fine-tuning strategies to improve model calibration. Experiments show that these strategies can enable smaller models to compete with large pre-trained models."
}