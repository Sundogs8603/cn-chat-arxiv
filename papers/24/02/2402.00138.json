{
    "title": "Decomposable Submodular Maximization in Federated Setting",
    "abstract": "Submodular functions, as well as the sub-class of decomposable submodular functions, and their optimization appear in a wide range of applications in machine learning, recommendation systems, and welfare maximization. However, optimization of decomposable submodular functions with millions of component functions is computationally prohibitive. Furthermore, the component functions may be private (they might represent user preference function, for example) and cannot be widely shared. To address these issues, we propose a {\\em federated optimization} setting for decomposable submodular optimization. In this setting, clients have their own preference functions, and a weighted sum of these preferences needs to be maximized. We implement the popular {\\em continuous greedy} algorithm in this setting where clients take parallel small local steps towards the local solution and then the local changes are aggregated at a central server. To address the large number of clients, the aggregation is ",
    "link": "https://arxiv.org/abs/2402.00138",
    "context": "Title: Decomposable Submodular Maximization in Federated Setting\nAbstract: Submodular functions, as well as the sub-class of decomposable submodular functions, and their optimization appear in a wide range of applications in machine learning, recommendation systems, and welfare maximization. However, optimization of decomposable submodular functions with millions of component functions is computationally prohibitive. Furthermore, the component functions may be private (they might represent user preference function, for example) and cannot be widely shared. To address these issues, we propose a {\\em federated optimization} setting for decomposable submodular optimization. In this setting, clients have their own preference functions, and a weighted sum of these preferences needs to be maximized. We implement the popular {\\em continuous greedy} algorithm in this setting where clients take parallel small local steps towards the local solution and then the local changes are aggregated at a central server. To address the large number of clients, the aggregation is ",
    "path": "papers/24/02/2402.00138.json",
    "total_tokens": 825,
    "translated_title": "在联邦设置中可分解子模函数的最大化",
    "translated_abstract": "在机器学习、推荐系统和福利最大化等众多应用中，子模函数以及可分解子模函数及其优化问题都得到了广泛应用。然而，对于具有数百万个组分函数的可分解子模函数的优化问题，在计算上是不可行的。此外，组分函数可能是私有的（例如可能表示用户偏好函数），不能广泛共享。为了解决这些问题，我们提出了一种适用于可分解子模函数优化的“联邦优化”设置。在这种设置下，客户端拥有自己的偏好函数，需要最大化这些偏好的加权和。我们在该设置中实现了流行的“连续贪婪”算法，其中客户端以并行的方式朝着局部解向前迈出小的局部步骤，然后将局部变化聚合到一个中央服务器上。",
    "tldr": "该论文提出了一种联邦优化设置用于在计算上不可行的可分解子模函数优化问题中。在这个设置中，客户端拥有私有的组件函数，通过并行计算和集中聚合的方式来求解最大化问题。",
    "en_tdlr": "This paper proposes a federated optimization setting for computationally prohibitive decomposable submodular maximization problems. In this setting, clients have private component functions which are aggregated through parallel computation and centralized aggregation to solve the maximization problem."
}