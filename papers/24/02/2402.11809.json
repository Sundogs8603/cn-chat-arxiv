{
    "title": "Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding",
    "abstract": "arXiv:2402.11809v1 Announce Type: cross  Abstract: This research aims to accelerate the inference speed of large language models (LLMs) with billions of parameters. We propose \\textbf{S}mart \\textbf{P}arallel \\textbf{A}uto-\\textbf{C}orrect d\\textbf{E}coding (SPACE), an innovative approach designed for achieving lossless acceleration of LLMs. By integrating semi-autoregressive inference and speculative decoding capabilities, SPACE uniquely enables autoregressive LLMs to parallelize token generation and verification. This is realized through a specialized semi-autoregressive supervised fine-tuning process that equips existing LLMs with the ability to simultaneously predict multiple tokens. Additionally, an auto-correct decoding algorithm facilitates the simultaneous generation and verification of token sequences within a single model invocation. Through extensive experiments on a range of LLMs, SPACE has demonstrated inference speedup ranging from 2.7x-4.0x on HumanEval-X while maintaini",
    "link": "https://arxiv.org/abs/2402.11809",
    "context": "Title: Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding\nAbstract: arXiv:2402.11809v1 Announce Type: cross  Abstract: This research aims to accelerate the inference speed of large language models (LLMs) with billions of parameters. We propose \\textbf{S}mart \\textbf{P}arallel \\textbf{A}uto-\\textbf{C}orrect d\\textbf{E}coding (SPACE), an innovative approach designed for achieving lossless acceleration of LLMs. By integrating semi-autoregressive inference and speculative decoding capabilities, SPACE uniquely enables autoregressive LLMs to parallelize token generation and verification. This is realized through a specialized semi-autoregressive supervised fine-tuning process that equips existing LLMs with the ability to simultaneously predict multiple tokens. Additionally, an auto-correct decoding algorithm facilitates the simultaneous generation and verification of token sequences within a single model invocation. Through extensive experiments on a range of LLMs, SPACE has demonstrated inference speedup ranging from 2.7x-4.0x on HumanEval-X while maintaini",
    "path": "papers/24/02/2402.11809.json",
    "total_tokens": 841,
    "translated_title": "智能并行自动纠错解码：加速大型语言模型推理",
    "translated_abstract": "这项研究旨在加速拥有数十亿参数的大型语言模型（LLMs）的推理速度。我们提出了“智能并行自动纠错解码”（SPACE），这是一种创新方法，旨在实现LLMs的无损加速。通过集成半自回归推理和猜测解码能力，SPACE独特地使自回归LLMs能够并行生成和验证令牌。这是通过专门的半自回归监督微调过程实现的，该过程使现有LLMs具有同时预测多个令牌的能力。此外，一种自动纠错解码算法促进了单个模型调用内令牌序列的同时生成和验证。通过在一系列LLMs上进行广泛实验证明，SPACE在HumanEval-X上表现出2.7倍至4.0倍的推理加速。",
    "tldr": "提出了Smart Parallel Auto-Correct Decoding (SPACE)方法，通过集成半自回归推理和猜测解码，实现了大型语言模型推理加速和并行生成验证令牌的功能。",
    "en_tdlr": "Introduced Smart Parallel Auto-Correct Decoding (SPACE) method, utilizing semi-autoregressive inference and speculative decoding to accelerate large language model inference and allow parallel token generation and verification."
}