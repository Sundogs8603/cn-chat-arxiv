{
    "title": "ScreenAI: A Vision-Language Model for UI and Infographics Understanding",
    "abstract": "Screen user interfaces (UIs) and infographics, sharing similar visual language and design principles, play important roles in human communication and human-machine interaction. We introduce ScreenAI, a vision-language model that specializes in UI and infographics understanding. Our model improves upon the PaLI architecture with the flexible patching strategy of pix2struct and is trained on a unique mixture of datasets. At the heart of this mixture is a novel screen annotation task in which the model has to identify the type and location of UI elements. We use these text annotations to describe screens to Large Language Models and automatically generate question-answering (QA), UI navigation, and summarization training datasets at scale. We run ablation studies to demonstrate the impact of these design choices. At only 5B parameters, ScreenAI achieves new state-of-the-artresults on UI- and infographics-based tasks (Multi-page DocVQA, WebSRC, MoTIF and Widget Captioning), and new best-in",
    "link": "https://arxiv.org/abs/2402.04615",
    "context": "Title: ScreenAI: A Vision-Language Model for UI and Infographics Understanding\nAbstract: Screen user interfaces (UIs) and infographics, sharing similar visual language and design principles, play important roles in human communication and human-machine interaction. We introduce ScreenAI, a vision-language model that specializes in UI and infographics understanding. Our model improves upon the PaLI architecture with the flexible patching strategy of pix2struct and is trained on a unique mixture of datasets. At the heart of this mixture is a novel screen annotation task in which the model has to identify the type and location of UI elements. We use these text annotations to describe screens to Large Language Models and automatically generate question-answering (QA), UI navigation, and summarization training datasets at scale. We run ablation studies to demonstrate the impact of these design choices. At only 5B parameters, ScreenAI achieves new state-of-the-artresults on UI- and infographics-based tasks (Multi-page DocVQA, WebSRC, MoTIF and Widget Captioning), and new best-in",
    "path": "papers/24/02/2402.04615.json",
    "total_tokens": 903,
    "translated_title": "ScreenAI: 用于UI和信息图表理解的视觉-语言模型",
    "translated_abstract": "屏幕用户界面（UI）和信息图表在人类沟通和人机交互中起着重要作用，并且共享相似的视觉语言和设计原则。我们介绍了ScreenAI，这是一个专门用于UI和信息图表理解的视觉-语言模型。我们的模型改进了PaLI架构，采用了pix2struct的灵活修补策略，并经过独特的数据集训练。在这个数据集的核心是一项新颖的屏幕注解任务，模型必须识别UI元素的类型和位置。我们使用这些文本注解来描述屏幕，并使用大规模的语言模型自动生成问答（QA），UI导航和摘要训练数据集。我们进行了消融研究以展示这些设计选择的影响。在仅有5B参数的情况下，ScreenAI在基于UI和信息图表的任务（多页文档VQA，WebSRC，MoTIF和Widget字幕）上取得了最新的最优结果，并且达到了最好的效果。",
    "tldr": "ScreenAI是一个专注于UI和信息图表理解的视觉-语言模型，通过灵活的修补策略和独特的数据集训练，以及针对UI元素的屏幕注解任务的处理，实现了在多个任务上的新的最优结果。"
}