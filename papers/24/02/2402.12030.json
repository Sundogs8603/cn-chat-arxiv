{
    "title": "Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs",
    "abstract": "arXiv:2402.12030v1 Announce Type: new  Abstract: Deploying large language models (LLMs) of several billion parameters can be impractical in most industrial use cases due to constraints such as cost, latency limitations, and hardware accessibility. Knowledge distillation (KD) offers a solution by compressing knowledge from resource-intensive large models to smaller ones. Various strategies exist, some relying on the text generated by the teacher model and optionally utilizing his logits to enhance learning. However, these methods based on logits often require both teacher and student models to share the same tokenizer, limiting their applicability across different LLM families. In this paper, we introduce Universal Logit Distillation (ULD) loss, grounded in optimal transport, to address this limitation. Our experimental results demonstrate the effectiveness of ULD loss in enabling distillation across models with different architectures and tokenizers, paving the way to a more widespread",
    "link": "https://arxiv.org/abs/2402.12030",
    "context": "Title: Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs\nAbstract: arXiv:2402.12030v1 Announce Type: new  Abstract: Deploying large language models (LLMs) of several billion parameters can be impractical in most industrial use cases due to constraints such as cost, latency limitations, and hardware accessibility. Knowledge distillation (KD) offers a solution by compressing knowledge from resource-intensive large models to smaller ones. Various strategies exist, some relying on the text generated by the teacher model and optionally utilizing his logits to enhance learning. However, these methods based on logits often require both teacher and student models to share the same tokenizer, limiting their applicability across different LLM families. In this paper, we introduce Universal Logit Distillation (ULD) loss, grounded in optimal transport, to address this limitation. Our experimental results demonstrate the effectiveness of ULD loss in enabling distillation across models with different architectures and tokenizers, paving the way to a more widespread",
    "path": "papers/24/02/2402.12030.json",
    "total_tokens": 858,
    "translated_title": "跨分词器蒸馏：用于LLM的通用logit蒸馏损失",
    "translated_abstract": "部署几十亿参数的大型语言模型 (LLMs) 在大多数工业应用中可能并不切实际，原因是诸如成本、延迟限制和硬件可访问性等约束。知识蒸馏 (KD) 通过将资源密集型大模型的知识压缩到较小模型中提供了解决方案。存在多种策略，一些依赖于教师模型生成的文本，并可选择性地利用其logits来增强学习。然而，基于logits的这些方法通常要求教师和学生模型共享相同的分词器，限制了它们在不同LLM系列中的适用性。本文引入了基于最优传输的通用logit蒸馏 (ULD) 损失，以解决这一限制。我们的实验结果显示了ULD损失在启用不同架构和分词器的模型之间的蒸馏方面的有效性，为更广泛的应用铺平了道路。",
    "tldr": "介绍了基于最优传输的通用logit蒸馏 (ULD) 损失，用于解决不同架构和分词器模型之间蒸馏的限制。",
    "en_tdlr": "Introducing the Universal Logit Distillation (ULD) loss based on optimal transport to address the limitation of distilling across models with different architectures and tokenizers."
}