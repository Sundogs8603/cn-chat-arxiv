{
    "title": "Confronting Reward Overoptimization for Diffusion Models: A Perspective of Inductive and Primacy Biases",
    "abstract": "Bridging the gap between diffusion models and human preferences is crucial for their integration into practical generative workflows. While optimizing downstream reward models has emerged as a promising alignment strategy, concerns arise regarding the risk of excessive optimization with learned reward models, which potentially compromises ground-truth performance. In this work, we confront the reward overoptimization problem in diffusion model alignment through the lenses of both inductive and primacy biases. We first identify the divergence of current methods from the temporal inductive bias inherent in the multi-step denoising process of diffusion models as a potential source of overoptimization. Then, we surprisingly discover that dormant neurons in our critic model act as a regularization against overoptimization, while active neurons reflect primacy bias in this setting. Motivated by these observations, we propose Temporal Diffusion Policy Optimization with critic active neuron Re",
    "link": "https://arxiv.org/abs/2402.08552",
    "context": "Title: Confronting Reward Overoptimization for Diffusion Models: A Perspective of Inductive and Primacy Biases\nAbstract: Bridging the gap between diffusion models and human preferences is crucial for their integration into practical generative workflows. While optimizing downstream reward models has emerged as a promising alignment strategy, concerns arise regarding the risk of excessive optimization with learned reward models, which potentially compromises ground-truth performance. In this work, we confront the reward overoptimization problem in diffusion model alignment through the lenses of both inductive and primacy biases. We first identify the divergence of current methods from the temporal inductive bias inherent in the multi-step denoising process of diffusion models as a potential source of overoptimization. Then, we surprisingly discover that dormant neurons in our critic model act as a regularization against overoptimization, while active neurons reflect primacy bias in this setting. Motivated by these observations, we propose Temporal Diffusion Policy Optimization with critic active neuron Re",
    "path": "papers/24/02/2402.08552.json",
    "total_tokens": 997,
    "translated_title": "面对扩散模型的奖励过度优化问题：归纳和优先偏差的视角",
    "translated_abstract": "将扩散模型与人类偏好融合是将其应用于实际生成工作流程中的关键。虽然通过优化下游奖励模型已经成为一种有前景的对齐策略，但同时也存在学习奖励模型时过度优化的风险，这可能会损害地面真实性能。在本研究中，我们从归纳和优先偏差的角度来对付扩散模型对齐中的奖励过度优化问题。首先，我们发现当前方法与扩散模型的多步去噪过程中固有的时间归纳偏差存在分歧，这可能是过度优化的一个潜在来源。然后，我们令人惊讶地发现，我们评论模型中的沉睡神经元充当一种对抗过度优化的正则化手段，而活跃神经元则反映了这个设置中的优先偏差。受这些观察的启发，我们提出了带有评论模型活跃神经元的时间扩散策略优化。",
    "tldr": "本论文探讨了扩散模型对齐中的奖励过度优化问题，并从归纳和优先偏差的角度提出了解决方案。作者通过发现当前方法与扩散模型的时间归纳偏差分歧，以及评论模型中的沉睡神经元和活跃神经元对抗过度优化，提出了一种新的时间扩散策略优化方法。",
    "en_tdlr": "This paper discusses the problem of reward overoptimization in diffusion model alignment and proposes a solution from the perspectives of inductive and primacy biases. The authors propose a new method of temporal diffusion policy optimization by addressing the divergence between current methods and the temporal inductive bias inherent in diffusion models, and leveraging dormant and active neurons in the critic model to counter overoptimization."
}