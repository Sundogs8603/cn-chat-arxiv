{
    "title": "Accelerating Convergence of Stein Variational Gradient Descent via Deep Unfolding",
    "abstract": "arXiv:2402.15125v1 Announce Type: new  Abstract: Stein variational gradient descent (SVGD) is a prominent particle-based variational inference method used for sampling a target distribution. SVGD has attracted interest for application in machine-learning techniques such as Bayesian inference. In this paper, we propose novel trainable algorithms that incorporate a deep-learning technique called deep unfolding,into SVGD. This approach facilitates the learning of the internal parameters of SVGD, thereby accelerating its convergence speed. To evaluate the proposed trainable SVGD algorithms, we conducted numerical simulations of three tasks: sampling a one-dimensional Gaussian mixture, performing Bayesian logistic regression, and learning Bayesian neural networks. The results show that our proposed algorithms exhibit faster convergence than the conventional variants of SVGD.",
    "link": "https://arxiv.org/abs/2402.15125",
    "context": "Title: Accelerating Convergence of Stein Variational Gradient Descent via Deep Unfolding\nAbstract: arXiv:2402.15125v1 Announce Type: new  Abstract: Stein variational gradient descent (SVGD) is a prominent particle-based variational inference method used for sampling a target distribution. SVGD has attracted interest for application in machine-learning techniques such as Bayesian inference. In this paper, we propose novel trainable algorithms that incorporate a deep-learning technique called deep unfolding,into SVGD. This approach facilitates the learning of the internal parameters of SVGD, thereby accelerating its convergence speed. To evaluate the proposed trainable SVGD algorithms, we conducted numerical simulations of three tasks: sampling a one-dimensional Gaussian mixture, performing Bayesian logistic regression, and learning Bayesian neural networks. The results show that our proposed algorithms exhibit faster convergence than the conventional variants of SVGD.",
    "path": "papers/24/02/2402.15125.json",
    "total_tokens": 781,
    "translated_title": "通过深度展开加速斯坦变分梯度下降的收敛速度",
    "translated_abstract": "Stein变分梯度下降（SVGD）是一种著名的基于粒子的变分推断方法，用于对目标分布进行采样。SVGD已经引起了人们的兴趣，应用于贝叶斯推理等机器学习技术。本文提出了将一种名为深度展开的深度学习技术融入SVGD的新型可训练算法。这种方法促进了对SVGD的内部参数进行学习，从而加速了其收敛速度。为了评估所提出的可训练SVGD算法，我们对三项任务进行了数值模拟：对一维高斯混合进行采样，进行贝叶斯逻辑回归以及学习贝叶斯神经网络。结果表明，我们提出的算法比SVGD的传统变体表现出更快的收敛速度。",
    "tldr": "通过深度展开技术，本文提出的可训练SVGD算法加速了其收敛速度，相比传统SVGD变体表现出更快的收敛速度。",
    "en_tdlr": "The proposed trainable SVGD algorithm in this paper accelerates its convergence speed by incorporating deep unfolding technique, showing faster convergence compared to conventional variants of SVGD."
}