{
    "title": "Transformer-based Causal Language Models Perform Clustering",
    "abstract": "arXiv:2402.12151v1 Announce Type: cross  Abstract: Even though large language models (LLMs) have demonstrated remarkable capability in solving various natural language tasks, the capability of an LLM to follow human instructions is still a concern. Recent works have shown great improvements in the instruction-following capability via additional training for instruction-following tasks. However, the mechanisms responsible for effective instruction-following capabilities remain inadequately understood. Here, we introduce a simplified instruction-following task and use synthetic datasets to analyze a Transformer-based causal language model. Our findings suggest that the model learns task-specific information by clustering data within its hidden space, with this clustering process evolving dynamically during learning. We also demonstrate how this phenomenon assists the model in handling unseen instances and validate our results in a more realistic setting.",
    "link": "https://arxiv.org/abs/2402.12151",
    "context": "Title: Transformer-based Causal Language Models Perform Clustering\nAbstract: arXiv:2402.12151v1 Announce Type: cross  Abstract: Even though large language models (LLMs) have demonstrated remarkable capability in solving various natural language tasks, the capability of an LLM to follow human instructions is still a concern. Recent works have shown great improvements in the instruction-following capability via additional training for instruction-following tasks. However, the mechanisms responsible for effective instruction-following capabilities remain inadequately understood. Here, we introduce a simplified instruction-following task and use synthetic datasets to analyze a Transformer-based causal language model. Our findings suggest that the model learns task-specific information by clustering data within its hidden space, with this clustering process evolving dynamically during learning. We also demonstrate how this phenomenon assists the model in handling unseen instances and validate our results in a more realistic setting.",
    "path": "papers/24/02/2402.12151.json",
    "total_tokens": 761,
    "translated_title": "基于Transformer的因果语言模型执行聚类",
    "translated_abstract": "即使大型语言模型(LLMs)已经展示出在解决各种自然语言任务方面的出色能力，LLM遵循人类指令的能力仍然是一个问题。最近的研究通过额外训练指令遵循任务已经显示出很大改进，然而，导致有效指令遵循能力的机制仍未得到充分理解。本文介绍了一个简化的指令遵循任务，并使用合成数据集分析了基于Transformer的因果语言模型。我们的发现表明，模型通过在其隐藏空间内对数据进行聚类而学习任务特定信息，这种聚类过程在学习过程中动态演变。我们还演示了这种现象如何帮助模型处理未见实例，并在更现实的环境中验证了我们的结果。",
    "tldr": "Transformer-based因果语言模型通过在隐藏空间内对数据进行聚类来学习任务特定信息，这种聚类过程在学习中动态演变，并有助于处理未见实例。",
    "en_tdlr": "Transformer-based causal language models learn task-specific information by clustering data within its hidden space, with this clustering process evolving dynamically during learning and assisting in handling unseen instances."
}