{
    "title": "Multi-Timescale Ensemble Q-learning for Markov Decision Process Policy Optimization",
    "abstract": "Reinforcement learning (RL) is a classical tool to solve network control or policy optimization problems in unknown environments. The original Q-learning suffers from performance and complexity challenges across very large networks. Herein, a novel model-free ensemble reinforcement learning algorithm which adapts the classical Q-learning is proposed to handle these challenges for networks which admit Markov decision process (MDP) models. Multiple Q-learning algorithms are run on multiple, distinct, synthetically created and structurally related Markovian environments in parallel; the outputs are fused using an adaptive weighting mechanism based on the Jensen-Shannon divergence (JSD) to obtain an approximately optimal policy with low complexity. The theoretical justification of the algorithm, including the convergence of key statistics and Q-functions are provided. Numerical results across several network models show that the proposed algorithm can achieve up to 55% less average policy ",
    "link": "https://arxiv.org/abs/2402.05476",
    "context": "Title: Multi-Timescale Ensemble Q-learning for Markov Decision Process Policy Optimization\nAbstract: Reinforcement learning (RL) is a classical tool to solve network control or policy optimization problems in unknown environments. The original Q-learning suffers from performance and complexity challenges across very large networks. Herein, a novel model-free ensemble reinforcement learning algorithm which adapts the classical Q-learning is proposed to handle these challenges for networks which admit Markov decision process (MDP) models. Multiple Q-learning algorithms are run on multiple, distinct, synthetically created and structurally related Markovian environments in parallel; the outputs are fused using an adaptive weighting mechanism based on the Jensen-Shannon divergence (JSD) to obtain an approximately optimal policy with low complexity. The theoretical justification of the algorithm, including the convergence of key statistics and Q-functions are provided. Numerical results across several network models show that the proposed algorithm can achieve up to 55% less average policy ",
    "path": "papers/24/02/2402.05476.json",
    "total_tokens": 920,
    "translated_title": "多时间尺度集合Q-learning用于马尔可夫决策过程政策优化",
    "translated_abstract": "强化学习是解决未知环境下网络控制或策略优化问题的经典工具。原始的Q-learning在非常大的网络中存在性能和复杂性挑战。本文提出了一种新颖的无模型集合强化学习算法，用于处理具有马尔可夫决策过程（MDP）模型的网络中的这些挑战。多个Q-learning算法在多个独立的、合成的和结构相关的马尔可夫环境中并行运行；通过基于Jensen-Shannon散度（JSD）的自适应加权机制来融合输出，以获得具有低复杂性的近似最优策略。论文还提供了算法的理论证明，包括关键统计量和Q函数的收敛性。在多个网络模型上的数值结果显示，所提出的算法可以实现平均策略减少多达55%。",
    "tldr": "本文提出了一种新颖的无模型集合强化学习算法，用于解决大型网络中的性能和复杂性挑战。算法使用多个Q-learning算法在多个马尔可夫环境中并行运行，并通过自适应加权融合输出，提供近似最优策略。实验结果表明，该算法可以实现多达55%的平均策略减少。",
    "en_tdlr": "This paper proposes a novel model-free ensemble reinforcement learning algorithm to address the challenges of performance and complexity in large networks. The algorithm runs multiple Q-learning algorithms in parallel on multiple Markov environments, and fuses the outputs using adaptive weighting to provide an approximately optimal policy. Experimental results show up to 55% reduction in average policy."
}