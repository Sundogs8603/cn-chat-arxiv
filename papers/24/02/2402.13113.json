{
    "title": "When Only Time Will Tell: Interpreting How Transformers Process Local Ambiguities Through the Lens of Restart-Incrementality",
    "abstract": "arXiv:2402.13113v1 Announce Type: new  Abstract: Incremental models that process sentences one token at a time will sometimes encounter points where more than one interpretation is possible. Causal models are forced to output one interpretation and continue, whereas models that can revise may edit their previous output as the ambiguity is resolved. In this work, we look at how restart-incremental Transformers build and update internal states, in an effort to shed light on what processes cause revisions not viable in autoregressive models. We propose an interpretable way to analyse the incremental states, showing that their sequential structure encodes information on the garden path effect and its resolution. Our method brings insights on various bidirectional encoders for contextualised meaning representation and dependency parsing, contributing to show their advantage over causal models when it comes to revisions.",
    "link": "https://arxiv.org/abs/2402.13113",
    "context": "Title: When Only Time Will Tell: Interpreting How Transformers Process Local Ambiguities Through the Lens of Restart-Incrementality\nAbstract: arXiv:2402.13113v1 Announce Type: new  Abstract: Incremental models that process sentences one token at a time will sometimes encounter points where more than one interpretation is possible. Causal models are forced to output one interpretation and continue, whereas models that can revise may edit their previous output as the ambiguity is resolved. In this work, we look at how restart-incremental Transformers build and update internal states, in an effort to shed light on what processes cause revisions not viable in autoregressive models. We propose an interpretable way to analyse the incremental states, showing that their sequential structure encodes information on the garden path effect and its resolution. Our method brings insights on various bidirectional encoders for contextualised meaning representation and dependency parsing, contributing to show their advantage over causal models when it comes to revisions.",
    "path": "papers/24/02/2402.13113.json",
    "total_tokens": 861,
    "translated_title": "当只有时间能告诉: 通过重启增量性的视角解释Transformer如何处理局部歧义",
    "translated_abstract": "处理一次一个令牌的增量模型有时会遇到可能有多种解释的点。因果模型被迫输出一个解释并继续，而可以修订的模型在消除歧义时可能会编辑其先前的输出。在这项工作中，我们研究了重启增量式Transformer如何构建和更新内部状态，以阐明导致不适用于自回归模型的修订的过程是什么。我们提出了一种可解释的方法来分析增量状态，显示它们的顺序结构编码了关于偏误效应及其解决方式的信息。我们的方法为分析上下文化意义表示和依赖解析的各种双向编码器带来了见解，有助于展示它们在涉及修订时相对于因果模型的优势。",
    "tldr": "研究了如何重启增量式Transformer构建和更新内部状态，揭示了增量状态的顺序结构如何编码关于偏误效应及其解决方式的信息，为分析上下文化意义表示和依赖解析的双向编码器带来见解，并显示它们在修订方面的优势。",
    "en_tdlr": "Investigated how restart-incremental Transformers build and update internal states, revealing how the sequential structure of incremental states encodes information about the garden path effect and its resolution, providing insights into bidirectional encoders for contextualized meaning representation and dependency parsing, showcasing their advantage in revisions over causal models."
}