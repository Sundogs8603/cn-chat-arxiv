{
    "title": "Accelerating Inverse Reinforcement Learning with Expert Bootstrapping",
    "abstract": "Existing inverse reinforcement learning methods (e.g. MaxEntIRL, $f$-IRL) search over candidate reward functions and solve a reinforcement learning problem in the inner loop. This creates a rather strange inversion where a harder problem, reinforcement learning, is in the inner loop of a presumably easier problem, imitation learning. In this work, we show that better utilization of expert demonstrations can reduce the need for hard exploration in the inner RL loop, hence accelerating learning. Specifically, we propose two simple recipes: (1) placing expert transitions into the replay buffer of the inner RL algorithm (e.g. Soft-Actor Critic) which directly informs the learner about high reward states instead of forcing the learner to discover them through extensive exploration, and (2) using expert actions in Q value bootstrapping in order to improve the target Q value estimates and more accurately describe high value expert states. Our methods show significant gains over a MaxEntIRL ba",
    "link": "https://arxiv.org/abs/2402.02608",
    "context": "Title: Accelerating Inverse Reinforcement Learning with Expert Bootstrapping\nAbstract: Existing inverse reinforcement learning methods (e.g. MaxEntIRL, $f$-IRL) search over candidate reward functions and solve a reinforcement learning problem in the inner loop. This creates a rather strange inversion where a harder problem, reinforcement learning, is in the inner loop of a presumably easier problem, imitation learning. In this work, we show that better utilization of expert demonstrations can reduce the need for hard exploration in the inner RL loop, hence accelerating learning. Specifically, we propose two simple recipes: (1) placing expert transitions into the replay buffer of the inner RL algorithm (e.g. Soft-Actor Critic) which directly informs the learner about high reward states instead of forcing the learner to discover them through extensive exploration, and (2) using expert actions in Q value bootstrapping in order to improve the target Q value estimates and more accurately describe high value expert states. Our methods show significant gains over a MaxEntIRL ba",
    "path": "papers/24/02/2402.02608.json",
    "total_tokens": 902,
    "translated_title": "加速逆强化学习与专家引导",
    "translated_abstract": "现有的逆强化学习方法（例如MaxEntIRL，f-IRL）在候选奖励函数上进行搜索，并在内循环中解决强化学习问题。这造成了一个比较奇怪的倒置，一个更难的问题，强化学习，位于一个相对较易的问题，模仿学习的内循环中。在这项工作中，我们展示了更好地利用专家示范可以减少内部强化学习循环中对硬探索的需求，从而加速学习。具体而言，我们提出了两个简单的方法：（1）将专家转场放入内部强化学习算法（例如Soft-Actor Critic）的回放缓冲区中，直接向学习者提供高奖励状态的信息，而不是通过广泛的探索来发现它们，（2）在Q值引导中使用专家行为，以改进目标Q值估计并更准确地描述高价值的专家状态。我们的方法在MaxEntIRL的基础上取得了显著的增益。",
    "tldr": "本文提出两个简单的方法来加速逆强化学习，通过更好地使用专家示范，减少对内部强化学习循环中的硬探索的需求。这些方法在MaxEntIRL的基础上取得了显著的增益。"
}