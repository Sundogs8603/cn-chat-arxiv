{
    "title": "Fair Resource Allocation in Multi-Task Learning",
    "abstract": "arXiv:2402.15638v1 Announce Type: new  Abstract: By jointly learning multiple tasks, multi-task learning (MTL) can leverage the shared knowledge across tasks, resulting in improved data efficiency and generalization performance. However, a major challenge in MTL lies in the presence of conflicting gradients, which can hinder the fair optimization of some tasks and subsequently impede MTL's ability to achieve better overall performance. Inspired by fair resource allocation in communication networks, we formulate the optimization of MTL as a utility maximization problem, where the loss decreases across tasks are maximized under different fairness measurements. To solve this problem, we propose FairGrad, a novel MTL optimization method. FairGrad not only enables flexible emphasis on certain tasks but also achieves a theoretical convergence guarantee. Extensive experiments demonstrate that our method can achieve state-of-the-art performance among gradient manipulation methods on a suite of",
    "link": "https://arxiv.org/abs/2402.15638",
    "context": "Title: Fair Resource Allocation in Multi-Task Learning\nAbstract: arXiv:2402.15638v1 Announce Type: new  Abstract: By jointly learning multiple tasks, multi-task learning (MTL) can leverage the shared knowledge across tasks, resulting in improved data efficiency and generalization performance. However, a major challenge in MTL lies in the presence of conflicting gradients, which can hinder the fair optimization of some tasks and subsequently impede MTL's ability to achieve better overall performance. Inspired by fair resource allocation in communication networks, we formulate the optimization of MTL as a utility maximization problem, where the loss decreases across tasks are maximized under different fairness measurements. To solve this problem, we propose FairGrad, a novel MTL optimization method. FairGrad not only enables flexible emphasis on certain tasks but also achieves a theoretical convergence guarantee. Extensive experiments demonstrate that our method can achieve state-of-the-art performance among gradient manipulation methods on a suite of",
    "path": "papers/24/02/2402.15638.json",
    "total_tokens": 790,
    "translated_title": "多任务学习中的公平资源分配",
    "translated_abstract": "通过联合学习多个任务，多任务学习（MTL）可以利用任务之间的共享知识，提高数据效率和泛化性能。然而，在MTL中的一个主要挑战在于存在冲突的梯度，这可能阻碍某些任务的公平优化，从而妨碍MTL实现更好的整体性能。受通信网络中公平资源分配的启发，我们将MTL的优化定式为一个效用最大化问题，其中在不同的公平度量下最大化跨任务的损失递减。为了解决这个问题，我们提出了一种新颖的MTL优化方法FairGrad。FairGrad不仅可以灵活地强调某些任务，而且可以实现理论收敛保证。大量实验证明，我们的方法在一系列梯度调整方法中可以实现最先进的性能。",
    "tldr": "通过将多任务学习的优化问题形式化为效用最大化问题，并提出FairGrad方法，实现了对不同任务损失递减的公平优化，同时具有理论收敛保证。",
    "en_tdlr": "By formulating the optimization problem of multi-task learning as a utility maximization problem and proposing the FairGrad method, fair optimization of loss decreases across tasks is achieved with a theoretical convergence guarantee."
}