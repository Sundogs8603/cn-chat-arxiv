{
    "title": "Dual-Student Knowledge Distillation Networks for Unsupervised Anomaly Detection",
    "abstract": "Due to the data imbalance and the diversity of defects, student-teacher networks (S-T) are favored in unsupervised anomaly detection, which explores the discrepancy in feature representation derived from the knowledge distillation process to recognize anomalies. However, vanilla S-T network is not stable. Employing identical structures to construct the S-T network may weaken the representative discrepancy on anomalies. But using different structures can increase the likelihood of divergent performance on normal data. To address this problem, we propose a novel dual-student knowledge distillation (DSKD) architecture. Different from other S-T networks, we use two student networks a single pre-trained teacher network, where the students have the same scale but inverted structures. This framework can enhance the distillation effect to improve the consistency in recognition of normal data, and simultaneously introduce diversity for anomaly representation. To explore high-dimensional semanti",
    "link": "https://arxiv.org/abs/2402.00448",
    "context": "Title: Dual-Student Knowledge Distillation Networks for Unsupervised Anomaly Detection\nAbstract: Due to the data imbalance and the diversity of defects, student-teacher networks (S-T) are favored in unsupervised anomaly detection, which explores the discrepancy in feature representation derived from the knowledge distillation process to recognize anomalies. However, vanilla S-T network is not stable. Employing identical structures to construct the S-T network may weaken the representative discrepancy on anomalies. But using different structures can increase the likelihood of divergent performance on normal data. To address this problem, we propose a novel dual-student knowledge distillation (DSKD) architecture. Different from other S-T networks, we use two student networks a single pre-trained teacher network, where the students have the same scale but inverted structures. This framework can enhance the distillation effect to improve the consistency in recognition of normal data, and simultaneously introduce diversity for anomaly representation. To explore high-dimensional semanti",
    "path": "papers/24/02/2402.00448.json",
    "total_tokens": 873,
    "translated_title": "双学生知识蒸馏网络用于无监督异常检测",
    "translated_abstract": "由于数据不平衡和缺陷的多样性，在无监督异常检测中，学生-教师网络（S-T）在探索由知识蒸馏过程得出的特征表示中的差异以识别异常方面得到了青睐。然而，传统的S-T网络不够稳定。采用相同的结构构建S-T网络可能会削弱对异常的代表性差异。但使用不同的结构可以增加在正常数据上产生差异性性能的可能性。为了解决这个问题，我们提出了一种新颖的双学生知识蒸馏（DSKD）架构。与其他S-T网络不同，我们使用两个具有相同规模但结构相反的学生网络和一个单一预训练的教师网络。该框架可以增强蒸馏效果，提高对正常数据的识别一致性，并同时引入异常表示的多样性。",
    "tldr": "这篇论文介绍了一种用于无监督异常检测的双学生知识蒸馏网络，通过引入两个具有相同规模但结构相反的学生网络和一个单一预训练的教师网络，提高了对正常数据的识别一致性，并同时引入了异常表示的多样性。"
}