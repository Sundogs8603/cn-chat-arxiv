{
    "title": "Hint-before-Solving Prompting: Guiding LLMs to Effectively Utilize Encoded Knowledge",
    "abstract": "arXiv:2402.14310v1 Announce Type: new  Abstract: Large Language Models (LLMs) have recently showcased remarkable generalizability in various domains. Despite their extensive knowledge, LLMs still face challenges in efficiently utilizing encoded knowledge to develop accurate and logical reasoning processes. To mitigate this problem, we introduced Hint-before-Solving Prompting (HSP), which guides the model to generate hints (e.g., specific knowledge or key ideas) for solving the problem and then generate solutions containing intermediate reasoning steps. Since HSP is orthogonal to prompting methods (e.g., Chain-of-Thought (CoT)), we applied HSP to CoT, Least-to-Most, Plan-and-Solve, and Standard promptings. The results of extensive experiments on 6 reasoning benchmarks and 4 open-source LLMs demonstrate that HSP can effectively improve the accuracy of reasoning tasks: (1) By applying high-quality hint-enhanced HSP to CoT prompting, Llama2-70B-Chat shows an improvement of 9.7. (2) Beyond ",
    "link": "https://arxiv.org/abs/2402.14310",
    "context": "Title: Hint-before-Solving Prompting: Guiding LLMs to Effectively Utilize Encoded Knowledge\nAbstract: arXiv:2402.14310v1 Announce Type: new  Abstract: Large Language Models (LLMs) have recently showcased remarkable generalizability in various domains. Despite their extensive knowledge, LLMs still face challenges in efficiently utilizing encoded knowledge to develop accurate and logical reasoning processes. To mitigate this problem, we introduced Hint-before-Solving Prompting (HSP), which guides the model to generate hints (e.g., specific knowledge or key ideas) for solving the problem and then generate solutions containing intermediate reasoning steps. Since HSP is orthogonal to prompting methods (e.g., Chain-of-Thought (CoT)), we applied HSP to CoT, Least-to-Most, Plan-and-Solve, and Standard promptings. The results of extensive experiments on 6 reasoning benchmarks and 4 open-source LLMs demonstrate that HSP can effectively improve the accuracy of reasoning tasks: (1) By applying high-quality hint-enhanced HSP to CoT prompting, Llama2-70B-Chat shows an improvement of 9.7. (2) Beyond ",
    "path": "papers/24/02/2402.14310.json",
    "total_tokens": 890,
    "translated_title": "提示-解决前提示：引导LLMs有效利用编码知识",
    "translated_abstract": "大型语言模型（LLMs）最近在各个领域展示了出色的泛化能力。尽管它们拥有丰富的知识，但LLMs仍然面临着有效利用编码知识来发展准确和合乎逻辑的推理过程的挑战。为了缓解这一问题，我们引入了提示-解决前提示（HSP），该方法引导模型生成提示（例如，特定知识或关键思想）来解决问题，然后生成包含中间推理步骤的解决方案。由于HSP与提示方法（例如，Chain-of-Thought（CoT））正交，我们将HSP应用于CoT、Least-to-Most、Plan-and-Solve和Standard提示。对6个推理基准和4个开源LLMs进行的大量实验结果表明，HSP可以有效提高推理任务的准确性：（1）通过将高质量的提示增强型HSP应用于CoT提示，Llama2-70B-Chat的准确性提高了9.7。",
    "tldr": "引入提示-解决前提示（HSP）方法，指导LLMs生成解决问题的提示并生成包含中间推理步骤的解决方案，有效提高了推理任务的准确性。",
    "en_tdlr": "The introduction of Hint-before-Solving Prompting (HSP) method, guiding LLMS to generate hints for solving problems and generate solutions containing intermediate reasoning steps, effectively improves the accuracy of reasoning tasks."
}