{
    "title": "VisLingInstruct: Elevating Zero-Shot Learning in Multi-Modal Language Models with Autonomous Instruction Optimization",
    "abstract": "This paper presents VisLingInstruct, a novel approach to advancing Multi-Modal Language Models (MMLMs) in zero-shot learning. Current MMLMs show impressive zero-shot abilities in multi-modal tasks, but their performance depends heavily on the quality of instructions. VisLingInstruct tackles this by autonomously evaluating and optimizing instructional texts through In-Context Learning, improving the synergy between visual perception and linguistic expression in MMLMs. Alongside this instructional advancement, we have also optimized the visual feature extraction modules in MMLMs, further augmenting their responsiveness to textual cues. Our comprehensive experiments on MMLMs, based on FlanT5 and Vicuna, show that VisLingInstruct significantly improves zero-shot performance in visual multi-modal tasks. Notably, it achieves a 13.1% and 9% increase in accuracy over the prior state-of-the-art on the TextVQA and HatefulMemes datasets.",
    "link": "https://arxiv.org/abs/2402.07398",
    "context": "Title: VisLingInstruct: Elevating Zero-Shot Learning in Multi-Modal Language Models with Autonomous Instruction Optimization\nAbstract: This paper presents VisLingInstruct, a novel approach to advancing Multi-Modal Language Models (MMLMs) in zero-shot learning. Current MMLMs show impressive zero-shot abilities in multi-modal tasks, but their performance depends heavily on the quality of instructions. VisLingInstruct tackles this by autonomously evaluating and optimizing instructional texts through In-Context Learning, improving the synergy between visual perception and linguistic expression in MMLMs. Alongside this instructional advancement, we have also optimized the visual feature extraction modules in MMLMs, further augmenting their responsiveness to textual cues. Our comprehensive experiments on MMLMs, based on FlanT5 and Vicuna, show that VisLingInstruct significantly improves zero-shot performance in visual multi-modal tasks. Notably, it achieves a 13.1% and 9% increase in accuracy over the prior state-of-the-art on the TextVQA and HatefulMemes datasets.",
    "path": "papers/24/02/2402.07398.json",
    "total_tokens": 931,
    "translated_title": "VisLingInstruct: 通过自主指导优化提升多模态语言模型中的零样本学习",
    "translated_abstract": "本文介绍了VisLingInstruct，这是一种在多模态语言模型中推进零样本学习的新方法。当前的多模态语言模型在多模态任务中展现出令人印象深刻的零样本能力，但它们的性能严重依赖于指导文本的质量。VisLingInstruct通过自主评估和优化指导文本，通过上下文学习改进多模态语言模型中视觉感知和语言表达之间的协同作用。除了指导文本的改进之外，我们还优化了多模态语言模型中的视觉特征提取模块，进一步增强了对文本提示的响应能力。基于FlanT5和Vicuna的综合实验结果显示，VisLingInstruct显著提高了在视觉多模态任务中的零样本性能。值得注意的是，它在TextVQA和HatefulMemes数据集上的准确率分别比之前的最先进方法提高了13.1%和9%。",
    "tldr": "VisLingInstruct通过自主优化指导文本和视觉特征提取模块，显著提高了多模态语言模型在零样本学习中的性能，在TextVQA和HatefulMemes数据集上的准确率分别提高了13.1%和9%。",
    "en_tdlr": "VisLingInstruct significantly improves the zero-shot performance of Multi-Modal Language Models (MMLMs) by autonomously optimizing instructional texts and visual feature extraction modules. It achieves a 13.1% and 9% increase in accuracy on the TextVQA and HatefulMemes datasets, respectively."
}