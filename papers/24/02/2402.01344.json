{
    "title": "Monotone, Bi-Lipschitz, and Polyak-\\L{}ojasiewicz Networks",
    "abstract": "This paper presents a new \\emph{bi-Lipschitz} invertible neural network, the BiLipNet, which has the ability to control both its \\emph{Lipschitzness} (output sensitivity to input perturbations) and \\emph{inverse Lipschitzness} (input distinguishability from different outputs). The main contribution is a novel invertible residual layer with certified strong monotonicity and Lipschitzness, which we compose with orthogonal layers to build bi-Lipschitz networks. The certification is based on incremental quadratic constraints, which achieves much tighter bounds compared to spectral normalization. Moreover, we formulate the model inverse calculation as a three-operator splitting problem, for which fast algorithms are known. Based on the proposed bi-Lipschitz network, we introduce a new scalar-output network, the PLNet, which satisfies the Polyak-\\L{}ojasiewicz condition. It can be applied to learn non-convex surrogate losses with favourable properties, e.g., a unique and efficiently-computab",
    "link": "https://rss.arxiv.org/abs/2402.01344",
    "context": "Title: Monotone, Bi-Lipschitz, and Polyak-\\L{}ojasiewicz Networks\nAbstract: This paper presents a new \\emph{bi-Lipschitz} invertible neural network, the BiLipNet, which has the ability to control both its \\emph{Lipschitzness} (output sensitivity to input perturbations) and \\emph{inverse Lipschitzness} (input distinguishability from different outputs). The main contribution is a novel invertible residual layer with certified strong monotonicity and Lipschitzness, which we compose with orthogonal layers to build bi-Lipschitz networks. The certification is based on incremental quadratic constraints, which achieves much tighter bounds compared to spectral normalization. Moreover, we formulate the model inverse calculation as a three-operator splitting problem, for which fast algorithms are known. Based on the proposed bi-Lipschitz network, we introduce a new scalar-output network, the PLNet, which satisfies the Polyak-\\L{}ojasiewicz condition. It can be applied to learn non-convex surrogate losses with favourable properties, e.g., a unique and efficiently-computab",
    "path": "papers/24/02/2402.01344.json",
    "total_tokens": 1045,
    "translated_title": "单调、Bi-Lipschitz和Polyak-\\L{}ojasiewicz网络",
    "translated_abstract": "本文介绍了一种新的BiLipNet，这是一种可逆的\\emph{Bi-Lipschitz}神经网络，具有控制其\\emph{Lipschitzness}（对输入扰动的输出敏感性）和\\emph{inverse Lipschitzness}（不同输出的输入可区分性）的能力。主要贡献是一个新颖的可逆残差层，具有认证的强单调性和Lipschitz性，我们将其与正交层组合以构建Bi-Lipschitz网络。认证是基于增量二次约束的，与谱归一化相比，它能实现更紧密的界限。此外，我们将模型的反向计算形式化为三算子分裂问题，已知存在快速算法。基于所提出的Bi-Lipschitz网络，我们引入了一种新的标量输出网络，即PLNet，它满足Polyak-\\L{}ojasiewicz条件。它可以用于学习具有有利特性的非凸代理损失，例如独特性和高效计算性。",
    "tldr": "这篇论文介绍了一种新的可逆神经网络BiLipNet，它具有调控输出敏感性和输入可区分性的能力。其中的主要创新是通过认证的强单调性和Lipschitz性的可逆残差层，与正交层组合构建了Bi-Lipschitz网络。另外，该论文还提出了满足Polyak-\\L{}ojasiewicz条件的PLNet，并介绍了其应用于学习非凸代理损失的优势特性。",
    "en_tdlr": "This paper presents a new invertible neural network, BiLipNet, which can control output sensitivity and input distinguishability. The main contribution is the novel invertible residual layer with certified strong monotonicity and Lipschitzness, combined with orthogonal layers to build bi-Lipschitz networks. Additionally, the paper introduces PLNet, a scalar-output network satisfying the Polyak-\\L{}ojasiewicz condition, for learning non-convex surrogate losses with favorable properties."
}