{
    "title": "Emergent Word Order Universals from Cognitively-Motivated Language Models",
    "abstract": "arXiv:2402.12363v1 Announce Type: new  Abstract: The world's languages exhibit certain so-called typological or implicational universals; for example, Subject-Object-Verb (SOV) word order typically employs postpositions. Explaining the source of such biases is a key goal in linguistics. We study the word-order universals through a computational simulation with language models (LMs). Our experiments show that typologically typical word orders tend to have lower perplexity estimated by LMs with cognitively plausible biases: syntactic biases, specific parsing strategies, and memory limitations. This suggests that the interplay of these cognitive biases and predictability (perplexity) can explain many aspects of word-order universals. This also showcases the advantage of cognitively-motivated LMs, which are typically employed in cognitive modeling, in the computational simulation of language universals.",
    "link": "https://arxiv.org/abs/2402.12363",
    "context": "Title: Emergent Word Order Universals from Cognitively-Motivated Language Models\nAbstract: arXiv:2402.12363v1 Announce Type: new  Abstract: The world's languages exhibit certain so-called typological or implicational universals; for example, Subject-Object-Verb (SOV) word order typically employs postpositions. Explaining the source of such biases is a key goal in linguistics. We study the word-order universals through a computational simulation with language models (LMs). Our experiments show that typologically typical word orders tend to have lower perplexity estimated by LMs with cognitively plausible biases: syntactic biases, specific parsing strategies, and memory limitations. This suggests that the interplay of these cognitive biases and predictability (perplexity) can explain many aspects of word-order universals. This also showcases the advantage of cognitively-motivated LMs, which are typically employed in cognitive modeling, in the computational simulation of language universals.",
    "path": "papers/24/02/2402.12363.json",
    "total_tokens": 795,
    "translated_title": "从认知驱动的语言模型中得出的词序普遍规律",
    "translated_abstract": "世界上的语言表现出某些所谓的类型学或蕴含规律；例如，主-宾-谓（SOV）的词序通常使用后置词。解释这些偏好的来源是语言学的一个关键目标。我们通过使用具有认知偏差的语言模型（LMs）进行计算模拟研究词序普遍规律。我们的实验证明，具有类型学典型词序的语言倾向于具有由具有认知合理偏差的LMs估计的较低困惑度：句法偏差、特定的解析策略和记忆限制。这表明，这些认知偏差和可预测性（困惑度）之间的相互作用可以解释词序普遍规律的许多方面。这也展示了认知驱动LMs的优势，在计算模拟语言普遍规律时通常用于认知建模。",
    "tldr": "认知驱动的语言模型显示出可以解释许多词序普遍规律的优势",
    "en_tdlr": "Cognitive-driven language models demonstrate advantages in explaining many word order universals."
}