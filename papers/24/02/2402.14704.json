{
    "title": "An LLM-Enhanced Adversarial Editing System for Lexical Simplification",
    "abstract": "arXiv:2402.14704v1 Announce Type: new  Abstract: Lexical Simplification (LS) aims to simplify text at the lexical level. Existing methods rely heavily on annotated data, making it challenging to apply in low-resource scenarios. In this paper, we propose a novel LS method without parallel corpora. This method employs an Adversarial Editing System with guidance from a confusion loss and an invariance loss to predict lexical edits in the original sentences. Meanwhile, we introduce an innovative LLM-enhanced loss to enable the distillation of knowledge from Large Language Models (LLMs) into a small-size LS system. From that, complex words within sentences are masked and a Difficulty-aware Filling module is crafted to replace masked positions with simpler words. At last, extensive experimental results and analyses on three benchmark LS datasets demonstrate the effectiveness of our proposed method.",
    "link": "https://arxiv.org/abs/2402.14704",
    "context": "Title: An LLM-Enhanced Adversarial Editing System for Lexical Simplification\nAbstract: arXiv:2402.14704v1 Announce Type: new  Abstract: Lexical Simplification (LS) aims to simplify text at the lexical level. Existing methods rely heavily on annotated data, making it challenging to apply in low-resource scenarios. In this paper, we propose a novel LS method without parallel corpora. This method employs an Adversarial Editing System with guidance from a confusion loss and an invariance loss to predict lexical edits in the original sentences. Meanwhile, we introduce an innovative LLM-enhanced loss to enable the distillation of knowledge from Large Language Models (LLMs) into a small-size LS system. From that, complex words within sentences are masked and a Difficulty-aware Filling module is crafted to replace masked positions with simpler words. At last, extensive experimental results and analyses on three benchmark LS datasets demonstrate the effectiveness of our proposed method.",
    "path": "papers/24/02/2402.14704.json",
    "total_tokens": 878,
    "translated_title": "一种LLM增强的词汇简化对抗编辑系统",
    "translated_abstract": "词汇简化（LS）旨在在词汇级别简化文本。现有方法严重依赖标注数据，这使得在资源匮乏的情况下难以应用。在本文中，我们提出了一种新颖的LS方法，不需要平行语料库。该方法采用对抗编辑系统，并结合混淆损失和不变性损失来预测原始句子中的词汇修改。同时，我们引入了一种创新的LLM增强损失，以将大型语言模型（LLMs）的知识提炼成小型LS系统。通过这种方式，句子中的复杂词被屏蔽，制作了一个基于难度感知的填充模块，用更简单的词替换屏蔽位置。最后，对三个基准LS数据集进行了广泛的实验结果和分析，证明了我们提出方法的有效性。",
    "tldr": "该论文提出了一种新颖的词汇简化方法，不需要平行语料库，在原始句子中预测词汇修改，引入LLM增强损失进行知识提炼，并采用基于难度感知的填充模块将复杂词替换为简单词，实验证明方法的有效性。",
    "en_tdlr": "The paper introduces a novel method for lexical simplification without parallel corpora, predicting lexical edits in the original sentences, incorporating LLM-enhanced loss for knowledge distillation, and utilizing a Difficulty-aware Filling module to replace complex words with simpler ones, with experimental results demonstrating its effectiveness."
}