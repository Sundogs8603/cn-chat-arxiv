{
    "title": "Are We Wasting Time? A Fast, Accurate Performance Evaluation Framework for Knowledge Graph Link Predictors",
    "abstract": "The standard evaluation protocol for measuring the quality of Knowledge Graph Completion methods - the task of inferring new links to be added to a graph - typically involves a step which ranks every entity of a Knowledge Graph to assess their fit as a head or tail of a candidate link to be added. In Knowledge Graphs on a larger scale, this task rapidly becomes prohibitively heavy. Previous approaches mitigate this problem by using random sampling of entities to assess the quality of links predicted or suggested by a method. However, we show that this approach has serious limitations since the ranking metrics produced do not properly reflect true outcomes. In this paper, we present a thorough analysis of these effects along with the following findings. First, we empirically find and theoretically motivate why sampling uniformly at random vastly overestimates the ranking performance of a method. We show that this can be attributed to the effect of easy versus hard negative candidates. S",
    "link": "https://arxiv.org/abs/2402.00053",
    "context": "Title: Are We Wasting Time? A Fast, Accurate Performance Evaluation Framework for Knowledge Graph Link Predictors\nAbstract: The standard evaluation protocol for measuring the quality of Knowledge Graph Completion methods - the task of inferring new links to be added to a graph - typically involves a step which ranks every entity of a Knowledge Graph to assess their fit as a head or tail of a candidate link to be added. In Knowledge Graphs on a larger scale, this task rapidly becomes prohibitively heavy. Previous approaches mitigate this problem by using random sampling of entities to assess the quality of links predicted or suggested by a method. However, we show that this approach has serious limitations since the ranking metrics produced do not properly reflect true outcomes. In this paper, we present a thorough analysis of these effects along with the following findings. First, we empirically find and theoretically motivate why sampling uniformly at random vastly overestimates the ranking performance of a method. We show that this can be attributed to the effect of easy versus hard negative candidates. S",
    "path": "papers/24/02/2402.00053.json",
    "total_tokens": 840,
    "translated_title": "我们在浪费时间吗？一种快速、准确的知识图谱链接预测评估框架",
    "translated_abstract": "用于衡量知识图谱完善方法质量的标准评估协议通常包括对知识图谱的每个实体进行排名，以评估它们作为候选链接头部或尾部的适合程度。然而，在规模较大的知识图谱中，这个任务很快变得难以承受。先前的方法通过对实体进行随机抽样来评估预测或建议方法的链接质量，但我们展示了这种方法存在严重局限性，因为产生的排名指标不正确地反映了真实结果。本文对这些效应进行了彻底分析，并得出以下发现。首先，我们通过实证研究和理论论证找出了为什么随机均匀抽样极大地高估了方法的排名表现。我们展示了这可以归因于易/难度负候选者的影响。",
    "tldr": "这篇论文提出了一种快速、准确的知识图谱链接预测评估框架，解决了现有方法中随机抽样带来的排名指标不准确的问题。"
}