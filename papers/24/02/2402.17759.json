{
    "title": "Towards Optimal Learning of Language Models",
    "abstract": "arXiv:2402.17759v1 Announce Type: new  Abstract: This work studies the general principles of improving the learning of language models (LMs), which aims at reducing the necessary training steps for achieving superior performance. Specifically, we present a theory for the optimal learning of LMs. We first propose an objective that optimizes LM learning by maximizing the data compression ratio in an \"LM-training-as-lossless-compression\" view. Then, we derive a theorem, named Learning Law, to reveal the properties of the dynamics in the optimal learning process under our objective. The theorem is then validated by experiments on a linear classification and a real-world language modeling task. Finally, we empirically verify that the optimal learning of LMs essentially stems from the improvement of the coefficients in the scaling law of LMs, indicating great promise and significance for designing practical learning acceleration methods. Our code can be found at https://aka.ms/LearningLaw.",
    "link": "https://arxiv.org/abs/2402.17759",
    "context": "Title: Towards Optimal Learning of Language Models\nAbstract: arXiv:2402.17759v1 Announce Type: new  Abstract: This work studies the general principles of improving the learning of language models (LMs), which aims at reducing the necessary training steps for achieving superior performance. Specifically, we present a theory for the optimal learning of LMs. We first propose an objective that optimizes LM learning by maximizing the data compression ratio in an \"LM-training-as-lossless-compression\" view. Then, we derive a theorem, named Learning Law, to reveal the properties of the dynamics in the optimal learning process under our objective. The theorem is then validated by experiments on a linear classification and a real-world language modeling task. Finally, we empirically verify that the optimal learning of LMs essentially stems from the improvement of the coefficients in the scaling law of LMs, indicating great promise and significance for designing practical learning acceleration methods. Our code can be found at https://aka.ms/LearningLaw.",
    "path": "papers/24/02/2402.17759.json",
    "total_tokens": 854,
    "translated_title": "实现语言模型的最佳学习",
    "translated_abstract": "这项工作研究了改进语言模型（LMs）学习的一般原则，旨在减少实现优越性能所需的训练步骤。具体来说，我们提出了一种关于LMs的最佳学习的理论。我们首先提出了一个通过在“LM训练作为无损压缩”视图中最大化数据压缩比率来优化LM学习的目标。然后，我们推导出一个定理，名为学习定律，揭示了在我们的目标下最佳学习过程中动态的特性。随后，我们通过线性分类和现实世界的语言建模任务上的实验验证了该定理。最后，我们经验证明，LMs的最佳学习主要源于改善LMs的缩放定律的系数，为设计实际学习加速方法展示了巨大的前景和重要性。我们的代码可以在https://aka.ms/LearningLaw找到。",
    "tldr": "本论文提出了一种关于语言模型最佳学习的理论，通过最大化数据压缩比率来优化学习过程，根据学习定律揭示了最佳学习过程的特性，并在实验中验证了该定理。",
    "en_tdlr": "This paper presents a theory on the optimal learning of language models, optimizing the learning process by maximizing data compression ratio, revealing the characteristics of the optimal learning process based on the Learning Law, and validating the theorem through experiments."
}