{
    "title": "A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation",
    "abstract": "Contrastive Language-Image Pretraining (CLIP) has gained popularity for its remarkable zero-shot capacity. Recent research has focused on developing efficient fine-tuning methods, such as prompt learning and adapter, to enhance CLIP's performance in downstream tasks. However, these methods still require additional training time and computational resources, which is undesirable for devices with limited resources. In this paper, we revisit a classical algorithm, Gaussian Discriminant Analysis (GDA), and apply it to the downstream classification of CLIP. Typically, GDA assumes that features of each class follow Gaussian distributions with identical covariance. By leveraging Bayes' formula, the classifier can be expressed in terms of the class means and covariance, which can be estimated from the data without the need for training. To integrate knowledge from both visual and textual modalities, we ensemble it with the original zero-shot classifier within CLIP. Extensive results on 17 datas",
    "link": "https://arxiv.org/abs/2402.04087",
    "context": "Title: A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation\nAbstract: Contrastive Language-Image Pretraining (CLIP) has gained popularity for its remarkable zero-shot capacity. Recent research has focused on developing efficient fine-tuning methods, such as prompt learning and adapter, to enhance CLIP's performance in downstream tasks. However, these methods still require additional training time and computational resources, which is undesirable for devices with limited resources. In this paper, we revisit a classical algorithm, Gaussian Discriminant Analysis (GDA), and apply it to the downstream classification of CLIP. Typically, GDA assumes that features of each class follow Gaussian distributions with identical covariance. By leveraging Bayes' formula, the classifier can be expressed in terms of the class means and covariance, which can be estimated from the data without the need for training. To integrate knowledge from both visual and textual modalities, we ensemble it with the original zero-shot classifier within CLIP. Extensive results on 17 datas",
    "path": "papers/24/02/2402.04087.json",
    "total_tokens": 902,
    "translated_title": "一个难以超越的基线用于无需训练的CLIP适应",
    "translated_abstract": "对比语言-图像预训练（CLIP）因其显著的零样本能力而受到青睐。最近的研究集中在开发高效的微调方法，如提示学习和适配器，以提高CLIP在下游任务中的性能。然而，这些方法仍然需要额外的训练时间和计算资源，这对于资源有限的设备来说是不可取的。在本文中，我们重新审视了一个经典算法，高斯判别分析（GDA），并将其应用于CLIP的下游分类。通常，GDA假设每个类别的特征都遵循具有相同协方差的高斯分布。通过利用贝叶斯公式，可以用类别的均值和协方差来表示分类器，这可以在无需训练的情况下从数据中估计得到。为了整合来自视觉和文本模态的知识，我们将其与CLIP中的原始零样本分类器集成。在17个数据集上进行了广泛的结果实验。",
    "tldr": "本文研究了CLIP训练方法的一个经典算法，称为高斯判别分析（GDA），并将其应用于CLIP的下游分类任务。通过对特征分布的估计，无需训练即可实现分类器，从而提高了CLIP的性能。",
    "en_tdlr": "This paper explores a classical algorithm, Gaussian Discriminant Analysis (GDA), for downstream classification in CLIP. By estimating the feature distributions, a classifier can be built without the need for training, leading to improved performance of CLIP."
}