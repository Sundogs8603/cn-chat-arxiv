{
    "title": "Flexible infinite-width graph convolutional networks and the importance of representation learning",
    "abstract": "A common theoretical approach to understanding neural networks is to take an infinite-width limit, at which point the outputs become Gaussian process (GP) distributed. This is known as a neural network Gaussian process (NNGP). However, the NNGP kernel is fixed, and tunable only through a small number of hyperparameters, eliminating any possibility of representation learning. This contrasts with finite-width NNs, which are often believed to perform well precisely because they are able to learn representations. Thus in simplifying NNs to make them theoretically tractable, NNGPs may eliminate precisely what makes them work well (representation learning). This motivated us to understand whether representation learning is necessary in a range of graph classification tasks. We develop a precise tool for this task, the graph convolutional deep kernel machine. This is very similar to an NNGP, in that it is an infinite width limit and uses kernels, but comes with a `knob' to control the amount ",
    "link": "https://arxiv.org/abs/2402.06525",
    "context": "Title: Flexible infinite-width graph convolutional networks and the importance of representation learning\nAbstract: A common theoretical approach to understanding neural networks is to take an infinite-width limit, at which point the outputs become Gaussian process (GP) distributed. This is known as a neural network Gaussian process (NNGP). However, the NNGP kernel is fixed, and tunable only through a small number of hyperparameters, eliminating any possibility of representation learning. This contrasts with finite-width NNs, which are often believed to perform well precisely because they are able to learn representations. Thus in simplifying NNs to make them theoretically tractable, NNGPs may eliminate precisely what makes them work well (representation learning). This motivated us to understand whether representation learning is necessary in a range of graph classification tasks. We develop a precise tool for this task, the graph convolutional deep kernel machine. This is very similar to an NNGP, in that it is an infinite width limit and uses kernels, but comes with a `knob' to control the amount ",
    "path": "papers/24/02/2402.06525.json",
    "total_tokens": 878,
    "translated_title": "灵活的无限宽图卷积网络及表示学习的重要性",
    "translated_abstract": "理解神经网络的一种常见理论方法是进行无限宽度限制，此时输出成为高斯过程（GP）分布。这被称为神经网络高斯过程（NNGP）。然而，NNGP内核是固定的，只能通过少量超参数进行调节，消除了任何表示学习的可能性。这与有限宽度的神经网络形成对比，后者通常被认为能够表现良好，正是因为它们能够学习表示。因此，简化神经网络以使其在理论上可处理的同时，NNGP可能会消除使其工作良好的因素（表示学习）。这激发了我们对一系列图分类任务中表示学习是否必要的理解。我们开发了一个精确的工具来完成这个任务，即图卷积深度内核机（graph convolutional deep kernel machine）。这与NNGP非常相似，因为它是无限宽度限制并使用内核，但它带有一个“旋钮”来控制表示学习的程度。",
    "tldr": "本文讨论了神经网络高斯过程（NNGP）在理论上的局限，提出图卷积深度内核机（graph convolutional deep kernel machine）来研究图分类任务中的表示学习问题。",
    "en_tdlr": "This paper discusses the limitations of Neural Network Gaussian Process (NNGP) in theoretical settings and proposes a Graph Convolutional Deep Kernel Machine for studying the problem of representation learning in graph classification tasks."
}