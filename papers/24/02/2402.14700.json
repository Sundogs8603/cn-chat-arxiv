{
    "title": "Unveiling Linguistic Regions in Large Language Models",
    "abstract": "arXiv:2402.14700v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated considerable cross-lingual alignment and generalization ability. Current research primarily focuses on improving LLMs' cross-lingual generalization capabilities. However, there is still a lack of research on the intrinsic mechanisms of how LLMs achieve cross-lingual alignment. From the perspective of region partitioning, this paper conducts several investigations on the linguistic competence of LLMs. We discover a core region in LLMs that corresponds to linguistic competence, accounting for approximately 1% of the total model parameters. Removing this core region by setting parameters to zero results in a significant performance decrease across 30 different languages. Furthermore, this core region exhibits significant dimensional dependency, perturbations to even a single parameter on specific dimensions leading to a loss of linguistic competence. Moreover, we discover that distinct regions ",
    "link": "https://arxiv.org/abs/2402.14700",
    "context": "Title: Unveiling Linguistic Regions in Large Language Models\nAbstract: arXiv:2402.14700v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated considerable cross-lingual alignment and generalization ability. Current research primarily focuses on improving LLMs' cross-lingual generalization capabilities. However, there is still a lack of research on the intrinsic mechanisms of how LLMs achieve cross-lingual alignment. From the perspective of region partitioning, this paper conducts several investigations on the linguistic competence of LLMs. We discover a core region in LLMs that corresponds to linguistic competence, accounting for approximately 1% of the total model parameters. Removing this core region by setting parameters to zero results in a significant performance decrease across 30 different languages. Furthermore, this core region exhibits significant dimensional dependency, perturbations to even a single parameter on specific dimensions leading to a loss of linguistic competence. Moreover, we discover that distinct regions ",
    "path": "papers/24/02/2402.14700.json",
    "total_tokens": 827,
    "translated_title": "揭示大型语言模型中的语言区域",
    "translated_abstract": "大型语言模型(LLMs)已经展示了相当大的跨语言对齐和泛化能力。当前研究主要集中在改善LLMs的跨语言泛化能力上。然而，关于LLMs如何实现跨语言对齐的内在机制仍然缺乏研究。从区域划分的角度出发，本文在LLMs的语言能力上进行了几项调查。我们发现LLMs中的一个核心区域对应于语言能力，大约占总模型参数的1%。通过将参数设置为零来去除这个核心区域，会导致30种不同语言的显著性能下降。此外，这个核心区域表现出显著的维度依赖性，对特定维度上的单个参数的扰动会导致语言能力的丧失。此外，我们发现独特的区域...",
    "tldr": "本文从区域划分的角度出发，发现了大型语言模型中对应语言能力的核心区域，并展示了去除该区域会导致跨30种不同语言的显著性能下降。",
    "en_tdlr": "This paper identifies a core region in large language models corresponding to linguistic competence, demonstrating that removing this region results in a significant performance decrease across 30 different languages."
}