{
    "title": "Chitchat as Interference: Adding User Backstories to Task-Oriented Dialogues",
    "abstract": "arXiv:2402.15248v1 Announce Type: new  Abstract: During task-oriented dialogues (TODs), human users naturally introduce chitchat that is beyond the immediate scope of the task, interfering with the flow of the conversation. To address this issue without the need for expensive manual data creation, we use few-shot prompting with Llama-2-70B to enhance the MultiWOZ dataset with user backstories, a typical example of chitchat interference in TODs. We assess the impact of this addition by testing two models: one trained solely on TODs and another trained on TODs with a preliminary chitchat interaction. Our analysis reveals that our enriched dataset poses a significant challenge to these systems. Moreover, we demonstrate that our dataset can be effectively used for training purposes, enabling a system to consistently acknowledge the user's backstory while also successfully moving the task forward in the same turn, as confirmed by human evaluation. These findings highlight the benefits of ge",
    "link": "https://arxiv.org/abs/2402.15248",
    "context": "Title: Chitchat as Interference: Adding User Backstories to Task-Oriented Dialogues\nAbstract: arXiv:2402.15248v1 Announce Type: new  Abstract: During task-oriented dialogues (TODs), human users naturally introduce chitchat that is beyond the immediate scope of the task, interfering with the flow of the conversation. To address this issue without the need for expensive manual data creation, we use few-shot prompting with Llama-2-70B to enhance the MultiWOZ dataset with user backstories, a typical example of chitchat interference in TODs. We assess the impact of this addition by testing two models: one trained solely on TODs and another trained on TODs with a preliminary chitchat interaction. Our analysis reveals that our enriched dataset poses a significant challenge to these systems. Moreover, we demonstrate that our dataset can be effectively used for training purposes, enabling a system to consistently acknowledge the user's backstory while also successfully moving the task forward in the same turn, as confirmed by human evaluation. These findings highlight the benefits of ge",
    "path": "papers/24/02/2402.15248.json",
    "total_tokens": 886,
    "translated_title": "Chitchat作为干扰：向面向任务的对话添加用户背景故事",
    "translated_abstract": "在面向任务的对话（TOD）中，人类用户自然会引入超出任务范围的闲聊，干扰了对话的流程。为了解决这一问题，我们利用Llama-2-70B进行少样本提示，以增强MultiWOZ数据集，其中包括用户背景故事，这是TOD中典型的闲聊干扰的一个例子。我们通过测试两个模型来评估此添加的影响：一个仅在TOD上进行训练，另一个在TOD上进行初步闲聊交互的训练。我们的分析表明，我们丰富的数据集对这些系统构成了重要挑战。此外，我们证明我们的数据集可以有效用于训练，使系统能够在同一轮中持续承认用户背景故事并成功推动任务的进行，这得到了人类评估的确认。这些发现突显了引入用户背景故事的好处。",
    "tldr": "通过使用少样本提示和Llama-2-70B增强MultiWOZ数据集，引入用户背景故事，有效解决面向任务的对话中的闲聊干扰问题，并能够同时承认用户背景故事并推动任务的进行。",
    "en_tdlr": "By enhancing the MultiWOZ dataset with user backstories using few-shot prompting and Llama-2-70B, this research effectively addresses chitchat interference in task-oriented dialogues, enabling acknowledgment of user backstories while advancing the task simultaneously."
}