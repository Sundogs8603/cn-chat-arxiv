{
    "title": "Sustainable Supercomputing for AI: GPU Power Capping at HPC Scale",
    "abstract": "arXiv:2402.18593v1 Announce Type: cross  Abstract: As research and deployment of AI grows, the computational burden to support and sustain its progress inevitably does too. To train or fine-tune state-of-the-art models in NLP, computer vision, etc., some form of AI hardware acceleration is virtually a requirement. Recent large language models require considerable resources to train and deploy, resulting in significant energy usage, potential carbon emissions, and massive demand for GPUs and other hardware accelerators. However, this surge carries large implications for energy sustainability at the HPC/datacenter level. In this paper, we study the aggregate effect of power-capping GPUs on GPU temperature and power draw at a research supercomputing center. With the right amount of power-capping, we show significant decreases in both temperature and power draw, reducing power consumption and potentially improving hardware life-span with minimal impact on job performance. While power-cappi",
    "link": "https://arxiv.org/abs/2402.18593",
    "context": "Title: Sustainable Supercomputing for AI: GPU Power Capping at HPC Scale\nAbstract: arXiv:2402.18593v1 Announce Type: cross  Abstract: As research and deployment of AI grows, the computational burden to support and sustain its progress inevitably does too. To train or fine-tune state-of-the-art models in NLP, computer vision, etc., some form of AI hardware acceleration is virtually a requirement. Recent large language models require considerable resources to train and deploy, resulting in significant energy usage, potential carbon emissions, and massive demand for GPUs and other hardware accelerators. However, this surge carries large implications for energy sustainability at the HPC/datacenter level. In this paper, we study the aggregate effect of power-capping GPUs on GPU temperature and power draw at a research supercomputing center. With the right amount of power-capping, we show significant decreases in both temperature and power draw, reducing power consumption and potentially improving hardware life-span with minimal impact on job performance. While power-cappi",
    "path": "papers/24/02/2402.18593.json",
    "total_tokens": 886,
    "translated_title": "可持续的AI超级计算：HPC规模下的GPU功率限制",
    "translated_abstract": "随着人工智能的研究和部署不断增长，支持和维持其进展的计算负担也必然增加。为了训练或微调自然语言处理、计算机视觉等领域的最先进模型，某种形式的AI硬件加速几乎是必需的。最近的大型语言模型需要大量资源来训练和部署，导致能源消耗巨大，潜在碳排放增加，并对GPU和其他硬件加速器的需求激增。然而，这种增长对HPC/数据中心级别的能源可持续性带来重大影响。本文研究了在研究超级计算中心对GPU进行功率限制的总体效果对GPU温度和功率消耗的影响。通过适当的功率限制，我们显示出温度和功率消耗显著降低，减少功耗，并可能在最小影响作业性能的情况下改善硬件寿命。",
    "tldr": "该研究探讨在超级计算中心对GPU进行功率限制对温度和功耗的影响，通过适当的功率限制，实现了降低能耗、改善硬件寿命的目的。",
    "en_tdlr": "This study examines the impact of power capping on GPU temperature and power draw in a research supercomputing center, showing significant reductions in energy consumption and potential improvement in hardware lifespan through appropriate power capping."
}