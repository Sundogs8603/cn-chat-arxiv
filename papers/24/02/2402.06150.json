{
    "title": "Domain Generalization with Small Data",
    "abstract": "In this work, we propose to tackle the problem of domain generalization in the context of \\textit{insufficient samples}. Instead of extracting latent feature embeddings based on deterministic models, we propose to learn a domain-invariant representation based on the probabilistic framework by mapping each data point into probabilistic embeddings. Specifically, we first extend empirical maximum mean discrepancy (MMD) to a novel probabilistic MMD that can measure the discrepancy between mixture distributions (i.e., source domains) consisting of a series of latent distributions rather than latent points. Moreover, instead of imposing the contrastive semantic alignment (CSA) loss based on pairs of latent points, a novel probabilistic CSA loss encourages positive probabilistic embedding pairs to be closer while pulling other negative ones apart. Benefiting from the learned representation captured by probabilistic models, our proposed method can marriage the measurement on the \\textit{distri",
    "link": "https://arxiv.org/abs/2402.06150",
    "context": "Title: Domain Generalization with Small Data\nAbstract: In this work, we propose to tackle the problem of domain generalization in the context of \\textit{insufficient samples}. Instead of extracting latent feature embeddings based on deterministic models, we propose to learn a domain-invariant representation based on the probabilistic framework by mapping each data point into probabilistic embeddings. Specifically, we first extend empirical maximum mean discrepancy (MMD) to a novel probabilistic MMD that can measure the discrepancy between mixture distributions (i.e., source domains) consisting of a series of latent distributions rather than latent points. Moreover, instead of imposing the contrastive semantic alignment (CSA) loss based on pairs of latent points, a novel probabilistic CSA loss encourages positive probabilistic embedding pairs to be closer while pulling other negative ones apart. Benefiting from the learned representation captured by probabilistic models, our proposed method can marriage the measurement on the \\textit{distri",
    "path": "papers/24/02/2402.06150.json",
    "total_tokens": 891,
    "translated_title": "用小数据进行领域泛化",
    "translated_abstract": "本研究中，我们提出在“样本不足”情况下解决领域泛化的问题。我们不是基于确定性模型提取潜在特征嵌入，而是提出基于概率框架学习领域不变表示的方法，通过将每个数据点映射为概率嵌入。具体来说，我们首先将经验最大均值差异（MMD）扩展为一种新的概率MMD，可以度量由一系列潜在分布（即源领域）组成的混合分布之间的差异，而不是潜在点。此外，我们通过一种新的概率对比语义对齐（CSA）损失来提倡使正概率嵌入对更接近，而将其他负概率嵌入拉开。通过概率模型捕捉到的学到表示，我们提出的方法可以结合在分布上的度量，从而提升领域泛化性能。",
    "tldr": "本研究提出了一种在样本不足情况下解决领域泛化问题的方法。该方法通过使用概率嵌入来学习领域不变表示，并使用概率框架中的新方法测量混合分布之间的差异。结果表明，该方法在领域泛化性能上有显著的提升。",
    "en_tdlr": "This work proposes a method to address the problem of domain generalization with limited samples. It learns a domain-invariant representation using probabilistic embeddings and measures the discrepancy between mixture distributions with a novel probabilistic framework. Results show significant improvement in domain generalization performance."
}