{
    "title": "Simulated Overparameterization",
    "abstract": "In this work, we introduce a novel paradigm called Simulated Overparametrization (SOP). SOP merges the computational efficiency of compact models with the advanced learning proficiencies of overparameterized models. SOP proposes a unique approach to model training and inference, where a model with a significantly larger number of parameters is trained in such a way that a smaller, efficient subset of these parameters is used for the actual computation during inference. Building upon this framework, we present a novel, architecture agnostic algorithm called \"majority kernels\", which seamlessly integrates with predominant architectures, including Transformer models. Majority kernels enables the simulated training of overparameterized models, resulting in performance gains across architectures and tasks. Furthermore, our approach adds minimal overhead to the cost incurred (wall clock time) at training time. The proposed approach shows strong performance on a wide variety of datasets and m",
    "link": "https://arxiv.org/abs/2402.05033",
    "context": "Title: Simulated Overparameterization\nAbstract: In this work, we introduce a novel paradigm called Simulated Overparametrization (SOP). SOP merges the computational efficiency of compact models with the advanced learning proficiencies of overparameterized models. SOP proposes a unique approach to model training and inference, where a model with a significantly larger number of parameters is trained in such a way that a smaller, efficient subset of these parameters is used for the actual computation during inference. Building upon this framework, we present a novel, architecture agnostic algorithm called \"majority kernels\", which seamlessly integrates with predominant architectures, including Transformer models. Majority kernels enables the simulated training of overparameterized models, resulting in performance gains across architectures and tasks. Furthermore, our approach adds minimal overhead to the cost incurred (wall clock time) at training time. The proposed approach shows strong performance on a wide variety of datasets and m",
    "path": "papers/24/02/2402.05033.json",
    "total_tokens": 881,
    "translated_title": "模拟过度参数化",
    "translated_abstract": "在这项工作中，我们引入了一种新的范式，称为模拟过度参数化（SOP）。SOP将紧凑模型的计算效率与过度参数化模型的高级学习能力相结合。SOP提出了一种独特的模型训练和推断方法，在推断过程中，使用显著更多参数的模型进行训练，但只使用其中较小、高效的子集进行实际计算。在此框架的基础上，我们提出了一种新颖的、与主要架构（包括Transformer模型）无关的算法，称为\"majority kernels\"。 majority kernels使得模拟训练过度参数化模型成为可能，从而在各种架构和任务中取得性能提升。此外，我们的方法在训练时对于计算成本（墙上挂钟时间）的增加非常小。所提出的方法在各种数据集和任务上表现出很强的性能。",
    "tldr": "模拟过度参数化（SOP）是一种将紧凑模型的计算效率与过度参数化模型的高级学习能力相结合的新范式。通过使用模拟训练过度参数化模型的方法，我们提出了一种与主要架构无关的算法，称为\"majority kernels\"，从而在各种架构和任务中实现性能提升。",
    "en_tdlr": "Simulated Overparameterization (SOP) is a new paradigm that combines the computational efficiency of compact models with the advanced learning proficiencies of overparameterized models. By simulating the training of overparameterized models, we propose a architecture-agnostic algorithm called \"majority kernels\" that achieves performance gains across architectures and tasks."
}