{
    "title": "Learning Communication Policies for Different Follower Behaviors in a Collaborative Reference Game",
    "abstract": "Albrecht and Stone (2018) state that modeling of changing behaviors remains an open problem \"due to the essentially unconstrained nature of what other agents may do\". In this work we evaluate the adaptability of neural artificial agents towards assumed partner behaviors in a collaborative reference game. In this game success is achieved when a knowledgeable Guide can verbally lead a Follower to the selection of a specific puzzle piece among several distractors. We frame this language grounding and coordination task as a reinforcement learning problem and measure to which extent a common reinforcement training algorithm (PPO) is able to produce neural agents (the Guides) that perform well with various heuristic Follower behaviors that vary along the dimensions of confidence and autonomy. We experiment with a learning signal that in addition to the goal condition also respects an assumed communicative effort. Our results indicate that this novel ingredient leads to communicative strategi",
    "link": "https://arxiv.org/abs/2402.04824",
    "context": "Title: Learning Communication Policies for Different Follower Behaviors in a Collaborative Reference Game\nAbstract: Albrecht and Stone (2018) state that modeling of changing behaviors remains an open problem \"due to the essentially unconstrained nature of what other agents may do\". In this work we evaluate the adaptability of neural artificial agents towards assumed partner behaviors in a collaborative reference game. In this game success is achieved when a knowledgeable Guide can verbally lead a Follower to the selection of a specific puzzle piece among several distractors. We frame this language grounding and coordination task as a reinforcement learning problem and measure to which extent a common reinforcement training algorithm (PPO) is able to produce neural agents (the Guides) that perform well with various heuristic Follower behaviors that vary along the dimensions of confidence and autonomy. We experiment with a learning signal that in addition to the goal condition also respects an assumed communicative effort. Our results indicate that this novel ingredient leads to communicative strategi",
    "path": "papers/24/02/2402.04824.json",
    "total_tokens": 940,
    "translated_title": "在协同参考游戏中学习不同追随者行为的通信策略",
    "translated_abstract": "Albrecht和Stone（2018）指出，建模不断变化的行为仍然是一个开放的问题，“由于其他智能体可能做出的行为本质上是无限制的”。在这项工作中，我们评估了神经人工智能代理对协同参考游戏中假设伙伴行为的适应能力。在这个游戏中，当有知识的导游能够通过口头引导追随者从多个干扰者中选择特定的拼图时，才能取得成功。我们把这个语言基础和协调任务作为一个强化学习问题来解决，并测量了一个常见的强化训练算法（PPO）在多种启发式追随者行为上产生的神经代理（导游）的表现程度，这些行为在自信度和自主性方面有所不同。我们尝试了一种学习信号，除了目标条件外还尊重假设的沟通努力。我们的结果表明，这种新颖的因素会导致沟通策略的改变。",
    "tldr": "本文研究了在协同参考游戏中学习适应不同追随者行为的通信策略。通过强化学习算法（PPO），证明了神经代理能够与具有不同自信度和自主性的追随者良好地协作，并且在学习信号中考虑了沟通努力的因素。",
    "en_tdlr": "This paper investigates learning communication policies for different follower behaviors in a collaborative reference game. Through reinforcement learning algorithm (PPO), it demonstrates that neural agents can effectively collaborate with followers of varying confidence and autonomy, while considering the communicative effort in the learning signal."
}