{
    "title": "Evaluation Metrics for Text Data Augmentation in NLP",
    "abstract": "Recent surveys on data augmentation for natural language processing have reported different techniques and advancements in the field. Several frameworks, tools, and repositories promote the implementation of text data augmentation pipelines. However, a lack of evaluation criteria and standards for method comparison due to different tasks, metrics, datasets, architectures, and experimental settings makes comparisons meaningless. Also, a lack of methods unification exists and text data augmentation research would benefit from unified metrics to compare different augmentation methods. Thus, academics and the industry endeavor relevant evaluation metrics for text data augmentation techniques. The contribution of this work is to provide a taxonomy of evaluation metrics for text augmentation methods and serve as a direction for a unified benchmark. The proposed taxonomy organizes categories that include tools for implementation and metrics calculation. Finally, with this study, we intend to ",
    "link": "https://arxiv.org/abs/2402.06766",
    "context": "Title: Evaluation Metrics for Text Data Augmentation in NLP\nAbstract: Recent surveys on data augmentation for natural language processing have reported different techniques and advancements in the field. Several frameworks, tools, and repositories promote the implementation of text data augmentation pipelines. However, a lack of evaluation criteria and standards for method comparison due to different tasks, metrics, datasets, architectures, and experimental settings makes comparisons meaningless. Also, a lack of methods unification exists and text data augmentation research would benefit from unified metrics to compare different augmentation methods. Thus, academics and the industry endeavor relevant evaluation metrics for text data augmentation techniques. The contribution of this work is to provide a taxonomy of evaluation metrics for text augmentation methods and serve as a direction for a unified benchmark. The proposed taxonomy organizes categories that include tools for implementation and metrics calculation. Finally, with this study, we intend to ",
    "path": "papers/24/02/2402.06766.json",
    "total_tokens": 811,
    "translated_title": "文本数据增强在自然语言处理中的评估指标",
    "translated_abstract": "最近关于自然语言处理的数据增强的调研报告指出了该领域的不同技术和进展。几个框架、工具和存储库推广了文本数据增强流水线的实施。然而，由于不同的任务、度量标准、数据集、体系结构和实验设置的缺乏评估标准和方法比较标准使得比较变得毫无意义。此外，缺乏方法的统一性，文本数据增强研究将受益于比较不同的增强方法的统一指标。因此，学术界和工业界都在努力寻找相关的文本数据增强技术的评估指标。本研究的贡献在于提供了文本增强方法的评估指标分类法，并作为统一基准的方向。所提出的分类法包括了实施工具和指标计算的类别。最后，通过这项研究，我们的目的是为了推进文本数据增强技术的评估指标的发展。",
    "tldr": "这项研究提供了文本增强方法的评估指标分类法，为统一基准提供方向和帮助。在不同任务、度量标准和实验设置下，该分类法有助于比较不同的增强方法。"
}