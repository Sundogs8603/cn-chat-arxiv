{
    "title": "Multilingual transformer and BERTopic for short text topic modeling: The case of Serbian",
    "abstract": "This paper presents the results of the first application of BERTopic, a state-of-the-art topic modeling technique, to short text written in a morphologi-cally rich language. We applied BERTopic with three multilingual embed-ding models on two levels of text preprocessing (partial and full) to evalu-ate its performance on partially preprocessed short text in Serbian. We also compared it to LDA and NMF on fully preprocessed text. The experiments were conducted on a dataset of tweets expressing hesitancy toward COVID-19 vaccination. Our results show that with adequate parameter setting, BERTopic can yield informative topics even when applied to partially pre-processed short text. When the same parameters are applied in both prepro-cessing scenarios, the performance drop on partially preprocessed text is minimal. Compared to LDA and NMF, judging by the keywords, BERTopic offers more informative topics and gives novel insights when the number of topics is not limited. The findings of this p",
    "link": "https://arxiv.org/abs/2402.03067",
    "context": "Title: Multilingual transformer and BERTopic for short text topic modeling: The case of Serbian\nAbstract: This paper presents the results of the first application of BERTopic, a state-of-the-art topic modeling technique, to short text written in a morphologi-cally rich language. We applied BERTopic with three multilingual embed-ding models on two levels of text preprocessing (partial and full) to evalu-ate its performance on partially preprocessed short text in Serbian. We also compared it to LDA and NMF on fully preprocessed text. The experiments were conducted on a dataset of tweets expressing hesitancy toward COVID-19 vaccination. Our results show that with adequate parameter setting, BERTopic can yield informative topics even when applied to partially pre-processed short text. When the same parameters are applied in both prepro-cessing scenarios, the performance drop on partially preprocessed text is minimal. Compared to LDA and NMF, judging by the keywords, BERTopic offers more informative topics and gives novel insights when the number of topics is not limited. The findings of this p",
    "path": "papers/24/02/2402.03067.json",
    "total_tokens": 907,
    "translated_title": "多语言Transformer和BERTopic用于短文本主题建模：塞尔维亚语案例",
    "translated_abstract": "本文介绍了BERTopic这种最先进的主题建模技术在具有丰富形态语言的短文本中的首次应用结果。我们使用三个多语言嵌入模型以及两种文本预处理水平（部分和完全）对塞尔维亚语的部分预处理的短文本进行了BERTopic的性能评估。我们还将其与LDA和NMF在完全预处理文本上进行了比较。实验使用了一个表达对COVID-19疫苗疑虑的推文数据集。我们的结果表明，通过适当的参数设置，BERTopic即使应用于部分预处理的短文本，也能产生信息丰富的主题。当在两种预处理场景下应用相同的参数时，部分预处理文本的性能下降很小。与LDA和NMF相比，从关键词来看，BERTopic提供了更丰富的主题，并在主题数量不受限制时提供了新的见解。",
    "tldr": "本文介绍了BERTopic在塞尔维亚语短文本中的首次应用，结果显示BERTopic可以产生丰富的主题，即使是部分预处理的文本。与传统方法相比，BERTopic在主题数量不受限制时提供了更多信息和新的见解。",
    "en_tdlr": "This paper presents the first application of BERTopic to short text in Serbian and demonstrates that it can generate informative topics even with partially preprocessed text. Compared to traditional methods, BERTopic offers more information and novel insights when the number of topics is not limited."
}