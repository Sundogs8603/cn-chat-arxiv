{
    "title": "On the Multi-turn Instruction Following for Conversational Web Agents",
    "abstract": "arXiv:2402.15057v1 Announce Type: cross  Abstract: Web agents powered by Large Language Models (LLMs) have demonstrated remarkable abilities in planning and executing multi-step interactions within complex web-based environments, fulfilling a wide range of web navigation tasks. Despite these advancements, the potential for LLM-powered agents to effectively engage with sequential user instructions in real-world scenarios has not been fully explored. In this work, we introduce a new task of Conversational Web Navigation, which necessitates sophisticated interactions that span multiple turns with both the users and the environment, supported by a specially developed dataset named Multi-Turn Mind2Web (MT-Mind2Web). To tackle the limited context length of LLMs and the context-dependency issue of the conversational tasks, we further propose a novel framework, named self-reflective memory-augmented planning (Self-MAP), which employs memory utilization and self-reflection techniques. Extensive",
    "link": "https://arxiv.org/abs/2402.15057",
    "context": "Title: On the Multi-turn Instruction Following for Conversational Web Agents\nAbstract: arXiv:2402.15057v1 Announce Type: cross  Abstract: Web agents powered by Large Language Models (LLMs) have demonstrated remarkable abilities in planning and executing multi-step interactions within complex web-based environments, fulfilling a wide range of web navigation tasks. Despite these advancements, the potential for LLM-powered agents to effectively engage with sequential user instructions in real-world scenarios has not been fully explored. In this work, we introduce a new task of Conversational Web Navigation, which necessitates sophisticated interactions that span multiple turns with both the users and the environment, supported by a specially developed dataset named Multi-Turn Mind2Web (MT-Mind2Web). To tackle the limited context length of LLMs and the context-dependency issue of the conversational tasks, we further propose a novel framework, named self-reflective memory-augmented planning (Self-MAP), which employs memory utilization and self-reflection techniques. Extensive",
    "path": "papers/24/02/2402.15057.json",
    "total_tokens": 868,
    "translated_title": "关于面向对话式网络代理的多轮指令跟踪",
    "translated_abstract": "由大型语言模型（LLMs）驱动的网络代理在规划和执行复杂基于网络的多步交互方面展示了出色的能力，完成了各种网络导航任务。然而，尽管取得了这些进展，以LLM为动力的代理在真实场景中有效与顺序用户指令进行交互的潜力尚未得到充分探索。本研究介绍了一个名为对话式网络导航的新任务，该任务需要与用户和环境进行跨多轮的复杂交互，支持使用一个名为多轮Mind2Web（MT-Mind2Web）的特别开发的数据集。为了解决LLMs的有限上下文长度和对话任务的上下文依赖性问题，我们进一步提出了一种名为自反映记忆增强规划（Self-MAP）的新框架，采用了记忆利用和自我反思技术。",
    "tldr": "提出了一个新任务——对话式网络导航，引入了一个名为MT-Mind2Web的特殊数据集，并提出了一个名为Self-MAP的框架，旨在解决大型语言模型在多轮指令跟踪中的长度和上下文依赖性问题。",
    "en_tdlr": "Introduced a new task - Conversational Web Navigation, introduced a specialized dataset named MT-Mind2Web, and proposed a framework named Self-MAP to address the length and context-dependency issues in multi-turn instruction following with large language models."
}