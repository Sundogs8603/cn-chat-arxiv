{
    "title": "PALO: A Polyglot Large Multimodal Model for 5B People",
    "abstract": "arXiv:2402.14818v1 Announce Type: new  Abstract: In pursuit of more inclusive Vision-Language Models (VLMs), this study introduces a Large Multilingual Multimodal Model called \\textsc{Palo}. \\textsc{Palo} offers visual reasoning capabilities in 10 major languages, including English, Chinese, Hindi, Spanish, French, Arabic, Bengali, Russian, Urdu, and Japanese, that span a total of $\\sim$5B people (65\\% of the world population). Our approach involves a semi-automated translation approach to adapt the multimodal instruction dataset from English to the target languages using a fine-tuned Large Language Model, thereby ensuring high linguistic fidelity while allowing scalability due to minimal manual effort. The incorporation of diverse instruction sets helps us boost overall performance across multiple languages especially those that are underrepresented like Hindi, Arabic, Bengali, and Urdu. The resulting models are trained across three scales (1.7B, 7B and 13B parameters) to show the gen",
    "link": "https://arxiv.org/abs/2402.14818",
    "context": "Title: PALO: A Polyglot Large Multimodal Model for 5B People\nAbstract: arXiv:2402.14818v1 Announce Type: new  Abstract: In pursuit of more inclusive Vision-Language Models (VLMs), this study introduces a Large Multilingual Multimodal Model called \\textsc{Palo}. \\textsc{Palo} offers visual reasoning capabilities in 10 major languages, including English, Chinese, Hindi, Spanish, French, Arabic, Bengali, Russian, Urdu, and Japanese, that span a total of $\\sim$5B people (65\\% of the world population). Our approach involves a semi-automated translation approach to adapt the multimodal instruction dataset from English to the target languages using a fine-tuned Large Language Model, thereby ensuring high linguistic fidelity while allowing scalability due to minimal manual effort. The incorporation of diverse instruction sets helps us boost overall performance across multiple languages especially those that are underrepresented like Hindi, Arabic, Bengali, and Urdu. The resulting models are trained across three scales (1.7B, 7B and 13B parameters) to show the gen",
    "path": "papers/24/02/2402.14818.json",
    "total_tokens": 983,
    "translated_title": "PALO：一个针对50亿人的多语言多模态模型",
    "translated_abstract": "本研究旨在推动更具包容性的视觉-语言模型(VLMs)，引入了一个名为\\Palo 的大型多语种多模态模型。Palo 在包括英语、中文、印地语、西班牙语、法语、阿拉伯语、孟加拉语、俄语、乌尔都语和日语在内的10种主要语言中提供视觉推理能力，涵盖了约50亿人口（全球人口的65%）。我们采用半自动化翻译方法，利用经过微调的大型语言模型，将多模态指导数据集从英语翻译到目标语言，从而确保了较高的语言保真度，同时由于减少了手动工作，使可扩展性更强。引入多样化的指导集有助于提升跨多种语言的总体性能，特别是对那些欠代表的语言如印地语、阿拉伯语、孟加拉语和乌尔都语。最终的模型在三个规模（17B、70B和130B参数）上进行训练，以展示其泛用性能",
    "tldr": "该文介绍了一个名为PALO的大型多语种多模态模型，实现了对10种主要语言的视觉推理能力，涵盖了约50亿人口。其通过半自动化翻译方法，将多语言多模态数据集从英语翻译为目标语言，以提升跨多种语言的性能。",
    "en_tdlr": "This study introduces a large multilingual multimodal model called PALO, providing visual reasoning capabilities in 10 major languages, covering approximately 5 billion people. It utilizes a semi-automated translation approach to adapt multimodal instruction dataset from English to target languages to boost performance across multiple languages."
}