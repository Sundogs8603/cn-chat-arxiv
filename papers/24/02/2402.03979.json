{
    "title": "Cross Entropy versus Label Smoothing: A Neural Collapse Perspective",
    "abstract": "Label smoothing loss is a widely adopted technique to mitigate overfitting in deep neural networks. This paper studies label smoothing from the perspective of Neural Collapse (NC), a powerful empirical and theoretical framework which characterizes model behavior during the terminal phase of training. We first show empirically that models trained with label smoothing converge faster to neural collapse solutions and attain a stronger level of neural collapse. Additionally, we show that at the same level of NC1, models under label smoothing loss exhibit intensified NC2. These findings provide valuable insights into the performance benefits and enhanced model calibration under label smoothing loss. We then leverage the unconstrained feature model to derive closed-form solutions for the global minimizers for both loss functions and further demonstrate that models under label smoothing have a lower conditioning number and, therefore, theoretically converge faster. Our study, combining empiri",
    "link": "https://arxiv.org/abs/2402.03979",
    "context": "Title: Cross Entropy versus Label Smoothing: A Neural Collapse Perspective\nAbstract: Label smoothing loss is a widely adopted technique to mitigate overfitting in deep neural networks. This paper studies label smoothing from the perspective of Neural Collapse (NC), a powerful empirical and theoretical framework which characterizes model behavior during the terminal phase of training. We first show empirically that models trained with label smoothing converge faster to neural collapse solutions and attain a stronger level of neural collapse. Additionally, we show that at the same level of NC1, models under label smoothing loss exhibit intensified NC2. These findings provide valuable insights into the performance benefits and enhanced model calibration under label smoothing loss. We then leverage the unconstrained feature model to derive closed-form solutions for the global minimizers for both loss functions and further demonstrate that models under label smoothing have a lower conditioning number and, therefore, theoretically converge faster. Our study, combining empiri",
    "path": "papers/24/02/2402.03979.json",
    "total_tokens": 978,
    "translated_title": "交叉熵与标签平滑：神经崩溃的视角",
    "translated_abstract": "标签平滑损失是深度神经网络中广泛采用的一种技术，用于减轻过拟合。本文从神经崩溃（NC）的视角研究了标签平滑，这是一个强大的经验和理论框架，用于描述训练的最后阶段模型的行为。首先，我们通过实验证明，在标签平滑训练的模型更快地收敛到神经崩溃解，并达到更强的神经崩溃水平。此外，我们还表明，在相同的NC1水平下，标签平滑损失下的模型显示出加强的NC2。这些发现为理解标签平滑损失下的性能优势和增强的模型校准提供了有价值的见解。然后，我们利用无约束特征模型推导出两种损失函数的全局最小值的闭式解，并进一步证明了标签平滑下的模型具有较低的条件数，因此在理论上更快地收敛。我们的研究综合了经验和理论的方法，为理解标签平滑的效果提供了重要的贡献。",
    "tldr": "本研究从神经崩溃的视角研究了标签平滑，并发现模型在标签平滑训练下更快地收敛到神经崩溃解，并达到更强的神经崩溃水平。此外，标签平滑损失下的模型在相同的NC1水平下表现出加强的NC2，并可在理论上更快地收敛。"
}