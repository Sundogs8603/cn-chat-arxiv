{
    "title": "Connect Later: Improving Fine-tuning for Robustness with Targeted Augmentations",
    "abstract": "Models trained on a labeled source domain (e.g., labeled images from wildlife camera traps) often generalize poorly when deployed on an out-of-distribution (OOD) target domain (e.g., images from new camera trap locations). In the domain adaptation setting where unlabeled target data is available, self-supervised pretraining (e.g., masked autoencoding or contrastive learning) is a promising method to mitigate this performance drop. Pretraining improves OOD error when the generic data augmentations used (e.g., masking or cropping) connect the source and target domains, which may be far apart in the input space. In this paper, we show on real-world tasks that standard fine-tuning after pretraining does not consistently improve OOD error over simply training from scratch on labeled source data. To better leverage pretraining for distribution shifts, we propose Connect Later: after pretraining with generic augmentations, fine-tune with targeted augmentations designed with knowledge of the d",
    "link": "https://arxiv.org/abs/2402.03325",
    "context": "Title: Connect Later: Improving Fine-tuning for Robustness with Targeted Augmentations\nAbstract: Models trained on a labeled source domain (e.g., labeled images from wildlife camera traps) often generalize poorly when deployed on an out-of-distribution (OOD) target domain (e.g., images from new camera trap locations). In the domain adaptation setting where unlabeled target data is available, self-supervised pretraining (e.g., masked autoencoding or contrastive learning) is a promising method to mitigate this performance drop. Pretraining improves OOD error when the generic data augmentations used (e.g., masking or cropping) connect the source and target domains, which may be far apart in the input space. In this paper, we show on real-world tasks that standard fine-tuning after pretraining does not consistently improve OOD error over simply training from scratch on labeled source data. To better leverage pretraining for distribution shifts, we propose Connect Later: after pretraining with generic augmentations, fine-tune with targeted augmentations designed with knowledge of the d",
    "path": "papers/24/02/2402.03325.json",
    "total_tokens": 962,
    "translated_title": "连接延迟：利用定向增强方法提高鲁棒性的微调改进",
    "translated_abstract": "在标记的源领域上训练的模型（例如野生动物相机陷阱的标记图像）通常在部署到分布不同的目标领域（例如新的相机陷阱位置的图像）时泛化能力较差。在存在未标记的目标数据的域适应设置中，自监督预训练（例如遮蔽自编码或对比学习）是缓解性能下降的一种有希望的方法。预训练可以通过将源领域和目标领域相连接的通用数据增强方法（例如遮蔽或剪裁）来提高分布不同的错误率，即使输入空间中的两个领域相差很远。本文通过真实任务展示了在预训练后进行标准微调并不能持续改善分布不同的错误率，相比在标记的源数据上从头训练。为了更好地利用预训练来应对分布转变，我们提出了连接延迟（Connect Later）：在使用通用增强方法进行预训练后，用基于对目标领域了解的定向增强方法进行微调。",
    "tldr": "本文研究了在标记的源领域上训练的模型在部署到分布不同的目标领域时的泛化问题，并提出了一种名为连接延迟的方法，通过自监督预训练和定向增强方法来改善模型鲁棒性。",
    "en_tdlr": "This paper investigates the generalization issue of models trained on labeled source domains when deployed on out-of-distribution target domains, and proposes a method called Connect Later, which combines self-supervised pretraining with targeted augmentations to improve model robustness."
}