{
    "title": "SaGE: Evaluating Moral Consistency in Large Language Models",
    "abstract": "arXiv:2402.13709v1 Announce Type: cross  Abstract: Despite recent advancements showcasing the impressive capabilities of Large Language Models (LLMs) in conversational systems, we show that even state-of-the-art LLMs are morally inconsistent in their generations, questioning their reliability (and trustworthiness in general). Prior works in LLM evaluation focus on developing ground-truth data to measure accuracy on specific tasks. However, for moral scenarios that often lack universally agreed-upon answers, consistency in model responses becomes crucial for their reliability. To address this issue, we propose an information-theoretic measure called Semantic Graph Entropy (SaGE), grounded in the concept of \"Rules of Thumb\" (RoTs) to measure a model's moral consistency. RoTs are abstract principles learned by a model and can help explain their decision-making strategies effectively. To this extent, we construct the Moral Consistency Corpus (MCC), containing 50K moral questions, responses",
    "link": "https://arxiv.org/abs/2402.13709",
    "context": "Title: SaGE: Evaluating Moral Consistency in Large Language Models\nAbstract: arXiv:2402.13709v1 Announce Type: cross  Abstract: Despite recent advancements showcasing the impressive capabilities of Large Language Models (LLMs) in conversational systems, we show that even state-of-the-art LLMs are morally inconsistent in their generations, questioning their reliability (and trustworthiness in general). Prior works in LLM evaluation focus on developing ground-truth data to measure accuracy on specific tasks. However, for moral scenarios that often lack universally agreed-upon answers, consistency in model responses becomes crucial for their reliability. To address this issue, we propose an information-theoretic measure called Semantic Graph Entropy (SaGE), grounded in the concept of \"Rules of Thumb\" (RoTs) to measure a model's moral consistency. RoTs are abstract principles learned by a model and can help explain their decision-making strategies effectively. To this extent, we construct the Moral Consistency Corpus (MCC), containing 50K moral questions, responses",
    "path": "papers/24/02/2402.13709.json",
    "total_tokens": 859,
    "translated_title": "SaGE：评估大型语言模型的道德一致性",
    "translated_abstract": "尽管最近展示出大型语言模型（LLMs）在会话系统中的印象深刻能力，但我们表明即使是最先进的LLMs在生成过程中也存在道德不一致，对其可靠性（以及总体可信赖性）提出了质疑。以往在LLM评估领域的工作侧重于开发地面真实数据，以衡量在特定任务上的准确性。然而，对于道德情景往往缺乏普遍认同答案的情况，模型响应的一致性对于其可靠性变得至关重要。为了解决这一问题，我们提出了一种信息理论度量方法，称为语义图熵（SaGE），基于“经验法则”（RoTs）的概念来衡量模型的道德一致性。RoTs是模型学习到的抽象原则，可有效帮助解释其决策策略。在此基础上，我们构建了道德一致性语料库（MCC），包含50K个道德问题、回答。",
    "tldr": "提出SaGE方法，通过语义图熵来衡量大型语言模型道德一致性，构建了MCC语料库。",
    "en_tdlr": "SaGE method is proposed to measure the moral consistency of Large Language Models using Semantic Graph Entropy and a Moral Consistency Corpus (MCC) is constructed."
}