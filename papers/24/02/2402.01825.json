{
    "title": "Fractal Patterns May Unravel the Intelligence in Next-Token Prediction",
    "abstract": "We study the fractal structure of language, aiming to provide a precise formalism for quantifying properties that may have been previously suspected but not formally shown. We establish that language is: (1) self-similar, exhibiting complexities at all levels of granularity, with no particular characteristic context length, and (2) long-range dependent (LRD), with a Hurst parameter of approximately H=0.70. Based on these findings, we argue that short-term patterns/dependencies in language, such as in paragraphs, mirror the patterns/dependencies over larger scopes, like entire documents. This may shed some light on how next-token prediction can lead to a comprehension of the structure of text at multiple levels of granularity, from words and clauses to broader contexts and intents. We also demonstrate that fractal parameters improve upon perplexity-based bits-per-byte (BPB) in predicting downstream performance. We hope these findings offer a fresh perspective on language and the mechani",
    "link": "https://arxiv.org/abs/2402.01825",
    "context": "Title: Fractal Patterns May Unravel the Intelligence in Next-Token Prediction\nAbstract: We study the fractal structure of language, aiming to provide a precise formalism for quantifying properties that may have been previously suspected but not formally shown. We establish that language is: (1) self-similar, exhibiting complexities at all levels of granularity, with no particular characteristic context length, and (2) long-range dependent (LRD), with a Hurst parameter of approximately H=0.70. Based on these findings, we argue that short-term patterns/dependencies in language, such as in paragraphs, mirror the patterns/dependencies over larger scopes, like entire documents. This may shed some light on how next-token prediction can lead to a comprehension of the structure of text at multiple levels of granularity, from words and clauses to broader contexts and intents. We also demonstrate that fractal parameters improve upon perplexity-based bits-per-byte (BPB) in predicting downstream performance. We hope these findings offer a fresh perspective on language and the mechani",
    "path": "papers/24/02/2402.01825.json",
    "total_tokens": 975,
    "translated_title": "分形模式可能揭示下一个词预测中的智能",
    "translated_abstract": "我们研究语言的分形结构，旨在提供一个精确的形式化方法来量化可能之前只有怀疑但尚未正式证明的属性。我们证明了语言具有以下特点：（1）自相似性，展示出各个层级上的复杂性，没有特定的特征上下文长度；（2）长程相关性（LRD），具有大约H=0.70的Hurst参数。基于这些发现，我们认为语言中的短期模式/依赖性，如段落中的模式/依赖性，反映了更大范围的模式/依赖性，如整个文档。这可能有助于理解下一个词预测如何导致对文本的多个层级结构，从单词和从句到更广泛的上下文和意图的理解。我们还证明了分形参数在预测下游性能方面优于基于困惑度的每字节比特（BPB）。我们希望这些发现能为语言和机制提供一种新的视角。",
    "tldr": "通过研究语言的分形结构，我们发现语言具有自相似性和长程相关性，从段落到整个文档都存在相似的模式和依赖性。这些发现有助于理解如何通过预测下一个词来理解文本的多个层级结构。分形参数在预测性能方面优于困惑度。这些发现提供了对语言和机制的新视角。",
    "en_tdlr": "By studying the fractal structure of language, we discovered that language exhibits self-similarity and long-range dependencies, with similar patterns and dependencies from paragraphs to entire documents. These findings contribute to understanding the hierarchical structure of text by predicting the next word. Fractal parameters outperform perplexity-based models in prediction performance. These findings offer a new perspective on language and mechanisms."
}