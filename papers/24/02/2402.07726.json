{
    "title": "Unsupervised Sign Language Translation and Generation",
    "abstract": "Motivated by the success of unsupervised neural machine translation (UNMT), we introduce an unsupervised sign language translation and generation network (USLNet), which learns from abundant single-modality (text and video) data without parallel sign language data. USLNet comprises two main components: single-modality reconstruction modules (text and video) that rebuild the input from its noisy version in the same modality and cross-modality back-translation modules (text-video-text and video-text-video) that reconstruct the input from its noisy version in the different modality using back-translation procedure.Unlike the single-modality back-translation procedure in text-based UNMT, USLNet faces the cross-modality discrepancy in feature representation, in which the length and the feature dimension mismatch between text and video sequences. We propose a sliding window method to address the issues of aligning variable-length text with video sequences. To our knowledge, USLNet is the fir",
    "link": "https://arxiv.org/abs/2402.07726",
    "context": "Title: Unsupervised Sign Language Translation and Generation\nAbstract: Motivated by the success of unsupervised neural machine translation (UNMT), we introduce an unsupervised sign language translation and generation network (USLNet), which learns from abundant single-modality (text and video) data without parallel sign language data. USLNet comprises two main components: single-modality reconstruction modules (text and video) that rebuild the input from its noisy version in the same modality and cross-modality back-translation modules (text-video-text and video-text-video) that reconstruct the input from its noisy version in the different modality using back-translation procedure.Unlike the single-modality back-translation procedure in text-based UNMT, USLNet faces the cross-modality discrepancy in feature representation, in which the length and the feature dimension mismatch between text and video sequences. We propose a sliding window method to address the issues of aligning variable-length text with video sequences. To our knowledge, USLNet is the fir",
    "path": "papers/24/02/2402.07726.json",
    "total_tokens": 1036,
    "translated_title": "无监督的手语翻译和生成",
    "translated_abstract": "受无监督神经机器翻译（UNMT）的成功启发，我们引入了一个无监督的手语翻译和生成网络（USLNet），它能够从大量的单模态（文本和视频）数据中学习，而无需平行的手语数据。USLNet包括两个主要组成部分：单模态重建模块（文本和视频）用于在相同模态下从嘈杂的输入中重建输入，以及跨模态反向翻译模块（文本-视频-文本和视频-文本-视频）用于使用反向翻译过程从不同模态下的嘈杂输入中重建输入。与基于文本的UNMT中的单模态反向翻译过程不同，USLNet面临着特征表示中的跨模态差异，即文本和视频序列之间的长度和特征维度的不匹配。我们提出了一种滑动窗口方法来解决对齐可变长度的文本和视频序列的问题。据我们所知，USLNet是第一个实现无监督手语翻译和生成的方法。",
    "tldr": "本文介绍了一个无监督的手语翻译和生成网络（USLNet），它通过利用大量的单模态数据（文本和视频）学习，而无需平行手语数据。USLNet采用不同模态的反向翻译和重建技术，面对文本和视频序列之间的特征表示差异。通过使用滑动窗口方法，USLNet能够有效对齐不同长度的文本和视频序列。这是第一个实现无监督手语翻译和生成的方法。",
    "en_tdlr": "This paper introduces an unsupervised sign language translation and generation network (USLNet), which learns from abundant single-modality data (text and video) without parallel sign language data. USLNet utilizes cross-modality back-translation and reconstruction techniques to address the discrepancies in feature representation between text and video sequences. By employing a sliding window method, USLNet is able to effectively align variable-length text and video sequences. It is the first method to achieve unsupervised sign language translation and generation."
}