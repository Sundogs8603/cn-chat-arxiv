{
    "title": "Overview of the L3DAS23 Challenge on Audio-Visual Extended Reality",
    "abstract": "arXiv:2402.09245v1 Announce Type: cross Abstract: The primary goal of the L3DAS23 Signal Processing Grand Challenge at ICASSP 2023 is to promote and support collaborative research on machine learning for 3D audio signal processing, with a specific emphasis on 3D speech enhancement and 3D Sound Event Localization and Detection in Extended Reality applications. As part of our latest competition, we provide a brand-new dataset, which maintains the same general characteristics of the L3DAS21 and L3DAS22 datasets, but with first-order Ambisonics recordings from multiple reverberant simulated environments. Moreover, we start exploring an audio-visual scenario by providing images of these environments, as perceived by the different microphone positions and orientations. We also propose updated baseline models for both tasks that can now support audio-image couples as input and a supporting API to replicate our results. Finally, we present the results of the participants. Further details about",
    "link": "https://arxiv.org/abs/2402.09245",
    "context": "Title: Overview of the L3DAS23 Challenge on Audio-Visual Extended Reality\nAbstract: arXiv:2402.09245v1 Announce Type: cross Abstract: The primary goal of the L3DAS23 Signal Processing Grand Challenge at ICASSP 2023 is to promote and support collaborative research on machine learning for 3D audio signal processing, with a specific emphasis on 3D speech enhancement and 3D Sound Event Localization and Detection in Extended Reality applications. As part of our latest competition, we provide a brand-new dataset, which maintains the same general characteristics of the L3DAS21 and L3DAS22 datasets, but with first-order Ambisonics recordings from multiple reverberant simulated environments. Moreover, we start exploring an audio-visual scenario by providing images of these environments, as perceived by the different microphone positions and orientations. We also propose updated baseline models for both tasks that can now support audio-image couples as input and a supporting API to replicate our results. Finally, we present the results of the participants. Further details about",
    "path": "papers/24/02/2402.09245.json",
    "total_tokens": 891,
    "translated_title": "L3DAS23挑战赛关于音频-视觉扩展现实的综述",
    "translated_abstract": "L3DAS23信号处理大赛的主要目标是促进和支持机器学习在3D音频信号处理方面的合作研究，特别关注扩展现实应用中的3D语音增强以及3D声音事件定位和检测。作为最新一届比赛的一部分，我们提供了全新的数据集，该数据集与L3DAS21和L3DAS22的数据集具有相同的一般特性，但是使用了多个混响模拟环境中的一阶Ambisonics录音。此外，我们开始探索音频-视觉场景，通过提供不同麦克风位置和方向所感知的这些环境的图像。我们还提出了更新的两个任务的基准模型，可以支持音频-图像对作为输入，并提供了一个支持的API来复制我们的结果。最后，我们呈现了参与者的结果。更多详情请参见",
    "tldr": "L3DAS23挑战赛旨在推进机器学习在3D音频信号处理方面的合作研究，并提供了更新的数据集和基准模型，用于音频-视觉扩展现实应用中的3D语音增强和3D声音事件定位和检测。"
}