{
    "title": "Generative Pretrained Hierarchical Transformer for Time Series Forecasting",
    "abstract": "arXiv:2402.16516v1 Announce Type: new  Abstract: Recent efforts have been dedicated to enhancing time series forecasting accuracy by introducing advanced network architectures and self-supervised pretraining strategies. Nevertheless, existing approaches still exhibit two critical drawbacks. Firstly, these methods often rely on a single dataset for training, limiting the model's generalizability due to the restricted scale of the training data. Secondly, the one-step generation schema is widely followed, which necessitates a customized forecasting head and overlooks the temporal dependencies in the output series, and also leads to increased training costs under different horizon length settings.   To address these issues, we propose a novel generative pretrained hierarchical transformer architecture for forecasting, named GPHT. There are two aspects of key designs in GPHT. On the one hand, we advocate for constructing a mixed dataset for pretraining our model, comprising various dataset",
    "link": "https://arxiv.org/abs/2402.16516",
    "context": "Title: Generative Pretrained Hierarchical Transformer for Time Series Forecasting\nAbstract: arXiv:2402.16516v1 Announce Type: new  Abstract: Recent efforts have been dedicated to enhancing time series forecasting accuracy by introducing advanced network architectures and self-supervised pretraining strategies. Nevertheless, existing approaches still exhibit two critical drawbacks. Firstly, these methods often rely on a single dataset for training, limiting the model's generalizability due to the restricted scale of the training data. Secondly, the one-step generation schema is widely followed, which necessitates a customized forecasting head and overlooks the temporal dependencies in the output series, and also leads to increased training costs under different horizon length settings.   To address these issues, we propose a novel generative pretrained hierarchical transformer architecture for forecasting, named GPHT. There are two aspects of key designs in GPHT. On the one hand, we advocate for constructing a mixed dataset for pretraining our model, comprising various dataset",
    "path": "papers/24/02/2402.16516.json",
    "total_tokens": 801,
    "translated_title": "针对时间序列预测的生成式预训练分层Transformer",
    "translated_abstract": "最近的研究致力于通过引入先进的网络架构和自监督预训练策略来提高时间序列预测的准确性。然而，现有方法仍存在两个关键缺陷。首先，这些方法通常依赖于单一数据集进行训练，由于训练数据的规模受限，限制了模型的泛化能力。其次，广泛采用一步生成模式，这需要定制化的预测头部，忽略了输出序列中的时间依赖性，同时在不同的时间跨度设置下会导致增加的训练成本。为了解决这些问题，我们提出了一种新颖的用于预测的生成式预训练分层Transformer架构，命名为GPHT。GPHT中有两个关键设计方面。",
    "tldr": "提出一种名为GPHT的新型生成式预训练分层Transformer架构，通过构建混合数据集来预训练模型，解决了时间序列预测中数据集限制和时间依赖性忽视的问题",
    "en_tdlr": "Introducing a novel generative pretrained hierarchical transformer architecture named GPHT, the paper addresses the limitations in time series forecasting by proposing mixed dataset pretraining to overcome dataset constraints and temporal dependencies neglect."
}