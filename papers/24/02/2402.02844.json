{
    "title": "Comparing Knowledge Sources for Open-Domain Scientific Claim Verification",
    "abstract": "The increasing rate at which scientific knowledge is discovered and health claims shared online has highlighted the importance of developing efficient fact-checking systems for scientific claims. The usual setting for this task in the literature assumes that the documents containing the evidence for claims are already provided and annotated or contained in a limited corpus. This renders the systems unrealistic for real-world settings where knowledge sources with potentially millions of documents need to be queried to find relevant evidence. In this paper, we perform an array of experiments to test the performance of open-domain claim verification systems. We test the final verdict prediction of systems on four datasets of biomedical and health claims in different settings. While keeping the pipeline's evidence selection and verdict prediction parts constant, document retrieval is performed over three common knowledge sources (PubMed, Wikipedia, Google) and using two different informati",
    "link": "https://arxiv.org/abs/2402.02844",
    "context": "Title: Comparing Knowledge Sources for Open-Domain Scientific Claim Verification\nAbstract: The increasing rate at which scientific knowledge is discovered and health claims shared online has highlighted the importance of developing efficient fact-checking systems for scientific claims. The usual setting for this task in the literature assumes that the documents containing the evidence for claims are already provided and annotated or contained in a limited corpus. This renders the systems unrealistic for real-world settings where knowledge sources with potentially millions of documents need to be queried to find relevant evidence. In this paper, we perform an array of experiments to test the performance of open-domain claim verification systems. We test the final verdict prediction of systems on four datasets of biomedical and health claims in different settings. While keeping the pipeline's evidence selection and verdict prediction parts constant, document retrieval is performed over three common knowledge sources (PubMed, Wikipedia, Google) and using two different informati",
    "path": "papers/24/02/2402.02844.json",
    "total_tokens": 829,
    "translated_title": "对开放领域科学主张验证的知识来源的比较",
    "translated_abstract": "科学知识的发现速度和在线健康主张的分享增加，凸显了为科学主张开发高效事实检核系统的重要性。现有文献中，这项任务的常见设置假设已经提供并注释或包含在有限语料库中的包含证据的文档。这使得该系统在可能需要查询数百万个文档以找到相关证据的现实世界环境中变得不切实际。在本文中，我们进行了一系列实验，以测试开放领域主张验证系统的性能。我们在不同设置下测试了四个数据集中的生物医学和健康主张系统的最终判断预测。在保持流水线的证据选择和判断预测部分不变的情况下，使用三种常见知识来源（PubMed、维基百科、谷歌）和两种不同的信息检索方法进行文档检索。",
    "tldr": "在本论文中，我们对开放领域科学主张验证系统的性能进行了测试，通过比较不同的知识来源（PubMed、维基百科、谷歌）和信息检索方法，我们发现在真实世界环境中进行科学主张验证的挑战和问题。",
    "en_tdlr": "In this paper, we perform experiments to compare the performance of open-domain claim verification systems using different knowledge sources (PubMed, Wikipedia, Google) and information retrieval methods. We find challenges and issues in conducting scientific claim verification in real-world settings."
}