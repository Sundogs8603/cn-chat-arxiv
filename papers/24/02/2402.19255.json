{
    "title": "GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers",
    "abstract": "arXiv:2402.19255v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved impressive performance across various mathematical reasoning benchmarks. However, there are increasing debates regarding whether these models truly understand and apply mathematical knowledge or merely rely on shortcuts for mathematical reasoning. One essential and frequently occurring evidence is that when the math questions are slightly changed, LLMs can behave incorrectly. This motivates us to evaluate the robustness of LLMs' math reasoning capability by testing a wide range of question variations. We introduce the adversarial grade school math (\\datasetname) dataset, an extension of GSM8K augmented with various mathematical perturbations. Our experiments on 25 LLMs and 4 prompting techniques show that while LLMs exhibit different levels of math reasoning abilities, their performances are far from robust. In particular, even for problems that have been solved in GSM8K, LLMs can make mistakes ",
    "link": "https://arxiv.org/abs/2402.19255",
    "context": "Title: GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers\nAbstract: arXiv:2402.19255v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved impressive performance across various mathematical reasoning benchmarks. However, there are increasing debates regarding whether these models truly understand and apply mathematical knowledge or merely rely on shortcuts for mathematical reasoning. One essential and frequently occurring evidence is that when the math questions are slightly changed, LLMs can behave incorrectly. This motivates us to evaluate the robustness of LLMs' math reasoning capability by testing a wide range of question variations. We introduce the adversarial grade school math (\\datasetname) dataset, an extension of GSM8K augmented with various mathematical perturbations. Our experiments on 25 LLMs and 4 prompting techniques show that while LLMs exhibit different levels of math reasoning abilities, their performances are far from robust. In particular, even for problems that have been solved in GSM8K, LLMs can make mistakes ",
    "path": "papers/24/02/2402.19255.json",
    "total_tokens": 885,
    "translated_title": "GSM-Plus：评估LLMs作为数学问题解决者的稳健性的全面基准",
    "translated_abstract": "大型语言模型（LLMs）在各种数学推理基准上取得了令人印象深刻的表现。然而，关于这些模型是否真正理解并应用数学知识，还是仅仅依赖于数学推理的捷径，存在越来越多的争论。一个基本且经常发生的证据是，当数学问题稍作更改时，LLMs可能会出现错误行为。这促使我们通过测试各种问题变化来评估LLMs的数学推理能力的稳健性。我们引入了对抗式小学数学（\\datasetname）数据集，这是对GSM8K的扩展，并添加了各种数学扰动。我们对25个LLMs和4种提示技术进行的实验表明，虽然LLMs展现出不同水平的数学推理能力，但它们的表现远非稳健。特别是，即使是在GSM8K中已解决的问题，LLMs也可能出错。",
    "tldr": "通过引入对抗式小学数学数据集（GSM-Plus），评估了25个LLMs和4种提示技术，在广泛的问题变化中展示LLMs的数学推理能力，并发现它们的表现远非稳健。"
}