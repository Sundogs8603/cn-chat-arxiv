{
    "title": "MULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse Worlds",
    "abstract": "Large Language Model (LLM) alignment aims to ensure that LLM outputs match with human values. Researchers have demonstrated the severity of alignment problems with a large spectrum of jailbreak techniques that can induce LLMs to produce malicious content during conversations. Finding the corresponding jailbreaking prompts usually requires substantial human intelligence or computation resources. In this paper, we report that LLMs have different levels of alignment in various contexts. As such, by systematically constructing many contexts, called worlds, leveraging a Domain Specific Language describing possible worlds (e.g., time, location, characters, actions and languages) and the corresponding compiler, we can cost-effectively expose latent alignment issues. Given the low cost of our method, we are able to conduct a large scale study regarding LLM alignment issues in different worlds. Our results show that our method outperforms the-state-of-the-art jailbreaking techniques on both eff",
    "link": "https://arxiv.org/abs/2402.01706",
    "context": "Title: MULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse Worlds\nAbstract: Large Language Model (LLM) alignment aims to ensure that LLM outputs match with human values. Researchers have demonstrated the severity of alignment problems with a large spectrum of jailbreak techniques that can induce LLMs to produce malicious content during conversations. Finding the corresponding jailbreaking prompts usually requires substantial human intelligence or computation resources. In this paper, we report that LLMs have different levels of alignment in various contexts. As such, by systematically constructing many contexts, called worlds, leveraging a Domain Specific Language describing possible worlds (e.g., time, location, characters, actions and languages) and the corresponding compiler, we can cost-effectively expose latent alignment issues. Given the low cost of our method, we are able to conduct a large scale study regarding LLM alignment issues in different worlds. Our results show that our method outperforms the-state-of-the-art jailbreaking techniques on both eff",
    "path": "papers/24/02/2402.01706.json",
    "total_tokens": 879,
    "translated_title": "MULTIVERSE: 在不同世界中揭示大型语言模型对齐问题",
    "translated_abstract": "大型语言模型（LLM）对齐旨在确保LLM的输出与人类价值相匹配。研究人员通过一系列越狱技术展示了对齐问题的严重性，这些技术可以在对话中诱使LLMs产生恶意内容。通常需要大量的人类智能或计算资源才能找到相应的越狱提示。本文报告了LLMs在不同语境下对齐水平的差异。因此，通过系统地构建许多被称为世界的语境、利用描述可能世界（如时间、地点、角色、行为和语言）的领域特定语言和相应的编译器，我们能够以较低成本揭示潜在的对齐问题。鉴于我们方法的低成本，我们能够对不同世界中LLM对齐问题进行大规模研究。我们的结果表明，我们的方法在效果上优于现有的越狱技术。",
    "tldr": "本文通过构建多种语境，使用领域特定语言描述可能世界，并利用编译器，发现了大型语言模型在不同语境下的对齐问题。这种方法成本较低，能够更全面地研究LLM对齐问题。"
}