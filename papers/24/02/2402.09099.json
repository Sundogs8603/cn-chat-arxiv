{
    "title": "Exploring Neuron Interactions and Emergence in LLMs: From the Multifractal Analysis Perspective",
    "abstract": "arXiv:2402.09099v1 Announce Type: new Abstract: Prior studies on the emergence in large models have primarily focused on how the functional capabilities of large language models (LLMs) scale with model size. Our research, however, transcends this traditional paradigm, aiming to deepen our understanding of the emergence within LLMs by placing a special emphasis not just on the model size but more significantly on the complex behavior of neuron interactions during the training process. By introducing the concepts of \"self-organization\" and \"multifractal analysis,\" we explore how neuron interactions dynamically evolve during training, leading to \"emergence,\" mirroring the phenomenon in natural systems where simple micro-level interactions give rise to complex macro-level behaviors. To quantitatively analyze the continuously evolving interactions among neurons in large models during training, we propose the Neuron-based Multifractal Analysis (NeuroMFA). Utilizing NeuroMFA, we conduct a com",
    "link": "https://arxiv.org/abs/2402.09099",
    "context": "Title: Exploring Neuron Interactions and Emergence in LLMs: From the Multifractal Analysis Perspective\nAbstract: arXiv:2402.09099v1 Announce Type: new Abstract: Prior studies on the emergence in large models have primarily focused on how the functional capabilities of large language models (LLMs) scale with model size. Our research, however, transcends this traditional paradigm, aiming to deepen our understanding of the emergence within LLMs by placing a special emphasis not just on the model size but more significantly on the complex behavior of neuron interactions during the training process. By introducing the concepts of \"self-organization\" and \"multifractal analysis,\" we explore how neuron interactions dynamically evolve during training, leading to \"emergence,\" mirroring the phenomenon in natural systems where simple micro-level interactions give rise to complex macro-level behaviors. To quantitatively analyze the continuously evolving interactions among neurons in large models during training, we propose the Neuron-based Multifractal Analysis (NeuroMFA). Utilizing NeuroMFA, we conduct a com",
    "path": "papers/24/02/2402.09099.json",
    "total_tokens": 1017,
    "translated_title": "通过多重分形分析视角探索LLMs中的神经元相互作用和出现现象",
    "translated_abstract": "在以往的大型模型中，关于出现现象的研究主要集中在大型语言模型（LLMs）的功能能力如何随模型规模的扩大而增加。然而，我们的研究超越了这一传统范式，旨在通过不仅仅依赖于模型规模，而更加关注训练过程中神经元相互作用的复杂行为，加深我们对LLMs内部出现现象的理解。通过引入“自组织”和“多重分形分析”概念，我们探索了神经元相互作用在训练过程中如何动态演化，从而导致“出现现象”，这种现象反映了自然系统中简单的微观相互作用如何导致复杂的宏观行为。为了定量分析训练过程中大型模型中神经元之间不断演化的相互作用，我们提出了基于神经元的多重分形分析（NeuroMFA）。利用NeuroMFA，我们进行了一系列的实验",
    "tldr": "该论文通过多重分形分析视角，深入研究了LLMs中神经元相互作用和出现现象。通过引入自组织和多重分形分析的概念，研究了神经元相互作用的动态演化过程，尤其关注训练中的复杂行为。通过提出基于神经元的多重分形分析方法，实现了对大型模型中神经元相互作用的定量分析。",
    "en_tdlr": "This paper explores neuron interactions and emergence in LLMs using the perspective of multifractal analysis. By introducing the concepts of self-organization and multifractal analysis, it investigates the dynamic evolution of neuron interactions during training, with a special emphasis on complex behavior. The proposed Neuron-based Multifractal Analysis enables quantitative analysis of neuron interactions in large models."
}