{
    "title": "Efficient Fine-tuning of Audio Spectrogram Transformers via Soft Mixture of Adapters",
    "abstract": "Mixture of Experts (MoE) architectures have recently started burgeoning due to their ability to scale model's capacity while maintaining the computational cost affordable. Furthermore, they can be applied to both Transformers and State Space Models, the current state-of-the-art models in numerous fields. While MoE has been mostly investigated for the pre-training stage, its use in parameter-efficient transfer learning settings is under-explored. To narrow this gap, this paper attempts to demystify the use of MoE for parameter-efficient fine-tuning of Audio Spectrogram Transformers to audio and speech downstream tasks. Specifically, we propose Soft Mixture of Adapters (Soft-MoA). It exploits adapters as the experts and, leveraging the recent Soft MoE method, it relies on a soft assignment between the input tokens and experts to keep the computational time limited. Extensive experiments across 4 benchmarks demonstrate that Soft-MoA outperforms the single adapter method and performs on pa",
    "link": "https://arxiv.org/abs/2402.00828",
    "context": "Title: Efficient Fine-tuning of Audio Spectrogram Transformers via Soft Mixture of Adapters\nAbstract: Mixture of Experts (MoE) architectures have recently started burgeoning due to their ability to scale model's capacity while maintaining the computational cost affordable. Furthermore, they can be applied to both Transformers and State Space Models, the current state-of-the-art models in numerous fields. While MoE has been mostly investigated for the pre-training stage, its use in parameter-efficient transfer learning settings is under-explored. To narrow this gap, this paper attempts to demystify the use of MoE for parameter-efficient fine-tuning of Audio Spectrogram Transformers to audio and speech downstream tasks. Specifically, we propose Soft Mixture of Adapters (Soft-MoA). It exploits adapters as the experts and, leveraging the recent Soft MoE method, it relies on a soft assignment between the input tokens and experts to keep the computational time limited. Extensive experiments across 4 benchmarks demonstrate that Soft-MoA outperforms the single adapter method and performs on pa",
    "path": "papers/24/02/2402.00828.json",
    "total_tokens": 842,
    "translated_title": "通过软适配器混合实现音频频谱变换的高效微调",
    "translated_abstract": "混合专家（MoE）架构近年来开始兴起，因为它们能够在保持计算成本可承受的情况下扩展模型容量。此外，它们可以应用于变换器和状态空间模型，这些是当前众多领域中颇有成就的模型。虽然MoE主要用于预训练阶段，但其在参数高效的迁移学习设置中的应用尚未得到充分探索。为了填补这一差距，本文试图揭示使用MoE进行参数高效音频频谱变换微调到音频和语音下游任务的方法。具体而言，我们提出了软适配器混合（Soft-MoA）方法。它使用适配器作为专家，并利用最近的软MoE方法，在输入记号和专家之间进行软分配，以保持计算时间有限。对4个基准任务的大量实验证明，Soft-MoA优于单一适配器方法，并在性能上表现出色。",
    "tldr": "本文研究了使用软适配器混合实现音频频谱变换的高效微调，证明了该方法在音频和语音任务中的优越性能。",
    "en_tdlr": "This paper investigates the efficient fine-tuning of audio spectrogram transformers via soft mixture of adapters and demonstrates its superior performance in audio and speech tasks."
}