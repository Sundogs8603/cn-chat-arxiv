{
    "title": "Scaffolding Coordinates to Promote Vision-Language Coordination in Large Multi-Modal Models",
    "abstract": "arXiv:2402.12058v1 Announce Type: cross  Abstract: State-of-the-art Large Multi-Modal Models (LMMs) have demonstrated exceptional capabilities in vision-language tasks. Despite their advanced functionalities, the performances of LMMs are still limited in challenging scenarios that require complex reasoning with multiple levels of visual information. Existing prompting techniques for LMMs focus on either improving textual reasoning or leveraging tools for image preprocessing, lacking a simple and general visual prompting scheme to promote vision-language coordination in LMMs. In this work, we propose Scaffold prompting that scaffolds coordinates to promote vision-language coordination. Specifically, Scaffold overlays a dot matrix within the image as visual information anchors and leverages multi-dimensional coordinates as textual positional references. Extensive experiments on a wide range of challenging vision-language tasks demonstrate the superiority of Scaffold over GPT-4V with the ",
    "link": "https://arxiv.org/abs/2402.12058",
    "context": "Title: Scaffolding Coordinates to Promote Vision-Language Coordination in Large Multi-Modal Models\nAbstract: arXiv:2402.12058v1 Announce Type: cross  Abstract: State-of-the-art Large Multi-Modal Models (LMMs) have demonstrated exceptional capabilities in vision-language tasks. Despite their advanced functionalities, the performances of LMMs are still limited in challenging scenarios that require complex reasoning with multiple levels of visual information. Existing prompting techniques for LMMs focus on either improving textual reasoning or leveraging tools for image preprocessing, lacking a simple and general visual prompting scheme to promote vision-language coordination in LMMs. In this work, we propose Scaffold prompting that scaffolds coordinates to promote vision-language coordination. Specifically, Scaffold overlays a dot matrix within the image as visual information anchors and leverages multi-dimensional coordinates as textual positional references. Extensive experiments on a wide range of challenging vision-language tasks demonstrate the superiority of Scaffold over GPT-4V with the ",
    "path": "papers/24/02/2402.12058.json",
    "total_tokens": 847,
    "translated_title": "通过搭建坐标来促进大型多模型的视觉-语言协调",
    "translated_abstract": "最先进的大型多模型(LMMs)在视觉-语言任务中展现出极高的能力。尽管它们功能先进，但在需要复杂推理和多个层次的视觉信息的挑战性场景中，LMMs的性能仍然受限。现有的LMMs提示技术要么专注于改进文本推理，要么利用图像预处理工具，缺乏一种简单且通用的视觉提示方案，以促进LMMs中的视觉-语言协调。在这项工作中，我们提出了搭建提示，通过搭建坐标来促进视觉-语言协调。具体而言，搭建提示在图像中叠加一个点阵作为视觉信息锚点，并利用多维坐标作为文本位置参考。对一系列具有挑战性的视觉-语言任务进行了大量实验，证明了搭建提示相对于GPT-4V的优越性。",
    "tldr": "提出了搭建提示方法，在图像中叠加点阵作为视觉信息锚点，并利用多维坐标作为文本位置参考，从而促进大型多模型的视觉-语言协调。"
}