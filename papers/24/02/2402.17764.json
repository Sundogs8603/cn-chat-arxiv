{
    "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits",
    "abstract": "arXiv:2402.17764v1 Announce Type: new  Abstract: Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.",
    "link": "https://arxiv.org/abs/2402.17764",
    "context": "Title: The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits\nAbstract: arXiv:2402.17764v1 Announce Type: new  Abstract: Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.",
    "path": "papers/24/02/2402.17764.json",
    "total_tokens": 896,
    "translated_title": "1比特LLM的时代：所有大型语言模型均为1.58比特",
    "translated_abstract": "近期的研究，如BitNet，正在为一个新时代的1比特大型语言模型（LLMs）铺平道路。在这项工作中，我们引入了一个1比特LLM变体，即BitNet b1.58，其中LLM的每个单个参数（或权重）均为三进制{-1, 0, 1}。它在困惑度和最终任务性能方面与相同模型大小和训练标记的全精度（即FP16或BF16）Transformer LLM相匹配，同时在延迟、内存、吞吐量和能耗方面显着更具成本效益。更重要的是，1.58比特的LLM定义了一种新的缩放定律和训练新一代既高性能又具成本效益的LLMs的配方。此外，它实现了一个新的计算范式，并为设计专门针对1比特LLMs优化的特定硬件敞开了大门。",
    "tldr": "介绍了一种新的1比特LLM变体，通过引入三进制参数在保持性能的情况下显著提高了成本效益，定义了新的训练规律，为设计专门硬件优化的1比特LLMs打开了大门",
    "en_tdlr": "Introducing a new variant of 1-bit LLM with ternary parameters significantly improves cost-effectiveness while maintaining performance, defining a new training law and opening the door for designing specific hardware optimized for 1-bit LLMs."
}