{
    "title": "The Surprising Effectiveness of Skip-Tuning in Diffusion Sampling",
    "abstract": "arXiv:2402.15170v1 Announce Type: cross  Abstract: With the incorporation of the UNet architecture, diffusion probabilistic models have become a dominant force in image generation tasks. One key design in UNet is the skip connections between the encoder and decoder blocks. Although skip connections have been shown to improve training stability and model performance, we reveal that such shortcuts can be a limiting factor for the complexity of the transformation. As the sampling steps decrease, the generation process and the role of the UNet get closer to the push-forward transformations from Gaussian distribution to the target, posing a challenge for the network's complexity. To address this challenge, we propose Skip-Tuning, a simple yet surprisingly effective training-free tuning method on the skip connections. Our method can achieve 100% FID improvement for pretrained EDM on ImageNet 64 with only 19 NFEs (1.75), breaking the limit of ODE samplers regardless of sampling steps. Surpris",
    "link": "https://arxiv.org/abs/2402.15170",
    "context": "Title: The Surprising Effectiveness of Skip-Tuning in Diffusion Sampling\nAbstract: arXiv:2402.15170v1 Announce Type: cross  Abstract: With the incorporation of the UNet architecture, diffusion probabilistic models have become a dominant force in image generation tasks. One key design in UNet is the skip connections between the encoder and decoder blocks. Although skip connections have been shown to improve training stability and model performance, we reveal that such shortcuts can be a limiting factor for the complexity of the transformation. As the sampling steps decrease, the generation process and the role of the UNet get closer to the push-forward transformations from Gaussian distribution to the target, posing a challenge for the network's complexity. To address this challenge, we propose Skip-Tuning, a simple yet surprisingly effective training-free tuning method on the skip connections. Our method can achieve 100% FID improvement for pretrained EDM on ImageNet 64 with only 19 NFEs (1.75), breaking the limit of ODE samplers regardless of sampling steps. Surpris",
    "path": "papers/24/02/2402.15170.json",
    "total_tokens": 853,
    "translated_title": "跳跃调谐在扩散采样中的惊人有效性",
    "translated_abstract": "随着UNet架构的整合，扩散概率模型已经成为图像生成任务中的一个主导力量。UNet中的一个关键设计是编码器和解码器块之间的跳跃连接。尽管已经证明跳跃连接可以提高训练稳定性和模型性能，我们发现这样的捷径可能限制了变换的复杂性。随着采样步骤减少，生成过程和UNet的作用更接近于从高斯分布向目标的推进转换，为网络的复杂性提出了挑战。为了解决这一挑战，我们提出了Skip-Tuning，一种简单而令人惊讶地有效的基于跳过连接的无需训练的调整方法。我们的方法可以在ImageNet 64上使用19个NFE（1.75）为预训练的EDM实现100%的FID改进，突破了ODE采样器的限制，不论采样步骤如何。",
    "tldr": "跳跃调谐是一种简单而惊人有效的训练方法，可以提高扩散采样中UNet模型的性能，并突破了ODE采样器的限制",
    "en_tdlr": "Skip-Tuning is a simple yet surprisingly effective training method that enhances the performance of UNet models in diffusion sampling and breaks the limitations of ODE samplers."
}