{
    "title": "FedMoE: Data-Level Personalization with Mixture of Experts for Model-Heterogeneous Personalized Federated Learning",
    "abstract": "Federated learning (FL) is widely employed for collaborative training on decentralized data but faces challenges like data, system, and model heterogeneity. This prompted the emergency of model-heterogeneous personalized federated learning (MHPFL). However, concerns persist regarding data and model privacy, model performance, communication, and computational costs in current MHPFL methods. To tackle these concerns, we propose a novel model-heterogeneous personalized Federated learning algorithm (FedMoE) with the Mixture of Experts (MoE), renowned for enhancing large language models (LLMs). It assigns a shared homogeneous small feature extractor and a local gating network for each client's local heterogeneous large model. (1) During local training, the local heterogeneous model's feature extractor acts as a local expert for personalized feature (representation) extraction, while the shared homogeneous small feature extractor serves as a global expert for generalized feature extraction. ",
    "link": "https://rss.arxiv.org/abs/2402.01350",
    "context": "Title: FedMoE: Data-Level Personalization with Mixture of Experts for Model-Heterogeneous Personalized Federated Learning\nAbstract: Federated learning (FL) is widely employed for collaborative training on decentralized data but faces challenges like data, system, and model heterogeneity. This prompted the emergency of model-heterogeneous personalized federated learning (MHPFL). However, concerns persist regarding data and model privacy, model performance, communication, and computational costs in current MHPFL methods. To tackle these concerns, we propose a novel model-heterogeneous personalized Federated learning algorithm (FedMoE) with the Mixture of Experts (MoE), renowned for enhancing large language models (LLMs). It assigns a shared homogeneous small feature extractor and a local gating network for each client's local heterogeneous large model. (1) During local training, the local heterogeneous model's feature extractor acts as a local expert for personalized feature (representation) extraction, while the shared homogeneous small feature extractor serves as a global expert for generalized feature extraction. ",
    "path": "papers/24/02/2402.01350.json",
    "total_tokens": 961,
    "translated_title": "FedMoE: 数据级别个性化的混合专家模型用于异构模型的个性化联邦学习",
    "translated_abstract": "联邦学习广泛应用于分散数据的协同训练，但面临数据、系统和模型异构等挑战。这导致模型异构个性化联邦学习 (MHPFL) 的出现。然而，当前的MHPFL方法在数据和模型隐私、模型性能、通信和计算成本方面仍存在关切。为应对这些问题，我们提出了一种新颖的模型异构个性化联邦学习算法 (FedMoE) ，采用著名的专家混合模型 (MoE) 来增强大型语言模型 (LLM)。它为每个客户端的本地异构大模型分配了一个共享的均匀小特征提取器和一个本地门控网络。(1) 在本地训练过程中，本地异构模型的特征提取器作为个性化特征（表示）提取的本地专家，而共享的均匀小特征提取器则作为广义特征提取的全局专家。",
    "tldr": "FedMoE是一种模型异构的个性化联邦学习算法，通过使用专家混合模型增强大型语言模型，将共享的小特征提取器和本地门控网络分配给每个客户端的本地异构大模型，以解决当前模型异构个性化联邦学习方法中存在的隐私、性能、通信和计算成本等问题。",
    "en_tdlr": "FedMoE is a model-heterogeneous personalized federated learning algorithm that enhances large language models by using a mixture of experts. It assigns a shared small feature extractor and a local gating network to each client's local heterogeneous large model, addressing concerns such as privacy, performance, communication, and computational costs in current model-heterogeneous personalized federated learning methods."
}