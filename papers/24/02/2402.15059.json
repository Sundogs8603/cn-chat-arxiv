{
    "title": "ColBERT-XM: A Modular Multi-Vector Representation Model for Zero-Shot Multilingual Information Retrieval",
    "abstract": "arXiv:2402.15059v1 Announce Type: new  Abstract: State-of-the-art neural retrievers predominantly focus on high-resource languages like English, which impedes their adoption in retrieval scenarios involving other languages. Current approaches circumvent the lack of high-quality labeled data in non-English languages by leveraging multilingual pretrained language models capable of cross-lingual transfer. However, these models require substantial task-specific fine-tuning across multiple languages, often perform poorly in languages with minimal representation in the pretraining corpus, and struggle to incorporate new languages after the pretraining phase. In this work, we present a novel modular dense retrieval model that learns from the rich data of a single high-resource language and effectively zero-shot transfers to a wide array of languages, thereby eliminating the need for language-specific labeled data. Our model, ColBERT-XM, demonstrates competitive performance against existing st",
    "link": "https://arxiv.org/abs/2402.15059",
    "context": "Title: ColBERT-XM: A Modular Multi-Vector Representation Model for Zero-Shot Multilingual Information Retrieval\nAbstract: arXiv:2402.15059v1 Announce Type: new  Abstract: State-of-the-art neural retrievers predominantly focus on high-resource languages like English, which impedes their adoption in retrieval scenarios involving other languages. Current approaches circumvent the lack of high-quality labeled data in non-English languages by leveraging multilingual pretrained language models capable of cross-lingual transfer. However, these models require substantial task-specific fine-tuning across multiple languages, often perform poorly in languages with minimal representation in the pretraining corpus, and struggle to incorporate new languages after the pretraining phase. In this work, we present a novel modular dense retrieval model that learns from the rich data of a single high-resource language and effectively zero-shot transfers to a wide array of languages, thereby eliminating the need for language-specific labeled data. Our model, ColBERT-XM, demonstrates competitive performance against existing st",
    "path": "papers/24/02/2402.15059.json",
    "total_tokens": 834,
    "translated_title": "ColBERT-XM：一种用于零-shot 多语信息检索的模块化多向量表示模型",
    "translated_abstract": "最先进的神经检索器主要集中在英语等高资源语言上，这阻碍了它们在涉及其他语言的检索场景中的应用。当前方法通过利用能够进行跨语言转移的多语言预训练语言模型来规避非英语语言中缺乏高质量标记数据的问题。然而，这些模型需要在多种语言上进行大量特定任务的微调，在预训练语料库中表示极少的语言中通常表现不佳，并且在预训练阶段之后难以融合新语言。在这项工作中，我们提出了一种新颖的模块化稠密检索模型，它从单一高资源语言的丰富数据中学习，并有效地零-shot 转移到广泛的语言，从而消除了对特定语言标记数据的需求。我们的模型 ColBERT-XM 在现有模型上展现出有竞争力的性能。",
    "tldr": "ColBERT-XM模型通过从单一高资源语言的数据中学习，实现了零-shot转移到广泛的语言，从而消除了对特定语言标记数据的需求。"
}