{
    "title": "The Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative",
    "abstract": "arXiv:2402.14859v1 Announce Type: cross  Abstract: Due to their unprecedented ability to process and respond to various types of data, Multimodal Large Language Models (MLLMs) are constantly defining the new boundary of Artificial General Intelligence (AGI). As these advanced generative models increasingly form collaborative networks for complex tasks, the integrity and security of these systems are crucial. Our paper, ``The Wolf Within'', explores a novel vulnerability in MLLM societies - the indirect propagation of malicious content. Unlike direct harmful output generation for MLLMs, our research demonstrates how a single MLLM agent can be subtly influenced to generate prompts that, in turn, induce other MLLM agents in the society to output malicious content. This subtle, yet potent method of indirect influence marks a significant escalation in the security risks associated with MLLMs. Our findings reveal that, with minimal or even no access to MLLMs' parameters, an MLLM agent, when ",
    "link": "https://arxiv.org/abs/2402.14859",
    "context": "Title: The Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative\nAbstract: arXiv:2402.14859v1 Announce Type: cross  Abstract: Due to their unprecedented ability to process and respond to various types of data, Multimodal Large Language Models (MLLMs) are constantly defining the new boundary of Artificial General Intelligence (AGI). As these advanced generative models increasingly form collaborative networks for complex tasks, the integrity and security of these systems are crucial. Our paper, ``The Wolf Within'', explores a novel vulnerability in MLLM societies - the indirect propagation of malicious content. Unlike direct harmful output generation for MLLMs, our research demonstrates how a single MLLM agent can be subtly influenced to generate prompts that, in turn, induce other MLLM agents in the society to output malicious content. This subtle, yet potent method of indirect influence marks a significant escalation in the security risks associated with MLLMs. Our findings reveal that, with minimal or even no access to MLLMs' parameters, an MLLM agent, when ",
    "path": "papers/24/02/2402.14859.json",
    "total_tokens": 867,
    "translated_title": "内在的狼：通过MLLM操作员向MLLM社会中渗入恶意",
    "translated_abstract": "由于其前所未有的处理和响应各种数据类型的能力，多模大型语言模型（MLLMs）不断定义人工通用智能（AGI）的新边界。随着这些先进的生成模型越来越多地形成用于复杂任务的协作网络，这些系统的完整性和安全性至关重要。我们的论文《内在的狼》探讨了MLLM社会中的一种新型漏洞 - 恶意内容的间接传播。与直接为MLLM生成有害输出不同，我们的研究展示了一个单个MLLM代理如何被微妙地影响，以生成再次诱使社会中其他MLLM代理输出恶意内容的提示。这种微妙而强有力的间接影响方法标志着与MLLM相关的安全风险的显著升级。我们的发现表明，即使几乎没有或是根本没有访问MLLM参数，一个MLLM代理，当",
    "tldr": "这里是中文总结出的一句话要点: 论文探讨了在MLLM社会中通过单个操作员间接影响其他代理生成恶意内容的新型漏洞。",
    "en_tdlr": "Here is the English TLDR: The paper explores a novel vulnerability in MLLM societies, where a single operative can indirectly influence other agents to generate malicious content."
}