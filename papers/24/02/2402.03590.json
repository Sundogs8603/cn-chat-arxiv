{
    "title": "Assessing the Impact of Distribution Shift on Reinforcement Learning Performance",
    "abstract": "Research in machine learning is making progress in fixing its own reproducibility crisis. Reinforcement learning (RL), in particular, faces its own set of unique challenges. Comparison of point estimates, and plots that show successful convergence to the optimal policy during training, may obfuscate overfitting or dependence on the experimental setup. Although researchers in RL have proposed reliability metrics that account for uncertainty to better understand each algorithm's strengths and weaknesses, the recommendations of past work do not assume the presence of out-of-distribution observations. We propose a set of evaluation methods that measure the robustness of RL algorithms under distribution shifts. The tools presented here argue for the need to account for performance over time while the agent is acting in its environment. In particular, we recommend time series analysis as a method of observational RL evaluation. We also show that the unique properties of RL and simulated dyna",
    "link": "https://arxiv.org/abs/2402.03590",
    "context": "Title: Assessing the Impact of Distribution Shift on Reinforcement Learning Performance\nAbstract: Research in machine learning is making progress in fixing its own reproducibility crisis. Reinforcement learning (RL), in particular, faces its own set of unique challenges. Comparison of point estimates, and plots that show successful convergence to the optimal policy during training, may obfuscate overfitting or dependence on the experimental setup. Although researchers in RL have proposed reliability metrics that account for uncertainty to better understand each algorithm's strengths and weaknesses, the recommendations of past work do not assume the presence of out-of-distribution observations. We propose a set of evaluation methods that measure the robustness of RL algorithms under distribution shifts. The tools presented here argue for the need to account for performance over time while the agent is acting in its environment. In particular, we recommend time series analysis as a method of observational RL evaluation. We also show that the unique properties of RL and simulated dyna",
    "path": "papers/24/02/2402.03590.json",
    "total_tokens": 820,
    "translated_title": "评估分布转变对强化学习性能的影响",
    "translated_abstract": "机器学习的研究正在解决自身的可重复性危机。特别是，强化学习(RL)面临着一系列独特的挑战。比较点估计和在训练过程中显示成功收敛到最优策略的图表可能会掩盖过拟合或对实验设置的依赖性。虽然RL的研究人员提出了可靠性指标以考虑不确定性，以更好地理解每个算法的优势和劣势，但过去的工作建议并不假设存在超出分布的观测结果。我们提出了一套评估方法，衡量了在分布转变下RL算法的稳健性。本文介绍的工具支持在代理在其环境中行动时考虑性能的需求。我们尤其推荐使用时间序列分析作为观测RL评估的方法。我们还展示了RL和模拟动力学的独特属性。",
    "tldr": "评估强化学习性能时需要考虑分布转变，我们提出了一套评估方法，并推荐使用时间序列分析进行观测RL评估。",
    "en_tdlr": "It is important to consider distribution shift when assessing reinforcement learning performance. We propose evaluation methods that account for this and recommend using time series analysis for observational RL evaluation."
}