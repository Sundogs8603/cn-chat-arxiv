{
    "title": "Faster Inference of Integer SWIN Transformer by Removing the GELU Activation",
    "abstract": "SWIN transformer is a prominent vision transformer model that has state-of-the-art accuracy in image classification tasks. Despite this success, its unique architecture causes slower inference compared with similar deep neural networks. Integer quantization of the model is one of the methods used to improve its inference latency. However, state-of-the-art has not been able to fully quantize the model. In this work, we improve upon the inference latency of the state-of-the-art methods by removing the floating-point operations, which are associated with the GELU activation in Swin Transformer. While previous work proposed to replace the non-integer operations with linear approximation functions, we propose to replace GELU with ReLU activation. The advantage of ReLU over previous methods is its low memory and computation complexity. We use iterative knowledge distillation to compensate for the lost accuracy due to replacing GELU with ReLU. We quantize our GELU-less SWIN transformer and sh",
    "link": "https://rss.arxiv.org/abs/2402.01169",
    "context": "Title: Faster Inference of Integer SWIN Transformer by Removing the GELU Activation\nAbstract: SWIN transformer is a prominent vision transformer model that has state-of-the-art accuracy in image classification tasks. Despite this success, its unique architecture causes slower inference compared with similar deep neural networks. Integer quantization of the model is one of the methods used to improve its inference latency. However, state-of-the-art has not been able to fully quantize the model. In this work, we improve upon the inference latency of the state-of-the-art methods by removing the floating-point operations, which are associated with the GELU activation in Swin Transformer. While previous work proposed to replace the non-integer operations with linear approximation functions, we propose to replace GELU with ReLU activation. The advantage of ReLU over previous methods is its low memory and computation complexity. We use iterative knowledge distillation to compensate for the lost accuracy due to replacing GELU with ReLU. We quantize our GELU-less SWIN transformer and sh",
    "path": "papers/24/02/2402.01169.json",
    "total_tokens": 911,
    "translated_title": "通过去除GELU激活快速推断整数SWIN Transformer",
    "translated_abstract": "SWIN transformer是一种突出的视觉transformer模型，在图像分类任务中具有最先进的准确性。尽管取得了成功，但其独特的架构导致与类似深度神经网络相比推断速度较慢。模型的整数量化是用于改善推断延迟的方法之一。然而，迄今为止还没有能够完全量化模型的最新方法。在这项工作中，我们通过去除Swin Transformer中与GELU激活相关的浮点运算来改善最新方法的推断延迟。虽然之前的工作提出用线性逼近函数替换非整数运算，但我们提出用ReLU激活代替GELU。ReLU相对于以前的方法的优势在于其内存和计算复杂性较低。我们使用迭代知识蒸馏来弥补由于用ReLU替换GELU而导致的精度损失。我们对去除GELU的SWIN transformer进行量化，并逐渐逼近最佳性能，并验证了我们方法的有效性。",
    "tldr": "本研究通过去除SWIN Transformer中的GELU激活，提升了整数SWIN Transformer的推断速度。我们使用ReLU激活替代GELU，并使用迭代知识蒸馏来调整精度损失，最终实现了模型的量化和优化。"
}