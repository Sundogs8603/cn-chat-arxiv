{
    "title": "The Reasoning Under Uncertainty Trap: A Structural AI Risk",
    "abstract": "This report examines a novel risk associated with current (and projected) AI tools. Making effective decisions about future actions requires us to reason under uncertainty (RUU), and doing so is essential to many critical real world problems. Overfaced by this challenge, there is growing demand for AI tools like LLMs to assist decision-makers. Having evidenced this demand and the incentives behind it, we expose a growing risk: we 1) do not currently sufficiently understand LLM capabilities in this regard, and 2) have no guarantees of performance given fundamental computational explosiveness and deep uncertainty constraints on accuracy. This report provides an exposition of what makes RUU so challenging for both humans and machines, and relates these difficulties to prospective AI timelines and capabilities. Having established this current potential misuse risk, we go on to expose how this seemingly additive risk (more misuse additively contributed to potential harm) in fact has multipl",
    "link": "https://arxiv.org/abs/2402.01743",
    "context": "Title: The Reasoning Under Uncertainty Trap: A Structural AI Risk\nAbstract: This report examines a novel risk associated with current (and projected) AI tools. Making effective decisions about future actions requires us to reason under uncertainty (RUU), and doing so is essential to many critical real world problems. Overfaced by this challenge, there is growing demand for AI tools like LLMs to assist decision-makers. Having evidenced this demand and the incentives behind it, we expose a growing risk: we 1) do not currently sufficiently understand LLM capabilities in this regard, and 2) have no guarantees of performance given fundamental computational explosiveness and deep uncertainty constraints on accuracy. This report provides an exposition of what makes RUU so challenging for both humans and machines, and relates these difficulties to prospective AI timelines and capabilities. Having established this current potential misuse risk, we go on to expose how this seemingly additive risk (more misuse additively contributed to potential harm) in fact has multipl",
    "path": "papers/24/02/2402.01743.json",
    "total_tokens": 972,
    "translated_title": "理性陷阱下的不确定性：一种结构化的人工智能风险",
    "translated_abstract": "本报告研究了当前（以及预测）人工智能工具所带来的一种新型风险。在对未来行动做出有效决策的过程中，我们需要在不确定性下进行推理，这对于许多关键的现实世界问题至关重要。面对这一挑战，对于类似LLMs的人工智能工具来辅助决策者的需求正在增长。在证明了这种需求及其背后的动机后，我们揭示了一个不断增加的风险：1）我们目前对LLMs在这方面的能力了解不够，2）在基本的计算爆炸性和深度不确定性约束准确性的情况下，我们无法保证其性能。本报告阐述了人类和机器在理性陷阱下面临的挑战，并把这些困难与潜在的人工智能时间表和能力联系起来。在确立了当前潜在的误用风险后，我们进一步揭示了这种看似累加的风险（越多的误用加剧了潜在的危害）实际上有多重的",
    "tldr": "当前使用的人工智能工具在面对不确定性推理时存在风险，尤其是对于LLMs的能力尚不清楚，并且无法保证性能。该报告强调了理性陷阱对人类和机器的挑战，并对潜在的人工智能风险进行了分析。",
    "en_tdlr": "There is a novel risk associated with current AI tools, especially LLMs, in reasoning under uncertainty. The capabilities of LLMs in this regard are not well understood and their performance cannot be guaranteed. This report highlights the challenges of reasoning under uncertainty for both humans and machines, and analyzes the potential risks of AI."
}