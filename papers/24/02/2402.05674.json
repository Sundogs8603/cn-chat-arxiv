{
    "title": "A High Dimensional Model for Adversarial Training: Geometry and Trade-Offs",
    "abstract": "This work investigates adversarial training in the context of margin-based linear classifiers in the high-dimensional regime where the dimension $d$ and the number of data points $n$ diverge with a fixed ratio $\\alpha = n / d$. We introduce a tractable mathematical model where the interplay between the data and adversarial attacker geometries can be studied, while capturing the core phenomenology observed in the adversarial robustness literature. Our main theoretical contribution is an exact asymptotic description of the sufficient statistics for the adversarial empirical risk minimiser, under generic convex and non-increasing losses. Our result allow us to precisely characterise which directions in the data are associated with a higher generalisation/robustness trade-off, as defined by a robustness and a usefulness metric. In particular, we unveil the existence of directions which can be defended without penalising accuracy. Finally, we show the advantage of defending non-robust featu",
    "link": "https://arxiv.org/abs/2402.05674",
    "context": "Title: A High Dimensional Model for Adversarial Training: Geometry and Trade-Offs\nAbstract: This work investigates adversarial training in the context of margin-based linear classifiers in the high-dimensional regime where the dimension $d$ and the number of data points $n$ diverge with a fixed ratio $\\alpha = n / d$. We introduce a tractable mathematical model where the interplay between the data and adversarial attacker geometries can be studied, while capturing the core phenomenology observed in the adversarial robustness literature. Our main theoretical contribution is an exact asymptotic description of the sufficient statistics for the adversarial empirical risk minimiser, under generic convex and non-increasing losses. Our result allow us to precisely characterise which directions in the data are associated with a higher generalisation/robustness trade-off, as defined by a robustness and a usefulness metric. In particular, we unveil the existence of directions which can be defended without penalising accuracy. Finally, we show the advantage of defending non-robust featu",
    "path": "papers/24/02/2402.05674.json",
    "total_tokens": 964,
    "translated_title": "高维模型的对抗训练：几何和权衡",
    "translated_abstract": "本研究在高维情况下，即维度$d$和数据点数$n$与固定比例$\\alpha = n / d$发散的上下文中，研究了基于边际的线性分类器中的对抗训练。我们引入了一个可处理的数学模型，可以研究数据和对抗攻击者几何之间的相互作用，同时捕捉到对抗鲁棒性文献中观察到的核心现象。我们的主要理论贡献是在通用的凸且非递增损失函数下，对于对抗性经验风险最小化器的充分统计的精确渐近描述。我们的结果使我们能够精确地刻画数据中与更高的泛化/鲁棒性权衡相关的方向，由一个鲁棒性度量和一个有用性度量定义。特别地，我们揭示了存在一些方向，可以进行防御而不惩罚准确性。最后，我们展示了防御非鲁棒特征的优势。",
    "tldr": "本论文研究了高维模型中的对抗训练，引入了一个可处理的数学模型，并给出了对抗性经验风险最小化器的充分统计的精确渐近描述。研究结果表明存在可以防御而不惩罚准确性的方向，揭示了防御非鲁棒特征的优势。",
    "en_tdlr": "This paper investigates adversarial training in high-dimensional models, introduces a tractable mathematical model, and provides an exact asymptotic description of the adversarial empirical risk minimiser. The results reveal directions in the data that can be defended without penalizing accuracy, highlighting the advantage of defending non-robust features."
}