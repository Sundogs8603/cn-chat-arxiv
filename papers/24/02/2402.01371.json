{
    "title": "Critic-Actor for Average Reward MDPs with Function Approximation: A Finite-Time Analysis",
    "abstract": "In recent years, there has been a lot of research work activity focused on carrying out asymptotic and non-asymptotic convergence analyses for two-timescale actor critic algorithms where the actor updates are performed on a timescale that is slower than that of the critic. In a recent work, the critic-actor algorithm has been presented for the infinite horizon discounted cost setting in the look-up table case where the timescales of the actor and the critic are reversed and asymptotic convergence analysis has been presented. In our work, we present the first critic-actor algorithm with function approximation and in the long-run average reward setting and present the first finite-time (non-asymptotic) analysis of such a scheme. We obtain optimal learning rates and prove that our algorithm achieves a sample complexity of $\\mathcal{\\tilde{O}}(\\epsilon^{-2.08})$ for the mean squared error of the critic to be upper bounded by $\\epsilon$ which is better than the one obtained for actor-critic",
    "link": "https://rss.arxiv.org/abs/2402.01371",
    "context": "Title: Critic-Actor for Average Reward MDPs with Function Approximation: A Finite-Time Analysis\nAbstract: In recent years, there has been a lot of research work activity focused on carrying out asymptotic and non-asymptotic convergence analyses for two-timescale actor critic algorithms where the actor updates are performed on a timescale that is slower than that of the critic. In a recent work, the critic-actor algorithm has been presented for the infinite horizon discounted cost setting in the look-up table case where the timescales of the actor and the critic are reversed and asymptotic convergence analysis has been presented. In our work, we present the first critic-actor algorithm with function approximation and in the long-run average reward setting and present the first finite-time (non-asymptotic) analysis of such a scheme. We obtain optimal learning rates and prove that our algorithm achieves a sample complexity of $\\mathcal{\\tilde{O}}(\\epsilon^{-2.08})$ for the mean squared error of the critic to be upper bounded by $\\epsilon$ which is better than the one obtained for actor-critic",
    "path": "papers/24/02/2402.01371.json",
    "total_tokens": 966,
    "translated_title": "Critic-Actor算法在平均奖励MDPs中的函数逼近问题：有限时间分析",
    "translated_abstract": "最近，关于两个时间尺度演员-评论家算法的渐近和非渐近收敛分析的研究工作非常活跃，其中演员的更新速度比评论家慢。在最近的一项工作中，提出了一个评论家-演员算法，用于无限时域折扣成本设置中的查找表情况，其中演员和评论家的时间尺度相反，并给出了渐近收敛分析。在我们的工作中，我们首次提出了一个具有函数逼近的评论家-演员算法，并在长期平均奖励设置中进行了首次有限时间（非渐近）分析。我们得到了最优的学习速率，并证明了我们的算法从评论家的均方误差上界为$\\epsilon$，其样本复杂度为$\\mathcal{\\tilde{O}}(\\epsilon^{-2.08})$，此结果比演员-评论家算法获得的结果要好。",
    "tldr": "本论文提出了一个评论家-演员算法，解决了长期平均奖励设置中的函数逼近问题，并进行了有限时间分析。实验结果表明，我们的算法能够在评论家的均方误差上界为$\\epsilon$的情况下，获得样本复杂度为$\\mathcal{\\tilde{O}}(\\epsilon^{-2.08})$，优于演员-评论家算法的结果。",
    "en_tdlr": "This paper presents a critic-actor algorithm that addresses the function approximation problem in the long-run average reward setting, and provides a finite-time analysis. Experimental results demonstrate that our algorithm achieves a sample complexity of $\\mathcal{\\tilde{O}}(\\epsilon^{-2.08})$ for the mean squared error of the critic to be upper bounded by $\\epsilon$, which outperforms the results obtained by actor-critic algorithms."
}