{
    "title": "MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning",
    "abstract": "arXiv:2402.11260v1 Announce Type: cross  Abstract: Adapting large language models (LLMs) to new domains/tasks and enabling them to be efficient lifelong learners is a pivotal challenge. In this paper, we propose MoRAL, i.e., Mixture-of-Experts augmented Low-Rank Adaptation for Lifelong Learning. MoRAL combines the multi-tasking abilities of MoE with the fine-tuning abilities of LoRA for effective life-long learning of LLMs. In contrast to the conventional approaches that use factual triplets as inputs MoRAL relies on simple question-answer pairs, which is a more practical and effective strategy for robust and efficient learning. Owing to new data settings, we introduce a new evaluation benchmark namely: Life Long Learning of LLM (5L-bench) encompassing a newly curated dataset of question-answer pairs, and a set of evaluation metrics for rigorous evaluation of MoRAL in open-book and closed-book settings. Experimental evaluation shows (i) LLMs learn fast in open-book settings with up to ",
    "link": "https://arxiv.org/abs/2402.11260",
    "context": "Title: MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning\nAbstract: arXiv:2402.11260v1 Announce Type: cross  Abstract: Adapting large language models (LLMs) to new domains/tasks and enabling them to be efficient lifelong learners is a pivotal challenge. In this paper, we propose MoRAL, i.e., Mixture-of-Experts augmented Low-Rank Adaptation for Lifelong Learning. MoRAL combines the multi-tasking abilities of MoE with the fine-tuning abilities of LoRA for effective life-long learning of LLMs. In contrast to the conventional approaches that use factual triplets as inputs MoRAL relies on simple question-answer pairs, which is a more practical and effective strategy for robust and efficient learning. Owing to new data settings, we introduce a new evaluation benchmark namely: Life Long Learning of LLM (5L-bench) encompassing a newly curated dataset of question-answer pairs, and a set of evaluation metrics for rigorous evaluation of MoRAL in open-book and closed-book settings. Experimental evaluation shows (i) LLMs learn fast in open-book settings with up to ",
    "path": "papers/24/02/2402.11260.json",
    "total_tokens": 824,
    "translated_title": "MoRAL: MoE增强LoRA用于LLM的终身学习",
    "translated_abstract": "本文提出了一种名为MoRAL的方法，即Mixture-of-Experts增强低秩适应性用于LLM的终身学习。 MoRAL将MoE的多任务能力与LoRA的微调能力相结合，实现了LLM的有效终身学习。与传统方法不同，MoRAL依赖于简单的问答对作为输入，这是一种更实用和有效的鲁棒学习策略。鉴于新数据设置，我们引入了一个新的评估基准，即LLM的终身学习（5L-bench），包括一个新的精心策划的问答对数据集，以及一组用于在开放式和封闭式环境下严格评估MoRAL的评估指标。实验评估表明，在开放式环境下，LLM在短时间内迅速学习",
    "tldr": "MoRAL结合了MoE的多任务能力和LoRA的微调能力，采用问答对作为输入，实现了LLM的有效终身学习。",
    "en_tdlr": "MoRAL combines the multi-tasking abilities of MoE with the fine-tuning abilities of LoRA, using question-answer pairs as inputs, to achieve effective lifelong learning of LLMs."
}