{
    "title": "Is Crowdsourcing Breaking Your Bank? Cost-Effective Fine-Tuning of Pre-trained Language Models with Proximal Policy Optimization",
    "abstract": "arXiv:2402.18284v1 Announce Type: cross  Abstract: Wide usage of ChatGPT has highlighted the potential of reinforcement learning from human feedback. However, its training pipeline relies on manual ranking, a resource-intensive process. To reduce labor costs, we propose a self-supervised text ranking approach for applying Proximal-Policy-Optimization to fine-tune language models while eliminating the need for human annotators. Our method begins with probabilistic sampling to encourage a language model to generate diverse responses for each input. We then employ TextRank and ISODATA algorithms to rank and cluster these responses based on their semantics. Subsequently, we construct a reward model to learn the rank and optimize our generative policy. Our experimental results, conducted using two language models on three tasks, demonstrate that the models trained by our method considerably outperform baselines regarding BLEU, GLEU, and METEOR scores. Furthermore, our manual evaluation show",
    "link": "https://arxiv.org/abs/2402.18284",
    "context": "Title: Is Crowdsourcing Breaking Your Bank? Cost-Effective Fine-Tuning of Pre-trained Language Models with Proximal Policy Optimization\nAbstract: arXiv:2402.18284v1 Announce Type: cross  Abstract: Wide usage of ChatGPT has highlighted the potential of reinforcement learning from human feedback. However, its training pipeline relies on manual ranking, a resource-intensive process. To reduce labor costs, we propose a self-supervised text ranking approach for applying Proximal-Policy-Optimization to fine-tune language models while eliminating the need for human annotators. Our method begins with probabilistic sampling to encourage a language model to generate diverse responses for each input. We then employ TextRank and ISODATA algorithms to rank and cluster these responses based on their semantics. Subsequently, we construct a reward model to learn the rank and optimize our generative policy. Our experimental results, conducted using two language models on three tasks, demonstrate that the models trained by our method considerably outperform baselines regarding BLEU, GLEU, and METEOR scores. Furthermore, our manual evaluation show",
    "path": "papers/24/02/2402.18284.json",
    "total_tokens": 906,
    "translated_title": "众包是否让您破产了？使用近端策略优化对预训练语言模型进行成本效益微调",
    "translated_abstract": "ChatGPT的广泛使用凸显了从人类反馈中进行强化学习的潜力。然而，其训练流程依赖于人工排序，这是一个资源密集型的过程。为了降低劳动成本，我们提出了一种自监督文本排序方法，用于应用近端策略优化来对语言模型进行微调，同时消除了对人工注释员的需求。我们的方法从概率抽样开始，鼓励语言模型为每个输入生成多样化的响应。然后，我们使用TextRank和ISODATA算法，基于语义对这些响应进行排序和聚类。随后，我们构建了一个奖励模型来学习排名并优化我们的生成策略。我们在三个任务上使用两个语言模型进行的实验结果表明，我们的方法训练的模型在BLEU、GLEU和METEOR得分方面明显优于基线。此外，我们的手动评估显示",
    "tldr": "提出了一种自监督文本排序方法，利用近端策略优化对语言模型进行微调，消除了对人工注释员的需求，实验结果表明该方法训练的模型在各项得分方面明显优于基线"
}