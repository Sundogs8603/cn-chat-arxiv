{
    "title": "RoboCodeX: Multimodal Code Generation for Robotic Behavior Synthesis",
    "abstract": "arXiv:2402.16117v1 Announce Type: cross  Abstract: Robotic behavior synthesis, the problem of understanding multimodal inputs and generating precise physical control for robots, is an important part of Embodied AI. Despite successes in applying multimodal large language models for high-level understanding, it remains challenging to translate these conceptual understandings into detailed robotic actions while achieving generalization across various scenarios. In this paper, we propose a tree-structured multimodal code generation framework for generalized robotic behavior synthesis, termed RoboCodeX. RoboCodeX decomposes high-level human instructions into multiple object-centric manipulation units consisting of physical preferences such as affordance and safety constraints, and applies code generation to introduce generalization ability across various robotics platforms. To further enhance the capability to map conceptual and perceptual understanding into control commands, a specialized ",
    "link": "https://arxiv.org/abs/2402.16117",
    "context": "Title: RoboCodeX: Multimodal Code Generation for Robotic Behavior Synthesis\nAbstract: arXiv:2402.16117v1 Announce Type: cross  Abstract: Robotic behavior synthesis, the problem of understanding multimodal inputs and generating precise physical control for robots, is an important part of Embodied AI. Despite successes in applying multimodal large language models for high-level understanding, it remains challenging to translate these conceptual understandings into detailed robotic actions while achieving generalization across various scenarios. In this paper, we propose a tree-structured multimodal code generation framework for generalized robotic behavior synthesis, termed RoboCodeX. RoboCodeX decomposes high-level human instructions into multiple object-centric manipulation units consisting of physical preferences such as affordance and safety constraints, and applies code generation to introduce generalization ability across various robotics platforms. To further enhance the capability to map conceptual and perceptual understanding into control commands, a specialized ",
    "path": "papers/24/02/2402.16117.json",
    "total_tokens": 808,
    "translated_title": "RoboCodeX: 用于机器人行为合成的多模态代码生成",
    "translated_abstract": "机器人行为合成是理解多模态输入并为机器人生成精确物理控制的问题，是具有体现特征的人工智能的重要部分。尽管在应用多模态大型语言模型进行高级理解方面取得了成功，但将这些概念理解转化为详细的机器人动作，在实现跨不同场景的泛化时仍然具有挑战性。本文提出了一种用于泛化机器人行为合成的树状多模态代码生成框架，名为RoboCodeX。RoboCodeX将高级人类指令分解为由物理偏好（如可用性和安全约束）组成的多个物体中心操纵单元，并应用代码生成来实现在不同机器人平台上的泛化能力。为了进一步增强将概念和感知理解映射到控制指令的能力，一种专门的",
    "tldr": "提出了一种用于泛化机器人行为合成的树状多模态代码生成框架，具有将高级人类指令转化为机器人动作的泛化能力。",
    "en_tdlr": "Proposed a tree-structured multimodal code generation framework for generalized robotic behavior synthesis, with the ability to translate high-level human instructions into robotic actions with generalization capabilities."
}