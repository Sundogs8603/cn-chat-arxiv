{
    "title": "Improving Dialog Safety using Socially Aware Contrastive Learning",
    "abstract": "State-of-the-art conversational AI systems raise concerns due to their potential risks of generating unsafe, toxic, unethical, or dangerous content. Previous works have developed datasets to teach conversational agents the appropriate social paradigms to respond effectively to specifically designed hazardous content. However, models trained on these adversarial datasets still struggle to recognize subtle unsafe situations that appear naturally in conversations or introduce an inappropriate response in a casual context. To understand the extent of this problem, we study prosociality in both adversarial and casual dialog contexts and audit the response quality of general-purpose language models in terms of propensity to produce unsafe content. We propose a dual-step fine-tuning process to address these issues using a socially aware n-pair contrastive loss. Subsequently, we train a base model that integrates prosocial behavior by leveraging datasets like Moral Integrity Corpus (MIC) and P",
    "link": "https://arxiv.org/abs/2402.00446",
    "context": "Title: Improving Dialog Safety using Socially Aware Contrastive Learning\nAbstract: State-of-the-art conversational AI systems raise concerns due to their potential risks of generating unsafe, toxic, unethical, or dangerous content. Previous works have developed datasets to teach conversational agents the appropriate social paradigms to respond effectively to specifically designed hazardous content. However, models trained on these adversarial datasets still struggle to recognize subtle unsafe situations that appear naturally in conversations or introduce an inappropriate response in a casual context. To understand the extent of this problem, we study prosociality in both adversarial and casual dialog contexts and audit the response quality of general-purpose language models in terms of propensity to produce unsafe content. We propose a dual-step fine-tuning process to address these issues using a socially aware n-pair contrastive loss. Subsequently, we train a base model that integrates prosocial behavior by leveraging datasets like Moral Integrity Corpus (MIC) and P",
    "path": "papers/24/02/2402.00446.json",
    "total_tokens": 973,
    "translated_title": "使用具有社会意识的对比学习来提高对话安全性",
    "translated_abstract": "最先进的对话型人工智能系统由于可能产生不安全、有毒、不道德或危险内容的潜在风险而引起关注。之前的研究已经开发了数据集，教会对话代理人适当地回应特定设计的危险内容的社交范式。然而，这些对抗性数据集上训练的模型仍然很难识别自然出现在对话中的微妙的不安全情况，或在随意环境中引入不恰当的回应。为了了解这个问题的程度，我们研究了对抗和随意对话背景下的亲社会性，并在是否产生不安全内容的倾向方面审查了通用语言模型的响应质量。我们提出了一个双步骤的微调过程，使用具有社会意识的n对对比损失来解决这些问题。随后，我们训练了一个基础模型，通过利用道德完整语料库（MIC）等数据集来整合亲社会行为。",
    "tldr": "本研究提出了使用具有社会意识的对比学习来提高对话安全性的方法。通过研究对抗和随意对话背景下的亲社会性，我们发现现有的模型很难识别自然对话中微妙的不安全情况。为了解决这个问题，我们采用了一个双步骤的微调过程，并通过整合亲社会行为的数据集来训练基础模型。"
}