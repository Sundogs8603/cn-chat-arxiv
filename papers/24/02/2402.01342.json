{
    "title": "Training-time Neuron Alignment through Permutation Subspace for Improving Linear Mode Connectivity and Model Fusion",
    "abstract": "In deep learning, stochastic gradient descent often yields functionally similar yet widely scattered solutions in the weight space even under the same initialization, causing barriers in the Linear Mode Connectivity (LMC) landscape. Overcoming these barriers is crucial for understanding deep learning dynamics and enhancing model-fusion algorithms. Previous studies highlight the role of permutation symmetry in reducing post-training barriers through network permutation. However, these post-hoc methods, demanding extra computations, are less effective for larger, complex models (e.g., ViT, LLM) due to numerous permutation matrices. Thus, in this paper, we study training-time neuron alignment. Our hypothesis suggests that training-time permutation subspace can reduce LMC barriers for free. We find that pruning at initialization supports this. Beyond pruning, we introduce TNA-PFN, a simple yet lossless algorithm using a partial gradient mask during training. TNA-PFN is theoretically and em",
    "link": "https://rss.arxiv.org/abs/2402.01342",
    "context": "Title: Training-time Neuron Alignment through Permutation Subspace for Improving Linear Mode Connectivity and Model Fusion\nAbstract: In deep learning, stochastic gradient descent often yields functionally similar yet widely scattered solutions in the weight space even under the same initialization, causing barriers in the Linear Mode Connectivity (LMC) landscape. Overcoming these barriers is crucial for understanding deep learning dynamics and enhancing model-fusion algorithms. Previous studies highlight the role of permutation symmetry in reducing post-training barriers through network permutation. However, these post-hoc methods, demanding extra computations, are less effective for larger, complex models (e.g., ViT, LLM) due to numerous permutation matrices. Thus, in this paper, we study training-time neuron alignment. Our hypothesis suggests that training-time permutation subspace can reduce LMC barriers for free. We find that pruning at initialization supports this. Beyond pruning, we introduce TNA-PFN, a simple yet lossless algorithm using a partial gradient mask during training. TNA-PFN is theoretically and em",
    "path": "papers/24/02/2402.01342.json",
    "total_tokens": 937,
    "translated_title": "通过置换子空间在训练过程中对神经元进行对齐，以改进线性模块连通性和模型融合",
    "translated_abstract": "在深度学习中，即使在相同初始化条件下，随机梯度下降算法经常产生具有功能相似但在权重空间中分散的解，这导致了线性模块连通性（LMC）的局限性。克服这些局限性对于理解深度学习动态和提高模型融合算法至关重要。以前的研究强调置换对称性在通过网络置换减少训练后的局限性方面的作用。然而，这些事后的方法需要额外的计算，在更大、更复杂的模型（如ViT，LLM）上效果较差，因为存在大量的置换矩阵。因此，在本文中，我们研究了训练过程中神经元的对齐。我们的假设是，在训练过程中的置换子空间可以免费减少LMC的局限性。我们发现，初始化时进行修剪可以支持这一假设。除了修剪之外，我们引入了TNA-PFN，一种简单而无损的算法，在训练过程中使用部分梯度掩码。TNA-PFN在理论上和实验上都得到了支持。",
    "tldr": "本文提出了一个在训练过程中进行神经元对齐的方法，通过置换子空间减少了线性模块连通性的局限性，为模型融合算法的改进提供了可能性。"
}