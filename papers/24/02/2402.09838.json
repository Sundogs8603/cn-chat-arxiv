{
    "title": "Performative Reinforcement Learning in Gradually Shifting Environments",
    "abstract": "arXiv:2402.09838v1 Announce Type: new  Abstract: When Reinforcement Learning (RL) agents are deployed in practice, they might impact their environment and change its dynamics. Ongoing research attempts to formally model this phenomenon and to analyze learning algorithms in these models. To this end, we propose a framework where the current environment depends on the deployed policy as well as its previous dynamics. This is a generalization of Performative RL (PRL) [Mandal et al., 2023]. Unlike PRL, our framework allows to model scenarios where the environment gradually adjusts to a deployed policy. We adapt two algorithms from the performative prediction literature to our setting and propose a novel algorithm called Mixed Delayed Repeated Retraining (MDRR). We provide conditions under which these algorithms converge and compare them using three metrics: number of retrainings, approximation guarantee, and number of samples per deployment. Unlike previous approaches, MDRR combines sample",
    "link": "https://arxiv.org/abs/2402.09838",
    "context": "Title: Performative Reinforcement Learning in Gradually Shifting Environments\nAbstract: arXiv:2402.09838v1 Announce Type: new  Abstract: When Reinforcement Learning (RL) agents are deployed in practice, they might impact their environment and change its dynamics. Ongoing research attempts to formally model this phenomenon and to analyze learning algorithms in these models. To this end, we propose a framework where the current environment depends on the deployed policy as well as its previous dynamics. This is a generalization of Performative RL (PRL) [Mandal et al., 2023]. Unlike PRL, our framework allows to model scenarios where the environment gradually adjusts to a deployed policy. We adapt two algorithms from the performative prediction literature to our setting and propose a novel algorithm called Mixed Delayed Repeated Retraining (MDRR). We provide conditions under which these algorithms converge and compare them using three metrics: number of retrainings, approximation guarantee, and number of samples per deployment. Unlike previous approaches, MDRR combines sample",
    "path": "papers/24/02/2402.09838.json",
    "total_tokens": 865,
    "translated_title": "渐变环境中的表演性强化学习",
    "translated_abstract": "当强化学习（RL）代理在实践中部署时，它们可能会影响环境并改变其动态。当前的研究试图形式化建模这种现象，并在这些模型中分析学习算法。为此，我们提出了一个框架，其中当前的环境取决于部署策略及其先前的动态。这是Performative RL（PRL）[Mandal et al., 2023]的一种泛化。与PRL不同，我们的框架允许对环境逐渐调整到部署策略的情景进行建模。我们将表演性预测文献中的两种算法适应到我们的设置，并提出了一种新的算法称为混合延迟重复训练（MDRR）。我们给出了这些算法收敛的条件，并使用三个指标进行比较：重训练次数，逼近保证和每次部署的样本数。与之前的方法不同，MDRR结合了样本",
    "tldr": "这项研究提出了一种在渐变环境中进行强化学习的框架，可以模拟部署策略对环境的影响，并提出了一种新的算法MDRR来应对这种情况。"
}