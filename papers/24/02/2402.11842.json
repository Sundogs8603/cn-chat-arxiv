{
    "title": "CodeArt: Better Code Models by Attention Regularization When Symbols Are Lacking",
    "abstract": "arXiv:2402.11842v1 Announce Type: cross  Abstract: Transformer based code models have impressive performance in many software engineering tasks. However, their effectiveness degrades when symbols are missing or not informative. The reason is that the model may not learn to pay attention to the right correlations/contexts without the help of symbols. We propose a new method to pre-train general code models when symbols are lacking. We observe that in such cases, programs degenerate to something written in a very primitive language. We hence propose to use program analysis to extract contexts a priori (instead of relying on symbols and masked language modeling as in vanilla models). We then leverage a novel attention masking method to only allow the model attending to these contexts, e.g., bi-directional program dependence transitive closures and token co-occurrences. In the meantime, the inherent self-attention mechanism is utilized to learn which of the allowed attentions are more impo",
    "link": "https://arxiv.org/abs/2402.11842",
    "context": "Title: CodeArt: Better Code Models by Attention Regularization When Symbols Are Lacking\nAbstract: arXiv:2402.11842v1 Announce Type: cross  Abstract: Transformer based code models have impressive performance in many software engineering tasks. However, their effectiveness degrades when symbols are missing or not informative. The reason is that the model may not learn to pay attention to the right correlations/contexts without the help of symbols. We propose a new method to pre-train general code models when symbols are lacking. We observe that in such cases, programs degenerate to something written in a very primitive language. We hence propose to use program analysis to extract contexts a priori (instead of relying on symbols and masked language modeling as in vanilla models). We then leverage a novel attention masking method to only allow the model attending to these contexts, e.g., bi-directional program dependence transitive closures and token co-occurrences. In the meantime, the inherent self-attention mechanism is utilized to learn which of the allowed attentions are more impo",
    "path": "papers/24/02/2402.11842.json",
    "total_tokens": 844,
    "translated_title": "CodeArt：当符号缺失时通过注意力规范化改进代码模型",
    "translated_abstract": "基于Transformer的代码模型在许多软件工程任务中表现出色。然而，当符号缺失或者不具信息量时，它们的有效性会下降。这是因为模型可能没有学会在没有符号的情况下正确地关注相关性/上下文。我们提出了一种新的方法，在符号缺失时预训练通用代码模型。我们观察到，在这种情况下，程序会退化为用非常原始的语言编写的内容。因此，我们建议使用程序分析来事先提取上下文（而不是像传统模型中依赖符号和掩码语言建模）。然后，我们利用一种新颖的注意力掩码方法，只允许模型关注这些上下文，例如双向程序依赖传递闭包和令牌共现。与此同时，内在的自注意力机制被用于学习允许的关注度哪些更重要。",
    "tldr": "提出一种在符号缺失时通过注意力规范化改进代码模型的新方法，使用程序分析提取上下文并利用注意力掩码方法，同时利用自注意力机制学习关注度的重要性",
    "en_tdlr": "Propose a new method to improve code models by attention regularization when symbols are lacking, using program analysis to extract contexts and utilizing attention masking method, while learning the importance of attention via self-attention mechanism."
}