{
    "title": "InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs ready for the Indian Legal Domain?",
    "abstract": "arXiv:2402.10567v1 Announce Type: cross  Abstract: Recent advancements in language technology and Artificial Intelligence have resulted in numerous Language Models being proposed to perform various tasks in the legal domain ranging from predicting judgments to generating summaries. Despite their immense potential, these models have been proven to learn and exhibit societal biases and make unfair predictions. In this study, we explore the ability of Large Language Models (LLMs) to perform legal tasks in the Indian landscape when social factors are involved. We present a novel metric, $\\beta$-weighted $\\textit{Legal Safety Score ($LSS_{\\beta}$)}$, which encapsulates both the fairness and accuracy aspects of the LLM. We assess LLMs' safety by considering its performance in the $\\textit{Binary Statutory Reasoning}$ task and its fairness exhibition with respect to various axes of disparities in the Indian society. Task performance and fairness scores of LLaMA and LLaMA--2 models indicate th",
    "link": "https://arxiv.org/abs/2402.10567",
    "context": "Title: InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs ready for the Indian Legal Domain?\nAbstract: arXiv:2402.10567v1 Announce Type: cross  Abstract: Recent advancements in language technology and Artificial Intelligence have resulted in numerous Language Models being proposed to perform various tasks in the legal domain ranging from predicting judgments to generating summaries. Despite their immense potential, these models have been proven to learn and exhibit societal biases and make unfair predictions. In this study, we explore the ability of Large Language Models (LLMs) to perform legal tasks in the Indian landscape when social factors are involved. We present a novel metric, $\\beta$-weighted $\\textit{Legal Safety Score ($LSS_{\\beta}$)}$, which encapsulates both the fairness and accuracy aspects of the LLM. We assess LLMs' safety by considering its performance in the $\\textit{Binary Statutory Reasoning}$ task and its fairness exhibition with respect to various axes of disparities in the Indian society. Task performance and fairness scores of LLaMA and LLaMA--2 models indicate th",
    "path": "papers/24/02/2402.10567.json",
    "total_tokens": 985,
    "translated_title": "在InSaAF中融入安全性，通过准确性和公平性 | LLM是否已经准备好进入印度法律领域？",
    "translated_abstract": "语言技术和人工智能的最新进展已经导致提出了众多语言模型，用于执行法律领域的各种任务，从预测判决到生成摘要。尽管它们具有巨大潜力，但已经证明这些模型学习并展示社会偏见，并做出不公平的预测。在这项研究中，我们探讨了当涉及社会因素时大型语言模型（LLMs）在印度法律领域执行任务的能力。我们提出了一种新颖的度量标准，$\\beta$-加权的$\\textit{法律安全分数($LSS_{\\beta}$)}$，将LLM的公平性和准确性两个方面结合起来。我们通过考虑LLM在$\\textit{二元法律推理}$任务中的表现以及其在印度社会各种不平等方面的公平展示来评估LLMs的安全性。LLaMA和LLaMA--2模型的任务表现和公平得分表明...",
    "tldr": "本研究在印度法律领域探讨了大型语言模型（LLMs）在处理社会因素时的能力，提出了结合公平性和准确性的新指标$LSS_{\\beta}$，并评估了模型在二元法律推理任务中的表现以及在印度社会各种不平等方面的公平性展示。",
    "en_tdlr": "This study explores the capability of Large Language Models (LLMs) in handling social factors in the Indian legal domain, introduces a novel metric $LSS_{\\beta}$ that combines fairness and accuracy, and evaluates the model's performance in the Binary Statutory Reasoning task and fairness exhibition across various disparities in Indian society."
}