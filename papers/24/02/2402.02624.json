{
    "title": "A Safe Reinforcement Learning driven Weights-varying Model Predictive Control for Autonomous Vehicle Motion Control",
    "abstract": "Determining the optimal cost function parameters of Model Predictive Control (MPC) to optimize multiple control objectives is a challenging and time-consuming task. Multiobjective Bayesian Optimization (BO) techniques solve this problem by determining a Pareto optimal parameter set for an MPC with static weights. However, a single parameter set may not deliver the most optimal closed-loop control performance when the context of the MPC operating conditions changes during its operation, urging the need to adapt the cost function weights at runtime. Deep Reinforcement Learning (RL) algorithms can automatically learn context-dependent optimal parameter sets and dynamically adapt for a Weightsvarying MPC (WMPC). However, learning cost function weights from scratch in a continuous action space may lead to unsafe operating states. To solve this, we propose a novel approach limiting the RL actions within a safe learning space representing a catalog of pre-optimized BO Pareto-optimal weight se",
    "link": "https://arxiv.org/abs/2402.02624",
    "context": "Title: A Safe Reinforcement Learning driven Weights-varying Model Predictive Control for Autonomous Vehicle Motion Control\nAbstract: Determining the optimal cost function parameters of Model Predictive Control (MPC) to optimize multiple control objectives is a challenging and time-consuming task. Multiobjective Bayesian Optimization (BO) techniques solve this problem by determining a Pareto optimal parameter set for an MPC with static weights. However, a single parameter set may not deliver the most optimal closed-loop control performance when the context of the MPC operating conditions changes during its operation, urging the need to adapt the cost function weights at runtime. Deep Reinforcement Learning (RL) algorithms can automatically learn context-dependent optimal parameter sets and dynamically adapt for a Weightsvarying MPC (WMPC). However, learning cost function weights from scratch in a continuous action space may lead to unsafe operating states. To solve this, we propose a novel approach limiting the RL actions within a safe learning space representing a catalog of pre-optimized BO Pareto-optimal weight se",
    "path": "papers/24/02/2402.02624.json",
    "total_tokens": 836,
    "translated_title": "一种安全的强化学习驱动的权重变化模型预测控制用于自动驾驶车辆运动控制",
    "translated_abstract": "确定模型预测控制(MPC)的最佳代价函数参数以优化多个控制目标是一项具有挑战性且耗时的任务。多目标贝叶斯优化(BO)技术通过确定MPC的 Pareto 最优参数集解决了这个问题，然而，当MPC的操作条件上下文在其运行过程中发生变化时，单个参数集可能无法提供最优的闭环控制性能，因此需要在运行时调整代价函数权重。深度强化学习(RL)算法能够自动学习上下文相关的最优参数集，并在权重变化的MPC中进行动态调整。然而，在连续动作空间中从头学习代价函数权重可能导致不安全的操作状态。为了解决这个问题，我们提出了一种新方法，将RL的动作限制在安全学习空间内，该空间表示经过预优化的BO Pareto最优权重的目录。",
    "tldr": "提出了一种安全的强化学习驱动的权重变化模型预测控制方法，用于自动驾驶车辆运动控制。该方法通过在运行时动态调整代价函数权重，可以提供上下文相关的最优闭环控制性能。"
}