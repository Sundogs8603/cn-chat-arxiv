{
    "title": "When does word order matter and when doesn't it?",
    "abstract": "arXiv:2402.18838v1 Announce Type: new  Abstract: Language models (LMs) may appear insensitive to word order changes in natural language understanding (NLU) tasks. In this paper, we propose that linguistic redundancy can explain this phenomenon, whereby word order and other linguistic cues such as case markers provide overlapping and thus redundant information. Our hypothesis is that models exhibit insensitivity to word order when the order provides redundant information, and the degree of insensitivity varies across tasks. We quantify how informative word order is using mutual information (MI) between unscrambled and scrambled sentences. Our results show the effect that the less informative word order is, the more consistent the model's predictions are between unscrambled and scrambled sentences. We also find that the effect varies across tasks: for some tasks, like SST-2, LMs' prediction is almost always consistent with the original one even if the Pointwise-MI (PMI) changes, while fo",
    "link": "https://arxiv.org/abs/2402.18838",
    "context": "Title: When does word order matter and when doesn't it?\nAbstract: arXiv:2402.18838v1 Announce Type: new  Abstract: Language models (LMs) may appear insensitive to word order changes in natural language understanding (NLU) tasks. In this paper, we propose that linguistic redundancy can explain this phenomenon, whereby word order and other linguistic cues such as case markers provide overlapping and thus redundant information. Our hypothesis is that models exhibit insensitivity to word order when the order provides redundant information, and the degree of insensitivity varies across tasks. We quantify how informative word order is using mutual information (MI) between unscrambled and scrambled sentences. Our results show the effect that the less informative word order is, the more consistent the model's predictions are between unscrambled and scrambled sentences. We also find that the effect varies across tasks: for some tasks, like SST-2, LMs' prediction is almost always consistent with the original one even if the Pointwise-MI (PMI) changes, while fo",
    "path": "papers/24/02/2402.18838.json",
    "total_tokens": 871,
    "translated_title": "什么时候词序重要，什么时候不重要？",
    "translated_abstract": "在自然语言理解（NLU）任务中，语言模型（LMs）可能对词序变化不敏感。本文提出语言冗余性可以解释这一现象，即词序和其他语言提示（如格标）提供重叠且冗余信息。我们的假设是，当顺序提供冗余信息时，模型对词序的不敏感表现，而不同任务之间的不敏感程度各不相同。我们使用无序和打乱顺序的句子之间的互信息（MI）来量化词序的信息量。我们的结果显示，词序信息越不具信息量，模型在无序和打乱顺序的句子之间的预测越一致。我们还发现这种影响在不同任务之间存在差异：对于一些任务，如SST-2，LM的预测几乎总是与原始结果一致，即使点间互信息（PMI）发生变化，也是如此。",
    "tldr": "本文研究了语言模型对词序的敏感度问题，通过量化词序信息量，发现在语言提示提供冗余信息时，模型对词序变化不敏感，且不同任务间的不敏感程度有所差异。",
    "en_tdlr": "This paper investigates the sensitivity of language models to word order by quantifying the information content of word order, revealing that models are insensitive to word order changes when linguistic cues provide redundant information, with varying degrees of insensitivity across different tasks."
}