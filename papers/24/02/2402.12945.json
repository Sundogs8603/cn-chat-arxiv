{
    "title": "Stochastic Approximation Approach to Federated Machine Learning",
    "abstract": "arXiv:2402.12945v1 Announce Type: new  Abstract: This paper examines Federated learning (FL) in a Stochastic Approximation (SA) framework. FL is a collaborative way to train neural network models across various participants or clients without centralizing their data. Each client will train a model on their respective data and send the weights across to a the server periodically for aggregation. The server aggregates these weights which are then used by the clients to re-initialize their neural network and continue the training. SA is an iterative algorithm that uses approximate sample gradients and tapering step size to locate a minimizer of a cost function. In this paper the clients use a stochastic approximation iterate to update the weights of its neural network. It is shown that the aggregated weights track an autonomous ODE. Numerical simulations are performed and the results are compared with standard algorithms like FedAvg and FedProx. It is observed that the proposed algorithm ",
    "link": "https://arxiv.org/abs/2402.12945",
    "context": "Title: Stochastic Approximation Approach to Federated Machine Learning\nAbstract: arXiv:2402.12945v1 Announce Type: new  Abstract: This paper examines Federated learning (FL) in a Stochastic Approximation (SA) framework. FL is a collaborative way to train neural network models across various participants or clients without centralizing their data. Each client will train a model on their respective data and send the weights across to a the server periodically for aggregation. The server aggregates these weights which are then used by the clients to re-initialize their neural network and continue the training. SA is an iterative algorithm that uses approximate sample gradients and tapering step size to locate a minimizer of a cost function. In this paper the clients use a stochastic approximation iterate to update the weights of its neural network. It is shown that the aggregated weights track an autonomous ODE. Numerical simulations are performed and the results are compared with standard algorithms like FedAvg and FedProx. It is observed that the proposed algorithm ",
    "path": "papers/24/02/2402.12945.json",
    "total_tokens": 887,
    "translated_title": "基于随机逼近的联邦机器学习方法",
    "translated_abstract": "本文在随机逼近（SA）框架下研究了联邦学习（FL）。 FL是一种协作方式，用于跨不同参与方或客户端训练神经网络模型，而无需将它们的数据集中。 每个客户端将根据各自的数据训练一个模型，并定期将权重发送到服务器进行聚合。 服务器对这些权重进行聚合，然后客户端使用这些权重重新初始化其神经网络并继续训练。 SA是一种使用近似样本梯度和缩小步长来定位成本函数极小值的迭代算法。 本文中，客户端使用随机逼近迭代更新其神经网络的权重。 结果表明，聚合权重跟踪一个自治ODE。 进行了数值模拟，并将结果与FedAvg和FedProx等标准算法进行了比较。",
    "tldr": "本文提出了一种基于随机逼近的联邦机器学习方法，通过使用近似样本梯度和缩小步长来定位成本函数的极小值，实现了在联邦学习中对神经网络模型进行协作训练的效果，并在数值模拟中与标准算法进行了比较。",
    "en_tdlr": "This paper presents a Stochastic Approximation approach to federated machine learning, which uses approximate sample gradients and tapering step size to locate the minimizer of a cost function, achieving collaborative training of neural network models in federated learning, and comparing results with standard algorithms in numerical simulations."
}