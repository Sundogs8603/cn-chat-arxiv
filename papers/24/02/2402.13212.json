{
    "title": "Soft Self-Consistency Improves Language Model Agents",
    "abstract": "arXiv:2402.13212v1 Announce Type: cross  Abstract: Generations from large language models (LLMs) can be improved by sampling and scoring multiple solutions to select a final answer. Current \"sample and select\" methods such as self-consistency (SC) rely on majority voting to score answers. However, when tasks have many distinct and valid answers, selection by voting requires a large number of samples. This makes SC prohibitively expensive for interactive tasks that involve generating multiple actions (answers) sequentially. After establishing that majority voting fails to provide consistent gains on such tasks, we demonstrate how to increase success rates by softening the scoring criterion. We introduce Soft Self-Consistency (Soft-SC), which replaces SC's discontinuous scoring with a continuous score computed from model likelihoods, allowing for selection even when actions are sparsely distributed. Soft-SC improves both performance and efficiency on long-horizon interactive tasks, requi",
    "link": "https://arxiv.org/abs/2402.13212",
    "context": "Title: Soft Self-Consistency Improves Language Model Agents\nAbstract: arXiv:2402.13212v1 Announce Type: cross  Abstract: Generations from large language models (LLMs) can be improved by sampling and scoring multiple solutions to select a final answer. Current \"sample and select\" methods such as self-consistency (SC) rely on majority voting to score answers. However, when tasks have many distinct and valid answers, selection by voting requires a large number of samples. This makes SC prohibitively expensive for interactive tasks that involve generating multiple actions (answers) sequentially. After establishing that majority voting fails to provide consistent gains on such tasks, we demonstrate how to increase success rates by softening the scoring criterion. We introduce Soft Self-Consistency (Soft-SC), which replaces SC's discontinuous scoring with a continuous score computed from model likelihoods, allowing for selection even when actions are sparsely distributed. Soft-SC improves both performance and efficiency on long-horizon interactive tasks, requi",
    "path": "papers/24/02/2402.13212.json",
    "total_tokens": 830,
    "translated_title": "软自一致性改善语言模型代理",
    "translated_abstract": "大型语言模型（LLMs）生成可以通过对多个解决方案进行抽样和评分来改进，以选择最终答案。当前的“抽样和选择”方法如自一致性（SC）依赖于多数投票来评分答案。然而，当任务有许多不同且有效的答案时，通过投票进行选择需要大量样本。这使得SC在涉及顺序生成多个动作（答案）的互动任务时成本过高。在确定大多数投票未能为此类任务提供一致的收益之后，我们展示了如何通过软化评分标准来提高成功率。我们引入了软自一致性（Soft-SC），它用模型可能性计算连续分数来取代SC的不连续评分，即使动作分布稀疏，也允许选择。软自一致性在长期互动任务上提高了性能和效率，需要较少的样本和投票。",
    "tldr": "Soft Self-Consistency (Soft-SC)通过软化评分标准替代自一致性的多数投票方法，提高了在涉及生成多个动作的长期互动任务中的性能和效率",
    "en_tdlr": "Soft Self-Consistency (Soft-SC) improves performance and efficiency on long-horizon interactive tasks by softening the scoring criterion to replace majority voting approach of self-consistency."
}