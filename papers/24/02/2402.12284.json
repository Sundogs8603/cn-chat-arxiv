{
    "title": "Refining Minimax Regret for Unsupervised Environment Design",
    "abstract": "arXiv:2402.12284v1 Announce Type: cross  Abstract: In unsupervised environment design, reinforcement learning agents are trained on environment configurations (levels) generated by an adversary that maximises some objective. Regret is a commonly used objective that theoretically results in a minimax regret (MMR) policy with desirable robustness guarantees; in particular, the agent's maximum regret is bounded. However, once the agent reaches this regret bound on all levels, the adversary will only sample levels where regret cannot be further reduced. Although there are possible performance improvements to be made outside of these regret-maximising levels, learning stagnates. In this work, we introduce Bayesian level-perfect MMR (BLP), a refinement of the minimax regret objective that overcomes this limitation. We formally show that solving for this objective results in a subset of MMR policies, and that BLP policies act consistently with a Perfect Bayesian policy over all levels. We fur",
    "link": "https://arxiv.org/abs/2402.12284",
    "context": "Title: Refining Minimax Regret for Unsupervised Environment Design\nAbstract: arXiv:2402.12284v1 Announce Type: cross  Abstract: In unsupervised environment design, reinforcement learning agents are trained on environment configurations (levels) generated by an adversary that maximises some objective. Regret is a commonly used objective that theoretically results in a minimax regret (MMR) policy with desirable robustness guarantees; in particular, the agent's maximum regret is bounded. However, once the agent reaches this regret bound on all levels, the adversary will only sample levels where regret cannot be further reduced. Although there are possible performance improvements to be made outside of these regret-maximising levels, learning stagnates. In this work, we introduce Bayesian level-perfect MMR (BLP), a refinement of the minimax regret objective that overcomes this limitation. We formally show that solving for this objective results in a subset of MMR policies, and that BLP policies act consistently with a Perfect Bayesian policy over all levels. We fur",
    "path": "papers/24/02/2402.12284.json",
    "total_tokens": 899,
    "translated_title": "为无监督环境设计优化极小化遗憾",
    "translated_abstract": "在无监督环境设计中，强化学习代理通过对对手最大化某个目标生成的环境配置（关卡）进行训练。遗憾是一种常用的目标，理论上导致具有良好鲁棒性保证的极小化遗憾（MMR）策略；特别是，代理的最大遗憾是有界的。然而，一旦代理在所有关卡上达到了这个遗憾上界，对手将只会对无法进一步减少遗憾的关卡进行采样。尽管在这些最大化遗憾的关卡之外可能存在性能改进空间，但学习停滞。在这项工作中，我们介绍了贝叶斯级别完美的MMR（BLP），它是极小化遗憾目标的精确化。我们正式证明，解决这个目标将导致MMR策略的子集，并且BLP策略在所有关卡上都与完美贝叶斯策略一致行事。",
    "tldr": "介绍了贝叶斯级别完美的MMR（BLP），它是极小化遗憾目标的精确化，能够克服极小化遗憾策略在遗憾上界时学习停滞的限制。",
    "en_tdlr": "Introduced Bayesian Level-Perfect MMR (BLP) as a refinement of the minimax regret objective to overcome learning stagnation when minimax regret policies reach their regret bound."
}