{
    "title": "NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data",
    "abstract": "arXiv:2402.15343v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown impressive abilities in data annotation, opening the way for new approaches to solve classic NLP problems. In this paper, we show how to use LLMs to create NuNER, a compact language representation model specialized in the Named Entity Recognition (NER) task. NuNER can be fine-tuned to solve downstream NER problems in a data-efficient way, outperforming similar-sized foundation models in the few-shot regime and competing with much larger LLMs. We find that the size and entity-type diversity of the pre-training dataset are key to achieving good performance. We view NuNER as a member of the broader family of task-specific foundation models, recently unlocked by LLMs.",
    "link": "https://arxiv.org/abs/2402.15343",
    "context": "Title: NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data\nAbstract: arXiv:2402.15343v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown impressive abilities in data annotation, opening the way for new approaches to solve classic NLP problems. In this paper, we show how to use LLMs to create NuNER, a compact language representation model specialized in the Named Entity Recognition (NER) task. NuNER can be fine-tuned to solve downstream NER problems in a data-efficient way, outperforming similar-sized foundation models in the few-shot regime and competing with much larger LLMs. We find that the size and entity-type diversity of the pre-training dataset are key to achieving good performance. We view NuNER as a member of the broader family of task-specific foundation models, recently unlocked by LLMs.",
    "path": "papers/24/02/2402.15343.json",
    "total_tokens": 806,
    "translated_title": "NuNER: 利用LLM注释数据进行实体识别编码器预训练",
    "translated_abstract": "大型语言模型（LLMs）展现出在数据标注方面令人印象深刻的能力，为解决经典的自然语言处理问题提供了新的途径。本文展示了如何利用LLMs创建NuNER，这是一个专门针对命名实体识别（NER）任务的紧凑语言表示模型。NuNER可以被微调以以高效的方式解决下游的NER问题，在少样本学习领域胜过相似大小的基础模型，并与更大的LLMs竞争。我们发现预训练数据集的大小和实体类型的多样性是取得良好性能的关键。我们将NuNER视为最近被LLMs解锁的更广泛的特定任务基础模型家族的一员。",
    "tldr": "利用LLM注释数据进行实体识别编码器预训练，创建了NuNER，一种专门用于命名实体识别任务的紧凑语言表示模型，可以在少样本学习领域胜过相似大小的基础模型，并与更大的LLMs竞争。",
    "en_tdlr": "Pre-training an entity recognition encoder using LLM-annotated data, NuNER is introduced as a compact language representation model specialized for Named Entity Recognition (NER) tasks, outperforming similar-sized foundation models in few-shot learning and competing with larger LLMs."
}