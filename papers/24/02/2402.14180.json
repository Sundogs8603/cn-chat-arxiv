{
    "title": "Linear Transformers are Versatile In-Context Learners",
    "abstract": "arXiv:2402.14180v1 Announce Type: new  Abstract: Recent research has demonstrated that transformers, particularly linear attention models, implicitly execute gradient-descent-like algorithms on data provided in-context during their forward inference step. However, their capability in handling more complex problems remains unexplored. In this paper, we prove that any linear transformer maintains an implicit linear model and can be interpreted as performing a variant of preconditioned gradient descent. We also investigate the use of linear transformers in a challenging scenario where the training data is corrupted with different levels of noise. Remarkably, we demonstrate that for this problem linear transformers discover an intricate and highly effective optimization algorithm, surpassing or matching in performance many reasonable baselines. We reverse-engineer this algorithm and show that it is a novel approach incorporating momentum and adaptive rescaling based on noise levels. Our fi",
    "link": "https://arxiv.org/abs/2402.14180",
    "context": "Title: Linear Transformers are Versatile In-Context Learners\nAbstract: arXiv:2402.14180v1 Announce Type: new  Abstract: Recent research has demonstrated that transformers, particularly linear attention models, implicitly execute gradient-descent-like algorithms on data provided in-context during their forward inference step. However, their capability in handling more complex problems remains unexplored. In this paper, we prove that any linear transformer maintains an implicit linear model and can be interpreted as performing a variant of preconditioned gradient descent. We also investigate the use of linear transformers in a challenging scenario where the training data is corrupted with different levels of noise. Remarkably, we demonstrate that for this problem linear transformers discover an intricate and highly effective optimization algorithm, surpassing or matching in performance many reasonable baselines. We reverse-engineer this algorithm and show that it is a novel approach incorporating momentum and adaptive rescaling based on noise levels. Our fi",
    "path": "papers/24/02/2402.14180.json",
    "total_tokens": 799,
    "translated_title": "线性变换器是多功能的上下文学习器",
    "translated_abstract": "最近的研究表明，变换器，特别是线性注意力模型，在前向推理步骤中对提供的上下文数据隐含地执行类似于梯度下降的算法。然而，它们在处理更复杂问题方面的能力尚未被探索。本文证明了任何线性变换器都保持隐式线性模型，并可解释为执行一种变形的预条件梯度下降。我们还研究了在线性变换器在训练数据受到不同水平噪音干扰的挑战性场景中的应用。值得注意的是，我们展示了对于这个问题，线性变换器发现了一种复杂且高效的优化算法，超越或与许多合理基线的表现相匹敌。我们反向工程了这个算法，并表明这是一种基于动量和噪音水平的自适应重缩放的新方法。",
    "tldr": "线性变换器展示了在处理复杂问题和噪音干扰数据时的多功能性，通过发现一种新颖的优化算法，超越了许多合理的基线。",
    "en_tdlr": "Linear transformers showcase versatility in handling complex problems and noisy data by discovering a novel optimization algorithm that surpasses many reasonable baselines."
}