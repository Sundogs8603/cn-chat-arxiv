{
    "title": "Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models",
    "abstract": "arXiv:2402.10430v1 Announce Type: new  Abstract: Instruction-tuning language models has become a crucial step in aligning them for general use. Typically, this process involves extensive training on large datasets, incurring high training costs. In this paper, we introduce a novel training data selection based on the learning percentage of the samples. We assert that current language models possess the capability to autonomously select high-quality training data, leading to comparable or improved performance compared to training on the entire dataset. Our experiments span different-sized models, revealing that this characteristic holds for models ranging from 1B (small) to 13B (large) in size. Moreover, we demonstrate an interesting finding that the data hardness transfers across model sizes, and a smaller 350M model can effectively curate high-quality training data with hard samples for a larger 13B model, resulting in an equally or superior instruction-tuned model compared to trainin",
    "link": "https://arxiv.org/abs/2402.10430",
    "context": "Title: Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models\nAbstract: arXiv:2402.10430v1 Announce Type: new  Abstract: Instruction-tuning language models has become a crucial step in aligning them for general use. Typically, this process involves extensive training on large datasets, incurring high training costs. In this paper, we introduce a novel training data selection based on the learning percentage of the samples. We assert that current language models possess the capability to autonomously select high-quality training data, leading to comparable or improved performance compared to training on the entire dataset. Our experiments span different-sized models, revealing that this characteristic holds for models ranging from 1B (small) to 13B (large) in size. Moreover, we demonstrate an interesting finding that the data hardness transfers across model sizes, and a smaller 350M model can effectively curate high-quality training data with hard samples for a larger 13B model, resulting in an equally or superior instruction-tuned model compared to trainin",
    "path": "papers/24/02/2402.10430.json",
    "total_tokens": 898,
    "translated_title": "更小的语言模型可以为更大的语言模型选择指导调整训练数据",
    "translated_abstract": "指导调整语言模型已成为使它们适用于一般用途的关键步骤。通常，这个过程涉及大量的大规模数据集训练，导致高昂的训练成本。本文引入了一种基于样本学习百分比的新颖训练数据选择方法。我们断言目前的语言模型具有自主选择高质量训练数据的能力，从而导致与在整个数据集上训练相比具有可比或更好性能。我们的实验涵盖了不同规模的模型，揭示出这一特征适用于从1B（小）到13B（大）大小的模型。此外，我们展示了一个有趣的发现，即数据难度在不同模型大小之间传递，并且更小的350M模型可以有效地筛选出包含困难样本的高质量训练数据，用于更大的13B模型，导致一个与在整个数据集上训练相比同样或更优秀的指导调整模型。",
    "tldr": "更小的语言模型可以根据样本学习百分比自主选择高质量训练数据，支持更大语言模型的指导调整，实现相媲美甚至优于在整个数据集训练的性能。",
    "en_tdlr": "Smaller language models can autonomously select high-quality training data based on the learning percentage of the samples, supporting instruction tuning for larger language models and achieving comparable or superior performance compared to training on the entire dataset."
}