{
    "title": "FIPO: Free-form Instruction-oriented Prompt Optimization with Preference Dataset and Modular Fine-tuning Schema",
    "abstract": "arXiv:2402.11811v1 Announce Type: new  Abstract: In the quest to facilitate the deep intelligence of Large Language Models (LLMs) accessible in final-end user-bot interactions, the art of prompt crafting emerges as a critical yet complex task for the average user. Contrast to previous model-oriented yet instruction-agnostic Automatic Prompt Optimization methodologies, yielding polished results for predefined target models while suffering rapid degradation with out-of-box models, we present Free-form Instruction-oriented Prompt Optimization (FIPO). This approach is supported by our large-scale prompt preference dataset and employs a modular fine-tuning schema. The FIPO schema reimagines the optimization process into manageable modules, anchored by a meta prompt that dynamically adapts content. This allows for the flexible integration of the raw task instruction, the optional instruction response, and the optional ground truth to produce finely optimized task prompts. The FIPO preference",
    "link": "https://arxiv.org/abs/2402.11811",
    "context": "Title: FIPO: Free-form Instruction-oriented Prompt Optimization with Preference Dataset and Modular Fine-tuning Schema\nAbstract: arXiv:2402.11811v1 Announce Type: new  Abstract: In the quest to facilitate the deep intelligence of Large Language Models (LLMs) accessible in final-end user-bot interactions, the art of prompt crafting emerges as a critical yet complex task for the average user. Contrast to previous model-oriented yet instruction-agnostic Automatic Prompt Optimization methodologies, yielding polished results for predefined target models while suffering rapid degradation with out-of-box models, we present Free-form Instruction-oriented Prompt Optimization (FIPO). This approach is supported by our large-scale prompt preference dataset and employs a modular fine-tuning schema. The FIPO schema reimagines the optimization process into manageable modules, anchored by a meta prompt that dynamically adapts content. This allows for the flexible integration of the raw task instruction, the optional instruction response, and the optional ground truth to produce finely optimized task prompts. The FIPO preference",
    "path": "papers/24/02/2402.11811.json",
    "total_tokens": 814,
    "translated_title": "FIPO：基于自由形式指导的提示优化与偏好数据集和模块化微调模式",
    "translated_abstract": "在促进大语言模型在最终用户-机器人交互中的深度智能方面，提示创作的艺术被视为普通用户的一项关键但复杂的任务。与之前基于模型而不考虑指导的自动提示优化方法形成对比，这些方法为预定义目标模型产生了光滑的结果，但在使用开箱即用模型时容易快速退化，我们提出了基于自由形式指导的提示优化（FIPO）。这种方法得到我们的大规模提示偏好数据集的支持，并采用模块化微调模式。FIPO模式重新构思了优化过程，将其分解为可管理的模块，以动态调整内容的元提示为锚点。这允许灵活整合原始任务指导、可选指导响应和可选真实值，以生成经过精心优化的任务提示。",
    "tldr": "FIPO提出了基于自由形式指导的提示优化方法，结合偏好数据集和模块化微调模式，重新构思了优化过程并实现了灵活的任务提示生成。",
    "en_tdlr": "FIPO introduces a free-form instruction-oriented prompt optimization approach, supported by a preference dataset and modular fine-tuning schema, reimagining the optimization process and enabling flexible task prompt generation."
}