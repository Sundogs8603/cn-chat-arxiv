{
    "title": "ARGS: Alignment as Reward-Guided Search",
    "abstract": "Aligning large language models with human objectives is paramount, yet common approaches including RLHF suffer from unstable and resource-intensive training. In response to this challenge, we introduce ARGS, Alignment as Reward-Guided Search, a novel framework that integrates alignment into the decoding process, eliminating the need for expensive RL training. By adjusting the model's probabilistic predictions using a reward signal, ARGS generates texts with semantic diversity while being aligned with human preferences, offering a promising and flexible solution for aligning language models. Notably, ARGS demonstrates consistent enhancements in average reward compared to baselines across diverse alignment tasks and various model dimensions. For example, under the same greedy-based decoding strategy, our method improves the average reward by 19.56% relative to the baseline and secures a preference or tie score of 64.33% in GPT-4 evaluation. We believe that our framework, emphasizing deco",
    "link": "https://arxiv.org/abs/2402.01694",
    "context": "Title: ARGS: Alignment as Reward-Guided Search\nAbstract: Aligning large language models with human objectives is paramount, yet common approaches including RLHF suffer from unstable and resource-intensive training. In response to this challenge, we introduce ARGS, Alignment as Reward-Guided Search, a novel framework that integrates alignment into the decoding process, eliminating the need for expensive RL training. By adjusting the model's probabilistic predictions using a reward signal, ARGS generates texts with semantic diversity while being aligned with human preferences, offering a promising and flexible solution for aligning language models. Notably, ARGS demonstrates consistent enhancements in average reward compared to baselines across diverse alignment tasks and various model dimensions. For example, under the same greedy-based decoding strategy, our method improves the average reward by 19.56% relative to the baseline and secures a preference or tie score of 64.33% in GPT-4 evaluation. We believe that our framework, emphasizing deco",
    "path": "papers/24/02/2402.01694.json",
    "total_tokens": 954,
    "translated_title": "ARGS: 对齐作为奖励导向的搜索",
    "translated_abstract": "将大规模语言模型与人类目标对齐是至关重要的，然而常见的方法包括RLHF在训练过程中存在不稳定和资源密集的问题。为应对这一挑战，我们引入了一个新的框架ARGS，即对齐作为奖励导向的搜索，它将对齐融入到解码过程中，消除了昂贵的RL训练的需求。通过使用奖励信号调整模型的概率预测，ARGS生成具有语义多样性的文本，同时与人类偏好对齐，为对齐语言模型提供了一种有前景且灵活的解决方案。值得注意的是，在不同的对齐任务和不同的模型维度下，ARGS相对于基线显示出持续的奖励改进。例如，采用相同的贪婪解码策略，我们的方法相对于基线提高了19.56%的平均奖励，并在GPT-4评估中获得了64.33%的偏好或并列分数。我们相信，我们的框架强调了解码的创新性和效果。",
    "tldr": "ARGS是一个对齐作为奖励导向的搜索框架，通过在解码过程中将模型的概率预测调整为奖励信号，实现生成具有语义多样性且与人类偏好对齐的文本。与基线相比，在不同任务和模型维度下，ARGS具有持续的奖励增益，表现出很好的性能。",
    "en_tdlr": "ARGS is a novel framework that integrates alignment into the decoding process by adjusting the model's probabilistic predictions using a reward signal, resulting in the generation of diverse texts aligned with human preferences. It consistently outperforms baselines across various alignment tasks and model dimensions, demonstrating promising performance."
}