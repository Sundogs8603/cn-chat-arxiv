{
    "title": "Spyx: A Library for Just-In-Time Compiled Optimization of Spiking Neural Networks",
    "abstract": "arXiv:2402.18994v1 Announce Type: cross  Abstract: As the role of artificial intelligence becomes increasingly pivotal in modern society, the efficient training and deployment of deep neural networks have emerged as critical areas of focus. Recent advancements in attention-based large neural architectures have spurred the development of AI accelerators, facilitating the training of extensive, multi-billion parameter models. Despite their effectiveness, these powerful networks often incur high execution costs in production environments. Neuromorphic computing, inspired by biological neural processes, offers a promising alternative. By utilizing temporally-sparse computations, Spiking Neural Networks (SNNs) offer to enhance energy efficiency through a reduced and low-power hardware footprint. However, the training of SNNs can be challenging due to their recurrent nature which cannot as easily leverage the massive parallelism of modern AI accelerators. To facilitate the investigation of S",
    "link": "https://arxiv.org/abs/2402.18994",
    "context": "Title: Spyx: A Library for Just-In-Time Compiled Optimization of Spiking Neural Networks\nAbstract: arXiv:2402.18994v1 Announce Type: cross  Abstract: As the role of artificial intelligence becomes increasingly pivotal in modern society, the efficient training and deployment of deep neural networks have emerged as critical areas of focus. Recent advancements in attention-based large neural architectures have spurred the development of AI accelerators, facilitating the training of extensive, multi-billion parameter models. Despite their effectiveness, these powerful networks often incur high execution costs in production environments. Neuromorphic computing, inspired by biological neural processes, offers a promising alternative. By utilizing temporally-sparse computations, Spiking Neural Networks (SNNs) offer to enhance energy efficiency through a reduced and low-power hardware footprint. However, the training of SNNs can be challenging due to their recurrent nature which cannot as easily leverage the massive parallelism of modern AI accelerators. To facilitate the investigation of S",
    "path": "papers/24/02/2402.18994.json",
    "total_tokens": 831,
    "translated_title": "Spyx：用于脉冲神经网络即时编译优化的库",
    "translated_abstract": "随着人工智能在现代社会中的作用日益关键，深度神经网络的高效训练和部署已成为关注的重点领域。最近关注度较高的大规模神经架构的进展推动了AI加速器的发展，促进了庞大、多十亿参数模型的训练。尽管这些强大的网络很有效，但在生产环境中往往会产生高昂的执行成本。受生物神经过程启发，神经形态计算提供了一个有前途的替代方案。通过利用时间稀疏计算，脉冲神经网络（SNNs）通过减少和降低功耗硬件占用来增强能效。然而，由于其循环性质，SNNs的训练可能具有挑战性，难以像现代AI加速器那样轻松利用巨大的并行性。为了促进对S的研究",
    "tldr": "Spyx旨在提供一个用于即时编译优化脉冲神经网络的库，以在训练时增强能效并减少硬件占用。",
    "en_tdlr": "Spyx aims to provide a library for just-in-time compiled optimization of spiking neural networks to enhance energy efficiency during training and reduce hardware footprint."
}