{
    "title": "Rethinking the Starting Point: Enhancing Performance and Fairness of Federated Learning via Collaborative Pre-Training",
    "abstract": "Most existing federated learning (FL) methodologies have assumed training begins from a randomly initialized model. Recently, several studies have empirically demonstrated that leveraging a pre-trained model can offer advantageous initializations for FL. In this paper, we propose a collaborative pre-training approach, CoPreFL, which strategically designs a pre-trained model to serve as a good initialization for any downstream FL task. The key idea of our pre-training algorithm is a meta-learning procedure which mimics downstream distributed scenarios, enabling it to adapt to any unforeseen FL task. CoPreFL's pre-training optimization procedure also strikes a balance between average performance and fairness, with the aim of addressing these competing challenges in downstream FL tasks through intelligent initializations. Extensive experimental results validate that our pre-training method provides a robust initialization for any unseen downstream FL task, resulting in enhanced average pe",
    "link": "https://arxiv.org/abs/2402.02225",
    "context": "Title: Rethinking the Starting Point: Enhancing Performance and Fairness of Federated Learning via Collaborative Pre-Training\nAbstract: Most existing federated learning (FL) methodologies have assumed training begins from a randomly initialized model. Recently, several studies have empirically demonstrated that leveraging a pre-trained model can offer advantageous initializations for FL. In this paper, we propose a collaborative pre-training approach, CoPreFL, which strategically designs a pre-trained model to serve as a good initialization for any downstream FL task. The key idea of our pre-training algorithm is a meta-learning procedure which mimics downstream distributed scenarios, enabling it to adapt to any unforeseen FL task. CoPreFL's pre-training optimization procedure also strikes a balance between average performance and fairness, with the aim of addressing these competing challenges in downstream FL tasks through intelligent initializations. Extensive experimental results validate that our pre-training method provides a robust initialization for any unseen downstream FL task, resulting in enhanced average pe",
    "path": "papers/24/02/2402.02225.json",
    "total_tokens": 891,
    "translated_title": "重思出发点：通过协作预训练增强联邦学习的性能和公平性",
    "translated_abstract": "大多数现有的联邦学习方法假设训练从一个随机初始化的模型开始。最近的研究实证了利用预训练模型可以为联邦学习提供有益的初始化。在本文中，我们提出了一种协作预训练方法CoPreFL，该方法通过策略性地设计一个预训练模型，为任何下游联邦学习任务提供良好的初始化。我们的预训练算法的关键思想是模仿下游分布式场景的元学习过程，使其能够适应任何未知的联邦学习任务。CoPreFL的预训练优化过程也在平均性能和公平性之间取得了平衡，旨在通过智能初始化来解决下游联邦学习任务中的竞争挑战。大量实验结果验证了我们的预训练方法为任何未知的下游联邦学习任务提供了可靠的初始化，从而提高了平均性能。",
    "tldr": "本文提出了一种名为CoPreFL的协作预训练方法，该方法通过设计一个可适应任何联邦学习任务的预训练模型来提高性能和公平性。大量实验证实了该方法在提供可靠的初始化方面的有效性。",
    "en_tdlr": "This paper proposes a collaborative pre-training approach, CoPreFL, that enhances performance and fairness of federated learning by designing a pre-trained model that adapts to any downstream FL task. Extensive experiments validate the effectiveness of the method in providing reliable initializations."
}