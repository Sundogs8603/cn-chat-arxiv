{
    "title": "Demographic Bias of Expert-Level Vision-Language Foundation Models in Medical Imaging",
    "abstract": "arXiv:2402.14815v1 Announce Type: cross  Abstract: Advances in artificial intelligence (AI) have achieved expert-level performance in medical imaging applications. Notably, self-supervised vision-language foundation models can detect a broad spectrum of pathologies without relying on explicit training annotations. However, it is crucial to ensure that these AI models do not mirror or amplify human biases, thereby disadvantaging historically marginalized groups such as females or Black patients. The manifestation of such biases could systematically delay essential medical care for certain patient subgroups. In this study, we investigate the algorithmic fairness of state-of-the-art vision-language foundation models in chest X-ray diagnosis across five globally-sourced datasets. Our findings reveal that compared to board-certified radiologists, these foundation models consistently underdiagnose marginalized groups, with even higher rates seen in intersectional subgroups, such as Black fem",
    "link": "https://arxiv.org/abs/2402.14815",
    "context": "Title: Demographic Bias of Expert-Level Vision-Language Foundation Models in Medical Imaging\nAbstract: arXiv:2402.14815v1 Announce Type: cross  Abstract: Advances in artificial intelligence (AI) have achieved expert-level performance in medical imaging applications. Notably, self-supervised vision-language foundation models can detect a broad spectrum of pathologies without relying on explicit training annotations. However, it is crucial to ensure that these AI models do not mirror or amplify human biases, thereby disadvantaging historically marginalized groups such as females or Black patients. The manifestation of such biases could systematically delay essential medical care for certain patient subgroups. In this study, we investigate the algorithmic fairness of state-of-the-art vision-language foundation models in chest X-ray diagnosis across five globally-sourced datasets. Our findings reveal that compared to board-certified radiologists, these foundation models consistently underdiagnose marginalized groups, with even higher rates seen in intersectional subgroups, such as Black fem",
    "path": "papers/24/02/2402.14815.json",
    "total_tokens": 835,
    "translated_title": "医学影像中专家级视觉语言基础模型的人口统计偏见",
    "translated_abstract": "人工智能的进展已经在医学影像应用中实现了专家级表现。值得注意的是，自监督视觉语言基础模型可以在不依赖明确培训注释的情况下检测广泛的病理。然而，确保这些人工智能模型不反映或放大人类偏见至关重要，从而使女性或黑人患者等历史上被边缘化的群体处于不利地位。这种偏见的体现可能会系统性地延迟特定患者亚组的重要医疗护理。",
    "tldr": "本研究调查了全球五个数据集中最先进的视觉语言基础模型在胸片诊断中的算法公平性。我们的发现表明，与董事会认证的放射科医师相比，这些基础模型在诊断边缘化群体时一贯存在低诊断率，甚至在诸如黑人女性之类的交叉亚组中看到更高的比例。",
    "en_tdlr": "This study investigates the algorithmic fairness of state-of-the-art vision-language foundation models in chest X-ray diagnosis across five globally-sourced datasets, revealing that compared to board-certified radiologists, these models consistently underdiagnose marginalized groups, with even higher rates seen in intersectional subgroups, such as Black females."
}