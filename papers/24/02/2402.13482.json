{
    "title": "Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks",
    "abstract": "arXiv:2402.13482v1 Announce Type: cross  Abstract: Despite large successes of recent language models on diverse tasks, they suffer from severe performance degeneration in low-resource settings with limited training data available. Many existing works tackle this problem by generating synthetic data from the training data and then training models on them, recently using Large Language Models (LLMs). However, in low-resource settings, the amount of seed data samples to use for data augmentation is very small, which makes generated samples suboptimal and less diverse. To tackle this challenge, we propose a novel method that augments training data by incorporating a wealth of examples from other datasets, along with the given training data. Specifically, we first retrieve the relevant instances from other datasets, such as their input-output pairs or contexts, based on their similarities with the given seed data, and then prompt LLMs to generate new samples with the contextual information ",
    "link": "https://arxiv.org/abs/2402.13482",
    "context": "Title: Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks\nAbstract: arXiv:2402.13482v1 Announce Type: cross  Abstract: Despite large successes of recent language models on diverse tasks, they suffer from severe performance degeneration in low-resource settings with limited training data available. Many existing works tackle this problem by generating synthetic data from the training data and then training models on them, recently using Large Language Models (LLMs). However, in low-resource settings, the amount of seed data samples to use for data augmentation is very small, which makes generated samples suboptimal and less diverse. To tackle this challenge, we propose a novel method that augments training data by incorporating a wealth of examples from other datasets, along with the given training data. Specifically, we first retrieve the relevant instances from other datasets, such as their input-output pairs or contexts, based on their similarities with the given seed data, and then prompt LLMs to generate new samples with the contextual information ",
    "path": "papers/24/02/2402.13482.json",
    "total_tokens": 847,
    "translated_title": "用于低资源领域任务的检索增强数据增强",
    "translated_abstract": "尽管最近语言模型在多样任务上取得了巨大成功，但在训练数据有限的低资源环境中，它们的性能会严重下降。许多现有作品通过从训练数据生成合成数据，然后在其上训练模型来解决这个问题，最近使用大型语言模型（LLM）进行。然而，在低资源环境中，用于数据增强的种子数据样本数量非常少，这使得生成的样本不够理想且缺乏多样性。为了解决这一挑战，我们提出了一种新颖的方法，通过将其他数据集中丰富的示例与给定的训练数据结合起来，来增强训练数据。具体来说，我们首先通过与给定种子数据相似性基于其他数据集检索相关实例，例如它们的输入-输出对或上下文，然后提示LLM使用上下文信息生成新样本。",
    "tldr": "提出了一种用于低资源领域任务的新方法，通过结合来自其他数据集的相关示例来增强训练数据，以解决在低资源环境中生成样本不够理想和缺乏多样性的挑战"
}