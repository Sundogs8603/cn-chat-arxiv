{
    "title": "Intriguing Differences Between Zero-Shot and Systematic Evaluations of Vision-Language Transformer Models",
    "abstract": "Transformer-based models have dominated natural language processing and other areas in the last few years due to their superior (zero-shot) performance on benchmark datasets. However, these models are poorly understood due to their complexity and size. While probing-based methods are widely used to understand specific properties, the structures of the representation space are not systematically characterized; consequently, it is unclear how such models generalize and overgeneralize to new inputs beyond datasets. In this paper, based on a new gradient descent optimization method, we are able to explore the embedding space of a commonly used vision-language model. Using the Imagenette dataset, we show that while the model achieves over 99\\% zero-shot classification performance, it fails systematic evaluations completely. Using a linear approximation, we provide a framework to explain the striking differences. We have also obtained similar results using a different model to support that o",
    "link": "https://arxiv.org/abs/2402.08473",
    "context": "Title: Intriguing Differences Between Zero-Shot and Systematic Evaluations of Vision-Language Transformer Models\nAbstract: Transformer-based models have dominated natural language processing and other areas in the last few years due to their superior (zero-shot) performance on benchmark datasets. However, these models are poorly understood due to their complexity and size. While probing-based methods are widely used to understand specific properties, the structures of the representation space are not systematically characterized; consequently, it is unclear how such models generalize and overgeneralize to new inputs beyond datasets. In this paper, based on a new gradient descent optimization method, we are able to explore the embedding space of a commonly used vision-language model. Using the Imagenette dataset, we show that while the model achieves over 99\\% zero-shot classification performance, it fails systematic evaluations completely. Using a linear approximation, we provide a framework to explain the striking differences. We have also obtained similar results using a different model to support that o",
    "path": "papers/24/02/2402.08473.json",
    "total_tokens": 930,
    "translated_title": "零样本和系统性评估视觉语言Transformer模型之间的有趣差异",
    "translated_abstract": "过去几年中，基于Transformer的模型在自然语言处理和其他领域中占据主导地位，因为它们在基准数据集上具有出色的(零样本)性能。然而，由于其复杂性和规模，这些模型目前尚未被很好地理解。虽然探测法已广泛用于理解特定属性，但表示空间的结构尚未得到系统性的刻画；因此，目前还不清楚这样的模型如何推广和过度推广到超出数据集范围的新输入。在本文中，基于一种新的梯度下降优化方法，我们能够探索常用的视觉语言模型的嵌入空间。我们使用Imagenette数据集表明，尽管该模型实现了超过99%的零样本分类性能，但它在系统性评估中完全失败。我们使用线性近似提供了一个框架来解释这些显著的差异。我们还使用另一个模型获得了类似的结果，以支持这个解释。",
    "tldr": "本文通过新的梯度下降优化方法，探索了常用的视觉语言模型的嵌入空间。结果显示，尽管该模型在零样本分类上有超过99%的表现，但在系统性评估中却完全失败。通过线性近似，提供了一个解释这些差异的框架。",
    "en_tdlr": "This paper explores the embedding space of a commonly used vision-language model using a new gradient descent optimization method. The results show that while the model achieves over 99% performance in zero-shot classification, it completely fails systematic evaluations. A framework is provided to explain these striking differences using a linear approximation."
}