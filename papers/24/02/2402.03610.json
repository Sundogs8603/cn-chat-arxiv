{
    "title": "RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents",
    "abstract": "Owing to recent advancements, Large Language Models (LLMs) can now be deployed as agents for increasingly complex decision-making applications in areas including robotics, gaming, and API integration. However, reflecting past experiences in current decision-making processes, an innate human behavior, continues to pose significant challenges. Addressing this, we propose Retrieval-Augmented Planning (RAP) framework, designed to dynamically leverage past experiences corresponding to the current situation and context, thereby enhancing agents' planning capabilities. RAP distinguishes itself by being versatile: it excels in both text-only and multimodal environments, making it suitable for a wide range of tasks. Empirical evaluations demonstrate RAP's effectiveness, where it achieves SOTA performance in textual scenarios and notably enhances multimodal LLM agents' performance for embodied tasks. These results highlight RAP's potential in advancing the functionality and applicability of LLM ",
    "link": "https://arxiv.org/abs/2402.03610",
    "context": "Title: RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents\nAbstract: Owing to recent advancements, Large Language Models (LLMs) can now be deployed as agents for increasingly complex decision-making applications in areas including robotics, gaming, and API integration. However, reflecting past experiences in current decision-making processes, an innate human behavior, continues to pose significant challenges. Addressing this, we propose Retrieval-Augmented Planning (RAP) framework, designed to dynamically leverage past experiences corresponding to the current situation and context, thereby enhancing agents' planning capabilities. RAP distinguishes itself by being versatile: it excels in both text-only and multimodal environments, making it suitable for a wide range of tasks. Empirical evaluations demonstrate RAP's effectiveness, where it achieves SOTA performance in textual scenarios and notably enhances multimodal LLM agents' performance for embodied tasks. These results highlight RAP's potential in advancing the functionality and applicability of LLM ",
    "path": "papers/24/02/2402.03610.json",
    "total_tokens": 916,
    "translated_title": "RAP：具有上下文记忆的检索增强规划用于多模态LLM代理",
    "translated_abstract": "最近的进展使得大型语言模型(LLM)可以被部署为用于机器人、游戏和API集成等领域中越来越复杂的决策应用的代理。然而，利用过去的经验来指导当前的决策过程仍然是一个困难的问题。为解决这个问题，我们提出了一种称为检索增强规划（RAP）的框架，旨在动态地利用过去与当前情况和上下文相关的经验，从而提升代理的规划能力。RAP的特点在于它的多功能性：它在纯文本和多模态环境中表现出色，适用于多个任务。实证评估结果显示RAP的有效性，在文本场景中取得了SOTA的表现，并显著提升了多模态LLM代理在具身任务中的性能。这些结果突显了RAP在推进LLM的功能和适用性方面的潜力。",
    "tldr": "本研究提出了一种称为RAP的框架，它能够以动态方式利用过去的经验来增强代理的规划能力，并在纯文本和多模态环境中表现出色。实验结果显示RAP在文本场景中达到了最先进水平，并显著提高了多模态LLM代理在具身任务中的性能。"
}