{
    "title": "OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via Vision-Language Foundation Models",
    "abstract": "arXiv:2402.10670v1 Announce Type: new  Abstract: Object navigation (ObjectNav) requires an agent to navigate through unseen environments to find queried objects. Many previous methods attempted to solve this task by relying on supervised or reinforcement learning, where they are trained on limited household datasets with close-set objects. However, two key challenges are unsolved: understanding free-form natural language instructions that demand open-set objects, and generalizing to new environments in a zero-shot manner. Aiming to solve the two challenges, in this paper, we propose OpenFMNav, an Open-set Foundation Model based framework for zero-shot object Navigation. We first unleash the reasoning abilities of large language models (LLMs) to extract proposed objects from natural language instructions that meet the user's demand. We then leverage the generalizability of large vision language models (VLMs) to actively discover and detect candidate objects from the scene, building a Ve",
    "link": "https://arxiv.org/abs/2402.10670",
    "context": "Title: OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via Vision-Language Foundation Models\nAbstract: arXiv:2402.10670v1 Announce Type: new  Abstract: Object navigation (ObjectNav) requires an agent to navigate through unseen environments to find queried objects. Many previous methods attempted to solve this task by relying on supervised or reinforcement learning, where they are trained on limited household datasets with close-set objects. However, two key challenges are unsolved: understanding free-form natural language instructions that demand open-set objects, and generalizing to new environments in a zero-shot manner. Aiming to solve the two challenges, in this paper, we propose OpenFMNav, an Open-set Foundation Model based framework for zero-shot object Navigation. We first unleash the reasoning abilities of large language models (LLMs) to extract proposed objects from natural language instructions that meet the user's demand. We then leverage the generalizability of large vision language models (VLMs) to actively discover and detect candidate objects from the scene, building a Ve",
    "path": "papers/24/02/2402.10670.json",
    "total_tokens": 875,
    "translated_title": "OpenFMNav: 通过视觉-语言基础模型实现开放式零样本目标导航",
    "translated_abstract": "目标导航(ObjectNav)需要一个代理在未知环境中导航以找到查询对象。许多先前的方法尝试通过依赖监督学习或强化学习来解决这一任务，其中它们是在具有闭集对象的有限家庭数据集上进行训练的。然而，仍有两个关键挑战尚未解决：理解要求开放集对象的自由形式自然语言指令，并以零样本方式推广到新环境。为了解决这两个挑战，在本文中，我们提出了OpenFMNav，一种基于开放集基础模型的零样本目标导航框架。我们首先释放大型语言模型(LLMs)的推理能力，从符合用户需求的自然语言指令中提取提议的对象。然后，利用大型视觉语言模型(VLMs)的泛化能力，积极发现并检测场景中的候选对象，构建一个Ve",
    "tldr": "本研究提出了一种名为OpenFMNav的框架，通过大型语言模型和视觉语言模型解决了目标导航领域中关于理解自然语言指令和零样本泛化的问题。",
    "en_tdlr": "This research introduces a framework called OpenFMNav that addresses the challenges in object navigation field related to understanding natural language instructions and zero-shot generalization through large language models and vision-language models."
}