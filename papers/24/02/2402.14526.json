{
    "title": "Balanced Data Sampling for Language Model Training with Clustering",
    "abstract": "arXiv:2402.14526v1 Announce Type: cross  Abstract: Data plays a fundamental role in the training of Large Language Models (LLMs). While attention has been paid to the collection and composition of datasets, determining the data sampling strategy in training remains an open question. Most LLMs are trained with a simple strategy, random sampling. However, this sampling strategy ignores the unbalanced nature of training data distribution, which can be sub-optimal. In this paper, we propose ClusterClip Sampling to balance the text distribution of training data for better model training. Specifically, ClusterClip Sampling utilizes data clustering to reflect the data distribution of the training set and balances the common samples and rare samples during training based on the cluster results. A repetition clip operation is introduced to mitigate the overfitting issue led by samples from certain clusters. Extensive experiments validate the effectiveness of ClusterClip Sampling, which outperfo",
    "link": "https://arxiv.org/abs/2402.14526",
    "context": "Title: Balanced Data Sampling for Language Model Training with Clustering\nAbstract: arXiv:2402.14526v1 Announce Type: cross  Abstract: Data plays a fundamental role in the training of Large Language Models (LLMs). While attention has been paid to the collection and composition of datasets, determining the data sampling strategy in training remains an open question. Most LLMs are trained with a simple strategy, random sampling. However, this sampling strategy ignores the unbalanced nature of training data distribution, which can be sub-optimal. In this paper, we propose ClusterClip Sampling to balance the text distribution of training data for better model training. Specifically, ClusterClip Sampling utilizes data clustering to reflect the data distribution of the training set and balances the common samples and rare samples during training based on the cluster results. A repetition clip operation is introduced to mitigate the overfitting issue led by samples from certain clusters. Extensive experiments validate the effectiveness of ClusterClip Sampling, which outperfo",
    "path": "papers/24/02/2402.14526.json",
    "total_tokens": 831,
    "translated_title": "带聚类的语言模型训练平衡数据抽样",
    "translated_abstract": "数据在训练大型语言模型（LLM）中起着基础性作用。尽管人们已经关注数据集的收集和组成，但确定训练中的数据抽样策略仍然是一个悬而未决的问题。大多数LLM使用简单的随机抽样策略进行训练。然而，这种抽样策略忽视了训练数据分布的不均衡性，这可能是次优的。在本文中，我们提出了ClusterClip Sampling，以平衡训练数据的文本分布，以实现更好的模型训练。具体而言，ClusterClip Sampling利用数据聚类来反映训练集的数据分布，并根据聚类结果在训练过程中平衡常见样本和稀有样本。引入了重复裁剪操作来减轻由于来自某些聚类的样本导致的过拟合问题。大量实验证实了ClusterClip Sampling的有效性，它的表现优于",
    "tldr": "本文提出了一种名为ClusterClip Sampling的数据抽样方法，利用数据聚类平衡训练数据的文本分布，为更好的模型训练提供支持。",
    "en_tdlr": "This paper proposes a data sampling method called ClusterClip Sampling, which balances the text distribution of training data using data clustering, to support better model training."
}