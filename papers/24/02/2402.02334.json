{
    "title": "Arithmetic Feature Interaction Is Necessary for Deep Tabular Learning",
    "abstract": "Until recently, the question of the effective inductive bias of deep models on tabular data has remained unanswered. This paper investigates the hypothesis that arithmetic feature interaction is necessary for deep tabular learning. To test this point, we create a synthetic tabular dataset with a mild feature interaction assumption and examine a modified transformer architecture enabling arithmetical feature interactions, referred to as AMFormer. Results show that AMFormer outperforms strong counterparts in fine-grained tabular data modeling, data efficiency in training, and generalization. This is attributed to its parallel additive and multiplicative attention operators and prompt-based optimization, which facilitate the separation of tabular samples in an extended space with arithmetically-engineered features. Our extensive experiments on real-world data also validate the consistent effectiveness, efficiency, and rationale of AMFormer, suggesting it has established a strong inductive",
    "link": "https://arxiv.org/abs/2402.02334",
    "context": "Title: Arithmetic Feature Interaction Is Necessary for Deep Tabular Learning\nAbstract: Until recently, the question of the effective inductive bias of deep models on tabular data has remained unanswered. This paper investigates the hypothesis that arithmetic feature interaction is necessary for deep tabular learning. To test this point, we create a synthetic tabular dataset with a mild feature interaction assumption and examine a modified transformer architecture enabling arithmetical feature interactions, referred to as AMFormer. Results show that AMFormer outperforms strong counterparts in fine-grained tabular data modeling, data efficiency in training, and generalization. This is attributed to its parallel additive and multiplicative attention operators and prompt-based optimization, which facilitate the separation of tabular samples in an extended space with arithmetically-engineered features. Our extensive experiments on real-world data also validate the consistent effectiveness, efficiency, and rationale of AMFormer, suggesting it has established a strong inductive",
    "path": "papers/24/02/2402.02334.json",
    "total_tokens": 845,
    "translated_title": "深度表格学习需要算术特征交互",
    "translated_abstract": "直到最近，关于深度模型在表格数据上的有效归纳偏见的问题仍然没有答案。本文调查了算术特征交互对于深度表格学习的必要性假设。为了测试这一观点，我们创建了一个具有轻微特征交互假设的合成表格数据集，并研究了一种改进的Transformer架构，使其能够进行算术特征交互，称为AMFormer。结果显示，AMFormer在细粒度表格数据建模、训练数据效率和泛化方面优于强对手。这归因于其并行的加性和乘性注意力操作符和基于提示的优化，这有助于在具有算术工程特征的扩展空间中分离表格样本。我们在真实世界数据上进行了广泛的实验，也验证了AMFormer的一致有效性、效率和合理性，表明它已经建立了强有力的归纳能力。",
    "tldr": "本文研究了深度表格学习中算术特征交互的必要性，通过引入AMFormer模型，实现了在细粒度表格数据建模、训练数据效率和泛化方面的优越性能。",
    "en_tdlr": "This paper investigates the necessity of arithmetic feature interaction in deep tabular learning, and introduces the AMFormer model that achieves superior performance in fine-grained tabular data modeling, training data efficiency, and generalization."
}