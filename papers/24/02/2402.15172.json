{
    "title": "Attention-Guided Masked Autoencoders For Learning Image Representations",
    "abstract": "arXiv:2402.15172v1 Announce Type: cross  Abstract: Masked autoencoders (MAEs) have established themselves as a powerful method for unsupervised pre-training for computer vision tasks. While vanilla MAEs put equal emphasis on reconstructing the individual parts of the image, we propose to inform the reconstruction process through an attention-guided loss function. By leveraging advances in unsupervised object discovery, we obtain an attention map of the scene which we employ in the loss function to put increased emphasis on reconstructing relevant objects, thus effectively incentivizing the model to learn more object-focused representations without compromising the established masking strategy. Our evaluations show that our pre-trained models learn better latent representations than the vanilla MAE, demonstrated by improved linear probing and k-NN classification results on several benchmarks while at the same time making ViTs more robust against varying backgrounds.",
    "link": "https://arxiv.org/abs/2402.15172",
    "context": "Title: Attention-Guided Masked Autoencoders For Learning Image Representations\nAbstract: arXiv:2402.15172v1 Announce Type: cross  Abstract: Masked autoencoders (MAEs) have established themselves as a powerful method for unsupervised pre-training for computer vision tasks. While vanilla MAEs put equal emphasis on reconstructing the individual parts of the image, we propose to inform the reconstruction process through an attention-guided loss function. By leveraging advances in unsupervised object discovery, we obtain an attention map of the scene which we employ in the loss function to put increased emphasis on reconstructing relevant objects, thus effectively incentivizing the model to learn more object-focused representations without compromising the established masking strategy. Our evaluations show that our pre-trained models learn better latent representations than the vanilla MAE, demonstrated by improved linear probing and k-NN classification results on several benchmarks while at the same time making ViTs more robust against varying backgrounds.",
    "path": "papers/24/02/2402.15172.json",
    "total_tokens": 793,
    "translated_title": "基于注意力引导的遮罩自编码器用于学习图像表示",
    "translated_abstract": "Masked autoencoders (MAEs)已经被证明是一种强大的方法，用于计算机视觉任务的无监督预训练。我们提出通过一个注意力引导的损失函数来通知重建过程。通过利用无监督目标发现的进展，我们获得了场景的注意力图，将其用于损失函数中，增强了对重建相关对象的重点重建，从而有效地激励模型学习更注重对象的表示，同时不会损害已建立的遮罩策略。我们的评估表明，我们的预训练模型学习了比普通MAE更好的潜在表示，证明了在几个基准测试上通过改进的线性探测和k-NN分类结果，同时使ViTs对不同背景更具鲁棒性。",
    "tldr": "通过引入注意力引导的损失函数，该论文提出了一种新的遮罩自编码器，可以更好地学习图像中的对象表示，相比普通MAE表现出更好的性能。",
    "en_tdlr": "This paper presents a new masked autoencoder approach with attention-guided loss function, which can learn object representations in images more effectively than traditional MAEs, showing improved performance in various benchmarks."
}