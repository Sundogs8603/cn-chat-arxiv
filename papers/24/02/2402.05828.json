{
    "title": "Discovering Temporally-Aware Reinforcement Learning Algorithms",
    "abstract": "Recent advancements in meta-learning have enabled the automatic discovery of novel reinforcement learning algorithms parameterized by surrogate objective functions. To improve upon manually designed algorithms, the parameterization of this learned objective function must be expressive enough to represent novel principles of learning (instead of merely recovering already established ones) while still generalizing to a wide range of settings outside of its meta-training distribution. However, existing methods focus on discovering objective functions that, like many widely used objective functions in reinforcement learning, do not take into account the total number of steps allowed for training, or \"training horizon\". In contrast, humans use a plethora of different learning objectives across the course of acquiring a new ability. For instance, students may alter their studying techniques based on the proximity to exam deadlines and their self-assessed capabilities. This paper contends tha",
    "link": "https://arxiv.org/abs/2402.05828",
    "context": "Title: Discovering Temporally-Aware Reinforcement Learning Algorithms\nAbstract: Recent advancements in meta-learning have enabled the automatic discovery of novel reinforcement learning algorithms parameterized by surrogate objective functions. To improve upon manually designed algorithms, the parameterization of this learned objective function must be expressive enough to represent novel principles of learning (instead of merely recovering already established ones) while still generalizing to a wide range of settings outside of its meta-training distribution. However, existing methods focus on discovering objective functions that, like many widely used objective functions in reinforcement learning, do not take into account the total number of steps allowed for training, or \"training horizon\". In contrast, humans use a plethora of different learning objectives across the course of acquiring a new ability. For instance, students may alter their studying techniques based on the proximity to exam deadlines and their self-assessed capabilities. This paper contends tha",
    "path": "papers/24/02/2402.05828.json",
    "total_tokens": 787,
    "translated_title": "发现具有时间意识的强化学习算法",
    "translated_abstract": "最近的元学习进展使得根据代理目标函数自动发现参数化的新型强化学习算法成为可能。为了改进手动设计的算法，必须对这个学习到的目标函数的参数化进行改进，使其能够表达出学习的新原则（而不仅仅是恢复已经建立的原则），同时仍然适用于其元训练分布之外的各种设置。然而，现有方法集中于发现类似于强化学习中广泛使用的目标函数，这些目标函数不考虑训练所允许的总步数或“训练视野”。相反，人类在获取新能力的过程中会使用各种不同的学习目标。例如，学生可能会根据考试截止日期和自我评估的能力来改变他们的学习技巧。本文认为...",
    "tldr": "这篇论文研究了发现具有时间意识的强化学习算法，用于改进手动设计的算法，使其能够表达出学习的新原则，并适用于各种不同的设置。",
    "en_tdlr": "This paper explores the discovery of temporally-aware reinforcement learning algorithms to improve upon manually designed algorithms, allowing them to express novel principles of learning and be applicable to various settings."
}