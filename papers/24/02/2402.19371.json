{
    "title": "OpenMedLM: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models",
    "abstract": "arXiv:2402.19371v1 Announce Type: cross  Abstract: LLMs have become increasingly capable at accomplishing a range of specialized-tasks and can be utilized to expand equitable access to medical knowledge. Most medical LLMs have involved extensive fine-tuning, leveraging specialized medical data and significant, thus costly, amounts of computational power. Many of the top performing LLMs are proprietary and their access is limited to very few research groups. However, open-source (OS) models represent a key area of growth for medical LLMs due to significant improvements in performance and an inherent ability to provide the transparency and compliance required in healthcare. We present OpenMedLM, a prompting platform which delivers state-of-the-art (SOTA) performance for OS LLMs on medical benchmarks. We evaluated a range of OS foundation LLMs (7B-70B) on four medical benchmarks (MedQA, MedMCQA, PubMedQA, MMLU medical-subset). We employed a series of prompting strategies, including zero-s",
    "link": "https://arxiv.org/abs/2402.19371",
    "context": "Title: OpenMedLM: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models\nAbstract: arXiv:2402.19371v1 Announce Type: cross  Abstract: LLMs have become increasingly capable at accomplishing a range of specialized-tasks and can be utilized to expand equitable access to medical knowledge. Most medical LLMs have involved extensive fine-tuning, leveraging specialized medical data and significant, thus costly, amounts of computational power. Many of the top performing LLMs are proprietary and their access is limited to very few research groups. However, open-source (OS) models represent a key area of growth for medical LLMs due to significant improvements in performance and an inherent ability to provide the transparency and compliance required in healthcare. We present OpenMedLM, a prompting platform which delivers state-of-the-art (SOTA) performance for OS LLMs on medical benchmarks. We evaluated a range of OS foundation LLMs (7B-70B) on four medical benchmarks (MedQA, MedMCQA, PubMedQA, MMLU medical-subset). We employed a series of prompting strategies, including zero-s",
    "path": "papers/24/02/2402.19371.json",
    "total_tokens": 911,
    "translated_title": "OpenMedLM：在医学问答中，提示工程可以胜过对开源大型语言模型进行微调",
    "translated_abstract": "LLMs 在完成一系列专门任务方面变得越来越有能力，并且可以用来扩大对医学知识的公平访问。大多数医学 LLMs 都涉及大量微调，利用专门的医学数据和大量的计算资源，因此成本高昂。许多表现前列的 LLMs 是专有的，他们的访问仅限于少数研究团体。然而，开源（OS）模型代表了医学 LLMs 的一个重要增长领域，由于性能显著提升以及提供卫生保健所需的透明度和合规性的内在能力。我们提出了 OpenMedLM，这是一个提示平台，为医学基准上的 OS LLMs 提供了最先进的性能。我们在四个医学基准（MedQA、MedMCQA、PubMedQA、MMLU 医学子集）上评估了一系列 OS 基础 LLMs（7B-70B）。我们采用了一系列提示策略，包括零s",
    "tldr": "OpenMedLM 提出了一个提示平台，利用提示工程在医学问答中能够超越对开源大型语言模型进行微调，实现了在医学基准上的 SOTA 性能。"
}