{
    "title": "StyleDubber: Towards Multi-Scale Style Learning for Movie Dubbing",
    "abstract": "arXiv:2402.12636v1 Announce Type: new  Abstract: Given a script, the challenge in Movie Dubbing (Visual Voice Cloning, V2C) is to generate speech that aligns well with the video in both time and emotion, based on the tone of a reference audio track. Existing state-of-the-art V2C models break the phonemes in the script according to the divisions between video frames, which solves the temporal alignment problem but leads to incomplete phoneme pronunciation and poor identity stability. To address this problem, we propose StyleDubber, which switches dubbing learning from the frame level to phoneme level. It contains three main components: (1) A multimodal style adaptor operating at the phoneme level to learn pronunciation style from the reference audio, and generate intermediate representations informed by the facial emotion presented in the video; (2) An utterance-level style learning module, which guides both the mel-spectrogram decoding and the refining processes from the intermediate e",
    "link": "https://arxiv.org/abs/2402.12636",
    "context": "Title: StyleDubber: Towards Multi-Scale Style Learning for Movie Dubbing\nAbstract: arXiv:2402.12636v1 Announce Type: new  Abstract: Given a script, the challenge in Movie Dubbing (Visual Voice Cloning, V2C) is to generate speech that aligns well with the video in both time and emotion, based on the tone of a reference audio track. Existing state-of-the-art V2C models break the phonemes in the script according to the divisions between video frames, which solves the temporal alignment problem but leads to incomplete phoneme pronunciation and poor identity stability. To address this problem, we propose StyleDubber, which switches dubbing learning from the frame level to phoneme level. It contains three main components: (1) A multimodal style adaptor operating at the phoneme level to learn pronunciation style from the reference audio, and generate intermediate representations informed by the facial emotion presented in the video; (2) An utterance-level style learning module, which guides both the mel-spectrogram decoding and the refining processes from the intermediate e",
    "path": "papers/24/02/2402.12636.json",
    "total_tokens": 853,
    "translated_title": "StyleDubber: 面向电影配音的多尺度风格学习",
    "translated_abstract": "给定一份剧本，在电影配音（视觉语音克隆，V2C）中的挑战是根据参考音轨的语气，生成与视频在时间和情绪上都良好对齐的语音。现有的 V2C 模型根据视频帧间的间隔字断分割剧本的音素，这解决了时间对齐问题，但导致音素发音不完整和身份稳定性差。为了解决这个问题，我们提出 StyleDubber，它将配音学习从帧级别转为音素级别。它包含三个主要组件：（1）一个多模态风格适配器，以音素级别操作，从参考音频中学习发音风格，并生成受视频中呈现的面部情绪影响的中间表示；（2）一个以语句级别风格学习模块，引导中间表现的 mel-spectrogram 解码和细化过程。",
    "tldr": "StyleDubber提出了一种新的电影配音方法，通过在音素级别进行学习，解决了当前 V2C 模型中存在的音素发音不完整和身份稳定性差的问题。",
    "en_tdlr": "StyleDubber proposes a new method for movie dubbing, addressing the issues of incomplete phoneme pronunciation and poor identity stability in current V2C models by learning at the phoneme level."
}