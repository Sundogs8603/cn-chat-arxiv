{
    "title": "Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning",
    "abstract": "arXiv:2402.14789v1 Announce Type: cross  Abstract: Self-supervised learning excels in learning representations from large amounts of unlabeled data, demonstrating success across multiple data modalities. Yet, extending self-supervised learning to new modalities is non-trivial because the specifics of existing methods are tailored to each domain, such as domain-specific augmentations which reflect the invariances in the target task. While masked modeling is promising as a domain-agnostic framework for self-supervised learning because it does not rely on input augmentations, its mask sampling procedure remains domain-specific. We present Self-guided Masked Autoencoders (SMA), a fully domain-agnostic masked modeling method. SMA trains an attention based model using a masked modeling objective, by learning masks to sample without any domain-specific assumptions. We evaluate SMA on three self-supervised learning benchmarks in protein biology, chemical property prediction, and particle physi",
    "link": "https://arxiv.org/abs/2402.14789",
    "context": "Title: Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning\nAbstract: arXiv:2402.14789v1 Announce Type: cross  Abstract: Self-supervised learning excels in learning representations from large amounts of unlabeled data, demonstrating success across multiple data modalities. Yet, extending self-supervised learning to new modalities is non-trivial because the specifics of existing methods are tailored to each domain, such as domain-specific augmentations which reflect the invariances in the target task. While masked modeling is promising as a domain-agnostic framework for self-supervised learning because it does not rely on input augmentations, its mask sampling procedure remains domain-specific. We present Self-guided Masked Autoencoders (SMA), a fully domain-agnostic masked modeling method. SMA trains an attention based model using a masked modeling objective, by learning masks to sample without any domain-specific assumptions. We evaluate SMA on three self-supervised learning benchmarks in protein biology, chemical property prediction, and particle physi",
    "path": "papers/24/02/2402.14789.json",
    "total_tokens": 922,
    "translated_title": "针对领域无关自监督学习的自导蒙面自动编码器",
    "translated_abstract": "自监督学习在大量无标签数据中学习表示方面表现出色，并在多个数据模态上取得成功。然而，将自监督学习扩展到新的模态并不容易，因为现有方法的具体细节是针对每个领域量身定制的，比如特定领域的数据增强反映了目标任务中的不变性。 而蒙面建模作为一种领域无关的自监督学习框架很有前途，因为它不依赖于输入增强，但其蒙面采样过程仍然领域特定。我们提出了自导蒙面自动编码器（SMA），这是一种完全领域无关的蒙面建模方法。SMA使用基于注意力的模型进行训练，使用蒙面建模目标学习蒙面采样，而不做任何领域特定的假设。我们在蛋白生物学、化学性质预测和粒子物理学三个自监督学习基准上评估了SMA。",
    "tldr": "自导蒙面自动编码器（SMA）是一种完全领域无关的蒙面建模方法，通过学习蒙面采样而不做任何领域特定的假设，可以在各种数据模态上进行自监督学习。",
    "en_tdlr": "Self-guided Masked Autoencoders (SMA) is a fully domain-agnostic masked modeling method that learns mask sampling without any domain-specific assumptions, enabling self-supervised learning across various data modalities."
}