{
    "title": "Unraveling Babel: Exploring Multilingual Activation Patterns within Large Language Models",
    "abstract": "arXiv:2402.16367v1 Announce Type: new  Abstract: Recently, large language models (LLMs) have achieved tremendous breakthroughs in the field of language processing, yet their mechanisms in processing multiple languages remain agnostic. Therefore, in this work we study the multilingual activation patterns of LLMs. By transforming the original Large Language Models (LLMs) into a Mixture of Experts (MoE) architecture, we analyze the expert activation patterns when processing various languages and demonstrate the connections of these activation patterns at the level of language families. We discover the existence of non-language-specific neurons as well as language-specific activation neurons. Further exploration even showcases that merely leveraging high-frequency activation neurons can accelerate inference while maintaining comparable performance. These findings shed light on the LLMs' multilingual processing mechanism, and are of significant importance in guiding the multilingual trainin",
    "link": "https://arxiv.org/abs/2402.16367",
    "context": "Title: Unraveling Babel: Exploring Multilingual Activation Patterns within Large Language Models\nAbstract: arXiv:2402.16367v1 Announce Type: new  Abstract: Recently, large language models (LLMs) have achieved tremendous breakthroughs in the field of language processing, yet their mechanisms in processing multiple languages remain agnostic. Therefore, in this work we study the multilingual activation patterns of LLMs. By transforming the original Large Language Models (LLMs) into a Mixture of Experts (MoE) architecture, we analyze the expert activation patterns when processing various languages and demonstrate the connections of these activation patterns at the level of language families. We discover the existence of non-language-specific neurons as well as language-specific activation neurons. Further exploration even showcases that merely leveraging high-frequency activation neurons can accelerate inference while maintaining comparable performance. These findings shed light on the LLMs' multilingual processing mechanism, and are of significant importance in guiding the multilingual trainin",
    "path": "papers/24/02/2402.16367.json",
    "total_tokens": 882,
    "translated_title": "揭示巴别塔：探究大型语言模型内的多语言激活模式",
    "translated_abstract": "最近，大型语言模型（LLMs）在语言处理领域取得了巨大突破，但它们在处理多种语言时的机制仍然是未知的。因此，在这项工作中，我们研究了LLMs的多语言激活模式。通过将原始的大型语言模型（LLMs）转化为专家混合（MoE）架构，我们分析了处理各种语言时专家的激活模式，并展示了这些激活模式在语言家族层面上的联系。我们发现了非特定语言的神经元以及特定语言激活神经元的存在。进一步的探索甚至展示了仅利用高频激活神经元可以加速推断，同时保持可比较的性能。这些发现揭示了LLMs的多语言处理机制，并在指导多语言训练方面具有重要意义。",
    "tldr": "通过将原始的大型语言模型转化为专家混合架构，研究了LLMs的多语言激活模式，发现存在非特定语言的神经元和特定语言激活神经元，以及高频激活神经元可以加速推断并保持性能。",
    "en_tdlr": "By transforming the original Large Language Models into a Mixture of Experts architecture, the study explores the multilingual activation patterns of LLMs, revealing the existence of non-language-specific neurons and language-specific activation neurons, and highlighting the potential of utilizing high-frequency activation neurons to accelerate inference while maintaining performance."
}