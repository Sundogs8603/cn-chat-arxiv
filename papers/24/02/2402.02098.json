{
    "title": "Self-attention Networks Localize When QK-eigenspectrum Concentrates",
    "abstract": "The self-attention mechanism prevails in modern machine learning. It has an interesting functionality of adaptively selecting tokens from an input sequence by modulating the degree of attention localization, which many researchers speculate is the basis of the powerful model performance but complicates the underlying mechanism of the learning dynamics. In recent years, mainly two arguments have connected attention localization to the model performances. One is the rank collapse, where the embedded tokens by a self-attention block become very similar across different tokens, leading to a less expressive network. The other is the entropy collapse, where the attention probability approaches non-uniform and entails low entropy, making the learning dynamics more likely to be trapped in plateaus. These two failure modes may apparently contradict each other because the rank and entropy collapses are relevant to uniform and non-uniform attention, respectively. To this end, we characterize the ",
    "link": "https://arxiv.org/abs/2402.02098",
    "context": "Title: Self-attention Networks Localize When QK-eigenspectrum Concentrates\nAbstract: The self-attention mechanism prevails in modern machine learning. It has an interesting functionality of adaptively selecting tokens from an input sequence by modulating the degree of attention localization, which many researchers speculate is the basis of the powerful model performance but complicates the underlying mechanism of the learning dynamics. In recent years, mainly two arguments have connected attention localization to the model performances. One is the rank collapse, where the embedded tokens by a self-attention block become very similar across different tokens, leading to a less expressive network. The other is the entropy collapse, where the attention probability approaches non-uniform and entails low entropy, making the learning dynamics more likely to be trapped in plateaus. These two failure modes may apparently contradict each other because the rank and entropy collapses are relevant to uniform and non-uniform attention, respectively. To this end, we characterize the ",
    "path": "papers/24/02/2402.02098.json",
    "total_tokens": 823,
    "translated_title": "自注意网络在QK特征值谱集中时进行定位",
    "translated_abstract": "自注意机制在现代机器学习中非常流行。它具有适应性选择输入序列中的标记，并通过调节注意力定位的程度来实现，这被很多研究人员认为是强大模型性能的基础，但也复杂化了学习动力学的基本机制。近年来，主要有两种观点将注意力定位与模型性能联系起来。一种观点是秩坍缩，即自注意块嵌入的标记在不同的标记之间变得非常相似，导致网络表达能力降低。另一种观点是熵坍缩，即注意概率接近非均匀且熵低，使得学习动力学更容易陷入平台期。这两种失效模式似乎相互矛盾，因为秩和熵坍缩分别与均匀和非均匀注意力相关。为此，我们对QK特征值谱的集中定位进行了表征。",
    "tldr": "本文研究了自注意网络中的注意力定位问题，通过QK特征值谱的集中定位现象来解决不同观点之间的矛盾。",
    "en_tdlr": "This paper investigates attention localization in self-attention networks and proposes a solution to the contradictory views by focusing on the concentration of the QK eigenspectrum."
}