{
    "title": "Robust Multi-Task Learning with Excess Risks",
    "abstract": "Multi-task learning (MTL) considers learning a joint model for multiple tasks by optimizing a convex combination of all task losses. To solve the optimization problem, existing methods use an adaptive weight updating scheme, where task weights are dynamically adjusted based on their respective losses to prioritize difficult tasks. However, these algorithms face a great challenge whenever label noise is present, in which case excessive weights tend to be assigned to noisy tasks that have relatively large Bayes optimal errors, thereby overshadowing other tasks and causing performance to drop across the board. To overcome this limitation, we propose Multi-Task Learning with Excess Risks (ExcessMTL), an excess risk-based task balancing method that updates the task weights by their distances to convergence instead. Intuitively, ExcessMTL assigns higher weights to worse-trained tasks that are further from convergence. To estimate the excess risks, we develop an efficient and accurate method ",
    "link": "https://arxiv.org/abs/2402.02009",
    "context": "Title: Robust Multi-Task Learning with Excess Risks\nAbstract: Multi-task learning (MTL) considers learning a joint model for multiple tasks by optimizing a convex combination of all task losses. To solve the optimization problem, existing methods use an adaptive weight updating scheme, where task weights are dynamically adjusted based on their respective losses to prioritize difficult tasks. However, these algorithms face a great challenge whenever label noise is present, in which case excessive weights tend to be assigned to noisy tasks that have relatively large Bayes optimal errors, thereby overshadowing other tasks and causing performance to drop across the board. To overcome this limitation, we propose Multi-Task Learning with Excess Risks (ExcessMTL), an excess risk-based task balancing method that updates the task weights by their distances to convergence instead. Intuitively, ExcessMTL assigns higher weights to worse-trained tasks that are further from convergence. To estimate the excess risks, we develop an efficient and accurate method ",
    "path": "papers/24/02/2402.02009.json",
    "total_tokens": 880,
    "translated_title": "具有过多风险的鲁棒多任务学习",
    "translated_abstract": "多任务学习（MTL）通过优化所有任务损失的凸组合来考虑为多个任务学习一个联合模型。为了解决优化问题，现有方法使用自适应权重更新方案，根据各自的损失动态调整任务权重，以优先考虑困难任务。然而，在存在标签噪声的情况下，这些算法会面临巨大挑战，因为过多的权重往往被分配给具有相对较大贝叶斯最优误差的噪声任务，从而掩盖其他任务并导致整体性能下降。为了克服这个限制，我们提出了具有过多风险的多任务学习（ExcessMTL），这是一种基于过多风险的任务平衡方法，通过任务到收敛的距离来更新任务权重。直观来说，ExcessMTL将更高的权重分配给较差训练的距离收敛较远的任务。为了估计过多风险，我们开发了一种高效而准确的方法。",
    "tldr": "提出了一种具有过多风险的多任务学习（ExcessMTL）方法，根据任务到收敛的距离来更新任务权重，以克服存在标签噪声时现有方法的限制。"
}