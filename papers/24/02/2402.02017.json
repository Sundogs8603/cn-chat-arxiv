{
    "title": "Value-Aided Conditional Supervised Learning for Offline RL",
    "abstract": "Offline reinforcement learning (RL) has seen notable advancements through return-conditioned supervised learning (RCSL) and value-based methods, yet each approach comes with its own set of practical challenges. Addressing these, we propose Value-Aided Conditional Supervised Learning (VCS), a method that effectively synergizes the stability of RCSL with the stitching ability of value-based methods. Based on the Neural Tangent Kernel analysis to discern instances where value function may not lead to stable stitching, VCS injects the value aid into the RCSL's loss function dynamically according to the trajectory return. Our empirical studies reveal that VCS not only significantly outperforms both RCSL and value-based methods but also consistently achieves, or often surpasses, the highest trajectory returns across diverse offline RL benchmarks. This breakthrough in VCS paves new paths in offline RL, pushing the limits of what can be achieved and fostering further innovations.",
    "link": "https://arxiv.org/abs/2402.02017",
    "context": "Title: Value-Aided Conditional Supervised Learning for Offline RL\nAbstract: Offline reinforcement learning (RL) has seen notable advancements through return-conditioned supervised learning (RCSL) and value-based methods, yet each approach comes with its own set of practical challenges. Addressing these, we propose Value-Aided Conditional Supervised Learning (VCS), a method that effectively synergizes the stability of RCSL with the stitching ability of value-based methods. Based on the Neural Tangent Kernel analysis to discern instances where value function may not lead to stable stitching, VCS injects the value aid into the RCSL's loss function dynamically according to the trajectory return. Our empirical studies reveal that VCS not only significantly outperforms both RCSL and value-based methods but also consistently achieves, or often surpasses, the highest trajectory returns across diverse offline RL benchmarks. This breakthrough in VCS paves new paths in offline RL, pushing the limits of what can be achieved and fostering further innovations.",
    "path": "papers/24/02/2402.02017.json",
    "total_tokens": 1001,
    "translated_title": "无需奖励的条件监督学习在离线强化学习中的价值增强",
    "translated_abstract": "离线强化学习通过基于回报的条件监督学习（RCSL）和基于价值的方法取得了显著进展，但每种方法都存在一些实际挑战。为了解决这些挑战，我们提出了价值增强的条件监督学习（VCS）方法，该方法将RCSL的稳定性与基于价值的方法的连接能力有效地结合在一起。通过神经切线核分析，VCS可以动态地根据轨迹回报将价值帮助注入RCSL的损失函数中，以区分价值函数可能无法实现稳定连接的实例。我们的实证研究表明，VCS不仅显著优于RCSL和基于价值的方法，而且在各种离线强化学习基准测试中始终实现了或经常超过最高的轨迹回报。这一突破为离线强化学习开辟了新的道路，推动了可实现的极限，并促进了进一步的创新。",
    "tldr": "该论文提出了一种称为价值增强的条件监督学习方法，通过将RCSL的稳定性与基于价值的方法的连接能力相结合，动态地根据轨迹回报将价值帮助注入损失函数中。实验证明，该方法不仅优于现有方法，而且在各种离线强化学习任务中实现了最高的轨迹回报，推动了离线强化学习的发展。",
    "en_tdlr": "This paper proposes a method called Value-Aided Conditional Supervised Learning (VCS) that effectively combines the stability of return-conditioned supervised learning (RCSL) with the stitching ability of value-based methods. By dynamically injecting value aid into the loss function according to trajectory return, VCS outperforms existing methods and achieves the highest trajectory returns in various offline RL benchmarks, pushing the boundaries of offline RL and fostering further innovation."
}