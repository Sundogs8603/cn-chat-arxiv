{
    "title": "Beyond Hate Speech: NLP's Challenges and Opportunities in Uncovering Dehumanizing Language",
    "abstract": "arXiv:2402.13818v1 Announce Type: new  Abstract: Dehumanization, characterized as a subtle yet harmful manifestation of hate speech, involves denying individuals of their human qualities and often results in violence against marginalized groups. Despite significant progress in Natural Language Processing across various domains, its application in detecting dehumanizing language is limited, largely due to the scarcity of publicly available annotated data for this domain. This paper evaluates the performance of cutting-edge NLP models, including GPT-4, GPT-3.5, and LLAMA-2, in identifying dehumanizing language. Our findings reveal that while these models demonstrate potential, achieving a 70\\% accuracy rate in distinguishing dehumanizing language from broader hate speech, they also display biases. They are over-sensitive in classifying other forms of hate speech as dehumanization for a specific subset of target groups, while more frequently failing to identify clear cases of dehumanizati",
    "link": "https://arxiv.org/abs/2402.13818",
    "context": "Title: Beyond Hate Speech: NLP's Challenges and Opportunities in Uncovering Dehumanizing Language\nAbstract: arXiv:2402.13818v1 Announce Type: new  Abstract: Dehumanization, characterized as a subtle yet harmful manifestation of hate speech, involves denying individuals of their human qualities and often results in violence against marginalized groups. Despite significant progress in Natural Language Processing across various domains, its application in detecting dehumanizing language is limited, largely due to the scarcity of publicly available annotated data for this domain. This paper evaluates the performance of cutting-edge NLP models, including GPT-4, GPT-3.5, and LLAMA-2, in identifying dehumanizing language. Our findings reveal that while these models demonstrate potential, achieving a 70\\% accuracy rate in distinguishing dehumanizing language from broader hate speech, they also display biases. They are over-sensitive in classifying other forms of hate speech as dehumanization for a specific subset of target groups, while more frequently failing to identify clear cases of dehumanizati",
    "path": "papers/24/02/2402.13818.json",
    "total_tokens": 930,
    "translated_title": "超越仇恨言论: 自然语言处理在发现贬低性语言中的挑战与机遇",
    "translated_abstract": "人身具象化被定义为仇恨言论的一种微妙但有害的表现形式，涉及否认个人的人类特质，通常导致对边缘群体的暴力行为。尽管自然语言处理在各个领域取得了显著进展，但其在检测贬低性言语方面的应用有限，主要是由于这一领域公开可用的带标签数据稀缺。本文评估了最先进的NLP模型（包括GPT-4、GPT-3.5和LLAMA-2）在识别贬低性语言方面的性能。我们的发现显示，虽然这些模型表现出潜力，达到了70%的准确率来区分贬低性言语和更广泛的仇恨言论，但它们也显示出偏见。它们在对其他形式的仇恨言论进行分类时过于敏感，将其误判为特定目标群体的人身具象化，同时更频繁地未能识别明显的人身具象化案例。",
    "tldr": "本文评估了几种最先进的NLP模型在识别贬低性语言方面的性能，发现它们能够以70%的准确率区分贬低性语言和更广泛的仇恨言论，但也存在着偏见。"
}