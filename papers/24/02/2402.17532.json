{
    "title": "Retrieval is Accurate Generation",
    "abstract": "arXiv:2402.17532v1 Announce Type: new  Abstract: Standard language models generate text by selecting tokens from a fixed, finite, and standalone vocabulary. We introduce a novel method that selects context-aware phrases from a collection of supporting documents. One of the most significant challenges for this paradigm shift is determining the training oracles, because a string of text can be segmented in various ways and each segment can be retrieved from numerous possible documents. To address this, we propose to initialize the training oracles using linguistic heuristics and, more importantly, bootstrap the oracles through iterative self-reinforcement. Extensive experiments show that our model not only outperforms standard language models on a variety of knowledge-intensive tasks but also demonstrates improved generation quality in open-ended text generation. For instance, compared to the standard language model counterpart, our model raises the accuracy from 23.47% to 36.27% on Open",
    "link": "https://arxiv.org/abs/2402.17532",
    "context": "Title: Retrieval is Accurate Generation\nAbstract: arXiv:2402.17532v1 Announce Type: new  Abstract: Standard language models generate text by selecting tokens from a fixed, finite, and standalone vocabulary. We introduce a novel method that selects context-aware phrases from a collection of supporting documents. One of the most significant challenges for this paradigm shift is determining the training oracles, because a string of text can be segmented in various ways and each segment can be retrieved from numerous possible documents. To address this, we propose to initialize the training oracles using linguistic heuristics and, more importantly, bootstrap the oracles through iterative self-reinforcement. Extensive experiments show that our model not only outperforms standard language models on a variety of knowledge-intensive tasks but also demonstrates improved generation quality in open-ended text generation. For instance, compared to the standard language model counterpart, our model raises the accuracy from 23.47% to 36.27% on Open",
    "path": "papers/24/02/2402.17532.json",
    "total_tokens": 834,
    "translated_title": "检索即精准生成",
    "translated_abstract": "标准语言模型通过从固定的、有限的和独立的词汇中选择标记来生成文本。我们介绍了一种新颖的方法，从一组支持文档中选择上下文感知的短语。这种范式转变中最重要的挑战之一是确定训练数据，因为文本可以以多种方式分割，并且每个片段都可以从多个可能的文档中检索到。为了解决这个问题，我们提出使用语言启发式初始化训练数据，更重要的是通过迭代式自我强化来引导训练数据。大量实验表明，我们的模型不仅在各种知识密集型任务上优于标准语言模型，而且在开放式文本生成中展现出更好的生成质量。例如，与标准语言模型对应的模型，在开放性任务上将准确率从23.47%提高到36.27%。",
    "tldr": "提出了一种从支持文档中选择上下文感知短语的新颖生成方法，通过迭代式自我强化提高训练数据准确性，在各种任务中表现优越，提高了开放式文本生成的质量。",
    "en_tdlr": "Introducing a novel method for selecting context-aware phrases from supporting documents, improving training data accuracy through iterative self-reinforcement, outperforming in various tasks, and enhancing the quality of open-ended text generation."
}