{
    "title": "Uncertainty Awareness of Large Language Models Under Code Distribution Shifts: A Benchmark Study",
    "abstract": "Large Language Models (LLMs) have been widely employed in programming language analysis to enhance human productivity. Yet, their reliability can be compromised by various code distribution shifts, leading to inconsistent outputs. While probabilistic methods are known to mitigate such impact through uncertainty calibration and estimation, their efficacy in the language domain remains underexplored compared to their application in image-based tasks. In this work, we first introduce a large-scale benchmark dataset, incorporating three realistic patterns of code distribution shifts at varying intensities. Then we thoroughly investigate state-of-the-art probabilistic methods applied to CodeLlama using these shifted code snippets. We observe that these methods generally improve the uncertainty awareness of CodeLlama, with increased calibration quality and higher uncertainty estimation~(UE) precision. However, our study further reveals varied performance dynamics across different criteria (e",
    "link": "https://arxiv.org/abs/2402.05939",
    "context": "Title: Uncertainty Awareness of Large Language Models Under Code Distribution Shifts: A Benchmark Study\nAbstract: Large Language Models (LLMs) have been widely employed in programming language analysis to enhance human productivity. Yet, their reliability can be compromised by various code distribution shifts, leading to inconsistent outputs. While probabilistic methods are known to mitigate such impact through uncertainty calibration and estimation, their efficacy in the language domain remains underexplored compared to their application in image-based tasks. In this work, we first introduce a large-scale benchmark dataset, incorporating three realistic patterns of code distribution shifts at varying intensities. Then we thoroughly investigate state-of-the-art probabilistic methods applied to CodeLlama using these shifted code snippets. We observe that these methods generally improve the uncertainty awareness of CodeLlama, with increased calibration quality and higher uncertainty estimation~(UE) precision. However, our study further reveals varied performance dynamics across different criteria (e",
    "path": "papers/24/02/2402.05939.json",
    "total_tokens": 838,
    "translated_title": "大型语言模型在代码分布转移下的不确定性意识：基准研究",
    "translated_abstract": "大型语言模型（LLMs）被广泛应用于编程语言分析，以提高人类生产力。然而，它们的可靠性可能会受到各种代码分布转移的影响，导致输出不一致。尽管众所周知，概率方法通过不确定性校准和估计可以减轻此类影响，但与其在基于图像的任务中的应用相比，它们在语言领域的效果尚未得到充分探索。在这项工作中，我们首先引入了一个大规模的基准数据集，其中包含三种代码分布转移的实际模式，强度各异。然后，我们对CodeLlama应用最先进的概率方法进行了全面调查。我们观察到，这些方法通常可以提高CodeLlama对不确定性的意识，提高了校准质量和更高的不确定性估计精度。然而，我们的研究进一步揭示了不同标准下的性能动态。",
    "tldr": "本文研究了大型语言模型在代码分布转移下的不确定性意识，并通过引入大规模基准数据集和应用概率方法来提高语言模型的可靠性。",
    "en_tdlr": "This study explores uncertainty awareness of large language models under code distribution shifts and improves model reliability through introducing a large-scale benchmark dataset and applying probabilistic methods."
}