{
    "title": "Investigating Out-of-Distribution Generalization of GNNs: An Architecture Perspective",
    "abstract": "Graph neural networks (GNNs) have exhibited remarkable performance under the assumption that test data comes from the same distribution of training data. However, in real-world scenarios, this assumption may not always be valid. Consequently, there is a growing focus on exploring the Out-of-Distribution (OOD) problem in the context of graphs. Most existing efforts have primarily concentrated on improving graph OOD generalization from two \\textbf{model-agnostic} perspectives: data-driven methods and strategy-based learning. However, there has been limited attention dedicated to investigating the impact of well-known \\textbf{GNN model architectures} on graph OOD generalization, which is orthogonal to existing research. In this work, we provide the first comprehensive investigation of OOD generalization on graphs from an architecture perspective, by examining the common building blocks of modern GNNs. Through extensive experiments, we reveal that both the graph self-attention mechanism an",
    "link": "https://arxiv.org/abs/2402.08228",
    "context": "Title: Investigating Out-of-Distribution Generalization of GNNs: An Architecture Perspective\nAbstract: Graph neural networks (GNNs) have exhibited remarkable performance under the assumption that test data comes from the same distribution of training data. However, in real-world scenarios, this assumption may not always be valid. Consequently, there is a growing focus on exploring the Out-of-Distribution (OOD) problem in the context of graphs. Most existing efforts have primarily concentrated on improving graph OOD generalization from two \\textbf{model-agnostic} perspectives: data-driven methods and strategy-based learning. However, there has been limited attention dedicated to investigating the impact of well-known \\textbf{GNN model architectures} on graph OOD generalization, which is orthogonal to existing research. In this work, we provide the first comprehensive investigation of OOD generalization on graphs from an architecture perspective, by examining the common building blocks of modern GNNs. Through extensive experiments, we reveal that both the graph self-attention mechanism an",
    "path": "papers/24/02/2402.08228.json",
    "total_tokens": 822,
    "translated_title": "研究GNN的超分布推广：从架构角度的视角",
    "translated_abstract": "图神经网络（GNN）在测试数据来自于训练数据相同分布的假设下表现出了出色的性能。然而，在真实场景中，这个假设可能并不总是成立。因此，在图的上下文中，对超分布（OOD）问题的探索日益受到关注。大部分现有的研究主要集中在改进图的OOD推广的两个“模型无关”角度上：数据驱动方法和策略学习。然而，对于已知的GNN模型架构对图的OOD推广的影响的研究相对较少，这与现有的研究相互独立。在这项工作中，我们从架构的角度首次全面调查了图的OOD推广，并对现代GNN的常见构建模块进行了考察。通过大量实验，我们揭示了图自我注意机制和...",
    "tldr": "这项研究从架构的角度全面调查了图的超分布推广，揭示了图自我注意机制和其他常见构建模块在超分布问题上的影响。",
    "en_tdlr": "This study provides a comprehensive investigation of out-of-distribution (OOD) generalization on graphs from an architecture perspective and reveals the impact of graph self-attention mechanism and other common building blocks on OOD generalization."
}