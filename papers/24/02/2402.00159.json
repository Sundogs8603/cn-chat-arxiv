{
    "title": "Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research",
    "abstract": "Language models have become a critical technology to tackling a wide range of natural language processing tasks, yet many details about how the best-performing language models were developed are not reported. In particular, information about their pretraining corpora is seldom discussed: commercial language models rarely provide any information about their data; even open models rarely release datasets they are trained on, or an exact recipe to reproduce them. As a result, it is challenging to conduct certain threads of language modeling research, such as understanding how training data impacts model capabilities and shapes their limitations. To facilitate open research on language model pretraining, we release Dolma, a three trillion tokens English corpus, built from a diverse mixture of web content, scientific papers, code, public-domain books, social media, and encyclopedic materials. In addition, we open source our data curation toolkit to enable further experimentation and reprodu",
    "link": "https://arxiv.org/abs/2402.00159",
    "context": "Title: Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research\nAbstract: Language models have become a critical technology to tackling a wide range of natural language processing tasks, yet many details about how the best-performing language models were developed are not reported. In particular, information about their pretraining corpora is seldom discussed: commercial language models rarely provide any information about their data; even open models rarely release datasets they are trained on, or an exact recipe to reproduce them. As a result, it is challenging to conduct certain threads of language modeling research, such as understanding how training data impacts model capabilities and shapes their limitations. To facilitate open research on language model pretraining, we release Dolma, a three trillion tokens English corpus, built from a diverse mixture of web content, scientific papers, code, public-domain books, social media, and encyclopedic materials. In addition, we open source our data curation toolkit to enable further experimentation and reprodu",
    "path": "papers/24/02/2402.00159.json",
    "total_tokens": 944,
    "translated_title": "Dolma:一个包含三万亿个令牌的开放语料库，用于语言模型预训练研究",
    "translated_abstract": "语言模型已成为处理各种自然语言处理任务的关键技术，然而，关于最佳表现的语言模型是如何开发的很多细节并未报道。特别是，其预训练语料库的信息很少被讨论：商业语言模型很少提供有关其数据的任何信息；即使是开放模型也很少发布它们所训练的数据集，或者提供一个精确的复现方法。因此，进行一些语言建模研究变得具有挑战性，比如理解训练数据如何影响模型的能力和限制。为了促进语言模型预训练的开放研究，我们发布了Dolma，一个包含三万亿个令牌的英语语料库，其中包括各种各样的网络内容、科技论文、代码、公共领域图书、社交媒体和百科全书材料。此外，我们还开源了我们的数据整理工具包，以便进一步的实验和再现。",
    "tldr": "Dolma是一个包含三万亿个令牌的开放语料库，用于语言模型预训练研究。它包含了来自多种来源的网络内容、科技论文、代码、公共领域图书、社交媒体和百科全书材料。为了促进开放研究，我们还开源了数据整理工具包。"
}