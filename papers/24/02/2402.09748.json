{
    "title": "Model Compression and Efficient Inference for Large Language Models: A Survey",
    "abstract": "arXiv:2402.09748v1 Announce Type: cross  Abstract: Transformer based large language models have achieved tremendous success. However, the significant memory and computational costs incurred during the inference process make it challenging to deploy large models on resource-constrained devices. In this paper, we investigate compression and efficient inference methods for large language models from an algorithmic perspective. Regarding taxonomy, similar to smaller models, compression and acceleration algorithms for large language models can still be categorized into quantization, pruning, distillation, compact architecture design, dynamic networks. However, Large language models have two prominent characteristics compared to smaller models: (1) Most of compression algorithms require finetuning or even retraining the model after compression. The most notable aspect of large models is the very high cost associated with model finetuning or training. Therefore, many algorithms for large mode",
    "link": "https://arxiv.org/abs/2402.09748",
    "context": "Title: Model Compression and Efficient Inference for Large Language Models: A Survey\nAbstract: arXiv:2402.09748v1 Announce Type: cross  Abstract: Transformer based large language models have achieved tremendous success. However, the significant memory and computational costs incurred during the inference process make it challenging to deploy large models on resource-constrained devices. In this paper, we investigate compression and efficient inference methods for large language models from an algorithmic perspective. Regarding taxonomy, similar to smaller models, compression and acceleration algorithms for large language models can still be categorized into quantization, pruning, distillation, compact architecture design, dynamic networks. However, Large language models have two prominent characteristics compared to smaller models: (1) Most of compression algorithms require finetuning or even retraining the model after compression. The most notable aspect of large models is the very high cost associated with model finetuning or training. Therefore, many algorithms for large mode",
    "path": "papers/24/02/2402.09748.json",
    "total_tokens": 888,
    "translated_title": "大规模语言模型的模型压缩和高效推理：一项综述",
    "translated_abstract": "基于Transformer的大规模语言模型取得了巨大的成功。然而，在推理过程中所产生的显著的内存和计算成本使得在资源受限设备上部署大模型变得具有挑战性。本文从算法的角度探讨了大规模语言模型的压缩和高效推理方法。在分类方面，与较小的模型类似，用于大规模语言模型的压缩和加速算法仍可以分为量化、修剪、蒸馏、紧凑架构设计和动态网络。然而，与较小的模型相比，大规模语言模型有两个突出的特点：（1）大多数压缩算法在压缩后需要微调甚至重新训练模型。大模型最显著的方面是与模型微调或训练相关的非常高的成本。因此，许多针对大规模模型的算法都需要考虑这一点。",
    "tldr": "这项综述研究了大规模语言模型的压缩和高效推理方法，包括量化、修剪、蒸馏、紧凑架构设计和动态网络等方面。大模型的突出特点是压缩后需要微调或重新训练，并且相关的成本很高。"
}