{
    "title": "Disentangling the Causes of Plasticity Loss in Neural Networks",
    "abstract": "arXiv:2402.18762v1 Announce Type: new  Abstract: Underpinning the past decades of work on the design, initialization, and optimization of neural networks is a seemingly innocuous assumption: that the network is trained on a \\textit{stationary} data distribution. In settings where this assumption is violated, e.g.\\ deep reinforcement learning, learning algorithms become unstable and brittle with respect to hyperparameters and even random seeds. One factor driving this instability is the loss of plasticity, meaning that updating the network's predictions in response to new information becomes more difficult as training progresses. While many recent works provide analyses and partial solutions to this phenomenon, a fundamental question remains unanswered: to what extent do known mechanisms of plasticity loss overlap, and how can mitigation strategies be combined to best maintain the trainability of a network? This paper addresses these questions, showing that loss of plasticity can be dec",
    "link": "https://arxiv.org/abs/2402.18762",
    "context": "Title: Disentangling the Causes of Plasticity Loss in Neural Networks\nAbstract: arXiv:2402.18762v1 Announce Type: new  Abstract: Underpinning the past decades of work on the design, initialization, and optimization of neural networks is a seemingly innocuous assumption: that the network is trained on a \\textit{stationary} data distribution. In settings where this assumption is violated, e.g.\\ deep reinforcement learning, learning algorithms become unstable and brittle with respect to hyperparameters and even random seeds. One factor driving this instability is the loss of plasticity, meaning that updating the network's predictions in response to new information becomes more difficult as training progresses. While many recent works provide analyses and partial solutions to this phenomenon, a fundamental question remains unanswered: to what extent do known mechanisms of plasticity loss overlap, and how can mitigation strategies be combined to best maintain the trainability of a network? This paper addresses these questions, showing that loss of plasticity can be dec",
    "path": "papers/24/02/2402.18762.json",
    "total_tokens": 863,
    "translated_title": "解开神经网络中可塑性丧失的原因",
    "translated_abstract": "设计、初始化和优化神经网络的过去几十年的工作的基础是一个看似无关紧要的假设：网络是在一个\\textit{固定的}数据分布上进行训练的。在违反这一假设的情况下，例如在深度强化学习中，学习算法对超参数甚至随机种子变得不稳定和脆弱。导致这种不稳定性的一个因素是可塑性的丢失，这意味着随着训练的进行，更新网络的预测以应对新信息变得更加困难。尽管许多最近的研究对这种现象进行了分析并提出了部分解决方案，但一个基本问题仍然没有得到回答：已知的可塑性丧失机制在多大程度上重叠，以及如何结合减轻策略才能最好地维持网络的可训练性？本文解决了这些问题，展示了可塑性丧失可以被解除。",
    "tldr": "这篇论文研究了神经网络中可塑性丧失的原因，探讨了不稳定性的根源以及如何结合减轻策略以保持网络的可训练性。"
}