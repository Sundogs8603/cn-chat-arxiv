{
    "title": "Classes Are Not Equal: An Empirical Study on Image Recognition Fairness",
    "abstract": "arXiv:2402.18133v1 Announce Type: new  Abstract: In this paper, we present an empirical study on image recognition fairness, i.e., extreme class accuracy disparity on balanced data like ImageNet. We experimentally demonstrate that classes are not equal and the fairness issue is prevalent for image classification models across various datasets, network architectures, and model capacities. Moreover, several intriguing properties of fairness are identified. First, the unfairness lies in problematic representation rather than classifier bias. Second, with the proposed concept of Model Prediction Bias, we investigate the origins of problematic representation during optimization. Our findings reveal that models tend to exhibit greater prediction biases for classes that are more challenging to recognize. It means that more other classes will be confused with harder classes. Then the False Positives (FPs) will dominate the learning in optimization, thus leading to their poor accuracy. Further,",
    "link": "https://arxiv.org/abs/2402.18133",
    "context": "Title: Classes Are Not Equal: An Empirical Study on Image Recognition Fairness\nAbstract: arXiv:2402.18133v1 Announce Type: new  Abstract: In this paper, we present an empirical study on image recognition fairness, i.e., extreme class accuracy disparity on balanced data like ImageNet. We experimentally demonstrate that classes are not equal and the fairness issue is prevalent for image classification models across various datasets, network architectures, and model capacities. Moreover, several intriguing properties of fairness are identified. First, the unfairness lies in problematic representation rather than classifier bias. Second, with the proposed concept of Model Prediction Bias, we investigate the origins of problematic representation during optimization. Our findings reveal that models tend to exhibit greater prediction biases for classes that are more challenging to recognize. It means that more other classes will be confused with harder classes. Then the False Positives (FPs) will dominate the learning in optimization, thus leading to their poor accuracy. Further,",
    "path": "papers/24/02/2402.18133.json",
    "total_tokens": 847,
    "translated_title": "类不平等：关于图像识别公平性的实证研究",
    "translated_abstract": "在这篇论文中，我们对图像识别公平性进行了实证研究，即在诸如ImageNet之类的平衡数据上存在极端类准确率差异。我们通过实验证明，不同类别并不相等，公平性问题在各种数据集、网络架构和模型容量上的图像分类模型中普遍存在。此外，我们还发现了公平性的几个有趣特性。首先，不公平性主要源于有问题的表示，而非分类器偏差。其次，通过提出的“模型预测偏差”概念，我们研究了优化过程中有问题表示的起源。我们的发现表明，模型倾向于对更具挑战性的类别表现出更大的预测偏差。这意味着更多其他类别将与较难识别的类别混淆。然后，假阳例（FPs）将主导优化中的学习过程，从而导致它们的准确率较低。",
    "tldr": "图像分类模型中存在类准确率差异导致的不公平现象，主要是因为有问题的表示方式导致模型对更具挑战性的类别表现出更大的预测偏差。"
}