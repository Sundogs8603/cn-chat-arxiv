{
    "title": "EvoGrad: A Dynamic Take on the Winograd Schema Challenge with Human Adversaries",
    "abstract": "arXiv:2402.13372v1 Announce Type: new  Abstract: While Large Language Models (LLMs) excel at the Winograd Schema Challenge (WSC), a coreference resolution task testing common-sense reasoning through pronoun disambiguation, they struggle with instances that feature minor alterations or rewording. To address this, we introduce EvoGrad, an open-source platform that harnesses a human-in-the-loop approach to create a dynamic dataset tailored to such altered WSC instances. Leveraging ChatGPT's capabilities, we expand our task instances from 182 to 3,691, setting a new benchmark for diverse common-sense reasoning datasets. Additionally, we introduce the error depth metric, assessing model stability in dynamic tasks. Our results emphasize the challenge posed by EvoGrad: Even the best performing LLM, GPT-3.5, achieves an accuracy of 65.0% with an average error depth of 7.2, a stark contrast to human performance of 92. 8% accuracy without perturbation errors. This highlights ongoing model limita",
    "link": "https://arxiv.org/abs/2402.13372",
    "context": "Title: EvoGrad: A Dynamic Take on the Winograd Schema Challenge with Human Adversaries\nAbstract: arXiv:2402.13372v1 Announce Type: new  Abstract: While Large Language Models (LLMs) excel at the Winograd Schema Challenge (WSC), a coreference resolution task testing common-sense reasoning through pronoun disambiguation, they struggle with instances that feature minor alterations or rewording. To address this, we introduce EvoGrad, an open-source platform that harnesses a human-in-the-loop approach to create a dynamic dataset tailored to such altered WSC instances. Leveraging ChatGPT's capabilities, we expand our task instances from 182 to 3,691, setting a new benchmark for diverse common-sense reasoning datasets. Additionally, we introduce the error depth metric, assessing model stability in dynamic tasks. Our results emphasize the challenge posed by EvoGrad: Even the best performing LLM, GPT-3.5, achieves an accuracy of 65.0% with an average error depth of 7.2, a stark contrast to human performance of 92. 8% accuracy without perturbation errors. This highlights ongoing model limita",
    "path": "papers/24/02/2402.13372.json",
    "total_tokens": 980,
    "translated_title": "EvoGrad：以人类对手为特点的Winograd Schema挑战的动态方法",
    "translated_abstract": "虽然大型语言模型（LLMs）在Winograd Schema Challenge（WSC）中表现出色，该任务通过代词消歧义测试常识推理，但它们对于包含轻微修改或改写的实例感到困难。为了解决这个问题，我们引入EvoGrad，这是一个开源平台，利用人在环中的方法来创建一个适用于这种修改后WSC实例的动态数据集。利用ChatGPT的功能，我们将我们的任务实例从182扩展到3,691个，为多样化的常识推理数据集设定了一个新的基准。此外，我们引入了错误深度度量标准，评估模型在动态任务中的稳定性。我们的结果强调了EvoGrad所提出的挑战：即使性能最佳的LLM，GPT-3.5，在平均错误深度为7.2的情况下仅达到65.0%的准确率，与人类92.8%的准确率形成了鲜明对比，人类准确率没有干扰性错误。这突显了持续存在的模型限制",
    "tldr": "EvoGrad是一个以人类对手为特点的用于解决Winograd Schema挑战的动态方法，通过人在环中方法创建动态数据集，拓展任务实例并引入错误深度度量标准，提出新的多样化常识推理数据集基准，揭示了当前语言模型在此类任务上的挑战。",
    "en_tdlr": "EvoGrad is a dynamic approach with human adversaries designed to tackle the Winograd Schema Challenge, creating a dynamic dataset using human-in-the-loop method, expanding task instances, introducing error depth metric, setting a new benchmark for diverse common-sense reasoning datasets, and highlighting the ongoing challenge faced by current language models in such tasks."
}