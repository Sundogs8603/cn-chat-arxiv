{
    "title": "Less is KEN: a Universal and Simple Non-Parametric Pruning Algorithm for Large Language Models",
    "abstract": "Neural network pruning has become increasingly crucial due to the complexity of neural network models and their widespread use in various fields. Existing pruning algorithms often suffer from limitations such as architecture specificity, excessive complexity and reliance on complex calculations, rendering them impractical for real-world applications. In this paper, we propose KEN: a straightforward, universal and unstructured pruning algorithm based on Kernel Density Estimation (KDE). KEN aims to construct optimized transformer models by selectively preserving the most significant parameters while restoring others to their pre-training state. This approach maintains model performance while allowing storage of only the optimized subnetwork, leading to significant memory savings. Extensive evaluations on seven transformer models demonstrate that KEN achieves equal or better performance than the original models with a minimum parameter reduction of 25%. In-depth comparisons against other ",
    "link": "https://arxiv.org/abs/2402.03142",
    "context": "Title: Less is KEN: a Universal and Simple Non-Parametric Pruning Algorithm for Large Language Models\nAbstract: Neural network pruning has become increasingly crucial due to the complexity of neural network models and their widespread use in various fields. Existing pruning algorithms often suffer from limitations such as architecture specificity, excessive complexity and reliance on complex calculations, rendering them impractical for real-world applications. In this paper, we propose KEN: a straightforward, universal and unstructured pruning algorithm based on Kernel Density Estimation (KDE). KEN aims to construct optimized transformer models by selectively preserving the most significant parameters while restoring others to their pre-training state. This approach maintains model performance while allowing storage of only the optimized subnetwork, leading to significant memory savings. Extensive evaluations on seven transformer models demonstrate that KEN achieves equal or better performance than the original models with a minimum parameter reduction of 25%. In-depth comparisons against other ",
    "path": "papers/24/02/2402.03142.json",
    "total_tokens": 899,
    "translated_title": "\"少即是多：一种针对大型语言模型的通用简单非参数剪枝算法\"",
    "translated_abstract": "神经网络剪枝由于神经网络模型的复杂性以及在各个领域的广泛应用而变得越来越重要。现有的剪枝算法通常存在架构特异性、过度复杂和依赖复杂计算等限制，使它们在实际应用中变得不可行。本文提出了基于核密度估计（KDE）的简单、通用、非结构化剪枝算法KEN。KEN的目标是通过有选择性地保留最重要的参数，同时将其他参数恢复到预训练状态，从而构建优化后的transformer模型。这种方法在保持模型性能的同时，只存储优化后的子网络，实现了显著的内存节省。对七个transformer模型进行了广泛的评估，结果表明KEN在最少参数减少25%的情况下实现了与原始模型相等或更好的性能。与其他方法进行了深入对比。",
    "tldr": "本文提出了一种简单、通用、非参数的剪枝算法KEN，它能在保持模型性能的同时大幅节省内存，通过选择性地保留最重要的参数实现了对transformer模型的优化。与其他方法相比，KEN在最少参数减少25%的情况下实现了与原始模型相等或更好的性能。",
    "en_tdlr": "This paper proposes a simple, universal, non-parametric pruning algorithm KEN, which achieves memory savings by selectively preserving the most important parameters and optimizing transformer models while maintaining performance. Compared to other techniques, KEN achieves equal or better performance than the original models with a minimum parameter reduction of 25%."
}