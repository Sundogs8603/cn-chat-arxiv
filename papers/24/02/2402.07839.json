{
    "title": "Towards Meta-Pruning via Optimal Transport",
    "abstract": "Structural pruning of neural networks conventionally relies on identifying and discarding less important neurons, a practice often resulting in significant accuracy loss that necessitates subsequent fine-tuning efforts. This paper introduces a novel approach named Intra-Fusion, challenging this prevailing pruning paradigm. Unlike existing methods that focus on designing meaningful neuron importance metrics, Intra-Fusion redefines the overlying pruning procedure. Through utilizing the concepts of model fusion and Optimal Transport, we leverage an agnostically given importance metric to arrive at a more effective sparse model representation. Notably, our approach achieves substantial accuracy recovery without the need for resource-intensive fine-tuning, making it an efficient and promising tool for neural network compression.   Additionally, we explore how fusion can be added to the pruning process to significantly decrease the training time while maintaining competitive performance. We ",
    "link": "https://arxiv.org/abs/2402.07839",
    "context": "Title: Towards Meta-Pruning via Optimal Transport\nAbstract: Structural pruning of neural networks conventionally relies on identifying and discarding less important neurons, a practice often resulting in significant accuracy loss that necessitates subsequent fine-tuning efforts. This paper introduces a novel approach named Intra-Fusion, challenging this prevailing pruning paradigm. Unlike existing methods that focus on designing meaningful neuron importance metrics, Intra-Fusion redefines the overlying pruning procedure. Through utilizing the concepts of model fusion and Optimal Transport, we leverage an agnostically given importance metric to arrive at a more effective sparse model representation. Notably, our approach achieves substantial accuracy recovery without the need for resource-intensive fine-tuning, making it an efficient and promising tool for neural network compression.   Additionally, we explore how fusion can be added to the pruning process to significantly decrease the training time while maintaining competitive performance. We ",
    "path": "papers/24/02/2402.07839.json",
    "total_tokens": 826,
    "translated_title": "通过最优传输实现元剪枝",
    "translated_abstract": "传统神经网络的结构剪枝通常依赖于识别和丢弃不重要的神经元，这种做法往往会导致显著的准确度损失，需要随后的精调工作。本文提出了一种名为Intra-Fusion的新方法，挑战了传统的剪枝范式。与现有方法只关注设计有意义的神经元重要性度量不同，Intra-Fusion重新定义了上层剪枝过程。通过利用模型融合和最优传输的概念，我们利用给定的不可知重要性度量，得到了更有效的稀疏模型表示。值得注意的是，我们的方法在不需要资源密集型的精调的情况下实现了显著的准确度恢复，使其成为神经网络压缩的高效且有前景的工具。",
    "tldr": "本文提出了一种名为Intra-Fusion的新方法，通过模型融合和最优传输的概念实现了神经网络的元剪枝，该方法能够有效恢复准确度并避免精调工作，同时具有高效和有前景的特点。"
}