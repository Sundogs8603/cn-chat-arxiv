{
    "title": "Can Large Language Models perform Relation-based Argument Mining?",
    "abstract": "arXiv:2402.11243v1 Announce Type: cross  Abstract: Argument mining (AM) is the process of automatically extracting arguments, their components and/or relations amongst arguments and components from text. As the number of platforms supporting online debate increases, the need for AM becomes ever more urgent, especially in support of downstream tasks. Relation-based AM (RbAM) is a form of AM focusing on identifying agreement (support) and disagreement (attack) relations amongst arguments. RbAM is a challenging classification task, with existing methods failing to perform satisfactorily. In this paper, we show that general-purpose Large Language Models (LLMs), appropriately primed and prompted, can significantly outperform the best performing (RoBERTa-based) baseline. Specifically, we experiment with two open-source LLMs (Llama-2 and Mistral) with ten datasets.",
    "link": "https://arxiv.org/abs/2402.11243",
    "context": "Title: Can Large Language Models perform Relation-based Argument Mining?\nAbstract: arXiv:2402.11243v1 Announce Type: cross  Abstract: Argument mining (AM) is the process of automatically extracting arguments, their components and/or relations amongst arguments and components from text. As the number of platforms supporting online debate increases, the need for AM becomes ever more urgent, especially in support of downstream tasks. Relation-based AM (RbAM) is a form of AM focusing on identifying agreement (support) and disagreement (attack) relations amongst arguments. RbAM is a challenging classification task, with existing methods failing to perform satisfactorily. In this paper, we show that general-purpose Large Language Models (LLMs), appropriately primed and prompted, can significantly outperform the best performing (RoBERTa-based) baseline. Specifically, we experiment with two open-source LLMs (Llama-2 and Mistral) with ten datasets.",
    "path": "papers/24/02/2402.11243.json",
    "total_tokens": 791,
    "translated_title": "大型语言模型能够进行基于关系的论证挖掘吗？",
    "translated_abstract": "论据挖掘（AM）是从文本中自动提取论据、它们的组成部分和/或论据和组成部分之间关系的过程。随着支持在线辩论的平台数量不断增加，对AM的需求变得愈发迫切，特别是为了支持下游任务。基于关系的AM（RbAM）是一种关注识别论据之间协议（支持）和不同意（攻击）关系的AM形式。RbAM是一个具有挑战性的分类任务，现有方法无法令人满意地执行。在本文中，我们展示了通用型大型语言模型（LLMs），经过适当的调整和提示，可以显著优于表现最好的（基于RoBERTa的）基准线。具体来说，我们对两个开源LLM（Llama-2和Mistral）在十个数据集上进行了实验。",
    "tldr": "大型语言模型在处理关系型论证挖掘方面表现更好，能够显著超过目前最佳基准线，并且在十个数据集上进行了实验验证。",
    "en_tdlr": "Large Language Models perform significantly better in relation-based argument mining, outperforming the current best baseline and validated on ten datasets."
}