{
    "title": "BeTAIL: Behavior Transformer Adversarial Imitation Learning from Human Racing Gameplay",
    "abstract": "arXiv:2402.14194v1 Announce Type: new  Abstract: Imitation learning learns a policy from demonstrations without requiring hand-designed reward functions. In many robotic tasks, such as autonomous racing, imitated policies must model complex environment dynamics and human decision-making. Sequence modeling is highly effective in capturing intricate patterns of motion sequences but struggles to adapt to new environments or distribution shifts that are common in real-world robotics tasks. In contrast, Adversarial Imitation Learning (AIL) can mitigate this effect, but struggles with sample inefficiency and handling complex motion patterns. Thus, we propose BeTAIL: Behavior Transformer Adversarial Imitation Learning, which combines a Behavior Transformer (BeT) policy from human demonstrations with online AIL. BeTAIL adds an AIL residual policy to the BeT policy to model the sequential decision-making process of human experts and correct for out-of-distribution states or shifts in environmen",
    "link": "https://arxiv.org/abs/2402.14194",
    "context": "Title: BeTAIL: Behavior Transformer Adversarial Imitation Learning from Human Racing Gameplay\nAbstract: arXiv:2402.14194v1 Announce Type: new  Abstract: Imitation learning learns a policy from demonstrations without requiring hand-designed reward functions. In many robotic tasks, such as autonomous racing, imitated policies must model complex environment dynamics and human decision-making. Sequence modeling is highly effective in capturing intricate patterns of motion sequences but struggles to adapt to new environments or distribution shifts that are common in real-world robotics tasks. In contrast, Adversarial Imitation Learning (AIL) can mitigate this effect, but struggles with sample inefficiency and handling complex motion patterns. Thus, we propose BeTAIL: Behavior Transformer Adversarial Imitation Learning, which combines a Behavior Transformer (BeT) policy from human demonstrations with online AIL. BeTAIL adds an AIL residual policy to the BeT policy to model the sequential decision-making process of human experts and correct for out-of-distribution states or shifts in environmen",
    "path": "papers/24/02/2402.14194.json",
    "total_tokens": 874,
    "translated_title": "BeTAIL：从人类赛车游戏中学习的行为转换器对抗性模仿学习",
    "translated_abstract": "模仿学习是从示范中学习策略而无需手工设计奖励函数的方法。在许多机器人任务中，如自主赛车，被模仿的策略必须对复杂的环境动态和人类决策建模。序列建模非常有效地捕捉运动序列的复杂模式，但在适应新环境或分布转移方面却很难。相反，对抗性模仿学习（AIL）可以缓解这种效应，但在样本效率和处理复杂运动模式方面存在困难。因此，我们提出了BeTAIL：行为转换器对抗性模仿学习，它将来自人类示范的行为转换器（BeT）策略与在线AIL相结合。BeTAIL将一个AIL剩余策略添加到BeT策略中，以建模人类专家的顺序决策过程并纠正分布外状态或环境中的转移。",
    "tldr": "BeTAIL结合了行为转换器（BeT）策略和在线对抗性模仿学习（AIL），以学习从人类专家示范中学到的顺序决策过程，并纠正环境中的分布转移。",
    "en_tdlr": "BeTAIL combines Behavior Transformer (BeT) policy with online Adversarial Imitation Learning (AIL) to learn sequential decision-making processes from human expert demonstrations and correct distribution shifts in the environment."
}