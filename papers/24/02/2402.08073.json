{
    "title": "Grounding Data Science Code Generation with Input-Output Specifications",
    "abstract": "Large language models (LLMs) have recently demonstrated a remarkable ability to generate code from natural language (NL) prompts. However, in the real world, NL is often too ambiguous to capture the true intent behind programming problems, requiring additional input-output (I/O) specifications. Unfortunately, LLMs can have difficulty aligning their outputs with both the NL prompt and the I/O specification. In this paper, we give a way to mitigate this issue in the context of data science programming, where tasks require explicit I/O specifications for clarity. Specifically, we propose GIFT4Code, a novel approach for the instruction fine-tuning of LLMs with respect to I/O specifications. Our method leverages synthetic data produced by the LLM itself and utilizes execution-derived feedback as a key learning signal. This feedback, in the form of program I/O specifications, is provided to the LLM to facilitate instruction fine-tuning. We evaluated our approach on two challenging data scien",
    "link": "https://arxiv.org/abs/2402.08073",
    "context": "Title: Grounding Data Science Code Generation with Input-Output Specifications\nAbstract: Large language models (LLMs) have recently demonstrated a remarkable ability to generate code from natural language (NL) prompts. However, in the real world, NL is often too ambiguous to capture the true intent behind programming problems, requiring additional input-output (I/O) specifications. Unfortunately, LLMs can have difficulty aligning their outputs with both the NL prompt and the I/O specification. In this paper, we give a way to mitigate this issue in the context of data science programming, where tasks require explicit I/O specifications for clarity. Specifically, we propose GIFT4Code, a novel approach for the instruction fine-tuning of LLMs with respect to I/O specifications. Our method leverages synthetic data produced by the LLM itself and utilizes execution-derived feedback as a key learning signal. This feedback, in the form of program I/O specifications, is provided to the LLM to facilitate instruction fine-tuning. We evaluated our approach on two challenging data scien",
    "path": "papers/24/02/2402.08073.json",
    "total_tokens": 856,
    "translated_title": "使用输入输出规范来支撑数据科学代码生成",
    "translated_abstract": "最近，大型语言模型(LLM)展示了从自然语言(NL)提示生成代码的卓越能力。然而，在现实世界中，NL往往过于模糊，无法捕捉编程问题背后的真实意图，需要额外的输入输出(I/O)规范。不幸的是，LLM可能难以将其输出与NL提示和I/O规范对齐。在这篇论文中，我们提出了一种方法来缓解数据科学编程中的这个问题，其中任务需要明确的I/O规范以保证清晰度。具体而言，我们提出了GIFT4Code，一种用于基于I/O规范进行指导微调的LLM的新方法。我们的方法利用LLM本身产生的合成数据，并利用执行派生的反馈作为关键的学习信号。将该反馈以程序I/O规范的形式提供给LLM以促进指导微调。我们在两个具有挑战性的数据科学任务上评估了我们的方法。",
    "tldr": "该论文提出了一种方法，通过使用输入输出规范来解决大型语言模型在生成代码时与自然语言提示和I/O规范对齐困难的问题，并在数据科学编程任务上进行了评估。",
    "en_tdlr": "This paper proposes a method to address the difficulty of aligning large language models with natural language prompts and input-output specifications in code generation, and evaluates the approach in the context of data science programming tasks."
}