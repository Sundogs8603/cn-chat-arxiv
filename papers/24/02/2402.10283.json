{
    "title": "Backdoor Attack against One-Class Sequential Anomaly Detection Models",
    "abstract": "arXiv:2402.10283v1 Announce Type: cross  Abstract: Deep anomaly detection on sequential data has garnered significant attention due to the wide application scenarios. However, deep learning-based models face a critical security threat - their vulnerability to backdoor attacks. In this paper, we explore compromising deep sequential anomaly detection models by proposing a novel backdoor attack strategy. The attack approach comprises two primary steps, trigger generation and backdoor injection. Trigger generation is to derive imperceptible triggers by crafting perturbed samples from the benign normal data, of which the perturbed samples are still normal. The backdoor injection is to properly inject the backdoor triggers to comprise the model only for the samples with triggers. The experimental results demonstrate the effectiveness of our proposed attack strategy by injecting backdoors on two well-established one-class anomaly detection models.",
    "link": "https://arxiv.org/abs/2402.10283",
    "context": "Title: Backdoor Attack against One-Class Sequential Anomaly Detection Models\nAbstract: arXiv:2402.10283v1 Announce Type: cross  Abstract: Deep anomaly detection on sequential data has garnered significant attention due to the wide application scenarios. However, deep learning-based models face a critical security threat - their vulnerability to backdoor attacks. In this paper, we explore compromising deep sequential anomaly detection models by proposing a novel backdoor attack strategy. The attack approach comprises two primary steps, trigger generation and backdoor injection. Trigger generation is to derive imperceptible triggers by crafting perturbed samples from the benign normal data, of which the perturbed samples are still normal. The backdoor injection is to properly inject the backdoor triggers to comprise the model only for the samples with triggers. The experimental results demonstrate the effectiveness of our proposed attack strategy by injecting backdoors on two well-established one-class anomaly detection models.",
    "path": "papers/24/02/2402.10283.json",
    "total_tokens": 837,
    "translated_title": "对一类序列异常检测模型的后门攻击",
    "translated_abstract": "arXiv:2402.10283v1 公告类型：跨界 摘要：深度学习在序列数据上的异常检测引起了广泛关注，然而，基于深度学习的模型面临一种关键的安全威胁 - 它们容易受到后门攻击的影响。本文研究了通过提出一种新型后门攻击策略来妥协深度序列异常检测模型。攻击方法包括两个主要步骤，触发器生成和后门注入。 触发器生成是通过从良性正常数据中制作扰动样本来导出几乎不可察觉的触发器，其中扰动样本仍然正常。 后门注入则是适当地将后门触发器注入模型，只为具有触发器的样本。 实验结果表明了我们提出的攻击策略的有效性，通过在两个知名的一类异常检测模型上注入后门触发器。",
    "tldr": "本文提出了一种新型后门攻击策略，可以通过在良性正常数据中制作几乎不可察觉的触发器，并将其注入模型，成功妥协了两个一类异常检测模型。",
    "en_tdlr": "This paper proposes a novel backdoor attack strategy that compromises two one-class anomaly detection models by crafting imperceptible triggers from benign normal data and injecting them into the models."
}