{
    "title": "Optimizing Language Models for Human Preferences is a Causal Inference Problem",
    "abstract": "arXiv:2402.14979v1 Announce Type: cross  Abstract: As large language models (LLMs) see greater use in academic and commercial settings, there is increasing interest in methods that allow language models to generate texts aligned with human preferences. In this paper, we present an initial exploration of language model optimization for human preferences from direct outcome datasets, where each sample consists of a text and an associated numerical outcome measuring the reader's response. We first propose that language model optimization should be viewed as a causal problem to ensure that the model correctly learns the relationship between the text and the outcome. We formalize this causal language optimization problem, and we develop a method--causal preference optimization (CPO)--that solves an unbiased surrogate objective for the problem. We further extend CPO with doubly robust CPO (DR-CPO), which reduces the variance of the surrogate objective while retaining provably strong guarante",
    "link": "https://arxiv.org/abs/2402.14979",
    "context": "Title: Optimizing Language Models for Human Preferences is a Causal Inference Problem\nAbstract: arXiv:2402.14979v1 Announce Type: cross  Abstract: As large language models (LLMs) see greater use in academic and commercial settings, there is increasing interest in methods that allow language models to generate texts aligned with human preferences. In this paper, we present an initial exploration of language model optimization for human preferences from direct outcome datasets, where each sample consists of a text and an associated numerical outcome measuring the reader's response. We first propose that language model optimization should be viewed as a causal problem to ensure that the model correctly learns the relationship between the text and the outcome. We formalize this causal language optimization problem, and we develop a method--causal preference optimization (CPO)--that solves an unbiased surrogate objective for the problem. We further extend CPO with doubly robust CPO (DR-CPO), which reduces the variance of the surrogate objective while retaining provably strong guarante",
    "path": "papers/24/02/2402.14979.json",
    "total_tokens": 831,
    "translated_title": "优化语言模型以符合人类偏好是一个因果推断问题",
    "translated_abstract": "随着大型语言模型(LLMs)在学术和商业领域的广泛应用，越来越多的人对允许语言模型生成符合人类偏好文本的方法产生了兴趣。本文首次探索了从直接结果数据集中针对人类偏好进行语言模型优化，其中每个样本由一段文本和一个衡量读者响应的相关数值结果组成。我们首次提出应将语言模型优化视为一个因果问题，以确保模型正确学习文本与结果之间的关系。我们正式化了这个因果语言优化问题，并开发了一种方法--因果偏好优化(CPO)--来解决该问题的无偏替代目标。我们进一步使用双重稳健的CPO(DR-CPO)扩展CPO，降低了替代目标的方差，同时保留了明显强有力的保证。",
    "tldr": "本文首次提出将语言模型优化视为一个因果问题，提出了因果偏好优化方法并通过双重稳健CPO(DR-CPO)降低了替代目标的方差。",
    "en_tdlr": "This paper proposes to view language model optimization as a causal problem, introduces a method called causal preference optimization (CPO), and reduces the variance of the surrogate objective through doubly robust CPO (DR-CPO)."
}