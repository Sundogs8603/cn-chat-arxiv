{
    "title": "Seeing is not always believing: The Space of Harmless Perturbations",
    "abstract": "In the context of deep neural networks, we expose the existence of a harmless perturbation space, where perturbations leave the network output entirely unaltered. Perturbations within this harmless perturbation space, regardless of their magnitude when applied to images, exhibit no impact on the network's outputs of the original images. Specifically, given any linear layer within the network, where the input dimension $n$ exceeds the output dimension $m$, we demonstrate the existence of a continuous harmless perturbation subspace with a dimension of $(n-m)$. Inspired by this, we solve for a family of general perturbations that consistently influence the network output, irrespective of their magnitudes. With these theoretical findings, we explore the application of harmless perturbations for privacy-preserving data usage. Our work reveals the difference between DNNs and human perception that the significant perturbations captured by humans may not affect the recognition of DNNs. As a re",
    "link": "https://arxiv.org/abs/2402.02095",
    "context": "Title: Seeing is not always believing: The Space of Harmless Perturbations\nAbstract: In the context of deep neural networks, we expose the existence of a harmless perturbation space, where perturbations leave the network output entirely unaltered. Perturbations within this harmless perturbation space, regardless of their magnitude when applied to images, exhibit no impact on the network's outputs of the original images. Specifically, given any linear layer within the network, where the input dimension $n$ exceeds the output dimension $m$, we demonstrate the existence of a continuous harmless perturbation subspace with a dimension of $(n-m)$. Inspired by this, we solve for a family of general perturbations that consistently influence the network output, irrespective of their magnitudes. With these theoretical findings, we explore the application of harmless perturbations for privacy-preserving data usage. Our work reveals the difference between DNNs and human perception that the significant perturbations captured by humans may not affect the recognition of DNNs. As a re",
    "path": "papers/24/02/2402.02095.json",
    "total_tokens": 1026,
    "translated_title": "眼见未必为实：无害扰动空间的探索",
    "translated_abstract": "在深度神经网络的背景下，我们揭示了一种无害扰动空间的存在，即扰动会使网络输出完全不变。无论这些扰动在应用于图像时的大小如何，只要它们位于无害扰动空间内，就不会对原始图像的网络输出产生影响。具体而言，对于网络中的任何线性层，输入维度$n$超过输出维度$m$的情况下，我们证明了连续无害扰动子空间的存在，其维度为$(n-m)$。受此启发，我们解决了一族一致影响网络输出的通用扰动，而不论它们的大小如何。基于这些理论发现，我们探索了无害扰动在保护隐私数据使用方面的应用。我们的工作揭示了深度神经网络与人类感知之间的差异，即被人类捕捉到的重要扰动可能不会影响深度神经网络的识别能力。",
    "tldr": "在深度神经网络中，我们发现了一种无害扰动空间的存在，这种扰动不会影响网络对原始图像的输出。我们证明了在输入维度超过输出维度的情况下，存在一个连续的无害扰动子空间。我们还解决了一族通用扰动，这些扰动一致地影响网络输出。我们的工作揭示了深度神经网络与人类感知之间的差异，即深度神经网络对人类认为重要的扰动可能不会影响其识别能力。"
}