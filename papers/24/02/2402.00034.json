{
    "title": "Why does Prediction Accuracy Decrease over Time? Uncertain Positive Learning for Cloud Failure Prediction",
    "abstract": "With the rapid growth of cloud computing, a variety of software services have been deployed in the cloud. To ensure the reliability of cloud services, prior studies focus on failure instance (disk, node, and switch, etc.) prediction. Once the output of prediction is positive, mitigation actions are taken to rapidly resolve the underlying failure. According to our real-world practice in Microsoft Azure, we find that the prediction accuracy may decrease by about 9% after retraining the models. Considering that the mitigation actions may result in uncertain positive instances since they cannot be verified after mitigation, which may introduce more noise while updating the prediction model. To the best of our knowledge, we are the first to identify this Uncertain Positive Learning (UPLearning) issue in the real-world cloud failure prediction scenario. To tackle this problem, we design an Uncertain Positive Learning Risk Estimator (Uptake) approach. Using two real-world datasets of disk fai",
    "link": "https://arxiv.org/abs/2402.00034",
    "context": "Title: Why does Prediction Accuracy Decrease over Time? Uncertain Positive Learning for Cloud Failure Prediction\nAbstract: With the rapid growth of cloud computing, a variety of software services have been deployed in the cloud. To ensure the reliability of cloud services, prior studies focus on failure instance (disk, node, and switch, etc.) prediction. Once the output of prediction is positive, mitigation actions are taken to rapidly resolve the underlying failure. According to our real-world practice in Microsoft Azure, we find that the prediction accuracy may decrease by about 9% after retraining the models. Considering that the mitigation actions may result in uncertain positive instances since they cannot be verified after mitigation, which may introduce more noise while updating the prediction model. To the best of our knowledge, we are the first to identify this Uncertain Positive Learning (UPLearning) issue in the real-world cloud failure prediction scenario. To tackle this problem, we design an Uncertain Positive Learning Risk Estimator (Uptake) approach. Using two real-world datasets of disk fai",
    "path": "papers/24/02/2402.00034.json",
    "total_tokens": 925,
    "translated_title": "为什么预测准确性会随时间降低？云故障预测的不确定正样本学习",
    "translated_abstract": "随着云计算的快速发展，各种软件服务被部署在云中。为了保证云服务的可靠性，之前的研究主要关注故障实例（磁盘、节点和交换机等）的预测。一旦预测的输出结果为正，就会采取应对措施快速解决潜在故障。在我们在Microsoft Azure的实际应用中，我们发现在重新训练模型之后，预测准确性可能会下降约9%。考虑到应对措施可能会导致不确定的正实例，因为在应对之后无法验证它们，这可能在更新预测模型时引入更多噪音。据我们所知，在实际云故障预测场景中，我们是首次发现这个不确定正样本学习问题。为了解决这个问题，我们设计了一种不确定正样本学习风险估算器（Uptake）方法。使用两个实际的磁盘故障数据集进行实验验证。",
    "tldr": "本论文针对云故障预测中不确定正样本学习问题，设计了一种不确定正样本学习风险估算器（Uptake）方法，实验结果表明重新训练模型后预测准确性可能下降约9%。"
}