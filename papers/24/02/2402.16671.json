{
    "title": "StructLM: Towards Building Generalist Models for Structured Knowledge Grounding",
    "abstract": "arXiv:2402.16671v1 Announce Type: new  Abstract: Structured data sources, such as tables, graphs, and databases, are ubiquitous knowledge sources. Despite the demonstrated capabilities of large language models (LLMs) on plain text, their proficiency in interpreting and utilizing structured data remains limited. Our investigation reveals a notable deficiency in LLMs' ability to process structured data, e.g., ChatGPT lags behind state-of-the-art (SoTA) model by an average of 35%. To augment the Structured Knowledge Grounding (SKG) capabilities in LLMs, we have developed a comprehensive instruction tuning dataset comprising 1.1 million examples. Utilizing this dataset, we train a series of models, referred to as StructLM, based on the Code-LLaMA architecture, ranging from 7B to 34B parameters. Our StructLM series surpasses task-specific models on 14 out of 18 evaluated datasets and establishes new SoTA achievements on 7 SKG tasks. Furthermore, StructLM demonstrates exceptional generalizat",
    "link": "https://arxiv.org/abs/2402.16671",
    "context": "Title: StructLM: Towards Building Generalist Models for Structured Knowledge Grounding\nAbstract: arXiv:2402.16671v1 Announce Type: new  Abstract: Structured data sources, such as tables, graphs, and databases, are ubiquitous knowledge sources. Despite the demonstrated capabilities of large language models (LLMs) on plain text, their proficiency in interpreting and utilizing structured data remains limited. Our investigation reveals a notable deficiency in LLMs' ability to process structured data, e.g., ChatGPT lags behind state-of-the-art (SoTA) model by an average of 35%. To augment the Structured Knowledge Grounding (SKG) capabilities in LLMs, we have developed a comprehensive instruction tuning dataset comprising 1.1 million examples. Utilizing this dataset, we train a series of models, referred to as StructLM, based on the Code-LLaMA architecture, ranging from 7B to 34B parameters. Our StructLM series surpasses task-specific models on 14 out of 18 evaluated datasets and establishes new SoTA achievements on 7 SKG tasks. Furthermore, StructLM demonstrates exceptional generalizat",
    "path": "papers/24/02/2402.16671.json",
    "total_tokens": 863,
    "translated_title": "StructLM: 朝向构建结构化知识连接的通用模型",
    "translated_abstract": "结构化数据源，如表格、图形和数据库，是普遍存在的知识源。尽管大型语言模型（LLM）在纯文本上表现出色，但它们在解释和利用结构化数据方面的能力仍然有限。我们的研究揭示了LLM在处理结构化数据方面的显着不足，例如，ChatGPT平均落后于最先进模型(SoTA)35%。为增强LLM中的结构化知识连接（SKG）能力，我们开发了一个包含110万个示例的全面指令调整数据集。利用这个数据集，我们训练了一系列基于Code-LLaMA架构的模型，称为StructLM，参数范围从7B到34B。我们的StructLM系列在18个评估数据集中有14个超越了特定任务的模型，并在7个SKG任务上确立了新的SoTA成就。此外，StructLM展现了卓越的泛化能力。",
    "tldr": "StructLM系列模型基于全面指令调整数据集，超越了大多数任务特定模型，在结构化知识连接任务中取得了新的最先进成就。",
    "en_tdlr": "The StructLM series of models, based on a comprehensive instruction tuning dataset, surpasses the majority of task-specific models and achieves new state-of-the-art results in structured knowledge grounding tasks."
}