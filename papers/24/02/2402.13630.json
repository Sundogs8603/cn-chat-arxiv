{
    "title": "UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural Language",
    "abstract": "arXiv:2402.13630v1 Announce Type: new  Abstract: Foundation models like ChatGPT and GPT-4 have revolutionized artificial intelligence, exhibiting remarkable abilities to generalize across a wide array of tasks and applications beyond their initial training objectives. However, when this concept is applied to graph learning, a stark contrast emerges. Graph learning has predominantly focused on single-graph models, tailored to specific tasks or datasets, lacking the ability to transfer learned knowledge to different domains. This limitation stems from the inherent complexity and diversity of graph structures, along with the different feature and label spaces specific to graph data. In this paper, we present our UniGraph framework, designed to train a graph foundation model capable of generalizing to unseen graphs and tasks across diverse domains. Unlike single-graph models that use pre-computed node features of varying dimensions as input, our approach leverages Text-Attributed Graphs (T",
    "link": "https://arxiv.org/abs/2402.13630",
    "context": "Title: UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural Language\nAbstract: arXiv:2402.13630v1 Announce Type: new  Abstract: Foundation models like ChatGPT and GPT-4 have revolutionized artificial intelligence, exhibiting remarkable abilities to generalize across a wide array of tasks and applications beyond their initial training objectives. However, when this concept is applied to graph learning, a stark contrast emerges. Graph learning has predominantly focused on single-graph models, tailored to specific tasks or datasets, lacking the ability to transfer learned knowledge to different domains. This limitation stems from the inherent complexity and diversity of graph structures, along with the different feature and label spaces specific to graph data. In this paper, we present our UniGraph framework, designed to train a graph foundation model capable of generalizing to unseen graphs and tasks across diverse domains. Unlike single-graph models that use pre-computed node features of varying dimensions as input, our approach leverages Text-Attributed Graphs (T",
    "path": "papers/24/02/2402.13630.json",
    "total_tokens": 789,
    "translated_title": "UniGraph: 从自然语言中学习跨领域图基础模型",
    "translated_abstract": "arXiv:2402.13630v1 公告类型: 新摘要: ChatGPT 和 GPT-4 等基础模型已经彻底改变了人工智能，展示出在各种任务和应用中泛化的显著能力，超越了它们最初的训练目标。然而，当这个概念应用于图学习时，出现了鲜明的对比。图学习主要集中在针对特定任务或数据集定制的单个图模型上，缺乏将学到的知识转移到不同领域的能力。这种限制源于图结构的内在复杂性和多样性，以及特定于图数据的不同特征和标签空间。在本文中，我们提出了我们的UniGraph框架，旨在训练一个能够泛化到不同领域的未见图和任务的图基础模型。",
    "tldr": "UniGraph框架旨在训练一个能够泛化到不同领域的未见图和任务的图基础模型",
    "en_tdlr": "The UniGraph framework aims to train a graph foundation model capable of generalizing to unseen graphs and tasks across diverse domains."
}