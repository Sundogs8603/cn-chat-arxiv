{
    "title": "Continual Learning for Large Language Models: A Survey",
    "abstract": "Large language models (LLMs) are not amenable to frequent re-training, due to high training costs arising from their massive scale. However, updates are necessary to endow LLMs with new skills and keep them up-to-date with rapidly evolving human knowledge. This paper surveys recent works on continual learning for LLMs. Due to the unique nature of LLMs, we catalog continue learning techniques in a novel multi-staged categorization scheme, involving continual pretraining, instruction tuning, and alignment. We contrast continual learning for LLMs with simpler adaptation methods used in smaller models, as well as with other enhancement strategies like retrieval-augmented generation and model editing. Moreover, informed by a discussion of benchmarks and evaluation, we identify several challenges and future work directions for this crucial task.",
    "link": "https://rss.arxiv.org/abs/2402.01364",
    "context": "Title: Continual Learning for Large Language Models: A Survey\nAbstract: Large language models (LLMs) are not amenable to frequent re-training, due to high training costs arising from their massive scale. However, updates are necessary to endow LLMs with new skills and keep them up-to-date with rapidly evolving human knowledge. This paper surveys recent works on continual learning for LLMs. Due to the unique nature of LLMs, we catalog continue learning techniques in a novel multi-staged categorization scheme, involving continual pretraining, instruction tuning, and alignment. We contrast continual learning for LLMs with simpler adaptation methods used in smaller models, as well as with other enhancement strategies like retrieval-augmented generation and model editing. Moreover, informed by a discussion of benchmarks and evaluation, we identify several challenges and future work directions for this crucial task.",
    "path": "papers/24/02/2402.01364.json",
    "total_tokens": 825,
    "translated_title": "大型语言模型的持续学习: 一项综述",
    "translated_abstract": "由于其庞大的规模导致训练成本高昂，大型语言模型(LLMs)不易频繁重新训练。然而，更新是必要的，以赋予LLMs新的技能，并使其与快速发展的人类知识保持同步。本文对LLMs的持续学习最新研究进行了综述。鉴于LLMs的独特性，我们以一种新颖的多阶段分类方案对持续学习技术进行了分类，涉及持续预训练、指令调整和对齐等方面。我们将LLMs的持续学习与在规模较小的模型中使用的简单适应方法以及其他增强策略(如检索增强生成和模型编辑)进行了对比。此外，根据对基准和评估的讨论，我们确定了这一重要任务面临的几个挑战和未来工作方向。",
    "tldr": "这篇综述文章调查了大型语言模型(LLMs)的持续学习，使用了一种新颖的多阶段分类方案对持续学习技术进行了分类，指出了该领域面临的挑战和未来工作方向。",
    "en_tdlr": "This survey investigates continual learning for large language models (LLMs) and categorizes continual learning techniques using a novel multi-staged classification scheme. It highlights the challenges and future directions in this field."
}