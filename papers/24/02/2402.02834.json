{
    "title": "Shortened LLaMA: A Simple Depth Pruning for Large Language Models",
    "abstract": "Structured pruning of modern large language models (LLMs) has emerged as a way of decreasing their high computational needs. Width pruning reduces the size of projection weight matrices (e.g., by removing attention heads) while maintaining the number of layers. Depth pruning, in contrast, removes entire layers or blocks, while keeping the size of the remaining weights unchanged. Most current research focuses on either width-only or a blend of width and depth pruning, with little comparative analysis between the two units (width vs. depth) concerning their impact on LLM inference efficiency. In this work, we show that a simple depth pruning approach can compete with recent width pruning methods in terms of zero-shot task performance. Our pruning method boosts inference speeds, especially under memory-constrained conditions that require limited batch sizes for running LLMs, where width pruning is ineffective. We hope this work can help deploy LLMs on local and edge devices.",
    "link": "https://arxiv.org/abs/2402.02834",
    "context": "Title: Shortened LLaMA: A Simple Depth Pruning for Large Language Models\nAbstract: Structured pruning of modern large language models (LLMs) has emerged as a way of decreasing their high computational needs. Width pruning reduces the size of projection weight matrices (e.g., by removing attention heads) while maintaining the number of layers. Depth pruning, in contrast, removes entire layers or blocks, while keeping the size of the remaining weights unchanged. Most current research focuses on either width-only or a blend of width and depth pruning, with little comparative analysis between the two units (width vs. depth) concerning their impact on LLM inference efficiency. In this work, we show that a simple depth pruning approach can compete with recent width pruning methods in terms of zero-shot task performance. Our pruning method boosts inference speeds, especially under memory-constrained conditions that require limited batch sizes for running LLMs, where width pruning is ineffective. We hope this work can help deploy LLMs on local and edge devices.",
    "path": "papers/24/02/2402.02834.json",
    "total_tokens": 885,
    "translated_title": "简化的LLaMA: 大规模语言模型的简单深度修剪",
    "translated_abstract": "现代大规模语言模型 (LLMs) 的结构化修剪已成为降低其高计算需求的一种方法。宽度修剪减小投影权重矩阵的大小 (例如通过删除注意力头)，同时保持层数不变。与此相反，深度修剪则删除整个层或块，同时保持剩余权重的大小不变。目前的大多数研究集中在宽度修剪或宽度和深度修剪的混合上，很少对两者 (宽度与深度) 在对LLM推理效率的影响方面进行比较分析。在这项工作中，我们展示了一种简单的深度修剪方法可以与最新的宽度修剪方法在零-shot任务性能方面竞争。我们的修剪方法提高了推理速度，特别是在内存受限的情况下，需要对运行LLMs进行有限批次大小的条件，此时宽度修剪无效。我们希望这项工作能够帮助将LLMs部署在本地和边缘设备上。",
    "tldr": "使用简单的深度修剪方法可以提高大规模语言模型的推理速度，在内存受限的条件下表现良好，对部署在本地和边缘设备上的LLMs有帮助。",
    "en_tdlr": "Our work demonstrates that a simple depth pruning approach can significantly improve inference speed of large language models, especially under memory-constrained conditions. It is beneficial for deploying LLMs on local and edge devices."
}