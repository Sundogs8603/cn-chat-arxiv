{
    "title": "LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language Models",
    "abstract": "arXiv:2402.11417v1 Announce Type: cross  Abstract: Various parameter-efficient fine-tuning (PEFT) techniques have been proposed to enable computationally efficient fine-tuning while maintaining model performance. However, existing PEFT methods are still limited by the growing number of trainable parameters with the rapid deployment of Large Language Models (LLMs). To address this challenge, we present LoRETTA, an ultra-parameter-efficient framework that significantly reduces trainable parameters through tensor-train decomposition. Specifically, we propose two methods, named {LoRETTA}$_{adp}$ and {LoRETTA}$_{rep}$. The former employs tensorized adapters, offering a high-performance yet lightweight approach for the fine-tuning of LLMs. The latter emphasizes fine-tuning via weight parameterization with a set of small tensor factors. LoRETTA achieves comparable or better performance than most widely used PEFT methods with up to $100\\times$ fewer parameters on the LLaMA-2-7B models. Further",
    "link": "https://arxiv.org/abs/2402.11417",
    "context": "Title: LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language Models\nAbstract: arXiv:2402.11417v1 Announce Type: cross  Abstract: Various parameter-efficient fine-tuning (PEFT) techniques have been proposed to enable computationally efficient fine-tuning while maintaining model performance. However, existing PEFT methods are still limited by the growing number of trainable parameters with the rapid deployment of Large Language Models (LLMs). To address this challenge, we present LoRETTA, an ultra-parameter-efficient framework that significantly reduces trainable parameters through tensor-train decomposition. Specifically, we propose two methods, named {LoRETTA}$_{adp}$ and {LoRETTA}$_{rep}$. The former employs tensorized adapters, offering a high-performance yet lightweight approach for the fine-tuning of LLMs. The latter emphasizes fine-tuning via weight parameterization with a set of small tensor factors. LoRETTA achieves comparable or better performance than most widely used PEFT methods with up to $100\\times$ fewer parameters on the LLaMA-2-7B models. Further",
    "path": "papers/24/02/2402.11417.json",
    "total_tokens": 920,
    "translated_title": "LoRETTA: 低秩经济张量训练适应大型语言模型的超低参数微调",
    "translated_abstract": "各种参数高效微调（PEFT）技术被提出以实现在保持模型性能的情况下进行计算高效的微调。然而，随着大型语言模型（LLMs）的快速部署，现有的PEFT方法仍然受到可训练参数数量增长的限制。为了解决这一挑战，我们提出了LoRETTA，这是一个超参数高效的框架，通过张量训练分解显著减少可训练参数。具体来说，我们提出了两种方法，分别命名为{LoRETTA}$_{adp}$和{LoRETTA}$_{rep}$。前者采用张量化适配器，为LLMs的微调提供了高性能且轻量级的方法。后者强调通过一组小张量因子进行权重参数化的微调。LoRETTA在LLaMA-2-7B模型上比大多数广泛使用的PEFT方法具有可比或更好的性能，并且参数少达到100倍。",
    "tldr": "LoRETTA是一个通过张量训练分解显著减少可训练参数的超低参数高效框架，在大型语言模型的微调中表现出与大多数PEFT方法相媲美甚至更好的性能。",
    "en_tdlr": "LoRETTA is an ultra-parameter-efficient framework that significantly reduces trainable parameters through tensor-train decomposition, achieving comparable or better performance than most PEFT methods in fine-tuning large language models."
}