{
    "title": "Fine-Grained Detoxification via Instance-Level Prefixes for Large Language Models",
    "abstract": "arXiv:2402.15202v1 Announce Type: new  Abstract: Impressive results have been achieved in natural language processing (NLP) tasks through the training of large language models (LLMs). However, these models occasionally produce toxic content such as insults, threats, and profanity in response to certain prompts, thereby constraining their practical utility. To tackle this issue, various finetuning-based and decoding-based approaches have been utilized to mitigate toxicity. However, these methods typically necessitate additional costs such as high-quality training data or auxiliary models. In this paper, we propose fine-grained detoxification via instance-level prefixes (FGDILP) to mitigate toxic text without additional cost. Specifically, FGDILP contrasts the contextualized representation in attention space using a positive prefix-prepended prompt against multiple negative prefix-prepended prompts at the instance level. This allows for constructing fine-grained subtoxicity vectors, whic",
    "link": "https://arxiv.org/abs/2402.15202",
    "context": "Title: Fine-Grained Detoxification via Instance-Level Prefixes for Large Language Models\nAbstract: arXiv:2402.15202v1 Announce Type: new  Abstract: Impressive results have been achieved in natural language processing (NLP) tasks through the training of large language models (LLMs). However, these models occasionally produce toxic content such as insults, threats, and profanity in response to certain prompts, thereby constraining their practical utility. To tackle this issue, various finetuning-based and decoding-based approaches have been utilized to mitigate toxicity. However, these methods typically necessitate additional costs such as high-quality training data or auxiliary models. In this paper, we propose fine-grained detoxification via instance-level prefixes (FGDILP) to mitigate toxic text without additional cost. Specifically, FGDILP contrasts the contextualized representation in attention space using a positive prefix-prepended prompt against multiple negative prefix-prepended prompts at the instance level. This allows for constructing fine-grained subtoxicity vectors, whic",
    "path": "papers/24/02/2402.15202.json",
    "total_tokens": 861,
    "translated_title": "通过实例级前缀实现大型语言模型的细粒度脱毒",
    "translated_abstract": "通过训练大型语言模型（LLMs），在自然语言处理（NLP）任务中取得了令人印象深刻的结果。然而，这些模型偶尔会对某些提示生成毒性内容，如侮辱、威胁和粗话，从而限制了它们的实际效用。为了解决这一问题，利用各种基于微调和基于解码的方法来减轻毒性。然而，这些方法通常需要额外的成本，如高质量的训练数据或辅助模型。在本文中，我们提出了通过实例级前缀进行细粒度脱毒（FGDILP），以减轻毒性文本而无需额外费用。具体来说，FGDILP通过在实例级别使用一个带有正前缀的提示来对比注意力空间中的上下文表示，而多个带有负前缀的提示。这允许构建细粒度的次毒性向量，使文本被识别为次毒性变得更加精细。",
    "tldr": "提出一种通过实例级前缀在注意力空间中进行细粒度比较，从而实现大型语言模型的细粒度脱毒的方法。",
    "en_tdlr": "Propose a method for fine-grained detoxification of large language models by conducting fine-grained comparisons in attention space through instance-level prefixes."
}