{
    "title": "FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability",
    "abstract": "arXiv:2402.18667v1 Announce Type: new  Abstract: This paper presents FoFo, a pioneering benchmark for evaluating large language models' (LLMs) ability to follow complex, domain-specific formats, a crucial yet underexamined capability for their application as AI agents. Despite LLMs' advancements, existing benchmarks fail to assess their format-following proficiency adequately. FoFo fills this gap with a diverse range of real-world formats and instructions, developed through an AI-Human collaborative method. Our evaluation across both open-source (e.g., Llama 2, WizardLM) and closed-source (e.g., GPT-4, PALM2, Gemini) LLMs highlights three key findings: open-source models significantly lag behind closed-source ones in format adherence; LLMs' format-following performance is independent of their content generation quality; and LLMs' format proficiency varies across different domains. These insights suggest the need for specialized tuning for format-following skills and highlight FoFo's ro",
    "link": "https://arxiv.org/abs/2402.18667",
    "context": "Title: FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability\nAbstract: arXiv:2402.18667v1 Announce Type: new  Abstract: This paper presents FoFo, a pioneering benchmark for evaluating large language models' (LLMs) ability to follow complex, domain-specific formats, a crucial yet underexamined capability for their application as AI agents. Despite LLMs' advancements, existing benchmarks fail to assess their format-following proficiency adequately. FoFo fills this gap with a diverse range of real-world formats and instructions, developed through an AI-Human collaborative method. Our evaluation across both open-source (e.g., Llama 2, WizardLM) and closed-source (e.g., GPT-4, PALM2, Gemini) LLMs highlights three key findings: open-source models significantly lag behind closed-source ones in format adherence; LLMs' format-following performance is independent of their content generation quality; and LLMs' format proficiency varies across different domains. These insights suggest the need for specialized tuning for format-following skills and highlight FoFo's ro",
    "path": "papers/24/02/2402.18667.json",
    "total_tokens": 909,
    "translated_title": "FOFO：用于评估LLMs格式追随能力的基准测试",
    "translated_abstract": "本文介绍了FoFo，这是一个开创性的基准测试，用于评估大型语言模型（LLMs）追随复杂领域特定格式的能力，这是它们作为AI代理的应用中至关重要但未经充分考虑的能力。尽管LLMs有了进展，现有的基准测试未能充分评估它们的格式追随能力。FoFo通过AI-人类协作方法开发了多样化的真实世界格式和指令，填补了这一空白。我们的评估跨越开源模型（例如Llama 2，WizardLM）和闭源模型（例如GPT-4，PALM2，Gemini），突出了三个关键发现：开源模型在格式遵循方面明显落后于闭源模型；LLMs的格式追随表现独立于它们的内容生成质量；LLMs的格式熟练度在不同领域之间变化。这些见解表明需要专门调整格式追随技能，并突出了FoFo的重要性",
    "tldr": "FOFO是一个基准测试，用于评估大型语言模型追随复杂、领域特定格式的能力，揭示了LLMs在格式追随能力方面的表现和不同领域之间的差异",
    "en_tdlr": "FOFO is a benchmark designed to evaluate the capability of large language models to follow complex, domain-specific formats, highlighting insights on LLMs' performance in format-following and variations across different domains."
}