{
    "title": "Universal Generalization Guarantees for Wasserstein Distributionally Robust Models",
    "abstract": "arXiv:2402.11981v1 Announce Type: cross  Abstract: Distributionally robust optimization has emerged as an attractive way to train robust machine learning models, capturing data uncertainty and distribution shifts. Recent statistical analyses have proved that robust models built from Wasserstein ambiguity sets have nice generalization guarantees, breaking the curse of dimensionality. However, these results are obtained in specific cases, at the cost of approximations, or under assumptions difficult to verify in practice. In contrast, we establish, in this article, exact generalization guarantees that cover all practical cases, including any transport cost function and any loss function, potentially non-convex and nonsmooth. For instance, our result applies to deep learning, without requiring restrictive assumptions. We achieve this result through a novel proof technique that combines nonsmooth analysis rationale with classical concentration results. Our approach is general enough to ext",
    "link": "https://arxiv.org/abs/2402.11981",
    "context": "Title: Universal Generalization Guarantees for Wasserstein Distributionally Robust Models\nAbstract: arXiv:2402.11981v1 Announce Type: cross  Abstract: Distributionally robust optimization has emerged as an attractive way to train robust machine learning models, capturing data uncertainty and distribution shifts. Recent statistical analyses have proved that robust models built from Wasserstein ambiguity sets have nice generalization guarantees, breaking the curse of dimensionality. However, these results are obtained in specific cases, at the cost of approximations, or under assumptions difficult to verify in practice. In contrast, we establish, in this article, exact generalization guarantees that cover all practical cases, including any transport cost function and any loss function, potentially non-convex and nonsmooth. For instance, our result applies to deep learning, without requiring restrictive assumptions. We achieve this result through a novel proof technique that combines nonsmooth analysis rationale with classical concentration results. Our approach is general enough to ext",
    "path": "papers/24/02/2402.11981.json",
    "total_tokens": 864,
    "translated_title": "Wasserstein分布鲁棒模型的通用泛化保证",
    "translated_abstract": "分布稳健优化已经成为一种训练鲁棒机器学习模型的吸引人方式，能够捕捉数据的不确定性和分布的变化。最近的统计分析证明，基于Wasserstein模糊集构建的鲁棒模型具有很好的泛化保证，打破了维度灾难。然而，这些结果是在特定情况下获得的，以近似代价获得，或者在实践中难以验证的假设下获得的。相反，我们在本文中建立了涵盖所有实际情况的确切泛化保证，包括任何传输成本函数和任何损失函数，可能是非凸和非平滑的情况。例如，我们的结果适用于深度学习，而不需要限制性假设。我们通过一种将非平滑分析理论与经典集中结果相结合的新颖证明技术来实现这一结果。我们的方法足够通用，可以拓展至",
    "tldr": "本文建立了涵盖所有实际情况的Wasserstein分布鲁棒模型确切泛化保证，不需要限制性假设，适用于各种传输成本函数和损失函数，包括深度学习。"
}