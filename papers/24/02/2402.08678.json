{
    "title": "Graph Mamba: Towards Learning on Graphs with State Space Models",
    "abstract": "Graph Neural Networks (GNNs) have shown promising potential in graph representation learning. The majority of GNNs define a local message-passing mechanism, propagating information over the graph by stacking multiple layers. These methods, however, are known to suffer from two major limitations: over-squashing and poor capturing of long-range dependencies. Recently, Graph Transformers (GTs) emerged as a powerful alternative to Message-Passing Neural Networks (MPNNs). GTs, however, have quadratic computational cost, lack inductive biases on graph structures, and rely on complex Positional/Structural Encodings (SE/PE). In this paper, we show that while Transformers, complex message-passing, and SE/PE are sufficient for good performance in practice, neither is necessary. Motivated by the recent success of State Space Models (SSMs), such as Mamba, we present Graph Mamba Networks (GMNs), a general framework for a new class of GNNs based on selective SSMs. We discuss and categorize the new c",
    "link": "https://arxiv.org/abs/2402.08678",
    "context": "Title: Graph Mamba: Towards Learning on Graphs with State Space Models\nAbstract: Graph Neural Networks (GNNs) have shown promising potential in graph representation learning. The majority of GNNs define a local message-passing mechanism, propagating information over the graph by stacking multiple layers. These methods, however, are known to suffer from two major limitations: over-squashing and poor capturing of long-range dependencies. Recently, Graph Transformers (GTs) emerged as a powerful alternative to Message-Passing Neural Networks (MPNNs). GTs, however, have quadratic computational cost, lack inductive biases on graph structures, and rely on complex Positional/Structural Encodings (SE/PE). In this paper, we show that while Transformers, complex message-passing, and SE/PE are sufficient for good performance in practice, neither is necessary. Motivated by the recent success of State Space Models (SSMs), such as Mamba, we present Graph Mamba Networks (GMNs), a general framework for a new class of GNNs based on selective SSMs. We discuss and categorize the new c",
    "path": "papers/24/02/2402.08678.json",
    "total_tokens": 960,
    "translated_title": "图马巴：面向基于状态空间模型的图学习",
    "translated_abstract": "图神经网络（GNNs）在图表示学习方面显示出了很大的潜力。大多数GNNs定义了一种局部消息传递机制，通过堆叠多个层在图上传播信息。然而，这些方法已知存在两个主要限制：过度压缩和无法捕捉长程依赖。最近，图转换器（GTs）作为消息传递神经网络（MPNNs）的一种强大替代方法出现。然而，GTs具有二次计算成本，在图结构上缺乏归纳偏差，并且依赖复杂的位置/结构编码（SE/PE）。在本文中，我们展示了在实践中，尽管Transformer、复杂的消息传递和SE/PE对于良好性能而言是足够的，但并非必需。受到最近的状态空间模型（SSMs）（例如Mamba）的成功启发，我们提出了图马巴网络（GMNs），这是一种基于选择性SSMs的新类GNNs的通用框架。我们讨论并对新的c进行分类。",
    "tldr": "本文提出了一种基于选择性SSMs的新类GNNs框架——图马巴网络（GMNs），通过不依赖于Transformer、复杂的消息传递和位置/结构编码（SE/PE），解决了传统GNNs的过度压缩和无法捕捉长程依赖的问题。",
    "en_tdlr": "This paper presents a new framework for GNNs called Graph Mamba Networks (GMNs), based on selective SSMs, which addresses the limitations of traditional GNNs in terms of over-squashing and poor capturing of long-range dependencies, without relying on Transformers, complex message-passing, and SE/PE."
}