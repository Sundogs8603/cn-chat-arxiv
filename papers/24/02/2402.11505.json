{
    "title": "Federated Fine-tuning of Large Language Models under Heterogeneous Language Tasks and Client Resources",
    "abstract": "arXiv:2402.11505v1 Announce Type: cross  Abstract: Federated Learning (FL) has recently been applied to the parameter-efficient fine-tuning of Large Language Models (LLMs). While promising, it raises significant challenges due to the heterogeneous resources and data distributions of clients.This study introduces FlexLoRA, a simple yet effective aggregation scheme for LLM fine-tuning, which mitigates the \"buckets effect\" in traditional FL that restricts the potential of clients with ample resources by tying them to the capabilities of the least-resourced participants. FlexLoRA allows for dynamic adjustment of local LoRA ranks, fostering the development of a global model imbued with broader, less task-specific knowledge. By synthesizing a full-size LoRA weight from individual client contributions and employing Singular Value Decomposition (SVD) for weight redistribution, FlexLoRA fully leverages heterogeneous client resources. Involving over 1,600 clients performing diverse NLP tasks, ou",
    "link": "https://arxiv.org/abs/2402.11505",
    "context": "Title: Federated Fine-tuning of Large Language Models under Heterogeneous Language Tasks and Client Resources\nAbstract: arXiv:2402.11505v1 Announce Type: cross  Abstract: Federated Learning (FL) has recently been applied to the parameter-efficient fine-tuning of Large Language Models (LLMs). While promising, it raises significant challenges due to the heterogeneous resources and data distributions of clients.This study introduces FlexLoRA, a simple yet effective aggregation scheme for LLM fine-tuning, which mitigates the \"buckets effect\" in traditional FL that restricts the potential of clients with ample resources by tying them to the capabilities of the least-resourced participants. FlexLoRA allows for dynamic adjustment of local LoRA ranks, fostering the development of a global model imbued with broader, less task-specific knowledge. By synthesizing a full-size LoRA weight from individual client contributions and employing Singular Value Decomposition (SVD) for weight redistribution, FlexLoRA fully leverages heterogeneous client resources. Involving over 1,600 clients performing diverse NLP tasks, ou",
    "path": "papers/24/02/2402.11505.json",
    "total_tokens": 928,
    "translated_title": "在异构语言任务和客户资源下对大型语言模型进行联邦微调",
    "translated_abstract": "近期，联邦学习（FL）被应用于对大型语言模型（LLMs）进行参数高效微调。然而，由于客户的资源和数据分布不均匀，这引发了重大挑战。本研究引入了FlexLoRA，这是一种简单而有效的LLM微调聚合方案，它可以缓解传统FL中的“桶效应”，该效应限制了拥有丰富资源的客户实现潜力，将他们与最缺乏资源的参与者的能力捆绑在一起。FlexLoRA允许动态调整本地LoRA排名，促进全局模型的发展，并赋予更广泛、不太任务特定的知识。通过从个体客户贡献中合成完整大小的LoRA权重，并利用奇异值分解（SVD）进行权重重新分配，FlexLoRA充分利用了客户间的资源差异。本研究涉及超过1600个执行多样NLP任务的客户。",
    "tldr": "该研究引入了FlexLoRA，一个简单而有效的大型语言模型微调聚合方案，能够在联邦学习中充分利用异质客户资源，通过动态调整本地LoRA排名和采用奇异值分解进行权重重新分配，提升全局模型的广泛知识。",
    "en_tdlr": "This study introduces FlexLoRA, a simple yet effective aggregation scheme for fine-tuning Large Language Models in federated learning, which fully leverages heterogeneous client resources by dynamically adjusting local LoRA ranks and employing Singular Value Decomposition for weight redistribution, enhancing the global model with broader knowledge."
}