{
    "title": "MM-Point: Multi-View Information-Enhanced Multi-Modal Self-Supervised 3D Point Cloud Understanding",
    "abstract": "arXiv:2402.10002v1 Announce Type: cross  Abstract: In perception, multiple sensory information is integrated to map visual information from 2D views onto 3D objects, which is beneficial for understanding in 3D environments. But in terms of a single 2D view rendered from different angles, only limited partial information can be provided.The richness and value of Multi-view 2D information can provide superior self-supervised signals for 3D objects. In this paper, we propose a novel self-supervised point cloud representation learning method, MM-Point, which is driven by intra-modal and inter-modal similarity objectives. The core of MM-Point lies in the Multi-modal interaction and transmission between 3D objects and multiple 2D views at the same time. In order to more effectively simultaneously perform the consistent cross-modal objective of 2D multi-view information based on contrastive learning, we further propose Multi-MLP and Multi-level Augmentation strategies. Through carefully desig",
    "link": "https://arxiv.org/abs/2402.10002",
    "context": "Title: MM-Point: Multi-View Information-Enhanced Multi-Modal Self-Supervised 3D Point Cloud Understanding\nAbstract: arXiv:2402.10002v1 Announce Type: cross  Abstract: In perception, multiple sensory information is integrated to map visual information from 2D views onto 3D objects, which is beneficial for understanding in 3D environments. But in terms of a single 2D view rendered from different angles, only limited partial information can be provided.The richness and value of Multi-view 2D information can provide superior self-supervised signals for 3D objects. In this paper, we propose a novel self-supervised point cloud representation learning method, MM-Point, which is driven by intra-modal and inter-modal similarity objectives. The core of MM-Point lies in the Multi-modal interaction and transmission between 3D objects and multiple 2D views at the same time. In order to more effectively simultaneously perform the consistent cross-modal objective of 2D multi-view information based on contrastive learning, we further propose Multi-MLP and Multi-level Augmentation strategies. Through carefully desig",
    "path": "papers/24/02/2402.10002.json",
    "total_tokens": 930,
    "translated_title": "MM-Point: 多视角信息增强的多模态自监督三维点云理解",
    "translated_abstract": "在感知领域中，将多种传感信息整合起来将2D视图上的视觉信息映射到3D物体上，这有助于在三维环境中进行理解。但是在从不同角度渲染的单个2D视图中，只能提供有限的部分信息。多视角2D信息的丰富性和价值可以为3D物体提供优秀的自监督信号。在本文中，我们提出了一种新颖的自监督点云表示学习方法MM-Point，它受到内模态和外模态相似度目标的驱动。MM-Point的核心在于3D物体和多个2D视图之间的多模态交互和传输。为了更有效地同时执行基于对比学习的2D多视图信息一致性交叉模态目标，我们进一步提出了多层感知机(Multi-MLP)和多层级增强策略。通过精心设计的实验，我们展示了MM-Point的有效性和优越性。",
    "tldr": "本文提出了一种新颖的自监督点云表示学习方法MM-Point，通过多模态交互和传输实现了3D物体和多个2D视图之间的信息增强。通过精心设计的实验，证明了MM-Point的有效性和优越性。"
}