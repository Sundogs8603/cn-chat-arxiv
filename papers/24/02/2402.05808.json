{
    "title": "Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning",
    "abstract": "In this paper, we propose R$^3$: Learning Reasoning through Reverse Curriculum Reinforcement Learning (RL), a novel method that employs only outcome supervision to achieve the benefits of process supervision for large language models. The core challenge in applying RL to complex reasoning is to identify a sequence of actions that result in positive rewards and provide appropriate supervision for optimization. Outcome supervision provides sparse rewards for final results without identifying error locations, whereas process supervision offers step-wise rewards but requires extensive manual annotation. R$^3$ overcomes these limitations by learning from correct demonstrations. Specifically, R$^3$ progressively slides the start state of reasoning from a demonstration's end to its beginning, facilitating easier model exploration at all stages. Thus, R$^3$ establishes a step-wise curriculum, allowing outcome supervision to offer step-level signals and precisely pinpoint errors. Using Llama2-7",
    "link": "https://arxiv.org/abs/2402.05808",
    "context": "Title: Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning\nAbstract: In this paper, we propose R$^3$: Learning Reasoning through Reverse Curriculum Reinforcement Learning (RL), a novel method that employs only outcome supervision to achieve the benefits of process supervision for large language models. The core challenge in applying RL to complex reasoning is to identify a sequence of actions that result in positive rewards and provide appropriate supervision for optimization. Outcome supervision provides sparse rewards for final results without identifying error locations, whereas process supervision offers step-wise rewards but requires extensive manual annotation. R$^3$ overcomes these limitations by learning from correct demonstrations. Specifically, R$^3$ progressively slides the start state of reasoning from a demonstration's end to its beginning, facilitating easier model exploration at all stages. Thus, R$^3$ establishes a step-wise curriculum, allowing outcome supervision to offer step-level signals and precisely pinpoint errors. Using Llama2-7",
    "path": "papers/24/02/2402.05808.json",
    "total_tokens": 801,
    "translated_title": "通过反向课程强化学习训练大型语言模型进行推理",
    "translated_abstract": "在本文中，我们提出了R$^3$：通过反向课程强化学习（RL）进行推理的学习方法，该方法只使用结果监督来实现大型语言模型的过程监督的好处。将RL应用于复杂推理的核心挑战是确定一系列行动，以获得正向奖励并提供适当的优化监督。结果监督为最终结果提供了稀疏奖励，而不识别错误位置，而过程监督提供了逐步奖励，但需要大量手动注释。R$^3$通过学习正确演示来克服这些限制。具体而言，R$^3$将推理的起始状态从演示的结束滑动到开始，从而在所有阶段都促进了更容易的模型探索。因此，R$^3$建立了一个逐步的课程，使结果监督能够提供阶段级信号并精确定位错误。",
    "tldr": "本文提出了一种通过反向课程强化学习训练大型语言模型进行推理的新方法，通过学习正确演示并建立逐步的课程，实现了结果监督和过程监督的优化。"
}