{
    "title": "Divide and Conquer: Provably Unveiling the Pareto Front with Multi-Objective Reinforcement Learning",
    "abstract": "A significant challenge in multi-objective reinforcement learning is obtaining a Pareto front of policies that attain optimal performance under different preferences. We introduce Iterated Pareto Referent Optimisation (IPRO), a principled algorithm that decomposes the task of finding the Pareto front into a sequence of single-objective problems for which various solution methods exist. This enables us to establish convergence guarantees while providing an upper bound on the distance to undiscovered Pareto optimal solutions at each step. Empirical evaluations demonstrate that IPRO matches or outperforms methods that require additional domain knowledge. By leveraging problem-specific single-objective solvers, our approach also holds promise for applications beyond multi-objective reinforcement learning, such as in pathfinding and optimisation.",
    "link": "https://arxiv.org/abs/2402.07182",
    "context": "Title: Divide and Conquer: Provably Unveiling the Pareto Front with Multi-Objective Reinforcement Learning\nAbstract: A significant challenge in multi-objective reinforcement learning is obtaining a Pareto front of policies that attain optimal performance under different preferences. We introduce Iterated Pareto Referent Optimisation (IPRO), a principled algorithm that decomposes the task of finding the Pareto front into a sequence of single-objective problems for which various solution methods exist. This enables us to establish convergence guarantees while providing an upper bound on the distance to undiscovered Pareto optimal solutions at each step. Empirical evaluations demonstrate that IPRO matches or outperforms methods that require additional domain knowledge. By leveraging problem-specific single-objective solvers, our approach also holds promise for applications beyond multi-objective reinforcement learning, such as in pathfinding and optimisation.",
    "path": "papers/24/02/2402.07182.json",
    "total_tokens": 838,
    "translated_title": "分而治之：用多目标强化学习可靠地揭示帕累托前沿",
    "translated_abstract": "在多目标强化学习中，获取在不同偏好下实现最优表现的策略的帕累托前沿是一个重大挑战。我们引入了迭代帕累托参考优化（IPRO），这是一个原则性算法，它将找到帕累托前沿的任务分解成一系列具有各种解决方法的单目标问题。这使我们能够在每个步骤中建立收敛保证并提供未发现帕累托最优解的距离上限。实证评估表明，IPRO能够与需要额外领域知识的方法相匹配或优于它们。通过利用问题特定的单目标求解器，我们的方法也有望在多目标强化学习之外的应用中发挥作用，比如路径规划和优化问题。",
    "tldr": "这项研究介绍了一个名为IPRO的算法，利用分解任务为一系列单目标问题方法，可可靠地揭示多目标强化学习中实现最优表现的策略的帕累托前沿，同时提供收敛保证和未发现解的距离上限。"
}