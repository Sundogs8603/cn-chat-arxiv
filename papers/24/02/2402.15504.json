{
    "title": "Gen4Gen: Generative Data Pipeline for Generative Multi-Concept Composition",
    "abstract": "arXiv:2402.15504v1 Announce Type: cross  Abstract: Recent text-to-image diffusion models are able to learn and synthesize images containing novel, personalized concepts (e.g., their own pets or specific items) with just a few examples for training. This paper tackles two interconnected issues within this realm of personalizing text-to-image diffusion models. First, current personalization techniques fail to reliably extend to multiple concepts -- we hypothesize this to be due to the mismatch between complex scenes and simple text descriptions in the pre-training dataset (e.g., LAION). Second, given an image containing multiple personalized concepts, there lacks a holistic metric that evaluates performance on not just the degree of resemblance of personalized concepts, but also whether all concepts are present in the image and whether the image accurately reflects the overall text description. To address these issues, we introduce Gen4Gen, a semi-automated dataset creation pipeline util",
    "link": "https://arxiv.org/abs/2402.15504",
    "context": "Title: Gen4Gen: Generative Data Pipeline for Generative Multi-Concept Composition\nAbstract: arXiv:2402.15504v1 Announce Type: cross  Abstract: Recent text-to-image diffusion models are able to learn and synthesize images containing novel, personalized concepts (e.g., their own pets or specific items) with just a few examples for training. This paper tackles two interconnected issues within this realm of personalizing text-to-image diffusion models. First, current personalization techniques fail to reliably extend to multiple concepts -- we hypothesize this to be due to the mismatch between complex scenes and simple text descriptions in the pre-training dataset (e.g., LAION). Second, given an image containing multiple personalized concepts, there lacks a holistic metric that evaluates performance on not just the degree of resemblance of personalized concepts, but also whether all concepts are present in the image and whether the image accurately reflects the overall text description. To address these issues, we introduce Gen4Gen, a semi-automated dataset creation pipeline util",
    "path": "papers/24/02/2402.15504.json",
    "total_tokens": 826,
    "translated_title": "Gen4Gen: 生成式多概念组合的生成数据管道",
    "translated_abstract": "最近的文本到图像扩散模型能够学习和合成包含新颖、个性化概念的图像（例如，他们自己的宠物或特定物品），只需少量示例进行训练。本文解决了文本到图像扩散模型个性化中的两个相互关联问题。首先，当前的个性化技术未能可靠地扩展到多个概念--我们假设这是由于预训练数据集（例如，LAION）中复杂场景与简单文本描述之间的不匹配所导致的。其次，对于包含多个个性化概念的图像，缺乏一个 holistic度量标准，旨在评估不仅个性化概念的相似度程度，还有图像是否包含所有概念以及图像是否准确地反映了整体文本描述。为了解决这些问题，我们引入Gen4Gen，一个半自动化的数据集创建管道。",
    "tldr": "该论文介绍了Gen4Gen，为解决文本到图像扩散模型中个性化多概念组合的问题而提出的生成数据管道。",
    "en_tdlr": "This paper introduces Gen4Gen, a generative data pipeline designed to address the issue of generating multi-concept compositions in personalizing text-to-image diffusion models."
}