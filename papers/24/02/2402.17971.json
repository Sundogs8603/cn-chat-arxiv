{
    "title": "All in a Single Image: Large Multimodal Models are In-Image Learners",
    "abstract": "arXiv:2402.17971v1 Announce Type: cross  Abstract: This paper introduces a new in-context learning (ICL) mechanism called In-Image Learning (I$^2$L) that combines demonstration examples, visual cues, and instructions into a single image to enhance the capabilities of GPT-4V. Unlike previous approaches that rely on converting images to text or incorporating visual input into language models, I$^2$L consolidates all information into one image and primarily leverages image processing, understanding, and reasoning abilities. This has several advantages: it avoids inaccurate textual descriptions of complex images, provides flexibility in positioning demonstration examples, reduces the input burden, and avoids exceeding input limits by eliminating the need for multiple images and lengthy text. To further combine the strengths of different ICL methods, we introduce an automatic strategy to select the appropriate ICL method for a data example in a given task. We conducted experiments on MathVi",
    "link": "https://arxiv.org/abs/2402.17971",
    "context": "Title: All in a Single Image: Large Multimodal Models are In-Image Learners\nAbstract: arXiv:2402.17971v1 Announce Type: cross  Abstract: This paper introduces a new in-context learning (ICL) mechanism called In-Image Learning (I$^2$L) that combines demonstration examples, visual cues, and instructions into a single image to enhance the capabilities of GPT-4V. Unlike previous approaches that rely on converting images to text or incorporating visual input into language models, I$^2$L consolidates all information into one image and primarily leverages image processing, understanding, and reasoning abilities. This has several advantages: it avoids inaccurate textual descriptions of complex images, provides flexibility in positioning demonstration examples, reduces the input burden, and avoids exceeding input limits by eliminating the need for multiple images and lengthy text. To further combine the strengths of different ICL methods, we introduce an automatic strategy to select the appropriate ICL method for a data example in a given task. We conducted experiments on MathVi",
    "path": "papers/24/02/2402.17971.json",
    "total_tokens": 903,
    "translated_title": "一张图片搞定：大型多模态模型是图片内学习者",
    "translated_abstract": "本文介绍了一种名为图片内学习（I$^2$L）的新型上下文学习（ICL）机制，将演示示例、视觉线索和指令合并到一张图片中，以增强GPT-4V的能力。与以往依赖将图像转换为文本或将视觉输入融入语言模型的方法不同，I$^2$L将所有信息整合到一张图片中，主要利用图像处理、理解和推理能力。这有几个优点：避免了对复杂图像的不准确文本描述，提供了在定位演示示例时的灵活性，减少了输入负担，并通过消除对多个图片和冗长文本的需求来避免超过输入限制。为了进一步结合不同ICL方法的优势，我们引入了一种自动策略，用于选择给定任务中数据示例的适当ICL方法。我们在MathVi上进行了实验",
    "tldr": "这项研究引入了一种名为图片内学习（I$^2$L）的新型上下文学习机制，将演示示例、视觉线索和指令合并到一个图片中，以提升GPT-4V的能力，并通过整合图像处理、理解和推理的能力来取得多个优点",
    "en_tdlr": "This research introduces a new in-context learning mechanism called In-Image Learning (I$^2$L), which combines demonstration examples, visual cues, and instructions into a single image to enhance the capabilities of GPT-4V primarily leveraging image processing, understanding, and reasoning abilities."
}