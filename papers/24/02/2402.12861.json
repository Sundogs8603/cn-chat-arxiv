{
    "title": "Bounding Reconstruction Attack Success of Adversaries Without Data Priors",
    "abstract": "arXiv:2402.12861v1 Announce Type: new  Abstract: Reconstruction attacks on machine learning (ML) models pose a strong risk of leakage of sensitive data. In specific contexts, an adversary can (almost) perfectly reconstruct training data samples from a trained model using the model's gradients. When training ML models with differential privacy (DP), formal upper bounds on the success of such reconstruction attacks can be provided. So far, these bounds have been formulated under worst-case assumptions that might not hold high realistic practicality. In this work, we provide formal upper bounds on reconstruction success under realistic adversarial settings against ML models trained with DP and support these bounds with empirical results. With this, we show that in realistic scenarios, (a) the expected reconstruction success can be bounded appropriately in different contexts and by different metrics, which (b) allows for a more educated choice of a privacy parameter.",
    "link": "https://arxiv.org/abs/2402.12861",
    "context": "Title: Bounding Reconstruction Attack Success of Adversaries Without Data Priors\nAbstract: arXiv:2402.12861v1 Announce Type: new  Abstract: Reconstruction attacks on machine learning (ML) models pose a strong risk of leakage of sensitive data. In specific contexts, an adversary can (almost) perfectly reconstruct training data samples from a trained model using the model's gradients. When training ML models with differential privacy (DP), formal upper bounds on the success of such reconstruction attacks can be provided. So far, these bounds have been formulated under worst-case assumptions that might not hold high realistic practicality. In this work, we provide formal upper bounds on reconstruction success under realistic adversarial settings against ML models trained with DP and support these bounds with empirical results. With this, we show that in realistic scenarios, (a) the expected reconstruction success can be bounded appropriately in different contexts and by different metrics, which (b) allows for a more educated choice of a privacy parameter.",
    "path": "papers/24/02/2402.12861.json",
    "total_tokens": 828,
    "translated_title": "在没有数据先验条件下限制对抗者重建攻击成功率",
    "translated_abstract": "机器学习模型的重建攻击存在泄漏敏感数据的风险。在特定情境下，对手可以使用模型的梯度几乎完美地重建训练数据样本。在使用差分隐私（DP）训练机器学习模型时，可以提供对这种重建攻击成功率的正式上限。迄今为止，这些上限是在可能不符合高度现实实用性的最坏情况假设下制定的。在本文中，我们针对差分隐私训练的机器学习模型提供了在现实对抗设置下的重建成功率正式上限，并通过实证结果支持这些上限。通过这一点，我们展示了在现实情境中，（a）预期的重建成功率可以在不同背景和不同度量下得到适当的限制，这（b）有助于更明智地选择隐私参数。",
    "tldr": "本研究提供了差分隐私训练的机器学习模型在现实对抗设置下重建成功率的正式上限，并通过实证结果支持，有助于更明智地选择隐私参数。",
    "en_tdlr": "This work provides formal upper bounds on the reconstruction success under realistic adversarial settings against differentially private trained machine learning models, supported by empirical results, enabling a more informed choice of privacy parameters."
}