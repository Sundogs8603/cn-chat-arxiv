{
    "title": "Data Engineering for Scaling Language Models to 128K Context",
    "abstract": "arXiv:2402.10171v1 Announce Type: cross  Abstract: We study the continual pretraining recipe for scaling language models' context lengths to 128K, with a focus on data engineering. We hypothesize that long context modeling, in particular \\textit{the ability to utilize information at arbitrary input locations}, is a capability that is mostly already acquired through large-scale pretraining, and that this capability can be readily extended to contexts substantially longer than seen during training~(e.g., 4K to 128K) through lightweight continual pretraining on appropriate data mixture. We investigate the \\textit{quantity} and \\textit{quality} of the data for continual pretraining: (1) for quantity, we show that 500 million to 5 billion tokens are enough to enable the model to retrieve information anywhere within the 128K context; (2) for quality, our results equally emphasize \\textit{domain balance} and \\textit{length upsampling}. Concretely, we find that naively upsampling longer data o",
    "link": "https://arxiv.org/abs/2402.10171",
    "context": "Title: Data Engineering for Scaling Language Models to 128K Context\nAbstract: arXiv:2402.10171v1 Announce Type: cross  Abstract: We study the continual pretraining recipe for scaling language models' context lengths to 128K, with a focus on data engineering. We hypothesize that long context modeling, in particular \\textit{the ability to utilize information at arbitrary input locations}, is a capability that is mostly already acquired through large-scale pretraining, and that this capability can be readily extended to contexts substantially longer than seen during training~(e.g., 4K to 128K) through lightweight continual pretraining on appropriate data mixture. We investigate the \\textit{quantity} and \\textit{quality} of the data for continual pretraining: (1) for quantity, we show that 500 million to 5 billion tokens are enough to enable the model to retrieve information anywhere within the 128K context; (2) for quality, our results equally emphasize \\textit{domain balance} and \\textit{length upsampling}. Concretely, we find that naively upsampling longer data o",
    "path": "papers/24/02/2402.10171.json",
    "total_tokens": 931,
    "translated_title": "将语言模型扩展到128K上下文的数据工程",
    "translated_abstract": "我们研究了将语言模型的上下文长度扩展到128K的连续预训练方法，着重考虑数据工程。我们假设长上下文建模，特别是“能够利用任意输入位置的信息”的能力，在大规模预训练中已经得到了掌握，并且这种能力可以通过轻量级连续预训练在比训练时更长的上下文(例如从4K到128K)下轻松扩展。我们研究了连续预训练的数据“数量”和“质量”：(1)对于数量，我们证明5亿到50亿个标记足以使模型能够检索到128K上下文中的任何位置的信息；(2)对于质量，我们的结果同等强调“领域平衡”和“长度上采样”。具体而言，我们发现简单地上采样更长的数据，并不能提供足够的质量，而是需要注意数据的领域平衡和长度上采样。",
    "tldr": "本论文研究了将语言模型的上下文长度扩展到128K的连续预训练方法，通过适当的数据混合和轻量级的预训练可以实现，其中关键要点在于数据的数量和质量，需要注意领域平衡和长度上采样。"
}