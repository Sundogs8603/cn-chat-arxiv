{
    "title": "TinyLLaVA: A Framework of Small-scale Large Multimodal Models",
    "abstract": "arXiv:2402.14289v1 Announce Type: cross  Abstract: We present the TinyLLaVA framework that provides a unified perspective in designing and analyzing the small-scale Large Multimodal Models (LMMs). We empirically study the effects of different vision encoders, connection modules, language models, training data and training recipes. Our extensive experiments showed that better quality of data combined with better training recipes, smaller LMMs can consistently achieve on-par performances compared to bigger LMMs. Under our framework, we train a family of small-scale LMMs. Our best model, TinyLLaVA-3.1B, achieves better overall performance against existing 7B models such as LLaVA-1.5 and Qwen-VL. We hope our findings can serve as baselines for future research in terms of data scaling, training setups and model selections. Our model weights and codes will be made public.",
    "link": "https://arxiv.org/abs/2402.14289",
    "context": "Title: TinyLLaVA: A Framework of Small-scale Large Multimodal Models\nAbstract: arXiv:2402.14289v1 Announce Type: cross  Abstract: We present the TinyLLaVA framework that provides a unified perspective in designing and analyzing the small-scale Large Multimodal Models (LMMs). We empirically study the effects of different vision encoders, connection modules, language models, training data and training recipes. Our extensive experiments showed that better quality of data combined with better training recipes, smaller LMMs can consistently achieve on-par performances compared to bigger LMMs. Under our framework, we train a family of small-scale LMMs. Our best model, TinyLLaVA-3.1B, achieves better overall performance against existing 7B models such as LLaVA-1.5 and Qwen-VL. We hope our findings can serve as baselines for future research in terms of data scaling, training setups and model selections. Our model weights and codes will be made public.",
    "path": "papers/24/02/2402.14289.json",
    "total_tokens": 880,
    "translated_title": "TinyLLaVA：小规模大型多模态模型框架",
    "translated_abstract": "我们提出了TinyLLaVA框架，为设计和分析小规模大型多模态模型（LMMs）提供了统一视角。我们从实证角度研究了不同的视觉编码器、连接模块、语言模型、训练数据和训练方案的影响。我们的大量实验表明，更好质量的数据结合更好的训练方案，使得较小的LMMs能够在整体性能上与更大的LMMs保持一致。在我们的框架下，我们训练了一系列小规模LMMs。我们最佳模型TinyLLaVA-3.1B在与现有的7B模型（如LLaVA-1.5和Qwen-VL）进行比较时，达到了更好的整体性能。我们希望我们的发现可以作为未来研究在数据扩展、训练设置和模型选择方面的基准。我们的模型权重和代码将被公开。",
    "tldr": "TinyLLaVA框架使得小型多模态模型能够通过更好的数据质量和训练方案达到与大型模型相媲美的性能，最佳模型TinyLLaVA-3.1B在整体性能上优于现有的7B模型。",
    "en_tdlr": "The TinyLLaVA framework enables small-scale multimodal models to achieve comparable performance to larger models through better data quality and training recipes, with the best model TinyLLaVA-3.1B outperforming existing 7B models in overall performance."
}