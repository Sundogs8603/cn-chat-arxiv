{
    "title": "FedSiKD: Clients Similarity and Knowledge Distillation: Addressing Non-i.i.d. and Constraints in Federated Learning",
    "abstract": "arXiv:2402.09095v1 Announce Type: new Abstract: In recent years, federated learning (FL) has emerged as a promising technique for training machine learning models in a decentralized manner while also preserving data privacy. The non-independent and identically distributed (non-i.i.d.) nature of client data, coupled with constraints on client or edge devices, presents significant challenges in FL. Furthermore, learning across a high number of communication rounds can be risky and potentially unsafe for model exploitation. Traditional FL approaches may suffer from these challenges. Therefore, we introduce FedSiKD, which incorporates knowledge distillation (KD) within a similarity-based federated learning framework. As clients join the system, they securely share relevant statistics about their data distribution, promoting intra-cluster homogeneity. This enhances optimization efficiency and accelerates the learning process, effectively transferring knowledge between teacher and student mo",
    "link": "https://arxiv.org/abs/2402.09095",
    "context": "Title: FedSiKD: Clients Similarity and Knowledge Distillation: Addressing Non-i.i.d. and Constraints in Federated Learning\nAbstract: arXiv:2402.09095v1 Announce Type: new Abstract: In recent years, federated learning (FL) has emerged as a promising technique for training machine learning models in a decentralized manner while also preserving data privacy. The non-independent and identically distributed (non-i.i.d.) nature of client data, coupled with constraints on client or edge devices, presents significant challenges in FL. Furthermore, learning across a high number of communication rounds can be risky and potentially unsafe for model exploitation. Traditional FL approaches may suffer from these challenges. Therefore, we introduce FedSiKD, which incorporates knowledge distillation (KD) within a similarity-based federated learning framework. As clients join the system, they securely share relevant statistics about their data distribution, promoting intra-cluster homogeneity. This enhances optimization efficiency and accelerates the learning process, effectively transferring knowledge between teacher and student mo",
    "path": "papers/24/02/2402.09095.json",
    "total_tokens": 910,
    "translated_title": "FedSiKD: 客户相似性和知识蒸馏：解决非独立同分布和联邦学习中的约束问题",
    "translated_abstract": "近年来，联邦学习（FL）作为一种在分散方式下训练机器学习模型并保护数据隐私的有前途的技术出现了。客户数据的非独立同分布（non-i.i.d.）特性以及对客户或边缘设备的限制，在FL中提出了重大挑战。此外，跨许多通信轮次的学习可能对模型的利用具有风险和潜在的不安全性。传统FL方法可能会受到这些挑战的影响。因此，我们引入了FedSiKD，它在基于相似性的联邦学习框架中融入了知识蒸馏（KD）。当客户端加入系统时，他们安全地共享有关其数据分布的相关统计信息，促进簇内同质性。这提高了优化效率并加速了学习过程，有效地在教师和学生模型之间进行知识传输。",
    "tldr": "FedSiKD是一种结合了知识蒸馏的相似性联邦学习框架，旨在解决非独立同分布和联邦学习中的约束问题，通过促进簇内同质性来提高优化效率和加速学习过程。"
}