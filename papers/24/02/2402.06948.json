{
    "title": "Should I try multiple optimizers when fine-tuning pre-trained Transformers for NLP tasks? Should I tune their hyperparameters?",
    "abstract": "NLP research has explored different neural model architectures and sizes, datasets, training objectives, and transfer learning techniques. However, the choice of optimizer during training has not been explored as extensively. Typically, some variant of Stochastic Gradient Descent (SGD) is employed, selected among numerous variants, using unclear criteria, often with minimal or no tuning of the optimizer's hyperparameters. Experimenting with five GLUE datasets, two models (DistilBERT and DistilRoBERTa), and seven popular optimizers (SGD, SGD with Momentum, Adam, AdaMax, Nadam, AdamW, and AdaBound), we find that when the hyperparameters of the optimizers are tuned, there is no substantial difference in test performance across the five more elaborate (adaptive) optimizers, despite differences in training loss. Furthermore, tuning just the learning rate is in most cases as good as tuning all the hyperparameters. Hence, we recommend picking any of the best-behaved adaptive optimizers (e.g.,",
    "link": "https://arxiv.org/abs/2402.06948",
    "context": "Title: Should I try multiple optimizers when fine-tuning pre-trained Transformers for NLP tasks? Should I tune their hyperparameters?\nAbstract: NLP research has explored different neural model architectures and sizes, datasets, training objectives, and transfer learning techniques. However, the choice of optimizer during training has not been explored as extensively. Typically, some variant of Stochastic Gradient Descent (SGD) is employed, selected among numerous variants, using unclear criteria, often with minimal or no tuning of the optimizer's hyperparameters. Experimenting with five GLUE datasets, two models (DistilBERT and DistilRoBERTa), and seven popular optimizers (SGD, SGD with Momentum, Adam, AdaMax, Nadam, AdamW, and AdaBound), we find that when the hyperparameters of the optimizers are tuned, there is no substantial difference in test performance across the five more elaborate (adaptive) optimizers, despite differences in training loss. Furthermore, tuning just the learning rate is in most cases as good as tuning all the hyperparameters. Hence, we recommend picking any of the best-behaved adaptive optimizers (e.g.,",
    "path": "papers/24/02/2402.06948.json",
    "total_tokens": 904,
    "translated_title": "我在为NLP任务微调预训练的Transformer时是否应该尝试多个优化器？是否应该调整它们的超参数？",
    "translated_abstract": "NLP研究已经探索了不同的神经模型架构和大小、数据集、训练目标和迁移学习技术。然而，在训练过程中选择优化器并没有得到广泛探讨。通常情况下，使用随机梯度下降（SGD）的某个变种，根据不明确的标准选择，并且往往对优化器的超参数进行最小或没有调整。我们在五个GLUE数据集、两个模型（DistilBERT和DistilRoBERTa）和七个常用的优化器（SGD、带动量的SGD、Adam、AdaMax、Nadam、AdamW和AdaBound）上进行实验发现，当调整优化器的超参数时，尽管训练损失有所不同，五个更复杂的（自适应）优化器在测试性能上没有实质性的差异。此外，只调整学习率在大多数情况下与调整所有超参数的效果相当。因此，我们建议选择表现最好的任何自适应优化器（例如，",
    "tldr": "在微调预训练的Transformer进行NLP任务时，调整优化器的超参数并不会对测试性能产生实质性差异，只调整学习率通常就足够。"
}