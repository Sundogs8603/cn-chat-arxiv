{
    "title": "Depth Separations in Neural Networks: Separating the Dimension from the Accuracy",
    "abstract": "We prove an exponential separation between depth 2 and depth 3 neural networks, when approximating an $\\mathcal{O}(1)$-Lipschitz target function to constant accuracy, with respect to a distribution with support in $[0,1]^{d}$, assuming exponentially bounded weights. This addresses an open problem posed in \\citet{safran2019depth}, and proves that the curse of dimensionality manifests in depth 2 approximation, even in cases where the target function can be represented efficiently using depth 3. Previously, lower bounds that were used to separate depth 2 from depth 3 required that at least one of the Lipschitz parameter, target accuracy or (some measure of) the size of the domain of approximation scale polynomially with the input dimension, whereas we fix the former two and restrict our domain to the unit hypercube. Our lower bound holds for a wide variety of activation functions, and is based on a novel application of an average- to worst-case random self-reducibility argument, to reduce",
    "link": "https://arxiv.org/abs/2402.07248",
    "context": "Title: Depth Separations in Neural Networks: Separating the Dimension from the Accuracy\nAbstract: We prove an exponential separation between depth 2 and depth 3 neural networks, when approximating an $\\mathcal{O}(1)$-Lipschitz target function to constant accuracy, with respect to a distribution with support in $[0,1]^{d}$, assuming exponentially bounded weights. This addresses an open problem posed in \\citet{safran2019depth}, and proves that the curse of dimensionality manifests in depth 2 approximation, even in cases where the target function can be represented efficiently using depth 3. Previously, lower bounds that were used to separate depth 2 from depth 3 required that at least one of the Lipschitz parameter, target accuracy or (some measure of) the size of the domain of approximation scale polynomially with the input dimension, whereas we fix the former two and restrict our domain to the unit hypercube. Our lower bound holds for a wide variety of activation functions, and is based on a novel application of an average- to worst-case random self-reducibility argument, to reduce",
    "path": "papers/24/02/2402.07248.json",
    "total_tokens": 977,
    "translated_title": "神经网络中的深度分离：将维度与准确度分离",
    "translated_abstract": "我们证明了深度2和深度3神经网络在逼近一个$\\mathcal{O}(1)$-Lipschitz目标函数至常数精度时的指数分离，对于支持在$[0,1]^{d}$上的分布，假设权重指数有界。这解决了在\\citet{safran2019depth}中提出的一个问题，并证明了维度诅咒在深度2逼近中的存在，即使在目标函数可以使用深度3高效表示的情况下也是如此。以前，将深度2和深度3分离的下界要求至少有一个Lipschitz参数、目标准确度或逼近域的大小（某种度量）与输入维度多项式地缩放，而我们保持前两者不变，并将我们的域限制在单位超立方体上。我们的下界适用于各种激活函数，并基于一种新的平均情况到最坏情况的随机自约化论证的应用，以减少",
    "tldr": "通过研究深度2和深度3神经网络在逼近Lipschitz目标函数时的分离性质，证明了维度诅咒也会在深度2逼近中存在，即使目标函数可以使用深度3高效表示。这为以前确定深度要求的下界提供了新的观点，并且适用于多种激活函数。",
    "en_tdlr": "By studying the separability of depth 2 and depth 3 neural networks in approximating Lipschitz target functions, it is proved that the curse of dimensionality also exists in depth 2 approximation, even when the target function can be efficiently represented using depth 3. This provides a new perspective on previous lower bounds for determining depth requirements, and is applicable to various activation functions."
}