{
    "title": "Quantized Approximately Orthogonal Recurrent Neural Networks",
    "abstract": "Orthogonal recurrent neural networks (ORNNs) are an appealing option for learning tasks involving time series with long-term dependencies, thanks to their simplicity and computational stability. However, these networks often require a substantial number of parameters to perform well, which can be prohibitive in power-constrained environments, such as compact devices. One approach to address this issue is neural network quantization. The construction of such networks remains an open problem, acknowledged for its inherent instability.In this paper, we explore the quantization of the recurrent and input weight matrices in ORNNs, leading to Quantized approximately Orthogonal RNNs (QORNNs). We investigate one post-training quantization (PTQ) strategy and three quantization-aware training (QAT) algorithms that incorporate orthogonal constraints and quantized weights. Empirical results demonstrate the advantages of employing QAT over PTQ. The most efficient model achieves results similar to s",
    "link": "https://arxiv.org/abs/2402.04012",
    "context": "Title: Quantized Approximately Orthogonal Recurrent Neural Networks\nAbstract: Orthogonal recurrent neural networks (ORNNs) are an appealing option for learning tasks involving time series with long-term dependencies, thanks to their simplicity and computational stability. However, these networks often require a substantial number of parameters to perform well, which can be prohibitive in power-constrained environments, such as compact devices. One approach to address this issue is neural network quantization. The construction of such networks remains an open problem, acknowledged for its inherent instability.In this paper, we explore the quantization of the recurrent and input weight matrices in ORNNs, leading to Quantized approximately Orthogonal RNNs (QORNNs). We investigate one post-training quantization (PTQ) strategy and three quantization-aware training (QAT) algorithms that incorporate orthogonal constraints and quantized weights. Empirical results demonstrate the advantages of employing QAT over PTQ. The most efficient model achieves results similar to s",
    "path": "papers/24/02/2402.04012.json",
    "total_tokens": 913,
    "translated_title": "量化近似正交循环神经网络",
    "translated_abstract": "正交循环神经网络（ORNN）是一种吸引人的选择，用于学习涉及具有长期依赖性的时间序列的任务，由于其简单性和计算稳定性。然而，这些网络通常需要大量参数才能表现良好，这在功率受限的环境（如紧凑设备）中可能是阻碍因素。解决这个问题的一种方法是神经网络量化。构建这样的网络仍然是一个待解决的问题，其固有的不稳定性是被认可的。在本文中，我们探讨了ORNN中的循环和输入权重矩阵的量化，导致了量化近似正交RNN（QORNN）。我们研究了一种后训练量化（PTQ）策略和三种融入正交约束和量化权重的量化感知训练（QAT）算法。实证结果证明了使用QAT的优势。最高效的模型实现了与s类似的结果。",
    "tldr": "本文提出了量化近似正交循环神经网络（QORNNs）来解决正交循环神经网络（ORNNs）中参数过多的问题，采用一种后训练量化策略和三种融入正交约束和量化权重的量化感知训练算法，取得了与s相似的结果。",
    "en_tdlr": "This paper introduces Quantized Approximately Orthogonal RNNs (QORNNs) as a solution to the issue of excessive parameters in Orthogonal RNNs (ORNNs), employing one post-training quantization strategy and three quantization-aware training algorithms with orthogonal constraints and quantized weights, achieving results similar to s."
}