{
    "title": "Privacy Attacks in Decentralized Learning",
    "abstract": "arXiv:2402.10001v1 Announce Type: new  Abstract: Decentralized Gradient Descent (D-GD) allows a set of users to perform collaborative learning without sharing their data by iteratively averaging local model updates with their neighbors in a network graph. The absence of direct communication between non-neighbor nodes might lead to the belief that users cannot infer precise information about the data of others. In this work, we demonstrate the opposite, by proposing the first attack against D-GD that enables a user (or set of users) to reconstruct the private data of other users outside their immediate neighborhood. Our approach is based on a reconstruction attack against the gossip averaging protocol, which we then extend to handle the additional challenges raised by D-GD. We validate the effectiveness of our attack on real graphs and datasets, showing that the number of users compromised by a single or a handful of attackers is often surprisingly large. We empirically investigate some",
    "link": "https://arxiv.org/abs/2402.10001",
    "context": "Title: Privacy Attacks in Decentralized Learning\nAbstract: arXiv:2402.10001v1 Announce Type: new  Abstract: Decentralized Gradient Descent (D-GD) allows a set of users to perform collaborative learning without sharing their data by iteratively averaging local model updates with their neighbors in a network graph. The absence of direct communication between non-neighbor nodes might lead to the belief that users cannot infer precise information about the data of others. In this work, we demonstrate the opposite, by proposing the first attack against D-GD that enables a user (or set of users) to reconstruct the private data of other users outside their immediate neighborhood. Our approach is based on a reconstruction attack against the gossip averaging protocol, which we then extend to handle the additional challenges raised by D-GD. We validate the effectiveness of our attack on real graphs and datasets, showing that the number of users compromised by a single or a handful of attackers is often surprisingly large. We empirically investigate some",
    "path": "papers/24/02/2402.10001.json",
    "total_tokens": 845,
    "translated_title": "分布式学习中的隐私攻击",
    "translated_abstract": "分布式梯度下降（D-GD）允许一组用户在网络图中通过迭代平均本地模型更新与其邻居合作学习而无需共享数据。非邻居节点之间的直接通信的缺失可能导致用户无法推断出关于其他用户数据的精确信息。在这项工作中，我们提出了首个针对D-GD的攻击，使一个用户（或一组用户）能够重建其邻域之外其他用户的私有数据。我们的方法基于对传闻平均协议的重建攻击，然后将其扩展以处理D-GD提出的额外挑战。我们在真实图和数据集上验证了我们攻击的有效性，结果显示单个或少数攻击者所威胁到的用户数量通常是令人惊讶的大。我们对一些方案进行了经验性的研究。",
    "tldr": "该论文介绍了分布式学习中的隐私攻击，针对分布式梯度下降（D-GD）提出了首个攻击方法，能够使用户重建其邻域之外其他用户的私有数据，并验证了这种攻击的有效性。",
    "en_tdlr": "This paper presents privacy attacks in decentralized learning and proposes the first attack method against decentralized gradient descent (D-GD) that allows users to reconstruct the private data of other users outside their immediate neighborhood. The effectiveness of the attack is validated through experiments on real graphs and datasets."
}