{
    "title": "Language Writ Large: LLMs, ChatGPT, Grounding, Meaning and Understanding",
    "abstract": "Apart from what (little) OpenAI may be concealing from us, we all know (roughly) how ChatGPT works (its huge text database, its statistics, its vector representations, and their huge number of parameters, its next-word training, and so on). But none of us can say (hand on heart) that we are not surprised by what ChatGPT has proved to be able to do with these resources. This has even driven some of us to conclude that ChatGPT actually understands. It is not true that it understands. But it is also not true that we understand how it can do what it can do. I will suggest some hunches about benign biases: convergent constraints that emerge at LLM scale that may be helping ChatGPT do so much better than we would have expected. These biases are inherent in the nature of language itself, at LLM scale, and they are closely linked to what it is that ChatGPT lacks, which is direct sensorimotor grounding to connect its words to their referents and its propositions to their meanings. These converg",
    "link": "https://arxiv.org/abs/2402.02243",
    "context": "Title: Language Writ Large: LLMs, ChatGPT, Grounding, Meaning and Understanding\nAbstract: Apart from what (little) OpenAI may be concealing from us, we all know (roughly) how ChatGPT works (its huge text database, its statistics, its vector representations, and their huge number of parameters, its next-word training, and so on). But none of us can say (hand on heart) that we are not surprised by what ChatGPT has proved to be able to do with these resources. This has even driven some of us to conclude that ChatGPT actually understands. It is not true that it understands. But it is also not true that we understand how it can do what it can do. I will suggest some hunches about benign biases: convergent constraints that emerge at LLM scale that may be helping ChatGPT do so much better than we would have expected. These biases are inherent in the nature of language itself, at LLM scale, and they are closely linked to what it is that ChatGPT lacks, which is direct sensorimotor grounding to connect its words to their referents and its propositions to their meanings. These converg",
    "path": "papers/24/02/2402.02243.json",
    "total_tokens": 876,
    "translated_title": "语言扩展：LLMs，ChatGPT，接地，意义和理解",
    "translated_abstract": "除了OpenAI可能对我们隐瞒的少量信息外，我们都大致知道ChatGPT是如何工作的（它的大型文本数据库，统计数据，向量表示以及它巨大的参数数量，其下一个词的训练等）。但我们谁也不能说我们对ChatGPT的这些资源所能做到的事情不感到惊讶。这甚至让我们有人得出结论，ChatGPT实际上理解了。它并不理解，但我们也不能说我们理解它是如何做到这一点的。我将提出关于良性偏见的一些猜想：在LLM规模上出现的收敛约束可能有助于ChatGPT做得比我们预期的好得多。这些偏见是语言本身在LLM规模上固有的，并且与ChatGPT缺乏直接的感觉动作接地以将其词与其所指的对象以及其命题与其意义联系起来密切相关。",
    "tldr": "ChatGPT在LLM规模上通过利用语言本身的收敛约束来做到超出预期的表现，但并不真正理解语义以及与感觉动作的直接联系。",
    "en_tdlr": "ChatGPT achieves better-than-expected performance at LLM scale by leveraging inherent convergent constraints of language, but it does not truly understand semantics or have direct grounding in sensorimotor connections."
}