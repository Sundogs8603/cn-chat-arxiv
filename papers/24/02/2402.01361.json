{
    "title": "To the Max: Reinventing Reward in Reinforcement Learning",
    "abstract": "In reinforcement learning (RL), different rewards can define the same optimal policy but result in drastically different learning performance. For some, the agent gets stuck with a suboptimal behavior, and for others, it solves the task efficiently. Choosing a good reward function is hence an extremely important yet challenging problem. In this paper, we explore an alternative approach to using rewards for learning. We introduce max-reward RL, where an agent optimizes the maximum rather than the cumulative reward. Unlike earlier works, our approach works for deterministic and stochastic environments and can be easily combined with state-of-the-art RL algorithms. In the experiments, we study the performance of max-reward RL algorithms in two goal-reaching environments from Gymnasium-Robotics and demonstrate its benefits over standard RL. The code is publicly available.",
    "link": "https://rss.arxiv.org/abs/2402.01361",
    "context": "Title: To the Max: Reinventing Reward in Reinforcement Learning\nAbstract: In reinforcement learning (RL), different rewards can define the same optimal policy but result in drastically different learning performance. For some, the agent gets stuck with a suboptimal behavior, and for others, it solves the task efficiently. Choosing a good reward function is hence an extremely important yet challenging problem. In this paper, we explore an alternative approach to using rewards for learning. We introduce max-reward RL, where an agent optimizes the maximum rather than the cumulative reward. Unlike earlier works, our approach works for deterministic and stochastic environments and can be easily combined with state-of-the-art RL algorithms. In the experiments, we study the performance of max-reward RL algorithms in two goal-reaching environments from Gymnasium-Robotics and demonstrate its benefits over standard RL. The code is publicly available.",
    "path": "papers/24/02/2402.01361.json",
    "total_tokens": 839,
    "translated_title": "尽善尽美：重新定义强化学习中的奖励",
    "translated_abstract": "在强化学习中，不同的奖励可以定义相同的最优策略，但学习性能却会有很大差异。对于某些情况，智能体会陷入次优行为，而对于其他情况，则能高效地解决任务。选择一个好的奖励函数因此是一个非常重要但具有挑战性的问题。在本文中，我们探索了一种替代奖励用于学习的方法。我们引入了最大奖励强化学习（max-reward RL），其中智能体优化的是最大奖励而不是累积奖励。与早期的方法不同，我们的方法适用于确定性和随机环境，并且可以与最先进的强化学习算法轻松结合。在实验中，我们研究了max-reward RL算法在Gymnasium-Robotics中的两个目标达成环境中的性能，并展示其相对于标准RL的优势。代码公开可用。",
    "tldr": "本研究通过引入最大奖励强化学习的方法，提出了一种替代传统奖励函数的学习方式，并在实验中证明了其在不同环境下的性能优势。",
    "en_tdlr": "This paper proposes an alternative approach to using rewards in reinforcement learning, called max-reward RL, which optimizes the maximum reward instead of the cumulative reward. The experiments demonstrate its superior performance in different environments compared to standard RL."
}