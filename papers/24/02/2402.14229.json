{
    "title": "Sample-Efficient Linear Regression with Self-Selection Bias",
    "abstract": "arXiv:2402.14229v1 Announce Type: cross  Abstract: We consider the problem of linear regression with self-selection bias in the unknown-index setting, as introduced in recent work by Cherapanamjeri, Daskalakis, Ilyas, and Zampetakis [STOC 2023]. In this model, one observes $m$ i.i.d. samples $(\\mathbf{x}_{\\ell},z_{\\ell})_{\\ell=1}^m$ where $z_{\\ell}=\\max_{i\\in [k]}\\{\\mathbf{x}_{\\ell}^T\\mathbf{w}_i+\\eta_{i,\\ell}\\}$, but the maximizing index $i_{\\ell}$ is unobserved. Here, the $\\mathbf{x}_{\\ell}$ are assumed to be $\\mathcal{N}(0,I_n)$ and the noise distribution $\\mathbf{\\eta}_{\\ell}\\sim \\mathcal{D}$ is centered and independent of $\\mathbf{x}_{\\ell}$. We provide a novel and near optimally sample-efficient (in terms of $k$) algorithm to recover $\\mathbf{w}_1,\\ldots,\\mathbf{w}_k\\in \\mathbb{R}^n$ up to additive $\\ell_2$-error $\\varepsilon$ with polynomial sample complexity $\\tilde{O}(n)\\cdot \\mathsf{poly}(k,1/\\varepsilon)$ and significantly improved time complexity $\\mathsf{poly}(n,k,1/\\varep",
    "link": "https://arxiv.org/abs/2402.14229",
    "context": "Title: Sample-Efficient Linear Regression with Self-Selection Bias\nAbstract: arXiv:2402.14229v1 Announce Type: cross  Abstract: We consider the problem of linear regression with self-selection bias in the unknown-index setting, as introduced in recent work by Cherapanamjeri, Daskalakis, Ilyas, and Zampetakis [STOC 2023]. In this model, one observes $m$ i.i.d. samples $(\\mathbf{x}_{\\ell},z_{\\ell})_{\\ell=1}^m$ where $z_{\\ell}=\\max_{i\\in [k]}\\{\\mathbf{x}_{\\ell}^T\\mathbf{w}_i+\\eta_{i,\\ell}\\}$, but the maximizing index $i_{\\ell}$ is unobserved. Here, the $\\mathbf{x}_{\\ell}$ are assumed to be $\\mathcal{N}(0,I_n)$ and the noise distribution $\\mathbf{\\eta}_{\\ell}\\sim \\mathcal{D}$ is centered and independent of $\\mathbf{x}_{\\ell}$. We provide a novel and near optimally sample-efficient (in terms of $k$) algorithm to recover $\\mathbf{w}_1,\\ldots,\\mathbf{w}_k\\in \\mathbb{R}^n$ up to additive $\\ell_2$-error $\\varepsilon$ with polynomial sample complexity $\\tilde{O}(n)\\cdot \\mathsf{poly}(k,1/\\varepsilon)$ and significantly improved time complexity $\\mathsf{poly}(n,k,1/\\varep",
    "path": "papers/24/02/2402.14229.json",
    "total_tokens": 1113,
    "translated_title": "具有自我选择偏差的高效线性回归",
    "translated_abstract": "我们考虑在未知指数设定中具有自我选择偏差的线性回归问题，该问题最近由Cherapanamjeri、Daskalakis、Ilyas和Zampetakis[STOC 2023]的研究引入。在这个模型中，观察到$m$个i.i.d.样本$(\\mathbf{x}_{\\ell},z_{\\ell})_{\\ell=1}^m$，其中$z_{\\ell}=\\max_{i\\in [k]}\\{\\mathbf{x}_{\\ell}^T\\mathbf{w}_i+\\eta_{i,\\ell}\\}$，但最大化指数$i_{\\ell}$是不可观测的。这里，$\\mathbf{x}_{\\ell}$被假设为$\\mathcal{N}(0,I_n)$，噪声分布$\\mathbf{\\eta}_{\\ell}\\sim \\mathcal{D}$是以$\\mathbf{x}_{\\ell}$为中心独立的。我们提供了一种新颖的、接近最优的样本高效（以$k$为度量）算法，用于恢复$\\mathbf{w}_1,\\ldots,\\mathbf{w}_k\\in \\mathbb{R}^n$，其$\\ell_2$-误差为$\\varepsilon$，具有多项式样本复杂度$\\tilde{O}(n)\\cdot \\mathsf{poly}(k,1/\\varepsilon)$和显著改善的时间复杂度$\\mathsf{poly}(n,k,1/\\varepsilon)$。",
    "tldr": "提出了一种新颖且接近最优的样本高效算法，可在未知指数设定下的具有自我选择偏差的线性回归问题中高效地恢复参数向量，具有显著优化的时间复杂度和多项式样本复杂度。"
}