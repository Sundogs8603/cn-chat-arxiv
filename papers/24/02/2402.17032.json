{
    "title": "REFACTOR: Learning to Extract Theorems from Proofs",
    "abstract": "arXiv:2402.17032v1 Announce Type: new  Abstract: Human mathematicians are often good at recognizing modular and reusable theorems that make complex mathematical results within reach. In this paper, we propose a novel method called theoREm-from-prooF extrACTOR (REFACTOR) for training neural networks to mimic this ability in formal mathematical theorem proving. We show on a set of unseen proofs, REFACTOR is able to extract 19.6% of the theorems that humans would use to write the proofs. When applying the model to the existing Metamath library, REFACTOR extracted 16 new theorems. With newly extracted theorems, we show that the existing proofs in the MetaMath database can be refactored. The new theorems are used very frequently after refactoring, with an average usage of 733.5 times, and help shorten the proof lengths. Lastly, we demonstrate that the prover trained on the new-theorem refactored dataset proves more test theorems and outperforms state-of-the-art baselines by frequently lever",
    "link": "https://arxiv.org/abs/2402.17032",
    "context": "Title: REFACTOR: Learning to Extract Theorems from Proofs\nAbstract: arXiv:2402.17032v1 Announce Type: new  Abstract: Human mathematicians are often good at recognizing modular and reusable theorems that make complex mathematical results within reach. In this paper, we propose a novel method called theoREm-from-prooF extrACTOR (REFACTOR) for training neural networks to mimic this ability in formal mathematical theorem proving. We show on a set of unseen proofs, REFACTOR is able to extract 19.6% of the theorems that humans would use to write the proofs. When applying the model to the existing Metamath library, REFACTOR extracted 16 new theorems. With newly extracted theorems, we show that the existing proofs in the MetaMath database can be refactored. The new theorems are used very frequently after refactoring, with an average usage of 733.5 times, and help shorten the proof lengths. Lastly, we demonstrate that the prover trained on the new-theorem refactored dataset proves more test theorems and outperforms state-of-the-art baselines by frequently lever",
    "path": "papers/24/02/2402.17032.json",
    "total_tokens": 888,
    "translated_title": "从证明中提取定理的学习",
    "translated_abstract": "人类数学家通常擅长识别模块化和可重用的定理，这些定理使复杂的数学结果易于获得。在本文中，我们提出了一个名为“定理从证明中提取器（REFACTOR）”的新方法，用于训练神经网络模仿这种形式数学定理证明的能力。我们展示了在一组未见证明上，REFACTOR能够提取出人类在写证明时会使用的19.6%的定理。当将模型应用于现有的Metamath库时，REFACTOR提取出了16个新定理。通过新提取的定理，我们展示了MetaMath数据库中的现有证明可以被重构。经重构后，这些新定理被非常频繁地使用，平均使用次数为733.5次，并有助于缩短证明长度。最后，我们展示了在经过新定理重构的数据集上训练的证明者证明了更多测试定理，并通过频繁利用超越了最先进的基准模型。",
    "tldr": "提出了一种名为REFACTOR的新方法，用于训练神经网络从证明中提取定理，新定理的引入帮助缩短证明长度并提高证明效率。"
}