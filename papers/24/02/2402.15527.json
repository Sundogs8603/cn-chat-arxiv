{
    "title": "PCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain",
    "abstract": "arXiv:2402.15527v1 Announce Type: cross  Abstract: We present PCA-Bench, a multimodal decision-making benchmark for evaluating the integrated capabilities of Multimodal Large Language Models (MLLMs). Departing from previous benchmarks focusing on simplistic tasks and individual model capability, PCA-Bench introduces three complex scenarios: autonomous driving, domestic robotics, and open-world games. Given task instructions and diverse contexts, the model is required to seamlessly integrate multiple capabilities of Perception, Cognition, and Action in a reasoning chain to make accurate decisions. Moreover, PCA-Bench features error localization capabilities, scrutinizing model inaccuracies in areas such as perception, knowledge, or reasoning. This enhances the reliability of deploying MLLMs. To balance accuracy and efficiency in evaluation, we propose PCA-Eval, an automatic evaluation protocol, and assess 10 prevalent MLLMs. The results reveal significant performance disparities between",
    "link": "https://arxiv.org/abs/2402.15527",
    "context": "Title: PCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain\nAbstract: arXiv:2402.15527v1 Announce Type: cross  Abstract: We present PCA-Bench, a multimodal decision-making benchmark for evaluating the integrated capabilities of Multimodal Large Language Models (MLLMs). Departing from previous benchmarks focusing on simplistic tasks and individual model capability, PCA-Bench introduces three complex scenarios: autonomous driving, domestic robotics, and open-world games. Given task instructions and diverse contexts, the model is required to seamlessly integrate multiple capabilities of Perception, Cognition, and Action in a reasoning chain to make accurate decisions. Moreover, PCA-Bench features error localization capabilities, scrutinizing model inaccuracies in areas such as perception, knowledge, or reasoning. This enhances the reliability of deploying MLLMs. To balance accuracy and efficiency in evaluation, we propose PCA-Eval, an automatic evaluation protocol, and assess 10 prevalent MLLMs. The results reveal significant performance disparities between",
    "path": "papers/24/02/2402.15527.json",
    "total_tokens": 884,
    "translated_title": "PCA-Bench: 评估多模大型语言模型在感知-认知-行动链中的表现",
    "translated_abstract": "我们提出了PCA-Bench，这是一个用于评估多模大型语言模型（MLLMs）综合能力的多模决策基准。与之前专注于简单任务和单个模型能力的基准不同，PCA-Bench引入了三个复杂场景：自动驾驶、家庭机器人和开放世界游戏。在给定任务指令和多样化上下文的情况下，模型需要无缝整合感知、认知和行动的多重能力，以进行推理链以做出准确决定。此外，PCA-Bench具有错误定位能力，审查模型在感知、知识或推理等领域的不准确性。这提高了部署MLLMs的可靠性。为了在评估中平衡准确性和效率，我们提出了PCA-Eval，一种自动评估协议，并评估了10种流行的MLLMs。结果显示了显著的性能差异。",
    "tldr": "PCA-Bench 提出了一个评估多模大型语言模型综合能力的基准，引入复杂场景和错误定位能力，提高部署可靠性，并提出了自动评估协议 PCA-Eval，发现了显著的性能差异。"
}