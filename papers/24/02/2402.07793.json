{
    "title": "Tuning-Free Stochastic Optimization",
    "abstract": "Large-scale machine learning problems make the cost of hyperparameter tuning ever more prohibitive. This creates a need for algorithms that can tune themselves on-the-fly. We formalize the notion of \"tuning-free\" algorithms that can match the performance of optimally-tuned optimization algorithms up to polylogarithmic factors given only loose hints on the relevant problem parameters. We consider in particular algorithms that can match optimally-tuned Stochastic Gradient Descent (SGD). When the domain of optimization is bounded, we show tuning-free matching of SGD is possible and achieved by several existing algorithms. We prove that for the task of minimizing a convex and smooth or Lipschitz function over an unbounded domain, tuning-free optimization is impossible. We discuss conditions under which tuning-free optimization is possible even over unbounded domains. In particular, we show that the recently proposed DoG and DoWG algorithms are tuning-free when the noise distribution is suf",
    "link": "https://arxiv.org/abs/2402.07793",
    "context": "Title: Tuning-Free Stochastic Optimization\nAbstract: Large-scale machine learning problems make the cost of hyperparameter tuning ever more prohibitive. This creates a need for algorithms that can tune themselves on-the-fly. We formalize the notion of \"tuning-free\" algorithms that can match the performance of optimally-tuned optimization algorithms up to polylogarithmic factors given only loose hints on the relevant problem parameters. We consider in particular algorithms that can match optimally-tuned Stochastic Gradient Descent (SGD). When the domain of optimization is bounded, we show tuning-free matching of SGD is possible and achieved by several existing algorithms. We prove that for the task of minimizing a convex and smooth or Lipschitz function over an unbounded domain, tuning-free optimization is impossible. We discuss conditions under which tuning-free optimization is possible even over unbounded domains. In particular, we show that the recently proposed DoG and DoWG algorithms are tuning-free when the noise distribution is suf",
    "path": "papers/24/02/2402.07793.json",
    "total_tokens": 837,
    "translated_title": "无调参的随机优化",
    "translated_abstract": "大规模机器学习问题使得调参的成本越来越高昂。这导致了需要能够即时自我调整的算法的需求。我们将“无调参”算法的概念形式化，即只给出问题参数的粗略提示即可与最优调参优化算法的性能相匹配，误差为对数多项式因子。我们特别考虑能够与最优调参的随机梯度下降(SGD)相匹配的算法。当优化的域是有界的时候，我们证明了调参自由与SGD的匹配是可能的，并且通过几个现有算法实现了这一点。我们证明了当优化的域是无界的时候，对于最小化凸平滑或者Lipschitz函数的任务，无调参优化是不可能的。我们讨论了在无界域中，何种情况下可以实现无调参优化。特别地，我们展示了最近提出的 DoG 和 DoWG 算法在噪声分布足够时是无调参的。",
    "tldr": "本文提出了一种无调参的随机优化算法，能够在只给出问题参数的粗略提示的情况下，与最优调参优化算法的性能相匹配。并且在有界的优化领域中证明了此算法的可行性，并探讨了在无界域中的条件。"
}