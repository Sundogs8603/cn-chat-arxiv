{
    "title": "SemPool: Simple, robust, and interpretable KG pooling for enhancing language models",
    "abstract": "Knowledge Graph (KG) powered question answering (QA) performs complex reasoning over language semantics as well as knowledge facts. Graph Neural Networks (GNNs) learn to aggregate information from the underlying KG, which is combined with Language Models (LMs) for effective reasoning with the given question. However, GNN-based methods for QA rely on the graph information of the candidate answer nodes, which limits their effectiveness in more challenging settings where critical answer information is not included in the KG. We propose a simple graph pooling approach that learns useful semantics of the KG that can aid the LM's reasoning and that its effectiveness is robust under graph perturbations. Our method, termed SemPool, represents KG facts with pre-trained LMs, learns to aggregate their semantic information, and fuses it at different layers of the LM. Our experimental results show that SemPool outperforms state-of-the-art GNN-based methods by 2.27% accuracy points on average when a",
    "link": "https://arxiv.org/abs/2402.02289",
    "context": "Title: SemPool: Simple, robust, and interpretable KG pooling for enhancing language models\nAbstract: Knowledge Graph (KG) powered question answering (QA) performs complex reasoning over language semantics as well as knowledge facts. Graph Neural Networks (GNNs) learn to aggregate information from the underlying KG, which is combined with Language Models (LMs) for effective reasoning with the given question. However, GNN-based methods for QA rely on the graph information of the candidate answer nodes, which limits their effectiveness in more challenging settings where critical answer information is not included in the KG. We propose a simple graph pooling approach that learns useful semantics of the KG that can aid the LM's reasoning and that its effectiveness is robust under graph perturbations. Our method, termed SemPool, represents KG facts with pre-trained LMs, learns to aggregate their semantic information, and fuses it at different layers of the LM. Our experimental results show that SemPool outperforms state-of-the-art GNN-based methods by 2.27% accuracy points on average when a",
    "path": "papers/24/02/2402.02289.json",
    "total_tokens": 896,
    "translated_title": "SemPool：简单、稳健、可解释的知识图加强语言模型的汇聚方法",
    "translated_abstract": "知识图（KG）驱动的问答系统在语义和知识事实上执行复杂的推理。图神经网络（GNNs）学习从底层KG中聚合信息，并与语言模型（LMs）结合，以有效地推理给定的问题。然而，基于GNN的问答方法依赖于候选答案节点的图形信息，在更具挑战性的环境中，其中关键答案信息未包含在KG中，限制了其有效性。我们提出了一种简单的图形汇聚方法，学习可以辅助LM推理的KG语义信息，并且在图形扰动下具有稳健的效果。我们的方法称为SemPool，使用预训练的LMs代表KG事实，学习聚合它们的语义信息，并在LM的不同层次上融合它们。我们的实验结果表明，当使用Avazu数据集时，SemPool相比最先进的基于GNN的方法平均提高了2.27%的准确率。",
    "tldr": "SemPool提出了一种简单、稳健、可解释的知识图加强语言模型的汇聚方法，通过学习和融合KG的语义信息，能够在更具挑战性的环境下提高问答系统的准确率。"
}