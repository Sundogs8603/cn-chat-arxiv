{
    "title": "Flashback: Understanding and Mitigating Forgetting in Federated Learning",
    "abstract": "In Federated Learning (FL), forgetting, or the loss of knowledge across rounds, hampers algorithm convergence, particularly in the presence of severe data heterogeneity among clients. This study explores the nuances of this issue, emphasizing the critical role of forgetting in FL's inefficient learning within heterogeneous data contexts. Knowledge loss occurs in both client-local updates and server-side aggregation steps; addressing one without the other fails to mitigate forgetting. We introduce a metric to measure forgetting granularly, ensuring distinct recognition amid new knowledge acquisition. Leveraging these insights, we propose Flashback, an FL algorithm with a dynamic distillation approach that is used to regularize the local models, and effectively aggregate their knowledge. Across different benchmarks, Flashback outperforms other methods, mitigates forgetting, and achieves faster round-to-target-accuracy, by converging in 6 to 16 rounds.",
    "link": "https://arxiv.org/abs/2402.05558",
    "context": "Title: Flashback: Understanding and Mitigating Forgetting in Federated Learning\nAbstract: In Federated Learning (FL), forgetting, or the loss of knowledge across rounds, hampers algorithm convergence, particularly in the presence of severe data heterogeneity among clients. This study explores the nuances of this issue, emphasizing the critical role of forgetting in FL's inefficient learning within heterogeneous data contexts. Knowledge loss occurs in both client-local updates and server-side aggregation steps; addressing one without the other fails to mitigate forgetting. We introduce a metric to measure forgetting granularly, ensuring distinct recognition amid new knowledge acquisition. Leveraging these insights, we propose Flashback, an FL algorithm with a dynamic distillation approach that is used to regularize the local models, and effectively aggregate their knowledge. Across different benchmarks, Flashback outperforms other methods, mitigates forgetting, and achieves faster round-to-target-accuracy, by converging in 6 to 16 rounds.",
    "path": "papers/24/02/2402.05558.json",
    "total_tokens": 914,
    "translated_title": "闪回：理解和减轻联邦学习中的遗忘问题",
    "translated_abstract": "在联邦学习中，遗忘或者说在不同轮次中的知识丢失阻碍了算法的收敛，尤其在客户端之间存在严重的数据异质性的情况下更为明显。本研究探究了这个问题的复杂性，强调了遗忘在异质数据环境中对联邦学习的低效学习起到的关键作用。知识丢失既发生在客户端局部更新中，也发生在服务器端的聚合步骤中；只解决其中一个而忽略另一个无法有效减轻遗忘问题。我们引入了一个度量遗忘的指标，以确保在新知识获取中明确识别遗忘。借助这些洞察，我们提出了一种名为\"闪回\"的联邦学习算法，该算法使用动态蒸馏方法来规范化局部模型，并有效地聚合它们的知识。在不同的基准测试中，\"闪回\"优于其他方法，减轻了遗忘问题，并在6到16个轮次内达到更快的目标准确度。",
    "tldr": "本研究深入探讨了联邦学习中的遗忘问题，强调了遗忘在异质数据环境中的关键性质，提出了\"闪回\"算法来解决遗忘问题并取得优异的学习结果。",
    "en_tdlr": "This study explores the issue of forgetting in Federated Learning, emphasizing its importance in heterogeneous data contexts. The proposed Flashback algorithm effectively mitigates forgetting and achieves superior learning results."
}