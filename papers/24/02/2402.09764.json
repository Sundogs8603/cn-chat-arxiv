{
    "title": "Aligning Crowd Feedback via Distributional Preference Reward Modeling",
    "abstract": "arXiv:2402.09764v1 Announce Type: new  Abstract: Deep Reinforcement Learning is widely used for aligning Large Language Models (LLM) with human preference. However, the conventional reward modelling has predominantly depended on human annotations provided by a select cohort of individuals. Such dependence may unintentionally result in models that are skewed to reflect the inclinations of these annotators, thereby failing to represent the expectations of the wider population adequately. In this paper, we introduce the Distributional Preference Reward Model (DPRM), a simple yet effective framework to align large language models with a diverse set of human preferences. To this end, we characterize the preferences by a beta distribution, which can dynamically adapt to fluctuations in preference trends. On top of that, we design an optimal-transportation-based loss to calibrate DPRM to align with the preference distribution. Finally, the expected reward is utilized to fine-tune an LLM polic",
    "link": "https://arxiv.org/abs/2402.09764",
    "context": "Title: Aligning Crowd Feedback via Distributional Preference Reward Modeling\nAbstract: arXiv:2402.09764v1 Announce Type: new  Abstract: Deep Reinforcement Learning is widely used for aligning Large Language Models (LLM) with human preference. However, the conventional reward modelling has predominantly depended on human annotations provided by a select cohort of individuals. Such dependence may unintentionally result in models that are skewed to reflect the inclinations of these annotators, thereby failing to represent the expectations of the wider population adequately. In this paper, we introduce the Distributional Preference Reward Model (DPRM), a simple yet effective framework to align large language models with a diverse set of human preferences. To this end, we characterize the preferences by a beta distribution, which can dynamically adapt to fluctuations in preference trends. On top of that, we design an optimal-transportation-based loss to calibrate DPRM to align with the preference distribution. Finally, the expected reward is utilized to fine-tune an LLM polic",
    "path": "papers/24/02/2402.09764.json",
    "total_tokens": 924,
    "translated_title": "通过分布偏好奖励建模对齐众包反馈",
    "translated_abstract": "深度强化学习广泛用于将大型语言模型与人类偏好对齐。然而，传统的奖励建模主要依赖于一组个体提供的人类标注。这种依赖可能会导致模型倾向于反映这些标注者的倾向，从而未能充分代表更广泛人群的期望。本文介绍了一种简单而有效的框架——分布偏好奖励模型(DPRM)，以将大型语言模型与多样的人类偏好对齐。为此，我们使用贝塔分布来刻画偏好，该分布能够动态适应偏好趋势的波动。在此基础上，我们设计了基于最优输运的损失函数，以校准DPRM与偏好分布的对齐度。最后，利用期望奖励来微调语言模型的策略。",
    "tldr": "本文提出了一种名为分布偏好奖励模型的框架，用于将大型语言模型与多样的人类偏好对齐。该框架使用贝塔分布刻画偏好，并设计了基于最优输运的损失函数来校准模型与偏好的对齐程度。最终利用期望奖励微调语言模型的策略。",
    "en_tdlr": "We propose a framework called Distributional Preference Reward Model (DPRM) to align large language models with diverse human preferences. This framework characterizes preferences using a beta distribution and incorporates an optimal-transportation-based loss to calibrate the alignment. The language model is then fine-tuned using expected rewards."
}