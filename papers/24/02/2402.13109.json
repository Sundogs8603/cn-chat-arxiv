{
    "title": "CIF-Bench: A Chinese Instruction-Following Benchmark for Evaluating the Generalizability of Large Language Models",
    "abstract": "arXiv:2402.13109v1 Announce Type: cross  Abstract: The advancement of large language models (LLMs) has enhanced the ability to generalize across a wide range of unseen natural language processing (NLP) tasks through instruction-following. Yet, their effectiveness often diminishes in low-resource languages like Chinese, exacerbated by biased evaluations from data leakage, casting doubt on their true generalizability to new linguistic territories. In response, we introduce the Chinese Instruction-Following Benchmark (CIF-Bench), designed to evaluate the zero-shot generalizability of LLMs to the Chinese language. CIF-Bench comprises 150 tasks and 15,000 input-output pairs, developed by native speakers to test complex reasoning and Chinese cultural nuances across 20 categories. To mitigate evaluation bias, we release only half of the dataset publicly, with the remainder kept private, and introduce diversified instructions to minimize score variance, totaling 45,000 data instances. Our eval",
    "link": "https://arxiv.org/abs/2402.13109",
    "context": "Title: CIF-Bench: A Chinese Instruction-Following Benchmark for Evaluating the Generalizability of Large Language Models\nAbstract: arXiv:2402.13109v1 Announce Type: cross  Abstract: The advancement of large language models (LLMs) has enhanced the ability to generalize across a wide range of unseen natural language processing (NLP) tasks through instruction-following. Yet, their effectiveness often diminishes in low-resource languages like Chinese, exacerbated by biased evaluations from data leakage, casting doubt on their true generalizability to new linguistic territories. In response, we introduce the Chinese Instruction-Following Benchmark (CIF-Bench), designed to evaluate the zero-shot generalizability of LLMs to the Chinese language. CIF-Bench comprises 150 tasks and 15,000 input-output pairs, developed by native speakers to test complex reasoning and Chinese cultural nuances across 20 categories. To mitigate evaluation bias, we release only half of the dataset publicly, with the remainder kept private, and introduce diversified instructions to minimize score variance, totaling 45,000 data instances. Our eval",
    "path": "papers/24/02/2402.13109.json",
    "total_tokens": 891,
    "translated_title": "CIF-Bench：用于评估大型语言模型泛化能力的中文指令遵循基准",
    "translated_abstract": "大型语言模型（LLMs）的进步增强了通过指令遵循在广泛范围的未见自然语言处理（NLP）任务上的泛化能力。然而，它们在如中文这样的低资源语言中的有效性常常会减弱，受到数据泄漏引起的偏见评估的影响，这使人对它们真正的泛化能力到新语言领域产生了怀疑。为了应对这一问题，我们引入了中文指令遵循基准（CIF-Bench），旨在评估LLMs对中文语言的零样本泛化能力。CIF-Bench 包含150个任务和15,000个输入输出对，由母语者开发，用于测试跨越20个类别的复杂推理和中国文化细微差别。为了减少评估偏见，我们只公开了数据集的一半，其余部分保持私密，并引入多样化的指令以最小化得分方差，共计45,000个数据实例。",
    "tldr": "CIF-Bench是一个用于评估大型语言模型在中文语言上零样本泛化能力的基准，通过多样化的指令和数据集划分来减少评估偏见。",
    "en_tdlr": "CIF-Bench is a benchmark for evaluating the zero-shot generalizability of large language models to the Chinese language, mitigating evaluation bias through diversified instructions and dataset partitioning."
}