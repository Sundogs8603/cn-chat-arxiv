{
    "title": "The Hidden Space of Transformer Language Adapters",
    "abstract": "arXiv:2402.13137v1 Announce Type: new  Abstract: We analyze the operation of transformer language adapters, which are small modules trained on top of a frozen language model to adapt its predictions to new target languages. We show that adapted predictions mostly evolve in the source language the model was trained on, while the target language becomes pronounced only in the very last layers of the model. Moreover, the adaptation process is gradual and distributed across layers, where it is possible to skip small groups of adapters without decreasing adaptation performance. Last, we show that adapters operate on top of the model's frozen representation space while largely preserving its structure, rather than on an 'isolated' subspace. Our findings provide a deeper view into the adaptation process of language models to new languages, showcasing the constraints imposed on it by the underlying model and introduces practical implications to enhance its efficiency.",
    "link": "https://arxiv.org/abs/2402.13137",
    "context": "Title: The Hidden Space of Transformer Language Adapters\nAbstract: arXiv:2402.13137v1 Announce Type: new  Abstract: We analyze the operation of transformer language adapters, which are small modules trained on top of a frozen language model to adapt its predictions to new target languages. We show that adapted predictions mostly evolve in the source language the model was trained on, while the target language becomes pronounced only in the very last layers of the model. Moreover, the adaptation process is gradual and distributed across layers, where it is possible to skip small groups of adapters without decreasing adaptation performance. Last, we show that adapters operate on top of the model's frozen representation space while largely preserving its structure, rather than on an 'isolated' subspace. Our findings provide a deeper view into the adaptation process of language models to new languages, showcasing the constraints imposed on it by the underlying model and introduces practical implications to enhance its efficiency.",
    "path": "papers/24/02/2402.13137.json",
    "total_tokens": 723,
    "translated_title": "Transformer语言适配器的隐藏空间",
    "translated_abstract": "我们分析了transformer语言适配器的操作方式，这些小模块在冻结的语言模型之上训练，以将其预测适应到新的目标语言。我们展示了适应后的预测主要在模型训练的源语言中演变，而目标语言仅在模型的最后几层中变得明显。此外，适应过程是渐进的，分布在多个层中，可以跳过少量适配器组而不降低适应性能。最后，我们发现适配器在模型的冻结表示空间上操作，同时在很大程度上保留其结构，而不是在“孤立”的子空间上。",
    "tldr": "Transformer语言适配器在冻结的表示空间上操作，适应过程渐进分布在多层中，并且大部分预测仍在源语言中进行演变。",
    "en_tdlr": "Transformer language adapters operate in the frozen representation space, with adaptation being gradual and distributed across multiple layers, while most predictions still evolve in the source language."
}