{
    "title": "How do Large Language Models Handle Multilingualism?",
    "abstract": "arXiv:2402.18815v1 Announce Type: cross  Abstract: Large language models (LLMs) demonstrate remarkable performance across a spectrum of languages. In this work, we delve into the question: How do LLMs handle multilingualism? We introduce a framework that depicts LLMs' processing of multilingual inputs: In the first several layers, LLMs understand the question, converting multilingual inputs into English to facilitate the task-solving phase. In the intermediate layers, LLMs engage in problem-solving by thinking in English and incorporating multilingual knowledge to obtain factual content, leveraging the self-attention and feed-forward structures, respectively. In the last several layers, LLMs generate responses that align with the original language of the query. In addition, we investigate the existence of language-specific neurons when processing a certain language. To detect neurons activated by the input language, even without labels, we innovatively design a Parallel Language specif",
    "link": "https://arxiv.org/abs/2402.18815",
    "context": "Title: How do Large Language Models Handle Multilingualism?\nAbstract: arXiv:2402.18815v1 Announce Type: cross  Abstract: Large language models (LLMs) demonstrate remarkable performance across a spectrum of languages. In this work, we delve into the question: How do LLMs handle multilingualism? We introduce a framework that depicts LLMs' processing of multilingual inputs: In the first several layers, LLMs understand the question, converting multilingual inputs into English to facilitate the task-solving phase. In the intermediate layers, LLMs engage in problem-solving by thinking in English and incorporating multilingual knowledge to obtain factual content, leveraging the self-attention and feed-forward structures, respectively. In the last several layers, LLMs generate responses that align with the original language of the query. In addition, we investigate the existence of language-specific neurons when processing a certain language. To detect neurons activated by the input language, even without labels, we innovatively design a Parallel Language specif",
    "path": "papers/24/02/2402.18815.json",
    "total_tokens": 807,
    "translated_title": "大型语言模型如何处理多语言？",
    "translated_abstract": "大型语言模型（LLMs）展现出在各种语言上出色的性能。本文探讨了一个问题：大型语言模型如何处理多语言？我们引入了一个框架，描述了LLMs处理多语言输入的过程：在前几层中，LLMs理解问题，将多语言输入转换为英语以便促进任务解决阶段。在中间层中，LLMs通过以英语思考并整合多语言知识来进行解决问题，利用自注意力和前馈结构，分别获取事实内容。在最后几层中，LLMs生成与查询的原始语言一致的响应。此外，我们研究了处理特定语言时特定语言神经元的存在。为了检测由输入语言激活的神经元，即使没有标签，我们创新性地设计了一个并行语言特定的",
    "tldr": "大型语言模型展示了处理多语言任务的出色性能，研究发现在不同层次中处理多语言输入的策略，以及处理特定语言时的语言特定神经元存在。",
    "en_tdlr": "Large language models demonstrate excellent performance in handling multilingual tasks, revealing strategies for processing multilingual inputs at different layers and the existence of language-specific neurons when dealing with specific languages."
}