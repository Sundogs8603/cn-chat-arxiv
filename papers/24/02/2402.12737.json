{
    "title": "Guarantee Regions for Local Explanations",
    "abstract": "arXiv:2402.12737v1 Announce Type: new  Abstract: Interpretability methods that utilise local surrogate models (e.g. LIME) are very good at describing the behaviour of the predictive model at a point of interest, but they are not guaranteed to extrapolate to the local region surrounding the point. However, overfitting to the local curvature of the predictive model and malicious tampering can significantly limit extrapolation. We propose an anchor-based algorithm for identifying regions in which local explanations are guaranteed to be correct by explicitly describing those intervals along which the input features can be trusted. Our method produces an interpretable feature-aligned box where the prediction of the local surrogate model is guaranteed to match the predictive model. We demonstrate that our algorithm can be used to find explanations with larger guarantee regions that better cover the data manifold compared to existing baselines. We also show how our method can identify mislead",
    "link": "https://arxiv.org/abs/2402.12737",
    "context": "Title: Guarantee Regions for Local Explanations\nAbstract: arXiv:2402.12737v1 Announce Type: new  Abstract: Interpretability methods that utilise local surrogate models (e.g. LIME) are very good at describing the behaviour of the predictive model at a point of interest, but they are not guaranteed to extrapolate to the local region surrounding the point. However, overfitting to the local curvature of the predictive model and malicious tampering can significantly limit extrapolation. We propose an anchor-based algorithm for identifying regions in which local explanations are guaranteed to be correct by explicitly describing those intervals along which the input features can be trusted. Our method produces an interpretable feature-aligned box where the prediction of the local surrogate model is guaranteed to match the predictive model. We demonstrate that our algorithm can be used to find explanations with larger guarantee regions that better cover the data manifold compared to existing baselines. We also show how our method can identify mislead",
    "path": "papers/24/02/2402.12737.json",
    "total_tokens": 807,
    "translated_title": "本地解释的保证区域",
    "translated_abstract": "使用局部替代模型（如LIME）的可解释性方法非常擅长描述在感兴趣点的预测模型行为，但它们不能保证对周围局部区域进行外推。然而，对预测模型的局部曲率过拟合和恶意篡改可以显著限制外推。我们提出了一种基于锚点的算法，用于识别局部解释被明确保证正确的区域，从而明确描述那些可以信任输入特征的间隔。我们的方法产生一个可解释的特征对齐盒，其中局部替代模型的预测保证与预测模型匹配。我们展示了我们的算法可以用于找到具有更大保证区域的解释，比现有基线更好地覆盖数据流形。我们还展示了我们的方法如何识别引导",
    "tldr": "提出了一种基于锚点的算法，可以识别出局部解释被明确保证正确的区域，产生一个可解释的特征对齐盒，相较于现有基线，能找到具有更大保证区域的解释。",
    "en_tdlr": "Proposed an anchor-based algorithm that identifies regions where local explanations are guaranteed to be correct, generates an interpretable feature-aligned box, and finds explanations with larger guarantee regions compared to existing baselines."
}