{
    "title": "LLMs Among Us: Generative AI Participating in Digital Discourse",
    "abstract": "The emergence of Large Language Models (LLMs) has great potential to reshape the landscape of many social media platforms. While this can bring promising opportunities, it also raises many threats, such as biases and privacy concerns, and may contribute to the spread of propaganda by malicious actors. We developed the \"LLMs Among Us\" experimental framework on top of the Mastodon social media platform for bot and human participants to communicate without knowing the ratio or nature of bot and human participants. We built 10 personas with three different LLMs, GPT-4, LLama 2 Chat, and Claude. We conducted three rounds of the experiment and surveyed participants after each round to measure the ability of LLMs to pose as human participants without human detection. We found that participants correctly identified the nature of other users in the experiment only 42% of the time despite knowing the presence of both bots and humans. We also found that the choice of persona had substantially mor",
    "link": "https://arxiv.org/abs/2402.07940",
    "context": "Title: LLMs Among Us: Generative AI Participating in Digital Discourse\nAbstract: The emergence of Large Language Models (LLMs) has great potential to reshape the landscape of many social media platforms. While this can bring promising opportunities, it also raises many threats, such as biases and privacy concerns, and may contribute to the spread of propaganda by malicious actors. We developed the \"LLMs Among Us\" experimental framework on top of the Mastodon social media platform for bot and human participants to communicate without knowing the ratio or nature of bot and human participants. We built 10 personas with three different LLMs, GPT-4, LLama 2 Chat, and Claude. We conducted three rounds of the experiment and surveyed participants after each round to measure the ability of LLMs to pose as human participants without human detection. We found that participants correctly identified the nature of other users in the experiment only 42% of the time despite knowing the presence of both bots and humans. We also found that the choice of persona had substantially mor",
    "path": "papers/24/02/2402.07940.json",
    "total_tokens": 926,
    "translated_title": "LLMs来袭: 生成型人工智能参与数字话语",
    "translated_abstract": "大型语言模型（LLMs）的出现对许多社交媒体平台的格局具有重塑潜力。虽然这带来了很多有前途的机会，但也引发了许多威胁，如偏见和隐私问题，并可能导致恶意行为者传播宣传。我们在Mastodon社交媒体平台上开发了“LLMs Among Us”实验框架，用于机器人和人类参与者在不了解机器人和人类参与者的比例或性质的情况下进行交流。我们建立了10个不同LLMs（GPT-4、LLama 2 Chat和Claude）的人物形象。我们进行了三轮实验，并在每轮实验后对参与者进行了调查，以衡量LLMs伪装成人类参与者而不被发现的能力。尽管知道实验中存在机器人和人类，参与者只有42％的时间能正确识别其他用户的性质。我们还发现，角色选择在实验中产生了显著影响。",
    "tldr": "LLMs Among Us实验框架通过在社交媒体平台上让机器人和人类互动的方式，研究了大型语言模型在伪装成人类参与者方面的能力，发现尽管存在一定的威胁，但参与者只有42%的时间能正确识别用户的性质。",
    "en_tdlr": "The \"LLMs Among Us\" experimental framework explores the ability of Large Language Models (LLMs) to pose as human participants on social media platforms, finding that participants can only correctly identify the nature of users 42% of the time, highlighting potential threats and privacy concerns."
}