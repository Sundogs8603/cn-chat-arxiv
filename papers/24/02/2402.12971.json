{
    "title": "How Temporal Unrolling Supports Neural Physics Simulators",
    "abstract": "arXiv:2402.12971v1 Announce Type: cross  Abstract: Unrolling training trajectories over time strongly influences the inference accuracy of neural network-augmented physics simulators. We analyze these effects by studying three variants of training neural networks on discrete ground truth trajectories. In addition to commonly used one-step setups and fully differentiable unrolling, we include a third, less widely used variant: unrolling without temporal gradients. Comparing networks trained with these three modalities makes it possible to disentangle the two dominant effects of unrolling, training distribution shift and long-term gradients. We present a detailed study across physical systems, network sizes, network architectures, training setups, and test scenarios. It provides an empirical basis for our main findings: A non-differentiable but unrolled training setup supported by a numerical solver can yield 4.5-fold improvements over a fully differentiable prediction setup that does no",
    "link": "https://arxiv.org/abs/2402.12971",
    "context": "Title: How Temporal Unrolling Supports Neural Physics Simulators\nAbstract: arXiv:2402.12971v1 Announce Type: cross  Abstract: Unrolling training trajectories over time strongly influences the inference accuracy of neural network-augmented physics simulators. We analyze these effects by studying three variants of training neural networks on discrete ground truth trajectories. In addition to commonly used one-step setups and fully differentiable unrolling, we include a third, less widely used variant: unrolling without temporal gradients. Comparing networks trained with these three modalities makes it possible to disentangle the two dominant effects of unrolling, training distribution shift and long-term gradients. We present a detailed study across physical systems, network sizes, network architectures, training setups, and test scenarios. It provides an empirical basis for our main findings: A non-differentiable but unrolled training setup supported by a numerical solver can yield 4.5-fold improvements over a fully differentiable prediction setup that does no",
    "path": "papers/24/02/2402.12971.json",
    "total_tokens": 821,
    "translated_title": "如何通过时间展开支持神经物理模拟器",
    "translated_abstract": "在时间上展开培训轨迹强烈影响神经网络增强型物理模拟器的推理精度。我们通过研究三种不同于离散GroundTruth轨迹训练神经网络的变体来分析这些影响。除了常用的一步设置和完全可微的展开外，我们还包括第三种不太常用的变体：没有时间梯度的展开。比较使用这三种模式训练的网络使得我们能够分解出展开的两个主要影响，即训练分布的转变和长期梯度。我们在物理系统、网络大小、网络架构、训练设置和测试场景中进行了详细研究。它为我们的主要发现提供了经验基础：通过数值求解器支持的不可微分但展开的培训设置，可以使预测设置的完全可微化带来4.5倍的改进。",
    "tldr": "在不可微分但展开的培训设置支持下，通过数值求解器支持的神经物理模拟器能够获得比完全可微化预测设置高出4.5倍的改进。",
    "en_tdlr": "Neural physics simulators supported by non-differentiable but unrolled training setups with numerical solver yield 4.5-fold improvements over fully differentiable prediction setups."
}