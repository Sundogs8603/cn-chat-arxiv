{
    "title": "EventRL: Enhancing Event Extraction with Outcome Supervision for Large Language Models",
    "abstract": "arXiv:2402.11430v1 Announce Type: new  Abstract: In this study, we present EventRL, a reinforcement learning approach developed to enhance event extraction for large language models (LLMs). EventRL utilizes outcome supervision with specific reward functions to tackle prevalent challenges in LLMs, such as instruction following and hallucination, manifested as the mismatch of event structure and the generation of undefined event types. We evaluate EventRL against existing methods like Few-Shot Prompting (FSP) (based on GPT4) and Supervised Fine-Tuning (SFT) across various LLMs, including GPT-4, LLaMa, and CodeLLaMa models. Our findings show that EventRL significantly outperforms these conventional approaches by improving the performance in identifying and structuring events, particularly in handling novel event types. The study emphasizes the critical role of reward function selection and demonstrates the benefits of incorporating code data for better event extraction. While increasing m",
    "link": "https://arxiv.org/abs/2402.11430",
    "context": "Title: EventRL: Enhancing Event Extraction with Outcome Supervision for Large Language Models\nAbstract: arXiv:2402.11430v1 Announce Type: new  Abstract: In this study, we present EventRL, a reinforcement learning approach developed to enhance event extraction for large language models (LLMs). EventRL utilizes outcome supervision with specific reward functions to tackle prevalent challenges in LLMs, such as instruction following and hallucination, manifested as the mismatch of event structure and the generation of undefined event types. We evaluate EventRL against existing methods like Few-Shot Prompting (FSP) (based on GPT4) and Supervised Fine-Tuning (SFT) across various LLMs, including GPT-4, LLaMa, and CodeLLaMa models. Our findings show that EventRL significantly outperforms these conventional approaches by improving the performance in identifying and structuring events, particularly in handling novel event types. The study emphasizes the critical role of reward function selection and demonstrates the benefits of incorporating code data for better event extraction. While increasing m",
    "path": "papers/24/02/2402.11430.json",
    "total_tokens": 838,
    "translated_title": "使用结果监督增强大型语言模型的事件提取的EventRL方法",
    "translated_abstract": "在这项研究中，我们提出了EventRL，这是一种用于增强大型语言模型（LLMs）的事件提取的强化学习方法。EventRL利用特定的奖励函数的结果监督来解决LLMs中普遍存在的问题，例如遵循指令和产生幻觉，这表现为事件结构不匹配和生成未定义事件类型。我们对EventRL进行了评估，与现有方法如Few-Shot Prompting（FSP）（基于GPT4）和Supervised Fine-Tuning（SFT）进行对比，涵盖了包括GPT-4、LLaMa和CodeLLaMa模型在内的各种LLMs。我们的研究结果显示，EventRL在识别和结构化事件方面显著优于这些传统方法，特别是在处理新颖事件类型方面表现得更好。该研究强调了奖励函数选择的关键作用，并展示了将代码数据纳入以获得更好事件提取效果的好处。",
    "tldr": "EventRL通过结果监督和特定奖励函数有效改善了大型语言模型的事件提取效果，尤其在处理新型事件类型方面表现优异。",
    "en_tdlr": "EventRL significantly improves event extraction for large language models through outcome supervision and specific reward functions, especially excelling in handling novel event types."
}