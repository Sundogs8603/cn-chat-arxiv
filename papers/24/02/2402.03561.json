{
    "title": "VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation",
    "abstract": "Outdoor Vision-and-Language Navigation (VLN) requires an agent to navigate through realistic 3D outdoor environments based on natural language instructions. The performance of existing VLN methods is limited by insufficient diversity in navigation environments and limited training data. To address these issues, we propose VLN-Video, which utilizes the diverse outdoor environments present in driving videos in multiple cities in the U.S. augmented with automatically generated navigation instructions and actions to improve outdoor VLN performance. VLN-Video combines the best of intuitive classical approaches and modern deep learning techniques, using template infilling to generate grounded navigation instructions, combined with an image rotation similarity-based navigation action predictor to obtain VLN style data from driving videos for pretraining deep learning VLN models. We pre-train the model on the Touchdown dataset and our video-augmented dataset created from driving videos with th",
    "link": "https://arxiv.org/abs/2402.03561",
    "context": "Title: VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation\nAbstract: Outdoor Vision-and-Language Navigation (VLN) requires an agent to navigate through realistic 3D outdoor environments based on natural language instructions. The performance of existing VLN methods is limited by insufficient diversity in navigation environments and limited training data. To address these issues, we propose VLN-Video, which utilizes the diverse outdoor environments present in driving videos in multiple cities in the U.S. augmented with automatically generated navigation instructions and actions to improve outdoor VLN performance. VLN-Video combines the best of intuitive classical approaches and modern deep learning techniques, using template infilling to generate grounded navigation instructions, combined with an image rotation similarity-based navigation action predictor to obtain VLN style data from driving videos for pretraining deep learning VLN models. We pre-train the model on the Touchdown dataset and our video-augmented dataset created from driving videos with th",
    "path": "papers/24/02/2402.03561.json",
    "total_tokens": 822,
    "translated_title": "VLN-Video: 利用行车视频进行室外视觉与语言导航",
    "translated_abstract": "室外视觉与语言导航（VLN）要求代理根据自然语言指令在逼真的三维室外环境中导航。现有的VLN方法在导航环境多样性和训练数据有限性方面存在限制。为解决这些问题，我们提出了VLN-Video，该方法利用在美国多个城市的行车视频中存在的多样化室外环境，并通过自动生成导航指令和动作来提高室外VLN性能。VLN-Video结合了直观经典方法和现代深度学习技术的优势，利用模板填充生成有实际基础的导航指令，并结合基于图像旋转相似度的导航动作预测器从行车视频中获取VLN风格的数据，用于预训练深度学习VLN模型。我们在Touchdown数据集和由行车视频创建的视频增强数据集上对模型进行预训练。",
    "tldr": "VLN-Video利用行车视频的多样室外环境和自动生成的导航指令与动作，通过深度学习方法提高了室外视觉与语言导航的性能。"
}