{
    "title": "A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models",
    "abstract": "arXiv:2402.13606v1 Announce Type: new  Abstract: The tendency of Large Language Models to generate hallucinations and exhibit overconfidence in predictions raises concerns regarding their reliability. Confidence or uncertainty estimations indicating the extent of trustworthiness of a model's response are essential to developing reliable AI systems. Current research primarily focuses on LLM confidence estimations in English, remaining a void for other widely used languages and impeding the global development of reliable AI applications. This paper introduces a comprehensive investigation of Multi-lingual confidence estimation (MlingConf) on LLMs. First, we introduce an elaborated and expert-checked multilingual QA dataset. Second, we delve into the performance of confidence estimations and examine how these confidence scores can enhance LLM performance through self-refinement across diverse languages. Finally, we propose a cross-lingual confidence estimation method to achieve more preci",
    "link": "https://arxiv.org/abs/2402.13606",
    "context": "Title: A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models\nAbstract: arXiv:2402.13606v1 Announce Type: new  Abstract: The tendency of Large Language Models to generate hallucinations and exhibit overconfidence in predictions raises concerns regarding their reliability. Confidence or uncertainty estimations indicating the extent of trustworthiness of a model's response are essential to developing reliable AI systems. Current research primarily focuses on LLM confidence estimations in English, remaining a void for other widely used languages and impeding the global development of reliable AI applications. This paper introduces a comprehensive investigation of Multi-lingual confidence estimation (MlingConf) on LLMs. First, we introduce an elaborated and expert-checked multilingual QA dataset. Second, we delve into the performance of confidence estimations and examine how these confidence scores can enhance LLM performance through self-refinement across diverse languages. Finally, we propose a cross-lingual confidence estimation method to achieve more preci",
    "path": "papers/24/02/2402.13606.json",
    "total_tokens": 884,
    "translated_title": "对大型语言模型的多语言置信度评估进行全面研究",
    "translated_abstract": "大型语言模型生成幻觉并在预测中表现过于自信的倾向引发了人们对其可靠性的担忧。表明模型响应的可信度或不确定性估计对于开发可靠的人工智能系统至关重要。目前的研究主要集中在英语中LLM的置信度估计上，在其他广泛使用的语言方面仍存在空白，阻碍了可靠AI应用的全球发展。本文介绍了对LLM上的多语言置信度评估（MlingConf）的全面调查。首先，我们引入了一个经过详细检查的专业多语言问答数据集。其次，我们深入研究置信度估计的性能，并研究这些置信度分数如何通过跨不同语言的自我完善来增强LLM的性能。最后，我们提出了一种跨语言置信度估计方法，以实现更精确的估计。",
    "tldr": "该论文介绍了对大型语言模型的多语言置信度评估的全面研究，提出了一个专业多语言问答数据集，并研究了这些置信度分数如何增强模型性能，最终提出了一种跨语言置信度估计方法。",
    "en_tdlr": "This paper presents a comprehensive study of multilingual confidence estimation on large language models, introducing a specialized multilingual QA dataset, investigating how these confidence scores can enhance model performance, and proposing a cross-lingual confidence estimation method."
}