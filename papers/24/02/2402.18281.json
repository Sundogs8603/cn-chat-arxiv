{
    "title": "Towards Better Understanding of Contrastive Sentence Representation Learning: A Unified Paradigm for Gradient",
    "abstract": "arXiv:2402.18281v1 Announce Type: new  Abstract: Sentence Representation Learning (SRL) is a crucial task in Natural Language Processing (NLP), where contrastive Self-Supervised Learning (SSL) is currently a mainstream approach. However, the reasons behind its remarkable effectiveness remain unclear. Specifically, in other research fields, contrastive SSL shares similarities in both theory and practical performance with non-contrastive SSL (e.g., alignment & uniformity, Barlow Twins, and VICReg). However, in SRL, contrastive SSL outperforms non-contrastive SSL significantly. Therefore, two questions arise: First, what commonalities enable various contrastive losses to achieve superior performance in SRL? Second, how can we make non-contrastive SSL, which is similar to contrastive SSL but ineffective in SRL, effective? To address these questions, we start from the perspective of gradients and discover that four effective contrastive losses can be integrated into a unified paradigm, whic",
    "link": "https://arxiv.org/abs/2402.18281",
    "context": "Title: Towards Better Understanding of Contrastive Sentence Representation Learning: A Unified Paradigm for Gradient\nAbstract: arXiv:2402.18281v1 Announce Type: new  Abstract: Sentence Representation Learning (SRL) is a crucial task in Natural Language Processing (NLP), where contrastive Self-Supervised Learning (SSL) is currently a mainstream approach. However, the reasons behind its remarkable effectiveness remain unclear. Specifically, in other research fields, contrastive SSL shares similarities in both theory and practical performance with non-contrastive SSL (e.g., alignment & uniformity, Barlow Twins, and VICReg). However, in SRL, contrastive SSL outperforms non-contrastive SSL significantly. Therefore, two questions arise: First, what commonalities enable various contrastive losses to achieve superior performance in SRL? Second, how can we make non-contrastive SSL, which is similar to contrastive SSL but ineffective in SRL, effective? To address these questions, we start from the perspective of gradients and discover that four effective contrastive losses can be integrated into a unified paradigm, whic",
    "path": "papers/24/02/2402.18281.json",
    "total_tokens": 860,
    "translated_title": "更好理解对比句子表示学习：梯度的统一范式",
    "translated_abstract": "句子表示学习（SRL）是自然语言处理（NLP）中至关重要的任务，对照自监督学习（SSL）是目前主流方法。然而，其显著有效性背后的原因仍不清楚。特别是，在其他研究领域中，对比SSL在理论和实际表现上与非对比SSL（例如，对齐和一致性、Barlow Twins和VICReg）有相似之处。然而，在SRL中，对比SSL明显优于非对比SSL。因此，出现了两个问题：首先，是什么共同点使各种对比损失在SRL中取得了优越性能？其次，我们如何使非对比SSL（与对比SSL相似但在SRL中无效）变得有效？为了解决这些问题，我们从梯度的角度出发，发现四种有效的对比损失可以集成到一个统一的范式中",
    "tldr": "通过研究梯度，将四种有效的对比损失集成到一个统一的范式中，以探究对比句子表示学习中各种对比损失达到卓越性能的共同特点。",
    "en_tdlr": "By studying gradients, integrating four effective contrastive losses into a unified paradigm to explore the common characteristics that enable various contrastive losses to achieve superior performance in contrastive sentence representation learning."
}