{
    "title": "Adversarial Feature Alignment: Balancing Robustness and Accuracy in Deep Learning via Adversarial Training",
    "abstract": "arXiv:2402.12187v1 Announce Type: cross  Abstract: Deep learning models continue to advance in accuracy, yet they remain vulnerable to adversarial attacks, which often lead to the misclassification of adversarial examples. Adversarial training is used to mitigate this problem by increasing robustness against these attacks. However, this approach typically reduces a model's standard accuracy on clean, non-adversarial samples. The necessity for deep learning models to balance both robustness and accuracy for security is obvious, but achieving this balance remains challenging, and the underlying reasons are yet to be clarified. This paper proposes a novel adversarial training method called Adversarial Feature Alignment (AFA), to address these problems. Our research unveils an intriguing insight: misalignment within the feature space often leads to misclassification, regardless of whether the samples are benign or adversarial. AFA mitigates this risk by employing a novel optimization algor",
    "link": "https://arxiv.org/abs/2402.12187",
    "context": "Title: Adversarial Feature Alignment: Balancing Robustness and Accuracy in Deep Learning via Adversarial Training\nAbstract: arXiv:2402.12187v1 Announce Type: cross  Abstract: Deep learning models continue to advance in accuracy, yet they remain vulnerable to adversarial attacks, which often lead to the misclassification of adversarial examples. Adversarial training is used to mitigate this problem by increasing robustness against these attacks. However, this approach typically reduces a model's standard accuracy on clean, non-adversarial samples. The necessity for deep learning models to balance both robustness and accuracy for security is obvious, but achieving this balance remains challenging, and the underlying reasons are yet to be clarified. This paper proposes a novel adversarial training method called Adversarial Feature Alignment (AFA), to address these problems. Our research unveils an intriguing insight: misalignment within the feature space often leads to misclassification, regardless of whether the samples are benign or adversarial. AFA mitigates this risk by employing a novel optimization algor",
    "path": "papers/24/02/2402.12187.json",
    "total_tokens": 961,
    "translated_title": "对抗特征对齐：通过对抗训练在深度学习中平衡鲁棒性和准确性",
    "translated_abstract": "深度学习模型在准确性方面不断取得进展，但仍然容易受到对抗性攻击的影响，这经常导致对抗性样本被错误分类。对抗训练被用来通过增强对这些攻击的鲁棒性来减轻这个问题。然而，这种方法通常会降低模型在干净的非对抗性样本上的标准准确性。深度学习模型需要在安全性方面平衡鲁棒性和准确性的必要性是显而易见的，但实现这种平衡仍然具有挑战性，其潜在原因尚未明确阐述。本文提出了一种新颖的对抗训练方法，称为对抗特征对齐（AFA），以解决这些问题。我们的研究揭示了一个有趣的见解：特征空间中的不对齐经常导致错误分类，无论样本是良性还是对抗性。AFA通过采用一种新颖的优化算法来减轻这种风险。",
    "tldr": "通过对抗训练，在深度学习中提出了一种对抗特征对齐的方法，以平衡鲁棒性和准确性，研究发现特征空间内部的不对齐经常导致 misclassification，这一方法旨在解决这些问题。",
    "en_tdlr": "Introducing Adversarial Feature Alignment as a method to balance robustness and accuracy in deep learning through adversarial training, the research reveals that misalignment within the feature space often leads to misclassification, aiming to address this issue."
}