{
    "title": "Attention as Robust Representation for Time Series Forecasting",
    "abstract": "Time series forecasting is essential for many practical applications, with the adoption of transformer-based models on the rise due to their impressive performance in NLP and CV. Transformers' key feature, the attention mechanism, dynamically fusing embeddings to enhance data representation, often relegating attention weights to a byproduct role. Yet, time series data, characterized by noise and non-stationarity, poses significant forecasting challenges. Our approach elevates attention weights as the primary representation for time series, capitalizing on the temporal relationships among data points to improve forecasting accuracy. Our study shows that an attention map, structured using global landmarks and local windows, acts as a robust kernel representation for data points, withstanding noise and shifts in distribution. Our method outperforms state-of-the-art models, reducing mean squared error (MSE) in multivariate time series forecasting by a notable 3.6% without altering the core",
    "link": "https://arxiv.org/abs/2402.05370",
    "context": "Title: Attention as Robust Representation for Time Series Forecasting\nAbstract: Time series forecasting is essential for many practical applications, with the adoption of transformer-based models on the rise due to their impressive performance in NLP and CV. Transformers' key feature, the attention mechanism, dynamically fusing embeddings to enhance data representation, often relegating attention weights to a byproduct role. Yet, time series data, characterized by noise and non-stationarity, poses significant forecasting challenges. Our approach elevates attention weights as the primary representation for time series, capitalizing on the temporal relationships among data points to improve forecasting accuracy. Our study shows that an attention map, structured using global landmarks and local windows, acts as a robust kernel representation for data points, withstanding noise and shifts in distribution. Our method outperforms state-of-the-art models, reducing mean squared error (MSE) in multivariate time series forecasting by a notable 3.6% without altering the core",
    "path": "papers/24/02/2402.05370.json",
    "total_tokens": 868,
    "translated_title": "作为时间序列预测的稳健表示的注意力",
    "translated_abstract": "时间序列预测在许多实际应用中至关重要，由于Transformer模型在自然语言处理和计算机视觉方面的优秀性能，其在时间序列预测中的应用逐渐增多。Transformers的关键特性，注意力机制，动态地融合嵌入以增强数据表示，通常将注意力权重作为副产品。然而，时间序列数据具有噪声和非平稳性，给预测带来了重大挑战。我们的方法将注意力权重提升为时间序列的主要表示，利用数据点之间的时间关系来改善预测准确性。我们的研究表明，使用全局标志和局部窗口构建的注意力图充当数据点的稳健核表示，能够抵抗噪声和分布移动。我们的方法在多变量时间序列预测中，将均方误差(MSE)显著降低了3.6%，超过了现有模型的表现，而不改变核心",
    "tldr": "在时间序列预测中，我们提出的方法将注意力权重提升为主要表示，使用全局标志和局部窗口构建的注意力图作为稳健核表示来克服噪声和分布变化，并取得了比现有模型更好的性能 improvement.",
    "en_tdlr": "In time series forecasting, we propose elevating attention weights as the primary representation, utilizing an attention map structured with global landmarks and local windows as robust kernel representation to overcome noise and distribution shifts, achieving better performance than existing models."
}