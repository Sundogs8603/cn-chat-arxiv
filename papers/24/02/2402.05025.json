{
    "title": "Strong convexity-guided hyper-parameter optimization for flatter losses",
    "abstract": "We propose a novel white-box approach to hyper-parameter optimization. Motivated by recent work establishing a relationship between flat minima and generalization, we first establish a relationship between the strong convexity of the loss and its flatness. Based on this, we seek to find hyper-parameter configurations that improve flatness by minimizing the strong convexity of the loss. By using the structure of the underlying neural network, we derive closed-form equations to approximate the strong convexity parameter, and attempt to find hyper-parameters that minimize it in a randomized fashion. Through experiments on 14 classification datasets, we show that our method achieves strong performance at a fraction of the runtime.",
    "link": "https://arxiv.org/abs/2402.05025",
    "context": "Title: Strong convexity-guided hyper-parameter optimization for flatter losses\nAbstract: We propose a novel white-box approach to hyper-parameter optimization. Motivated by recent work establishing a relationship between flat minima and generalization, we first establish a relationship between the strong convexity of the loss and its flatness. Based on this, we seek to find hyper-parameter configurations that improve flatness by minimizing the strong convexity of the loss. By using the structure of the underlying neural network, we derive closed-form equations to approximate the strong convexity parameter, and attempt to find hyper-parameters that minimize it in a randomized fashion. Through experiments on 14 classification datasets, we show that our method achieves strong performance at a fraction of the runtime.",
    "path": "papers/24/02/2402.05025.json",
    "total_tokens": 819,
    "translated_title": "强凸性引导的更平坦损失超参数优化",
    "translated_abstract": "我们提出了一种新颖的白盒方法来进行超参数优化。受到最近的关于平坦最小值和泛化之间关系的工作的启发，我们首先建立了损失函数的强凸性和其平坦性之间的关系。基于此，我们试图通过最小化损失函数的强凸性来寻找改善平坦性的超参数配置。通过利用底层神经网络的结构，我们推导出用于近似计算强凸性参数的闭式方程，并尝试以随机化的方式寻找最小化它的超参数。通过在14个分类数据集上进行实验，我们展示了我们的方法在更短的运行时间下取得了较强的性能。",
    "tldr": "本文提出了一种强凸性引导的超参数优化方法，通过最小化损失函数的强凸性来改善其平坦性。通过利用底层神经网络的结构，我们提出了一种闭式方程来近似计算强凸性参数，并以随机化的方式寻找最小化该参数的超参数配置。实验证明，该方法在更短的运行时间下取得了较强的性能。",
    "en_tdlr": "This paper proposes a strong convexity-guided approach to hyper-parameter optimization, aiming to improve the flatness of the loss by minimizing its strong convexity. By utilizing the structure of the underlying neural network, closed-form equations are derived to approximate the strong convexity parameter and randomized hyper-parameter configurations are searched. Experimental results show that this method achieves strong performance with shorter runtime."
}