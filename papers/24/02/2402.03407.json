{
    "title": "Enhancing the Stability of LLM-based Speech Generation Systems through Self-Supervised Representations",
    "abstract": "Large Language Models (LLMs) are one of the most promising technologies for the next era of speech generation systems, due to their scalability and in-context learning capabilities. Nevertheless, they suffer from multiple stability issues at inference time, such as hallucinations, content skipping or speech repetitions. In this work, we introduce a new self-supervised Voice Conversion (VC) architecture which can be used to learn to encode transitory features, such as content, separately from stationary ones, such as speaker ID or recording conditions, creating speaker-disentangled representations. Using speaker-disentangled codes to train LLMs for text-to-speech (TTS) allows the LLM to generate the content and the style of the speech only from the text, similarly to humans, while the speaker identity is provided by the decoder of the VC model. Results show that LLMs trained over speaker-disentangled self-supervised representations provide an improvement of 4.7pp in speaker similarity o",
    "link": "https://arxiv.org/abs/2402.03407",
    "context": "Title: Enhancing the Stability of LLM-based Speech Generation Systems through Self-Supervised Representations\nAbstract: Large Language Models (LLMs) are one of the most promising technologies for the next era of speech generation systems, due to their scalability and in-context learning capabilities. Nevertheless, they suffer from multiple stability issues at inference time, such as hallucinations, content skipping or speech repetitions. In this work, we introduce a new self-supervised Voice Conversion (VC) architecture which can be used to learn to encode transitory features, such as content, separately from stationary ones, such as speaker ID or recording conditions, creating speaker-disentangled representations. Using speaker-disentangled codes to train LLMs for text-to-speech (TTS) allows the LLM to generate the content and the style of the speech only from the text, similarly to humans, while the speaker identity is provided by the decoder of the VC model. Results show that LLMs trained over speaker-disentangled self-supervised representations provide an improvement of 4.7pp in speaker similarity o",
    "path": "papers/24/02/2402.03407.json",
    "total_tokens": 921,
    "translated_title": "通过自监督表示增强LLM基础的语音生成系统的稳定性",
    "translated_abstract": "大型语言模型（LLMs）是下一代语音生成系统最有前景的技术之一，由于其可扩展性和上下文学习能力。然而，在推理时，它们遇到了多个稳定性问题，如幻觉、跳过内容或语音重复。在这项工作中，我们介绍了一种新的自监督语音转换（VC）架构，可以用于学习将瞬态特征（如内容）与固定特征（如说话者ID或录制条件）分开编码，创建说话者解耦表示。使用说话者解耦编码来训练LLMs进行文本到语音（TTS）允许LLM仅通过文本生成语音的内容和风格，类似于人类，而说话者身份由VC模型的解码器提供。结果显示，训练过说话者解耦的自监督表示的LLMs在说话者相似性方面提供了4.7pp的改进。",
    "tldr": "通过自监督表示，我们提出了一种新的自助转换架构，该架构可以增强LLM基础的语音生成系统的稳定性，并克服了在推理时出现的多个稳定性问题。使用这种方法，LLM可以从文本中仅生成语音的内容和风格，而说话者身份由另一个模型提供。",
    "en_tdlr": "We propose a new self-supervised Voice Conversion architecture to enhance the stability of LLM-based speech generation systems, overcoming multiple stability issues at inference time. By using this approach, LLMs can generate the content and style of speech only from the text, while the speaker identity is provided by another model."
}