{
    "title": "Only the Curve Shape Matters: Training Foundation Models for Zero-Shot Multivariate Time Series Forecasting through Next Curve Shape Prediction",
    "abstract": "We present General Time Transformer (GTT), an encoder-only style foundation model for zero-shot multivariate time series forecasting. GTT is pretrained on a large dataset of 200M high-quality time series samples spanning diverse domains. In our proposed framework, the task of multivariate time series forecasting is formulated as a channel-wise next curve shape prediction problem, where each time series sample is represented as a sequence of non-overlapping curve shapes with a unified numerical magnitude. GTT is trained to predict the next curve shape based on a window of past curve shapes in a channel-wise manner. Experimental results demonstrate that GTT exhibits superior zero-shot multivariate forecasting capabilities on unseen time series datasets, even surpassing state-of-the-art supervised baselines. Additionally, we investigate the impact of varying GTT model parameters and training dataset scales, observing that the scaling law also holds in the context of zero-shot multivariate",
    "link": "https://arxiv.org/abs/2402.07570",
    "context": "Title: Only the Curve Shape Matters: Training Foundation Models for Zero-Shot Multivariate Time Series Forecasting through Next Curve Shape Prediction\nAbstract: We present General Time Transformer (GTT), an encoder-only style foundation model for zero-shot multivariate time series forecasting. GTT is pretrained on a large dataset of 200M high-quality time series samples spanning diverse domains. In our proposed framework, the task of multivariate time series forecasting is formulated as a channel-wise next curve shape prediction problem, where each time series sample is represented as a sequence of non-overlapping curve shapes with a unified numerical magnitude. GTT is trained to predict the next curve shape based on a window of past curve shapes in a channel-wise manner. Experimental results demonstrate that GTT exhibits superior zero-shot multivariate forecasting capabilities on unseen time series datasets, even surpassing state-of-the-art supervised baselines. Additionally, we investigate the impact of varying GTT model parameters and training dataset scales, observing that the scaling law also holds in the context of zero-shot multivariate",
    "path": "papers/24/02/2402.07570.json",
    "total_tokens": 938,
    "translated_title": "只有曲线形状有关：通过下一个曲线形状预测训练基础模型进行零样本多元时间序列预测",
    "translated_abstract": "我们提出了General Time Transformer (GTT)，一种仅有编码器的基础模型，用于零样本多元时间序列预测。GTT在一个包含2亿个高质量时间序列样本的大型数据集上进行预训练，涵盖了不同领域。在我们提出的框架中，多元时间序列预测的任务被建模为一个逐通道的下一个曲线形状预测问题，其中每个时间序列样本表示为一系列非重叠的曲线形状，具有统一的数值大小。GTT在通道级别上通过预测过去曲线形状的窗口来预测下一个曲线形状。实验结果表明，GTT在未见时间序列数据集上展现出优秀的零样本多元预测能力，甚至超过了最先进的有监督基线模型。此外，我们还研究了GTT模型参数和训练数据集规模变化的影响，观察到在零样本多元预测的背景下，规模定律也成立。",
    "tldr": "通过下一个曲线形状预测，我们提出了基于编码器的零样本多元时间序列预测模型GTT，通过预训练和通道级别的曲线形状预测，展现出优秀的预测能力，甚至超过了最先进的有监督模型。",
    "en_tdlr": "We propose GTT, an encoder-only model trained for zero-shot multivariate time series forecasting through next curve shape prediction. GTT demonstrates superior forecasting capabilities on unseen datasets, surpassing state-of-the-art supervised baselines."
}