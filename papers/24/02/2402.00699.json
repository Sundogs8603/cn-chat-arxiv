{
    "title": "PeaTMOSS: A Dataset and Initial Analysis of Pre-Trained Models in Open-Source Software",
    "abstract": "The development and training of deep learning models have become increasingly costly and complex. Consequently, software engineers are adopting pre-trained models (PTMs) for their downstream applications. The dynamics of the PTM supply chain remain largely unexplored, signaling a clear need for structured datasets that document not only the metadata but also the subsequent applications of these models. Without such data, the MSR community cannot comprehensively understand the impact of PTM adoption and reuse. This paper presents the PeaTMOSS dataset, which comprises metadata for 281,638 PTMs and detailed snapshots for all PTMs with over 50 monthly downloads (14,296 PTMs), along with 28,575 open-source software repositories from GitHub that utilize these models. Additionally, the dataset includes 44,337 mappings from 15,129 downstream GitHub repositories to the 2,530 PTMs they use. To enhance the dataset's comprehensiveness, we developed prompts for a large language model to automatical",
    "link": "https://arxiv.org/abs/2402.00699",
    "context": "Title: PeaTMOSS: A Dataset and Initial Analysis of Pre-Trained Models in Open-Source Software\nAbstract: The development and training of deep learning models have become increasingly costly and complex. Consequently, software engineers are adopting pre-trained models (PTMs) for their downstream applications. The dynamics of the PTM supply chain remain largely unexplored, signaling a clear need for structured datasets that document not only the metadata but also the subsequent applications of these models. Without such data, the MSR community cannot comprehensively understand the impact of PTM adoption and reuse. This paper presents the PeaTMOSS dataset, which comprises metadata for 281,638 PTMs and detailed snapshots for all PTMs with over 50 monthly downloads (14,296 PTMs), along with 28,575 open-source software repositories from GitHub that utilize these models. Additionally, the dataset includes 44,337 mappings from 15,129 downstream GitHub repositories to the 2,530 PTMs they use. To enhance the dataset's comprehensiveness, we developed prompts for a large language model to automatical",
    "path": "papers/24/02/2402.00699.json",
    "total_tokens": 896,
    "translated_title": "PeaTMOSS: 一个开源软件中预训练模型的数据集和初步分析",
    "translated_abstract": "深度学习模型的开发和训练变得越来越昂贵和复杂。因此，软件工程师正在采用预训练模型(PTMs)来进行后续应用。PTM供应链的动态仍然很少被探索，这表明需要结构化的数据集，不仅记录元数据，还记录这些模型的后续应用。没有这样的数据，MSR社区无法全面理解PTM的采用和重复使用的影响。本文提出了PeaTMOSS数据集，其中包括281,638个PTM的元数据和超过50个月下载量的所有PTM的详细快照(14,296个PTMs)，以及利用这些模型的28,575个来自GitHub的开源软件代码库。此外，数据集还包括15,129个GitHub代码库到它们使用的2,530个PTMs的44,337个映射。为了提高数据集的全面性，我们为一个大型语言模型开发了提示，以自动地进行摘要生成。",
    "tldr": "本研究介绍了PeaTMOSS数据集，用于记录和分析开源软件中预训练模型的元数据和应用情况。这对于了解预训练模型的采用和重复使用的影响具有重要意义。",
    "en_tdlr": "This paper presents the PeaTMOSS dataset, which documents and analyzes metadata and applications of pre-trained models in open-source software. It is important for understanding the impact of pre-trained model adoption and reuse."
}