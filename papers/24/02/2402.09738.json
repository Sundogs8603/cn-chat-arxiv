{
    "title": "Align before Attend: Aligning Visual and Textual Features for Multimodal Hateful Content Detection",
    "abstract": "arXiv:2402.09738v1 Announce Type: new  Abstract: Multimodal hateful content detection is a challenging task that requires complex reasoning across visual and textual modalities. Therefore, creating a meaningful multimodal representation that effectively captures the interplay between visual and textual features through intermediate fusion is critical. Conventional fusion techniques are unable to attend to the modality-specific features effectively. Moreover, most studies exclusively concentrated on English and overlooked other low-resource languages. This paper proposes a context-aware attention framework for multimodal hateful content detection and assesses it for both English and non-English languages. The proposed approach incorporates an attention layer to meaningfully align the visual and textual features. This alignment enables selective focus on modality-specific features before fusing them. We evaluate the proposed approach on two benchmark hateful meme datasets, viz. MUTE (Ben",
    "link": "https://arxiv.org/abs/2402.09738",
    "context": "Title: Align before Attend: Aligning Visual and Textual Features for Multimodal Hateful Content Detection\nAbstract: arXiv:2402.09738v1 Announce Type: new  Abstract: Multimodal hateful content detection is a challenging task that requires complex reasoning across visual and textual modalities. Therefore, creating a meaningful multimodal representation that effectively captures the interplay between visual and textual features through intermediate fusion is critical. Conventional fusion techniques are unable to attend to the modality-specific features effectively. Moreover, most studies exclusively concentrated on English and overlooked other low-resource languages. This paper proposes a context-aware attention framework for multimodal hateful content detection and assesses it for both English and non-English languages. The proposed approach incorporates an attention layer to meaningfully align the visual and textual features. This alignment enables selective focus on modality-specific features before fusing them. We evaluate the proposed approach on two benchmark hateful meme datasets, viz. MUTE (Ben",
    "path": "papers/24/02/2402.09738.json",
    "total_tokens": 941,
    "translated_title": "在注意之前进行对齐：用于多模态恶意内容检测的视觉和文本特征对齐",
    "translated_abstract": "多模态恶意内容检测是一个具有挑战性的任务，需要对视觉和文本模态进行复杂推理。因此，通过中间融合有效地捕捉视觉和文本特征之间的相互作用，创建有意义的多模态表示是至关重要的。传统的融合技术无法有效地关注模态特定的特征。此外，大多数研究仅关注英语，忽视了其他低资源语言。本文提出了一种上下文感知的注意力框架，用于多模态恶意内容检测，并对英语和非英语语言进行了评估。所提出的方法将注意力层引入到视觉和文本特征之间进行有意义的对齐。这种对齐使得在融合之前可以有选择性地关注模态特定的特征。我们在两个基准恶意模因数据集上评估了所提出的方法，即MUTE（Ben）",
    "tldr": "本论文提出了一种上下文感知的注意力框架，用于在视觉和文本特征之间进行对齐，以有效地进行多模态恶意内容检测。该方法能够选择性地关注模态特定的特征，解决了传统的融合技术无法有效关注模态特定特征的问题。研究在英语和非英语语言上进行了评估。",
    "en_tdlr": "This paper proposes a context-aware attention framework for effective multimodal hateful content detection, addressing the limitation of conventional fusion techniques in attending to modality-specific features. The proposed approach aligns visual and textual features, allowing selective focus on modality-specific features before fusion. The study evaluates the approach in both English and non-English languages."
}