{
    "title": "Absolute convergence and error thresholds in non-active adaptive sampling",
    "abstract": "Non-active adaptive sampling is a way of building machine learning models from a training data base which are supposed to dynamically and automatically derive guaranteed sample size. In this context and regardless of the strategy used in both scheduling and generating of weak predictors, a proposal for calculating absolute convergence and error thresholds is described. We not only make it possible to establish when the quality of the model no longer increases, but also supplies a proximity condition to estimate in absolute terms how close it is to achieving such a goal, thus supporting decision making for fine-tuning learning parameters in model selection. The technique proves its correctness and completeness with respect to our working hypotheses, in addition to strengthening the robustness of the sampling scheme. Tests meet our expectations and illustrate the proposal in the domain of natural language processing, taking the generation of part-of-speech taggers as case study.",
    "link": "https://arxiv.org/abs/2402.02522",
    "context": "Title: Absolute convergence and error thresholds in non-active adaptive sampling\nAbstract: Non-active adaptive sampling is a way of building machine learning models from a training data base which are supposed to dynamically and automatically derive guaranteed sample size. In this context and regardless of the strategy used in both scheduling and generating of weak predictors, a proposal for calculating absolute convergence and error thresholds is described. We not only make it possible to establish when the quality of the model no longer increases, but also supplies a proximity condition to estimate in absolute terms how close it is to achieving such a goal, thus supporting decision making for fine-tuning learning parameters in model selection. The technique proves its correctness and completeness with respect to our working hypotheses, in addition to strengthening the robustness of the sampling scheme. Tests meet our expectations and illustrate the proposal in the domain of natural language processing, taking the generation of part-of-speech taggers as case study.",
    "path": "papers/24/02/2402.02522.json",
    "total_tokens": 835,
    "translated_title": "非主动自适应采样中的绝对收敛和误差阈值",
    "translated_abstract": "非主动自适应采样是一种从训练数据中构建机器学习模型的方法，它可以动态和自动地确定保证的样本大小。在这个背景下，无论所采用的调度和生成弱预测器的策略如何，我们描述了一种计算绝对收敛和误差阈值的方法。我们不仅可以确定模型的质量何时不再增加，还提供了一个接近条件来绝对地估算模型实现这一目标的接近度，从而支持在模型选择中进行学习参数的微调。该技术在工作假设方面证明了其正确性和完备性，同时增强了采样方案的鲁棒性。测试结果符合我们的预期，并以自然语言处理领域中词性标注生成为案例研究来说明这一提议。",
    "tldr": "提出了一种计算非主动自适应采样中绝对收敛和误差阈值的方法，可以确定模型何时不再增加质量，并提供了一个接近条件来估算模型实现目标的接近度，从而支持模型选择中学习参数的微调。",
    "en_tdlr": "A method is proposed to calculate the absolute convergence and error thresholds in non-active adaptive sampling, allowing to determine when the model no longer improves in quality and providing a proximity condition to estimate how close it is to achieving its goal, facilitating fine-tuning of learning parameters in model selection."
}