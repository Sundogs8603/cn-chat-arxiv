{
    "title": "EPSD: Early Pruning with Self-Distillation for Efficient Model Compression",
    "abstract": "Neural network compression techniques, such as knowledge distillation (KD) and network pruning, have received increasing attention. Recent work `Prune, then Distill' reveals that a pruned student-friendly teacher network can benefit the performance of KD. However, the conventional teacher-student pipeline, which entails cumbersome pre-training of the teacher and complicated compression steps, makes pruning with KD less efficient. In addition to compressing models, recent compression techniques also emphasize the aspect of efficiency. Early pruning demands significantly less computational cost in comparison to the conventional pruning methods as it does not require a large pre-trained model. Likewise, a special case of KD, known as self-distillation (SD), is more efficient since it requires no pre-training or student-teacher pair selection. This inspires us to collaborate early pruning with SD for efficient model compression. In this work, we propose the framework named Early Pruning wi",
    "link": "https://arxiv.org/abs/2402.00084",
    "context": "Title: EPSD: Early Pruning with Self-Distillation for Efficient Model Compression\nAbstract: Neural network compression techniques, such as knowledge distillation (KD) and network pruning, have received increasing attention. Recent work `Prune, then Distill' reveals that a pruned student-friendly teacher network can benefit the performance of KD. However, the conventional teacher-student pipeline, which entails cumbersome pre-training of the teacher and complicated compression steps, makes pruning with KD less efficient. In addition to compressing models, recent compression techniques also emphasize the aspect of efficiency. Early pruning demands significantly less computational cost in comparison to the conventional pruning methods as it does not require a large pre-trained model. Likewise, a special case of KD, known as self-distillation (SD), is more efficient since it requires no pre-training or student-teacher pair selection. This inspires us to collaborate early pruning with SD for efficient model compression. In this work, we propose the framework named Early Pruning wi",
    "path": "papers/24/02/2402.00084.json",
    "total_tokens": 875,
    "translated_title": "EPSD: 早期修剪与自蒸馏相结合的高效模型压缩",
    "translated_abstract": "神经网络压缩技术，如知识蒸馏和网络修剪，受到越来越多的关注。最近的研究“先修剪，然后蒸馏”表明，修剪后的适合学生的教师网络可以提高知识蒸馏的性能。然而，传统的教师-学生流程，涉及繁琐的教师预训练和复杂的压缩步骤，使得使用知识蒸馏的修剪变得不够高效。除了压缩模型，最近的压缩技术也强调效率方面的考量。早期修剪与传统的修剪方法相比，要求的计算成本要小得多，因为它不需要一个大型预训练模型。同样，自蒸馏作为知识蒸馏的一种特殊情况，更加高效，因为它不需要预训练或学生-教师对的选择。这激发了我们将早期修剪与自蒸馏相结合，实现高效的模型压缩。在这项工作中，我们提出了名为“早期修剪”的框架。",
    "tldr": "提出了一种早期修剪与自蒸馏相结合的框架，实现了高效的模型压缩。"
}