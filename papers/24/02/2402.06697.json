{
    "title": "Feed-Forward Neural Networks as a Mixed-Integer Program",
    "abstract": "Deep neural networks (DNNs) are widely studied in various applications. A DNN consists of layers of neurons that compute affine combinations, apply nonlinear operations, and produce corresponding activations. The rectified linear unit (ReLU) is a typical nonlinear operator, outputting the max of its input and zero. In scenarios like max pooling, where multiple input values are involved, a fixed-parameter DNN can be modeled as a mixed-integer program (MIP). This formulation, with continuous variables representing unit outputs and binary variables for ReLU activation, finds applications across diverse domains. This study explores the formulation of trained ReLU neurons as MIP and applies MIP models for training neural networks (NNs). Specifically, it investigates interactions between MIP techniques and various NN architectures, including binary DNNs (employing step activation functions) and binarized DNNs (with weights and activations limited to $-1,0,+1$). The research focuses on traini",
    "link": "https://arxiv.org/abs/2402.06697",
    "context": "Title: Feed-Forward Neural Networks as a Mixed-Integer Program\nAbstract: Deep neural networks (DNNs) are widely studied in various applications. A DNN consists of layers of neurons that compute affine combinations, apply nonlinear operations, and produce corresponding activations. The rectified linear unit (ReLU) is a typical nonlinear operator, outputting the max of its input and zero. In scenarios like max pooling, where multiple input values are involved, a fixed-parameter DNN can be modeled as a mixed-integer program (MIP). This formulation, with continuous variables representing unit outputs and binary variables for ReLU activation, finds applications across diverse domains. This study explores the formulation of trained ReLU neurons as MIP and applies MIP models for training neural networks (NNs). Specifically, it investigates interactions between MIP techniques and various NN architectures, including binary DNNs (employing step activation functions) and binarized DNNs (with weights and activations limited to $-1,0,+1$). The research focuses on traini",
    "path": "papers/24/02/2402.06697.json",
    "total_tokens": 1002,
    "translated_title": "前馈神经网络作为混合整数规划",
    "translated_abstract": "深度神经网络(DNN)在各个应用领域都得到了广泛的研究。DNN由神经元层组成，计算仿射组合，应用非线性操作，并产生相应的激活。修正的线性单元(ReLU)是一种典型的非线性运算符，输出其输入和零的最大值。在像最大池化这样涉及多个输入值的场景中，固定参数的DNN可以被建模为混合整数规划(MIP)。这种形式，使用连续变量表示单元输出和ReLU激活的二进制变量，可以在不同领域中找到应用。本研究探讨了训练的ReLU神经元作为MIP的形式，并将MIP模型应用于训练神经网络(NN)。具体而言，它研究了MIP技术和不同的NN架构之间的相互作用，包括二进制DNN(采用阶梯激活函数)和二值化DNN(权重和激活限制为$-1,0,+1$)。该研究重点关注训练前馈神经网络中的混合整数规划。",
    "tldr": "这项研究探索了将训练的修正线性单元(ReLU)神经元作为混合整数规划(MIP)的形式，并将MIP模型应用于训练神经网络。研究发现MIP技术在不同的神经网络架构中具有广泛的应用潜力，包括二进制DNN和二值化DNN。",
    "en_tdlr": "This study explores the formulation of trained rectified linear unit (ReLU) neurons as mixed-integer programs (MIP) and applies MIP models for training neural networks. The research finds that MIP techniques have wide potential applications in different neural network architectures, including binary DNNs and binarized DNNs."
}