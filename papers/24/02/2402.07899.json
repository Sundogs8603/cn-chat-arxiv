{
    "title": "A systematic investigation of learnability from single child linguistic input",
    "abstract": "Language models (LMs) have demonstrated remarkable proficiency in generating linguistically coherent text, sparking discussions about their relevance to understanding human language learnability. However, a significant gap exists between the training data for these models and the linguistic input a child receives. LMs are typically trained on data that is orders of magnitude larger and fundamentally different from child-directed speech (Warstadt and Bowman, 2022; Warstadt et al., 2023; Frank, 2023a). Addressing this discrepancy, our research focuses on training LMs on subsets of a single child's linguistic input. Previously, Wang, Vong, Kim, and Lake (2023) found that LMs trained in this setting can form syntactic and semantic word clusters and develop sensitivity to certain linguistic phenomena, but they only considered LSTMs and simpler neural networks trained from just one single-child dataset. Here, to examine the robustness of learnability from single-child input, we systematicall",
    "link": "https://arxiv.org/abs/2402.07899",
    "context": "Title: A systematic investigation of learnability from single child linguistic input\nAbstract: Language models (LMs) have demonstrated remarkable proficiency in generating linguistically coherent text, sparking discussions about their relevance to understanding human language learnability. However, a significant gap exists between the training data for these models and the linguistic input a child receives. LMs are typically trained on data that is orders of magnitude larger and fundamentally different from child-directed speech (Warstadt and Bowman, 2022; Warstadt et al., 2023; Frank, 2023a). Addressing this discrepancy, our research focuses on training LMs on subsets of a single child's linguistic input. Previously, Wang, Vong, Kim, and Lake (2023) found that LMs trained in this setting can form syntactic and semantic word clusters and develop sensitivity to certain linguistic phenomena, but they only considered LSTMs and simpler neural networks trained from just one single-child dataset. Here, to examine the robustness of learnability from single-child input, we systematicall",
    "path": "papers/24/02/2402.07899.json",
    "total_tokens": 876,
    "translated_title": "从单一儿童语言输入的可学习性的系统调查",
    "translated_abstract": "语言模型（LM）在生成语言连贯文本方面表现出了 remarkable proficiency，引发了关于它们与人类语言可学习性的相关讨论。然而，这些模型的训练数据与儿童接收到的语言输入之间存在着显著差距。LMs通常在数量级上更大且本质与儿童语言输入不同的数据上进行训练。针对这一差距，我们的研究侧重于在单个儿童语言输入的子集上训练LMs。先前的研究发现，在这种设置下训练的LMs可以形成句法和语义词群，并对某些语言现象具有敏感性。然而，这些研究仅考虑了仅使用一个单一儿童数据集训练的LSTMs和更简单的神经网络。为了检验从单一儿童输入可学习性的鲁棒性，我们系统地…",
    "tldr": "我们的研究探索了用单个儿童的语言输入训练语言模型的可学习性，我们发现这种设置下的语言模型能够形成句法和语义词群，并对某些语言现象具有敏感性。",
    "en_tdlr": "Our research investigates the learnability of language models trained on single-child linguistic input, finding that such models can form syntactic and semantic word clusters and exhibit sensitivity to certain linguistic phenomena."
}