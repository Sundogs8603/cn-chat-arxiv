{
    "title": "QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning",
    "abstract": "arXiv:2402.10462v1 Announce Type: cross  Abstract: Finetuning large language models requires huge GPU memory, restricting the choice to acquire Larger models. While the quantized version of the Low-Rank Adaptation technique, named QLoRA, significantly alleviates this issue, finding the efficient LoRA rank is still challenging. Moreover, QLoRA is trained on a pre-defined rank and, therefore, cannot be reconfigured for its lower ranks without requiring further fine-tuning steps. This paper proposes QDyLoRA -Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach for dynamic low-rank adaptation. Motivated by Dynamic LoRA, QDyLoRA is able to efficiently finetune LLMs on a set of pre-defined LoRA ranks. QDyLoRA enables fine-tuning Falcon-40b for ranks 1 to 64 on a single 32 GB V100-GPU through one round of fine-tuning. Experimental results show that QDyLoRA is competitive to QLoRA and outperforms when employing its optimal rank.",
    "link": "https://arxiv.org/abs/2402.10462",
    "context": "Title: QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning\nAbstract: arXiv:2402.10462v1 Announce Type: cross  Abstract: Finetuning large language models requires huge GPU memory, restricting the choice to acquire Larger models. While the quantized version of the Low-Rank Adaptation technique, named QLoRA, significantly alleviates this issue, finding the efficient LoRA rank is still challenging. Moreover, QLoRA is trained on a pre-defined rank and, therefore, cannot be reconfigured for its lower ranks without requiring further fine-tuning steps. This paper proposes QDyLoRA -Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach for dynamic low-rank adaptation. Motivated by Dynamic LoRA, QDyLoRA is able to efficiently finetune LLMs on a set of pre-defined LoRA ranks. QDyLoRA enables fine-tuning Falcon-40b for ranks 1 to 64 on a single 32 GB V100-GPU through one round of fine-tuning. Experimental results show that QDyLoRA is competitive to QLoRA and outperforms when employing its optimal rank.",
    "path": "papers/24/02/2402.10462.json",
    "total_tokens": 945,
    "translated_title": "QDyLoRA: 高效的大型语言模型调优的量化动态低秩适应",
    "translated_abstract": "Finetuning大型语言模型需要巨大的GPU内存，限制了获取更大模型的选择。虽然命名为QLoRA的低秩适应技术的量化版本显著缓解了这一问题，但是找到高效的LoRA秩仍然具有挑战性。此外，QLoRA是在预定义的秩上训练的，因此，在不需要进一步微调步骤的情况下无法重新配置为其较低的秩。本文提出了QDyLoRA-Quantized Dynamic Low-Rank Adaptation-，作为一种用于动态低秩适应的高效量化方法。受Dynamic LoRA的启发，QDyLoRA能够在一组预定义的LoRA秩上有效地微调LLMs。通过一轮微调，QDyLoRA能够在单个32 GB V100-GPU上为1到64个秩的Falcon-40b进行微调。实验结果表明，QDyLoRA与QLoRA具有竞争力，在使用其最佳秩时表现优越。",
    "tldr": "本论文提出了一种名为QDyLoRA的高效量化动态低秩适应方法，能够在大型语言模型的预定义秩上实现有效微调，与QLoRA相竞争，并且在采用其最佳秩时表现更好。",
    "en_tdlr": "This paper proposes an efficient quantized dynamic low-rank adaptation method called QDyLoRA, which can effectively fine-tune on pre-defined ranks of large language models, competitive with QLoRA and outperforming when using its optimal rank."
}