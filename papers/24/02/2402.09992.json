{
    "title": "Risk-Sensitive Soft Actor-Critic for Robust Deep Reinforcement Learning under Distribution Shifts",
    "abstract": "arXiv:2402.09992v1 Announce Type: new  Abstract: We study the robustness of deep reinforcement learning algorithms against distribution shifts within contextual multi-stage stochastic combinatorial optimization problems from the operations research domain. In this context, risk-sensitive algorithms promise to learn robust policies. While this field is of general interest to the reinforcement learning community, most studies up-to-date focus on theoretical results rather than real-world performance. With this work, we aim to bridge this gap by formally deriving a novel risk-sensitive deep reinforcement learning algorithm while providing numerical evidence for its efficacy. Specifically, we introduce discrete Soft Actor-Critic for the entropic risk measure by deriving a version of the Bellman equation for the respective Q-values. We establish a corresponding policy improvement result and infer a practical algorithm. We introduce an environment that represents typical contextual multi-sta",
    "link": "https://arxiv.org/abs/2402.09992",
    "context": "Title: Risk-Sensitive Soft Actor-Critic for Robust Deep Reinforcement Learning under Distribution Shifts\nAbstract: arXiv:2402.09992v1 Announce Type: new  Abstract: We study the robustness of deep reinforcement learning algorithms against distribution shifts within contextual multi-stage stochastic combinatorial optimization problems from the operations research domain. In this context, risk-sensitive algorithms promise to learn robust policies. While this field is of general interest to the reinforcement learning community, most studies up-to-date focus on theoretical results rather than real-world performance. With this work, we aim to bridge this gap by formally deriving a novel risk-sensitive deep reinforcement learning algorithm while providing numerical evidence for its efficacy. Specifically, we introduce discrete Soft Actor-Critic for the entropic risk measure by deriving a version of the Bellman equation for the respective Q-values. We establish a corresponding policy improvement result and infer a practical algorithm. We introduce an environment that represents typical contextual multi-sta",
    "path": "papers/24/02/2402.09992.json",
    "total_tokens": 949,
    "translated_title": "风险敏感的软策演员-评论家算法在分布偏移下的鲁棒深度强化学习中的应用",
    "translated_abstract": "我们研究了在运营研究领域的上下文多阶段随机组合优化问题中，深度强化学习算法对于分布偏移的鲁棒性。在这个背景下，风险敏感算法可以学习到鲁棒的策略。尽管这个领域对强化学习社区非常重要，但大部分研究都着重于理论结果而不是实际性能。本文的目标是填补这一空白，通过正式推导出一种新颖的风险敏感的深度强化学习算法，并提供其有效性的数值证据。具体地，我们通过导出相应的Q值的Bellman方程的版本，引入了离散式的软策演员-评论家算法来进行基于熵风险度量的策略学习。我们建立了一个相应的策略改进结果并推导出一个实际的算法。我们还引入了一个典型的上下文多阶段环境。",
    "tldr": "本论文研究了在运营研究领域中，深度强化学习算法在面对分布偏移时的鲁棒性。通过推导出一种风险敏感的深度强化学习算法，并通过数值证据验证其有效性，填补了这一领域实际性能研究的空白。"
}