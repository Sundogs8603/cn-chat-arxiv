{
    "title": "Testing Calibration in Subquadratic Time",
    "abstract": "arXiv:2402.13187v1 Announce Type: new  Abstract: In the recent literature on machine learning and decision making, calibration has emerged as a desirable and widely-studied statistical property of the outputs of binary prediction models. However, the algorithmic aspects of measuring model calibration have remained relatively less well-explored. Motivated by [BGHN23], which proposed a rigorous framework for measuring distances to calibration, we initiate the algorithmic study of calibration through the lens of property testing. We define the problem of calibration testing from samples where given $n$ draws from a distribution $\\mathcal{D}$ on (predictions, binary outcomes), our goal is to distinguish between the case where $\\mathcal{D}$ is perfectly calibrated, and the case where $\\mathcal{D}$ is $\\varepsilon$-far from calibration.   We design an algorithm based on approximate linear programming, which solves calibration testing information-theoretically optimally (up to constant factor",
    "link": "https://arxiv.org/abs/2402.13187",
    "context": "Title: Testing Calibration in Subquadratic Time\nAbstract: arXiv:2402.13187v1 Announce Type: new  Abstract: In the recent literature on machine learning and decision making, calibration has emerged as a desirable and widely-studied statistical property of the outputs of binary prediction models. However, the algorithmic aspects of measuring model calibration have remained relatively less well-explored. Motivated by [BGHN23], which proposed a rigorous framework for measuring distances to calibration, we initiate the algorithmic study of calibration through the lens of property testing. We define the problem of calibration testing from samples where given $n$ draws from a distribution $\\mathcal{D}$ on (predictions, binary outcomes), our goal is to distinguish between the case where $\\mathcal{D}$ is perfectly calibrated, and the case where $\\mathcal{D}$ is $\\varepsilon$-far from calibration.   We design an algorithm based on approximate linear programming, which solves calibration testing information-theoretically optimally (up to constant factor",
    "path": "papers/24/02/2402.13187.json",
    "total_tokens": 820,
    "translated_title": "在次线性时间内测试校准性",
    "translated_abstract": "在最近的机器学习和决策制定文献中，校准性已经成为二元预测模型输出的一个值得期望和广泛研究的统计性质。然而，测量模型校准性的算法方面仍然相对较少被探索。在论文 [BGHN23] 的启发下，该论文提出了一个严格的框架来衡量到校准性的距离，我们通过属性测试的视角引入了校准性研究的算法方面。我们定义了从样本中进行校准性测试的问题，其中从分布 $\\mathcal{D}$（预测，二元结果）中给出 $n$ 次抽样，我们的目标是区分 $\\mathcal{D}$ 完全校准和 $\\mathcal{D}$ 距离校准性为 $\\varepsilon$ 的情况。",
    "tldr": "该论文通过属性测试算法方面的研究，提出了一种基于近似线性规划的算法，可以在信息理论上最优地解决校准性测试问题（最多一个常数倍数）。",
    "en_tdlr": "This paper introduces an algorithmic study of calibration through property testing, proposing an algorithm based on approximate linear programming that information-theoretically optimally solves the calibration testing problem (up to a constant factor)."
}