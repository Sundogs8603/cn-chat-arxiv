{
    "title": "SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models",
    "abstract": "In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount. To meet this crucial need, we propose \\emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile functionalities.SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice. To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation. Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility. Our e",
    "link": "https://arxiv.org/abs/2402.05044",
    "context": "Title: SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models\nAbstract: In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount. To meet this crucial need, we propose \\emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile functionalities.SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice. To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation. Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility. Our e",
    "path": "papers/24/02/2402.05044.json",
    "total_tokens": 888,
    "translated_title": "SALAD-Bench: 一个针对大语言模型的层次化和全面性安全基准",
    "translated_abstract": "在快速发展的大语言模型（LLM）领域中，确保强大的安全措施至关重要。为了满足这一关键需求，我们提出了一种特别设计用于评估LLMs、攻击和防御方法的安全基准，称为SALAD-Bench。SALAD-Bench通过其大规模、丰富多样的特性，以及跨三个层次的细致分类和多功能性，超越了传统基准。SALAD-Bench通过对标准查询和复杂查询（包括攻击、防御修改和多项选择）的精心设计，有效管理其固有的复杂性。为了确保无缝可靠的评估，我们引入了一种创新的评估器：基于LLM的MD-Judge，专注于攻击增强查询的问答对评估。以上组件将SALAD-Bench从标准的LLM安全评估扩展到了LLM攻击和防御方法评估，确保了联合目标的实用性。",
    "tldr": "SALAD-Bench是一个针对大语言模型的全面安全基准，通过其大规模、丰富的分类和多功能性，以及对攻击和防御方法的评估，实现了对LLMs的有效管理和保护。"
}