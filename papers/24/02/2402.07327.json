{
    "title": "Multi-Modal Emotion Recognition by Text, Speech and Video Using Pretrained Transformers",
    "abstract": "Due to the complex nature of human emotions and the diversity of emotion representation methods in humans, emotion recognition is a challenging field. In this research, three input modalities, namely text, audio (speech), and video, are employed to generate multimodal feature vectors. For generating features for each of these modalities, pre-trained Transformer models with fine-tuning are utilized. In each modality, a Transformer model is used with transfer learning to extract feature and emotional structure. These features are then fused together, and emotion recognition is performed using a classifier. To select an appropriate fusion method and classifier, various feature-level and decision-level fusion techniques have been experimented with, and ultimately, the best model, which combines feature-level fusion by concatenating feature vectors and classification using a Support Vector Machine on the IEMOCAP multimodal dataset, achieves an accuracy of 75.42%. Keywords: Multimodal Emotio",
    "link": "https://arxiv.org/abs/2402.07327",
    "context": "Title: Multi-Modal Emotion Recognition by Text, Speech and Video Using Pretrained Transformers\nAbstract: Due to the complex nature of human emotions and the diversity of emotion representation methods in humans, emotion recognition is a challenging field. In this research, three input modalities, namely text, audio (speech), and video, are employed to generate multimodal feature vectors. For generating features for each of these modalities, pre-trained Transformer models with fine-tuning are utilized. In each modality, a Transformer model is used with transfer learning to extract feature and emotional structure. These features are then fused together, and emotion recognition is performed using a classifier. To select an appropriate fusion method and classifier, various feature-level and decision-level fusion techniques have been experimented with, and ultimately, the best model, which combines feature-level fusion by concatenating feature vectors and classification using a Support Vector Machine on the IEMOCAP multimodal dataset, achieves an accuracy of 75.42%. Keywords: Multimodal Emotio",
    "path": "papers/24/02/2402.07327.json",
    "total_tokens": 845,
    "translated_title": "使用预训练的Transformer模型进行文本、语音和视频的多模态情感识别",
    "translated_abstract": "由于人类情感的复杂性和表达方法的多样性，情感识别是一个具有挑战性的领域。本研究利用文本、音频（语音）和视频三种输入模态生成多模态特征向量。为了为每种模态生成特征，使用了经过微调的预训练Transformer模型。在每个模态中，使用了一个带有迁移学习的Transformer模型来提取特征和情感结构。这些特征然后被融合在一起，并使用分类器进行情感识别。为了选择合适的融合方法和分类器，尝试了各种特征级和决策级融合技术，并最终在IEMOCAP多模态数据集上通过特征级向量连接和支持向量机分类的方法实现了75.42%的准确率。",
    "tldr": "本研究使用了文本、语音和视频三种输入模态，并利用预训练的Transformer模型进行特征提取和情感结构分析。通过特征级融合和支持向量机分类，实现了75.42%的情感识别准确率。",
    "en_tdlr": "This research utilizes text, speech, and video as input modalities and applies pretrained Transformer models for feature extraction and emotional analysis. By combining features at the feature level and using a Support Vector Machine classifier, an accuracy of 75.42% is achieved in emotion recognition."
}