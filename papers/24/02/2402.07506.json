{
    "title": "NeuralSentinel: Safeguarding Neural Network Reliability and Trustworthiness",
    "abstract": "The usage of Artificial Intelligence (AI) systems has increased exponentially, thanks to their ability to reduce the amount of data to be analyzed, the user efforts and preserving a high rate of accuracy. However, introducing this new element in the loop has converted them into attacked points that can compromise the reliability of the systems. This new scenario has raised crucial challenges regarding the reliability and trustworthiness of the AI models, as well as about the uncertainties in their response decisions, becoming even more crucial when applied in critical domains such as healthcare, chemical, electrical plants, etc. To contain these issues, in this paper, we present NeuralSentinel (NS), a tool able to validate the reliability and trustworthiness of AI models. This tool combines attack and defence strategies and explainability concepts to stress an AI model and help non-expert staff increase their confidence in this new system by understanding the model decisions. NS provid",
    "link": "https://arxiv.org/abs/2402.07506",
    "context": "Title: NeuralSentinel: Safeguarding Neural Network Reliability and Trustworthiness\nAbstract: The usage of Artificial Intelligence (AI) systems has increased exponentially, thanks to their ability to reduce the amount of data to be analyzed, the user efforts and preserving a high rate of accuracy. However, introducing this new element in the loop has converted them into attacked points that can compromise the reliability of the systems. This new scenario has raised crucial challenges regarding the reliability and trustworthiness of the AI models, as well as about the uncertainties in their response decisions, becoming even more crucial when applied in critical domains such as healthcare, chemical, electrical plants, etc. To contain these issues, in this paper, we present NeuralSentinel (NS), a tool able to validate the reliability and trustworthiness of AI models. This tool combines attack and defence strategies and explainability concepts to stress an AI model and help non-expert staff increase their confidence in this new system by understanding the model decisions. NS provid",
    "path": "papers/24/02/2402.07506.json",
    "total_tokens": 839,
    "translated_title": "NeuralSentinel: 保障神经网络的可靠性和可信度",
    "translated_abstract": "人工智能系统的使用量呈指数增长，由于其能够减少待分析的数据量，减轻用户的工作量并保持高准确率。然而，将这一新元素引入系统中却成为了攻击点，可能会危及系统的可靠性。这个新的情景提出了关于人工智能模型的可靠性和可信度的重要挑战，以及关于其响应决策的不确定性，尤其是在应用于关键领域如医疗保健、化学和电力等。为了解决这些问题，本文介绍了NeuralSentinel（NS），一个能够验证人工智能模型可靠性和可信度的工具。该工具结合了攻击和防御策略以及可解释性概念，可以对人工智能模型进行压力测试，并帮助非专业人员通过理解模型的决策来增强对这一新系统的信任度。",
    "tldr": "NeuralSentinel是一个工具，能够验证人工智能模型的可靠性和可信度，并帮助非专业人员通过理解模型的决策来增强对这一新系统的信任度。"
}