{
    "title": "Text-to-Code Generation with Modality-relative Pre-training",
    "abstract": "Large pre-trained language models have recently been expanded and applied to programming language tasks with great success, often through further pre-training of a strictly-natural language model--where training sequences typically contain both natural and (linearised) programming language. Such approaches effectively map both modalities of the sequence into the same embedding space. However, programming language keywords (e.g. ``while'') often have very strictly defined semantics. As such, transfer learning from their natural language usage may not necessarily be beneficial to their code application and vise versa. Assuming an already pre-trained language model, in this work we investigate how sequence tokens can be adapted and represented differently, depending on which modality they belong to, and to the ultimate benefit of the downstream task. We experiment with separating embedding spaces between modalities during further model pre-training with modality-relative training objectiv",
    "link": "https://arxiv.org/abs/2402.05783",
    "context": "Title: Text-to-Code Generation with Modality-relative Pre-training\nAbstract: Large pre-trained language models have recently been expanded and applied to programming language tasks with great success, often through further pre-training of a strictly-natural language model--where training sequences typically contain both natural and (linearised) programming language. Such approaches effectively map both modalities of the sequence into the same embedding space. However, programming language keywords (e.g. ``while'') often have very strictly defined semantics. As such, transfer learning from their natural language usage may not necessarily be beneficial to their code application and vise versa. Assuming an already pre-trained language model, in this work we investigate how sequence tokens can be adapted and represented differently, depending on which modality they belong to, and to the ultimate benefit of the downstream task. We experiment with separating embedding spaces between modalities during further model pre-training with modality-relative training objectiv",
    "path": "papers/24/02/2402.05783.json",
    "total_tokens": 802,
    "translated_title": "使用模态相对预训练的方法生成文本到代码的转换",
    "translated_abstract": "最近，大型预训练语言模型被广泛应用于编程语言任务，并取得了巨大成功，通常通过进一步预先训练严格自然语言模型，其中训练序列通常包含自然语言和（线性化的）编程语言。这种方法有效地将序列的两种模态映射到同一嵌入空间中。然而，编程语言关键词（例如“while”）往往具有非常严格的定义语义。因此，从自然语言用法进行的迁移学习对其代码应用可能并不一定有益，反之亦然。在本研究中，我们假设已经预先训练好的语言模型，探讨了如何根据它们所属的模态不同来调整和表示序列标记，并最终有益于下游任务。我们在模态相对训练目标的进一步模型预训练中尝试了在模态之间分离嵌入空间的方法。",
    "tldr": "本论文研究了如何根据不同的模态调整和表示序列标记，以进一步提高文本到代码生成的效果。",
    "en_tdlr": "This paper investigates how to adapt and represent sequence tokens differently based on their modalities, to further improve text-to-code generation."
}