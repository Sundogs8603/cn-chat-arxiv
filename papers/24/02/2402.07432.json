{
    "title": "Intrinsic Task-based Evaluation for Referring Expression Generation",
    "abstract": "Recently, a human evaluation study of Referring Expression Generation (REG) models had an unexpected conclusion: on \\textsc{webnlg}, Referring Expressions (REs) generated by the state-of-the-art neural models were not only indistinguishable from the REs in \\textsc{webnlg} but also from the REs generated by a simple rule-based system. Here, we argue that this limitation could stem from the use of a purely ratings-based human evaluation (which is a common practice in Natural Language Generation). To investigate these issues, we propose an intrinsic task-based evaluation for REG models, in which, in addition to rating the quality of REs, participants were asked to accomplish two meta-level tasks. One of these tasks concerns the referential success of each RE; the other task asks participants to suggest a better alternative for each RE. The outcomes suggest that, in comparison to previous evaluations, the new evaluation protocol assesses the performance of each REG model more comprehensive",
    "link": "https://arxiv.org/abs/2402.07432",
    "context": "Title: Intrinsic Task-based Evaluation for Referring Expression Generation\nAbstract: Recently, a human evaluation study of Referring Expression Generation (REG) models had an unexpected conclusion: on \\textsc{webnlg}, Referring Expressions (REs) generated by the state-of-the-art neural models were not only indistinguishable from the REs in \\textsc{webnlg} but also from the REs generated by a simple rule-based system. Here, we argue that this limitation could stem from the use of a purely ratings-based human evaluation (which is a common practice in Natural Language Generation). To investigate these issues, we propose an intrinsic task-based evaluation for REG models, in which, in addition to rating the quality of REs, participants were asked to accomplish two meta-level tasks. One of these tasks concerns the referential success of each RE; the other task asks participants to suggest a better alternative for each RE. The outcomes suggest that, in comparison to previous evaluations, the new evaluation protocol assesses the performance of each REG model more comprehensive",
    "path": "papers/24/02/2402.07432.json",
    "total_tokens": 879,
    "translated_title": "内在任务驱动的分发表达生成评估",
    "translated_abstract": "最近，对于分发表达生成（REG）模型的人工评估研究得出了一个令人意外的结论：在\\textsc{webnlg}上，最先进的神经模型生成的分发表达（REs）不仅与\\textsc{webnlg}中的REs无法区分，而且与简单的基于规则的系统生成的REs也无法区分。在这里，我们认为这个局限可能源于纯评分的人工评估方法（这是自然语言生成中的常见实践）。为了调查这些问题，我们提出了一种针对REG模型的内在任务驱动评估方法，除了评估REs的质量外，参与者还需要完成两个元级任务。其中一个任务涉及每个RE的引用成功程度，另一个任务要求参与者为每个RE提出更好的替代方案。结果表明，与之前的评估相比，新的评估协议更全面地评估了每个REG模型的性能。",
    "tldr": "该论文提出了一种内在任务驱动的评估方法，用于评估分发表达生成（REG）模型。该方法不仅评估了分发表达的质量，还通过两个元级任务评估了模型的引用成功程度和提出替代方案的能力。",
    "en_tdlr": "This paper proposes an intrinsic task-based evaluation for Referring Expression Generation (REG) models. The evaluation assesses not only the quality of referring expressions, but also the referential success and the capability of suggesting alternative expressions."
}