{
    "title": "Approximate Attributions for Off-the-Shelf Siamese Transformers",
    "abstract": "Siamese encoders such as sentence transformers are among the least understood deep models. Established attribution methods cannot tackle this model class since it compares two inputs rather than processing a single one. To address this gap, we have recently proposed an attribution method specifically for Siamese encoders (M\\\"oller et al., 2023). However, it requires models to be adjusted and fine-tuned and therefore cannot be directly applied to off-the-shelf models. In this work, we reassess these restrictions and propose (i) a model with exact attribution ability that retains the original model's predictive performance and (ii) a way to compute approximate attributions for off-the-shelf models. We extensively compare approximate and exact attributions and use them to analyze the models' attendance to different linguistic aspects. We gain insights into which syntactic roles Siamese transformers attend to, confirm that they mostly ignore negation, explore how they judge semantically op",
    "link": "https://arxiv.org/abs/2402.02883",
    "context": "Title: Approximate Attributions for Off-the-Shelf Siamese Transformers\nAbstract: Siamese encoders such as sentence transformers are among the least understood deep models. Established attribution methods cannot tackle this model class since it compares two inputs rather than processing a single one. To address this gap, we have recently proposed an attribution method specifically for Siamese encoders (M\\\"oller et al., 2023). However, it requires models to be adjusted and fine-tuned and therefore cannot be directly applied to off-the-shelf models. In this work, we reassess these restrictions and propose (i) a model with exact attribution ability that retains the original model's predictive performance and (ii) a way to compute approximate attributions for off-the-shelf models. We extensively compare approximate and exact attributions and use them to analyze the models' attendance to different linguistic aspects. We gain insights into which syntactic roles Siamese transformers attend to, confirm that they mostly ignore negation, explore how they judge semantically op",
    "path": "papers/24/02/2402.02883.json",
    "total_tokens": 1039,
    "translated_title": "翻译过的论文标题: 适用于现有孪生变压器的近似归因方法",
    "translated_abstract": "翻译过的论文摘要: 孪生编码器如句子变换器是目前最不理解的深度模型之一。现有的归因方法无法处理这种模型类别，因为它们比较两个输入而不是处理单个输入。为了弥补这一空白，我们最近提出了一种专门针对孪生编码器的归因方法(Moller等，2023)。然而，它需要对模型进行调整和微调，因此无法直接应用于现有模型。在这项工作中，我们重新评估了这些限制，并提出了(i)一种具有准确归因能力且保留原模型预测性能的模型，以及(ii)一种计算现有模型近似归因的方法。我们广泛比较了近似和准确归因，并使用它们来分析模型对不同语言方面的关注。我们深入了解了孪生变压器对句法角色的关注程度，确认它们主要忽略否定，并探索了它们如何判断语义上的差异。",
    "tldr": "中文总结出的一句话要点: 本文介绍了一种适用于现有孪生变压器的近似归因方法，该方法在保留原模型性能的同时实现了准确归因能力。我们通过比较近似和准确归因，分析了模型对不同语言方面的关注，并发现孪生变压器主要忽略否定，同时深入研究了它们对句法角色的关注程度，以及如何判断语义上的差异。"
}