{
    "title": "Speaking in Wavelet Domain: A Simple and Efficient Approach to Speed up Speech Diffusion Model",
    "abstract": "arXiv:2402.10642v1 Announce Type: cross  Abstract: Recently, Denoising Diffusion Probabilistic Models (DDPMs) have attained leading performances across a diverse range of generative tasks. However, in the field of speech synthesis, although DDPMs exhibit impressive performance, their long training duration and substantial inference costs hinder practical deployment. Existing approaches primarily focus on enhancing inference speed, while approaches to accelerate training a key factor in the costs associated with adding or customizing voices often necessitate complex modifications to the model, compromising their universal applicability. To address the aforementioned challenges, we propose an inquiry: is it possible to enhance the training/inference speed and performance of DDPMs by modifying the speech signal itself? In this paper, we double the training and inference speed of Speech DDPMs by simply redirecting the generative target to the wavelet domain. This method not only achieves c",
    "link": "https://arxiv.org/abs/2402.10642",
    "context": "Title: Speaking in Wavelet Domain: A Simple and Efficient Approach to Speed up Speech Diffusion Model\nAbstract: arXiv:2402.10642v1 Announce Type: cross  Abstract: Recently, Denoising Diffusion Probabilistic Models (DDPMs) have attained leading performances across a diverse range of generative tasks. However, in the field of speech synthesis, although DDPMs exhibit impressive performance, their long training duration and substantial inference costs hinder practical deployment. Existing approaches primarily focus on enhancing inference speed, while approaches to accelerate training a key factor in the costs associated with adding or customizing voices often necessitate complex modifications to the model, compromising their universal applicability. To address the aforementioned challenges, we propose an inquiry: is it possible to enhance the training/inference speed and performance of DDPMs by modifying the speech signal itself? In this paper, we double the training and inference speed of Speech DDPMs by simply redirecting the generative target to the wavelet domain. This method not only achieves c",
    "path": "papers/24/02/2402.10642.json",
    "total_tokens": 765,
    "translated_title": "在小波域说话：加速语音扩散模型的简单高效方法",
    "translated_abstract": "最近，去噪扩散概率模型（DDPMs）在各种生成任务中表现出色。然而，在语音合成领域，尽管DDPMs表现出色，但其长时间训练和大量推理成本阻碍了实际部署。现有方法主要集中在增强推理速度，而加速训练的方法通常需要对模型进行复杂修改，从而损害其通用性。为了解决上述挑战，我们提出了一个问题：通过修改语音信号本身，是否可能提高DDPMs的训练/推理速度和性能？在本文中，我们通过简单地将生成目标重定向到小波域，将语音DDPMs的训练和推理速度提高了一倍。该方法不仅取得了…",
    "tldr": "通过将生成目标重定向到小波域，我们成功将语音DDPMs的训练和推理速度提高了一倍。"
}