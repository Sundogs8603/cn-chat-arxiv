{
    "title": "Triple-Encoders: Representations That Fire Together, Wire Together",
    "abstract": "arXiv:2402.12332v1 Announce Type: new  Abstract: Search-based dialog models typically re-encode the dialog history at every turn, incurring high cost. Curved Contrastive Learning, a representation learning method that encodes relative distances between utterances into the embedding space via a bi-encoder, has recently shown promising results for dialog modeling at far superior efficiency. While high efficiency is achieved through independently encoding utterances, this ignores the importance of contextualization. To overcome this issue, this study introduces triple-encoders, which efficiently compute distributed utterance mixtures from these independently encoded utterances through a novel hebbian inspired co-occurrence learning objective without using any weights. Empirically, we find that triple-encoders lead to a substantial improvement over bi-encoders, and even to better zero-shot generalization than single-vector representation models without requiring re-encoding. Our code/model",
    "link": "https://arxiv.org/abs/2402.12332",
    "context": "Title: Triple-Encoders: Representations That Fire Together, Wire Together\nAbstract: arXiv:2402.12332v1 Announce Type: new  Abstract: Search-based dialog models typically re-encode the dialog history at every turn, incurring high cost. Curved Contrastive Learning, a representation learning method that encodes relative distances between utterances into the embedding space via a bi-encoder, has recently shown promising results for dialog modeling at far superior efficiency. While high efficiency is achieved through independently encoding utterances, this ignores the importance of contextualization. To overcome this issue, this study introduces triple-encoders, which efficiently compute distributed utterance mixtures from these independently encoded utterances through a novel hebbian inspired co-occurrence learning objective without using any weights. Empirically, we find that triple-encoders lead to a substantial improvement over bi-encoders, and even to better zero-shot generalization than single-vector representation models without requiring re-encoding. Our code/model",
    "path": "papers/24/02/2402.12332.json",
    "total_tokens": 780,
    "translated_title": "三重编码器：共同激活的表示一起连接",
    "translated_abstract": "搜索导向的对话模型通常在每个轮次重新对对话历史进行编码，造成高昂的成本。曲率对比学习是一种最近展示出对话建模远超效率的表示学习方法，通过双编码器将话语之间的相对距离编码到嵌入空间中。高效率是通过独立编码话语实现的，然而这忽略了上下文化的重要性。为了解决这个问题，本研究引入了三重编码器，通过一种新颖的Hebbian启发的共存学习目标，从这些独立编码的话语中高效计算分布式话语混合，而不使用任何权重。实证上，我们发现三重编码器在比编码器上有显著改进，甚至比单向量表示模型实现更好的零-shot泛化，而无需重新编码。",
    "tldr": "通过三重编码器计算话语混合，实现了对话模型的显着改进和零-shot泛化性能",
    "en_tdlr": "Significant improvement and zero-shot generalization of dialog models achieved through triple-encoders computing utterance mixtures."
}