{
    "title": "Data Distribution Distilled Generative Model for Generalized Zero-Shot Recognition",
    "abstract": "arXiv:2402.11424v1 Announce Type: cross  Abstract: In the realm of Zero-Shot Learning (ZSL), we address biases in Generalized Zero-Shot Learning (GZSL) models, which favor seen data. To counter this, we introduce an end-to-end generative GZSL framework called D$^3$GZSL. This framework respects seen and synthesized unseen data as in-distribution and out-of-distribution data, respectively, for a more balanced model. D$^3$GZSL comprises two core modules: in-distribution dual space distillation (ID$^2$SD) and out-of-distribution batch distillation (O$^2$DBD). ID$^2$SD aligns teacher-student outcomes in embedding and label spaces, enhancing learning coherence. O$^2$DBD introduces low-dimensional out-of-distribution representations per batch sample, capturing shared structures between seen and unseen categories. Our approach demonstrates its effectiveness across established GZSL benchmarks, seamlessly integrating into mainstream generative frameworks. Extensive experiments consistently showc",
    "link": "https://arxiv.org/abs/2402.11424",
    "context": "Title: Data Distribution Distilled Generative Model for Generalized Zero-Shot Recognition\nAbstract: arXiv:2402.11424v1 Announce Type: cross  Abstract: In the realm of Zero-Shot Learning (ZSL), we address biases in Generalized Zero-Shot Learning (GZSL) models, which favor seen data. To counter this, we introduce an end-to-end generative GZSL framework called D$^3$GZSL. This framework respects seen and synthesized unseen data as in-distribution and out-of-distribution data, respectively, for a more balanced model. D$^3$GZSL comprises two core modules: in-distribution dual space distillation (ID$^2$SD) and out-of-distribution batch distillation (O$^2$DBD). ID$^2$SD aligns teacher-student outcomes in embedding and label spaces, enhancing learning coherence. O$^2$DBD introduces low-dimensional out-of-distribution representations per batch sample, capturing shared structures between seen and unseen categories. Our approach demonstrates its effectiveness across established GZSL benchmarks, seamlessly integrating into mainstream generative frameworks. Extensive experiments consistently showc",
    "path": "papers/24/02/2402.11424.json",
    "total_tokens": 960,
    "translated_title": "用于广义零样本识别的数据分布精馏生成模型",
    "translated_abstract": "在零样本学习领域中，我们解决了偏向已见数据的广义零样本学习模型中存在的偏见。为了应对这一问题，我们引入了一个端到端的生成式广义零样本学习框架，称为D$^3$GZSL。该框架将已见和合成的未见数据分别视为分布内和分布外数据，以获得更加平衡的模型。D$^3$GZSL包括两个核心模块：分布内双空间精馏（ID$^2$SD）和分布外批次精馏（O$^2$DBD）。ID$^2$SD在嵌入和标签空间中对师生输出进行对齐，增强了学习的一致性。O$^2$DBD引入了每个批次样本的低维分布外表示，捕捉了已见和未见类别之间的共享结构。我们的方法在已建立的广义零样本学习基准测试中展示了其有效性，并能够无缝集成到主流生成式框架中。大量实验一致地表明...",
    "tldr": "在广义零样本识别中解决已见数据偏见问题的D$^3$GZSL框架，通过引入分布内双空间精馏和分布外批次精馏模块，实现了更平衡的模型学习。",
    "en_tdlr": "D$^3$GZSL framework addresses biases towards seen data in Generalized Zero-Shot Learning (GZSL) by introducing in-distribution dual space distillation and out-of-distribution batch distillation modules, achieving a more balanced model learning."
}