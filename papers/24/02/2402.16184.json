{
    "title": "Deep Neural Network Initialization with Sparsity Inducing Activations",
    "abstract": "arXiv:2402.16184v1 Announce Type: new  Abstract: Inducing and leveraging sparse activations during training and inference is a promising avenue for improving the computational efficiency of deep networks, which is increasingly important as network sizes continue to grow and their application becomes more widespread. Here we use the large width Gaussian process limit to analyze the behaviour, at random initialization, of nonlinear activations that induce sparsity in the hidden outputs. A previously unreported form of training instability is proven for arguably two of the most natural candidates for hidden layer sparsification; those being a shifted ReLU ($\\phi(x)=\\max(0, x-\\tau)$ for $\\tau\\ge 0$) and soft thresholding ($\\phi(x)=0$ for $|x|\\le\\tau$ and $x-\\text{sign}(x)\\tau$ for $|x|>\\tau$). We show that this instability is overcome by clipping the nonlinear activation magnitude, at a level prescribed by the shape of the associated Gaussian process variance map. Numerical experiments ver",
    "link": "https://arxiv.org/abs/2402.16184",
    "context": "Title: Deep Neural Network Initialization with Sparsity Inducing Activations\nAbstract: arXiv:2402.16184v1 Announce Type: new  Abstract: Inducing and leveraging sparse activations during training and inference is a promising avenue for improving the computational efficiency of deep networks, which is increasingly important as network sizes continue to grow and their application becomes more widespread. Here we use the large width Gaussian process limit to analyze the behaviour, at random initialization, of nonlinear activations that induce sparsity in the hidden outputs. A previously unreported form of training instability is proven for arguably two of the most natural candidates for hidden layer sparsification; those being a shifted ReLU ($\\phi(x)=\\max(0, x-\\tau)$ for $\\tau\\ge 0$) and soft thresholding ($\\phi(x)=0$ for $|x|\\le\\tau$ and $x-\\text{sign}(x)\\tau$ for $|x|>\\tau$). We show that this instability is overcome by clipping the nonlinear activation magnitude, at a level prescribed by the shape of the associated Gaussian process variance map. Numerical experiments ver",
    "path": "papers/24/02/2402.16184.json",
    "total_tokens": 880,
    "translated_title": "用稀疏诱导激活初始化深度神经网络",
    "translated_abstract": "在训练和推理过程中诱导和利用稀疏激活是改善深度网络计算效率的一个有前途的途径，随着网络规模的不断增长和应用范围的扩大，这一点变得越来越重要。本文利用大宽高斯过程极限来分析非线性激活函数在随机初始化时诱导隐藏输出稀疏的行为。我们证明了先前未报道的训练不稳定形式，针对隐藏层稀疏化的两个最自然的候选者：移位ReLU（$\\phi(x)=\\max(0, x-\\tau)$，其中$\\tau\\ge 0$）和软阈值（$\\phi(x)=0$，当$|x|\\le\\tau$时，$x-\\text{sign}(x)\\tau$，当$|x|>\\tau$时）。我们展示了这种不稳定性通过将非线性激活幅度修剪到由相关高斯过程方差图的形状规定的水平上被克服。数值实验证明",
    "tldr": "本文利用大宽高斯过程极限分析随机初始化时诱导隐藏输出稀疏行为的激活函数，克服了训练不稳定性。",
    "en_tdlr": "This paper analyzes the behavior of inducing sparse hidden outputs with non-linear activations at random initialization using the large width Gaussian process limit, and overcomes the training instability."
}