{
    "title": "Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions Without the Question?",
    "abstract": "arXiv:2402.12483v1 Announce Type: new  Abstract: Multiple-choice question answering (MCQA) is often used to evaluate large language models (LLMs). To see if MCQA assesses LLMs as intended, we probe if LLMs can perform MCQA with choices-only prompts, where models must select the correct answer only from the choices. In three MCQA datasets and four LLMs, this prompt bests a majority baseline in 11/12 cases, with up to 0.33 accuracy gain. To help explain this behavior, we conduct an in-depth, black-box analysis on memorization, choice dynamics, and question inference. Our key findings are threefold. First, we find no evidence that the choices-only accuracy stems from memorization alone. Second, priors over individual choices do not fully explain choices-only accuracy, hinting that LLMs use the group dynamics of choices. Third, LLMs have some ability to infer a relevant question from choices, and surprisingly can sometimes even match the original question. We hope to motivate the use of st",
    "link": "https://arxiv.org/abs/2402.12483",
    "context": "Title: Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions Without the Question?\nAbstract: arXiv:2402.12483v1 Announce Type: new  Abstract: Multiple-choice question answering (MCQA) is often used to evaluate large language models (LLMs). To see if MCQA assesses LLMs as intended, we probe if LLMs can perform MCQA with choices-only prompts, where models must select the correct answer only from the choices. In three MCQA datasets and four LLMs, this prompt bests a majority baseline in 11/12 cases, with up to 0.33 accuracy gain. To help explain this behavior, we conduct an in-depth, black-box analysis on memorization, choice dynamics, and question inference. Our key findings are threefold. First, we find no evidence that the choices-only accuracy stems from memorization alone. Second, priors over individual choices do not fully explain choices-only accuracy, hinting that LLMs use the group dynamics of choices. Third, LLMs have some ability to infer a relevant question from choices, and surprisingly can sometimes even match the original question. We hope to motivate the use of st",
    "path": "papers/24/02/2402.12483.json",
    "total_tokens": 928,
    "translated_title": "文物还是绑架：LLMs如何在没有问题的情况下回答多项选择题？",
    "translated_abstract": "多项选择题回答（MCQA）通常用于评估大型语言模型（LLMs）。为了确定MCQA是否按预期评估LLMs，我们探究LLMs是否可以通过只有选项的提示来进行MCQA，其中模型必须仅从选项中选择正确答案。在三个MCQA数据集和四个LLMs中，这个提示在12个案例中的11个中优于多数基线，并可获得高达0.33的准确度提升。为了帮助解释这种行为，我们对记忆、选择动态和问题推理进行了深入的黑盒分析。我们的关键发现有三个。首先，我们发现没有证据表明只有选择的准确性仅源自记忆。其次，对单个选择的先验并不能完全解释只有选择的准确性，暗示LLMs使用选择的集体动态。第三，LLMs有一定能力从选择中推断出相关问题，并且令人惊讶地有时甚至可以匹配原始问题。我们希望鼓励利用这种方法。",
    "tldr": "LLMs能够在没有问题的情况下仅从选项中回答多项选择题，通过记忆、选择动态和问题推理进行黑盒分析，揭示了LLMs在选择性准确性方面的三个关键发现。",
    "en_tdlr": "LLMs can answer multiple-choice questions only from the choices without the question, and through black-box analysis on memorization, choice dynamics, and question inference, three key findings about LLMs' choices-only accuracy are revealed."
}