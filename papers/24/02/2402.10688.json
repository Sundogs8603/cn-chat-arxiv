{
    "title": "Opening the Black Box of Large Language Models: Two Views on Holistic Interpretability",
    "abstract": "arXiv:2402.10688v1 Announce Type: new  Abstract: As large language models (LLMs) grow more powerful, concerns around potential harms like toxicity, unfairness, and hallucination threaten user trust. Ensuring beneficial alignment of LLMs with human values through model alignment is thus critical yet challenging, requiring a deeper understanding of LLM behaviors and mechanisms. We propose opening the black box of LLMs through a framework of holistic interpretability encompassing complementary bottom-up and top-down perspectives. The bottom-up view, enabled by mechanistic interpretability, focuses on component functionalities and training dynamics. The top-down view utilizes representation engineering to analyze behaviors through hidden representations. In this paper, we review the landscape around mechanistic interpretability and representation engineering, summarizing approaches, discussing limitations and applications, and outlining future challenges in using these techniques to achiev",
    "link": "https://arxiv.org/abs/2402.10688",
    "context": "Title: Opening the Black Box of Large Language Models: Two Views on Holistic Interpretability\nAbstract: arXiv:2402.10688v1 Announce Type: new  Abstract: As large language models (LLMs) grow more powerful, concerns around potential harms like toxicity, unfairness, and hallucination threaten user trust. Ensuring beneficial alignment of LLMs with human values through model alignment is thus critical yet challenging, requiring a deeper understanding of LLM behaviors and mechanisms. We propose opening the black box of LLMs through a framework of holistic interpretability encompassing complementary bottom-up and top-down perspectives. The bottom-up view, enabled by mechanistic interpretability, focuses on component functionalities and training dynamics. The top-down view utilizes representation engineering to analyze behaviors through hidden representations. In this paper, we review the landscape around mechanistic interpretability and representation engineering, summarizing approaches, discussing limitations and applications, and outlining future challenges in using these techniques to achiev",
    "path": "papers/24/02/2402.10688.json",
    "total_tokens": 878,
    "translated_title": "打开大型语言模型的黑匣子：整体可解释性的两个视角",
    "translated_abstract": "随着大型语言模型(LLMs)变得越来越强大，人们对潜在伤害(如毒性、不公平和幻觉)的担忧威胁到用户的信任。通过模型对齐确保LLMs与人类价值观的有益契合因此至关重要，但具有挑战性，需要对LLMs的行为和机制有更深入的理解。我们提出通过一个涵盖互补的自下而上和自上而下视角的整体解释框架来打开LLMs的黑匣子。自下而上视角由机械解释能力实现，侧重于组件功能和训练动态。自上而下视角利用表示工程通过隐藏表示分析行为。在本文中，我们回顾了周围关于机械解释能力和表示工程的情况，总结了方法，讨论了限制和应用，并概述了将这些技术用于达到的未来挑战。",
    "tldr": "通过整体可解释性框架，本文提出了打开大型语言模型黑匣子的方法，包括自下而上的机械解释和自上而下的表示工程视角，有助于深入理解和应用LLMs的行为和机制。",
    "en_tdlr": "This paper proposes a method to open the black box of large language models through a framework of holistic interpretability, which includes bottom-up mechanistic interpretability and top-down representation engineering perspectives, aiding in a deeper understanding and application of LLMs' behaviors and mechanisms."
}