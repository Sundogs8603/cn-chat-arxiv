{
    "title": "Dynamic Graph Information Bottleneck",
    "abstract": "Dynamic Graphs widely exist in the real world, which carry complicated spatial and temporal feature patterns, challenging their representation learning. Dynamic Graph Neural Networks (DGNNs) have shown impressive predictive abilities by exploiting the intrinsic dynamics. However, DGNNs exhibit limited robustness, prone to adversarial attacks. This paper presents the novel Dynamic Graph Information Bottleneck (DGIB) framework to learn robust and discriminative representations. Leveraged by the Information Bottleneck (IB) principle, we first propose the expected optimal representations should satisfy the Minimal-Sufficient-Consensual (MSC) Condition. To compress redundant as well as conserve meritorious information into latent representation, DGIB iteratively directs and refines the structural and feature information flow passing through graph snapshots. To meet the MSC Condition, we decompose the overall IB objectives into DGIB$_{MS}$ and DGIB$_C$, in which the DGIB$_{MS}$ channel aims ",
    "link": "https://arxiv.org/abs/2402.06716",
    "context": "Title: Dynamic Graph Information Bottleneck\nAbstract: Dynamic Graphs widely exist in the real world, which carry complicated spatial and temporal feature patterns, challenging their representation learning. Dynamic Graph Neural Networks (DGNNs) have shown impressive predictive abilities by exploiting the intrinsic dynamics. However, DGNNs exhibit limited robustness, prone to adversarial attacks. This paper presents the novel Dynamic Graph Information Bottleneck (DGIB) framework to learn robust and discriminative representations. Leveraged by the Information Bottleneck (IB) principle, we first propose the expected optimal representations should satisfy the Minimal-Sufficient-Consensual (MSC) Condition. To compress redundant as well as conserve meritorious information into latent representation, DGIB iteratively directs and refines the structural and feature information flow passing through graph snapshots. To meet the MSC Condition, we decompose the overall IB objectives into DGIB$_{MS}$ and DGIB$_C$, in which the DGIB$_{MS}$ channel aims ",
    "path": "papers/24/02/2402.06716.json",
    "total_tokens": 976,
    "translated_title": "动态图信息瓶颈",
    "translated_abstract": "动态图广泛存在于现实世界中，它们携带着复杂的时空特征模式，对于它们的表示学习提出了挑战。动态图神经网络（DGNNs）通过利用内在的动态性展示了令人印象深刻的预测能力。然而，DGNNs展示了有限的鲁棒性，易受对抗攻击。本文提出了一种新颖的动态图信息瓶颈（DGIB）框架来学习鲁棒且有区分性的表示。借助信息瓶颈（IB）原理，我们首先提出期望的最优表示应满足最小-全局-一致（MSC）条件。为了在潜在表示中压缩冗余信息和保留有价值的信息，DGIB迭代地引导和改进通过图快照传递的结构和特征信息流。为了满足MSC条件，我们将整体IB目标分解为DGIB$_{MS}$和DGIB$_C$，其中DGIB$_{MS}$通道的目标是...",
    "tldr": "动态图信息瓶颈框架（DGIB）能够学习鲁棒且有区分性的动态图表示。利用信息瓶颈原理，通过迭代引导和改进图快照传递的结构和特征信息流，压缩冗余信息并保留有价值的信息。该框架能满足最小-全局-一致条件，提高了动态图神经网络的鲁棒性。"
}