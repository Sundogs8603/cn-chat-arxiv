{
    "title": "Parametric Feature Transfer: One-shot Federated Learning with Foundation Models",
    "abstract": "In one-shot federated learning (FL), clients collaboratively train a global model in a single round of communication. Existing approaches for one-shot FL enhance communication efficiency at the expense of diminished accuracy. This paper introduces FedPFT (Federated Learning with Parametric Feature Transfer), a methodology that harnesses the transferability of foundation models to enhance both accuracy and communication efficiency in one-shot FL. The approach involves transferring per-client parametric models (specifically, Gaussian mixtures) of features extracted from foundation models. Subsequently, each parametric model is employed to generate synthetic features for training a classifier head. Experimental results on eight datasets demonstrate that FedPFT enhances the communication-accuracy frontier in both centralized and decentralized FL scenarios, as well as across diverse data-heterogeneity settings such as covariate shift and task shift, with improvements of up to 20.6%. Additio",
    "link": "https://arxiv.org/abs/2402.01862",
    "context": "Title: Parametric Feature Transfer: One-shot Federated Learning with Foundation Models\nAbstract: In one-shot federated learning (FL), clients collaboratively train a global model in a single round of communication. Existing approaches for one-shot FL enhance communication efficiency at the expense of diminished accuracy. This paper introduces FedPFT (Federated Learning with Parametric Feature Transfer), a methodology that harnesses the transferability of foundation models to enhance both accuracy and communication efficiency in one-shot FL. The approach involves transferring per-client parametric models (specifically, Gaussian mixtures) of features extracted from foundation models. Subsequently, each parametric model is employed to generate synthetic features for training a classifier head. Experimental results on eight datasets demonstrate that FedPFT enhances the communication-accuracy frontier in both centralized and decentralized FL scenarios, as well as across diverse data-heterogeneity settings such as covariate shift and task shift, with improvements of up to 20.6%. Additio",
    "path": "papers/24/02/2402.01862.json",
    "total_tokens": 836,
    "translated_title": "参数特征迁移：一次性联邦学习与基础模型",
    "translated_abstract": "在一次性联邦学习中，客户端在一轮通信中共同训练一个全局模型。现有的一次性联邦学习方法在增强通信效率的同时损失了准确性。本文介绍了FedPFT（带参数特征迁移的联邦学习），这是一种利用基础模型的可迁移性来提高一次性联邦学习的准确性和通信效率的方法。该方法涉及从基础模型中提取的每个客户端参数模型（具体来说是高斯混合模型）的特征进行迁移。随后，每个参数模型被用来生成用于训练分类器头的合成特征。在八个数据集上的实验结果表明，FedPFT在集中和分散的联邦学习场景中以及在协变量转移和任务转移等多样的数据异质性设置中，增强了通信-准确性的边界，改进了最高达20.6%。",
    "tldr": "FedPFT 使用参数特征迁移提高了一次性联邦学习的准确性和通信效率，并在不同数据异质性设置中显示出改进效果。"
}