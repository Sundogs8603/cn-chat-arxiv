{
    "title": "Model Editing with Canonical Examples",
    "abstract": "We introduce model editing with canonical examples, a setting in which (1) a single learning example is provided per desired behavior, (2) evaluation is performed exclusively out-of-distribution, and (3) deviation from an initial model is strictly limited. A canonical example is a simple instance of good behavior, e.g., The capital of Mauritius is Port Louis) or bad behavior, e.g., An aspect of researchers is coldhearted). The evaluation set contains more complex examples of each behavior (like a paragraph in which the capital of Mauritius is called for.) We create three datasets and modify three more for model editing with canonical examples, covering knowledge-intensive improvements, social bias mitigation, and syntactic edge cases. In our experiments on Pythia language models, we find that LoRA outperforms full finetuning and MEMIT. We then turn to the Backpack language model architecture because it is intended to enable targeted improvement. The Backpack defines a large bank of sen",
    "link": "https://arxiv.org/abs/2402.06155",
    "context": "Title: Model Editing with Canonical Examples\nAbstract: We introduce model editing with canonical examples, a setting in which (1) a single learning example is provided per desired behavior, (2) evaluation is performed exclusively out-of-distribution, and (3) deviation from an initial model is strictly limited. A canonical example is a simple instance of good behavior, e.g., The capital of Mauritius is Port Louis) or bad behavior, e.g., An aspect of researchers is coldhearted). The evaluation set contains more complex examples of each behavior (like a paragraph in which the capital of Mauritius is called for.) We create three datasets and modify three more for model editing with canonical examples, covering knowledge-intensive improvements, social bias mitigation, and syntactic edge cases. In our experiments on Pythia language models, we find that LoRA outperforms full finetuning and MEMIT. We then turn to the Backpack language model architecture because it is intended to enable targeted improvement. The Backpack defines a large bank of sen",
    "path": "papers/24/02/2402.06155.json",
    "total_tokens": 883,
    "translated_title": "模型编辑与规范示例",
    "translated_abstract": "我们引入了使用规范示例进行模型编辑的方法，其中(1)每个期望行为只提供一个学习示例，(2)评估仅在分布之外进行，(3)对初始模型的偏离严格受限制。规范示例是良好行为的简单实例，例如，“毛里求斯的首都是路易港”，或者坏行为的实例，例如，“研究人员的一个方面是冷酷无情”。评估集包含更复杂的每种行为的示例（例如，在一个段落中呼叫毛里求斯的首都）。我们创建了三个数据集并修改了另外三个数据集，用于模型编辑与规范示例，涵盖了知识密集型改进、社会偏见缓解和句法边缘案例。在我们对Pythia语言模型的实验中，我们发现LoRA优于完全微调和MEMIT。然后我们转向了Backpack语言模型架构，因为它旨在实现有针对性的改进。Backpack定义了一个大型的sen...",
    "tldr": "使用规范示例进行模型编辑，每个期望行为只有一个学习示例，评估仅在分布之外进行，对初始模型的偏离受限，通过实验发现LoRA优于完全微调和MEMIT。",
    "en_tdlr": "Model editing is performed using canonical examples, with only one learning example provided per desired behavior. Evaluation is done out-of-distribution, and model deviation is strictly limited. Experimental results show that LoRA outperforms full finetuning and MEMIT."
}