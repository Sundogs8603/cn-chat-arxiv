{
    "title": "\\textit{MinMaxMin} $Q$-learning",
    "abstract": "\\textit{MinMaxMin} $Q$-learning is a novel \\textit{optimistic} Actor-Critic algorithm that addresses the problem of \\textit{overestimation} bias ($Q$-estimations are overestimating the real $Q$-values) inherent in \\textit{conservative} RL algorithms. Its core formula relies on the disagreement among $Q$-networks in the form of the min-batch MaxMin $Q$-networks distance which is added to the $Q$-target and used as the priority experience replay sampling-rule. We implement \\textit{MinMaxMin} on top of TD3 and TD7, subjecting it to rigorous testing against state-of-the-art continuous-space algorithms-DDPG, TD3, and TD7-across popular MuJoCo and Bullet environments. The results show a consistent performance improvement of \\textit{MinMaxMin} over DDPG, TD3, and TD7 across all tested tasks.",
    "link": "https://arxiv.org/abs/2402.05951",
    "context": "Title: \\textit{MinMaxMin} $Q$-learning\nAbstract: \\textit{MinMaxMin} $Q$-learning is a novel \\textit{optimistic} Actor-Critic algorithm that addresses the problem of \\textit{overestimation} bias ($Q$-estimations are overestimating the real $Q$-values) inherent in \\textit{conservative} RL algorithms. Its core formula relies on the disagreement among $Q$-networks in the form of the min-batch MaxMin $Q$-networks distance which is added to the $Q$-target and used as the priority experience replay sampling-rule. We implement \\textit{MinMaxMin} on top of TD3 and TD7, subjecting it to rigorous testing against state-of-the-art continuous-space algorithms-DDPG, TD3, and TD7-across popular MuJoCo and Bullet environments. The results show a consistent performance improvement of \\textit{MinMaxMin} over DDPG, TD3, and TD7 across all tested tasks.",
    "path": "papers/24/02/2402.05951.json",
    "total_tokens": 783,
    "translated_title": "\\textit{MinMaxMin} $Q$-learning",
    "translated_abstract": "\\textit{MinMaxMin} $Q$-learning是一种新颖的乐观型Actor-Critic算法，解决了保守型强化学习算法中存在的过高估计偏差的问题（$Q$-估计过高估计了真实的$Q$值）。其核心公式依赖于$Q$-网络之间的差异，采用最小批次最大最小$Q$-网络距离作为$Q$-目标加入，并作为优先级经验回放采样规则。我们在TD3和TD7之上实施了\\textit{MinMaxMin}，并对其在流行的MuJoCo和Bullet环境中对抗现有的连续空间算法-DDPG，TD3和TD7进行了严格测试。结果显示，在所有测试任务中，\\textit{MinMaxMin}相对于DDPG，TD3和TD7均表现出了稳定的性能提升。",
    "tldr": "\\textit{MinMaxMin} $Q$-learning是一种乐观型Actor-Critic算法，通过解决过高估计偏差的问题，在各种基准任务中相对于现有算法表现出稳定的性能提升。"
}