{
    "title": "Boosting gets full Attention for Relational Learning",
    "abstract": "arXiv:2402.14926v1 Announce Type: new  Abstract: More often than not in benchmark supervised ML, tabular data is flat, i.e. consists of a single $m \\times d$ (rows, columns) file, but cases abound in the real world where observations are described by a set of tables with structural relationships. Neural nets-based deep models are a classical fit to incorporate general topological dependence among description features (pixels, words, etc.), but their suboptimality to tree-based models on tabular data is still well documented. In this paper, we introduce an attention mechanism for structured data that blends well with tree-based models in the training context of (gradient) boosting. Each aggregated model is a tree whose training involves two steps: first, simple tabular models are learned descending tables in a top-down fashion with boosting's class residuals on tables' features. Second, what has been learned progresses back bottom-up via attention and aggregation mechanisms, progressive",
    "link": "https://arxiv.org/abs/2402.14926",
    "context": "Title: Boosting gets full Attention for Relational Learning\nAbstract: arXiv:2402.14926v1 Announce Type: new  Abstract: More often than not in benchmark supervised ML, tabular data is flat, i.e. consists of a single $m \\times d$ (rows, columns) file, but cases abound in the real world where observations are described by a set of tables with structural relationships. Neural nets-based deep models are a classical fit to incorporate general topological dependence among description features (pixels, words, etc.), but their suboptimality to tree-based models on tabular data is still well documented. In this paper, we introduce an attention mechanism for structured data that blends well with tree-based models in the training context of (gradient) boosting. Each aggregated model is a tree whose training involves two steps: first, simple tabular models are learned descending tables in a top-down fashion with boosting's class residuals on tables' features. Second, what has been learned progresses back bottom-up via attention and aggregation mechanisms, progressive",
    "path": "papers/24/02/2402.14926.json",
    "total_tokens": 645,
    "translated_title": "提升关系学习的全注意力机制",
    "translated_abstract": "在基准监督机器学习中，常常会遇到平面化的表格数据，即由一个$m \\times d$（行，列）文件组成，但现实世界中有很多情况是由一组带有结构关系的表格描述的观察数据。本文引入了一个适合于结构化数据的注意力机制，与树模型结合在（梯度）提升的训练背景下。",
    "tldr": "引入了适合结构化数据的注意力机制，与树模型结合，用于（梯度）提升的训练。",
    "en_tdlr": "Introduced an attention mechanism for structured data that integrates with tree-based models for training in boosting context."
}