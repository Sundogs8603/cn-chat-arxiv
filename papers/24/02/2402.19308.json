{
    "title": "Loss-Free Machine Unlearning",
    "abstract": "arXiv:2402.19308v1 Announce Type: new  Abstract: We present a machine unlearning approach that is both retraining- and label-free. Most existing machine unlearning approaches require a model to be fine-tuned to remove information while preserving performance. This is computationally expensive and necessitates the storage of the whole dataset for the lifetime of the model. Retraining-free approaches often utilise Fisher information, which is derived from the loss and requires labelled data which may not be available. Thus, we present an extension to the Selective Synaptic Dampening algorithm, substituting the diagonal of the Fisher information matrix for the gradient of the l2 norm of the model output to approximate sensitivity. We evaluate our method in a range of experiments using ResNet18 and Vision Transformer. Results show our label-free method is competitive with existing state-of-the-art approaches.",
    "link": "https://arxiv.org/abs/2402.19308",
    "context": "Title: Loss-Free Machine Unlearning\nAbstract: arXiv:2402.19308v1 Announce Type: new  Abstract: We present a machine unlearning approach that is both retraining- and label-free. Most existing machine unlearning approaches require a model to be fine-tuned to remove information while preserving performance. This is computationally expensive and necessitates the storage of the whole dataset for the lifetime of the model. Retraining-free approaches often utilise Fisher information, which is derived from the loss and requires labelled data which may not be available. Thus, we present an extension to the Selective Synaptic Dampening algorithm, substituting the diagonal of the Fisher information matrix for the gradient of the l2 norm of the model output to approximate sensitivity. We evaluate our method in a range of experiments using ResNet18 and Vision Transformer. Results show our label-free method is competitive with existing state-of-the-art approaches.",
    "path": "papers/24/02/2402.19308.json",
    "total_tokens": 779,
    "translated_title": "无损机器遗忘",
    "translated_abstract": "我们提出了一种无需重新训练和标记的机器遗忘方法。大多数现有的机器遗忘方法要求对模型进行微调以删除信息同时保持性能。这在计算上消耗巨大，并且需要存储整个数据集以供模型的整个生命周期使用。无需重新训练的方法通常利用来自损失函数的Fisher信息，但这需要标记数据，而这可能不容易获取。因此，我们提出了对选择性突触抑制算法的扩展，将Fisher信息矩阵的对角线替换为模型输出l2范数的梯度以近似灵敏度。我们在一系列使用ResNet18和Vision Transformer的实验证明了我们的方法与现有最先进方法的竞争性。",
    "tldr": "我们提出了一种无需重新训练和标记的机器遗忘方法，通过改进算法以在不需要标记数据的情况下近似灵敏度，实验结果表明我们的方法与现有最先进方法竞争力强。",
    "en_tdlr": "We propose a loss-free machine unlearning approach that does not require retraining or labeling, by improving the algorithm to approximate sensitivity without the need for labeled data, and experimental results demonstrate the competitiveness of our method with existing state-of-the-art approaches."
}