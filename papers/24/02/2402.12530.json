{
    "title": "Parallel Structures in Pre-training Data Yield In-Context Learning",
    "abstract": "arXiv:2402.12530v1 Announce Type: cross  Abstract: Pre-trained language models (LMs) are capable of in-context learning (ICL): they can adapt to a task with only a few examples given in the prompt without any parameter update. However, it is unclear where this capability comes from as there is a stark distribution shift between pre-training text and ICL prompts. In this work, we study what patterns of the pre-training data contribute to ICL. We find that LMs' ICL ability depends on $\\textit{parallel structures}$ in the pre-training data -- pairs of phrases following similar templates in the same context window. Specifically, we detect parallel structures by checking whether training on one phrase improves prediction of the other, and conduct ablation experiments to study their effect on ICL. We show that removing parallel structures in the pre-training data reduces LMs' ICL accuracy by 51% (vs 2% from random ablation). This drop persists even when excluding common patterns such as n-gr",
    "link": "https://arxiv.org/abs/2402.12530",
    "context": "Title: Parallel Structures in Pre-training Data Yield In-Context Learning\nAbstract: arXiv:2402.12530v1 Announce Type: cross  Abstract: Pre-trained language models (LMs) are capable of in-context learning (ICL): they can adapt to a task with only a few examples given in the prompt without any parameter update. However, it is unclear where this capability comes from as there is a stark distribution shift between pre-training text and ICL prompts. In this work, we study what patterns of the pre-training data contribute to ICL. We find that LMs' ICL ability depends on $\\textit{parallel structures}$ in the pre-training data -- pairs of phrases following similar templates in the same context window. Specifically, we detect parallel structures by checking whether training on one phrase improves prediction of the other, and conduct ablation experiments to study their effect on ICL. We show that removing parallel structures in the pre-training data reduces LMs' ICL accuracy by 51% (vs 2% from random ablation). This drop persists even when excluding common patterns such as n-gr",
    "path": "papers/24/02/2402.12530.json",
    "total_tokens": 913,
    "translated_title": "在预训练数据中的平行结构实现上下文学习",
    "translated_abstract": "预训练语言模型（LMs）具备上下文学习（ICL）的能力：它们可以在只给出少量示例的情况下适应任务而无需进行任何参数更新。然而，目前尚不清楚这种能力来自何处，因为预训练文本与ICL提示之间存在明显的分布偏移。在本研究中，我们探讨了预训练数据中的哪些模式有助于ICL。我们发现LMs的ICL能力取决于预训练数据中的“平行结构”——在相同上下文窗口中遵循相似模板的短语对。具体来说，通过检查训练一个短语是否提高了对另一个短语的预测来检测平行结构，并进行消融实验以研究其对ICL的影响。我们展示了从预训练数据中去除平行结构会导致LMs的ICL准确度下降51％（与随机切除的2％相比）。即使排除常见模式如 n-gram",
    "tldr": "本研究发现，语言模型的上下文学习能力取决于预训练数据中的平行结构，通过在相似模板的短语对中学习来提高上下文学习准确度。",
    "en_tdlr": "This study finds that the ability of language models for in-context learning depends on the parallel structures in pre-training data, improving in-context learning accuracy through learning from pairs of phrases following similar templates."
}