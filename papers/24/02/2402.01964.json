{
    "title": "No Need to Look Back: An Efficient and Scalable Approach for Temporal Network Representation Learning",
    "abstract": "Temporal graph representation learning (TGRL) is crucial for modeling complex, dynamic systems in real-world networks. Traditional TGRL methods, though effective, suffer from high computational demands and inference latency. This is mainly induced by their inefficient sampling of temporal neighbors by backtracking the interaction history of each node when making model inference. This paper introduces a novel efficient TGRL framework, No-Looking-Back (NLB). NLB employs a \"forward recent sampling\" strategy, which bypasses the need for backtracking historical interactions. This strategy is implemented using a GPU-executable size-constrained hash table for each node, recording down-sampled recent interactions, which enables rapid response to queries with minimal inference latency. The maintenance of this hash table is highly efficient, with $O(1)$ complexity. NLB is fully compatible with GPU processing, maximizing programmability, parallelism, and power efficiency. Empirical evaluations de",
    "link": "https://arxiv.org/abs/2402.01964",
    "context": "Title: No Need to Look Back: An Efficient and Scalable Approach for Temporal Network Representation Learning\nAbstract: Temporal graph representation learning (TGRL) is crucial for modeling complex, dynamic systems in real-world networks. Traditional TGRL methods, though effective, suffer from high computational demands and inference latency. This is mainly induced by their inefficient sampling of temporal neighbors by backtracking the interaction history of each node when making model inference. This paper introduces a novel efficient TGRL framework, No-Looking-Back (NLB). NLB employs a \"forward recent sampling\" strategy, which bypasses the need for backtracking historical interactions. This strategy is implemented using a GPU-executable size-constrained hash table for each node, recording down-sampled recent interactions, which enables rapid response to queries with minimal inference latency. The maintenance of this hash table is highly efficient, with $O(1)$ complexity. NLB is fully compatible with GPU processing, maximizing programmability, parallelism, and power efficiency. Empirical evaluations de",
    "path": "papers/24/02/2402.01964.json",
    "total_tokens": 872,
    "translated_title": "不需回顾：一种高效可扩展的时态网络表示学习方法",
    "translated_abstract": "时态图表示学习（TGRL）对于建模实际网络中复杂动态系统至关重要。传统的TGRL方法虽然有效，但计算需求和推理延迟较高。这主要是由于在进行模型推理时，通过回溯每个节点的交互历史来进行时态邻居的低效采样所致。本文介绍了一种新颖的高效TGRL框架，名为No-Looking-Back（NLB）。NLB采用了“前向最近采样”策略，绕过了回溯历史交互的需求。该策略通过使用针对每个节点的GPU可执行的大小受限哈希表记录下采样后的最近交互，实现对查询的快速响应和最小化推理延迟。该哈希表的维护具有高效性，复杂度为$O(1)$。NLB与GPU处理完全兼容，最大化了可编程性、并行性和能效。实证评估表明...",
    "tldr": "本论文提出了一种高效可扩展的时态网络表示学习方法，该方法通过前向最近采样策略和GPU可执行的大小受限哈希表实现了对查询的快速响应和最小化推理延迟。",
    "en_tdlr": "This paper introduces an efficient and scalable approach for temporal network representation learning, which utilizes a forward recent sampling strategy and GPU-executable size-constrained hash table to enable rapid query response and minimal inference latency."
}