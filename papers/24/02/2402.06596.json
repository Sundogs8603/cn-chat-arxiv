{
    "title": "Understanding the Weakness of Large Language Model Agents within a Complex Android Environment",
    "abstract": "Large language models (LLMs) have empowered intelligent agents to execute intricate tasks within domain-specific software such as browsers and games. However, when applied to general-purpose software systems like operating systems, LLM agents face three primary challenges. Firstly, the action space is vast and dynamic, posing difficulties for LLM agents to maintain an up-to-date understanding and deliver accurate responses. Secondly, real-world tasks often require inter-application cooperation}, demanding farsighted planning from LLM agents. Thirdly, agents need to identify optimal solutions aligning with user constraints, such as security concerns and preferences. These challenges motivate AndroidArena, an environment and benchmark designed to evaluate LLM agents on a modern operating system. To address high-cost of manpower, we design a scalable and semi-automated method to construct the benchmark. In the task evaluation, AndroidArena incorporates accurate and adaptive metrics to add",
    "link": "https://arxiv.org/abs/2402.06596",
    "context": "Title: Understanding the Weakness of Large Language Model Agents within a Complex Android Environment\nAbstract: Large language models (LLMs) have empowered intelligent agents to execute intricate tasks within domain-specific software such as browsers and games. However, when applied to general-purpose software systems like operating systems, LLM agents face three primary challenges. Firstly, the action space is vast and dynamic, posing difficulties for LLM agents to maintain an up-to-date understanding and deliver accurate responses. Secondly, real-world tasks often require inter-application cooperation}, demanding farsighted planning from LLM agents. Thirdly, agents need to identify optimal solutions aligning with user constraints, such as security concerns and preferences. These challenges motivate AndroidArena, an environment and benchmark designed to evaluate LLM agents on a modern operating system. To address high-cost of manpower, we design a scalable and semi-automated method to construct the benchmark. In the task evaluation, AndroidArena incorporates accurate and adaptive metrics to add",
    "path": "papers/24/02/2402.06596.json",
    "total_tokens": 847,
    "translated_title": "理解大型语言模型在复杂Android环境中的弱点",
    "translated_abstract": "大型语言模型（LLM）将智能代理运用到诸如浏览器和游戏等领域特定软件上执行复杂任务。然而，当应用于操作系统等通用软件系统时，LLM代理面临三大主要挑战。首先，动作空间广阔且动态，使LLM代理难以保持更新的理解和提供准确的回复。其次，现实世界任务经常需要应用间的协作，要求LLM代理具备远见的规划能力。第三，代理需要识别与用户约束条件（如安全问题和偏好）相符的最优解。这些挑战促使我们设计了AndroidArena，一个用于在现代操作系统上评估LLM代理的环境和基准。为解决高人力成本，我们设计了一种可扩展的、半自动化的方法来构建基准。在任务评估中，AndroidArena采用准确和自适应的指标来补充...",
    "tldr": "这项研究揭示了在复杂的Android环境中，大型语言模型代理面临的挑战，提出了AndroidArena环境和基准来评估其性能，以及降低人力成本的方法。",
    "en_tdlr": "This research reveals the challenges faced by large language model agents in complex Android environments and introduces the AndroidArena environment and benchmark for performance evaluation, along with a method to reduce manpower costs."
}