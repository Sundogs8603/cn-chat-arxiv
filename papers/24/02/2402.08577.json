{
    "title": "Test-Time Backdoor Attacks on Multimodal Large Language Models",
    "abstract": "Backdoor attacks are commonly executed by contaminating training data, such that a trigger can activate predetermined harmful effects during the test phase. In this work, we present AnyDoor, a test-time backdoor attack against multimodal large language models (MLLMs), which involves injecting the backdoor into the textual modality using adversarial test images (sharing the same universal perturbation), without requiring access to or modification of the training data. AnyDoor employs similar techniques used in universal adversarial attacks, but distinguishes itself by its ability to decouple the timing of setup and activation of harmful effects. In our experiments, we validate the effectiveness of AnyDoor against popular MLLMs such as LLaVA-1.5, MiniGPT-4, InstructBLIP, and BLIP-2, as well as provide comprehensive ablation studies. Notably, because the backdoor is injected by a universal perturbation, AnyDoor can dynamically change its backdoor trigger prompts/harmful effects, exposing ",
    "link": "https://arxiv.org/abs/2402.08577",
    "context": "Title: Test-Time Backdoor Attacks on Multimodal Large Language Models\nAbstract: Backdoor attacks are commonly executed by contaminating training data, such that a trigger can activate predetermined harmful effects during the test phase. In this work, we present AnyDoor, a test-time backdoor attack against multimodal large language models (MLLMs), which involves injecting the backdoor into the textual modality using adversarial test images (sharing the same universal perturbation), without requiring access to or modification of the training data. AnyDoor employs similar techniques used in universal adversarial attacks, but distinguishes itself by its ability to decouple the timing of setup and activation of harmful effects. In our experiments, we validate the effectiveness of AnyDoor against popular MLLMs such as LLaVA-1.5, MiniGPT-4, InstructBLIP, and BLIP-2, as well as provide comprehensive ablation studies. Notably, because the backdoor is injected by a universal perturbation, AnyDoor can dynamically change its backdoor trigger prompts/harmful effects, exposing ",
    "path": "papers/24/02/2402.08577.json",
    "total_tokens": 968,
    "translated_title": "对多模态大型语言模型的测试时反向门控攻击",
    "translated_abstract": "反向门控攻击通常通过污染训练数据来执行，从而在测试阶段触发预定的有害效果。在本文中，我们提出了AnyDoor，一种针对多模态大型语言模型（MLLMs）的测试时反向门控攻击，它使用对抗性测试图像将反向门控注入到文本模态中（共享相同的通用扰动），而无需访问或修改训练数据。AnyDoor采用类似于通用对抗攻击的技术，但其通过能够分离有害效果的设置和激活的时间来区别于其他攻击。在我们的实验中，我们验证了AnyDoor对流行的MLLMs（如LLaVA-1.5、MiniGPT-4、InstructBLIP和BLIP-2）的有效性，并提供了全面的消融研究。值得注意的是，由于反向门控由通用扰动注入，AnyDoor可以动态改变其反向门触发提示/有害效果，从而暴露出...",
    "tldr": "本文提出了一种针对多模态大型语言模型的测试时反向门控攻击（AnyDoor），通过使用对抗性测试图像将反向门控注入到文本模态中，而无需访问或修改训练数据。AnyDoor具有分离设置和激活有害效果的时间的能力，并且在实验中证明了其有效性。",
    "en_tdlr": "This paper presents a test-time backdoor attack called AnyDoor, which targets multimodal large language models (MLLMs) by injecting the backdoor into the textual modality using adversarial test images. AnyDoor does not require access to or modification of the training data, and it is able to decouple the timing of setup and activation of harmful effects. The effectiveness of AnyDoor is validated through experiments."
}