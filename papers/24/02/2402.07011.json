{
    "title": "FedImpro: Measuring and Improving Client Update in Federated Learning",
    "abstract": "Federated Learning (FL) models often experience client drift caused by heterogeneous data, where the distribution of data differs across clients. To address this issue, advanced research primarily focuses on manipulating the existing gradients to achieve more consistent client models. In this paper, we present an alternative perspective on client drift and aim to mitigate it by generating improved local models. First, we analyze the generalization contribution of local training and conclude that this generalization contribution is bounded by the conditional Wasserstein distance between the data distribution of different clients. Then, we propose FedImpro, to construct similar conditional distributions for local training. Specifically, FedImpro decouples the model into high-level and low-level components, and trains the high-level portion on reconstructed feature distributions. This approach enhances the generalization contribution and reduces the dissimilarity of gradients in FL. Exper",
    "link": "https://arxiv.org/abs/2402.07011",
    "context": "Title: FedImpro: Measuring and Improving Client Update in Federated Learning\nAbstract: Federated Learning (FL) models often experience client drift caused by heterogeneous data, where the distribution of data differs across clients. To address this issue, advanced research primarily focuses on manipulating the existing gradients to achieve more consistent client models. In this paper, we present an alternative perspective on client drift and aim to mitigate it by generating improved local models. First, we analyze the generalization contribution of local training and conclude that this generalization contribution is bounded by the conditional Wasserstein distance between the data distribution of different clients. Then, we propose FedImpro, to construct similar conditional distributions for local training. Specifically, FedImpro decouples the model into high-level and low-level components, and trains the high-level portion on reconstructed feature distributions. This approach enhances the generalization contribution and reduces the dissimilarity of gradients in FL. Exper",
    "path": "papers/24/02/2402.07011.json",
    "total_tokens": 899,
    "translated_title": "FedImpro: 测量和改善联邦学习中的客户更新",
    "translated_abstract": "联邦学习模型通常会受到异构数据引起的客户漂移的影响，其中数据的分布在不同的客户之间存在差异。为了解决这个问题，先进的研究主要关注于操作现有的梯度，以实现更一致的客户模型。在本文中，我们从另一个角度分析了客户漂移，并旨在通过生成改进的本地模型来减轻这种漂移。首先，我们分析了本地训练的泛化贡献，并得出结论，这种泛化贡献受到不同客户的数据分布之间的条件Wasserstein距离的限制。然后，我们提出了FedImpro，用于构建类似的条件分布进行本地训练。具体而言，FedImpro将模型分解为高层和低层组件，并对重构特征分布上的高层部分进行训练。这种方法增强了泛化贡献，并减小了联邦学习中梯度的差异性。",
    "tldr": "本文提出了FedImpro方法，通过生成改进的本地模型来减轻联邦学习中的客户漂移问题。该方法通过分析本地训练的泛化贡献，并利用类似的条件分布进行训练，增强了泛化贡献并减小了梯度的差异性。"
}