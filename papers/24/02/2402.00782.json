{
    "title": "Dense Reward for Free in Reinforcement Learning from Human Feedback",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) has been credited as the key advance that has allowed Large Language Models (LLMs) to effectively follow instructions and produce useful assistance. Classically, this involves generating completions from the LLM in response to a query before using a separate reward model to assign a score to the full completion. As an auto-regressive process, the LLM has to take many \"actions\" (selecting individual tokens) and only receives a single, sparse reward at the end of an episode, a setup that is known to be difficult to optimise in traditional reinforcement learning. In this work we leverage the fact that the reward model contains more information than just its scalar output, in particular, it calculates an attention map over tokens as part of the transformer architecture. We use these attention weights to redistribute the reward along the whole completion, effectively densifying the signal and highlighting the most important tokens, all witho",
    "link": "https://arxiv.org/abs/2402.00782",
    "context": "Title: Dense Reward for Free in Reinforcement Learning from Human Feedback\nAbstract: Reinforcement Learning from Human Feedback (RLHF) has been credited as the key advance that has allowed Large Language Models (LLMs) to effectively follow instructions and produce useful assistance. Classically, this involves generating completions from the LLM in response to a query before using a separate reward model to assign a score to the full completion. As an auto-regressive process, the LLM has to take many \"actions\" (selecting individual tokens) and only receives a single, sparse reward at the end of an episode, a setup that is known to be difficult to optimise in traditional reinforcement learning. In this work we leverage the fact that the reward model contains more information than just its scalar output, in particular, it calculates an attention map over tokens as part of the transformer architecture. We use these attention weights to redistribute the reward along the whole completion, effectively densifying the signal and highlighting the most important tokens, all witho",
    "path": "papers/24/02/2402.00782.json",
    "total_tokens": 898,
    "translated_title": "强化学习从人类反馈中获得的稠密奖励自由",
    "translated_abstract": "强化学习从人类反馈中获得的稠密奖励自由（RLHF）被认为是使大型语言模型（LLMs）能够有效地遵循指示并产生有用协助的关键进展。传统上，这涉及到在回答查询之前从LLM中生成完成，并使用单独的奖励模型为完整的完成指定一个分数。作为一个自回归过程，LLM必须经历许多“动作”（选择单个标记），并在一个episode结束时只收到一个单独的稀疏奖励，这一设置被认为在传统的强化学习中很难优化。在这项工作中，我们利用奖励模型包含的不仅仅是标量输出的更多信息，尤其是作为transformer架构的一部分，它计算了一个对标记进行注意力映射。我们利用这些注意力权重来重新分配奖励，使信号变得密集并突出显示最重要的标记，而无需",
    "tldr": "在强化学习中，我们通过利用奖励模型中的注意权重，将奖励重新分配到完成的所有标记上，从而稠密化信号并突出显示最重要的标记。这项工作使得在传统的强化学习中很难优化的问题得到了解决。"
}