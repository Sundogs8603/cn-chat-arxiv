{
    "title": "Learning Mutual Excitation for Hand-to-Hand and Human-to-Human Interaction Recognition",
    "abstract": "Recognizing interactive actions, including hand-to-hand interaction and human-to-human interaction, has attracted increasing attention for various applications in the field of video analysis and human-robot interaction. Considering the success of graph convolution in modeling topology-aware features from skeleton data, recent methods commonly operate graph convolution on separate entities and use late fusion for interactive action recognition, which can barely model the mutual semantic relationships between pairwise entities. To this end, we propose a mutual excitation graph convolutional network (me-GCN) by stacking mutual excitation graph convolution (me-GC) layers. Specifically, me-GC uses a mutual topology excitation module to firstly extract adjacency matrices from individual entities and then adaptively model the mutual constraints between them. Moreover, me-GC extends the above idea and further uses a mutual feature excitation module to extract and merge deep features from pairw",
    "link": "https://arxiv.org/abs/2402.02431",
    "context": "Title: Learning Mutual Excitation for Hand-to-Hand and Human-to-Human Interaction Recognition\nAbstract: Recognizing interactive actions, including hand-to-hand interaction and human-to-human interaction, has attracted increasing attention for various applications in the field of video analysis and human-robot interaction. Considering the success of graph convolution in modeling topology-aware features from skeleton data, recent methods commonly operate graph convolution on separate entities and use late fusion for interactive action recognition, which can barely model the mutual semantic relationships between pairwise entities. To this end, we propose a mutual excitation graph convolutional network (me-GCN) by stacking mutual excitation graph convolution (me-GC) layers. Specifically, me-GC uses a mutual topology excitation module to firstly extract adjacency matrices from individual entities and then adaptively model the mutual constraints between them. Moreover, me-GC extends the above idea and further uses a mutual feature excitation module to extract and merge deep features from pairw",
    "path": "papers/24/02/2402.02431.json",
    "total_tokens": 927,
    "translated_title": "学习相互激励以实现手对手和人对人交互识别",
    "translated_abstract": "识别交互动作，包括手对手交互和人对人交互，在视频分析和人机交互领域具有广泛的应用。考虑到图卷积在建模骨骼数据的拓扑感知特征方面的成功，最近的方法通常将图卷积应用于独立实体，并在交互动作识别时使用后期融合，这几乎无法建模成对实体之间的互相语义关系。为此，我们通过堆叠相互激励图卷积（me-GC）层，提出了一种相互激励图卷积网络（me-GCN）。具体来说，me-GC使用相互拓扑激励模块首先从单个实体中提取邻接矩阵，然后自适应地对它们之间的相互约束进行建模。此外，me-GC进一步使用相互特征激励模块从成对实体中提取和合并深度特征。",
    "tldr": "本文介绍了一种学习相互激励的图卷积网络（me-GCN），用于手对手和人对人交互识别。通过堆叠相互激励图卷积层（me-GC），该网络能够自适应地建模成对实体之间的互相约束，并提取和合并深度特征。",
    "en_tdlr": "This paper introduces a mutual excitation graph convolutional network (me-GCN) for hand-to-hand and human-to-human interaction recognition. By stacking mutual excitation graph convolution (me-GC) layers, the network can adaptively model the mutual constraints between pairwise entities and extract and merge deep features."
}