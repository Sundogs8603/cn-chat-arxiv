{
    "title": "LiPO: Listwise Preference Optimization through Learning-to-Rank",
    "abstract": "Aligning language models (LMs) with curated human feedback is critical to control their behaviors in real-world applications. Several recent policy optimization methods, such as DPO and SLiC, serve as promising alternatives to the traditional Reinforcement Learning from Human Feedback (RLHF) approach. In practice, human feedback often comes in a format of a ranked list over multiple responses to amortize the cost of reading prompt. Multiple responses can also be ranked by reward models or AI feedback. There lacks such a study on directly fitting upon a list of responses. In this work, we formulate the LM alignment as a listwise ranking problem and describe the Listwise Preference Optimization (LiPO) framework, where the policy can potentially learn more effectively from a ranked list of plausible responses given the prompt. This view draws an explicit connection to Learning-to-Rank (LTR), where most existing preference optimization work can be mapped to existing ranking objectives, esp",
    "link": "https://arxiv.org/abs/2402.01878",
    "context": "Title: LiPO: Listwise Preference Optimization through Learning-to-Rank\nAbstract: Aligning language models (LMs) with curated human feedback is critical to control their behaviors in real-world applications. Several recent policy optimization methods, such as DPO and SLiC, serve as promising alternatives to the traditional Reinforcement Learning from Human Feedback (RLHF) approach. In practice, human feedback often comes in a format of a ranked list over multiple responses to amortize the cost of reading prompt. Multiple responses can also be ranked by reward models or AI feedback. There lacks such a study on directly fitting upon a list of responses. In this work, we formulate the LM alignment as a listwise ranking problem and describe the Listwise Preference Optimization (LiPO) framework, where the policy can potentially learn more effectively from a ranked list of plausible responses given the prompt. This view draws an explicit connection to Learning-to-Rank (LTR), where most existing preference optimization work can be mapped to existing ranking objectives, esp",
    "path": "papers/24/02/2402.01878.json",
    "total_tokens": 865,
    "translated_title": "LiPO: 通过学习排序进行列表型偏好优化",
    "translated_abstract": "将语言模型与人工反馈进行对齐是控制其在实际应用中行为的关键。最近的一些策略优化方法，如DPO和SLiC，成为传统的来自人类反馈的增强学习方法的有希望的替代方案。实际上，人工反馈通常以对多个响应进行排序的格式提供，以摊销阅读提示的成本。多个响应也可以通过奖励模型或AI反馈进行排序。缺少关于直接适应响应列表的研究。在这项工作中，我们将语言模型对齐问题定义为一个列表型排序问题，并描述了列表型偏好优化（LiPO）框架，在给定提示的情况下，策略可以从一个排名列表中更有效地学习可行响应。这种观点与学习排序（LTR）形成明确的联系，其中大多数现有的偏好优化工作可以映射到现有的排名目标，特别是",
    "tldr": "本研究提出了一种名为LiPO的框架，用于将语言模型对齐问题定义为一个列表型排序问题。通过从排名列表中学习，该框架可以使策略更有效地学习到可行的响应。",
    "en_tdlr": "This study proposes a framework called LiPO to address the language model alignment problem as a listwise ranking problem. By learning from a ranked list, the framework allows the policy to learn more effectively from plausible responses."
}