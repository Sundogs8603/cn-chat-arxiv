{
    "title": "Misalignment, Learning, and Ranking: Harnessing Users Limited Attention",
    "abstract": "arXiv:2402.14013v1 Announce Type: new  Abstract: In digital health and EdTech, recommendation systems face a significant challenge: users often choose impulsively, in ways that conflict with the platform's long-term payoffs. This misalignment makes it difficult to effectively learn to rank items, as it may hinder exploration of items with greater long-term payoffs. Our paper tackles this issue by utilizing users' limited attention spans. We propose a model where a platform presents items with unknown payoffs to the platform in a ranked list to $T$ users over time. Each user selects an item by first considering a prefix window of these ranked items and then picking the highest preferred item in that window (and the platform observes its payoff for this item). We study the design of online bandit algorithms that obtain vanishing regret against hindsight optimal benchmarks.   We first consider adversarial window sizes and stochastic iid payoffs. We design an active-elimination-based algor",
    "link": "https://arxiv.org/abs/2402.14013",
    "context": "Title: Misalignment, Learning, and Ranking: Harnessing Users Limited Attention\nAbstract: arXiv:2402.14013v1 Announce Type: new  Abstract: In digital health and EdTech, recommendation systems face a significant challenge: users often choose impulsively, in ways that conflict with the platform's long-term payoffs. This misalignment makes it difficult to effectively learn to rank items, as it may hinder exploration of items with greater long-term payoffs. Our paper tackles this issue by utilizing users' limited attention spans. We propose a model where a platform presents items with unknown payoffs to the platform in a ranked list to $T$ users over time. Each user selects an item by first considering a prefix window of these ranked items and then picking the highest preferred item in that window (and the platform observes its payoff for this item). We study the design of online bandit algorithms that obtain vanishing regret against hindsight optimal benchmarks.   We first consider adversarial window sizes and stochastic iid payoffs. We design an active-elimination-based algor",
    "path": "papers/24/02/2402.14013.json",
    "total_tokens": 909,
    "translated_title": "不对齐、学习和排名：利用用户有限的注意力",
    "translated_abstract": "在数字健康和教育科技领域，推荐系统面临着一个重大挑战：用户通常会冲动地选择，这与平台的长期回报相冲突。这种不对齐使得有效地学习排名项目变得困难，因为它可能阻碍了对具有更大长期回报的项目的探索。我们的论文通过利用用户有限的注意力跨越这个问题。我们提出了一个模型，在这个模型中，平台会随着时间向$T$个用户呈现具有未知回报的项目的排名列表。每个用户通过首先考虑这些排名项目的一个前缀窗口，然后选择该窗口中最受欢迎的项目来选择一个项目（平台观察到了该项目的回报）。我们研究了在线赌博算法的设计，使得它们在面对事后最优基准时能够实现逐渐减少的后悔。我们首先考虑对抗性窗口大小和随机iid回报。我们设计了一种基于主动淘汰的算法",
    "tldr": "通过利用用户有限的注意力，我们提出了一种模型来解决推荐系统面临的用户选择冲动与长期回报不一致的挑战，设计在线赌博算法以逐渐减少后悔。",
    "en_tdlr": "By harnessing users' limited attention, a model is proposed to address the challenge of impulsive user choices conflicting with long-term payoffs in recommendation systems, with the design of online bandit algorithms to gradually reduce regrets against optimal benchmarks."
}