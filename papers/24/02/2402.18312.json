{
    "title": "How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning",
    "abstract": "arXiv:2402.18312v1 Announce Type: new  Abstract: Despite superior reasoning prowess demonstrated by Large Language Models (LLMs) with Chain-of-Thought (CoT) prompting, a lack of understanding prevails around the internal mechanisms of the models that facilitate CoT generation. This work investigates the neural sub-structures within LLMs that manifest CoT reasoning from a mechanistic point of view. From an analysis of LLaMA-2 7B applied to multistep reasoning over fictional ontologies, we demonstrate that LLMs deploy multiple parallel pathways of answer generation for step-by-step reasoning. These parallel pathways provide sequential answers from the input question context as well as the generated CoT. We observe a striking functional rift in the middle layers of the LLM. Token representations in the initial half remain strongly biased towards the pretraining prior, with the in-context taking over abruptly in the later half. This internal phase shift manifests in different functional co",
    "link": "https://arxiv.org/abs/2402.18312",
    "context": "Title: How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning\nAbstract: arXiv:2402.18312v1 Announce Type: new  Abstract: Despite superior reasoning prowess demonstrated by Large Language Models (LLMs) with Chain-of-Thought (CoT) prompting, a lack of understanding prevails around the internal mechanisms of the models that facilitate CoT generation. This work investigates the neural sub-structures within LLMs that manifest CoT reasoning from a mechanistic point of view. From an analysis of LLaMA-2 7B applied to multistep reasoning over fictional ontologies, we demonstrate that LLMs deploy multiple parallel pathways of answer generation for step-by-step reasoning. These parallel pathways provide sequential answers from the input question context as well as the generated CoT. We observe a striking functional rift in the middle layers of the LLM. Token representations in the initial half remain strongly biased towards the pretraining prior, with the in-context taking over abruptly in the later half. This internal phase shift manifests in different functional co",
    "path": "papers/24/02/2402.18312.json",
    "total_tokens": 833,
    "translated_title": "如何逐步思考：对思维链推理的机械理解",
    "translated_abstract": "尽管大型语言模型（LLMs）展示了出色的推理能力，通过思维链（CoT）提示，但对于促进CoT生成的模型内部机制仍存在缺乏理解的问题。本文从机械性的角度研究了LLMs中表现出CoT推理的神经子结构。通过对LLaMA-2 7B应用于虚构本体论的多步推理的分析，我们展示了LLMs为逐步推理部署了多个并行答案生成路径。这些并行路径提供了来自输入问题上下文以及生成的CoT的序贯答案。我们观察到LLMs中间层存在引人瞩目的功能分歧。初始一半的令牌表示仍然强烈偏向预训练先验，而后半部分突然被上下文所取代。这种内部相位转变在不同的功能协同中体现出来。",
    "tldr": "通过对LLMs进行机械性研究，我们发现模型在进行逐步推理时使用多个并行路径生成答案，同时存在功能性分歧。",
    "en_tdlr": "By investigating the mechanistic view of LLMs, we found that the models employ multiple parallel pathways for answer generation in step-by-step reasoning, revealing a functional rift in the process."
}