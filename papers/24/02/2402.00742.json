{
    "title": "Transforming and Combining Rewards for Aligning Large Language Models",
    "abstract": "A common approach for aligning language models to human preferences is to first learn a reward model from preference data, and then use this reward model to update the language model. We study two closely related problems that arise in this approach. First, any monotone transformation of the reward model preserves preference ranking; is there a choice that is ``better'' than others? Second, we often wish to align language models to multiple properties: how should we combine multiple reward models? Using a probabilistic interpretation of the alignment procedure, we identify a natural choice for transformation for (the common case of) rewards learned from Bradley-Terry preference models. This derived transformation has two important properties. First, it emphasizes improving poorly-performing outputs, rather than outputs that already score well. This mitigates both underfitting (where some prompts are not improved) and reward hacking (where the model learns to exploit misspecification of",
    "link": "https://arxiv.org/abs/2402.00742",
    "context": "Title: Transforming and Combining Rewards for Aligning Large Language Models\nAbstract: A common approach for aligning language models to human preferences is to first learn a reward model from preference data, and then use this reward model to update the language model. We study two closely related problems that arise in this approach. First, any monotone transformation of the reward model preserves preference ranking; is there a choice that is ``better'' than others? Second, we often wish to align language models to multiple properties: how should we combine multiple reward models? Using a probabilistic interpretation of the alignment procedure, we identify a natural choice for transformation for (the common case of) rewards learned from Bradley-Terry preference models. This derived transformation has two important properties. First, it emphasizes improving poorly-performing outputs, rather than outputs that already score well. This mitigates both underfitting (where some prompts are not improved) and reward hacking (where the model learns to exploit misspecification of",
    "path": "papers/24/02/2402.00742.json",
    "total_tokens": 964,
    "translated_title": "改变和组合奖励以对齐大规模语言模型",
    "translated_abstract": "将语言模型与人类偏好对齐的常见方法是首先从偏好数据中学习奖励模型，然后使用该奖励模型来更新语言模型。我们研究了这种方法中出现的两个密切相关的问题。首先，奖励模型的任何单调变换都保持偏好排名；是否有一种比其他选择“更好”的选择？其次，我们经常希望将语言模型与多个特性对齐：我们如何组合多个奖励模型？通过对齐过程的概率解释，我们确定了从Bradley-Terry偏好模型学习的奖励（常见情况）的自然变换选择。这个派生的变换具有两个重要的属性。首先，它强调改进表现不佳的输出，而不是已经得分良好的输出。这既减轻了欠拟合（其中一些提示没有得到改进），又减少了奖励欺骗（模型学习利用错误指定）。",
    "tldr": "本研究主要研究了对齐大规模语言模型的方法中出现的两个问题：奖励模型的选择以及多个奖励模型的组合。通过引入概率解释，我们提出了一种从Bradley-Terry偏好模型中学习的奖励的自然变换选择，该变换强调改善表现不佳的输出，从而减轻了欠拟合和奖励欺骗。"
}