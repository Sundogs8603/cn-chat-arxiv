{
    "title": "AM^2-EmoJE: Adaptive Missing-Modality Emotion Recognition in Conversation via Joint Embedding Learning",
    "abstract": "arXiv:2402.10921v1 Announce Type: new  Abstract: Human emotion can be presented in different modes i.e., audio, video, and text. However, the contribution of each mode in exhibiting each emotion is not uniform. Furthermore, the availability of complete mode-specific details may not always be guaranteed in the test time. In this work, we propose AM^2-EmoJE, a model for Adaptive Missing-Modality Emotion Recognition in Conversation via Joint Embedding Learning model that is grounded on two-fold contributions: First, a query adaptive fusion that can automatically learn the relative importance of its mode-specific representations in a query-specific manner. By this the model aims to prioritize the mode-invariant spatial query details of the emotion patterns, while also retaining its mode-exclusive aspects within the learned multimodal query descriptor. Second the multimodal joint embedding learning module that explicitly addresses various missing modality scenarios in test-time. By this, th",
    "link": "https://arxiv.org/abs/2402.10921",
    "context": "Title: AM^2-EmoJE: Adaptive Missing-Modality Emotion Recognition in Conversation via Joint Embedding Learning\nAbstract: arXiv:2402.10921v1 Announce Type: new  Abstract: Human emotion can be presented in different modes i.e., audio, video, and text. However, the contribution of each mode in exhibiting each emotion is not uniform. Furthermore, the availability of complete mode-specific details may not always be guaranteed in the test time. In this work, we propose AM^2-EmoJE, a model for Adaptive Missing-Modality Emotion Recognition in Conversation via Joint Embedding Learning model that is grounded on two-fold contributions: First, a query adaptive fusion that can automatically learn the relative importance of its mode-specific representations in a query-specific manner. By this the model aims to prioritize the mode-invariant spatial query details of the emotion patterns, while also retaining its mode-exclusive aspects within the learned multimodal query descriptor. Second the multimodal joint embedding learning module that explicitly addresses various missing modality scenarios in test-time. By this, th",
    "path": "papers/24/02/2402.10921.json",
    "total_tokens": 888,
    "translated_title": "AM^2-EmoJE：通过联合嵌入学习实现对话中的自适应缺失模态情绪识别",
    "translated_abstract": "人类情绪可以通过不同模式表达，例如音频、视频和文本。然而，每种模式在展示情绪时的贡献并不均匀。此外，在测试时，不一定总是能够保证完整的模式特定细节可用。在这项工作中，我们提出了一种名为AM^2-EmoJE的模型，通过联合嵌入学习模型，在对话中实现自适应缺失模态情绪识别，该模型基于两方面的贡献：首先，查询自适应融合可以自动学习其模式特定表示在查询特定方式下的相对重要性。通过这种方式，模型旨在优先考虑情绪模式的模式不变空间查询细节，同时在学习的多模式查询描述符中保留其独占模式方面。其次，多模态联合嵌入学习模块明确解决了测试时的各种缺失模态场景。",
    "tldr": "通过自适应缺失模态情绪识别, 该模型包括查询自适应融合和多模态联合嵌入学习两大特点，旨在提高情绪识别的准确性和鲁棒性。",
    "en_tdlr": "This model of adaptive missing-modality emotion recognition in conversation includes query adaptive fusion and multimodal joint embedding learning, aiming to improve the accuracy and robustness of emotion recognition."
}