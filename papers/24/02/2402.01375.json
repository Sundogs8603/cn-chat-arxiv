{
    "title": "Dive into the Chasm: Probing the Gap between In- and Cross-Topic Generalization",
    "abstract": "Pre-trained language models (LMs) perform well in In-Topic setups, where training and testing data come from the same topics. However, they face challenges in Cross-Topic scenarios where testing data is derived from distinct topics -- such as Gun Control. This study analyzes various LMs with three probing-based experiments to shed light on the reasons behind the In- vs. Cross-Topic generalization gap. Thereby, we demonstrate, for the first time, that generalization gaps and the robustness of the embedding space vary significantly across LMs. Additionally, we assess larger LMs and underscore the relevance of our analysis for recent models. Overall, diverse pre-training objectives, architectural regularization, or data deduplication contribute to more robust LMs and diminish generalization gaps. Our research contributes to a deeper understanding and comparison of language models across different generalization scenarios.",
    "link": "https://rss.arxiv.org/abs/2402.01375",
    "context": "Title: Dive into the Chasm: Probing the Gap between In- and Cross-Topic Generalization\nAbstract: Pre-trained language models (LMs) perform well in In-Topic setups, where training and testing data come from the same topics. However, they face challenges in Cross-Topic scenarios where testing data is derived from distinct topics -- such as Gun Control. This study analyzes various LMs with three probing-based experiments to shed light on the reasons behind the In- vs. Cross-Topic generalization gap. Thereby, we demonstrate, for the first time, that generalization gaps and the robustness of the embedding space vary significantly across LMs. Additionally, we assess larger LMs and underscore the relevance of our analysis for recent models. Overall, diverse pre-training objectives, architectural regularization, or data deduplication contribute to more robust LMs and diminish generalization gaps. Our research contributes to a deeper understanding and comparison of language models across different generalization scenarios.",
    "path": "papers/24/02/2402.01375.json",
    "total_tokens": 973,
    "translated_title": "跳入分歧点：探究主题内和跨主题泛化之间的差距",
    "translated_abstract": "预训练语言模型（LMs）在主题内的设定中表现良好，这里训练和测试数据来自相同的主题。然而，在跨主题的情况下，例如枪支管制，它们在测试数据来自不同主题时面临挑战。本研究通过三个探究实验证明了不同LMs之间主题内和跨主题泛化差距的原因，并首次展示了嵌入空间的稳健性和通用泛化差距在LMs之间显著变化。此外，我们还评估了更大的LMs，并强调了我们分析对于最新模型的相关性。总的来说，多样的预训练目标、架构规范化或数据去重都有助于更稳健的LMs并减小泛化差距。我们的研究为深入理解和比较不同泛化场景下的语言模型做出了贡献。",
    "tldr": "本研究通过分析不同LMs的探究实验，首次展示了主题内和跨主题泛化差距的原因，并表明嵌入空间的稳健性和LMs之间通用泛化差距的显著变化。我们的研究还发现多样的预训练目标、架构规范化或数据去重对于增强LMs的稳健性和减小泛化差距有积极作用。",
    "en_tdlr": "This study investigates the reasons behind the generalization gap between in-topic and cross-topic scenarios in pre-trained language models (LMs), demonstrating the significant variability in robustness and generalization gaps across different LMs. The research also highlights the positive impact of diverse pre-training objectives, architectural regularization, and data deduplication in enhancing the robustness of LMs and reducing generalization gaps."
}