{
    "title": "Diffusion Model with Cross Attention as an Inductive Bias for Disentanglement",
    "abstract": "arXiv:2402.09712v1 Announce Type: cross  Abstract: Disentangled representation learning strives to extract the intrinsic factors within observed data. Factorizing these representations in an unsupervised manner is notably challenging and usually requires tailored loss functions or specific structural designs. In this paper, we introduce a new perspective and framework, demonstrating that diffusion models with cross-attention can serve as a powerful inductive bias to facilitate the learning of disentangled representations. We propose to encode an image to a set of concept tokens and treat them as the condition of the latent diffusion for image reconstruction, where cross-attention over the concept tokens is used to bridge the interaction between the encoder and diffusion. Without any additional regularization, this framework achieves superior disentanglement performance on the benchmark datasets, surpassing all previous methods with intricate designs. We have conducted comprehensive abl",
    "link": "https://arxiv.org/abs/2402.09712",
    "context": "Title: Diffusion Model with Cross Attention as an Inductive Bias for Disentanglement\nAbstract: arXiv:2402.09712v1 Announce Type: cross  Abstract: Disentangled representation learning strives to extract the intrinsic factors within observed data. Factorizing these representations in an unsupervised manner is notably challenging and usually requires tailored loss functions or specific structural designs. In this paper, we introduce a new perspective and framework, demonstrating that diffusion models with cross-attention can serve as a powerful inductive bias to facilitate the learning of disentangled representations. We propose to encode an image to a set of concept tokens and treat them as the condition of the latent diffusion for image reconstruction, where cross-attention over the concept tokens is used to bridge the interaction between the encoder and diffusion. Without any additional regularization, this framework achieves superior disentanglement performance on the benchmark datasets, surpassing all previous methods with intricate designs. We have conducted comprehensive abl",
    "path": "papers/24/02/2402.09712.json",
    "total_tokens": 810,
    "translated_title": "使用交叉注意力作为归纳偏置的扩散模型用于解缠表示学习",
    "translated_abstract": "解缠表示学习致力于提取观测数据中的内在因素。以无监督方式因式分解这些表示通常具有挑战性，并且通常需要定制的损失函数或特定结构设计。本文引入了一个新的观点和框架，证明了具有交叉注意力的扩散模型可以作为强大的归纳偏置，促进解缠表示的学习。我们提出将图像编码为一组概念令牌，并将它们视为图像重构的潜在扩散的条件，其中通过概念令牌的交叉注意力用于连接编码器和扩散之间的交互。在基准数据集上，该框架无需任何额外的正则化就能达到更优秀的解缠性能，超越了所有先前设计复杂的方法。",
    "tldr": "本文介绍了一种新的观点和框架，证明了具有交叉注意力的扩散模型可以作为强大的归纳偏置，促进解缠表示的学习。",
    "en_tdlr": "This paper introduces a new perspective and framework, demonstrating that diffusion models with cross-attention can serve as a powerful inductive bias to facilitate the learning of disentangled representations."
}