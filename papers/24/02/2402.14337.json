{
    "title": "AURA: Natural Language Reasoning for Aleatoric Uncertainty in Rationales",
    "abstract": "arXiv:2402.14337v1 Announce Type: new  Abstract: Rationales behind answers not only explain model decisions but boost language models to reason well on complex reasoning tasks. However, obtaining impeccable rationales is often impossible. Besides, it is non-trivial to estimate the degree to which the rationales are faithful enough to encourage model performance. Thus, such reasoning tasks often compel models to output correct answers under undesirable rationales and are sub-optimal compared to what the models are fully capable of. In this work, we propose how to deal with imperfect rationales causing aleatoric uncertainty. We first define the ambiguous rationales with entropy scores of given rationales, using model prior beliefs as informativeness. We then guide models to select one of two different reasoning models according to the ambiguity of rationales. We empirically argue that our proposed method produces robust performance superiority against the adversarial quality of rationale",
    "link": "https://arxiv.org/abs/2402.14337",
    "context": "Title: AURA: Natural Language Reasoning for Aleatoric Uncertainty in Rationales\nAbstract: arXiv:2402.14337v1 Announce Type: new  Abstract: Rationales behind answers not only explain model decisions but boost language models to reason well on complex reasoning tasks. However, obtaining impeccable rationales is often impossible. Besides, it is non-trivial to estimate the degree to which the rationales are faithful enough to encourage model performance. Thus, such reasoning tasks often compel models to output correct answers under undesirable rationales and are sub-optimal compared to what the models are fully capable of. In this work, we propose how to deal with imperfect rationales causing aleatoric uncertainty. We first define the ambiguous rationales with entropy scores of given rationales, using model prior beliefs as informativeness. We then guide models to select one of two different reasoning models according to the ambiguity of rationales. We empirically argue that our proposed method produces robust performance superiority against the adversarial quality of rationale",
    "path": "papers/24/02/2402.14337.json",
    "total_tokens": 892,
    "translated_title": "AURA：自然语言推理中的模式合理性不确定性",
    "translated_abstract": "回策背后的理由不仅解释了模型决策，而且提升了语言模型在复杂推理任务上的推理能力。然而，获得无懈可击的理由通常是不可能的。此外，估计理由足够忠实以鼓励模型表现的程度并不是微不足道的。因此，这些推理任务通常迫使模型在不理想的理由下输出正确答案，并且与模型完全有能力的情况相比是次优的。在这项工作中，我们提出了如何应对引发模式合理性不确定性的不完美理由。我们首先用给定理由的熵分数来定义模糊的理由，使用模型先验信念作为信息量。然后根据理由的模糊性来引导模型选择两种不同的推理模型中的一种。我们在实证上论证了我们提出的方法相对于理由的敌对质量产生了稳健的性能优势。",
    "tldr": "提出了在自然语言推理中处理引发模式合理性不确定性的不完美理由的方法，实施了使用熵分数和模型先验信念来指导模型的策略，并在实证中展示了方法相对于敌对理由的稳健性能优势"
}