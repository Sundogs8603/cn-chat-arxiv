{
    "title": "Instruction Diversity Drives Generalization To Unseen Tasks",
    "abstract": "arXiv:2402.10891v1 Announce Type: cross  Abstract: Instruction tuning -- fine-tuning a large language model (LLM) on pairs of instructions and desired outcomes -- is an approach that enables pre-trained language models to perform real-world tasks and follow human instructions. Its practical success depends on the model learning a broader set of instructions than those it was trained on. Yet the factors that determine model generalization to such \\emph{unseen tasks} are not well understood. %To understand the driving factors of generalization, In this paper, we experiment with string rewrites, a symbolic task that serves as a building block for Turing complete Markov algorithms while allowing experimental control of \"inputs\" and \"instructions\". We investigate the trade-off between the number of instructions the model is trained on and the number of training samples provided for each instruction and observe that the diversity of the instruction set determines generalization. Generalizati",
    "link": "https://arxiv.org/abs/2402.10891",
    "context": "Title: Instruction Diversity Drives Generalization To Unseen Tasks\nAbstract: arXiv:2402.10891v1 Announce Type: cross  Abstract: Instruction tuning -- fine-tuning a large language model (LLM) on pairs of instructions and desired outcomes -- is an approach that enables pre-trained language models to perform real-world tasks and follow human instructions. Its practical success depends on the model learning a broader set of instructions than those it was trained on. Yet the factors that determine model generalization to such \\emph{unseen tasks} are not well understood. %To understand the driving factors of generalization, In this paper, we experiment with string rewrites, a symbolic task that serves as a building block for Turing complete Markov algorithms while allowing experimental control of \"inputs\" and \"instructions\". We investigate the trade-off between the number of instructions the model is trained on and the number of training samples provided for each instruction and observe that the diversity of the instruction set determines generalization. Generalizati",
    "path": "papers/24/02/2402.10891.json",
    "total_tokens": 761,
    "translated_title": "指导多样性推动对未见任务的泛化",
    "translated_abstract": "指导调整——在指令和期望结果之间微调大型语言模型（LLM）的方法——是一种使预训练语言模型执行现实世界任务并遵循人类指令的方法。其实际成功取决于模型学习比其训练时更广泛的指令集。然而，决定模型对这种“未见任务”的泛化的因素尚不十分清楚。为了了解泛化的驱动因素，本文通过字符串重写进行实验，这是一个符号任务，是图灵完整马尔可夫算法的基本组成部分，同时允许实验对“输入”和“指令”进行控制。我们调查了模型接受的指令数量和为每个指令提供的训练样本数量之间的权衡，并观察到指令集的多样性确定了泛化。",
    "tldr": "指导调整通过增加指令集的多样性来推动模型对未见任务的泛化。",
    "en_tdlr": "Instruction tuning drives model generalization to unseen tasks by increasing the diversity of the instruction set."
}