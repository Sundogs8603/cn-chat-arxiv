{
    "title": "Position Paper: Toward New Frameworks for Studying Model Representations",
    "abstract": "Mechanistic interpretability (MI) aims to understand AI models by reverse-engineering the exact algorithms neural networks learn. Most works in MI so far have studied behaviors and capabilities that are trivial and token-aligned. However, most capabilities are not that trivial, which advocates for the study of hidden representations inside these networks as the unit of analysis. We do a literature review, formalize representations for features and behaviors, highlight their importance and evaluation, and perform some basic exploration in the mechanistic interpretability of representations. With discussion and exploratory results, we justify our position that studying representations is an important and under-studied field, and that currently established methods in MI are not sufficient to understand representations, thus pushing for the research community to work toward new frameworks for studying representations.",
    "link": "https://arxiv.org/abs/2402.03855",
    "context": "Title: Position Paper: Toward New Frameworks for Studying Model Representations\nAbstract: Mechanistic interpretability (MI) aims to understand AI models by reverse-engineering the exact algorithms neural networks learn. Most works in MI so far have studied behaviors and capabilities that are trivial and token-aligned. However, most capabilities are not that trivial, which advocates for the study of hidden representations inside these networks as the unit of analysis. We do a literature review, formalize representations for features and behaviors, highlight their importance and evaluation, and perform some basic exploration in the mechanistic interpretability of representations. With discussion and exploratory results, we justify our position that studying representations is an important and under-studied field, and that currently established methods in MI are not sufficient to understand representations, thus pushing for the research community to work toward new frameworks for studying representations.",
    "path": "papers/24/02/2402.03855.json",
    "total_tokens": 859,
    "translated_title": "《定位论文：探索研究模型表示的新框架》",
    "translated_abstract": "机制解释性（MI）旨在通过逆向工程AI模型学习的确切算法来理解模型。迄今为止，大多数MI研究的行为和能力都是微不足道的和符号对齐的。然而，大多数能力并不那么微不足道，这为研究网络内部的隐藏表示作为分析单位提供了支持。我们进行了文献回顾，对特征和行为进行了形式化的表示，强调了它们的重要性和评估，并进行了一些基本的探索性研究。通过讨论和探索性结果，我们证明了研究表示是一个重要且未被充分研究的领域，并且目前MI中建立的方法不足以理解表示，因此推动研究界朝着研究表示的新框架努力。",
    "tldr": "通过逆向工程AI模型的确切算法，机制解释性（MI）旨在理解模型。然而，目前的研究主要关注微不足道的行为和能力，而忽视了隐藏在网络内部的表示。因此，我们呼吁研究界朝着新的框架努力，研究这些表示。",
    "en_tdlr": "Mechanistic interpretability (MI) aims to understand AI models by reverse-engineering the exact algorithms neural networks learn. However, current research primarily focuses on trivial behaviors and capabilities, neglecting the hidden representations inside the networks. This paper advocates for new frameworks to study these representations."
}