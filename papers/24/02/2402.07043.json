{
    "title": "A Tale of Tails: Model Collapse as a Change of Scaling Laws",
    "abstract": "As AI model size grows, neural scaling laws have become a crucial tool to predict the improvements of large models when increasing capacity and the size of original (human or natural) training data. Yet, the widespread use of popular models means that the ecosystem of online data and text will co-evolve to progressively contain increased amounts of synthesized data. In this paper we ask: How will the scaling laws change in the inevitable regime where synthetic data makes its way into the training corpus? Will future models, still improve, or be doomed to degenerate up to total (model) collapse? We develop a theoretical framework of model collapse through the lens of scaling laws. We discover a wide range of decay phenomena, analyzing loss of scaling, shifted scaling with number of generations, the ''un-learning\" of skills, and grokking when mixing human and synthesized data. Our theory is validated by large-scale experiments with a transformer on an arithmetic task and text generation ",
    "link": "https://arxiv.org/abs/2402.07043",
    "context": "Title: A Tale of Tails: Model Collapse as a Change of Scaling Laws\nAbstract: As AI model size grows, neural scaling laws have become a crucial tool to predict the improvements of large models when increasing capacity and the size of original (human or natural) training data. Yet, the widespread use of popular models means that the ecosystem of online data and text will co-evolve to progressively contain increased amounts of synthesized data. In this paper we ask: How will the scaling laws change in the inevitable regime where synthetic data makes its way into the training corpus? Will future models, still improve, or be doomed to degenerate up to total (model) collapse? We develop a theoretical framework of model collapse through the lens of scaling laws. We discover a wide range of decay phenomena, analyzing loss of scaling, shifted scaling with number of generations, the ''un-learning\" of skills, and grokking when mixing human and synthesized data. Our theory is validated by large-scale experiments with a transformer on an arithmetic task and text generation ",
    "path": "papers/24/02/2402.07043.json",
    "total_tokens": 823,
    "translated_title": "尾巴的故事：作为尺度律变化的模型崩溃",
    "translated_abstract": "随着AI模型大小的增长，神经尺度律已成为预测大模型在扩容和原始（人类或自然）训练数据大小增加时改善的关键工具。然而，流行模型的广泛使用意味着在线数据和文本的生态系统将逐渐包含越来越多的合成数据。在本文中，我们问：当合成数据进入训练语料库时，尺度律会如何改变？未来的模型仍会改善，还是注定会完全崩溃（模型崩溃）？通过尺度律的视角，我们开发了一个模型崩溃的理论框架。我们发现了广泛的衰减现象，分析了尺度的丧失、与代数的变化尺度、技能的\"遗忘\"以及混合人类和合成数据时的理解能力。我们的理论通过对一个算术任务和文本生成的转换器进行大规模实验证实。",
    "tldr": "本文通过尺度律的视角，研究了AI模型大小增长和合成数据引入对模型性能的影响，发现模型可能会遭遇总体崩溃，并通过大规模实验证实了这一理论。"
}