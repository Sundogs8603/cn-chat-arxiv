{
    "title": "The Cost of Parallelizing Boosting",
    "abstract": "arXiv:2402.15145v1 Announce Type: new  Abstract: We study the cost of parallelizing weak-to-strong boosting algorithms for learning, following the recent work of Karbasi and Larsen. Our main results are two-fold:   - First, we prove a tight lower bound, showing that even \"slight\" parallelization of boosting requires an exponential blow-up in the complexity of training.   Specifically, let $\\gamma$ be the weak learner's advantage over random guessing. The famous \\textsc{AdaBoost} algorithm produces an accurate hypothesis by interacting with the weak learner for $\\tilde{O}(1 / \\gamma^2)$ rounds where each round runs in polynomial time.   Karbasi and Larsen showed that \"significant\" parallelization must incur exponential blow-up: Any boosting algorithm either interacts with the weak learner for $\\Omega(1 / \\gamma)$ rounds or incurs an $\\exp(d / \\gamma)$ blow-up in the complexity of training, where $d$ is the VC dimension of the hypothesis class. We close the gap by showing that any boosti",
    "link": "https://arxiv.org/abs/2402.15145",
    "context": "Title: The Cost of Parallelizing Boosting\nAbstract: arXiv:2402.15145v1 Announce Type: new  Abstract: We study the cost of parallelizing weak-to-strong boosting algorithms for learning, following the recent work of Karbasi and Larsen. Our main results are two-fold:   - First, we prove a tight lower bound, showing that even \"slight\" parallelization of boosting requires an exponential blow-up in the complexity of training.   Specifically, let $\\gamma$ be the weak learner's advantage over random guessing. The famous \\textsc{AdaBoost} algorithm produces an accurate hypothesis by interacting with the weak learner for $\\tilde{O}(1 / \\gamma^2)$ rounds where each round runs in polynomial time.   Karbasi and Larsen showed that \"significant\" parallelization must incur exponential blow-up: Any boosting algorithm either interacts with the weak learner for $\\Omega(1 / \\gamma)$ rounds or incurs an $\\exp(d / \\gamma)$ blow-up in the complexity of training, where $d$ is the VC dimension of the hypothesis class. We close the gap by showing that any boosti",
    "path": "papers/24/02/2402.15145.json",
    "total_tokens": 884,
    "translated_title": "平行Boosting的成本",
    "translated_abstract": "我们研究了并行化弱到强Boosting算法学习的成本，延续了Karbasi和Larsen最近的工作。我们的主要结果是双重的：首先，我们证明了一个紧密的下界，表明即使是对Boosting的轻微并行化也需要在训练复杂度上呈指数级增长。具体来说，设$\\gamma$为弱学习器优于随机猜测的优势。著名的\\textsc{AdaBoost}算法通过与弱学习器进行$\\tilde{O}(1 / \\gamma^2)$轮交互来产生准确的假设，其中每轮都在多项式时间内运行。Karbasi和Larsen表明“显著”的并行化必须导致指数级增长：任何Boosting算法要么与弱学习器交互$\\Omega(1 / \\gamma)$轮，要么在训练复杂度上出现$\\exp(d/\\gamma)$的增长，其中$d$是假设类的VC维度。我们通过展示任何Boosti",
    "tldr": "我们研究了并行化弱到强Boosting算法学习的成本，证明了即使是对Boosting的轻微并行化也需要在训练复杂度上呈指数级增长。",
    "en_tdlr": "We study the cost of parallelizing weak-to-strong boosting algorithms for learning and show that even slight parallelization of boosting requires an exponential blow-up in the complexity of training."
}