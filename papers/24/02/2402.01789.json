{
    "title": "The Political Preferences of LLMs",
    "abstract": "We report here a comprehensive analysis about the political preferences embedded in Large Language Models (LLMs). Namely, we administer 11 political orientation tests, designed to identify the political preferences of the test taker, to 24 state-of-the-art conversational LLMs, both close and open source. The results indicate that when probed with questions/statements with political connotations most conversational LLMs tend to generate responses that are diagnosed by most political test instruments as manifesting preferences for left-of-center viewpoints. We note that this is not the case for base (i.e. foundation) models upon which LLMs optimized for conversation with humans are built. However, base models' suboptimal performance at coherently answering questions suggests caution when interpreting their classification by political orientation tests. Though not conclusive, our results provide preliminary evidence for the intriguing hypothesis that the embedding of political preferences",
    "link": "https://arxiv.org/abs/2402.01789",
    "context": "Title: The Political Preferences of LLMs\nAbstract: We report here a comprehensive analysis about the political preferences embedded in Large Language Models (LLMs). Namely, we administer 11 political orientation tests, designed to identify the political preferences of the test taker, to 24 state-of-the-art conversational LLMs, both close and open source. The results indicate that when probed with questions/statements with political connotations most conversational LLMs tend to generate responses that are diagnosed by most political test instruments as manifesting preferences for left-of-center viewpoints. We note that this is not the case for base (i.e. foundation) models upon which LLMs optimized for conversation with humans are built. However, base models' suboptimal performance at coherently answering questions suggests caution when interpreting their classification by political orientation tests. Though not conclusive, our results provide preliminary evidence for the intriguing hypothesis that the embedding of political preferences",
    "path": "papers/24/02/2402.01789.json",
    "total_tokens": 799,
    "translated_title": "LLM的政治偏好分析",
    "translated_abstract": "我们在这里报告了关于大型语言模型（LLMs）中内嵌的政治偏好的全面分析。具体而言，我们对24个最先进的对话型LLM进行了11项政治倾向测试，旨在确定测试者的政治偏好。结果表明，当使用具有政治含义的问题/陈述进行探究时，大多数对话型LLM倾向于生成被大多数政治测试仪器诊断为左翼观点的回答。我们注意到，这对于用于与人类对话优化的LLM基础模型并非如此。然而，基础模型在连贯回答问题方面表现不佳，需要对其政治倾向测试的分类进行谨慎解读。虽然还没有定论，但我们的结果为有趣的假设提供了初步证据，即政治偏好会嵌入其中。",
    "tldr": "该研究对LLM的政治偏好进行了分析，结果发现大多数对话型LLM表现出左翼观点，但需谨慎解读基础模型在政治倾向测试中的分类。",
    "en_tdlr": "This study analyzed the political preferences of LLMs and found that most conversational LLMs exhibit left-of-center viewpoints. However, caution should be exercised when interpreting the classification of base models in political orientation tests."
}