{
    "title": "Explainability for Machine Learning Models: From Data Adaptability to User Perception",
    "abstract": "arXiv:2402.10888v1 Announce Type: new  Abstract: This thesis explores the generation of local explanations for already deployed machine learning models, aiming to identify optimal conditions for producing meaningful explanations considering both data and user requirements. The primary goal is to develop methods for generating explanations for any model while ensuring that these explanations remain faithful to the underlying model and comprehensible to the users.   The thesis is divided into two parts. The first enhances a widely used rule-based explanation method. It then introduces a novel approach for evaluating the suitability of linear explanations to approximate a model. Additionally, it conducts a comparative experiment between two families of counterfactual explanation methods to analyze the advantages of one over the other. The second part focuses on user experiments to assess the impact of three explanation methods and two distinct representations. These experiments measure ho",
    "link": "https://arxiv.org/abs/2402.10888",
    "context": "Title: Explainability for Machine Learning Models: From Data Adaptability to User Perception\nAbstract: arXiv:2402.10888v1 Announce Type: new  Abstract: This thesis explores the generation of local explanations for already deployed machine learning models, aiming to identify optimal conditions for producing meaningful explanations considering both data and user requirements. The primary goal is to develop methods for generating explanations for any model while ensuring that these explanations remain faithful to the underlying model and comprehensible to the users.   The thesis is divided into two parts. The first enhances a widely used rule-based explanation method. It then introduces a novel approach for evaluating the suitability of linear explanations to approximate a model. Additionally, it conducts a comparative experiment between two families of counterfactual explanation methods to analyze the advantages of one over the other. The second part focuses on user experiments to assess the impact of three explanation methods and two distinct representations. These experiments measure ho",
    "path": "papers/24/02/2402.10888.json",
    "total_tokens": 801,
    "translated_title": "机器学习模型的可解释性：从数据适应性到用户感知",
    "translated_abstract": "这篇论文探讨了为已部署的机器学习模型生成局部解释，旨在确定产生有意义解释的最佳条件，考虑到数据和用户需求。主要目标是开发方法，生成任何模型的解释，同时确保这些解释保持忠实于基础模型并对用户具有可理解性。论文分为两部分。第一部分增强了一个广泛使用的基于规则的解释方法。然后引入了一个评估线性解释逼近模型适宜性的新方法。此外，对比了两种反事实解释方法族以分析其中一种相对另一种的优势。第二部分侧重于用户实验，评估三种解释方法和两种不同表示的影响。这些实验测量了用户理解解释的速度，可信度和关注程度。",
    "tldr": "本文旨在为已部署的机器学习模型生成局部解释并确保这些解释对用户具有可理解性，主要创新在于开发具有数据适应性和用户感知要求站点解释方法。",
    "en_tdlr": "This paper aims to generate local explanations for already deployed machine learning models and ensure these explanations are understandable to users, with the main innovation being the development of explanation methods that consider both data adaptability and user perception requirements."
}