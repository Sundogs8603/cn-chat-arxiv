{
    "title": "Addressing cognitive bias in medical language models",
    "abstract": "The integration of large language models (LLMs) into the medical field has gained significant attention due to their promising accuracy in simulated clinical decision-making settings. However, clinical decision-making is more complex than simulations because physicians' decisions are shaped by many factors, including the presence of cognitive bias. However, the degree to which LLMs are susceptible to the same cognitive biases that affect human clinicians remains unexplored. Our hypothesis posits that when LLMs are confronted with clinical questions containing cognitive biases, they will yield significantly less accurate responses compared to the same questions presented without such biases.In this study, we developed BiasMedQA, a novel benchmark for evaluating cognitive biases in LLMs applied to medical tasks. Using BiasMedQA we evaluated six LLMs, namely GPT-4, Mixtral-8x70B, GPT-3.5, PaLM-2, Llama 2 70B-chat, and the medically specialized PMC Llama 13B. We tested these models on 1,27",
    "link": "https://arxiv.org/abs/2402.08113",
    "context": "Title: Addressing cognitive bias in medical language models\nAbstract: The integration of large language models (LLMs) into the medical field has gained significant attention due to their promising accuracy in simulated clinical decision-making settings. However, clinical decision-making is more complex than simulations because physicians' decisions are shaped by many factors, including the presence of cognitive bias. However, the degree to which LLMs are susceptible to the same cognitive biases that affect human clinicians remains unexplored. Our hypothesis posits that when LLMs are confronted with clinical questions containing cognitive biases, they will yield significantly less accurate responses compared to the same questions presented without such biases.In this study, we developed BiasMedQA, a novel benchmark for evaluating cognitive biases in LLMs applied to medical tasks. Using BiasMedQA we evaluated six LLMs, namely GPT-4, Mixtral-8x70B, GPT-3.5, PaLM-2, Llama 2 70B-chat, and the medically specialized PMC Llama 13B. We tested these models on 1,27",
    "path": "papers/24/02/2402.08113.json",
    "total_tokens": 965,
    "translated_title": "解决医学语言模型中的认知偏见问题",
    "translated_abstract": "将大型语言模型（LLMs）整合到医学领域已经引起了重大关注，因为它们在模拟临床决策场景中的准确性很有前景。然而，临床决策比模拟更复杂，因为医生的决策受到许多因素的影响，包括认知偏见的存在。然而，LLMs在面对包含认知偏见的临床问题时，与不包含这些偏见的问题相比，其回答的准确性会明显降低，这一问题尚未被探索。本研究的假设认为，当LLMs面对包含认知偏见的临床问题时，与不包含这些偏见的问题相比，其回答的准确性会明显降低。我们开发了BiasMedQA，这是一个用于评估LLMs在医学任务中的认知偏见的新型基准。使用BiasMedQA，我们评估了六个LLMs，分别是GPT-4、Mixtral-8x70B、GPT-3.5、PaLM-2、Llama 2 70B-chat和医学专业的PMC Llama 13B。我们在127个临床问题上测试了这些模型。",
    "tldr": "本研究通过开发BiasMedQA，一个用于评估医学任务中LLMs的认知偏见的新型基准，发现LLMs在面对包含认知偏见的临床问题时，其回答的准确性明显降低。",
    "en_tdlr": "This research developed a new benchmark called BiasMedQA to evaluate cognitive biases in large language models (LLMs) applied to medical tasks. The study found that LLMs yield significantly less accurate responses when confronted with clinical questions containing cognitive biases."
}