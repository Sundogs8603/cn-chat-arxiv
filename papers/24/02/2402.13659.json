{
    "title": "Privacy-Preserving Instructions for Aligning Large Language Models",
    "abstract": "arXiv:2402.13659v1 Announce Type: cross  Abstract: Service providers of large language model (LLM) applications collect user instructions in the wild and use them in further aligning LLMs with users' intentions. These instructions, which potentially contain sensitive information, are annotated by human workers in the process. This poses a new privacy risk not addressed by the typical private optimization. To this end, we propose using synthetic instructions to replace real instructions in data annotation and model fine-tuning. Formal differential privacy is guaranteed by generating those synthetic instructions using privately fine-tuned generators. Crucial in achieving the desired utility is our novel filtering algorithm that matches the distribution of the synthetic instructions to that of the real ones. In both supervised fine-tuning and reinforcement learning from human feedback, our extensive experiments demonstrate the high utility of the final set of synthetic instructions by sho",
    "link": "https://arxiv.org/abs/2402.13659",
    "context": "Title: Privacy-Preserving Instructions for Aligning Large Language Models\nAbstract: arXiv:2402.13659v1 Announce Type: cross  Abstract: Service providers of large language model (LLM) applications collect user instructions in the wild and use them in further aligning LLMs with users' intentions. These instructions, which potentially contain sensitive information, are annotated by human workers in the process. This poses a new privacy risk not addressed by the typical private optimization. To this end, we propose using synthetic instructions to replace real instructions in data annotation and model fine-tuning. Formal differential privacy is guaranteed by generating those synthetic instructions using privately fine-tuned generators. Crucial in achieving the desired utility is our novel filtering algorithm that matches the distribution of the synthetic instructions to that of the real ones. In both supervised fine-tuning and reinforcement learning from human feedback, our extensive experiments demonstrate the high utility of the final set of synthetic instructions by sho",
    "path": "papers/24/02/2402.13659.json",
    "total_tokens": 839,
    "translated_title": "大型语言模型对齐的隐私保护指南",
    "translated_abstract": "大型语言模型（LLM）应用的服务提供商在野外收集用户指南，并在进一步对齐LLM与用户意图中使用这些指南。这些潜在包含敏感信息的指南在流程中由人工工作者标注。这带来了新的隐私风险，而Typical Private Optimization没有解决这个问题。为此，我们提议使用合成指南替换数据标注和模型微调中的真实指南。通过使用私密微调生成器生成这些合成指南，可以确保形式差异隐私。在实现所需效用方面至关重要的是我们的新颖过滤算法，将合成指南的分布与实际指南的分布进行匹配。在有人反馈的受监督微调和强化学习中，我们的广泛实验表明，通过展示合成指南的最终集合的高效用性",
    "tldr": "提出使用合成指南替换真实指南以增强隐私保护，并通过私密微调生成器生成此类合成指南，并通过新颖的过滤算法使合成指南的分布与真实指南一致，展示了在大型语言模型对齐中的高效用性。",
    "en_tdlr": "We propose using synthetic instructions to replace real instructions to enhance privacy protection, generated by privately fine-tuned generators, along with a novel filtering algorithm matching the distribution of synthetic instructions to real ones, demonstrating high utility in aligning large language models."
}