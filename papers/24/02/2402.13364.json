{
    "title": "A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction",
    "abstract": "arXiv:2402.13364v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated impressive abilities in generating unstructured natural language according to instructions. However, their performance can be inconsistent when tasked with producing text that adheres to specific structured formats, which is crucial in applications like named entity recognition (NER) or relation extraction (RE). To address this issue, this paper introduces an efficient method, G&O, to enhance their structured text generation capabilities. It breaks the generation into a two-step pipeline: initially, LLMs generate answers in natural language as intermediate responses. Subsequently, LLMs are asked to organize the output into the desired structure, using the intermediate responses as context. G&O effectively separates the generation of content from the structuring process, reducing the pressure of completing two orthogonal tasks simultaneously. Tested on zero-shot NER and RE, the results indica",
    "link": "https://arxiv.org/abs/2402.13364",
    "context": "Title: A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction\nAbstract: arXiv:2402.13364v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated impressive abilities in generating unstructured natural language according to instructions. However, their performance can be inconsistent when tasked with producing text that adheres to specific structured formats, which is crucial in applications like named entity recognition (NER) or relation extraction (RE). To address this issue, this paper introduces an efficient method, G&O, to enhance their structured text generation capabilities. It breaks the generation into a two-step pipeline: initially, LLMs generate answers in natural language as intermediate responses. Subsequently, LLMs are asked to organize the output into the desired structure, using the intermediate responses as context. G&O effectively separates the generation of content from the structuring process, reducing the pressure of completing two orthogonal tasks simultaneously. Tested on zero-shot NER and RE, the results indica",
    "path": "papers/24/02/2402.13364.json",
    "total_tokens": 826,
    "translated_title": "一种简单而有效的方法，改善结构化语言模型在信息抽取中的输出",
    "translated_abstract": "大型语言模型（LLMs）已经展示出在根据指令生成非结构化自然语言方面具有令人印象深刻的能力。然而，当要求它们生成符合特定结构化格式的文本时，它们的表现可能不一致，在命名实体识别（NER）或关系抽取（RE）等应用中这一点至关重要。为了解决这个问题，本文引入了一种高效的方法，G&O，以增强它们的结构化文本生成能力。它将生成分解为一个两步流程：首先，LLMs生成自然语言中的答案作为中间响应。随后，要求LLMs将输出组织成所需的结构，使用中间响应作为上下文。G&O有效地将内容生成与构建过程分离，减少了同时完成两个正交任务的压力。在零-shot NER和RE上进行测试，结果表明",
    "tldr": "该论文提出了一种名为G&O的方法，通过将内容生成与结构化过程分离，有效提升了大型语言模型在生成特定结构化文本上的性能。",
    "en_tdlr": "This paper introduces an efficient method, G&O, to enhance the structured text generation capabilities of large language models by separating content generation from the structuring process."
}