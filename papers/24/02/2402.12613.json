{
    "title": "Analysis of Using Sigmoid Loss for Contrastive Learning",
    "abstract": "arXiv:2402.12613v1 Announce Type: new  Abstract: Contrastive learning has emerged as a prominent branch of self-supervised learning for several years. Especially, CLIP, which applies contrastive learning to large sets of captioned images, has garnered significant attention. Recently, SigLIP, a variant of CLIP, has been proposed, which uses the sigmoid loss instead of the standard InfoNCE loss. SigLIP achieves the performance comparable to CLIP in a more efficient manner by eliminating the need for a global view. However, theoretical understanding of using the sigmoid loss in contrastive learning is underexplored. In this paper, we provide a theoretical analysis of using the sigmoid loss in contrastive learning, in the perspective of the geometric structure of learned embeddings. First, we propose the double-Constant Embedding Model (CCEM), a framework for parameterizing various well-known embedding structures by a single variable. Interestingly, the proposed CCEM is proven to contain t",
    "link": "https://arxiv.org/abs/2402.12613",
    "context": "Title: Analysis of Using Sigmoid Loss for Contrastive Learning\nAbstract: arXiv:2402.12613v1 Announce Type: new  Abstract: Contrastive learning has emerged as a prominent branch of self-supervised learning for several years. Especially, CLIP, which applies contrastive learning to large sets of captioned images, has garnered significant attention. Recently, SigLIP, a variant of CLIP, has been proposed, which uses the sigmoid loss instead of the standard InfoNCE loss. SigLIP achieves the performance comparable to CLIP in a more efficient manner by eliminating the need for a global view. However, theoretical understanding of using the sigmoid loss in contrastive learning is underexplored. In this paper, we provide a theoretical analysis of using the sigmoid loss in contrastive learning, in the perspective of the geometric structure of learned embeddings. First, we propose the double-Constant Embedding Model (CCEM), a framework for parameterizing various well-known embedding structures by a single variable. Interestingly, the proposed CCEM is proven to contain t",
    "path": "papers/24/02/2402.12613.json",
    "total_tokens": 815,
    "translated_title": "使用Sigmoid Loss进行对比学习的分析",
    "translated_abstract": "对比学习已经成为自监督学习中一个重要的分支数年。特别是，将对比学习应用于大量带标题的图片集的CLIP引起了很大关注。最近，提出了SigLIP，CLIP的一种变体，它使用sigmoid loss而不是标准的InfoNCE loss。SigLIP通过消除对全局视图的需求，以更有效的方式达到与CLIP相当的性能。然而，对比学习中使用sigmoid loss的理论理解尚未被充分探讨。在本文中，我们从学习嵌入的几何结构的角度，对在对比学习中使用sigmoid loss进行了理论分析。首先，我们提出了双常数嵌入模型（CCEM），一个通过单个变量来参数化各种众所周知的嵌入结构的框架。有趣的是，所提出的CCEM被证明包含了t",
    "tldr": "提出了双常数嵌入模型（CCEM）以理论分析使用Sigmoid Loss在对比学习中的应用，有可能提供更高效的性能。",
    "en_tdlr": "Proposed the Double-Constant Embedding Model (CCEM) to theoretically analyze the application of using Sigmoid Loss in contrastive learning, potentially providing more efficient performance."
}