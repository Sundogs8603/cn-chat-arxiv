{
    "title": "(Ir)rationality and Cognitive Biases in Large Language Models",
    "abstract": "arXiv:2402.09193v1 Announce Type: cross Abstract: Do large language models (LLMs) display rational reasoning? LLMs have been shown to contain human biases due to the data they have been trained on; whether this is reflected in rational reasoning remains less clear. In this paper, we answer this question by evaluating seven language models using tasks from the cognitive psychology literature. We find that, like humans, LLMs display irrationality in these tasks. However, the way this irrationality is displayed does not reflect that shown by humans. When incorrect answers are given by LLMs to these tasks, they are often incorrect in ways that differ from human-like biases. On top of this, the LLMs reveal an additional layer of irrationality in the significant inconsistency of the responses. Aside from the experimental results, this paper seeks to make a methodological contribution by showing how we can assess and compare different capabilities of these types of models, in this case with r",
    "link": "https://arxiv.org/abs/2402.09193",
    "context": "Title: (Ir)rationality and Cognitive Biases in Large Language Models\nAbstract: arXiv:2402.09193v1 Announce Type: cross Abstract: Do large language models (LLMs) display rational reasoning? LLMs have been shown to contain human biases due to the data they have been trained on; whether this is reflected in rational reasoning remains less clear. In this paper, we answer this question by evaluating seven language models using tasks from the cognitive psychology literature. We find that, like humans, LLMs display irrationality in these tasks. However, the way this irrationality is displayed does not reflect that shown by humans. When incorrect answers are given by LLMs to these tasks, they are often incorrect in ways that differ from human-like biases. On top of this, the LLMs reveal an additional layer of irrationality in the significant inconsistency of the responses. Aside from the experimental results, this paper seeks to make a methodological contribution by showing how we can assess and compare different capabilities of these types of models, in this case with r",
    "path": "papers/24/02/2402.09193.json",
    "total_tokens": 862,
    "translated_title": "(不)理性与大型语言模型中的认知偏差",
    "translated_abstract": "大型语言模型(LLMs)是否展现出理性推理？由于其训练数据所含的人类偏见，LLMs已被证实存在人类偏见；然而，其是否反映出了理性推理还不太清楚。本文通过评估七个语言模型在来自认知心理学文献的任务中回答了这个问题。我们发现，和人类一样，LLMs在这些任务中展现出了非理性。然而，LLMs展现出的这种非理性与人类的偏见不同。当LLMs给出错误答案时，它们通常会以与人类偏见不同的方式错误。除此之外，LLMs还展现出了响应的显著不一致性，这表明了额外的非理性层面。除了实验结果，本文还通过展示如何评估和比较这类模型的不同功能，对方法论作出了贡献。",
    "tldr": "本研究评估了七个大型语言模型在认知心理学任务中的表现，发现它们与人类一样存在非理性，但展示的非理性方式与人类偏见不同，同时还表现出了显著的回答不一致性。",
    "en_tdlr": "This study evaluates the performance of seven large language models on cognitive psychology tasks, finding that they exhibit irrationality similar to humans but in a different way, and also demonstrate significant inconsistency in their responses."
}