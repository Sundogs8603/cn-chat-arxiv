{
    "title": "Extreme Miscalibration and the Illusion of Adversarial Robustness",
    "abstract": "arXiv:2402.17509v1 Announce Type: new  Abstract: Deep learning-based Natural Language Processing (NLP) models are vulnerable to adversarial attacks, where small perturbations can cause a model to misclassify. Adversarial Training (AT) is often used to increase model robustness. However, we have discovered an intriguing phenomenon: deliberately or accidentally miscalibrating models masks gradients in a way that interferes with adversarial attack search methods, giving rise to an apparent increase in robustness. We show that this observed gain in robustness is an illusion of robustness (IOR), and demonstrate how an adversary can perform various forms of test-time temperature calibration to nullify the aforementioned interference and allow the adversarial attack to find adversarial examples. Hence, we urge the NLP community to incorporate test-time temperature scaling into their robustness evaluations to ensure that any observed gains are genuine. Finally, we show how the temperature can ",
    "link": "https://arxiv.org/abs/2402.17509",
    "context": "Title: Extreme Miscalibration and the Illusion of Adversarial Robustness\nAbstract: arXiv:2402.17509v1 Announce Type: new  Abstract: Deep learning-based Natural Language Processing (NLP) models are vulnerable to adversarial attacks, where small perturbations can cause a model to misclassify. Adversarial Training (AT) is often used to increase model robustness. However, we have discovered an intriguing phenomenon: deliberately or accidentally miscalibrating models masks gradients in a way that interferes with adversarial attack search methods, giving rise to an apparent increase in robustness. We show that this observed gain in robustness is an illusion of robustness (IOR), and demonstrate how an adversary can perform various forms of test-time temperature calibration to nullify the aforementioned interference and allow the adversarial attack to find adversarial examples. Hence, we urge the NLP community to incorporate test-time temperature scaling into their robustness evaluations to ensure that any observed gains are genuine. Finally, we show how the temperature can ",
    "path": "papers/24/02/2402.17509.json",
    "total_tokens": 856,
    "translated_title": "极端失调与对抗鲁棒性的幻觉",
    "translated_abstract": "基于深度学习的自然语言处理（NLP）模型容易受到对抗攻击的影响，微小的扰动可能导致模型误分类。对抗训练（AT）经常被用来提升模型的鲁棒性。然而，我们发现了一个有趣的现象：有意或无意地失调模型会掩盖梯度，从而干扰对抗攻击搜索方法，导致表面上看似增加了鲁棒性。我们展示了这种观察到的鲁棒性增益是一种鲁棒性幻觉（IOR），并展示了对手如何执行各种形式的测试时间温度校准来抵消上述干扰，使对抗攻击能够找到对抗样本。因此，我们敦促NLP社区在其鲁棒性评估中纳入测试时间温度缩放，以确保观察到的任何增益都是真实的。",
    "tldr": "深度学习模型的对抗训练可能会造成对抗性强化学习的幻觉，研究表明通过测试时间温度缩放可以消除这种幻觉。",
    "en_tdlr": "Adversarial training in deep learning models may create an illusion of adversarial robustness, and research suggests that this illusion can be eliminated through test-time temperature scaling."
}