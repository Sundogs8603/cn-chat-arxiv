{
    "title": "DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging",
    "abstract": "The transformer architecture from Vaswani et al. (2017) is now ubiquitous across application domains, from natural language processing to speech processing and image understanding. We propose DenseFormer, a simple modification to the standard architecture that improves the perplexity of the model without increasing its size -- adding a few thousand parameters for large-scale models in the 100B parameters range. Our approach relies on an additional averaging step after each transformer block, which computes a weighted average of current and past representations -- we refer to this operation as Depth-Weighted-Average (DWA). The learned DWA weights exhibit coherent patterns of information flow, revealing the strong and structured reuse of activations from distant layers. Experiments demonstrate that DenseFormer is more data efficient, reaching the same perplexity of much deeper transformer models, and that for the same perplexity, these new models outperform transformer baselines in terms",
    "link": "https://arxiv.org/abs/2402.02622",
    "context": "Title: DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging\nAbstract: The transformer architecture from Vaswani et al. (2017) is now ubiquitous across application domains, from natural language processing to speech processing and image understanding. We propose DenseFormer, a simple modification to the standard architecture that improves the perplexity of the model without increasing its size -- adding a few thousand parameters for large-scale models in the 100B parameters range. Our approach relies on an additional averaging step after each transformer block, which computes a weighted average of current and past representations -- we refer to this operation as Depth-Weighted-Average (DWA). The learned DWA weights exhibit coherent patterns of information flow, revealing the strong and structured reuse of activations from distant layers. Experiments demonstrate that DenseFormer is more data efficient, reaching the same perplexity of much deeper transformer models, and that for the same perplexity, these new models outperform transformer baselines in terms",
    "path": "papers/24/02/2402.02622.json",
    "total_tokens": 902,
    "translated_title": "DenseFormer: 通过深度加权平均增强Transformer中的信息流",
    "translated_abstract": "从Vaswani等人（2017）的Transformer架构现已普遍应用于各个应用领域，从自然语言处理到语音处理和图像理解。我们提出了DenseFormer，这是对标准架构的简单修改，提高了模型的困惑度，而不增加其大小-对于拥有100B参数范围的大规模模型，只需添加几千个参数。我们的方法在每个transformer块之后依靠额外的平均步骤，计算当前和过去表示的加权平均-我们将这个操作称为深度加权平均（DWA）。学到的DWA权重展现了信息流的连贯模式，揭示了来自远层的激活的强大且结构化的重复使用。实验证明DenseFormer具有更高的数据效率，能够达到比更深的transformer模型相同的困惑度，并且在相同困惑度下，这些新模型在性能上超过了transformer基准模型。",
    "tldr": "DenseFormer是对Transformer的简单修改，通过在每个transformer块之后进行深度加权平均，提高了模型的困惑度。学到的加权平均权重揭示了信息流的连贯模式，使得DenseFormer具有更高的数据效率，并且在相同困惑度下胜过传统的Transformer模型。",
    "en_tdlr": "DenseFormer is a simple modification to the Transformer architecture that improves model perplexity by performing depth weighted averaging after each transformer block. The learned weights exhibit coherent patterns of information flow, making DenseFormer more data efficient and outperforming baseline Transformer models at the same perplexity."
}