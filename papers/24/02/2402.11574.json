{
    "title": "Visual In-Context Learning for Large Vision-Language Models",
    "abstract": "arXiv:2402.11574v1 Announce Type: cross  Abstract: In Large Visual Language Models (LVLMs), the efficacy of In-Context Learning (ICL) remains limited by challenges in cross-modal interactions and representation disparities. To overcome these challenges, we introduce a novel Visual In-Context Learning (VICL) method comprising Visual Demonstration Retrieval, Intent-Oriented Image Summarization, and Intent-Oriented Demonstration Composition. Our approach retrieves images via ''Retrieval & Rerank'' paradigm, summarises images with task intent and task-specific visual parsing, and composes language-based demonstrations that reduce token count and alleviate cross-modal interaction problem. Experimental evaluations on five visual reasoning datasets demonstrate the effectiveness of our method. Moreover, our extensive experiments leverage information flow analysis to elucidate the effectiveness of our method, and investigate the impact of length and position of demonstrations for LVLM. The use ",
    "link": "https://arxiv.org/abs/2402.11574",
    "context": "Title: Visual In-Context Learning for Large Vision-Language Models\nAbstract: arXiv:2402.11574v1 Announce Type: cross  Abstract: In Large Visual Language Models (LVLMs), the efficacy of In-Context Learning (ICL) remains limited by challenges in cross-modal interactions and representation disparities. To overcome these challenges, we introduce a novel Visual In-Context Learning (VICL) method comprising Visual Demonstration Retrieval, Intent-Oriented Image Summarization, and Intent-Oriented Demonstration Composition. Our approach retrieves images via ''Retrieval & Rerank'' paradigm, summarises images with task intent and task-specific visual parsing, and composes language-based demonstrations that reduce token count and alleviate cross-modal interaction problem. Experimental evaluations on five visual reasoning datasets demonstrate the effectiveness of our method. Moreover, our extensive experiments leverage information flow analysis to elucidate the effectiveness of our method, and investigate the impact of length and position of demonstrations for LVLM. The use ",
    "path": "papers/24/02/2402.11574.json",
    "total_tokens": 836,
    "translated_title": "大视觉-语言模型中的视觉上下文学习",
    "translated_abstract": "在大视觉语言模型（LVLMs）中，上下文学习（ICL）的有效性仍受到跨模态交互和表示差异的挑战的限制。为了克服这些挑战，我们引入了一种新颖的Visual In-Context Learning（VICL）方法，包括Visual Demonstration Retrieval、Intent-Oriented Image Summarization和Intent-Oriented Demonstration Composition。我们的方法通过“检索与重新排名”范式检索图像，用任务意图和任务特定的视觉解析总结图像，并组成基于语言的演示，减少标记计数并缓解跨模态交互问题。对五个可视推理数据集的实验评估证明了我们方法的有效性。此外，我们广泛的实验利用信息流分析阐明了我们方法的有效性，并研究了演示的长度和位置对LVLM的影响。",
    "tldr": "提出了一种新的Visual In-Context Learning（VICL）方法，通过检索和重新排名图像、用任务意图和任务特定的视觉解析总结图像，以及组成语言演示来减少标记计数和减轻跨模态交互问题。",
    "en_tdlr": "Introduced a novel Visual In-Context Learning (VICL) method, which retrieves and reranks images, summarizes images with task intent and task-specific visual parsing, and composes language-based demonstrations to reduce token count and alleviate cross-modal interaction problems."
}