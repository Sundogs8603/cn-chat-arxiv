{
    "title": "Don't Label Twice: Quantity Beats Quality when Comparing Binary Classifiers on a Budget",
    "abstract": "We study how to best spend a budget of noisy labels to compare the accuracy of two binary classifiers. It's common practice to collect and aggregate multiple noisy labels for a given data point into a less noisy label via a majority vote. We prove a theorem that runs counter to conventional wisdom. If the goal is to identify the better of two classifiers, we show it's best to spend the budget on collecting a single label for more samples. Our result follows from a non-trivial application of Cram\\'er's theorem, a staple in the theory of large deviations. We discuss the implications of our work for the design of machine learning benchmarks, where they overturn some time-honored recommendations. In addition, our results provide sample size bounds superior to what follows from Hoeffding's bound.",
    "link": "https://arxiv.org/abs/2402.02249",
    "context": "Title: Don't Label Twice: Quantity Beats Quality when Comparing Binary Classifiers on a Budget\nAbstract: We study how to best spend a budget of noisy labels to compare the accuracy of two binary classifiers. It's common practice to collect and aggregate multiple noisy labels for a given data point into a less noisy label via a majority vote. We prove a theorem that runs counter to conventional wisdom. If the goal is to identify the better of two classifiers, we show it's best to spend the budget on collecting a single label for more samples. Our result follows from a non-trivial application of Cram\\'er's theorem, a staple in the theory of large deviations. We discuss the implications of our work for the design of machine learning benchmarks, where they overturn some time-honored recommendations. In addition, our results provide sample size bounds superior to what follows from Hoeffding's bound.",
    "path": "papers/24/02/2402.02249.json",
    "total_tokens": 769,
    "translated_title": "不要重复标记：在有限预算下比较二元分类器时，数量胜过质量",
    "translated_abstract": "我们研究了如何更好地利用有限预算来比较两个二元分类器的准确性。通常的做法是通过多次收集和汇总给定数据点的多个噪声标签，通过多数投票形成一个不太噪声的标签。我们证明了一个与常识相反的定理。如果目标是确定两个分类器中的较好者，我们展示了更好的做法是将预算用于收集更多样本的单个标签。我们的结果来自于对Cram\\'er定理的非平凡应用，这是大偏差理论中的一个重要工具。我们讨论了我们的工作对机器学习基准设计的影响，其中它们推翻了一些历史上的建议。此外，我们的结果提供了比Hoeffding界更优的样本大小界限。",
    "tldr": "在比较两个二元分类器的准确性时，通过收集更多样本的单个标签而不是汇总多个噪声标签能更好地利用预算。",
    "en_tdlr": "When comparing the accuracy of two binary classifiers, it is better to allocate the budget to collect a single label for more samples rather than aggregating multiple noisy labels."
}