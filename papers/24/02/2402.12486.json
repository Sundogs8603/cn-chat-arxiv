{
    "title": "Do Pre-Trained Language Models Detect and Understand Semantic Underspecification? Ask the DUST!",
    "abstract": "arXiv:2402.12486v1 Announce Type: new  Abstract: In everyday language use, speakers frequently utter and interpret sentences that are semantically underspecified, namely, whose content is insufficient to fully convey their message or interpret them univocally. For example, to interpret the underspecified sentence \"Don't spend too much\", which leaves implicit what (not) to spend, additional linguistic context or outside knowledge is needed. In this work, we propose a novel Dataset of semantically Underspecified Sentences grouped by Type (DUST) and use it to study whether pre-trained language models (LMs) correctly identify and interpret underspecified sentences. We find that newer LMs are reasonably able to identify underspecified sentences when explicitly prompted. However, interpreting them correctly is much harder for any LMs. Our experiments show that when interpreting underspecified sentences, LMs exhibit little uncertainty, contrary to what theoretical accounts of underspecificati",
    "link": "https://arxiv.org/abs/2402.12486",
    "context": "Title: Do Pre-Trained Language Models Detect and Understand Semantic Underspecification? Ask the DUST!\nAbstract: arXiv:2402.12486v1 Announce Type: new  Abstract: In everyday language use, speakers frequently utter and interpret sentences that are semantically underspecified, namely, whose content is insufficient to fully convey their message or interpret them univocally. For example, to interpret the underspecified sentence \"Don't spend too much\", which leaves implicit what (not) to spend, additional linguistic context or outside knowledge is needed. In this work, we propose a novel Dataset of semantically Underspecified Sentences grouped by Type (DUST) and use it to study whether pre-trained language models (LMs) correctly identify and interpret underspecified sentences. We find that newer LMs are reasonably able to identify underspecified sentences when explicitly prompted. However, interpreting them correctly is much harder for any LMs. Our experiments show that when interpreting underspecified sentences, LMs exhibit little uncertainty, contrary to what theoretical accounts of underspecificati",
    "path": "papers/24/02/2402.12486.json",
    "total_tokens": 758,
    "translated_title": "预训练语言模型是否能检测和理解语义不确定性？问问DUST！",
    "translated_abstract": "在日常语言使用中，说话者经常使用语义不确定的句子，即内容不足以完全传达其信息或以唯一方式解释它们。本文提出了一个新的“语义不确定句子类型分组数据集”（DUST），用来研究预训练语言模型是否能正确识别和解释语义不确定的句子。我们发现，新的语言模型在明确提示的情况下，能够相当准确地识别语义不确定的句子，但对其进行正确解释则更为困难。我们的实验表明，在解释语义不确定的句子时，语言模型表现出较小的不确定性，与语义不确定性的理论描述相反。",
    "tldr": "预训练语言模型在明确提示时能够相当准确地识别语义不确定的句子，但对其进行正确解释则更为困难。"
}