{
    "title": "Achieving $\\tilde{O}(1/\\epsilon)$ Sample Complexity for Constrained Markov Decision Process",
    "abstract": "arXiv:2402.16324v1 Announce Type: new  Abstract: We consider the reinforcement learning problem for the constrained Markov decision process (CMDP), which plays a central role in satisfying safety or resource constraints in sequential learning and decision-making. In this problem, we are given finite resources and a MDP with unknown transition probabilities. At each stage, we take an action, collecting a reward and consuming some resources, all assumed to be unknown and need to be learned over time. In this work, we take the first step towards deriving optimal problem-dependent guarantees for the CMDP problems. We derive a logarithmic regret bound, which translates into a $O(\\frac{\\kappa}{\\epsilon}\\cdot\\log^2(1/\\epsilon))$ sample complexity bound, with $\\kappa$ being a problem-dependent parameter, yet independent of $\\epsilon$. Our sample complexity bound improves upon the state-of-art $O(1/\\epsilon^2)$ sample complexity for CMDP problems established in the previous literature, in terms",
    "link": "https://arxiv.org/abs/2402.16324",
    "context": "Title: Achieving $\\tilde{O}(1/\\epsilon)$ Sample Complexity for Constrained Markov Decision Process\nAbstract: arXiv:2402.16324v1 Announce Type: new  Abstract: We consider the reinforcement learning problem for the constrained Markov decision process (CMDP), which plays a central role in satisfying safety or resource constraints in sequential learning and decision-making. In this problem, we are given finite resources and a MDP with unknown transition probabilities. At each stage, we take an action, collecting a reward and consuming some resources, all assumed to be unknown and need to be learned over time. In this work, we take the first step towards deriving optimal problem-dependent guarantees for the CMDP problems. We derive a logarithmic regret bound, which translates into a $O(\\frac{\\kappa}{\\epsilon}\\cdot\\log^2(1/\\epsilon))$ sample complexity bound, with $\\kappa$ being a problem-dependent parameter, yet independent of $\\epsilon$. Our sample complexity bound improves upon the state-of-art $O(1/\\epsilon^2)$ sample complexity for CMDP problems established in the previous literature, in terms",
    "path": "papers/24/02/2402.16324.json",
    "total_tokens": 911,
    "translated_title": "实现约$O(1/\\epsilon)$的样本复杂度用于约束马尔可夫决策过程",
    "translated_abstract": "我们考虑约束马尔可夫决策过程（CMDP）的强化学习问题，在顺序学习和决策中满足安全性或资源约束方面起着关键作用。在这个问题中，我们拥有有限资源和未知转移概率的MDP。在每个阶段，我们采取一个行动，收集奖励并消耗一些资源，所有假设都是未知的，并且需要随着时间学习。在这项工作中，我们迈出了为CMDP问题推导出最优的问题相关保证的第一步。我们得出了一个对数遗憾界限，这转化为$O(\\frac{\\kappa}{\\epsilon}\\cdot\\log^2(1/\\epsilon))$的样本复杂度界限，其中$\\kappa$是一个与问题相关的参数，但与$\\epsilon$无关。我们的样本复杂度界限改进了先前文献中针对CMDP问题建立的$O(1/\\epsilon^2)$样本复杂度。",
    "tldr": "该论文提出了一种算法，在约束马尔可夫决策过程中实现了约$O(1/\\epsilon)$的样本复杂度，相比先前文献中已有的$O(1/\\epsilon^2)$样本复杂度有所提升。"
}