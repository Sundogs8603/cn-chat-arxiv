{
    "title": "OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems",
    "abstract": "arXiv:2402.14008v1 Announce Type: new  Abstract: Recent advancements have seen Large Language Models (LLMs) and Large Multimodal Models (LMMs) surpassing general human capabilities in various tasks, approaching the proficiency level of human experts across multiple domains. With traditional benchmarks becoming less challenging for these models, new rigorous challenges are essential to gauge their advanced abilities. In this work, we present OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring 8,952 problems from Olympiad-level mathematics and physics competitions, including the Chinese college entrance exam. Each problem is detailed with expert-level annotations for step-by-step reasoning. Evaluating top-tier models on OlympiadBench, we implement a comprehensive assessment methodology to accurately evaluate model responses. Notably, the best-performing model, GPT-4V, attains an average score of 17.23% on OlympiadBench, with a mere 11.28% in physics, hig",
    "link": "https://arxiv.org/abs/2402.14008",
    "context": "Title: OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems\nAbstract: arXiv:2402.14008v1 Announce Type: new  Abstract: Recent advancements have seen Large Language Models (LLMs) and Large Multimodal Models (LMMs) surpassing general human capabilities in various tasks, approaching the proficiency level of human experts across multiple domains. With traditional benchmarks becoming less challenging for these models, new rigorous challenges are essential to gauge their advanced abilities. In this work, we present OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring 8,952 problems from Olympiad-level mathematics and physics competitions, including the Chinese college entrance exam. Each problem is detailed with expert-level annotations for step-by-step reasoning. Evaluating top-tier models on OlympiadBench, we implement a comprehensive assessment methodology to accurately evaluate model responses. Notably, the best-performing model, GPT-4V, attains an average score of 17.23% on OlympiadBench, with a mere 11.28% in physics, hig",
    "path": "papers/24/02/2402.14008.json",
    "total_tokens": 878,
    "translated_title": "OlympiadBench：一个具有奥林匹亚级别双语多模态科学问题的挑战性基准",
    "translated_abstract": "最近的进展使得大型语言模型（LLMs）和大型多模态模型（LMMs）在各种任务中超越了一般人类的能力，接近了多个领域人类专家的熟练水平。本文提出了OlympiadBench，一个奥林匹亚级别的双语多模态科学基准，包括来自奥林匹亚级别数学和物理竞赛以及中国高考的8952个问题。每个问题都配有专家级注释，以进行逐步的推理。在OlympiadBench上评估顶尖模型，我们实施了全面的评估方法，以准确评估模型的响应。值得注意的是，表现最佳的模型GPT-4V在OlympiadBench上获得了17.23%的平均分，其中在物理学中仅为11.28%。",
    "tldr": "提出了OlympiadBench，一个奥林匹亚级别的双语多模态科学基准，包括8952个问题，旨在评估大型语言模型和多模态模型在复杂问题上的能力。",
    "en_tdlr": "Introduced OlympiadBench, a challenging benchmark with Olympiad-level bilingual multimodal scientific problems, aiming to evaluate the capabilities of Large Language Models (LLMs) and Large Multimodal Models (LMMs) in complex problems."
}