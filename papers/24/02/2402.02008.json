{
    "title": "How well do LLMs cite relevant medical references? An evaluation framework and analyses",
    "abstract": "Large language models (LLMs) are currently being used to answer medical questions across a variety of clinical domains. Recent top-performing commercial LLMs, in particular, are also capable of citing sources to support their responses. In this paper, we ask: do the sources that LLMs generate actually support the claims that they make? To answer this, we propose three contributions. First, as expert medical annotations are an expensive and time-consuming bottleneck for scalable evaluation, we demonstrate that GPT-4 is highly accurate in validating source relevance, agreeing 88% of the time with a panel of medical doctors. Second, we develop an end-to-end, automated pipeline called \\textit{SourceCheckup} and use it to evaluate five top-performing LLMs on a dataset of 1200 generated questions, totaling over 40K pairs of statements and sources. Interestingly, we find that between ~50% to 90% of LLM responses are not fully supported by the sources they provide. We also evaluate GPT-4 with ",
    "link": "https://arxiv.org/abs/2402.02008",
    "context": "Title: How well do LLMs cite relevant medical references? An evaluation framework and analyses\nAbstract: Large language models (LLMs) are currently being used to answer medical questions across a variety of clinical domains. Recent top-performing commercial LLMs, in particular, are also capable of citing sources to support their responses. In this paper, we ask: do the sources that LLMs generate actually support the claims that they make? To answer this, we propose three contributions. First, as expert medical annotations are an expensive and time-consuming bottleneck for scalable evaluation, we demonstrate that GPT-4 is highly accurate in validating source relevance, agreeing 88% of the time with a panel of medical doctors. Second, we develop an end-to-end, automated pipeline called \\textit{SourceCheckup} and use it to evaluate five top-performing LLMs on a dataset of 1200 generated questions, totaling over 40K pairs of statements and sources. Interestingly, we find that between ~50% to 90% of LLM responses are not fully supported by the sources they provide. We also evaluate GPT-4 with ",
    "path": "papers/24/02/2402.02008.json",
    "total_tokens": 954,
    "translated_title": "LLM是否可以正确引用相关医学参考文献？一个评估框架和分析",
    "translated_abstract": "目前在各种临床领域中，大型语言模型（LLM）被广泛用于回答医学问题。特别是最近表现出色的商业LLM，它们能够引用来源来支持其回答。本文提出一个问题：LLM生成的来源是否真正支持它们所做的主张？为了回答这个问题，我们提出了三个贡献。首先，专家的医学注释是一种昂贵且耗时的评估瓶颈，我们证明GPT-4在验证源的相关性方面非常准确，与一组医生的判断达到88%的一致性。其次，我们开发了一个名为“SourceCheckup”的端到端自动化流水线，并使用它评估了1200个生成的问题上的五个表现最佳的LLM，总计超过40K对的陈述和来源。有趣的是，我们发现50%到90%的LLM回答并没有充分地得到它们所提供的来源的支持。我们还对GPT-4进行了评估...",
    "tldr": "这项研究提出了一个问题：LLM生成的来源是否真正支持它们所做的主张？通过验证源的相关性和开发端到端的自动化流水线，研究人员发现50%到90%的LLM回答并没有充分地得到它们所提供的来源的支持。"
}