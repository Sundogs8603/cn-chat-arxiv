{
    "title": "On the Transferability of Large-Scale Self-Supervision to Few-Shot Audio Classification",
    "abstract": "In recent years, self-supervised learning has excelled for its capacity to learn robust feature representations from unlabelled data. Networks pretrained through self-supervision serve as effective feature extractors for downstream tasks, including Few-Shot Learning. While the evaluation of unsupervised approaches for few-shot learning is well-established in imagery, it is notably absent in acoustics. This study addresses this gap by assessing large-scale self-supervised models' performance in few-shot audio classification. Additionally, we explore the relationship between a model's few-shot learning capability and other downstream task benchmarks. Our findings reveal state-of-the-art performance in some few-shot problems such as SpeechCommandsv2, as well as strong correlations between speech-based few-shot problems and various downstream audio tasks.",
    "link": "https://rss.arxiv.org/abs/2402.01274",
    "context": "Title: On the Transferability of Large-Scale Self-Supervision to Few-Shot Audio Classification\nAbstract: In recent years, self-supervised learning has excelled for its capacity to learn robust feature representations from unlabelled data. Networks pretrained through self-supervision serve as effective feature extractors for downstream tasks, including Few-Shot Learning. While the evaluation of unsupervised approaches for few-shot learning is well-established in imagery, it is notably absent in acoustics. This study addresses this gap by assessing large-scale self-supervised models' performance in few-shot audio classification. Additionally, we explore the relationship between a model's few-shot learning capability and other downstream task benchmarks. Our findings reveal state-of-the-art performance in some few-shot problems such as SpeechCommandsv2, as well as strong correlations between speech-based few-shot problems and various downstream audio tasks.",
    "path": "papers/24/02/2402.01274.json",
    "total_tokens": 850,
    "translated_title": "关于大规模自监督学习在少样本音频分类中的可迁移性",
    "translated_abstract": "近年来，自监督学习因其能够从无标签数据中学习到稳健的特征表示而表现出色。经过自监督预训练的网络可作为下游任务（包括少样本学习）中有效的特征提取器。尽管对于图像的无监督学习方法在少样本学习中的评估已经有了良好的基础，但在声学领域却明显缺失。本研究通过评估大规模自监督模型在少样本音频分类中的性能，弥补了这一空白。此外，我们还探讨了模型的少样本学习能力与其他下游任务基准之间的关系。我们的研究结果表明，在一些少样本问题（如SpeechCommandsv2）中，我们取得了最先进的性能，并且语音为基础的少样本问题与多个下游音频任务之间存在着较强的相关性。",
    "tldr": "本研究评估了大规模自监督模型在少样本音频分类中的性能，并发现在一些少样本问题中取得了最先进的性能，同时发现语音为基础的少样本问题与多个下游音频任务之间存在较强的相关性。",
    "en_tdlr": "This study evaluates the performance of large-scale self-supervised models in few-shot audio classification and finds state-of-the-art performance in some few-shot problems. It also reveals strong correlations between speech-based few-shot problems and various downstream audio tasks."
}