{
    "title": "Large Language Models for Psycholinguistic Plausibility Pretesting",
    "abstract": "In psycholinguistics, the creation of controlled materials is crucial to ensure that research outcomes are solely attributed to the intended manipulations and not influenced by extraneous factors. To achieve this, psycholinguists typically pretest linguistic materials, where a common pretest is to solicit plausibility judgments from human evaluators on specific sentences. In this work, we investigate whether Language Models (LMs) can be used to generate these plausibility judgements. We investigate a wide range of LMs across multiple linguistic structures and evaluate whether their plausibility judgements correlate with human judgements. We find that GPT-4 plausibility judgements highly correlate with human judgements across the structures we examine, whereas other LMs correlate well with humans on commonly used syntactic structures. We then test whether this correlation implies that LMs can be used instead of humans for pretesting. We find that when coarse-grained plausibility judgeme",
    "link": "https://arxiv.org/abs/2402.05455",
    "context": "Title: Large Language Models for Psycholinguistic Plausibility Pretesting\nAbstract: In psycholinguistics, the creation of controlled materials is crucial to ensure that research outcomes are solely attributed to the intended manipulations and not influenced by extraneous factors. To achieve this, psycholinguists typically pretest linguistic materials, where a common pretest is to solicit plausibility judgments from human evaluators on specific sentences. In this work, we investigate whether Language Models (LMs) can be used to generate these plausibility judgements. We investigate a wide range of LMs across multiple linguistic structures and evaluate whether their plausibility judgements correlate with human judgements. We find that GPT-4 plausibility judgements highly correlate with human judgements across the structures we examine, whereas other LMs correlate well with humans on commonly used syntactic structures. We then test whether this correlation implies that LMs can be used instead of humans for pretesting. We find that when coarse-grained plausibility judgeme",
    "path": "papers/24/02/2402.05455.json",
    "total_tokens": 960,
    "translated_title": "大型语言模型用于心理语言学合理性预测",
    "translated_abstract": "在心理语言学中，创建受控材料对于确保研究结果仅归因于预期操作而不受外部因素影响至关重要。为了实现这一目标，心理语言学家通常会对语言材料进行预测试，其中一种常见的预测试是向人类评估者征求关于特定句子的合理性判断。在本文中，我们研究了是否可以利用语言模型(LMs)生成这些合理性判断。我们研究了多种不同语言结构的LMs，并评估它们的合理性判断与人类判断之间是否相关。我们发现GPT-4的合理性判断与我们研究的不同结构的人类判断高度相关，而其他LMs在常用的句法结构上与人类的判断具有良好的相关性。然后，我们测试了这种相关性是否意味着可以用LMs代替人类进行预测试。我们发现，当粗粒度的合理性判断进行时，LMs的结果与人类评估者的结果高度相关，并且LMs的合理性判断可以代替人类进行合理性预测试。",
    "tldr": "本研究探讨了使用语言模型(LMs)生成语言材料的合理性判断，并发现GPT-4的合理性判断与人类判断高度相关。这意味着LMs可以替代人类进行预测试，从而在心理语言学中具有重要的应用潜力。",
    "en_tdlr": "This paper investigates the use of language models (LMs) to generate plausibility judgements for linguistic materials in psycholinguistics, and finds that the plausibility judgements of GPT-4 highly correlate with human judgements. This suggests that LMs can be used instead of humans for pretesting, demonstrating their potential application in psycholinguistics."
}