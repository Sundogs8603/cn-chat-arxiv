{
    "title": "A Critical Evaluation of AI Feedback for Aligning Large Language Models",
    "abstract": "arXiv:2402.12366v1 Announce Type: cross  Abstract: Reinforcement learning with AI feedback (RLAIF) is a popular paradigm for improving the instruction-following abilities of powerful pre-trained language models. RLAIF first performs supervised fine-tuning (SFT) using demonstrations from a teacher model and then further fine-tunes the model with reinforcement learning (RL), using feedback from a critic model. While recent popular open-source models have demonstrated substantial improvements in performance from the RL step, in this paper we question whether the complexity of this RL step is truly warranted for AI feedback. We show that the improvements of the RL step are virtually entirely due to the widespread practice of using a weaker teacher model (e.g. GPT-3.5) for SFT data collection than the critic (e.g., GPT-4) used for AI feedback generation. Specifically, we show that simple supervised fine-tuning with GPT-4 as the teacher outperforms existing RLAIF pipelines. More generally, w",
    "link": "https://arxiv.org/abs/2402.12366",
    "context": "Title: A Critical Evaluation of AI Feedback for Aligning Large Language Models\nAbstract: arXiv:2402.12366v1 Announce Type: cross  Abstract: Reinforcement learning with AI feedback (RLAIF) is a popular paradigm for improving the instruction-following abilities of powerful pre-trained language models. RLAIF first performs supervised fine-tuning (SFT) using demonstrations from a teacher model and then further fine-tunes the model with reinforcement learning (RL), using feedback from a critic model. While recent popular open-source models have demonstrated substantial improvements in performance from the RL step, in this paper we question whether the complexity of this RL step is truly warranted for AI feedback. We show that the improvements of the RL step are virtually entirely due to the widespread practice of using a weaker teacher model (e.g. GPT-3.5) for SFT data collection than the critic (e.g., GPT-4) used for AI feedback generation. Specifically, we show that simple supervised fine-tuning with GPT-4 as the teacher outperforms existing RLAIF pipelines. More generally, w",
    "path": "papers/24/02/2402.12366.json",
    "total_tokens": 870,
    "translated_title": "对大型语言模型进行AI反馈的关键评估",
    "translated_abstract": "强化学习与AI反馈（RLAIF）是一种用于提高强大预训练语言模型的指令遵循能力的流行范式。 RLAIF首先使用来自教师模型的示范进行监督微调（SFT），然后再使用来自评论模型的反馈进行强化学习（RL）进一步微调模型。尽管最近流行的开源模型已经证明了从RL步骤中获得的性能显着提高，但在本文中，我们质疑是否复杂的RL步骤真正有必要为AI反馈。我们展示了RL步骤的改进几乎完全是因为使用较弱的教师模型（例如GPT-3.5）用于SFT数据收集而不是用于AI反馈生成的评论者（例如GPT-4）的广泛实践。具体而言，我们展示了简单的以GPT-4作为教师的监督微调优于现有的RLAIF管道。",
    "tldr": "研究质疑复杂的强化学习在AI反馈中的必要性，表明使用更强的教师模型进行监督微调可以超越现有的RLAIF管道。",
    "en_tdlr": "The study questions the necessity of complex reinforcement learning in AI feedback, demonstrating that using a stronger teacher model for supervised fine-tuning can outperform existing RLAIF pipelines."
}