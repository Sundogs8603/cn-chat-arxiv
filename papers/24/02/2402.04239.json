{
    "title": "CAST: Clustering Self-Attention using Surrogate Tokens for Efficient Transformers",
    "abstract": "The Transformer architecture has shown to be a powerful tool for a wide range of tasks. It is based on the self-attention mechanism, which is an inherently computationally expensive operation with quadratic computational complexity: memory usage and compute time increase quadratically with the length of the input sequences, thus limiting the application of Transformers. In this work, we propose a novel Clustering self-Attention mechanism using Surrogate Tokens (CAST), to optimize the attention computation and achieve efficient transformers. CAST utilizes learnable surrogate tokens to construct a cluster affinity matrix, used to cluster the input sequence and generate novel cluster summaries. The self-attention from within each cluster is then combined with the cluster summaries of other clusters, enabling information flow across the entire input sequence. CAST improves efficiency by reducing the complexity from $O(N^2)$ to $O(\\alpha N)$ where N is the sequence length, and {\\alpha} is c",
    "link": "https://arxiv.org/abs/2402.04239",
    "context": "Title: CAST: Clustering Self-Attention using Surrogate Tokens for Efficient Transformers\nAbstract: The Transformer architecture has shown to be a powerful tool for a wide range of tasks. It is based on the self-attention mechanism, which is an inherently computationally expensive operation with quadratic computational complexity: memory usage and compute time increase quadratically with the length of the input sequences, thus limiting the application of Transformers. In this work, we propose a novel Clustering self-Attention mechanism using Surrogate Tokens (CAST), to optimize the attention computation and achieve efficient transformers. CAST utilizes learnable surrogate tokens to construct a cluster affinity matrix, used to cluster the input sequence and generate novel cluster summaries. The self-attention from within each cluster is then combined with the cluster summaries of other clusters, enabling information flow across the entire input sequence. CAST improves efficiency by reducing the complexity from $O(N^2)$ to $O(\\alpha N)$ where N is the sequence length, and {\\alpha} is c",
    "path": "papers/24/02/2402.04239.json",
    "total_tokens": 832,
    "translated_title": "CAST：利用替代令牌进行聚类自注意力的高效Transformer",
    "translated_abstract": "Transformer架构已经证明是一种强大的工具，适用于广泛的任务。它基于自注意机制，这是一种本质上计算密集型的操作，其计算复杂性呈二次增长：内存使用和计算时间与输入序列的长度呈二次增加，从而限制了Transformers的应用。在这项工作中，我们提出了一种新颖的利用替代令牌进行聚类自注意力机制（CAST），以优化注意力计算，实现高效的Transformer。CAST利用可学习的替代令牌构建聚类亲和矩阵，用于对输入序列进行聚类并生成新的聚类摘要。然后将每个聚类内的自注意力与其他聚类的聚类摘要相结合，实现整个输入序列上的信息流动。CAST通过将复杂度从O(N^2)降低到O(αN)，其中N为序列长度，α为常数，从而提高了效率。",
    "tldr": "CAST提出了一种利用替代令牌进行聚类自注意力的高效Transformer机制，通过减少计算复杂度，实现了在整个输入序列上的信息流动，从而提高了效率。",
    "en_tdlr": "CAST proposes an efficient Transformer mechanism by using surrogate tokens for clustering self-attention, which improves efficiency by reducing computational complexity and enables information flow across the entire input sequence."
}