{
    "title": "MSPipe: Efficient Temporal GNN Training via Staleness-aware Pipeline",
    "abstract": "arXiv:2402.15113v1 Announce Type: new  Abstract: Memory-based Temporal Graph Neural Networks (MTGNNs) are a class of temporal graph neural networks that utilize a node memory module to capture and retain long-term temporal dependencies, leading to superior performance compared to memory-less counterparts. However, the iterative reading and updating process of the memory module in MTGNNs to obtain up-to-date information needs to follow the temporal dependencies. This introduces significant overhead and limits training throughput. Existing optimizations for static GNNs are not directly applicable to MTGNNs due to differences in training paradigm, model architecture, and the absence of a memory module. Moreover, they do not effectively address the challenges posed by temporal dependencies, making them ineffective for MTGNN training. In this paper, we propose MSPipe, a general and efficient framework for MTGNNs that maximizes training throughput while maintaining model accuracy. Our design",
    "link": "https://arxiv.org/abs/2402.15113",
    "context": "Title: MSPipe: Efficient Temporal GNN Training via Staleness-aware Pipeline\nAbstract: arXiv:2402.15113v1 Announce Type: new  Abstract: Memory-based Temporal Graph Neural Networks (MTGNNs) are a class of temporal graph neural networks that utilize a node memory module to capture and retain long-term temporal dependencies, leading to superior performance compared to memory-less counterparts. However, the iterative reading and updating process of the memory module in MTGNNs to obtain up-to-date information needs to follow the temporal dependencies. This introduces significant overhead and limits training throughput. Existing optimizations for static GNNs are not directly applicable to MTGNNs due to differences in training paradigm, model architecture, and the absence of a memory module. Moreover, they do not effectively address the challenges posed by temporal dependencies, making them ineffective for MTGNN training. In this paper, we propose MSPipe, a general and efficient framework for MTGNNs that maximizes training throughput while maintaining model accuracy. Our design",
    "path": "papers/24/02/2402.15113.json",
    "total_tokens": 841,
    "translated_title": "MSPipe: 通过意识到陈旧性的管道实现高效的时间性GNN训练",
    "translated_abstract": "记忆型时间性图神经网络（MTGNNs）是一类利用节点记忆模块捕获和保留长期时间依赖关系的时间性图神经网络，相对于无记忆的对应网络具有卓越的性能。然而，在MTGNNs中，为了获取最新的信息，记忆模块的迭代读取和更新过程需要遵循时间依赖关系，这引入了显著的开销并限制了训练吞吐量。现有静态GNNs的优化不适用于MTGNNs，因为两者在训练范式、模型架构和缺乏记忆模块上存在差异。此外，它们并未有效地解决时间依赖带来的挑战，使其对MTGNN训练无效。在本文中，我们提出了MSPipe，这是一个通用而高效的MTGNNs框架，可以最大化训练吞吐量同时保持模型准确性。",
    "tldr": "提出了MSPipe，一个通用而高效的MTGNNs框架，实现了最大化训练吞吐量同时保持模型准确性"
}