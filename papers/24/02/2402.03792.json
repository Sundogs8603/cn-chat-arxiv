{
    "title": "No-Regret Reinforcement Learning in Smooth MDPs",
    "abstract": "Obtaining no-regret guarantees for reinforcement learning (RL) in the case of problems with continuous state and/or action spaces is still one of the major open challenges in the field. Recently, a variety of solutions have been proposed, but besides very specific settings, the general problem remains unsolved. In this paper, we introduce a novel structural assumption on the Markov decision processes (MDPs), namely $\\nu-$smoothness, that generalizes most of the settings proposed so far (e.g., linear MDPs and Lipschitz MDPs). To face this challenging scenario, we propose two algorithms for regret minimization in $\\nu-$smooth MDPs. Both algorithms build upon the idea of constructing an MDP representation through an orthogonal feature map based on Legendre polynomials. The first algorithm, \\textsc{Legendre-Eleanor}, archives the no-regret property under weaker assumptions but is computationally inefficient, whereas the second one, \\textsc{Legendre-LSVI}, runs in polynomial time, although ",
    "link": "https://arxiv.org/abs/2402.03792",
    "context": "Title: No-Regret Reinforcement Learning in Smooth MDPs\nAbstract: Obtaining no-regret guarantees for reinforcement learning (RL) in the case of problems with continuous state and/or action spaces is still one of the major open challenges in the field. Recently, a variety of solutions have been proposed, but besides very specific settings, the general problem remains unsolved. In this paper, we introduce a novel structural assumption on the Markov decision processes (MDPs), namely $\\nu-$smoothness, that generalizes most of the settings proposed so far (e.g., linear MDPs and Lipschitz MDPs). To face this challenging scenario, we propose two algorithms for regret minimization in $\\nu-$smooth MDPs. Both algorithms build upon the idea of constructing an MDP representation through an orthogonal feature map based on Legendre polynomials. The first algorithm, \\textsc{Legendre-Eleanor}, archives the no-regret property under weaker assumptions but is computationally inefficient, whereas the second one, \\textsc{Legendre-LSVI}, runs in polynomial time, although ",
    "path": "papers/24/02/2402.03792.json",
    "total_tokens": 971,
    "translated_title": "连续状态和/或动作空间中的无悔强化学习在平滑MDPs中的应用",
    "translated_abstract": "对于具有连续状态和/或动作空间的问题的强化学习（RL），如何获得无悔保证仍然是该领域的主要挑战之一。最近，提出了多种解决方案，但除非是非常特定的情景，否则这个普遍问题仍未解决。本文引入了一种新的对马尔可夫决策过程（MDPs）进行结构假设的方法，即$\\nu-$平滑性，该方法推广了目前提出的多种设置（如线性MDPs和Lipschitz MDPs）。为了应对这个具有挑战性的场景，我们提出了两种算法来在$\\nu-$平滑 MDPs中进行悔恨最小化。这两种算法都基于通过基于Legendre多项式的正交特征映射构建MDP表示的思想。第一个算法\\textsc{Legendre-Eleanor}在较弱的假设下可以达到无悔特性，但计算效率低下，而第二个\\textsc{Legendre-LSVI}算法在多项式时间内运行，",
    "tldr": "本论文针对具有连续状态和/或动作空间的强化学习问题提出了一种无悔保证的新方法，即通过在Legendre多项式基础上构建MDP表示来解决$\\nu-$平滑 MDPs，在较弱的假设下可以达到无悔特性，同时在多项式时间内运行。"
}