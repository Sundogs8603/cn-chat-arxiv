{
    "title": "Bayesian Neural Networks with Domain Knowledge Priors",
    "abstract": "arXiv:2402.13410v1 Announce Type: new  Abstract: Bayesian neural networks (BNNs) have recently gained popularity due to their ability to quantify model uncertainty. However, specifying a prior for BNNs that captures relevant domain knowledge is often extremely challenging. In this work, we propose a framework for integrating general forms of domain knowledge (i.e., any knowledge that can be represented by a loss function) into a BNN prior through variational inference, while enabling computationally efficient posterior inference and sampling. Specifically, our approach results in a prior over neural network weights that assigns high probability mass to models that better align with our domain knowledge, leading to posterior samples that also exhibit this behavior. We show that BNNs using our proposed domain knowledge priors outperform those with standard priors (e.g., isotropic Gaussian, Gaussian process), successfully incorporating diverse types of prior information such as fairness, ",
    "link": "https://arxiv.org/abs/2402.13410",
    "context": "Title: Bayesian Neural Networks with Domain Knowledge Priors\nAbstract: arXiv:2402.13410v1 Announce Type: new  Abstract: Bayesian neural networks (BNNs) have recently gained popularity due to their ability to quantify model uncertainty. However, specifying a prior for BNNs that captures relevant domain knowledge is often extremely challenging. In this work, we propose a framework for integrating general forms of domain knowledge (i.e., any knowledge that can be represented by a loss function) into a BNN prior through variational inference, while enabling computationally efficient posterior inference and sampling. Specifically, our approach results in a prior over neural network weights that assigns high probability mass to models that better align with our domain knowledge, leading to posterior samples that also exhibit this behavior. We show that BNNs using our proposed domain knowledge priors outperform those with standard priors (e.g., isotropic Gaussian, Gaussian process), successfully incorporating diverse types of prior information such as fairness, ",
    "path": "papers/24/02/2402.13410.json",
    "total_tokens": 887,
    "translated_title": "具有领域知识先验的贝叶斯神经网络",
    "translated_abstract": "最近，由于其能够量化模型不确定性的能力，贝叶斯神经网络（BNNs）变得越来越受欢迎。然而，为BNNs指定能够捕捉相关领域知识的先验往往极具挑战性。在这项工作中，我们提出了一个框架，通过变分推断将各种形式的领域知识（即可以用损失函数表示的任何知识）整合到BNN先验中，同时实现高效的后验推断和抽样。具体来说，我们的方法导致对神经网络权重的先验分配高概率质量给更符合我们领域知识的模型，从而导致后验样本也表现出这种行为。我们展示了，使用我们提出的领域知识先验的BNNs优于具有标准先验（例如各向同性高斯、高斯过程）的模型，在成功整合多种类型的先验信息（例如公平性）方面表现出色。",
    "tldr": "提出了一个框架，通过变分推断将各种形式的领域知识整合到贝叶斯神经网络（BNNs）先验中，以实现更好符合领域知识的模型，从而获得更具表现力的后验样本。",
    "en_tdlr": "A framework is proposed to integrate various forms of domain knowledge into Bayesian Neural Networks (BNNs) priors through variational inference, aiming to achieve models that better conform to domain knowledge and result in more expressive posterior samples."
}