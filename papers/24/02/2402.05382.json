{
    "title": "Task-customized Masked AutoEncoder via Mixture of Cluster-conditional Experts",
    "abstract": "Masked Autoencoder~(MAE) is a prevailing self-supervised learning method that achieves promising results in model pre-training. However, when the various downstream tasks have data distributions different from the pre-training data, the semantically irrelevant pre-training information might result in negative transfer, impeding MAE's scalability. To address this issue, we propose a novel MAE-based pre-training paradigm, Mixture of Cluster-conditional Experts (MoCE), which can be trained once but provides customized pre-training models for diverse downstream tasks. Different from the mixture of experts (MoE), our MoCE trains each expert only with semantically relevant images by using cluster-conditional gates. Thus, each downstream task can be allocated to its customized model pre-trained with data most similar to the downstream data. Experiments on a collection of 11 downstream tasks show that MoCE outperforms the vanilla MAE by 2.45\\% on average. It also obtains new state-of-the-art s",
    "link": "https://arxiv.org/abs/2402.05382",
    "context": "Title: Task-customized Masked AutoEncoder via Mixture of Cluster-conditional Experts\nAbstract: Masked Autoencoder~(MAE) is a prevailing self-supervised learning method that achieves promising results in model pre-training. However, when the various downstream tasks have data distributions different from the pre-training data, the semantically irrelevant pre-training information might result in negative transfer, impeding MAE's scalability. To address this issue, we propose a novel MAE-based pre-training paradigm, Mixture of Cluster-conditional Experts (MoCE), which can be trained once but provides customized pre-training models for diverse downstream tasks. Different from the mixture of experts (MoE), our MoCE trains each expert only with semantically relevant images by using cluster-conditional gates. Thus, each downstream task can be allocated to its customized model pre-trained with data most similar to the downstream data. Experiments on a collection of 11 downstream tasks show that MoCE outperforms the vanilla MAE by 2.45\\% on average. It also obtains new state-of-the-art s",
    "path": "papers/24/02/2402.05382.json",
    "total_tokens": 929,
    "translated_title": "基于混合聚类条件专家的任务定制化遮蔽自编码器",
    "translated_abstract": "遮蔽自编码器(MAE)是一种流行的自监督学习方法，可以在模型预训练中取得有希望的结果。然而，当各种下游任务的数据分布与预训练数据不同时，语义不相关的预训练信息可能导致负迁移，制约了MAE的可扩展性。为了解决这个问题，我们提出了一种新颖的基于MAE的预训练范式，即混合聚类条件专家(MoCE)，它可以仅训练一次，但为不同的下游任务提供定制的预训练模型。与专家混合模型(MoE)不同，我们的MoCE使用聚类条件门只训练每个专家以语义相关的图像。因此，每个下游任务可以分配到其与下游数据最相似的数据预训练模型。在11个下游任务的实验中，MoCE的平均性能比普通MAE提高了2.45%。它还达到了新的最先进水平。",
    "tldr": "提出了一种基于混合聚类条件专家的任务定制化遮蔽自编码器(MoCE)方法，通过使用聚类条件门将每个专家只训练与语义相关的图像，为不同的下游任务提供定制的预训练模型，取得了比传统MAE更好的性能。"
}