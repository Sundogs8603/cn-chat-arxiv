{
    "title": "Let LLMs Take on the Latest Challenges! A Chinese Dynamic Question Answering Benchmark",
    "abstract": "arXiv:2402.19248v1 Announce Type: new  Abstract: How to better evaluate the capabilities of Large Language Models (LLMs) is the focal point and hot topic in current LLMs research. Previous work has noted that due to the extremely high cost of iterative updates of LLMs, they are often unable to answer the latest dynamic questions well. To promote the improvement of Chinese LLMs' ability to answer dynamic questions, in this paper, we introduce CDQA, a Chinese Dynamic QA benchmark containing question-answer pairs related to the latest news on the Chinese Internet. We obtain high-quality data through a pipeline that combines humans and models, and carefully classify the samples according to the frequency of answer changes to facilitate a more fine-grained observation of LLMs' capabilities. We have also evaluated and analyzed mainstream and advanced Chinese LLMs on CDQA. Extensive experiments and valuable insights suggest that our proposed CDQA is challenging and worthy of more further stud",
    "link": "https://arxiv.org/abs/2402.19248",
    "context": "Title: Let LLMs Take on the Latest Challenges! A Chinese Dynamic Question Answering Benchmark\nAbstract: arXiv:2402.19248v1 Announce Type: new  Abstract: How to better evaluate the capabilities of Large Language Models (LLMs) is the focal point and hot topic in current LLMs research. Previous work has noted that due to the extremely high cost of iterative updates of LLMs, they are often unable to answer the latest dynamic questions well. To promote the improvement of Chinese LLMs' ability to answer dynamic questions, in this paper, we introduce CDQA, a Chinese Dynamic QA benchmark containing question-answer pairs related to the latest news on the Chinese Internet. We obtain high-quality data through a pipeline that combines humans and models, and carefully classify the samples according to the frequency of answer changes to facilitate a more fine-grained observation of LLMs' capabilities. We have also evaluated and analyzed mainstream and advanced Chinese LLMs on CDQA. Extensive experiments and valuable insights suggest that our proposed CDQA is challenging and worthy of more further stud",
    "path": "papers/24/02/2402.19248.json",
    "total_tokens": 965,
    "translated_title": "让大型语言模型应对最新挑战！一个中文动态问答基准测试",
    "translated_abstract": "arXiv:2402.19248v1 公告类型：新  摘要：如何更好地评估大型语言模型（LLMs）的能力是当前LLMs研究的焦点和热点。先前的研究指出，由于大规模迭代更新LLMs的成本极高，它们经常无法很好地回答最新的动态问题。为了促进中文LLMs回答动态问题的能力提升，在本文中，我们引入了 CDQA，一个包含与中国互联网上最新新闻相关的问答对的中文动态问答基准测试。我们通过将人类和模型结合的流程获得高质量数据，并根据答案变化频率精细分类样本，以便更细致地观察LLMs的能力。我们还在CDQA上评估和分析了主流和先进的中文LLMs。广泛的实验和宝贵的见解表明，我们提出的CDQA是具有挑战性且值得进一步研究的。",
    "tldr": "本论文提出了CDQA，一个中文动态问答基准测试，致力于提高中文大型语言模型（LLMs）回答动态问题的能力，并通过高质量数据和精细样本分类实现了对LLMs能力更细致的观察。实验结果表明，CDQA具有挑战性且值得进一步研究。",
    "en_tdlr": "This paper introduces CDQA, a Chinese Dynamic QA benchmark aiming to improve the capabilities of Chinese Large Language Models (LLMs) in answering dynamic questions, enabling a more fine-grained observation of LLMs' abilities through high-quality data and careful sample classification. Experimental results indicate that CDQA is challenging and worthy of further investigation."
}