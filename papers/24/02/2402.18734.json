{
    "title": "Priority Sampling of Large Language Models for Compilers",
    "abstract": "arXiv:2402.18734v1 Announce Type: cross  Abstract: Large language models show great potential in generating and optimizing code. Widely used sampling methods such as Nucleus Sampling increase the diversity of generation but often produce repeated samples for low temperatures and incoherent samples for high temperatures. Furthermore, the temperature coefficient has to be tuned for each task, limiting its usability. We present Priority Sampling, a simple and deterministic sampling technique that produces unique samples ordered by the model's confidence. Each new sample expands the unexpanded token with the highest probability in the augmented search tree. Additionally, Priority Sampling supports generation based on regular expression that provides a controllable and structured exploration process. Priority Sampling outperforms Nucleus Sampling for any number of samples, boosting the performance of the original model from 2.87% to 5% improvement over -Oz. Moreover, it outperforms the auto",
    "link": "https://arxiv.org/abs/2402.18734",
    "context": "Title: Priority Sampling of Large Language Models for Compilers\nAbstract: arXiv:2402.18734v1 Announce Type: cross  Abstract: Large language models show great potential in generating and optimizing code. Widely used sampling methods such as Nucleus Sampling increase the diversity of generation but often produce repeated samples for low temperatures and incoherent samples for high temperatures. Furthermore, the temperature coefficient has to be tuned for each task, limiting its usability. We present Priority Sampling, a simple and deterministic sampling technique that produces unique samples ordered by the model's confidence. Each new sample expands the unexpanded token with the highest probability in the augmented search tree. Additionally, Priority Sampling supports generation based on regular expression that provides a controllable and structured exploration process. Priority Sampling outperforms Nucleus Sampling for any number of samples, boosting the performance of the original model from 2.87% to 5% improvement over -Oz. Moreover, it outperforms the auto",
    "path": "papers/24/02/2402.18734.json",
    "total_tokens": 818,
    "translated_title": "编译器大型语言模型的优先采样",
    "translated_abstract": "大型语言模型在生成和优化代码方面表现出巨大潜力。广泛使用的采样方法，比如核采样（Nucleus Sampling），增加了生成的多样性，但在低温度下经常产生重复的样本，在高温度下产生不连贯的样本。此外，温度系数必须针对每个任务进行调整，限制了其可用性。我们提出了优先采样（Priority Sampling），一种简单且确定性的采样技术，它产生按模型置信度排序的唯一样本。每个新样本都会扩展扩展搜索树中概率最高的未扩展令牌。此外，优先采样支持基于正则表达式的生成，提供可控和结构化的探索过程。优先采样在任意数量的样本情况下均优于核采样，将原始模型的性能从2.87%提升至5%超过-Oz。此外，它超过了自",
    "tldr": "提出了一种优先采样技术，能够按照模型信心度产生唯一样本，在生成和优化代码时表现优于核采样方法。",
    "en_tdlr": "Introduced Priority Sampling, a sampling technique that produces unique samples ordered by model confidence, outperforming Nucleus Sampling in code generation and optimization."
}