{
    "title": "GenEFT: Understanding Statics and Dynamics of Model Generalization via Effective Theory",
    "abstract": "We present GenEFT: an effective theory framework for shedding light on the statics and dynamics of neural network generalization, and illustrate it with graph learning examples. We first investigate the generalization phase transition as data size increases, comparing experimental results with information-theory-based approximations. We find generalization in a Goldilocks zone where the decoder is neither too weak nor too powerful. We then introduce an effective theory for the dynamics of representation learning, where latent-space representations are modeled as interacting particles (repons), and find that it explains our experimentally observed phase transition between generalization and overfitting as encoder and decoder learning rates are scanned. This highlights the power of physics-inspired effective theories for bridging the gap between theoretical predictions and practice in machine learning.",
    "link": "https://arxiv.org/abs/2402.05916",
    "context": "Title: GenEFT: Understanding Statics and Dynamics of Model Generalization via Effective Theory\nAbstract: We present GenEFT: an effective theory framework for shedding light on the statics and dynamics of neural network generalization, and illustrate it with graph learning examples. We first investigate the generalization phase transition as data size increases, comparing experimental results with information-theory-based approximations. We find generalization in a Goldilocks zone where the decoder is neither too weak nor too powerful. We then introduce an effective theory for the dynamics of representation learning, where latent-space representations are modeled as interacting particles (repons), and find that it explains our experimentally observed phase transition between generalization and overfitting as encoder and decoder learning rates are scanned. This highlights the power of physics-inspired effective theories for bridging the gap between theoretical predictions and practice in machine learning.",
    "path": "papers/24/02/2402.05916.json",
    "total_tokens": 827,
    "translated_title": "GenEFT: 通过有效理论理解模型泛化的静态和动态",
    "translated_abstract": "我们提出了GenEFT：一个有效的理论框架，用于揭示神经网络泛化的静态和动态，以图学习为例进行了说明。首先，我们研究了数据规模增加时的泛化相变，将实验结果与基于信息理论的近似进行比较。我们发现，在解码器既不太弱也不太强的“小熊宝贝区域”中存在着泛化。然后，我们介绍了一种表示学习动态的有效理论，将潜在空间表示建模为相互作用粒子（repons），发现它解释了我们在编码器和解码器学习速率扫描时观察到的泛化和过拟合之间的相变。这突出了受物理启发的有效理论在弥合机器学习中理论预测与实践之间的差距方面的力量。",
    "tldr": "GenEFT是一个有效的理论框架，通过研究泛化相变和表示学习动态，揭示了神经网络泛化的静态和动态特性，这弥合了机器学习理论预测与实践之间的差距。",
    "en_tdlr": "GenEFT is an effective theoretical framework that sheds light on the statics and dynamics of neural network generalization by studying phase transitions in generalization and dynamics of representation learning. It bridges the gap between theoretical predictions and practice in machine learning."
}