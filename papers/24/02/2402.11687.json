{
    "title": "Evaluating Efficacy of Model Stealing Attacks and Defenses on Quantum Neural Networks",
    "abstract": "arXiv:2402.11687v1 Announce Type: cross  Abstract: Cloud hosting of quantum machine learning (QML) models exposes them to a range of vulnerabilities, the most significant of which is the model stealing attack. In this study, we assess the efficacy of such attacks in the realm of quantum computing. We conducted comprehensive experiments on various datasets with multiple QML model architectures. Our findings revealed that model stealing attacks can produce clone models achieving up to $0.9\\times$ and $0.99\\times$ clone test accuracy when trained using Top-$1$ and Top-$k$ labels, respectively ($k:$ num\\_classes). To defend against these attacks, we leverage the unique properties of current noisy hardware and perturb the victim model outputs and hinder the attacker's training process. In particular, we propose: 1) hardware variation-induced perturbation (HVIP) and 2) hardware and architecture variation-induced perturbation (HAVIP). Although noise and architectural variability can provide u",
    "link": "https://arxiv.org/abs/2402.11687",
    "context": "Title: Evaluating Efficacy of Model Stealing Attacks and Defenses on Quantum Neural Networks\nAbstract: arXiv:2402.11687v1 Announce Type: cross  Abstract: Cloud hosting of quantum machine learning (QML) models exposes them to a range of vulnerabilities, the most significant of which is the model stealing attack. In this study, we assess the efficacy of such attacks in the realm of quantum computing. We conducted comprehensive experiments on various datasets with multiple QML model architectures. Our findings revealed that model stealing attacks can produce clone models achieving up to $0.9\\times$ and $0.99\\times$ clone test accuracy when trained using Top-$1$ and Top-$k$ labels, respectively ($k:$ num\\_classes). To defend against these attacks, we leverage the unique properties of current noisy hardware and perturb the victim model outputs and hinder the attacker's training process. In particular, we propose: 1) hardware variation-induced perturbation (HVIP) and 2) hardware and architecture variation-induced perturbation (HAVIP). Although noise and architectural variability can provide u",
    "path": "papers/24/02/2402.11687.json",
    "total_tokens": 913,
    "translated_title": "评估量子神经网络模型窃取攻击和防御的有效性",
    "translated_abstract": "arXiv:2402.11687v1 通报类型：跨领域 摘要：通过云托管量子机器学习（QML）模型使其面临一系列漏洞，其中最重要的是模型窃取攻击。在本研究中，我们评估了在量子计算领域中此类攻击的有效性。我们对多个数据集以及多种QML模型架构进行了全面的实验。我们的研究发现，当使用Top-$1$和Top-$k$标签分别进行训练时，模型窃取攻击可以产生达到$0.9\\times$和$0.99\\times$克隆测试精度的克隆模型（$k:$ num\\_classes）。为了防御这些攻击，我们利用当前嘈杂硬件的独特特性扰乱受害模型输出并阻碍攻击者的训练过程。具体而言，我们提出了：1）硬件变化诱发扰动（HVIP）和 2）硬件和架构变化诱发扰动（HAVIP）",
    "tldr": "本研究评估了量子神经网络模型窃取攻击的有效性，并提出了硬件变化诱发扰动和硬件与架构变化诱发扰动作为防御方法。",
    "en_tdlr": "This study evaluates the efficacy of model stealing attacks on quantum neural networks and proposes hardware variation-induced perturbation (HVIP) and hardware and architecture variation-induced perturbation (HAVIP) as defense mechanisms."
}