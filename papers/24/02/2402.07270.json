{
    "title": "Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy",
    "abstract": "The evaluation of text-generative vision-language models is a challenging yet crucial endeavor. By addressing the limitations of existing Visual Question Answering (VQA) benchmarks and proposing innovative evaluation methodologies, our research seeks to advance our understanding of these models' capabilities. We propose a novel VQA benchmark based on well-known visual classification datasets which allows a granular evaluation of text-generative vision-language models and their comparison with discriminative vision-language models. To improve the assessment of coarse answers on fine-grained classification tasks, we suggest using the semantic hierarchy of the label space to ask automatically generated follow-up questions about the ground-truth category. Finally, we compare traditional NLP and LLM-based metrics for the problem of evaluating model predictions given ground-truth answers. We perform a human evaluation study upon which we base our decision on the final metric. We apply our be",
    "link": "https://arxiv.org/abs/2402.07270",
    "context": "Title: Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy\nAbstract: The evaluation of text-generative vision-language models is a challenging yet crucial endeavor. By addressing the limitations of existing Visual Question Answering (VQA) benchmarks and proposing innovative evaluation methodologies, our research seeks to advance our understanding of these models' capabilities. We propose a novel VQA benchmark based on well-known visual classification datasets which allows a granular evaluation of text-generative vision-language models and their comparison with discriminative vision-language models. To improve the assessment of coarse answers on fine-grained classification tasks, we suggest using the semantic hierarchy of the label space to ask automatically generated follow-up questions about the ground-truth category. Finally, we compare traditional NLP and LLM-based metrics for the problem of evaluating model predictions given ground-truth answers. We perform a human evaluation study upon which we base our decision on the final metric. We apply our be",
    "path": "papers/24/02/2402.07270.json",
    "total_tokens": 961,
    "translated_title": "通过利用分类数据集和其语义层次，开展视觉语言模型的开放式VQA评估",
    "translated_abstract": "评估文本生成的视觉语言模型是一项具有挑战性但至关重要的工作。通过解决现有视觉问答（VQA）基准的局限性并提出创新的评估方法，我们的研究旨在推动我们对这些模型能力的理解。我们提出了一种基于知名视觉分类数据集的新型VQA基准，可以对文本生成的视觉语言模型进行细粒度评估，并与判别性视觉语言模型进行比较。为了改善对细粒度分类任务上粗糙答案的评估，我们建议使用标签空间的语义层次来提出关于基准类别的自动生成的后续问题。最后，我们比较了传统的自然语言处理和基于LLM的度量标准来评估给定基准答案的模型预测问题。我们进行了人工评估研究，基于此决定最终度量标准的选择。",
    "tldr": "该研究通过提出创新的评估方法和基于分类数据集的新型VQA基准，推动了对文本生成的视觉语言模型能力的理解。同时，他们还提出了使用语义层次和自动生成的后续问题来改进对细粒度分类任务上粗糙答案的评估。通过比较不同度量标准，他们在进行人工评估研究的基础上选择了最终的度量标准。",
    "en_tdlr": "This research advances our understanding of text-generative vision-language models by proposing innovative evaluation methodologies and a new VQA benchmark based on classification datasets. They also suggest using the semantic hierarchy and automatically generated follow-up questions to improve evaluation on fine-grained classification tasks. The final metric is selected based on a human evaluation study comparing different metrics."
}