{
    "title": "How Can LLM Guide RL? A Value-Based Approach",
    "abstract": "arXiv:2402.16181v1 Announce Type: cross  Abstract: Reinforcement learning (RL) has become the de facto standard practice for sequential decision-making problems by improving future acting policies with feedback. However, RL algorithms may require extensive trial-and-error interactions to collect useful feedback for improvement. On the other hand, recent developments in large language models (LLMs) have showcased impressive capabilities in language understanding and generation, yet they fall short in exploration and self-improvement capabilities for planning tasks, lacking the ability to autonomously refine their responses based on feedback. Therefore, in this paper, we study how the policy prior provided by the LLM can enhance the sample efficiency of RL algorithms. Specifically, we develop an algorithm named LINVIT that incorporates LLM guidance as a regularization factor in value-based RL, leading to significant reductions in the amount of data needed for learning, particularly when ",
    "link": "https://arxiv.org/abs/2402.16181",
    "context": "Title: How Can LLM Guide RL? A Value-Based Approach\nAbstract: arXiv:2402.16181v1 Announce Type: cross  Abstract: Reinforcement learning (RL) has become the de facto standard practice for sequential decision-making problems by improving future acting policies with feedback. However, RL algorithms may require extensive trial-and-error interactions to collect useful feedback for improvement. On the other hand, recent developments in large language models (LLMs) have showcased impressive capabilities in language understanding and generation, yet they fall short in exploration and self-improvement capabilities for planning tasks, lacking the ability to autonomously refine their responses based on feedback. Therefore, in this paper, we study how the policy prior provided by the LLM can enhance the sample efficiency of RL algorithms. Specifically, we develop an algorithm named LINVIT that incorporates LLM guidance as a regularization factor in value-based RL, leading to significant reductions in the amount of data needed for learning, particularly when ",
    "path": "papers/24/02/2402.16181.json",
    "total_tokens": 804,
    "translated_title": "LLM如何指导强化学习？一种基于价值的方法",
    "translated_abstract": "强化学习（RL）已经成为通过改进未来的行动策略来解决序贯决策问题的事实标准实践，但是RL算法可能需要大量的试错交互来收集有用的反馈以进行改进。与此同时，最近大型语言模型（LLMs）的发展展示了在语言理解和生成方面令人印象深刻的能力，然而它们在探索和自我改进规划任务的能力上仍存在不足，缺乏基于反馈自主改进响应的能力。因此，在本文中，我们研究了LLM提供的策略先验如何增强RL算法的样本效率。具体而言，我们开发了一种名为LINVIT的算法，该算法将LLM引导作为价值型RL中的正则化因子，可以显著减少学习所需的数据量，特别是当……",
    "tldr": "本文研究了如何利用大型语言模型（LLM）提供的策略先验来增强强化学习（RL）算法的样本效率。",
    "en_tdlr": "This paper investigates how the policy prior provided by large language models (LLMs) can enhance the sample efficiency of reinforcement learning (RL) algorithms."
}