{
    "title": "Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent",
    "abstract": "arXiv:2402.13717v1 Announce Type: new  Abstract: Large Language Models (LLMs) have revolutionized open-domain dialogue agents but encounter challenges in multi-character role-playing (MCRP) scenarios. To address the issue, we present Neeko, an innovative framework designed for efficient multiple characters imitation. Unlike existing methods, Neeko employs a dynamic low-rank adapter (LoRA) strategy, enabling it to adapt seamlessly to diverse characters. Our framework breaks down the role-playing process into agent pre-training, multiple characters playing, and character incremental learning, effectively handling both seen and unseen roles. This dynamic approach, coupled with distinct LoRA blocks for each character, enhances Neeko's adaptability to unique attributes, personalities, and speaking patterns. As a result, Neeko demonstrates superior performance in MCRP over most existing methods, offering more engaging and versatile user interaction experiences. Code and data are available at",
    "link": "https://arxiv.org/abs/2402.13717",
    "context": "Title: Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent\nAbstract: arXiv:2402.13717v1 Announce Type: new  Abstract: Large Language Models (LLMs) have revolutionized open-domain dialogue agents but encounter challenges in multi-character role-playing (MCRP) scenarios. To address the issue, we present Neeko, an innovative framework designed for efficient multiple characters imitation. Unlike existing methods, Neeko employs a dynamic low-rank adapter (LoRA) strategy, enabling it to adapt seamlessly to diverse characters. Our framework breaks down the role-playing process into agent pre-training, multiple characters playing, and character incremental learning, effectively handling both seen and unseen roles. This dynamic approach, coupled with distinct LoRA blocks for each character, enhances Neeko's adaptability to unique attributes, personalities, and speaking patterns. As a result, Neeko demonstrates superior performance in MCRP over most existing methods, offering more engaging and versatile user interaction experiences. Code and data are available at",
    "path": "papers/24/02/2402.13717.json",
    "total_tokens": 875,
    "translated_title": "Neeko：利用动态LoRA实现高效多角色扮演代理",
    "translated_abstract": "大型语言模型（LLMs）在开放领域对话代理程序中起着革命性作用，但在多角色扮演（MCRP）场景中遇到挑战。为了解决这个问题，我们提出了Neeko，这是一个专为高效模仿多个角色而设计的创新框架。与现有方法不同，Neeko采用动态低秩适配器（LoRA）策略，使其能够无缝适应不同的角色。我们的框架将角色扮演过程分解为代理预训练、多个角色扮演和角色增量学习，有效处理已知和未知角色。这种动态方法，结合为每个角色设计的独特LoRA块，增强了Neeko对独特属性、个性和说话模式的适应能力。因此，Neeko在MCRP方面表现出比大多数现有方法更出色的性能，为用户提供更具吸引力和多样化的互动体验。代码和数据可在（链接中提供）。",
    "tldr": "Neeko利用动态低秩适配器（LoRA）策略，有效处理多角色扮演过程中的挑战，提升了对不同属性、个性和说话模式的适应能力。"
}