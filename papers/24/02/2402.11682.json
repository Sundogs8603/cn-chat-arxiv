{
    "title": "Learning Conditional Invariances through Non-Commutativity",
    "abstract": "arXiv:2402.11682v1 Announce Type: new  Abstract: Invariance learning algorithms that conditionally filter out domain-specific random variables as distractors, do so based only on the data semantics, and not the target domain under evaluation. We show that a provably optimal and sample-efficient way of learning conditional invariances is by relaxing the invariance criterion to be non-commutatively directed towards the target domain. Under domain asymmetry, i.e., when the target domain contains semantically relevant information absent in the source, the risk of the encoder $\\varphi^*$ that is optimal on average across domains is strictly lower-bounded by the risk of the target-specific optimal encoder $\\Phi^*_\\tau$. We prove that non-commutativity steers the optimization towards $\\Phi^*_\\tau$ instead of $\\varphi^*$, bringing the $\\mathcal{H}$-divergence between domains down to zero, leading to a stricter bound on the target risk. Both our theory and experiments demonstrate that non-commu",
    "link": "https://arxiv.org/abs/2402.11682",
    "context": "Title: Learning Conditional Invariances through Non-Commutativity\nAbstract: arXiv:2402.11682v1 Announce Type: new  Abstract: Invariance learning algorithms that conditionally filter out domain-specific random variables as distractors, do so based only on the data semantics, and not the target domain under evaluation. We show that a provably optimal and sample-efficient way of learning conditional invariances is by relaxing the invariance criterion to be non-commutatively directed towards the target domain. Under domain asymmetry, i.e., when the target domain contains semantically relevant information absent in the source, the risk of the encoder $\\varphi^*$ that is optimal on average across domains is strictly lower-bounded by the risk of the target-specific optimal encoder $\\Phi^*_\\tau$. We prove that non-commutativity steers the optimization towards $\\Phi^*_\\tau$ instead of $\\varphi^*$, bringing the $\\mathcal{H}$-divergence between domains down to zero, leading to a stricter bound on the target risk. Both our theory and experiments demonstrate that non-commu",
    "path": "papers/24/02/2402.11682.json",
    "total_tokens": 874,
    "translated_title": "通过非交换性学习条件不变性",
    "translated_abstract": "条件过滤器学习算法可以根据数据语义而非目标域在评估中仅基于的方法来过滤掉特定于域的随机变量作为干扰因素。我们展示了学习条件不变性的一种经过证明的最优和样本高效的方法是将不变性准则放宽为非交换性，指向目标域。在存在领域不对称性的情况下，即当目标领域包含源领域中不包含的语义相关信息时，跨领域平均最优编码器$\\varphi^*$的风险严格地被目标特定最优编码器$\\Phi^*_\\tau$的风险下界限制。我们证明非交换性将优化导向$\\Phi^*_\\tau$而非$\\varphi^*$，将领域之间的$\\mathcal{H}$-散度降至零，从而导致对目标风险的更严格限制。我们的理论和实验均表明非交换性可以改进风险边界。",
    "tldr": "学习非交换性条件不变性是一种更严格的优化方法，可以通过将优化导向目标特定编码器来实现对目标风险的更严格限制。",
    "en_tdlr": "Learning conditional invariances through non-commutativity provides a stricter optimization approach by directing the optimization towards the target-specific encoder, leading to a more stringent bound on the target risk."
}