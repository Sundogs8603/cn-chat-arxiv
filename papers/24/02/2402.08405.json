{
    "title": "A Novel Approach to Regularising 1NN classifier for Improved Generalization",
    "abstract": "In this paper, we propose a class of non-parametric classifiers, that learn arbitrary boundaries and generalize well.   Our approach is based on a novel way to regularize 1NN classifiers using a greedy approach. We refer to this class of classifiers as Watershed Classifiers. 1NN classifiers are known to trivially over-fit but have very large VC dimension, hence do not generalize well. We show that watershed classifiers can find arbitrary boundaries on any dense enough dataset, and, at the same time, have very small VC dimension; hence a watershed classifier leads to good generalization.   Traditional approaches to regularize 1NN classifiers are to consider $K$ nearest neighbours. Neighbourhood component analysis (NCA) proposes a way to learn representations consistent with ($n-1$) nearest neighbour classifier, where $n$ denotes the size of the dataset. In this article, we propose a loss function which can learn representations consistent with watershed classifiers, and show that it out",
    "link": "https://arxiv.org/abs/2402.08405",
    "context": "Title: A Novel Approach to Regularising 1NN classifier for Improved Generalization\nAbstract: In this paper, we propose a class of non-parametric classifiers, that learn arbitrary boundaries and generalize well.   Our approach is based on a novel way to regularize 1NN classifiers using a greedy approach. We refer to this class of classifiers as Watershed Classifiers. 1NN classifiers are known to trivially over-fit but have very large VC dimension, hence do not generalize well. We show that watershed classifiers can find arbitrary boundaries on any dense enough dataset, and, at the same time, have very small VC dimension; hence a watershed classifier leads to good generalization.   Traditional approaches to regularize 1NN classifiers are to consider $K$ nearest neighbours. Neighbourhood component analysis (NCA) proposes a way to learn representations consistent with ($n-1$) nearest neighbour classifier, where $n$ denotes the size of the dataset. In this article, we propose a loss function which can learn representations consistent with watershed classifiers, and show that it out",
    "path": "papers/24/02/2402.08405.json",
    "total_tokens": 907,
    "translated_title": "一种改进通用化能力的1NN分类器正则化方法的创新途径",
    "translated_abstract": "在本文中，我们提出了一种非参数分类器，可以学习任意边界并具有良好的泛化能力。我们的方法基于一种新颖的贪心方法，对1NN分类器进行正则化。我们将这类分类器称为分水岭分类器。已知1NN分类器容易过拟合，但具有很大的VC维度，因此不能很好地泛化。我们展示了分水岭分类器可以在任意足够密集的数据集上找到任意边界，并且具有很小的VC维度；因此，分水岭分类器可以实现良好的泛化。传统的1NN分类器正则化方法是考虑K个最近邻。邻域组件分析（NCA）提出了一种学习与（n-1）最近邻分类器一致的表示的方法，其中n表示数据集的大小。在本文中，我们提出了一种可以学习与分水岭分类器一致的表示的损失函数，并且展示了其结果。",
    "tldr": "本文提出了一种改进通用化能力的非参数分类器，即分水岭分类器，通过对1NN分类器进行正则化。分水岭分类器可以在任意密集的数据集上找到任意边界，并具有很小的VC维度，从而能够实现良好的泛化。",
    "en_tdlr": "This paper introduces a novel approach to regularizing 1NN classifiers, using a greedy approach and proposing a class of non-parametric classifiers called Watershed Classifiers. The watershed classifiers can find arbitrary boundaries on dense datasets and have small VC dimension, leading to improved generalization."
}