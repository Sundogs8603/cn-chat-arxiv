{
    "title": "InstructGraph: Boosting Large Language Models via Graph-centric Instruction Tuning and Preference Alignment",
    "abstract": "arXiv:2402.08785v1 Announce Type: new Abstract: Do current large language models (LLMs) better solve graph reasoning and generation tasks with parameter updates? In this paper, we propose InstructGraph, a framework that empowers LLMs with the abilities of graph reasoning and generation by instruction tuning and preference alignment. Specifically, we first propose a structured format verbalizer to unify all graph data into a universal code-like format, which can simply represent the graph without any external graph-specific encoders. Furthermore, a graph instruction tuning stage is introduced to guide LLMs in solving graph reasoning and generation tasks. Finally, we identify potential hallucination problems in graph tasks and sample negative instances for preference alignment, the target of which is to enhance the output's reliability of the model. Extensive experiments across multiple graph-centric tasks exhibit that InstructGraph can achieve the best performance and outperform GPT-4 a",
    "link": "https://arxiv.org/abs/2402.08785",
    "context": "Title: InstructGraph: Boosting Large Language Models via Graph-centric Instruction Tuning and Preference Alignment\nAbstract: arXiv:2402.08785v1 Announce Type: new Abstract: Do current large language models (LLMs) better solve graph reasoning and generation tasks with parameter updates? In this paper, we propose InstructGraph, a framework that empowers LLMs with the abilities of graph reasoning and generation by instruction tuning and preference alignment. Specifically, we first propose a structured format verbalizer to unify all graph data into a universal code-like format, which can simply represent the graph without any external graph-specific encoders. Furthermore, a graph instruction tuning stage is introduced to guide LLMs in solving graph reasoning and generation tasks. Finally, we identify potential hallucination problems in graph tasks and sample negative instances for preference alignment, the target of which is to enhance the output's reliability of the model. Extensive experiments across multiple graph-centric tasks exhibit that InstructGraph can achieve the best performance and outperform GPT-4 a",
    "path": "papers/24/02/2402.08785.json",
    "total_tokens": 857,
    "translated_title": "InstructGraph: 通过图中心的指导调整和偏好对齐提升大型语言模型",
    "translated_abstract": "目前的大型语言模型（LLM）是否通过参数更新更好地解决图推理和生成任务？本文提出了InstructGraph，这是一个通过指令调整和偏好对齐赋予LLM图推理和生成能力的框架。具体而言，我们首先提出了一个结构化格式的语言描述器，将所有的图数据统一到一种通用的类似代码的格式中，这样可以简洁地表示图而不需要外部的图特定编码器。此外，我们引入图指令调整阶段，引导LLM解决图推理和生成任务。最后，我们在图任务中识别出潜在的虚构问题，并为偏好对齐样本负实例，其目标是提高模型输出的可靠性。多个图中心任务的大量实验证明，InstructGraph可以达到最佳的性能，并且超过GPT-4。",
    "tldr": "InstructGraph是一个通过指令调整和偏好对齐提升大型语言模型图推理和生成能力的框架，通过统一图数据格式、引导LLM解决图任务和提高输出可靠性来实现最佳性能。",
    "en_tdlr": "InstructGraph is a framework that enhances the graph reasoning and generation capabilities of large language models (LLMs) through instruction tuning and preference alignment. It achieves optimal performance by unifying graph data format, guiding LLMs in solving graph tasks, and improving output reliability."
}