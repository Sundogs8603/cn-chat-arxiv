{
    "title": "Unveiling the Magic: Investigating Attention Distillation in Retrieval-augmented Generation",
    "abstract": "arXiv:2402.11794v1 Announce Type: new  Abstract: Retrieval-augmented generation framework can address the limitations of large language models by enabling real-time knowledge updates for more accurate answers. An efficient way in the training phase of retrieval-augmented models is attention distillation, which uses attention scores as a supervision signal instead of manually annotated query-document pairs. Despite its growing popularity, the detailed mechanisms behind the success of attention distillation remain unexplored, particularly the specific patterns it leverages to benefit training. In this paper, we address this gap by conducting a comprehensive review of attention distillation workflow and identifying key factors influencing the learning quality of retrieval-augmented language models. We further propose indicators for optimizing models' training methods and avoiding ineffective training.",
    "link": "https://arxiv.org/abs/2402.11794",
    "context": "Title: Unveiling the Magic: Investigating Attention Distillation in Retrieval-augmented Generation\nAbstract: arXiv:2402.11794v1 Announce Type: new  Abstract: Retrieval-augmented generation framework can address the limitations of large language models by enabling real-time knowledge updates for more accurate answers. An efficient way in the training phase of retrieval-augmented models is attention distillation, which uses attention scores as a supervision signal instead of manually annotated query-document pairs. Despite its growing popularity, the detailed mechanisms behind the success of attention distillation remain unexplored, particularly the specific patterns it leverages to benefit training. In this paper, we address this gap by conducting a comprehensive review of attention distillation workflow and identifying key factors influencing the learning quality of retrieval-augmented language models. We further propose indicators for optimizing models' training methods and avoiding ineffective training.",
    "path": "papers/24/02/2402.11794.json",
    "total_tokens": 775,
    "translated_title": "揭示魔法：探究检索增强生成中的注意力精炼",
    "translated_abstract": "检索增强生成框架可以通过实时知识更新来解决大型语言模型的局限，以实现更准确的答案。在检索增强模型的训练阶段中，一种高效的方法是注意力精炼，它使用注意力分数作为监督信号，而不是手动注释的查询文档对。尽管注意力精炼越来越受欢迎，但在它成功背后的详细机制仍未被探索，特别是它利用以受益于训练的具体模式。在本文中，我们通过对注意力精炼工作流程的全面回顾，识别影响检索增强语言模型学习质量的关键因素来填补这一空白。我们进一步提出了优化模型训练方法和避免低效训练的指标。",
    "tldr": "本文揭示了检索增强生成中的注意力精炼的成功机制，并提出了优化模型训练方法的指标."
}