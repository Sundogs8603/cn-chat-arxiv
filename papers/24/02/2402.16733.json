{
    "title": "DREsS: Dataset for Rubric-based Essay Scoring on EFL Writing",
    "abstract": "arXiv:2402.16733v1 Announce Type: new  Abstract: Automated essay scoring (AES) is a useful tool in English as a Foreign Language (EFL) writing education, offering real-time essay scores for students and instructors. However, previous AES models were trained on essays and scores irrelevant to the practical scenarios of EFL writing education and usually provided a single holistic score due to the lack of appropriate datasets. In this paper, we release DREsS, a large-scale, standard dataset for rubric-based automated essay scoring. DREsS comprises three sub-datasets: DREsS_New, DREsS_Std., and DREsS_CASE. We collect DREsS_New, a real-classroom dataset with 1.7K essays authored by EFL undergraduate students and scored by English education experts. We also standardize existing rubric-based essay scoring datasets as DREsS_Std. We suggest CASE, a corruption-based augmentation strategy for essays, which generates 20K synthetic samples of DREsS_CASE and improves the baseline results by 45.44%. ",
    "link": "https://arxiv.org/abs/2402.16733",
    "context": "Title: DREsS: Dataset for Rubric-based Essay Scoring on EFL Writing\nAbstract: arXiv:2402.16733v1 Announce Type: new  Abstract: Automated essay scoring (AES) is a useful tool in English as a Foreign Language (EFL) writing education, offering real-time essay scores for students and instructors. However, previous AES models were trained on essays and scores irrelevant to the practical scenarios of EFL writing education and usually provided a single holistic score due to the lack of appropriate datasets. In this paper, we release DREsS, a large-scale, standard dataset for rubric-based automated essay scoring. DREsS comprises three sub-datasets: DREsS_New, DREsS_Std., and DREsS_CASE. We collect DREsS_New, a real-classroom dataset with 1.7K essays authored by EFL undergraduate students and scored by English education experts. We also standardize existing rubric-based essay scoring datasets as DREsS_Std. We suggest CASE, a corruption-based augmentation strategy for essays, which generates 20K synthetic samples of DREsS_CASE and improves the baseline results by 45.44%. ",
    "path": "papers/24/02/2402.16733.json",
    "total_tokens": 944,
    "translated_title": "DREsS: 英语作为外语写作基于评分标准的数据集",
    "translated_abstract": "自动化作文评分（AES）是英语作为外语写作教育中一种有用的工具，为学生和教师提供实时作文评分。然而，先前的AES模型是在与EFL写作教育实际场景不相关的作文和分数上进行训练的，并且通常由于缺乏适当的数据集而提供单一的整体评分。在本文中，我们发布了DREsS，这是一个用于基于评分标准的自动作文评分的大型标准数据集。DREsS包括三个子数据集：DREsS_New，DREsS_Std.和DREsS_CASE。我们收集了DREsS_New，这是一个由EFL本科生撰写并由英语教育专家评分的真实课堂数据集。我们还将现有的基于评分标准的作文评分数据集标准化为DREsS_Std。我们提出了一个名为CASE的基于破坏的作文增强策略，用于生成20K个DREsS_CASE的合成样本，并将基线结果提高了45.44％。",
    "tldr": "本文发布了一个大型标准数据集DREsS，用于基于评分标准的自动作文评分，在提出了一种基于破坏的作文增强策略CASE后，这个数据集的基线结果提高了45.44％。"
}