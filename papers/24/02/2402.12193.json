{
    "title": "A Chinese Dataset for Evaluating the Safeguards in Large Language Models",
    "abstract": "arXiv:2402.12193v1 Announce Type: new  Abstract: Many studies have demonstrated that large language models (LLMs) can produce harmful responses, exposing users to unexpected risks when LLMs are deployed. Previous studies have proposed comprehensive taxonomies of the risks posed by LLMs, as well as corresponding prompts that can be used to examine the safety mechanisms of LLMs. However, the focus has been almost exclusively on English, and little has been explored for other languages. Here we aim to bridge this gap. We first introduce a dataset for the safety evaluation of Chinese LLMs, and then extend it to two other scenarios that can be used to better identify false negative and false positive examples in terms of risky prompt rejections. We further present a set of fine-grained safety assessment criteria for each risk type, facilitating both manual annotation and automatic evaluation in terms of LLM response harmfulness. Our experiments on five LLMs show that region-specific risks a",
    "link": "https://arxiv.org/abs/2402.12193",
    "context": "Title: A Chinese Dataset for Evaluating the Safeguards in Large Language Models\nAbstract: arXiv:2402.12193v1 Announce Type: new  Abstract: Many studies have demonstrated that large language models (LLMs) can produce harmful responses, exposing users to unexpected risks when LLMs are deployed. Previous studies have proposed comprehensive taxonomies of the risks posed by LLMs, as well as corresponding prompts that can be used to examine the safety mechanisms of LLMs. However, the focus has been almost exclusively on English, and little has been explored for other languages. Here we aim to bridge this gap. We first introduce a dataset for the safety evaluation of Chinese LLMs, and then extend it to two other scenarios that can be used to better identify false negative and false positive examples in terms of risky prompt rejections. We further present a set of fine-grained safety assessment criteria for each risk type, facilitating both manual annotation and automatic evaluation in terms of LLM response harmfulness. Our experiments on five LLMs show that region-specific risks a",
    "path": "papers/24/02/2402.12193.json",
    "total_tokens": 907,
    "translated_title": "用于评估大型语言模型中安全机制的中文数据集",
    "translated_abstract": "许多研究已经证明大型语言模型（LLMs）可能产生有害响应，在LLMs部署时使用户面临意外风险。先前的研究提出了关于LLMs引发风险的综合分类法，以及相应的提示，可用于检查LLMs的安全机制。然而，这些研究几乎完全集中在英语上，其他语言的研究较少。在本文中，我们旨在弥补这一空白。我们首先介绍了一个用于评估中文LLMs安全性的数据集，然后将其扩展到另外两种情景，可用于更好地识别关于有风险提示拒绝的虚阔负面和错误肯定示例。我们进一步针对每种风险类型提出一组细粒度的安全评估标准，促进人工标注和自动评估LLM响应有害性。我们对五个LLM的实验表明，特定于地区的风险...",
    "tldr": "该研究介绍了一个用于评估中文LLMs安全性的数据集，提出了细粒度的安全评估标准，以及扩展了两种场景用于识别有风险提示拒绝的虚阔负面和错误肯定示例。"
}