{
    "title": "Private Gradient Descent for Linear Regression: Tighter Error Bounds and Instance-Specific Uncertainty Estimation",
    "abstract": "arXiv:2402.13531v1 Announce Type: new  Abstract: We provide an improved analysis of standard differentially private gradient descent for linear regression under the squared error loss. Under modest assumptions on the input, we characterize the distribution of the iterate at each time step.   Our analysis leads to new results on the algorithm's accuracy: for a proper fixed choice of hyperparameters, the sample complexity depends only linearly on the dimension of the data. This matches the dimension-dependence of the (non-private) ordinary least squares estimator as well as that of recent private algorithms that rely on sophisticated adaptive gradient-clipping schemes (Varshney et al., 2022; Liu et al., 2023).   Our analysis of the iterates' distribution also allows us to construct confidence intervals for the empirical optimizer which adapt automatically to the variance of the algorithm on a particular data set. We validate our theorems through experiments on synthetic data.",
    "link": "https://arxiv.org/abs/2402.13531",
    "context": "Title: Private Gradient Descent for Linear Regression: Tighter Error Bounds and Instance-Specific Uncertainty Estimation\nAbstract: arXiv:2402.13531v1 Announce Type: new  Abstract: We provide an improved analysis of standard differentially private gradient descent for linear regression under the squared error loss. Under modest assumptions on the input, we characterize the distribution of the iterate at each time step.   Our analysis leads to new results on the algorithm's accuracy: for a proper fixed choice of hyperparameters, the sample complexity depends only linearly on the dimension of the data. This matches the dimension-dependence of the (non-private) ordinary least squares estimator as well as that of recent private algorithms that rely on sophisticated adaptive gradient-clipping schemes (Varshney et al., 2022; Liu et al., 2023).   Our analysis of the iterates' distribution also allows us to construct confidence intervals for the empirical optimizer which adapt automatically to the variance of the algorithm on a particular data set. We validate our theorems through experiments on synthetic data.",
    "path": "papers/24/02/2402.13531.json",
    "total_tokens": 855,
    "translated_title": "私有梯度下降用于线性回归：更紧的误差界限和实例特定的不确定性估计",
    "translated_abstract": "我们对标准的具有差分隐私的线性回归梯度下降进行了改进分析，针对平方误差损失。 在对输入进行适度假设的情况下，我们表征了每个时间步骤中迭代的分布。 我们的分析导致了算法准确性方面的新结果：对于适当固定的超参数选择，样本复杂度仅与数据维度线性相关。 这与（非私有）普通最小二乘估计器的维度相关性以及依赖于复杂自适应梯度裁剪方案的最近私有算法（Varshney等人，2022年; Liu等人，2023年）的维度相关性相匹配。 我们对迭代分布的分析还允许我们构建置信区间，用于自动适应于特定数据集上算法的方差的经验优化器。 我们通过对合成数据的实验验证了我们的定理。",
    "tldr": "对于线性回归的标准差分隐私梯度下降，通过改进分析得出了更紧的误差界限和实例特定的不确定性估计。",
    "en_tdlr": "Improved analysis of differentially private gradient descent for linear regression leads to tighter error bounds and instance-specific uncertainty estimation."
}