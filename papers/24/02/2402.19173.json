{
    "title": "StarCoder 2 and The Stack v2: The Next Generation",
    "abstract": "arXiv:2402.19173v1 Announce Type: cross  Abstract: The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other mode",
    "link": "https://arxiv.org/abs/2402.19173",
    "context": "Title: StarCoder 2 and The Stack v2: The Next Generation\nAbstract: arXiv:2402.19173v1 Announce Type: cross  Abstract: The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other mode",
    "path": "papers/24/02/2402.19173.json",
    "total_tokens": 924,
    "translated_title": "StarCoder 2和The Stack v2: 下一代",
    "translated_abstract": "BigCode项目是一个专注于对代码（代码LLMs）进行负责任开发的开放科学合作项目，引入了StarCoder2。我们与Software Heritage（SWH）合作，在他们的源代码存档的数字公共资源之上构建The Stack v2。除了涵盖619种编程语言的SWH存储库外，我们还精心选择其他高质量的数据来源，如GitHub拉取请求、Kaggle笔记本和代码文档。这导致一个训练集，比第一个StarCoder数据集大4倍。我们使用3B、7B和15B参数的StarCoder2模型训练3.3至4.3万亿个标记，并在一套全面的Code LLM基准测试中进行彻底评估。我们发现，我们的小型模型StarCoder2-3B在大多数基准测试上表现优于其他类似规模的Code LLM，并且也优于StarCoderBase-15B。我们的大型模型StarCoder2-15B明显优于其他模型。",
    "tldr": "BigCode项目引入了StarCoder2和The Stack v2，在SWH存储库的基础上构建，并通过综合的Code LLM基准测试表明，StarCoder2-3B模型在大多数基准测试上优于其他同等规模的模型，甚至优于StarCoderBase-15B。",
    "en_tdlr": "The BigCode project introduces StarCoder2 and The Stack v2, built on the SWH repositories, and through comprehensive Code LLM benchmarks, it is shown that the StarCoder2-3B model outperforms other models of similar size on most benchmarks, even outperforming StarCoderBase-15B."
}