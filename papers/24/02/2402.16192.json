{
    "title": "Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing",
    "abstract": "arXiv:2402.16192v1 Announce Type: new  Abstract: Aligned large language models (LLMs) are vulnerable to jailbreaking attacks, which bypass the safeguards of targeted LLMs and fool them into generating objectionable content. While initial defenses show promise against token-based threat models, there do not exist defenses that provide robustness against semantic attacks and avoid unfavorable trade-offs between robustness and nominal performance. To meet this need, we propose SEMANTICSMOOTH, a smoothing-based defense that aggregates the predictions of multiple semantically transformed copies of a given input prompt. Experimental results demonstrate that SEMANTICSMOOTH achieves state-of-the-art robustness against GCG, PAIR, and AutoDAN attacks while maintaining strong nominal performance on instruction following benchmarks such as InstructionFollowing and AlpacaEval. The codes will be publicly available at https://github.com/UCSB-NLP-Chang/SemanticSmooth.",
    "link": "https://arxiv.org/abs/2402.16192",
    "context": "Title: Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing\nAbstract: arXiv:2402.16192v1 Announce Type: new  Abstract: Aligned large language models (LLMs) are vulnerable to jailbreaking attacks, which bypass the safeguards of targeted LLMs and fool them into generating objectionable content. While initial defenses show promise against token-based threat models, there do not exist defenses that provide robustness against semantic attacks and avoid unfavorable trade-offs between robustness and nominal performance. To meet this need, we propose SEMANTICSMOOTH, a smoothing-based defense that aggregates the predictions of multiple semantically transformed copies of a given input prompt. Experimental results demonstrate that SEMANTICSMOOTH achieves state-of-the-art robustness against GCG, PAIR, and AutoDAN attacks while maintaining strong nominal performance on instruction following benchmarks such as InstructionFollowing and AlpacaEval. The codes will be publicly available at https://github.com/UCSB-NLP-Chang/SemanticSmooth.",
    "path": "papers/24/02/2402.16192.json",
    "total_tokens": 802,
    "translated_title": "通过语义平滑防御大型语言模型遭遇监狱攻击",
    "translated_abstract": "对齐的大型语言模型(LLMs)容易受到监狱攻击的威胁，这些攻击可以绕过目标LLMs的保护措施，并骗过它们生成令人反感的内容。我们提出了SEMANTICSMOOTH，一种基于平滑的防御方法，通过聚合多个经过语义转换的给定输入提示的预测结果，来提高对GCG、PAIR和AutoDAN攻击的抵抗能力。实验结果表明，SEMANTICSMOOTH在保持指导性基准测试（如InstructionFollowing和AlpacaEval）上的强劲性能的同时，实现了对各种攻击的最新技术防御。",
    "tldr": "提出了一种名为SEMANTICSMOOTH的防御方法，通过聚合多个语义转换副本的预测结果来防御大型语言模型遭遇GCG、PAIR和AutoDAN攻击，同时保持了较强的正常性能。",
    "en_tdlr": "Proposed a defense method called SEMANTICSMOOTH, which enhances the robustness of large language models against GCG, PAIR, and AutoDAN attacks by aggregating predictions from multiple semantically transformed copies, while maintaining strong nominal performance."
}