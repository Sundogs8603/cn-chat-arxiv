{
    "title": "Towards Accurate Post-training Quantization for Reparameterized Models",
    "abstract": "arXiv:2402.16121v1 Announce Type: cross  Abstract: Model reparameterization is a widely accepted technique for improving inference speed without compromising performance. However, current Post-training Quantization (PTQ) methods often lead to significant accuracy degradation when applied to reparameterized models. This is primarily caused by channel-specific and sample-specific outliers, which appear only at specific samples and channels and impact on the selection of quantization parameters. To address this issue, we propose RepAPQ, a novel framework that preserves the accuracy of quantized reparameterization models. Different from previous frameworks using Mean Squared Error (MSE) as a measurement, we utilize Mean Absolute Error (MAE) to mitigate the influence of outliers on quantization parameters. Our framework comprises two main components: Quantization Protecting Reparameterization and Across-block Calibration. For effective calibration, Quantization Protecting Reparameterization",
    "link": "https://arxiv.org/abs/2402.16121",
    "context": "Title: Towards Accurate Post-training Quantization for Reparameterized Models\nAbstract: arXiv:2402.16121v1 Announce Type: cross  Abstract: Model reparameterization is a widely accepted technique for improving inference speed without compromising performance. However, current Post-training Quantization (PTQ) methods often lead to significant accuracy degradation when applied to reparameterized models. This is primarily caused by channel-specific and sample-specific outliers, which appear only at specific samples and channels and impact on the selection of quantization parameters. To address this issue, we propose RepAPQ, a novel framework that preserves the accuracy of quantized reparameterization models. Different from previous frameworks using Mean Squared Error (MSE) as a measurement, we utilize Mean Absolute Error (MAE) to mitigate the influence of outliers on quantization parameters. Our framework comprises two main components: Quantization Protecting Reparameterization and Across-block Calibration. For effective calibration, Quantization Protecting Reparameterization",
    "path": "papers/24/02/2402.16121.json",
    "total_tokens": 858,
    "translated_title": "为重新参数化模型实现准确的后训练量化",
    "translated_abstract": "模型重新参数化是一种被广泛接受的技术，可以在不损害性能的情况下提高推断速度。然而，当前的后训练量化（PTQ）方法在应用于重新参数化模型时往往会导致显著的准确性下降。这主要是由特定样本和通道上出现的异常值所导致的，这些异常值仅出现在特定的样本和通道上，并对量化参数的选择产生影响。为了解决这个问题，我们提出了RepAPQ，这是一个新颖的框架，可以保持量化后重新参数化模型的准确性。与以往使用均方误差（MSE）作为度量的框架不同，我们利用平均绝对误差（MAE）来减轻异常值对量化参数的影响。我们的框架包括两个主要组件：保护量化重新参数化和跨块校准。为了有效校准，请了解保护量化重新参数化",
    "tldr": "提出了一个新框架RepAPQ，利用平均绝对误差（MAE）来减轻异常值对量化参数的影响，从而保持量化后重新参数化模型的准确性",
    "en_tdlr": "Proposed a new framework RepAPQ that uses Mean Absolute Error (MAE) to mitigate the impact of outliers on quantization parameters, preserving the accuracy of quantized reparameterization models."
}