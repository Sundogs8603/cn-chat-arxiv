{
    "title": "FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition",
    "abstract": "In this paper, we introduce FROSTER, an effective framework for open-vocabulary action recognition. The CLIP model has achieved remarkable success in a range of image-based tasks, benefiting from its strong generalization capability stemming from pretaining on massive image-text pairs. However, applying CLIP directly to the open-vocabulary action recognition task is challenging due to the absence of temporal information in CLIP's pretraining. Further, fine-tuning CLIP on action recognition datasets may lead to overfitting and hinder its generalizability, resulting in unsatisfactory results when dealing with unseen actions.   To address these issues, FROSTER employs a residual feature distillation approach to ensure that CLIP retains its generalization capability while effectively adapting to the action recognition task. Specifically, the residual feature distillation treats the frozen CLIP model as a teacher to maintain the generalizability exhibited by the original CLIP and supervises",
    "link": "https://arxiv.org/abs/2402.03241",
    "context": "Title: FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition\nAbstract: In this paper, we introduce FROSTER, an effective framework for open-vocabulary action recognition. The CLIP model has achieved remarkable success in a range of image-based tasks, benefiting from its strong generalization capability stemming from pretaining on massive image-text pairs. However, applying CLIP directly to the open-vocabulary action recognition task is challenging due to the absence of temporal information in CLIP's pretraining. Further, fine-tuning CLIP on action recognition datasets may lead to overfitting and hinder its generalizability, resulting in unsatisfactory results when dealing with unseen actions.   To address these issues, FROSTER employs a residual feature distillation approach to ensure that CLIP retains its generalization capability while effectively adapting to the action recognition task. Specifically, the residual feature distillation treats the frozen CLIP model as a teacher to maintain the generalizability exhibited by the original CLIP and supervises",
    "path": "papers/24/02/2402.03241.json",
    "total_tokens": 783,
    "translated_title": "FROSTER: 冻结的CLIP是用于开放式词汇动作识别的有力教师",
    "translated_abstract": "本文介绍了FROSTER，一种用于开放式词汇动作识别的有效框架。CLIP模型在各种基于图像的任务中取得了显著的成功，得益于其在大规模图像-文本对上的预训练所展现出的强大泛化能力。然而，将CLIP直接应用于开放式词汇动作识别任务是具有挑战性的，因为CLIP的预训练中缺少时间信息。此外，对动作识别数据集进行CLIP的微调可能会导致过拟合，阻碍其泛化能力，导致处理未知动作时结果不尽如人意。",
    "tldr": "FROSTER是一种用于开放式词汇动作识别的框架，通过使用冻结的CLIP模型作为教师，在保持CLIP泛化能力的同时有效适应动作识别任务。",
    "en_tdlr": "FROSTER is an effective framework for open-vocabulary action recognition. It addresses the challenge of applying CLIP directly to this task by using the frozen CLIP model as a teacher to maintain its generalization capability while effectively adapting to action recognition."
}