{
    "title": "LongHeads: Multi-Head Attention is Secretly a Long Context Processor",
    "abstract": "arXiv:2402.10685v1 Announce Type: cross  Abstract: Large language models (LLMs) have achieved impressive performance in numerous domains but often struggle to process lengthy inputs effectively and efficiently due to limited length generalization and attention's quadratic computational demands. Many sought to mitigate this by restricting the attention window within the pre-trained length. However, these methods introduce new issues such as ignoring the middle context and requiring additional training. To address these problems, we propose LongHeads, a training-free framework that enhances LLM's long context ability by unlocking multi-head attention's untapped potential. Instead of allowing each head to attend to the full sentence, which struggles with generalizing to longer sequences due to out-of-distribution (OOD) issues, we allow each head to process in-distribution length by selecting and attending to important context chunks. To this end, we propose a chunk selection strategy that",
    "link": "https://arxiv.org/abs/2402.10685",
    "context": "Title: LongHeads: Multi-Head Attention is Secretly a Long Context Processor\nAbstract: arXiv:2402.10685v1 Announce Type: cross  Abstract: Large language models (LLMs) have achieved impressive performance in numerous domains but often struggle to process lengthy inputs effectively and efficiently due to limited length generalization and attention's quadratic computational demands. Many sought to mitigate this by restricting the attention window within the pre-trained length. However, these methods introduce new issues such as ignoring the middle context and requiring additional training. To address these problems, we propose LongHeads, a training-free framework that enhances LLM's long context ability by unlocking multi-head attention's untapped potential. Instead of allowing each head to attend to the full sentence, which struggles with generalizing to longer sequences due to out-of-distribution (OOD) issues, we allow each head to process in-distribution length by selecting and attending to important context chunks. To this end, we propose a chunk selection strategy that",
    "path": "papers/24/02/2402.10685.json",
    "total_tokens": 805,
    "translated_title": "LongHeads: 多头注意力其实是一个长上下文处理器",
    "translated_abstract": "大型语言模型(LLMs)在许多领域取得了令人印象深刻的表现，但由于有限长度泛化和注意力的二次计算需求，往往难以有效高效地处理较长的输入。 许多人试图通过限制在预训练长度内的注意力窗口来缓解这一问题。 然而，这些方法引入了新问题，如忽略中间上下文和需要额外训练。 为了解决这些问题，我们提出了LongHeads，一个无需训练的框架，通过释放多头注意力的潜力来增强LLM的长上下文能力。 我们允许每个头部选择并关注重要的上下文块，以处理分布长度，而不是让每个头部都参与全句注意力，这样做由于分布之外的问题而难以泛化到更长的序列。",
    "tldr": "LongHeads 提出了一个无需训练的框架，通过释放多头注意力的潜力来增强大型语言模型(LLM)处理长上下文的能力。",
    "en_tdlr": "LongHeads proposes a training-free framework to enhance the ability of large language models (LLMs) in processing long contexts by unlocking the potential of multi-head attention."
}