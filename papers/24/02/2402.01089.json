{
    "title": "No Free Prune: Information-Theoretic Barriers to Pruning at Initialization",
    "abstract": "The existence of \"lottery tickets\" arXiv:1803.03635 at or near initialization raises the tantalizing question of whether large models are necessary in deep learning, or whether sparse networks can be quickly identified and trained without ever training the dense models that contain them. However, efforts to find these sparse subnetworks without training the dense model (\"pruning at initialization\") have been broadly unsuccessful arXiv:2009.08576. We put forward a theoretical explanation for this, based on the model's effective parameter count, $p_\\text{eff}$, given by the sum of the number of non-zero weights in the final network and the mutual information between the sparsity mask and the data. We show the Law of Robustness of arXiv:2105.12806 extends to sparse networks with the usual parameter count replaced by $p_\\text{eff}$, meaning a sparse neural network which robustly interpolates noisy data requires a heavily data-dependent mask. We posit that pruning during and after training ",
    "link": "https://rss.arxiv.org/abs/2402.01089",
    "context": "Title: No Free Prune: Information-Theoretic Barriers to Pruning at Initialization\nAbstract: The existence of \"lottery tickets\" arXiv:1803.03635 at or near initialization raises the tantalizing question of whether large models are necessary in deep learning, or whether sparse networks can be quickly identified and trained without ever training the dense models that contain them. However, efforts to find these sparse subnetworks without training the dense model (\"pruning at initialization\") have been broadly unsuccessful arXiv:2009.08576. We put forward a theoretical explanation for this, based on the model's effective parameter count, $p_\\text{eff}$, given by the sum of the number of non-zero weights in the final network and the mutual information between the sparsity mask and the data. We show the Law of Robustness of arXiv:2105.12806 extends to sparse networks with the usual parameter count replaced by $p_\\text{eff}$, meaning a sparse neural network which robustly interpolates noisy data requires a heavily data-dependent mask. We posit that pruning during and after training ",
    "path": "papers/24/02/2402.01089.json",
    "total_tokens": 967,
    "translated_title": "无免费修剪：初始化时剪枝的信息论障碍",
    "translated_abstract": "“抽奖中奖者”是否在初始化时存在，引发了一个令人着迷的问题：深度学习是否需要大型模型，或者可以在不训练包含它们的密集模型的情况下迅速识别和训练稀疏网络。然而，尝试在初始化时找到这些稀疏子网络（“初始化时修剪”）的努力在广泛上都没有成功。我们提出了一个理论解释，基于模型的有效参数数量$p_\\text{eff}$，由最终网络中非零权重的数量和稀疏掩码与数据之间的相互信息的总和给出。我们展示了“鲁棒性定律”（arXiv:2105.12806）延伸到稀疏网络，其中常规参数数量被$p_\\text{eff}$所取代，这意味着一个能够在嘈杂数据中鲁棒地插值的稀疏神经网络需要严重依赖于数据的掩码。我们假设在训练过程中和训练后修剪。",
    "tldr": "本文解释了为什么在初始化时修剪神经网络困难，并提出了一个关于有效参数数量的理论解释。我们指出，在嘈杂数据中鲁棒地插值的稀疏神经网络需要严重依赖于数据的掩码。为此，我们怀疑在训练过程中和训练后修剪是必要的。",
    "en_tdlr": "This paper explains why pruning neural networks during initialization is difficult and provides a theoretical explanation about the effective parameter count. We point out that sparse neural networks robustly interpolating noisy data heavily rely on data-dependent masks, suggesting the necessity of pruning during and after training."
}