{
    "title": "GenRES: Rethinking Evaluation for Generative Relation Extraction in the Era of Large Language Models",
    "abstract": "arXiv:2402.10744v1 Announce Type: cross  Abstract: The field of relation extraction (RE) is experiencing a notable shift towards generative relation extraction (GRE), leveraging the capabilities of large language models (LLMs). However, we discovered that traditional relation extraction (RE) metrics like precision and recall fall short in evaluating GRE methods. This shortfall arises because these metrics rely on exact matching with human-annotated reference relations, while GRE methods often produce diverse and semantically accurate relations that differ from the references. To fill this gap, we introduce GenRES for a multi-dimensional assessment in terms of the topic similarity, uniqueness, granularity, factualness, and completeness of the GRE results. With GenRES, we empirically identified that (1) precision/recall fails to justify the performance of GRE methods; (2) human-annotated referential relations can be incomplete; (3) prompting LLMs with a fixed set of relations or entities",
    "link": "https://arxiv.org/abs/2402.10744",
    "context": "Title: GenRES: Rethinking Evaluation for Generative Relation Extraction in the Era of Large Language Models\nAbstract: arXiv:2402.10744v1 Announce Type: cross  Abstract: The field of relation extraction (RE) is experiencing a notable shift towards generative relation extraction (GRE), leveraging the capabilities of large language models (LLMs). However, we discovered that traditional relation extraction (RE) metrics like precision and recall fall short in evaluating GRE methods. This shortfall arises because these metrics rely on exact matching with human-annotated reference relations, while GRE methods often produce diverse and semantically accurate relations that differ from the references. To fill this gap, we introduce GenRES for a multi-dimensional assessment in terms of the topic similarity, uniqueness, granularity, factualness, and completeness of the GRE results. With GenRES, we empirically identified that (1) precision/recall fails to justify the performance of GRE methods; (2) human-annotated referential relations can be incomplete; (3) prompting LLMs with a fixed set of relations or entities",
    "path": "papers/24/02/2402.10744.json",
    "total_tokens": 833,
    "translated_title": "GenRES：在大语言模型时代重新思考生成式关系抽取的评估",
    "translated_abstract": "关系抽取（RE）领域正朝着利用大语言模型（LLM）的能力的生成式关系抽取（GRE）方向发生显着转变。然而，我们发现传统的关系抽取（RE）指标如精确率和召回率在评估GRE方法时存在不足。这种不足的原因在于这些指标依赖于与人工注释的参考关系的精确匹配，而GRE方法通常会产生与参考不同的多样且语义准确的关系。为填补这一空白，我们提出了GenRES，以多维度评估GRE结果的主题相似性、独特性、粒度、真实性和完整性。通过GenRES，我们实证发现：（1）精确率/召回率不能充分证明GRE方法的性能；（2）人工注释的参考关系可能存在不完整情况；（3）以固定一组关系或实体提示LLM",
    "tldr": "GenRES提出了一种多维度评估生成式关系抽取结果的方法，填补了使用传统指标评估GRE方法时的不足之处。",
    "en_tdlr": "GenRES introduces a multidimensional assessment approach for evaluating generative relation extraction results, addressing the shortcomings of using traditional metrics to evaluate GRE methods."
}