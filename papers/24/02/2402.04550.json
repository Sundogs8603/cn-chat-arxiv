{
    "title": "Riemann-Lebesgue Forest for Regression",
    "abstract": "We propose a novel ensemble method called Riemann-Lebesgue Forest (RLF) for regression. The core idea of RLF is to mimic the way how a measurable function can be approximated by partitioning its range into a few intervals. With this idea in mind, we develop a new tree learner named Riemann-Lebesgue Tree which has a chance to split the node from response $Y$ or a direction in feature space $\\mathbf{X}$ at each non-terminal node. We generalize the asymptotic performance of RLF under different parameter settings mainly through Hoeffding decomposition \\cite{Vaart} and Stein's method \\cite{Chen2010NormalAB}. When the underlying function $Y=f(\\mathbf{X})$ follows an additive regression model, RLF is consistent with the argument from \\cite{Scornet2014ConsistencyOR}. The competitive performance of RLF against original random forest \\cite{Breiman2001RandomF} is demonstrated by experiments in simulation data and real world datasets.",
    "link": "https://arxiv.org/abs/2402.04550",
    "context": "Title: Riemann-Lebesgue Forest for Regression\nAbstract: We propose a novel ensemble method called Riemann-Lebesgue Forest (RLF) for regression. The core idea of RLF is to mimic the way how a measurable function can be approximated by partitioning its range into a few intervals. With this idea in mind, we develop a new tree learner named Riemann-Lebesgue Tree which has a chance to split the node from response $Y$ or a direction in feature space $\\mathbf{X}$ at each non-terminal node. We generalize the asymptotic performance of RLF under different parameter settings mainly through Hoeffding decomposition \\cite{Vaart} and Stein's method \\cite{Chen2010NormalAB}. When the underlying function $Y=f(\\mathbf{X})$ follows an additive regression model, RLF is consistent with the argument from \\cite{Scornet2014ConsistencyOR}. The competitive performance of RLF against original random forest \\cite{Breiman2001RandomF} is demonstrated by experiments in simulation data and real world datasets.",
    "path": "papers/24/02/2402.04550.json",
    "total_tokens": 956,
    "translated_title": "Riemann-Lebesgue Forest回归方法的研究",
    "translated_abstract": "我们提出了一种新颖的用于回归问题的集成方法，称为Riemann-Lebesgue Forest (RLF)。RLF的核心思想是通过将函数的值域划分为几个区间来模拟可测函数的逼近方式。基于这个思想，我们开发了一种新的树学习算法，称为Riemann-Lebesgue Tree，它在每个非叶节点上有机会从响应Y或特征空间X中的方向进行切割。我们通过Hoeffding分解和Stein方法来推导不同参数设置下RLF的渐近性能。当底层函数Y=f(X)遵循加法回归模型时，RLF与Scornet等人的论证（2014年）保持一致。通过在仿真数据和真实世界数据集上的实验证明，RLF与原始随机森林相比具有竞争力的性能。",
    "tldr": "提出了一种新颖的集成方法Riemann-Lebesgue Forest (RLF)用于回归问题，通过划分函数的值域为多个区间来逼近可测函数的思想，开发了一种新的树学习算法Riemann-Lebesgue Tree。通过Hoeffding分解和Stein方法推导了RLF在不同参数设置下的渐近性能，并在仿真数据和真实世界数据集上的实验中证明了RLF与原始随机森林相比具有竞争力的性能。",
    "en_tdlr": "A novel ensemble method called Riemann-Lebesgue Forest (RLF) is proposed for regression, which approximates measurable functions by partitioning their range. RLF utilizes a new tree learner called Riemann-Lebesgue Tree and generalizes its asymptotic performance through Hoeffding decomposition and Stein's method. Experimental results show that RLF performs competitively against the original random forest."
}