{
    "title": "Breaking the Curse of Dimensionality with Distributed Neural Computation",
    "abstract": "We present a theoretical approach to overcome the curse of dimensionality using a neural computation algorithm which can be distributed across several machines. Our modular distributed deep learning paradigm, termed \\textit{neural pathways}, can achieve arbitrary accuracy while only loading a small number of parameters into GPU VRAM. Formally, we prove that for every error level $\\varepsilon>0$ and every Lipschitz function $f:[0,1]^n\\to \\mathbb{R}$, one can construct a neural pathways model which uniformly approximates $f$ to $\\varepsilon$ accuracy over $[0,1]^n$ while only requiring networks of $\\mathcal{O}(\\varepsilon^{-1})$ parameters to be loaded in memory and $\\mathcal{O}(\\varepsilon^{-1}\\log(\\varepsilon^{-1}))$ to be loaded during the forward pass. This improves the optimal bounds for traditional non-distributed deep learning models, namely ReLU MLPs, which need $\\mathcal{O}(\\varepsilon^{-n/2})$ parameters to achieve the same accuracy. The only other available deep learning model",
    "link": "https://arxiv.org/abs/2402.03460",
    "context": "Title: Breaking the Curse of Dimensionality with Distributed Neural Computation\nAbstract: We present a theoretical approach to overcome the curse of dimensionality using a neural computation algorithm which can be distributed across several machines. Our modular distributed deep learning paradigm, termed \\textit{neural pathways}, can achieve arbitrary accuracy while only loading a small number of parameters into GPU VRAM. Formally, we prove that for every error level $\\varepsilon>0$ and every Lipschitz function $f:[0,1]^n\\to \\mathbb{R}$, one can construct a neural pathways model which uniformly approximates $f$ to $\\varepsilon$ accuracy over $[0,1]^n$ while only requiring networks of $\\mathcal{O}(\\varepsilon^{-1})$ parameters to be loaded in memory and $\\mathcal{O}(\\varepsilon^{-1}\\log(\\varepsilon^{-1}))$ to be loaded during the forward pass. This improves the optimal bounds for traditional non-distributed deep learning models, namely ReLU MLPs, which need $\\mathcal{O}(\\varepsilon^{-n/2})$ parameters to achieve the same accuracy. The only other available deep learning model",
    "path": "papers/24/02/2402.03460.json",
    "total_tokens": 959,
    "translated_title": "用分布式神经计算突破维度灾难",
    "translated_abstract": "我们提出了一种理论方法，利用分布式神经计算算法来克服维度灾难。我们的模块化分布式深度学习范式，称为“神经途径”，可以在多台机器上实现任意精度，同时仅加载少量参数到GPU VRAM中。具体地，我们证明了对于每个误差水平$\\varepsilon>0$和每个Lipschitz函数$f:[0,1]^n\\to \\mathbb{R}$，我们可以构建一个神经途径模型，该模型能够在$[0,1]^n$上以$\\varepsilon$精度均匀逼近$f$，并且仅需要在内存中加载$\\mathcal{O}(\\varepsilon^{-1})$个网络参数以及在前向传播期间加载$\\mathcal{O}(\\varepsilon^{-1}\\log(\\varepsilon^{-1}))$个网络参数。这改进了传统非分布式深度学习模型（即ReLU多层感知机）的最优界限，后者需要$\\mathcal{O}(\\varepsilon^{-n/2})$个参数来达到相同的精度。目前唯一的其他可用深度学习模型",
    "tldr": "通过分布式神经计算算法，我们提出了一种理论方法来克服维度灾难，并证明了我们的模型可以在任意精度下逼近Lipschitz函数，在参数量和前向传播方面具有优势。",
    "en_tdlr": "We propose a theoretical approach to overcome the curse of dimensionality using distributed neural computation. Our model, called \"neural pathways\", achieves arbitrary accuracy while loading fewer parameters and during forward pass compared to traditional non-distributed deep learning models."
}