{
    "title": "PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA",
    "abstract": "arXiv:2402.16902v1 Announce Type: new  Abstract: With the rapid scaling of large language models (LLMs), serving numerous LoRAs concurrently has become increasingly impractical, leading to unaffordable costs and necessitating more parameter-efficient finetuning methods. In this work, we introduce Partially Rotation-enhanced Low-Rank Adaptation (PRoLoRA), an intra-layer sharing mechanism comprising four essential components: broadcast reduction, rotation enhancement, partially-sharing refinement, and rectified initialization strategy. As a superset of LoRA, PRoLoRA pertains its advantages, and effectively circumvent the drawbacks of peer parameter-sharing methods with superior model capacity, practical feasibility, and broad applicability. Empirical experiments demonstrate the remarkably higher parameter efficiency of PRoLoRA in both specific parameter budget and performance target scenarios, and its scalability to larger LLMs. Notably, with one time less trainable parameters, PRoLoRA s",
    "link": "https://arxiv.org/abs/2402.16902",
    "context": "Title: PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA\nAbstract: arXiv:2402.16902v1 Announce Type: new  Abstract: With the rapid scaling of large language models (LLMs), serving numerous LoRAs concurrently has become increasingly impractical, leading to unaffordable costs and necessitating more parameter-efficient finetuning methods. In this work, we introduce Partially Rotation-enhanced Low-Rank Adaptation (PRoLoRA), an intra-layer sharing mechanism comprising four essential components: broadcast reduction, rotation enhancement, partially-sharing refinement, and rectified initialization strategy. As a superset of LoRA, PRoLoRA pertains its advantages, and effectively circumvent the drawbacks of peer parameter-sharing methods with superior model capacity, practical feasibility, and broad applicability. Empirical experiments demonstrate the remarkably higher parameter efficiency of PRoLoRA in both specific parameter budget and performance target scenarios, and its scalability to larger LLMs. Notably, with one time less trainable parameters, PRoLoRA s",
    "path": "papers/24/02/2402.16902.json",
    "total_tokens": 923,
    "translated_title": "PRoLoRA: 部分旋转增强更高效LoRA",
    "translated_abstract": "随着大型语言模型（LLMs）的快速扩展，同时服务多个LoRAs变得日益不切实际，导致成本不可承受，需要更具参数效率的微调方法。在这项工作中，我们引入了部分旋转增强低秩适应（PRoLoRA），这是一种由四个关键组件组成的层内共享机制：广播减少、旋转增强、部分共享细化和修正的初始化策略。作为LoRA的超集，PRoLoRA保留了其优势，并通过具有卓越的模型容量、实用可行性和广泛适用性，有效地避开了其他参数共享方法的缺点。实证实验表明，PRoLoRA在特定参数预算和性能目标情景下，具有显着更高的参数效率，并且可以扩展到更大的LLMs。值得注意的是，PRoLoRA在可训练参数减少一倍的情况下，",
    "tldr": "PRoLoRA是一个新的部分旋转增强低秩适应方法，通过引入广播减少、旋转增强、部分共享细化和修正初始化策略等四个组件，实现了对LoRA的优势提升，避免了其他参数共享方法的缺点，具有更高的参数效率和可扩展性。"
}