{
    "title": "The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large Language Models",
    "abstract": "In this work, we review research studies that combine Reinforcement Learning (RL) and Large Language Models (LLMs), two areas that owe their momentum to the development of deep neural networks. We propose a novel taxonomy of three main classes based on the way that the two model types interact with each other. The first class, RL4LLM, includes studies where RL is leveraged to improve the performance of LLMs on tasks related to Natural Language Processing. L4LLM is divided into two sub-categories depending on whether RL is used to directly fine-tune an existing LLM or to improve the prompt of the LLM. In the second class, LLM4RL, an LLM assists the training of an RL model that performs a task that is not inherently related to natural language. We further break down LLM4RL based on the component of the RL training framework that the LLM assists or replaces, namely reward shaping, goal generation, and policy function. Finally, in the third class, RL+LLM, an LLM and an RL agent are embedde",
    "link": "https://arxiv.org/abs/2402.01874",
    "context": "Title: The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large Language Models\nAbstract: In this work, we review research studies that combine Reinforcement Learning (RL) and Large Language Models (LLMs), two areas that owe their momentum to the development of deep neural networks. We propose a novel taxonomy of three main classes based on the way that the two model types interact with each other. The first class, RL4LLM, includes studies where RL is leveraged to improve the performance of LLMs on tasks related to Natural Language Processing. L4LLM is divided into two sub-categories depending on whether RL is used to directly fine-tune an existing LLM or to improve the prompt of the LLM. In the second class, LLM4RL, an LLM assists the training of an RL model that performs a task that is not inherently related to natural language. We further break down LLM4RL based on the component of the RL training framework that the LLM assists or replaces, namely reward shaping, goal generation, and policy function. Finally, in the third class, RL+LLM, an LLM and an RL agent are embedde",
    "path": "papers/24/02/2402.01874.json",
    "total_tokens": 858,
    "translated_title": "RL/LLM分类树：回顾强化学习和大语言模型之间的协同关系",
    "translated_abstract": "在这项工作中，我们回顾了将强化学习（RL）和大语言模型（LLM）结合起来的研究，并提出了一种新的三类分类方法，该分类方法基于这两种模型类型之间的交互方式。第一类是RL4LLM，包括利用RL改进与自然语言处理相关任务上LLM性能的研究。L4LLM分为两个子类，取决于RL是直接微调现有LLM还是改进LLM的提示。在第二类LLM4RL中，LLM辅助训练一个与自然语言无关的RL模型。我们进一步根据LLM辅助或替代RL训练框架的组件（奖励塑造、目标生成和策略函数）对LLM4RL进行了细分。最后，在第三类RL+LLM中，一个LLM和一个RL代理被嵌入其中。",
    "tldr": "这项工作回顾了将强化学习和大语言模型结合起来的研究，并提出了一个新的分类方法，以审视这两个领域之间的协同关系。",
    "en_tdlr": "This work reviews studies that combine reinforcement learning and large language models, proposing a novel taxonomy to examine the synergies between these two areas."
}