{
    "title": "Non-convergence to global minimizers for Adam and stochastic gradient descent optimization and constructions of local minimizers in the training of artificial neural networks",
    "abstract": "Stochastic gradient descent (SGD) optimization methods such as the plain vanilla SGD method and the popular Adam optimizer are nowadays the method of choice in the training of artificial neural networks (ANNs). Despite the remarkable success of SGD methods in the ANN training in numerical simulations, it remains in essentially all practical relevant scenarios an open problem to rigorously explain why SGD methods seem to succeed to train ANNs. In particular, in most practically relevant supervised learning problems, it seems that SGD methods do with high probability not converge to global minimizers in the optimization landscape of the ANN training problem. Nevertheless, it remains an open problem of research to disprove the convergence of SGD methods to global minimizers. In this work we solve this research problem in the situation of shallow ANNs with the rectified linear unit (ReLU) and related activations with the standard mean square error loss by disproving in the training of such",
    "link": "https://arxiv.org/abs/2402.05155",
    "context": "Title: Non-convergence to global minimizers for Adam and stochastic gradient descent optimization and constructions of local minimizers in the training of artificial neural networks\nAbstract: Stochastic gradient descent (SGD) optimization methods such as the plain vanilla SGD method and the popular Adam optimizer are nowadays the method of choice in the training of artificial neural networks (ANNs). Despite the remarkable success of SGD methods in the ANN training in numerical simulations, it remains in essentially all practical relevant scenarios an open problem to rigorously explain why SGD methods seem to succeed to train ANNs. In particular, in most practically relevant supervised learning problems, it seems that SGD methods do with high probability not converge to global minimizers in the optimization landscape of the ANN training problem. Nevertheless, it remains an open problem of research to disprove the convergence of SGD methods to global minimizers. In this work we solve this research problem in the situation of shallow ANNs with the rectified linear unit (ReLU) and related activations with the standard mean square error loss by disproving in the training of such",
    "path": "papers/24/02/2402.05155.json",
    "total_tokens": 914,
    "translated_title": "Adam和随机梯度下降优化在人工神经网络训练中的全局最小值非收敛性及局部最小解构造",
    "translated_abstract": "随机梯度下降（SGD）优化方法，如普通的SGD方法和流行的Adam优化器，现在是人工神经网络（ANNs）训练的首选方法。尽管SGD方法在数值模拟中在ANN训练中取得了显著的成功，但在所有实际相关的场景中，严格解释SGD方法为何成功训练ANNs仍然是一个未解决的问题。特别是在大多数实际相关的监督学习问题中，SGD方法似乎以高概率不收敛于ANN训练问题的优化空间的全局最小值。然而，证明SGD方法不会收敛于全局最小值仍然是一个未解决的研究问题。在本研究中，我们解决了这个研究问题，在具有修正线性单元（ReLU）和相关激活函数以及标准均方误差损失的浅层ANNs训练中，推翻了这种结论。",
    "tldr": "本研究解决了随机梯度下降方法在浅层人工神经网络训练中不收敛于全局最小值的问题，并提出了适用于该问题的局部最小解构造方法。",
    "en_tdlr": "This study addresses the issue of non-convergence to global minimizers for stochastic gradient descent methods in shallow artificial neural network training, particularly in the case of rectified linear unit (ReLU) activations and standard mean square error loss. It also proposes a method to construct local minimizers for this problem."
}