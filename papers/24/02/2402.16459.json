{
    "title": "Defending LLMs against Jailbreaking Attacks via Backtranslation",
    "abstract": "arXiv:2402.16459v1 Announce Type: cross  Abstract: Although many large language models (LLMs) have been trained to refuse harmful requests, they are still vulnerable to jailbreaking attacks, which rewrite the original prompt to conceal its harmful intent. In this paper, we propose a new method for defending LLMs against jailbreaking attacks by ``backtranslation''. Specifically, given an initial response generated by the target LLM from an input prompt, our backtranslation prompts a language model to infer an input prompt that can lead to the response. The inferred prompt is called the backtranslated prompt which tends to reveal the actual intent of the original prompt, since it is generated based on the LLM's response and is not directly manipulated by the attacker. We then run the target LLM again on the backtranslated prompt, and we refuse the original prompt if the model refuses the backtranslated prompt. We explain that the proposed defense provides several benefits on its effectiv",
    "link": "https://arxiv.org/abs/2402.16459",
    "context": "Title: Defending LLMs against Jailbreaking Attacks via Backtranslation\nAbstract: arXiv:2402.16459v1 Announce Type: cross  Abstract: Although many large language models (LLMs) have been trained to refuse harmful requests, they are still vulnerable to jailbreaking attacks, which rewrite the original prompt to conceal its harmful intent. In this paper, we propose a new method for defending LLMs against jailbreaking attacks by ``backtranslation''. Specifically, given an initial response generated by the target LLM from an input prompt, our backtranslation prompts a language model to infer an input prompt that can lead to the response. The inferred prompt is called the backtranslated prompt which tends to reveal the actual intent of the original prompt, since it is generated based on the LLM's response and is not directly manipulated by the attacker. We then run the target LLM again on the backtranslated prompt, and we refuse the original prompt if the model refuses the backtranslated prompt. We explain that the proposed defense provides several benefits on its effectiv",
    "path": "papers/24/02/2402.16459.json",
    "total_tokens": 873,
    "translated_title": "通过反向翻译防御LLMs免受越狱攻击",
    "translated_abstract": "尽管许多大型语言模型（LLMs）已经被训练成拒绝有害请求，但它们仍然容易受到越狱攻击的影响，这种攻击会重写原始提示以隐藏其有害意图。在本文中，我们提出了一种新方法，通过“反向翻译”来防御LLMs免受越狱攻击。具体来说，给定目标LLM从输入提示生成的初始响应，我们的反向翻译提示一个语言模型来推断可以导致该响应的输入提示。推断的提示称为反向翻译提示，倾向于揭示原始提示的实际意图，因为它是基于LLM的响应生成的，不是直接由攻击者操纵的。然后，我们再次在反向翻译提示上运行目标LLM，如果模型拒绝了反向翻译提示，则拒绝原始提示。我们解释了所提出的防御措施对其有效性的几个好处。",
    "tldr": "通过反向翻译来防御LLMs免受越狱攻击，将生成的反向翻译提示用于揭示原始提示的实际意图，提高了模型的安全性。"
}