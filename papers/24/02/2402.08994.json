{
    "title": "CLIP-MUSED: CLIP-Guided Multi-Subject Visual Neural Information Semantic Decoding",
    "abstract": "arXiv:2402.08994v1 Announce Type: cross Abstract: The study of decoding visual neural information faces challenges in generalizing single-subject decoding models to multiple subjects, due to individual differences. Moreover, the limited availability of data from a single subject has a constraining impact on model performance. Although prior multi-subject decoding methods have made significant progress, they still suffer from several limitations, including difficulty in extracting global neural response features, linear scaling of model parameters with the number of subjects, and inadequate characterization of the relationship between neural responses of different subjects to various stimuli. To overcome these limitations, we propose a CLIP-guided Multi-sUbject visual neural information SEmantic Decoding (CLIP-MUSED) method. Our method consists of a Transformer-based feature extractor to effectively model global neural representations. It also incorporates learnable subject-specific tok",
    "link": "https://arxiv.org/abs/2402.08994",
    "context": "Title: CLIP-MUSED: CLIP-Guided Multi-Subject Visual Neural Information Semantic Decoding\nAbstract: arXiv:2402.08994v1 Announce Type: cross Abstract: The study of decoding visual neural information faces challenges in generalizing single-subject decoding models to multiple subjects, due to individual differences. Moreover, the limited availability of data from a single subject has a constraining impact on model performance. Although prior multi-subject decoding methods have made significant progress, they still suffer from several limitations, including difficulty in extracting global neural response features, linear scaling of model parameters with the number of subjects, and inadequate characterization of the relationship between neural responses of different subjects to various stimuli. To overcome these limitations, we propose a CLIP-guided Multi-sUbject visual neural information SEmantic Decoding (CLIP-MUSED) method. Our method consists of a Transformer-based feature extractor to effectively model global neural representations. It also incorporates learnable subject-specific tok",
    "path": "papers/24/02/2402.08994.json",
    "total_tokens": 848,
    "translated_title": "CLIP-MUSED：CLIP引导的多主题视觉神经信息语义解码",
    "translated_abstract": "研究解码视觉神经信息面临着将单主题解码模型推广到多个主题的挑战，这是由于个体差异所导致的。此外，来自单个主题的数据有限对模型性能有着限制性影响。尽管之前的多主题解码方法取得了相当大的进展，但仍然存在一些限制，包括难以提取全局神经响应特征，模型参数随主题数量线性扩展以及对于不同主题的神经响应与各种刺激之间的关系描述不足。为了克服这些限制，我们提出了一种CLIP引导的多主题视觉神经信息语义解码（CLIP-MUSED）方法。我们的方法包括一个基于Transformer的特征提取器，用于有效地建模全局神经表示。它还包含学习得到的主题特定的tok",
    "tldr": "CLIP-MUSED是一种CLIP引导的多主题视觉神经信息语义解码方法，可以克服单主题解码模型推广到多个主题的挑战，并且通过使用Transformer-based特征提取器来有效建模全局神经表示。"
}