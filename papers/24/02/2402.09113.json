{
    "title": "Measuring Exploration in Reinforcement Learning via Optimal Transport in Policy Space",
    "abstract": "arXiv:2402.09113v1 Announce Type: new Abstract: Exploration is the key ingredient of reinforcement learning (RL) that determines the speed and success of learning. Here, we quantify and compare the amount of exploration and learning accomplished by a Reinforcement Learning (RL) algorithm. Specifically, we propose a novel measure, named Exploration Index, that quantifies the relative effort of knowledge transfer (transferability) by an RL algorithm in comparison to supervised learning (SL) that transforms the initial data distribution of RL to the corresponding final data distribution. The comparison is established by formulating learning in RL as a sequence of SL tasks, and using optimal transport based metrics to compare the total path traversed by the RL and SL algorithms in the data distribution space. We perform extensive empirical analysis on various environments and with multiple algorithms to demonstrate that the exploration index yields insights about the exploration behaviour ",
    "link": "https://arxiv.org/abs/2402.09113",
    "context": "Title: Measuring Exploration in Reinforcement Learning via Optimal Transport in Policy Space\nAbstract: arXiv:2402.09113v1 Announce Type: new Abstract: Exploration is the key ingredient of reinforcement learning (RL) that determines the speed and success of learning. Here, we quantify and compare the amount of exploration and learning accomplished by a Reinforcement Learning (RL) algorithm. Specifically, we propose a novel measure, named Exploration Index, that quantifies the relative effort of knowledge transfer (transferability) by an RL algorithm in comparison to supervised learning (SL) that transforms the initial data distribution of RL to the corresponding final data distribution. The comparison is established by formulating learning in RL as a sequence of SL tasks, and using optimal transport based metrics to compare the total path traversed by the RL and SL algorithms in the data distribution space. We perform extensive empirical analysis on various environments and with multiple algorithms to demonstrate that the exploration index yields insights about the exploration behaviour ",
    "path": "papers/24/02/2402.09113.json",
    "total_tokens": 822,
    "translated_title": "通过策略空间中的最优输运测量强化学习的探索",
    "translated_abstract": "探索是决定强化学习（RL）学习速度和成功的关键因素。我们在这里量化和比较强化学习算法所完成的探索和学习的数量。具体来说，我们提出了一种新的度量指标，称为探索指数，用于比较强化学习算法相对于监督学习算法在知识传输（可转移性）方面所付出的相对努力，以及将 RL 的初始数据分布转化为对应的最终数据分布。我们将强化学习中的学习建模为一系列监督学习任务，并使用基于最优输运的度量标准来比较 RL 和 SL 算法在数据分布空间中的总路径。我们在各种环境和多种算法上进行了广泛的实证分析，证明探索指数可以揭示探索行为的特点。",
    "tldr": "本论文提出了一种通过比较强化学习和监督学习算法在数据分布空间中路径长度来量化探索的方法，并在各种环境和多种算法上进行了实证分析。",
    "en_tdlr": "This paper proposes a method to quantify exploration in reinforcement learning by comparing the path lengths of reinforcement learning and supervised learning algorithms in data distribution space, and conducts empirical analysis on various environments and algorithms."
}