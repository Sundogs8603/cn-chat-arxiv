{
    "title": "On Leveraging Encoder-only Pre-trained Language Models for Effective Keyphrase Generation",
    "abstract": "arXiv:2402.14052v1 Announce Type: new  Abstract: This study addresses the application of encoder-only Pre-trained Language Models (PLMs) in keyphrase generation (KPG) amidst the broader availability of domain-tailored encoder-only models compared to encoder-decoder models. We investigate three core inquiries: (1) the efficacy of encoder-only PLMs in KPG, (2) optimal architectural decisions for employing encoder-only PLMs in KPG, and (3) a performance comparison between in-domain encoder-only and encoder-decoder PLMs across varied resource settings. Our findings, derived from extensive experimentation in two domains reveal that with encoder-only PLMs, although KPE with Conditional Random Fields slightly excels in identifying present keyphrases, the KPG formulation renders a broader spectrum of keyphrase predictions. Additionally, prefix-LM fine-tuning of encoder-only PLMs emerges as a strong and data-efficient strategy for KPG, outperforming general-domain seq2seq PLMs. We also identify",
    "link": "https://arxiv.org/abs/2402.14052",
    "context": "Title: On Leveraging Encoder-only Pre-trained Language Models for Effective Keyphrase Generation\nAbstract: arXiv:2402.14052v1 Announce Type: new  Abstract: This study addresses the application of encoder-only Pre-trained Language Models (PLMs) in keyphrase generation (KPG) amidst the broader availability of domain-tailored encoder-only models compared to encoder-decoder models. We investigate three core inquiries: (1) the efficacy of encoder-only PLMs in KPG, (2) optimal architectural decisions for employing encoder-only PLMs in KPG, and (3) a performance comparison between in-domain encoder-only and encoder-decoder PLMs across varied resource settings. Our findings, derived from extensive experimentation in two domains reveal that with encoder-only PLMs, although KPE with Conditional Random Fields slightly excels in identifying present keyphrases, the KPG formulation renders a broader spectrum of keyphrase predictions. Additionally, prefix-LM fine-tuning of encoder-only PLMs emerges as a strong and data-efficient strategy for KPG, outperforming general-domain seq2seq PLMs. We also identify",
    "path": "papers/24/02/2402.14052.json",
    "total_tokens": 922,
    "translated_title": "利用仅编码器预训练语言模型进行有效关键短语生成",
    "translated_abstract": "本研究探讨了在领域特定编码器模型相比编码器-解码器模型更广泛可用的情况下，将仅编码器预训练语言模型（PLMs）应用于关键短语生成（KPG）。我们研究了三个核心问题：（1）编码器-only PLMs在KPG中的功效，（2）在KPG中使用编码器-only PLMs的最佳架构决策，以及（3）不同资源设置下领域内编码器-only和编码器-解码器PLMs之间的性能比较。我们的研究结果，通过在两个领域进行广泛实验得出，表明尽管带有条件随机场的KPE在识别当前关键短语方面稍微优于仅编码器PLMs，但KPG公式提供了更广泛的关键短语预测。此外，仅编码器PLMs的前缀LM微调出现为KPG的强大且高效策略，优于通用领域的seq2seq PLMs。我们还确定",
    "tldr": "本研究探讨了在关键短语生成中利用仅编码器预训练语言模型的效果和优化策略，并比较了不同资源设置下领域内编码器-only和编码器-解码器模型的性能。",
    "en_tdlr": "This study investigates the effectiveness and optimization strategies of leveraging encoder-only pre-trained language models in keyphrase generation, and compares the performance of domain-tailored encoder-only models and encoder-decoder models under different resource settings."
}