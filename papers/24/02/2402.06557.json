{
    "title": "The Quantified Boolean Bayesian Network: Theory and Experiments with a Logical Graphical Model",
    "abstract": "This paper introduces the Quantified Boolean Bayesian Network (QBBN), which provides a unified view of logical and probabilistic reasoning. The QBBN is meant to address a central problem with the Large Language Model (LLM), which has become extremely popular in Information Retrieval, which is that the LLM hallucinates. A Bayesian Network, by construction, cannot hallucinate, because it can only return answers that it can explain. We show how a Bayesian Network over an unbounded number of boolean variables can be configured to represent the logical reasoning underlying human language. We do this by creating a key-value version of the First-Order Calculus, for which we can prove consistency and completeness. We show that the model is trivially trained over fully observed data, but that inference is non-trivial. Exact inference in a Bayesian Network is intractable (i.e. $\\Omega(2^N)$ for $N$ variables). For inference, we investigate the use of Loopy Belief Propagation (LBP), which is not ",
    "link": "https://arxiv.org/abs/2402.06557",
    "context": "Title: The Quantified Boolean Bayesian Network: Theory and Experiments with a Logical Graphical Model\nAbstract: This paper introduces the Quantified Boolean Bayesian Network (QBBN), which provides a unified view of logical and probabilistic reasoning. The QBBN is meant to address a central problem with the Large Language Model (LLM), which has become extremely popular in Information Retrieval, which is that the LLM hallucinates. A Bayesian Network, by construction, cannot hallucinate, because it can only return answers that it can explain. We show how a Bayesian Network over an unbounded number of boolean variables can be configured to represent the logical reasoning underlying human language. We do this by creating a key-value version of the First-Order Calculus, for which we can prove consistency and completeness. We show that the model is trivially trained over fully observed data, but that inference is non-trivial. Exact inference in a Bayesian Network is intractable (i.e. $\\Omega(2^N)$ for $N$ variables). For inference, we investigate the use of Loopy Belief Propagation (LBP), which is not ",
    "path": "papers/24/02/2402.06557.json",
    "total_tokens": 922,
    "translated_title": "量化布尔贝叶斯网络：逻辑图模型的理论和实验",
    "translated_abstract": "本文介绍了量化布尔贝叶斯网络（QBBN），它提供了逻辑和概率推理的统一视角。QBBN旨在解决大型语言模型（LLM）的一个核心问题，即LLM会出现妄想现象。由于贝叶斯网络的构建方式，它无法产生妄想，因为它只能返回可以解释的答案。我们展示了如何配置一个含有无限数量布尔变量的贝叶斯网络来表示人类语言背后的逻辑推理。我们通过创建一种键-值版本的一阶演算法来实现这一点，我们可以证明其一致性和完备性。我们展示了该模型在完全观测数据上是易于训练的，但推理是非平凡的。贝叶斯网络的精确推理是不可解的（即$N$个变量的推理复杂度为$\\Omega(2^N)$）。对于推理，我们研究了循环信念传播（LBP）的使用，它并不...",
    "tldr": "本文介绍了量化布尔贝叶斯网络（QBBN），它提供了逻辑和概率推理的统一视角，并解决了大型语言模型（LLM）的妄想问题。通过创建一阶演算法的键值版本，QBBN能够表示人类语言背后的逻辑推理。精确推理是不可解的，但可以使用循环信念传播（LBP）进行推理。"
}