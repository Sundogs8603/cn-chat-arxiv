{
    "title": "Vision Language Model-based Caption Evaluation Method Leveraging Visual Context Extraction",
    "abstract": "arXiv:2402.17969v1 Announce Type: cross  Abstract: Given the accelerating progress of vision and language modeling, accurate evaluation of machine-generated image captions remains critical. In order to evaluate captions more closely to human preferences, metrics need to discriminate between captions of varying quality and content. However, conventional metrics fail short of comparing beyond superficial matches of words or embedding similarities; thus, they still need improvement. This paper presents VisCE$^2$, a vision language model-based caption evaluation method. Our method focuses on visual context, which refers to the detailed content of images, including objects, attributes, and relationships. By extracting and organizing them into a structured format, we replace the human-written references with visual contexts and help VLMs better understand the image, enhancing evaluation performance. Through meta-evaluation on multiple datasets, we validated that VisCE$^2$ outperforms the con",
    "link": "https://arxiv.org/abs/2402.17969",
    "context": "Title: Vision Language Model-based Caption Evaluation Method Leveraging Visual Context Extraction\nAbstract: arXiv:2402.17969v1 Announce Type: cross  Abstract: Given the accelerating progress of vision and language modeling, accurate evaluation of machine-generated image captions remains critical. In order to evaluate captions more closely to human preferences, metrics need to discriminate between captions of varying quality and content. However, conventional metrics fail short of comparing beyond superficial matches of words or embedding similarities; thus, they still need improvement. This paper presents VisCE$^2$, a vision language model-based caption evaluation method. Our method focuses on visual context, which refers to the detailed content of images, including objects, attributes, and relationships. By extracting and organizing them into a structured format, we replace the human-written references with visual contexts and help VLMs better understand the image, enhancing evaluation performance. Through meta-evaluation on multiple datasets, we validated that VisCE$^2$ outperforms the con",
    "path": "papers/24/02/2402.17969.json",
    "total_tokens": 827,
    "translated_title": "基于视觉语言模型的图像标题评估方法，利用视觉上下文提取",
    "translated_abstract": "鉴于视觉和语言建模的加速进展，准确评估机器生成的图像标题仍然至关重要。为了更贴近人类偏好地评估标题，度量标准需要区分不同质量和内容的标题。然而，传统度量标准仅限于比较单词或嵌入相似性的表面匹配，因此仍有改进的空间。本文提出了基于视觉语言模型的标题评估方法VisCE$^2$。我们的方法侧重于视觉上下文，即图像的详细内容，包括对象、属性和关系。通过提取并组织成结构化格式，我们用视觉上下文替换人工编写的参考文献，帮助视觉语言模型更好地理解图像，提升评估性能。通过在多个数据集上的元评估，我们验证了VisCE$^2$优于传統方法。",
    "tldr": "该论文提出了一种基于视觉语言模型的图像标题评估方法，通过提取和组织视觉上下文来替代人工参考文献，从而提升图像标题的评估性能。",
    "en_tdlr": "This paper introduces a vision language model-based caption evaluation method that enhances evaluation performance by extracting and organizing visual context to replace human-written references."
}