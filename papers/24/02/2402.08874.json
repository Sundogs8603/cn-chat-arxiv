{
    "title": "Tree-Based Hard Attention with Self-Motivation for Large Language Models",
    "abstract": "arXiv:2402.08874v1 Announce Type: new Abstract: While large language models (LLMs) excel at understanding and generating plain text, they are not specifically tailored to handle hierarchical text structures. Extracting the task-desired property from their natural language responses typically necessitates additional processing steps. In fact, selectively comprehending the hierarchical structure of large-scale text is pivotal to understanding its substance. Aligning LLMs more closely with the classification or regression values of specific task through prompting also remains challenging. To this end, we propose a novel framework called Tree-Based Hard Attention with Self-Motivation for Large Language Models (TEAROOM). TEAROOM incorporates a tree-based hard attention mechanism for LLMs to process hierarchically structured text inputs. By leveraging prompting, it enables a frozen LLM to selectively focus on relevant leaves in relation to the root, generating a tailored symbolic representat",
    "link": "https://arxiv.org/abs/2402.08874",
    "context": "Title: Tree-Based Hard Attention with Self-Motivation for Large Language Models\nAbstract: arXiv:2402.08874v1 Announce Type: new Abstract: While large language models (LLMs) excel at understanding and generating plain text, they are not specifically tailored to handle hierarchical text structures. Extracting the task-desired property from their natural language responses typically necessitates additional processing steps. In fact, selectively comprehending the hierarchical structure of large-scale text is pivotal to understanding its substance. Aligning LLMs more closely with the classification or regression values of specific task through prompting also remains challenging. To this end, we propose a novel framework called Tree-Based Hard Attention with Self-Motivation for Large Language Models (TEAROOM). TEAROOM incorporates a tree-based hard attention mechanism for LLMs to process hierarchically structured text inputs. By leveraging prompting, it enables a frozen LLM to selectively focus on relevant leaves in relation to the root, generating a tailored symbolic representat",
    "path": "papers/24/02/2402.08874.json",
    "total_tokens": 850,
    "translated_title": "基于树状硬注意力和自我激励的大型语言模型",
    "translated_abstract": "虽然大型语言模型在理解和生成纯文本方面表现出色，但它们并没有专门设计来处理分层文本结构。从它们的自然语言回复中提取任务所需的属性通常需要额外的处理步骤。事实上，选择性地理解大规模文本的层次结构对于理解其实质至关重要。通过提示将LLM与特定任务的分类或回归值更紧密地对齐也仍然具有挑战性。为此，我们提出了一种新颖的框架，称为Tree-Based Hard Attention with Self-Motivation for Large Language Models（TEAROOM）。TEAROOM将树状硬注意力机制纳入LLM中，以处理分层结构的文本输入。通过利用提示机制，它使冻结的LLM能够选择性地关注与根节点相关的叶子节点，生成一个定制的符号表示。",
    "tldr": "提出了一种名为TEAROOM的框架，该框架采用基于树状硬注意力和自我激励的机制，用于处理大型语言模型中的分层文本输入，并通过提示机制使模型能够选择性地关注与特定任务相关的叶子节点。"
}