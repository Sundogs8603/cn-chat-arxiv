{
    "title": "Dynamic Byzantine-Robust Learning: Adapting to Switching Byzantine Workers",
    "abstract": "Byzantine-robust learning has emerged as a prominent fault-tolerant distributed machine learning framework. However, most techniques consider the static setting, wherein the identity of Byzantine machines remains fixed during the learning process. This assumption does not capture real-world dynamic Byzantine behaviors, which may include transient malfunctions or targeted temporal attacks. Addressing this limitation, we propose $\\textsf{DynaBRO}$ -- a new method capable of withstanding $\\mathcal{O}(\\sqrt{T})$ rounds of Byzantine identity alterations (where $T$ is the total number of training rounds), while matching the asymptotic convergence rate of the static setting. Our method combines a multi-level Monte Carlo (MLMC) gradient estimation technique with robust aggregation of worker updates and incorporates a fail-safe filter to limit bias from dynamic Byzantine strategies. Additionally, by leveraging an adaptive learning rate, our approach eliminates the need for knowing the percentag",
    "link": "https://arxiv.org/abs/2402.02951",
    "context": "Title: Dynamic Byzantine-Robust Learning: Adapting to Switching Byzantine Workers\nAbstract: Byzantine-robust learning has emerged as a prominent fault-tolerant distributed machine learning framework. However, most techniques consider the static setting, wherein the identity of Byzantine machines remains fixed during the learning process. This assumption does not capture real-world dynamic Byzantine behaviors, which may include transient malfunctions or targeted temporal attacks. Addressing this limitation, we propose $\\textsf{DynaBRO}$ -- a new method capable of withstanding $\\mathcal{O}(\\sqrt{T})$ rounds of Byzantine identity alterations (where $T$ is the total number of training rounds), while matching the asymptotic convergence rate of the static setting. Our method combines a multi-level Monte Carlo (MLMC) gradient estimation technique with robust aggregation of worker updates and incorporates a fail-safe filter to limit bias from dynamic Byzantine strategies. Additionally, by leveraging an adaptive learning rate, our approach eliminates the need for knowing the percentag",
    "path": "papers/24/02/2402.02951.json",
    "total_tokens": 1125,
    "translated_title": "动态拜占庭-强鲁棒学习：适应切换拜占庭工作机制",
    "translated_abstract": "拜占庭-强鲁棒学习作为一种突出的容错分布式机器学习框架已经出现。然而，大多数技术考虑的是静态情况，其中在学习过程中拜占庭机器的身份保持不变。这种假设不能捕捉到现实世界中的动态拜占庭行为，可能包括短暂故障或有针对性的时间攻击。为了解决这个限制，我们提出了一种新的方法$\\textsf{DynaBRO}$，它能够经受住$\\mathcal{O}(\\sqrt{T})$轮拜占庭身份的改变（其中$T$是总训练轮数），同时与静态情况下的渐近收敛速率相匹配。我们的方法将多级蒙特卡洛（MLMC）渐变估计技术与工作机制更新的强鲁棒聚合相结合，并引入了一个故障安全过滤器来限制动态拜占庭策略的偏差。此外，通过利用自适应学习率，我们的方法消除了对百分比的需求。",
    "tldr": "$\\textsf{DynaBRO}$是一种动态拜占庭-强鲁棒学习的方法，能够适应切换拜占庭工作机制，并且在渐近收敛速率上与静态情况相匹配。通过多级蒙特卡洛渐变估计技术、强鲁棒工作机制更新的聚合和故障安全过滤器的引入，我们的方法能够经受住$\\mathcal{O}(\\sqrt{T})$轮拜占庭身份的改变。另外，通过使用自适应学习率，我们的方法消除了对百分比的需求。",
    "en_tdlr": "$\\textsf{DynaBRO}$ is a dynamic Byzantine-robust learning method that adapts to switching Byzantine workers and matches the asymptotic convergence rate of static setting. By combining multi-level Monte Carlo gradient estimation technique, robust aggregation of worker updates, and a fail-safe filter, our method can withstand $\\mathcal{O}(\\sqrt{T})$ rounds of Byzantine identity alterations. Additionally, the use of adaptive learning rate eliminates the need for percentage knowledge."
}