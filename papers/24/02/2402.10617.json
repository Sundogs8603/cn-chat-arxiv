{
    "title": "Multitask Kernel-based Learning with Logic Constraints",
    "abstract": "arXiv:2402.10617v1 Announce Type: cross  Abstract: This paper presents a general framework to integrate prior knowledge in the form of logic constraints among a set of task functions into kernel machines. The logic propositions provide a partial representation of the environment, in which the learner operates, that is exploited by the learning algorithm together with the information available in the supervised examples. In particular, we consider a multi-task learning scheme, where multiple unary predicates on the feature space are to be learned by kernel machines and a higher level abstract representation consists of logic clauses on these predicates, known to hold for any input. A general approach is presented to convert the logic clauses into a continuous implementation, that processes the outputs computed by the kernel-based predicates. The learning task is formulated as a primal optimization problem of a loss function that combines a term measuring the fitting of the supervised ex",
    "link": "https://arxiv.org/abs/2402.10617",
    "context": "Title: Multitask Kernel-based Learning with Logic Constraints\nAbstract: arXiv:2402.10617v1 Announce Type: cross  Abstract: This paper presents a general framework to integrate prior knowledge in the form of logic constraints among a set of task functions into kernel machines. The logic propositions provide a partial representation of the environment, in which the learner operates, that is exploited by the learning algorithm together with the information available in the supervised examples. In particular, we consider a multi-task learning scheme, where multiple unary predicates on the feature space are to be learned by kernel machines and a higher level abstract representation consists of logic clauses on these predicates, known to hold for any input. A general approach is presented to convert the logic clauses into a continuous implementation, that processes the outputs computed by the kernel-based predicates. The learning task is formulated as a primal optimization problem of a loss function that combines a term measuring the fitting of the supervised ex",
    "path": "papers/24/02/2402.10617.json",
    "total_tokens": 755,
    "translated_title": "带有逻辑约束的多任务基于核心学习",
    "translated_abstract": "本文提出了一个通用框架，将逻辑约束形式的先验知识整合到核心机器中的一组任务函数中。逻辑命题提供了环境的部分表示，学习算法利用它与监督样本中可用的信息。我们考虑了一个多任务学习方案，其中多个特征空间上的一元谓词要由核心机器学习，高级抽象表示由这些谓词的逻辑子句组成，已知对于任何输入都成立。我们提出了一种通用方法，将逻辑子句转换为连续实现，处理核心谓词计算的输出。学习任务被制定为损失函数的原始优化问题，其结合了测量监督样本拟合度的项",
    "tldr": "将逻辑约束融合到多任务核心学习中，提出了一种通用方法来转换逻辑陈述为连续实现，以实现核心谓词计算输出。",
    "en_tdlr": "Integrating logic constraints into multitask kernel learning, a general approach is proposed to convert logic propositions into continuous implementations for computing kernel predicate outputs."
}