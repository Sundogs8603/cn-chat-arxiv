{
    "title": "Towards Safer Large Language Models through Machine Unlearning",
    "abstract": "arXiv:2402.10058v1 Announce Type: new  Abstract: The rapid advancement of Large Language Models (LLMs) has demonstrated their vast potential across various domains, attributed to their extensive pretraining knowledge and exceptional generalizability. However, LLMs often encounter challenges in generating harmful content when faced with problematic prompts. To address this problem, existing work attempted to implement a gradient ascent based approach to prevent LLMs from producing harmful output. While these methods can be effective, they frequently impact the model utility in responding to normal prompts. To address this gap, we introduce Selective Knowledge negation Unlearning (SKU), a novel unlearning framework for LLMs, designed to eliminate harmful knowledge while preserving utility on normal prompts. Specifically, SKU is consisted of two stages: harmful knowledge acquisition stage and knowledge negation stage. The first stage aims to identify and acquire harmful knowledge within t",
    "link": "https://arxiv.org/abs/2402.10058",
    "context": "Title: Towards Safer Large Language Models through Machine Unlearning\nAbstract: arXiv:2402.10058v1 Announce Type: new  Abstract: The rapid advancement of Large Language Models (LLMs) has demonstrated their vast potential across various domains, attributed to their extensive pretraining knowledge and exceptional generalizability. However, LLMs often encounter challenges in generating harmful content when faced with problematic prompts. To address this problem, existing work attempted to implement a gradient ascent based approach to prevent LLMs from producing harmful output. While these methods can be effective, they frequently impact the model utility in responding to normal prompts. To address this gap, we introduce Selective Knowledge negation Unlearning (SKU), a novel unlearning framework for LLMs, designed to eliminate harmful knowledge while preserving utility on normal prompts. Specifically, SKU is consisted of two stages: harmful knowledge acquisition stage and knowledge negation stage. The first stage aims to identify and acquire harmful knowledge within t",
    "path": "papers/24/02/2402.10058.json",
    "total_tokens": 841,
    "translated_title": "通过机器遗忘实现更安全的大型语言模型",
    "translated_abstract": "大型语言模型（LLM）的快速发展展示了它们在各个领域的巨大潜力，归因于它们广泛的预训练知识和出色的泛化能力。然而，当LLM面对有问题的提示时，经常会遇到生成有害内容的挑战。为了解决这个问题，现有的工作尝试通过梯度上升方法阻止LLM产生有害输出。虽然这些方法可能有效，但它们经常影响模型对正常提示的实用性。为了解决这一差距，我们引入了选择性知识否认遗忘（SKU），这是一种新颖的针对LLM的遗忘框架，旨在消除有害知识，同时保留对正常提示的实用性。具体而言，SKU由两个阶段组成：有害知识获取阶段和知识否定阶段。第一个阶段旨在识别和获取有害知识。",
    "tldr": "这项研究提出了一种新的大型语言模型遗忘框架SKU，旨在消除有害知识的同时保留模型对正常提示的实用性。"
}