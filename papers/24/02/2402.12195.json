{
    "title": "Browse and Concentrate: Comprehending Multimodal Content via prior-LLM Context Fusion",
    "abstract": "arXiv:2402.12195v1 Announce Type: new  Abstract: With the bloom of Large Language Models (LLMs), Multimodal Large Language Models (MLLMs) that incorporate LLMs with pre-trained vision models have recently demonstrated impressive performance across diverse vision-language tasks. However, they fall short to comprehend context involving multiple images. A primary reason for this shortcoming is that the visual features for each images are encoded individually by frozen encoders before feeding into the LLM backbone, lacking awareness of other images and the multimodal instructions. We term this issue as prior-LLM modality isolation and propose a two phase paradigm, browse-and-concentrate, to enable in-depth multimodal context fusion prior to feeding the features into LLMs. This paradigm initially \"browses\" through the inputs for essential insights, and then revisits the inputs to \"concentrate\" on crucial details, guided by these insights, to achieve a more comprehensive understanding of the",
    "link": "https://arxiv.org/abs/2402.12195",
    "context": "Title: Browse and Concentrate: Comprehending Multimodal Content via prior-LLM Context Fusion\nAbstract: arXiv:2402.12195v1 Announce Type: new  Abstract: With the bloom of Large Language Models (LLMs), Multimodal Large Language Models (MLLMs) that incorporate LLMs with pre-trained vision models have recently demonstrated impressive performance across diverse vision-language tasks. However, they fall short to comprehend context involving multiple images. A primary reason for this shortcoming is that the visual features for each images are encoded individually by frozen encoders before feeding into the LLM backbone, lacking awareness of other images and the multimodal instructions. We term this issue as prior-LLM modality isolation and propose a two phase paradigm, browse-and-concentrate, to enable in-depth multimodal context fusion prior to feeding the features into LLMs. This paradigm initially \"browses\" through the inputs for essential insights, and then revisits the inputs to \"concentrate\" on crucial details, guided by these insights, to achieve a more comprehensive understanding of the",
    "path": "papers/24/02/2402.12195.json",
    "total_tokens": 910,
    "translated_title": "通过 prior-LLM 上下文融合来理解多模态内容",
    "translated_abstract": "随着大型语言模型（LLMs）的兴起，近期将LLMs与预训练的视觉模型相结合的多模态大型语言模型（MLLMs）已经展现出在各种视觉语言任务上令人印象深刻的性能。然而，它们在理解涉及多张图片的上下文方面仍有不足。这一缺陷的主要原因是，在将视觉特征输入LLM主干之前，每张图片的视觉特征都是由冻结的编码器单独编码的，缺乏对其他图片和多模态指令的意识。我们将这一问题称为 prior-LLM 模态隔离，并提出了一个两阶段范式，即“浏览和集中”，以实现在将特征输入LLMs之前进行深入的多模态上下文融合。这种范式最初“浏览”输入以获取关键见解，然后再次回顾输入“集中”于关键细节，通过这些见解的指导，从而实现对多模态内容更全面的理解。",
    "tldr": "提出了一个两阶段范式\"浏览和集中\"，通过在将特征输入LLMs之前进行深入的多模态上下文融合，解决了多模态内容理解中的 prior-LLM 模态隔离问题",
    "en_tdlr": "Introduced a two-phase paradigm \"browse and concentrate\" to address the modality isolation issue in multimodal content comprehension by enabling in-depth multimodal context fusion before feeding the features into LLMs."
}