{
    "title": "MoELoRA: Contrastive Learning Guided Mixture of Experts on Parameter-Efficient Fine-Tuning for Large Language Models",
    "abstract": "arXiv:2402.12851v1 Announce Type: new  Abstract: Fine-tuning is often necessary to enhance the adaptability of Large Language Models (LLM) to downstream tasks. Nonetheless, the process of updating billions of parameters demands significant computational resources and training time, which poses a substantial obstacle to the widespread application of large-scale models in various scenarios. To address this issue, Parameter-Efficient Fine-Tuning (PEFT) has emerged as a prominent paradigm in recent research. However, current PEFT approaches that employ a limited set of global parameters (such as LoRA, which adds low-rank approximation matrices to all weights) face challenges in flexibly combining different computational modules in downstream tasks. In this work, we introduce a novel PEFT method: MoELoRA. We consider LoRA as Mixture of Experts (MoE), and to mitigate the random routing phenomenon observed in MoE, we propose the utilization of contrastive learning to encourage experts to lear",
    "link": "https://arxiv.org/abs/2402.12851",
    "context": "Title: MoELoRA: Contrastive Learning Guided Mixture of Experts on Parameter-Efficient Fine-Tuning for Large Language Models\nAbstract: arXiv:2402.12851v1 Announce Type: new  Abstract: Fine-tuning is often necessary to enhance the adaptability of Large Language Models (LLM) to downstream tasks. Nonetheless, the process of updating billions of parameters demands significant computational resources and training time, which poses a substantial obstacle to the widespread application of large-scale models in various scenarios. To address this issue, Parameter-Efficient Fine-Tuning (PEFT) has emerged as a prominent paradigm in recent research. However, current PEFT approaches that employ a limited set of global parameters (such as LoRA, which adds low-rank approximation matrices to all weights) face challenges in flexibly combining different computational modules in downstream tasks. In this work, we introduce a novel PEFT method: MoELoRA. We consider LoRA as Mixture of Experts (MoE), and to mitigate the random routing phenomenon observed in MoE, we propose the utilization of contrastive learning to encourage experts to lear",
    "path": "papers/24/02/2402.12851.json",
    "total_tokens": 869,
    "translated_title": "MoELoRA：对大型语言模型进行对比学习引导下的参数高效微调的混合专家模型",
    "translated_abstract": "微调经常是必要的，以增强大型语言模型（LLM）适应下游任务的能力。然而，更新数十亿参数的过程需要大量的计算资源和训练时间，这对大规模模型在各种场景中的广泛应用构成了重大障碍。为了解决这一问题，最近的研究中出现了参数高效微调（PEFT）作为一个杰出的范式。然而，目前采用有限全局参数集的PEFT方法（如LoRA，将低秩逼近矩阵添加到所有权重）在灵活组合下游任务中的不同计算模块方面面临挑战。在本研究中，我们引入了一种新的PEFT方法：MoELoRA。我们将LoRA视为专家混合（MoE），为了减轻MoE中观察到的随机路由现象，我们提出了利用对比学习来鼓励专家学习。",
    "tldr": "提出了一种利用对比学习引导的混合专家模型MoELoRA解决大型语言模型进行参数高效微调时的灵活组合挑战"
}