{
    "title": "Evaluating the Factuality of Zero-shot Summarizers Across Varied Domains",
    "abstract": "Recent work has shown that large language models (LLMs) are capable of generating summaries zero-shot (i.e., without explicit supervision) that, under human assessment, are often comparable or even preferred to manually composed reference summaries. However, this prior work has focussed almost exclusively on evaluating news article summarization. How do zero-shot summarizers perform in other (potentially more specialized) domains? In this work we evaluate zero-shot generated summaries across specialized domains including biomedical articles, and legal bills (in addition to standard news benchmarks for reference). We focus especially on the factuality of outputs. We acquire annotations from domain experts to identify inconsistencies in summaries and systematically categorize these errors. We analyze whether the prevalence of a given domain in the pretraining corpus affects extractiveness and faithfulness of generated summaries of articles in this domain. We release all collected annotat",
    "link": "https://arxiv.org/abs/2402.03509",
    "context": "Title: Evaluating the Factuality of Zero-shot Summarizers Across Varied Domains\nAbstract: Recent work has shown that large language models (LLMs) are capable of generating summaries zero-shot (i.e., without explicit supervision) that, under human assessment, are often comparable or even preferred to manually composed reference summaries. However, this prior work has focussed almost exclusively on evaluating news article summarization. How do zero-shot summarizers perform in other (potentially more specialized) domains? In this work we evaluate zero-shot generated summaries across specialized domains including biomedical articles, and legal bills (in addition to standard news benchmarks for reference). We focus especially on the factuality of outputs. We acquire annotations from domain experts to identify inconsistencies in summaries and systematically categorize these errors. We analyze whether the prevalence of a given domain in the pretraining corpus affects extractiveness and faithfulness of generated summaries of articles in this domain. We release all collected annotat",
    "path": "papers/24/02/2402.03509.json",
    "total_tokens": 921,
    "translated_title": "跨领域评估零样本摘要生成器的真实性",
    "translated_abstract": "最近的研究表明，大型语言模型(LLMs)能够零样本（即在没有明确监督的情况下）生成摘要，经过人工评估，这些摘要往往与手工编写的参考摘要相比，甚至更受欢迎。然而，这些早期的研究几乎专注于评估新闻文章的摘要。零样本摘要生成器在其他（可能更专业）领域中的表现如何？在这项工作中，我们评估了跨专业领域中零样本生成的摘要，包括生物医学文章和法律法案（除了标准新闻摘要的参考）。我们特别关注输出的真实性。我们从领域专家处获取注释，以识别摘要中的不一致之处，并对这些错误进行系统分类。我们分析了预训练语料库中给定领域的普遍性是否会影响在该领域的文章的摘要的提取和忠实度。我们发布了所有收集到的注释。",
    "tldr": "本研究跨领域评估了零样本摘要生成器的真实性。通过对生物医学文章和法律法案等专业领域进行评估，我们特别关注生成摘要的真实性，并分析了预训练语料库中给定领域的普遍性对摘要质量的影响。"
}