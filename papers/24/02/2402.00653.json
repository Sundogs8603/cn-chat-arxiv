{
    "title": "Coherent Feed Forward Quantum Neural Network",
    "abstract": "Quantum machine learning, focusing on quantum neural networks (QNNs), remains a vastly uncharted field of study. Current QNN models primarily employ variational circuits on an ansatz or a quantum feature map, often requiring multiple entanglement layers. This methodology not only increases the computational cost of the circuit beyond what is practical on near-term quantum devices but also misleadingly labels these models as neural networks, given their divergence from the structure of a typical feed-forward neural network (FFNN). Moreover, the circuit depth and qubit needs of these models scale poorly with the number of data features, resulting in an efficiency challenge for real-world machine-learning tasks. We introduce a bona fide QNN model, which seamlessly aligns with the versatility of a traditional FFNN in terms of its adaptable intermediate layers and nodes, absent from intermediate measurements such that our entire model is coherent. This model stands out with its reduced circ",
    "link": "https://arxiv.org/abs/2402.00653",
    "context": "Title: Coherent Feed Forward Quantum Neural Network\nAbstract: Quantum machine learning, focusing on quantum neural networks (QNNs), remains a vastly uncharted field of study. Current QNN models primarily employ variational circuits on an ansatz or a quantum feature map, often requiring multiple entanglement layers. This methodology not only increases the computational cost of the circuit beyond what is practical on near-term quantum devices but also misleadingly labels these models as neural networks, given their divergence from the structure of a typical feed-forward neural network (FFNN). Moreover, the circuit depth and qubit needs of these models scale poorly with the number of data features, resulting in an efficiency challenge for real-world machine-learning tasks. We introduce a bona fide QNN model, which seamlessly aligns with the versatility of a traditional FFNN in terms of its adaptable intermediate layers and nodes, absent from intermediate measurements such that our entire model is coherent. This model stands out with its reduced circ",
    "path": "papers/24/02/2402.00653.json",
    "total_tokens": 887,
    "translated_title": "相干前馈量子神经网络",
    "translated_abstract": "量子机器学习，特别是量子神经网络(QNNs)，仍然是一个广阔的未知领域。目前的QNN模型主要采用变分电路或量子特征映射，在ansatz或者量子特征图上进行，通常需要多个纠缠层。这种方法不仅增加了电路的计算成本，超出了在近期量子设备上的实际可行性，而且由于与典型的前馈神经网络(FFNN)结构的偏离，误导性地将这些模型标记为神经网络。此外，这些模型的电路深度和量子比特需求在数据特征数量增加时扩展性较差，从而对真实世界的机器学习任务造成了效率挑战。我们引入了一个地道的QNN模型，它在可调整的中间层和节点上与传统的FFNN相协调，在没有中间测量的情况下，整个模型是相干的。该模型以其减少的电路深度和量子比特需求脱颖而出，使其在真实世界的机器学习任务中效率更高。",
    "tldr": "这项研究提出了一种相干前馈量子神经网络模型，该模型在电路深度和量子比特需求方面更为高效，能够适应真实世界的机器学习任务。",
    "en_tdlr": "This study introduces a coherent feed-forward quantum neural network model that is more efficient in terms of circuit depth and qubit requirements, and is capable of handling real-world machine learning tasks."
}