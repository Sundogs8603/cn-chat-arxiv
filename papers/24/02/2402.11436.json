{
    "title": "Perils of Self-Feedback: Self-Bias Amplifies in Large Language Models",
    "abstract": "arXiv:2402.11436v1 Announce Type: cross  Abstract: Recent studies show that self-feedback improves large language models (LLMs) on certain tasks while worsens other tasks. We discovered that such a contrary is due to LLM's bias towards their own output. In this paper, we formally define LLM's self-bias -- the tendency to favor its own generation -- using two statistics. We analyze six LLMs on translation, constrained text generation, and mathematical reasoning tasks. We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks. Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias. To mitigate such biases, we discover that larger model size and external feedback with accurate assessment can significantly reduce bias in the self-refine pipeline, leading to actual performance improvement in downstream tasks.",
    "link": "https://arxiv.org/abs/2402.11436",
    "context": "Title: Perils of Self-Feedback: Self-Bias Amplifies in Large Language Models\nAbstract: arXiv:2402.11436v1 Announce Type: cross  Abstract: Recent studies show that self-feedback improves large language models (LLMs) on certain tasks while worsens other tasks. We discovered that such a contrary is due to LLM's bias towards their own output. In this paper, we formally define LLM's self-bias -- the tendency to favor its own generation -- using two statistics. We analyze six LLMs on translation, constrained text generation, and mathematical reasoning tasks. We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks. Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias. To mitigate such biases, we discover that larger model size and external feedback with accurate assessment can significantly reduce bias in the self-refine pipeline, leading to actual performance improvement in downstream tasks.",
    "path": "papers/24/02/2402.11436.json",
    "total_tokens": 883,
    "translated_title": "自我反馈的危险：在大型语言模型中自我偏见被放大",
    "translated_abstract": "最近的研究表明，自我反馈可以改善大型语言模型在某些任务上的表现，但在其他任务上却会恶化。我们发现这种矛盾是由于大型语言模型对其自身输出的偏见。本文正式定义了大型语言模型的自我偏见——倾向于偏爱自身生成——并使用两个统计量进行了分析。我们在翻译、受限文本生成和数学推理任务上分析了六种大型语言模型。我们发现自我偏见在所有检测的大型语言模型中都普遍存在，跨多种语言和任务。我们的分析表明，虽然自我改进管道提高了模型输出的流畅性和可理解性，但它进一步放大了自我偏见。为了缓解这种偏见，我们发现更大的模型规模和具有准确评估的外部反馈可以显著减少自我改进管道中的偏见，从而实际改善下游任务的性能。",
    "tldr": "大型语言模型存在自我偏见，研究发现通过更大的模型规模和准确评估的外部反馈可以显著减少这种偏见，并提高后续任务的实际表现。",
    "en_tdlr": "Large language models exhibit self-bias, and the study reveals that increasing model size and providing accurate external feedback can significantly reduce this bias, leading to improved performance in downstream tasks."
}