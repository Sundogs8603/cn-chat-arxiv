{
    "title": "Smoothed Graph Contrastive Learning via Seamless Proximity Integration",
    "abstract": "arXiv:2402.15270v1 Announce Type: cross  Abstract: Graph contrastive learning (GCL) aligns node representations by classifying node pairs into positives and negatives using a selection process that typically relies on establishing correspondences within two augmented graphs. The conventional GCL approaches incorporate negative samples uniformly in the contrastive loss, resulting in the equal treatment negative nodes, regardless of their proximity to the true positive. In this paper, we present a Smoothed Graph Contrastive Learning model (SGCL), which leverages the geometric structure of augmented graphs to inject proximity information associated with positive/negative pairs in the contrastive loss, thus significantly regularizing the learning process. The proposed SGCL adjusts the penalties associated with node pairs in the contrastive loss by incorporating three distinct smoothing techniques that result in proximity aware positives and negatives. To enhance scalability for large-scale",
    "link": "https://arxiv.org/abs/2402.15270",
    "context": "Title: Smoothed Graph Contrastive Learning via Seamless Proximity Integration\nAbstract: arXiv:2402.15270v1 Announce Type: cross  Abstract: Graph contrastive learning (GCL) aligns node representations by classifying node pairs into positives and negatives using a selection process that typically relies on establishing correspondences within two augmented graphs. The conventional GCL approaches incorporate negative samples uniformly in the contrastive loss, resulting in the equal treatment negative nodes, regardless of their proximity to the true positive. In this paper, we present a Smoothed Graph Contrastive Learning model (SGCL), which leverages the geometric structure of augmented graphs to inject proximity information associated with positive/negative pairs in the contrastive loss, thus significantly regularizing the learning process. The proposed SGCL adjusts the penalties associated with node pairs in the contrastive loss by incorporating three distinct smoothing techniques that result in proximity aware positives and negatives. To enhance scalability for large-scale",
    "path": "papers/24/02/2402.15270.json",
    "total_tokens": 769,
    "translated_title": "通过无缝接近度整合的平滑图对比学习",
    "translated_abstract": "图对比学习（GCL）通过将节点对归类为正样本和负样本来对齐节点表示，其选择过程通常依赖于在两个增强图中建立对应关系。传统的GCL方法在对比损失中统一地融入负样本，导致负节点被平等对待，而不考虑它们与真正正样本的接近程度。本文提出了一种平滑图对比学习模型（SGCL），利用增强图的几何结构来在对比损失中注入与正负样本相关的接近度信息，从而显著规范化学习过程。所提出的SGCL通过整合三种不同的平滑技术调整对比损失中节点对的惩罚，形成了具有接近度感知的正样本和负样本。",
    "tldr": "SGCL模型通过三种不同的平滑技术调整对比损失中节点对的惩罚，从而形成具有接近度感知的正样本和负样本",
    "en_tdlr": "SGCL adjusts penalties associated with node pairs in the contrastive loss by incorporating three distinct smoothing techniques, resulting in proximity aware positives and negatives."
}