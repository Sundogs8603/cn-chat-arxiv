{
    "title": "Sleep-Like Unsupervised Replay Improves Performance when Data are Limited or Unbalanced",
    "abstract": "arXiv:2402.10956v1 Announce Type: cross  Abstract: The performance of artificial neural networks (ANNs) degrades when training data are limited or imbalanced. In contrast, the human brain can learn quickly from just a few examples. Here, we investigated the role of sleep in improving the performance of ANNs trained with limited data on the MNIST and Fashion MNIST datasets. Sleep was implemented as an unsupervised phase with local Hebbian type learning rules. We found a significant boost in accuracy after the sleep phase for models trained with limited data in the range of 0.5-10% of total MNIST or Fashion MNIST datasets. When more than 10% of the total data was used, sleep alone had a slight negative impact on performance, but this was remedied by fine-tuning on the original data. This study sheds light on a potential synaptic weight dynamics strategy employed by the brain during sleep to enhance memory performance when training data are limited or imbalanced.",
    "link": "https://arxiv.org/abs/2402.10956",
    "context": "Title: Sleep-Like Unsupervised Replay Improves Performance when Data are Limited or Unbalanced\nAbstract: arXiv:2402.10956v1 Announce Type: cross  Abstract: The performance of artificial neural networks (ANNs) degrades when training data are limited or imbalanced. In contrast, the human brain can learn quickly from just a few examples. Here, we investigated the role of sleep in improving the performance of ANNs trained with limited data on the MNIST and Fashion MNIST datasets. Sleep was implemented as an unsupervised phase with local Hebbian type learning rules. We found a significant boost in accuracy after the sleep phase for models trained with limited data in the range of 0.5-10% of total MNIST or Fashion MNIST datasets. When more than 10% of the total data was used, sleep alone had a slight negative impact on performance, but this was remedied by fine-tuning on the original data. This study sheds light on a potential synaptic weight dynamics strategy employed by the brain during sleep to enhance memory performance when training data are limited or imbalanced.",
    "path": "papers/24/02/2402.10956.json",
    "total_tokens": 912,
    "translated_title": "类似于睡眠的无监督重播在数据有限或不平衡时改善性能",
    "translated_abstract": "人工神经网络（ANNs）在训练数据有限或不平衡时性能下降。相反，人脑可以快速从少量示例中学习。本研究探讨了在MNIST和Fashion MNIST数据集上用有限数据训练的ANNs性能改善作用。睡眠被实现为具有本地Hebbian类型学习规则的无监督阶段。我们发现，在使用MNIST或Fashion MNIST数据集总量的0.5-10%有限数据训练模型后，经过睡眠阶段后准确性显著提高。当使用超过总数据量的10%时，仅睡眠对性能有轻微负面影响，但这可以通过对原始数据进行微调来纠正。该研究揭示了大脑在睡眠时利用的潜在突触权重动态策略，以增强在训练数据有限或不平衡情况下的记忆性能。",
    "tldr": "研究发现，通过类似于睡眠的无监督重播阶段，可以显著提高在有限数据情况下训练的人工神经网络的准确性，这为解决数据有限或不平衡时的性能问题提供了新方法。",
    "en_tdlr": "The study found that significant improvement in the accuracy of artificial neural networks trained with limited data can be achieved through a sleep-like unsupervised replay phase, providing a new approach to addressing performance issues when data are limited or unbalanced."
}