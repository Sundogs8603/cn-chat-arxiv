{
    "title": "AuditLLM: A Tool for Auditing Large Language Models Using Multiprobe Approach",
    "abstract": "arXiv:2402.09334v1 Announce Type: new Abstract: As Large Language Models (LLMs) gain wider adoption in various contexts, it becomes crucial to ensure they are reasonably safe, consistent, and reliable for an application at hand. This may require probing or auditing them. Probing LLMs with varied iterations of a single question could reveal potential inconsistencies in their knowledge or functionality. However, a tool for performing such audits with simple workflow and low technical threshold is lacking. In this demo, we introduce \"AuditLLM,\" a novel tool designed to evaluate the performance of various LLMs in a methodical way. AuditLLM's core functionality lies in its ability to test a given LLM by auditing it using multiple probes generated from a single question, thereby identifying any inconsistencies in the model's understanding or operation. A reasonably robust, reliable, and consistent LLM should output semantically similar responses for a question asked differently or by differe",
    "link": "https://arxiv.org/abs/2402.09334",
    "context": "Title: AuditLLM: A Tool for Auditing Large Language Models Using Multiprobe Approach\nAbstract: arXiv:2402.09334v1 Announce Type: new Abstract: As Large Language Models (LLMs) gain wider adoption in various contexts, it becomes crucial to ensure they are reasonably safe, consistent, and reliable for an application at hand. This may require probing or auditing them. Probing LLMs with varied iterations of a single question could reveal potential inconsistencies in their knowledge or functionality. However, a tool for performing such audits with simple workflow and low technical threshold is lacking. In this demo, we introduce \"AuditLLM,\" a novel tool designed to evaluate the performance of various LLMs in a methodical way. AuditLLM's core functionality lies in its ability to test a given LLM by auditing it using multiple probes generated from a single question, thereby identifying any inconsistencies in the model's understanding or operation. A reasonably robust, reliable, and consistent LLM should output semantically similar responses for a question asked differently or by differe",
    "path": "papers/24/02/2402.09334.json",
    "total_tokens": 914,
    "translated_title": "AuditLLM:一种使用多探测方法对大型语言模型进行审计的工具",
    "translated_abstract": "当大型语言模型（LLMs）在各种情境中越来越广泛地被采用时，确保它们在特定应用中是相对安全、一致和可靠的变得至关重要。这可能需要对它们进行探测或审计。通过使用多次迭代的单个问题对LLMs进行探测，可以揭示它们的知识或功能的潜在不一致性。然而，目前缺乏一种能够以简单工作流程和低技术门槛进行此类审计的工具。在这个演示中，我们介绍了一种名为\"AuditLLM\"的新颖工具，它旨在以系统的方式评估各种LLMs的性能。AuditLLM的核心功能在于能够通过使用从单个问题生成的多个探测来对给定的LLM进行审计，从而识别模型在理解或操作方面的任何不一致性。一个相对健壮、可靠和一致的LLM应该对以不同方式或由不同人提出的问题给出语义相似的回答。",
    "tldr": "\"AuditLLM\"是一种能够以系统的方式评估各种LLMs的性能的工具，通过使用从单个问题生成的多个探测来对给定的LLM进行审计，从而识别模型在理解或操作方面的任何不一致性。",
    "en_tdlr": "\"AuditLLM\" is a tool that systematically evaluates the performance of various LLMs by auditing them using multiple probes generated from a single question, thereby identifying any inconsistencies in the model's understanding or operation."
}