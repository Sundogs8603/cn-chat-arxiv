{
    "title": "Robust Training of Temporal GNNs using Nearest Neighbours based Hard Negatives",
    "abstract": "arXiv:2402.09239v1 Announce Type: new Abstract: Temporal graph neural networks Tgnn have exhibited state-of-art performance in future-link prediction tasks. Training of these TGNNs is enumerated by uniform random sampling based unsupervised loss. During training, in the context of a positive example, the loss is computed over uninformative negatives, which introduces redundancy and sub-optimal performance. In this paper, we propose modified unsupervised learning of Tgnn, by replacing the uniform negative sampling with importance-based negative sampling. We theoretically motivate and define the dynamically computed distribution for a sampling of negative examples. Finally, using empirical evaluations over three real-world datasets, we show that Tgnn trained using loss based on proposed negative sampling provides consistent superior performance.",
    "link": "https://arxiv.org/abs/2402.09239",
    "context": "Title: Robust Training of Temporal GNNs using Nearest Neighbours based Hard Negatives\nAbstract: arXiv:2402.09239v1 Announce Type: new Abstract: Temporal graph neural networks Tgnn have exhibited state-of-art performance in future-link prediction tasks. Training of these TGNNs is enumerated by uniform random sampling based unsupervised loss. During training, in the context of a positive example, the loss is computed over uninformative negatives, which introduces redundancy and sub-optimal performance. In this paper, we propose modified unsupervised learning of Tgnn, by replacing the uniform negative sampling with importance-based negative sampling. We theoretically motivate and define the dynamically computed distribution for a sampling of negative examples. Finally, using empirical evaluations over three real-world datasets, we show that Tgnn trained using loss based on proposed negative sampling provides consistent superior performance.",
    "path": "papers/24/02/2402.09239.json",
    "total_tokens": 769,
    "translated_title": "使用基于最近邻的硬负样本的鲁棒性训练时间GNN",
    "translated_abstract": "时间图神经网络(TGNNS)在未来链接预测任务中表现出最先进的性能。这些TGNNS的训练是通过均匀随机抽样的无监督损失进行列举的。在训练过程中，对于正例情况，损失是在无信息的负样本上计算的，这引入了冗余和次优的性能。在本文中，我们提出了改进的TGNNS无监督学习，通过使用基于重要性的负样本抽样来替换均匀负样本抽样。我们从理论上对负例采样的动态计算分布进行了理论验证和定义。最后，通过对三个真实世界数据集进行实证评估，我们展示了使用基于提出的负样本抽样的损失训练的TGNNS提供了一致的优越性能。",
    "tldr": "本研究提出了使用基于重要性的负样本抽样训练TGNNS的方法，并通过实证评估证明了其优越性能。"
}