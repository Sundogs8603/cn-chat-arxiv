{
    "title": "Single-Reset Divide & Conquer Imitation Learning",
    "abstract": "arXiv:2402.09355v1 Announce Type: cross Abstract: Demonstrations are commonly used to speed up the learning process of Deep Reinforcement Learning algorithms. To cope with the difficulty of accessing multiple demonstrations, some algorithms have been developed to learn from a single demonstration. In particular, the Divide & Conquer Imitation Learning algorithms leverage a sequential bias to learn a control policy for complex robotic tasks using a single state-based demonstration. The latest version, DCIL-II demonstrates remarkable sample efficiency. This novel method operates within an extended Goal-Conditioned Reinforcement Learning framework, ensuring compatibility between intermediate and subsequent goals extracted from the demonstration. However, a fundamental limitation arises from the assumption that the system can be reset to specific states along the demonstrated trajectory, confining the application to simulated systems. In response, we introduce an extension called Single-Re",
    "link": "https://arxiv.org/abs/2402.09355",
    "context": "Title: Single-Reset Divide & Conquer Imitation Learning\nAbstract: arXiv:2402.09355v1 Announce Type: cross Abstract: Demonstrations are commonly used to speed up the learning process of Deep Reinforcement Learning algorithms. To cope with the difficulty of accessing multiple demonstrations, some algorithms have been developed to learn from a single demonstration. In particular, the Divide & Conquer Imitation Learning algorithms leverage a sequential bias to learn a control policy for complex robotic tasks using a single state-based demonstration. The latest version, DCIL-II demonstrates remarkable sample efficiency. This novel method operates within an extended Goal-Conditioned Reinforcement Learning framework, ensuring compatibility between intermediate and subsequent goals extracted from the demonstration. However, a fundamental limitation arises from the assumption that the system can be reset to specific states along the demonstrated trajectory, confining the application to simulated systems. In response, we introduce an extension called Single-Re",
    "path": "papers/24/02/2402.09355.json",
    "total_tokens": 804,
    "translated_title": "单重置的分而治之模仿学习",
    "translated_abstract": "演示通常用于加快深度强化学习算法的学习过程。为了应对访问多个演示的困难，一些算法已经被开发出来从单个演示中学习。其中，分而治之模仿学习算法利用顺序偏差来学习一个控制策略，用于复杂机器人任务的单状态演示。最新版本DCIL-II展示了显著的样本效率。这种新方法在扩展的目标条件强化学习框架内运作，确保了从演示中提取的中间目标和后续目标之间的兼容性。然而，一个基本的限制是该系统可以被重置到演示轨迹上的特定状态，将应用限制在模拟系统中。为了解决这个问题，我们引入了一种名为单重置的扩展方法。",
    "tldr": "本文介绍了一种名为单重置的分而治之模仿学习的扩展方法，该方法解决了在演示学习中需要重置特定状态的限制，使其可以应用于实际系统。",
    "en_tdlr": "This paper introduces an extension called Single-Reset for the Divide & Conquer Imitation Learning, which addresses the limitation of needing to reset specific states in demonstration learning and allows its application to real systems."
}