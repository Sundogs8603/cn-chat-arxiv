{
    "title": "Amplifying Training Data Exposure through Fine-Tuning with Pseudo-Labeled Memberships",
    "abstract": "arXiv:2402.12189v1 Announce Type: new  Abstract: Neural language models (LMs) are vulnerable to training data extraction attacks due to data memorization. This paper introduces a novel attack scenario wherein an attacker adversarially fine-tunes pre-trained LMs to amplify the exposure of the original training data. This strategy differs from prior studies by aiming to intensify the LM's retention of its pre-training dataset. To achieve this, the attacker needs to collect generated texts that are closely aligned with the pre-training data. However, without knowledge of the actual dataset, quantifying the amount of pre-training data within generated texts is challenging. To address this, we propose the use of pseudo-labels for these generated texts, leveraging membership approximations indicated by machine-generated probabilities from the target LM. We subsequently fine-tune the LM to favor generations with higher likelihoods of originating from the pre-training data, based on their memb",
    "link": "https://arxiv.org/abs/2402.12189",
    "context": "Title: Amplifying Training Data Exposure through Fine-Tuning with Pseudo-Labeled Memberships\nAbstract: arXiv:2402.12189v1 Announce Type: new  Abstract: Neural language models (LMs) are vulnerable to training data extraction attacks due to data memorization. This paper introduces a novel attack scenario wherein an attacker adversarially fine-tunes pre-trained LMs to amplify the exposure of the original training data. This strategy differs from prior studies by aiming to intensify the LM's retention of its pre-training dataset. To achieve this, the attacker needs to collect generated texts that are closely aligned with the pre-training data. However, without knowledge of the actual dataset, quantifying the amount of pre-training data within generated texts is challenging. To address this, we propose the use of pseudo-labels for these generated texts, leveraging membership approximations indicated by machine-generated probabilities from the target LM. We subsequently fine-tune the LM to favor generations with higher likelihoods of originating from the pre-training data, based on their memb",
    "path": "papers/24/02/2402.12189.json",
    "total_tokens": 863,
    "translated_title": "通过使用伪标签成员资格进行微调来增强训练数据曝光",
    "translated_abstract": "神经语言模型(LMs)由于数据记忆而容易受到训练数据提取攻击的影响。本文介绍了一种新的攻击场景，在这种场景中，攻击者对预训练LM进行对抗微调，以放大原始训练数据的曝光。该策略不同于先前的研究，其目的是加强LM对其预训练数据集的保留。为了实现这一目标，攻击者需要收集与预训练数据密切相关的生成文本。然而，如果没有实际数据集的知识，衡量生成文本中预训练数据的量是具有挑战性的。为了解决这个问题，我们提出利用目标LM的机器生成概率所表示的成员近似值为这些生成文本使用伪标签。随后，我们微调LM以支持那些更有可能源自预训练数据的生成文本，根据其成员资格。",
    "tldr": "攻击者通过对预训练LM进行对抗微调，以放大原始训练数据的曝光，采用伪标签和机器生成概率来加强LM对预训练数据的保留。",
    "en_tdlr": "Adversarially fine-tuning pre-trained LMs, attackers amplify the exposure of original training data by leveraging pseudo-labels and machine-generated probabilities to intensify the LM's retention of pre-training data."
}