{
    "title": "Boosting Graph Pooling with Persistent Homology",
    "abstract": "arXiv:2402.16346v1 Announce Type: new  Abstract: Recently, there has been an emerging trend to integrate persistent homology (PH) into graph neural networks (GNNs) to enrich expressive power. However, naively plugging PH features into GNN layers always results in marginal improvement with low interpretability. In this paper, we investigate a novel mechanism for injecting global topological invariance into pooling layers using PH, motivated by the observation that filtration operation in PH naturally aligns graph pooling in a cut-off manner. In this fashion, message passing in the coarsened graph acts along persistent pooled topology, leading to improved performance. Experimentally, we apply our mechanism to a collection of graph pooling methods and observe consistent and substantial performance gain over several popular datasets, demonstrating its wide applicability and flexibility.",
    "link": "https://arxiv.org/abs/2402.16346",
    "context": "Title: Boosting Graph Pooling with Persistent Homology\nAbstract: arXiv:2402.16346v1 Announce Type: new  Abstract: Recently, there has been an emerging trend to integrate persistent homology (PH) into graph neural networks (GNNs) to enrich expressive power. However, naively plugging PH features into GNN layers always results in marginal improvement with low interpretability. In this paper, we investigate a novel mechanism for injecting global topological invariance into pooling layers using PH, motivated by the observation that filtration operation in PH naturally aligns graph pooling in a cut-off manner. In this fashion, message passing in the coarsened graph acts along persistent pooled topology, leading to improved performance. Experimentally, we apply our mechanism to a collection of graph pooling methods and observe consistent and substantial performance gain over several popular datasets, demonstrating its wide applicability and flexibility.",
    "path": "papers/24/02/2402.16346.json",
    "total_tokens": 756,
    "translated_title": "用持久同调增强图池化",
    "translated_abstract": "最近，将持久同调（PH）纳入图神经网络（GNN）以丰富表达能力的趋势越来越明显。然而，简单地将PH特征插入GNN层总是带来较低可解释性的边际改进。本文研究了一种新颖的机制，通过PH向池化层注入全局拓扑不变性，灵感来自PH中的过滤操作自然地使图池化以截断方式对齐。这种方式下，粗化图中的消息传递沿着持久池化拓扑进行，从而提升性能。在实验中，我们将该机制应用于一系列图池化方法，并观察到在几个常见数据集上持续且显著的性能提升，展示了其广泛适用性和灵活性。",
    "tldr": "通过PH向池化层注入全局拓扑不变性的机制显著提升了图神经网络的性能。",
    "en_tdlr": "Mechanism of injecting global topological invariance into pooling layers using persistent homology significantly improves the performance of graph neural networks."
}