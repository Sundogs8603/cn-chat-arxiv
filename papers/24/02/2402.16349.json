{
    "title": "C-GAIL: Stabilizing Generative Adversarial Imitation Learning with Control Theory",
    "abstract": "arXiv:2402.16349v1 Announce Type: new  Abstract: Generative Adversarial Imitation Learning (GAIL) trains a generative policy to mimic a demonstrator. It uses on-policy Reinforcement Learning (RL) to optimize a reward signal derived from a GAN-like discriminator. A major drawback of GAIL is its training instability - it inherits the complex training dynamics of GANs, and the distribution shift introduced by RL. This can cause oscillations during training, harming its sample efficiency and final policy performance. Recent work has shown that control theory can help with the convergence of a GAN's training. This paper extends this line of work, conducting a control-theoretic analysis of GAIL and deriving a novel controller that not only pushes GAIL to the desired equilibrium but also achieves asymptotic stability in a 'one-step' setting. Based on this, we propose a practical algorithm 'Controlled-GAIL' (C-GAIL). On MuJoCo tasks, our controlled variant is able to speed up the rate of conve",
    "link": "https://arxiv.org/abs/2402.16349",
    "context": "Title: C-GAIL: Stabilizing Generative Adversarial Imitation Learning with Control Theory\nAbstract: arXiv:2402.16349v1 Announce Type: new  Abstract: Generative Adversarial Imitation Learning (GAIL) trains a generative policy to mimic a demonstrator. It uses on-policy Reinforcement Learning (RL) to optimize a reward signal derived from a GAN-like discriminator. A major drawback of GAIL is its training instability - it inherits the complex training dynamics of GANs, and the distribution shift introduced by RL. This can cause oscillations during training, harming its sample efficiency and final policy performance. Recent work has shown that control theory can help with the convergence of a GAN's training. This paper extends this line of work, conducting a control-theoretic analysis of GAIL and deriving a novel controller that not only pushes GAIL to the desired equilibrium but also achieves asymptotic stability in a 'one-step' setting. Based on this, we propose a practical algorithm 'Controlled-GAIL' (C-GAIL). On MuJoCo tasks, our controlled variant is able to speed up the rate of conve",
    "path": "papers/24/02/2402.16349.json",
    "total_tokens": 971,
    "translated_title": "C-GAIL: 利用控制理论稳定生成对抗模仿学习",
    "translated_abstract": "生成对抗模仿学习（GAIL）训练一个生成策略来模仿一个演示者。它使用基于策略的强化学习（RL）来优化从类似GAN的鉴别器中导出的奖励信号。GAIL的一个主要缺点是其训练不稳定性 - 它继承了GAN的复杂训练动态，以及RL引入的分布转移。这可能导致训练过程中的振荡，从而影响其样本效率和最终策略性能。最近的工作表明，控制理论可以帮助GAN的训练收敛。本文延伸了这一线路的工作，对GAIL进行了控制理论分析，并导出了一种新颖的控制器，该控制器不仅将GAIL推向期望的均衡点，还在“单步”设置中实现了渐近稳定性。基于此，我们提出了一个实用算法“Controlled-GAIL”（C-GAIL）。在MuJoCo任务中，我们的受控变体能够加速收敛速度。",
    "tldr": "该论文利用控制理论改进了生成对抗模仿学习（GAIL），提出了一种名为“Controlled-GAIL”（C-GAIL）的算法，能够解决GAIL训练不稳定性的问题，并在MuJoCo任务中取得了较快的收敛速度。",
    "en_tdlr": "This paper improves Generative Adversarial Imitation Learning (GAIL) using control theory, proposes an algorithm named \"Controlled-GAIL\" (C-GAIL) to address the training instability issue of GAIL, and achieves faster convergence on MuJoCo tasks."
}