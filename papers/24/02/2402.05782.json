{
    "title": "Analysing the Sample Complexity of Opponent Shaping",
    "abstract": "Learning in general-sum games often yields collectively sub-optimal results. Addressing this, opponent shaping (OS) methods actively guide the learning processes of other agents, empirically leading to improved individual and group performances in many settings. Early OS methods use higher-order derivatives to shape the learning of co-players, making them unsuitable for shaping multiple learning steps. Follow-up work, Model-free Opponent Shaping (M-FOS), addresses these by reframing the OS problem as a meta-game. In contrast to early OS methods, there is little theoretical understanding of the M-FOS framework. Providing theoretical guarantees for M-FOS is hard because A) there is little literature on theoretical sample complexity bounds for meta-reinforcement learning B) M-FOS operates in continuous state and action spaces, so theoretical analysis is challenging. In this work, we present R-FOS, a tabular version of M-FOS that is more suitable for theoretical analysis. R-FOS discretises",
    "link": "https://arxiv.org/abs/2402.05782",
    "context": "Title: Analysing the Sample Complexity of Opponent Shaping\nAbstract: Learning in general-sum games often yields collectively sub-optimal results. Addressing this, opponent shaping (OS) methods actively guide the learning processes of other agents, empirically leading to improved individual and group performances in many settings. Early OS methods use higher-order derivatives to shape the learning of co-players, making them unsuitable for shaping multiple learning steps. Follow-up work, Model-free Opponent Shaping (M-FOS), addresses these by reframing the OS problem as a meta-game. In contrast to early OS methods, there is little theoretical understanding of the M-FOS framework. Providing theoretical guarantees for M-FOS is hard because A) there is little literature on theoretical sample complexity bounds for meta-reinforcement learning B) M-FOS operates in continuous state and action spaces, so theoretical analysis is challenging. In this work, we present R-FOS, a tabular version of M-FOS that is more suitable for theoretical analysis. R-FOS discretises",
    "path": "papers/24/02/2402.05782.json",
    "total_tokens": 850,
    "translated_title": "分析对手塑造的样本复杂性",
    "translated_abstract": "在一般和博弈中，学习通常会导致集体性的次优结果。对此进行改进，对手塑造（OS）方法积极引导其他智能体的学习过程，经验性地提高了个体和群体在许多情景下的表现。早期的OS方法使用高阶导数来塑造合作玩家的学习，这使得它们不适用于塑造多个学习步骤。后续的工作，即无模型对手塑造（M-FOS），通过将OS问题重新定义为元博弈来解决这些问题。与早期的OS方法相比，对于M-FOS框架的理论理解还很少。为M-FOS提供理论保证是困难的，因为A）元强化学习的理论样本复杂性界限的文献很少 B）M-FOS在连续状态和动作空间中运作，所以理论分析具有挑战性。在这项工作中，我们提出了R-FOS，这是M-FOS的表格化版本，更适合进行理论分析。R-FOS离散化了",
    "tldr": "本文研究了对手塑造的样本复杂性，并提出了一种适合理论分析的表格化版本R-FOS。",
    "en_tdlr": "This paper analyzes the sample complexity of opponent shaping and presents a tabular version R-FOS suitable for theoretical analysis."
}