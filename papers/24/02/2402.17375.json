{
    "title": "Impact of Computation in Integral Reinforcement Learning for Continuous-Time Control",
    "abstract": "arXiv:2402.17375v1 Announce Type: cross  Abstract: Integral reinforcement learning (IntRL) demands the precise computation of the utility function's integral at its policy evaluation (PEV) stage. This is achieved through quadrature rules, which are weighted sums of utility functions evaluated from state samples obtained in discrete time. Our research reveals a critical yet underexplored phenomenon: the choice of the computational method -- in this case, the quadrature rule -- can significantly impact control performance. This impact is traced back to the fact that computational errors introduced in the PEV stage can affect the policy iteration's convergence behavior, which in turn affects the learned controller. To elucidate how computation impacts control, we draw a parallel between IntRL's policy iteration and Newton's method applied to the Hamilton-Jacobi-Bellman equation. In this light, computational error in PEV manifests as an extra error term in each iteration of Newton's method",
    "link": "https://arxiv.org/abs/2402.17375",
    "context": "Title: Impact of Computation in Integral Reinforcement Learning for Continuous-Time Control\nAbstract: arXiv:2402.17375v1 Announce Type: cross  Abstract: Integral reinforcement learning (IntRL) demands the precise computation of the utility function's integral at its policy evaluation (PEV) stage. This is achieved through quadrature rules, which are weighted sums of utility functions evaluated from state samples obtained in discrete time. Our research reveals a critical yet underexplored phenomenon: the choice of the computational method -- in this case, the quadrature rule -- can significantly impact control performance. This impact is traced back to the fact that computational errors introduced in the PEV stage can affect the policy iteration's convergence behavior, which in turn affects the learned controller. To elucidate how computation impacts control, we draw a parallel between IntRL's policy iteration and Newton's method applied to the Hamilton-Jacobi-Bellman equation. In this light, computational error in PEV manifests as an extra error term in each iteration of Newton's method",
    "path": "papers/24/02/2402.17375.json",
    "total_tokens": 805,
    "translated_title": "连续时间控制中积分强化学习中计算的影响",
    "translated_abstract": "积分强化学习(IntRL)在政策评估(PEV)阶段需要精确计算效用函数的积分。这是通过积分规则实现的，即来自离散时间中获得的状态样本评估的效用函数的加权总和。我们的研究揭示了一个关键但被忽视的现象：计算方法的选择--在本例中是积分规则--可以显著影响控制性能。这种影响可追溯到引入于PEV阶段的计算错误可能影响政策迭代的收敛行为，进而影响所学控制器。为了阐明计算如何影响控制，我们将IntRL的政策迭代与应用于哈密顿-雅可比-贝尔曼方程的牛顿法进行了类比。在这种光下，PEV中的计算误差表现为牛顿法的每次迭代中的额外误差项",
    "tldr": "计算方法选择会显著影响连续时间控制中积分强化学习的性能表现",
    "en_tdlr": "The choice of computational method significantly impacts the performance of integral reinforcement learning in continuous-time control."
}