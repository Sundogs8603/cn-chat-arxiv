{
    "title": "Towards Few-Shot Adaptation of Foundation Models via Multitask Finetuning",
    "abstract": "arXiv:2402.15017v1 Announce Type: cross  Abstract: Foundation models have emerged as a powerful tool for many AI problems. Despite the tremendous success of foundation models, effective adaptation to new tasks, particularly those with limited labels, remains an open question and lacks theoretical understanding. An emerging solution with recent success in vision and NLP involves finetuning a foundation model on a selection of relevant tasks, before its adaptation to a target task with limited labeled samples. In this paper, we study the theoretical justification of this multitask finetuning approach. Our theoretical analysis reveals that with a diverse set of related tasks, this multitask finetuning leads to reduced error in the target task, in comparison to directly adapting the same pretrained model. We quantify the relationship between finetuning tasks and target tasks by diversity and consistency metrics, and further propose a practical task selection algorithm. We substantiate our ",
    "link": "https://arxiv.org/abs/2402.15017",
    "context": "Title: Towards Few-Shot Adaptation of Foundation Models via Multitask Finetuning\nAbstract: arXiv:2402.15017v1 Announce Type: cross  Abstract: Foundation models have emerged as a powerful tool for many AI problems. Despite the tremendous success of foundation models, effective adaptation to new tasks, particularly those with limited labels, remains an open question and lacks theoretical understanding. An emerging solution with recent success in vision and NLP involves finetuning a foundation model on a selection of relevant tasks, before its adaptation to a target task with limited labeled samples. In this paper, we study the theoretical justification of this multitask finetuning approach. Our theoretical analysis reveals that with a diverse set of related tasks, this multitask finetuning leads to reduced error in the target task, in comparison to directly adapting the same pretrained model. We quantify the relationship between finetuning tasks and target tasks by diversity and consistency metrics, and further propose a practical task selection algorithm. We substantiate our ",
    "path": "papers/24/02/2402.15017.json",
    "total_tokens": 835,
    "translated_title": "通过多任务微调实现基础模型的少样本适应",
    "translated_abstract": "基础模型已经成为许多人工智能问题的有力工具。尽管基础模型取得了巨大成功，但有效地适应新任务，特别是那些数据标签有限的任务，仍然是一个开放问题，并且缺乏理论理解。最近在视觉和自然语言处理领域取得成功的一种新兴解决方案是，在基础模型上对一系列相关任务进行微调，然后再适应具有有限标记样本的目标任务。本文研究了这种多任务微调方法的理论验证。我们的理论分析表明，通过一个多样化的相关任务集，这种多任务微调可以降低目标任务中的误差，与直接适应相同预训练模型相比。我们通过多样性和一致性指标量化了微调任务和目标任务之间的关系，并进一步提出了一个实用的任务选择算法。",
    "tldr": "多任务微调的方法通过在基础模型上对相关任务进行微调，然后适应限制标签数的目标任务，能够降低目标任务中的误差，并提出了一种实用的任务选择算法。"
}