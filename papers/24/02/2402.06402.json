{
    "title": "Hierarchical Transformers are Efficient Meta-Reinforcement Learners",
    "abstract": "We introduce Hierarchical Transformers for Meta-Reinforcement Learning (HTrMRL), a powerful online meta-reinforcement learning approach. HTrMRL aims to address the challenge of enabling reinforcement learning agents to perform effectively in previously unseen tasks. We demonstrate how past episodes serve as a rich source of information, which our model effectively distills and applies to new contexts. Our learned algorithm is capable of outperforming the previous state-of-the-art and provides more efficient meta-training while significantly improving generalization capabilities. Experimental results, obtained across various simulated tasks of the Meta-World Benchmark, indicate a significant improvement in learning efficiency and adaptability compared to the state-of-the-art on a variety of tasks. Our approach not only enhances the agent's ability to generalize from limited data but also paves the way for more robust and versatile AI systems.",
    "link": "https://arxiv.org/abs/2402.06402",
    "context": "Title: Hierarchical Transformers are Efficient Meta-Reinforcement Learners\nAbstract: We introduce Hierarchical Transformers for Meta-Reinforcement Learning (HTrMRL), a powerful online meta-reinforcement learning approach. HTrMRL aims to address the challenge of enabling reinforcement learning agents to perform effectively in previously unseen tasks. We demonstrate how past episodes serve as a rich source of information, which our model effectively distills and applies to new contexts. Our learned algorithm is capable of outperforming the previous state-of-the-art and provides more efficient meta-training while significantly improving generalization capabilities. Experimental results, obtained across various simulated tasks of the Meta-World Benchmark, indicate a significant improvement in learning efficiency and adaptability compared to the state-of-the-art on a variety of tasks. Our approach not only enhances the agent's ability to generalize from limited data but also paves the way for more robust and versatile AI systems.",
    "path": "papers/24/02/2402.06402.json",
    "total_tokens": 852,
    "translated_title": "层次化Transformer是高效的元强化学习者",
    "translated_abstract": "我们介绍了一种强大的在线元强化学习方法，即层次化Transformer用于元强化学习（HTrMRL）。HTrMRL旨在解决使强化学习代理能够在以前未见任务中有效执行的挑战。我们展示了过去的经验作为信息丰富的资源，我们的模型有效地提炼和应用到新的上下文中。我们学习到的算法能够超越以前的最先进，并提供更高效的元训练，同时显著改善了泛化能力。在Meta-World基准的各种模拟任务上获得的实验结果表明，在各种任务上相比最先进的方法，学习效率和适应性显著提升。我们的方法不仅增强了代理从有限数据中的泛化能力，还为更强大和多功能的AI系统铺平了道路。",
    "tldr": "层次化Transformer是一种高效的元强化学习方法，通过有效地提取过去经验的信息丰富资源，并应用于新的环境中，实现了超越最先进方法的元训练效果，并显著提高了泛化能力和学习效率。",
    "en_tdlr": "Hierarchical Transformers for Meta-Reinforcement Learning (HTrMRL) is an efficient approach that improves meta-training and generalization capabilities by effectively distilling and applying rich information from past experiences to new tasks. It outperforms previous state-of-the-art methods and shows significant improvement in learning efficiency and adaptability across various tasks."
}