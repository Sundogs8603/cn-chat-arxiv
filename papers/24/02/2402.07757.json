{
    "title": "Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model",
    "abstract": "Stepwise inference protocols, such as scratchpads and chain-of-thought, help language models solve complex problems by decomposing them into a sequence of simpler subproblems. Despite the significant gain in performance achieved via these protocols, the underlying mechanisms of stepwise inference have remained elusive. To address this, we propose to study autoregressive Transformer models on a synthetic task that embodies the multi-step nature of problems where stepwise inference is generally most useful. Specifically, we define a graph navigation problem wherein a model is tasked with traversing a path from a start to a goal node on the graph. Despite is simplicity, we find we can empirically reproduce and analyze several phenomena observed at scale: (i) the stepwise inference reasoning gap, the cause of which we find in the structure of the training data; (ii) a diversity-accuracy tradeoff in model generations as sampling temperature varies; (iii) a simplicity bias in the model's out",
    "link": "https://arxiv.org/abs/2402.07757",
    "context": "Title: Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model\nAbstract: Stepwise inference protocols, such as scratchpads and chain-of-thought, help language models solve complex problems by decomposing them into a sequence of simpler subproblems. Despite the significant gain in performance achieved via these protocols, the underlying mechanisms of stepwise inference have remained elusive. To address this, we propose to study autoregressive Transformer models on a synthetic task that embodies the multi-step nature of problems where stepwise inference is generally most useful. Specifically, we define a graph navigation problem wherein a model is tasked with traversing a path from a start to a goal node on the graph. Despite is simplicity, we find we can empirically reproduce and analyze several phenomena observed at scale: (i) the stepwise inference reasoning gap, the cause of which we find in the structure of the training data; (ii) a diversity-accuracy tradeoff in model generations as sampling temperature varies; (iii) a simplicity bias in the model's out",
    "path": "papers/24/02/2402.07757.json",
    "total_tokens": 895,
    "translated_title": "对Transformer中逐步推理的理解: 一个合成图导航模型的研究",
    "translated_abstract": "逐步推理协议，如scratchpads和chain-of-thought，通过将复杂问题分解为一系列较简单的子问题，帮助语言模型解决复杂问题。尽管这些协议在性能上取得了显著的提升，但逐步推理的底层机制仍然难以理解。为了解决这个问题，我们提出在合成任务中研究自回归Transformer模型，该任务体现了问题的多步性质，其中逐步推理通常最有用。具体而言，我们定义了一个图导航问题，模型的任务是在图上从起始节点到目标节点的路径上进行遍历。尽管简单，我们发现我们可以通过经验重现和分析观察到的几个现象：(i)逐步推理推理间隙，我们发现其原因在于训练数据的结构；(ii)在模型生成中的多样性-准确性权衡，随着采样温度的变化；(iii)模型在输出中的简单性偏见。",
    "tldr": "该论文研究了Transformer中的逐步推理，提出了一个合成图导航模型来探索逐步推理的底层机制，并通过该模型在合成任务上的实验证明了几个关键现象的存在。"
}