{
    "title": "Aligning Individual and Collective Objectives in Multi-Agent Cooperation",
    "abstract": "arXiv:2402.12416v1 Announce Type: cross  Abstract: In the field of multi-agent learning, the challenge of mixed-motive cooperation is pronounced, given the inherent contradictions between individual and collective goals. Current research in this domain primarily focuses on incorporating domain knowledge into rewards or introducing additional mechanisms to foster cooperation. However, many of these methods suffer from the drawbacks of manual design costs and the lack of a theoretical grounding convergence procedure to the solution. To address this gap, we approach the mixed-motive game by modeling it as a differentiable game to study learning dynamics. We introduce a novel optimization method named Altruistic Gradient Adjustment (AgA) that employs gradient adjustments to novelly align individual and collective objectives. Furthermore, we provide theoretical proof that the selection of an appropriate alignment weight in AgA can accelerate convergence towards the desired solutions while e",
    "link": "https://arxiv.org/abs/2402.12416",
    "context": "Title: Aligning Individual and Collective Objectives in Multi-Agent Cooperation\nAbstract: arXiv:2402.12416v1 Announce Type: cross  Abstract: In the field of multi-agent learning, the challenge of mixed-motive cooperation is pronounced, given the inherent contradictions between individual and collective goals. Current research in this domain primarily focuses on incorporating domain knowledge into rewards or introducing additional mechanisms to foster cooperation. However, many of these methods suffer from the drawbacks of manual design costs and the lack of a theoretical grounding convergence procedure to the solution. To address this gap, we approach the mixed-motive game by modeling it as a differentiable game to study learning dynamics. We introduce a novel optimization method named Altruistic Gradient Adjustment (AgA) that employs gradient adjustments to novelly align individual and collective objectives. Furthermore, we provide theoretical proof that the selection of an appropriate alignment weight in AgA can accelerate convergence towards the desired solutions while e",
    "path": "papers/24/02/2402.12416.json",
    "total_tokens": 799,
    "translated_title": "在多智能体合作中对齐个体和集体目标",
    "translated_abstract": "在多智能体学习领域，面临着混合动机合作的挑战，因为个体和集体目标之间存在固有的矛盾。当前在该领域的研究主要集中在将领域知识纳入奖励或引入额外机制来促进合作。然而，许多这些方法存在着手动设计成本和缺乏理论基础的收敛程序解决方案的缺点。为了填补这一空白，我们将混合动机博弈建模为一个可微分的博弈以研究学习动态。我们提出了一种名为Altruistic Gradient Adjustment (AgA)的新优化方法，利用梯度调整来新颖地对齐个体和集体目标。此外，我们提供理论证明，AgA中选择适当的对齐权重可以加速收敛到期望解决方案。",
    "tldr": "引入Altruistic Gradient Adjustment (AgA)优化方法，利用梯度调整来对齐个体和集体目标，加速收敛到期望解决方案",
    "en_tdlr": "Introducing the Altruistic Gradient Adjustment (AgA) optimization method to align individual and collective objectives using gradient adjustments, accelerating convergence towards desired solutions."
}