{
    "title": "Spin: An Efficient Secure Computation Framework with GPU Acceleration",
    "abstract": "Accuracy and efficiency remain challenges for multi-party computation (MPC) frameworks. Spin is a GPU-accelerated MPC framework that supports multiple computation parties and a dishonest majority adversarial setup. We propose optimized protocols for non-linear functions that are critical for machine learning, as well as several novel optimizations specific to attention that is the fundamental unit of Transformer models, allowing Spin to perform non-trivial CNNs training and Transformer inference without sacrificing security. At the backend level, Spin leverages GPU, CPU, and RDMA-enabled smart network cards for acceleration. Comprehensive evaluations demonstrate that Spin can be up to $2\\times$ faster than the state-of-the-art for deep neural network training. For inference on a Transformer model with 18.9 million parameters, our attention-specific optimizations enable Spin to achieve better efficiency, less communication, and better accuracy.",
    "link": "https://arxiv.org/abs/2402.02320",
    "context": "Title: Spin: An Efficient Secure Computation Framework with GPU Acceleration\nAbstract: Accuracy and efficiency remain challenges for multi-party computation (MPC) frameworks. Spin is a GPU-accelerated MPC framework that supports multiple computation parties and a dishonest majority adversarial setup. We propose optimized protocols for non-linear functions that are critical for machine learning, as well as several novel optimizations specific to attention that is the fundamental unit of Transformer models, allowing Spin to perform non-trivial CNNs training and Transformer inference without sacrificing security. At the backend level, Spin leverages GPU, CPU, and RDMA-enabled smart network cards for acceleration. Comprehensive evaluations demonstrate that Spin can be up to $2\\times$ faster than the state-of-the-art for deep neural network training. For inference on a Transformer model with 18.9 million parameters, our attention-specific optimizations enable Spin to achieve better efficiency, less communication, and better accuracy.",
    "path": "papers/24/02/2402.02320.json",
    "total_tokens": 846,
    "translated_title": "Spin: 一种具备GPU加速的高效安全计算框架",
    "translated_abstract": "准确性和效率对于多方计算（MPC）框架仍然是挑战。Spin是一个支持多个计算方和不诚实多数对抗设置的GPU加速的MPC框架。我们提出了针对机器学习关键的非线性函数的优化协议，以及针对Transformer模型的基本单元注意力的几种新颖优化，使Spin能够在不牺牲安全性的情况下进行非常规CNN训练和Transformer推断。在后端层面，Spin利用GPU、CPU和RDMA启用的智能网络卡进行加速。全面的评估表明，Spin在深度神经网络训练方面比最先进技术快两倍。对于具有1890万参数的Transformer模型的推断，我们的注意力特定优化使Spin能够实现更好的效率、更少的通信和更好的准确性。",
    "tldr": "Spin是一个GPU加速的多方计算(MPC)框架，支持多个计算方和不诚实多数对抗设置。该框架提出了针对机器学习关键的非线性函数的优化协议，并进行了针对Transformer模型的注意力的新颖优化，以实现高效且安全的计算。"
}