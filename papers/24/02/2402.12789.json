{
    "title": "Fair Classifiers Without Fair Training: An Influence-Guided Data Sampling Approach",
    "abstract": "arXiv:2402.12789v1 Announce Type: cross  Abstract: A fair classifier should ensure the benefit of people from different groups, while the group information is often sensitive and unsuitable for model training. Therefore, learning a fair classifier but excluding sensitive attributes in the training dataset is important. In this paper, we study learning fair classifiers without implementing fair training algorithms to avoid possible leakage of sensitive information. Our theoretical analyses validate the possibility of this approach, that traditional training on a dataset with an appropriate distribution shift can reduce both the upper bound for fairness disparity and model generalization error, indicating that fairness and accuracy can be improved simultaneously with simply traditional training. We then propose a tractable solution to progressively shift the original training data during training by sampling influential data, where the sensitive attribute of new data is not accessed in s",
    "link": "https://arxiv.org/abs/2402.12789",
    "context": "Title: Fair Classifiers Without Fair Training: An Influence-Guided Data Sampling Approach\nAbstract: arXiv:2402.12789v1 Announce Type: cross  Abstract: A fair classifier should ensure the benefit of people from different groups, while the group information is often sensitive and unsuitable for model training. Therefore, learning a fair classifier but excluding sensitive attributes in the training dataset is important. In this paper, we study learning fair classifiers without implementing fair training algorithms to avoid possible leakage of sensitive information. Our theoretical analyses validate the possibility of this approach, that traditional training on a dataset with an appropriate distribution shift can reduce both the upper bound for fairness disparity and model generalization error, indicating that fairness and accuracy can be improved simultaneously with simply traditional training. We then propose a tractable solution to progressively shift the original training data during training by sampling influential data, where the sensitive attribute of new data is not accessed in s",
    "path": "papers/24/02/2402.12789.json",
    "total_tokens": 812,
    "translated_title": "无需公平训练的公平分类器：一种受影响数据抽样方法",
    "translated_abstract": "一个公平的分类器应该确保来自不同群体的人们受益，而群体信息往往是敏感的，不适合模型训练。因此，在训练数据集中学习一个公平的分类器但排除敏感属性是很重要的。本文研究了学习公平分类器而不实现公平训练算法的方法，以避免可能泄露敏感信息。我们的理论分析验证了这种方法的可能性，即在具有适当分布偏移的数据集上进行传统训练可以同时减少公平差距的上限和模型泛化误差，表明公平性和准确性可以同步提高，只需简单地进行传统训练。然后，我们提出了一个可行的解决方案，通过抽样有影响力的数据逐步转移原始训练数据，在训练过程中不访问新数据的敏感属性。",
    "tldr": "在不实施公平训练算法的情况下学习公平分类器，通过抽样具有影响力的数据来逐步转移原始训练数据，从而提高公平性和准确性。",
    "en_tdlr": "Learning fair classifiers without fair training algorithms to ensure fairness and accuracy by progressively shifting original training data through sampling influential data."
}