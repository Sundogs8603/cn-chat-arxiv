{
    "title": "LEVI: Generalizable Fine-tuning via Layer-wise Ensemble of Different Views",
    "abstract": "Fine-tuning is becoming widely used for leveraging the power of pre-trained foundation models in new downstream tasks. While there are many successes of fine-tuning on various tasks, recent studies have observed challenges in the generalization of fine-tuned models to unseen distributions (i.e., out-of-distribution; OOD). To improve OOD generalization, some previous studies identify the limitations of fine-tuning data and regulate fine-tuning to preserve the general representation learned from pre-training data. However, potential limitations in the pre-training data and models are often ignored. In this paper, we contend that overly relying on the pre-trained representation may hinder fine-tuning from learning essential representations for downstream tasks and thus hurt its OOD generalization. It can be especially catastrophic when new tasks are from different (sub)domains compared to pre-training data. To address the issues in both pre-training and fine-tuning data, we propose a nove",
    "link": "https://arxiv.org/abs/2402.04644",
    "context": "Title: LEVI: Generalizable Fine-tuning via Layer-wise Ensemble of Different Views\nAbstract: Fine-tuning is becoming widely used for leveraging the power of pre-trained foundation models in new downstream tasks. While there are many successes of fine-tuning on various tasks, recent studies have observed challenges in the generalization of fine-tuned models to unseen distributions (i.e., out-of-distribution; OOD). To improve OOD generalization, some previous studies identify the limitations of fine-tuning data and regulate fine-tuning to preserve the general representation learned from pre-training data. However, potential limitations in the pre-training data and models are often ignored. In this paper, we contend that overly relying on the pre-trained representation may hinder fine-tuning from learning essential representations for downstream tasks and thus hurt its OOD generalization. It can be especially catastrophic when new tasks are from different (sub)domains compared to pre-training data. To address the issues in both pre-training and fine-tuning data, we propose a nove",
    "path": "papers/24/02/2402.04644.json",
    "total_tokens": 880,
    "translated_title": "LEVI:通过以层为单位的多视角集成实现可泛化的微调",
    "translated_abstract": "微调越来越广泛地用于在新的下游任务中利用预训练基础模型的能力。虽然在各种任务上微调取得了许多成功，但最近的研究观察到微调模型在未见过的分布（即，超出分布；OOD）上的泛化存在挑战。为了改善OOB泛化，一些先前的研究确定了微调数据的限制，并调整微调以保留自预训练数据学习到的通用表示。然而，预训练数据和模型中的潜在限制经常被忽视。在本文中，我们认为过度依赖预训练表示可能会阻碍微调学习下游任务的重要表示，从而影响其OOB泛化。当新任务来自于与预训练数据不同的（子）领域时，这可能尤为灾难性。为了解决预训练和微调数据中的问题，我们提出了一种新颖的方法。",
    "tldr": "本论文中，我们提出了一种名为LEVI的方法，通过以层为单位的多视角集成，实现了对预训练和微调数据中问题的解决，并提升微调模型对未见过的分布的泛化能力。",
    "en_tdlr": "In this paper, we propose a method called LEVI, which addresses the issues in pre-training and fine-tuning data through layer-wise ensemble of different views, and improves the generalization ability of fine-tuned models to unseen distributions."
}