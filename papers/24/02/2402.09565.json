{
    "title": "Graph-Skeleton: ~1% Nodes are Sufficient to Represent Billion-Scale Graph",
    "abstract": "arXiv:2402.09565v1 Announce Type: new  Abstract: Due to the ubiquity of graph data on the web, web graph mining has become a hot research spot. Nonetheless, the prevalence of large-scale web graphs in real applications poses significant challenges to storage, computational capacity and graph model design. Despite numerous studies to enhance the scalability of graph models, a noticeable gap remains between academic research and practical web graph mining applications. One major cause is that in most industrial scenarios, only a small part of nodes in a web graph are actually required to be analyzed, where we term these nodes as target nodes, while others as background nodes. In this paper, we argue that properly fetching and condensing the background nodes from massive web graph data might be a more economical shortcut to tackle the obstacles fundamentally. To this end, we make the first attempt to study the problem of massive background nodes compression for target nodes classification",
    "link": "https://arxiv.org/abs/2402.09565",
    "context": "Title: Graph-Skeleton: ~1% Nodes are Sufficient to Represent Billion-Scale Graph\nAbstract: arXiv:2402.09565v1 Announce Type: new  Abstract: Due to the ubiquity of graph data on the web, web graph mining has become a hot research spot. Nonetheless, the prevalence of large-scale web graphs in real applications poses significant challenges to storage, computational capacity and graph model design. Despite numerous studies to enhance the scalability of graph models, a noticeable gap remains between academic research and practical web graph mining applications. One major cause is that in most industrial scenarios, only a small part of nodes in a web graph are actually required to be analyzed, where we term these nodes as target nodes, while others as background nodes. In this paper, we argue that properly fetching and condensing the background nodes from massive web graph data might be a more economical shortcut to tackle the obstacles fundamentally. To this end, we make the first attempt to study the problem of massive background nodes compression for target nodes classification",
    "path": "papers/24/02/2402.09565.json",
    "total_tokens": 833,
    "translated_title": "图骨架：仅有约1%的节点足以表示十亿规模的图",
    "translated_abstract": "由于Web上图数据的普遍存在，Web图挖掘已成为热门研究领域。然而，在实际应用中，大规模Web图的普及给存储、计算能力和图模型设计带来了重大挑战。尽管已进行了大量的研究来提高图模型的可扩展性，但学术研究与实际Web图挖掘应用之间仍存在明显差距。其中一个主要原因是，在大多数工业场景中，实际上只需要分析Web图中的一小部分节点，我们将这些节点称为目标节点，其他节点称为背景节点。在本文中，我们认为从海量Web图数据中恰当地提取和压缩背景节点可能是解决问题的更经济的捷径。为此，我们首次尝试研究了目标节点分类的大规模背景节点压缩问题。",
    "tldr": "本文探讨了如何从海量的Web图数据中对背景节点进行压缩，并将重点放在分析目标节点上，以解决图数据存储和计算能力的挑战。",
    "en_tdlr": "This paper investigates how to compress background nodes from massive web graph data, with a focus on analyzing target nodes, to address the challenges of graph data storage and computational capacity."
}