{
    "title": "Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models",
    "abstract": "arXiv:2402.14714v1 Announce Type: cross  Abstract: This report introduces \\texttt{EEVE-Korean-v1.0}, a Korean adaptation of large language models that exhibit remarkable capabilities across English and Korean text understanding. Building on recent highly capable but English-centric LLMs, such as SOLAR-10.7B and Phi-2, where non-English texts are inefficiently processed with English-centric tokenizers, we present an efficient and effective vocabulary expansion (EEVE) method, which encompasses parameter freezing and subword initialization. In contrast to previous efforts that believe new embeddings require trillions of training tokens, we show that our method can significantly boost non-English proficiency within just 2 billion tokens. Surpassing most instruction-tuned LLMs on the Open Ko-LLM Leaderboard, as of January 2024, our model \\texttt{EEVE-Korean-10.8B-v1.0} ranks as the leading Korean pre-trained model in the open-source community, according to Hugging Face's leaderboard. We ope",
    "link": "https://arxiv.org/abs/2402.14714",
    "context": "Title: Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models\nAbstract: arXiv:2402.14714v1 Announce Type: cross  Abstract: This report introduces \\texttt{EEVE-Korean-v1.0}, a Korean adaptation of large language models that exhibit remarkable capabilities across English and Korean text understanding. Building on recent highly capable but English-centric LLMs, such as SOLAR-10.7B and Phi-2, where non-English texts are inefficiently processed with English-centric tokenizers, we present an efficient and effective vocabulary expansion (EEVE) method, which encompasses parameter freezing and subword initialization. In contrast to previous efforts that believe new embeddings require trillions of training tokens, we show that our method can significantly boost non-English proficiency within just 2 billion tokens. Surpassing most instruction-tuned LLMs on the Open Ko-LLM Leaderboard, as of January 2024, our model \\texttt{EEVE-Korean-10.8B-v1.0} ranks as the leading Korean pre-trained model in the open-source community, according to Hugging Face's leaderboard. We ope",
    "path": "papers/24/02/2402.14714.json",
    "total_tokens": 854,
    "translated_title": "高效有效的词汇扩展方法在多语言大型语言模型中的应用",
    "translated_abstract": "这篇报告介绍了\\texttt{EEVE-Korean-v1.0}，这是大型语言模型的韩文适配版本，展现出在英文和韩文文本理解方面的显著能力。我们提出了一种高效且有效的词汇扩展方法（EEVE），包括参数冻结和子词初始化。与先前认为新嵌入需要上万亿训练标记的努力相反，我们展示了我们的方法可以在仅20亿标记内显着提升非英语熟练度。截至2024年1月，我们的模型\\texttt{EEVE-Korean-10.8B-v1.0}在Open Ko-LLM榜单上超越了大多数经过指导调整的LLMs，成为了开源社区中表现最好的韩文预训练模型，根据Hugging Face的排行榜。",
    "tldr": "提出了一种高效且有效的词汇扩展方法（EEVE），可以显著提升非英语语言模型的性能，使得其在韩文文本理解方面表现出色。",
    "en_tdlr": "Introduced an efficient and effective vocabulary expansion method (EEVE) that significantly boosts the performance of non-English language models, enabling them to exhibit remarkable capabilities in Korean text understanding."
}