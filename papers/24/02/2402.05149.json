{
    "title": "FlowPG: Action-constrained Policy Gradient with Normalizing Flows",
    "abstract": "Action-constrained reinforcement learning (ACRL) is a popular approach for solving safety-critical and resource-allocation related decision making problems. A major challenge in ACRL is to ensure agent taking a valid action satisfying constraints in each RL step. Commonly used approach of using a projection layer on top of the policy network requires solving an optimization program which can result in longer training time, slow convergence, and zero gradient problem. To address this, first we use a normalizing flow model to learn an invertible, differentiable mapping between the feasible action space and the support of a simple distribution on a latent variable, such as Gaussian. Second, learning the flow model requires sampling from the feasible action space, which is also challenging. We develop multiple methods, based on Hamiltonian Monte-Carlo and probabilistic sentential decision diagrams for such action sampling for convex and non-convex constraints. Third, we integrate the learn",
    "link": "https://arxiv.org/abs/2402.05149",
    "context": "Title: FlowPG: Action-constrained Policy Gradient with Normalizing Flows\nAbstract: Action-constrained reinforcement learning (ACRL) is a popular approach for solving safety-critical and resource-allocation related decision making problems. A major challenge in ACRL is to ensure agent taking a valid action satisfying constraints in each RL step. Commonly used approach of using a projection layer on top of the policy network requires solving an optimization program which can result in longer training time, slow convergence, and zero gradient problem. To address this, first we use a normalizing flow model to learn an invertible, differentiable mapping between the feasible action space and the support of a simple distribution on a latent variable, such as Gaussian. Second, learning the flow model requires sampling from the feasible action space, which is also challenging. We develop multiple methods, based on Hamiltonian Monte-Carlo and probabilistic sentential decision diagrams for such action sampling for convex and non-convex constraints. Third, we integrate the learn",
    "path": "papers/24/02/2402.05149.json",
    "total_tokens": 948,
    "translated_title": "FlowPG: 使用正则化流的动作约束策略梯度",
    "translated_abstract": "动作约束强化学习（ACRL）是解决安全关键和资源分配相关决策问题的常用方法。ACRL的一个主要挑战是确保代理在每个强化学习步骤中采取满足约束条件的有效动作。通常使用在策略网络之上的投影层的方法需要解决一个优化问题，这可能导致训练时间较长、收敛速度慢和零梯度问题。为了解决这个问题，我们首先使用正则化流模型学习一个可逆的、可微分的映射，将可行动作空间映射到一个潜变量上的简单分布的支撑集合，例如高斯分布。其次，学习流模型需要从可行动作空间中进行采样，这也是一个具有挑战性的问题。我们开发了多种方法，基于哈密顿蒙特卡洛和概率表决图，用于凸约束和非凸约束下的动作采样。接下来，我们将学习的流模型与强化学习的策略网络相整合。",
    "tldr": "本文提出了使用正则化流的动作约束策略梯度（FlowPG）方法，以解决动作约束强化学习中的挑战。该方法通过学习一个可逆映射和开发多种动作采样方法，有效地解决了在每个强化学习步骤中确保代理采取合理动作的问题。"
}