{
    "title": "How Interpretable are Reasoning Explanations from Prompting Large Language Models?",
    "abstract": "arXiv:2402.11863v1 Announce Type: new  Abstract: Prompt Engineering has garnered significant attention for enhancing the performance of large language models across a multitude of tasks. Techniques such as the Chain-of-Thought not only bolster task performance but also delineate a clear trajectory of reasoning steps, offering a tangible form of explanation for the audience. Prior works on interpretability assess the reasoning chains yielded by Chain-of-Thought solely along a singular axis, namely faithfulness. We present a comprehensive and multifaceted evaluation of interpretability, examining not only faithfulness but also robustness and utility across multiple commonsense reasoning benchmarks. Likewise, our investigation is not confined to a single prompting technique; it expansively covers a multitude of prevalent prompting techniques employed in large language models, thereby ensuring a wide-ranging and exhaustive evaluation. In addition, we introduce a simple interpretability ali",
    "link": "https://arxiv.org/abs/2402.11863",
    "context": "Title: How Interpretable are Reasoning Explanations from Prompting Large Language Models?\nAbstract: arXiv:2402.11863v1 Announce Type: new  Abstract: Prompt Engineering has garnered significant attention for enhancing the performance of large language models across a multitude of tasks. Techniques such as the Chain-of-Thought not only bolster task performance but also delineate a clear trajectory of reasoning steps, offering a tangible form of explanation for the audience. Prior works on interpretability assess the reasoning chains yielded by Chain-of-Thought solely along a singular axis, namely faithfulness. We present a comprehensive and multifaceted evaluation of interpretability, examining not only faithfulness but also robustness and utility across multiple commonsense reasoning benchmarks. Likewise, our investigation is not confined to a single prompting technique; it expansively covers a multitude of prevalent prompting techniques employed in large language models, thereby ensuring a wide-ranging and exhaustive evaluation. In addition, we introduce a simple interpretability ali",
    "path": "papers/24/02/2402.11863.json",
    "total_tokens": 706,
    "translated_title": "大型语言模型推理解释的可解释性有多高？",
    "translated_abstract": "Prompt Engineering已经引起了人们的极大关注，可以增强大型语言模型在多项任务中的性能。Chain-of-Thought等技术不仅增强了任务性能，还描绘了清晰的推理步骤轨迹，为观众提供了一种有形的解释形式。我们对可解释性进行了全面多角度的评估，不仅考虑了忠实度，还考虑了在多个常识推理基准测试中的强健性和效用。此外，我们引入了一个简单的可解释性指标。",
    "tldr": "对大型语言模型推理解释提出了全面、多角度的评估，包括忠实度、强健性和效用，并引入了新的可解释性指标。",
    "en_tdlr": "Proposed a comprehensive and multifaceted evaluation of interpretability for reasoning explanations from prompting large language models, including faithfulness, robustness, and utility, and introduced a new interpretability metric."
}