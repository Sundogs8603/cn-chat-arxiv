{
    "title": "Data-freeWeight Compress and Denoise for Large Language Models",
    "abstract": "arXiv:2402.16319v1 Announce Type: new  Abstract: Large Language Models (LLMs) are reshaping the research landscape in artificial intelligence, particularly as model parameters scale up significantly, unlocking remarkable capabilities across various domains. Nevertheless, the scalability of model parameters faces constraints due to limitations in GPU memory and computational speed. To address these constraints, various weight compression methods have emerged, such as Pruning and Quantization. Given the low-rank nature of weight matrices in language models, the reduction of weights through matrix decomposition undoubtedly holds significant potential and promise. In this paper, drawing upon the intrinsic structure of LLMs, we propose a novel approach termed Data-free Joint Rank-k Approximation for compressing the parameter matrices. Significantly, our method is characterized by without necessitating additional involvement of any corpus, while simultaneously preserving orthogonality in con",
    "link": "https://arxiv.org/abs/2402.16319",
    "context": "Title: Data-freeWeight Compress and Denoise for Large Language Models\nAbstract: arXiv:2402.16319v1 Announce Type: new  Abstract: Large Language Models (LLMs) are reshaping the research landscape in artificial intelligence, particularly as model parameters scale up significantly, unlocking remarkable capabilities across various domains. Nevertheless, the scalability of model parameters faces constraints due to limitations in GPU memory and computational speed. To address these constraints, various weight compression methods have emerged, such as Pruning and Quantization. Given the low-rank nature of weight matrices in language models, the reduction of weights through matrix decomposition undoubtedly holds significant potential and promise. In this paper, drawing upon the intrinsic structure of LLMs, we propose a novel approach termed Data-free Joint Rank-k Approximation for compressing the parameter matrices. Significantly, our method is characterized by without necessitating additional involvement of any corpus, while simultaneously preserving orthogonality in con",
    "path": "papers/24/02/2402.16319.json",
    "total_tokens": 815,
    "translated_title": "大型语言模型的无数据权重压缩和去噪",
    "translated_abstract": "大型语言模型(LLMs)正在重塑人工智能研究领域的格局，特别是随着模型参数的显著扩大，跨越各个领域展现出卓越能力。然而，模型参数的可扩展性受限于GPU内存和计算速度的限制。为了解决这些限制，出现了各种权重压缩方法，如剪枝和量化。鉴于语言模型中权重矩阵的低秩特性，通过矩阵分解减少权重在压缩参数方面无疑具有显著潜力和前景。在本文中，借鉴LLMs的内在结构，我们提出了一种称为无数据联合秩-k逼近的新方法，用于压缩参数矩阵。值得注意的是，我们的方法特点在于无需额外涉及任何语料库，同时保持正交性。",
    "tldr": "无需数据参与，基于大型语言模型结构提出了一种新的权重压缩方法，可有效压缩参数矩阵并保持正交性。",
    "en_tdlr": "Introducing a novel weight compression method for large language models based on the model's structure, which does not require data involvement, effectively compresses parameter matrices while preserving orthogonality."
}