{
    "title": "Information Complexity of Stochastic Convex Optimization: Applications to Generalization and Memorization",
    "abstract": "arXiv:2402.09327v1 Announce Type: new Abstract: In this work, we investigate the interplay between memorization and learning in the context of \\emph{stochastic convex optimization} (SCO). We define memorization via the information a learning algorithm reveals about its training data points. We then quantify this information using the framework of conditional mutual information (CMI) proposed by Steinke and Zakynthinou (2020). Our main result is a precise characterization of the tradeoff between the accuracy of a learning algorithm and its CMI, answering an open question posed by Livni (2023). We show that, in the $L^2$ Lipschitz--bounded setting and under strong convexity, every learner with an excess error $\\varepsilon$ has CMI bounded below by $\\Omega(1/\\varepsilon^2)$ and $\\Omega(1/\\varepsilon)$, respectively. We further demonstrate the essential role of memorization in learning problems in SCO by designing an adversary capable of accurately identifying a significant fraction of the",
    "link": "https://arxiv.org/abs/2402.09327",
    "context": "Title: Information Complexity of Stochastic Convex Optimization: Applications to Generalization and Memorization\nAbstract: arXiv:2402.09327v1 Announce Type: new Abstract: In this work, we investigate the interplay between memorization and learning in the context of \\emph{stochastic convex optimization} (SCO). We define memorization via the information a learning algorithm reveals about its training data points. We then quantify this information using the framework of conditional mutual information (CMI) proposed by Steinke and Zakynthinou (2020). Our main result is a precise characterization of the tradeoff between the accuracy of a learning algorithm and its CMI, answering an open question posed by Livni (2023). We show that, in the $L^2$ Lipschitz--bounded setting and under strong convexity, every learner with an excess error $\\varepsilon$ has CMI bounded below by $\\Omega(1/\\varepsilon^2)$ and $\\Omega(1/\\varepsilon)$, respectively. We further demonstrate the essential role of memorization in learning problems in SCO by designing an adversary capable of accurately identifying a significant fraction of the",
    "path": "papers/24/02/2402.09327.json",
    "total_tokens": 1017,
    "translated_title": "随机凸优化的信息复杂度：泛化和记忆的应用",
    "translated_abstract": "在这项工作中，我们研究了在随机凸优化（SCO）的背景下记忆和学习的相互作用。我们通过学习算法对其训练数据点揭示的信息来定义记忆。然后，我们使用Steinke和Zakynthinou（2020）提出的条件互信息（CMI）框架来量化这些信息。我们的主要结果是对学习算法的准确性和它的CMI之间的权衡的精确描述，回答了Livni（2023）提出的一个未解之问。我们证明，在$L^2$ Lipschitz-有界的设置和强凸性下，每个具有超额错误$\\varepsilon$的学习算法的CMI下界分别被$\\Omega(1/\\varepsilon^2)$和$\\Omega(1/\\varepsilon)$所限制。我们进一步通过设计一个能够准确识别出大部分训练数据的对手来展示记忆在SCO中学习问题中的重要作用。",
    "tldr": "本论文研究了随机凸优化中记忆和学习之间的相互作用。通过量化学习算法对训练数据点揭示的信息来定义记忆，并准确定义了学习算法准确性与条件互信息（CMI）之间的权衡关系。在特定条件下，我们证明了学习算法的准确性与CMI之间的最佳边界。通过设计对手，我们进一步展示了记忆在随机凸优化中学习问题中的重要性。",
    "en_tdlr": "This paper investigates the interplay between memorization and learning in stochastic convex optimization (SCO). It defines memorization based on the information revealed by a learning algorithm about its training data points and characterizes the tradeoff between algorithm accuracy and conditional mutual information (CMI). The paper provides precise bounds on the accuracy and CMI and demonstrates the importance of memorization in learning problems within SCO."
}