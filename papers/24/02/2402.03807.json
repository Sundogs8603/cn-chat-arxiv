{
    "title": "SEABO: A Simple Search-Based Method for Offline Imitation Learning",
    "abstract": "Offline reinforcement learning (RL) has attracted much attention due to its ability in learning from static offline datasets and eliminating the need of interacting with the environment. Nevertheless, the success of offline RL relies heavily on the offline transitions annotated with reward labels. In practice, we often need to hand-craft the reward function, which is sometimes difficult, labor-intensive, or inefficient. To tackle this challenge, we set our focus on the offline imitation learning (IL) setting, and aim at getting a reward function based on the expert data and unlabeled data. To that end, we propose a simple yet effective search-based offline IL method, tagged SEABO. SEABO allocates a larger reward to the transition that is close to its closest neighbor in the expert demonstration, and a smaller reward otherwise, all in an unsupervised learning manner. Experimental results on a variety of D4RL datasets indicate that SEABO can achieve competitive performance to offline RL ",
    "link": "https://arxiv.org/abs/2402.03807",
    "context": "Title: SEABO: A Simple Search-Based Method for Offline Imitation Learning\nAbstract: Offline reinforcement learning (RL) has attracted much attention due to its ability in learning from static offline datasets and eliminating the need of interacting with the environment. Nevertheless, the success of offline RL relies heavily on the offline transitions annotated with reward labels. In practice, we often need to hand-craft the reward function, which is sometimes difficult, labor-intensive, or inefficient. To tackle this challenge, we set our focus on the offline imitation learning (IL) setting, and aim at getting a reward function based on the expert data and unlabeled data. To that end, we propose a simple yet effective search-based offline IL method, tagged SEABO. SEABO allocates a larger reward to the transition that is close to its closest neighbor in the expert demonstration, and a smaller reward otherwise, all in an unsupervised learning manner. Experimental results on a variety of D4RL datasets indicate that SEABO can achieve competitive performance to offline RL ",
    "path": "papers/24/02/2402.03807.json",
    "total_tokens": 892,
    "translated_title": "SEABO: 一种简单的基于搜索的离线模仿学习方法",
    "translated_abstract": "离线强化学习（RL）由于能够从静态离线数据集中学习并消除与环境交互的需求，受到了广泛关注。然而，离线RL的成功在很大程度上取决于标有奖励标签的离线转换。在实践中，我们经常需要手工设计奖励函数，这有时是困难的、劳动密集的或低效的。为了解决这个挑战，我们把重点放在离线模仿学习（IL）设置上，旨在基于专家数据和无标签数据得到一个奖励函数。为此，我们提出了一种简单但有效的基于搜索的离线IL方法，称为SEABO。SEABO以无监督学习的方式，将较大的奖励分配给与专家演示中最接近的转换，否则分配较小的奖励。在多个D4RL数据集上的实验结果表明，SEABO能够达到与离线RL相当的性能水平。",
    "tldr": "SEABO是一种简单而有效的基于搜索的离线模仿学习方法，它以无监督学习的方式，根据专家数据和无标签数据得到奖励函数，实验结果表明其性能与离线强化学习相当。",
    "en_tdlr": "SEABO is a simple and effective search-based method for offline imitation learning that obtains a reward function based on expert data and unlabeled data in an unsupervised manner, achieving competitive performance with offline reinforcement learning."
}