{
    "title": "Developing a Framework for Auditing Large Language Models Using Human-in-the-Loop",
    "abstract": "arXiv:2402.09346v1 Announce Type: new Abstract: As LLMs become more pervasive across various users and scenarios, identifying potential issues when using these models becomes essential. Examples include bias, inconsistencies, and hallucination. Although auditing the LLM for these problems is desirable, it is far from being easy or solved. An effective method is to probe the LLM using different versions of the same question. This could expose inconsistencies in its knowledge or operation, indicating potential for bias or hallucination. However, to operationalize this auditing method at scale, we need an approach to create those probes reliably and automatically. In this paper we propose an automatic and scalable solution, where one uses a different LLM along with human-in-the-loop. This approach offers verifiability and transparency, while avoiding circular reliance on the same LLMs, and increasing scientific rigor and generalizability. Specifically, we present a novel methodology with ",
    "link": "https://arxiv.org/abs/2402.09346",
    "context": "Title: Developing a Framework for Auditing Large Language Models Using Human-in-the-Loop\nAbstract: arXiv:2402.09346v1 Announce Type: new Abstract: As LLMs become more pervasive across various users and scenarios, identifying potential issues when using these models becomes essential. Examples include bias, inconsistencies, and hallucination. Although auditing the LLM for these problems is desirable, it is far from being easy or solved. An effective method is to probe the LLM using different versions of the same question. This could expose inconsistencies in its knowledge or operation, indicating potential for bias or hallucination. However, to operationalize this auditing method at scale, we need an approach to create those probes reliably and automatically. In this paper we propose an automatic and scalable solution, where one uses a different LLM along with human-in-the-loop. This approach offers verifiability and transparency, while avoiding circular reliance on the same LLMs, and increasing scientific rigor and generalizability. Specifically, we present a novel methodology with ",
    "path": "papers/24/02/2402.09346.json",
    "total_tokens": 879,
    "translated_title": "使用人机协同的方法开发大型语言模型审计框架",
    "translated_abstract": "随着大型语言模型在各种用户和场景中的普及，识别使用这些模型时可能存在的问题变得至关重要，例如偏见、不一致性和幻觉。尽管对这些问题进行审计是可取的，但并不容易解决。一种有效的方法是使用不同版本的相同问题来探测语言模型，这可以暴露其知识或运行中的不一致性，从而表明可能存在偏见或幻觉。然而，要在大规模上实现这种审计方法，我们需要一种可靠且自动化的生成这些探测的方法。在本文中，我们提出了一种自动化和可扩展的解决方案，其中一种方法是使用不同的语言模型和人机协同。这种方法提供了可验证性和透明性，避免对同一语言模型的循环依赖，并增加了科学严谨性和普适性。",
    "tldr": "本文提出了一种使用人机协同的方法开发大型语言模型审计框架，通过使用不同版本的相同问题来探测模型可能存在的偏见或幻觉，实现了自动化和可扩展的审计方法。"
}