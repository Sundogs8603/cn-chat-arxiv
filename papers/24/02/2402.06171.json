{
    "title": "Pushing Boundaries: Mixup's Influence on Neural Collapse",
    "abstract": "Mixup is a data augmentation strategy that employs convex combinations of training instances and their respective labels to augment the robustness and calibration of deep neural networks. Despite its widespread adoption, the nuanced mechanisms that underpin its success are not entirely understood. The observed phenomenon of Neural Collapse, where the last-layer activations and classifier of deep networks converge to a simplex equiangular tight frame (ETF), provides a compelling motivation to explore whether mixup induces alternative geometric configurations and whether those could explain its success. In this study, we delve into the last-layer activations of training data for deep networks subjected to mixup, aiming to uncover insights into its operational efficacy. Our investigation, spanning various architectures and dataset pairs, reveals that mixup's last-layer activations predominantly converge to a distinctive configuration different than one might expect. In this configuration,",
    "link": "https://arxiv.org/abs/2402.06171",
    "context": "Title: Pushing Boundaries: Mixup's Influence on Neural Collapse\nAbstract: Mixup is a data augmentation strategy that employs convex combinations of training instances and their respective labels to augment the robustness and calibration of deep neural networks. Despite its widespread adoption, the nuanced mechanisms that underpin its success are not entirely understood. The observed phenomenon of Neural Collapse, where the last-layer activations and classifier of deep networks converge to a simplex equiangular tight frame (ETF), provides a compelling motivation to explore whether mixup induces alternative geometric configurations and whether those could explain its success. In this study, we delve into the last-layer activations of training data for deep networks subjected to mixup, aiming to uncover insights into its operational efficacy. Our investigation, spanning various architectures and dataset pairs, reveals that mixup's last-layer activations predominantly converge to a distinctive configuration different than one might expect. In this configuration,",
    "path": "papers/24/02/2402.06171.json",
    "total_tokens": 845,
    "translated_title": "Pushing Boundaries: Mixup对神经塌陷的影响",
    "translated_abstract": "Mixup是一种数据增强策略，它利用训练实例及其相应的标签的凸组合来增强深度神经网络的稳健性和校准性。尽管它被广泛应用，但其成功的细微机制尚未完全理解。观察到的神经塌陷现象，即深度网络的最后一层激活和分类器收敛到一个简单光角紧框架（ETF），为探索mixup是否引发了替代几何配置及其能解释其成功的动机提供了有力支持。在本研究中，我们深入研究了经过mixup处理的深度网络训练数据的最后一层激活，旨在揭示其运行有效性的洞见。我们的调查涵盖了各种架构和数据集对，揭示了mixup的最后一层激活主要收敛到与预期不同的独特配置。",
    "tldr": "本研究揭示了Mixup对神经塌陷的影响，通过对深度网络训练数据的最后一层激活进行研究，发现Mixup的最后一层激活收敛到与预期不同的独特配置。",
    "en_tdlr": "This study reveals the impact of Mixup on neural collapse by investigating the last-layer activations of deep network training data, showing that the last-layer activations of Mixup converge to a distinctive configuration different than expected."
}