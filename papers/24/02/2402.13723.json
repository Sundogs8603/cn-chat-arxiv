{
    "title": "The Effect of Batch Size on Contrastive Self-Supervised Speech Representation Learning",
    "abstract": "arXiv:2402.13723v1 Announce Type: cross  Abstract: Foundation models in speech are often trained using many GPUs, which implicitly leads to large effective batch sizes. In this paper we study the effect of batch size on pre-training, both in terms of statistics that can be monitored during training, and in the effect on the performance of a downstream fine-tuning task. By using batch sizes varying from 87.5 seconds to 80 minutes of speech we show that, for a fixed amount of iterations, larger batch sizes result in better pre-trained models. However, there is lower limit for stability, and an upper limit for effectiveness. We then show that the quality of the pre-trained model depends mainly on the amount of speech data seen during training, i.e., on the product of batch size and number of iterations. All results are produced with an independent implementation of the wav2vec 2.0 architecture, which to a large extent reproduces the results of the original work (arXiv:2006.11477). Our ext",
    "link": "https://arxiv.org/abs/2402.13723",
    "context": "Title: The Effect of Batch Size on Contrastive Self-Supervised Speech Representation Learning\nAbstract: arXiv:2402.13723v1 Announce Type: cross  Abstract: Foundation models in speech are often trained using many GPUs, which implicitly leads to large effective batch sizes. In this paper we study the effect of batch size on pre-training, both in terms of statistics that can be monitored during training, and in the effect on the performance of a downstream fine-tuning task. By using batch sizes varying from 87.5 seconds to 80 minutes of speech we show that, for a fixed amount of iterations, larger batch sizes result in better pre-trained models. However, there is lower limit for stability, and an upper limit for effectiveness. We then show that the quality of the pre-trained model depends mainly on the amount of speech data seen during training, i.e., on the product of batch size and number of iterations. All results are produced with an independent implementation of the wav2vec 2.0 architecture, which to a large extent reproduces the results of the original work (arXiv:2006.11477). Our ext",
    "path": "papers/24/02/2402.13723.json",
    "total_tokens": 900,
    "translated_title": "批大小对比自监督语音表示学习的影响",
    "translated_abstract": "在语音中，基础模型通常使用多个GPU进行训练，这隐含地导致了较大的有效批处理大小。本文研究了批处理大小对预训练的影响，无论是在训练过程中可以监视的统计信息方面，还是对下游微调任务性能的影响。通过使用从87.5秒到80分钟的语音不同批次大小，我们发现，对于相同迭代次数，较大的批次大小会导致更好的预训练模型。然而，稳定性存在下限，有效性存在上限。然后我们指出，预训练模型的质量主要取决于训练过程中看到的语音数据量，即批处理大小与迭代次数的乘积。所有结果均通过独立实现的wav2vec 2.0架构生成，该架构在很大程度上复现了原始作品的结果(arXiv:2006.11477)。",
    "tldr": "通过研究批处理大小对预训练的影响，本研究表明较大的批次大小有助于更好的预训练模型，但存在稳定性下限和有效性上限，模型质量取决于训练过程中看到的语音数据量。",
    "en_tdlr": "This study investigates the impact of batch size on pre-training and demonstrates that larger batch sizes lead to better pre-trained models, with limits on stability and effectiveness, depending on the amount of speech data seen during training."
}