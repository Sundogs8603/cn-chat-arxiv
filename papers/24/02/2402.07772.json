{
    "title": "End-to-End Learning for Fair Multiobjective Optimization Under Uncertainty",
    "abstract": "Many decision processes in artificial intelligence and operations research are modeled by parametric optimization problems whose defining parameters are unknown and must be inferred from observable data. The Predict-Then-Optimize (PtO) paradigm in machine learning aims to maximize downstream decision quality by training the parametric inference model end-to-end with the subsequent constrained optimization. This requires backpropagation through the optimization problem using approximation techniques specific to the problem's form, especially for nondifferentiable linear and mixed-integer programs. This paper extends the PtO methodology to optimization problems with nondifferentiable Ordered Weighted Averaging (OWA) objectives, known for their ability to ensure properties of fairness and robustness in decision models. Through a collection of training techniques and proposed application settings, it shows how optimization of OWA functions can be effectively integrated with parametric pred",
    "link": "https://arxiv.org/abs/2402.07772",
    "context": "Title: End-to-End Learning for Fair Multiobjective Optimization Under Uncertainty\nAbstract: Many decision processes in artificial intelligence and operations research are modeled by parametric optimization problems whose defining parameters are unknown and must be inferred from observable data. The Predict-Then-Optimize (PtO) paradigm in machine learning aims to maximize downstream decision quality by training the parametric inference model end-to-end with the subsequent constrained optimization. This requires backpropagation through the optimization problem using approximation techniques specific to the problem's form, especially for nondifferentiable linear and mixed-integer programs. This paper extends the PtO methodology to optimization problems with nondifferentiable Ordered Weighted Averaging (OWA) objectives, known for their ability to ensure properties of fairness and robustness in decision models. Through a collection of training techniques and proposed application settings, it shows how optimization of OWA functions can be effectively integrated with parametric pred",
    "path": "papers/24/02/2402.07772.json",
    "total_tokens": 840,
    "translated_title": "不确定性下公平多目标优化的端到端学习",
    "translated_abstract": "许多人工智能和运筹学中的决策过程是由参数优化问题建模的，其定义参数是未知的，必须从可观测数据中推断出来。机器学习中的“先预测再优化”（PtO）范式旨在通过训练参数推断模型与后续的约束优化来最大化下游决策质量。这需要通过适用于问题形式的逼近技术，特别是对于不可微分的线性和混合整数程序，通过优化问题进行反向传播。本文将PtO方法扩展到具有不可微分的有序加权平均（OWA）目标的优化问题上，OWA目标在决策模型中保证公平性和鲁棒性的能力是众所周知的。通过一系列的训练技巧和提出的应用设置，本文展示了如何有效地将OWA函数的优化与参数推断相结合。",
    "tldr": "本文将机器学习中的“先预测再优化”（PtO）范式扩展到具有不可微分的有序加权平均（OWA）目标的优化问题上，并展示了如何有效地将OWA函数的优化与参数推断相结合。",
    "en_tdlr": "This paper extends the Predict-Then-Optimize (PtO) paradigm in machine learning to optimization problems with nondifferentiable Ordered Weighted Averaging (OWA) objectives, and demonstrates how to effectively integrate the optimization of OWA functions with parametric inference."
}