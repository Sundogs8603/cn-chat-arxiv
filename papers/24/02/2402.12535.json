{
    "title": "Locality-Sensitive Hashing-Based Efficient Point Transformer with Applications in High-Energy Physics",
    "abstract": "arXiv:2402.12535v1 Announce Type: new  Abstract: This study introduces a novel transformer model optimized for large-scale point cloud processing in scientific domains such as high-energy physics (HEP) and astrophysics. Addressing the limitations of graph neural networks and standard transformers, our model integrates local inductive bias and achieves near-linear complexity with hardware-friendly regular operations. One contribution of this work is the quantitative analysis of the error-complexity tradeoff of various sparsification techniques for building efficient transformers. Our findings highlight the superiority of using locality-sensitive hashing (LSH), especially OR \\& AND-construction LSH, in kernel approximation for large-scale point cloud data with local inductive bias. Based on this finding, we propose LSH-based Efficient Point Transformer (\\textbf{HEPT}), which combines E$^2$LSH with OR \\& AND constructions and is built upon regular computations. HEPT demonstrates remarkabl",
    "link": "https://arxiv.org/abs/2402.12535",
    "context": "Title: Locality-Sensitive Hashing-Based Efficient Point Transformer with Applications in High-Energy Physics\nAbstract: arXiv:2402.12535v1 Announce Type: new  Abstract: This study introduces a novel transformer model optimized for large-scale point cloud processing in scientific domains such as high-energy physics (HEP) and astrophysics. Addressing the limitations of graph neural networks and standard transformers, our model integrates local inductive bias and achieves near-linear complexity with hardware-friendly regular operations. One contribution of this work is the quantitative analysis of the error-complexity tradeoff of various sparsification techniques for building efficient transformers. Our findings highlight the superiority of using locality-sensitive hashing (LSH), especially OR \\& AND-construction LSH, in kernel approximation for large-scale point cloud data with local inductive bias. Based on this finding, we propose LSH-based Efficient Point Transformer (\\textbf{HEPT}), which combines E$^2$LSH with OR \\& AND constructions and is built upon regular computations. HEPT demonstrates remarkabl",
    "path": "papers/24/02/2402.12535.json",
    "total_tokens": 869,
    "translated_title": "基于局部敏感哈希的高能物理中应用的高效点变换器",
    "translated_abstract": "这项研究介绍了一种针对大规模点云处理在科学领域（如高能物理和天体物理）进行优化的新型Transformer模型。该模型解决了图神经网络和标准Transformer的局限性，集成了局部归纳偏差，并通过硬件友好的常规操作实现接近线性复杂度。本工作的一个贡献是对各种稀疏化技术进行误差-复杂度权衡的定量分析，以构建高效Transformer。我们的发现突显了在具有局部归纳偏差的大规模点云数据中使用局部敏感哈希（LSH），尤其是OR＆AND构造LSH，在核近似中的优越性。基于这一发现，我们提出了基于LSH的高效点变换器（HEPT），它将E^2LSH与OR＆AND构造相结合，并建立在常规计算之上。",
    "tldr": "本研究提出了一种针对科学领域中大规模点云处理优化的Transformer模型，通过局部敏感哈希技术实现了近线性复杂度，并提出了基于LSH的高效点变换器HEPT。",
    "en_tdlr": "This study introduces a novel transformer model optimized for large-scale point cloud processing in scientific domains by leveraging locality-sensitive hashing, achieving near-linear complexity, and proposing the HEPT transformer model based on LSH."
}