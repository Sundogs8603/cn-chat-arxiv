{
    "title": "The Duet of Representations and How Explanations Exacerbate It",
    "abstract": "An algorithm effects a causal representation of relations between features and labels in the human's perception. Such a representation might conflict with the human's prior belief. Explanations can direct the human's attention to the conflicting feature and away from other relevant features. This leads to causal overattribution and may adversely affect the human's information processing. In a field experiment we implemented an XGBoost-trained model as a decision-making aid for counselors at a public employment service to predict candidates' risk of long-term unemployment. The treatment group of counselors was also provided with SHAP. The results show that the quality of the human's decision-making is worse when a feature on which the human holds a conflicting prior belief is displayed as part of the explanation.",
    "link": "https://arxiv.org/abs/2402.08379",
    "context": "Title: The Duet of Representations and How Explanations Exacerbate It\nAbstract: An algorithm effects a causal representation of relations between features and labels in the human's perception. Such a representation might conflict with the human's prior belief. Explanations can direct the human's attention to the conflicting feature and away from other relevant features. This leads to causal overattribution and may adversely affect the human's information processing. In a field experiment we implemented an XGBoost-trained model as a decision-making aid for counselors at a public employment service to predict candidates' risk of long-term unemployment. The treatment group of counselors was also provided with SHAP. The results show that the quality of the human's decision-making is worse when a feature on which the human holds a conflicting prior belief is displayed as part of the explanation.",
    "path": "papers/24/02/2402.08379.json",
    "total_tokens": 775,
    "translated_title": "表示和解释的二重奏及其加剧作用",
    "translated_abstract": "算法能够对人类的感知中特征和标签之间的关系进行因果表示。这种表示可能与人类的先前信念相冲突。解释可以将人类的注意力引导到冲突的特征上，并使其远离其他相关特征。这导致因果过度归因，并可能对人类的信息处理产生不利影响。在一项实地实验中，我们将一个经过XGBoost训练的模型作为决策辅助工具应用于公共就业服务中心的顾问，用于预测候选人长期失业的风险。治疗组的顾问还提供了SHAP作为解释。结果显示，当冲突特征作为解释的一部分显示时，人类的决策质量较差。",
    "tldr": "该研究发现，算法的因果表示可能与人类的先前信念相冲突，解释会将注意力导向冲突特征并远离其他相关特征，这可能导致因果过度归因并对人类的信息处理产生不利影响。",
    "en_tdlr": "This study reveals that the causal representation of an algorithm may conflict with human prior beliefs, and explanations can direct attention to conflicting features, leading to causal overattribution and adverse effects on human information processing."
}