{
    "title": "Safety Optimized Reinforcement Learning via Multi-Objective Policy Optimization",
    "abstract": "arXiv:2402.15197v1 Announce Type: cross  Abstract: Safe reinforcement learning (Safe RL) refers to a class of techniques that aim to prevent RL algorithms from violating constraints in the process of decision-making and exploration during trial and error. In this paper, a novel model-free Safe RL algorithm, formulated based on the multi-objective policy optimization framework is introduced where the policy is optimized towards optimality and safety, simultaneously. The optimality is achieved by the environment reward function that is subsequently shaped using a safety critic. The advantage of the Safety Optimized RL (SORL) algorithm compared to the traditional Safe RL algorithms is that it omits the need to constrain the policy search space. This allows SORL to find a natural tradeoff between safety and optimality without compromising the performance in terms of either safety or optimality due to strict search space constraints. Through our theoretical analysis of SORL, we propose a co",
    "link": "https://arxiv.org/abs/2402.15197",
    "context": "Title: Safety Optimized Reinforcement Learning via Multi-Objective Policy Optimization\nAbstract: arXiv:2402.15197v1 Announce Type: cross  Abstract: Safe reinforcement learning (Safe RL) refers to a class of techniques that aim to prevent RL algorithms from violating constraints in the process of decision-making and exploration during trial and error. In this paper, a novel model-free Safe RL algorithm, formulated based on the multi-objective policy optimization framework is introduced where the policy is optimized towards optimality and safety, simultaneously. The optimality is achieved by the environment reward function that is subsequently shaped using a safety critic. The advantage of the Safety Optimized RL (SORL) algorithm compared to the traditional Safe RL algorithms is that it omits the need to constrain the policy search space. This allows SORL to find a natural tradeoff between safety and optimality without compromising the performance in terms of either safety or optimality due to strict search space constraints. Through our theoretical analysis of SORL, we propose a co",
    "path": "papers/24/02/2402.15197.json",
    "total_tokens": 911,
    "translated_title": "安全优化的多目标策略优化强化学习",
    "translated_abstract": "安全强化学习（安全RL）指的是一类技术，旨在防止RL算法在试错决策和探索过程中违反约束。本文介绍了一种基于多目标策略优化框架制定的新型无模型安全RL算法，其中策略同时朝着最优性和安全性进行优化。通过使用安全评论家来塑造环境奖励函数，从而实现最优性。相较于传统安全RL算法，安全优化RL（SORL）算法的优势在于省略了对策略搜索空间的约束需要。这使得SORL能够在不受严格搜索空间约束的情况下找到安全性和最优性之间的自然权衡，而无需因严格搜索空间约束而在安全性或最优性方面性能受损。通过对SORL的理论分析，我们提出了一种co",
    "tldr": "本文提出了一种基于多目标策略优化框架的安全强化学习算法，通过安全评论家塑造环境奖励函数，使得策略可以同时朝着最优性和安全性优化，相较于传统方法，该算法无需约束策略搜索空间，实现了安全性和最优性之间的自然权衡。",
    "en_tdlr": "This paper introduces a novel model-free safe reinforcement learning algorithm based on multi-objective policy optimization framework, shaping environment reward function using a safety critic to optimize policy towards both optimality and safety, omitting the need for policy search space constraints and achieving a natural tradeoff between safety and optimality compared to traditional approaches."
}