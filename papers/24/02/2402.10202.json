{
    "title": "Bridging Associative Memory and Probabilistic Modeling",
    "abstract": "arXiv:2402.10202v1 Announce Type: new  Abstract: Associative memory and probabilistic modeling are two fundamental topics in artificial intelligence. The first studies recurrent neural networks designed to denoise, complete and retrieve data, whereas the second studies learning and sampling from probability distributions. Based on the observation that associative memory's energy functions can be seen as probabilistic modeling's negative log likelihoods, we build a bridge between the two that enables useful flow of ideas in both directions. We showcase four examples: First, we propose new energy-based models that flexibly adapt their energy functions to new in-context datasets, an approach we term \\textit{in-context learning of energy functions}. Second, we propose two new associative memory models: one that dynamically creates new memories as necessitated by the training data using Bayesian nonparametrics, and another that explicitly computes proportional memory assignments using the e",
    "link": "https://arxiv.org/abs/2402.10202",
    "context": "Title: Bridging Associative Memory and Probabilistic Modeling\nAbstract: arXiv:2402.10202v1 Announce Type: new  Abstract: Associative memory and probabilistic modeling are two fundamental topics in artificial intelligence. The first studies recurrent neural networks designed to denoise, complete and retrieve data, whereas the second studies learning and sampling from probability distributions. Based on the observation that associative memory's energy functions can be seen as probabilistic modeling's negative log likelihoods, we build a bridge between the two that enables useful flow of ideas in both directions. We showcase four examples: First, we propose new energy-based models that flexibly adapt their energy functions to new in-context datasets, an approach we term \\textit{in-context learning of energy functions}. Second, we propose two new associative memory models: one that dynamically creates new memories as necessitated by the training data using Bayesian nonparametrics, and another that explicitly computes proportional memory assignments using the e",
    "path": "papers/24/02/2402.10202.json",
    "total_tokens": 917,
    "translated_title": "构建联想记忆与概率建模之间的桥梁",
    "translated_abstract": "arXiv:2402.10202v1 公告类型：新的 摘要：联想记忆和概率建模是人工智能中两个基本的主题。第一个研究设计用于去噪、完成和检索数据的递归神经网络，而第二个研究学习和从概率分布中采样。基于联想记忆的能量函数可以被视为概率建模的负对数似然函数的观察，我们在两个之间建立了一座桥梁，使得想法能在两个方向上有益的流动。我们展示了四个例子：首先，我们提出了新的以能量为基础的模型，这些模型可以灵活地适应新的上下文数据集，这种方法称为“上下文学习能量函数”。其次，我们提出了两种新的联想记忆模型：一种是根据训练数据的需要动态创建新的记忆，使用贝叶斯非参数方法，另一种是明确计算比例记忆分配，使用e作为概率函数分配记忆。",
    "tldr": "基于联想记忆的能量函数可以被视为概率建模的对数似然函数，这篇论文构建了两者之间的桥梁，提出了新的基于能量的模型，并展示了两种新的联想记忆模型，可灵活适应上下文数据集。",
    "en_tdlr": "This paper bridges associative memory and probabilistic modeling by showing that the energy functions of associative memory can be seen as negative log likelihoods of probabilistic modeling, and introduces new energy-based models and two associative memory models that can adapt to context datasets."
}