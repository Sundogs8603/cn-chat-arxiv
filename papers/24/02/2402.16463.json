{
    "title": "Learning to Schedule Online Tasks with Bandit Feedback",
    "abstract": "arXiv:2402.16463v1 Announce Type: new  Abstract: Online task scheduling serves an integral role for task-intensive applications in cloud computing and crowdsourcing. Optimal scheduling can enhance system performance, typically measured by the reward-to-cost ratio, under some task arrival distribution. On one hand, both reward and cost are dependent on task context (e.g., evaluation metric) and remain black-box in practice. These render reward and cost hard to model thus unknown before decision making. On the other hand, task arrival behaviors remain sensitive to factors like unpredictable system fluctuation whereby a prior estimation or the conventional assumption of arrival distribution (e.g., Poisson) may fail. This implies another practical yet often neglected challenge, i.e., uncertain task arrival distribution. Towards effective scheduling under a stationary environment with various uncertainties, we propose a double-optimistic learning based Robbins-Monro (DOL-RM) algorithm. Spec",
    "link": "https://arxiv.org/abs/2402.16463",
    "context": "Title: Learning to Schedule Online Tasks with Bandit Feedback\nAbstract: arXiv:2402.16463v1 Announce Type: new  Abstract: Online task scheduling serves an integral role for task-intensive applications in cloud computing and crowdsourcing. Optimal scheduling can enhance system performance, typically measured by the reward-to-cost ratio, under some task arrival distribution. On one hand, both reward and cost are dependent on task context (e.g., evaluation metric) and remain black-box in practice. These render reward and cost hard to model thus unknown before decision making. On the other hand, task arrival behaviors remain sensitive to factors like unpredictable system fluctuation whereby a prior estimation or the conventional assumption of arrival distribution (e.g., Poisson) may fail. This implies another practical yet often neglected challenge, i.e., uncertain task arrival distribution. Towards effective scheduling under a stationary environment with various uncertainties, we propose a double-optimistic learning based Robbins-Monro (DOL-RM) algorithm. Spec",
    "path": "papers/24/02/2402.16463.json",
    "total_tokens": 846,
    "translated_title": "使用赌博反馈学习在线任务调度",
    "translated_abstract": "在云计算和众包中，在线任务调度对于任务密集型应用起着至关重要的作用。优化的调度可以在某些任务到达分布下增强系统性能，通常通过奖励成本比来衡量。然而，奖励和成本都依赖于任务上下文（例如，评价指标）且在实践中保持黑盒状态。这使得奖励和成本难以建模，因此在决策之前未知。另一方面，任务到达行为对于诸如系统波动等因素保持敏感，先前的估计或常规到达分布假设（例如，泊松分布）可能失效。这意味着另一个实际但常被忽视的挑战，即不确定的任务到达分布。为了解决在各种不确定性的静态环境下有效调度的问题，我们提出了一种基于双乐观学习的Robbins-Monro（DOL-RM）算法。",
    "tldr": "通过提出一种基于双乐观学习的Robbins-Monro算法，解决了在线任务调度中奖励和成本难以建模、任务到达分布不确定等挑战",
    "en_tdlr": "The paper proposes a double-optimistic learning based Robbins-Monro (DOL-RM) algorithm to address challenges in online task scheduling, such as difficulty in modeling rewards and costs, and uncertainty in task arrival distribution."
}