{
    "title": "Why Lift so Heavy? Slimming Large Language Models by Cutting Off the Layers",
    "abstract": "arXiv:2402.11700v1 Announce Type: new  Abstract: Large Language Models (LLMs) possess outstanding capabilities in addressing various natural language processing (NLP) tasks. However, the sheer size of these models poses challenges in terms of storage, training and inference due to the inclusion of billions of parameters through layer stacking. While traditional approaches such as model pruning or distillation offer ways for reducing model size, they often come at the expense of performance retention. In our investigation, we systematically explore the approach of reducing the number of layers in LLMs. Surprisingly, we observe that even with fewer layers, LLMs maintain similar or better performance levels, particularly in prompt-based fine-tuning for text classification tasks. Remarkably, in certain cases, models with a single layer outperform their fully layered counterparts. These findings offer valuable insights for future work aimed at mitigating the size constraints of LLMs while p",
    "link": "https://arxiv.org/abs/2402.11700",
    "context": "Title: Why Lift so Heavy? Slimming Large Language Models by Cutting Off the Layers\nAbstract: arXiv:2402.11700v1 Announce Type: new  Abstract: Large Language Models (LLMs) possess outstanding capabilities in addressing various natural language processing (NLP) tasks. However, the sheer size of these models poses challenges in terms of storage, training and inference due to the inclusion of billions of parameters through layer stacking. While traditional approaches such as model pruning or distillation offer ways for reducing model size, they often come at the expense of performance retention. In our investigation, we systematically explore the approach of reducing the number of layers in LLMs. Surprisingly, we observe that even with fewer layers, LLMs maintain similar or better performance levels, particularly in prompt-based fine-tuning for text classification tasks. Remarkably, in certain cases, models with a single layer outperform their fully layered counterparts. These findings offer valuable insights for future work aimed at mitigating the size constraints of LLMs while p",
    "path": "papers/24/02/2402.11700.json",
    "total_tokens": 874,
    "translated_title": "为什么要举得那么沉？通过修剪层来减轻大型语言模型",
    "translated_abstract": "大型语言模型(LLMs)在处理各种自然语言处理(NLP)任务方面具有出色的能力。然而，这些模型的巨大规模在存储、训练和推理方面带来挑战，因为它们通过层叠包含了数十亿个参数。尽管传统方法如模型修剪或蒸馏为减小模型大小提供了途径，但往往会以性能保留为代价。在我们的调查中，我们系统地探讨了通过减少LLMs中的层数来减少模型规模的方法。令人惊讶的是，我们观察到，即使层数较少，LLMs在特别是基于提示的文本分类任务的微调中也能保持类似或更好的性能水平。值得注意的是，在某些情况下，只有一个层的模型可以胜过完全层式的对应项。这些发现为未来旨在减轻LLMs大小约束的工作提供了宝贵的见解。",
    "tldr": "减少大型语言模型的层数可在不损失性能的情况下减轻模型规模，甚至在某些情况下只有一个层的模型可以超越完全层式的对应项。",
    "en_tdlr": "Reducing the number of layers in large language models can reduce model size without compromising performance, and in some cases, models with just one layer can outperform their fully layered counterparts."
}