{
    "title": "Stable Update of Regression Trees",
    "abstract": "arXiv:2402.13655v1 Announce Type: new  Abstract: Updating machine learning models with new information usually improves their predictive performance, yet, in many applications, it is also desirable to avoid changing the model predictions too much. This property is called stability. In most cases when stability matters, so does explainability. We therefore focus on the stability of an inherently explainable machine learning method, namely regression trees. We aim to use the notion of empirical stability and design algorithms for updating regression trees that provide a way to balance between predictability and empirical stability. To achieve this, we propose a regularization method, where data points are weighted based on the uncertainty in the initial model. The balance between predictability and empirical stability can be adjusted through hyperparameters. This regularization method is evaluated in terms of loss and stability and assessed on a broad range of data characteristics. The r",
    "link": "https://arxiv.org/abs/2402.13655",
    "context": "Title: Stable Update of Regression Trees\nAbstract: arXiv:2402.13655v1 Announce Type: new  Abstract: Updating machine learning models with new information usually improves their predictive performance, yet, in many applications, it is also desirable to avoid changing the model predictions too much. This property is called stability. In most cases when stability matters, so does explainability. We therefore focus on the stability of an inherently explainable machine learning method, namely regression trees. We aim to use the notion of empirical stability and design algorithms for updating regression trees that provide a way to balance between predictability and empirical stability. To achieve this, we propose a regularization method, where data points are weighted based on the uncertainty in the initial model. The balance between predictability and empirical stability can be adjusted through hyperparameters. This regularization method is evaluated in terms of loss and stability and assessed on a broad range of data characteristics. The r",
    "path": "papers/24/02/2402.13655.json",
    "total_tokens": 771,
    "translated_title": "回归树的稳定更新",
    "translated_abstract": "更新机器学习模型以获取新信息通常会改善它们的预测性能，然而，在许多应用中，也希望避免过多改变模型的预测。这种属性被称为稳定性。在大多数情况下，稳定性很重要，解释性也很重要。因此，我们专注于固有可解释的机器学习方法，即回归树的稳定性。我们旨在利用经验稳定性概念，并设计用于更新回归树的算法，以提供平衡预测性能和经验稳定性的方法。为实现这一目标，我们提出一种正则化方法，其中数据点根据初始模型的不确定性进行加权。通过超参数可以调整预测性能和经验稳定性之间的平衡。这种正则化方法在损失和稳定性方面进行评估，并在广泛的数据特征上进行评估。",
    "tldr": "提出了一种回归树的稳定更新方法，通过正则化和调整超参数来平衡预测性能和经验稳定性。"
}