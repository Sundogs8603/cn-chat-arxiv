{
    "title": "II-MMR: Identifying and Improving Multi-modal Multi-hop Reasoning in Visual Question Answering",
    "abstract": "arXiv:2402.11058v1 Announce Type: cross  Abstract: Visual Question Answering (VQA) often involves diverse reasoning scenarios across Vision and Language (V&L). Most prior VQA studies, however, have merely focused on assessing the model's overall accuracy without evaluating it on different reasoning cases. Furthermore, some recent works observe that conventional Chain-of-Thought (CoT) prompting fails to generate effective reasoning for VQA, especially for complex scenarios requiring multi-hop reasoning. In this paper, we propose II-MMR, a novel idea to identify and improve multi-modal multi-hop reasoning in VQA. In specific, II-MMR takes a VQA question with an image and finds a reasoning path to reach its answer using two novel language promptings: (i) answer prediction-guided CoT prompt, or (ii) knowledge triplet-guided prompt. II-MMR then analyzes this path to identify different reasoning cases in current VQA benchmarks by estimating how many hops and what types (i.e., visual or beyon",
    "link": "https://arxiv.org/abs/2402.11058",
    "context": "Title: II-MMR: Identifying and Improving Multi-modal Multi-hop Reasoning in Visual Question Answering\nAbstract: arXiv:2402.11058v1 Announce Type: cross  Abstract: Visual Question Answering (VQA) often involves diverse reasoning scenarios across Vision and Language (V&L). Most prior VQA studies, however, have merely focused on assessing the model's overall accuracy without evaluating it on different reasoning cases. Furthermore, some recent works observe that conventional Chain-of-Thought (CoT) prompting fails to generate effective reasoning for VQA, especially for complex scenarios requiring multi-hop reasoning. In this paper, we propose II-MMR, a novel idea to identify and improve multi-modal multi-hop reasoning in VQA. In specific, II-MMR takes a VQA question with an image and finds a reasoning path to reach its answer using two novel language promptings: (i) answer prediction-guided CoT prompt, or (ii) knowledge triplet-guided prompt. II-MMR then analyzes this path to identify different reasoning cases in current VQA benchmarks by estimating how many hops and what types (i.e., visual or beyon",
    "path": "papers/24/02/2402.11058.json",
    "total_tokens": 929,
    "translated_title": "II-MMR：在视觉问答中识别和改进多模态多跳推理",
    "translated_abstract": "视觉问答（VQA）通常涉及视觉和语言之间多样推理场景。然而，大多数先前的VQA研究仅关注评估模型的整体准确性，而没有在不同推理情况下对其进行评估。此外，一些最近的研究发现，传统的\"CoT\"提示无法有效生成VQA的推理，尤其是对于需要多跳推理的复杂场景。在本文中，我们提出了II-MMR，这是一个新颖的想法，用于识别和改进VQA中的多模态多跳推理。具体而言，II-MMR接受带有图像的VQA问题，并使用两种新颖的语言提示找到推理路径以获得答案：(i)答案预测引导的CoT提示，或者(ii)知识三元组引导的提示。然后，II-MMR分析这条路径，通过估计有多少跳和什么类型（即视觉或超出）来识别当前VQA基准中的不同推理情况。",
    "tldr": "II-MMR提出了一种新颖的方法，用于识别和改进视觉问答中的多模态多跳推理，通过引入答案预测引导的CoT提示和知识三元组引导的提示，实现对不同推理情况的分析和识别。",
    "en_tdlr": "II-MMR proposes a novel approach to identify and improve multi-modal multi-hop reasoning in Visual Question Answering, analyzing and identifying different reasoning cases by introducing answer prediction-guided CoT prompt and knowledge triplet-guided prompt."
}