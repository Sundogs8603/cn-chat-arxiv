{
    "title": "LEMMA: Towards LVLM-Enhanced Multimodal Misinformation Detection with External Knowledge Augmentation",
    "abstract": "arXiv:2402.11943v1 Announce Type: new  Abstract: The rise of multimodal misinformation on social platforms poses significant challenges for individuals and societies. Its increased credibility and broader impact compared to textual misinformation make detection complex, requiring robust reasoning across diverse media types and profound knowledge for accurate verification. The emergence of Large Vision Language Model (LVLM) offers a potential solution to this problem. Leveraging their proficiency in processing visual and textual information, LVLM demonstrates promising capabilities in recognizing complex information and exhibiting strong reasoning skills. In this paper, we first investigate the potential of LVLM on multimodal misinformation detection. We find that even though LVLM has a superior performance compared to LLMs, its profound reasoning may present limited power with a lack of evidence. Based on these observations, we propose LEMMA: LVLM-Enhanced Multimodal Misinformation Det",
    "link": "https://arxiv.org/abs/2402.11943",
    "context": "Title: LEMMA: Towards LVLM-Enhanced Multimodal Misinformation Detection with External Knowledge Augmentation\nAbstract: arXiv:2402.11943v1 Announce Type: new  Abstract: The rise of multimodal misinformation on social platforms poses significant challenges for individuals and societies. Its increased credibility and broader impact compared to textual misinformation make detection complex, requiring robust reasoning across diverse media types and profound knowledge for accurate verification. The emergence of Large Vision Language Model (LVLM) offers a potential solution to this problem. Leveraging their proficiency in processing visual and textual information, LVLM demonstrates promising capabilities in recognizing complex information and exhibiting strong reasoning skills. In this paper, we first investigate the potential of LVLM on multimodal misinformation detection. We find that even though LVLM has a superior performance compared to LLMs, its profound reasoning may present limited power with a lack of evidence. Based on these observations, we propose LEMMA: LVLM-Enhanced Multimodal Misinformation Det",
    "path": "papers/24/02/2402.11943.json",
    "total_tokens": 845,
    "translated_title": "LEMMA: 支持外部知识增强的LVLM增强多模态虚假信息检测",
    "translated_abstract": "社交平台上多模态虚假信息的兴起对个人和社会都带来了重大挑战。与文本虚假信息相比，多模态虚假信息具有更高的可信度和更广泛的影响，使得检测变得复杂，需要跨越不同媒体类型进行强大的推理，并具备准确验证的深刻知识。大规模视觉语言模型（LVLM）的出现为解决这一问题提供了潜在方案。利用LVLM在处理视觉和文本信息方面的熟练能力，LVLM在识别复杂信息和展现强大推理能力方面展示出有希望的能力。在本文中，我们首先研究了LVLM在多模态虚假信息检测中的潜力。我们发现，尽管LVLM的性能优于LLMs，但其深刻推理可能由于缺乏证据而表现出有限的效力。基于这些观察，我们提出了LEMMA：LVLM增强多模态虚假信息检测。",
    "tldr": "LVLM-Enhanced Multimodal Misinformation Detection with LEMMA proposes a solution for detecting complex misinformation by enhancing the reasoning capability of Large Vision Language Models.",
    "en_tdlr": "LEMMA enhances LVLM for multimodal misinformation detection, providing a solution for detecting complex misinformation by improving reasoning capabilities."
}