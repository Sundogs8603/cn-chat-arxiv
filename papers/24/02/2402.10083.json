{
    "title": "Fine-tuning Large Language Model (LLM) Artificial Intelligence Chatbots in Ophthalmology and LLM-based evaluation using GPT-4",
    "abstract": "arXiv:2402.10083v1 Announce Type: new  Abstract: Purpose: To assess the alignment of GPT-4-based evaluation to human clinician experts, for the evaluation of responses to ophthalmology-related patient queries generated by fine-tuned LLM chatbots. Methods: 400 ophthalmology questions and paired answers were created by ophthalmologists to represent commonly asked patient questions, divided into fine-tuning (368; 92%), and testing (40; 8%). We find-tuned 5 different LLMs, including LLAMA2-7b, LLAMA2-7b-Chat, LLAMA2-13b, and LLAMA2-13b-Chat. For the testing dataset, additional 8 glaucoma QnA pairs were included. 200 responses to the testing dataset were generated by 5 fine-tuned LLMs for evaluation. A customized clinical evaluation rubric was used to guide GPT-4 evaluation, grounded on clinical accuracy, relevance, patient safety, and ease of understanding. GPT-4 evaluation was then compared against ranking by 5 clinicians for clinical alignment. Results: Among all fine-tuned LLMs, GPT-3.5",
    "link": "https://arxiv.org/abs/2402.10083",
    "context": "Title: Fine-tuning Large Language Model (LLM) Artificial Intelligence Chatbots in Ophthalmology and LLM-based evaluation using GPT-4\nAbstract: arXiv:2402.10083v1 Announce Type: new  Abstract: Purpose: To assess the alignment of GPT-4-based evaluation to human clinician experts, for the evaluation of responses to ophthalmology-related patient queries generated by fine-tuned LLM chatbots. Methods: 400 ophthalmology questions and paired answers were created by ophthalmologists to represent commonly asked patient questions, divided into fine-tuning (368; 92%), and testing (40; 8%). We find-tuned 5 different LLMs, including LLAMA2-7b, LLAMA2-7b-Chat, LLAMA2-13b, and LLAMA2-13b-Chat. For the testing dataset, additional 8 glaucoma QnA pairs were included. 200 responses to the testing dataset were generated by 5 fine-tuned LLMs for evaluation. A customized clinical evaluation rubric was used to guide GPT-4 evaluation, grounded on clinical accuracy, relevance, patient safety, and ease of understanding. GPT-4 evaluation was then compared against ranking by 5 clinicians for clinical alignment. Results: Among all fine-tuned LLMs, GPT-3.5",
    "path": "papers/24/02/2402.10083.json",
    "total_tokens": 1056,
    "translated_title": "在眼科中对大型语言模型（LLM）聊天机器人进行微调，并使用GPT-4进行LLM评估",
    "translated_abstract": "目的：评估基于GPT-4的评估与人类临床专家对经过精调的LLM聊天机器人生成的眼科相关患者查询的回答的一致性。方法：400个眼科问题和配对答案由眼科医生创建，以代表常见的患者问题，分为用于微调的368个（92％）和测试的40个（8％）。我们对5个不同的LLM进行了精调，包括LLAMA2-7b，LLAMA2-7b-Chat，LLAMA2-13b和LLAMA2-13b-Chat。对于测试数据集，还包括8个青光眼问答对。由5个经过精调的LLM生成了200个对测试数据集的回答用于评估。采用定制的临床评估指标来指导GPT-4的评估，以确保临床准确性、相关性、患者安全性和易理解性。然后将GPT-4的评估与5名临床医生的排序进行对比以评估其临床一致性。结果：在所有经过精调的LLM中，GPT-3.5",
    "tldr": "本论文评估了使用GPT-4的LLM评估的临床一致性，以评估经过精调的LLM聊天机器人生成的眼科患者查询的回答。通过与医生排序进行对比，发现GPT-3.5在临床上的一致性比其他经过精调的LLM更高。",
    "en_tdlr": "This paper assesses the clinical alignment of LLM evaluation using GPT-4 for responses generated by fine-tuned LLM chatbots in ophthalmology. Comparing with rankings by clinicians, it is found that GPT-3.5 shows higher clinical alignment than other fine-tuned LLMs."
}