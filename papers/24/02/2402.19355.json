{
    "title": "Unraveling Adversarial Examples against Speaker Identification -- Techniques for Attack Detection and Victim Model Classification",
    "abstract": "arXiv:2402.19355v1 Announce Type: cross  Abstract: Adversarial examples have proven to threaten speaker identification systems, and several countermeasures against them have been proposed. In this paper, we propose a method to detect the presence of adversarial examples, i.e., a binary classifier distinguishing between benign and adversarial examples. We build upon and extend previous work on attack type classification by exploring new architectures. Additionally, we introduce a method for identifying the victim model on which the adversarial attack is carried out. To achieve this, we generate a new dataset containing multiple attacks performed against various victim models. We achieve an AUC of 0.982 for attack detection, with no more than a 0.03 drop in performance for unknown attacks. Our attack classification accuracy (excluding benign) reaches 86.48% across eight attack types using our LightResNet34 architecture, while our victim model classification accuracy reaches 72.28% across",
    "link": "https://arxiv.org/abs/2402.19355",
    "context": "Title: Unraveling Adversarial Examples against Speaker Identification -- Techniques for Attack Detection and Victim Model Classification\nAbstract: arXiv:2402.19355v1 Announce Type: cross  Abstract: Adversarial examples have proven to threaten speaker identification systems, and several countermeasures against them have been proposed. In this paper, we propose a method to detect the presence of adversarial examples, i.e., a binary classifier distinguishing between benign and adversarial examples. We build upon and extend previous work on attack type classification by exploring new architectures. Additionally, we introduce a method for identifying the victim model on which the adversarial attack is carried out. To achieve this, we generate a new dataset containing multiple attacks performed against various victim models. We achieve an AUC of 0.982 for attack detection, with no more than a 0.03 drop in performance for unknown attacks. Our attack classification accuracy (excluding benign) reaches 86.48% across eight attack types using our LightResNet34 architecture, while our victim model classification accuracy reaches 72.28% across",
    "path": "papers/24/02/2402.19355.json",
    "total_tokens": 935,
    "translated_title": "揭示针对说话人识别的对抗样本--攻击检测和受害模型分类技术",
    "translated_abstract": "对抗样本已经被证明对说话人识别系统构成威胁，并已经提出了一些针对它们的对策。本文提出了一种检测对抗样本存在的方法，即通过区分良性样本和对抗样本的二元分类器。我们在攻击类型分类的基础上进行扩展，探索新的架构。此外，我们介绍了一种用于识别进行对抗攻击的受害模型的方法。为了实现这一目标，我们生成了一个包含针对各种受害模型进行多次攻击的新数据集。我们在攻击检测方面实现了0.982的AUC，在未知攻击情况下性能下降不超过0.03。使用我们的LightResNet34架构，我们可以实现在八种攻击类型上达到86.48%的攻击分类准确率（除去良性样本），而我们的受害模型分类准确率达到72.28%。",
    "tldr": "该论文提出了一种检测对抗性样本和识别受害模型的方法，通过扩展先前的攻击类型分类工作和引入新的架构，在攻击检测方面取得了很高的AUC，攻击分类准确率达到86.48%，受害模型分类准确率达到72.28%。"
}