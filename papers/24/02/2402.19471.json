{
    "title": "Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling",
    "abstract": "arXiv:2402.19471v1 Announce Type: cross  Abstract: Questions combine our mastery of language with our remarkable facility for reasoning about uncertainty. How do people navigate vast hypothesis spaces to pose informative questions given limited cognitive resources? We study these tradeoffs in a classic grounded question-asking task based on the board game Battleship. Our language-informed program sampling (LIPS) model uses large language models (LLMs) to generate natural language questions, translate them into symbolic programs, and evaluate their expected information gain. We find that with a surprisingly modest resource budget, this simple Monte Carlo optimization strategy yields informative questions that mirror human performance across varied Battleship board scenarios. In contrast, LLM-only baselines struggle to ground questions in the board state; notably, GPT-4V provides no improvement over non-visual baselines. Our results illustrate how Bayesian models of question-asking can l",
    "link": "https://arxiv.org/abs/2402.19471",
    "context": "Title: Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling\nAbstract: arXiv:2402.19471v1 Announce Type: cross  Abstract: Questions combine our mastery of language with our remarkable facility for reasoning about uncertainty. How do people navigate vast hypothesis spaces to pose informative questions given limited cognitive resources? We study these tradeoffs in a classic grounded question-asking task based on the board game Battleship. Our language-informed program sampling (LIPS) model uses large language models (LLMs) to generate natural language questions, translate them into symbolic programs, and evaluate their expected information gain. We find that with a surprisingly modest resource budget, this simple Monte Carlo optimization strategy yields informative questions that mirror human performance across varied Battleship board scenarios. In contrast, LLM-only baselines struggle to ground questions in the board state; notably, GPT-4V provides no improvement over non-visual baselines. Our results illustrate how Bayesian models of question-asking can l",
    "path": "papers/24/02/2402.19471.json",
    "total_tokens": 880,
    "translated_title": "严格的LIPS沉没舰船：在Battleship中使用语言信息程序抽样提出问题",
    "translated_abstract": "问题结合了我们对语言的掌握和我们对于在有限认知资源情况下推断不确定性的出色能力。人们如何在巨大假设空间中提出信息量丰富的问题？我们研究了这些在基于战舰游戏Battleship的经典提问任务中的权衡。我们的语言信息程序抽样（LIPS）模型利用大型语言模型（LLMs）生成自然语言问题，将其转化为符号程序，并评估其预期信息增益。我们发现，即使在一个令人惊讶的资源预算下，这种简单的蒙特卡罗优化策略也能产生反映人类在各种Battleship棋盘场景中表现的丰富问题。相比之下，仅使用LLM的基线在将问题与棋盘状态联系起来方面存在困难；值得注意的是，GPT-4V并没有比无视觉基线提供改进。我们的结果展示了贝叶斯提问模型如何可能模拟和指导人类的问问题行为。",
    "tldr": "研究利用大型语言模型提出信息量丰富的问题，在Battleship游戏中展示出与人类表现相匹配的效果，并揭示了贝叶斯模型如何指导问问题行为。",
    "en_tdlr": "Study explores generating informative questions with large language models in Battleship game, showing performance matching human behavior and illustrating how Bayesian models can guide question-asking."
}