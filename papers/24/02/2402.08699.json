{
    "title": "Unsupervised Evaluation of Code LLMs with Round-Trip Correctness",
    "abstract": "arXiv:2402.08699v1 Announce Type: cross Abstract: To evaluate code large language models (LLMs), research has relied on a few small manually curated benchmarks, such as HumanEval and MBPP, which represent a narrow part of the real-world software domains. In this work, we introduce round-trip correctness (RTC) as an alternative evaluation method. RTC allows Code LLM evaluation on a broader spectrum of real-world software domains without the need for costly human curation. RTC rests on the idea that we can ask a model to make a prediction (e.g., describe some code using natural language), feed that prediction back (e.g., synthesize code from the predicted description), and check if this round-trip leads to code that is semantically equivalent to the original input. We show how to employ RTC to evaluate code synthesis and editing. We find that RTC strongly correlates with model performance on existing narrow-domain code synthesis benchmarks while allowing us to expand to a much broader se",
    "link": "https://arxiv.org/abs/2402.08699",
    "context": "Title: Unsupervised Evaluation of Code LLMs with Round-Trip Correctness\nAbstract: arXiv:2402.08699v1 Announce Type: cross Abstract: To evaluate code large language models (LLMs), research has relied on a few small manually curated benchmarks, such as HumanEval and MBPP, which represent a narrow part of the real-world software domains. In this work, we introduce round-trip correctness (RTC) as an alternative evaluation method. RTC allows Code LLM evaluation on a broader spectrum of real-world software domains without the need for costly human curation. RTC rests on the idea that we can ask a model to make a prediction (e.g., describe some code using natural language), feed that prediction back (e.g., synthesize code from the predicted description), and check if this round-trip leads to code that is semantically equivalent to the original input. We show how to employ RTC to evaluate code synthesis and editing. We find that RTC strongly correlates with model performance on existing narrow-domain code synthesis benchmarks while allowing us to expand to a much broader se",
    "path": "papers/24/02/2402.08699.json",
    "total_tokens": 919,
    "translated_title": "无监督评估具有往返正确性的代码LLMs",
    "translated_abstract": "为了评估代码大型语言模型（LLMs），研究一直依赖于一些小的手动策划的基准，如HumanEval和MBPP，这些基准只代表了真实世界软件领域的一个狭窄部分。在这项工作中，我们引入了往返正确性（RTC）作为一种替代评估方法。RTC允许在更广泛的真实世界软件领域对代码LLM进行评估，而无需昂贵的人工策划。RTC的基本思想是我们可以要求模型做出预测（例如用自然语言描述一些代码），将该预测返回（例如从预测的描述中合成代码），并检查这个往返过程是否导致与原始输入语义等效的代码。我们展示了如何利用RTC来评估代码合成和编辑。我们发现RTC与现有狭窄领域代码合成基准上的模型性能强相关，同时也允许我们扩展到更广阔的领域。",
    "tldr": "无需人工策划，我们提出了往返正确性（RTC）作为评估代码大型语言模型（LLMs）的替代方法，RTC可以在更广泛的真实世界软件领域对代码进行评估，并且与现有基准具有强相关性。",
    "en_tdlr": "Without the need for expensive human curation, we propose round-trip correctness (RTC) as an alternative evaluation method for code large language models (LLMs). RTC allows for code evaluation in a broader spectrum of real-world software domains and shows strong correlation with existing benchmarks."
}