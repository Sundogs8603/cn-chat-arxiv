{
    "title": "A Survey on Data Selection for Language Models",
    "abstract": "arXiv:2402.16827v1 Announce Type: new  Abstract: A major factor in the recent success of large language models is the use of enormous and ever-growing text datasets for unsupervised pre-training. However, naively training a model on all available data may not be optimal (or feasible), as the quality of available text data can vary. Filtering out data can also decrease the carbon footprint and financial costs of training models by reducing the amount of training required.   Data selection methods aim to determine which candidate data points to include in the training dataset and how to appropriately sample from the selected data points. The promise of improved data selection methods has caused the volume of research in the area to rapidly expand. However, because deep learning is mostly driven by empirical evidence and experimentation on large-scale data is expensive, few organizations have the resources for extensive data selection research. Consequently, knowledge of effective data se",
    "link": "https://arxiv.org/abs/2402.16827",
    "context": "Title: A Survey on Data Selection for Language Models\nAbstract: arXiv:2402.16827v1 Announce Type: new  Abstract: A major factor in the recent success of large language models is the use of enormous and ever-growing text datasets for unsupervised pre-training. However, naively training a model on all available data may not be optimal (or feasible), as the quality of available text data can vary. Filtering out data can also decrease the carbon footprint and financial costs of training models by reducing the amount of training required.   Data selection methods aim to determine which candidate data points to include in the training dataset and how to appropriately sample from the selected data points. The promise of improved data selection methods has caused the volume of research in the area to rapidly expand. However, because deep learning is mostly driven by empirical evidence and experimentation on large-scale data is expensive, few organizations have the resources for extensive data selection research. Consequently, knowledge of effective data se",
    "path": "papers/24/02/2402.16827.json",
    "total_tokens": 849,
    "translated_title": "语言模型数据选择概述",
    "translated_abstract": "最近大型语言模型取得成功的一个主要因素是利用巨大且不断增长的文本数据集进行无监督预训练。然而，简单地在所有可用数据上训练模型可能并不是最佳选择（或不可行），因为可用文本数据的质量可能有所不同。数据过滤也可以通过减少所需的训练量来降低训练模型的碳足迹和财务成本。数据选择方法旨在确定要包括在训练数据集中的哪些候选数据点，以及如何从所选数据点中适当采样。改进的数据选择方法的前景已经导致该领域的研究量迅速扩大。然而，由于深度学习主要受实证证据驱动，对大规模数据进行实验成本昂贵，很少有组织拥有资源进行广泛的数据选择研究。因此，有效数据选择的知识可能大多局限于大型技术公司或研究机构内部。",
    "tldr": "大型语言模型成功的关键在于使用大规模的文本数据集进行无监督预训练，但如何优化选择数据以降低碳足迹和财务成本仍是一个挑战。",
    "en_tdlr": "The key to the success of large language models lies in unsupervised pre-training with massive text datasets, but optimizing data selection to reduce carbon footprint and financial costs remains a challenge."
}