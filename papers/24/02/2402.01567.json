{
    "title": "Understanding Adam Optimizer via Online Learning of Updates: Adam is FTRL in Disguise",
    "abstract": "Despite the success of the Adam optimizer in practice, the theoretical understanding of its algorithmic components still remains limited. In particular, most existing analyses of Adam show the convergence rate that can be simply achieved by non-adative algorithms like SGD. In this work, we provide a different perspective based on online learning that underscores the importance of Adam's algorithmic components. Inspired by Cutkosky et al. (2023), we consider the framework called online learning of updates, where we choose the updates of an optimizer based on an online learner. With this framework, the design of a good optimizer is reduced to the design of a good online learner. Our main observation is that Adam corresponds to a principled online learning framework called Follow-the-Regularized-Leader (FTRL). Building on this observation, we study the benefits of its algorithmic components from the online learning perspective.",
    "link": "https://rss.arxiv.org/abs/2402.01567",
    "context": "Title: Understanding Adam Optimizer via Online Learning of Updates: Adam is FTRL in Disguise\nAbstract: Despite the success of the Adam optimizer in practice, the theoretical understanding of its algorithmic components still remains limited. In particular, most existing analyses of Adam show the convergence rate that can be simply achieved by non-adative algorithms like SGD. In this work, we provide a different perspective based on online learning that underscores the importance of Adam's algorithmic components. Inspired by Cutkosky et al. (2023), we consider the framework called online learning of updates, where we choose the updates of an optimizer based on an online learner. With this framework, the design of a good optimizer is reduced to the design of a good online learner. Our main observation is that Adam corresponds to a principled online learning framework called Follow-the-Regularized-Leader (FTRL). Building on this observation, we study the benefits of its algorithmic components from the online learning perspective.",
    "path": "papers/24/02/2402.01567.json",
    "total_tokens": 841,
    "translated_title": "通过在线学习更新理解Adam优化器：Adam是伪装成FTRL的",
    "translated_abstract": "尽管Adam优化器在实践中取得了成功，但对其算法组成的理论理解仍然有限。特别是，大多数现有的对Adam的分析仅显示了可以简单地通过非自适应算法如SGD实现的收敛速度。在本文中，我们提供了一种基于在线学习的不同视角，强调了Adam算法的重要性。受Cutkosky等人（2023）的启发，我们考虑了一个称为在线学习更新的框架，其中我们根据在线学习者选择优化器的更新。在这个框架下，设计一个好的优化器就等同于设计一个好的在线学习者。我们的主要观察是，Adam对应于一种被称为Follow-the-Regularized-Leader (FTRL)的原则性在线学习框架。基于这一观察，我们从在线学习的角度研究了其算法组成的好处。",
    "tldr": "本文通过在线学习的视角，揭示了Adam优化器的构成和算法组成的重要性，发现Adam实际上是伪装成FTRL的，研究了在线学习的角度对其算法组成的好处。",
    "en_tdlr": "This paper provides a different perspective on understanding the Adam optimizer, revealing the importance of its algorithmic components through the lens of online learning. It discovers that Adam is actually disguised as FTRL and explores the benefits of its algorithmic components from an online learning perspective."
}