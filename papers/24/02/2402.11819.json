{
    "title": "Head-wise Shareable Attention for Large Language Models",
    "abstract": "arXiv:2402.11819v1 Announce Type: new  Abstract: Large Language Models (LLMs) suffer from huge number of parameters, which restricts their deployment on edge devices. Weight sharing is one promising solution that encourages weight reuse, effectively reducing memory usage with less performance drop. However, current weight sharing techniques primarily focus on small-scale models like BERT and employ coarse-grained sharing rules, e.g., layer-wise. This becomes limiting given the prevalence of LLMs and sharing an entire layer or block obviously diminishes the flexibility of weight sharing. In this paper, we present a perspective on $\\textit{$\\textbf{head-wise shareable attention for large language models}$}$. We further propose two memory-efficient methods that share parameters across attention heads, with a specific focus on LLMs. Both of them use the same dynamic strategy to select the shared weight matrices. The first method directly reuses the pre-trained weights without retraining, d",
    "link": "https://arxiv.org/abs/2402.11819",
    "context": "Title: Head-wise Shareable Attention for Large Language Models\nAbstract: arXiv:2402.11819v1 Announce Type: new  Abstract: Large Language Models (LLMs) suffer from huge number of parameters, which restricts their deployment on edge devices. Weight sharing is one promising solution that encourages weight reuse, effectively reducing memory usage with less performance drop. However, current weight sharing techniques primarily focus on small-scale models like BERT and employ coarse-grained sharing rules, e.g., layer-wise. This becomes limiting given the prevalence of LLMs and sharing an entire layer or block obviously diminishes the flexibility of weight sharing. In this paper, we present a perspective on $\\textit{$\\textbf{head-wise shareable attention for large language models}$}$. We further propose two memory-efficient methods that share parameters across attention heads, with a specific focus on LLMs. Both of them use the same dynamic strategy to select the shared weight matrices. The first method directly reuses the pre-trained weights without retraining, d",
    "path": "papers/24/02/2402.11819.json",
    "total_tokens": 871,
    "translated_title": "大语言模型的适用于头部共享注意力",
    "translated_abstract": "大型语言模型(LLMs)由于参数数量巨大受到限制，这限制了它们在边缘设备上的部署。参数共享是一种有利的解决方案，鼓励权重重用，有效地减少内存使用量并降低性能下降。然而，当前的参数共享技术主要专注于像BERT这样的小规模模型，并采用粗粒度的共享规则，例如逐层共享。鉴于LLMs的普及，这变得有限，并且共享整个层或块显然降低了参数共享的灵活性。在本文中，我们提出了$\\textbf{面向大语言模型的头部共享注意力}$的观点。我们进一步提出了两种在注意力头之间共享参数的内存高效方法，特别关注LLMs。它们都使用相同的动态策略选择共享的参数矩阵。第一种方法直接重复使用预训练权重而无需重新训练。",
    "tldr": "本文提出了面向大语言模型的头部共享注意力的观点，提出了两种在注意力头之间共享参数的内存高效方法，以解决大型语言模型参数数量巨大导致部署受限的问题。"
}