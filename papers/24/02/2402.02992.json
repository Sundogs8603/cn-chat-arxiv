{
    "title": "Decoding-time Realignment of Language Models",
    "abstract": "Aligning language models with human preferences is crucial for reducing errors and biases in these models. Alignment techniques, such as reinforcement learning from human feedback (RLHF), are typically cast as optimizing a tradeoff between human preference rewards and a proximity regularization term that encourages staying close to the unaligned model. Selecting an appropriate level of regularization is critical: insufficient regularization can lead to reduced model capabilities due to reward hacking, whereas excessive regularization hinders alignment. Traditional methods for finding the optimal regularization level require retraining multiple models with varying regularization strengths. This process, however, is resource-intensive, especially for large models. To address this challenge, we propose decoding-time realignment (DeRa), a simple method to explore and evaluate different regularization strengths in aligned models without retraining. DeRa enables control over the degree of al",
    "link": "https://arxiv.org/abs/2402.02992",
    "context": "Title: Decoding-time Realignment of Language Models\nAbstract: Aligning language models with human preferences is crucial for reducing errors and biases in these models. Alignment techniques, such as reinforcement learning from human feedback (RLHF), are typically cast as optimizing a tradeoff between human preference rewards and a proximity regularization term that encourages staying close to the unaligned model. Selecting an appropriate level of regularization is critical: insufficient regularization can lead to reduced model capabilities due to reward hacking, whereas excessive regularization hinders alignment. Traditional methods for finding the optimal regularization level require retraining multiple models with varying regularization strengths. This process, however, is resource-intensive, especially for large models. To address this challenge, we propose decoding-time realignment (DeRa), a simple method to explore and evaluate different regularization strengths in aligned models without retraining. DeRa enables control over the degree of al",
    "path": "papers/24/02/2402.02992.json",
    "total_tokens": 802,
    "translated_title": "论文标题：解码时间对齐的语言模型",
    "translated_abstract": "将语言模型与人类偏好对齐对于减少模型中的错误和偏差非常重要。对齐技术，如从人类反馈中进行的强化学习（RLHF），通常被视为在人类偏好奖励和鼓励保持与未对齐模型接近的接近性规则项之间进行优化的权衡。选择适当的规则化水平至关重要：规则化不足可能导致由于奖励欺骗而降低模型能力，而过度规则化则阻碍对齐。传统方法找到最佳规则化水平需要使用不同规则化强度重新训练多个模型。然而，这个过程耗费资源，特别是对于大型模型来说。为了解决这个挑战，我们提出了解码时间对齐（DeRa），一种简单的方法，在无需重新训练的情况下探索和评估不同的规则化强度。DeRa可以对对齐模型的程度进行控制。",
    "tldr": "本研究提出了解码时间对齐（DeRa）方法，可以在不重新训练模型的情况下探索和评估不同的规则化强度，从而对齐语言模型和人类偏好。"
}