{
    "title": "Contrastive Diffuser: Planning Towards High Return States via Contrastive Learning",
    "abstract": "Applying diffusion models in reinforcement learning for long-term planning has gained much attention recently. Several diffusion-based methods have successfully leveraged the modeling capabilities of diffusion for arbitrary distributions. These methods generate subsequent trajectories for planning and have demonstrated significant improvement. However, these methods are limited by their plain base distributions and their overlooking of the diversity of samples, in which different states have different returns. They simply leverage diffusion to learn the distribution of offline dataset, generate the trajectories whose states share the same distribution with the offline dataset. As a result, the probability of these models reaching the high-return states is largely dependent on the dataset distribution. Even equipped with the guidance model, the performance is still suppressed. To address these limitations, in this paper, we propose a novel method called CDiffuser, which devises a return",
    "link": "https://arxiv.org/abs/2402.02772",
    "context": "Title: Contrastive Diffuser: Planning Towards High Return States via Contrastive Learning\nAbstract: Applying diffusion models in reinforcement learning for long-term planning has gained much attention recently. Several diffusion-based methods have successfully leveraged the modeling capabilities of diffusion for arbitrary distributions. These methods generate subsequent trajectories for planning and have demonstrated significant improvement. However, these methods are limited by their plain base distributions and their overlooking of the diversity of samples, in which different states have different returns. They simply leverage diffusion to learn the distribution of offline dataset, generate the trajectories whose states share the same distribution with the offline dataset. As a result, the probability of these models reaching the high-return states is largely dependent on the dataset distribution. Even equipped with the guidance model, the performance is still suppressed. To address these limitations, in this paper, we propose a novel method called CDiffuser, which devises a return",
    "path": "papers/24/02/2402.02772.json",
    "total_tokens": 806,
    "translated_title": "对比扩散器：通过对比学习规划高回报状态",
    "translated_abstract": "最近在强化学习中应用扩散模型进行长期规划引起了广泛关注。几种基于扩散的方法成功地利用了扩散的建模能力进行任意分布的规划。这些方法为规划生成了后续轨迹，并取得了显著改进。然而，这些方法受到基础分布的限制，并忽视了样本的多样性，在这些方法中，不同状态具有不同的回报。它们仅仅利用扩散模型来学习离线数据集的分布，并生成与离线数据集具有相同分布的轨迹。因此，这些模型到达高回报状态的概率在很大程度上依赖于数据集的分布。即使配备了引导模型，性能仍然受到压制。针对这些限制，本文提出了一种名为CDiffuser的新方法，设计了一个返回函数",
    "tldr": "在强化学习中应用扩散模型进行规划常受限于基础分布和样本多样性。本文提出的CDiffuser方法通过对比学习来提高到达高回报状态的概率。",
    "en_tdlr": "Applying diffusion models for planning in reinforcement learning is limited by base distribution and sample diversity. The proposed CDiffuser method aims to improve the probability of reaching high-return states through contrastive learning."
}