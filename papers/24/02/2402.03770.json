{
    "title": "Fed-CVLC: Compressing Federated Learning Communications with Variable-Length Codes",
    "abstract": "In Federated Learning (FL) paradigm, a parameter server (PS) concurrently communicates with distributed participating clients for model collection, update aggregation, and model distribution over multiple rounds, without touching private data owned by individual clients. FL is appealing in preserving data privacy; yet the communication between the PS and scattered clients can be a severe bottleneck. Model compression algorithms, such as quantization and sparsification, have been suggested but they generally assume a fixed code length, which does not reflect the heterogeneity and variability of model updates. In this paper, through both analysis and experiments, we show strong evidences that variable-length is beneficial for compression in FL. We accordingly present Fed-CVLC (Federated Learning Compression with Variable-Length Codes), which fine-tunes the code length in response of the dynamics of model updates. We develop optimal tuning strategy that minimizes the loss function (equiva",
    "link": "https://arxiv.org/abs/2402.03770",
    "context": "Title: Fed-CVLC: Compressing Federated Learning Communications with Variable-Length Codes\nAbstract: In Federated Learning (FL) paradigm, a parameter server (PS) concurrently communicates with distributed participating clients for model collection, update aggregation, and model distribution over multiple rounds, without touching private data owned by individual clients. FL is appealing in preserving data privacy; yet the communication between the PS and scattered clients can be a severe bottleneck. Model compression algorithms, such as quantization and sparsification, have been suggested but they generally assume a fixed code length, which does not reflect the heterogeneity and variability of model updates. In this paper, through both analysis and experiments, we show strong evidences that variable-length is beneficial for compression in FL. We accordingly present Fed-CVLC (Federated Learning Compression with Variable-Length Codes), which fine-tunes the code length in response of the dynamics of model updates. We develop optimal tuning strategy that minimizes the loss function (equiva",
    "path": "papers/24/02/2402.03770.json",
    "total_tokens": 834,
    "translated_title": "Fed-CVLC:使用可变长度编码压缩联邦学习通信",
    "translated_abstract": "在联邦学习（FL）范 paradigm 下，一个参数服务器（PS）同时与分布式参与的客户端进行通信，进行模型收集、更新聚合和模型分发，同时不接触个别客户端拥有的私有数据。FL 在保护数据隐私方面具有吸引力；然而，PS 与分散客户端之间的通信可能成为严重的瓶颈。已经提出了模型压缩算法，如量化和稀疏化，但它们一般假设了固定的代码长度，这不反映模型更新的异质性和可变性。本文通过分析和实验证明了在 FL 中可变长度对于压缩是有益的。因此，我们提出了 Fed-CVLC（带有可变长度编码的联邦学习压缩），它根据模型更新的动态对代码长度进行微调。我们开发了最优的调整策略，最小化损失函数（等价于 ...",
    "tldr": "通过分析和实验证明，在联邦学习中使用可变长度编码可以有效压缩通信。本文提出了Fed-CVLC，可以根据模型更新的动态进行代码长度微调。"
}