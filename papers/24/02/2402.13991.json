{
    "title": "Analysing The Impact of Sequence Composition on Language Model Pre-Training",
    "abstract": "arXiv:2402.13991v1 Announce Type: new  Abstract: Most language model pre-training frameworks concatenate multiple documents into fixed-length sequences and use causal masking to compute the likelihood of each token given its context; this strategy is widely adopted due to its simplicity and efficiency. However, to this day, the influence of the pre-training sequence composition strategy on the generalisation properties of the model remains under-explored. In this work, we find that applying causal masking can lead to the inclusion of distracting information from previous documents during pre-training, which negatively impacts the performance of the models on language modelling and downstream tasks. In intra-document causal masking, the likelihood of each token is only conditioned on the previous tokens in the same document, eliminating potential distracting information from previous documents and significantly improving performance. Furthermore, we find that concatenating related docum",
    "link": "https://arxiv.org/abs/2402.13991",
    "context": "Title: Analysing The Impact of Sequence Composition on Language Model Pre-Training\nAbstract: arXiv:2402.13991v1 Announce Type: new  Abstract: Most language model pre-training frameworks concatenate multiple documents into fixed-length sequences and use causal masking to compute the likelihood of each token given its context; this strategy is widely adopted due to its simplicity and efficiency. However, to this day, the influence of the pre-training sequence composition strategy on the generalisation properties of the model remains under-explored. In this work, we find that applying causal masking can lead to the inclusion of distracting information from previous documents during pre-training, which negatively impacts the performance of the models on language modelling and downstream tasks. In intra-document causal masking, the likelihood of each token is only conditioned on the previous tokens in the same document, eliminating potential distracting information from previous documents and significantly improving performance. Furthermore, we find that concatenating related docum",
    "path": "papers/24/02/2402.13991.json",
    "total_tokens": 877,
    "translated_title": "分析序列组成对语言模型预训练的影响",
    "translated_abstract": "大多数语言模型预训练框架将多个文档连接成固定长度的序列，并使用因果遮盖来计算每个标记在给定上下文下的可能性；这种策略由于简单和高效而被广泛采用。然而，迄今为止，预训练序列组成策略对模型的泛化特性的影响仍未被深入研究。 在这项工作中，我们发现应用因果遮盖可能导致在预训练过程中包括来自之前文档的干扰信息，从而对模型在语言建模和下游任务上的性能产生负面影响。 在内部文档因果遮盖中，每个标记的可能性仅取决于同一文档中的先前标记，消除了来自之前文档的潜在干扰信息并显著提高了性能。 此外，我们发现连接相关文档",
    "tldr": "序列组成对语言模型预训练的影响一直未被深入探讨，在这项研究中发现，应用内部文档因果遮盖可以消除来自之前文档的干扰信息，显著提高模型在语言建模和下游任务中的性能。",
    "en_tdlr": "The impact of sequence composition on language model pre-training has been under-explored, and in this study, it was found that applying intra-document causal masking can eliminate distracting information from previous documents, significantly improving the model's performance in language modeling and downstream tasks."
}