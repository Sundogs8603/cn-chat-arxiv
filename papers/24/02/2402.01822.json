{
    "title": "Building Guardrails for Large Language Models",
    "abstract": "As Large Language Models (LLMs) become more integrated into our daily lives, it is crucial to identify and mitigate their risks, especially when the risks can have profound impacts on human users and societies. Guardrails, which filter the inputs or outputs of LLMs, have emerged as a core safeguarding technology. This position paper takes a deep look at current open-source solutions (Llama Guard, Nvidia NeMo, Guardrails AI), and discusses the challenges and the road towards building more complete solutions. Drawing on robust evidence from previous research, we advocate for a systematic approach to construct guardrails for LLMs, based on comprehensive consideration of diverse contexts across various LLMs applications. We propose employing socio-technical methods through collaboration with a multi-disciplinary team to pinpoint precise technical requirements, exploring advanced neural-symbolic implementations to embrace the complexity of the requirements, and developing verification and t",
    "link": "https://arxiv.org/abs/2402.01822",
    "context": "Title: Building Guardrails for Large Language Models\nAbstract: As Large Language Models (LLMs) become more integrated into our daily lives, it is crucial to identify and mitigate their risks, especially when the risks can have profound impacts on human users and societies. Guardrails, which filter the inputs or outputs of LLMs, have emerged as a core safeguarding technology. This position paper takes a deep look at current open-source solutions (Llama Guard, Nvidia NeMo, Guardrails AI), and discusses the challenges and the road towards building more complete solutions. Drawing on robust evidence from previous research, we advocate for a systematic approach to construct guardrails for LLMs, based on comprehensive consideration of diverse contexts across various LLMs applications. We propose employing socio-technical methods through collaboration with a multi-disciplinary team to pinpoint precise technical requirements, exploring advanced neural-symbolic implementations to embrace the complexity of the requirements, and developing verification and t",
    "path": "papers/24/02/2402.01822.json",
    "total_tokens": 869,
    "translated_title": "为大型语言模型构建防护措施",
    "translated_abstract": "随着大型语言模型（LLM）越来越多地融入我们的日常生活中，识别和减轻它们的风险变得至关重要，特别是当这些风险对人类用户和社会产生深远影响时。防护措施，即过滤LLM的输入或输出，已经成为一种核心的安全技术。本文深入研究了当前的开源解决方案（Llama Guard，Nvidia NeMo，Guardrails AI），讨论了构建更完整解决方案的挑战和路径。基于前期研究的有力证据，我们倡导采用系统化方法构建LLM的防护措施，全面考虑不同LLM应用的多样化上下文。我们建议通过与多学科团队的合作，采用社会技术方法来确定精确的技术要求，探索面向需求复杂性的先进神经符号实现，并开展验证和测试。",
    "tldr": "本文旨在为大型语言模型构建防护措施，并倡导采用系统化方法，通过与多学科团队合作来确定精确的技术要求，以减轻LLM的风险，并全面考虑不同LLM应用的多样化上下文。"
}