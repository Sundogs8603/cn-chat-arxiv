{
    "title": "Sinkhorn Distance Minimization for Knowledge Distillation",
    "abstract": "arXiv:2402.17110v1 Announce Type: new  Abstract: Knowledge distillation (KD) has been widely adopted to compress large language models (LLMs). Existing KD methods investigate various divergence measures including the Kullback-Leibler (KL), reverse Kullback-Leibler (RKL), and Jensen-Shannon (JS) divergences. However, due to limitations inherent in their assumptions and definitions, these measures fail to deliver effective supervision when few distribution overlap exists between the teacher and the student. In this paper, we show that the aforementioned KL, RKL, and JS divergences respectively suffer from issues of mode-averaging, mode-collapsing, and mode-underestimation, which deteriorates logits-based KD for diverse NLP tasks. We propose the Sinkhorn Knowledge Distillation (SinKD) that exploits the Sinkhorn distance to ensure a nuanced and precise assessment of the disparity between teacher and student distributions. Besides, profit by properties of the Sinkhorn metric, we can get rid",
    "link": "https://arxiv.org/abs/2402.17110",
    "context": "Title: Sinkhorn Distance Minimization for Knowledge Distillation\nAbstract: arXiv:2402.17110v1 Announce Type: new  Abstract: Knowledge distillation (KD) has been widely adopted to compress large language models (LLMs). Existing KD methods investigate various divergence measures including the Kullback-Leibler (KL), reverse Kullback-Leibler (RKL), and Jensen-Shannon (JS) divergences. However, due to limitations inherent in their assumptions and definitions, these measures fail to deliver effective supervision when few distribution overlap exists between the teacher and the student. In this paper, we show that the aforementioned KL, RKL, and JS divergences respectively suffer from issues of mode-averaging, mode-collapsing, and mode-underestimation, which deteriorates logits-based KD for diverse NLP tasks. We propose the Sinkhorn Knowledge Distillation (SinKD) that exploits the Sinkhorn distance to ensure a nuanced and precise assessment of the disparity between teacher and student distributions. Besides, profit by properties of the Sinkhorn metric, we can get rid",
    "path": "papers/24/02/2402.17110.json",
    "total_tokens": 842,
    "translated_title": "Sinkhorn Distance Minimization for Knowledge Distillation",
    "translated_abstract": "知识蒸馏（KD）被广泛采用来压缩大型语言模型（LLMs）。现有的KD方法研究包括Kullback-Leibler（KL）、反向Kullback-Leibler（RKL）和Jensen-Shannon（JS）散度在内的各种散度度量。然而，由于它们的假设和定义中固有的限制，这些度量在教师和学生之间存在少量分布重叠时未能提供有效的监督。本文表明，前述的KL、RKL和JS散度分别存在模式平均、模式坍塌和模式低估的问题，这恶化了基于logits的多样NLP任务的KD。我们提出了利用Sinkhorn距离的Sinkhorn知识蒸馏（SinKD），以确保对教师和学生分布之间差异的细致和准确评估。此外，由于Sinkhorn度量的属性，我们可以摆脱",
    "tldr": "提出了Sinkhorn Knowledge Distillation（SinKD）来解决知识蒸馏过程中散度度量存在的问题，确保对教师和学生分布之间差异的准确评估",
    "en_tdlr": "Introducing Sinkhorn Knowledge Distillation (SinKD) to address issues in divergence measures during knowledge distillation, ensuring accurate assessment of the disparity between teacher and student distributions."
}