{
    "title": "Leveraging LLMs for Unsupervised Dense Retriever Ranking",
    "abstract": "This paper introduces a novel unsupervised technique that utilizes large language models (LLMs) to determine the most suitable dense retriever for a specific test(target) corpus. Selecting the appropriate dense retriever is vital for numerous IR applications that employ these retrievers, trained on public datasets, to encode or conduct searches within a new private target corpus. The effectiveness of a dense retriever can significantly diminish when applied to a target corpus that diverges in domain or task from the original training set. The problem becomes more pronounced in cases where the target corpus is unlabeled, e.g. in zero-shot scenarios, rendering direct evaluation of the model's effectiveness on the target corpus unattainable. Therefore, the unsupervised selection of an optimally pre-trained dense retriever, especially under conditions of domain shift, emerges as a critical challenge. Existing methodologies for ranking dense retrievers fall short in addressing these domain ",
    "link": "https://arxiv.org/abs/2402.04853",
    "context": "Title: Leveraging LLMs for Unsupervised Dense Retriever Ranking\nAbstract: This paper introduces a novel unsupervised technique that utilizes large language models (LLMs) to determine the most suitable dense retriever for a specific test(target) corpus. Selecting the appropriate dense retriever is vital for numerous IR applications that employ these retrievers, trained on public datasets, to encode or conduct searches within a new private target corpus. The effectiveness of a dense retriever can significantly diminish when applied to a target corpus that diverges in domain or task from the original training set. The problem becomes more pronounced in cases where the target corpus is unlabeled, e.g. in zero-shot scenarios, rendering direct evaluation of the model's effectiveness on the target corpus unattainable. Therefore, the unsupervised selection of an optimally pre-trained dense retriever, especially under conditions of domain shift, emerges as a critical challenge. Existing methodologies for ranking dense retrievers fall short in addressing these domain ",
    "path": "papers/24/02/2402.04853.json",
    "total_tokens": 872,
    "translated_title": "利用LLMs进行无监督的密集检索排名",
    "translated_abstract": "本文介绍了一种利用大型语言模型（LLMs）确定特定测试（目标）语料库最合适的密集检索器的新颖无监督技术。选择合适的密集检索器对于许多采用这些在公开数据集上训练的检索器进行编码或在新的私有目标语料库中进行搜索的信息检索应用程序至关重要。当密集检索器应用于与原始训练集在领域或任务上有差异的目标语料库时，其有效性可能会大大降低。在目标语料库没有标注的情况下，例如在零样本场景中，无法直接评估模型在目标语料库上的效果。因此，无监督选择最佳预训练的密集检索器，特别是在领域迁移条件下，成为一个关键挑战。现有的密集检索器排序方法在解决这些领域迁移问题方面存在不足。",
    "tldr": "本文介绍了一种利用大型语言模型（LLMs）进行无监督选择最佳预训练的密集检索器的新技术。选择合适的检索器对于应用于新的目标语料库并且存在领域转移的情况非常重要。",
    "en_tdlr": "This paper introduces a novel technique that utilizes large language models (LLMs) for unsupervised selection of the optimal pre-trained dense retriever. Selecting the appropriate retriever is crucial for applications where there is a domain shift in the target corpus."
}