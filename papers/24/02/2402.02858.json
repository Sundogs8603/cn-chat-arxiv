{
    "title": "Deep autoregressive density nets vs neural ensembles for model-based offline reinforcement learning",
    "abstract": "We consider the problem of offline reinforcement learning where only a set of system transitions is made available for policy optimization. Following recent advances in the field, we consider a model-based reinforcement learning algorithm that infers the system dynamics from the available data and performs policy optimization on imaginary model rollouts. This approach is vulnerable to exploiting model errors which can lead to catastrophic failures on the real system. The standard solution is to rely on ensembles for uncertainty heuristics and to avoid exploiting the model where it is too uncertain. We challenge the popular belief that we must resort to ensembles by showing that better performance can be obtained with a single well-calibrated autoregressive model on the D4RL benchmark. We also analyze static metrics of model-learning and conclude on the important model properties for the final performance of the agent.",
    "link": "https://arxiv.org/abs/2402.02858",
    "context": "Title: Deep autoregressive density nets vs neural ensembles for model-based offline reinforcement learning\nAbstract: We consider the problem of offline reinforcement learning where only a set of system transitions is made available for policy optimization. Following recent advances in the field, we consider a model-based reinforcement learning algorithm that infers the system dynamics from the available data and performs policy optimization on imaginary model rollouts. This approach is vulnerable to exploiting model errors which can lead to catastrophic failures on the real system. The standard solution is to rely on ensembles for uncertainty heuristics and to avoid exploiting the model where it is too uncertain. We challenge the popular belief that we must resort to ensembles by showing that better performance can be obtained with a single well-calibrated autoregressive model on the D4RL benchmark. We also analyze static metrics of model-learning and conclude on the important model properties for the final performance of the agent.",
    "path": "papers/24/02/2402.02858.json",
    "total_tokens": 954,
    "translated_title": "深度自回归密度网络与神经集合在基于模型的离线强化学习中的对比",
    "translated_abstract": "我们考虑仅有系统转换集合可用于策略优化的离线强化学习问题。在最近的研究进展中，我们考虑了一种基于模型的强化学习算法，该算法从可用数据中推断系统动态，并在模型推演上进行策略优化。这种方法容易受到模型误差的影响，可能会导致在真实系统上的灾难性失败。标准解决方案是依靠集合进行不确定性启发式，并避免在模型不确定性太大时利用模型。通过展示在D4RL基准测试上使用单个良好校准的自回归模型可以获得更好的性能，我们质疑了必须使用集合的普遍观点。我们还分析了与模型学习有关的静态指标，并得出了关于代理的最终性能的重要模型特性的结论。",
    "tldr": "本文对比了在基于模型的离线强化学习中，使用深度自回归密度网络和神经集合的方法。通过在D4RL基准测试上展示，我们质疑了使用神经集合的普遍观点，并发现单个良好校准的自回归模型可以获得更好的性能。同时，我们还分析了模型学习的静态指标，并得出了关于代理最终性能的重要模型特性。",
    "en_tdlr": "This paper compares the use of deep autoregressive density nets and neural ensembles for model-based offline reinforcement learning. By challenging the popular belief and demonstrating better performance with a single well-calibrated autoregressive model on the D4RL benchmark, it questions the necessity of using ensembles. The paper also analyzes static metrics of model-learning and concludes on the important model properties for the agent's final performance."
}