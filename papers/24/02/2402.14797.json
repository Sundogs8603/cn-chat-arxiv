{
    "title": "Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video Synthesis",
    "abstract": "arXiv:2402.14797v1 Announce Type: cross  Abstract: Contemporary models for generating images show remarkable quality and versatility. Swayed by these advantages, the research community repurposes them to generate videos. Since video content is highly redundant, we argue that naively bringing advances of image models to the video generation domain reduces motion fidelity, visual quality and impairs scalability. In this work, we build Snap Video, a video-first model that systematically addresses these challenges. To do that, we first extend the EDM framework to take into account spatially and temporally redundant pixels and naturally support video generation. Second, we show that a U-Net - a workhorse behind image generation - scales poorly when generating videos, requiring significant computational overhead. Hence, we propose a new transformer-based architecture that trains 3.31 times faster than U-Nets (and is ~4.5 faster at inference). This allows us to efficiently train a text-to-vid",
    "link": "https://arxiv.org/abs/2402.14797",
    "context": "Title: Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video Synthesis\nAbstract: arXiv:2402.14797v1 Announce Type: cross  Abstract: Contemporary models for generating images show remarkable quality and versatility. Swayed by these advantages, the research community repurposes them to generate videos. Since video content is highly redundant, we argue that naively bringing advances of image models to the video generation domain reduces motion fidelity, visual quality and impairs scalability. In this work, we build Snap Video, a video-first model that systematically addresses these challenges. To do that, we first extend the EDM framework to take into account spatially and temporally redundant pixels and naturally support video generation. Second, we show that a U-Net - a workhorse behind image generation - scales poorly when generating videos, requiring significant computational overhead. Hence, we propose a new transformer-based architecture that trains 3.31 times faster than U-Nets (and is ~4.5 faster at inference). This allows us to efficiently train a text-to-vid",
    "path": "papers/24/02/2402.14797.json",
    "total_tokens": 878,
    "translated_title": "Snap Video: 规模化时空Transformer用于文本到视频合成",
    "translated_abstract": "当代生成图像的模型展现出卓越的质量和多功能性。受到这些优势的影响，研究团体重新运用它们来生成视频。由于视频内容高度冗余，我们认为单纯地将图像模型的进展带到视频生成领域会降低动态保真度、视觉质量并影响可扩展性。在这项研究中，我们构建了Snap Video，一个以视频为先的模型，系统地解决了这些挑战。为此，我们首先扩展EDM框架以考虑时空冗余像素并自然地支持视频生成。其次，我们展示了U-Net - 图像生成背后的得力工具 - 在生成视频时扩展性较差，需要显著的计算开销。因此，我们提出了一种新的基于Transformer的架构，训练速度比U-Nets快3.31倍（推理速度约快4.5倍）。这使我们能够高效地训练一个文本到视频的模型。",
    "tldr": "Snap Video是一个视频优先模型，通过扩展EDM框架并提出基于Transformer的架构，解决了文本到视频合成中的动态保真度、视觉质量和可扩展性挑战。",
    "en_tdlr": "Snap Video is a video-first model that addresses the challenges of motion fidelity, visual quality, and scalability in text-to-video synthesis by extending the EDM framework and proposing a Transformer-based architecture."
}