{
    "title": "Beyond Uniform Scaling: Exploring Depth Heterogeneity in Neural Architectures",
    "abstract": "arXiv:2402.12418v1 Announce Type: cross  Abstract: Conventional scaling of neural networks typically involves designing a base network and growing different dimensions like width, depth, etc. of the same by some predefined scaling factors. We introduce an automated scaling approach leveraging second-order loss landscape information. Our method is flexible towards skip connections a mainstay in modern vision transformers. Our training-aware method jointly scales and trains transformers without additional training iterations. Motivated by the hypothesis that not all neurons need uniform depth complexity, our approach embraces depth heterogeneity. Extensive evaluations on DeiT-S with ImageNet100 show a 2.5% accuracy gain and 10% parameter efficiency improvement over conventional scaling. Scaled networks demonstrate superior performance upon training small scale datasets from scratch. We introduce the first intact scaling mechanism for vision transformers, a step towards efficient model sc",
    "link": "https://arxiv.org/abs/2402.12418",
    "context": "Title: Beyond Uniform Scaling: Exploring Depth Heterogeneity in Neural Architectures\nAbstract: arXiv:2402.12418v1 Announce Type: cross  Abstract: Conventional scaling of neural networks typically involves designing a base network and growing different dimensions like width, depth, etc. of the same by some predefined scaling factors. We introduce an automated scaling approach leveraging second-order loss landscape information. Our method is flexible towards skip connections a mainstay in modern vision transformers. Our training-aware method jointly scales and trains transformers without additional training iterations. Motivated by the hypothesis that not all neurons need uniform depth complexity, our approach embraces depth heterogeneity. Extensive evaluations on DeiT-S with ImageNet100 show a 2.5% accuracy gain and 10% parameter efficiency improvement over conventional scaling. Scaled networks demonstrate superior performance upon training small scale datasets from scratch. We introduce the first intact scaling mechanism for vision transformers, a step towards efficient model sc",
    "path": "papers/24/02/2402.12418.json",
    "total_tokens": 860,
    "translated_title": "超越统一缩放：探索神经架构中的深度异质性",
    "translated_abstract": "传统的神经网络缩放通常涉及设计基本网络，并通过一些预定义的缩放因子增加不同维度（如宽度、深度等）。我们引入了一种利用二阶损失景观信息的自动缩放方法。我们的方法对现代视觉transformers中的跳过连接具有灵活性。我们的训练感知方法同时扩展和训练transformers，而无需额外的训练迭代。受到并非所有神经元都需要统一深度复杂性的假设启发，我们的方法采用深度异质性。对DeiT-S在ImageNet100上进行的广泛评估显示比传统缩放提高了2.5％的准确性并提高了10％的参数效率。在从头开始训练小规模数据集时，缩放的网络表现出色。我们引入了视觉transformers的第一个完整缩放机制，这是朝向高效模型场景的一步。",
    "tldr": "引入了一种基于二阶损失景观信息的自动缩放方法，同时扩展和训练transformers，提出了神经架构中的深度异质性概念，并在ImageNet100上实现了准确性和参数效率的提升。"
}