{
    "title": "Non-Exchangeable Conformal Language Generation with Nearest Neighbors",
    "abstract": "Quantifying uncertainty in automatically generated text is important for letting humans check potential hallucinations and making systems more reliable. Conformal prediction is an attractive framework to provide predictions imbued with statistical guarantees, however, its application to text generation is challenging since any i.i.d. assumptions are not realistic. In this paper, we bridge this gap by leveraging recent results on non-exchangeable conformal prediction, which still ensures bounds on coverage. The result, non-exchangeable conformal nucleus sampling, is a novel extension of the conformal prediction framework to generation based on nearest neighbors. Our method can be used post-hoc for an arbitrary model without extra training and supplies token-level, calibrated prediction sets equipped with statistical guarantees. Experiments in machine translation and language modeling show encouraging results in generation quality. By also producing tighter prediction sets with good cove",
    "link": "https://arxiv.org/abs/2402.00707",
    "context": "Title: Non-Exchangeable Conformal Language Generation with Nearest Neighbors\nAbstract: Quantifying uncertainty in automatically generated text is important for letting humans check potential hallucinations and making systems more reliable. Conformal prediction is an attractive framework to provide predictions imbued with statistical guarantees, however, its application to text generation is challenging since any i.i.d. assumptions are not realistic. In this paper, we bridge this gap by leveraging recent results on non-exchangeable conformal prediction, which still ensures bounds on coverage. The result, non-exchangeable conformal nucleus sampling, is a novel extension of the conformal prediction framework to generation based on nearest neighbors. Our method can be used post-hoc for an arbitrary model without extra training and supplies token-level, calibrated prediction sets equipped with statistical guarantees. Experiments in machine translation and language modeling show encouraging results in generation quality. By also producing tighter prediction sets with good cove",
    "path": "papers/24/02/2402.00707.json",
    "total_tokens": 853,
    "translated_title": "非交换的共形语言生成与最近邻",
    "translated_abstract": "在自动生成的文本中量化不确定性对于让人们检查潜在的错觉和使系统更可靠是很重要的。共形预测是一个有吸引力的框架，能够提供带有统计保证的预测，然而，将其应用于文本生成是具有挑战性的，因为任何独立同分布的假设都是不现实的。在本文中，我们通过利用最近关于非交换的共形预测的结果来填补这一差距，该方法仍然确保覆盖范围。结果--非交换的共形核采样，是对基于最近邻的生成的共形预测框架的一种新颖扩展。我们的方法可以用于任意模型的事后处理，无需额外训练，并提供带有统计保证的标记级别、校准的预测集。在机器翻译和语言建模的实验中，我们展示了令人鼓舞的生成质量结果。通过同时产生具有良好覆盖度的更紧密的预测集，",
    "tldr": "本文介绍了一种利用最近邻方法扩展的非交换的共形语言生成框架，用于量化自动生成文本的不确定性，并提供带有统计保证的预测集。",
    "en_tdlr": "This paper presents a non-exchangeable conformal language generation framework based on nearest neighbors, which quantifies uncertainty in automatically generated text and provides prediction sets with statistical guarantees."
}