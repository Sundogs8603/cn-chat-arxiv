{
    "title": "Panacea: Pareto Alignment via Preference Adaptation for LLMs",
    "abstract": "Current methods for large language model alignment typically use scalar human preference labels. However, this convention tends to oversimplify the multi-dimensional and heterogeneous nature of human preferences, leading to reduced expressivity and even misalignment. This paper presents Panacea, an innovative approach that reframes alignment as a multi-dimensional preference optimization problem. Panacea trains a single model capable of adapting online and Pareto-optimally to diverse sets of preferences without the need for further tuning. A major challenge here is using a low-dimensional preference vector to guide the model's behavior, despite it being governed by an overwhelmingly large number of parameters. To address this, Panacea is designed to use singular value decomposition (SVD)-based low-rank adaptation, which allows the preference vector to be simply injected online as singular values. Theoretically, we prove that Panacea recovers the entire Pareto front with common loss agg",
    "link": "https://arxiv.org/abs/2402.02030",
    "context": "Title: Panacea: Pareto Alignment via Preference Adaptation for LLMs\nAbstract: Current methods for large language model alignment typically use scalar human preference labels. However, this convention tends to oversimplify the multi-dimensional and heterogeneous nature of human preferences, leading to reduced expressivity and even misalignment. This paper presents Panacea, an innovative approach that reframes alignment as a multi-dimensional preference optimization problem. Panacea trains a single model capable of adapting online and Pareto-optimally to diverse sets of preferences without the need for further tuning. A major challenge here is using a low-dimensional preference vector to guide the model's behavior, despite it being governed by an overwhelmingly large number of parameters. To address this, Panacea is designed to use singular value decomposition (SVD)-based low-rank adaptation, which allows the preference vector to be simply injected online as singular values. Theoretically, we prove that Panacea recovers the entire Pareto front with common loss agg",
    "path": "papers/24/02/2402.02030.json",
    "total_tokens": 890,
    "translated_title": "Panacea: 通过偏好适应实现 Pareto 对齐的 LLMS",
    "translated_abstract": "当前的大型语言模型对齐方法通常使用标量人类偏好标签。然而，这种约定倾向于过度简化人类偏好的多维和异质性特性，导致表达能力降低甚至失配。本文提出了一种创新的方法 Panacea，将对齐重新定义为多维偏好优化问题。Panacea 训练了一个单一模型，能够在线适应并 Pareto 最优地满足各种偏好集，而无需进一步的调整。一个主要挑战是使用低维偏好向量来引导模型的行为，尽管模型由数量庞大的参数所控制。为了解决这个问题，Panacea 被设计为使用基于奇异值分解（SVD）的低秩适应，可以将偏好向量作为奇异值简单在线注入。从理论上讲，我们证明了 Panacea 能够恢复整个 Pareto 前沿与常见损失聚合。",
    "tldr": "Panacea 是一种创新方法，将大型语言模型对齐重新定义为多维偏好优化问题，通过使用奇异值分解的低秩适应，以在线注入偏好向量的形式，使模型能够适应并 Pareto 最优地满足各种偏好集。",
    "en_tdlr": "Panacea is an innovative approach that reframes large language model alignment as a multi-dimensional preference optimization problem. It uses low-rank adaptation based on singular value decomposition to inject preference vectors online, enabling the model to adapt and Pareto-optimize to diverse sets of preferences."
}