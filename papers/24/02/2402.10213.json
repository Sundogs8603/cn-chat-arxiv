{
    "title": "Clustering Inductive Biases with Unrolled Networks",
    "abstract": "arXiv:2402.10213v1 Announce Type: cross  Abstract: The classical sparse coding (SC) model represents visual stimuli as a linear combination of a handful of learned basis functions that are Gabor-like when trained on natural image data. However, the Gabor-like filters learned by classical sparse coding far overpredict well-tuned simple cell receptive field profiles observed empirically. While neurons fire sparsely, neuronal populations are also organized in physical space by their sensitivity to certain features. In V1, this organization is a smooth progression of orientations along the cortical sheet. A number of subsequent models have either discarded the sparse dictionary learning framework entirely or whose updates have yet to take advantage of the surge in unrolled, neural dictionary learning architectures. A key missing theme of these updates is a stronger notion of \\emph{structured sparsity}. We propose an autoencoder architecture (WLSC) whose latent representations are implicitl",
    "link": "https://arxiv.org/abs/2402.10213",
    "context": "Title: Clustering Inductive Biases with Unrolled Networks\nAbstract: arXiv:2402.10213v1 Announce Type: cross  Abstract: The classical sparse coding (SC) model represents visual stimuli as a linear combination of a handful of learned basis functions that are Gabor-like when trained on natural image data. However, the Gabor-like filters learned by classical sparse coding far overpredict well-tuned simple cell receptive field profiles observed empirically. While neurons fire sparsely, neuronal populations are also organized in physical space by their sensitivity to certain features. In V1, this organization is a smooth progression of orientations along the cortical sheet. A number of subsequent models have either discarded the sparse dictionary learning framework entirely or whose updates have yet to take advantage of the surge in unrolled, neural dictionary learning architectures. A key missing theme of these updates is a stronger notion of \\emph{structured sparsity}. We propose an autoencoder architecture (WLSC) whose latent representations are implicitl",
    "path": "papers/24/02/2402.10213.json",
    "total_tokens": 675,
    "translated_title": "使用展开网络对聚类归纳偏好进行建模",
    "translated_abstract": "经典的稀疏编码（SC）模型将视觉刺激表示为少量学习基函数的线性组合，在对自然图像数据进行训练时，这些基函数类似于Gabor。然而，经典稀疏编码学习的类Gabor滤波器远远超过了实际观察到的简单细胞感受野轮廓的良好预测。我们提出了一种自动编码器架构（WLSC），其潜在表示是隐含的",
    "tldr": "提出了一种自动编码器架构（WLSC），其潜在表示是隐含的，用于对聚类归纳偏好进行建模",
    "en_tdlr": "Proposed an autoencoder architecture (WLSC) with implicit latent representations for modeling clustering inductive biases."
}