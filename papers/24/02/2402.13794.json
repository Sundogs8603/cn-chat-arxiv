{
    "title": "Revisiting Convergence of AdaGrad with Relaxed Assumptions",
    "abstract": "arXiv:2402.13794v1 Announce Type: cross  Abstract: In this study, we revisit the convergence of AdaGrad with momentum (covering AdaGrad as a special case) on non-convex smooth optimization problems. We consider a general noise model where the noise magnitude is controlled by the function value gap together with the gradient magnitude. This model encompasses a broad range of noises including bounded noise, sub-Gaussian noise, affine variance noise and the expected smoothness, and it has been shown to be more realistic in many practical applications. Our analysis yields a probabilistic convergence rate which, under the general noise, could reach at (\\tilde{\\mathcal{O}}(1/\\sqrt{T})). This rate does not rely on prior knowledge of problem-parameters and could accelerate to (\\tilde{\\mathcal{O}}(1/T)) where (T) denotes the total number iterations, when the noise parameters related to the function value gap and noise level are sufficiently small. The convergence rate thus matches the lower rat",
    "link": "https://arxiv.org/abs/2402.13794",
    "context": "Title: Revisiting Convergence of AdaGrad with Relaxed Assumptions\nAbstract: arXiv:2402.13794v1 Announce Type: cross  Abstract: In this study, we revisit the convergence of AdaGrad with momentum (covering AdaGrad as a special case) on non-convex smooth optimization problems. We consider a general noise model where the noise magnitude is controlled by the function value gap together with the gradient magnitude. This model encompasses a broad range of noises including bounded noise, sub-Gaussian noise, affine variance noise and the expected smoothness, and it has been shown to be more realistic in many practical applications. Our analysis yields a probabilistic convergence rate which, under the general noise, could reach at (\\tilde{\\mathcal{O}}(1/\\sqrt{T})). This rate does not rely on prior knowledge of problem-parameters and could accelerate to (\\tilde{\\mathcal{O}}(1/T)) where (T) denotes the total number iterations, when the noise parameters related to the function value gap and noise level are sufficiently small. The convergence rate thus matches the lower rat",
    "path": "papers/24/02/2402.13794.json",
    "total_tokens": 909,
    "translated_title": "重新审视AdaGrad在宽松假设下的收敛性",
    "translated_abstract": "在这项研究中，我们重新审视了AdaGrad在非凸光滑优化问题上的收敛性，包括AdaGrad作为一种特殊情况。我们考虑了一个通用的噪声模型，其中噪声的大小由函数值差和梯度大小控制。这个模型涵盖了广泛范围的噪声，包括有界噪声、次高斯噪声、仿射方差噪声和预期光滑度，并且在许多实际应用中被证明更加现实。我们的分析得出了一个概率收敛速度，根据通用噪声，可以达到( \\tilde{\\mathcal{O}}(1/\\sqrt{T}))。这个速度不依赖于先前对问题参数的了解，当与函数值差和噪声水平相关的参数足够小时，它可以加速到(\\tilde{\\mathcal{O}}(1/T))，其中(T)表示总迭代次数。收敛速度因此匹配了下限速度。",
    "tldr": "重新审视了AdaGrad在非凸光滑优化问题上的收敛性，提出了通用噪声模型，得出了概率收敛速度，无需先验知识，且可以在噪声参数足够小时加速至更快的速度。",
    "en_tdlr": "Revisiting the convergence of AdaGrad on non-convex smooth optimization problems, proposing a general noise model, deriving a probabilistic convergence rate without prior knowledge, and achieving faster convergence when noise parameters are small enough."
}