{
    "title": "Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals",
    "abstract": "arXiv:2402.11655v1 Announce Type: new  Abstract: Interpretability research aims to bridge the gap between the empirical success and our scientific understanding of the inner workings of large language models (LLMs). However, most existing research in this area focused on analyzing a single mechanism, such as how models copy or recall factual knowledge. In this work, we propose the formulation of competition of mechanisms, which instead of individual mechanisms focuses on the interplay of multiple mechanisms, and traces how one of them becomes dominant in the final prediction. We uncover how and where the competition of mechanisms happens within LLMs using two interpretability methods, logit inspection and attention modification. Our findings show traces of the mechanisms and their competition across various model components, and reveal attention positions that effectively control the strength of certain mechanisms. Our code and data are at https://github.com/francescortu/Competition_of",
    "link": "https://arxiv.org/abs/2402.11655",
    "context": "Title: Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals\nAbstract: arXiv:2402.11655v1 Announce Type: new  Abstract: Interpretability research aims to bridge the gap between the empirical success and our scientific understanding of the inner workings of large language models (LLMs). However, most existing research in this area focused on analyzing a single mechanism, such as how models copy or recall factual knowledge. In this work, we propose the formulation of competition of mechanisms, which instead of individual mechanisms focuses on the interplay of multiple mechanisms, and traces how one of them becomes dominant in the final prediction. We uncover how and where the competition of mechanisms happens within LLMs using two interpretability methods, logit inspection and attention modification. Our findings show traces of the mechanisms and their competition across various model components, and reveal attention positions that effectively control the strength of certain mechanisms. Our code and data are at https://github.com/francescortu/Competition_of",
    "path": "papers/24/02/2402.11655.json",
    "total_tokens": 805,
    "translated_title": "机制之争：追踪语言模型处理事实和虚拟语境的方式",
    "translated_abstract": "可解释性研究旨在弥合大型语言模型（LLMs）的经验成功和我们对内部机制的科学理解之间的差距。本研究提出了机制之争的形式，其不再关注单个机制，而是关注多个机制之间的相互作用，并追踪其中一个在最终预测中如何成为主导因素。我们利用logit检验和注意力修改两种可解释性方法揭示了机制之争在LLMs中的发生方式和位置。我们的发现显示了机制及其在各种模型组件中的竞争痕迹，并揭示了有效控制特定机制强度的注意力位置。我们的代码和数据位于https://github.com/francescortu/Competition_of",
    "tldr": "本研究提出了机制之争的概念，关注语言模型中多个机制的相互作用，并揭示了它们之间的竞争过程，以及影响某些机制强度的注意力位置。"
}