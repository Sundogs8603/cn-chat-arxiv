{
    "title": "Evading Data Contamination Detection for Language Models is (too) Easy",
    "abstract": "Large language models are widespread, with their performance on benchmarks frequently guiding user preferences for one model over another. However, the vast amount of data these models are trained on can inadvertently lead to contamination with public benchmarks, thus compromising performance measurements. While recently developed contamination detection methods try to address this issue, they overlook the possibility of deliberate contamination by malicious model providers aiming to evade detection. We argue that this setting is of crucial importance as it casts doubt on the reliability of public benchmarks. To more rigorously study this issue, we propose a categorization of both model providers and contamination detection methods. This reveals vulnerabilities in existing methods that we exploit with EAL, a simple yet effective contamination technique that significantly inflates benchmark performance while completely evading current detection methods.",
    "link": "https://arxiv.org/abs/2402.02823",
    "context": "Title: Evading Data Contamination Detection for Language Models is (too) Easy\nAbstract: Large language models are widespread, with their performance on benchmarks frequently guiding user preferences for one model over another. However, the vast amount of data these models are trained on can inadvertently lead to contamination with public benchmarks, thus compromising performance measurements. While recently developed contamination detection methods try to address this issue, they overlook the possibility of deliberate contamination by malicious model providers aiming to evade detection. We argue that this setting is of crucial importance as it casts doubt on the reliability of public benchmarks. To more rigorously study this issue, we propose a categorization of both model providers and contamination detection methods. This reveals vulnerabilities in existing methods that we exploit with EAL, a simple yet effective contamination technique that significantly inflates benchmark performance while completely evading current detection methods.",
    "path": "papers/24/02/2402.02823.json",
    "total_tokens": 847,
    "translated_title": "逃避语言模型数据污染检测（太）容易",
    "translated_abstract": "大型语言模型广泛使用，它们在基准测试上的性能经常指导用户对一个模型与另一个模型的偏好。然而，这些模型所训练的大量数据可能会意外地与公共基准测试数据发生污染，从而损害性能评估。尽管最近开发了一些污染检测方法来解决这个问题，但它们忽视了恶意模型提供者有意进行污染以避免被检测的可能性。我们认为这种情况非常重要，因为它对公共基准测试的可信度产生了怀疑。为了更严格地研究这个问题，我们提出了模型提供者和污染检测方法的分类，这揭示了现有方法中的漏洞，我们通过使用EAL这种简单而有效的污染技术，明显提高了基准测试的性能，并完全逃避了当前的检测方法。",
    "tldr": "本研究指出语言模型数据污染的检测方法在面对恶意模型提供者的有意污染时存在漏洞，并提出了一种简单而有效的污染技术（EAL）来显著提高基准测试性能且逃避当前的检测方法。",
    "en_tdlr": "This study highlights vulnerabilities in contamination detection methods for language models when facing deliberate contamination and proposes a simple yet effective contamination technique (EAL) that significantly enhances benchmark performance while evading current detection methods."
}