{
    "title": "Chain-of-Thought Reasoning Without Prompting",
    "abstract": "arXiv:2402.10200v1 Announce Type: new  Abstract: In enhancing the reasoning capabilities of large language models (LLMs), prior research primarily focuses on specific prompting techniques such as few-shot or zero-shot chain-of-thought (CoT) prompting. These methods, while effective, often involve manually intensive prompt engineering. Our study takes a novel approach by asking: Can LLMs reason effectively without prompting? Our findings reveal that, intriguingly, CoT reasoning paths can be elicited from pre-trained LLMs by simply altering the \\textit{decoding} process. Rather than conventional greedy decoding, we investigate the top-$k$ alternative tokens, uncovering that CoT paths are frequently inherent in these sequences. This approach not only bypasses the confounders of prompting but also allows us to assess the LLMs' \\textit{intrinsic} reasoning abilities. Moreover, we observe that the presence of a CoT in the decoding path correlates with a higher confidence in the model's decod",
    "link": "https://arxiv.org/abs/2402.10200",
    "context": "Title: Chain-of-Thought Reasoning Without Prompting\nAbstract: arXiv:2402.10200v1 Announce Type: new  Abstract: In enhancing the reasoning capabilities of large language models (LLMs), prior research primarily focuses on specific prompting techniques such as few-shot or zero-shot chain-of-thought (CoT) prompting. These methods, while effective, often involve manually intensive prompt engineering. Our study takes a novel approach by asking: Can LLMs reason effectively without prompting? Our findings reveal that, intriguingly, CoT reasoning paths can be elicited from pre-trained LLMs by simply altering the \\textit{decoding} process. Rather than conventional greedy decoding, we investigate the top-$k$ alternative tokens, uncovering that CoT paths are frequently inherent in these sequences. This approach not only bypasses the confounders of prompting but also allows us to assess the LLMs' \\textit{intrinsic} reasoning abilities. Moreover, we observe that the presence of a CoT in the decoding path correlates with a higher confidence in the model's decod",
    "path": "papers/24/02/2402.10200.json",
    "total_tokens": 863,
    "translated_title": "不需要提示的思维链推理",
    "translated_abstract": "在提升大型语言模型（LLMs）的推理能力方面，以往的研究主要集中在特定的提示技术，如少样本或零样本的思维链提示。这些方法虽然有效，但往往需要手动进行大量的提示工程。我们的研究采用了一种新的方法：LLMs是否可以在没有提示的情况下有效推理？我们的研究结果表明，有趣的是，通过简单地改变解码过程，就可以从预训练的LLMs中引出思维链推理路径。我们研究了前$k$个替代标记，发现这些序列中经常存在思维链路径。这种方法不仅绕过了提示的混淆因素，还可以评估LLMs的内在推理能力。此外，我们观察到解码路径中的思维链存在与模型解码的置信度较高的相关性。",
    "tldr": "本研究发现，通过改变解码过程而不是使用提示工程，可以从预训练的大型语言模型中引出思维链推理路径，这种方法绕过了提示的限制，并且可以评估模型的内在推理能力。",
    "en_tdlr": "This study discovers that by altering the decoding process instead of using prompt engineering, chain-of-thought reasoning paths can be elicited from pre-trained large language models. This approach bypasses the limitations of prompting and allows for the evaluation of the model's intrinsic reasoning abilities."
}