{
    "title": "Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)",
    "abstract": "arXiv:2402.10376v1 Announce Type: new  Abstract: CLIP embeddings have demonstrated remarkable performance across a wide range of computer vision tasks. However, these high-dimensional, dense vector representations are not easily interpretable, restricting their usefulness in downstream applications that require transparency. In this work, we empirically show that CLIP's latent space is highly structured, and consequently that CLIP representations can be decomposed into their underlying semantic components. We leverage this understanding to propose a novel method, Sparse Linear Concept Embeddings (SpLiCE), for transforming CLIP representations into sparse linear combinations of human-interpretable concepts. Distinct from previous work, SpLiCE does not require concept labels and can be applied post hoc. Through extensive experimentation with multiple real-world datasets, we validate that the representations output by SpLiCE can explain and even replace traditional dense CLIP representati",
    "link": "https://arxiv.org/abs/2402.10376",
    "context": "Title: Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)\nAbstract: arXiv:2402.10376v1 Announce Type: new  Abstract: CLIP embeddings have demonstrated remarkable performance across a wide range of computer vision tasks. However, these high-dimensional, dense vector representations are not easily interpretable, restricting their usefulness in downstream applications that require transparency. In this work, we empirically show that CLIP's latent space is highly structured, and consequently that CLIP representations can be decomposed into their underlying semantic components. We leverage this understanding to propose a novel method, Sparse Linear Concept Embeddings (SpLiCE), for transforming CLIP representations into sparse linear combinations of human-interpretable concepts. Distinct from previous work, SpLiCE does not require concept labels and can be applied post hoc. Through extensive experimentation with multiple real-world datasets, we validate that the representations output by SpLiCE can explain and even replace traditional dense CLIP representati",
    "path": "papers/24/02/2402.10376.json",
    "total_tokens": 807,
    "translated_title": "用稀疏线性概念嵌入（SpLiCE）解释CLIP",
    "translated_abstract": "CLIP嵌入在各种计算机视觉任务中表现出色，但这些高维稠密向量表示并不容易解释，限制了它们在需要透明度的下游应用中的实用性。本文经验性地展示了CLIP的潜在空间高度结构化，因此可以将CLIP表示分解为其潜在语义组件。我们利用这一理解提出了一种新方法，稀疏线性概念嵌入（SpLiCE），用于将CLIP表示转换为人可解释概念的稀疏线性组合。与先前的工作不同，SpLiCE不需要概念标签，并且可以后期应用。通过对多个真实世界数据集进行广泛实验，我们验证了SpLiCE输出的表示可以解释甚至取代传统的密集CLIP表示。",
    "tldr": "本研究提出了一种新方法，Sparse Linear Concept Embeddings（SpLiCE），通过将CLIP表示转换为人可解释概念的稀疏线性组合，实现了对CLIP嵌入的解释。",
    "en_tdlr": "This work introduces a novel method, Sparse Linear Concept Embeddings (SpLiCE), which transforms CLIP representations into sparse linear combinations of human-interpretable concepts, enabling the interpretation of CLIP embeddings."
}