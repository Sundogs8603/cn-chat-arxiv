{
    "title": "Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding",
    "abstract": "To combat the memory bandwidth-bound nature of autoregressive LLM inference, previous research has proposed the speculative decoding framework. To perform speculative decoding, a small draft model proposes candidate continuations of the input sequence, that are then verified in parallel by the base model. One way to specify the draft model, as used in the recent Medusa decoding framework, is as a collection of light-weight heads, called draft heads, that operate on the base model's hidden states. To date, all existing draft heads have been sequentially independent, meaning that they speculate tokens in the candidate continuation independently of any preceding tokens in the candidate continuation. In this work, we propose Hydra heads, a sequentially dependent, drop-in replacement for standard draft heads that significantly improves speculation accuracy. Decoding with Hydra heads improves throughput compared to Medusa decoding with standard draft heads. We further explore the design spac",
    "link": "https://arxiv.org/abs/2402.05109",
    "context": "Title: Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding\nAbstract: To combat the memory bandwidth-bound nature of autoregressive LLM inference, previous research has proposed the speculative decoding framework. To perform speculative decoding, a small draft model proposes candidate continuations of the input sequence, that are then verified in parallel by the base model. One way to specify the draft model, as used in the recent Medusa decoding framework, is as a collection of light-weight heads, called draft heads, that operate on the base model's hidden states. To date, all existing draft heads have been sequentially independent, meaning that they speculate tokens in the candidate continuation independently of any preceding tokens in the candidate continuation. In this work, we propose Hydra heads, a sequentially dependent, drop-in replacement for standard draft heads that significantly improves speculation accuracy. Decoding with Hydra heads improves throughput compared to Medusa decoding with standard draft heads. We further explore the design spac",
    "path": "papers/24/02/2402.05109.json",
    "total_tokens": 862,
    "translated_title": "Hydra: 循序依赖的草稿头部用于Medusa解码",
    "translated_abstract": "为了解决自回归LLM推理的内存带宽限制问题，先前的研究提出了推测解码框架。为了执行推测解码，一个小的草稿模型提出了输入序列的候选延续，然后由基础模型并行验证。在最近的Medusa解码框架中，一种指定草稿模型的方法是将其作为一组称为草稿头部的轻量级头部，这些头部对基础模型的隐藏状态进行操作。迄今为止，所有现有的草稿头部都是顺序独立的，这意味着它们在候选延续中的令牌推测与候选延续中的任何前面的令牌无关。在这项工作中，我们提出了循序依赖的Hydra heads，它们是标准草稿头部的可替换组件，显著提高了推测准确性。使用Hydra heads进行解码比使用标准草稿头部的Medusa解码具有更高的吞吐量。我们进一步研究了设计空间。",
    "tldr": "Hydra heads是一种循序依赖的草稿头部，取代了标准草稿头部，显著提高了推测准确性，在Medusa解码中具有更高的吞吐量。",
    "en_tdlr": "Hydra heads are sequentially dependent draft heads that replace standard draft heads, significantly improving speculation accuracy and throughput in Medusa decoding."
}