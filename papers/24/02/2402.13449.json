{
    "title": "CAMELoT: Towards Large Language Models with Training-Free Consolidated Associative Memory",
    "abstract": "arXiv:2402.13449v1 Announce Type: new  Abstract: Large Language Models (LLMs) struggle to handle long input sequences due to high memory and runtime costs. Memory-augmented models have emerged as a promising solution to this problem, but current methods are hindered by limited memory capacity and require costly re-training to integrate with a new LLM. In this work, we introduce an associative memory module which can be coupled to any pre-trained (frozen) attention-based LLM without re-training, enabling it to handle arbitrarily long input sequences. Unlike previous methods, our associative memory module consolidates representations of individual tokens into a non-parametric distribution model, dynamically managed by properly balancing the novelty and recency of the incoming data. By retrieving information from this consolidated associative memory, the base LLM can achieve significant (up to 29.7% on Arxiv) perplexity reduction in long-context modeling compared to other baselines evalua",
    "link": "https://arxiv.org/abs/2402.13449",
    "context": "Title: CAMELoT: Towards Large Language Models with Training-Free Consolidated Associative Memory\nAbstract: arXiv:2402.13449v1 Announce Type: new  Abstract: Large Language Models (LLMs) struggle to handle long input sequences due to high memory and runtime costs. Memory-augmented models have emerged as a promising solution to this problem, but current methods are hindered by limited memory capacity and require costly re-training to integrate with a new LLM. In this work, we introduce an associative memory module which can be coupled to any pre-trained (frozen) attention-based LLM without re-training, enabling it to handle arbitrarily long input sequences. Unlike previous methods, our associative memory module consolidates representations of individual tokens into a non-parametric distribution model, dynamically managed by properly balancing the novelty and recency of the incoming data. By retrieving information from this consolidated associative memory, the base LLM can achieve significant (up to 29.7% on Arxiv) perplexity reduction in long-context modeling compared to other baselines evalua",
    "path": "papers/24/02/2402.13449.json",
    "total_tokens": 886,
    "translated_title": "朝着无需训练的巨型语言模型与训练自由化的关联内存模块",
    "translated_abstract": "大型语言模型在处理长输入序列时面临内存和运行时间成本高的问题。增强记忆模型已经成为解决这一问题的有希望的方法，但目前的方法受限于有限的记忆容量，并且需要昂贵的重新训练才能与新的LLM集成。在这项工作中，我们引入了一个关联内存模块，可以与任何预先训练（冻结）的基于注意力的LLM耦合，无需重新训练，使其能够处理任意长的输入序列。与先前的方法不同，我们的关联内存模块将单个标记的表示合并到一个非参数分布模型中，通过适当平衡传入数据的新颖性和最近性进行动态管理。通过从这个合并的关联内存中检索信息，基本LLM可以在长上下文建模中显著减少（高达Arxiv的29.7％）与其他基线评估相比的困惑度。",
    "tldr": "引入了一个关联内存模块，可以无需重新训练即可与任何预先训练的大型语言模型耦合，解决了长输入序列处理问题，并在长上下文建模中显著降低困惑度。"
}