{
    "title": "Can Large Multimodal Models Uncover Deep Semantics Behind Images?",
    "abstract": "arXiv:2402.11281v1 Announce Type: new  Abstract: Understanding the deep semantics of images is essential in the era dominated by social media. However, current research works primarily on the superficial description of images, revealing a notable deficiency in the systematic investigation of the inherent deep semantics. In this work, we introduce DEEPEVAL, a comprehensive benchmark to assess Large Multimodal Models' (LMMs) capacities of visual deep semantics. DEEPEVAL includes human-annotated dataset and three progressive subtasks: fine-grained description selection, in-depth title matching, and deep semantics understanding. Utilizing DEEPEVAL, we evaluate 9 open-source LMMs and GPT-4V(ision).Our evaluation demonstrates a substantial gap between the deep semantic comprehension capabilities of existing LMMs and humans. For example, GPT-4V is 30% behind humans in understanding deep semantics, even though it achieves human-comparable performance in image description. Further analysis indi",
    "link": "https://arxiv.org/abs/2402.11281",
    "context": "Title: Can Large Multimodal Models Uncover Deep Semantics Behind Images?\nAbstract: arXiv:2402.11281v1 Announce Type: new  Abstract: Understanding the deep semantics of images is essential in the era dominated by social media. However, current research works primarily on the superficial description of images, revealing a notable deficiency in the systematic investigation of the inherent deep semantics. In this work, we introduce DEEPEVAL, a comprehensive benchmark to assess Large Multimodal Models' (LMMs) capacities of visual deep semantics. DEEPEVAL includes human-annotated dataset and three progressive subtasks: fine-grained description selection, in-depth title matching, and deep semantics understanding. Utilizing DEEPEVAL, we evaluate 9 open-source LMMs and GPT-4V(ision).Our evaluation demonstrates a substantial gap between the deep semantic comprehension capabilities of existing LMMs and humans. For example, GPT-4V is 30% behind humans in understanding deep semantics, even though it achieves human-comparable performance in image description. Further analysis indi",
    "path": "papers/24/02/2402.11281.json",
    "total_tokens": 912,
    "translated_title": "大型多模态模型能揭示图像背后的深层语义吗？",
    "translated_abstract": "理解图像的深层语义在社交媒体主导的时代至关重要。然而，当前研究主要集中在对图像的表面描述上，揭示了在对内在深层语义进行系统调查方面的明显不足。在这项工作中，我们引入了DEEPEVAL，一个全面的基准，用于评估大型多模态模型(LMMs)对视觉深层语义的能力。 DEEPEVAL 包括人工注释的数据集和三个渐进的子任务：细粒度描述选择、深度标题匹配和深层语义理解。利用 DEEPEVAL，我们评估了9个开源LMMs和GPT-4V(ision)。我们的评估显示了现有LMMs与人类在深层语义理解能力上存在着实质差距。例如，即使在图像描述方面达到与人类可比的表现，GPT-4V在理解深层语义方面仍落后于人类30%。进一步的分析表明",
    "tldr": "该论文引入了一个名为DEEPEVAL的基准，用于评估大型多模态模型对视觉深层语义的能力，揭示了现有LMMs在深层语义理解方面与人类之间存在显著差距。",
    "en_tdlr": "This paper introduces a benchmark called DEEPEVAL to evaluate the capability of large multimodal models in understanding the deep semantics of images, revealing a significant gap between existing LMMs and humans in deep semantic comprehension."
}