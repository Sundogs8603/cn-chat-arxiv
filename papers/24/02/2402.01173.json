{
    "title": "Efficient Prompt Caching via Embedding Similarity",
    "abstract": "Large language models (LLMs) have achieved huge success in numerous natural language process (NLP) tasks. However, it faces the challenge of significant resource consumption during inference. In this paper, we aim to improve the inference efficiency of LLMs by prompt caching, i.e., if the current prompt can be answered by the same response of a previous prompt, one can directly utilize that previous response without calling the LLM. Specifically, we focus on the prediction accuracy of prompt caching for single-round question-answering tasks via embedding similarity. The existing embeddings of prompts mostly focus on whether two prompts are semantically similar, which is not necessarily equivalent to whether the same response can answer them. Therefore, we propose a distillation-based method to fine-tune the existing embeddings for better caching prediction. Theoretically, we provide finite-sample guarantees for the convergence of our method under different types of loss functions. Empi",
    "link": "https://rss.arxiv.org/abs/2402.01173",
    "context": "Title: Efficient Prompt Caching via Embedding Similarity\nAbstract: Large language models (LLMs) have achieved huge success in numerous natural language process (NLP) tasks. However, it faces the challenge of significant resource consumption during inference. In this paper, we aim to improve the inference efficiency of LLMs by prompt caching, i.e., if the current prompt can be answered by the same response of a previous prompt, one can directly utilize that previous response without calling the LLM. Specifically, we focus on the prediction accuracy of prompt caching for single-round question-answering tasks via embedding similarity. The existing embeddings of prompts mostly focus on whether two prompts are semantically similar, which is not necessarily equivalent to whether the same response can answer them. Therefore, we propose a distillation-based method to fine-tune the existing embeddings for better caching prediction. Theoretically, we provide finite-sample guarantees for the convergence of our method under different types of loss functions. Empi",
    "path": "papers/24/02/2402.01173.json",
    "total_tokens": 872,
    "translated_title": "通过嵌入相似性实现高效的提示缓存",
    "translated_abstract": "大规模语言模型(LLMs)在许多自然语言处理(NLP)任务中取得了巨大成功。然而，在推理过程中，它面临着资源消耗的挑战。本文旨在通过提示缓存来提高LLMs的推理效率，即，如果当前提示可以由前一个提示的同样回答而得到回答，就可以直接利用该前一个回答而不调用LLMs。具体而言，我们关注通过嵌入相似性来提升单轮问答任务的缓存预测准确性。目前已有的提示嵌入主要关注两个提示是否语义相似，这与同样的回答是否可以回答它们并不等价。因此，我们提出了一种基于蒸馏的方法来优化现有的嵌入以获得更好的缓存预测。理论上，我们在不同类型的损失函数下提供了我们方法收敛的有限样本保证。实验结果表明，我们的方法在推理效率方面取得了显著的改进。",
    "tldr": "本论文通过嵌入相似性的提示缓存方法来提高大规模语言模型(LMMs)的推理效率，并提出一种蒸馏方法来优化现有嵌入以获得更好的缓存预测准确性。",
    "en_tdlr": "This paper proposes an efficient prompt caching method using embedding similarity to improve the inference efficiency of large language models (LLMs). It also introduces a distillation-based method to optimize existing embeddings for better caching prediction accuracy."
}