{
    "title": "Tempered Calculus for ML: Application to Hyperbolic Model Embedding",
    "abstract": "Most mathematical distortions used in ML are fundamentally integral in nature: $f$-divergences, Bregman divergences, (regularized) optimal transport distances, integral probability metrics, geodesic distances, etc. In this paper, we unveil a grounded theory and tools which can help improve these distortions to better cope with ML requirements. We start with a generalization of Riemann integration that also encapsulates functions that are not strictly additive but are, more generally, $t$-additive, as in nonextensive statistical mechanics. Notably, this recovers Volterra's product integral as a special case. We then generalize the Fundamental Theorem of calculus using an extension of the (Euclidean) derivative. This, along with a series of more specific Theorems, serves as a basis for results showing how one can specifically design, alter, or change fundamental properties of distortion measures in a simple way, with a special emphasis on geometric- and ML-related properties that are the",
    "link": "https://arxiv.org/abs/2402.04163",
    "context": "Title: Tempered Calculus for ML: Application to Hyperbolic Model Embedding\nAbstract: Most mathematical distortions used in ML are fundamentally integral in nature: $f$-divergences, Bregman divergences, (regularized) optimal transport distances, integral probability metrics, geodesic distances, etc. In this paper, we unveil a grounded theory and tools which can help improve these distortions to better cope with ML requirements. We start with a generalization of Riemann integration that also encapsulates functions that are not strictly additive but are, more generally, $t$-additive, as in nonextensive statistical mechanics. Notably, this recovers Volterra's product integral as a special case. We then generalize the Fundamental Theorem of calculus using an extension of the (Euclidean) derivative. This, along with a series of more specific Theorems, serves as a basis for results showing how one can specifically design, alter, or change fundamental properties of distortion measures in a simple way, with a special emphasis on geometric- and ML-related properties that are the",
    "path": "papers/24/02/2402.04163.json",
    "total_tokens": 850,
    "translated_title": "Tempered Calculus for ML: 应用于双曲模型嵌入的研究",
    "translated_abstract": "大多数在机器学习中使用的数学扭曲本质上都是积分的：$f$-divergences, Bregman divergences, (regularized) optimal transport distances, integral probability metrics, geodesic distances等。在本文中，我们揭示了一个基于温和微积分的理论和工具，可以帮助改进这些扭曲以更好地满足机器学习的要求。我们从广义的黎曼积分开始，这种积分还包括不严格可加的函数，而是更一般地是$t$-可加的，就像非极限统计力学中的情况一样。特别地，这种方法将Volterra的乘积积分作为特例。然后，我们使用（欧几里得）导数的扩展来推广基本定理。这些推广以及一系列更具体的定理为结果提供了基础，展示了如何以简单的方式专门设计、改变或改变扭曲度量的基本特性，特别强调与几何和机器学习相关的特性。",
    "tldr": "本论文介绍了基于温和微积分的理论和工具，来改进目前在机器学习中使用的数学扭曲方法，特别强调与几何和机器学习相关的特性。"
}