{
    "title": "Zero-shot Sentiment Analysis in Low-Resource Languages Using a Multilingual Sentiment Lexicon",
    "abstract": "Improving multilingual language models capabilities in low-resource languages is generally difficult due to the scarcity of large-scale data in those languages. In this paper, we relax the reliance on texts in low-resource languages by using multilingual lexicons in pretraining to enhance multilingual capabilities. Specifically, we focus on zero-shot sentiment analysis tasks across 34 languages, including 6 high/medium-resource languages, 25 low-resource languages, and 3 code-switching datasets. We demonstrate that pretraining using multilingual lexicons, without using any sentence-level sentiment data, achieves superior zero-shot performance compared to models fine-tuned on English sentiment datasets, and large language models like GPT--3.5, BLOOMZ, and XGLM. These findings are observable for unseen low-resource languages to code-mixed scenarios involving high-resource languages.",
    "link": "https://arxiv.org/abs/2402.02113",
    "context": "Title: Zero-shot Sentiment Analysis in Low-Resource Languages Using a Multilingual Sentiment Lexicon\nAbstract: Improving multilingual language models capabilities in low-resource languages is generally difficult due to the scarcity of large-scale data in those languages. In this paper, we relax the reliance on texts in low-resource languages by using multilingual lexicons in pretraining to enhance multilingual capabilities. Specifically, we focus on zero-shot sentiment analysis tasks across 34 languages, including 6 high/medium-resource languages, 25 low-resource languages, and 3 code-switching datasets. We demonstrate that pretraining using multilingual lexicons, without using any sentence-level sentiment data, achieves superior zero-shot performance compared to models fine-tuned on English sentiment datasets, and large language models like GPT--3.5, BLOOMZ, and XGLM. These findings are observable for unseen low-resource languages to code-mixed scenarios involving high-resource languages.",
    "path": "papers/24/02/2402.02113.json",
    "total_tokens": 837,
    "translated_title": "使用多语言情感词典进行低资源语言的零样本情感分析",
    "translated_abstract": "在低资源语言中改善多语言语言模型的能力通常很困难，因为这些语言的大规模数据很少。本文通过在预训练中使用多语言词典来增强多语言能力，从而减少对低资源语言文本的依赖。具体而言，我们专注于跨34种语言的零样本情感分析任务，包括6种高/中资源语言、25种低资源语言和3个混合代码数据集。我们证明，在不使用任何句子级情感数据的情况下，使用多语言词典进行预训练的模型相比于在英文情感数据集上微调以及像GPT--3.5、BLOOMZ和XGLM这样的大型语言模型，具有更好的零样本性能。这些发现适用于对低资源语言的未见情况，以及涉及高资源语言的混合代码情况。",
    "tldr": "本文提出一种使用多语言情感词典进行零样本情感分析的方法，通过预训练模型在低资源语言中获得优越的性能，包括对高/中资源语言和混合代码数据的处理。",
    "en_tdlr": "This paper proposes a method for zero-shot sentiment analysis using a multilingual sentiment lexicon, achieving superior performance in low-resource languages by pretraining models, including handling high/medium-resource languages and code-mixed data."
}