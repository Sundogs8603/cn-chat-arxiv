{
    "title": "FuseChat: Knowledge Fusion of Chat Models",
    "abstract": "arXiv:2402.16107v1 Announce Type: new  Abstract: While training large language models (LLMs) from scratch can indeed lead to models with distinct capabilities and strengths, this approach incurs substantial costs and may lead to potential redundancy in competencies. An alternative strategy is to combine existing LLMs into a more robust LLM, thereby diminishing the necessity for expensive pre-training. However, due to the diverse architectures of LLMs, direct parameter blending proves to be unfeasible. Recently, \\textsc{FuseLLM} introduced the concept of knowledge fusion to transfer the collective knowledge of multiple structurally varied LLMs into a target LLM through lightweight continual training. In this report, we extend the scalability and flexibility of the \\textsc{FuseLLM} framework to realize the fusion of chat LLMs, resulting in \\textsc{FuseChat}. \\textsc{FuseChat} comprises two main stages. Firstly, we undertake knowledge fusion for structurally and scale-varied source LLMs t",
    "link": "https://arxiv.org/abs/2402.16107",
    "context": "Title: FuseChat: Knowledge Fusion of Chat Models\nAbstract: arXiv:2402.16107v1 Announce Type: new  Abstract: While training large language models (LLMs) from scratch can indeed lead to models with distinct capabilities and strengths, this approach incurs substantial costs and may lead to potential redundancy in competencies. An alternative strategy is to combine existing LLMs into a more robust LLM, thereby diminishing the necessity for expensive pre-training. However, due to the diverse architectures of LLMs, direct parameter blending proves to be unfeasible. Recently, \\textsc{FuseLLM} introduced the concept of knowledge fusion to transfer the collective knowledge of multiple structurally varied LLMs into a target LLM through lightweight continual training. In this report, we extend the scalability and flexibility of the \\textsc{FuseLLM} framework to realize the fusion of chat LLMs, resulting in \\textsc{FuseChat}. \\textsc{FuseChat} comprises two main stages. Firstly, we undertake knowledge fusion for structurally and scale-varied source LLMs t",
    "path": "papers/24/02/2402.16107.json",
    "total_tokens": 844,
    "translated_title": "FuseChat：对话模型知识融合",
    "translated_abstract": "虽然从头开始训练大型语言模型（LLMs）确实可以导致具有独特能力和优势的模型，但这种方法会产生巨大成本，并可能导致竞争能力的潜在冗余。一种替代策略是将现有的LLMs组合成更强大的LLM，从而减少昂贵的预训练的必要性。但是，由于LLMs的多样化架构，直接参数融合被证明是不可行的。最近，FuseLLM引入了知识融合的概念，通过轻量级的持续训练将多个结构多样的LLM的集体知识转移至目标LLM。在本报告中，我们扩展了FuseLLM框架的可扩展性和灵活性，实现了对话LLM的融合，生成了FuseChat。FuseChat包括两个主要阶段。首先，我们对结构和规模不同的源LLMs进行知识融合",
    "tldr": "FuseChat通过知识融合将多个对话模型的集体知识转移到目标语言模型中，避免了昂贵的预训练成本。",
    "en_tdlr": "FuseChat transfers collective knowledge from multiple chat models to a target language model through knowledge fusion, avoiding the costly pre-training expenses."
}