{
    "title": "Downstream Task Guided Masking Learning in Masked Autoencoders Using Multi-Level Optimization",
    "abstract": "arXiv:2402.18128v1 Announce Type: cross  Abstract: Masked Autoencoder (MAE) is a notable method for self-supervised pretraining in visual representation learning. It operates by randomly masking image patches and reconstructing these masked patches using the unmasked ones. A key limitation of MAE lies in its disregard for the varying informativeness of different patches, as it uniformly selects patches to mask. To overcome this, some approaches propose masking based on patch informativeness. However, these methods often do not consider the specific requirements of downstream tasks, potentially leading to suboptimal representations for these tasks. In response, we introduce the Multi-level Optimized Mask Autoencoder (MLO-MAE), a novel framework that leverages end-to-end feedback from downstream tasks to learn an optimal masking strategy during pretraining. Our experimental findings highlight MLO-MAE's significant advancements in visual representation learning. Compared to existing metho",
    "link": "https://arxiv.org/abs/2402.18128",
    "context": "Title: Downstream Task Guided Masking Learning in Masked Autoencoders Using Multi-Level Optimization\nAbstract: arXiv:2402.18128v1 Announce Type: cross  Abstract: Masked Autoencoder (MAE) is a notable method for self-supervised pretraining in visual representation learning. It operates by randomly masking image patches and reconstructing these masked patches using the unmasked ones. A key limitation of MAE lies in its disregard for the varying informativeness of different patches, as it uniformly selects patches to mask. To overcome this, some approaches propose masking based on patch informativeness. However, these methods often do not consider the specific requirements of downstream tasks, potentially leading to suboptimal representations for these tasks. In response, we introduce the Multi-level Optimized Mask Autoencoder (MLO-MAE), a novel framework that leverages end-to-end feedback from downstream tasks to learn an optimal masking strategy during pretraining. Our experimental findings highlight MLO-MAE's significant advancements in visual representation learning. Compared to existing metho",
    "path": "papers/24/02/2402.18128.json",
    "total_tokens": 914,
    "translated_title": "在遮罩自动编码器中使用多级优化的下游任务引导学习",
    "translated_abstract": "遮罩自动编码器（MAE）是视觉表示学习中自监督预训练的一个显著方法。它通过随机遮罩图像补丁，并使用未遮罩的补丁重建这些遮罩补丁。 MAE的一个关键局限性在于其忽视不同补丁的信息量不同，因为它会统一选择要遮罩的补丁。为了克服这一问题，一些方法提出基于补丁信息量进行遮罩。然而，这些方法通常不考虑下游任务的特定需求，可能导致这些任务的表示次优。作为响应，我们引入了多级优化遮罩自动编码器（MLO-MAE），这是一个新颖的框架，利用来自下游任务的端到端反馈，在预训练期间学习最佳遮罩策略。我们的实验结果突显了MLO-MAE在视觉表示学习中的显著进展。与现有方法相比，",
    "tldr": "提出了一种新颖的框架 - 多级优化遮罩自动编码器（MLO-MAE），该框架利用来自下游任务的反馈，在预训练期间学习最佳的遮罩策略，显著提升了视觉表示学习。"
}