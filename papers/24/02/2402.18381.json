{
    "title": "Large Language Models As Evolution Strategies",
    "abstract": "arXiv:2402.18381v1 Announce Type: new  Abstract: Large Transformer models are capable of implementing a plethora of so-called in-context learning algorithms. These include gradient descent, classification, sequence completion, transformation, and improvement. In this work, we investigate whether large language models (LLMs), which never explicitly encountered the task of black-box optimization, are in principle capable of implementing evolutionary optimization algorithms. While previous works have solely focused on language-based task specification, we move forward and focus on the zero-shot application of LLMs to black-box optimization. We introduce a novel prompting strategy, consisting of least-to-most sorting of discretized population members and querying the LLM to propose an improvement to the mean statistic, i.e. perform a type of black-box recombination operation. Empirically, we find that our setup allows the user to obtain an LLM-based evolution strategy, which we call `EvoLL",
    "link": "https://arxiv.org/abs/2402.18381",
    "context": "Title: Large Language Models As Evolution Strategies\nAbstract: arXiv:2402.18381v1 Announce Type: new  Abstract: Large Transformer models are capable of implementing a plethora of so-called in-context learning algorithms. These include gradient descent, classification, sequence completion, transformation, and improvement. In this work, we investigate whether large language models (LLMs), which never explicitly encountered the task of black-box optimization, are in principle capable of implementing evolutionary optimization algorithms. While previous works have solely focused on language-based task specification, we move forward and focus on the zero-shot application of LLMs to black-box optimization. We introduce a novel prompting strategy, consisting of least-to-most sorting of discretized population members and querying the LLM to propose an improvement to the mean statistic, i.e. perform a type of black-box recombination operation. Empirically, we find that our setup allows the user to obtain an LLM-based evolution strategy, which we call `EvoLL",
    "path": "papers/24/02/2402.18381.json",
    "total_tokens": 748,
    "translated_title": "大型语言模型作为进化策略",
    "translated_abstract": "大型Transformer模型能够实现各种所谓的上下文学习算法，包括梯度下降、分类、序列完成、转换和改进。在这项工作中，我们探讨了从未明确遇到过黑盒优化任务的大型语言模型（LLMs）是否基本上能够实现进化优化算法。我们介绍了一种新的提示策略，通过对离散化的种群成员进行从少到多的排序，并询问LLM提出对均值统计的改进，执行一种黑盒重组操作。实证上，我们发现我们的设置允许用户获得基于LLM的进化策略，我们称之为`EvoLL`。",
    "tldr": "探索大型语言模型是否能够在处理黑盒优化任务中实现进化优化算法，并引入了一种新的提示策略来提高均值统计，从而实现黑盒重组操作。",
    "en_tdlr": "Investigating the capability of large language models in implementing evolutionary optimization algorithms for black-box tasks, introducing a novel prompting strategy to enhance mean statistic and perform black-box recombination operations."
}