{
    "title": "Eagle: Ethical Dataset Given from Real Interactions",
    "abstract": "arXiv:2402.14258v1 Announce Type: new  Abstract: Recent studies have demonstrated that large language models (LLMs) have ethical-related problems such as social biases, lack of moral reasoning, and generation of offensive content. The existing evaluation metrics and methods to address these ethical challenges use datasets intentionally created by instructing humans to create instances including ethical problems. Therefore, the data does not reflect prompts that users actually provide when utilizing LLM services in everyday contexts. This may not lead to the development of safe LLMs that can address ethical challenges arising in real-world applications. In this paper, we create Eagle datasets extracted from real interactions between ChatGPT and users that exhibit social biases, toxicity, and immoral problems. Our experiments show that Eagle captures complementary aspects, not covered by existing datasets proposed for evaluation and mitigation of such ethical challenges. Our code is publ",
    "link": "https://arxiv.org/abs/2402.14258",
    "context": "Title: Eagle: Ethical Dataset Given from Real Interactions\nAbstract: arXiv:2402.14258v1 Announce Type: new  Abstract: Recent studies have demonstrated that large language models (LLMs) have ethical-related problems such as social biases, lack of moral reasoning, and generation of offensive content. The existing evaluation metrics and methods to address these ethical challenges use datasets intentionally created by instructing humans to create instances including ethical problems. Therefore, the data does not reflect prompts that users actually provide when utilizing LLM services in everyday contexts. This may not lead to the development of safe LLMs that can address ethical challenges arising in real-world applications. In this paper, we create Eagle datasets extracted from real interactions between ChatGPT and users that exhibit social biases, toxicity, and immoral problems. Our experiments show that Eagle captures complementary aspects, not covered by existing datasets proposed for evaluation and mitigation of such ethical challenges. Our code is publ",
    "path": "papers/24/02/2402.14258.json",
    "total_tokens": 850,
    "translated_title": "鹰：来自真实互动的道德数据集",
    "translated_abstract": "最近的研究表明，大型语言模型（LLMs）存在道德相关问题，如社会偏见、缺乏道德推理和生成具有攻击性内容。现有的评估指标和方法处理这些道德挑战使用诱使人类制作包含道德问题的实例的数据集。因此，这些数据并不反映用户在日常使用LLM服务时实际提供的提示。这可能不会导致开发能够解决实际应用中出现的道德挑战的安全LLMs。在本文中，我们创建了从ChatGPT和用户之间的真实互动中提取的鹰数据集，展示了社会偏见、毒性和不道德问题。我们的实验表明，鹰捕捉了现有用于评估和减轻此类道德挑战的数据集所未涵盖的补充方面。",
    "tldr": "该论文提出了鹰数据集，从ChatGPT和用户的真实互动中提取，展示了社会偏见、毒性和不道德问题。实验证明鹰捕捉到了现有用于评估和减轻这些道德挑战的数据集所未覆盖的补充方面。",
    "en_tdlr": "This paper introduces the Eagle dataset extracted from real interactions between ChatGPT and users, demonstrating social biases, toxicity, and immoral problems. Experiments show that Eagle captures complementary aspects not covered by existing datasets for evaluating and mitigating such ethical challenges."
}