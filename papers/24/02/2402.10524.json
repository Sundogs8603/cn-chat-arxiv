{
    "title": "LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models",
    "abstract": "arXiv:2402.10524v1 Announce Type: cross  Abstract: Automatic side-by-side evaluation has emerged as a promising approach to evaluating the quality of responses from large language models (LLMs). However, analyzing the results from this evaluation approach raises scalability and interpretability challenges. In this paper, we present LLM Comparator, a novel visual analytics tool for interactively analyzing results from automatic side-by-side evaluation. The tool supports interactive workflows for users to understand when and why a model performs better or worse than a baseline model, and how the responses from two models are qualitatively different. We iteratively designed and developed the tool by closely working with researchers and engineers at a large technology company. This paper details the user challenges we identified, the design and development of the tool, and an observational study with participants who regularly evaluate their models.",
    "link": "https://arxiv.org/abs/2402.10524",
    "context": "Title: LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models\nAbstract: arXiv:2402.10524v1 Announce Type: cross  Abstract: Automatic side-by-side evaluation has emerged as a promising approach to evaluating the quality of responses from large language models (LLMs). However, analyzing the results from this evaluation approach raises scalability and interpretability challenges. In this paper, we present LLM Comparator, a novel visual analytics tool for interactively analyzing results from automatic side-by-side evaluation. The tool supports interactive workflows for users to understand when and why a model performs better or worse than a baseline model, and how the responses from two models are qualitatively different. We iteratively designed and developed the tool by closely working with researchers and engineers at a large technology company. This paper details the user challenges we identified, the design and development of the tool, and an observational study with participants who regularly evaluate their models.",
    "path": "papers/24/02/2402.10524.json",
    "total_tokens": 839,
    "translated_title": "LLM比较器：用于大型语言模型并行评估的可视化分析",
    "translated_abstract": "自动并行评估已成为评估大型语言模型（LLMs）响应质量的一种有前途的方法。然而，分析这种评估方法的结果存在可扩展性和可解释性挑战。本文提出了LLM比较器，这是一种新颖的可视化分析工具，用于交互式地分析自动并行评估结果。该工具支持用户进行交互式工作流，以了解为什么和何时模型比基准模型表现更好或更差，以及两个模型的响应在质量上有何不同。我们通过与一家大型科技公司的研究人员和工程师密切合作，迭代设计和开发了该工具。本文详细介绍了我们识别的用户挑战、该工具的设计和开发，以及定期评估其模型的参与者的观察研究。",
    "tldr": "LLM Comparator是一种用于交互式分析自动并行评估结果的新型可视化工具，支持用户理解模型表现优劣和不同之处，解决了大型语言模型评估中的可扩展性和可解释性挑战。",
    "en_tdlr": "LLM Comparator is a novel visual analytics tool for interactively analyzing results from automatic side-by-side evaluation, supporting users to understand model performance differences and addressing scalability and interpretability challenges in evaluating large language models."
}