{
    "title": "BootsTAP: Bootstrapped Training for Tracking-Any-Point",
    "abstract": "To endow models with greater understanding of physics and motion, it is useful to enable them to perceive how solid surfaces move and deform in real scenes. This can be formalized as Tracking-Any-Point (TAP), which requires the algorithm to be able to track any point corresponding to a solid surface in a video, potentially densely in space and time. Large-scale ground-truth training data for TAP is only available in simulation, which currently has limited variety of objects and motion. In this work, we demonstrate how large-scale, unlabeled, uncurated real-world data can improve a TAP model with minimal architectural changes, using a self-supervised student-teacher setup. We demonstrate state-of-the-art performance on the TAP-Vid benchmark surpassing previous results by a wide margin: for example, TAP-Vid-DAVIS performance improves from 61.3% to 66.4%, and TAP-Vid-Kinetics from 57.2% to 61.5%.",
    "link": "https://arxiv.org/abs/2402.00847",
    "context": "Title: BootsTAP: Bootstrapped Training for Tracking-Any-Point\nAbstract: To endow models with greater understanding of physics and motion, it is useful to enable them to perceive how solid surfaces move and deform in real scenes. This can be formalized as Tracking-Any-Point (TAP), which requires the algorithm to be able to track any point corresponding to a solid surface in a video, potentially densely in space and time. Large-scale ground-truth training data for TAP is only available in simulation, which currently has limited variety of objects and motion. In this work, we demonstrate how large-scale, unlabeled, uncurated real-world data can improve a TAP model with minimal architectural changes, using a self-supervised student-teacher setup. We demonstrate state-of-the-art performance on the TAP-Vid benchmark surpassing previous results by a wide margin: for example, TAP-Vid-DAVIS performance improves from 61.3% to 66.4%, and TAP-Vid-Kinetics from 57.2% to 61.5%.",
    "path": "papers/24/02/2402.00847.json",
    "total_tokens": 944,
    "translated_title": "BootsTAP: 针对Tracking-Any-Point的自举训练",
    "translated_abstract": "为了使模型对物理和运动有更深入的理解，让它们能够感知实景中固体表面的移动和变形是很有用的。这可以形式化为Tracking-Any-Point (TAP)，要求算法能够追踪视频中与固体表面对应的任意点，可能是在空间和时间上密集的。目前，TAP需要大规模的真实数据进行训练，但目前只能在模拟环境中获得有限种类的对象和运动。在这项工作中，我们演示了如何使用大规模、无标签、未筛选的真实世界数据，在仅进行最小架构更改的情况下提高TAP模型的性能，采用了自监督的师生设置。我们在TAP-Vid基准测试上展示了超过以往成果的最先进性能：例如，TAP-Vid-DAVIS的性能从61.3%提升到66.4%，TAP-Vid-Kinetics从57.2%提升到61.5%。",
    "tldr": "该论文展示了如何通过使用大规模、无标签、未筛选的真实世界数据，以及采用自监督的师生设置，来改进Tracking-Any-Point（TAP）模型的性能。通过在TAP-Vid基准测试上取得了state-of-the-art的结果，该方法在TAP任务上取得了显著的改进。"
}