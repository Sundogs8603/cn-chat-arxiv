{
    "title": "Cascaded Scaling Classifier: class incremental learning with probability scaling",
    "abstract": "Humans are capable of acquiring new knowledge and transferring learned knowledge into different domains, incurring a small forgetting. The same ability, called Continual Learning, is challenging to achieve when operating with neural networks due to the forgetting affecting past learned tasks when learning new ones. This forgetting can be mitigated by replaying stored samples from past tasks, but a large memory size may be needed for long sequences of tasks; moreover, this could lead to overfitting on saved samples. In this paper, we propose a novel regularisation approach and a novel incremental classifier called, respectively, Margin Dampening and Cascaded Scaling Classifier. The first combines a soft constraint and a knowledge distillation approach to preserve past learned knowledge while allowing the model to learn new patterns effectively. The latter is a gated incremental classifier, helping the model modify past predictions without directly interfering with them. This is achieved",
    "link": "https://rss.arxiv.org/abs/2402.01262",
    "context": "Title: Cascaded Scaling Classifier: class incremental learning with probability scaling\nAbstract: Humans are capable of acquiring new knowledge and transferring learned knowledge into different domains, incurring a small forgetting. The same ability, called Continual Learning, is challenging to achieve when operating with neural networks due to the forgetting affecting past learned tasks when learning new ones. This forgetting can be mitigated by replaying stored samples from past tasks, but a large memory size may be needed for long sequences of tasks; moreover, this could lead to overfitting on saved samples. In this paper, we propose a novel regularisation approach and a novel incremental classifier called, respectively, Margin Dampening and Cascaded Scaling Classifier. The first combines a soft constraint and a knowledge distillation approach to preserve past learned knowledge while allowing the model to learn new patterns effectively. The latter is a gated incremental classifier, helping the model modify past predictions without directly interfering with them. This is achieved",
    "path": "papers/24/02/2402.01262.json",
    "total_tokens": 831,
    "translated_title": "级联缩放分类器：通过概率缩放进行类别增量学习",
    "translated_abstract": "人类有能力获取新知识并将学习到的知识转移到不同的领域，仅有轻微的遗忘。同样的能力，在神经网络中实现连续学习是具有挑战性的，因为在学习新任务时会影响到过去学习的任务。这种遗忘可以通过回放存储的过去任务样本来缓解，但是对于长序列任务可能需要较大的存储空间；此外，这可能导致对保存样本的过拟合。在本文中，我们提出了一种新颖的正则化方法和一种新颖的增量分类器，分别称为边际抑制和级联缩放分类器。前者结合了软约束和知识蒸馏方法，以保留过去学习的知识同时有效地学习新的模式。后者是一种带有门控的增量分类器，帮助模型修改过去的预测而不直接干扰它们。这是通过...",
    "tldr": "提出了级联缩放分类器，结合边际抑制和知识蒸馏方法，用于实现神经网络中的连续学习，并降低过去任务的遗忘。",
    "en_tdlr": "Proposed the Cascaded Scaling Classifier which combines Margin Dampening and knowledge distillation methods to achieve continual learning in neural networks and reduce forgetting of past tasks."
}