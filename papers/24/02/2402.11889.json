{
    "title": "ROSE Doesn't Do That: Boosting the Safety of Instruction-Tuned Large Language Models with Reverse Prompt Contrastive Decoding",
    "abstract": "arXiv:2402.11889v1 Announce Type: new  Abstract: With the development of instruction-tuned large language models (LLMs), improving the safety of LLMs has become more critical. However, the current approaches for aligning the LLMs output with expected safety usually require substantial training efforts, e.g., high-quality safety data and expensive computational resources, which are costly and inefficient. To this end, we present reverse prompt contrastive decoding (ROSE), a simple-yet-effective method to directly boost the safety of existing instruction-tuned LLMs without any additional training. The principle of ROSE is to improve the probability of desired safe output via suppressing the undesired output induced by the carefully-designed reverse prompts. Experiments on 6 safety and 2 general-purpose tasks show that, our ROSE not only brings consistent and significant safety improvements (up to +13.8% safety score) upon 5 types of instruction-tuned LLMs, but also benefits the general-p",
    "link": "https://arxiv.org/abs/2402.11889",
    "context": "Title: ROSE Doesn't Do That: Boosting the Safety of Instruction-Tuned Large Language Models with Reverse Prompt Contrastive Decoding\nAbstract: arXiv:2402.11889v1 Announce Type: new  Abstract: With the development of instruction-tuned large language models (LLMs), improving the safety of LLMs has become more critical. However, the current approaches for aligning the LLMs output with expected safety usually require substantial training efforts, e.g., high-quality safety data and expensive computational resources, which are costly and inefficient. To this end, we present reverse prompt contrastive decoding (ROSE), a simple-yet-effective method to directly boost the safety of existing instruction-tuned LLMs without any additional training. The principle of ROSE is to improve the probability of desired safe output via suppressing the undesired output induced by the carefully-designed reverse prompts. Experiments on 6 safety and 2 general-purpose tasks show that, our ROSE not only brings consistent and significant safety improvements (up to +13.8% safety score) upon 5 types of instruction-tuned LLMs, but also benefits the general-p",
    "path": "papers/24/02/2402.11889.json",
    "total_tokens": 897,
    "translated_title": "ROSE 不这样做：使用反向提示对比解码提升调整指令大型语言模型的安全性",
    "translated_abstract": "随着调整指令大型语言模型（LLMs）的发展，提高LLMs的安全性变得更加关键。然而，目前用于将LLMs输出与预期安全性对齐的方法通常需要大量的训练工作，例如高质量的安全数据和昂贵的计算资源，这些都是昂贵且低效的。因此，我们提出了反向提示对比解码（ROSE），这是一种简单而有效的方法，可以直接提升现有调整指令LLMs的安全性，而无需额外训练。ROSE的原则是通过抑制经过精心设计的反向提示诱导的不受欢迎的输出，提高期望的安全输出的概率。在6个安全任务和2个通用任务上的实验表明，我们的ROSE不仅在5种类型的调整指令LLMs上带来了一致且显著的安全性改进（高达+13.8%的安全分数），而且对通用任务也有益处。",
    "tldr": "ROSE是一种简单而有效的方法，通过抑制不受欢迎的输出来提高期望的安全输出的概率，从而直接提升现有调整指令LLMs的安全性。"
}