{
    "title": "Pre-training of Lightweight Vision Transformers on Small Datasets with Minimally Scaled Images",
    "abstract": "Can a lightweight Vision Transformer (ViT) match or exceed the performance of Convolutional Neural Networks (CNNs) like ResNet on small datasets with small image resolutions? This report demonstrates that a pure ViT can indeed achieve superior performance through pre-training, using a masked auto-encoder technique with minimal image scaling. Our experiments on the CIFAR-10 and CIFAR-100 datasets involved ViT models with fewer than 3.65 million parameters and a multiply-accumulate (MAC) count below 0.27G, qualifying them as 'lightweight' models. Unlike previous approaches, our method attains state-of-the-art performance among similar lightweight transformer-based architectures without significantly scaling up images from CIFAR-10 and CIFAR-100. This achievement underscores the efficiency of our model, not only in handling small datasets but also in effectively processing images close to their original scale.",
    "link": "https://arxiv.org/abs/2402.03752",
    "context": "Title: Pre-training of Lightweight Vision Transformers on Small Datasets with Minimally Scaled Images\nAbstract: Can a lightweight Vision Transformer (ViT) match or exceed the performance of Convolutional Neural Networks (CNNs) like ResNet on small datasets with small image resolutions? This report demonstrates that a pure ViT can indeed achieve superior performance through pre-training, using a masked auto-encoder technique with minimal image scaling. Our experiments on the CIFAR-10 and CIFAR-100 datasets involved ViT models with fewer than 3.65 million parameters and a multiply-accumulate (MAC) count below 0.27G, qualifying them as 'lightweight' models. Unlike previous approaches, our method attains state-of-the-art performance among similar lightweight transformer-based architectures without significantly scaling up images from CIFAR-10 and CIFAR-100. This achievement underscores the efficiency of our model, not only in handling small datasets but also in effectively processing images close to their original scale.",
    "path": "papers/24/02/2402.03752.json",
    "total_tokens": 917,
    "translated_title": "在小数据集上使用最小缩放图像对轻量级视觉Transformer进行预训练",
    "translated_abstract": "轻量级视觉Transformer（ViT）能否在分辨率较小的小数据集上与ResNet等卷积神经网络（CNN）的性能相匹敌或超越？本报告通过使用最小图像缩放的掩码自编码器技术，证明了纯ViT确实可以通过预训练实现优秀的性能。我们在CIFAR-10和CIFAR-100数据集上进行了实验，所使用的ViT模型参数不超过365万个，并且乘累加（MAC）计数低于0.27G，可以称之为“轻量级”模型。与以往方法不同的是，我们的方法在不显著增加CIFAR-10和CIFAR-100图像尺寸的情况下，达到了类似轻量级基于Transformer的架构的最先进性能。这一成就凸显了我们模型的效率，它不仅可以处理小数据集，而且可以有效处理接近原始尺寸的图像。",
    "tldr": "本研究展示了轻量级视觉Transformer（ViT）在小数据集上使用最小缩放图像进行预训练，通过掩码自编码器技术实现优秀性能，无需显著增加图像尺寸。该方法不仅高效处理小数据集，还能有效处理接近原始尺寸的图像。",
    "en_tdlr": "This paper demonstrates that lightweight Vision Transformers (ViTs) can achieve superior performance on small datasets with minimal image scaling through pre-training using a masked auto-encoder technique. It achieves state-of-the-art results among similar lightweight transformer-based architectures without significantly scaling up images, showcasing its efficiency in handling small datasets and processing images close to their original scale."
}