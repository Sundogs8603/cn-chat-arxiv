{
    "title": "A Multimodal In-Context Tuning Approach for E-Commerce Product Description Generation",
    "abstract": "arXiv:2402.13587v1 Announce Type: new  Abstract: In this paper, we propose a new setting for generating product descriptions from images, augmented by marketing keywords. It leverages the combined power of visual and textual information to create descriptions that are more tailored to the unique features of products. For this setting, previous methods utilize visual and textual encoders to encode the image and keywords and employ a language model-based decoder to generate the product description. However, the generated description is often inaccurate and generic since same-category products have similar copy-writings, and optimizing the overall framework on large-scale samples makes models concentrate on common words yet ignore the product features. To alleviate the issue, we present a simple and effective Multimodal In-Context Tuning approach, named ModICT, which introduces a similar product sample as the reference and utilizes the in-context learning capability of language models to ",
    "link": "https://arxiv.org/abs/2402.13587",
    "context": "Title: A Multimodal In-Context Tuning Approach for E-Commerce Product Description Generation\nAbstract: arXiv:2402.13587v1 Announce Type: new  Abstract: In this paper, we propose a new setting for generating product descriptions from images, augmented by marketing keywords. It leverages the combined power of visual and textual information to create descriptions that are more tailored to the unique features of products. For this setting, previous methods utilize visual and textual encoders to encode the image and keywords and employ a language model-based decoder to generate the product description. However, the generated description is often inaccurate and generic since same-category products have similar copy-writings, and optimizing the overall framework on large-scale samples makes models concentrate on common words yet ignore the product features. To alleviate the issue, we present a simple and effective Multimodal In-Context Tuning approach, named ModICT, which introduces a similar product sample as the reference and utilizes the in-context learning capability of language models to ",
    "path": "papers/24/02/2402.13587.json",
    "total_tokens": 698,
    "translated_title": "一种用于电子商务产品描述生成的多模态上下文调整方法",
    "translated_abstract": "在本文中，我们提出了一种新的设置，用于从图像中生成产品描述，其中包含营销关键词。它利用视觉和文本信息的综合能力，创建更加符合产品独特特性的描述。我们提出了一种简单有效的多模态上下文调整方法ModICT，通过引入相似的产品样本作为参考，并利用语言模型的上下文学习能力",
    "tldr": "提出了一种用于电子商务产品描述生成的多模态上下文调整方法ModICT，通过引入相似产品样本和利用语言模型的上下文学习能力，旨在解决生成描述中常见且忽略产品特征的问题",
    "en_tdlr": "Proposed a multimodal in-context tuning approach, named ModICT, for e-commerce product description generation, addressing the issue of common and generic descriptions by introducing similar product samples and leveraging the contextual learning capability of language models."
}