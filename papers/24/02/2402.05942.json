{
    "title": "Cooperative Knowledge Distillation: A Learner Agnostic Approach",
    "abstract": "Knowledge distillation is a simple but powerful way to transfer knowledge between a teacher model to a student model. Existing work suffers from at least one of the following key limitations in terms of direction and scope of transfer which restrict its use: all knowledge is transferred from teacher to student regardless of whether or not that knowledge is useful, the student is the only one learning in this exchange, and typically distillation transfers knowledge only from a single teacher to a single student. We formulate a novel form of knowledge distillation in which many models can act as both students and teachers which we call cooperative distillation. The models cooperate as follows: a model (the student) identifies specific deficiencies in it's performance and searches for another model (the teacher) who encodes learned knowledge into instructional virtual instances via counterfactual instance generation. Because different models may have different strengths and weaknesses, al",
    "link": "https://arxiv.org/abs/2402.05942",
    "context": "Title: Cooperative Knowledge Distillation: A Learner Agnostic Approach\nAbstract: Knowledge distillation is a simple but powerful way to transfer knowledge between a teacher model to a student model. Existing work suffers from at least one of the following key limitations in terms of direction and scope of transfer which restrict its use: all knowledge is transferred from teacher to student regardless of whether or not that knowledge is useful, the student is the only one learning in this exchange, and typically distillation transfers knowledge only from a single teacher to a single student. We formulate a novel form of knowledge distillation in which many models can act as both students and teachers which we call cooperative distillation. The models cooperate as follows: a model (the student) identifies specific deficiencies in it's performance and searches for another model (the teacher) who encodes learned knowledge into instructional virtual instances via counterfactual instance generation. Because different models may have different strengths and weaknesses, al",
    "path": "papers/24/02/2402.05942.json",
    "total_tokens": 854,
    "translated_title": "合作知识蒸馏：一种学习者无关的方法",
    "translated_abstract": "知识蒸馏是一种简单而强大的将教师模型的知识传递给学生模型的方法。现有的研究存在以下至少一种关键限制，限制其使用范围和方向：无论该知识是否有用，所有知识都从教师传递给学生；学生是这种交流中唯一学习的一方；典型的蒸馏只从一个教师向一个学生传递知识。我们提出了一种新形式的知识蒸馏，即合作蒸馏，其中许多模型可以同时充当学生和教师的角色。模型之间的合作方式如下：一个模型（学生）识别其性能中的特定缺陷，并搜索另一个模型（教师），通过生成对应事实情况的虚拟实例来编码所学知识。由于不同模型可能具有不同的优势和劣势，因此合作蒸馏的方法可以更有效地传递知识。",
    "tldr": "合作知识蒸馏是一种通过多个模型相互合作来传递知识的方法，可以弥补传统知识蒸馏的局限性。不同模型的优劣势可以更有效地传递知识。",
    "en_tdlr": "Cooperative Knowledge Distillation is an approach that transfers knowledge through the collaboration of multiple models, addressing the limitations of traditional knowledge distillation by leveraging the strengths of different models."
}