{
    "title": "Steady-State Error Compensation for Reinforcement Learning with Quadratic Rewards",
    "abstract": "arXiv:2402.09075v1 Announce Type: cross Abstract: The selection of a reward function in Reinforcement Learning (RL) has garnered significant attention because of its impact on system performance. Issues of steady-state error often manifest when quadratic reward functions are employed. Although existing solutions using absolute-value-type reward functions partially address this problem, they tend to induce substantial fluctuations in specific system states, leading to abrupt changes. In response to this challenge, this study proposes an approach that introduces an integral term. By integrating this term into quadratic-type reward functions, the RL algorithm is adeptly tuned, augmenting the system's consideration of long-term rewards and, consequently, alleviating concerns related to steady-state errors. Through experiments and performance evaluations on the Adaptive Cruise Control (ACC) model and lane change models, we validate that the proposed method not only effectively diminishes st",
    "link": "https://arxiv.org/abs/2402.09075",
    "context": "Title: Steady-State Error Compensation for Reinforcement Learning with Quadratic Rewards\nAbstract: arXiv:2402.09075v1 Announce Type: cross Abstract: The selection of a reward function in Reinforcement Learning (RL) has garnered significant attention because of its impact on system performance. Issues of steady-state error often manifest when quadratic reward functions are employed. Although existing solutions using absolute-value-type reward functions partially address this problem, they tend to induce substantial fluctuations in specific system states, leading to abrupt changes. In response to this challenge, this study proposes an approach that introduces an integral term. By integrating this term into quadratic-type reward functions, the RL algorithm is adeptly tuned, augmenting the system's consideration of long-term rewards and, consequently, alleviating concerns related to steady-state errors. Through experiments and performance evaluations on the Adaptive Cruise Control (ACC) model and lane change models, we validate that the proposed method not only effectively diminishes st",
    "path": "papers/24/02/2402.09075.json",
    "total_tokens": 867,
    "translated_title": "用于具有二次奖励的强化学习的稳态误差补偿",
    "translated_abstract": "强化学习中的奖励函数选择对系统性能的影响引起了广泛关注。当使用二次奖励函数时，经常会出现稳态误差的问题。尽管已有的使用绝对值类型奖励函数的解决方案在一定程度上解决了这个问题，但往往会在特定系统状态下引起较大波动，导致突然的变化。为了应对这一挑战，本研究提出了一种引入积分项的方法。通过将这个项积分进二次奖励函数中，对RL算法进行精确调整，增强系统对长期奖励的考虑，从而缓解稳态误差的问题。通过在自适应巡航控制（ACC）模型和变道模型上的实验和性能评估，我们验证了提出的方法不仅有效地减小了稳态误差，还提高了系统的性能表现。",
    "tldr": "该论文研究提出了一种使用积分项补偿二次奖励函数稳态误差的方法，通过增强长期奖励的考虑，有效降低了系统性能（如自适应巡航控制和变道模型）中的稳态误差问题。",
    "en_tdlr": "This paper proposes a method to compensate for steady-state error in quadratic reward functions by introducing an integral term, effectively reducing the performance impact of steady-state errors in systems such as Adaptive Cruise Control and lane change models."
}