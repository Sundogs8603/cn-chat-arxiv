{
    "title": "Abstracted Trajectory Visualization for Explainability in Reinforcement Learning",
    "abstract": "Explainable AI (XAI) has demonstrated the potential to help reinforcement learning (RL) practitioners to understand how RL models work. However, XAI for users who do not have RL expertise (non-RL experts), has not been studied sufficiently. This results in a difficulty for the non-RL experts to participate in the fundamental discussion of how RL models should be designed for an incoming society where humans and AI coexist. Solving such a problem would enable RL experts to communicate with the non-RL experts in producing machine learning solutions that better fit our society. We argue that abstracted trajectories, that depicts transitions between the major states of the RL model, will be useful for non-RL experts to build a mental model of the agents. Our early results suggest that by leveraging a visualization of the abstracted trajectories, users without RL expertise are able to infer the behavior patterns of RL.",
    "link": "https://arxiv.org/abs/2402.07928",
    "context": "Title: Abstracted Trajectory Visualization for Explainability in Reinforcement Learning\nAbstract: Explainable AI (XAI) has demonstrated the potential to help reinforcement learning (RL) practitioners to understand how RL models work. However, XAI for users who do not have RL expertise (non-RL experts), has not been studied sufficiently. This results in a difficulty for the non-RL experts to participate in the fundamental discussion of how RL models should be designed for an incoming society where humans and AI coexist. Solving such a problem would enable RL experts to communicate with the non-RL experts in producing machine learning solutions that better fit our society. We argue that abstracted trajectories, that depicts transitions between the major states of the RL model, will be useful for non-RL experts to build a mental model of the agents. Our early results suggest that by leveraging a visualization of the abstracted trajectories, users without RL expertise are able to infer the behavior patterns of RL.",
    "path": "papers/24/02/2402.07928.json",
    "total_tokens": 773,
    "translated_title": "在强化学习中，模糊轨迹可视化用于可解释性",
    "translated_abstract": "可解释的人工智能（XAI）已经展示出帮助强化学习（RL）从业者理解RL模型工作原理的潜力。然而，对于没有RL专业知识的用户（非RL专家），XAI的研究还不够。这导致非RL专家难以参与如何为人类和AI共存的社会设计RL模型的基本讨论。解决这个问题将使RL专家能够与非RL专家进行沟通，从而生成更适合我们社会的机器学习解决方案。我们认为，描述RL模型主要状态之间转换的模糊轨迹对于非RL专家构建代理的心理模型将是有用的。我们的初步结果表明，通过利用模糊轨迹的可视化，没有RL专业知识的用户能够推断出RL的行为模式。",
    "tldr": "强化学习(RL)模型中使用模糊轨迹可视化，使非RL专家能够推断出RL的行为模式。",
    "en_tdlr": "The use of abstracted trajectory visualization in reinforcement learning (RL) models enables non-RL experts to infer RL behavior patterns."
}