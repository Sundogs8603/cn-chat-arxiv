{
    "title": "Making Pre-trained Language Models Better Continual Few-Shot Relation Extractors",
    "abstract": "arXiv:2402.15713v1 Announce Type: cross  Abstract: Continual Few-shot Relation Extraction (CFRE) is a practical problem that requires the model to continuously learn novel relations while avoiding forgetting old ones with few labeled training data. The primary challenges are catastrophic forgetting and overfitting. This paper harnesses prompt learning to explore the implicit capabilities of pre-trained language models to address the above two challenges, thereby making language models better continual few-shot relation extractors. Specifically, we propose a Contrastive Prompt Learning framework, which designs prompt representation to acquire more generalized knowledge that can be easily adapted to old and new categories, and margin-based contrastive learning to focus more on hard samples, therefore alleviating catastrophic forgetting and overfitting issues. To further remedy overfitting in low-resource scenarios, we introduce an effective memory augmentation strategy that employs well-",
    "link": "https://arxiv.org/abs/2402.15713",
    "context": "Title: Making Pre-trained Language Models Better Continual Few-Shot Relation Extractors\nAbstract: arXiv:2402.15713v1 Announce Type: cross  Abstract: Continual Few-shot Relation Extraction (CFRE) is a practical problem that requires the model to continuously learn novel relations while avoiding forgetting old ones with few labeled training data. The primary challenges are catastrophic forgetting and overfitting. This paper harnesses prompt learning to explore the implicit capabilities of pre-trained language models to address the above two challenges, thereby making language models better continual few-shot relation extractors. Specifically, we propose a Contrastive Prompt Learning framework, which designs prompt representation to acquire more generalized knowledge that can be easily adapted to old and new categories, and margin-based contrastive learning to focus more on hard samples, therefore alleviating catastrophic forgetting and overfitting issues. To further remedy overfitting in low-resource scenarios, we introduce an effective memory augmentation strategy that employs well-",
    "path": "papers/24/02/2402.15713.json",
    "total_tokens": 852,
    "translated_title": "提高预训练语言模型的连续少样本关系提取器能力",
    "translated_abstract": "持续少样本关系提取（CFRE）是一个实际问题，需要模型在避免忘记旧关系的同时连续学习新关系，只有极少量标记训练数据。主要挑战是灾难性遗忘和过拟合。本文利用提示学习来探索预训练语言模型的隐式能力，以解决上述两个挑战，从而使语言模型成为更好的连续少样本关系提取器。具体来说，我们提出了一种对比提示学习框架，设计提示表示以获得更广义的知识，可以轻松适应旧的和新的类别，并基于边界的对比学习，更多地关注困难样本，从而缓解灾难性遗忘和过拟合问题。为了进一步解决低资源场景中的过拟合问题，我们引入了一种有效的记忆增强策略，利用了...",
    "tldr": "提出了一种对比提示学习框架，利用预训练语言模型的潜在能力解决灾难性遗忘和过拟合问题，使其成为更好的连续少样本关系提取器"
}