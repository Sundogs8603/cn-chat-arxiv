{
    "title": "Improving Deep Generative Models on Many-To-One Image-to-Image Translation",
    "abstract": "arXiv:2402.12531v1 Announce Type: cross  Abstract: Deep generative models have been applied to multiple applications in image- to-image translation. Generative Adversarial Networks and Diffusion Models have presented impressive results, setting new state-of-the-art results on these tasks. Most methods have symmetric setups across the different domains in a dataset. These methods assume that all domains have either multiple modalities or only one modality. However, there are many datasets that have a many-to-one relationship between two domains. In this work, we first introduce a Colorized MNIST dataset and a Color-Recall score that can provide a simple benchmark for evaluating models on many-to-one translation. We then introduce a new asymmetric framework to improve existing deep generative models on many-to-one image-to- image translation. We apply this framework to StarGAN V2 and show that in both unsupervised and semi-supervised settings, the performance of this new model improves o",
    "link": "https://arxiv.org/abs/2402.12531",
    "context": "Title: Improving Deep Generative Models on Many-To-One Image-to-Image Translation\nAbstract: arXiv:2402.12531v1 Announce Type: cross  Abstract: Deep generative models have been applied to multiple applications in image- to-image translation. Generative Adversarial Networks and Diffusion Models have presented impressive results, setting new state-of-the-art results on these tasks. Most methods have symmetric setups across the different domains in a dataset. These methods assume that all domains have either multiple modalities or only one modality. However, there are many datasets that have a many-to-one relationship between two domains. In this work, we first introduce a Colorized MNIST dataset and a Color-Recall score that can provide a simple benchmark for evaluating models on many-to-one translation. We then introduce a new asymmetric framework to improve existing deep generative models on many-to-one image-to- image translation. We apply this framework to StarGAN V2 and show that in both unsupervised and semi-supervised settings, the performance of this new model improves o",
    "path": "papers/24/02/2402.12531.json",
    "total_tokens": 899,
    "translated_title": "在多对一图像到图像翻译上改进深度生成模型",
    "translated_abstract": "arXiv:2402.12531v1 通告类型: 跨 针对图像到图像翻译中的多个应用，已应用深度生成模型。 生成对抗网络和扩散模型展示了令人印象深刻的结果，在这些任务上取得了新的最先进结果。 大多数方法在数据集中的不同领域之间具有对称设置。 这些方法假设所有领域都具有多个模态或仅一个模态。 但是，许多数据集存在两个域之间的多对一关系。 在这项工作中，我们首先介绍了一个Colorized MNIST数据集和一个Color-Recall分数，它可以为在多对一翻译上评估模型提供一个简单的基准。 然后，我们引入了一个新的非对称框架，以改进现有的深度生成模型在多对一图像到图像翻译上的表现。 我们将这个框架应用到 StarGAN V2 上，并表明在无监督和半监督设置中，这个新模型的性能得到了提升。",
    "tldr": "介绍了一种新的非对称框架，可改进现有深度生成模型在多对一图像到图像翻译上的效果，并在 StarGAN V2 上展示了其性能优化。",
    "en_tdlr": "Introduces a new asymmetric framework to enhance existing deep generative models on many-to-one image-to-image translation, demonstrated on StarGAN V2 for improved performance in unsupervised and semi-supervised settings."
}