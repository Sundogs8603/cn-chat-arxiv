{
    "title": "CliqueParcel: An Approach For Batching LLM Prompts That Jointly Optimizes Efficiency And Faithfulness",
    "abstract": "arXiv:2402.14833v1 Announce Type: cross  Abstract: Large language models (LLMs) have become pivotal in recent research. However, during the inference process, LLMs still require substantial resources. In this paper, we propose CliqueParcel, a method designed to improve the efficiency of LLMs via prompt batching. Existing strategies to optimize inference efficiency often compromise on output quality, leading to a discounted output problem. This issue might result in reduced accuracy or outputs that are less detailed. CliqueParcel is our answer to this challenge. While ensuring accuracy and minimizing deviations from the original outputs (i.e., faithfulness), our method significantly improves efficiency during inference.   To lay the groundwork, we first redefine efficiency measurements by excluding the reduction in running time due to shorter lengths. Then, we provide a comprehensive trade-off between efficiency and faithfulness to clarify the nature of the 'discounted output' problem. ",
    "link": "https://arxiv.org/abs/2402.14833",
    "context": "Title: CliqueParcel: An Approach For Batching LLM Prompts That Jointly Optimizes Efficiency And Faithfulness\nAbstract: arXiv:2402.14833v1 Announce Type: cross  Abstract: Large language models (LLMs) have become pivotal in recent research. However, during the inference process, LLMs still require substantial resources. In this paper, we propose CliqueParcel, a method designed to improve the efficiency of LLMs via prompt batching. Existing strategies to optimize inference efficiency often compromise on output quality, leading to a discounted output problem. This issue might result in reduced accuracy or outputs that are less detailed. CliqueParcel is our answer to this challenge. While ensuring accuracy and minimizing deviations from the original outputs (i.e., faithfulness), our method significantly improves efficiency during inference.   To lay the groundwork, we first redefine efficiency measurements by excluding the reduction in running time due to shorter lengths. Then, we provide a comprehensive trade-off between efficiency and faithfulness to clarify the nature of the 'discounted output' problem. ",
    "path": "papers/24/02/2402.14833.json",
    "total_tokens": 865,
    "translated_title": "CliqueParcel：一种同时优化效率和忠实度的批处理LLM提示的方法",
    "translated_abstract": "大型语言模型（LLM）在最近的研究中变得至关重要。然而，在推理过程中，LLM仍然需要大量资源。本文提出了CliqueParcel，一种旨在通过提示批处理来提高LLM效率的方法。现有的优化推理效率的策略通常会对输出质量进行妥协，导致折价输出问题。这个问题可能导致准确性降低或输出缺乏细节。CliqueParcel是我们对这一挑战的回应。在确保准确性和最小化与原始输出的偏差（即忠实度）的情况下，我们的方法在推理过程中显著提高了效率。为了奠定基础，我们首先通过排除由于长度缩短而导致的运行时间减少来重新定义效率测量标准。然后，我们提供了效率和忠实度之间的全面权衡，以阐明“折价输出”问题的本质。",
    "tldr": "CliqueParcel提出了一种通过提示批处理来提高LLM效率的方法，旨在在推理过程中同时确保准确性和最小化与原始输出的偏差，解决了折价输出问题。",
    "en_tdlr": "CliqueParcel proposes a method to improve the efficiency of LLMs via prompt batching, aiming to ensure accuracy and minimize deviations from the original outputs during inference, addressing the discounted output problem."
}