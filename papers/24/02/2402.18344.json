{
    "title": "Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning",
    "abstract": "arXiv:2402.18344v1 Announce Type: new  Abstract: Large language models exhibit high-level commonsense reasoning abilities, especially with enhancement methods like Chain-of-Thought (CoT). However, we find these CoT-like methods lead to a considerable number of originally correct answers turning wrong, which we define as the Toxic CoT problem. To interpret and mitigate this problem, we first utilize attribution tracing and causal tracing methods to probe the internal working mechanism of the LLM during CoT reasoning. Through comparisons, we prove that the model exhibits information loss from the question over the shallow attention layers when generating rationales or answers. Based on the probing findings, we design a novel method called RIDERS (Residual decodIng and sERial-position Swap), which compensates for the information deficit in the model from both decoding and serial-position perspectives. Through extensive experiments on multiple commonsense reasoning benchmarks, we validate ",
    "link": "https://arxiv.org/abs/2402.18344",
    "context": "Title: Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning\nAbstract: arXiv:2402.18344v1 Announce Type: new  Abstract: Large language models exhibit high-level commonsense reasoning abilities, especially with enhancement methods like Chain-of-Thought (CoT). However, we find these CoT-like methods lead to a considerable number of originally correct answers turning wrong, which we define as the Toxic CoT problem. To interpret and mitigate this problem, we first utilize attribution tracing and causal tracing methods to probe the internal working mechanism of the LLM during CoT reasoning. Through comparisons, we prove that the model exhibits information loss from the question over the shallow attention layers when generating rationales or answers. Based on the probing findings, we design a novel method called RIDERS (Residual decodIng and sERial-position Swap), which compensates for the information deficit in the model from both decoding and serial-position perspectives. Through extensive experiments on multiple commonsense reasoning benchmarks, we validate ",
    "path": "papers/24/02/2402.18344.json",
    "total_tokens": 836,
    "translated_title": "专注于你的问题！解释和减轻常识推理中的有害CoT问题",
    "translated_abstract": "大型语言模型表现出高水平的常识推理能力，尤其是通过Chain-of-Thought（CoT）等增强方法。然而，我们发现这些类似CoT的方法导致了原本正确的答案变得错误的问题，我们将其定义为有害的CoT问题。为了解释和减轻这一问题，我们首先利用属性跟踪和因果跟踪方法来探究LLM在CoT推理过程中的内部工作机制。通过比较，我们证明了模型在生成推理或答案时存在来自问题的信息丢失现象在浅层注意力层中。基于探究结果，我们设计了一种名为RIDERS（Residual decodIng and sERial-position Swap）的新方法，从解码和序列位置的角度补偿模型中的信息亏缺。通过对多个常识推理基准的广泛实验，我们验证了",
    "tldr": "大型语言模型在常识推理中表现出高水平的能力，但由于信息丢失问题，提出了新方法RIDERS来解释和减轻有害CoT问题",
    "en_tdlr": "Large language models exhibit high-level commonsense reasoning abilities; however, a considerable number of originally correct answers turning wrong due to information loss issue, the novel RIDERS method is proposed to interpret and mitigate the toxic CoT problem."
}