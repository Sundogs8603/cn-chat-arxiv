{
    "title": "Enabling Multi-Agent Transfer Reinforcement Learning via Scenario Independent Representation",
    "abstract": "Multi-Agent Reinforcement Learning (MARL) algorithms are widely adopted in tackling complex tasks that require collaboration and competition among agents in dynamic Multi-Agent Systems (MAS). However, learning such tasks from scratch is arduous and may not always be feasible, particularly for MASs with a large number of interactive agents due to the extensive sample complexity. Therefore, reusing knowledge gained from past experiences or other agents could efficiently accelerate the learning process and upscale MARL algorithms. In this study, we introduce a novel framework that enables transfer learning for MARL through unifying various state spaces into fixed-size inputs that allow one unified deep-learning policy viable in different scenarios within a MAS. We evaluated our approach in a range of scenarios within the StarCraft Multi-Agent Challenge (SMAC) environment, and the findings show significant enhancements in multi-agent learning performance using maneuvering skills learned fr",
    "link": "https://arxiv.org/abs/2402.08184",
    "context": "Title: Enabling Multi-Agent Transfer Reinforcement Learning via Scenario Independent Representation\nAbstract: Multi-Agent Reinforcement Learning (MARL) algorithms are widely adopted in tackling complex tasks that require collaboration and competition among agents in dynamic Multi-Agent Systems (MAS). However, learning such tasks from scratch is arduous and may not always be feasible, particularly for MASs with a large number of interactive agents due to the extensive sample complexity. Therefore, reusing knowledge gained from past experiences or other agents could efficiently accelerate the learning process and upscale MARL algorithms. In this study, we introduce a novel framework that enables transfer learning for MARL through unifying various state spaces into fixed-size inputs that allow one unified deep-learning policy viable in different scenarios within a MAS. We evaluated our approach in a range of scenarios within the StarCraft Multi-Agent Challenge (SMAC) environment, and the findings show significant enhancements in multi-agent learning performance using maneuvering skills learned fr",
    "path": "papers/24/02/2402.08184.json",
    "total_tokens": 904,
    "translated_title": "通过场景无关表示实现多智能体转移强化学习",
    "translated_abstract": "多智能体强化学习（MARL）算法广泛应用于解决在动态的多智能体系统中需要合作和竞争的复杂任务。然而，从头开始学习这种任务是困难且可能不可行的，尤其对于具有大量交互智能体的多智能体系统而言，由于样本复杂性极高。因此，重复使用过去经验或其他智能体获得的知识可以有效加快学习过程并提升MARL算法。在本研究中，我们介绍了一种新颖的框架，通过将各种状态空间统一为固定大小的输入，实现了MARL的转移学习，从而在多智能体系统的不同场景中实现了可行的统一深度学习策略。我们在StarCraft多智能体挑战（SMAC）环境中评估了我们的方法，并发现使用通过机动技能学习得到的知识的多智能体学习性能大幅提升。",
    "tldr": "本研究引入了一种新的框架，通过将各种状态空间统一为固定大小的输入，实现了在多智能体系统中进行转移学习的能力。在SMAC环境中的实验结果表明，通过学习机动技能获得的知识可以显著提高多智能体的学习性能。",
    "en_tdlr": "This study introduces a novel framework that enables transfer learning in multi-agent systems by unifying various state spaces into fixed-size inputs. Experimental results in the SMAC environment show that knowledge gained from learning maneuvering skills significantly enhances multi-agent learning performance."
}