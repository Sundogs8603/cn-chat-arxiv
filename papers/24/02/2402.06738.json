{
    "title": "EntGPT: Linking Generative Large Language Models with Knowledge Bases",
    "abstract": "The ability of Large Language Models (LLMs) to generate factually correct output remains relatively unexplored due to the lack of fact-checking and knowledge grounding during training and inference. In this work, we aim to address this challenge through the Entity Disambiguation (ED) task. We first consider prompt engineering, and design a three-step hard-prompting method to probe LLMs' ED performance without supervised fine-tuning (SFT). Overall, the prompting method improves the micro-F_1 score of the original vanilla models by a large margin, on some cases up to 36% and higher, and obtains comparable performance across 10 datasets when compared to existing methods with SFT. We further improve the knowledge grounding ability through instruction tuning (IT) with similar prompts and responses. The instruction-tuned model not only achieves higher micro-F1 score performance as compared to several baseline methods on supervised entity disambiguation tasks with an average micro-F_1 improve",
    "link": "https://arxiv.org/abs/2402.06738",
    "context": "Title: EntGPT: Linking Generative Large Language Models with Knowledge Bases\nAbstract: The ability of Large Language Models (LLMs) to generate factually correct output remains relatively unexplored due to the lack of fact-checking and knowledge grounding during training and inference. In this work, we aim to address this challenge through the Entity Disambiguation (ED) task. We first consider prompt engineering, and design a three-step hard-prompting method to probe LLMs' ED performance without supervised fine-tuning (SFT). Overall, the prompting method improves the micro-F_1 score of the original vanilla models by a large margin, on some cases up to 36% and higher, and obtains comparable performance across 10 datasets when compared to existing methods with SFT. We further improve the knowledge grounding ability through instruction tuning (IT) with similar prompts and responses. The instruction-tuned model not only achieves higher micro-F1 score performance as compared to several baseline methods on supervised entity disambiguation tasks with an average micro-F_1 improve",
    "path": "papers/24/02/2402.06738.json",
    "total_tokens": 940,
    "translated_title": "EntGPT: 将生成型大型语言模型与知识库相连接",
    "translated_abstract": "由于训练和推理过程中缺乏事实核实和知识基础，大型语言模型（LLM）生成的事实正确输出的能力相对较少被研究。在这项工作中，我们通过Entity Disambiguation（ED）任务来解决这一挑战。我们首先考虑了提示工程，并设计了一个三步硬提示方法，以在没有有监督微调（SFT）的情况下探测LLM的ED性能。总体而言，该提示方法显著提高了原始基准模型的微F_1得分，在某些情况下提高了36%甚至更高，并在10个数据集上与现有的SFT方法相比，获得了可比较的性能。我们通过使用类似的提示和响应进行指令调整（IT）进一步提高了知识基础。指令调整的模型在受监督实体消歧任务上不仅实现了更高的微F1得分性能，而且平均微F_1提高了。",
    "tldr": "本文介绍了一种名为EntGPT的模型，通过Entity Disambiguation（ED）任务，连接了生成型大型语言模型与知识库。通过提示工程和指令调整，该模型在没有有监督微调的情况下，显著提高了LLMs的性能，并在实体消歧任务上取得了可比较的性能。",
    "en_tdlr": "This paper presents EntGPT, a model that links generative large language models with knowledge bases through the Entity Disambiguation (ED) task. Through prompt engineering and instruction tuning, the model significantly improves the performance of LLMs without supervised fine-tuning and achieves comparable performance on entity disambiguation tasks."
}