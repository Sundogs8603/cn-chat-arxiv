{
    "title": "Understanding Likelihood of Normalizing Flow and Image Complexity through the Lens of Out-of-Distribution Detection",
    "abstract": "arXiv:2402.10477v1 Announce Type: new  Abstract: Out-of-distribution (OOD) detection is crucial to safety-critical machine learning applications and has been extensively studied. While recent studies have predominantly focused on classifier-based methods, research on deep generative model (DGM)-based methods have lagged relatively. This disparity may be attributed to a perplexing phenomenon: DGMs often assign higher likelihoods to unknown OOD inputs than to their known training data. This paper focuses on explaining the underlying mechanism of this phenomenon. We propose a hypothesis that less complex images concentrate in high-density regions in the latent space, resulting in a higher likelihood assignment in the Normalizing Flow (NF). We experimentally demonstrate its validity for five NF architectures, concluding that their likelihood is untrustworthy. Additionally, we show that this problem can be alleviated by treating image complexity as an independent variable. Finally, we provi",
    "link": "https://arxiv.org/abs/2402.10477",
    "context": "Title: Understanding Likelihood of Normalizing Flow and Image Complexity through the Lens of Out-of-Distribution Detection\nAbstract: arXiv:2402.10477v1 Announce Type: new  Abstract: Out-of-distribution (OOD) detection is crucial to safety-critical machine learning applications and has been extensively studied. While recent studies have predominantly focused on classifier-based methods, research on deep generative model (DGM)-based methods have lagged relatively. This disparity may be attributed to a perplexing phenomenon: DGMs often assign higher likelihoods to unknown OOD inputs than to their known training data. This paper focuses on explaining the underlying mechanism of this phenomenon. We propose a hypothesis that less complex images concentrate in high-density regions in the latent space, resulting in a higher likelihood assignment in the Normalizing Flow (NF). We experimentally demonstrate its validity for five NF architectures, concluding that their likelihood is untrustworthy. Additionally, we show that this problem can be alleviated by treating image complexity as an independent variable. Finally, we provi",
    "path": "papers/24/02/2402.10477.json",
    "total_tokens": 799,
    "translated_title": "通过异常检测探究正态流模型的可能性和图像复杂度",
    "translated_abstract": "异常检测对于安全关键的机器学习应用至关重要，并得到了广泛研究。本文重点解释了深度生成模型（DGM）常常将未知的OOD输入分配更高的可能性，而不是它们已知的训练数据的潜在机制。我们提出了一个假设，即较简单的图像在潜在空间的高密度区域集中，导致在正态流（NF）中分配更高的可能性。我们通过实验证明了该假设在五种NF体系结构中的有效性，得出它们的可能性是不可信的结论。此外，我们展示了这个问题可以通过将图像复杂度作为一个独立变量来缓解。最后，我们提供了一个...",
    "tldr": "通过实验证明较简单的图像在正态流模型中得到更高可能性，揭示了对该现象的潜在机制，并提出了图像复杂度作为独立变量来解决这一问题。"
}