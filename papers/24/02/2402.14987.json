{
    "title": "On the Performance of Empirical Risk Minimization with Smoothed Data",
    "abstract": "arXiv:2402.14987v1 Announce Type: cross  Abstract: In order to circumvent statistical and computational hardness results in sequential decision-making, recent work has considered smoothed online learning, where the distribution of data at each time is assumed to have bounded likeliehood ratio with respect to a base measure when conditioned on the history. While previous works have demonstrated the benefits of smoothness, they have either assumed that the base measure is known to the learner or have presented computationally inefficient algorithms applying only in special cases. This work investigates the more general setting where the base measure is \\emph{unknown} to the learner, focusing in particular on the performance of Empirical Risk Minimization (ERM) with square loss when the data are well-specified and smooth. We show that in this setting, ERM is able to achieve sublinear error whenever a class is learnable with iid data; in particular, ERM achieves error scaling as $\\tilde O(",
    "link": "https://arxiv.org/abs/2402.14987",
    "context": "Title: On the Performance of Empirical Risk Minimization with Smoothed Data\nAbstract: arXiv:2402.14987v1 Announce Type: cross  Abstract: In order to circumvent statistical and computational hardness results in sequential decision-making, recent work has considered smoothed online learning, where the distribution of data at each time is assumed to have bounded likeliehood ratio with respect to a base measure when conditioned on the history. While previous works have demonstrated the benefits of smoothness, they have either assumed that the base measure is known to the learner or have presented computationally inefficient algorithms applying only in special cases. This work investigates the more general setting where the base measure is \\emph{unknown} to the learner, focusing in particular on the performance of Empirical Risk Minimization (ERM) with square loss when the data are well-specified and smooth. We show that in this setting, ERM is able to achieve sublinear error whenever a class is learnable with iid data; in particular, ERM achieves error scaling as $\\tilde O(",
    "path": "papers/24/02/2402.14987.json",
    "total_tokens": 876,
    "translated_title": "在平滑数据上的经验风险最小化性能研究",
    "translated_abstract": "为了避开在序贯决策中的统计和计算困难结果，最近的工作考虑了平滑的在线学习，其中假设每个时间点的数据分布在给定历史条件下相对于基础度量具有有界的似然比。虽然先前的工作已经证明了平滑性的好处，但它们要么假设基础度量对学习者是已知的，要么提出的算法在特殊情况下仅适用于计算效率低下。本研究研究了更一般的设置，即基础度量对学习者是\\emph{未知}的情况，特别关注在数据明确定和平滑的情况下，当数据是良好指定时经验风险最小化（ERM）与平方损失的性能。我们展示，在这种设置下，只要类是可从iid数据中学习的，ERM就能够实现次线性误差；特别是，当数据是iid时，ERM实现的错误尺度为$\\tilde O(",
    "tldr": "在数据是良好指定和平滑的情况下，对于经验风险最小化（ERM）与平方损失的性能，当类是可从 iid 数据中学习时，ERM能够实现次线性误差。",
    "en_tdlr": "ERM is able to achieve sublinear error in the setting of well-specified and smooth data, when a class is learnable from iid data."
}