{
    "title": "On the Conflict of Robustness and Learning in Collaborative Machine Learning",
    "abstract": "arXiv:2402.13700v1 Announce Type: new  Abstract: Collaborative Machine Learning (CML) allows participants to jointly train a machine learning model while keeping their training data private. In scenarios where privacy is a strong requirement, such as health-related applications, safety is also a primary concern. This means that privacy-preserving CML processes must produce models that output correct and reliable decisions \\emph{even in the presence of potentially untrusted participants}. In response to this issue, researchers propose to use \\textit{robust aggregators} that rely on metrics which help filter out malicious contributions that could compromise the training process. In this work, we formalize the landscape of robust aggregators in the literature. Our formalization allows us to show that existing robust aggregators cannot fulfill their goal: either they use distance-based metrics that cannot accurately identify targeted malicious updates; or propose methods whose success is i",
    "link": "https://arxiv.org/abs/2402.13700",
    "context": "Title: On the Conflict of Robustness and Learning in Collaborative Machine Learning\nAbstract: arXiv:2402.13700v1 Announce Type: new  Abstract: Collaborative Machine Learning (CML) allows participants to jointly train a machine learning model while keeping their training data private. In scenarios where privacy is a strong requirement, such as health-related applications, safety is also a primary concern. This means that privacy-preserving CML processes must produce models that output correct and reliable decisions \\emph{even in the presence of potentially untrusted participants}. In response to this issue, researchers propose to use \\textit{robust aggregators} that rely on metrics which help filter out malicious contributions that could compromise the training process. In this work, we formalize the landscape of robust aggregators in the literature. Our formalization allows us to show that existing robust aggregators cannot fulfill their goal: either they use distance-based metrics that cannot accurately identify targeted malicious updates; or propose methods whose success is i",
    "path": "papers/24/02/2402.13700.json",
    "total_tokens": 909,
    "translated_title": "在协作机器学习中稳健性和学习的冲突",
    "translated_abstract": "协作机器学习（CML）允许参与者共同训练机器学习模型，同时保持他们的训练数据私密。在隐私是一个强烈要求的情况下，比如健康相关应用中，安全也是首要关注的问题。这意味着保护隐私的CML流程必须产生能够输出正确可靠决策的模型，甚至在可能不受信任参与者的情况下也是如此。为了解决这个问题，研究人员提出使用依赖于帮助过滤可能危及训练过程的恶意贡献的度量的“稳健聚合器”。在这项工作中，我们在文献中规范化了稳健聚合器的进展。我们的规范化能够表明现有的稳健聚合器无法实现其目标：无论是它们使用无法准确识别有针对性的恶意更新的基于距离的度量；还是提出的方法成功率不够。",
    "tldr": "在协作机器学习中，研究人员正式规范了稳健聚合器的领域，并发现现有的稳健聚合器无法实现其目标，要么无法准确识别有针对性的恶意更新，要么方法成功率不够。"
}