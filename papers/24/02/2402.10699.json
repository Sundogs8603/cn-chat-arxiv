{
    "title": "Rethinking Human-like Translation Strategy: Integrating Drift-Diffusion Model with Large Language Models for Machine Translation",
    "abstract": "arXiv:2402.10699v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated promising potential in various downstream tasks, including machine translation. However, prior work on LLM-based machine translation has mainly focused on better utilizing training data, demonstrations, or pre-defined and universal knowledge to improve performance, with a lack of consideration of decision-making like human translators. In this paper, we incorporate Thinker with the Drift-Diffusion Model (Thinker-DDM) to address this issue. We then redefine the Drift-Diffusion process to emulate human translators' dynamic decision-making under constrained resources. We conduct extensive experiments under the high-resource, low-resource, and commonsense translation settings using the WMT22 and CommonMT datasets, in which Thinker-DDM outperforms baselines in the first two scenarios. We also perform additional analysis and evaluation on commonsense translation to illustrate the high effectivenes",
    "link": "https://arxiv.org/abs/2402.10699",
    "context": "Title: Rethinking Human-like Translation Strategy: Integrating Drift-Diffusion Model with Large Language Models for Machine Translation\nAbstract: arXiv:2402.10699v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated promising potential in various downstream tasks, including machine translation. However, prior work on LLM-based machine translation has mainly focused on better utilizing training data, demonstrations, or pre-defined and universal knowledge to improve performance, with a lack of consideration of decision-making like human translators. In this paper, we incorporate Thinker with the Drift-Diffusion Model (Thinker-DDM) to address this issue. We then redefine the Drift-Diffusion process to emulate human translators' dynamic decision-making under constrained resources. We conduct extensive experiments under the high-resource, low-resource, and commonsense translation settings using the WMT22 and CommonMT datasets, in which Thinker-DDM outperforms baselines in the first two scenarios. We also perform additional analysis and evaluation on commonsense translation to illustrate the high effectivenes",
    "path": "papers/24/02/2402.10699.json",
    "total_tokens": 898,
    "translated_title": "重新思考类人翻译策略：将漂移扩散模型与大型语言模型集成用于机器翻译",
    "translated_abstract": "大型语言模型（LLMs）在包括机器翻译在内的各种下游任务中展现出了巨大潜力。然而，基于LLM的机器翻译先前的工作主要集中在更好地利用训练数据、演示版本或预定义的普遍知识来提高性能，缺乏对类似人类翻译者的决策制定的考虑。本文将“Thinker”与漂移扩散模型（Thinker-DDM）相结合，以解决这一问题。然后，我们重新定义了漂移扩散过程，以模拟受限资源情况下类人翻译者的动态决策制定。我们在高资源、低资源和常识翻译设置下，使用WMT22和CommonMT数据集进行了大量实验，在前两种场景中，Thinker-DDM的表现优于基准。我们还对常识翻译进行了额外的分析和评估，以说明其高效性。",
    "tldr": "将Thinker与漂移扩散模型集成，重新定义漂移扩散过程以模拟人类翻译者的决策制定，实验证明在机器翻译中取得了优异成绩。"
}