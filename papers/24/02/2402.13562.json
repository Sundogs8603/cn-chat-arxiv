{
    "title": "Analysis of Multi-Source Language Training in Cross-Lingual Transfer",
    "abstract": "arXiv:2402.13562v1 Announce Type: new  Abstract: The successful adaptation of multilingual language models (LMs) to a specific language-task pair critically depends on the availability of data tailored for that condition. While cross-lingual transfer (XLT) methods have contributed to addressing this data scarcity problem, there still exists ongoing debate about the mechanisms behind their effectiveness. In this work, we focus on one of promising assumptions about inner workings of XLT, that it encourages multilingual LMs to place greater emphasis on language-agnostic or task-specific features. We test this hypothesis by examining how the patterns of XLT change with a varying number of source languages involved in the process. Our experimental findings show that the use of multiple source languages in XLT-a technique we term Multi-Source Language Training (MSLT)-leads to increased mingling of embedding spaces for different languages, supporting the claim that XLT benefits from making us",
    "link": "https://arxiv.org/abs/2402.13562",
    "context": "Title: Analysis of Multi-Source Language Training in Cross-Lingual Transfer\nAbstract: arXiv:2402.13562v1 Announce Type: new  Abstract: The successful adaptation of multilingual language models (LMs) to a specific language-task pair critically depends on the availability of data tailored for that condition. While cross-lingual transfer (XLT) methods have contributed to addressing this data scarcity problem, there still exists ongoing debate about the mechanisms behind their effectiveness. In this work, we focus on one of promising assumptions about inner workings of XLT, that it encourages multilingual LMs to place greater emphasis on language-agnostic or task-specific features. We test this hypothesis by examining how the patterns of XLT change with a varying number of source languages involved in the process. Our experimental findings show that the use of multiple source languages in XLT-a technique we term Multi-Source Language Training (MSLT)-leads to increased mingling of embedding spaces for different languages, supporting the claim that XLT benefits from making us",
    "path": "papers/24/02/2402.13562.json",
    "total_tokens": 848,
    "translated_title": "多源语言训练在跨语言转移中的分析",
    "translated_abstract": "成功地将多语言语言模型（LMs）调整到特定语言-任务对上至关重要的是定制数据的可用性。虽然跨语言转移（XLT）方法有助于解决这种数据稀缺问题，但关于其有效性背后的机制仍存在持续的讨论。在这项工作中，我们关注了关于XLT内部工作的一个有希望的假设，即它鼓励多语言LMs更加强调语言不可知或任务特定特征。我们通过考察XLT随涉及过程中源语言数量的变化而改变的模式来测试这一假设。我们的实验结果表明，在XLT中使用多个源语言-一种我们称之为多源语言训练（MSLT）的技术-会导致不同语言的嵌入空间的交织增加，支持了XLT受益于这一点的说法。",
    "tldr": "多源语言训练（MSLT）技术通过使用多个源语言，在跨语言转移中增加了不同语言嵌入空间的交织，从而支持了XLT受益于这种方法的说法。",
    "en_tdlr": "The Multi-Source Language Training (MSLT) technique, by using multiple source languages, increases the intermingling of embedding spaces for different languages in cross-lingual transfer (XLT), supporting the claim that XLT benefits from this approach."
}