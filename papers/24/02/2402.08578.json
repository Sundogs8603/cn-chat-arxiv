{
    "title": "FedLPS: Heterogeneous Federated Learning for Multiple Tasks with Local Parameter Sharing",
    "abstract": "Federated Learning (FL) has emerged as a promising solution in Edge Computing (EC) environments to process the proliferation of data generated by edge devices. By collaboratively optimizing the global machine learning models on distributed edge devices, FL circumvents the need for transmitting raw data and enhances user privacy. Despite practical successes, FL still confronts significant challenges including constrained edge device resources, multiple tasks deployment, and data heterogeneity. However, existing studies focus on mitigating the FL training costs of each single task whereas neglecting the resource consumption across multiple tasks in heterogeneous FL scenarios. In this paper, we propose Heterogeneous Federated Learning with Local Parameter Sharing (FedLPS) to fill this gap. FedLPS leverages principles from transfer learning to facilitate the deployment of multiple tasks on a single device by dividing the local model into a shareable encoder and task-specific encoders. To f",
    "link": "https://arxiv.org/abs/2402.08578",
    "context": "Title: FedLPS: Heterogeneous Federated Learning for Multiple Tasks with Local Parameter Sharing\nAbstract: Federated Learning (FL) has emerged as a promising solution in Edge Computing (EC) environments to process the proliferation of data generated by edge devices. By collaboratively optimizing the global machine learning models on distributed edge devices, FL circumvents the need for transmitting raw data and enhances user privacy. Despite practical successes, FL still confronts significant challenges including constrained edge device resources, multiple tasks deployment, and data heterogeneity. However, existing studies focus on mitigating the FL training costs of each single task whereas neglecting the resource consumption across multiple tasks in heterogeneous FL scenarios. In this paper, we propose Heterogeneous Federated Learning with Local Parameter Sharing (FedLPS) to fill this gap. FedLPS leverages principles from transfer learning to facilitate the deployment of multiple tasks on a single device by dividing the local model into a shareable encoder and task-specific encoders. To f",
    "path": "papers/24/02/2402.08578.json",
    "total_tokens": 879,
    "translated_title": "FedLPS: 多任务异构联邦学习中的本地参数共享",
    "translated_abstract": "联邦学习（FL）已成为边缘计算环境中处理边缘设备生成的大量数据的一种有希望的解决方案。通过在分布式边缘设备上共同优化全局机器学习模型，FL避免了传输原始数据的需求，并增强了用户隐私保护。尽管实际上取得了成功，但FL仍然面临重大挑战，包括有限的边缘设备资源、多任务部署和数据异构性。然而，现有研究侧重于减少每个单独任务的FL训练成本，忽略了在异构FL场景中多个任务之间的资源消耗。在本文中，我们提出了一种称为具有本地参数共享的异构联邦学习（FedLPS）的方法来填补这一空白。FedLPS利用迁移学习的原理，在单个设备上实现多任务部署，通过将本地模型划分为可共享的编码器和任务特定的编码器。",
    "tldr": "FedLPS提出了一种在边缘计算环境中处理边缘设备生成的数据的多任务异构联邦学习方法，通过本地参数共享和迁移学习的原理来减少资源消耗和提高部署效率。",
    "en_tdlr": "FedLPS proposes a method for handling data generated by edge devices in Edge Computing environments, using heterogeneous federated learning for multiple tasks with local parameter sharing and transfer learning principles to reduce resource consumption and improve deployment efficiency."
}