{
    "title": "Communication Efficient ConFederated Learning: An Event-Triggered SAGA Approach",
    "abstract": "arXiv:2402.18018v1 Announce Type: new  Abstract: Federated learning (FL) is a machine learning paradigm that targets model training without gathering the local data dispersed over various data sources. Standard FL, which employs a single server, can only support a limited number of users, leading to degraded learning capability. In this work, we consider a multi-server FL framework, referred to as \\emph{Confederated Learning} (CFL), in order to accommodate a larger number of users. A CFL system is composed of multiple networked edge servers, with each server connected to an individual set of users. Decentralized collaboration among servers is leveraged to harness all users' data for model training. Due to the potentially massive number of users involved, it is crucial to reduce the communication overhead of the CFL system. We propose a stochastic gradient method for distributed learning in the CFL framework. The proposed method incorporates a conditionally-triggered user selection (CTU",
    "link": "https://arxiv.org/abs/2402.18018",
    "context": "Title: Communication Efficient ConFederated Learning: An Event-Triggered SAGA Approach\nAbstract: arXiv:2402.18018v1 Announce Type: new  Abstract: Federated learning (FL) is a machine learning paradigm that targets model training without gathering the local data dispersed over various data sources. Standard FL, which employs a single server, can only support a limited number of users, leading to degraded learning capability. In this work, we consider a multi-server FL framework, referred to as \\emph{Confederated Learning} (CFL), in order to accommodate a larger number of users. A CFL system is composed of multiple networked edge servers, with each server connected to an individual set of users. Decentralized collaboration among servers is leveraged to harness all users' data for model training. Due to the potentially massive number of users involved, it is crucial to reduce the communication overhead of the CFL system. We propose a stochastic gradient method for distributed learning in the CFL framework. The proposed method incorporates a conditionally-triggered user selection (CTU",
    "path": "papers/24/02/2402.18018.json",
    "total_tokens": 822,
    "translated_title": "通信高效的ConFederated Learning: 基于事件触发的SAGA方法",
    "translated_abstract": "联邦学习（FL）是一种旨在进行模型训练而无需收集分散在各种数据源上的本地数据的机器学习范式。标准FL只能支持有限数量的用户，从而导致学习能力下降。本研究考虑了一种多服务器FL框架，称为\"ConFederated Learning\"（CFL），以容纳更多用户。CFL系统由多个网络边缘服务器组成，每个服务器连接到一组独立的用户。利用服务器之间的分散协作来利用所有用户的数据进行模型训练。由于涉及的用户数量可能很大，因此减少CFL系统的通信开销至关重要。我们提出了一种用于CFL框架中的分布式学习的随机梯度方法。所提出的方法包括一种有条件触发的用户选择（CTU",
    "tldr": "本研究提出了一种基于事件触发的SAGA方法，用于通信高效的ConFederated Learning，在多服务器FL框架中实现分布式学习。",
    "en_tdlr": "This work introduces an event-triggered SAGA approach for communication efficient ConFederated Learning, enabling distributed learning in a multi-server FL framework."
}