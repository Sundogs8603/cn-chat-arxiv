{
    "title": "Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation",
    "abstract": "arXiv:2402.11443v1 Announce Type: new  Abstract: This paper presents a benchmark self-evolving framework to dynamically evaluate rapidly advancing Large Language Models (LLMs), aiming for a more accurate assessment of their capabilities and limitations. We utilize a multi-agent system to manipulate the context or question of original instances, reframing new evolving instances with high confidence that dynamically extend existing benchmarks. Towards a more scalable, robust and fine-grained evaluation, we implement six reframing operations to construct evolving instances testing LLMs against diverse queries, data noise and probing their problem-solving sub-abilities. With this framework, we extend benchmark datasets of four tasks. Experimental results show a general performance decline in most LLMs against their original results. This decline under our scalable and robust evaluations, alongside our fine-grained evaluation, more accurately reflect models' capabilities. Besides, our frame",
    "link": "https://arxiv.org/abs/2402.11443",
    "context": "Title: Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation\nAbstract: arXiv:2402.11443v1 Announce Type: new  Abstract: This paper presents a benchmark self-evolving framework to dynamically evaluate rapidly advancing Large Language Models (LLMs), aiming for a more accurate assessment of their capabilities and limitations. We utilize a multi-agent system to manipulate the context or question of original instances, reframing new evolving instances with high confidence that dynamically extend existing benchmarks. Towards a more scalable, robust and fine-grained evaluation, we implement six reframing operations to construct evolving instances testing LLMs against diverse queries, data noise and probing their problem-solving sub-abilities. With this framework, we extend benchmark datasets of four tasks. Experimental results show a general performance decline in most LLMs against their original results. This decline under our scalable and robust evaluations, alongside our fine-grained evaluation, more accurately reflect models' capabilities. Besides, our frame",
    "path": "papers/24/02/2402.11443.json",
    "total_tokens": 893,
    "translated_title": "基准自我演进: 用于动态LLM评估的多Agent框架",
    "translated_abstract": "这篇论文介绍了一个基准自我演进框架，用于动态评估快速发展的大型语言模型（LLMs），旨在更准确地评估它们的能力和局限性。我们利用一个多Agent系统来操作原始实例的环境或问题，通过重构新的演化实例来扩展现有基准，以更具高信心地动态扩展现有基准。为了实现更具可扩展性、强健性和精细化评价，我们实施了六个重构操作来构建演化实例，测试LLMs对各种查询、数据噪声并探究它们的问题解决子能力。通过这个框架，我们扩展了四个任务的基准数据集。实验结果显示，在大多数LLMs中，针对原始结果表现出普遍的性能下降。在我们的可扩展和强健的评估下，以及我们的精细化评估下，这种下降更准确地反映了模型的能力。",
    "tldr": "介绍了一个基准自我演进框架，通过多Agent系统操作环境或问题，构建演化实例来更准确地评估LLMs的能力和限制，并扩展基准数据集以进行更具可扩展性和精细化的评估。"
}