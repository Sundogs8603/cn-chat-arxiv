{
    "title": "Data-Efficient Learning via Clustering-Based Sensitivity Sampling: Foundation Models and Beyond",
    "abstract": "arXiv:2402.17327v1 Announce Type: new  Abstract: We study the data selection problem, whose aim is to select a small representative subset of data that can be used to efficiently train a machine learning model. We present a new data selection approach based on $k$-means clustering and sensitivity sampling. Assuming access to an embedding representation of the data with respect to which the model loss is H\\\"older continuous, our approach provably allows selecting a set of ``typical'' $k + 1/\\varepsilon^2$ elements whose average loss corresponds to the average loss of the whole dataset, up to a multiplicative $(1\\pm\\varepsilon)$ factor and an additive $\\varepsilon \\lambda \\Phi_k$, where $\\Phi_k$ represents the $k$-means cost for the input embeddings and $\\lambda$ is the H\\\"older constant.   We furthermore demonstrate the performance and scalability of our approach on fine-tuning foundation models and show that it outperforms state-of-the-art methods. We also show how it can be applied on",
    "link": "https://arxiv.org/abs/2402.17327",
    "context": "Title: Data-Efficient Learning via Clustering-Based Sensitivity Sampling: Foundation Models and Beyond\nAbstract: arXiv:2402.17327v1 Announce Type: new  Abstract: We study the data selection problem, whose aim is to select a small representative subset of data that can be used to efficiently train a machine learning model. We present a new data selection approach based on $k$-means clustering and sensitivity sampling. Assuming access to an embedding representation of the data with respect to which the model loss is H\\\"older continuous, our approach provably allows selecting a set of ``typical'' $k + 1/\\varepsilon^2$ elements whose average loss corresponds to the average loss of the whole dataset, up to a multiplicative $(1\\pm\\varepsilon)$ factor and an additive $\\varepsilon \\lambda \\Phi_k$, where $\\Phi_k$ represents the $k$-means cost for the input embeddings and $\\lambda$ is the H\\\"older constant.   We furthermore demonstrate the performance and scalability of our approach on fine-tuning foundation models and show that it outperforms state-of-the-art methods. We also show how it can be applied on",
    "path": "papers/24/02/2402.17327.json",
    "total_tokens": 858,
    "translated_title": "基于聚类敏感度采样的数据高效学习：基础模型及其延伸",
    "translated_abstract": "我们研究了数据选择问题，其目的是选择一个小的代表性数据子集，可以用来高效地训练机器学习模型。我们提出了一种基于$k$-means聚类和敏感度采样的新数据选择方法。假设我们可以访问数据的嵌入表示，其中模型损失是H\\\"older连续的，我们的方法可以选择一组“典型”的$k + 1/\\varepsilon^2$个元素，这些元素的平均损失与整个数据集的平均损失对应，乘以一个$(1\\pm\\varepsilon)$的因子并加上一个$\\varepsilon \\lambda \\Phi_k$，其中$\\Phi_k$代表输入嵌入的$k$-means成本，$\\lambda$是H\\\"older常数。此外，我们展示了我们的方法在微调基础模型上的性能和可扩展性，并表明它胜过了最先进的方法。",
    "tldr": "通过基于聚类和敏感度采样的数据选择方法，可以高效选择代表性数据子集来训练机器学习模型，在微调基础模型方面表现优异。"
}