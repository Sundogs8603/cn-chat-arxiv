{
    "title": "From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks via Translation of Text Embeddings",
    "abstract": "arXiv:2402.16006v1 Announce Type: new  Abstract: The safety defense methods of Large language models(LLMs) stays limited because the dangerous prompts are manually curated to just few known attack types, which fails to keep pace with emerging varieties. Recent studies found that attaching suffixes to harmful instructions can hack the defense of LLMs and lead to dangerous outputs. This method, while effective, leaves a gap in understanding the underlying mechanics of such adversarial suffix due to the non-readability and it can be relatively easily seen through by common defense methods such as perplexity filters.To cope with this challenge, in this paper, we propose an Adversarial Suffixes Embedding Translation Framework(ASETF) that are able to translate the unreadable adversarial suffixes into coherent, readable text, which makes it easier to understand and analyze the reasons behind harmful content generation by large language models. We conducted experiments on LLMs such as LLaMa2, ",
    "link": "https://arxiv.org/abs/2402.16006",
    "context": "Title: From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks via Translation of Text Embeddings\nAbstract: arXiv:2402.16006v1 Announce Type: new  Abstract: The safety defense methods of Large language models(LLMs) stays limited because the dangerous prompts are manually curated to just few known attack types, which fails to keep pace with emerging varieties. Recent studies found that attaching suffixes to harmful instructions can hack the defense of LLMs and lead to dangerous outputs. This method, while effective, leaves a gap in understanding the underlying mechanics of such adversarial suffix due to the non-readability and it can be relatively easily seen through by common defense methods such as perplexity filters.To cope with this challenge, in this paper, we propose an Adversarial Suffixes Embedding Translation Framework(ASETF) that are able to translate the unreadable adversarial suffixes into coherent, readable text, which makes it easier to understand and analyze the reasons behind harmful content generation by large language models. We conducted experiments on LLMs such as LLaMa2, ",
    "path": "papers/24/02/2402.16006.json",
    "total_tokens": 911,
    "translated_title": "从噪音到清晰：通过文本嵌入的翻译揭示大型语言模型攻击的敌对后缀",
    "translated_abstract": "大型语言模型（LLMs）的安全防御方法仍然有限，因为危险提示被手工策划为仅几种已知的攻击类型，这丧失了与新兴变体同步的能力。最近的研究发现，在有害指令后添加后缀可以突破LLMs的防御，并导致危险输出。虽然这种方法是有效的，但由于不可读性，存在一种孔隙，使得通过常见的防御方法如困惑度过滤器相对容易看穿这种对抗性后缀的内在机制。为了应对这一挑战，本文提出了一种敌对后缀嵌入翻译框架（ASETF），可以将不可读的敌对后缀翻译成连贯的可读文本，从而更容易理解和分析大型语言模型生成有害内容的原因。我们在LLMs上进行了实验，如LLaMa2等。",
    "tldr": "通过Adversarial Suffixes Embedding Translation Framework，将不可读的敌对后缀翻译为连贯、可读的文本，有助于更容易理解和分析大型语言模型生成有害内容的原因。",
    "en_tdlr": "The paper proposes an Adversarial Suffixes Embedding Translation Framework to translate unreadable adversarial suffixes into coherent, readable text, facilitating a better understanding and analysis of the reasons behind harmful content generation by large language models."
}