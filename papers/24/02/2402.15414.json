{
    "title": "Does Combining Parameter-efficient Modules Improve Few-shot Transfer Accuracy?",
    "abstract": "arXiv:2402.15414v1 Announce Type: new  Abstract: Parameter-efficient fine-tuning stands as the standard for efficiently fine-tuning large language and vision models on downstream tasks. Specifically, the efficiency of low-rank adaptation has facilitated the creation and sharing of hundreds of custom LoRA modules, each trained on distinct data from various downstream tasks. In this paper, we explore the composability of LoRA modules, examining if combining these pre-trained modules enhances generalization to unseen downstream tasks. Our investigation involves evaluating two approaches: (a) uniform composition, involving averaging upstream LoRA modules with equal weights, and (b) learned composition, where we learn the weights for each upstream module and perform weighted averaging. Our experimental results on both vision and language models reveal that in few-shot settings, where only a limited number of samples are available for the downstream task, both uniform and learned composition",
    "link": "https://arxiv.org/abs/2402.15414",
    "context": "Title: Does Combining Parameter-efficient Modules Improve Few-shot Transfer Accuracy?\nAbstract: arXiv:2402.15414v1 Announce Type: new  Abstract: Parameter-efficient fine-tuning stands as the standard for efficiently fine-tuning large language and vision models on downstream tasks. Specifically, the efficiency of low-rank adaptation has facilitated the creation and sharing of hundreds of custom LoRA modules, each trained on distinct data from various downstream tasks. In this paper, we explore the composability of LoRA modules, examining if combining these pre-trained modules enhances generalization to unseen downstream tasks. Our investigation involves evaluating two approaches: (a) uniform composition, involving averaging upstream LoRA modules with equal weights, and (b) learned composition, where we learn the weights for each upstream module and perform weighted averaging. Our experimental results on both vision and language models reveal that in few-shot settings, where only a limited number of samples are available for the downstream task, both uniform and learned composition",
    "path": "papers/24/02/2402.15414.json",
    "total_tokens": 857,
    "translated_title": "组合高效参数模块是否改善小样本迁移精度？",
    "translated_abstract": "参数高效微调被视为在下游任务上高效微调大型语言和视觉模型的标准。特别是，低秩参数适应性的高效性促进了数百个定制LoRA模块的创建和共享，每个模块都是在各种不同下游任务的数据上训练而成。在本文中，我们探索了LoRA模块的可组合性，研究了合并这些预训练模块是否增强了对未知下游任务的泛化能力。我们的研究涉及评估两种方法：（a）均匀组成，包括将上游LoRA模块均匀加权平均，以及（b）学习组成，我们在每个上游模块上学习权重，并执行加权平均。我们在视觉和语言模型上的实验结果表明，在少样本情景中，即只有有限数量的样本可用于下游任务时，无论是均匀组成还是学习组成",
    "tldr": "本文探索了LoRA模块的可组合性，并发现在少样本情景下，无论是均匀组成还是学习组成，对视觉和语言模型进行组合都可以提高对未知下游任务的泛化能力。",
    "en_tdlr": "This paper explores the composability of LoRA modules and finds that in few-shot scenarios, combining both uniform and learned composition can improve the generalization to unseen downstream tasks for vision and language models."
}