{
    "title": "TEncDM: Understanding the Properties of Diffusion Model in the Space of Language Model Encodings",
    "abstract": "arXiv:2402.19097v1 Announce Type: new  Abstract: Drawing inspiration from the success of diffusion models in various domains, numerous research papers proposed methods for adapting them to text data. Despite these efforts, none of them has managed to achieve the quality of the large language models. In this paper, we conduct a comprehensive analysis of key components of the text diffusion models and introduce a novel approach named Text Encoding Diffusion Model (TEncDM). Instead of the commonly used token embedding space, we train our model in the space of the language model encodings. Additionally, we propose to use a Transformer-based decoder that utilizes contextual information for text reconstruction. We also analyse self-conditioning and find that it increases the magnitude of the model outputs, allowing the reduction of the number of denoising steps at the inference stage. Evaluation of TEncDM on two downstream text generation tasks, QQP and XSum, demonstrates its superiority ove",
    "link": "https://arxiv.org/abs/2402.19097",
    "context": "Title: TEncDM: Understanding the Properties of Diffusion Model in the Space of Language Model Encodings\nAbstract: arXiv:2402.19097v1 Announce Type: new  Abstract: Drawing inspiration from the success of diffusion models in various domains, numerous research papers proposed methods for adapting them to text data. Despite these efforts, none of them has managed to achieve the quality of the large language models. In this paper, we conduct a comprehensive analysis of key components of the text diffusion models and introduce a novel approach named Text Encoding Diffusion Model (TEncDM). Instead of the commonly used token embedding space, we train our model in the space of the language model encodings. Additionally, we propose to use a Transformer-based decoder that utilizes contextual information for text reconstruction. We also analyse self-conditioning and find that it increases the magnitude of the model outputs, allowing the reduction of the number of denoising steps at the inference stage. Evaluation of TEncDM on two downstream text generation tasks, QQP and XSum, demonstrates its superiority ove",
    "path": "papers/24/02/2402.19097.json",
    "total_tokens": 851,
    "translated_title": "TEncDM: 在语言模型编码空间中理解扩散模型的属性",
    "translated_abstract": "受到扩散模型在各个领域取得成功的启发，许多研究论文提出了将其应用于文本数据的方法。尽管有这些努力，但没有一种方法能够达到大型语言模型的质量。本文对文本扩散模型的关键组件进行了全面分析，并介绍了一种名为Text Encoding Diffusion Model (TEncDM)的新方法。我们在语言模型编码空间中训练我们的模型，而不是通常使用的标记嵌入空间。此外，我们提出使用基于Transformer的解码器，利用上下文信息进行文本重构。我们还分析了自我调节，并发现这会增加模型输出的数量级，从而减少推理阶段的去噪步骤数量。在两个下游文本生成任务QQP和XSum上对TEncDM的评估表明其优越性。",
    "tldr": "通过在语言模型编码空间中训练模型，并使用基于Transformer的解码器以及自我调节，本文提出了名为TEncDM的文本编码扩散模型，在两个文本生成任务上展示了其优越性",
    "en_tdlr": "By training the model in the space of language model encodings, incorporating a Transformer-based decoder, and self-conditioning, this paper introduces a Text Encoding Diffusion Model (TEncDM) which demonstrates superiority in two text generation tasks."
}