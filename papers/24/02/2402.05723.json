{
    "title": "In-Context Learning Can Re-learn Forbidden Tasks",
    "abstract": "Despite significant investment into safety training, large language models (LLMs) deployed in the real world still suffer from numerous vulnerabilities. One perspective on LLM safety training is that it algorithmically forbids the model from answering toxic or harmful queries. To assess the effectiveness of safety training, in this work, we study forbidden tasks, i.e., tasks the model is designed to refuse to answer. Specifically, we investigate whether in-context learning (ICL) can be used to re-learn forbidden tasks despite the explicit fine-tuning of the model to refuse them. We first examine a toy example of refusing sentiment classification to demonstrate the problem. Then, we use ICL on a model fine-tuned to refuse to summarise made-up news articles. Finally, we investigate whether ICL can undo safety training, which could represent a major security risk. For the safety task, we look at Vicuna-7B, Starling-7B, and Llama2-7B. We show that the attack works out-of-the-box on Starlin",
    "link": "https://arxiv.org/abs/2402.05723",
    "context": "Title: In-Context Learning Can Re-learn Forbidden Tasks\nAbstract: Despite significant investment into safety training, large language models (LLMs) deployed in the real world still suffer from numerous vulnerabilities. One perspective on LLM safety training is that it algorithmically forbids the model from answering toxic or harmful queries. To assess the effectiveness of safety training, in this work, we study forbidden tasks, i.e., tasks the model is designed to refuse to answer. Specifically, we investigate whether in-context learning (ICL) can be used to re-learn forbidden tasks despite the explicit fine-tuning of the model to refuse them. We first examine a toy example of refusing sentiment classification to demonstrate the problem. Then, we use ICL on a model fine-tuned to refuse to summarise made-up news articles. Finally, we investigate whether ICL can undo safety training, which could represent a major security risk. For the safety task, we look at Vicuna-7B, Starling-7B, and Llama2-7B. We show that the attack works out-of-the-box on Starlin",
    "path": "papers/24/02/2402.05723.json",
    "total_tokens": 966,
    "translated_title": "在上下文学习中可以重新学习禁止的任务",
    "translated_abstract": "尽管对安全培训进行了大量投入，但在现实世界中部署的大型语言模型（LLMs）仍然存在许多漏洞。有一种观点认为，LLM安全培训可以通过算法禁止模型回答有毒或有害的查询。为了评估安全培训的有效性，本研究研究了禁止任务，即模型设计为拒绝回答的任务。具体而言，我们研究了在明确调优模型拒绝禁止任务的情况下，上下文学习（ICL）是否可以用于重新学习禁止任务。我们首先通过一个拒绝情感分类的玩具示例来演示问题。然后，我们使用ICL来处理一个被细化拒绝总结虚构新闻文章的模型。最后，我们调查ICL是否可以撤销安全培训，这可能代表着重大的安全风险。对于安全任务，我们查看了Vicuna-7B，Starling-7B和Llama2-7B。我们证明了该攻击可以直接对Starlin进行。",
    "tldr": "本研究通过研究禁止任务，即模型设计为拒绝回答的任务，探究了在明确调优模型拒绝禁止任务的情况下，上下文学习（ICL）是否可以用于重新学习禁止任务。研究发现，ICL可以成功地撤销安全培训，从而造成重大的安全风险。",
    "en_tdlr": "This study investigates the re-learning of forbidden tasks using in-context learning (ICL), despite models being explicitly fine-tuned to refuse them. The findings show that ICL can undo safety training, representing a major security risk."
}