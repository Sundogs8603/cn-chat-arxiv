{
    "title": "Few-Shot Class-Incremental Learning with Prior Knowledge",
    "abstract": "To tackle the issues of catastrophic forgetting and overfitting in few-shot class-incremental learning (FSCIL), previous work has primarily concentrated on preserving the memory of old knowledge during the incremental phase. The role of pre-trained model in shaping the effectiveness of incremental learning is frequently underestimated in these studies. Therefore, to enhance the generalization ability of the pre-trained model, we propose Learning with Prior Knowledge (LwPK) by introducing nearly free prior knowledge from a few unlabeled data of subsequent incremental classes. We cluster unlabeled incremental class samples to produce pseudo-labels, then jointly train these with labeled base class samples, effectively allocating embedding space for both old and new class data. Experimental results indicate that LwPK effectively enhances the model resilience against catastrophic forgetting, with theoretical analysis based on empirical risk minimization and class distance measurement corrob",
    "link": "https://rss.arxiv.org/abs/2402.01201",
    "context": "Title: Few-Shot Class-Incremental Learning with Prior Knowledge\nAbstract: To tackle the issues of catastrophic forgetting and overfitting in few-shot class-incremental learning (FSCIL), previous work has primarily concentrated on preserving the memory of old knowledge during the incremental phase. The role of pre-trained model in shaping the effectiveness of incremental learning is frequently underestimated in these studies. Therefore, to enhance the generalization ability of the pre-trained model, we propose Learning with Prior Knowledge (LwPK) by introducing nearly free prior knowledge from a few unlabeled data of subsequent incremental classes. We cluster unlabeled incremental class samples to produce pseudo-labels, then jointly train these with labeled base class samples, effectively allocating embedding space for both old and new class data. Experimental results indicate that LwPK effectively enhances the model resilience against catastrophic forgetting, with theoretical analysis based on empirical risk minimization and class distance measurement corrob",
    "path": "papers/24/02/2402.01201.json",
    "total_tokens": 894,
    "translated_title": "具有先验知识的少样本类增量学习",
    "translated_abstract": "为解决少样本类增量学习(FSCIL)中的灾难性遗忘和过拟合问题，之前的研究主要集中在在增量阶段保留旧知识的记忆上。这些研究经常低估了预训练模型在塑造增量学习有效性方面的作用。因此，为了增强预训练模型的泛化能力，我们提出了具有先验知识的学习(LwPK)，通过引入来自后续增量类别中少量无标签数据的几乎自由的先验知识。我们将无标签的增量类样本聚类，生成伪标签，并与带标签的基类样本进行联合训练，有效地为旧类和新类数据分配嵌入空间。实验结果表明，LwPK有效提高了模型对灾难性遗忘的韧性，基于经验风险最小化和类间距离度量的理论分析得到了验证。",
    "tldr": "该论文提出了一种具有先验知识的学习方法，通过引入从后续增量类别的无标签数据中产生的伪标签，与带标签的基类样本一起进行联合训练，有效地为旧类和新类数据分配嵌入空间，从而提高了模型对灾难性遗忘的韧性。",
    "en_tdlr": "This paper proposes a learning method with prior knowledge, which effectively enhances model resilience against catastrophic forgetting by introducing pseudo-labels generated from unlabeled data of subsequent incremental classes, and jointly training them with labeled base class samples to allocate embedding space for both old and new class data."
}