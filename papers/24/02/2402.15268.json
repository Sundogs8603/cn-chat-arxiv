{
    "title": "MemoryPrompt: A Light Wrapper to Improve Context Tracking in Pre-trained Language Models",
    "abstract": "arXiv:2402.15268v1 Announce Type: cross  Abstract: Transformer-based language models (LMs) track contextual information through large, hard-coded input windows. We introduce MemoryPrompt, a leaner approach in which the LM is complemented by a small auxiliary recurrent network that passes information to the LM by prefixing its regular input with a sequence of vectors, akin to soft prompts, without requiring LM finetuning. Tested on a task designed to probe a LM's ability to keep track of multiple fact updates, a MemoryPrompt-augmented LM outperforms much larger LMs that have access to the full input history. We also test MemoryPrompt on a long-distance dialogue dataset, where its performance is comparable to that of a model conditioned on the entire conversation history. In both experiments we also observe that, unlike full-finetuning approaches, MemoryPrompt does not suffer from catastrophic forgetting when adapted to new tasks, thus not disrupting the generalist capabilities of the un",
    "link": "https://arxiv.org/abs/2402.15268",
    "context": "Title: MemoryPrompt: A Light Wrapper to Improve Context Tracking in Pre-trained Language Models\nAbstract: arXiv:2402.15268v1 Announce Type: cross  Abstract: Transformer-based language models (LMs) track contextual information through large, hard-coded input windows. We introduce MemoryPrompt, a leaner approach in which the LM is complemented by a small auxiliary recurrent network that passes information to the LM by prefixing its regular input with a sequence of vectors, akin to soft prompts, without requiring LM finetuning. Tested on a task designed to probe a LM's ability to keep track of multiple fact updates, a MemoryPrompt-augmented LM outperforms much larger LMs that have access to the full input history. We also test MemoryPrompt on a long-distance dialogue dataset, where its performance is comparable to that of a model conditioned on the entire conversation history. In both experiments we also observe that, unlike full-finetuning approaches, MemoryPrompt does not suffer from catastrophic forgetting when adapted to new tasks, thus not disrupting the generalist capabilities of the un",
    "path": "papers/24/02/2402.15268.json",
    "total_tokens": 875,
    "translated_title": "MemoryPrompt: 一种改进预训练语言模型上下文跟踪的轻量封装方法",
    "translated_abstract": "基于Transformer的语言模型通过大型硬编码输入窗口跟踪上下文信息。我们引入MemoryPrompt，一种更精简的方法，其中语言模型由一个小的辅助循环网络补充，通过在其常规输入之前添加一系列向量（类似于软提示）将信息传递给语言模型，而无需需要对语言模型进行微调。在对一个旨在检测语言模型跟踪多个事实更新能力的任务上进行测试时，MemoryPrompt增强的语言模型优于那些可以访问完整输入历史记录的更大型语言模型。我们还在一个长距离对话数据集上测试了MemoryPrompt，在该数据集上，其性能与在整个对话历史记录上进行条件处理的模型相当。在这两个实验中，我们还观察到，与完全微调方法不同，MemoryPrompt在适应新任务时不会出现灾难性遗忘，因此不会破坏非专家能力。",
    "tldr": "MemoryPrompt方法通过引入辅助循环网络，将信息传递给语言模型，从而改进了预训练语言模型在上下文跟踪方面的性能，避免了灾难性遗忘现象。",
    "en_tdlr": "MemoryPrompt improves the performance of pre-trained language models in context tracking by introducing an auxiliary recurrent network to pass information to the language model, avoiding catastrophic forgetting."
}