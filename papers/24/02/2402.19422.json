{
    "title": "PEM: Prototype-based Efficient MaskFormer for Image Segmentation",
    "abstract": "arXiv:2402.19422v1 Announce Type: cross  Abstract: Recent transformer-based architectures have shown impressive results in the field of image segmentation. Thanks to their flexibility, they obtain outstanding performance in multiple segmentation tasks, such as semantic and panoptic, under a single unified framework. To achieve such impressive performance, these architectures employ intensive operations and require substantial computational resources, which are often not available, especially on edge devices. To fill this gap, we propose Prototype-based Efficient MaskFormer (PEM), an efficient transformer-based architecture that can operate in multiple segmentation tasks. PEM proposes a novel prototype-based cross-attention which leverages the redundancy of visual features to restrict the computation and improve the efficiency without harming the performance. In addition, PEM introduces an efficient multi-scale feature pyramid network, capable of extracting features that have high seman",
    "link": "https://arxiv.org/abs/2402.19422",
    "context": "Title: PEM: Prototype-based Efficient MaskFormer for Image Segmentation\nAbstract: arXiv:2402.19422v1 Announce Type: cross  Abstract: Recent transformer-based architectures have shown impressive results in the field of image segmentation. Thanks to their flexibility, they obtain outstanding performance in multiple segmentation tasks, such as semantic and panoptic, under a single unified framework. To achieve such impressive performance, these architectures employ intensive operations and require substantial computational resources, which are often not available, especially on edge devices. To fill this gap, we propose Prototype-based Efficient MaskFormer (PEM), an efficient transformer-based architecture that can operate in multiple segmentation tasks. PEM proposes a novel prototype-based cross-attention which leverages the redundancy of visual features to restrict the computation and improve the efficiency without harming the performance. In addition, PEM introduces an efficient multi-scale feature pyramid network, capable of extracting features that have high seman",
    "path": "papers/24/02/2402.19422.json",
    "total_tokens": 753,
    "translated_title": "PEM：基于原型的高效MaskFormer用于图像分割",
    "translated_abstract": "近期基于transformer的架构在图像分割领域展现出令人印象深刻的结果。由于其灵活性，它们在多个分割任务中获得出色的性能，如语义分割和全景分割，在一个统一的框架下。为了填补这一差距，我们提出了基于原型的高效MaskFormer（PEM），这是一个可以在多个分割任务中运行的高效transformer架构。PEM提出了一种新颖的基于原型的交叉注意力，利用视觉特征的冗余性来限制计算并提高效率而不损害性能。此外，PEM引入了一个高效的多尺度特征金字塔网络，能够提取具有高语义的特征。",
    "tldr": "PEM提出了基于原型的高效MaskFormer，通过引入原型交叉注意力和多尺度特征金字塔网络，实现了在多个分割任务中的高效运行。",
    "en_tdlr": "PEM introduces an efficient transformer architecture called MaskFormer, which utilizes prototype-based cross-attention and a multi-scale feature pyramid network to perform efficiently in multiple segmentation tasks."
}