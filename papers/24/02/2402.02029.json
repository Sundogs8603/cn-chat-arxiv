{
    "title": "ScribFormer: Transformer Makes CNN Work Better for Scribble-based Medical Image Segmentation",
    "abstract": "Most recent scribble-supervised segmentation methods commonly adopt a CNN framework with an encoder-decoder architecture. Despite its multiple benefits, this framework generally can only capture small-range feature dependency for the convolutional layer with the local receptive field, which makes it difficult to learn global shape information from the limited information provided by scribble annotations. To address this issue, this paper proposes a new CNN-Transformer hybrid solution for scribble-supervised medical image segmentation called ScribFormer. The proposed ScribFormer model has a triple-branch structure, i.e., the hybrid of a CNN branch, a Transformer branch, and an attention-guided class activation map (ACAM) branch. Specifically, the CNN branch collaborates with the Transformer branch to fuse the local features learned from CNN with the global representations obtained from Transformer, which can effectively overcome limitations of existing scribble-supervised segmentation m",
    "link": "https://arxiv.org/abs/2402.02029",
    "context": "Title: ScribFormer: Transformer Makes CNN Work Better for Scribble-based Medical Image Segmentation\nAbstract: Most recent scribble-supervised segmentation methods commonly adopt a CNN framework with an encoder-decoder architecture. Despite its multiple benefits, this framework generally can only capture small-range feature dependency for the convolutional layer with the local receptive field, which makes it difficult to learn global shape information from the limited information provided by scribble annotations. To address this issue, this paper proposes a new CNN-Transformer hybrid solution for scribble-supervised medical image segmentation called ScribFormer. The proposed ScribFormer model has a triple-branch structure, i.e., the hybrid of a CNN branch, a Transformer branch, and an attention-guided class activation map (ACAM) branch. Specifically, the CNN branch collaborates with the Transformer branch to fuse the local features learned from CNN with the global representations obtained from Transformer, which can effectively overcome limitations of existing scribble-supervised segmentation m",
    "path": "papers/24/02/2402.02029.json",
    "total_tokens": 846,
    "translated_title": "ScribFormer: Transformer使得基于涂鸦的医学图像分割的CNN工作更好",
    "translated_abstract": "最近的涂鸦监督分割方法通常采用具有编码器-解码器架构的CNN框架。尽管这个框架有多个好处，但是由于卷积层只能捕捉具有局部感受野的小范围特征依赖关系，它很难从涂鸦注释提供的有限信息中学习全局形状信息。为了解决这个问题，本文提出了一种新的CNN-Transformer混合解决方案，用于涂鸦监督的医学图像分割，称为ScribFormer。所提出的ScribFormer模型具有三个分支结构，即CNN分支，Transformer分支和注意力引导的类激活图（ACAM）分支的混合。具体而言，CNN分支与Transformer分支合作，将从CNN学习的局部特征与从Transformer获得的全局表示相融合，可以有效克服现有涂鸦监督分割方法的局限性。",
    "tldr": "ScribFormer是一种新的CNN-Transformer混合解决方案，用于涂鸦监督的医学图像分割。它通过融合CNN学习的局部特征和Transformer获得的全局表示，有效克服了现有方法在涂鸦注释中学习全局形状信息的局限性。"
}