{
    "title": "BlackMamba: Mixture of Experts for State-Space Models",
    "abstract": "State-space models (SSMs) have recently demonstrated competitive performance to transformers at large-scale language modeling benchmarks while achieving linear time and memory complexity as a function of sequence length. Mamba, a recently released SSM model, shows impressive performance in both language modeling and long sequence processing tasks. Simultaneously, mixture-of-expert (MoE) models have shown remarkable performance while significantly reducing the compute and latency costs of inference at the expense of a larger memory footprint. In this paper, we present BlackMamba, a novel architecture that combines the Mamba SSM with MoE to obtain the benefits of both. We demonstrate that BlackMamba performs competitively against both Mamba and transformer baselines, and outperforms in inference and training FLOPs. We fully train and open-source 340M/1.5B and 630M/2.8B BlackMamba models on 300B tokens of a custom dataset. We show that BlackMamba inherits and combines both of the benefits",
    "link": "https://arxiv.org/abs/2402.01771",
    "context": "Title: BlackMamba: Mixture of Experts for State-Space Models\nAbstract: State-space models (SSMs) have recently demonstrated competitive performance to transformers at large-scale language modeling benchmarks while achieving linear time and memory complexity as a function of sequence length. Mamba, a recently released SSM model, shows impressive performance in both language modeling and long sequence processing tasks. Simultaneously, mixture-of-expert (MoE) models have shown remarkable performance while significantly reducing the compute and latency costs of inference at the expense of a larger memory footprint. In this paper, we present BlackMamba, a novel architecture that combines the Mamba SSM with MoE to obtain the benefits of both. We demonstrate that BlackMamba performs competitively against both Mamba and transformer baselines, and outperforms in inference and training FLOPs. We fully train and open-source 340M/1.5B and 630M/2.8B BlackMamba models on 300B tokens of a custom dataset. We show that BlackMamba inherits and combines both of the benefits",
    "path": "papers/24/02/2402.01771.json",
    "total_tokens": 879,
    "translated_title": "BlackMamba: 混合专家模型的状态空间模型",
    "translated_abstract": "最近的研究表明，状态空间模型（SSMs）在大规模语言建模基准测试中表现出与transformer竞争力的性能，同时，其时间和内存复杂度与序列长度成线性关系。最近发布的SSM模型Mamba在语言建模和处理长序列任务方面表现出色。与此同时，专家混合模型（MoE）在显著降低推断计算和延迟成本的同时，也增加了更大的内存占用。本文提出了一种名为BlackMamba的新型架构，将Mamba SSM与MoE相结合，以获得两者的好处。我们证明BlackMamba在Mamba和transformer基准测试中表现出竞争力，并在推断和训练FLOPs方面表现出色。我们在自定义数据集的300B标记上全面训练并开源了340M/1.5B和630M/2.8B的BlackMamba模型。我们展示了BlackMamba继承并结合了这两种模型的优势。",
    "tldr": "BlackMamba是一种结合了Mamba SSM和MoE的新型架构，它具有竞争力的性能和较低的推断和训练成本，在大规模语言建模领域具有潜在应用价值。"
}