{
    "title": "Adversarial Text Purification: A Large Language Model Approach for Defense",
    "abstract": "Adversarial purification is a defense mechanism for safeguarding classifiers against adversarial attacks without knowing the type of attacks or training of the classifier. These techniques characterize and eliminate adversarial perturbations from the attacked inputs, aiming to restore purified samples that retain similarity to the initially attacked ones and are correctly classified by the classifier. Due to the inherent challenges associated with characterizing noise perturbations for discrete inputs, adversarial text purification has been relatively unexplored. In this paper, we investigate the effectiveness of adversarial purification methods in defending text classifiers. We propose a novel adversarial text purification that harnesses the generative capabilities of Large Language Models (LLMs) to purify adversarial text without the need to explicitly characterize the discrete noise perturbations. We utilize prompt engineering to exploit LLMs for recovering the purified examples for",
    "link": "https://arxiv.org/abs/2402.06655",
    "context": "Title: Adversarial Text Purification: A Large Language Model Approach for Defense\nAbstract: Adversarial purification is a defense mechanism for safeguarding classifiers against adversarial attacks without knowing the type of attacks or training of the classifier. These techniques characterize and eliminate adversarial perturbations from the attacked inputs, aiming to restore purified samples that retain similarity to the initially attacked ones and are correctly classified by the classifier. Due to the inherent challenges associated with characterizing noise perturbations for discrete inputs, adversarial text purification has been relatively unexplored. In this paper, we investigate the effectiveness of adversarial purification methods in defending text classifiers. We propose a novel adversarial text purification that harnesses the generative capabilities of Large Language Models (LLMs) to purify adversarial text without the need to explicitly characterize the discrete noise perturbations. We utilize prompt engineering to exploit LLMs for recovering the purified examples for",
    "path": "papers/24/02/2402.06655.json",
    "total_tokens": 814,
    "translated_title": "对抗性文本净化：一种基于大型语言模型的防御方法",
    "translated_abstract": "对抗性净化是一种防御机制，用于保护分类器免受对抗性攻击，而无需了解攻击类型或分类器的训练。这些技术对被攻击输入进行特征化和消除对抗性扰动，旨在恢复出与最初被攻击的输入相似且被分类器正确分类的净化样本。由于离散输入的噪声扰动特征化所带来的固有挑战，对抗性文本净化一直相对未被探索。在本文中，我们研究了对抗性净化方法在保护文本分类器中的有效性。我们提出了一种新颖的对抗性文本净化方法，利用大型语言模型（LLMs）的生成能力来净化对抗性文本，而无需明确特征化离散噪声扰动。我们利用提示工程来利用LLMs恢复净化的示例。",
    "tldr": "本文研究了防御文本分类器中对抗性净化方法的有效性，并提出了一种基于大型语言模型加以净化的方法。",
    "en_tdlr": "This paper investigates the effectiveness of adversarial purification methods in defending text classifiers and proposes a method that utilizes Large Language Models (LLMs) for text purification."
}