{
    "title": "Loss Shaping Constraints for Long-Term Time Series Forecasting",
    "abstract": "arXiv:2402.09373v1 Announce Type: new Abstract: Several applications in time series forecasting require predicting multiple steps ahead. Despite the vast amount of literature in the topic, both classical and recent deep learning based approaches have mostly focused on minimising performance averaged over the predicted window. We observe that this can lead to disparate distributions of errors across forecasting steps, especially for recent transformer architectures trained on popular forecasting benchmarks. That is, optimising performance on average can lead to undesirably large errors at specific time-steps. In this work, we present a Constrained Learning approach for long-term time series forecasting that aims to find the best model in terms of average performance that respects a user-defined upper bound on the loss at each time-step. We call our approach loss shaping constraints because it imposes constraints on the loss at each time step, and leverage recent duality results to show ",
    "link": "https://arxiv.org/abs/2402.09373",
    "context": "Title: Loss Shaping Constraints for Long-Term Time Series Forecasting\nAbstract: arXiv:2402.09373v1 Announce Type: new Abstract: Several applications in time series forecasting require predicting multiple steps ahead. Despite the vast amount of literature in the topic, both classical and recent deep learning based approaches have mostly focused on minimising performance averaged over the predicted window. We observe that this can lead to disparate distributions of errors across forecasting steps, especially for recent transformer architectures trained on popular forecasting benchmarks. That is, optimising performance on average can lead to undesirably large errors at specific time-steps. In this work, we present a Constrained Learning approach for long-term time series forecasting that aims to find the best model in terms of average performance that respects a user-defined upper bound on the loss at each time-step. We call our approach loss shaping constraints because it imposes constraints on the loss at each time step, and leverage recent duality results to show ",
    "path": "papers/24/02/2402.09373.json",
    "total_tokens": 817,
    "translated_title": "长期时间序列预测的损失塑造约束",
    "translated_abstract": "许多时间序列预测应用程序需要预测多个步骤。尽管在这个主题上有大量的文献，但经典和最近的基于深度学习的方法主要集中在最小化预测窗口上的性能平均值。我们观察到，这可能导致在预测步骤之间存在不同的错误分布，尤其是对于在常见预测基准上训练的最近的变换器架构。也就是说，平均性能优化可能导致特定时间步骤上的错误过大。在这项工作中，我们提出了一种长期时间序列预测的受限学习方法，旨在找到在平均性能上最好的模型，并且在每个时间步骤上保持用户定义的损失上限。我们称这种方法为损失塑造约束，因为它对每个时间步骤的损失施加约束，并利用最近的对偶性结果展示了...",
    "tldr": "该论文提出了一种用于长期时间序列预测的受限学习方法，通过在每个时间步骤上设置损失上限来寻找最佳模型，以解决平均性能优化导致特定时间步骤上误差过大的问题。"
}