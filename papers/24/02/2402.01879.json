{
    "title": "$\\sigma$-zero: Gradient-based Optimization of $\\ell_0$-norm Adversarial Examples",
    "abstract": "Evaluating the adversarial robustness of deep networks to gradient-based attacks is challenging. While most attacks consider $\\ell_2$- and $\\ell_\\infty$-norm constraints to craft input perturbations, only a few investigate sparse $\\ell_1$- and $\\ell_0$-norm attacks. In particular, $\\ell_0$-norm attacks remain the least studied due to the inherent complexity of optimizing over a non-convex and non-differentiable constraint. However, evaluating adversarial robustness under these attacks could reveal weaknesses otherwise left untested with more conventional $\\ell_2$- and $\\ell_\\infty$-norm attacks. In this work, we propose a novel $\\ell_0$-norm attack, called $\\sigma$-zero, which leverages an ad hoc differentiable approximation of the $\\ell_0$ norm to facilitate gradient-based optimization, and an adaptive projection operator to dynamically adjust the trade-off between loss minimization and perturbation sparsity. Extensive evaluations using MNIST, CIFAR10, and ImageNet datasets, involving",
    "link": "https://arxiv.org/abs/2402.01879",
    "context": "Title: $\\sigma$-zero: Gradient-based Optimization of $\\ell_0$-norm Adversarial Examples\nAbstract: Evaluating the adversarial robustness of deep networks to gradient-based attacks is challenging. While most attacks consider $\\ell_2$- and $\\ell_\\infty$-norm constraints to craft input perturbations, only a few investigate sparse $\\ell_1$- and $\\ell_0$-norm attacks. In particular, $\\ell_0$-norm attacks remain the least studied due to the inherent complexity of optimizing over a non-convex and non-differentiable constraint. However, evaluating adversarial robustness under these attacks could reveal weaknesses otherwise left untested with more conventional $\\ell_2$- and $\\ell_\\infty$-norm attacks. In this work, we propose a novel $\\ell_0$-norm attack, called $\\sigma$-zero, which leverages an ad hoc differentiable approximation of the $\\ell_0$ norm to facilitate gradient-based optimization, and an adaptive projection operator to dynamically adjust the trade-off between loss minimization and perturbation sparsity. Extensive evaluations using MNIST, CIFAR10, and ImageNet datasets, involving",
    "path": "papers/24/02/2402.01879.json",
    "total_tokens": 1061,
    "translated_title": "$\\sigma$-zero: 基于梯度的$\\ell_0$-范数对抗样本优化",
    "translated_abstract": "评估深度网络对基于梯度攻击的对抗鲁棒性是具有挑战性的。虽然大多数攻击考虑$\\ell_2$和$\\ell_\\infty$范数约束来制造输入扰动，但只有少数研究了稀疏的$\\ell_1$和$\\ell_0$范数攻击。特别是，由于在非凸且非可微约束上进行优化的固有复杂性，$\\ell_0$范数攻击是研究最少的。然而，使用这些攻击评估对抗鲁棒性可以揭示在更传统的$\\ell_2$和$\\ell_\\infty$范数攻击中未能测试出的弱点。在这项工作中，我们提出了一种新颖的$\\ell_0$范数攻击，称为$\\sigma$-zero，它利用了$\\ell_0$范数的一个特殊可微近似来促进基于梯度的优化，并利用自适应投影运算符动态调整损失最小化和扰动稀疏性之间的权衡。通过在MNIST、CIFAR10和ImageNet数据集上进行广泛评估，包括...",
    "tldr": "该论文提出了一种新的基于梯度的$\\ell_0$范数攻击方法$\\sigma$-zero，其利用了$\\ell_0$范数的可微近似和自适应投影运算符，能够在非凸和非可微的约束下优化，从而评估深度网络对稀疏$\\ell_0$范数攻击的鲁棒性。",
    "en_tdlr": "This paper proposes a novel gradient-based $\\ell_0$-norm attack, called $\\sigma$-zero, which leverages a differentiable approximation of the $\\ell_0$ norm and an adaptive projection operator to optimize under the non-convex and non-differentiable constraint. It evaluates the adversarial robustness of deep networks under sparse $\\ell_0$-norm attacks."
}