{
    "title": "Goal-Conditioned Offline Reinforcement Learning via Metric Learning",
    "abstract": "arXiv:2402.10820v1 Announce Type: new  Abstract: In this work, we address the problem of learning optimal behavior from sub-optimal datasets in the context of goal-conditioned offline reinforcement learning. To do so, we propose a novel way of approximating the optimal value function for goal-conditioned offline RL problems under sparse rewards, symmetric and deterministic actions. We study a property for representations to recover optimality and propose a new optimization objective that leads to such property. We use the learned value function to guide the learning of a policy in an actor-critic fashion, a method we name MetricRL. Experimentally, we show how our method consistently outperforms other offline RL baselines in learning from sub-optimal offline datasets. Moreover, we show the effectiveness of our method in dealing with high-dimensional observations and in multi-goal tasks.",
    "link": "https://arxiv.org/abs/2402.10820",
    "context": "Title: Goal-Conditioned Offline Reinforcement Learning via Metric Learning\nAbstract: arXiv:2402.10820v1 Announce Type: new  Abstract: In this work, we address the problem of learning optimal behavior from sub-optimal datasets in the context of goal-conditioned offline reinforcement learning. To do so, we propose a novel way of approximating the optimal value function for goal-conditioned offline RL problems under sparse rewards, symmetric and deterministic actions. We study a property for representations to recover optimality and propose a new optimization objective that leads to such property. We use the learned value function to guide the learning of a policy in an actor-critic fashion, a method we name MetricRL. Experimentally, we show how our method consistently outperforms other offline RL baselines in learning from sub-optimal offline datasets. Moreover, we show the effectiveness of our method in dealing with high-dimensional observations and in multi-goal tasks.",
    "path": "papers/24/02/2402.10820.json",
    "total_tokens": 806,
    "translated_title": "通过度量学习的目标条件离线强化学习",
    "translated_abstract": "在这项工作中，我们解决了在目标条件下离线强化学习中从次优数据集中学习最优行为的问题。为此，我们提出了一种新颖的方法来近似处理稀疏奖励、对称且确定性动作下的目标条件离线RL问题的最优值函数。我们研究了一种表示恢复优化的属性，并提出了导致该属性的新优化目标。我们使用学习到的值函数以演员-评论者的方式指导策略的学习，这种方法被我们称为MetricRL。在实验中，我们展示了我们的方法如何始终优于其他离线RL基线在从次优离线数据集中学习方面的表现。此外，我们展示了我们的方法在处理高维观测和多目标任务中的有效性。",
    "tldr": "通过度量学习的目标条件离线强化学习提出了一种新的方法来处理稀疏奖励、对称和确定性动作下的最优值函数近似，并展示了在学习次优离线数据集方面的显着优越性。",
    "en_tdlr": "Goal-conditioned offline reinforcement learning via metric learning introduces a novel approach to approximate the optimal value function under sparse rewards, symmetric, and deterministic actions, demonstrating significant superiority in learning from sub-optimal offline datasets."
}