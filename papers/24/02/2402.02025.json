{
    "title": "A Survey of Constraint Formulations in Safe Reinforcement Learning",
    "abstract": "Ensuring safety is critical when applying reinforcement learning (RL) to real-world problems. Consequently, safe RL emerges as a fundamental and powerful paradigm for safely optimizing an agent's policy from experimental data. A popular safe RL approach is based on a constrained criterion, which solves the problem of maximizing expected cumulative reward under safety constraints. Though there has been recently a surge of such attempts to achieve safety in RL, a systematic understanding of the field is difficult due to 1) the diversity of constraint representations and 2) little discussion of their interrelations. To address this knowledge gap, we provide a comprehensive review of representative constraint formulations, along with a curated selection of algorithms specifically designed for each formulation. Furthermore, we elucidate the theoretical underpinnings that reveal the mathematical mutual relations among common problem formulations. We conclude with a discussion of the current ",
    "link": "https://arxiv.org/abs/2402.02025",
    "context": "Title: A Survey of Constraint Formulations in Safe Reinforcement Learning\nAbstract: Ensuring safety is critical when applying reinforcement learning (RL) to real-world problems. Consequently, safe RL emerges as a fundamental and powerful paradigm for safely optimizing an agent's policy from experimental data. A popular safe RL approach is based on a constrained criterion, which solves the problem of maximizing expected cumulative reward under safety constraints. Though there has been recently a surge of such attempts to achieve safety in RL, a systematic understanding of the field is difficult due to 1) the diversity of constraint representations and 2) little discussion of their interrelations. To address this knowledge gap, we provide a comprehensive review of representative constraint formulations, along with a curated selection of algorithms specifically designed for each formulation. Furthermore, we elucidate the theoretical underpinnings that reveal the mathematical mutual relations among common problem formulations. We conclude with a discussion of the current ",
    "path": "papers/24/02/2402.02025.json",
    "total_tokens": 821,
    "translated_title": "安全强化学习中的约束形式综述",
    "translated_abstract": "在将强化学习（RL）应用于现实世界问题时，确保安全性至关重要。因此，安全RL成为一种从实验数据中安全优化代理策略的基本而强大的范例。基于约束准则的安全RL方法被广泛采用，它解决了在安全约束下最大化预期累积奖励的问题。虽然近年来在RL中实现安全性的尝试激增，但由于约束表示的多样性和对它们之间关系的讨论很少，对该领域的系统性了解仍然困难。为了解决这一知识差距，我们提供了对代表性约束形式的全面回顾，以及针对每种形式特别设计的算法的精选。此外，我们揭示了揭示常见问题形式之间的数学相互关系的理论基础。最后，我们讨论了当前研究的一些挑战和未来方向。",
    "tldr": "本文综述了安全强化学习中的约束形式，包括对每种形式特别设计的算法。同时，揭示了常见问题形式之间的数学相互关系。",
    "en_tdlr": "This paper surveys constraint formulations in safe reinforcement learning, including algorithms specifically designed for each formulation. It also reveals the mathematical mutual relations among common problem formulations."
}