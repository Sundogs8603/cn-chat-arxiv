{
    "title": "HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination Tendency of LLMs",
    "abstract": "arXiv:2402.16211v1 Announce Type: new  Abstract: Hallucinations pose a significant challenge to the reliability and alignment of Large Language Models (LLMs), limiting their widespread acceptance beyond chatbot applications. Despite ongoing efforts, hallucinations remain a prevalent challenge in LLMs. The detection of hallucinations itself is also a formidable task, frequently requiring manual labeling or constrained evaluations. This paper introduces an automated scalable framework that combines benchmarking LLMs' hallucination tendencies with efficient hallucination detection. We leverage LLMs to generate challenging tasks related to hypothetical phenomena, subsequently employing them as agents for efficient hallucination detection. The framework is domain-agnostic, allowing the use of any language model for benchmark creation or evaluation in any domain. We introduce the publicly available HypoTermQA Benchmarking Dataset, on which state-of-the-art models' performance ranged between ",
    "link": "https://arxiv.org/abs/2402.16211",
    "context": "Title: HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination Tendency of LLMs\nAbstract: arXiv:2402.16211v1 Announce Type: new  Abstract: Hallucinations pose a significant challenge to the reliability and alignment of Large Language Models (LLMs), limiting their widespread acceptance beyond chatbot applications. Despite ongoing efforts, hallucinations remain a prevalent challenge in LLMs. The detection of hallucinations itself is also a formidable task, frequently requiring manual labeling or constrained evaluations. This paper introduces an automated scalable framework that combines benchmarking LLMs' hallucination tendencies with efficient hallucination detection. We leverage LLMs to generate challenging tasks related to hypothetical phenomena, subsequently employing them as agents for efficient hallucination detection. The framework is domain-agnostic, allowing the use of any language model for benchmark creation or evaluation in any domain. We introduce the publicly available HypoTermQA Benchmarking Dataset, on which state-of-the-art models' performance ranged between ",
    "path": "papers/24/02/2402.16211.json",
    "total_tokens": 869,
    "translated_title": "HypoTermQA：用于评估LLMs幻觉倾向的假设术语数据集",
    "translated_abstract": "幻觉对于大型语言模型（LLMs）的可靠性和对齐性构成了重大挑战，限制了它们在聊天机器人应用以外的广泛接受程度。尽管不断努力，但幻觉仍然是LLMs中一个普遍存在的挑战。幻觉的检测本身也是一项艰巨的任务，经常需要人工标注或受限制的评估。本文介绍了一个自动可扩展的框架，将LLMs的幻觉倾向与高效的幻觉检测相结合进行基准测试。我们利用LLMs生成与假设现象相关的具有挑战性的任务，随后将它们用作有效幻觉检测的代理。该框架与领域无关，允许在任何领域中使用任何语言模型进行基准测试数据集的创建或评估。我们介绍了公开可用的HypoTermQA基准数据集，其中最先进模型的性能范围如下：",
    "tldr": "该论文引入了一个自动可扩展的框架，结合LLMs的幻觉倾向与高效的幻觉检测，创建了用于基准测试的HypoTermQA数据集。"
}