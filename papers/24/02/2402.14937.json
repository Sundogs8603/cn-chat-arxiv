{
    "title": "SoK: Analyzing Adversarial Examples: A Framework to Study Adversary Knowledge",
    "abstract": "arXiv:2402.14937v1 Announce Type: new  Abstract: Adversarial examples are malicious inputs to machine learning models that trigger a misclassification. This type of attack has been studied for close to a decade, and we find that there is a lack of study and formalization of adversary knowledge when mounting attacks. This has yielded a complex space of attack research with hard-to-compare threat models and attacks. We focus on the image classification domain and provide a theoretical framework to study adversary knowledge inspired by work in order theory. We present an adversarial example game, inspired by cryptographic games, to standardize attacks. We survey recent attacks in the image classification domain and classify their adversary's knowledge in our framework. From this systematization, we compile results that both confirm existing beliefs about adversary knowledge, such as the potency of information about the attacked model as well as allow us to derive new conclusions on the di",
    "link": "https://arxiv.org/abs/2402.14937",
    "context": "Title: SoK: Analyzing Adversarial Examples: A Framework to Study Adversary Knowledge\nAbstract: arXiv:2402.14937v1 Announce Type: new  Abstract: Adversarial examples are malicious inputs to machine learning models that trigger a misclassification. This type of attack has been studied for close to a decade, and we find that there is a lack of study and formalization of adversary knowledge when mounting attacks. This has yielded a complex space of attack research with hard-to-compare threat models and attacks. We focus on the image classification domain and provide a theoretical framework to study adversary knowledge inspired by work in order theory. We present an adversarial example game, inspired by cryptographic games, to standardize attacks. We survey recent attacks in the image classification domain and classify their adversary's knowledge in our framework. From this systematization, we compile results that both confirm existing beliefs about adversary knowledge, such as the potency of information about the attacked model as well as allow us to derive new conclusions on the di",
    "path": "papers/24/02/2402.14937.json",
    "total_tokens": 890,
    "translated_title": "SoK: 分析对抗性样本：研究对手知识的框架",
    "translated_abstract": "对抗性样本是恶意输入到机器学习模型中的内容，会引发误分类。这种类型的攻击已经研究了近十年，我们发现在发起攻击时对对手知识的研究和形式化存在缺乏。这导致了一个复杂的攻击研究领域，具有难以比较的威胁模型和攻击方式。我们专注于图像分类领域，并提供了一个受启发于序理论工作的框架来研究对手知识。我们提出了一个对抗性样本游戏，受到密码游戏的启发，用以标准化攻击。我们调查了图像分类领域的最新攻击，并在我们的框架中对攻击者的知识进行分类。通过这种系统化方法，我们汇编了结果，既确认了关于对手知识的现有观点，例如关于被攻击模型信息的有效性，也让我们得出了关于对抗性样本的新结论。",
    "tldr": "提出一个理论框架来研究对手知识，通过对抗性样本游戏标准化攻击，对图像分类领域的最新攻击进行分类，从而系统地总结了对手知识，得出新的结论。",
    "en_tdlr": "Propose a theoretical framework to study adversary knowledge, standardize attacks through an adversarial example game, classify recent attacks in the image classification domain, and systematically summarize adversary knowledge to derive new conclusions."
}