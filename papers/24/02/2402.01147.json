{
    "title": "Efficient Reinforcement Learning for Routing Jobs in Heterogeneous Queueing Systems",
    "abstract": "We consider the problem of efficiently routing jobs that arrive into a central queue to a system of heterogeneous servers. Unlike homogeneous systems, a threshold policy, that routes jobs to the slow server(s) when the queue length exceeds a certain threshold, is known to be optimal for the one-fast-one-slow two-server system. But an optimal policy for the multi-server system is unknown and non-trivial to find. While Reinforcement Learning (RL) has been recognized to have great potential for learning policies in such cases, our problem has an exponentially large state space size, rendering standard RL inefficient. In this work, we propose ACHQ, an efficient policy gradient based algorithm with a low dimensional soft threshold policy parameterization that leverages the underlying queueing structure. We provide stationary-point convergence guarantees for the general case and despite the low-dimensional parameterization prove that ACHQ converges to an approximate global optimum for the sp",
    "link": "https://rss.arxiv.org/abs/2402.01147",
    "context": "Title: Efficient Reinforcement Learning for Routing Jobs in Heterogeneous Queueing Systems\nAbstract: We consider the problem of efficiently routing jobs that arrive into a central queue to a system of heterogeneous servers. Unlike homogeneous systems, a threshold policy, that routes jobs to the slow server(s) when the queue length exceeds a certain threshold, is known to be optimal for the one-fast-one-slow two-server system. But an optimal policy for the multi-server system is unknown and non-trivial to find. While Reinforcement Learning (RL) has been recognized to have great potential for learning policies in such cases, our problem has an exponentially large state space size, rendering standard RL inefficient. In this work, we propose ACHQ, an efficient policy gradient based algorithm with a low dimensional soft threshold policy parameterization that leverages the underlying queueing structure. We provide stationary-point convergence guarantees for the general case and despite the low-dimensional parameterization prove that ACHQ converges to an approximate global optimum for the sp",
    "path": "papers/24/02/2402.01147.json",
    "total_tokens": 934,
    "translated_title": "异构排队系统中用于路由作业的高效强化学习",
    "translated_abstract": "本研究考虑将到达中央队列的作业高效路由到异构服务器系统中的问题。与同质系统不同，对于一快一慢的双服务器系统，已知阈值策略，即在队列长度超过某一阈值时将作业路由到慢服务器，是最优策略。但多服务器系统的最优策略未知且难以找到。虽然强化学习（RL）已被认为对于学习此类策略具有巨大潜力，但我们的问题具有指数级的状态空间大小，使标准RL效率低下。在这项工作中，我们提出了ACHQ，一种基于策略梯度的高效算法，具有低维度的软阈值策略参数化，利用底层排队结构。我们为一般情况提供了稳态收敛性保证，并且尽管参数化维度较低，但证明了ACHQ收敛到近似全局最优序列。",
    "tldr": "本研究针对异构排队系统中作业路由问题，提出了ACHQ算法，通过低维度的软阈值策略参数化和基于策略梯度的方法，利用系统的排队结构，实现了高效的强化学习求解策略。实验结果表明，ACHQ算法能够收敛到近似全局最优解。"
}