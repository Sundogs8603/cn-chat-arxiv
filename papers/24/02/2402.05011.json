{
    "title": "Navigating Complexity: Toward Lossless Graph Condensation via Expanding Window Matching",
    "abstract": "Graph condensation aims to reduce the size of a large-scale graph dataset by synthesizing a compact counterpart without sacrificing the performance of Graph Neural Networks (GNNs) trained on it, which has shed light on reducing the computational cost for training GNNs. Nevertheless, existing methods often fall short of accurately replicating the original graph for certain datasets, thereby failing to achieve the objective of lossless condensation. To understand this phenomenon, we investigate the potential reasons and reveal that the previous state-of-the-art trajectory matching method provides biased and restricted supervision signals from the original graph when optimizing the condensed one. This significantly limits both the scale and efficacy of the condensed graph. In this paper, we make the first attempt toward \\textit{lossless graph condensation} by bridging the previously neglected supervision signals. Specifically, we employ a curriculum learning strategy to train expert traje",
    "link": "https://arxiv.org/abs/2402.05011",
    "context": "Title: Navigating Complexity: Toward Lossless Graph Condensation via Expanding Window Matching\nAbstract: Graph condensation aims to reduce the size of a large-scale graph dataset by synthesizing a compact counterpart without sacrificing the performance of Graph Neural Networks (GNNs) trained on it, which has shed light on reducing the computational cost for training GNNs. Nevertheless, existing methods often fall short of accurately replicating the original graph for certain datasets, thereby failing to achieve the objective of lossless condensation. To understand this phenomenon, we investigate the potential reasons and reveal that the previous state-of-the-art trajectory matching method provides biased and restricted supervision signals from the original graph when optimizing the condensed one. This significantly limits both the scale and efficacy of the condensed graph. In this paper, we make the first attempt toward \\textit{lossless graph condensation} by bridging the previously neglected supervision signals. Specifically, we employ a curriculum learning strategy to train expert traje",
    "path": "papers/24/02/2402.05011.json",
    "total_tokens": 813,
    "translated_title": "导航复杂性：通过扩展窗口匹配实现无损图谱精简",
    "translated_abstract": "图谱精简旨在通过合成紧凑的图谱来减少大规模图谱数据集的大小，同时不损失在其上训练的图神经网络（GNNs）的性能，这为减少训练GNNs的计算成本提供了启示。然而，现有方法往往无法准确复制某些数据集的原始图谱，从而未能实现无损精简的目标。为了理解这一现象，我们调查了潜在的原因，并揭示了先前最先进的轨迹匹配方法在优化精简图谱时提供了来自原始图谱的偏倚和受限的监督信号。这严重限制了精简图谱的规模和功效。在本文中，我们首次尝试通过连接先前被忽视的监督信号来实现无损图谱精简。",
    "tldr": "本文通过连接先前被忽视的监督信号的方式，首次尝试实现无损图谱精简，以解决现有方法无法准确复制原始图谱的问题。",
    "en_tdlr": "This paper makes the first attempt towards lossless graph condensation by connecting previously neglected supervision signals, addressing the problem of existing methods's inability to accurately replicate the original graph."
}