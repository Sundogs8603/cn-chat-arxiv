{
    "title": "Analysis of Off-Policy Multi-Step TD-Learning with Linear Function Approximation",
    "abstract": "arXiv:2402.15781v1 Announce Type: cross  Abstract: This paper analyzes multi-step TD-learning algorithms within the `deadly triad' scenario, characterized by linear function approximation, off-policy learning, and bootstrapping. In particular, we prove that n-step TD-learning algorithms converge to a solution as the sampling horizon n increases sufficiently. The paper is divided into two parts. In the first part, we comprehensively examine the fundamental properties of their model-based deterministic counterparts, including projected value iteration, gradient descent algorithms, and the control theoretic approach, which can be viewed as prototype deterministic algorithms whose analysis plays a pivotal role in understanding and developing their model-free reinforcement learning counterparts. In particular, we prove that these algorithms converge to meaningful solutions when n is sufficiently large. Based on these findings, two n-step TD-learning algorithms are proposed and analyzed, whi",
    "link": "https://arxiv.org/abs/2402.15781",
    "context": "Title: Analysis of Off-Policy Multi-Step TD-Learning with Linear Function Approximation\nAbstract: arXiv:2402.15781v1 Announce Type: cross  Abstract: This paper analyzes multi-step TD-learning algorithms within the `deadly triad' scenario, characterized by linear function approximation, off-policy learning, and bootstrapping. In particular, we prove that n-step TD-learning algorithms converge to a solution as the sampling horizon n increases sufficiently. The paper is divided into two parts. In the first part, we comprehensively examine the fundamental properties of their model-based deterministic counterparts, including projected value iteration, gradient descent algorithms, and the control theoretic approach, which can be viewed as prototype deterministic algorithms whose analysis plays a pivotal role in understanding and developing their model-free reinforcement learning counterparts. In particular, we prove that these algorithms converge to meaningful solutions when n is sufficiently large. Based on these findings, two n-step TD-learning algorithms are proposed and analyzed, whi",
    "path": "papers/24/02/2402.15781.json",
    "total_tokens": 844,
    "translated_title": "对具有线性函数逼近的离策略多步TD学习算法的分析",
    "translated_abstract": "本文分析了在具有线性函数逼近、离策略学习和自举的“致命三连”场景中的多步TD学习算法。特别地，我们证明了当采样时间跨度 n 足够大时，n步TD学习算法会收敛到一个解。该论文分为两部分。第一部分全面研究了模型基础确定性算法的基本性质，包括投影值迭代、梯度下降算法和控制理论方法，它们可以被视为原型确定性算法，对于理解和发展无模型增强学习算法起着关键作用。特别地，我们证明了当 n 足够大时，这些算法会收敛到有意义的解。根据这些发现，提出并分析了两种n步TD学习算法。",
    "tldr": "该论文分析了在具有线性函数逼近、离策略学习和自举的“致命三连”场景中的多步TD学习算法，并证明了当采样时间跨度 n 足够大时这些算法会收敛到有意义的解。",
    "en_tdlr": "This paper analyzes multi-step TD-learning algorithms within the 'deadly triad' scenario, characterized by linear function approximation, off-policy learning, and bootstrapping, and proves that these algorithms converge to meaningful solutions as the sampling horizon n increases sufficiently."
}