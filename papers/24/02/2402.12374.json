{
    "title": "Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding",
    "abstract": "arXiv:2402.12374v1 Announce Type: new  Abstract: As the usage of large language models (LLMs) grows, performing efficient inference with these models becomes increasingly important. While speculative decoding has recently emerged as a promising direction for speeding up inference, existing methods are limited in their ability to scale to larger speculation budgets, and adapt to different hyperparameters and hardware. This paper introduces Sequoia, a scalable, robust, and hardware-aware algorithm for speculative decoding. To attain better scalability, Sequoia introduces a dynamic programming algorithm to find the optimal tree structure for the speculated tokens. To achieve robust speculative performance, Sequoia uses a novel sampling and verification method that outperforms prior work across different decoding temperatures. Finally, Sequoia introduces a hardware-aware tree optimizer that maximizes speculative performance by automatically selecting the token tree size and depth for a giv",
    "link": "https://arxiv.org/abs/2402.12374",
    "context": "Title: Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding\nAbstract: arXiv:2402.12374v1 Announce Type: new  Abstract: As the usage of large language models (LLMs) grows, performing efficient inference with these models becomes increasingly important. While speculative decoding has recently emerged as a promising direction for speeding up inference, existing methods are limited in their ability to scale to larger speculation budgets, and adapt to different hyperparameters and hardware. This paper introduces Sequoia, a scalable, robust, and hardware-aware algorithm for speculative decoding. To attain better scalability, Sequoia introduces a dynamic programming algorithm to find the optimal tree structure for the speculated tokens. To achieve robust speculative performance, Sequoia uses a novel sampling and verification method that outperforms prior work across different decoding temperatures. Finally, Sequoia introduces a hardware-aware tree optimizer that maximizes speculative performance by automatically selecting the token tree size and depth for a giv",
    "path": "papers/24/02/2402.12374.json",
    "total_tokens": 891,
    "translated_title": "Sequoia: 可扩展、稳健且硬件感知的推测解码",
    "translated_abstract": "随着大型语言模型（LLMs）的使用增多，使用这些模型进行高效推理变得日益重要。虽然最近推测解码已经成为加速推理的一个有前途的方向，但现有方法在扩展到较大的推测预算、适应不同超参数和硬件方面存在局限性。本文介绍了Sequoia，一个可扩展、稳健且硬件感知的用于推测解码的算法。为了实现更好的可扩展性，Sequoia引入了一个动态规划算法来找到用于被推测标记的最佳树结构。为了实现稳健的推测性能，Sequoia使用了一种新颖的采样和验证方法，该方法在不同解码温度下优于先前的方法。最后，Sequoia引入了一种硬件感知的树优化器，通过自动选择给定情况下的标记树大小和深度来最大化推测性能。",
    "tldr": "Sequoia是一种可扩展、稳健且硬件感知的推测解码算法，通过引入动态规划算法优化标记树结构、采用新颖的采样和验证方法实现稳健性能以及硬件感知的树优化器最大化推测性能。",
    "en_tdlr": "Sequoia is a scalable, robust, and hardware-aware speculative decoding algorithm that optimizes token tree structure using dynamic programming, achieves robust performance with novel sampling and verification methods, and maximizes speculative performance through a hardware-aware tree optimizer."
}