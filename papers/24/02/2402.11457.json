{
    "title": "When Do LLMs Need Retrieval Augmentation? Mitigating LLMs' Overconfidence Helps Retrieval Augmentation",
    "abstract": "arXiv:2402.11457v1 Announce Type: new  Abstract: Large Language Models (LLMs) have been found to have difficulty knowing they do not possess certain knowledge and tend to provide specious answers in such cases. Retrieval Augmentation (RA) has been extensively studied to mitigate LLMs' hallucinations. However, due to the extra overhead and unassured quality of retrieval, it may not be optimal to conduct RA all the time. A straightforward idea is to only conduct retrieval when LLMs are uncertain about a question. This motivates us to enhance the LLMs' ability to perceive their knowledge boundaries to help RA. In this paper, we first quantitatively measure LLMs' such ability and confirm their overconfidence. Then, we study how LLMs' certainty about a question correlates with their dependence on external retrieved information. We propose several methods to enhance LLMs' perception of knowledge boundaries and show that they are effective in reducing overconfidence. Additionally, equipped wi",
    "link": "https://arxiv.org/abs/2402.11457",
    "context": "Title: When Do LLMs Need Retrieval Augmentation? Mitigating LLMs' Overconfidence Helps Retrieval Augmentation\nAbstract: arXiv:2402.11457v1 Announce Type: new  Abstract: Large Language Models (LLMs) have been found to have difficulty knowing they do not possess certain knowledge and tend to provide specious answers in such cases. Retrieval Augmentation (RA) has been extensively studied to mitigate LLMs' hallucinations. However, due to the extra overhead and unassured quality of retrieval, it may not be optimal to conduct RA all the time. A straightforward idea is to only conduct retrieval when LLMs are uncertain about a question. This motivates us to enhance the LLMs' ability to perceive their knowledge boundaries to help RA. In this paper, we first quantitatively measure LLMs' such ability and confirm their overconfidence. Then, we study how LLMs' certainty about a question correlates with their dependence on external retrieved information. We propose several methods to enhance LLMs' perception of knowledge boundaries and show that they are effective in reducing overconfidence. Additionally, equipped wi",
    "path": "papers/24/02/2402.11457.json",
    "total_tokens": 831,
    "translated_title": "LLM何时需要检索增强？减轻LLM的过度自信有助于检索增强",
    "translated_abstract": "大型语言模型（LLMs）被发现很难知道自己不具备某些知识，并且在这种情况下往往会提供虚假答案。检索增强（RA）已被广泛研究以减轻LLMs的幻觉。然而，由于额外的开销和检索质量不确定，始终进行RA可能并不是最佳选择。一个直观的想法是只有在LLMs对问题不确定时才进行检索。这激发我们增强LLMs感知知识边界的能力以帮助RA。本文首先定量衡量LLMs的这种能力并确认它们的过度自信。然后，我们研究LLMs对问题的确定性如何与他们依赖外部检索信息相关。我们提出了几种方法来增强LLMs对知识边界的感知，并显示它们在减少过度自信方面是有效的。",
    "tldr": "LLM何时需要检索增强？减轻LLM的过度自信有助于检索增强",
    "en_tdlr": "When should LLMs use retrieval augmentation? Mitigating LLMs' overconfidence helps retrieval augmentation."
}