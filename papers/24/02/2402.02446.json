{
    "title": "LQER: Low-Rank Quantization Error Reconstruction for LLMs",
    "abstract": "Post-training quantization of Large Language Models (LLMs) is challenging. In this work, we introduce Low-rank Quantization Error Reduction (LQER), which combines quantization and low-rank approximation to recover the model capability. LQER leverages an activation-induced scale matrix to drive the singular value distribution of quantization error towards a desirable distribution, which enables nearly-lossless W4A8 quantization on various LLMs and downstream tasks without the need for knowledge distillation, grid search, or gradient-base iterative optimization. Unlike existing methods, the computation pattern of LQER eliminates the need for specialized Scatter and Gather processes to collect high-precision weights from irregular memory locations. Our W4A8 LLMs achieve near-lossless performance on six popular downstream tasks, while using 1.36$\\times$ fewer hardware resources than the leading state-of-the-art method. We will open-source our framework once the paper is accepted.",
    "link": "https://arxiv.org/abs/2402.02446",
    "context": "Title: LQER: Low-Rank Quantization Error Reconstruction for LLMs\nAbstract: Post-training quantization of Large Language Models (LLMs) is challenging. In this work, we introduce Low-rank Quantization Error Reduction (LQER), which combines quantization and low-rank approximation to recover the model capability. LQER leverages an activation-induced scale matrix to drive the singular value distribution of quantization error towards a desirable distribution, which enables nearly-lossless W4A8 quantization on various LLMs and downstream tasks without the need for knowledge distillation, grid search, or gradient-base iterative optimization. Unlike existing methods, the computation pattern of LQER eliminates the need for specialized Scatter and Gather processes to collect high-precision weights from irregular memory locations. Our W4A8 LLMs achieve near-lossless performance on six popular downstream tasks, while using 1.36$\\times$ fewer hardware resources than the leading state-of-the-art method. We will open-source our framework once the paper is accepted.",
    "path": "papers/24/02/2402.02446.json",
    "total_tokens": 838,
    "translated_title": "LQER: 低秩量化误差重建用于LLMs",
    "translated_abstract": "大型语言模型（LLMs）的训练后量化是具有挑战性的。在这项工作中，我们介绍了低秩量化误差减少（LQER）方法，该方法结合了量化和低秩逼近来恢复模型的能力。LQER利用激活引起的尺度矩阵将量化误差的奇异值分布推向期望的分布，从而实现了在各种LLMs和下游任务上近乎无损的W4A8量化，无需知识蒸馏、网格搜索或基于梯度的迭代优化。与现有方法不同，LQER的计算模式消除了从不规则内存位置收集高精度权重所需的专用Scatter和Gather过程。我们的W4A8 LLMs在六个热门下游任务上实现了近乎无损的性能，同时使用的硬件资源比领先的最新方法少1.36倍。一旦论文被接受，我们将开源我们的框架。",
    "tldr": "LQER使用低秩逼近和激活引起的尺度矩阵，实现了对LLMs的近乎无损量化，无需知识蒸馏或梯度优化，并大幅减少硬件资源的使用。"
}