{
    "title": "G-SciEdBERT: A Contextualized LLM for Science Assessment Tasks in German",
    "abstract": "The advancement of natural language processing has paved the way for automated scoring systems in various languages, such as German (e.g., German BERT [G-BERT]). Automatically scoring written responses to science questions in German is a complex task and challenging for standard G-BERT as they lack contextual knowledge in the science domain and may be unaligned with student writing styles. This paper developed a contextualized German Science Education BERT (G-SciEdBERT), an innovative large language model tailored for scoring German-written responses to science tasks. Using G-BERT, we pre-trained G-SciEdBERT on a corpus of 50K German written science responses with 5M tokens to the Programme for International Student Assessment (PISA) 2015. We fine-tuned G-SciEdBERT on 59 assessment items and examined the scoring accuracy. We then compared its performance with G-BERT. Our findings reveal a substantial improvement in scoring accuracy with G-SciEdBERT, demonstrating a 10% increase of quad",
    "link": "https://arxiv.org/abs/2402.06584",
    "context": "Title: G-SciEdBERT: A Contextualized LLM for Science Assessment Tasks in German\nAbstract: The advancement of natural language processing has paved the way for automated scoring systems in various languages, such as German (e.g., German BERT [G-BERT]). Automatically scoring written responses to science questions in German is a complex task and challenging for standard G-BERT as they lack contextual knowledge in the science domain and may be unaligned with student writing styles. This paper developed a contextualized German Science Education BERT (G-SciEdBERT), an innovative large language model tailored for scoring German-written responses to science tasks. Using G-BERT, we pre-trained G-SciEdBERT on a corpus of 50K German written science responses with 5M tokens to the Programme for International Student Assessment (PISA) 2015. We fine-tuned G-SciEdBERT on 59 assessment items and examined the scoring accuracy. We then compared its performance with G-BERT. Our findings reveal a substantial improvement in scoring accuracy with G-SciEdBERT, demonstrating a 10% increase of quad",
    "path": "papers/24/02/2402.06584.json",
    "total_tokens": 968,
    "translated_title": "G-SciEdBERT: 用于德语科学评估任务的上下文化大型语言模型",
    "translated_abstract": "自然语言处理的进步为各种语言（例如德语中的德语BERT [G-BERT]）的自动评分系统铺平了道路。自动评分德语科学问题的书面回答是一项复杂的任务，对于标准的G-BERT来说具有挑战性，因为它们缺乏科学领域的上下文知识，并且可能与学生的写作风格不一致。本文开发了一种上下文化德语科学教育BERT（G-SciEdBERT），一个创新的大型语言模型，专门用于评分德语科学任务的书面回答。我们使用G-BERT，在5M个标记的PISA 2015国际学生评估的50K个德语科学回答语料库上对G-SciEdBERT进行了预训练。然后，我们在59个评估项目上对G-SciEdBERT进行了微调，并检查了评分准确性。然后，我们将其性能与G-BERT进行了比较。我们的研究结果显示，G-SciEdBERT在评分准确性方面取得了显著的改进，表明其评分准确性提高了10%。",
    "tldr": "G-SciEdBERT是一种上下文化德语科学教育BERT，用于评分德语科学任务的书面回答。通过在大规模德语科学回答语料库上进行预训练，并在评分准确性方面取得了10%的改善。",
    "en_tdlr": "G-SciEdBERT is a contextualized German Science Education BERT that is used to score written responses to science tasks in German. It achieves a significant improvement of 10% in scoring accuracy by pre-training on a large corpus of German science responses and fine-tuning on assessment items."
}