{
    "title": "Reinforcement Learning with Dynamic Multi-Reward Weighting for Multi-Style Controllable Generation",
    "abstract": "arXiv:2402.14146v1 Announce Type: new  Abstract: Style is an integral component of text that expresses a diverse set of information, including interpersonal dynamics (e.g. formality) and the author's emotions or attitudes (e.g. disgust). Humans often employ multiple styles simultaneously. An open question is how large language models can be explicitly controlled so that they weave together target styles when generating text: for example, to produce text that is both negative and non-toxic. Previous work investigates the controlled generation of a single style, or else controlled generation of a style and other attributes. In this paper, we expand this into controlling multiple styles simultaneously. Specifically, we investigate various formulations of multiple style rewards for a reinforcement learning (RL) approach to controlled multi-style generation. These reward formulations include calibrated outputs from discriminators and dynamic weighting by discriminator gradient magnitudes. W",
    "link": "https://arxiv.org/abs/2402.14146",
    "context": "Title: Reinforcement Learning with Dynamic Multi-Reward Weighting for Multi-Style Controllable Generation\nAbstract: arXiv:2402.14146v1 Announce Type: new  Abstract: Style is an integral component of text that expresses a diverse set of information, including interpersonal dynamics (e.g. formality) and the author's emotions or attitudes (e.g. disgust). Humans often employ multiple styles simultaneously. An open question is how large language models can be explicitly controlled so that they weave together target styles when generating text: for example, to produce text that is both negative and non-toxic. Previous work investigates the controlled generation of a single style, or else controlled generation of a style and other attributes. In this paper, we expand this into controlling multiple styles simultaneously. Specifically, we investigate various formulations of multiple style rewards for a reinforcement learning (RL) approach to controlled multi-style generation. These reward formulations include calibrated outputs from discriminators and dynamic weighting by discriminator gradient magnitudes. W",
    "path": "papers/24/02/2402.14146.json",
    "total_tokens": 824,
    "translated_title": "使用动态多重奖励加权的强化学习用于多样式可控生成",
    "translated_abstract": "风格是表达各种信息的文本中的一个组成部分，包括人际动态（例如正式性）和作者的情绪或态度（例如厌恶）。人类经常同时采用多种风格。一个待解决的问题是如何明确控制大型语言模型，使它们在生成文本时编织目标风格：例如，生成既消极又无毒的文本。先前的工作探讨了对单一风格的控制生成，或者对风格和其他属性的控制生成。在本文中，我们将这扩展到同时控制多种风格。具体而言，我们研究了用于受控多样式生成的强化学习（RL）方法的多种风格奖励的各种公式。这些奖励公式包括来自鉴别器的校准输出以及通过鉴别器梯度幅度进行动态加权。",
    "tldr": "本文提出了一种使用强化学习来控制多种风格生成的方法，通过动态权重调整多重奖励，实现了在生成文本时同时控制多种风格。",
    "en_tdlr": "This paper presents a method for controlling multi-style generation using reinforcement learning, achieving simultaneous control of various styles in text generation through dynamic weighting of multiple rewards."
}