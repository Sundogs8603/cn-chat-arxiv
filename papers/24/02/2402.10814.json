{
    "title": "Associative Memories in the Feature Space",
    "abstract": "arXiv:2402.10814v1 Announce Type: new  Abstract: An autoassociative memory model is a function that, given a set of data points, takes as input an arbitrary vector and outputs the most similar data point from the memorized set. However, popular memory models fail to retrieve images even when the corruption is mild and easy to detect for a human evaluator. This is because similarities are evaluated in the raw pixel space, which does not contain any semantic information about the images. This problem can be easily solved by computing \\emph{similarities} in an embedding space instead of the pixel space. We show that an effective way of computing such embeddings is via a network pretrained with a contrastive loss. As the dimension of embedding spaces is often significantly smaller than the pixel space, we also have a faster computation of similarity scores. We test this method on complex datasets such as CIFAR10 and STL10. An additional drawback of current models is the need of storing the",
    "link": "https://arxiv.org/abs/2402.10814",
    "context": "Title: Associative Memories in the Feature Space\nAbstract: arXiv:2402.10814v1 Announce Type: new  Abstract: An autoassociative memory model is a function that, given a set of data points, takes as input an arbitrary vector and outputs the most similar data point from the memorized set. However, popular memory models fail to retrieve images even when the corruption is mild and easy to detect for a human evaluator. This is because similarities are evaluated in the raw pixel space, which does not contain any semantic information about the images. This problem can be easily solved by computing \\emph{similarities} in an embedding space instead of the pixel space. We show that an effective way of computing such embeddings is via a network pretrained with a contrastive loss. As the dimension of embedding spaces is often significantly smaller than the pixel space, we also have a faster computation of similarity scores. We test this method on complex datasets such as CIFAR10 and STL10. An additional drawback of current models is the need of storing the",
    "path": "papers/24/02/2402.10814.json",
    "total_tokens": 849,
    "translated_title": "特征空间中的联想记忆",
    "translated_abstract": "自联想存储器模型是一个函数，给定一组数据点，以任意向量作为输入，并输出与记忆集中最相似的数据点。然而，流行的存储模型在轻微损坏的情况下甚至无法检索图像，尽管这种损坏对人类评估者来说很容易检测。这是因为相似性是在原始像素空间中评估的，而原始像素空间不包含有关图像的任何语义信息。这个问题可以通过在嵌入空间而不是像素空间中计算相似性来轻松解决。我们展示了通过使用带有对比损失预训练的网络来计算这些嵌入的有效方法。由于嵌入空间的维度通常显着小于像素空间，我们还能更快地计算相似度分数。我们在复杂的数据集（如CIFAR10和STL10）上测试了这种方法。当前模型的另一个缺点是需要存储...",
    "tldr": "在特征空间中计算相似性以提升联想存储器模型的性能，并通过使用对比损失预训练网络来生成嵌入向量，从而提高检索速度。",
    "en_tdlr": "Computing similarities in the feature space improves the performance of autoassociative memory models, employing a pretrained network with contrastive loss for generating embedding vectors enhances retrieval speed."
}