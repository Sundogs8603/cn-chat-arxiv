{
    "title": "Don't Go To Extremes: Revealing the Excessive Sensitivity and Calibration Limitations of LLMs in Implicit Hate Speech Detection",
    "abstract": "arXiv:2402.11406v1 Announce Type: new  Abstract: The fairness and trustworthiness of Large Language Models (LLMs) are receiving increasing attention. Implicit hate speech, which employs indirect language to convey hateful intentions, occupies a significant portion of practice. However, the extent to which LLMs effectively address this issue remains insufficiently examined. This paper delves into the capability of LLMs to detect implicit hate speech (Classification Task) and express confidence in their responses (Calibration Task). Our evaluation meticulously considers various prompt patterns and mainstream uncertainty estimation methods. Our findings highlight that LLMs exhibit two extremes: (1) LLMs display excessive sensitivity towards groups or topics that may cause fairness issues, resulting in misclassifying benign statements as hate speech. (2) LLMs' confidence scores for each method excessively concentrate on a fixed range, remaining unchanged regardless of the dataset's complex",
    "link": "https://arxiv.org/abs/2402.11406",
    "context": "Title: Don't Go To Extremes: Revealing the Excessive Sensitivity and Calibration Limitations of LLMs in Implicit Hate Speech Detection\nAbstract: arXiv:2402.11406v1 Announce Type: new  Abstract: The fairness and trustworthiness of Large Language Models (LLMs) are receiving increasing attention. Implicit hate speech, which employs indirect language to convey hateful intentions, occupies a significant portion of practice. However, the extent to which LLMs effectively address this issue remains insufficiently examined. This paper delves into the capability of LLMs to detect implicit hate speech (Classification Task) and express confidence in their responses (Calibration Task). Our evaluation meticulously considers various prompt patterns and mainstream uncertainty estimation methods. Our findings highlight that LLMs exhibit two extremes: (1) LLMs display excessive sensitivity towards groups or topics that may cause fairness issues, resulting in misclassifying benign statements as hate speech. (2) LLMs' confidence scores for each method excessively concentrate on a fixed range, remaining unchanged regardless of the dataset's complex",
    "path": "papers/24/02/2402.11406.json",
    "total_tokens": 918,
    "translated_title": "不要走向极端：揭示LLMs在隐式仇恨言论检测中的过度敏感性和校准限制",
    "translated_abstract": "大型语言模型（LLMs）的公平性和可信度越来越受到关注。隐式仇恨言论，利用间接语言传达仇恨意图，占据实践中的重要部分。然而，LLMs有效解决这一问题的程度尚未得到充分审查。本文探讨了LLMs检测隐式仇恨言论（分类任务）以及对其响应的信心进行表达（校准任务）的能力。我们的评估细致考虑了各种提示模式和主流的不确定性估计方法。我们的研究结果突出了LLMs展示了两个极端：（1）LLMs对可能导致公平性问题的群体或话题显示出过度的敏感性，导致将良性陈述错误分类为仇恨言论。 （2）LLMs对每种方法的置信度得分过度集中在一个固定范围上，无论数据集的复杂性如何也保持不变。",
    "tldr": "本文研究了LLMs在检测隐式仇恨言论和表达信心时的能力，发现LLMs存在两个极端：对可能引起公平问题的群体或话题表现出过于敏感，同时置信度评分过度集中在一个范围内。"
}