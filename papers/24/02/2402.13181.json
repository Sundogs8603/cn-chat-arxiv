{
    "title": "DINOBot: Robot Manipulation via Retrieval and Alignment with Vision Foundation Models",
    "abstract": "arXiv:2402.13181v1 Announce Type: cross  Abstract: We propose DINOBot, a novel imitation learning framework for robot manipulation, which leverages the image-level and pixel-level capabilities of features extracted from Vision Transformers trained with DINO. When interacting with a novel object, DINOBot first uses these features to retrieve the most visually similar object experienced during human demonstrations, and then uses this object to align its end-effector with the novel object to enable effective interaction. Through a series of real-world experiments on everyday tasks, we show that exploiting both the image-level and pixel-level properties of vision foundation models enables unprecedented learning efficiency and generalisation. Videos and code are available at https://www.robot-learning.uk/dinobot.",
    "link": "https://arxiv.org/abs/2402.13181",
    "context": "Title: DINOBot: Robot Manipulation via Retrieval and Alignment with Vision Foundation Models\nAbstract: arXiv:2402.13181v1 Announce Type: cross  Abstract: We propose DINOBot, a novel imitation learning framework for robot manipulation, which leverages the image-level and pixel-level capabilities of features extracted from Vision Transformers trained with DINO. When interacting with a novel object, DINOBot first uses these features to retrieve the most visually similar object experienced during human demonstrations, and then uses this object to align its end-effector with the novel object to enable effective interaction. Through a series of real-world experiments on everyday tasks, we show that exploiting both the image-level and pixel-level properties of vision foundation models enables unprecedented learning efficiency and generalisation. Videos and code are available at https://www.robot-learning.uk/dinobot.",
    "path": "papers/24/02/2402.13181.json",
    "total_tokens": 716,
    "translated_title": "DINOBot：通过视觉基础模型的检索和对齐实现机器人操作",
    "translated_abstract": "我们提出了DINOBot，这是一个新颖的模仿学习框架，用于机器人操作，它利用了从使用DINO训练的Vision Transformers提取的特征的图像级和像素级能力。与新对象交互时，DINOBot首先使用这些特征来检索在人类演示中经历过的最相似的对象，然后使用该对象来将其末端执行器与新对象对齐，以实现有效的交互。通过一系列在日常任务中的真实世界实验，我们展示利用视觉基础模型的图像级和像素级属性能够实现前所未有的学习效率和泛化能力。视频和代码可在https://www.robot-learning.uk/dinobot找到。",
    "tldr": "DINOBot利用Vision Transformers的特征，实现了通过检索和对齐来进行机器人操作，提高了学习效率和泛化能力"
}