{
    "title": "SA-MDKIF: A Scalable and Adaptable Medical Domain Knowledge Injection Framework for Large Language Models",
    "abstract": "Recent advances in large language models (LLMs) have demonstrated exceptional performance in various natural language processing (NLP) tasks. However, their effective application in the medical domain is hampered by a lack of medical domain knowledge. In this study, we present SA-MDKIF, a scalable and adaptable framework that aims to inject medical knowledge into general-purpose LLMs through instruction tuning, thereby enabling adaptability for various downstream tasks. SA-MDKIF consists of two stages: skill training and skill adaptation. In the first stage, we define 12 basic medical skills and use AdaLoRA to train these skills based on uniformly formatted instructional datasets that we have constructed. In the next stage, we train the skill router using task-specific downstream data and use this router to integrate the acquired skills with LLMs during inference. Experimental results on 9 different medical tasks show that SA-MDKIF improves performance by 10-20% compared to the origina",
    "link": "https://arxiv.org/abs/2402.00474",
    "context": "Title: SA-MDKIF: A Scalable and Adaptable Medical Domain Knowledge Injection Framework for Large Language Models\nAbstract: Recent advances in large language models (LLMs) have demonstrated exceptional performance in various natural language processing (NLP) tasks. However, their effective application in the medical domain is hampered by a lack of medical domain knowledge. In this study, we present SA-MDKIF, a scalable and adaptable framework that aims to inject medical knowledge into general-purpose LLMs through instruction tuning, thereby enabling adaptability for various downstream tasks. SA-MDKIF consists of two stages: skill training and skill adaptation. In the first stage, we define 12 basic medical skills and use AdaLoRA to train these skills based on uniformly formatted instructional datasets that we have constructed. In the next stage, we train the skill router using task-specific downstream data and use this router to integrate the acquired skills with LLMs during inference. Experimental results on 9 different medical tasks show that SA-MDKIF improves performance by 10-20% compared to the origina",
    "path": "papers/24/02/2402.00474.json",
    "total_tokens": 938,
    "translated_title": "SA-MDKIF：一种可扩展和适应性强的大型语言模型医学领域知识注入框架",
    "translated_abstract": "大型语言模型(LLMs)的最新进展在各种自然语言处理(NLP)任务中展现出了卓越的性能。然而，它们在医学领域的有效应用受到医学领域知识的缺乏的限制。在本研究中，我们提出了一种可扩展和适应性强的框架SA-MDKIF，旨在通过指令调整将医学知识注入通用型LLMs中，从而实现对各种下游任务的适应性。SA-MDKIF包括两个阶段：技能训练和技能适应。在第一阶段，我们定义了12种基本的医学技能，并使用AdaLoRA根据我们构建的统一格式的指令数据集来训练这些技能。在下一个阶段，我们使用特定任务的下游数据来训练技能路由器，并在推理过程中使用该路由器将获取的技能与LLMs集成。对9个不同的医学任务的实验结果显示，与原始模型相比，SA-MDKIF的性能提高了10-20％。",
    "tldr": "SA-MDKIF是一种可扩展和适应性强的医学领域知识注入框架，通过指令调整并训练医学技能，并在推理中将其与大型语言模型集成，提高了医学任务的性能。"
}