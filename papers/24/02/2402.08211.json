{
    "title": "Transformer Mechanisms Mimic Frontostriatal Gating Operations When Trained on Human Working Memory Tasks",
    "abstract": "Models based on the Transformer neural network architecture have seen success on a wide variety of tasks that appear to require complex \"cognitive branching\" -- or the ability to maintain pursuit of one goal while accomplishing others. In cognitive neuroscience, success on such tasks is thought to rely on sophisticated frontostriatal mechanisms for selective \\textit{gating}, which enable role-addressable updating -- and later readout -- of information to and from distinct \"addresses\" of memory, in the form of clusters of neurons. However, Transformer models have no such mechanisms intentionally built-in. It is thus an open question how Transformers solve such tasks, and whether the mechanisms that emerge to help them to do so bear any resemblance to the gating mechanisms in the human brain. In this work, we analyze the mechanisms that emerge within a vanilla attention-only Transformer trained on a simple sequence modeling task inspired by a task explicitly designed to study working mem",
    "link": "https://arxiv.org/abs/2402.08211",
    "context": "Title: Transformer Mechanisms Mimic Frontostriatal Gating Operations When Trained on Human Working Memory Tasks\nAbstract: Models based on the Transformer neural network architecture have seen success on a wide variety of tasks that appear to require complex \"cognitive branching\" -- or the ability to maintain pursuit of one goal while accomplishing others. In cognitive neuroscience, success on such tasks is thought to rely on sophisticated frontostriatal mechanisms for selective \\textit{gating}, which enable role-addressable updating -- and later readout -- of information to and from distinct \"addresses\" of memory, in the form of clusters of neurons. However, Transformer models have no such mechanisms intentionally built-in. It is thus an open question how Transformers solve such tasks, and whether the mechanisms that emerge to help them to do so bear any resemblance to the gating mechanisms in the human brain. In this work, we analyze the mechanisms that emerge within a vanilla attention-only Transformer trained on a simple sequence modeling task inspired by a task explicitly designed to study working mem",
    "path": "papers/24/02/2402.08211.json",
    "total_tokens": 876,
    "translated_title": "当在人类工作记忆任务上训练时，Transformer机制模仿额顶回路封锁操作",
    "translated_abstract": "基于Transformer神经网络架构的模型在许多需要复杂的“认知分支”（或在实现其他目标的同时保持对一个目标的追求）的任务上取得了成功。在认知神经科学中，成功完成这样的任务被认为依赖于复杂的额顶回路机制，这些机制通过选择性“封锁”实现了对信息的更新和读取，这些信息以神经元团簇的形式存储在记忆的不同“地址”上。然而，Transformer模型并没有有意地内置这样的机制。因此，如何解决这些任务以及为此而出现的机制是否与人脑的封锁机制相似都是一个开放问题。在这项工作中，我们分析了在仅使用注意力机制的基本Transformer上训练一个受到工作记忆研究任务启发的简单序列建模任务中出现的机制。",
    "tldr": "本研究分析了基于Transformer的神经网络在人类工作记忆任务上训练时所出现的机制，揭示了这种模型如何解决复杂的认知分支任务，并探讨了这些机制与人脑封锁机制的相似性。",
    "en_tdlr": "This study analyzes the mechanisms that emerge in Transformer neural networks when trained on human working memory tasks, revealing how the model tackles complex cognitive branching tasks and exploring the resemblance between these mechanisms and the gating mechanisms in the human brain."
}