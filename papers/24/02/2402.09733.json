{
    "title": "Do LLMs Know about Hallucination? An Empirical Investigation of LLM's Hidden States",
    "abstract": "arXiv:2402.09733v1 Announce Type: new  Abstract: Large Language Models (LLMs) can make up answers that are not real, and this is known as hallucination. This research aims to see if, how, and to what extent LLMs are aware of hallucination. More specifically, we check whether and how an LLM reacts differently in its hidden states when it answers a question right versus when it hallucinates. To do this, we introduce an experimental framework which allows examining LLM's hidden states in different hallucination situations. Building upon this framework, we conduct a series of experiments with language models in the LLaMA family (Touvron et al., 2023). Our empirical findings suggest that LLMs react differently when processing a genuine response versus a fabricated one. We then apply various model interpretation techniques to help understand and explain the findings better. Moreover, informed by the empirical observations, we show great potential of using the guidance derived from LLM's hidd",
    "link": "https://arxiv.org/abs/2402.09733",
    "context": "Title: Do LLMs Know about Hallucination? An Empirical Investigation of LLM's Hidden States\nAbstract: arXiv:2402.09733v1 Announce Type: new  Abstract: Large Language Models (LLMs) can make up answers that are not real, and this is known as hallucination. This research aims to see if, how, and to what extent LLMs are aware of hallucination. More specifically, we check whether and how an LLM reacts differently in its hidden states when it answers a question right versus when it hallucinates. To do this, we introduce an experimental framework which allows examining LLM's hidden states in different hallucination situations. Building upon this framework, we conduct a series of experiments with language models in the LLaMA family (Touvron et al., 2023). Our empirical findings suggest that LLMs react differently when processing a genuine response versus a fabricated one. We then apply various model interpretation techniques to help understand and explain the findings better. Moreover, informed by the empirical observations, we show great potential of using the guidance derived from LLM's hidd",
    "path": "papers/24/02/2402.09733.json",
    "total_tokens": 902,
    "translated_title": "LLM是否了解幻觉？对LLMs隐藏状态的实证调查",
    "translated_abstract": "大型语言模型（LLMs）可能会虚构出不真实的答案，这就是所谓的幻觉。本研究旨在观察LLMs是否意识到幻觉以及其程度。更具体地说，我们检查了LLMs在其隐藏状态下对正确回答和幻觉回答的不同反应。为了实现这一目标，我们引入了一个实验框架，允许检查LLMs在不同幻觉情况下的隐藏状态。基于这个框架，我们进行了一系列实验，使用LLaMA家族的语言模型（Touvron等，2023）。我们的实证发现表明，LLMs在处理真实回答和虚构回答时有不同的反应。然后，我们应用各种模型解释技术来更好地理解和解释这些发现。此外，根据实证观察结果，我们展示了利用从LLMs隐藏状态中得到的指导的巨大潜力。",
    "tldr": "本研究探索了大型语言模型（LLMs）对幻觉的意识程度，通过实验发现LLMs在处理真实回答和虚构回答时有不同的反应，并展示了利用LLMs隐藏状态的指导具有巨大潜力。",
    "en_tdlr": "This study investigates the awareness of Large Language Models (LLMs) of hallucination and finds that LLMs exhibit different reactions when processing genuine responses versus fabricated ones. The potential of utilizing guidance derived from LLM's hidden states is also demonstrated."
}