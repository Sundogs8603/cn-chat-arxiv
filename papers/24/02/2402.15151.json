{
    "title": "Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing",
    "abstract": "arXiv:2402.15151v1 Announce Type: cross  Abstract: In visual speech processing, context modeling capability is one of the most important requirements due to the ambiguous nature of lip movements. For example, homophenes, words that share identical lip movements but produce different sounds, can be distinguished by considering the context. In this paper, we propose a novel framework, namely Visual Speech Processing incorporated with LLMs (VSP-LLM), to maximize the context modeling ability by bringing the overwhelming power of LLMs. Specifically, VSP-LLM is designed to perform multi-tasks of visual speech recognition and translation, where the given instructions control the type of task. The input video is mapped to the input latent space of a LLM by employing a self-supervised visual speech model. Focused on the fact that there is redundant information in input frames, we propose a novel deduplication method that reduces the embedded visual features by employing visual speech units. Thr",
    "link": "https://arxiv.org/abs/2402.15151",
    "context": "Title: Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing\nAbstract: arXiv:2402.15151v1 Announce Type: cross  Abstract: In visual speech processing, context modeling capability is one of the most important requirements due to the ambiguous nature of lip movements. For example, homophenes, words that share identical lip movements but produce different sounds, can be distinguished by considering the context. In this paper, we propose a novel framework, namely Visual Speech Processing incorporated with LLMs (VSP-LLM), to maximize the context modeling ability by bringing the overwhelming power of LLMs. Specifically, VSP-LLM is designed to perform multi-tasks of visual speech recognition and translation, where the given instructions control the type of task. The input video is mapped to the input latent space of a LLM by employing a self-supervised visual speech model. Focused on the fact that there is redundant information in input frames, we propose a novel deduplication method that reduces the embedded visual features by employing visual speech units. Thr",
    "path": "papers/24/02/2402.15151.json",
    "total_tokens": 846,
    "translated_title": "视觉语音遇见语言：VSP-LLM框架用于高效和上下文感知的视觉语音处理",
    "translated_abstract": "在视觉语音处理中，由于唇部运动的模糊性质，上下文建模能力是最重要的要求之一。例如，同音异义词，即具有相同唇部运动但产生不同声音的单词，可以通过考虑上下文来区分。本文提出了一种新颖的框架，称为集成LLM的视觉语音处理（VSP-LLM），通过引入LLM的强大能力来最大化上下文建模能力。具体来说，VSP-LLM旨在执行视觉语音识别和翻译的多任务，其中给定的指令控制任务类型。通过利用自监督视觉语音模型，将输入视频映射到LLM的输入潜在空间。针对输入帧存在冗余信息的事实，我们提出了一种新颖的去重方法，通过使用视觉语音单元减少嵌入的视觉特征。",
    "tldr": "提出了一个新颖的VSP-LLM框架，用于最大化上下文建模能力，实现视觉语音识别和翻译的多任务执行。"
}