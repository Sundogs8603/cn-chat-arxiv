{
    "title": "Prompt Perturbation Consistency Learning for Robust Language Models",
    "abstract": "arXiv:2402.15833v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated impressive performance on a number of natural language processing tasks, such as question answering and text summarization. However, their performance on sequence labeling tasks such as intent classification and slot filling (IC-SF), which is a central component in personal assistant systems, lags significantly behind discriminative models. Furthermore, there is a lack of substantive research on the robustness of LLMs to various perturbations in the input prompts. The contributions of this paper are three-fold. First, we show that fine-tuning sufficiently large LLMs can produce IC-SF performance comparable to discriminative models. Next, we systematically analyze the performance deterioration of those fine-tuned models due to three distinct yet relevant types of input perturbations - oronyms, synonyms, and paraphrasing. Finally, we propose an efficient mitigation approach, Prompt Perturbatio",
    "link": "https://arxiv.org/abs/2402.15833",
    "context": "Title: Prompt Perturbation Consistency Learning for Robust Language Models\nAbstract: arXiv:2402.15833v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated impressive performance on a number of natural language processing tasks, such as question answering and text summarization. However, their performance on sequence labeling tasks such as intent classification and slot filling (IC-SF), which is a central component in personal assistant systems, lags significantly behind discriminative models. Furthermore, there is a lack of substantive research on the robustness of LLMs to various perturbations in the input prompts. The contributions of this paper are three-fold. First, we show that fine-tuning sufficiently large LLMs can produce IC-SF performance comparable to discriminative models. Next, we systematically analyze the performance deterioration of those fine-tuned models due to three distinct yet relevant types of input perturbations - oronyms, synonyms, and paraphrasing. Finally, we propose an efficient mitigation approach, Prompt Perturbatio",
    "path": "papers/24/02/2402.15833.json",
    "total_tokens": 806,
    "translated_title": "针对鲁棒性语言模型的提示扰动一致性学习",
    "translated_abstract": "大型语言模型（LLMs）在诸如问答和文本总结等多个自然语言处理任务中展现出令人印象深刻的性能。然而，在意图分类和槽填充（IC-SF）等序列标注任务上，它们的性能显著落后于判别模型。本文的贡献有三个方面。首先，我们展示了对足够大的LLMs进行微调可以产生与判别模型相媲美的IC-SF性能。接下来，我们系统分析了这些经过微调的模型由于三种不同而相关的输入扰动 - 同音词、同义词和释义 - 导致的性能恶化。最后，我们提出了一种高效的缓解方法，即提示扰动一致性学习。",
    "tldr": "微调大型语言模型可以产生与判别模型相当的性能，在分析和解决LLMs对输入提示中不同类型扰动的鲁棒性方面取得了重要进展",
    "en_tdlr": "Fine-tuning large language models can achieve performance comparable to discriminative models, and this paper makes significant progress in analyzing and addressing the robustness of LLMs to different types of perturbations in input prompts."
}