{
    "title": "On the Stability of Gradient Descent for Large Learning Rate",
    "abstract": "arXiv:2402.13108v1 Announce Type: new  Abstract: There currently is a significant interest in understanding the Edge of Stability (EoS) phenomenon, which has been observed in neural networks training, characterized by a non-monotonic decrease of the loss function over epochs, while the sharpness of the loss (spectral norm of the Hessian) progressively approaches and stabilizes around 2/(learning rate). Reasons for the existence of EoS when training using gradient descent have recently been proposed -- a lack of flat minima near the gradient descent trajectory together with the presence of compact forward-invariant sets. In this paper, we show that linear neural networks optimized under a quadratic loss function satisfy the first assumption and also a necessary condition for the second assumption. More precisely, we prove that the gradient descent map is non-singular, the set of global minimizers of the loss function forms a smooth manifold, and the stable minima form a bounded subset i",
    "link": "https://arxiv.org/abs/2402.13108",
    "context": "Title: On the Stability of Gradient Descent for Large Learning Rate\nAbstract: arXiv:2402.13108v1 Announce Type: new  Abstract: There currently is a significant interest in understanding the Edge of Stability (EoS) phenomenon, which has been observed in neural networks training, characterized by a non-monotonic decrease of the loss function over epochs, while the sharpness of the loss (spectral norm of the Hessian) progressively approaches and stabilizes around 2/(learning rate). Reasons for the existence of EoS when training using gradient descent have recently been proposed -- a lack of flat minima near the gradient descent trajectory together with the presence of compact forward-invariant sets. In this paper, we show that linear neural networks optimized under a quadratic loss function satisfy the first assumption and also a necessary condition for the second assumption. More precisely, we prove that the gradient descent map is non-singular, the set of global minimizers of the loss function forms a smooth manifold, and the stable minima form a bounded subset i",
    "path": "papers/24/02/2402.13108.json",
    "total_tokens": 917,
    "translated_title": "关于大学习率下梯度下降的稳定性",
    "translated_abstract": "目前对理解“稳定性边缘（EoS）”现象存在着相当大的兴趣，这一现象在神经网络训练中被观察到，其特点是损失函数在不同纪元间的非单调下降，而损失的陡峭度（Hessian的谱范数）逐渐接近并稳定在2/(学习率)附近。最近有人提出了使用梯度下降训练时出现EoS的原因——沿梯度下降轨迹附近缺乏平坦的极小值点，同时存在紧致的正向不变集。在本文中，我们证明了在二次损失函数下优化的线性神经网络满足第一个假设以及第二个假设的一个必要条件。更具体地，我们证明了梯度下降映射是非奇异的，损失函数的全局最小值点集构成一个光滑流形，并且稳定的极小值构成有界子集。",
    "tldr": "本文研究了线性神经网络在二次损失函数下的优化问题，证明了梯度下降映射的非奇异性以及全局最小值点集的光滑流形特性，为理解大学习率下梯度下降的稳定性提供了重要线索。",
    "en_tdlr": "This paper investigates the optimization of linear neural networks under quadratic loss function, demonstrating the non-singularity of gradient descent map and the smooth manifold property of the set of global minimizers, offering crucial insights into the stability of gradient descent with large learning rates."
}