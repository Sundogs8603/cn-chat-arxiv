{
    "title": "Progressive Gradient Flow for Robust N:M Sparsity Training in Transformers",
    "abstract": "N:M Structured sparsity has garnered significant interest as a result of relatively modest overhead and improved efficiency. Additionally, this form of sparsity holds considerable appeal for reducing the memory footprint owing to their modest representation overhead. There have been efforts to develop training recipes for N:M structured sparsity, they primarily focus on low-sparsity regions ($\\sim$50\\%). Nonetheless, performance of models trained using these approaches tends to decline when confronted with high-sparsity regions ($>$80\\%). In this work, we study the effectiveness of existing sparse training recipes at \\textit{high-sparsity regions} and argue that these methods fail to sustain the model quality on par with low-sparsity regions. We demonstrate that the significant factor contributing to this disparity is the presence of elevated levels of induced noise in the gradient magnitudes. To mitigate this undesirable effect, we employ decay mechanisms to progressively restrict the",
    "link": "https://arxiv.org/abs/2402.04744",
    "context": "Title: Progressive Gradient Flow for Robust N:M Sparsity Training in Transformers\nAbstract: N:M Structured sparsity has garnered significant interest as a result of relatively modest overhead and improved efficiency. Additionally, this form of sparsity holds considerable appeal for reducing the memory footprint owing to their modest representation overhead. There have been efforts to develop training recipes for N:M structured sparsity, they primarily focus on low-sparsity regions ($\\sim$50\\%). Nonetheless, performance of models trained using these approaches tends to decline when confronted with high-sparsity regions ($>$80\\%). In this work, we study the effectiveness of existing sparse training recipes at \\textit{high-sparsity regions} and argue that these methods fail to sustain the model quality on par with low-sparsity regions. We demonstrate that the significant factor contributing to this disparity is the presence of elevated levels of induced noise in the gradient magnitudes. To mitigate this undesirable effect, we employ decay mechanisms to progressively restrict the",
    "path": "papers/24/02/2402.04744.json",
    "total_tokens": 970,
    "translated_title": "渐进梯度流在Transformer中稀疏训练的鲁棒N:M结构",
    "translated_abstract": "由于相对较低的开销和提高的效率，N:M结构的稀疏性引起了广泛的关注。此外，这种稀疏性形式对于降低内存占用具有相当大的吸引力，因为其表示开销较小。已经有一些努力针对N:M结构的稀疏性开发训练方法，但是它们主要关注低稀疏区域($\\sim$50\\%)。然而，使用这些方法训练的模型在面对高稀疏区域($>$80\\%)时往往性能下降。在这项工作中，我们研究了现有稀疏训练方法在高稀疏区域的有效性，并认为这些方法无法保持与低稀疏区域相当的模型质量。我们证明造成这种差距的主要因素是梯度幅值中引入的噪音水平提高。为了减轻这种不良影响，我们采用衰减机制逐渐限制梯度的流动。",
    "tldr": "本文研究了Transformer中高稀疏区域稀疏训练方法的有效性，发现现有方法无法保持与低稀疏区域相当的模型质量，主要原因是梯度幅值中引入的噪音水平提高。为了解决这个问题，采用了渐进限制梯度流动的衰减机制。",
    "en_tdlr": "This paper investigates the effectiveness of sparse training methods in high-sparsity regions of Transformers and finds that existing methods fail to maintain the same model quality as in low-sparsity regions, mainly due to elevated levels of induced noise in gradient magnitudes. To address this issue, a progressive decay mechanism is employed to gradually restrict the flow of gradients."
}