{
    "title": "Language Model Adaptation to Specialized Domains through Selective Masking based on Genre and Topical Characteristics",
    "abstract": "arXiv:2402.12036v1 Announce Type: new  Abstract: Recent advances in pre-trained language modeling have facilitated significant progress across various natural language processing (NLP) tasks. Word masking during model training constitutes a pivotal component of language modeling in architectures like BERT. However, the prevalent method of word masking relies on random selection, potentially disregarding domain-specific linguistic attributes. In this article, we introduce an innovative masking approach leveraging genre and topicality information to tailor language models to specialized domains. Our method incorporates a ranking process that prioritizes words based on their significance, subsequently guiding the masking procedure. Experiments conducted using continual pre-training within the legal domain have underscored the efficacy of our approach on the LegalGLUE benchmark in the English language. Pre-trained language models and code are freely available for use.",
    "link": "https://arxiv.org/abs/2402.12036",
    "context": "Title: Language Model Adaptation to Specialized Domains through Selective Masking based on Genre and Topical Characteristics\nAbstract: arXiv:2402.12036v1 Announce Type: new  Abstract: Recent advances in pre-trained language modeling have facilitated significant progress across various natural language processing (NLP) tasks. Word masking during model training constitutes a pivotal component of language modeling in architectures like BERT. However, the prevalent method of word masking relies on random selection, potentially disregarding domain-specific linguistic attributes. In this article, we introduce an innovative masking approach leveraging genre and topicality information to tailor language models to specialized domains. Our method incorporates a ranking process that prioritizes words based on their significance, subsequently guiding the masking procedure. Experiments conducted using continual pre-training within the legal domain have underscored the efficacy of our approach on the LegalGLUE benchmark in the English language. Pre-trained language models and code are freely available for use.",
    "path": "papers/24/02/2402.12036.json",
    "total_tokens": 820,
    "translated_title": "通过基于体裁和主题特征的选择性屏蔽，将语言模型适应专业领域",
    "translated_abstract": "最近，预训练语言建模的进展在各种自然语言处理（NLP）任务中取得了显著进展。模型训练期间的词屏蔽构成了像BERT这样的架构中语言建模的关键组成部分。然而，目前的词屏蔽方法依赖于随机选择，可能忽视领域特定的语言属性。在本文中，我们介绍了一种创新的屏蔽方法，利用体裁和主题信息来定制语言模型以适应专业领域。我们的方法包括一个排名过程，根据单词的重要性对其进行优先级排序，随后引导屏蔽过程。我们进行的实验使用法律领域内的持续预训练，强调了我们的方法在英语LegalGLUE基准上的有效性。预训练语言模型和代码可免费使用。",
    "tldr": "通过排名关键词并指导屏蔽过程，这项研究提出了一种利用体裁和主题信息定制语言模型适应专业领域的创新方法。",
    "en_tdlr": "This study introduces an innovative approach to customize language models for specialized domains by ranking keywords and guiding the masking process with genre and topical information."
}