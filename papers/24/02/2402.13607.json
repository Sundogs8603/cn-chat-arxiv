{
    "title": "CODIS: Benchmarking Context-Dependent Visual Comprehension for Multimodal Large Language Models",
    "abstract": "arXiv:2402.13607v1 Announce Type: cross  Abstract: Multimodal large language models (MLLMs) have demonstrated promising results in a variety of tasks that combine vision and language. As these models become more integral to research and applications, conducting comprehensive evaluations of their capabilities has grown increasingly important. However, most existing benchmarks fail to consider that, in certain situations, images need to be interpreted within a broader context. In this work, we introduce a new benchmark, named as CODIS, designed to assess the ability of models to use context provided in free-form text to enhance visual comprehension. Our findings indicate that MLLMs consistently fall short of human performance on this benchmark. Further analysis confirms that these models struggle to effectively extract and utilize contextual information to improve their understanding of images. This underscores the pressing need to enhance the ability of MLLMs to comprehend visuals in a ",
    "link": "https://arxiv.org/abs/2402.13607",
    "context": "Title: CODIS: Benchmarking Context-Dependent Visual Comprehension for Multimodal Large Language Models\nAbstract: arXiv:2402.13607v1 Announce Type: cross  Abstract: Multimodal large language models (MLLMs) have demonstrated promising results in a variety of tasks that combine vision and language. As these models become more integral to research and applications, conducting comprehensive evaluations of their capabilities has grown increasingly important. However, most existing benchmarks fail to consider that, in certain situations, images need to be interpreted within a broader context. In this work, we introduce a new benchmark, named as CODIS, designed to assess the ability of models to use context provided in free-form text to enhance visual comprehension. Our findings indicate that MLLMs consistently fall short of human performance on this benchmark. Further analysis confirms that these models struggle to effectively extract and utilize contextual information to improve their understanding of images. This underscores the pressing need to enhance the ability of MLLMs to comprehend visuals in a ",
    "path": "papers/24/02/2402.13607.json",
    "total_tokens": 891,
    "translated_title": "CODIS：为多模态大型语言模型基准化上下文相关的视觉理解",
    "translated_abstract": "多模态大型语言模型（MLLMs）在结合视觉和语言的各种任务中展现出了有希望的结果。随着这些模型在研究和应用中变得更加重要，对它们能力进行全面评估的重要性也日益增加。然而，大多数现有的基准测试未考虑到在某些情况下，图像需要在更广泛的上下文中被解释。在这项工作中，我们引入了一个名为CODIS的新基准，旨在评估模型使用在自由形式文本中提供的上下文来增强视觉理解的能力。我们的发现表明，MLLMs在这个基准上始终无法达到人类表现。进一步的分析证实了这些模型难以有效提取和利用上下文信息以提高它们对图像的理解能力。这凸显了提升MLLMs理解视觉能力的迫切需求。",
    "tldr": "介绍了CODIS基准，用于评估模型利用自由形式文本提供的上下文来增强视觉理解的能力，发现多模态大型语言模型在此基准上表现未达到人类水平，需要提升模型理解视觉能力。"
}