{
    "title": "Rethinking the Capacity of Graph Neural Networks for Branching Strategy",
    "abstract": "Graph neural networks (GNNs) have been widely used to predict properties and heuristics of mixed-integer linear programs (MILPs) and hence accelerate MILP solvers. This paper investigates the capacity of GNNs to represent strong branching (SB) scores that provide an efficient strategy in the branch-and-bound algorithm.   Although message-passing GNN (MP-GNN), as the simplest GNN structure, is frequently employed in the existing literature to learn SB scores, we prove a fundamental limitation in its expressive power -- there exist two MILP instances with different SB scores that cannot be distinguished by any MP-GNN, regardless of the number of parameters. In addition, we establish a universal approximation theorem for another GNN structure called the second-order folklore GNN (2-FGNN). We show that for any data distribution over MILPs, there always exists a 2-FGNN that can approximate the SB score with arbitrarily high accuracy and arbitrarily high probability. A small-scale numerical ",
    "link": "https://arxiv.org/abs/2402.07099",
    "context": "Title: Rethinking the Capacity of Graph Neural Networks for Branching Strategy\nAbstract: Graph neural networks (GNNs) have been widely used to predict properties and heuristics of mixed-integer linear programs (MILPs) and hence accelerate MILP solvers. This paper investigates the capacity of GNNs to represent strong branching (SB) scores that provide an efficient strategy in the branch-and-bound algorithm.   Although message-passing GNN (MP-GNN), as the simplest GNN structure, is frequently employed in the existing literature to learn SB scores, we prove a fundamental limitation in its expressive power -- there exist two MILP instances with different SB scores that cannot be distinguished by any MP-GNN, regardless of the number of parameters. In addition, we establish a universal approximation theorem for another GNN structure called the second-order folklore GNN (2-FGNN). We show that for any data distribution over MILPs, there always exists a 2-FGNN that can approximate the SB score with arbitrarily high accuracy and arbitrarily high probability. A small-scale numerical ",
    "path": "papers/24/02/2402.07099.json",
    "total_tokens": 888,
    "translated_title": "重新思考图神经网络在分支策略中的容量",
    "translated_abstract": "图神经网络（GNNs）被广泛应用于预测混合整数线性规划（MILPs）的属性和启发式，并加速MILP求解器。本文研究了GNNs在表示提供分支限界算法中高效策略的强分支（SB）得分方面的能力。尽管现有文献中经常使用最简单的消息传递GNN（MP-GNN）来学习SB得分，但我们证明了其表达能力的一个根本局限性--存在两个不同SB得分的MILP实例，无论参数的数量如何，都无法通过任何MP-GNN区分。此外，我们建立了一个通用逼近定理，用于另一种GNN结构称为second-order folklore GNN（2-FGNN）。我们证明了对于任何MILP数据分布，总是存在一个可以以任意高精度和任意高概率逼近SB得分的2-FGNN。一个小规模的数值实验",
    "tldr": "本文研究了图神经网络（GNNs）在分支策略中的容量，并发现了消息传递GNN (MP-GNN) 的表达能力的局限性以及另一种GNN结构 second-order folklore GNN (2-FGNN) 的通用逼近性质。"
}