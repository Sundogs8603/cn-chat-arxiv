{
    "title": "RL-GPT: Integrating Reinforcement Learning and Code-as-policy",
    "abstract": "arXiv:2402.19299v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated proficiency in utilizing various tools by coding, yet they face limitations in handling intricate logic and precise control. In embodied tasks, high-level planning is amenable to direct coding, while low-level actions often necessitate task-specific refinement, such as Reinforcement Learning (RL). To seamlessly integrate both modalities, we introduce a two-level hierarchical framework, RL-GPT, comprising a slow agent and a fast agent. The slow agent analyzes actions suitable for coding, while the fast agent executes coding tasks. This decomposition effectively focuses each agent on specific tasks, proving highly efficient within our pipeline. Our approach outperforms traditional RL methods and existing GPT agents, demonstrating superior efficiency. In the Minecraft game, it rapidly obtains diamonds within a single day on an RTX3090. Additionally, it achieves SOTA performance across all desig",
    "link": "https://arxiv.org/abs/2402.19299",
    "context": "Title: RL-GPT: Integrating Reinforcement Learning and Code-as-policy\nAbstract: arXiv:2402.19299v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated proficiency in utilizing various tools by coding, yet they face limitations in handling intricate logic and precise control. In embodied tasks, high-level planning is amenable to direct coding, while low-level actions often necessitate task-specific refinement, such as Reinforcement Learning (RL). To seamlessly integrate both modalities, we introduce a two-level hierarchical framework, RL-GPT, comprising a slow agent and a fast agent. The slow agent analyzes actions suitable for coding, while the fast agent executes coding tasks. This decomposition effectively focuses each agent on specific tasks, proving highly efficient within our pipeline. Our approach outperforms traditional RL methods and existing GPT agents, demonstrating superior efficiency. In the Minecraft game, it rapidly obtains diamonds within a single day on an RTX3090. Additionally, it achieves SOTA performance across all desig",
    "path": "papers/24/02/2402.19299.json",
    "total_tokens": 858,
    "translated_title": "RL-GPT: 将强化学习和代码作为策略进行整合",
    "translated_abstract": "大型语言模型(LLMs)表现出在利用编码时各种工具方面的熟练程度，但在处理复杂逻辑和精确控制方面存在局限性。在具体任务中，高层规划适宜于直接编码，而低层动作通常需要任务特定的细化，比如强化学习（RL）。为了无缝整合这两种模式，我们引入了一个两级分层框架RL-GPT，包括一个慢速代理和一个快速代理。慢速代理分析适合编码的动作，而快速代理执行编码任务。这种分解有效地使每个代理专注于特定任务，在我们的流水线中证明是非常高效的。我们的方法胜过传统的RL方法和现有的GPT代理，表现出卓越的效率。在Minecraft游戏中，它在RTX3090上在一天内迅速获得了钻石。此外，它在所有设计方面实现了SOTA性能。",
    "tldr": "RL-GPT 是一个两级分层框架，结合了慢速代理和快速代理，能够高效地整合强化学习和编码任务，在Minecraft游戏中表现出卓越效率。",
    "en_tdlr": "RL-GPT is a two-level hierarchical framework that integrates slow and fast agents, efficiently combining reinforcement learning and coding tasks, demonstrating superior efficiency in Minecraft game."
}