{
    "title": "Training Heterogeneous Client Models using Knowledge Distillation in Serverless Federated Learning",
    "abstract": "Federated Learning (FL) is an emerging machine learning paradigm that enables the collaborative training of a shared global model across distributed clients while keeping the data decentralized. Recent works on designing systems for efficient FL have shown that utilizing serverless computing technologies, particularly Function-as-a-Service (FaaS) for FL, can enhance resource efficiency, reduce training costs, and alleviate the complex infrastructure management burden on data holders. However, existing serverless FL systems implicitly assume a uniform global model architecture across all participating clients during training. This assumption fails to address fundamental challenges in practical FL due to the resource and statistical data heterogeneity among FL clients. To address these challenges and enable heterogeneous client models in serverless FL, we utilize Knowledge Distillation (KD) in this paper. Towards this, we propose novel optimized serverless workflows for two popular conve",
    "link": "https://arxiv.org/abs/2402.07295",
    "context": "Title: Training Heterogeneous Client Models using Knowledge Distillation in Serverless Federated Learning\nAbstract: Federated Learning (FL) is an emerging machine learning paradigm that enables the collaborative training of a shared global model across distributed clients while keeping the data decentralized. Recent works on designing systems for efficient FL have shown that utilizing serverless computing technologies, particularly Function-as-a-Service (FaaS) for FL, can enhance resource efficiency, reduce training costs, and alleviate the complex infrastructure management burden on data holders. However, existing serverless FL systems implicitly assume a uniform global model architecture across all participating clients during training. This assumption fails to address fundamental challenges in practical FL due to the resource and statistical data heterogeneity among FL clients. To address these challenges and enable heterogeneous client models in serverless FL, we utilize Knowledge Distillation (KD) in this paper. Towards this, we propose novel optimized serverless workflows for two popular conve",
    "path": "papers/24/02/2402.07295.json",
    "total_tokens": 857,
    "translated_title": "在无服务器联邦学习中使用知识蒸馏训练异构客户端模型",
    "translated_abstract": "联邦学习是一种新兴的机器学习范式，它使得分布式客户端之间能够协作训练共享的全局模型，同时保持数据的去中心化。最近的研究在设计高效的联邦学习系统方面表明，利用无服务器计算技术，特别是函数即服务（FaaS）用于联邦学习，可以提高资源效率，降低训练成本，并减轻数据持有者面临的复杂基础设施管理负担。然而，现有的无服务器联邦学习系统在训练过程中隐式地假设所有参与客户端具有相同的全局模型架构。这一假设未能解决实际联邦学习中的基本挑战，因为不同客户端之间存在资源和统计数据的异质性。为了解决这些挑战并在无服务器联邦学习中实现异构客户端模型，本文提出了利用知识蒸馏（KD）的新型优化无服务器工作流。",
    "tldr": "本文提出在无服务器联邦学习中利用知识蒸馏训练异构客户端模型的方法，以解决资源和统计数据的异质性挑战。"
}