{
    "title": "Sparse Linear Regression and Lattice Problems",
    "abstract": "arXiv:2402.14645v1 Announce Type: new  Abstract: Sparse linear regression (SLR) is a well-studied problem in statistics where one is given a design matrix $X\\in\\mathbb{R}^{m\\times n}$ and a response vector $y=X\\theta^*+w$ for a $k$-sparse vector $\\theta^*$ (that is, $\\|\\theta^*\\|_0\\leq k$) and small, arbitrary noise $w$, and the goal is to find a $k$-sparse $\\widehat{\\theta} \\in \\mathbb{R}^n$ that minimizes the mean squared prediction error $\\frac{1}{m}\\|X\\widehat{\\theta}-X\\theta^*\\|^2_2$. While $\\ell_1$-relaxation methods such as basis pursuit, Lasso, and the Dantzig selector solve SLR when the design matrix is well-conditioned, no general algorithm is known, nor is there any formal evidence of hardness in an average-case setting with respect to all efficient algorithms.   We give evidence of average-case hardness of SLR w.r.t. all efficient algorithms assuming the worst-case hardness of lattice problems. Specifically, we give an instance-by-instance reduction from a variant of the bo",
    "link": "https://arxiv.org/abs/2402.14645",
    "context": "Title: Sparse Linear Regression and Lattice Problems\nAbstract: arXiv:2402.14645v1 Announce Type: new  Abstract: Sparse linear regression (SLR) is a well-studied problem in statistics where one is given a design matrix $X\\in\\mathbb{R}^{m\\times n}$ and a response vector $y=X\\theta^*+w$ for a $k$-sparse vector $\\theta^*$ (that is, $\\|\\theta^*\\|_0\\leq k$) and small, arbitrary noise $w$, and the goal is to find a $k$-sparse $\\widehat{\\theta} \\in \\mathbb{R}^n$ that minimizes the mean squared prediction error $\\frac{1}{m}\\|X\\widehat{\\theta}-X\\theta^*\\|^2_2$. While $\\ell_1$-relaxation methods such as basis pursuit, Lasso, and the Dantzig selector solve SLR when the design matrix is well-conditioned, no general algorithm is known, nor is there any formal evidence of hardness in an average-case setting with respect to all efficient algorithms.   We give evidence of average-case hardness of SLR w.r.t. all efficient algorithms assuming the worst-case hardness of lattice problems. Specifically, we give an instance-by-instance reduction from a variant of the bo",
    "path": "papers/24/02/2402.14645.json",
    "total_tokens": 902,
    "translated_title": "稀疏线性回归和格问题",
    "translated_abstract": "稀疏线性回归（SLR）是统计学中一个研究良好的问题，其中给定设计矩阵 $X\\in\\mathbb{R}^{m\\times n}$ 和响应向量 $y=X\\theta^*+w$，其中 $\\theta^*$ 是 $k$-稀疏向量（即，$\\|\\theta^*\\|_0\\leq k$），$w$ 是小的、任意的噪声，目标是找到一个 $k$-稀疏的 $\\widehat{\\theta} \\in \\mathbb{R}^n$，使得均方预测误差 $\\frac{1}{m}\\|X\\widehat{\\theta}-X\\theta^*\\|^2_2$ 最小化。虽然 $\\ell_1$-松弛方法如基 Pursuit、Lasso 和 Dantzig 选择器在设计矩阵条件良好时解决了 SLR，但没有已知通用算法，也没有任何关于在所有高效算法的平均情况设置中的困难性的正式证据。",
    "tldr": "本文提供了关于稀疏线性回归在所有高效算法的平均情况困难性的证据，假设格问题的最坏情况困难性。"
}