{
    "title": "LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation",
    "abstract": "Low-Rank Adaptation (LoRA) introduces auxiliary parameters for each layer to fine-tune the pre-trained model under limited computing resources. But it still faces challenges of resource consumption when scaling up to larger models. Previous studies employ pruning techniques by evaluating the importance of LoRA parameters for different layers to address the problem. However, these efforts only analyzed parameter features to evaluate their importance. Indeed, the output of LoRA related to the parameters and data is the factor that directly impacts the frozen model. To this end, we propose LoRA-drop which evaluates the importance of the parameters by analyzing the LoRA output. We retain LoRA for important layers and the LoRA of the other layers share the same parameters. Abundant experiments on NLU and NLG tasks demonstrate the effectiveness of LoRA-drop.",
    "link": "https://arxiv.org/abs/2402.07721",
    "context": "Title: LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation\nAbstract: Low-Rank Adaptation (LoRA) introduces auxiliary parameters for each layer to fine-tune the pre-trained model under limited computing resources. But it still faces challenges of resource consumption when scaling up to larger models. Previous studies employ pruning techniques by evaluating the importance of LoRA parameters for different layers to address the problem. However, these efforts only analyzed parameter features to evaluate their importance. Indeed, the output of LoRA related to the parameters and data is the factor that directly impacts the frozen model. To this end, we propose LoRA-drop which evaluates the importance of the parameters by analyzing the LoRA output. We retain LoRA for important layers and the LoRA of the other layers share the same parameters. Abundant experiments on NLU and NLG tasks demonstrate the effectiveness of LoRA-drop.",
    "path": "papers/24/02/2402.07721.json",
    "total_tokens": 793,
    "translated_title": "LoRA-drop：基于输出评估的高效LoRA参数剪枝",
    "translated_abstract": "低秩适应（LoRA）为每个层引入辅助参数，以在有限的计算资源下微调预训练模型。但是，当扩展到更大的模型时，仍然面临资源消耗的挑战。先前的研究通过评估不同层的LoRA参数的重要性来采用剪枝技术来解决这个问题。然而，这些努力只分析了参数的特征以评估其重要性。事实上，与参数和数据相关的LoRA的输出是直接影响冻结模型的因素。为此，我们提出了LoRA-drop，通过分析LoRA输出来评估参数的重要性。我们保留重要层的LoRA，而其他层的LoRA共享相同的参数。在NLU和NLG任务上进行了充分的实验，证明了LoRA-drop的有效性。",
    "tldr": "本文提出了LoRA-drop方法，通过分析LoRA输出评估参数的重要性，并且保留重要层的LoRA，其余层共享相同参数。实验结果表明LoRA-drop有很好的效果。",
    "en_tdlr": "This paper proposes LoRA-drop, which evaluates the importance of parameters by analyzing LoRA output and retains LoRA for important layers while the other layers share the same parameters. Abundant experiments demonstrate the effectiveness of LoRA-drop."
}