{
    "title": "Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic",
    "abstract": "arXiv:2402.11746v1 Announce Type: cross  Abstract: Aligned language models face a significant limitation as their fine-tuning often results in compromised safety. To tackle this, we propose a simple method RESTA that performs LLM safety realignment. RESTA stands for REstoring Safety through Task Arithmetic. At its core, it involves a simple arithmetic addition of a safety vector to the weights of the compromised model. We demonstrate the effectiveness of RESTA in both parameter-efficient and full fine-tuning, covering a wide range of downstream tasks, including instruction following in Chinese, English, and Hindi, as well as problem-solving capabilities in Code and Math. We also showcase the generalizability of RESTA on three existing safety evaluation benchmarks and a multilingual benchmark dataset proposed as a part of this work, consisting of 550 harmful questions covering 11 categories, each with 5 sub-categories of harm. Overall, RESTA decreases the harmfulness of the compromised ",
    "link": "https://arxiv.org/abs/2402.11746",
    "context": "Title: Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic\nAbstract: arXiv:2402.11746v1 Announce Type: cross  Abstract: Aligned language models face a significant limitation as their fine-tuning often results in compromised safety. To tackle this, we propose a simple method RESTA that performs LLM safety realignment. RESTA stands for REstoring Safety through Task Arithmetic. At its core, it involves a simple arithmetic addition of a safety vector to the weights of the compromised model. We demonstrate the effectiveness of RESTA in both parameter-efficient and full fine-tuning, covering a wide range of downstream tasks, including instruction following in Chinese, English, and Hindi, as well as problem-solving capabilities in Code and Math. We also showcase the generalizability of RESTA on three existing safety evaluation benchmarks and a multilingual benchmark dataset proposed as a part of this work, consisting of 550 harmful questions covering 11 categories, each with 5 sub-categories of harm. Overall, RESTA decreases the harmfulness of the compromised ",
    "path": "papers/24/02/2402.11746.json",
    "total_tokens": 850,
    "translated_title": "语言模型就是霍默·辛普森！通过任务算法对精调语言模型进行安全重新定位",
    "translated_abstract": "经过微调的对齐语言模型往往会导致安全性受损，为了解决这一问题，我们提出了一种简单的方法RESTA，该方法执行LLM安全重新定位。RESTA代表通过任务算法恢复安全。在其核心思想中，它涉及将一个安全向量简单地加到受损模型的权重上。我们展示了RESTA在参数高效和完全微调中的有效性，涵盖了广泛的下游任务，包括中文、英文和印地文的指令跟随，以及代码和数学的问题解决能力。我们还展示了RESTA在三个现有安全评估基准和作为本项工作一部分提出的一个多语言基准数据集上的通用性，该数据集包括550个有害问题，涵盖11个类别，每个类别下包含5个有害的子类别。总的来说，RESTA降低了受损模型的有害程度。",
    "tldr": "提出了一种简单方法RESTA，通过任务算法对精调语言模型进行安全重新定位，有效降低了其有害程度。"
}