{
    "title": "Score-based generative models break the curse of dimensionality in learning a family of sub-Gaussian probability distributions",
    "abstract": "While score-based generative models (SGMs) have achieved remarkable success in enormous image generation tasks, their mathematical foundations are still limited. In this paper, we analyze the approximation and generalization of SGMs in learning a family of sub-Gaussian probability distributions. We introduce a notion of complexity for probability distributions in terms of their relative density with respect to the standard Gaussian measure. We prove that if the log-relative density can be locally approximated by a neural network whose parameters can be suitably bounded, then the distribution generated by empirical score matching approximates the target distribution in total variation with a dimension-independent rate. We illustrate our theory through examples, which include certain mixtures of Gaussians. An essential ingredient of our proof is to derive a dimension-free deep neural network approximation rate for the true score function associated with the forward process, which is inte",
    "link": "https://arxiv.org/abs/2402.08082",
    "context": "Title: Score-based generative models break the curse of dimensionality in learning a family of sub-Gaussian probability distributions\nAbstract: While score-based generative models (SGMs) have achieved remarkable success in enormous image generation tasks, their mathematical foundations are still limited. In this paper, we analyze the approximation and generalization of SGMs in learning a family of sub-Gaussian probability distributions. We introduce a notion of complexity for probability distributions in terms of their relative density with respect to the standard Gaussian measure. We prove that if the log-relative density can be locally approximated by a neural network whose parameters can be suitably bounded, then the distribution generated by empirical score matching approximates the target distribution in total variation with a dimension-independent rate. We illustrate our theory through examples, which include certain mixtures of Gaussians. An essential ingredient of our proof is to derive a dimension-free deep neural network approximation rate for the true score function associated with the forward process, which is inte",
    "path": "papers/24/02/2402.08082.json",
    "total_tokens": 852,
    "translated_title": "基于分数的生成模型在学习一个子高斯概率分布族中突破了维数灾难",
    "translated_abstract": "尽管基于分数的生成模型（SGMs）在巨大的图像生成任务中取得了显著的成功，但它们的数学基础仍然有限。在本文中，我们分析了SGMs在学习一个子高斯概率分布族中的近似和泛化。我们引入了一种关于概率分布复杂性的概念，即相对密度与标准高斯测度的相对密度。我们证明，如果对数相对密度可以通过神经网络进行局部逼近，并且网络参数可以适当地受限，那么通过经验分数匹配生成的分布以维度无关的速率逼近目标分布的总变差。我们通过示例说明了我们的理论，其中包括某些高斯混合分布。我们证明的一个关键点是推导出与正向过程相关的真实得分函数的维度无关的深度神经网络逼近速率。",
    "tldr": "基于分数的生成模型在学习一个子高斯概率分布族中突破了维数灾难，通过分数匹配生成的分布以维度无关的速率逼近目标分布的总变差。",
    "en_tdlr": "Score-based generative models break the curse of dimensionality in learning a family of sub-Gaussian probability distributions by approximating the target distribution in total variation with a dimension-independent rate through empirical score matching."
}