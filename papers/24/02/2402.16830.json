{
    "title": "SKILL: Similarity-aware Knowledge distILLation for Speech Self-Supervised Learning",
    "abstract": "arXiv:2402.16830v1 Announce Type: cross  Abstract: Self-supervised learning (SSL) has achieved remarkable success across various speech-processing tasks. To enhance its efficiency, previous works often leverage the use of compression techniques. A notable recent attempt is DPHuBERT, which applies joint knowledge distillation (KD) and structured pruning to learn a significantly smaller SSL model. In this paper, we contribute to this research domain by introducing SKILL, a novel method that conducts distillation across groups of layers instead of distilling individual arbitrarily selected layers within the teacher network. The identification of the layers to distill is achieved through a hierarchical clustering procedure applied to layer similarity measures. Extensive experiments demonstrate that our distilled version of WavLM Base+ not only outperforms DPHuBERT but also achieves state-of-the-art results in the 30M parameters model class across several SUPERB tasks.",
    "link": "https://arxiv.org/abs/2402.16830",
    "context": "Title: SKILL: Similarity-aware Knowledge distILLation for Speech Self-Supervised Learning\nAbstract: arXiv:2402.16830v1 Announce Type: cross  Abstract: Self-supervised learning (SSL) has achieved remarkable success across various speech-processing tasks. To enhance its efficiency, previous works often leverage the use of compression techniques. A notable recent attempt is DPHuBERT, which applies joint knowledge distillation (KD) and structured pruning to learn a significantly smaller SSL model. In this paper, we contribute to this research domain by introducing SKILL, a novel method that conducts distillation across groups of layers instead of distilling individual arbitrarily selected layers within the teacher network. The identification of the layers to distill is achieved through a hierarchical clustering procedure applied to layer similarity measures. Extensive experiments demonstrate that our distilled version of WavLM Base+ not only outperforms DPHuBERT but also achieves state-of-the-art results in the 30M parameters model class across several SUPERB tasks.",
    "path": "papers/24/02/2402.16830.json",
    "total_tokens": 950,
    "translated_title": "SKILL：面向语音自监督学习的相似性感知知识蒸馏",
    "translated_abstract": "自监督学习（SSL）在各种语音处理任务中取得了显著的成功。为了提高其效率，先前的工作通常利用压缩技术。最近一个显著的尝试是DPHuBERT，它应用联合知识蒸馏（KD）和结构化修剪来学习一个显著较小的SSL模型。本文通过引入SKILL贡献到这一研究领域，SKILL是一种新颖的方法，它通过对层组进行蒸馏，而不是对教师网络中任意选择的单个层进行蒸馏。确定要蒸馏的层是通过应用于层相似度度量的层次聚类程序实现的。大量实验表明，我们蒸馏后的WavLM Base+不仅胜过DPHuBERT，而且在30M参数模型类别中在几个SUPERB任务中实现了最先进的结果。",
    "tldr": "SKILL是一种新颖的面向语音自监督学习的知识蒸馏方法，通过在层组之间进行蒸馏，而不是蒸馏教师网络中任意选择的单个层，通过层次聚类程序确定要蒸馏的层，实现了超越DPHuBERT的性能，并在30M参数模型类别中在几个SUPERB任务中取得了最先进的结果。",
    "en_tdlr": "SKILL is a novel knowledge distillation method for speech self-supervised learning, distilling across groups of layers instead of individual selected layers in the teacher network, identifying layers to distill through a hierarchical clustering procedure applied to layer similarity measures, outperforming DPHuBERT and achieving state-of-the-art results in several SUPERB tasks in the 30M parameters model class."
}