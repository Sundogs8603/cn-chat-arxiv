{
    "title": "Why do Learning Rates Transfer? Reconciling Optimization and Scaling Limits for Deep Learning",
    "abstract": "arXiv:2402.17457v1 Announce Type: new  Abstract: Recently, there has been growing evidence that if the width and depth of a neural network are scaled toward the so-called rich feature learning limit ($\\mu$P and its depth extension), then some hyperparameters - such as the learning rate - exhibit transfer from small to very large models, thus reducing the cost of hyperparameter tuning. From an optimization perspective, this phenomenon is puzzling, as it implies that the loss landscape is remarkably consistent across very different model sizes. In this work, we find empirical evidence that learning rate transfer can be attributed to the fact that under $\\mu$P and its depth extension, the largest eigenvalue of the training loss Hessian (i.e. the sharpness) is largely independent of the width and depth of the network for a sustained period of training time. On the other hand, we show that under the neural tangent kernel (NTK) regime, the sharpness exhibits very different dynamics at differ",
    "link": "https://arxiv.org/abs/2402.17457",
    "context": "Title: Why do Learning Rates Transfer? Reconciling Optimization and Scaling Limits for Deep Learning\nAbstract: arXiv:2402.17457v1 Announce Type: new  Abstract: Recently, there has been growing evidence that if the width and depth of a neural network are scaled toward the so-called rich feature learning limit ($\\mu$P and its depth extension), then some hyperparameters - such as the learning rate - exhibit transfer from small to very large models, thus reducing the cost of hyperparameter tuning. From an optimization perspective, this phenomenon is puzzling, as it implies that the loss landscape is remarkably consistent across very different model sizes. In this work, we find empirical evidence that learning rate transfer can be attributed to the fact that under $\\mu$P and its depth extension, the largest eigenvalue of the training loss Hessian (i.e. the sharpness) is largely independent of the width and depth of the network for a sustained period of training time. On the other hand, we show that under the neural tangent kernel (NTK) regime, the sharpness exhibits very different dynamics at differ",
    "path": "papers/24/02/2402.17457.json",
    "total_tokens": 930,
    "translated_title": "为什么学习速率具有迁移性？调和深度学习中的优化和尺度极限",
    "translated_abstract": "最近，越来越多的证据表明，如果神经网络的宽度和深度朝着所谓的丰富特征学习极限（μP及其深度扩展）进行缩放，那么某些超参数 - 例如学习速率 - 就会从小模型转移到非常大的模型，从而降低了超参数调整的成本。从优化的角度来看，这种现象令人困惑，因为它意味着损失景观在非常不同的模型尺寸之间是非常一致的。在这项工作中，我们找到证据支持学习速率迁移可以归因于事实：在μP及其深度扩展下，训练损失Hessian的最大特征值（即锐度）在较长的训练时间内在很大程度上独立于网络的宽度和深度。另一方面，我们展示在神经切线核（NTK）体系下，锐度在不同",
    "tldr": "学习速率迁移现象可以归因于在μP和其深度延伸下，训练损失Hessian矩阵的最大特征值（即锐度）在较长时间的训练过程中，基本独立于网络的宽度和深度。",
    "en_tdlr": "The phenomenon of learning rate transfer can be attributed to the fact that the largest eigenvalue of the training loss Hessian under μP and its depth extension is largely independent of the width and depth of the network for a sustained period of training time."
}