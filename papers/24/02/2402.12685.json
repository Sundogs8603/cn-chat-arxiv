{
    "title": "XRL-Bench: A Benchmark for Evaluating and Comparing Explainable Reinforcement Learning Techniques",
    "abstract": "arXiv:2402.12685v1 Announce Type: new  Abstract: Reinforcement Learning (RL) has demonstrated substantial potential across diverse fields, yet understanding its decision-making process, especially in real-world scenarios where rationality and safety are paramount, is an ongoing challenge. This paper delves in to Explainable RL (XRL), a subfield of Explainable AI (XAI) aimed at unravelling the complexities of RL models. Our focus rests on state-explaining techniques, a crucial subset within XRL methods, as they reveal the underlying factors influencing an agent's actions at any given time. Despite their significant role, the lack of a unified evaluation framework hinders assessment of their accuracy and effectiveness. To address this, we introduce XRL-Bench, a unified standardized benchmark tailored for the evaluation and comparison of XRL methods, encompassing three main modules: standard RL environments, explainers based on state importance, and standard evaluators. XRL-Bench supports",
    "link": "https://arxiv.org/abs/2402.12685",
    "context": "Title: XRL-Bench: A Benchmark for Evaluating and Comparing Explainable Reinforcement Learning Techniques\nAbstract: arXiv:2402.12685v1 Announce Type: new  Abstract: Reinforcement Learning (RL) has demonstrated substantial potential across diverse fields, yet understanding its decision-making process, especially in real-world scenarios where rationality and safety are paramount, is an ongoing challenge. This paper delves in to Explainable RL (XRL), a subfield of Explainable AI (XAI) aimed at unravelling the complexities of RL models. Our focus rests on state-explaining techniques, a crucial subset within XRL methods, as they reveal the underlying factors influencing an agent's actions at any given time. Despite their significant role, the lack of a unified evaluation framework hinders assessment of their accuracy and effectiveness. To address this, we introduce XRL-Bench, a unified standardized benchmark tailored for the evaluation and comparison of XRL methods, encompassing three main modules: standard RL environments, explainers based on state importance, and standard evaluators. XRL-Bench supports",
    "path": "papers/24/02/2402.12685.json",
    "total_tokens": 954,
    "translated_title": "XRL-Bench：用于评估和比较可解释强化学习技术的基准",
    "translated_abstract": "强化学习（RL）已在不同领域展现出巨大潜力，然而理解其决策过程，特别是在现实世界场景中理性和安全至关重要的情况下，仍然是一个持续挑战。本文着手于可解释强化学习（XRL），这是可解释人工智能（XAI）的一个子领域，旨在揭示RL模型的复杂性。我们关注状态解释技术，这是XRL方法中一个至关重要的子集，因为它们揭示了影响代理程序在任何给定时间采取行动的潜在因素。尽管它们的重要作用，缺乏一个统一的评估框架阻碍了对它们准确性和有效性的评估。为了解决这个问题，我们介绍了XRL-Bench，一种统一的标准化基准，专为评估和比较XRL方法而设计，包括三个主要模块：标准RL环境、基于状态重要性的解释器和标准评估器。XRL-Bench支持载入模型和评估器，详细说明了适用于强化学习任务的各种技术。",
    "tldr": "XRL-Bench是一个用于评估和比较可解释强化学习技术的统一标准化基准，旨在解决状态解释方法在RL中的重要性评估框架不足的问题。",
    "en_tdlr": "XRL-Bench is a standardized benchmark for evaluating and comparing explainable reinforcement learning techniques, addressing the lack of evaluation framework for assessing the importance of state explanation methods in RL."
}