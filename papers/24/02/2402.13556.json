{
    "title": "Inductive Graph Alignment Prompt: Bridging the Gap between Graph Pre-training and Inductive Fine-tuning From Spectral Perspective",
    "abstract": "arXiv:2402.13556v1 Announce Type: cross  Abstract: The \"Graph pre-training and fine-tuning\" paradigm has significantly improved Graph Neural Networks(GNNs) by capturing general knowledge without manual annotations for downstream tasks. However, due to the immense gap of data and tasks between the pre-training and fine-tuning stages, the model performance is still limited. Inspired by prompt fine-tuning in Natural Language Processing(NLP), many endeavors have been made to bridge the gap in graph domain. But existing methods simply reformulate the form of fine-tuning tasks to the pre-training ones. With the premise that the pre-training graphs are compatible with the fine-tuning ones, these methods typically operate in transductive setting. In order to generalize graph pre-training to inductive scenario where the fine-tuning graphs might significantly differ from pre-training ones, we propose a novel graph prompt based method called Inductive Graph Alignment Prompt(IGAP). Firstly, we uni",
    "link": "https://arxiv.org/abs/2402.13556",
    "context": "Title: Inductive Graph Alignment Prompt: Bridging the Gap between Graph Pre-training and Inductive Fine-tuning From Spectral Perspective\nAbstract: arXiv:2402.13556v1 Announce Type: cross  Abstract: The \"Graph pre-training and fine-tuning\" paradigm has significantly improved Graph Neural Networks(GNNs) by capturing general knowledge without manual annotations for downstream tasks. However, due to the immense gap of data and tasks between the pre-training and fine-tuning stages, the model performance is still limited. Inspired by prompt fine-tuning in Natural Language Processing(NLP), many endeavors have been made to bridge the gap in graph domain. But existing methods simply reformulate the form of fine-tuning tasks to the pre-training ones. With the premise that the pre-training graphs are compatible with the fine-tuning ones, these methods typically operate in transductive setting. In order to generalize graph pre-training to inductive scenario where the fine-tuning graphs might significantly differ from pre-training ones, we propose a novel graph prompt based method called Inductive Graph Alignment Prompt(IGAP). Firstly, we uni",
    "path": "papers/24/02/2402.13556.json",
    "total_tokens": 863,
    "translated_title": "归纳图对齐提示：从谱角度弥合图预训练和归纳微调之间的差距",
    "translated_abstract": "“图预训练和微调”范式通过捕捉下游任务无需手动标注的通用知识，显著改进了图神经网络(GNNs)。然而，由于在预训练和微调阶段之间的数据和任务巨大差距，模型性能仍然受限。受自然语言处理(NLP)中提示微调的启发，许多努力已经为在图领域中弥合差距做出了努力。但现有方法仅仅将微调任务的形式重新表述为预训练任务。在预训练图与微调图兼容的前提下，这些方法通常在转导设置中运行。为了将图预训练泛化到归纳场景，其中微调图可能与预训练图显著不同，我们提出了一种名为归纳图对齐提示(IGAP)的新型基于图提示的方法。首先，我们统",
    "tldr": "IGAP提出了一种新型的基于图提示的方法，用于将图预训练泛化到归纳场景，弥合了预训练图和微调图之间的差距。"
}