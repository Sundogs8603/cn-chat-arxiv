{
    "title": "Memory-Augmented Generative Adversarial Transformers",
    "abstract": "arXiv:2402.19218v1 Announce Type: new  Abstract: Conversational AI systems that rely on Large Language Models, like Transformers, have difficulty interweaving external data (like facts) with the language they generate. Vanilla Transformer architectures are not designed for answering factual questions with high accuracy. This paper investigates a possible route for addressing this problem. We propose to extend the standard Transformer architecture with an additional memory bank holding extra information (such as facts drawn from a knowledge base), and an extra attention layer for addressing this memory. We add this augmented memory to a Generative Adversarial Network-inspired Transformer architecture. This setup allows for implementing arbitrary felicity conditions on the generated language of the Transformer. We first demonstrate how this machinery can be deployed for handling factual questions in goal-oriented dialogues. Secondly, we demonstrate that our approach can be useful for app",
    "link": "https://arxiv.org/abs/2402.19218",
    "context": "Title: Memory-Augmented Generative Adversarial Transformers\nAbstract: arXiv:2402.19218v1 Announce Type: new  Abstract: Conversational AI systems that rely on Large Language Models, like Transformers, have difficulty interweaving external data (like facts) with the language they generate. Vanilla Transformer architectures are not designed for answering factual questions with high accuracy. This paper investigates a possible route for addressing this problem. We propose to extend the standard Transformer architecture with an additional memory bank holding extra information (such as facts drawn from a knowledge base), and an extra attention layer for addressing this memory. We add this augmented memory to a Generative Adversarial Network-inspired Transformer architecture. This setup allows for implementing arbitrary felicity conditions on the generated language of the Transformer. We first demonstrate how this machinery can be deployed for handling factual questions in goal-oriented dialogues. Secondly, we demonstrate that our approach can be useful for app",
    "path": "papers/24/02/2402.19218.json",
    "total_tokens": 788,
    "translated_title": "基于记忆增强的生成对抗变压器",
    "translated_abstract": "依赖大型语言模型（如变压器）的会话AI系统在将外部数据（如事实）与其生成的语言相互交织时存在困难。普通的变压器架构并未设计用于准确回答事实问题。本文探讨了解决这一问题的可能途径。我们建议通过在标准变压器架构上扩展额外信息的记忆库（如来自知识库的事实）和用于处理这一记忆的额外注意力层。我们将这个增强记忆添加到启发式生成对抗网络的变压器架构中。这种设置允许在变压器生成的语言上实施任意的快乐条件。首先，我们展示了这种机制如何被用于处理目标导向对话中的事实问题。其次，我们展示了我们的方法如何对应用程序可能是有用的。",
    "tldr": "通过在标准变压器架构中增加额外的记忆库和注意力层，该研究提出了一种可以提高变压器生成语言准确性的方法。",
    "en_tdlr": "By adding an extra memory bank and attention layer to the standard Transformer architecture, this research proposes a method to improve the accuracy of language generation by Transformers."
}