{
    "title": "How do Hyenas deal with Human Speech? Speech Recognition and Translation with ConfHyena",
    "abstract": "arXiv:2402.13208v1 Announce Type: cross  Abstract: The attention mechanism, a cornerstone of state-of-the-art neural models, faces computational hurdles in processing long sequences due to its quadratic complexity. Consequently, research efforts in the last few years focused on finding more efficient alternatives. Among them, Hyena (Poli et al., 2023) stands out for achieving competitive results in both language modeling and image classification, while offering sub-quadratic memory and computational complexity. Building on these promising results, we propose ConfHyena, a Conformer whose encoder self-attentions are replaced with an adaptation of Hyena for speech processing, where the long input sequences cause high computational costs. Through experiments in automatic speech recognition (for English) and translation (from English into 8 target languages), we show that our best ConfHyena model significantly reduces the training time by 27%, at the cost of minimal quality degradation (~1%",
    "link": "https://arxiv.org/abs/2402.13208",
    "context": "Title: How do Hyenas deal with Human Speech? Speech Recognition and Translation with ConfHyena\nAbstract: arXiv:2402.13208v1 Announce Type: cross  Abstract: The attention mechanism, a cornerstone of state-of-the-art neural models, faces computational hurdles in processing long sequences due to its quadratic complexity. Consequently, research efforts in the last few years focused on finding more efficient alternatives. Among them, Hyena (Poli et al., 2023) stands out for achieving competitive results in both language modeling and image classification, while offering sub-quadratic memory and computational complexity. Building on these promising results, we propose ConfHyena, a Conformer whose encoder self-attentions are replaced with an adaptation of Hyena for speech processing, where the long input sequences cause high computational costs. Through experiments in automatic speech recognition (for English) and translation (from English into 8 target languages), we show that our best ConfHyena model significantly reduces the training time by 27%, at the cost of minimal quality degradation (~1%",
    "path": "papers/24/02/2402.13208.json",
    "total_tokens": 822,
    "translated_title": "出于性能考虑，Hyena如何处理人类语音？使用ConfHyena进行语音识别和翻译",
    "translated_abstract": "注意机制是现代神经模型的基石，但由于其二次复杂度而在处理长序列时面临计算障碍。因此，过去几年的研究工作侧重于寻找更有效的替代方案。其中，Hyena（Poli等，2023年）在语言建模和图像分类方面取得了竞争性结果，同时提供次线性的存储和计算复杂度。基于这些有希望的结果，我们提出了ConfHyena，这是一个Conformer，其编码器的自注意力被Hyena的一种变体取代，用于处理语音，其中长输入序列导致高计算成本。通过自动语音识别（英语）和翻译实验（从英语翻译成8种目标语言），我们展示了我们最好的ConfHyena模型将训练时间显著减少了27%，而品质损失仅为1%。",
    "tldr": "ConfHyena是一种基于Hyena的改进模型，通过在处理语音时减少计算成本，显著缩短了训练时间。",
    "en_tdlr": "ConfHyena is an improved model based on Hyena that significantly reduces training time by lowering computational costs in speech processing."
}