{
    "title": "Is Adversarial Training with Compressed Datasets Effective?",
    "abstract": "Dataset Condensation (DC) refers to the recent class of dataset compression methods that generate a smaller, synthetic, dataset from a larger dataset. This synthetic dataset retains the essential information of the original dataset, enabling models trained on it to achieve performance levels comparable to those trained on the full dataset. Most current DC methods have mainly concerned with achieving high test performance with limited data budget, and have not directly addressed the question of adversarial robustness. In this work, we investigate the impact of adversarial robustness on models trained with compressed datasets. We show that the compressed datasets obtained from DC methods are not effective in transferring adversarial robustness to models. As a solution to improve dataset compression efficiency and adversarial robustness simultaneously, we propose a novel robustness-aware dataset compression method based on finding the Minimal Finite Covering (MFC) of the dataset. The prop",
    "link": "https://arxiv.org/abs/2402.05675",
    "context": "Title: Is Adversarial Training with Compressed Datasets Effective?\nAbstract: Dataset Condensation (DC) refers to the recent class of dataset compression methods that generate a smaller, synthetic, dataset from a larger dataset. This synthetic dataset retains the essential information of the original dataset, enabling models trained on it to achieve performance levels comparable to those trained on the full dataset. Most current DC methods have mainly concerned with achieving high test performance with limited data budget, and have not directly addressed the question of adversarial robustness. In this work, we investigate the impact of adversarial robustness on models trained with compressed datasets. We show that the compressed datasets obtained from DC methods are not effective in transferring adversarial robustness to models. As a solution to improve dataset compression efficiency and adversarial robustness simultaneously, we propose a novel robustness-aware dataset compression method based on finding the Minimal Finite Covering (MFC) of the dataset. The prop",
    "path": "papers/24/02/2402.05675.json",
    "total_tokens": 830,
    "translated_title": "压缩数据集的对抗训练是否有效？",
    "translated_abstract": "数据集压缩（DC）是指从较大数据集中生成较小的合成数据集的一类最近的数据集压缩方法。这个合成数据集保留了原始数据集的基本信息，使得在其上训练的模型能够达到与在完整数据集上训练的模型相当的性能水平。目前大多数的DC方法主要关注如何在有限的数据预算下实现高测试性能，并没有直接解决对抗鲁棒性的问题。在本工作中，我们研究了在压缩数据集上训练的模型对对抗鲁棒性的影响。我们发现从DC方法获得的压缩数据集对模型的对抗鲁棒性没有有效的传递性。为了同时提高数据集压缩效率和对抗鲁棒性，我们提出了一种基于寻找数据集的最小有限覆盖（MFC）的新型鲁棒性感知数据集压缩方法。",
    "tldr": "本论文研究了在压缩数据集上训练的模型对对抗鲁棒性的影响，并提出了一种同时提高数据集压缩效率和对抗鲁棒性的方法。",
    "en_tdlr": "This paper investigates the impact of adversarial robustness on models trained with compressed datasets and proposes a method to improve dataset compression efficiency and adversarial robustness simultaneously."
}