{
    "title": "RoCoIns: Enhancing Robustness of Large Language Models through Code-Style Instructions",
    "abstract": "arXiv:2402.16431v1 Announce Type: new  Abstract: Large Language Models (LLMs) have showcased remarkable capabilities in following human instructions. However, recent studies have raised concerns about the robustness of LLMs when prompted with instructions combining textual adversarial samples. In this paper, drawing inspiration from recent works that LLMs are sensitive to the design of the instructions, we utilize instructions in code style, which are more structural and less ambiguous, to replace typically natural language instructions. Through this conversion, we provide LLMs with more precise instructions and strengthen the robustness of LLMs. Moreover, under few-shot scenarios, we propose a novel method to compose in-context demonstrations using both clean and adversarial samples (\\textit{adversarial context method}) to further boost the robustness of the LLMs. Experiments on eight robustness datasets show that our method consistently outperforms prompting LLMs with natural languag",
    "link": "https://arxiv.org/abs/2402.16431",
    "context": "Title: RoCoIns: Enhancing Robustness of Large Language Models through Code-Style Instructions\nAbstract: arXiv:2402.16431v1 Announce Type: new  Abstract: Large Language Models (LLMs) have showcased remarkable capabilities in following human instructions. However, recent studies have raised concerns about the robustness of LLMs when prompted with instructions combining textual adversarial samples. In this paper, drawing inspiration from recent works that LLMs are sensitive to the design of the instructions, we utilize instructions in code style, which are more structural and less ambiguous, to replace typically natural language instructions. Through this conversion, we provide LLMs with more precise instructions and strengthen the robustness of LLMs. Moreover, under few-shot scenarios, we propose a novel method to compose in-context demonstrations using both clean and adversarial samples (\\textit{adversarial context method}) to further boost the robustness of the LLMs. Experiments on eight robustness datasets show that our method consistently outperforms prompting LLMs with natural languag",
    "path": "papers/24/02/2402.16431.json",
    "total_tokens": 910,
    "translated_title": "RoCoIns: 通过代码风格指令增强大型语言模型的鲁棒性",
    "translated_abstract": "大型语言模型(LLMs)在遵循人类指令方面表现出卓越能力。然而，最近的研究引起了人们对LLMs在接受结合文本对抗样本的指令时鲁棒性的担忧。本文从最近的研究中获得启示，LLMs对指令设计敏感，我们利用代码风格的指令替换了通常的自然语言指令，这些指令更具结构性和 less模糊性。通过这种转换，我们为LLMs提供了更精确的指令，增强了LLMs的鲁棒性。此外，在少样本情景下，我们提出了一种新方法，利用干净和对抗样本来组成上下文演示（\\textit{对抗性上下文方法）来进一步增强LLMs的鲁棒性。在八个鲁棒性数据集上的实验表明，我们的方法始终优于使用自然语言提示LLMs的方法。",
    "tldr": "通过代码风格指令替换自然语言指令，提供更精确的指令并增强大型语言模型的鲁棒性。同时提出一种新方法，在少样本情景下通过组合干净和对抗样本加强模型的鲁棒性。",
    "en_tdlr": "Replacing natural language instructions with code-style instructions to provide more precise guidance and enhance the robustness of large language models. Introducing a novel method in few-shot scenarios to combine clean and adversarial samples to further boost model robustness."
}