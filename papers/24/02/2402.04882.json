{
    "title": "LMUFormer: Low Complexity Yet Powerful Spiking Model With Legendre Memory Units",
    "abstract": "Transformer models have demonstrated high accuracy in numerous applications but have high complexity and lack sequential processing capability making them ill-suited for many streaming applications at the edge where devices are heavily resource-constrained. Thus motivated, many researchers have proposed reformulating the transformer models as RNN modules which modify the self-attention computation with explicit states. However, these approaches often incur significant performance degradation. The ultimate goal is to develop a model that has the following properties: parallel training, streaming and low-cost inference, and SOTA performance. In this paper, we propose a new direction to achieve this goal. We show how architectural modifications to a recurrent model can help push its performance toward Transformer models while retaining its sequential processing capability. Specifically, inspired by the recent success of Legendre Memory Units (LMU) in sequence learning tasks, we propose LM",
    "link": "https://arxiv.org/abs/2402.04882",
    "context": "Title: LMUFormer: Low Complexity Yet Powerful Spiking Model With Legendre Memory Units\nAbstract: Transformer models have demonstrated high accuracy in numerous applications but have high complexity and lack sequential processing capability making them ill-suited for many streaming applications at the edge where devices are heavily resource-constrained. Thus motivated, many researchers have proposed reformulating the transformer models as RNN modules which modify the self-attention computation with explicit states. However, these approaches often incur significant performance degradation. The ultimate goal is to develop a model that has the following properties: parallel training, streaming and low-cost inference, and SOTA performance. In this paper, we propose a new direction to achieve this goal. We show how architectural modifications to a recurrent model can help push its performance toward Transformer models while retaining its sequential processing capability. Specifically, inspired by the recent success of Legendre Memory Units (LMU) in sequence learning tasks, we propose LM",
    "path": "papers/24/02/2402.04882.json",
    "total_tokens": 853,
    "translated_title": "LMUFormer：具有Legendre记忆单元的低复杂度但强大的脉冲模型",
    "translated_abstract": "Transformer模型在许多应用中表现出高准确性，但复杂度高且缺乏顺序处理能力，使其不适用于资源受限的许多边缘流应用。因此，许多研究人员提出将Transformer模型重新定义为具有显式状态的RNN模块，来修改自注意力计算。然而，这些方法往往会导致性能显著降低。本文旨在开发具有以下特性的模型：并行训练、流式处理和低成本推理，并具有SOTA性能。我们提出了一种新的方法来实现这个目标。我们展示了如何通过对循环模型进行架构修改，将其性能推向Transformer模型，同时保留其顺序处理能力。具体来说，我们受到了Legendre记忆单元（LMU）在序列学习任务中的最近成功启发，提出了LMUFormer模型。",
    "tldr": "本文提出了一种改进的脉冲模型LMUFormer，通过对循环模型进行架构修改，将其性能推向Transformer模型，同时保留其顺序处理能力，以实现并行训练、流式处理和低成本推理。",
    "en_tdlr": "This paper proposes an enhanced spiking model called LMUFormer, which modifies the architecture of recurrent models to achieve performance closer to Transformer models while retaining sequential processing capability. The goal is to enable parallel training, streaming, low-cost inference, and achieve state-of-the-art performance."
}