{
    "title": "Turn Waste into Worth: Rectifying Top-$k$ Router of MoE",
    "abstract": "arXiv:2402.12399v1 Announce Type: cross  Abstract: Sparse Mixture of Experts (MoE) models are popular for training large language models due to their computational efficiency. However, the commonly used top-$k$ routing mechanism suffers from redundancy computation and memory costs due to the unbalanced routing. Some experts are overflow, where the exceeding tokens are dropped. While some experts are vacant, which are padded with zeros, negatively impacting model performance. To address the dropped tokens and padding, we propose the Rectify-Router, comprising the Intra-GPU Rectification and the Fill-in Rectification. The Intra-GPU Rectification handles dropped tokens, efficiently routing them to experts within the GPU where they are located to avoid inter-GPU communication. The Fill-in Rectification addresses padding by replacing padding tokens with the tokens that have high routing scores. Our experimental results demonstrate that the Intra-GPU Rectification and the Fill-in Rectificati",
    "link": "https://arxiv.org/abs/2402.12399",
    "context": "Title: Turn Waste into Worth: Rectifying Top-$k$ Router of MoE\nAbstract: arXiv:2402.12399v1 Announce Type: cross  Abstract: Sparse Mixture of Experts (MoE) models are popular for training large language models due to their computational efficiency. However, the commonly used top-$k$ routing mechanism suffers from redundancy computation and memory costs due to the unbalanced routing. Some experts are overflow, where the exceeding tokens are dropped. While some experts are vacant, which are padded with zeros, negatively impacting model performance. To address the dropped tokens and padding, we propose the Rectify-Router, comprising the Intra-GPU Rectification and the Fill-in Rectification. The Intra-GPU Rectification handles dropped tokens, efficiently routing them to experts within the GPU where they are located to avoid inter-GPU communication. The Fill-in Rectification addresses padding by replacing padding tokens with the tokens that have high routing scores. Our experimental results demonstrate that the Intra-GPU Rectification and the Fill-in Rectificati",
    "path": "papers/24/02/2402.12399.json",
    "total_tokens": 867,
    "translated_title": "将废料变废为宝：矫正MoE的Top-k路由器",
    "translated_abstract": "稀疏混合专家（MoE）模型因其计算效率而受到欢迎，用于训练大型语言模型。然而，常用的Top-k路由机制由于不平衡的路由导致冗余计算和内存成本过高。一些专家会溢出，其中超出的令牌会被丢弃。而一些专家是空闲的，这些专家会填充为零，负面影响了模型性能。为了解决丢弃令牌和填充问题，我们提出了Rectify-Router，包括Intra-GPU矫正和Fill-in矫正。Intra-GPU矫正处理丢弃的令牌，将它们有效地路由到GPU内的专家，避免跨GPU通信。Fill-in矫正通过用具有高路由分数的令牌替换填充令牌来解决填充问题。我们的实验结果表明，Intra-GPU矫正和Fill-in矫正",
    "tldr": "提出了Rectify-Router解决了MoE模型中常用的Top-k路由机制所带来的令牌丢失和填充问题，通过Intra-GPU矫正和Fill-in矫正来实现。",
    "en_tdlr": "Introduced Rectify-Router to address token dropping and padding issues caused by the commonly used Top-k routing mechanism in MoE models, implemented through Intra-GPU rectification and Fill-in rectification."
}