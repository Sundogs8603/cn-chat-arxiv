{
    "title": "Investigating Bias Representations in Llama 2 Chat via Activation Steering",
    "abstract": "We address the challenge of societal bias in Large Language Models (LLMs), focusing on the Llama 2 7B Chat model. As LLMs are increasingly integrated into decision-making processes with substantial societal impact, it becomes imperative to ensure these models do not reinforce existing biases. Our approach employs activation steering to probe for and mitigate biases related to gender, race, and religion. This method manipulates model activations to direct responses towards or away from biased outputs, utilizing steering vectors derived from the StereoSet dataset and custom GPT4 generated gender bias prompts. Our findings reveal inherent gender bias in Llama 2 7B Chat, persisting even after Reinforcement Learning from Human Feedback (RLHF). We also observe a predictable negative correlation between bias and the model's tendency to refuse responses. Significantly, our study uncovers that RLHF tends to increase the similarity in the model's representation of different forms of societal bia",
    "link": "https://arxiv.org/abs/2402.00402",
    "context": "Title: Investigating Bias Representations in Llama 2 Chat via Activation Steering\nAbstract: We address the challenge of societal bias in Large Language Models (LLMs), focusing on the Llama 2 7B Chat model. As LLMs are increasingly integrated into decision-making processes with substantial societal impact, it becomes imperative to ensure these models do not reinforce existing biases. Our approach employs activation steering to probe for and mitigate biases related to gender, race, and religion. This method manipulates model activations to direct responses towards or away from biased outputs, utilizing steering vectors derived from the StereoSet dataset and custom GPT4 generated gender bias prompts. Our findings reveal inherent gender bias in Llama 2 7B Chat, persisting even after Reinforcement Learning from Human Feedback (RLHF). We also observe a predictable negative correlation between bias and the model's tendency to refuse responses. Significantly, our study uncovers that RLHF tends to increase the similarity in the model's representation of different forms of societal bia",
    "path": "papers/24/02/2402.00402.json",
    "total_tokens": 944,
    "translated_title": "通过激活导向研究 Llama 2 Chat 中的偏见表示",
    "translated_abstract": "我们解决了大型语言模型（LLMs）中的社会偏见挑战，重点关注 Llama 2 7B Chat 模型。随着 LLMs 被越来越多地整合到具有重大社会影响的决策过程中，确保这些模型不会强化现有的偏见变得至关重要。我们的方法采用激活导向技术来探测和减轻与性别、种族和宗教有关的偏见。该方法通过操纵模型的激活来指导回应朝向或远离有偏见的输出，利用从StereoSet数据集和自定义GPT4生成的性别偏见提示得到的导向向量。我们的研究结果揭示了Llama 2 7B Chat中固有的性别偏见，即使在通过人类反馈的强化学习之后仍然存在。我们还观察到偏见与模型拒绝回应的倾向之间存在可预测的负相关关系。值得注意的是，我们的研究揭示了强化学习从人类反馈中 tend 趋向增加模型对不同形式社会偏见的表示的相似性。",
    "tldr": "我们通过激活导向技术研究了Llama 2 Chat中的偏见表示问题，发现该模型存在固有的性别偏见，并观察到偏见与模型拒绝回应的倾向之间存在负相关关系。",
    "en_tdlr": "We investigated bias representations in Llama 2 Chat using activation steering. Our findings reveal inherent gender bias in the model, even after reinforcement learning. We also observed a negative correlation between bias and the model's tendency to refuse responses."
}