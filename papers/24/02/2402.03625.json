{
    "title": "Convex Relaxations of ReLU Neural Networks Approximate Global Optima in Polynomial Time",
    "abstract": "In this paper, we study the optimality gap between two-layer ReLU networks regularized with weight decay and their convex relaxations. We show that when the training data is random, the relative optimality gap between the original problem and its relaxation can be bounded by a factor of $O(\\sqrt{\\log n})$, where $n$ is the number of training samples. A simple application leads to a tractable polynomial-time algorithm that is guaranteed to solve the original non-convex problem up to a logarithmic factor. Moreover, under mild assumptions, we show that with random initialization on the parameters local gradient methods almost surely converge to a point that has low training loss. Our result is an exponential improvement compared to existing results and sheds new light on understanding why local gradient methods work well.",
    "link": "https://arxiv.org/abs/2402.03625",
    "context": "Title: Convex Relaxations of ReLU Neural Networks Approximate Global Optima in Polynomial Time\nAbstract: In this paper, we study the optimality gap between two-layer ReLU networks regularized with weight decay and their convex relaxations. We show that when the training data is random, the relative optimality gap between the original problem and its relaxation can be bounded by a factor of $O(\\sqrt{\\log n})$, where $n$ is the number of training samples. A simple application leads to a tractable polynomial-time algorithm that is guaranteed to solve the original non-convex problem up to a logarithmic factor. Moreover, under mild assumptions, we show that with random initialization on the parameters local gradient methods almost surely converge to a point that has low training loss. Our result is an exponential improvement compared to existing results and sheds new light on understanding why local gradient methods work well.",
    "path": "papers/24/02/2402.03625.json",
    "total_tokens": 837,
    "translated_title": "ReLU神经网络的凸松弛在多项式时间内逼近全局最优解",
    "translated_abstract": "本文研究了两层ReLU网络在加权衰减正则化下及其凸松弛之间的最优性差距。我们证明了当训练数据是随机的时候，原始问题与其凸松弛之间的相对最优性差距可以被一个$O(\\sqrt{\\log n})$的因子界限，其中$n$是训练样本的数量。一个简单的应用可以导出一个可行的多项式时间算法，该算法能够保证在对数因子范围内解决原始的非凸问题。此外，在温和的假设下，我们证明了在参数的随机初始化下，局部梯度方法几乎肯定会收敛到训练损失较低的点。我们的结果相对于现有结果而言是指数级的改进，并且揭示了为什么局部梯度方法表现良好的新见解。",
    "tldr": "本文研究了两层ReLU网络在加权衰减正则化下及其凸松弛之间的最优性差距，证明了当训练数据是随机的时候，相对最优性差距可以被一个$O(\\sqrt{\\log n})$的因子界限。此外，在温和的假设下，局部梯度方法几乎肯定会收敛到训练损失较低的点。"
}