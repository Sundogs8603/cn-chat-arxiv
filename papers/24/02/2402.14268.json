{
    "title": "Can Large Language Models Detect Misinformation in Scientific News Reporting?",
    "abstract": "arXiv:2402.14268v1 Announce Type: cross  Abstract: Scientific facts are often spun in the popular press with the intent to influence public opinion and action, as was evidenced during the COVID-19 pandemic. Automatic detection of misinformation in the scientific domain is challenging because of the distinct styles of writing in these two media types and is still in its nascence. Most research on the validity of scientific reporting treats this problem as a claim verification challenge. In doing so, significant expert human effort is required to generate appropriate claims. Our solution bypasses this step and addresses a more real-world scenario where such explicit, labeled claims may not be available. The central research question of this paper is whether it is possible to use large language models (LLMs) to detect misinformation in scientific reporting. To this end, we first present a new labeled dataset SciNews, containing 2.4k scientific news stories drawn from trusted and untrustwo",
    "link": "https://arxiv.org/abs/2402.14268",
    "context": "Title: Can Large Language Models Detect Misinformation in Scientific News Reporting?\nAbstract: arXiv:2402.14268v1 Announce Type: cross  Abstract: Scientific facts are often spun in the popular press with the intent to influence public opinion and action, as was evidenced during the COVID-19 pandemic. Automatic detection of misinformation in the scientific domain is challenging because of the distinct styles of writing in these two media types and is still in its nascence. Most research on the validity of scientific reporting treats this problem as a claim verification challenge. In doing so, significant expert human effort is required to generate appropriate claims. Our solution bypasses this step and addresses a more real-world scenario where such explicit, labeled claims may not be available. The central research question of this paper is whether it is possible to use large language models (LLMs) to detect misinformation in scientific reporting. To this end, we first present a new labeled dataset SciNews, containing 2.4k scientific news stories drawn from trusted and untrustwo",
    "path": "papers/24/02/2402.14268.json",
    "total_tokens": 718,
    "translated_title": "大型语言模型能够检测科学新闻报道中的错误信息吗？",
    "translated_abstract": "科学事实经常被在流行媒体中操纵，意图影响公众舆论和行动，正如在COVID-19大流行期间所证实的那样。在科学领域中自动检测错误信息具有挑战性，因为这两种媒体类型的写作风格有着明显不同，并且仍处于萌芽阶段。本文的核心研究问题是是否可以利用大型语言模型(LLMs)来检测科学报道中的错误信息。",
    "tldr": "大型语言模型探测科学报道中的错误信息的可行性，绕过生成明确标记索赔的步骤，处理现实场景中可能不存在明确标记索赔的挑战。",
    "en_tdlr": "The paper investigates the feasibility of using large language models to detect misinformation in scientific reporting by bypassing the step of generating explicitly labeled claims, addressing the challenge of scenarios where such claims may not be available."
}