{
    "title": "TexShape: Information Theoretic Sentence Embedding for Language Models",
    "abstract": "With the exponential growth in data volume and the emergence of data-intensive applications, particularly in the field of machine learning, concerns related to resource utilization, privacy, and fairness have become paramount. This paper focuses on the textual domain of data and addresses challenges regarding encoding sentences to their optimized representations through the lens of information-theory. In particular, we use empirical estimates of mutual information, using the Donsker-Varadhan definition of Kullback-Leibler divergence. Our approach leverages this estimation to train an information-theoretic sentence embedding, called TexShape, for (task-based) data compression or for filtering out sensitive information, enhancing privacy and fairness. In this study, we employ a benchmark language model for initial text representation, complemented by neural networks for information-theoretic compression and mutual information estimations. Our experiments demonstrate significant advanceme",
    "link": "https://arxiv.org/abs/2402.05132",
    "context": "Title: TexShape: Information Theoretic Sentence Embedding for Language Models\nAbstract: With the exponential growth in data volume and the emergence of data-intensive applications, particularly in the field of machine learning, concerns related to resource utilization, privacy, and fairness have become paramount. This paper focuses on the textual domain of data and addresses challenges regarding encoding sentences to their optimized representations through the lens of information-theory. In particular, we use empirical estimates of mutual information, using the Donsker-Varadhan definition of Kullback-Leibler divergence. Our approach leverages this estimation to train an information-theoretic sentence embedding, called TexShape, for (task-based) data compression or for filtering out sensitive information, enhancing privacy and fairness. In this study, we employ a benchmark language model for initial text representation, complemented by neural networks for information-theoretic compression and mutual information estimations. Our experiments demonstrate significant advanceme",
    "path": "papers/24/02/2402.05132.json",
    "total_tokens": 831,
    "translated_title": "TexShape:信息论句子嵌入用于语言模型",
    "translated_abstract": "随着数据量的指数增长和数据密集应用的出现，尤其是在机器学习领域，与资源利用、隐私和公平性相关的问题变得越来越重要。本文关注数据的文本领域，并通过信息论的视角解决了将句子编码为其优化表示的挑战。具体而言，我们使用Donsker-Varadhan定义的Kullback-Leibler散度的经验估计值来计算互信息。我们的方法利用这种估计来训练一种信息论句子嵌入模型，称为TexShape，用于（基于任务的）数据压缩或过滤敏感信息，从而增强隐私和公平性。在本研究中，我们使用基准语言模型进行初步文本表示，并用神经网络进行信息理论压缩和互信息估计。我们的实验表明，我们的方法显著进展。",
    "tldr": "这项研究提出了一种名为TexShape的信息论句子嵌入模型，通过使用互信息的经验估计来优化文本表示，可用于数据压缩和敏感信息过滤，提升隐私和公平性。",
    "en_tdlr": "This study introduces TexShape, an information-theoretic sentence embedding model, which optimizes text representation by using empirical estimates of mutual information. It can be used for data compression and filtering sensitive information, enhancing privacy and fairness."
}