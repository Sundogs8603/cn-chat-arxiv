{
    "title": "SubGen: Token Generation in Sublinear Time and Memory",
    "abstract": "Despite the significant success of large language models (LLMs), their extensive memory requirements pose challenges for deploying them in long-context token generation. The substantial memory footprint of LLM decoders arises from the necessity to store all previous tokens in the attention module, a requirement imposed by key-value (KV) caching. In this work, our focus is on developing an efficient compression technique for the KV cache. Empirical evidence indicates a significant clustering tendency within key embeddings in the attention module. Building on this key insight, we have devised a novel caching method with sublinear complexity, employing online clustering on key tokens and online $\\ell_2$ sampling on values. The result is a provably accurate and efficient attention decoding algorithm, termed SubGen. Not only does this algorithm ensure a sublinear memory footprint and sublinear time complexity, but we also establish a tight error bound for our approach. Empirical evaluations",
    "link": "https://arxiv.org/abs/2402.06082",
    "context": "Title: SubGen: Token Generation in Sublinear Time and Memory\nAbstract: Despite the significant success of large language models (LLMs), their extensive memory requirements pose challenges for deploying them in long-context token generation. The substantial memory footprint of LLM decoders arises from the necessity to store all previous tokens in the attention module, a requirement imposed by key-value (KV) caching. In this work, our focus is on developing an efficient compression technique for the KV cache. Empirical evidence indicates a significant clustering tendency within key embeddings in the attention module. Building on this key insight, we have devised a novel caching method with sublinear complexity, employing online clustering on key tokens and online $\\ell_2$ sampling on values. The result is a provably accurate and efficient attention decoding algorithm, termed SubGen. Not only does this algorithm ensure a sublinear memory footprint and sublinear time complexity, but we also establish a tight error bound for our approach. Empirical evaluations",
    "path": "papers/24/02/2402.06082.json",
    "total_tokens": 830,
    "translated_title": "SubGen：子线性时间和内存的令牌生成",
    "translated_abstract": "尽管大型语言模型（LLM）取得了显著的成功，但它们广泛的内存需求使得在长上下文令牌生成环境中部署它们存在挑战。LLM解码器的巨大内存占用量来自于在注意模块中存储所有先前令牌的必要性，这是由键-值（KV）缓存所强制的要求。在这项工作中，我们的重点是开发一种高效的键值缓存压缩技术。经验证据表明，在注意模块中的键嵌入中存在显著的聚类倾向。基于这一关键洞察，我们设计了一种具有子线性复杂度的新型缓存方法，采用键令牌的在线聚类和值的在线l2采样。结果是一个可以证明准确和高效的注意解码算法，称为SubGen。这个算法不仅确保了子线性内存占用和子线性时间复杂度，还为我们的方法建立了一个紧密的误差界。经验评估...",
    "tldr": "这项工作提出了一种名为SubGen的高效缓存压缩技术，通过在Attention模块中进行在线聚类和采样，实现了子线性的内存占用和时间复杂度，并建立了一个紧密的误差界。"
}