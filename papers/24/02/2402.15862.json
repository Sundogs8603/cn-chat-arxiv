{
    "title": "SportQA: A Benchmark for Sports Understanding in Large Language Models",
    "abstract": "arXiv:2402.15862v1 Announce Type: new  Abstract: A deep understanding of sports, a field rich in strategic and dynamic content, is crucial for advancing Natural Language Processing (NLP). This holds particular significance in the context of evaluating and advancing Large Language Models (LLMs), given the existing gap in specialized benchmarks. To bridge this gap, we introduce SportQA, a novel benchmark specifically designed for evaluating LLMs in the context of sports understanding. SportQA encompasses over 70,000 multiple-choice questions across three distinct difficulty levels, each targeting different aspects of sports knowledge from basic historical facts to intricate, scenario-based reasoning tasks. We conducted a thorough evaluation of prevalent LLMs, mainly utilizing few-shot learning paradigms supplemented by chain-of-thought (CoT) prompting. Our results reveal that while LLMs exhibit competent performance in basic sports knowledge, they struggle with more complex, scenario-bas",
    "link": "https://arxiv.org/abs/2402.15862",
    "context": "Title: SportQA: A Benchmark for Sports Understanding in Large Language Models\nAbstract: arXiv:2402.15862v1 Announce Type: new  Abstract: A deep understanding of sports, a field rich in strategic and dynamic content, is crucial for advancing Natural Language Processing (NLP). This holds particular significance in the context of evaluating and advancing Large Language Models (LLMs), given the existing gap in specialized benchmarks. To bridge this gap, we introduce SportQA, a novel benchmark specifically designed for evaluating LLMs in the context of sports understanding. SportQA encompasses over 70,000 multiple-choice questions across three distinct difficulty levels, each targeting different aspects of sports knowledge from basic historical facts to intricate, scenario-based reasoning tasks. We conducted a thorough evaluation of prevalent LLMs, mainly utilizing few-shot learning paradigms supplemented by chain-of-thought (CoT) prompting. Our results reveal that while LLMs exhibit competent performance in basic sports knowledge, they struggle with more complex, scenario-bas",
    "path": "papers/24/02/2402.15862.json",
    "total_tokens": 957,
    "translated_title": "SportQA：大型语言模型中体育理解的基准评估",
    "translated_abstract": "arXiv:2402.15862v1 报告类型：新的 摘要：对体育领域进行深入理解，这是一项充满战略和动态内容的领域，对于推动自然语言处理（NLP）至关重要。这在评估和推进大型语言模型（LLMs）的背景下尤为重要，鉴于现有专门基准测试之间存在差距。为了弥合这一差距，我们引入了SportQA，这是一个专门设计用于评估LLMs在体育理解方面的新型基准测试。SportQA涵盖了超过70,000个跨三个不同难度级别的多项选择题，每个级别针对体育知识的不同方面，从基本历史事实到复杂的基于情景的推理任务。我们主要利用少样本学习范式辅以“联想链”提示方法对普遍的LLMs进行了彻底评估。我们的结果显示，虽然LLMs在基本的体育知识方面表现出色，但在更复杂的基于情景的推理方面却遇到了困难。",
    "tldr": "SportQA是一个新的基准测试，旨在评估大型语言模型在体育理解方面的表现，包含超过70,000个问题涵盖不同难度级别的体育知识，并揭示了LLMs在基本体育知识上表现优异但在复杂情境推理方面存在挑战。",
    "en_tdlr": "SportQA is a new benchmark designed to evaluate the performance of Large Language Models in sports understanding, containing over 70,000 questions covering different difficulty levels of sports knowledge, and revealing that while LLMs excel in basic sports knowledge, they face challenges in complex scenario-based reasoning."
}