{
    "title": "ArabicMMLU: Assessing Massive Multitask Language Understanding in Arabic",
    "abstract": "arXiv:2402.12840v1 Announce Type: new  Abstract: The focus of language model evaluation has transitioned towards reasoning and knowledge-intensive tasks, driven by advancements in pretraining large models. While state-of-the-art models are partially trained on large Arabic texts, evaluating their performance in Arabic remains challenging due to the limited availability of relevant datasets. To bridge this gap, we present ArabicMMLU, the first multi-task language understanding benchmark for Arabic language, sourced from school exams across diverse educational levels in different countries spanning North Africa, the Levant, and the Gulf regions. Our data comprises 40 tasks and 14,575 multiple-choice questions in Modern Standard Arabic (MSA), and is carefully constructed by collaborating with native speakers in the region. Our comprehensive evaluations of 35 models reveal substantial room for improvement, particularly among the best open-source models. Notably, BLOOMZ, mT0, LLama2, and Fa",
    "link": "https://arxiv.org/abs/2402.12840",
    "context": "Title: ArabicMMLU: Assessing Massive Multitask Language Understanding in Arabic\nAbstract: arXiv:2402.12840v1 Announce Type: new  Abstract: The focus of language model evaluation has transitioned towards reasoning and knowledge-intensive tasks, driven by advancements in pretraining large models. While state-of-the-art models are partially trained on large Arabic texts, evaluating their performance in Arabic remains challenging due to the limited availability of relevant datasets. To bridge this gap, we present ArabicMMLU, the first multi-task language understanding benchmark for Arabic language, sourced from school exams across diverse educational levels in different countries spanning North Africa, the Levant, and the Gulf regions. Our data comprises 40 tasks and 14,575 multiple-choice questions in Modern Standard Arabic (MSA), and is carefully constructed by collaborating with native speakers in the region. Our comprehensive evaluations of 35 models reveal substantial room for improvement, particularly among the best open-source models. Notably, BLOOMZ, mT0, LLama2, and Fa",
    "path": "papers/24/02/2402.12840.json",
    "total_tokens": 886,
    "translated_title": "ArabicMMLU：评估阿拉伯语中的大规模多任务语言理解",
    "translated_abstract": "语言模型评估的重点已经转向推理和知识密集型任务，这得益于预训练大型模型的进展。尽管最先进的模型部分在大量阿拉伯文本上进行了训练，但由于相关数据集的有限可用性，评估它们在阿拉伯语中的性能仍然具有挑战性。为了弥合这一差距，我们提出了ArabicMMLU，这是第一个针对阿拉伯语言的多任务语言理解基准测试，其数据来自于跨越北非、黎凡特和海湾地区不同国家教育水平的学校考试。我们的数据包括40个任务和14,575个现代标准阿拉伯语（MSA）的多项选择题，通过与该地区的母语者合作精心构建。我们对35个模型的全面评估显示出相当大的改进空间，特别是在最好的开源模型中。值得注意的是BLOOMZ、mT0、LLama2和Fa。",
    "tldr": "ArabicMMLU是针对阿拉伯语的第一个多任务语言理解基准测试，通过学校考试中收集的数据对35个模型进行全面评估，揭示了在阿拉伯语中性能改进的潜力。",
    "en_tdlr": "ArabicMMLU is the first multitask language understanding benchmark for Arabic, evaluating 35 models using data from school exams and revealing the potential for performance improvement in Arabic."
}