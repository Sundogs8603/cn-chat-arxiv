{
    "title": "Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents",
    "abstract": "arXiv:2402.11651v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved success in acting as agents, which interact with environments through tools like search engines. However, LLMs are not optimized specifically for tool use during training or alignment, limiting their effectiveness as agents. To resolve this problem, previous work has collected interaction trajectories between GPT-4 and environments, and fine-tuned smaller models with them. As part of this, the standard approach has been to simply discard trajectories that do not finish the task successfully, which, on the one hand, leads to a significant waste of data and resources, and on the other hand, has the potential to limit the possible optimization paths during fine-tuning. In this paper, we contend that large language models can learn from failures through appropriate data cleaning and fine-tuning strategies. We conduct experiments on mathematical reasoning, multi-hop question answering, and strategic ",
    "link": "https://arxiv.org/abs/2402.11651",
    "context": "Title: Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents\nAbstract: arXiv:2402.11651v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved success in acting as agents, which interact with environments through tools like search engines. However, LLMs are not optimized specifically for tool use during training or alignment, limiting their effectiveness as agents. To resolve this problem, previous work has collected interaction trajectories between GPT-4 and environments, and fine-tuned smaller models with them. As part of this, the standard approach has been to simply discard trajectories that do not finish the task successfully, which, on the one hand, leads to a significant waste of data and resources, and on the other hand, has the potential to limit the possible optimization paths during fine-tuning. In this paper, we contend that large language models can learn from failures through appropriate data cleaning and fine-tuning strategies. We conduct experiments on mathematical reasoning, multi-hop question answering, and strategic ",
    "path": "papers/24/02/2402.11651.json",
    "total_tokens": 821,
    "translated_title": "从失败中学习：在对大型语言模型进行微调时整合负面示例",
    "translated_abstract": "大型语言模型(LLMs)在充当与环境进行交互的工具（如搜索引擎）时取得了成功。然而，LLMs在训练或对齐过程中并未专门针对工具使用进行优化，限制了它们作为代理的效果。为解决这一问题，之前的研究已经收集了GPT-4与环境之间的交互轨迹，并用它们对较小的模型进行微调。作为这一过程的一部分，标准方法通常是简单地丢弃未成功完成任务的轨迹，这一方面导致了数据和资源的显著浪费，另一方面有可能限制微调过程中的优化路径。本文认为大型语言模型可以通过适当的数据清洗和微调策略从失败中学习。我们在数学推理、多跳问题回答和战略方面进行了实验。",
    "tldr": "大型语言模型通过整合负面示例和适当的数据清洗与微调策略，从失败中学习，提高作为代理的效果。",
    "en_tdlr": "Large language models learn from failures by integrating negative examples and using appropriate data cleaning and fine-tuning strategies to improve their effectiveness as agents."
}