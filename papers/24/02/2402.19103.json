{
    "title": "Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models",
    "abstract": "arXiv:2402.19103v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown impressive capabilities but still suffer from the issue of hallucinations. A significant type of this issue is the false premise hallucination, which we define as the phenomenon when LLMs generate hallucinated text when confronted with false premise questions. In this paper, we perform a comprehensive analysis of the false premise hallucination and elucidate its internal working mechanism: a small subset of attention heads (which we designate as false premise heads) disturb the knowledge extraction process, leading to the occurrence of false premise hallucination. Based on our analysis, we propose \\textbf{FAITH} (\\textbf{F}alse premise \\textbf{A}ttention head constra\\textbf{I}ining for mi\\textbf{T}igating \\textbf{H}allucinations), a novel and effective method to mitigate false premise hallucinations. It constrains the false premise attention heads during the model inference process. Impressively,",
    "link": "https://arxiv.org/abs/2402.19103",
    "context": "Title: Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models\nAbstract: arXiv:2402.19103v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown impressive capabilities but still suffer from the issue of hallucinations. A significant type of this issue is the false premise hallucination, which we define as the phenomenon when LLMs generate hallucinated text when confronted with false premise questions. In this paper, we perform a comprehensive analysis of the false premise hallucination and elucidate its internal working mechanism: a small subset of attention heads (which we designate as false premise heads) disturb the knowledge extraction process, leading to the occurrence of false premise hallucination. Based on our analysis, we propose \\textbf{FAITH} (\\textbf{F}alse premise \\textbf{A}ttention head constra\\textbf{I}ining for mi\\textbf{T}igating \\textbf{H}allucinations), a novel and effective method to mitigate false premise hallucinations. It constrains the false premise attention heads during the model inference process. Impressively,",
    "path": "papers/24/02/2402.19103.json",
    "total_tokens": 873,
    "translated_title": "震撼基础的细语：分析和减轻大型语言模型中的虚假前提幻觉",
    "translated_abstract": "大型语言模型(LLMs)展现出令人印象深刻的能力，但仍然受到幻觉问题的困扰。这个问题的一个重要类型是虚假前提幻觉，我们定义为当LLMs面对虚假前提问题时生成幻觉文本的现象。本文对虚假前提幻觉进行了全面分析，并阐明了其内部工作机制：一小部分注意力头(我们将其指定为虚假前提头)扰乱了知识提取过程，导致虚假前提幻觉的发生。基于我们的分析，我们提出了“FAITH”(虚假前提注意力头约束以减轻幻觉)这一新颖有效的方法来减轻虚假前提幻觉。它在模型推理过程中约束虚假前提注意力头。令人印象深刻的是，",
    "tldr": "该论文对大型语言模型中的虚假前提幻觉进行了全面分析，提出了一种名为“FAITH”的方法，用于减轻虚假前提幻觉。",
    "en_tdlr": "The paper provides a comprehensive analysis of false premise hallucinations in large language models, and proposes a method named \"FAITH\" for mitigating such hallucinations."
}