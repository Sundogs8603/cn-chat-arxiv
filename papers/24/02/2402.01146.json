{
    "title": "Limited Memory Online Gradient Descent for Kernelized Pairwise Learning with Dynamic Averaging",
    "abstract": "Pairwise learning, an important domain within machine learning, addresses loss functions defined on pairs of training examples, including those in metric learning and AUC maximization. Acknowledging the quadratic growth in computation complexity accompanying pairwise loss as the sample size grows, researchers have turned to online gradient descent (OGD) methods for enhanced scalability. Recently, an OGD algorithm emerged, employing gradient computation involving prior and most recent examples, a step that effectively reduces algorithmic complexity to $O(T)$, with $T$ being the number of received examples. This approach, however, confines itself to linear models while assuming the independence of example arrivals. We introduce a lightweight OGD algorithm that does not require the independence of examples and generalizes to kernel pairwise learning. Our algorithm builds the gradient based on a random example and a moving average representing the past data, which results in a sub-linear r",
    "link": "https://rss.arxiv.org/abs/2402.01146",
    "context": "Title: Limited Memory Online Gradient Descent for Kernelized Pairwise Learning with Dynamic Averaging\nAbstract: Pairwise learning, an important domain within machine learning, addresses loss functions defined on pairs of training examples, including those in metric learning and AUC maximization. Acknowledging the quadratic growth in computation complexity accompanying pairwise loss as the sample size grows, researchers have turned to online gradient descent (OGD) methods for enhanced scalability. Recently, an OGD algorithm emerged, employing gradient computation involving prior and most recent examples, a step that effectively reduces algorithmic complexity to $O(T)$, with $T$ being the number of received examples. This approach, however, confines itself to linear models while assuming the independence of example arrivals. We introduce a lightweight OGD algorithm that does not require the independence of examples and generalizes to kernel pairwise learning. Our algorithm builds the gradient based on a random example and a moving average representing the past data, which results in a sub-linear r",
    "path": "papers/24/02/2402.01146.json",
    "total_tokens": 867,
    "translated_title": "基于动态平均的核化配对学习的有限内存在线梯度下降算法",
    "translated_abstract": "配对学习是机器学习中的重要领域，处理的是基于训练示例对定义的损失函数，包括度量学习和AUC最大化等。为了提高可扩展性，研究人员采用在线梯度下降（OGD）方法来解决随着样本大小增长而带来的计算复杂度的二次增长问题。最近，提出了一种OGD算法，通过在先前和最近的示例上进行梯度计算，将算法复杂度有效降低到$O(T)$，其中$T$是接收到的示例数。然而，这种方法仅限于线性模型，同时假设示例的到达是独立的。我们提出了一种轻量级的OGD算法，不需要示例的独立性，并且适用于核化配对学习。我们的算法基于一个随机示例和代表过去数据的移动平均数构建梯度，从而实现了亚线性的运行时间复杂度。",
    "tldr": "本文提出了一种基于动态平均的核化配对学习的有限内存在线梯度下降算法，解决了配对学习中计算复杂度增长问题，并且不需要示例的独立性。",
    "en_tdlr": "This paper introduces a limited memory online gradient descent algorithm for kernelized pairwise learning, addressing the issue of quadratic growth in computation complexity and not requiring independence of examples."
}