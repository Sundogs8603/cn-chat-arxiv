{
    "title": "A hierarchical decomposition for explaining ML performance discrepancies",
    "abstract": "arXiv:2402.14254v1 Announce Type: new  Abstract: Machine learning (ML) algorithms can often differ in performance across domains. Understanding $\\textit{why}$ their performance differs is crucial for determining what types of interventions (e.g., algorithmic or operational) are most effective at closing the performance gaps. Existing methods focus on $\\textit{aggregate decompositions}$ of the total performance gap into the impact of a shift in the distribution of features $p(X)$ versus the impact of a shift in the conditional distribution of the outcome $p(Y|X)$; however, such coarse explanations offer only a few options for how one can close the performance gap. $\\textit{Detailed variable-level decompositions}$ that quantify the importance of each variable to each term in the aggregate decomposition can provide a much deeper understanding and suggest much more targeted interventions. However, existing methods assume knowledge of the full causal graph or make strong parametric assumpti",
    "link": "https://arxiv.org/abs/2402.14254",
    "context": "Title: A hierarchical decomposition for explaining ML performance discrepancies\nAbstract: arXiv:2402.14254v1 Announce Type: new  Abstract: Machine learning (ML) algorithms can often differ in performance across domains. Understanding $\\textit{why}$ their performance differs is crucial for determining what types of interventions (e.g., algorithmic or operational) are most effective at closing the performance gaps. Existing methods focus on $\\textit{aggregate decompositions}$ of the total performance gap into the impact of a shift in the distribution of features $p(X)$ versus the impact of a shift in the conditional distribution of the outcome $p(Y|X)$; however, such coarse explanations offer only a few options for how one can close the performance gap. $\\textit{Detailed variable-level decompositions}$ that quantify the importance of each variable to each term in the aggregate decomposition can provide a much deeper understanding and suggest much more targeted interventions. However, existing methods assume knowledge of the full causal graph or make strong parametric assumpti",
    "path": "papers/24/02/2402.14254.json",
    "total_tokens": 811,
    "translated_title": "解释机器学习性能差异的分层分解方法",
    "translated_abstract": "机器学习（ML）算法在不同领域的性能往往有所不同。了解它们的性能差异的原因对于确定何种干预措施（例如算法或运营）最有效以缩小性能差距至关重要。现有方法侧重于将总性能差异分解为特征分布$p(X)$变化的影响与结果条件分布$p(Y|X)$变化的影响的$\\textit{汇总分解}$；然而，这样粗糙的解释只提供了很少的方法来缩小性能差距。$\\textit{详细的变量级分解}$可以量化每个变量对汇总分解中每个项的重要性，从而提供更深入的理解，并提出更有针对性的干预措施。然而，现有方法假设有关全因果图的完整知识或进行强参数假设。",
    "tldr": "提出了一种详细的变量级分解方法，可以量化每个变量对性能差异的影响，为实现有针对性干预措施提供更深入的理解"
}