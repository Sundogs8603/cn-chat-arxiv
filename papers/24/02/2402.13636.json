{
    "title": "A Unified Framework and Dataset for Assessing Gender Bias in Vision-Language Models",
    "abstract": "arXiv:2402.13636v1 Announce Type: cross  Abstract: Large vision-language models (VLMs) are widely getting adopted in industry and academia. In this work we build a unified framework to systematically evaluate gender-profession bias in VLMs. Our evaluation encompasses all supported inference modes of the recent VLMs, including image-to-text, text-to-text, text-to-image, and image-to-image. We construct a synthetic, high-quality dataset of text and images that blurs gender distinctions across professional actions to benchmark gender bias. In our benchmarking of recent vision-language models (VLMs), we observe that different input-output modalities result in distinct bias magnitudes and directions. We hope our work will help guide future progress in improving VLMs to learn socially unbiased representations. We will release our data and code.",
    "link": "https://arxiv.org/abs/2402.13636",
    "context": "Title: A Unified Framework and Dataset for Assessing Gender Bias in Vision-Language Models\nAbstract: arXiv:2402.13636v1 Announce Type: cross  Abstract: Large vision-language models (VLMs) are widely getting adopted in industry and academia. In this work we build a unified framework to systematically evaluate gender-profession bias in VLMs. Our evaluation encompasses all supported inference modes of the recent VLMs, including image-to-text, text-to-text, text-to-image, and image-to-image. We construct a synthetic, high-quality dataset of text and images that blurs gender distinctions across professional actions to benchmark gender bias. In our benchmarking of recent vision-language models (VLMs), we observe that different input-output modalities result in distinct bias magnitudes and directions. We hope our work will help guide future progress in improving VLMs to learn socially unbiased representations. We will release our data and code.",
    "path": "papers/24/02/2402.13636.json",
    "total_tokens": 807,
    "translated_title": "用于评估视觉-语言模型中性别偏见的统一框架和数据集",
    "translated_abstract": "大型视觉-语言模型（VLMs）广泛应用于工业和学术界。本研究构建了一个统一框架，系统评估VLMs中的性别-职业偏见。我们的评估涵盖了最近VLMs支持的所有推断模式，包括图像到文本、文本到文本、文本到图像和图像到图像。我们构建了一个合成的、高质量的文本和图像数据集，模糊了职业动作中的性别差异，以评估性别偏见。在我们对最近的视觉-语言模型（VLMs）进行基准测试时，我们观察到不同的输入-输出模式会导致不同的偏见大小和方向。我们希望我们的工作能够指导未来改进VLMs以学习社会无偏见表示。我们将发布我们的数据和代码。",
    "tldr": "构建了一个统一框架和数据集，用于评估视觉-语言模型中的性别偏见，并观察到不同的输入-输出模式导致不同的偏见大小和方向。",
    "en_tdlr": "A unified framework and dataset were built to assess gender bias in vision-language models, revealing varying bias magnitudes and directions across different input-output modalities."
}