{
    "title": "Replicability is Asymptotically Free in Multi-armed Bandits",
    "abstract": "This work is motivated by the growing demand for reproducible machine learning. We study the stochastic multi-armed bandit problem. In particular, we consider a replicable algorithm that ensures, with high probability, that the algorithm's sequence of actions is not affected by the randomness inherent in the dataset. We observe that existing algorithms require $O(1/\\rho^2)$ times more regret than nonreplicable algorithms, where $\\rho$ is the level of nonreplication. However, we demonstrate that this additional cost is unnecessary when the time horizon $T$ is sufficiently large for a given $\\rho$, provided that the magnitude of the confidence bounds is chosen carefully. We introduce an explore-then-commit algorithm that draws arms uniformly before committing to a single arm. Additionally, we examine a successive elimination algorithm that eliminates suboptimal arms at the end of each phase. To ensure the replicability of these algorithms, we incorporate randomness into their decision-ma",
    "link": "https://arxiv.org/abs/2402.07391",
    "context": "Title: Replicability is Asymptotically Free in Multi-armed Bandits\nAbstract: This work is motivated by the growing demand for reproducible machine learning. We study the stochastic multi-armed bandit problem. In particular, we consider a replicable algorithm that ensures, with high probability, that the algorithm's sequence of actions is not affected by the randomness inherent in the dataset. We observe that existing algorithms require $O(1/\\rho^2)$ times more regret than nonreplicable algorithms, where $\\rho$ is the level of nonreplication. However, we demonstrate that this additional cost is unnecessary when the time horizon $T$ is sufficiently large for a given $\\rho$, provided that the magnitude of the confidence bounds is chosen carefully. We introduce an explore-then-commit algorithm that draws arms uniformly before committing to a single arm. Additionally, we examine a successive elimination algorithm that eliminates suboptimal arms at the end of each phase. To ensure the replicability of these algorithms, we incorporate randomness into their decision-ma",
    "path": "papers/24/02/2402.07391.json",
    "total_tokens": 900,
    "translated_title": "在多臂赌博机中，可复制性渐进自由",
    "translated_abstract": "本研究受可复制的机器学习需求的推动，研究了随机多臂赌博机问题。特别地，我们考虑了一个可复制算法，确保算法的操作序列不受数据集固有随机性的影响。我们观察到，现有算法所需的遗憾值比不可复制算法多$O(1/\\rho^2)$倍，其中$\\rho$是非复制程度。然而，我们证明了当给定的$\\rho$下时间界$T$足够大时，此额外代价是不必要的，前提是谨慎选择置信区间的幅度。我们引入了一个先探索后决策的算法，在决策之前均匀选择动作。此外，我们还研究了一个连续淘汰算法，在每个阶段结束时淘汰次优动作。为了确保这些算法的可复制性，我们将随机性引入决策制定中。",
    "tldr": "本论文研究在多臂赌博机问题中，通过引入探索-再确定算法和连续淘汰算法，以及谨慎选择置信区间的幅度，实现了可复制性，并证明了当时间界足够大时，可复制算法的额外代价是不必要的。",
    "en_tdlr": "This paper studies the problem of replicability in multi-armed bandits and demonstrates that with careful selection of confidence bounds and the introduction of explore-then-commit and successive elimination algorithms, replicability can be achieved without incurring additional costs when the time horizon is sufficiently large."
}