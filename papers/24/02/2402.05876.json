{
    "title": "Federated Offline Reinforcement Learning: Collaborative Single-Policy Coverage Suffices",
    "abstract": "Offline reinforcement learning (RL), which seeks to learn an optimal policy using offline data, has garnered significant interest due to its potential in critical applications where online data collection is infeasible or expensive. This work explores the benefit of federated learning for offline RL, aiming at collaboratively leveraging offline datasets at multiple agents. Focusing on finite-horizon episodic tabular Markov decision processes (MDPs), we design FedLCB-Q, a variant of the popular model-free Q-learning algorithm tailored for federated offline RL. FedLCB-Q updates local Q-functions at agents with novel learning rate schedules and aggregates them at a central server using importance averaging and a carefully designed pessimistic penalty term. Our sample complexity analysis reveals that, with appropriately chosen parameters and synchronization schedules, FedLCB-Q achieves linear speedup in terms of the number of agents without requiring high-quality datasets at individual age",
    "link": "https://arxiv.org/abs/2402.05876",
    "context": "Title: Federated Offline Reinforcement Learning: Collaborative Single-Policy Coverage Suffices\nAbstract: Offline reinforcement learning (RL), which seeks to learn an optimal policy using offline data, has garnered significant interest due to its potential in critical applications where online data collection is infeasible or expensive. This work explores the benefit of federated learning for offline RL, aiming at collaboratively leveraging offline datasets at multiple agents. Focusing on finite-horizon episodic tabular Markov decision processes (MDPs), we design FedLCB-Q, a variant of the popular model-free Q-learning algorithm tailored for federated offline RL. FedLCB-Q updates local Q-functions at agents with novel learning rate schedules and aggregates them at a central server using importance averaging and a carefully designed pessimistic penalty term. Our sample complexity analysis reveals that, with appropriately chosen parameters and synchronization schedules, FedLCB-Q achieves linear speedup in terms of the number of agents without requiring high-quality datasets at individual age",
    "path": "papers/24/02/2402.05876.json",
    "total_tokens": 929,
    "translated_title": "联邦离线强化学习：合作单一策略即可覆盖",
    "translated_abstract": "离线强化学习通过使用离线数据来学习最优策略，在无法在线收集数据或成本高昂的关键应用中引起了广泛关注。本研究探讨了联邦学习在离线强化学习中的好处，并旨在合作利用多个代理的离线数据集。针对有限时段的表格化马尔可夫决策过程(MDP)，我们设计了FedLCB-Q，这是一种针对联邦离线强化学习量身定制的流行的无模型Q-learning算法的变种。FedLCB-Q使用新颖的学习速率调度在代理处更新本地Q函数，并使用重要性平均和精心设计的悲观惩罚项在中央服务器上聚合它们。我们的样本复杂度分析表明，在适当选择的参数和同步时间表下，FedLCB-Q在代理数量上实现了线性加速，而不需要个别代理拥有高质量的数据集。",
    "tldr": "本研究探索了联邦学习在离线强化学习中的应用，设计了一种适用于联邦离线强化学习的无模型Q-learning算法FedLCB-Q。通过合作利用多个代理的离线数据集，并使用特定的学习速率调度和聚合方法，FedLCB-Q实现了线性加速。"
}