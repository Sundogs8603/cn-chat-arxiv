{
    "title": "Inverse Reinforcement Learning by Estimating Expertise of Demonstrators",
    "abstract": "In Imitation Learning (IL), utilizing suboptimal and heterogeneous demonstrations presents a substantial challenge due to the varied nature of real-world data. However, standard IL algorithms consider these datasets as homogeneous, thereby inheriting the deficiencies of suboptimal demonstrators. Previous approaches to this issue typically rely on impractical assumptions like high-quality data subsets, confidence rankings, or explicit environmental knowledge. This paper introduces IRLEED, Inverse Reinforcement Learning by Estimating Expertise of Demonstrators, a novel framework that overcomes these hurdles without prior knowledge of demonstrator expertise. IRLEED enhances existing Inverse Reinforcement Learning (IRL) algorithms by combining a general model for demonstrator suboptimality to address reward bias and action variance, with a Maximum Entropy IRL framework to efficiently derive the optimal policy from diverse, suboptimal demonstrations. Experiments in both online and offline I",
    "link": "https://arxiv.org/abs/2402.01886",
    "context": "Title: Inverse Reinforcement Learning by Estimating Expertise of Demonstrators\nAbstract: In Imitation Learning (IL), utilizing suboptimal and heterogeneous demonstrations presents a substantial challenge due to the varied nature of real-world data. However, standard IL algorithms consider these datasets as homogeneous, thereby inheriting the deficiencies of suboptimal demonstrators. Previous approaches to this issue typically rely on impractical assumptions like high-quality data subsets, confidence rankings, or explicit environmental knowledge. This paper introduces IRLEED, Inverse Reinforcement Learning by Estimating Expertise of Demonstrators, a novel framework that overcomes these hurdles without prior knowledge of demonstrator expertise. IRLEED enhances existing Inverse Reinforcement Learning (IRL) algorithms by combining a general model for demonstrator suboptimality to address reward bias and action variance, with a Maximum Entropy IRL framework to efficiently derive the optimal policy from diverse, suboptimal demonstrations. Experiments in both online and offline I",
    "path": "papers/24/02/2402.01886.json",
    "total_tokens": 938,
    "translated_title": "通过估计演示者的专业知识的逆向强化学习",
    "translated_abstract": "在模仿学习中，利用次优和异质的演示提出了一个重大挑战，因为现实世界数据的性质各不相同。然而，标准的模仿学习算法将这些数据集视为同质的，从而继承了次优演示的缺陷。先前处理这个问题的方法通常依赖于不切实际的假设，如高质量的数据子集、置信度排名或明确的环境知识。本文介绍了IRLEED（通过估计演示者的专业知识的逆向强化学习），这是一个新颖的框架，能够克服这些障碍，而不需要先前对演示者专业知识进行了解。IRLEED通过将演示者次优性的普适模型与最大熵IRL框架相结合，来处理奖励偏差和行动方差，从而有效地从多样的次优演示中得出最优策略。在在线和离线实验中进行了验证。",
    "tldr": "本文介绍了一个新颖的框架，IRLEED，它通过估计演示者的专业知识来解决模仿学习中的次优和异质演示的问题。IRLEED通过结合演示者次优性的普适模型和最大熵IRL框架，有效地从多样的次优演示中得出最佳策略。",
    "en_tdlr": "This paper introduces a novel framework, IRLEED, that addresses the issue of utilizing suboptimal and heterogeneous demonstrations in imitation learning. IRLEED combines a general model for demonstrator suboptimality with a Maximum Entropy IRL framework to efficiently derive the optimal policy from diverse, suboptimal demonstrations."
}