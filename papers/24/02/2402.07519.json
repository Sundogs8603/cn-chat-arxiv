{
    "title": "MAFIA: Multi-Adapter Fused Inclusive LanguAge Models",
    "abstract": "Pretrained Language Models (PLMs) are widely used in NLP for various tasks. Recent studies have identified various biases that such models exhibit and have proposed methods to correct these biases. However, most of the works address a limited set of bias dimensions independently such as gender, race, or religion. Moreover, the methods typically involve finetuning the full model to maintain the performance on the downstream task. In this work, we aim to modularly debias a pretrained language model across multiple dimensions. Previous works extensively explored debiasing PLMs using limited US-centric counterfactual data augmentation (CDA). We use structured knowledge and a large generative model to build a diverse CDA across multiple bias dimensions in a semi-automated way. We highlight how existing debiasing methods do not consider interactions between multiple societal biases and propose a debiasing model that exploits the synergy amongst various societal biases and enables multi-bias ",
    "link": "https://arxiv.org/abs/2402.07519",
    "context": "Title: MAFIA: Multi-Adapter Fused Inclusive LanguAge Models\nAbstract: Pretrained Language Models (PLMs) are widely used in NLP for various tasks. Recent studies have identified various biases that such models exhibit and have proposed methods to correct these biases. However, most of the works address a limited set of bias dimensions independently such as gender, race, or religion. Moreover, the methods typically involve finetuning the full model to maintain the performance on the downstream task. In this work, we aim to modularly debias a pretrained language model across multiple dimensions. Previous works extensively explored debiasing PLMs using limited US-centric counterfactual data augmentation (CDA). We use structured knowledge and a large generative model to build a diverse CDA across multiple bias dimensions in a semi-automated way. We highlight how existing debiasing methods do not consider interactions between multiple societal biases and propose a debiasing model that exploits the synergy amongst various societal biases and enables multi-bias ",
    "path": "papers/24/02/2402.07519.json",
    "total_tokens": 980,
    "translated_title": "MAFIA: 多适配器融合的包容性语言模型",
    "translated_abstract": "预训练语言模型（PLMs）被广泛应用于自然语言处理的各种任务中。最近的研究发现这些模型存在各种偏见，并提出方法来纠正这些偏见。然而，大多数工作仅独立地解决了有限的偏见维度，如性别、种族或宗教。此外，这些方法通常需要对整个模型进行微调以保持在下游任务上的性能。在本文中，我们旨在在多个维度上模块化地去偏倚预训练语言模型。先前的工作已经广泛探索了使用有限的美国中心的反事实数据增强（CDA）来去偏倚PLMs。我们使用结构化知识和大规模生成模型来以半自动化的方式构建多个偏见维度上的多样化CDA。我们强调现有的去偏倚方法不考虑多个社会偏见之间的相互作用，并提出了一种能够利用各种社会偏见之间的协同效应并实现多重偏见的去偏倚模型。",
    "tldr": "本文提出了一种名为MAFIA的多适配器融合的包容性语言模型，在多个偏见维度上进行模块化去偏倚，利用结构化知识和大规模生成模型构建了多样化的反事实数据增强，并强调了现有去偏倚方法对多个社会偏见之间的相互作用缺乏考虑。",
    "en_tdlr": "This paper proposes a multi-adapter fused inclusive language model called MAFIA, which modularly debiases a pretrained language model across multiple bias dimensions. It utilizes structured knowledge and a large generative model to build diverse counterfactual data augmentation and highlights the lack of consideration of interactions between multiple societal biases in existing debiasing methods."
}