{
    "title": "The I/O Complexity of Attention, or How Optimal is Flash Attention?",
    "abstract": "Self-attention is at the heart of the popular Transformer architecture, yet suffers from quadratic time and memory complexity. The breakthrough FlashAttention algorithm revealed I/O complexity as the true bottleneck in scaling Transformers. Given two levels of memory hierarchy, a fast cache (e.g. GPU on-chip SRAM) and a slow memory (e.g. GPU high-bandwidth memory), the I/O complexity measures the number of accesses to memory. FlashAttention computes attention using $\\frac{N^2d^2}{M}$ I/O operations where $N$ is the dimension of the attention matrix, $d$ the head-dimension and $M$ the cache size. However, is this I/O complexity optimal? The known lower bound only rules out an I/O complexity of $o(Nd)$ when $M=\\Theta(Nd)$, since the output that needs to be written to slow memory is $\\Omega(Nd)$. This leads to the main question of our work: Is FlashAttention I/O optimal for all values of $M$?   We resolve the above question in its full generality by showing an I/O complexity lower bound t",
    "link": "https://arxiv.org/abs/2402.07443",
    "context": "Title: The I/O Complexity of Attention, or How Optimal is Flash Attention?\nAbstract: Self-attention is at the heart of the popular Transformer architecture, yet suffers from quadratic time and memory complexity. The breakthrough FlashAttention algorithm revealed I/O complexity as the true bottleneck in scaling Transformers. Given two levels of memory hierarchy, a fast cache (e.g. GPU on-chip SRAM) and a slow memory (e.g. GPU high-bandwidth memory), the I/O complexity measures the number of accesses to memory. FlashAttention computes attention using $\\frac{N^2d^2}{M}$ I/O operations where $N$ is the dimension of the attention matrix, $d$ the head-dimension and $M$ the cache size. However, is this I/O complexity optimal? The known lower bound only rules out an I/O complexity of $o(Nd)$ when $M=\\Theta(Nd)$, since the output that needs to be written to slow memory is $\\Omega(Nd)$. This leads to the main question of our work: Is FlashAttention I/O optimal for all values of $M$?   We resolve the above question in its full generality by showing an I/O complexity lower bound t",
    "path": "papers/24/02/2402.07443.json",
    "total_tokens": 880,
    "translated_title": "注意力机制的I/O复杂性，或者闪存注意力有多么优化？",
    "translated_abstract": "自注意力是流行的Transformer架构的核心，但是受到二次时间和空间复杂性的限制。突破性的FlashAttention算法揭示了I/O复杂性是扩展Transformer的真正瓶颈。给定两个层级的内存层次结构，一个快速缓存（例如GPU片上静态随机存储器）和一个慢速内存（例如GPU高带宽内存），I/O复杂性衡量了对内存的访问次数。FlashAttention使用$\\frac{N^2d^2}{M}$的I/O操作来计算注意力，其中$N$是注意力矩阵的维度，$d$是头部维度，$M$是缓存大小。然而，这种I/O复杂性是否是最优的？已知的下界只排除了$M=\\Theta(Nd)$时的$o(Nd)$的I/O复杂性，因为需要写入慢速内存的输出是$\\Omega(Nd)$。这引出了我们工作的主要问题：对于所有的$M$值，FlashAttention是否是I/O最优的？我们通过展示一个I/O复杂性的下界来解决上述问题。",
    "tldr": "这项研究解决了FlashAttention算法的I/O复杂性是否是最优的问题。",
    "en_tdlr": "This study resolves the question of whether the I/O complexity of the FlashAttention algorithm is optimal or not."
}