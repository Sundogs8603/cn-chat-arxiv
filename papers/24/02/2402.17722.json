{
    "title": "Taming Nonconvex Stochastic Mirror Descent with General Bregman Divergence",
    "abstract": "arXiv:2402.17722v1 Announce Type: cross  Abstract: This paper revisits the convergence of Stochastic Mirror Descent (SMD) in the contemporary nonconvex optimization setting. Existing results for batch-free nonconvex SMD restrict the choice of the distance generating function (DGF) to be differentiable with Lipschitz continuous gradients, thereby excluding important setups such as Shannon entropy. In this work, we present a new convergence analysis of nonconvex SMD supporting general DGF, that overcomes the above limitations and relies solely on the standard assumptions. Moreover, our convergence is established with respect to the Bregman Forward-Backward envelope, which is a stronger measure than the commonly used squared norm of gradient mapping. We further extend our results to guarantee high probability convergence under sub-Gaussian noise and global convergence under the generalized Bregman Proximal Polyak-{\\L}ojasiewicz condition. Additionally, we illustrate the advantages of our ",
    "link": "https://arxiv.org/abs/2402.17722",
    "context": "Title: Taming Nonconvex Stochastic Mirror Descent with General Bregman Divergence\nAbstract: arXiv:2402.17722v1 Announce Type: cross  Abstract: This paper revisits the convergence of Stochastic Mirror Descent (SMD) in the contemporary nonconvex optimization setting. Existing results for batch-free nonconvex SMD restrict the choice of the distance generating function (DGF) to be differentiable with Lipschitz continuous gradients, thereby excluding important setups such as Shannon entropy. In this work, we present a new convergence analysis of nonconvex SMD supporting general DGF, that overcomes the above limitations and relies solely on the standard assumptions. Moreover, our convergence is established with respect to the Bregman Forward-Backward envelope, which is a stronger measure than the commonly used squared norm of gradient mapping. We further extend our results to guarantee high probability convergence under sub-Gaussian noise and global convergence under the generalized Bregman Proximal Polyak-{\\L}ojasiewicz condition. Additionally, we illustrate the advantages of our ",
    "path": "papers/24/02/2402.17722.json",
    "total_tokens": 847,
    "translated_title": "控制非凸随机镜像下降与一般Bregman散度",
    "translated_abstract": "本文重新探讨了在当代非凸优化设置中随机镜像下降（SMD）的收敛性。现有关于无批处理非凸SMD的结果限制了选择不同iable不可间断梯度的距离生成函数（DGF），从而排除了重要设置，如Shannon熵。在这项工作中，我们提出了一种支持一般DGF的非凸SMD的新收敛分析，克服了上述限制，仅依赖于标准假设。此外，我们的收敛性是针对Bregman前向-后向包络建立的，这比常用的梯度映射的平方范数更强。我们进一步将我们的结果扩展到在次高斯噪声下保证高概率收敛，并在广义Bregman Proximal Polyak-{\\L}ojasiewicz条件下保证全局收敛。此外，我们说明了我们的优势。",
    "tldr": "提出了对一般Bregman散度支持的非凸SMD新收敛分析，克服了先前的限制，并在全局收敛性和高概率收敛性方面取得了进展。",
    "en_tdlr": "Introduced a new convergence analysis of nonconvex SMD supporting general Bregman divergence, overcoming previous limitations, and making progress in global convergence and high probability convergence."
}