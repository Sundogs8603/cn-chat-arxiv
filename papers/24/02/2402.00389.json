{
    "title": "On the $O(\\frac{\\sqrt{d}}{T^{1/4}})$ Convergence Rate of RMSProp and Its Momentum Extension Measured by $\\ell_1$ Norm: Better Dependence on the Dimension",
    "abstract": "Although adaptive gradient methods have been extensively used in deep learning, their convergence rates have not been thoroughly studied, particularly with respect to their dependence on the dimension. This paper considers the classical RMSProp and its momentum extension and establishes the convergence rate of $\\frac{1}{T}\\sum_{k=1}^TE\\left[\\|\\nabla f(x^k)\\|_1\\right]\\leq O(\\frac{\\sqrt{d}}{T^{1/4}})$ measured by $\\ell_1$ norm without the bounded gradient assumption, where $d$ is the dimension of the optimization variable and $T$ is the iteration number. Since $\\|x\\|_2\\ll\\|x\\|_1\\leq\\sqrt{d}\\|x\\|_2$ for problems with extremely large $d$, our convergence rate can be considered to be analogous to the $\\frac{1}{T}\\sum_{k=1}^TE\\left[\\|\\nabla f(x^k)\\|_2\\right]\\leq O(\\frac{1}{T^{1/4}})$ one of SGD measured by $\\ell_1$ norm.",
    "link": "https://arxiv.org/abs/2402.00389",
    "context": "Title: On the $O(\\frac{\\sqrt{d}}{T^{1/4}})$ Convergence Rate of RMSProp and Its Momentum Extension Measured by $\\ell_1$ Norm: Better Dependence on the Dimension\nAbstract: Although adaptive gradient methods have been extensively used in deep learning, their convergence rates have not been thoroughly studied, particularly with respect to their dependence on the dimension. This paper considers the classical RMSProp and its momentum extension and establishes the convergence rate of $\\frac{1}{T}\\sum_{k=1}^TE\\left[\\|\\nabla f(x^k)\\|_1\\right]\\leq O(\\frac{\\sqrt{d}}{T^{1/4}})$ measured by $\\ell_1$ norm without the bounded gradient assumption, where $d$ is the dimension of the optimization variable and $T$ is the iteration number. Since $\\|x\\|_2\\ll\\|x\\|_1\\leq\\sqrt{d}\\|x\\|_2$ for problems with extremely large $d$, our convergence rate can be considered to be analogous to the $\\frac{1}{T}\\sum_{k=1}^TE\\left[\\|\\nabla f(x^k)\\|_2\\right]\\leq O(\\frac{1}{T^{1/4}})$ one of SGD measured by $\\ell_1$ norm.",
    "path": "papers/24/02/2402.00389.json",
    "total_tokens": 1038,
    "translated_title": "关于RMSProp及其动量扩展方法的$O(\\frac{\\sqrt{d}}{T^{1/4}})$收敛速度和对维度的改进依赖性",
    "translated_abstract": "尽管自适应梯度方法在深度学习中被广泛使用，但其收敛速度尚未得到彻底研究，特别是对于其对维度的依赖性。本文考虑了经典的RMSProp及其动量扩展方法，并通过$\\ell_1$范数建立了收敛率$\\frac{1}{T}\\sum_{k=1}^TE\\left[\\|\\nabla f(x^k)\\|_1\\right]\\leq O(\\frac{\\sqrt{d}}{T^{1/4}})$，无需假设梯度有界，其中$d$是优化变量的维度，$T$是迭代次数。由于对于维度极大的问题，$\\|x\\|_2\\ll\\|x\\|_1\\leq\\sqrt{d}\\|x\\|_2$，因此我们的收敛速度可以类比为SGD的$\\frac{1}{T}\\sum_{k=1}^TE\\left[\\|\\nabla f(x^k)\\|_2\\right]\\leq O(\\frac{1}{T^{1/4}})$，测度为$\\ell_1$范数。",
    "tldr": "这项研究探讨了RMSProp及其动量扩展方法的收敛速度，并发现使用$\\ell_1$范数测度时，收敛速度为$O(\\frac{\\sqrt{d}}{T^{1/4}})$，在维度极大的问题中具有改进依赖性。",
    "en_tdlr": "This study investigates the convergence rates of RMSProp and its momentum extension and finds that when measured using the $\\ell_1$ norm, the convergence rate is $O(\\frac{\\sqrt{d}}{T^{1/4}})$, demonstrating improved dependence on the dimension for problems with extremely large dimensions."
}