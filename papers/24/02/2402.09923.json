{
    "title": "A Dataset of Open-Domain Question Answering with Multiple-Span Answers",
    "abstract": "arXiv:2402.09923v1 Announce Type: cross  Abstract: Multi-span answer extraction, also known as the task of multi-span question answering (MSQA), is critical for real-world applications, as it requires extracting multiple pieces of information from a text to answer complex questions. Despite the active studies and rapid progress in English MSQA research, there is a notable lack of publicly available MSQA benchmark in Chinese. Previous efforts for constructing MSQA datasets predominantly emphasized entity-centric contextualization, resulting in a bias towards collecting factoid questions and potentially overlooking questions requiring more detailed descriptive responses. To overcome these limitations, we present CLEAN, a comprehensive Chinese multi-span question answering dataset that involves a wide range of open-domain subjects with a substantial number of instances requiring descriptive answers. Additionally, we provide established models from relevant literature as baselines for CLEA",
    "link": "https://arxiv.org/abs/2402.09923",
    "context": "Title: A Dataset of Open-Domain Question Answering with Multiple-Span Answers\nAbstract: arXiv:2402.09923v1 Announce Type: cross  Abstract: Multi-span answer extraction, also known as the task of multi-span question answering (MSQA), is critical for real-world applications, as it requires extracting multiple pieces of information from a text to answer complex questions. Despite the active studies and rapid progress in English MSQA research, there is a notable lack of publicly available MSQA benchmark in Chinese. Previous efforts for constructing MSQA datasets predominantly emphasized entity-centric contextualization, resulting in a bias towards collecting factoid questions and potentially overlooking questions requiring more detailed descriptive responses. To overcome these limitations, we present CLEAN, a comprehensive Chinese multi-span question answering dataset that involves a wide range of open-domain subjects with a substantial number of instances requiring descriptive answers. Additionally, we provide established models from relevant literature as baselines for CLEA",
    "path": "papers/24/02/2402.09923.json",
    "total_tokens": 838,
    "translated_title": "一个包含多段答案的开放领域问答数据集",
    "translated_abstract": "多段答案提取，也称为多段问答（MSQA）任务，在实际应用中至关重要，因为它需要从文本中提取多个信息片段来回答复杂的问题。尽管英文MSQA研究活跃并取得了快速进展，但在中文领域缺乏公开可用的MSQA基准数据集。以往构建MSQA数据集的努力主要强调实体中心的情境化，导致偏向收集事实性问题并可能忽视需要更详细描述性回答的问题。为了克服这些限制，我们提供了CLEAN，一个全面的中文多段问答数据集，涉及各种开放领域的主题，并包含大量需要描述性答案的实例。此外，我们还提供了相关文献中的已建立模型作为CLEAN的基线。",
    "tldr": "提出了一个中文多段答案的开放领域问答数据集CLEAN，弥补了中文MSQA研究中的不足，包括多样的主题和需要详细回答的问题。提供了相关文献中的基线模型作为参考。"
}