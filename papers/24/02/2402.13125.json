{
    "title": "TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning",
    "abstract": "arXiv:2402.13125v1 Announce Type: cross  Abstract: Recently, numerous new benchmarks have been established to evaluate the performance of large language models (LLMs) via either computing a holistic score or employing another LLM as a judge. However, these approaches suffer from data leakage due to the open access of the benchmark and inflexible evaluation process. To address this issue, we introduce $\\textbf{TreeEval}$, a benchmark-free evaluation method for LLMs that let a high-performance LLM host an irreproducible evaluation session and essentially avoids the data leakage. Moreover, this LLM performs as an examiner to raise up a series of questions under a topic with a tree planing strategy, which considers the current evaluation status to decide the next question generation and ensures the completeness and efficiency of the evaluation process. We evaluate $6$ models of different parameter sizes, including $7$B, $13$B, and $33$B, and ultimately achieved the highest correlation coef",
    "link": "https://arxiv.org/abs/2402.13125",
    "context": "Title: TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning\nAbstract: arXiv:2402.13125v1 Announce Type: cross  Abstract: Recently, numerous new benchmarks have been established to evaluate the performance of large language models (LLMs) via either computing a holistic score or employing another LLM as a judge. However, these approaches suffer from data leakage due to the open access of the benchmark and inflexible evaluation process. To address this issue, we introduce $\\textbf{TreeEval}$, a benchmark-free evaluation method for LLMs that let a high-performance LLM host an irreproducible evaluation session and essentially avoids the data leakage. Moreover, this LLM performs as an examiner to raise up a series of questions under a topic with a tree planing strategy, which considers the current evaluation status to decide the next question generation and ensures the completeness and efficiency of the evaluation process. We evaluate $6$ models of different parameter sizes, including $7$B, $13$B, and $33$B, and ultimately achieved the highest correlation coef",
    "path": "papers/24/02/2402.13125.json",
    "total_tokens": 829,
    "translated_title": "TreeEval：通过树规划实现对大型语言模型的无基准评估",
    "translated_abstract": "最近，建立了许多新的基准来评估大型语言模型（LLMs）的性能，通过计算整体得分或使用另一个LLM作为评判者。然而，这些方法由于基准的公开访问和评估过程的不灵活而遭受数据泄漏的困扰。为了解决这个问题，我们引入了TreeEval，这是一种无基准评估方法，让一个高性能的LLM主持一个不可重现的评估会话，从根本上避免了数据泄漏。此外，这个LLM充当一个考官，提出一系列关于一个主题的问题，并采用树规划策略，考虑当前的评估状态来决定下一个问题的生成，确保评估过程的完整性和效率。我们评估了不同参数大小的6个模型，包括7B、13B和33B，最终实现了最高的相关系数。",
    "tldr": "TreeEval提出了一种无基准评估方法，通过树规划策略提升了大型语言模型的评估效率和完整性",
    "en_tdlr": "TreeEval introduces a benchmark-free evaluation method that utilizes tree planning strategy to enhance the efficiency and completeness of evaluating large language models."
}