{
    "title": "Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap",
    "abstract": "arXiv:2402.19450v1 Announce Type: new  Abstract: We propose a framework for robust evaluation of reasoning capabilities of language models, using functional variants of benchmarks. Models that solve a reasoning test should exhibit no difference in performance over the static version of a problem compared to a snapshot of the functional variant. We have rewritten the relevant fragment of the MATH benchmark into its functional variant MATH(), with functionalization of other benchmarks to follow. When evaluating current state-of-the-art models over snapshots of MATH(), we find a reasoning gap -- the percentage difference between the static and functional accuracies. We find reasoning gaps from 58.35% to 80.31% among the state-of-the-art closed and open weights models that perform well on static benchmarks, with the caveat that the gaps are likely to be smaller with more sophisticated prompting strategies. Here we show that models which anecdotally have good reasoning performance over real",
    "link": "https://arxiv.org/abs/2402.19450",
    "context": "Title: Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap\nAbstract: arXiv:2402.19450v1 Announce Type: new  Abstract: We propose a framework for robust evaluation of reasoning capabilities of language models, using functional variants of benchmarks. Models that solve a reasoning test should exhibit no difference in performance over the static version of a problem compared to a snapshot of the functional variant. We have rewritten the relevant fragment of the MATH benchmark into its functional variant MATH(), with functionalization of other benchmarks to follow. When evaluating current state-of-the-art models over snapshots of MATH(), we find a reasoning gap -- the percentage difference between the static and functional accuracies. We find reasoning gaps from 58.35% to 80.31% among the state-of-the-art closed and open weights models that perform well on static benchmarks, with the caveat that the gaps are likely to be smaller with more sophisticated prompting strategies. Here we show that models which anecdotally have good reasoning performance over real",
    "path": "papers/24/02/2402.19450.json",
    "total_tokens": 850,
    "translated_title": "用于鲁棒推理性能评估的功能基准及推理差距",
    "translated_abstract": "我们提出了一个框架，利用基准的功能变体对语言模型的推理能力进行鲁棒评估。解决推理测试的模型在静态问题的表现与功能变体快照相比应该没有差异。我们将MATH基准的相关片段重写为其功能变体MATH()，并将其他基准的功能化随之而来。在对当前最先进模型在MATH()快照上进行评估时，我们发现了推理差距--静态准确性与功能准确性之间的百分比差异。我们发现了在表现良好的静态基准上的最先进封闭和开放权重模型之间的推理差距，从58.35%到80.31%，但要注意的是，这些差距可能在使用更复杂提示策略时更小。在这里，我们展示了这样的模型，在真实情况下具有良好的推理性能",
    "tldr": "提出了一个用于评估语言模型推理能力的框架，通过功能变体的基准进行鲁棒性评估，发现静态基准和功能基准的准确性之间存在推理差距",
    "en_tdlr": "Proposed a framework for evaluating the reasoning capabilities of language models using functional variants of benchmarks, found a reasoning gap between static and functional benchmarks' accuracies."
}