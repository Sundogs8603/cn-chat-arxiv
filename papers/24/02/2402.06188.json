{
    "title": "A self-supervised framework for learning whole slide representations",
    "abstract": "Whole slide imaging is fundamental to biomedical microscopy and computational pathology. However, whole slide images (WSIs) present a complex computer vision challenge due to their gigapixel size, diverse histopathologic features, spatial heterogeneity, and limited/absent data annotations. These challenges highlight that supervised training alone can result in suboptimal whole slide representations. Self-supervised representation learning can achieve high-quality WSI visual feature learning for downstream diagnostic tasks, such as cancer diagnosis or molecular genetic prediction. Here, we present a general self-supervised whole slide learning (S3L) framework for gigapixel-scale self-supervision of WSIs. S3L combines data transformation strategies from transformer-based vision and language modeling into a single unified framework to generate paired views for self-supervision. S3L leverages the inherent regional heterogeneity, histologic feature variability, and information redundancy wi",
    "link": "https://arxiv.org/abs/2402.06188",
    "context": "Title: A self-supervised framework for learning whole slide representations\nAbstract: Whole slide imaging is fundamental to biomedical microscopy and computational pathology. However, whole slide images (WSIs) present a complex computer vision challenge due to their gigapixel size, diverse histopathologic features, spatial heterogeneity, and limited/absent data annotations. These challenges highlight that supervised training alone can result in suboptimal whole slide representations. Self-supervised representation learning can achieve high-quality WSI visual feature learning for downstream diagnostic tasks, such as cancer diagnosis or molecular genetic prediction. Here, we present a general self-supervised whole slide learning (S3L) framework for gigapixel-scale self-supervision of WSIs. S3L combines data transformation strategies from transformer-based vision and language modeling into a single unified framework to generate paired views for self-supervision. S3L leverages the inherent regional heterogeneity, histologic feature variability, and information redundancy wi",
    "path": "papers/24/02/2402.06188.json",
    "total_tokens": 938,
    "translated_title": "一个自监督学习框架用于学习整个切片的表示",
    "translated_abstract": "整个切片成像对于生物医学显微镜和计算病理学至关重要。然而，由于其千兆像素的大小、多样的组织病理学特征、空间异质性以及有限的/不存在的数据注释，整个切片图像 (WSIs) 构成了一个复杂的计算机视觉挑战。这些挑战突显了仅依靠监督训练可能导致次优的整个切片表示。自监督表示学习可以为下游诊断任务（如癌症诊断或分子遗传预测）实现高质量的WSI视觉特征学习。在这里，我们提出了一个通用的自监督整个切片学习（S3L）框架，用于千兆像素规模的WSI自监督。S3L将来自基于变压器的视觉和语言建模的数据转换策略结合到一个统一的框架中，以生成用于自监督的配对视图。S3L利用内在的区域异质性、组织学特征的可变性和信息冗余性",
    "tldr": "这个论文提出了一个自监督学习框架（S3L），用于学习整个切片的表示。它结合了变压器模型的视觉和语言建模策略，通过生成配对视图进行自监督学习，以实现高质量的WSI视觉特征学习。"
}