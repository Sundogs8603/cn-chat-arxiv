{
    "title": "Pruner: An Efficient Cross-Platform Tensor Compiler with Dual Awareness",
    "abstract": "Tensor program optimization on Deep Learning Accelerators (DLAs) is critical for efficient model deployment. Although search-based Deep Learning Compilers (DLCs) have achieved significant performance gains compared to manual methods, they still suffer from the persistent challenges of low search efficiency and poor cross-platform adaptability. In this paper, we propose $\\textbf{Pruner}$, following hardware/software co-design principles to hierarchically boost tensor program optimization. Pruner comprises two primary components: a Parameterized Static Analyzer ($\\textbf{PSA}$) and a Pattern-aware Cost Model ($\\textbf{PaCM}$). The former serves as a hardware-aware and formulaic performance analysis tool, guiding the pruning of the search space, while the latter enables the performance prediction of tensor programs according to the critical data-flow patterns. Furthermore, to ensure effective cross-platform adaptation, we design a Momentum Transfer Learning ($\\textbf{MTL}$) strategy using",
    "link": "https://arxiv.org/abs/2402.02361",
    "context": "Title: Pruner: An Efficient Cross-Platform Tensor Compiler with Dual Awareness\nAbstract: Tensor program optimization on Deep Learning Accelerators (DLAs) is critical for efficient model deployment. Although search-based Deep Learning Compilers (DLCs) have achieved significant performance gains compared to manual methods, they still suffer from the persistent challenges of low search efficiency and poor cross-platform adaptability. In this paper, we propose $\\textbf{Pruner}$, following hardware/software co-design principles to hierarchically boost tensor program optimization. Pruner comprises two primary components: a Parameterized Static Analyzer ($\\textbf{PSA}$) and a Pattern-aware Cost Model ($\\textbf{PaCM}$). The former serves as a hardware-aware and formulaic performance analysis tool, guiding the pruning of the search space, while the latter enables the performance prediction of tensor programs according to the critical data-flow patterns. Furthermore, to ensure effective cross-platform adaptation, we design a Momentum Transfer Learning ($\\textbf{MTL}$) strategy using",
    "path": "papers/24/02/2402.02361.json",
    "total_tokens": 881,
    "translated_title": "Pruner:一种具有双重感知能力的高效跨平台张量编译器",
    "translated_abstract": "对深度学习加速器（DLAs）上的张量程序优化对于有效的模型部署至关重要。虽然基于搜索的深度学习编译器（DLC）与手动方法相比取得了显著的性能提升，但仍然面临着搜索效率低和跨平台适应性差的挑战。在本文中，我们提出了Pruner，遵循硬件/软件协同设计原则来分层提升张量程序优化。Pruner由两个主要组件组成：参数化静态分析器（PSA）和模式感知成本模型（PaCM）。前者作为一种硬件感知和公式化的性能分析工具，引导搜索空间的修剪，而后者根据关键的数据流模式实现了对张量程序的性能预测。此外，为了保证有效的跨平台适应性，我们设计了一个动量转移学习（MTL）策略。",
    "tldr": "Pruner是一种高效跨平台张量编译器，通过参数化静态分析器（PSA）和模式感知成本模型（PaCM）实现张量程序优化，并使用动量转移学习（MTL）策略实现了跨平台适应性。"
}