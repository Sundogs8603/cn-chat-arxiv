{
    "title": "Fine-tuning CLIP Text Encoders with Two-step Paraphrasing",
    "abstract": "arXiv:2402.15120v1 Announce Type: cross  Abstract: Contrastive language-image pre-training (CLIP) models have demonstrated considerable success across various vision-language tasks, such as text-to-image retrieval, where the model is required to effectively process natural language input to produce an accurate visual output. However, current models still face limitations in dealing with linguistic variations in input queries, such as paraphrases, making it challenging to handle a broad range of user queries in real-world applications. In this study, we introduce a straightforward fine-tuning approach to enhance the representations of CLIP models for paraphrases. Our approach involves a two-step paraphrase generation process, where we automatically create two categories of paraphrases from web-scale image captions by leveraging large language models. Subsequently, we fine-tune the CLIP text encoder using these generated paraphrases while freezing the image encoder. Our resulting model, ",
    "link": "https://arxiv.org/abs/2402.15120",
    "context": "Title: Fine-tuning CLIP Text Encoders with Two-step Paraphrasing\nAbstract: arXiv:2402.15120v1 Announce Type: cross  Abstract: Contrastive language-image pre-training (CLIP) models have demonstrated considerable success across various vision-language tasks, such as text-to-image retrieval, where the model is required to effectively process natural language input to produce an accurate visual output. However, current models still face limitations in dealing with linguistic variations in input queries, such as paraphrases, making it challenging to handle a broad range of user queries in real-world applications. In this study, we introduce a straightforward fine-tuning approach to enhance the representations of CLIP models for paraphrases. Our approach involves a two-step paraphrase generation process, where we automatically create two categories of paraphrases from web-scale image captions by leveraging large language models. Subsequently, we fine-tune the CLIP text encoder using these generated paraphrases while freezing the image encoder. Our resulting model, ",
    "path": "papers/24/02/2402.15120.json",
    "total_tokens": 739,
    "translated_title": "使用两步重述对CLIP文本编码器进行微调",
    "translated_abstract": "Contrastive language-image pre-training (CLIP)模型在各种视觉-语言任务中表现出色，如文本到图像检索，其中模型需要有效处理自然语言输入以产生准确的视觉输出。然而，当前模型在处理输入查询中的语言变化（如释义）方面仍然面临限制，这使得难以处理现实应用中用户查询的广泛范围。在这项研究中，我们引入了一种简单的微调方法，以增强CLIP模型对释义的表示。我们的方法涉及一个两步释义生成过程，我们通过利用大型语言模型从网页规模的图像标题中自动创建两类释义。随后，我们通过使用这些生成的释义来微调CLIP文本编码器，同时冻结图像编码器。我们的结果模型，...",
    "tldr": "通过两步重述生成过程对CLIP模型进行微调，以增强对释义的表示能力。",
    "en_tdlr": "Fine-tuning the CLIP model with a two-step paraphrase generation process to enhance the representation ability for paraphrases."
}