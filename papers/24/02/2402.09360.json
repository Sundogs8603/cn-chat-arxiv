{
    "title": "HiRE: High Recall Approximate Top-$k$ Estimation for Efficient LLM Inference",
    "abstract": "arXiv:2402.09360v1 Announce Type: cross Abstract: Autoregressive decoding with generative Large Language Models (LLMs) on accelerators (GPUs/TPUs) is often memory-bound where most of the time is spent on transferring model parameters from high bandwidth memory (HBM) to cache. On the other hand, recent works show that LLMs can maintain quality with significant sparsity/redundancy in the feedforward (FFN) layers by appropriately training the model to operate on a top-$k$ fraction of rows/columns (where $k \\approx 0.05$), there by suggesting a way to reduce the transfer of model parameters, and hence latency. However, exploiting this sparsity for improving latency is hindered by the fact that identifying top rows/columns is data-dependent and is usually performed using full matrix operations, severely limiting potential gains. To address these issues, we introduce HiRE (High Recall Approximate Top-k Estimation). HiRE comprises of two novel components: (i) a compression scheme to cheaply p",
    "link": "https://arxiv.org/abs/2402.09360",
    "context": "Title: HiRE: High Recall Approximate Top-$k$ Estimation for Efficient LLM Inference\nAbstract: arXiv:2402.09360v1 Announce Type: cross Abstract: Autoregressive decoding with generative Large Language Models (LLMs) on accelerators (GPUs/TPUs) is often memory-bound where most of the time is spent on transferring model parameters from high bandwidth memory (HBM) to cache. On the other hand, recent works show that LLMs can maintain quality with significant sparsity/redundancy in the feedforward (FFN) layers by appropriately training the model to operate on a top-$k$ fraction of rows/columns (where $k \\approx 0.05$), there by suggesting a way to reduce the transfer of model parameters, and hence latency. However, exploiting this sparsity for improving latency is hindered by the fact that identifying top rows/columns is data-dependent and is usually performed using full matrix operations, severely limiting potential gains. To address these issues, we introduce HiRE (High Recall Approximate Top-k Estimation). HiRE comprises of two novel components: (i) a compression scheme to cheaply p",
    "path": "papers/24/02/2402.09360.json",
    "total_tokens": 900,
    "translated_title": "HiRE:高召回率的高效LLM推理的近似Top-$k$估计",
    "translated_abstract": "在加速器（GPU/TPU）上使用生成式大型语言模型（LLM）进行自回归解码往往受到内存限制，大部分时间用于将模型参数从高带宽内存（HBM）传输到缓存中。与此同时，最近的研究表明，通过适当训练模型，在前馈（FFN）层中维持质量的同时具有显著的稀疏性/冗余性（其中$k \\approx 0.05$），从而提出了减少模型参数传输和延迟的方法。然而，利用这种稀疏性来改善延迟的过程受到数据依赖性的限制，通常需要进行完整的矩阵操作来识别前$k$个行/列，严重限制了潜在的收益。为了解决这些问题，我们引入了HiRE（高召回率的近似Top-k估计）。HiRE包括两个新颖的组件：（i）一种压缩方案以便廉价地估计前$k$个行/列。",
    "tldr": "HiRE是一种用于高效的LLM推理的高召回率的近似Top-k估计方法，通过压缩方案减少了模型参数传输和延迟。"
}