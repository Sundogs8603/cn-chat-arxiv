{
    "title": "Unsupervised Text Style Transfer via LLMs and Attention Masking with Multi-way Interactions",
    "abstract": "arXiv:2402.13647v1 Announce Type: cross  Abstract: Unsupervised Text Style Transfer (UTST) has emerged as a critical task within the domain of Natural Language Processing (NLP), aiming to transfer one stylistic aspect of a sentence into another style without changing its semantics, syntax, or other attributes. This task is especially challenging given the intrinsic lack of parallel text pairings. Among existing methods for UTST tasks, attention masking approach and Large Language Models (LLMs) are deemed as two pioneering methods. However, they have shortcomings in generating unsmooth sentences and changing the original contents, respectively. In this paper, we investigate if we can combine these two methods effectively. We propose four ways of interactions, that are pipeline framework with tuned orders; knowledge distillation from LLMs to attention masking model; in-context learning with constructed parallel examples. We empirically show these multi-way interactions can improve the ba",
    "link": "https://arxiv.org/abs/2402.13647",
    "context": "Title: Unsupervised Text Style Transfer via LLMs and Attention Masking with Multi-way Interactions\nAbstract: arXiv:2402.13647v1 Announce Type: cross  Abstract: Unsupervised Text Style Transfer (UTST) has emerged as a critical task within the domain of Natural Language Processing (NLP), aiming to transfer one stylistic aspect of a sentence into another style without changing its semantics, syntax, or other attributes. This task is especially challenging given the intrinsic lack of parallel text pairings. Among existing methods for UTST tasks, attention masking approach and Large Language Models (LLMs) are deemed as two pioneering methods. However, they have shortcomings in generating unsmooth sentences and changing the original contents, respectively. In this paper, we investigate if we can combine these two methods effectively. We propose four ways of interactions, that are pipeline framework with tuned orders; knowledge distillation from LLMs to attention masking model; in-context learning with constructed parallel examples. We empirically show these multi-way interactions can improve the ba",
    "path": "papers/24/02/2402.13647.json",
    "total_tokens": 728,
    "translated_title": "通过LLMs和注意力遮罩完成无监督文本风格转移与多路交互",
    "translated_abstract": "无监督文本风格转移（UTST）已成为自然语言处理（NLP）领域中的一个关键任务，旨在将句子的一种风格方面转换为另一种风格，同时不改变其语义、句法或其他属性。本文探讨了如何有效结合注意力遮罩方法和大型语言模型（LLMs），提出了四种交互方式：具有调整顺序的流水线框架；知识蒸馏从LLMs到注意力遮罩模型；使用构建的并行示例进行上下文学习。我们实验证明这些多路交互可以改进性",
    "tldr": "通过组合注意力遮罩方法和大型语言模型，提出多种交互方式，可以改进无监督文本风格转移任务。",
    "en_tdlr": "Propose multiple interactions by combining attention masking method and large language models to improve unsupervised text style transfer task."
}