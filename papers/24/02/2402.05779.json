{
    "title": "Examining Gender and Racial Bias in Large Vision-Language Models Using a Novel Dataset of Parallel Images",
    "abstract": "Following on recent advances in large language models (LLMs) and subsequent chat models, a new wave of large vision-language models (LVLMs) has emerged. Such models can incorporate images as input in addition to text, and perform tasks such as visual question answering, image captioning, story generation, etc. Here, we examine potential gender and racial biases in such systems, based on the perceived characteristics of the people in the input images. To accomplish this, we present a new dataset PAIRS (PArallel Images for eveRyday Scenarios). The PAIRS dataset contains sets of AI-generated images of people, such that the images are highly similar in terms of background and visual content, but differ along the dimensions of gender (man, woman) and race (Black, white). By querying the LVLMs with such images, we observe significant differences in the responses according to the perceived gender or race of the person depicted.",
    "link": "https://arxiv.org/abs/2402.05779",
    "context": "Title: Examining Gender and Racial Bias in Large Vision-Language Models Using a Novel Dataset of Parallel Images\nAbstract: Following on recent advances in large language models (LLMs) and subsequent chat models, a new wave of large vision-language models (LVLMs) has emerged. Such models can incorporate images as input in addition to text, and perform tasks such as visual question answering, image captioning, story generation, etc. Here, we examine potential gender and racial biases in such systems, based on the perceived characteristics of the people in the input images. To accomplish this, we present a new dataset PAIRS (PArallel Images for eveRyday Scenarios). The PAIRS dataset contains sets of AI-generated images of people, such that the images are highly similar in terms of background and visual content, but differ along the dimensions of gender (man, woman) and race (Black, white). By querying the LVLMs with such images, we observe significant differences in the responses according to the perceived gender or race of the person depicted.",
    "path": "papers/24/02/2402.05779.json",
    "total_tokens": 845,
    "translated_title": "使用一个新颖的平行图像数据集，研究大型视觉语言模型中的性别和种族偏见",
    "translated_abstract": "在最近大语言模型(LLMs)和相应的对话模型的进步后，出现了一波新的大型视觉语言模型(LVLMs)。这样的模型可以将图像作为输入，并执行视觉问答、图像字幕、故事生成等任务。在这里，我们在这些系统中考察潜在的性别和种族偏见，基于输入图像中人物的被感知特征。为了实现这一目标，我们提出了一个新的数据集PAIRS（日常情景下的平行图像）。PAIRS数据集包含一组人工智能生成的人物图像，这些图像在背景和视觉内容方面非常相似，但在性别（男性、女性）和种族（黑人、白人）维度上有所不同。通过使用这样的图像查询LVLMs，我们观察到根据人物的被感知性别或种族，响应有明显差异。",
    "tldr": "本研究使用了一个新的平行图像数据集，通过查询大型视觉语言模型，观察到在性别和种族维度上存在显著的偏见。"
}