{
    "title": "RECOST: External Knowledge Guided Data-efficient Instruction Tuning",
    "abstract": "arXiv:2402.17355v1 Announce Type: new  Abstract: In the current landscape of large language models (LLMs), the process of instruction tuning serves as an essential step. Considering the high computing power overhead, data-efficient instruction tuning was proposed to reduce the training data size in this process, aiming at selecting high-quality instructional data. Nevertheless, we argue that most current data-efficient instruction-tuning methods are highly dependent on the quality of the original instruction-tuning dataset. When it comes to datasets synthesized by LLMs, a common scenario in this field, dirty samples will even be selected with a higher probability than other samples. To address these challenges, we utilized external knowledge (relevant examples or paragraphs) to evaluate those samples synthesized by LLMs with an in-context-based relative predictive entropy. Based on the new metric, we proposed a framework, dubbed as \\textbf{RECOST}, which integrates external-knowledge-b",
    "link": "https://arxiv.org/abs/2402.17355",
    "context": "Title: RECOST: External Knowledge Guided Data-efficient Instruction Tuning\nAbstract: arXiv:2402.17355v1 Announce Type: new  Abstract: In the current landscape of large language models (LLMs), the process of instruction tuning serves as an essential step. Considering the high computing power overhead, data-efficient instruction tuning was proposed to reduce the training data size in this process, aiming at selecting high-quality instructional data. Nevertheless, we argue that most current data-efficient instruction-tuning methods are highly dependent on the quality of the original instruction-tuning dataset. When it comes to datasets synthesized by LLMs, a common scenario in this field, dirty samples will even be selected with a higher probability than other samples. To address these challenges, we utilized external knowledge (relevant examples or paragraphs) to evaluate those samples synthesized by LLMs with an in-context-based relative predictive entropy. Based on the new metric, we proposed a framework, dubbed as \\textbf{RECOST}, which integrates external-knowledge-b",
    "path": "papers/24/02/2402.17355.json",
    "total_tokens": 832,
    "translated_title": "RECOST: 外部知识引导的数据高效指导调整",
    "translated_abstract": "在当前大型语言模型（LLMs）的领域中，指导调整的过程是至关重要的一步。考虑到高昂的计算能力开销，提出了数据高效指导调整，以减少该过程中的训练数据量，旨在选择高质量的指示性数据。然而，我们认为目前大多数数据高效指导调整方法高度依赖于原指导调整数据集的质量。关于由LLMs合成的数据集，这个领域中常见的情况，脏样本甚至会比其他样本被更高概率地选择。为了解决这些挑战，我们利用外部知识（相关示例或段落）通过基于上下文的相对预测熵评估由LLMs合成的这些样本。基于新的度量标准，我们提出了一个名为 \\textbf{RECOST} 的框架，该框架整合了外部知识。",
    "tldr": "提出了RECOST框架，利用外部知识评估由大型语言模型合成的样本，通过相对预测熵解决数据高效指导调整中数据质量不足的挑战",
    "en_tdlr": "Introduced RECOST framework that utilizes external knowledge to evaluate samples synthesized by large language models, addressing the challenge of insufficient data quality in data-efficient instruction tuning by relative predictive entropy."
}