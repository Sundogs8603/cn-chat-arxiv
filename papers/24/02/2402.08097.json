{
    "title": "An Accelerated Gradient Method for Simple Bilevel Optimization with Convex Lower-level Problem",
    "abstract": "In this paper, we focus on simple bilevel optimization problems, where we minimize a convex smooth objective function over the optimal solution set of another convex smooth constrained optimization problem. We present a novel bilevel optimization method that locally approximates the solution set of the lower-level problem using a cutting plane approach and employs an accelerated gradient-based update to reduce the upper-level objective function over the approximated solution set. We measure the performance of our method in terms of suboptimality and infeasibility errors and provide non-asymptotic convergence guarantees for both error criteria. Specifically, when the feasible set is compact, we show that our method requires at most $\\mathcal{O}(\\max\\{1/\\sqrt{\\epsilon_{f}}, 1/\\epsilon_g\\})$ iterations to find a solution that is $\\epsilon_f$-suboptimal and $\\epsilon_g$-infeasible. Moreover, under the additional assumption that the lower-level objective satisfies the $r$-th H\\\"olderian err",
    "link": "https://arxiv.org/abs/2402.08097",
    "context": "Title: An Accelerated Gradient Method for Simple Bilevel Optimization with Convex Lower-level Problem\nAbstract: In this paper, we focus on simple bilevel optimization problems, where we minimize a convex smooth objective function over the optimal solution set of another convex smooth constrained optimization problem. We present a novel bilevel optimization method that locally approximates the solution set of the lower-level problem using a cutting plane approach and employs an accelerated gradient-based update to reduce the upper-level objective function over the approximated solution set. We measure the performance of our method in terms of suboptimality and infeasibility errors and provide non-asymptotic convergence guarantees for both error criteria. Specifically, when the feasible set is compact, we show that our method requires at most $\\mathcal{O}(\\max\\{1/\\sqrt{\\epsilon_{f}}, 1/\\epsilon_g\\})$ iterations to find a solution that is $\\epsilon_f$-suboptimal and $\\epsilon_g$-infeasible. Moreover, under the additional assumption that the lower-level objective satisfies the $r$-th H\\\"olderian err",
    "path": "papers/24/02/2402.08097.json",
    "total_tokens": 924,
    "translated_title": "一种加速梯度方法求解具有凸下层问题的简单双层优化问题",
    "translated_abstract": "本文主要研究简单的双层优化问题，即在另一个凸光滑约束优化问题的最优解集上最小化一个凸光滑目标函数。我们提出了一种新颖的双层优化方法，通过切平面方法局部逼近下层问题的解集，并采用加速梯度更新方法降低近似解集上的上层目标函数。我们通过子最优解和不可行误差度量我们方法的性能，并提供了对两个误差标准的非渐进收敛性保证。特别地，当可行集是紧致的时候，我们证明了我们的方法最多需要$\\mathcal{O}(\\max\\{1/\\sqrt{\\epsilon_{f}}, 1/\\epsilon_g\\})$次迭代才能找到一个$\\epsilon_f$-子最优且$\\epsilon_g$-不可行的解。此外，在额外假设下，下层目标满足$r$阶H\\\"olderian误差时，我们给出了解的收敛速度估计。",
    "tldr": "本文提出了一种加速梯度方法来解决具有凸下层问题的简单双层优化问题，通过局部逼近下层问题的解集和加速梯度更新方法，在有限次迭代内找到一个具有一定精度的最优解。"
}