{
    "title": "Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs",
    "abstract": "arXiv:2402.10517v1 Announce Type: new  Abstract: Recently, considerable efforts have been directed towards compressing Large Language Models (LLMs), which showcase groundbreaking capabilities across diverse applications but entail significant deployment costs due to their large sizes. Meanwhile, much less attention has been given to mitigating the costs associated with deploying multiple LLMs of varying sizes despite its practical significance. Thus, this paper introduces \\emph{any-precision LLM}, extending the concept of any-precision DNN to LLMs. Addressing challenges in any-precision LLM, we propose a lightweight method for any-precision quantization of LLMs, leveraging a post-training quantization framework, and develop a specialized software engine for its efficient serving. As a result, our solution significantly reduces the high costs of deploying multiple, different-sized LLMs by overlaying LLMs quantized to varying bit-widths, such as 3, 4, ..., $n$ bits, into a memory footpri",
    "link": "https://arxiv.org/abs/2402.10517",
    "context": "Title: Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs\nAbstract: arXiv:2402.10517v1 Announce Type: new  Abstract: Recently, considerable efforts have been directed towards compressing Large Language Models (LLMs), which showcase groundbreaking capabilities across diverse applications but entail significant deployment costs due to their large sizes. Meanwhile, much less attention has been given to mitigating the costs associated with deploying multiple LLMs of varying sizes despite its practical significance. Thus, this paper introduces \\emph{any-precision LLM}, extending the concept of any-precision DNN to LLMs. Addressing challenges in any-precision LLM, we propose a lightweight method for any-precision quantization of LLMs, leveraging a post-training quantization framework, and develop a specialized software engine for its efficient serving. As a result, our solution significantly reduces the high costs of deploying multiple, different-sized LLMs by overlaying LLMs quantized to varying bit-widths, such as 3, 4, ..., $n$ bits, into a memory footpri",
    "path": "papers/24/02/2402.10517.json",
    "total_tokens": 925,
    "translated_title": "任意精度LLM：多个不同大小LLM的低成本部署",
    "translated_abstract": "最近，人们对压缩大型语言模型（LLMs）进行了相当多的努力，这些LLMs在各种应用中展示了突破性的能力，但由于其庞大的体积而导致部署成本高昂。与此同时，尽管多个不同大小的LLMs部署的成本在实际意义上很重要，但却受到的关注较少。因此，本文引入了“任意精度LLM”，将任意精度DNN的概念扩展到LLMs。解决了任意精度LLM中的挑战，我们提出了一种轻量级的LLMs任意精度量化方法，利用后训练量化框架，并开发了一个专门的软件引擎来实现其有效的服务。结果，我们的解决方案通过将以不同位宽（如3、4、…，n位）量化的LLMs叠加到内存足印中，显着降低了部署多个不同大小的LLMs的高成本。",
    "tldr": "任意精度LLM引入了一种轻量级方法，通过将不同大小LLMs量化为不同位宽（如3、4、...，n位）并叠加到内存中，显着降低了部署多个不同大小LLMs的高成本"
}