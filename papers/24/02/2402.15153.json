{
    "title": "Self-Adaptive Reconstruction with Contrastive Learning for Unsupervised Sentence Embeddings",
    "abstract": "arXiv:2402.15153v1 Announce Type: new  Abstract: Unsupervised sentence embeddings task aims to convert sentences to semantic vector representations. Most previous works directly use the sentence representations derived from pretrained language models. However, due to the token bias in pretrained language models, the models can not capture the fine-grained semantics in sentences, which leads to poor predictions. To address this issue, we propose a novel Self-Adaptive Reconstruction Contrastive Sentence Embeddings (SARCSE) framework, which reconstructs all tokens in sentences with an AutoEncoder to help the model to preserve more fine-grained semantics during tokens aggregating. In addition, we proposed a self-adaptive reconstruction loss to alleviate the token bias towards frequency. Experimental results show that SARCSE gains significant improvements compared with the strong baseline SimCSE on the 7 STS tasks.",
    "link": "https://arxiv.org/abs/2402.15153",
    "context": "Title: Self-Adaptive Reconstruction with Contrastive Learning for Unsupervised Sentence Embeddings\nAbstract: arXiv:2402.15153v1 Announce Type: new  Abstract: Unsupervised sentence embeddings task aims to convert sentences to semantic vector representations. Most previous works directly use the sentence representations derived from pretrained language models. However, due to the token bias in pretrained language models, the models can not capture the fine-grained semantics in sentences, which leads to poor predictions. To address this issue, we propose a novel Self-Adaptive Reconstruction Contrastive Sentence Embeddings (SARCSE) framework, which reconstructs all tokens in sentences with an AutoEncoder to help the model to preserve more fine-grained semantics during tokens aggregating. In addition, we proposed a self-adaptive reconstruction loss to alleviate the token bias towards frequency. Experimental results show that SARCSE gains significant improvements compared with the strong baseline SimCSE on the 7 STS tasks.",
    "path": "papers/24/02/2402.15153.json",
    "total_tokens": 893,
    "translated_title": "自适应对比学习的无监督句子嵌入自适应重建",
    "translated_abstract": "无监督句子嵌入任务旨在将句子转换为语义向量表示。大多数先前的工作直接使用从预训练语言模型派生的句子表示。然而，由于预训练语言模型中的令牌偏差，模型无法捕捉句子中的细粒度语义，导致预测能力不佳。为解决这一问题，我们提出了一种新颖的自适应重建对比句子嵌入（SARCSE）框架，该框架利用自动编码器重建句子中的所有令牌，帮助模型在聚合令牌过程中保留更多细粒度语义。此外，我们提出了一种自适应重建损失来缓解对令牌频率的偏见。实验结果表明，与强基准SimCSE相比，SARCSE在7个STS任务上取得了显著的改进。",
    "tldr": "提出了一种新颖的Self-Adaptive Reconstruction Contrastive Sentence Embeddings（SARCSE）框架，利用自动编码器重建句子中的所有令牌，以帮助模型保留更多细粒度语义，并提出了一种自适应重建损失来缓解对令牌频率的偏见。",
    "en_tdlr": "Introduced a novel Self-Adaptive Reconstruction Contrastive Sentence Embeddings (SARCSE) framework that reconstructs all tokens in sentences using an AutoEncoder to preserve more fine-grained semantics, along with a self-adaptive reconstruction loss to mitigate bias towards token frequency."
}