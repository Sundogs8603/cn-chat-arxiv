{
    "title": "Fine-Tuned Language Models Generate Stable Inorganic Materials as Text",
    "abstract": "We propose fine-tuning large language models for generation of stable materials. While unorthodox, fine-tuning large language models on text-encoded atomistic data is simple to implement yet reliable, with around 90% of sampled structures obeying physical constraints on atom positions and charges. Using energy above hull calculations from both learned ML potentials and gold-standard DFT calculations, we show that our strongest model (fine-tuned LLaMA-2 70B) can generate materials predicted to be metastable at about twice the rate (49% vs 28%) of CDVAE, a competing diffusion model. Because of text prompting's inherent flexibility, our models can simultaneously be used for unconditional generation of stable material, infilling of partial structures and text-conditional generation. Finally, we show that language models' ability to capture key symmetries of crystal structures improves with model scale, suggesting that the biases of pretrained LLMs are surprisingly well-suited for atomistic",
    "link": "https://arxiv.org/abs/2402.04379",
    "context": "Title: Fine-Tuned Language Models Generate Stable Inorganic Materials as Text\nAbstract: We propose fine-tuning large language models for generation of stable materials. While unorthodox, fine-tuning large language models on text-encoded atomistic data is simple to implement yet reliable, with around 90% of sampled structures obeying physical constraints on atom positions and charges. Using energy above hull calculations from both learned ML potentials and gold-standard DFT calculations, we show that our strongest model (fine-tuned LLaMA-2 70B) can generate materials predicted to be metastable at about twice the rate (49% vs 28%) of CDVAE, a competing diffusion model. Because of text prompting's inherent flexibility, our models can simultaneously be used for unconditional generation of stable material, infilling of partial structures and text-conditional generation. Finally, we show that language models' ability to capture key symmetries of crystal structures improves with model scale, suggesting that the biases of pretrained LLMs are surprisingly well-suited for atomistic",
    "path": "papers/24/02/2402.04379.json",
    "total_tokens": 859,
    "translated_title": "细调语言模型生成稳定的无机材料文本",
    "translated_abstract": "我们提出了对大型语言模型进行细调，以生成稳定材料。虽然非传统，但在文本编码的原子数据上细调大型语言模型非常简单易行，同时可靠性高，约90%的采样结构遵守原子位置和电荷的物理约束。通过来自学习的机器学习势和金标准DFT计算的能量以上的计算，我们表明我们的最强模型（细调LLaMA-2 70B）可以以CDVAE竞争扩散模型的约两倍速率（49% vs 28%）生成被预测为亚稳态的材料。由于文本提示的固有灵活性，我们的模型可以同时用于稳定材料的无条件生成、部分结构的填充和文本条件生成。最后，我们表明语言模型捕捉晶体结构的关键对称性的能力随模型规模的增大而改善，这表明预训练的LLM的偏差出奇地适合原子性的应用。",
    "tldr": "细调语言模型用于生成稳定材料，具有可靠性高和灵活性强的优势，能以较高的速率生成被预测为亚稳态的材料。",
    "en_tdlr": "Fine-tuning language models for material generation shows high reliability and flexibility, allowing for the generation of metastable materials at a high rate."
}