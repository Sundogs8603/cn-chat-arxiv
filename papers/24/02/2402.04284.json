{
    "title": "PRES: Toward Scalable Memory-Based Dynamic Graph Neural Networks",
    "abstract": "Memory-based Dynamic Graph Neural Networks (MDGNNs) are a family of dynamic graph neural networks that leverage a memory module to extract, distill, and memorize long-term temporal dependencies, leading to superior performance compared to memory-less counterparts. However, training MDGNNs faces the challenge of handling entangled temporal and structural dependencies, requiring sequential and chronological processing of data sequences to capture accurate temporal patterns. During the batch training, the temporal data points within the same batch will be processed in parallel, while their temporal dependencies are neglected. This issue is referred to as temporal discontinuity and restricts the effective temporal batch size, limiting data parallelism and reducing MDGNNs' flexibility in industrial applications. This paper studies the efficient training of MDGNNs at scale, focusing on the temporal discontinuity in training MDGNNs with large temporal batch sizes. We first conduct a theoretic",
    "link": "https://arxiv.org/abs/2402.04284",
    "context": "Title: PRES: Toward Scalable Memory-Based Dynamic Graph Neural Networks\nAbstract: Memory-based Dynamic Graph Neural Networks (MDGNNs) are a family of dynamic graph neural networks that leverage a memory module to extract, distill, and memorize long-term temporal dependencies, leading to superior performance compared to memory-less counterparts. However, training MDGNNs faces the challenge of handling entangled temporal and structural dependencies, requiring sequential and chronological processing of data sequences to capture accurate temporal patterns. During the batch training, the temporal data points within the same batch will be processed in parallel, while their temporal dependencies are neglected. This issue is referred to as temporal discontinuity and restricts the effective temporal batch size, limiting data parallelism and reducing MDGNNs' flexibility in industrial applications. This paper studies the efficient training of MDGNNs at scale, focusing on the temporal discontinuity in training MDGNNs with large temporal batch sizes. We first conduct a theoretic",
    "path": "papers/24/02/2402.04284.json",
    "total_tokens": 914,
    "translated_title": "PRES: 实现可扩展的基于内存的动态图神经网络",
    "translated_abstract": "基于内存的动态图神经网络（MDGNNs）是一类动态图神经网络，利用内存模块提取、提炼和记忆长期的时间依赖关系，相比于无内存的对应物，表现出更卓越的性能。然而，训练MDGNNs面临着处理纠结的时间和结构依赖关系的挑战，需要对数据序列进行顺序和时间顺序的处理，以捕捉准确的时间模式。在批量训练中，同一批次内的时间数据点将被并行处理，而它们的时间依赖关系将被忽视。这个问题被称为时间间断，限制了有效的时间批量大小，限制了数据的并行性，并降低了MDGNNs在工业应用中的灵活性。本文研究了MDGNNs的大规模高效训练，重点关注在大的时间批量大小下训练MDGNNs时的时间间断问题。我们首先进行了理论工作。",
    "tldr": "这篇论文研究了如何实现可扩展的基于内存的动态图神经网络，并解决了训练中的时间间断问题。通过使用内存模块提取和记忆长期的时间依赖关系，MDGNNs在处理大的时间批量时表现出更好的性能和灵活性。",
    "en_tdlr": "This paper investigates the scalability of memory-based dynamic graph neural networks (MDGNNs) and addresses the issue of temporal discontinuity in training. By utilizing a memory module to extract and memorize long-term temporal dependencies, MDGNNs exhibit better performance and flexibility when processing large temporal batches."
}