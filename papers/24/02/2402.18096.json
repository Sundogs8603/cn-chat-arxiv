{
    "title": "No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization",
    "abstract": "arXiv:2402.18096v1 Announce Type: cross  Abstract: Key-Value (KV) Caching has become an essential technique for accelerating the inference speed and throughput of generative Large Language Models~(LLMs). However, the memory footprint of the KV cache poses a critical bottleneck in LLM deployment as the cache size grows with batch size and sequence length, often surpassing even the size of the model itself. Although recent methods were proposed to select and evict unimportant KV pairs from the cache to reduce memory consumption, the potential ramifications of eviction on the generative process are yet to be thoroughly examined. In this paper, we examine the detrimental impact of cache eviction and observe that unforeseen risks arise as the information contained in the KV pairs is exhaustively discarded, resulting in safety breaches, hallucinations, and context loss. Surprisingly, we find that preserving even a small amount of information contained in the evicted KV pairs via reduced prec",
    "link": "https://arxiv.org/abs/2402.18096",
    "context": "Title: No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization\nAbstract: arXiv:2402.18096v1 Announce Type: cross  Abstract: Key-Value (KV) Caching has become an essential technique for accelerating the inference speed and throughput of generative Large Language Models~(LLMs). However, the memory footprint of the KV cache poses a critical bottleneck in LLM deployment as the cache size grows with batch size and sequence length, often surpassing even the size of the model itself. Although recent methods were proposed to select and evict unimportant KV pairs from the cache to reduce memory consumption, the potential ramifications of eviction on the generative process are yet to be thoroughly examined. In this paper, we examine the detrimental impact of cache eviction and observe that unforeseen risks arise as the information contained in the KV pairs is exhaustively discarded, resulting in safety breaches, hallucinations, and context loss. Surprisingly, we find that preserving even a small amount of information contained in the evicted KV pairs via reduced prec",
    "path": "papers/24/02/2402.18096.json",
    "total_tokens": 913,
    "translated_title": "不丢弃任何令牌: 通过重要性感知混合精度量化实现可靠的KV缓存压缩",
    "translated_abstract": "键值（KV）缓存已成为加速生成大型语言模型（LLMs）推理速度和吞吐量的基本技术。然而，随着批量大小和序列长度的增长，KV缓存的内存占用成为LLM部署中的关键瓶颈，常常超过模型本身的大小。尽管最近提出了一些方法来选择和驱逐缓存中的不重要KV对以减少内存消耗，但驱逐对生成过程的潜在影响尚未得到彻底检验。在本文中，我们检查了缓存驱逐的有害影响，并观察到由于KV对中包含的信息被彻底丢弃而导致安全漏洞、幻觉和上下文丢失的不良后果。令人惊讶的是，我们发现即使通过降低精度保留被驱逐KV对中包含的一小部分信息，",
    "tldr": "通过重要性感知混合精度量化，本论文研究了KV缓存压缩中不丢弃令牌的方法，并发现保留被驱逐KV对中的一小部分信息可以避免安全漏洞、幻觉和上下文丢失。",
    "en_tdlr": "This paper investigates the method of not discarding tokens in KV cache compression through importance-aware mixed precision quantization, and finds that preserving a small portion of information in the evicted KV pairs can prevent security breaches, hallucinations, and context loss."
}