{
    "title": "Two Heads Are Better Than One: Boosting Graph Sparse Training via Semantic and Topological Awareness",
    "abstract": "Graph Neural Networks (GNNs) excel in various graph learning tasks but face computational challenges when applied to large-scale graphs. A promising solution is to remove non-essential edges to reduce the computational overheads in GNN. Previous literature generally falls into two categories: topology-guided and semantic-guided. The former maintains certain graph topological properties yet often underperforms on GNNs due to low integration with neural network training. The latter performs well at lower sparsity on GNNs but faces performance collapse at higher sparsity levels. With this in mind, we take the first step to propose a new research line and concept termed Graph Sparse Training (GST), which dynamically manipulates sparsity at the data level. Specifically, GST initially constructs a topology & semantic anchor at a low training cost, followed by performing dynamic sparse training to align the sparse graph with the anchor. We introduce the Equilibria Sparsification Principle to ",
    "link": "https://rss.arxiv.org/abs/2402.01242",
    "context": "Title: Two Heads Are Better Than One: Boosting Graph Sparse Training via Semantic and Topological Awareness\nAbstract: Graph Neural Networks (GNNs) excel in various graph learning tasks but face computational challenges when applied to large-scale graphs. A promising solution is to remove non-essential edges to reduce the computational overheads in GNN. Previous literature generally falls into two categories: topology-guided and semantic-guided. The former maintains certain graph topological properties yet often underperforms on GNNs due to low integration with neural network training. The latter performs well at lower sparsity on GNNs but faces performance collapse at higher sparsity levels. With this in mind, we take the first step to propose a new research line and concept termed Graph Sparse Training (GST), which dynamically manipulates sparsity at the data level. Specifically, GST initially constructs a topology & semantic anchor at a low training cost, followed by performing dynamic sparse training to align the sparse graph with the anchor. We introduce the Equilibria Sparsification Principle to ",
    "path": "papers/24/02/2402.01242.json",
    "total_tokens": 971,
    "translated_title": "两个头胜过一个：通过语义和拓扑意识增强图稀疏训练",
    "translated_abstract": "图神经网络在各种图学习任务中表现出色，但在应用于大规模图时面临计算挑战。一种有希望的解决方案是通过去除非必要边缘，减少GNN中的计算开销。以前的文献通常分为两类：拓扑引导和语义引导。前者保持某些图拓扑属性，但由于与神经网络训练的低集成性而在GNN上表现不佳。后者在GNN上的较低稀疏度下表现良好，但在较高稀疏度下面临性能崩溃。基于这一点，我们首次提出了一种新的研究方向和概念，即图稀疏训练（GST），该方法在数据级别动态操作稀疏性。具体而言，GST首先以较低的训练成本构建拓扑和语义锚点，然后通过执行动态稀疏训练来使稀疏图与锚点对齐。我们引入了平衡稀疏化原则来...（摘要省略）",
    "tldr": "该论文提出了一种新的研究方向和概念，即图稀疏训练（GST），通过动态稀疏训练使稀疏图与拓扑和语义锚点对齐，以降低计算开销，并在大规模图上提高图神经网络（GNN）的性能。",
    "en_tdlr": "This paper proposes a new research direction and concept called Graph Sparse Training (GST), which aligns sparse graphs with topology and semantic anchors through dynamic sparse training to reduce computational overheads and improve the performance of Graph Neural Networks (GNNs) on large-scale graphs."
}