{
    "title": "Personalized Text Generation with Fine-Grained Linguistic Control",
    "abstract": "As the text generation capabilities of large language models become increasingly prominent, recent studies have focused on controlling particular aspects of the generated text to make it more personalized. However, most research on controllable text generation focuses on controlling the content or modeling specific high-level/coarse-grained attributes that reflect authors' writing styles, such as formality, domain, or sentiment. In this paper, we focus on controlling fine-grained attributes spanning multiple linguistic dimensions, such as lexical and syntactic attributes. We introduce a novel benchmark to train generative models and evaluate their ability to generate personalized text based on multiple fine-grained linguistic attributes. We systematically investigate the performance of various large language models on our benchmark and draw insights from the factors that impact their performance. We make our code, data, and pretrained models publicly available.",
    "link": "https://arxiv.org/abs/2402.04914",
    "context": "Title: Personalized Text Generation with Fine-Grained Linguistic Control\nAbstract: As the text generation capabilities of large language models become increasingly prominent, recent studies have focused on controlling particular aspects of the generated text to make it more personalized. However, most research on controllable text generation focuses on controlling the content or modeling specific high-level/coarse-grained attributes that reflect authors' writing styles, such as formality, domain, or sentiment. In this paper, we focus on controlling fine-grained attributes spanning multiple linguistic dimensions, such as lexical and syntactic attributes. We introduce a novel benchmark to train generative models and evaluate their ability to generate personalized text based on multiple fine-grained linguistic attributes. We systematically investigate the performance of various large language models on our benchmark and draw insights from the factors that impact their performance. We make our code, data, and pretrained models publicly available.",
    "path": "papers/24/02/2402.04914.json",
    "total_tokens": 851,
    "translated_title": "细粒度语言控制下的个性化文本生成",
    "translated_abstract": "随着大型语言模型的文本生成能力越来越突出，最近的研究集中在控制生成文本的特定方面，使其更加个性化。然而，大多数关于可控文本生成的研究都集中在控制内容或建模特定的高级/粗粒度属性，如正式程度、领域或情感，以反映作者的写作风格。在本文中，我们专注于控制跨多个语言维度的细粒度属性，如词汇和句法属性。我们引入了一个新颖的基准来训练生成模型，并评估它们根据多个细粒度语言属性生成个性化文本的能力。我们系统地研究了各种大型语言模型在我们的基准上的性能，并从影响它们性能的因素中得出了一些见解。我们公开了我们的代码、数据和预训练模型。",
    "tldr": "本文研究了细粒度语言控制下的个性化文本生成，引入了一个新的基准来评估生成模型在多个细粒度语言属性上的表现，探索了影响模型性能的因素，并公开了代码、数据和预训练模型。",
    "en_tdlr": "This paper investigates personalized text generation with fine-grained linguistic control, introduces a novel benchmark to evaluate the performance of generative models based on multiple fine-grained linguistic attributes, explores factors that impact model performance, and provides code, data, and pretrained models publicly."
}