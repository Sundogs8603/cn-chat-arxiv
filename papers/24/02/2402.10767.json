{
    "title": "Inference to the Best Explanation in Large Language Models",
    "abstract": "arXiv:2402.10767v1 Announce Type: cross  Abstract: While Large Language Models (LLMs) have found success in real-world applications, their underlying explanatory process is still poorly understood. This paper proposes IBE-Eval, a framework inspired by philosophical accounts on Inference to the Best Explanation (IBE) to advance the interpretation and evaluation of LLMs' explanations. IBE-Eval estimates the plausibility of natural language explanations through a combination of explicit logical and linguistic features including: consistency, parsimony, coherence, and uncertainty. Extensive experiments are conducted on Causal Question Answering (CQA), where \\textit{IBE-Eval} is tasked to select the most plausible causal explanation amongst competing ones generated by LLMs (i.e., GPT 3.5 and Llama 2). The experiments reveal that IBE-Eval can successfully identify the best explanation with up to 77\\% accuracy ($\\approx 27\\%$ above random), improving upon a GPT 3.5-as-a-Judge baseline ($\\appr",
    "link": "https://arxiv.org/abs/2402.10767",
    "context": "Title: Inference to the Best Explanation in Large Language Models\nAbstract: arXiv:2402.10767v1 Announce Type: cross  Abstract: While Large Language Models (LLMs) have found success in real-world applications, their underlying explanatory process is still poorly understood. This paper proposes IBE-Eval, a framework inspired by philosophical accounts on Inference to the Best Explanation (IBE) to advance the interpretation and evaluation of LLMs' explanations. IBE-Eval estimates the plausibility of natural language explanations through a combination of explicit logical and linguistic features including: consistency, parsimony, coherence, and uncertainty. Extensive experiments are conducted on Causal Question Answering (CQA), where \\textit{IBE-Eval} is tasked to select the most plausible causal explanation amongst competing ones generated by LLMs (i.e., GPT 3.5 and Llama 2). The experiments reveal that IBE-Eval can successfully identify the best explanation with up to 77\\% accuracy ($\\approx 27\\%$ above random), improving upon a GPT 3.5-as-a-Judge baseline ($\\appr",
    "path": "papers/24/02/2402.10767.json",
    "total_tokens": 873,
    "translated_title": "大型语言模型中的最佳解释推断",
    "translated_abstract": "虽然大型语言模型（LLMs）在现实应用中取得了成功，但它们的基本解释过程仍然知之甚少。本文提出了IBE-Eval，这是一个受哲学关于最佳解释推断（IBE）的启发而设计的框架，旨在推进对LLMs解释的解释和评估。IBE-Eval通过结合包括一致性、简洁性、连贯性和不确定性在内的显式逻辑和语言特征来估计自然语言解释的合理性。在因果问答（CQA）领域进行了大量实验，其中IBE-Eval被要求在多个由LLMs（即GPT 3.5和Llama 2）生成的竞争性因果解释中选择最合理的因果解释。实验证明，IBE-Eval可以成功地以高达77\\%的准确率（比随机高约27%）识别最佳解释，优于GPT 3.5作为判定基线的表现。",
    "tldr": "该论文提出了一个受哲学启发设计的框架IBE-Eval，用于推进对大型语言模型解释的解释和评估，在因果问答实验中显示出高达77%的准确率。",
    "en_tdlr": "This paper introduces a framework IBE-Eval inspired by philosophy for interpreting and evaluating explanations of Large Language Models, achieving up to 77% accuracy in causal question answering experiments."
}