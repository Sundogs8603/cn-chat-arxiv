{
    "title": "Equivariant Pretrained Transformer for Unified Geometric Learning on Multi-Domain 3D Molecules",
    "abstract": "arXiv:2402.12714v1 Announce Type: new  Abstract: Pretraining on a large number of unlabeled 3D molecules has showcased superiority in various scientific applications. However, prior efforts typically focus on pretraining models on a specific domain, either proteins or small molecules, missing the opportunity to leverage the cross-domain knowledge. To mitigate this gap, we introduce Equivariant Pretrained Transformer (EPT), a novel pretraining framework designed to harmonize the geometric learning of small molecules and proteins. To be specific, EPT unifies the geometric modeling of multi-domain molecules via the block-enhanced representation that can attend a broader context of each atom. Upon transformer framework, EPT is further enhanced with E(3) equivariance to facilitate the accurate representation of 3D structures. Another key innovation of EPT is its block-level pretraining task, which allows for joint pretraining on datasets comprising both small molecules and proteins. Experim",
    "link": "https://arxiv.org/abs/2402.12714",
    "context": "Title: Equivariant Pretrained Transformer for Unified Geometric Learning on Multi-Domain 3D Molecules\nAbstract: arXiv:2402.12714v1 Announce Type: new  Abstract: Pretraining on a large number of unlabeled 3D molecules has showcased superiority in various scientific applications. However, prior efforts typically focus on pretraining models on a specific domain, either proteins or small molecules, missing the opportunity to leverage the cross-domain knowledge. To mitigate this gap, we introduce Equivariant Pretrained Transformer (EPT), a novel pretraining framework designed to harmonize the geometric learning of small molecules and proteins. To be specific, EPT unifies the geometric modeling of multi-domain molecules via the block-enhanced representation that can attend a broader context of each atom. Upon transformer framework, EPT is further enhanced with E(3) equivariance to facilitate the accurate representation of 3D structures. Another key innovation of EPT is its block-level pretraining task, which allows for joint pretraining on datasets comprising both small molecules and proteins. Experim",
    "path": "papers/24/02/2402.12714.json",
    "total_tokens": 911,
    "translated_title": "具有等变性的预训练Transformer用于多域3D分子的统一几何学习",
    "translated_abstract": "通过在大量未标记的3D分子上进行预训练已经展示出在各种科学应用中具有优势。然而，先前的努力通常集中在特定领域（蛋白质或小分子）的模型预训练上，错失了利用跨领域知识的机会。为了弥补这一差距，我们引入了等变预训练Transformer（EPT），这是一个新颖的预训练框架，旨在协调小分子和蛋白质的几何学习。具体来说，EPT通过块增强表示统一了多领域分子的几何建模，能够关注每个原子更广泛的上下文。在Transformer框架上，EPT进一步通过E(3)等变性进行增强，以促进准确表示3D结构。EPT的另一个关键创新是其块级预训练任务，这允许在包含小分子和蛋白质的数据集上进行联合预训练。",
    "tldr": "提出了具有等变性的预训练Transformer(EPT)框架，能够统一多领域分子的几何学习，通过块增强表示和E(3)等变性实现更准确的3D结构表示。",
    "en_tdlr": "Introduced Equivariant Pretrained Transformer (EPT) framework with the capability to unify geometric learning on multi-domain molecules, achieving more accurate 3D structure representation through block-enhanced representation and E(3) equivariance."
}