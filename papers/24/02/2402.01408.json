{
    "title": "Climbing the Ladder of Interpretability with Counterfactual Concept Bottleneck Models",
    "abstract": "Current deep learning models are not designed to simultaneously address three fundamental questions: predict class labels to solve a given classification task (the \"What?\"), explain task predictions (the \"Why?\"), and imagine alternative scenarios that could result in different predictions (the \"What if?\"). The inability to answer these questions represents a crucial gap in deploying reliable AI agents, calibrating human trust, and deepening human-machine interaction. To bridge this gap, we introduce CounterFactual Concept Bottleneck Models (CF-CBMs), a class of models designed to efficiently address the above queries all at once without the need to run post-hoc searches. Our results show that CF-CBMs produce: accurate predictions (the \"What?\"), simple explanations for task predictions (the \"Why?\"), and interpretable counterfactuals (the \"What if?\"). CF-CBMs can also sample or estimate the most probable counterfactual to: (i) explain the effect of concept interventions on tasks, (ii) sh",
    "link": "https://rss.arxiv.org/abs/2402.01408",
    "context": "Title: Climbing the Ladder of Interpretability with Counterfactual Concept Bottleneck Models\nAbstract: Current deep learning models are not designed to simultaneously address three fundamental questions: predict class labels to solve a given classification task (the \"What?\"), explain task predictions (the \"Why?\"), and imagine alternative scenarios that could result in different predictions (the \"What if?\"). The inability to answer these questions represents a crucial gap in deploying reliable AI agents, calibrating human trust, and deepening human-machine interaction. To bridge this gap, we introduce CounterFactual Concept Bottleneck Models (CF-CBMs), a class of models designed to efficiently address the above queries all at once without the need to run post-hoc searches. Our results show that CF-CBMs produce: accurate predictions (the \"What?\"), simple explanations for task predictions (the \"Why?\"), and interpretable counterfactuals (the \"What if?\"). CF-CBMs can also sample or estimate the most probable counterfactual to: (i) explain the effect of concept interventions on tasks, (ii) sh",
    "path": "papers/24/02/2402.01408.json",
    "total_tokens": 947,
    "translated_title": "通过反事实概念瓶颈模型攀登解释性的阶梯",
    "translated_abstract": "当前的深度学习模型没有同时解决三个基本问题的设计：预测类别标签以解决给定的分类任务（“是什么？”），解释任务预测（“为什么？”），并想象可能导致不同预测的替代情景（“如果怎样？”）。无法回答这些问题代表了部署可靠的AI代理、校准人类信任和加深人机交互的关键差距。为了弥合这一差距，我们引入了反事实概念瓶颈模型（CF-CBMs），这是一类能够高效同时解决上述查询而无需进行事后搜索的模型。我们的结果表明，CF-CBMs能够产生准确的预测（“是什么？”），对任务预测提供简单的解释（“为什么？”），以及可解释的反事实情况（“如果怎样？”）。CF-CBMs还可以对概念干预的影响进行采样或估计最可能的反事实情况，以解释事件，并优化产生多样化的反事实。",
    "tldr": "本论文提出了一种新的模型 CF-CBMs，可以同时解决深度学习模型的预测、解释和想象能力的不足，为部署可靠的AI代理、校准人类信任和加深人机交互提供了一种有效的解决方法。",
    "en_tdlr": "This paper introduces a new model called CF-CBMs, which addresses the limitations of current deep learning models in terms of prediction, explanation, and imagination, providing an effective solution for deploying reliable AI agents, calibrating human trust, and deepening human-machine interaction."
}