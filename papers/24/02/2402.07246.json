{
    "title": "Towards Generalized Inverse Reinforcement Learning",
    "abstract": "This paper studies generalized inverse reinforcement learning (GIRL) in Markov decision processes (MDPs), that is, the problem of learning the basic components of an MDP given observed behavior (policy) that might not be optimal. These components include not only the reward function and transition probability matrices, but also the action space and state space that are not exactly known but are known to belong to given uncertainty sets. We address two key challenges in GIRL: first, the need to quantify the discrepancy between the observed policy and the underlying optimal policy; second, the difficulty of mathematically characterizing the underlying optimal policy when the basic components of an MDP are unobservable or partially observable. Then, we propose the mathematical formulation for GIRL and develop a fast heuristic algorithm. Numerical results on both finite and infinite state problems show the merit of our formulation and algorithm.",
    "link": "https://arxiv.org/abs/2402.07246",
    "context": "Title: Towards Generalized Inverse Reinforcement Learning\nAbstract: This paper studies generalized inverse reinforcement learning (GIRL) in Markov decision processes (MDPs), that is, the problem of learning the basic components of an MDP given observed behavior (policy) that might not be optimal. These components include not only the reward function and transition probability matrices, but also the action space and state space that are not exactly known but are known to belong to given uncertainty sets. We address two key challenges in GIRL: first, the need to quantify the discrepancy between the observed policy and the underlying optimal policy; second, the difficulty of mathematically characterizing the underlying optimal policy when the basic components of an MDP are unobservable or partially observable. Then, we propose the mathematical formulation for GIRL and develop a fast heuristic algorithm. Numerical results on both finite and infinite state problems show the merit of our formulation and algorithm.",
    "path": "papers/24/02/2402.07246.json",
    "total_tokens": 929,
    "translated_title": "朝着广义逆强化学习的方向",
    "translated_abstract": "本文研究了在马尔科夫决策过程（MDP）中的广义逆强化学习（GIRL），即在观察到的行为（策略）可能不是最优的情况下学习MDP的基本组成部分的问题。这些组成部分不仅包括奖励函数和转移概率矩阵，还包括动作空间和状态空间，虽然不完全已知但知道属于给定不确定性集合。我们解决了GIRL中的两个关键挑战：首先，需要量化观察策略与潜在最优策略之间的差异；其次，当MDP的基本组成部分不可观察或部分可观察时，数学上描述潜在最优策略的困难。然后，我们提出了GIRL的数学公式并开发了一种快速的启发式算法。在有限状态和无限状态问题上的数值结果显示了我们公式和算法的优点。",
    "tldr": "本文研究了广义逆强化学习（GIRL）在马尔科夫决策过程（MDP）中的应用，提出了一种解决观察策略与最优策略之间差异以及不完全可观察情况下数学表述最优策略的方法，并开发了一种快速的启发式算法，数值结果显示其有效性。",
    "en_tdlr": "This paper studies the application of generalized inverse reinforcement learning (GIRL) in Markov decision processes (MDPs), proposes a method to address the discrepancy between observed policy and optimal policy as well as mathematically characterize the optimal policy in partially observable scenarios, and develops a fast heuristic algorithm, with numerical results demonstrating its effectiveness."
}