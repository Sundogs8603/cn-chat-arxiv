{
    "title": "Adaptive Primal-Dual Method for Safe Reinforcement Learning",
    "abstract": "Primal-dual methods have a natural application in Safe Reinforcement Learning (SRL), posed as a constrained policy optimization problem. In practice however, applying primal-dual methods to SRL is challenging, due to the inter-dependency of the learning rate (LR) and Lagrangian multipliers (dual variables) each time an embedded unconstrained RL problem is solved. In this paper, we propose, analyze and evaluate adaptive primal-dual (APD) methods for SRL, where two adaptive LRs are adjusted to the Lagrangian multipliers so as to optimize the policy in each iteration. We theoretically establish the convergence, optimality and feasibility of the APD algorithm. Finally, we conduct numerical evaluation of the practical APD algorithm with four well-known environments in Bullet-Safey-Gym employing two state-of-the-art SRL algorithms: PPO-Lagrangian and DDPG-Lagrangian. All experiments show that the practical APD algorithm outperforms (or achieves comparable performance) and attains more stable",
    "link": "https://arxiv.org/abs/2402.00355",
    "context": "Title: Adaptive Primal-Dual Method for Safe Reinforcement Learning\nAbstract: Primal-dual methods have a natural application in Safe Reinforcement Learning (SRL), posed as a constrained policy optimization problem. In practice however, applying primal-dual methods to SRL is challenging, due to the inter-dependency of the learning rate (LR) and Lagrangian multipliers (dual variables) each time an embedded unconstrained RL problem is solved. In this paper, we propose, analyze and evaluate adaptive primal-dual (APD) methods for SRL, where two adaptive LRs are adjusted to the Lagrangian multipliers so as to optimize the policy in each iteration. We theoretically establish the convergence, optimality and feasibility of the APD algorithm. Finally, we conduct numerical evaluation of the practical APD algorithm with four well-known environments in Bullet-Safey-Gym employing two state-of-the-art SRL algorithms: PPO-Lagrangian and DDPG-Lagrangian. All experiments show that the practical APD algorithm outperforms (or achieves comparable performance) and attains more stable",
    "path": "papers/24/02/2402.00355.json",
    "total_tokens": 973,
    "translated_title": "自适应原始-对偶方法用于安全强化学习",
    "translated_abstract": "原始-对偶方法在安全强化学习中有自然应用，被提出作为一个约束策略优化问题。然而，在实践中，将原始-对偶方法应用于安全强化学习是具有挑战性的，因为每次解决嵌入的无约束强化学习问题时，学习速率（LR）和拉格朗日乘子（对偶变量）之间存在相互依赖。在本文中，我们提出、分析和评估了适应性原始-对偶（APD）方法用于安全强化学习，在每次迭代中，调整两个自适应LR以使之优化策略。我们从理论上建立了APD算法的收敛性、最优性和可行性。最后，我们使用Bullet-Safey-Gym中的四个知名环境，利用两个先进的安全强化学习算法（PPO-Lagrangian和DDPG-Lagrangian）对实际APD算法进行了数值评估。所有实验表明，实际APD算法的性能优于（或达到可比较的性能），并且具有更稳定的效果。",
    "tldr": "本论文提出了一种自适应原始-对偶方法用于安全强化学习，通过调整自适应学习速率以优化策略，实现了算法的收敛性、最优性和可行性。实验结果表明，该方法在安全强化学习中具有更好的性能和稳定性。",
    "en_tdlr": "This paper proposes an adaptive primal-dual method for safe reinforcement learning, which adjusts the adaptive learning rate to optimize policies and achieves convergence, optimality, and feasibility. Experimental results demonstrate that this method outperforms and attains more stability in safe reinforcement learning."
}