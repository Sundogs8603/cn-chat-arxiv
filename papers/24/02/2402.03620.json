{
    "title": "Self-Discover: Large Language Models Self-Compose Reasoning Structures",
    "abstract": "We introduce SELF-DISCOVER, a general framework for LLMs to self-discover the task-intrinsic reasoning structures to tackle complex reasoning problems that are challenging for typical prompting methods. Core to the framework is a self-discovery process where LLMs select multiple atomic reasoning modules such as critical thinking and step-by-step thinking, and compose them into an explicit reasoning structure for LLMs to follow during decoding. SELF-DISCOVER substantially improves GPT-4 and PaLM 2's performance on challenging reasoning benchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, by as much as 32% compared to Chain of Thought (CoT). Furthermore, SELF-DISCOVER outperforms inference-intensive methods such as CoT-Self-Consistency by more than 20%, while requiring 10-40x fewer inference compute. Finally, we show that the self-discovered reasoning structures are universally applicable across model families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share ",
    "link": "https://arxiv.org/abs/2402.03620",
    "context": "Title: Self-Discover: Large Language Models Self-Compose Reasoning Structures\nAbstract: We introduce SELF-DISCOVER, a general framework for LLMs to self-discover the task-intrinsic reasoning structures to tackle complex reasoning problems that are challenging for typical prompting methods. Core to the framework is a self-discovery process where LLMs select multiple atomic reasoning modules such as critical thinking and step-by-step thinking, and compose them into an explicit reasoning structure for LLMs to follow during decoding. SELF-DISCOVER substantially improves GPT-4 and PaLM 2's performance on challenging reasoning benchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, by as much as 32% compared to Chain of Thought (CoT). Furthermore, SELF-DISCOVER outperforms inference-intensive methods such as CoT-Self-Consistency by more than 20%, while requiring 10-40x fewer inference compute. Finally, we show that the self-discovered reasoning structures are universally applicable across model families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share ",
    "path": "papers/24/02/2402.03620.json",
    "total_tokens": 909,
    "translated_title": "自我发现：大型语言模型自动构建推理结构",
    "translated_abstract": "我们引入了SELF-DISCOVER，一个通用框架，用于使LLM自主发现任务内在的推理结构，以应对对于典型提示方法而言具有挑战性的复杂推理问题。该框架的核心是一个自我发现过程，在这个过程中，LLM选择多个原子推理模块，如批判性思维和逐步思考，并将它们组合成LLM在解码过程中遵循的明确推理结构。SELF-DISCOVER在具有挑战性的推理基准测试中，如BigBench-Hard、基于代理的推理和MATH上，相较于Chain of Thought (CoT)的性能提升高达32%。此外，SELF-DISCOVER在需要10-40倍较少推理计算的情况下，较CoT-Self-Consistency等推理密集方法的表现更好，超过20%。最后，我们证明了自我发现的推理结构在模型家族之间是普遍适用的：从PaLM 2-L到GPT-4，从GPT-4到Llama2，并分享了",
    "tldr": "SELF-DISCOVER是一个通用框架，能让大型语言模型自主发现任务内在的推理结构，显著提升了复杂推理问题的解决性能，并在推理计算方面取得更好的效果。"
}