{
    "title": "Rethinking Propagation for Unsupervised Graph Domain Adaptation",
    "abstract": "Unsupervised Graph Domain Adaptation (UGDA) aims to transfer knowledge from a labelled source graph to an unlabelled target graph in order to address the distribution shifts between graph domains. Previous works have primarily focused on aligning data from the source and target graph in the representation space learned by graph neural networks (GNNs). However, the inherent generalization capability of GNNs has been largely overlooked. Motivated by our empirical analysis, we reevaluate the role of GNNs in graph domain adaptation and uncover the pivotal role of the propagation process in GNNs for adapting to different graph domains. We provide a comprehensive theoretical analysis of UGDA and derive a generalization bound for multi-layer GNNs. By formulating GNN Lipschitz for k-layer GNNs, we show that the target risk bound can be tighter by removing propagation layers in source graph and stacking multiple propagation layers in target graph. Based on the empirical and theoretical analysis",
    "link": "https://arxiv.org/abs/2402.05660",
    "context": "Title: Rethinking Propagation for Unsupervised Graph Domain Adaptation\nAbstract: Unsupervised Graph Domain Adaptation (UGDA) aims to transfer knowledge from a labelled source graph to an unlabelled target graph in order to address the distribution shifts between graph domains. Previous works have primarily focused on aligning data from the source and target graph in the representation space learned by graph neural networks (GNNs). However, the inherent generalization capability of GNNs has been largely overlooked. Motivated by our empirical analysis, we reevaluate the role of GNNs in graph domain adaptation and uncover the pivotal role of the propagation process in GNNs for adapting to different graph domains. We provide a comprehensive theoretical analysis of UGDA and derive a generalization bound for multi-layer GNNs. By formulating GNN Lipschitz for k-layer GNNs, we show that the target risk bound can be tighter by removing propagation layers in source graph and stacking multiple propagation layers in target graph. Based on the empirical and theoretical analysis",
    "path": "papers/24/02/2402.05660.json",
    "total_tokens": 885,
    "translated_title": "重新思考无监督图领域适应中的传播问题",
    "translated_abstract": "无监督图领域适应（UGDA）旨在将标记源图的知识传输到未标记的目标图中，以解决图领域之间的分布偏移问题。以往的研究主要集中在通过图神经网络（GNN）学习的表示空间中对源图和目标图的数据进行对齐。然而，GNN的内在泛化能力很大程度上被忽视了。在经验分析的基础上，我们重新评估了GNN在图领域适应中的作用，揭示了传播过程在GNN中适应不同图领域中的关键作用。我们对UGDA进行了全面的理论分析，并推导出多层GNN的泛化界限。通过将GNN Lipschitz应用于k层GNNs，我们证明通过在源图中删除传播层并在目标图中堆叠多个传播层，可以使目标风险界限更紧密。基于实证和理论分析结果",
    "tldr": "通过重新评估GNN在图领域适应中的作用，本论文揭示了传播过程对于适应不同图领域至关重要，并通过理论分析提供了多层GNN的泛化界限的证明。",
    "en_tdlr": "This paper reevaluates the role of propagation in graph neural networks (GNNs) for adapting to different graph domains in unsupervised graph domain adaptation (UGDA), and provides a theoretical analysis and proof of the generalization bound for multi-layer GNNs."
}