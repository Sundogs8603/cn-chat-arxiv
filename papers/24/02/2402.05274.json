{
    "title": "Convergence for Natural Policy Gradient on Infinite-State Average-Reward Markov Decision Processes",
    "abstract": "Infinite-state Markov Decision Processes (MDPs) are essential in modeling and optimizing a wide variety of engineering problems. In the reinforcement learning (RL) context, a variety of algorithms have been developed to learn and optimize these MDPs. At the heart of many popular policy-gradient based learning algorithms, such as natural actor-critic, TRPO, and PPO, lies the Natural Policy Gradient (NPG) algorithm. Convergence results for these RL algorithms rest on convergence results for the NPG algorithm. However, all existing results on the convergence of the NPG algorithm are limited to finite-state settings.   We prove the first convergence rate bound for the NPG algorithm for infinite-state average-reward MDPs, proving a $O(1/\\sqrt{T})$ convergence rate, if the NPG algorithm is initialized with a good initial policy. Moreover, we show that in the context of a large class of queueing MDPs, the MaxWeight policy suffices to satisfy our initial-policy requirement and achieve a $O(1/\\",
    "link": "https://arxiv.org/abs/2402.05274",
    "context": "Title: Convergence for Natural Policy Gradient on Infinite-State Average-Reward Markov Decision Processes\nAbstract: Infinite-state Markov Decision Processes (MDPs) are essential in modeling and optimizing a wide variety of engineering problems. In the reinforcement learning (RL) context, a variety of algorithms have been developed to learn and optimize these MDPs. At the heart of many popular policy-gradient based learning algorithms, such as natural actor-critic, TRPO, and PPO, lies the Natural Policy Gradient (NPG) algorithm. Convergence results for these RL algorithms rest on convergence results for the NPG algorithm. However, all existing results on the convergence of the NPG algorithm are limited to finite-state settings.   We prove the first convergence rate bound for the NPG algorithm for infinite-state average-reward MDPs, proving a $O(1/\\sqrt{T})$ convergence rate, if the NPG algorithm is initialized with a good initial policy. Moreover, we show that in the context of a large class of queueing MDPs, the MaxWeight policy suffices to satisfy our initial-policy requirement and achieve a $O(1/\\",
    "path": "papers/24/02/2402.05274.json",
    "total_tokens": 1034,
    "translated_title": "无穷状态平均奖励马尔可夫决策过程中的自然策略梯度收敛性",
    "translated_abstract": "无穷状态马尔可夫决策过程（MDPs）在建模和优化各种工程问题中起着重要作用。在强化学习（RL）环境中，已经开发了各种算法来学习和优化这些MDPs。在许多流行的基于策略梯度的学习算法中，如自然演员-评论家、TRPO和PPO，都基于自然策略梯度（NPG）算法。这些RL算法的收敛结果建立在NPG算法的收敛结果上。然而，所有现有的NPG算法收敛性结果均仅限于有限状态设置。我们证明了NPG算法在无穷状态平均奖励MDPs中的首个收敛速率界限，证明了$O(1/\\sqrt{T})$的收敛速率，如果NPG算法以良好的初始策略进行初始化。此外，我们还展示了在大类排队MDPs的情况下，MaxWeight策略足够满足我们的初始策略要求，并实现了$O(1/...",
    "tldr": "本文针对无穷状态平均奖励马尔可夫决策过程中的自然策略梯度（NPG）算法进行了收敛性分析，证明了在良好的初始策略条件下，该算法能够以$O(1/\\sqrt{T})$的收敛速率收敛。同时，对于一类大类排队MDPs，MaxWeight策略足以满足初始策略要求并实现收敛。"
}