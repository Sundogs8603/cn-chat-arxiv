{
    "title": "Multi-Lingual Malaysian Embedding: Leveraging Large Language Models for Semantic Representations",
    "abstract": "In this work, we present a comprehensive exploration of finetuning Malaysian language models, specifically Llama2 and Mistral, on embedding tasks involving negative and positive pairs. We release two distinct models tailored for Semantic Similarity and Retrieval-Augmented Generation (RAG).   For Semantic Similarity, our 600 million parameter Llama2 model outperforms OpenAI text-embedding-ada-002 across all recall@k metrics for b.cari.com.my, c.cari.com.my, Malay news, and Malaysian Twitter test sets.   In the realm of RAG models, our approach proves competitive with OpenAI text-embedding-ada-002 in the Malaysian context. Notably, our 2 billion parameter Llama2 model achieves superior Recall@5, Recall@10 for the \"Melayu\" keyword research papers dataset and excels in Recall@3, Recall@5, and Recall@10 for the lom.agc.gov.my dataset.   These findings underscore the effectiveness of our finetuning strategy and highlight the performance gains in both Semantic Similarity and RAG tasks.   All ",
    "link": "https://arxiv.org/abs/2402.03053",
    "context": "Title: Multi-Lingual Malaysian Embedding: Leveraging Large Language Models for Semantic Representations\nAbstract: In this work, we present a comprehensive exploration of finetuning Malaysian language models, specifically Llama2 and Mistral, on embedding tasks involving negative and positive pairs. We release two distinct models tailored for Semantic Similarity and Retrieval-Augmented Generation (RAG).   For Semantic Similarity, our 600 million parameter Llama2 model outperforms OpenAI text-embedding-ada-002 across all recall@k metrics for b.cari.com.my, c.cari.com.my, Malay news, and Malaysian Twitter test sets.   In the realm of RAG models, our approach proves competitive with OpenAI text-embedding-ada-002 in the Malaysian context. Notably, our 2 billion parameter Llama2 model achieves superior Recall@5, Recall@10 for the \"Melayu\" keyword research papers dataset and excels in Recall@3, Recall@5, and Recall@10 for the lom.agc.gov.my dataset.   These findings underscore the effectiveness of our finetuning strategy and highlight the performance gains in both Semantic Similarity and RAG tasks.   All ",
    "path": "papers/24/02/2402.03053.json",
    "total_tokens": 958,
    "translated_title": "多语言马来西亚嵌入：利用大型语言模型进行语义表示",
    "translated_abstract": "在这项工作中，我们对Llama2和Mistral两种马来西亚语言模型进行了全面探索，并在涉及负正组对的嵌入任务中进行了微调。我们发布了两个专门针对语义相似性和检索辅助生成（RAG）的模型。在语义相似性方面，我们的6亿参数Llama2模型在b.cari.com.my、c.cari.com.my、马来西亚新闻和马来西亚Twitter测试集的所有recall@k指标上都优于OpenAI的text-embedding-ada-002。在RAG模型领域，我们的方法在马来西亚环境中与OpenAI的text-embedding-ada-002相竞争。值得注意的是，我们的20亿参数Llama2模型在“Melayu”关键词研究论文数据集的Recall@5、Recall@10上取得了卓越表现，并在lom.agc.gov.my数据集的Recall@3、Recall@5和Recall@10上表现出色。这些发现强调了我们的微调策略的有效性，并突显了在语义相似性和RAG任务中的性能提升。",
    "tldr": "本文提出了一种基于大型语言模型的多语言马来西亚嵌入方法，通过微调Llama2和Mistral模型，在语义相似性和RAG任务中取得了显著的性能提升。",
    "en_tdlr": "This paper presents a method for multi-lingual Malaysian embedding using large language models, specifically Llama2 and Mistral. Through finetuning, the models achieve significant performance improvements in semantic similarity and retrieval-augmented generation (RAG) tasks."
}