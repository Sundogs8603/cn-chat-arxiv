{
    "title": "GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis",
    "abstract": "arXiv:2402.13494v1 Announce Type: new  Abstract: Large Language Models (LLMs) face threats from unsafe prompts. Existing methods for detecting unsafe prompts are primarily online moderation APIs or finetuned LLMs. These strategies, however, often require extensive and resource-intensive data collection and training processes. In this study, we propose GradSafe, which effectively detects unsafe prompts by scrutinizing the gradients of safety-critical parameters in LLMs. Our methodology is grounded in a pivotal observation: the gradients of an LLM's loss for unsafe prompts paired with compliance response exhibit similar patterns on certain safety-critical parameters. In contrast, safe prompts lead to markedly different gradient patterns. Building on this observation, GradSafe analyzes the gradients from prompts (paired with compliance responses) to accurately detect unsafe prompts. We show that GradSafe, applied to Llama-2 without further training, outperforms Llama Guard, despite its ex",
    "link": "https://arxiv.org/abs/2402.13494",
    "context": "Title: GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis\nAbstract: arXiv:2402.13494v1 Announce Type: new  Abstract: Large Language Models (LLMs) face threats from unsafe prompts. Existing methods for detecting unsafe prompts are primarily online moderation APIs or finetuned LLMs. These strategies, however, often require extensive and resource-intensive data collection and training processes. In this study, we propose GradSafe, which effectively detects unsafe prompts by scrutinizing the gradients of safety-critical parameters in LLMs. Our methodology is grounded in a pivotal observation: the gradients of an LLM's loss for unsafe prompts paired with compliance response exhibit similar patterns on certain safety-critical parameters. In contrast, safe prompts lead to markedly different gradient patterns. Building on this observation, GradSafe analyzes the gradients from prompts (paired with compliance responses) to accurately detect unsafe prompts. We show that GradSafe, applied to Llama-2 without further training, outperforms Llama Guard, despite its ex",
    "path": "papers/24/02/2402.13494.json",
    "total_tokens": 837,
    "translated_title": "GradSafe: 通过安全关键梯度分析检测LLMs中的不安全提示",
    "translated_abstract": "大型语言模型（LLMs）面临来自不安全提示的威胁。现有的检测不安全提示的方法主要是在线内容审核API或微调LLMs。然而，这些策略通常需要大量和资源密集型的数据收集和训练过程。在这项研究中，我们提出了GradSafe，通过仔细审查LLMs中的安全关键参数的梯度有效地检测不安全提示。我们的方法基于一个关键观察：LLMs对于与合规响应配对的不安全提示的损失的梯度在某些安全关键参数上表现出相似的模式。相比之下，安全提示导致明显不同的梯度模式。基于这一观察，GradSafe分析来自提示（与合规响应配对）的梯度以准确地检测不安全提示。我们展示了，应用于Llama-2的GradSafe，无需进一步训练即可胜过Llama Guard。",
    "tldr": "GradSafe通过分析LLMs中关键安全参数的梯度，有效检测不安全提示，无需额外训练即可优于现有方法。",
    "en_tdlr": "GradSafe effectively detects unsafe prompts in LLMs by analyzing gradients of safety-critical parameters, outperforming existing methods without the need for additional training."
}