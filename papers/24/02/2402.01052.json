{
    "title": "Weakly Convex Regularisers for Inverse Problems: Convergence of Critical Points and Primal-Dual Optimisation",
    "abstract": "Variational regularisation is the primary method for solving inverse problems, and recently there has been considerable work leveraging deeply learned regularisation for enhanced performance. However, few results exist addressing the convergence of such regularisation, particularly within the context of critical points as opposed to global minima. In this paper, we present a generalised formulation of convergent regularisation in terms of critical points, and show that this is achieved by a class of weakly convex regularisers. We prove convergence of the primal-dual hybrid gradient method for the associated variational problem, and, given a Kurdyka-Lojasiewicz condition, an $\\mathcal{O}(\\log{k}/k)$ ergodic convergence rate. Finally, applying this theory to learned regularisation, we prove universal approximation for input weakly convex neural networks (IWCNN), and show empirically that IWCNNs can lead to improved performance of learned adversarial regularisers for computed tomography (",
    "link": "https://rss.arxiv.org/abs/2402.01052",
    "context": "Title: Weakly Convex Regularisers for Inverse Problems: Convergence of Critical Points and Primal-Dual Optimisation\nAbstract: Variational regularisation is the primary method for solving inverse problems, and recently there has been considerable work leveraging deeply learned regularisation for enhanced performance. However, few results exist addressing the convergence of such regularisation, particularly within the context of critical points as opposed to global minima. In this paper, we present a generalised formulation of convergent regularisation in terms of critical points, and show that this is achieved by a class of weakly convex regularisers. We prove convergence of the primal-dual hybrid gradient method for the associated variational problem, and, given a Kurdyka-Lojasiewicz condition, an $\\mathcal{O}(\\log{k}/k)$ ergodic convergence rate. Finally, applying this theory to learned regularisation, we prove universal approximation for input weakly convex neural networks (IWCNN), and show empirically that IWCNNs can lead to improved performance of learned adversarial regularisers for computed tomography (",
    "path": "papers/24/02/2402.01052.json",
    "total_tokens": 937,
    "translated_title": "弱凸正则化器在逆问题中的收敛性：临界点和原始-对偶优化的收敛",
    "translated_abstract": "变分正则化是解决逆问题的主要方法，最近有很多研究利用深度学习的正则化方法来提高性能。然而，在解决这种正则化收敛性的问题上，很少有关于临界点收敛性的结果，而非全局极小值点的收敛性。本文提出了一种关于临界点收敛性的一般化公式，并证明了这是通过一类弱凸正则化器实现的。我们证明了与相关变分问题相关的原始-对偶混合梯度方法的收敛性，并在给定Kurdyka-Lojasiewicz条件的情况下，证明了O(log(k)/k)的遗传收敛速度。最后，将这个理论应用于学习的正则化中，我们证明了输入为弱凸神经网络（IWCNN）的通用逼近性，并通过实验证明，IWCNN可以提高计算机层析成像中学习对抗性正则化器的性能。",
    "tldr": "本文提出了一种关于逆问题的弱凸正则化器的收敛性问题的一般化公式，并证明了通过一类弱凸正则化器的实现可以达到收敛，并应用于学习的正则化中实现了对计算机层析成像中学习对抗性正则化器性能的提高。",
    "en_tdlr": "This paper presents a generalized formulation for the convergence of weakly convex regularizers for inverse problems and proves convergence can be achieved through a class of weakly convex regularizers. The theory is applied to learned regularizations and shows improved performance for adversarial regularizers in computed tomography."
}