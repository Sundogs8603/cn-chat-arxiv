{
    "title": "Why Does Differential Privacy with Large Epsilon Defend Against Practical Membership Inference Attacks?",
    "abstract": "arXiv:2402.09540v1 Announce Type: cross  Abstract: For small privacy parameter $\\epsilon$, $\\epsilon$-differential privacy (DP) provides a strong worst-case guarantee that no membership inference attack (MIA) can succeed at determining whether a person's data was used to train a machine learning model. The guarantee of DP is worst-case because: a) it holds even if the attacker already knows the records of all but one person in the data set; and b) it holds uniformly over all data sets. In practical applications, such a worst-case guarantee may be overkill: practical attackers may lack exact knowledge of (nearly all of) the private data, and our data set might be easier to defend, in some sense, than the worst-case data set. Such considerations have motivated the industrial deployment of DP models with large privacy parameter (e.g. $\\epsilon \\geq 7$), and it has been observed empirically that DP with large $\\epsilon$ can successfully defend against state-of-the-art MIAs. Existing DP the",
    "link": "https://arxiv.org/abs/2402.09540",
    "context": "Title: Why Does Differential Privacy with Large Epsilon Defend Against Practical Membership Inference Attacks?\nAbstract: arXiv:2402.09540v1 Announce Type: cross  Abstract: For small privacy parameter $\\epsilon$, $\\epsilon$-differential privacy (DP) provides a strong worst-case guarantee that no membership inference attack (MIA) can succeed at determining whether a person's data was used to train a machine learning model. The guarantee of DP is worst-case because: a) it holds even if the attacker already knows the records of all but one person in the data set; and b) it holds uniformly over all data sets. In practical applications, such a worst-case guarantee may be overkill: practical attackers may lack exact knowledge of (nearly all of) the private data, and our data set might be easier to defend, in some sense, than the worst-case data set. Such considerations have motivated the industrial deployment of DP models with large privacy parameter (e.g. $\\epsilon \\geq 7$), and it has been observed empirically that DP with large $\\epsilon$ can successfully defend against state-of-the-art MIAs. Existing DP the",
    "path": "papers/24/02/2402.09540.json",
    "total_tokens": 980,
    "translated_title": "为什么具有较大ε的差分隐私可以防御实际成员推理攻击？",
    "translated_abstract": "对于较小的隐私参数ε，ε-差分隐私（DP）提供了一个强大的最坏情况保证，即没有成员推理攻击（MIA）能够成功确定一个人的数据是否被用于训练机器学习模型。DP的保证是最坏情况下的，因为：a）即使攻击者已经知道数据集中除一个人的记录之外的所有记录；b）它在所有数据集上均匀适用。在实际应用中，这样的最坏情况保证可能过于严格：实际攻击者可能缺乏（几乎所有）私有数据的精确知识，并且我们的数据集可能在某种意义上比最坏情况的数据集更容易被防御。这些考虑推动了具有大的隐私参数（例如ε≥7）的DP模型的工业部署，并且经验上观察到具有大ε的DP可以成功防御最先进的MIA。现有的DP模型研究一般集中于小ε，因此尚不清楚为什么具有较大ε的DP可以防御实际成员推理攻击。",
    "tldr": "本论文研究了为什么具有较大ε的差分隐私可以防御实际成员推理攻击，因为实际攻击者可能缺乏准确的私有数据知识，并且在实际应用中，数据集可能相对容易被防御。",
    "en_tdlr": "This paper investigates why differential privacy with large epsilon can defend against practical membership inference attacks, as practical attackers may lack precise knowledge of private data and datasets may be relatively easier to defend in real-world applications."
}