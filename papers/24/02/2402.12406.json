{
    "title": "Teacher as a Lenient Expert: Teacher-Agnostic Data-Free Knowledge Distillation",
    "abstract": "arXiv:2402.12406v1 Announce Type: cross  Abstract: Data-free knowledge distillation (DFKD) aims to distill pretrained knowledge to a student model with the help of a generator without using original data. In such data-free scenarios, achieving stable performance of DFKD is essential due to the unavailability of validation data. Unfortunately, this paper has discovered that existing DFKD methods are quite sensitive to different teacher models, occasionally showing catastrophic failures of distillation, even when using well-trained teacher models. Our observation is that the generator in DFKD is not always guaranteed to produce precise yet diverse samples using the existing representative strategy of minimizing both class-prior and adversarial losses. Through our empirical study, we focus on the fact that class-prior not only decreases the diversity of generated samples, but also cannot completely address the problem of generating unexpectedly low-quality samples depending on teacher mod",
    "link": "https://arxiv.org/abs/2402.12406",
    "context": "Title: Teacher as a Lenient Expert: Teacher-Agnostic Data-Free Knowledge Distillation\nAbstract: arXiv:2402.12406v1 Announce Type: cross  Abstract: Data-free knowledge distillation (DFKD) aims to distill pretrained knowledge to a student model with the help of a generator without using original data. In such data-free scenarios, achieving stable performance of DFKD is essential due to the unavailability of validation data. Unfortunately, this paper has discovered that existing DFKD methods are quite sensitive to different teacher models, occasionally showing catastrophic failures of distillation, even when using well-trained teacher models. Our observation is that the generator in DFKD is not always guaranteed to produce precise yet diverse samples using the existing representative strategy of minimizing both class-prior and adversarial losses. Through our empirical study, we focus on the fact that class-prior not only decreases the diversity of generated samples, but also cannot completely address the problem of generating unexpectedly low-quality samples depending on teacher mod",
    "path": "papers/24/02/2402.12406.json",
    "total_tokens": 829,
    "translated_title": "教师作为宽容的专家：不依赖于教师的无数据知识蒸馏",
    "translated_abstract": "无数据知识蒸馏（DFKD）旨在在不使用原始数据的情况下，借助生成器将预训练知识蒸馏给学生模型。在这种无数据情况下，由于验证数据不可用，实现DFKD的稳定性是必不可少的。不幸的是，本文发现现有的DFKD方法对不同的教师模型非常敏感，有时即使使用训练良好的教师模型也会出现蒸馏的灾难性失败。我们的观察是DFKD中的生成器并不总是保证使用现有的旨在最小化类先验和对抗损失的代表性策略产生精确而多样化的样本。通过我们的实证研究，我们关注的事实是类先验不仅减少了生成样本的多样性，还不能完全解决根据教师模型生成意外低质量样本的问题。",
    "tldr": "该论文发现现有的无数据知识蒸馏方法对不同的教师模型非常敏感，生成的样本可能出现质量问题。"
}