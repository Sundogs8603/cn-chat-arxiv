{
    "title": "Leveraging Diverse Modeling Contexts with Collaborating Learning for Neural Machine Translation",
    "abstract": "arXiv:2402.18428v1 Announce Type: new  Abstract: Autoregressive (AR) and Non-autoregressive (NAR) models are two types of generative models for Neural Machine Translation (NMT). AR models predict tokens in a word-by-word manner and can effectively capture the distribution of real translations. NAR models predict tokens by extracting bidirectional contextual information which can improve the inference speed but they suffer from performance degradation. Previous works utilized AR models to enhance NAR models by reducing the training data's complexity or incorporating the global information into AR models by virtue of NAR models. However, those investigated methods only take advantage of the contextual information of a single type of model while neglecting the diversity in the contextual information that can be provided by different types of models. In this paper, we propose a novel generic collaborative learning method, DCMCL, where AR and NAR models are treated as collaborators instead ",
    "link": "https://arxiv.org/abs/2402.18428",
    "context": "Title: Leveraging Diverse Modeling Contexts with Collaborating Learning for Neural Machine Translation\nAbstract: arXiv:2402.18428v1 Announce Type: new  Abstract: Autoregressive (AR) and Non-autoregressive (NAR) models are two types of generative models for Neural Machine Translation (NMT). AR models predict tokens in a word-by-word manner and can effectively capture the distribution of real translations. NAR models predict tokens by extracting bidirectional contextual information which can improve the inference speed but they suffer from performance degradation. Previous works utilized AR models to enhance NAR models by reducing the training data's complexity or incorporating the global information into AR models by virtue of NAR models. However, those investigated methods only take advantage of the contextual information of a single type of model while neglecting the diversity in the contextual information that can be provided by different types of models. In this paper, we propose a novel generic collaborative learning method, DCMCL, where AR and NAR models are treated as collaborators instead ",
    "path": "papers/24/02/2402.18428.json",
    "total_tokens": 842,
    "translated_title": "利用协同学习在神经机器翻译中利用多样化建模上下文",
    "translated_abstract": "自回归（AR）和非自回归（NAR）模型是神经机器翻译（NMT）的两种生成模型。AR模型以逐词方式预测令牌，可以有效捕捉真实翻译的分布。NAR模型通过提取双向上下文信息来预测令牌，可以提高推理速度，但它们会遭受性能下降。 以往的研究利用AR模型来改进NAR模型，通过减少训练数据的复杂性或通过NAR模型将全局信息融入AR模型。然而，这些方法仅利用单一类型模型的上下文信息，忽视了不同类型模型可以提供的上下文信息的多样性。在本文中，我们提出了一种新颖的通用协作学习方法，DCMCL，其中将AR和NAR模型视为合作者。",
    "tldr": "提出了一种新颖的通用协作学习方法DCMCL，在神经机器翻译中利用多样化建模上下文，将自回归（AR）和非自回归（NAR）模型作为合作者。",
    "en_tdlr": "Proposed a novel generic collaborative learning method DCMCL, leveraging diverse modeling contexts with collaborating Autoregressive (AR) and Non-autoregressive (NAR) models for Neural Machine Translation (NMT)."
}