{
    "title": "Lifelong Benchmarks: Efficient Model Evaluation in an Era of Rapid Progress",
    "abstract": "arXiv:2402.19472v1 Announce Type: new  Abstract: Standardized benchmarks drive progress in machine learning. However, with repeated testing, the risk of overfitting grows as algorithms over-exploit benchmark idiosyncrasies. In our work, we seek to mitigate this challenge by compiling ever-expanding large-scale benchmarks called Lifelong Benchmarks. As exemplars of our approach, we create Lifelong-CIFAR10 and Lifelong-ImageNet, containing (for now) 1.69M and 1.98M test samples, respectively. While reducing overfitting, lifelong benchmarks introduce a key challenge: the high cost of evaluating a growing number of models across an ever-expanding sample set. To address this challenge, we also introduce an efficient evaluation framework: Sort \\& Search (S&S), which reuses previously evaluated models by leveraging dynamic programming algorithms to selectively rank and sub-select test samples, enabling cost-effective lifelong benchmarking. Extensive empirical evaluations across 31,000 models ",
    "link": "https://arxiv.org/abs/2402.19472",
    "context": "Title: Lifelong Benchmarks: Efficient Model Evaluation in an Era of Rapid Progress\nAbstract: arXiv:2402.19472v1 Announce Type: new  Abstract: Standardized benchmarks drive progress in machine learning. However, with repeated testing, the risk of overfitting grows as algorithms over-exploit benchmark idiosyncrasies. In our work, we seek to mitigate this challenge by compiling ever-expanding large-scale benchmarks called Lifelong Benchmarks. As exemplars of our approach, we create Lifelong-CIFAR10 and Lifelong-ImageNet, containing (for now) 1.69M and 1.98M test samples, respectively. While reducing overfitting, lifelong benchmarks introduce a key challenge: the high cost of evaluating a growing number of models across an ever-expanding sample set. To address this challenge, we also introduce an efficient evaluation framework: Sort \\& Search (S&S), which reuses previously evaluated models by leveraging dynamic programming algorithms to selectively rank and sub-select test samples, enabling cost-effective lifelong benchmarking. Extensive empirical evaluations across 31,000 models ",
    "path": "papers/24/02/2402.19472.json",
    "total_tokens": 906,
    "translated_title": "终身基准：在快速进展时代中高效的模型评估",
    "translated_abstract": "标准化基准推动机器学习的进步。然而，通过重复测试，算法对基准的特殊性过度利用，会增加过拟合的风险。在我们的工作中，我们试图通过编制不断扩展的大规模基准（称为终身基准）来缓解这一挑战。作为我们方法的示例，我们创建了终身-CIFAR10和终身-ImageNet，分别包含（目前）1.69百万和1.98百万个测试样本。尽管减少了过拟合，终身基准引入了一个关键挑战：评估日益增多的模型在不断扩大的样本集上的高成本。为了解决这一挑战，我们还引入了一种高效的评估框架：Sort \\& Search (S&S)，通过利用动态规划算法有选择地对测试样本进行排序和子选择，使得终身基准评估具有成本效益。通过对31,000个模型进行广泛的实证评估",
    "tldr": "提出了终身基准的概念，通过创建不断扩展的大规模基准来减少过拟合风险，并引入了高效的评估框架Sort \\& Search（S&S）来解决评估成本问题。",
    "en_tdlr": "Introduced the concept of lifelong benchmarks to reduce overfitting risk by creating ever-expanding large-scale benchmarks, and introduced an efficient evaluation framework Sort \\& Search (S&S) to address the evaluation cost issue."
}