{
    "title": "Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model",
    "abstract": "Recent breakthroughs in large language models (LLMs) have centered around a handful of data-rich languages. What does it take to broaden access to breakthroughs beyond first-class citizen languages? Our work introduces Aya, a massively multilingual generative language model that follows instructions in 101 languages of which over 50% are considered as lower-resourced. Aya outperforms mT0 and BLOOMZ on the majority of tasks while covering double the number of languages. We introduce extensive new evaluation suites that broaden the state-of-art for multilingual eval across 99 languages -- including discriminative and generative tasks, human evaluation, and simulated win rates that cover both held-out tasks and in-distribution performance. Furthermore, we conduct detailed investigations on the optimal finetuning mixture composition, data pruning, as well as the toxicity, bias, and safety of our models. We open-source our instruction datasets and our model at https://hf.co/CohereForAI/aya-",
    "link": "https://arxiv.org/abs/2402.07827",
    "context": "Title: Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model\nAbstract: Recent breakthroughs in large language models (LLMs) have centered around a handful of data-rich languages. What does it take to broaden access to breakthroughs beyond first-class citizen languages? Our work introduces Aya, a massively multilingual generative language model that follows instructions in 101 languages of which over 50% are considered as lower-resourced. Aya outperforms mT0 and BLOOMZ on the majority of tasks while covering double the number of languages. We introduce extensive new evaluation suites that broaden the state-of-art for multilingual eval across 99 languages -- including discriminative and generative tasks, human evaluation, and simulated win rates that cover both held-out tasks and in-distribution performance. Furthermore, we conduct detailed investigations on the optimal finetuning mixture composition, data pruning, as well as the toxicity, bias, and safety of our models. We open-source our instruction datasets and our model at https://hf.co/CohereForAI/aya-",
    "path": "papers/24/02/2402.07827.json",
    "total_tokens": 927,
    "translated_title": "Aya模型：一个经过指令微调的开放多语言模型",
    "translated_abstract": "最近大规模语言模型（LLMs）的突破主要集中在少数数据丰富的语言上。如何拓宽对突破性成果的访问范围以覆盖非主流语言呢？我们的工作引入了Aya，一个支持101种语言的大规模多语言生成模型，其中超过50％的语言被认为是资源较少的。Aya在大多数任务上表现优于mT0和BLOOMZ，同时覆盖的语言数量是它们的两倍。我们引入了广泛的新评估套件，扩展了99种语言的多语种评估的最新技术，其中包括区分和生成任务、人工评估以及模拟的胜率评估，涵盖了保留任务和分布内性能。此外，我们对最佳微调混合物组成、数据修剪以及模型的毒性、偏差和安全性进行了详细研究。我们将我们的指令数据集和模型开源在https://hf.co/CohereForAI/aya上。",
    "tldr": "Aya是一个开放多语言模型，通过指令微调，在101种语言中表现优于其他模型，扩展了多语言评估的技术，并进行了深入研究优化微调组合、数据修剪以及模型的毒性、偏差和安全性。"
}