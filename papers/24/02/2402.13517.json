{
    "title": "Round Trip Translation Defence against Large Language Model Jailbreaking Attacks",
    "abstract": "arXiv:2402.13517v1 Announce Type: cross  Abstract: Large language models (LLMs) are susceptible to social-engineered attacks that are human-interpretable but require a high level of comprehension for LLMs to counteract. Existing defensive measures can only mitigate less than half of these attacks at most. To address this issue, we propose the Round Trip Translation (RTT) method, the first algorithm specifically designed to defend against social-engineered attacks on LLMs. RTT paraphrases the adversarial prompt and generalizes the idea conveyed, making it easier for LLMs to detect induced harmful behavior. This method is versatile, lightweight, and transferrable to different LLMs. Our defense successfully mitigated over 70% of Prompt Automatic Iterative Refinement (PAIR) attacks, which is currently the most effective defense to the best of our knowledge. We are also the first to attempt mitigating the MathsAttack and reduced its attack success rate by almost 40%. Our code is publicly av",
    "link": "https://arxiv.org/abs/2402.13517",
    "context": "Title: Round Trip Translation Defence against Large Language Model Jailbreaking Attacks\nAbstract: arXiv:2402.13517v1 Announce Type: cross  Abstract: Large language models (LLMs) are susceptible to social-engineered attacks that are human-interpretable but require a high level of comprehension for LLMs to counteract. Existing defensive measures can only mitigate less than half of these attacks at most. To address this issue, we propose the Round Trip Translation (RTT) method, the first algorithm specifically designed to defend against social-engineered attacks on LLMs. RTT paraphrases the adversarial prompt and generalizes the idea conveyed, making it easier for LLMs to detect induced harmful behavior. This method is versatile, lightweight, and transferrable to different LLMs. Our defense successfully mitigated over 70% of Prompt Automatic Iterative Refinement (PAIR) attacks, which is currently the most effective defense to the best of our knowledge. We are also the first to attempt mitigating the MathsAttack and reduced its attack success rate by almost 40%. Our code is publicly av",
    "path": "papers/24/02/2402.13517.json",
    "total_tokens": 872,
    "translated_title": "大型语言模型逆向翻译防御对抗攻击",
    "translated_abstract": "大型语言模型（LLMs）容易受到社交工程攻击，这些攻击对人类具有可解释性，但需要LLMs具有高水平的理解能力才能抵抗。现有的防御措施最多只能缓解这些攻击的不到一半。为解决这一问题，我们提出了往返翻译（RTT）方法，这是第一个专门设计用于抵御LLMs社交工程攻击的算法。RTT会改写对抗性提示并推广表达的思想，使LLMs更容易检测出诱发有害行为。这种方法灵活、轻量且可转移至不同的LLMs。我们的防御成功地缓解了超过70%的Prompt Automatic Iterative Refinement (PAIR)攻击，这是目前我们所知最有效的防御。我们也是首次尝试缓解MathsAttack，并将其攻击成功率降低了近40%。我们的代码已公开发布。",
    "tldr": "往返翻译（RTT）方法是第一个专门设计用于抵御大型语言模型（LLMs）社交工程攻击的算法，成功地减少了多种攻击形式的成功率。"
}