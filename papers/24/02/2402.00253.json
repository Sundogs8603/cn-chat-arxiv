{
    "title": "A Survey on Hallucination in Large Vision-Language Models",
    "abstract": "Recent development of Large Vision-Language Models (LVLMs) has attracted growing attention within the AI landscape for its practical implementation potential. However, ``hallucination'', or more specifically, the misalignment between factual visual content and corresponding textual generation, poses a significant challenge of utilizing LVLMs. In this comprehensive survey, we dissect LVLM-related hallucinations in an attempt to establish an overview and facilitate future mitigation. Our scrutiny starts with a clarification of the concept of hallucinations in LVLMs, presenting a variety of hallucination symptoms and highlighting the unique challenges inherent in LVLM hallucinations. Subsequently, we outline the benchmarks and methodologies tailored specifically for evaluating hallucinations unique to LVLMs. Additionally, we delve into an investigation of the root causes of these hallucinations, encompassing insights from the training data and model components. We also critically review e",
    "link": "https://arxiv.org/abs/2402.00253",
    "context": "Title: A Survey on Hallucination in Large Vision-Language Models\nAbstract: Recent development of Large Vision-Language Models (LVLMs) has attracted growing attention within the AI landscape for its practical implementation potential. However, ``hallucination'', or more specifically, the misalignment between factual visual content and corresponding textual generation, poses a significant challenge of utilizing LVLMs. In this comprehensive survey, we dissect LVLM-related hallucinations in an attempt to establish an overview and facilitate future mitigation. Our scrutiny starts with a clarification of the concept of hallucinations in LVLMs, presenting a variety of hallucination symptoms and highlighting the unique challenges inherent in LVLM hallucinations. Subsequently, we outline the benchmarks and methodologies tailored specifically for evaluating hallucinations unique to LVLMs. Additionally, we delve into an investigation of the root causes of these hallucinations, encompassing insights from the training data and model components. We also critically review e",
    "path": "papers/24/02/2402.00253.json",
    "total_tokens": 866,
    "translated_title": "大规模视觉-语言模型中的幻觉调查",
    "translated_abstract": "大规模视觉-语言模型（LVLMs）的发展引起了人工智能领域越来越多的关注，因为它具有实际的实施潜力。然而，“幻觉”，或者更具体地说，即视觉内容与相应文本生成之间的不一致，在利用LVLMs方面提出了重大挑战。在这份综合调查中，我们对LVLM相关的幻觉进行了深入剖析，旨在建立一个概览并促进未来的缓解。我们首先澄清了LVLMs中幻觉概念，呈现了各种幻觉症状，并强调了LVLM幻觉固有的独特挑战。随后，我们概述了专门用于评估LVLM独特幻觉的基准和方法论。此外，我们深入调查了这些幻觉的根本原因，包括来自训练数据和模型组件的见解。我们还对现有的幻觉缓解方法进行了批判性的回顾。",
    "tldr": "这份综合调查研究了大规模视觉-语言模型中的幻觉问题，澄清了幻觉的概念，提出了评估方法，并对幻觉的根本原因进行了调查。",
    "en_tdlr": "This comprehensive survey investigates the issue of hallucination in large vision-language models, clarifies the concept, presents evaluation methodologies, and investigates the root causes of hallucination."
}