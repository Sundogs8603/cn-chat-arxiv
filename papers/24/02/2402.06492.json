{
    "title": "Inducing Systematicity in Transformers by Attending to Structurally Quantized Embeddings",
    "abstract": "Transformers generalize to novel compositions of structures and entities after being trained on a complex dataset, but easily overfit on datasets of insufficient complexity. We observe that when the training set is sufficiently complex, the model encodes sentences that have a common syntactic structure using a systematic attention pattern. Inspired by this observation, we propose SQ-Transformer (Structurally Quantized) that explicitly encourages systematicity in the embeddings and attention layers, even with a training set of low complexity. At the embedding level, we introduce Structure-oriented Vector Quantization (SoVQ) to cluster word embeddings into several classes of structurally equivalent entities. At the attention level, we devise the Systematic Attention Layer (SAL) and an alternative, Systematically Regularized Layer (SRL) that operate on the quantized word embeddings so that sentences of the same structure are encoded with invariant or similar attention patterns. Empiricall",
    "link": "https://arxiv.org/abs/2402.06492",
    "context": "Title: Inducing Systematicity in Transformers by Attending to Structurally Quantized Embeddings\nAbstract: Transformers generalize to novel compositions of structures and entities after being trained on a complex dataset, but easily overfit on datasets of insufficient complexity. We observe that when the training set is sufficiently complex, the model encodes sentences that have a common syntactic structure using a systematic attention pattern. Inspired by this observation, we propose SQ-Transformer (Structurally Quantized) that explicitly encourages systematicity in the embeddings and attention layers, even with a training set of low complexity. At the embedding level, we introduce Structure-oriented Vector Quantization (SoVQ) to cluster word embeddings into several classes of structurally equivalent entities. At the attention level, we devise the Systematic Attention Layer (SAL) and an alternative, Systematically Regularized Layer (SRL) that operate on the quantized word embeddings so that sentences of the same structure are encoded with invariant or similar attention patterns. Empiricall",
    "path": "papers/24/02/2402.06492.json",
    "total_tokens": 867,
    "translated_title": "通过关注结构化量化的嵌入在Transformer中引导系统性",
    "translated_abstract": "Transformer在训练过复杂数据集后能够推广到结构和实体的新组合，但在复杂度不足的数据集上容易过拟合。我们观察到，当训练集足够复杂时，模型使用系统性的注意模式对具有共同句法结构的句子进行编码。受到这一观察的启发，我们提出了SQ-Transformer（结构化量化），即使使用低复杂度的训练集，也能明确地在嵌入和注意层中鼓励系统性。在嵌入层面上，我们引入了结构导向的向量量化（SoVQ），将单词嵌入聚类成若干类具有结构等价的实体。在注意层面上，我们设计了系统性注意层（SAL）和另一种替代性的系统性正则化层（SRL），它们都在量化的词嵌入上操作，以便以不变或类似的注意模式编码具有相同结构的句子。",
    "tldr": "本论文提出了SQ-Transformer模型，通过在嵌入和注意层中引入结构化量化的方法，无论训练集的复杂度如何，都能够明确地鼓励模型在编码句子时保持系统性。"
}