{
    "title": "Hydragen: High-Throughput LLM Inference with Shared Prefixes",
    "abstract": "Transformer-based large language models (LLMs) are now deployed to hundreds of millions of users. LLM inference is commonly performed on batches of sequences that share a prefix, such as few-shot examples or a chatbot system prompt. Decoding in this large-batch setting can be bottlenecked by the attention operation, which reads large key-value (KV) caches from memory and computes inefficient matrix-vector products for every sequence in the batch. In this work, we introduce Hydragen, a hardware-aware exact implementation of attention with shared prefixes. Hydragen computes attention over the shared prefix and unique suffixes separately. This decomposition enables efficient prefix attention by batching queries together across sequences, reducing redundant memory reads and enabling the use of hardware-friendly matrix multiplications. Our method can improve end-to-end LLM throughput by up to 32x against competitive baselines, with speedup growing with the batch size and shared prefix lengt",
    "link": "https://arxiv.org/abs/2402.05099",
    "context": "Title: Hydragen: High-Throughput LLM Inference with Shared Prefixes\nAbstract: Transformer-based large language models (LLMs) are now deployed to hundreds of millions of users. LLM inference is commonly performed on batches of sequences that share a prefix, such as few-shot examples or a chatbot system prompt. Decoding in this large-batch setting can be bottlenecked by the attention operation, which reads large key-value (KV) caches from memory and computes inefficient matrix-vector products for every sequence in the batch. In this work, we introduce Hydragen, a hardware-aware exact implementation of attention with shared prefixes. Hydragen computes attention over the shared prefix and unique suffixes separately. This decomposition enables efficient prefix attention by batching queries together across sequences, reducing redundant memory reads and enabling the use of hardware-friendly matrix multiplications. Our method can improve end-to-end LLM throughput by up to 32x against competitive baselines, with speedup growing with the batch size and shared prefix lengt",
    "path": "papers/24/02/2402.05099.json",
    "total_tokens": 910,
    "translated_title": "Hydragen：共享前缀的高吞吐量LLM推理",
    "translated_abstract": "基于Transformer的大规模语言模型（LLM）现在已经部署到数亿用户上。LLM推理通常在共享前缀的序列批次上执行，例如少量样本示例或聊天机器人系统提示。在这种大批量设置下，解码可能会受到注意操作的瓶颈，该操作从内存中读取大型键值（KV）缓存，并为批次中的每个序列计算低效的矩阵-向量乘积。在这项工作中，我们介绍了Hydragen，一种具有共享前缀的硬件感知精确实现的注意力。Hydragen将注意力分别计算在共享前缀和唯一后缀上。这种分解通过在序列之间批量查询一起减少冗余内存读取，从而实现了高效的前缀注意力，并使得可以使用硬件友好的矩阵乘法。我们的方法可以将端到端的LLM吞吐量提高多达32倍，超过竞争基线，并且随着批次大小和共享前缀的长度增加，速度提高的幅度也增加。",
    "tldr": "Hydragen是一种具有共享前缀的高吞吐量LLM推理方法，通过将注意力计算分解为共享前缀和唯一后缀，来提高推理效率，并能够提高端到端LLM吞吐量多达32倍。",
    "en_tdlr": "Hydragen is a high-throughput LLM inference method with shared prefixes, which improves efficiency by decomposing attention computation into shared prefixes and unique suffixes, and can increase end-to-end LLM throughput by up to 32x."
}