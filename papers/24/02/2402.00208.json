{
    "title": "MP-SL: Multihop Parallel Split Learning",
    "abstract": "Federated Learning (FL) stands out as a widely adopted protocol facilitating the training of Machine Learning (ML) models while maintaining decentralized data. However, challenges arise when dealing with a heterogeneous set of participating devices, causing delays in the training process, particularly among devices with limited resources. Moreover, the task of training ML models with a vast number of parameters demands computing and memory resources beyond the capabilities of small devices, such as mobile and Internet of Things (IoT) devices. To address these issues, techniques like Parallel Split Learning (SL) have been introduced, allowing multiple resource-constrained devices to actively participate in collaborative training processes with assistance from resourceful compute nodes. Nonetheless, a drawback of Parallel SL is the substantial memory allocation required at the compute nodes, for instance training VGG-19 with 100 participants needs 80 GB. In this paper, we introduce Multi",
    "link": "https://arxiv.org/abs/2402.00208",
    "context": "Title: MP-SL: Multihop Parallel Split Learning\nAbstract: Federated Learning (FL) stands out as a widely adopted protocol facilitating the training of Machine Learning (ML) models while maintaining decentralized data. However, challenges arise when dealing with a heterogeneous set of participating devices, causing delays in the training process, particularly among devices with limited resources. Moreover, the task of training ML models with a vast number of parameters demands computing and memory resources beyond the capabilities of small devices, such as mobile and Internet of Things (IoT) devices. To address these issues, techniques like Parallel Split Learning (SL) have been introduced, allowing multiple resource-constrained devices to actively participate in collaborative training processes with assistance from resourceful compute nodes. Nonetheless, a drawback of Parallel SL is the substantial memory allocation required at the compute nodes, for instance training VGG-19 with 100 participants needs 80 GB. In this paper, we introduce Multi",
    "path": "papers/24/02/2402.00208.json",
    "total_tokens": 894,
    "translated_title": "MP-SL：多跳并行分割学习",
    "translated_abstract": "联邦学习（FL）作为一种广泛采用的协议，可以在保持分散化数据的同时促进机器学习（ML）模型的训练。然而，当处理不同参与设备的异构集时，会遇到挑战，导致训练过程中出现延迟，特别是在资源有限的设备中。此外，使用大量参数进行ML模型训练需要计算和内存资源超出小设备（例如移动设备和物联网设备）的能力。为了解决这些问题，引入了一些技术，如并行分割学习（SL），允许多个资源受限设备在有资源的计算节点的帮助下积极参与协作训练过程。然而，并行SL的缺点是需要在计算节点上分配大量内存，例如使用100个参与者训练VGG-19需要80 GB的内存。在本文中，我们介绍了多跳并行分割学习（MP-SL），该方法可以减少计算节点的内存需求。",
    "tldr": "引入了多跳并行分割学习（MP-SL）来缓解联邦学习（FL）中处理异构设备和大量参数的挑战。该方法允许多个资源受限设备积极参与协作训练过程，并减少计算节点的内存需求。",
    "en_tdlr": "Multi-hop Parallel Split Learning (MP-SL) is introduced to address the challenges of dealing with heterogeneous devices and a large number of parameters in Federated Learning (FL). This technique allows multiple resource-constrained devices to actively participate in collaborative training processes and reduces the memory requirement at the compute nodes."
}