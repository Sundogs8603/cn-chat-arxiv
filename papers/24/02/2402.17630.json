{
    "title": "Fine-Grained Natural Language Inference Based Faithfulness Evaluation for Diverse Summarisation Tasks",
    "abstract": "arXiv:2402.17630v1 Announce Type: new  Abstract: We study existing approaches to leverage off-the-shelf Natural Language Inference (NLI) models for the evaluation of summary faithfulness and argue that these are sub-optimal due to the granularity level considered for premises and hypotheses. That is, the smaller content unit considered as hypothesis is a sentence and premises are made up of a fixed number of document sentences. We propose a novel approach, namely InFusE, that uses a variable premise size and simplifies summary sentences into shorter hypotheses. Departing from previous studies which focus on single short document summarisation, we analyse NLI based faithfulness evaluation for diverse summarisation tasks. We introduce DiverSumm, a new benchmark comprising long form summarisation (long documents and summaries) and diverse summarisation tasks (e.g., meeting and multi-document summarisation). In experiments, InFusE obtains superior performance across the different summarisa",
    "link": "https://arxiv.org/abs/2402.17630",
    "context": "Title: Fine-Grained Natural Language Inference Based Faithfulness Evaluation for Diverse Summarisation Tasks\nAbstract: arXiv:2402.17630v1 Announce Type: new  Abstract: We study existing approaches to leverage off-the-shelf Natural Language Inference (NLI) models for the evaluation of summary faithfulness and argue that these are sub-optimal due to the granularity level considered for premises and hypotheses. That is, the smaller content unit considered as hypothesis is a sentence and premises are made up of a fixed number of document sentences. We propose a novel approach, namely InFusE, that uses a variable premise size and simplifies summary sentences into shorter hypotheses. Departing from previous studies which focus on single short document summarisation, we analyse NLI based faithfulness evaluation for diverse summarisation tasks. We introduce DiverSumm, a new benchmark comprising long form summarisation (long documents and summaries) and diverse summarisation tasks (e.g., meeting and multi-document summarisation). In experiments, InFusE obtains superior performance across the different summarisa",
    "path": "papers/24/02/2402.17630.json",
    "total_tokens": 904,
    "translated_title": "面向多样化摘要任务的细粒度自然语言推理信念评估",
    "translated_abstract": "我们研究了利用现成的自然语言推理（NLI）模型来评估摘要忠实性的现有方法，并指出这些方法在考虑前提和假设的细粒度级别时存在不足。换句话说，被视为假设的较小内容单元是一个句子，而前提由固定数量的文档句子组成。我们提出了一种新颖的方法，即InFusE，它使用可变的前提大小，并将摘要句子简化为更短的假设。与以往侧重于单个短文档摘要的研究不同，我们分析了用于多样化摘要任务的基于NLI的忠实性评估。我们引入了一个新的基准数据集DiverSumm，其中包括长篇摘要（长文档和摘要）和多样化摘要任务（例如会议和多文档摘要）。在实验中，InFusE在不同摘要任务中表现出更优异的性能。",
    "tldr": "提出了一种新颖的方法InFusE，用于面向多样化摘要任务的细粒度自然语言推理信念评估，并引入了包括长篇摘要和多样化摘要任务在内的新基准数据集DiverSumm。",
    "en_tdlr": "Proposed a novel approach InFusE for fine-grained natural language inference-based faithfulness evaluation for diverse summarisation tasks, introducing a new benchmark dataset DiverSumm including long-form summarisation and diverse summarisation tasks."
}