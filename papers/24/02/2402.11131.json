{
    "title": "Speculative Streaming: Fast LLM Inference without Auxiliary Models",
    "abstract": "arXiv:2402.11131v1 Announce Type: cross  Abstract: Speculative decoding is a prominent technique to speed up the inference of a large target language model based on predictions of an auxiliary draft model. While effective, in application-specific settings, it often involves fine-tuning both draft and target models to achieve high acceptance rates. As the number of downstream tasks grows, these draft models add significant complexity to inference systems. We propose Speculative Streaming, a single-model speculative decoding method that fuses drafting into the target model by changing the fine-tuning objective from next token prediction to future n-gram prediction. Speculative Streaming speeds up decoding by 1.8 - 3.1X in a diverse set of tasks, such as Summarization, Structured Queries, and Meaning Representation, without sacrificing generation quality. Additionally, Speculative Streaming is parameter-efficient. It achieves on-par/higher speed-ups than Medusa-style architectures while u",
    "link": "https://arxiv.org/abs/2402.11131",
    "context": "Title: Speculative Streaming: Fast LLM Inference without Auxiliary Models\nAbstract: arXiv:2402.11131v1 Announce Type: cross  Abstract: Speculative decoding is a prominent technique to speed up the inference of a large target language model based on predictions of an auxiliary draft model. While effective, in application-specific settings, it often involves fine-tuning both draft and target models to achieve high acceptance rates. As the number of downstream tasks grows, these draft models add significant complexity to inference systems. We propose Speculative Streaming, a single-model speculative decoding method that fuses drafting into the target model by changing the fine-tuning objective from next token prediction to future n-gram prediction. Speculative Streaming speeds up decoding by 1.8 - 3.1X in a diverse set of tasks, such as Summarization, Structured Queries, and Meaning Representation, without sacrificing generation quality. Additionally, Speculative Streaming is parameter-efficient. It achieves on-par/higher speed-ups than Medusa-style architectures while u",
    "path": "papers/24/02/2402.11131.json",
    "total_tokens": 862,
    "translated_title": "推测式流式处理: 无需辅助模型的快速LLM推理",
    "translated_abstract": "推测式解码是一种突出的技术，可以提高基于辅助草稿模型预测的大型目标语言模型的推理速度。虽然在特定应用设置中有效，但通常需要微调草稿和目标模型以实现较高的接受率。随着下游任务数量的增加，这些草稿模型给推理系统增加了显著的复杂性。我们提出了Speculative Streaming，一种单模型的推测式解码方法，通过将草拟融入目标模型，将微调目标从下一个令牌预测对象更改为未来的n-gram预测。 Speculative Streaming在各种任务中加速解码1.8-3.1倍，如摘要、结构化查询和意义表达，同时不降低生成质量。此外，Speculative Streaming参数有效。它实现了与Medusa风格架构相媲美/更高的加速度",
    "tldr": "提出了一种Speculative Streaming方法，将草稿模型融入目标模型，并通过将微调目标从下一个令牌预测更改为未来的n-gram预测，加速解码1.8-3.1倍，同时保持生成质量。"
}