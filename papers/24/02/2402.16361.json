{
    "title": "Layer-wise Regularized Dropout for Neural Language Models",
    "abstract": "arXiv:2402.16361v1 Announce Type: cross  Abstract: Among the various pre-trained neural language models that are popular today, dropout is already an indispensable regularization technique. To solve the inconsistency between training and inference caused by the randomness of dropout, some studies use consistency training to regularize dropout at the output layer. In this paper, we propose a novel Layer-wise Regularized Dropout (LR-Drop), which is specially designed for Transformer-based Language models. Specifically, LR-Drop layer-wise regularizes each Transformer layer using the consistency training strategy. Each training sample passes through the two siamese sub-models sampled by dropout, and then LR-Drop forces the hidden states, multi-head attention matrices, and output distribution of the two siamese sub-models to be consistent. The proposed LR-Drop can be regarded as a \"self-distillation\" framework, in which each sub-model generated by dropout is the other's \"teacher\" model and ",
    "link": "https://arxiv.org/abs/2402.16361",
    "context": "Title: Layer-wise Regularized Dropout for Neural Language Models\nAbstract: arXiv:2402.16361v1 Announce Type: cross  Abstract: Among the various pre-trained neural language models that are popular today, dropout is already an indispensable regularization technique. To solve the inconsistency between training and inference caused by the randomness of dropout, some studies use consistency training to regularize dropout at the output layer. In this paper, we propose a novel Layer-wise Regularized Dropout (LR-Drop), which is specially designed for Transformer-based Language models. Specifically, LR-Drop layer-wise regularizes each Transformer layer using the consistency training strategy. Each training sample passes through the two siamese sub-models sampled by dropout, and then LR-Drop forces the hidden states, multi-head attention matrices, and output distribution of the two siamese sub-models to be consistent. The proposed LR-Drop can be regarded as a \"self-distillation\" framework, in which each sub-model generated by dropout is the other's \"teacher\" model and ",
    "path": "papers/24/02/2402.16361.json",
    "total_tokens": 888,
    "translated_title": "分层正则化Dropout用于神经语言模型",
    "translated_abstract": "在当今流行的各种预训练神经语言模型中，dropout已经成为一种不可或缺的正则化技术。为了解决dropout随机性引起的训练和推理不一致性，一些研究采用一致性训练来对输出层的dropout进行正则化。本文提出了一种新颖的分层正则化Dropout（LR-Drop），专为基于Transformer的语言模型设计。具体而言，LR-Drop使用层次一致性训练策略，逐层对每个Transformer层进行正则化。每个训练样本通过dropout采样的两个孪生子模型，然后LR-Drop强制使两个孪生子模型的隐藏状态、多头注意力矩阵和输出分布保持一致。所提出的LR-Drop可以被视为一种“自蒸馏”框架，其中dropout生成的每个子模型都是另一个的“教师”模型。",
    "tldr": "本文提出了一种专为Transformer-based语言模型设计的新颖的分层正则化Dropout（LR-Drop）方法，通过一致性训练策略逐层对每个Transformer层进行正则化，实现了隐藏状态、多头注意力矩阵和输出分布的一致性。",
    "en_tdlr": "This paper introduces a novel Layer-wise Regularized Dropout (LR-Drop) method designed specifically for Transformer-based language models, which regularizes each Transformer layer layer-wise through consistency training strategy, achieving consistency in hidden states, multi-head attention matrices, and output distribution."
}