{
    "title": "The Fine-Grained Complexity of Gradient Computation for Training Large Language Models",
    "abstract": "Large language models (LLMs) have made fundamental contributions over the last a few years. To train an LLM, one needs to alternatingly run `forward' computations and `backward' computations. The forward computation can be viewed as attention function evaluation, and the backward computation can be viewed as a gradient computation. In previous work by [Alman and Song, NeurIPS 2023], it was proved that the forward step can be performed in almost-linear time in certain parameter regimes, but that there is no truly sub-quadratic time algorithm in the remaining parameter regimes unless the popular hypothesis SETH is false. In this work, we show nearly identical results for the harder-seeming problem of computing the gradient of loss function of one layer attention network, and thus for the entire process of LLM training. This completely characterizes the fine-grained complexity of every step of LLM training.",
    "link": "https://arxiv.org/abs/2402.04497",
    "context": "Title: The Fine-Grained Complexity of Gradient Computation for Training Large Language Models\nAbstract: Large language models (LLMs) have made fundamental contributions over the last a few years. To train an LLM, one needs to alternatingly run `forward' computations and `backward' computations. The forward computation can be viewed as attention function evaluation, and the backward computation can be viewed as a gradient computation. In previous work by [Alman and Song, NeurIPS 2023], it was proved that the forward step can be performed in almost-linear time in certain parameter regimes, but that there is no truly sub-quadratic time algorithm in the remaining parameter regimes unless the popular hypothesis SETH is false. In this work, we show nearly identical results for the harder-seeming problem of computing the gradient of loss function of one layer attention network, and thus for the entire process of LLM training. This completely characterizes the fine-grained complexity of every step of LLM training.",
    "path": "papers/24/02/2402.04497.json",
    "total_tokens": 881,
    "translated_title": "训练大型语言模型的梯度计算的细粒度复杂性",
    "translated_abstract": "在过去几年中，大型语言模型（LLM）已经作出了基本贡献。要训练一个LLM，人们需要交替运行“前向计算”和“反向计算”。前向计算可以视为注意力函数的评估，而后向计算可以视为梯度计算。在之前的研究中，[Alman和Song，NeurIPS 2023]证明在某些参数区域中前向步骤可以在几乎线性时间内执行，但在其余参数区域内，除非流行的假设SETH不成立，否则没有真正的亚二次时间算法。在这项工作中，我们展示了对于计算单层注意力网络损失函数的梯度，以及LLM训练的整个过程中似乎更难的问题几乎完全相同的结果。这完全刻画了LLM训练每个步骤的细粒度复杂性。",
    "tldr": "本文研究了训练大型语言模型中梯度计算的复杂性，证明了在某些参数区域内可以以几乎线性时间进行前向计算，但在其余参数区域内需要超过二次时间，这对于LLM训练的每个步骤都具有重要意义。",
    "en_tdlr": "This paper investigates the complexity of gradient computation for training large language models, demonstrating that forward computations can be done in almost-linear time in certain parameter regimes, but require super-quadratic time in others, which is crucial for every step of LLM training."
}