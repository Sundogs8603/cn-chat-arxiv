{
    "title": "Understanding Neural Network Binarization with Forward and Backward Proximal Quantizers",
    "abstract": "arXiv:2402.17710v1 Announce Type: new  Abstract: In neural network binarization, BinaryConnect (BC) and its variants are considered the standard. These methods apply the sign function in their forward pass and their respective gradients are backpropagated to update the weights. However, the derivative of the sign function is zero whenever defined, which consequently freezes training. Therefore, implementations of BC (e.g., BNN) usually replace the derivative of sign in the backward computation with identity or other approximate gradient alternatives. Although such practice works well empirically, it is largely a heuristic or ''training trick.'' We aim at shedding some light on these training tricks from the optimization perspective. Building from existing theory on ProxConnect (PC, a generalization of BC), we (1) equip PC with different forward-backward quantizers and obtain ProxConnect++ (PC++) that includes existing binarization techniques as special cases; (2) derive a principled wa",
    "link": "https://arxiv.org/abs/2402.17710",
    "context": "Title: Understanding Neural Network Binarization with Forward and Backward Proximal Quantizers\nAbstract: arXiv:2402.17710v1 Announce Type: new  Abstract: In neural network binarization, BinaryConnect (BC) and its variants are considered the standard. These methods apply the sign function in their forward pass and their respective gradients are backpropagated to update the weights. However, the derivative of the sign function is zero whenever defined, which consequently freezes training. Therefore, implementations of BC (e.g., BNN) usually replace the derivative of sign in the backward computation with identity or other approximate gradient alternatives. Although such practice works well empirically, it is largely a heuristic or ''training trick.'' We aim at shedding some light on these training tricks from the optimization perspective. Building from existing theory on ProxConnect (PC, a generalization of BC), we (1) equip PC with different forward-backward quantizers and obtain ProxConnect++ (PC++) that includes existing binarization techniques as special cases; (2) derive a principled wa",
    "path": "papers/24/02/2402.17710.json",
    "total_tokens": 851,
    "translated_title": "通过前向和后向近端量化器理解神经网络二值化",
    "translated_abstract": "在神经网络二值化中，BinaryConnect（BC）及其变体被认为是标准。这些方法在前向传播中应用符号函数，它们的梯度被反向传播来更新权重。然而，当定义时符号函数的导数为零，这会导致训练停滞。因此，BC的实现（例如BNN）通常使用恒等或其他近似梯度替代符号函数在反向计算中的导数。虽然这种做法在经验上表现良好，但主要是一种启发式或“训练技巧”。我们的目标是从优化的角度阐明这些训练技巧。基于现有的ProxConnect（PC，BC的一种泛化）理论，我们（1）为PC配备了不同的前向-后向量化器，获得了包括现有二值化技术在内的ProxConnect++（PC++）特例；（2）推导了一个原则性的",
    "tldr": "从优化的角度阐明神经网络二值化中的训练技巧，提出了ProxConnect++（PC++）这一泛化模型，将现有二值化技术视为其特例",
    "en_tdlr": "Shedding light on the training tricks in neural network binarization from an optimization perspective, introducing ProxConnect++ (PC++) as a generalization including existing binarization techniques as special cases"
}