{
    "title": "Asymmetry in Low-Rank Adapters of Foundation Models",
    "abstract": "arXiv:2402.16842v2 Announce Type: replace  Abstract: Parameter-efficient fine-tuning optimizes large, pre-trained foundation models by updating a subset of parameters; in this class, Low-Rank Adaptation (LoRA) is particularly effective. Inspired by an effort to investigate the different roles of LoRA matrices during fine-tuning, this paper characterizes and leverages unexpected asymmetry in the importance of low-rank adapter matrices. Specifically, when updating the parameter matrices of a neural network by adding a product $BA$, we observe that the $B$ and $A$ matrices have distinct functions: $A$ extracts features from the input, while $B$ uses these features to create the desired output. Based on this observation, we demonstrate that fine-tuning $B$ is inherently more effective than fine-tuning $A$, and that a random untrained $A$ should perform nearly as well as a fine-tuned one. Using an information-theoretic lens, we also bound the generalization of low-rank adapters, showing tha",
    "link": "https://arxiv.org/abs/2402.16842",
    "context": "Title: Asymmetry in Low-Rank Adapters of Foundation Models\nAbstract: arXiv:2402.16842v2 Announce Type: replace  Abstract: Parameter-efficient fine-tuning optimizes large, pre-trained foundation models by updating a subset of parameters; in this class, Low-Rank Adaptation (LoRA) is particularly effective. Inspired by an effort to investigate the different roles of LoRA matrices during fine-tuning, this paper characterizes and leverages unexpected asymmetry in the importance of low-rank adapter matrices. Specifically, when updating the parameter matrices of a neural network by adding a product $BA$, we observe that the $B$ and $A$ matrices have distinct functions: $A$ extracts features from the input, while $B$ uses these features to create the desired output. Based on this observation, we demonstrate that fine-tuning $B$ is inherently more effective than fine-tuning $A$, and that a random untrained $A$ should perform nearly as well as a fine-tuned one. Using an information-theoretic lens, we also bound the generalization of low-rank adapters, showing tha",
    "path": "papers/24/02/2402.16842.json",
    "total_tokens": 927,
    "translated_title": "基于基础模型低秩适配器的不对称性",
    "translated_abstract": "参数高效微调通过更新参数的子集来优化大型、预先训练的基础模型；在这一类中，低秩适应（LoRA）特别有效。受到调查LoRA矩阵在微调过程中不同作用的启发，本文表征和利用了低秩适配器矩阵重要性的意外不对称性。具体地，在通过添加乘积$BA$来更新神经网络的参数矩阵时，我们观察到$B$和$A$矩阵具有不同的功能：$A$从输入中提取特征，而$B$利用这些特征创建期望的输出。基于这一观察，我们证明微调$B$固有地比微调$A$更有效，并且一个随机未经训练的$A$应该表现几乎与经过微调的$A$一样好。同时，我们还使用信息论视角限制了低秩适配器的泛化能力。",
    "tldr": "本文研究发现在预训练模型的参数微调过程中存在着低秩适配器矩阵重要性的不对称性，特别是在更新参数矩阵时，$B$和$A$矩阵具有不同功能，微调$B$比微调$A$更加有效。",
    "en_tdlr": "This paper identifies an unexpected asymmetry in the importance of low-rank adapter matrices during fine-tuning of pre-trained models, highlighting the distinct functions of $B$ and $A$ matrices when updating parameter matrices, with fine-tuning $B being more effective than fine-tuning $A."
}