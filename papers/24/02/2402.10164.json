{
    "title": "Random features and polynomial rules",
    "abstract": "arXiv:2402.10164v1 Announce Type: cross  Abstract: Random features models play a distinguished role in the theory of deep learning, describing the behavior of neural networks close to their infinite-width limit. In this work, we present a thorough analysis of the generalization performance of random features models for generic supervised learning problems with Gaussian data. Our approach, built with tools from the statistical mechanics of disordered systems, maps the random features model to an equivalent polynomial model, and allows us to plot average generalization curves as functions of the two main control parameters of the problem: the number of random features $N$ and the size $P$ of the training set, both assumed to scale as powers in the input dimension $D$. Our results extend the case of proportional scaling between $N$, $P$ and $D$. They are in accordance with rigorous bounds known for certain particular learning tasks and are in quantitative agreement with numerical experime",
    "link": "https://arxiv.org/abs/2402.10164",
    "context": "Title: Random features and polynomial rules\nAbstract: arXiv:2402.10164v1 Announce Type: cross  Abstract: Random features models play a distinguished role in the theory of deep learning, describing the behavior of neural networks close to their infinite-width limit. In this work, we present a thorough analysis of the generalization performance of random features models for generic supervised learning problems with Gaussian data. Our approach, built with tools from the statistical mechanics of disordered systems, maps the random features model to an equivalent polynomial model, and allows us to plot average generalization curves as functions of the two main control parameters of the problem: the number of random features $N$ and the size $P$ of the training set, both assumed to scale as powers in the input dimension $D$. Our results extend the case of proportional scaling between $N$, $P$ and $D$. They are in accordance with rigorous bounds known for certain particular learning tasks and are in quantitative agreement with numerical experime",
    "path": "papers/24/02/2402.10164.json",
    "total_tokens": 830,
    "translated_title": "随机特征和多项式规则",
    "translated_abstract": "随机特征模型在深度学习理论中起着重要的作用，描述了神经网络接近无限宽度极限时的行为。在本工作中，我们对具有高斯数据的一般监督学习问题的随机特征模型的泛化性能进行了详细分析。我们利用无序系统的统计力学工具将随机特征模型映射到等效的多项式模型，并绘制了平均泛化曲线作为问题的两个主要控制参数的函数：随机特征的数量N和训练集的大小P，假设它们都按照输入维度D的幂进行缩放。我们的结果扩展了N，P和D之间比例缩放的情况。它们与特定学习任务已知的严格界限一致，并与数值实验定量一致。",
    "tldr": "本论文分析了随机特征模型在具有高斯数据的一般监督学习问题中的泛化性能，并将随机特征模型映射到等效的多项式模型，得到了与严格界限和数值实验一致的结果。",
    "en_tdlr": "This paper analyzes the generalization performance of random feature models in generic supervised learning problems with Gaussian data. It maps the random feature model to an equivalent polynomial model and obtains results consistent with rigorous bounds and numerical experiments."
}