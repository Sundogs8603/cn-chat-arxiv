{
    "title": "On the Generalization Capability of Temporal Graph Learning Algorithms: Theoretical Insights and a Simpler Method",
    "abstract": "arXiv:2402.16387v1 Announce Type: cross  Abstract: Temporal Graph Learning (TGL) has become a prevalent technique across diverse real-world applications, especially in domains where data can be represented as a graph and evolves over time. Although TGL has recently seen notable progress in algorithmic solutions, its theoretical foundations remain largely unexplored. This paper aims at bridging this gap by investigating the generalization ability of different TGL algorithms (e.g., GNN-based, RNN-based, and memory-based methods) under the finite-wide over-parameterized regime. We establish the connection between the generalization error of TGL algorithms and \"the number of layers/steps\" in the GNN-/RNN-based TGL methods and \"the feature-label alignment (FLA) score\", where FLA can be used as a proxy for the expressive power and explains the performance of memory-based methods. Guided by our theoretical analysis, we propose Simplified-Temporal-Graph-Network, which enjoys a small generaliza",
    "link": "https://arxiv.org/abs/2402.16387",
    "context": "Title: On the Generalization Capability of Temporal Graph Learning Algorithms: Theoretical Insights and a Simpler Method\nAbstract: arXiv:2402.16387v1 Announce Type: cross  Abstract: Temporal Graph Learning (TGL) has become a prevalent technique across diverse real-world applications, especially in domains where data can be represented as a graph and evolves over time. Although TGL has recently seen notable progress in algorithmic solutions, its theoretical foundations remain largely unexplored. This paper aims at bridging this gap by investigating the generalization ability of different TGL algorithms (e.g., GNN-based, RNN-based, and memory-based methods) under the finite-wide over-parameterized regime. We establish the connection between the generalization error of TGL algorithms and \"the number of layers/steps\" in the GNN-/RNN-based TGL methods and \"the feature-label alignment (FLA) score\", where FLA can be used as a proxy for the expressive power and explains the performance of memory-based methods. Guided by our theoretical analysis, we propose Simplified-Temporal-Graph-Network, which enjoys a small generaliza",
    "path": "papers/24/02/2402.16387.json",
    "total_tokens": 848,
    "translated_title": "关于时间图学习算法的泛化能力：理论见解与一种更简单的方法",
    "translated_abstract": "时间图学习（TGL）已成为不同真实应用中普遍采用的技术，尤其是在数据可以表示为随时间演变的图的领域。尽管TGL在算法解决方案方面最近取得了显着进展，但其理论基础仍然大部分未被探索。本文旨在通过研究有限宽超参数化条件下不同TGL算法（如基于GNN、基于RNN和基于内存的方法）的泛化能力，来弥合这一差距。我们建立了TGL算法的泛化误差与GNN-/RNN- TGL方法中“层数/步数”以及特征-标签对齐（FLA）分数之间的关系，其中FLA可用作表达能力的代理，并解释了基于内存的方法的性能。在我们的理论分析的指导下，我们提出了简化时间图网络，该方法具有较小的泛化误差。",
    "tldr": "本文探讨了时间图学习算法的泛化能力，并提出了一种更简单的方法Simplified-Temporal-Graph-Network。"
}