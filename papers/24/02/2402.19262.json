{
    "title": "Masks, Signs, And Learning Rate Rewinding",
    "abstract": "arXiv:2402.19262v1 Announce Type: new  Abstract: Learning Rate Rewinding (LRR) has been established as a strong variant of Iterative Magnitude Pruning (IMP) to find lottery tickets in deep overparameterized neural networks. While both iterative pruning schemes couple structure and parameter learning, understanding how LRR excels in both aspects can bring us closer to the design of more flexible deep learning algorithms that can optimize diverse sets of sparse architectures. To this end, we conduct experiments that disentangle the effect of mask learning and parameter optimization and how both benefit from overparameterization. The ability of LRR to flip parameter signs early and stay robust to sign perturbations seems to make it not only more effective in mask identification but also in optimizing diverse sets of masks, including random ones. In support of this hypothesis, we prove in a simplified single hidden neuron setting that LRR succeeds in more cases than IMP, as it can escape i",
    "link": "https://arxiv.org/abs/2402.19262",
    "context": "Title: Masks, Signs, And Learning Rate Rewinding\nAbstract: arXiv:2402.19262v1 Announce Type: new  Abstract: Learning Rate Rewinding (LRR) has been established as a strong variant of Iterative Magnitude Pruning (IMP) to find lottery tickets in deep overparameterized neural networks. While both iterative pruning schemes couple structure and parameter learning, understanding how LRR excels in both aspects can bring us closer to the design of more flexible deep learning algorithms that can optimize diverse sets of sparse architectures. To this end, we conduct experiments that disentangle the effect of mask learning and parameter optimization and how both benefit from overparameterization. The ability of LRR to flip parameter signs early and stay robust to sign perturbations seems to make it not only more effective in mask identification but also in optimizing diverse sets of masks, including random ones. In support of this hypothesis, we prove in a simplified single hidden neuron setting that LRR succeeds in more cases than IMP, as it can escape i",
    "path": "papers/24/02/2402.19262.json",
    "total_tokens": 886,
    "translated_title": "口罩、标志和学习率回溯",
    "translated_abstract": "学习率回溯（LRR）已被确定为深度超参数化神经网络中寻找“中奖券”的迭代幅度剪枝（IMP）的强变体。虽然两种迭代剪枝方案都将结构和参数学习耦合在一起，但理解LRR在两个方面的优势如何可以使我们更接近设计更灵活的深度学习算法，从而可以优化各种稀疏架构。为此，我们进行了实验，解开了掩模学习和参数优化的效果以及两者如何从超参数化中受益的方式。LRR早期翻转参数符号并保持对符号扰动的稳健性的能力似乎使其在不仅在掩模识别方面更加有效，而且在优化各种掩模，包括随机掩模方面也更加有效。支持这一假设，我们在简化的单个隐藏神经元设置中证明LRR成功的情况比IMP更多，因为它可以摆脱",
    "tldr": "学习率回溯（LRR）通过早期翻转参数符号且对符号扰动保持稳健性的能力，不仅在掩模识别方面更有效，而且可以优化各种掩模，包括随机掩模。"
}