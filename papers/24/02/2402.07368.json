{
    "title": "Assessing Generalization for Subpopulation Representative Modeling via In-Context Learning",
    "abstract": "This study evaluates the ability of Large Language Model (LLM)-based Subpopulation Representative Models (SRMs) to generalize from empirical data, utilizing in-context learning with data from the 2016 and 2020 American National Election Studies. We explore generalization across response variables and demographic subgroups. While conditioning with empirical data improves performance on the whole, the benefit of in-context learning varies considerably across demographics, sometimes hurting performance for one demographic while helping performance for others. The inequitable benefits of in-context learning for SRM present a challenge for practitioners implementing SRMs, and for decision-makers who might come to rely on them. Our work highlights a need for fine-grained benchmarks captured from diverse subpopulations that test not only fidelity but generalization.",
    "link": "https://arxiv.org/abs/2402.07368",
    "context": "Title: Assessing Generalization for Subpopulation Representative Modeling via In-Context Learning\nAbstract: This study evaluates the ability of Large Language Model (LLM)-based Subpopulation Representative Models (SRMs) to generalize from empirical data, utilizing in-context learning with data from the 2016 and 2020 American National Election Studies. We explore generalization across response variables and demographic subgroups. While conditioning with empirical data improves performance on the whole, the benefit of in-context learning varies considerably across demographics, sometimes hurting performance for one demographic while helping performance for others. The inequitable benefits of in-context learning for SRM present a challenge for practitioners implementing SRMs, and for decision-makers who might come to rely on them. Our work highlights a need for fine-grained benchmarks captured from diverse subpopulations that test not only fidelity but generalization.",
    "path": "papers/24/02/2402.07368.json",
    "total_tokens": 936,
    "translated_title": "通过上下文学习评估分组代表建模的泛化能力",
    "translated_abstract": "本研究通过对2016年和2020年美国全国选举研究的数据进行上下文学习，评估基于大型语言模型（LLM）的分组代表模型（SRMs）从实证数据中的泛化能力。我们探讨了在不同响应变量和人口子群组之间的泛化能力。尽管使用实证数据进行条件设定可以提高整体性能，但上下文学习的益处在不同人口子群组之间差异很大，有时对某个人口子群组的性能产生了负面影响，但对其他人口子群组的性能产生了积极影响。上下文学习对SRM的不公平益处为实施SRM的从业人员和依赖于其的决策者带来了挑战。我们的工作突出了对来自不同人口子群组的细粒度基准的需求，这些基准不仅测试忠实度，还测试泛化能力。",
    "tldr": "本研究通过使用2016年和2020年的选举数据，评估了基于大型语言模型的分组代表模型在泛化能力上的表现。研究发现，尽管使用实证数据进行条件设定可以提高整体性能，但上下文学习的益处在不同人口子群组之间差异很大，这对实施分组代表模型的从业人员和决策者构成了挑战。"
}