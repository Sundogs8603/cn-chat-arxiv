{
    "title": "Provably Sample Efficient RLHF via Active Preference Optimization",
    "abstract": "arXiv:2402.10500v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) is pivotal in aligning Large Language Models (LLMs) with human preferences. While these aligned generative models have demonstrated impressive capabilities across various tasks, the dependence on high-quality human preference data poses a costly bottleneck in practical implementation of RLHF. Hence better and adaptive strategies for data collection is needed. To this end, we frame RLHF as a contextual preference bandit problem with prompts as contexts and show that the naive way of collecting preference data by choosing prompts uniformly at random leads to a policy that suffers an $\\Omega(1)$ suboptimality gap in rewards. Then we propose $\\textit{Active Preference Optimization}$ ($\\texttt{APO}$), an algorithm that actively selects prompts to collect preference data. Under the Bradley-Terry-Luce (BTL) preference model, \\texttt{APO} achieves sample efficiency without compromising on polic",
    "link": "https://arxiv.org/abs/2402.10500",
    "context": "Title: Provably Sample Efficient RLHF via Active Preference Optimization\nAbstract: arXiv:2402.10500v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) is pivotal in aligning Large Language Models (LLMs) with human preferences. While these aligned generative models have demonstrated impressive capabilities across various tasks, the dependence on high-quality human preference data poses a costly bottleneck in practical implementation of RLHF. Hence better and adaptive strategies for data collection is needed. To this end, we frame RLHF as a contextual preference bandit problem with prompts as contexts and show that the naive way of collecting preference data by choosing prompts uniformly at random leads to a policy that suffers an $\\Omega(1)$ suboptimality gap in rewards. Then we propose $\\textit{Active Preference Optimization}$ ($\\texttt{APO}$), an algorithm that actively selects prompts to collect preference data. Under the Bradley-Terry-Luce (BTL) preference model, \\texttt{APO} achieves sample efficiency without compromising on polic",
    "path": "papers/24/02/2402.10500.json",
    "total_tokens": 862,
    "translated_title": "通过主动偏好优化实现经验证的样本效率的RLHF",
    "translated_abstract": "强化学习从人类反馈（RLHF）在将大型语言模型（LLMs）与人类偏好相一致方面至关重要。虽然这些对齐的生成模型已经在各种任务中展示出令人印象深刻的能力，但是依赖高质量的人类偏好数据在实际RLHF实施中构成了昂贵的瓶颈。因此，需要更好和自适应的数据收集策略。为此，我们将RLHF以上下文偏好赌博机问题的形式框定，其中提示作为上下文，并表明通过随机选择提示收集偏好数据的天真方式导致一个在奖励方面具有$\\Omega(1)$次优性差距的策略。然后，我们提出了$\\textit{Active Preference Optimization}$（$\\texttt{APO}$）算法，该算法积极选择提示以收集偏好数据。在Bradley-Terry-Luce（BTL）偏好模型下，\\texttt{APO}实现了样本效率，而不会妥协于polic",
    "tldr": "通过Active Preference Optimization算法，在Bradley-Terry-Luce偏好模型下实现了RLHF的样本效率提高，优化了对提示收集偏好数据的策略。",
    "en_tdlr": "The sample efficiency of RLHF is improved through Active Preference Optimization algorithm under the Bradley-Terry-Luce preference model, optimizing the strategy for collecting preference data on prompts."
}