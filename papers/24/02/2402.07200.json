{
    "title": "Outlier-Aware Training for Low-Bit Quantization of Structural Re-Parameterized Networks",
    "abstract": "Lightweight design of Convolutional Neural Networks (CNNs) requires co-design efforts in the model architectures and compression techniques. As a novel design paradigm that separates training and inference, a structural re-parameterized (SR) network such as the representative RepVGG revitalizes the simple VGG-like network with a high accuracy comparable to advanced and often more complicated networks. However, the merging process in SR networks introduces outliers into weights, making their distribution distinct from conventional networks and thus heightening difficulties in quantization. To address this, we propose an operator-level improvement for training called Outlier Aware Batch Normalization (OABN). Additionally, to meet the demands of limited bitwidths while upkeeping the inference accuracy, we develop a clustering-based non-uniform quantization framework for Quantization-Aware Training (QAT) named ClusterQAT. Integrating OABN with ClusterQAT, the quantized performance of RepVG",
    "link": "https://arxiv.org/abs/2402.07200",
    "context": "Title: Outlier-Aware Training for Low-Bit Quantization of Structural Re-Parameterized Networks\nAbstract: Lightweight design of Convolutional Neural Networks (CNNs) requires co-design efforts in the model architectures and compression techniques. As a novel design paradigm that separates training and inference, a structural re-parameterized (SR) network such as the representative RepVGG revitalizes the simple VGG-like network with a high accuracy comparable to advanced and often more complicated networks. However, the merging process in SR networks introduces outliers into weights, making their distribution distinct from conventional networks and thus heightening difficulties in quantization. To address this, we propose an operator-level improvement for training called Outlier Aware Batch Normalization (OABN). Additionally, to meet the demands of limited bitwidths while upkeeping the inference accuracy, we develop a clustering-based non-uniform quantization framework for Quantization-Aware Training (QAT) named ClusterQAT. Integrating OABN with ClusterQAT, the quantized performance of RepVG",
    "path": "papers/24/02/2402.07200.json",
    "total_tokens": 895,
    "translated_title": "异常值感知的结构再参数网络低比特量化训练",
    "translated_abstract": "轻量级卷积神经网络(CNNs)的设计需要在模型架构和压缩技术方面进行协同设计。作为一种将训练和推断分离的新设计范式，结构再参数化(SR)网络，如代表性的RepVGG，使简单的VGG样式网络焕发了新的生机，具有与先进且常常更复杂的网络相当的高准确度。然而，SR网络中的合并过程会在权重中引入异常值，使其分布与传统网络不同，因此增加了量化的困难。为了解决这个问题，我们提出了一种称为异常值感知批归一化(OABN)的操作级改进方法。此外，为了满足有限比特宽度的要求并保持推断准确性，我们开发了一种基于聚类的非均匀量化训练(ClusterQAT)框架。将OABN与ClusterQAT相结合，可以提高RepVG的量化性能。",
    "tldr": "这项研究提出了一种异常值感知批归一化(OABN)和一种聚类的非均匀量化训练(ClusterQAT)框架，用于处理结构再参数化网络(SR)中由合并过程引入的异常值，以提高量化性能。",
    "en_tdlr": "This research proposes an Outlier Aware Batch Normalization (OABN) and a clustering-based non-uniform quantization framework (ClusterQAT) to address the outliers introduced by the merging process in Structural Re-Parameterized (SR) networks, improving the quantization performance."
}