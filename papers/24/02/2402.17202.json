{
    "title": "FedBRB: An Effective Solution to the Small-to-Large Scenario in Device-Heterogeneity Federated Learning",
    "abstract": "arXiv:2402.17202v1 Announce Type: new  Abstract: Recently, the success of large models has demonstrated the importance of scaling up model size. This has spurred interest in exploring collaborative training of large-scale models from federated learning perspective. Due to computational constraints, many institutions struggle to train a large-scale model locally. Thus, training a larger global model using only smaller local models has become an important scenario (i.e., the \\textbf{small-to-large scenario}). Although recent device-heterogeneity federated learning approaches have started to explore this area, they face limitations in fully covering the parameter space of the global model. In this paper, we propose a method called \\textbf{FedBRB} (\\underline{B}lock-wise \\underline{R}olling and weighted \\underline{B}roadcast) based on the block concept. FedBRB can uses small local models to train all blocks of the large global model, and broadcasts the trained parameters to the entire spac",
    "link": "https://arxiv.org/abs/2402.17202",
    "context": "Title: FedBRB: An Effective Solution to the Small-to-Large Scenario in Device-Heterogeneity Federated Learning\nAbstract: arXiv:2402.17202v1 Announce Type: new  Abstract: Recently, the success of large models has demonstrated the importance of scaling up model size. This has spurred interest in exploring collaborative training of large-scale models from federated learning perspective. Due to computational constraints, many institutions struggle to train a large-scale model locally. Thus, training a larger global model using only smaller local models has become an important scenario (i.e., the \\textbf{small-to-large scenario}). Although recent device-heterogeneity federated learning approaches have started to explore this area, they face limitations in fully covering the parameter space of the global model. In this paper, we propose a method called \\textbf{FedBRB} (\\underline{B}lock-wise \\underline{R}olling and weighted \\underline{B}roadcast) based on the block concept. FedBRB can uses small local models to train all blocks of the large global model, and broadcasts the trained parameters to the entire spac",
    "path": "papers/24/02/2402.17202.json",
    "total_tokens": 892,
    "translated_title": "FedBRB：解决设备异构联邦学习中小规模到大规模场景的有效方案",
    "translated_abstract": "最近，大型模型的成功证明了扩大模型规模的重要性。这引发了从联邦学习的角度探索大规模模型协作训练的兴趣。由于计算约束，许多机构难以在本地训练大规模模型。因此，仅使用较小的本地模型训练更大的全局模型已成为一个重要情景，即\\textbf{小规模到大规模场景}。尽管最近设备异构联邦学习方法已经开始探索这一领域，但它们在完全涵盖全局模型的参数空间方面存在局限性。在本文中，我们基于块概念提出了一种称为\\textbf{FedBRB}（基于块概念的\\underline{B}locking和加权\\underline{R}olling与\\underline{B}roadcasting）的方法。FedBRB可以使用小型本地模型训练大型全局模型的所有块，并将训练好的参数广播到整个空间。",
    "tldr": "FedBRB方法提出了基于块概念的解决方案，实现了使用小型本地模型训练大型全局模型的所有块，并在设备异构联邦学习中有效应用。",
    "en_tdlr": "FedBRB proposes a block-wise solution that enables training of all blocks of a large global model using small local models, effectively applied in device-heterogeneity federated learning."
}