{
    "title": "Auditing Counterfire: Evaluating Advanced Counterargument Generation with Evidence and Style",
    "abstract": "We present a novel dataset for the controlled composition of counterarguments designed for further applications in argument refining, mining, and evaluation. Our dataset constitutes enriched counter-arguments to posts in the Reddit ChangeMyView dataset that are integrated with evidence retrieved from high-quality sources and generated based on user preferences, adjusting the critical attributes of evidence and argument style. The resultant Counterfire corpus comprises arguments generated from GPT-3.5 turbo, Koala, and PaLM 2 models and two of their finetuned variants (N = 32,000). Model evaluation indicates strong paraphrasing abilities with evidence, albeit limited word overlap, while demonstrating high style integration (0.9682 for 'reciprocity'), showing the ability of LLM to assimilate diverse styles. Of all models, GPT-3.5 turbo showed the highest scores in argument quality evaluation, showing consistent accuracy (score >0.8). In further analyses, reciprocity-style counterargument",
    "link": "https://arxiv.org/abs/2402.08498",
    "context": "Title: Auditing Counterfire: Evaluating Advanced Counterargument Generation with Evidence and Style\nAbstract: We present a novel dataset for the controlled composition of counterarguments designed for further applications in argument refining, mining, and evaluation. Our dataset constitutes enriched counter-arguments to posts in the Reddit ChangeMyView dataset that are integrated with evidence retrieved from high-quality sources and generated based on user preferences, adjusting the critical attributes of evidence and argument style. The resultant Counterfire corpus comprises arguments generated from GPT-3.5 turbo, Koala, and PaLM 2 models and two of their finetuned variants (N = 32,000). Model evaluation indicates strong paraphrasing abilities with evidence, albeit limited word overlap, while demonstrating high style integration (0.9682 for 'reciprocity'), showing the ability of LLM to assimilate diverse styles. Of all models, GPT-3.5 turbo showed the highest scores in argument quality evaluation, showing consistent accuracy (score >0.8). In further analyses, reciprocity-style counterargument",
    "path": "papers/24/02/2402.08498.json",
    "total_tokens": 1004,
    "translated_title": "审计反火：评估具有证据和风格的先进反驳生成",
    "translated_abstract": "我们提出了一个新颖的数据集，用于控制性反驳的合成，旨在进一步应用于论证的改进、挖掘和评估。我们的数据集包含与Reddit ChangeMyView数据集中的帖子相结合的丰富的反驳，这些反驳融入了从高质量来源中检索到的证据，并根据用户偏好生成，调整了证据和论证风格的关键属性。由此产生的Counterfire语料库包括从GPT-3.5 turbo、Koala和PaLM 2模型以及它们的两个微调变体生成的论证（N = 32,000）。模型评估表明，在证据方面具有强大的改写能力，尽管词汇重叠有限，同时表现出高度的风格融合（对于“互惠”的得分为0.9682），显示了LLM融合多样风格的能力。在所有模型中，GPT-3.5 turbo在论证质量评估中显示出最高分数，表现出一致准确性（得分 >0.8）。在进一步的分析中，互惠式反驳证明效果最佳，能够产生更好的论证结果。",
    "tldr": "这项研究提出了一个新的数据集，用于生成具有证据和风格的反驳，该数据集基于Reddit ChangeMyView数据集中的帖子，并可用于论证的改进和评估。评估结果显示，GPT-3.5 turbo模型在论证质量方面表现出色，并且具有很高的风格融合能力。互惠式反驳的效果最佳。",
    "en_tdlr": "This study introduces a novel dataset for generating counterarguments with evidence and style, utilizing posts from the Reddit ChangeMyView dataset. Evaluation results indicate that the GPT-3.5 turbo model performs well in argument quality and has high style integration. Reciprocity-style counterarguments show the best effectiveness."
}