{
    "title": "LAPDoc: Layout-Aware Prompting for Documents",
    "abstract": "arXiv:2402.09841v1 Announce Type: new  Abstract: Recent advances in training large language models (LLMs) using massive amounts of solely textual data lead to strong generalization across many domains and tasks, including document-specific tasks. Opposed to that there is a trend to train multi-modal transformer architectures tailored for document understanding that are designed specifically to fuse textual inputs with the corresponding document layout. This involves a separate fine-tuning step for which additional training data is required. At present, no document transformers with comparable generalization to LLMs are available That raises the question which type of model is to be preferred for document understanding tasks. In this paper we investigate the possibility to use purely text-based LLMs for document-specific tasks by using layout enrichment. We explore drop-in modifications and rule-based methods to enrich purely textual LLM prompts with layout information. In our experimen",
    "link": "https://arxiv.org/abs/2402.09841",
    "context": "Title: LAPDoc: Layout-Aware Prompting for Documents\nAbstract: arXiv:2402.09841v1 Announce Type: new  Abstract: Recent advances in training large language models (LLMs) using massive amounts of solely textual data lead to strong generalization across many domains and tasks, including document-specific tasks. Opposed to that there is a trend to train multi-modal transformer architectures tailored for document understanding that are designed specifically to fuse textual inputs with the corresponding document layout. This involves a separate fine-tuning step for which additional training data is required. At present, no document transformers with comparable generalization to LLMs are available That raises the question which type of model is to be preferred for document understanding tasks. In this paper we investigate the possibility to use purely text-based LLMs for document-specific tasks by using layout enrichment. We explore drop-in modifications and rule-based methods to enrich purely textual LLM prompts with layout information. In our experimen",
    "path": "papers/24/02/2402.09841.json",
    "total_tokens": 762,
    "translated_title": "LAPDoc：面向文档的布局感知提示",
    "translated_abstract": "最近，使用大量纯文本数据训练大型语言模型(LLM)取得了重大突破，在许多领域和任务中实现了强大的泛化能力，包括文档特定任务。相比之下，训练针对文档理解的多模态变压器体系结构，专门设计用于将文本输入与相应的文档布局融合。这需要单独的微调步骤，需要额外的训练数据。目前，尚没有具有与LLM相当泛化能力的文档变压器可用。这引发了一个问题，即在文档理解任务中应该选择哪种类型的模型。在本文中，我们通过使用布局增强来调查使用纯文本LLM用于文档特定任务的可能性。我们探索了添加修改和基于规则的方法，以在纯文本LLM提示中添加布局信息。",
    "tldr": "本文研究了通过使用布局增强来使用纯文本LLM进行文档特定任务的可能性。",
    "en_tdlr": "This paper investigates the possibility of using layout enrichment to employ purely text-based LLMs for document-specific tasks."
}