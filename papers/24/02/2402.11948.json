{
    "title": "Mini-Hes: A Parallelizable Second-order Latent Factor Analysis Model",
    "abstract": "arXiv:2402.11948v1 Announce Type: cross  Abstract: Interactions among large number of entities is naturally high-dimensional and incomplete (HDI) in many big data related tasks. Behavioral characteristics of users are hidden in these interactions, hence, effective representation of the HDI data is a fundamental task for understanding user behaviors. Latent factor analysis (LFA) model has proven to be effective in representing HDI data. The performance of an LFA model relies heavily on its training process, which is a non-convex optimization. It has been proven that incorporating local curvature and preprocessing gradients during its training process can lead to superior performance compared to LFA models built with first-order family methods. However, with the escalation of data volume, the feasibility of second-order algorithms encounters challenges. To address this pivotal issue, this paper proposes a mini-block diagonal hessian-free (Mini-Hes) optimization for building an LFA model.",
    "link": "https://arxiv.org/abs/2402.11948",
    "context": "Title: Mini-Hes: A Parallelizable Second-order Latent Factor Analysis Model\nAbstract: arXiv:2402.11948v1 Announce Type: cross  Abstract: Interactions among large number of entities is naturally high-dimensional and incomplete (HDI) in many big data related tasks. Behavioral characteristics of users are hidden in these interactions, hence, effective representation of the HDI data is a fundamental task for understanding user behaviors. Latent factor analysis (LFA) model has proven to be effective in representing HDI data. The performance of an LFA model relies heavily on its training process, which is a non-convex optimization. It has been proven that incorporating local curvature and preprocessing gradients during its training process can lead to superior performance compared to LFA models built with first-order family methods. However, with the escalation of data volume, the feasibility of second-order algorithms encounters challenges. To address this pivotal issue, this paper proposes a mini-block diagonal hessian-free (Mini-Hes) optimization for building an LFA model.",
    "path": "papers/24/02/2402.11948.json",
    "total_tokens": 830,
    "translated_title": "Mini-Hes：一种可并行化的二阶潜在因子分析模型",
    "translated_abstract": "与大量实体之间的交互在许多与大数据相关的任务中自然是高维且不完整的（HDI）。用户的行为特征隐藏在这些交互中，因此，有效地表示HDI数据是理解用户行为的基本任务。潜在因子分析（LFA）模型已被证明在表示HDI数据方面是有效的。 LFA模型的性能严重依赖于其训练过程，这是一个非凸优化问题。已经证明，在其训练过程中同时包含局部曲率和预处理梯度可以比使用一阶方法构建的LFA模型表现出更优异的性能。然而，随着数据量的增加，二阶算法的可行性面临挑战。为解决这一关键问题，本文提出了一种用于构建LFA模型的mini-block对角黑塞无约束（Mini-Hes）优化方法。",
    "tldr": "Mini-Hes提出了一种新的mini-block对角黑塞无约束优化方法，用于构建二阶潜在因子分析模型，解决了大数据量下二阶算法可行性的挑战。",
    "en_tdlr": "Mini-Hes proposes a new mini-block diagonal Hessian-free optimization method for building second-order latent factor analysis model, addressing the challenge of feasibility of second-order algorithms with large data volume."
}