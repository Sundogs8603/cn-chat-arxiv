{
    "title": "Scalable Graph Self-Supervised Learning",
    "abstract": "arXiv:2402.09603v1 Announce Type: cross  Abstract: In regularization Self-Supervised Learning (SSL) methods for graphs, computational complexity increases with the number of nodes in graphs and embedding dimensions. To mitigate the scalability of non-contrastive graph SSL, we propose a novel approach to reduce the cost of computing the covariance matrix for the pre-training loss function with volume-maximization terms. Our work focuses on reducing the cost associated with the loss computation via graph node or dimension sampling. We provide theoretical insight into why dimension sampling would result in accurate loss computations and support it with mathematical derivation of the novel approach. We develop our experimental setup on the node-level graph prediction tasks, where SSL pre-training has shown to be difficult due to the large size of real world graphs. Our experiments demonstrate that the cost associated with the loss computation can be reduced via node or dimension sampling w",
    "link": "https://arxiv.org/abs/2402.09603",
    "context": "Title: Scalable Graph Self-Supervised Learning\nAbstract: arXiv:2402.09603v1 Announce Type: cross  Abstract: In regularization Self-Supervised Learning (SSL) methods for graphs, computational complexity increases with the number of nodes in graphs and embedding dimensions. To mitigate the scalability of non-contrastive graph SSL, we propose a novel approach to reduce the cost of computing the covariance matrix for the pre-training loss function with volume-maximization terms. Our work focuses on reducing the cost associated with the loss computation via graph node or dimension sampling. We provide theoretical insight into why dimension sampling would result in accurate loss computations and support it with mathematical derivation of the novel approach. We develop our experimental setup on the node-level graph prediction tasks, where SSL pre-training has shown to be difficult due to the large size of real world graphs. Our experiments demonstrate that the cost associated with the loss computation can be reduced via node or dimension sampling w",
    "path": "papers/24/02/2402.09603.json",
    "total_tokens": 820,
    "translated_title": "可扩展图自监督学习",
    "translated_abstract": "在图的正则化自监督学习方法中，计算复杂度随节点数和嵌入维度的增加而增加。为了减轻非对比图自监督学习的可扩展性问题，我们提出了一种新方法，通过体积最大化项减少预训练损失函数的协方差矩阵计算成本。我们的工作重点是通过图节点或维度采样减少损失计算的成本。我们从理论上解释了为什么维度采样会导致准确的损失计算，并用数学推导支持了这种新方法。我们在节点级图预测任务上进行了实验，因为现实世界图的规模很大，所以在这方面进行自监督预训练是困难的。我们的实验表明，通过节点或维度采样可以减少损失计算的成本。",
    "tldr": "该论文提出了一种通过体积最大化项减少图自监督学习预训练损失函数计算成本的方法。实验证明，采用节点或维度采样可以降低损失计算的成本。",
    "en_tdlr": "This paper proposes a method for reducing the cost of computing the pre-training loss function in graph self-supervised learning through volume-maximization terms. Experiments demonstrate that using node or dimension sampling can reduce the cost of loss computation."
}