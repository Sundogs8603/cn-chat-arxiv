{
    "title": "Learning by Doing: An Online Causal Reinforcement Learning Framework with Causal-Aware Policy",
    "abstract": "As a key component to intuitive cognition and reasoning solutions in human intelligence, causal knowledge provides great potential for reinforcement learning (RL) agents' interpretability towards decision-making by helping reduce the searching space. However, there is still a considerable gap in discovering and incorporating causality into RL, which hinders the rapid development of causal RL. In this paper, we consider explicitly modeling the generation process of states with the causal graphical model, based on which we augment the policy. We formulate the causal structure updating into the RL interaction process with active intervention learning of the environment. To optimize the derived objective, we propose a framework with theoretical performance guarantees that alternates between two steps: using interventions for causal structure learning during exploration and using the learned causal structure for policy guidance during exploitation. Due to the lack of public benchmarks that ",
    "link": "https://arxiv.org/abs/2402.04869",
    "context": "Title: Learning by Doing: An Online Causal Reinforcement Learning Framework with Causal-Aware Policy\nAbstract: As a key component to intuitive cognition and reasoning solutions in human intelligence, causal knowledge provides great potential for reinforcement learning (RL) agents' interpretability towards decision-making by helping reduce the searching space. However, there is still a considerable gap in discovering and incorporating causality into RL, which hinders the rapid development of causal RL. In this paper, we consider explicitly modeling the generation process of states with the causal graphical model, based on which we augment the policy. We formulate the causal structure updating into the RL interaction process with active intervention learning of the environment. To optimize the derived objective, we propose a framework with theoretical performance guarantees that alternates between two steps: using interventions for causal structure learning during exploration and using the learned causal structure for policy guidance during exploitation. Due to the lack of public benchmarks that ",
    "path": "papers/24/02/2402.04869.json",
    "total_tokens": 913,
    "translated_title": "学习通过实践：具有因果感知的在线因果强化学习框架",
    "translated_abstract": "因果知识作为人类智能中直观认知和推理解决方案的关键组成部分，为强化学习（RL）代理的可解释性决策提供了巨大的潜力，通过帮助减少搜索空间。然而，发现和整合因果关系进入RL仍存在相当大的差距，这阻碍了因果关系RL的快速发展。在本文中，我们考虑使用因果图模型明确地建模状态的生成过程，并在此基础上增强策略。我们将因果结构更新形式化为具有主动环境干预学习的RL交互过程。为了优化衍生的目标，我们提出了一个具有理论性能保证的框架，两个步骤交替进行：在探索过程中使用干预进行因果结构学习，在开发过程中使用学习到的因果结构进行策略引导。由于缺少公共基准，用于对所提出的方法进行评估。",
    "tldr": "本文提出了一个在线因果强化学习框架，其中通过明确建模状态的生成过程和使用因果结构进行策略引导，以帮助强化学习代理的决策可解释性。该框架具有理论性能保证，并在探索和开发过程中使用干预进行因果结构学习。",
    "en_tdlr": "This paper proposes an online causal reinforcement learning framework that explicitly models the generation process of states and uses causal structure for policy guidance, aiming to enhance the interpretability of decision-making in reinforcement learning agents. The framework has theoretical performance guarantees and utilizes intervention for causal structure learning during exploration and policy guidance during exploitation."
}