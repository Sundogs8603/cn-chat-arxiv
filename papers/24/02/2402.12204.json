{
    "title": "Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages",
    "abstract": "arXiv:2402.12204v1 Announce Type: new  Abstract: While large language models (LLMs) have been pre-trained on multilingual corpora, their performance still lags behind in most languages compared to a few resource-rich languages. One common approach to mitigate this issue is to translate training data from resource-rich languages into other languages and then continue training. However, using the data obtained solely relying on translation while ignoring the original capabilities of LLMs across languages is not always effective, which we show will limit the performance of cross-lingual knowledge transfer. In this work, we propose SDRRL, a method based on Self-Distillation from Resource-Rich Languages that effectively improve multilingual performance by leveraging the internal capabilities of LLMs on resource-rich languages. We evaluate on different LLMs (LLaMA-2 and SeaLLM) and source languages across various comprehension and generation tasks, experimental results demonstrate that SDRRL",
    "link": "https://arxiv.org/abs/2402.12204",
    "context": "Title: Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages\nAbstract: arXiv:2402.12204v1 Announce Type: new  Abstract: While large language models (LLMs) have been pre-trained on multilingual corpora, their performance still lags behind in most languages compared to a few resource-rich languages. One common approach to mitigate this issue is to translate training data from resource-rich languages into other languages and then continue training. However, using the data obtained solely relying on translation while ignoring the original capabilities of LLMs across languages is not always effective, which we show will limit the performance of cross-lingual knowledge transfer. In this work, we propose SDRRL, a method based on Self-Distillation from Resource-Rich Languages that effectively improve multilingual performance by leveraging the internal capabilities of LLMs on resource-rich languages. We evaluate on different LLMs (LLaMA-2 and SeaLLM) and source languages across various comprehension and generation tasks, experimental results demonstrate that SDRRL",
    "path": "papers/24/02/2402.12204.json",
    "total_tokens": 823,
    "translated_title": "通过从资源丰富的语言进行自蒸馏来增强大型语言模型的多语言能力",
    "translated_abstract": "虽然大型语言模型（LLMs）已经在多语言语料库上进行了预训练，但在大多数语言中，它们的性能仍然落后于少数资源丰富的语言。一个常见的方法是将来自资源丰富语言的训练数据翻译成其他语言，然后继续训练，但是仅依赖翻译获得的数据，而忽略了LLMs跨语言的原始能力，不总是有效的，我们展示将限制跨语言知识转移的性能。在这项工作中，我们提出了SDRRL，一种基于从资源丰富的语言进行自蒸馏的方法，通过利用LLMs在资源丰富语言上的内在能力，有效地提高多语言性能。我们在不同的LLMs（LLaMA-2和SeaLLM）和源语言上评估各种理解和生成任务，实验结果表明SDRRL",
    "tldr": "通过自蒸馏从资源丰富的语言进行方法，提高了大型语言模型在多语言任务上的性能。",
    "en_tdlr": "Enhanced the multilingual capabilities of large language models through self-distillation from resource-rich languages."
}