{
    "title": "LaCo: Large Language Model Pruning via Layer Collapse",
    "abstract": "arXiv:2402.11187v1 Announce Type: cross  Abstract: Large language models (LLMs) based on transformer are witnessing a notable trend of size expansion, which brings considerable costs to both model training and inference. However, existing methods such as model quantization, knowledge distillation, and model pruning are constrained by various issues, including hardware support limitations, the need for extensive training, and alterations to the internal structure of the model. In this paper, we propose a concise layer-wise pruning method called \\textit{Layer Collapse (LaCo)}, in which rear model layers collapse into a prior layer, enabling a rapid reduction in model size while preserving the model structure. Comprehensive experiments show that our method maintains an average task performance of over 80\\% at pruning ratios of 25-30\\%, significantly outperforming existing state-of-the-art structured pruning methods. We also conduct post-training experiments to confirm that the proposed pr",
    "link": "https://arxiv.org/abs/2402.11187",
    "context": "Title: LaCo: Large Language Model Pruning via Layer Collapse\nAbstract: arXiv:2402.11187v1 Announce Type: cross  Abstract: Large language models (LLMs) based on transformer are witnessing a notable trend of size expansion, which brings considerable costs to both model training and inference. However, existing methods such as model quantization, knowledge distillation, and model pruning are constrained by various issues, including hardware support limitations, the need for extensive training, and alterations to the internal structure of the model. In this paper, we propose a concise layer-wise pruning method called \\textit{Layer Collapse (LaCo)}, in which rear model layers collapse into a prior layer, enabling a rapid reduction in model size while preserving the model structure. Comprehensive experiments show that our method maintains an average task performance of over 80\\% at pruning ratios of 25-30\\%, significantly outperforming existing state-of-the-art structured pruning methods. We also conduct post-training experiments to confirm that the proposed pr",
    "path": "papers/24/02/2402.11187.json",
    "total_tokens": 904,
    "translated_title": "LaCo：通过层叠实现大型语言模型的剪枝",
    "translated_abstract": "基于Transformer的大型语言模型（LLMs）正经历着尺寸扩大的明显趋势，这给模型的训练和推理带来了相当大的成本。然而，现有的方法如模型量化、知识蒸馏和模型剪枝受到各种问题的限制，包括硬件支持限制、需要大量的训练和对模型内部结构的改变。在本文中，我们提出了一种简洁的逐层剪枝方法，称为Layer Collapse（LaCo），其中后置模型层折叠到前置层，使模型尺寸迅速减小同时保持模型结构。综合实验表明，我们的方法在剪枝比例达到25-30%时，保持了超过80%的平均任务性能，明显优于现有最先进的结构化剪枝方法。我们还进行了后训练实验以确认所提方法的有效性。",
    "tldr": "提出了一种名为Layer Collapse（LaCo）的简明的逐层剪枝方法，使得大型语言模型可以在保持模型结构的同时迅速减小尺寸，并在剪枝比例达到25-30%时保持超过80%的平均任务性能，明显优于现有最先进的结构化剪枝方法。",
    "en_tdlr": "Introduced a concise layer-wise pruning method called Layer Collapse (LaCo) which enables rapid reduction in the size of large language models while preserving the model structure, maintaining over 80% average task performance at pruning ratios of 25-30%, significantly outperforming existing state-of-the-art structured pruning methods."
}