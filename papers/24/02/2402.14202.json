{
    "title": "Comparing Graph Transformers via Positional Encodings",
    "abstract": "arXiv:2402.14202v1 Announce Type: new  Abstract: The distinguishing power of graph transformers is closely tied to the choice of positional encoding: features used to augment the base transformer with information about the graph. There are two primary types of positional encoding: absolute positional encodings (APEs) and relative positional encodings (RPEs). APEs assign features to each node and are given as input to the transformer. RPEs instead assign a feature to each pair of nodes, e.g., graph distance, and are used to augment the attention block. A priori, it is unclear which method is better for maximizing the power of the resulting graph transformer. In this paper, we aim to understand the relationship between these different types of positional encodings. Interestingly, we show that graph transformers using APEs and RPEs are equivalent in terms of distinguishing power. In particular, we demonstrate how to interchange APEs and RPEs while maintaining their distinguishing power in",
    "link": "https://arxiv.org/abs/2402.14202",
    "context": "Title: Comparing Graph Transformers via Positional Encodings\nAbstract: arXiv:2402.14202v1 Announce Type: new  Abstract: The distinguishing power of graph transformers is closely tied to the choice of positional encoding: features used to augment the base transformer with information about the graph. There are two primary types of positional encoding: absolute positional encodings (APEs) and relative positional encodings (RPEs). APEs assign features to each node and are given as input to the transformer. RPEs instead assign a feature to each pair of nodes, e.g., graph distance, and are used to augment the attention block. A priori, it is unclear which method is better for maximizing the power of the resulting graph transformer. In this paper, we aim to understand the relationship between these different types of positional encodings. Interestingly, we show that graph transformers using APEs and RPEs are equivalent in terms of distinguishing power. In particular, we demonstrate how to interchange APEs and RPEs while maintaining their distinguishing power in",
    "path": "papers/24/02/2402.14202.json",
    "total_tokens": 782,
    "translated_title": "通过位置编码比较图变换器",
    "translated_abstract": "图变换器的区分能力与位置编码的选择紧密相关：用于增强基本变换器与图信息的特征。有两种主要类型的位置编码：绝对位置编码（APEs）和相对位置编码（RPEs）。APEs为每个节点分配特征，并作为变换器的输入。而RPEs则为每对节点（例如，图距离）分配一个特征，并用于增强注意力块。先验上，目前不清楚哪种方法更有利于最大化生成的图变换器的能力。本文旨在了解这两种不同类型位置编码之间的关系。有趣的是，我们展示了使用APEs和RPEs的图变换器在区分能力方面是等效的。特别地，我们展示了如何在保持其区分能力的同时交换APEs和RPEs。",
    "tldr": "本文比较了使用绝对位置编码（APEs）和相对位置编码（RPEs）的图变换器，在最大化区分能力方面是等效的。"
}