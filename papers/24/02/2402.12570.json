{
    "title": "Offline Multi-task Transfer RL with Representational Penalization",
    "abstract": "arXiv:2402.12570v1 Announce Type: new  Abstract: We study the problem of representation transfer in offline Reinforcement Learning (RL), where a learner has access to episodic data from a number of source tasks collected a priori, and aims to learn a shared representation to be used in finding a good policy for a target task. Unlike in online RL where the agent interacts with the environment while learning a policy, in the offline setting there cannot be such interactions in either the source tasks or the target task; thus multi-task offline RL can suffer from incomplete coverage.   We propose an algorithm to compute pointwise uncertainty measures for the learnt representation, and establish a data-dependent upper bound for the suboptimality of the learnt policy for the target task. Our algorithm leverages the collective exploration done by source tasks to mitigate poor coverage at some points by a few tasks, thus overcoming the limitation of needing uniformly good coverage for a meani",
    "link": "https://arxiv.org/abs/2402.12570",
    "context": "Title: Offline Multi-task Transfer RL with Representational Penalization\nAbstract: arXiv:2402.12570v1 Announce Type: new  Abstract: We study the problem of representation transfer in offline Reinforcement Learning (RL), where a learner has access to episodic data from a number of source tasks collected a priori, and aims to learn a shared representation to be used in finding a good policy for a target task. Unlike in online RL where the agent interacts with the environment while learning a policy, in the offline setting there cannot be such interactions in either the source tasks or the target task; thus multi-task offline RL can suffer from incomplete coverage.   We propose an algorithm to compute pointwise uncertainty measures for the learnt representation, and establish a data-dependent upper bound for the suboptimality of the learnt policy for the target task. Our algorithm leverages the collective exploration done by source tasks to mitigate poor coverage at some points by a few tasks, thus overcoming the limitation of needing uniformly good coverage for a meani",
    "path": "papers/24/02/2402.12570.json",
    "total_tokens": 718,
    "translated_title": "离线多任务转移强化学习与表征惩罚",
    "translated_abstract": "我们研究了离线强化学习中表示转移的问题，其中学习者可以访问事先收集的多个源任务的序列数据，并旨在学习一个共享表示，以用于为目标任务找到一个良好的策略。我们提出了一种算法来计算学到的表示的逐点不确定性度量，并为目标任务学到的策略的次优性建立了一个数据相关的上界。我们的算法利用源任务的集体探索来减轻少数任务在某些点的覆盖不足，从而克服了需要平均覆盖良好的限制。",
    "tldr": "提出了一种计算学习表示不确定性度量的算法，为目标任务学到的策略的次优性建立了数据相关的上界。",
    "en_tdlr": "Proposed an algorithm to compute uncertainty measures for learned representations and established a data-dependent upper bound for the suboptimality of the learned policy for the target task."
}