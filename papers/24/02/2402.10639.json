{
    "title": "Generalizability of Mixture of Domain-Specific Adapters from the Lens of Signed Weight Directions and its Application to Effective Model Pruning",
    "abstract": "arXiv:2402.10639v1 Announce Type: new  Abstract: Several parameter-efficient fine-tuning methods based on adapters have been proposed as a streamlined approach to incorporate not only a single specialized knowledge into existing Pre-Trained Language Models (PLMs) but also multiple of them at once. Recent works such as AdapterSoup propose to mix not all but only a selective sub-set of domain-specific adapters during inference via model weight averaging to optimize performance on novel, unseen domains with excellent computational efficiency. However, the essential generalizability of this emerging weight-space adapter mixing mechanism on unseen, in-domain examples remains unexplored. Thus, in this study, we conduct a comprehensive analysis to elucidate the generalizability of domain-specific adapter mixtures in in-domain evaluation. We also provide investigations into the inner workings of the mixture of domain-specific adapters by analyzing their weight signs, yielding critical analysis",
    "link": "https://arxiv.org/abs/2402.10639",
    "context": "Title: Generalizability of Mixture of Domain-Specific Adapters from the Lens of Signed Weight Directions and its Application to Effective Model Pruning\nAbstract: arXiv:2402.10639v1 Announce Type: new  Abstract: Several parameter-efficient fine-tuning methods based on adapters have been proposed as a streamlined approach to incorporate not only a single specialized knowledge into existing Pre-Trained Language Models (PLMs) but also multiple of them at once. Recent works such as AdapterSoup propose to mix not all but only a selective sub-set of domain-specific adapters during inference via model weight averaging to optimize performance on novel, unseen domains with excellent computational efficiency. However, the essential generalizability of this emerging weight-space adapter mixing mechanism on unseen, in-domain examples remains unexplored. Thus, in this study, we conduct a comprehensive analysis to elucidate the generalizability of domain-specific adapter mixtures in in-domain evaluation. We also provide investigations into the inner workings of the mixture of domain-specific adapters by analyzing their weight signs, yielding critical analysis",
    "path": "papers/24/02/2402.10639.json",
    "total_tokens": 909,
    "translated_title": "基于符号权重方向的领域特定适配器混合的泛化与其在有效模型剪枝中的应用",
    "translated_abstract": "基于适配器的几种参数高效的微调方法被提出作为一种简化的方法，不仅能将单一专业知识整合到现有的预训练语言模型（PLM）中，还能一次性整合多个专业知识。最近的作品如AdapterSoup提出了通过模型权重平均化在推理过程中混合领域特定适配器的方法，以优化在新领域中的性能，并具有出色的计算效率。然而，当前这种新兴的权重空间适配器混合机制在未知的领域内例子上的基本泛化性仍未被探讨。因此，在本研究中，我们进行了全面分析，阐明了领域特定适配器混合在领域内评估中的泛化性。我们还通过分析它们的权重符号来深入研究领域特定适配器混合的内部运作，得出了关键的分析。",
    "tldr": "本研究对领域特定适配器混合在领域内评估中的泛化性进行了全面分析，并探讨了混合适配器的内部运作，为适应新领域的性能优化提供了关键洞见",
    "en_tdlr": "This study provides a comprehensive analysis of the generalizability of domain-specific adapter mixtures in in-domain evaluation and investigates the inner workings of the mixed adapters, offering critical insights for optimizing performance in new domains."
}