{
    "title": "FlexHB: a More Efficient and Flexible Framework for Hyperparameter Optimization",
    "abstract": "arXiv:2402.13641v1 Announce Type: new  Abstract: Given a Hyperparameter Optimization(HPO) problem, how to design an algorithm to find optimal configurations efficiently? Bayesian Optimization(BO) and the multi-fidelity BO methods employ surrogate models to sample configurations based on history evaluations. More recent studies obtain better performance by integrating BO with HyperBand(HB), which accelerates evaluation by early stopping mechanism. However, these methods ignore the advantage of a suitable evaluation scheme over the default HyperBand, and the capability of BO is still constrained by skewed evaluation results. In this paper, we propose FlexHB, a new method pushing multi-fidelity BO to the limit as well as re-designing a framework for early stopping with Successive Halving(SH). Comprehensive study on FlexHB shows that (1) our fine-grained fidelity method considerably enhances the efficiency of searching optimal configurations, (2) our FlexBand framework (self-adaptive alloc",
    "link": "https://arxiv.org/abs/2402.13641",
    "context": "Title: FlexHB: a More Efficient and Flexible Framework for Hyperparameter Optimization\nAbstract: arXiv:2402.13641v1 Announce Type: new  Abstract: Given a Hyperparameter Optimization(HPO) problem, how to design an algorithm to find optimal configurations efficiently? Bayesian Optimization(BO) and the multi-fidelity BO methods employ surrogate models to sample configurations based on history evaluations. More recent studies obtain better performance by integrating BO with HyperBand(HB), which accelerates evaluation by early stopping mechanism. However, these methods ignore the advantage of a suitable evaluation scheme over the default HyperBand, and the capability of BO is still constrained by skewed evaluation results. In this paper, we propose FlexHB, a new method pushing multi-fidelity BO to the limit as well as re-designing a framework for early stopping with Successive Halving(SH). Comprehensive study on FlexHB shows that (1) our fine-grained fidelity method considerably enhances the efficiency of searching optimal configurations, (2) our FlexBand framework (self-adaptive alloc",
    "path": "papers/24/02/2402.13641.json",
    "total_tokens": 823,
    "translated_title": "FlexHB: 用于超参数优化的更高效灵活框架",
    "translated_abstract": "给定一个超参数优化（HPO）问题，如何设计一种算法来高效地找到最佳配置？贝叶斯优化（BO）和多保真度BO方法利用替代模型根据历史评估来采样配置。更近期的研究通过将BO与HyperBand（HB）相结合获得了更好的性能，后者通过提前停止机制加快评估。然而，这些方法忽略了适当评估方案相对于默认HyperBand的优势，而且BO的能力仍受到倾斜评估结果的限制。本文提出了FlexHB，一种将多保真度BO推至极限并重新设计早停框架与连续减半（SH）的新方法。对FlexHB的全面研究表明（1）我们的细粒度保真度方法显著提高了搜索最佳配置的效率，（2）我们的FlexBand框架（自适应配置）",
    "tldr": "FlexHB通过细粒度保真度方法提高了搜索最佳配置的效率，重新设计了早停框架，并结合了连续减半的方法。"
}