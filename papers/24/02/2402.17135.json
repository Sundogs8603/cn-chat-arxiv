{
    "title": "Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings",
    "abstract": "arXiv:2402.17135v1 Announce Type: cross  Abstract: Can we pre-train a generalist agent from a large amount of unlabeled offline trajectories such that it can be immediately adapted to any new downstream tasks in a zero-shot manner? In this work, we present a functional reward encoding (FRE) as a general, scalable solution to this zero-shot RL problem. Our main idea is to learn functional representations of any arbitrary tasks by encoding their state-reward samples using a transformer-based variational auto-encoder. This functional encoding not only enables the pre-training of an agent from a wide diversity of general unsupervised reward functions, but also provides a way to solve any new downstream tasks in a zero-shot manner, given a small number of reward-annotated samples. We empirically show that FRE agents trained on diverse random unsupervised reward functions can generalize to solve novel tasks in a range of simulated robotic benchmarks, often outperforming previous zero-shot RL",
    "link": "https://arxiv.org/abs/2402.17135",
    "context": "Title: Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings\nAbstract: arXiv:2402.17135v1 Announce Type: cross  Abstract: Can we pre-train a generalist agent from a large amount of unlabeled offline trajectories such that it can be immediately adapted to any new downstream tasks in a zero-shot manner? In this work, we present a functional reward encoding (FRE) as a general, scalable solution to this zero-shot RL problem. Our main idea is to learn functional representations of any arbitrary tasks by encoding their state-reward samples using a transformer-based variational auto-encoder. This functional encoding not only enables the pre-training of an agent from a wide diversity of general unsupervised reward functions, but also provides a way to solve any new downstream tasks in a zero-shot manner, given a small number of reward-annotated samples. We empirically show that FRE agents trained on diverse random unsupervised reward functions can generalize to solve novel tasks in a range of simulated robotic benchmarks, often outperforming previous zero-shot RL",
    "path": "papers/24/02/2402.17135.json",
    "total_tokens": 856,
    "translated_title": "通过功能奖励编码实现的无监督零样本强化学习",
    "translated_abstract": "这项工作提出了一种称为功能奖励编码（FRE）的通用、可扩展的解决方案，用于零样本强化学习问题。我们的主要想法是通过使用基于transformer的变分自动编码器对任意任务的状态-奖励样本进行编码，从而学习任意任务的功能表示。这种功能编码不仅使得能够从各种通用无监督奖励函数进行预训练，而且还提供了一种在零样本情况下解决任何新的下游任务的方法，只需少量奖励注释样本。我们在实验中显示，针对多样的随机无监督奖励函数进行训练的FRE代理能够推广到解决一系列模拟机器人基准测试中的新任务，通常优于先前的零样本强化学习方法。",
    "tldr": "通过功能奖励编码实现的无监督零样本强化学习方法能够在各种模拟机器人基准测试中训练代理并成功解决新任务，相比以往的零样本强化学习方法表现更优秀。"
}