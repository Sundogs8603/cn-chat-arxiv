{
    "title": "Quantum linear algebra is all you need for Transformer architectures",
    "abstract": "arXiv:2402.16714v1 Announce Type: cross  Abstract: Generative machine learning methods such as large-language models are revolutionizing the creation of text and images. While these models are powerful they also harness a large amount of computational resources. The transformer is a key component in large language models that aims to generate a suitable completion of a given partial sequence. In this work, we investigate transformer architectures under the lens of fault-tolerant quantum computing. The input model is one where pre-trained weight matrices are given as block encodings to construct the query, key, and value matrices for the transformer. As a first step, we show how to prepare a block encoding of the self-attention matrix, with a row-wise application of the softmax function using the Hadamard product. In addition, we combine quantum subroutines to construct important building blocks in the transformer, the residual connection, layer normalization, and the feed-forward neura",
    "link": "https://arxiv.org/abs/2402.16714",
    "context": "Title: Quantum linear algebra is all you need for Transformer architectures\nAbstract: arXiv:2402.16714v1 Announce Type: cross  Abstract: Generative machine learning methods such as large-language models are revolutionizing the creation of text and images. While these models are powerful they also harness a large amount of computational resources. The transformer is a key component in large language models that aims to generate a suitable completion of a given partial sequence. In this work, we investigate transformer architectures under the lens of fault-tolerant quantum computing. The input model is one where pre-trained weight matrices are given as block encodings to construct the query, key, and value matrices for the transformer. As a first step, we show how to prepare a block encoding of the self-attention matrix, with a row-wise application of the softmax function using the Hadamard product. In addition, we combine quantum subroutines to construct important building blocks in the transformer, the residual connection, layer normalization, and the feed-forward neura",
    "path": "papers/24/02/2402.16714.json",
    "total_tokens": 631,
    "translated_title": "量子线性代数是Transformer架构所需的一切",
    "translated_abstract": "生成式机器学习方法如大型语言模型正在彻底改变文本和图像的创作。本文通过容错性量子计算的视角研究了Transformer架构。我们展示了如何准备self-attention矩阵的块编码，并结合量子子程序构建了Transformer中的重要组成部分。",
    "tldr": "本文研究了在容错性量子计算的视角下Transformer架构，展示了如何利用量子线性代数构建Transformer的关键组件。",
    "en_tdlr": "This paper investigates Transformer architectures from the perspective of fault-tolerant quantum computing, demonstrating how to utilize quantum linear algebra to construct key components of Transformers."
}