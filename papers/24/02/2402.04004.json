{
    "title": "Understanding the Effect of Noise in LLM Training Data with Algorithmic Chains of Thought",
    "abstract": "During both pretraining and fine-tuning, Large Language Models (\\textbf{LLMs}) are trained on trillions of tokens of text of widely varying quality. Both phases of training typically involve heuristically filtering out ``low-quality'' or \\textit{noisy} training samples, yet little is known quantitatively about how the type or intensity of noise affects downstream performance. In this work, we study how noise in chain of thought (\\textbf{CoT}) impacts task performance in the highly-controlled setting of algorithmically solvable tasks. First, we develop the Traced Integer (\\textbf{TInt}) framework to generate highly customizable noised execution traces for any arithmetic function on lists of integers. We then define two types of noise: \\textit{static} noise, a local form of noise which is applied after the CoT trace is computed, and \\textit{dynamic} noise, a global form of noise which propagates errors in the trace as it is computed. We then evaluate the test performance of pretrained mo",
    "link": "https://arxiv.org/abs/2402.04004",
    "context": "Title: Understanding the Effect of Noise in LLM Training Data with Algorithmic Chains of Thought\nAbstract: During both pretraining and fine-tuning, Large Language Models (\\textbf{LLMs}) are trained on trillions of tokens of text of widely varying quality. Both phases of training typically involve heuristically filtering out ``low-quality'' or \\textit{noisy} training samples, yet little is known quantitatively about how the type or intensity of noise affects downstream performance. In this work, we study how noise in chain of thought (\\textbf{CoT}) impacts task performance in the highly-controlled setting of algorithmically solvable tasks. First, we develop the Traced Integer (\\textbf{TInt}) framework to generate highly customizable noised execution traces for any arithmetic function on lists of integers. We then define two types of noise: \\textit{static} noise, a local form of noise which is applied after the CoT trace is computed, and \\textit{dynamic} noise, a global form of noise which propagates errors in the trace as it is computed. We then evaluate the test performance of pretrained mo",
    "path": "papers/24/02/2402.04004.json",
    "total_tokens": 995,
    "translated_title": "了解算法式思维链中噪声对LLM训练数据的影响",
    "translated_abstract": "在预训练和微调过程中，大型语言模型（LLMs）通常会使用数万亿个标记的文本进行训练，这些文本质量各异。在训练的两个阶段中，通常会根据启发式方法过滤掉“低质量”或“有噪声”的训练样本，然而很少有人量化地了解噪声的类型或强度如何影响下游性能。在本研究中，我们研究了链式思维（CoT）中的噪声如何影响在算法可解任务的高度控制环境下的任务性能。首先，我们开发了追踪整数（TInt）框架，用于为任意整数列表上的算术函数生成高度可定制的噪声执行跟踪。然后，我们定义了两种类型的噪声：局部形式的静态噪声，在计算CoT跟踪后应用；以及全局形式的动态噪声，在计算中传播跟踪中的错误。然后，我们评估了预训练模型在测试性能上的表现。",
    "tldr": "本论文研究了链式思维中的噪声对LLM训练数据的影响，并开发了追踪整数框架来生成可定制的噪声执行跟踪。通过评估预训练模型在算法可解任务中的表现，揭示了噪声的类型和强度对任务性能的影响。",
    "en_tdlr": "This paper studies the impact of noise in chain of thought on LLM training data and develops a Traced Integer framework to generate customizable noisy execution traces. By evaluating the performance of pretrained models on algorithmically solvable tasks, the paper reveals the influence of noise types and intensity on task performance."
}