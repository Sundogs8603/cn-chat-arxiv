{
    "title": "The Counterfeit Conundrum: Can Code Language Models Grasp the Nuances of Their Incorrect Generations?",
    "abstract": "arXiv:2402.19475v1 Announce Type: cross  Abstract: While language models are increasingly more proficient at code generation, they still frequently generate incorrect programs. Many of these programs are obviously wrong, but others are more subtle and pass weaker correctness checks such as being able to compile. In this work, we focus on these counterfeit samples: programs sampled from a language model that 1) have a high enough log-probability to be generated at a moderate temperature and 2) pass weak correctness checks. Overall, we discover that most models have a very shallow understanding of counterfeits through three clear failure modes. First, models mistakenly classify them as correct. Second, models are worse at reasoning about the execution behaviour of counterfeits and often predict their execution results as if they were correct. Third, when asking models to fix counterfeits, the likelihood of a model successfully repairing a counterfeit is often even lower than that of samp",
    "link": "https://arxiv.org/abs/2402.19475",
    "context": "Title: The Counterfeit Conundrum: Can Code Language Models Grasp the Nuances of Their Incorrect Generations?\nAbstract: arXiv:2402.19475v1 Announce Type: cross  Abstract: While language models are increasingly more proficient at code generation, they still frequently generate incorrect programs. Many of these programs are obviously wrong, but others are more subtle and pass weaker correctness checks such as being able to compile. In this work, we focus on these counterfeit samples: programs sampled from a language model that 1) have a high enough log-probability to be generated at a moderate temperature and 2) pass weak correctness checks. Overall, we discover that most models have a very shallow understanding of counterfeits through three clear failure modes. First, models mistakenly classify them as correct. Second, models are worse at reasoning about the execution behaviour of counterfeits and often predict their execution results as if they were correct. Third, when asking models to fix counterfeits, the likelihood of a model successfully repairing a counterfeit is often even lower than that of samp",
    "path": "papers/24/02/2402.19475.json",
    "total_tokens": 931,
    "translated_title": "冒牌难题：代码语言模型能理解其不正确生成的微妙之处吗？",
    "translated_abstract": "尽管语言模型在代码生成方面变得越来越熟练，它们仍然经常生成不正确的程序。许多这些程序显然是错误的，但其他一些则更为微妙，可以通过更弱的正确性检查，例如能够编译。在这项工作中，我们关注这些伪造的样本：从语言模型中抽样得到的程序，这些程序1）在适度温度下生成的对数概率足够高，2）通过弱正确性检查。总体而言，我们发现大多数模型对伪造品的理解非常肤浅，存在三种明显的失败模式。首先，模型错误地将它们分类为正确。其次，模型在推理伪造品的执行行为方面更差，通常将它们的执行结果预测为如果它们是正确的一样。第三，在要求模型修复伪造品时，模型成功修复伪造品的可能性往往甚至低于抽样生成伪造品的可能性。",
    "tldr": "大多数代码语言模型对生成的冒牌样本的理解较为肤浅，存在三种明显的失败模式：误将冒牌样本分类为正确、在推理冒牌样本的执行行为时表现更差、修复冒牌样本的成功率往往低于生成它们的成功率。",
    "en_tdlr": "Most code language models have a shallow understanding of generated counterfeit samples, showing three clear failure modes: incorrectly classifying counterfeits as correct, performing poorly in reasoning about the execution behavior of counterfeits, and having a lower success rate in repairing counterfeits compared to generating them."
}