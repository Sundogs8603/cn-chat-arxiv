{
    "title": "Meta-Task Prompting Elicits Embedding from Large Language Models",
    "abstract": "arXiv:2402.18458v1 Announce Type: new  Abstract: In this work, we introduce a new unsupervised embedding method, Meta-Task Prompting with Explicit One-Word Limitation (MetaEOL), for generating high-quality sentence embeddings from Large Language Models (LLMs) without the need for model fine-tuning or task-specific engineering. Leveraging meta-task prompting, MetaEOL guides LLMs to produce embeddings through a series of carefully designed prompts that address multiple representational aspects. Our comprehensive experiments demonstrate that embeddings averaged from various meta-tasks yield competitive performance on Semantic Textual Similarity (STS) benchmarks and excel in downstream tasks, surpassing contrastive-trained models. Our findings suggest a new scaling law for embedding generation, offering a versatile, resource-efficient approach for embedding extraction across diverse sentence-centric scenarios.",
    "link": "https://arxiv.org/abs/2402.18458",
    "context": "Title: Meta-Task Prompting Elicits Embedding from Large Language Models\nAbstract: arXiv:2402.18458v1 Announce Type: new  Abstract: In this work, we introduce a new unsupervised embedding method, Meta-Task Prompting with Explicit One-Word Limitation (MetaEOL), for generating high-quality sentence embeddings from Large Language Models (LLMs) without the need for model fine-tuning or task-specific engineering. Leveraging meta-task prompting, MetaEOL guides LLMs to produce embeddings through a series of carefully designed prompts that address multiple representational aspects. Our comprehensive experiments demonstrate that embeddings averaged from various meta-tasks yield competitive performance on Semantic Textual Similarity (STS) benchmarks and excel in downstream tasks, surpassing contrastive-trained models. Our findings suggest a new scaling law for embedding generation, offering a versatile, resource-efficient approach for embedding extraction across diverse sentence-centric scenarios.",
    "path": "papers/24/02/2402.18458.json",
    "total_tokens": 847,
    "translated_title": "使用元任务提示从大型语言模型中引出嵌入",
    "translated_abstract": "在这项工作中，我们引入了一种新的无监督嵌入方法，即带显式单词限制的元任务提示（MetaEOL），用于从大型语言模型（LLMs）中生成高质量的句子嵌入，无需对模型进行微调或特定任务的工程。通过利用元任务提示，MetaEOL引导LLMs通过一系列精心设计的提示生成嵌入，这些提示涵盖了多个表示方面。我们全面的实验表明，从各种元任务平均得到的嵌入在语义文本相似性（STS）基准测试中表现出竞争力，并在下游任务中表现卓越，超越了对比训练模型。我们的发现提出了一种嵌入生成的新的扩展定律，为跨多种以句子为中心的场景中的嵌入提取提供了一种多才多艺、资源高效的方法。",
    "tldr": "提出了一种新的无监督嵌入方法MetaEOL，通过元任务提示引导大型语言模型生成高质量句子嵌入，无需模型微调或特定任务工程，实验表明其在语义文本相似性测试和下游任务中表现出色",
    "en_tdlr": "Introduced a new unsupervised embedding method MetaEOL that guides Large Language Models to generate high-quality sentence embeddings through meta-task prompting without the need for model fine-tuning or task-specific engineering, demonstrating competitive performance on Semantic Textual Similarity tests and downstream tasks."
}