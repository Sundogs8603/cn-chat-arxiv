{
    "title": "Measuring Sharpness in Grokking",
    "abstract": "arXiv:2402.08946v1 Announce Type: new Abstract: Neural networks sometimes exhibit grokking, a phenomenon where perfect or near-perfect performance is achieved on a validation set well after the same performance has been obtained on the corresponding training set. In this workshop paper, we introduce a robust technique for measuring grokking, based on fitting an appropriate functional form. We then use this to investigate the sharpness of transitions in training and validation accuracy under two settings. The first setting is the theoretical framework developed by Levi et al. (2023) where closed form expressions are readily accessible. The second setting is a two-layer MLP trained to predict the parity of bits, with grokking induced by the concealment strategy of Miller et al. (2023). We find that trends between relative grokking gap and grokking sharpness are similar in both settings when using absolute and relative measures of sharpness. Reflecting on this, we make progress toward exp",
    "link": "https://arxiv.org/abs/2402.08946",
    "context": "Title: Measuring Sharpness in Grokking\nAbstract: arXiv:2402.08946v1 Announce Type: new Abstract: Neural networks sometimes exhibit grokking, a phenomenon where perfect or near-perfect performance is achieved on a validation set well after the same performance has been obtained on the corresponding training set. In this workshop paper, we introduce a robust technique for measuring grokking, based on fitting an appropriate functional form. We then use this to investigate the sharpness of transitions in training and validation accuracy under two settings. The first setting is the theoretical framework developed by Levi et al. (2023) where closed form expressions are readily accessible. The second setting is a two-layer MLP trained to predict the parity of bits, with grokking induced by the concealment strategy of Miller et al. (2023). We find that trends between relative grokking gap and grokking sharpness are similar in both settings when using absolute and relative measures of sharpness. Reflecting on this, we make progress toward exp",
    "path": "papers/24/02/2402.08946.json",
    "total_tokens": 896,
    "translated_title": "在理解中测量锐度",
    "translated_abstract": "神经网络有时会出现理解现象，即在验证集上达到完美或接近完美的性能，而在相应的训练集上已经获得了相同的性能。在本研讨会论文中，我们引入了一种基于拟合适当的函数形式的强大技术来测量理解，并利用此技术在两个设置下研究训练和验证精度的变化的锐度。第一个设置是由Levi等人（2023）开发的理论框架，其中可以方便地得到闭合形式的表达式。第二个设置是训练一个两层MLP来预测位的奇偶性，通过Miller等人（2023）的隐藏策略引发理解。我们发现，在使用绝对和相对锐度测量时，相对理解差距和理解锐度之间的趋势在这两个设置中是相似的。在此基础上，我们对此进行了思考，为解决该问题取得了进展。",
    "tldr": "本研究介绍了一种基于适当函数形式的技术来测量理解现象，并研究了在不同设置下训练和验证精度变化的锐度。在两个设置中，相对理解差距和理解锐度之间的趋势相似。",
    "en_tdlr": "This study introduces a technique based on an appropriate functional form to measure the phenomenon of grokking and investigates the sharpness of transitions in training and validation accuracy under different settings. The trends between relative grokking gap and grokking sharpness are similar in both settings."
}