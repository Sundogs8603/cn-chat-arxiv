{
    "title": "Assessing LLMs' Mathematical Reasoning in Financial Document Question Answering",
    "abstract": "arXiv:2402.11194v1 Announce Type: new  Abstract: Large Language Models (LLMs), excel in natural language understanding, but their capability for complex mathematical reasoning with an amalgamation of structured tables and unstructured text is uncertain. This study explores LLMs' mathematical reasoning on four financial tabular question-answering datasets: TATQA, FinQA, ConvFinQA, and Multihiertt. Through extensive experiments with various models and prompting techniques, we assess how LLMs adapt to complex tables and mathematical tasks. We focus on sensitivity to table complexity and performance variations with an increasing number of arithmetic reasoning steps. The results provide insights into LLMs' capabilities and limitations in handling complex mathematical scenarios for semi-structured tables. Ultimately, we introduce a novel prompting technique tailored to semi-structured documents, matching or outperforming other baselines in performance while providing a nuanced understanding ",
    "link": "https://arxiv.org/abs/2402.11194",
    "context": "Title: Assessing LLMs' Mathematical Reasoning in Financial Document Question Answering\nAbstract: arXiv:2402.11194v1 Announce Type: new  Abstract: Large Language Models (LLMs), excel in natural language understanding, but their capability for complex mathematical reasoning with an amalgamation of structured tables and unstructured text is uncertain. This study explores LLMs' mathematical reasoning on four financial tabular question-answering datasets: TATQA, FinQA, ConvFinQA, and Multihiertt. Through extensive experiments with various models and prompting techniques, we assess how LLMs adapt to complex tables and mathematical tasks. We focus on sensitivity to table complexity and performance variations with an increasing number of arithmetic reasoning steps. The results provide insights into LLMs' capabilities and limitations in handling complex mathematical scenarios for semi-structured tables. Ultimately, we introduce a novel prompting technique tailored to semi-structured documents, matching or outperforming other baselines in performance while providing a nuanced understanding ",
    "path": "papers/24/02/2402.11194.json",
    "total_tokens": 837,
    "translated_title": "在金融文档问答中评估LLMs的数学推理能力",
    "translated_abstract": "大型语言模型（LLMs）在自然语言理解方面表现出色，但它们在具有结构化表格和非结构化文本混合的复杂数学推理方面的能力尚不确定。本研究探讨了LLMs在四个金融表格问答数据集上的数学推理能力：TATQA、FinQA、ConvFinQA和Multihiertt。通过对各种模型和提示技术进行广泛实验，我们评估了LLMs如何适应复杂表格和数学任务。我们关注对表格复杂性的敏感性以及在增加算术推理步骤数量时性能变化。结果揭示了LLMs处理半结构化表格中复杂数学场景的能力和局限性。最终，我们引入了一种针对半结构化文档的新型提示技术，在性能方面与其他基线相匹配或胜过，并提供了对LLMs能力的微妙理解。",
    "tldr": "通过实验评估了LLMs在金融表格问答中的数学推理能力，发现引入了一种新型提示技术，能够在性能上胜过其他基线模型"
}