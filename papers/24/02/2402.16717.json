{
    "title": "CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models",
    "abstract": "arXiv:2402.16717v1 Announce Type: new  Abstract: Adversarial misuse, particularly through `jailbreaking' that circumvents a model's safety and ethical protocols, poses a significant challenge for Large Language Models (LLMs). This paper delves into the mechanisms behind such successful attacks, introducing a hypothesis for the safety mechanism of aligned LLMs: intent security recognition followed by response generation. Grounded in this hypothesis, we propose CodeChameleon, a novel jailbreak framework based on personalized encryption tactics. To elude the intent security recognition phase, we reformulate tasks into a code completion format, enabling users to encrypt queries using personalized encryption functions. To guarantee response generation functionality, we embed a decryption function within the instructions, which allows the LLM to decrypt and execute the encrypted queries successfully. We conduct extensive experiments on 7 LLMs, achieving state-of-the-art average Attack Succes",
    "link": "https://arxiv.org/abs/2402.16717",
    "context": "Title: CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models\nAbstract: arXiv:2402.16717v1 Announce Type: new  Abstract: Adversarial misuse, particularly through `jailbreaking' that circumvents a model's safety and ethical protocols, poses a significant challenge for Large Language Models (LLMs). This paper delves into the mechanisms behind such successful attacks, introducing a hypothesis for the safety mechanism of aligned LLMs: intent security recognition followed by response generation. Grounded in this hypothesis, we propose CodeChameleon, a novel jailbreak framework based on personalized encryption tactics. To elude the intent security recognition phase, we reformulate tasks into a code completion format, enabling users to encrypt queries using personalized encryption functions. To guarantee response generation functionality, we embed a decryption function within the instructions, which allows the LLM to decrypt and execute the encrypted queries successfully. We conduct extensive experiments on 7 LLMs, achieving state-of-the-art average Attack Succes",
    "path": "papers/24/02/2402.16717.json",
    "total_tokens": 741,
    "translated_title": "CodeChameleon: 针对越狱大型语言模型的个性化加密框架",
    "translated_abstract": "Adversarial misuse, particularly through `jailbreaking' that circumvents a model's safety and ethical protocols, poses a significant challenge for Large Language Models (LLMs). This paper delves into the mechanisms behind such successful attacks, introducing a hypothesis for the safety mechanism of aligned LLMs: intent security recognition followed by response generation. Grounded in this hypothesis, we propose CodeChameleon, a novel jailbreak framework based on personalized encryption tactics. To elude the intent security recognition phase, we reformulate tasks into a code completion format, enabling users to encrypt queries using personalized encryption functions. To guarantee response generation functionality, we embed a decryption function within the instructions, which allows the LLM to decrypt and execute the encrypted queries successfully. We conduct extensive experiments on 7 LLMs, achieving state-of-the-art average Attack Success",
    "tldr": "该论文提出了一种基于个性化加密策略的新型越狱框架CodeChameleon，通过重塑任务格式和嵌入解密功能，成功应对大型语言模型面临的安全挑战。",
    "en_tdlr": "This paper introduces a novel jailbreak framework CodeChameleon based on personalized encryption tactics, which successfully addresses the security challenges faced by Large Language Models through task reformulation and embedding decryption functionality."
}