{
    "title": "Improving Open-Ended Text Generation via Adaptive Decoding",
    "abstract": "arXiv:2402.18223v1 Announce Type: new  Abstract: Current language models decode text token by token according to probabilistic distribution, and determining the appropriate candidates for the next token is crucial to ensure generation quality. This study introduces adaptive decoding, a mechanism that empowers the language models to ascertain a sensible candidate set during the generation process dynamically. Specifically, we introduce an entropy-based metric called confidence and conceptualize determining the optimal candidate set as a confidence-increasing process. The rationality of including a token in the candidate set is assessed by leveraging the increment of confidence, enabling the model to determine the most suitable candidate set adaptively. The experimental results reveal that our method achieves higher MAUVE and diversity in story generation tasks and maintains certain coherence, underscoring its superiority over existing algorithms. The code is available at https://github.",
    "link": "https://arxiv.org/abs/2402.18223",
    "context": "Title: Improving Open-Ended Text Generation via Adaptive Decoding\nAbstract: arXiv:2402.18223v1 Announce Type: new  Abstract: Current language models decode text token by token according to probabilistic distribution, and determining the appropriate candidates for the next token is crucial to ensure generation quality. This study introduces adaptive decoding, a mechanism that empowers the language models to ascertain a sensible candidate set during the generation process dynamically. Specifically, we introduce an entropy-based metric called confidence and conceptualize determining the optimal candidate set as a confidence-increasing process. The rationality of including a token in the candidate set is assessed by leveraging the increment of confidence, enabling the model to determine the most suitable candidate set adaptively. The experimental results reveal that our method achieves higher MAUVE and diversity in story generation tasks and maintains certain coherence, underscoring its superiority over existing algorithms. The code is available at https://github.",
    "path": "papers/24/02/2402.18223.json",
    "total_tokens": 792,
    "translated_title": "通过自适应解码改进开放式文本生成",
    "translated_abstract": "当前语言模型根据概率分布逐标记解码文本，确定下一个标记的恰当候选者对于保证生成质量至关重要。本研究引入了自适应解码，一种机制使语言模型能够在生成过程中动态确定一个合理的候选集。具体来说，我们引入了一种基于熵的度量标准，称之为置信度，并将确定最佳候选集视为一个增加置信度的过程。通过利用置信度增加来评估将标记包含在候选集中的合理性，使模型能够自适应地确定最合适的候选集。实验结果表明，我们的方法在故事生成任务中实现了更高的MAUVE和多样性，并保持了一定的连贯性，突显了其优于现有算法的优越性。",
    "tldr": "引入自适应解码机制，通过置信度动态确定生成过程中的候选集，在故事生成任务中实现了更高的MAUVE和多样性，保持一定的连贯性，优于现有算法。",
    "en_tdlr": "Introducing adaptive decoding mechanism that dynamically determines candidate set during generation process based on confidence, achieving higher MAUVE and diversity in story generation tasks while maintaining certain coherence, superior to existing algorithms."
}