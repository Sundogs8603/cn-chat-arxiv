{
    "title": "RLVF: Learning from Verbal Feedback without Overgeneralization",
    "abstract": "arXiv:2402.10893v1 Announce Type: cross  Abstract: The diversity of contexts in which large language models (LLMs) are deployed requires the ability to modify or customize default model behaviors to incorporate nuanced requirements and preferences. A convenient interface to specify such model adjustments is high-level verbal feedback, such as \"Don't use emojis when drafting emails to my boss.\" However, while writing high-level feedback is far simpler than collecting annotations for reinforcement learning from human feedback (RLHF), we find that simply prompting a model with such feedback leads to overgeneralization of the feedback to contexts where it is not relevant. We study the problem of incorporating verbal feedback without such overgeneralization, inspiring a new method Contextualized Critiques with Constrained Preference Optimization (C3PO). C3PO uses a piece of high-level feedback to generate a small synthetic preference dataset specifying how the feedback should (and should no",
    "link": "https://arxiv.org/abs/2402.10893",
    "context": "Title: RLVF: Learning from Verbal Feedback without Overgeneralization\nAbstract: arXiv:2402.10893v1 Announce Type: cross  Abstract: The diversity of contexts in which large language models (LLMs) are deployed requires the ability to modify or customize default model behaviors to incorporate nuanced requirements and preferences. A convenient interface to specify such model adjustments is high-level verbal feedback, such as \"Don't use emojis when drafting emails to my boss.\" However, while writing high-level feedback is far simpler than collecting annotations for reinforcement learning from human feedback (RLHF), we find that simply prompting a model with such feedback leads to overgeneralization of the feedback to contexts where it is not relevant. We study the problem of incorporating verbal feedback without such overgeneralization, inspiring a new method Contextualized Critiques with Constrained Preference Optimization (C3PO). C3PO uses a piece of high-level feedback to generate a small synthetic preference dataset specifying how the feedback should (and should no",
    "path": "papers/24/02/2402.10893.json",
    "total_tokens": 825,
    "translated_title": "RLVF: 学习如何在没有泛化的情况下从口头反馈中学习",
    "translated_abstract": "大型语言模型（LLMs）部署的不同情境的多样性要求能够修改或定制默认模型行为，以满足细微的要求和偏好。规定这种模型调整的方便界面是高层次口头反馈，比如\"在给老板起草邮件时不要使用表情符号\"。然而，尽管撰写高层反馈比从人类反馈中收集强化学习注释（RLHF）简单得多，但我们发现只是用这种反馈提示模型会导致反馈在不相关的情境中产生泛化。我们研究了如何在没有这种泛化的情况下整合口头反馈的问题，并启发了一个新方法：带约束偏好优化的情境化评论（C3PO）。C3PO使用一段高层次反馈生成一个小的合成偏好数据集，指定了反馈应该如何（以及不应该如何）进行。",
    "tldr": "研究了如何在大型语言模型中利用口头反馈进行定制化调整而不发生过度泛化，并提出了一种新的方法C3PO。",
    "en_tdlr": "Explored the method of customizing large language models with verbal feedback without overgeneralization, proposing a new method C3PO."
}