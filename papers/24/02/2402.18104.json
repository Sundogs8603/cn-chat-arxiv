{
    "title": "Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction",
    "abstract": "arXiv:2402.18104v1 Announce Type: cross  Abstract: In recent years, large language models (LLMs) have demonstrated notable success across various tasks, but the trustworthiness of LLMs is still an open problem. One specific threat is the potential to generate toxic or harmful responses. Attackers can craft adversarial prompts that induce harmful responses from LLMs. In this work, we pioneer a theoretical foundation in LLMs security by identifying bias vulnerabilities within the safety fine-tuning and design a black-box jailbreak method named DRA (Disguise and Reconstruction Attack), which conceals harmful instructions through disguise and prompts the model to reconstruct the original harmful instruction within its completion. We evaluate DRA across various open-source and close-source models, showcasing state-of-the-art jailbreak success rates and attack efficiency. Notably, DRA boasts a 90\\% attack success rate on LLM chatbots GPT-4.",
    "link": "https://arxiv.org/abs/2402.18104",
    "context": "Title: Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction\nAbstract: arXiv:2402.18104v1 Announce Type: cross  Abstract: In recent years, large language models (LLMs) have demonstrated notable success across various tasks, but the trustworthiness of LLMs is still an open problem. One specific threat is the potential to generate toxic or harmful responses. Attackers can craft adversarial prompts that induce harmful responses from LLMs. In this work, we pioneer a theoretical foundation in LLMs security by identifying bias vulnerabilities within the safety fine-tuning and design a black-box jailbreak method named DRA (Disguise and Reconstruction Attack), which conceals harmful instructions through disguise and prompts the model to reconstruct the original harmful instruction within its completion. We evaluate DRA across various open-source and close-source models, showcasing state-of-the-art jailbreak success rates and attack efficiency. Notably, DRA boasts a 90\\% attack success rate on LLM chatbots GPT-4.",
    "path": "papers/24/02/2402.18104.json",
    "total_tokens": 867,
    "translated_title": "通过伪装和重构在少量查询中越狱大型语言模型",
    "translated_abstract": "近年来，大型语言模型（LLMs）在各种任务中取得显著成功，但LLMs的可信度仍然是一个未解之谜。其中一个特定的威胁是可能生成有毒或有害的回应。攻击者可以制作有针对性的提示，诱使LLMs生成有害的回应。在这项工作中，我们通过识别在安全微调中的偏见漏洞，开创了LLMs安全领域的理论基础，并设计了一种名为DRA（伪装和重构攻击）的黑盒越狱方法，通过伪装来隐藏有害指令，并提示模型在完成过程中重构原始有害指令。我们评估了DRA在各种开源和闭源模型上的表现，展示了最先进的越狱成功率和攻击效率。值得注意的是，DRA在LLM聊天机器人GPT-4上拥有90\\%的攻击成功率。",
    "tldr": "通过伪装和重构攻击方法，我们提出了一种在大型语言模型中越狱的方法，通过成功隐藏有害指令并引导模型重构原指令，取得了90%的攻击成功率。",
    "en_tdlr": "We propose a jailbreak method in large language models using disguise and reconstruction attacks, achieving a 90% attack success rate by concealing harmful instructions and prompting the model to reconstruct the original instructions."
}