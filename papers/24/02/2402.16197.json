{
    "title": "Language Models for Code Completion: A Practical Evaluation",
    "abstract": "arXiv:2402.16197v1 Announce Type: cross  Abstract: Transformer-based language models for automatic code completion have shown great promise so far, yet the evaluation of these models rarely uses real data. This study provides both quantitative and qualitative assessments of three public code language models when completing real-world code. We first developed an open-source IDE extension, Code4Me, for the online evaluation of the models. We collected real auto-completion usage data for over a year from more than 1200 users, resulting in over 600K valid completions. These models were then evaluated using six standard metrics across twelve programming languages. Next, we conducted a qualitative study of 1690 real-world completion requests to identify the reasons behind the poor model performance. A comparative analysis of the models' performance in online and offline settings was also performed, using benchmark synthetic datasets and two masking strategies. Our findings suggest that while",
    "link": "https://arxiv.org/abs/2402.16197",
    "context": "Title: Language Models for Code Completion: A Practical Evaluation\nAbstract: arXiv:2402.16197v1 Announce Type: cross  Abstract: Transformer-based language models for automatic code completion have shown great promise so far, yet the evaluation of these models rarely uses real data. This study provides both quantitative and qualitative assessments of three public code language models when completing real-world code. We first developed an open-source IDE extension, Code4Me, for the online evaluation of the models. We collected real auto-completion usage data for over a year from more than 1200 users, resulting in over 600K valid completions. These models were then evaluated using six standard metrics across twelve programming languages. Next, we conducted a qualitative study of 1690 real-world completion requests to identify the reasons behind the poor model performance. A comparative analysis of the models' performance in online and offline settings was also performed, using benchmark synthetic datasets and two masking strategies. Our findings suggest that while",
    "path": "papers/24/02/2402.16197.json",
    "total_tokens": 873,
    "translated_title": "用于代码自动补全的语言模型：实践评估",
    "translated_abstract": "基于Transformer的语言模型在自动代码补全方面表现出很大的潜力，然而这些模型的评估很少使用真实数据。本研究对完成真实世界代码时的三种公共代码语言模型进行了定量和定性评估。我们首先开发了一个开源的IDE扩展工具，Code4Me，用于在线评估这些模型。我们从1200多名用户那里收集了一年多的真实自动补全使用数据，产生了超过60万个有效的完成结果。然后，我们使用六个标准指标对这些模型进行评估，涵盖了十二种编程语言。接下来，我们进行了一个定性研究，通过对1690个真实世界的完成请求进行分析，以确定模型表现不佳背后的原因。还对模型在在线和离线环境中的表现进行了比较分析，使用两种遮蔽策略和基准合成数据集。我们的研究结果表明，虽然",
    "tldr": "这项研究对完成真实世界代码时的三种公共代码语言模型进行了定量和定性评估，在线和离线环境中进行了比较分析，为自动代码补全的语言模型评估提供了有益的发现。"
}