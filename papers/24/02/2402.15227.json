{
    "title": "Fixed Random Classifier Rearrangement for Continual Learning",
    "abstract": "arXiv:2402.15227v1 Announce Type: cross  Abstract: With the explosive growth of data, continual learning capability is increasingly important for neural networks. Due to catastrophic forgetting, neural networks inevitably forget the knowledge of old tasks after learning new ones. In visual classification scenario, a common practice of alleviating the forgetting is to constrain the backbone. However, the impact of classifiers is underestimated. In this paper, we analyze the variation of model predictions in sequential binary classification tasks and find that the norm of the equivalent one-class classifiers significantly affects the forgetting level. Based on this conclusion, we propose a two-stage continual learning algorithm named Fixed Random Classifier Rearrangement (FRCR). In first stage, FRCR replaces the learnable classifiers with fixed random classifiers, constraining the norm of the equivalent one-class classifiers without affecting the performance of the network. In second sta",
    "link": "https://arxiv.org/abs/2402.15227",
    "context": "Title: Fixed Random Classifier Rearrangement for Continual Learning\nAbstract: arXiv:2402.15227v1 Announce Type: cross  Abstract: With the explosive growth of data, continual learning capability is increasingly important for neural networks. Due to catastrophic forgetting, neural networks inevitably forget the knowledge of old tasks after learning new ones. In visual classification scenario, a common practice of alleviating the forgetting is to constrain the backbone. However, the impact of classifiers is underestimated. In this paper, we analyze the variation of model predictions in sequential binary classification tasks and find that the norm of the equivalent one-class classifiers significantly affects the forgetting level. Based on this conclusion, we propose a two-stage continual learning algorithm named Fixed Random Classifier Rearrangement (FRCR). In first stage, FRCR replaces the learnable classifiers with fixed random classifiers, constraining the norm of the equivalent one-class classifiers without affecting the performance of the network. In second sta",
    "path": "papers/24/02/2402.15227.json",
    "total_tokens": 851,
    "translated_title": "固定随机分类器重排在持续学习中的应用",
    "translated_abstract": "随着数据的爆炸性增长，神经网络的持续学习能力变得越来越重要。由于灾难性遗忘，神经网络在学习新任务后不可避免地会忘记旧任务的知识。在视觉分类场景中，缓解遗忘的常见做法是限制主干网络，然而分类器的影响被低估了。本文分析了模型在顺序二元分类任务中的预测变化，并发现等价单分类器的范数显著影响遗忘水平。基于这一结论，我们提出了一个名为固定随机分类器重排（Fixed Random Classifier Rearrangement，FRCR）的两阶段持续学习算法。在第一阶段，FRCR用固定的随机分类器替换可学习的分类器，约束了等价的单分类器的范数，而不影响网络的性能。",
    "tldr": "提出了一种名为固定随机分类器重排（FRCR）的两阶段持续学习算法，通过替换可学习的分类器为固定的随机分类器，在不影响网络性能的情况下，约束了等价的单分类器的范数。",
    "en_tdlr": "Introduced a two-stage continual learning algorithm named Fixed Random Classifier Rearrangement (FRCR), which replaces learnable classifiers with fixed random classifiers to constrain the norm of equivalent one-class classifiers without affecting network performance."
}