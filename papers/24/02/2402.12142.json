{
    "title": "Federated Bayesian Network Ensembles",
    "abstract": "arXiv:2402.12142v1 Announce Type: new  Abstract: Federated learning allows us to run machine learning algorithms on decentralized data when data sharing is not permitted due to privacy concerns. Ensemble-based learning works by training multiple (weak) classifiers whose output is aggregated. Federated ensembles are ensembles applied to a federated setting, where each classifier in the ensemble is trained on one data location.   In this article, we explore the use of federated ensembles of Bayesian networks (FBNE) in a range of experiments and compare their performance with locally trained models and models trained with VertiBayes, a federated learning algorithm to train Bayesian networks from decentralized data. Our results show that FBNE outperforms local models and provides a significant increase in training speed compared with VertiBayes while maintaining a similar performance in most settings, among other advantages. We show that FBNE is a potentially useful tool within the federat",
    "link": "https://arxiv.org/abs/2402.12142",
    "context": "Title: Federated Bayesian Network Ensembles\nAbstract: arXiv:2402.12142v1 Announce Type: new  Abstract: Federated learning allows us to run machine learning algorithms on decentralized data when data sharing is not permitted due to privacy concerns. Ensemble-based learning works by training multiple (weak) classifiers whose output is aggregated. Federated ensembles are ensembles applied to a federated setting, where each classifier in the ensemble is trained on one data location.   In this article, we explore the use of federated ensembles of Bayesian networks (FBNE) in a range of experiments and compare their performance with locally trained models and models trained with VertiBayes, a federated learning algorithm to train Bayesian networks from decentralized data. Our results show that FBNE outperforms local models and provides a significant increase in training speed compared with VertiBayes while maintaining a similar performance in most settings, among other advantages. We show that FBNE is a potentially useful tool within the federat",
    "path": "papers/24/02/2402.12142.json",
    "total_tokens": 920,
    "translated_title": "联邦贝叶斯网络集成",
    "translated_abstract": "arXiv:2402.12142v1 公布类型: 新的 摘要: 联邦学习允许我们在去中心化数据上运行机器学习算法，当由于隐私问题而不允许数据共享时。基于集成的学习通过训练多个(弱)分类器，然后对其输出进行聚合。联邦集成是应用于联邦设置的集成，其中集成中的每个分类器都在一个数据位置上进行训练。 在本文中，我们探讨了在一系列实验中使用联邦贝叶斯网络集成(FBNE)的情况，并将其性能与本地训练模型和使用VertiBayes进行训练的模型进行比较，后者是一种用于从去中心化数据训练贝叶斯网络的联邦学习算法。我们的研究结果表明，FBNE的表现优于本地模型，在大多数情况下在维持类似性能的情况下比VertiBayes提供了显著的训练速度增加，另外还具有其他优势。我们展示了FBNE是联邦环境中潜在有用的工具。",
    "tldr": "联邦贝叶斯网络集成(FBNE)在联邦设置下表现优异，相较于本地模型和VertiBayes训练的模型，在保持类似性能的情况下提供了显著的训练速度提升。",
    "en_tdlr": "Federated Bayesian Network Ensembles (FBNE) perform significantly better in a federated setting, providing a significant increase in training speed compared to local models and models trained with VertiBayes while maintaining similar performance."
}