{
    "title": "Offline Actor-Critic Reinforcement Learning Scales to Large Models",
    "abstract": "We show that offline actor-critic reinforcement learning can scale to large models - such as transformers - and follows similar scaling laws as supervised learning. We find that offline actor-critic algorithms can outperform strong, supervised, behavioral cloning baselines for multi-task training on a large dataset containing both sub-optimal and expert behavior on 132 continuous control tasks. We introduce a Perceiver-based actor-critic model and elucidate the key model features needed to make offline RL work with self- and cross-attention modules. Overall, we find that: i) simple offline actor critic algorithms are a natural choice for gradually moving away from the currently predominant paradigm of behavioral cloning, and ii) via offline RL it is possible to learn multi-task policies that master many domains simultaneously, including real robotics tasks, from sub-optimal demonstrations or self-generated data.",
    "link": "https://arxiv.org/abs/2402.05546",
    "context": "Title: Offline Actor-Critic Reinforcement Learning Scales to Large Models\nAbstract: We show that offline actor-critic reinforcement learning can scale to large models - such as transformers - and follows similar scaling laws as supervised learning. We find that offline actor-critic algorithms can outperform strong, supervised, behavioral cloning baselines for multi-task training on a large dataset containing both sub-optimal and expert behavior on 132 continuous control tasks. We introduce a Perceiver-based actor-critic model and elucidate the key model features needed to make offline RL work with self- and cross-attention modules. Overall, we find that: i) simple offline actor critic algorithms are a natural choice for gradually moving away from the currently predominant paradigm of behavioral cloning, and ii) via offline RL it is possible to learn multi-task policies that master many domains simultaneously, including real robotics tasks, from sub-optimal demonstrations or self-generated data.",
    "path": "papers/24/02/2402.05546.json",
    "total_tokens": 1042,
    "translated_title": "离线演员-评论者强化学习扩展到大型模型",
    "translated_abstract": "我们证明了离线演员-评论者强化学习可以扩展到大型模型，如transformer，并且遵循与监督学习类似的扩展规律。我们发现，离线演员-评论者算法在包含132个连续控制任务的大型数据集上的多任务训练中，可以胜过强大的监督式行为克隆基线，该数据集包含了次优和专家行为。我们引入了一种基于Perceiver的演员-评论者模型，并阐明了使离线强化学习与自注意机制和跨注意力模块配合工作所需的关键模型特征。总的来说，我们发现：i）简单的离线演员评论者算法是逐渐远离当前主流行为克隆范式的自然选择，ii）通过离线强化学习，可以从次优示范或自动生成的数据中学习掌握许多领域的多任务策略，包括真实机器人任务。",
    "tldr": "本文证明了离线演员-评论者强化学习方法可以扩展到大型模型，并且比基线方法在多任务训练中表现更好。通过引入Perceiver-based演员-评论者模型，我们揭示了离线强化学习与自注意机制和跨注意力模块配合的关键模型特征。这项研究的发现表明：离线演员-评论者算法是逐渐摆脱行为克隆范式的一种自然选择，并且通过离线强化学习可以从次优示范或自动生成的数据中学习掌握多个领域的多任务策略。",
    "en_tdlr": "This paper demonstrates that offline actor-critic reinforcement learning can scale to large models and outperform baseline methods in multi-task training. The introduction of the Perceiver-based actor-critic model reveals key features necessary for offline RL with self-attention and cross-attention modules. The findings suggest that simple offline actor-critic algorithms are a natural choice for moving away from the prevalent paradigm of behavioral cloning, and that offline RL enables learning multi-task strategies across various domains from sub-optimal demonstrations or self-generated data."
}