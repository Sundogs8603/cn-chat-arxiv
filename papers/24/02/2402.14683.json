{
    "title": "Visual Hallucinations of Multi-modal Large Language Models",
    "abstract": "arXiv:2402.14683v1 Announce Type: cross  Abstract: Visual hallucination (VH) means that a multi-modal LLM (MLLM) imagines incorrect details about an image in visual question answering. Existing studies find VH instances only in existing image datasets, which results in biased understanding of MLLMs' performance under VH due to limited diversity of such VH instances. In this work, we propose a tool called VHTest to generate a diverse set of VH instances. Specifically, VHTest finds some initial VH instances in existing image datasets (e.g., COCO), generates a text description for each VH mode, and uses a text-to-image generative model (e.g., DALL-E-3) to generate VH images based on the text descriptions. We collect a benchmark dataset with 1,200 VH instances in 8 VH modes using VHTest. We find that existing MLLMs such as GPT-4V, LLaVA-1.5, and MiniGPT-v2 hallucinate for a large fraction of the instances in our benchmark. Moreover, we find that fine-tuning an MLLM using our benchmark data",
    "link": "https://arxiv.org/abs/2402.14683",
    "context": "Title: Visual Hallucinations of Multi-modal Large Language Models\nAbstract: arXiv:2402.14683v1 Announce Type: cross  Abstract: Visual hallucination (VH) means that a multi-modal LLM (MLLM) imagines incorrect details about an image in visual question answering. Existing studies find VH instances only in existing image datasets, which results in biased understanding of MLLMs' performance under VH due to limited diversity of such VH instances. In this work, we propose a tool called VHTest to generate a diverse set of VH instances. Specifically, VHTest finds some initial VH instances in existing image datasets (e.g., COCO), generates a text description for each VH mode, and uses a text-to-image generative model (e.g., DALL-E-3) to generate VH images based on the text descriptions. We collect a benchmark dataset with 1,200 VH instances in 8 VH modes using VHTest. We find that existing MLLMs such as GPT-4V, LLaVA-1.5, and MiniGPT-v2 hallucinate for a large fraction of the instances in our benchmark. Moreover, we find that fine-tuning an MLLM using our benchmark data",
    "path": "papers/24/02/2402.14683.json",
    "total_tokens": 925,
    "translated_title": "多模大语言模型的视觉幻觉",
    "translated_abstract": "视觉幻觉（VH）意味着多模大语言模型（MLLM）在视觉问答中对图像想象出错误的细节。现有研究发现VH实例仅存在于现有图像数据集中，这导致了对MLLM在VH下的性能理解存在偏差，原因在于这类VH实例的多样性有限。在本研究中，我们提出了一个名为VHTest的工具，用于生成多样的VH实例。具体来说，VHTest在现有图像数据集（例如COCO）中找到一些初始的VH实例，为每个VH模式生成一个文本描述，并使用文本到图像生成模型（例如DALL-E-3）基于文本描述生成VH图像。我们利用VHTest收集了一个包含8个VH模式中1,200个VH实例的基准数据集。我们发现，现有的MLLM（例如GPT-4V、LLaVA-1.5和MiniGPT-v2）在我们的基准测试中对大部分实例产生幻觉。此外，我们发现使用我们的基准数据对MLLM进行微调",
    "tldr": "多模大语言模型通过生成多样的视觉幻觉实例来检验其性能，发现现有的模型在这方面存在幻觉问题，为进一步研究和改进提供了线索。",
    "en_tdlr": "Multi-modal large language models are examined for performance by generating diverse visual hallucination instances, revealing existing models' hallucination issues and providing clues for further research and improvement."
}