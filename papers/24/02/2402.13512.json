{
    "title": "From Self-Attention to Markov Models: Unveiling the Dynamics of Generative Transformers",
    "abstract": "arXiv:2402.13512v1 Announce Type: cross  Abstract: Modern language models rely on the transformer architecture and attention mechanism to perform language understanding and text generation. In this work, we study learning a 1-layer self-attention model from a set of prompts and associated output data sampled from the model. We first establish a precise mapping between the self-attention mechanism and Markov models: Inputting a prompt to the model samples the output token according to a context-conditioned Markov chain (CCMC) which weights the transition matrix of a base Markov chain. Additionally, incorporating positional encoding results in position-dependent scaling of the transition probabilities. Building on this formalism, we develop identifiability/coverage conditions for the prompt distribution that guarantee consistent estimation and establish sample complexity guarantees under IID samples. Finally, we study the problem of learning from a single output trajectory generated from",
    "link": "https://arxiv.org/abs/2402.13512",
    "context": "Title: From Self-Attention to Markov Models: Unveiling the Dynamics of Generative Transformers\nAbstract: arXiv:2402.13512v1 Announce Type: cross  Abstract: Modern language models rely on the transformer architecture and attention mechanism to perform language understanding and text generation. In this work, we study learning a 1-layer self-attention model from a set of prompts and associated output data sampled from the model. We first establish a precise mapping between the self-attention mechanism and Markov models: Inputting a prompt to the model samples the output token according to a context-conditioned Markov chain (CCMC) which weights the transition matrix of a base Markov chain. Additionally, incorporating positional encoding results in position-dependent scaling of the transition probabilities. Building on this formalism, we develop identifiability/coverage conditions for the prompt distribution that guarantee consistent estimation and establish sample complexity guarantees under IID samples. Finally, we study the problem of learning from a single output trajectory generated from",
    "path": "papers/24/02/2402.13512.json",
    "total_tokens": 827,
    "translated_title": "从自注意力到马尔可夫模型：揭示生成Transformer的动态",
    "translated_abstract": "现代语言模型依赖Transformer架构和注意力机制来进行语言理解和文本生成。本文研究了从一组提示和与模型采样的关联输出数据中学习一个单层自注意模型。我们首先建立了自注意机制和马尔可夫模型之间的精确映射：将提示输入模型会根据上下文条件的马尔可夫链（CCMC）对输出标记进行采样，该链加权了基本马尔可夫链的转移矩阵。此外，引入位置编码导致了转移概率的位置相关缩放。基于这种形式主义，我们为提示分布开发了可辨识性/覆盖条件，确保一致估计，并在IID样本下建立了样本复杂性保证。最后，我们研究了从单个输出轨迹生成中学习的问题。",
    "tldr": "本文研究了从自注意力模型到马尔可夫模型的转变，揭示了生成Transformer动态的机理和相关条件，为一致估计提供了保证，并在IID样本下建立了样本复杂性保证。",
    "en_tdlr": "This paper investigates the transition from self-attention models to Markov models, revealing the dynamics of generative Transformers, providing guarantees for consistent estimation and establishing sample complexity guarantees under IID samples."
}