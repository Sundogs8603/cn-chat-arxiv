{
    "title": "T-RAG: Lessons from the LLM Trenches",
    "abstract": "Large Language Models (LLM) have shown remarkable language capabilities fueling attempts to integrate them into applications across a wide range of domains. An important application area is question answering over private enterprise documents where the main considerations are data security, which necessitates applications that can be deployed on-prem, limited computational resources and the need for a robust application that correctly responds to queries. Retrieval-Augmented Generation (RAG) has emerged as the most prominent framework for building LLM-based applications. While building a RAG is relatively straightforward, making it robust and a reliable application requires extensive customization and relatively deep knowledge of the application domain. We share our experiences building and deploying an LLM application for question answering over private organizational documents. Our application combines the use of RAG with a finetuned open-source LLM. Additionally, our system, which w",
    "link": "https://arxiv.org/abs/2402.07483",
    "context": "Title: T-RAG: Lessons from the LLM Trenches\nAbstract: Large Language Models (LLM) have shown remarkable language capabilities fueling attempts to integrate them into applications across a wide range of domains. An important application area is question answering over private enterprise documents where the main considerations are data security, which necessitates applications that can be deployed on-prem, limited computational resources and the need for a robust application that correctly responds to queries. Retrieval-Augmented Generation (RAG) has emerged as the most prominent framework for building LLM-based applications. While building a RAG is relatively straightforward, making it robust and a reliable application requires extensive customization and relatively deep knowledge of the application domain. We share our experiences building and deploying an LLM application for question answering over private organizational documents. Our application combines the use of RAG with a finetuned open-source LLM. Additionally, our system, which w",
    "path": "papers/24/02/2402.07483.json",
    "total_tokens": 816,
    "translated_title": "T-RAG: 来自LLM战场的经验教训",
    "translated_abstract": "大型语言模型（LLM）展示了惊人的语言能力，推动了将它们整合到各个领域的应用的尝试。一个重要的应用领域是对私人企业文件进行问答，其中主要考虑因素是数据安全，需要能够在本地部署的应用程序，有限的计算资源和对查询正确响应的健壮应用的需求。检索增强生成（RAG）已成为构建基于LLM的应用程序的最重要的框架。虽然构建RAG相对简单，但要使其健壮和可靠的应用程序需要广泛的定制化和相对深入的应用领域知识。我们分享了构建和部署一个基于LLM的私人组织文件问答应用的经验。我们的应用结合了RAG的使用和经过微调的开源LLM。此外，我们的系统还具有 ...",
    "tldr": "T-RAG是一个基于LLM的应用程序，用于私人企业文件问答，它结合了RAG框架和经过微调的开源LLM，并分享了构建和部署过程中的经验教训。",
    "en_tdlr": "T-RAG is an LLM-based application for question answering over private enterprise documents. It combines the RAG framework with a fine-tuned open-source LLM, and shares lessons learned in the process of building and deploying the system."
}