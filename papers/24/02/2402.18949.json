{
    "title": "Improving Group Connectivity for Generalization of Federated Deep Learning",
    "abstract": "arXiv:2402.18949v1 Announce Type: new  Abstract: Federated learning (FL) involves multiple heterogeneous clients collaboratively training a global model via iterative local updates and model fusion. The generalization of FL's global model has a large gap compared with centralized training, which is its bottleneck for broader applications. In this paper, we study and improve FL's generalization through a fundamental ``connectivity'' perspective, which means how the local models are connected in the parameter region and fused into a generalized global model. The term ``connectivity'' is derived from linear mode connectivity (LMC), studying the interpolated loss landscape of two different solutions (e.g., modes) of neural networks. Bridging the gap between LMC and FL, in this paper, we leverage fixed anchor models to empirically and theoretically study the transitivity property of connectivity from two models (LMC) to a group of models (model fusion in FL). Based on the findings, we propo",
    "link": "https://arxiv.org/abs/2402.18949",
    "context": "Title: Improving Group Connectivity for Generalization of Federated Deep Learning\nAbstract: arXiv:2402.18949v1 Announce Type: new  Abstract: Federated learning (FL) involves multiple heterogeneous clients collaboratively training a global model via iterative local updates and model fusion. The generalization of FL's global model has a large gap compared with centralized training, which is its bottleneck for broader applications. In this paper, we study and improve FL's generalization through a fundamental ``connectivity'' perspective, which means how the local models are connected in the parameter region and fused into a generalized global model. The term ``connectivity'' is derived from linear mode connectivity (LMC), studying the interpolated loss landscape of two different solutions (e.g., modes) of neural networks. Bridging the gap between LMC and FL, in this paper, we leverage fixed anchor models to empirically and theoretically study the transitivity property of connectivity from two models (LMC) to a group of models (model fusion in FL). Based on the findings, we propo",
    "path": "papers/24/02/2402.18949.json",
    "total_tokens": 837,
    "translated_title": "提高联邦深度学习的群组连接性以实现泛化能力",
    "translated_abstract": "联邦学习（FL）涉及多个异构客户端通过迭代本地更新和模型融合共同训练全局模型。与集中式训练相比，FL的全局模型的泛化存在很大差距，这是其在更广泛应用中的瓶颈。本文通过基本的“连接性”视角研究和改进FL的泛化，即本地模型在参数区域中如何连接并融合为泛化的全局模型。术语“连接性”源自线性模式连接（LMC），研究神经网络的两种不同解决方案（例如模式）的内插损失景观。在本文中，我们通过利用固定的锚定模型来研究连接性的传递性质，从两个模型（LMC）到一组模型（FL中的模型融合）。根据所发现的结果，我们提出",
    "tldr": "通过研究和改进联邦学习的泛化能力，本文从“连接性”视角探讨了如何改善本地模型间的连接性以生成更具泛化能力的全局模型。",
    "en_tdlr": "This paper enhances the generalization of federated learning by studying and improving the connectivity among local models to achieve a more generalized global model."
}