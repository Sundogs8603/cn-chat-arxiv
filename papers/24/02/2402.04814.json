{
    "title": "BOWLL: A Deceptively Simple Open World Lifelong Learner",
    "abstract": "The quest to improve scalar performance numbers on predetermined benchmarks seems to be deeply engraved in deep learning. However, the real world is seldom carefully curated and applications are seldom limited to excelling on test sets. A practical system is generally required to recognize novel concepts, refrain from actively including uninformative data, and retain previously acquired knowledge throughout its lifetime. Despite these key elements being rigorously researched individually, the study of their conjunction, open world lifelong learning, is only a recent trend. To accelerate this multifaceted field's exploration, we introduce its first monolithic and much-needed baseline. Leveraging the ubiquitous use of batch normalization across deep neural networks, we propose a deceptively simple yet highly effective way to repurpose standard models for open world lifelong learning. Through extensive empirical evaluation, we highlight why our approach should serve as a future standard f",
    "link": "https://arxiv.org/abs/2402.04814",
    "context": "Title: BOWLL: A Deceptively Simple Open World Lifelong Learner\nAbstract: The quest to improve scalar performance numbers on predetermined benchmarks seems to be deeply engraved in deep learning. However, the real world is seldom carefully curated and applications are seldom limited to excelling on test sets. A practical system is generally required to recognize novel concepts, refrain from actively including uninformative data, and retain previously acquired knowledge throughout its lifetime. Despite these key elements being rigorously researched individually, the study of their conjunction, open world lifelong learning, is only a recent trend. To accelerate this multifaceted field's exploration, we introduce its first monolithic and much-needed baseline. Leveraging the ubiquitous use of batch normalization across deep neural networks, we propose a deceptively simple yet highly effective way to repurpose standard models for open world lifelong learning. Through extensive empirical evaluation, we highlight why our approach should serve as a future standard f",
    "path": "papers/24/02/2402.04814.json",
    "total_tokens": 865,
    "translated_title": "BOWLL：一个看似简单的开放世界终身学习者",
    "translated_abstract": "深度学习中对预先确定的基准测试的标量性能数字的改进似乎深深植根于其中。然而，现实世界很少精心策划，应用也很少仅限于在测试集上表现出色。通常需要一个实际的系统来识别新概念，避免主动包括无信息的数据，并在其生命周期内保留先前获取的知识。尽管这些关键要素在个体上已经进行了严格的研究，但对它们的结合，即开放世界终身学习，只是最近的趋势。为了加速这个多方面领域的探索，我们引入其首个完整且极度需要的基准。利用深度神经网络中批量归一化的普遍应用，我们提出了一个看似简单但非常有效的方法来重新利用标准模型进行开放世界终身学习。通过广泛的实证评估，我们强调为什么我们的方法应该成为未来的标准。",
    "tldr": "这项研究提出了BOWLL，一个看似简单但极其有效的方法，通过重新利用标准模型用于开放世界终身学习，加速了这个多方面领域的探索。"
}