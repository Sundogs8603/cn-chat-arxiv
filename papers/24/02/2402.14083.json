{
    "title": "Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping",
    "abstract": "arXiv:2402.14083v1 Announce Type: new  Abstract: While Transformers have enabled tremendous progress in various application settings, such architectures still lag behind traditional symbolic planners for solving complex decision making tasks. In this work, we demonstrate how to train Transformers to solve complex planning tasks and present Searchformer, a Transformer model that optimally solves previously unseen Sokoban puzzles 93.7% of the time, while using up to 26.8% fewer search steps than standard $A^*$ search. Searchformer is an encoder-decoder Transformer model trained to predict the search dynamics of $A^*$. This model is then fine-tuned via expert iterations to perform fewer search steps than $A^*$ search while still generating an optimal plan. In our training method, $A^*$'s search dynamics are expressed as a token sequence outlining when task states are added and removed into the search tree during symbolic planning. In our ablation studies on maze navigation, we find that S",
    "link": "https://arxiv.org/abs/2402.14083",
    "context": "Title: Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping\nAbstract: arXiv:2402.14083v1 Announce Type: new  Abstract: While Transformers have enabled tremendous progress in various application settings, such architectures still lag behind traditional symbolic planners for solving complex decision making tasks. In this work, we demonstrate how to train Transformers to solve complex planning tasks and present Searchformer, a Transformer model that optimally solves previously unseen Sokoban puzzles 93.7% of the time, while using up to 26.8% fewer search steps than standard $A^*$ search. Searchformer is an encoder-decoder Transformer model trained to predict the search dynamics of $A^*$. This model is then fine-tuned via expert iterations to perform fewer search steps than $A^*$ search while still generating an optimal plan. In our training method, $A^*$'s search dynamics are expressed as a token sequence outlining when task states are added and removed into the search tree during symbolic planning. In our ablation studies on maze navigation, we find that S",
    "path": "papers/24/02/2402.14083.json",
    "total_tokens": 873,
    "translated_title": "超越A*：通过搜索动力学引导以改进变压器规划",
    "translated_abstract": "尽管变压器在各种应用场景中取得了巨大进展，但这种架构在解决复杂决策任务方面仍落后于传统的符号规划器。在这项工作中，我们展示了如何训练变压器来解决复杂的规划任务，并提出了Searchformer，这是一个变压器模型，可以在93.7%的时间内最优地解决以前未见的Sokoban谜题，同时比标准的$A^*$搜索使用少达26.8%的搜索步骤。Searchformer是一个经过训练的编码器-解码器变压器模型，用于预测$A^*$的搜索动力学。然后通过专家迭代进行微调，以执行比$A^*$搜索更少的搜索步骤，同时生成一个最佳计划。在我们的训练方法中，$A^*$的搜索动力学被表达为一个标记序列，描述了符号规划期间任务状态何时被加入和移除到搜索树中。在我们关于迷宫导航的消融研究中，我们发现S",
    "tldr": "通过专家迭代训练的Searchformer模型，可以更少的搜索步骤来解决复杂规划任务，同时生成最佳计划。",
    "en_tdlr": "The Searchformer model, trained via expert iterations, can solve complex planning tasks with fewer search steps while generating optimal plans."
}