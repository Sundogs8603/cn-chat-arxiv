{
    "title": "InteractiveVideo: User-Centric Controllable Video Generation with Synergistic Multimodal Instructions",
    "abstract": "We introduce $\\textit{InteractiveVideo}$, a user-centric framework for video generation. Different from traditional generative approaches that operate based on user-provided images or text, our framework is designed for dynamic interaction, allowing users to instruct the generative model through various intuitive mechanisms during the whole generation process, e.g. text and image prompts, painting, drag-and-drop, etc. We propose a Synergistic Multimodal Instruction mechanism, designed to seamlessly integrate users' multimodal instructions into generative models, thus facilitating a cooperative and responsive interaction between user inputs and the generative process. This approach enables iterative and fine-grained refinement of the generation result through precise and effective user instructions. With $\\textit{InteractiveVideo}$, users are given the flexibility to meticulously tailor key aspects of a video. They can paint the reference image, edit semantics, and adjust video motions ",
    "link": "https://arxiv.org/abs/2402.03040",
    "context": "Title: InteractiveVideo: User-Centric Controllable Video Generation with Synergistic Multimodal Instructions\nAbstract: We introduce $\\textit{InteractiveVideo}$, a user-centric framework for video generation. Different from traditional generative approaches that operate based on user-provided images or text, our framework is designed for dynamic interaction, allowing users to instruct the generative model through various intuitive mechanisms during the whole generation process, e.g. text and image prompts, painting, drag-and-drop, etc. We propose a Synergistic Multimodal Instruction mechanism, designed to seamlessly integrate users' multimodal instructions into generative models, thus facilitating a cooperative and responsive interaction between user inputs and the generative process. This approach enables iterative and fine-grained refinement of the generation result through precise and effective user instructions. With $\\textit{InteractiveVideo}$, users are given the flexibility to meticulously tailor key aspects of a video. They can paint the reference image, edit semantics, and adjust video motions ",
    "path": "papers/24/02/2402.03040.json",
    "total_tokens": 817,
    "translated_title": "交互式视频：以用户为中心的可控视频生成与多模态指令协同",
    "translated_abstract": "我们引入了一种名为“交互式视频”的用户中心视频生成框架。与传统的基于用户提供图像或文本的生成方法不同，我们的框架设计用于动态交互，允许用户通过各种直观的机制在整个生成过程中指导生成模型，例如文本和图像提示、绘画、拖放等。我们提出了一种协同多模态指令机制，旨在将用户的多模态指令无缝集成到生成模型中，从而实现用户输入和生成过程之间的合作和响应式交互。这种方法通过精确而有效的用户指令，实现了对生成结果的迭代和精细化调整。通过“交互式视频”，用户可以灵活地精心调整视频的关键方面，包括绘画参考图像、编辑语义和调整视频动作等。",
    "tldr": "“InteractiveVideo”是一个以用户为中心的视频生成框架，通过交互式的多模态指令，用户可以在整个生成过程中精确、有效地指导生成模型，并对视频的关键方面进行灵活调整。",
    "en_tdlr": "\"InteractiveVideo\" is a user-centric framework for video generation that allows users to interactively guide the generative model using multimodal instructions, facilitating fine-grained adjustments of key aspects of the video."
}