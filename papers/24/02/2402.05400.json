{
    "title": "Optimizing for ROC Curves on Class-Imbalanced Data by Training over a Family of Loss Functions",
    "abstract": "Although binary classification is a well-studied problem in computer vision, training reliable classifiers under severe class imbalance remains a challenging problem. Recent work has proposed techniques that mitigate the effects of training under imbalance by modifying the loss functions or optimization methods. While this work has led to significant improvements in the overall accuracy in the multi-class case, we observe that slight changes in hyperparameter values of these methods can result in highly variable performance in terms of Receiver Operating Characteristic (ROC) curves on binary problems with severe imbalance. To reduce the sensitivity to hyperparameter choices and train more general models, we propose training over a family of loss functions, instead of a single loss function. We develop a method for applying Loss Conditional Training (LCT) to an imbalanced classification problem. Extensive experiment results, on both CIFAR and Kaggle competition datasets, show that our m",
    "link": "https://arxiv.org/abs/2402.05400",
    "context": "Title: Optimizing for ROC Curves on Class-Imbalanced Data by Training over a Family of Loss Functions\nAbstract: Although binary classification is a well-studied problem in computer vision, training reliable classifiers under severe class imbalance remains a challenging problem. Recent work has proposed techniques that mitigate the effects of training under imbalance by modifying the loss functions or optimization methods. While this work has led to significant improvements in the overall accuracy in the multi-class case, we observe that slight changes in hyperparameter values of these methods can result in highly variable performance in terms of Receiver Operating Characteristic (ROC) curves on binary problems with severe imbalance. To reduce the sensitivity to hyperparameter choices and train more general models, we propose training over a family of loss functions, instead of a single loss function. We develop a method for applying Loss Conditional Training (LCT) to an imbalanced classification problem. Extensive experiment results, on both CIFAR and Kaggle competition datasets, show that our m",
    "path": "papers/24/02/2402.05400.json",
    "total_tokens": 821,
    "translated_title": "在类别不平衡数据上通过训练一系列损失函数来优化ROC曲线",
    "translated_abstract": "虽然二元分类在计算机视觉领域已经得到了深入研究，但在严重类别不平衡的情况下训练可靠的分类器仍然是一项具有挑战性的问题。最近的研究提出了通过修改损失函数或优化方法来减轻在不平衡情况下训练的影响的技术。虽然这些研究在多类别情况下整体准确率有了显著改进，但我们观察到这些方法的超参数值的微小变化可能导致在严重不平衡的二元问题上以ROC曲线为指标的性能高度变化。为了降低对超参数选择的敏感性，训练更通用的模型，我们提出了在一系列损失函数上训练，而不是单一损失函数。我们开发了一种在类别不平衡分类问题上应用损失条件训练（Loss Conditional Training，LCT）的方法。",
    "tldr": "通过训练一系列损失函数来优化类别不平衡数据上的ROC曲线，并减少对超参数选择的敏感性。",
    "en_tdlr": "Optimizing ROC curves on class-imbalanced data by training over a family of loss functions to reduce sensitivity to hyperparameter choices."
}