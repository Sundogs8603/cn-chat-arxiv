{
    "title": "Compressing Deep Reinforcement Learning Networks with a Dynamic Structured Pruning Method for Autonomous Driving",
    "abstract": "Deep reinforcement learning (DRL) has shown remarkable success in complex autonomous driving scenarios. However, DRL models inevitably bring high memory consumption and computation, which hinders their wide deployment in resource-limited autonomous driving devices. Structured Pruning has been recognized as a useful method to compress and accelerate DRL models, but it is still challenging to estimate the contribution of a parameter (i.e., neuron) to DRL models. In this paper, we introduce a novel dynamic structured pruning approach that gradually removes a DRL model's unimportant neurons during the training stage. Our method consists of two steps, i.e. training DRL models with a group sparse regularizer and removing unimportant neurons with a dynamic pruning threshold. To efficiently train the DRL model with a small number of important neurons, we employ a neuron-importance group sparse regularizer. In contrast to conventional regularizers, this regularizer imposes a penalty on redundan",
    "link": "https://arxiv.org/abs/2402.05146",
    "context": "Title: Compressing Deep Reinforcement Learning Networks with a Dynamic Structured Pruning Method for Autonomous Driving\nAbstract: Deep reinforcement learning (DRL) has shown remarkable success in complex autonomous driving scenarios. However, DRL models inevitably bring high memory consumption and computation, which hinders their wide deployment in resource-limited autonomous driving devices. Structured Pruning has been recognized as a useful method to compress and accelerate DRL models, but it is still challenging to estimate the contribution of a parameter (i.e., neuron) to DRL models. In this paper, we introduce a novel dynamic structured pruning approach that gradually removes a DRL model's unimportant neurons during the training stage. Our method consists of two steps, i.e. training DRL models with a group sparse regularizer and removing unimportant neurons with a dynamic pruning threshold. To efficiently train the DRL model with a small number of important neurons, we employ a neuron-importance group sparse regularizer. In contrast to conventional regularizers, this regularizer imposes a penalty on redundan",
    "path": "papers/24/02/2402.05146.json",
    "total_tokens": 959,
    "translated_title": "用动态结构化剪枝方法压缩深度强化学习网络，用于自动驾驶",
    "translated_abstract": "深度强化学习在复杂的自动驾驶场景中取得了显著的成功。然而，深度强化学习模型不可避免地带来了高内存消耗和计算量，这阻碍了它们在资源受限的自动驾驶设备中的广泛应用。结构化剪枝被认为是一种有用的方法来压缩和加速深度强化学习模型，但是估计一个参数（即神经元）对深度强化学习模型的贡献仍然具有挑战性。在本文中，我们引入了一种新颖的动态结构化剪枝方法，在训练阶段逐渐删除深度强化学习模型中不重要的神经元。我们的方法包括两个步骤，即使用组稀疏正则化器训练深度强化学习模型，以及使用动态剪枝阈值去除不重要的神经元。为了使用少量重要的神经元有效地训练深度强化学习模型，我们采用了一个神经元重要性组稀疏正则化器。与传统的正则化器不同，这个正则化器对冗余参数施加了惩罚。",
    "tldr": "本文提出了一种动态结构化剪枝方法，用于压缩深度强化学习网络，以便在资源受限的自动驾驶设备中实现高效部署。通过逐渐删除不重要的神经元，我们的方法显著减小了内存消耗和计算量。"
}