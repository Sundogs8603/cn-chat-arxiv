{
    "title": "Scalable Kernel Logistic Regression with Nystr\\\"om Approximation: Theoretical Analysis and Application to Discrete Choice Modelling",
    "abstract": "The application of kernel-based Machine Learning (ML) techniques to discrete choice modelling using large datasets often faces challenges due to memory requirements and the considerable number of parameters involved in these models. This complexity hampers the efficient training of large-scale models. This paper addresses these problems of scalability by introducing the Nystr\\\"om approximation for Kernel Logistic Regression (KLR) on large datasets. The study begins by presenting a theoretical analysis in which: i) the set of KLR solutions is characterised, ii) an upper bound to the solution of KLR with Nystr\\\"om approximation is provided, and finally iii) a specialisation of the optimisation algorithms to Nystr\\\"om KLR is described. After this, the Nystr\\\"om KLR is computationally validated. Four landmark selection methods are tested, including basic uniform sampling, a k-means sampling strategy, and two non-uniform methods grounded in leverage scores. The performance of these strategi",
    "link": "https://arxiv.org/abs/2402.06763",
    "context": "Title: Scalable Kernel Logistic Regression with Nystr\\\"om Approximation: Theoretical Analysis and Application to Discrete Choice Modelling\nAbstract: The application of kernel-based Machine Learning (ML) techniques to discrete choice modelling using large datasets often faces challenges due to memory requirements and the considerable number of parameters involved in these models. This complexity hampers the efficient training of large-scale models. This paper addresses these problems of scalability by introducing the Nystr\\\"om approximation for Kernel Logistic Regression (KLR) on large datasets. The study begins by presenting a theoretical analysis in which: i) the set of KLR solutions is characterised, ii) an upper bound to the solution of KLR with Nystr\\\"om approximation is provided, and finally iii) a specialisation of the optimisation algorithms to Nystr\\\"om KLR is described. After this, the Nystr\\\"om KLR is computationally validated. Four landmark selection methods are tested, including basic uniform sampling, a k-means sampling strategy, and two non-uniform methods grounded in leverage scores. The performance of these strategi",
    "path": "papers/24/02/2402.06763.json",
    "total_tokens": 867,
    "translated_title": "使用Nystr\\\"om近似的可扩展核逻辑回归：理论分析和离散选择建模应用",
    "translated_abstract": "将基于核的机器学习技术应用于使用大规模数据集的离散选择建模时，经常面临存储需求和模型中涉及的大量参数的挑战。这种复杂性影响了大规模模型的高效训练。本文通过引入Nystr\\\"om近似方法解决了可扩展性问题，用于大规模数据集上的核逻辑回归。研究首先进行了理论分析，其中：i) 对KLR解的集合进行了描述，ii) 给出了使用Nystr\\\"om近似的KLR解的上界，并最后描述了专门用于Nystr\\\"om KLR的优化算法的特化。之后，对Nystr\\\"om KLR进行了计算验证。测试了四种地标选择方法，包括基本均匀采样、k-means采样策略和基于杠杆得分的两种非均匀方法。这些策略的性能进行了评估。",
    "tldr": "本文介绍了使用Nystr\\\"om近似方法解决大规模数据集上核逻辑回归的可扩展性问题。研究提供了理论分析并验证了不同的地标选择方法的性能。"
}