{
    "title": "Distilling Morphology-Conditioned Hypernetworks for Efficient Universal Morphology Control",
    "abstract": "Learning a universal policy across different robot morphologies can significantly improve learning efficiency and enable zero-shot generalization to unseen morphologies. However, learning a highly performant universal policy requires sophisticated architectures like transformers (TF) that have larger memory and computational cost than simpler multi-layer perceptrons (MLP). To achieve both good performance like TF and high efficiency like MLP at inference time, we propose HyperDistill, which consists of: (1) A morphology-conditioned hypernetwork (HN) that generates robot-wise MLP policies, and (2) A policy distillation approach that is essential for successful training. We show that on UNIMAL, a benchmark with hundreds of diverse morphologies, HyperDistill performs as well as a universal TF teacher policy on both training and unseen test robots, but reduces model size by 6-14 times, and computational cost by 67-160 times in different environments. Our analysis attributes the efficiency ",
    "link": "https://arxiv.org/abs/2402.06570",
    "context": "Title: Distilling Morphology-Conditioned Hypernetworks for Efficient Universal Morphology Control\nAbstract: Learning a universal policy across different robot morphologies can significantly improve learning efficiency and enable zero-shot generalization to unseen morphologies. However, learning a highly performant universal policy requires sophisticated architectures like transformers (TF) that have larger memory and computational cost than simpler multi-layer perceptrons (MLP). To achieve both good performance like TF and high efficiency like MLP at inference time, we propose HyperDistill, which consists of: (1) A morphology-conditioned hypernetwork (HN) that generates robot-wise MLP policies, and (2) A policy distillation approach that is essential for successful training. We show that on UNIMAL, a benchmark with hundreds of diverse morphologies, HyperDistill performs as well as a universal TF teacher policy on both training and unseen test robots, but reduces model size by 6-14 times, and computational cost by 67-160 times in different environments. Our analysis attributes the efficiency ",
    "path": "papers/24/02/2402.06570.json",
    "total_tokens": 907,
    "translated_title": "通过精简形态条件超网络实现高效的通用形态控制",
    "translated_abstract": "在不同机器人形态之间学习一个通用策略可以显著提高学习效率，并实现对未知形态的零样本泛化。然而，学习一个高性能的通用策略需要像transformers（TF）这样具有较大内存和计算成本的复杂架构，而比较简单的多层感知器（MLP）则具有更高的效率。为了在推理时既能达到像TF一样好的性能，又能具有像MLP一样的高效率，我们提出了HyperDistill。它包括：（1）一个形态条件的超网络（HN），用于生成机器人特定的MLP策略，和（2）一个对于成功训练至关重要的策略蒸馏方法。我们展示了在UNIMAL上，一个包含数百种不同形态的基准测试中，HyperDistill在训练和未知测试机器人上都能和通用的TF教师策略一样表现出色，同时将模型尺寸减小了6-14倍，计算成本在不同环境下减小了67-160倍。",
    "tldr": "本论文提出了一种名为HyperDistill的方法，通过精简形态条件超网络，可在训练和未知测试机器人上实现与通用的transformers策略相当的性能，同时大大减小模型尺寸和计算成本。",
    "en_tdlr": "This paper proposes a method called HyperDistill that achieves performance comparable to universal transformers policy on both training and unseen test robots, while significantly reducing model size and computational cost through distilling morphology-conditioned hypernetwork."
}