{
    "title": "Robustness to Subpopulation Shift with Domain Label Noise via Regularized Annotation of Domains",
    "abstract": "arXiv:2402.11039v1 Announce Type: new  Abstract: Existing methods for last layer retraining that aim to optimize worst-group accuracy (WGA) rely heavily on well-annotated groups in the training data. We show, both in theory and practice, that annotation-based data augmentations using either downsampling or upweighting for WGA are susceptible to domain annotation noise, and in high-noise regimes approach the WGA of a model trained with vanilla empirical risk minimization. We introduce Regularized Annotation of Domains (RAD) in order to train robust last layer classifiers without the need for explicit domain annotations. Our results show that RAD is competitive with other recently proposed domain annotation-free techniques. Most importantly, RAD outperforms state-of-the-art annotation-reliant methods even with only 5% noise in the training data for several publicly available datasets.",
    "link": "https://arxiv.org/abs/2402.11039",
    "context": "Title: Robustness to Subpopulation Shift with Domain Label Noise via Regularized Annotation of Domains\nAbstract: arXiv:2402.11039v1 Announce Type: new  Abstract: Existing methods for last layer retraining that aim to optimize worst-group accuracy (WGA) rely heavily on well-annotated groups in the training data. We show, both in theory and practice, that annotation-based data augmentations using either downsampling or upweighting for WGA are susceptible to domain annotation noise, and in high-noise regimes approach the WGA of a model trained with vanilla empirical risk minimization. We introduce Regularized Annotation of Domains (RAD) in order to train robust last layer classifiers without the need for explicit domain annotations. Our results show that RAD is competitive with other recently proposed domain annotation-free techniques. Most importantly, RAD outperforms state-of-the-art annotation-reliant methods even with only 5% noise in the training data for several publicly available datasets.",
    "path": "papers/24/02/2402.11039.json",
    "total_tokens": 872,
    "translated_title": "具有领域标签噪声的亚群体转移鲁棒性通过领域标注的正则化",
    "translated_abstract": "现有针对最优组准确性(WGA)进行最后一层重新训练的方法在训练数据中过于依赖于良好标注的组。我们理论上和实践中展示了，基于注释的数据增强使用下采样或上加权用于WGA是容易受到领域标注噪声干扰，在高噪声情况下接近使用原始经验风险最小化训练的模型的WGA。我们引入了领域标注正则化(RAD)来训练具有鲁棒性的最后一层分类器，而无需明确的领域标注。我们的结果表明，RAD与其他最近提出的无领域标注技术具有竞争力。最重要的是，即使在训练数据中仅有5%的噪声，RAD也在几个公开可用数据集上胜过最先进的依赖注释的方法。",
    "tldr": "提出了一种名为RAD的方法，通过领域标注的正则化来训练鲁棒的最后一层分类器，无需显式的领域标注，在具有领域标签噪声的情况下表现优越。",
    "en_tdlr": "Introduced a method called RAD, which trains robust last layer classifiers through regularized annotation of domains without the need for explicit domain annotations, outperforming state-of-the-art annotation-reliant methods even with domain label noise present."
}