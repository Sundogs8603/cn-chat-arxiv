{
    "title": "WeakSAM: Segment Anything Meets Weakly-supervised Instance-level Recognition",
    "abstract": "arXiv:2402.14812v1 Announce Type: cross  Abstract: Weakly supervised visual recognition using inexact supervision is a critical yet challenging learning problem. It significantly reduces human labeling costs and traditionally relies on multi-instance learning and pseudo-labeling. This paper introduces WeakSAM and solves the weakly-supervised object detection (WSOD) and segmentation by utilizing the pre-learned world knowledge contained in a vision foundation model, i.e., the Segment Anything Model (SAM). WeakSAM addresses two critical limitations in traditional WSOD retraining, i.e., pseudo ground truth (PGT) incompleteness and noisy PGT instances, through adaptive PGT generation and Region of Interest (RoI) drop regularization. It also addresses the SAM's problems of requiring prompts and category unawareness for automatic object detection and segmentation. Our results indicate that WeakSAM significantly surpasses previous state-of-the-art methods in WSOD and WSIS benchmarks with larg",
    "link": "https://arxiv.org/abs/2402.14812",
    "context": "Title: WeakSAM: Segment Anything Meets Weakly-supervised Instance-level Recognition\nAbstract: arXiv:2402.14812v1 Announce Type: cross  Abstract: Weakly supervised visual recognition using inexact supervision is a critical yet challenging learning problem. It significantly reduces human labeling costs and traditionally relies on multi-instance learning and pseudo-labeling. This paper introduces WeakSAM and solves the weakly-supervised object detection (WSOD) and segmentation by utilizing the pre-learned world knowledge contained in a vision foundation model, i.e., the Segment Anything Model (SAM). WeakSAM addresses two critical limitations in traditional WSOD retraining, i.e., pseudo ground truth (PGT) incompleteness and noisy PGT instances, through adaptive PGT generation and Region of Interest (RoI) drop regularization. It also addresses the SAM's problems of requiring prompts and category unawareness for automatic object detection and segmentation. Our results indicate that WeakSAM significantly surpasses previous state-of-the-art methods in WSOD and WSIS benchmarks with larg",
    "path": "papers/24/02/2402.14812.json",
    "total_tokens": 885,
    "translated_title": "WeakSAM: 任意分割遇上弱监督实例级别识别",
    "translated_abstract": "弱监督的视觉识别使用不精确的监督是一个关键但具有挑战性的学习问题。它显著降低了人工标注成本，并且传统上依赖多实例学习和伪标签。本文介绍了WeakSAM，并通过利用包含在视觉基础模型中的预先学习的全球知识，即Segment Anything Model (SAM)，来解决弱监督物体检测（WSOD）和分割。WeakSAM通过自适应PGT生成和感兴趣区域（RoI）丢弃正则化，解决了传统WSOD重新训练中的两个关键限制，即伪标准地面真相（PGT）的不完整性和具有嘈杂PGT实例。它还解决了SAM在自动对象检测和分割时需要提示和类别无感知性的问题。我们的结果表明，WeakSAM在WSOD和WSIS基准测试中显著超越了先前的最先进方法。",
    "tldr": "WeakSAM通过利用预先学习的全球知识，解决了弱监督对象检测和分割问题，提出了自适应PGT生成和RoI丢弃正则化，显著超越了先前的最先进方法。",
    "en_tdlr": "WeakSAM addresses weakly supervised object detection and segmentation by leveraging pre-learned world knowledge, introducing adaptive PGT generation and RoI drop regularization, showing significant improvement over previous state-of-the-art methods in WSOD and WSIS benchmarks."
}