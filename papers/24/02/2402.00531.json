{
    "title": "Preconditioning for Physics-Informed Neural Networks",
    "abstract": "Physics-informed neural networks (PINNs) have shown promise in solving various partial differential equations (PDEs). However, training pathologies have negatively affected the convergence and prediction accuracy of PINNs, which further limits their practical applications. In this paper, we propose to use condition number as a metric to diagnose and mitigate the pathologies in PINNs. Inspired by classical numerical analysis, where the condition number measures sensitivity and stability, we highlight its pivotal role in the training dynamics of PINNs. We prove theorems to reveal how condition number is related to both the error control and convergence of PINNs. Subsequently, we present an algorithm that leverages preconditioning to improve the condition number. Evaluations of 18 PDE problems showcase the superior performance of our method. Significantly, in 7 of these problems, our method reduces errors by an order of magnitude. These empirical findings verify the critical role of the c",
    "link": "https://arxiv.org/abs/2402.00531",
    "context": "Title: Preconditioning for Physics-Informed Neural Networks\nAbstract: Physics-informed neural networks (PINNs) have shown promise in solving various partial differential equations (PDEs). However, training pathologies have negatively affected the convergence and prediction accuracy of PINNs, which further limits their practical applications. In this paper, we propose to use condition number as a metric to diagnose and mitigate the pathologies in PINNs. Inspired by classical numerical analysis, where the condition number measures sensitivity and stability, we highlight its pivotal role in the training dynamics of PINNs. We prove theorems to reveal how condition number is related to both the error control and convergence of PINNs. Subsequently, we present an algorithm that leverages preconditioning to improve the condition number. Evaluations of 18 PDE problems showcase the superior performance of our method. Significantly, in 7 of these problems, our method reduces errors by an order of magnitude. These empirical findings verify the critical role of the c",
    "path": "papers/24/02/2402.00531.json",
    "total_tokens": 895,
    "translated_title": "预处理对物理信息神经网络的作用",
    "translated_abstract": "物理信息神经网络（PINNs）已经展示出在解决各种偏微分方程（PDEs）方面的潜力。然而，训练病态影响了PINNs的收敛性和预测精度，进一步限制了它们的实际应用。本文中，我们提出使用条件数作为一种度量标准来诊断和缓解PINNs中的训练病态。受经典数值分析的启发，其中条件数测量敏感性和稳定性，我们强调其在PINNs的训练动态中的关键作用。我们证明了定理，揭示了条件数与PINNs的误差控制和收敛性的关系。随后，我们提出了一种利用预处理来改善条件数的算法。对18个PDE问题的评估展示了我们方法的优越性能。值得注意的是，在其中的7个问题中，我们的方法将错误减少了一个数量级。这些实证发现验证了条件数的关键作用。",
    "tldr": "使用条件数作为度量标准来诊断和缓解物理信息神经网络中的训练病态。使用预处理来改善条件数。在18个PDE问题的评估中，我们的方法展示出了优越的性能，特别是在7个问题中将误差减少了一个数量级。",
    "en_tdlr": "Diagnosing and mitigating training pathologies in Physics-Informed Neural Networks (PINNs) using condition number as a metric. Improving condition number through preconditioning. Our method demonstrates superior performance in evaluating 18 PDE problems, reducing errors by an order of magnitude in 7 of them."
}