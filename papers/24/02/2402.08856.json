{
    "title": "Approximation of relation functions and attention mechanisms",
    "abstract": "arXiv:2402.08856v1 Announce Type: new Abstract: Inner products of neural network feature maps arises in a wide variety of machine learning frameworks as a method of modeling relations between inputs. This work studies the approximation properties of inner products of neural networks. It is shown that the inner product of a multi-layer perceptron with itself is a universal approximator for symmetric positive-definite relation functions. In the case of asymmetric relation functions, it is shown that the inner product of two different multi-layer perceptrons is a universal approximator. In both cases, a bound is obtained on the number of neurons required to achieve a given accuracy of approximation. In the symmetric case, the function class can be identified with kernels of reproducing kernel Hilbert spaces, whereas in the asymmetric case the function class can be identified with kernels of reproducing kernel Banach spaces. Finally, these approximation results are applied to analyzing the",
    "link": "https://arxiv.org/abs/2402.08856",
    "context": "Title: Approximation of relation functions and attention mechanisms\nAbstract: arXiv:2402.08856v1 Announce Type: new Abstract: Inner products of neural network feature maps arises in a wide variety of machine learning frameworks as a method of modeling relations between inputs. This work studies the approximation properties of inner products of neural networks. It is shown that the inner product of a multi-layer perceptron with itself is a universal approximator for symmetric positive-definite relation functions. In the case of asymmetric relation functions, it is shown that the inner product of two different multi-layer perceptrons is a universal approximator. In both cases, a bound is obtained on the number of neurons required to achieve a given accuracy of approximation. In the symmetric case, the function class can be identified with kernels of reproducing kernel Hilbert spaces, whereas in the asymmetric case the function class can be identified with kernels of reproducing kernel Banach spaces. Finally, these approximation results are applied to analyzing the",
    "path": "papers/24/02/2402.08856.json",
    "total_tokens": 803,
    "translated_title": "关于关系函数和注意力机制的近似方法",
    "translated_abstract": "神经网络特征映射的内积在各种机器学习框架中被用于建模输入之间的关系。本研究探讨了神经网络内积的近似性质。研究结果表明，多层感知机自身的内积是对称正定关系函数的通用逼近器。对于非对称关系函数，不同的多层感知机的内积是一个通用逼近器。在两种情况下，都得到了达到给定逼近精度所需的神经元数量的界限。对称情况下，函数类可以被认为是再生核希尔伯特空间中的核函数，而对称情况下函数类可以被认为是再生核巴拿赫空间中的核函数。最后，这些逼近结果被应用于分析...",
    "tldr": "研究了多层感知机内积的近似性质，揭示了它们作为通用逼近器的能力。得到了对称和非对称关系函数逼近所需神经元数量的界限。",
    "en_tdlr": "This study investigates the approximation properties of inner products of multi-layer perceptrons, revealing their ability as universal approximators. Bounds on the number of neurons required for approximating symmetric and asymmetric relation functions are obtained."
}