{
    "title": "A Theoretical Result on the Inductive Bias of RNN Language Models",
    "abstract": "arXiv:2402.15814v1 Announce Type: new  Abstract: Recent work by Hewitt et al. (2020) provides a possible interpretation of the empirical success of recurrent neural networks (RNNs) as language models (LMs).   It shows that RNNs can efficiently represent bounded hierarchical structures that are prevalent in human language.   This suggests that RNNs' success might be linked to their ability to model hierarchy.   However, a closer inspection of Hewitt et al.'s (2020) construction shows that it is not limited to hierarchical LMs, posing the question of what \\emph{other classes} of LMs can be efficiently represented by RNNs.   To this end, we generalize their construction to show that RNNs can efficiently represent a larger class of LMs: Those that can be represented by a pushdown automaton with a bounded stack and a generalized stack update function.   This is analogous to an automaton that keeps a memory of a fixed number of symbols and updates the memory with a simple update mechanism.  ",
    "link": "https://arxiv.org/abs/2402.15814",
    "context": "Title: A Theoretical Result on the Inductive Bias of RNN Language Models\nAbstract: arXiv:2402.15814v1 Announce Type: new  Abstract: Recent work by Hewitt et al. (2020) provides a possible interpretation of the empirical success of recurrent neural networks (RNNs) as language models (LMs).   It shows that RNNs can efficiently represent bounded hierarchical structures that are prevalent in human language.   This suggests that RNNs' success might be linked to their ability to model hierarchy.   However, a closer inspection of Hewitt et al.'s (2020) construction shows that it is not limited to hierarchical LMs, posing the question of what \\emph{other classes} of LMs can be efficiently represented by RNNs.   To this end, we generalize their construction to show that RNNs can efficiently represent a larger class of LMs: Those that can be represented by a pushdown automaton with a bounded stack and a generalized stack update function.   This is analogous to an automaton that keeps a memory of a fixed number of symbols and updates the memory with a simple update mechanism.  ",
    "path": "papers/24/02/2402.15814.json",
    "total_tokens": 870,
    "translated_title": "RNN语言模型归纳偏差的一个理论结果",
    "translated_abstract": "最近Hewitt等人（2020）的工作提出了对循环神经网络（RNNs）作为语言模型（LMs）的经验成功可能性的一个解释。 它显示RNNs可以有效地表示在人类语言中普遍存在的有界分层结构。 这表明RNNs的成功可能与它们建模层次结构的能力有关。 然而，对Hewitt等人（2020）构造的更详细检查表明，它不限于分层LMs，这引出了RNNs可以有效表示哪些\\emph{其他类型} LMs的问题。 为此，我们概括他们的构造以展示RNNs可以有效表示更大类别的LMs：可以通过带有有界堆栈和广义堆栈更新函数的下推自动机表示的那些。 这类似于一个保留固定数量符号记忆并使用简单更新机制更新记忆的自动机。",
    "tldr": "RNN语言模型能够有效表示更大类别的语言模型，具有有界堆栈和广义堆栈更新函数，类似于保留固定数量符号记忆并使用简单更新机制的自动机。",
    "en_tdlr": "RNN language models can efficiently represent a larger class of language models with bounded stack and generalized stack update function, similar to an automaton that keeps memory of a fixed number of symbols and updates memory with a simple update mechanism."
}