{
    "title": "Language Models as Science Tutors",
    "abstract": "arXiv:2402.11111v1 Announce Type: cross  Abstract: NLP has recently made exciting progress toward training language models (LMs) with strong scientific problem-solving skills. However, model development has not focused on real-life use-cases of LMs for science, including applications in education that require processing long scientific documents. To address this, we introduce TutorEval and TutorChat. TutorEval is a diverse question-answering benchmark consisting of questions about long chapters from STEM textbooks, written by experts. TutorEval helps measure real-life usability of LMs as scientific assistants, and it is the first benchmark combining long contexts, free-form generation, and multi-disciplinary scientific knowledge. Moreover, we show that fine-tuning base models with existing dialogue datasets leads to poor performance on TutorEval. Therefore, we create TutorChat, a dataset of 80,000 long synthetic dialogues about textbooks. We use TutorChat to fine-tune Llemma models wit",
    "link": "https://arxiv.org/abs/2402.11111",
    "context": "Title: Language Models as Science Tutors\nAbstract: arXiv:2402.11111v1 Announce Type: cross  Abstract: NLP has recently made exciting progress toward training language models (LMs) with strong scientific problem-solving skills. However, model development has not focused on real-life use-cases of LMs for science, including applications in education that require processing long scientific documents. To address this, we introduce TutorEval and TutorChat. TutorEval is a diverse question-answering benchmark consisting of questions about long chapters from STEM textbooks, written by experts. TutorEval helps measure real-life usability of LMs as scientific assistants, and it is the first benchmark combining long contexts, free-form generation, and multi-disciplinary scientific knowledge. Moreover, we show that fine-tuning base models with existing dialogue datasets leads to poor performance on TutorEval. Therefore, we create TutorChat, a dataset of 80,000 long synthetic dialogues about textbooks. We use TutorChat to fine-tune Llemma models wit",
    "path": "papers/24/02/2402.11111.json",
    "total_tokens": 841,
    "translated_title": "语言模型作为科学导师",
    "translated_abstract": "NLP最近取得了令人兴奋的进展，朝着训练具有较强科学问题解决能力的语言模型（LMs）的方向发展。然而，模型的发展并没有专注于LMs在科学教育中的实际用例，包括需要处理长篇科学文档的应用。为了解决这个问题，我们引入了TutorEval和TutorChat。TutorEval是一个多样化的问答基准，其中包含有关STEM教科书长篇章节的问题，由专家编写。TutorEval有助于衡量LMs作为科学助手的实际可用性，它是第一个结合长上下文、自由生成和多学科科学知识的基准。此外，我们展示了使用现有对话数据集对基础模型进行微调会导致TutorEval性能不佳。因此，我们创建了TutorChat，这是一个包含80,000个关于教科书的长合成对话的数据集。我们使用TutorChat来微调Llemma模型。",
    "tldr": "介绍了TutorEval和TutorChat，通过TutorEval基准可以衡量LMs作为科学助手的实际可用性，TutorChat数据集用于微调模型。",
    "en_tdlr": "Introducing TutorEval and TutorChat, where TutorEval benchmark measures the real-life usability of LMs as scientific assistants, and TutorChat dataset is used for model finetuning."
}