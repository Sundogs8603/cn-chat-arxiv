{
    "title": "Image-Caption Encoding for Improving Zero-Shot Generalization",
    "abstract": "Recent advances in vision-language models have combined contrastive approaches with generative methods to achieve state-of-the-art (SOTA) on downstream inference tasks like zero-shot image classification. However, a persistent issue of these models for image classification is their out-of-distribution (OOD) generalization capabilities. We first show that when an OOD data point is misclassified, the correct class can be typically found in the Top-K predicted classes. In order to steer the model prediction toward the correct class within the top predicted classes, we propose the Image-Caption Encoding (ICE) method, a straightforward approach that directly enforces consistency between the image-conditioned and caption-conditioned predictions at evaluation time only. Intuitively, we take advantage of unique properties of the generated captions to guide our local search for the correct class label within the Top-K predicted classes. We show that our method can be easily combined with other ",
    "link": "https://arxiv.org/abs/2402.02662",
    "context": "Title: Image-Caption Encoding for Improving Zero-Shot Generalization\nAbstract: Recent advances in vision-language models have combined contrastive approaches with generative methods to achieve state-of-the-art (SOTA) on downstream inference tasks like zero-shot image classification. However, a persistent issue of these models for image classification is their out-of-distribution (OOD) generalization capabilities. We first show that when an OOD data point is misclassified, the correct class can be typically found in the Top-K predicted classes. In order to steer the model prediction toward the correct class within the top predicted classes, we propose the Image-Caption Encoding (ICE) method, a straightforward approach that directly enforces consistency between the image-conditioned and caption-conditioned predictions at evaluation time only. Intuitively, we take advantage of unique properties of the generated captions to guide our local search for the correct class label within the Top-K predicted classes. We show that our method can be easily combined with other ",
    "path": "papers/24/02/2402.02662.json",
    "total_tokens": 824,
    "translated_title": "提升零样本泛化能力的图像-字幕编码方法",
    "translated_abstract": "最近，视觉-语言模型在对比方法与生成方法相结合的基础上，在零样本图像分类等下游推断任务上取得了最先进的结果。然而，图像分类模型的一个持续存在的问题是其在分布外泛化能力的不足。我们首先展示了当一个分布外数据点被错误分类时，正确类别通常可以在前K个预测类别中找到。为了将模型预测导向前K个预测类别中的正确类别，我们提出了图像-字幕编码（ICE）方法，这是一种直接在评估时只在图像条件和字幕条件下进行一致性约束的简单方法。直观地说，我们利用生成的字幕的独特属性来指导我们在前K个预测类别中寻找正确类别标签的局部搜索。我们展示了我们的方法可以与其他方法轻松结合。",
    "tldr": "本研究提出了一种名为图像-字幕编码（ICE）的方法，通过在评估时对图像和字幕条件下的预测进行一致性约束，来改善图像分类模型的分布外泛化能力。"
}