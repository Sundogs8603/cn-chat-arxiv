{
    "title": "An Inexact Halpern Iteration for with Application to Distributionally Robust Optimization",
    "abstract": "The Halpern iteration for solving monotone inclusion problems has gained increasing interests in recent years due to its simple form and appealing convergence properties. In this paper, we investigate the inexact variants of the scheme in both deterministic and stochastic settings. We conduct extensive convergence analysis and show that by choosing the inexactness tolerances appropriately, the inexact schemes admit an $O(k^{-1})$ convergence rate in terms of the (expected) residue norm. Our results relax the state-of-the-art inexactness conditions employed in the literature while sharing the same competitive convergence properties. We then demonstrate how the proposed methods can be applied for solving two classes of data-driven Wasserstein distributionally robust optimization problems that admit convex-concave min-max optimization reformulations. We highlight its capability of performing inexact computations for distributionally robust learning with stochastic first-order methods.",
    "link": "https://arxiv.org/abs/2402.06033",
    "context": "Title: An Inexact Halpern Iteration for with Application to Distributionally Robust Optimization\nAbstract: The Halpern iteration for solving monotone inclusion problems has gained increasing interests in recent years due to its simple form and appealing convergence properties. In this paper, we investigate the inexact variants of the scheme in both deterministic and stochastic settings. We conduct extensive convergence analysis and show that by choosing the inexactness tolerances appropriately, the inexact schemes admit an $O(k^{-1})$ convergence rate in terms of the (expected) residue norm. Our results relax the state-of-the-art inexactness conditions employed in the literature while sharing the same competitive convergence properties. We then demonstrate how the proposed methods can be applied for solving two classes of data-driven Wasserstein distributionally robust optimization problems that admit convex-concave min-max optimization reformulations. We highlight its capability of performing inexact computations for distributionally robust learning with stochastic first-order methods.",
    "path": "papers/24/02/2402.06033.json",
    "total_tokens": 963,
    "translated_title": "不精确的Halpern迭代算法及其在分布鲁棒优化中的应用",
    "translated_abstract": "Halpern迭代算法因其简单形式和吸引人的收敛性质，近年来在解决单调包含问题方面引起了越来越多的关注。本文研究了确定性和随机环境下该方案的不精确变种。我们进行了广泛的收敛性分析，并表明通过适当选择不精确的容差，不精确方案在（期望的）残差范数上具有O(k^-1)的收敛速度。我们的结果放宽了文献中采用的最新不精确性条件，同时具有相同的竞争性收敛特性。然后，我们演示了如何使用所提出的方法解决两类具有凸凹最小-最大优化重构的数据驱动Wasserstein分布鲁棒优化问题。我们强调了其在使用随机一阶方法进行分布鲁棒学习中的不精确计算能力。",
    "tldr": "本文研究了确定性和随机环境下Halpern迭代算法的不精确变种，通过适当选择不精确的容差，这些变种展现出O(k^-1)的收敛速度，同时具有竞争性的收敛特性。并且我们还展示了这些方法在两类数据驱动Wasserstein分布鲁棒优化问题中的应用，以及在分布鲁棒学习中使用随机一阶方法进行不精确计算的能力。",
    "en_tdlr": "This paper investigates inexact variants of the Halpern iteration in both deterministic and stochastic settings, which exhibit competitive convergence properties with an O(k^-1) convergence rate by appropriately choosing the inexactness tolerances. The proposed methods are also applied to data-driven Wasserstein distributionally robust optimization problems and showcase the ability to perform inexact computations for distributionally robust learning with stochastic first-order methods."
}