{
    "title": "Secure Federated Learning Across Heterogeneous Cloud and High-Performance Computing Resources -- A Case Study on Federated Fine-tuning of LLaMA 2",
    "abstract": "arXiv:2402.12271v1 Announce Type: cross  Abstract: Federated learning enables multiple data owners to collaboratively train robust machine learning models without transferring large or sensitive local datasets by only sharing the parameters of the locally trained models. In this paper, we elaborate on the design of our Advanced Privacy-Preserving Federated Learning (APPFL) framework, which streamlines end-to-end secure and reliable federated learning experiments across cloud computing facilities and high-performance computing resources by leveraging Globus Compute, a distributed function as a service platform, and Amazon Web Services. We further demonstrate the use case of APPFL in fine-tuning a LLaMA 2 7B model using several cloud resources and supercomputers.",
    "link": "https://arxiv.org/abs/2402.12271",
    "context": "Title: Secure Federated Learning Across Heterogeneous Cloud and High-Performance Computing Resources -- A Case Study on Federated Fine-tuning of LLaMA 2\nAbstract: arXiv:2402.12271v1 Announce Type: cross  Abstract: Federated learning enables multiple data owners to collaboratively train robust machine learning models without transferring large or sensitive local datasets by only sharing the parameters of the locally trained models. In this paper, we elaborate on the design of our Advanced Privacy-Preserving Federated Learning (APPFL) framework, which streamlines end-to-end secure and reliable federated learning experiments across cloud computing facilities and high-performance computing resources by leveraging Globus Compute, a distributed function as a service platform, and Amazon Web Services. We further demonstrate the use case of APPFL in fine-tuning a LLaMA 2 7B model using several cloud resources and supercomputers.",
    "path": "papers/24/02/2402.12271.json",
    "total_tokens": 801,
    "translated_title": "跨异构云和高性能计算资源的安全联邦学习——以LLaMA 2的联邦微调为例",
    "translated_abstract": "联邦学习使多个数据所有者能够协作训练健壮的机器学习模型，而无需转移大型或敏感的本地数据集，只需共享本地训练模型的参数。本文详细阐述了我们的高级隐私保护联邦学习（APPFL）框架的设计，通过利用Globus Compute（一种分布式函数即服务平台）和亚马逊云服务，在云计算设施和高性能计算资源之间简化端到端安全可靠的联邦学习实验。我们进一步演示了在多个云资源和超级计算机上使用APPFL来微调LLaMA 27B模型的用例。",
    "tldr": "本文介绍了一种跨异构云和高性能计算资源的安全联邦学习框架，利用Globus Compute和亚马逊云服务，实现了端到端的隐私保护，文中以LLaMA 27B模型的联邦微调为例。"
}