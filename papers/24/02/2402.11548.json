{
    "title": "KMMLU: Measuring Massive Multitask Language Understanding in Korean",
    "abstract": "arXiv:2402.11548v1 Announce Type: new  Abstract: We propose KMMLU, a new Korean benchmark with 35,030 expert-level multiple-choice questions across 45 subjects ranging from humanities to STEM. Unlike previous Korean benchmarks that are translated from existing English benchmarks, KMMLU is collected from original Korean exams, capturing linguistic and cultural aspects of the Korean language. We test 26 publically available and proprietary LLMs, identifying significant room for improvement. The best publicly available model achieves 50.54% on KMMLU, far below the average human performance of 62.6%. This model was primarily trained for English and Chinese, not Korean. Current LLMs tailored to Korean, such as Polyglot-Ko, perform far worse. Surprisingly, even the most capable proprietary LLMs, e.g., GPT-4 and HyperCLOVA X, achieve 59.95% and 53.40%, respectively. This suggests that further work is needed to improve Korean LLMs, and KMMLU offers the right tool to track this progress. We mak",
    "link": "https://arxiv.org/abs/2402.11548",
    "context": "Title: KMMLU: Measuring Massive Multitask Language Understanding in Korean\nAbstract: arXiv:2402.11548v1 Announce Type: new  Abstract: We propose KMMLU, a new Korean benchmark with 35,030 expert-level multiple-choice questions across 45 subjects ranging from humanities to STEM. Unlike previous Korean benchmarks that are translated from existing English benchmarks, KMMLU is collected from original Korean exams, capturing linguistic and cultural aspects of the Korean language. We test 26 publically available and proprietary LLMs, identifying significant room for improvement. The best publicly available model achieves 50.54% on KMMLU, far below the average human performance of 62.6%. This model was primarily trained for English and Chinese, not Korean. Current LLMs tailored to Korean, such as Polyglot-Ko, perform far worse. Surprisingly, even the most capable proprietary LLMs, e.g., GPT-4 and HyperCLOVA X, achieve 59.95% and 53.40%, respectively. This suggests that further work is needed to improve Korean LLMs, and KMMLU offers the right tool to track this progress. We mak",
    "path": "papers/24/02/2402.11548.json",
    "total_tokens": 958,
    "translated_title": "KMMLU：在韩语中测量大规模多任务语言理解",
    "translated_abstract": "我们提出了KMMLU，这是一个新的韩语基准，涵盖了来自人文科学到STEM的45个学科的35,030道专家级多项选择题。与之前从现有英语基准翻译而来的韩语基准不同，KMMLU是从原始韩语考试中收集的，捕捉了韩语的语言和文化方面。我们测试了26个公开可用的和专有的LLM，发现有很大的改进空间。最好的公开模型在KMMLU上的准确率为50.54%，远低于平均人类表现的62.6%。这个模型主要是针对英语和中文进行训练的，而不是韩语。目前针对韩语的LLM，如Polyglot-Ko，表现得更糟。令人惊讶的是，即使是最有能力的专有LLM，例如GPT-4和HyperCLOVA X，也只分别达到了59.95%和53.40%。这表明需要进一步的工作来改进韩语LLM，而KMMLU提供了追踪这一进展的正确工具。",
    "tldr": "KMMLU是一个新的韩语基准，包含35,030道专家级多选题，从原始韩语考试中收集而来，测试了26个LLM模型，发现这些模型在KMMLU上的表现有很大提升空间。",
    "en_tdlr": "KMMLU is a new Korean benchmark with 35,030 expert-level multiple-choice questions collected from original Korean exams, testing 26 LLMs and identifying significant room for improvement in their performance on KMMLU."
}