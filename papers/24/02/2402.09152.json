{
    "title": "Improved Regret for Bandit Convex Optimization with Delayed Feedback",
    "abstract": "arXiv:2402.09152v1 Announce Type: new Abstract: We investigate bandit convex optimization (BCO) with delayed feedback, where only the loss value of the action is revealed under an arbitrary delay. Previous studies have established a regret bound of $O(T^{3/4}+d^{1/3}T^{2/3})$ for this problem, where $d$ is the maximum delay, by simply feeding delayed loss values to the classical bandit gradient descent (BGD) algorithm. In this paper, we develop a novel algorithm to enhance the regret, which carefully exploits the delayed bandit feedback via a blocking update mechanism. Our analysis first reveals that the proposed algorithm can decouple the joint effect of the delays and bandit feedback on the regret, and improve the regret bound to $O(T^{3/4}+\\sqrt{dT})$ for convex functions. Compared with the previous result, our regret matches the $O(T^{3/4})$ regret of BGD in the non-delayed setting for a larger amount of delay, i.e., $d=O(\\sqrt{T})$, instead of $d=O(T^{1/4})$. Furthermore, we consi",
    "link": "https://arxiv.org/abs/2402.09152",
    "context": "Title: Improved Regret for Bandit Convex Optimization with Delayed Feedback\nAbstract: arXiv:2402.09152v1 Announce Type: new Abstract: We investigate bandit convex optimization (BCO) with delayed feedback, where only the loss value of the action is revealed under an arbitrary delay. Previous studies have established a regret bound of $O(T^{3/4}+d^{1/3}T^{2/3})$ for this problem, where $d$ is the maximum delay, by simply feeding delayed loss values to the classical bandit gradient descent (BGD) algorithm. In this paper, we develop a novel algorithm to enhance the regret, which carefully exploits the delayed bandit feedback via a blocking update mechanism. Our analysis first reveals that the proposed algorithm can decouple the joint effect of the delays and bandit feedback on the regret, and improve the regret bound to $O(T^{3/4}+\\sqrt{dT})$ for convex functions. Compared with the previous result, our regret matches the $O(T^{3/4})$ regret of BGD in the non-delayed setting for a larger amount of delay, i.e., $d=O(\\sqrt{T})$, instead of $d=O(T^{1/4})$. Furthermore, we consi",
    "path": "papers/24/02/2402.09152.json",
    "total_tokens": 1131,
    "translated_title": "改进了具有延迟反馈的强化学习问题的后悔率",
    "translated_abstract": "我们研究了具有延迟反馈的强化学习问题，其中只有在任意延迟下才会显示动作的损失值。先前的研究通过简单地将延迟的损失值输入到经典的强化学习梯度下降算法，建立了该问题的后悔界限为$O(T^{3/4}+d^{1/3}T^{2/3})$。在本文中，我们开发了一种新的算法来提高后悔率，通过一个阻塞更新机制精确利用延迟的强化学习反馈。我们的分析首先揭示了所提出的算法可以分离延迟和强化学习反馈对后悔率的联合影响，并将凸函数的后悔界限改进为$O(T^{3/4}+\\sqrt{dT})$。与先前的结果相比，在更大的延迟量$d=O(\\sqrt{T})$，而不是$d=O(T^{1/4})$的情况下，我们的后悔与非延迟设置下强化学习梯度下降算法的$O(T^{3/4})$后悔相匹配。",
    "tldr": "本文针对具有延迟反馈的强化学习问题提出了一种改进后悔率的算法，通过精确利用延迟的强化学习反馈，成功将后悔界限从$O(T^{3/4}+d^{1/3}T^{2/3})$改进为$O(T^{3/4}+\\sqrt{dT})$，并在更大的延迟量$d=O(\\sqrt{T})$情况下与非延迟设置下的强化学习梯度下降算法相匹配。",
    "en_tdlr": "This paper proposes an improved algorithm for regret reduction in bandit convex optimization problem with delayed feedback. By accurately utilizing the delayed bandit feedback, the regret bound is improved from $O(T^{3/4}+d^{1/3}T^{2/3})$ to $O(T^{3/4}+\\sqrt{dT})$, matching the regret of non-delayed bandit gradient descent algorithm for larger delay $d=O(\\sqrt{T})$."
}