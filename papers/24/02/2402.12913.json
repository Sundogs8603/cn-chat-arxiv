{
    "title": "OPDAI at SemEval-2024 Task 6: Small LLMs can Accelerate Hallucination Detection with Weakly Supervised Data",
    "abstract": "arXiv:2402.12913v1 Announce Type: new  Abstract: This paper mainly describes a unified system for hallucination detection of LLMs, which wins the second prize in the model-agnostic track of the SemEval-2024 Task 6, and also achieves considerable results in the model-aware track. This task aims to detect hallucination with LLMs for three different text-generation tasks without labeled training data. We utilize prompt engineering and few-shot learning to verify the performance of different LLMs on the validation data. Then we select the LLMs with better performance to generate high-quality weakly supervised training data, which not only satisfies the consistency of different LLMs, but also satisfies the consistency of the optimal LLM with different sampling parameters. Furthermore, we finetune different LLMs by using the constructed training data, and finding that a relatively small LLM can achieve a competitive level of performance in hallucination detection, when compared to the large ",
    "link": "https://arxiv.org/abs/2402.12913",
    "context": "Title: OPDAI at SemEval-2024 Task 6: Small LLMs can Accelerate Hallucination Detection with Weakly Supervised Data\nAbstract: arXiv:2402.12913v1 Announce Type: new  Abstract: This paper mainly describes a unified system for hallucination detection of LLMs, which wins the second prize in the model-agnostic track of the SemEval-2024 Task 6, and also achieves considerable results in the model-aware track. This task aims to detect hallucination with LLMs for three different text-generation tasks without labeled training data. We utilize prompt engineering and few-shot learning to verify the performance of different LLMs on the validation data. Then we select the LLMs with better performance to generate high-quality weakly supervised training data, which not only satisfies the consistency of different LLMs, but also satisfies the consistency of the optimal LLM with different sampling parameters. Furthermore, we finetune different LLMs by using the constructed training data, and finding that a relatively small LLM can achieve a competitive level of performance in hallucination detection, when compared to the large ",
    "path": "papers/24/02/2402.12913.json",
    "total_tokens": 838,
    "translated_title": "OPDAI在SemEval-2024任务6中：小型LLMs可以加速幻觉检测与弱监督数据",
    "translated_abstract": "这篇论文主要描述了一种统一的系统，用于LLMs的幻觉检测，在SemEval-2024任务6的模型无关跟踪中赢得第二名，并且在模型感知跟踪中取得了可观的结果。该任务旨在使用LLMs检测三种不同的文本生成任务中的幻觉，而无需标记训练数据。我们利用提示工程和少样本学习来验证不同LLMs在验证数据上的性能。然后，我们选择性能更好的LLMs生成高质量的弱监督训练数据，这不仅满足了不同LLMs的一致性，还满足了最佳LLM与不同抽样参数的一致性。此外，我们通过使用构建的训练数据微调不同的LLMs，并发现相对较小的LLM在与大型LLM相比，在幻觉检测方面可以达到具有竞争力的性能水平。",
    "tldr": "相对较小的LLMs可以与大型LLM相比，在幻觉检测方面达到竞争性的性能水平"
}