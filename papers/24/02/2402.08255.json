{
    "title": "Distal Interference: Exploring the Limits of Model-Based Continual Learning",
    "abstract": "Continual learning is the sequential learning of different tasks by a machine learning model. Continual learning is known to be hindered by catastrophic interference or forgetting, i.e. rapid unlearning of earlier learned tasks when new tasks are learned. Despite their practical success, artificial neural networks (ANNs) are prone to catastrophic interference. This study analyses how gradient descent and overlapping representations between distant input points lead to distal interference and catastrophic interference. Distal interference refers to the phenomenon where training a model on a subset of the domain leads to non-local changes on other subsets of the domain. This study shows that uniformly trainable models without distal interference must be exponentially large. A novel antisymmetric bounded exponential layer B-spline ANN architecture named ABEL-Spline is proposed that can approximate any continuous function, is uniformly trainable, has polynomial computational complexity, an",
    "link": "https://arxiv.org/abs/2402.08255",
    "context": "Title: Distal Interference: Exploring the Limits of Model-Based Continual Learning\nAbstract: Continual learning is the sequential learning of different tasks by a machine learning model. Continual learning is known to be hindered by catastrophic interference or forgetting, i.e. rapid unlearning of earlier learned tasks when new tasks are learned. Despite their practical success, artificial neural networks (ANNs) are prone to catastrophic interference. This study analyses how gradient descent and overlapping representations between distant input points lead to distal interference and catastrophic interference. Distal interference refers to the phenomenon where training a model on a subset of the domain leads to non-local changes on other subsets of the domain. This study shows that uniformly trainable models without distal interference must be exponentially large. A novel antisymmetric bounded exponential layer B-spline ANN architecture named ABEL-Spline is proposed that can approximate any continuous function, is uniformly trainable, has polynomial computational complexity, an",
    "path": "papers/24/02/2402.08255.json",
    "total_tokens": 915,
    "translated_title": "Distal Interference: 探索基于模型的持续学习的极限",
    "translated_abstract": "持续学习是机器学习模型按顺序学习不同任务的过程。持续学习被称为灾难性干扰或遗忘的阻碍，即在学习新任务时快速遗忘之前学习的任务。尽管人工神经网络（ANNs）在实践中取得了成功，但它们容易受到灾难性干扰的影响。该研究分析了梯度下降和远距离输入点之间重叠表示如何导致远距离干扰和灾难性干扰。远距离干扰是指在对域的子集进行模型训练时，对域的其他子集造成非局部变化的现象。该研究表明，没有远距离干扰的均匀可训练模型必须具有指数级的规模。提出了一种名为ABEL-Spline的新型反对称有界指数层B-spline ANN架构，该架构可以近似任何连续函数，具有均匀可训练性、多项式计算复杂度。",
    "tldr": "本研究探讨了持续学习中远距离干扰的极限问题，并提出了一种新型的可近似任何连续函数的反对称有界指数层B-spline ANN架构，用以解决灾难性干扰的挑战。"
}