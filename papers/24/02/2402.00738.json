{
    "title": "FM3Q: Factorized Multi-Agent MiniMax Q-Learning for Two-Team Zero-Sum Markov Game",
    "abstract": "Many real-world applications involve some agents that fall into two teams, with payoffs that are equal within the same team but of opposite sign across the opponent team. The so-called two-team zero-sum Markov games (2t0sMGs) can be resolved with reinforcement learning in recent years. However, existing methods are thus inefficient in light of insufficient consideration of intra-team credit assignment, data utilization and computational intractability. In this paper, we propose the individual-global-minimax (IGMM) principle to ensure the coherence between two-team minimax behaviors and the individual greedy behaviors through Q functions in 2t0sMGs. Based on it, we present a novel multi-agent reinforcement learning framework, Factorized Multi-Agent MiniMax Q-Learning (FM3Q), which can factorize the joint minimax Q function into individual ones and iteratively solve for the IGMM-satisfied minimax Q functions for 2t0sMGs. Moreover, an online learning algorithm with neural networks is prop",
    "link": "https://arxiv.org/abs/2402.00738",
    "context": "Title: FM3Q: Factorized Multi-Agent MiniMax Q-Learning for Two-Team Zero-Sum Markov Game\nAbstract: Many real-world applications involve some agents that fall into two teams, with payoffs that are equal within the same team but of opposite sign across the opponent team. The so-called two-team zero-sum Markov games (2t0sMGs) can be resolved with reinforcement learning in recent years. However, existing methods are thus inefficient in light of insufficient consideration of intra-team credit assignment, data utilization and computational intractability. In this paper, we propose the individual-global-minimax (IGMM) principle to ensure the coherence between two-team minimax behaviors and the individual greedy behaviors through Q functions in 2t0sMGs. Based on it, we present a novel multi-agent reinforcement learning framework, Factorized Multi-Agent MiniMax Q-Learning (FM3Q), which can factorize the joint minimax Q function into individual ones and iteratively solve for the IGMM-satisfied minimax Q functions for 2t0sMGs. Moreover, an online learning algorithm with neural networks is prop",
    "path": "papers/24/02/2402.00738.json",
    "total_tokens": 973,
    "translated_title": "FM3Q：分解的多智能体最小最大Q学习用于两个团队的零和马尔可夫游戏",
    "translated_abstract": "许多现实世界的应用涉及到一些智能体分为两个团队，同一团队内的回报相等，而对手团队之间的回报则相反。这种所谓的两个团队零和马尔可夫游戏（2t0sMGs）可以在最近几年通过强化学习来解决。然而，现有方法在考虑团队内信用分配、数据利用和计算复杂性不足方面效率低下。在本文中，我们提出了个体全局最小最大（IGMM）原则，通过Q函数在2t0sMGs中确保两个团队最小最大行为和个体贪婪行为之间的一致性。基于此，我们提出了一种新型的多智能体强化学习框架，分解的多智能体最小最大Q学习（FM3Q），它可以将联合最小最大Q函数分解为个体函数，并迭代解出满足IGMM的2t0sMGs最小最大Q函数。此外，我们提出了一种使用神经网络的在线学习算法。",
    "tldr": "本论文提出了一种名为FM3Q的新型多智能体强化学习框架，采用个体全局最小最大原则，通过分解联合最小最大Q函数为个体函数，并解决了团队内信用分配、数据利用和计算复杂性不足的问题。"
}