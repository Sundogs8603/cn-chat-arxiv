{
    "title": "Dissecting Human and LLM Preferences",
    "abstract": "arXiv:2402.11296v1 Announce Type: cross  Abstract: As a relative quality comparison of model responses, human and Large Language Model (LLM) preferences serve as common alignment goals in model fine-tuning and criteria in evaluation. Yet, these preferences merely reflect broad tendencies, resulting in less explainable and controllable models with potential safety risks. In this work, we dissect the preferences of human and 32 different LLMs to understand their quantitative composition, using annotations from real-world user-model conversations for a fine-grained, scenario-wise analysis. We find that humans are less sensitive to errors, favor responses that support their stances, and show clear dislike when models admit their limits. On the contrary, advanced LLMs like GPT-4-Turbo emphasize correctness, clarity, and harmlessness more. Additionally, LLMs of similar sizes tend to exhibit similar preferences, regardless of their training methods, and fine-tuning for alignment does not sign",
    "link": "https://arxiv.org/abs/2402.11296",
    "context": "Title: Dissecting Human and LLM Preferences\nAbstract: arXiv:2402.11296v1 Announce Type: cross  Abstract: As a relative quality comparison of model responses, human and Large Language Model (LLM) preferences serve as common alignment goals in model fine-tuning and criteria in evaluation. Yet, these preferences merely reflect broad tendencies, resulting in less explainable and controllable models with potential safety risks. In this work, we dissect the preferences of human and 32 different LLMs to understand their quantitative composition, using annotations from real-world user-model conversations for a fine-grained, scenario-wise analysis. We find that humans are less sensitive to errors, favor responses that support their stances, and show clear dislike when models admit their limits. On the contrary, advanced LLMs like GPT-4-Turbo emphasize correctness, clarity, and harmlessness more. Additionally, LLMs of similar sizes tend to exhibit similar preferences, regardless of their training methods, and fine-tuning for alignment does not sign",
    "path": "papers/24/02/2402.11296.json",
    "total_tokens": 908,
    "translated_title": "分析人类和大型语言模型（LLM）的偏好",
    "translated_abstract": "作为模型响应的相对质量比较，人类和大型语言模型（LLM）的偏好在模型微调中作为共同的对齐目标和评估标准。然而，这些偏好仅反映了广泛趋势，导致了模型的可解释性和可控性较差，可能存在潜在的安全风险。本研究剖析了人类和32种不同LLM的偏好，以了解它们的定量组成，利用来自真实用户-模型对话的注释进行细粒度、场景化分析。我们发现人类对错误不太敏感，偏好支持其立场的回应，并在模型承认其局限性时表现出明显的不喜欢。相反，像GPT-4-Turbo这样的先进LLM更加强调正确性、清晰性和无害性。此外，大小相似的LLM倾向于展现出类似的偏好，无论它们的训练方法如何，并且为了对齐而进行的微调并不会导致显著的改变。",
    "tldr": "本研究分析了人类和32种不同LLM的偏好，发现人类不太在意错误，偏好支持立场的回应，而先进的LLM更注重正确性、清晰性和无害性。",
    "en_tdlr": "This study analyzes the preferences of humans and 32 different LLMs, revealing that humans are less sensitive to errors, favor responses that support their stances, while advanced LLMs prioritize correctness, clarity, and harmlessness."
}