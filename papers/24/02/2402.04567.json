{
    "title": "OIL-AD: An Anomaly Detection Framework for Sequential Decision Sequences",
    "abstract": "Anomaly detection in decision-making sequences is a challenging problem due to the complexity of normality representation learning and the sequential nature of the task. Most existing methods based on Reinforcement Learning (RL) are difficult to implement in the real world due to unrealistic assumptions, such as having access to environment dynamics, reward signals, and online interactions with the environment. To address these limitations, we propose an unsupervised method named Offline Imitation Learning based Anomaly Detection (OIL-AD), which detects anomalies in decision-making sequences using two extracted behaviour features: action optimality and sequential association. Our offline learning model is an adaptation of behavioural cloning with a transformer policy network, where we modify the training process to learn a Q function and a state value function from normal trajectories. We propose that the Q function and the state value function can provide sufficient information about ",
    "link": "https://arxiv.org/abs/2402.04567",
    "context": "Title: OIL-AD: An Anomaly Detection Framework for Sequential Decision Sequences\nAbstract: Anomaly detection in decision-making sequences is a challenging problem due to the complexity of normality representation learning and the sequential nature of the task. Most existing methods based on Reinforcement Learning (RL) are difficult to implement in the real world due to unrealistic assumptions, such as having access to environment dynamics, reward signals, and online interactions with the environment. To address these limitations, we propose an unsupervised method named Offline Imitation Learning based Anomaly Detection (OIL-AD), which detects anomalies in decision-making sequences using two extracted behaviour features: action optimality and sequential association. Our offline learning model is an adaptation of behavioural cloning with a transformer policy network, where we modify the training process to learn a Q function and a state value function from normal trajectories. We propose that the Q function and the state value function can provide sufficient information about ",
    "path": "papers/24/02/2402.04567.json",
    "total_tokens": 860,
    "translated_title": "OIL-AD: 一种用于顺序决策序列的异常检测框架",
    "translated_abstract": "决策序列中的异常检测是一个具有挑战性的问题，因为正常性表示学习的复杂性和任务的顺序性质。大部分基于强化学习（RL）的现有方法由于对环境动态、奖励信号和与环境的在线交互等不切实际的假设，在现实世界中难以实施。为了解决这些限制，我们提出了一种名为离线模仿学习异常检测（OIL-AD）的无监督方法，它使用两个提取的行为特征（动作优化和顺序关联）来检测决策序列中的异常。我们的离线学习模型是基于变压器策略网络的行为克隆的适应，我们修改了训练过程，从正常轨迹中学习Q函数和状态值函数。我们认为，Q函数和状态值函数可以提供关于异常的足够信息。",
    "tldr": "本论文提出了一种用于顺序决策序列的异常检测框架，通过提取行为特征来检测异常。这种方法克服了强化学习方法在真实世界中应用困难的问题，并提供了关于异常的足够信息。",
    "en_tdlr": "This paper presents an anomaly detection framework for sequential decision sequences, which detects anomalies by extracting behavior features. This method overcomes the challenges of applying reinforcement learning methods in the real world and provides sufficient information about anomalies."
}