{
    "title": "A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task",
    "abstract": "arXiv:2402.11917v1 Announce Type: new  Abstract: Transformers demonstrate impressive performance on a range of reasoning benchmarks. To evaluate the degree to which these abilities are a result of actual reasoning, existing work has focused on developing sophisticated benchmarks for behavioral studies. However, these studies do not provide insights into the internal mechanisms driving the observed capabilities. To improve our understanding of the internal mechanisms of transformers, we present a comprehensive mechanistic analysis of a transformer trained on a synthetic reasoning task. We identify a set of interpretable mechanisms the model uses to solve the task, and validate our findings using correlational and causal evidence. Our results suggest that it implements a depth-bounded recurrent mechanisms that operates in parallel and stores intermediate results in selected token positions. We anticipate that the motifs we identified in our synthetic setting can provide valuable insights",
    "link": "https://arxiv.org/abs/2402.11917",
    "context": "Title: A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task\nAbstract: arXiv:2402.11917v1 Announce Type: new  Abstract: Transformers demonstrate impressive performance on a range of reasoning benchmarks. To evaluate the degree to which these abilities are a result of actual reasoning, existing work has focused on developing sophisticated benchmarks for behavioral studies. However, these studies do not provide insights into the internal mechanisms driving the observed capabilities. To improve our understanding of the internal mechanisms of transformers, we present a comprehensive mechanistic analysis of a transformer trained on a synthetic reasoning task. We identify a set of interpretable mechanisms the model uses to solve the task, and validate our findings using correlational and causal evidence. Our results suggest that it implements a depth-bounded recurrent mechanisms that operates in parallel and stores intermediate results in selected token positions. We anticipate that the motifs we identified in our synthetic setting can provide valuable insights",
    "path": "papers/24/02/2402.11917.json",
    "total_tokens": 805,
    "translated_title": "在符号化多步推理任务上训练的Transformer的机理分析",
    "translated_abstract": "Transformer在一系列推理基准测试中展现出令人印象深刻的性能。为了评估这些能力在多大程度上是实际推理的结果，现有工作集中于开发复杂的行为研究基准。然而，这些研究并未提供关于驱动观察到的能力的内部机制的见解。为了改善我们对Transformer内部机制的理解，我们提出了对一个在合成推理任务上训练的Transformer进行全面的机理分析。我们确定了模型用来解决任务的一组可解释机制，并利用相关和因果证据验证了我们的发现。我们的结果表明，它实现了一个在并行运行的有界深度循环机制，并将中间结果存储在选定的令牌位置。我们预期我们在我们的合成环境中识别的主题可以提供有价值的见解",
    "tldr": "对在合成推理任务上训练的Transformer进行的机理分析揭示其实现了一个在并行运行的有界深度循环机制，并将中间结果存储在选定的令牌位置",
    "en_tdlr": "Mechanistic analysis of a transformer trained on a synthetic reasoning task reveals the implementation of a depth-bounded recurrent mechanism operating in parallel and storing intermediate results in selected token positions."
}