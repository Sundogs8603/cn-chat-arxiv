{
    "title": "Injecting Wiktionary to improve token-level contextual representations using contrastive learning",
    "abstract": "While static word embeddings are blind to context, for lexical semantics tasks context is rather too present in contextual word embeddings, vectors of same-meaning occurrences being too different (Ethayarajh, 2019). Fine-tuning pre-trained language models (PLMs) using contrastive learning was proposed, leveraging automatically self-augmented examples (Liu et al., 2021b). In this paper, we investigate how to inject a lexicon as an alternative source of supervision, using the English Wiktionary. We also test how dimensionality reduction impacts the resulting contextual word embeddings. We evaluate our approach on the Word-In-Context (WiC) task, in the unsupervised setting (not using the training set). We achieve new SoTA result on the original WiC test set. We also propose two new WiC test sets for which we show that our fine-tuning method achieves substantial improvements. We also observe improvements, although modest, for the semantic frame induction task. Although we experimented on E",
    "link": "https://arxiv.org/abs/2402.07817",
    "context": "Title: Injecting Wiktionary to improve token-level contextual representations using contrastive learning\nAbstract: While static word embeddings are blind to context, for lexical semantics tasks context is rather too present in contextual word embeddings, vectors of same-meaning occurrences being too different (Ethayarajh, 2019). Fine-tuning pre-trained language models (PLMs) using contrastive learning was proposed, leveraging automatically self-augmented examples (Liu et al., 2021b). In this paper, we investigate how to inject a lexicon as an alternative source of supervision, using the English Wiktionary. We also test how dimensionality reduction impacts the resulting contextual word embeddings. We evaluate our approach on the Word-In-Context (WiC) task, in the unsupervised setting (not using the training set). We achieve new SoTA result on the original WiC test set. We also propose two new WiC test sets for which we show that our fine-tuning method achieves substantial improvements. We also observe improvements, although modest, for the semantic frame induction task. Although we experimented on E",
    "path": "papers/24/02/2402.07817.json",
    "total_tokens": 871,
    "translated_title": "利用对比学习注入Wiktionary改善词级上下文表示的研究",
    "translated_abstract": "虽然静态词嵌入对上下文是无感的，但对于词汇语义任务来说，上下文在上下文词嵌入中过于明显，相同含义的词向量差异较大。本文提出使用对比学习来微调预训练语言模型（PLMs），利用自动自增示例。我们调查了如何将词典作为替代的监督资源注入，使用英文Wiktionary。我们还测试了降维对结果上下文词嵌入的影响。我们在无监督设置下（不使用训练集）在Word-In-Context（WiC）任务上评估了我们的方法。我们在原始WiC测试集上取得了新的SoTA结果。我们还提出了两个新的WiC测试集，其中我们展示了我们的微调方法取得了显著的改进。我们还观察到在语义框架识别任务中的改进，尽管效果较为温和。尽管我们在E进行了实验",
    "tldr": "本文研究了利用对比学习注入Wiktionary来改善词级上下文表示，并在Word-In-Context（WiC）任务上取得了新的最佳结果。"
}