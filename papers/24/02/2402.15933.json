{
    "title": "Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA",
    "abstract": "arXiv:2402.15933v1 Announce Type: cross  Abstract: In 3D Visual Question Answering (3D VQA), the scarcity of fully annotated data and limited visual content diversity hampers the generalization to novel scenes and 3D concepts (e.g., only around 800 scenes are utilized in ScanQA and SQA dataset). Current approaches resort supplement 3D reasoning with 2D information. However, these methods face challenges: either they use top-down 2D views that introduce overly complex and sometimes question-irrelevant visual clues, or they rely on globally aggregated scene/image-level representations from 2D VLMs, losing the fine-grained vision-language correlations. To overcome these limitations, our approach utilizes question-conditional 2D view selection procedure, pinpointing semantically relevant 2D inputs for crucial visual clues. We then integrate this 2D knowledge into the 3D-VQA system via a two-branch Transformer structure. This structure, featuring a Twin-Transformer design, compactly combine",
    "link": "https://arxiv.org/abs/2402.15933",
    "context": "Title: Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA\nAbstract: arXiv:2402.15933v1 Announce Type: cross  Abstract: In 3D Visual Question Answering (3D VQA), the scarcity of fully annotated data and limited visual content diversity hampers the generalization to novel scenes and 3D concepts (e.g., only around 800 scenes are utilized in ScanQA and SQA dataset). Current approaches resort supplement 3D reasoning with 2D information. However, these methods face challenges: either they use top-down 2D views that introduce overly complex and sometimes question-irrelevant visual clues, or they rely on globally aggregated scene/image-level representations from 2D VLMs, losing the fine-grained vision-language correlations. To overcome these limitations, our approach utilizes question-conditional 2D view selection procedure, pinpointing semantically relevant 2D inputs for crucial visual clues. We then integrate this 2D knowledge into the 3D-VQA system via a two-branch Transformer structure. This structure, featuring a Twin-Transformer design, compactly combine",
    "path": "papers/24/02/2402.15933.json",
    "total_tokens": 943,
    "translated_title": "跨越2D和3D视觉问答之间的鸿沟：一种用于3D VQA的融合方法",
    "translated_abstract": "在3D视觉问答（3D VQA）中，充分注释数据的稀缺性和有限的视觉内容多样性阻碍了对新颖场景和3D概念的泛化（如ScanQA和SQA数据集仅利用了约800个场景）。目前的方法通过补充2D信息来辅助3D推理。然而，这些方法面临挑战：它们要么使用引入过于复杂且有时与问题无关的视觉线索的自上而下的2D视图，要么依靠来自2D VLM的全局聚合场景/图像级表示，从而丢失了细粒度的视觉语言相关性。为了克服这些局限性，我们的方法利用了问题条件下的2D视图选择过程，准确地指出了关键视觉线索的语义相关2D输入。然后，我们通过双分支Transformer结构将这种2D知识整合到3D-VQA系统中。这种结构采用了双Transformer设计，紧凑地结合",
    "tldr": "通过问题条件的2D视图选择过程和双分支Transformer结构，将2D知识整合到3D-VQA系统中，从而弥补了当前方法在3D视觉问答中遇到的挑战。"
}