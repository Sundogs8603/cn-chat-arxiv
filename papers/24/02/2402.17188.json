{
    "title": "PromptMM: Multi-Modal Knowledge Distillation for Recommendation with Prompt-Tuning",
    "abstract": "arXiv:2402.17188v1 Announce Type: new  Abstract: Multimedia online platforms (e.g., Amazon, TikTok) have greatly benefited from the incorporation of multimedia (e.g., visual, textual, and acoustic) content into their personal recommender systems. These modalities provide intuitive semantics that facilitate modality-aware user preference modeling. However, two key challenges in multi-modal recommenders remain unresolved: i) The introduction of multi-modal encoders with a large number of additional parameters causes overfitting, given high-dimensional multi-modal features provided by extractors (e.g., ViT, BERT). ii) Side information inevitably introduces inaccuracies and redundancies, which skew the modality-interaction dependency from reflecting true user preference. To tackle these problems, we propose to simplify and empower recommenders through Multi-modal Knowledge Distillation (PromptMM) with the prompt-tuning that enables adaptive quality distillation. Specifically, PromptMM cond",
    "link": "https://arxiv.org/abs/2402.17188",
    "context": "Title: PromptMM: Multi-Modal Knowledge Distillation for Recommendation with Prompt-Tuning\nAbstract: arXiv:2402.17188v1 Announce Type: new  Abstract: Multimedia online platforms (e.g., Amazon, TikTok) have greatly benefited from the incorporation of multimedia (e.g., visual, textual, and acoustic) content into their personal recommender systems. These modalities provide intuitive semantics that facilitate modality-aware user preference modeling. However, two key challenges in multi-modal recommenders remain unresolved: i) The introduction of multi-modal encoders with a large number of additional parameters causes overfitting, given high-dimensional multi-modal features provided by extractors (e.g., ViT, BERT). ii) Side information inevitably introduces inaccuracies and redundancies, which skew the modality-interaction dependency from reflecting true user preference. To tackle these problems, we propose to simplify and empower recommenders through Multi-modal Knowledge Distillation (PromptMM) with the prompt-tuning that enables adaptive quality distillation. Specifically, PromptMM cond",
    "path": "papers/24/02/2402.17188.json",
    "total_tokens": 830,
    "translated_title": "PromptMM：多模式知识蒸馏用于基于Prompt-Tuning的推荐",
    "translated_abstract": "多媒体在线平台（例如亚马逊、TikTok）通过将多媒体（例如视觉、文本和声学）内容纳入其个性化推荐系统中获益匪浅。这些模态提供直观语义，有助于进行模态感知的用户偏好建模。然而，多模式推荐器中存在两个关键挑战尚未解决：i）引入具有大量额外参数的多模式编码器会导致过拟合，考虑到提取器（例如ViT、BERT）提供的高维多模式特征。ii）辅助信息不可避免地引入不准确性和冗余，导致模态交互依赖偏离真实用户偏好。为了解决这些问题，我们提出通过Prompt-Tuning赋能、简化推荐器的PromptMM（多模式知识蒸馏），实现自适应质量蒸馏。",
    "tldr": "提出了一种通过Prompt-Tuning赋能的PromptMM多模式知识蒸馏方法，用于简化和增强推荐系统，实现自适应的质量蒸馏。",
    "en_tdlr": "Proposed a PromptMM multi-modal knowledge distillation method empowered by Prompt-Tuning for simplifying and enhancing recommendation systems with adaptive quality distillation."
}