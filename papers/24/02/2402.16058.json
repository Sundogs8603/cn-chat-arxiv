{
    "title": "Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression",
    "abstract": "arXiv:2402.16058v1 Announce Type: new  Abstract: Large language models (LLMs) require lengthy prompts as the input context to produce output aligned with user intentions, a process that incurs extra costs during inference. In this paper, we propose the Gist COnditioned deCOding (Gist-COCO) model, introducing a novel method for compressing prompts which also can assist the prompt interpretation and engineering. Gist-COCO employs an encoder-decoder based language model and then incorporates an additional encoder as a plugin module to compress prompts with inputs using gist tokens. It finetunes the compression plugin module and uses the representations of gist tokens to emulate the raw prompts in the vanilla language model. By verbalizing the representations of gist tokens into gist prompts, the compression ability of Gist-COCO can be generalized to different LLMs with high compression rates. Our experiments demonstrate that Gist-COCO outperforms previous prompt compression models in both",
    "link": "https://arxiv.org/abs/2402.16058",
    "context": "Title: Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression\nAbstract: arXiv:2402.16058v1 Announce Type: new  Abstract: Large language models (LLMs) require lengthy prompts as the input context to produce output aligned with user intentions, a process that incurs extra costs during inference. In this paper, we propose the Gist COnditioned deCOding (Gist-COCO) model, introducing a novel method for compressing prompts which also can assist the prompt interpretation and engineering. Gist-COCO employs an encoder-decoder based language model and then incorporates an additional encoder as a plugin module to compress prompts with inputs using gist tokens. It finetunes the compression plugin module and uses the representations of gist tokens to emulate the raw prompts in the vanilla language model. By verbalizing the representations of gist tokens into gist prompts, the compression ability of Gist-COCO can be generalized to different LLMs with high compression rates. Our experiments demonstrate that Gist-COCO outperforms previous prompt compression models in both",
    "path": "papers/24/02/2402.16058.json",
    "total_tokens": 864,
    "translated_title": "用更少的文字说更多：通过要点压缩理解提示学习行为",
    "translated_abstract": "大型语言模型（LLMs）需要长度较长的提示作为输入上下文，以产生与用户意图一致的输出，这个过程在推理期间会产生额外的成本。本文提出了Gist COnditioned deCOding（Gist-COCO）模型，引入了一种新颖的压缩提示方法，同时还可以协助提示的解释和工程。Gist-COCO采用基于编码器-解码器的语言模型，然后将额外的编码器作为插件模块整合进来，使用要点标记来压缩提示。它微调压缩插件模块，并使用要点标记的表示来模拟原始提示在基本语言模型中的情况。通过将要点标记的表示口头化为要点提示，Gist-COCO的压缩能力可以推广到不同的LLMs，并且具有较高的压缩率。我们的实验表明，Gist-COCO在两个方面均优于以前的提示压缩模型",
    "tldr": "提出了一种名为Gist-COCO的模型，通过要点压缩来帮助提示解释和工程，可以达到较高的压缩率，并且在实验中表现出优异性能。",
    "en_tdlr": "Introducing the Gist-COCO model for prompt compression, aiding in prompt interpretation and engineering with high compression rates, outperforming previous models in experiments."
}