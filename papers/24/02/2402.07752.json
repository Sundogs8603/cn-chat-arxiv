{
    "title": "Mixed Q-Functionals: Advancing Value-Based Methods in Cooperative MARL with Continuous Action Domains",
    "abstract": "Tackling multi-agent learning problems efficiently is a challenging task in continuous action domains. While value-based algorithms excel in sample efficiency when applied to discrete action domains, they are usually inefficient when dealing with continuous actions. Policy-based algorithms, on the other hand, attempt to address this challenge by leveraging critic networks for guiding the learning process and stabilizing the gradient estimation. The limitations in the estimation of true return and falling into local optima in these methods result in inefficient and often sub-optimal policies. In this paper, we diverge from the trend of further enhancing critic networks, and focus on improving the effectiveness of value-based methods in multi-agent continuous domains by concurrently evaluating numerous actions. We propose a novel multi-agent value-based algorithm, Mixed Q-Functionals (MQF), inspired from the idea of Q-Functionals, that enables agents to transform their states into basis ",
    "link": "https://arxiv.org/abs/2402.07752",
    "context": "Title: Mixed Q-Functionals: Advancing Value-Based Methods in Cooperative MARL with Continuous Action Domains\nAbstract: Tackling multi-agent learning problems efficiently is a challenging task in continuous action domains. While value-based algorithms excel in sample efficiency when applied to discrete action domains, they are usually inefficient when dealing with continuous actions. Policy-based algorithms, on the other hand, attempt to address this challenge by leveraging critic networks for guiding the learning process and stabilizing the gradient estimation. The limitations in the estimation of true return and falling into local optima in these methods result in inefficient and often sub-optimal policies. In this paper, we diverge from the trend of further enhancing critic networks, and focus on improving the effectiveness of value-based methods in multi-agent continuous domains by concurrently evaluating numerous actions. We propose a novel multi-agent value-based algorithm, Mixed Q-Functionals (MQF), inspired from the idea of Q-Functionals, that enables agents to transform their states into basis ",
    "path": "papers/24/02/2402.07752.json",
    "total_tokens": 910,
    "translated_title": "混合Q函数：推动连续动作领域合作MARL中基于值的方法的进展",
    "translated_abstract": "在连续动作领域中高效解决多智能体学习问题是一项具有挑战性的任务。虽然基于值的算法在应用于离散动作领域时具有样本效率优势，但在处理连续动作时通常效率低下。另一方面，策略型算法通过利用评论家网络来引导学习过程和稳定梯度估计来解决这一挑战。但是，这些方法在估计真实回报和陷入局部最优解方面存在限制，导致策略效率低下且通常次优。在本文中，我们远离进一步增强评论家网络的趋势，专注于通过同时评估多个动作来改进基于值的方法在多智能体连续动作领域中的有效性。我们提出了一种新颖的多智能体基于值的算法，混合Q函数（MQF），灵感来自于Q函数，使智能体能够将其状态转化为基底。",
    "tldr": "本文提出了一种新颖的多智能体基于值的算法，混合Q函数（MQF），提高了基于值的方法在多智能体连续动作领域中的效果，通过同时评估多个动作来解决真实回报估计和局部最优解问题。"
}