{
    "title": "Compound Returns Reduce Variance in Reinforcement Learning",
    "abstract": "Multistep returns, such as $n$-step returns and $\\lambda$-returns, are commonly used to improve the sample efficiency of reinforcement learning (RL) methods. The variance of the multistep returns becomes the limiting factor in their length; looking too far into the future increases variance and reverses the benefits of multistep learning. In our work, we demonstrate the ability of compound returns -- weighted averages of $n$-step returns -- to reduce variance. We prove for the first time that any compound return with the same contraction modulus as a given $n$-step return has strictly lower variance. We additionally prove that this variance-reduction property improves the finite-sample complexity of temporal-difference learning under linear function approximation. Because general compound returns can be expensive to implement, we introduce two-bootstrap returns which reduce variance while remaining efficient, even when using minibatched experience replay. We conduct experiments showing",
    "link": "https://arxiv.org/abs/2402.03903",
    "context": "Title: Compound Returns Reduce Variance in Reinforcement Learning\nAbstract: Multistep returns, such as $n$-step returns and $\\lambda$-returns, are commonly used to improve the sample efficiency of reinforcement learning (RL) methods. The variance of the multistep returns becomes the limiting factor in their length; looking too far into the future increases variance and reverses the benefits of multistep learning. In our work, we demonstrate the ability of compound returns -- weighted averages of $n$-step returns -- to reduce variance. We prove for the first time that any compound return with the same contraction modulus as a given $n$-step return has strictly lower variance. We additionally prove that this variance-reduction property improves the finite-sample complexity of temporal-difference learning under linear function approximation. Because general compound returns can be expensive to implement, we introduce two-bootstrap returns which reduce variance while remaining efficient, even when using minibatched experience replay. We conduct experiments showing",
    "path": "papers/24/02/2402.03903.json",
    "total_tokens": 807,
    "translated_title": "复合回报降低强化学习中的方差",
    "translated_abstract": "多步回报，例如$n$步回报和$\\lambda$回报，通常用于提高强化学习方法的样本效率。多步回报的方差成为其长度的限制因素，过度远望未来会增加方差并逆转多步学习的好处。在我们的工作中，我们展示了复合回报（$n$步回报的加权平均）降低方差的能力。我们首次证明了任何与给定$n$步回报具有相同收缩模数的复合回报的方差严格较低。我们还证明了这种降低方差的特性改善了线性函数逼近下时序差分学习的有限样本复杂性。由于一般复合回报的实施可能代价高昂，我们引入了两个自助回报，它们在保持高效性的同时降低了方差，即使在使用小批量经验回放时也是如此。我们进行了实验，显示……",
    "tldr": "复合回报是一种新的强化学习方法，在降低方差和提高样本效率方面具有重要的贡献和创新。",
    "en_tdlr": "Compound returns are a new method in reinforcement learning that make important contributions and innovations in reducing variance and improving sample efficiency."
}