{
    "title": "Higher Layers Need More LoRA Experts",
    "abstract": "Parameter-efficient tuning (PEFT) techniques like low-rank adaptation (LoRA) offer training efficiency on Large Language Models, but their impact on model performance remains limited. Recent efforts integrate LoRA and Mixture-of-Experts (MoE) to improve the performance of PEFT methods. Despite promising results, research on improving the efficiency of LoRA with MoE is still in its early stages. Recent studies have shown that experts in the MoE architecture have different strengths and also exhibit some redundancy. Does this statement also apply to parameter-efficient MoE? In this paper, we introduce a novel parameter-efficient MoE method, \\textit{\\textbf{M}oE-L\\textbf{o}RA with \\textbf{L}ayer-wise Expert \\textbf{A}llocation (MoLA)} for Transformer-based models, where each model layer has the flexibility to employ a varying number of LoRA experts. We investigate several architectures with varying layer-wise expert configurations. Experiments on six well-known NLP and commonsense QA benc",
    "link": "https://arxiv.org/abs/2402.08562",
    "context": "Title: Higher Layers Need More LoRA Experts\nAbstract: Parameter-efficient tuning (PEFT) techniques like low-rank adaptation (LoRA) offer training efficiency on Large Language Models, but their impact on model performance remains limited. Recent efforts integrate LoRA and Mixture-of-Experts (MoE) to improve the performance of PEFT methods. Despite promising results, research on improving the efficiency of LoRA with MoE is still in its early stages. Recent studies have shown that experts in the MoE architecture have different strengths and also exhibit some redundancy. Does this statement also apply to parameter-efficient MoE? In this paper, we introduce a novel parameter-efficient MoE method, \\textit{\\textbf{M}oE-L\\textbf{o}RA with \\textbf{L}ayer-wise Expert \\textbf{A}llocation (MoLA)} for Transformer-based models, where each model layer has the flexibility to employ a varying number of LoRA experts. We investigate several architectures with varying layer-wise expert configurations. Experiments on six well-known NLP and commonsense QA benc",
    "path": "papers/24/02/2402.08562.json",
    "total_tokens": 950,
    "translated_title": "Higher Layers Need More LoRA Experts",
    "translated_abstract": "参数高效调整（PEFT）技术，如低秩适应（LoRA），在大型语言模型上提供了训练效率，但对模型性能的影响仍有限。最近的努力整合了LoRA和专家混合（MoE），以提高PEFT方法的性能。尽管有了有希望的结果，但改进带有MoE的LoRA的效率的研究仍处于初级阶段。最近的研究表明，MoE体系结构中的专家具有不同的优势，并且还存在一些冗余。这个论断是否也适用于参数高效的MoE？在本文中，我们介绍了一种新颖的参数高效的MoE方法，称为MoLA（\\textit{\\textbf{M}oE-L\\textbf{o}RA with \\textbf{L}ayer-wise Expert \\textbf{A}llocation）），用于基于Transformer的模型，其中每个模型层可以灵活地使用不同数量的LoRA专家。我们研究了几种具有不同层级专家配置的体系结构。对六个知名的自然语言处理和常识问答基准进行了实验。",
    "tldr": "这篇论文提出了一种新颖的参数高效的MoE方法（MoLA），用于Transformer-based模型，其中每个模型层可以灵活地使用不同数量的LoRA专家。通过在多个基准数据集上进行实验，研究结果表明高层需要更多的LoRA专家来提高模型性能。",
    "en_tdlr": "This paper introduces a novel parameter-efficient MoE method (MoLA) for Transformer-based models, where each model layer can flexibly utilize a varying number of LoRA experts. Experimental results on multiple benchmark datasets show that higher layers require more LoRA experts to improve model performance."
}