{
    "title": "Stumbling Blocks: Stress Testing the Robustness of Machine-Generated Text Detectors Under Attacks",
    "abstract": "arXiv:2402.11638v1 Announce Type: new  Abstract: The widespread use of large language models (LLMs) is increasing the demand for methods that detect machine-generated text to prevent misuse. The goal of our study is to stress test the detectors' robustness to malicious attacks under realistic scenarios. We comprehensively study the robustness of popular machine-generated text detectors under attacks from diverse categories: editing, paraphrasing, prompting, and co-generating. Our attacks assume limited access to the generator LLMs, and we compare the performance of detectors on different attacks under different budget levels. Our experiments reveal that almost none of the existing detectors remain robust under all the attacks, and all detectors exhibit different loopholes. Averaging all detectors, the performance drops by 35% across all attacks. Further, we investigate the reasons behind these defects and propose initial out-of-the-box patches to improve robustness.",
    "link": "https://arxiv.org/abs/2402.11638",
    "context": "Title: Stumbling Blocks: Stress Testing the Robustness of Machine-Generated Text Detectors Under Attacks\nAbstract: arXiv:2402.11638v1 Announce Type: new  Abstract: The widespread use of large language models (LLMs) is increasing the demand for methods that detect machine-generated text to prevent misuse. The goal of our study is to stress test the detectors' robustness to malicious attacks under realistic scenarios. We comprehensively study the robustness of popular machine-generated text detectors under attacks from diverse categories: editing, paraphrasing, prompting, and co-generating. Our attacks assume limited access to the generator LLMs, and we compare the performance of detectors on different attacks under different budget levels. Our experiments reveal that almost none of the existing detectors remain robust under all the attacks, and all detectors exhibit different loopholes. Averaging all detectors, the performance drops by 35% across all attacks. Further, we investigate the reasons behind these defects and propose initial out-of-the-box patches to improve robustness.",
    "path": "papers/24/02/2402.11638.json",
    "total_tokens": 906,
    "translated_title": "绊脚石：在攻击下对机器生成文本检测器的鲁棒性进行压力测试",
    "translated_abstract": "大型语言模型（LLMs）的广泛使用增加了检测机器生成文本以防止滥用的需求。我们研究的目标是在现实场景下对检测器对恶意攻击的鲁棒性进行压力测试。我们全面研究了流行的机器生成文本检测器在不同攻击类型下的鲁棒性：编辑、改写、提示和共生成。我们的攻击假设对生成LLMs的访问受限，并比较了不同预算水平下检测器在不同攻击下的性能。我们的实验发现几乎没有现有检测器在所有攻击下保持稳健，所有检测器都展示出不同的漏洞。平均所有检测器，在所有攻击下性能下降了35%。此外，我们调查了这些缺陷背后的原因，并提出了初步的即插即用补丁来提高鲁棒性。",
    "tldr": "本研究压力测试了机器生成文本检测器在恶意攻击下的鲁棒性，实验证明几乎所有现有检测器在各种攻击下都表现不稳定，平均性能下降35%，并提出了改善鲁棒性的初步解决方案。",
    "en_tdlr": "This study stress tests the robustness of machine-generated text detectors under malicious attacks, showing that almost all existing detectors exhibit instability under various attacks with an average performance drop of 35%, and proposes initial solutions to improve robustness."
}