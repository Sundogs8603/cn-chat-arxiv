{
    "title": "Understanding News Thumbnail Representativeness by Counterfactual Text-Guided Contrastive Language-Image Pretraining",
    "abstract": "arXiv:2402.11159v1 Announce Type: new  Abstract: This paper delves into the critical challenge of understanding the representativeness of news thumbnail images, which often serve as the first visual engagement for readers when an article is disseminated on social media. We focus on whether a news image represents the main subject discussed in the news text. To serve the challenge, we introduce \\textsc{NewsTT}, a manually annotated dataset of news thumbnail image and text pairs. We found that pretrained vision and language models, such as CLIP and BLIP-2, struggle with this task. Since news subjects frequently involve named entities or proper nouns, a pretrained model could not have the ability to match its visual and textual appearances. To fill the gap, we propose CFT-CLIP, a counterfactual text-guided contrastive language-image pretraining framework. We hypothesize that learning to contrast news text with its counterfactual, of which named entities are replaced, can enhance the cross",
    "link": "https://arxiv.org/abs/2402.11159",
    "context": "Title: Understanding News Thumbnail Representativeness by Counterfactual Text-Guided Contrastive Language-Image Pretraining\nAbstract: arXiv:2402.11159v1 Announce Type: new  Abstract: This paper delves into the critical challenge of understanding the representativeness of news thumbnail images, which often serve as the first visual engagement for readers when an article is disseminated on social media. We focus on whether a news image represents the main subject discussed in the news text. To serve the challenge, we introduce \\textsc{NewsTT}, a manually annotated dataset of news thumbnail image and text pairs. We found that pretrained vision and language models, such as CLIP and BLIP-2, struggle with this task. Since news subjects frequently involve named entities or proper nouns, a pretrained model could not have the ability to match its visual and textual appearances. To fill the gap, we propose CFT-CLIP, a counterfactual text-guided contrastive language-image pretraining framework. We hypothesize that learning to contrast news text with its counterfactual, of which named entities are replaced, can enhance the cross",
    "path": "papers/24/02/2402.11159.json",
    "total_tokens": 866,
    "translated_title": "通过反事实文本引导的对比语言-图像预训练来理解新闻缩略图的代表性",
    "translated_abstract": "本文深入探讨了理解新闻缩略图的代表性这一关键挑战，这些缩略图通常在文章在社交媒体上传播时作为读者的第一个视觉参与。我们关注新闻图像是否代表新闻文本中讨论的主要主题。为了应对这一挑战，我们引入了一个手动注释的新闻缩略图和文本配对数据集\\textsc{NewsTT}。我们发现，例如CLIP和BLIP-2这样的预训练视觉和语言模型在这一任务上表现不佳。由于新闻主题经常涉及命名实体或专有名词，预训练模型缺乏匹配其视觉和文本外观的能力。为了填补这一空白，我们提出了CFT-CLIP，一个反事实文本引导的对比语言-图像预训练框架。",
    "tldr": "提出了一种反事实文本引导的对比语言-图像预训练框架CFT-CLIP，用于增强新闻文本和缩略图之间的对比学习。",
    "en_tdlr": "Introduced a counterfactual text-guided contrastive language-image pretraining framework CFT-CLIP to enhance the contrastive learning between news text and thumbnail images."
}