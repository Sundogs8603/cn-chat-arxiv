{
    "title": "Switch EMA: A Free Lunch for Better Flatness and Sharpness",
    "abstract": "arXiv:2402.09240v1 Announce Type: new Abstract: Exponential Moving Average (EMA) is a widely used weight averaging (WA) regularization to learn flat optima for better generalizations without extra cost in deep neural network (DNN) optimization. Despite achieving better flatness, existing WA methods might fall into worse final performances or require extra test-time computations. This work unveils the full potential of EMA with a single line of modification, i.e., switching the EMA parameters to the original model after each epoch, dubbed as Switch EMA (SEMA). From both theoretical and empirical aspects, we demonstrate that SEMA can help DNNs to reach generalization optima that better trade-off between flatness and sharpness. To verify the effectiveness of SEMA, we conduct comparison experiments with discriminative, generative, and regression tasks on vision and language datasets, including image classification, self-supervised learning, object detection and segmentation, image generati",
    "link": "https://arxiv.org/abs/2402.09240",
    "context": "Title: Switch EMA: A Free Lunch for Better Flatness and Sharpness\nAbstract: arXiv:2402.09240v1 Announce Type: new Abstract: Exponential Moving Average (EMA) is a widely used weight averaging (WA) regularization to learn flat optima for better generalizations without extra cost in deep neural network (DNN) optimization. Despite achieving better flatness, existing WA methods might fall into worse final performances or require extra test-time computations. This work unveils the full potential of EMA with a single line of modification, i.e., switching the EMA parameters to the original model after each epoch, dubbed as Switch EMA (SEMA). From both theoretical and empirical aspects, we demonstrate that SEMA can help DNNs to reach generalization optima that better trade-off between flatness and sharpness. To verify the effectiveness of SEMA, we conduct comparison experiments with discriminative, generative, and regression tasks on vision and language datasets, including image classification, self-supervised learning, object detection and segmentation, image generati",
    "path": "papers/24/02/2402.09240.json",
    "total_tokens": 925,
    "translated_title": "Switch EMA: 更好的平坦性和锐度的免费午餐",
    "translated_abstract": "指数移动平均（EMA）是一种广泛使用的权重平均正则化方法，在深度神经网络（DNN）优化中学习平坦的最优解，以获得更好的概括能力而不增加额外的成本。尽管可以获得更好的平坦性，但现有的平均方法可能陷入更差的最终性能或需要额外的测试时间计算。本文通过一个简单的修改，即在每个周期后将EMA参数切换回原始模型，揭示了EMA的充分潜力，被称为Switch EMA（SEMA）。从理论和实证方面都证明了SEMA可以帮助DNNs达到更好的平坦性和锐度之间平衡的概括最优解。为了验证SEMA的效果，我们在视觉和语言数据集上进行了辨别性、生成性和回归任务的比较实验，包括图像分类、自监督学习、目标检测和分割、图像生成等。",
    "tldr": "本研究提出了一种称为Switch EMA（SEMA）的方法，通过简单的修改指数移动平均（EMA）参数，可以帮助深度神经网络（DNN）达到更好的平坦性和锐度的概括最优解。通过在多个任务和数据集上进行实验证明了SEMA的有效性。",
    "en_tdlr": "This study proposes a method called Switch EMA (SEMA), which achieves better trade-off between flatness and sharpness in deep neural network (DNN) optimization by simply modifying the parameters of exponential moving average (EMA). The effectiveness of SEMA is demonstrated through experiments on various tasks and datasets."
}