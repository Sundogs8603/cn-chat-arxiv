{
    "title": "Your Large Language Model is Secretly a Fairness Proponent and You Should Prompt it Like One",
    "abstract": "arXiv:2402.12150v1 Announce Type: cross  Abstract: The widespread adoption of large language models (LLMs) underscores the urgent need to ensure their fairness. However, LLMs frequently present dominant viewpoints while ignoring alternative perspectives from minority parties, resulting in potential biases. We hypothesize that these fairness-violating behaviors occur because LLMs express their viewpoints using a human personality that represents the majority of training data. In response to this, we validate that prompting LLMs with specific roles can allow LLMs to express diverse viewpoints. Building on this insight and observation, we develop FairThinking, a pipeline designed to automatically generate roles that enable LLMs to articulate diverse perspectives for fair expressions. To evaluate FairThinking, we create a dataset with a thousand items covering three fairness-related topics and conduct experiments on GPT-3.5, GPT-4, Llama2, and Mistral to demonstrate its superior performanc",
    "link": "https://arxiv.org/abs/2402.12150",
    "context": "Title: Your Large Language Model is Secretly a Fairness Proponent and You Should Prompt it Like One\nAbstract: arXiv:2402.12150v1 Announce Type: cross  Abstract: The widespread adoption of large language models (LLMs) underscores the urgent need to ensure their fairness. However, LLMs frequently present dominant viewpoints while ignoring alternative perspectives from minority parties, resulting in potential biases. We hypothesize that these fairness-violating behaviors occur because LLMs express their viewpoints using a human personality that represents the majority of training data. In response to this, we validate that prompting LLMs with specific roles can allow LLMs to express diverse viewpoints. Building on this insight and observation, we develop FairThinking, a pipeline designed to automatically generate roles that enable LLMs to articulate diverse perspectives for fair expressions. To evaluate FairThinking, we create a dataset with a thousand items covering three fairness-related topics and conduct experiments on GPT-3.5, GPT-4, Llama2, and Mistral to demonstrate its superior performanc",
    "path": "papers/24/02/2402.12150.json",
    "total_tokens": 838,
    "translated_title": "您的大型语言模型暗中支持公平，您应该像对待一个公平者那样提示它",
    "translated_abstract": "大规模语言模型（LLMs）的广泛应用凸显了确保其公平性的迫切需要。然而，LLMs经常展示支配性观点，同时忽视来自少数派的替代观点，可能导致潜在偏见。我们假设这些违反公平的行为发生是因为LLMs使用代表训练数据大多数的人类个性来表达他们的观点。作为回应，我们验证提示LLMs使用特定角色可以使LLMs表达多样观点。基于这一洞察和观察，我们开发了FairThinking，这是一个旨在自动生成能让LLMs表达多样观点以实现公平表达的流水线。为了评估FairThinking，我们创建了一个包含三个与公平相关主题的一千个项目的数据集，并在GPT-3.5，GPT-4，Llama2和Mistral上进行实验，以展示其优越性能。",
    "tldr": "提出了一种通过提示大型语言模型（LLMs）具有特定角色以表达多样观点的方法，并开发了FairThinking流水线，以实现公平表达。",
    "en_tdlr": "Proposes a method to prompt large language models (LLMs) with specific roles to express diverse viewpoints, and develops the FairThinking pipeline to achieve fair expressions."
}