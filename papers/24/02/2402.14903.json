{
    "title": "Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs",
    "abstract": "arXiv:2402.14903v1 Announce Type: new  Abstract: Tokenization, the division of input text into input tokens, is an often overlooked aspect of the large language model (LLM) pipeline and could be the source of useful or harmful inductive biases. Historically, LLMs have relied on byte pair encoding, without care to specific input domains. With the increased use of LLMs for reasoning, various number-specific tokenization schemes have been adopted, with popular models like LLaMa and PaLM opting for single-digit tokenization while GPT-3.5 and GPT-4 have separate tokens for each 1-, 2-, and 3-digit numbers. In this work, we study the effect this choice has on numerical reasoning through the use of arithmetic tasks. We consider left-to-right and right-to-left tokenization for GPT-3.5 and -4, finding that right-to-left tokenization (enforced by comma separating numbers at inference time) leads to largely improved performance. Furthermore, we find that model errors when using standard left-to-r",
    "link": "https://arxiv.org/abs/2402.14903",
    "context": "Title: Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs\nAbstract: arXiv:2402.14903v1 Announce Type: new  Abstract: Tokenization, the division of input text into input tokens, is an often overlooked aspect of the large language model (LLM) pipeline and could be the source of useful or harmful inductive biases. Historically, LLMs have relied on byte pair encoding, without care to specific input domains. With the increased use of LLMs for reasoning, various number-specific tokenization schemes have been adopted, with popular models like LLaMa and PaLM opting for single-digit tokenization while GPT-3.5 and GPT-4 have separate tokens for each 1-, 2-, and 3-digit numbers. In this work, we study the effect this choice has on numerical reasoning through the use of arithmetic tasks. We consider left-to-right and right-to-left tokenization for GPT-3.5 and -4, finding that right-to-left tokenization (enforced by comma separating numbers at inference time) leads to largely improved performance. Furthermore, we find that model errors when using standard left-to-r",
    "path": "papers/24/02/2402.14903.json",
    "total_tokens": 899,
    "translated_title": "Tokenization计数：Tokenization对前沿LLMs中算术的影响",
    "translated_abstract": "Tokenization，即将输入文本分成输入token的过程，是大型语言模型（LLM）管道中经常被忽视的一个方面，可能是有用的或有害的归纳偏差的来源。在历史上，LLMs倾向于使用字节对编码，而没有考虑特定的输入领域。随着LLMs用于推理的增加，各种特定于数字的tokenization方案得到了采用，像LLaMa和PaLM这样的流行模型选择了单个数字tokenization，而GPT-3.5和GPT-4为每个1位、2位和3位数字都有单独的token。在这项工作中，我们通过算术任务研究这种选择对数值推理的影响。我们考虑了GPT-3.5和-4的从左到右和从右到左的tokenization，发现从右到左的tokenization（在推理时通过逗号分离数字）导致了大幅提高的性能。此外，我们发现在使用标准的从左到右tokenization时模型存在误差",
    "tldr": "本研究探讨了在大型语言模型中对输入文本进行tokenization对数值推理的影响，发现采用从右到左的tokenization方式可显著提高算术任务的性能表现。"
}