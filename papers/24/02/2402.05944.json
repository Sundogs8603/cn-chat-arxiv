{
    "title": "Todyformer: Towards Holistic Dynamic Graph Transformers with Structure-Aware Tokenization",
    "abstract": "Temporal Graph Neural Networks have garnered substantial attention for their capacity to model evolving structural and temporal patterns while exhibiting impressive performance. However, it is known that these architectures are encumbered by issues that constrain their performance, such as over-squashing and over-smoothing. Meanwhile, Transformers have demonstrated exceptional computational capacity to effectively address challenges related to long-range dependencies. Consequently, we introduce Todyformer-a novel Transformer-based neural network tailored for dynamic graphs. It unifies the local encoding capacity of Message-Passing Neural Networks (MPNNs) with the global encoding of Transformers through i) a novel patchifying paradigm for dynamic graphs to improve over-squashing, ii) a structure-aware parametric tokenization strategy leveraging MPNNs, iii) a Transformer with temporal positional-encoding to capture long-range dependencies, and iv) an encoding architecture that alternates",
    "link": "https://arxiv.org/abs/2402.05944",
    "context": "Title: Todyformer: Towards Holistic Dynamic Graph Transformers with Structure-Aware Tokenization\nAbstract: Temporal Graph Neural Networks have garnered substantial attention for their capacity to model evolving structural and temporal patterns while exhibiting impressive performance. However, it is known that these architectures are encumbered by issues that constrain their performance, such as over-squashing and over-smoothing. Meanwhile, Transformers have demonstrated exceptional computational capacity to effectively address challenges related to long-range dependencies. Consequently, we introduce Todyformer-a novel Transformer-based neural network tailored for dynamic graphs. It unifies the local encoding capacity of Message-Passing Neural Networks (MPNNs) with the global encoding of Transformers through i) a novel patchifying paradigm for dynamic graphs to improve over-squashing, ii) a structure-aware parametric tokenization strategy leveraging MPNNs, iii) a Transformer with temporal positional-encoding to capture long-range dependencies, and iv) an encoding architecture that alternates",
    "path": "papers/24/02/2402.05944.json",
    "total_tokens": 872,
    "translated_title": "Todyformer：面向结构感知标记化的整体动态图形变压器",
    "translated_abstract": "时间关联的图神经网络因其能够模拟演化结构和时间模式并展示出良好性能而受到了广泛关注。然而，已知这些架构受到了一些问题的限制，如过度压缩和过度平滑。与此同时，变压器已经展示了出色的计算能力，能够有效解决与长程依赖性相关的挑战。因此，我们引入了一种新颖的基于变压器的神经网络Todyformer，专为动态图形设计。它通过以下方式将消息传递神经网络（MPNNs）的局部编码能力与变压器的全局编码能力统一起来：i）采用新颖的面向动态图形的块状化范式来改善过度压缩，ii）利用MPNNs的结构感知参数化标记化策略，iii）引入带有时间位置编码的变压器来捕捉长程依赖性，以及iv）交替的编码架构。",
    "tldr": "Todyformer是一个全新的基于变压器的神经网络，它通过结合局部编码和全局编码能力，采用新颖的标记化策略和时间位置编码来解决动态图形中的过度压缩和长程依赖性问题。",
    "en_tdlr": "Todyformer is a novel Transformer-based neural network that addresses over-squashing and long-range dependency issues in dynamic graphs by combining local and global encoding capabilities, employing a novel tokenization strategy and temporal positional encoding."
}