{
    "title": "Is it Possible to Edit Large Language Models Robustly?",
    "abstract": "Large language models (LLMs) have played a pivotal role in building communicative AI to imitate human behaviors but face the challenge of efficient customization. To tackle this challenge, recent studies have delved into the realm of model editing, which manipulates specific memories of language models and changes the related language generation. However, the robustness of model editing remains an open question. This work seeks to understand the strengths and limitations of editing methods, thus facilitating robust, realistic applications of communicative AI. Concretely, we conduct extensive analysis to address the three key research questions. Q1: Can edited LLMs behave consistently resembling communicative AI in realistic situations? Q2: To what extent does the rephrasing of prompts lead LLMs to deviate from the edited knowledge memory? Q3: Which knowledge features are correlated with the performance and robustness of editing? Our experimental results uncover a substantial disparity ",
    "link": "https://arxiv.org/abs/2402.05827",
    "context": "Title: Is it Possible to Edit Large Language Models Robustly?\nAbstract: Large language models (LLMs) have played a pivotal role in building communicative AI to imitate human behaviors but face the challenge of efficient customization. To tackle this challenge, recent studies have delved into the realm of model editing, which manipulates specific memories of language models and changes the related language generation. However, the robustness of model editing remains an open question. This work seeks to understand the strengths and limitations of editing methods, thus facilitating robust, realistic applications of communicative AI. Concretely, we conduct extensive analysis to address the three key research questions. Q1: Can edited LLMs behave consistently resembling communicative AI in realistic situations? Q2: To what extent does the rephrasing of prompts lead LLMs to deviate from the edited knowledge memory? Q3: Which knowledge features are correlated with the performance and robustness of editing? Our experimental results uncover a substantial disparity ",
    "path": "papers/24/02/2402.05827.json",
    "total_tokens": 867,
    "translated_title": "是否可以稳健地编辑大型语言模型？",
    "translated_abstract": "大型语言模型（LLM）在构建能模仿人类行为的交流型人工智能方面发挥了关键作用，但也面临着高效定制的挑战。为了解决这个挑战，最近的研究涉及到了模型编辑的领域，通过操纵语言模型的特定记忆并改变相关的语言生成来进行编辑。然而，模型编辑的稳健性仍然是一个悬而未决的问题。本研究旨在了解编辑方法的优势和局限性，从而促进对交流型人工智能的稳健、现实应用。具体而言，我们进行了广泛的分析以回答三个关键的研究问题。Q1：编辑后的LLM是否能在现实情境中一致地表现出类似于交流型人工智能的行为？Q2：改写提示在多大程度上导致LLM偏离编辑的知识记忆？Q3：哪些知识特征与编辑的性能和稳健性相关？我们的实验结果揭示了显著的差异。",
    "tldr": "本研究旨在了解大型语言模型的稳健编辑方法的优势和局限性，从而促进对交流型人工智能的稳健、现实应用。",
    "en_tdlr": "This study aims to understand the strengths and limitations of robust editing methods for large language models, thus facilitating robust and realistic applications of communicative AI."
}