{
    "title": "ConstitutionalExperts: Training a Mixture of Principle-based Prompts",
    "abstract": "arXiv:2403.04894v1 Announce Type: cross  Abstract: Large language models (LLMs) are highly capable at a variety of tasks given the right prompt, but writing one is still a difficult and tedious process. In this work, we introduce ConstitutionalExperts, a method for learning a prompt consisting of constitutional principles (i.e. rules), given a training dataset. Unlike prior methods that optimize the prompt as a single entity, our method incrementally improves the prompt by surgically editing individual principles. We also show that we can improve overall performance by learning unique prompts for different semantic regions of the training data and using a mixture-of-experts (MoE) architecture to route inputs at inference time. We compare our method to other state of the art prompt-optimization techniques across six benchmark datasets. We also investigate whether MoE improves these other techniques. Our results suggest that ConstitutionalExperts outperforms other prompt optimization tec",
    "link": "https://arxiv.org/abs/2403.04894",
    "context": "Title: ConstitutionalExperts: Training a Mixture of Principle-based Prompts\nAbstract: arXiv:2403.04894v1 Announce Type: cross  Abstract: Large language models (LLMs) are highly capable at a variety of tasks given the right prompt, but writing one is still a difficult and tedious process. In this work, we introduce ConstitutionalExperts, a method for learning a prompt consisting of constitutional principles (i.e. rules), given a training dataset. Unlike prior methods that optimize the prompt as a single entity, our method incrementally improves the prompt by surgically editing individual principles. We also show that we can improve overall performance by learning unique prompts for different semantic regions of the training data and using a mixture-of-experts (MoE) architecture to route inputs at inference time. We compare our method to other state of the art prompt-optimization techniques across six benchmark datasets. We also investigate whether MoE improves these other techniques. Our results suggest that ConstitutionalExperts outperforms other prompt optimization tec",
    "path": "papers/24/03/2403.04894.json",
    "total_tokens": 835,
    "translated_title": "ConstitutionalExperts: 训练基于原则的提示混合体",
    "translated_abstract": "大型语言模型（LLMs）在各种任务上表现出色，但写作仍然是一个困难且繁琐的过程。 在这项工作中，我们介绍了ConstitutionalExperts，这是一种学习由宪法原则（即规则）组成的提示的方法，给定一个训练数据集。 与以往优化提示作为单个实体的方法不同，我们的方法通过分别编辑各个原则逐步改进提示。 我们还展示了通过为训练数据的不同语义区域学习唯一的提示，并在推断时使用专家混合（MoE）架构来提高整体性能。 我们将我们的方法与其他六个基准数据集上的其他最先进的提示优化技术进行了比较。 我们还调查了MoE是否改善这些其他技术。 我们的结果表明，ConstitutionalExperts的表现优于其他提示优化技术。",
    "tldr": "提出了ConstitutionalExperts方法，通过学习宪法原则构建提示，采用逐步改进提示和MoE架构，展现出在不同语义区域学习独特提示的潜力，并在六个基准数据集上表现优异。"
}