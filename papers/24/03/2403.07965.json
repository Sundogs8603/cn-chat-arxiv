{
    "title": "Conditional computation in neural networks: principles and research trends",
    "abstract": "arXiv:2403.07965v1 Announce Type: cross  Abstract: This article summarizes principles and ideas from the emerging area of applying \\textit{conditional computation} methods to the design of neural networks. In particular, we focus on neural networks that can dynamically activate or de-activate parts of their computational graph conditionally on their input. Examples include the dynamic selection of, e.g., input tokens, layers (or sets of layers), and sub-modules inside each layer (e.g., channels in a convolutional filter). We first provide a general formalism to describe these techniques in an uniform way. Then, we introduce three notable implementations of these principles: mixture-of-experts (MoEs) networks, token selection mechanisms, and early-exit neural networks. The paper aims to provide a tutorial-like introduction to this growing field. To this end, we analyze the benefits of these modular designs in terms of efficiency, explainability, and transfer learning, with a focus on em",
    "link": "https://arxiv.org/abs/2403.07965",
    "context": "Title: Conditional computation in neural networks: principles and research trends\nAbstract: arXiv:2403.07965v1 Announce Type: cross  Abstract: This article summarizes principles and ideas from the emerging area of applying \\textit{conditional computation} methods to the design of neural networks. In particular, we focus on neural networks that can dynamically activate or de-activate parts of their computational graph conditionally on their input. Examples include the dynamic selection of, e.g., input tokens, layers (or sets of layers), and sub-modules inside each layer (e.g., channels in a convolutional filter). We first provide a general formalism to describe these techniques in an uniform way. Then, we introduce three notable implementations of these principles: mixture-of-experts (MoEs) networks, token selection mechanisms, and early-exit neural networks. The paper aims to provide a tutorial-like introduction to this growing field. To this end, we analyze the benefits of these modular designs in terms of efficiency, explainability, and transfer learning, with a focus on em",
    "path": "papers/24/03/2403.07965.json",
    "total_tokens": 833,
    "translated_title": "神经网络中的条件计算: 原理与研究趋势",
    "translated_abstract": "这篇文章总结了将条件计算方法应用于设计神经网络的新兴领域中的原理和思想。我们特别关注可以根据输入动态激活或去激活其计算图部分的神经网络。例如，动态选择输入标记、层（或一组层）以及每个层内的子模块（例如，卷积滤波器中的通道）。我们首先提供一个通用形式来统一描述这些技术。然后，我们介绍了这些原则的三个值得注意的实现：专家混合（MoEs）网络、标记选择机制和提前退出神经网络。本文旨在向这一不断发展的领域提供类似教程的介绍。为此，我们分析了这些模块化设计在效率、可解释性和迁移学习方面的好处，重点放在...",
    "tldr": "该论文总结了将条件计算方法应用于设计神经网络的新兴领域中的原理和思想，并介绍了专家混合网络、标记选择机制和提前退出神经网络等三种实现方式。"
}