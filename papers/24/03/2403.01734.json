{
    "title": "Offline Goal-Conditioned Reinforcement Learning for Safety-Critical Tasks with Recovery Policy",
    "abstract": "arXiv:2403.01734v1 Announce Type: cross  Abstract: Offline goal-conditioned reinforcement learning (GCRL) aims at solving goal-reaching tasks with sparse rewards from an offline dataset. While prior work has demonstrated various approaches for agents to learn near-optimal policies, these methods encounter limitations when dealing with diverse constraints in complex environments, such as safety constraints. Some of these approaches prioritize goal attainment without considering safety, while others excessively focus on safety at the expense of training efficiency. In this paper, we study the problem of constrained offline GCRL and propose a new method called Recovery-based Supervised Learning (RbSL) to accomplish safety-critical tasks with various goals. To evaluate the method performance, we build a benchmark based on the robot-fetching environment with a randomly positioned obstacle and use expert or random policies to generate an offline dataset. We compare RbSL with three offline GC",
    "link": "https://arxiv.org/abs/2403.01734",
    "context": "Title: Offline Goal-Conditioned Reinforcement Learning for Safety-Critical Tasks with Recovery Policy\nAbstract: arXiv:2403.01734v1 Announce Type: cross  Abstract: Offline goal-conditioned reinforcement learning (GCRL) aims at solving goal-reaching tasks with sparse rewards from an offline dataset. While prior work has demonstrated various approaches for agents to learn near-optimal policies, these methods encounter limitations when dealing with diverse constraints in complex environments, such as safety constraints. Some of these approaches prioritize goal attainment without considering safety, while others excessively focus on safety at the expense of training efficiency. In this paper, we study the problem of constrained offline GCRL and propose a new method called Recovery-based Supervised Learning (RbSL) to accomplish safety-critical tasks with various goals. To evaluate the method performance, we build a benchmark based on the robot-fetching environment with a randomly positioned obstacle and use expert or random policies to generate an offline dataset. We compare RbSL with three offline GC",
    "path": "papers/24/03/2403.01734.json",
    "total_tokens": 665,
    "translated_title": "离线目标条件强化学习在具有恢复策略的安全关键任务中的应用",
    "translated_abstract": "离线目标条件强化学习（GCRL）旨在通过离线数据集解决具有稀疏奖励的目标达成任务。本文研究了约束下的离线GCRL问题，并提出了一种名为基于恢复的监督学习（RbSL）的新方法，用于完成具有不同目标的安全关键任务。",
    "tldr": "提出了一种名为RbSL的新方法，用于解决具有不同目标的安全关键任务，克服了传统方法在复杂环境中处理多样约束时的限制。",
    "en_tdlr": "A new method called RbSL is proposed to solve safety-critical tasks with various goals, overcoming limitations of traditional approaches when dealing with diverse constraints in complex environments."
}