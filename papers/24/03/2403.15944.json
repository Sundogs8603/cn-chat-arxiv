{
    "title": "Adaptive Super Resolution For One-Shot Talking-Head Generation",
    "abstract": "arXiv:2403.15944v1 Announce Type: cross  Abstract: The one-shot talking-head generation learns to synthesize a talking-head video with one source portrait image under the driving of same or different identity video. Usually these methods require plane-based pixel transformations via Jacobin matrices or facial image warps for novel poses generation. The constraints of using a single image source and pixel displacements often compromise the clarity of the synthesized images. Some methods try to improve the quality of synthesized videos by introducing additional super-resolution modules, but this will undoubtedly increase computational consumption and destroy the original data distribution. In this work, we propose an adaptive high-quality talking-head video generation method, which synthesizes high-resolution video without additional pre-trained modules. Specifically, inspired by existing super-resolution methods, we down-sample the one-shot source image, and then adaptively reconstruct ",
    "link": "https://arxiv.org/abs/2403.15944",
    "context": "Title: Adaptive Super Resolution For One-Shot Talking-Head Generation\nAbstract: arXiv:2403.15944v1 Announce Type: cross  Abstract: The one-shot talking-head generation learns to synthesize a talking-head video with one source portrait image under the driving of same or different identity video. Usually these methods require plane-based pixel transformations via Jacobin matrices or facial image warps for novel poses generation. The constraints of using a single image source and pixel displacements often compromise the clarity of the synthesized images. Some methods try to improve the quality of synthesized videos by introducing additional super-resolution modules, but this will undoubtedly increase computational consumption and destroy the original data distribution. In this work, we propose an adaptive high-quality talking-head video generation method, which synthesizes high-resolution video without additional pre-trained modules. Specifically, inspired by existing super-resolution methods, we down-sample the one-shot source image, and then adaptively reconstruct ",
    "path": "papers/24/03/2403.15944.json",
    "total_tokens": 821,
    "translated_title": "自适应超分辨率用于一拍即合的说唱头视频生成",
    "translated_abstract": "arXiv:2403.15944v1 类型：交叉 摘要：一拍即合的说唱头视频生成学习如何在相同或不同身份视频的操控下合成一个以源肖像图像为基础的说唱头视频。通常这些方法通过雅可比矩阵或脸部图像变形来进行基于平面的像素变换，以生成新的姿势。使用单一图像源和像素位移的约束通常会损害合成图像的清晰度。一些方法尝试通过引入额外的超分辨率模块来提高合成视频的质量，但这无疑会增加计算开销并破坏原始数据分布。在本研究中，我们提出了一种自适应高质量说唱头视频生成方法，可以合成高分辨率视频，而无需额外的预训练模块。具体来说，受现有超分辨率方法的启发，我们对一拍即合的源图像进行下采样，然后自适应重建。",
    "tldr": "该论文提出了一种自适应高质量说唱头视频生成方法，可以合成高分辨率视频，无需额外的预训练模块。"
}