{
    "title": "TWOLAR: a TWO-step LLM-Augmented distillation method for passage Reranking",
    "abstract": "arXiv:2403.17759v1 Announce Type: new  Abstract: In this paper, we present TWOLAR: a two-stage pipeline for passage reranking based on the distillation of knowledge from Large Language Models (LLM). TWOLAR introduces a new scoring strategy and a distillation process consisting in the creation of a novel and diverse training dataset. The dataset consists of 20K queries, each associated with a set of documents retrieved via four distinct retrieval methods to ensure diversity, and then reranked by exploiting the zero-shot reranking capabilities of an LLM. Our ablation studies demonstrate the contribution of each new component we introduced. Our experimental results show that TWOLAR significantly enhances the document reranking ability of the underlying model, matching and in some cases even outperforming state-of-the-art models with three orders of magnitude more parameters on the TREC-DL test sets and the zero-shot evaluation benchmark BEIR. To facilitate future work we release our data ",
    "link": "https://arxiv.org/abs/2403.17759",
    "context": "Title: TWOLAR: a TWO-step LLM-Augmented distillation method for passage Reranking\nAbstract: arXiv:2403.17759v1 Announce Type: new  Abstract: In this paper, we present TWOLAR: a two-stage pipeline for passage reranking based on the distillation of knowledge from Large Language Models (LLM). TWOLAR introduces a new scoring strategy and a distillation process consisting in the creation of a novel and diverse training dataset. The dataset consists of 20K queries, each associated with a set of documents retrieved via four distinct retrieval methods to ensure diversity, and then reranked by exploiting the zero-shot reranking capabilities of an LLM. Our ablation studies demonstrate the contribution of each new component we introduced. Our experimental results show that TWOLAR significantly enhances the document reranking ability of the underlying model, matching and in some cases even outperforming state-of-the-art models with three orders of magnitude more parameters on the TREC-DL test sets and the zero-shot evaluation benchmark BEIR. To facilitate future work we release our data ",
    "path": "papers/24/03/2403.17759.json",
    "total_tokens": 911,
    "translated_title": "TWOLAR：一种基于大型语言模型增强蒸馏方法的段落重新排序的两步骤方法",
    "translated_abstract": "在本文中，我们提出了TWOLAR：一种基于大型语言模型（LLM）知识蒸馏的段落重新排序的两阶段流水线。TWOLAR引入了一种新的评分策略和一个蒸馏过程，包括创建一个新颖且多样的训练数据集。该数据集包含20K个查询，每个查询与通过四种不同检索方法检索到的一组文档相关联，以确保多样性，然后通过利用LLM的零-shot重新排序能力进行重新排序。我们的消融研究证明了我们引入的每个新组件的贡献。我们的实验结果表明，TWOLAR显著增强了基础模型的文档重新排序能力，在TREC-DL测试集和零-shot评估基准BEIR上与拥有三个数量级更多参数的最先进模型相匹配甚至在某些情况下超越。为了促进未来工作，我们发布了我们的数据。",
    "tldr": "TWOLAR引入了新的评分策略和蒸馏过程，创建了多样性训练数据集，显著增强了文档重新排序能力，与三个数量级更多参数的最先进模型相匹配甚至在某些情况下超越。",
    "en_tdlr": "TWOLAR introduces a new scoring strategy and distillation process, creates a diverse training dataset, significantly enhances document reranking ability, matching or even outperforming state-of-the-art models with three orders of magnitude more parameters."
}