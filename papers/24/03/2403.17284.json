{
    "title": "Common Ground Tracking in Multimodal Dialogue",
    "abstract": "arXiv:2403.17284v1 Announce Type: new  Abstract: Within Dialogue Modeling research in AI and NLP, considerable attention has been spent on ``dialogue state tracking'' (DST), which is the ability to update the representations of the speaker's needs at each turn in the dialogue by taking into account the past dialogue moves and history. Less studied but just as important to dialogue modeling, however, is ``common ground tracking'' (CGT), which identifies the shared belief space held by all of the participants in a task-oriented dialogue: the task-relevant propositions all participants accept as true. In this paper we present a method for automatically identifying the current set of shared beliefs and ``questions under discussion'' (QUDs) of a group with a shared goal. We annotate a dataset of multimodal interactions in a shared physical space with speech transcriptions, prosodic features, gestures, actions, and facets of collaboration, and operationalize these features for use in a deep ",
    "link": "https://arxiv.org/abs/2403.17284",
    "context": "Title: Common Ground Tracking in Multimodal Dialogue\nAbstract: arXiv:2403.17284v1 Announce Type: new  Abstract: Within Dialogue Modeling research in AI and NLP, considerable attention has been spent on ``dialogue state tracking'' (DST), which is the ability to update the representations of the speaker's needs at each turn in the dialogue by taking into account the past dialogue moves and history. Less studied but just as important to dialogue modeling, however, is ``common ground tracking'' (CGT), which identifies the shared belief space held by all of the participants in a task-oriented dialogue: the task-relevant propositions all participants accept as true. In this paper we present a method for automatically identifying the current set of shared beliefs and ``questions under discussion'' (QUDs) of a group with a shared goal. We annotate a dataset of multimodal interactions in a shared physical space with speech transcriptions, prosodic features, gestures, actions, and facets of collaboration, and operationalize these features for use in a deep ",
    "path": "papers/24/03/2403.17284.json",
    "total_tokens": 813,
    "translated_title": "多模态对话中的共同地面跟踪",
    "translated_abstract": "人工智能和自然语言处理中的对话建模研究已经花费了相当多的精力在“对话状态跟踪”（DST）上，即通过考虑过去的对话移动和历史来更新每次对话中发言者需求的表示能力。然而，在对话建模中同样重要但研究较少的是“共同地面跟踪”（CGT），它确定了所有任务导向对话中所有参与者持有的共享信念空间：所有参与者接受为真的与任务相关的命题。在本文中，我们提出了一种自动识别具有共享目标的群体的当前共享信念集合和“正在讨论的问题”（QUDs）的方法。我们使用语音转录，语调特征，手势，行为和协作方面的要素对共享物理空间中的多模态交互数据集进行了标注，并使这些要素能够用于深度",
    "tldr": "本文提出了一种自动识别多模态对话中参与者共享信念以及正在讨论问题的方法。"
}