{
    "title": "TextCraftor: Your Text Encoder Can be Image Quality Controller",
    "abstract": "arXiv:2403.18978v1 Announce Type: cross  Abstract: Diffusion-based text-to-image generative models, e.g., Stable Diffusion, have revolutionized the field of content generation, enabling significant advancements in areas like image editing and video synthesis. Despite their formidable capabilities, these models are not without their limitations. It is still challenging to synthesize an image that aligns well with the input text, and multiple runs with carefully crafted prompts are required to achieve satisfactory results. To mitigate these limitations, numerous studies have endeavored to fine-tune the pre-trained diffusion models, i.e., UNet, utilizing various technologies. Yet, amidst these efforts, a pivotal question of text-to-image diffusion model training has remained largely unexplored: Is it possible and feasible to fine-tune the text encoder to improve the performance of text-to-image diffusion models? Our findings reveal that, instead of replacing the CLIP text encoder used in ",
    "link": "https://arxiv.org/abs/2403.18978",
    "context": "Title: TextCraftor: Your Text Encoder Can be Image Quality Controller\nAbstract: arXiv:2403.18978v1 Announce Type: cross  Abstract: Diffusion-based text-to-image generative models, e.g., Stable Diffusion, have revolutionized the field of content generation, enabling significant advancements in areas like image editing and video synthesis. Despite their formidable capabilities, these models are not without their limitations. It is still challenging to synthesize an image that aligns well with the input text, and multiple runs with carefully crafted prompts are required to achieve satisfactory results. To mitigate these limitations, numerous studies have endeavored to fine-tune the pre-trained diffusion models, i.e., UNet, utilizing various technologies. Yet, amidst these efforts, a pivotal question of text-to-image diffusion model training has remained largely unexplored: Is it possible and feasible to fine-tune the text encoder to improve the performance of text-to-image diffusion models? Our findings reveal that, instead of replacing the CLIP text encoder used in ",
    "path": "papers/24/03/2403.18978.json",
    "total_tokens": 833,
    "translated_title": "TextCraftor：您的文本编码器可以成为图像质量控制器",
    "translated_abstract": "基于扩散的文本到图像生成模型，如稳定扩散，已经彻底改变了内容生成领域，使得在诸如图像编辑和视频合成等领域取得了重大进展。尽管这些模型具有强大的功能，但它们并非没有局限性。合成与输入文本相契合的图像仍然具有挑战性，并且需要多次运行以精心设计的提示才能实现令人满意的结果。为了减轻这些局限性，许多研究努力对预训练的扩散模型，即UNet，进行微调，利用各种技术。然而，在这些努力中，一个关键问题，即对文本到图像扩散模型训练进行微调以改善文本到图像扩散模型性能是否可能和可行，仍然大多未被探讨。我们的研究结果显示，与其替换CLIP文本编码器，更好的方法是微调文本编码器以提升文本到图像扩散模型的性能。",
    "tldr": "通过微调文本编码器来改进文本到图像扩散模型的性能。",
    "en_tdlr": "Improving the performance of text-to-image diffusion models by fine-tuning the text encoder."
}