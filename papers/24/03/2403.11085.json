{
    "title": "m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks",
    "abstract": "arXiv:2403.11085v1 Announce Type: cross  Abstract: Real-world multi-modal problems are rarely solved by a single machine learning model, and often require multi-step computational plans that involve stitching several models. Tool-augmented LLMs hold tremendous promise for automating the generation of such computational plans. However, the lack of standardized benchmarks for evaluating LLMs as planners for multi-step multi-modal tasks has prevented a systematic study of planner design decisions. Should LLMs generate a full plan in a single shot or step-by-step? Should they invoke tools directly with Python code or through structured data formats like JSON? Does feedback improve planning? To answer these questions and more, we introduce m&m's: a benchmark containing 4K+ multi-step multi-modal tasks involving 33 tools that include multi-modal models, (free) public APIs, and image processing modules. For each of these task queries, we provide automatically generated plans using this realis",
    "link": "https://arxiv.org/abs/2403.11085",
    "context": "Title: m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks\nAbstract: arXiv:2403.11085v1 Announce Type: cross  Abstract: Real-world multi-modal problems are rarely solved by a single machine learning model, and often require multi-step computational plans that involve stitching several models. Tool-augmented LLMs hold tremendous promise for automating the generation of such computational plans. However, the lack of standardized benchmarks for evaluating LLMs as planners for multi-step multi-modal tasks has prevented a systematic study of planner design decisions. Should LLMs generate a full plan in a single shot or step-by-step? Should they invoke tools directly with Python code or through structured data formats like JSON? Does feedback improve planning? To answer these questions and more, we introduce m&m's: a benchmark containing 4K+ multi-step multi-modal tasks involving 33 tools that include multi-modal models, (free) public APIs, and image processing modules. For each of these task queries, we provide automatically generated plans using this realis",
    "path": "papers/24/03/2403.11085.json",
    "total_tokens": 860,
    "translated_title": "m&m's: 一个用于评估多步骤多模态任务工具使用的基准",
    "translated_abstract": "现实世界中的多模态问题很少由单个机器学习模型解决，通常需要多步骤计算计划，涉及拼接多个模型。 工具增强型LLM极有可能自动化生成这种计算计划。然而，缺乏用于评估LLM作为多步骤多模态任务规划器的标准化基准，阻碍了对规划器设计决策的系统研究。LLM是否应一次性生成整个计划还是逐步生成？它们是否应该直接使用Python代码调用工具，还是通过类似JSON的结构化数据格式？反馈是否改善规划？为了回答这些问题以及更多问题，我们介绍了m&m's：一个基准，包含4K+个涉及33种工具的多步骤多模态任务，其中包括多模态模型、(免费)公共API和图像处理模块。对于每个任务查询，我们提供使用这种方法自动生成的计划。",
    "tldr": "m&m's引入了一个包含4K+多步骤多模态任务的基准，涉及33种工具，用于评估LLM作为规划器的设计决策。",
    "en_tdlr": "m&m's introduces a benchmark with 4K+ multi-step multi-modal tasks involving 33 tools to evaluate the design decisions of LLM as planners."
}