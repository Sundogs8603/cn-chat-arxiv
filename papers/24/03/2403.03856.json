{
    "title": "Public-data Assisted Private Stochastic Optimization: Power and Limitations",
    "abstract": "arXiv:2403.03856v1 Announce Type: new  Abstract: We study the limits and capability of public-data assisted differentially private (PA-DP) algorithms. Specifically, we focus on the problem of stochastic convex optimization (SCO) with either labeled or unlabeled public data. For complete/labeled public data, we show that any $(\\epsilon,\\delta)$-PA-DP has excess risk $\\tilde{\\Omega}\\big(\\min\\big\\{\\frac{1}{\\sqrt{n_{\\text{pub}}}},\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d}}{n\\epsilon} \\big\\} \\big)$, where $d$ is the dimension, ${n_{\\text{pub}}}$ is the number of public samples, ${n_{\\text{priv}}}$ is the number of private samples, and $n={n_{\\text{pub}}}+{n_{\\text{priv}}}$. These lower bounds are established via our new lower bounds for PA-DP mean estimation, which are of a similar form. Up to constant factors, these lower bounds show that the simple strategy of either treating all data as private or discarding the private data, is optimal. We also study PA-DP supervised learning with \\textit{unlabe",
    "link": "https://arxiv.org/abs/2403.03856",
    "context": "Title: Public-data Assisted Private Stochastic Optimization: Power and Limitations\nAbstract: arXiv:2403.03856v1 Announce Type: new  Abstract: We study the limits and capability of public-data assisted differentially private (PA-DP) algorithms. Specifically, we focus on the problem of stochastic convex optimization (SCO) with either labeled or unlabeled public data. For complete/labeled public data, we show that any $(\\epsilon,\\delta)$-PA-DP has excess risk $\\tilde{\\Omega}\\big(\\min\\big\\{\\frac{1}{\\sqrt{n_{\\text{pub}}}},\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d}}{n\\epsilon} \\big\\} \\big)$, where $d$ is the dimension, ${n_{\\text{pub}}}$ is the number of public samples, ${n_{\\text{priv}}}$ is the number of private samples, and $n={n_{\\text{pub}}}+{n_{\\text{priv}}}$. These lower bounds are established via our new lower bounds for PA-DP mean estimation, which are of a similar form. Up to constant factors, these lower bounds show that the simple strategy of either treating all data as private or discarding the private data, is optimal. We also study PA-DP supervised learning with \\textit{unlabe",
    "path": "papers/24/03/2403.03856.json",
    "total_tokens": 931,
    "translated_title": "公共数据辅助下的私有随机优化：动力和限制",
    "translated_abstract": "我们研究了公共数据辅助的差分隐私（PA-DP）算法的限制和能力。具体来说，我们关注具有标记或未标记公共数据的随机凸优化（SCO）问题。对于完整/标记的公共数据，我们表明任何$(\\epsilon,\\delta)$-PA-DP都具有超出风险$\\tilde{\\Omega}\\big(\\min\\big\\{\\frac{1}{\\sqrt{n_{\\text{pub}}}},\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d}}{n\\epsilon} \\big\\} \\big)$，其中$d$是维数，${n_{\\text{pub}}}$是公共样本数量，${n_{\\text{priv}}}$是私有样本数量，$n={n_{\\text{pub}}}+{n_{\\text{priv}}}$. 这些下界是通过我们对PA-DP均值估计的新下界建立的，其形式相似。在常数因素的影响下，这些下界表明将所有数据视为私有或丢弃私有数据的简单策略是最优的。我们还研究了具有未标记公共数据的PA-DP监督学习。",
    "tldr": "该研究研究了公共数据辅助的私有差分隐私算法在随机凸优化问题中的限制和能力，展示出简单策略在这一领域的最优性。"
}