{
    "title": "Learning Useful Representations of Recurrent Neural Network Weight Matrices",
    "abstract": "arXiv:2403.11998v1 Announce Type: new  Abstract: Recurrent Neural Networks (RNNs) are general-purpose parallel-sequential computers. The program of an RNN is its weight matrix. How to learn useful representations of RNN weights that facilitate RNN analysis as well as downstream tasks? While the mechanistic approach directly looks at some RNN's weights to predict its behavior, the functionalist approach analyzes its overall functionality -- specifically, its input-output mapping. We consider several mechanistic approaches for RNN weights and adapt the permutation equivariant Deep Weight Space layer for RNNs. Our two novel functionalist approaches extract information from RNN weights by 'interrogating' the RNN through probing inputs. We develop a theoretical framework that demonstrates conditions under which the functionalist approach can generate rich representations that help determine RNN behavior. We create and release the first two 'model zoo' datasets for RNN weight representation ",
    "link": "https://arxiv.org/abs/2403.11998",
    "context": "Title: Learning Useful Representations of Recurrent Neural Network Weight Matrices\nAbstract: arXiv:2403.11998v1 Announce Type: new  Abstract: Recurrent Neural Networks (RNNs) are general-purpose parallel-sequential computers. The program of an RNN is its weight matrix. How to learn useful representations of RNN weights that facilitate RNN analysis as well as downstream tasks? While the mechanistic approach directly looks at some RNN's weights to predict its behavior, the functionalist approach analyzes its overall functionality -- specifically, its input-output mapping. We consider several mechanistic approaches for RNN weights and adapt the permutation equivariant Deep Weight Space layer for RNNs. Our two novel functionalist approaches extract information from RNN weights by 'interrogating' the RNN through probing inputs. We develop a theoretical framework that demonstrates conditions under which the functionalist approach can generate rich representations that help determine RNN behavior. We create and release the first two 'model zoo' datasets for RNN weight representation ",
    "path": "papers/24/03/2403.11998.json",
    "total_tokens": 833,
    "translated_title": "学习递归神经网络权重矩阵的有用表示",
    "translated_abstract": "递归神经网络(RNNs)是通用的并行串行计算机。 RNN的程序是其权重矩阵。 如何学习有助于RNN分析以及下游任务的RNN权重的有用表示？ 尽管机械主义方法直接查看一些RNN的权重来预测其行为，功能主义方法分析其整体功能--具体来说是其输入输出映射。 我们考虑了几种适用于RNN权重的机械主义方法，并为RNN引入了置换等变的深度权重空间层。我们的两种新颖的功能主义方法通过“询问”输入而从RNN权重中提取信息。 我们开发了一个理论框架，证明了功能主义方法能够产生有助于确定RNN行为的丰富表示的条件。 我们创建并发布了第一个两个“模型动物园”数据集，用于RNN权重表示",
    "tldr": "提出了机械主义和功能主义两种方法以学习递归神经网络(RNN)权重的有用表示，并发展了框架来生成有助于确定RNN行为的丰富表示",
    "en_tdlr": "Proposed mechanistic and functionalist approaches to learn useful representations of Recurrent Neural Network (RNN) weights, with a developed framework to generate rich representations that help determine RNN behavior."
}