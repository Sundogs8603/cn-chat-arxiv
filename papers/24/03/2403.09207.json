{
    "title": "TaxoLLaMA: WordNet-based Model for Solving Multiple Lexical Sematic Tasks",
    "abstract": "arXiv:2403.09207v1 Announce Type: new  Abstract: In this paper, we explore the capabilities of LLMs in capturing lexical-semantic knowledge from WordNet on the example of the LLaMA-2-7b model and test it on multiple lexical semantic tasks. As the outcome of our experiments, we present TaxoLLaMA, the everything-in-one model, lightweight due to 4-bit quantization and LoRA. It achieves 11 SotA results, 4 top-2 results out of 16 tasks for the Taxonomy Enrichment, Hypernym Discovery, Taxonomy Construction, and Lexical Entailment tasks. Moreover, it demonstrates very strong zero-shot performance on Lexical Entailment and Taxonomy Construction with no fine-tuning. We also explore its hidden multilingual and domain adaptation capabilities with a little tuning or few-shot learning. All datasets, code, and model are available online at https://github.com/VityaVitalich/TaxoLLaMA",
    "link": "https://arxiv.org/abs/2403.09207",
    "context": "Title: TaxoLLaMA: WordNet-based Model for Solving Multiple Lexical Sematic Tasks\nAbstract: arXiv:2403.09207v1 Announce Type: new  Abstract: In this paper, we explore the capabilities of LLMs in capturing lexical-semantic knowledge from WordNet on the example of the LLaMA-2-7b model and test it on multiple lexical semantic tasks. As the outcome of our experiments, we present TaxoLLaMA, the everything-in-one model, lightweight due to 4-bit quantization and LoRA. It achieves 11 SotA results, 4 top-2 results out of 16 tasks for the Taxonomy Enrichment, Hypernym Discovery, Taxonomy Construction, and Lexical Entailment tasks. Moreover, it demonstrates very strong zero-shot performance on Lexical Entailment and Taxonomy Construction with no fine-tuning. We also explore its hidden multilingual and domain adaptation capabilities with a little tuning or few-shot learning. All datasets, code, and model are available online at https://github.com/VityaVitalich/TaxoLLaMA",
    "path": "papers/24/03/2403.09207.json",
    "total_tokens": 929,
    "translated_title": "TaxoLLaMA:基于WordNet的模型用于解决多个词汇语义任务",
    "translated_abstract": "这篇论文探讨了LLMs在捕捉WordNet中的词汇语义知识方面的能力，以LLaMA-2-7b模型为例，并在多个词汇语义任务上对其进行了测试。作为我们实验的结果，我们提出了TaxoLLaMA，即一切皆在其中的模型，由于4位量化和LoRA而轻量化。它在分类学丰富化、上位词发现、分类学构建和词汇蕴涵任务中实现了11个SotA结果，16个任务中的4个前2名结果。此外，它展示了在词汇蕴涵和分类学构建上具有非常强大的零样本性能，无需微调。我们还探讨了其具有的隐藏多语言和领域适应能力，仅需少量调整或少量学习。所有数据集、代码和模型都可在https://github.com/VityaVitalich/TaxoLLaMA找到。",
    "tldr": "通过基于WordNet的LLMs模型，提出了TaxoLLaMA模型，采用4位量化和LoRA技术轻量化，在多个词汇语义任务中取得11个SotA结果，且在词汇蕴涵和分类学构建任务上表现出强大的零样本性能。",
    "en_tdlr": "Proposing the TaxoLLaMA model based on LLMs with WordNet, lightweighted by utilizing 4-bit quantization and LoRA, achieving 11 SotA results in multiple lexical semantic tasks and demonstrating strong zero-shot performance in Lexical Entailment and Taxonomy Construction tasks."
}