{
    "title": "Magic for the Age of Quantized DNNs",
    "abstract": "arXiv:2403.14999v1 Announce Type: cross  Abstract: Recently, the number of parameters in DNNs has explosively increased, as exemplified by LLMs (Large Language Models), making inference on small-scale computers more difficult. Model compression technology is, therefore, essential for integration into products. In this paper, we propose a method of quantization-aware training. We introduce a novel normalization (Layer-Batch Normalization) that is independent of the mini-batch size and does not require any additional computation cost during inference. Then, we quantize the weights by the scaled round-clip function with the weight standardization. We also quantize activation functions using the same function and apply surrogate gradients to train the model with both quantized weights and the quantized activation functions. We call this method Magic for the age of Quantised DNNs (MaQD). Experimental results show that our quantization method can be achieved with minimal accuracy degradation",
    "link": "https://arxiv.org/abs/2403.14999",
    "context": "Title: Magic for the Age of Quantized DNNs\nAbstract: arXiv:2403.14999v1 Announce Type: cross  Abstract: Recently, the number of parameters in DNNs has explosively increased, as exemplified by LLMs (Large Language Models), making inference on small-scale computers more difficult. Model compression technology is, therefore, essential for integration into products. In this paper, we propose a method of quantization-aware training. We introduce a novel normalization (Layer-Batch Normalization) that is independent of the mini-batch size and does not require any additional computation cost during inference. Then, we quantize the weights by the scaled round-clip function with the weight standardization. We also quantize activation functions using the same function and apply surrogate gradients to train the model with both quantized weights and the quantized activation functions. We call this method Magic for the age of Quantised DNNs (MaQD). Experimental results show that our quantization method can be achieved with minimal accuracy degradation",
    "path": "papers/24/03/2403.14999.json",
    "total_tokens": 831,
    "translated_title": "魔法与量子化深度神经网络时代",
    "translated_abstract": "最近，深度神经网络中的参数数量急剧增加，如大型语言模型（LLMs）所示，使得在小规模计算机上进行推理变得更加困难。因此，模型压缩技术对产品整合至关重要。在本文中，我们提出了一种量化感知训练方法。我们引入了一种新颖的标准化方法（层批标准化），其独立于小批量大小，并且在推理期间不需要额外的计算成本。然后，我们通过带权标准化的缩放量化加权。我们还使用相同的函数量化激活函数，并应用代理梯度来训练既具有量化权重又具有量化激活函数的模型。我们将这种方法称为魔法与量子化DNN时代（MaQD）。实验结果表明，我们的量化方法可以在最小精度降级的情况下实现。",
    "tldr": "提出了一种量化感知训练方法，引入新型标准化方法并使用缩放量化加权，实现了在最小精度降级的情况下有效的量化深度神经网络"
}