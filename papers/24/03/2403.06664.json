{
    "title": "Smart-Infinity: Fast Large Language Model Training using Near-Storage Processing on a Real System",
    "abstract": "arXiv:2403.06664v1 Announce Type: cross  Abstract: The recent huge advance of Large Language Models (LLMs) is mainly driven by the increase in the number of parameters. This has led to substantial memory capacity requirements, necessitating the use of dozens of GPUs just to meet the capacity. One popular solution to this is storage-offloaded training, which uses host memory and storage as an extended memory hierarchy. However, this obviously comes at the cost of storage bandwidth bottleneck because storage devices have orders of magnitude lower bandwidth compared to that of GPU device memories. Our work, Smart-Infinity, addresses the storage bandwidth bottleneck of storage-offloaded LLM training using near-storage processing devices on a real system. The main component of Smart-Infinity is SmartUpdate, which performs parameter updates on custom near-storage accelerators. We identify that moving parameter updates to the storage side removes most of the storage traffic. In addition, we p",
    "link": "https://arxiv.org/abs/2403.06664",
    "context": "Title: Smart-Infinity: Fast Large Language Model Training using Near-Storage Processing on a Real System\nAbstract: arXiv:2403.06664v1 Announce Type: cross  Abstract: The recent huge advance of Large Language Models (LLMs) is mainly driven by the increase in the number of parameters. This has led to substantial memory capacity requirements, necessitating the use of dozens of GPUs just to meet the capacity. One popular solution to this is storage-offloaded training, which uses host memory and storage as an extended memory hierarchy. However, this obviously comes at the cost of storage bandwidth bottleneck because storage devices have orders of magnitude lower bandwidth compared to that of GPU device memories. Our work, Smart-Infinity, addresses the storage bandwidth bottleneck of storage-offloaded LLM training using near-storage processing devices on a real system. The main component of Smart-Infinity is SmartUpdate, which performs parameter updates on custom near-storage accelerators. We identify that moving parameter updates to the storage side removes most of the storage traffic. In addition, we p",
    "path": "papers/24/03/2403.06664.json",
    "total_tokens": 823,
    "translated_title": "Smart-Infinity: 使用真实系统上的近存储处理进行快速大型语言模型训练",
    "translated_abstract": "大型语言模型（LLMs）的最新巨大进展主要是由参数数量的增加驱动的。这导致了对大量内存容量的需求，需要使用数十个GPU才能满足容量。一个常见的解决方案是存储外部训练，它使用主机内存和存储作为扩展内存层次结构。然而，这显然会以存储带宽瓶颈为代价，因为存储设备的带宽比GPU设备内存低几个数量级。我们的工作Smart-Infinity，通过在真实系统上使用近存储处理设备来解决存储外部化的LLM训练的存储带宽瓶颈。Smart-Infinity的主要组成部分是SmartUpdate，它在自定义近存储加速器上执行参数更新。我们发现将参数更新移到存储端可以消除大部分存储流量。此外，",
    "tldr": "Smart-Infinity通过在真实系统上使用近存储处理设备，解决了存储外部化的大型语言模型训练中存储带宽瓶颈的问题",
    "en_tdlr": "Smart-Infinity addresses the storage bandwidth bottleneck of storage-offloaded Large Language Model training by utilizing near-storage processing devices on a real system."
}