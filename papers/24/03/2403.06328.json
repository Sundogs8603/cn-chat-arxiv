{
    "title": "Transferable Reinforcement Learning via Generalized Occupancy Models",
    "abstract": "arXiv:2403.06328v1 Announce Type: new  Abstract: Intelligent agents must be generalists - showing the ability to quickly adapt and generalize to varying tasks. Within the framework of reinforcement learning (RL), model-based RL algorithms learn a task-agnostic dynamics model of the world, in principle allowing them to generalize to arbitrary rewards. However, one-step models naturally suffer from compounding errors, making them ineffective for problems with long horizons and large state spaces. In this work, we propose a novel class of models - generalized occupancy models (GOMs) - that retain the generality of model-based RL while avoiding compounding error. The key idea behind GOMs is to model the distribution of all possible long-term outcomes from a given state under the coverage of a stationary dataset, along with a policy that realizes a particular outcome from the given state. These models can then quickly be used to select the optimal action for arbitrary new tasks, without hav",
    "link": "https://arxiv.org/abs/2403.06328",
    "context": "Title: Transferable Reinforcement Learning via Generalized Occupancy Models\nAbstract: arXiv:2403.06328v1 Announce Type: new  Abstract: Intelligent agents must be generalists - showing the ability to quickly adapt and generalize to varying tasks. Within the framework of reinforcement learning (RL), model-based RL algorithms learn a task-agnostic dynamics model of the world, in principle allowing them to generalize to arbitrary rewards. However, one-step models naturally suffer from compounding errors, making them ineffective for problems with long horizons and large state spaces. In this work, we propose a novel class of models - generalized occupancy models (GOMs) - that retain the generality of model-based RL while avoiding compounding error. The key idea behind GOMs is to model the distribution of all possible long-term outcomes from a given state under the coverage of a stationary dataset, along with a policy that realizes a particular outcome from the given state. These models can then quickly be used to select the optimal action for arbitrary new tasks, without hav",
    "path": "papers/24/03/2403.06328.json",
    "total_tokens": 853,
    "translated_title": "通过广义占有模型实现可迁移的强化学习",
    "translated_abstract": "智能代理必须是通用的 - 具有快速适应和概括到不同任务的能力。在强化学习（RL）框架内，基于模型的RL算法学习世界的任务不可知动态模型，原则上使它们能够概括到任意奖励。然而，一步模型自然会受到累积错误的影响，使它们在具有长时间跨度和大状态空间的问题上失效。在这项工作中，我们提出了一类新型模型 - 广义占有模型（GOMs），保留了基于模型的RL的通用性，同时避免了累积性错误。GOMs的关键思想是在一个固定数据集的覆盖下，建模给定状态的所有可能长期结果的分布，以及实现给定状态的特定结果的策略。然后，这些模型可以迅速用于为任意新任务选择最优操作，而无需担心累积错误。",
    "tldr": "通过广义占有模型，本研究提出了一种新颖的模型类别，保留了模型化强化学习的通用性，并避免了累积错误的问题。",
    "en_tdlr": "This work introduces a novel class of models, generalized occupancy models (GOMs), to maintain the generality of model-based RL while addressing the issue of compounding errors."
}