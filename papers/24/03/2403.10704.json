{
    "title": "PERL: Parameter Efficient Reinforcement Learning from Human Feedback",
    "abstract": "arXiv:2403.10704v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) has proven to be a strong method to align Pretrained Large Language Models (LLMs) with human preferences. But training models with RLHF is computationally expensive, and an overall complex process. In this work, we study RLHF where the underlying models are trained using the parameter efficient method of Low-Rank Adaptation (LoRA) introduced by Hu et al. [2021]. We investigate the setup of \"Parameter Efficient Reinforcement Learning\" (PERL), in which we perform reward model training and reinforcement learning using LoRA. We compare PERL to conventional fine-tuning (full-tuning) across various configurations for 7 benchmarks, including 2 novel datasets, of reward modeling and reinforcement learning. We find that PERL performs on par with the conventional RLHF setting, while training faster, and with less memory. This enables the high performance of RLHF, while reducing the computational ",
    "link": "https://arxiv.org/abs/2403.10704",
    "context": "Title: PERL: Parameter Efficient Reinforcement Learning from Human Feedback\nAbstract: arXiv:2403.10704v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) has proven to be a strong method to align Pretrained Large Language Models (LLMs) with human preferences. But training models with RLHF is computationally expensive, and an overall complex process. In this work, we study RLHF where the underlying models are trained using the parameter efficient method of Low-Rank Adaptation (LoRA) introduced by Hu et al. [2021]. We investigate the setup of \"Parameter Efficient Reinforcement Learning\" (PERL), in which we perform reward model training and reinforcement learning using LoRA. We compare PERL to conventional fine-tuning (full-tuning) across various configurations for 7 benchmarks, including 2 novel datasets, of reward modeling and reinforcement learning. We find that PERL performs on par with the conventional RLHF setting, while training faster, and with less memory. This enables the high performance of RLHF, while reducing the computational ",
    "path": "papers/24/03/2403.10704.json",
    "total_tokens": 890,
    "translated_title": "PERL: 从人类反馈中实现参数高效强化学习",
    "translated_abstract": "强化学习从人类反馈（RLHF）已被证明是一种将预训练的大型语言模型（LLMs）与人类偏好对齐的有效方法。然而，使用RLHF训练模型计算成本高昂，且整个过程复杂。在本研究中，我们研究了RLHF，其中基础模型使用胡等人提出的低秩适应（LoRA）的参数高效方法进行训练。我们探讨了“参数高效强化学习”（PERL）的设置，在其中我们使用LoRA进行奖励模型训练和强化学习。我们将PERL与传统的微调（全调）在包括2个新数据集在内的7个基准测试中的奖励建模和强化学习方面的各种配置进行了比较。我们发现，PERL的性能与传统的RLHF设置相当，同时训练速度更快，内存占用更少。这使得RLHF具有很高的性能，同时减少了计算成本。",
    "tldr": "使用低秩适应（LoRA）方法进行参数高效强化学习（PERL），能够在与传统RLHF设置相当的性能下，实现更快的训练和更少的内存占用。",
    "en_tdlr": "Performing parameter efficient reinforcement learning (PERL) using the Low-Rank Adaptation (LoRA) method allows for faster training and reduced memory usage while achieving performance on par with traditional RLHF settings."
}