{
    "title": "LoRA Meets Dropout under a Unified Framework",
    "abstract": "arXiv:2403.00812v1 Announce Type: cross  Abstract: With the remarkable capabilities, large language models (LLMs) have emerged as essential elements in numerous NLP applications, while parameter-efficient finetuning, especially LoRA, has gained popularity as a lightweight approach for model customization. Meanwhile, various dropout methods, initially designed for full finetuning with all the parameters updated, alleviates overfitting associated with excessive parameter redundancy. Hence, a possible contradiction arises from negligible trainable parameters of LoRA and the effectiveness of previous dropout methods, which has been largely overlooked. To fill this gap, we first confirm that parameter-efficient LoRA is also overfitting-prone. We then revisit transformer-specific dropout methods, and establish their equivalence and distinctions mathematically and empirically. Building upon this comparative analysis, we introduce a unified framework for a comprehensive investigation, which in",
    "link": "https://arxiv.org/abs/2403.00812",
    "context": "Title: LoRA Meets Dropout under a Unified Framework\nAbstract: arXiv:2403.00812v1 Announce Type: cross  Abstract: With the remarkable capabilities, large language models (LLMs) have emerged as essential elements in numerous NLP applications, while parameter-efficient finetuning, especially LoRA, has gained popularity as a lightweight approach for model customization. Meanwhile, various dropout methods, initially designed for full finetuning with all the parameters updated, alleviates overfitting associated with excessive parameter redundancy. Hence, a possible contradiction arises from negligible trainable parameters of LoRA and the effectiveness of previous dropout methods, which has been largely overlooked. To fill this gap, we first confirm that parameter-efficient LoRA is also overfitting-prone. We then revisit transformer-specific dropout methods, and establish their equivalence and distinctions mathematically and empirically. Building upon this comparative analysis, we introduce a unified framework for a comprehensive investigation, which in",
    "path": "papers/24/03/2403.00812.json",
    "total_tokens": 829,
    "translated_title": "LoRA在统一框架下遇见了Dropout",
    "translated_abstract": "具有显著能力的大型语言模型（LLMs）已成为许多自然语言处理应用中不可或缺的元素，而参数高效微调，特别是LoRA，已经成为模型定制的轻量级方法的流行选择。同时，各种dropout方法最初是为所有参数进行完整微调而设计的，有助于减轻与过多参数冗余相关的过拟合问题。因此，LoRA的可训练参数微不足道与先前dropout方法的有效性之间存在可能的矛盾，这一点之前大多被忽视。为填补这一空白，我们首先确认高效参数的LoRA也容易出现过拟合问题。然后，我们重新审视特定于transformer的dropout方法，从数学和经验上建立它们的等价性和区别。基于这种比较分析，我们引入了一个统一框架进行全面研究，",
    "tldr": "LoRA是一个轻量级的参数高效微调方法，该论文研究了LoRA与dropout方法在模型定制中的矛盾，重新审视了transformer-specific的dropout方法，并建立了它们之间的数学和经验上的等价性和区别。",
    "en_tdlr": "LoRA is a lightweight parameter-efficient finetuning approach. The paper investigates the contradiction between LoRA and dropout methods in model customization, re-evaluates transformer-specific dropout methods, and establishes their mathematical and empirical equivalence and distinctions."
}