{
    "title": "Efficient Language Model Architectures for Differentially Private Federated Learning",
    "abstract": "arXiv:2403.08100v1 Announce Type: new  Abstract: Cross-device federated learning (FL) is a technique that trains a model on data distributed across typically millions of edge devices without data leaving the devices. SGD is the standard client optimizer for on device training in cross-device FL, favored for its memory and computational efficiency. However, in centralized training of neural language models, adaptive optimizers are preferred as they offer improved stability and performance. In light of this, we ask if language models can be modified such that they can be efficiently trained with SGD client optimizers and answer this affirmatively.   We propose a scale-invariant Coupled Input Forget Gate (SI CIFG) recurrent network by modifying the sigmoid and tanh activations in the recurrent cell and show that this new model converges faster and achieves better utility than the standard CIFG recurrent model in cross-device FL in large scale experiments. We further show that the proposed",
    "link": "https://arxiv.org/abs/2403.08100",
    "context": "Title: Efficient Language Model Architectures for Differentially Private Federated Learning\nAbstract: arXiv:2403.08100v1 Announce Type: new  Abstract: Cross-device federated learning (FL) is a technique that trains a model on data distributed across typically millions of edge devices without data leaving the devices. SGD is the standard client optimizer for on device training in cross-device FL, favored for its memory and computational efficiency. However, in centralized training of neural language models, adaptive optimizers are preferred as they offer improved stability and performance. In light of this, we ask if language models can be modified such that they can be efficiently trained with SGD client optimizers and answer this affirmatively.   We propose a scale-invariant Coupled Input Forget Gate (SI CIFG) recurrent network by modifying the sigmoid and tanh activations in the recurrent cell and show that this new model converges faster and achieves better utility than the standard CIFG recurrent model in cross-device FL in large scale experiments. We further show that the proposed",
    "path": "papers/24/03/2403.08100.json",
    "total_tokens": 856,
    "translated_title": "用于差分隐私联邦学习的高效语言模型架构",
    "translated_abstract": "跨设备联邦学习(FL)是一种在通常分布在数百万台边缘设备上的数据上训练模型的技术，而数据不离开设备。 SGD是交叉设备FL中标准的客户端优化器，因其内存和计算效率而受青睐。然而，在神经语言模型的集中式训练中，自适应优化器被认为更稳定和性能更好。鉴于此，我们想知道是否可以修改语言模型，使其可以通过SGD客户端优化器进行高效训练，并肯定地回答了这个问题。我们提出了一个具有尺度不变性的耦合输入遗忘门(SI CIFG)递归网络，通过修改循环单元中的Sigmoid和tanh激活，并展示这个新模型在大规模实验中比标准CIFG递归模型更快地收敛并实现更好的效用。",
    "tldr": "提出了一个具有尺度不变性的耦合输入遗忘门递归网络，通过修改循环单元中的激活函数，使其能够更快地收敛并在跨设备联邦学习中取得更好的效果。",
    "en_tdlr": "Proposed a scale-invariant Coupled Input Forget Gate recurrent network by modifying the activation functions in the recurrent cell, enabling faster convergence and better performance in cross-device federated learning."
}