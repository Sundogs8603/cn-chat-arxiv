{
    "title": "\"Lossless\" Compression of Deep Neural Networks: A High-dimensional Neural Tangent Kernel Approach",
    "abstract": "arXiv:2403.00258v1 Announce Type: cross  Abstract: Modern deep neural networks (DNNs) are extremely powerful; however, this comes at the price of increased depth and having more parameters per layer, making their training and inference more computationally challenging. In an attempt to address this key limitation, efforts have been devoted to the compression (e.g., sparsification and/or quantization) of these large-scale machine learning models, so that they can be deployed on low-power IoT devices. In this paper, building upon recent advances in neural tangent kernel (NTK) and random matrix theory (RMT), we provide a novel compression approach to wide and fully-connected \\emph{deep} neural nets. Specifically, we demonstrate that in the high-dimensional regime where the number of data points $n$ and their dimension $p$ are both large, and under a Gaussian mixture model for the data, there exists \\emph{asymptotic spectral equivalence} between the NTK matrices for a large family of DNN m",
    "link": "https://arxiv.org/abs/2403.00258",
    "context": "Title: \"Lossless\" Compression of Deep Neural Networks: A High-dimensional Neural Tangent Kernel Approach\nAbstract: arXiv:2403.00258v1 Announce Type: cross  Abstract: Modern deep neural networks (DNNs) are extremely powerful; however, this comes at the price of increased depth and having more parameters per layer, making their training and inference more computationally challenging. In an attempt to address this key limitation, efforts have been devoted to the compression (e.g., sparsification and/or quantization) of these large-scale machine learning models, so that they can be deployed on low-power IoT devices. In this paper, building upon recent advances in neural tangent kernel (NTK) and random matrix theory (RMT), we provide a novel compression approach to wide and fully-connected \\emph{deep} neural nets. Specifically, we demonstrate that in the high-dimensional regime where the number of data points $n$ and their dimension $p$ are both large, and under a Gaussian mixture model for the data, there exists \\emph{asymptotic spectral equivalence} between the NTK matrices for a large family of DNN m",
    "path": "papers/24/03/2403.00258.json",
    "total_tokens": 910,
    "translated_title": "“无损”压缩深度神经网络：一种高维神经切线核方法",
    "translated_abstract": "现代深度神经网络（DNNs）非常强大；然而，这是以增加深度并使每一层的参数更多为代价的，这使得它们的训练和推断变得更具挑战性。为了应对这一关键限制，人们致力于对这些大规模机器学习模型进行压缩（如稀疏化和/或量化），以便它们可以部署在低功耗的物联网设备上。本文基于神经切线核（NTK）和随机矩阵理论（RMT）的最新进展，提出了一种新颖的压缩方法，适用于宽而全连接的\\emph{深}神经网络。具体而言，我们展示了在高维度情形下，当数据点的数量$n$和它们的维度$p$都很大，并且数据遵循高斯混合模型时，对于一大类DNN，NTK矩阵之间存在\\emph{渐近谱等价}。",
    "tldr": "本研究利用神经切线核和随机矩阵理论，提出了一种新颖的压缩方法，能够在高维度情形下对宽而全连接的深度神经网络进行有效压缩。"
}