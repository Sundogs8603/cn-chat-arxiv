{
    "title": "Online Policy Learning from Offline Preferences",
    "abstract": "arXiv:2403.10160v1 Announce Type: new  Abstract: In preference-based reinforcement learning (PbRL), a reward function is learned from a type of human feedback called preference. To expedite preference collection, recent works have leveraged \\emph{offline preferences}, which are preferences collected for some offline data. In this scenario, the learned reward function is fitted on the offline data. If a learning agent exhibits behaviors that do not overlap with the offline data, the learned reward function may encounter generalizability issues. To address this problem, the present study introduces a framework that consolidates offline preferences and \\emph{virtual preferences} for PbRL, which are comparisons between the agent's behaviors and the offline data. Critically, the reward function can track the agent's behaviors using the virtual preferences, thereby offering well-aligned guidance to the agent. Through experiments on continuous control tasks, this study demonstrates the effect",
    "link": "https://arxiv.org/abs/2403.10160",
    "context": "Title: Online Policy Learning from Offline Preferences\nAbstract: arXiv:2403.10160v1 Announce Type: new  Abstract: In preference-based reinforcement learning (PbRL), a reward function is learned from a type of human feedback called preference. To expedite preference collection, recent works have leveraged \\emph{offline preferences}, which are preferences collected for some offline data. In this scenario, the learned reward function is fitted on the offline data. If a learning agent exhibits behaviors that do not overlap with the offline data, the learned reward function may encounter generalizability issues. To address this problem, the present study introduces a framework that consolidates offline preferences and \\emph{virtual preferences} for PbRL, which are comparisons between the agent's behaviors and the offline data. Critically, the reward function can track the agent's behaviors using the virtual preferences, thereby offering well-aligned guidance to the agent. Through experiments on continuous control tasks, this study demonstrates the effect",
    "path": "papers/24/03/2403.10160.json",
    "total_tokens": 845,
    "translated_title": "从离线偏好中学习在线策略",
    "translated_abstract": "在基于偏好的强化学习（PbRL）中，人类反馈被称为偏好，用于学习奖励函数。为了加快偏好收集的速度，最近的研究利用了离线偏好，即收集用于某些离线数据的偏好。在这种情况下，学习的奖励函数适合于离线数据。如果学习代理展现出与离线数据不重叠的行为，学习的奖励函数可能会遇到泛化问题。为了解决这个问题，本研究引入了一个框架，将离线偏好和虚拟偏好结合起来，用于PbRL，虚拟偏好是代理的行为与离线数据之间的比较。关键是，奖励函数可以使用虚拟偏好追踪代理的行为，从而为代理提供良好对齐的指导。通过连续控制任务的实验，本研究展示了效果。",
    "tldr": "引入了一个框架，将离线偏好和虚拟偏好结合起来用于基于偏好的强化学习，以解决在线学习中奖励函数泛化性问题。",
    "en_tdlr": "Introducing a framework that combines offline preferences with virtual preferences for preference-based reinforcement learning to address the reward function generalizability issues in online learning."
}