{
    "title": "Fine-tuning Large Language Models with Sequential Instructions",
    "abstract": "arXiv:2403.07794v1 Announce Type: new  Abstract: Large language models (LLMs) struggle to follow a sequence of instructions in a single query as they may ignore or misinterpret part of it. This impairs their performance in complex problems whose solution requires multiple intermediate steps, such as multilingual (translate then answer) and multimodal (caption then answer) tasks. We empirically verify this with open-source LLMs as large as LLaMA-2 70B and Mixtral-8x7B. Targeting the scarcity of sequential instructions in present-day data, we propose sequential instruction tuning, a simple yet effective strategy to automatically augment instruction tuning data and equip LLMs with the ability to execute multiple sequential instructions. After exploring interleaving instructions in existing datasets, such as Alpaca, with a wide range of intermediate tasks, we find that sequential instruction-tuned models consistently outperform the conventional instruction-tuned baselines in downstream tas",
    "link": "https://arxiv.org/abs/2403.07794",
    "context": "Title: Fine-tuning Large Language Models with Sequential Instructions\nAbstract: arXiv:2403.07794v1 Announce Type: new  Abstract: Large language models (LLMs) struggle to follow a sequence of instructions in a single query as they may ignore or misinterpret part of it. This impairs their performance in complex problems whose solution requires multiple intermediate steps, such as multilingual (translate then answer) and multimodal (caption then answer) tasks. We empirically verify this with open-source LLMs as large as LLaMA-2 70B and Mixtral-8x7B. Targeting the scarcity of sequential instructions in present-day data, we propose sequential instruction tuning, a simple yet effective strategy to automatically augment instruction tuning data and equip LLMs with the ability to execute multiple sequential instructions. After exploring interleaving instructions in existing datasets, such as Alpaca, with a wide range of intermediate tasks, we find that sequential instruction-tuned models consistently outperform the conventional instruction-tuned baselines in downstream tas",
    "path": "papers/24/03/2403.07794.json",
    "total_tokens": 845,
    "translated_title": "使用顺序指令对大型语言模型进行微调",
    "translated_abstract": "大型语言模型（LLMs）在单个查询中遵循一系列指令时往往会忽略或误解其中的一部分，这影响了它们在解决需要多个中间步骤的复杂问题中的性能，例如多语言（先翻译再回答）和多模态（标题后回答）任务。我们通过开源LLMs（如LLaMA-2 70B和Mixtral-8x7B）的实证验证了这一点。针对当前数据中顺序指令稀缺的问题，我们提出了顺序指令微调，这是一种简单而有效的策略，可以自动增加指令调整数据，使LLMs具备执行多个顺序指令的能力。在探索现有数据集（如Alpaca）中插入指令并进行一系列中间任务后，我们发现，顺序指令微调的模型在下游任务中始终优于传统的指令微调基线。",
    "tldr": "通过顺序指令微调，研究提出了一种简单且有效的策略，可以使大型语言模型具备执行多个顺序指令的能力，优于传统指令微调模型。",
    "en_tdlr": "The study introduces a simple and effective strategy of sequential instruction tuning to equip large language models with the capability of executing multiple sequential instructions, outperforming traditional instruction-tuned models."
}