{
    "title": "HyperLLaVA: Dynamic Visual and Language Expert Tuning for Multimodal Large Language Models",
    "abstract": "arXiv:2403.13447v1 Announce Type: cross  Abstract: Recent advancements indicate that scaling up Multimodal Large Language Models (MLLMs) effectively enhances performance on downstream multimodal tasks. The prevailing MLLM paradigm, \\emph{e.g.}, LLaVA, transforms visual features into text-like tokens using a \\emph{static} vision-language mapper, thereby enabling \\emph{static} LLMs to develop the capability to comprehend visual information through visual instruction tuning. Although promising, the \\emph{static} tuning strategy~\\footnote{The static tuning refers to the trained model with static parameters.} that shares the same parameters may constrain performance across different downstream multimodal tasks. In light of this, we introduce HyperLLaVA, which involves adaptive tuning of the projector and LLM parameters, in conjunction with a dynamic visual expert and language expert, respectively. These experts are derived from HyperNetworks, which generates adaptive parameter shifts throug",
    "link": "https://arxiv.org/abs/2403.13447",
    "context": "Title: HyperLLaVA: Dynamic Visual and Language Expert Tuning for Multimodal Large Language Models\nAbstract: arXiv:2403.13447v1 Announce Type: cross  Abstract: Recent advancements indicate that scaling up Multimodal Large Language Models (MLLMs) effectively enhances performance on downstream multimodal tasks. The prevailing MLLM paradigm, \\emph{e.g.}, LLaVA, transforms visual features into text-like tokens using a \\emph{static} vision-language mapper, thereby enabling \\emph{static} LLMs to develop the capability to comprehend visual information through visual instruction tuning. Although promising, the \\emph{static} tuning strategy~\\footnote{The static tuning refers to the trained model with static parameters.} that shares the same parameters may constrain performance across different downstream multimodal tasks. In light of this, we introduce HyperLLaVA, which involves adaptive tuning of the projector and LLM parameters, in conjunction with a dynamic visual expert and language expert, respectively. These experts are derived from HyperNetworks, which generates adaptive parameter shifts throug",
    "path": "papers/24/03/2403.13447.json",
    "total_tokens": 838,
    "translated_title": "HyperLLaVA: 多模态大型语言模型的动态视觉和语言专家调优",
    "translated_abstract": "最近的进展表明，扩展多模态大型语言模型(MLLMs)有效地提升了在下游多模态任务上的性能。当前的MLLM范式，如LLaVA，通过使用静态视觉-语言映射器将视觉特征转换为类似文本的标记，从而使静态LLMs通过视觉指令调优获得理解视觉信息的能力。尽管有所希望，但相同参数的静态调优策略可能限制不同下游多模态任务的性能。鉴于此，我们介绍了HyperLLaVA，其中包括投影仪和LLM参数的自适应调整，以及动态视觉专家和语言专家。这些专家源自HyperNetworks，它通过生成自适应参数偏移来实现",
    "tldr": "HyperLLaVA通过引入自适应调整的投影仪和LLM参数，以及动态的视觉专家和语言专家，从而弥补了静态调整策略在不同下游多模态任务上性能受限的不足。",
    "en_tdlr": "HyperLLaVA addresses the limitation of static tuning strategy on performance across different downstream multimodal tasks by introducing adaptive tuning of the projector and LLM parameters, along with dynamic visual expert and language expert."
}