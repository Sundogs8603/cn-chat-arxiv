{
    "title": "VDSC: Enhancing Exploration Timing with Value Discrepancy and State Counts",
    "abstract": "arXiv:2403.17542v1 Announce Type: cross  Abstract: Despite the considerable attention given to the questions of \\textit{how much} and \\textit{how to} explore in deep reinforcement learning, the investigation into \\textit{when} to explore remains relatively less researched. While more sophisticated exploration strategies can excel in specific, often sparse reward environments, existing simpler approaches, such as $\\epsilon$-greedy, persist in outperforming them across a broader spectrum of domains. The appeal of these simpler strategies lies in their ease of implementation and generality across a wide range of domains. The downside is that these methods are essentially a blind switching mechanism, which completely disregards the agent's internal state. In this paper, we propose to leverage the agent's internal state to decide \\textit{when} to explore, addressing the shortcomings of blind switching mechanisms. We present Value Discrepancy and State Counts through homeostasis (VDSC), a no",
    "link": "https://arxiv.org/abs/2403.17542",
    "context": "Title: VDSC: Enhancing Exploration Timing with Value Discrepancy and State Counts\nAbstract: arXiv:2403.17542v1 Announce Type: cross  Abstract: Despite the considerable attention given to the questions of \\textit{how much} and \\textit{how to} explore in deep reinforcement learning, the investigation into \\textit{when} to explore remains relatively less researched. While more sophisticated exploration strategies can excel in specific, often sparse reward environments, existing simpler approaches, such as $\\epsilon$-greedy, persist in outperforming them across a broader spectrum of domains. The appeal of these simpler strategies lies in their ease of implementation and generality across a wide range of domains. The downside is that these methods are essentially a blind switching mechanism, which completely disregards the agent's internal state. In this paper, we propose to leverage the agent's internal state to decide \\textit{when} to explore, addressing the shortcomings of blind switching mechanisms. We present Value Discrepancy and State Counts through homeostasis (VDSC), a no",
    "path": "papers/24/03/2403.17542.json",
    "total_tokens": 846,
    "translated_title": "VDSC：利用价值差异和状态计数增强探索时间",
    "translated_abstract": "尽管深度强化学习中对于“探索多少”和“如何探索”问题受到了相当大的关注，但对于“何时”探索的研究相对较少。在更复杂的探索策略可以在特定的、通常稀疏的奖励环境中表现出色的同时，现有的简单方法，如$\\epsilon$-贪心，在更广泛的领域中继续表现优异。这些简单策略的吸引力在于它们的易实现性和对各种领域的普遍适用性。然而，这些方法的缺点在于它们本质上是一种盲目的切换机制，完全忽略了代理的内部状态。本文提出利用代理的内部状态来决定“何时”进行探索，从而解决盲目切换机制的缺点。我们通过稳态（VDSC）提出了价值差异和状态计数。",
    "tldr": "通过价值差异和状态计数，利用代理的内部状态来决定何时进行探索，解决了盲目切换机制的缺点。",
    "en_tdlr": "Leveraging the agent's internal state to decide when to explore using Value Discrepancy and State Counts addresses the shortcomings of blind switching mechanisms."
}