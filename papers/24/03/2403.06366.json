{
    "title": "Finite-Time Error Analysis of Soft Q-Learning: Switching System Approach",
    "abstract": "arXiv:2403.06366v1 Announce Type: new  Abstract: Soft Q-learning is a variation of Q-learning designed to solve entropy regularized Markov decision problems where an agent aims to maximize the entropy regularized value function. Despite its empirical success, there have been limited theoretical studies of soft Q-learning to date. This paper aims to offer a novel and unified finite-time, control-theoretic analysis of soft Q-learning algorithms. We focus on two types of soft Q-learning algorithms: one utilizing the log-sum-exp operator and the other employing the Boltzmann operator. By using dynamical switching system models, we derive novel finite-time error bounds for both soft Q-learning algorithms. We hope that our analysis will deepen the current understanding of soft Q-learning by establishing connections with switching system models and may even pave the way for new frameworks in the finite-time analysis of other reinforcement learning algorithms.",
    "link": "https://arxiv.org/abs/2403.06366",
    "context": "Title: Finite-Time Error Analysis of Soft Q-Learning: Switching System Approach\nAbstract: arXiv:2403.06366v1 Announce Type: new  Abstract: Soft Q-learning is a variation of Q-learning designed to solve entropy regularized Markov decision problems where an agent aims to maximize the entropy regularized value function. Despite its empirical success, there have been limited theoretical studies of soft Q-learning to date. This paper aims to offer a novel and unified finite-time, control-theoretic analysis of soft Q-learning algorithms. We focus on two types of soft Q-learning algorithms: one utilizing the log-sum-exp operator and the other employing the Boltzmann operator. By using dynamical switching system models, we derive novel finite-time error bounds for both soft Q-learning algorithms. We hope that our analysis will deepen the current understanding of soft Q-learning by establishing connections with switching system models and may even pave the way for new frameworks in the finite-time analysis of other reinforcement learning algorithms.",
    "path": "papers/24/03/2403.06366.json",
    "total_tokens": 808,
    "translated_title": "软Q-learning的有限时间误差分析：切换系统方法",
    "translated_abstract": "Soft Q-learning是Q-learning的一种变体，旨在解决熵正则化马尔可夫决策问题，其中代理的目标是最大化熵正则化值函数。尽管在经验上取得成功，但迄今为止对软Q-learning的理论研究有限。本文旨在提供对软Q-learning算法的新颖和统一的有限时间、控制论分析。我们专注于两种类型的软Q-learning算法：一种利用对数和指数运算子，另一种采用玻尔兹曼运算子。通过使用动态切换系统模型，我们为两种软Q-learning算法推导出了新颖的有限时间误差界限。我们希望我们的分析能够通过与切换系统模型建立联系来加深对软Q-learning的当前理解，甚至为其他强化学习算法的有限时间分析的新框架铺平道路。",
    "tldr": "本文通过切换系统模型，针对软Q-learning算法进行了有限时间误差分析，为两种软Q-learning算法导出了新颖的误差界限。"
}