{
    "title": "Going Beyond Word Matching: Syntax Improves In-context Example Selection for Machine Translation",
    "abstract": "arXiv:2403.19285v1 Announce Type: new  Abstract: In-context learning (ICL) is the trending prompting strategy in the era of large language models (LLMs), where a few examples are demonstrated to evoke LLMs' power for a given task. How to select informative examples remains an open issue. Previous works on in-context example selection for machine translation (MT) focus on superficial word-level features while ignoring deep syntax-level knowledge. In this paper, we propose a syntax-based in-context example selection method for MT, by computing the syntactic similarity between dependency trees using Polynomial Distance. In addition, we propose an ensemble strategy combining examples selected by both word-level and syntax-level criteria. Experimental results between English and 6 common languages indicate that syntax can effectively enhancing ICL for MT, obtaining the highest COMET scores on 11 out of 12 translation directions.",
    "link": "https://arxiv.org/abs/2403.19285",
    "context": "Title: Going Beyond Word Matching: Syntax Improves In-context Example Selection for Machine Translation\nAbstract: arXiv:2403.19285v1 Announce Type: new  Abstract: In-context learning (ICL) is the trending prompting strategy in the era of large language models (LLMs), where a few examples are demonstrated to evoke LLMs' power for a given task. How to select informative examples remains an open issue. Previous works on in-context example selection for machine translation (MT) focus on superficial word-level features while ignoring deep syntax-level knowledge. In this paper, we propose a syntax-based in-context example selection method for MT, by computing the syntactic similarity between dependency trees using Polynomial Distance. In addition, we propose an ensemble strategy combining examples selected by both word-level and syntax-level criteria. Experimental results between English and 6 common languages indicate that syntax can effectively enhancing ICL for MT, obtaining the highest COMET scores on 11 out of 12 translation directions.",
    "path": "papers/24/03/2403.19285.json",
    "total_tokens": 941,
    "translated_title": "超越词语匹配：句法改善上下文例句选择以提高机器翻译质量",
    "translated_abstract": "arXiv:2403.19285v1 公告类型: 新的 摘要: 在大语言模型（LLMs）时代，上下文学习（ICL）是一种流行的提示策略，其中展示了一些示例以唤起LLMs在给定任务上的能力。如何选择信息量大的例句仍然是一个悬而未决的问题。先前关于机器翻译（MT）的上下文例句选择的作品侧重于表面的词级特征，而忽略了深层次的句法层次知识。在本文中，我们提出了一种基于语法的机器翻译上下文例句选择方法，通过使用多项式距离计算依赖树之间的句法相似性。此外，我们提出了一种综合策略，将通过词级和句法水平标准选择的例句进行组合。对英语和6种常见语言之间的实验结果表明，语法可以有效提升MT的ICL，获得了在12个翻译方向中11个方向上最高的COMET分数。",
    "tldr": "本文提出了一种基于句法的机器翻译上下文例句选择方法，通过计算依存树之间的句法相似性，结合词级和句法水平标准选择例句，实验结果表明语法可以有效提升机器翻译上下文学习质量。",
    "en_tdlr": "This paper proposes a syntax-based method for in-context example selection in machine translation, which computes syntactic similarity between dependency trees and combines examples selected by word-level and syntax-level criteria. Experimental results show that syntax can effectively enhance in-context learning for machine translation."
}