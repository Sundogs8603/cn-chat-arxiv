{
    "title": "LM2D: Lyrics- and Music-Driven Dance Synthesis",
    "abstract": "arXiv:2403.09407v1 Announce Type: cross  Abstract: Dance typically involves professional choreography with complex movements that follow a musical rhythm and can also be influenced by lyrical content. The integration of lyrics in addition to the auditory dimension, enriches the foundational tone and makes motion generation more amenable to its semantic meanings. However, existing dance synthesis methods tend to model motions only conditioned on audio signals. In this work, we make two contributions to bridge this gap. First, we propose LM2D, a novel probabilistic architecture that incorporates a multimodal diffusion model with consistency distillation, designed to create dance conditioned on both music and lyrics in one diffusion generation step. Second, we introduce the first 3D dance-motion dataset that encompasses both music and lyrics, obtained with pose estimation technologies. We evaluate our model against music-only baseline models with objective metrics and human evaluations, i",
    "link": "https://arxiv.org/abs/2403.09407",
    "context": "Title: LM2D: Lyrics- and Music-Driven Dance Synthesis\nAbstract: arXiv:2403.09407v1 Announce Type: cross  Abstract: Dance typically involves professional choreography with complex movements that follow a musical rhythm and can also be influenced by lyrical content. The integration of lyrics in addition to the auditory dimension, enriches the foundational tone and makes motion generation more amenable to its semantic meanings. However, existing dance synthesis methods tend to model motions only conditioned on audio signals. In this work, we make two contributions to bridge this gap. First, we propose LM2D, a novel probabilistic architecture that incorporates a multimodal diffusion model with consistency distillation, designed to create dance conditioned on both music and lyrics in one diffusion generation step. Second, we introduce the first 3D dance-motion dataset that encompasses both music and lyrics, obtained with pose estimation technologies. We evaluate our model against music-only baseline models with objective metrics and human evaluations, i",
    "path": "papers/24/03/2403.09407.json",
    "total_tokens": 898,
    "translated_title": "LM2D：以歌词和音乐驱动的舞蹈合成",
    "translated_abstract": "舞蹈通常涉及专业编舞，包含按照音乐节奏进行的复杂动作，还可能受到歌词内容的影响。将歌词整合到听觉维度之外，丰富了基础音色，并使运动生成更易于语义含义。然而，现有的舞蹈合成方法往往只建模于音频信号。在这项工作中，我们做出了两点贡献，首先，我们提出了LM2D，这是一个结合了多模态扩散模型和一致性蒸馏的新颖概率架构，旨在通过一次扩散生成步骤在音乐和歌词的条件下创建舞蹈。其次，我们介绍了第一个涵盖音乐和歌词的3D舞蹈运动数据集，通过姿势估计技术获得。我们通过客观指标和人类评价将我们的模型与仅有音乐的基线模型进行评估。",
    "tldr": "提出了LM2D，一种结合了多模态扩散模型和一致性蒸馏的新颖概率架构，旨在在音乐和歌词条件下进行舞蹈生成；介绍了第一个涵盖音乐和歌词的3D舞蹈运动数据集。",
    "en_tdlr": "Proposed LM2D, a novel probabilistic architecture that combines a multimodal diffusion model with consistency distillation for dance generation conditioned on both music and lyrics; introduced the first 3D dance-motion dataset that encompasses both music and lyrics."
}