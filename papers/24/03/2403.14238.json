{
    "title": "Reinforcement Learning from Reflective Feedback (RLRF): Aligning and Improving LLMs via Fine-Grained Self-Reflection",
    "abstract": "arXiv:2403.14238v1 Announce Type: cross  Abstract: Despite the promise of RLHF in aligning LLMs with human preferences, it often leads to superficial alignment, prioritizing stylistic changes over improving downstream performance of LLMs. Underspecified preferences could obscure directions to align the models. Lacking exploration restricts identification of desirable outputs to improve the models. To overcome these challenges, we propose a novel framework: Reinforcement Learning from Reflective Feedback (RLRF), which leverages fine-grained feedback based on detailed criteria to improve the core capabilities of LLMs. RLRF employs a self-reflection mechanism to systematically explore and refine LLM responses, then fine-tuning the models via a RL algorithm along with promising responses. Our experiments across Just-Eval, Factuality, and Mathematical Reasoning demonstrate the efficacy and transformative potential of RLRF beyond superficial surface-level adjustment.",
    "link": "https://arxiv.org/abs/2403.14238",
    "context": "Title: Reinforcement Learning from Reflective Feedback (RLRF): Aligning and Improving LLMs via Fine-Grained Self-Reflection\nAbstract: arXiv:2403.14238v1 Announce Type: cross  Abstract: Despite the promise of RLHF in aligning LLMs with human preferences, it often leads to superficial alignment, prioritizing stylistic changes over improving downstream performance of LLMs. Underspecified preferences could obscure directions to align the models. Lacking exploration restricts identification of desirable outputs to improve the models. To overcome these challenges, we propose a novel framework: Reinforcement Learning from Reflective Feedback (RLRF), which leverages fine-grained feedback based on detailed criteria to improve the core capabilities of LLMs. RLRF employs a self-reflection mechanism to systematically explore and refine LLM responses, then fine-tuning the models via a RL algorithm along with promising responses. Our experiments across Just-Eval, Factuality, and Mathematical Reasoning demonstrate the efficacy and transformative potential of RLRF beyond superficial surface-level adjustment.",
    "path": "papers/24/03/2403.14238.json",
    "total_tokens": 853,
    "translated_title": "反思反馈中的强化学习（RLRF）：通过细粒度自我反思对LLMs进行调整和改进",
    "translated_abstract": "尽管RLHF在将LLMs与人类偏好进行调整方面很有前景，但往往会导致表面调整，优先考虑风格变化而非改善LLMs的下游性能。未明确规定的偏好可能会模糊对齐模型的方向。缺乏探索会限制识别出改善模型的理想输出。为了克服这些挑战，我们提出了一个新颖的框架：反思反馈中的强化学习（RLRF），该框架利用基于详细标准的细粒度反馈，以改进LLMs的核心能力。RLRF采用自我反思机制系统地探索和完善LLMs的回应，然后通过RL算法对模型进行微调，同时结合有希望的回应。在Just-Eval、Factuality和Mathematical Reasoning等实验中，我们展示了RLRF的功效和超越表面调整的转变潜力。",
    "tldr": "RLRF提出了一种新颖的框架，通过细粒度反馈和自我反思机制，可以改进LLMs的核心能力，超越表面调整。",
    "en_tdlr": "RLRF introduces a novel framework that improves the core capabilities of LLMs beyond superficial adjustment, leveraging fine-grained feedback and a self-reflection mechanism."
}