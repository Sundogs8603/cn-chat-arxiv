{
    "title": "Vanilla Transformers are Transfer Capability Teachers",
    "abstract": "arXiv:2403.01994v1 Announce Type: new  Abstract: Recently, Mixture of Experts (MoE) Transformers have garnered increasing attention due to their advantages in model capacity and computational efficiency. However, studies have indicated that MoE Transformers underperform vanilla Transformers in many downstream tasks, significantly diminishing the practical value of MoE models. To explain this issue, we propose that the pre-training performance and transfer capability of a model are joint determinants of its downstream task performance. MoE models, in comparison to vanilla models, have poorer transfer capability, leading to their subpar performance in downstream tasks. To address this issue, we introduce the concept of transfer capability distillation, positing that although vanilla models have weaker performance, they are effective teachers of transfer capability. The MoE models guided by vanilla models can achieve both strong pre-training performance and transfer capability, ultimately",
    "link": "https://arxiv.org/abs/2403.01994",
    "context": "Title: Vanilla Transformers are Transfer Capability Teachers\nAbstract: arXiv:2403.01994v1 Announce Type: new  Abstract: Recently, Mixture of Experts (MoE) Transformers have garnered increasing attention due to their advantages in model capacity and computational efficiency. However, studies have indicated that MoE Transformers underperform vanilla Transformers in many downstream tasks, significantly diminishing the practical value of MoE models. To explain this issue, we propose that the pre-training performance and transfer capability of a model are joint determinants of its downstream task performance. MoE models, in comparison to vanilla models, have poorer transfer capability, leading to their subpar performance in downstream tasks. To address this issue, we introduce the concept of transfer capability distillation, positing that although vanilla models have weaker performance, they are effective teachers of transfer capability. The MoE models guided by vanilla models can achieve both strong pre-training performance and transfer capability, ultimately",
    "path": "papers/24/03/2403.01994.json",
    "total_tokens": 899,
    "translated_title": "香草变压器是迁移能力教师",
    "translated_abstract": "最近，由于在模型容量和计算效率方面的优势，混合专家（MoE）变压器引起了越来越多的关注。然而，研究表明，在许多下游任务中，MoE变压器的表现不及香草变压器，这显著降低了MoE模型的实用价值。为了解释这个问题，我们提出模型的预训练性能和迁移能力是影响其下游任务性能的联合决定因素。与香草模型相比，MoE模型的迁移能力较差，导致它们在下游任务中表现不佳。为了解决这个问题，我们引入了迁移能力蒸馏的概念，认为虽然香草模型性能较弱，但它们是迁移能力的有效教师。由香草模型指导的MoE模型可以实现强大的预训练性能和迁移能力，最终",
    "tldr": "混合专家（MoE）变压器在模型预训练性能和传输能力方面表现不如香草变压器，为此提出了迁移能力蒸馏的概念，指出香草模型是迁移能力的有效教师，指导MoE模型实现预训练性能和传输能力的结合。",
    "en_tdlr": "Mixture of Experts (MoE) Transformers fall behind vanilla Transformers in pre-training performance and transfer capability, leading to the proposal of transfer capability distillation where vanilla models serve as effective teachers guiding MoE models to achieve a balance between pre-training performance and transfer capability."
}