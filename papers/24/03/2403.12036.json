{
    "title": "One-Step Image Translation with Text-to-Image Models",
    "abstract": "arXiv:2403.12036v1 Announce Type: cross  Abstract: In this work, we address two limitations of existing conditional diffusion models: their slow inference speed due to the iterative denoising process and their reliance on paired data for model fine-tuning. To tackle these issues, we introduce a general method for adapting a single-step diffusion model to new tasks and domains through adversarial learning objectives. Specifically, we consolidate various modules of the vanilla latent diffusion model into a single end-to-end generator network with small trainable weights, enhancing its ability to preserve the input image structure while reducing overfitting. We demonstrate that, for unpaired settings, our model CycleGAN-Turbo outperforms existing GAN-based and diffusion-based methods for various scene translation tasks, such as day-to-night conversion and adding/removing weather effects like fog, snow, and rain. We extend our method to paired settings, where our model pix2pix-Turbo is on ",
    "link": "https://arxiv.org/abs/2403.12036",
    "context": "Title: One-Step Image Translation with Text-to-Image Models\nAbstract: arXiv:2403.12036v1 Announce Type: cross  Abstract: In this work, we address two limitations of existing conditional diffusion models: their slow inference speed due to the iterative denoising process and their reliance on paired data for model fine-tuning. To tackle these issues, we introduce a general method for adapting a single-step diffusion model to new tasks and domains through adversarial learning objectives. Specifically, we consolidate various modules of the vanilla latent diffusion model into a single end-to-end generator network with small trainable weights, enhancing its ability to preserve the input image structure while reducing overfitting. We demonstrate that, for unpaired settings, our model CycleGAN-Turbo outperforms existing GAN-based and diffusion-based methods for various scene translation tasks, such as day-to-night conversion and adding/removing weather effects like fog, snow, and rain. We extend our method to paired settings, where our model pix2pix-Turbo is on ",
    "path": "papers/24/03/2403.12036.json",
    "total_tokens": 885,
    "translated_title": "一步图像翻译与文本到图像模型",
    "translated_abstract": "在这项工作中，我们解决了现有条件扩散模型的两个局限性：由于迭代去噪过程而导致的推断速度慢以及对配对数据进行模型微调的依赖。为了解决这些问题，我们引入了一种通过对抗学习目标将单步扩散模型调整到新任务和领域的通用方法。具体而言，我们将传统的潜在扩散模型各模块整合到一个具有小可训练权重的端到端生成器网络中，增强了其保留输入图像结构的能力，同时减少了过拟合。我们展示了，在无配对设置下，我们的模型 CycleGAN-Turbo 在各种场景翻译任务上优于现有的基于GAN和基于扩散的方法，如日夜转换以及添加/移除雾、雪和雨等天气效果。我们将我们的方法扩展到配对设置，我们的模型 pix2pix-Turbo 的性能",
    "tldr": "通过引入单步扩散模型并通过对抗学习目标调整到新任务和领域，我们提出了一种解决现有条件扩散模型局限性的通用方法，适用于无配对和配对设置下的图像翻译任务。"
}