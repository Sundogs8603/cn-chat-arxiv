{
    "title": "Robust and Scalable Model Editing for Large Language Models",
    "abstract": "arXiv:2403.17431v1 Announce Type: new  Abstract: Large language models (LLMs) can make predictions using parametric knowledge--knowledge encoded in the model weights--or contextual knowledge--knowledge presented in the context. In many scenarios, a desirable behavior is that LLMs give precedence to contextual knowledge when it conflicts with the parametric knowledge, and fall back to using their parametric knowledge when the context is irrelevant. This enables updating and correcting the model's knowledge by in-context editing instead of retraining. Previous works have shown that LLMs are inclined to ignore contextual knowledge and fail to reliably fall back to parametric knowledge when presented with irrelevant context. In this work, we discover that, with proper prompting methods, instruction-finetuned LLMs can be highly controllable by contextual knowledge and robust to irrelevant context. Utilizing this feature, we propose EREN (Edit models by REading Notes) to improve the scalabil",
    "link": "https://arxiv.org/abs/2403.17431",
    "context": "Title: Robust and Scalable Model Editing for Large Language Models\nAbstract: arXiv:2403.17431v1 Announce Type: new  Abstract: Large language models (LLMs) can make predictions using parametric knowledge--knowledge encoded in the model weights--or contextual knowledge--knowledge presented in the context. In many scenarios, a desirable behavior is that LLMs give precedence to contextual knowledge when it conflicts with the parametric knowledge, and fall back to using their parametric knowledge when the context is irrelevant. This enables updating and correcting the model's knowledge by in-context editing instead of retraining. Previous works have shown that LLMs are inclined to ignore contextual knowledge and fail to reliably fall back to parametric knowledge when presented with irrelevant context. In this work, we discover that, with proper prompting methods, instruction-finetuned LLMs can be highly controllable by contextual knowledge and robust to irrelevant context. Utilizing this feature, we propose EREN (Edit models by REading Notes) to improve the scalabil",
    "path": "papers/24/03/2403.17431.json",
    "total_tokens": 896,
    "translated_title": "大规模语言模型的鲁棒且可扩展的模型编辑",
    "translated_abstract": "大型语言模型（LLMs）可以使用参数化知识进行预测--即编码在模型权重中的知识--或者是上下文知识--即呈现在上下文中的知识。在许多场景下，一个理想的行为是当LLMs在参数化知识与上下文知识发生冲突时，优先考虑上下文知识，并在上下文无关时回退到使用他们的参数化知识。这使得通过上下文编辑来更新和纠正模型的知识成为可能，而无需重新训练。先前的研究表明，LLMs倾向于忽视上下文知识，并且在面对无关上下文时无法可靠地回退到参数化知识。在这项工作中，我们发现，通过适当的提示方法，经过指令微调的LLMs可以被上下文知识高度控制，并对无关上下文具有鲁棒性。利用这一特性，我们提出EREN（通过阅读笔记来编辑模型）来提高可扩展性。",
    "tldr": "通过适当的提示方法，经过指令微调的大型语言模型可以高度控制上下文知识，并对无关上下文具有鲁棒性，提出了EREN（通过阅读笔记来编辑模型），以改善可扩展性。",
    "en_tdlr": "Through proper prompting methods, instruction-finetuned large language models can be highly controllable by contextual knowledge and robust to irrelevant context, proposing EREN (Edit models by REading Notes) to improve scalability."
}