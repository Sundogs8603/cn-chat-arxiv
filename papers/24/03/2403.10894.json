{
    "title": "Towards Robustness and Diversity: Continual Learning in Dialog Generation with Text-Mixup and Batch Nuclear-Norm Maximization",
    "abstract": "arXiv:2403.10894v1 Announce Type: new  Abstract: In our dynamic world where data arrives in a continuous stream, continual learning enables us to incrementally add new tasks/domains without the need to retrain from scratch. A major challenge in continual learning of language model is catastrophic forgetting, the tendency of models to forget knowledge from previously trained tasks/domains when training on new ones. This paper studies dialog generation under the continual learning setting. We propose a novel method that 1) uses \\textit{Text-Mixup} as data augmentation to avoid model overfitting on replay memory and 2) leverages Batch-Nuclear Norm Maximization (BNNM) to alleviate the problem of mode collapse. Experiments on a $37$-domain task-oriented dialog dataset and DailyDialog (a $10$-domain chitchat dataset) demonstrate that our proposed approach outperforms the state-of-the-art in continual learning.",
    "link": "https://arxiv.org/abs/2403.10894",
    "context": "Title: Towards Robustness and Diversity: Continual Learning in Dialog Generation with Text-Mixup and Batch Nuclear-Norm Maximization\nAbstract: arXiv:2403.10894v1 Announce Type: new  Abstract: In our dynamic world where data arrives in a continuous stream, continual learning enables us to incrementally add new tasks/domains without the need to retrain from scratch. A major challenge in continual learning of language model is catastrophic forgetting, the tendency of models to forget knowledge from previously trained tasks/domains when training on new ones. This paper studies dialog generation under the continual learning setting. We propose a novel method that 1) uses \\textit{Text-Mixup} as data augmentation to avoid model overfitting on replay memory and 2) leverages Batch-Nuclear Norm Maximization (BNNM) to alleviate the problem of mode collapse. Experiments on a $37$-domain task-oriented dialog dataset and DailyDialog (a $10$-domain chitchat dataset) demonstrate that our proposed approach outperforms the state-of-the-art in continual learning.",
    "path": "papers/24/03/2403.10894.json",
    "total_tokens": 934,
    "translated_title": "朝向稳健性和多样性：使用文本混合和批量核范数最大化进行对话生成的继续学习",
    "translated_abstract": "在我们持续不断接收数据的动态世界中，继续学习使我们能够逐步添加新的任务/领域，而无需从头开始重新训练。在语言模型的继续学习中，一个主要挑战是灾难性遗忘，即模型在训练新任务/领域时忘记之前训练任务/领域中的知识的倾向。本文研究了在继续学习设置下的对话生成。我们提出了一种新颖方法，1）使用Text-Mixup作为数据增强，以避免模型在重放记忆上过拟合，2）利用批量核范数最大化（BNNM）来缓解模式崩溃问题。对一个包含37个领域的任务驱动对话数据集和DailyDialog（一个包含10个领域的闲聊数据集）的实验表明，我们提出的方法在继续学习方面胜过了现有技术。",
    "tldr": "本文探讨了在对话生成领域中的继续学习，提出了利用文本混合和批量核范数最大化的方法来解决灾难性遗忘和模式崩溃问题，实验证明这种方法在继续学习方面优于现有技术。",
    "en_tdlr": "This paper investigates continual learning in dialog generation, proposing a method using Text-Mixup and Batch-Nuclear Norm Maximization to address catastrophic forgetting and mode collapse issues, with experiments showing its superiority in continual learning."
}