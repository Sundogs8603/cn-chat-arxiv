{
    "title": "Transformers for Supervised Online Continual Learning",
    "abstract": "arXiv:2403.01554v1 Announce Type: new  Abstract: Transformers have become the dominant architecture for sequence modeling tasks such as natural language processing or audio processing, and they are now even considered for tasks that are not naturally sequential such as image classification. Their ability to attend to and to process a set of tokens as context enables them to develop in-context few-shot learning abilities. However, their potential for online continual learning remains relatively unexplored. In online continual learning, a model must adapt to a non-stationary stream of data, minimizing the cumulative nextstep prediction loss. We focus on the supervised online continual learning setting, where we learn a predictor $x_t \\rightarrow y_t$ for a sequence of examples $(x_t, y_t)$. Inspired by the in-context learning capabilities of transformers and their connection to meta-learning, we propose a method that leverages these strengths for online continual learning. Our approach e",
    "link": "https://arxiv.org/abs/2403.01554",
    "context": "Title: Transformers for Supervised Online Continual Learning\nAbstract: arXiv:2403.01554v1 Announce Type: new  Abstract: Transformers have become the dominant architecture for sequence modeling tasks such as natural language processing or audio processing, and they are now even considered for tasks that are not naturally sequential such as image classification. Their ability to attend to and to process a set of tokens as context enables them to develop in-context few-shot learning abilities. However, their potential for online continual learning remains relatively unexplored. In online continual learning, a model must adapt to a non-stationary stream of data, minimizing the cumulative nextstep prediction loss. We focus on the supervised online continual learning setting, where we learn a predictor $x_t \\rightarrow y_t$ for a sequence of examples $(x_t, y_t)$. Inspired by the in-context learning capabilities of transformers and their connection to meta-learning, we propose a method that leverages these strengths for online continual learning. Our approach e",
    "path": "papers/24/03/2403.01554.json",
    "total_tokens": 699,
    "translated_title": "用于监督在线持续学习的变压器",
    "translated_abstract": "变压器已成为序列建模任务（如自然语言处理或音频处理）的主导架构，甚至被考虑用于非自然顺序任务，如图像分类。它们能够关注和处理一组标记作为上下文，使其能够发展出上下文少样本学习的能力。然而，它们在在线持续学习中的潜力仍然相对未被探究。在线持续学习中，模型必须适应非静态数据流，最小化累积的下一步预测损失。",
    "tldr": "本文提出了一种方法，利用变压器的上下文学习能力以及它们与元学习的关联，用于监督在线持续学习。"
}