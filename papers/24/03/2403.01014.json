{
    "title": "A Case for Validation Buffer in Pessimistic Actor-Critic",
    "abstract": "arXiv:2403.01014v1 Announce Type: new  Abstract: In this paper, we investigate the issue of error accumulation in critic networks updated via pessimistic temporal difference objectives. We show that the critic approximation error can be approximated via a recursive fixed-point model similar to that of the Bellman value. We use such recursive definition to retrieve the conditions under which the pessimistic critic is unbiased. Building on these insights, we propose Validation Pessimism Learning (VPL) algorithm. VPL uses a small validation buffer to adjust the levels of pessimism throughout the agent training, with the pessimism set such that the approximation error of the critic targets is minimized. We investigate the proposed approach on a variety of locomotion and manipulation tasks and report improvements in sample efficiency and performance.",
    "link": "https://arxiv.org/abs/2403.01014",
    "context": "Title: A Case for Validation Buffer in Pessimistic Actor-Critic\nAbstract: arXiv:2403.01014v1 Announce Type: new  Abstract: In this paper, we investigate the issue of error accumulation in critic networks updated via pessimistic temporal difference objectives. We show that the critic approximation error can be approximated via a recursive fixed-point model similar to that of the Bellman value. We use such recursive definition to retrieve the conditions under which the pessimistic critic is unbiased. Building on these insights, we propose Validation Pessimism Learning (VPL) algorithm. VPL uses a small validation buffer to adjust the levels of pessimism throughout the agent training, with the pessimism set such that the approximation error of the critic targets is minimized. We investigate the proposed approach on a variety of locomotion and manipulation tasks and report improvements in sample efficiency and performance.",
    "path": "papers/24/03/2403.01014.json",
    "total_tokens": 774,
    "translated_title": "论证悲观演员-评论家算法中验证缓冲区的必要性",
    "translated_abstract": "在这篇论文中，我们研究了通过悲观时序差异目标更新的评论家网络中错误累积的问题。我们展示了评论家逼近误差可以通过类似贝尔曼值的递归不动点模型来近似。我们利用这种递归定义来找到悲观评论家无偏的条件。基于这些见解，我们提出了验证悲观学习（VPL）算法。VPL使用一个小的验证缓冲区来调整整个代理训练过程中的悲观水平，其中悲观设置为使评论家目标的逼近误差最小化。我们在各种运动和操纵任务上研究了所提出的方法，并报告了在样本效率和性能方面的改进。",
    "tldr": "提出了验证悲观学习（VPL）算法，通过使用小的验证缓冲区调整悲观水平，以最小化评论家目标的逼近误差，从而改善了样本效率和性能。"
}