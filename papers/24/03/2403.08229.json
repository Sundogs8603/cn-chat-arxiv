{
    "title": "Boosting Disfluency Detection with Large Language Model as Disfluency Generator",
    "abstract": "arXiv:2403.08229v1 Announce Type: new  Abstract: Current disfluency detection methods heavily rely on costly and scarce human-annotated data. To tackle this issue, some approaches employ heuristic or statistical features to generate disfluent sentences, partially improving detection performance. However, these sentences often deviate from real-life scenarios, constraining overall model enhancement. In this study, we propose a lightweight data augmentation approach for disfluency detection, utilizing the superior generative and semantic understanding capabilities of large language model (LLM) to generate disfluent sentences as augmentation data. We leverage LLM to generate diverse and more realistic sentences guided by specific prompts, without the need for fine-tuning the LLM. Subsequently, we apply an uncertainty-aware data filtering approach to improve the quality of the generated sentences, utilized in training a small detection model for improved performance. Experiments using enha",
    "link": "https://arxiv.org/abs/2403.08229",
    "context": "Title: Boosting Disfluency Detection with Large Language Model as Disfluency Generator\nAbstract: arXiv:2403.08229v1 Announce Type: new  Abstract: Current disfluency detection methods heavily rely on costly and scarce human-annotated data. To tackle this issue, some approaches employ heuristic or statistical features to generate disfluent sentences, partially improving detection performance. However, these sentences often deviate from real-life scenarios, constraining overall model enhancement. In this study, we propose a lightweight data augmentation approach for disfluency detection, utilizing the superior generative and semantic understanding capabilities of large language model (LLM) to generate disfluent sentences as augmentation data. We leverage LLM to generate diverse and more realistic sentences guided by specific prompts, without the need for fine-tuning the LLM. Subsequently, we apply an uncertainty-aware data filtering approach to improve the quality of the generated sentences, utilized in training a small detection model for improved performance. Experiments using enha",
    "path": "papers/24/03/2403.08229.json",
    "total_tokens": 825,
    "translated_title": "利用大型语言模型作为语篇生成器提升不流畅检测",
    "translated_abstract": "当前的不流畅检测方法严重依赖昂贵且稀缺的人工标注数据。为了解决这一问题，一些方法采用启发式或统计特征来生成不流畅句子，部分提高了检测性能。然而，这些句子常常偏离真实场景，限制了整体模型改善。本研究提出了一种轻量级数据增强方法，利用大型语言模型（LLM）卓越的生成和语义理解能力生成不流畅句子作为增强数据。我们利用LLM生成多样且更真实的句子，通过具体提示进行引导，无需对LLM进行微调。随后，我们应用一种基于不确定性的数据过滤方法来提高生成句子的质量，用于训练小型检测模型以提高性能。",
    "tldr": "本研究提出了一种利用大型语言模型生成不流畅句子作为数据增强的轻量级方法，通过数据过滤和小型模型训练实现了不流畅检测性能的提升。"
}