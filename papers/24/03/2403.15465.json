{
    "title": "Most Likely Sequence Generation for $n$-Grams, Transformers, HMMs, and Markov Chains, by Using Rollout Algorithms",
    "abstract": "arXiv:2403.15465v1 Announce Type: cross  Abstract: In this paper we consider a transformer with an $n$-gram structure, such as the one underlying ChatGPT. The transformer provides next word probabilities, which can be used to generate word sequences. We consider methods for computing word sequences that are highly likely, based on these probabilities. Computing the optimal (i.e., most likely) word sequence starting with a given initial state is an intractable problem, so we propose methods to compute highly likely sequences of $N$ words in time that is a low order polynomial in $N$ and in the vocabulary size of the $n$-gram. These methods are based on the rollout approach from approximate dynamic programming, a form of single policy iteration, which can improve the performance of any given heuristic policy. In our case we use a greedy heuristic that generates as next word one that has the highest probability. We show with analysis, examples, and computational experimentation that our m",
    "link": "https://arxiv.org/abs/2403.15465",
    "context": "Title: Most Likely Sequence Generation for $n$-Grams, Transformers, HMMs, and Markov Chains, by Using Rollout Algorithms\nAbstract: arXiv:2403.15465v1 Announce Type: cross  Abstract: In this paper we consider a transformer with an $n$-gram structure, such as the one underlying ChatGPT. The transformer provides next word probabilities, which can be used to generate word sequences. We consider methods for computing word sequences that are highly likely, based on these probabilities. Computing the optimal (i.e., most likely) word sequence starting with a given initial state is an intractable problem, so we propose methods to compute highly likely sequences of $N$ words in time that is a low order polynomial in $N$ and in the vocabulary size of the $n$-gram. These methods are based on the rollout approach from approximate dynamic programming, a form of single policy iteration, which can improve the performance of any given heuristic policy. In our case we use a greedy heuristic that generates as next word one that has the highest probability. We show with analysis, examples, and computational experimentation that our m",
    "path": "papers/24/03/2403.15465.json",
    "total_tokens": 843,
    "translated_title": "使用展开算法为$n$-grams，Transformers，HMMs和马尔可夫链生成最有可能的序列",
    "translated_abstract": "在本文中，我们考虑了一个具有$n$-gram结构的transformer，例如底层的ChatGPT。Transformer提供了下一个单词的概率，可以用来生成单词序列。我们考虑了基于这些概率计算高可能性单词序列的方法。计算从给定初始状态开始的最优（即最有可能）单词序列是一个棘手的问题，因此我们提出了在时间复杂度为$N$和$n$-gram词汇量的低阶多项式的方法来计算$N$个单词的高可能性序列。这些方法基于近似动态规划中的展开方法，一种单策略迭代，可以改善任何给定启发式策略的性能。在我们的情况下，我们使用一种贪婪启发式，生成具有最高概率的下一个单词。我们通过分析、示例和计算实验表明了我们的m",
    "tldr": "本文介绍了一种使用展开算法为$n$-grams，Transformers，HMMs和马尔可夫链生成最有可能的序列的方法"
}