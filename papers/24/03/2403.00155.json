{
    "title": "Towards Explaining Deep Neural Network Compression Through a Probabilistic Latent Space",
    "abstract": "arXiv:2403.00155v1 Announce Type: new  Abstract: Despite the impressive performance of deep neural networks (DNNs), their computational complexity and storage space consumption have led to the concept of network compression. While DNN compression techniques such as pruning and low-rank decomposition have been extensively studied, there has been insufficient attention paid to their theoretical explanation. In this paper, we propose a novel theoretical framework that leverages a probabilistic latent space of DNN weights and explains the optimal network sparsity by using the information-theoretic divergence measures. We introduce new analogous projected patterns (AP2) and analogous-in-probability projected patterns (AP3) notions for DNNs and prove that there exists a relationship between AP3/AP2 property of layers in the network and its performance. Further, we provide a theoretical analysis that explains the training process of the compressed network. The theoretical results are empirica",
    "link": "https://arxiv.org/abs/2403.00155",
    "context": "Title: Towards Explaining Deep Neural Network Compression Through a Probabilistic Latent Space\nAbstract: arXiv:2403.00155v1 Announce Type: new  Abstract: Despite the impressive performance of deep neural networks (DNNs), their computational complexity and storage space consumption have led to the concept of network compression. While DNN compression techniques such as pruning and low-rank decomposition have been extensively studied, there has been insufficient attention paid to their theoretical explanation. In this paper, we propose a novel theoretical framework that leverages a probabilistic latent space of DNN weights and explains the optimal network sparsity by using the information-theoretic divergence measures. We introduce new analogous projected patterns (AP2) and analogous-in-probability projected patterns (AP3) notions for DNNs and prove that there exists a relationship between AP3/AP2 property of layers in the network and its performance. Further, we provide a theoretical analysis that explains the training process of the compressed network. The theoretical results are empirica",
    "path": "papers/24/03/2403.00155.json",
    "total_tokens": 843,
    "translated_title": "通过概率潜在空间解释深度神经网络压缩",
    "translated_abstract": "尽管深度神经网络（DNNs）表现出色，但它们的计算复杂性和存储空间消耗导致了网络压缩的概念。尽管已广泛研究了诸如修剪和低秩分解等DNN压缩技术，但对它们的理论解释仍未受到足够关注。本文提出了一个利用DNN权重的概率潜在空间并利用信息理论分歧度量解释最佳网络稀疏性的新理论框架。我们为DNN引入了新的类比投影模式（AP2）和概率中的类比投影模式（AP3）概念，并证明网络中层的AP3/AP2特性与其性能之间存在关系。此外，我们提供了一个理论分析，解释了压缩网络的训练过程。这些理论结果是从实证实验",
    "tldr": "通过概率潜在空间提出了一个新的理论框架，解释了深度神经网络压缩的优化网络稀疏度，并探讨了网络层的AP3/AP2属性与性能之间的关系。",
    "en_tdlr": "A novel theoretical framework leveraging probabilistic latent space is proposed to explain optimal network sparsity in deep neural network compression, exploring the relationship between AP3/AP2 properties of network layers and performance."
}