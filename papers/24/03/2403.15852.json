{
    "title": "When LLM-based Code Generation Meets the Software Development Process",
    "abstract": "arXiv:2403.15852v1 Announce Type: cross  Abstract: Software process models play a pivotal role in fostering collaboration and communication within software teams, enabling them to tackle intricate development tasks effectively. This paper introduces LCG, a code generation framework inspired by established software engineering practices. LCG leverages multiple Large Language Model (LLM) agents to emulate various software process models, namely LCGWaterfall, LCGTDD, and LCGScrum. Each model assigns LLM agents specific roles such as requirement engineer, architect, developer, tester, and scrum master, mirroring typical development activities and communication patterns. Through collaborative efforts utilizing chain-of-thought and prompt composition techniques, the agents continuously refine themselves to enhance code quality. Utilizing GPT3.5 as the underlying LLM and baseline (GPT), we evaluate LCG across four code generation benchmarks: HumanEval, HumanEval-ET, MBPP, and MBPP-ET. Results",
    "link": "https://arxiv.org/abs/2403.15852",
    "context": "Title: When LLM-based Code Generation Meets the Software Development Process\nAbstract: arXiv:2403.15852v1 Announce Type: cross  Abstract: Software process models play a pivotal role in fostering collaboration and communication within software teams, enabling them to tackle intricate development tasks effectively. This paper introduces LCG, a code generation framework inspired by established software engineering practices. LCG leverages multiple Large Language Model (LLM) agents to emulate various software process models, namely LCGWaterfall, LCGTDD, and LCGScrum. Each model assigns LLM agents specific roles such as requirement engineer, architect, developer, tester, and scrum master, mirroring typical development activities and communication patterns. Through collaborative efforts utilizing chain-of-thought and prompt composition techniques, the agents continuously refine themselves to enhance code quality. Utilizing GPT3.5 as the underlying LLM and baseline (GPT), we evaluate LCG across four code generation benchmarks: HumanEval, HumanEval-ET, MBPP, and MBPP-ET. Results",
    "path": "papers/24/03/2403.15852.json",
    "total_tokens": 872,
    "translated_title": "当基于LLM的代码生成遇上软件开发流程",
    "translated_abstract": "软件过程模型在促进软件团队内协作与沟通，使其能够有效应对复杂的开发任务方面担当着关键角色。本文介绍了LCG，这是一个受到成熟软件工程实践启发的代码生成框架。LCG利用多个大型语言模型(LLM)代理来模拟各种软件过程模型，即LCGWaterfall、LCGTDD和LCGScrum。每个模型为LLM代理分配特定角色，如需求工程师、架构师、开发人员、测试人员和Scrum Master，反映了典型的开发活动和沟通模式。通过利用思维链和提示组合技术进行协作，代理不断完善自身以提高代码质量。在GPT3.5作为基础LLM和基准(GPT)的情况下，我们评估了LCG在四个代码生成基准测试上的表现：HumanEval、HumanEval-ET、MBPP和MBPP-ET。",
    "tldr": "该研究引入了基于LLM的代码生成框架LCG，通过模拟各种软件过程模型以及利用协作和技术提高代码质量。评估结果表明其在代码生成基准上的有效性。",
    "en_tdlr": "This paper introduces LCG, a code generation framework based on LLM, which simulates various software process models and improves code quality through collaboration and techniques. Evaluation results demonstrate its effectiveness on code generation benchmarks."
}