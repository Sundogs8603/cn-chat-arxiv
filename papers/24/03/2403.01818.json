{
    "title": "AllSpark: Reborn Labeled Features from Unlabeled in Transformer for Semi-Supervised Semantic Segmentation",
    "abstract": "arXiv:2403.01818v1 Announce Type: cross  Abstract: Semi-supervised semantic segmentation (SSSS) has been proposed to alleviate the burden of time-consuming pixel-level manual labeling, which leverages limited labeled data along with larger amounts of unlabeled data. Current state-of-the-art methods train the labeled data with ground truths and unlabeled data with pseudo labels. However, the two training flows are separate, which allows labeled data to dominate the training process, resulting in low-quality pseudo labels and, consequently, sub-optimal results. To alleviate this issue, we present AllSpark, which reborns the labeled features from unlabeled ones with the channel-wise cross-attention mechanism. We further introduce a Semantic Memory along with a Channel Semantic Grouping strategy to ensure that unlabeled features adequately represent labeled features. The AllSpark shed new light on the architecture level designs of SSSS rather than framework level, which avoids increasingly",
    "link": "https://arxiv.org/abs/2403.01818",
    "context": "Title: AllSpark: Reborn Labeled Features from Unlabeled in Transformer for Semi-Supervised Semantic Segmentation\nAbstract: arXiv:2403.01818v1 Announce Type: cross  Abstract: Semi-supervised semantic segmentation (SSSS) has been proposed to alleviate the burden of time-consuming pixel-level manual labeling, which leverages limited labeled data along with larger amounts of unlabeled data. Current state-of-the-art methods train the labeled data with ground truths and unlabeled data with pseudo labels. However, the two training flows are separate, which allows labeled data to dominate the training process, resulting in low-quality pseudo labels and, consequently, sub-optimal results. To alleviate this issue, we present AllSpark, which reborns the labeled features from unlabeled ones with the channel-wise cross-attention mechanism. We further introduce a Semantic Memory along with a Channel Semantic Grouping strategy to ensure that unlabeled features adequately represent labeled features. The AllSpark shed new light on the architecture level designs of SSSS rather than framework level, which avoids increasingly",
    "path": "papers/24/03/2403.01818.json",
    "total_tokens": 852,
    "translated_title": "AllSpark: 利用Transformer中未标记的特征重新生成标记特征，用于半监督语义分割",
    "translated_abstract": "半监督语义分割（SSSS）旨在减轻耗时的像素级手动标注负担，它利用有限的标记数据以及更多的未标记数据。目前最先进的方法使用基准真值训练标记数据和使用伪标签训练未标记数据。然而，这两种训练流程是分开的，这使得标记数据主导训练过程，导致低质量的伪标签和从而次优的结果。为了解决这个问题，我们提出了AllSpark，利用通道级交叉注意机制从未标记的特征中重新生成标记的特征。我们进一步引入了语义记忆和通道语义分组策略，以确保未标记特征充分代表标记特征。AllSpark为SSSS的架构级设计带来了新的视角，而非框架级别，避免了越来越常见的问题。",
    "tldr": "AllSpark利用通道级交叉注意机制从未标记的特征中重新生成标记特征，以改善半监督语义分割中低质量伪标签的问题。",
    "en_tdlr": "AllSpark utilizes a channel-wise cross-attention mechanism to reborn labeled features from unlabeled ones, aiming to improve the issue of low-quality pseudo labels in semi-supervised semantic segmentation."
}