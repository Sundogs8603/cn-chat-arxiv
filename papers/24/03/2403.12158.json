{
    "title": "Variational Approach for Efficient KL Divergence Estimation in Dirichlet Mixture Models",
    "abstract": "arXiv:2403.12158v1 Announce Type: cross  Abstract: This study tackles the efficient estimation of Kullback-Leibler (KL) Divergence in Dirichlet Mixture Models (DMM), crucial for clustering compositional data. Despite the significance of DMMs, obtaining an analytically tractable solution for KL Divergence has proven elusive. Past approaches relied on computationally demanding Monte Carlo methods, motivating our introduction of a novel variational approach. Our method offers a closed-form solution, significantly enhancing computational efficiency for swift model comparisons and robust estimation evaluations. Validation using real and simulated data showcases its superior efficiency and accuracy over traditional Monte Carlo-based methods, opening new avenues for rapid exploration of diverse DMM models and advancing statistical analyses of compositional data.",
    "link": "https://arxiv.org/abs/2403.12158",
    "context": "Title: Variational Approach for Efficient KL Divergence Estimation in Dirichlet Mixture Models\nAbstract: arXiv:2403.12158v1 Announce Type: cross  Abstract: This study tackles the efficient estimation of Kullback-Leibler (KL) Divergence in Dirichlet Mixture Models (DMM), crucial for clustering compositional data. Despite the significance of DMMs, obtaining an analytically tractable solution for KL Divergence has proven elusive. Past approaches relied on computationally demanding Monte Carlo methods, motivating our introduction of a novel variational approach. Our method offers a closed-form solution, significantly enhancing computational efficiency for swift model comparisons and robust estimation evaluations. Validation using real and simulated data showcases its superior efficiency and accuracy over traditional Monte Carlo-based methods, opening new avenues for rapid exploration of diverse DMM models and advancing statistical analyses of compositional data.",
    "path": "papers/24/03/2403.12158.json",
    "total_tokens": 790,
    "translated_title": "变分方法用于Dirichlet混合模型中KL散度的高效估计",
    "translated_abstract": "本研究致力于在Dirichlet混合模型（DMM）中高效估计Kullback-Leibler（KL）散度，这对于对成分数据进行聚类至关重要。尽管DMM的重要性，但获得KL散度的解析解仍然是困难的。过去的方法依赖于计算密集型的蒙特卡洛方法，这促使我们引入了一种新颖的变分方法。我们的方法提供了一个封闭形式的解，显著提高了计算效率，可以快速进行模型比较和稳健的估计评估。实际数据和模拟数据的验证显示，我们的方法比传统的基于蒙特卡洛的方法更加高效准确，为快速探索不同DMM模型和推进成分数据的统计分析开辟了新途径。",
    "tldr": "引入变分方法在Dirichlet混合模型中提供了封闭形式的解，显著提高了计算效率，可用于快速模型比较和稳健估计评估。",
    "en_tdlr": "Introducing a variational approach in Dirichlet mixture models provides a closed-form solution, significantly enhancing computational efficiency for swift model comparisons and robust estimation evaluations."
}