{
    "title": "Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation",
    "abstract": "arXiv:2403.09625v1 Announce Type: cross  Abstract: Recent years have witnessed the strong power of 3D generation models, which offer a new level of creative flexibility by allowing users to guide the 3D content generation process through a single image or natural language. However, it remains challenging for existing 3D generation methods to create subject-driven 3D content across diverse prompts. In this paper, we introduce a novel 3D customization method, dubbed Make-Your-3D that can personalize high-fidelity and consistent 3D content from only a single image of a subject with text description within 5 minutes. Our key insight is to harmonize the distributions of a multi-view diffusion model and an identity-specific 2D generative model, aligning them with the distribution of the desired 3D subject. Specifically, we design a co-evolution framework to reduce the variance of distributions, where each model undergoes a process of learning from the other through identity-aware optimizatio",
    "link": "https://arxiv.org/abs/2403.09625",
    "context": "Title: Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation\nAbstract: arXiv:2403.09625v1 Announce Type: cross  Abstract: Recent years have witnessed the strong power of 3D generation models, which offer a new level of creative flexibility by allowing users to guide the 3D content generation process through a single image or natural language. However, it remains challenging for existing 3D generation methods to create subject-driven 3D content across diverse prompts. In this paper, we introduce a novel 3D customization method, dubbed Make-Your-3D that can personalize high-fidelity and consistent 3D content from only a single image of a subject with text description within 5 minutes. Our key insight is to harmonize the distributions of a multi-view diffusion model and an identity-specific 2D generative model, aligning them with the distribution of the desired 3D subject. Specifically, we design a co-evolution framework to reduce the variance of distributions, where each model undergoes a process of learning from the other through identity-aware optimizatio",
    "path": "papers/24/03/2403.09625.json",
    "total_tokens": 865,
    "translated_title": "Make-Your-3D: 快速并一致的主体驱动3D内容生成",
    "translated_abstract": "近年来，强调3D生成模型的强大力量，通过允许用户通过单个图像或自然语言引导3D内容生成过程，提供了新水平的创造灵活性。然而，现有的3D生成方法仍然面临着跨不同提示创建主体驱动3D内容的挑战。本文介绍了一种名为Make-Your-3D的新颖3D定制方法，可以从仅一张主题的单个图像和文本描述中个性化地生成高保真、一致的3D内容，仅需5分钟。我们的关键见解是协调多视扩散模型和特定身份的2D生成模型的分布，将它们与所需的3D主体的分布对齐。具体地，我们设计了一个共同进化框架来减少分布的差异，其中每个模型通过身份感知优化互相学习的过程。",
    "tldr": "Make-Your-3D是一种新颖的3D定制方法，可以在5分钟内从单个图像和文本描述中实现高保真、一致的3D内容生成，通过协调不同模型的分布来实现这一目标",
    "en_tdlr": "Make-Your-3D is a novel 3D customization method that can generate high-fidelity and consistent 3D content from a single image and text description within 5 minutes by harmonizing the distributions of different models."
}