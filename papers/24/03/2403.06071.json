{
    "title": "Bit-mask Robust Contrastive Knowledge Distillation for Unsupervised Semantic Hashing",
    "abstract": "arXiv:2403.06071v1 Announce Type: cross  Abstract: Unsupervised semantic hashing has emerged as an indispensable technique for fast image search, which aims to convert images into binary hash codes without relying on labels. Recent advancements in the field demonstrate that employing large-scale backbones (e.g., ViT) in unsupervised semantic hashing models can yield substantial improvements. However, the inference delay has become increasingly difficult to overlook. Knowledge distillation provides a means for practical model compression to alleviate this delay. Nevertheless, the prevailing knowledge distillation approaches are not explicitly designed for semantic hashing. They ignore the unique search paradigm of semantic hashing, the inherent necessities of the distillation process, and the property of hash codes. In this paper, we propose an innovative Bit-mask Robust Contrastive knowledge Distillation (BRCD) method, specifically devised for the distillation of semantic hashing model",
    "link": "https://arxiv.org/abs/2403.06071",
    "context": "Title: Bit-mask Robust Contrastive Knowledge Distillation for Unsupervised Semantic Hashing\nAbstract: arXiv:2403.06071v1 Announce Type: cross  Abstract: Unsupervised semantic hashing has emerged as an indispensable technique for fast image search, which aims to convert images into binary hash codes without relying on labels. Recent advancements in the field demonstrate that employing large-scale backbones (e.g., ViT) in unsupervised semantic hashing models can yield substantial improvements. However, the inference delay has become increasingly difficult to overlook. Knowledge distillation provides a means for practical model compression to alleviate this delay. Nevertheless, the prevailing knowledge distillation approaches are not explicitly designed for semantic hashing. They ignore the unique search paradigm of semantic hashing, the inherent necessities of the distillation process, and the property of hash codes. In this paper, we propose an innovative Bit-mask Robust Contrastive knowledge Distillation (BRCD) method, specifically devised for the distillation of semantic hashing model",
    "path": "papers/24/03/2403.06071.json",
    "total_tokens": 859,
    "translated_title": "位掩码健壮对比度知识蒸馏用于无监督语义哈希",
    "translated_abstract": "无监督语义哈希已经成为快速图像搜索的不可或缺的技术，旨在将图像转换为二进制哈希码而不依赖标签。最近在该领域的进展表明，在无监督语义哈希模型中使用大规模骨干（例如 ViT）可以带来显著的改进。然而，推断延迟变得越来越难以忽视。知识蒸馏提供了一种实现模型压缩以缓解此延迟的方法。然而，目前的知识蒸馏方法并未专门针对语义哈希进行设计。它们忽略了语义哈希的独特搜索范式，蒸馏过程的固有必需性以及哈希码的属性。在本文中，我们提出了一种创新的位掩码健壮对比度知识蒸馏（BRCD）方法，专为语义哈希模型的蒸馏而设计。",
    "tldr": "本文提出了一种创新的位掩码健壮对比度知识蒸馏（BRCD）方法，专门为语义哈希模型的蒸馏而设计，以取得更好的性能。",
    "en_tdlr": "This paper proposes an innovative Bit-mask Robust Contrastive knowledge Distillation (BRCD) method, specifically designed for distilling semantic hashing models to achieve better performance."
}