{
    "title": "Multilingual Sentence-T5: Scalable Sentence Encoders for Multilingual Applications",
    "abstract": "arXiv:2403.17528v1 Announce Type: new  Abstract: Prior work on multilingual sentence embedding has demonstrated that the efficient use of natural language inference (NLI) data to build high-performance models can outperform conventional methods. However, the potential benefits from the recent ``exponential'' growth of language models with billions of parameters have not yet been fully explored. In this paper, we introduce Multilingual Sentence T5 (m-ST5), as a larger model of NLI-based multilingual sentence embedding, by extending Sentence T5, an existing monolingual model. By employing the low-rank adaptation (LoRA) technique, we have achieved a successful scaling of the model's size to 5.7 billion parameters. We conducted experiments to evaluate the performance of sentence embedding and verified that the method outperforms the NLI-based prior approach. Furthermore, we also have confirmed a positive correlation between the size of the model and its performance. It was particularly not",
    "link": "https://arxiv.org/abs/2403.17528",
    "context": "Title: Multilingual Sentence-T5: Scalable Sentence Encoders for Multilingual Applications\nAbstract: arXiv:2403.17528v1 Announce Type: new  Abstract: Prior work on multilingual sentence embedding has demonstrated that the efficient use of natural language inference (NLI) data to build high-performance models can outperform conventional methods. However, the potential benefits from the recent ``exponential'' growth of language models with billions of parameters have not yet been fully explored. In this paper, we introduce Multilingual Sentence T5 (m-ST5), as a larger model of NLI-based multilingual sentence embedding, by extending Sentence T5, an existing monolingual model. By employing the low-rank adaptation (LoRA) technique, we have achieved a successful scaling of the model's size to 5.7 billion parameters. We conducted experiments to evaluate the performance of sentence embedding and verified that the method outperforms the NLI-based prior approach. Furthermore, we also have confirmed a positive correlation between the size of the model and its performance. It was particularly not",
    "path": "papers/24/03/2403.17528.json",
    "total_tokens": 836,
    "translated_title": "多语言句子-T5: 用于多语言应用的可扩展句子编码器",
    "translated_abstract": "先前关于多语言句子嵌入的工作表明，有效利用自然语言推理（NLI）数据来构建高性能模型能够胜过传统方法。然而，最近“指数”增长的拥有数十亿参数的语言模型的潜在好处尚未被充分探索。在本文中，我们通过扩展现有的单语言模型Sentence T5，引入了Multilingual Sentence T5（m-ST5），作为一个更大的基于NLI的多语言句子嵌入模型。通过使用低秩适应（LoRA）技术，我们成功地将模型的规模扩展到57亿个参数。我们进行了实验来评估句子嵌入的性能，并验证了该方法优于基于NLI的先前方法。此外，我们还确认了模型规模与性能之间的正相关性。",
    "tldr": "介绍了一个基于NLI的多语言句子嵌入模型Multilingual Sentence T5，通过低秩适应技术成功将模型规模扩展到57亿参数，并实现了优于先前方法的性能。",
    "en_tdlr": "Introducing a multilingual sentence embedding model, Multilingual Sentence T5, based on NLI, successfully scaled the model to 5.7 billion parameters using the low-rank adaptation technique, achieving better performance than previous methods."
}