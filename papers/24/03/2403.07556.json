{
    "title": "Truth-Aware Context Selection: Mitigating the Hallucinations of Large Language Models Being Misled by Untruthful Contexts",
    "abstract": "arXiv:2403.07556v1 Announce Type: new  Abstract: Although large language models (LLMs) have demonstrated impressive text generation capabilities, they are easily misled by the untruthful context provided by users or knowledge argumentation tools, thereby producing hallucinations. To alleviate the LLMs from being misled by untruthful information and take advantage of knowledge argumentation, we propose Truth-Aware Context Selection (TACS), a lightweight method to shield untruthful context from the inputs. TACS begins by performing truth detection on the input context, leveraging the parameterized knowledge within the LLM. Subsequently, it constructs a corresponding attention mask based on the truthfulness of each position, selecting the truthful context and discarding the untruthful context. Additionally, we introduce a new evaluation metric, Disturbance Adaption Rate, to further study the LLMs' ability to accept truthful information and resist untruthful information. Experimental resul",
    "link": "https://arxiv.org/abs/2403.07556",
    "context": "Title: Truth-Aware Context Selection: Mitigating the Hallucinations of Large Language Models Being Misled by Untruthful Contexts\nAbstract: arXiv:2403.07556v1 Announce Type: new  Abstract: Although large language models (LLMs) have demonstrated impressive text generation capabilities, they are easily misled by the untruthful context provided by users or knowledge argumentation tools, thereby producing hallucinations. To alleviate the LLMs from being misled by untruthful information and take advantage of knowledge argumentation, we propose Truth-Aware Context Selection (TACS), a lightweight method to shield untruthful context from the inputs. TACS begins by performing truth detection on the input context, leveraging the parameterized knowledge within the LLM. Subsequently, it constructs a corresponding attention mask based on the truthfulness of each position, selecting the truthful context and discarding the untruthful context. Additionally, we introduce a new evaluation metric, Disturbance Adaption Rate, to further study the LLMs' ability to accept truthful information and resist untruthful information. Experimental resul",
    "path": "papers/24/03/2403.07556.json",
    "total_tokens": 882,
    "translated_title": "真相感知的上下文选择：缓解大型语言模型被不真实上下文误导产生幻觉",
    "translated_abstract": "尽管大型语言模型（LLMs）展示了令人印象深刻的文本生成能力，但它们很容易被用户或知识论证工具提供的不真实上下文误导，从而产生幻觉。为了减轻LLMs被不真实信息误导并利用知识论证，我们提出了真相感知的上下文选择（TACS），这是一种轻量级方法，可以从输入中屏蔽不真实的上下文。TACS首先对输入上下文进行真相检测，利用LLM内的参数化知识。随后，根据每个位置的真实性构建相应的注意力蒙版，选择真实的上下文并丢弃不真实的上下文。此外，我们引入一个新的评估指标，扰动适应率，以进一步研究LLMs接受真实信息和抵制不真实信息的能力。",
    "tldr": "提出了一种名为真相感知的上下文选择（TACS）的轻量级方法，可以通过对输入上下文进行真相检测并构建相应的注意力蒙版来缓解大型语言模型被不真实上下文误导产生幻觉"
}