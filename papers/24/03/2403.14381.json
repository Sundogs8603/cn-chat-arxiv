{
    "title": "Editing Knowledge Representation of Language Lodel via Rephrased Prefix Prompts",
    "abstract": "arXiv:2403.14381v1 Announce Type: cross  Abstract: Neural language models (LMs) have been extensively trained on vast corpora to store factual knowledge about various aspects of the world described in texts. Current technologies typically employ knowledge editing methods or specific prompts to modify LM outputs. However, existing knowledge editing methods are costly and inefficient, struggling to produce appropriate text. Additionally, prompt engineering is opaque and requires significant effort to find suitable prompts. To address these issues, we introduce a new method called PSPEM (Prefix Soft Prompt Editing Method), that can be used for a lifetime with just one training. It resolves the inefficiencies and generalizability issues in knowledge editing methods and overcomes the opacity of prompt engineering by automatically seeking optimal soft prompts. Specifically, PSPEM utilizes a prompt encoder and an encoding converter to refine key information in prompts and uses prompt alignmen",
    "link": "https://arxiv.org/abs/2403.14381",
    "context": "Title: Editing Knowledge Representation of Language Lodel via Rephrased Prefix Prompts\nAbstract: arXiv:2403.14381v1 Announce Type: cross  Abstract: Neural language models (LMs) have been extensively trained on vast corpora to store factual knowledge about various aspects of the world described in texts. Current technologies typically employ knowledge editing methods or specific prompts to modify LM outputs. However, existing knowledge editing methods are costly and inefficient, struggling to produce appropriate text. Additionally, prompt engineering is opaque and requires significant effort to find suitable prompts. To address these issues, we introduce a new method called PSPEM (Prefix Soft Prompt Editing Method), that can be used for a lifetime with just one training. It resolves the inefficiencies and generalizability issues in knowledge editing methods and overcomes the opacity of prompt engineering by automatically seeking optimal soft prompts. Specifically, PSPEM utilizes a prompt encoder and an encoding converter to refine key information in prompts and uses prompt alignmen",
    "path": "papers/24/03/2403.14381.json",
    "total_tokens": 839,
    "translated_title": "通过重新表述前缀提示来编辑语言Lodel的知识表示",
    "translated_abstract": "神经语言模型（LMs）已在广泛的语料库上进行了大量培训，以存储关于文本描述的世界各个方面的事实知识。当前技术通常采用知识编辑方法或特定提示来修改LM输出。然而，现有的知识编辑方法成本高昂且低效，难以产生适当的文本。此外，提示工程是不透明的，需要大量努力找到合适的提示。为解决这些问题，我们引入了一种称为PSPEM（前缀软提示编辑方法）的新方法，可以仅通过一次训练而终身使用。它解决了知识编辑方法中的低效性和通用性问题，并通过自动寻找最佳软提示来克服提示工程的不透明性。具体而言，PSPEM利用提示编码器和编码转换器来精炼提示中的关键信息，并使用提示对齐",
    "tldr": "引入了一种名为PSPEM的新方法，通过重新表述前缀提示来编辑语言Lodel的知识表示，解决了知识编辑方法中的低效性、通用性问题，以及提示工程的不透明性。",
    "en_tdlr": "Introduced a new method called PSPEM, which edits the knowledge representation of language Lodel by rephrasing prefix prompts, addressing inefficiencies and generalizability issues in knowledge editing methods, as well as the opacity of prompt engineering."
}