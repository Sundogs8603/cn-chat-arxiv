{
    "title": "Grounding Spatial Relations in Text-Only Language Models",
    "abstract": "arXiv:2403.13666v1 Announce Type: new  Abstract: This paper shows that text-only Language Models (LM) can learn to ground spatial relations like \"left of\" or \"below\" if they are provided with explicit location information of objects and they are properly trained to leverage those locations. We perform experiments on a verbalized version of the Visual Spatial Reasoning (VSR) dataset, where images are coupled with textual statements which contain real or fake spatial relations between two objects of the image. We verbalize the images using an off-the-shelf object detector, adding location tokens to every object label to represent their bounding boxes in textual form. Given the small size of VSR, we do not observe any improvement when using locations, but pretraining the LM over a synthetic dataset automatically derived by us improves results significantly when using location tokens. We thus show that locations allow LMs to ground spatial relations, with our text-only LMs outperforming Vi",
    "link": "https://arxiv.org/abs/2403.13666",
    "context": "Title: Grounding Spatial Relations in Text-Only Language Models\nAbstract: arXiv:2403.13666v1 Announce Type: new  Abstract: This paper shows that text-only Language Models (LM) can learn to ground spatial relations like \"left of\" or \"below\" if they are provided with explicit location information of objects and they are properly trained to leverage those locations. We perform experiments on a verbalized version of the Visual Spatial Reasoning (VSR) dataset, where images are coupled with textual statements which contain real or fake spatial relations between two objects of the image. We verbalize the images using an off-the-shelf object detector, adding location tokens to every object label to represent their bounding boxes in textual form. Given the small size of VSR, we do not observe any improvement when using locations, but pretraining the LM over a synthetic dataset automatically derived by us improves results significantly when using location tokens. We thus show that locations allow LMs to ground spatial relations, with our text-only LMs outperforming Vi",
    "path": "papers/24/03/2403.13666.json",
    "total_tokens": 845,
    "translated_title": "在纯文本语言模型中落实空间关系",
    "translated_abstract": "本文表明，如果为纯文本语言模型（LM）提供了对象的显式位置信息并进行了适当的训练以利用这些位置信息，它们就可以学习到像“左侧”或“下方”这样的空间关系。我们在一种口头化版本的视觉空间推理（VSR）数据集上进行了实验，在这个数据集中，图像与包含图像两个对象之间真假空间关系的文本陈述相结合。我们使用现成的物体检测器对图像进行口头描述，并在每个对象标签中添加位置标记，以代表它们的边界框的文本形式。鉴于VSR数据集规模较小，当使用位置信息时我们没有观察到任何改进，但是在由我们自动导出的一个合成数据集上对LM进行预训练可以显著改善结果。因此，我们表明位置信息使LM能够落实空间关系，我们的纯文本LM在性能上胜过了Vi。",
    "tldr": "纯文本语言模型可以通过提供对象位置信息并经过适当训练来学习落实空间关系，这可以通过预训练LM在合成数据集上显著改善结果。",
    "en_tdlr": "Text-only Language Models can learn to ground spatial relations by providing object location information and proper training, which can significantly improve results through pretraining LM on synthetic datasets."
}