{
    "title": "Semi-Instruct: Bridging Natural-Instruct and Self-Instruct for Code Large Language Models",
    "abstract": "arXiv:2403.00338v1 Announce Type: new  Abstract: Instruction tuning plays a pivotal role in Code Large Language Models (Code LLMs) for the task of program synthesis. Presently, two dominant paradigms for collecting tuning data are natural-instruct (human-written) and self-instruct (automatically generated). Natural-instruct includes diverse and correct codes but lacks instruction-code pairs, and exists improper code formats like nested single-line codes. In contrast, self-instruct automatically generates proper paired data. However, it suffers from low diversity due to generating duplicates and cannot ensure the correctness of codes. To bridge the both paradigms, we propose \\textbf{Semi-Instruct}. It first converts diverse but improper codes from natural-instruct into proper instruction-code pairs through a method similar to self-instruct. To verify the correctness of generated codes, we design a novel way to construct test cases by generating cases' inputs and executing correct codes ",
    "link": "https://arxiv.org/abs/2403.00338",
    "context": "Title: Semi-Instruct: Bridging Natural-Instruct and Self-Instruct for Code Large Language Models\nAbstract: arXiv:2403.00338v1 Announce Type: new  Abstract: Instruction tuning plays a pivotal role in Code Large Language Models (Code LLMs) for the task of program synthesis. Presently, two dominant paradigms for collecting tuning data are natural-instruct (human-written) and self-instruct (automatically generated). Natural-instruct includes diverse and correct codes but lacks instruction-code pairs, and exists improper code formats like nested single-line codes. In contrast, self-instruct automatically generates proper paired data. However, it suffers from low diversity due to generating duplicates and cannot ensure the correctness of codes. To bridge the both paradigms, we propose \\textbf{Semi-Instruct}. It first converts diverse but improper codes from natural-instruct into proper instruction-code pairs through a method similar to self-instruct. To verify the correctness of generated codes, we design a novel way to construct test cases by generating cases' inputs and executing correct codes ",
    "path": "papers/24/03/2403.00338.json",
    "total_tokens": 911,
    "translated_title": "Semi-Instruct: 桥接自然指导与自我指导以应用于大型编程语言模型",
    "translated_abstract": "arXiv:2403.00338v1 公告类型：新的 摘要：指导调整在代码大型语言模型（Code LLMs）中的程序合成任务中起着关键作用。目前，收集调整数据的两种主要范式是自然指令（人工编写）和自我指令（自动生成）。自然指令包含多样且正确的代码，但缺乏指令-代码配对，并存在不当的嵌套单行代码格式。相反，自我指令自动生成适当配对数据。然而，由于生成重复内容，存在多样性低的问题，并且无法确保代码的正确性。为了桥接这两个范式，我们提出了Semi-Instruct。它首先通过类似于自我指令的方法，将自然指令中的多样但不当的代码转换为适当的指令-代码配对。为了验证生成代码的正确性，我们设计了一种新颖的测试用例构建方法，通过生成用例输入并执行正确代码来实现。",
    "tldr": "提出了一种名为Semi-Instruct的方法，桥接了自然指导和自我指导两种范式，能够将自然指导中的多样但不当的代码转换为适当的指令-代码配对，并设计了一种新颖的测试用例构建方法以验证生成代码的正确性",
    "en_tdlr": "Proposed a method named Semi-Instruct that bridges the natural-instruct and self-instruct paradigms, which can convert diverse but improper codes from natural-instruct into proper instruction-code pairs, and designed a novel way to construct test cases to verify the correctness of the generated codes."
}