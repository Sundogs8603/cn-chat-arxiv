{
    "title": "Rapidly Developing High-quality Instruction Data and Evaluation Benchmark for Large Language Models with Minimal Human Effort: A Case Study on Japanese",
    "abstract": "arXiv:2403.03690v1 Announce Type: cross  Abstract: The creation of instruction data and evaluation benchmarks for serving Large language models often involves enormous human annotation. This issue becomes particularly pronounced when rapidly developing such resources for a non-English language like Japanese. Instead of following the popular practice of directly translating existing English resources into Japanese (e.g., Japanese-Alpaca), we propose an efficient self-instruct method based on GPT-4. We first translate a small amount of English instructions into Japanese and post-edit them to obtain native-level quality. GPT-4 then utilizes them as demonstrations to automatically generate Japanese instruction data. We also construct an evaluation benchmark containing 80 questions across 8 categories, using GPT-4 to automatically assess the response quality of LLMs without human references. The empirical results suggest that the models fine-tuned on our GPT-4 self-instruct data significant",
    "link": "https://arxiv.org/abs/2403.03690",
    "context": "Title: Rapidly Developing High-quality Instruction Data and Evaluation Benchmark for Large Language Models with Minimal Human Effort: A Case Study on Japanese\nAbstract: arXiv:2403.03690v1 Announce Type: cross  Abstract: The creation of instruction data and evaluation benchmarks for serving Large language models often involves enormous human annotation. This issue becomes particularly pronounced when rapidly developing such resources for a non-English language like Japanese. Instead of following the popular practice of directly translating existing English resources into Japanese (e.g., Japanese-Alpaca), we propose an efficient self-instruct method based on GPT-4. We first translate a small amount of English instructions into Japanese and post-edit them to obtain native-level quality. GPT-4 then utilizes them as demonstrations to automatically generate Japanese instruction data. We also construct an evaluation benchmark containing 80 questions across 8 categories, using GPT-4 to automatically assess the response quality of LLMs without human references. The empirical results suggest that the models fine-tuned on our GPT-4 self-instruct data significant",
    "path": "papers/24/03/2403.03690.json",
    "total_tokens": 840,
    "translated_title": "快速开发高质量的指令数据和评估基准，减少人力投入：以日语为例的案例研究",
    "translated_abstract": "为了为大型语言模型提供服务，创建指令数据和评估基准通常需要大量的人工标注。当为日语等非英语语言快速开发这些资源时，这个问题尤为突出。我们提出了一种基于GPT-4的高效自指导方法，而不是直接将现有的英语资源翻译成日语（例如Japanese-Alpaca）。我们首先将少量英语指令翻译成日语，并进行后期编辑以获得native-level质量。然后，GPT-4利用这些指令作为示范，自动生成日语指令数据。我们还利用GPT-4构建了一个包含80个问题跨8个类别的评估基准，使用GPT-4自动评估LLMs的响应质量，无需人工参考。实证结果表明，对我们的GPT-4自指导数据进行微调的模型显著",
    "tldr": "通过GPT-4自指导方法，快速开发高质量的日语指令数据和评估基准，无需大量人力投入，并为大型语言模型提供了有效的资源",
    "en_tdlr": "Rapidly developing high-quality instruction data and evaluation benchmark for Japanese language models with minimal human effort using self-instruct method based on GPT-4."
}