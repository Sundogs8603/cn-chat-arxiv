{
    "title": "WikiFactDiff: A Large, Realistic, and Temporally Adaptable Dataset for Atomic Factual Knowledge Update in Causal Language Models",
    "abstract": "arXiv:2403.14364v1 Announce Type: new  Abstract: The factuality of large language model (LLMs) tends to decay over time since events posterior to their training are \"unknown\" to them. One way to keep models up-to-date could be factual update: the task of inserting, replacing, or removing certain simple (atomic) facts within the model. To study this task, we present WikiFactDiff, a dataset that describes the evolution of factual knowledge between two dates as a collection of simple facts divided into three categories: new, obsolete, and static. We describe several update scenarios arising from various combinations of these three types of basic update. The facts are represented by subject-relation-object triples; indeed, WikiFactDiff was constructed by comparing the state of the Wikidata knowledge base at 4 January 2021 and 27 February 2023. Those fact are accompanied by verbalization templates and cloze tests that enable running update algorithms and their evaluation metrics. Contrary t",
    "link": "https://arxiv.org/abs/2403.14364",
    "context": "Title: WikiFactDiff: A Large, Realistic, and Temporally Adaptable Dataset for Atomic Factual Knowledge Update in Causal Language Models\nAbstract: arXiv:2403.14364v1 Announce Type: new  Abstract: The factuality of large language model (LLMs) tends to decay over time since events posterior to their training are \"unknown\" to them. One way to keep models up-to-date could be factual update: the task of inserting, replacing, or removing certain simple (atomic) facts within the model. To study this task, we present WikiFactDiff, a dataset that describes the evolution of factual knowledge between two dates as a collection of simple facts divided into three categories: new, obsolete, and static. We describe several update scenarios arising from various combinations of these three types of basic update. The facts are represented by subject-relation-object triples; indeed, WikiFactDiff was constructed by comparing the state of the Wikidata knowledge base at 4 January 2021 and 27 February 2023. Those fact are accompanied by verbalization templates and cloze tests that enable running update algorithms and their evaluation metrics. Contrary t",
    "path": "papers/24/03/2403.14364.json",
    "total_tokens": 915,
    "translated_title": "WikiFactDiff: 一个大型、现实且时间上可适应的数据集，用于因果语言模型中的原子事实知识更新",
    "translated_abstract": "大型语言模型的事实性随时间衰减，因为它们的训练之后的事件对它们来说是“未知”的。为了保持模型的最新，一种方法可能是通过事实更新：即在模型中插入、替换或删除某些简单（原子）事实的任务。为了研究这一任务，我们提出了WikiFactDiff，这是一个描述事实知识在两个日期之间演变的数据集，其中包含了被分为三类的一系列简单事实：新事实、过时事实和静态事实。我们描述了由这三种基本更新类型的各种组合产生的多个更新方案。这些事实由主-谓-宾三元组表示；确实，WikiFactDiff是通过比较Wikidata知识库在2021年1月4日和2023年2月27日期间的状态而构建的。这些事实伴随有用于运行更新算法和评估指标的文本化模板和填空测试。",
    "tldr": "WikiFactDiff是一个用于因果语言模型中的原子事实知识更新的大型、现实且时间上可适应的数据集，通过描述事实知识在两个日期之间的演变，并提供不同类型基本更新的更新方案和评估指标。",
    "en_tdlr": "WikiFactDiff is a large, realistic, and temporally adaptable dataset for atomic factual knowledge update in causal language models, providing scenarios for updating factual knowledge between two dates and evaluation metrics for different types of basic updates."
}