{
    "title": "Do Generated Data Always Help Contrastive Learning?",
    "abstract": "arXiv:2403.12448v1 Announce Type: cross  Abstract: Contrastive Learning (CL) has emerged as one of the most successful paradigms for unsupervised visual representation learning, yet it often depends on intensive manual data augmentations. With the rise of generative models, especially diffusion models, the ability to generate realistic images close to the real data distribution has been well recognized. These generated high-equality images have been successfully applied to enhance contrastive representation learning, a technique termed ``data inflation''. However, we find that the generated data (even from a good diffusion model like DDPM) may sometimes even harm contrastive learning. We investigate the causes behind this failure from the perspective of both data inflation and data augmentation. For the first time, we reveal the complementary roles that stronger data inflation should be accompanied by weaker augmentations, and vice versa. We also provide rigorous theoretical explanatio",
    "link": "https://arxiv.org/abs/2403.12448",
    "context": "Title: Do Generated Data Always Help Contrastive Learning?\nAbstract: arXiv:2403.12448v1 Announce Type: cross  Abstract: Contrastive Learning (CL) has emerged as one of the most successful paradigms for unsupervised visual representation learning, yet it often depends on intensive manual data augmentations. With the rise of generative models, especially diffusion models, the ability to generate realistic images close to the real data distribution has been well recognized. These generated high-equality images have been successfully applied to enhance contrastive representation learning, a technique termed ``data inflation''. However, we find that the generated data (even from a good diffusion model like DDPM) may sometimes even harm contrastive learning. We investigate the causes behind this failure from the perspective of both data inflation and data augmentation. For the first time, we reveal the complementary roles that stronger data inflation should be accompanied by weaker augmentations, and vice versa. We also provide rigorous theoretical explanatio",
    "path": "papers/24/03/2403.12448.json",
    "total_tokens": 856,
    "translated_title": "生成的数据总是有助于对比学习吗？",
    "translated_abstract": "对比学习（CL）已经成为无监督视觉表示学习中最成功的范式之一，然而它往往依赖大量手工数据增强。随着生成模型的兴起，特别是扩散模型，生成接近真实数据分布的逼真图像的能力得到了很好的认可。这些生成的高质量图像已成功应用于增强对比表示学习，一种称为“数据膨胀”的技术。然而，我们发现生成的数据（甚至来自像DDPM这样的好扩散模型）有时甚至会对对比学习造成伤害。我们从数据膨胀和数据增强的角度探讨了这种失败的原因。我们首次揭示了更强的数据膨胀应该伴随着更弱的增强，反之亦然的互补作用。我们还提供了严格的理论解释。",
    "tldr": "生成的高质量图像已成功应用于增强对比表示学习，但我们发现有时生成的数据甚至会对对比学习造成伤害，通过研究发现更强的数据膨胀应该伴随着更弱的增强。",
    "en_tdlr": "Generated high-quality images have been successfully applied to enhance contrastive representation learning, but we find that sometimes generated data may even harm contrastive learning, and through our investigation, we reveal that stronger data inflation should be accompanied by weaker augmentations."
}