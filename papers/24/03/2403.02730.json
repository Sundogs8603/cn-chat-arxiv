{
    "title": "A Two-Stage Training Method for Modeling Constrained Systems With Neural Networks",
    "abstract": "arXiv:2403.02730v1 Announce Type: new  Abstract: Real-world systems are often formulated as constrained optimization problems. Techniques to incorporate constraints into Neural Networks (NN), such as Neural Ordinary Differential Equations (Neural ODEs), have been used. However, these introduce hyperparameters that require manual tuning through trial and error, raising doubts about the successful incorporation of constraints into the generated model. This paper describes in detail the two-stage training method for Neural ODEs, a simple, effective, and penalty parameter-free approach to model constrained systems. In this approach the constrained optimization problem is rewritten as two unconstrained sub-problems that are solved in two stages. The first stage aims at finding feasible NN parameters by minimizing a measure of constraints violation. The second stage aims to find the optimal NN parameters by minimizing the loss function while keeping inside the feasible region. We experimenta",
    "link": "https://arxiv.org/abs/2403.02730",
    "context": "Title: A Two-Stage Training Method for Modeling Constrained Systems With Neural Networks\nAbstract: arXiv:2403.02730v1 Announce Type: new  Abstract: Real-world systems are often formulated as constrained optimization problems. Techniques to incorporate constraints into Neural Networks (NN), such as Neural Ordinary Differential Equations (Neural ODEs), have been used. However, these introduce hyperparameters that require manual tuning through trial and error, raising doubts about the successful incorporation of constraints into the generated model. This paper describes in detail the two-stage training method for Neural ODEs, a simple, effective, and penalty parameter-free approach to model constrained systems. In this approach the constrained optimization problem is rewritten as two unconstrained sub-problems that are solved in two stages. The first stage aims at finding feasible NN parameters by minimizing a measure of constraints violation. The second stage aims to find the optimal NN parameters by minimizing the loss function while keeping inside the feasible region. We experimenta",
    "path": "papers/24/03/2403.02730.json",
    "total_tokens": 823,
    "translated_title": "用于建模受限系统的神经网络的两阶段训练方法",
    "translated_abstract": "现实世界中的系统经常被制定为受限制的优化问题。将约束引入神经网络（NN）的技术，如神经常微分方程（Neural ODEs），已经被使用。然而，这些方法会引入需要通过试错手动调整的超参数，这使得约束成功融入生成模型中产生疑虑。本文详细描述了神经常微分方程的两阶段训练方法，这是一种简单、有效且无需惩罚参数的方法，用于建模受限系统。在这种方法中，约束优化问题被重写为两个无约束的子问题，分两个阶段解决。第一阶段的目标是通过最小化约束违反程度来找到合适的NN参数。第二阶段的目标是通过最小化损失函数来找到最佳的NN参数，同时保持在可行域内。",
    "tldr": "描述了一种用于模拟受限系统的神经常微分方程的两阶段训练方法，在第一阶段寻找合适的神经网络参数，第二阶段找到最佳参数并保持在可行域内。",
    "en_tdlr": "Introduces a two-stage training method for modeling constrained systems with Neural ODEs, aiming to find suitable and optimal neural network parameters while maintaining feasibility."
}