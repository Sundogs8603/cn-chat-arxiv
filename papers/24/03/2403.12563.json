{
    "title": "Simple Hack for Transformers against Heavy Long-Text Classification on a Time- and Memory-Limited GPU Service",
    "abstract": "arXiv:2403.12563v1 Announce Type: cross  Abstract: Many NLP researchers rely on free computational services, such as Google Colab, to fine-tune their Transformer models, causing a limitation for hyperparameter optimization (HPO) in long-text classification due to the method having quadratic complexity and needing a bigger resource. In Indonesian, only a few works were found on long-text classification using Transformers. Most only use a small amount of data and do not report any HPO. In this study, using 18k news articles, we investigate which pretrained models are recommended to use based on the output length of the tokenizer. We then compare some hacks to shorten and enrich the sequences, which are the removals of stopwords, punctuation, low-frequency words, and recurring words. To get a fair comparison, we propose and run an efficient and dynamic HPO procedure that can be done gradually on a limited resource and does not require a long-running optimization library. Using the best ha",
    "link": "https://arxiv.org/abs/2403.12563",
    "context": "Title: Simple Hack for Transformers against Heavy Long-Text Classification on a Time- and Memory-Limited GPU Service\nAbstract: arXiv:2403.12563v1 Announce Type: cross  Abstract: Many NLP researchers rely on free computational services, such as Google Colab, to fine-tune their Transformer models, causing a limitation for hyperparameter optimization (HPO) in long-text classification due to the method having quadratic complexity and needing a bigger resource. In Indonesian, only a few works were found on long-text classification using Transformers. Most only use a small amount of data and do not report any HPO. In this study, using 18k news articles, we investigate which pretrained models are recommended to use based on the output length of the tokenizer. We then compare some hacks to shorten and enrich the sequences, which are the removals of stopwords, punctuation, low-frequency words, and recurring words. To get a fair comparison, we propose and run an efficient and dynamic HPO procedure that can be done gradually on a limited resource and does not require a long-running optimization library. Using the best ha",
    "path": "papers/24/03/2403.12563.json",
    "total_tokens": 856,
    "translated_title": "针对时间和内存受限的GPU服务上的大文本分类的Transformer简单技巧",
    "translated_abstract": "许多NLP研究人员依赖免费的计算服务，如Google Colab，来优化他们的Transformer模型，但由于该方法具有二次复杂性并需要更大的资源，这导致了在长文本分类中的超参数优化（HPO）存在局限性。在印尼，仅发现了少量关于使用Transformer进行长文本分类的研究。大多数仅使用少量数据，并且没有报告任何HPO。在这项研究中，我们使用18k篇新闻文章，研究了基于分词器输出长度建议使用哪些预训练模型。然后，我们比较了一些缩短和丰富序列的技巧，包括停用词、标点符号、低频词和重复词的去除。为了进行公平比较，我们提出并运行了一种高效动态的HPO过程，可以逐步在有限资源上进行，并且不需要长时间运行的优化库。利用最佳的方法...",
    "tldr": "通过在有限资源上逐步执行高效动态的HPO过程，提出了一种注重将Transformers模型适用于长文本分类任务的简单技巧。",
    "en_tdlr": "Proposing a simple hack that focuses on adapting Transformer models for long-text classification tasks by gradually executing an efficient dynamic HPO process on limited resources."
}