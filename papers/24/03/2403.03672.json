{
    "title": "Learning Adversarial MDPs with Stochastic Hard Constraints",
    "abstract": "arXiv:2403.03672v1 Announce Type: new  Abstract: We study online learning problems in constrained Markov decision processes (CMDPs) with adversarial losses and stochastic hard constraints. We consider two different scenarios. In the first one, we address general CMDPs, where we design an algorithm that attains sublinear regret and cumulative positive constraints violation. In the second scenario, under the mild assumption that a policy strictly satisfying the constraints exists and is known to the learner, we design an algorithm that achieves sublinear regret while ensuring that the constraints are satisfied at every episode with high probability. To the best of our knowledge, our work is the first to study CMDPs involving both adversarial losses and hard constraints. Indeed, previous works either focus on much weaker soft constraints--allowing for positive violation to cancel out negative ones--or are restricted to stochastic losses. Thus, our algorithms can deal with general non-stat",
    "link": "https://arxiv.org/abs/2403.03672",
    "context": "Title: Learning Adversarial MDPs with Stochastic Hard Constraints\nAbstract: arXiv:2403.03672v1 Announce Type: new  Abstract: We study online learning problems in constrained Markov decision processes (CMDPs) with adversarial losses and stochastic hard constraints. We consider two different scenarios. In the first one, we address general CMDPs, where we design an algorithm that attains sublinear regret and cumulative positive constraints violation. In the second scenario, under the mild assumption that a policy strictly satisfying the constraints exists and is known to the learner, we design an algorithm that achieves sublinear regret while ensuring that the constraints are satisfied at every episode with high probability. To the best of our knowledge, our work is the first to study CMDPs involving both adversarial losses and hard constraints. Indeed, previous works either focus on much weaker soft constraints--allowing for positive violation to cancel out negative ones--or are restricted to stochastic losses. Thus, our algorithms can deal with general non-stat",
    "path": "papers/24/03/2403.03672.json",
    "total_tokens": 878,
    "translated_title": "在具有随机硬约束的对抗MDP中学习",
    "translated_abstract": "我们研究带有对抗损失和随机硬约束的受限马尔可夫决策过程（CMDP）中的在线学习问题。我们考虑两种不同的情形。在第一种情形中，我们解决了一般CMDP问题，设计了一个算法，实现了次线性遗憾和累积正约束违反。在第二种情形中，在一个政策严格满足约束存在且为学习者所了解的温和假设下，我们设计了一个算法，实现了次线性遗憾，同时确保在每一轮中约束以高概率得到满足。据我们所知，我们的工作是第一个研究既涉及对抗损失又涉及硬约束的CMDP的工作。实际上，先前的研究要么集中在更弱的软约束上--允许正违反来抵消负违反--要么局限于随机损失。因此，我们的算法可以处理一般的非统计",
    "tldr": "本论文首次研究了涉及对抗损失和硬约束的CMDP，在两种不同情形下设计了具有次线性遗憾的算法，填补了先前研究中对这一问题的空白。",
    "en_tdlr": "This paper studies CMDPs involving both adversarial losses and hard constraints, proposing algorithms with sublinear regret in two different scenarios and filling a gap in previous research on this issue."
}