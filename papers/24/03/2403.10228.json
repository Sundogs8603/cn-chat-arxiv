{
    "title": "HawkEye: Training Video-Text LLMs for Grounding Text in Videos",
    "abstract": "arXiv:2403.10228v1 Announce Type: cross  Abstract: Video-text Large Language Models (video-text LLMs) have shown remarkable performance in answering questions and holding conversations on simple videos. However, they perform almost the same as random on grounding text queries in long and complicated videos, having little ability to understand and reason about temporal information, which is the most fundamental difference between videos and images. In this paper, we propose HawkEye, one of the first video-text LLMs that can perform temporal video grounding in a fully text-to-text manner. To collect training data that is applicable for temporal video grounding, we construct InternVid-G, a large-scale video-text corpus with segment-level captions and negative spans, with which we introduce two new time-aware training objectives to video-text LLMs. We also propose a coarse-grained method of representing segments in videos, which is more robust and easier for LLMs to learn and follow than o",
    "link": "https://arxiv.org/abs/2403.10228",
    "context": "Title: HawkEye: Training Video-Text LLMs for Grounding Text in Videos\nAbstract: arXiv:2403.10228v1 Announce Type: cross  Abstract: Video-text Large Language Models (video-text LLMs) have shown remarkable performance in answering questions and holding conversations on simple videos. However, they perform almost the same as random on grounding text queries in long and complicated videos, having little ability to understand and reason about temporal information, which is the most fundamental difference between videos and images. In this paper, we propose HawkEye, one of the first video-text LLMs that can perform temporal video grounding in a fully text-to-text manner. To collect training data that is applicable for temporal video grounding, we construct InternVid-G, a large-scale video-text corpus with segment-level captions and negative spans, with which we introduce two new time-aware training objectives to video-text LLMs. We also propose a coarse-grained method of representing segments in videos, which is more robust and easier for LLMs to learn and follow than o",
    "path": "papers/24/03/2403.10228.json",
    "total_tokens": 893,
    "translated_title": "HawkEye: 用于将文本与视频相关联的训练视频文本LLMs",
    "translated_abstract": "视频文本大型语言模型（video-text LLMs）在回答问题和进行简单视频对话方面表现出色。然而，在长而复杂的视频中，它们在文本查询上的表现几乎与随机相同，几乎没有能力理解和推理关于时间信息的内容，这是视频和图像之间最基本的区别。本文提出了HawkEye，这是第一个可以完全以文本方式执行时间视频定位的视频文本LLMs之一。为了收集适用于时间视频定位的训练数据，我们构建了InternVid-G，一个具有分段级标题和负间距的大规模视频文本语料库，通过该语料库引入了两个新的面向时间的训练目标以供视频文本LLMs使用。我们还提出了一种表示视频中段的粗粒度方法，这种方法比LLMs学习和遵循的方法更稳健且更易学习。",
    "tldr": "本文提出了HawkEye，一个可以以完全文本方式执行时间视频定位的视频文本LLMs，并通过构建大规模视频文本语料库InternVid-G以及引入两个新的面向时间的训练目标，以及一种新的粗粒度表示视频段的方法来实现这一目标。",
    "en_tdlr": "This paper introduces HawkEye, one of the first video-text LLMs that can perform temporal video grounding in a fully text-to-text manner, by constructing a large-scale video-text corpus InternVid-G, introducing two new time-aware training objectives, and proposing a new coarse-grained method of representing segments in videos."
}