{
    "title": "Have You Poisoned My Data? Defending Neural Networks against Data Poisoning",
    "abstract": "arXiv:2403.13523v1 Announce Type: new  Abstract: The unprecedented availability of training data fueled the rapid development of powerful neural networks in recent years. However, the need for such large amounts of data leads to potential threats such as poisoning attacks: adversarial manipulations of the training data aimed at compromising the learned model to achieve a given adversarial goal.   This paper investigates defenses against clean-label poisoning attacks and proposes a novel approach to detect and filter poisoned datapoints in the transfer learning setting. We define a new characteristic vector representation of datapoints and show that it effectively captures the intrinsic properties of the data distribution. Through experimental analysis, we demonstrate that effective poisons can be successfully differentiated from clean points in the characteristic vector space. We thoroughly evaluate our proposed approach and compare it to existing state-of-the-art defenses using multip",
    "link": "https://arxiv.org/abs/2403.13523",
    "context": "Title: Have You Poisoned My Data? Defending Neural Networks against Data Poisoning\nAbstract: arXiv:2403.13523v1 Announce Type: new  Abstract: The unprecedented availability of training data fueled the rapid development of powerful neural networks in recent years. However, the need for such large amounts of data leads to potential threats such as poisoning attacks: adversarial manipulations of the training data aimed at compromising the learned model to achieve a given adversarial goal.   This paper investigates defenses against clean-label poisoning attacks and proposes a novel approach to detect and filter poisoned datapoints in the transfer learning setting. We define a new characteristic vector representation of datapoints and show that it effectively captures the intrinsic properties of the data distribution. Through experimental analysis, we demonstrate that effective poisons can be successfully differentiated from clean points in the characteristic vector space. We thoroughly evaluate our proposed approach and compare it to existing state-of-the-art defenses using multip",
    "path": "papers/24/03/2403.13523.json",
    "total_tokens": 836,
    "translated_title": "你中了数据毒药吗？防御神经网络免受数据毒药攻击",
    "translated_abstract": "近年来，训练数据的空前可用性推动了强大神经网络的快速发展。然而，对于如此大量数据的需求导致了潜在威胁，如数据毒药攻击：针对训练数据的对抗性篡改，旨在损害学习模型以实现给定的对抗性目标。本文研究了防御对干净标签数据的毒药攻击，并提出了一种新方法来检测和过滤在迁移学习环境中的被毒害数据点。我们定义了数据点的新特征向量表示，并展示其有效地捕捉了数据分布的固有属性。通过实验分析，我们表明有效的毒成分可以成功区分出特征向量空间中的干净点。我们对所提出的方法进行了彻底评估，并与现有最先进的防御方法进行了比较。",
    "tldr": "本文研究神经网络如何防御数据毒药攻击，提出了一种新方法来检测和过滤被毒害的数据点，有效区分出特征向量空间中的干净点。",
    "en_tdlr": "This paper investigates defenses against data poisoning attacks on neural networks and proposes a novel approach to detect and filter poisoned data points, effectively distinguishing clean points in the characteristic vector space."
}