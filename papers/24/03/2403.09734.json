{
    "title": "Do Large Language Models Solve ARC Visual Analogies Like People Do?",
    "abstract": "arXiv:2403.09734v1 Announce Type: cross  Abstract: The Abstraction Reasoning Corpus (ARC) is a visual analogical reasoning test designed for humans and machines (Chollet, 2019). We compared human and large language model (LLM) performance on a new child-friendly set of ARC items. Results show that both children and adults outperform most LLMs on these tasks. Error analysis revealed a similar \"fallback\" solution strategy in LLMs and young children, where part of the analogy is simply copied. In addition, we found two other error types, one based on seemingly grasping key concepts (e.g., Inside-Outside) and the other based on simple combinations of analogy input matrices. On the whole, \"concept\" errors were more common in humans, and \"matrix\" errors were more common in LLMs. This study sheds new light on LLM reasoning ability and the extent to which we can use error analyses and comparisons with human development to understand how LLMs solve visual analogies.",
    "link": "https://arxiv.org/abs/2403.09734",
    "context": "Title: Do Large Language Models Solve ARC Visual Analogies Like People Do?\nAbstract: arXiv:2403.09734v1 Announce Type: cross  Abstract: The Abstraction Reasoning Corpus (ARC) is a visual analogical reasoning test designed for humans and machines (Chollet, 2019). We compared human and large language model (LLM) performance on a new child-friendly set of ARC items. Results show that both children and adults outperform most LLMs on these tasks. Error analysis revealed a similar \"fallback\" solution strategy in LLMs and young children, where part of the analogy is simply copied. In addition, we found two other error types, one based on seemingly grasping key concepts (e.g., Inside-Outside) and the other based on simple combinations of analogy input matrices. On the whole, \"concept\" errors were more common in humans, and \"matrix\" errors were more common in LLMs. This study sheds new light on LLM reasoning ability and the extent to which we can use error analyses and comparisons with human development to understand how LLMs solve visual analogies.",
    "path": "papers/24/03/2403.09734.json",
    "total_tokens": 963,
    "translated_title": "大型语言模型是否像人一样解决ARC视觉类比问题？",
    "translated_abstract": "抑制论文（Chollet, 2019）形式，我们比较了儿童友好的ARC项目上人类和大型语言模型（LLM）的表现。结果表明，无论是儿童还是成年人，在这些任务上都胜过大多数LLMs。错误分析揭示了LLMs和年幼儿童之间类似的“倒退”解决策略，其中类比的一部分被简单复制。此外，我们发现其他两种错误类型，一种基于表面掌握关键概念（例如，内外关系），另一种基于类比输入矩阵的简单组合。总体而言，“概念”错误在人类中更常见，“矩阵”错误在LLMs中更常见。这项研究为LLM的推理能力和我们可以使用错误分析以及与人类发展的比较来理解LLMs如何解决视觉类比问题提供了新的视角。",
    "tldr": "该研究比较了人类和大型语言模型在ARC视觉类比问题上的表现，发现在特定任务上，人类和成年人的表现均优于大多数大型语言模型。对LLMs和年幼儿童错误分析揭示了类似的解决策略，同时指出了两种不同的错误类型，为我们理解LLMs如何解决视觉类比问题提供了新的启示。",
    "en_tdlr": "This study compared the performance of humans and large language models on visual analogical reasoning tasks, revealing that both children and adults outperform most LLMs in specific tasks. Error analyses between LLMs and young children showed similar solution strategies while highlighting two distinct error types, shedding new light on understanding how LLMs solve visual analogies."
}