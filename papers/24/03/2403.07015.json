{
    "title": "Adaptive Hyperparameter Optimization for Continual Learning Scenarios",
    "abstract": "arXiv:2403.07015v1 Announce Type: new  Abstract: Hyperparameter selection in continual learning scenarios is a challenging and underexplored aspect, especially in practical non-stationary environments. Traditional approaches, such as grid searches with held-out validation data from all tasks, are unrealistic for building accurate lifelong learning systems. This paper aims to explore the role of hyperparameter selection in continual learning and the necessity of continually and automatically tuning them according to the complexity of the task at hand. Hence, we propose leveraging the nature of sequence task learning to improve Hyperparameter Optimization efficiency. By using the functional analysis of variance-based techniques, we identify the most crucial hyperparameters that have an impact on performance. We demonstrate empirically that this approach, agnostic to continual scenarios and strategies, allows us to speed up hyperparameters optimization continually across tasks and exhibit",
    "link": "https://arxiv.org/abs/2403.07015",
    "context": "Title: Adaptive Hyperparameter Optimization for Continual Learning Scenarios\nAbstract: arXiv:2403.07015v1 Announce Type: new  Abstract: Hyperparameter selection in continual learning scenarios is a challenging and underexplored aspect, especially in practical non-stationary environments. Traditional approaches, such as grid searches with held-out validation data from all tasks, are unrealistic for building accurate lifelong learning systems. This paper aims to explore the role of hyperparameter selection in continual learning and the necessity of continually and automatically tuning them according to the complexity of the task at hand. Hence, we propose leveraging the nature of sequence task learning to improve Hyperparameter Optimization efficiency. By using the functional analysis of variance-based techniques, we identify the most crucial hyperparameters that have an impact on performance. We demonstrate empirically that this approach, agnostic to continual scenarios and strategies, allows us to speed up hyperparameters optimization continually across tasks and exhibit",
    "path": "papers/24/03/2403.07015.json",
    "total_tokens": 798,
    "translated_title": "针对持续学习场景的自适应超参数优化",
    "translated_abstract": "在持续学习场景中，超参数选择是一个具有挑战性和尚未充分探索的方面，特别是在实际非平稳环境中。本文旨在探讨超参数选择在持续学习中的作用，以及根据手头任务的复杂性持续自动调整它们的必要性。因此，我们提出利用序列任务学习的特性来提高超参数优化效率。通过使用基于方差的功能分析技术，我们识别出对性能产生影响的最关键的超参数。我们通过经验性地证明，这种方法无视持续场景和策略，使我们能够持续加快超参数在不同任务间的优化，并展示",
    "tldr": "本文旨在探讨在持续学习中的超参数选择作用和根据任务复杂性持续自动调整它们的必要性，通过利用序列任务学习特性来提高超参数优化效率，实验证明这种方法可以使超参数优化在不同任务中持续加速。",
    "en_tdlr": "This paper aims to explore the role of hyperparameter selection and the necessity of continually adjusting them based on task complexity in continual learning, by leveraging the characteristics of sequence task learning to improve optimization efficiency, empirically demonstrating the acceleration of hyperparameter optimization across tasks."
}