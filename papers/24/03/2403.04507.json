{
    "title": "NLPre: a revised approach towards language-centric benchmarking of Natural Language Preprocessing systems",
    "abstract": "arXiv:2403.04507v1 Announce Type: new  Abstract: With the advancements of transformer-based architectures, we observe the rise of natural language preprocessing (NLPre) tools capable of solving preliminary NLP tasks (e.g. tokenisation, part-of-speech tagging, dependency parsing, or morphological analysis) without any external linguistic guidance. It is arduous to compare novel solutions to well-entrenched preprocessing toolkits, relying on rule-based morphological analysers or dictionaries. Aware of the shortcomings of existing NLPre evaluation approaches, we investigate a novel method of reliable and fair evaluation and performance reporting. Inspired by the GLUE benchmark, the proposed language-centric benchmarking system enables comprehensive ongoing evaluation of multiple NLPre tools, while credibly tracking their performance. The prototype application is configured for Polish and integrated with the thoroughly assembled NLPre-PL benchmark. Based on this benchmark, we conduct an ex",
    "link": "https://arxiv.org/abs/2403.04507",
    "context": "Title: NLPre: a revised approach towards language-centric benchmarking of Natural Language Preprocessing systems\nAbstract: arXiv:2403.04507v1 Announce Type: new  Abstract: With the advancements of transformer-based architectures, we observe the rise of natural language preprocessing (NLPre) tools capable of solving preliminary NLP tasks (e.g. tokenisation, part-of-speech tagging, dependency parsing, or morphological analysis) without any external linguistic guidance. It is arduous to compare novel solutions to well-entrenched preprocessing toolkits, relying on rule-based morphological analysers or dictionaries. Aware of the shortcomings of existing NLPre evaluation approaches, we investigate a novel method of reliable and fair evaluation and performance reporting. Inspired by the GLUE benchmark, the proposed language-centric benchmarking system enables comprehensive ongoing evaluation of multiple NLPre tools, while credibly tracking their performance. The prototype application is configured for Polish and integrated with the thoroughly assembled NLPre-PL benchmark. Based on this benchmark, we conduct an ex",
    "path": "papers/24/03/2403.04507.json",
    "total_tokens": 865,
    "translated_title": "NLPre: 一种面向自然语言处理系统的语言中心基准测试方法的改进",
    "translated_abstract": "随着基于变压器的架构的不断发展，我们观察到自然语言处理（NLPre）工具的崛起，这些工具能够解决初步的NLP任务（例如标记化、词性标注、依存句法分析或形态分析）而无需任何外部语言指导。在费时费力地比较新型解决方案与依赖基于规则的形态分析器或词典的成熟预处理工具包的基础上是很困难的。鉴于现有NLPre评估方法的缺陷，我们探讨了一种可靠和公平的评估方法和性能报告。受GLUE基准测试的启发，提出的语言中心基准测试系统实现了对多个NLPre工具的全面持续评估，同时可靠地跟踪它们的性能。原型应用程序被配置为波兰语，并与精心组织的NLPre-PL基准套件集成。基于这一基准，我们进行了一项实验...",
    "tldr": "NLPre提出了一种可靠且公平的语言中心基准测试方法，使得可以全面持续评估多个NLPre工具的性能，并可靠地跟踪其表现。",
    "en_tdlr": "NLPre introduces a reliable and fair language-centric benchmarking system, enabling comprehensive and ongoing evaluation of multiple NLPre tools with credible performance tracking."
}