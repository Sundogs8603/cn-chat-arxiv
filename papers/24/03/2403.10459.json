{
    "title": "Understanding the Double Descent Phenomenon in Deep Learning",
    "abstract": "arXiv:2403.10459v1 Announce Type: new  Abstract: Combining empirical risk minimization with capacity control is a classical strategy in machine learning when trying to control the generalization gap and avoid overfitting, as the model class capacity gets larger. Yet, in modern deep learning practice, very large over-parameterized models (e.g. neural networks) are optimized to fit perfectly the training data and still obtain great generalization performance. Past the interpolation point, increasing model complexity seems to actually lower the test error.   In this tutorial, we explain the concept of double descent and its mechanisms. The first section sets the classical statistical learning framework and introduces the double descent phenomenon. By looking at a number of examples, section 2 introduces inductive biases that appear to have a key role in double descent by selecting, among the multiple interpolating solutions, a smooth empirical risk minimizer. Finally, section 3 explores t",
    "link": "https://arxiv.org/abs/2403.10459",
    "context": "Title: Understanding the Double Descent Phenomenon in Deep Learning\nAbstract: arXiv:2403.10459v1 Announce Type: new  Abstract: Combining empirical risk minimization with capacity control is a classical strategy in machine learning when trying to control the generalization gap and avoid overfitting, as the model class capacity gets larger. Yet, in modern deep learning practice, very large over-parameterized models (e.g. neural networks) are optimized to fit perfectly the training data and still obtain great generalization performance. Past the interpolation point, increasing model complexity seems to actually lower the test error.   In this tutorial, we explain the concept of double descent and its mechanisms. The first section sets the classical statistical learning framework and introduces the double descent phenomenon. By looking at a number of examples, section 2 introduces inductive biases that appear to have a key role in double descent by selecting, among the multiple interpolating solutions, a smooth empirical risk minimizer. Finally, section 3 explores t",
    "path": "papers/24/03/2403.10459.json",
    "total_tokens": 832,
    "translated_title": "深度学习中的双下降现象探究",
    "translated_abstract": "将经验风险最小化与容量控制相结合是机器学习中经典的策略，用于控制泛化差距并避免过拟合，因为模型类容量变大。然而，在现代深度学习实践中，非常庞大的过参数化模型（如神经网络）被优化以完美拟合训练数据，并且仍然可以获得良好的泛化性能。超越插值点后，增加模型复杂度似乎实际上会降低测试误差。在本教程中，我们解释了双下降的概念及其机制。第一部分建立了经典的统计学习框架并介绍了双下降现象。通过观察多个示例，第二部分介绍了归纳偏差，在双下降中选择平滑的经验风险最小化器起着关键作用。最后，第三部分探讨了t",
    "tldr": "在现代深度学习中，庞大的过参数化模型通过增加模型复杂度来降低测试误差，这就是双下降现象。",
    "en_tdlr": "In modern deep learning, very large over-parameterized models reduce test error by increasing model complexity, which is the double descent phenomenon."
}