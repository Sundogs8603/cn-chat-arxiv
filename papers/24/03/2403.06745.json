{
    "title": "ACT-MNMT Auto-Constriction Turning for Multilingual Neural Machine Translation",
    "abstract": "arXiv:2403.06745v1 Announce Type: cross  Abstract: Large language model (LLM) has achieved promising performance in multilingual machine translation tasks through zero/few-shot prompts or prompt-tuning. However, due to the mixture of multilingual data during the pre-training of LLM, the LLM-based translation models face the off-target issue in both prompt-based methods, including a series of phenomena, namely instruction misunderstanding, translation with wrong language and over-generation. For this issue, this paper introduces an \\textbf{\\underline{A}}uto-\\textbf{\\underline{C}}onstriction \\textbf{\\underline{T}}urning mechanism for \\textbf{\\underline{M}}ultilingual \\textbf{\\underline{N}}eural \\textbf{\\underline{M}}achine \\textbf{\\underline{T}}ranslation (\\model), which is a novel supervised fine-tuning mechanism and orthogonal to the traditional prompt-based methods. In this method, \\model automatically constructs a constrained template in the target side by adding trigger tokens ahead",
    "link": "https://arxiv.org/abs/2403.06745",
    "context": "Title: ACT-MNMT Auto-Constriction Turning for Multilingual Neural Machine Translation\nAbstract: arXiv:2403.06745v1 Announce Type: cross  Abstract: Large language model (LLM) has achieved promising performance in multilingual machine translation tasks through zero/few-shot prompts or prompt-tuning. However, due to the mixture of multilingual data during the pre-training of LLM, the LLM-based translation models face the off-target issue in both prompt-based methods, including a series of phenomena, namely instruction misunderstanding, translation with wrong language and over-generation. For this issue, this paper introduces an \\textbf{\\underline{A}}uto-\\textbf{\\underline{C}}onstriction \\textbf{\\underline{T}}urning mechanism for \\textbf{\\underline{M}}ultilingual \\textbf{\\underline{N}}eural \\textbf{\\underline{M}}achine \\textbf{\\underline{T}}ranslation (\\model), which is a novel supervised fine-tuning mechanism and orthogonal to the traditional prompt-based methods. In this method, \\model automatically constructs a constrained template in the target side by adding trigger tokens ahead",
    "path": "papers/24/03/2403.06745.json",
    "total_tokens": 867,
    "translated_title": "ACT-MNMT自动收缩转向多语言神经机器翻译",
    "translated_abstract": "大型语言模型（LLM）通过零/少shot提示或提示调整在多语言机器翻译任务中取得了令人期待的表现。然而，由于在LLM的预训练过程中混合了多语言数据，基于LLM的翻译模型在基于提示的方法中面临了离靶问题，包括一系列现象，即指令误解、错误语言翻译和过度生成。针对这一问题，本文介绍了一种用于多语言神经机器翻译（\\model）的\\textbf{\\underline{A}}uto-\\textbf{\\underline{C}}onstriction \\textbf{\\underline{T}}urning机制，这是一种新颖的监督微调机制，与传统的基于提示的方法正交。在这种方法中，\\model通过在目标端添加触发令牌自动构造受限模板。",
    "tldr": "该论文介绍了一种针对多语言神经机器翻译中出现的离靶问题的新型监督微调机制ACT-MNMT Auto-Constriction Turning，与传统基于提示的方法正交，并通过自动构建受限模板来解决该问题。",
    "en_tdlr": "This paper introduces a novel supervised fine-tuning mechanism ACT-MNMT Auto-Constriction Turning for addressing off-target issues in multilingual neural machine translation, orthogonal to traditional prompt-based methods, by automatically constructing constrained templates."
}