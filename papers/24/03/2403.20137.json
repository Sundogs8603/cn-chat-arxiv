{
    "title": "Accurate Block Quantization in LLMs with Outliers",
    "abstract": "arXiv:2403.20137v1 Announce Type: new  Abstract: The demand for inference on extremely large scale LLMs has seen enormous growth in the recent months. It made evident the colossal shortage of dedicated hardware capable of efficient and fast processing of the involved compute and memory movement. The problem is aggravated by the exploding raise in the lengths of the sequences being processed, since those require efficient on-chip storage of the KV-cache of size proportional to the sequence length. To make the required compute feasible and fit the involved data into available memory, numerous quantization techniques have been proposed that allow accurate quantization for both weights and activations. One of the main recent breakthroughs in this direction was introduction of the family of Block Floating Point (BFP) formats characterized by a block of mantissas with a shared scale factor. These enable memory- power-, and compute- efficient hardware support of the tensor operations and prov",
    "link": "https://arxiv.org/abs/2403.20137",
    "context": "Title: Accurate Block Quantization in LLMs with Outliers\nAbstract: arXiv:2403.20137v1 Announce Type: new  Abstract: The demand for inference on extremely large scale LLMs has seen enormous growth in the recent months. It made evident the colossal shortage of dedicated hardware capable of efficient and fast processing of the involved compute and memory movement. The problem is aggravated by the exploding raise in the lengths of the sequences being processed, since those require efficient on-chip storage of the KV-cache of size proportional to the sequence length. To make the required compute feasible and fit the involved data into available memory, numerous quantization techniques have been proposed that allow accurate quantization for both weights and activations. One of the main recent breakthroughs in this direction was introduction of the family of Block Floating Point (BFP) formats characterized by a block of mantissas with a shared scale factor. These enable memory- power-, and compute- efficient hardware support of the tensor operations and prov",
    "path": "papers/24/03/2403.20137.json",
    "total_tokens": 806,
    "translated_title": "LLM中带有离群值的准确块量化",
    "translated_abstract": "最近几个月来，对LLMs进行极大规模推理的需求呈现出巨大增长。这凸显了专用硬件严重短缺的问题，因为它们无法高效快速地处理相关的计算和内存移动。问题加剧于所处理序列长度不断增长，因为这些序列需要与序列长度成比例的KV-cache的高效片上存储。为了使所需计算可行并将相关数据放入可用内存中，已经提出了许多量化技术，允许权重和激活的准确量化。在这方面的一个主要最近突破是引入一组具有共享比例因子的尾数块的Block浮点（BFP）格式，这些技术使得具有高效硬件支持的张量操作的记忆、功耗和计算变得更加高效。",
    "tldr": "LLM中出现的离群值问题，通过引入Block浮点（BFP）格式，实现了准确的块量化，解决了大规模推理需求中的硬件支持效率问题。",
    "en_tdlr": "Accurate block quantization in LLMs with outliers is achieved through the introduction of Block Floating Point (BFP) formats, addressing the efficiency issue in hardware support for massive-scale inference demand."
}