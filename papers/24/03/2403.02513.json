{
    "title": "Balancing Enhancement, Harmlessness, and General Capabilities: Enhancing Conversational LLMs with Direct RLHF",
    "abstract": "arXiv:2403.02513v1 Announce Type: new  Abstract: In recent advancements in Conversational Large Language Models (LLMs), a concerning trend has emerged, showing that many new base LLMs experience a knowledge reduction in their foundational capabilities following Supervised Fine-Tuning (SFT). This process often leads to issues such as forgetting or a decrease in the base model's abilities. Moreover, fine-tuned models struggle to align with user preferences, inadvertently increasing the generation of toxic outputs when specifically prompted. To overcome these challenges, we adopted an innovative approach by completely bypassing SFT and directly implementing Harmless Reinforcement Learning from Human Feedback (RLHF). Our method not only preserves the base model's general capabilities but also significantly enhances its conversational abilities, while notably reducing the generation of toxic outputs. Our approach holds significant implications for fields that demand a nuanced understanding ",
    "link": "https://arxiv.org/abs/2403.02513",
    "context": "Title: Balancing Enhancement, Harmlessness, and General Capabilities: Enhancing Conversational LLMs with Direct RLHF\nAbstract: arXiv:2403.02513v1 Announce Type: new  Abstract: In recent advancements in Conversational Large Language Models (LLMs), a concerning trend has emerged, showing that many new base LLMs experience a knowledge reduction in their foundational capabilities following Supervised Fine-Tuning (SFT). This process often leads to issues such as forgetting or a decrease in the base model's abilities. Moreover, fine-tuned models struggle to align with user preferences, inadvertently increasing the generation of toxic outputs when specifically prompted. To overcome these challenges, we adopted an innovative approach by completely bypassing SFT and directly implementing Harmless Reinforcement Learning from Human Feedback (RLHF). Our method not only preserves the base model's general capabilities but also significantly enhances its conversational abilities, while notably reducing the generation of toxic outputs. Our approach holds significant implications for fields that demand a nuanced understanding ",
    "path": "papers/24/03/2403.02513.json",
    "total_tokens": 875,
    "translated_title": "在增强对话式LLM与直接RLHF的无害性和一般性能之间取得平衡",
    "translated_abstract": "在最近对话式大型语言模型（LLMs）的发展中，出现了一个令人关注的趋势，即许多新的基本LLMs在经过监督微调（SFT）后会经历基础能力的知识减少。这一过程经常导致问题，比如遗忘或基本模型能力的降低。此外，微调模型往往难以与用户偏好保持一致，在特定提示时无意中增加有毒输出的生成。为了克服这些挑战，我们采用了一种创新的方法，即完全绕过SFT并直接实施无害的人类反馈强化学习（RLHF）。我们的方法不仅保留了基本模型的一般能力，还显著增强了其对话能力，同时明显减少了有毒输出的生成。我们的方法对那些需要微妙理解的领域具有重要意义",
    "tldr": "采用直接实施无害的人类反馈强化学习（RLHF）的创新方法，不仅保留了基本模型的一般能力，还显著增强了其对话能力，同时明显减少了有毒输出的生成",
    "en_tdlr": "An innovative approach of directly implementing Harmless Reinforcement Learning from Human Feedback (RLHF) preserves the base model's general capabilities, significantly enhances its conversational abilities, and notably reduces the generation of toxic outputs."
}