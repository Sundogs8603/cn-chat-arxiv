{
    "title": "HGT: Leveraging Heterogeneous Graph-enhanced Large Language Models for Few-shot Complex Table Understanding",
    "abstract": "arXiv:2403.19723v1 Announce Type: cross  Abstract: Table understanding (TU) has achieved promising advancements, but it faces the challenges of the scarcity of manually labeled tables and the presence of complex table structures.To address these challenges, we propose HGT, a framework with a heterogeneous graph (HG)-enhanced large language model (LLM) to tackle few-shot TU tasks.It leverages the LLM by aligning the table semantics with the LLM's parametric knowledge through soft prompts and instruction turning and deals with complex tables by a multi-task pre-training scheme involving three novel multi-granularity self-supervised HG pre-training objectives.We empirically demonstrate the effectiveness of HGT, showing that it outperforms the SOTA for few-shot complex TU on several benchmarks.",
    "link": "https://arxiv.org/abs/2403.19723",
    "context": "Title: HGT: Leveraging Heterogeneous Graph-enhanced Large Language Models for Few-shot Complex Table Understanding\nAbstract: arXiv:2403.19723v1 Announce Type: cross  Abstract: Table understanding (TU) has achieved promising advancements, but it faces the challenges of the scarcity of manually labeled tables and the presence of complex table structures.To address these challenges, we propose HGT, a framework with a heterogeneous graph (HG)-enhanced large language model (LLM) to tackle few-shot TU tasks.It leverages the LLM by aligning the table semantics with the LLM's parametric knowledge through soft prompts and instruction turning and deals with complex tables by a multi-task pre-training scheme involving three novel multi-granularity self-supervised HG pre-training objectives.We empirically demonstrate the effectiveness of HGT, showing that it outperforms the SOTA for few-shot complex TU on several benchmarks.",
    "path": "papers/24/03/2403.19723.json",
    "total_tokens": 775,
    "translated_title": "HGT：利用异质图增强的大型语言模型进行少样本复杂表格理解",
    "translated_abstract": "表格理解 (TU) 取得了显著进展，但面临手动标记表格的稀缺性和复杂表格结构的挑战。为解决这些问题，我们提出了 HGT 框架，其中包含一个异质图 (HG) 增强的大型语言模型 (LLM)，用于解决少样本 TU 任务。它通过软提示和指导转换将表格语义与LLM的参数化知识对齐，并通过涉及三种新的多粒度自监督HG预训练目标的多任务预训练方案处理复杂表格。我们在几个基准测试上通过实证方法展示了HGT的有效性，表明它在少样本复杂TU方面的表现优于SOTA。",
    "tldr": "HGT框架结合了异质图增强的大型语言模型，通过软提示和多粒度自监督HG预训练目标，实现了少样本复杂表格理解任务的最新成果。",
    "en_tdlr": "The HGT framework combines a heterogeneous graph-enhanced large language model, achieving the latest progress in few-shot complex table understanding tasks through soft prompts and multi-granularity self-supervised HG pre-training objectives."
}