{
    "title": "Are Compressed Language Models Less Subgroup Robust?",
    "abstract": "arXiv:2403.17811v1 Announce Type: cross  Abstract: To reduce the inference cost of large language models, model compression is increasingly used to create smaller scalable models. However, little is known about their robustness to minority subgroups defined by the labels and attributes of a dataset. In this paper, we investigate the effects of 18 different compression methods and settings on the subgroup robustness of BERT language models. We show that worst-group performance does not depend on model size alone, but also on the compression method used. Additionally, we find that model compression does not always worsen the performance on minority subgroups. Altogether, our analysis serves to further research into the subgroup robustness of model compression.",
    "link": "https://arxiv.org/abs/2403.17811",
    "context": "Title: Are Compressed Language Models Less Subgroup Robust?\nAbstract: arXiv:2403.17811v1 Announce Type: cross  Abstract: To reduce the inference cost of large language models, model compression is increasingly used to create smaller scalable models. However, little is known about their robustness to minority subgroups defined by the labels and attributes of a dataset. In this paper, we investigate the effects of 18 different compression methods and settings on the subgroup robustness of BERT language models. We show that worst-group performance does not depend on model size alone, but also on the compression method used. Additionally, we find that model compression does not always worsen the performance on minority subgroups. Altogether, our analysis serves to further research into the subgroup robustness of model compression.",
    "path": "papers/24/03/2403.17811.json",
    "total_tokens": 754,
    "translated_title": "压缩语言模型是否对子群体稳健性影响较小？",
    "translated_abstract": "为了减少大型语言模型的推理成本，越来越多地使用模型压缩来创建更小规模的模型。然而，我们对由数据集的标签和属性定义的少数子群体的稳健性知之甚少。在本文中，我们研究了18种不同的压缩方法和设置对BERT语言模型的子群体稳健性的影响。我们发现最差群组的性能不仅取决于模型大小，还取决于所使用的压缩方法。此外，我们发现模型压缩并不总是会使在少数子群体上的性能变差。总的来说，我们的分析有助于进一步研究模型压缩对子群体稳健性的影响。",
    "tldr": "压缩语言模型的影响不仅取决于模型大小，还取决于压缩方法，同时发现模型压缩并不总是会使在少数子群体上的性能变差。",
    "en_tdlr": "The impact of compressing language models depends not only on model size but also on the compression method used, and it is found that model compression does not always deteriorate performance on minority subgroups."
}