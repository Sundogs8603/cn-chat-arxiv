{
    "title": "Fully-fused Multi-Layer Perceptrons on Intel Data Center GPUs",
    "abstract": "arXiv:2403.17607v1 Announce Type: new  Abstract: This paper presents a SYCL implementation of Multi-Layer Perceptrons (MLPs), which targets and is optimized for the Intel Data Center GPU Max 1550. To increase the performance, our implementation minimizes the slow global memory accesses by maximizing the data reuse within the general register file and the shared local memory by fusing the operations in each layer of the MLP. We show with a simple roofline model that this results in a significant increase in the arithmetic intensity, leading to improved performance, especially for inference. We compare our approach to a similar CUDA implementation for MLPs and show that our implementation on the Intel Data Center GPU outperforms the CUDA implementation on Nvidia's H100 GPU by a factor up to 2.84 in inference and 1.75 in training. The paper also showcases the efficiency of our SYCL implementation in three significant areas: Image Compression, Neural Radiance Fields, and Physics-Informed M",
    "link": "https://arxiv.org/abs/2403.17607",
    "context": "Title: Fully-fused Multi-Layer Perceptrons on Intel Data Center GPUs\nAbstract: arXiv:2403.17607v1 Announce Type: new  Abstract: This paper presents a SYCL implementation of Multi-Layer Perceptrons (MLPs), which targets and is optimized for the Intel Data Center GPU Max 1550. To increase the performance, our implementation minimizes the slow global memory accesses by maximizing the data reuse within the general register file and the shared local memory by fusing the operations in each layer of the MLP. We show with a simple roofline model that this results in a significant increase in the arithmetic intensity, leading to improved performance, especially for inference. We compare our approach to a similar CUDA implementation for MLPs and show that our implementation on the Intel Data Center GPU outperforms the CUDA implementation on Nvidia's H100 GPU by a factor up to 2.84 in inference and 1.75 in training. The paper also showcases the efficiency of our SYCL implementation in three significant areas: Image Compression, Neural Radiance Fields, and Physics-Informed M",
    "path": "papers/24/03/2403.17607.json",
    "total_tokens": 886,
    "translated_title": "在英特尔数据中心GPU上全融合的多层感知机",
    "translated_abstract": "本文提出了一种针对英特尔数据中心GPU Max 1550进行优化的多层感知机（MLPs）的SYCL实现。为了提高性能，我们的实现通过融合MLP每一层中的操作，最小化了慢速全局内存访问，从而最大化了普通寄存器文件和共享本地内存中的数据重用。我们用一个简单的roofline模型展示了这种方法导致算术强度显著提高，从而提高了性能，尤其是推理方面。我们将我们的方法与类似的基于CUDA的MLP实现进行了比较，并展示了我们在英特尔数据中心GPU上对Nvidia的H100 GPU基于CUDA的实现在推理上的性能优势高达2.84倍，在训练上高达1.75倍。本文还展示了我们的SYCL实现在图像压缩、神经光辐射场和物理信息中的三个重要领域的高效性。",
    "tldr": "通过全融合操作、最小化全局内存访问以及最大化数据重用，本文提出了在英特尔数据中心GPU上的多层感知机实现，显著提高了性能，尤其在推理方面表现出色。",
    "en_tdlr": "By fully fusing operations, minimizing global memory accesses, and maximizing data reuse, this paper presents a Multi-Layer Perceptrons implementation on Intel Data Center GPU, which significantly improves performance, especially excelling in inference."
}