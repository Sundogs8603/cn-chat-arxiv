{
    "title": "Transformers for Low-Resource Languages:Is F\\'eidir Linn!",
    "abstract": "arXiv:2403.01985v1 Announce Type: cross  Abstract: The Transformer model is the state-of-the-art in Machine Translation. However, in general, neural translation models often under perform on language pairs with insufficient training data. As a consequence, relatively few experiments have been carried out using this architecture on low-resource language pairs. In this study, hyperparameter optimization of Transformer models in translating the low-resource English-Irish language pair is evaluated. We demonstrate that choosing appropriate parameters leads to considerable performance improvements. Most importantly, the correct choice of subword model is shown to be the biggest driver of translation performance. SentencePiece models using both unigram and BPE approaches were appraised. Variations on model architectures included modifying the number of layers, testing various regularisation techniques and evaluating the optimal number of heads for attention. A generic 55k DGT corpus and an i",
    "link": "https://arxiv.org/abs/2403.01985",
    "context": "Title: Transformers for Low-Resource Languages:Is F\\'eidir Linn!\nAbstract: arXiv:2403.01985v1 Announce Type: cross  Abstract: The Transformer model is the state-of-the-art in Machine Translation. However, in general, neural translation models often under perform on language pairs with insufficient training data. As a consequence, relatively few experiments have been carried out using this architecture on low-resource language pairs. In this study, hyperparameter optimization of Transformer models in translating the low-resource English-Irish language pair is evaluated. We demonstrate that choosing appropriate parameters leads to considerable performance improvements. Most importantly, the correct choice of subword model is shown to be the biggest driver of translation performance. SentencePiece models using both unigram and BPE approaches were appraised. Variations on model architectures included modifying the number of layers, testing various regularisation techniques and evaluating the optimal number of heads for attention. A generic 55k DGT corpus and an i",
    "path": "papers/24/03/2403.01985.json",
    "total_tokens": 804,
    "translated_title": "低资源语言的变压器：Is F\\'eidir Linn！",
    "translated_abstract": "Transformer 模型是机器翻译领域的最先进技术。然而，一般来说，神经翻译模型在训练数据不足的语言对上常常表现不佳。因此，对于低资源语言对，使用该结构进行实验的研究相对较少。本研究评估了将变压器模型进行超参数优化以翻译低资源的英语-爱尔兰语语言对。我们展示了选择适当的参数会带来相当大的性能提升。最重要的是，正确选择子词模型被证明是翻译性能最大的驱动因素。评估了使用 unigram 和 BPE 方法的 SentencePiece 模型。对模型架构的变化包括修改层数、测试各种正则化技术以及评估用于注意力的最佳头数。",
    "tldr": "本研究评估了对低资源的英语-爱尔兰语语言对进行超参数优化的 Transformer 模型，发现正确选择子词模型是翻译性能的最大驱动因素。"
}