{
    "title": "Leveraging Continuous Time to Understand Momentum When Training Diagonal Linear Networks",
    "abstract": "arXiv:2403.05293v1 Announce Type: new  Abstract: In this work, we investigate the effect of momentum on the optimisation trajectory of gradient descent. We leverage a continuous-time approach in the analysis of momentum gradient descent with step size $\\gamma$ and momentum parameter $\\beta$ that allows us to identify an intrinsic quantity $\\lambda = \\frac{ \\gamma }{ (1 - \\beta)^2 }$ which uniquely defines the optimisation path and provides a simple acceleration rule. When training a $2$-layer diagonal linear network in an overparametrised regression setting, we characterise the recovered solution through an implicit regularisation problem. We then prove that small values of $\\lambda$ help to recover sparse solutions. Finally, we give similar but weaker results for stochastic momentum gradient descent. We provide numerical experiments which support our claims.",
    "link": "https://arxiv.org/abs/2403.05293",
    "context": "Title: Leveraging Continuous Time to Understand Momentum When Training Diagonal Linear Networks\nAbstract: arXiv:2403.05293v1 Announce Type: new  Abstract: In this work, we investigate the effect of momentum on the optimisation trajectory of gradient descent. We leverage a continuous-time approach in the analysis of momentum gradient descent with step size $\\gamma$ and momentum parameter $\\beta$ that allows us to identify an intrinsic quantity $\\lambda = \\frac{ \\gamma }{ (1 - \\beta)^2 }$ which uniquely defines the optimisation path and provides a simple acceleration rule. When training a $2$-layer diagonal linear network in an overparametrised regression setting, we characterise the recovered solution through an implicit regularisation problem. We then prove that small values of $\\lambda$ help to recover sparse solutions. Finally, we give similar but weaker results for stochastic momentum gradient descent. We provide numerical experiments which support our claims.",
    "path": "papers/24/03/2403.05293.json",
    "total_tokens": 775,
    "translated_title": "利用连续时间理解对角线性网络训练中的动量",
    "translated_abstract": "在这项工作中，我们研究了动量对梯度下降优化轨迹的影响。我们利用连续时间方法分析带有步长 $\\gamma$ 和动量参数 $\\beta$ 的动量梯度下降，从而找到一个固有量 $\\lambda = \\frac{ \\gamma }{ (1 - \\beta)^2 }$，这一量唯一定义了优化路径并提供了一个简单的加速规则。在超参数化回归设置中训练 $2$ 层对角线性网络时，通过一个隐式正则化问题来表征恢复的解。然后证明小的 $\\lambda$ 值有助于恢复稀疏解。最后，我们给出了随机动量梯度下降的类似但较弱结果。我们提供支持我们论断的数值实验。",
    "tldr": "研究了动量对梯度下降优化轨迹的影响，通过连续时间方法找到固有量 $\\lambda$，对于恢复稀疏解具有帮助。",
    "en_tdlr": "Investigated the impact of momentum on the optimization trajectory of gradient descent, identified an intrinsic quantity $\\lambda$ through continuous-time approach, which helps recover sparse solutions."
}