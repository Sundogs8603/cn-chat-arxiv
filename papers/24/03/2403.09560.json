{
    "title": "Self-Consistency Training for Hamiltonian Prediction",
    "abstract": "arXiv:2403.09560v1 Announce Type: new  Abstract: Hamiltonian prediction is a versatile formulation to leverage machine learning for solving molecular science problems. Yet, its applicability is limited by insufficient labeled data for training. In this work, we highlight that Hamiltonian prediction possesses a self-consistency principle, based on which we propose an exact training method that does not require labeled data. This merit addresses the data scarcity difficulty, and distinguishes the task from other property prediction formulations with unique benefits: (1) self-consistency training enables the model to be trained on a large amount of unlabeled data, hence substantially enhances generalization; (2) self-consistency training is more efficient than labeling data with DFT for supervised training, since it is an amortization of DFT calculation over a set of molecular structures. We empirically demonstrate the better generalization in data-scarce and out-of-distribution scenarios",
    "link": "https://arxiv.org/abs/2403.09560",
    "context": "Title: Self-Consistency Training for Hamiltonian Prediction\nAbstract: arXiv:2403.09560v1 Announce Type: new  Abstract: Hamiltonian prediction is a versatile formulation to leverage machine learning for solving molecular science problems. Yet, its applicability is limited by insufficient labeled data for training. In this work, we highlight that Hamiltonian prediction possesses a self-consistency principle, based on which we propose an exact training method that does not require labeled data. This merit addresses the data scarcity difficulty, and distinguishes the task from other property prediction formulations with unique benefits: (1) self-consistency training enables the model to be trained on a large amount of unlabeled data, hence substantially enhances generalization; (2) self-consistency training is more efficient than labeling data with DFT for supervised training, since it is an amortization of DFT calculation over a set of molecular structures. We empirically demonstrate the better generalization in data-scarce and out-of-distribution scenarios",
    "path": "papers/24/03/2403.09560.json",
    "total_tokens": 857,
    "translated_title": "自洽训练用于哈密顿量预测",
    "translated_abstract": "arXiv:2403.09560v1 公告类型:新 提要: 哈密顿量预测是一种利用机器学习解决分子科学问题的多功能公式。然而，其适用性受到训练数据不足的限制。在这项工作中，我们强调哈密顿量预测具有自洽原理，基于此我们提出了一种不需要标记数据的精确训练方法。这一优点解决了数据稀缺困难，并将该任务与其他具有独特优势的属性预测公式区分开：（1）自洽训练使模型能够在大量未标记数据上训练，因此极大地增强了泛化能力；（2）自洽训练比使用DFT标记数据进行监督训练更有效，因为它是对一组分子结构上的DFT计算的摊销。我们通过实验证明，在数据稀缺和分布之外的情况下有更好的泛化能力。",
    "tldr": "提出了一种基于自洽原理的哈密顿量预测训练方法，无需标记数据，能够在大量未标记数据上训练，极大地增强了泛化能力，并提高了训练效率。"
}