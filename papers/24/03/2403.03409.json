{
    "title": "Sparse Spiking Neural Network: Exploiting Heterogeneity in Timescales for Pruning Recurrent SNN",
    "abstract": "arXiv:2403.03409v1 Announce Type: cross  Abstract: Recurrent Spiking Neural Networks (RSNNs) have emerged as a computationally efficient and brain-inspired learning model. The design of sparse RSNNs with fewer neurons and synapses helps reduce the computational complexity of RSNNs. Traditionally, sparse SNNs are obtained by first training a dense and complex SNN for a target task, and, then, pruning neurons with low activity (activity-based pruning) while maintaining task performance. In contrast, this paper presents a task-agnostic methodology for designing sparse RSNNs by pruning a large randomly initialized model. We introduce a novel Lyapunov Noise Pruning (LNP) algorithm that uses graph sparsification methods and utilizes Lyapunov exponents to design a stable sparse RSNN from a randomly initialized RSNN. We show that the LNP can leverage diversity in neuronal timescales to design a sparse Heterogeneous RSNN (HRSNN). Further, we show that the same sparse HRSNN model can be trained ",
    "link": "https://arxiv.org/abs/2403.03409",
    "context": "Title: Sparse Spiking Neural Network: Exploiting Heterogeneity in Timescales for Pruning Recurrent SNN\nAbstract: arXiv:2403.03409v1 Announce Type: cross  Abstract: Recurrent Spiking Neural Networks (RSNNs) have emerged as a computationally efficient and brain-inspired learning model. The design of sparse RSNNs with fewer neurons and synapses helps reduce the computational complexity of RSNNs. Traditionally, sparse SNNs are obtained by first training a dense and complex SNN for a target task, and, then, pruning neurons with low activity (activity-based pruning) while maintaining task performance. In contrast, this paper presents a task-agnostic methodology for designing sparse RSNNs by pruning a large randomly initialized model. We introduce a novel Lyapunov Noise Pruning (LNP) algorithm that uses graph sparsification methods and utilizes Lyapunov exponents to design a stable sparse RSNN from a randomly initialized RSNN. We show that the LNP can leverage diversity in neuronal timescales to design a sparse Heterogeneous RSNN (HRSNN). Further, we show that the same sparse HRSNN model can be trained ",
    "path": "papers/24/03/2403.03409.json",
    "total_tokens": 1006,
    "translated_title": "稀疏脉冲神经网络：利用时间尺度的异质性来剪枝循环SNN",
    "translated_abstract": "递归脉冲神经网络（RSNNs）已经被证明是一种计算效率高且启发于大脑的学习模型。稀疏RSNNs的设计通过减少神经元和突触的数量来降低RSNNs的计算复杂度。传统上，稀疏SNNs是通过首先训练一个密集而复杂的SNN来实现的，然后在保持任务性能的同时修剪低活跃度的神经元（基于活动的剪枝）来获得的。相比之下，本文提出了一种用于设计稀疏RSNNs的与任务无关的方法，通过修剪一个大型随机初始化模型。我们介绍了一种新颖的Lyapunov噪声剪枝（LNP）算法，该算法使用图的稀疏化方法，并利用Lyapunov指数从随机初始化的RSNN设计一个稳定的稀疏RSNN。我们展示LNP可以利用神经元时间尺度的多样性来设计稀疏异质RSNN（HRSNN）。此外，我们展示了相同的稀疏HRSNN模型可以被训练。",
    "tldr": "该论文提出了一种利用 Lyapunov Noise Pruning (LNP) 算法，通过随机初始化模型修剪神经元，利用神经元时间尺度的异质性设计出稀疏RSNN，实现了设计稀疏脉冲神经网络的任务-无关方法。",
    "en_tdlr": "This paper introduces a task-agnostic methodology for designing sparse RSNNs by utilizing the Lyapunov Noise Pruning (LNP) algorithm to prune neurons with heterogeneous timescales in a randomly initialized model, achieving a sparse Spiking Neural Network design."
}