{
    "title": "EEE-QA: Exploring Effective and Efficient Question-Answer Representations",
    "abstract": "arXiv:2403.02176v1 Announce Type: new  Abstract: Current approaches to question answering rely on pre-trained language models (PLMs) like RoBERTa. This work challenges the existing question-answer encoding convention and explores finer representations. We begin with testing various pooling methods compared to using the begin-of-sentence token as a question representation for better quality. Next, we explore opportunities to simultaneously embed all answer candidates with the question. This enables cross-reference between answer choices and improves inference throughput via reduced memory usage. Despite their simplicity and effectiveness, these methods have yet to be widely studied in current frameworks. We experiment with different PLMs, and with and without the integration of knowledge graphs. Results prove that the memory efficacy of the proposed techniques with little sacrifice in performance. Practically, our work enhances 38-100% throughput with 26-65% speedups on consumer-grade G",
    "link": "https://arxiv.org/abs/2403.02176",
    "context": "Title: EEE-QA: Exploring Effective and Efficient Question-Answer Representations\nAbstract: arXiv:2403.02176v1 Announce Type: new  Abstract: Current approaches to question answering rely on pre-trained language models (PLMs) like RoBERTa. This work challenges the existing question-answer encoding convention and explores finer representations. We begin with testing various pooling methods compared to using the begin-of-sentence token as a question representation for better quality. Next, we explore opportunities to simultaneously embed all answer candidates with the question. This enables cross-reference between answer choices and improves inference throughput via reduced memory usage. Despite their simplicity and effectiveness, these methods have yet to be widely studied in current frameworks. We experiment with different PLMs, and with and without the integration of knowledge graphs. Results prove that the memory efficacy of the proposed techniques with little sacrifice in performance. Practically, our work enhances 38-100% throughput with 26-65% speedups on consumer-grade G",
    "path": "papers/24/03/2403.02176.json",
    "total_tokens": 820,
    "translated_title": "EEE-QA: 探索有效和高效的问题-答案表示",
    "translated_abstract": "当前的问题回答方法依赖于预训练语言模型（PLMs）如RoBERTa。这项工作挑战现有的问题-答案编码约定，探索更精细的表示。我们首先测试了与使用句子开头标记作为问题表示相比，更好质量的各种池化方法。接下来，我们探索了同时将所有答案候选项与问题嵌入的机会。这样可以使答案选择之间进行交叉参考，并通过减少内存使用量来提高推断吞吐量。尽管这些方法简单而有效，但目前尚未在当前框架中得到广泛研究。我们尝试了不同的PLMs，并在是否集成知识图的情况下进行了实验。结果证明了所提出技术的内存效率，在性能上几乎没有牺牲。在实践中，我们的工作在消费级GPU上提高了38-100%的吞吐量，并加速了26-65%。",
    "tldr": "该研究探索了新的问题-答案编码方法，通过精细表示、答案候选项嵌入和内存效率提高推断效率。",
    "en_tdlr": "This research explores new question-answer encoding methods to improve inference efficiency through fine representation, embedding answer candidates, and memory efficacy."
}