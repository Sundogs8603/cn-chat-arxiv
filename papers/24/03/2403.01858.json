{
    "title": "An Improved Traditional Chinese Evaluation Suite for Foundation Model",
    "abstract": "arXiv:2403.01858v1 Announce Type: new  Abstract: We present TMMLU+, a comprehensive dataset designed for the Traditional Chinese massive multitask language understanding dataset. TMMLU+ is a multiple-choice question-answering dataset with 66 subjects from elementary to professional level. Compared to its predecessor, TMMLU, TMMLU+ is six times larger and boasts a more balanced subject distribution. We included benchmark results in TMMLU+ from closed-source models and 24 open-weight Chinese large language models of parameters ranging from 1.8B to 72B. Our findings reveal that Traditional Chinese models still trail behind their Simplified Chinese counterparts. Additionally, current large language models have yet to outperform human performance in average scores. We publicly release our dataset and the corresponding benchmark source code.",
    "link": "https://arxiv.org/abs/2403.01858",
    "context": "Title: An Improved Traditional Chinese Evaluation Suite for Foundation Model\nAbstract: arXiv:2403.01858v1 Announce Type: new  Abstract: We present TMMLU+, a comprehensive dataset designed for the Traditional Chinese massive multitask language understanding dataset. TMMLU+ is a multiple-choice question-answering dataset with 66 subjects from elementary to professional level. Compared to its predecessor, TMMLU, TMMLU+ is six times larger and boasts a more balanced subject distribution. We included benchmark results in TMMLU+ from closed-source models and 24 open-weight Chinese large language models of parameters ranging from 1.8B to 72B. Our findings reveal that Traditional Chinese models still trail behind their Simplified Chinese counterparts. Additionally, current large language models have yet to outperform human performance in average scores. We publicly release our dataset and the corresponding benchmark source code.",
    "path": "papers/24/03/2403.01858.json",
    "total_tokens": 817,
    "translated_title": "一种改进的传统中文基金模型评估套件",
    "translated_abstract": "我们提出了TMMLU+，这是一个为传统中文大规模多任务语言理解数据集设计的综合数据集。 TMMLU+是一个包含66个从基础到专业水平的选择题答题数据集。与其前身TMMLU相比，TMMLU+的规模大六倍，主题分布更加平衡。我们在TMMLU+中包含了来自闭源模型以及24个参数范围从1.8B到72B的开源中文大语言模型的基准结果。我们的研究发现传统中文模型仍然落后于简体中文对应模型。此外，目前的大语言模型在平均分数上仍未超过人类表现。我们公开发布了数据集及相应的基准源代码。",
    "tldr": "TMMLU+是传统中文大规模多任务语言理解数据集的改进版本，规模是前者的六倍，包含66个多样化主题。研究显示传统中文模型仍然落后于简体中文模型，并且目前的大语言模型在平均得分上尚未超过人类表现。",
    "en_tdlr": "TMMLU+ is an improved version of the Traditional Chinese massive multitask language understanding dataset, which is six times larger than its predecessor and includes 66 diverse subjects. The research shows that Traditional Chinese models still lag behind Simplified Chinese models, and current large language models have not yet exceeded human performance in average scores."
}