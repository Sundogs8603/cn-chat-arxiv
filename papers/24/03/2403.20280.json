{
    "title": "Sparse multimodal fusion with modal channel attention",
    "abstract": "arXiv:2403.20280v1 Announce Type: cross  Abstract: The ability of masked multimodal transformer architectures to learn a robust embedding space when modality samples are sparsely aligned is studied by measuring the quality of generated embedding spaces as a function of modal sparsity. An extension to the masked multimodal transformer model is proposed which incorporates modal-incomplete channels in the multihead attention mechanism called modal channel attention (MCA). Two datasets with 4 modalities are used, CMU-MOSEI for multimodal sentiment recognition and TCGA for multiomics. Models are shown to learn uniform and aligned embedding spaces with only two out of four modalities in most samples. It was found that, even with no modal sparsity, the proposed MCA mechanism improves the quality of generated embedding spaces, recall metrics, and subsequent performance on downstream tasks.",
    "link": "https://arxiv.org/abs/2403.20280",
    "context": "Title: Sparse multimodal fusion with modal channel attention\nAbstract: arXiv:2403.20280v1 Announce Type: cross  Abstract: The ability of masked multimodal transformer architectures to learn a robust embedding space when modality samples are sparsely aligned is studied by measuring the quality of generated embedding spaces as a function of modal sparsity. An extension to the masked multimodal transformer model is proposed which incorporates modal-incomplete channels in the multihead attention mechanism called modal channel attention (MCA). Two datasets with 4 modalities are used, CMU-MOSEI for multimodal sentiment recognition and TCGA for multiomics. Models are shown to learn uniform and aligned embedding spaces with only two out of four modalities in most samples. It was found that, even with no modal sparsity, the proposed MCA mechanism improves the quality of generated embedding spaces, recall metrics, and subsequent performance on downstream tasks.",
    "path": "papers/24/03/2403.20280.json",
    "total_tokens": 863,
    "translated_title": "带有模态通道注意力的稀疏多模态融合",
    "translated_abstract": "通过测量生成的嵌入空间质量作为模态稀疏度函数的能力，研究了蒙特卡罗多模态变换器架构在模态样本稀疏对齐时学习稳健嵌入空间的能力。提出了一种扩展的蒙特卡罗多模态变压器模型，该模型在多头注意机制中引入了模态不完全通道，称为模态通道注意力（MCA）。使用了包含4种模态的两个数据集，CMU-MOSEI用于多模态情感识别，TCGA用于多组学。模型显示在大多数样本中只用了四种模态中的两种就学习出统一且对齐的嵌入空间。发现，即使没有模态稀疏，所提出的MCA机制也能提高生成的嵌入空间质量，召回指标，并提高下游任务的性能。",
    "tldr": "研究了蒙特卡罗多模态变换器架构在模态样本稀疏对齐时学习稳健嵌入空间的能力，并提出了模态通道注意力（MCA）机制，可以改善生成的嵌入空间质量和下游任务性能。",
    "en_tdlr": "Explored the ability of masked multimodal transformer architectures to learn a robust embedding space when modality samples are sparsely aligned, and proposed the modal channel attention (MCA) mechanism to improve the quality of generated embedding spaces and downstream task performance."
}