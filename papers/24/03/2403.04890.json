{
    "title": "Few shot chain-of-thought driven reasoning to prompt LLMs for open ended medical question answering",
    "abstract": "arXiv:2403.04890v1 Announce Type: new  Abstract: Large Language models (LLMs) have demonstrated significant potential in transforming healthcare by automating tasks such as clinical documentation, information retrieval, and decision support. In this aspect, carefully engineered prompts have emerged as a powerful tool for using LLMs for medical scenarios, e.g., patient clinical scenarios. In this paper, we propose a modified version of the MedQA-USMLE dataset, which is subjective, to mimic real-life clinical scenarios. We explore the Chain of Thought (CoT) reasoning based on subjective response generation for the modified MedQA-USMLE dataset with appropriate LM-driven forward reasoning for correct responses to the medical questions. Keeping in mind the importance of response verification in the medical setting, we utilize a reward training mechanism whereby the language model also provides an appropriate verified response for a particular response to a clinical question. In this regard,",
    "link": "https://arxiv.org/abs/2403.04890",
    "context": "Title: Few shot chain-of-thought driven reasoning to prompt LLMs for open ended medical question answering\nAbstract: arXiv:2403.04890v1 Announce Type: new  Abstract: Large Language models (LLMs) have demonstrated significant potential in transforming healthcare by automating tasks such as clinical documentation, information retrieval, and decision support. In this aspect, carefully engineered prompts have emerged as a powerful tool for using LLMs for medical scenarios, e.g., patient clinical scenarios. In this paper, we propose a modified version of the MedQA-USMLE dataset, which is subjective, to mimic real-life clinical scenarios. We explore the Chain of Thought (CoT) reasoning based on subjective response generation for the modified MedQA-USMLE dataset with appropriate LM-driven forward reasoning for correct responses to the medical questions. Keeping in mind the importance of response verification in the medical setting, we utilize a reward training mechanism whereby the language model also provides an appropriate verified response for a particular response to a clinical question. In this regard,",
    "path": "papers/24/03/2403.04890.json",
    "total_tokens": 888,
    "translated_title": "基于少样本推动推理的链式思维驱动LLMs用于开放式医学问题回答",
    "translated_abstract": "大型语言模型（LLMs）已经展示了在转变医疗保健方面的巨大潜力，通过自动化诸如临床文档、信息检索和决策支持等任务。在这方面，精心设计的提示已经成为在医疗场景中使用LLMs的强大工具，例如患者临床场景。在本文中，我们提出了MedQA-USMLE数据集的修改版本，目的是模拟真实临床场景。我们探讨了基于主观响应生成的Chain of Thought（CoT）推理，用于修改后的MedQA-USMLE数据集，通过适当的LM驱动前向推理来获得正确的医学问题答案。考虑到在医疗环境中响应验证的重要性，我们利用奖励训练机制，其中语言模型还为特定的临床问题回应提供了适当的验证响应。",
    "tldr": "本文提出了基于少样本推动推理的链式思维驱动LLMs用于开放式医学问题回答，通过修改MedQA-USMLE数据集并采用奖励训练机制，实现了在医疗场景中正确响应临床问题的有效方法。",
    "en_tdlr": "This paper proposes few-shot chain-of-thought driven reasoning to prompt LLMs for open-ended medical question answering, by modifying the MedQA-USMLE dataset and utilizing a reward training mechanism, providing an effective approach for responding correctly to clinical questions in medical scenarios."
}