{
    "title": "A Stochastic Quasi-Newton Method for Non-convex Optimization with Non-uniform Smoothness",
    "abstract": "arXiv:2403.15244v1 Announce Type: new  Abstract: Classical convergence analyses for optimization algorithms rely on the widely-adopted uniform smoothness assumption. However, recent experimental studies have demonstrated that many machine learning problems exhibit non-uniform smoothness, meaning the smoothness factor is a function of the model parameter instead of a universal constant. In particular, it has been observed that the smoothness grows with respect to the gradient norm along the training trajectory. Motivated by this phenomenon, the recently introduced $(L_0, L_1)$-smoothness is a more general notion, compared to traditional $L$-smoothness, that captures such positive relationship between smoothness and gradient norm. Under this type of non-uniform smoothness, existing literature has designed stochastic first-order algorithms by utilizing gradient clipping techniques to obtain the optimal $\\mathcal{O}(\\epsilon^{-3})$ sample complexity for finding an $\\epsilon$-approximate fi",
    "link": "https://arxiv.org/abs/2403.15244",
    "context": "Title: A Stochastic Quasi-Newton Method for Non-convex Optimization with Non-uniform Smoothness\nAbstract: arXiv:2403.15244v1 Announce Type: new  Abstract: Classical convergence analyses for optimization algorithms rely on the widely-adopted uniform smoothness assumption. However, recent experimental studies have demonstrated that many machine learning problems exhibit non-uniform smoothness, meaning the smoothness factor is a function of the model parameter instead of a universal constant. In particular, it has been observed that the smoothness grows with respect to the gradient norm along the training trajectory. Motivated by this phenomenon, the recently introduced $(L_0, L_1)$-smoothness is a more general notion, compared to traditional $L$-smoothness, that captures such positive relationship between smoothness and gradient norm. Under this type of non-uniform smoothness, existing literature has designed stochastic first-order algorithms by utilizing gradient clipping techniques to obtain the optimal $\\mathcal{O}(\\epsilon^{-3})$ sample complexity for finding an $\\epsilon$-approximate fi",
    "path": "papers/24/03/2403.15244.json",
    "total_tokens": 963,
    "translated_title": "非凸优化的随机拟牛顿方法与非均匀平滑度",
    "translated_abstract": "传统优化算法的经典收敛分析依赖于广泛采用的均匀平滑度假设。然而，最近的实验研究表明，许多机器学习问题表现出非均匀平滑度，这意味着平滑度因子是模型参数的函数，而不是一个普遍常数。尤其是观察到，平滑度随着训练轨迹中的梯度范数增长。受这一现象的启发，最近引入的$(L_0, L_1)$-平滑度是一个比传统的$L$-平滑度更一般的概念，它捕捉了平滑度与梯度范数之间的这种正相关关系。在这种非均匀平滑度下，现有文献通过利用梯度裁剪技术设计了随机一阶算法，以获得找到$\\epsilon$-近似解的$\\mathcal{O}(\\epsilon^{-3})$样本复杂度。",
    "tldr": "论文提出了一种针对非凸优化问题的随机拟牛顿方法，适用于具有非均匀平滑度的情况，其创新之处在于引入了$(L_0, L_1)$-平滑度，相比传统的$L$-平滑度，能更好地捕捉平滑度与梯度范数之间的正相关关系。",
    "en_tdlr": "The paper presents a stochastic quasi-Newton method for non-convex optimization problems with non-uniform smoothness, introducing $(L_0, L_1)$-smoothness as a more general concept compared to traditional $L$-smoothness, better capturing the positive relationship between smoothness and gradient norm."
}