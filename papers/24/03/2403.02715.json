{
    "title": "Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of Vietnamese Large Language Models",
    "abstract": "arXiv:2403.02715v1 Announce Type: cross  Abstract: Recent advancements in large language models (LLMs) have underscored their importance in the evolution of artificial intelligence. However, despite extensive pretraining on multilingual datasets, available open-sourced LLMs exhibit limited effectiveness in processing Vietnamese. The challenge is exacerbated by the absence of systematic benchmark datasets and metrics tailored for Vietnamese LLM evaluation. To mitigate these issues, we have finetuned LLMs specifically for Vietnamese and developed a comprehensive evaluation framework encompassing 10 common tasks and 31 metrics. Our evaluation results reveal that the fine-tuned LLMs exhibit enhanced comprehension and generative capabilities in Vietnamese. Moreover, our analysis indicates that models with more parameters can introduce more biases and uncalibrated outputs and the key factor influencing LLM performance is the quality of the training or fine-tuning datasets. These insights und",
    "link": "https://arxiv.org/abs/2403.02715",
    "context": "Title: Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of Vietnamese Large Language Models\nAbstract: arXiv:2403.02715v1 Announce Type: cross  Abstract: Recent advancements in large language models (LLMs) have underscored their importance in the evolution of artificial intelligence. However, despite extensive pretraining on multilingual datasets, available open-sourced LLMs exhibit limited effectiveness in processing Vietnamese. The challenge is exacerbated by the absence of systematic benchmark datasets and metrics tailored for Vietnamese LLM evaluation. To mitigate these issues, we have finetuned LLMs specifically for Vietnamese and developed a comprehensive evaluation framework encompassing 10 common tasks and 31 metrics. Our evaluation results reveal that the fine-tuned LLMs exhibit enhanced comprehension and generative capabilities in Vietnamese. Moreover, our analysis indicates that models with more parameters can introduce more biases and uncalibrated outputs and the key factor influencing LLM performance is the quality of the training or fine-tuning datasets. These insights und",
    "path": "papers/24/03/2403.02715.json",
    "total_tokens": 896,
    "translated_title": "跨越语言视野：越南大型语言模型的微调和综合评估",
    "translated_abstract": "最近大型语言模型（LLMs）的进展强调了它们在人工智能发展中的重要性。然而，尽管在多语言数据集上进行了广泛的预训练，但目前开源的LLMs在处理越南语方面的效果有限。这一挑战是由于缺乏专门针对越南语LLM评估的系统化基准数据集和指标。为了缓解这些问题，我们专门为越南语进行了LLM的微调，并开发了一个涵盖10个常见任务和31个指标的综合评估框架。我们的评估结果表明，经过微调的LLMs在越南语方面表现出了增强的理解和生成能力。此外，我们的分析表明，具有更多参数的模型可能会带来更多的偏见和未校准的输出，而影响LLM性能的关键因素是训练或微调数据集的质量。",
    "tldr": "该研究通过对LLMs在越南语上进行微调和综合评估，发现微调后的模型在越南语理解和生成方面表现出良好能力，同时指出模型参数数量与性能之间存在着权衡关系，训练或微调数据集的质量是影响LLM性能的关键因素。",
    "en_tdlr": "This study fine-tuned and comprehensively evaluated LLMs for Vietnamese, showing improved comprehension and generation capabilities in the language. It also highlighted the trade-off between model parameters and performance, emphasizing the importance of training dataset quality in LLM performance."
}