{
    "title": "Function-space Parameterization of Neural Networks for Sequential Learning",
    "abstract": "arXiv:2403.10929v1 Announce Type: cross  Abstract: Sequential learning paradigms pose challenges for gradient-based deep learning due to difficulties incorporating new data and retaining prior knowledge. While Gaussian processes elegantly tackle these problems, they struggle with scalability and handling rich inputs, such as images. To address these issues, we introduce a technique that converts neural networks from weight space to function space, through a dual parameterization. Our parameterization offers: (i) a way to scale function-space methods to large data sets via sparsification, (ii) retention of prior knowledge when access to past data is limited, and (iii) a mechanism to incorporate new data without retraining. Our experiments demonstrate that we can retain knowledge in continual learning and incorporate new data efficiently. We further show its strengths in uncertainty quantification and guiding exploration in model-based RL. Further information and code is available on the",
    "link": "https://arxiv.org/abs/2403.10929",
    "context": "Title: Function-space Parameterization of Neural Networks for Sequential Learning\nAbstract: arXiv:2403.10929v1 Announce Type: cross  Abstract: Sequential learning paradigms pose challenges for gradient-based deep learning due to difficulties incorporating new data and retaining prior knowledge. While Gaussian processes elegantly tackle these problems, they struggle with scalability and handling rich inputs, such as images. To address these issues, we introduce a technique that converts neural networks from weight space to function space, through a dual parameterization. Our parameterization offers: (i) a way to scale function-space methods to large data sets via sparsification, (ii) retention of prior knowledge when access to past data is limited, and (iii) a mechanism to incorporate new data without retraining. Our experiments demonstrate that we can retain knowledge in continual learning and incorporate new data efficiently. We further show its strengths in uncertainty quantification and guiding exploration in model-based RL. Further information and code is available on the",
    "path": "papers/24/03/2403.10929.json",
    "total_tokens": 830,
    "translated_title": "神经网络的函数空间参数化用于序列学习",
    "translated_abstract": "由于在梯度下降深度学习中难以整合新数据并保留先前知识，顺序学习范式提出了挑战。虽然高斯过程优雅地解决了这些问题，但在处理诸如图像之类的丰富输入和伸缩性方面存在困难。为了解决这些问题，我们引入了一种将神经网络从权重空间转换到函数空间的技术，即双参数化。我们的参数化提供了：(i) 通过稀疏化将函数空间方法扩展到大数据集的途径，(ii) 在访问过去数据受限的情况下保留先前知识，以及(iii) 在不重新训练的情况下合并新数据的机制。我们的实验表明，我们可以在持续学习中保留知识，并有效地合并新数据。我们进一步展示了其在不确定性量化和引导基于模型的RL中探索的优点。",
    "tldr": "提出了一种神经网络的函数空间参数化方法，能够在序列学习中有效整合新数据并保留先前知识，同时在不重新训练的情况下合并新数据。",
    "en_tdlr": "Proposed a function-space parameterization technique for neural networks that effectively integrates new data and retains prior knowledge in sequential learning, while incorporating new data without retraining."
}