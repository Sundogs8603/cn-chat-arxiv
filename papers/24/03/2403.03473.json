{
    "title": "Inverse-Free Fast Natural Gradient Descent Method for Deep Learning",
    "abstract": "arXiv:2403.03473v1 Announce Type: new  Abstract: Second-order methods can converge much faster than first-order methods by incorporating second-order derivates or statistics, but they are far less prevalent in deep learning due to their computational inefficiency. To handle this, many of the existing solutions focus on reducing the size of the matrix to be inverted. However, it is still needed to perform the inverse operator in each iteration. In this paper, we present a fast natural gradient descent (FNGD) method, which only requires computing the inverse during the first epoch. Firstly, we reformulate the gradient preconditioning formula in the natural gradient descent (NGD) as a weighted sum of per-sample gradients using the Sherman-Morrison-Woodbury formula. Building upon this, to avoid the iterative inverse operation involved in computing coefficients, the weighted coefficients are shared across epochs without affecting the empirical performance.   FNGD approximates the NGD as a f",
    "link": "https://arxiv.org/abs/2403.03473",
    "context": "Title: Inverse-Free Fast Natural Gradient Descent Method for Deep Learning\nAbstract: arXiv:2403.03473v1 Announce Type: new  Abstract: Second-order methods can converge much faster than first-order methods by incorporating second-order derivates or statistics, but they are far less prevalent in deep learning due to their computational inefficiency. To handle this, many of the existing solutions focus on reducing the size of the matrix to be inverted. However, it is still needed to perform the inverse operator in each iteration. In this paper, we present a fast natural gradient descent (FNGD) method, which only requires computing the inverse during the first epoch. Firstly, we reformulate the gradient preconditioning formula in the natural gradient descent (NGD) as a weighted sum of per-sample gradients using the Sherman-Morrison-Woodbury formula. Building upon this, to avoid the iterative inverse operation involved in computing coefficients, the weighted coefficients are shared across epochs without affecting the empirical performance.   FNGD approximates the NGD as a f",
    "path": "papers/24/03/2403.03473.json",
    "total_tokens": 841,
    "translated_title": "无逆矩阵快速自然梯度下降方法用于深度学习",
    "translated_abstract": "二阶方法通过包含二阶导数或统计量可以比一阶方法收敛得更快，但由于计算效率低，它们在深度学习中很少被使用。为了解决这个问题，现有的许多解决方案都集中在减小需要求逆的矩阵的大小。然而，仍然需要在每次迭代中执行求逆操作。本文提出了一种快速自然梯度下降（FNGD）方法，只需在第一个时代计算逆运算。首先，我们将自然梯度下降（NGD）的梯度预处理公式重构为使用Sherman-Morrison-Woodbury公式的每个样本梯度的加权和。基于此，为了避免涉及计算系数的迭代逆操作，这些加权系数在整个时代共享而不影响经验性能。FNGD将NGD近似为f",
    "tldr": "本文提出一种快速自然梯度下降（FNGD）方法，在深度学习中仅需要在第一个时代计算逆运算，避免了迭代求逆操作。",
    "en_tdlr": "This paper proposes a fast natural gradient descent (FNGD) method that only requires computing the inverse during the first epoch in deep learning, avoiding iterative inverse operations."
}