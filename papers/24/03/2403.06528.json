{
    "title": "Adaptive Federated Learning Over the Air",
    "abstract": "arXiv:2403.06528v1 Announce Type: new  Abstract: We propose a federated version of adaptive gradient methods, particularly AdaGrad and Adam, within the framework of over-the-air model training. This approach capitalizes on the inherent superposition property of wireless channels, facilitating fast and scalable parameter aggregation. Meanwhile, it enhances the robustness of the model training process by dynamically adjusting the stepsize in accordance with the global gradient update. We derive the convergence rate of the training algorithms, encompassing the effects of channel fading and interference, for a broad spectrum of nonconvex loss functions. Our analysis shows that the AdaGrad-based algorithm converges to a stationary point at the rate of $\\mathcal{O}( \\ln{(T)} /{ T^{ 1 - \\frac{1}{\\alpha} } } )$, where $\\alpha$ represents the tail index of the electromagnetic interference. This result indicates that the level of heavy-tailedness in interference distribution plays a crucial role",
    "link": "https://arxiv.org/abs/2403.06528",
    "context": "Title: Adaptive Federated Learning Over the Air\nAbstract: arXiv:2403.06528v1 Announce Type: new  Abstract: We propose a federated version of adaptive gradient methods, particularly AdaGrad and Adam, within the framework of over-the-air model training. This approach capitalizes on the inherent superposition property of wireless channels, facilitating fast and scalable parameter aggregation. Meanwhile, it enhances the robustness of the model training process by dynamically adjusting the stepsize in accordance with the global gradient update. We derive the convergence rate of the training algorithms, encompassing the effects of channel fading and interference, for a broad spectrum of nonconvex loss functions. Our analysis shows that the AdaGrad-based algorithm converges to a stationary point at the rate of $\\mathcal{O}( \\ln{(T)} /{ T^{ 1 - \\frac{1}{\\alpha} } } )$, where $\\alpha$ represents the tail index of the electromagnetic interference. This result indicates that the level of heavy-tailedness in interference distribution plays a crucial role",
    "path": "papers/24/03/2403.06528.json",
    "total_tokens": 868,
    "translated_title": "适应性空中联邦学习",
    "translated_abstract": "我们在空中模型训练框架内提出了一种自适应梯度方法的联邦版本，特别是AdaGrad和Adam。这种方法充分利用了无线信道的固有叠加特性，促进了快速可伸缩的参数聚合。同时，通过根据全局梯度更新动态调整步长，增强了模型训练过程的稳健性。我们推导了训练算法的收敛速率，包括信道衰落和干扰对各种非凸损失函数的影响。我们的分析表明，基于AdaGrad的算法以$\\mathcal{O}( \\ln{(T)} /{ T^{ 1 - \\frac{1}{\\alpha} } } )$的速率收敛到一个稳态点，其中$\\alpha$表示电磁干扰的拖尾指数。这一结果表明，干扰分布的重尾特性水平起着关键作用。",
    "tldr": "提出了在空中模型训练框架内的自适应梯度方法的联邦版本，能够利用无线信道的叠加特性实现快速可伸缩的参数聚合，并通过动态调整步长增强了模型训练的稳健性。",
    "en_tdlr": "Proposed a federated version of adaptive gradient methods within the framework of over-the-air model training, leveraging the superposition property of wireless channels for fast and scalable parameter aggregation, while enhancing robustness through dynamically adjusting step size."
}