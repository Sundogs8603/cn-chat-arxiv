{
    "title": "Eye-gaze Guided Multi-modal Alignment Framework for Radiology",
    "abstract": "arXiv:2403.12416v1 Announce Type: cross  Abstract: In multi-modal frameworks, the alignment of cross-modal features presents a significant challenge. The predominant approach in multi-modal pre-training emphasizes either global or local alignment between modalities, utilizing extensive datasets. This bottom-up driven method often suffers from a lack of interpretability, a critical concern in radiology. Previous studies have integrated high-level labels in medical images or text, but these still rely on manual annotation, a costly and labor-intensive process. Our work introduces a novel approach by using eye-gaze data, collected synchronously by radiologists during diagnostic evaluations. This data, indicating radiologists' focus areas, naturally links chest X-rays to diagnostic texts. We propose the Eye-gaze Guided Multi-modal Alignment (EGMA) framework to harness eye-gaze data for better alignment of image and text features, aiming to reduce reliance on manual annotations and thus cut",
    "link": "https://arxiv.org/abs/2403.12416",
    "context": "Title: Eye-gaze Guided Multi-modal Alignment Framework for Radiology\nAbstract: arXiv:2403.12416v1 Announce Type: cross  Abstract: In multi-modal frameworks, the alignment of cross-modal features presents a significant challenge. The predominant approach in multi-modal pre-training emphasizes either global or local alignment between modalities, utilizing extensive datasets. This bottom-up driven method often suffers from a lack of interpretability, a critical concern in radiology. Previous studies have integrated high-level labels in medical images or text, but these still rely on manual annotation, a costly and labor-intensive process. Our work introduces a novel approach by using eye-gaze data, collected synchronously by radiologists during diagnostic evaluations. This data, indicating radiologists' focus areas, naturally links chest X-rays to diagnostic texts. We propose the Eye-gaze Guided Multi-modal Alignment (EGMA) framework to harness eye-gaze data for better alignment of image and text features, aiming to reduce reliance on manual annotations and thus cut",
    "path": "papers/24/03/2403.12416.json",
    "total_tokens": 702,
    "translated_title": "针对放射学的眼控引导多模态对齐框架",
    "translated_abstract": "在多模态框架中，跨模态特征的对齐是一个重要挑战。现有的方法强调全局或局部模态之间的对齐，利用大量数据集。然而，这种自底向上的方法在放射学中常常缺乏可解释性。我们的工作提出了一种新的方法，通过使用放射科医生在诊断评估过程中同步收集的眼控数据，将胸部X线自然地与诊断文本相关联，以更好地对齐图像和文本特征，旨在减少对手动注释的依赖。",
    "tldr": "提出一种利用眼控数据的多模态对齐框架，可降低对手动注释的依赖",
    "en_tdlr": "Proposed a multi-modal alignment framework utilizing eye-gaze data to reduce reliance on manual annotations."
}