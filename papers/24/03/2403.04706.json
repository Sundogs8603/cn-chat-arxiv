{
    "title": "Common 7B Language Models Already Possess Strong Math Capabilities",
    "abstract": "arXiv:2403.04706v1 Announce Type: cross  Abstract: Mathematical capabilities were previously believed to emerge in common language models only at a very large scale or require extensive math-related pre-training. This paper shows that the LLaMA-2 7B model with common pre-training already exhibits strong mathematical abilities, as evidenced by its impressive accuracy of 97.7% and 72.0% on the GSM8K and MATH benchmarks, respectively, when selecting the best response from 256 random generations. The primary issue with the current base model is the difficulty in consistently eliciting its inherent mathematical capabilities. Notably, the accuracy for the first answer drops to 49.5% and 7.9% on the GSM8K and MATH benchmarks, respectively. We find that simply scaling up the SFT data can significantly enhance the reliability of generating correct answers. However, the potential for extensive scaling is constrained by the scarcity of publicly available math questions. To overcome this limitatio",
    "link": "https://arxiv.org/abs/2403.04706",
    "context": "Title: Common 7B Language Models Already Possess Strong Math Capabilities\nAbstract: arXiv:2403.04706v1 Announce Type: cross  Abstract: Mathematical capabilities were previously believed to emerge in common language models only at a very large scale or require extensive math-related pre-training. This paper shows that the LLaMA-2 7B model with common pre-training already exhibits strong mathematical abilities, as evidenced by its impressive accuracy of 97.7% and 72.0% on the GSM8K and MATH benchmarks, respectively, when selecting the best response from 256 random generations. The primary issue with the current base model is the difficulty in consistently eliciting its inherent mathematical capabilities. Notably, the accuracy for the first answer drops to 49.5% and 7.9% on the GSM8K and MATH benchmarks, respectively. We find that simply scaling up the SFT data can significantly enhance the reliability of generating correct answers. However, the potential for extensive scaling is constrained by the scarcity of publicly available math questions. To overcome this limitatio",
    "path": "papers/24/03/2403.04706.json",
    "total_tokens": 855,
    "translated_title": "通用7B语言模型已经具备强大的数学能力",
    "translated_abstract": "先前人们相信通用语言模型只有在非常大的规模上或需要大量与数学相关的预训练才能展现出数学能力。本文表明，具有通用预训练的LLaMA-2 7B模型已经表现出强大的数学能力，其在GSM8K和MATH基准测试上选择256个随机生成的最佳响应时，准确率分别为97.7%和72.0%。目前基础模型的主要问题在于难以一致地引出其固有的数学能力。值得注意的是，对于第一个答案的准确率分别下降到了49.5%和7.9%。我们发现，简单地扩大SFT数据可以显著提高生成正确答案的可靠性。然而，广泛扩展的潜力受到公开可用数学问题的稀缺性的限制。为了克服这一限制",
    "tldr": "通用7B语言模型LLaMA-2展现出强大数学能力，准确率分别达到97.7%和72.0%，扩大SFT数据可以提高生成正确答案的可靠性",
    "en_tdlr": "Common 7B language model LLaMA-2 demonstrates strong mathematical abilities with accuracy of 97.7% and 72.0% on benchmarks, scaling SFT data enhances reliability of generating correct answers."
}