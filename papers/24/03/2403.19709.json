{
    "title": "Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models",
    "abstract": "arXiv:2403.19709v1 Announce Type: cross  Abstract: Parameter efficient adaptation methods have become a key mechanism to train large pre-trained models for downstream tasks. However, their per-task parameter overhead is considered still high when the number of downstream tasks to adapt for is large. We introduce an adapter module that has a better efficiency in large scale multi-task adaptation scenario. Our adapter is hierarchical in terms of how the adapter parameters are allocated. The adapter consists of a single shared controller network and multiple task-level adapter heads to reduce the per-task parameter overhead without performance regression on downstream tasks. The adapter is also recurrent so the entire adapter parameters are reused across different layers of the pre-trained model. Our Hierarchical Recurrent Adapter (HRA) outperforms the previous adapter-based approaches as well as full model fine-tuning baseline in both single and multi-task adaptation settings when evalua",
    "link": "https://arxiv.org/abs/2403.19709",
    "context": "Title: Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models\nAbstract: arXiv:2403.19709v1 Announce Type: cross  Abstract: Parameter efficient adaptation methods have become a key mechanism to train large pre-trained models for downstream tasks. However, their per-task parameter overhead is considered still high when the number of downstream tasks to adapt for is large. We introduce an adapter module that has a better efficiency in large scale multi-task adaptation scenario. Our adapter is hierarchical in terms of how the adapter parameters are allocated. The adapter consists of a single shared controller network and multiple task-level adapter heads to reduce the per-task parameter overhead without performance regression on downstream tasks. The adapter is also recurrent so the entire adapter parameters are reused across different layers of the pre-trained model. Our Hierarchical Recurrent Adapter (HRA) outperforms the previous adapter-based approaches as well as full model fine-tuning baseline in both single and multi-task adaptation settings when evalua",
    "path": "papers/24/03/2403.19709.json",
    "total_tokens": 814,
    "translated_title": "高效多任务调整大型语音模型的分层递归适配器",
    "translated_abstract": "参数高效的适配方法已经成为训练大型预训练模型用于下游任务的关键机制。我们引入了一种适配器模块，在大规模多任务适配场景下具有更好的效率。我们的适配器在适配器参数分配方面是分层的。适配器由一个共享的控制网络和多个任务级适配器头组成，以减少每个任务的参数开销，而不会影响下游任务的性能。适配器还是递归的，因此整个适配器参数在预训练模型的不同层之间被重用。我们的分层递归适配器（HRA）在单任务和多任务适配设置中都优于先前的基于适配器的方法以及完整模型微调基线。",
    "tldr": "提出了一种分层递归适配器模块，能够在大规模多任务适配场景下降低每个任务的参数开销，同时保持在下游任务中的性能表现，优于先前的适配器方法和完整模型微调基线",
    "en_tdlr": "Introduced a hierarchical recurrent adapter module that reduces per-task parameter overhead in large-scale multi-task adaptation scenario while maintaining performance on downstream tasks, outperforming previous adapter-based methods and full model fine-tuning baseline."
}