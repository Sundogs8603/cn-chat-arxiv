{
    "title": "Deep Gaussian Covariance Network with Trajectory Sampling for Data-Efficient Policy Search",
    "abstract": "arXiv:2403.15908v1 Announce Type: new  Abstract: Probabilistic world models increase data efficiency of model-based reinforcement learning (MBRL) by guiding the policy with their epistemic uncertainty to improve exploration and acquire new samples. Moreover, the uncertainty-aware learning procedures in probabilistic approaches lead to robust policies that are less sensitive to noisy observations compared to uncertainty unaware solutions. We propose to combine trajectory sampling and deep Gaussian covariance network (DGCN) for a data-efficient solution to MBRL problems in an optimal control setting. We compare trajectory sampling with density-based approximation for uncertainty propagation using three different probabilistic world models; Gaussian processes, Bayesian neural networks, and DGCNs. We provide empirical evidence using four different well-known test environments, that our method improves the sample-efficiency over other combinations of uncertainty propagation methods and prob",
    "link": "https://arxiv.org/abs/2403.15908",
    "context": "Title: Deep Gaussian Covariance Network with Trajectory Sampling for Data-Efficient Policy Search\nAbstract: arXiv:2403.15908v1 Announce Type: new  Abstract: Probabilistic world models increase data efficiency of model-based reinforcement learning (MBRL) by guiding the policy with their epistemic uncertainty to improve exploration and acquire new samples. Moreover, the uncertainty-aware learning procedures in probabilistic approaches lead to robust policies that are less sensitive to noisy observations compared to uncertainty unaware solutions. We propose to combine trajectory sampling and deep Gaussian covariance network (DGCN) for a data-efficient solution to MBRL problems in an optimal control setting. We compare trajectory sampling with density-based approximation for uncertainty propagation using three different probabilistic world models; Gaussian processes, Bayesian neural networks, and DGCNs. We provide empirical evidence using four different well-known test environments, that our method improves the sample-efficiency over other combinations of uncertainty propagation methods and prob",
    "path": "papers/24/03/2403.15908.json",
    "total_tokens": 823,
    "translated_title": "使用轨迹抽样的深度高斯协方差网络进行数据高效策略搜索",
    "translated_abstract": "概率世界模型通过利用其认识不确定性指导策略，提高了基于模型的强化学习（MBRL）的数据效率，改善了探索性能并获得了新样本。此外，概率方法中的不确定性感知学习流程导致的稳健策略比不考虑不确定性的解决方案对噪声观测更不敏感。我们提出将轨迹抽样和深度高斯协方差网络（DGCN）相结合，以在最优控制环境中实现MBRL问题的数据高效解决方案。我们使用高斯过程、贝叶斯神经网络和DGCN三种不同的概率世界模型，比较了轨迹抽样和基于密度的近似法在不确定性传播方面的效果。我们通过四个不同的知名测试环境提供了经验证据，证明我们的方法提高了其他不确定性传播方法和概率世界模型组合的样本效率。",
    "tldr": "结合轨迹抽样和深度高斯协方差网络，以提高基于模型的强化学习问题的数据高效性。",
    "en_tdlr": "Combining trajectory sampling and deep Gaussian covariance network to improve data efficiency of model-based reinforcement learning problems."
}