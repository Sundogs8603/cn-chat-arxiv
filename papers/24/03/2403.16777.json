{
    "title": "Can Machine Translation Bridge Multilingual Pretraining and Cross-lingual Transfer Learning?",
    "abstract": "arXiv:2403.16777v1 Announce Type: new  Abstract: Multilingual pretraining and fine-tuning have remarkably succeeded in various natural language processing tasks. Transferring representations from one language to another is especially crucial for cross-lingual learning. One can expect machine translation objectives to be well suited to fostering such capabilities, as they involve the explicit alignment of semantically equivalent sentences from different languages. This paper investigates the potential benefits of employing machine translation as a continued training objective to enhance language representation learning, bridging multilingual pretraining and cross-lingual applications. We study this question through two lenses: a quantitative evaluation of the performance of existing models and an analysis of their latent representations. Our results show that, contrary to expectations, machine translation as the continued training fails to enhance cross-lingual representation learning i",
    "link": "https://arxiv.org/abs/2403.16777",
    "context": "Title: Can Machine Translation Bridge Multilingual Pretraining and Cross-lingual Transfer Learning?\nAbstract: arXiv:2403.16777v1 Announce Type: new  Abstract: Multilingual pretraining and fine-tuning have remarkably succeeded in various natural language processing tasks. Transferring representations from one language to another is especially crucial for cross-lingual learning. One can expect machine translation objectives to be well suited to fostering such capabilities, as they involve the explicit alignment of semantically equivalent sentences from different languages. This paper investigates the potential benefits of employing machine translation as a continued training objective to enhance language representation learning, bridging multilingual pretraining and cross-lingual applications. We study this question through two lenses: a quantitative evaluation of the performance of existing models and an analysis of their latent representations. Our results show that, contrary to expectations, machine translation as the continued training fails to enhance cross-lingual representation learning i",
    "path": "papers/24/03/2403.16777.json",
    "total_tokens": 856,
    "translated_title": "机器翻译是否能够连接多语言预训练和跨语言迁移学习？",
    "translated_abstract": "多语言预训练和微调在各种自然语言处理任务中取得了显著成功。将表示从一种语言转移到另一种语言对于跨语言学习尤为重要。可以期望机器翻译目标非常适合促进这种能力，因为它们涉及不同语言中语义等价句子的显式对齐。本文研究了采用机器翻译作为持续训练目标以增强语言表示学习、连接多语言预训练和跨语言应用的潜在益处。我们通过两个视角来研究这个问题：对现有模型性能的定量评估以及它们潜在表示的分析。我们的结果表明，与预期相反，作为持续训练的机器翻译未能增强跨语言表示学习。",
    "tldr": "本文研究了机器翻译作为持续训练目标以增强语言表示学习、连接多语言预训练和跨语言应用的潜在益处，结果显示机器翻译未能增强跨语言表示学习。",
    "en_tdlr": "This paper investigates the potential benefits of employing machine translation as a continued training objective to enhance language representation learning, bridging multilingual pretraining and cross-lingual applications, and the results show that machine translation fails to enhance cross-lingual representation learning."
}