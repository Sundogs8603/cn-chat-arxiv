{
    "title": "BloomGML: Graph Machine Learning through the Lens of Bilevel Optimization",
    "abstract": "arXiv:2403.04763v1 Announce Type: new  Abstract: Bilevel optimization refers to scenarios whereby the optimal solution of a lower-level energy function serves as input features to an upper-level objective of interest. These optimal features typically depend on tunable parameters of the lower-level energy in such a way that the entire bilevel pipeline can be trained end-to-end. Although not generally presented as such, this paper demonstrates how a variety of graph learning techniques can be recast as special cases of bilevel optimization or simplifications thereof. In brief, building on prior work we first derive a more flexible class of energy functions that, when paired with various descent steps (e.g., gradient descent, proximal methods, momentum, etc.), form graph neural network (GNN) message-passing layers; critically, we also carefully unpack where any residual approximation error lies with respect to the underlying constituent message-passing functions. We then probe several sim",
    "link": "https://arxiv.org/abs/2403.04763",
    "context": "Title: BloomGML: Graph Machine Learning through the Lens of Bilevel Optimization\nAbstract: arXiv:2403.04763v1 Announce Type: new  Abstract: Bilevel optimization refers to scenarios whereby the optimal solution of a lower-level energy function serves as input features to an upper-level objective of interest. These optimal features typically depend on tunable parameters of the lower-level energy in such a way that the entire bilevel pipeline can be trained end-to-end. Although not generally presented as such, this paper demonstrates how a variety of graph learning techniques can be recast as special cases of bilevel optimization or simplifications thereof. In brief, building on prior work we first derive a more flexible class of energy functions that, when paired with various descent steps (e.g., gradient descent, proximal methods, momentum, etc.), form graph neural network (GNN) message-passing layers; critically, we also carefully unpack where any residual approximation error lies with respect to the underlying constituent message-passing functions. We then probe several sim",
    "path": "papers/24/03/2403.04763.json",
    "total_tokens": 859,
    "translated_title": "BloomGML: 透过双层优化镜头的图机器学习",
    "translated_abstract": "双层优化是指在一个低层次能量函数的最优解用作感兴趣的上层目标的输入特征的情况。这些最优特征通常依赖于低层次能量的可调参数，使得整个双层管道可以端对端训练。尽管通常未被提出，本文演示了如何将各种图学习技术重新构建为双层优化的特例或简化形式。简而言之，基于先前工作，我们首先推导出一类更灵活的能量函数，当与各种下降步骤配对（例如梯度下降、近端方法、动量等），形成图神经网络（GNN）消息传递层；关键是，我们还仔细解释了与底层传递函数有关的任何剩余近似误差所在。然后，我们探究了几个相",
    "tldr": "本文将各种图学习技术重新构建为双层优化的特例或简化形式，并提出了更灵活的能量函数以形成图神经网络消息传递层，同时揭示了残余近似误差的来源。",
    "en_tdlr": "This paper recasts various graph learning techniques as special cases or simplifications of bilevel optimization, introduces a more flexible class of energy functions to form graph neural network message-passing layers, and uncovers the sources of residual approximation error."
}