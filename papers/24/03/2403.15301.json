{
    "title": "Planning with a Learned Policy Basis to Optimally Solve Complex Tasks",
    "abstract": "arXiv:2403.15301v1 Announce Type: cross  Abstract: Conventional reinforcement learning (RL) methods can successfully solve a wide range of sequential decision problems. However, learning policies that can generalize predictably across multiple tasks in a setting with non-Markovian reward specifications is a challenging problem. We propose to use successor features to learn a policy basis so that each (sub)policy in it solves a well-defined subproblem. In a task described by a finite state automaton (FSA) that involves the same set of subproblems, the combination of these (sub)policies can then be used to generate an optimal solution without additional learning. In contrast to other methods that combine (sub)policies via planning, our method asymptotically attains global optimality, even in stochastic environments.",
    "link": "https://arxiv.org/abs/2403.15301",
    "context": "Title: Planning with a Learned Policy Basis to Optimally Solve Complex Tasks\nAbstract: arXiv:2403.15301v1 Announce Type: cross  Abstract: Conventional reinforcement learning (RL) methods can successfully solve a wide range of sequential decision problems. However, learning policies that can generalize predictably across multiple tasks in a setting with non-Markovian reward specifications is a challenging problem. We propose to use successor features to learn a policy basis so that each (sub)policy in it solves a well-defined subproblem. In a task described by a finite state automaton (FSA) that involves the same set of subproblems, the combination of these (sub)policies can then be used to generate an optimal solution without additional learning. In contrast to other methods that combine (sub)policies via planning, our method asymptotically attains global optimality, even in stochastic environments.",
    "path": "papers/24/03/2403.15301.json",
    "total_tokens": 816,
    "translated_title": "使用学习的策略基础进行规划以最优地解决复杂任务",
    "translated_abstract": "传统的强化学习方法可以成功解决各种顺序决策问题。然而，在具有非马尔可夫奖励规范的情景中学习能够可靠泛化于多个任务的策略是一个具有挑战性的问题。我们提出使用继承特征来学习一个策略基础，使得其中的每一个（子）策略解决一个明确定义的子问题。在由有限状态自动机（FSA）描述的任务中涉及相同一组子问题时，这些（子）策略的组合可以被用来生成一个最优解决方案而无需额外的学习。与其他通过规划组合（子）策略的方法相比，我们的方法在渐近上达到全局最优性，即使在随机环境中也是如此。",
    "tldr": "使用继承特征学习策略基础，使每个（子）策略解决一个子问题，在FSA描述的任务中，组合这些（子）策略可用于无需额外学习生成最优解决方案，方法能够渐近达到全局最优性，即使在随机环境中也如此。",
    "en_tdlr": "Learning policy basis with successor features to solve subproblems, achieving global optimality asymptotically in tasks described by FSA even in stochastic environments."
}