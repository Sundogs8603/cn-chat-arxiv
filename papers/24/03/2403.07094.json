{
    "title": "FALCON: FLOP-Aware Combinatorial Optimization for Neural Network Pruning",
    "abstract": "arXiv:2403.07094v1 Announce Type: new  Abstract: The increasing computational demands of modern neural networks present deployment challenges on resource-constrained devices. Network pruning offers a solution to reduce model size and computational cost while maintaining performance. However, most current pruning methods focus primarily on improving sparsity by reducing the number of nonzero parameters, often neglecting other deployment costs such as inference time, which are closely related to the number of floating-point operations (FLOPs). In this paper, we propose FALCON, a novel combinatorial-optimization-based framework for network pruning that jointly takes into account model accuracy (fidelity), FLOPs, and sparsity constraints. A main building block of our approach is an integer linear program (ILP) that simultaneously handles FLOP and sparsity constraints. We present a novel algorithm to approximately solve the ILP. We propose a novel first-order method for our optimization fra",
    "link": "https://arxiv.org/abs/2403.07094",
    "context": "Title: FALCON: FLOP-Aware Combinatorial Optimization for Neural Network Pruning\nAbstract: arXiv:2403.07094v1 Announce Type: new  Abstract: The increasing computational demands of modern neural networks present deployment challenges on resource-constrained devices. Network pruning offers a solution to reduce model size and computational cost while maintaining performance. However, most current pruning methods focus primarily on improving sparsity by reducing the number of nonzero parameters, often neglecting other deployment costs such as inference time, which are closely related to the number of floating-point operations (FLOPs). In this paper, we propose FALCON, a novel combinatorial-optimization-based framework for network pruning that jointly takes into account model accuracy (fidelity), FLOPs, and sparsity constraints. A main building block of our approach is an integer linear program (ILP) that simultaneously handles FLOP and sparsity constraints. We present a novel algorithm to approximately solve the ILP. We propose a novel first-order method for our optimization fra",
    "path": "papers/24/03/2403.07094.json",
    "total_tokens": 874,
    "translated_title": "FALCON：面向神经网络剪枝的FLOP感知组合优化",
    "translated_abstract": "现代神经网络增加的计算需求给资源受限设备上的部署带来挑战。网络剪枝提供了一种减小模型大小和计算成本的解决方案，同时保持性能。然而，大多数当前的剪枝方法主要专注于通过减少非零参数的数量来提高稀疏性，通常忽略了与浮点运算数量（FLOPs）密切相关的其他部署成本，如推断时间。在本文中，我们提出了FALCON，一个新颖的基于组合优化的网络剪枝框架，它同时考虑了模型准确性（忠实度）、FLOPs和稀疏性约束。我们方法的一个主要组成部分是一个整数线性规划（ILP），它同时处理FLOP和稀疏性约束。我们提出了一个新颖的算法来近似解决ILP。我们为我们的优化框架提出了一种新颖的一阶方法。",
    "tldr": "FALCON提出了一种面向神经网络剪枝的新型优化框架，同时考虑了模型准确性、FLOPs和稀疏性约束。提出了一种新颖的一阶方法来优化网络剪枝。",
    "en_tdlr": "FALCON introduces a novel optimization framework for neural network pruning that considers model accuracy, FLOPs, and sparsity constraints, along with proposing a novel first-order method for optimization."
}