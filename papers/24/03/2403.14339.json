{
    "title": "$\\nabla \\tau$: Gradient-based and Task-Agnostic machine Unlearning",
    "abstract": "arXiv:2403.14339v1 Announce Type: cross  Abstract: Machine Unlearning, the process of selectively eliminating the influence of certain data examples used during a model's training, has gained significant attention as a means for practitioners to comply with recent data protection regulations. However, existing unlearning methods face critical drawbacks, including their prohibitively high cost, often associated with a large number of hyperparameters, and the limitation of forgetting only relatively small data portions. This often makes retraining the model from scratch a quicker and more effective solution. In this study, we introduce Gradient-based and Task-Agnostic machine Unlearning ($\\nabla \\tau$), an optimization framework designed to remove the influence of a subset of training data efficiently. It applies adaptive gradient ascent to the data to be forgotten while using standard gradient descent for the remaining data. $\\nabla \\tau$ offers multiple benefits over existing approache",
    "link": "https://arxiv.org/abs/2403.14339",
    "context": "Title: $\\nabla \\tau$: Gradient-based and Task-Agnostic machine Unlearning\nAbstract: arXiv:2403.14339v1 Announce Type: cross  Abstract: Machine Unlearning, the process of selectively eliminating the influence of certain data examples used during a model's training, has gained significant attention as a means for practitioners to comply with recent data protection regulations. However, existing unlearning methods face critical drawbacks, including their prohibitively high cost, often associated with a large number of hyperparameters, and the limitation of forgetting only relatively small data portions. This often makes retraining the model from scratch a quicker and more effective solution. In this study, we introduce Gradient-based and Task-Agnostic machine Unlearning ($\\nabla \\tau$), an optimization framework designed to remove the influence of a subset of training data efficiently. It applies adaptive gradient ascent to the data to be forgotten while using standard gradient descent for the remaining data. $\\nabla \\tau$ offers multiple benefits over existing approache",
    "path": "papers/24/03/2403.14339.json",
    "total_tokens": 833,
    "translated_title": "$\\nabla \\tau$: 基于梯度且任务无关的机器遗忘",
    "translated_abstract": "机器遗忘是一种有选择性地消除模型训练过程中某些数据示例影响的过程，作为从业者遵守最近的数据保护法规的手段，已经引起了显著关注。然而，现有的遗忘方法面临着关键缺点，包括其成本过高，通常与大量超参数相关，以及仅忘记相对较小数据部分的限制。这经常导致从头开始重新训练模型成为更快速和更有效的解决方案。在本研究中，我们介绍了基于梯度且任务无关的机器遗忘（$\\nabla \\tau$），这是一种旨在高效消除部分训练数据影响的优化框架。它对待遗忘的数据应用自适应梯度上升，同时对其余数据使用标准梯度下降。$\\nabla \\tau$相对于现有方法提供了多种优势。",
    "tldr": "$\\nabla \\tau$ 是一种旨在高效消除部分训练数据影响的机器遗忘优化框架。",
    "en_tdlr": "$\\nabla \\tau$ is an optimization framework designed to efficiently remove the influence of a subset of training data in machine unlearning."
}