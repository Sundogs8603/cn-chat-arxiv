{
    "title": "The Anatomy of Adversarial Attacks: Concept-based XAI Dissection",
    "abstract": "arXiv:2403.16782v1 Announce Type: cross  Abstract: Adversarial attacks (AAs) pose a significant threat to the reliability and robustness of deep neural networks. While the impact of these attacks on model predictions has been extensively studied, their effect on the learned representations and concepts within these models remains largely unexplored. In this work, we perform an in-depth analysis of the influence of AAs on the concepts learned by convolutional neural networks (CNNs) using eXplainable artificial intelligence (XAI) techniques. Through an extensive set of experiments across various network architectures and targeted AA techniques, we unveil several key findings. First, AAs induce substantial alterations in the concept composition within the feature space, introducing new concepts or modifying existing ones. Second, the adversarial perturbation itself can be linearly decomposed into a set of latent vector components, with a subset of these being responsible for the attack's ",
    "link": "https://arxiv.org/abs/2403.16782",
    "context": "Title: The Anatomy of Adversarial Attacks: Concept-based XAI Dissection\nAbstract: arXiv:2403.16782v1 Announce Type: cross  Abstract: Adversarial attacks (AAs) pose a significant threat to the reliability and robustness of deep neural networks. While the impact of these attacks on model predictions has been extensively studied, their effect on the learned representations and concepts within these models remains largely unexplored. In this work, we perform an in-depth analysis of the influence of AAs on the concepts learned by convolutional neural networks (CNNs) using eXplainable artificial intelligence (XAI) techniques. Through an extensive set of experiments across various network architectures and targeted AA techniques, we unveil several key findings. First, AAs induce substantial alterations in the concept composition within the feature space, introducing new concepts or modifying existing ones. Second, the adversarial perturbation itself can be linearly decomposed into a set of latent vector components, with a subset of these being responsible for the attack's ",
    "path": "papers/24/03/2403.16782.json",
    "total_tokens": 882,
    "translated_title": "对抗性攻击的解剖学：基于概念的XAI解剖",
    "translated_abstract": "对抗性攻击(AAs)对深度神经网络的可靠性和鲁棒性构成重大威胁。虽然这些攻击对模型预测的影响已得到广泛研究，但它们对这些模型中学习到的表示和概念的影响仍然大多未被探索。在这项工作中，我们使用可解释的人工智能(XAI)技术，对卷积神经网络(CNNs)中对抗性攻击对学习到的概念的影响进行了深入分析。通过对各种网络架构和有针对性的AA技术进行了大量实验，我们揭示了几个关键发现。首先，AAs在特征空间中造成概念组成的实质性变化，引入新概念或修改现有概念。其次，对抗性扰动本身可以被线性分解为一组潜在矢量分量，其中部分分量负责攻击。",
    "tldr": "对抗性攻击对卷积神经网络学到的概念产生了实质性的影响，引入新概念或修改现有概念，并且这种影响可以通过线性分解对扰动进行解释。",
    "en_tdlr": "Adversarial attacks have a substantial impact on the concepts learned by convolutional neural networks, introducing new concepts or modifying existing ones, and this impact can be explained by linear decomposition of the perturbation."
}