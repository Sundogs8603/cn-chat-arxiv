{
    "title": "Can Large Language Models (or Humans) Distill Text?",
    "abstract": "arXiv:2403.16584v1 Announce Type: new  Abstract: We investigate the potential of large language models (LLMs) to distill text: to remove the textual traces of an undesired forbidden variable. We employ a range of LLMs with varying architectures and training approaches to distill text by identifying and removing information about the target variable while preserving other relevant signals. Our findings shed light on the strengths and limitations of LLMs in addressing the distillation and provide insights into the strategies for leveraging these models in computational social science investigations involving text data. In particular, we show that in the strong test of removing sentiment, the statistical association between the processed text and sentiment is still clearly detectable to machine learning classifiers post-LLM-distillation. Furthermore, we find that human annotators also struggle to distill sentiment while preserving other semantic content. This suggests there may be limited",
    "link": "https://arxiv.org/abs/2403.16584",
    "context": "Title: Can Large Language Models (or Humans) Distill Text?\nAbstract: arXiv:2403.16584v1 Announce Type: new  Abstract: We investigate the potential of large language models (LLMs) to distill text: to remove the textual traces of an undesired forbidden variable. We employ a range of LLMs with varying architectures and training approaches to distill text by identifying and removing information about the target variable while preserving other relevant signals. Our findings shed light on the strengths and limitations of LLMs in addressing the distillation and provide insights into the strategies for leveraging these models in computational social science investigations involving text data. In particular, we show that in the strong test of removing sentiment, the statistical association between the processed text and sentiment is still clearly detectable to machine learning classifiers post-LLM-distillation. Furthermore, we find that human annotators also struggle to distill sentiment while preserving other semantic content. This suggests there may be limited",
    "path": "papers/24/03/2403.16584.json",
    "total_tokens": 821,
    "translated_title": "大型语言模型（或人类）是否能进行文本提炼？",
    "translated_abstract": "我们研究了大型语言模型（LLMs）在文本提炼方面的潜力：即去除不需要的禁止变量的文本痕迹。我们利用具有不同架构和训练方法的一系列LLMs来识别和去除关于目标变量的信息，同时保留其他相关信号。我们的研究结果揭示了LLMs在处理提炼中的优势和局限性，并为在涉及文本数据的计算社会科学调查中利用这些模型的策略提供了见解。尤其是，我们发现在强烈测试情感移除时，经过LLM提炼的文本与情感之间的统计关联仍然可以被机器学习分类器清晰地检测到。此外，我们发现人类注释员在保留其他语义内容的同时，也很难提炼出情感。这表明可能存在一定的局限性。",
    "tldr": "大型语言模型（LLMs）在文本提炼中具有独特优势，但在处理情感时仍存在一定局限性，无论是对机器学习分类器还是人类注释员而言。",
    "en_tdlr": "Large language models (LLMs) show unique strengths in text distillation but have limitations in handling sentiment, for both machine learning classifiers and human annotators."
}