{
    "title": "Benchmarking Hallucination in Large Language Models based on Unanswerable Math Word Problem",
    "abstract": "arXiv:2403.03558v1 Announce Type: new  Abstract: Large language models (LLMs) are highly effective in various natural language processing (NLP) tasks. However, they are susceptible to producing unreliable conjectures in ambiguous contexts called hallucination. This paper presents a new method for evaluating LLM hallucination in Question Answering (QA) based on the unanswerable math word problem (MWP). To support this approach, we innovatively develop a dataset called Unanswerable Math Word Problem (UMWP) which comprises 5200 questions across five categories. We developed an evaluation methodology combining text similarity and mathematical expression detection to determine whether LLM considers the question unanswerable. The results of extensive experiments conducted on 31 LLMs, including GPT-3, InstructGPT, LLaMA, and Claude, demonstrate that in-context learning and reinforcement learning with human feedback (RLHF) training significantly enhance the model's ability to avoid hallucinati",
    "link": "https://arxiv.org/abs/2403.03558",
    "context": "Title: Benchmarking Hallucination in Large Language Models based on Unanswerable Math Word Problem\nAbstract: arXiv:2403.03558v1 Announce Type: new  Abstract: Large language models (LLMs) are highly effective in various natural language processing (NLP) tasks. However, they are susceptible to producing unreliable conjectures in ambiguous contexts called hallucination. This paper presents a new method for evaluating LLM hallucination in Question Answering (QA) based on the unanswerable math word problem (MWP). To support this approach, we innovatively develop a dataset called Unanswerable Math Word Problem (UMWP) which comprises 5200 questions across five categories. We developed an evaluation methodology combining text similarity and mathematical expression detection to determine whether LLM considers the question unanswerable. The results of extensive experiments conducted on 31 LLMs, including GPT-3, InstructGPT, LLaMA, and Claude, demonstrate that in-context learning and reinforcement learning with human feedback (RLHF) training significantly enhance the model's ability to avoid hallucinati",
    "path": "papers/24/03/2403.03558.json",
    "total_tokens": 958,
    "translated_title": "基于无法回答的数学单词问题对大型语言模型中幻觉能力的基准测试",
    "translated_abstract": "大型语言模型(大型语言模型)在各种自然语言处理(NLP)任务中非常有效。然而，在模糊语境中容易产生不可靠的推测，称为幻觉。本文提出了一种新方法，用于评估基于无法回答的数学单词问题(QA)的LLM幻觉。为了支持这种方法，我们创新地开发了一个名为无法回答的数学单词问题(UMWP)的数据集，包括五个类别的5200个问题。我们开发了一个评估方法，结合文本相似度和数学表达式检测，以确定LLM是否认为问题无法回答。对31个LLM进行了大量实验，包括GPT-3、InstructGPT、LLaMA和Claude，在-context学习和强化学习与人类反馈(RLHF)训练显著增强了模型避免幻觉的能力。",
    "tldr": "本文提出了一种新方法，通过无法回答的数学单词问题评估大型语言模型中的幻觉能力，通过开发包含五个类别的5200个问题的UMWP数据集，结合文本相似度和数学表达式检测的评估方法，表明在-context学习和强化学习与人类反馈(RLHF)训练显著增强了模型避免幻觉的能力。",
    "en_tdlr": "This paper introduces a new method to evaluate hallucination in large language models based on unanswerable math word problems, with the development of a dataset called UMWP, and demonstrates that in-context learning and reinforcement learning with human feedback training significantly enhance the model's ability to avoid hallucination."
}