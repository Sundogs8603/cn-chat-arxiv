{
    "title": "SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation",
    "abstract": "arXiv:2403.07088v1 Announce Type: new  Abstract: Large language models(LLMs) have shown its outperforming ability on various tasks and question answering. However, LLMs require high computation cost and large memory cost. At the same time, LLMs may cause privacy leakage when training or prediction procedure contains sensitive information. In this paper, we propose SPA(Side Plugin Adaption), a lightweight architecture for fast on-devices inference and privacy retaining on the constraints of strict on-devices computation and memory constraints. Compared with other on-devices seq2seq generation, SPA could make a fast and stable inference on low-resource constraints, allowing it to obtain cost effiency. Our method establish an interaction between a pretrained LLMs on-cloud and additive parameters on-devices, which could provide the knowledge on both pretrained LLMs and private personal feature.Further more, SPA provides a framework to keep feature-base parameters on private guaranteed but ",
    "link": "https://arxiv.org/abs/2403.07088",
    "context": "Title: SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation\nAbstract: arXiv:2403.07088v1 Announce Type: new  Abstract: Large language models(LLMs) have shown its outperforming ability on various tasks and question answering. However, LLMs require high computation cost and large memory cost. At the same time, LLMs may cause privacy leakage when training or prediction procedure contains sensitive information. In this paper, we propose SPA(Side Plugin Adaption), a lightweight architecture for fast on-devices inference and privacy retaining on the constraints of strict on-devices computation and memory constraints. Compared with other on-devices seq2seq generation, SPA could make a fast and stable inference on low-resource constraints, allowing it to obtain cost effiency. Our method establish an interaction between a pretrained LLMs on-cloud and additive parameters on-devices, which could provide the knowledge on both pretrained LLMs and private personal feature.Further more, SPA provides a framework to keep feature-base parameters on private guaranteed but ",
    "path": "papers/24/03/2403.07088.json",
    "total_tokens": 705,
    "translated_title": "SPA：面向云端和设备协作的计算友好型Seq2seq个性化生成",
    "translated_abstract": "大语言模型(LLMs)表现出色的能力已在各种任务和问答中得到展示。然而，LLMs需要高计算成本和大内存成本。同时，当训练或预测过程包含敏感信息时，LLMs可能会导致隐私泄露。在本文中，我们提出了SPA（Side Plugin Adaption），这是一个轻量级架构，用于快速设备上的推断和在严格的设备计算和内存约束条件下保持隐私。",
    "tldr": "提出了SPA（Side Plugin Adaption）的轻量级架构，用于在严格的设备计算和内存约束条件下快速进行推断，同时保留隐私。",
    "en_tdlr": "Introducing a lightweight architecture called SPA (Side Plugin Adaption) for fast on-device inference and privacy retention under strict device computation and memory constraints."
}