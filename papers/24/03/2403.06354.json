{
    "title": "Amharic LLaMA and LLaVA: Multimodal LLMs for Low Resource Languages",
    "abstract": "arXiv:2403.06354v1 Announce Type: new  Abstract: Large Language Models (LLMs) like GPT-4 and LLaMA have shown incredible proficiency at natural language processing tasks and have even begun to excel at tasks across other modalities such as vision and audio. Despite their success, LLMs often struggle to perform well on low-resource languages because there is so little training data available. This shortcoming is especially prevalent with open source models. In this work, we explore training LLaMA-2 to speak Amharic, a language which is spoken by over 50 million people world wide, but has orders of magnitude less data available than languages like English. We employ methods previously used for training LLMs on other languages with data scarcity, and use open source translation models to perform data augmentation and grow our dataset from millions of tokens to billions. We further enhance the capabilities of our model by connecting an image encoder and training on a translated visual inst",
    "link": "https://arxiv.org/abs/2403.06354",
    "context": "Title: Amharic LLaMA and LLaVA: Multimodal LLMs for Low Resource Languages\nAbstract: arXiv:2403.06354v1 Announce Type: new  Abstract: Large Language Models (LLMs) like GPT-4 and LLaMA have shown incredible proficiency at natural language processing tasks and have even begun to excel at tasks across other modalities such as vision and audio. Despite their success, LLMs often struggle to perform well on low-resource languages because there is so little training data available. This shortcoming is especially prevalent with open source models. In this work, we explore training LLaMA-2 to speak Amharic, a language which is spoken by over 50 million people world wide, but has orders of magnitude less data available than languages like English. We employ methods previously used for training LLMs on other languages with data scarcity, and use open source translation models to perform data augmentation and grow our dataset from millions of tokens to billions. We further enhance the capabilities of our model by connecting an image encoder and training on a translated visual inst",
    "path": "papers/24/03/2403.06354.json",
    "total_tokens": 877,
    "translated_title": "阿姆哈拉语LLaMA和LLaVA：低资源语言的多模态LLMs",
    "translated_abstract": "像GPT-4和LLaMA这样的大型语言模型已经展现出在自然语言处理任务上的惊人能力，甚至开始擅长跨越视觉和音频等其他模态的任务。然而，尽管取得成功，LLMs在低资源语言上往往表现不佳，因为可用的训练数据非常少。这一不足在开源模型中尤为突出。在这项工作中，我们探讨了对LLaMA-2进行训练以讲阿姆哈拉语，这是一种全球有超过5千万人口使用的语言，但可用数据远少于英语等语言。我们采用之前用于在数据稀缺情况下训练LLMs的方法，并使用开源翻译模型进行数据增强，将我们的数据集从数百万个记号增长到数十亿个。我们通过连接图像编码器并训练翻译视觉理解任务的图像来进一步增强模型的能力。",
    "tldr": "本论文研究了如何训练LLaMA-2模型来学习阿姆哈拉语，使用了数据增强和连接图像编码器的方法以解决低资源语言的问题。",
    "en_tdlr": "This paper explores training the LLaMA-2 model to learn Amharic, using data augmentation and connecting an image encoder to address the challenges of low-resource languages."
}