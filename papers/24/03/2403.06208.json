{
    "title": "Personalized LoRA for Human-Centered Text Understanding",
    "abstract": "arXiv:2403.06208v1 Announce Type: new  Abstract: Effectively and efficiently adapting a pre-trained language model (PLM) for human-centered text understanding (HCTU) is challenging since user tokens are million-level in most personalized applications and do not have concrete explicit semantics. A standard and parameter-efficient approach (e.g., LoRA) necessitates memorizing numerous suits of adapters for each user. In this work, we introduce a personalized LoRA (PLoRA) with a plug-and-play (PnP) framework for the HCTU task. PLoRA is effective, parameter-efficient, and dynamically deploying in PLMs. Moreover, a personalized dropout and a mutual information maximizing strategies are adopted and hence the proposed PLoRA can be well adapted to few/zero-shot learning scenarios for the cold-start issue. Experiments conducted on four benchmark datasets show that the proposed method outperforms existing methods in full/few/zero-shot learning scenarios for the HCTU task, even though it has fewe",
    "link": "https://arxiv.org/abs/2403.06208",
    "context": "Title: Personalized LoRA for Human-Centered Text Understanding\nAbstract: arXiv:2403.06208v1 Announce Type: new  Abstract: Effectively and efficiently adapting a pre-trained language model (PLM) for human-centered text understanding (HCTU) is challenging since user tokens are million-level in most personalized applications and do not have concrete explicit semantics. A standard and parameter-efficient approach (e.g., LoRA) necessitates memorizing numerous suits of adapters for each user. In this work, we introduce a personalized LoRA (PLoRA) with a plug-and-play (PnP) framework for the HCTU task. PLoRA is effective, parameter-efficient, and dynamically deploying in PLMs. Moreover, a personalized dropout and a mutual information maximizing strategies are adopted and hence the proposed PLoRA can be well adapted to few/zero-shot learning scenarios for the cold-start issue. Experiments conducted on four benchmark datasets show that the proposed method outperforms existing methods in full/few/zero-shot learning scenarios for the HCTU task, even though it has fewe",
    "path": "papers/24/03/2403.06208.json",
    "total_tokens": 852,
    "translated_title": "面向人类中心文本理解的个性化LoRA",
    "translated_abstract": "在大多数个性化应用中，用户标记通常达到百万级，并且没有明确的具体语义，因此对于有效且高效地调整预训练语言模型（PLM）以用于人类中心文本理解（HCTU）颇具挑战性。标准且参数高效的方法（如LoRA）需要为每个用户记忆大量适配器套件。在本文中，我们引入了一种具有插拔（PnP）框架的个性化LoRA（PLoRA）用于HCTU任务。PLoRA在PLMs中是有效的、参数高效的，可以动态部署。此外，我们采用了个性化的dropout和最大化互信息策略，因此提出的PLoRA可以很好地适应少量/零次学习场景以解决冷启动问题。在四个基准数据集上进行的实验表明，所提出的方法在HCTU任务的全/少/零次学习场景中优于现有方法，即使它的适配器套件较少。",
    "tldr": "提出了一种个性化LoRA（PLoRA）用于人类中心文本理解任务，在适应寒启动问题上具有优越性能。"
}