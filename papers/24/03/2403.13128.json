{
    "title": "AdaFish: Fast low-rank parameter-efficient fine-tuning by using second-order information",
    "abstract": "arXiv:2403.13128v1 Announce Type: new  Abstract: Recent advancements in large-scale pretrained models have significantly improved performance across a variety of tasks in natural language processing and computer vision. However, the extensive number of parameters in these models necessitates substantial memory and computational resources for full training. To adapt these models for downstream tasks or specific application-oriented datasets, parameter-efficient fine-tuning methods leveraging pretrained parameters have gained considerable attention. However, it can still be time-consuming due to lots of parameters and epochs. In this work, we introduce AdaFish, an efficient algorithm of the second-order type designed to expedite the training process within low-rank decomposition-based fine-tuning frameworks. Our key observation is that the associated generalized Fisher information matrix is either low-rank or extremely small-scaled. Such a generalized Fisher information matrix is shown t",
    "link": "https://arxiv.org/abs/2403.13128",
    "context": "Title: AdaFish: Fast low-rank parameter-efficient fine-tuning by using second-order information\nAbstract: arXiv:2403.13128v1 Announce Type: new  Abstract: Recent advancements in large-scale pretrained models have significantly improved performance across a variety of tasks in natural language processing and computer vision. However, the extensive number of parameters in these models necessitates substantial memory and computational resources for full training. To adapt these models for downstream tasks or specific application-oriented datasets, parameter-efficient fine-tuning methods leveraging pretrained parameters have gained considerable attention. However, it can still be time-consuming due to lots of parameters and epochs. In this work, we introduce AdaFish, an efficient algorithm of the second-order type designed to expedite the training process within low-rank decomposition-based fine-tuning frameworks. Our key observation is that the associated generalized Fisher information matrix is either low-rank or extremely small-scaled. Such a generalized Fisher information matrix is shown t",
    "path": "papers/24/03/2403.13128.json",
    "total_tokens": 758,
    "translated_title": "AdaFish: 利用二阶信息进行快速的低秩参数高效微调",
    "translated_abstract": "arXiv:2403.13128v1 公告类型: 新的 摘要: 最近大规模预训练模型的进展显著提高了自然语言处理和计算机视觉等各种任务的性能。然而，这些模型中庞大的参数数量需要大量的内存和计算资源进行完整训练。为了调整这些模型以适用于下游任务或特定应用导向的数据集，利用预训练参数的参数高效微调方法引起了相当大的关注。然而，由于参数和时代太多，这仍然可能耗时。在这项工作中，我们介绍AdaFish，一种效率高的第二阶类型算法，旨在在基于低秩分解的微调框架内加速训练过程",
    "tldr": "AdaFish 是一种高效的第二阶算法，旨在通过低阶分解来加速训练过程，提高低秩参数高效微调的效率",
    "en_tdlr": "AdaFish is an efficient second-order algorithm designed to expedite the training process through low-rank decomposition, improving the efficiency of parameter-efficient fine-tuning."
}