{
    "title": "The Role of $n$-gram Smoothing in the Age of Neural Networks",
    "abstract": "arXiv:2403.17240v1 Announce Type: new  Abstract: For nearly three decades, language models derived from the $n$-gram assumption held the state of the art on the task. The key to their success lay in the application of various smoothing techniques that served to combat overfitting. However, when neural language models toppled $n$-gram models as the best performers, $n$-gram smoothing techniques became less relevant. Indeed, it would hardly be an understatement to suggest that the line of inquiry into $n$-gram smoothing techniques became dormant. This paper re-opens the role classical $n$-gram smoothing techniques may play in the age of neural language models. First, we draw a formal equivalence between label smoothing, a popular regularization technique for neural language models, and add-$\\lambda$ smoothing. Second, we derive a generalized framework for converting \\emph{any} $n$-gram smoothing technique into a regularizer compatible with neural language models. Our empirical results fi",
    "link": "https://arxiv.org/abs/2403.17240",
    "context": "Title: The Role of $n$-gram Smoothing in the Age of Neural Networks\nAbstract: arXiv:2403.17240v1 Announce Type: new  Abstract: For nearly three decades, language models derived from the $n$-gram assumption held the state of the art on the task. The key to their success lay in the application of various smoothing techniques that served to combat overfitting. However, when neural language models toppled $n$-gram models as the best performers, $n$-gram smoothing techniques became less relevant. Indeed, it would hardly be an understatement to suggest that the line of inquiry into $n$-gram smoothing techniques became dormant. This paper re-opens the role classical $n$-gram smoothing techniques may play in the age of neural language models. First, we draw a formal equivalence between label smoothing, a popular regularization technique for neural language models, and add-$\\lambda$ smoothing. Second, we derive a generalized framework for converting \\emph{any} $n$-gram smoothing technique into a regularizer compatible with neural language models. Our empirical results fi",
    "path": "papers/24/03/2403.17240.json",
    "total_tokens": 921,
    "translated_title": "在神经网络时代的$n$-gram平滑作用",
    "translated_abstract": "在将近三十年的时间里，基于$n$-gram假设的语言模型一直是该任务的技术水平。它们成功的关键在于应用各种平滑技术来对抗过拟合。然而，当神经语言模型取代$n$-gram模型成为最佳表现者时，$n$-gram平滑技术变得不太相关。事实上，可以毫不夸张地说，对$n$-gram平滑技术的研究在这一时代变得停滞。本文重新探讨了在神经语言模型时代古典$n$-gram平滑技术可能发挥的作用。首先，我们在标签平滑和add-$\\lambda$平滑之间建立了一个正式等价性，标签平滑是一种神经语言模型的流行正则化技术。其次，我们推导了一个通用框架，将\\emph{任何} $n$-gram平滑技术转换为与神经语言模型兼容的正则化器。我们的实证结果表明",
    "tldr": "本文重新探讨了在神经语言模型时代古典$n$-gram平滑技术可能发挥的作用，并提出了将任何$n$-gram平滑技术转换为神经语言模型兼容正则化器的通用框架",
    "en_tdlr": "This paper re-examines the potential role of classical $n$-gram smoothing techniques in the age of neural language models and introduces a general framework for converting any $n$-gram smoothing technique into a regularizer compatible with neural language models."
}