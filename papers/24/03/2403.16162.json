{
    "title": "Multi-Task Learning with Multi-Task Optimization",
    "abstract": "arXiv:2403.16162v1 Announce Type: new  Abstract: Multi-task learning solves multiple correlated tasks. However, conflicts may exist between them. In such circumstances, a single solution can rarely optimize all the tasks, leading to performance trade-offs. To arrive at a set of optimized yet well-distributed models that collectively embody different trade-offs in one algorithmic pass, this paper proposes to view Pareto multi-task learning through the lens of multi-task optimization. Multi-task learning is first cast as a multi-objective optimization problem, which is then decomposed into a diverse set of unconstrained scalar-valued subproblems. These subproblems are solved jointly using a novel multi-task gradient descent method, whose uniqueness lies in the iterative transfer of model parameters among the subproblems during the course of optimization. A theorem proving faster convergence through the inclusion of such transfers is presented. We investigate the proposed multi-task learn",
    "link": "https://arxiv.org/abs/2403.16162",
    "context": "Title: Multi-Task Learning with Multi-Task Optimization\nAbstract: arXiv:2403.16162v1 Announce Type: new  Abstract: Multi-task learning solves multiple correlated tasks. However, conflicts may exist between them. In such circumstances, a single solution can rarely optimize all the tasks, leading to performance trade-offs. To arrive at a set of optimized yet well-distributed models that collectively embody different trade-offs in one algorithmic pass, this paper proposes to view Pareto multi-task learning through the lens of multi-task optimization. Multi-task learning is first cast as a multi-objective optimization problem, which is then decomposed into a diverse set of unconstrained scalar-valued subproblems. These subproblems are solved jointly using a novel multi-task gradient descent method, whose uniqueness lies in the iterative transfer of model parameters among the subproblems during the course of optimization. A theorem proving faster convergence through the inclusion of such transfers is presented. We investigate the proposed multi-task learn",
    "path": "papers/24/03/2403.16162.json",
    "total_tokens": 872,
    "translated_title": "基于多任务优化的多任务学习",
    "translated_abstract": "多任务学习解决了多个相关任务。然而，它们之间可能存在冲突。在这种情况下，单个解决方案很少能够优化所有任务，导致性能折衷。为了在一个算法通过中获得一组优化且分布良好的模型，这些模型集体体现了不同权衡，本文提出通过多任务优化的视角看待帕累托多任务学习。首先将多任务学习视为多目标优化问题，然后将其分解为一组不受约束的标量价值子问题。使用一种新颖的多任务梯度下降方法共同解决这些子问题，其独特之处在于在优化过程中在子问题之间迭代传输模型参数。提出了一个定理，证明通过包含这样的传输可以实现更快的收敛速度。我们调查了提出的多任务学习方法。",
    "tldr": "本文提出了一种通过多任务优化视角看待帕累托多任务学习的方法，将多任务学习转化为多目标优化问题，并通过独特的多任务梯度下降方法联合解决多个子问题，从而实现一组优化且分布良好的模型。",
    "en_tdlr": "This paper proposes a method to view Pareto multi-task learning through the perspective of multi-task optimization, by transforming multi-task learning into a multi-objective optimization problem and jointly solving a diverse set of unconstrained scalar-valued subproblems using a unique multi-task gradient descent method, to achieve a set of optimized yet well-distributed models."
}