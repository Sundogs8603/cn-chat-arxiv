{
    "title": "InterLUDE: Interactions between Labeled and Unlabeled Data to Enhance Semi-Supervised Learning",
    "abstract": "arXiv:2403.10658v1 Announce Type: cross  Abstract: Semi-supervised learning (SSL) seeks to enhance task performance by training on both labeled and unlabeled data. Mainstream SSL image classification methods mostly optimize a loss that additively combines a supervised classification objective with a regularization term derived solely from unlabeled data. This formulation neglects the potential for interaction between labeled and unlabeled images. In this paper, we introduce InterLUDE, a new approach to enhance SSL made of two parts that each benefit from labeled-unlabeled interaction. The first part, embedding fusion, interpolates between labeled and unlabeled embeddings to improve representation learning. The second part is a new loss, grounded in the principle of consistency regularization, that aims to minimize discrepancies in the model's predictions between labeled versus unlabeled inputs. Experiments on standard closed-set SSL benchmarks and a medical SSL task with an uncurated u",
    "link": "https://arxiv.org/abs/2403.10658",
    "context": "Title: InterLUDE: Interactions between Labeled and Unlabeled Data to Enhance Semi-Supervised Learning\nAbstract: arXiv:2403.10658v1 Announce Type: cross  Abstract: Semi-supervised learning (SSL) seeks to enhance task performance by training on both labeled and unlabeled data. Mainstream SSL image classification methods mostly optimize a loss that additively combines a supervised classification objective with a regularization term derived solely from unlabeled data. This formulation neglects the potential for interaction between labeled and unlabeled images. In this paper, we introduce InterLUDE, a new approach to enhance SSL made of two parts that each benefit from labeled-unlabeled interaction. The first part, embedding fusion, interpolates between labeled and unlabeled embeddings to improve representation learning. The second part is a new loss, grounded in the principle of consistency regularization, that aims to minimize discrepancies in the model's predictions between labeled versus unlabeled inputs. Experiments on standard closed-set SSL benchmarks and a medical SSL task with an uncurated u",
    "path": "papers/24/03/2403.10658.json",
    "total_tokens": 906,
    "translated_title": "InterLUDE：标记数据与未标记数据间的相互作用以增强半监督学习",
    "translated_abstract": "半监督学习旨在通过在标记数据和未标记数据上进行训练来提高任务性能。当前主流的图像分类半监督学习方法大多优化损失函数，其中将监督分类目标与仅从未标记数据导出的正则化项相加。 这种表达方式忽略了标记和未标记图像之间相互作用的潜力。 本文介绍了InterLUDE，这是一种新的增强SSL方法，由两部分组成，每个部分都受益于标记-未标记交互。 第一部分是嵌入融合，它在标记和未标记的嵌入之间插值以改进表示学习。 第二部分是一种新的损失函数，基于一致性正则化原则，旨在最小化模型在标记与未标记输入之间的预测差异。 在标准闭集SSL基准测试和医学SSL任务上进行的实验表明，InterLUDE能取得显著改进。",
    "tldr": "InterLUDE提出了一种新的半监督学习方法，通过两部分相互作用来增强SSL，包括嵌入融合和基于一致性正则化的新损失函数，实验证明该方法在图像分类和医学任务上取得显著改进。",
    "en_tdlr": "InterLUDE introduces a novel approach for semi-supervised learning, enhancing SSL through the interaction of two components, embedding fusion and a new loss based on consistency regularization, with experiments demonstrating significant improvements in image classification and medical tasks."
}