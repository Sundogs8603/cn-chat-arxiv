{
    "title": "On Transfer in Classification: How Well do Subsets of Classes Generalize?",
    "abstract": "arXiv:2403.03569v1 Announce Type: new  Abstract: In classification, it is usual to observe that models trained on a given set of classes can generalize to previously unseen ones, suggesting the ability to learn beyond the initial task. This ability is often leveraged in the context of transfer learning where a pretrained model can be used to process new classes, with or without fine tuning. Surprisingly, there are a few papers looking at the theoretical roots beyond this phenomenon. In this work, we are interested in laying the foundations of such a theoretical framework for transferability between sets of classes. Namely, we establish a partially ordered set of subsets of classes. This tool allows to represent which subset of classes can generalize to others. In a more practical setting, we explore the ability of our framework to predict which subset of classes can lead to the best performance when testing on all of them. We also explore few-shot learning, where transfer is the golden",
    "link": "https://arxiv.org/abs/2403.03569",
    "context": "Title: On Transfer in Classification: How Well do Subsets of Classes Generalize?\nAbstract: arXiv:2403.03569v1 Announce Type: new  Abstract: In classification, it is usual to observe that models trained on a given set of classes can generalize to previously unseen ones, suggesting the ability to learn beyond the initial task. This ability is often leveraged in the context of transfer learning where a pretrained model can be used to process new classes, with or without fine tuning. Surprisingly, there are a few papers looking at the theoretical roots beyond this phenomenon. In this work, we are interested in laying the foundations of such a theoretical framework for transferability between sets of classes. Namely, we establish a partially ordered set of subsets of classes. This tool allows to represent which subset of classes can generalize to others. In a more practical setting, we explore the ability of our framework to predict which subset of classes can lead to the best performance when testing on all of them. We also explore few-shot learning, where transfer is the golden",
    "path": "papers/24/03/2403.03569.json",
    "total_tokens": 867,
    "translated_title": "在分类任务中的迁移：子类别的泛化能力如何？",
    "translated_abstract": "在分类任务中，通常会发现在给定一组类别上训练的模型可以推广到以前未见过的类别，这表明了学习超越初始任务的能力。这种能力通常在迁移学习的背景下得到利用，其中预训练模型可以用于处理新类别，无论是否进行微调。令人惊讶的是，很少有论文探讨这一现象背后的理论根基。在这项工作中，我们试图为类别集之间的可迁移性奠定这样一个理论框架的基础。具体来说，我们建立了一个类别子集的部分有序集。这个工具可以表示哪个类别子集可以推广到其他类别。在更实际的情境中，我们探讨了我们的框架预测哪个类别子集可以在对所有类别进行测试时实现最佳性能的能力。我们还探讨了少样本学习，其中转移是至关重要的。",
    "tldr": "本文的创新之处在于建立了一个部分有序集，用以代表哪些类别子集可以推广到其他类别，并探索了类别子集如何提高测试性能以及少样本学习中迁移的重要性。"
}