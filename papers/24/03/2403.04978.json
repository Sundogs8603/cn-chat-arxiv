{
    "title": "Stacking as Accelerated Gradient Descent",
    "abstract": "arXiv:2403.04978v1 Announce Type: new  Abstract: Stacking, a heuristic technique for training deep residual networks by progressively increasing the number of layers and initializing new layers by copying parameters from older layers, has proven quite successful in improving the efficiency of training deep neural networks. In this paper, we propose a theoretical explanation for the efficacy of stacking: viz., stacking implements a form of Nesterov's accelerated gradient descent. The theory also covers simpler models such as the additive ensembles constructed in boosting methods, and provides an explanation for a similar widely-used practical heuristic for initializing the new classifier in each round of boosting. We also prove that for certain deep linear residual networks, stacking does provide accelerated training, via a new potential function analysis of the Nesterov's accelerated gradient method which allows errors in updates. We conduct proof-of-concept experiments to validate our",
    "link": "https://arxiv.org/abs/2403.04978",
    "context": "Title: Stacking as Accelerated Gradient Descent\nAbstract: arXiv:2403.04978v1 Announce Type: new  Abstract: Stacking, a heuristic technique for training deep residual networks by progressively increasing the number of layers and initializing new layers by copying parameters from older layers, has proven quite successful in improving the efficiency of training deep neural networks. In this paper, we propose a theoretical explanation for the efficacy of stacking: viz., stacking implements a form of Nesterov's accelerated gradient descent. The theory also covers simpler models such as the additive ensembles constructed in boosting methods, and provides an explanation for a similar widely-used practical heuristic for initializing the new classifier in each round of boosting. We also prove that for certain deep linear residual networks, stacking does provide accelerated training, via a new potential function analysis of the Nesterov's accelerated gradient method which allows errors in updates. We conduct proof-of-concept experiments to validate our",
    "path": "papers/24/03/2403.04978.json",
    "total_tokens": 806,
    "translated_title": "Stacking作为加速梯度下降算法",
    "translated_abstract": "Stacking是一种启发式技术，通过逐渐增加层数并通过从旧层复制参数来初始化新层，用于训练深度残差网络，已经被证明在提高深度神经网络训练效率方面非常成功。本文提出了对于Stacking有效性的理论解释：即，Stacking实现了Nesterov的加速梯度下降的一种形式。该理论还涵盖了诸如提升方法中构建的加法集成等更简单的模型，并为每一轮提升过程中初始化新分类器的类似广泛使用的实用启发式提供了解释。我们还证明了对于某些深度线性残差网络，通过对Nesterov的加速梯度方法的一个新的潜能函数分析，Stacking确实提供了加速训练，从而允许更新中的误差。我们进行了概念验证实验来验证我们的理论。",
    "tldr": "Stacking提出了一种理论解释，即实现了Nesterov的加速梯度下降形式，并证明对于某些深度线性残差网络，提供了加速训练。",
    "en_tdlr": "Stacking provides a theoretical explanation by implementing a form of Nesterov's accelerated gradient descent, and proves to accelerate training for certain deep linear residual networks."
}