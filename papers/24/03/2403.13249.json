{
    "title": "A Unified and General Framework for Continual Learning",
    "abstract": "arXiv:2403.13249v1 Announce Type: new  Abstract: Continual Learning (CL) focuses on learning from dynamic and changing data distributions while retaining previously acquired knowledge. Various methods have been developed to address the challenge of catastrophic forgetting, including regularization-based, Bayesian-based, and memory-replay-based techniques. However, these methods lack a unified framework and common terminology for describing their approaches. This research aims to bridge this gap by introducing a comprehensive and overarching framework that encompasses and reconciles these existing methodologies. Notably, this new framework is capable of encompassing established CL approaches as special instances within a unified and general optimization objective. An intriguing finding is that despite their diverse origins, these methods share common mathematical structures. This observation highlights the compatibility of these seemingly distinct techniques, revealing their interconnec",
    "link": "https://arxiv.org/abs/2403.13249",
    "context": "Title: A Unified and General Framework for Continual Learning\nAbstract: arXiv:2403.13249v1 Announce Type: new  Abstract: Continual Learning (CL) focuses on learning from dynamic and changing data distributions while retaining previously acquired knowledge. Various methods have been developed to address the challenge of catastrophic forgetting, including regularization-based, Bayesian-based, and memory-replay-based techniques. However, these methods lack a unified framework and common terminology for describing their approaches. This research aims to bridge this gap by introducing a comprehensive and overarching framework that encompasses and reconciles these existing methodologies. Notably, this new framework is capable of encompassing established CL approaches as special instances within a unified and general optimization objective. An intriguing finding is that despite their diverse origins, these methods share common mathematical structures. This observation highlights the compatibility of these seemingly distinct techniques, revealing their interconnec",
    "path": "papers/24/03/2403.13249.json",
    "total_tokens": 824,
    "translated_title": "一个统一和通用的持续学习框架",
    "translated_abstract": "持续学习（CL）侧重于从动态和变化的数据分布中学习，同时保留先前获得的知识。已经开发了各种方法来解决灾难性遗忘的挑战，包括基于正则化、基于贝叶斯和基于记忆重放的技术。然而，这些方法缺乏统一框架和共同术语来描述它们的方法。这项研究旨在通过引入一个全面且全面的框架来弥合这一差距，该框架涵盖并调和这些现有方法。值得注意的是，这个新框架能够将已建立的CL方法作为统一和通用的优化目标中的特例。一个有趣的发现是，尽管它们起源各异，但这些方法共享共同的数学结构。这一观察突显了这些看似不同技术的兼容性，揭示了它们之间的相互联系。",
    "tldr": "该研究提出了一个统一和通用的持续学习框架，能够将各种现有的方法融合在一起，并发现这些方法尽管来源多样，但存在共同的数学结构。"
}