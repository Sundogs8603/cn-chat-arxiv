{
    "title": "Fairness-Aware Interpretable Modeling (FAIM) for Trustworthy Machine Learning in Healthcare",
    "abstract": "arXiv:2403.05235v1 Announce Type: cross  Abstract: The escalating integration of machine learning in high-stakes fields such as healthcare raises substantial concerns about model fairness. We propose an interpretable framework - Fairness-Aware Interpretable Modeling (FAIM), to improve model fairness without compromising performance, featuring an interactive interface to identify a \"fairer\" model from a set of high-performing models and promoting the integration of data-driven evidence and clinical expertise to enhance contextualized fairness. We demonstrated FAIM's value in reducing sex and race biases by predicting hospital admission with two real-world databases, MIMIC-IV-ED and SGH-ED. We show that for both datasets, FAIM models not only exhibited satisfactory discriminatory performance but also significantly mitigated biases as measured by well-established fairness metrics, outperforming commonly used bias-mitigation methods. Our approach demonstrates the feasibility of improving f",
    "link": "https://arxiv.org/abs/2403.05235",
    "context": "Title: Fairness-Aware Interpretable Modeling (FAIM) for Trustworthy Machine Learning in Healthcare\nAbstract: arXiv:2403.05235v1 Announce Type: cross  Abstract: The escalating integration of machine learning in high-stakes fields such as healthcare raises substantial concerns about model fairness. We propose an interpretable framework - Fairness-Aware Interpretable Modeling (FAIM), to improve model fairness without compromising performance, featuring an interactive interface to identify a \"fairer\" model from a set of high-performing models and promoting the integration of data-driven evidence and clinical expertise to enhance contextualized fairness. We demonstrated FAIM's value in reducing sex and race biases by predicting hospital admission with two real-world databases, MIMIC-IV-ED and SGH-ED. We show that for both datasets, FAIM models not only exhibited satisfactory discriminatory performance but also significantly mitigated biases as measured by well-established fairness metrics, outperforming commonly used bias-mitigation methods. Our approach demonstrates the feasibility of improving f",
    "path": "papers/24/03/2403.05235.json",
    "total_tokens": 899,
    "translated_title": "用于医疗保健领域可信机器学习的公平感知可解释建模（FAIM）",
    "translated_abstract": "在高风险领域如医疗保健中机器学习不断融入的情况下，对模型公平性提出了重要关切。我们提出了一个可解释框架 - 公平感知可解释建模（FAIM），旨在提高模型的公平性而不影响性能，其特点是一个交互界面，可以从一组高性能模型中识别出一个“更公平”的模型，并促进数据驱动证据与临床专家知识的整合，以增强情境公平性。我们通过在两个真实世界数据库MIMIC-IV-ED和SGH-ED上预测医院入院情况，展示了FAIM在减少性别和种族偏见方面的价值。我们展示了对于这两个数据集，FAIM模型不仅展现了令人满意的歧视性能，而且通过已建立的公平性度量明显减轻了偏见，优于常用的偏见缓解方法。",
    "tldr": "FAIM是一个用于提高医疗保健领域机器学习公平性的可解释框架，通过交互界面识别最公平模型，并结合数据驱动证据与临床专家知识，成功减少了性别和种族偏见。",
    "en_tdlr": "FAIM is an interpretable framework for improving fairness in machine learning in healthcare, identifying the fairest model through an interactive interface and integrating data-driven evidence and clinical expertise to successfully mitigate gender and race biases."
}