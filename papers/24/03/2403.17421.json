{
    "title": "MA4DIV: Multi-Agent Reinforcement Learning for Search Result Diversification",
    "abstract": "arXiv:2403.17421v1 Announce Type: cross  Abstract: The objective of search result diversification (SRD) is to ensure that selected documents cover as many different subtopics as possible. Existing methods primarily utilize a paradigm of \"greedy selection\", i.e., selecting one document with the highest diversity score at a time. These approaches tend to be inefficient and are easily trapped in a suboptimal state. In addition, some other methods aim to approximately optimize the diversity metric, such as $\\alpha$-NDCG, but the results still remain suboptimal. To address these challenges, we introduce Multi-Agent reinforcement learning (MARL) for search result DIVersity, which called MA4DIV. In this approach, each document is an agent and the search result diversification is modeled as a cooperative task among multiple agents. This approach allows for directly optimizing the diversity metrics, such as $\\alpha$-NDCG, while achieving high training efficiency. We conducted preliminary experi",
    "link": "https://arxiv.org/abs/2403.17421",
    "context": "Title: MA4DIV: Multi-Agent Reinforcement Learning for Search Result Diversification\nAbstract: arXiv:2403.17421v1 Announce Type: cross  Abstract: The objective of search result diversification (SRD) is to ensure that selected documents cover as many different subtopics as possible. Existing methods primarily utilize a paradigm of \"greedy selection\", i.e., selecting one document with the highest diversity score at a time. These approaches tend to be inefficient and are easily trapped in a suboptimal state. In addition, some other methods aim to approximately optimize the diversity metric, such as $\\alpha$-NDCG, but the results still remain suboptimal. To address these challenges, we introduce Multi-Agent reinforcement learning (MARL) for search result DIVersity, which called MA4DIV. In this approach, each document is an agent and the search result diversification is modeled as a cooperative task among multiple agents. This approach allows for directly optimizing the diversity metrics, such as $\\alpha$-NDCG, while achieving high training efficiency. We conducted preliminary experi",
    "path": "papers/24/03/2403.17421.json",
    "total_tokens": 867,
    "translated_title": "MA4DIV：用于搜索结果多样化的多智能体强化学习",
    "translated_abstract": "搜索结果多样化（SRD）的目标是确保所选文档涵盖尽可能多的不同子主题。现有方法主要利用“贪婪选择”范式，即一次选择一个具有最高多样性分数的文档。这些方法往往效率低下，容易陷入次优状态。此外，一些其他方法旨在近似优化多样性指标，如$\\alpha$-NDCG，但结果仍然不尽如人意。为了解决这些挑战，我们引入了用于搜索结果多样性的多智能体强化学习（MARL）方法，称为MA4DIV。在这种方法中，每个文档都是一个智能体，搜索结果多样化被建模为多个智能体之间的合作任务。该方法允许直接优化多样性指标，如$\\alpha$-NDCG，同时实现高训练效率。我们进行了初步实验。",
    "tldr": "引入了基于多智能体强化学习的MA4DIV方法，将搜索结果多样化建模为多个智能体之间的合作任务，直接优化多样性指标，如$\\alpha$-NDCG，以实现高训练效率。",
    "en_tdlr": "Introduced the MA4DIV method based on multi-agent reinforcement learning, which models search result diversification as a cooperative task among multiple agents, directly optimizing diversity metrics such as $\\alpha$-NDCG for high training efficiency."
}