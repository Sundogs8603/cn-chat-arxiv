{
    "title": "Evaluating the Elementary Multilingual Capabilities of Large Language Models with MultiQ",
    "abstract": "arXiv:2403.03814v1 Announce Type: cross  Abstract: Large language models (LLMs) need to serve everyone, including a global majority of non-English speakers. However, most LLMs today, and open LLMs in particular, are often intended for use in just English (e.g. Llama2, Mistral) or a small handful of high-resource languages (e.g. Mixtral, Qwen). Recent research shows that, despite limits in their intended use, people prompt LLMs in many different languages. Therefore, in this paper, we investigate the basic multilingual capabilities of state-of-the-art open LLMs beyond their intended use. For this purpose, we introduce MultiQ, a new silver standard benchmark for basic open-ended question answering with 27.4k test questions across a typologically diverse set of 137 languages. With MultiQ, we evaluate language fidelity, i.e.\\ whether models respond in the prompted language, and question answering accuracy. All LLMs we test respond faithfully and/or accurately for at least some languages be",
    "link": "https://arxiv.org/abs/2403.03814",
    "context": "Title: Evaluating the Elementary Multilingual Capabilities of Large Language Models with MultiQ\nAbstract: arXiv:2403.03814v1 Announce Type: cross  Abstract: Large language models (LLMs) need to serve everyone, including a global majority of non-English speakers. However, most LLMs today, and open LLMs in particular, are often intended for use in just English (e.g. Llama2, Mistral) or a small handful of high-resource languages (e.g. Mixtral, Qwen). Recent research shows that, despite limits in their intended use, people prompt LLMs in many different languages. Therefore, in this paper, we investigate the basic multilingual capabilities of state-of-the-art open LLMs beyond their intended use. For this purpose, we introduce MultiQ, a new silver standard benchmark for basic open-ended question answering with 27.4k test questions across a typologically diverse set of 137 languages. With MultiQ, we evaluate language fidelity, i.e.\\ whether models respond in the prompted language, and question answering accuracy. All LLMs we test respond faithfully and/or accurately for at least some languages be",
    "path": "papers/24/03/2403.03814.json",
    "total_tokens": 892,
    "translated_title": "用MultiQ评估大型语言模型的基本多语能力",
    "translated_abstract": "大型语言模型（LLMs）需要为全球大多数非英语使用者提供服务。然而，大多数LLMs今天，特别是开放的LLMs，通常仅意为在英语（例如Llama2、Mistral）或少数几种高资源语言（例如Mixtral、Qwen）中使用。最近的研究表明，尽管存在使用上的限制，人们会用许多不同的语言提示LLMs。因此，在本文中，我们调查了最先进的开放LLMs在其预期使用范围之外的基本多语能力。为此，我们引入了MultiQ，一个新的用于基本开放式问答的银标准基准，涵盖137种语言的27.4k个测试问题。通过MultiQ，我们评估了语言忠实度，即模型是否以提示的语言回复，以及问题回答准确性。我们测试的所有LLMs对至少某些语言响应得忠实和/或准确。",
    "tldr": "本研究通过引入MultiQ基准，调查了最先进的开放LLMs在其预期使用范围之外的基本多语能力，发现这些模型对于至少某些语言能够忠实和准确地进行回答。",
    "en_tdlr": "This study investigates the elementary multilingual capabilities of state-of-the-art open LLMs beyond their intended use by introducing MultiQ benchmark, revealing that these models can faithfully and accurately respond to at least some languages."
}