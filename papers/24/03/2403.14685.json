{
    "title": "Cyclical Log Annealing as a Learning Rate Scheduler",
    "abstract": "arXiv:2403.14685v1 Announce Type: new  Abstract: A learning rate scheduler is a predefined set of instructions for varying search stepsizes during model training processes. This paper introduces a new logarithmic method using harsh restarting of step sizes through stochastic gradient descent. Cyclical log annealing implements the restart pattern more aggressively to maybe allow the usage of more greedy algorithms on the online convex optimization framework. The algorithm was tested on the CIFAR-10 image datasets, and seemed to perform analogously with cosine annealing on large transformer-enhanced residual neural networks. Future experiments would involve testing the scheduler in generative adversarial networks and finding the best parameters for the scheduler with more experiments.",
    "link": "https://arxiv.org/abs/2403.14685",
    "context": "Title: Cyclical Log Annealing as a Learning Rate Scheduler\nAbstract: arXiv:2403.14685v1 Announce Type: new  Abstract: A learning rate scheduler is a predefined set of instructions for varying search stepsizes during model training processes. This paper introduces a new logarithmic method using harsh restarting of step sizes through stochastic gradient descent. Cyclical log annealing implements the restart pattern more aggressively to maybe allow the usage of more greedy algorithms on the online convex optimization framework. The algorithm was tested on the CIFAR-10 image datasets, and seemed to perform analogously with cosine annealing on large transformer-enhanced residual neural networks. Future experiments would involve testing the scheduler in generative adversarial networks and finding the best parameters for the scheduler with more experiments.",
    "path": "papers/24/03/2403.14685.json",
    "total_tokens": 755,
    "translated_title": "周期性对数温度调度作为学习率调度器",
    "translated_abstract": "学习率调度器是一组预定义的指令，用于在模型训练过程中改变搜索步长。本文介绍了一种新的对数方法，通过随机梯度下降对步长进行严格的重启。周期性对数温度调度更积极地实现了重启模式，或许可以允许在在线凸优化框架上使用更贪婪的算法。该算法在CIFAR-10图像数据集上进行了测试，似乎在大型变压器增强残差神经网络上的余弦退火方案表现类似。未来的实验将涉及在生成对抗网络中测试调度器，并通过更多实验找到调度器的最佳参数。",
    "tldr": "该论文介绍了一种新的对数方法作为学习率调度器，通过更积极的重启模式，可能使得在在线凸优化框架上使用更贪婪的算法，实验结果表明它性能类似于余弦退火方案。",
    "en_tdlr": "This paper introduces a new logarithmic method as a learning rate scheduler, which implements a more aggressive restart pattern that may allow the usage of more greedy algorithms on the online convex optimization framework, with experimental results showing its performance similar to cosine annealing."
}