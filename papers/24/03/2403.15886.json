{
    "title": "Leveraging Zero-Shot Prompting for Efficient Language Model Distillation",
    "abstract": "arXiv:2403.15886v1 Announce Type: cross  Abstract: This paper introduces a novel approach for efficiently distilling LLMs into smaller, application-specific models, significantly reducing operational costs and manual labor. Addressing the challenge of deploying computationally intensive LLMs in specific applications or edge devices, this technique utilizes LLMs' reasoning capabilities to generate labels and natural language rationales for unlabeled data. Our approach enhances both finetuning and distillation by employing a multi-task training framework where student models mimic these rationales alongside teacher predictions. Key contributions include the employment of zero-shot prompting to elicit teacher model rationales, reducing the necessity for handcrafted few-shot examples and lowering the overall token count required, which directly translates to cost savings given the pay-per-token billing model of major tech companies' LLM APIs. Additionally, the paper investigates the impact",
    "link": "https://arxiv.org/abs/2403.15886",
    "context": "Title: Leveraging Zero-Shot Prompting for Efficient Language Model Distillation\nAbstract: arXiv:2403.15886v1 Announce Type: cross  Abstract: This paper introduces a novel approach for efficiently distilling LLMs into smaller, application-specific models, significantly reducing operational costs and manual labor. Addressing the challenge of deploying computationally intensive LLMs in specific applications or edge devices, this technique utilizes LLMs' reasoning capabilities to generate labels and natural language rationales for unlabeled data. Our approach enhances both finetuning and distillation by employing a multi-task training framework where student models mimic these rationales alongside teacher predictions. Key contributions include the employment of zero-shot prompting to elicit teacher model rationales, reducing the necessity for handcrafted few-shot examples and lowering the overall token count required, which directly translates to cost savings given the pay-per-token billing model of major tech companies' LLM APIs. Additionally, the paper investigates the impact",
    "path": "papers/24/03/2403.15886.json",
    "total_tokens": 818,
    "translated_title": "利用零-shot提示实现高效语言模型蒸馏",
    "translated_abstract": "本文介绍了一种新颖的方法，用于将LLMs高效地蒸馏为更小、特定于应用的模型，显著降低运营成本和人工劳动。该技术利用LLMs的推理能力为未标记数据生成标签和自然语言理由，以解决将计算密集型LLMs部署到特定应用或边缘设备的挑战。我们的方法通过采用多任务训练框架，其中学生模型模仿这些理由以及教师模型的预测，来增强微调和蒸馏。关键贡献包括利用零-shot提示来引出教师模型的理由，减少手工制作的少-shot示例的必要性，并降低所需的总记号数，这直接转化为成本节约，考虑到主要技术公司LLM APIs的按记号计费模型。此外，本文还调查了影响",
    "tldr": "通过利用零-shot提示来引出教师模型的理由，减少手工制作的少-shot示例的必要性，并降低所需的总记号数，这直接转化为成本节约。",
    "en_tdlr": "Reducing the necessity for handcrafted few-shot examples and lowering the overall token count required by leveraging zero-shot prompting directly translates to cost savings."
}