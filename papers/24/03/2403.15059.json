{
    "title": "MM-Diff: High-Fidelity Image Personalization via Multi-Modal Condition Integration",
    "abstract": "arXiv:2403.15059v1 Announce Type: cross  Abstract: Recent advances in tuning-free personalized image generation based on diffusion models are impressive. However, to improve subject fidelity, existing methods either retrain the diffusion model or infuse it with dense visual embeddings, both of which suffer from poor generalization and efficiency. Also, these methods falter in multi-subject image generation due to the unconstrained cross-attention mechanism. In this paper, we propose MM-Diff, a unified and tuning-free image personalization framework capable of generating high-fidelity images of both single and multiple subjects in seconds. Specifically, to simultaneously enhance text consistency and subject fidelity, MM-Diff employs a vision encoder to transform the input image into CLS and patch embeddings. CLS embeddings are used on the one hand to augment the text embeddings, and on the other hand together with patch embeddings to derive a small number of detail-rich subject embeddin",
    "link": "https://arxiv.org/abs/2403.15059",
    "context": "Title: MM-Diff: High-Fidelity Image Personalization via Multi-Modal Condition Integration\nAbstract: arXiv:2403.15059v1 Announce Type: cross  Abstract: Recent advances in tuning-free personalized image generation based on diffusion models are impressive. However, to improve subject fidelity, existing methods either retrain the diffusion model or infuse it with dense visual embeddings, both of which suffer from poor generalization and efficiency. Also, these methods falter in multi-subject image generation due to the unconstrained cross-attention mechanism. In this paper, we propose MM-Diff, a unified and tuning-free image personalization framework capable of generating high-fidelity images of both single and multiple subjects in seconds. Specifically, to simultaneously enhance text consistency and subject fidelity, MM-Diff employs a vision encoder to transform the input image into CLS and patch embeddings. CLS embeddings are used on the one hand to augment the text embeddings, and on the other hand together with patch embeddings to derive a small number of detail-rich subject embeddin",
    "path": "papers/24/03/2403.15059.json",
    "total_tokens": 877,
    "translated_title": "MM-Diff：通过多模态条件融合实现高保真图像个性化",
    "translated_abstract": "最近，基于扩散模型的免调优个性化图像生成的进展令人印象深刻。然而，为了提高主题保真度，现有方法要么重新训练扩散模型，要么将其融入密集的视觉嵌入，这两者都存在泛化和效率低的问题。此外，由于无约束的跨注意机制，这些方法在多主体图像生成中出现问题。在本文中，我们提出了MM-Diff，一个统一且免调优的图像个性化框架，能够在几秒内生成单个和多个主题的高保真图像。具体地，为了同时增强文本一致性和主题保真度，MM-Diff利用视觉编码器将输入图像转换为CLS和补丁嵌入。CLS嵌入一方面用于增强文本嵌入，另一方面与补丁嵌入一起得出少量富含细节的主题嵌入。",
    "tldr": "提出了一种名为MM-Diff的统一且免调优的图像个性化框架，能够在几秒内生成高保真的单个和多个主体图像，并利用视觉编码器同时增强文本一致性和主题保真度。",
    "en_tdlr": "Proposed a unified and tuning-free image personalization framework called MM-Diff, capable of generating high-fidelity images of both single and multiple subjects in seconds, enhancing text consistency and subject fidelity simultaneously using a vision encoder."
}