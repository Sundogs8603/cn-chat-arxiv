{
    "title": "Topic Aware Probing: From Sentence Length Prediction to Idiom Identification how reliant are Neural Language Models on Topic?",
    "abstract": "arXiv:2403.02009v1 Announce Type: new  Abstract: Transformer-based Neural Language Models achieve state-of-the-art performance on various natural language processing tasks. However, an open question is the extent to which these models rely on word-order/syntactic or word co-occurrence/topic-based information when processing natural language. This work contributes to this debate by addressing the question of whether these models primarily use topic as a signal, by exploring the relationship between Transformer-based models' (BERT and RoBERTa's) performance on a range of probing tasks in English, from simple lexical tasks such as sentence length prediction to complex semantic tasks such as idiom token identification, and the sensitivity of these tasks to the topic information. To this end, we propose a novel probing method which we call topic-aware probing. Our initial results indicate that Transformer-based models encode both topic and non-topic information in their intermediate layers,",
    "link": "https://arxiv.org/abs/2403.02009",
    "context": "Title: Topic Aware Probing: From Sentence Length Prediction to Idiom Identification how reliant are Neural Language Models on Topic?\nAbstract: arXiv:2403.02009v1 Announce Type: new  Abstract: Transformer-based Neural Language Models achieve state-of-the-art performance on various natural language processing tasks. However, an open question is the extent to which these models rely on word-order/syntactic or word co-occurrence/topic-based information when processing natural language. This work contributes to this debate by addressing the question of whether these models primarily use topic as a signal, by exploring the relationship between Transformer-based models' (BERT and RoBERTa's) performance on a range of probing tasks in English, from simple lexical tasks such as sentence length prediction to complex semantic tasks such as idiom token identification, and the sensitivity of these tasks to the topic information. To this end, we propose a novel probing method which we call topic-aware probing. Our initial results indicate that Transformer-based models encode both topic and non-topic information in their intermediate layers,",
    "path": "papers/24/03/2403.02009.json",
    "total_tokens": 852,
    "translated_title": "主题感知探究：从句子长度预测到习语识别，神经语言模型在多大程度上依赖于主题？",
    "translated_abstract": "基于Transformer的神经语言模型在各种自然语言处理任务上实现了最先进的性能。然而，一个开放的问题是这些模型在处理自然语言时在多大程度上依赖于词序/句法或词共现/主题信息。本研究通过探讨Transformer-based模型（如BERT和RoBERTa）在一系列英语探究任务（从简单的词汇任务如句子长度预测到复杂的语义任务如习语标记识别）上的表现与这些任务对主题信息的敏感性，为这一争论做出了贡献。为此，我们提出了一种我们称之为主题感知探究的新颖探究方法。我们的初步结果表明，基于Transformer的模型在其中间层中编码了主题信息和非主题信息。",
    "tldr": "本论文通过主题感知探究方法探讨了Transformer-based模型在处理自然语言时对主题信号的主要依赖程度，并初步结果表明这些模型在中间层中编码了主题信息和非主题信息。",
    "en_tdlr": "This paper investigates the extent to which Transformer-based models rely on topic signals in processing natural language through a topic-aware probing method, with initial results indicating that these models encode both topic and non-topic information in their intermediate layers."
}