{
    "title": "Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs",
    "abstract": "arXiv:2403.07398v1 Announce Type: cross  Abstract: Event commonsense reasoning requires the ability to reason about the relationship between events, as well as infer implicit context underlying that relationship. However, data scarcity makes it challenging for language models to learn to generate commonsense inferences for contexts and questions involving interactions between complex events. To address this demand, we present COM2 (COMplex COMmonsense), a new dataset created by sampling multi-hop logical queries (e.g., the joint effect or cause of both event A and B, or the effect of the effect of event C) from an existing commonsense knowledge graph (CSKG), and verbalizing them using handcrafted rules and large language models into multiple-choice and text generation questions. Our experiments show that language models trained on COM2 exhibit significant improvements in complex reasoning ability, resulting in enhanced zero-shot performance in both in-domain and out-of-domain tasks for",
    "link": "https://arxiv.org/abs/2403.07398",
    "context": "Title: Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs\nAbstract: arXiv:2403.07398v1 Announce Type: cross  Abstract: Event commonsense reasoning requires the ability to reason about the relationship between events, as well as infer implicit context underlying that relationship. However, data scarcity makes it challenging for language models to learn to generate commonsense inferences for contexts and questions involving interactions between complex events. To address this demand, we present COM2 (COMplex COMmonsense), a new dataset created by sampling multi-hop logical queries (e.g., the joint effect or cause of both event A and B, or the effect of the effect of event C) from an existing commonsense knowledge graph (CSKG), and verbalizing them using handcrafted rules and large language models into multiple-choice and text generation questions. Our experiments show that language models trained on COM2 exhibit significant improvements in complex reasoning ability, resulting in enhanced zero-shot performance in both in-domain and out-of-domain tasks for",
    "path": "papers/24/03/2403.07398.json",
    "total_tokens": 839,
    "translated_title": "在常识知识图上进行逻辑查询的复杂推理",
    "translated_abstract": "事件常识推理需要具有推理事件之间关系的能力，以及推断在这种关系之下的隐含上下文。然而，数据稀缺使得语言模型难以学会为涉及复杂事件相互作用的背景和问题生成常识推断变得具有挑战性。为了满足这种需求，我们提出了COM2（COMplex COMmonsense），这是一个通过从现有常识知识图（CSKG）中抽样多跳逻辑查询（例如，事件A和B的联合效果或因果关系，或事件C的效果的效果），并利用手工制作的规则和大型语言模型将其用多选和文本生成问题的形式表达出来的新数据集。我们的实验表明，在COM2上训练的语言模型在复杂推理能力方面取得了显著的改进，从而增强了零-shot性能，无论是在领域内还是领域外的任务中。",
    "tldr": "提出了COM2数据集，通过在常识知识图中抽样多跳逻辑查询并结合大型语言模型，显著提高了语言模型在复杂推理能力方面的性能。",
    "en_tdlr": "Introduced the COM2 dataset, which significantly enhances the performance of language models in complex reasoning by sampling multi-hop logical queries in commonsense knowledge graphs and leveraging large language models."
}