{
    "title": "Information-Theoretic Distillation for Reference-less Summarization",
    "abstract": "arXiv:2403.13780v1 Announce Type: new  Abstract: The current winning recipe for automatic summarization is using proprietary large-scale language models (LLMs) such as ChatGPT as is, or imitation learning from them as teacher models. While increasingly ubiquitous dependence on such large-scale language models is convenient, there remains an important question of whether small-scale models could have achieved competitive results, if we were to seek an alternative learning method -- that allows for a more cost-efficient, controllable, yet powerful summarizer. We present InfoSumm, a novel framework to distill a powerful summarizer based on the information-theoretic objective for summarization, without relying on either the LLM's capability or human-written references. To achieve this, we first propose a novel formulation of the desiderata of summarization (saliency, faithfulness and brevity) through the lens of mutual information between the original document and the summary. Based on thi",
    "link": "https://arxiv.org/abs/2403.13780",
    "context": "Title: Information-Theoretic Distillation for Reference-less Summarization\nAbstract: arXiv:2403.13780v1 Announce Type: new  Abstract: The current winning recipe for automatic summarization is using proprietary large-scale language models (LLMs) such as ChatGPT as is, or imitation learning from them as teacher models. While increasingly ubiquitous dependence on such large-scale language models is convenient, there remains an important question of whether small-scale models could have achieved competitive results, if we were to seek an alternative learning method -- that allows for a more cost-efficient, controllable, yet powerful summarizer. We present InfoSumm, a novel framework to distill a powerful summarizer based on the information-theoretic objective for summarization, without relying on either the LLM's capability or human-written references. To achieve this, we first propose a novel formulation of the desiderata of summarization (saliency, faithfulness and brevity) through the lens of mutual information between the original document and the summary. Based on thi",
    "path": "papers/24/03/2403.13780.json",
    "total_tokens": 645,
    "translated_title": "无参考摘要的信息论精炼",
    "translated_abstract": "当前自动摘要的主要方法是使用专有的大规模语言模型（LLMs）如ChatGPT，或者从它们作为教师模型进行模仿学习。本文提出了一种名为InfoSumm的新型框架，通过信息论目标进行精炼强大的摘要生成器，而不依赖于LLM的能力或人工编写的参考文献。",
    "tldr": "提出了一种名为InfoSumm的框架，通过信息论目标实现了无参考摘要的精炼生成器",
    "en_tdlr": "Introduced InfoSumm, a framework for distilling a powerful summarizer for reference-less summarization based on information-theoretic objective."
}