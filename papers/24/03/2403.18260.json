{
    "title": "Toward Interactive Regional Understanding in Vision-Large Language Models",
    "abstract": "arXiv:2403.18260v1 Announce Type: cross  Abstract: Recent Vision-Language Pre-training (VLP) models have demonstrated significant advancements. Nevertheless, these models heavily rely on image-text pairs that capture only coarse and global information of an image, leading to a limitation in their regional understanding ability. In this work, we introduce \\textbf{RegionVLM}, equipped with explicit regional modeling capabilities, allowing them to understand user-indicated image regions. To achieve this, we design a simple yet innovative architecture, requiring no modifications to the model architecture or objective function. Additionally, we leverage a dataset that contains a novel source of information, namely Localized Narratives, which has been overlooked in previous VLP research. Our experiments demonstrate that our single generalist model not only achieves an interactive dialogue system but also exhibits superior performance on various zero-shot region understanding tasks, without c",
    "link": "https://arxiv.org/abs/2403.18260",
    "context": "Title: Toward Interactive Regional Understanding in Vision-Large Language Models\nAbstract: arXiv:2403.18260v1 Announce Type: cross  Abstract: Recent Vision-Language Pre-training (VLP) models have demonstrated significant advancements. Nevertheless, these models heavily rely on image-text pairs that capture only coarse and global information of an image, leading to a limitation in their regional understanding ability. In this work, we introduce \\textbf{RegionVLM}, equipped with explicit regional modeling capabilities, allowing them to understand user-indicated image regions. To achieve this, we design a simple yet innovative architecture, requiring no modifications to the model architecture or objective function. Additionally, we leverage a dataset that contains a novel source of information, namely Localized Narratives, which has been overlooked in previous VLP research. Our experiments demonstrate that our single generalist model not only achieves an interactive dialogue system but also exhibits superior performance on various zero-shot region understanding tasks, without c",
    "path": "papers/24/03/2403.18260.json",
    "total_tokens": 842,
    "translated_title": "在视觉-大规模语言模型中实现交互式区域理解",
    "translated_abstract": "最近的视觉-语言预训练（VLP）模型取得了显著进展。然而，这些模型过分依赖于捕捉图像粗糙和全局信息的图像-文本对，导致它们在区域理解能力上存在局限性。在本研究中，我们引入了\\textbf{RegionVLM}，具备显式的区域建模能力，使其能够理解用户指定的图像区域。为实现这一目标，我们设计了一个简单却创新的架构，无需修改模型架构或目标函数。此外，我们利用了一个包含新颖信息源的数据集，即局部叙事，这在先前的VLP研究中被忽视。我们的实验证明，我们的单一通用模型不仅实现了交互式对话系统，还在各种零样本区域理解任务上表现出优越性能。",
    "tldr": "该研究提出了被称为RegionVLM的模型，具备显式的区域建模能力，通过引入新的信息源Local Narratives，设计简洁而创新的架构，实现了交互式区域理解，取得了优越性能。",
    "en_tdlr": "The study introduces a model called RegionVLM with explicit regional modeling capabilities, achieving interactive regional understanding and superior performance by incorporating new information source Local Narratives and a simple yet innovative architecture."
}