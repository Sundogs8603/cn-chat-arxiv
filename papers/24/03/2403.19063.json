{
    "title": "Instruction-based Hypergraph Pretraining",
    "abstract": "arXiv:2403.19063v1 Announce Type: new  Abstract: Pretraining has been widely explored to augment the adaptability of graph learning models to transfer knowledge from large datasets to a downstream task, such as link prediction or classification. However, the gap between training objectives and the discrepancy between data distributions in pretraining and downstream tasks hinders the transfer of the pretrained knowledge. Inspired by instruction-based prompts widely used in pretrained language models, we introduce instructions into graph pretraining. In this paper, we propose a novel pretraining framework named Instruction-based Hypergraph Pretraining. To overcome the discrepancy between pretraining and downstream tasks, text-based instructions are applied to provide explicit guidance on specific tasks for representation learning. Compared to learnable prompts, whose effectiveness depends on the quality and the diversity of training data, text-based instructions intrinsically encapsulate",
    "link": "https://arxiv.org/abs/2403.19063",
    "context": "Title: Instruction-based Hypergraph Pretraining\nAbstract: arXiv:2403.19063v1 Announce Type: new  Abstract: Pretraining has been widely explored to augment the adaptability of graph learning models to transfer knowledge from large datasets to a downstream task, such as link prediction or classification. However, the gap between training objectives and the discrepancy between data distributions in pretraining and downstream tasks hinders the transfer of the pretrained knowledge. Inspired by instruction-based prompts widely used in pretrained language models, we introduce instructions into graph pretraining. In this paper, we propose a novel pretraining framework named Instruction-based Hypergraph Pretraining. To overcome the discrepancy between pretraining and downstream tasks, text-based instructions are applied to provide explicit guidance on specific tasks for representation learning. Compared to learnable prompts, whose effectiveness depends on the quality and the diversity of training data, text-based instructions intrinsically encapsulate",
    "path": "papers/24/03/2403.19063.json",
    "total_tokens": 797,
    "translated_title": "基于指令的超图预训练",
    "translated_abstract": "预训练被广泛探索，以增强图学习模型对从大型数据集传输知识至下游任务（如链接预测或分类）的适应能力。然而，在预训练目标之间的差距以及预训练和下游任务中数据分布的差异阻碍了预训练知识的转移。受预训练语言模型中广泛使用的基于指令的提示启发，我们将指令引入图预训练。在本文中，我们提出了一种名为基于指令的超图预训练的新型预训练框架。为了克服预训练和下游任务之间的差异，应用基于文本的指令来为表示学习中的特定任务提供明确指导。与可学习提示相比，其有效性取决于训练数据的质量和多样性，文本指令固有地蕴含了",
    "tldr": "引入基于指令的提示到图预训练中，通过文本指令提供具体任务的明确指导，以克服预训练和下游任务之间的差异。",
    "en_tdlr": "Introducing instruction-based prompts into graph pretraining to provide explicit guidance on specific tasks through text instructions, overcoming the gap between pretraining and downstream tasks."
}