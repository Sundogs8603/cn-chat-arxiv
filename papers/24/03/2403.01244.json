{
    "title": "Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal",
    "abstract": "arXiv:2403.01244v1 Announce Type: cross  Abstract: Large language models (LLMs) suffer from catastrophic forgetting during continual learning. Conventional rehearsal-based methods rely on previous training data to retain the model's ability, which may not be feasible in real-world applications. When conducting continual learning based on a publicly-released LLM checkpoint, the availability of the original training data may be non-existent. To address this challenge, we propose a framework called Self-Synthesized Rehearsal (SSR) that uses the LLM to generate synthetic instances for rehearsal. Concretely, we first employ the base LLM for in-context learning to generate synthetic instances. Subsequently, we utilize the latest LLM to refine the instance outputs based on the synthetic inputs, preserving its acquired ability. Finally, we select diverse high-quality synthetic instances for rehearsal in future stages. Experimental results demonstrate that SSR achieves superior or comparable pe",
    "link": "https://arxiv.org/abs/2403.01244",
    "context": "Title: Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal\nAbstract: arXiv:2403.01244v1 Announce Type: cross  Abstract: Large language models (LLMs) suffer from catastrophic forgetting during continual learning. Conventional rehearsal-based methods rely on previous training data to retain the model's ability, which may not be feasible in real-world applications. When conducting continual learning based on a publicly-released LLM checkpoint, the availability of the original training data may be non-existent. To address this challenge, we propose a framework called Self-Synthesized Rehearsal (SSR) that uses the LLM to generate synthetic instances for rehearsal. Concretely, we first employ the base LLM for in-context learning to generate synthetic instances. Subsequently, we utilize the latest LLM to refine the instance outputs based on the synthetic inputs, preserving its acquired ability. Finally, we select diverse high-quality synthetic instances for rehearsal in future stages. Experimental results demonstrate that SSR achieves superior or comparable pe",
    "path": "papers/24/03/2403.01244.json",
    "total_tokens": 716,
    "translated_title": "使用自我生成的复述来减轻大型语言模型中的灾难性遗忘",
    "translated_abstract": "大型语言模型（LLMs）在持续学习过程中遭受灾难性遗忘。传统的基于复述的方法依赖于先前的训练数据来保留模型的能力，然而这在现实应用中可能无法实现。为了解决这一挑战，我们提出了一个名为自我生成复述（SSR）的框架，利用LLM生成合成实例进行复述。",
    "tldr": "提出了一种称为Self-Synthesized Rehearsal（SSR）的框架，利用大型语言模型生成合成实例用于持续学习中的复述，以解决大型语言模型遭受灾难性遗忘的问题。",
    "en_tdlr": "Proposed a framework called Self-Synthesized Rehearsal (SSR) that uses large language models to generate synthetic instances for rehearsal in continual learning to address catastrophic forgetting issues."
}