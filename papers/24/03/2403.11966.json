{
    "title": "Informed Spectral Normalized Gaussian Processes for Trajectory Prediction",
    "abstract": "arXiv:2403.11966v1 Announce Type: cross  Abstract: Prior parameter distributions provide an elegant way to represent prior expert and world knowledge for informed learning. Previous work has shown that using such informative priors to regularize probabilistic deep learning (DL) models increases their performance and data-efficiency. However, commonly used sampling-based approximations for probabilistic DL models can be computationally expensive, requiring multiple inference passes and longer training times. Promising alternatives are compute-efficient last layer kernel approximations like spectral normalized Gaussian processes (SNGPs). We propose a novel regularization-based continual learning method for SNGPs, which enables the use of informative priors that represent prior knowledge learned from previous tasks. Our proposal builds upon well-established methods and requires no rehearsal memory or parameter expansion. We apply our informed SNGP model to the trajectory prediction proble",
    "link": "https://arxiv.org/abs/2403.11966",
    "context": "Title: Informed Spectral Normalized Gaussian Processes for Trajectory Prediction\nAbstract: arXiv:2403.11966v1 Announce Type: cross  Abstract: Prior parameter distributions provide an elegant way to represent prior expert and world knowledge for informed learning. Previous work has shown that using such informative priors to regularize probabilistic deep learning (DL) models increases their performance and data-efficiency. However, commonly used sampling-based approximations for probabilistic DL models can be computationally expensive, requiring multiple inference passes and longer training times. Promising alternatives are compute-efficient last layer kernel approximations like spectral normalized Gaussian processes (SNGPs). We propose a novel regularization-based continual learning method for SNGPs, which enables the use of informative priors that represent prior knowledge learned from previous tasks. Our proposal builds upon well-established methods and requires no rehearsal memory or parameter expansion. We apply our informed SNGP model to the trajectory prediction proble",
    "path": "papers/24/03/2403.11966.json",
    "total_tokens": 807,
    "translated_title": "通知谱归一化高斯过程用于轨迹预测",
    "translated_abstract": "先前的参数分布为通知式学习提供了一种优雅的方式，以表示先验专家和世界知识。以前的工作表明，使用这样的信息先验来规范概率深度学习（DL）模型会增加它们的性能和数据效率。然而，用于概率DL模型的常用基于采样的近似方法可能计算昂贵，需要多次推理和更长的训练时间。计算高效的最后一层核逼近如谱归一化高斯过程（SNGPs）是有希望的替代方案。我们提出了一种针对SNGPs的基于新颖正则化的持续学习方法，它可以使用代表先前任务学习的先验知识的通知先验。我们的提议建立在成熟方法的基础上，不需要记忆或参数扩展。我们将我们的通知SNGP模型应用于轨迹预测问题。",
    "tldr": "该论文提出了一种用于轨迹预测的新颖正则化持续学习方法，利用通知式先验知识，提高了模型性能和数据效率。",
    "en_tdlr": "This paper introduces a novel regularization-based continual learning method for trajectory prediction, leveraging informative priors to enhance model performance and data efficiency."
}