{
    "title": "Breaking the Length Barrier: LLM-Enhanced CTR Prediction in Long Textual User Behaviors",
    "abstract": "arXiv:2403.19347v1 Announce Type: cross  Abstract: With the rise of large language models (LLMs), recent works have leveraged LLMs to improve the performance of click-through rate (CTR) prediction. However, we argue that a critical obstacle remains in deploying LLMs for practical use: the efficiency of LLMs when processing long textual user behaviors. As user sequences grow longer, the current efficiency of LLMs is inadequate for training on billions of users and items. To break through the efficiency barrier of LLMs, we propose Behavior Aggregated Hierarchical Encoding (BAHE) to enhance the efficiency of LLM-based CTR modeling. Specifically, BAHE proposes a novel hierarchical architecture that decouples the encoding of user behaviors from inter-behavior interactions. Firstly, to prevent computational redundancy from repeated encoding of identical user behaviors, BAHE employs the LLM's pre-trained shallow layers to extract embeddings of the most granular, atomic user behaviors from ext",
    "link": "https://arxiv.org/abs/2403.19347",
    "context": "Title: Breaking the Length Barrier: LLM-Enhanced CTR Prediction in Long Textual User Behaviors\nAbstract: arXiv:2403.19347v1 Announce Type: cross  Abstract: With the rise of large language models (LLMs), recent works have leveraged LLMs to improve the performance of click-through rate (CTR) prediction. However, we argue that a critical obstacle remains in deploying LLMs for practical use: the efficiency of LLMs when processing long textual user behaviors. As user sequences grow longer, the current efficiency of LLMs is inadequate for training on billions of users and items. To break through the efficiency barrier of LLMs, we propose Behavior Aggregated Hierarchical Encoding (BAHE) to enhance the efficiency of LLM-based CTR modeling. Specifically, BAHE proposes a novel hierarchical architecture that decouples the encoding of user behaviors from inter-behavior interactions. Firstly, to prevent computational redundancy from repeated encoding of identical user behaviors, BAHE employs the LLM's pre-trained shallow layers to extract embeddings of the most granular, atomic user behaviors from ext",
    "path": "papers/24/03/2403.19347.json",
    "total_tokens": 862,
    "translated_title": "打破长度限制：LLM增强长文本用户行为中的CTR预测",
    "translated_abstract": "随着大型语言模型（LLMs）的兴起，最近的研究利用LLMs提高了点击率（CTR）预测的性能。然而，我们认为在实际应用中部署LLMs仍然存在一个关键障碍：LLMs在处理长文本用户行为时的效率。随着用户序列变得更长，当前的LLMs效率不足以在数十亿用户和项目上进行训练。为了突破LLMs的效率障碍，我们提出了行为聚合分层编码（BAHE）来增强基于LLM的CTR建模的效率。具体地，BAHE提出了一种新颖的分层架构，将用户行为的编码与行为之间的交互解耦。首先，为了防止由于重复编码相同用户行为而产生的计算冗余，BAHE利用LLM的预训练浅层来提取最粒度的原子用户行为的嵌入。",
    "tldr": "BAHE提出了行为聚合分层编码（BAHE）来增强LLM-based CTR建模的效率，通过解耦用户行为的编码与行为之间的交互。",
    "en_tdlr": "BAHE proposes Behavior Aggregated Hierarchical Encoding (BAHE) to enhance the efficiency of LLM-based CTR modeling by decoupling the encoding of user behaviors from inter-behavior interactions."
}