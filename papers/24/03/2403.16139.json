{
    "title": "A Little Leak Will Sink a Great Ship: Survey of Transparency for Large Language Models from Start to Finish",
    "abstract": "arXiv:2403.16139v1 Announce Type: new  Abstract: Large Language Models (LLMs) are trained on massive web-crawled corpora. This poses risks of leakage, including personal information, copyrighted texts, and benchmark datasets. Such leakage leads to undermining human trust in AI due to potential unauthorized generation of content or overestimation of performance. We establish the following three criteria concerning the leakage issues: (1) leakage rate: the proportion of leaked data in training data, (2) output rate: the ease of generating leaked data, and (3) detection rate: the detection performance of leaked versus non-leaked data. Despite the leakage rate being the origin of data leakage issues, it is not understood how it affects the output rate and detection rate. In this paper, we conduct an experimental survey to elucidate the relationship between the leakage rate and both the output rate and detection rate for personal information, copyrighted texts, and benchmark data. Additiona",
    "link": "https://arxiv.org/abs/2403.16139",
    "context": "Title: A Little Leak Will Sink a Great Ship: Survey of Transparency for Large Language Models from Start to Finish\nAbstract: arXiv:2403.16139v1 Announce Type: new  Abstract: Large Language Models (LLMs) are trained on massive web-crawled corpora. This poses risks of leakage, including personal information, copyrighted texts, and benchmark datasets. Such leakage leads to undermining human trust in AI due to potential unauthorized generation of content or overestimation of performance. We establish the following three criteria concerning the leakage issues: (1) leakage rate: the proportion of leaked data in training data, (2) output rate: the ease of generating leaked data, and (3) detection rate: the detection performance of leaked versus non-leaked data. Despite the leakage rate being the origin of data leakage issues, it is not understood how it affects the output rate and detection rate. In this paper, we conduct an experimental survey to elucidate the relationship between the leakage rate and both the output rate and detection rate for personal information, copyrighted texts, and benchmark data. Additiona",
    "path": "papers/24/03/2403.16139.json",
    "total_tokens": 920,
    "translated_title": "一点小漏洞就会使一艘巨轮沉没：从头到尾对大型语言模型的透明度进行调查",
    "translated_abstract": "大型语言模型(LLMs)是在海量网络抓取的语料库上训练的。这带来了泄漏风险，包括个人信息、受版权保护的文本和基准数据集。这种泄漏导致人类对人工智能的信任受损，因为可能会未经授权地生成内容或高估性能。我们建立了关于泄漏问题的三个标准：(1)泄漏率：训练数据中泄漏数据的比例，(2)生成率：生成泄漏数据的难易程度，以及(3)检测率：检测泄漏数据与非泄漏数据的性能。尽管泄漏率是数据泄漏问题的根源，但人们不清楚它会如何影响输出率和检测率。在本文中，我们进行了实验调查，阐明了个人信息、受版权保护的文本和基准数据的泄漏率与输出率以及检测率之间的关系。",
    "tldr": "对大型语言模型的透明度进行调查，探讨泄漏问题的影响，建立了泄漏率、生成率和检测率这三个标准，并实验阐明了泄漏率与输出率以及检测率之间的关系。",
    "en_tdlr": "Investigating the transparency of large language models, exploring the impact of leakage issues, establishing criteria of leakage rate, output rate, and detection rate, and experimentally elucidating the relationship between leakage rate and both output rate and detection rate."
}