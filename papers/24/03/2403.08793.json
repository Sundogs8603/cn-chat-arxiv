{
    "title": "Neural Loss Function Evolution for Large-Scale Image Classifier Convolutional Neural Networks",
    "abstract": "arXiv:2403.08793v1 Announce Type: cross  Abstract: For classification, neural networks typically learn by minimizing cross-entropy, but are evaluated and compared using accuracy. This disparity suggests neural loss function search (NLFS), the search for a drop-in replacement loss function of cross-entropy for neural networks. We apply NLFS to image classifier convolutional neural networks. We propose a new search space for NLFS that encourages more diverse loss functions to be explored, and a surrogate function that accurately transfers to large-scale convolutional neural networks. We search the space using regularized evolution, a mutation-only aging genetic algorithm. After evolution and a proposed loss function elimination protocol, we transferred the final loss functions across multiple architectures, datasets, and image augmentation techniques to assess generalization. In the end, we discovered three new loss functions, called NeuroLoss1, NeuroLoss2, and NeuroLoss3 that were able ",
    "link": "https://arxiv.org/abs/2403.08793",
    "context": "Title: Neural Loss Function Evolution for Large-Scale Image Classifier Convolutional Neural Networks\nAbstract: arXiv:2403.08793v1 Announce Type: cross  Abstract: For classification, neural networks typically learn by minimizing cross-entropy, but are evaluated and compared using accuracy. This disparity suggests neural loss function search (NLFS), the search for a drop-in replacement loss function of cross-entropy for neural networks. We apply NLFS to image classifier convolutional neural networks. We propose a new search space for NLFS that encourages more diverse loss functions to be explored, and a surrogate function that accurately transfers to large-scale convolutional neural networks. We search the space using regularized evolution, a mutation-only aging genetic algorithm. After evolution and a proposed loss function elimination protocol, we transferred the final loss functions across multiple architectures, datasets, and image augmentation techniques to assess generalization. In the end, we discovered three new loss functions, called NeuroLoss1, NeuroLoss2, and NeuroLoss3 that were able ",
    "path": "papers/24/03/2403.08793.json",
    "total_tokens": 889,
    "translated_title": "大规模图像分类器卷积神经网络的神经损失函数进化",
    "translated_abstract": "在分类任务中，神经网络通常通过最小化交叉熵来学习，但是却使用准确度进行评估和比较。 这种差异性促使了神经损失函数搜索（NLFS），即寻找可以替代神经网络交叉熵损失函数的新损失函数。 我们将NLFS应用于图像分类器卷积神经网络。 我们提出了一个新的NLFS搜索空间，鼓励探索更多不同的损失函数，并提出了一个可以准确转换到大规模卷积神经网络的替代函数。 我们使用正则化进化进行空间搜索，这是一种仅通过变异的年龄遗传算法。 经过进化和提出的损失函数消除方案后，我们将最终得到的损失函数在多个架构、数据集和图像增强技术之间传递，以评估泛化性。最终，我们发现了三种新的损失函数，分别称为NeuroLoss1、NeuroLoss2和NeuroLoss3。",
    "tldr": "通过神经损失函数搜索，本研究发现了三种新的损失函数NeuroLoss1、NeuroLoss2和NeuroLoss3，这些损失函数能够在大规模图像分类器卷积神经网络中发挥作用。",
    "en_tdlr": "This study discovered three new loss functions, NeuroLoss1, NeuroLoss2, and NeuroLoss3, through neural loss function search, which can be effective in large-scale image classifier convolutional neural networks."
}