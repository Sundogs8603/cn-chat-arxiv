{
    "title": "AffineQuant: Affine Transformation Quantization for Large Language Models",
    "abstract": "arXiv:2403.12544v1 Announce Type: new  Abstract: The significant resource requirements associated with Large-scale Language Models (LLMs) have generated considerable interest in the development of techniques aimed at compressing and accelerating neural networks. Among these techniques, Post-Training Quantization (PTQ) has emerged as a subject of considerable interest due to its noteworthy compression efficiency and cost-effectiveness in the context of training. Existing PTQ methods for LLMs limit the optimization scope to scaling transformations between pre- and post-quantization weights. In this paper, we advocate for the direct optimization using equivalent Affine transformations in PTQ (AffineQuant). This approach extends the optimization scope and thus significantly minimizing quantization errors. Additionally, by employing the corresponding inverse matrix, we can ensure equivalence between the pre- and post-quantization outputs of PTQ, thereby maintaining its efficiency and genera",
    "link": "https://arxiv.org/abs/2403.12544",
    "context": "Title: AffineQuant: Affine Transformation Quantization for Large Language Models\nAbstract: arXiv:2403.12544v1 Announce Type: new  Abstract: The significant resource requirements associated with Large-scale Language Models (LLMs) have generated considerable interest in the development of techniques aimed at compressing and accelerating neural networks. Among these techniques, Post-Training Quantization (PTQ) has emerged as a subject of considerable interest due to its noteworthy compression efficiency and cost-effectiveness in the context of training. Existing PTQ methods for LLMs limit the optimization scope to scaling transformations between pre- and post-quantization weights. In this paper, we advocate for the direct optimization using equivalent Affine transformations in PTQ (AffineQuant). This approach extends the optimization scope and thus significantly minimizing quantization errors. Additionally, by employing the corresponding inverse matrix, we can ensure equivalence between the pre- and post-quantization outputs of PTQ, thereby maintaining its efficiency and genera",
    "path": "papers/24/03/2403.12544.json",
    "total_tokens": 810,
    "translated_title": "AffineQuant：用于大型语言模型的仿射变换量化",
    "translated_abstract": "大规模语言模型（LLMs）所需的显著资源需求引起了人们对开发旨在压缩和加速神经网络的技术的极大兴趣。在这些技术中，后训练量化（PTQ）由于在训练背景下的显著压缩效率和成本效益而引起了相当大的关注。现有的LLMs后量化方法限制优化范围在前后量化权重之间的缩放变换。在本文中，我们提倡使用等效仿射变换进行直接优化（AffineQuant）的PTQ。这种方法扩展了优化范围，从而显著减少量化误差。此外，通过使用相应的逆矩阵，我们可以确保PTQ的前后量化输出之间的等效性，从而保持其效率和泛化性能。",
    "tldr": "AffineQuant是一种用于大型语言模型的直接优化后训练量化方法，通过使用等效的仿射变换来扩展优化范围并显著减小量化误差。",
    "en_tdlr": "AffineQuant is a post-training quantization method for large language models that directly optimizes by utilizing equivalent affine transformations to extend the optimization scope and significantly reduce quantization errors."
}