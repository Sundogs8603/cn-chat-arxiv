{
    "title": "Self-supervised Contrastive Learning for Implicit Collaborative Filtering",
    "abstract": "arXiv:2403.07265v1 Announce Type: new  Abstract: Contrastive learning-based recommendation algorithms have significantly advanced the field of self-supervised recommendation, particularly with BPR as a representative ranking prediction task that dominates implicit collaborative filtering. However, the presence of false-positive and false-negative examples in recommendation systems hampers accurate preference learning. In this study, we propose a simple self-supervised contrastive learning framework that leverages positive feature augmentation and negative label augmentation to improve the self-supervisory signal. Theoretical analysis demonstrates that our learning method is equivalent to maximizing the likelihood estimation with latent variables representing user interest centers. Additionally, we establish an efficient negative label augmentation technique that samples unlabeled examples with a probability linearly dependent on their relative ranking positions, enabling efficient augm",
    "link": "https://arxiv.org/abs/2403.07265",
    "context": "Title: Self-supervised Contrastive Learning for Implicit Collaborative Filtering\nAbstract: arXiv:2403.07265v1 Announce Type: new  Abstract: Contrastive learning-based recommendation algorithms have significantly advanced the field of self-supervised recommendation, particularly with BPR as a representative ranking prediction task that dominates implicit collaborative filtering. However, the presence of false-positive and false-negative examples in recommendation systems hampers accurate preference learning. In this study, we propose a simple self-supervised contrastive learning framework that leverages positive feature augmentation and negative label augmentation to improve the self-supervisory signal. Theoretical analysis demonstrates that our learning method is equivalent to maximizing the likelihood estimation with latent variables representing user interest centers. Additionally, we establish an efficient negative label augmentation technique that samples unlabeled examples with a probability linearly dependent on their relative ranking positions, enabling efficient augm",
    "path": "papers/24/03/2403.07265.json",
    "total_tokens": 821,
    "translated_title": "基于自监督对比学习的隐式协同过滤",
    "translated_abstract": "基于对比学习的推荐算法显著推进了自监督推荐领域的发展，特别是以BPR作为主要代表的掌握隐式协同过滤的排名预测任务。然而，在推荐系统中存在的误正例和误负例阻碍了准确的偏好学习。本研究提出了一个简单的自监督对比学习框架，利用正特征增强和负标签增强来改进自我监督信号。理论分析表明，我们的学习方法等效于利用代表用户兴趣中心的潜在变量最大化似然估计。此外，我们建立了一种有效的负标签增强技术，根据它们的相对排名位置线性相关地采样未标记的示例，实现了有效的增强。",
    "tldr": "提出了一个简单的自监督对比学习框架，通过正特征增强和负标签增强改进自我监督信号，提高了隐式协同过滤准确性，并建立了一种有效的负标签增强技术，实现了有效的增强",
    "en_tdlr": "A simple self-supervised contrastive learning framework is proposed to improve the accuracy of implicit collaborative filtering by leveraging positive feature augmentation and negative label augmentation, with the establishment of an efficient negative label augmentation technique."
}