{
    "title": "Is Cosine-Similarity of Embeddings Really About Similarity?",
    "abstract": "arXiv:2403.05440v1 Announce Type: cross  Abstract: Cosine-similarity is the cosine of the angle between two vectors, or equivalently the dot product between their normalizations. A popular application is to quantify semantic similarity between high-dimensional objects by applying cosine-similarity to a learned low-dimensional feature embedding. This can work better but sometimes also worse than the unnormalized dot-product between embedded vectors in practice. To gain insight into this empirical observation, we study embeddings derived from regularized linear models, where closed-form solutions facilitate analytical insights. We derive analytically how cosine-similarity can yield arbitrary and therefore meaningless `similarities.' For some linear models the similarities are not even unique, while for others they are implicitly controlled by the regularization. We discuss implications beyond linear models: a combination of different regularizations are employed when learning deep models",
    "link": "https://arxiv.org/abs/2403.05440",
    "context": "Title: Is Cosine-Similarity of Embeddings Really About Similarity?\nAbstract: arXiv:2403.05440v1 Announce Type: cross  Abstract: Cosine-similarity is the cosine of the angle between two vectors, or equivalently the dot product between their normalizations. A popular application is to quantify semantic similarity between high-dimensional objects by applying cosine-similarity to a learned low-dimensional feature embedding. This can work better but sometimes also worse than the unnormalized dot-product between embedded vectors in practice. To gain insight into this empirical observation, we study embeddings derived from regularized linear models, where closed-form solutions facilitate analytical insights. We derive analytically how cosine-similarity can yield arbitrary and therefore meaningless `similarities.' For some linear models the similarities are not even unique, while for others they are implicitly controlled by the regularization. We discuss implications beyond linear models: a combination of different regularizations are employed when learning deep models",
    "path": "papers/24/03/2403.05440.json",
    "total_tokens": 852,
    "translated_title": "嵌入的余弦相似性真的只是关于相似性吗？",
    "translated_abstract": "余弦相似度是两个向量之间夹角的余弦，或者等价地说是它们归一化后的点积。一个常见的应用是通过将余弦相似度应用于学习的低维特征嵌入来量化高维对象之间的语义相似性。在实践中，这种方法有时比嵌入向量之间的未归一化点积效果更好，但有时也更差。为了深入了解这一经验观察，我们研究了由正则化线性模型导出的嵌入，其中封闭形式的解决方案有助于分析洞察力。我们在分析上推导出余弦相似性如何产生任意且因此无意义的“相似性”。对于一些线性模型，相似性甚至不是唯一的，而对于其他一些模型，它们受到正则化的隐式控制。我们讨论了线性模型之外的影响：在学习深层模型时，会采用不同正则化的组合。",
    "tldr": "余弦相似度可以产生任意和无意义的“相似性”，受正则化控制，并讨论了深层模型学习中的影响。",
    "en_tdlr": "Cosine similarity can produce arbitrary and meaningless \"similarities\", controlled by regularization, and implications beyond linear models in deep model learning are discussed."
}