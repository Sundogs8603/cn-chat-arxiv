{
    "title": "CodeShell Technical Report",
    "abstract": "arXiv:2403.15747v1 Announce Type: cross  Abstract: Code large language models mark a pivotal breakthrough in artificial intelligence. They are specifically crafted to understand and generate programming languages, significantly boosting the efficiency of coding development workflows. In this technical report, we present CodeShell-Base, a seven billion-parameter foundation model with 8K context length, showcasing exceptional proficiency in code comprehension. By incorporating Grouped-Query Attention and Rotary Positional Embedding into GPT-2, CodeShell-Base integrates the structural merits of StarCoder and CodeLlama and forms its unique architectural design. We then carefully built a comprehensive data pre-processing process, including similar data deduplication, perplexity-based data filtering, and model-based data filtering. Through this process, We have curated 100 billion high-quality pre-training data from GitHub. Benefiting from the high-quality data, CodeShell-Base outperforms Co",
    "link": "https://arxiv.org/abs/2403.15747",
    "context": "Title: CodeShell Technical Report\nAbstract: arXiv:2403.15747v1 Announce Type: cross  Abstract: Code large language models mark a pivotal breakthrough in artificial intelligence. They are specifically crafted to understand and generate programming languages, significantly boosting the efficiency of coding development workflows. In this technical report, we present CodeShell-Base, a seven billion-parameter foundation model with 8K context length, showcasing exceptional proficiency in code comprehension. By incorporating Grouped-Query Attention and Rotary Positional Embedding into GPT-2, CodeShell-Base integrates the structural merits of StarCoder and CodeLlama and forms its unique architectural design. We then carefully built a comprehensive data pre-processing process, including similar data deduplication, perplexity-based data filtering, and model-based data filtering. Through this process, We have curated 100 billion high-quality pre-training data from GitHub. Benefiting from the high-quality data, CodeShell-Base outperforms Co",
    "path": "papers/24/03/2403.15747.json",
    "total_tokens": 809,
    "translated_title": "CodeShell技术报告",
    "translated_abstract": "大型语言模型在人工智能领域取得了重要突破。它们专门设计用于理解和生成编程语言，显著提高了编码开发工作流的效率。本技术报告介绍了CodeShell-Base，这是一个70亿参数规模的基础模型，具有8K上下文长度，在代码理解方面表现出色。通过将Grouped-Query Attention和Rotary Positional Embedding整合到GPT-2中，CodeShell-Base融合了StarCoder和CodeLlama的结构优点，并形成了其独特的架构设计。我们还精心构建了包括类似数据去重、基于困惑度的数据过滤和基于模型的数据过滤在内的全面数据预处理流程。通过这一过程，我们从GitHub中筛选出了1000亿条高质量的预训练数据。凭借这些高质量数据，CodeShell-Base胜过了Co",
    "tldr": "CodeShell-Base是一个70亿参数规模的基础模型，在代码理解方面表现出色，并通过集成Grouped-Query Attention和Rotary Positional Embedding等技术形成独特的架构设计。",
    "en_tdlr": "CodeShell-Base is a foundational model with 7 billion parameters that excels in code comprehension, featuring a unique architectural design through the integration of technologies like Grouped-Query Attention and Rotary Positional Embedding."
}