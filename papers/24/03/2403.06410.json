{
    "title": "A Logical Pattern Memory Pre-trained Model for Entailment Tree Generation",
    "abstract": "arXiv:2403.06410v1 Announce Type: cross  Abstract: Generating coherent and credible explanations remains a significant challenge in the field of AI. In recent years, researchers have delved into the utilization of entailment trees to depict explanations, which exhibit a reasoning process of how a hypothesis is deduced from the supporting facts. However, existing models often overlook the importance of generating intermediate conclusions with logical consistency from the given facts, leading to inaccurate conclusions and undermining the overall credibility of entailment trees. To address this limitation, we propose the logical pattern memory pre-trained model (LMPM). LMPM incorporates an external memory structure to learn and store the latent representations of logical patterns, which aids in generating logically consistent conclusions. Furthermore, to mitigate the influence of logically irrelevant domain knowledge in the Wikipedia-based data, we introduce an entity abstraction approach",
    "link": "https://arxiv.org/abs/2403.06410",
    "context": "Title: A Logical Pattern Memory Pre-trained Model for Entailment Tree Generation\nAbstract: arXiv:2403.06410v1 Announce Type: cross  Abstract: Generating coherent and credible explanations remains a significant challenge in the field of AI. In recent years, researchers have delved into the utilization of entailment trees to depict explanations, which exhibit a reasoning process of how a hypothesis is deduced from the supporting facts. However, existing models often overlook the importance of generating intermediate conclusions with logical consistency from the given facts, leading to inaccurate conclusions and undermining the overall credibility of entailment trees. To address this limitation, we propose the logical pattern memory pre-trained model (LMPM). LMPM incorporates an external memory structure to learn and store the latent representations of logical patterns, which aids in generating logically consistent conclusions. Furthermore, to mitigate the influence of logically irrelevant domain knowledge in the Wikipedia-based data, we introduce an entity abstraction approach",
    "path": "papers/24/03/2403.06410.json",
    "total_tokens": 898,
    "translated_title": "用于蕴涵树生成的逻辑模式记忆预训练模型",
    "translated_abstract": "在人工智能领域，生成连贯可信的解释仍然是一个重大挑战。最近，研究人员深入研究了利用蕴涵树来描述解释的方法，这展示了一个假设如何从支持事实中推导出的推理过程。然而，现有模型通常忽视了从给定事实中生成具有逻辑一致性的中间结论的重要性，导致不准确的结论，削弱了蕴涵树的整体可信度。为了解决这一限制，我们提出了逻辑模式记忆预训练模型（LMPM）。LMPM结合了外部存储结构，学习和存储逻辑模式的潜在表示，有助于生成逻辑一致的结论。此外，为了减少维基百科数据中逻辑无关领域知识的影响，我们引入了一种实体抽象方法。",
    "tldr": "提出了逻辑模式记忆预训练模型（LMPM），通过结合外部存储结构学习和存储逻辑模式的潜在表示，有助于生成逻辑一致的结论，并引入实体抽象方法来减少维基百科数据中的逻辑无关领域知识的影响。",
    "en_tdlr": "Introduced the logical pattern memory pre-trained model (LMPM) which incorporates an external memory structure to learn and store the latent representations of logical patterns, aiding in generating logically consistent conclusions, and introduced an entity abstraction approach to mitigate the influence of logically irrelevant domain knowledge in the Wikipedia-based data."
}