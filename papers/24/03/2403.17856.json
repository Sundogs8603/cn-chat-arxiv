{
    "title": "Verbing Weirds Language (Models): Evaluation of English Zero-Derivation in Five LLMs",
    "abstract": "arXiv:2403.17856v1 Announce Type: new  Abstract: Lexical-syntactic flexibility, in the form of conversion (or zero-derivation) is a hallmark of English morphology. In conversion, a word with one part of speech is placed in a non-prototypical context, where it is coerced to behave as if it had a different part of speech. However, while this process affects a large part of the English lexicon, little work has been done to establish the degree to which language models capture this type of generalization. This paper reports the first study on the behavior of large language models with reference to conversion. We design a task for testing lexical-syntactic flexibility -- the degree to which models can generalize over words in a construction with a non-prototypical part of speech. This task is situated within a natural language inference paradigm. We test the abilities of five language models -- two proprietary models (GPT-3.5 and GPT-4), three open-source models (Mistral 7B, Falcon 40B, and",
    "link": "https://arxiv.org/abs/2403.17856",
    "context": "Title: Verbing Weirds Language (Models): Evaluation of English Zero-Derivation in Five LLMs\nAbstract: arXiv:2403.17856v1 Announce Type: new  Abstract: Lexical-syntactic flexibility, in the form of conversion (or zero-derivation) is a hallmark of English morphology. In conversion, a word with one part of speech is placed in a non-prototypical context, where it is coerced to behave as if it had a different part of speech. However, while this process affects a large part of the English lexicon, little work has been done to establish the degree to which language models capture this type of generalization. This paper reports the first study on the behavior of large language models with reference to conversion. We design a task for testing lexical-syntactic flexibility -- the degree to which models can generalize over words in a construction with a non-prototypical part of speech. This task is situated within a natural language inference paradigm. We test the abilities of five language models -- two proprietary models (GPT-3.5 and GPT-4), three open-source models (Mistral 7B, Falcon 40B, and",
    "path": "papers/24/03/2403.17856.json",
    "total_tokens": 916,
    "translated_title": "对五个大型语言模型中英语零派生进行评估",
    "translated_abstract": "词汇-句法的灵活性，以转换（或零派生）的形式出现，是英语形态学的一个标志。在转换中，一个词与一个词性被放置在一个非典型的上下文中，被迫表现得好像它有一个不同的词性。然而，虽然这一过程影响了英语词汇的一大部分，但很少有工作着手确定语言模型到底捕捉到了这种泛化的程度。本文首次报道了关于大型语言模型行为与转换之间关系的研究。我们设计了一个用于测试词汇-句法灵活性的任务，即模型能够在一个非典型词性构造的词上进行推广的程度。这个任务位于自然语言推理范式之内。我们测试了五个语言模型的能力：两个专有模型（GPT-3.5 和 GPT-4）、三个开源模型（Mistral 7B、Falcon 40B 等）",
    "tldr": "这项研究是关于五个大型语言模型中对英语零派生的评估，首次研究了这类泛化的程度，通过设计了一个测试词汇-句法灵活性的任务来实现。",
    "en_tdlr": "This study evaluates English zero-derivation in five large language models for the first time, examining the extent to which they capture such generalization by designing a task to test lexical-syntactic flexibility."
}