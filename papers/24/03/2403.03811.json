{
    "title": "Incentivized Learning in Principal-Agent Bandit Games",
    "abstract": "arXiv:2403.03811v1 Announce Type: cross  Abstract: This work considers a repeated principal-agent bandit game, where the principal can only interact with her environment through the agent. The principal and the agent have misaligned objectives and the choice of action is only left to the agent. However, the principal can influence the agent's decisions by offering incentives which add up to his rewards. The principal aims to iteratively learn an incentive policy to maximize her own total utility. This framework extends usual bandit problems and is motivated by several practical applications, such as healthcare or ecological taxation, where traditionally used mechanism design theories often overlook the learning aspect of the problem. We present nearly optimal (with respect to a horizon $T$) learning algorithms for the principal's regret in both multi-armed and linear contextual settings. Finally, we support our theoretical guarantees through numerical experiments.",
    "link": "https://arxiv.org/abs/2403.03811",
    "context": "Title: Incentivized Learning in Principal-Agent Bandit Games\nAbstract: arXiv:2403.03811v1 Announce Type: cross  Abstract: This work considers a repeated principal-agent bandit game, where the principal can only interact with her environment through the agent. The principal and the agent have misaligned objectives and the choice of action is only left to the agent. However, the principal can influence the agent's decisions by offering incentives which add up to his rewards. The principal aims to iteratively learn an incentive policy to maximize her own total utility. This framework extends usual bandit problems and is motivated by several practical applications, such as healthcare or ecological taxation, where traditionally used mechanism design theories often overlook the learning aspect of the problem. We present nearly optimal (with respect to a horizon $T$) learning algorithms for the principal's regret in both multi-armed and linear contextual settings. Finally, we support our theoretical guarantees through numerical experiments.",
    "path": "papers/24/03/2403.03811.json",
    "total_tokens": 920,
    "translated_title": "委托-代理赌博游戏中的激励学习",
    "translated_abstract": "本文考虑了一个重复的委托-代理赌博游戏，委托方只能通过代理与环境互动。委托方和代理方的目标不一致，选择行动的权利仅归代理方所有。然而，委托方可以通过提供激励来影响代理方的决策，激励将计入其奖励之中。委托方的目标是迭代学习一种激励策略，以最大化其总效用。这一框架扩展了传统的赌博问题，并受到了几个实际应用的启发，如医疗保健或生态税收，传统的机制设计理论往往忽视了问题的学习方面。我们在多臂和线性背景设置下提出了关于委托方后悔的几乎最优（相对于一个时域 T 的）学习算法。最后，我们通过数值实验支持我们的理论保证。",
    "tldr": "本文考虑了一个重复的委托-代理赌博游戏，在其中委托方通过提供激励以影响代理方的决策，目标是迭代学习激励策略最大化效用，提出了关于委托方后悔的几乎最优学习算法，支撑理论保证通过数值实验。",
    "en_tdlr": "This work explores a repeated principal-agent bandit game where the principal influences the agent's decisions by offering incentives, aims to iteratively learn an incentive policy to maximize utility, proposes nearly optimal learning algorithms for the principal's regret, and supports theoretical guarantees through numerical experiments."
}