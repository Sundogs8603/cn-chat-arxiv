{
    "title": "Adaptive Decentralized Federated Learning in Energy and Latency Constrained Wireless Networks",
    "abstract": "arXiv:2403.20075v1 Announce Type: new  Abstract: In Federated Learning (FL), with parameter aggregated by a central node, the communication overhead is a substantial concern. To circumvent this limitation and alleviate the single point of failure within the FL framework, recent studies have introduced Decentralized Federated Learning (DFL) as a viable alternative. Considering the device heterogeneity, and energy cost associated with parameter aggregation, in this paper, the problem on how to efficiently leverage the limited resources available to enhance the model performance is investigated. Specifically, we formulate a problem that minimizes the loss function of DFL while considering energy and latency constraints. The proposed solution involves optimizing the number of local training rounds across diverse devices with varying resource budgets. To make this problem tractable, we first analyze the convergence of DFL with edge devices with different rounds of local training. The derive",
    "link": "https://arxiv.org/abs/2403.20075",
    "context": "Title: Adaptive Decentralized Federated Learning in Energy and Latency Constrained Wireless Networks\nAbstract: arXiv:2403.20075v1 Announce Type: new  Abstract: In Federated Learning (FL), with parameter aggregated by a central node, the communication overhead is a substantial concern. To circumvent this limitation and alleviate the single point of failure within the FL framework, recent studies have introduced Decentralized Federated Learning (DFL) as a viable alternative. Considering the device heterogeneity, and energy cost associated with parameter aggregation, in this paper, the problem on how to efficiently leverage the limited resources available to enhance the model performance is investigated. Specifically, we formulate a problem that minimizes the loss function of DFL while considering energy and latency constraints. The proposed solution involves optimizing the number of local training rounds across diverse devices with varying resource budgets. To make this problem tractable, we first analyze the convergence of DFL with edge devices with different rounds of local training. The derive",
    "path": "papers/24/03/2403.20075.json",
    "total_tokens": 854,
    "translated_title": "能源和延迟受限的无线网络中的自适应分散式联邦学习",
    "translated_abstract": "在联邦学习（FL）中，由中心节点聚合参数，通信开销是一个重要的问题。为了规避这一限制并减轻FL框架内的单点故障，最近的研究引入了分散式联邦学习（DFL）作为一种可行的替代方案。考虑到设备的异构性，以及与参数聚合相关的能量成本，本文研究了如何高效利用有限资源来提高模型性能的问题。具体而言，我们提出了一个问题，即在考虑能源和延迟约束的情况下最小化DFL的损失函数。所提出的解决方案涉及优化在不同资源预算的多样化设备之间进行的本地训练轮次的数量。为了使这个问题可行，我们首先分析了具有不同本地训练轮次的边缘设备的DFL的收敛性。",
    "tldr": "本文提出了在考虑能源和延迟约束的情况下，通过优化不同资源预算的设备之间的本地训练轮次数量，以最小化分散式联邦学习（DFL）的损失函数。"
}