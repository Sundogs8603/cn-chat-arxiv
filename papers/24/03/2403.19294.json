{
    "title": "FlowDepth: Decoupling Optical Flow for Self-Supervised Monocular Depth Estimation",
    "abstract": "arXiv:2403.19294v1 Announce Type: cross  Abstract: Self-supervised multi-frame methods have currently achieved promising results in depth estimation. However, these methods often suffer from mismatch problems due to the moving objects, which break the static assumption. Additionally, unfairness can occur when calculating photometric errors in high-freq or low-texture regions of the images. To address these issues, existing approaches use additional semantic priori black-box networks to separate moving objects and improve the model only at the loss level. Therefore, we propose FlowDepth, where a Dynamic Motion Flow Module (DMFM) decouples the optical flow by a mechanism-based approach and warps the dynamic regions thus solving the mismatch problem. For the unfairness of photometric errors caused by high-freq and low-texture regions, we use Depth-Cue-Aware Blur (DCABlur) and Cost-Volume sparsity loss respectively at the input and the loss level to solve the problem. Experimental results ",
    "link": "https://arxiv.org/abs/2403.19294",
    "context": "Title: FlowDepth: Decoupling Optical Flow for Self-Supervised Monocular Depth Estimation\nAbstract: arXiv:2403.19294v1 Announce Type: cross  Abstract: Self-supervised multi-frame methods have currently achieved promising results in depth estimation. However, these methods often suffer from mismatch problems due to the moving objects, which break the static assumption. Additionally, unfairness can occur when calculating photometric errors in high-freq or low-texture regions of the images. To address these issues, existing approaches use additional semantic priori black-box networks to separate moving objects and improve the model only at the loss level. Therefore, we propose FlowDepth, where a Dynamic Motion Flow Module (DMFM) decouples the optical flow by a mechanism-based approach and warps the dynamic regions thus solving the mismatch problem. For the unfairness of photometric errors caused by high-freq and low-texture regions, we use Depth-Cue-Aware Blur (DCABlur) and Cost-Volume sparsity loss respectively at the input and the loss level to solve the problem. Experimental results ",
    "path": "papers/24/03/2403.19294.json",
    "total_tokens": 932,
    "translated_title": "FlowDepth: 解耦光流用于自监督单目深度估计",
    "translated_abstract": "自监督多帧方法目前在深度估计方面取得了令人期待的结果。然而，这些方法经常因为移动物体而遭受不匹配问题，这打破了静态假设。此外，在计算图像的高频或低纹理区域的光度误差时可能会出现不公平现象。为了解决这些问题，现有方法使用额外的语义先验黑盒网络来分离移动物体，并仅在损失水平上改进模型。因此，我们提出了FlowDepth，其中动态运动光流模块（DMFM）通过基于机制的方法解耦光流，并对动态区域进行变形，从而解决了不匹配问题。为解决由高频和低纹理区域引起的光度误差的不公平问题，我们分别在输入和损失水平上使用Depth-Cue-Aware Blur（DCABlur）和Cost-Volume稀疏损失来解决这个问题。",
    "tldr": "FlowDepth提出了通过Dynamic Motion Flow Module（DMFM）解耦光流，通过运用基于机制的方法解决移动物体引起的不匹配问题，同时使用Depth-Cue-Aware Blur（DCABlur）和Cost-Volume稀疏损失来解决高频和低纹理区域的光度误差不公平问题。",
    "en_tdlr": "FlowDepth proposes to decouple the optical flow using the Dynamic Motion Flow Module (DMFM) to address mismatch problems caused by moving objects, and utilizes Depth-Cue-Aware Blur (DCABlur) and Cost-Volume sparsity loss to tackle the unfairness of photometric errors in high-frequency and low-texture regions."
}