{
    "title": "IM-Unpack: Training and Inference with Arbitrarily Low Precision Integers",
    "abstract": "arXiv:2403.07339v1 Announce Type: cross  Abstract: GEneral Matrix Multiply (GEMM) is a central operation in deep learning and corresponds to the largest chunk of the compute footprint. Therefore, improving its efficiency is an active topic of ongoing research. A popular strategy is the use of low bit-width integers to approximate the original entries in a matrix. This allows efficiency gains, but often requires sophisticated techniques to control the rounding error incurred. In this work, we first verify/check that when the low bit-width restriction is removed, for a variety of Transformer-based models, whether integers are sufficient for all GEMMs need -- for {\\em both} training and inference stages, and can achieve parity with floating point counterparts. No sophisticated techniques are needed. We find that while a large majority of entries in matrices (encountered in such models) can be easily represented by {\\em low} bit-width integers, the existence of a few heavy hitter entries m",
    "link": "https://arxiv.org/abs/2403.07339",
    "context": "Title: IM-Unpack: Training and Inference with Arbitrarily Low Precision Integers\nAbstract: arXiv:2403.07339v1 Announce Type: cross  Abstract: GEneral Matrix Multiply (GEMM) is a central operation in deep learning and corresponds to the largest chunk of the compute footprint. Therefore, improving its efficiency is an active topic of ongoing research. A popular strategy is the use of low bit-width integers to approximate the original entries in a matrix. This allows efficiency gains, but often requires sophisticated techniques to control the rounding error incurred. In this work, we first verify/check that when the low bit-width restriction is removed, for a variety of Transformer-based models, whether integers are sufficient for all GEMMs need -- for {\\em both} training and inference stages, and can achieve parity with floating point counterparts. No sophisticated techniques are needed. We find that while a large majority of entries in matrices (encountered in such models) can be easily represented by {\\em low} bit-width integers, the existence of a few heavy hitter entries m",
    "path": "papers/24/03/2403.07339.json",
    "total_tokens": 886,
    "translated_title": "IM-Unpack: 使用任意低精度整数进行训练和推断",
    "translated_abstract": "GEneral Matrix Multiply (GEMM)是深度学习中的一个核心操作，对应于计算占比最大的部分。因此，提高其效率是一个正在进行研究的热门主题。一种流行的策略是使用低位宽整数来近似矩阵中的原始条目。这样可以提高效率，但常常需要复杂的技术来控制产生的舍入误差。在这项工作中，我们首次验证当移除低位宽限制时，对于各种基于Transformer的模型，整数是否足够满足所有GEMMs的需求 - 无论是训练阶段还是推断阶段，并且可以与浮点数对应项达到一致。无需复杂技术。我们发现，虽然在这些模型中遇到的大多数矩阵条目可以很容易地用低位宽整数表示，但存在一些重要条目",
    "tldr": "本研究旨在验证在移除低位宽限制时，对于各种Transformer-based模型，整数是否足以满足所有GEMM需求（训练和推断阶段），并且可以与浮点数相媲美，而无需复杂技巧。",
    "en_tdlr": "This study aims to verify whether integers are sufficient for all GEMM requirements (training and inference stages) for various Transformer-based models without the need for sophisticated techniques when the low bit-width restriction is removed, achieving parity with floating point counterparts."
}