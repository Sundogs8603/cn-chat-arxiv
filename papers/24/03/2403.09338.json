{
    "title": "LocalMamba: Visual State Space Model with Windowed Selective Scan",
    "abstract": "arXiv:2403.09338v1 Announce Type: cross  Abstract: Recent advancements in state space models, notably Mamba, have demonstrated significant progress in modeling long sequences for tasks like language understanding. Yet, their application in vision tasks has not markedly surpassed the performance of traditional Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). This paper posits that the key to enhancing Vision Mamba (ViM) lies in optimizing scan directions for sequence modeling. Traditional ViM approaches, which flatten spatial tokens, overlook the preservation of local 2D dependencies, thereby elongating the distance between adjacent tokens. We introduce a novel local scanning strategy that divides images into distinct windows, effectively capturing local dependencies while maintaining a global perspective. Additionally, acknowledging the varying preferences for scan patterns across different network layers, we propose a dynamic method to independently search for the ",
    "link": "https://arxiv.org/abs/2403.09338",
    "context": "Title: LocalMamba: Visual State Space Model with Windowed Selective Scan\nAbstract: arXiv:2403.09338v1 Announce Type: cross  Abstract: Recent advancements in state space models, notably Mamba, have demonstrated significant progress in modeling long sequences for tasks like language understanding. Yet, their application in vision tasks has not markedly surpassed the performance of traditional Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). This paper posits that the key to enhancing Vision Mamba (ViM) lies in optimizing scan directions for sequence modeling. Traditional ViM approaches, which flatten spatial tokens, overlook the preservation of local 2D dependencies, thereby elongating the distance between adjacent tokens. We introduce a novel local scanning strategy that divides images into distinct windows, effectively capturing local dependencies while maintaining a global perspective. Additionally, acknowledging the varying preferences for scan patterns across different network layers, we propose a dynamic method to independently search for the ",
    "path": "papers/24/03/2403.09338.json",
    "total_tokens": 866,
    "translated_title": "LocalMamba: 带窗口选择扫描的视觉状态空间模型",
    "translated_abstract": "最近状态空间模型的进展，尤其是Mamba，已经在长序列建模方面取得了显著进展，例如语言理解等任务。然而，它们在视觉任务中的应用并没有明显超越传统的卷积神经网络(CNN)和视觉Transformer(ViTs)的性能。本文认为增强视觉Mamba(ViM)的关键在于优化序列建模的扫描方向。传统的ViM方法将空间令牌展平，忽视了局部2D依赖性的保留，从而拉长了相邻令牌之间的距离。我们引入了一种新颖的局部扫描策略，将图像分割成不同窗口，有效地捕捉局部依赖性同时保持全局视角。此外，我们认识到不同网络层之间扫描模式的偏好可能不同，因此提出了一种动态方法，独立搜索最佳的扫描方向。",
    "tldr": "本论文提出了一种新颖的局部扫描策略，通过在图像中引入窗口划分的方法，有效捕捉局部依赖性，同时保持全局视角，从而增强了视觉Mamba模型的序列建模性能。",
    "en_tdlr": "This paper introduces a novel local scanning strategy that effectively captures local dependencies and maintains a global perspective by dividing images into distinct windows, enhancing the performance of the Vision Mamba model in sequence modeling."
}