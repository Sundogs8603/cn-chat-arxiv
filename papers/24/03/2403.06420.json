{
    "title": "RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic Manipulations With Large Language Models",
    "abstract": "arXiv:2403.06420v1 Announce Type: cross  Abstract: Reinforcement learning (RL) has demonstrated its capability in solving various tasks but is notorious for its low sample efficiency. In this paper, we propose RLingua, a framework that can leverage the internal knowledge of large language models (LLMs) to reduce the sample complexity of RL in robotic manipulations. To this end, we first present how to extract the prior knowledge of LLMs by prompt engineering so that a preliminary rule-based robot controller for a specific task can be generated. Despite being imperfect, the LLM-generated robot controller is utilized to produce action samples during rollouts with a decaying probability, thereby improving RL's sample efficiency. We employ the actor-critic framework and modify the actor loss to regularize the policy learning towards the LLM-generated controller. RLingua also provides a novel method of improving the imperfect LLM-generated robot controllers by RL. We demonstrated that RLing",
    "link": "https://arxiv.org/abs/2403.06420",
    "context": "Title: RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic Manipulations With Large Language Models\nAbstract: arXiv:2403.06420v1 Announce Type: cross  Abstract: Reinforcement learning (RL) has demonstrated its capability in solving various tasks but is notorious for its low sample efficiency. In this paper, we propose RLingua, a framework that can leverage the internal knowledge of large language models (LLMs) to reduce the sample complexity of RL in robotic manipulations. To this end, we first present how to extract the prior knowledge of LLMs by prompt engineering so that a preliminary rule-based robot controller for a specific task can be generated. Despite being imperfect, the LLM-generated robot controller is utilized to produce action samples during rollouts with a decaying probability, thereby improving RL's sample efficiency. We employ the actor-critic framework and modify the actor loss to regularize the policy learning towards the LLM-generated controller. RLingua also provides a novel method of improving the imperfect LLM-generated robot controllers by RL. We demonstrated that RLing",
    "path": "papers/24/03/2403.06420.json",
    "total_tokens": 849,
    "translated_title": "RLingua：利用大型语言模型改善在机器人操作中的强化学习样本效率",
    "translated_abstract": "强化学习（RL）已经证明了其在解决各种任务中的能力，但以其低样本效率而声名狼藉。在本文中，我们提出了RLingua，这是一个可以利用大型语言模型（LLMs）的内部知识来减少机器人操作中RL的样本复杂性的框架。为此，我们首先介绍了如何通过提示工程提取LLMs的先验知识，从而生成特定任务的初步基于规则的机器人控制器。尽管不完美，LLM生成的机器人控制器被用于在rollout时以衰减概率生成动作样本，从而提高RL的样本效率。我们采用了演员-评论家框架，并修改了演员损失，以使策略学习朝着LLM生成的控制器规范化。RLingua还提供了一种改善不完美的LLM生成机器人控制器的新方法。我们展示了RLing",
    "tldr": "RLingua提出了一个框架，利用大型语言模型的内部知识来提高机器人操作中强化学习的样本效率。",
    "en_tdlr": "RLingua proposes a framework to improve the sample efficiency of reinforcement learning in robotic manipulations by leveraging internal knowledge of large language models."
}