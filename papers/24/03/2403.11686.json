{
    "title": "Crystalformer: Infinitely Connected Attention for Periodic Structure Encoding",
    "abstract": "arXiv:2403.11686v1 Announce Type: new  Abstract: Predicting physical properties of materials from their crystal structures is a fundamental problem in materials science. In peripheral areas such as the prediction of molecular properties, fully connected attention networks have been shown to be successful. However, unlike these finite atom arrangements, crystal structures are infinitely repeating, periodic arrangements of atoms, whose fully connected attention results in infinitely connected attention. In this work, we show that this infinitely connected attention can lead to a computationally tractable formulation, interpreted as neural potential summation, that performs infinite interatomic potential summations in a deeply learned feature space. We then propose a simple yet effective Transformer-based encoder architecture for crystal structures called Crystalformer. Compared to an existing Transformer-based model, the proposed model requires only 29.4% of the number of parameters, wit",
    "link": "https://arxiv.org/abs/2403.11686",
    "context": "Title: Crystalformer: Infinitely Connected Attention for Periodic Structure Encoding\nAbstract: arXiv:2403.11686v1 Announce Type: new  Abstract: Predicting physical properties of materials from their crystal structures is a fundamental problem in materials science. In peripheral areas such as the prediction of molecular properties, fully connected attention networks have been shown to be successful. However, unlike these finite atom arrangements, crystal structures are infinitely repeating, periodic arrangements of atoms, whose fully connected attention results in infinitely connected attention. In this work, we show that this infinitely connected attention can lead to a computationally tractable formulation, interpreted as neural potential summation, that performs infinite interatomic potential summations in a deeply learned feature space. We then propose a simple yet effective Transformer-based encoder architecture for crystal structures called Crystalformer. Compared to an existing Transformer-based model, the proposed model requires only 29.4% of the number of parameters, wit",
    "path": "papers/24/03/2403.11686.json",
    "total_tokens": 764,
    "translated_title": "Crystalformer：用于周期结构编码的无限连接注意力",
    "translated_abstract": "材料科学中的一个基本问题是从它们的晶体结构预测材料的物理性质。在预测分子性质等边缘领域，全连接注意力网络已被证明是成功的。然而，与这些有限原子排列不同，晶体结构是无限重复的，周期性的原子排列，其全连接注意力导致无限连接注意力。在这项工作中，我们展示了这种无限连接注意力可以导致一个可计算的公式形式，解释为神经势求和，在一个深度学习特征空间中执行无限的原子间势求和。然后，我们提出了一种简单而有效的基于Transformer的晶体结构编码器架构，称为Crystalformer。与现有的基于Transformer的模型相比，所提出的模型仅需要29.4%的参数数量，wit",
    "tldr": "Crystalformer是一种用于晶体结构的Transformer-based编码器，利用无限连接注意力进行无限的原子间势求和，具有较低的参数需求。",
    "en_tdlr": "Crystalformer is a Transformer-based encoder for crystal structures that utilizes infinitely connected attention for infinite interatomic potential summations, with lower parameter requirements."
}