{
    "title": "FOCIL: Finetune-and-Freeze for Online Class Incremental Learning by Training Randomly Pruned Sparse Experts",
    "abstract": "arXiv:2403.14684v1 Announce Type: cross  Abstract: Class incremental learning (CIL) in an online continual learning setting strives to acquire knowledge on a series of novel classes from a data stream, using each data point only once for training. This is more realistic compared to offline modes, where it is assumed that all data from novel class(es) is readily available. Current online CIL approaches store a subset of the previous data which creates heavy overhead costs in terms of both memory and computation, as well as privacy issues. In this paper, we propose a new online CIL approach called FOCIL. It fine-tunes the main architecture continually by training a randomly pruned sparse subnetwork for each task. Then, it freezes the trained connections to prevent forgetting. FOCIL also determines the sparsity level and learning rate per task adaptively and ensures (almost) zero forgetting across all tasks without storing any replay data. Experimental results on 10-Task CIFAR100, 20-Task",
    "link": "https://arxiv.org/abs/2403.14684",
    "context": "Title: FOCIL: Finetune-and-Freeze for Online Class Incremental Learning by Training Randomly Pruned Sparse Experts\nAbstract: arXiv:2403.14684v1 Announce Type: cross  Abstract: Class incremental learning (CIL) in an online continual learning setting strives to acquire knowledge on a series of novel classes from a data stream, using each data point only once for training. This is more realistic compared to offline modes, where it is assumed that all data from novel class(es) is readily available. Current online CIL approaches store a subset of the previous data which creates heavy overhead costs in terms of both memory and computation, as well as privacy issues. In this paper, we propose a new online CIL approach called FOCIL. It fine-tunes the main architecture continually by training a randomly pruned sparse subnetwork for each task. Then, it freezes the trained connections to prevent forgetting. FOCIL also determines the sparsity level and learning rate per task adaptively and ensures (almost) zero forgetting across all tasks without storing any replay data. Experimental results on 10-Task CIFAR100, 20-Task",
    "path": "papers/24/03/2403.14684.json",
    "total_tokens": 846,
    "translated_title": "FOCIL: 通过训练随机修剪稀疏专家进行在线类递增学习的微调和冻结",
    "translated_abstract": "在线持续学习中的类递增学习（CIL）旨在从数据流中获取一系列新类的知识，仅使用每个数据点进行一次训练。与离线模式相比，这更加现实，离线模式假定所有新类的数据已经准备好。当前的在线CIL方法存储先前数据的子集，这会在内存和计算方面造成沉重的开销，还存在隐私问题。本文提出了一种名为FOCIL的新型在线CIL方法。它通过训练随机修剪稀疏子网络不断微调主体系结构，然后冻结训练连接以防止遗忘。FOCIL还自适应确定每个任务的稀疏度级别和学习速率，并确保（几乎）零遗忘跨所有任务，且不存储任何重放数据。",
    "tldr": "FOCIL通过训练随机修剪稀疏子网络实现在线持续类递增学习，在避免存储重放数据的同时有效防止遗忘。",
    "en_tdlr": "FOCIL achieves online continual class incremental learning by training randomly pruned sparse subnetworks, effectively preventing forgetting without storing replay data."
}