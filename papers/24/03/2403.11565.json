{
    "title": "Decentralized Stochastic Subgradient Methods for Nonsmooth Nonconvex Optimization",
    "abstract": "arXiv:2403.11565v1 Announce Type: cross  Abstract: In this paper, we concentrate on decentralized optimization problems with nonconvex and nonsmooth objective functions, especially on the decentralized training of nonsmooth neural networks. We introduce a unified framework, named DSM, to analyze the global convergence of decentralized stochastic subgradient methods. We prove the global convergence of our proposed framework under mild conditions, by establishing that the generated sequence asymptotically approximates the trajectories of its associated differential inclusion. Furthermore, we establish that our proposed framework encompasses a wide range of existing efficient decentralized subgradient methods, including decentralized stochastic subgradient descent (DSGD), DSGD with gradient-tracking technique (DSGD-T), and DSGD with momentum (DSGDm). In addition, we introduce SignSGD employing the sign map to regularize the update directions in DSGDm, and show it is enclosed in our propos",
    "link": "https://arxiv.org/abs/2403.11565",
    "context": "Title: Decentralized Stochastic Subgradient Methods for Nonsmooth Nonconvex Optimization\nAbstract: arXiv:2403.11565v1 Announce Type: cross  Abstract: In this paper, we concentrate on decentralized optimization problems with nonconvex and nonsmooth objective functions, especially on the decentralized training of nonsmooth neural networks. We introduce a unified framework, named DSM, to analyze the global convergence of decentralized stochastic subgradient methods. We prove the global convergence of our proposed framework under mild conditions, by establishing that the generated sequence asymptotically approximates the trajectories of its associated differential inclusion. Furthermore, we establish that our proposed framework encompasses a wide range of existing efficient decentralized subgradient methods, including decentralized stochastic subgradient descent (DSGD), DSGD with gradient-tracking technique (DSGD-T), and DSGD with momentum (DSGDm). In addition, we introduce SignSGD employing the sign map to regularize the update directions in DSGDm, and show it is enclosed in our propos",
    "path": "papers/24/03/2403.11565.json",
    "total_tokens": 893,
    "translated_title": "基于去中心化随机次梯度法的非平滑非凸优化问题",
    "translated_abstract": "在这篇论文中，我们关注具有非凸和非平滑目标函数的去中心化优化问题，特别是关注非平滑神经网络的去中心化训练。我们提出了一个统一的框架，称为DSM，用于分析去中心化随机次梯度法的全局收敛性。我们证明了在温和条件下，我们提出的框架的全局收敛性，通过建立生成序列渐近逼近其关联微分包含的轨迹。此外，我们证明了我们提出的框架涵盖了各种现有高效的去中心化次梯度方法，包括去中心化随机次梯度下降（DSGD），具有梯度跟踪技术的DSGD（DSGD-T）和带动量的DSGD（DSGDm）。此外，我们引入SignSGD，采用符号映射来正则化DSGDm中的更新方向，并表明它被包含在我们的提议中。",
    "tldr": "该论文介绍了一种名为DSM的统一框架，用于分析去中心化随机次梯度方法的全局收敛性，证明了在温和条件下的全局收敛性，并展示其涵盖了各种现有高效的去中心化次梯度方法。",
    "en_tdlr": "This paper introduces a unified framework named DSM for analyzing the global convergence of decentralized stochastic subgradient methods, proving global convergence under mild conditions, and showing its coverage of various existing efficient decentralized subgradient methods."
}