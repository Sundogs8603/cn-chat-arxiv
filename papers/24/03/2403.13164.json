{
    "title": "VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning",
    "abstract": "arXiv:2403.13164v1 Announce Type: new  Abstract: Large language models (LLMs) famously exhibit emergent in-context learning (ICL) -- the ability to rapidly adapt to new tasks using few-shot examples provided as a prompt, without updating the model's weights. Built on top of LLMs, vision large language models (VLLMs) have advanced significantly in areas such as recognition, reasoning, and grounding. However, investigations into \\emph{multimodal ICL} have predominantly focused on few-shot visual question answering (VQA), and image captioning, which we will show neither exploit the strengths of ICL, nor test its limitations. The broader capabilities and limitations of multimodal ICL remain under-explored. In this study, we introduce a comprehensive benchmark VL-ICL Bench for multimodal in-context learning, encompassing a broad spectrum of tasks that involve both images and text as inputs and outputs, and different types of challenges, from {perception to reasoning and long context length}",
    "link": "https://arxiv.org/abs/2403.13164",
    "context": "Title: VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning\nAbstract: arXiv:2403.13164v1 Announce Type: new  Abstract: Large language models (LLMs) famously exhibit emergent in-context learning (ICL) -- the ability to rapidly adapt to new tasks using few-shot examples provided as a prompt, without updating the model's weights. Built on top of LLMs, vision large language models (VLLMs) have advanced significantly in areas such as recognition, reasoning, and grounding. However, investigations into \\emph{multimodal ICL} have predominantly focused on few-shot visual question answering (VQA), and image captioning, which we will show neither exploit the strengths of ICL, nor test its limitations. The broader capabilities and limitations of multimodal ICL remain under-explored. In this study, we introduce a comprehensive benchmark VL-ICL Bench for multimodal in-context learning, encompassing a broad spectrum of tasks that involve both images and text as inputs and outputs, and different types of challenges, from {perception to reasoning and long context length}",
    "path": "papers/24/03/2403.13164.json",
    "total_tokens": 938,
    "translated_title": "VL-ICL Bench: 基于细节的多模态上下文学习基准测试中的细节之魔",
    "translated_abstract": "大型语言模型（LLMs）以其著名的出现式上下文学习（ICL）而闻名——即在仅提供几个示例作为提示的情况下，快速适应新任务的能力，而无需更新模型的权重。构建在LLMs之上的视觉大型语言模型（VLLMs）在识别、推理和基准确定等领域取得了显著进展。然而，对于\\emph{多模态ICL}的研究主要集中在少样本视觉问题回答（VQA）和图像字幕上，我们将展示二者既没有充分利用ICL的优势，也没有测试其限制。对多模态ICL的更广泛能力和局限性尚未得到充分探讨。在本研究中，我们引入了一个全面的多模态上下文学习基准测试 VL-ICL Bench，涵盖了涉及图像和文本作为输入和输出的广泛任务范围，并涵盖了从{感知到推理和长期上下文长度}的不同类型挑战。",
    "tldr": "大型语言模型的视觉变种在识别、推理和基准确定等领域取得了显著进展，但多模态上下文学习的广泛能力和限制仍未得到充分探讨。",
    "en_tdlr": "Vision large language models have made significant progress in recognition, reasoning, and grounding, but the broader capabilities and limitations of multimodal in-context learning remain under-explored."
}