{
    "title": "Improving LoRA in Privacy-preserving Federated Learning",
    "abstract": "arXiv:2403.12313v1 Announce Type: new  Abstract: Low-rank adaptation (LoRA) is one of the most popular task-specific parameter-efficient fine-tuning (PEFT) methods on pre-trained language models for its good performance and computational efficiency. LoRA injects a product of two trainable rank decomposition matrices over the top of each frozen pre-trained model module. However, when applied in the setting of privacy-preserving federated learning (FL), LoRA may become unstable due to the following facts: 1) the effects of data heterogeneity and multi-step local updates are non-negligible, 2) additive noise enforced on updating gradients to guarantee differential privacy (DP) can be amplified and 3) the final performance is susceptible to hyper-parameters. A key factor leading to these phenomena is the discordance between jointly optimizing the two low-rank matrices by local clients and separately aggregating them by the central server. Thus, this paper proposes an efficient and effectiv",
    "link": "https://arxiv.org/abs/2403.12313",
    "context": "Title: Improving LoRA in Privacy-preserving Federated Learning\nAbstract: arXiv:2403.12313v1 Announce Type: new  Abstract: Low-rank adaptation (LoRA) is one of the most popular task-specific parameter-efficient fine-tuning (PEFT) methods on pre-trained language models for its good performance and computational efficiency. LoRA injects a product of two trainable rank decomposition matrices over the top of each frozen pre-trained model module. However, when applied in the setting of privacy-preserving federated learning (FL), LoRA may become unstable due to the following facts: 1) the effects of data heterogeneity and multi-step local updates are non-negligible, 2) additive noise enforced on updating gradients to guarantee differential privacy (DP) can be amplified and 3) the final performance is susceptible to hyper-parameters. A key factor leading to these phenomena is the discordance between jointly optimizing the two low-rank matrices by local clients and separately aggregating them by the central server. Thus, this paper proposes an efficient and effectiv",
    "path": "papers/24/03/2403.12313.json",
    "total_tokens": 893,
    "translated_title": "在隐私保护联邦学习中改进 LoRA",
    "translated_abstract": "低秩适应（LoRA）是最流行的面向任务特定参数高效微调（PEFT）方法之一，对于预训练语言模型具有良好的性能和计算效率。LoRA在每个冻结的预训练模型模块顶部注入了两个可训练秩分解矩阵的乘积。然而，在隐私保护联邦学习设置中应用时，LoRA可能由于以下事实而变得不稳定：1）数据异构性和多步本地更新的影响不可忽略，2）为了保证差分隐私（DP）而强制执行的添加噪声可能会被放大，3）最终性能容易受到超参数的影响。导致这些现象的关键因素之一是本地客户端联合优化两个低秩矩阵与中央服务器分别聚合它们之间的不一致。因此，本文提出了一种高效且有效的方法",
    "tldr": "该论文提出了一种在隐私保护联邦学习中改进LoRA方法，解决了在此设置下LoRA不稳定的问题，特别是由于数据异构性、差分隐私噪声放大和超参数的敏感性引起的挑战。"
}