{
    "title": "Posterior Uncertainty Quantification in Neural Networks using Data Augmentation",
    "abstract": "arXiv:2403.12729v1 Announce Type: cross  Abstract: In this paper, we approach the problem of uncertainty quantification in deep learning through a predictive framework, which captures uncertainty in model parameters by specifying our assumptions about the predictive distribution of unseen future data. Under this view, we show that deep ensembling (Lakshminarayanan et al., 2017) is a fundamentally mis-specified model class, since it assumes that future data are supported on existing observations only -- a situation rarely encountered in practice. To address this limitation, we propose MixupMP, a method that constructs a more realistic predictive distribution using popular data augmentation techniques. MixupMP operates as a drop-in replacement for deep ensembles, where each ensemble member is trained on a random simulation from this predictive distribution. Grounded in the recently-proposed framework of Martingale posteriors (Fong et al., 2023), MixupMP returns samples from an implicitly",
    "link": "https://arxiv.org/abs/2403.12729",
    "context": "Title: Posterior Uncertainty Quantification in Neural Networks using Data Augmentation\nAbstract: arXiv:2403.12729v1 Announce Type: cross  Abstract: In this paper, we approach the problem of uncertainty quantification in deep learning through a predictive framework, which captures uncertainty in model parameters by specifying our assumptions about the predictive distribution of unseen future data. Under this view, we show that deep ensembling (Lakshminarayanan et al., 2017) is a fundamentally mis-specified model class, since it assumes that future data are supported on existing observations only -- a situation rarely encountered in practice. To address this limitation, we propose MixupMP, a method that constructs a more realistic predictive distribution using popular data augmentation techniques. MixupMP operates as a drop-in replacement for deep ensembles, where each ensemble member is trained on a random simulation from this predictive distribution. Grounded in the recently-proposed framework of Martingale posteriors (Fong et al., 2023), MixupMP returns samples from an implicitly",
    "path": "papers/24/03/2403.12729.json",
    "total_tokens": 852,
    "translated_title": "使用数据增强在神经网络中对后验不确定性进行量化",
    "translated_abstract": "在这篇论文中，我们通过一个预测框架来处理深度学习中的不确定性量化问题，该框架通过指定有关未来未见数据的预测分布的假设来捕捉模型参数的不确定性。在这个观点下，我们展示了深度集成（Lakshminarayanan等，2017）是一个基本上错误规范化的模型类，因为它假设未来数据仅支持现有观察结果 -- 这种情况在实践中很少遇到。为了解决这个局限性，我们提出了MixupMP，一种使用流行的数据增强技术构建更现实的预测分布的方法。MixupMP作为深度集成的替代方案，其中每个集成成员都是在这个预测分布的随机模拟上训练的。基于最近提出的马丁格尔后验框架（Fong等，2023），MixupMP返回隐式样本。",
    "tldr": "通过提出MixupMP方法，使用数据增强构建更现实的预测分布，从而解决了在神经网络中对后验不确定性进行量化时的基本模型类错误规范化问题。",
    "en_tdlr": "By introducing the MixupMP method, which constructs a more realistic predictive distribution using data augmentation, this paper addresses the fundamental mis-specification issue in quantifying posterior uncertainty in neural networks."
}