{
    "title": "Multi-Label Adaptive Batch Selection by Highlighting Hard and Imbalanced Samples",
    "abstract": "arXiv:2403.18192v1 Announce Type: new  Abstract: Deep neural network models have demonstrated their effectiveness in classifying multi-label data from various domains. Typically, they employ a training mode that combines mini-batches with optimizers, where each sample is randomly selected with equal probability when constructing mini-batches. However, the intrinsic class imbalance in multi-label data may bias the model towards majority labels, since samples relevant to minority labels may be underrepresented in each mini-batch. Meanwhile, during the training process, we observe that instances associated with minority labels tend to induce greater losses. Existing heuristic batch selection methods, such as priority selection of samples with high contribution to the objective function, i.e., samples with high loss, have been proven to accelerate convergence while reducing the loss and test error in single-label data. However, batch selection methods have not yet been applied and validate",
    "link": "https://arxiv.org/abs/2403.18192",
    "context": "Title: Multi-Label Adaptive Batch Selection by Highlighting Hard and Imbalanced Samples\nAbstract: arXiv:2403.18192v1 Announce Type: new  Abstract: Deep neural network models have demonstrated their effectiveness in classifying multi-label data from various domains. Typically, they employ a training mode that combines mini-batches with optimizers, where each sample is randomly selected with equal probability when constructing mini-batches. However, the intrinsic class imbalance in multi-label data may bias the model towards majority labels, since samples relevant to minority labels may be underrepresented in each mini-batch. Meanwhile, during the training process, we observe that instances associated with minority labels tend to induce greater losses. Existing heuristic batch selection methods, such as priority selection of samples with high contribution to the objective function, i.e., samples with high loss, have been proven to accelerate convergence while reducing the loss and test error in single-label data. However, batch selection methods have not yet been applied and validate",
    "path": "papers/24/03/2403.18192.json",
    "total_tokens": 842,
    "translated_title": "通过突出困难和不平衡样本进行多标签自适应批次选择",
    "translated_abstract": "arXiv：2403.18192v1 公告类型：新 深度神经网络模型已经证明了它们在分类来自各个领域的多标签数据方面的有效性。通常，它们采用组合小批次和优化器的训练模式，在构建小批次时每个样本被随机选择，概率相等。然而，在多标签数据中固有的类别不平衡可能会使模型倾向于主导标签，因为与少数标签相关的样本可能在每个小批次中代表不足。同时，在训练过程中，我们观察到与少数标签相关的实例倾向于引起更大的损失。现有的启发式批次选择方法，例如选择对目标函数具有很高贡献度的样本，即损失很高的样本，已经被证明可以加快收敛速度，同时减少单标签数据中的损失和测试误差。然而，批次选择方法尚未被应用和验证。",
    "tldr": "多标签数据中困难和不平衡样本的突出，引入了一种自适应批次选择方法，以解决训练过程中的类别不平衡问题。",
    "en_tdlr": "Introducing an adaptive batch selection approach that highlights hard and imbalanced samples in multi-label data to address the issue of class imbalance during training."
}