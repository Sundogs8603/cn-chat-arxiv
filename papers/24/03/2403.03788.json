{
    "title": "PPTC-R benchmark: Towards Evaluating the Robustness of Large Language Models for PowerPoint Task Completion",
    "abstract": "arXiv:2403.03788v1 Announce Type: new  Abstract: The growing dependence on Large Language Models (LLMs) for finishing user instructions necessitates a comprehensive understanding of their robustness to complex task completion in real-world situations. To address this critical need, we propose the PowerPoint Task Completion Robustness benchmark (PPTC-R) to measure LLMs' robustness to the user PPT task instruction and software version. Specifically, we construct adversarial user instructions by attacking user instructions at sentence, semantic, and multi-language levels. To assess the robustness of Language Models to software versions, we vary the number of provided APIs to simulate both the newest version and earlier version settings. Subsequently, we test 3 closed-source and 4 open-source LLMs using a benchmark that incorporates these robustness settings, aiming to evaluate how deviations impact LLMs' API calls for task completion. We find that GPT-4 exhibits the highest performance an",
    "link": "https://arxiv.org/abs/2403.03788",
    "context": "Title: PPTC-R benchmark: Towards Evaluating the Robustness of Large Language Models for PowerPoint Task Completion\nAbstract: arXiv:2403.03788v1 Announce Type: new  Abstract: The growing dependence on Large Language Models (LLMs) for finishing user instructions necessitates a comprehensive understanding of their robustness to complex task completion in real-world situations. To address this critical need, we propose the PowerPoint Task Completion Robustness benchmark (PPTC-R) to measure LLMs' robustness to the user PPT task instruction and software version. Specifically, we construct adversarial user instructions by attacking user instructions at sentence, semantic, and multi-language levels. To assess the robustness of Language Models to software versions, we vary the number of provided APIs to simulate both the newest version and earlier version settings. Subsequently, we test 3 closed-source and 4 open-source LLMs using a benchmark that incorporates these robustness settings, aiming to evaluate how deviations impact LLMs' API calls for task completion. We find that GPT-4 exhibits the highest performance an",
    "path": "papers/24/03/2403.03788.json",
    "total_tokens": 879,
    "translated_title": "PPTC-R基准：评估大型语言模型在PowerPoint任务完成中的鲁棒性",
    "translated_abstract": "随着对大型语言模型（LLMs）在完成用户指令方面的日益依赖，有必要全面了解它们在现实世界复杂任务完成中的鲁棒性。为了解决这一关键需求，我们提出了PowerPoint任务完成鲁棒性基准（PPTC-R），以衡量LLMs对用户PPT任务指令和软件版本的鲁棒性。具体而言，我们通过在句子、语义和多语言级别攻击用户指令来构建对抗性用户指令。为了评估语言模型对软件版本的鲁棒性，我们改变提供的API数量以模拟最新版本和早期版本设置。随后，我们使用包含这些鲁棒性设置的基准测试3个闭源和4个开源LLMs，旨在评估偏离如何影响LLMs的API调用以完成任务。我们发现GPT-4表现最佳。",
    "tldr": "提出了PowerPoint任务完成鲁棒性基准（PPTC-R），旨在评估大型语言模型（LLMs）对用户PPT任务指令和软件版本的鲁棒性，发现GPT-4表现最佳。",
    "en_tdlr": "Introduced the PowerPoint Task Completion Robustness benchmark (PPTC-R) to evaluate the robustness of Large Language Models (LLMs) towards user PPT task instructions and software versions, revealing GPT-4's superior performance."
}