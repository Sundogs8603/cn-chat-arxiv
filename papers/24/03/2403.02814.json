{
    "title": "InjectTST: A Transformer Method of Injecting Global Information into Independent Channels for Long Time Series Forecasting",
    "abstract": "arXiv:2403.02814v1 Announce Type: cross  Abstract: Transformer has become one of the most popular architectures for multivariate time series (MTS) forecasting. Recent Transformer-based MTS models generally prefer channel-independent structures with the observation that channel independence can alleviate noise and distribution drift issues, leading to more robustness. Nevertheless, it is essential to note that channel dependency remains an inherent characteristic of MTS, carrying valuable information. Designing a model that incorporates merits of both channel-independent and channel-mixing structures is a key to further improvement of MTS forecasting, which poses a challenging conundrum. To address the problem, an injection method for global information into channel-independent Transformer, InjectTST, is proposed in this paper. Instead of designing a channel-mixing model directly, we retain the channel-independent backbone and gradually inject global information into individual channels",
    "link": "https://arxiv.org/abs/2403.02814",
    "context": "Title: InjectTST: A Transformer Method of Injecting Global Information into Independent Channels for Long Time Series Forecasting\nAbstract: arXiv:2403.02814v1 Announce Type: cross  Abstract: Transformer has become one of the most popular architectures for multivariate time series (MTS) forecasting. Recent Transformer-based MTS models generally prefer channel-independent structures with the observation that channel independence can alleviate noise and distribution drift issues, leading to more robustness. Nevertheless, it is essential to note that channel dependency remains an inherent characteristic of MTS, carrying valuable information. Designing a model that incorporates merits of both channel-independent and channel-mixing structures is a key to further improvement of MTS forecasting, which poses a challenging conundrum. To address the problem, an injection method for global information into channel-independent Transformer, InjectTST, is proposed in this paper. Instead of designing a channel-mixing model directly, we retain the channel-independent backbone and gradually inject global information into individual channels",
    "path": "papers/24/03/2403.02814.json",
    "total_tokens": 820,
    "translated_title": "InjectTST: 将全局信息注入独立通道的变压器方法用于长时间序列预测",
    "translated_abstract": "变压器已成为多变量时间序列（MTS）预测中最流行的架构之一。最近基于Transformer的MTS模型通常倾向于具有通道独立结构，因为通道独立可以减轻噪声和分布漂移问题，从而更具鲁棒性。然而，值得注意的是，通道依赖性仍然是MTS的固有特性，蕴含着宝贵的信息。设计一个结合了通道独立和通道混合结构优点的模型是进一步改进MTS预测的关键，这带来了一个具有挑战性的难题。为了解决这个问题，在本文中提出了一种将全局信息注入通道无关Transformer的注入方法InjectTST。我们没有直接设计一个混合通道模型，而是保留了通道独立的框架，并逐渐将全局信息注入到单个通道中。",
    "tldr": "提出了InjectTST这种将全局信息注入独立通道的变压器方法，用于改进多变量时间序列（MTS）预测性能。",
    "en_tdlr": "Proposed InjectTST, a transformer method that injects global information into independent channels for enhanced multivariate time series (MTS) forecasting performance."
}