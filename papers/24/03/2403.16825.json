{
    "title": "Weak Convergence Analysis of Online Neural Actor-Critic Algorithms",
    "abstract": "arXiv:2403.16825v1 Announce Type: new  Abstract: We prove that a single-layer neural network trained with the online actor critic algorithm converges in distribution to a random ordinary differential equation (ODE) as the number of hidden units and the number of training steps $\\rightarrow \\infty$. In the online actor-critic algorithm, the distribution of the data samples dynamically changes as the model is updated, which is a key challenge for any convergence analysis. We establish the geometric ergodicity of the data samples under a fixed actor policy. Then, using a Poisson equation, we prove that the fluctuations of the model updates around the limit distribution due to the randomly-arriving data samples vanish as the number of parameter updates $\\rightarrow \\infty$. Using the Poisson equation and weak convergence techniques, we prove that the actor neural network and critic neural network converge to the solutions of a system of ODEs with random initial conditions. Analysis of the ",
    "link": "https://arxiv.org/abs/2403.16825",
    "context": "Title: Weak Convergence Analysis of Online Neural Actor-Critic Algorithms\nAbstract: arXiv:2403.16825v1 Announce Type: new  Abstract: We prove that a single-layer neural network trained with the online actor critic algorithm converges in distribution to a random ordinary differential equation (ODE) as the number of hidden units and the number of training steps $\\rightarrow \\infty$. In the online actor-critic algorithm, the distribution of the data samples dynamically changes as the model is updated, which is a key challenge for any convergence analysis. We establish the geometric ergodicity of the data samples under a fixed actor policy. Then, using a Poisson equation, we prove that the fluctuations of the model updates around the limit distribution due to the randomly-arriving data samples vanish as the number of parameter updates $\\rightarrow \\infty$. Using the Poisson equation and weak convergence techniques, we prove that the actor neural network and critic neural network converge to the solutions of a system of ODEs with random initial conditions. Analysis of the ",
    "path": "papers/24/03/2403.16825.json",
    "total_tokens": 924,
    "translated_title": "在线神经演员-评论算法的弱收敛分析",
    "translated_abstract": "我们证明，使用在线演员评论算法训练的单层神经网络在隐藏单元和训练步数的数量$\\rightarrow \\infty$时，收敛于一个随机常微分方程（ODE）。在线演员评论算法中，随着模型的更新，数据样本的分布会动态变化，这对于任何收敛分析来说都是一个关键挑战。我们在固定演员策略下建立了数据样本的几何遍历性。然后，使用泊松方程，我们证明由于随机到达的数据样本带来的模型更新波动会随着参数更新次数的增加$\\rightarrow \\infty$而消失。利用泊松方程和弱收敛技术，我们证明演员神经网络和评论神经网络收敛到具有随机初始条件的ODE系统的解。",
    "tldr": "在线神经演员-评论算法中，我们证明当隐藏单元和训练步数的数量$\\rightarrow \\infty$时，单层神经网络将收敛于随机ODE，通过建立数据样本的几何遍历性和使用泊松方程证明模型更新波动消失，演员神经网络和评论神经网络收敛到具有随机初始条件的ODE系统的解。",
    "en_tdlr": "In the online neural actor-critic algorithms, it is demonstrated that a single-layer neural network converges to a random ODE as the number of hidden units and training steps tend to infinity. The geometric ergodicity of data samples is established, and using a Poisson equation, it is proven that fluctuations in model updates vanish with increasing parameter updates. The actor and critic neural networks converge to solutions of a system of ODEs with random initial conditions."
}