{
    "title": "Visual Hallucination: Definition, Quantification, and Prescriptive Remediations",
    "abstract": "arXiv:2403.17306v1 Announce Type: new  Abstract: The troubling rise of hallucination presents perhaps the most significant impediment to the advancement of responsible AI. In recent times, considerable research has focused on detecting and mitigating hallucination in Large Language Models (LLMs). However, it's worth noting that hallucination is also quite prevalent in Vision-Language models (VLMs). In this paper, we offer a fine-grained discourse on profiling VLM hallucination based on two tasks: i) image captioning, and ii) Visual Question Answering (VQA). We delineate eight fine-grained orientations of visual hallucination: i) Contextual Guessing, ii) Identity Incongruity, iii) Geographical Erratum, iv) Visual Illusion, v) Gender Anomaly, vi) VLM as Classifier, vii) Wrong Reading, and viii) Numeric Discrepancy. We curate Visual HallucInation eLiciTation (VHILT), a publicly available dataset comprising 2,000 samples generated using eight VLMs across two tasks of captioning and VQA alo",
    "link": "https://arxiv.org/abs/2403.17306",
    "context": "Title: Visual Hallucination: Definition, Quantification, and Prescriptive Remediations\nAbstract: arXiv:2403.17306v1 Announce Type: new  Abstract: The troubling rise of hallucination presents perhaps the most significant impediment to the advancement of responsible AI. In recent times, considerable research has focused on detecting and mitigating hallucination in Large Language Models (LLMs). However, it's worth noting that hallucination is also quite prevalent in Vision-Language models (VLMs). In this paper, we offer a fine-grained discourse on profiling VLM hallucination based on two tasks: i) image captioning, and ii) Visual Question Answering (VQA). We delineate eight fine-grained orientations of visual hallucination: i) Contextual Guessing, ii) Identity Incongruity, iii) Geographical Erratum, iv) Visual Illusion, v) Gender Anomaly, vi) VLM as Classifier, vii) Wrong Reading, and viii) Numeric Discrepancy. We curate Visual HallucInation eLiciTation (VHILT), a publicly available dataset comprising 2,000 samples generated using eight VLMs across two tasks of captioning and VQA alo",
    "path": "papers/24/03/2403.17306.json",
    "total_tokens": 930,
    "translated_title": "视觉幻觉：定义、量化和处方修复",
    "translated_abstract": "引发幻觉的不断增加，或许是对负责任人工智能进展的最显著障碍。最近，相当多的研究侧重于检测和缓解大型语言模型（LLMs）中的幻觉。然而，值得注意的是，视觉-语言模型（VLMs）中幻觉也相当普遍。在本文中，我们提供了一个关于基于图像字幕和视觉问答两个任务的VLM幻觉剖析的细致讨论。我们阐明了八个细致的视觉幻觉取向：i) 上下文猜测，ii) 身份不一致，iii) 地理错误，iv) 视觉幻觉，v) 性别异常，vi) VLM作为分类器，vii) 错误阅读，和viii) 数字差异。我们策划了一份名为视觉幻觉诱发（VHILT）的公开数据集，包含了通过两个任务（字幕和VQA）生成的来自八个VLM的2,000个样本。",
    "tldr": "本文提出了关于视觉-语言模型中幻觉的细致讨论，对幻觉进行了量化，并提供了一个新的公开数据集VHILT，有助于研究此问题。"
}