{
    "title": "AI and Memory Wall",
    "abstract": "arXiv:2403.14123v1 Announce Type: new  Abstract: The availability of unprecedented unsupervised training data, along with neural scaling laws, has resulted in an unprecedented surge in model size and compute requirements for serving/training LLMs. However, the main performance bottleneck is increasingly shifting to memory bandwidth. Over the past 20 years, peak server hardware FLOPS has been scaling at 3.0x/2yrs, outpacing the growth of DRAM and interconnect bandwidth, which have only scaled at 1.6 and 1.4 times every 2 years, respectively. This disparity has made memory, rather than compute, the primary bottleneck in AI applications, particularly in serving. Here, we analyze encoder and decoder Transformer models and show how memory bandwidth can become the dominant bottleneck for decoder models. We argue for a redesign in model architecture, training, and deployment strategies to overcome this memory limitation.",
    "link": "https://arxiv.org/abs/2403.14123",
    "context": "Title: AI and Memory Wall\nAbstract: arXiv:2403.14123v1 Announce Type: new  Abstract: The availability of unprecedented unsupervised training data, along with neural scaling laws, has resulted in an unprecedented surge in model size and compute requirements for serving/training LLMs. However, the main performance bottleneck is increasingly shifting to memory bandwidth. Over the past 20 years, peak server hardware FLOPS has been scaling at 3.0x/2yrs, outpacing the growth of DRAM and interconnect bandwidth, which have only scaled at 1.6 and 1.4 times every 2 years, respectively. This disparity has made memory, rather than compute, the primary bottleneck in AI applications, particularly in serving. Here, we analyze encoder and decoder Transformer models and show how memory bandwidth can become the dominant bottleneck for decoder models. We argue for a redesign in model architecture, training, and deployment strategies to overcome this memory limitation.",
    "path": "papers/24/03/2403.14123.json",
    "total_tokens": 815,
    "translated_title": "AI与内存墙",
    "translated_abstract": "出现了前所未有的无监督训练数据可用性，加上神经缩放定律，导致用于服务/训练LLMs的模型大小和计算需求出现了前所未有的激增。然而，主要性能瓶颈日益转向内存带宽。在过去的20年中，服务器硬件FLOPS的峰值每2年增长3.0倍，超过了DRAM和互连带宽的增长，它们分别仅每2年增长1.6倍和1.4倍。这种不平衡使得内存，而非计算，成为AI应用中的主要瓶颈，特别是在服务方面。在这里，我们分析了编码器和解码器Transformer模型，并展示了内存带宽如何成为解码器模型的主要瓶颈。我们主张对模型架构、训练和部署策略进行重新设计，以克服这一内存限制。",
    "tldr": "这项研究分析了AI应用中内存带宽成为主要瓶颈的问题，并提出了对模型架构、训练和部署策略进行重新设计的观点。"
}