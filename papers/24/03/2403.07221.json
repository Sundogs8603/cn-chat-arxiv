{
    "title": "LookupFFN: Making Transformers Compute-lite for CPU inference",
    "abstract": "arXiv:2403.07221v1 Announce Type: new  Abstract: While GPU clusters are the de facto choice for training large deep neural network (DNN) models today, several reasons including ease of workflow, security and cost have led to efforts investigating whether CPUs may be viable for inference in routine use in many sectors of the industry. But the imbalance between the compute capabilities of GPUs and CPUs is huge. Motivated by these considerations, we study a module which is a workhorse within modern DNN architectures, GEMM based Feed Forward Networks (FFNs), and assess the extent to which it can be made compute- (or FLOP-) lite. Specifically, we propose an alternative formulation (we call it LookupFFN) to GEMM based FFNs inspired by the recent studies of using Locality Sensitive Hashing (LSH) to approximate FFNs. Our formulation recasts most essential operations as a memory look-up, leveraging the trade-off between the two resources on any platform: compute and memory (since CPUs offer it ",
    "link": "https://arxiv.org/abs/2403.07221",
    "context": "Title: LookupFFN: Making Transformers Compute-lite for CPU inference\nAbstract: arXiv:2403.07221v1 Announce Type: new  Abstract: While GPU clusters are the de facto choice for training large deep neural network (DNN) models today, several reasons including ease of workflow, security and cost have led to efforts investigating whether CPUs may be viable for inference in routine use in many sectors of the industry. But the imbalance between the compute capabilities of GPUs and CPUs is huge. Motivated by these considerations, we study a module which is a workhorse within modern DNN architectures, GEMM based Feed Forward Networks (FFNs), and assess the extent to which it can be made compute- (or FLOP-) lite. Specifically, we propose an alternative formulation (we call it LookupFFN) to GEMM based FFNs inspired by the recent studies of using Locality Sensitive Hashing (LSH) to approximate FFNs. Our formulation recasts most essential operations as a memory look-up, leveraging the trade-off between the two resources on any platform: compute and memory (since CPUs offer it ",
    "path": "papers/24/03/2403.07221.json",
    "total_tokens": 880,
    "translated_title": "LookupFFN: 让变压器在CPU推断中更轻巧",
    "translated_abstract": "虽然GPU集群如今是训练大型深度神经网络（DNN）模型的首选，但出于诸多原因，包括工作流程的便利性、安全性和成本，一些努力正在探讨CPU在行业的许多领域中是否可以成为推断的可行选择。受这些考虑的启发，我们研究了现代DNN架构中的一个工作模块，基于GEMM的前馈网络（FFNs），并评估它可以被制作为计算轻量（或FLOP轻量）的程度。具体来说，我们提出了一种替代公式（我们称之为LookupFFN），用于取代受最近关于使用局部敏感哈希（LSH）近似FFNs的研究启发的GEMM基础的FFNs。我们的公式将大部分基本操作重新构建为内存查找，利用了任何平台上两种资源之间的权衡：计算和内存（因为CPU提供了这种权衡）。",
    "tldr": "提出了一种名为LookupFFN的替代模块，通过将关键操作重新构建为内存查找，使得基于变压器的前馈网络（FFNs）在CPU推断中变得更轻巧",
    "en_tdlr": "Introduced an alternative module called LookupFFN that makes Transformers-based Feed Forward Networks (FFNs) more lightweight for CPU inference by recasting key operations as memory look-ups."
}