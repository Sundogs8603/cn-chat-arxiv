{
    "title": "VCR-Graphormer: A Mini-batch Graph Transformer via Virtual Connections",
    "abstract": "arXiv:2403.16030v1 Announce Type: new  Abstract: Graph transformer has been proven as an effective graph learning method for its adoption of attention mechanism that is capable of capturing expressive representations from complex topological and feature information of graphs. Graph transformer conventionally performs dense attention (or global attention) for every pair of nodes to learn node representation vectors, resulting in quadratic computational costs that are unaffordable for large-scale graph data. Therefore, mini-batch training for graph transformers is a promising direction, but limited samples in each mini-batch can not support effective dense attention to encode informative representations. Facing this bottleneck, (1) we start by assigning each node a token list that is sampled by personalized PageRank (PPR) and then apply standard multi-head self-attention only on this list to compute its node representations. This PPR tokenization method decouples model training from comp",
    "link": "https://arxiv.org/abs/2403.16030",
    "context": "Title: VCR-Graphormer: A Mini-batch Graph Transformer via Virtual Connections\nAbstract: arXiv:2403.16030v1 Announce Type: new  Abstract: Graph transformer has been proven as an effective graph learning method for its adoption of attention mechanism that is capable of capturing expressive representations from complex topological and feature information of graphs. Graph transformer conventionally performs dense attention (or global attention) for every pair of nodes to learn node representation vectors, resulting in quadratic computational costs that are unaffordable for large-scale graph data. Therefore, mini-batch training for graph transformers is a promising direction, but limited samples in each mini-batch can not support effective dense attention to encode informative representations. Facing this bottleneck, (1) we start by assigning each node a token list that is sampled by personalized PageRank (PPR) and then apply standard multi-head self-attention only on this list to compute its node representations. This PPR tokenization method decouples model training from comp",
    "path": "papers/24/03/2403.16030.json",
    "total_tokens": 754,
    "translated_title": "VCR-Graphormer：通过虚拟连接实现的小批量图变换器",
    "translated_abstract": "图变换器通过采用能够捕获复杂拓扑和特征信息的表达性表示的注意力机制，被证明是一种有效的图学习方法。图变换器传统上对每对节点执行密集注意力（或全局注意力）来学习节点表示向量，导致二次计算成本对于大规模图数据是无法承受的。因此，图变换器的小批量训练是一个有前途的方向，但每个小批量中的有限样本无法支持有效的密集注意力以编码信息丰富的表示。",
    "tldr": "通过为每个节点分配由个性化PageRank（PPR）采样的令牌列表，然后仅在此列表上应用标准多头自注意力来计算其节点表示，来解决图变换器小批量训练中的样本限制问题。",
    "en_tdlr": "The paper proposes a solution to the limited samples issue in mini-batch training for graph transformers by assigning each node a token list sampled by personalized PageRank (PPR) and applying standard multi-head self-attention only on this list to compute node representations."
}