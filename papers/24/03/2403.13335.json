{
    "title": "Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated Text Detection",
    "abstract": "arXiv:2403.13335v1 Announce Type: new  Abstract: Large language models (LLMs) have reached human-like proficiency in generating diverse textual content, underscoring the necessity for effective fake text detection to avoid potential risks such as fake news in social media. Previous research has mostly tested single models on in-distribution datasets, limiting our understanding of how these models perform on different types of data for LLM-generated text detection task. We researched this by testing five specialized transformer-based models on both in-distribution and out-of-distribution datasets to better assess their performance and generalizability. Our results revealed that single transformer-based classifiers achieved decent performance on in-distribution dataset but limited generalization ability on out-of-distribution dataset. To improve it, we combined the individual classifiers models using adaptive ensemble algorithms, which improved the average accuracy significantly from 91.",
    "link": "https://arxiv.org/abs/2403.13335",
    "context": "Title: Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated Text Detection\nAbstract: arXiv:2403.13335v1 Announce Type: new  Abstract: Large language models (LLMs) have reached human-like proficiency in generating diverse textual content, underscoring the necessity for effective fake text detection to avoid potential risks such as fake news in social media. Previous research has mostly tested single models on in-distribution datasets, limiting our understanding of how these models perform on different types of data for LLM-generated text detection task. We researched this by testing five specialized transformer-based models on both in-distribution and out-of-distribution datasets to better assess their performance and generalizability. Our results revealed that single transformer-based classifiers achieved decent performance on in-distribution dataset but limited generalization ability on out-of-distribution dataset. To improve it, we combined the individual classifiers models using adaptive ensemble algorithms, which improved the average accuracy significantly from 91.",
    "path": "papers/24/03/2403.13335.json",
    "total_tokens": 832,
    "translated_title": "自适应细调Transformer集成用于LLM生成文本检测",
    "translated_abstract": "大型语言模型（LLMs）已经达到了生成各种文本内容的人类水平，凸显了有效检测虚假文本的必要性，以避免潜在风险，如社交媒体中的假新闻。在LLM生成文本检测任务中，先前的研究主要在分布数据集上测试单一模型，限制了我们对这些模型在不同类型数据上表现的理解。我们通过测试五个专门的基于Transformer的模型在分布和非分布数据集上，以更好地评估它们的性能和泛化能力。我们的结果表明，单一基于Transformer的分类器在分布数据集上表现良好，但在非分布数据集上的泛化能力有限。为了改善这一点，我们使用自适应集成算法结合了个体分类器模型，显著提高了平均准确性。",
    "tldr": "通过测试专门的基于Transformer的模型在分布和非分布数据集上，我们发现单一分类器在分布数据集上表现良好，但泛化能力有限，因此提出了使用自适应集成算法来改善性能。",
    "en_tdlr": "By testing specialized transformer-based models on both in-distribution and out-of-distribution datasets, we found that single classifiers performed well on in-distribution data but had limited generalization ability, leading us to propose using adaptive ensemble algorithms for performance improvement."
}