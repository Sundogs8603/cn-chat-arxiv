{
    "title": "A Simple Finite-Time Analysis of TD Learning with Linear Function Approximation",
    "abstract": "arXiv:2403.02476v1 Announce Type: new  Abstract: We study the finite-time convergence of TD learning with linear function approximation under Markovian sampling. Existing proofs for this setting either assume a projection step in the algorithm to simplify the analysis, or require a fairly intricate argument to ensure stability of the iterates. We ask: \\textit{Is it possible to retain the simplicity of a projection-based analysis without actually performing a projection step in the algorithm?} Our main contribution is to show this is possible via a novel two-step argument. In the first step, we use induction to prove that under a standard choice of a constant step-size $\\alpha$, the iterates generated by TD learning remain uniformly bounded in expectation. In the second step, we establish a recursion that mimics the steady-state dynamics of TD learning up to a bounded perturbation on the order of $O(\\alpha^2)$ that captures the effect of Markovian sampling. Combining these pieces leads ",
    "link": "https://arxiv.org/abs/2403.02476",
    "context": "Title: A Simple Finite-Time Analysis of TD Learning with Linear Function Approximation\nAbstract: arXiv:2403.02476v1 Announce Type: new  Abstract: We study the finite-time convergence of TD learning with linear function approximation under Markovian sampling. Existing proofs for this setting either assume a projection step in the algorithm to simplify the analysis, or require a fairly intricate argument to ensure stability of the iterates. We ask: \\textit{Is it possible to retain the simplicity of a projection-based analysis without actually performing a projection step in the algorithm?} Our main contribution is to show this is possible via a novel two-step argument. In the first step, we use induction to prove that under a standard choice of a constant step-size $\\alpha$, the iterates generated by TD learning remain uniformly bounded in expectation. In the second step, we establish a recursion that mimics the steady-state dynamics of TD learning up to a bounded perturbation on the order of $O(\\alpha^2)$ that captures the effect of Markovian sampling. Combining these pieces leads ",
    "path": "papers/24/03/2403.02476.json",
    "total_tokens": 834,
    "translated_title": "使用线性函数逼近的TD学习的简单有限时间分析",
    "translated_abstract": "我们研究了在马尔可夫采样下使用线性函数逼近的TD学习的有限时间收敛性。此设置下现有的证明要么假定算法中存在投影步骤以简化分析，要么需要一个相当复杂的论证来确保迭代的稳定性。我们提出：\\textit{在不实际执行投影步骤的情况下保留投影基础分析的简单性是否可能？}我们的主要贡献是通过一个新颖的两步论证来展示这是可能的。在第一步中，我们使用归纳证明，在标准选择常量步长$\\alpha$下，由TD学习生成的迭代保持期望上的一致有界性。在第二步中，我们建立了一个递归，模拟了TD学习的稳态动态，受马尔可夫采样效果的$O(\\alpha^2)$数量级上的有界摄动影响。",
    "tldr": "通过一个新颖的两步论证，我们展示了在不实际执行投影步骤的情况下保留投影基础分析的简单性是可能的。",
    "en_tdlr": "We show that it is possible to retain the simplicity of a projection-based analysis without actually performing a projection step in the algorithm through a novel two-step argument."
}