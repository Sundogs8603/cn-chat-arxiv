{
    "title": "InstUPR : Instruction-based Unsupervised Passage Reranking with Large Language Models",
    "abstract": "arXiv:2403.16435v1 Announce Type: new  Abstract: This paper introduces InstUPR, an unsupervised passage reranking method based on large language models (LLMs). Different from existing approaches that rely on extensive training with query-document pairs or retrieval-specific instructions, our method leverages the instruction-following capabilities of instruction-tuned LLMs for passage reranking without any additional fine-tuning. To achieve this, we introduce a soft score aggregation technique and employ pairwise reranking for unsupervised passage reranking. Experiments on the BEIR benchmark demonstrate that InstUPR outperforms unsupervised baselines as well as an instruction-tuned reranker, highlighting its effectiveness and superiority. Source code to reproduce all experiments is open-sourced at https://github.com/MiuLab/InstUPR",
    "link": "https://arxiv.org/abs/2403.16435",
    "context": "Title: InstUPR : Instruction-based Unsupervised Passage Reranking with Large Language Models\nAbstract: arXiv:2403.16435v1 Announce Type: new  Abstract: This paper introduces InstUPR, an unsupervised passage reranking method based on large language models (LLMs). Different from existing approaches that rely on extensive training with query-document pairs or retrieval-specific instructions, our method leverages the instruction-following capabilities of instruction-tuned LLMs for passage reranking without any additional fine-tuning. To achieve this, we introduce a soft score aggregation technique and employ pairwise reranking for unsupervised passage reranking. Experiments on the BEIR benchmark demonstrate that InstUPR outperforms unsupervised baselines as well as an instruction-tuned reranker, highlighting its effectiveness and superiority. Source code to reproduce all experiments is open-sourced at https://github.com/MiuLab/InstUPR",
    "path": "papers/24/03/2403.16435.json",
    "total_tokens": 836,
    "translated_title": "基于大型语言模型的基于指令的无监督段落重新排序方法InstUPR",
    "translated_abstract": "本文介绍了InstUPR，一种基于大型语言模型（LLMs）的无监督段落重新排序方法。与现有依赖于query-document对进行大量训练或特定于检索的指令的方法不同，我们的方法利用了经过指令调整的LLMs的按照指令进行操作的能力来进行段落重新排序，而无需任何额外的微调。为实现这一目标，我们引入了一种软得分聚合技术，并采用了成对重新排序的无监督段落重新排序。在BEIR基准测试上的实验表明，InstUPR优于无监督基线以及一个经过指令调整的重新排序器，突显了其有效性和优越性。复现所有实验的源代码已在https://github.com/MiuLab/InstUPR 开源。",
    "tldr": "InstUPR是一种基于大型语言模型的无监督段落重新排序方法，利用了LLMs的指令跟踪能力，无需额外微调，通过软得分聚合技术和成对重新排序，在BEIR基准测试中表现优秀。",
    "en_tdlr": "InstUPR is an unsupervised passage reranking method based on large language models (LLMs) that leverages the instruction-following capabilities of LLMs without additional fine-tuning. It outperforms unsupervised baselines and an instruction-tuned reranker on the BEIR benchmark through soft score aggregation and pairwise reranking."
}