{
    "title": "Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring",
    "abstract": "arXiv:2403.09333v1 Announce Type: cross  Abstract: Large Vision Language Models have achieved fine-grained object perception, but the limitation of image resolution remains a significant obstacle to surpass the performance of task-specific experts in complex and dense scenarios. Such limitation further restricts the model's potential to achieve nuanced visual and language referring in domains such as GUI Agents, Counting and \\etc. To address this issue, we introduce a unified high-resolution generalist model, Griffon v2, enabling flexible object referring with visual and textual prompts. To efficiently scaling up image resolution, we design a simple and lightweight down-sampling projector to overcome the input tokens constraint in Large Language Models. This design inherently preserves the complete contexts and fine details, and significantly improves multimodal perception ability especially for small objects. Building upon this, we further equip the model with visual-language co-refer",
    "link": "https://arxiv.org/abs/2403.09333",
    "context": "Title: Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring\nAbstract: arXiv:2403.09333v1 Announce Type: cross  Abstract: Large Vision Language Models have achieved fine-grained object perception, but the limitation of image resolution remains a significant obstacle to surpass the performance of task-specific experts in complex and dense scenarios. Such limitation further restricts the model's potential to achieve nuanced visual and language referring in domains such as GUI Agents, Counting and \\etc. To address this issue, we introduce a unified high-resolution generalist model, Griffon v2, enabling flexible object referring with visual and textual prompts. To efficiently scaling up image resolution, we design a simple and lightweight down-sampling projector to overcome the input tokens constraint in Large Language Models. This design inherently preserves the complete contexts and fine details, and significantly improves multimodal perception ability especially for small objects. Building upon this, we further equip the model with visual-language co-refer",
    "path": "papers/24/03/2403.09333.json",
    "total_tokens": 784,
    "translated_title": "Griffon v2：通过高分辨率缩放和视觉-语言共指推进多模态感知",
    "translated_abstract": "大型视觉语言模型已经实现了细粒度对象感知，但图像分辨率的限制仍然是超越复杂和密集场景中特定任务专家表现的重要障碍。为了解决这一问题，我们引入了一个统一的高分辨率通用模型，Griffon v2，实现了具有视觉和文本提示的灵活对象引用。为了有效提高图像分辨率，我们设计了一个简单轻量级的下采样投影仪，以克服大型语言模型中输入标记的约束。这种设计固有地保留了完整的上下文和细节，并显著提高了多模态感知能力，特别是对于小对象。在此基础上，我们进一步为模型配置了视觉-语言共指。",
    "tldr": "Griffon v2通过引入高分辨率缩放和视觉-语言共指，提升了多模态感知能力，尤其是对于小对象的感知能力。"
}