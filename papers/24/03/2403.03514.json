{
    "title": "CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models",
    "abstract": "arXiv:2403.03514v1 Announce Type: new  Abstract: Developing Large Language Models (LLMs) with robust long-context capabilities has been the recent research focus, resulting in the emergence of long-context LLMs proficient in Chinese. However, the evaluation of these models remains underdeveloped due to a lack of benchmarks. To address this gap, we present CLongEval, a comprehensive Chinese benchmark for evaluating long-context LLMs. CLongEval is characterized by three key features: (1) Sufficient data volume, comprising 7 distinct tasks and 7,267 examples; (2) Broad applicability, accommodating to models with context windows size from 1K to 100K; (3) High quality, with over 2,000 manually annotated question-answer pairs in addition to the automatically constructed labels. With CLongEval, we undertake a comprehensive assessment of 6 open-source long-context LLMs and 2 leading commercial counterparts that feature both long-context abilities and proficiency in Chinese. We also provide in-",
    "link": "https://arxiv.org/abs/2403.03514",
    "context": "Title: CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models\nAbstract: arXiv:2403.03514v1 Announce Type: new  Abstract: Developing Large Language Models (LLMs) with robust long-context capabilities has been the recent research focus, resulting in the emergence of long-context LLMs proficient in Chinese. However, the evaluation of these models remains underdeveloped due to a lack of benchmarks. To address this gap, we present CLongEval, a comprehensive Chinese benchmark for evaluating long-context LLMs. CLongEval is characterized by three key features: (1) Sufficient data volume, comprising 7 distinct tasks and 7,267 examples; (2) Broad applicability, accommodating to models with context windows size from 1K to 100K; (3) High quality, with over 2,000 manually annotated question-answer pairs in addition to the automatically constructed labels. With CLongEval, we undertake a comprehensive assessment of 6 open-source long-context LLMs and 2 leading commercial counterparts that feature both long-context abilities and proficiency in Chinese. We also provide in-",
    "path": "papers/24/03/2403.03514.json",
    "total_tokens": 922,
    "translated_title": "CLongEval: 用于评估长上下文大语言模型的中文基准",
    "translated_abstract": "arXiv:2403.03514v1 公告类型: 新的 摘要: 开发具有强大长上下文能力的大型语言模型(LLMs)一直是最近的研究重点，导致长上下文中文能力娴熟的LLMs的出现。然而，由于缺乏基准测试，这些模型的评估仍然不够完善。为填补这一空白，我们提出CLongEval，一个用于评估长上下文LLMs的全面中文基准。CLongEval具有三个关键特征：(1)足够的数据量，包括7个不同的任务和7,267个示例；(2)广泛的适用性，适用于上下文窗口大小从1K到100K的模型；(3)高质量，除了自动构建的标签外，还有超过2,000个手工注释的问答对。借助CLongEval，我们对6个开源长上下文LLMs和2个具有长上下文能力和中文熟练度的领先商业竞争对手进行了全面评估。",
    "tldr": "CLongEval是一个用于评估长上下文大语言模型的全面中文基准，具有足够的数据量、广泛的适用性和高质量，可以对多个开源和商业模型进行全面评估。",
    "en_tdlr": "CLongEval is a comprehensive Chinese benchmark for evaluating long-context large language models, featuring sufficient data volume, broad applicability, and high quality for comprehensive assessment of multiple open-source and commercial models."
}