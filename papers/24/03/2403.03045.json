{
    "title": "Adding Multimodal Capabilities to a Text-only Translation Model",
    "abstract": "arXiv:2403.03045v1 Announce Type: new  Abstract: While most current work in multimodal machine translation (MMT) uses the Multi30k dataset for training and evaluation, we find that the resulting models overfit to the Multi30k dataset to an extreme degree. Consequently, these models perform very badly when evaluated against typical text-only testing sets such as the WMT newstest datasets. In order to perform well on both Multi30k and typical text-only datasets, we use a performant text-only machine translation (MT) model as the starting point of our MMT model. We add vision-text adapter layers connected via gating mechanisms to the MT model, and incrementally transform the MT model into an MMT model by 1) pre-training using vision-based masking of the source text and 2) fine-tuning on Multi30k.",
    "link": "https://arxiv.org/abs/2403.03045",
    "context": "Title: Adding Multimodal Capabilities to a Text-only Translation Model\nAbstract: arXiv:2403.03045v1 Announce Type: new  Abstract: While most current work in multimodal machine translation (MMT) uses the Multi30k dataset for training and evaluation, we find that the resulting models overfit to the Multi30k dataset to an extreme degree. Consequently, these models perform very badly when evaluated against typical text-only testing sets such as the WMT newstest datasets. In order to perform well on both Multi30k and typical text-only datasets, we use a performant text-only machine translation (MT) model as the starting point of our MMT model. We add vision-text adapter layers connected via gating mechanisms to the MT model, and incrementally transform the MT model into an MMT model by 1) pre-training using vision-based masking of the source text and 2) fine-tuning on Multi30k.",
    "path": "papers/24/03/2403.03045.json",
    "total_tokens": 804,
    "translated_title": "将多模态功能添加到仅文本翻译模型中",
    "translated_abstract": "大多数当前的多模态机器翻译（MMT）工作在训练和评估时使用Multi30k数据集，然而我们发现导致的模型严重过拟合Multi30k数据集。因此，这些模型在典型的仅文本测试集（如WMT新闻测试数据集）上表现非常糟糕。为了在Multi30k和典型的仅文本数据集上表现良好，我们使用一个性能优异的文本机器翻译（MT）模型作为我们MMT模型的起点。我们通过连接门控机制的视觉-文本适配器层将MT模型逐步转化为MMT模型，方法是：1）使用基于视觉的源文本掩蔽进行预训练，2）在Multi30k上进行微调。",
    "tldr": "将性能优异的文本机器翻译模型转化为多模态机器翻译模型，通过连接视觉-文本适配器层并利用门控机制，在Multi30k数据集和典型的仅文本数据集上取得良好效果。",
    "en_tdlr": "Transforming a high-performing text machine translation model into a multimodal machine translation model by adding vision-text adapter layers connected via gating mechanisms, achieving good performance on both Multi30k dataset and typical text-only datasets."
}