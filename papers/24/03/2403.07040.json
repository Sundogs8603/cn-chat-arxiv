{
    "title": "All in One: Multi-Task Prompting for Graph Neural Networks (Extended Abstract)",
    "abstract": "arXiv:2403.07040v1 Announce Type: cross  Abstract: This paper is an extended abstract of our original work published in KDD23, where we won the best research paper award (Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, and Jihong Guan. All in one: Multi-task prompting for graph neural networks. KDD 23) The paper introduces a novel approach to bridging the gap between pre-trained graph models and the diverse tasks they're applied to, inspired by the success of prompt learning in NLP. Recognizing the challenge of aligning pre-trained models with varied graph tasks (node level, edge level, and graph level), which can lead to negative transfer and poor performance, we propose a multi-task prompting method for graphs. This method involves unifying graph and language prompt formats, enabling NLP's prompting strategies to be adapted for graph tasks. By analyzing the task space of graph applications, we reformulate problems to fit graph-level tasks and apply meta-learning to improve prompt initializ",
    "link": "https://arxiv.org/abs/2403.07040",
    "context": "Title: All in One: Multi-Task Prompting for Graph Neural Networks (Extended Abstract)\nAbstract: arXiv:2403.07040v1 Announce Type: cross  Abstract: This paper is an extended abstract of our original work published in KDD23, where we won the best research paper award (Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, and Jihong Guan. All in one: Multi-task prompting for graph neural networks. KDD 23) The paper introduces a novel approach to bridging the gap between pre-trained graph models and the diverse tasks they're applied to, inspired by the success of prompt learning in NLP. Recognizing the challenge of aligning pre-trained models with varied graph tasks (node level, edge level, and graph level), which can lead to negative transfer and poor performance, we propose a multi-task prompting method for graphs. This method involves unifying graph and language prompt formats, enabling NLP's prompting strategies to be adapted for graph tasks. By analyzing the task space of graph applications, we reformulate problems to fit graph-level tasks and apply meta-learning to improve prompt initializ",
    "path": "papers/24/03/2403.07040.json",
    "total_tokens": 817,
    "translated_title": "一站式：图神经网络的多任务提示（扩展摘要）",
    "translated_abstract": "本文是我们在KDD23中获得最佳研究论文奖的原始工作的扩展摘要，其中我们介绍了一个新颖的方法，用于弥合预训练图模型和它们应用于的不同任务之间的差距，灵感来源于NLP中提示学习的成功。我们意识到了将预训练模型与各种图任务（节点级、边级和图级）对齐的挑战，这可能导致负迁移和性能下降，因此我们提出了一种用于图的多任务提示方法。该方法涉及统一图和语言提示格式，使NLP的提示策略能够适用于图任务。通过分析图应用的任务空间，我们重新制定问题以适应图级任务，并应用元学习来改进提示初始化。",
    "tldr": "本文介绍了一种新颖的多任务提示方法，用于解决预训练图模型与不同任务之间的差距，启发自NLP中提示学习的成功。",
    "en_tdlr": "This paper introduces a novel multi-task prompting method to bridge the gap between pre-trained graph models and diverse tasks, inspired by the success of prompt learning in NLP."
}