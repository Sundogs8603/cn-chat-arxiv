{
    "title": "SCORE: Self-supervised Correspondence Fine-tuning for Improved Content Representations",
    "abstract": "arXiv:2403.06260v1 Announce Type: new  Abstract: There is a growing interest in cost-effective self-supervised fine-tuning (SSFT) of self-supervised learning (SSL)-based speech models to obtain task-specific representations. These task-specific representations are used for robust performance on various downstream tasks by fine-tuning on the labelled data. This work presents a cost-effective SSFT method named Self-supervised Correspondence (SCORE) fine-tuning to adapt the SSL speech representations for content-related tasks. The proposed method uses a correspondence training strategy, aiming to learn similar representations from perturbed speech and original speech. Commonly used data augmentation techniques for content-related tasks (ASR) are applied to obtain perturbed speech. SCORE fine-tuned HuBERT outperforms the vanilla HuBERT on SUPERB benchmark with only a few hours of fine-tuning (< 5 hrs) on a single GPU for automatic speech recognition, phoneme recognition, and query-by-examp",
    "link": "https://arxiv.org/abs/2403.06260",
    "context": "Title: SCORE: Self-supervised Correspondence Fine-tuning for Improved Content Representations\nAbstract: arXiv:2403.06260v1 Announce Type: new  Abstract: There is a growing interest in cost-effective self-supervised fine-tuning (SSFT) of self-supervised learning (SSL)-based speech models to obtain task-specific representations. These task-specific representations are used for robust performance on various downstream tasks by fine-tuning on the labelled data. This work presents a cost-effective SSFT method named Self-supervised Correspondence (SCORE) fine-tuning to adapt the SSL speech representations for content-related tasks. The proposed method uses a correspondence training strategy, aiming to learn similar representations from perturbed speech and original speech. Commonly used data augmentation techniques for content-related tasks (ASR) are applied to obtain perturbed speech. SCORE fine-tuned HuBERT outperforms the vanilla HuBERT on SUPERB benchmark with only a few hours of fine-tuning (< 5 hrs) on a single GPU for automatic speech recognition, phoneme recognition, and query-by-examp",
    "path": "papers/24/03/2403.06260.json",
    "total_tokens": 888,
    "translated_title": "SCORE: 自监督对齐微调，提升内容表示",
    "translated_abstract": "arXiv:2403.06260v1 公告类型：新 提要：对于通过自监督学习（SSL）基础语音模型来获得任务特定表示以在各种下游任务上获得稳健性能，自监督微调（SSFT）引起了越来越多的关注。这些任务特定表示通过在标记数据上进行微调，用于提高各种下游任务的性能。本工作提出了一种名为自监督对齐（SCORE）微调的成本效益的SSFT方法，以调整SSL语音表示以适应与内容相关的任务。该方法使用一种对齐训练策略，旨在从扰动语音和原始语音中学习类似的表示。通常用于内容相关任务（ASR）的数据增强技术被应用于获取扰动语音。SCORE微调的HuBERT在超级基准上表现优于普通HuBERT，仅通过在单个GPU上进行少量时间的微调（<5小时）即可用于自动语音识别、音素识别和基于查询的示例。",
    "tldr": "提出了一种名为SCORE的自监督对齐微调方法，通过对齐训练策略学习类似表示，并应用于内容相关任务，显著提高了HuBERT在各种任务上的性能。",
    "en_tdlr": "Introduced a self-supervised correspondence fine-tuning method named SCORE, which learns similar representations through alignment training strategy and is applied to content-related tasks, significantly improving the performance of HuBERT on various tasks."
}