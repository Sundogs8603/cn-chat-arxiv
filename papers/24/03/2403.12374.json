{
    "title": "Improving Generalizability of Extracting Social Determinants of Health Using Large Language Models through Prompt-tuning",
    "abstract": "arXiv:2403.12374v1 Announce Type: new  Abstract: The progress in natural language processing (NLP) using large language models (LLMs) has greatly improved patient information extraction from clinical narratives. However, most methods based on the fine-tuning strategy have limited transfer learning ability for cross-domain applications. This study proposed a novel approach that employs a soft prompt-based learning architecture, which introduces trainable prompts to guide LLMs toward desired outputs. We examined two types of LLM architectures, including encoder-only GatorTron and decoder-only GatorTronGPT, and evaluated their performance for the extraction of social determinants of health (SDoH) using a cross-institution dataset from the 2022 n2c2 challenge and a cross-disease dataset from the University of Florida (UF) Health. The results show that decoder-only LLMs with prompt tuning achieved better performance in cross-domain applications. GatorTronGPT achieved the best F1 scores for ",
    "link": "https://arxiv.org/abs/2403.12374",
    "context": "Title: Improving Generalizability of Extracting Social Determinants of Health Using Large Language Models through Prompt-tuning\nAbstract: arXiv:2403.12374v1 Announce Type: new  Abstract: The progress in natural language processing (NLP) using large language models (LLMs) has greatly improved patient information extraction from clinical narratives. However, most methods based on the fine-tuning strategy have limited transfer learning ability for cross-domain applications. This study proposed a novel approach that employs a soft prompt-based learning architecture, which introduces trainable prompts to guide LLMs toward desired outputs. We examined two types of LLM architectures, including encoder-only GatorTron and decoder-only GatorTronGPT, and evaluated their performance for the extraction of social determinants of health (SDoH) using a cross-institution dataset from the 2022 n2c2 challenge and a cross-disease dataset from the University of Florida (UF) Health. The results show that decoder-only LLMs with prompt tuning achieved better performance in cross-domain applications. GatorTronGPT achieved the best F1 scores for ",
    "path": "papers/24/03/2403.12374.json",
    "total_tokens": 948,
    "translated_title": "通过Prompt-tuning改进使用大型语言模型提取健康社会决定因素的泛化能力",
    "translated_abstract": "自然语言处理（NLP）中使用大型语言模型（LLMs）的进展大大改善了从临床叙述中提取患者信息的能力。然而，大多数基于微调策略的方法在跨领域应用中具有有限的迁移学习能力。本研究提出了一种新颖的方法，采用软提示式学习架构，引入可训练的提示以指导LLMs朝向期望的输出。我们研究了两种LLM架构，分别是仅编码器GatorTron和仅解码器GatorTronGPT，并评估它们在使用来自2022年n2c2挑战的跨机构数据集和来自佛罗里达大学（UF）Health的跨疾病数据集进行健康社会决定因素（SDoH）提取的性能。结果表明，通过Prompt-tuning的解码器型LLMs在跨领域应用中取得了更好的性能。GatorTronGPT在交叉领域应用中取得了最好的F1分数。",
    "tldr": "该研究提出了一种通过软提示式学习架构来改进使用大型语言模型提取健康社会决定因素的泛化能力的方法，并发现decoder-only的LLMs在跨领域应用中通过Prompt-tuning取得了更好的性能。"
}