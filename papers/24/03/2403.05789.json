{
    "title": "ItD: Large Language Models Can Teach Themselves Induction through Deduction",
    "abstract": "arXiv:2403.05789v1 Announce Type: cross  Abstract: Although Large Language Models (LLMs) are showing impressive performance on a wide range of Natural Language Processing tasks, researchers have found that they still have limited ability to conduct induction. Recent works mainly adopt ``post processes'' paradigms to improve the performance of LLMs on induction (e.g., the hypothesis search & refinement methods), but their performance is still constrained by the inherent inductive capability of the LLMs. In this paper, we propose a novel framework, Induction through Deduction (ItD), to enable the LLMs to teach themselves induction through deduction. The ItD framework is composed of two main components: a Deductive Data Generation module to generate induction data and a Naive Bayesian Induction module to optimize the fine-tuning and decoding of LLMs. Our empirical results showcase the effectiveness of ItD on two induction benchmarks, achieving relative performance improvement of 36% and 1",
    "link": "https://arxiv.org/abs/2403.05789",
    "context": "Title: ItD: Large Language Models Can Teach Themselves Induction through Deduction\nAbstract: arXiv:2403.05789v1 Announce Type: cross  Abstract: Although Large Language Models (LLMs) are showing impressive performance on a wide range of Natural Language Processing tasks, researchers have found that they still have limited ability to conduct induction. Recent works mainly adopt ``post processes'' paradigms to improve the performance of LLMs on induction (e.g., the hypothesis search & refinement methods), but their performance is still constrained by the inherent inductive capability of the LLMs. In this paper, we propose a novel framework, Induction through Deduction (ItD), to enable the LLMs to teach themselves induction through deduction. The ItD framework is composed of two main components: a Deductive Data Generation module to generate induction data and a Naive Bayesian Induction module to optimize the fine-tuning and decoding of LLMs. Our empirical results showcase the effectiveness of ItD on two induction benchmarks, achieving relative performance improvement of 36% and 1",
    "path": "papers/24/03/2403.05789.json",
    "total_tokens": 851,
    "translated_title": "ItD：大型语言模型可以通过演绎自学归纳",
    "translated_abstract": "虽然大型语言模型（LLMs）在各种自然语言处理任务上表现出色，研究人员发现它们在进行归纳推理方面的能力仍然有限。最近的作品主要采用“后处理”范式来提高LLMs在归纳方面的表现（如假设搜索和细化方法），但它们的性能仍受限于LLMs的固有归纳能力。本文提出了一个新颖的框架，即演绎通过归纳（ItD），以使LLMs能够通过演绎自学归纳。ItD框架由两个主要组件组成：演绎数据生成模块用于生成归纳数据，以及朴素贝叶斯归纳模块用于优化LLMs的微调和解码。我们的实证结果展示了ItD在两个归纳基准上的有效性，相对性能提升分别为36%和1%。",
    "tldr": "提出了演绎通过归纳（ItD）框架，使大型语言模型能够通过演绎自学归纳，显著提升了归纳任务的性能。",
    "en_tdlr": "Introduced the Induction through Deduction (ItD) framework to enable large language models to self-learn induction through deduction, significantly improving performance on induction tasks."
}