{
    "title": "DiTMoS: Delving into Diverse Tiny-Model Selection on Microcontrollers",
    "abstract": "arXiv:2403.09035v1 Announce Type: new  Abstract: Enabling efficient and accurate deep neural network (DNN) inference on microcontrollers is non-trivial due to the constrained on-chip resources. Current methodologies primarily focus on compressing larger models yet at the expense of model accuracy. In this paper, we rethink the problem from the inverse perspective by constructing small/weak models directly and improving their accuracy. Thus, we introduce DiTMoS, a novel DNN training and inference framework with a selector-classifiers architecture, where the selector routes each input sample to the appropriate classifier for classification. DiTMoS is grounded on a key insight: a composition of weak models can exhibit high diversity and the union of them can significantly boost the accuracy upper bound. To approach the upper bound, DiTMoS introduces three strategies including diverse training data splitting to increase the classifiers' diversity, adversarial selector-classifiers training ",
    "link": "https://arxiv.org/abs/2403.09035",
    "context": "Title: DiTMoS: Delving into Diverse Tiny-Model Selection on Microcontrollers\nAbstract: arXiv:2403.09035v1 Announce Type: new  Abstract: Enabling efficient and accurate deep neural network (DNN) inference on microcontrollers is non-trivial due to the constrained on-chip resources. Current methodologies primarily focus on compressing larger models yet at the expense of model accuracy. In this paper, we rethink the problem from the inverse perspective by constructing small/weak models directly and improving their accuracy. Thus, we introduce DiTMoS, a novel DNN training and inference framework with a selector-classifiers architecture, where the selector routes each input sample to the appropriate classifier for classification. DiTMoS is grounded on a key insight: a composition of weak models can exhibit high diversity and the union of them can significantly boost the accuracy upper bound. To approach the upper bound, DiTMoS introduces three strategies including diverse training data splitting to increase the classifiers' diversity, adversarial selector-classifiers training ",
    "path": "papers/24/03/2403.09035.json",
    "total_tokens": 813,
    "translated_title": "探究微控制器上多样微小模型选择",
    "translated_abstract": "在微控制器上实现高效准确的深度神经网络（DNN）推断并不容易，因为芯片资源受限。本文从构建小型/弱模型并提高其准确性的反向角度重新思考这个问题。因此，我们引入了DiTMoS，一种新颖的DNN训练和推断框架，具有选择器-分类器架构，其中选择器将每个输入样本定位到适当的分类器以进行分类。DiTMoS建立在一个关键洞见上：弱模型的组合可以表现出高多样性，它们的联合可以显着提升准确性上限。为了接近这个上限，DiTMoS引入了包括增加分类器多样性的多样训练数据拆分在内的三种策略，对抗选择器-分类器训练",
    "tldr": "本文提出了一种新颖的DNN训练和推断框架DiTMoS，通过选择器-分类器架构和多样性策略，构建小型/弱模型并提高准确性上限。",
    "en_tdlr": "This paper introduces a novel DNN training and inference framework, DiTMoS, which constructs small/weak models directly and improves their accuracy with a selector-classifiers architecture and diversity strategies to approach the accuracy upper bound."
}