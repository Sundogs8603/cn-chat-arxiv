{
    "title": "AS-ES Learning: Towards Efficient CoT Learning in Small Models",
    "abstract": "arXiv:2403.01969v1 Announce Type: new  Abstract: Chain-of-Thought (CoT) serves as a critical emerging ability in LLMs, especially when it comes to logical reasoning. Attempts have been made to induce such ability in small models as well by distilling from the data with CoT generated by Large Language Models (LLMs). However, existing methods often simply generate and incorporate more data from LLMs and fail to note the importance of efficiently utilizing existing CoT data. We here propose a new training paradigm AS-ES (Abstractive Segments - Extractive Segments) learning, which exploits the inherent information in CoT for iterative generation. Experiments show that our methods surpass the direct seq2seq training on CoT-extensive tasks like MWP and PET summarization, without data augmentation or altering the model itself. Furthermore, we explore the reason behind the inefficiency of small models in learning CoT and provide an explanation of why AS-ES learning works, giving insights into ",
    "link": "https://arxiv.org/abs/2403.01969",
    "context": "Title: AS-ES Learning: Towards Efficient CoT Learning in Small Models\nAbstract: arXiv:2403.01969v1 Announce Type: new  Abstract: Chain-of-Thought (CoT) serves as a critical emerging ability in LLMs, especially when it comes to logical reasoning. Attempts have been made to induce such ability in small models as well by distilling from the data with CoT generated by Large Language Models (LLMs). However, existing methods often simply generate and incorporate more data from LLMs and fail to note the importance of efficiently utilizing existing CoT data. We here propose a new training paradigm AS-ES (Abstractive Segments - Extractive Segments) learning, which exploits the inherent information in CoT for iterative generation. Experiments show that our methods surpass the direct seq2seq training on CoT-extensive tasks like MWP and PET summarization, without data augmentation or altering the model itself. Furthermore, we explore the reason behind the inefficiency of small models in learning CoT and provide an explanation of why AS-ES learning works, giving insights into ",
    "path": "papers/24/03/2403.01969.json",
    "total_tokens": 913,
    "translated_title": "AS-ES学习：小型模型中高效CoT学习研究",
    "translated_abstract": "Chain-of-Thought（CoT）在LLM中是一种关键的新型能力，特别在逻辑推理方面尤为重要。为了在小型模型中引入这种能力，人们尝试通过从大型语言模型（LLMs）生成的CoT数据中提取信息。然而，现有方法往往只是简单地从LLMs生成更多数据并将其加以利用，而忽视了高效利用现有CoT数据的重要性。我们在这里提出了一种新的训练范式AS-ES（抽象片段 - 提取性片段）学习，该方法利用CoT中的固有信息进行迭代生成。实验证明，我们的方法在类似MWP和PET摘要等CoT丰富任务上超越了直接的seq2seq训练，而无需进行数据增强或修改模型本身。此外，我们探讨了小型模型在学习CoT方面效率低下的原因，并解释了为什么AS-ES学习有效，从而深入研究了CoT学习的原理。",
    "tldr": "AS-ES学习提出了一种新的训练范式，通过高效利用现有CoT数据来实现在小型模型中的高效迭代生成，超越了直接seq2seq训练，在CoT丰富任务上取得了显著表现。",
    "en_tdlr": "AS-ES learning proposes a new training paradigm that achieves efficient iterative generation in small models by effectively utilizing existing CoT data, surpassing direct seq2seq training and showing significant performance on CoT-extensive tasks."
}