{
    "title": "On the Approximation of Kernel functions",
    "abstract": "arXiv:2403.06731v1 Announce Type: cross  Abstract: Various methods in statistical learning build on kernels considered in reproducing kernel Hilbert spaces. In applications, the kernel is often selected based on characteristics of the problem and the data. This kernel is then employed to infer response variables at points, where no explanatory data were observed. The data considered here are located in compact sets in higher dimensions and the paper addresses approximations of the kernel itself. The new approach considers Taylor series approximations of radial kernel functions. For the Gauss kernel on the unit cube, the paper establishes an upper bound of the associated eigenfunctions, which grows only polynomially with respect to the index. The novel approach substantiates smaller regularization parameters than considered in the literature, overall leading to better approximations. This improvement confirms low rank approximation methods such as the Nystr\\\"om method.",
    "link": "https://arxiv.org/abs/2403.06731",
    "context": "Title: On the Approximation of Kernel functions\nAbstract: arXiv:2403.06731v1 Announce Type: cross  Abstract: Various methods in statistical learning build on kernels considered in reproducing kernel Hilbert spaces. In applications, the kernel is often selected based on characteristics of the problem and the data. This kernel is then employed to infer response variables at points, where no explanatory data were observed. The data considered here are located in compact sets in higher dimensions and the paper addresses approximations of the kernel itself. The new approach considers Taylor series approximations of radial kernel functions. For the Gauss kernel on the unit cube, the paper establishes an upper bound of the associated eigenfunctions, which grows only polynomially with respect to the index. The novel approach substantiates smaller regularization parameters than considered in the literature, overall leading to better approximations. This improvement confirms low rank approximation methods such as the Nystr\\\"om method.",
    "path": "papers/24/03/2403.06731.json",
    "total_tokens": 797,
    "translated_title": "对核函数的近似方法",
    "translated_abstract": "统计学习中的各种方法都建立在再生核希尔伯特空间中考虑的核函数基础之上。在应用中，通常根据问题和数据的特征选择核函数。然后利用这个核函数推断那些没有观测到解释数据的点的响应变量。本文研究的数据位于高维空间中的紧致集合中，并解决了核函数本身的近似问题。新方法考虑了径向核函数的泰勒级数逼近。对于单位立方体上的高斯核函数，本文建立了关联特征函数的上限，这个上限随指数多项式增长。该新方法证明了比文献中考虑的更小的正则化参数，从而整体上实现更好的近似。这一改进证实了低秩逼近方法，如Nystrom方法。",
    "tldr": "本文提出了一种新的方法，通过考虑径向核函数的泰勒级数逼近，建立了对核函数的较好近似，证实了可以使用比文献中更小的正则化参数来实现更好的结果。",
    "en_tdlr": "This paper introduces a novel approach that establishes a better approximation of the kernel function by considering Taylor series approximations of radial kernel functions, confirming the ability to achieve better results with smaller regularization parameters than those considered in the literature."
}