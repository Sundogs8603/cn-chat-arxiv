{
    "title": "Towards Explainability in Legal Outcome Prediction Models",
    "abstract": "arXiv:2403.16852v1 Announce Type: cross  Abstract: Current legal outcome prediction models - a staple of legal NLP - do not explain their reasoning. However, to employ these models in the real world, human legal actors need to be able to understand their decisions. In the case of common law, legal practitioners reason towards the outcome of a case by referring to past case law, known as precedent. We contend that precedent is, therefore, a natural way of facilitating explainability for legal NLP models. In this paper, we contribute a novel method for identifying the precedent employed by legal outcome prediction models. Furthermore, by developing a taxonomy of legal precedent, we are able to compare human judges and our models with respect to the different types of precedent they rely on. We find that while the models learn to predict outcomes reasonably well, their use of precedent is unlike that of human judges.",
    "link": "https://arxiv.org/abs/2403.16852",
    "context": "Title: Towards Explainability in Legal Outcome Prediction Models\nAbstract: arXiv:2403.16852v1 Announce Type: cross  Abstract: Current legal outcome prediction models - a staple of legal NLP - do not explain their reasoning. However, to employ these models in the real world, human legal actors need to be able to understand their decisions. In the case of common law, legal practitioners reason towards the outcome of a case by referring to past case law, known as precedent. We contend that precedent is, therefore, a natural way of facilitating explainability for legal NLP models. In this paper, we contribute a novel method for identifying the precedent employed by legal outcome prediction models. Furthermore, by developing a taxonomy of legal precedent, we are able to compare human judges and our models with respect to the different types of precedent they rely on. We find that while the models learn to predict outcomes reasonably well, their use of precedent is unlike that of human judges.",
    "path": "papers/24/03/2403.16852.json",
    "total_tokens": 855,
    "translated_title": "在法律结果预测模型中迈向可解释性",
    "translated_abstract": "当前的法律结果预测模型 - 法律NLP的基本组成部分 - 不能解释其推理过程。然而，为了在现实世界中应用这些模型，人类法律主体需要能够理解它们的决策。在普通法案例中，法律从业者通过参考被称为先例的过去案例法律推理到案件结果。我们认为，先例因此成为促进法律NLP模型可解释性的一种自然方式。在本文中，我们提出了一种新颖的方法来识别法律结果预测模型使用的先例。此外，通过制定法律先例的分类法，我们能够比较人类法官和我们的模型在他们依赖的不同类型先例方面的差异。我们发现，虽然模型学会了合理地预测结果，但它们使用的先例方式不同于人类法官。",
    "tldr": "先例是促进法律NLP模型可解释性的一种自然方式，我们提出了一种新颖的方法来识别法律结果预测模型使用的先例，并发现模型预测结果的能力不错，但其使用先例的方式与人类法官不同。",
    "en_tdlr": "Precedent serves as a natural way to facilitate explainability for legal NLP models, and the paper introduces a novel method to identify the precedent used by these models, highlighting that while the models predict outcomes reasonably well, their utilization of precedent differs from that of human judges."
}