{
    "title": "Analysis of Privacy Leakage in Federated Large Language Models",
    "abstract": "arXiv:2403.04784v1 Announce Type: cross  Abstract: With the rapid adoption of Federated Learning (FL) as the training and tuning protocol for applications utilizing Large Language Models (LLMs), recent research highlights the need for significant modifications to FL to accommodate the large-scale of LLMs. While substantial adjustments to the protocol have been introduced as a response, comprehensive privacy analysis for the adapted FL protocol is currently lacking.   To address this gap, our work delves into an extensive examination of the privacy analysis of FL when used for training LLMs, both from theoretical and practical perspectives. In particular, we design two active membership inference attacks with guaranteed theoretical success rates to assess the privacy leakages of various adapted FL configurations. Our theoretical findings are translated into practical attacks, revealing substantial privacy vulnerabilities in popular LLMs, including BERT, RoBERTa, DistilBERT, and OpenAI's",
    "link": "https://arxiv.org/abs/2403.04784",
    "context": "Title: Analysis of Privacy Leakage in Federated Large Language Models\nAbstract: arXiv:2403.04784v1 Announce Type: cross  Abstract: With the rapid adoption of Federated Learning (FL) as the training and tuning protocol for applications utilizing Large Language Models (LLMs), recent research highlights the need for significant modifications to FL to accommodate the large-scale of LLMs. While substantial adjustments to the protocol have been introduced as a response, comprehensive privacy analysis for the adapted FL protocol is currently lacking.   To address this gap, our work delves into an extensive examination of the privacy analysis of FL when used for training LLMs, both from theoretical and practical perspectives. In particular, we design two active membership inference attacks with guaranteed theoretical success rates to assess the privacy leakages of various adapted FL configurations. Our theoretical findings are translated into practical attacks, revealing substantial privacy vulnerabilities in popular LLMs, including BERT, RoBERTa, DistilBERT, and OpenAI's",
    "path": "papers/24/03/2403.04784.json",
    "total_tokens": 898,
    "translated_title": "在联邦式大语言模型中隐私泄露的分析",
    "translated_abstract": "随着联邦学习（FL）作为利用大语言模型（LLMs）的应用的训练和调优协议的快速采用，最近的研究突出了对FL进行重大修改以适应LLMs的大规模的需要。虽然作为回应已经引入了对协议的重大调整，但目前缺乏对适应后的FL协议进行全面隐私分析的研究。为填补这一空白，我们的工作深入探讨了在训练LLMs时使用FL的隐私分析，既从理论角度又从实际角度。具体来说，我们设计了两种带有有理论成功率保证的主动成员推理攻击，以评估各种调整后的FL配置的隐私泄漏。我们的理论发现转化为实际攻击，揭示了流行的LLMs（包括BERT、RoBERTa、DistilBERT和OpenAI的）存在重大的隐私漏洞。",
    "tldr": "该论文对在训练大语言模型时使用联邦学习进行隐私分析进行了全面研究，设计了两种主动成员推理攻击来评估各种调整后的联邦学习配置的隐私泄漏，并揭示了流行的大语言模型存在的重大隐私漏洞。",
    "en_tdlr": "This paper extensively examines the privacy analysis of using federated learning for training large language models, designs two active membership inference attacks to assess privacy leakages of various adapted FL configurations, and reveals significant privacy vulnerabilities in popular large language models."
}