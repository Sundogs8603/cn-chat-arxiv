{
    "title": "Convergence analysis of OT-Flow for sample generation",
    "abstract": "arXiv:2403.16208v1 Announce Type: cross  Abstract: Deep generative models aim to learn the underlying distribution of data and generate new ones. Despite the diversity of generative models and their high-quality generation performance in practice, most of them lack rigorous theoretical convergence proofs. In this work, we aim to establish some convergence results for OT-Flow, one of the deep generative models. First, by reformulating the framework of OT-Flow model, we establish the $\\Gamma$-convergence of the formulation of OT-flow to the corresponding optimal transport (OT) problem as the regularization term parameter $\\alpha$ goes to infinity. Second, since the loss function will be approximated by Monte Carlo method in training, we established the convergence between the discrete loss function and the continuous one when the sample number $N$ goes to infinity as well. Meanwhile, the approximation capability of the neural network provides an upper bound for the discrete loss function",
    "link": "https://arxiv.org/abs/2403.16208",
    "context": "Title: Convergence analysis of OT-Flow for sample generation\nAbstract: arXiv:2403.16208v1 Announce Type: cross  Abstract: Deep generative models aim to learn the underlying distribution of data and generate new ones. Despite the diversity of generative models and their high-quality generation performance in practice, most of them lack rigorous theoretical convergence proofs. In this work, we aim to establish some convergence results for OT-Flow, one of the deep generative models. First, by reformulating the framework of OT-Flow model, we establish the $\\Gamma$-convergence of the formulation of OT-flow to the corresponding optimal transport (OT) problem as the regularization term parameter $\\alpha$ goes to infinity. Second, since the loss function will be approximated by Monte Carlo method in training, we established the convergence between the discrete loss function and the continuous one when the sample number $N$ goes to infinity as well. Meanwhile, the approximation capability of the neural network provides an upper bound for the discrete loss function",
    "path": "papers/24/03/2403.16208.json",
    "total_tokens": 891,
    "translated_title": "OT-Flow样本生成的收敛性分析",
    "translated_abstract": "深度生成模型的目标是学习数据的潜在分布并生成新的数据。尽管生成模型的多样性和实践中的高质量生成性能，大多数模型缺乏严格的理论收敛证明。在这项工作中，我们旨在建立OT-Flow这一深度生成模型的一些收敛结果。首先，通过重新构建OT-Flow模型的框架，我们证明了在正则化项参数$\\alpha$趋于无穷大时，OT-Flow模型的表述与对应的最优输运（OT）问题之间的$\\Gamma$-收敛。其次，由于损失函数在训练中将通过蒙特卡洛方法逼近，当样本数$N$趋于无穷大时，我们也证明了离散损失函数与连续损失函数之间的收敛性。与此同时，神经网络的逼近能力为离散损失函数提供了一个上界。",
    "tldr": "本文旨在为OT-Flow这一深度生成模型建立一些收敛结果，包括在正则化项参数趋于无穷大时，OT-Flow与最优输运问题的收敛关系，以及随着样本数趋于无穷大时，离散损失函数与连续损失函数之间的收敛关系。",
    "en_tdlr": "This work aims to establish convergence results for the OT-Flow deep generative model, including the convergence between OT-Flow and the optimal transport problem as the regularization term parameter goes to infinity, and the convergence between discrete and continuous loss functions as the sample number goes to infinity."
}