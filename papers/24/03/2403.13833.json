{
    "title": "Linearly Constrained Weights: Reducing Activation Shift for Faster Training of Neural Networks",
    "abstract": "arXiv:2403.13833v1 Announce Type: cross  Abstract: In this paper, we first identify activation shift, a simple but remarkable phenomenon in a neural network in which the preactivation value of a neuron has non-zero mean that depends on the angle between the weight vector of the neuron and the mean of the activation vector in the previous layer. We then propose linearly constrained weights (LCW) to reduce the activation shift in both fully connected and convolutional layers. The impact of reducing the activation shift in a neural network is studied from the perspective of how the variance of variables in the network changes through layer operations in both forward and backward chains. We also discuss its relationship to the vanishing gradient problem. Experimental results show that LCW enables a deep feedforward network with sigmoid activation functions to be trained efficiently by resolving the vanishing gradient problem. Moreover, combined with batch normalization, LCW improves genera",
    "link": "https://arxiv.org/abs/2403.13833",
    "context": "Title: Linearly Constrained Weights: Reducing Activation Shift for Faster Training of Neural Networks\nAbstract: arXiv:2403.13833v1 Announce Type: cross  Abstract: In this paper, we first identify activation shift, a simple but remarkable phenomenon in a neural network in which the preactivation value of a neuron has non-zero mean that depends on the angle between the weight vector of the neuron and the mean of the activation vector in the previous layer. We then propose linearly constrained weights (LCW) to reduce the activation shift in both fully connected and convolutional layers. The impact of reducing the activation shift in a neural network is studied from the perspective of how the variance of variables in the network changes through layer operations in both forward and backward chains. We also discuss its relationship to the vanishing gradient problem. Experimental results show that LCW enables a deep feedforward network with sigmoid activation functions to be trained efficiently by resolving the vanishing gradient problem. Moreover, combined with batch normalization, LCW improves genera",
    "path": "papers/24/03/2403.13833.json",
    "total_tokens": 845,
    "translated_title": "线性约束权重：减少神经网络训练中的激活偏移",
    "translated_abstract": "在本文中，我们首次确定了激活偏移，这是神经网络中的一个简单但显著的现象，即神经元的预激活值具有非零均值，该均值取决于神经元的权重向量与前一层激活向量均值之间的夹角。然后，我们提出了线性约束权重（LCW），以减少全连接和卷积层中的激活偏移。从网络变量的方差如何通过前向和反向链中的层操作来改变的角度研究了减少神经网络中激活偏移的影响。我们还讨论了它与梯度消失问题的关系。实验结果表明，LCW使具有sigmoid激活函数的深度前向网络能够通过解决梯度消失问题而得以有效训练。此外，与批归一化结合使用，LCW改进了genera",
    "tldr": "神经网络中引入线性约束权重（LCW）来减少激活偏移，有效解决了梯度消失问题，提高了深度前向网络的训练效率。",
    "en_tdlr": "Introducing Linearly Constrained Weights (LCW) in neural networks reduces activation shift, effectively addresses the vanishing gradient problem, and improves the training efficiency of deep feedforward networks."
}