{
    "title": "SIFiD: Reassess Summary Factual Inconsistency Detection with LLM",
    "abstract": "arXiv:2403.07557v1 Announce Type: new  Abstract: Ensuring factual consistency between the summary and the original document is paramount in summarization tasks. Consequently, considerable effort has been dedicated to detecting inconsistencies. With the advent of Large Language Models (LLMs), recent studies have begun to leverage their advanced language understanding capabilities for inconsistency detection. However, early attempts have shown that LLMs underperform traditional models due to their limited ability to follow instructions and the absence of an effective detection methodology. In this study, we reassess summary inconsistency detection with LLMs, comparing the performances of GPT-3.5 and GPT-4. To advance research in LLM-based inconsistency detection, we propose SIFiD (Summary Inconsistency Detection with Filtered Document) that identify key sentences within documents by either employing natural language inference or measuring semantic similarity between summaries and documen",
    "link": "https://arxiv.org/abs/2403.07557",
    "context": "Title: SIFiD: Reassess Summary Factual Inconsistency Detection with LLM\nAbstract: arXiv:2403.07557v1 Announce Type: new  Abstract: Ensuring factual consistency between the summary and the original document is paramount in summarization tasks. Consequently, considerable effort has been dedicated to detecting inconsistencies. With the advent of Large Language Models (LLMs), recent studies have begun to leverage their advanced language understanding capabilities for inconsistency detection. However, early attempts have shown that LLMs underperform traditional models due to their limited ability to follow instructions and the absence of an effective detection methodology. In this study, we reassess summary inconsistency detection with LLMs, comparing the performances of GPT-3.5 and GPT-4. To advance research in LLM-based inconsistency detection, we propose SIFiD (Summary Inconsistency Detection with Filtered Document) that identify key sentences within documents by either employing natural language inference or measuring semantic similarity between summaries and documen",
    "path": "papers/24/03/2403.07557.json",
    "total_tokens": 900,
    "translated_title": "SIFiD：使用LLM重新评估摘要的事实不一致性检测",
    "translated_abstract": "确保摘要与原始文档之间的事实一致性在摘要任务中至关重要。因此，人们致力于检测不一致性。随着大型语言模型（LLMs）的出现，最近的研究开始利用它们先进的语言理解能力进行不一致性检测。然而，早期尝试表明，由于LLMs有限的遵循指令能力和缺乏有效的检测方法论，它们的性能不及传统模型。在这项研究中，我们使用GPT-3.5和GPT-4比较LLMs的摘要不一致性检测表现，以推动基于LLM的不一致性检测研究。我们提出了SIFiD（带有过滤文档的摘要不一致性检测），它通过使用自然语言推理或测量摘要和文档之间的语义相似性来识别文档中的关键句子。",
    "tldr": "本研究重新评估了使用LLM进行摘要不一致性检测的方法，提出了SIFiD（带有过滤文档的摘要不一致性检测），旨在通过自然语言推理或测量语义相似性来识别文档中的关键句子。",
    "en_tdlr": "This study reassesses the approach of detecting summary inconsistency with LLMs and proposes SIFiD (Summary Inconsistency Detection with Filtered Document), aiming to identify key sentences within documents through natural language inference or measuring semantic similarity."
}