{
    "title": "MedFLIP: Medical Vision-and-Language Self-supervised Fast Pre-Training with Masked Autoencoder",
    "abstract": "arXiv:2403.04626v1 Announce Type: cross  Abstract: Within the domain of medical analysis, extensive research has explored the potential of mutual learning between Masked Autoencoders(MAEs) and multimodal data. However, the impact of MAEs on intermodality remains a key challenge. We introduce MedFLIP, a Fast Language-Image Pre-training method for Medical analysis. We explore MAEs for zero-shot learning with crossed domains, which enhances the model ability to learn from limited data, a common scenario in medical diagnostics. We verify that masking an image does not affect intermodal learning. Furthermore, we propose the SVD loss to enhance the representation learning for characteristics of medical images, aiming to improve classification accuracy by leveraging the structural intricacies of such data. Lastly, we validate using language will improve the zero-shot performance for the medical image analysis. MedFLIP scaling of the masking process marks an advancement in the field, offering ",
    "link": "https://arxiv.org/abs/2403.04626",
    "context": "Title: MedFLIP: Medical Vision-and-Language Self-supervised Fast Pre-Training with Masked Autoencoder\nAbstract: arXiv:2403.04626v1 Announce Type: cross  Abstract: Within the domain of medical analysis, extensive research has explored the potential of mutual learning between Masked Autoencoders(MAEs) and multimodal data. However, the impact of MAEs on intermodality remains a key challenge. We introduce MedFLIP, a Fast Language-Image Pre-training method for Medical analysis. We explore MAEs for zero-shot learning with crossed domains, which enhances the model ability to learn from limited data, a common scenario in medical diagnostics. We verify that masking an image does not affect intermodal learning. Furthermore, we propose the SVD loss to enhance the representation learning for characteristics of medical images, aiming to improve classification accuracy by leveraging the structural intricacies of such data. Lastly, we validate using language will improve the zero-shot performance for the medical image analysis. MedFLIP scaling of the masking process marks an advancement in the field, offering ",
    "path": "papers/24/03/2403.04626.json",
    "total_tokens": 907,
    "translated_title": "MedFLIP：医学视觉与语言自监督快速预训练与掩蔽自编码器",
    "translated_abstract": "在医学分析领域，广泛的研究探讨了掩蔽自编码器（MAEs）和多模态数据之间互相学习的潜力。然而，MAEs对跨模态学习的影响仍然是一个关键挑战。我们引入了MedFLIP，一种用于医学分析的快速语言-图像预训练方法。我们探索使用MAEs进行跨领域零样本学习，从而增强模型在医学诊断中常见的有限数据中学习的能力。我们验证了对图像进行掩蔽不会影响跨模态学习。此外，我们提出了SVD损失以增强医学图像特征的表示学习，旨在通过利用这类数据的结构复杂性来提高分类准确性。最后，我们验证了使用语言将提高医学图像分析的零样本性能。MedFLIP对掩蔽过程的扩展标志着该领域的进步。",
    "tldr": "MedFLIP是一种用于医学分析的快速语言-图像预训练方法，通过引入SVD损失增强医学图像特征表示学习，验证了用语言可以提高零样本医学图像分析的性能。",
    "en_tdlr": "MedFLIP is a fast language-image pre-training method for medical analysis, which enhances medical image feature representation learning by introducing SVD loss and validates that using language can improve zero-shot performance for medical image analysis."
}