{
    "title": "A non-asymptotic theory of Kernel Ridge Regression: deterministic equivalents, test error, and GCV estimator",
    "abstract": "arXiv:2403.08938v1 Announce Type: cross  Abstract: We consider learning an unknown target function $f_*$ using kernel ridge regression (KRR) given i.i.d. data $(u_i,y_i)$, $i\\leq n$, where $u_i \\in U$ is a covariate vector and $y_i = f_* (u_i) +\\varepsilon_i \\in \\mathbb{R}$. A recent string of work has empirically shown that the test error of KRR can be well approximated by a closed-form estimate derived from an `equivalent' sequence model that only depends on the spectrum of the kernel operator. However, a theoretical justification for this equivalence has so far relied either on restrictive assumptions -- such as subgaussian independent eigenfunctions -- , or asymptotic derivations for specific kernels in high dimensions.   In this paper, we prove that this equivalence holds for a general class of problems satisfying some spectral and concentration properties on the kernel eigendecomposition. Specifically, we establish in this setting a non-asymptotic deterministic approximation for ",
    "link": "https://arxiv.org/abs/2403.08938",
    "context": "Title: A non-asymptotic theory of Kernel Ridge Regression: deterministic equivalents, test error, and GCV estimator\nAbstract: arXiv:2403.08938v1 Announce Type: cross  Abstract: We consider learning an unknown target function $f_*$ using kernel ridge regression (KRR) given i.i.d. data $(u_i,y_i)$, $i\\leq n$, where $u_i \\in U$ is a covariate vector and $y_i = f_* (u_i) +\\varepsilon_i \\in \\mathbb{R}$. A recent string of work has empirically shown that the test error of KRR can be well approximated by a closed-form estimate derived from an `equivalent' sequence model that only depends on the spectrum of the kernel operator. However, a theoretical justification for this equivalence has so far relied either on restrictive assumptions -- such as subgaussian independent eigenfunctions -- , or asymptotic derivations for specific kernels in high dimensions.   In this paper, we prove that this equivalence holds for a general class of problems satisfying some spectral and concentration properties on the kernel eigendecomposition. Specifically, we establish in this setting a non-asymptotic deterministic approximation for ",
    "path": "papers/24/03/2403.08938.json",
    "total_tokens": 826,
    "translated_title": "Kernel Ridge Regression的非渐近理论：确定性等价，测试误差和GCV估计",
    "translated_abstract": "我们考虑使用核岭回归（KRR）学习未知目标函数$f_*$，给定i.i.d.数据$(u_i,y_i)$，$i\\leq n$，其中$u_i \\in U$是一个协变量向量，$y_i = f_* (u_i) +\\varepsilon_i \\in \\mathbb{R}$。最近的研究连续表明，KRR的测试误差可以通过一个从核算子的谱依赖的等价序列模型导出的封闭形式估计很好地近似。然而，对于此等价性的理论证明迄今为止要么依赖于限制性假设--如次高斯独立本征函数--，要么对高维具体核进行渐近推导。",
    "tldr": "该论文证明了对于满足一定核特征值分解的谱和集中性质的一般类问题，具有一种非渐近确定性近似的等价性。",
    "en_tdlr": "This paper establishes a non-asymptotic deterministic approximation for the equivalence of Kernel Ridge Regression test error derived from a general class of problems satisfying specific spectral and concentration properties on the kernel eigendecomposition."
}