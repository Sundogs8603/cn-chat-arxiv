{
    "title": "Embedded Translations for Low-resource Automated Glossing",
    "abstract": "arXiv:2403.08189v1 Announce Type: new  Abstract: We investigate automatic interlinear glossing in low-resource settings. We augment a hard-attentional neural model with embedded translation information extracted from interlinear glossed text. After encoding these translations using large language models, specifically BERT and T5, we introduce a character-level decoder for generating glossed output. Aided by these enhancements, our model demonstrates an average improvement of 3.97\\%-points over the previous state of the art on datasets from the SIGMORPHON 2023 Shared Task on Interlinear Glossing. In a simulated ultra low-resource setting, trained on as few as 100 sentences, our system achieves an average 9.78\\%-point improvement over the plain hard-attentional baseline. These results highlight the critical role of translation information in boosting the system's performance, especially in processing and interpreting modest data sources. Our findings suggest a promising avenue for the do",
    "link": "https://arxiv.org/abs/2403.08189",
    "context": "Title: Embedded Translations for Low-resource Automated Glossing\nAbstract: arXiv:2403.08189v1 Announce Type: new  Abstract: We investigate automatic interlinear glossing in low-resource settings. We augment a hard-attentional neural model with embedded translation information extracted from interlinear glossed text. After encoding these translations using large language models, specifically BERT and T5, we introduce a character-level decoder for generating glossed output. Aided by these enhancements, our model demonstrates an average improvement of 3.97\\%-points over the previous state of the art on datasets from the SIGMORPHON 2023 Shared Task on Interlinear Glossing. In a simulated ultra low-resource setting, trained on as few as 100 sentences, our system achieves an average 9.78\\%-point improvement over the plain hard-attentional baseline. These results highlight the critical role of translation information in boosting the system's performance, especially in processing and interpreting modest data sources. Our findings suggest a promising avenue for the do",
    "path": "papers/24/03/2403.08189.json",
    "total_tokens": 869,
    "translated_title": "嵌入式翻译用于低资源自动互译词汇标注",
    "translated_abstract": "我们研究了在低资源环境下自动词间标注。我们使用从词间标注文本中提取的嵌入式翻译信息来增强基于硬注意力的神经模型。通过使用大型语言模型（具体来说是BERT和T5）对这些翻译进行编码，我们引入了一个字符级解码器来生成标注输出。在这些增强的帮助下，我们的模型在SIGMORPHON 2023词间标注共享任务数据集上相比之前的最先进技术展现出了平均提升3.97\\%。在模拟的超低资源环境中，即使只训练了100个句子，我们的系统也比简单的硬注意力基线平均提升了9.78\\%。这些结果突显了翻译信息在提升系统性能中的关键作用，尤其是在处理和解释适度数据源时。我们的发现为这一领域未来研究指明了一个有前景的方向。",
    "tldr": "研究在低资源环境下自动词间标注，通过嵌入式翻译信息提升神经模型性能达到提升，尤其在处理和解释适度数据源时。"
}