{
    "title": "Thelxino\\\"e: Recognizing Human Emotions Using Pupillometry and Machine Learning",
    "abstract": "arXiv:2403.19014v1 Announce Type: new  Abstract: In this study, we present a method for emotion recognition in Virtual Reality (VR) using pupillometry. We analyze pupil diameter responses to both visual and auditory stimuli via a VR headset and focus on extracting key features in the time-domain, frequency-domain, and time-frequency domain from VR generated data. Our approach utilizes feature selection to identify the most impactful features using Maximum Relevance Minimum Redundancy (mRMR). By applying a Gradient Boosting model, an ensemble learning technique using stacked decision trees, we achieve an accuracy of 98.8% with feature engineering, compared to 84.9% without it. This research contributes significantly to the Thelxino\\\"e framework, aiming to enhance VR experiences by integrating multiple sensor data for realistic and emotionally resonant touch interactions. Our findings open new avenues for developing more immersive and interactive VR environments, paving the way for futur",
    "link": "https://arxiv.org/abs/2403.19014",
    "context": "Title: Thelxino\\\"e: Recognizing Human Emotions Using Pupillometry and Machine Learning\nAbstract: arXiv:2403.19014v1 Announce Type: new  Abstract: In this study, we present a method for emotion recognition in Virtual Reality (VR) using pupillometry. We analyze pupil diameter responses to both visual and auditory stimuli via a VR headset and focus on extracting key features in the time-domain, frequency-domain, and time-frequency domain from VR generated data. Our approach utilizes feature selection to identify the most impactful features using Maximum Relevance Minimum Redundancy (mRMR). By applying a Gradient Boosting model, an ensemble learning technique using stacked decision trees, we achieve an accuracy of 98.8% with feature engineering, compared to 84.9% without it. This research contributes significantly to the Thelxino\\\"e framework, aiming to enhance VR experiences by integrating multiple sensor data for realistic and emotionally resonant touch interactions. Our findings open new avenues for developing more immersive and interactive VR environments, paving the way for futur",
    "path": "papers/24/03/2403.19014.json",
    "total_tokens": 965,
    "translated_title": "Thelxino\\\"e:使用瞳孔测量和机器学习识别人类情绪",
    "translated_abstract": "在这项研究中，我们提出了一种使用瞳孔测量来识别虚拟现实（VR）中情绪的方法。我们通过VR头显分析对视觉和听觉刺激的瞳孔直径响应，并专注于从VR生成的数据中提取时域、频域和时频域的关键特征。我们的方法利用特征选择，通过最大相关性最小冗余性（mRMR）识别最具影响力的特征。通过应用梯度提升模型，一种使用叠加决策树的集成学习技术，我们在进行特征工程时实现了98.8%的准确率，而未进行特征工程时为84.9%。这项研究对Thelxino\\\"e框架做出了重大贡献，旨在通过整合多传感器数据实现更加逼真和情感共鸣的触觉交互，为开发更具沉浸感和互动性的VR环境打开了新的途径，为未来铺平了道路。",
    "tldr": "该研究提出了一种使用瞳孔测量来识别 VR 中情绪的方法，通过特征工程和梯度提升模型，在 Thelxino\\\"e 框架中取得了98.8%的高准确率，为发展更具沉浸感和互动性的 VR 环境开辟了新的途径。",
    "en_tdlr": "The study presents a method for emotion recognition in VR using pupillometry, achieving a high accuracy of 98.8% within the Thelxino\\\"e framework through feature engineering and Gradient Boosting model, paving the way for more immersive and interactive VR environments."
}