{
    "title": "Stragglers-Aware Low-Latency Synchronous Federated Learning via Layer-Wise Model Updates",
    "abstract": "arXiv:2403.18375v1 Announce Type: new  Abstract: Synchronous federated learning (FL) is a popular paradigm for collaborative edge learning. It typically involves a set of heterogeneous devices locally training neural network (NN) models in parallel with periodic centralized aggregations. As some of the devices may have limited computational resources and varying availability, FL latency is highly sensitive to stragglers. Conventional approaches discard incomplete intra-model updates done by stragglers, alter the amount of local workload and architecture, or resort to asynchronous settings; which all affect the trained model performance under tight training latency constraints. In this work, we propose straggler-aware layer-wise federated learning (SALF) that leverages the optimization procedure of NNs via backpropagation to update the global model in a layer-wise fashion. SALF allows stragglers to synchronously convey partial gradients, having each layer of the global model be updated ",
    "link": "https://arxiv.org/abs/2403.18375",
    "context": "Title: Stragglers-Aware Low-Latency Synchronous Federated Learning via Layer-Wise Model Updates\nAbstract: arXiv:2403.18375v1 Announce Type: new  Abstract: Synchronous federated learning (FL) is a popular paradigm for collaborative edge learning. It typically involves a set of heterogeneous devices locally training neural network (NN) models in parallel with periodic centralized aggregations. As some of the devices may have limited computational resources and varying availability, FL latency is highly sensitive to stragglers. Conventional approaches discard incomplete intra-model updates done by stragglers, alter the amount of local workload and architecture, or resort to asynchronous settings; which all affect the trained model performance under tight training latency constraints. In this work, we propose straggler-aware layer-wise federated learning (SALF) that leverages the optimization procedure of NNs via backpropagation to update the global model in a layer-wise fashion. SALF allows stragglers to synchronously convey partial gradients, having each layer of the global model be updated ",
    "path": "papers/24/03/2403.18375.json",
    "total_tokens": 893,
    "translated_title": "基于层次模型更新的关注Stragglers的低延迟同步联邦学习",
    "translated_abstract": "同步联邦学习(FL)是协作边缘学习的一种流行范式。它通常涉及一组异构设备在本地并行训练神经网络(NN)模型，并定期进行集中聚合。由于一些设备可能具有有限的计算资源和可变的可用性，FL的延迟对stragglers非常敏感。传统方法会丢弃stragglers执行的不完整的模型内更新，改变本地工作量和体系结构的数量，或者转而使用异步设置；所有这些都会影响在严格的训练延迟约束下的训练模型性能。在这项工作中，我们提出了关注stragglers的基于层次的联邦学习(SALF)，通过反向传播利用神经网络(NNs)的优化过程以层次方式更新全局模型。SALF允许stragglers同步传递部分梯度，使得全局模型的每一层都可以更新",
    "tldr": "提出了一种基于层次的关注Stragglers的低延迟同步联邦学习方法，允许stragglers同步传递部分梯度，从而改善在训练延迟约束下的模型性能。",
    "en_tdlr": "Proposed a layer-wise stragglers-aware low-latency synchronous federated learning method that allows stragglers to synchronously convey partial gradients, improving model performance under training latency constraints."
}