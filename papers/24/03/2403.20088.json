{
    "title": "An Efficient Approach for Studying Cross-Lingual Transfer in Multilingual Language Models",
    "abstract": "arXiv:2403.20088v1 Announce Type: new  Abstract: The capacity and effectiveness of pre-trained multilingual models (MLMs) for zero-shot cross-lingual transfer is well established. However, phenomena of positive or negative transfer, and the effect of language choice still need to be fully understood, especially in the complex setting of massively multilingual LMs. We propose an \\textit{efficient} method to study transfer language influence in zero-shot performance on another target language. Unlike previous work, our approach disentangles downstream tasks from language, using dedicated adapter units. Our findings suggest that some languages do not largely affect others, while some languages, especially ones unseen during pre-training, can be extremely beneficial or detrimental for different target languages. We find that no transfer language is beneficial for all target languages. We do, curiously, observe languages previously unseen by MLMs consistently benefit from transfer from almo",
    "link": "https://arxiv.org/abs/2403.20088",
    "context": "Title: An Efficient Approach for Studying Cross-Lingual Transfer in Multilingual Language Models\nAbstract: arXiv:2403.20088v1 Announce Type: new  Abstract: The capacity and effectiveness of pre-trained multilingual models (MLMs) for zero-shot cross-lingual transfer is well established. However, phenomena of positive or negative transfer, and the effect of language choice still need to be fully understood, especially in the complex setting of massively multilingual LMs. We propose an \\textit{efficient} method to study transfer language influence in zero-shot performance on another target language. Unlike previous work, our approach disentangles downstream tasks from language, using dedicated adapter units. Our findings suggest that some languages do not largely affect others, while some languages, especially ones unseen during pre-training, can be extremely beneficial or detrimental for different target languages. We find that no transfer language is beneficial for all target languages. We do, curiously, observe languages previously unseen by MLMs consistently benefit from transfer from almo",
    "path": "papers/24/03/2403.20088.json",
    "total_tokens": 975,
    "translated_title": "在多语言语言模型中研究跨语言迁移的高效方法",
    "translated_abstract": "arXiv:2403.20088v1 公告类型：新摘要：众所周知，预训练的多语言模型（MLMs）在零翻译跨语言迁移的容量和效果已经得到确认。然而，正向或负向迁移的现象以及语言选择的影响仍需要得到充分理解，特别是在大规模多语言LMs的复杂环境中。我们提出了一种\\textit {高效}方法来研究零翻译性能上对目标语言的迁移语言影响。与以往工作不同，我们的方法将下游任务与语言分离，使用专用的适配器单元。我们的研究结果表明，一些语言对其他语言影响不大，而一些语言，尤其是在预训练期间未见过的语言，对不同的目标语言可能极为有利或有害。我们发现没有一种迁移语言对所有目标语言都有益。我们奇怪地观察到，此前未被MLMs看到的语言一直受益于来自几乎所有迁移",
    "tldr": "提出了一种高效方法来研究零翻译语言模型在目标语言上的迁移语言影响，发现一些语言对其他语言影响不大，而一些语言对不同目标语言可能极为有利或有害，同时也观察到以前未被MLMs看到的语言始终受益于来自几乎所有迁移。",
    "en_tdlr": "Proposing an efficient method to study the transfer language influence of zero-shot language models on target languages, we find that some languages have minimal impact on others, while others can be extremely beneficial or detrimental for different target languages. Interestingly, languages unseen by MLMs before consistently benefit from transfer."
}