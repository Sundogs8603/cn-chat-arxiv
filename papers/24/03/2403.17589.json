{
    "title": "Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models",
    "abstract": "arXiv:2403.17589v1 Announce Type: cross  Abstract: With the emergence of pre-trained vision-language models like CLIP, how to adapt them to various downstream classification tasks has garnered significant attention in recent research. The adaptation strategies can be typically categorized into three paradigms: zero-shot adaptation, few-shot adaptation, and the recently-proposed training-free few-shot adaptation. Most existing approaches are tailored for a specific setting and can only cater to one or two of these paradigms. In this paper, we introduce a versatile adaptation approach that can effectively work under all three settings. Specifically, we propose the dual memory networks that comprise dynamic and static memory components. The static memory caches training data knowledge, enabling training-free few-shot adaptation, while the dynamic memory preserves historical test features online during the testing process, allowing for the exploration of additional data insights beyond the",
    "link": "https://arxiv.org/abs/2403.17589",
    "context": "Title: Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models\nAbstract: arXiv:2403.17589v1 Announce Type: cross  Abstract: With the emergence of pre-trained vision-language models like CLIP, how to adapt them to various downstream classification tasks has garnered significant attention in recent research. The adaptation strategies can be typically categorized into three paradigms: zero-shot adaptation, few-shot adaptation, and the recently-proposed training-free few-shot adaptation. Most existing approaches are tailored for a specific setting and can only cater to one or two of these paradigms. In this paper, we introduce a versatile adaptation approach that can effectively work under all three settings. Specifically, we propose the dual memory networks that comprise dynamic and static memory components. The static memory caches training data knowledge, enabling training-free few-shot adaptation, while the dynamic memory preserves historical test features online during the testing process, allowing for the exploration of additional data insights beyond the",
    "path": "papers/24/03/2403.17589.json",
    "total_tokens": 824,
    "translated_title": "双存储网络：一种用于视觉与语言模型的多功能适应方法",
    "translated_abstract": "随着像CLIP这样的预训练视觉与语言模型的出现，如何将它们调整到各种下游分类任务已经引起了最近研究的重视。该适应策略通常可以归类为三种范式：零次适应、少次适应和最近提出的无需训练的少次适应。大多数现有方法都是针对特定设置量身定制的，只能满足其中一种或两种范式。本文介绍了一种多功能适应方法，能够有效地在这三种设置下运行。具体地，我们提出了双存储网络，包括动态和静态记忆组件。静态记忆缓存训练数据知识，实现了无需训练的少次适应，而动态记忆在测试过程中在线保存历史测试特征，允许探索超出论文中已训练数据的额外数据洞察",
    "tldr": "提出了双存储网络的多功能适应方法，能在零次适应、少次适应和无需训练的少次适应三种设置下高效运行",
    "en_tdlr": "Proposed a versatile adaptation approach with dual memory networks that can effectively work under zero-shot adaptation, few-shot adaptation, and training-free few-shot adaptation settings."
}