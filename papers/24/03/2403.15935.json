{
    "title": "Sample and Communication Efficient Fully Decentralized MARL Policy Evaluation via a New Approach: Local TD update",
    "abstract": "arXiv:2403.15935v1 Announce Type: new  Abstract: In actor-critic framework for fully decentralized multi-agent reinforcement learning (MARL), one of the key components is the MARL policy evaluation (PE) problem, where a set of $N$ agents work cooperatively to evaluate the value function of the global states for a given policy through communicating with their neighbors. In MARL-PE, a critical challenge is how to lower the sample and communication complexities, which are defined as the number of training samples and communication rounds needed to converge to some $\\epsilon$-stationary point. To lower communication complexity in MARL-PE, a \"natural'' idea is to perform multiple local TD-update steps between each consecutive rounds of communication to reduce the communication frequency. However, the validity of the local TD-update approach remains unclear due to the potential \"agent-drift'' phenomenon resulting from heterogeneous rewards across agents in general. This leads to an interesti",
    "link": "https://arxiv.org/abs/2403.15935",
    "context": "Title: Sample and Communication Efficient Fully Decentralized MARL Policy Evaluation via a New Approach: Local TD update\nAbstract: arXiv:2403.15935v1 Announce Type: new  Abstract: In actor-critic framework for fully decentralized multi-agent reinforcement learning (MARL), one of the key components is the MARL policy evaluation (PE) problem, where a set of $N$ agents work cooperatively to evaluate the value function of the global states for a given policy through communicating with their neighbors. In MARL-PE, a critical challenge is how to lower the sample and communication complexities, which are defined as the number of training samples and communication rounds needed to converge to some $\\epsilon$-stationary point. To lower communication complexity in MARL-PE, a \"natural'' idea is to perform multiple local TD-update steps between each consecutive rounds of communication to reduce the communication frequency. However, the validity of the local TD-update approach remains unclear due to the potential \"agent-drift'' phenomenon resulting from heterogeneous rewards across agents in general. This leads to an interesti",
    "path": "papers/24/03/2403.15935.json",
    "total_tokens": 872,
    "translated_title": "一种新方法：通过本地TD更新实现样本和通讯高效的完全分散式MARL策略评估",
    "translated_abstract": "在完全分散式多智能体强化学习（MARL）的Actor-Critic框架中，MARL策略评估（PE）问题是其中的一个关键组成部分，其中一组$N$个智能体通过与邻居通信合作评估给定策略的全局状态值函数。对于MARL-PE，一个关键挑战是如何降低采样和通讯复杂性，这些复杂性定义为收敛到一些$\\epsilon$-稳定点所需的训练样本数和通讯轮次。为了降低MARL-PE中的通讯复杂性，一个“自然”的想法是在每次连续通讯的轮之间执行多个本地TD更新步骤，以减少通讯频率。然而，由于普遍存在的不同代理之间奖励异质性可能导致的“代理漂移”现象，本地TD更新方法的有效性仍不清楚。这引发了一个有趣的研究问题。",
    "tldr": "通过引入本地TD更新方法，该研究提出了一种新的方法来降低完全分散式MARL策略评估中的通讯和样本复杂性。",
    "en_tdlr": "This paper introduces a new approach to reduce the communication and sample complexity in fully decentralized MARL policy evaluation by incorporating local TD updates."
}