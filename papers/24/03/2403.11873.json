{
    "title": "CO3: Low-resource Contrastive Co-training for Generative Conversational Query Rewrite",
    "abstract": "arXiv:2403.11873v1 Announce Type: new  Abstract: Generative query rewrite generates reconstructed query rewrites using the conversation history while rely heavily on gold rewrite pairs that are expensive to obtain. Recently, few-shot learning is gaining increasing popularity for this task, whereas these methods are sensitive to the inherent noise due to limited data size. Besides, both attempts face performance degradation when there exists language style shift between training and testing cases. To this end, we study low-resource generative conversational query rewrite that is robust to both noise and language style shift. The core idea is to utilize massive unlabeled data to make further improvements via a contrastive co-training paradigm. Specifically, we co-train two dual models (namely Rewriter and Simplifier) such that each of them provides extra guidance through pseudo-labeling for enhancing the other in an iterative manner. We also leverage contrastive learning with data augmen",
    "link": "https://arxiv.org/abs/2403.11873",
    "context": "Title: CO3: Low-resource Contrastive Co-training for Generative Conversational Query Rewrite\nAbstract: arXiv:2403.11873v1 Announce Type: new  Abstract: Generative query rewrite generates reconstructed query rewrites using the conversation history while rely heavily on gold rewrite pairs that are expensive to obtain. Recently, few-shot learning is gaining increasing popularity for this task, whereas these methods are sensitive to the inherent noise due to limited data size. Besides, both attempts face performance degradation when there exists language style shift between training and testing cases. To this end, we study low-resource generative conversational query rewrite that is robust to both noise and language style shift. The core idea is to utilize massive unlabeled data to make further improvements via a contrastive co-training paradigm. Specifically, we co-train two dual models (namely Rewriter and Simplifier) such that each of them provides extra guidance through pseudo-labeling for enhancing the other in an iterative manner. We also leverage contrastive learning with data augmen",
    "path": "papers/24/03/2403.11873.json",
    "total_tokens": 885,
    "translated_title": "CO3: 低资源对比协同训练用于生成对话式查询重写",
    "translated_abstract": "arXiv:2403.11873v1 公告类型：新  摘要：生成式查询重写利用对话历史生成重写查询，但依赖于昂贵的金标重写对，最近，少样本学习在这个任务中越来越受欢迎。而这些方法对由于数据规模有限而产生的固有噪声敏感。此外，当训练和测试情况之间存在语言风格转变时，这两种尝试都会面临性能下降。因此，我们研究了对噪声和语言风格转变都具有鲁棒性的低资源生成式对话式查询重写。其核心思想是利用大量未标记数据，通过对比式协同训练范式进一步改进。具体来说，我们同时训练两个对偶模型（分别为Rewriter和Simplifier），使得它们中的每一个都通过伪标记为另一个提供额外指导，从而以迭代的方式增强另一个。我们还利用对比学习与数据增强进行",
    "tldr": "通过对比协同训练，本研究提出了一种低资源生成式对话式查询重写方法，可以在数据有限、存在噪声和语言风格转变的情况下保持鲁棒性。",
    "en_tdlr": "This paper proposes a low-resource generative conversational query rewrite method using contrastive co-training, which remains robust in scenarios with limited data, noise, and language style shifts."
}