{
    "title": "Disentangling Policy from Offline Task Representation Learning via Adversarial Data Augmentation",
    "abstract": "arXiv:2403.07261v1 Announce Type: cross  Abstract: Offline meta-reinforcement learning (OMRL) proficiently allows an agent to tackle novel tasks while solely relying on a static dataset. For precise and efficient task identification, existing OMRL research suggests learning separate task representations that be incorporated with policy input, thus forming a context-based meta-policy. A major approach to train task representations is to adopt contrastive learning using multi-task offline data. The dataset typically encompasses interactions from various policies (i.e., the behavior policies), thus providing a plethora of contextual information regarding different tasks. Nonetheless, amassing data from a substantial number of policies is not only impractical but also often unattainable in realistic settings. Instead, we resort to a more constrained yet practical scenario, where multi-task data collection occurs with a limited number of policies. We observed that learned task representatio",
    "link": "https://arxiv.org/abs/2403.07261",
    "context": "Title: Disentangling Policy from Offline Task Representation Learning via Adversarial Data Augmentation\nAbstract: arXiv:2403.07261v1 Announce Type: cross  Abstract: Offline meta-reinforcement learning (OMRL) proficiently allows an agent to tackle novel tasks while solely relying on a static dataset. For precise and efficient task identification, existing OMRL research suggests learning separate task representations that be incorporated with policy input, thus forming a context-based meta-policy. A major approach to train task representations is to adopt contrastive learning using multi-task offline data. The dataset typically encompasses interactions from various policies (i.e., the behavior policies), thus providing a plethora of contextual information regarding different tasks. Nonetheless, amassing data from a substantial number of policies is not only impractical but also often unattainable in realistic settings. Instead, we resort to a more constrained yet practical scenario, where multi-task data collection occurs with a limited number of policies. We observed that learned task representatio",
    "path": "papers/24/03/2403.07261.json",
    "total_tokens": 840,
    "translated_title": "通过对抗性数据增强来将策略从离线任务表示学习中解藕",
    "translated_abstract": "离线元强化学习（OMRL）高效地允许一个代理在仅依赖于静态数据集的情况下处理新颖任务。为了精准高效地识别任务，现有的OMRL研究建议学习单独的任务表示，并将其与策略输入结合，从而形成基于上下文的元策略。训练任务表示的主要方法是采用使用多任务离线数据的对比学习。数据集通常涵盖来自各种策略（即行为策略）的相互作用，从而提供了关于不同任务的大量上下文信息。然而，在现实设置中，收集来自大量策略的数据不仅不切实际，而且通常难以实现。相反，我们转而采取更受限制但更实际的情形，即使用有限数量的策略进行多任务数据收集。我们观察到，学习到的任务表示",
    "tldr": "通过对抗性数据增强方法，该研究解决了从有限数量策略中学习任务表示的问题，从而将策略与离线任务表示学习分离。",
    "en_tdlr": "By using adversarial data augmentation, this research addresses the challenge of learning task representations from a limited number of policies, thus separating policy from offline task representation learning."
}