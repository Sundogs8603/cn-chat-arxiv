{
    "title": "SGD with Partial Hessian for Deep Neural Networks Optimization",
    "abstract": "arXiv:2403.02681v1 Announce Type: new  Abstract: Due to the effectiveness of second-order algorithms in solving classical optimization problems, designing second-order optimizers to train deep neural networks (DNNs) has attracted much research interest in recent years. However, because of the very high dimension of intermediate features in DNNs, it is difficult to directly compute and store the Hessian matrix for network optimization. Most of the previous second-order methods approximate the Hessian information imprecisely, resulting in unstable performance. In this work, we propose a compound optimizer, which is a combination of a second-order optimizer with a precise partial Hessian matrix for updating channel-wise parameters and the first-order stochastic gradient descent (SGD) optimizer for updating the other parameters. We show that the associated Hessian matrices of channel-wise parameters are diagonal and can be extracted directly and precisely from Hessian-free methods. The pro",
    "link": "https://arxiv.org/abs/2403.02681",
    "context": "Title: SGD with Partial Hessian for Deep Neural Networks Optimization\nAbstract: arXiv:2403.02681v1 Announce Type: new  Abstract: Due to the effectiveness of second-order algorithms in solving classical optimization problems, designing second-order optimizers to train deep neural networks (DNNs) has attracted much research interest in recent years. However, because of the very high dimension of intermediate features in DNNs, it is difficult to directly compute and store the Hessian matrix for network optimization. Most of the previous second-order methods approximate the Hessian information imprecisely, resulting in unstable performance. In this work, we propose a compound optimizer, which is a combination of a second-order optimizer with a precise partial Hessian matrix for updating channel-wise parameters and the first-order stochastic gradient descent (SGD) optimizer for updating the other parameters. We show that the associated Hessian matrices of channel-wise parameters are diagonal and can be extracted directly and precisely from Hessian-free methods. The pro",
    "path": "papers/24/03/2403.02681.json",
    "total_tokens": 823,
    "translated_title": "使用部分Hessian的SGD优化深度神经网络",
    "translated_abstract": "由于二阶算法在解决经典优化问题方面的有效性，设计二阶优化器来训练深度神经网络(DNNs)近年来吸引了很多研究兴趣。然而，由于DNN中间特征的非常高维度，直接计算和存储Hessian矩阵以进行网络优化是困难的。大多数先前的二阶方法对Hessian信息进行不精确近似，导致性能不稳定。在这项工作中，我们提出了一个复合优化器，它是一个将二阶优化器与用于更新通道参数的精确部分Hessian矩阵以及用于更新其他参数的一阶随机梯度下降(SGD)优化器相结合的优化器。我们证明了通道参数的相关Hessian矩阵是对角线型的，并且可以直接且精确地从无Hessian方法中提取。",
    "tldr": "提出了一种深度神经网络优化方法，结合了部分Hessian信息的二阶优化器和一阶随机梯度下降(SGD)优化器，从而提高了性能稳定性"
}