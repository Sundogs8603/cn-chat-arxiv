{
    "title": "Policy Bifurcation in Safe Reinforcement Learning",
    "abstract": "arXiv:2403.12847v1 Announce Type: new  Abstract: Safe reinforcement learning (RL) offers advanced solutions to constrained optimal control problems. Existing studies in safe RL implicitly assume continuity in policy functions, where policies map states to actions in a smooth, uninterrupted manner; however, our research finds that in some scenarios, the feasible policy should be discontinuous or multi-valued, interpolating between discontinuous local optima can inevitably lead to constraint violations. We are the first to identify the generating mechanism of such a phenomenon, and employ topological analysis to rigorously prove the existence of policy bifurcation in safe RL, which corresponds to the contractibility of the reachable tuple. Our theorem reveals that in scenarios where the obstacle-free state space is non-simply connected, a feasible policy is required to be bifurcated, meaning its output action needs to change abruptly in response to the varying state. To train such a bifu",
    "link": "https://arxiv.org/abs/2403.12847",
    "context": "Title: Policy Bifurcation in Safe Reinforcement Learning\nAbstract: arXiv:2403.12847v1 Announce Type: new  Abstract: Safe reinforcement learning (RL) offers advanced solutions to constrained optimal control problems. Existing studies in safe RL implicitly assume continuity in policy functions, where policies map states to actions in a smooth, uninterrupted manner; however, our research finds that in some scenarios, the feasible policy should be discontinuous or multi-valued, interpolating between discontinuous local optima can inevitably lead to constraint violations. We are the first to identify the generating mechanism of such a phenomenon, and employ topological analysis to rigorously prove the existence of policy bifurcation in safe RL, which corresponds to the contractibility of the reachable tuple. Our theorem reveals that in scenarios where the obstacle-free state space is non-simply connected, a feasible policy is required to be bifurcated, meaning its output action needs to change abruptly in response to the varying state. To train such a bifu",
    "path": "papers/24/03/2403.12847.json",
    "total_tokens": 913,
    "translated_title": "安全强化学习中的策略分叉",
    "translated_abstract": "安全强化学习为受限最优控制问题提供了先进的解决方案。现有的安全强化学习研究隐含地假设策略函数具有连续性，即策略以平稳、连续的方式将状态映射到动作；然而，我们的研究发现在某些情况下，可行策略应该是不连续或多值的，而在不连续的局部极小值之间插值可能会不可避免地导致约束违规。我们是第一个识别出这种现象生成机制的研究，并采用拓扑分析严谨地证明了安全强化学习中策略分叉的存在，这对应于可达元组的可收缩性。我们的定理揭示了在障碍物自由状态空间为非单连通的情景中，需要策略分叉，意味着其输出动作需要迅速响应状态的变化。",
    "tldr": "我们的研究发现在安全强化学习中可能存在策略分叉现象，提出了拓扑分析以证明在一些情景下，策略需要具有不连续性或多值性，这对应于障碍物自由状态空间为非单连通时需要策略分叉的情况。",
    "en_tdlr": "Our research identifies the phenomenon of policy bifurcation in safe reinforcement learning, presenting topological analysis to demonstrate that in some scenarios policies may need to be discontinuous or multi-valued, corresponding to the requirement for policy bifurcation when the obstacle-free state space is non-simply connected."
}