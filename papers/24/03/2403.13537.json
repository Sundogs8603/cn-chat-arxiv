{
    "title": "What explains the success of cross-modal fine-tuning with ORCA?",
    "abstract": "arXiv:2403.13537v1 Announce Type: new  Abstract: ORCA (Shen et al., 2023) is a recent technique for cross-modal fine-tuning, i.e., applying pre-trained transformer models to modalities beyond their training data. The technique consists primarily of training an embedder and fine-tuning the embedder and model. Despite its high performance on a variety of downstream tasks, we do not understand precisely how each of these components contribute to ORCA's success. Therefore, we run a series of ablations and find that embedder training does not help 2D tasks at all, contrary to what the original paper posits. In 1D tasks, some amount of embedder training is necessary but more is not better. In 4 out of 6 datasets we experiment with, it is model fine-tuning that makes the biggest difference. Through our ablations and baselines, we contribute a better understanding of the individual components of ORCA.",
    "link": "https://arxiv.org/abs/2403.13537",
    "context": "Title: What explains the success of cross-modal fine-tuning with ORCA?\nAbstract: arXiv:2403.13537v1 Announce Type: new  Abstract: ORCA (Shen et al., 2023) is a recent technique for cross-modal fine-tuning, i.e., applying pre-trained transformer models to modalities beyond their training data. The technique consists primarily of training an embedder and fine-tuning the embedder and model. Despite its high performance on a variety of downstream tasks, we do not understand precisely how each of these components contribute to ORCA's success. Therefore, we run a series of ablations and find that embedder training does not help 2D tasks at all, contrary to what the original paper posits. In 1D tasks, some amount of embedder training is necessary but more is not better. In 4 out of 6 datasets we experiment with, it is model fine-tuning that makes the biggest difference. Through our ablations and baselines, we contribute a better understanding of the individual components of ORCA.",
    "path": "papers/24/03/2403.13537.json",
    "total_tokens": 868,
    "translated_title": "解释ORCA交叉模态微调成功的因素是什么？",
    "translated_abstract": "ORCA（Shen等人，2023）是一种最近的交叉模态微调技术，即将预训练的转换器模型应用于其训练数据之外的模态。 该技术主要包括训练一个嵌入器并微调嵌入器和模型。 尽管它在各种下游任务上表现出色，但我们并不确切了解这些组件中的每个如何促成ORCA的成功。 因此，我们进行了一系列消融实验，并发现嵌入器训练对2D任务毫无帮助，与原始论文所言相反。 在1D任务中，一定量的嵌入器训练是必要的，但更多并非总是更好。 在我们尝试的6个数据集中的4个数据集中，模型微调产生了最大的差异。 通过我们的消融实验和基线，我们对ORCA的各个组件有了更好的理解。",
    "tldr": "通过一系列消融实验，确定了ORCA中嵌入器训练对2D任务无帮助、1D任务需要适量嵌入器训练、以及模型微调对性能影响最大的结论。"
}