{
    "title": "Beyond Inference: Performance Analysis of DNN Server Overheads for Computer Vision",
    "abstract": "arXiv:2403.12981v1 Announce Type: cross  Abstract: Deep neural network (DNN) inference has become an important part of many data-center workloads. This has prompted focused efforts to design ever-faster deep learning accelerators such as GPUs and TPUs. However, an end-to-end DNN-based vision application contains more than just DNN inference, including input decompression, resizing, sampling, normalization, and data transfer. In this paper, we perform a thorough evaluation of computer vision inference requests performed on a throughput-optimized serving system. We quantify the performance impact of server overheads such as data movement, preprocessing, and message brokers between two DNNs producing outputs at different rates. Our empirical analysis encompasses many computer vision tasks including image classification, segmentation, detection, depth-estimation, and more complex processing pipelines with multiple DNNs. Our results consistently demonstrate that end-to-end application perfo",
    "link": "https://arxiv.org/abs/2403.12981",
    "context": "Title: Beyond Inference: Performance Analysis of DNN Server Overheads for Computer Vision\nAbstract: arXiv:2403.12981v1 Announce Type: cross  Abstract: Deep neural network (DNN) inference has become an important part of many data-center workloads. This has prompted focused efforts to design ever-faster deep learning accelerators such as GPUs and TPUs. However, an end-to-end DNN-based vision application contains more than just DNN inference, including input decompression, resizing, sampling, normalization, and data transfer. In this paper, we perform a thorough evaluation of computer vision inference requests performed on a throughput-optimized serving system. We quantify the performance impact of server overheads such as data movement, preprocessing, and message brokers between two DNNs producing outputs at different rates. Our empirical analysis encompasses many computer vision tasks including image classification, segmentation, detection, depth-estimation, and more complex processing pipelines with multiple DNNs. Our results consistently demonstrate that end-to-end application perfo",
    "path": "papers/24/03/2403.12981.json",
    "total_tokens": 699,
    "translated_title": "超越推断：计算机视觉中DNN服务器开销的性能分析",
    "translated_abstract": "深度神经网络（DNN）推断已成为许多数据中心工作负载的重要组成部分。我们对在吞吐量优化的服务器系统上执行的计算机视觉推断请求进行了彻底评估。我们量化了服务器开销（如数据移动、预处理和两个以不同速率产生输出的DNN之间的消息代理）对性能的影响。我们的实证分析涵盖了许多计算机视觉任务，包括图像分类、分割、检测、深度估计以及具有多个DNN的复杂处理管线。",
    "tldr": "对计算机视觉推断请求的性能影响进行了全面评估，量化了服务器开销对性能的影响。",
    "en_tdlr": "Thorough analysis of the performance impact of server overheads on computer vision inference requests, quantifying their influence on performance."
}