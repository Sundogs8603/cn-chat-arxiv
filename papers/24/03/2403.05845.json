{
    "title": "Reverse That Number! Decoding Order Matters in Arithmetic Learning",
    "abstract": "arXiv:2403.05845v1 Announce Type: cross  Abstract: Recent advancements in pretraining have demonstrated that modern Large Language Models (LLMs) possess the capability to effectively learn arithmetic operations. However, despite acknowledging the significance of digit order in arithmetic computation, current methodologies predominantly rely on sequential, step-by-step approaches for teaching LLMs arithmetic, resulting in a conclusion where obtaining better performance involves fine-grained step-by-step. Diverging from this conventional path, our work introduces a novel strategy that not only reevaluates the digit order by prioritizing output from the least significant digit but also incorporates a step-by-step methodology to substantially reduce complexity. We have developed and applied this method in a comprehensive set of experiments. Compared to the previous state-of-the-art (SOTA) method, our findings reveal an overall improvement of in accuracy while requiring only a third of the ",
    "link": "https://arxiv.org/abs/2403.05845",
    "context": "Title: Reverse That Number! Decoding Order Matters in Arithmetic Learning\nAbstract: arXiv:2403.05845v1 Announce Type: cross  Abstract: Recent advancements in pretraining have demonstrated that modern Large Language Models (LLMs) possess the capability to effectively learn arithmetic operations. However, despite acknowledging the significance of digit order in arithmetic computation, current methodologies predominantly rely on sequential, step-by-step approaches for teaching LLMs arithmetic, resulting in a conclusion where obtaining better performance involves fine-grained step-by-step. Diverging from this conventional path, our work introduces a novel strategy that not only reevaluates the digit order by prioritizing output from the least significant digit but also incorporates a step-by-step methodology to substantially reduce complexity. We have developed and applied this method in a comprehensive set of experiments. Compared to the previous state-of-the-art (SOTA) method, our findings reveal an overall improvement of in accuracy while requiring only a third of the ",
    "path": "papers/24/03/2403.05845.json",
    "total_tokens": 841,
    "translated_title": "数字反转！算术学习中顺序解码很重要",
    "translated_abstract": "最近预训练的最新进展表明，现代大型语言模型（LLMs）具有有效学习算术操作的能力。然而，尽管承认数字顺序在算术计算中的重要性，但目前的方法主要依赖于顺序、逐步的方法来教授LLMs算术，导致结论是获得更好的性能涉及到精细的逐步操作。与传统路径不同，我们的工作引入了一种新颖的策略，不仅通过优先考虑从最低有效数字输出来重新评估数字顺序，还结合逐步方法大幅减少了复杂性。我们已经在全面的一系列实验中开发并应用了这种方法。与先前的最先进（SOTA）方法相比，我们的研究结果显示总体准确度的提高，同时仅需要三分之一的复杂度。",
    "tldr": "本研究提出了一种新颖的算术学习策略，重点考虑最低有效数字的输出，重新评估数字顺序，并结合逐步方法大幅减少了复杂性，从而取得了比之前方法更好的性能提升。",
    "en_tdlr": "This study introduces a novel arithmetic learning strategy that prioritizes the output from the least significant digit, reevaluates digit order, and substantially reduces complexity through step-by-step approach, achieving better performance compared to previous methods."
}