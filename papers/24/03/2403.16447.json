{
    "title": "A Study on How Attention Scores in the BERT Model are Aware of Lexical Categories in Syntactic and Semantic Tasks on the GLUE Benchmark",
    "abstract": "arXiv:2403.16447v1 Announce Type: new  Abstract: This study examines whether the attention scores between tokens in the BERT model significantly vary based on lexical categories during the fine-tuning process for downstream tasks. Drawing inspiration from the notion that in human language processing, syntactic and semantic information is parsed differently, we categorize tokens in sentences according to their lexical categories and focus on changes in attention scores among these categories. Our hypothesis posits that in downstream tasks that prioritize semantic information, attention scores centered on content words are enhanced, while in cases emphasizing syntactic information, attention scores centered on function words are intensified. Through experimentation conducted on six tasks from the GLUE benchmark dataset, we substantiate our hypothesis regarding the fine-tuning process. Furthermore, our additional investigations reveal the presence of BERT layers that consistently assign m",
    "link": "https://arxiv.org/abs/2403.16447",
    "context": "Title: A Study on How Attention Scores in the BERT Model are Aware of Lexical Categories in Syntactic and Semantic Tasks on the GLUE Benchmark\nAbstract: arXiv:2403.16447v1 Announce Type: new  Abstract: This study examines whether the attention scores between tokens in the BERT model significantly vary based on lexical categories during the fine-tuning process for downstream tasks. Drawing inspiration from the notion that in human language processing, syntactic and semantic information is parsed differently, we categorize tokens in sentences according to their lexical categories and focus on changes in attention scores among these categories. Our hypothesis posits that in downstream tasks that prioritize semantic information, attention scores centered on content words are enhanced, while in cases emphasizing syntactic information, attention scores centered on function words are intensified. Through experimentation conducted on six tasks from the GLUE benchmark dataset, we substantiate our hypothesis regarding the fine-tuning process. Furthermore, our additional investigations reveal the presence of BERT layers that consistently assign m",
    "path": "papers/24/03/2403.16447.json",
    "total_tokens": 880,
    "translated_title": "研究BERT模型中的注意力分数如何感知GLUE基准测试中的句法和语义任务中的词汇类别",
    "translated_abstract": "该研究检查了在BERT模型中，token之间的注意力分数在下游任务的微调过程中是否根据词汇类别显着变化。受到人类语言处理中句法和语义信息被不同解析的概念启发，我们根据其词汇类别对句子中的token进行分类，并关注这些类别之间注意力分数的变化。我们的假设认为，在注重语义信息的下游任务中，以内容词为中心的注意力分数会增强，而在强调句法信息的情况下，以功能词为中心的注意力分数会增强。通过对GLUE基准数据集中的六个任务进行实验，我们证实了关于微调过程的假设。此外，我们的其他调查揭示了BERT层会一致分配 m",
    "tldr": "本研究探讨了BERT模型中的注意力分数如何根据词汇类别的不同而变化，在GLUE基准测试下的句法和语义任务中，证实在强调语义信息的任务中，注意力主要集中于内容词，而在强调句法信息的任务中，注意力主要集中在功能词上",
    "en_tdlr": "This study examines how attention scores in the BERT model vary based on lexical categories in syntactic and semantic tasks on the GLUE benchmark, confirming that in tasks emphasizing semantic information, attention is focused on content words, while in tasks emphasizing syntactic information, attention is focused on function words."
}