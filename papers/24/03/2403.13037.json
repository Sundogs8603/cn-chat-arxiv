{
    "title": "BiLoRA: A Bi-level Optimization Framework for Overfitting-Resilient Low-Rank Adaptation of Large Pre-trained Models",
    "abstract": "arXiv:2403.13037v1 Announce Type: cross  Abstract: Low-rank adaptation (LoRA) is a popular method for fine-tuning large-scale pre-trained models in downstream tasks by learning low-rank incremental matrices. Though LoRA and its variants effectively reduce the number of trainable parameters compared to full fine-tuning methods, they often overfit training data, resulting in sub-optimal generalization on test data. To address this problem, we introduce BiLoRA, an overfitting-alleviating fine-tuning approach based on bi-level optimization (BLO). BiLoRA employs pseudo singular value decomposition to parameterize low-rank incremental matrices and splits the training of pseudo singular vectors and values across two different subsets of training data. This division, embedded within separate levels of the BLO framework, mitigates the risk of overfitting to a single dataset. Tested on ten datasets covering natural language understanding and generation tasks and applied to various well-known lar",
    "link": "https://arxiv.org/abs/2403.13037",
    "context": "Title: BiLoRA: A Bi-level Optimization Framework for Overfitting-Resilient Low-Rank Adaptation of Large Pre-trained Models\nAbstract: arXiv:2403.13037v1 Announce Type: cross  Abstract: Low-rank adaptation (LoRA) is a popular method for fine-tuning large-scale pre-trained models in downstream tasks by learning low-rank incremental matrices. Though LoRA and its variants effectively reduce the number of trainable parameters compared to full fine-tuning methods, they often overfit training data, resulting in sub-optimal generalization on test data. To address this problem, we introduce BiLoRA, an overfitting-alleviating fine-tuning approach based on bi-level optimization (BLO). BiLoRA employs pseudo singular value decomposition to parameterize low-rank incremental matrices and splits the training of pseudo singular vectors and values across two different subsets of training data. This division, embedded within separate levels of the BLO framework, mitigates the risk of overfitting to a single dataset. Tested on ten datasets covering natural language understanding and generation tasks and applied to various well-known lar",
    "path": "papers/24/03/2403.13037.json",
    "total_tokens": 958,
    "translated_title": "BiLoRA：一种面向大型预训练模型的过度拟合鲁棒低秩适应的双层优化框架",
    "translated_abstract": "低秩适应（LoRA）是一种用于微调大规模预训练模型以解决下游任务的流行方法，通过学习低秩增量矩阵。尽管LoRA及其变体相对于完全微调方法有效地减少了可训练参数的数量，但它们经常会在训练数据上过拟合，导致在测试数据上的次优泛化。为解决这一问题，我们介绍了BiLoRA，一种基于双层优化（BLO）的减轻过拟合微调方法。BiLoRA采用伪奇异值分解来参数化低秩增量矩阵，并将伪奇异向量和值的训练分成两个不同的训练数据子集。这种划分嵌入在BLO框架的不同层次中，有助于减轻对单一数据集过度拟合的风险。在涵盖自然语言理解和生成任务的十个数据集上进行测试，并应用于各种知名的大型预训练模型以验证其有效性。",
    "tldr": "BiLoRA提出了一种基于双层优化框架的减轻过拟合的微调方法，通过对低秩增量矩阵进行参数化和将训练分为不同的子集，降低了对单一数据集过拟合的风险。"
}