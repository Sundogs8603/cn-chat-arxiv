{
    "title": "Consecutive Model Editing with Batch alongside HooK Layers",
    "abstract": "arXiv:2403.05330v1 Announce Type: new  Abstract: As the typical retraining paradigm is unacceptably time- and resource-consuming, researchers are turning to model editing in order to seek an effective, consecutive, and batch-supportive way to edit the model behavior directly. Despite all these practical expectations, existing model editing methods fail to realize all of them. Furthermore, the memory demands for such succession-supportive model editing approaches tend to be prohibitive, frequently necessitating an external memory that grows incrementally over time. To cope with these challenges, we propose COMEBA-HK, a model editing method that is both consecutive and batch-supportive. COMEBA-HK is memory-friendly as it only needs a small amount of it to store several hook layers with updated weights. Experimental results demonstrate the superiority of our method over other batch-supportive model editing methods under both single-round and consecutive batch editing scenarios. Extensive ",
    "link": "https://arxiv.org/abs/2403.05330",
    "context": "Title: Consecutive Model Editing with Batch alongside HooK Layers\nAbstract: arXiv:2403.05330v1 Announce Type: new  Abstract: As the typical retraining paradigm is unacceptably time- and resource-consuming, researchers are turning to model editing in order to seek an effective, consecutive, and batch-supportive way to edit the model behavior directly. Despite all these practical expectations, existing model editing methods fail to realize all of them. Furthermore, the memory demands for such succession-supportive model editing approaches tend to be prohibitive, frequently necessitating an external memory that grows incrementally over time. To cope with these challenges, we propose COMEBA-HK, a model editing method that is both consecutive and batch-supportive. COMEBA-HK is memory-friendly as it only needs a small amount of it to store several hook layers with updated weights. Experimental results demonstrate the superiority of our method over other batch-supportive model editing methods under both single-round and consecutive batch editing scenarios. Extensive ",
    "path": "papers/24/03/2403.05330.json",
    "total_tokens": 797,
    "translated_title": "连续模型编辑与批量支持的HooK层",
    "translated_abstract": "由于典型的重新训练范式耗时且消耗资源，研究人员正在转向模型编辑，以寻找一种有效的、连续的、并支持批量方式直接编辑模型行为的方法。然而，尽管存在所有这些实用期望，现有的模型编辑方法却未能实现所有这些目标。此外，对于这种支持连续性模型编辑方法的内存需求往往是禁止性的，经常需要随着时间的增长逐步增加外部内存。为了应对这些挑战，我们提出了一种名为COMEBA-HK的模型编辑方法，该方法既是连续的又支持批量。COMEBA-HK对于存储几个具有更新权重的hook层仅需少量内存，是内存友好的。实验结果表明，我们的方法在单轮和连续批量编辑场景下优于其他支持批量模型编辑方法。",
    "tldr": "提出了一种内存友好的连续模型编辑与批量支持的方法COMEBA-HK，在实验中表现出优越性。",
    "en_tdlr": "Proposed a memory-friendly method COMEBA-HK for consecutive model editing with batch support, demonstrating superiority in experiments."
}