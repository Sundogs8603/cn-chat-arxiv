{
    "title": "CLHA: A Simple yet Effective Contrastive Learning Framework for Human Alignment",
    "abstract": "arXiv:2403.16649v1 Announce Type: new  Abstract: Reinforcement learning from human feedback (RLHF) is a crucial technique in aligning large language models (LLMs) with human preferences, ensuring these LLMs behave in beneficial and comprehensible ways to users. However, a longstanding challenge in human alignment techniques based on reinforcement learning lies in their inherent complexity and difficulty in training. To address this challenge, we present a simple yet effective Contrastive Learning Framework for Human Alignment (CLHA) to align LLMs with human preferences directly. CLHA employs a novel rescoring strategy to evaluate the noise within the data by considering its inherent quality and dynamically adjusting the training process. Simultaneously, CLHA utilizes pairwise contrastive loss and adaptive supervised fine-tuning loss to adaptively modify the likelihood of generating responses, ensuring enhanced alignment with human preferences. Using advanced methods, CLHA surpasses oth",
    "link": "https://arxiv.org/abs/2403.16649",
    "context": "Title: CLHA: A Simple yet Effective Contrastive Learning Framework for Human Alignment\nAbstract: arXiv:2403.16649v1 Announce Type: new  Abstract: Reinforcement learning from human feedback (RLHF) is a crucial technique in aligning large language models (LLMs) with human preferences, ensuring these LLMs behave in beneficial and comprehensible ways to users. However, a longstanding challenge in human alignment techniques based on reinforcement learning lies in their inherent complexity and difficulty in training. To address this challenge, we present a simple yet effective Contrastive Learning Framework for Human Alignment (CLHA) to align LLMs with human preferences directly. CLHA employs a novel rescoring strategy to evaluate the noise within the data by considering its inherent quality and dynamically adjusting the training process. Simultaneously, CLHA utilizes pairwise contrastive loss and adaptive supervised fine-tuning loss to adaptively modify the likelihood of generating responses, ensuring enhanced alignment with human preferences. Using advanced methods, CLHA surpasses oth",
    "path": "papers/24/03/2403.16649.json",
    "total_tokens": 885,
    "translated_title": "CLHA: 一种简单而有效的用于人类对齐的对比学习框架",
    "translated_abstract": "人类反馈（RLHF）的强化学习是将大型语言模型（LLMs）与人类偏好对齐的关键技术，确保这些LLMs以对用户有益且易于理解的方式行为。然而，基于RL的人类对齐技术中存在的长期挑战在于其固有的复杂性和训练的困难。为了解决这一挑战，我们提出了一种简单而有效的用于人类对齐的对比学习框架（CLHA），直接将LLMs与人类偏好对齐。CLHA采用一种新颖的重评分策略来评估数据内的噪声，考虑其固有质量并动态调整训练过程。同时，CLHA利用成对对比损失和自适应监督微调损失来自适应地修改生成响应的可能性，确保与人类偏好的增强对齐。使用先进方法，CLHA超越了其他",
    "tldr": "CLHA提出了一种简单而有效的对比学习框架，可以帮助大型语言模型与人类偏好对齐，通过新颖的重评分策略和损失函数调整，在提升对齐效果的同时简化了训练过程。",
    "en_tdlr": "CLHA introduces a simple yet effective contrastive learning framework to align large language models with human preferences, enhancing alignment while simplifying training through innovative rescoring strategy and loss functions."
}