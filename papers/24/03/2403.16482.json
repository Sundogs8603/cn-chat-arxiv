{
    "title": "Determined Multi-Label Learning via Similarity-Based Prompt",
    "abstract": "arXiv:2403.16482v1 Announce Type: new  Abstract: In multi-label classification, each training instance is associated with multiple class labels simultaneously. Unfortunately, collecting the fully precise class labels for each training instance is time- and labor-consuming for real-world applications. To alleviate this problem, a novel labeling setting termed \\textit{Determined Multi-Label Learning} (DMLL) is proposed, aiming to effectively alleviate the labeling cost inherent in multi-label tasks. In this novel labeling setting, each training instance is associated with a \\textit{determined label} (either \"Yes\" or \"No\"), which indicates whether the training instance contains the provided class label. The provided class label is randomly and uniformly selected from the whole candidate labels set. Besides, each training instance only need to be determined once, which significantly reduce the annotation cost of the labeling task for multi-label datasets. In this paper, we theoretically de",
    "link": "https://arxiv.org/abs/2403.16482",
    "context": "Title: Determined Multi-Label Learning via Similarity-Based Prompt\nAbstract: arXiv:2403.16482v1 Announce Type: new  Abstract: In multi-label classification, each training instance is associated with multiple class labels simultaneously. Unfortunately, collecting the fully precise class labels for each training instance is time- and labor-consuming for real-world applications. To alleviate this problem, a novel labeling setting termed \\textit{Determined Multi-Label Learning} (DMLL) is proposed, aiming to effectively alleviate the labeling cost inherent in multi-label tasks. In this novel labeling setting, each training instance is associated with a \\textit{determined label} (either \"Yes\" or \"No\"), which indicates whether the training instance contains the provided class label. The provided class label is randomly and uniformly selected from the whole candidate labels set. Besides, each training instance only need to be determined once, which significantly reduce the annotation cost of the labeling task for multi-label datasets. In this paper, we theoretically de",
    "path": "papers/24/03/2403.16482.json",
    "total_tokens": 785,
    "translated_title": "基于相似性提示的确定性多标签学习",
    "translated_abstract": "在多标签分类中，每个训练实例同时与多个类标签相关联。然而，为每个训练实例收集完全精确的类标签对于现实世界的应用来说是耗时且耗力的。为了缓解这一问题，提出了一种名为“Determined Multi-Label Learning”（DMLL）的新的标注设置，旨在有效减少多标签任务中固有的标注成本。在这种新的标注设置中，每个训练实例与一个“确定性标签”（“是”或“否”）相关联，表示该训练实例是否包含所提供的类标签。所提供的类标签从整个候选标签集中随机均匀选择。此外，每个训练实例只需确定一次，这显著降低了多标签数据集标注任务的成本。",
    "tldr": "提出了一种名为“Determined Multi-Label Learning”（DMLL）的新的标注设置，旨在有效减少多标签任务中固有的标注成本。",
    "en_tdlr": "Introducing a novel labeling setting called \"Determined Multi-Label Learning\" (DMLL) to effectively reduce the inherent labeling cost in multi-label tasks."
}