{
    "title": "Nonsmooth Implicit Differentiation: Deterministic and Stochastic Convergence Rates",
    "abstract": "arXiv:2403.11687v1 Announce Type: cross  Abstract: We study the problem of efficiently computing the derivative of the fixed-point of a parametric non-differentiable contraction map. This problem has wide applications in machine learning, including hyperparameter optimization, meta-learning and data poisoning attacks. We analyze two popular approaches: iterative differentiation (ITD) and approximate implicit differentiation (AID). A key challenge behind the nonsmooth setting is that the chain rule does not hold anymore. Building upon the recent work by Bolte et al. (2022), who proved the linear convergence of non-differentiable ITD, we provide refined linear convergence rates for both ITD and AID in the deterministic case. We further introduce NSID, a new method to compute the implicit derivative when the fixed point is defined as the composition of an outer map and an inner map which is accessible only through a stochastic unbiased estimator. We establish rates for the convergence of ",
    "link": "https://arxiv.org/abs/2403.11687",
    "context": "Title: Nonsmooth Implicit Differentiation: Deterministic and Stochastic Convergence Rates\nAbstract: arXiv:2403.11687v1 Announce Type: cross  Abstract: We study the problem of efficiently computing the derivative of the fixed-point of a parametric non-differentiable contraction map. This problem has wide applications in machine learning, including hyperparameter optimization, meta-learning and data poisoning attacks. We analyze two popular approaches: iterative differentiation (ITD) and approximate implicit differentiation (AID). A key challenge behind the nonsmooth setting is that the chain rule does not hold anymore. Building upon the recent work by Bolte et al. (2022), who proved the linear convergence of non-differentiable ITD, we provide refined linear convergence rates for both ITD and AID in the deterministic case. We further introduce NSID, a new method to compute the implicit derivative when the fixed point is defined as the composition of an outer map and an inner map which is accessible only through a stochastic unbiased estimator. We establish rates for the convergence of ",
    "path": "papers/24/03/2403.11687.json",
    "total_tokens": 891,
    "translated_title": "非光滑隐式微分：确定性和随机收敛速率",
    "translated_abstract": "我们研究了有效计算参数化不可微收缩映射固定点导数的问题。这个问题在机器学习中有广泛的应用，包括超参数优化、元学习和数据污染攻击。我们分析了两种流行的方法：迭代微分（ITD）和近似隐式微分（AID）。在非光滑设置中的一个关键挑战是链规则不再成立。在Bolte等人（2022）最近的工作基础上，他们证明了不可微分ITD的线性收敛，我们提供了确定性情况下ITD和AID的改进线性收敛速率。我们进一步介绍了NSID，一种新的方法，用于在固定点被定义为只通过随机无偏估计器访问的外映射和内映射的组合时计算隐式导数。我们建立了该方法的收敛速率。",
    "tldr": "在非光滑设置下，提出了用于计算具有内映射的外映射固定点的隐式导数的新方法NSID，并提供了确定性情况下迭代微分（ITD）和近似隐式微分（AID）的改进线性收敛速率。"
}