{
    "title": "What Makes Quantization for Large Language Models Hard? An Empirical Study from the Lens of Perturbation",
    "abstract": "arXiv:2403.06408v1 Announce Type: cross  Abstract: Quantization has emerged as a promising technique for improving the memory and computational efficiency of large language models (LLMs). Though the trade-off between performance and efficiency is well-known, there is still much to be learned about the relationship between quantization and LLM performance. To shed light on this relationship, we propose a new perspective on quantization, viewing it as perturbations added to the weights and activations of LLMs. We call this approach \"the lens of perturbation\". Using this lens, we conduct experiments with various artificial perturbations to explore their impact on LLM performance. Our findings reveal several connections between the properties of perturbations and LLM performance, providing insights into the failure cases of uniform quantization and suggesting potential solutions to improve the robustness of LLM quantization. To demonstrate the significance of our findings, we implement a s",
    "link": "https://arxiv.org/abs/2403.06408",
    "context": "Title: What Makes Quantization for Large Language Models Hard? An Empirical Study from the Lens of Perturbation\nAbstract: arXiv:2403.06408v1 Announce Type: cross  Abstract: Quantization has emerged as a promising technique for improving the memory and computational efficiency of large language models (LLMs). Though the trade-off between performance and efficiency is well-known, there is still much to be learned about the relationship between quantization and LLM performance. To shed light on this relationship, we propose a new perspective on quantization, viewing it as perturbations added to the weights and activations of LLMs. We call this approach \"the lens of perturbation\". Using this lens, we conduct experiments with various artificial perturbations to explore their impact on LLM performance. Our findings reveal several connections between the properties of perturbations and LLM performance, providing insights into the failure cases of uniform quantization and suggesting potential solutions to improve the robustness of LLM quantization. To demonstrate the significance of our findings, we implement a s",
    "path": "papers/24/03/2403.06408.json",
    "total_tokens": 891,
    "translated_title": "量化对大型语言模型的困难在哪里？基于扰动视角的实证研究",
    "translated_abstract": "量化已经成为提高大型语言模型（LLMs）内存和计算效率的一种有前途的技术。尽管性能和效率之间的权衡是众所周知的，但关于量化与LLM性能之间的关系仍有很多待探索。为了阐明这种关系，我们提出了一种量化新视角，将其视为添加到LLMs权重和激活上的扰动。我们称这种方法为“扰动视角”。利用这一视角，我们进行了各种人为扰动的实验，探讨它们对LLM性能的影响。我们的研究结果揭示了扰动的特性与LLM性能之间的几个联系，为均匀量化的失败案例提供了见解，并暗示了改善LLM量化稳健性的潜在解决方案。",
    "tldr": "量化对大型语言模型的困难主要表现在如何通过添加扰动来改善模型性能和效率的关系，研究通过对不同人为扰动进行实验，发现了扰动特性与模型性能之间的联系，提出了改善量化稳健性的潜在解决方案。",
    "en_tdlr": "The difficulty of quantization for large language models lies in the relationship between improving model performance and efficiency by adding perturbations, with experiments on various artificial perturbations revealing connections between perturbation properties and model performance, suggesting potential solutions to enhance quantization robustness."
}