{
    "title": "Unsupervised Audio-Visual Segmentation with Modality Alignment",
    "abstract": "arXiv:2403.14203v1 Announce Type: cross  Abstract: Audio-Visual Segmentation (AVS) aims to identify, at the pixel level, the object in a visual scene that produces a given sound. Current AVS methods rely on costly fine-grained annotations of mask-audio pairs, making them impractical for scalability. To address this, we introduce unsupervised AVS, eliminating the need for such expensive annotation. To tackle this more challenging problem, we propose an unsupervised learning method, named Modality Correspondence Alignment (MoCA), which seamlessly integrates off-the-shelf foundation models like DINO, SAM, and ImageBind. This approach leverages their knowledge complementarity and optimizes their joint usage for multi-modality association. Initially, we estimate positive and negative image pairs in the feature space. For pixel-level association, we introduce an audio-visual adapter and a novel pixel matching aggregation strategy within the image-level contrastive learning framework. This al",
    "link": "https://arxiv.org/abs/2403.14203",
    "context": "Title: Unsupervised Audio-Visual Segmentation with Modality Alignment\nAbstract: arXiv:2403.14203v1 Announce Type: cross  Abstract: Audio-Visual Segmentation (AVS) aims to identify, at the pixel level, the object in a visual scene that produces a given sound. Current AVS methods rely on costly fine-grained annotations of mask-audio pairs, making them impractical for scalability. To address this, we introduce unsupervised AVS, eliminating the need for such expensive annotation. To tackle this more challenging problem, we propose an unsupervised learning method, named Modality Correspondence Alignment (MoCA), which seamlessly integrates off-the-shelf foundation models like DINO, SAM, and ImageBind. This approach leverages their knowledge complementarity and optimizes their joint usage for multi-modality association. Initially, we estimate positive and negative image pairs in the feature space. For pixel-level association, we introduce an audio-visual adapter and a novel pixel matching aggregation strategy within the image-level contrastive learning framework. This al",
    "path": "papers/24/03/2403.14203.json",
    "total_tokens": 902,
    "translated_title": "无监督音频-视觉分割与模态对齐",
    "translated_abstract": "音频-视觉分割（AVS）旨在识别在视觉场景中产生特定声音的对象，这一研究在像素级别进行。当前AVS方法依赖于昂贵的精细标注的掩码-音频对，这使得它们在可扩展性方面不切实际。为解决这一问题，我们引入了无监督AVS，消除了这种昂贵标注的必要性。为解决这个更具挑战性的问题，我们提出了一种无监督学习方法，名为模态对应对齐（MoCA），它无缝整合了像DINO，SAM和ImageBind这样的现成基础模型。这种方法利用它们的知识互补性，优化它们的联合使用以实现多模态关联。起初，我们在特征空间中估计正负图像对。对于像素级别的关联，我们在图像级对比学习框架内引入了视觉适配器和一种新颖的像素匹配聚合策略。",
    "tldr": "提出了一种无监督音频-视觉分割方法 MoCA，在模态对应对齐的基础上使用DINO、SAM和ImageBind模型，实现了多模态关联，并引入了像素匹配聚合策略。",
    "en_tdlr": "Proposed an unsupervised audio-visual segmentation method MoCA that integrates DINO, SAM, and ImageBind models based on modality alignment, achieving multi-modality association, and introducing a pixel matching aggregation strategy."
}