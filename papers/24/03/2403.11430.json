{
    "title": "A Novel Paradigm Boosting Translation Capabilities of Large Language Models",
    "abstract": "arXiv:2403.11430v1 Announce Type: new  Abstract: This paper presents a study on strategies to enhance the translation capabilities of large language models (LLMs) in the context of machine translation (MT) tasks. The paper proposes a novel paradigm consisting of three stages: Secondary Pre-training using Extensive Monolingual Data, Continual Pre-training with Interlinear Text Format Documents, and Leveraging Source-Language Consistent Instruction for Supervised Fine-Tuning. Previous research on LLMs focused on various strategies for supervised fine-tuning (SFT), but their effectiveness has been limited. While traditional machine translation approaches rely on vast amounts of parallel bilingual data, our paradigm highlights the importance of using smaller sets of high-quality bilingual data. We argue that the focus should be on augmenting LLMs' cross-lingual alignment abilities during pre-training rather than solely relying on extensive bilingual data during SFT. Experimental results co",
    "link": "https://arxiv.org/abs/2403.11430",
    "context": "Title: A Novel Paradigm Boosting Translation Capabilities of Large Language Models\nAbstract: arXiv:2403.11430v1 Announce Type: new  Abstract: This paper presents a study on strategies to enhance the translation capabilities of large language models (LLMs) in the context of machine translation (MT) tasks. The paper proposes a novel paradigm consisting of three stages: Secondary Pre-training using Extensive Monolingual Data, Continual Pre-training with Interlinear Text Format Documents, and Leveraging Source-Language Consistent Instruction for Supervised Fine-Tuning. Previous research on LLMs focused on various strategies for supervised fine-tuning (SFT), but their effectiveness has been limited. While traditional machine translation approaches rely on vast amounts of parallel bilingual data, our paradigm highlights the importance of using smaller sets of high-quality bilingual data. We argue that the focus should be on augmenting LLMs' cross-lingual alignment abilities during pre-training rather than solely relying on extensive bilingual data during SFT. Experimental results co",
    "path": "papers/24/03/2403.11430.json",
    "total_tokens": 813,
    "translated_title": "一种提升大型语言模型翻译能力的新范式",
    "translated_abstract": "这篇论文研究了在机器翻译任务中增强大型语言模型（LLMs）翻译能力的策略。论文提出了一个包括三个阶段的新范式：使用大量单语数据的次级预训练，使用互文格式文档进行持续预训练，以及利用与源语言一致的指导进行监督微调。我们认为，重点应该放在增强LLMs在预训练期间的跨语言对齐能力，而不仅仅依靠SFT期间大量的双语数据。实验结果证实了我们的方法在不依赖大量双语数据的情况下，在传统机器翻译方法无法胜任的任务上取得了良好的效果。",
    "tldr": "新范式关注在LLMs的预训练阶段增强跨语言对齐能力，强调使用高质量的小型双语数据，在传统机器翻译方法无法胜任的任务上取得了良好的效果。",
    "en_tdlr": "The novel paradigm focuses on enhancing cross-lingual alignment abilities of LLMs during pre-training, highlighting the use of high-quality small bilingual datasets to achieve good results in tasks traditional machine translation methods cannot handle."
}