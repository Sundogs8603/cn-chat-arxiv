{
    "title": "Second-Order Information Matters: Revisiting Machine Unlearning for Large Language Models",
    "abstract": "arXiv:2403.10557v1 Announce Type: cross  Abstract: With the rapid development of Large Language Models (LLMs), we have witnessed intense competition among the major LLM products like ChatGPT, LLaMa, and Gemini. However, various issues (e.g. privacy leakage and copyright violation) of the training corpus still remain underexplored. For example, the Times sued OpenAI and Microsoft for infringing on its copyrights by using millions of its articles for training. From the perspective of LLM practitioners, handling such unintended privacy violations can be challenging. Previous work addressed the ``unlearning\" problem of LLMs using gradient information, while they mostly introduced significant overheads like data preprocessing or lacked robustness. In this paper, contrasting with the methods based on first-order information, we revisit the unlearning problem via the perspective of second-order information (Hessian). Our unlearning algorithms, which are inspired by classic Newton update, are ",
    "link": "https://arxiv.org/abs/2403.10557",
    "context": "Title: Second-Order Information Matters: Revisiting Machine Unlearning for Large Language Models\nAbstract: arXiv:2403.10557v1 Announce Type: cross  Abstract: With the rapid development of Large Language Models (LLMs), we have witnessed intense competition among the major LLM products like ChatGPT, LLaMa, and Gemini. However, various issues (e.g. privacy leakage and copyright violation) of the training corpus still remain underexplored. For example, the Times sued OpenAI and Microsoft for infringing on its copyrights by using millions of its articles for training. From the perspective of LLM practitioners, handling such unintended privacy violations can be challenging. Previous work addressed the ``unlearning\" problem of LLMs using gradient information, while they mostly introduced significant overheads like data preprocessing or lacked robustness. In this paper, contrasting with the methods based on first-order information, we revisit the unlearning problem via the perspective of second-order information (Hessian). Our unlearning algorithms, which are inspired by classic Newton update, are ",
    "path": "papers/24/03/2403.10557.json",
    "total_tokens": 835,
    "translated_title": "二阶信息很重要：重访大型语言模型的机器遗忘问题",
    "translated_abstract": "随着大型语言模型（LLMs）的快速发展，我们目睹了ChatGPT、LLaMa和Gemini等主要LLM产品之间的激烈竞争。然而，训练语料库的各种问题（如隐私泄露和版权侵犯）仍然未被充分探讨。以LLM从业者的视角来看，处理这些意外的隐私侵犯可能具有挑战性。之前的研究通过使用梯度信息解决了LLMs的“遗忘”问题，但它们大多引入了显著的开销，如数据预处理或缺乏鲁棒性。在本文中，与基于一阶信息的方法形成对比，我们通过二阶信息（Hessian）的视角重新审视了遗忘问题。受经典牛顿更新启发，我们的遗忘算法具有",
    "tldr": "本论文通过二阶信息（Hessian）的视角重新审视了大型语言模型的机器遗忘问题，提出了遗忘算法，具有较高的鲁棒性。",
    "en_tdlr": "This paper revisits the machine unlearning problem of large language models from the perspective of second-order information (Hessian) and proposes unlearning algorithms with higher robustness."
}