{
    "title": "GRAWA: Gradient-based Weighted Averaging for Distributed Training of Deep Learning Models",
    "abstract": "arXiv:2403.04206v1 Announce Type: new  Abstract: We study distributed training of deep learning models in time-constrained environments. We propose a new algorithm that periodically pulls workers towards the center variable computed as a weighted average of workers, where the weights are inversely proportional to the gradient norms of the workers such that recovering the flat regions in the optimization landscape is prioritized. We develop two asynchronous variants of the proposed algorithm that we call Model-level and Layer-level Gradient-based Weighted Averaging (resp. MGRAWA and LGRAWA), which differ in terms of the weighting scheme that is either done with respect to the entire model or is applied layer-wise. On the theoretical front, we prove the convergence guarantee for the proposed approach in both convex and non-convex settings. We then experimentally demonstrate that our algorithms outperform the competitor methods by achieving faster convergence and recovering better quality",
    "link": "https://arxiv.org/abs/2403.04206",
    "context": "Title: GRAWA: Gradient-based Weighted Averaging for Distributed Training of Deep Learning Models\nAbstract: arXiv:2403.04206v1 Announce Type: new  Abstract: We study distributed training of deep learning models in time-constrained environments. We propose a new algorithm that periodically pulls workers towards the center variable computed as a weighted average of workers, where the weights are inversely proportional to the gradient norms of the workers such that recovering the flat regions in the optimization landscape is prioritized. We develop two asynchronous variants of the proposed algorithm that we call Model-level and Layer-level Gradient-based Weighted Averaging (resp. MGRAWA and LGRAWA), which differ in terms of the weighting scheme that is either done with respect to the entire model or is applied layer-wise. On the theoretical front, we prove the convergence guarantee for the proposed approach in both convex and non-convex settings. We then experimentally demonstrate that our algorithms outperform the competitor methods by achieving faster convergence and recovering better quality",
    "path": "papers/24/03/2403.04206.json",
    "total_tokens": 852,
    "translated_title": "GRAWA: 基于梯度的加权平均用于深度学习模型的分布式训练",
    "translated_abstract": "我们研究了在时间受限的环境中的深度学习模型的分布式训练。我们提出了一种新算法，周期性地将工作者拉向作为工作者加权平均值计算的中心变量，其中权重与工作者的梯度范数成反比，从而优先恢复优化景观中的平坦区域。我们开发了所提算法的两种异步变体，分别称为模型级和层级梯度加权平均（MGRAWA和LGRAWA），在加权方案上有所不同，要么是针对整个模型进行，要么是逐层应用。在理论上，我们证明了所提方法在凸性和非凸性环境中的收敛保证。然后，我们通过实验证明了我们的算法通过实现更快的收敛速度和恢复更好的质量而胜过竞争方法。",
    "tldr": "提出了一种新的基于梯度的加权平均算法，以恢复优化景观中的平坦区域为优先，实验结果表明该算法在分布式训练中取得了更快的收敛速度和更好的质量"
}