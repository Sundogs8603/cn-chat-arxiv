{
    "title": "Neural-Kernel Conditional Mean Embeddings",
    "abstract": "arXiv:2403.10859v1 Announce Type: cross  Abstract: Kernel conditional mean embeddings (CMEs) offer a powerful framework for representing conditional distribution, but they often face scalability and expressiveness challenges. In this work, we propose a new method that effectively combines the strengths of deep learning with CMEs in order to address these challenges. Specifically, our approach leverages the end-to-end neural network (NN) optimization framework using a kernel-based objective. This design circumvents the computationally expensive Gram matrix inversion required by current CME methods. To further enhance performance, we provide efficient strategies to optimize the remaining kernel hyperparameters. In conditional density estimation tasks, our NN-CME hybrid achieves competitive performance and often surpasses existing deep learning-based methods. Lastly, we showcase its remarkable versatility by seamlessly integrating it into reinforcement learning (RL) contexts. Building on ",
    "link": "https://arxiv.org/abs/2403.10859",
    "context": "Title: Neural-Kernel Conditional Mean Embeddings\nAbstract: arXiv:2403.10859v1 Announce Type: cross  Abstract: Kernel conditional mean embeddings (CMEs) offer a powerful framework for representing conditional distribution, but they often face scalability and expressiveness challenges. In this work, we propose a new method that effectively combines the strengths of deep learning with CMEs in order to address these challenges. Specifically, our approach leverages the end-to-end neural network (NN) optimization framework using a kernel-based objective. This design circumvents the computationally expensive Gram matrix inversion required by current CME methods. To further enhance performance, we provide efficient strategies to optimize the remaining kernel hyperparameters. In conditional density estimation tasks, our NN-CME hybrid achieves competitive performance and often surpasses existing deep learning-based methods. Lastly, we showcase its remarkable versatility by seamlessly integrating it into reinforcement learning (RL) contexts. Building on ",
    "path": "papers/24/03/2403.10859.json",
    "total_tokens": 819,
    "translated_title": "神经核条件均值嵌入",
    "translated_abstract": "核条件均值嵌入（CME）为表示条件分布提供了强大的框架，但通常面临可伸缩性和表现力挑战。在这项工作中，我们提出了一种新方法，有效地结合了深度学习与CME，以解决这些挑战。具体而言，我们的方法利用端到端神经网络（NN）优化框架，使用基于核的目标函数。这种设计避免了当前CME方法所需的计算昂贵的Gram矩阵求逆。为进一步提高性能，我们提供了有效的策略来优化剩余的核超参数。在条件密度估计任务中，我们的NN-CME混合方法实现了竞争性能，并常常超过现有基于深度学习的方法。最后，我们展示了其在无缝集成到强化学习（RL）环境中的卓越多功能性。",
    "tldr": "结合深度学习和CME的神经网络方法，解决了核条件均值嵌入面临的可伸缩性和表现力挑战，并在条件密度估计任务和强化学习中展现出卓越性能。",
    "en_tdlr": "A neural network method combining deep learning and CME addressed scalability and expressiveness challenges faced by kernel conditional mean embeddings, showcasing remarkable performance in conditional density estimation tasks and reinforcement learning."
}