{
    "title": "The Fallacy of Minimizing Local Regret in the Sequential Task Setting",
    "abstract": "arXiv:2403.10946v1 Announce Type: cross  Abstract: In the realm of Reinforcement Learning (RL), online RL is often conceptualized as an optimization problem, where an algorithm interacts with an unknown environment to minimize cumulative regret. In a stationary setting, strong theoretical guarantees, like a sublinear ($\\sqrt{T}$) regret bound, can be obtained, which typically implies the convergence to an optimal policy and the cessation of exploration. However, these theoretical setups often oversimplify the complexities encountered in real-world RL implementations, where tasks arrive sequentially with substantial changes between tasks and the algorithm may not be allowed to adaptively learn within certain tasks. We study the changes beyond the outcome distributions, encompassing changes in the reward designs (mappings from outcomes to rewards) and the permissible policy spaces. Our results reveal the fallacy of myopically minimizing regret within each task: obtaining optimal regret r",
    "link": "https://arxiv.org/abs/2403.10946",
    "context": "Title: The Fallacy of Minimizing Local Regret in the Sequential Task Setting\nAbstract: arXiv:2403.10946v1 Announce Type: cross  Abstract: In the realm of Reinforcement Learning (RL), online RL is often conceptualized as an optimization problem, where an algorithm interacts with an unknown environment to minimize cumulative regret. In a stationary setting, strong theoretical guarantees, like a sublinear ($\\sqrt{T}$) regret bound, can be obtained, which typically implies the convergence to an optimal policy and the cessation of exploration. However, these theoretical setups often oversimplify the complexities encountered in real-world RL implementations, where tasks arrive sequentially with substantial changes between tasks and the algorithm may not be allowed to adaptively learn within certain tasks. We study the changes beyond the outcome distributions, encompassing changes in the reward designs (mappings from outcomes to rewards) and the permissible policy spaces. Our results reveal the fallacy of myopically minimizing regret within each task: obtaining optimal regret r",
    "path": "papers/24/03/2403.10946.json",
    "total_tokens": 843,
    "translated_title": "在序列任务设置中最小化局部遗憾的谬误",
    "translated_abstract": "在强化学习领域，在线强化学习经常被概念化为一个优化问题，其中算法与未知环境交互以最小化累积遗憾。在静态设置中，可以获得强大的理论保证，如次线性（$\\sqrt{T}$）遗憾界限，通常意味着收敛到最优策略并停止探索。然而，这些理论设置通常过分简化了真实世界强化学习实现中遇到的复杂性，其中任务按顺序到达，任务之间有重大变化，并且算法可能不允许在某些任务中进行自适应学习。我们研究超出结果分布的变化，涵盖奖励设计（从结果到奖励的映射）和允许的策略空间的变化。我们的结果揭示了在每个任务中近视地最小化遗憾的谬误：获得最优遗憾r",
    "tldr": "研究了强化学习中在序列任务设置下最小化局部遗憾的谬误，揭示了近视地最小化遗憾在实际应用中的复杂性。",
    "en_tdlr": "Investigated the fallacy of minimizing local regret in the sequential task setting in reinforcement learning, revealing the complexities of myopically minimizing regret in practical applications."
}