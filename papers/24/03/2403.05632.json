{
    "title": "Can Large Language Models Play Games? A Case Study of A Self-Play Approach",
    "abstract": "arXiv:2403.05632v1 Announce Type: new  Abstract: Large Language Models (LLMs) harness extensive data from the Internet, storing a broad spectrum of prior knowledge. While LLMs have proven beneficial as decision-making aids, their reliability is hampered by limitations in reasoning, hallucination phenomenon, and so on. On the other hand, Monte-Carlo Tree Search (MCTS) is a heuristic search algorithm that provides reliable decision-making solutions, achieved through recursive rollouts and self-play. However, the effectiveness of MCTS relies heavily on heuristic pruning and external value functions, particularly in complex decision scenarios. This work introduces an innovative approach that bolsters LLMs with MCTS self-play to efficiently resolve deterministic turn-based zero-sum games (DTZG), such as chess and go, without the need for additional training. Specifically, we utilize LLMs as both action pruners and proxies for value functions without the need for additional training. We theo",
    "link": "https://arxiv.org/abs/2403.05632",
    "context": "Title: Can Large Language Models Play Games? A Case Study of A Self-Play Approach\nAbstract: arXiv:2403.05632v1 Announce Type: new  Abstract: Large Language Models (LLMs) harness extensive data from the Internet, storing a broad spectrum of prior knowledge. While LLMs have proven beneficial as decision-making aids, their reliability is hampered by limitations in reasoning, hallucination phenomenon, and so on. On the other hand, Monte-Carlo Tree Search (MCTS) is a heuristic search algorithm that provides reliable decision-making solutions, achieved through recursive rollouts and self-play. However, the effectiveness of MCTS relies heavily on heuristic pruning and external value functions, particularly in complex decision scenarios. This work introduces an innovative approach that bolsters LLMs with MCTS self-play to efficiently resolve deterministic turn-based zero-sum games (DTZG), such as chess and go, without the need for additional training. Specifically, we utilize LLMs as both action pruners and proxies for value functions without the need for additional training. We theo",
    "path": "papers/24/03/2403.05632.json",
    "total_tokens": 946,
    "translated_title": "大型语言模型能玩游戏吗？一种自我博弈方法的案例研究",
    "translated_abstract": "大型语言模型（LLMs）利用来自互联网的大量数据，存储广泛的先验知识。虽然LLMs已被证明有助于决策辅助，但它们的可靠性受到推理能力的限制、幻觉现象等问题的影响。另一方面，蒙特卡罗树搜索（MCTS）是一种启发式搜索算法，通过递归展开和自我博弈提供可靠的决策解决方案。然而，在复杂的决策场景中，MCTS的有效性在很大程度上依赖于启发式修剪和外部值函数。这项工作引入了一种创新方法，通过将MCTS自我博弈与LLMs相结合，有效解决确定性轮流零和游戏（DTZG），如国际象棋和围棋，而无需额外的训练。具体而言，我们利用LLMs作为行动剪枝器和值函数的代理，而无需额外的训练。",
    "tldr": "该研究引入了一种创新方法，将蒙特卡罗树搜索（MCTS）的自我博弈与大型语言模型（LLMs）相结合，有效解决了确定性轮流零和游戏（DTZG），如国际象棋和围棋，而无需额外的训练。"
}