{
    "title": "SpikingResformer: Bridging ResNet and Vision Transformer in Spiking Neural Networks",
    "abstract": "arXiv:2403.14302v1 Announce Type: cross  Abstract: The remarkable success of Vision Transformers in Artificial Neural Networks (ANNs) has led to a growing interest in incorporating the self-attention mechanism and transformer-based architecture into Spiking Neural Networks (SNNs). While existing methods propose spiking self-attention mechanisms that are compatible with SNNs, they lack reasonable scaling methods, and the overall architectures proposed by these methods suffer from a bottleneck in effectively extracting local features. To address these challenges, we propose a novel spiking self-attention mechanism named Dual Spike Self-Attention (DSSA) with a reasonable scaling method. Based on DSSA, we propose a novel spiking Vision Transformer architecture called SpikingResformer, which combines the ResNet-based multi-stage architecture with our proposed DSSA to improve both performance and energy efficiency while reducing parameters. Experimental results show that SpikingResformer ach",
    "link": "https://arxiv.org/abs/2403.14302",
    "context": "Title: SpikingResformer: Bridging ResNet and Vision Transformer in Spiking Neural Networks\nAbstract: arXiv:2403.14302v1 Announce Type: cross  Abstract: The remarkable success of Vision Transformers in Artificial Neural Networks (ANNs) has led to a growing interest in incorporating the self-attention mechanism and transformer-based architecture into Spiking Neural Networks (SNNs). While existing methods propose spiking self-attention mechanisms that are compatible with SNNs, they lack reasonable scaling methods, and the overall architectures proposed by these methods suffer from a bottleneck in effectively extracting local features. To address these challenges, we propose a novel spiking self-attention mechanism named Dual Spike Self-Attention (DSSA) with a reasonable scaling method. Based on DSSA, we propose a novel spiking Vision Transformer architecture called SpikingResformer, which combines the ResNet-based multi-stage architecture with our proposed DSSA to improve both performance and energy efficiency while reducing parameters. Experimental results show that SpikingResformer ach",
    "path": "papers/24/03/2403.14302.json",
    "total_tokens": 758,
    "translated_title": "SpikingResformer: 将ResNet和Vision Transformer在脉冲神经网络中进行桥接",
    "translated_abstract": "Vision Transformer在人工神经网络中取得了显著成功，这导致了在脉冲神经网络(SNNs)中结合自注意机制和基于Transformer的结构引起越来越多的兴趣。为了解决这些挑战，我们提出了一种名为Dual Spike Self-Attention (DSSA)的新型脉冲自注意机制，带有合理的扩展方法。基于DSSA，我们提出了一种名为SpikingResformer的新型脉冲Vision Transformer架构，将基于ResNet的多阶段架构与我们提出的DSSA相结合，以提高性能和能效，同时减少参数。",
    "tldr": "提出了一种新型脉冲自注意机制DSSA以及结合ResNet的多阶段架构的SpikingResformer架构，旨在改善性能和能效，并减少参数。",
    "en_tdlr": "Introduced a novel spiking self-attention mechanism DSSA and SpikingResformer architecture that combines ResNet-based multi-stage architecture, aiming to improve performance, energy efficiency, and reduce parameters."
}