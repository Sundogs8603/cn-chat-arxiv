{
    "title": "Rethinking LLM Language Adaptation: A Case Study on Chinese Mixtral",
    "abstract": "arXiv:2403.01851v1 Announce Type: cross  Abstract: Mixtral, a representative sparse mixture of experts (SMoE) language model, has received significant attention due to its unique model design and superior performance. Based on Mixtral-8x7B-v0.1, in this paper, we propose Chinese-Mixtral and Chinese-Mixtral-Instruct with improved Chinese language abilities by adopting further pre-training and instruction fine-tuning. Experimental results show that our Chinese-Mixtral and Chinese-Mixtral-Instruct successfully improve Chinese understanding and generation performance while retaining the original English abilities. Then, we discuss several key questions when performing language adaptation on large language models, including the necessity of extending the language-specific vocabulary and the choice of the initialization model (foundation model v.s. instruction model), by providing empirical results and analysis. We also present the visualizations of each expert to examine their importance on",
    "link": "https://arxiv.org/abs/2403.01851",
    "context": "Title: Rethinking LLM Language Adaptation: A Case Study on Chinese Mixtral\nAbstract: arXiv:2403.01851v1 Announce Type: cross  Abstract: Mixtral, a representative sparse mixture of experts (SMoE) language model, has received significant attention due to its unique model design and superior performance. Based on Mixtral-8x7B-v0.1, in this paper, we propose Chinese-Mixtral and Chinese-Mixtral-Instruct with improved Chinese language abilities by adopting further pre-training and instruction fine-tuning. Experimental results show that our Chinese-Mixtral and Chinese-Mixtral-Instruct successfully improve Chinese understanding and generation performance while retaining the original English abilities. Then, we discuss several key questions when performing language adaptation on large language models, including the necessity of extending the language-specific vocabulary and the choice of the initialization model (foundation model v.s. instruction model), by providing empirical results and analysis. We also present the visualizations of each expert to examine their importance on",
    "path": "papers/24/03/2403.01851.json",
    "total_tokens": 820,
    "translated_title": "重新思考LLM语言适应性：以中文Mixtral为例",
    "translated_abstract": "Mixtral是一种代表性的稀疏专家混合(SMoE)语言模型，由于其独特的模型设计和卓越的性能而受到广泛关注。本文以Mixtral-8x7B-v0.1为基础，提出了改进的中文-Mixtral和中文-Mixtral-Instruct，通过进一步的预训练和指导微调提高了中文语言能力。实验结果表明，我们的中文-Mixtral和中文-Mixtral-Instruct成功提升了中文理解和生成性能，同时保留了原始的英文能力。然后，我们讨论了在大型语言模型进行语言适应时的一些关键问题，包括扩展语言特定词汇的必要性以及初始化模型的选择（基础模型vs.指导模型），通过提供实证结果和分析。我们还呈现了每个专家的可视化结果以检验其重要性。",
    "tldr": "本文以中文Mixtral为案例，提出了改进的中文语言能力的Mixtral模型，并讨论了在大型语言模型进行语言适应时的关键问题。"
}