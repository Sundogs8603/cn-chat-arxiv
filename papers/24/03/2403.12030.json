{
    "title": "Expandable Subspace Ensemble for Pre-Trained Model-Based Class-Incremental Learning",
    "abstract": "arXiv:2403.12030v1 Announce Type: cross  Abstract: Class-Incremental Learning (CIL) requires a learning system to continually learn new classes without forgetting. Despite the strong performance of Pre-Trained Models (PTMs) in CIL, a critical issue persists: learning new classes often results in the overwriting of old ones. Excessive modification of the network causes forgetting, while minimal adjustments lead to an inadequate fit for new classes. As a result, it is desired to figure out a way of efficient model updating without harming former knowledge. In this paper, we propose ExpAndable Subspace Ensemble (EASE) for PTM-based CIL. To enable model updating without conflict, we train a distinct lightweight adapter module for each new task, aiming to create task-specific subspaces. These adapters span a high-dimensional feature space, enabling joint decision-making across multiple subspaces. As data evolves, the expanding subspaces render the old class classifiers incompatible with new",
    "link": "https://arxiv.org/abs/2403.12030",
    "context": "Title: Expandable Subspace Ensemble for Pre-Trained Model-Based Class-Incremental Learning\nAbstract: arXiv:2403.12030v1 Announce Type: cross  Abstract: Class-Incremental Learning (CIL) requires a learning system to continually learn new classes without forgetting. Despite the strong performance of Pre-Trained Models (PTMs) in CIL, a critical issue persists: learning new classes often results in the overwriting of old ones. Excessive modification of the network causes forgetting, while minimal adjustments lead to an inadequate fit for new classes. As a result, it is desired to figure out a way of efficient model updating without harming former knowledge. In this paper, we propose ExpAndable Subspace Ensemble (EASE) for PTM-based CIL. To enable model updating without conflict, we train a distinct lightweight adapter module for each new task, aiming to create task-specific subspaces. These adapters span a high-dimensional feature space, enabling joint decision-making across multiple subspaces. As data evolves, the expanding subspaces render the old class classifiers incompatible with new",
    "path": "papers/24/03/2403.12030.json",
    "total_tokens": 916,
    "translated_title": "基于预训练模型的增量类别学习的可扩展子空间集成",
    "translated_abstract": "arXiv:2403.12030v1 发表类型：交叉摘要：类增量学习（CIL）要求学习系统不断学习新类别而不会遗忘旧知识。尽管预训练模型在CIL中表现出色，但一个关键问题仍然存在：学习新类别通常会导致旧类别的覆盖。网络过度修改会导致遗忘，而最小调整会导致新类别拟合不足。因此，希望找到一种有效的模型更新方式，既不损害先前知识。在本文中，我们提出了适用于基于PTM的CIL的Extended Subspace Ensemble（EASE）。为了使模型更新不冲突，我们为每个新任务训练一个独特的轻量级适配器模块，旨在创建任务特定的子空间。这些适配器跨越高维特征空间，实现跨多个子空间的联合决策。随着数据的演变，不断扩展的子空间使旧类别分类器与新类别不兼容。",
    "tldr": "提出了一种基于预训练模型的增量类别学习方法，通过训练每个新任务的轻量级适配器模块来创建任务特定的子空间，实现了模型更新而不损害先前知识。",
    "en_tdlr": "Proposed an approach for class-incremental learning based on pre-trained models, which creates task-specific subspaces by training lightweight adapter modules for each new task, enabling model updating without harming former knowledge."
}