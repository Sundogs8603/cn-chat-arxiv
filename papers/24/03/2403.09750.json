{
    "title": "Meta-Cognitive Analysis: Evaluating Declarative and Procedural Knowledge in Datasets and Large Language Models",
    "abstract": "arXiv:2403.09750v1 Announce Type: cross  Abstract: Declarative knowledge and procedural knowledge are two key parts in meta-cognitive theory, and these two hold significant importance in pre-training and inference of LLMs. However, a comprehensive analysis comparing these two types of knowledge is lacking, primarily due to challenges in definition, probing and quantitative assessment. In this paper, we explore from a new perspective by providing ground-truth knowledge for LLMs and evaluating the effective score. Through extensive experiments with widely-used datasets and models, we get conclusions: (1) In most tasks, benefits from declarative knowledge are greater than those from procedural knowledge. (2) Profits of procedural knowledge are larger than declarative knowledge only in reasoning tasks with simple logic. (3) As pre-training progresses and size increases, model ability to utilize both kinds of knowledge significantly improves, but in different speed. We do detailed analysis ",
    "link": "https://arxiv.org/abs/2403.09750",
    "context": "Title: Meta-Cognitive Analysis: Evaluating Declarative and Procedural Knowledge in Datasets and Large Language Models\nAbstract: arXiv:2403.09750v1 Announce Type: cross  Abstract: Declarative knowledge and procedural knowledge are two key parts in meta-cognitive theory, and these two hold significant importance in pre-training and inference of LLMs. However, a comprehensive analysis comparing these two types of knowledge is lacking, primarily due to challenges in definition, probing and quantitative assessment. In this paper, we explore from a new perspective by providing ground-truth knowledge for LLMs and evaluating the effective score. Through extensive experiments with widely-used datasets and models, we get conclusions: (1) In most tasks, benefits from declarative knowledge are greater than those from procedural knowledge. (2) Profits of procedural knowledge are larger than declarative knowledge only in reasoning tasks with simple logic. (3) As pre-training progresses and size increases, model ability to utilize both kinds of knowledge significantly improves, but in different speed. We do detailed analysis ",
    "path": "papers/24/03/2403.09750.json",
    "total_tokens": 938,
    "translated_title": "元认知分析：评估数据集和大型语言模型中的陈述性和程序性知识",
    "translated_abstract": "元认知理论中的陈述性知识和程序性知识是两个关键部分，在LLM的预训练和推理中非常重要。然而，由于对这两种知识的定义、探究和定量评估存在挑战，缺乏对这两种知识进行全面比较的分析。本文从一个新的角度提供了LLMs的地面真知，并评估了有效得分。通过对广泛使用的数据集和模型进行大量实验，我们得出结论：(1) 在大多数任务中，来自陈述性知识的益处大于来自程序性知识的益处。(2) 仅在具有简单逻辑推理的任务中，程序性知识的利润大于陈述性知识。(3) 随着预训练的进行和规模的增加，模型利用两种知识的能力显著提高，但速度不同。我们进行了详细分析。",
    "tldr": "通过广泛实验探索了LLMs中的陈述性知识和程序性知识对各种任务的影响，发现陈述性知识在大多数任务中的益处大于程序性知识，在简单逻辑推理任务中反之；随着预训练和规模的增加，模型利用两种知识的能力均显著提高。",
    "en_tdlr": "Explored the impact of declarative and procedural knowledge in LLMs through extensive experiments, finding that declarative knowledge is more beneficial than procedural knowledge in most tasks, but the opposite is true in simple logical reasoning tasks; as pre-training and size increase, the model's ability to utilize both kinds of knowledge significantly improves."
}