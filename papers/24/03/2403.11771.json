{
    "title": "Modality-Agnostic fMRI Decoding of Vision and Language",
    "abstract": "arXiv:2403.11771v1 Announce Type: cross  Abstract: Previous studies have shown that it is possible to map brain activation data of subjects viewing images onto the feature representation space of not only vision models (modality-specific decoding) but also language models (cross-modal decoding). In this work, we introduce and use a new large-scale fMRI dataset (~8,500 trials per subject) of people watching both images and text descriptions of such images. This novel dataset enables the development of modality-agnostic decoders: a single decoder that can predict which stimulus a subject is seeing, irrespective of the modality (image or text) in which the stimulus is presented. We train and evaluate such decoders to map brain signals onto stimulus representations from a large range of publicly available vision, language and multimodal (vision+language) models. Our findings reveal that (1) modality-agnostic decoders perform as well as (and sometimes even better than) modality-specific dec",
    "link": "https://arxiv.org/abs/2403.11771",
    "context": "Title: Modality-Agnostic fMRI Decoding of Vision and Language\nAbstract: arXiv:2403.11771v1 Announce Type: cross  Abstract: Previous studies have shown that it is possible to map brain activation data of subjects viewing images onto the feature representation space of not only vision models (modality-specific decoding) but also language models (cross-modal decoding). In this work, we introduce and use a new large-scale fMRI dataset (~8,500 trials per subject) of people watching both images and text descriptions of such images. This novel dataset enables the development of modality-agnostic decoders: a single decoder that can predict which stimulus a subject is seeing, irrespective of the modality (image or text) in which the stimulus is presented. We train and evaluate such decoders to map brain signals onto stimulus representations from a large range of publicly available vision, language and multimodal (vision+language) models. Our findings reveal that (1) modality-agnostic decoders perform as well as (and sometimes even better than) modality-specific dec",
    "path": "papers/24/03/2403.11771.json",
    "total_tokens": 924,
    "translated_title": "视觉和语言的模态无关fMRI解码",
    "translated_abstract": "先前的研究表明，可以将受试者观看图像的大脑激活数据映射到视觉模型（模态特定解码）以及语言模型（跨模态解码）的特征表示空间中。本研究引入并使用了一个新的大规模fMRI数据集（每个受试者约8500个试验），让人们既观看图像又观看这些图像的文本描述。这一新颖的数据集实现了模态无关解码器的开发：一种单一解码器可以预测受试者正在看到的刺激，而不考虑刺激以何种模态（图像或文本）呈现。我们训练和评估这类解码器，以将大范围公开可用的视觉、语言和多模态（视觉+语言）模型中的刺激表示映射到大脑信号上。我们的研究结果表明，（1）模态无关解码器表现得与（有时甚至更好）模态特定解码一样好。",
    "tldr": "本研究介绍了一种能够预测受试者正在看到的刺激的单一解码器，无论刺激是以何种模态呈现。研究发现模态无关解码器表现与模态特定解码器相当甚至更好。",
    "en_tdlr": "This study introduces a single decoder that can predict the stimulus a subject is seeing, regardless of the modality in which the stimulus is presented. The findings reveal that modality-agnostic decoders perform as well as or even better than modality-specific decoders."
}