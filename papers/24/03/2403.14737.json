{
    "title": "FedMef: Towards Memory-efficient Federated Dynamic Pruning",
    "abstract": "arXiv:2403.14737v1 Announce Type: new  Abstract: Federated learning (FL) promotes decentralized training while prioritizing data confidentiality. However, its application on resource-constrained devices is challenging due to the high demand for computation and memory resources to train deep learning models. Neural network pruning techniques, such as dynamic pruning, could enhance model efficiency, but directly adopting them in FL still poses substantial challenges, including post-pruning performance degradation, high activation memory usage, etc. To address these challenges, we propose FedMef, a novel and memory-efficient federated dynamic pruning framework. FedMef comprises two key components. First, we introduce the budget-aware extrusion that maintains pruning efficiency while preserving post-pruning performance by salvaging crucial information from parameters marked for pruning within a given budget. Second, we propose scaled activation pruning to effectively reduce activation memo",
    "link": "https://arxiv.org/abs/2403.14737",
    "context": "Title: FedMef: Towards Memory-efficient Federated Dynamic Pruning\nAbstract: arXiv:2403.14737v1 Announce Type: new  Abstract: Federated learning (FL) promotes decentralized training while prioritizing data confidentiality. However, its application on resource-constrained devices is challenging due to the high demand for computation and memory resources to train deep learning models. Neural network pruning techniques, such as dynamic pruning, could enhance model efficiency, but directly adopting them in FL still poses substantial challenges, including post-pruning performance degradation, high activation memory usage, etc. To address these challenges, we propose FedMef, a novel and memory-efficient federated dynamic pruning framework. FedMef comprises two key components. First, we introduce the budget-aware extrusion that maintains pruning efficiency while preserving post-pruning performance by salvaging crucial information from parameters marked for pruning within a given budget. Second, we propose scaled activation pruning to effectively reduce activation memo",
    "path": "papers/24/03/2403.14737.json",
    "total_tokens": 663,
    "translated_title": "FedMef：面向内存高效的联邦动态剪枝",
    "translated_abstract": "引入预算感知的挤出机制以维持剪枝效率，并通过在给定预算内从标记为剪枝的参数中挽救关键信息来保持剪枝后性能；提出了缩放激活剪枝以有效减少激活内存使用的Federated dynamic pruning framework。",
    "tldr": "FedMef提出了一种新颖且内存高效的联邦动态剪枝框架，通过预算感知的挤出机制和缩放激活剪枝来解决联邦学习中的性能退化和高激活内存使用等挑战。",
    "en_tdlr": "FedMef proposes a novel and memory-efficient federated dynamic pruning framework to address challenges in federated learning such as performance degradation and high activation memory usage by introducing budget-aware extrusion and scaled activation pruning."
}