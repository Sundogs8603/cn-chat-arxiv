{
    "title": "Learning From Correctness Without Prompting Makes LLM Efficient Reasoner",
    "abstract": "arXiv:2403.19094v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated outstanding performance across various tasks, yet they still exhibit limitations such as hallucination, unfaithful reasoning, and toxic content. One potential approach to mitigate these issues is learning from human or external feedback (e.g. tools). In this paper, we introduce an intrinsic self-correct reasoning framework for LLMs that eliminates the need for human feedback, external tools, and handcraft prompts. The proposed framework, based on a multi-step reasoning paradigm \\textbf{Le}arning from \\textbf{Co}rrectness (\\textsc{LeCo}), improves reasoning performance without needing to learn from errors. This paradigm prioritizes learning from correct reasoning steps, and a unique method to measure confidence for each reasoning step based on generation logits. Experimental results across various multi-step reasoning tasks demonstrate the effectiveness of the framework in improving reasoning",
    "link": "https://arxiv.org/abs/2403.19094",
    "context": "Title: Learning From Correctness Without Prompting Makes LLM Efficient Reasoner\nAbstract: arXiv:2403.19094v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated outstanding performance across various tasks, yet they still exhibit limitations such as hallucination, unfaithful reasoning, and toxic content. One potential approach to mitigate these issues is learning from human or external feedback (e.g. tools). In this paper, we introduce an intrinsic self-correct reasoning framework for LLMs that eliminates the need for human feedback, external tools, and handcraft prompts. The proposed framework, based on a multi-step reasoning paradigm \\textbf{Le}arning from \\textbf{Co}rrectness (\\textsc{LeCo}), improves reasoning performance without needing to learn from errors. This paradigm prioritizes learning from correct reasoning steps, and a unique method to measure confidence for each reasoning step based on generation logits. Experimental results across various multi-step reasoning tasks demonstrate the effectiveness of the framework in improving reasoning",
    "path": "papers/24/03/2403.19094.json",
    "total_tokens": 836,
    "translated_title": "没有提示的情况下学习正确性使LLM成为高效推理者",
    "translated_abstract": "大型语言模型（LLMs）在各种任务中表现出色，但仍然存在幻觉、不忠实的推理和有毒内容等局限性。缓解这些问题的一个潜在方法是从人类或外部反馈（例如工具）中学习。本文介绍了一种用于LLMs的内在自我修正推理框架，消除了人类反馈、外部工具和手工提示的需求。提出的框架基于一种多步推理范式Learning from Correctness (LeCo)，在不需要从错误中学习的情况下提高了推理性能。该范式优先学习正确的推理步骤，并基于生成logits来衡量每个推理步骤的置信度。在各种多步推理任务上的实验结果表明，该框架在改善推理方面的有效性。",
    "tldr": "本文介绍了一种用于大型语言模型的内在自我修正推理框架LeCo，无需人类反馈、外部工具或手动提示，通过学习正确的推理步骤并基于生成logits来提高推理性能。",
    "en_tdlr": "This paper introduces an intrinsic self-correct reasoning framework LeCo for large language models, which improves reasoning performance by learning from correct reasoning steps based on generation logits without requiring human feedback, external tools, or handcraft prompts."
}