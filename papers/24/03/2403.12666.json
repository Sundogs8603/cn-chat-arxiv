{
    "title": "Multi-Dimensional Machine Translation Evaluation: Model Evaluation and Resource for Korean",
    "abstract": "arXiv:2403.12666v1 Announce Type: new  Abstract: Almost all frameworks for the manual or automatic evaluation of machine translation characterize the quality of an MT output with a single number. An exception is the Multidimensional Quality Metrics (MQM) framework which offers a fine-grained ontology of quality dimensions for scoring (such as style, fluency, accuracy, and terminology). Previous studies have demonstrated the feasibility of MQM annotation but there are, to our knowledge, no computational models that predict MQM scores for novel texts, due to a lack of resources. In this paper, we address these shortcomings by (a) providing a 1200-sentence MQM evaluation benchmark for the language pair English-Korean and (b) reframing MT evaluation as the multi-task problem of simultaneously predicting several MQM scores using SOTA language models, both in a reference-based MT evaluation setup and a reference-free quality estimation (QE) setup. We find that reference-free setup outperform",
    "link": "https://arxiv.org/abs/2403.12666",
    "context": "Title: Multi-Dimensional Machine Translation Evaluation: Model Evaluation and Resource for Korean\nAbstract: arXiv:2403.12666v1 Announce Type: new  Abstract: Almost all frameworks for the manual or automatic evaluation of machine translation characterize the quality of an MT output with a single number. An exception is the Multidimensional Quality Metrics (MQM) framework which offers a fine-grained ontology of quality dimensions for scoring (such as style, fluency, accuracy, and terminology). Previous studies have demonstrated the feasibility of MQM annotation but there are, to our knowledge, no computational models that predict MQM scores for novel texts, due to a lack of resources. In this paper, we address these shortcomings by (a) providing a 1200-sentence MQM evaluation benchmark for the language pair English-Korean and (b) reframing MT evaluation as the multi-task problem of simultaneously predicting several MQM scores using SOTA language models, both in a reference-based MT evaluation setup and a reference-free quality estimation (QE) setup. We find that reference-free setup outperform",
    "path": "papers/24/03/2403.12666.json",
    "total_tokens": 859,
    "translated_title": "多维机器翻译评估：模型评估和韩语资源",
    "translated_abstract": "几乎所有手动或自动评估机器翻译质量的框架都使用单一数字来描述MT输出的质量。多维质量指标（MQM）框架是一个例外，它提供了一个细粒度的质量维度本体（如风格、流畅性、准确性和术语）。先前的研究已经证明了MQM注释的可行性，但据我们所知，目前没有计算模型可以预测新文本的MQM分数，这是因为缺乏资源。在本文中，我们通过(a)提供一个英韩语言对1200句MQM评估基准，以及(b)将MT评估重新构建为同时预测多个MQM分数的多任务问题，使用最先进的语言模型，分别在基于参考的MT评估设置和不需要参考的质量估计（QE）设置中。我们发现不需要参考的设置表现优于",
    "tldr": "本文提出了一个针对英韩语言对的1200句MQM评估基准，并通过使用最先进的语言模型，将MT评估转化为同时预测多个MQM分数的多任务问题。",
    "en_tdlr": "This paper introduces a 1200-sentence MQM evaluation benchmark for English-Korean language pair and reframes MT evaluation as the multi-task problem of predicting multiple MQM scores simultaneously using state-of-the-art language models."
}