{
    "title": "Standardizing the Measurement of Text Diversity: A Tool and a Comparative Analysis of Scores",
    "abstract": "arXiv:2403.00553v1 Announce Type: new  Abstract: The diversity across outputs generated by large language models shapes the perception of their quality and utility. Prompt leaks, templated answer structure, and canned responses across different interactions are readily noticed by people, but there is no standard score to measure this aspect of model behavior. In this work we empirically investigate diversity scores on English texts. We find that computationally efficient compression algorithms capture information similar to what is measured by slow to compute $n$-gram overlap homogeneity scores. Further, a combination of measures -- compression ratios, self-repetition of long $n$-grams and Self-BLEU and BERTScore -- are sufficient to report, as they have low mutual correlation with each other. The applicability of scores extends beyond analysis of generative models; for example, we highlight applications on instruction-tuning datasets and human-produced texts. We release a diversity sc",
    "link": "https://arxiv.org/abs/2403.00553",
    "context": "Title: Standardizing the Measurement of Text Diversity: A Tool and a Comparative Analysis of Scores\nAbstract: arXiv:2403.00553v1 Announce Type: new  Abstract: The diversity across outputs generated by large language models shapes the perception of their quality and utility. Prompt leaks, templated answer structure, and canned responses across different interactions are readily noticed by people, but there is no standard score to measure this aspect of model behavior. In this work we empirically investigate diversity scores on English texts. We find that computationally efficient compression algorithms capture information similar to what is measured by slow to compute $n$-gram overlap homogeneity scores. Further, a combination of measures -- compression ratios, self-repetition of long $n$-grams and Self-BLEU and BERTScore -- are sufficient to report, as they have low mutual correlation with each other. The applicability of scores extends beyond analysis of generative models; for example, we highlight applications on instruction-tuning datasets and human-produced texts. We release a diversity sc",
    "path": "papers/24/03/2403.00553.json",
    "total_tokens": 833,
    "translated_title": "规范文本多样性的测量：一个工具和对分数的比较分析",
    "translated_abstract": "大型语言模型生成的输出之间的多样性塑造了人们对其质量和实用性的看法。我们的工作通过实证研究英语文本的多样性得分。我们发现，计算效率高的压缩算法捕捉到与$n$-gram的重叠同质性得分所衡量的信息相似。此外，结合多种度量方法——压缩比、长$n$-gram的自重复、Self-BLEU和BERTScore——足以报告，因为它们彼此之间的相互关联较低。这些分数的适用性超出了生成模型的分析；例如，我们突出了在指导调整数据集和人类生成的文本上的应用。我们发布了一个多样性程度",
    "tldr": "本论文提出了一种用于衡量文本多样性的标准分数，通过实证研究发现压缩算法可以捕捉类似于$n$-gram重叠同质性得分的信息，并结合多种度量方法来报告分数，适用于不同类型的文本分析。",
    "en_tdlr": "This paper introduces a standard score for measuring text diversity, empirically showing that compression algorithms can capture information similar to $n$-gram overlap homogeneity scores, and a combination of measures can be used to report scores applicable to various types of text analysis."
}