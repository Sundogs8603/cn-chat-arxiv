{
    "title": "Developing Healthcare Language Model Embedding Spaces",
    "abstract": "arXiv:2403.19802v1 Announce Type: cross  Abstract: Pre-trained Large Language Models (LLMs) often struggle on out-of-domain datasets like healthcare focused text. We explore specialized pre-training to adapt smaller LLMs to different healthcare datasets. Three methods are assessed: traditional masked language modeling, Deep Contrastive Learning for Unsupervised Textual Representations (DeCLUTR), and a novel pre-training objective utilizing metadata categories from the healthcare settings. These schemes are evaluated on downstream document classification tasks for each dataset, with additional analysis of the resultant embedding spaces. Contrastively trained models outperform other approaches on the classification tasks, delivering strong performance from limited labeled data and with fewer model parameter updates required. While metadata-based pre-training does not further improve classifications across the datasets, it yields interesting embedding cluster separability. All domain adap",
    "link": "https://arxiv.org/abs/2403.19802",
    "context": "Title: Developing Healthcare Language Model Embedding Spaces\nAbstract: arXiv:2403.19802v1 Announce Type: cross  Abstract: Pre-trained Large Language Models (LLMs) often struggle on out-of-domain datasets like healthcare focused text. We explore specialized pre-training to adapt smaller LLMs to different healthcare datasets. Three methods are assessed: traditional masked language modeling, Deep Contrastive Learning for Unsupervised Textual Representations (DeCLUTR), and a novel pre-training objective utilizing metadata categories from the healthcare settings. These schemes are evaluated on downstream document classification tasks for each dataset, with additional analysis of the resultant embedding spaces. Contrastively trained models outperform other approaches on the classification tasks, delivering strong performance from limited labeled data and with fewer model parameter updates required. While metadata-based pre-training does not further improve classifications across the datasets, it yields interesting embedding cluster separability. All domain adap",
    "path": "papers/24/03/2403.19802.json",
    "total_tokens": 838,
    "translated_title": "开发医疗保健语言模型嵌入空间",
    "translated_abstract": "预训练大型语言模型 (LLMs) 在诸如专注于医疗保健文本之类的跨领域数据集上经常面临困难。我们探索专门的预训练方法，以调整较小的LLMs以适应不同的医疗保健数据集。评估了三种方法：传统的掩码语言建模、用于无监督文本表示的深度对比学习 (DeCLUTR) 和一种利用医疗保健环境中的元数据类别的新颖预训练目标。对每个数据集进行了下游文档分类任务的评估，并对生成的嵌入空间进行了额外分析。对比训练的模型在分类任务上表现优于其他方法，能够在有限标记数据的情况下提供强大性能，并且需要较少的模型参数更新。虽然基于元数据的预训练并未进一步提高数据集上的分类性能，但它确实产生了有趣的嵌入聚类可分性。所有领域的调整",
    "tldr": "通过深度对比学习训练的模型在医疗保健文本分类任务中表现出色，有效利用有限标记数据，并减少了模型参数更新。"
}