{
    "title": "Encode Once and Decode in Parallel: Efficient Transformer Decoding",
    "abstract": "arXiv:2403.13112v1 Announce Type: new  Abstract: Transformer-based NLP models are powerful but have high computational costs that limit deployment scenarios. Finetuned encoder-decoder models are popular in specialized domains and can outperform larger more generalized decoder-only models, such as GPT-4. We introduce a new configuration for encoder-decoder models that improves efficiency on structured output and question-answering tasks where multiple outputs are required of a single input. Our method, prompt-in-decoder (PiD), encodes the input once and decodes output in parallel, boosting both training and inference efficiency by avoiding duplicate input encoding, thereby reducing the decoder's memory footprint. We achieve computation reduction that roughly scales with the number of subtasks, gaining up to 4.6x speed-up over state-of-the-art models for dialogue state tracking, summarization, and question-answering tasks with comparable or better performance. We release our training/inf",
    "link": "https://arxiv.org/abs/2403.13112",
    "context": "Title: Encode Once and Decode in Parallel: Efficient Transformer Decoding\nAbstract: arXiv:2403.13112v1 Announce Type: new  Abstract: Transformer-based NLP models are powerful but have high computational costs that limit deployment scenarios. Finetuned encoder-decoder models are popular in specialized domains and can outperform larger more generalized decoder-only models, such as GPT-4. We introduce a new configuration for encoder-decoder models that improves efficiency on structured output and question-answering tasks where multiple outputs are required of a single input. Our method, prompt-in-decoder (PiD), encodes the input once and decodes output in parallel, boosting both training and inference efficiency by avoiding duplicate input encoding, thereby reducing the decoder's memory footprint. We achieve computation reduction that roughly scales with the number of subtasks, gaining up to 4.6x speed-up over state-of-the-art models for dialogue state tracking, summarization, and question-answering tasks with comparable or better performance. We release our training/inf",
    "path": "papers/24/03/2403.13112.json",
    "total_tokens": 904,
    "translated_title": "一次编码，多次并行解码：高效Transformer解码",
    "translated_abstract": "基于Transformer的自然语言处理模型功能强大，但计算成本高，限制了部署场景。在专业领域中，微调的编码器-解码器模型备受青睐，可以胜过更大更通用的仅解码器模型，例如GPT-4。我们介绍了一种新的编码器-解码器模型配置，可以提高在结构化输出和问答任务中的效率，在这些任务中，需要从单个输入中产生多个输出。我们的方法，prompt-in-decoder（PiD），只对输入进行一次编码，并且并行解码输出，通过避免重复输入编码，从而减少解码器的内存占用，提升了训练和推断效率。我们实现了计算减少，大致随子任务数量增加而扩展，相比最先进模型，在对话状态追踪、摘要和问答任务中获得高达4.6倍的速度提升，并且性能相当或更好。我们发布了我们的训练/推断代码。",
    "tldr": "提出了一种新的编码器-解码器模型配置，称为prompt-in-decoder（PiD），可以一次编码输入并并行解码输出，在结构化输出和问答任务中取得高效率，避免了重复输入编码，大幅减少了解码器的内存占用。"
}