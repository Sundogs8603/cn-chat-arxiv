{
    "title": "Are Classification Robustness and Explanation Robustness Really Strongly Correlated? An Analysis Through Input Loss Landscape",
    "abstract": "arXiv:2403.06013v1 Announce Type: new  Abstract: This paper delves into the critical area of deep learning robustness, challenging the conventional belief that classification robustness and explanation robustness in image classification systems are inherently correlated. Through a novel evaluation approach leveraging clustering for efficient assessment of explanation robustness, we demonstrate that enhancing explanation robustness does not necessarily flatten the input loss landscape with respect to explanation loss - contrary to flattened loss landscapes indicating better classification robustness. To deeply investigate this contradiction, a groundbreaking training method designed to adjust the loss landscape with respect to explanation loss is proposed. Through the new training method, we uncover that although such adjustments can impact the robustness of explanations, they do not have an influence on the robustness of classification. These findings not only challenge the prevailing ",
    "link": "https://arxiv.org/abs/2403.06013",
    "context": "Title: Are Classification Robustness and Explanation Robustness Really Strongly Correlated? An Analysis Through Input Loss Landscape\nAbstract: arXiv:2403.06013v1 Announce Type: new  Abstract: This paper delves into the critical area of deep learning robustness, challenging the conventional belief that classification robustness and explanation robustness in image classification systems are inherently correlated. Through a novel evaluation approach leveraging clustering for efficient assessment of explanation robustness, we demonstrate that enhancing explanation robustness does not necessarily flatten the input loss landscape with respect to explanation loss - contrary to flattened loss landscapes indicating better classification robustness. To deeply investigate this contradiction, a groundbreaking training method designed to adjust the loss landscape with respect to explanation loss is proposed. Through the new training method, we uncover that although such adjustments can impact the robustness of explanations, they do not have an influence on the robustness of classification. These findings not only challenge the prevailing ",
    "path": "papers/24/03/2403.06013.json",
    "total_tokens": 842,
    "translated_title": "分类鲁棒性和解释鲁棒性是否真的强相关？通过输入损失景观的分析",
    "translated_abstract": "本文深入探讨了深度学习鲁棒性领域，挑战了传统观念，即图像分类系统中的分类鲁棒性和解释鲁棒性本质上是相关的。通过一种新颖的评估方法，利用聚类来有效评估解释鲁棒性，我们展示了增强解释鲁棒性并不一定会使输入损失景观相对于解释损失变平 - 与损失景观变平表示更好的分类鲁棒性相反。为了深入研究这一矛盾，提出了一种突破性的训练方法，旨在调整相对于解释损失的损失景观。通过这种新的训练方法，我们发现虽然这种调整可以影响解释的鲁棒性，但它们对分类的鲁棒性没有影响。这些发现不仅挑战了流行的观念",
    "tldr": "通过新颖的评估方法和训练方法，本研究发现增强解释鲁棒性并不能提高分类鲁棒性，这一发现挑战了传统观念。"
}