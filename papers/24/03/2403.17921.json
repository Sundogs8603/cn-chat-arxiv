{
    "title": "The Need for Speed: Pruning Transformers with One Recipe",
    "abstract": "arXiv:2403.17921v1 Announce Type: new  Abstract: We introduce the $\\textbf{O}$ne-shot $\\textbf{P}$runing $\\textbf{T}$echnique for $\\textbf{I}$nterchangeable $\\textbf{N}$etworks ($\\textbf{OPTIN}$) framework as a tool to increase the efficiency of pre-trained transformer architectures $\\textit{without requiring re-training}$. Recent works have explored improving transformer efficiency, however often incur computationally expensive re-training procedures or depend on architecture-specific characteristics, thus impeding practical wide-scale adoption. To address these shortcomings, the OPTIN framework leverages intermediate feature distillation, capturing the long-range dependencies of model parameters (coined $\\textit{trajectory}$), to produce state-of-the-art results on natural language, image classification, transfer learning, and semantic segmentation tasks $\\textit{without re-training}$. Given a FLOP constraint, the OPTIN framework will compress the network while maintaining competitiv",
    "link": "https://arxiv.org/abs/2403.17921",
    "context": "Title: The Need for Speed: Pruning Transformers with One Recipe\nAbstract: arXiv:2403.17921v1 Announce Type: new  Abstract: We introduce the $\\textbf{O}$ne-shot $\\textbf{P}$runing $\\textbf{T}$echnique for $\\textbf{I}$nterchangeable $\\textbf{N}$etworks ($\\textbf{OPTIN}$) framework as a tool to increase the efficiency of pre-trained transformer architectures $\\textit{without requiring re-training}$. Recent works have explored improving transformer efficiency, however often incur computationally expensive re-training procedures or depend on architecture-specific characteristics, thus impeding practical wide-scale adoption. To address these shortcomings, the OPTIN framework leverages intermediate feature distillation, capturing the long-range dependencies of model parameters (coined $\\textit{trajectory}$), to produce state-of-the-art results on natural language, image classification, transfer learning, and semantic segmentation tasks $\\textit{without re-training}$. Given a FLOP constraint, the OPTIN framework will compress the network while maintaining competitiv",
    "path": "papers/24/03/2403.17921.json",
    "total_tokens": 873,
    "translated_title": "需要速度：用一种方法对Transformer进行修剪",
    "translated_abstract": "我们介绍了$\\textbf{O}$ne-shot $\\textbf{P}$runing $\\textbf{T}$echnique for $\\textbf{I}$nterchangeable $\\textbf{N}$etworks ($\\textbf{OPTIN}$)框架，作为一种工具，可以提高预训练的Transformer架构的效率，而无需重新训练。最近的研究探索了改进Transformer效率的方法，但通常需要计算密集型的重新训练过程或依赖于特定的架构特征，从而阻碍了广泛的实际应用。为了解决这些缺点，OPTIN框架利用中间特征蒸馏，捕获模型参数的长程依赖性（称为$\\textit{trajectory}$），在自然语言处理、图像分类、迁移学习和语义分割任务中产生了最先进的结果，而无需重新训练。在给定的FLOP约束下，OPTIN框架将压缩网络同时保持竞争性能。",
    "tldr": "提出了一种名为OPTIN的框架，利用一次性修剪技术和中间特征蒸馏来提高预训练Transformer架构的效率，无需重新训练，并在多项任务中取得最先进的结果。"
}