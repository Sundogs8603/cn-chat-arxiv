{
    "title": "Teacher-Student Training for Debiasing: General Permutation Debiasing for Large Language Models",
    "abstract": "arXiv:2403.13590v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated impressive zero-shot capabilities and versatility in NLP tasks, however they sometimes fail to maintain crucial invariances for specific tasks. One example is permutation sensitivity, where LLMs' outputs may significantly vary depending on the order of the input options. While debiasing techniques can mitigate these issues, and yield better performance and reliability, they often come with a high computational cost at inference. This paper addresses this inefficiency at inference time. The aim is to distill the capabilities of a computationally intensive, debiased, teacher model into a more compact student model. We explore two variants of student models: one based on pure distillation, and the other on an error-correction approach for more complex tasks, where the student corrects a single biased decision from the teacher to achieve a debiased output. Our approach is general and can be appl",
    "link": "https://arxiv.org/abs/2403.13590",
    "context": "Title: Teacher-Student Training for Debiasing: General Permutation Debiasing for Large Language Models\nAbstract: arXiv:2403.13590v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated impressive zero-shot capabilities and versatility in NLP tasks, however they sometimes fail to maintain crucial invariances for specific tasks. One example is permutation sensitivity, where LLMs' outputs may significantly vary depending on the order of the input options. While debiasing techniques can mitigate these issues, and yield better performance and reliability, they often come with a high computational cost at inference. This paper addresses this inefficiency at inference time. The aim is to distill the capabilities of a computationally intensive, debiased, teacher model into a more compact student model. We explore two variants of student models: one based on pure distillation, and the other on an error-correction approach for more complex tasks, where the student corrects a single biased decision from the teacher to achieve a debiased output. Our approach is general and can be appl",
    "path": "papers/24/03/2403.13590.json",
    "total_tokens": 949,
    "translated_title": "老师-学生培训用于去偏见：大型语言模型的通用置换去偏见",
    "translated_abstract": "大型语言模型（LLMs）展示了在自然语言处理任务中令人印象深刻的零-shot能力和多功能性，然而它们有时会无法保持特定任务的关键不变性。一个例子就是置换敏感性，LLMs的输出可能会根据输入选项的顺序不同而明显不同。虽然去偏见技术可以减轻这些问题，并带来更好的性能和可靠性，但它们在推理时往往会伴随着高计算成本。本文解决了这种推理时的低效率。其目的是将计算密集型、去偏见的老师模型的能力蒸馏到一个更紧凑的学生模型中。我们探讨了两种学生模型的变种：一种基于纯粹蒸馏，另一种基于纠正方法，针对更复杂的任务，学生通过纠正老师的一个有偏决策来实现去偏见的输出。我们的方法是通用的，并可应用于各种任务和语言模型大小。",
    "tldr": "本论文提出了一种教师-学生培训方法，通过在推理时将高计算成本的去偏见老师模型的能力蒸馏到更紧凑的学生模型中，来解决大型语言模型在保持特定任务不变性时的低效率问题。",
    "en_tdlr": "This paper introduces a teacher-student training approach to address the inefficiency of large language models in maintaining specific task invariances by distilling the capabilities of a computationally intensive, debiased teacher model into a more compact student model at inference time."
}