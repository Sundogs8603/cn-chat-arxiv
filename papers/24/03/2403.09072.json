{
    "title": "UniCode: Learning a Unified Codebook for Multimodal Large Language Models",
    "abstract": "arXiv:2403.09072v1 Announce Type: cross  Abstract: In this paper, we propose \\textbf{UniCode}, a novel approach within the domain of multimodal large language models (MLLMs) that learns a unified codebook to efficiently tokenize visual, text, and potentially other types of signals. This innovation addresses a critical limitation in existing MLLMs: their reliance on a text-only codebook, which restricts MLLM's ability to generate images and texts in a multimodal context. Towards this end, we propose a language-driven iterative training paradigm, coupled with an in-context pre-training task we term ``image decompression'', enabling our model to interpret compressed visual data and generate high-quality images.The unified codebook empowers our model to extend visual instruction tuning to non-linguistic generation tasks. Moreover, UniCode is adaptable to diverse stacked quantization approaches in order to compress visual signals into a more compact token representation. Despite using signi",
    "link": "https://arxiv.org/abs/2403.09072",
    "context": "Title: UniCode: Learning a Unified Codebook for Multimodal Large Language Models\nAbstract: arXiv:2403.09072v1 Announce Type: cross  Abstract: In this paper, we propose \\textbf{UniCode}, a novel approach within the domain of multimodal large language models (MLLMs) that learns a unified codebook to efficiently tokenize visual, text, and potentially other types of signals. This innovation addresses a critical limitation in existing MLLMs: their reliance on a text-only codebook, which restricts MLLM's ability to generate images and texts in a multimodal context. Towards this end, we propose a language-driven iterative training paradigm, coupled with an in-context pre-training task we term ``image decompression'', enabling our model to interpret compressed visual data and generate high-quality images.The unified codebook empowers our model to extend visual instruction tuning to non-linguistic generation tasks. Moreover, UniCode is adaptable to diverse stacked quantization approaches in order to compress visual signals into a more compact token representation. Despite using signi",
    "path": "papers/24/03/2403.09072.json",
    "total_tokens": 879,
    "translated_title": "UniCode: 学习用于多模大语言模型的统一码书",
    "translated_abstract": "在本文中，我们提出了一种名为UniCode的新方法，该方法属于多模大语言模型（MLLMs）领域，它学习了一个统一的码书来高效地标记视觉、文本和潜在其他类型的信号。这一创新解决了现有MLLMs存在的一个关键局限：它们依赖于仅限于文本的码书，这限制了MLLM在多模态环境中生成图像和文本的能力。为此，我们提出了一种以语言驱动的迭代训练范式，结合我们称之为“图像解压缩”的上下文预训练任务，使我们的模型能够解释压缩的视觉数据并生成高质量的图像。统一的码书使我们的模型能够将视觉指令调整扩展到非语言生成任务。此外，UniCode适应了各种叠加量化方法，以将视觉信号压缩为更紧凑的标记表示。",
    "tldr": "UniCode提出一种学习统一码书的方法，解决多模大语言模型中对视觉和文本进行标记的关键问题，使模型能够生成高质量的图像，并可适应各种压缩方法。",
    "en_tdlr": "UniCode proposes a method to learn a unified codebook for multimodal large language models, addressing the critical issue of tokenizing visual and text signals, enabling high-quality image generation, and adaptability to various compression methods."
}