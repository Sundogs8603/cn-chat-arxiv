{
    "title": "GOLD: Generalized Knowledge Distillation via Out-of-Distribution-Guided Language Data Generation",
    "abstract": "arXiv:2403.19754v1 Announce Type: new  Abstract: Knowledge distillation from LLMs is essential for the efficient deployment of language models. Prior works have proposed data generation using LLMs for preparing distilled models. We argue that generating data with LLMs is prone to sampling mainly from the center of original content distribution. This limitation hinders the distilled model from learning the true underlying data distribution and to forget the tails of the distributions (samples with lower probability). To this end, we propose GOLD, a task-agnostic data generation and knowledge distillation framework, which employs an iterative out-of-distribution-guided feedback mechanism for the LLM. As a result, the generated data improves the generalizability of distilled models. An energy-based OOD evaluation approach is also introduced to deal with noisy generated data. Our extensive experiments on 10 different classification and sequence-to-sequence tasks in NLP show that GOLD respe",
    "link": "https://arxiv.org/abs/2403.19754",
    "context": "Title: GOLD: Generalized Knowledge Distillation via Out-of-Distribution-Guided Language Data Generation\nAbstract: arXiv:2403.19754v1 Announce Type: new  Abstract: Knowledge distillation from LLMs is essential for the efficient deployment of language models. Prior works have proposed data generation using LLMs for preparing distilled models. We argue that generating data with LLMs is prone to sampling mainly from the center of original content distribution. This limitation hinders the distilled model from learning the true underlying data distribution and to forget the tails of the distributions (samples with lower probability). To this end, we propose GOLD, a task-agnostic data generation and knowledge distillation framework, which employs an iterative out-of-distribution-guided feedback mechanism for the LLM. As a result, the generated data improves the generalizability of distilled models. An energy-based OOD evaluation approach is also introduced to deal with noisy generated data. Our extensive experiments on 10 different classification and sequence-to-sequence tasks in NLP show that GOLD respe",
    "path": "papers/24/03/2403.19754.json",
    "total_tokens": 885,
    "translated_title": "GOLD: 通过超出分布引导的语言数据生成实现通用知识蒸馏",
    "translated_abstract": "arXiv:2403.19754v1 通告类型：新 抽象：从LLMs进行知识蒸馏对于有效部署语言模型至关重要。之前的研究提出使用LLMs生成数据来准备蒸馏模型。我们认为使用LLMs生成数据容易从原始内容分布的中心进行抽样。这种局限性阻碍了蒸馏模型学习真实的潜在数据分布并且遗忘分布的尾部（具有较低概率的样本）。因此，我们提出了GOLD，一个任务无关的数据生成和知识蒸馏框架，它采用迭代的超出分布引导反馈机制用于LLM。因此，生成的数据提高了蒸馏模型的泛化能力。同时，还引入了基于能量的OOD评估方法来处理嘈杂的生成数据。我们在NLP的10个不同分类和序列到序列任务上进行了大量实验证明GOLD respe",
    "tldr": "GOLD提出了一种任务无关的数据生成和知识蒸馏框架，通过使用超出分布引导的反馈机制，提高了生成数据的泛化能力，并引入了处理嘈杂生成数据的基于能量的OOD评估方法。"
}