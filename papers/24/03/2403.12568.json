{
    "title": "Memory-Efficient and Secure DNN Inference on TrustZone-enabled Consumer IoT Devices",
    "abstract": "arXiv:2403.12568v1 Announce Type: cross  Abstract: Edge intelligence enables resource-demanding Deep Neural Network (DNN) inference without transferring original data, addressing concerns about data privacy in consumer Internet of Things (IoT) devices. For privacy-sensitive applications, deploying models in hardware-isolated trusted execution environments (TEEs) becomes essential. However, the limited secure memory in TEEs poses challenges for deploying DNN inference, and alternative techniques like model partitioning and offloading introduce performance degradation and security issues. In this paper, we present a novel approach for advanced model deployment in TrustZone that ensures comprehensive privacy preservation during model inference. We design a memory-efficient management method to support memory-demanding inference in TEEs. By adjusting the memory priority, we effectively mitigate memory leakage risks and memory overlap conflicts, resulting in 32 lines of code alterations in ",
    "link": "https://arxiv.org/abs/2403.12568",
    "context": "Title: Memory-Efficient and Secure DNN Inference on TrustZone-enabled Consumer IoT Devices\nAbstract: arXiv:2403.12568v1 Announce Type: cross  Abstract: Edge intelligence enables resource-demanding Deep Neural Network (DNN) inference without transferring original data, addressing concerns about data privacy in consumer Internet of Things (IoT) devices. For privacy-sensitive applications, deploying models in hardware-isolated trusted execution environments (TEEs) becomes essential. However, the limited secure memory in TEEs poses challenges for deploying DNN inference, and alternative techniques like model partitioning and offloading introduce performance degradation and security issues. In this paper, we present a novel approach for advanced model deployment in TrustZone that ensures comprehensive privacy preservation during model inference. We design a memory-efficient management method to support memory-demanding inference in TEEs. By adjusting the memory priority, we effectively mitigate memory leakage risks and memory overlap conflicts, resulting in 32 lines of code alterations in ",
    "path": "papers/24/03/2403.12568.json",
    "total_tokens": 817,
    "translated_title": "基于TrustZone启用的消费者IoT设备上的内存高效和安全的DNN推断",
    "translated_abstract": "边缘智能使资源需求高的深度神经网络（DNN）推理成为可能，而无需传输原始数据，从而解决了消费者物联网（IoT）设备中数据隐私的担忧。针对隐私敏感型应用，将模型部署在硬件隔离的受信执行环境（TEEs）中变得至关重要。然而，TEE中有限的安全内存对于部署DNN推断构成挑战，而诸如模型分区和卸载等替代技术则引入了性能降级和安全问题。本文提出了一种新颖的方法，用于在TrustZone中实现先进的模型部署，以确保模型推断期间全面保护隐私。我们设计了一种内存高效管理方法，以支持TEE中的内存需求推断。通过调整内存优先级，我们有效地减轻了内存泄漏风险和内存重叠冲突，从而导致对32行代码进行修改。",
    "tldr": "提出了一种在TrustZone中进行高级模型部署的新方法，确保模型推断期间全面保护隐私。",
    "en_tdlr": "Proposed a novel approach for advanced model deployment in TrustZone that ensures comprehensive privacy preservation during model inference."
}