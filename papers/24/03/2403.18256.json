{
    "title": "Manipulating Neural Path Planners via Slight Perturbations",
    "abstract": "arXiv:2403.18256v1 Announce Type: cross  Abstract: Data-driven neural path planners are attracting increasing interest in the robotics community. However, their neural network components typically come as black boxes, obscuring their underlying decision-making processes. Their black-box nature exposes them to the risk of being compromised via the insertion of hidden malicious behaviors. For example, an attacker may hide behaviors that, when triggered, hijack a delivery robot by guiding it to a specific (albeit wrong) destination, trapping it in a predefined region, or inducing unnecessary energy expenditure by causing the robot to repeatedly circle a region. In this paper, we propose a novel approach to specify and inject a range of hidden malicious behaviors, known as backdoors, into neural path planners. Our approach provides a concise but flexible way to define these behaviors, and we show that hidden behaviors can be triggered by slight perturbations (e.g., inserting a tiny unnotic",
    "link": "https://arxiv.org/abs/2403.18256",
    "context": "Title: Manipulating Neural Path Planners via Slight Perturbations\nAbstract: arXiv:2403.18256v1 Announce Type: cross  Abstract: Data-driven neural path planners are attracting increasing interest in the robotics community. However, their neural network components typically come as black boxes, obscuring their underlying decision-making processes. Their black-box nature exposes them to the risk of being compromised via the insertion of hidden malicious behaviors. For example, an attacker may hide behaviors that, when triggered, hijack a delivery robot by guiding it to a specific (albeit wrong) destination, trapping it in a predefined region, or inducing unnecessary energy expenditure by causing the robot to repeatedly circle a region. In this paper, we propose a novel approach to specify and inject a range of hidden malicious behaviors, known as backdoors, into neural path planners. Our approach provides a concise but flexible way to define these behaviors, and we show that hidden behaviors can be triggered by slight perturbations (e.g., inserting a tiny unnotic",
    "path": "papers/24/03/2403.18256.json",
    "total_tokens": 776,
    "translated_title": "通过轻微扰动操纵神经路径规划器",
    "translated_abstract": "数据驱动的神经路径规划器在机器人领域越来越受到关注。然而，它们的神经网络部件通常作为黑匣子呈现，掩盖了其基础决策过程。它们的黑匣子性质使它们面临被通过插入隐藏恶意行为来篡改的风险。本文提出了一种新颖的方法，用于指定和注入各种隐藏恶意行为，称为后门，到神经路径规划器中。我们的方法提供了一种简洁但灵活的定义这些行为的方式，我们展示了隐藏行为可以通过轻微扰动（例如，插入微小的不明显的扰动）来触发。",
    "tldr": "在这篇论文中，我们提出了一种新颖的方法，可以通过轻微扰动来指定和注入各种隐藏的恶意行为，即后门，到神经路径规划器中。",
    "en_tdlr": "This paper proposes a novel approach to specify and inject a range of hidden malicious behaviors, known as backdoors, into neural path planners, which can be triggered by slight perturbations."
}