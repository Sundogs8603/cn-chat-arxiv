{
    "title": "Fantastic Semantics and Where to Find Them: Investigating Which Layers of Generative LLMs Reflect Lexical Semantics",
    "abstract": "arXiv:2403.01509v1 Announce Type: new  Abstract: Large language models have achieved remarkable success in general language understanding tasks. However, as a family of generative methods with the objective of next token prediction, the semantic evolution with the depth of these models are not fully explored, unlike their predecessors, such as BERT-like architectures. In this paper, we specifically investigate the bottom-up evolution of lexical semantics for a popular LLM, namely Llama2, by probing its hidden states at the end of each layer using a contextualized word identification task. Our experiments show that the representations in lower layers encode lexical semantics, while the higher layers, with weaker semantic induction, are responsible for prediction. This is in contrast to models with discriminative objectives, such as mask language modeling, where the higher layers obtain better lexical semantics. The conclusion is further supported by the monotonic increase in performance",
    "link": "https://arxiv.org/abs/2403.01509",
    "context": "Title: Fantastic Semantics and Where to Find Them: Investigating Which Layers of Generative LLMs Reflect Lexical Semantics\nAbstract: arXiv:2403.01509v1 Announce Type: new  Abstract: Large language models have achieved remarkable success in general language understanding tasks. However, as a family of generative methods with the objective of next token prediction, the semantic evolution with the depth of these models are not fully explored, unlike their predecessors, such as BERT-like architectures. In this paper, we specifically investigate the bottom-up evolution of lexical semantics for a popular LLM, namely Llama2, by probing its hidden states at the end of each layer using a contextualized word identification task. Our experiments show that the representations in lower layers encode lexical semantics, while the higher layers, with weaker semantic induction, are responsible for prediction. This is in contrast to models with discriminative objectives, such as mask language modeling, where the higher layers obtain better lexical semantics. The conclusion is further supported by the monotonic increase in performance",
    "path": "papers/24/03/2403.01509.json",
    "total_tokens": 907,
    "translated_title": "奇幻语义与寻找之地：探讨生成型LLM的哪些层次反映词汇语义",
    "translated_abstract": "大型语言模型在一般语言理解任务中取得了显著的成功。然而，作为一个追求下一个标记预测目标的生成方法家族，这些模型的语义演变随着深度并没有得到充分探索，不像它们的前辈，比如BERT类架构。本文具体研究了一款流行LLM，即Llama2的自下而上词汇语义演变，通过在每个层的末端探测其隐藏状态，使用一个上下文化的词识别任务。我们的实验表明，较低层的表示编码了词汇语义，而较高层，其语义归纳较弱，负责预测。这与具有判别目标的模型形成对比，比如掩码语言建模，在那里较高层获得更好的词汇语义。结论进一步得到性能单调增加的支持。",
    "tldr": "通过在每个层的末端探测其隐藏状态，使用一个上下文化的词识别任务，本文具体研究了Llama2的自下而上词汇语义演变，发现较低层的表示编码了词汇语义，而较高层负责预测。",
    "en_tdlr": "By probing the hidden states at the end of each layer using a contextualized word identification task, this paper specifically investigated the bottom-up evolution of lexical semantics for Llama2, revealing that representations in lower layers encode lexical semantics while higher layers are responsible for prediction."
}