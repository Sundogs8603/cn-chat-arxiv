{
    "title": "Variance-Dependent Regret Bounds for Non-stationary Linear Bandits",
    "abstract": "arXiv:2403.10732v1 Announce Type: cross  Abstract: We investigate the non-stationary stochastic linear bandit problem where the reward distribution evolves each round. Existing algorithms characterize the non-stationarity by the total variation budget $B_K$, which is the summation of the change of the consecutive feature vectors of the linear bandits over $K$ rounds. However, such a quantity only measures the non-stationarity with respect to the expectation of the reward distribution, which makes existing algorithms sub-optimal under the general non-stationary distribution setting. In this work, we propose algorithms that utilize the variance of the reward distribution as well as the $B_K$, and show that they can achieve tighter regret upper bounds. Specifically, we introduce two novel algorithms: Restarted Weighted$\\text{OFUL}^+$ and Restarted $\\text{SAVE}^+$. These algorithms address cases where the variance information of the rewards is known and unknown, respectively. Notably, when",
    "link": "https://arxiv.org/abs/2403.10732",
    "context": "Title: Variance-Dependent Regret Bounds for Non-stationary Linear Bandits\nAbstract: arXiv:2403.10732v1 Announce Type: cross  Abstract: We investigate the non-stationary stochastic linear bandit problem where the reward distribution evolves each round. Existing algorithms characterize the non-stationarity by the total variation budget $B_K$, which is the summation of the change of the consecutive feature vectors of the linear bandits over $K$ rounds. However, such a quantity only measures the non-stationarity with respect to the expectation of the reward distribution, which makes existing algorithms sub-optimal under the general non-stationary distribution setting. In this work, we propose algorithms that utilize the variance of the reward distribution as well as the $B_K$, and show that they can achieve tighter regret upper bounds. Specifically, we introduce two novel algorithms: Restarted Weighted$\\text{OFUL}^+$ and Restarted $\\text{SAVE}^+$. These algorithms address cases where the variance information of the rewards is known and unknown, respectively. Notably, when",
    "path": "papers/24/03/2403.10732.json",
    "total_tokens": 822,
    "translated_title": "针对非平稳线性赌博机的方差相关遗憾界限",
    "translated_abstract": "我们研究了非平稳随机线性赌博机问题，其中奖励分布每一轮都在演变。现有算法通过总变化预算$B_K$来表征非平稳性，该预算是线性赌博机每$K$轮连续特征向量变化的总和。然而，这样的量只衡量了相对于奖励分布期望的非平稳性，这使得现有算法在一般非平稳分布情况下表现不佳。在这项工作中，我们提出了利用奖励分布方差以及$B_K$的算法，并展示它们可以实现更紧的遗憾上限界限。具体来说，我们介绍了两种新算法: 重新启动的加权$\\text{OFUL}^+$和重新启动的$\\text{SAVE}^+$。这些算法分别处理了奖励方差信息已知和未知的情况。",
    "tldr": "提出利用奖励分布方差和变化预算的算法，可以实现更紧的遗憾上限界限。",
    "en_tdlr": "Proposed algorithms that utilize the variance of the reward distribution and the total variation budget can achieve tighter regret upper bounds."
}