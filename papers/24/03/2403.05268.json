{
    "title": "Deep Prompt Multi-task Network for Abuse Language Detection",
    "abstract": "arXiv:2403.05268v1 Announce Type: cross  Abstract: The detection of abusive language remains a long-standing challenge with the extensive use of social networks. The detection task of abusive language suffers from limited accuracy. We argue that the existing detection methods utilize the fine-tuning technique of the pre-trained language models (PLMs) to handle downstream tasks. Hence, these methods fail to stimulate the general knowledge of the PLMs. To address the problem, we propose a novel Deep Prompt Multi-task Network (DPMN) for abuse language detection. Specifically, DPMN first attempts to design two forms of deep prompt tuning and light prompt tuning for the PLMs. The effects of different prompt lengths, tuning strategies, and prompt initialization methods on detecting abusive language are studied. In addition, we propose a Task Head based on Bi-LSTM and FFN, which can be used as a short text classifier. Eventually, DPMN utilizes multi-task learning to improve detection metrics ",
    "link": "https://arxiv.org/abs/2403.05268",
    "context": "Title: Deep Prompt Multi-task Network for Abuse Language Detection\nAbstract: arXiv:2403.05268v1 Announce Type: cross  Abstract: The detection of abusive language remains a long-standing challenge with the extensive use of social networks. The detection task of abusive language suffers from limited accuracy. We argue that the existing detection methods utilize the fine-tuning technique of the pre-trained language models (PLMs) to handle downstream tasks. Hence, these methods fail to stimulate the general knowledge of the PLMs. To address the problem, we propose a novel Deep Prompt Multi-task Network (DPMN) for abuse language detection. Specifically, DPMN first attempts to design two forms of deep prompt tuning and light prompt tuning for the PLMs. The effects of different prompt lengths, tuning strategies, and prompt initialization methods on detecting abusive language are studied. In addition, we propose a Task Head based on Bi-LSTM and FFN, which can be used as a short text classifier. Eventually, DPMN utilizes multi-task learning to improve detection metrics ",
    "path": "papers/24/03/2403.05268.json",
    "total_tokens": 878,
    "translated_title": "深度提示多任务网络用于辱骂语言检测",
    "translated_abstract": "滥用语言的检测仍然是社交网络广泛使用中存在的一项长期挑战。滥用语言检测任务存在着准确性有限的问题。我们认为现有的检测方法利用了预训练语言模型（PLMs）的微调技术来处理下游任务。因此，这些方法未能激发PLMs的一般知识。为了解决这个问题，我们提出了一种新颖的用于滥用语言检测的深度提示多任务网络（DPMN）。具体而言，DPMN首先尝试为PLMs设计两种形式的深度提示调整和轻提示调整。研究了不同提示长度、调整策略和提示初始化方法对于检测滥用语言的影响。此外，我们提出了基于Bi-LSTM和FFN的任务头，可用作短文本分类器。最终，DPMN利用多任务学习来提高检测度量标准。",
    "tldr": "提出了一种新颖的Deep Prompt Multi-task Network (DPMN)用于滥用语言检测，通过设计深度提示调整和轻提示调整来激发预训练语言模型的一般知识，并利用多任务学习来提高检测度量标准",
    "en_tdlr": "Introduced a novel Deep Prompt Multi-task Network (DPMN) for abuse language detection, which stimulates the general knowledge of pre-trained language models by designing deep prompt tuning and light prompt tuning, and improves detection metrics using multi-task learning."
}