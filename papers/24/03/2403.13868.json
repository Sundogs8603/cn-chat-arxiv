{
    "title": "Analysing heavy-tail properties of Stochastic Gradient Descent by means of Stochastic Recurrence Equations",
    "abstract": "arXiv:2403.13868v1 Announce Type: cross  Abstract: In recent works on the theory of machine learning, it has been observed that heavy tail properties of Stochastic Gradient Descent (SGD) can be studied in the probabilistic framework of stochastic recursions. In particular, G\\\"{u}rb\\\"{u}zbalaban et al. (arXiv:2006.04740) considered a setup corresponding to linear regression for which iterations of SGD can be modelled by a multivariate affine stochastic recursion $X_k=A_k X_{k-1}+B_k$, for independent and identically distributed pairs $(A_k, B_k)$, where $A_k$ is a random symmetric matrix and $B_k$ is a random vector. In this work, we will answer several open questions of the quoted paper and extend their results by applying the theory of irreducible-proximal (i-p) matrices.",
    "link": "https://arxiv.org/abs/2403.13868",
    "context": "Title: Analysing heavy-tail properties of Stochastic Gradient Descent by means of Stochastic Recurrence Equations\nAbstract: arXiv:2403.13868v1 Announce Type: cross  Abstract: In recent works on the theory of machine learning, it has been observed that heavy tail properties of Stochastic Gradient Descent (SGD) can be studied in the probabilistic framework of stochastic recursions. In particular, G\\\"{u}rb\\\"{u}zbalaban et al. (arXiv:2006.04740) considered a setup corresponding to linear regression for which iterations of SGD can be modelled by a multivariate affine stochastic recursion $X_k=A_k X_{k-1}+B_k$, for independent and identically distributed pairs $(A_k, B_k)$, where $A_k$ is a random symmetric matrix and $B_k$ is a random vector. In this work, we will answer several open questions of the quoted paper and extend their results by applying the theory of irreducible-proximal (i-p) matrices.",
    "path": "papers/24/03/2403.13868.json",
    "total_tokens": 868,
    "translated_title": "通过随机递归方程分析随机梯度下降的重尾特性",
    "translated_abstract": "在机器学习理论的最近研究中，观察到可以在随机递归的概率框架下研究随机梯度下降（SGD）的重尾特性。特别地，G\\\"{u}rb\\\"{u}zbalaban等人（arXiv:2006.04740）考虑了一个对应于线性回归的设置，其中SGD的迭代可以通过多变量仿射随机递归$X_k=A_k X_{k-1}+B_k$来建模，其中$(A_k, B_k)$是独立同分布对，$A_k$是一个随机对称矩阵，$B_k$是一个随机向量。本文将回答引用论文中的几个未解问题，并通过应用不可约-近端（i-p）矩阵理论扩展他们的结果。",
    "tldr": "本文通过随机递归方程的概率框架，研究了随机梯度下降的重尾特性，并通过i-p矩阵理论扩展了G\\\"{u}rb\\\"{u}zbalaban等人的结果。",
    "en_tdlr": "This paper analyzes the heavy-tail properties of Stochastic Gradient Descent in a probabilistic framework of stochastic recursions, extends the results of G\\\"{u}rb\\\"{u}zbalaban et al., and addresses several open questions by applying the theory of irreducible-proximal matrices."
}