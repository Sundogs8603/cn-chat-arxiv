{
    "title": "Influencing Bandits: Arm Selection for Preference Shaping",
    "abstract": "arXiv:2403.00036v1 Announce Type: cross  Abstract: We consider a non stationary multi-armed bandit in which the population preferences are positively and negatively reinforced by the observed rewards. The objective of the algorithm is to shape the population preferences to maximize the fraction of the population favouring a predetermined arm. For the case of binary opinions, two types of opinion dynamics are considered -- decreasing elasticity (modeled as a Polya urn with increasing number of balls) and constant elasticity (using the voter model). For the first case, we describe an Explore-then-commit policy and a Thompson sampling policy and analyse the regret for each of these policies. We then show that these algorithms and their analyses carry over to the constant elasticity case. We also describe a Thompson sampling based algorithm for the case when more than two types of opinions are present. Finally, we discuss the case where presence of multiple recommendation systems gives ris",
    "link": "https://arxiv.org/abs/2403.00036",
    "context": "Title: Influencing Bandits: Arm Selection for Preference Shaping\nAbstract: arXiv:2403.00036v1 Announce Type: cross  Abstract: We consider a non stationary multi-armed bandit in which the population preferences are positively and negatively reinforced by the observed rewards. The objective of the algorithm is to shape the population preferences to maximize the fraction of the population favouring a predetermined arm. For the case of binary opinions, two types of opinion dynamics are considered -- decreasing elasticity (modeled as a Polya urn with increasing number of balls) and constant elasticity (using the voter model). For the first case, we describe an Explore-then-commit policy and a Thompson sampling policy and analyse the regret for each of these policies. We then show that these algorithms and their analyses carry over to the constant elasticity case. We also describe a Thompson sampling based algorithm for the case when more than two types of opinions are present. Finally, we discuss the case where presence of multiple recommendation systems gives ris",
    "path": "papers/24/03/2403.00036.json",
    "total_tokens": 930,
    "translated_title": "影响Bandits：用于形塑偏好的手臂选择",
    "translated_abstract": "我们考虑一个非静态多臂赌博机，在这其中人群的偏好受到观察到的奖励的积极和消极强化。算法的目标是塑造人群的偏好，以最大化支持预定手臂的人口比例。对于二元意见的情况，考虑了两种意见动态 -- 递减弹性（建模为具有增加球数的Polya采样）和常量弹性（使用投票者模型）。对于第一种情况，我们描述了一种探索-然后-承诺策略和一种Thompson采样策略，并分析了每种策略的后悔。然后，我们展示了这些算法及其分析可推广到常弹性情况。我们还描述了一种基于Thompson采样的算法，用于当存在两种以上类型的意见情况。最后，我们讨论了存在多个推荐系统的情况引发的情况。",
    "tldr": "该论文考虑了在非静态多臂赌博机中，通过观察奖励来积极和消极地强化人群偏好，并提出了用于最大化支持预定手臂的人口比例的算法。对于不同意见动态，提出了不同的策略并分析了后悔，最后讨论了多个推荐系统共存的情况。"
}