{
    "title": "Measuring the Energy Consumption and Efficiency of Deep Neural Networks: An Empirical Analysis and Design Recommendations",
    "abstract": "arXiv:2403.08151v1 Announce Type: cross  Abstract: Addressing the so-called ``Red-AI'' trend of rising energy consumption by large-scale neural networks, this study investigates the actual energy consumption, as measured by node-level watt-meters, of training various fully connected neural network architectures. We introduce the BUTTER-E dataset, an augmentation to the BUTTER Empirical Deep Learning dataset, containing energy consumption and performance data from 63,527 individual experimental runs spanning 30,582 distinct configurations: 13 datasets, 20 sizes (number of trainable parameters), 8 network ``shapes'', and 14 depths on both CPU and GPU hardware collected using node-level watt-meters. This dataset reveals the complex relationship between dataset size, network structure, and energy use, and highlights the impact of cache effects. We propose a straightforward and effective energy model that accounts for network size, computing, and memory hierarchy. Our analysis also uncovers",
    "link": "https://arxiv.org/abs/2403.08151",
    "context": "Title: Measuring the Energy Consumption and Efficiency of Deep Neural Networks: An Empirical Analysis and Design Recommendations\nAbstract: arXiv:2403.08151v1 Announce Type: cross  Abstract: Addressing the so-called ``Red-AI'' trend of rising energy consumption by large-scale neural networks, this study investigates the actual energy consumption, as measured by node-level watt-meters, of training various fully connected neural network architectures. We introduce the BUTTER-E dataset, an augmentation to the BUTTER Empirical Deep Learning dataset, containing energy consumption and performance data from 63,527 individual experimental runs spanning 30,582 distinct configurations: 13 datasets, 20 sizes (number of trainable parameters), 8 network ``shapes'', and 14 depths on both CPU and GPU hardware collected using node-level watt-meters. This dataset reveals the complex relationship between dataset size, network structure, and energy use, and highlights the impact of cache effects. We propose a straightforward and effective energy model that accounts for network size, computing, and memory hierarchy. Our analysis also uncovers",
    "path": "papers/24/03/2403.08151.json",
    "total_tokens": 865,
    "translated_title": "衡量深度神经网络的能耗和效率：实证分析与设计建议",
    "translated_abstract": "针对大规模神经网络日益增长的能耗问题（所谓的“红色AI”趋势），本研究通过节点级瓦特表测量了训练各种全连接神经网络架构的实际能耗。我们介绍了BUTTER-E数据集，这是BUTTER实证深度学习数据集的一个扩充，包含了来自63,527个单独实验运行的能耗和性能数据，涵盖了30,582个不同的配置：13个数据集、20个大小（可训练参数数量）、8个网络“形状”和14个深度，以及在CPU和GPU硬件上使用节点级瓦特表收集的数据。这个数据集揭示了数据集大小、网络结构和能耗之间复杂的关系，并突出了缓存效应的影响。我们提出了一个简单而有效的能耗模型，考虑了网络大小、计算和内存层次结构。我们的分析还揭示了",
    "tldr": "这项研究针对大规模神经网络不断增长的能耗问题进行了实证分析，提出了一个考虑网络尺寸、计算和内存层次结构的能耗模型。",
    "en_tdlr": "This study conducted an empirical analysis on the escalating energy consumption of large-scale neural networks and proposed an energy model that takes into account network size, computing, and memory hierarchy."
}