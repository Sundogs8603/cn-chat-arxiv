{
    "title": "Efficient Adapter Tuning of Pre-trained Speech Models for Automatic Speaker Verification",
    "abstract": "arXiv:2403.00293v1 Announce Type: cross  Abstract: With excellent generalization ability, self-supervised speech models have shown impressive performance on various downstream speech tasks in the pre-training and fine-tuning paradigm. However, as the growing size of pre-trained models, fine-tuning becomes practically unfeasible due to heavy computation and storage overhead, as well as the risk of overfitting. Adapters are lightweight modules inserted into pre-trained models to facilitate parameter-efficient adaptation. In this paper, we propose an effective adapter framework designed for adapting self-supervised speech models to the speaker verification task. With a parallel adapter design, our proposed framework inserts two types of adapters into the pre-trained model, allowing the adaptation of latent features within intermediate Transformer layers and output embeddings from all Transformer layers. We conduct comprehensive experiments to validate the efficiency and effectiveness of t",
    "link": "https://arxiv.org/abs/2403.00293",
    "context": "Title: Efficient Adapter Tuning of Pre-trained Speech Models for Automatic Speaker Verification\nAbstract: arXiv:2403.00293v1 Announce Type: cross  Abstract: With excellent generalization ability, self-supervised speech models have shown impressive performance on various downstream speech tasks in the pre-training and fine-tuning paradigm. However, as the growing size of pre-trained models, fine-tuning becomes practically unfeasible due to heavy computation and storage overhead, as well as the risk of overfitting. Adapters are lightweight modules inserted into pre-trained models to facilitate parameter-efficient adaptation. In this paper, we propose an effective adapter framework designed for adapting self-supervised speech models to the speaker verification task. With a parallel adapter design, our proposed framework inserts two types of adapters into the pre-trained model, allowing the adaptation of latent features within intermediate Transformer layers and output embeddings from all Transformer layers. We conduct comprehensive experiments to validate the efficiency and effectiveness of t",
    "path": "papers/24/03/2403.00293.json",
    "total_tokens": 848,
    "translated_title": "为自动说话者验证效率调整预训练语音模型的适配器",
    "translated_abstract": "带有出色泛化能力的自监督语音模型在预训练和微调范式中在各种下游语音任务上展现出令人印象深刻的性能。然而，随着预训练模型规模的增长，微调变得不可行，因为需要巨大的计算和存储开销，以及过拟合的风险。适配器是轻量级模块，插入到预训练模型中以促进参数高效的调整。本文提出了一个有效的适配器框架，旨在将自监督语音模型调整为说话者验证任务。通过并行适配器设计，我们的框架将两种类型的适配器插入到预训练模型中，允许在中间Transformer层中调整潜在特征以及在所有Transformer层中的输出嵌入。我们进行了全面实验来验证该方法的效率和有效性。",
    "tldr": "提出了一个有效的适配器框架，旨在将自监督语音模型调整为说话者验证任务，通过并行适配器设计，允许在中间Transformer层调整潜在特征以及在所有Transformer层的输出嵌入",
    "en_tdlr": "Proposed an efficient adapter framework aimed at adapting self-supervised speech models to the speaker verification task, allowing the adjustment of latent features within intermediate Transformer layers and output embeddings from all Transformer layers through a parallel adapter design."
}