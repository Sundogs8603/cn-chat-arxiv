{
    "title": "Posterior concentrations of fully-connected Bayesian neural networks with general priors on the weights",
    "abstract": "arXiv:2403.14225v1 Announce Type: cross  Abstract: Bayesian approaches for training deep neural networks (BNNs) have received significant interest and have been effectively utilized in a wide range of applications. There have been several studies on the properties of posterior concentrations of BNNs. However, most of these studies only demonstrate results in BNN models with sparse or heavy-tailed priors. Surprisingly, no theoretical results currently exist for BNNs using Gaussian priors, which are the most commonly used one. The lack of theory arises from the absence of approximation results of Deep Neural Networks (DNNs) that are non-sparse and have bounded parameters. In this paper, we present a new approximation theory for non-sparse DNNs with bounded parameters. Additionally, based on the approximation theory, we show that BNNs with non-sparse general priors can achieve near-minimax optimal posterior concentration rates to the true model.",
    "link": "https://arxiv.org/abs/2403.14225",
    "context": "Title: Posterior concentrations of fully-connected Bayesian neural networks with general priors on the weights\nAbstract: arXiv:2403.14225v1 Announce Type: cross  Abstract: Bayesian approaches for training deep neural networks (BNNs) have received significant interest and have been effectively utilized in a wide range of applications. There have been several studies on the properties of posterior concentrations of BNNs. However, most of these studies only demonstrate results in BNN models with sparse or heavy-tailed priors. Surprisingly, no theoretical results currently exist for BNNs using Gaussian priors, which are the most commonly used one. The lack of theory arises from the absence of approximation results of Deep Neural Networks (DNNs) that are non-sparse and have bounded parameters. In this paper, we present a new approximation theory for non-sparse DNNs with bounded parameters. Additionally, based on the approximation theory, we show that BNNs with non-sparse general priors can achieve near-minimax optimal posterior concentration rates to the true model.",
    "path": "papers/24/03/2403.14225.json",
    "total_tokens": 855,
    "translated_title": "具有权重通用先验的全连接贝叶斯神经网络的后验浓度",
    "translated_abstract": "训练深度神经网络（BNNs）的贝叶斯方法备受关注，并已在广泛的应用中得到有效利用。先前有关BNNs后验浓度性质的研究已有几项。然而，大多数这些研究仅在具有稀疏或重尾先验的BNN模型中展示结果。令人惊讶的是，目前尚无关于使用高斯先验的BNNs的理论结果，而高斯先验是最常用的先验之一。这种理论缺失源于缺乏近似非稀疏且具有有界参数的深度神经网络（DNNs）的结果。本文提出了用于具有有界参数的非稀疏DNNs的新近似理论。此外，基于这一近似理论，我们展示了具有非稀疏通用先验的BNNs可以实现接近最小化最优后验浓度速率至真实模型的结果。",
    "tldr": "本文提出了一种新的近似理论，表明具有非稀疏通用先验的BNNs可以实现接近最小化最优后验浓度速率至真实模型。",
    "en_tdlr": "This paper presents a new approximation theory showing that BNNs with non-sparse general priors can achieve near-minimax optimal posterior concentration rates to the true model."
}