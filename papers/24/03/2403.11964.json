{
    "title": "Probabilistic Calibration by Design for Neural Network Regression",
    "abstract": "arXiv:2403.11964v1 Announce Type: new  Abstract: Generating calibrated and sharp neural network predictive distributions for regression problems is essential for optimal decision-making in many real-world applications. To address the miscalibration issue of neural networks, various methods have been proposed to improve calibration, including post-hoc methods that adjust predictions after training and regularization methods that act during training. While post-hoc methods have shown better improvement in calibration compared to regularization methods, the post-hoc step is completely independent of model training. We introduce a novel end-to-end model training procedure called Quantile Recalibration Training, integrating post-hoc calibration directly into the training process without additional parameters. We also present a unified algorithm that includes our method and other post-hoc and regularization methods, as particular cases. We demonstrate the performance of our method in a large",
    "link": "https://arxiv.org/abs/2403.11964",
    "context": "Title: Probabilistic Calibration by Design for Neural Network Regression\nAbstract: arXiv:2403.11964v1 Announce Type: new  Abstract: Generating calibrated and sharp neural network predictive distributions for regression problems is essential for optimal decision-making in many real-world applications. To address the miscalibration issue of neural networks, various methods have been proposed to improve calibration, including post-hoc methods that adjust predictions after training and regularization methods that act during training. While post-hoc methods have shown better improvement in calibration compared to regularization methods, the post-hoc step is completely independent of model training. We introduce a novel end-to-end model training procedure called Quantile Recalibration Training, integrating post-hoc calibration directly into the training process without additional parameters. We also present a unified algorithm that includes our method and other post-hoc and regularization methods, as particular cases. We demonstrate the performance of our method in a large",
    "path": "papers/24/03/2403.11964.json",
    "total_tokens": 837,
    "translated_title": "神经网络回归的设计概率校准",
    "translated_abstract": "为了在许多真实世界应用中进行最佳决策，为回归问题生成经过校准且精确的神经网络预测分布至关重要。为解决神经网络的误校准问题，提出了各种改善校准的方法，包括在训练后调整预测的后处理方法和在训练过程中进行操作的正则化方法。虽然与正则化方法相比，后处理方法在校准方面表现出更好的改进，但后处理步骤与模型训练完全独立。我们引入了一种称为Quantile Recalibration Training的新型端到端模型训练过程，将后处理校准直接整合到训练过程中，无需额外的参数。我们还提出了一个统一的算法，将我们的方法和其他后处理方法以及正则化方法作为特殊情况包含在内。我们展示了我们的方法在一个大规模问题上的性能。",
    "tldr": "提出了一种称为Quantile Recalibration Training的新型端到端模型训练过程，将后处理校准直接整合到训练过程中，无需额外参数，展示出在神经网络回归中提高校准性能的方法。"
}