{
    "title": "XB-MAML: Learning Expandable Basis Parameters for Effective Meta-Learning with Wide Task Coverage",
    "abstract": "arXiv:2403.06768v1 Announce Type: new  Abstract: Meta-learning, which pursues an effective initialization model, has emerged as a promising approach to handling unseen tasks. However, a limitation remains to be evident when a meta-learner tries to encompass a wide range of task distribution, e.g., learning across distinctive datasets or domains. Recently, a group of works has attempted to employ multiple model initializations to cover widely-ranging tasks, but they are limited in adaptively expanding initializations. We introduce XB-MAML, which learns expandable basis parameters, where they are linearly combined to form an effective initialization to a given task. XB-MAML observes the discrepancy between the vector space spanned by the basis and fine-tuned parameters to decide whether to expand the basis. Our method surpasses the existing works in the multi-domain meta-learning benchmarks and opens up new chances of meta-learning for obtaining the diverse inductive bias that can be com",
    "link": "https://arxiv.org/abs/2403.06768",
    "context": "Title: XB-MAML: Learning Expandable Basis Parameters for Effective Meta-Learning with Wide Task Coverage\nAbstract: arXiv:2403.06768v1 Announce Type: new  Abstract: Meta-learning, which pursues an effective initialization model, has emerged as a promising approach to handling unseen tasks. However, a limitation remains to be evident when a meta-learner tries to encompass a wide range of task distribution, e.g., learning across distinctive datasets or domains. Recently, a group of works has attempted to employ multiple model initializations to cover widely-ranging tasks, but they are limited in adaptively expanding initializations. We introduce XB-MAML, which learns expandable basis parameters, where they are linearly combined to form an effective initialization to a given task. XB-MAML observes the discrepancy between the vector space spanned by the basis and fine-tuned parameters to decide whether to expand the basis. Our method surpasses the existing works in the multi-domain meta-learning benchmarks and opens up new chances of meta-learning for obtaining the diverse inductive bias that can be com",
    "path": "papers/24/03/2403.06768.json",
    "total_tokens": 867,
    "translated_title": "XB-MAML: 学习可扩展基参数以实现具有广泛任务覆盖范围的有效元学习",
    "translated_abstract": "元学习追求有效的初始化模型，已成为处理未知任务的有前途方法。然而，当元学习器试图涵盖广泛的任务分布时，即跨不同数据集或领域进行学习时，一个限制仍然显而易见。最近，一组作品尝试使用多个模型初始化来覆盖广泛的任务，但它们在自适应扩展初始化方面受限。我们引入了XB-MAML，它学习可扩展的基本参数，其中它们线性组合以形成给定任务的有效初始化。XB-MAML观察基与微调参数张成的向量空间之间的差异，以决定是否扩展基础。我们的方法在多领域元学习基准测试中超越了现有作品，并为获取可以应用于不同领域的多样归纳偏差打开了新的机会。",
    "tldr": "XB-MAML引入了学习可扩展基参数的新方法，通过线性组合形成有效初始化，观察并利用基与微调参数在向量空间的差异，从而在多领域元学习中取得突破性进展。",
    "en_tdlr": "XB-MAML introduces a novel approach of learning expandable basis parameters, forming effective initialization through linear combination, leveraging the discrepancy between basis and fine-tuned parameters in the vector space, achieving breakthroughs in multi-domain meta-learning."
}