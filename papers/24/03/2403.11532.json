{
    "title": "Out-of-Distribution Detection Should Use Conformal Prediction (and Vice-versa?)",
    "abstract": "arXiv:2403.11532v1 Announce Type: cross  Abstract: Research on Out-Of-Distribution (OOD) detection focuses mainly on building scores that efficiently distinguish OOD data from In Distribution (ID) data. On the other hand, Conformal Prediction (CP) uses non-conformity scores to construct prediction sets with probabilistic coverage guarantees. In this work, we propose to use CP to better assess the efficiency of OOD scores. Specifically, we emphasize that in standard OOD benchmark settings, evaluation metrics can be overly optimistic due to the finite sample size of the test dataset. Based on the work of (Bates et al., 2022), we define new conformal AUROC and conformal FRP@TPR95 metrics, which are corrections that provide probabilistic conservativeness guarantees on the variability of these metrics. We show the effect of these corrections on two reference OOD and anomaly detection benchmarks, OpenOOD (Yang et al., 2022) and ADBench (Han et al., 2022). We also show that the benefits of us",
    "link": "https://arxiv.org/abs/2403.11532",
    "context": "Title: Out-of-Distribution Detection Should Use Conformal Prediction (and Vice-versa?)\nAbstract: arXiv:2403.11532v1 Announce Type: cross  Abstract: Research on Out-Of-Distribution (OOD) detection focuses mainly on building scores that efficiently distinguish OOD data from In Distribution (ID) data. On the other hand, Conformal Prediction (CP) uses non-conformity scores to construct prediction sets with probabilistic coverage guarantees. In this work, we propose to use CP to better assess the efficiency of OOD scores. Specifically, we emphasize that in standard OOD benchmark settings, evaluation metrics can be overly optimistic due to the finite sample size of the test dataset. Based on the work of (Bates et al., 2022), we define new conformal AUROC and conformal FRP@TPR95 metrics, which are corrections that provide probabilistic conservativeness guarantees on the variability of these metrics. We show the effect of these corrections on two reference OOD and anomaly detection benchmarks, OpenOOD (Yang et al., 2022) and ADBench (Han et al., 2022). We also show that the benefits of us",
    "path": "papers/24/03/2403.11532.json",
    "total_tokens": 910,
    "translated_title": "应该使用符合预测进行分布外检测（反之亦然？）",
    "translated_abstract": "关于分布外（OOD）检测的研究主要集中在构建有效区分OOD数据和分布内（ID）数据的分数上。另一方面，符合预测（CP）使用非一致性分数构建具有概率覆盖保证的预测集。在这项工作中，我们提出使用CP更好地评估OOD分数的效率。具体而言，我们强调在标准OOD基准设置中，由于测试数据集的有限样本大小，评估指标可能过于乐观。基于（Bates等人，2022）的工作，我们定义了新的符合AUROC和符合FRP@TPR95指标，这些修正提供了关于这些指标变异性的概率保守性保证。我们展示了这些修正对两个参考OOD和异常检测基准OpenOOD（Yang等人，2022）和AD-Bench（Han等人，2022）的影响。我们还展示了我们的作用的好处",
    "tldr": "本研究提出使用符合预测来评估分布外（OOD）检测中效率的新方法，并定义了新的符合AUROC和符合FRP@TPR95指标，为OOD和异常检测基准提供了概率保守性保证。",
    "en_tdlr": "This paper proposes a new method of using conformal prediction to evaluate the efficiency in out-of-distribution (OOD) detection, defining new conformal AUROC and conformal FRP@TPR95 metrics to provide probabilistic conservativeness guarantees for OOD and anomaly detection benchmarks."
}