{
    "title": "LORD: Large Models based Opposite Reward Design for Autonomous Driving",
    "abstract": "arXiv:2403.18965v1 Announce Type: cross  Abstract: Reinforcement learning (RL) based autonomous driving has emerged as a promising alternative to data-driven imitation learning approaches. However, crafting effective reward functions for RL poses challenges due to the complexity of defining and quantifying good driving behaviors across diverse scenarios. Recently, large pretrained models have gained significant attention as zero-shot reward models for tasks specified with desired linguistic goals. However, the desired linguistic goals for autonomous driving such as \"drive safely\" are ambiguous and incomprehensible by pretrained models. On the other hand, undesired linguistic goals like \"collision\" are more concrete and tractable. In this work, we introduce LORD, a novel large models based opposite reward design through undesired linguistic goals to enable the efficient use of large pretrained models as zero-shot reward models. Through extensive experiments, our proposed framework shows",
    "link": "https://arxiv.org/abs/2403.18965",
    "context": "Title: LORD: Large Models based Opposite Reward Design for Autonomous Driving\nAbstract: arXiv:2403.18965v1 Announce Type: cross  Abstract: Reinforcement learning (RL) based autonomous driving has emerged as a promising alternative to data-driven imitation learning approaches. However, crafting effective reward functions for RL poses challenges due to the complexity of defining and quantifying good driving behaviors across diverse scenarios. Recently, large pretrained models have gained significant attention as zero-shot reward models for tasks specified with desired linguistic goals. However, the desired linguistic goals for autonomous driving such as \"drive safely\" are ambiguous and incomprehensible by pretrained models. On the other hand, undesired linguistic goals like \"collision\" are more concrete and tractable. In this work, we introduce LORD, a novel large models based opposite reward design through undesired linguistic goals to enable the efficient use of large pretrained models as zero-shot reward models. Through extensive experiments, our proposed framework shows",
    "path": "papers/24/03/2403.18965.json",
    "total_tokens": 860,
    "translated_title": "LORD：基于大模型的相反奖励设计用于自动驾驶",
    "translated_abstract": "强化学习（RL）驱动的自动驾驶已经成为一种有希望的替代数据驱动模仿学习方法的选择。然而，为RL制定有效的奖励函数面临挑战，因为要在不同场景中定义和量化良好的驾驶行为的复杂性。最近，大型预训练模型作为零-shot奖励模型，为指定具有期望语言目标的任务引起了重要关注。然而，对于自动驾驶的期望语言目标，如“安全驾驶”，对于预训练模型来说是模糊且难以理解的。另一方面，不期望的语言目标，比如“碰撞”，更加具体且可跟踪。在这项工作中，我们引入了LORD，这是一种新颖的基于大模型的相反奖励设计，通过不期望的语言目标来实现对大型预训练模型的有效使用，作为零-shot奖励模型。通过大量实验，我们提出的框架显示",
    "tldr": "LORD通过不期望的语言目标，提出了一种基于大模型的相反奖励设计，以便有效利用大型预训练模型作为零-shot奖励模型。"
}