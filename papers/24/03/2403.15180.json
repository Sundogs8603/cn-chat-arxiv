{
    "title": "Self-Improvement for Neural Combinatorial Optimization: Sample without Replacement, but Improvement",
    "abstract": "arXiv:2403.15180v1 Announce Type: new  Abstract: Current methods for end-to-end constructive neural combinatorial optimization usually train a policy using behavior cloning from expert solutions or policy gradient methods from reinforcement learning. While behavior cloning is straightforward, it requires expensive expert solutions, and policy gradient methods are often computationally demanding and complex to fine-tune. In this work, we bridge the two and simplify the training process by sampling multiple solutions for random instances using the current model in each epoch and then selecting the best solution as an expert trajectory for supervised imitation learning. To achieve progressively improving solutions with minimal sampling, we introduce a method that combines round-wise Stochastic Beam Search with an update strategy derived from a provable policy improvement. This strategy refines the policy between rounds by utilizing the advantage of the sampled sequences with almost no com",
    "link": "https://arxiv.org/abs/2403.15180",
    "context": "Title: Self-Improvement for Neural Combinatorial Optimization: Sample without Replacement, but Improvement\nAbstract: arXiv:2403.15180v1 Announce Type: new  Abstract: Current methods for end-to-end constructive neural combinatorial optimization usually train a policy using behavior cloning from expert solutions or policy gradient methods from reinforcement learning. While behavior cloning is straightforward, it requires expensive expert solutions, and policy gradient methods are often computationally demanding and complex to fine-tune. In this work, we bridge the two and simplify the training process by sampling multiple solutions for random instances using the current model in each epoch and then selecting the best solution as an expert trajectory for supervised imitation learning. To achieve progressively improving solutions with minimal sampling, we introduce a method that combines round-wise Stochastic Beam Search with an update strategy derived from a provable policy improvement. This strategy refines the policy between rounds by utilizing the advantage of the sampled sequences with almost no com",
    "path": "papers/24/03/2403.15180.json",
    "total_tokens": 917,
    "translated_title": "自我改进用于神经组合优化问题：无需替换进行采样，但改进",
    "translated_abstract": "目前，对于端到端的构造性神经组合优化方法通常是使用行为克隆来训练策略，从专家解决方案中或使用策略梯度从强化学习中进行训练。虽然行为克隆方法很直接，但需要昂贵的专家解决方案，而策略梯度方法往往计算要求很高，难以进行精细调整。在这项工作中，我们桥接了这两种方法，并通过在每个纪元中使用当前模型对随机实例进行多个解决方案的采样，然后选择最佳解作为专家轨迹进行监督模仿学习，从而简化了训练过程。为了在最小采样次数中实现逐步改进的解决方案，我们引入了一种将循环式随机束搜索与一种推导自可证策略改进的更新策略相结合的方法。该策略通过利用几乎没有通信开销的样本序列的优势，在轮之间调整策略，以精细化策略。",
    "tldr": "通过结合循环式随机束搜索和来自可证策略改进的更新策略，本研究在神经组合优化中引入了一种新的训练方法，从而在最小采样次数中实现逐步改进的解决方案。",
    "en_tdlr": "This study introduces a new training method for neural combinatorial optimization by combining round-wise Stochastic Beam Search with an update strategy derived from a provable policy improvement, in order to achieve progressively improving solutions with minimal sampling."
}