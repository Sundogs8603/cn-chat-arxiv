{
    "title": "Leveraging Weakly Annotated Data for Hate Speech Detection in Code-Mixed Hinglish: A Feasibility-Driven Transfer Learning Approach with Large Language Models",
    "abstract": "arXiv:2403.02121v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) has advanced the benchmark in various Natural Language Processing (NLP) tasks. However, large amounts of labelled training data are required to train LLMs. Furthermore, data annotation and training are computationally expensive and time-consuming. Zero and few-shot learning have recently emerged as viable options for labelling data using large pre-trained models. Hate speech detection in mix-code low-resource languages is an active problem area where the use of LLMs has proven beneficial. In this study, we have compiled a dataset of 100 YouTube comments, and weakly labelled them for coarse and fine-grained misogyny classification in mix-code Hinglish. Weak annotation was applied due to the labor-intensive annotation process. Zero-shot learning, one-shot learning, and few-shot learning and prompting approaches have then been applied to assign labels to the comments and compare them to human-ass",
    "link": "https://arxiv.org/abs/2403.02121",
    "context": "Title: Leveraging Weakly Annotated Data for Hate Speech Detection in Code-Mixed Hinglish: A Feasibility-Driven Transfer Learning Approach with Large Language Models\nAbstract: arXiv:2403.02121v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) has advanced the benchmark in various Natural Language Processing (NLP) tasks. However, large amounts of labelled training data are required to train LLMs. Furthermore, data annotation and training are computationally expensive and time-consuming. Zero and few-shot learning have recently emerged as viable options for labelling data using large pre-trained models. Hate speech detection in mix-code low-resource languages is an active problem area where the use of LLMs has proven beneficial. In this study, we have compiled a dataset of 100 YouTube comments, and weakly labelled them for coarse and fine-grained misogyny classification in mix-code Hinglish. Weak annotation was applied due to the labor-intensive annotation process. Zero-shot learning, one-shot learning, and few-shot learning and prompting approaches have then been applied to assign labels to the comments and compare them to human-ass",
    "path": "papers/24/03/2403.02121.json",
    "total_tokens": 933,
    "translated_title": "在混合代码的印地语中利用弱标注数据进行仇恨言论检测：一种基于可行性驱动的大型语言模型迁移学习方法",
    "translated_abstract": "大型语言模型的出现推动了各种自然语言处理任务的基准。然而，训练大型语言模型需要大量标记的训练数据。此外，数据标注和训练都是计算昂贵且耗时的。最近，零次和少次学习成为使用大型预训练模型标记数据的可行选择。在混合代码低资源语言中的仇恨言论检测是一个活跃的问题领域，LLM的使用在这方面已被证明是有益的。在这项研究中，我们收集了100条YouTube评论的数据集，并对混合代码的印地语中的粗粒度和细粒度的性别歧视进行了弱标注。由于需要耗费大量人力进行注释，因此采用了弱标注。然后，应用了零次学习、一次学习和少次学习以及提示方法来为评论分配标签，并将其与人工标记进行比较。",
    "tldr": "本研究利用弱标注数据和大型语言模型，针对混合代码的印地语进行仇恨言论检测，探索了零次学习、一次学习和少次学习方法，解决了标记数据的问题。",
    "en_tdlr": "This study leverages weakly annotated data and large language models for hate speech detection in code-mixed Hinglish, exploring zero-shot learning, one-shot learning, and few-shot learning approaches to address data labeling challenges."
}