{
    "title": "Sequence-to-Sequence Language Models for Character and Emotion Detection in Dream Narratives",
    "abstract": "arXiv:2403.15486v1 Announce Type: cross  Abstract: The study of dreams has been central to understanding human (un)consciousness, cognition, and culture for centuries. Analyzing dreams quantitatively depends on labor-intensive, manual annotation of dream narratives. We automate this process through a natural language sequence-to-sequence generation framework. This paper presents the first study on character and emotion detection in the English portion of the open DreamBank corpus of dream narratives. Our results show that language models can effectively address this complex task. To get insight into prediction performance, we evaluate the impact of model size, prediction order of characters, and the consideration of proper names and character traits. We compare our approach with a large language model using in-context learning. Our supervised models perform better while having 28 times fewer parameters. Our model and its generated annotations are made publicly available.",
    "link": "https://arxiv.org/abs/2403.15486",
    "context": "Title: Sequence-to-Sequence Language Models for Character and Emotion Detection in Dream Narratives\nAbstract: arXiv:2403.15486v1 Announce Type: cross  Abstract: The study of dreams has been central to understanding human (un)consciousness, cognition, and culture for centuries. Analyzing dreams quantitatively depends on labor-intensive, manual annotation of dream narratives. We automate this process through a natural language sequence-to-sequence generation framework. This paper presents the first study on character and emotion detection in the English portion of the open DreamBank corpus of dream narratives. Our results show that language models can effectively address this complex task. To get insight into prediction performance, we evaluate the impact of model size, prediction order of characters, and the consideration of proper names and character traits. We compare our approach with a large language model using in-context learning. Our supervised models perform better while having 28 times fewer parameters. Our model and its generated annotations are made publicly available.",
    "path": "papers/24/03/2403.15486.json",
    "total_tokens": 866,
    "translated_title": "序列到序列语言模型用于梦境叙事中的角色和情感检测",
    "translated_abstract": "梦境研究对于理解人类的(非)意识、认知和文化数个世纪来一直至关重要。定量分析梦境依赖于对梦境叙述的劳动密集型手动注释。我们通过一种自然语言序列到序列生成框架自动化这一过程。本文首次在梦境叙事的开放DreamBank语料库英文部分中进行了角色和情感检测研究。我们的结果表明语言模型可以有效地解决这一复杂任务。为了了解预测性能，我们评估了模型大小、角色的预测顺序以及对专有名称和角色特征的考虑的影响。我们将我们的方法与使用上下文学习的大型语言模型进行了比较。我们的监督模型表现更好，同时参数数量减少了28倍。我们的模型及其生成的注释已公开可用。",
    "tldr": "本研究提出一种序列到序列语言模型框架，首次在梦境叙事中进行角色和情感检测研究，展示语言模型可以有效应对该复杂任务，监督模型表现更佳且参数更少。",
    "en_tdlr": "This study introduces a sequence-to-sequence language model framework for character and emotion detection in dream narratives, showing the effectiveness of language models in addressing this complex task, with supervised models outperforming with fewer parameters."
}