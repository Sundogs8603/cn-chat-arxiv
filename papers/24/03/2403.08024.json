{
    "title": "xMLP: Revolutionizing Private Inference with Exclusive Square Activation",
    "abstract": "arXiv:2403.08024v1 Announce Type: new  Abstract: Private Inference (PI) enables deep neural networks (DNNs) to work on private data without leaking sensitive information by exploiting cryptographic primitives such as multi-party computation (MPC) and homomorphic encryption (HE). However, the use of non-linear activations such as ReLU in DNNs can lead to impractically high PI latency in existing PI systems, as ReLU requires the use of costly MPC computations, such as Garbled Circuits. Since square activations can be processed by Beaver's triples hundreds of times faster compared to ReLU, they are more friendly to PI tasks, but using them leads to a notable drop in model accuracy. This paper starts by exploring the reason for such an accuracy drop after using square activations, and concludes that this is due to an \"information compounding\" effect. Leveraging this insight, we propose xMLP, a novel DNN architecture that uses square activations exclusively while maintaining parity in both ",
    "link": "https://arxiv.org/abs/2403.08024",
    "context": "Title: xMLP: Revolutionizing Private Inference with Exclusive Square Activation\nAbstract: arXiv:2403.08024v1 Announce Type: new  Abstract: Private Inference (PI) enables deep neural networks (DNNs) to work on private data without leaking sensitive information by exploiting cryptographic primitives such as multi-party computation (MPC) and homomorphic encryption (HE). However, the use of non-linear activations such as ReLU in DNNs can lead to impractically high PI latency in existing PI systems, as ReLU requires the use of costly MPC computations, such as Garbled Circuits. Since square activations can be processed by Beaver's triples hundreds of times faster compared to ReLU, they are more friendly to PI tasks, but using them leads to a notable drop in model accuracy. This paper starts by exploring the reason for such an accuracy drop after using square activations, and concludes that this is due to an \"information compounding\" effect. Leveraging this insight, we propose xMLP, a novel DNN architecture that uses square activations exclusively while maintaining parity in both ",
    "path": "papers/24/03/2403.08024.json",
    "total_tokens": 866,
    "translated_title": "xMLP：利用独占方激活革命化私密推断",
    "translated_abstract": "私密推断（PI）通过利用密码原语，如多方计算（MPC）和同态加密（HE），使深度神经网络（DNNs）能够在私人数据上运行，而不会泄露敏感信息。然而，现有PI系统中使用诸如ReLU之类的非线性激活可能导致不切实际的高PI延迟，因为ReLU需要使用昂贵的MPC计算，如Garbled Circuits。由于与ReLU相比，方激活可通过比Beaver的三元组快数百倍的速度进行处理，因此更适合PI任务，但是使用它们会导致模型准确性显著下降。本文首先探讨了在使用方激活后出现准确性下降的原因，并得出结论，即这是由于“信息复合”效应。利用这一见解，我们提出了xMLP，这是一种使用独占方激活的新颖DNN架构，同时在两者之间保持平衡",
    "tldr": "本文提出了xMLP，这是一种独特的DNN架构，使用独占的方激活，在维持准确性的同时减少私密推断系统中的延迟",
    "en_tdlr": "This paper introduces xMLP, a novel DNN architecture that uses exclusive square activations to reduce latency in private inference systems while maintaining accuracy."
}