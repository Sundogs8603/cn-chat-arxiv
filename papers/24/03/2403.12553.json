{
    "title": "Pretraining Codomain Attention Neural Operators for Solving Multiphysics PDEs",
    "abstract": "arXiv:2403.12553v1 Announce Type: new  Abstract: Existing neural operator architectures face challenges when solving multiphysics problems with coupled partial differential equations (PDEs), due to complex geometries, interactions between physical variables, and the lack of large amounts of high-resolution training data. To address these issues, we propose Codomain Attention Neural Operator (CoDA-NO), which tokenizes functions along the codomain or channel space, enabling self-supervised learning or pretraining of multiple PDE systems. Specifically, we extend positional encoding, self-attention, and normalization layers to the function space. CoDA-NO can learn representations of different PDE systems with a single model. We evaluate CoDA-NO's potential as a backbone for learning multiphysics PDEs over multiple systems by considering few-shot learning settings. On complex downstream tasks with limited data, such as fluid flow simulations and fluid-structure interactions, we found CoDA-N",
    "link": "https://arxiv.org/abs/2403.12553",
    "context": "Title: Pretraining Codomain Attention Neural Operators for Solving Multiphysics PDEs\nAbstract: arXiv:2403.12553v1 Announce Type: new  Abstract: Existing neural operator architectures face challenges when solving multiphysics problems with coupled partial differential equations (PDEs), due to complex geometries, interactions between physical variables, and the lack of large amounts of high-resolution training data. To address these issues, we propose Codomain Attention Neural Operator (CoDA-NO), which tokenizes functions along the codomain or channel space, enabling self-supervised learning or pretraining of multiple PDE systems. Specifically, we extend positional encoding, self-attention, and normalization layers to the function space. CoDA-NO can learn representations of different PDE systems with a single model. We evaluate CoDA-NO's potential as a backbone for learning multiphysics PDEs over multiple systems by considering few-shot learning settings. On complex downstream tasks with limited data, such as fluid flow simulations and fluid-structure interactions, we found CoDA-N",
    "path": "papers/24/03/2403.12553.json",
    "total_tokens": 918,
    "translated_title": "为求解多物理学偏微分方程预训练象域关注神经算子",
    "translated_abstract": "现有的神经算子架构在解决涉及耦合偏微分方程的多物理问题时面临挑战，这是由于复杂的几何形状、物理变量之间的相互作用以及缺乏大量高分辨率训练数据所致。为了解决这些问题，我们提出了象域关注神经算子（CoDA-NO），该算子将函数在象域或通道空间上进行标记，实现了多个PDE系统的自监督学习或预训练。具体来说，我们将位置编码、自注意力和归一化层扩展到函数空间。CoDA-NO可以使用单个模型学习不同PDE系统的表示。通过考虑少样本学习设置，我们评估了CoDA-NO作为学习多物理PDE的骨干的潜力。在具有有限数据的复杂下游任务（如流体流动模拟和流固相互作用）中，我们发现CoDA-N",
    "tldr": "该研究提出了一种名为CoDA-NO的神经算子，通过在象域或通道空间对函数进行标记，实现了多个PDE系统的自监督学习或预训练，为解决涉及耦合偏微分方程的多物理问题提供了新思路。",
    "en_tdlr": "This study introduces a neural operator called CoDA-NO, which tokenizes functions along the codomain or channel space, enabling self-supervised learning or pretraining of multiple PDE systems, offering a new approach to solving multiphysics problems involving coupled partial differential equations."
}