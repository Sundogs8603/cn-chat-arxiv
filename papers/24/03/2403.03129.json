{
    "title": "CoGenesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following",
    "abstract": "arXiv:2403.03129v1 Announce Type: new  Abstract: With the advancement of language models (LMs), their exposure to private data is increasingly inevitable, and their deployment (especially for smaller ones) on personal devices, such as PCs and smartphones, has become a prevailing trend. In contexts laden with user information, enabling models to both safeguard user privacy and execute commands efficiently emerges as an essential research imperative. In this paper, we propose CoGenesis, a collaborative generation framework integrating large (hosted on cloud infrastructure) and small models (deployed on local devices) to address privacy concerns logically. Initially, we design a pipeline to create personalized writing instruction datasets enriched with extensive context details as the testbed of this research issue. Subsequently, we introduce two variants of CoGenesis based on sketch and logits respectively. Our experimental findings, based on our synthesized dataset and two additional op",
    "link": "https://arxiv.org/abs/2403.03129",
    "context": "Title: CoGenesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following\nAbstract: arXiv:2403.03129v1 Announce Type: new  Abstract: With the advancement of language models (LMs), their exposure to private data is increasingly inevitable, and their deployment (especially for smaller ones) on personal devices, such as PCs and smartphones, has become a prevailing trend. In contexts laden with user information, enabling models to both safeguard user privacy and execute commands efficiently emerges as an essential research imperative. In this paper, we propose CoGenesis, a collaborative generation framework integrating large (hosted on cloud infrastructure) and small models (deployed on local devices) to address privacy concerns logically. Initially, we design a pipeline to create personalized writing instruction datasets enriched with extensive context details as the testbed of this research issue. Subsequently, we introduce two variants of CoGenesis based on sketch and logits respectively. Our experimental findings, based on our synthesized dataset and two additional op",
    "path": "papers/24/03/2403.03129.json",
    "total_tokens": 839,
    "translated_title": "CoGenesis：一个协作大型和小型语言模型的框架，用于安全的上下文感知指令跟随",
    "translated_abstract": "随着语言模型（LMs）的发展，它们接触私人数据的可能性越来越不可避免，它们的部署（尤其是较小的模型）在个人设备上，如PC和智能手机上，已成为一种盛行趋势。在充满用户信息的环境中，使模型既能保护用户隐私又能高效执行命令成为一项重要的研究课题。本文提出了CoGenesis，一个协作生成框架，集成了大型模型（托管在云基础设施上）和小型模型（部署在本地设备上），以逻辑方式解决隐私问题。最初，我们设计了一个管道来创建个性化的写作指导数据集，其中包含了丰富的上下文细节，作为这一研究问题的测试基础。随后，我们介绍了基于草图和对数的两个CoGenesis变体。我们的实验结果基于我们合成的数据集和另外两个op",
    "tldr": "提出了一个协作生成框架CoGenesis，整合大型和小型模型，以逻辑方式解决隐私问题。",
    "en_tdlr": "Proposed a collaborative generation framework CoGenesis, integrating large and small models to address privacy issues logically."
}