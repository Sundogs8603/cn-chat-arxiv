{
    "title": "Distributional Dataset Distillation with Subtask Decomposition",
    "abstract": "arXiv:2403.00999v1 Announce Type: new  Abstract: What does a neural network learn when training from a task-specific dataset? Synthesizing this knowledge is the central idea behind Dataset Distillation, which recent work has shown can be used to compress large datasets into a small set of input-label pairs ($\\textit{prototypes}$) that capture essential aspects of the original dataset. In this paper, we make the key observation that existing methods distilling into explicit prototypes are very often suboptimal, incurring in unexpected storage cost from distilled labels. In response, we propose $\\textit{Distributional Dataset Distillation}$ (D3), which encodes the data using minimal sufficient per-class statistics and paired with a decoder, we distill dataset into a compact distributional representation that is more memory-efficient compared to prototype-based methods. To scale up the process of learning these representations, we propose $\\textit{Federated distillation}$, which decompose",
    "link": "https://arxiv.org/abs/2403.00999",
    "context": "Title: Distributional Dataset Distillation with Subtask Decomposition\nAbstract: arXiv:2403.00999v1 Announce Type: new  Abstract: What does a neural network learn when training from a task-specific dataset? Synthesizing this knowledge is the central idea behind Dataset Distillation, which recent work has shown can be used to compress large datasets into a small set of input-label pairs ($\\textit{prototypes}$) that capture essential aspects of the original dataset. In this paper, we make the key observation that existing methods distilling into explicit prototypes are very often suboptimal, incurring in unexpected storage cost from distilled labels. In response, we propose $\\textit{Distributional Dataset Distillation}$ (D3), which encodes the data using minimal sufficient per-class statistics and paired with a decoder, we distill dataset into a compact distributional representation that is more memory-efficient compared to prototype-based methods. To scale up the process of learning these representations, we propose $\\textit{Federated distillation}$, which decompose",
    "path": "papers/24/03/2403.00999.json",
    "total_tokens": 866,
    "translated_title": "基于子任务分解的分布式数据集提炼",
    "translated_abstract": "神经网络在从特定任务数据集进行训练时学到了什么？综合这种知识是数据集提炼背后的中心思想，最近的研究表明可以将大型数据集压缩成一小组捕捉原始数据集关键方面的输入标签对 ($\\textit{prototypes}$)。本文的关键观察是，现有的提取明确原型的方法往往是次优的，导致通过提炼标签而产生意外的存储成本。为此，我们提出了$\\textit{分布式数据集提炼}$ (D3)，它使用最小的每类统计信息对数据进行编码，并与解码器配对，将数据集提炼成一种更节省内存的分布式表示形式，与基于原型的方法相比更高效。为了扩大学习这些表示形式的过程，我们提出了$\\textit{联合提炼}$，它分解",
    "tldr": "提出了基于子任务分解的分布式数据集提炼方法，通过使用最小的每类统计信息进行数据编码，并结合解码器，将数据集提炼成更节省内存的分布式表示形式，相比于传统基于原型的方法更高效。",
    "en_tdlr": "Proposed a distributional dataset distillation method based on subtask decomposition, which encodes data using minimal per-class statistics, paired with a decoder to distill the dataset into a more memory-efficient distributional representation compared to traditional prototype-based methods."
}