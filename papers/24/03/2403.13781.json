{
    "title": "Sparse Implementation of Versatile Graph-Informed Layers",
    "abstract": "arXiv:2403.13781v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have emerged as effective tools for learning tasks on graph-structured data. Recently, Graph-Informed (GI) layers were introduced to address regression tasks on graph nodes, extending their applicability beyond classic GNNs. However, existing implementations of GI layers lack efficiency due to dense memory allocation. This paper presents a sparse implementation of GI layers, leveraging the sparsity of adjacency matrices to reduce memory usage significantly. Additionally, a versatile general form of GI layers is introduced, enabling their application to subsets of graph nodes. The proposed sparse implementation improves the concrete computational efficiency and scalability of the GI layers, permitting to build deeper Graph-Informed Neural Networks (GINNs) and facilitating their scalability to larger graphs.",
    "link": "https://arxiv.org/abs/2403.13781",
    "context": "Title: Sparse Implementation of Versatile Graph-Informed Layers\nAbstract: arXiv:2403.13781v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have emerged as effective tools for learning tasks on graph-structured data. Recently, Graph-Informed (GI) layers were introduced to address regression tasks on graph nodes, extending their applicability beyond classic GNNs. However, existing implementations of GI layers lack efficiency due to dense memory allocation. This paper presents a sparse implementation of GI layers, leveraging the sparsity of adjacency matrices to reduce memory usage significantly. Additionally, a versatile general form of GI layers is introduced, enabling their application to subsets of graph nodes. The proposed sparse implementation improves the concrete computational efficiency and scalability of the GI layers, permitting to build deeper Graph-Informed Neural Networks (GINNs) and facilitating their scalability to larger graphs.",
    "path": "papers/24/03/2403.13781.json",
    "total_tokens": 787,
    "translated_title": "稀疏实现多功能图信息层",
    "translated_abstract": "图神经网络(GNNs)已经成为在图结构数据上学习任务的有效工具。最近，引入了图信息(GI)层，以解决图节点上的回归任务，扩展了它们 beyond 经典 GNNs 的适用性。然而，由于密集内存分配，现有的 GI 层实现缺乏效率。本文提出了 GI 层的稀疏实现，利用邻接矩阵的稀疏性显著减少内存使用。此外，引入了 GI 层的通用形式，使其能够应用于图节点的子集。所提出的稀疏实现改进了 GI 层的具体计算效率和可扩展性，允许构建更深的图信息神经网络(GINNs)并促进其向更大图的可扩展性。",
    "tldr": "本文提出了稀疏实现的多功能图信息层，通过利用邻接矩阵的稀疏性显著减少内存使用，改进了图信息神经网络(GINNs)的计算效率和可扩展性。"
}