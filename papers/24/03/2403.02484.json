{
    "title": "Encodings for Prediction-based Neural Architecture Search",
    "abstract": "arXiv:2403.02484v1 Announce Type: cross  Abstract: Predictor-based methods have substantially enhanced Neural Architecture Search (NAS) optimization. The efficacy of these predictors is largely influenced by the method of encoding neural network architectures. While traditional encodings used an adjacency matrix describing the graph structure of a neural network, novel encodings embrace a variety of approaches from unsupervised pretraining of latent representations to vectors of zero-cost proxies. In this paper, we categorize and investigate neural encodings from three main types: structural, learned, and score-based. Furthermore, we extend these encodings and introduce \\textit{unified encodings}, that extend NAS predictors to multiple search spaces. Our analysis draws from experiments conducted on over 1.5 million neural network architectures on NAS spaces such as NASBench-101 (NB101), NB201, NB301, Network Design Spaces (NDS), and TransNASBench-101. Building on our study, we present ",
    "link": "https://arxiv.org/abs/2403.02484",
    "context": "Title: Encodings for Prediction-based Neural Architecture Search\nAbstract: arXiv:2403.02484v1 Announce Type: cross  Abstract: Predictor-based methods have substantially enhanced Neural Architecture Search (NAS) optimization. The efficacy of these predictors is largely influenced by the method of encoding neural network architectures. While traditional encodings used an adjacency matrix describing the graph structure of a neural network, novel encodings embrace a variety of approaches from unsupervised pretraining of latent representations to vectors of zero-cost proxies. In this paper, we categorize and investigate neural encodings from three main types: structural, learned, and score-based. Furthermore, we extend these encodings and introduce \\textit{unified encodings}, that extend NAS predictors to multiple search spaces. Our analysis draws from experiments conducted on over 1.5 million neural network architectures on NAS spaces such as NASBench-101 (NB101), NB201, NB301, Network Design Spaces (NDS), and TransNASBench-101. Building on our study, we present ",
    "path": "papers/24/03/2403.02484.json",
    "total_tokens": 792,
    "translated_title": "基于预测的神经架构搜索的编码方式",
    "translated_abstract": "预测器方法大大增强了神经架构搜索（NAS）优化的效果。这些预测器的有效性在很大程度上取决于神经网络架构的编码方法。本文对三种主要类型的神经编码进行了分类和研究：结构型、学习型和基于分数的。此外，我们扩展了这些编码，并引入了“统一编码”，将NAS预测器扩展到多个搜索空间。我们的分析来自于在NASBench-101（NB101）、NB201、NB301、网络设计空间（NDS）和TransNASBench-101等NAS空间上进行的超过150万个神经网络架构的实验。根据我们的研究，我们提出了",
    "tldr": "预测器方法在神经架构搜索方面起到了显著作用，本文对不同类型的神经编码进行了分类和研究，并引入了统一编码，扩展了NAS预测器到多个搜索空间。",
    "en_tdlr": "Predictor-based methods have substantially enhanced Neural Architecture Search (NAS) optimization, and this paper categorizes and investigates neural encodings from different types while introducing unified encodings to extend NAS predictors to multiple search spaces."
}