{
    "title": "A&B BNN: Add&Bit-Operation-Only Hardware-Friendly Binary Neural Network",
    "abstract": "arXiv:2403.03739v1 Announce Type: cross  Abstract: Binary neural networks utilize 1-bit quantized weights and activations to reduce both the model's storage demands and computational burden. However, advanced binary architectures still incorporate millions of inefficient and nonhardware-friendly full-precision multiplication operations. A&B BNN is proposed to directly remove part of the multiplication operations in a traditional BNN and replace the rest with an equal number of bit operations, introducing the mask layer and the quantized RPReLU structure based on the normalizer-free network architecture. The mask layer can be removed during inference by leveraging the intrinsic characteristics of BNN with straightforward mathematical transformations to avoid the associated multiplication operations. The quantized RPReLU structure enables more efficient bit operations by constraining its slope to be integer powers of 2. Experimental results achieved 92.30%, 69.35%, and 66.89% on the CIFA",
    "link": "https://arxiv.org/abs/2403.03739",
    "context": "Title: A&B BNN: Add&Bit-Operation-Only Hardware-Friendly Binary Neural Network\nAbstract: arXiv:2403.03739v1 Announce Type: cross  Abstract: Binary neural networks utilize 1-bit quantized weights and activations to reduce both the model's storage demands and computational burden. However, advanced binary architectures still incorporate millions of inefficient and nonhardware-friendly full-precision multiplication operations. A&B BNN is proposed to directly remove part of the multiplication operations in a traditional BNN and replace the rest with an equal number of bit operations, introducing the mask layer and the quantized RPReLU structure based on the normalizer-free network architecture. The mask layer can be removed during inference by leveraging the intrinsic characteristics of BNN with straightforward mathematical transformations to avoid the associated multiplication operations. The quantized RPReLU structure enables more efficient bit operations by constraining its slope to be integer powers of 2. Experimental results achieved 92.30%, 69.35%, and 66.89% on the CIFA",
    "path": "papers/24/03/2403.03739.json",
    "total_tokens": 850,
    "translated_title": "A&B BNN: A&B BNN：仅使用加和位操作的硬件友好的二值神经网络",
    "translated_abstract": "二值神经网络利用1位量化的权重和激活来减少模型的存储需求和计算负担。然而，先进的二值架构仍然包含数百万个低效且对硬件不友好的全精度乘法操作。A&B BNN 提出了直接移除传统 BNN 中的部分乘法操作，并用相同数量的位操作替换剩余部分，引入了基于无归一化网络架构的掩码层和量化 RPReLU 结构。掩码层可以通过利用 BNN 的内在特征以及简单的数学变换在推断期间将其移除，以避免相关的乘法操作。量化 RPReLU 结构通过将其斜率限制为2的整数幂，实现更高效的位操作。实验结果在CIFA数据集上达到了92.30%、69.35%和66.89%的准确率。",
    "tldr": "A&B BNN 提出了一种只使用加和位操作的硬件友好二值神经网络，通过引入掩码层和量化 RPReLU 结构，能够更高效地进行计算，并在CIFA数据集上取得了良好的实验结果。",
    "en_tdlr": "A&B BNN proposes a hardware-friendly binary neural network that only uses addition and bit operations, introducing a mask layer and quantized RPReLU structure to achieve more efficient computation, and achieving promising experimental results on the CIFA dataset."
}