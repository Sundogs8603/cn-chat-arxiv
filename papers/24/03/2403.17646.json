{
    "title": "Uncertainty-aware Distributional Offline Reinforcement Learning",
    "abstract": "arXiv:2403.17646v1 Announce Type: new  Abstract: Offline reinforcement learning (RL) presents distinct challenges as it relies solely on observational data. A central concern in this context is ensuring the safety of the learned policy by quantifying uncertainties associated with various actions and environmental stochasticity. Traditional approaches primarily emphasize mitigating epistemic uncertainty by learning risk-averse policies, often overlooking environmental stochasticity. In this study, we propose an uncertainty-aware distributional offline RL method to simultaneously address both epistemic uncertainty and environmental stochasticity. We propose a model-free offline RL algorithm capable of learning risk-averse policies and characterizing the entire distribution of discounted cumulative rewards, as opposed to merely maximizing the expected value of accumulated discounted returns. Our method is rigorously evaluated through comprehensive experiments in both risk-sensitive and ri",
    "link": "https://arxiv.org/abs/2403.17646",
    "context": "Title: Uncertainty-aware Distributional Offline Reinforcement Learning\nAbstract: arXiv:2403.17646v1 Announce Type: new  Abstract: Offline reinforcement learning (RL) presents distinct challenges as it relies solely on observational data. A central concern in this context is ensuring the safety of the learned policy by quantifying uncertainties associated with various actions and environmental stochasticity. Traditional approaches primarily emphasize mitigating epistemic uncertainty by learning risk-averse policies, often overlooking environmental stochasticity. In this study, we propose an uncertainty-aware distributional offline RL method to simultaneously address both epistemic uncertainty and environmental stochasticity. We propose a model-free offline RL algorithm capable of learning risk-averse policies and characterizing the entire distribution of discounted cumulative rewards, as opposed to merely maximizing the expected value of accumulated discounted returns. Our method is rigorously evaluated through comprehensive experiments in both risk-sensitive and ri",
    "path": "papers/24/03/2403.17646.json",
    "total_tokens": 835,
    "translated_title": "不确定性感知的分布式离线强化学习",
    "translated_abstract": "离线强化学习面临独特挑战，因其仅依赖于观测数据。在这一背景下中心关注点是通过量化与各种行动和环境随机性相关的不确定性，确保所学策略的安全性。传统方法主要强调通过学习风险规避策略来缓解认知不确定性，往往忽视环境随机性。在本研究中，我们提出了一种不确定性感知的分布式离线强化学习方法，以同时处理认知不确定性和环境随机性。我们提出了一种能够学习风险规避策略并表征折现累积奖励的整个分布的无模型离线强化学习算法，而不仅仅是最大化累积折现回报的期望值。我们的方法通过在风险敏感和风险规避设置下的全面实验得到严格评估。",
    "tldr": "提出了一种不确定性感知的分布式离线强化学习方法，同时解决认知不确定性和环境随机性，在风险敏感和规避设置下进行了全面实验评估"
}