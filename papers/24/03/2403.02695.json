{
    "title": "Controllable Prompt Tuning For Balancing Group Distributional Robustness",
    "abstract": "arXiv:2403.02695v1 Announce Type: new  Abstract: Models trained on data composed of different groups or domains can suffer from severe performance degradation under distribution shifts. While recent methods have largely focused on optimizing the worst-group objective, this often comes at the expense of good performance on other groups. To address this problem, we introduce an optimization scheme to achieve good performance across groups and find a good solution for all without severely sacrificing performance on any of them. However, directly applying such optimization involves updating the parameters of the entire network, making it both computationally expensive and challenging. Thus, we introduce Controllable Prompt Tuning (CPT), which couples our approach with prompt-tuning techniques. On spurious correlation benchmarks, our procedures achieve state-of-the-art results across both transformer and non-transformer architectures, as well as unimodal and multimodal data, while requiring",
    "link": "https://arxiv.org/abs/2403.02695",
    "context": "Title: Controllable Prompt Tuning For Balancing Group Distributional Robustness\nAbstract: arXiv:2403.02695v1 Announce Type: new  Abstract: Models trained on data composed of different groups or domains can suffer from severe performance degradation under distribution shifts. While recent methods have largely focused on optimizing the worst-group objective, this often comes at the expense of good performance on other groups. To address this problem, we introduce an optimization scheme to achieve good performance across groups and find a good solution for all without severely sacrificing performance on any of them. However, directly applying such optimization involves updating the parameters of the entire network, making it both computationally expensive and challenging. Thus, we introduce Controllable Prompt Tuning (CPT), which couples our approach with prompt-tuning techniques. On spurious correlation benchmarks, our procedures achieve state-of-the-art results across both transformer and non-transformer architectures, as well as unimodal and multimodal data, while requiring",
    "path": "papers/24/03/2403.02695.json",
    "total_tokens": 839,
    "translated_title": "可控提示调整用于平衡组分布鲁棒性",
    "translated_abstract": "在由不同组或领域组成的数据上训练的模型可能会在分布偏移下出现严重的性能下降。尽管最近的方法主要集中在优化最差组的目标上，但这往往是以牺牲其他组上的良好性能为代价的。为了解决这个问题，我们引入了一种优化方案，以实现组内良好性能，并找到一个良好的解决方案，而不会严重牺牲任何一个组的性能。然而，直接应用这种优化会涉及更新整个网络的参数，这既耗时又具有挑战性。因此，我们引入了可控提示调整（CPT），将我们的方法与提示调整技术相结合。在虚假相关基准测试中，我们的程序在变压器和非变压器架构以及单模态和多模态数据上均实现了最先进的结果，同时也需要",
    "tldr": "引入了可控提示调整（CPT）技术，通过优化方案在不同组之间实现良好性能，避免牺牲任何一个组的性能，在虚假相关基准测试中取得了最先进的结果。",
    "en_tdlr": "Introducing Controllable Prompt Tuning (CPT) technique to achieve good performance across different groups without sacrificing any group's performance, resulting in state-of-the-art results in spurious correlation benchmarks."
}