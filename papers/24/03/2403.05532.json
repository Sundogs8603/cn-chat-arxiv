{
    "title": "Tune without Validation: Searching for Learning Rate and Weight Decay on Training Sets",
    "abstract": "arXiv:2403.05532v1 Announce Type: new  Abstract: We introduce Tune without Validation (Twin), a pipeline for tuning learning rate and weight decay without validation sets. We leverage a recent theoretical framework concerning learning phases in hypothesis space to devise a heuristic that predicts what hyper-parameter (HP) combinations yield better generalization. Twin performs a grid search of trials according to an early-/non-early-stopping scheduler and then segments the region that provides the best results in terms of training loss. Among these trials, the weight norm strongly correlates with predicting generalization. To assess the effectiveness of Twin, we run extensive experiments on 20 image classification datasets and train several families of deep networks, including convolutional, transformer, and feed-forward models. We demonstrate proper HP selection when training from scratch and fine-tuning, emphasizing small-sample scenarios.",
    "link": "https://arxiv.org/abs/2403.05532",
    "context": "Title: Tune without Validation: Searching for Learning Rate and Weight Decay on Training Sets\nAbstract: arXiv:2403.05532v1 Announce Type: new  Abstract: We introduce Tune without Validation (Twin), a pipeline for tuning learning rate and weight decay without validation sets. We leverage a recent theoretical framework concerning learning phases in hypothesis space to devise a heuristic that predicts what hyper-parameter (HP) combinations yield better generalization. Twin performs a grid search of trials according to an early-/non-early-stopping scheduler and then segments the region that provides the best results in terms of training loss. Among these trials, the weight norm strongly correlates with predicting generalization. To assess the effectiveness of Twin, we run extensive experiments on 20 image classification datasets and train several families of deep networks, including convolutional, transformer, and feed-forward models. We demonstrate proper HP selection when training from scratch and fine-tuning, emphasizing small-sample scenarios.",
    "path": "papers/24/03/2403.05532.json",
    "total_tokens": 844,
    "translated_title": "在训练集上搜索学习率和权重衰减：无需验证的调参方法",
    "translated_abstract": "我们介绍了一种叫做Tune without Validation (Twin)的方法，用于在没有验证集的情况下调整学习率和权重衰减。我们利用了关于假设空间中学习阶段的最新理论框架，设计了一种启发式方法，可以预测哪些超参数组合会产生更好的泛化性能。Twin根据一个早停/非早停的调度程序对试验进行网格搜索，然后分割出在训练损失方面提供最佳结果的区域。在这些试验中，权重范数与泛化性能的预测强相关。为了评估Twin的有效性，我们在20个图像分类数据集上进行了大量实验，并训练了几个系列的深度网络，包括卷积、Transformer和前馈模型。我们展示了在从头开始训练和微调时正确的超参数选择，重点强调了小样本场景。",
    "tldr": "提出了一种名为Tune without Validation (Twin)的方法，在没有验证集的情况下通过学习率和权重衰减的调整来预测泛化性能，强调了权重范数与泛化性能预测的强相关性。",
    "en_tdlr": "Introduced a method called Tune without Validation (Twin) for predicting generalization performance through tuning learning rate and weight decay without validation sets, emphasizing a strong correlation between weight norm and generalization prediction."
}