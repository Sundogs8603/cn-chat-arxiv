{
    "title": "Prioritized League Reinforcement Learning for Large-Scale Heterogeneous Multiagent Systems",
    "abstract": "arXiv:2403.18057v1 Announce Type: new  Abstract: Large-scale heterogeneous multiagent systems feature various realistic factors in the real world, such as agents with diverse abilities and overall system cost. In comparison to homogeneous systems, heterogeneous systems offer significant practical advantages. Nonetheless, they also present challenges for multiagent reinforcement learning, including addressing the non-stationary problem and managing an imbalanced number of agents with different types. We propose a Prioritized Heterogeneous League Reinforcement Learning (PHLRL) method to address large-scale heterogeneous cooperation problems. PHLRL maintains a record of various policies that agents have explored during their training and establishes a heterogeneous league consisting of diverse policies to aid in future policy optimization. Furthermore, we design a prioritized policy gradient approach to compensate for the gap caused by differences in the number of different types of agent",
    "link": "https://arxiv.org/abs/2403.18057",
    "context": "Title: Prioritized League Reinforcement Learning for Large-Scale Heterogeneous Multiagent Systems\nAbstract: arXiv:2403.18057v1 Announce Type: new  Abstract: Large-scale heterogeneous multiagent systems feature various realistic factors in the real world, such as agents with diverse abilities and overall system cost. In comparison to homogeneous systems, heterogeneous systems offer significant practical advantages. Nonetheless, they also present challenges for multiagent reinforcement learning, including addressing the non-stationary problem and managing an imbalanced number of agents with different types. We propose a Prioritized Heterogeneous League Reinforcement Learning (PHLRL) method to address large-scale heterogeneous cooperation problems. PHLRL maintains a record of various policies that agents have explored during their training and establishes a heterogeneous league consisting of diverse policies to aid in future policy optimization. Furthermore, we design a prioritized policy gradient approach to compensate for the gap caused by differences in the number of different types of agent",
    "path": "papers/24/03/2403.18057.json",
    "total_tokens": 850,
    "translated_title": "面向大规模异构多智能体系统的优先级联强化学习",
    "translated_abstract": "大规模异构多智能体系统在现实世界中具有各种真实因素，如具有不同能力和整体系统成本的智能体。与同质系统相比，异构系统具有显著的实际优势。然而，它们也为多智能体强化学习带来挑战，包括解决非静态问题和管理具有不同类型的不平衡数量智能体。我们提出了一种名为优先级异构联盟强化学习（PHLRL）的方法，以解决大规模异构合作问题。PHLRL记录智能体在训练过程中探索过的各种策略，并建立一个由多样策略组成的异构联盟，以帮助未来的策略优化。此外，我们设计了一种优先级策略梯度方法，以补偿由不同类型智能体数量差异引起的差距。",
    "tldr": "提出了一种优先级异构联盟强化学习（PHLRL）方法，旨在解决大规模异构合作问题，通过记录智能体探索过的各种策略并建立异构联盟来优化未来的策略。"
}