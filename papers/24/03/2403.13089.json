{
    "title": "Automatic Summarization of Doctor-Patient Encounter Dialogues Using Large Language Model through Prompt Tuning",
    "abstract": "arXiv:2403.13089v1 Announce Type: new  Abstract: Automatic text summarization (ATS) is an emerging technology to assist clinicians in providing continuous and coordinated care. This study presents an approach to summarize doctor-patient dialogues using generative large language models (LLMs). We developed prompt-tuning algorithms to instruct generative LLMs to summarize clinical text. We examined the prompt-tuning strategies, the size of soft prompts, and the few-short learning ability of GatorTronGPT, a generative clinical LLM developed using 277 billion clinical and general English words with up to 20 billion parameters. We compared GatorTronGPT with a previous solution based on fine-tuning of a widely used T5 model, using a clinical benchmark dataset MTS-DIALOG. The experimental results show that the GatorTronGPT- 20B model achieved the best performance on all evaluation metrics. The proposed solution has a low computing cost as the LLM parameters are not updated during prompt-tunin",
    "link": "https://arxiv.org/abs/2403.13089",
    "context": "Title: Automatic Summarization of Doctor-Patient Encounter Dialogues Using Large Language Model through Prompt Tuning\nAbstract: arXiv:2403.13089v1 Announce Type: new  Abstract: Automatic text summarization (ATS) is an emerging technology to assist clinicians in providing continuous and coordinated care. This study presents an approach to summarize doctor-patient dialogues using generative large language models (LLMs). We developed prompt-tuning algorithms to instruct generative LLMs to summarize clinical text. We examined the prompt-tuning strategies, the size of soft prompts, and the few-short learning ability of GatorTronGPT, a generative clinical LLM developed using 277 billion clinical and general English words with up to 20 billion parameters. We compared GatorTronGPT with a previous solution based on fine-tuning of a widely used T5 model, using a clinical benchmark dataset MTS-DIALOG. The experimental results show that the GatorTronGPT- 20B model achieved the best performance on all evaluation metrics. The proposed solution has a low computing cost as the LLM parameters are not updated during prompt-tunin",
    "path": "papers/24/03/2403.13089.json",
    "total_tokens": 917,
    "translated_title": "使用大型语言模型通过提示调整自动总结医患对话",
    "translated_abstract": "自动文本总结（ATS）是一种新兴技术，可以帮助临床医生提供持续和协调的护理。本研究介绍了一种使用生成式大型语言模型（LLMs）对医患对话进行总结的方法。我们开发了提示调整算法来指导生成式LLMs对临床文本进行总结。我们研究了提示调整策略、软提示的大小以及GatorTronGPT的few-short学习能力，该模型是使用2770亿临床和通用英语词汇开发的、拥有高达200亿参数的生成式临床LLM。我们将GatorTronGPT与基于广泛使用的T5模型微调的先前解决方案进行了比较，使用了临床基准数据集MTS-DIALOG。实验结果表明，GatorTronGPT-20B模型在所有评估指标上均取得了最佳性能。所提出的解决方案具有较低的计算成本，因为在提示调整过程中不更新LLM参数。",
    "tldr": "本研究提出了一种使用生成式大型语言模型对医患对话进行总结的方法，并通过提示调整算法指导模型进行临床文本总结，实现了在临床基准数据集上表现最佳的性能。",
    "en_tdlr": "This study presents an approach to summarize doctor-patient dialogues using generative large language models (LLMs) through prompt tuning algorithms, achieving the best performance on a clinical benchmark dataset."
}