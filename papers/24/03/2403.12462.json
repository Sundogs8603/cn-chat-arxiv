{
    "title": "Topological Representations of Heterogeneous Learning Dynamics of Recurrent Spiking Neural Networks",
    "abstract": "arXiv:2403.12462v1 Announce Type: cross  Abstract: Spiking Neural Networks (SNNs) have become an essential paradigm in neuroscience and artificial intelligence, providing brain-inspired computation. Recent advances in literature have studied the network representations of deep neural networks. However, there has been little work that studies representations learned by SNNs, especially using unsupervised local learning methods like spike-timing dependent plasticity (STDP). Recent work by \\cite{barannikov2021representation} has introduced a novel method to compare topological mappings of learned representations called Representation Topology Divergence (RTD). Though useful, this method is engineered particularly for feedforward deep neural networks and cannot be used for recurrent networks like Recurrent SNNs (RSNNs). This paper introduces a novel methodology to use RTD to measure the difference between distributed representations of RSNN models with different learning methods. We propos",
    "link": "https://arxiv.org/abs/2403.12462",
    "context": "Title: Topological Representations of Heterogeneous Learning Dynamics of Recurrent Spiking Neural Networks\nAbstract: arXiv:2403.12462v1 Announce Type: cross  Abstract: Spiking Neural Networks (SNNs) have become an essential paradigm in neuroscience and artificial intelligence, providing brain-inspired computation. Recent advances in literature have studied the network representations of deep neural networks. However, there has been little work that studies representations learned by SNNs, especially using unsupervised local learning methods like spike-timing dependent plasticity (STDP). Recent work by \\cite{barannikov2021representation} has introduced a novel method to compare topological mappings of learned representations called Representation Topology Divergence (RTD). Though useful, this method is engineered particularly for feedforward deep neural networks and cannot be used for recurrent networks like Recurrent SNNs (RSNNs). This paper introduces a novel methodology to use RTD to measure the difference between distributed representations of RSNN models with different learning methods. We propos",
    "path": "papers/24/03/2403.12462.json",
    "total_tokens": 840,
    "translated_title": "复发性尖峰神经网络异质学习动态的拓扑表示",
    "translated_abstract": "尖峰神经网络（SNNs）已成为神经科学和人工智能中一个重要范式，提供了类似大脑的计算。最近文献中的进展已经研究了深度神经网络的网络表示。然而，对SNNs学习到的表示的研究很少，尤其是使用类似时序相关可塑性（STDP）的无监督局部学习方法。最近\\cite{barannikov2021representation}的工作引入了一种比较学习表示的拓扑映射的新方法，称为表示拓扑离散度（RTD）。虽然有用，但这种方法特别针对前馈深度神经网络设计，不能用于像复发性SNNs（RSNNs）这样的循环网络。本文介绍了一种新的方法，使用RTD来衡量具有不同学习方法的RSNN模型的分布式表示之间的差异。",
    "tldr": "本文引入了一种新的方法，使用RTD来衡量不同学习方法下复发性尖峰神经网络（RSNN）模型分布式表示之间的差异。",
    "en_tdlr": "This paper introduces a novel methodology to use RTD to measure the difference between distributed representations of recurrent spiking neural network (RSNN) models with different learning methods."
}