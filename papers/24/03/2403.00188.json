{
    "title": "Impact of Decentralized Learning on Player Utilities in Stackelberg Games",
    "abstract": "arXiv:2403.00188v1 Announce Type: new  Abstract: When deployed in the world, a learning agent such as a recommender system or a chatbot often repeatedly interacts with another learning agent (such as a user) over time. In many such two-agent systems, each agent learns separately and the rewards of the two agents are not perfectly aligned. To better understand such cases, we examine the learning dynamics of the two-agent system and the implications for each agent's objective. We model these systems as Stackelberg games with decentralized learning and show that standard regret benchmarks (such as Stackelberg equilibrium payoffs) result in worst-case linear regret for at least one player. To better capture these systems, we construct a relaxed regret benchmark that is tolerant to small learning errors by agents. We show that standard learning algorithms fail to provide sublinear regret, and we develop algorithms to achieve near-optimal $O(T^{2/3})$ regret for both players with respect to ",
    "link": "https://arxiv.org/abs/2403.00188",
    "context": "Title: Impact of Decentralized Learning on Player Utilities in Stackelberg Games\nAbstract: arXiv:2403.00188v1 Announce Type: new  Abstract: When deployed in the world, a learning agent such as a recommender system or a chatbot often repeatedly interacts with another learning agent (such as a user) over time. In many such two-agent systems, each agent learns separately and the rewards of the two agents are not perfectly aligned. To better understand such cases, we examine the learning dynamics of the two-agent system and the implications for each agent's objective. We model these systems as Stackelberg games with decentralized learning and show that standard regret benchmarks (such as Stackelberg equilibrium payoffs) result in worst-case linear regret for at least one player. To better capture these systems, we construct a relaxed regret benchmark that is tolerant to small learning errors by agents. We show that standard learning algorithms fail to provide sublinear regret, and we develop algorithms to achieve near-optimal $O(T^{2/3})$ regret for both players with respect to ",
    "path": "papers/24/03/2403.00188.json",
    "total_tokens": 954,
    "translated_title": "分散学习对斯塔克尔贝格博弈中玩家效用的影响",
    "translated_abstract": "当学习代理（如推荐系统或聊天机器人）在现实世界中部署时，通常会随时间反复与另一个学习代理（如用户）交互。在许多这样的双代理系统中，每个代理单独学习，而两个代理的奖励并不完全一致。为了更好地理解这类情况，我们研究了两代理系统的学习动态以及对每个代理目标的影响。我们将这些系统建模为具有分散学习的斯塔克尔贝格博弈，并展示标准遗憾基准（如斯塔克尔贝格均衡回报）导致至少有一名玩家出现最坏情况的线性遗憾。为了更好地捕捉这些系统，我们构建了一种对代理的学习误差容忍的放松遗憾基准。我们展示了标准学习算法未能提供次线性遗憾，并开发了算法，实现了对于双方玩家而言与理想$O(T^{2/3})$遗憾接近最优。",
    "tldr": "研究了分散学习对斯塔克尔贝格博弈中玩家效用的影响，提出了一种放松遗憾基准来更好捕捉系统特征，并开发了实现近乎最优遗憾的算法。",
    "en_tdlr": "Investigated the impact of decentralized learning on player utilities in Stackelberg games, proposed a relaxed regret benchmark to better capture system characteristics, and developed algorithms achieving near-optimal regret for both players."
}