{
    "title": "Role Prompting Guided Domain Adaptation with General Capability Preserve for Large Language Models",
    "abstract": "arXiv:2403.02756v1 Announce Type: new  Abstract: The growing interest in Large Language Models (LLMs) for specialized applications has revealed a significant challenge: when tailored to specific domains, LLMs tend to experience catastrophic forgetting, compromising their general capabilities and leading to a suboptimal user experience. Additionally, crafting a versatile model for multiple domains simultaneously often results in a decline in overall performance due to confusion between domains. In response to these issues, we present the RolE Prompting Guided Multi-Domain Adaptation (REGA) strategy. This novel approach effectively manages multi-domain LLM adaptation through three key components: 1) Self-Distillation constructs and replays general-domain exemplars to alleviate catastrophic forgetting. 2) Role Prompting assigns a central prompt to the general domain and a unique role prompt to each specific domain to minimize inter-domain confusion during training. 3) Role Integration reu",
    "link": "https://arxiv.org/abs/2403.02756",
    "context": "Title: Role Prompting Guided Domain Adaptation with General Capability Preserve for Large Language Models\nAbstract: arXiv:2403.02756v1 Announce Type: new  Abstract: The growing interest in Large Language Models (LLMs) for specialized applications has revealed a significant challenge: when tailored to specific domains, LLMs tend to experience catastrophic forgetting, compromising their general capabilities and leading to a suboptimal user experience. Additionally, crafting a versatile model for multiple domains simultaneously often results in a decline in overall performance due to confusion between domains. In response to these issues, we present the RolE Prompting Guided Multi-Domain Adaptation (REGA) strategy. This novel approach effectively manages multi-domain LLM adaptation through three key components: 1) Self-Distillation constructs and replays general-domain exemplars to alleviate catastrophic forgetting. 2) Role Prompting assigns a central prompt to the general domain and a unique role prompt to each specific domain to minimize inter-domain confusion during training. 3) Role Integration reu",
    "path": "papers/24/03/2403.02756.json",
    "total_tokens": 830,
    "translated_title": "通过角色提示指导的通用能力保留的领域自适应大型语言模型",
    "translated_abstract": "针对大型语言模型（LLMs）应用于专门领域时遭遇的严重遗忘问题，以及为多个领域构建多功能模型时出现的性能下降问题，本文提出了RolE Prompting Guided Multi-Domain Adaptation (REGA) 策略。这一新颖方法通过三个关键组件有效管理多领域LLM适应：1）自我蒸馏（Self-Distillation）构建和重播通用域实例以减轻遗忘；2）角色提示（Role Prompting）为通用域分配中心提示和为每个特定领域分配独特角色提示，以最小化训练过程中不同领域之间的混淆；3）角色集成（Role Integration）",
    "tldr": "通过RolE Prompting Guided Multi-Domain Adaptation (REGA)策略，这篇论文提出了一种有效管理多领域大型语言模型适应的新方法，包括自我蒸馏、角色提示和角色集成这三个关键组件。",
    "en_tdlr": "This paper presents a novel approach, the RolE Prompting Guided Multi-Domain Adaptation (REGA) strategy, to effectively manage multi-domain adaptation of large language models, including three key components: Self-Distillation, Role Prompting, and Role Integration."
}