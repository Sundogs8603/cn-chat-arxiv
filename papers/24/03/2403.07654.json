{
    "title": "Analyzing Adversarial Attacks on Sequence-to-Sequence Relevance Models",
    "abstract": "arXiv:2403.07654v1 Announce Type: new  Abstract: Modern sequence-to-sequence relevance models like monoT5 can effectively capture complex textual interactions between queries and documents through cross-encoding. However, the use of natural language tokens in prompts, such as Query, Document, and Relevant for monoT5, opens an attack vector for malicious documents to manipulate their relevance score through prompt injection, e.g., by adding target words such as true. Since such possibilities have not yet been considered in retrieval evaluation, we analyze the impact of query-independent prompt injection via manually constructed templates and LLM-based rewriting of documents on several existing relevance models. Our experiments on the TREC Deep Learning track show that adversarial documents can easily manipulate different sequence-to-sequence relevance models, while BM25 (as a typical lexical model) is not affected. Remarkably, the attacks also affect encoder-only relevance models (which",
    "link": "https://arxiv.org/abs/2403.07654",
    "context": "Title: Analyzing Adversarial Attacks on Sequence-to-Sequence Relevance Models\nAbstract: arXiv:2403.07654v1 Announce Type: new  Abstract: Modern sequence-to-sequence relevance models like monoT5 can effectively capture complex textual interactions between queries and documents through cross-encoding. However, the use of natural language tokens in prompts, such as Query, Document, and Relevant for monoT5, opens an attack vector for malicious documents to manipulate their relevance score through prompt injection, e.g., by adding target words such as true. Since such possibilities have not yet been considered in retrieval evaluation, we analyze the impact of query-independent prompt injection via manually constructed templates and LLM-based rewriting of documents on several existing relevance models. Our experiments on the TREC Deep Learning track show that adversarial documents can easily manipulate different sequence-to-sequence relevance models, while BM25 (as a typical lexical model) is not affected. Remarkably, the attacks also affect encoder-only relevance models (which",
    "path": "papers/24/03/2403.07654.json",
    "total_tokens": 861,
    "translated_title": "分析序列到序列相关性模型上的对抗攻击",
    "translated_abstract": "现代序列到序列相关性模型（如monoT5）能够通过交叉编码有效捕捉查询和文档之间的复杂文本交互。然而，在提示中使用自然语言标记，如Query、Document和Relevant对于monoT5而言，为恶意文档开辟了注入攻击向量，通过注入提示（例如添加true等目标词）来操纵其相关性得分。由于检索评估中尚未考虑这样的可能性，我们通过手动构建的模板和基于LLM的文档重写，分析了查询无关提示注入对多个现有相关性模型的影响。我们在TREC深度学习跟踪上的实验表明，对抗性文档可以轻松操纵不同的序列到序列相关性模型，而BM25（作为典型的词汇模型）不受影响。值得注意的是，这些攻击也影响仅编码器相关性模型（）",
    "tldr": "对现代序列到序列相关性模型进行了对抗攻击分析，发现恶意文档可以通过注入提示来操纵其相关性得分，这一攻击机制影响了不同相关性模型，但词汇模型BM25不受影响。",
    "en_tdlr": "Adversarial attacks on modern sequence-to-sequence relevance models were analyzed, revealing that malicious documents can manipulate their relevance score by injecting prompts, affecting various relevance models, while BM25 lexical model remains unaffected."
}