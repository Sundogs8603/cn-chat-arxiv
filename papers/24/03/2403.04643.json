{
    "title": "QAQ: Quality Adaptive Quantization for LLM KV Cache",
    "abstract": "arXiv:2403.04643v1 Announce Type: new  Abstract: The emergence of LLMs has ignited a fresh surge of breakthroughs in NLP applications, particularly in domains such as question-answering systems and text generation. As the need for longer context grows, a significant bottleneck in model deployment emerges due to the linear expansion of the Key-Value (KV) cache with the context length. Existing methods primarily rely on various hypotheses, such as sorting the KV cache based on attention scores for replacement or eviction, to compress the KV cache and improve model throughput. However, heuristics used by these strategies may wrongly evict essential KV cache, which can significantly degrade model performance. In this paper, we propose QAQ, a Quality Adaptive Quantization scheme for the KV cache. We theoretically demonstrate that key cache and value cache exhibit distinct sensitivities to quantization, leading to the formulation of separate quantization strategies for their non-uniform quan",
    "link": "https://arxiv.org/abs/2403.04643",
    "context": "Title: QAQ: Quality Adaptive Quantization for LLM KV Cache\nAbstract: arXiv:2403.04643v1 Announce Type: new  Abstract: The emergence of LLMs has ignited a fresh surge of breakthroughs in NLP applications, particularly in domains such as question-answering systems and text generation. As the need for longer context grows, a significant bottleneck in model deployment emerges due to the linear expansion of the Key-Value (KV) cache with the context length. Existing methods primarily rely on various hypotheses, such as sorting the KV cache based on attention scores for replacement or eviction, to compress the KV cache and improve model throughput. However, heuristics used by these strategies may wrongly evict essential KV cache, which can significantly degrade model performance. In this paper, we propose QAQ, a Quality Adaptive Quantization scheme for the KV cache. We theoretically demonstrate that key cache and value cache exhibit distinct sensitivities to quantization, leading to the formulation of separate quantization strategies for their non-uniform quan",
    "path": "papers/24/03/2403.04643.json",
    "total_tokens": 874,
    "translated_title": "QAQ：用于LLM KV缓存的质量自适应量化",
    "translated_abstract": "LLM的出现在NLP应用中引发了一波新的突破，尤其在诸如问答系统和文本生成等领域。随着对更长上下文的需求增长，模型部署中出现了一个重要瓶颈，即由于上下文长度的线性增加而导致的Key-Value (KV) cache的扩展。现有方法主要依赖于各种假设，例如根据注意力分数对KV cache进行排序以进行替换或驱逐，以压缩KV cache并提高模型吞吐量。然而，这些策略使用的启发式方法可能会错误地驱逐关键的KV缓存，从而严重降低模型性能。本文提出了QAQ，一种用于KV缓存的质量自适应量化方案。我们在理论上证明了关键缓存和值缓存对量化表现出不同的敏感性，从而引发了针对它们的非均匀量化策略的制定。",
    "tldr": "提出了QAQ，一种用于KV缓存的质量自适应量化方案，理论上证明了关键缓存和值缓存对量化表现出不同的敏感性，因此制定了不同的量化策略。",
    "en_tdlr": "Proposed QAQ, a Quality Adaptive Quantization scheme for the KV cache, theoretically demonstrating distinct sensitivities of key cache and value cache to quantization, thus formulating separate quantization strategies."
}