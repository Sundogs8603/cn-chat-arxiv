{
    "title": "Naive Bayes-based Context Extension for Large Language Models",
    "abstract": "arXiv:2403.17552v1 Announce Type: new  Abstract: Large Language Models (LLMs) have shown promising in-context learning abilities. However, conventional In-Context Learning (ICL) approaches are often impeded by length limitations of transformer architecture, which pose challenges when attempting to effectively integrate supervision from a substantial number of demonstration examples. In this paper, we introduce a novel framework, called Naive Bayes-based Context Extension (NBCE), to enable existing LLMs to perform ICL with an increased number of demonstrations by significantly expanding their context size. Importantly, this expansion does not require fine-tuning or dependence on particular model architectures, all the while preserving linear efficiency. NBCE initially splits the context into equal-sized windows fitting the target LLM's maximum length. Then, it introduces a voting mechanism to select the most relevant window, regarded as the posterior context. Finally, it employs Bayes' ",
    "link": "https://arxiv.org/abs/2403.17552",
    "context": "Title: Naive Bayes-based Context Extension for Large Language Models\nAbstract: arXiv:2403.17552v1 Announce Type: new  Abstract: Large Language Models (LLMs) have shown promising in-context learning abilities. However, conventional In-Context Learning (ICL) approaches are often impeded by length limitations of transformer architecture, which pose challenges when attempting to effectively integrate supervision from a substantial number of demonstration examples. In this paper, we introduce a novel framework, called Naive Bayes-based Context Extension (NBCE), to enable existing LLMs to perform ICL with an increased number of demonstrations by significantly expanding their context size. Importantly, this expansion does not require fine-tuning or dependence on particular model architectures, all the while preserving linear efficiency. NBCE initially splits the context into equal-sized windows fitting the target LLM's maximum length. Then, it introduces a voting mechanism to select the most relevant window, regarded as the posterior context. Finally, it employs Bayes' ",
    "path": "papers/24/03/2403.17552.json",
    "total_tokens": 914,
    "translated_title": "基于朴素贝叶斯的大语言模型上下文扩展",
    "translated_abstract": "大型语言模型(LLMs)展现出令人期待的上下文学习能力。然而，传统的上下文学习(ICL)方法常常受到转换器架构长度限制的阻碍，在试图有效整合大量演示示例的监督时面临挑战。在本文中，我们引入了一种新颖的框架，称为基于朴素贝叶斯的上下文扩展(NBCE)，以使现有的LLMs能够通过显著扩展上下文大小执行ICL，从而增加演示示例的数量。重要的是，这种扩展不需要微调或依赖特定的模型架构，同时保持线性效率。NBCE首先将上下文分割成适合目标LLM最大长度的等大小窗口。然后，它引入了一个投票机制来选择最相关的窗口，被视为后验上下文。最后，它应用贝叶斯的方法",
    "tldr": "介绍了一种新颖的基于朴素贝叶斯的上下文扩展框架(NBCE)，能够通过显著扩展上下文大小使现有的大型语言模型(LLMs)执行上下文学习(ICL)以整合更多演示示例，而且不需要微调或依赖特定模型架构。",
    "en_tdlr": "Introducing a novel Naive Bayes-based Context Extension (NBCE) framework that enables existing Large Language Models (LLMs) to perform In-Context Learning (ICL) with an increased number of demonstration examples by significantly expanding their context size, without requiring fine-tuning or dependence on particular model architectures."
}