{
    "title": "Cross-Lingual Transfer for Natural Language Inference via Multilingual Prompt Translator",
    "abstract": "arXiv:2403.12407v1 Announce Type: new  Abstract: Based on multilingual pre-trained models, cross-lingual transfer with prompt learning has shown promising effectiveness, where soft prompt learned in a source language is transferred to target languages for downstream tasks, particularly in the low-resource scenario. To efficiently transfer soft prompt, we propose a novel framework, Multilingual Prompt Translator (MPT), where a multilingual prompt translator is introduced to properly process crucial knowledge embedded in prompt by changing language knowledge while retaining task knowledge. Concretely, we first train prompt in source language and employ translator to translate it into target prompt. Besides, we extend an external corpus as auxiliary data, on which an alignment task for predicted answer probability is designed to convert language knowledge, thereby equipping target prompt with multilingual knowledge. In few-shot settings on XNLI, MPT demonstrates superiority over baselines",
    "link": "https://arxiv.org/abs/2403.12407",
    "context": "Title: Cross-Lingual Transfer for Natural Language Inference via Multilingual Prompt Translator\nAbstract: arXiv:2403.12407v1 Announce Type: new  Abstract: Based on multilingual pre-trained models, cross-lingual transfer with prompt learning has shown promising effectiveness, where soft prompt learned in a source language is transferred to target languages for downstream tasks, particularly in the low-resource scenario. To efficiently transfer soft prompt, we propose a novel framework, Multilingual Prompt Translator (MPT), where a multilingual prompt translator is introduced to properly process crucial knowledge embedded in prompt by changing language knowledge while retaining task knowledge. Concretely, we first train prompt in source language and employ translator to translate it into target prompt. Besides, we extend an external corpus as auxiliary data, on which an alignment task for predicted answer probability is designed to convert language knowledge, thereby equipping target prompt with multilingual knowledge. In few-shot settings on XNLI, MPT demonstrates superiority over baselines",
    "path": "papers/24/03/2403.12407.json",
    "total_tokens": 866,
    "translated_title": "通过多语言提示翻译实现自然语言推理的跨语言转移",
    "translated_abstract": "基于多语言预训练模型，跨语言转移与提示学习表现出很高的有效性，其中在源语言中学习的软提示被转移到目标语言用于下游任务，尤其是在低资源情境下。为了有效地转移软提示，我们提出了一个新颖的框架，多语言提示翻译（MPT），引入了一个多语言提示翻译器来适当地处理提示中嵌入的关键知识，从而在保留任务知识的同时改变语言知识。具体来说，我们首先在源语言中训练提示，然后利用翻译器将其翻译成目标提示。此外，我们扩展了一个外部语料库作为辅助数据，对预测答案概率进行对齐任务，以转换语言知识，从而为目标提示提供多语言知识。在XNLI的少样本设置中，MPT表现出比基线方法更优越",
    "tldr": "提出了一种名为多语言提示翻译（MPT）的框架，通过引入多语言提示翻译器实现在低资源情境下将软提示从源语言有效转移到目标语言，在少样本设置下表现优越",
    "en_tdlr": "Introduced a framework called Multilingual Prompt Translator (MPT) to efficiently transfer soft prompts from source language to target language in low-resource scenarios using a multilingual prompt translator, demonstrating superiority in few-shot settings"
}