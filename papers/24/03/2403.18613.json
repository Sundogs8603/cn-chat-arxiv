{
    "title": "Scalable Lipschitz Estimation for CNNs",
    "abstract": "arXiv:2403.18613v1 Announce Type: new  Abstract: Estimating the Lipschitz constant of deep neural networks is of growing interest as it is useful for informing on generalisability and adversarial robustness. Convolutional neural networks (CNNs) in particular, underpin much of the recent success in computer vision related applications. However, although existing methods for estimating the Lipschitz constant can be tight, they have limited scalability when applied to CNNs. To tackle this, we propose a novel method to accelerate Lipschitz constant estimation for CNNs. The core idea is to divide a large convolutional block via a joint layer and width-wise partition, into a collection of smaller blocks. We prove an upper-bound on the Lipschitz constant of the larger block in terms of the Lipschitz constants of the smaller blocks. Through varying the partition factor, the resulting method can be adjusted to prioritise either accuracy or scalability and permits parallelisation. We demonstrate",
    "link": "https://arxiv.org/abs/2403.18613",
    "context": "Title: Scalable Lipschitz Estimation for CNNs\nAbstract: arXiv:2403.18613v1 Announce Type: new  Abstract: Estimating the Lipschitz constant of deep neural networks is of growing interest as it is useful for informing on generalisability and adversarial robustness. Convolutional neural networks (CNNs) in particular, underpin much of the recent success in computer vision related applications. However, although existing methods for estimating the Lipschitz constant can be tight, they have limited scalability when applied to CNNs. To tackle this, we propose a novel method to accelerate Lipschitz constant estimation for CNNs. The core idea is to divide a large convolutional block via a joint layer and width-wise partition, into a collection of smaller blocks. We prove an upper-bound on the Lipschitz constant of the larger block in terms of the Lipschitz constants of the smaller blocks. Through varying the partition factor, the resulting method can be adjusted to prioritise either accuracy or scalability and permits parallelisation. We demonstrate",
    "path": "papers/24/03/2403.18613.json",
    "total_tokens": 849,
    "translated_title": "可扩展的CNN的Lipschitz估计",
    "translated_abstract": "估计深度神经网络的Lipschitz常数正日益受到关注，因为它对通用性和对抗鲁棒性的评估很有用。卷积神经网络（CNNs）特别是在计算机视觉相关应用的成功中起着关键作用。然而，尽管现有的估计Lipschitz常数的方法可能很紧致，但当应用于CNNs时，它们的可扩展性有限。为了解决这个问题，我们提出了一种加速CNNs Lipschitz常数估计的新方法。其核心思想是通过联合层和宽度划分将大卷积块分割为一系列较小的块。我们证明了大块的Lipschitz常数上界与较小块的Lipschitz常数之间的关系。通过变化划分因子，得到的方法可以调节以优先考虑准确性或可扩展性，并支持并行化。我们展示",
    "tldr": "提出了一种加速CNN Lipschitz常数估计的方法，通过分割大卷积块为一系列较小块，并通过调节划分因子以优先考虑准确性或可扩展性。",
    "en_tdlr": "Proposed a method to accelerate Lipschitz constant estimation for CNNs by dividing large convolutional blocks into smaller blocks and adjusting the partition factor to prioritize either accuracy or scalability."
}