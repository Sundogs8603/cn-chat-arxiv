{
    "title": "SoK: Reducing the Vulnerability of Fine-tuned Language Models to Membership Inference Attacks",
    "abstract": "arXiv:2403.08481v1 Announce Type: new  Abstract: Natural language processing models have experienced a significant upsurge in recent years, with numerous applications being built upon them. Many of these applications require fine-tuning generic base models on customized, proprietary datasets. This fine-tuning data is especially likely to contain personal or sensitive information about individuals, resulting in increased privacy risk. Membership inference attacks are the most commonly employed attack to assess the privacy leakage of a machine learning model. However, limited research is available on the factors that affect the vulnerability of language models to this kind of attack, or on the applicability of different defense strategies in the language domain. We provide the first systematic review of the vulnerability of fine-tuned large language models to membership inference attacks, the various factors that come into play, and the effectiveness of different defense strategies. We f",
    "link": "https://arxiv.org/abs/2403.08481",
    "context": "Title: SoK: Reducing the Vulnerability of Fine-tuned Language Models to Membership Inference Attacks\nAbstract: arXiv:2403.08481v1 Announce Type: new  Abstract: Natural language processing models have experienced a significant upsurge in recent years, with numerous applications being built upon them. Many of these applications require fine-tuning generic base models on customized, proprietary datasets. This fine-tuning data is especially likely to contain personal or sensitive information about individuals, resulting in increased privacy risk. Membership inference attacks are the most commonly employed attack to assess the privacy leakage of a machine learning model. However, limited research is available on the factors that affect the vulnerability of language models to this kind of attack, or on the applicability of different defense strategies in the language domain. We provide the first systematic review of the vulnerability of fine-tuned large language models to membership inference attacks, the various factors that come into play, and the effectiveness of different defense strategies. We f",
    "path": "papers/24/03/2403.08481.json",
    "total_tokens": 810,
    "translated_title": "SoK：减少经过精细调整的语言模型对成员推断攻击的脆弱性",
    "translated_abstract": "自然语言处理模型近年来经历了显著增长，许多应用程序都是基于它们构建的。许多这些应用程序需要在定制的专有数据集上对通用基础模型进行微调。这种微调数据特别容易包含个人或敏感信息，从而增加了隐私风险。成员推断攻击是评估机器学习模型隐私泄漏的常用攻击方法。然而，目前对语言模型对这种攻击的脆弱性影响因素及在语言领域中不同防御策略的适用性的研究有限。我们提供了对大型经过微调的语言模型对成员推断攻击的脆弱性、涉及的各种因素以及不同防御策略有效性的第一次系统评估。",
    "tldr": "首次系统评估了大型经过微调的语言模型对成员推断攻击的脆弱性，以及相关因素和不同防御策略的有效性。"
}