{
    "title": "UniTable: Towards a Unified Framework for Table Structure Recognition via Self-Supervised Pretraining",
    "abstract": "arXiv:2403.04822v1 Announce Type: cross  Abstract: Tables convey factual and quantitative data with implicit conventions created by humans that are often challenging for machines to parse. Prior work on table structure recognition (TSR) has mainly centered around complex task-specific combinations of available inputs and tools. We present UniTable, a training framework that unifies both the training paradigm and training objective of TSR. Its training paradigm combines the simplicity of purely pixel-level inputs with the effectiveness and scalability empowered by self-supervised pretraining (SSP) from diverse unannotated tabular images. Our framework unifies the training objectives of all three TSR tasks - extracting table structure, cell content, and cell bounding box (bbox) - into a unified task-agnostic training objective: language modeling. Extensive quantitative and qualitative analyses highlight UniTable's state-of-the-art (SOTA) performance on four of the largest TSR datasets. T",
    "link": "https://arxiv.org/abs/2403.04822",
    "context": "Title: UniTable: Towards a Unified Framework for Table Structure Recognition via Self-Supervised Pretraining\nAbstract: arXiv:2403.04822v1 Announce Type: cross  Abstract: Tables convey factual and quantitative data with implicit conventions created by humans that are often challenging for machines to parse. Prior work on table structure recognition (TSR) has mainly centered around complex task-specific combinations of available inputs and tools. We present UniTable, a training framework that unifies both the training paradigm and training objective of TSR. Its training paradigm combines the simplicity of purely pixel-level inputs with the effectiveness and scalability empowered by self-supervised pretraining (SSP) from diverse unannotated tabular images. Our framework unifies the training objectives of all three TSR tasks - extracting table structure, cell content, and cell bounding box (bbox) - into a unified task-agnostic training objective: language modeling. Extensive quantitative and qualitative analyses highlight UniTable's state-of-the-art (SOTA) performance on four of the largest TSR datasets. T",
    "path": "papers/24/03/2403.04822.json",
    "total_tokens": 857,
    "translated_title": "UniTable: 朝向通过自监督预训练实现表结构识别的统一框架",
    "translated_abstract": "表格传达由人类创建的隐式约定的事实和数量数据，这往往是机器难以解析的。以往关于表结构识别（TSR）的工作主要集中在利用可用输入和工具的复杂特定任务组合上。我们提出了UniTable，这是一个统一TSR的训练框架，其训练范式结合了纯粹像素级输入的简单性以及来自各种未注释表格图像的自监督预训练（SSP）赋予的有效性和可伸缩性。我们的框架将所有三个TSR任务的训练目标 - 提取表结构，单元格内容和单元格边界框（bbox） - 统一为一个统一的与任务无关的训练目标：语言建模。广泛的定量和定性分析突显了UniTable在四个最大的TSR数据集上的最先进性能。",
    "tldr": "UniTable 提出了一个统一框架，通过自监督预训练实现表结构识别，将不同的任务和目标统一到语言建模中，在四个最大的TSR数据集上表现出最先进的性能。",
    "en_tdlr": "UniTable proposes a unified framework for table structure recognition via self-supervised pretraining, unifying different tasks and objectives into language modeling, with state-of-the-art performance on four of the largest TSR datasets."
}