{
    "title": "On the Global Convergence of Policy Gradient in Average Reward Markov Decision Processes",
    "abstract": "arXiv:2403.06806v1 Announce Type: new  Abstract: We present the first finite time global convergence analysis of policy gradient in the context of infinite horizon average reward Markov decision processes (MDPs). Specifically, we focus on ergodic tabular MDPs with finite state and action spaces. Our analysis shows that the policy gradient iterates converge to the optimal policy at a sublinear rate of $O\\left({\\frac{1}{T}}\\right),$ which translates to $O\\left({\\log(T)}\\right)$ regret, where $T$ represents the number of iterations. Prior work on performance bounds for discounted reward MDPs cannot be extended to average reward MDPs because the bounds grow proportional to the fifth power of the effective horizon. Thus, our primary contribution is in proving that the policy gradient algorithm converges for average-reward MDPs and in obtaining finite-time performance guarantees. In contrast to the existing discounted reward performance bounds, our performance bounds have an explicit depende",
    "link": "https://arxiv.org/abs/2403.06806",
    "context": "Title: On the Global Convergence of Policy Gradient in Average Reward Markov Decision Processes\nAbstract: arXiv:2403.06806v1 Announce Type: new  Abstract: We present the first finite time global convergence analysis of policy gradient in the context of infinite horizon average reward Markov decision processes (MDPs). Specifically, we focus on ergodic tabular MDPs with finite state and action spaces. Our analysis shows that the policy gradient iterates converge to the optimal policy at a sublinear rate of $O\\left({\\frac{1}{T}}\\right),$ which translates to $O\\left({\\log(T)}\\right)$ regret, where $T$ represents the number of iterations. Prior work on performance bounds for discounted reward MDPs cannot be extended to average reward MDPs because the bounds grow proportional to the fifth power of the effective horizon. Thus, our primary contribution is in proving that the policy gradient algorithm converges for average-reward MDPs and in obtaining finite-time performance guarantees. In contrast to the existing discounted reward performance bounds, our performance bounds have an explicit depende",
    "path": "papers/24/03/2403.06806.json",
    "total_tokens": 884,
    "translated_title": "关于平均奖励马尔可夫决策过程中策略梯度的全局收敛性",
    "translated_abstract": "我们首次在无限时间间隔的平均奖励马尔可夫决策过程（MDP）的背景下，对策略梯度的有限时间全局收敛性进行了分析。具体而言，我们关注具有有限状态和动作空间的遍历标签MDP。我们的分析表明，策略梯度迭代以$O\\left({\\frac{1}{T}}\\right)$的次线性速率收敛到最优策略，这对应于$O\\left({\\log(T)}\\right)$的后悔，其中$T$表示迭代次数。之前关于折现奖励MDPs性能界的工作无法推广到平均奖励MDPs，因为界会随有效视距的五次方增长。因此，我们的主要贡献在于证明策略梯度算法在平均奖励MDPs中收敛，并得到有限时间的性能保证。与现有的折现奖励性能界相比，我们的性能界具有明确的依赖关系。",
    "tldr": "该论文首次证明了策略梯度算法在平均奖励MDPs中的收敛性，并获得了有限时间的性能保证。",
    "en_tdlr": "The paper demonstrates the convergence of policy gradient algorithm in average reward MDPs for the first time and provides finite-time performance guarantees."
}