{
    "title": "CAUSE: Counterfactual Assessment of User Satisfaction Estimation in Task-Oriented Dialogue Systems",
    "abstract": "arXiv:2403.19056v1 Announce Type: new  Abstract: An important unexplored aspect in previous work on user satisfaction estimation for Task-Oriented Dialogue (TOD) systems is their evaluation in terms of robustness for the identification of user dissatisfaction: current benchmarks for user satisfaction estimation in TOD systems are highly skewed towards dialogues for which the user is satisfied. The effect of having a more balanced set of satisfaction labels on performance is unknown. However, balancing the data with more dissatisfactory dialogue samples requires further data collection and human annotation, which is costly and time-consuming. In this work, we leverage large language models (LLMs) and unlock their ability to generate satisfaction-aware counterfactual dialogues to augment the set of original dialogues of a test collection. We gather human annotations to ensure the reliability of the generated samples. We evaluate two open-source LLMs as user satisfaction estimators on our",
    "link": "https://arxiv.org/abs/2403.19056",
    "context": "Title: CAUSE: Counterfactual Assessment of User Satisfaction Estimation in Task-Oriented Dialogue Systems\nAbstract: arXiv:2403.19056v1 Announce Type: new  Abstract: An important unexplored aspect in previous work on user satisfaction estimation for Task-Oriented Dialogue (TOD) systems is their evaluation in terms of robustness for the identification of user dissatisfaction: current benchmarks for user satisfaction estimation in TOD systems are highly skewed towards dialogues for which the user is satisfied. The effect of having a more balanced set of satisfaction labels on performance is unknown. However, balancing the data with more dissatisfactory dialogue samples requires further data collection and human annotation, which is costly and time-consuming. In this work, we leverage large language models (LLMs) and unlock their ability to generate satisfaction-aware counterfactual dialogues to augment the set of original dialogues of a test collection. We gather human annotations to ensure the reliability of the generated samples. We evaluate two open-source LLMs as user satisfaction estimators on our",
    "path": "papers/24/03/2403.19056.json",
    "total_tokens": 822,
    "translated_title": "CAUSE: 在面向任务型对话系统中利用反事实评估用户满意度估计",
    "translated_abstract": "先前关于任务型对话系统中用户满意度估计的工作中一个重要但未被探索的方面是对其在识别用户不满意方面的鲁棒性进行评估：当前用于任务型对话系统中用户满意度估计的基准测试高度倾向于用户满意的对话。具有更平衡满意度标签集合对性能的影响是未知的。然而，通过更多的不满对话样本平衡数据需要进一步的数据收集和人工注释，这是昂贵和耗时的。本工作中，我们利用大型语言模型（LLMs）并解锁其生成满意感知反事实对话的能力，以增加测试集合的原始对话集合。我们收集人工注释以确保生成样本的可靠性。我们评估两个开源LLM作为用户满意度估计器。",
    "tldr": "本文通过利用大型语言模型生成满意感知的反事实对话来增加任务型对话系统的原始对话集合，以改善用户满意度估计的鲁棒性。"
}