{
    "title": "Improving Generalization via Meta-Learning on Hard Samples",
    "abstract": "arXiv:2403.12236v1 Announce Type: new  Abstract: Learned reweighting (LRW) approaches to supervised learning use an optimization criterion to assign weights for training instances, in order to maximize performance on a representative validation dataset. We pose and formalize the problem of optimized selection of the validation set used in LRW training, to improve classifier generalization. In particular, we show that using hard-to-classify instances in the validation set has both a theoretical connection to, and strong empirical evidence of generalization. We provide an efficient algorithm for training this meta-optimized model, as well as a simple train-twice heuristic for careful comparative study. We demonstrate that LRW with easy validation data performs consistently worse than LRW with hard validation data, establishing the validity of our meta-optimization problem. Our proposed algorithm outperforms a wide range of baselines on a range of datasets and domain shift challenges (Ima",
    "link": "https://arxiv.org/abs/2403.12236",
    "context": "Title: Improving Generalization via Meta-Learning on Hard Samples\nAbstract: arXiv:2403.12236v1 Announce Type: new  Abstract: Learned reweighting (LRW) approaches to supervised learning use an optimization criterion to assign weights for training instances, in order to maximize performance on a representative validation dataset. We pose and formalize the problem of optimized selection of the validation set used in LRW training, to improve classifier generalization. In particular, we show that using hard-to-classify instances in the validation set has both a theoretical connection to, and strong empirical evidence of generalization. We provide an efficient algorithm for training this meta-optimized model, as well as a simple train-twice heuristic for careful comparative study. We demonstrate that LRW with easy validation data performs consistently worse than LRW with hard validation data, establishing the validity of our meta-optimization problem. Our proposed algorithm outperforms a wide range of baselines on a range of datasets and domain shift challenges (Ima",
    "path": "papers/24/03/2403.12236.json",
    "total_tokens": 825,
    "translated_title": "通过元学习在困难样本上改善泛化性能",
    "translated_abstract": "学习的重加权(LRW)方法用一个优化准则为训练实例分配权重，以便在一个代表性验证数据集上最大化性能。我们提出并形式化了LRW训练中的优化验证集选择的问题，以改善分类器的泛化性能。特别地，我们展示了在验证集中使用难以分类的实例既与理论相关，又有强有力的实证证据支持泛化。我们提供了一个高效的算法来训练这个元优化模型，以及一个简单的两次训练启发式方法以进行谨慎的比较研究。我们证明，与易验证数据一起的LRW表现始终比难验证数据一起的LRW表现差，从而确立了我们的元优化问题的有效性。我们提出的算法在各种数据集和领域转移挑战上胜过了各种基线。",
    "tldr": "在学习的重加权(LRW)方法中，通过元学习中使用难以分类的实例作为验证集，来改善分类器的泛化性能，并提出了一个高效的算法来训练这个模型。",
    "en_tdlr": "Meta-learning with hard-to-classify instances in Learned Reweighting (LRW) approach improves classifier generalization, with an efficient algorithm proposed for training this model."
}