{
    "title": "One Category One Prompt: Dataset Distillation using Diffusion Models",
    "abstract": "arXiv:2403.07142v1 Announce Type: cross  Abstract: The extensive amounts of data required for training deep neural networks pose significant challenges on storage and transmission fronts. Dataset distillation has emerged as a promising technique to condense the information of massive datasets into a much smaller yet representative set of synthetic samples. However, traditional dataset distillation approaches often struggle to scale effectively with high-resolution images and more complex architectures due to the limitations in bi-level optimization. Recently, several works have proposed exploiting knowledge distillation with decoupled optimization schemes to scale up dataset distillation. Although these methods effectively address the scalability issue, they rely on extensive image augmentations requiring the storage of soft labels for augmented images. In this paper, we introduce Dataset Distillation using Diffusion Models (D3M) as a novel paradigm for dataset distillation, leveraging",
    "link": "https://arxiv.org/abs/2403.07142",
    "context": "Title: One Category One Prompt: Dataset Distillation using Diffusion Models\nAbstract: arXiv:2403.07142v1 Announce Type: cross  Abstract: The extensive amounts of data required for training deep neural networks pose significant challenges on storage and transmission fronts. Dataset distillation has emerged as a promising technique to condense the information of massive datasets into a much smaller yet representative set of synthetic samples. However, traditional dataset distillation approaches often struggle to scale effectively with high-resolution images and more complex architectures due to the limitations in bi-level optimization. Recently, several works have proposed exploiting knowledge distillation with decoupled optimization schemes to scale up dataset distillation. Although these methods effectively address the scalability issue, they rely on extensive image augmentations requiring the storage of soft labels for augmented images. In this paper, we introduce Dataset Distillation using Diffusion Models (D3M) as a novel paradigm for dataset distillation, leveraging",
    "path": "papers/24/03/2403.07142.json",
    "total_tokens": 819,
    "translated_title": "一个类别一个提示：使用扩散模型进行数据集精炼",
    "translated_abstract": "深度神经网络训练所需的大量数据对存储和传输方面提出了重大挑战。数据集精炼已经成为将大规模数据集的信息压缩成一组代表性合成样本的有前途的技术。然而，传统的数据集精炼方法通常在处理高分辨率图像和更复杂架构时很难有效扩展，这是由于双层优化的限制。最近，一些工作提出利用分离的优化方案将知识精炼和数据集精炼相结合，以扩大数据集精炼规模。尽管这些方法有效地解决了可扩展性问题，但它们依赖于广泛的图像增强，需要存储增强图像的软标签。在本文中，我们引入了使用扩散模型的数据集精炼（D3M）作为数据集精炼的新范式，利用",
    "tldr": "提出了使用扩散模型进行数据集精炼的新方法，有效地解决了数据集精炼在处理高分辨率图像和复杂架构时的可扩展性问题",
    "en_tdlr": "Introduces a novel approach using diffusion models for dataset distillation, effectively addressing the scalability issue when dealing with high-resolution images and complex architectures."
}