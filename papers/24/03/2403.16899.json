{
    "title": "State Space Models as Foundation Models: A Control Theoretic Overview",
    "abstract": "arXiv:2403.16899v1 Announce Type: cross  Abstract: In recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their m",
    "link": "https://arxiv.org/abs/2403.16899",
    "context": "Title: State Space Models as Foundation Models: A Control Theoretic Overview\nAbstract: arXiv:2403.16899v1 Announce Type: cross  Abstract: In recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their m",
    "path": "papers/24/03/2403.16899.json",
    "total_tokens": 804,
    "translated_title": "基于状态空间模型的基础模型：一个控制理论概述",
    "translated_abstract": "近年来，将线性状态空间模型（SSM）整合到基础模型的深度神经网络架构中引起了越来越多的关注。最近Mamba的成功展示了比现有最先进的Transformer架构在语言任务中表现更好。基础模型，如GPT-4，旨在将序列数据编码为潜在空间，以学习数据的压缩表示。控制理论家使用SSM来有效地建模动态系统追求相同的目标。因此，SSM可以自然地与深度序列建模相连接，提供了在相应研究领域之间创造协同作用的机会。本文旨在向控制理论家简要介绍基于SSM的架构，并总结最新的研究进展。它系统回顾了最成功的SSM提议，并突出了它们的m",
    "tldr": "将状态空间模型整合到深度神经网络架构中，为控制理论家和研究人员提供了一种有效建模动态系统的新途径。",
    "en_tdlr": "Integrating state space models into deep neural network architectures provides a new approach for control theorists and researchers to effectively model dynamical systems."
}