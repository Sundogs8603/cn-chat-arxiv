{
    "title": "EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs",
    "abstract": "arXiv:2403.02775v1 Announce Type: new  Abstract: Large language models (LLMs) have proven to be very superior to conventional methods in various tasks. However, their expensive computations and high memory requirements are prohibitive for deployment. Model quantization is an effective method for reducing this overhead. The problem is that in most previous works, the quantized model was calibrated using few samples from the training data, which might affect the generalization of the quantized LLMs to unknown cases and tasks. Hence in this work, we explore an important question: Can we design a data-independent quantization method for LLMs to guarantee its generalization performance? In this work, we propose EasyQuant, a training-free and data-independent weight-only quantization algorithm for LLMs. Our observation indicates that two factors: outliers in the weight and quantization ranges, are essential for reducing the quantization error. Therefore, in EasyQuant, we leave the outliers (",
    "link": "https://arxiv.org/abs/2403.02775",
    "context": "Title: EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs\nAbstract: arXiv:2403.02775v1 Announce Type: new  Abstract: Large language models (LLMs) have proven to be very superior to conventional methods in various tasks. However, their expensive computations and high memory requirements are prohibitive for deployment. Model quantization is an effective method for reducing this overhead. The problem is that in most previous works, the quantized model was calibrated using few samples from the training data, which might affect the generalization of the quantized LLMs to unknown cases and tasks. Hence in this work, we explore an important question: Can we design a data-independent quantization method for LLMs to guarantee its generalization performance? In this work, we propose EasyQuant, a training-free and data-independent weight-only quantization algorithm for LLMs. Our observation indicates that two factors: outliers in the weight and quantization ranges, are essential for reducing the quantization error. Therefore, in EasyQuant, we leave the outliers (",
    "path": "papers/24/03/2403.02775.json",
    "total_tokens": 840,
    "translated_title": "EasyQuant: 一种用于LLM的高效无数据量化算法",
    "translated_abstract": "大型语言模型(LLMs)在各种任务中已被证明要比传统方法优越得多。然而，它们昂贵的计算和高内存需求使其难以部署。模型量化是减少这种开销的有效方法。然而，大多数先前的工作中，量化模型是使用少量训练数据样本进行校准的，这可能会影响前人工作里量化后的LLMs对未知情况和任务的泛化性能。因此，在这项工作中，我们探讨了一个重要问题：我们是否可以为LLM设计一种无数据量化方法以保证其泛化性能？在这项工作中，我们提出了EasyQuant，一种用于LLM的无需训练和无数据的仅针对权重的量化算法。我们的观察表明，权重和量化范围中的异常值是减少量化误差的关键因素。因此，在EasyQuant中，我们保留了这些异常值（待续）。",
    "tldr": "EasyQuant是一种无需训练的、无需数据的仅针对权重的量化算法，旨在减少量化误差并保证LLM的泛化性能。",
    "en_tdlr": "EasyQuant is a data-free and training-free weight-only quantization algorithm designed to reduce quantization error and ensure the generalization performance of LLMs."
}