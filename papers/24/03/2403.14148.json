{
    "title": "Efficient Video Diffusion Models via Content-Frame Motion-Latent Decomposition",
    "abstract": "arXiv:2403.14148v1 Announce Type: cross  Abstract: Video diffusion models have recently made great progress in generation quality, but are still limited by the high memory and computational requirements. This is because current video diffusion models often attempt to process high-dimensional videos directly. To tackle this issue, we propose content-motion latent diffusion model (CMD), a novel efficient extension of pretrained image diffusion models for video generation. Specifically, we propose an autoencoder that succinctly encodes a video as a combination of a content frame (like an image) and a low-dimensional motion latent representation. The former represents the common content, and the latter represents the underlying motion in the video, respectively. We generate the content frame by fine-tuning a pretrained image diffusion model, and we generate the motion latent representation by training a new lightweight diffusion model. A key innovation here is the design of a compact laten",
    "link": "https://arxiv.org/abs/2403.14148",
    "context": "Title: Efficient Video Diffusion Models via Content-Frame Motion-Latent Decomposition\nAbstract: arXiv:2403.14148v1 Announce Type: cross  Abstract: Video diffusion models have recently made great progress in generation quality, but are still limited by the high memory and computational requirements. This is because current video diffusion models often attempt to process high-dimensional videos directly. To tackle this issue, we propose content-motion latent diffusion model (CMD), a novel efficient extension of pretrained image diffusion models for video generation. Specifically, we propose an autoencoder that succinctly encodes a video as a combination of a content frame (like an image) and a low-dimensional motion latent representation. The former represents the common content, and the latter represents the underlying motion in the video, respectively. We generate the content frame by fine-tuning a pretrained image diffusion model, and we generate the motion latent representation by training a new lightweight diffusion model. A key innovation here is the design of a compact laten",
    "path": "papers/24/03/2403.14148.json",
    "total_tokens": 751,
    "translated_title": "通过内容-帧动态潜分解实现高效视频扩散模型",
    "translated_abstract": "视频扩散模型在生成质量方面取得了长足进展，但仍受制于高内存和计算要求。我们提出内容-动态潜扩散模型（CMD），作为预训练图像扩散模型在视频生成中的高效扩展。CMD通过自动编码器将视频简洁地编码为内容帧和低维动态潜表示的组合。我们通过对预训练图像扩散模型进行微调来生成内容帧，并通过训练新的轻量级扩散模型来生成动态潜表示。这里的关键创新在于设计了一种紧凑的潜",
    "tldr": "提出了一种高效视频扩散模型CMD，通过预训练图像扩散模型和新的轻量级扩散模型生成内容帧和动态潜表示，以解决高内存和计算要求的问题。",
    "en_tdlr": "Proposed an efficient video diffusion model CMD that generates content frames and motion latent representations using pretrained image diffusion models and new lightweight diffusion models to address high memory and computational requirements."
}