{
    "title": "Online Continual Learning For Interactive Instruction Following Agents",
    "abstract": "arXiv:2403.07548v1 Announce Type: new  Abstract: In learning an embodied agent executing daily tasks via language directives, the literature largely assumes that the agent learns all training data at the beginning. We argue that such a learning scenario is less realistic since a robotic agent is supposed to learn the world continuously as it explores and perceives it. To take a step towards a more realistic embodied agent learning scenario, we propose two continual learning setups for embodied agents; learning new behaviors (Behavior Incremental Learning, Behavior-IL) and new environments (Environment Incremental Learning, Environment-IL) For the tasks, previous 'data prior' based continual learning methods maintain logits for the past tasks. However, the stored information is often insufficiently learned information and requires task boundary information, which might not always be available. Here, we propose to update them based on confidence scores without task boundary information d",
    "link": "https://arxiv.org/abs/2403.07548",
    "context": "Title: Online Continual Learning For Interactive Instruction Following Agents\nAbstract: arXiv:2403.07548v1 Announce Type: new  Abstract: In learning an embodied agent executing daily tasks via language directives, the literature largely assumes that the agent learns all training data at the beginning. We argue that such a learning scenario is less realistic since a robotic agent is supposed to learn the world continuously as it explores and perceives it. To take a step towards a more realistic embodied agent learning scenario, we propose two continual learning setups for embodied agents; learning new behaviors (Behavior Incremental Learning, Behavior-IL) and new environments (Environment Incremental Learning, Environment-IL) For the tasks, previous 'data prior' based continual learning methods maintain logits for the past tasks. However, the stored information is often insufficiently learned information and requires task boundary information, which might not always be available. Here, we propose to update them based on confidence scores without task boundary information d",
    "path": "papers/24/03/2403.07548.json",
    "total_tokens": 830,
    "translated_title": "互动指令跟随代理的在线持续学习",
    "translated_abstract": "在通过语言指令执行日常任务的具身代理学习过程中，文献大都假定代理在开始时就学习所有训练数据。我们认为这样的学习场景较不现实，因为机器人代理应该在探索和感知世界的过程中不断地学习。为了朝着更真实的具身代理学习场景迈进一步，我们提出了两种持续学习设置供具身代理使用；学习新行为（行为增量学习，Behavior-IL）和新环境（环境增量学习，Environment-IL）。在任务中，先前基于“数据先验”的持续学习方法维护过去任务的logits。然而，存储的信息往往是不充分学习的信息，需要任务边界信息，而这种信息并不总是可用。在这里，我们提议基于自信度得分而无需任务边界信息来更新它们。",
    "tldr": "我们提出了针对具身代理的两种持续学习设置：学习新行为和新环境。同时，我们通过自信度得分来更新存储的信息，从而避免需要任务边界信息的问题。",
    "en_tdlr": "We propose two continual learning setups for embodied agents: learning new behaviors and new environments. Meanwhile, we update the stored information based on confidence scores to avoid the need for task boundary information."
}