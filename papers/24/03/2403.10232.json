{
    "title": "Matrix Completion via Nonsmooth Regularization of Fully Connected Neural Networks",
    "abstract": "arXiv:2403.10232v1 Announce Type: cross  Abstract: Conventional matrix completion methods approximate the missing values by assuming the matrix to be low-rank, which leads to a linear approximation of missing values. It has been shown that enhanced performance could be attained by using nonlinear estimators such as deep neural networks. Deep fully connected neural networks (FCNNs), one of the most suitable architectures for matrix completion, suffer from over-fitting due to their high capacity, which leads to low generalizability. In this paper, we control over-fitting by regularizing the FCNN model in terms of the $\\ell_{1}$ norm of intermediate representations and nuclear norm of weight matrices. As such, the resulting regularized objective function becomes nonsmooth and nonconvex, i.e., existing gradient-based methods cannot be applied to our model. We propose a variant of the proximal gradient method and investigate its convergence to a critical point. In the initial epochs of FCNN",
    "link": "https://arxiv.org/abs/2403.10232",
    "context": "Title: Matrix Completion via Nonsmooth Regularization of Fully Connected Neural Networks\nAbstract: arXiv:2403.10232v1 Announce Type: cross  Abstract: Conventional matrix completion methods approximate the missing values by assuming the matrix to be low-rank, which leads to a linear approximation of missing values. It has been shown that enhanced performance could be attained by using nonlinear estimators such as deep neural networks. Deep fully connected neural networks (FCNNs), one of the most suitable architectures for matrix completion, suffer from over-fitting due to their high capacity, which leads to low generalizability. In this paper, we control over-fitting by regularizing the FCNN model in terms of the $\\ell_{1}$ norm of intermediate representations and nuclear norm of weight matrices. As such, the resulting regularized objective function becomes nonsmooth and nonconvex, i.e., existing gradient-based methods cannot be applied to our model. We propose a variant of the proximal gradient method and investigate its convergence to a critical point. In the initial epochs of FCNN",
    "path": "papers/24/03/2403.10232.json",
    "total_tokens": 822,
    "translated_title": "通过对全连接神经网络进行非光滑正则化的矩阵补全",
    "translated_abstract": "传统的矩阵补全方法通过假设矩阵具有低秩来逼近缺失值，从而导致缺失值的线性逼近。已经表明，使用非线性估计器（如深度神经网络）可以获得更好的性能。深度全连接神经网络（FCNN）是矩阵补全最适合的架构之一，由于其高容量而导致过拟合，进而导致泛化能力低。本文通过在中间表示的 $\\ell_{1}$ 范数和权重矩阵的核范数方面对FCNN模型进行正则化来控制过拟合。因此，得到的正则化目标函数变得非光滑和非凸，即现有的基于梯度的方法无法应用于我们的模型。我们提出了一种近端梯度方法的变体，并研究其收敛到临界点。在FCNN的初始时期。",
    "tldr": "通过对全连接神经网络进行非光滑正则化，可以控制过拟合问题，提高矩阵补全性能。",
    "en_tdlr": "Controlling over-fitting by nonsmooth regularization of fully connected neural networks improves matrix completion performance."
}