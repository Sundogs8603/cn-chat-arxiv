{
    "title": "\"Sorry, Come Again?\" Prompting -- Enhancing Comprehension and Diminishing Hallucination with [PAUSE]-injected Optimal Paraphrasing",
    "abstract": "arXiv:2403.18976v1 Announce Type: cross  Abstract: Hallucination has emerged as the most vulnerable aspect of contemporary Large Language Models (LLMs). In this paper, we introduce the Sorry, Come Again (SCA) prompting, aimed to avoid LLM hallucinations by enhancing comprehension through: (i) optimal paraphrasing and (ii) injecting [PAUSE] tokens to delay LLM generation. First, we provide an in-depth analysis of linguistic nuances: formality, readability, and concreteness of prompts for 21 LLMs, and elucidate how these nuances contribute to hallucinated generation. Prompts with lower readability, formality, or concreteness pose comprehension challenges for LLMs, similar to those faced by humans. In such scenarios, an LLM tends to speculate and generate content based on its imagination (associative memory) to fill these information gaps. Although these speculations may occasionally align with factual information, their accuracy is not assured, often resulting in hallucination. Recent st",
    "link": "https://arxiv.org/abs/2403.18976",
    "context": "Title: \"Sorry, Come Again?\" Prompting -- Enhancing Comprehension and Diminishing Hallucination with [PAUSE]-injected Optimal Paraphrasing\nAbstract: arXiv:2403.18976v1 Announce Type: cross  Abstract: Hallucination has emerged as the most vulnerable aspect of contemporary Large Language Models (LLMs). In this paper, we introduce the Sorry, Come Again (SCA) prompting, aimed to avoid LLM hallucinations by enhancing comprehension through: (i) optimal paraphrasing and (ii) injecting [PAUSE] tokens to delay LLM generation. First, we provide an in-depth analysis of linguistic nuances: formality, readability, and concreteness of prompts for 21 LLMs, and elucidate how these nuances contribute to hallucinated generation. Prompts with lower readability, formality, or concreteness pose comprehension challenges for LLMs, similar to those faced by humans. In such scenarios, an LLM tends to speculate and generate content based on its imagination (associative memory) to fill these information gaps. Although these speculations may occasionally align with factual information, their accuracy is not assured, often resulting in hallucination. Recent st",
    "path": "papers/24/03/2403.18976.json",
    "total_tokens": 879,
    "translated_title": "\"对不起，再次来吗？提示——通过注入[PAUSE]优化改写来增强理解力和减少幻觉\"",
    "translated_abstract": "Hallucination已经成为当代大规模语言模型（LLMs）中最脆弱的方面。本文介绍了Sorry, Come Again (SCA)提示，旨在通过：(i) 最佳的改写和(ii) 注入[PAUSE]标记来延迟LLMs的生成，以避免LLM产生幻觉。首先，我们对21个LLMs的提示的语言细微差别进行了深入分析：正式性、可读性和具体性，并阐明了这些差别是如何导致产生幻觉的。提示的可读性、正式性或具体性较低会给LLMs带来理解挑战，类似于人类所面临的挑战。在这种情况下，LLM倾向于根据其想象力（联想记忆）推测和生成内容来填补这些信息缺失。尽管这些猜测偶尔可能与事实信息一致，但其准确性并不保证，经常导致幻觉。",
    "tldr": "介绍了一种新的提示策略“Sorry, Come Again (SCA)”来避免大规模语言模型（LLMs）产生幻觉，通过进行最佳的改写和注入[PAUSE]标记来增强理解力。"
}