{
    "title": "LNPT: Label-free Network Pruning and Training",
    "abstract": "arXiv:2403.12690v1 Announce Type: new  Abstract: Pruning before training enables the deployment of neural networks on smart devices. By retaining weights conducive to generalization, pruned networks can be accommodated on resource-constrained smart devices. It is commonly held that the distance on weight norms between the initialized and the fully-trained networks correlates with generalization performance. However, as we have uncovered, inconsistency between this metric and generalization during training processes, which poses an obstacle to determine the pruned structures on smart devices in advance. In this paper, we introduce the concept of the learning gap, emphasizing its accurate correlation with generalization. Experiments show that the learning gap, in the form of feature maps from the penultimate layer of networks, aligns with variations of generalization performance. We propose a novel learning framework, LNPT, which enables mature networks on the cloud to provide online gui",
    "link": "https://arxiv.org/abs/2403.12690",
    "context": "Title: LNPT: Label-free Network Pruning and Training\nAbstract: arXiv:2403.12690v1 Announce Type: new  Abstract: Pruning before training enables the deployment of neural networks on smart devices. By retaining weights conducive to generalization, pruned networks can be accommodated on resource-constrained smart devices. It is commonly held that the distance on weight norms between the initialized and the fully-trained networks correlates with generalization performance. However, as we have uncovered, inconsistency between this metric and generalization during training processes, which poses an obstacle to determine the pruned structures on smart devices in advance. In this paper, we introduce the concept of the learning gap, emphasizing its accurate correlation with generalization. Experiments show that the learning gap, in the form of feature maps from the penultimate layer of networks, aligns with variations of generalization performance. We propose a novel learning framework, LNPT, which enables mature networks on the cloud to provide online gui",
    "path": "papers/24/03/2403.12690.json",
    "total_tokens": 748,
    "translated_title": "LNPT：无标签网络修剪与训练",
    "translated_abstract": "在训练之前修剪神经网络，使其能够部署在智能设备上。通过保留有助于泛化的权重，修剪后的网络可以在资源受限的智能设备上运行。我们提出了学习差距的概念，并强调它与泛化的准确相关性。实验表明，学习差距通过网络倒数第二层的特征图形式与泛化性能的变化相一致。我们提出了一种新的学习框架 LNPT，使得云端成熟网络能够提供在线指导。",
    "tldr": "本文介绍了LNPT，一种无标签网络修剪和训练的新框架，通过引入学习差距的概念，强调其准确相关性，以解决在智能设备上确定修剪结构的难题。",
    "en_tdlr": "This paper presents LNPT, a new framework for label-free network pruning and training, which introduces the concept of learning gap to accurately correlate with generalization, aiming to address the challenge of determining pruned structures on smart devices."
}