{
    "title": "Span-Based Optimal Sample Complexity for Weakly Communicating and General Average Reward MDPs",
    "abstract": "arXiv:2403.11477v1 Announce Type: new  Abstract: We study the sample complexity of learning an $\\epsilon$-optimal policy in an average-reward Markov decision process (MDP) under a generative model. For weakly communicating MDPs, we establish the complexity bound $\\tilde{O}(SA\\frac{H}{\\epsilon^2})$, where $H$ is the span of the bias function of the optimal policy and $SA$ is the cardinality of the state-action space. Our result is the first that is minimax optimal (up to log factors) in all parameters $S,A,H$ and $\\epsilon$, improving on existing work that either assumes uniformly bounded mixing times for all policies or has suboptimal dependence on the parameters. We further investigate sample complexity in general (non-weakly-communicating) average-reward MDPs. We argue a new transient time parameter $B$ is necessary, establish an $\\tilde{O}(SA\\frac{B+H}{\\epsilon^2})$ complexity bound, and prove a matching (up to log factors) minimax lower bound. Both results are based on reducing the",
    "link": "https://arxiv.org/abs/2403.11477",
    "context": "Title: Span-Based Optimal Sample Complexity for Weakly Communicating and General Average Reward MDPs\nAbstract: arXiv:2403.11477v1 Announce Type: new  Abstract: We study the sample complexity of learning an $\\epsilon$-optimal policy in an average-reward Markov decision process (MDP) under a generative model. For weakly communicating MDPs, we establish the complexity bound $\\tilde{O}(SA\\frac{H}{\\epsilon^2})$, where $H$ is the span of the bias function of the optimal policy and $SA$ is the cardinality of the state-action space. Our result is the first that is minimax optimal (up to log factors) in all parameters $S,A,H$ and $\\epsilon$, improving on existing work that either assumes uniformly bounded mixing times for all policies or has suboptimal dependence on the parameters. We further investigate sample complexity in general (non-weakly-communicating) average-reward MDPs. We argue a new transient time parameter $B$ is necessary, establish an $\\tilde{O}(SA\\frac{B+H}{\\epsilon^2})$ complexity bound, and prove a matching (up to log factors) minimax lower bound. Both results are based on reducing the",
    "path": "papers/24/03/2403.11477.json",
    "total_tokens": 978,
    "translated_title": "弱通信和一般平均奖赏MDPs的基于跨度的最佳样本复杂度",
    "translated_abstract": "我们研究了在生成模型下学习平均奖赏马尔可夫决策过程（MDP）中$\\epsilon$-最佳策略的样本复杂度。对于弱通信MDPs，我们建立了复杂度界限为$\\tilde{O}(SA\\frac{H}{\\epsilon^2})$，其中$H$是最优策略的偏差函数的跨度，$SA$是状态-动作空间的基数。我们的结果是在所有参数$S,A,H$和$\\epsilon$上（最多对数因子）最小最优的，改进了现有工作，现有工作要么假设所有策略的混合时间均匀有界，要么对参数有次优的依赖。我们进一步研究一般（非弱通信）平均奖赏MDPs中的样本复杂度。我们认为需要一个新的瞬态时间参数$B$，建立了一个$\\tilde{O}(SA\\frac{B+H}{\\epsilon^2})$的复杂度界限，并证明了匹配的（最多对数因子）最小最优下界。这两个结果都是基于减少",
    "tldr": "该研究提出了对于弱通信MDPs的样本复杂度界限为 $\\tilde{O}(SA\\frac{H}{\\epsilon^2})$，改进了现有工作，是在所有参数上最小最优的。",
    "en_tdlr": "This research establishes a sample complexity bound of $\\tilde{O}(SA\\frac{H}{\\epsilon^2})$ for weakly communicating MDPs, improving on existing work and being minimax optimal in all parameters."
}