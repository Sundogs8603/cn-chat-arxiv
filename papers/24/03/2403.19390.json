{
    "title": "Checkpoint Merging via Bayesian Optimization in LLM Pretraining",
    "abstract": "arXiv:2403.19390v1 Announce Type: new  Abstract: The rapid proliferation of large language models (LLMs) such as GPT-4 and Gemini underscores the intense demand for resources during their training processes, posing significant challenges due to substantial computational and environmental costs. To alleviate this issue, we propose checkpoint merging in pretraining LLM. This method utilizes LLM checkpoints with shared training trajectories, and is rooted in an extensive search space exploration for the best merging weight via Bayesian optimization. Through various experiments, we demonstrate that: (1) Our proposed methodology exhibits the capacity to augment pretraining, presenting an opportunity akin to obtaining substantial benefits at minimal cost; (2) Our proposed methodology, despite requiring a given held-out dataset, still demonstrates robust generalization capabilities across diverse domains, a pivotal aspect in pretraining.",
    "link": "https://arxiv.org/abs/2403.19390",
    "context": "Title: Checkpoint Merging via Bayesian Optimization in LLM Pretraining\nAbstract: arXiv:2403.19390v1 Announce Type: new  Abstract: The rapid proliferation of large language models (LLMs) such as GPT-4 and Gemini underscores the intense demand for resources during their training processes, posing significant challenges due to substantial computational and environmental costs. To alleviate this issue, we propose checkpoint merging in pretraining LLM. This method utilizes LLM checkpoints with shared training trajectories, and is rooted in an extensive search space exploration for the best merging weight via Bayesian optimization. Through various experiments, we demonstrate that: (1) Our proposed methodology exhibits the capacity to augment pretraining, presenting an opportunity akin to obtaining substantial benefits at minimal cost; (2) Our proposed methodology, despite requiring a given held-out dataset, still demonstrates robust generalization capabilities across diverse domains, a pivotal aspect in pretraining.",
    "path": "papers/24/03/2403.19390.json",
    "total_tokens": 826,
    "translated_title": "在LLM预训练中通过贝叶斯优化进行检查点合并",
    "translated_abstract": "大型语言模型（LLMs）如GPT-4和Gemini的迅速增长突显了在它们的训练过程中对资源的强烈需求，由于巨大的计算和环境成本，这提出了重大挑战。为了缓解这一问题，我们提出了LLM预训练中的检查点合并。该方法利用具有共享训练轨迹的LLM检查点，并通过贝叶斯优化对最佳合并权重进行广泛的搜索空间探索。通过各种实验，我们展示了：（1）我们提出的方法展示了增强预训练的能力，类似于在最小成本下获得重大收益的机会；（2）尽管我们提出的方法需要一个给定的保留数据集，但仍展示了跨多个领域的稳健泛化能力，这是预训练中的一个关键方面。",
    "tldr": "通过贝叶斯优化，我们提出了LLM预训练中的检查点合并方法，展现了在最小成本下增强预训练的能力以及在不同领域展示鲁棒泛化能力的特点。"
}