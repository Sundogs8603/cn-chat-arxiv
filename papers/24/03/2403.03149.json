{
    "title": "Robust Federated Learning Mitigates Client-side Training Data Distribution Inference Attacks",
    "abstract": "arXiv:2403.03149v1 Announce Type: cross  Abstract: Recent studies have revealed that federated learning (FL), once considered secure due to clients not sharing their private data with the server, is vulnerable to attacks such as client-side training data distribution inference, where a malicious client can recreate the victim's data. While various countermeasures exist, they are not practical, often assuming server access to some training data or knowledge of label distribution before the attack.   In this work, we bridge the gap by proposing InferGuard, a novel Byzantine-robust aggregation rule aimed at defending against client-side training data distribution inference attacks. In our proposed InferGuard, the server first calculates the coordinate-wise median of all the model updates it receives. A client's model update is considered malicious if it significantly deviates from the computed median update. We conduct a thorough evaluation of our proposed InferGuard on five benchmark dat",
    "link": "https://arxiv.org/abs/2403.03149",
    "context": "Title: Robust Federated Learning Mitigates Client-side Training Data Distribution Inference Attacks\nAbstract: arXiv:2403.03149v1 Announce Type: cross  Abstract: Recent studies have revealed that federated learning (FL), once considered secure due to clients not sharing their private data with the server, is vulnerable to attacks such as client-side training data distribution inference, where a malicious client can recreate the victim's data. While various countermeasures exist, they are not practical, often assuming server access to some training data or knowledge of label distribution before the attack.   In this work, we bridge the gap by proposing InferGuard, a novel Byzantine-robust aggregation rule aimed at defending against client-side training data distribution inference attacks. In our proposed InferGuard, the server first calculates the coordinate-wise median of all the model updates it receives. A client's model update is considered malicious if it significantly deviates from the computed median update. We conduct a thorough evaluation of our proposed InferGuard on five benchmark dat",
    "path": "papers/24/03/2403.03149.json",
    "total_tokens": 808,
    "translated_title": "强大的联邦学习方法缓解客户端训练数据分布推断攻击",
    "translated_abstract": "近期研究揭示了联邦学习（FL）曾被认为安全的漏洞，因为客户端不向服务器共享其私有数据，然而这种方法容易遭受诸如客户端训练数据分布推断攻击等攻击，此攻击可以让恶意客户端重现受害者的数据。本文提出了一种新颖的拜占庭-鲁棒聚合规则InferGuard，旨在防御客户端训练数据分布推断攻击。在我们提出的InferGuard中，服务器首先计算其收到的所有模型更新的坐标中位数。如果客户端的模型更新与计算出的中位数更新显著偏离，则视为恶意。我们对我们的InferGuard在五个基准数据集上进行了彻底评估。",
    "tldr": "本研究提出了一种新颖的拜占庭-鲁棒聚合规则InferGuard，用于防御客户端训练数据分布推断攻击。"
}