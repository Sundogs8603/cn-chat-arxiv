{
    "title": "Probabilistic Topic Modelling with Transformer Representations",
    "abstract": "arXiv:2403.03737v1 Announce Type: cross  Abstract: Topic modelling was mostly dominated by Bayesian graphical models during the last decade. With the rise of transformers in Natural Language Processing, however, several successful models that rely on straightforward clustering approaches in transformer-based embedding spaces have emerged and consolidated the notion of topics as clusters of embedding vectors. We propose the Transformer-Representation Neural Topic Model (TNTM), which combines the benefits of topic representations in transformer-based embedding spaces and probabilistic modelling. Therefore, this approach unifies the powerful and versatile notion of topics based on transformer embeddings with fully probabilistic modelling, as in models such as Latent Dirichlet Allocation (LDA). We utilize the variational autoencoder (VAE) framework for improved inference speed and modelling flexibility. Experimental results show that our proposed model achieves results on par with various ",
    "link": "https://arxiv.org/abs/2403.03737",
    "context": "Title: Probabilistic Topic Modelling with Transformer Representations\nAbstract: arXiv:2403.03737v1 Announce Type: cross  Abstract: Topic modelling was mostly dominated by Bayesian graphical models during the last decade. With the rise of transformers in Natural Language Processing, however, several successful models that rely on straightforward clustering approaches in transformer-based embedding spaces have emerged and consolidated the notion of topics as clusters of embedding vectors. We propose the Transformer-Representation Neural Topic Model (TNTM), which combines the benefits of topic representations in transformer-based embedding spaces and probabilistic modelling. Therefore, this approach unifies the powerful and versatile notion of topics based on transformer embeddings with fully probabilistic modelling, as in models such as Latent Dirichlet Allocation (LDA). We utilize the variational autoencoder (VAE) framework for improved inference speed and modelling flexibility. Experimental results show that our proposed model achieves results on par with various ",
    "path": "papers/24/03/2403.03737.json",
    "total_tokens": 814,
    "translated_title": "使用Transformer表示的概率主题建模",
    "translated_abstract": "主题建模在过去的十年中大多由贝叶斯图模型主导。然而，随着Transformer在自然语言处理中的兴起，一些依赖于transformer嵌入空间中简单聚类方法的成功模型已经出现并巩固了主题作为嵌入向量聚类的概念。我们提出了Transformer-Representation神经主题模型（TNTM），结合了transformer嵌入空间中主题表示的优势和概率建模。因此，这种方法将基于transformer嵌入的强大多功能主题概念与完全概率建模统一起来，如Latent Dirichlet Allocation（LDA）等模型。我们利用变分自动编码器（VAE）框架改进推理速度和建模灵活性。实验结果显示我们提出的模型与各种模型达到了类似的结果。",
    "tldr": "提出了Transformer-Representation神经主题模型（TNTM），结合了transformer嵌入空间中主题表示的优势和概率建模以及变分自动编码器（VAE）框架，实现了主题建模的强大和多功能性",
    "en_tdlr": "Introduced the Transformer-Representation Neural Topic Model (TNTM), which combines the advantages of topic representations in transformer-based embedding spaces, probabilistic modeling, and the variational autoencoder (VAE) framework, achieving powerful and versatile topic modeling."
}