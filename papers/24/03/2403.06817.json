{
    "title": "Are Targeted Messages More Effective?",
    "abstract": "arXiv:2403.06817v1 Announce Type: cross  Abstract: Graph neural networks (GNN) are deep learning architectures for graphs. Essentially, a GNN is a distributed message passing algorithm, which is controlled by parameters learned from data. It operates on the vertices of a graph: in each iteration, vertices receive a message on each incoming edge, aggregate these messages, and then update their state based on their current state and the aggregated messages. The expressivity of GNNs can be characterised in terms of certain fragments of first-order logic with counting and the Weisfeiler-Lehman algorithm.   The core GNN architecture comes in two different versions. In the first version, a message only depends on the state of the source vertex, whereas in the second version it depends on the states of the source and target vertices. In practice, both of these versions are used, but the theory of GNNs so far mostly focused on the first one. On the logical side, the two versions correspond to ",
    "link": "https://arxiv.org/abs/2403.06817",
    "context": "Title: Are Targeted Messages More Effective?\nAbstract: arXiv:2403.06817v1 Announce Type: cross  Abstract: Graph neural networks (GNN) are deep learning architectures for graphs. Essentially, a GNN is a distributed message passing algorithm, which is controlled by parameters learned from data. It operates on the vertices of a graph: in each iteration, vertices receive a message on each incoming edge, aggregate these messages, and then update their state based on their current state and the aggregated messages. The expressivity of GNNs can be characterised in terms of certain fragments of first-order logic with counting and the Weisfeiler-Lehman algorithm.   The core GNN architecture comes in two different versions. In the first version, a message only depends on the state of the source vertex, whereas in the second version it depends on the states of the source and target vertices. In practice, both of these versions are used, but the theory of GNNs so far mostly focused on the first one. On the logical side, the two versions correspond to ",
    "path": "papers/24/03/2403.06817.json",
    "total_tokens": 834,
    "translated_title": "目标信息更有效吗？",
    "translated_abstract": "图神经网络（GNN）是用于图形的深度学习架构。本质上，GNN是一个分布式的消息传递算法，其受到从数据中学到的参数的控制。它在图的顶点上操作：在每次迭代中，顶点在每个传入边上接收一条消息，聚合这些消息，然后根据它们当前的状态和聚合的消息更新它们的状态。GNN的表达能力可以用带计数的一阶逻辑的某些片段和Weisfeiler-Lehman算法来描述。GNN的核心架构有两个不同的版本。在第一个版本中，消息仅取决于源顶点的状态，而在第二个版本中，消息取决于源顶点和目标顶点的状态。实际上，这两个版本都被使用，但迄今为止GNN的理论大多集中在第一个版本上。在逻辑方面，这两个版本对应着",
    "tldr": "GNN的核心架构有两个版本，第一个版本消息仅取决于源顶点的状态，而第二个版本消息取决于源顶点和目标顶点的状态。",
    "en_tdlr": "The core architecture of GNN has two versions, with the first version of message dependency based solely on the state of the source vertex, and the second version dependent on the states of both the source and target vertices."
}