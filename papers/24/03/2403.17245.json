{
    "title": "SPLICE: A Singleton-Enhanced PipeLIne for Coreference REsolution",
    "abstract": "arXiv:2403.17245v1 Announce Type: new  Abstract: Singleton mentions, i.e.~entities mentioned only once in a text, are important to how humans understand discourse from a theoretical perspective. However previous attempts to incorporate their detection in end-to-end neural coreference resolution for English have been hampered by the lack of singleton mention spans in the OntoNotes benchmark. This paper addresses this limitation by combining predicted mentions from existing nested NER systems and features derived from OntoNotes syntax trees. With this approach, we create a near approximation of the OntoNotes dataset with all singleton mentions, achieving ~94% recall on a sample of gold singletons. We then propose a two-step neural mention and coreference resolution system, named SPLICE, and compare its performance to the end-to-end approach in two scenarios: the OntoNotes test set and the out-of-domain (OOD) OntoGUM corpus. Results indicate that reconstructed singleton training yields re",
    "link": "https://arxiv.org/abs/2403.17245",
    "context": "Title: SPLICE: A Singleton-Enhanced PipeLIne for Coreference REsolution\nAbstract: arXiv:2403.17245v1 Announce Type: new  Abstract: Singleton mentions, i.e.~entities mentioned only once in a text, are important to how humans understand discourse from a theoretical perspective. However previous attempts to incorporate their detection in end-to-end neural coreference resolution for English have been hampered by the lack of singleton mention spans in the OntoNotes benchmark. This paper addresses this limitation by combining predicted mentions from existing nested NER systems and features derived from OntoNotes syntax trees. With this approach, we create a near approximation of the OntoNotes dataset with all singleton mentions, achieving ~94% recall on a sample of gold singletons. We then propose a two-step neural mention and coreference resolution system, named SPLICE, and compare its performance to the end-to-end approach in two scenarios: the OntoNotes test set and the out-of-domain (OOD) OntoGUM corpus. Results indicate that reconstructed singleton training yields re",
    "path": "papers/24/03/2403.17245.json",
    "total_tokens": 995,
    "translated_title": "SPLICE：单例增强管道用于指代消解",
    "translated_abstract": "arXiv:2403.17245v1  公告类型：新的  摘要：单例提及，即文本中仅被提及一次的实体，从理论角度来看对人类理解话语很重要。然而，以往将其检测纳入用于英语端到端神经指代消解的尝试受到了OntoNotes基准中单例提及跨度不足的限制。本文通过将现有的嵌套NER系统的预测提及与从OntoNotes句法树导出的特征相结合来解决这一限制。借助这种方法，我们创建了一个近似包含所有单例提及的OntoNotes数据集，对金标准单例样本达到了约94%的召回率。然后，我们提出了一个名为SPLICE的两步神经提及和指代消解系统，并将其性能与端到端方法在两种情景下进行了比较：OntoNotes测试集和域外（OOD）OntoGUM语料库。结果表明，重建的单例训练产生了较高的效果。",
    "tldr": "本文通过结合现有的嵌套NER系统的预测提及和从OntoNotes句法树导出的特征，解决了在英语端到端神经指代消解中使用OntoNotes基准时单例提及跨度不足的问题，提出了一个名为SPLICE的两步神经提及和指代消解系统，并在OntoNotes测试集和域外OntoGUM语料库上对其性能进行了比较，结果表明重建的单例训练效果良好。",
    "en_tdlr": "This paper addresses the issue of insufficient singleton mention spans in end-to-end neural coreference resolution for English by combining predicted mentions from nested NER systems and features derived from OntoNotes syntax trees, introducing a two-step neural mention and coreference resolution system called SPLICE, and comparing its performance in different scenarios. The reconstructed singleton training shows promising results."
}