{
    "title": "CaT: Constraints as Terminations for Legged Locomotion Reinforcement Learning",
    "abstract": "arXiv:2403.18765v1 Announce Type: cross  Abstract: Deep Reinforcement Learning (RL) has demonstrated impressive results in solving complex robotic tasks such as quadruped locomotion. Yet, current solvers fail to produce efficient policies respecting hard constraints. In this work, we advocate for integrating constraints into robot learning and present Constraints as Terminations (CaT), a novel constrained RL algorithm. Departing from classical constrained RL formulations, we reformulate constraints through stochastic terminations during policy learning: any violation of a constraint triggers a probability of terminating potential future rewards the RL agent could attain. We propose an algorithmic approach to this formulation, by minimally modifying widely used off-the-shelf RL algorithms in robot learning (such as Proximal Policy Optimization). Our approach leads to excellent constraint adherence without introducing undue complexity and computational overhead, thus mitigating barriers ",
    "link": "https://arxiv.org/abs/2403.18765",
    "context": "Title: CaT: Constraints as Terminations for Legged Locomotion Reinforcement Learning\nAbstract: arXiv:2403.18765v1 Announce Type: cross  Abstract: Deep Reinforcement Learning (RL) has demonstrated impressive results in solving complex robotic tasks such as quadruped locomotion. Yet, current solvers fail to produce efficient policies respecting hard constraints. In this work, we advocate for integrating constraints into robot learning and present Constraints as Terminations (CaT), a novel constrained RL algorithm. Departing from classical constrained RL formulations, we reformulate constraints through stochastic terminations during policy learning: any violation of a constraint triggers a probability of terminating potential future rewards the RL agent could attain. We propose an algorithmic approach to this formulation, by minimally modifying widely used off-the-shelf RL algorithms in robot learning (such as Proximal Policy Optimization). Our approach leads to excellent constraint adherence without introducing undue complexity and computational overhead, thus mitigating barriers ",
    "path": "papers/24/03/2403.18765.json",
    "total_tokens": 845,
    "translated_title": "CaT: 约束作为四足行走强化学习的终结",
    "translated_abstract": "深度强化学习（RL）在解决复杂的机器人任务（如四足动态）方面取得了令人印象深刻的结果。然而，当前的解算器未能产生遵守严格约束的有效策略。在这项工作中，我们主张将约束集成到机器人学习中，并提出了约束作为终结（CaT），这是一种新颖的受限RL算法。我们离开传统的受限RL公式，通过策略学习中的随机终结来重新制定约束：任何约束违规都会触发RL代理可以获得潜在未来奖励的终结概率。我们提出了这种公式的算法方法，通过对机器人学习中广泛使用的现成RL算法（如近端策略优化）进行最小化修改。我们的方法在不引入不必要的复杂性和计算开销的情况下，导致出色的约束依从性，从而减轻了障碍。",
    "tldr": "CaT是一种新颖的受限RL算法，通过将约束集成到机器人学习中，通过随机终结的方式重新制定约束，达到了出色的约束依从性。",
    "en_tdlr": "CaT is a novel constrained RL algorithm that integrates constraints into robot learning by reformulating constraints through stochastic terminations, achieving excellent constraint adherence."
}