{
    "title": "Multi-modal Semantic Understanding with Contrastive Cross-modal Feature Alignment",
    "abstract": "arXiv:2403.06355v1 Announce Type: new  Abstract: Multi-modal semantic understanding requires integrating information from different modalities to extract users' real intention behind words. Most previous work applies a dual-encoder structure to separately encode image and text, but fails to learn cross-modal feature alignment, making it hard to achieve cross-modal deep information interaction. This paper proposes a novel CLIP-guided contrastive-learning-based architecture to perform multi-modal feature alignment, which projects the features derived from different modalities into a unified deep space. On multi-modal sarcasm detection (MMSD) and multi-modal sentiment analysis (MMSA) tasks, the experimental results show that our proposed model significantly outperforms several baselines, and our feature alignment strategy brings obvious performance gain over models with different aggregating methods and models even enriched with knowledge. More importantly, our model is simple to implemen",
    "link": "https://arxiv.org/abs/2403.06355",
    "context": "Title: Multi-modal Semantic Understanding with Contrastive Cross-modal Feature Alignment\nAbstract: arXiv:2403.06355v1 Announce Type: new  Abstract: Multi-modal semantic understanding requires integrating information from different modalities to extract users' real intention behind words. Most previous work applies a dual-encoder structure to separately encode image and text, but fails to learn cross-modal feature alignment, making it hard to achieve cross-modal deep information interaction. This paper proposes a novel CLIP-guided contrastive-learning-based architecture to perform multi-modal feature alignment, which projects the features derived from different modalities into a unified deep space. On multi-modal sarcasm detection (MMSD) and multi-modal sentiment analysis (MMSA) tasks, the experimental results show that our proposed model significantly outperforms several baselines, and our feature alignment strategy brings obvious performance gain over models with different aggregating methods and models even enriched with knowledge. More importantly, our model is simple to implemen",
    "path": "papers/24/03/2403.06355.json",
    "total_tokens": 884,
    "translated_title": "具有对比交叉模态特征对齐的多模态语义理解",
    "translated_abstract": "arXiv:2403.06355v1 公告类型: 新摘要: 多模态语义理解需要整合来自不同模态的信息，以提取用户言辞背后的真实意图。大多数先前的工作应用双编码器结构分别对图像和文本进行编码，但未能学习跨模态特征对齐，这使得难以实现跨模态的深度信息交互。本文提出了一种新颖的以CLIP为引导的基于对比学习的架构，以执行多模态特征对齐，将来自不同模态的特征投影到统一的深度空间中。在多模态讽刺检测（MMSD）和多模态情感分析（MMSA）任务上，实验结果表明我们提出的模型显著优于几个基线模型，并且我们的特征对齐策略比具有不同聚合方法的模型和甚至富含知识的模型带来明显的性能提升。更重要的是，我们的模型实现简单。",
    "tldr": "提出了一种以CLIP为引导的对比学习架构，实现了多模态特征对齐，在多模态任务上表现显著优于多个基线模型，并展示了对模型性能提升的明显作用。"
}