{
    "title": "J-CRe3: A Japanese Conversation Dataset for Real-world Reference Resolution",
    "abstract": "arXiv:2403.19259v1 Announce Type: new  Abstract: Understanding expressions that refer to the physical world is crucial for such human-assisting systems in the real world, as robots that must perform actions that are expected by users. In real-world reference resolution, a system must ground the verbal information that appears in user interactions to the visual information observed in egocentric views. To this end, we propose a multimodal reference resolution task and construct a Japanese Conversation dataset for Real-world Reference Resolution (J-CRe3). Our dataset contains egocentric video and dialogue audio of real-world conversations between two people acting as a master and an assistant robot at home. The dataset is annotated with crossmodal tags between phrases in the utterances and the object bounding boxes in the video frames. These tags include indirect reference relations, such as predicate-argument structures and bridging references as well as direct reference relations. We a",
    "link": "https://arxiv.org/abs/2403.19259",
    "context": "Title: J-CRe3: A Japanese Conversation Dataset for Real-world Reference Resolution\nAbstract: arXiv:2403.19259v1 Announce Type: new  Abstract: Understanding expressions that refer to the physical world is crucial for such human-assisting systems in the real world, as robots that must perform actions that are expected by users. In real-world reference resolution, a system must ground the verbal information that appears in user interactions to the visual information observed in egocentric views. To this end, we propose a multimodal reference resolution task and construct a Japanese Conversation dataset for Real-world Reference Resolution (J-CRe3). Our dataset contains egocentric video and dialogue audio of real-world conversations between two people acting as a master and an assistant robot at home. The dataset is annotated with crossmodal tags between phrases in the utterances and the object bounding boxes in the video frames. These tags include indirect reference relations, such as predicate-argument structures and bridging references as well as direct reference relations. We a",
    "path": "papers/24/03/2403.19259.json",
    "total_tokens": 876,
    "translated_title": "J-CRe3：日本对话数据集用于现实世界参考解析",
    "translated_abstract": "arXiv:2403.19259v1 公告类型：新的 摘要：理解指代物理世界的表达对于在真实世界中执行用户期望动作的机器人等助人系统至关重要。在现实世界的参考解析中，系统必须将用户交互中出现的语言信息与自我中心视图中观察到的视觉信息进行关联。为此，我们提出了一项多模式参考解析任务，并构建了一个用于现实世界参考解析的日语对话数据集（J-CRe3）。我们的数据集包含在家中两个人扮演主人和助理机器人之间的真实对话的自我中心视频和对话音频。该数据集带有话语中短语和视频帧中物体边界框之间的跨模态标记。这些标记包括间接参考关系，例如谓语-论元结构和桥接引用，以及直接引用关系。",
    "tldr": "我们提出了一个多模式参考解析任务，并构建了日本对话数据集J-CRe3，用于现实世界参考解析，其中包含了自我中心视频和对话音频，以及短语和视频帧中物体边界框之间的跨模态标记。",
    "en_tdlr": "We propose a multimodal reference resolution task and construct a Japanese Conversation dataset J-CRe3 for real-world reference resolution, which includes egocentric video, dialogue audio, and crossmodal tags between phrases in the utterances and the object bounding boxes in the video frames."
}