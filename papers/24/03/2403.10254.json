{
    "title": "Magic Tokens: Select Diverse Tokens for Multi-modal Object Re-Identification",
    "abstract": "arXiv:2403.10254v1 Announce Type: cross  Abstract: Single-modal object re-identification (ReID) faces great challenges in maintaining robustness within complex visual scenarios. In contrast, multi-modal object ReID utilizes complementary information from diverse modalities, showing great potentials for practical applications. However, previous methods may be easily affected by irrelevant backgrounds and usually ignore the modality gaps. To address above issues, we propose a novel learning framework named \\textbf{EDITOR} to select diverse tokens from vision Transformers for multi-modal object ReID. We begin with a shared vision Transformer to extract tokenized features from different input modalities. Then, we introduce a Spatial-Frequency Token Selection (SFTS) module to adaptively select object-centric tokens with both spatial and frequency information. Afterwards, we employ a Hierarchical Masked Aggregation (HMA) module to facilitate feature interactions within and across modalities.",
    "link": "https://arxiv.org/abs/2403.10254",
    "context": "Title: Magic Tokens: Select Diverse Tokens for Multi-modal Object Re-Identification\nAbstract: arXiv:2403.10254v1 Announce Type: cross  Abstract: Single-modal object re-identification (ReID) faces great challenges in maintaining robustness within complex visual scenarios. In contrast, multi-modal object ReID utilizes complementary information from diverse modalities, showing great potentials for practical applications. However, previous methods may be easily affected by irrelevant backgrounds and usually ignore the modality gaps. To address above issues, we propose a novel learning framework named \\textbf{EDITOR} to select diverse tokens from vision Transformers for multi-modal object ReID. We begin with a shared vision Transformer to extract tokenized features from different input modalities. Then, we introduce a Spatial-Frequency Token Selection (SFTS) module to adaptively select object-centric tokens with both spatial and frequency information. Afterwards, we employ a Hierarchical Masked Aggregation (HMA) module to facilitate feature interactions within and across modalities.",
    "path": "papers/24/03/2403.10254.json",
    "total_tokens": 963,
    "translated_title": "魔法令牌：为多模态物体再识别选择多样化令牌",
    "translated_abstract": "单模态物体再识别（ReID）在复杂视觉场景中保持稳健性面临巨大挑战。相比之下，多模态物体ReID利用来自多样化模态的互补信息，显示出在实际应用中具有巨大潜力。然而，先前的方法可能容易受到无关背景的影响，并通常忽视模态之间的差距。为了解决上述问题，我们提出了一个名为\\textbf{EDITOR}的新型学习框架，用于为多模态物体ReID选择来自视觉Transformer的多样化令牌。我们从一个共享的视觉Transformer开始，从不同的输入模态中提取令牌化特征。然后，我们引入了一个名为空间频率令牌选择（SFTS）模块，自适应地选择具有空间和频率信息的以对象为中心的令牌。随后，我们采用了一个名为分层掩码聚合（HMA）模块，促进跨模态内部和之间的特征交互。",
    "tldr": "提出了一种名为EDITOR的学习框架，用于为多模态物体再识别选择来自视觉Transformer的多样化令牌，通过Spatial-Frequency Token Selection（SFTS）模块自适应选择以对象为中心的令牌，同时通过Hierarchical Masked Aggregation（HMA）模块促进跨模态内部和之间的特征交互",
    "en_tdlr": "Introduced a learning framework named EDITOR to select diverse tokens from vision Transformers for multi-modal object re-identification, adaptively selecting object-centric tokens with spatial and frequency information through the Spatial-Frequency Token Selection (SFTS) module, and facilitating feature interactions within and across modalities with the Hierarchical Masked Aggregation (HMA) module."
}