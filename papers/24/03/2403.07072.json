{
    "title": "Explainable Learning with Gaussian Processes",
    "abstract": "arXiv:2403.07072v1 Announce Type: new  Abstract: The field of explainable artificial intelligence (XAI) attempts to develop methods that provide insight into how complicated machine learning methods make predictions. Many methods of explanation have focused on the concept of feature attribution, a decomposition of the model's prediction into individual contributions corresponding to each input feature. In this work, we explore the problem of feature attribution in the context of Gaussian process regression (GPR). We take a principled approach to defining attributions under model uncertainty, extending the existing literature. We show that although GPR is a highly flexible and non-parametric approach, we can derive interpretable, closed-form expressions for the feature attributions. When using integrated gradients as an attribution method, we show that the attributions of a GPR model also follow a Gaussian process distribution, which quantifies the uncertainty in attribution arising fro",
    "link": "https://arxiv.org/abs/2403.07072",
    "context": "Title: Explainable Learning with Gaussian Processes\nAbstract: arXiv:2403.07072v1 Announce Type: new  Abstract: The field of explainable artificial intelligence (XAI) attempts to develop methods that provide insight into how complicated machine learning methods make predictions. Many methods of explanation have focused on the concept of feature attribution, a decomposition of the model's prediction into individual contributions corresponding to each input feature. In this work, we explore the problem of feature attribution in the context of Gaussian process regression (GPR). We take a principled approach to defining attributions under model uncertainty, extending the existing literature. We show that although GPR is a highly flexible and non-parametric approach, we can derive interpretable, closed-form expressions for the feature attributions. When using integrated gradients as an attribution method, we show that the attributions of a GPR model also follow a Gaussian process distribution, which quantifies the uncertainty in attribution arising fro",
    "path": "papers/24/03/2403.07072.json",
    "total_tokens": 863,
    "translated_title": "使用高斯过程进行可解释学习",
    "translated_abstract": "人工智能领域中的可解释性研究旨在开发能够揭示复杂机器学习模型进行预测的方法。很多解释方法都集中在特征归因的概念上，即将模型的预测分解为对应于每个输入特征的个体贡献。在这项工作中，我们探讨了在高斯过程回归（GPR）背景下的特征归因问题。我们采取了一种基于模型不确定性的原则性方法来定义特征归因，从而扩展了现有的文献。我们展示了，尽管GPR是一种高度灵活和非参数化的方法，我们可以推导出对特征归因的可解释的闭式表达式。当使用集成梯度作为归因方法时，我们展示了GPR模型的归因也遵循一个高斯过程分布，用以量化归因中出现的不确定性。",
    "tldr": "本工作探讨了在高斯过程回归中的特征归因问题，提出了一种基于模型不确定性的定义方法，得到了可解释的特征归因的闭式表达式，同时展示了GPR模型归因也遵循高斯过程分布。",
    "en_tdlr": "This work discusses the problem of feature attribution in Gaussian process regression, introduces a principled approach based on model uncertainty for defining attributions, derives interpretable closed-form expressions for feature attributions, and demonstrates that attributions of GPR models follow a Gaussian process distribution."
}