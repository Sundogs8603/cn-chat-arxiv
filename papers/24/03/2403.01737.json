{
    "title": "Deep Horseshoe Gaussian Processes",
    "abstract": "arXiv:2403.01737v1 Announce Type: cross  Abstract: Deep Gaussian processes have recently been proposed as natural objects to fit, similarly to deep neural networks, possibly complex features present in modern data samples, such as compositional structures. Adopting a Bayesian nonparametric approach, it is natural to use deep Gaussian processes as prior distributions, and use the corresponding posterior distributions for statistical inference. We introduce the deep Horseshoe Gaussian process Deep-HGP, a new simple prior based on deep Gaussian processes with a squared-exponential kernel, that in particular enables data-driven choices of the key lengthscale parameters. For nonparametric regression with random design, we show that the associated tempered posterior distribution recovers the unknown true regression curve optimally in terms of quadratic loss, up to a logarithmic factor, in an adaptive way. The convergence rates are simultaneously adaptive to both the smoothness of the regress",
    "link": "https://arxiv.org/abs/2403.01737",
    "context": "Title: Deep Horseshoe Gaussian Processes\nAbstract: arXiv:2403.01737v1 Announce Type: cross  Abstract: Deep Gaussian processes have recently been proposed as natural objects to fit, similarly to deep neural networks, possibly complex features present in modern data samples, such as compositional structures. Adopting a Bayesian nonparametric approach, it is natural to use deep Gaussian processes as prior distributions, and use the corresponding posterior distributions for statistical inference. We introduce the deep Horseshoe Gaussian process Deep-HGP, a new simple prior based on deep Gaussian processes with a squared-exponential kernel, that in particular enables data-driven choices of the key lengthscale parameters. For nonparametric regression with random design, we show that the associated tempered posterior distribution recovers the unknown true regression curve optimally in terms of quadratic loss, up to a logarithmic factor, in an adaptive way. The convergence rates are simultaneously adaptive to both the smoothness of the regress",
    "path": "papers/24/03/2403.01737.json",
    "total_tokens": 863,
    "translated_title": "深马蹄高斯过程",
    "translated_abstract": "最近提出深高斯过程作为一种自然对象，类似于深度神经网络，可能拟合现代数据样本中存在的复杂特征，如组合结构。采用贝叶斯非参数方法，自然地利用深高斯过程作为先验分布，并将相应的后验分布用于统计推断。我们介绍了深马蹄高斯过程Deep-HGP，这是一种基于带有平方指数核的深高斯过程的新简单先验，特别是使得可以对关键长度尺度参数进行数据驱动选择。对于随机设计的非参数回归，我们展示了相应的调节后验分布以一种自适应方式，最优地在二次损失的意义下恢复未知的真回归曲线，最多只有一个对数因子。收敛速率同时对回归的平滑度和设计维度自适应。",
    "tldr": "深马蹄高斯过程Deep-HGP是一种简单的先验，采用深高斯过程并允许数据驱动选择关键长度尺度参数，对于非参数回归表现出良好的性能，实现了对未知真实回归曲线的优化回复，具有自适应的收敛速率。"
}