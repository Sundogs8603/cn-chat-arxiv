{
    "title": "Belief-Enriched Pessimistic Q-Learning against Adversarial State Perturbations",
    "abstract": "arXiv:2403.04050v1 Announce Type: new  Abstract: Reinforcement learning (RL) has achieved phenomenal success in various domains. However, its data-driven nature also introduces new vulnerabilities that can be exploited by malicious opponents. Recent work shows that a well-trained RL agent can be easily manipulated by strategically perturbing its state observations at the test stage. Existing solutions either introduce a regularization term to improve the smoothness of the trained policy against perturbations or alternatively train the agent's policy and the attacker's policy. However, the former does not provide sufficient protection against strong attacks, while the latter is computationally prohibitive for large environments. In this work, we propose a new robust RL algorithm for deriving a pessimistic policy to safeguard against an agent's uncertainty about true states. This approach is further enhanced with belief state inference and diffusion-based state purification to reduce unc",
    "link": "https://arxiv.org/abs/2403.04050",
    "context": "Title: Belief-Enriched Pessimistic Q-Learning against Adversarial State Perturbations\nAbstract: arXiv:2403.04050v1 Announce Type: new  Abstract: Reinforcement learning (RL) has achieved phenomenal success in various domains. However, its data-driven nature also introduces new vulnerabilities that can be exploited by malicious opponents. Recent work shows that a well-trained RL agent can be easily manipulated by strategically perturbing its state observations at the test stage. Existing solutions either introduce a regularization term to improve the smoothness of the trained policy against perturbations or alternatively train the agent's policy and the attacker's policy. However, the former does not provide sufficient protection against strong attacks, while the latter is computationally prohibitive for large environments. In this work, we propose a new robust RL algorithm for deriving a pessimistic policy to safeguard against an agent's uncertainty about true states. This approach is further enhanced with belief state inference and diffusion-based state purification to reduce unc",
    "path": "papers/24/03/2403.04050.json",
    "total_tokens": 887,
    "translated_title": "基于信念丰富的悲观Q学习抵抗对抗性状态扰动",
    "translated_abstract": "强化学习在各个领域取得了巨大成功。然而，其数据驱动的特性也引入了新的漏洞，可以被恶意对手利用。最近的研究表明，通过在测试阶段有策略地扰乱其状态观察，一个训练良好的RL代理很容易被操纵。现有解决方案要么引入正则化项以改善受扰动影响训练策略的平滑性，要么分别训练代理的策略和攻击者的策略。然而，前者不能提供足够的保护来抵御强攻击，而后者对于大环境而言在计算上是禁止的。在本文中，我们提出了一种新的鲁棒RL算法，用于推导出一种悲观策略，以防范代理对真实状态的不确定性。这种方法进一步结合了信念状态推理和基于扩散的状态净化，以减少不确定性。",
    "tldr": "提出了一种新的鲁棒RL算法，通过引入信念状态推理和基于扩散的状态净化，推导出一种悲观策略，以对抗代理对真实状态的不确定性。",
    "en_tdlr": "Introducing a new robust RL algorithm that derives a pessimistic policy to safeguard against an agent's uncertainty about true states, enhanced with belief state inference and diffusion-based state purification."
}