{
    "title": "SLCF-Net: Sequential LiDAR-Camera Fusion for Semantic Scene Completion using a 3D Recurrent U-Net",
    "abstract": "arXiv:2403.08885v1 Announce Type: cross  Abstract: We introduce SLCF-Net, a novel approach for the Semantic Scene Completion (SSC) task that sequentially fuses LiDAR and camera data. It jointly estimates missing geometry and semantics in a scene from sequences of RGB images and sparse LiDAR measurements. The images are semantically segmented by a pre-trained 2D U-Net and a dense depth prior is estimated from a depth-conditioned pipeline fueled by Depth Anything. To associate the 2D image features with the 3D scene volume, we introduce Gaussian-decay Depth-prior Projection (GDP). This module projects the 2D features into the 3D volume along the line of sight with a Gaussian-decay function, centered around the depth prior. Volumetric semantics is computed by a 3D U-Net. We propagate the hidden 3D U-Net state using the sensor motion and design a novel loss to ensure temporal consistency. We evaluate our approach on the SemanticKITTI dataset and compare it with leading SSC approaches. The ",
    "link": "https://arxiv.org/abs/2403.08885",
    "context": "Title: SLCF-Net: Sequential LiDAR-Camera Fusion for Semantic Scene Completion using a 3D Recurrent U-Net\nAbstract: arXiv:2403.08885v1 Announce Type: cross  Abstract: We introduce SLCF-Net, a novel approach for the Semantic Scene Completion (SSC) task that sequentially fuses LiDAR and camera data. It jointly estimates missing geometry and semantics in a scene from sequences of RGB images and sparse LiDAR measurements. The images are semantically segmented by a pre-trained 2D U-Net and a dense depth prior is estimated from a depth-conditioned pipeline fueled by Depth Anything. To associate the 2D image features with the 3D scene volume, we introduce Gaussian-decay Depth-prior Projection (GDP). This module projects the 2D features into the 3D volume along the line of sight with a Gaussian-decay function, centered around the depth prior. Volumetric semantics is computed by a 3D U-Net. We propagate the hidden 3D U-Net state using the sensor motion and design a novel loss to ensure temporal consistency. We evaluate our approach on the SemanticKITTI dataset and compare it with leading SSC approaches. The ",
    "path": "papers/24/03/2403.08885.json",
    "total_tokens": 1045,
    "translated_title": "SLCF-Net：使用3D循环U-Net进行序列式激光雷达-相机融合的语义场景完善",
    "translated_abstract": "我们引入了SLCF-Net，一种新颖的方法用于语义场景完善（SSC）任务，通过序列融合激光雷达和相机数据。它通过RGB图像序列和稀疏激光雷达测量数据联合估计场景中缺失的几何和语义信息。图像经过经过预训练的2D U-Net进行语义分割，并从Depth Anything提供的深度条件管线中估计出密集深度先验。为了将2D图像特征与3D场景体积关联起来，我们引入了高斯衰减深度先验投影（GDP）。该模块使用高斯衰减函数沿着以深度先验为中心的视线将2D特征投影到3D体积中。体积语义由3D U-Net计算。我们利用传感器运动传播隐藏的3D U-Net状态，并设计了一种新颖的损失函数来确保时间一致性。我们在SemanticKITTI数据集上评估了我们的方法，并将其与领先的SSC方法进行了比较。",
    "tldr": "SLCF-Net是一种用于语义场景完善任务的新方法，通过序列融合激光雷达和相机数据，联合估计缺失的几何和语义信息，并引入了高斯衰减深度先验投影模块以实现2D图像特征和3D场景体积的关联，同时设计了一种新颖的损失函数来确保时间一致性。",
    "en_tdlr": "SLCF-Net is a novel method for the Semantic Scene Completion task, which sequentially fuses LiDAR and camera data, jointly estimates missing geometry and semantics, introduces a Gaussian-decay depth-prior projection module to associate 2D image features with 3D scene volume, and designs a novel loss function to ensure temporal consistency."
}