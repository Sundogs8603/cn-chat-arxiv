{
    "title": "LLMs Instruct LLMs:An Extraction and Editing Method",
    "abstract": "arXiv:2403.15736v1 Announce Type: new  Abstract: The interest in updating Large Language Models (LLMs) without retraining from scratch is substantial, yet it comes with some challenges.This is especially true for situations demanding complex reasoning with limited samples, a scenario we refer to as the Paucity-Constrained Complex Reasoning Adaptation for LLMs (PCRA-LLM).Traditional methods like Low-Rank Adaptation (LoRA) and Retrieval-Augmented Generation (RAG) are inadequate for this critical issue, particularly evident in our exploration of a specific medical context that epitomize the PCRA-LLM's distinct needs.To address the issue, we propose a Sequential Fusion method to incorporate knowledge from complex context into LLMs. This method employs a two-stage framework: initially, it leverages general LLMs to construct knowledge graphs (KGs) for extracting knowledge from complex texts; subsequently, it updates the domain LLMs through knowledge edit. According to our method, the domain ",
    "link": "https://arxiv.org/abs/2403.15736",
    "context": "Title: LLMs Instruct LLMs:An Extraction and Editing Method\nAbstract: arXiv:2403.15736v1 Announce Type: new  Abstract: The interest in updating Large Language Models (LLMs) without retraining from scratch is substantial, yet it comes with some challenges.This is especially true for situations demanding complex reasoning with limited samples, a scenario we refer to as the Paucity-Constrained Complex Reasoning Adaptation for LLMs (PCRA-LLM).Traditional methods like Low-Rank Adaptation (LoRA) and Retrieval-Augmented Generation (RAG) are inadequate for this critical issue, particularly evident in our exploration of a specific medical context that epitomize the PCRA-LLM's distinct needs.To address the issue, we propose a Sequential Fusion method to incorporate knowledge from complex context into LLMs. This method employs a two-stage framework: initially, it leverages general LLMs to construct knowledge graphs (KGs) for extracting knowledge from complex texts; subsequently, it updates the domain LLMs through knowledge edit. According to our method, the domain ",
    "path": "papers/24/03/2403.15736.json",
    "total_tokens": 866,
    "translated_title": "LLMs指导LLMs：一种提取和编辑方法",
    "translated_abstract": "arXiv:2403.15736v1 公告类型：新 兴趣点在于无需从头开始训练即可更新大型语言模型（LLMs），但是这也带来了一些挑战。尤其是对于需要用有限样本进行复杂推理的情况来说，我们称之为适用于LLMs的贫乏约束复杂推理（PCRA-LLM）的情况。传统方法如低秩适应（LoRA）和检索增强生成（RAG）对这一关键问题是不足够的，尤其在我们探索特定医学背景时尤为明显，这体现了PCRA-LLM的独特需求。为了解决这个问题，我们提出了一种顺序融合方法，将复杂环境中的知识融入LLMs中。该方法采用两阶段框架：首先，利用通用LLMs构建知识图谱（KGs）来从复杂文本中提取知识；随后，通过知识编辑来更新领域LLMs。根据我们的方法，领域",
    "tldr": "提出了一种顺序融合方法，将复杂环境中的知识融入LLMs中，用于更新大型语言模型。",
    "en_tdlr": "Propose a Sequential Fusion method to incorporate knowledge from complex context into LLMs for updating large language models."
}