{
    "title": "Improving Uncertainty Sampling with Bell Curve Weight Function",
    "abstract": "arXiv:2403.01352v1 Announce Type: new  Abstract: Typically, a supervised learning model is trained using passive learning by randomly selecting unlabelled instances to annotate. This approach is effective for learning a model, but can be costly in cases where acquiring labelled instances is expensive. For example, it can be time-consuming to manually identify spam mails (labelled instances) from thousands of emails (unlabelled instances) flooding an inbox during initial data collection. Generally, we answer the above scenario with uncertainty sampling, an active learning method that improves the efficiency of supervised learning by using fewer labelled instances than passive learning. Given an unlabelled data pool, uncertainty sampling queries the labels of instances where the predicted probabilities, p, fall into the uncertainty region, i.e., $p \\approx 0.5$. The newly acquired labels are then added to the existing labelled data pool to learn a new model. Nonetheless, the performance ",
    "link": "https://arxiv.org/abs/2403.01352",
    "context": "Title: Improving Uncertainty Sampling with Bell Curve Weight Function\nAbstract: arXiv:2403.01352v1 Announce Type: new  Abstract: Typically, a supervised learning model is trained using passive learning by randomly selecting unlabelled instances to annotate. This approach is effective for learning a model, but can be costly in cases where acquiring labelled instances is expensive. For example, it can be time-consuming to manually identify spam mails (labelled instances) from thousands of emails (unlabelled instances) flooding an inbox during initial data collection. Generally, we answer the above scenario with uncertainty sampling, an active learning method that improves the efficiency of supervised learning by using fewer labelled instances than passive learning. Given an unlabelled data pool, uncertainty sampling queries the labels of instances where the predicted probabilities, p, fall into the uncertainty region, i.e., $p \\approx 0.5$. The newly acquired labels are then added to the existing labelled data pool to learn a new model. Nonetheless, the performance ",
    "path": "papers/24/03/2403.01352.json",
    "total_tokens": 607,
    "translated_title": "使用钟形曲线权重函数改进不确定性采样",
    "translated_abstract": "通常，通过随机选择未标记实例进行注释来训练监督学习模型。这种方法对于学习模型是有效的，但在获取标记实例昂贵的情况下可能成本高昂。因此，我们提出了使用钟形曲线权重函数改进不确定性采样，以提高监督学习的效率。",
    "tldr": "使用钟形曲线权重函数改进了不确定性采样方法，提高了监督学习效率。"
}