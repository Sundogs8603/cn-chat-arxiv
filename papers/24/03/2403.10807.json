{
    "title": "FlyKD: Graph Knowledge Distillation on the Fly with Curriculum Learning",
    "abstract": "arXiv:2403.10807v1 Announce Type: new  Abstract: Knowledge Distillation (KD) aims to transfer a more capable teacher model's knowledge to a lighter student model in order to improve the efficiency of the model, making it faster and more deployable. However, the student model's optimization process over the noisy pseudo labels (generated by the teacher model) is tricky and the amount of pseudo labels one can generate is limited due to Out of Memory (OOM) error. In this paper, we propose FlyKD (Knowledge Distillation on the Fly) which enables the generation of virtually unlimited number of pseudo labels, coupled with Curriculum Learning that greatly alleviates the optimization process over the noisy pseudo labels. Empirically, we observe that FlyKD outperforms vanilla KD and the renown Local Structure Preserving Graph Convolutional Network (LSPGCN). Lastly, with the success of Curriculum Learning, we shed light on a new research direction of improving optimization over noisy pseudo label",
    "link": "https://arxiv.org/abs/2403.10807",
    "context": "Title: FlyKD: Graph Knowledge Distillation on the Fly with Curriculum Learning\nAbstract: arXiv:2403.10807v1 Announce Type: new  Abstract: Knowledge Distillation (KD) aims to transfer a more capable teacher model's knowledge to a lighter student model in order to improve the efficiency of the model, making it faster and more deployable. However, the student model's optimization process over the noisy pseudo labels (generated by the teacher model) is tricky and the amount of pseudo labels one can generate is limited due to Out of Memory (OOM) error. In this paper, we propose FlyKD (Knowledge Distillation on the Fly) which enables the generation of virtually unlimited number of pseudo labels, coupled with Curriculum Learning that greatly alleviates the optimization process over the noisy pseudo labels. Empirically, we observe that FlyKD outperforms vanilla KD and the renown Local Structure Preserving Graph Convolutional Network (LSPGCN). Lastly, with the success of Curriculum Learning, we shed light on a new research direction of improving optimization over noisy pseudo label",
    "path": "papers/24/03/2403.10807.json",
    "total_tokens": 920,
    "translated_title": "FlyKD: 飞行中的图知识蒸馏与课程学习",
    "translated_abstract": "知识蒸馏（KD）旨在将更强大的教师模型的知识转移给更轻便的学生模型，以提高模型的效率，使其更快速和更易部署。然而，学生模型在优化过程中面临由教师模型生成的嘈杂伪标签的困难，而能够生成的伪标签量受到内存不足（OOM）错误的限制。在本文中，我们提出了 FlyKD（飞行中的知识蒸馏），它可以生成几乎无限数量的伪标签，结合课程学习大大缓解了对嘈杂伪标签的优化过程。实验证明，FlyKD的性能优于基准KD和著名的局部结构保持图卷积网络（LSPGCN）。最后，通过课程学习的成功，我们揭示了一个新的研究方向，即改善对嘈杂伪标签的优化。",
    "tldr": "FlyKD提出了一种名为飞行中的知识蒸馏，通过结合课程学习，能够生成几乎无限数量的伪标签，极大缓解了在嘈杂伪标签上的优化过程，并显示出优于基准KD和局部结构保持图卷积网络的性能。"
}