{
    "title": "Risk and Response in Large Language Models: Evaluating Key Threat Categories",
    "abstract": "arXiv:2403.14988v1 Announce Type: new  Abstract: This paper explores the pressing issue of risk assessment in Large Language Models (LLMs) as they become increasingly prevalent in various applications. Focusing on how reward models, which are designed to fine-tune pretrained LLMs to align with human values, perceive and categorize different types of risks, we delve into the challenges posed by the subjective nature of preference-based training data. By utilizing the Anthropic Red-team dataset, we analyze major risk categories, including Information Hazards, Malicious Uses, and Discrimination/Hateful content. Our findings indicate that LLMs tend to consider Information Hazards less harmful, a finding confirmed by a specially developed regression model. Additionally, our analysis shows that LLMs respond less stringently to Information Hazards compared to other risks. The study further reveals a significant vulnerability of LLMs to jailbreaking attacks in Information Hazard scenarios, hig",
    "link": "https://arxiv.org/abs/2403.14988",
    "context": "Title: Risk and Response in Large Language Models: Evaluating Key Threat Categories\nAbstract: arXiv:2403.14988v1 Announce Type: new  Abstract: This paper explores the pressing issue of risk assessment in Large Language Models (LLMs) as they become increasingly prevalent in various applications. Focusing on how reward models, which are designed to fine-tune pretrained LLMs to align with human values, perceive and categorize different types of risks, we delve into the challenges posed by the subjective nature of preference-based training data. By utilizing the Anthropic Red-team dataset, we analyze major risk categories, including Information Hazards, Malicious Uses, and Discrimination/Hateful content. Our findings indicate that LLMs tend to consider Information Hazards less harmful, a finding confirmed by a specially developed regression model. Additionally, our analysis shows that LLMs respond less stringently to Information Hazards compared to other risks. The study further reveals a significant vulnerability of LLMs to jailbreaking attacks in Information Hazard scenarios, hig",
    "path": "papers/24/03/2403.14988.json",
    "total_tokens": 893,
    "translated_title": "大型语言模型中的风险与响应：评估关键威胁类别",
    "translated_abstract": "本文探讨了大型语言模型（LLMs）在各种应用中日益普及的风险评估问题。重点关注奖励模型如何感知和分类不同类型的风险，奖励模型旨在微调预训练的LLMs以与人类价值观一致，我们深入探讨了基于偏好训练数据的主观性质带来的挑战。通过使用Anthropic Red-team数据集，我们分析了包括信息风险、恶意用途和歧视/仇恨内容在内的主要风险类别。我们的研究结果表明，LLMs倾向于认为信息危害较少有害，这一发现得到了一个特别开发的回归模型的证实。此外，我们的分析显示，LLMs对信息危害的响应相对不那么严格。研究进一步揭示了LLMs在信息危害场景中对越狱攻击存在显著的漏洞。",
    "tldr": "本研究探讨了大型语言模型中的风险评估问题，发现LLMs倾向于认为信息风险较少有害，同时在信息风险场景中对越狱攻击存在漏洞。",
    "en_tdlr": "This study explores risk assessment in Large Language Models, revealing that LLMs tend to perceive Information Hazards as less harmful and are vulnerable to jailbreaking attacks in Information Hazard scenarios."
}