{
    "title": "Code-Mixed Probes Show How Pre-Trained Models Generalise On Code-Switched Text",
    "abstract": "arXiv:2403.04872v1 Announce Type: new  Abstract: Code-switching is a prevalent linguistic phenomenon in which multilingual individuals seamlessly alternate between languages. Despite its widespread use online and recent research trends in this area, research in code-switching presents unique challenges, primarily stemming from the scarcity of labelled data and available resources. In this study we investigate how pre-trained Language Models handle code-switched text in three dimensions: a) the ability of PLMs to detect code-switched text, b) variations in the structural information that PLMs utilise to capture code-switched text, and c) the consistency of semantic information representation in code-switched text. To conduct a systematic and controlled evaluation of the language models in question, we create a novel dataset of well-formed naturalistic code-switched text along with parallel translations into the source languages. Our findings reveal that pre-trained language models are e",
    "link": "https://arxiv.org/abs/2403.04872",
    "context": "Title: Code-Mixed Probes Show How Pre-Trained Models Generalise On Code-Switched Text\nAbstract: arXiv:2403.04872v1 Announce Type: new  Abstract: Code-switching is a prevalent linguistic phenomenon in which multilingual individuals seamlessly alternate between languages. Despite its widespread use online and recent research trends in this area, research in code-switching presents unique challenges, primarily stemming from the scarcity of labelled data and available resources. In this study we investigate how pre-trained Language Models handle code-switched text in three dimensions: a) the ability of PLMs to detect code-switched text, b) variations in the structural information that PLMs utilise to capture code-switched text, and c) the consistency of semantic information representation in code-switched text. To conduct a systematic and controlled evaluation of the language models in question, we create a novel dataset of well-formed naturalistic code-switched text along with parallel translations into the source languages. Our findings reveal that pre-trained language models are e",
    "path": "papers/24/03/2403.04872.json",
    "total_tokens": 781,
    "translated_title": "使用混合码调查表明预训练模型在混合码文本上的泛化能力",
    "translated_abstract": "混合码是一种普遍的语言现象，多语种个体可以无缝地在语言之间交替。本研究探讨了预训练语言模型在处理混合码文本方面的能力，包括模型检测混合码文本的能力、模型利用的结构信息变化以捕捉混合码文本的能力，以及混合码文本中语义信息表达的一致性。通过创建一个新颖的数据集，结合了自然形态的混合码文本与源语言的平行翻译，我们进行了系统化和控制性评估。研究结果显示，预训练语言模型在处理混合码文本时表现良好。",
    "tldr": "这项研究探讨了预训练语言模型在处理混合码文本方面的能力，发现它们在检测、利用结构信息和表达语义信息方面表现良好",
    "en_tdlr": "This study investigates the ability of pre-trained language models to handle code-switched text, revealing their good performance in detecting, utilizing structural information, and representing semantic information."
}