{
    "title": "Capacity of the Hebbian-Hopfield network associative memory",
    "abstract": "arXiv:2403.01907v1 Announce Type: cross  Abstract: In \\cite{Hop82}, Hopfield introduced a \\emph{Hebbian} learning rule based neural network model and suggested how it can efficiently operate as an associative memory. Studying random binary patterns, he also uncovered that, if a small fraction of errors is tolerated in the stored patterns retrieval, the capacity of the network (maximal number of memorized patterns, $m$) scales linearly with each pattern's size, $n$. Moreover, he famously predicted $\\alpha_c=\\lim_{n\\rightarrow\\infty}\\frac{m}{n}\\approx 0.14$. We study this very same scenario with two famous pattern's basins of attraction: \\textbf{\\emph{(i)}} The AGS one from \\cite{AmiGutSom85}; and \\textbf{\\emph{(ii)}} The NLT one from \\cite{Newman88,Louk94,Louk94a,Louk97,Tal98}. Relying on the \\emph{fully lifted random duality theory} (fl RDT) from \\cite{Stojnicflrdt23}, we obtain the following explicit capacity characterizations on the first level of lifting:   \\begin{equation}   \\alpha",
    "link": "https://arxiv.org/abs/2403.01907",
    "context": "Title: Capacity of the Hebbian-Hopfield network associative memory\nAbstract: arXiv:2403.01907v1 Announce Type: cross  Abstract: In \\cite{Hop82}, Hopfield introduced a \\emph{Hebbian} learning rule based neural network model and suggested how it can efficiently operate as an associative memory. Studying random binary patterns, he also uncovered that, if a small fraction of errors is tolerated in the stored patterns retrieval, the capacity of the network (maximal number of memorized patterns, $m$) scales linearly with each pattern's size, $n$. Moreover, he famously predicted $\\alpha_c=\\lim_{n\\rightarrow\\infty}\\frac{m}{n}\\approx 0.14$. We study this very same scenario with two famous pattern's basins of attraction: \\textbf{\\emph{(i)}} The AGS one from \\cite{AmiGutSom85}; and \\textbf{\\emph{(ii)}} The NLT one from \\cite{Newman88,Louk94,Louk94a,Louk97,Tal98}. Relying on the \\emph{fully lifted random duality theory} (fl RDT) from \\cite{Stojnicflrdt23}, we obtain the following explicit capacity characterizations on the first level of lifting:   \\begin{equation}   \\alpha",
    "path": "papers/24/03/2403.01907.json",
    "total_tokens": 1017,
    "translated_title": "Hebbian-Hopfield网络关联记忆的容量",
    "translated_abstract": "在Hopfield的论文中，他提出了一种基于Hebbian学习规则的神经网络模型，并提出了它如何可以高效地作为一个关联记忆。在研究随机二进制模式时，他还发现，如果存储模式检索中容忍一小部分错误，网络的容量（最大记忆模式数，$m$）与每个模式的大小$n$呈线性关系。此外，他著名地预测了$\\alpha_c=\\lim_{n\\rightarrow\\infty}\\frac{m}{n}\\approx 0.14$。我们研究了这个非常相同的情景，使用了两种著名模式的吸引盆地：\\textbf{\\emph{(i)}}来自\\cite{AmiGutSom85}的AGS；以及\\textbf{\\emph{(ii)}}来自\\cite{Newman88,Louk94,Louk94a,Louk97,Tal98}的NLT。依赖于来自\\cite{Stojnicflrdt23}的\\emph{完全提升的随机对偶理论}（fl RDT），我们获得了在第一层提升上的以下明确容量特性描述：\\begin{equation} \\alpha ...",
    "tldr": "Hopfield提出了一种Hebbian学习规则的神经网络模型，研究了关联记忆的容量，指出网络的容量与模式大小线性相关，提出了容量预测值，并使用两个著名模式的吸引盆地来探讨相关问题。",
    "en_tdlr": "Hopfield introduced a neural network model based on Hebbian learning rule, studied the capacity of associative memory, pointed out the linear relationship between network capacity and pattern size, proposed a predicted capacity value, and used basins of attraction from two famous patterns for further exploration."
}