{
    "title": "FedFixer: Mitigating Heterogeneous Label Noise in Federated Learning",
    "abstract": "arXiv:2403.16561v1 Announce Type: cross  Abstract: Federated Learning (FL) heavily depends on label quality for its performance. However, the label distribution among individual clients is always both noisy and heterogeneous. The high loss incurred by client-specific samples in heterogeneous label noise poses challenges for distinguishing between client-specific and noisy label samples, impacting the effectiveness of existing label noise learning approaches. To tackle this issue, we propose FedFixer, where the personalized model is introduced to cooperate with the global model to effectively select clean client-specific samples. In the dual models, updating the personalized model solely at a local level can lead to overfitting on noisy data due to limited samples, consequently affecting both the local and global models' performance. To mitigate overfitting, we address this concern from two perspectives. Firstly, we employ a confidence regularizer to alleviate the impact of unconfident ",
    "link": "https://arxiv.org/abs/2403.16561",
    "context": "Title: FedFixer: Mitigating Heterogeneous Label Noise in Federated Learning\nAbstract: arXiv:2403.16561v1 Announce Type: cross  Abstract: Federated Learning (FL) heavily depends on label quality for its performance. However, the label distribution among individual clients is always both noisy and heterogeneous. The high loss incurred by client-specific samples in heterogeneous label noise poses challenges for distinguishing between client-specific and noisy label samples, impacting the effectiveness of existing label noise learning approaches. To tackle this issue, we propose FedFixer, where the personalized model is introduced to cooperate with the global model to effectively select clean client-specific samples. In the dual models, updating the personalized model solely at a local level can lead to overfitting on noisy data due to limited samples, consequently affecting both the local and global models' performance. To mitigate overfitting, we address this concern from two perspectives. Firstly, we employ a confidence regularizer to alleviate the impact of unconfident ",
    "path": "papers/24/03/2403.16561.json",
    "total_tokens": 860,
    "translated_title": "FedFixer：在联邦学习中缓解异构标签噪声",
    "translated_abstract": "联邦学习(FL)在性能上严重依赖标签质量。然而，个体客户端之间的标签分布通常同时存在噪声和异构性。在异构标签噪声中由客户端特定样本引起的高损失对区分客户端特定和嘈杂标签样本构成了挑战，影响了现有标签噪声学习方法的有效性。为了解决这个问题，我们提出了FedFixer，其中引入了个性化模型与全局模型合作，以有效选择干净的客户端特定样本。在双模型中，仅在本地级别更新个性化模型可能导致由于样本有限而对噪声数据过度拟合，进而影响局部和全局模型的性能。为减轻过拟合，我们从两个角度解决了这个问题。",
    "tldr": "提出了FedFixer来解决联邦学习中异构标签噪声的问题，引入个性化模型与全局模型合作来有效选择干净的客户端特定样本，通过置信度正则化器和基于样本共享的双模型更新策略来减轻过拟合问题",
    "en_tdlr": "Proposed FedFixer to address heterogeneous label noise in federated learning, introducing personalized model to work with the global model to effectively select clean client-specific samples, mitigating overfitting through confidence regularizer and dual model update strategy based on sample sharing."
}