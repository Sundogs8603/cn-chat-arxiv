{
    "title": "FACTOID: FACtual enTailment fOr hallucInation Detection",
    "abstract": "arXiv:2403.19113v1 Announce Type: cross  Abstract: The widespread adoption of Large Language Models (LLMs) has facilitated numerous benefits. However, hallucination is a significant concern. In response, Retrieval Augmented Generation (RAG) has emerged as a highly promising paradigm to improve LLM outputs by grounding them in factual information. RAG relies on textual entailment (TE) or similar methods to check if the text produced by LLMs is supported or contradicted, compared to retrieved documents. This paper argues that conventional TE methods are inadequate for spotting hallucinations in content generated by LLMs. For instance, consider a prompt about the 'USA's stance on the Ukraine war''. The AI-generated text states, ...U.S. President Barack Obama says the U.S. will not put troops in Ukraine...'' However, during the war the U.S. president is Joe Biden which contradicts factual reality. Moreover, current TE systems are unable to accurately annotate the given text and identify th",
    "link": "https://arxiv.org/abs/2403.19113",
    "context": "Title: FACTOID: FACtual enTailment fOr hallucInation Detection\nAbstract: arXiv:2403.19113v1 Announce Type: cross  Abstract: The widespread adoption of Large Language Models (LLMs) has facilitated numerous benefits. However, hallucination is a significant concern. In response, Retrieval Augmented Generation (RAG) has emerged as a highly promising paradigm to improve LLM outputs by grounding them in factual information. RAG relies on textual entailment (TE) or similar methods to check if the text produced by LLMs is supported or contradicted, compared to retrieved documents. This paper argues that conventional TE methods are inadequate for spotting hallucinations in content generated by LLMs. For instance, consider a prompt about the 'USA's stance on the Ukraine war''. The AI-generated text states, ...U.S. President Barack Obama says the U.S. will not put troops in Ukraine...'' However, during the war the U.S. president is Joe Biden which contradicts factual reality. Moreover, current TE systems are unable to accurately annotate the given text and identify th",
    "path": "papers/24/03/2403.19113.json",
    "total_tokens": 841,
    "translated_title": "FACTOID: 用于幻觉检测的事实推理（FACTOID: FACtual enTailment fOr hallucInation Detection）",
    "translated_abstract": "大型语言模型（LLMs）的广泛应用带来了许多好处，但幻觉是一个重要的问题。为应对这一问题，检索增强生成（RAG）作为一种高度有前途的范式出现，通过在事实信息中接地来提升LLMs的输出。RAG依赖于文本蕴涵（TE）或类似方法，检查LLMs生成的文本是否被检索文档支持或反驳。本文认为传统的TE方法不适用于发现LLMs生成内容中的幻觉。例如，考虑一个关于“美国对乌克兰战争立场”的提示。AI生成的文本表示，“美国总统巴拉克·奥巴马说美国不会派兵进入乌克兰”，然而在战争期间，美国总统是乔·拜登，这与事实不符。此外，当前的TE系统无法准确注释给定的文本和识别幻觉。",
    "tldr": "本文表明传统的文本蕴涵方法无法有效地发现大型语言模型生成的内容中存在的幻觉问题。"
}