{
    "title": "Dynamic Reward Adjustment in Multi-Reward Reinforcement Learning for Counselor Reflection Generation",
    "abstract": "arXiv:2403.13578v1 Announce Type: new  Abstract: In this paper, we study the problem of multi-reward reinforcement learning to jointly optimize for multiple text qualities for natural language generation. We focus on the task of counselor reflection generation, where we optimize the generators to simultaneously improve the fluency, coherence, and reflection quality of generated counselor responses. We introduce two novel bandit methods, DynaOpt and C-DynaOpt, which rely on the broad strategy of combining rewards into a single value and optimizing them simultaneously. Specifically, we employ non-contextual and contextual multi-arm bandits to dynamically adjust multiple reward weights during training. Through automatic and manual evaluations, we show that our proposed techniques, DynaOpt and C-DynaOpt, outperform existing naive and bandit baselines, showcasing their potential for enhancing language models.",
    "link": "https://arxiv.org/abs/2403.13578",
    "context": "Title: Dynamic Reward Adjustment in Multi-Reward Reinforcement Learning for Counselor Reflection Generation\nAbstract: arXiv:2403.13578v1 Announce Type: new  Abstract: In this paper, we study the problem of multi-reward reinforcement learning to jointly optimize for multiple text qualities for natural language generation. We focus on the task of counselor reflection generation, where we optimize the generators to simultaneously improve the fluency, coherence, and reflection quality of generated counselor responses. We introduce two novel bandit methods, DynaOpt and C-DynaOpt, which rely on the broad strategy of combining rewards into a single value and optimizing them simultaneously. Specifically, we employ non-contextual and contextual multi-arm bandits to dynamically adjust multiple reward weights during training. Through automatic and manual evaluations, we show that our proposed techniques, DynaOpt and C-DynaOpt, outperform existing naive and bandit baselines, showcasing their potential for enhancing language models.",
    "path": "papers/24/03/2403.13578.json",
    "total_tokens": 848,
    "translated_title": "多重奖励强化学习中的动态奖励调整用于辅导员反思生成",
    "translated_abstract": "在本文中，我们研究了多重奖励强化学习问题，以共同优化自然语言生成中多个文本质量。我们关注辅导员反思生成的任务，我们优化生成器以同时提高生成的辅导员回复的流畅性、连贯性和反思质量。我们引入了两种新的赌博方法，DynaOpt和C-DynaOpt，这些方法依赖于将奖励组合成一个单一值并同时优化它们的广泛策略。具体来说，我们利用非情境和情境多臂赌博来在训练过程中动态调整多个奖励权重。通过自动和手动评估，我们展示了我们提出的技术DynaOpt和C-DynaOpt优于现有的朴素和赌博基线，展示了它们增强语言模型的潜力。",
    "tldr": "本文研究了多重奖励强化学习在辅导员反思生成中的应用，引入了两种新的赌博方法DynOpt和C-DynaOpt，动态调整多个奖励权重，通过实验表明这些方法能够优于现有的基线方法。",
    "en_tdlr": "This paper explores the application of multi-reward reinforcement learning in counselor reflection generation, introducing two novel bandit methods DynaOpt and C-DynaOpt that dynamically adjust multiple reward weights, and experiments show their superiority over existing baselines."
}