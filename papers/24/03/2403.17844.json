{
    "title": "Mechanistic Design and Scaling of Hybrid Architectures",
    "abstract": "arXiv:2403.17844v1 Announce Type: new  Abstract: The development of deep learning architectures is a resource-demanding process, due to a vast design space, long prototyping times, and high compute costs associated with at-scale model training and evaluation. We set out to simplify this process by grounding it in an end-to-end mechanistic architecture design (MAD) pipeline, encompassing small-scale capability unit tests predictive of scaling laws. Through a suite of synthetic token manipulation tasks such as compression and recall, designed to probe capabilities, we identify and test new hybrid architectures constructed from a variety of computational primitives. We experimentally validate the resulting architectures via an extensive compute-optimal and a new state-optimal scaling law analysis, training over 500 language models between 70M to 7B parameters. Surprisingly, we find MAD synthetics to correlate with compute-optimal perplexity, enabling accurate evaluation of new architectur",
    "link": "https://arxiv.org/abs/2403.17844",
    "context": "Title: Mechanistic Design and Scaling of Hybrid Architectures\nAbstract: arXiv:2403.17844v1 Announce Type: new  Abstract: The development of deep learning architectures is a resource-demanding process, due to a vast design space, long prototyping times, and high compute costs associated with at-scale model training and evaluation. We set out to simplify this process by grounding it in an end-to-end mechanistic architecture design (MAD) pipeline, encompassing small-scale capability unit tests predictive of scaling laws. Through a suite of synthetic token manipulation tasks such as compression and recall, designed to probe capabilities, we identify and test new hybrid architectures constructed from a variety of computational primitives. We experimentally validate the resulting architectures via an extensive compute-optimal and a new state-optimal scaling law analysis, training over 500 language models between 70M to 7B parameters. Surprisingly, we find MAD synthetics to correlate with compute-optimal perplexity, enabling accurate evaluation of new architectur",
    "path": "papers/24/03/2403.17844.json",
    "total_tokens": 830,
    "translated_title": "混合体系结构的机理设计和尺度变换",
    "translated_abstract": "深度学习架构的开发是一个资源密集型的过程，由于设计空间广阔、原型制作时间长以及与规模化模型训练和评估相关的高计算成本。我们致力于通过以端到端机械式架构设计（MAD）管线为基础来简化这一过程，包括小规模能力单元测试，预测尺度规律。通过一系列合成的令牌操作任务，如压缩和回忆，旨在探索能力，我们识别并测试由各种计算基元构建的新型混合体系结构。通过广泛的计算优化和一项新的状态最优化尺度分析，我们通过训练70M到7B参数之间的500多种语言模型对结果体系结构进行实验证实了这一点。令人惊讶的是，我们发现MAD合成与计算最优困惑度相关，能够准确评估新架构。",
    "tldr": "通过机理设计和尺度变换，我们提出了一种新的混合体系结构设计方法，可以简化深度学习架构的开发过程，并可以准确评估新架构。",
    "en_tdlr": "We propose a new approach to hybrid architecture design through mechanistic design and scaling, which can simplify the development process of deep learning architectures and enable accurate evaluation of new architectures."
}