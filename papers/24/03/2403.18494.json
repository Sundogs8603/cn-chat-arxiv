{
    "title": "Learning in PINNs: Phase transition, total diffusion, and generalization",
    "abstract": "arXiv:2403.18494v1 Announce Type: new  Abstract: We investigate the learning dynamics of fully-connected neural networks through the lens of gradient signal-to-noise ratio (SNR), examining the behavior of first-order optimizers like Adam in non-convex objectives. By interpreting the drift/diffusion phases in the information bottleneck theory, focusing on gradient homogeneity, we identify a third phase termed ``total diffusion\", characterized by equilibrium in the learning rates and homogeneous gradients. This phase is marked by an abrupt SNR increase, uniform residuals across the sample space and the most rapid training convergence. We propose a residual-based re-weighting scheme to accelerate this diffusion in quadratic loss functions, enhancing generalization. We also explore the information compression phenomenon, pinpointing a significant saturation-induced compression of activations at the total diffusion phase, with deeper layers experiencing negligible information loss. Supporte",
    "link": "https://arxiv.org/abs/2403.18494",
    "context": "Title: Learning in PINNs: Phase transition, total diffusion, and generalization\nAbstract: arXiv:2403.18494v1 Announce Type: new  Abstract: We investigate the learning dynamics of fully-connected neural networks through the lens of gradient signal-to-noise ratio (SNR), examining the behavior of first-order optimizers like Adam in non-convex objectives. By interpreting the drift/diffusion phases in the information bottleneck theory, focusing on gradient homogeneity, we identify a third phase termed ``total diffusion\", characterized by equilibrium in the learning rates and homogeneous gradients. This phase is marked by an abrupt SNR increase, uniform residuals across the sample space and the most rapid training convergence. We propose a residual-based re-weighting scheme to accelerate this diffusion in quadratic loss functions, enhancing generalization. We also explore the information compression phenomenon, pinpointing a significant saturation-induced compression of activations at the total diffusion phase, with deeper layers experiencing negligible information loss. Supporte",
    "path": "papers/24/03/2403.18494.json",
    "total_tokens": 922,
    "translated_title": "在PINNs中学习：相变、总扩散和泛化",
    "translated_abstract": "我们通过梯度信噪比（SNR）的视角研究全连接神经网络的学习动态，探讨了Adam等一阶优化器在非凸目标中的行为。通过在信息瓶颈理论中解释漂移/扩散相，聚焦梯度均一性，我们确定了一个被称为“总扩散”的第三阶段，其特征是学习速率和梯度均匀，这一阶段标志着SNR急剧增加，样本空间中残差均匀且训练收敛最快。我们提出了一种基于残差的重新加权方案来加速二次损失函数中的扩散，从而增强泛化能力。我们还探索了信息压缩现象，确定在总扩散阶段激活发生显著饱和诱导的压缩，更深的层次经历可以忽略的信息损失。",
    "tldr": "该研究通过梯度信噪比（SNR）的视角研究了全连接神经网络的学习动态，并在信息瓶颈理论中发现了第三阶段\"总扩散\"，其特征是均匀的学习速率和梯度，这一阶段标志着快速的训练收敛和增强泛化能力。",
    "en_tdlr": "This study investigates the learning dynamics of fully-connected neural networks through the lens of gradient signal-to-noise ratio (SNR) and identifies a third phase named \"total diffusion\" in the information bottleneck theory, characterized by equilibrium in learning rates and gradients, rapid training convergence, and enhanced generalization."
}