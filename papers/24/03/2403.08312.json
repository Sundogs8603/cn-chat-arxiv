{
    "title": "StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses",
    "abstract": "arXiv:2403.08312v1 Announce Type: cross  Abstract: Standard Large Language Models (LLMs) struggle with handling dialogues with long contexts due to efficiency and consistency issues. According to our observation, dialogue contexts are highly structured, and the special token of \\textit{End-of-Utterance} (EoU) in dialogues has the potential to aggregate information. We refer to the EoU tokens as ``conversational attention sinks'' (conv-attn sinks). Accordingly, we introduce StreamingDialogue, which compresses long dialogue history into conv-attn sinks with minimal losses, and thus reduces computational complexity quadratically with the number of sinks (i.e., the number of utterances). Current LLMs already demonstrate the ability to handle long context window, e.g., a window size of 200k or more. To this end, by compressing utterances into EoUs, our method has the potential to handle more than 200k of utterances, resulting in a prolonged dialogue learning. In order to minimize informatio",
    "link": "https://arxiv.org/abs/2403.08312",
    "context": "Title: StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses\nAbstract: arXiv:2403.08312v1 Announce Type: cross  Abstract: Standard Large Language Models (LLMs) struggle with handling dialogues with long contexts due to efficiency and consistency issues. According to our observation, dialogue contexts are highly structured, and the special token of \\textit{End-of-Utterance} (EoU) in dialogues has the potential to aggregate information. We refer to the EoU tokens as ``conversational attention sinks'' (conv-attn sinks). Accordingly, we introduce StreamingDialogue, which compresses long dialogue history into conv-attn sinks with minimal losses, and thus reduces computational complexity quadratically with the number of sinks (i.e., the number of utterances). Current LLMs already demonstrate the ability to handle long context window, e.g., a window size of 200k or more. To this end, by compressing utterances into EoUs, our method has the potential to handle more than 200k of utterances, resulting in a prolonged dialogue learning. In order to minimize informatio",
    "path": "papers/24/03/2403.08312.json",
    "total_tokens": 881,
    "translated_title": "通过最小损失进行长上下文压缩的StreamingDialogue：长对话学习",
    "translated_abstract": "标准的大型语言模型(LLMs)在处理具有长上下文的对话时遇到了效率和一致性问题。根据我们的观察，对话上下文具有高度结构化，并且对话中的特殊标记\\textit{End-of-Utterance} (EoU) 有聚合信息的潜力。我们将EoU标记称为\"会话注意力汇集点\"（conv-attn sinks）。因此，我们介绍了StreamingDialogue，将长对话历史压缩为conv-attn沉点，并最小化损失，从而使计算复杂度与沉点数量（即话语数量）的平方成正比。当前的LLMs已经展示了处理长上下文窗口的能力，例如，窗口大小达到200k甚至更大。通过将话语压缩为EoUs，我们的方法有潜力处理超过200k条话语，实现长时间对话学习。",
    "tldr": "提出了StreamingDialogue，通过将长对话历史压缩为\"会话注意力汇集点\"，最小化损失，使计算复杂度减少，并有潜力处理超过200k条话语，实现长时间对话学习",
    "en_tdlr": "Introducing StreamingDialogue, which compresses long dialogue history into \"conversational attention sinks\" with minimal losses, reduces computational complexity quadratically, and has the potential to handle over 200k utterances for prolonged dialogue learning."
}