{
    "title": "GNN-VPA: A Variance-Preserving Aggregation Strategy for Graph Neural Networks",
    "abstract": "arXiv:2403.04747v1 Announce Type: cross  Abstract: Graph neural networks (GNNs), and especially message-passing neural networks, excel in various domains such as physics, drug discovery, and molecular modeling. The expressivity of GNNs with respect to their ability to discriminate non-isomorphic graphs critically depends on the functions employed for message aggregation and graph-level readout. By applying signal propagation theory, we propose a variance-preserving aggregation function (VPA) that maintains expressivity, but yields improved forward and backward dynamics. Experiments demonstrate that VPA leads to increased predictive performance for popular GNN architectures as well as improved learning dynamics. Our results could pave the way towards normalizer-free or self-normalizing GNNs.",
    "link": "https://arxiv.org/abs/2403.04747",
    "context": "Title: GNN-VPA: A Variance-Preserving Aggregation Strategy for Graph Neural Networks\nAbstract: arXiv:2403.04747v1 Announce Type: cross  Abstract: Graph neural networks (GNNs), and especially message-passing neural networks, excel in various domains such as physics, drug discovery, and molecular modeling. The expressivity of GNNs with respect to their ability to discriminate non-isomorphic graphs critically depends on the functions employed for message aggregation and graph-level readout. By applying signal propagation theory, we propose a variance-preserving aggregation function (VPA) that maintains expressivity, but yields improved forward and backward dynamics. Experiments demonstrate that VPA leads to increased predictive performance for popular GNN architectures as well as improved learning dynamics. Our results could pave the way towards normalizer-free or self-normalizing GNNs.",
    "path": "papers/24/03/2403.04747.json",
    "total_tokens": 771,
    "translated_title": "GNN-VPA: 一种用于图神经网络的方差保持聚合策略",
    "translated_abstract": "图神经网络（GNNs），特别是消息传递神经网络，在物理学、药物发现和分子建模等各个领域表现出色。GNNs的表达能力，特别是在区分非同构图的能力，关键取决于用于消息聚合和图级读出的函数。通过应用信号传播理论，我们提出了一种保持方差的聚合函数（VPA），该函数保持了表达能力，同时提高了前向和后向动力学。实验证明，VPA导致了流行的GNN架构的预测性能提高，同时改善了学习动态。我们的结果可能为无归一化或自归一化的GNNs铺平道路。",
    "tldr": "通过提出一种保持方差的聚合函数（VPA），该函数在维持图神经网络（GNNs）的表达能力的基础上，提高了前向和后向动力学，进而导致了增强的预测性能和改善的学习动态。",
    "en_tdlr": "By proposing a variance-preserving aggregation function (VPA) that improves forward and backward dynamics while maintaining expressivity of graph neural networks (GNNs), this work leads to enhanced predictive performance and improved learning dynamics."
}