{
    "title": "Revealing Vulnerabilities of Neural Networks in Parameter Learning and Defense Against Explanation-Aware Backdoors",
    "abstract": "arXiv:2403.16569v1 Announce Type: new  Abstract: Explainable Artificial Intelligence (XAI) strategies play a crucial part in increasing the understanding and trustworthiness of neural networks. Nonetheless, these techniques could potentially generate misleading explanations. Blinding attacks can drastically alter a machine learning algorithm's prediction and explanation, providing misleading information by adding visually unnoticeable artifacts into the input, while maintaining the model's accuracy. It poses a serious challenge in ensuring the reliability of XAI methods. To ensure the reliability of XAI methods poses a real challenge, we leverage statistical analysis to highlight the changes in CNN weights within a CNN following blinding attacks. We introduce a method specifically designed to limit the effectiveness of such attacks during the evaluation phase, avoiding the need for extra training. The method we suggest defences against most modern explanation-aware adversarial attacks,",
    "link": "https://arxiv.org/abs/2403.16569",
    "context": "Title: Revealing Vulnerabilities of Neural Networks in Parameter Learning and Defense Against Explanation-Aware Backdoors\nAbstract: arXiv:2403.16569v1 Announce Type: new  Abstract: Explainable Artificial Intelligence (XAI) strategies play a crucial part in increasing the understanding and trustworthiness of neural networks. Nonetheless, these techniques could potentially generate misleading explanations. Blinding attacks can drastically alter a machine learning algorithm's prediction and explanation, providing misleading information by adding visually unnoticeable artifacts into the input, while maintaining the model's accuracy. It poses a serious challenge in ensuring the reliability of XAI methods. To ensure the reliability of XAI methods poses a real challenge, we leverage statistical analysis to highlight the changes in CNN weights within a CNN following blinding attacks. We introduce a method specifically designed to limit the effectiveness of such attacks during the evaluation phase, avoiding the need for extra training. The method we suggest defences against most modern explanation-aware adversarial attacks,",
    "path": "papers/24/03/2403.16569.json",
    "total_tokens": 835,
    "translated_title": "揭示神经网络在参数学习和对抗中的漏洞",
    "translated_abstract": "解释性人工智能（XAI）策略在增加对神经网络的理解和可信度方面起着至关重要的作用。然而，这些技术可能会产生误导性的解释。遮蔽攻击可以严重改变机器学习算法的预测和解释，在输入中添加不可见的视觉工件，同时保持模型的准确性，从而提供误导性信息。这给确保XAI方法的可信度带来了严峻挑战。为了保证XAI方法的可靠性，我们利用统计分析来突出遮蔽攻击后CNN内部CNN权重的变化。我们引入了一种专门设计的方法，旨在限制此类攻击在评估阶段的有效性，避免额外的训练需求。我们提出的方法可以抵御大多数现代的对解释敏感的对抗攻击。",
    "tldr": "该研究揭示了神经网络在参数学习和对抗中的漏洞，引入了一种针对解释敏感的后门攻击的防御方法，通过统计分析来限制对神经网络模型的攻击。",
    "en_tdlr": "This study exposes vulnerabilities of neural networks in parameter learning and defense against explanation-aware backdoors, introducing a defense method against explanation-sensitive adversarial attacks and using statistical analysis to limit attacks on neural network models."
}