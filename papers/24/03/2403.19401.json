{
    "title": "Hardness of Learning Boolean Functions from Label Proportions",
    "abstract": "arXiv:2403.19401v1 Announce Type: cross  Abstract: In recent years the framework of learning from label proportions (LLP) has been gaining importance in machine learning. In this setting, the training examples are aggregated into subsets or bags and only the average label per bag is available for learning an example-level predictor. This generalizes traditional PAC learning which is the special case of unit-sized bags. The computational learning aspects of LLP were studied in recent works (Saket, NeurIPS'21; Saket, NeurIPS'22) which showed algorithms and hardness for learning halfspaces in the LLP setting. In this work we focus on the intractability of LLP learning Boolean functions. Our first result shows that given a collection of bags of size at most $2$ which are consistent with an OR function, it is NP-hard to find a CNF of constantly many clauses which satisfies any constant-fraction of the bags. This is in contrast with the work of (Saket, NeurIPS'21) which gave a $(2/5)$-approx",
    "link": "https://arxiv.org/abs/2403.19401",
    "context": "Title: Hardness of Learning Boolean Functions from Label Proportions\nAbstract: arXiv:2403.19401v1 Announce Type: cross  Abstract: In recent years the framework of learning from label proportions (LLP) has been gaining importance in machine learning. In this setting, the training examples are aggregated into subsets or bags and only the average label per bag is available for learning an example-level predictor. This generalizes traditional PAC learning which is the special case of unit-sized bags. The computational learning aspects of LLP were studied in recent works (Saket, NeurIPS'21; Saket, NeurIPS'22) which showed algorithms and hardness for learning halfspaces in the LLP setting. In this work we focus on the intractability of LLP learning Boolean functions. Our first result shows that given a collection of bags of size at most $2$ which are consistent with an OR function, it is NP-hard to find a CNF of constantly many clauses which satisfies any constant-fraction of the bags. This is in contrast with the work of (Saket, NeurIPS'21) which gave a $(2/5)$-approx",
    "path": "papers/24/03/2403.19401.json",
    "total_tokens": 908,
    "translated_title": "从标签比例学习布尔函数的困难性",
    "translated_abstract": "近年来，从标签比例进行学习(LLP)的框架在机器学习中变得越来越重要。在这种情况下，训练示例被聚合到子集或袋中，只有每个袋的平均标签可用于学习一个基于示例的预测器。这是对传统的PAC学习的推广，后者是单位大小袋的特例。最近的研究对LLP的计算学习方面进行了研究(Saket, NeurIPS'21; Saket, NeurIPS'22)，展示了在LLP设置中学习半空间的算法和困难。在这项工作中，我们专注于LLP学习布尔函数的难题。我们的第一个结果表明，考虑到大小不超过$2$的袋子集合，这些袋子集合与OR函数一致时，要找到一个满足任何常数部分袋子的常数多子句CNF是NP难的。这与(Saket, NeurIPS'21)的工作形成对比，后者提供了一个$(2/5)$-approx。",
    "tldr": "本研究针对从标签比例学习布尔函数的难题性展开研究，发现在特定情况下寻找满足子集合中常数比例的布尔函数的子句是NP难的。",
    "en_tdlr": "This study investigates the intractability of learning Boolean functions from label proportions, showing the NP-hardness of finding clauses satisfying a constant fraction of bags in specific cases."
}