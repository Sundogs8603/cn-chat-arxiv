{
    "title": "Stabilizing Policy Gradients for Stochastic Differential Equations via Consistency with Perturbation Process",
    "abstract": "arXiv:2403.04154v1 Announce Type: new  Abstract: Considering generating samples with high rewards, we focus on optimizing deep neural networks parameterized stochastic differential equations (SDEs), the advanced generative models with high expressiveness, with policy gradient, the leading algorithm in reinforcement learning. Nevertheless, when applying policy gradients to SDEs, since the policy gradient is estimated on a finite set of trajectories, it can be ill-defined, and the policy behavior in data-scarce regions may be uncontrolled. This challenge compromises the stability of policy gradients and negatively impacts sample complexity. To address these issues, we propose constraining the SDE to be consistent with its associated perturbation process. Since the perturbation process covers the entire space and is easy to sample, we can mitigate the aforementioned problems. Our framework offers a general approach allowing for a versatile selection of policy gradient methods to effective",
    "link": "https://arxiv.org/abs/2403.04154",
    "context": "Title: Stabilizing Policy Gradients for Stochastic Differential Equations via Consistency with Perturbation Process\nAbstract: arXiv:2403.04154v1 Announce Type: new  Abstract: Considering generating samples with high rewards, we focus on optimizing deep neural networks parameterized stochastic differential equations (SDEs), the advanced generative models with high expressiveness, with policy gradient, the leading algorithm in reinforcement learning. Nevertheless, when applying policy gradients to SDEs, since the policy gradient is estimated on a finite set of trajectories, it can be ill-defined, and the policy behavior in data-scarce regions may be uncontrolled. This challenge compromises the stability of policy gradients and negatively impacts sample complexity. To address these issues, we propose constraining the SDE to be consistent with its associated perturbation process. Since the perturbation process covers the entire space and is easy to sample, we can mitigate the aforementioned problems. Our framework offers a general approach allowing for a versatile selection of policy gradient methods to effective",
    "path": "papers/24/03/2403.04154.json",
    "total_tokens": 825,
    "translated_title": "通过与扰动过程一致性稳定随机微分方程的策略梯度",
    "translated_abstract": "考虑产生高回报样本，我们专注于优化参数化为深度神经网络的随机微分方程（SDEs），这是具有高可表达性的先进生成模型，利用强化学习中的主导算法策略梯度。然而，当将策略梯度应用于SDE时，由于策略梯度是在有限轨迹集上估计的，它可能是不明确定义的，并且数据稀缺区域的策略行为可能无法控制。这一挑战影响了策略梯度的稳定性，并对样本复杂度产生了负面影响。为了解决这些问题，我们提出将SDE限制为与其相关的扰动过程保持一致。由于扰动过程覆盖整个空间且易于抽样，我们可以缓解前述问题。我们的框架提供了一种通用方法，允许灵活选择策略梯度方法以有效。",
    "tldr": "通过与SDE相关的扰动过程一致性，稳定策略梯度，提高样本效率。",
    "en_tdlr": "Stabilize policy gradients for stochastic differential equations by enforcing consistency with perturbation process, improving sample efficiency."
}