{
    "title": "Distilling Text Style Transfer With Self-Explanation From LLMs",
    "abstract": "arXiv:2403.01106v1 Announce Type: cross  Abstract: Text Style Transfer (TST) seeks to alter the style of text while retaining its core content. Given the constraints of limited parallel datasets for TST, we propose CoTeX, a framework that leverages large language models (LLMs) alongside chain-of-thought (CoT) prompting to facilitate TST. CoTeX distills the complex rewriting and reasoning capabilities of LLMs into more streamlined models capable of working with both non-parallel and parallel data. Through experimentation across four TST datasets, CoTeX is shown to surpass traditional supervised fine-tuning and knowledge distillation methods, particularly in low-resource settings. We conduct a comprehensive evaluation, comparing CoTeX against current unsupervised, supervised, in-context learning (ICL) techniques, and instruction-tuned LLMs. Furthermore, CoTeX distinguishes itself by offering transparent explanations for its style transfer process.",
    "link": "https://arxiv.org/abs/2403.01106",
    "context": "Title: Distilling Text Style Transfer With Self-Explanation From LLMs\nAbstract: arXiv:2403.01106v1 Announce Type: cross  Abstract: Text Style Transfer (TST) seeks to alter the style of text while retaining its core content. Given the constraints of limited parallel datasets for TST, we propose CoTeX, a framework that leverages large language models (LLMs) alongside chain-of-thought (CoT) prompting to facilitate TST. CoTeX distills the complex rewriting and reasoning capabilities of LLMs into more streamlined models capable of working with both non-parallel and parallel data. Through experimentation across four TST datasets, CoTeX is shown to surpass traditional supervised fine-tuning and knowledge distillation methods, particularly in low-resource settings. We conduct a comprehensive evaluation, comparing CoTeX against current unsupervised, supervised, in-context learning (ICL) techniques, and instruction-tuned LLMs. Furthermore, CoTeX distinguishes itself by offering transparent explanations for its style transfer process.",
    "path": "papers/24/03/2403.01106.json",
    "total_tokens": 930,
    "translated_title": "通过从大型语言模型中自我解释提炼文本风格转移",
    "translated_abstract": "文本风格转移（TST）旨在改变文本的风格同时保留其核心内容。鉴于TST的有限平行数据集的限制，我们提出了CoTeX，这是一个利用大型语言模型（LLMs）和思维链（CoT）提示来促进TST的框架。CoTeX将LLMs的复杂重写和推理能力提炼成更简化的模型，能够处理非平行数据和平行数据。通过在四个TST数据集上的实验，CoTeX显示出超越传统监督微调和知识蒸馏方法的能力，特别是在资源匮乏的情况下。我们进行了全面评估，将CoTeX与当前的无监督、监督、上下文学习（ICL）技术以及指导调整的LLMs进行了比较。此外，CoTeX通过提供透明的解释其风格转移过程而脱颖而出。",
    "tldr": "CoTeX是一个利用大型语言模型和思维链提示来促进文本风格转移的框架，通过提炼LLMs的能力为处理非平行数据和平行数据的简化模型，在低资源情况下表现优于传统的监督微调和知识蒸馏方法，并通过透明的解释在风格转移过程中有显著优势。"
}