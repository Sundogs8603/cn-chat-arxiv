{
    "title": "Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization",
    "abstract": "arXiv:2403.08730v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) excel in generating responses based on visual inputs. However, they often suffer from a bias towards generating responses similar to their pretraining corpus, overshadowing the importance of visual information. We treat this bias as a \"preference\" for pretraining statistics, which hinders the model's grounding in visual input. To mitigate this issue, we propose Bootstrapped Preference Optimization (BPO), which conducts preference learning with datasets containing negative responses bootstrapped from the model itself. Specifically, we propose the following two strategies: 1) using distorted image inputs to the MLLM for eliciting responses that contain signified pretraining bias; 2) leveraging text-based LLM to explicitly inject erroneous but common elements into the original response. Those undesirable responses are paired with original annotated responses from the datasets to construct the prefere",
    "link": "https://arxiv.org/abs/2403.08730",
    "context": "Title: Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization\nAbstract: arXiv:2403.08730v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) excel in generating responses based on visual inputs. However, they often suffer from a bias towards generating responses similar to their pretraining corpus, overshadowing the importance of visual information. We treat this bias as a \"preference\" for pretraining statistics, which hinders the model's grounding in visual input. To mitigate this issue, we propose Bootstrapped Preference Optimization (BPO), which conducts preference learning with datasets containing negative responses bootstrapped from the model itself. Specifically, we propose the following two strategies: 1) using distorted image inputs to the MLLM for eliciting responses that contain signified pretraining bias; 2) leveraging text-based LLM to explicitly inject erroneous but common elements into the original response. Those undesirable responses are paired with original annotated responses from the datasets to construct the prefere",
    "path": "papers/24/03/2403.08730.json",
    "total_tokens": 907,
    "translated_title": "通过引入引导优先优化加强多模态大语言模型",
    "translated_abstract": "多模态大语言模型（MLLMs）在基于视觉输入生成响应方面表现出色。然而，它们往往存在偏向于生成与其预训练语料库相似响应的偏见，掩盖了视觉信息的重要性。我们将这种偏见视为对预训练统计数据的“偏好”，这阻碍了模型对视觉输入的基础。为了缓解这一问题，我们提出了引导优先优化（BPO），该方法使用包含负面响应的数据集进行偏好学习，这些数据集是从模型本身中引导出来的。具体而言，我们提出了以下两种策略：1）使用扭曲的图像输入到MLLM中，以引发包含显著预训练偏见的响应；2）利用基于文本的LLM将错误但常见的元素明确地注入原始响应中。这些不良响应与数据集中原始的注释响应配对，构建了偏好。",
    "tldr": "提出了引导优先优化（BPO）策略，通过使用包含负面响应的数据集来减轻多模态大语言模型（MLLMs）对预训练语料库偏见的影响，并使用两种策略来促进模型在视觉输入中的表现。"
}