{
    "title": "Compiler generated feedback for Large Language Models",
    "abstract": "arXiv:2403.14714v1 Announce Type: cross  Abstract: We introduce a novel paradigm in compiler optimization powered by Large Language Models with compiler feedback to optimize the code size of LLVM assembly. The model takes unoptimized LLVM IR as input and produces optimized IR, the best optimization passes, and instruction counts of both unoptimized and optimized IRs. Then we compile the input with generated optimization passes and evaluate if the predicted instruction count is correct, generated IR is compilable, and corresponds to compiled code. We provide this feedback back to LLM and give it another chance to optimize code. This approach adds an extra 0.53% improvement over -Oz to the original model. Even though, adding more information with feedback seems intuitive, simple sampling techniques achieve much higher performance given 10 or more samples.",
    "link": "https://arxiv.org/abs/2403.14714",
    "context": "Title: Compiler generated feedback for Large Language Models\nAbstract: arXiv:2403.14714v1 Announce Type: cross  Abstract: We introduce a novel paradigm in compiler optimization powered by Large Language Models with compiler feedback to optimize the code size of LLVM assembly. The model takes unoptimized LLVM IR as input and produces optimized IR, the best optimization passes, and instruction counts of both unoptimized and optimized IRs. Then we compile the input with generated optimization passes and evaluate if the predicted instruction count is correct, generated IR is compilable, and corresponds to compiled code. We provide this feedback back to LLM and give it another chance to optimize code. This approach adds an extra 0.53% improvement over -Oz to the original model. Even though, adding more information with feedback seems intuitive, simple sampling techniques achieve much higher performance given 10 or more samples.",
    "path": "papers/24/03/2403.14714.json",
    "total_tokens": 750,
    "translated_title": "编译器生成的大型语言模型反馈",
    "translated_abstract": "我们引入了一种新颖的编译器优化范式，由大型语言模型提供编译器反馈，以优化LLVM汇编代码的大小。该模型以未优化的LLVM IR作为输入，生成优化的IR、最佳优化传递以及未优化和优化IR的指令计数。然后我们使用生成的优化传递编译输入，并评估预测的指令计数是否正确，生成的IR是否可编译，并且与编译后的代码相对应。我们将这些反馈发送回LLM，让其再次优化代码。该方法比原模型的-Oz额外提升了0.53%。尽管添加更多反馈信息似乎直观，但简单的抽样技术在采样10次或更多次时能够实现更高的性能。",
    "tldr": "该研究提出了一种利用大型语言模型进行编译器优化反馈的新范式，能够在优化LLVM汇编代码大小方面取得额外改进。",
    "en_tdlr": "This study introduces a novel paradigm utilizing Large Language Models for compiler optimization feedback, leading to additional improvement in optimizing the code size of LLVM assembly."
}