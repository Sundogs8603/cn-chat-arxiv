{
    "title": "Safety Cases: Justifying the Safety of Advanced AI Systems",
    "abstract": "arXiv:2403.10462v1 Announce Type: cross  Abstract: As AI systems become more advanced, companies and regulators will make difficult decisions about whether it is safe to train and deploy them. To prepare for these decisions, we investigate how developers could make a 'safety case,' which is a structured rationale that AI systems are unlikely to cause a catastrophe. We propose a framework for organizing a safety case and discuss four categories of arguments to justify safety: total inability to cause a catastrophe, sufficiently strong control measures, trustworthiness despite capability to cause harm, and deference to credible AI advisors. We evaluate concrete examples of arguments in each category and outline how arguments could be combined to justify that AI systems are safe to deploy.",
    "link": "https://arxiv.org/abs/2403.10462",
    "context": "Title: Safety Cases: Justifying the Safety of Advanced AI Systems\nAbstract: arXiv:2403.10462v1 Announce Type: cross  Abstract: As AI systems become more advanced, companies and regulators will make difficult decisions about whether it is safe to train and deploy them. To prepare for these decisions, we investigate how developers could make a 'safety case,' which is a structured rationale that AI systems are unlikely to cause a catastrophe. We propose a framework for organizing a safety case and discuss four categories of arguments to justify safety: total inability to cause a catastrophe, sufficiently strong control measures, trustworthiness despite capability to cause harm, and deference to credible AI advisors. We evaluate concrete examples of arguments in each category and outline how arguments could be combined to justify that AI systems are safe to deploy.",
    "path": "papers/24/03/2403.10462.json",
    "total_tokens": 837,
    "translated_title": "安全案例：证明先进人工智能系统的安全性",
    "translated_abstract": "随着人工智能系统变得更加先进，企业和监管机构将面临关于是否安全进行训练和部署的困难决策。为了为这些决策做准备，我们研究了开发人员如何制定一种\"安全案例\"，这是一种结构化的理由，证明人工智能系统不太可能造成灾难。我们提出了一个组织安全案例的框架，并讨论了四类论证安全的论点：完全无法造成灾难，足够强大的控制措施，尽管能够造成伤害仍值得信赖，以及对可信的人工智能顾问的尊重。我们评估了每个类别中的具体论点示例，并概述了如何组合论点来证明人工智能系统可以安全部署。",
    "tldr": "本研究提出了一个安全案例框架来组织人工智能系统的安全性论证，包括四类论点：完全无法造成灾难，强大的控制措施，尽管有可能造成伤害，但依然可信赖，以及对可信的人工智能顾问的尊重。",
    "en_tdlr": "This study proposes a safety case framework for organizing the safety justification of AI systems, including four categories of arguments: total inability to cause a catastrophe, strong control measures, trustworthiness despite the capability to cause harm, and deference to credible AI advisors."
}