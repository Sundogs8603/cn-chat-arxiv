{
    "title": "Horizon-Free Regret for Linear Markov Decision Processes",
    "abstract": "arXiv:2403.10738v1 Announce Type: new  Abstract: A recent line of works showed regret bounds in reinforcement learning (RL) can be (nearly) independent of planning horizon, a.k.a.~the horizon-free bounds. However, these regret bounds only apply to settings where a polynomial dependency on the size of transition model is allowed, such as tabular Markov Decision Process (MDP) and linear mixture MDP. We give the first horizon-free bound for the popular linear MDP setting where the size of the transition model can be exponentially large or even uncountable. In contrast to prior works which explicitly estimate the transition model and compute the inhomogeneous value functions at different time steps, we directly estimate the value functions and confidence sets. We obtain the horizon-free bound by: (1) maintaining multiple weighted least square estimators for the value functions; and (2) a structural lemma which shows the maximal total variation of the inhomogeneous value functions is bounde",
    "link": "https://arxiv.org/abs/2403.10738",
    "context": "Title: Horizon-Free Regret for Linear Markov Decision Processes\nAbstract: arXiv:2403.10738v1 Announce Type: new  Abstract: A recent line of works showed regret bounds in reinforcement learning (RL) can be (nearly) independent of planning horizon, a.k.a.~the horizon-free bounds. However, these regret bounds only apply to settings where a polynomial dependency on the size of transition model is allowed, such as tabular Markov Decision Process (MDP) and linear mixture MDP. We give the first horizon-free bound for the popular linear MDP setting where the size of the transition model can be exponentially large or even uncountable. In contrast to prior works which explicitly estimate the transition model and compute the inhomogeneous value functions at different time steps, we directly estimate the value functions and confidence sets. We obtain the horizon-free bound by: (1) maintaining multiple weighted least square estimators for the value functions; and (2) a structural lemma which shows the maximal total variation of the inhomogeneous value functions is bounde",
    "path": "papers/24/03/2403.10738.json",
    "total_tokens": 917,
    "translated_title": "无视规划时Horizon的线性马尔可夫决策过程的遗憾",
    "translated_abstract": "一系列最近的研究表明在强化学习（RL）中的遗憾界限可以（几乎）独立于规划时Horizon，也就是无视Horizon的界限。然而，这些遗憾界限仅适用于允许对转换模型大小进行多项式依赖的设置，例如表格型马尔可夫决策过程（MDP）和线性混合MDP。我们为流行的线性MDP设置给出了第一个无视Horizon的界限，其中转换模型的大小可以是指数级大甚至不可数。与先前的工作不同，先前的工作明确估计转换模型并计算不同时间步长的非齐次值函数，我们直接估计值函数和置信区间。我们通过：（1）维护用于值函数的多个加权最小二乘估计器；以及（2）一个结构引理来获得无视Horizon的界限，该引理表明非齐次值函数的最大总变异是有界的。",
    "tldr": "该论文提出了第一个适用于线性马尔可夫决策过程的无视规划时Horizon的界限，与先前的方法相比，直接估计值函数和置信区间，避免显式估计转换模型和计算不同时间步长的非齐次值函数。",
    "en_tdlr": "This paper presents the first horizon-free bound for linear Markov Decision Processes, where direct estimation of value functions and confidence sets is used instead of explicitly estimating the transition model and computing inhomogeneous value functions at different time steps."
}