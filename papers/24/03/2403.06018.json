{
    "title": "Few-Shot Cross-Lingual Transfer for Prompting Large Language Models in Low-Resource Languages",
    "abstract": "arXiv:2403.06018v1 Announce Type: cross  Abstract: Large pre-trained language models (PLMs) are at the forefront of advances in Natural Language Processing. One widespread use case of PLMs is \"prompting\" - or in-context learning - where a user provides a description of a task and some completed examples of the task to a PLM as context before prompting the PLM to perform the task on a new example. Only the largest, most capable PLMs are able to perform in-context learning effectively, and these models are typically trained with a predominantly English corpus, leaving all other languages behind. The data limitations in most languages preclude the training of language-specific PLMs capable of prompting. Albeit the surge in work of prompting settings, it is still unclear how PLMs should be adapted cross-lingually specifically for prompting. We evaluate the possible methods to adapt LLaMa, a 7B parameter open-source PLM mainly trained in English, for prompting in low-resource languages, nam",
    "link": "https://arxiv.org/abs/2403.06018",
    "context": "Title: Few-Shot Cross-Lingual Transfer for Prompting Large Language Models in Low-Resource Languages\nAbstract: arXiv:2403.06018v1 Announce Type: cross  Abstract: Large pre-trained language models (PLMs) are at the forefront of advances in Natural Language Processing. One widespread use case of PLMs is \"prompting\" - or in-context learning - where a user provides a description of a task and some completed examples of the task to a PLM as context before prompting the PLM to perform the task on a new example. Only the largest, most capable PLMs are able to perform in-context learning effectively, and these models are typically trained with a predominantly English corpus, leaving all other languages behind. The data limitations in most languages preclude the training of language-specific PLMs capable of prompting. Albeit the surge in work of prompting settings, it is still unclear how PLMs should be adapted cross-lingually specifically for prompting. We evaluate the possible methods to adapt LLaMa, a 7B parameter open-source PLM mainly trained in English, for prompting in low-resource languages, nam",
    "path": "papers/24/03/2403.06018.json",
    "total_tokens": 850,
    "translated_title": "少样本跨语言迁移用于在低资源语言中提示大型语言模型",
    "translated_abstract": "大型预训练语言模型（PLMs）处于自然语言处理进展的前沿。PLMs的一个广泛应用是“提示” - 或上下文学习 - 用户在提示PLM对新示例执行任务之前向PLM提供任务描述和一些完成的示例作为上下文。目前只有最大、最有能力的PLMs才能有效地执行上下文学习，而这些模型通常是通过主要以英语为语料库训练的，其他所有语言都落后。大多数语言的数据限制阻碍了训练具有提示能力的语言特定PLMs。尽管在提示设置方面的工作激增，目前仍不清楚如何将PLMs专门用于跨语言适应提示。我们评估了适应LLaMa的可能方法，LLaMa是一个主要在英语中训练的具有70亿参数的开源PLM，用于在低资源语言中进行提示。",
    "tldr": "本研究评估了如何将具有70亿参数的开源PLM LLaMa 用于低资源语言的提示，解决了跨语言适应提示的问题。",
    "en_tdlr": "This study evaluates how to adapt the open-source PLM LLaMa with 7 billion parameters for prompting in low-resource languages, addressing the issue of cross-lingual transfer for prompting."
}