{
    "title": "LHMKE: A Large-scale Holistic Multi-subject Knowledge Evaluation Benchmark for Chinese Large Language Models",
    "abstract": "arXiv:2403.12601v1 Announce Type: new  Abstract: Chinese Large Language Models (LLMs) have recently demonstrated impressive capabilities across various NLP benchmarks and real-world applications. However, the existing benchmarks for comprehensively evaluating these LLMs are still insufficient, particularly in terms of measuring knowledge that LLMs capture. Current datasets collect questions from Chinese examinations across different subjects and educational levels to address this issue. Yet, these benchmarks primarily focus on objective questions such as multiple-choice questions, leading to a lack of diversity in question types. To tackle this problem, we propose LHMKE, a Large-scale, Holistic, and Multi-subject Knowledge Evaluation benchmark in this paper. LHMKE is designed to provide a comprehensive evaluation of the knowledge acquisition capabilities of Chinese LLMs. It encompasses 10,465 questions across 75 tasks covering 30 subjects, ranging from primary school to professional ce",
    "link": "https://arxiv.org/abs/2403.12601",
    "context": "Title: LHMKE: A Large-scale Holistic Multi-subject Knowledge Evaluation Benchmark for Chinese Large Language Models\nAbstract: arXiv:2403.12601v1 Announce Type: new  Abstract: Chinese Large Language Models (LLMs) have recently demonstrated impressive capabilities across various NLP benchmarks and real-world applications. However, the existing benchmarks for comprehensively evaluating these LLMs are still insufficient, particularly in terms of measuring knowledge that LLMs capture. Current datasets collect questions from Chinese examinations across different subjects and educational levels to address this issue. Yet, these benchmarks primarily focus on objective questions such as multiple-choice questions, leading to a lack of diversity in question types. To tackle this problem, we propose LHMKE, a Large-scale, Holistic, and Multi-subject Knowledge Evaluation benchmark in this paper. LHMKE is designed to provide a comprehensive evaluation of the knowledge acquisition capabilities of Chinese LLMs. It encompasses 10,465 questions across 75 tasks covering 30 subjects, ranging from primary school to professional ce",
    "path": "papers/24/03/2403.12601.json",
    "total_tokens": 775,
    "translated_title": "LHMKE：面向中文大型语言模型的大规模整体多学科知识评估基准",
    "translated_abstract": "最近，中文大型语言模型（LLMs）在各种自然语言处理基准和实际应用中展示出令人印象深刻的能力。然而，目前用于全面评估这些LLMs的基准仍然不足，尤其在衡量LLMs捕捉的知识方面。为了解决这个问题，我们在本文中提出了LHMKE，一个面向中文大型语言模型的大规模、整体和多学科知识评估基准。 LHMKE旨在全面评估中文LLMs的知识获取能力。它包括了来自30个学科的75个任务的10,465个问题，涵盖从小学到专业水平。",
    "tldr": "LHMKE是一个面向中文大型语言模型的大规模、整体和多学科知识评估基准，旨在全面评估这些模型的知识获取能力。",
    "en_tdlr": "LHMKE is a large-scale, holistic, and multi-subject knowledge evaluation benchmark for Chinese large language models, aiming to provide a comprehensive evaluation of these models' knowledge acquisition capabilities."
}