{
    "title": "Semiparametric Token-Sequence Co-Supervision",
    "abstract": "arXiv:2403.09024v1 Announce Type: cross  Abstract: In this work, we introduce a semiparametric token-sequence co-supervision training method. It trains a language model by simultaneously leveraging supervision from the traditional next token prediction loss which is calculated over the parametric token embedding space and the next sequence prediction loss which is calculated over the nonparametric sequence embedding space. The nonparametric sequence embedding space is constructed by a separate language model tasked to condense an input text into a single representative embedding. Our experiments demonstrate that a model trained via both supervisions consistently surpasses models trained via each supervision independently. Analysis suggests that this co-supervision encourages a broader generalization capability across the model. Especially, the robustness of parametric token space which is established during the pretraining step tends to effectively enhance the stability of nonparametri",
    "link": "https://arxiv.org/abs/2403.09024",
    "context": "Title: Semiparametric Token-Sequence Co-Supervision\nAbstract: arXiv:2403.09024v1 Announce Type: cross  Abstract: In this work, we introduce a semiparametric token-sequence co-supervision training method. It trains a language model by simultaneously leveraging supervision from the traditional next token prediction loss which is calculated over the parametric token embedding space and the next sequence prediction loss which is calculated over the nonparametric sequence embedding space. The nonparametric sequence embedding space is constructed by a separate language model tasked to condense an input text into a single representative embedding. Our experiments demonstrate that a model trained via both supervisions consistently surpasses models trained via each supervision independently. Analysis suggests that this co-supervision encourages a broader generalization capability across the model. Especially, the robustness of parametric token space which is established during the pretraining step tends to effectively enhance the stability of nonparametri",
    "path": "papers/24/03/2403.09024.json",
    "total_tokens": 843,
    "translated_title": "半参数令牌序列共监督",
    "translated_abstract": "在这项工作中，我们引入了一种半参数令牌序列共监督训练方法。该方法通过同时利用传统的基于参数化令牌嵌入空间计算的下一个令牌预测损失和基于非参数化序列嵌入空间计算的下一个序列预测损失来训练语言模型。非参数序列嵌入空间是由一个单独的语言模型构建的，其任务是将输入文本压缩成一个单一的代表性嵌入。我们的实验表明，通过这两种监督训练的模型始终优于单独通过每种监督训练的模型。分析表明，这种共监督鼓励模型具有更广泛的泛化能力。特别是，在预训练步骤中建立的参数化标记空间的鲁棒性倾向于有效增强非参数化的稳定性。",
    "tldr": "引入了一种半参数令牌序列共监督训练方法，通过同时利用传统的下一个令牌预测损失和下一个序列预测损失来训练语言模型，实验结果显示这种方法能够提高模型的泛化能力。",
    "en_tdlr": "Introduced a semiparametric token-sequence co-supervision training method that improves the generalization capability of the model by leveraging both traditional next token prediction loss and next sequence prediction loss in the training process."
}