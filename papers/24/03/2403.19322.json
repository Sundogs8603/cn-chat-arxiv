{
    "title": "Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models",
    "abstract": "arXiv:2403.19322v1 Announce Type: cross  Abstract: The surge of Multimodal Large Language Models (MLLMs), given their prominent emergent capabilities in instruction following and reasoning, has greatly advanced the field of visual reasoning. However, constrained by their non-lossless image tokenization, most MLLMs fall short of comprehensively capturing details of text and objects, especially in high-resolution images. To address this, we propose P2G, a novel framework for plug-and-play grounding of reasoning in MLLMs. Specifically, P2G exploits the tool-usage potential of MLLMs to employ expert agents to achieve on-the-fly grounding to critical visual and textual objects of image, thus achieving deliberate reasoning via multimodal prompting. We further create P2GB, a benchmark aimed at assessing MLLMs' ability to understand inter-object relationships and text in challenging high-resolution images. Comprehensive experiments on visual reasoning tasks demonstrate the superiority of P2G. ",
    "link": "https://arxiv.org/abs/2403.19322",
    "context": "Title: Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models\nAbstract: arXiv:2403.19322v1 Announce Type: cross  Abstract: The surge of Multimodal Large Language Models (MLLMs), given their prominent emergent capabilities in instruction following and reasoning, has greatly advanced the field of visual reasoning. However, constrained by their non-lossless image tokenization, most MLLMs fall short of comprehensively capturing details of text and objects, especially in high-resolution images. To address this, we propose P2G, a novel framework for plug-and-play grounding of reasoning in MLLMs. Specifically, P2G exploits the tool-usage potential of MLLMs to employ expert agents to achieve on-the-fly grounding to critical visual and textual objects of image, thus achieving deliberate reasoning via multimodal prompting. We further create P2GB, a benchmark aimed at assessing MLLMs' ability to understand inter-object relationships and text in challenging high-resolution images. Comprehensive experiments on visual reasoning tasks demonstrate the superiority of P2G. ",
    "path": "papers/24/03/2403.19322.json",
    "total_tokens": 950,
    "translated_title": "在多模式大型语言模型中实现即插即用的推理基础",
    "translated_abstract": "随着多模式大型语言模型（MLLMs）的兴起，由于其在指令遵循和推理方面突出的新功能，这些模型极大地推动了视觉推理领域的发展。然而，受到其非无损图像标记化的限制，大多数MLLMs在全面捕捉文本和对象细节方面存在不足，尤其是在高分辨率图像中。为解决这一问题，我们提出了P2G，一种用于在MLLMs中实现即插即用推理基础的新框架。具体而言，P2G利用MLLMs的工具使用潜力，利用专家代理来实现对图像的关键视觉和文本对象的即时确定性基础，从而通过多模式提示实现有意识的推理。我们进一步创建了P2GB，一个旨在评估MLLMs在理解具有挑战性的高分辨率图像中的物体间关系和文本能力的基准。对视觉推理任务的全面实验表明了P2G的优越性。",
    "tldr": "提出了一种用于在多模式大型语言模型中实现即插即用推理基础的新框架P2G，通过利用MLLMs的工具使用潜力和专家代理实现对图像关键视觉和文本对象的即时确定性基础，从而实现有意识的推理。"
}