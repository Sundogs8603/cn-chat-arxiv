{
    "title": "Accelerating Greedy Coordinate Gradient via Probe Sampling",
    "abstract": "arXiv:2403.01251v1 Announce Type: new  Abstract: Safety of Large Language Models (LLMs) has become a central issue given their rapid progress and wide applications. Greedy Coordinate Gradient (GCG) is shown to be effective in constructing prompts containing adversarial suffixes to break the presumingly safe LLMs, but the optimization of GCG is time-consuming and limits its practicality. To reduce the time cost of GCG and enable more comprehensive studies of LLM safety, in this work, we study a new algorithm called $\\texttt{Probe sampling}$ to accelerate the GCG algorithm. At the core of the algorithm is a mechanism that dynamically determines how similar a smaller draft model's predictions are to the target model's predictions for prompt candidates. When the target model is similar to the draft model, we rely heavily on the draft model to filter out a large number of potential prompt candidates to reduce the computation time. Probe sampling achieves up to $5.6$ times speedup using Llam",
    "link": "https://arxiv.org/abs/2403.01251",
    "context": "Title: Accelerating Greedy Coordinate Gradient via Probe Sampling\nAbstract: arXiv:2403.01251v1 Announce Type: new  Abstract: Safety of Large Language Models (LLMs) has become a central issue given their rapid progress and wide applications. Greedy Coordinate Gradient (GCG) is shown to be effective in constructing prompts containing adversarial suffixes to break the presumingly safe LLMs, but the optimization of GCG is time-consuming and limits its practicality. To reduce the time cost of GCG and enable more comprehensive studies of LLM safety, in this work, we study a new algorithm called $\\texttt{Probe sampling}$ to accelerate the GCG algorithm. At the core of the algorithm is a mechanism that dynamically determines how similar a smaller draft model's predictions are to the target model's predictions for prompt candidates. When the target model is similar to the draft model, we rely heavily on the draft model to filter out a large number of potential prompt candidates to reduce the computation time. Probe sampling achieves up to $5.6$ times speedup using Llam",
    "path": "papers/24/03/2403.01251.json",
    "total_tokens": 903,
    "translated_title": "通过探查采样加速贪婪坐标梯度",
    "translated_abstract": "大型语言模型（LLMs）的安全性已成为一个中心问题，考虑到它们的快速发展和广泛应用。研究表明，贪婪坐标梯度（GCG）在构建包含对抗后缀的提示时非常有效，以破坏被认为是安全的LLMs，但GCG的优化耗时较长，限制了其实用性。为了减少GCG的时间成本并实现对LLMs安全性更全面的研究，在这项工作中，我们研究了一种称为“探查采样”的新算法，以加速GCG算法。该算法的核心是一种机制，动态确定较小草稿模型的预测与目标模型的提示候选预测的相似程度。当目标模型与草稿模型相似时，我们大量依赖于草稿模型来过滤大量潜在提示候选，以减少计算时间。探查采样使用Llam实现高达5.6倍的加速。",
    "tldr": "研究引入了一种名为“探查采样”的新算法，通过动态确定草稿模型和目标模型的相似度，来加速贪婪坐标梯度算法，实现高达5.6倍的加速。",
    "en_tdlr": "The study introduces a new algorithm called \"Probe sampling\" to accelerate the Greedy Coordinate Gradient algorithm by dynamically determining the similarity between a smaller draft model and the target model, achieving up to 5.6 times speedup."
}