{
    "title": "CYCLE: Learning to Self-Refine the Code Generation",
    "abstract": "arXiv:2403.18746v1 Announce Type: cross  Abstract: Pre-trained code language models have achieved promising performance in code generation and improved the programming efficiency of human developers. However, their self-refinement capability is typically overlooked by the existing evaluations of code LMs, which focus only on the accuracy of the one-time prediction. For the cases when code LMs fail to implement the correct program, developers actually find it hard to debug and fix the faulty prediction since it is not written by the developers themselves. Unfortunately, our study reveals that code LMs cannot efficiently self-refine their faulty generations as well.   In this paper, we propose CYCLE framework, learning to self-refine the faulty generation according to the available feedback, such as the execution results reported by the test suites. We evaluate CYCLE on three popular code generation benchmarks, HumanEval, MBPP, and APPS. The results reveal that CYCLE successfully maintai",
    "link": "https://arxiv.org/abs/2403.18746",
    "context": "Title: CYCLE: Learning to Self-Refine the Code Generation\nAbstract: arXiv:2403.18746v1 Announce Type: cross  Abstract: Pre-trained code language models have achieved promising performance in code generation and improved the programming efficiency of human developers. However, their self-refinement capability is typically overlooked by the existing evaluations of code LMs, which focus only on the accuracy of the one-time prediction. For the cases when code LMs fail to implement the correct program, developers actually find it hard to debug and fix the faulty prediction since it is not written by the developers themselves. Unfortunately, our study reveals that code LMs cannot efficiently self-refine their faulty generations as well.   In this paper, we propose CYCLE framework, learning to self-refine the faulty generation according to the available feedback, such as the execution results reported by the test suites. We evaluate CYCLE on three popular code generation benchmarks, HumanEval, MBPP, and APPS. The results reveal that CYCLE successfully maintai",
    "path": "papers/24/03/2403.18746.json",
    "total_tokens": 744,
    "translated_title": "CYCLE: 学习自我调整代码生成",
    "translated_abstract": "预训练的代码语言模型在代码生成中取得了有希望的表现，并提高了人类开发者的编程效率。然而，现有对代码语言模型的评估通常忽视了它们的自我调整能力，而是只关注一次性预测的准确性。在代码语言模型未能实现正确程序的情况下，开发者实际上很难调试和修复错误的预测，因为这些预测不是由开发者自己编写的。不幸的是，我们的研究表明，代码语言模型也无法高效地自我调整其错误的生成。",
    "tldr": "CYCLE框架提出了学习如何根据可用反馈，如测试套件报告的执行结果，自我调整错误生成的方法，成功地解决了代码语言模型无法自我调整的问题"
}