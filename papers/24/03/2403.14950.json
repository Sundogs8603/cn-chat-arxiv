{
    "title": "KnowLA: Enhancing Parameter-efficient Finetuning with Knowledgeable Adaptation",
    "abstract": "arXiv:2403.14950v1 Announce Type: new  Abstract: Parameter-efficient finetuning (PEFT) is a key technique for adapting large language models (LLMs) to downstream tasks. In this paper, we study leveraging knowledge graph embeddings to improve the effectiveness of PEFT. We propose a knowledgeable adaptation method called KnowLA. It inserts an adaptation layer into an LLM to integrate the embeddings of entities appearing in the input text. The adaptation layer is trained in combination with LoRA on instruction data. Experiments on six benchmarks with two popular LLMs and three knowledge graphs demonstrate the effectiveness and robustness of KnowLA. We show that \\modelname can help activate the relevant parameterized knowledge in an LLM to answer a question without changing its parameters or input prompts.",
    "link": "https://arxiv.org/abs/2403.14950",
    "context": "Title: KnowLA: Enhancing Parameter-efficient Finetuning with Knowledgeable Adaptation\nAbstract: arXiv:2403.14950v1 Announce Type: new  Abstract: Parameter-efficient finetuning (PEFT) is a key technique for adapting large language models (LLMs) to downstream tasks. In this paper, we study leveraging knowledge graph embeddings to improve the effectiveness of PEFT. We propose a knowledgeable adaptation method called KnowLA. It inserts an adaptation layer into an LLM to integrate the embeddings of entities appearing in the input text. The adaptation layer is trained in combination with LoRA on instruction data. Experiments on six benchmarks with two popular LLMs and three knowledge graphs demonstrate the effectiveness and robustness of KnowLA. We show that \\modelname can help activate the relevant parameterized knowledge in an LLM to answer a question without changing its parameters or input prompts.",
    "path": "papers/24/03/2403.14950.json",
    "total_tokens": 772,
    "translated_title": "KnowLA：利用知识自适应提升参数高效微调",
    "translated_abstract": "参数高效微调（PEFT）是调整大型语言模型（LLMs）以适应下游任务的关键技术。本文研究了利用知识图嵌入来改善PEFT的有效性。我们提出了一种称为KnowLA的知识自适应方法。它在LLM中插入一个自适应层，将输入文本中出现的实体的嵌入整合在一起。自适应层与LoRA在指导数据上组合训练。在两个流行的LLMs和三个知识图上进行的六项基准实验表明了KnowLA的有效性和鲁棒性。我们展示了 \\modelname 能够帮助激活LLM中的相关参数化知识以回答问题，而不改变其参数或输入提示。",
    "tldr": "该论文提出了一种名为KnowLA的知识自适应方法，通过在大型语言模型中插入自适应层和知识图嵌入，能够提升参数高效微调的有效性和鲁棒性。",
    "en_tdlr": "This paper introduces a knowledgeable adaptation method called KnowLA, which enhances the effectiveness and robustness of parameter-efficient finetuning by inserting an adaptation layer and knowledge graph embeddings into large language models."
}