{
    "title": "VisionCLIP: An Med-AIGC based Ethical Language-Image Foundation Model for Generalizable Retina Image Analysis",
    "abstract": "arXiv:2403.10823v1 Announce Type: cross  Abstract: Generalist foundation model has ushered in newfound capabilities in medical domain. However, the contradiction between the growing demand for high-quality annotated data with patient privacy continues to intensify. The utilization of medical artificial intelligence generated content (Med-AIGC) as an inexhaustible resource repository arises as a potential solution to address the aforementioned challenge. Here we harness 1 million open-source synthetic fundus images paired with natural language descriptions, to curate an ethical language-image foundation model for retina image analysis named VisionCLIP. VisionCLIP achieves competitive performance on three external datasets compared with the existing method pre-trained on real-world data in a zero-shot fashion. The employment of artificially synthetic images alongside corresponding textual data for training enables the medical foundation model to successfully assimilate knowledge of disea",
    "link": "https://arxiv.org/abs/2403.10823",
    "context": "Title: VisionCLIP: An Med-AIGC based Ethical Language-Image Foundation Model for Generalizable Retina Image Analysis\nAbstract: arXiv:2403.10823v1 Announce Type: cross  Abstract: Generalist foundation model has ushered in newfound capabilities in medical domain. However, the contradiction between the growing demand for high-quality annotated data with patient privacy continues to intensify. The utilization of medical artificial intelligence generated content (Med-AIGC) as an inexhaustible resource repository arises as a potential solution to address the aforementioned challenge. Here we harness 1 million open-source synthetic fundus images paired with natural language descriptions, to curate an ethical language-image foundation model for retina image analysis named VisionCLIP. VisionCLIP achieves competitive performance on three external datasets compared with the existing method pre-trained on real-world data in a zero-shot fashion. The employment of artificially synthetic images alongside corresponding textual data for training enables the medical foundation model to successfully assimilate knowledge of disea",
    "path": "papers/24/03/2403.10823.json",
    "total_tokens": 836,
    "translated_title": "VisionCLIP：一种基于Med-AIGC的伦理语言-图像基础模型，用于通用性视网膜图像分析",
    "translated_abstract": "通用基础模型在医学领域引入了新的能力。然而，高质量注释数据需求与患者隐私之间的矛盾持续加剧。利用医学人工智能生成的内容（Med-AIGC）作为一个源源不断的资源库，成为解决上述挑战的潜在解决方案。本文利用100万个开源合成底片图像与自然语言描述，策划了一种名为VisionCLIP的伦理语言-图像基础模型，用于视网膜图像分析。VisionCLIP以零-shot方式在三个外部数据集上实现了与现有方法在真实数据上预训练的竞争性性能。在训练过程中同时使用人工合成图像和相应的文本数据使得这个医学基础模型成功地吸收了疾病知识。",
    "tldr": "VisionCLIP利用Med-AIGC生成的合成数据，并结合自然语言描述，建立了一种伦理语言-图像模型，在视网膜图像分析中取得了竞争性表现。",
    "en_tdlr": "VisionCLIP leverages synthetic data generated by Med-AIGC, combined with natural language descriptions, to establish an ethical language-image model that achieves competitive performance in retinal image analysis."
}