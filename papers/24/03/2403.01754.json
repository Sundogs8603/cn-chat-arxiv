{
    "title": "Derivative-Free Optimization for Low-Rank Adaptation in Large Language Models",
    "abstract": "arXiv:2403.01754v1 Announce Type: new  Abstract: Parameter-efficient tuning methods such as LoRA could achieve comparable performance to model tuning by tuning a small portion of the parameters. However, substantial computational resources are still required, as this process involves calculating gradients and performing back-propagation throughout the model. Much effort has recently been devoted to utilizing the derivative-free optimization method to eschew the computation of gradients and showcase an augmented level of robustness in few-shot settings. In this paper, we prepend the low-rank modules into each self-attention layer of the model and employ two derivative-free optimization methods to optimize these low-rank modules at each layer alternately. Extensive results on various tasks and language models demonstrate that our proposed method achieves substantial improvement and exhibits clear advantages in memory usage and convergence speed compared to existing gradient-based paramet",
    "link": "https://arxiv.org/abs/2403.01754",
    "context": "Title: Derivative-Free Optimization for Low-Rank Adaptation in Large Language Models\nAbstract: arXiv:2403.01754v1 Announce Type: new  Abstract: Parameter-efficient tuning methods such as LoRA could achieve comparable performance to model tuning by tuning a small portion of the parameters. However, substantial computational resources are still required, as this process involves calculating gradients and performing back-propagation throughout the model. Much effort has recently been devoted to utilizing the derivative-free optimization method to eschew the computation of gradients and showcase an augmented level of robustness in few-shot settings. In this paper, we prepend the low-rank modules into each self-attention layer of the model and employ two derivative-free optimization methods to optimize these low-rank modules at each layer alternately. Extensive results on various tasks and language models demonstrate that our proposed method achieves substantial improvement and exhibits clear advantages in memory usage and convergence speed compared to existing gradient-based paramet",
    "path": "papers/24/03/2403.01754.json",
    "total_tokens": 817,
    "translated_title": "大型语言模型的低秩适应性无导数优化",
    "translated_abstract": "LoRA等参数高效调整方法可以通过调整部分参数实现与模型调优性能相媲美，但仍需要大量计算资源，因为这一过程涉及计算梯度并在整个模型中执行反向传播。最近，许多工作致力于利用无导数优化方法，避免计算梯度，并展示在少样本设置中增强的鲁棒性。本文在模型的每个自注意力层前置低秩模块，并采用两种无导数优化方法交替优化每层的这些低秩模块。对各种任务和语言模型的广泛结果表明，我们提出的方法取得了实质性改进，并在内存使用和收敛速度上相比现有基于梯度的参数具有明显优势。",
    "tldr": "本文将低秩模块添加到模型的每个自注意层中，并采用两种无导数优化方法交替优化这些低秩模块，相比现有基于梯度的参数调整方法，我们的方法在内存使用和收敛速度上显示出明显优势。"
}