{
    "title": "Error bounds for particle gradient descent, and extensions of the log-Sobolev and Talagrand inequalities",
    "abstract": "arXiv:2403.02004v1 Announce Type: new  Abstract: We prove non-asymptotic error bounds for particle gradient descent (PGD)~(Kuntz et al., 2023), a recently introduced algorithm for maximum likelihood estimation of large latent variable models obtained by discretizing a gradient flow of the free energy. We begin by showing that, for models satisfying a condition generalizing both the log-Sobolev and the Polyak--{\\L}ojasiewicz inequalities (LSI and P{\\L}I, respectively), the flow converges exponentially fast to the set of minimizers of the free energy. We achieve this by extending a result well-known in the optimal transport literature (that the LSI implies the Talagrand inequality) and its counterpart in the optimization literature (that the P{\\L}I implies the so-called quadratic growth condition), and applying it to our new setting. We also generalize the Bakry--\\'Emery Theorem and show that the LSI/P{\\L}I generalization holds for models with strongly concave log-likelihoods. For such m",
    "link": "https://arxiv.org/abs/2403.02004",
    "context": "Title: Error bounds for particle gradient descent, and extensions of the log-Sobolev and Talagrand inequalities\nAbstract: arXiv:2403.02004v1 Announce Type: new  Abstract: We prove non-asymptotic error bounds for particle gradient descent (PGD)~(Kuntz et al., 2023), a recently introduced algorithm for maximum likelihood estimation of large latent variable models obtained by discretizing a gradient flow of the free energy. We begin by showing that, for models satisfying a condition generalizing both the log-Sobolev and the Polyak--{\\L}ojasiewicz inequalities (LSI and P{\\L}I, respectively), the flow converges exponentially fast to the set of minimizers of the free energy. We achieve this by extending a result well-known in the optimal transport literature (that the LSI implies the Talagrand inequality) and its counterpart in the optimization literature (that the P{\\L}I implies the so-called quadratic growth condition), and applying it to our new setting. We also generalize the Bakry--\\'Emery Theorem and show that the LSI/P{\\L}I generalization holds for models with strongly concave log-likelihoods. For such m",
    "path": "papers/24/03/2403.02004.json",
    "total_tokens": 904,
    "translated_title": "粒子梯度下降的误差界限，以及log-Sobolev和Talagrand不等式的推广",
    "translated_abstract": "我们证明了粒子梯度下降(PGD)~(Kuntz等人，2023)的非渐近误差界限，这是一种最大似然估计的算法，用于离散化自由能梯度流获得的大型潜变量模型。我们首先展示了对于满足一般化log-Sobolev和Polyak-Lojasiewicz不等式（LSI和PLI）的模型，流以指数速度收敛到自由能的极小化集合。我们通过将最优输运文献中众所周知的结果（LSI意味着Talagrand不等式）及其在优化文献中的对应物（PLI意味着所谓的二次增长条件）扩展并应用到我们的新设置，来实现这一点。我们还推广了Bakry-Emery定理，并展示了对于具有强凹对数似然的模型，LSI/PLI的概括成立。",
    "tldr": "证明了粒子梯度下降算法对于一般化的log-Sobolev和Polyak-Lojasiewicz不等式模型的收敛速度，以及推广了Bakry-Emery定理。",
    "en_tdlr": "Proved convergence speed of particle gradient descent algorithm for generalized log-Sobolev and Polyak-Lojasiewicz inequality models, and generalized the Bakry-Emery theorem."
}