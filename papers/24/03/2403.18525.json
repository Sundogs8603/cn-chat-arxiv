{
    "title": "Language Plays a Pivotal Role in the Object-Attribute Compositional Generalization of CLIP",
    "abstract": "arXiv:2403.18525v1 Announce Type: cross  Abstract: Vision-language models, such as CLIP, have shown promising Out-of-Distribution (OoD) generalization under various types of distribution shifts. Recent studies attempted to investigate the leading cause of this capability. In this work, we follow the same path, but focus on a specific type of OoD data - images with novel compositions of attribute-object pairs - and study whether such models can successfully classify those images into composition classes. We carefully designed an authentic image test dataset called ImageNet-AO, consisting of attributes for objects that are unlikely encountered in the CLIP training sets. We found that CLIPs trained with large datasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude improvement in effective compositional OoD generalization compared to both supervised models and CLIPs trained with smaller datasets, such as CC-12M and YFCC-15M. Our results provide evidence that the sca",
    "link": "https://arxiv.org/abs/2403.18525",
    "context": "Title: Language Plays a Pivotal Role in the Object-Attribute Compositional Generalization of CLIP\nAbstract: arXiv:2403.18525v1 Announce Type: cross  Abstract: Vision-language models, such as CLIP, have shown promising Out-of-Distribution (OoD) generalization under various types of distribution shifts. Recent studies attempted to investigate the leading cause of this capability. In this work, we follow the same path, but focus on a specific type of OoD data - images with novel compositions of attribute-object pairs - and study whether such models can successfully classify those images into composition classes. We carefully designed an authentic image test dataset called ImageNet-AO, consisting of attributes for objects that are unlikely encountered in the CLIP training sets. We found that CLIPs trained with large datasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude improvement in effective compositional OoD generalization compared to both supervised models and CLIPs trained with smaller datasets, such as CC-12M and YFCC-15M. Our results provide evidence that the sca",
    "path": "papers/24/03/2403.18525.json",
    "total_tokens": 941,
    "translated_title": "语言在CLIP对象-属性组合泛化中发挥关键作用",
    "translated_abstract": "arXiv:2403.18525v1 公告类型:跨领域 摘要:视觉-语言模型，如CLIP，在各种类型的分布变化下展现了令人鼓舞的超出分布的泛化能力。最近的研究尝试调查这种能力的主要原因。在这项工作中，我们沿用了相同的思路，但专注于特定类型的超出分布数据 - 具有新颖属性-对象对组合的图像 - 并研究这样的模型是否能够成功地将这些图像分类到组合类别中。我们精心设计了一个名为ImageNet-AO的真实图像测试数据集，其中包含了CLIP训练集中不太可能遇到的对象的属性。我们发现，通过大型数据集训练的CLIP模型（如OpenAI CLIP，LAION-400M和LAION-2B）在有效的组合超出分布泛化方面比受监督模型和通过较小数据集训练的CLIP模型（如CC-12M和YFCC-15M）表现出数量级的改进。我们的结果证明了该方法提供了关于规模、数据集和泛化之间关系的见解。",
    "tldr": "本研究发现，通过大型数据集训练的CLIP模型在对象-属性组合泛化中表现出明显优势，为泛化和数据集规模之间的关系提供了重要见解。",
    "en_tdlr": "This study found that CLIP models trained with large datasets show significant advantages in object-attribute compositional generalization, providing important insights into the relationship between generalization and dataset scale."
}