{
    "title": "Don't be a Fool: Pooling Strategies in Offensive Language Detection from User-Intended Adversarial Attacks",
    "abstract": "arXiv:2403.15467v1 Announce Type: new  Abstract: Offensive language detection is an important task for filtering out abusive expressions and improving online user experiences. However, malicious users often attempt to avoid filtering systems through the involvement of textual noises. In this paper, we propose these evasions as user-intended adversarial attacks that insert special symbols or leverage the distinctive features of the Korean language. Furthermore, we introduce simple yet effective pooling strategies in a layer-wise manner to defend against the proposed attacks, focusing on the preceding layers not just the last layer to capture both offensiveness and token embeddings. We demonstrate that these pooling strategies are more robust to performance degradation even when the attack rate is increased, without directly training of such patterns. Notably, we found that models pre-trained on clean texts could achieve a comparable performance in detecting attacked offensive language, ",
    "link": "https://arxiv.org/abs/2403.15467",
    "context": "Title: Don't be a Fool: Pooling Strategies in Offensive Language Detection from User-Intended Adversarial Attacks\nAbstract: arXiv:2403.15467v1 Announce Type: new  Abstract: Offensive language detection is an important task for filtering out abusive expressions and improving online user experiences. However, malicious users often attempt to avoid filtering systems through the involvement of textual noises. In this paper, we propose these evasions as user-intended adversarial attacks that insert special symbols or leverage the distinctive features of the Korean language. Furthermore, we introduce simple yet effective pooling strategies in a layer-wise manner to defend against the proposed attacks, focusing on the preceding layers not just the last layer to capture both offensiveness and token embeddings. We demonstrate that these pooling strategies are more robust to performance degradation even when the attack rate is increased, without directly training of such patterns. Notably, we found that models pre-trained on clean texts could achieve a comparable performance in detecting attacked offensive language, ",
    "path": "papers/24/03/2403.15467.json",
    "total_tokens": 891,
    "translated_title": "在冒犯性语言检测中的池化策略：针对用户故意的对抗攻击",
    "translated_abstract": "冒犯性语言检测是过滤辱骂表达并改善在线用户体验的重要任务。然而，恶意用户常常通过引入文本噪音来规避过滤系统。本文将这些规避行为作为用户故意的对抗攻击提出，这些攻击插入特殊符号或利用韩文的独特特征。此外，我们以逐层方式引入简单而有效的池化策略，以抵御所提出的攻击，侧重于前置层而不仅仅是最后一层以捕捉冒犯性和标记嵌入。我们展示了这些池化策略在攻击率增加时更加稳健，即使没有直接训练这样的模式。值得注意的是，我们发现在干净文本上预训练的模型在检测遭受攻击的冒犯性语言方面可以达到可比的性能。",
    "tldr": "本文提出了一种针对用户故意的对抗攻击的池化策略，通过引入逐层的简单但有效的池化策略来捕捉冒犯性和标记嵌入，使模型更加稳健，即使攻击率增加。",
    "en_tdlr": "This paper proposes pooling strategies against user-intended adversarial attacks, by introducing simple yet effective layer-wise pooling strategies to capture offensiveness and token embeddings, making the model more robust even with increased attack rates."
}