{
    "title": "Boosting Meta-Training with Base Class Information for Few-Shot Learning",
    "abstract": "arXiv:2403.03472v1 Announce Type: new  Abstract: Few-shot learning, a challenging task in machine learning, aims to learn a classifier adaptable to recognize new, unseen classes with limited labeled examples. Meta-learning has emerged as a prominent framework for few-shot learning. Its training framework is originally a task-level learning method, such as Model-Agnostic Meta-Learning (MAML) and Prototypical Networks. And a recently proposed training paradigm called Meta-Baseline, which consists of sequential pre-training and meta-training stages, gains state-of-the-art performance. However, as a non-end-to-end training method, indicating the meta-training stage can only begin after the completion of pre-training, Meta-Baseline suffers from higher training cost and suboptimal performance due to the inherent conflicts of the two training stages. To address these limitations, we propose an end-to-end training paradigm consisting of two alternative loops. In the outer loop, we calculate cr",
    "link": "https://arxiv.org/abs/2403.03472",
    "context": "Title: Boosting Meta-Training with Base Class Information for Few-Shot Learning\nAbstract: arXiv:2403.03472v1 Announce Type: new  Abstract: Few-shot learning, a challenging task in machine learning, aims to learn a classifier adaptable to recognize new, unseen classes with limited labeled examples. Meta-learning has emerged as a prominent framework for few-shot learning. Its training framework is originally a task-level learning method, such as Model-Agnostic Meta-Learning (MAML) and Prototypical Networks. And a recently proposed training paradigm called Meta-Baseline, which consists of sequential pre-training and meta-training stages, gains state-of-the-art performance. However, as a non-end-to-end training method, indicating the meta-training stage can only begin after the completion of pre-training, Meta-Baseline suffers from higher training cost and suboptimal performance due to the inherent conflicts of the two training stages. To address these limitations, we propose an end-to-end training paradigm consisting of two alternative loops. In the outer loop, we calculate cr",
    "path": "papers/24/03/2403.03472.json",
    "total_tokens": 858,
    "translated_title": "通过基础类别信息增强元训练，用于少样本学习",
    "translated_abstract": "少样本学习是机器学习中的一项具有挑战性的任务，旨在学习一个能够适应识别具有有限标记示例的新类别的分类器。元学习已经成为少样本学习的重要框架。最初的训练框架是一个任务级学习方法，如模型无关的元学习（MAML）和原型网络。最近提出的培训范式称为元Baseline，它由序贯预训练和元训练阶段组成，取得了最先进的性能。然而，作为一种非端到端的训练方法，表明元训练阶段只能在预训练完成后开始，Meta-Baseline由于两个训练阶段的固有冲突而导致更高的训练成本和次优的性能。为了解决这些限制，我们提出了一个由两个交替循环组成的端到端训练范式。在外循环中，我们计算cr",
    "tldr": "提出了一个端到端的训练范式，通过增强元训练的基础类别信息，解决了Meta-Baseline两个训练阶段固有冲突的问题",
    "en_tdlr": "Proposed an end-to-end training paradigm that addresses the inherent conflicts of Meta-Baseline's two training stages by boosting meta-training with base class information."
}