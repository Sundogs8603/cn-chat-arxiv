{
    "title": "Fusion approaches for emotion recognition from speech using acoustic and text-based features",
    "abstract": "arXiv:2403.18635v1 Announce Type: new  Abstract: In this paper, we study different approaches for classifying emotions from speech using acoustic and text-based features. We propose to obtain contextualized word embeddings with BERT to represent the information contained in speech transcriptions and show that this results in better performance than using Glove embeddings. We also propose and compare different strategies to combine the audio and text modalities, evaluating them on IEMOCAP and MSP-PODCAST datasets. We find that fusing acoustic and text-based systems is beneficial on both datasets, though only subtle differences are observed across the evaluated fusion approaches. Finally, for IEMOCAP, we show the large effect that the criteria used to define the cross-validation folds have on results. In particular, the standard way of creating folds for this dataset results in a highly optimistic estimation of performance for the text-based system, suggesting that some previous works ma",
    "link": "https://arxiv.org/abs/2403.18635",
    "context": "Title: Fusion approaches for emotion recognition from speech using acoustic and text-based features\nAbstract: arXiv:2403.18635v1 Announce Type: new  Abstract: In this paper, we study different approaches for classifying emotions from speech using acoustic and text-based features. We propose to obtain contextualized word embeddings with BERT to represent the information contained in speech transcriptions and show that this results in better performance than using Glove embeddings. We also propose and compare different strategies to combine the audio and text modalities, evaluating them on IEMOCAP and MSP-PODCAST datasets. We find that fusing acoustic and text-based systems is beneficial on both datasets, though only subtle differences are observed across the evaluated fusion approaches. Finally, for IEMOCAP, we show the large effect that the criteria used to define the cross-validation folds have on results. In particular, the standard way of creating folds for this dataset results in a highly optimistic estimation of performance for the text-based system, suggesting that some previous works ma",
    "path": "papers/24/03/2403.18635.json",
    "total_tokens": 899,
    "translated_title": "使用声学和基于文本的特征进行情感识别的融合方法研究",
    "translated_abstract": "在本文中，我们研究了使用声学和基于文本特征对语音进行情感分类的不同方法。我们提出使用BERT获取上下文化的词嵌入来表征语音转录中包含的信息，并展示这比使用Glove嵌入的性能更好。我们还提出并比较了不同策略来结合音频和文本模态，对IEMOCAP和MSP-PODCAST数据集进行评估。我们发现融合声学和基于文本的系统在两个数据集上都是有益的，尽管在评估的融合方法中观察到的差异很细微。最后，针对IEMOCAP，我们展示了用于定义交叉验证折叠的标准标准对结果的巨大影响效果。特别是，为该数据集创建折叠的标准方法导致文本系统性能的高度乐观估计，这表明一些先前的工作可能...",
    "tldr": "该研究提出了使用BERT获取上下文化的词嵌入来表征语音转录信息，并展示这比使用Glove嵌入的性能更好，同时对结合音频和文本模态的策略进行了比较，发现在两个数据集上融合声学和基于文本的系统是有益的。",
    "en_tdlr": "This study proposes using BERT to obtain contextualized word embeddings to represent speech transcriptions, showing better performance than using Glove embeddings, and compares strategies of combining audio and text modalities, finding the fusion of acoustic and text-based systems beneficial on two datasets."
}