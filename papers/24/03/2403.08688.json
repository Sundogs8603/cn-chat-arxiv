{
    "title": "Token Alignment via Character Matching for Subword Completion",
    "abstract": "arXiv:2403.08688v1 Announce Type: cross  Abstract: Generative models, widely utilized in various applications, can often struggle with prompts corresponding to partial tokens. This struggle stems from tokenization, where partial tokens fall out of distribution during inference, leading to incorrect or nonsensical outputs. This paper examines a technique to alleviate the tokenization artifact on text completion in generative models, maintaining performance even in regular non-subword cases. The method, termed token alignment, involves backtracking to the last complete tokens and ensuring the model's generation aligns with the prompt. This approach showcases marked improvement across many partial token scenarios, including nuanced cases like space-prefix and partial indentation, with only a minor time increase. The technique and analysis detailed in this paper contribute to the continuous advancement of generative models in handling partial inputs, bearing relevance for applications like",
    "link": "https://arxiv.org/abs/2403.08688",
    "context": "Title: Token Alignment via Character Matching for Subword Completion\nAbstract: arXiv:2403.08688v1 Announce Type: cross  Abstract: Generative models, widely utilized in various applications, can often struggle with prompts corresponding to partial tokens. This struggle stems from tokenization, where partial tokens fall out of distribution during inference, leading to incorrect or nonsensical outputs. This paper examines a technique to alleviate the tokenization artifact on text completion in generative models, maintaining performance even in regular non-subword cases. The method, termed token alignment, involves backtracking to the last complete tokens and ensuring the model's generation aligns with the prompt. This approach showcases marked improvement across many partial token scenarios, including nuanced cases like space-prefix and partial indentation, with only a minor time increase. The technique and analysis detailed in this paper contribute to the continuous advancement of generative models in handling partial inputs, bearing relevance for applications like",
    "path": "papers/24/03/2403.08688.json",
    "total_tokens": 821,
    "translated_title": "通过字符匹配实现标记对齐用于子词补全",
    "translated_abstract": "生成模型在各种应用中被广泛使用，但通常难以处理与部分标记对齐的提示。这种困难源自标记化，在推理过程中部分标记会脱离分布，导致不正确或荒谬的输出。本文研究了一种技术，用于减轻生成模型中文本补全时的标记化问题，即使在常规非子词情况下也能保持性能。该方法称为标记对齐，涉及回溯到最后完整标记，并确保模型生成与提示对齐。这种方法在许多部分标记场景中展示了显著的改进，包括空格前缀和部分缩进等微妙情况，仅增加了少量时间。本文详细介绍的技术和分析有助于在处理部分输入方面不断推进生成模型，对诸如的应用具有相关意义",
    "tldr": "通过字符匹配实现标记对齐的方法，显著改进了生成模型在处理部分标记对齐的情景中的性能，包括空格前缀和部分缩进等微妙情况。",
    "en_tdlr": "The method of token alignment via character matching significantly improves the performance of generative models in handling scenarios involving partial token alignment, including nuanced cases like space-prefix and partial indentation."
}