{
    "title": "Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models",
    "abstract": "arXiv:2403.00231v1 Announce Type: cross  Abstract: Large vision-language models (LVLMs), exemplified by GPT-4V, excel across diverse tasks involving concrete images from natural scenes. However, their ability to interpret abstract figures, such as geometry shapes and scientific plots, remains limited due to a scarcity of training datasets in scientific domains. To fill this gap, we introduce Multimodal ArXiv, consisting of ArXivCap and ArXivQA, for enhancing LVLMs scientific comprehension. ArXivCap is a figure-caption dataset comprising 6.4M images and 3.9M captions sourced from 572K ArXiv papers spanning various scientific domains. Drawing from ArXivCap, we introduce ArXivQA, a question-answering dataset generated by prompting GPT-4V based on scientific figures. ArXivQA greatly enhances LVLMs' mathematical reasoning capabilities, achieving a 10.4% absolute accuracy gain on a multimodal mathematical reasoning benchmark. Furthermore, employing ArXivCap, we devise four vision-to-text tas",
    "link": "https://arxiv.org/abs/2403.00231",
    "context": "Title: Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models\nAbstract: arXiv:2403.00231v1 Announce Type: cross  Abstract: Large vision-language models (LVLMs), exemplified by GPT-4V, excel across diverse tasks involving concrete images from natural scenes. However, their ability to interpret abstract figures, such as geometry shapes and scientific plots, remains limited due to a scarcity of training datasets in scientific domains. To fill this gap, we introduce Multimodal ArXiv, consisting of ArXivCap and ArXivQA, for enhancing LVLMs scientific comprehension. ArXivCap is a figure-caption dataset comprising 6.4M images and 3.9M captions sourced from 572K ArXiv papers spanning various scientific domains. Drawing from ArXivCap, we introduce ArXivQA, a question-answering dataset generated by prompting GPT-4V based on scientific figures. ArXivQA greatly enhances LVLMs' mathematical reasoning capabilities, achieving a 10.4% absolute accuracy gain on a multimodal mathematical reasoning benchmark. Furthermore, employing ArXivCap, we devise four vision-to-text tas",
    "path": "papers/24/03/2403.00231.json",
    "total_tokens": 974,
    "translated_title": "Multimodal ArXiv: 用于提升大型视觉-语言模型对科学理解的数据集",
    "translated_abstract": "大型视觉-语言模型（LVLMs），以GPT-4V为例，在涉及自然场景中的具体图像的各种任务中表现出色。然而，由于科学领域训练数据集的稀缺，它们在解释抽象图形（例如几何形状和科学图）方面的能力仍然有限。为了填补这一空白，我们介绍了Multimodal ArXiv，包括ArXivCap和ArXivQA，以增强LVLMs的科学理解。ArXivCap是一个包含来自涵盖各种科学领域的572K份ArXiv论文的6.4M张图像和3.9M个标题的图像标题数据集。借鉴ArXivCap，我们介绍了ArXivQA，这是一个通过提示GPT-4V生成的基于科学图的问答数据集。ArXivQA极大地增强了LVLMs的数学推理能力，在多模态数学推理基准上实现了10.4%的绝对准确率提升。此外，利用ArXivCap，我们设计了四个从视觉到文本的任务。",
    "tldr": "提出了Multimodal ArXiv数据集，包括ArXivCap和ArXivQA，用于增强大型视觉-语言模型对科学理解的能力，ArXivQA通过科学图生成问题，显著提高了数学推理准确率。",
    "en_tdlr": "Introduced the Multimodal ArXiv dataset, consisting of ArXivCap and ArXivQA, to enhance the scientific comprehension of large vision-language models; ArXivQA significantly improves mathematical reasoning accuracy by generating questions based on scientific figures."
}