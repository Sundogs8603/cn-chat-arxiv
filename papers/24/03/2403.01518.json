{
    "title": "Revisiting Dynamic Evaluation: Online Adaptation for Large Language Models",
    "abstract": "arXiv:2403.01518v1 Announce Type: new  Abstract: We consider the problem of online fine tuning the parameters of a language model at test time, also known as dynamic evaluation. While it is generally known that this approach improves the overall predictive performance, especially when considering distributional shift between training and evaluation data, we here emphasize the perspective that online adaptation turns parameters into temporally changing states and provides a form of context-length extension with memory in weights, more in line with the concept of memory in neuroscience. We pay particular attention to the speed of adaptation (in terms of sample efficiency),sensitivity to the overall distributional drift, and the computational overhead for performing gradient computations and parameter updates. Our empirical study provides insights on when online adaptation is particularly interesting. We highlight that with online adaptation the conceptual distinction between in-context l",
    "link": "https://arxiv.org/abs/2403.01518",
    "context": "Title: Revisiting Dynamic Evaluation: Online Adaptation for Large Language Models\nAbstract: arXiv:2403.01518v1 Announce Type: new  Abstract: We consider the problem of online fine tuning the parameters of a language model at test time, also known as dynamic evaluation. While it is generally known that this approach improves the overall predictive performance, especially when considering distributional shift between training and evaluation data, we here emphasize the perspective that online adaptation turns parameters into temporally changing states and provides a form of context-length extension with memory in weights, more in line with the concept of memory in neuroscience. We pay particular attention to the speed of adaptation (in terms of sample efficiency),sensitivity to the overall distributional drift, and the computational overhead for performing gradient computations and parameter updates. Our empirical study provides insights on when online adaptation is particularly interesting. We highlight that with online adaptation the conceptual distinction between in-context l",
    "path": "papers/24/03/2403.01518.json",
    "total_tokens": 813,
    "translated_title": "重新审视动态评估: 大型语言模型的在线调整",
    "translated_abstract": "我们考虑在线微调语言模型参数的问题，也即称为动态评估。虽然一般认为这种方法可以提高整体的预测性能，特别是在考虑训练和评估数据之间的分布转移时，我们在这里强调在线调整将参数转变为时间变化状态，并提供了一种具有内存权重记忆的上下文长度扩展形式，更符合神经科学中记忆概念的思路。我们特别关注适应速度（以样本效率衡量）、对整体分布性漂移的敏感性以及执行梯度计算和参数更新的计算负担。我们的实证研究提供了关于何时在线调整尤为有趣的见解。我们强调，在线调整使得上下文长度和内存在概念上的区分模糊化。",
    "tldr": "在线适应可以将参数转变为时间变化状态，提供一种具有内存权重记忆的上下文长度扩展形式，更符合神经科学中记忆概念，且在提升整体预测性能时尤其有趣。",
    "en_tdlr": "Online adaptation transforms parameters into temporally changing states, providing a form of context-length extension with memory in weights more in line with the concept of memory in neuroscience, and is particularly interesting in improving overall predictive performance."
}