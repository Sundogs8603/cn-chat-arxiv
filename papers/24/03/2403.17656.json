{
    "title": "SGHormer: An Energy-Saving Graph Transformer Driven by Spikes",
    "abstract": "arXiv:2403.17656v1 Announce Type: cross  Abstract: Graph Transformers (GTs) with powerful representation learning ability make a huge success in wide range of graph tasks. However, the costs behind outstanding performances of GTs are higher energy consumption and computational overhead. The complex structure and quadratic complexity during attention calculation in vanilla transformer seriously hinder its scalability on the large-scale graph data. Though existing methods have made strides in simplifying combinations among blocks or attention-learning paradigm to improve GTs' efficiency, a series of energy-saving solutions originated from biologically plausible structures are rarely taken into consideration when constructing GT framework. To this end, we propose a new spiking-based graph transformer (SGHormer). It turns full-precision embeddings into sparse and binarized spikes to reduce memory and computational costs. The spiking graph self-attention and spiking rectify blocks in SGHorm",
    "link": "https://arxiv.org/abs/2403.17656",
    "context": "Title: SGHormer: An Energy-Saving Graph Transformer Driven by Spikes\nAbstract: arXiv:2403.17656v1 Announce Type: cross  Abstract: Graph Transformers (GTs) with powerful representation learning ability make a huge success in wide range of graph tasks. However, the costs behind outstanding performances of GTs are higher energy consumption and computational overhead. The complex structure and quadratic complexity during attention calculation in vanilla transformer seriously hinder its scalability on the large-scale graph data. Though existing methods have made strides in simplifying combinations among blocks or attention-learning paradigm to improve GTs' efficiency, a series of energy-saving solutions originated from biologically plausible structures are rarely taken into consideration when constructing GT framework. To this end, we propose a new spiking-based graph transformer (SGHormer). It turns full-precision embeddings into sparse and binarized spikes to reduce memory and computational costs. The spiking graph self-attention and spiking rectify blocks in SGHorm",
    "path": "papers/24/03/2403.17656.json",
    "total_tokens": 873,
    "translated_title": "SGHormer：一种由脉冲驱动的节能图变换器",
    "translated_abstract": "具有强大表示学习能力的图变换器（GTs）在各种图任务中取得巨大成功。然而，GTs出色性能背后的代价是更高的能量消耗和计算开销。传统变换器中注意力计算过程中的复杂结构和二次复杂度严重影响其在大规模图数据上的可扩展性。虽然现有方法在简化块之间的组合或注意力学习范式方面取得了进展以提高GTs的效率，但在构建GT框架时很少考虑源自生物学上合理结构的一系列节能解决方案。为此，我们提出了一种新的基于脉冲的图变换器（SGHormer）。它将全精度嵌入转换为稀疏和二值化脉冲以减少内存和计算成本。SGHormer中的脉冲图自注意力和脉冲修正块可以显著减少计算和存储开销。",
    "tldr": "SGHormer是一种由脉冲驱动的节能图变换器，通过将全精度嵌入转换为稀疏和二值化脉冲以减少内存和计算成本，提高了图变换器的效率。",
    "en_tdlr": "SGHormer is an energy-saving graph transformer driven by spikes, which improves the efficiency of graph transformers by converting full-precision embeddings into sparse and binarized spikes to reduce memory and computational costs."
}