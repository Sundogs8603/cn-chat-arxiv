{
    "title": "Decoding Probing: Revealing Internal Linguistic Structures in Neural Language Models using Minimal Pairs",
    "abstract": "arXiv:2403.17299v1 Announce Type: new  Abstract: Inspired by cognitive neuroscience studies, we introduce a novel `decoding probing' method that uses minimal pairs benchmark (BLiMP) to probe internal linguistic characteristics in neural language models layer by layer. By treating the language model as the `brain' and its representations as `neural activations', we decode grammaticality labels of minimal pairs from the intermediate layers' representations. This approach reveals: 1) Self-supervised language models capture abstract linguistic structures in intermediate layers that GloVe and RNN language models cannot learn. 2) Information about syntactic grammaticality is robustly captured through the first third layers of GPT-2 and also distributed in later layers. As sentence complexity increases, more layers are required for learning grammatical capabilities. 3) Morphological and semantics/syntax interface-related features are harder to capture than syntax. 4) For Transformer-based mod",
    "link": "https://arxiv.org/abs/2403.17299",
    "context": "Title: Decoding Probing: Revealing Internal Linguistic Structures in Neural Language Models using Minimal Pairs\nAbstract: arXiv:2403.17299v1 Announce Type: new  Abstract: Inspired by cognitive neuroscience studies, we introduce a novel `decoding probing' method that uses minimal pairs benchmark (BLiMP) to probe internal linguistic characteristics in neural language models layer by layer. By treating the language model as the `brain' and its representations as `neural activations', we decode grammaticality labels of minimal pairs from the intermediate layers' representations. This approach reveals: 1) Self-supervised language models capture abstract linguistic structures in intermediate layers that GloVe and RNN language models cannot learn. 2) Information about syntactic grammaticality is robustly captured through the first third layers of GPT-2 and also distributed in later layers. As sentence complexity increases, more layers are required for learning grammatical capabilities. 3) Morphological and semantics/syntax interface-related features are harder to capture than syntax. 4) For Transformer-based mod",
    "path": "papers/24/03/2403.17299.json",
    "total_tokens": 890,
    "translated_title": "使用最小对比项揭示神经语言模型中的内部语言结构：解码探测",
    "translated_abstract": "受认知神经科学研究启发，我们引入了一种称为“解码探测”的新方法，利用最小对比项基准（BLiMP）逐层探查神经语言模型中的内部语言特征。通过将语言模型视为‘大脑’，其表示为‘神经激活’，我们从中间层的表示中解码最小对比项的语法标签。这种方法揭示了：1）自监督语言模型在中间层捕获了抽象的语言结构，而GloVe和RNN语言模型无法学习。2）有关句法语法性的信息通过GPT-2的前三层稳健地捕获，同时也分布在后续层中。随着句子复杂性增加，需要更多层来学习语法能力。3）比起语法，形态和语义/句法接口相关特征更难捕获。4）基于Transformer的模型...",
    "tldr": "解码探测这种方法揭示了自监督语言模型在中间层捕获抽象的语言结构，揭示了语法的学习需要更多层次，而形态和语义/句法接口相关特征则更难捕获。",
    "en_tdlr": "Decoding probing method reveals that self-supervised language models capture abstract linguistic structures in intermediate layers, showing that learning syntax requires more layers while capturing morphological and semantics/syntax interface-related features is more challenging."
}