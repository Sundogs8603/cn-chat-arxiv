{
    "title": "3D-SceneDreamer: Text-Driven 3D-Consistent Scene Generation",
    "abstract": "arXiv:2403.09439v1 Announce Type: cross  Abstract: Text-driven 3D scene generation techniques have made rapid progress in recent years. Their success is mainly attributed to using existing generative models to iteratively perform image warping and inpainting to generate 3D scenes. However, these methods heavily rely on the outputs of existing models, leading to error accumulation in geometry and appearance that prevent the models from being used in various scenarios (e.g., outdoor and unreal scenarios). To address this limitation, we generatively refine the newly generated local views by querying and aggregating global 3D information, and then progressively generate the 3D scene. Specifically, we employ a tri-plane features-based NeRF as a unified representation of the 3D scene to constrain global 3D consistency, and propose a generative refinement network to synthesize new contents with higher quality by exploiting the natural image prior from 2D diffusion model as well as the global ",
    "link": "https://arxiv.org/abs/2403.09439",
    "context": "Title: 3D-SceneDreamer: Text-Driven 3D-Consistent Scene Generation\nAbstract: arXiv:2403.09439v1 Announce Type: cross  Abstract: Text-driven 3D scene generation techniques have made rapid progress in recent years. Their success is mainly attributed to using existing generative models to iteratively perform image warping and inpainting to generate 3D scenes. However, these methods heavily rely on the outputs of existing models, leading to error accumulation in geometry and appearance that prevent the models from being used in various scenarios (e.g., outdoor and unreal scenarios). To address this limitation, we generatively refine the newly generated local views by querying and aggregating global 3D information, and then progressively generate the 3D scene. Specifically, we employ a tri-plane features-based NeRF as a unified representation of the 3D scene to constrain global 3D consistency, and propose a generative refinement network to synthesize new contents with higher quality by exploiting the natural image prior from 2D diffusion model as well as the global ",
    "path": "papers/24/03/2403.09439.json",
    "total_tokens": 851,
    "translated_title": "3D-SceneDreamer: 文本驱动的3D一致场景生成",
    "translated_abstract": "近年来，文本驱动的3D场景生成技术取得了快速进展。这些方法的成功主要归因于利用现有生成模型进行图像扭曲和修补，生成3D场景。然而，这些方法严重依赖于现有模型的输出，在几何和外观上会导致错误累积，阻碍了模型在各种场景（例如户外和虚拟场景）中的使用。为了解决这一限制，我们通过查询和聚合全局3D信息，对新生成的局部视图进行生成性细化，然后逐步生成3D场景。具体来说，我们使用基于三平面特征的NeRF作为3D场景的统一表示，以约束全局3D一致性，并提出一个生成性细化网络，通过利用来自2D扩散模型的自然图像先验以及全局信息来合成质量更高的新内容。",
    "tldr": "通过引入全局3D信息和生成性细化网络，结合NeRF模型和2D扩散模型先验，提出了3D-SceneDreamer这一文本驱动的一致3D场景生成方法。",
    "en_tdlr": "By incorporating global 3D information and generative refinement network, combining NeRF model and 2D diffusion model prior, a text-driven consistent 3D scene generation approach named 3D-SceneDreamer is proposed."
}