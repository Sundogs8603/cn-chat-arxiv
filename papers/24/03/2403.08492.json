{
    "title": "Rich Semantic Knowledge Enhanced Large Language Models for Few-shot Chinese Spell Checking",
    "abstract": "arXiv:2403.08492v1 Announce Type: new  Abstract: Chinese Spell Checking (CSC) is a widely used technology, which plays a vital role in speech to text (STT) and optical character recognition (OCR). Most of the existing CSC approaches relying on BERT architecture achieve excellent performance. However, limited by the scale of the foundation model, BERT-based method does not work well in few-shot scenarios, showing certain limitations in practical applications. In this paper, we explore using an in-context learning method named RS-LLM (Rich Semantic based LLMs) to introduce large language models (LLMs) as the foundation model. Besides, we study the impact of introducing various Chinese rich semantic information in our framework. We found that by introducing a small number of specific Chinese rich semantic structures, LLMs achieve better performance than the BERT-based model on few-shot CSC task. Furthermore, we conduct experiments on multiple datasets, and the experimental results verifie",
    "link": "https://arxiv.org/abs/2403.08492",
    "context": "Title: Rich Semantic Knowledge Enhanced Large Language Models for Few-shot Chinese Spell Checking\nAbstract: arXiv:2403.08492v1 Announce Type: new  Abstract: Chinese Spell Checking (CSC) is a widely used technology, which plays a vital role in speech to text (STT) and optical character recognition (OCR). Most of the existing CSC approaches relying on BERT architecture achieve excellent performance. However, limited by the scale of the foundation model, BERT-based method does not work well in few-shot scenarios, showing certain limitations in practical applications. In this paper, we explore using an in-context learning method named RS-LLM (Rich Semantic based LLMs) to introduce large language models (LLMs) as the foundation model. Besides, we study the impact of introducing various Chinese rich semantic information in our framework. We found that by introducing a small number of specific Chinese rich semantic structures, LLMs achieve better performance than the BERT-based model on few-shot CSC task. Furthermore, we conduct experiments on multiple datasets, and the experimental results verifie",
    "path": "papers/24/03/2403.08492.json",
    "total_tokens": 689,
    "translated_title": "富含语义知识增强的大型语言模型用于少样本中文拼写检查",
    "translated_abstract": "本文探讨了使用一种名为RS-LLM（基于丰富语义的LLMs）的上下文学习方法将大型语言模型（LLMs）引入作为基础模型，以及在我们的框架中引入各种中文丰富语义信息的影响。实验结果表明，通过引入少量特定的中文丰富语义结构，LLMs在少样本中文拼写检查任务上比基于BERT模型表现更好。",
    "tldr": "本文使用富含语义知识的大型语言模型在少样本中文拼写检查任务上取得了比BERT模型更好的性能。",
    "en_tdlr": "This paper utilizes large language models enriched with semantic knowledge to achieve better performance than BERT models in a few-shot Chinese spell checking task."
}