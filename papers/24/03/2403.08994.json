{
    "title": "Ethos: Rectifying Language Models in Orthogonal Parameter Space",
    "abstract": "arXiv:2403.08994v1 Announce Type: new  Abstract: Language models (LMs) have greatly propelled the research on natural language processing. However, LMs also raise concerns regarding the generation of biased or toxic content and the potential disclosure of private information from the training dataset. In this work, we present a new efficient approach, Ethos, that rectifies LMs to mitigate toxicity and bias in outputs and avoid privacy leakage. Ethos is built on task arithmetic. However, unlike current task arithmetic algorithms, Ethos distinguishes general beneficial and undesired knowledge when reconstructing task vectors. Specifically, Ethos first obtains a set of principal components from the pre-trained models using singular value decomposition. Then, by projecting the task vector onto principal components, Ethos identifies the principal components that encode general or undesired knowledge. Ethos performs negating using the task vector with undesired knowledge only, thereby minimi",
    "link": "https://arxiv.org/abs/2403.08994",
    "context": "Title: Ethos: Rectifying Language Models in Orthogonal Parameter Space\nAbstract: arXiv:2403.08994v1 Announce Type: new  Abstract: Language models (LMs) have greatly propelled the research on natural language processing. However, LMs also raise concerns regarding the generation of biased or toxic content and the potential disclosure of private information from the training dataset. In this work, we present a new efficient approach, Ethos, that rectifies LMs to mitigate toxicity and bias in outputs and avoid privacy leakage. Ethos is built on task arithmetic. However, unlike current task arithmetic algorithms, Ethos distinguishes general beneficial and undesired knowledge when reconstructing task vectors. Specifically, Ethos first obtains a set of principal components from the pre-trained models using singular value decomposition. Then, by projecting the task vector onto principal components, Ethos identifies the principal components that encode general or undesired knowledge. Ethos performs negating using the task vector with undesired knowledge only, thereby minimi",
    "path": "papers/24/03/2403.08994.json",
    "total_tokens": 823,
    "translated_title": "Ethos：在正交参数空间中矫正语言模型",
    "translated_abstract": "语言模型（LMs）极大推动了自然语言处理研究的发展。然而，LMs也引发了关于生成偏见或有毒内容以及训练数据集中私人信息可能泄露的担忧。在这项工作中，我们提出了一种新的高效方法，Ethos，通过在任务向量上进行矫正LMs以减轻产生毒性和偏见输出以及避免隐私泄露。Ethos建立在任务算术基础上。然而，与当前的任务算法不同的是，Ethos在重构任务向量时区分了一般有益和不良知识。具体而言，Ethos首先使用奇异值分解从预训练模型中获得一组主成分。然后，通过将任务向量投影到主成分上，Ethos识别编码一般或不良知识的主成分。Ethos仅使用带有不良知识的任务向量进行否定，从而最小",
    "tldr": "Ethos提出了一种新的高效方法，通过在任务向量上进行矫正LMs以减轻产生毒性和偏见输出以及避免隐私泄露。",
    "en_tdlr": "Ethos introduces a new efficient approach to rectify language models on task vectors to reduce toxicity and bias in outputs and prevent privacy leakage."
}