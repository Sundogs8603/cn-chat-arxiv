{
    "title": "Initial Decoding with Minimally Augmented Language Model for Improved Lattice Rescoring in Low Resource ASR",
    "abstract": "arXiv:2403.10937v1 Announce Type: cross  Abstract: This paper addresses the problem of improving speech recognition accuracy with lattice rescoring in low-resource languages where the baseline language model is insufficient for generating inclusive lattices. We minimally augment the baseline language model with word unigram counts that are present in a larger text corpus of the target language but absent in the baseline. The lattices generated after decoding with such an augmented baseline language model are more comprehensive. We obtain 21.8% (Telugu) and 41.8% (Kannada) relative word error reduction with our proposed method. This reduction in word error rate is comparable to 21.5% (Telugu) and 45.9% (Kannada) relative word error reduction obtained by decoding with full Wikipedia text augmented language mode while our approach consumes only 1/8th the memory. We demonstrate that our method is comparable with various text selection-based language model augmentation and also consistent f",
    "link": "https://arxiv.org/abs/2403.10937",
    "context": "Title: Initial Decoding with Minimally Augmented Language Model for Improved Lattice Rescoring in Low Resource ASR\nAbstract: arXiv:2403.10937v1 Announce Type: cross  Abstract: This paper addresses the problem of improving speech recognition accuracy with lattice rescoring in low-resource languages where the baseline language model is insufficient for generating inclusive lattices. We minimally augment the baseline language model with word unigram counts that are present in a larger text corpus of the target language but absent in the baseline. The lattices generated after decoding with such an augmented baseline language model are more comprehensive. We obtain 21.8% (Telugu) and 41.8% (Kannada) relative word error reduction with our proposed method. This reduction in word error rate is comparable to 21.5% (Telugu) and 45.9% (Kannada) relative word error reduction obtained by decoding with full Wikipedia text augmented language mode while our approach consumes only 1/8th the memory. We demonstrate that our method is comparable with various text selection-based language model augmentation and also consistent f",
    "path": "papers/24/03/2403.10937.json",
    "total_tokens": 873,
    "translated_title": "通过最小化增强语言模型进行初步解码以改进低资源ASR中的晶格重新打分",
    "translated_abstract": "本文解决了在基线语言模型无法生成全面晶格的低资源语言中通过晶格重新打分改善语音识别准确性的问题。我们通过将基线语言模型最小程度地增强为目标语言更大文本语料库中出现但基线语言模型中不存在的词的词频来解决这一问题。通过使用这种增强的基线语言模型进行解码生成的晶格更加全面。我们的方法使词误差率相对减少了21.8%（泰卢固语）和41.8%（卡纳达语）。与使用完整维基百科文本增强语言模型解码相比，我们的方法在减少词误差率方面相当，而我们的方法只消耗1/8的内存。",
    "tldr": "通过最小化增强语言模型进行初步解码，以提高低资源ASR中晶格重新打分的语音识别准确性，相对减少了泰卢固语21.8%和卡纳达语41.8%的词误差率，同时仅消耗1/8内存。",
    "en_tdlr": "Minimally augmenting language model for initial decoding improves lattice rescoring in low-resource ASR, reducing word error rate by 21.8% (Telugu) and 41.8% (Kannada) with 1/8 memory consumption."
}