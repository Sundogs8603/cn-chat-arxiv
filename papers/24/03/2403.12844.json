{
    "title": "MELTing point: Mobile Evaluation of Language Transformers",
    "abstract": "arXiv:2403.12844v1 Announce Type: new  Abstract: Transformers have revolutionized the machine learning landscape, gradually making their way into everyday tasks and equipping our computers with ``sparks of intelligence''. However, their runtime requirements have prevented them from being broadly deployed on mobile. As personal devices become increasingly powerful and prompt privacy becomes an ever more pressing issue, we explore the current state of mobile execution of Large Language Models (LLMs). To achieve this, we have created our own automation infrastructure, MELT, which supports the headless execution and benchmarking of LLMs on device, supporting different models, devices and frameworks, including Android, iOS and Nvidia Jetson devices. We evaluate popular instruction fine-tuned LLMs and leverage different frameworks to measure their end-to-end and granular performance, tracing their memory and energy requirements along the way.   Our analysis is the first systematic study of o",
    "link": "https://arxiv.org/abs/2403.12844",
    "context": "Title: MELTing point: Mobile Evaluation of Language Transformers\nAbstract: arXiv:2403.12844v1 Announce Type: new  Abstract: Transformers have revolutionized the machine learning landscape, gradually making their way into everyday tasks and equipping our computers with ``sparks of intelligence''. However, their runtime requirements have prevented them from being broadly deployed on mobile. As personal devices become increasingly powerful and prompt privacy becomes an ever more pressing issue, we explore the current state of mobile execution of Large Language Models (LLMs). To achieve this, we have created our own automation infrastructure, MELT, which supports the headless execution and benchmarking of LLMs on device, supporting different models, devices and frameworks, including Android, iOS and Nvidia Jetson devices. We evaluate popular instruction fine-tuned LLMs and leverage different frameworks to measure their end-to-end and granular performance, tracing their memory and energy requirements along the way.   Our analysis is the first systematic study of o",
    "path": "papers/24/03/2403.12844.json",
    "total_tokens": 841,
    "translated_title": "MELTing point: 移动语言转换器的评估",
    "translated_abstract": "Transformers已经彻底改变了机器学习领域，逐渐应用于日常任务，赋予我们的计算机“智能的火花”。然而，它们的运行时需求阻碍了它们在移动设备上的广泛部署。在个人设备变得越来越强大，以及迅速隐私问题变得更加紧迫的情况下，我们探讨了大型语言模型（LLMs）在移动设备上执行的现状。为了实现这一目标，我们创建了自己的自动化基础架构MELT，支持在设备上无界面执行和评估LLMs，并支持不同的模型、设备和框架，包括Android、iOS和Nvidia Jetson设备。我们评估了流行的指令微调的LLMs，并利用不同的框架来测量它们的端到端和细粒度性能，跟踪它们的内存和能耗需求。",
    "tldr": "该研究对移动设备上大型语言模型（LLMs）的执行进行了首次系统性研究，并创建了自动化基础架构MELT来支持其评估和性能测试。"
}