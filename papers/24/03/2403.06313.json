{
    "title": "Optimal Policy Sparsification and Low Rank Decomposition for Deep Reinforcement Learning",
    "abstract": "arXiv:2403.06313v1 Announce Type: cross  Abstract: Deep reinforcement learning(DRL) has shown significant promise in a wide range of applications including computer games and robotics. Yet, training DRL policies consume extraordinary computing resources resulting in dense policies which are prone to overfitting. Moreover, inference with dense DRL policies limit their practical applications, especially in edge computing. Techniques such as pruning and singular value decomposition have been used with deep learning models to achieve sparsification and model compression to limit overfitting and reduce memory consumption. However, these techniques resulted in sub-optimal performance with notable decay in rewards. $L_1$ and $L_2$ regularization techniques have been proposed for neural network sparsification and sparse auto-encoder development, but their implementation in DRL environments has not been apparent. We propose a novel $L_0$-norm-regularization technique using an optimal sparsity m",
    "link": "https://arxiv.org/abs/2403.06313",
    "context": "Title: Optimal Policy Sparsification and Low Rank Decomposition for Deep Reinforcement Learning\nAbstract: arXiv:2403.06313v1 Announce Type: cross  Abstract: Deep reinforcement learning(DRL) has shown significant promise in a wide range of applications including computer games and robotics. Yet, training DRL policies consume extraordinary computing resources resulting in dense policies which are prone to overfitting. Moreover, inference with dense DRL policies limit their practical applications, especially in edge computing. Techniques such as pruning and singular value decomposition have been used with deep learning models to achieve sparsification and model compression to limit overfitting and reduce memory consumption. However, these techniques resulted in sub-optimal performance with notable decay in rewards. $L_1$ and $L_2$ regularization techniques have been proposed for neural network sparsification and sparse auto-encoder development, but their implementation in DRL environments has not been apparent. We propose a novel $L_0$-norm-regularization technique using an optimal sparsity m",
    "path": "papers/24/03/2403.06313.json",
    "total_tokens": 906,
    "translated_title": "深度强化学习的最优策略稀疏化和低秩分解",
    "translated_abstract": "深度强化学习在计算机游戏和机器人等多个领域显示出巨大潜力。然而，训练深度强化学习策略耗费了大量的计算资源，导致密集策略容易过拟合。此外，使用密集深度强化学习策略进行推理限制了它们在边缘计算等实际应用中的适用性。为了限制过拟合和减少内存消耗，研究者已经使用了像剪枝和奇异值分解这样的技术来对深度学习模型进行稀疏化和模型压缩。然而，这些技术导致了性能次优，在奖励方面出现显著的减弱。在神经网络稀疏化和稀疏自编码器开发中已经提出了$L_1$和$L_2$正则化技术，但它们在深度强化学习环境中的实现尚不明显。我们提出了一种新颖的$L_0$范数正则化技术，使用了一种最优稀疏度",
    "tldr": "提出一种新颖的$L_0$范数正则化技术，用于深度强化学习的最优策略稀疏化和低秩分解",
    "en_tdlr": "Proposing a novel $L_0$-norm-regularization technique for optimal policy sparsification and low rank decomposition in deep reinforcement learning."
}