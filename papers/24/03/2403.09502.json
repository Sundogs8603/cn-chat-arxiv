{
    "title": "EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning",
    "abstract": "arXiv:2403.09502v1 Announce Type: cross  Abstract: Recent advancements in self-supervised audio-visual representation learning have demonstrated its potential to capture rich and comprehensive representations. However, despite the advantages of data augmentation verified in many learning methods, audio-visual learning has struggled to fully harness these benefits, as augmentations can easily disrupt the correspondence between input pairs. To address this limitation, we introduce EquiAV, a novel framework that leverages equivariance for audio-visual contrastive learning. Our approach begins with extending equivariance to audio-visual learning, facilitated by a shared attention-based transformation predictor. It enables the aggregation of features from diverse augmentations into a representative embedding, providing robust supervision. Notably, this is achieved with minimal computational overhead. Extensive ablation studies and qualitative results verify the effectiveness of our method. ",
    "link": "https://arxiv.org/abs/2403.09502",
    "context": "Title: EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning\nAbstract: arXiv:2403.09502v1 Announce Type: cross  Abstract: Recent advancements in self-supervised audio-visual representation learning have demonstrated its potential to capture rich and comprehensive representations. However, despite the advantages of data augmentation verified in many learning methods, audio-visual learning has struggled to fully harness these benefits, as augmentations can easily disrupt the correspondence between input pairs. To address this limitation, we introduce EquiAV, a novel framework that leverages equivariance for audio-visual contrastive learning. Our approach begins with extending equivariance to audio-visual learning, facilitated by a shared attention-based transformation predictor. It enables the aggregation of features from diverse augmentations into a representative embedding, providing robust supervision. Notably, this is achieved with minimal computational overhead. Extensive ablation studies and qualitative results verify the effectiveness of our method. ",
    "path": "papers/24/03/2403.09502.json",
    "total_tokens": 876,
    "translated_title": "EquiAV: 利用等变性进行音频-视觉对比学习",
    "translated_abstract": "自我监督的音频-视觉表示学习最近取得了重大进展，展示出捕捉丰富综合表示的潜力。然而，尽管数据增强在许多学习方法中已经得到验证，音频-视觉学习仍然很难充分利用这些优势，因为增强可能会轻易破坏输入对之间的对应关系。为了解决这一限制，我们引入了EquiAV，一种利用等变性进行音频-视觉对比学习的新框架。我们的方法从扩展等变性开始进行音频-视觉学习，通过一个共享的基于注意力的变换预测器来促进。它使得来自不同增强的特征能够聚合到一个代表性的嵌入中，提供强大的监督。值得注意的是，这是在最小计算开销的情况下实现的。大量消融研究和定性结果验证了我们方法的有效性。",
    "tldr": "这项研究提出了一种利用等变性进行音频-视觉对比学习的新框架，通过一个共享的基于注意力的变换预测器来实现特征聚合和嵌入表示，有效提供了强大的监督，且计算开销最小。",
    "en_tdlr": "This study introduces a novel framework leveraging equivariance for audio-visual contrastive learning, facilitated by a shared attention-based transformation predictor, enabling feature aggregation into representative embeddings with robust supervision and minimal computational overhead."
}