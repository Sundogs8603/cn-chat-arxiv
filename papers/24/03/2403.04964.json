{
    "title": "Tell me the truth: A system to measure the trustworthiness of Large Language Models",
    "abstract": "arXiv:2403.04964v1 Announce Type: new  Abstract: Large Language Models (LLM) have taken the front seat in most of the news since November 2023, when ChatGPT was introduced. After more than one year, one of the major reasons companies are resistant to adopting them is the limited confidence they have in the trustworthiness of those systems. In a study by (Baymard, 2023), ChatGPT-4 showed an 80.1% false-positive error rate in identifying usability issues on websites. A Jan. '24 study by JAMA Pediatrics found that ChatGPT has an accuracy rate of 17% percent when diagnosing pediatric medical cases (Barile et al., 2024). But then, what is \"trust\"? Trust is a relative, subject condition that can change based on culture, domain, individuals. And then, given a domain, how can the trustworthiness of a system be measured? In this paper, I present a systematic approach to measure trustworthiness based on a predefined ground truth, represented as a knowledge graph of the domain. The approach is a ",
    "link": "https://arxiv.org/abs/2403.04964",
    "context": "Title: Tell me the truth: A system to measure the trustworthiness of Large Language Models\nAbstract: arXiv:2403.04964v1 Announce Type: new  Abstract: Large Language Models (LLM) have taken the front seat in most of the news since November 2023, when ChatGPT was introduced. After more than one year, one of the major reasons companies are resistant to adopting them is the limited confidence they have in the trustworthiness of those systems. In a study by (Baymard, 2023), ChatGPT-4 showed an 80.1% false-positive error rate in identifying usability issues on websites. A Jan. '24 study by JAMA Pediatrics found that ChatGPT has an accuracy rate of 17% percent when diagnosing pediatric medical cases (Barile et al., 2024). But then, what is \"trust\"? Trust is a relative, subject condition that can change based on culture, domain, individuals. And then, given a domain, how can the trustworthiness of a system be measured? In this paper, I present a systematic approach to measure trustworthiness based on a predefined ground truth, represented as a knowledge graph of the domain. The approach is a ",
    "path": "papers/24/03/2403.04964.json",
    "total_tokens": 889,
    "translated_title": "告诉我实话：一种用于衡量大型语言模型可信度的系统",
    "translated_abstract": "大型语言模型（LLM）自从2023年11月ChatGPT推出以来，在大多数新闻中占据了重要位置。然而，一年多过去了，公司抵触采用它们的一个主要原因是他们对这些系统的可信度缺乏信心。一项由Baymard（2023）进行的研究发现，ChatGPT-4 在识别网站可用性问题时有80.1%的假阳性错误率。而《JAMA儿科学》杂志（JAMA Pediatrics）于2024年1月的研究发现，ChatGPT 在诊断儿科医疗案例时的准确率为17%（Barile et al., 2024）。那么，何为“信任”？信任是一个相对的、主观的条件，可以根据文化、领域和个体而变化。那么，在给定一个领域的情况下，如何衡量系统的可信度呢？本文提出了一种基于预定义领域知识图表示的系统化方法来衡量可信度。",
    "tldr": "本文提出了一种基于预定义领域知识图的系统化方法来衡量大型语言模型的可信度。",
    "en_tdlr": "This paper presents a systematic approach based on a predefined knowledge graph of the domain to measure the trustworthiness of Large Language Models."
}