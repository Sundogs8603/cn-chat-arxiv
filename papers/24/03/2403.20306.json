{
    "title": "Towards Greener LLMs: Bringing Energy-Efficiency to the Forefront of LLM Inference",
    "abstract": "arXiv:2403.20306v1 Announce Type: new  Abstract: With the ubiquitous use of modern large language models (LLMs) across industries, the inference serving for these models is ever expanding. Given the high compute and memory requirements of modern LLMs, more and more top-of-the-line GPUs are being deployed to serve these models. Energy availability has come to the forefront as the biggest challenge for data center expansion to serve these models. In this paper, we present the trade-offs brought up by making energy efficiency the primary goal of LLM serving under performance SLOs. We show that depending on the inputs, the model, and the service-level agreements, there are several knobs available to the LLM inference provider to use for being energy efficient. We characterize the impact of these knobs on the latency, throughput, as well as the energy. By exploring these trade-offs, we offer valuable insights into optimizing energy usage without compromising on performance, thereby paving t",
    "link": "https://arxiv.org/abs/2403.20306",
    "context": "Title: Towards Greener LLMs: Bringing Energy-Efficiency to the Forefront of LLM Inference\nAbstract: arXiv:2403.20306v1 Announce Type: new  Abstract: With the ubiquitous use of modern large language models (LLMs) across industries, the inference serving for these models is ever expanding. Given the high compute and memory requirements of modern LLMs, more and more top-of-the-line GPUs are being deployed to serve these models. Energy availability has come to the forefront as the biggest challenge for data center expansion to serve these models. In this paper, we present the trade-offs brought up by making energy efficiency the primary goal of LLM serving under performance SLOs. We show that depending on the inputs, the model, and the service-level agreements, there are several knobs available to the LLM inference provider to use for being energy efficient. We characterize the impact of these knobs on the latency, throughput, as well as the energy. By exploring these trade-offs, we offer valuable insights into optimizing energy usage without compromising on performance, thereby paving t",
    "path": "papers/24/03/2403.20306.json",
    "total_tokens": 839,
    "translated_title": "朝着更绿色的LLMs：将能效引入LLM推理的前沿",
    "translated_abstract": "随着现代大型语言模型（LLMs）在各行业中的普遍使用，用于这些模型的推理服务正在不断扩展。鉴于现代LLMs的高计算和内存要求，越来越多顶尖的GPU被部署来提供这些模型的服务。能源供应已成为数据中心扩展以提供这些模型服务的最大挑战。本文提出了以能效为LLM服务的首要目标所带来的折衷方案，同时满足性能SLOs。我们表明，根据输入、模型和服务水平协议，LLM推理提供者有几个旋钮可用于实现能效。我们对这些旋钮对延迟、吞吐量以及能源的影响进行了表征。通过探索这些折衷方案，我们提供了优化能源使用而不牺牲性能的宝贵见解，从而铺平了道路。",
    "tldr": "本文旨在将能效作为LLM推理的主要目标，并探讨在满足性能目标的前提下如何最大程度地提高能源利用效率。",
    "en_tdlr": "This paper aims to make energy efficiency the primary goal of LLM inference and explores how to optimize energy usage without compromising on performance."
}