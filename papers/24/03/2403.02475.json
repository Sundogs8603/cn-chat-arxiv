{
    "title": "Enhancing LLM Safety via Constrained Direct Preference Optimization",
    "abstract": "arXiv:2403.02475v1 Announce Type: cross  Abstract: The rapidly increasing capabilities of large language models (LLMs) raise an urgent need to align AI systems with diverse human preferences to simultaneously enhance their usefulness and safety, despite the often conflicting nature of these goals. To address this important problem, a promising approach is to enforce a safety constraint at the fine-tuning stage through a constrained Reinforcement Learning from Human Feedback (RLHF) framework. This approach, however, is computationally expensive and often unstable. In this work, we introduce Constrained DPO (C-DPO), a novel extension of the recently proposed Direct Preference Optimization (DPO) approach for fine-tuning LLMs that is both efficient and lightweight. By integrating dual gradient descent and DPO, our method identifies a nearly optimal trade-off between helpfulness and harmlessness without using reinforcement learning. Empirically, our approach provides a safety guarantee to L",
    "link": "https://arxiv.org/abs/2403.02475",
    "context": "Title: Enhancing LLM Safety via Constrained Direct Preference Optimization\nAbstract: arXiv:2403.02475v1 Announce Type: cross  Abstract: The rapidly increasing capabilities of large language models (LLMs) raise an urgent need to align AI systems with diverse human preferences to simultaneously enhance their usefulness and safety, despite the often conflicting nature of these goals. To address this important problem, a promising approach is to enforce a safety constraint at the fine-tuning stage through a constrained Reinforcement Learning from Human Feedback (RLHF) framework. This approach, however, is computationally expensive and often unstable. In this work, we introduce Constrained DPO (C-DPO), a novel extension of the recently proposed Direct Preference Optimization (DPO) approach for fine-tuning LLMs that is both efficient and lightweight. By integrating dual gradient descent and DPO, our method identifies a nearly optimal trade-off between helpfulness and harmlessness without using reinforcement learning. Empirically, our approach provides a safety guarantee to L",
    "path": "papers/24/03/2403.02475.json",
    "total_tokens": 892,
    "translated_title": "通过受限直接偏好优化增强LLM安全性",
    "translated_abstract": "大型语言模型（LLMs）的快速增强能力提高了将人工智能系统与不同人类偏好相一致以同时增强其有用性和安全性的迫切需要，尽管这些目标常常相互冲突。为解决这一重要问题，一种有前途的方法是在微调阶段通过受限制的人类反馈强化学习（RLHF）框架施加安全约束。然而，这种方法计算成本高且常常不稳定。本文引入了受限制的DPO（C-DPO），这是对最近提出的直接偏好优化（DPO）方法的一种新颖扩展，用于优化LLMs的微调，具有高效和轻量的特点。通过融合双梯度下降和DPO，我们的方法在不使用强化学习的情况下确定了帮助性和无害性之间的几乎最佳折衷。从经验上看，我们的方法为LLMs提供了安全保障。",
    "tldr": "通过引入Constrained DPO（C-DPO）方法，我们提出了一种高效且轻量的微调大型语言模型（LLMs）的方法，能有效平衡有用性和安全性之间的权衡，为LLMs提供了安全保障。",
    "en_tdlr": "We introduce Constrained DPO (C-DPO), a novel and efficient method for fine-tuning large language models (LLMs), which effectively balances the trade-off between usefulness and safety, providing a safety guarantee for LLMs without using reinforcement learning."
}