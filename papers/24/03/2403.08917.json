{
    "title": "Efficiently Computing Similarities to Private Datasets",
    "abstract": "arXiv:2403.08917v1 Announce Type: cross  Abstract: Many methods in differentially private model training rely on computing the similarity between a query point (such as public or synthetic data) and private data. We abstract out this common subroutine and study the following fundamental algorithmic problem: Given a similarity function $f$ and a large high-dimensional private dataset $X \\subset \\mathbb{R}^d$, output a differentially private (DP) data structure which approximates $\\sum_{x \\in X} f(x,y)$ for any query $y$. We consider the cases where $f$ is a kernel function, such as $f(x,y) = e^{-\\|x-y\\|_2^2/\\sigma^2}$ (also known as DP kernel density estimation), or a distance function such as $f(x,y) = \\|x-y\\|_2$, among others.   Our theoretical results improve upon prior work and give better privacy-utility trade-offs as well as faster query times for a wide range of kernels and distance functions. The unifying approach behind our results is leveraging `low-dimensional structures' pre",
    "link": "https://arxiv.org/abs/2403.08917",
    "context": "Title: Efficiently Computing Similarities to Private Datasets\nAbstract: arXiv:2403.08917v1 Announce Type: cross  Abstract: Many methods in differentially private model training rely on computing the similarity between a query point (such as public or synthetic data) and private data. We abstract out this common subroutine and study the following fundamental algorithmic problem: Given a similarity function $f$ and a large high-dimensional private dataset $X \\subset \\mathbb{R}^d$, output a differentially private (DP) data structure which approximates $\\sum_{x \\in X} f(x,y)$ for any query $y$. We consider the cases where $f$ is a kernel function, such as $f(x,y) = e^{-\\|x-y\\|_2^2/\\sigma^2}$ (also known as DP kernel density estimation), or a distance function such as $f(x,y) = \\|x-y\\|_2$, among others.   Our theoretical results improve upon prior work and give better privacy-utility trade-offs as well as faster query times for a wide range of kernels and distance functions. The unifying approach behind our results is leveraging `low-dimensional structures' pre",
    "path": "papers/24/03/2403.08917.json",
    "total_tokens": 884,
    "translated_title": "高效计算与私有数据集的相似性",
    "translated_abstract": "许多差分私有模型训练方法依赖于计算查询点（如公共数据或合成数据）与私有数据之间的相似性。我们将这个常见子例程抽象出来，并研究以下基本算法问题：给定一个相似性函数$f$和一个大的高维私有数据集$X \\subset \\mathbb{R}^d$，输出一个差分私有（DP）数据结构，用于近似计算任何查询$y$的$\\sum_{x \\in X} f(x,y)$。我们考虑$f$为核函数的情况，例如$f(x,y) = e^{-\\|x-y\\|_2^2/\\sigma^2}$（也称为差分私有核密度估计），或者距离函数的情况，如$f(x,y) = \\|x-y\\|_2$等。我们的理论结果改进了先前的工作，并为一系列核函数和距离函数提供了更好的隐私-效用权衡和更快的查询时间。我们结果背后的统一方法是利用“低维结构”。",
    "tldr": "提出了一种能够高效计算与私有数据集相似性的方法，改进了先前工作的理论结果，提供了更好的隐私-效用权衡和更快的查询时间。"
}