{
    "title": "Decomposing Control Lyapunov Functions for Efficient Reinforcement Learning",
    "abstract": "arXiv:2403.12210v1 Announce Type: cross  Abstract: Recent methods using Reinforcement Learning (RL) have proven to be successful for training intelligent agents in unknown environments. However, RL has not been applied widely in real-world robotics scenarios. This is because current state-of-the-art RL methods require large amounts of data to learn a specific task, leading to unreasonable costs when deploying the agent to collect data in real-world applications. In this paper, we build from existing work that reshapes the reward function in RL by introducing a Control Lyapunov Function (CLF), which is demonstrated to reduce the sample complexity. Still, this formulation requires knowing a CLF of the system, but due to the lack of a general method, it is often a challenge to identify a suitable CLF. Existing work can compute low-dimensional CLFs via a Hamilton-Jacobi reachability procedure. However, this class of methods becomes intractable on high-dimensional systems, a problem that we",
    "link": "https://arxiv.org/abs/2403.12210",
    "context": "Title: Decomposing Control Lyapunov Functions for Efficient Reinforcement Learning\nAbstract: arXiv:2403.12210v1 Announce Type: cross  Abstract: Recent methods using Reinforcement Learning (RL) have proven to be successful for training intelligent agents in unknown environments. However, RL has not been applied widely in real-world robotics scenarios. This is because current state-of-the-art RL methods require large amounts of data to learn a specific task, leading to unreasonable costs when deploying the agent to collect data in real-world applications. In this paper, we build from existing work that reshapes the reward function in RL by introducing a Control Lyapunov Function (CLF), which is demonstrated to reduce the sample complexity. Still, this formulation requires knowing a CLF of the system, but due to the lack of a general method, it is often a challenge to identify a suitable CLF. Existing work can compute low-dimensional CLFs via a Hamilton-Jacobi reachability procedure. However, this class of methods becomes intractable on high-dimensional systems, a problem that we",
    "path": "papers/24/03/2403.12210.json",
    "total_tokens": 780,
    "translated_title": "将控制Lyapunov函数分解以实现高效的强化学习",
    "translated_abstract": "近期使用强化学习（RL）的方法已被证明对于训练智能体在未知环境中表现成功。然而，RL并未广泛应用于现实世界的机器人场景。本文构建于现有将RL中的奖励函数重塑的工作基础之上，引入了一个控制Lyapunov函数（CLF），证明能减少样本复杂性。然而，此公式需要知道系统的一个CLF，但由于缺乏通用方法，通常很难确定一个合适的CLF。现有工作可以通过哈密尔顿-雅可比可达性程序计算低维CLFs。然而，这类方法在高维系统上变得难以处理，这是我们...",
    "tldr": "引入控制Lyapunov函数（CLF）来降低强化学习中的样本复杂性，为现实世界机器人场景的训练提供了新的高效方法。",
    "en_tdlr": "Introducing Control Lyapunov Functions (CLF) to reduce sample complexity in reinforcement learning, providing a new efficient method for training in real-world robotics scenarios."
}