{
    "title": "Resonance RoPE: Improving Context Length Generalization of Large Language Models",
    "abstract": "arXiv:2403.00071v1 Announce Type: cross  Abstract: This paper addresses the challenge of train-short-test-long (TSTL) scenarios in Large Language Models (LLMs) equipped with Rotary Position Embedding (RoPE), where models pre-trained on shorter sequences face difficulty with out-of-distribution (OOD) token positions in longer sequences. We introduce Resonance RoPE, a novel approach designed to narrow the generalization gap in TSTL scenarios by refining the interpolation of RoPE features for OOD positions, significantly improving the model performance without additional online computational costs. Furthermore, we present PosGen, a new synthetic benchmark specifically designed for fine-grained behavior analysis in TSTL scenarios, aiming to isolate the constantly increasing difficulty of token generation on long contexts from the challenges of recognizing new token positions. Our experiments on synthetic tasks show that after applying Resonance RoPE, Transformers recognize OOD position bet",
    "link": "https://arxiv.org/abs/2403.00071",
    "context": "Title: Resonance RoPE: Improving Context Length Generalization of Large Language Models\nAbstract: arXiv:2403.00071v1 Announce Type: cross  Abstract: This paper addresses the challenge of train-short-test-long (TSTL) scenarios in Large Language Models (LLMs) equipped with Rotary Position Embedding (RoPE), where models pre-trained on shorter sequences face difficulty with out-of-distribution (OOD) token positions in longer sequences. We introduce Resonance RoPE, a novel approach designed to narrow the generalization gap in TSTL scenarios by refining the interpolation of RoPE features for OOD positions, significantly improving the model performance without additional online computational costs. Furthermore, we present PosGen, a new synthetic benchmark specifically designed for fine-grained behavior analysis in TSTL scenarios, aiming to isolate the constantly increasing difficulty of token generation on long contexts from the challenges of recognizing new token positions. Our experiments on synthetic tasks show that after applying Resonance RoPE, Transformers recognize OOD position bet",
    "path": "papers/24/03/2403.00071.json",
    "total_tokens": 881,
    "translated_title": "提升大型语言模型的上下文长度泛化能力：共振 RoPE",
    "translated_abstract": "本文针对大型语言模型（LLMs）中的训练短-测试长（TSTL）场景挑战，引入了Rotary Position Embedding（RoPE）技术，解决了在较短序列上预训练的模型在较长序列中遇到位置超出分布（OOD）的困难。我们提出了Resonance RoPE，一种新颖的方法，通过精细调整RoPE特征的插值来缩小TSTL场景中的泛化差距，显著提高了模型性能，而无需额外的在线计算成本。此外，我们提出了PosGen，这是一个新的合成基准，专门针对TSTL场景中的精细行为分析，旨在从长上下文中不断增加的令牌生成困难和识别新令牌位置的挑战中分离出来。我们在合成任务上的实验表明，在应用Resonance RoPE后，Transformer模型可以识别OOD位置。",
    "tldr": "Resonance RoPE是一种新颖方法，通过调整RoPE特征的插值来缩小训练短-测试长场景下的泛化差距，在不增加额外在线计算成本的情况下显著提高模型性能。"
}