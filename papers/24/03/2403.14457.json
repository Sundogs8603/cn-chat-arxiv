{
    "title": "gTBLS: Generating Tables from Text by Conditional Question Answering",
    "abstract": "arXiv:2403.14457v1 Announce Type: new  Abstract: Distilling large, unstructured text into a structured, condensed form such as tables is an open research problem. One of the primary challenges in automatically generating tables is ensuring their syntactic validity. Prior approaches address this challenge by including additional parameters in the Transformer's attention mechanism to attend to specific rows and column headers. In contrast to this single-stage method, this paper presents a two-stage approach called Generative Tables (gTBLS). The first stage infers table structure (row and column headers) from the text. The second stage formulates questions using these headers and fine-tunes a causal language model to answer them. Furthermore, the gTBLS approach is amenable to the utilization of pre-trained Large Language Models in a zero-shot configuration, presenting a solution for table generation in situations where fine-tuning is not feasible. gTBLS improves prior approaches by up to ",
    "link": "https://arxiv.org/abs/2403.14457",
    "context": "Title: gTBLS: Generating Tables from Text by Conditional Question Answering\nAbstract: arXiv:2403.14457v1 Announce Type: new  Abstract: Distilling large, unstructured text into a structured, condensed form such as tables is an open research problem. One of the primary challenges in automatically generating tables is ensuring their syntactic validity. Prior approaches address this challenge by including additional parameters in the Transformer's attention mechanism to attend to specific rows and column headers. In contrast to this single-stage method, this paper presents a two-stage approach called Generative Tables (gTBLS). The first stage infers table structure (row and column headers) from the text. The second stage formulates questions using these headers and fine-tunes a causal language model to answer them. Furthermore, the gTBLS approach is amenable to the utilization of pre-trained Large Language Models in a zero-shot configuration, presenting a solution for table generation in situations where fine-tuning is not feasible. gTBLS improves prior approaches by up to ",
    "path": "papers/24/03/2403.14457.json",
    "total_tokens": 888,
    "translated_title": "gTBLS：通过条件问答从文本中生成表格",
    "translated_abstract": "arXiv:2403.14457v1 公告类型：新的 摘要：将大段的非结构化文本提炼为结构化、简化的形式，如表格，是一个开放的研究问题。自动生成表格的主要挑战之一是确保其句法有效性。之前的方法通过在Transformer的注意力机制中包含额外的参数，以便注意特定的行和列标题来解决这一挑战。与这种单阶段方法相反，本文提出了一种名为生成式表格（gTBLS）的两阶段方法。第一阶段从文本中推断表格结构（行和列标题）。第二阶段利用这些标题提出问题，并微调因果语言模型来回答这些问题。此外，gTBLS方法易于在零短配置中利用预训练的大型语言模型，为在不能进行微调的情况下生成表格提供了解决方案。gTBLS提高了之前方法达到的效果",
    "tldr": "gTBLS通过两阶段方法从文本中生成表格，第一阶段推断表格结构，第二阶段利用结构提出问题并通过微调语言模型来回答，能够在零短配置下利用预训练的大型语言模型，改进了先前方法的效果。",
    "en_tdlr": "gTBLS generates tables from text through a two-stage approach – inferring table structure in the first stage and formulating questions in the second stage using a causal language model, with the ability to utilize pre-trained large language models in a zero-shot configuration, improving upon prior methods."
}