{
    "title": "Reusing Historical Trajectories in Natural Policy Gradient via Importance Sampling: Convergence and Convergence Rate",
    "abstract": "arXiv:2403.00675v1 Announce Type: new  Abstract: Reinforcement learning provides a mathematical framework for learning-based control, whose success largely depends on the amount of data it can utilize. The efficient utilization of historical trajectories obtained from previous policies is essential for expediting policy optimization. Empirical evidence has shown that policy gradient methods based on importance sampling work well. However, existing literature often neglect the interdependence between trajectories from different iterations, and the good empirical performance lacks a rigorous theoretical justification. In this paper, we study a variant of the natural policy gradient method with reusing historical trajectories via importance sampling. We show that the bias of the proposed estimator of the gradient is asymptotically negligible, the resultant algorithm is convergent, and reusing past trajectories helps improve the convergence rate. We further apply the proposed estimator to ",
    "link": "https://arxiv.org/abs/2403.00675",
    "context": "Title: Reusing Historical Trajectories in Natural Policy Gradient via Importance Sampling: Convergence and Convergence Rate\nAbstract: arXiv:2403.00675v1 Announce Type: new  Abstract: Reinforcement learning provides a mathematical framework for learning-based control, whose success largely depends on the amount of data it can utilize. The efficient utilization of historical trajectories obtained from previous policies is essential for expediting policy optimization. Empirical evidence has shown that policy gradient methods based on importance sampling work well. However, existing literature often neglect the interdependence between trajectories from different iterations, and the good empirical performance lacks a rigorous theoretical justification. In this paper, we study a variant of the natural policy gradient method with reusing historical trajectories via importance sampling. We show that the bias of the proposed estimator of the gradient is asymptotically negligible, the resultant algorithm is convergent, and reusing past trajectories helps improve the convergence rate. We further apply the proposed estimator to ",
    "path": "papers/24/03/2403.00675.json",
    "total_tokens": 810,
    "translated_title": "通过重要性抽样在自然策略梯度中重用历史轨迹：收敛性和收敛速率",
    "translated_abstract": "强化学习提供了一个学习控制的数学框架，其成功在很大程度上取决于它可以利用的数据量。有效利用先前策略得到的历史轨迹对于加快策略优化至关重要。实证证据表明基于重要性抽样的策略梯度方法效果良好。然而，现有文献往往忽视了不同迭代之间轨迹的相互依赖性，且良好的实证表现缺乏严格的理论证明。本文研究了一种通过重要性抽样重新利用历史轨迹的自然策略梯度方法的变体。我们表明了所提梯度估计器的偏差渐近可忽略，得到的算法是收敛的，并且重用过去的轨迹有助于提高收敛速率。我们进一步将所提估计器应用于",
    "tldr": "通过重要性抽样在自然策略梯度中重用历史轨迹可提高收敛速率",
    "en_tdlr": "Reusing historical trajectories via importance sampling in natural policy gradient methods improves the convergence rate."
}