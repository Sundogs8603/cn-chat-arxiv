{
    "title": "$\\textbf{S}^2$IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting",
    "abstract": "arXiv:2403.05798v1 Announce Type: new  Abstract: Recently, there has been a growing interest in leveraging pre-trained large language models (LLMs) for various time series applications. However, the semantic space of LLMs, established through the pre-training, is still underexplored and may help yield more distinctive and informative representations to facilitate time series forecasting. To this end, we propose Semantic Space Informed Prompt learning with LLM ($S^2$IP-LLM) to align the pre-trained semantic space with time series embeddings space and perform time series forecasting based on learned prompts from the joint space. We first design a tokenization module tailored for cross-modality alignment, which explicitly concatenates patches of decomposed time series components to create embeddings that effectively encode the temporal dynamics. Next, we leverage the pre-trained word token embeddings to derive semantic anchors and align selected anchors with time series embeddings by maxi",
    "link": "https://arxiv.org/abs/2403.05798",
    "context": "Title: $\\textbf{S}^2$IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting\nAbstract: arXiv:2403.05798v1 Announce Type: new  Abstract: Recently, there has been a growing interest in leveraging pre-trained large language models (LLMs) for various time series applications. However, the semantic space of LLMs, established through the pre-training, is still underexplored and may help yield more distinctive and informative representations to facilitate time series forecasting. To this end, we propose Semantic Space Informed Prompt learning with LLM ($S^2$IP-LLM) to align the pre-trained semantic space with time series embeddings space and perform time series forecasting based on learned prompts from the joint space. We first design a tokenization module tailored for cross-modality alignment, which explicitly concatenates patches of decomposed time series components to create embeddings that effectively encode the temporal dynamics. Next, we leverage the pre-trained word token embeddings to derive semantic anchors and align selected anchors with time series embeddings by maxi",
    "path": "papers/24/03/2403.05798.json",
    "total_tokens": 895,
    "translated_title": "$\\textbf{S}^2$IP-LLM: 借助LLM进行时间序列预测的语义空间提示学习",
    "translated_abstract": "最近，利用预训练的大型语言模型（LLM）进行各种时间序列应用引起了越来越多的关注。然而，通过预训练建立的LLM的语义空间仍然未被充分探索，可能有助于产生更加独特和信息丰富的表示，以促进时间序列预测。为此，我们提出了借助LLM进行语义空间提示学习（$\\textbf{S}^2$IP-LLM），将预训练的语义空间与时间序列嵌入空间进行对齐，并基于联合空间中学到的提示进行时间序列预测。我们首先设计了一个专为跨模态对齐定制的标记化模块，显式地串联分解的时间序列组件的补丁，以创建能够有效编码时间动态的嵌入。接下来，我们利用预训练的单词标记嵌入来导出语义锚点，并通过最大化对齐所选锚点与时间序列嵌入。",
    "tldr": "提出了$\\textbf{S}^2$IP-LLM，利用预训练的语言模型进行时间序列预测，并将语义空间与时间序列嵌入空间对齐进行提示学习。",
    "en_tdlr": "Proposed $\\textbf{S}^2$IP-LLM, which utilizes pre-trained language models for time series forecasting and aligns semantic space with time series embeddings for prompt learning."
}