{
    "title": "TroubleLLM: Align to Red Team Expert",
    "abstract": "arXiv:2403.00829v1 Announce Type: new  Abstract: Large Language Models (LLMs) become the start-of-the-art solutions for a variety of natural language tasks and are integrated into real-world applications. However, LLMs can be potentially harmful in manifesting undesirable safety issues like social biases and toxic content. It is imperative to assess its safety issues before deployment. However, the quality and diversity of test prompts generated by existing methods are still far from satisfactory. Not only are these methods labor-intensive and require large budget costs, but the controllability of test prompt generation is lacking for the specific testing domain of LLM applications. With the idea of LLM for LLM testing, we propose the first LLM, called TroubleLLM, to generate controllable test prompts on LLM safety issues. Extensive experiments and human evaluation illustrate the superiority of TroubleLLM on generation quality and generation controllability.",
    "link": "https://arxiv.org/abs/2403.00829",
    "context": "Title: TroubleLLM: Align to Red Team Expert\nAbstract: arXiv:2403.00829v1 Announce Type: new  Abstract: Large Language Models (LLMs) become the start-of-the-art solutions for a variety of natural language tasks and are integrated into real-world applications. However, LLMs can be potentially harmful in manifesting undesirable safety issues like social biases and toxic content. It is imperative to assess its safety issues before deployment. However, the quality and diversity of test prompts generated by existing methods are still far from satisfactory. Not only are these methods labor-intensive and require large budget costs, but the controllability of test prompt generation is lacking for the specific testing domain of LLM applications. With the idea of LLM for LLM testing, we propose the first LLM, called TroubleLLM, to generate controllable test prompts on LLM safety issues. Extensive experiments and human evaluation illustrate the superiority of TroubleLLM on generation quality and generation controllability.",
    "path": "papers/24/03/2403.00829.json",
    "total_tokens": 735,
    "translated_title": "TroubleLLM: 对齐红队专家",
    "translated_abstract": "大型语言模型（LLMs）已成为各种自然语言任务的最先进解决方案，并被整合到现实世界的应用中。然而，LLMs可能在展现诸如社会偏见和有毒内容等不良安全问题方面具有潜在危害。在部署之前评估其安全问题至关重要。然而，现有方法生成的测试提示的质量和多样性仍然远远不尽人意。这些方法不仅劳动密集且需要大量预算成本，而且测试提示生成的可控性在LLM应用的具体测试领域中缺乏。",
    "tldr": "TroubleLLM是第一个用于生成关于LLMs安全问题的可控测试提示的LLM，通过广泛实验和人类评估展示了其在生成质量和生成可控性方面的优越性",
    "en_tdlr": "TroubleLLM is the first LLM designed to generate controllable test prompts on safety issues of LLMs, demonstrating superiority in generation quality and controllability through extensive experiments and human evaluation."
}