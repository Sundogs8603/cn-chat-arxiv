{
    "title": "Leveraging Large Language Model-based Room-Object Relationships Knowledge for Enhancing Multimodal-Input Object Goal Navigation",
    "abstract": "arXiv:2403.14163v1 Announce Type: cross  Abstract: Object-goal navigation is a crucial engineering task for the community of embodied navigation; it involves navigating to an instance of a specified object category within unseen environments. Although extensive investigations have been conducted on both end-to-end and modular-based, data-driven approaches, fully enabling an agent to comprehend the environment through perceptual knowledge and perform object-goal navigation as efficiently as humans remains a significant challenge. Recently, large language models have shown potential in this task, thanks to their powerful capabilities for knowledge extraction and integration. In this study, we propose a data-driven, modular-based approach, trained on a dataset that incorporates common-sense knowledge of object-to-room relationships extracted from a large language model. We utilize the multi-channel Swin-Unet architecture to conduct multi-task learning incorporating with multimodal inputs.",
    "link": "https://arxiv.org/abs/2403.14163",
    "context": "Title: Leveraging Large Language Model-based Room-Object Relationships Knowledge for Enhancing Multimodal-Input Object Goal Navigation\nAbstract: arXiv:2403.14163v1 Announce Type: cross  Abstract: Object-goal navigation is a crucial engineering task for the community of embodied navigation; it involves navigating to an instance of a specified object category within unseen environments. Although extensive investigations have been conducted on both end-to-end and modular-based, data-driven approaches, fully enabling an agent to comprehend the environment through perceptual knowledge and perform object-goal navigation as efficiently as humans remains a significant challenge. Recently, large language models have shown potential in this task, thanks to their powerful capabilities for knowledge extraction and integration. In this study, we propose a data-driven, modular-based approach, trained on a dataset that incorporates common-sense knowledge of object-to-room relationships extracted from a large language model. We utilize the multi-channel Swin-Unet architecture to conduct multi-task learning incorporating with multimodal inputs.",
    "path": "papers/24/03/2403.14163.json",
    "total_tokens": 856,
    "translated_title": "利用基于大型语言模型的房间-物体关系知识增强多模态输入的目标导航",
    "translated_abstract": "物体目标导航是具有身体封装导航任务的一个关键工程任务；它涉及在看不见的环境中导航到指定物体类别的一个实例。尽管在端到端和模块化的数据驱动方法上进行了大量研究，但要完全使代理人通过感知知识理解环境并像人类一样有效地执行物体目标导航仍然是一个重大挑战。最近，大型语言模型在这一任务中显示出潜力，这要归功于它们在知识提取和整合方面的强大能力。在这项研究中，我们提出了一种基于数据驱动的模块化方法，该方法在数据集上进行训练，该数据集包含从大型语言模型提取的对象-房间关系的常识知识。我们利用多通道Swin-Unet架构进行多任务学习，同时结合多模态输入。",
    "tldr": "提出了一种基于大型语言模型的房间-物体关系知识的数据驱动、模块化方法，利用多通道Swin-Unet架构进行多任务学习，以增强多模态输入的目标导航。",
    "en_tdlr": "A data-driven, modular-based approach leveraging large language model-based room-object relationships knowledge is proposed, utilizing multi-channel Swin-Unet architecture for multi-task learning to enhance multimodal-input object goal navigation."
}