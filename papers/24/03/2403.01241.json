{
    "title": "IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact",
    "abstract": "arXiv:2403.01241v1 Announce Type: cross  Abstract: Large language models (LLMs) excel in natural language processing but demand intensive computation. To mitigate this, various quantization methods have been explored, yet they compromise LLM performance. This paper unveils a previously overlooked type of outlier in LLMs. Such outliers are found to allocate most of the attention scores on initial tokens of input, termed as pivot tokens, which is crucial to the performance of quantized LLMs. Given that, we propose IntactKV to generate the KV cache of pivot tokens losslessly from the full-precision model. The approach is simple and easy to combine with existing quantization solutions. Besides, IntactKV can be calibrated as additional LLM parameters to boost the quantized LLMs further. Mathematical analysis also proves that IntactKV effectively reduces the upper bound of quantization error. Empirical results show that IntactKV brings consistent improvement and achieves lossless weight-only",
    "link": "https://arxiv.org/abs/2403.01241",
    "context": "Title: IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact\nAbstract: arXiv:2403.01241v1 Announce Type: cross  Abstract: Large language models (LLMs) excel in natural language processing but demand intensive computation. To mitigate this, various quantization methods have been explored, yet they compromise LLM performance. This paper unveils a previously overlooked type of outlier in LLMs. Such outliers are found to allocate most of the attention scores on initial tokens of input, termed as pivot tokens, which is crucial to the performance of quantized LLMs. Given that, we propose IntactKV to generate the KV cache of pivot tokens losslessly from the full-precision model. The approach is simple and easy to combine with existing quantization solutions. Besides, IntactKV can be calibrated as additional LLM parameters to boost the quantized LLMs further. Mathematical analysis also proves that IntactKV effectively reduces the upper bound of quantization error. Empirical results show that IntactKV brings consistent improvement and achieves lossless weight-only",
    "path": "papers/24/03/2403.01241.json",
    "total_tokens": 911,
    "translated_title": "IntactKV: 通过保持关键标记完整改进大型语言模型量化",
    "translated_abstract": "大型语言模型在自然语言处理方面表现出色，但需要大量计算。为了减少这一难题，人们已经探索了各种量化方法，然而这些方法会损害大型语言模型的性能。本文揭示了大型语言模型中一个以前被忽视的异常点类型。这些异常点被发现将大部分注意力分配给输入的初始标记，被称为关键标记，这对于量化的大型语言模型的性能至关重要。鉴于此，我们提出了IntactKV，从完整精度模型中无损地生成关键标记的KV缓存。这种方法简单易行，可以轻松与现有的量化解决方案结合。此外，IntactKV可以被校准为额外的大型语言模型参数，以进一步增强量化的大型语言模型。数学分析还证明了IntactKV有效地降低了量化误差的上限。实证结果表明，IntactKV带来了持续的改进，并实现了无损的仅权重量。",
    "tldr": "本研究提出了IntactKV方法，通过保持关键标记的完整性，改善了大型语言模型的量化过程，进一步降低了量化误差的上限，并取得了显著的性能提升。",
    "en_tdlr": "This paper introduces IntactKV method, which improves the quantization process of large language models by preserving the integrity of pivotal tokens, significantly reducing the upper bound of quantization error and achieving notable performance gains."
}