{
    "title": "EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling",
    "abstract": "arXiv:2403.14541v1 Announce Type: new  Abstract: Recently, Large Language Models (LLMs) have demonstrated outstanding performance across a wide range of downstream language tasks. Temperature sampling is a commonly used decoding strategy for LLMs' generation process. However, a fixed temperature parameter is used in most cases, which may not always be an optimal choice for balancing generation quality and diversity. In this paper, we propose an effective Entropy-based Dynamic Temperature (EDT) Sampling method, to achieve a more balanced performance in terms of both generation quality and diversity by dynamically selecting the temperature parameter. Additionally, we also show model performance and comprehensive analyses for 4 different generation benchmarks. Our experiments show that EDT significantly outperforms the existing strategies across different tasks.",
    "link": "https://arxiv.org/abs/2403.14541",
    "context": "Title: EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling\nAbstract: arXiv:2403.14541v1 Announce Type: new  Abstract: Recently, Large Language Models (LLMs) have demonstrated outstanding performance across a wide range of downstream language tasks. Temperature sampling is a commonly used decoding strategy for LLMs' generation process. However, a fixed temperature parameter is used in most cases, which may not always be an optimal choice for balancing generation quality and diversity. In this paper, we propose an effective Entropy-based Dynamic Temperature (EDT) Sampling method, to achieve a more balanced performance in terms of both generation quality and diversity by dynamically selecting the temperature parameter. Additionally, we also show model performance and comprehensive analyses for 4 different generation benchmarks. Our experiments show that EDT significantly outperforms the existing strategies across different tasks.",
    "path": "papers/24/03/2403.14541.json",
    "total_tokens": 761,
    "translated_title": "通过基于熵的动态温度采样改进大型语言模型的生成",
    "translated_abstract": "最近，大型语言模型（LLMs）在各种下游语言任务中展现出了出色的性能。温度采样是LLMs生成过程中常用的解码策略。然而，大多数情况下使用固定的温度参数，这可能并非始终是平衡生成质量和多样性的最佳选择。在本文中，我们提出了一种有效的基于熵的动态温度（EDT）采样方法，通过动态选择温度参数实现在生成质量和多样性方面更平衡的性能。此外，我们还展示了4个不同生成基准的模型性能和全面分析。我们的实验表明，EDT在不同任务中明显优于现有策略。",
    "tldr": "通过提出基于熵的动态温度采样方法，本文在大语言模型的生成中实现了更平衡的性能表现，并在四个不同生成基准上展示了显著优于现有策略的结果。"
}