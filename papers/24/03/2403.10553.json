{
    "title": "Learning to Watermark LLM-generated Text via Reinforcement Learning",
    "abstract": "arXiv:2403.10553v1 Announce Type: cross  Abstract: We study how to watermark LLM outputs, i.e. embedding algorithmically detectable signals into LLM-generated text to track misuse. Unlike the current mainstream methods that work with a fixed LLM, we expand the watermark design space by including the LLM tuning stage in the watermark pipeline. While prior works focus on token-level watermark that embeds signals into the output, we design a model-level watermark that embeds signals into the LLM weights, and such signals can be detected by a paired detector. We propose a co-training framework based on reinforcement learning that iteratively (1) trains a detector to detect the generated watermarked text and (2) tunes the LLM to generate text easily detectable by the detector while keeping its normal utility. We empirically show that our watermarks are more accurate, robust, and adaptable (to new attacks). It also allows watermarked model open-sourcing. In addition, if used together with al",
    "link": "https://arxiv.org/abs/2403.10553",
    "context": "Title: Learning to Watermark LLM-generated Text via Reinforcement Learning\nAbstract: arXiv:2403.10553v1 Announce Type: cross  Abstract: We study how to watermark LLM outputs, i.e. embedding algorithmically detectable signals into LLM-generated text to track misuse. Unlike the current mainstream methods that work with a fixed LLM, we expand the watermark design space by including the LLM tuning stage in the watermark pipeline. While prior works focus on token-level watermark that embeds signals into the output, we design a model-level watermark that embeds signals into the LLM weights, and such signals can be detected by a paired detector. We propose a co-training framework based on reinforcement learning that iteratively (1) trains a detector to detect the generated watermarked text and (2) tunes the LLM to generate text easily detectable by the detector while keeping its normal utility. We empirically show that our watermarks are more accurate, robust, and adaptable (to new attacks). It also allows watermarked model open-sourcing. In addition, if used together with al",
    "path": "papers/24/03/2403.10553.json",
    "total_tokens": 931,
    "translated_title": "通过强化学习学习给LLM生成的文本添加水印",
    "translated_abstract": "我们研究如何给LLM生成的文本添加水印，即将可以通过算法检测到的信号嵌入LLM生成的文本中，以追踪滥用情况。与当前主流方法不同，我们通过将LLM调优阶段纳入水印流程，扩展了水印设计空间。先前的研究侧重于在输出中嵌入信号的标记级水印，我们设计了一种模型级水印，将信号嵌入LLM的权重中，可以被配对的检测器检测到。我们提出了一个基于强化学习的协同训练框架，迭代地（1）训练一个检测器来检测生成的带水印文本，并（2）调整LLM以生成检测器轻松检测到但保持正常效用的文本。我们实验证明，我们的水印更准确、更稳健且更适应（应对新攻击）。它还允许水印模型开源。此外，如果与al一起使用",
    "tldr": "通过将信号嵌入LLM的权重中，我们设计一种模型级水印，有效追踪生成文本的滥用情况，并提出基于强化学习的协同训练框架，使水印更准确、更稳健且更适应新攻击。",
    "en_tdlr": "By embedding signals into the LLM weights, we design a model-level watermark to effectively track the misuse of generated text, and propose a co-training framework based on reinforcement learning to make the watermark more accurate, robust, and adaptable to new attacks."
}