{
    "title": "Parallel Hyperparameter Optimization Of Spiking Neural Network",
    "abstract": "arXiv:2403.00450v1 Announce Type: cross  Abstract: Spiking Neural Networks (SNN). SNNs are based on a more biologically inspired approach than usual artificial neural networks. Such models are characterized by complex dynamics between neurons and spikes. These are very sensitive to the hyperparameters, making their optimization challenging. To tackle hyperparameter optimization of SNNs, we initially extended the signal loss issue of SNNs to what we call silent networks. These networks fail to emit enough spikes at their outputs due to mistuned hyperparameters or architecture. Generally, search spaces are heavily restrained, sometimes even discretized, to prevent the sampling of such networks. By defining an early stopping criterion detecting silent networks and by designing specific constraints, we were able to instantiate larger and more flexible search spaces. We applied a constrained Bayesian optimization technique, which was asynchronously parallelized, as the evaluation time of a ",
    "link": "https://arxiv.org/abs/2403.00450",
    "context": "Title: Parallel Hyperparameter Optimization Of Spiking Neural Network\nAbstract: arXiv:2403.00450v1 Announce Type: cross  Abstract: Spiking Neural Networks (SNN). SNNs are based on a more biologically inspired approach than usual artificial neural networks. Such models are characterized by complex dynamics between neurons and spikes. These are very sensitive to the hyperparameters, making their optimization challenging. To tackle hyperparameter optimization of SNNs, we initially extended the signal loss issue of SNNs to what we call silent networks. These networks fail to emit enough spikes at their outputs due to mistuned hyperparameters or architecture. Generally, search spaces are heavily restrained, sometimes even discretized, to prevent the sampling of such networks. By defining an early stopping criterion detecting silent networks and by designing specific constraints, we were able to instantiate larger and more flexible search spaces. We applied a constrained Bayesian optimization technique, which was asynchronously parallelized, as the evaluation time of a ",
    "path": "papers/24/03/2403.00450.json",
    "total_tokens": 857,
    "translated_title": "并行脉冲神经网络的超参数优化",
    "translated_abstract": "脉冲神经网络（SNN）。SNN基于比通常的人工神经网络更具生物启发的方法。这些模型的特点是神经元和脉冲之间的复杂动态。这些模型对超参数非常敏感，使得它们的优化具有挑战性。为了解决SNN的超参数优化问题，我们首先将SNN的信号损失问题扩展到我们称之为静默网络。这些网络由于超参数或架构设置不当而无法在输出端发出足够的脉冲。一般来说，搜索空间受到严格限制，有时甚至是离散化的，以防止对这样的网络进行采样。通过定义一个早停准则来检测静默网络，并设计特定的约束，我们能够实例化更大、更灵活的搜索空间。我们应用了一种受约束的贝叶斯优化技术，并将其异步并行化，因为一个评估时间",
    "tldr": "该研究通过引入静默网络的概念，并设计特定约束，实现了更大更灵活的脉冲神经网络超参数的优化搜索空间，并应用了并行化的受约束贝叶斯优化技术。"
}