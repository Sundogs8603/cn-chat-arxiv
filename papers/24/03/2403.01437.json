{
    "title": "GPTSee: Enhancing Moment Retrieval and Highlight Detection via Description-Based Similarity Features",
    "abstract": "arXiv:2403.01437v1 Announce Type: cross  Abstract: Moment retrieval (MR) and highlight detection (HD) aim to identify relevant moments and highlights in video from corresponding natural language query. Large language models (LLMs) have demonstrated proficiency in various computer vision tasks. However, existing methods for MR\\&HD have not yet been integrated with LLMs. In this letter, we propose a novel two-stage model that takes the output of LLMs as the input to the second-stage transformer encoder-decoder. First, MiniGPT-4 is employed to generate the detailed description of the video frame and rewrite the query statement, fed into the encoder as new features. Then, semantic similarity is computed between the generated description and the rewritten queries. Finally, continuous high-similarity video frames are converted into span anchors, serving as prior position information for the decoder. Experiments demonstrate that our approach achieves a state-of-the-art result, and by using on",
    "link": "https://arxiv.org/abs/2403.01437",
    "context": "Title: GPTSee: Enhancing Moment Retrieval and Highlight Detection via Description-Based Similarity Features\nAbstract: arXiv:2403.01437v1 Announce Type: cross  Abstract: Moment retrieval (MR) and highlight detection (HD) aim to identify relevant moments and highlights in video from corresponding natural language query. Large language models (LLMs) have demonstrated proficiency in various computer vision tasks. However, existing methods for MR\\&HD have not yet been integrated with LLMs. In this letter, we propose a novel two-stage model that takes the output of LLMs as the input to the second-stage transformer encoder-decoder. First, MiniGPT-4 is employed to generate the detailed description of the video frame and rewrite the query statement, fed into the encoder as new features. Then, semantic similarity is computed between the generated description and the rewritten queries. Finally, continuous high-similarity video frames are converted into span anchors, serving as prior position information for the decoder. Experiments demonstrate that our approach achieves a state-of-the-art result, and by using on",
    "path": "papers/24/03/2403.01437.json",
    "total_tokens": 867,
    "translated_title": "GPTSee：通过基于描述的相似特征增强时刻检索和重点检测",
    "translated_abstract": "时刻检索（MR）和重点检测（HD）旨在从相应的自然语言查询中识别视频中的相关时刻和重点。大型语言模型（LLMs）已经展示了在各种计算机视觉任务中的熟练程度。然而，现有的MR和HD方法尚未与LLMs集成。在这封信中，我们提出了一种新颖的两阶段模型，将LLMs的输出作为第二阶段变压器编码器-解码器的输入。首先，利用MiniGPT-4生成视频帧的详细描述并重写查询语句，将其作为新特征输入编码器。然后计算生成描述和重写查询之间的语义相似性。最后，连续高相似性视频帧被转换为范围锚点，作为解码器的先验位置信息。实验证明，我们的方法达到了最先进的结果，并通过使用...",
    "tldr": "该研究提出了一个新颖的两阶段模型，将大型语言模型（LLMs）的输出用作第二阶段变压器编码器-解码器的输入，实现了时刻检索和重点检测的最先进结果。"
}