{
    "title": "Improving Implicit Regularization of SGD with Preconditioning for Least Square Problems",
    "abstract": "arXiv:2403.08585v1 Announce Type: new  Abstract: Stochastic gradient descent (SGD) exhibits strong algorithmic regularization effects in practice and plays an important role in the generalization of modern machine learning. However, prior research has revealed instances where the generalization performance of SGD is worse than ridge regression due to uneven optimization along different dimensions. Preconditioning offers a natural solution to this issue by rebalancing optimization across different directions. Yet, the extent to which preconditioning can enhance the generalization performance of SGD and whether it can bridge the existing gap with ridge regression remains uncertain. In this paper, we study the generalization performance of SGD with preconditioning for the least squared problem. We make a comprehensive comparison between preconditioned SGD and (standard \\& preconditioned) ridge regression. Our study makes several key contributions toward understanding and improving SGD wit",
    "link": "https://arxiv.org/abs/2403.08585",
    "context": "Title: Improving Implicit Regularization of SGD with Preconditioning for Least Square Problems\nAbstract: arXiv:2403.08585v1 Announce Type: new  Abstract: Stochastic gradient descent (SGD) exhibits strong algorithmic regularization effects in practice and plays an important role in the generalization of modern machine learning. However, prior research has revealed instances where the generalization performance of SGD is worse than ridge regression due to uneven optimization along different dimensions. Preconditioning offers a natural solution to this issue by rebalancing optimization across different directions. Yet, the extent to which preconditioning can enhance the generalization performance of SGD and whether it can bridge the existing gap with ridge regression remains uncertain. In this paper, we study the generalization performance of SGD with preconditioning for the least squared problem. We make a comprehensive comparison between preconditioned SGD and (standard \\& preconditioned) ridge regression. Our study makes several key contributions toward understanding and improving SGD wit",
    "path": "papers/24/03/2403.08585.json",
    "total_tokens": 863,
    "translated_title": "通过预条件化改善最小二乘问题随机梯度下降的隐式正则化",
    "translated_abstract": "随机梯度下降（SGD）在实践中表现出强大的算法正则化效果，在现代机器学习的泛化中起着重要作用。然而，先前的研究发现，SGD的泛化性能有时会比岭回归差，这是由于沿不同维度的优化不均匀造成的。预条件化通过重新平衡沿不同方向的优化来提供解决这一问题的自然方法。然而，预条件化能够提升SGD的泛化性能的程度以及它是否能够填补现有与岭回归之间的差距仍不确定。本文研究了预条件化对最小二乘问题中SGD的泛化性能的影响。我们全面比较了预条件化SGD与（标准和预条件化）岭回归。我们的研究对了解和改善SGD做出了几项关键贡献。",
    "tldr": "通过预条件化，研究了SGD在最小二乘问题中的泛化性能，对比了预条件化SGD和（标准和预条件化）岭回归，为改善SGD理解和应用提供了关键贡献。",
    "en_tdlr": "Investigated the generalization performance of SGD in least square problems with preconditioning, compared preconditioned SGD with (standard and preconditioned) ridge regression, and made key contributions to understanding and improving the application of SGD."
}