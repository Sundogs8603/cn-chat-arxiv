{
    "title": "Software Vulnerability and Functionality Assessment using LLMs",
    "abstract": "arXiv:2403.08429v1 Announce Type: cross  Abstract: While code review is central to the software development process, it can be tedious and expensive to carry out. In this paper, we investigate whether and how Large Language Models (LLMs) can aid with code reviews. Our investigation focuses on two tasks that we argue are fundamental to good reviews: (i) flagging code with security vulnerabilities and (ii) performing software functionality validation, i.e., ensuring that code meets its intended functionality. To test performance on both tasks, we use zero-shot and chain-of-thought prompting to obtain final ``approve or reject'' recommendations. As data, we employ seminal code generation datasets (HumanEval and MBPP) along with expert-written code snippets with security vulnerabilities from the Common Weakness Enumeration (CWE). Our experiments consider a mixture of three proprietary models from OpenAI and smaller open-source LLMs. We find that the former outperforms the latter by a large",
    "link": "https://arxiv.org/abs/2403.08429",
    "context": "Title: Software Vulnerability and Functionality Assessment using LLMs\nAbstract: arXiv:2403.08429v1 Announce Type: cross  Abstract: While code review is central to the software development process, it can be tedious and expensive to carry out. In this paper, we investigate whether and how Large Language Models (LLMs) can aid with code reviews. Our investigation focuses on two tasks that we argue are fundamental to good reviews: (i) flagging code with security vulnerabilities and (ii) performing software functionality validation, i.e., ensuring that code meets its intended functionality. To test performance on both tasks, we use zero-shot and chain-of-thought prompting to obtain final ``approve or reject'' recommendations. As data, we employ seminal code generation datasets (HumanEval and MBPP) along with expert-written code snippets with security vulnerabilities from the Common Weakness Enumeration (CWE). Our experiments consider a mixture of three proprietary models from OpenAI and smaller open-source LLMs. We find that the former outperforms the latter by a large",
    "path": "papers/24/03/2403.08429.json",
    "total_tokens": 851,
    "translated_title": "使用LLMs进行软件漏洞和功能评估",
    "translated_abstract": "虽然代码审查在软件开发过程中至关重要，但进行代码审查可能会很繁琐且昂贵。在本文中，我们调查了大型语言模型（LLMs）是否可以帮助进行代码审查，重点关注两项我们认为对良好审查至关重要的任务：（i）标记具有安全漏洞的代码和（ii）执行软件功能验证，即确保代码符合其预期功能。为了测试在这两个任务上的表现，我们使用零镜像和链式提示来获得最终的“批准或拒绝”建议。作为数据，我们使用了里程碑式的代码生成数据集（HumanEval和MBPP），以及来自通用弱点枚举（CWE）的带有安全漏洞的专家编写代码片段。我们的实验考虑了来自OpenAI的三个专有模型和较小的开源LLMs的组合。我们发现前者表现优于后者",
    "tldr": "本文调查了如何使用大型语言模型（LLMs）协助进行代码审查，重点关注标记安全漏洞代码和执行软件功能验证两个任务，结果显示专有模型表现优于开源模型",
    "en_tdlr": "This paper investigates how to use Large Language Models (LLMs) to assist with code reviews, focusing on flagging code with security vulnerabilities and performing software functionality validation, and the results show that proprietary models outperform open-source ones."
}