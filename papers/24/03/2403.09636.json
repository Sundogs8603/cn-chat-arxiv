{
    "title": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference",
    "abstract": "arXiv:2403.09636v1 Announce Type: new  Abstract: Transformers have emerged as the backbone of large language models (LLMs). However, generation remains inefficient due to the need to store in memory a cache of key-value representations for past tokens, whose size scales linearly with the input sequence length and batch size. As a solution, we propose Dynamic Memory Compression (DMC), a method for on-line key-value cache compression at inference time. Most importantly, the model learns to apply different compression rates in different heads and layers. We retrofit pre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers, achieving up to ~3.7x throughput increase in auto-regressive inference on a NVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible percentage of the original data without adding any extra parameters. We find that DMC preserves the original downstream performance with up to 4x cache compression, outperforming up-trained grouped-query a",
    "link": "https://arxiv.org/abs/2403.09636",
    "context": "Title: Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference\nAbstract: arXiv:2403.09636v1 Announce Type: new  Abstract: Transformers have emerged as the backbone of large language models (LLMs). However, generation remains inefficient due to the need to store in memory a cache of key-value representations for past tokens, whose size scales linearly with the input sequence length and batch size. As a solution, we propose Dynamic Memory Compression (DMC), a method for on-line key-value cache compression at inference time. Most importantly, the model learns to apply different compression rates in different heads and layers. We retrofit pre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers, achieving up to ~3.7x throughput increase in auto-regressive inference on a NVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible percentage of the original data without adding any extra parameters. We find that DMC preserves the original downstream performance with up to 4x cache compression, outperforming up-trained grouped-query a",
    "path": "papers/24/03/2403.09636.json",
    "total_tokens": 953,
    "translated_title": "动态内存压缩：用于加速推断的LLMs的改装",
    "translated_abstract": "Transformers已经成为大型语言模型（LLMs）的支柱。然而，由于需要在内存中存储关键-值表示的缓存以用于过去的标记，其大小与输入序列长度和批处理大小呈线性比例，因此生成仍然低效。作为解决方案，我们提出了动态内存压缩（DMC），这是一种用于在线关键-值缓存压缩的方法。最重要的是，模型学习在不同的头部和层中应用不同的压缩率。我们将预训练的LLMs（如Llama 2（7B、13B和70B））改装为DMC Transformers，在NVIDIA H100 GPU上的自回归推断中实现了高达~3.7倍的吞吐量增加。DMC通过在原始数据的可忽略百分比上进行持续的预训练而应用，并且不添加任何额外参数。我们发现，在高达4倍缓存压缩的情况下，DMC保留了原始的下游性能，优于up-trained grouped-query a。",
    "tldr": "提出了动态内存压缩（DMC）方法，用于在线关键-值缓存压缩，模型学习在不同的头部和层中应用不同的压缩率，并且通过将预训练的LLMs改装为DMC Transformers，在自回归推断中实现了高达~3.7倍的吞吐量增加。",
    "en_tdlr": "Introduced Dynamic Memory Compression (DMC) method for online key-value cache compression, where the model learns to apply different compression rates in different heads and layers, achieving up to ~3.7x throughput increase in auto-regressive inference by retrofitting pre-trained LLMS into DMC Transformers."
}