{
    "title": "Sample Complexity of Offline Distributionally Robust Linear Markov Decision Processes",
    "abstract": "arXiv:2403.12946v1 Announce Type: new  Abstract: In offline reinforcement learning (RL), the absence of active exploration calls for attention on the model robustness to tackle the sim-to-real gap, where the discrepancy between the simulated and deployed environments can significantly undermine the performance of the learned policy. To endow the learned policy with robustness in a sample-efficient manner in the presence of high-dimensional state-action space, this paper considers the sample complexity of distributionally robust linear Markov decision processes (MDPs) with an uncertainty set characterized by the total variation distance using offline data. We develop a pessimistic model-based algorithm and establish its sample complexity bound under minimal data coverage assumptions, which outperforms prior art by at least $\\tilde{O}(d)$, where $d$ is the feature dimension. We further improve the performance guarantee of the proposed algorithm by incorporating a carefully-designed varia",
    "link": "https://arxiv.org/abs/2403.12946",
    "context": "Title: Sample Complexity of Offline Distributionally Robust Linear Markov Decision Processes\nAbstract: arXiv:2403.12946v1 Announce Type: new  Abstract: In offline reinforcement learning (RL), the absence of active exploration calls for attention on the model robustness to tackle the sim-to-real gap, where the discrepancy between the simulated and deployed environments can significantly undermine the performance of the learned policy. To endow the learned policy with robustness in a sample-efficient manner in the presence of high-dimensional state-action space, this paper considers the sample complexity of distributionally robust linear Markov decision processes (MDPs) with an uncertainty set characterized by the total variation distance using offline data. We develop a pessimistic model-based algorithm and establish its sample complexity bound under minimal data coverage assumptions, which outperforms prior art by at least $\\tilde{O}(d)$, where $d$ is the feature dimension. We further improve the performance guarantee of the proposed algorithm by incorporating a carefully-designed varia",
    "path": "papers/24/03/2403.12946.json",
    "total_tokens": 890,
    "translated_title": "线性马尔可夫决策过程的离线分布鲁棒性样本复杂度",
    "translated_abstract": "在离线强化学习（RL）中，缺乏积极探索需要关注模型的鲁棒性，以解决模拟和部署环境之间的差距，其中模拟和实际环境之间的差异可能严重损害学习策略的性能。为了以样本高效的方式赋予学习策略在高维状态-动作空间中的鲁棒性，本文考虑使用离线数据，通过总变差距离表征的不确定性集合，分布鲁棒线性马尔可夫决策过程（MDPs）的样本复杂性。我们开发了一种悲观模型算法，并在最小数据覆盖假设下建立了其样本复杂性界限，其性能至少比以前的方法优于$\\tilde{O}(d)$，其中$d$是特征维度。",
    "tldr": "本文研究了离线强化学习中线性马尔可夫决策过程的分布鲁棒性样本复杂度问题，提出了一种悲观模型算法并建立了其样本复杂性界限，能在高维状态-动作空间中提高学习策略的性能。",
    "en_tdlr": "This paper explores the sample complexity of distributionally robust linear Markov decision processes in offline reinforcement learning, proposes a pessimistic model-based algorithm, and establishes its sample complexity bound to enhance the performance of learned policy in high-dimensional state-action space."
}