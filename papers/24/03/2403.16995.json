{
    "title": "Language Rectified Flow: Advancing Diffusion Language Generation with Probabilistic Flows",
    "abstract": "arXiv:2403.16995v1 Announce Type: cross  Abstract: Recent works have demonstrated success in controlling sentence attributes ($e.g.$, sentiment) and structure ($e.g.$, syntactic structure) based on the diffusion language model. A key component that drives theimpressive performance for generating high-quality samples from noise is iteratively denoise for thousands of steps. While beneficial, the complexity of starting from the noise and the learning steps has limited its implementation to many NLP real-world applications. This paper proposes Language Rectified Flow ({\\ours}). Our method is based on the reformulation of the standard probabilistic flow models. Language rectified flow learns (neural) ordinary differential equation models to transport between the source distribution and the target distribution, hence providing a unified and effective solution to generative modeling and domain transfer. From the source distribution, our language rectified flow yields fast simulation and effe",
    "link": "https://arxiv.org/abs/2403.16995",
    "context": "Title: Language Rectified Flow: Advancing Diffusion Language Generation with Probabilistic Flows\nAbstract: arXiv:2403.16995v1 Announce Type: cross  Abstract: Recent works have demonstrated success in controlling sentence attributes ($e.g.$, sentiment) and structure ($e.g.$, syntactic structure) based on the diffusion language model. A key component that drives theimpressive performance for generating high-quality samples from noise is iteratively denoise for thousands of steps. While beneficial, the complexity of starting from the noise and the learning steps has limited its implementation to many NLP real-world applications. This paper proposes Language Rectified Flow ({\\ours}). Our method is based on the reformulation of the standard probabilistic flow models. Language rectified flow learns (neural) ordinary differential equation models to transport between the source distribution and the target distribution, hence providing a unified and effective solution to generative modeling and domain transfer. From the source distribution, our language rectified flow yields fast simulation and effe",
    "path": "papers/24/03/2403.16995.json",
    "total_tokens": 820,
    "translated_title": "语言校正流：通过概率流推动扩散语言生成",
    "translated_abstract": "最近的研究表明，在扩散语言模型基础上控制句子属性（例如情感）和结构（例如句法结构）取得了成功。一个推动高质量样本生成的关键组成部分是迭代去噪数千步。尽管有益，但从噪声开始的复杂性和学习步骤限制了其在许多NLP实际应用中的实现。本文提出了Language Rectified Flow方法。我们的方法基于标准概率流模型的重构。语言校正流学习（神经）常微分方程模型在源分布和目标分布之间传输，为生成建模和域转移提供了统一和有效的解决方案。从源分布开始，我们的语言校正流产生快速仿真和有效。",
    "tldr": "语言校正流是一种基于标准概率流模型的新方法，通过学习常微分方程模型在源分布和目标分布之间传输，提供了统一和有效的生成模型和领域转移解决方案。",
    "en_tdlr": "Language Rectified Flow is a novel method based on standard probabilistic flow models, which learns ordinary differential equation models to transport between the source distribution and the target distribution, providing a unified and effective solution for generative modeling and domain transfer."
}