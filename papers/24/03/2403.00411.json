{
    "title": "Cross-Lingual Learning vs. Low-Resource Fine-Tuning: A Case Study with Fact-Checking in Turkish",
    "abstract": "arXiv:2403.00411v1 Announce Type: new  Abstract: The rapid spread of misinformation through social media platforms has raised concerns regarding its impact on public opinion. While misinformation is prevalent in other languages, the majority of research in this field has concentrated on the English language. Hence, there is a scarcity of datasets for other languages, including Turkish. To address this concern, we have introduced the FCTR dataset, consisting of 3238 real-world claims. This dataset spans multiple domains and incorporates evidence collected from three Turkish fact-checking organizations. Additionally, we aim to assess the effectiveness of cross-lingual transfer learning for low-resource languages, with a particular focus on Turkish. We demonstrate in-context learning (zero-shot and few-shot) performance of large language models in this context. The experimental results indicate that the dataset has the potential to advance research in the Turkish language.",
    "link": "https://arxiv.org/abs/2403.00411",
    "context": "Title: Cross-Lingual Learning vs. Low-Resource Fine-Tuning: A Case Study with Fact-Checking in Turkish\nAbstract: arXiv:2403.00411v1 Announce Type: new  Abstract: The rapid spread of misinformation through social media platforms has raised concerns regarding its impact on public opinion. While misinformation is prevalent in other languages, the majority of research in this field has concentrated on the English language. Hence, there is a scarcity of datasets for other languages, including Turkish. To address this concern, we have introduced the FCTR dataset, consisting of 3238 real-world claims. This dataset spans multiple domains and incorporates evidence collected from three Turkish fact-checking organizations. Additionally, we aim to assess the effectiveness of cross-lingual transfer learning for low-resource languages, with a particular focus on Turkish. We demonstrate in-context learning (zero-shot and few-shot) performance of large language models in this context. The experimental results indicate that the dataset has the potential to advance research in the Turkish language.",
    "path": "papers/24/03/2403.00411.json",
    "total_tokens": 878,
    "translated_title": "跨语言学习与低资源微调：以土耳其事实检查为例的案例研究",
    "translated_abstract": "通过社交媒体平台迅速传播错误信息引起了人们对其对公众舆论的影响的担忧。尽管错误信息在其他语言中普遍存在，但该领域的大部分研究集中在英语上。因此，其他语言，包括土耳其语，的数据集稀缺。为了解决这一问题，我们介绍了由3238个真实声明组成的FCTR数据集。该数据集涵盖多个领域，并整合了三家土耳其事实检查组织收集的证据。此外，我们旨在评估跨语言转移学习对于低资源语言的有效性，特别关注土耳其语。我们展示了大型语言模型在这一背景下的上下文学习（零次和少次）表现。实验结果表明，该数据集有推动土耳其语研究的潜力。",
    "tldr": "提出了FCTR数据集，旨在解决英语以外语言，尤其是土耳其语，的数据稀缺问题，并探讨了跨语言转移学习在低资源语言中的有效性，实验证明数据集潜力推动土耳其语研究。",
    "en_tdlr": "Introduced the FCTR dataset to address the scarcity of data in languages other than English, particularly Turkish, and examined the effectiveness of cross-lingual transfer learning in low-resource languages, with experimental results demonstrating the dataset's potential to advance research in the Turkish language."
}