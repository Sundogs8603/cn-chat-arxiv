{
    "title": "Upper Bound of Bayesian Generalization Error in Partial Concept Bottleneck Model (CBM): Partial CBM outperforms naive CBM",
    "abstract": "arXiv:2403.09206v1 Announce Type: cross  Abstract: Concept Bottleneck Model (CBM) is a methods for explaining neural networks. In CBM, concepts which correspond to reasons of outputs are inserted in the last intermediate layer as observed values. It is expected that we can interpret the relationship between the output and concept similar to linear regression. However, this interpretation requires observing all concepts and decreases the generalization performance of neural networks. Partial CBM (PCBM), which uses partially observed concepts, has been devised to resolve these difficulties. Although some numerical experiments suggest that the generalization performance of PCBMs is almost as high as that of the original neural networks, the theoretical behavior of its generalization error has not been yet clarified since PCBM is singular statistical model. In this paper, we reveal the Bayesian generalization error in PCBM with a three-layered and linear architecture. The result indcates t",
    "link": "https://arxiv.org/abs/2403.09206",
    "context": "Title: Upper Bound of Bayesian Generalization Error in Partial Concept Bottleneck Model (CBM): Partial CBM outperforms naive CBM\nAbstract: arXiv:2403.09206v1 Announce Type: cross  Abstract: Concept Bottleneck Model (CBM) is a methods for explaining neural networks. In CBM, concepts which correspond to reasons of outputs are inserted in the last intermediate layer as observed values. It is expected that we can interpret the relationship between the output and concept similar to linear regression. However, this interpretation requires observing all concepts and decreases the generalization performance of neural networks. Partial CBM (PCBM), which uses partially observed concepts, has been devised to resolve these difficulties. Although some numerical experiments suggest that the generalization performance of PCBMs is almost as high as that of the original neural networks, the theoretical behavior of its generalization error has not been yet clarified since PCBM is singular statistical model. In this paper, we reveal the Bayesian generalization error in PCBM with a three-layered and linear architecture. The result indcates t",
    "path": "papers/24/03/2403.09206.json",
    "total_tokens": 861,
    "translated_title": "贝叶斯概化错误在部分概念瓶颈模型中的上界：部分CBM胜过朴素CBM",
    "translated_abstract": "arXiv：2403.09206v1 类型通告：交叉摘要：概念瓶颈模型（CBM）是解释神经网络的方法。在CBM中，对应于输出原因的概念被插入到最后一个中间层作为观察值。人们预期我们可以解释输出和概念之间的关系，类似于线性回归。然而，这种解释需要观察所有概念，并且降低了神经网络的泛化性能。部分CBM（PCBM）使用部分观察到的概念，旨在解决这些困难。尽管一些数值实验表明PCBM的泛化性能几乎与原始神经网络一样高，但由于PCBM是奇异的统计模型，其泛化错误的理论行为尚未明确。在本文中，我们揭示了具有三层线性架构的PCBM中的贝叶斯泛化错误。",
    "tldr": "本文在三层线性结构的部分CBM中揭示了贝叶斯概化错误的上界，进一步证明部分CBM优于朴素CBM。"
}