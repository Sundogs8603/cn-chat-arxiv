{
    "title": "Are Large Language Models Good at Utility Judgments?",
    "abstract": "arXiv:2403.19216v1 Announce Type: new  Abstract: Retrieval-augmented generation (RAG) is considered to be a promising approach to alleviate the hallucination issue of large language models (LLMs), and it has received widespread attention from researchers recently. Due to the limitation in the semantic understanding of retrieval models, the success of RAG heavily lies on the ability of LLMs to identify passages with utility. Recent efforts have explored the ability of LLMs to assess the relevance of passages in retrieval, but there has been limited work on evaluating the utility of passages in supporting question answering. In this work, we conduct a comprehensive study about the capabilities of LLMs in utility evaluation for open-domain QA. Specifically, we introduce a benchmarking procedure and collection of candidate passages with different characteristics, facilitating a series of experiments with five representative LLMs. Our experiments reveal that: (i) well-instructed LLMs can di",
    "link": "https://arxiv.org/abs/2403.19216",
    "context": "Title: Are Large Language Models Good at Utility Judgments?\nAbstract: arXiv:2403.19216v1 Announce Type: new  Abstract: Retrieval-augmented generation (RAG) is considered to be a promising approach to alleviate the hallucination issue of large language models (LLMs), and it has received widespread attention from researchers recently. Due to the limitation in the semantic understanding of retrieval models, the success of RAG heavily lies on the ability of LLMs to identify passages with utility. Recent efforts have explored the ability of LLMs to assess the relevance of passages in retrieval, but there has been limited work on evaluating the utility of passages in supporting question answering. In this work, we conduct a comprehensive study about the capabilities of LLMs in utility evaluation for open-domain QA. Specifically, we introduce a benchmarking procedure and collection of candidate passages with different characteristics, facilitating a series of experiments with five representative LLMs. Our experiments reveal that: (i) well-instructed LLMs can di",
    "path": "papers/24/03/2403.19216.json",
    "total_tokens": 827,
    "translated_title": "大型语言模型擅长实用性判断吗？",
    "translated_abstract": "检索增强生成（RAG）被认为是缓解大型语言模型（LLMs）幻觉问题的一种有前途的方法，并且近期已受到研究人员的广泛关注。由于检索模型在语义理解上的局限性，RAG的成功在很大程度上取决于LLMs识别具有实用性的段落的能力。最近的研究探讨了LLMs评估检索中段落相关性的能力，但对评估支持问答的段落实用性的工作还很有限。在本工作中，我们进行了一项关于LLMs在开放域QA实用性评估方面能力的全面研究。具体而言，我们引入了一个基准测试程序和不同特征的候选段落集合，促进了与五个代表性LLMs的一系列实验。我们的实验证明：（i）受过良好指导的LLMs可以进行...",
    "tldr": "大型语言模型（LLMs）在评估段落实用性方面的能力进行了全面研究，实验发现受过良好指导的LLMs可以..."
}