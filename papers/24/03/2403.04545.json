{
    "title": "Improve Generalization Ability of Deep Wide Residual Network with A Suitable Scaling Factor",
    "abstract": "arXiv:2403.04545v1 Announce Type: new  Abstract: Deep Residual Neural Networks (ResNets) have demonstrated remarkable success across a wide range of real-world applications. In this paper, we identify a suitable scaling factor (denoted by $\\alpha$) on the residual branch of deep wide ResNets to achieve good generalization ability. We show that if $\\alpha$ is a constant, the class of functions induced by Residual Neural Tangent Kernel (RNTK) is asymptotically not learnable, as the depth goes to infinity. We also highlight a surprising phenomenon: even if we allow $\\alpha$ to decrease with increasing depth $L$, the degeneration phenomenon may still occur. However, when $\\alpha$ decreases rapidly with $L$, the kernel regression with deep RNTK with early stopping can achieve the minimax rate provided that the target regression function falls in the reproducing kernel Hilbert space associated with the infinite-depth RNTK. Our simulation studies on synthetic data and real classification task",
    "link": "https://arxiv.org/abs/2403.04545",
    "context": "Title: Improve Generalization Ability of Deep Wide Residual Network with A Suitable Scaling Factor\nAbstract: arXiv:2403.04545v1 Announce Type: new  Abstract: Deep Residual Neural Networks (ResNets) have demonstrated remarkable success across a wide range of real-world applications. In this paper, we identify a suitable scaling factor (denoted by $\\alpha$) on the residual branch of deep wide ResNets to achieve good generalization ability. We show that if $\\alpha$ is a constant, the class of functions induced by Residual Neural Tangent Kernel (RNTK) is asymptotically not learnable, as the depth goes to infinity. We also highlight a surprising phenomenon: even if we allow $\\alpha$ to decrease with increasing depth $L$, the degeneration phenomenon may still occur. However, when $\\alpha$ decreases rapidly with $L$, the kernel regression with deep RNTK with early stopping can achieve the minimax rate provided that the target regression function falls in the reproducing kernel Hilbert space associated with the infinite-depth RNTK. Our simulation studies on synthetic data and real classification task",
    "path": "papers/24/03/2403.04545.json",
    "total_tokens": 897,
    "translated_title": "通过合适的缩放因子提高深广残差网络的泛化能力",
    "translated_abstract": "深残差神经网络（ResNets）在广泛的实际应用中取得了显著的成功。在本文中，我们确定了深广残差网络中残差分支上的合适缩放因子（用$\\alpha$表示），以实现良好的泛化能力。我们展示了如果$\\alpha$是一个常数，由残差神经切向核（RNTK）引发的函数类在深度趋近无穷时是渐近不可学习的。我们还强调了一个令人惊讶的现象：即使我们允许$\\alpha$随着深度$L$的增加而减小，退化现象仍可能发生。然而，当$\\alpha$与$L$快速减小时，使用深层RNTK进行核回归，并且在早停条件下可以实现最小最大速率，前提是目标回归函数落在与无限深度RNTK相关的再生核希尔伯特空间中。我们对合成数据和真实分类任务进行了模拟研究。",
    "tldr": "通过在深广残差网络中使用适当的缩放因子，可以提高泛化能力，即使允许缩放因子随深度减小，也可以实现最小最大速率。",
    "en_tdlr": "The generalization ability of deep wide Residual Networks can be improved by using a suitable scaling factor, even allowing the scaling factor to decrease with depth, achieving the minimax rate."
}