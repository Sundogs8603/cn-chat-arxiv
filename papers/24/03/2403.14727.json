{
    "title": "Protected group bias and stereotypes in Large Language Models",
    "abstract": "arXiv:2403.14727v1 Announce Type: cross  Abstract: As modern Large Language Models (LLMs) shatter many state-of-the-art benchmarks in a variety of domains, this paper investigates their behavior in the domains of ethics and fairness, focusing on protected group bias. We conduct a two-part study: first, we solicit sentence continuations describing the occupations of individuals from different protected groups, including gender, sexuality, religion, and race. Second, we have the model generate stories about individuals who hold different types of occupations. We collect >10k sentence completions made by a publicly available LLM, which we subject to human annotation. We find bias across minoritized groups, but in particular in the domains of gender and sexuality, as well as Western bias, in model generations. The model not only reflects societal biases, but appears to amplify them. The model is additionally overly cautious in replies to queries relating to minoritized groups, providing re",
    "link": "https://arxiv.org/abs/2403.14727",
    "context": "Title: Protected group bias and stereotypes in Large Language Models\nAbstract: arXiv:2403.14727v1 Announce Type: cross  Abstract: As modern Large Language Models (LLMs) shatter many state-of-the-art benchmarks in a variety of domains, this paper investigates their behavior in the domains of ethics and fairness, focusing on protected group bias. We conduct a two-part study: first, we solicit sentence continuations describing the occupations of individuals from different protected groups, including gender, sexuality, religion, and race. Second, we have the model generate stories about individuals who hold different types of occupations. We collect >10k sentence completions made by a publicly available LLM, which we subject to human annotation. We find bias across minoritized groups, but in particular in the domains of gender and sexuality, as well as Western bias, in model generations. The model not only reflects societal biases, but appears to amplify them. The model is additionally overly cautious in replies to queries relating to minoritized groups, providing re",
    "path": "papers/24/03/2403.14727.json",
    "total_tokens": 883,
    "translated_title": "大型语言模型中的受保护群体偏见和刻板印象",
    "translated_abstract": "随着现代大型语言模型在各种领域中打破许多最新技术基准，本文调查了它们在伦理和公平领域的行为，重点关注受保护群体偏见。我们进行了两部分研究：首先，我们征集了描述来自不同受保护群体（包括性别、性取向、宗教和种族）个人职业的句子延续；其次，我们让模型生成关于拥有不同类型职业的个人的故事。我们收集了一款公开可用的大型语言模型生成的 >10k 个句子延续，受到人类标注。我们发现模型在被边缘化群体中存在偏见，尤其在性别和性取向领域，以及在模型生成中存在西方偏见。模型不仅反映了社会偏见，还似乎放大了这些偏见。该模型对于与边缘化群体相关的查询回复过于谨慎，提供了",
    "tldr": "该研究调查了大型语言模型在伦理和公平领域中的行为，发现模型不仅反映了社会偏见，还似乎放大了这些偏见。",
    "en_tdlr": "This study investigated the behavior of Large Language Models in the domains of ethics and fairness, revealing that the models not only reflect societal biases but also appear to amplify them."
}