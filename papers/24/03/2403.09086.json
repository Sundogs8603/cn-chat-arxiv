{
    "title": "Learning from straggler clients in federated learning",
    "abstract": "arXiv:2403.09086v1 Announce Type: new  Abstract: How well do existing federated learning algorithms learn from client devices that return model updates with a significant time delay? Is it even possible to learn effectively from clients that report back minutes, hours, or days after being scheduled? We answer these questions by developing Monte Carlo simulations of client latency that are guided by real-world applications. We study synchronous optimization algorithms like FedAvg and FedAdam as well as the asynchronous FedBuff algorithm, and observe that all these existing approaches struggle to learn from severely delayed clients. To improve upon this situation, we experiment with modifications, including distillation regularization and exponential moving averages of model weights. Finally, we introduce two new algorithms, FARe-DUST and FeAST-on-MSG, based on distillation and averaging, respectively. Experiments with the EMNIST, CIFAR-100, and StackOverflow benchmark federated learning",
    "link": "https://arxiv.org/abs/2403.09086",
    "context": "Title: Learning from straggler clients in federated learning\nAbstract: arXiv:2403.09086v1 Announce Type: new  Abstract: How well do existing federated learning algorithms learn from client devices that return model updates with a significant time delay? Is it even possible to learn effectively from clients that report back minutes, hours, or days after being scheduled? We answer these questions by developing Monte Carlo simulations of client latency that are guided by real-world applications. We study synchronous optimization algorithms like FedAvg and FedAdam as well as the asynchronous FedBuff algorithm, and observe that all these existing approaches struggle to learn from severely delayed clients. To improve upon this situation, we experiment with modifications, including distillation regularization and exponential moving averages of model weights. Finally, we introduce two new algorithms, FARe-DUST and FeAST-on-MSG, based on distillation and averaging, respectively. Experiments with the EMNIST, CIFAR-100, and StackOverflow benchmark federated learning",
    "path": "papers/24/03/2403.09086.json",
    "total_tokens": 856,
    "translated_title": "从联邦学习中学习迟到的客户端",
    "translated_abstract": "现有的联邦学习算法有多大程度上能够从返回具有显著时间延迟的模型更新的客户设备中进行学习？学习效果是否可能受到客户端在被安排后几分钟、几小时或几天才报告回来的影响？通过开发由实际应用指导的蒙特卡洛模拟客户端延迟，我们回答了这些问题。我们研究了像FedAvg和FedAdam这样的同步优化算法，以及异步FedBuff算法，并观察到所有这些现有方法都难以从严重延迟的客户端中学习。为了改善这种情况，我们尝试了修改，包括蒸馏正则化和模型权重的指数移动平均值。最后，我们介绍了两种新算法，FARe-DUST和FeAST-on-MSG，分别基于蒸馏和平均化。对EMNIST、CIFAR-100和StackOverflow基准联邦学习进行了实验",
    "tldr": "现有联邦学习算法难以从严重延迟的客户端学习，为了改进这种情况，引入了两种新算法FARe-DUST和FeAST-on-MSG。",
    "en_tdlr": "Existing federated learning algorithms struggle to learn from severely delayed clients, leading to the introduction of two new algorithms, FARe-DUST and FeAST-on-MSG, to address this issue."
}