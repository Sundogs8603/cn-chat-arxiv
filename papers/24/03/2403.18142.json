{
    "title": "HERTA: A High-Efficiency and Rigorous Training Algorithm for Unfolded Graph Neural Networks",
    "abstract": "arXiv:2403.18142v1 Announce Type: new  Abstract: As a variant of Graph Neural Networks (GNNs), Unfolded GNNs offer enhanced interpretability and flexibility over traditional designs. Nevertheless, they still suffer from scalability challenges when it comes to the training cost. Although many methods have been proposed to address the scalability issues, they mostly focus on per-iteration efficiency, without worst-case convergence guarantees. Moreover, those methods typically add components to or modify the original model, thus possibly breaking the interpretability of Unfolded GNNs. In this paper, we propose HERTA: a High-Efficiency and Rigorous Training Algorithm for Unfolded GNNs that accelerates the whole training process, achieving a nearly-linear time worst-case training guarantee. Crucially, HERTA converges to the optimum of the original model, thus preserving the interpretability of Unfolded GNNs. Additionally, as a byproduct of HERTA, we propose a new spectral sparsification met",
    "link": "https://arxiv.org/abs/2403.18142",
    "context": "Title: HERTA: A High-Efficiency and Rigorous Training Algorithm for Unfolded Graph Neural Networks\nAbstract: arXiv:2403.18142v1 Announce Type: new  Abstract: As a variant of Graph Neural Networks (GNNs), Unfolded GNNs offer enhanced interpretability and flexibility over traditional designs. Nevertheless, they still suffer from scalability challenges when it comes to the training cost. Although many methods have been proposed to address the scalability issues, they mostly focus on per-iteration efficiency, without worst-case convergence guarantees. Moreover, those methods typically add components to or modify the original model, thus possibly breaking the interpretability of Unfolded GNNs. In this paper, we propose HERTA: a High-Efficiency and Rigorous Training Algorithm for Unfolded GNNs that accelerates the whole training process, achieving a nearly-linear time worst-case training guarantee. Crucially, HERTA converges to the optimum of the original model, thus preserving the interpretability of Unfolded GNNs. Additionally, as a byproduct of HERTA, we propose a new spectral sparsification met",
    "path": "papers/24/03/2403.18142.json",
    "total_tokens": 926,
    "translated_title": "HERTA：一种高效且严格的展开图神经网络训练算法",
    "translated_abstract": "作为图神经网络（GNNs）的一种变体，展开GNNs在传统设计上提供了更强的可解释性和灵活性。然而，当涉及到训练成本时，它们仍然面临着可扩展性挑战。尽管已经提出了许多方法来解决这些可扩展性问题，但它们大多集中在每次迭代的效率上，没有最坏情况收敛保证。此外，这些方法通常会向原模型添加组件或进行修改，可能会破坏展开GNNs的可解释性。在本文中，我们提出了HERTA：一种用于加速整个训练过程的高效且严格的展开GNNs训练算法，实现了近线性时间最坏情况训练保证。至关重要的是，HERTA收敛到原始模型的最优解，从而保持了展开GNNs的可解释性。此外，作为HERTA的副产品，我们提出了一种新的谱稀疏化方法。",
    "tldr": "HERTA是针对展开图神经网络提出的一种高效且严格的训练算法，加速整个训练过程并保留模型的可解释性，同时实现了近线性时间最坏情况训练保证。",
    "en_tdlr": "HERTA is a high-efficiency and rigorous training algorithm proposed for unfolded graph neural networks, accelerating the entire training process while preserving the interpretability of the model, and achieving nearly-linear time worst-case training guarantee."
}