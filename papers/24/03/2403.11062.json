{
    "title": "A Simple Mixture Policy Parameterization for Improving Sample Efficiency of CVaR Optimization",
    "abstract": "arXiv:2403.11062v1 Announce Type: new  Abstract: Reinforcement learning algorithms utilizing policy gradients (PG) to optimize Conditional Value at Risk (CVaR) face significant challenges with sample inefficiency, hindering their practical applications. This inefficiency stems from two main facts: a focus on tail-end performance that overlooks many sampled trajectories, and the potential of gradient vanishing when the lower tail of the return distribution is overly flat. To address these challenges, we propose a simple mixture policy parameterization. This method integrates a risk-neutral policy with an adjustable policy to form a risk-averse policy. By employing this strategy, all collected trajectories can be utilized for policy updating, and the issue of vanishing gradients is counteracted by stimulating higher returns through the risk-neutral component, thus lifting the tail and preventing flatness. Our empirical study reveals that this mixture parameterization is uniquely effectiv",
    "link": "https://arxiv.org/abs/2403.11062",
    "context": "Title: A Simple Mixture Policy Parameterization for Improving Sample Efficiency of CVaR Optimization\nAbstract: arXiv:2403.11062v1 Announce Type: new  Abstract: Reinforcement learning algorithms utilizing policy gradients (PG) to optimize Conditional Value at Risk (CVaR) face significant challenges with sample inefficiency, hindering their practical applications. This inefficiency stems from two main facts: a focus on tail-end performance that overlooks many sampled trajectories, and the potential of gradient vanishing when the lower tail of the return distribution is overly flat. To address these challenges, we propose a simple mixture policy parameterization. This method integrates a risk-neutral policy with an adjustable policy to form a risk-averse policy. By employing this strategy, all collected trajectories can be utilized for policy updating, and the issue of vanishing gradients is counteracted by stimulating higher returns through the risk-neutral component, thus lifting the tail and preventing flatness. Our empirical study reveals that this mixture parameterization is uniquely effectiv",
    "path": "papers/24/03/2403.11062.json",
    "total_tokens": 779,
    "translated_title": "用于提高CVaR优化样本效率的简单混合策略参数化",
    "translated_abstract": "利用策略梯度(PG)优化条件值风险(CVaR)的强化学习算法在提高样本效率方面面临着重大挑战，限制了它们的实际应用。为了解决这些挑战，我们提出了一种简单的混合策略参数化方法。这种方法将风险中性策略与可调整策略整合为一个风险厌恶策略。通过采用这种策略，所有收集到的轨迹都可以用于策略更新，并且通过风险中性组件刺激更高的回报，从而提升尾部并防止扁平化。我们的实证研究表明，这种混合参数化是非常有效的。",
    "tldr": "提出了一种简单的混合策略参数化方法，通过整合风险中性策略和可调整策略，提高了CVaR优化的样本效率。",
    "en_tdlr": "A simple mixture policy parameterization is proposed to improve the sample efficiency of CVaR optimization by integrating a risk-neutral policy with an adjustable policy."
}