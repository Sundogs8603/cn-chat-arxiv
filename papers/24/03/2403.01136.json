{
    "title": "LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization",
    "abstract": "arXiv:2403.01136v1 Announce Type: cross  Abstract: Recent breakthroughs in Large-scale language models (LLMs) have demonstrated impressive performance on various tasks. The immense sizes of LLMs have led to very high resource demand and cost for running the models. Though the models are largely served using uniform high-caliber GPUs nowadays, utilizing a heterogeneous cluster with a mix of available high- and low-capacity GPUs can potentially substantially reduce the serving cost. There is a lack of designs to support efficient LLM serving using a heterogeneous cluster, while the current solutions focus on model partition and uniform compression among homogeneous devices. This paper proposes LLM-PQ, a system that advocates adaptive model quantization and phase-aware partition to improve LLM serving efficiency on heterogeneous GPU clusters. We carefully decide on mixed-precision model quantization together with phase-aware model partition and micro-batch sizing in distributed LLM servin",
    "link": "https://arxiv.org/abs/2403.01136",
    "context": "Title: LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization\nAbstract: arXiv:2403.01136v1 Announce Type: cross  Abstract: Recent breakthroughs in Large-scale language models (LLMs) have demonstrated impressive performance on various tasks. The immense sizes of LLMs have led to very high resource demand and cost for running the models. Though the models are largely served using uniform high-caliber GPUs nowadays, utilizing a heterogeneous cluster with a mix of available high- and low-capacity GPUs can potentially substantially reduce the serving cost. There is a lack of designs to support efficient LLM serving using a heterogeneous cluster, while the current solutions focus on model partition and uniform compression among homogeneous devices. This paper proposes LLM-PQ, a system that advocates adaptive model quantization and phase-aware partition to improve LLM serving efficiency on heterogeneous GPU clusters. We carefully decide on mixed-precision model quantization together with phase-aware model partition and micro-batch sizing in distributed LLM servin",
    "path": "papers/24/03/2403.01136.json",
    "total_tokens": 837,
    "translated_title": "在具有相位感知分区和自适应量化的异构集群上提供LLM-PQ",
    "translated_abstract": "最近大规模语言模型（LLMs）的突破性进展在各种任务上展示了令人印象深刻的性能。LLMs的巨大规模导致了非常高的资源需求和成本。尽管目前主要使用统一高性能GPU来服务这些模型，但利用一种混合可用高低容量GPU的异构集群可能会大幅降低服务成本。然而，目前缺乏支持使用异构集群高效提供LLM服务的设计，而当前的解决方案主要集中在模型分区和均匀压缩在同质设备之间。本文提出了LLM-PQ，这是一个倡导自适应模型量化和相位感知分区以提高异构GPU集群上LLM服务效率的系统。我们在分布式LLM服务中仔细选择了混合精度模型量化、相位感知模型分区和微批量大小。",
    "tldr": "这项研究提出了LLM-PQ系统，通过采用自适应模型量化和相位感知分区，在异构GPU集群上提高了LLM服务的效率。",
    "en_tdlr": "This research introduces the LLM-PQ system, which improves the efficiency of LLM serving on heterogeneous GPU clusters by employing adaptive model quantization and phase-aware partitioning."
}