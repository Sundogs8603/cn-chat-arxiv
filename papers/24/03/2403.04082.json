{
    "title": "Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference",
    "abstract": "arXiv:2403.04082v1 Announce Type: new  Abstract: Given time series data, how can we answer questions like \"what will happen in the future?\" and \"how did we get here?\" These sorts of probabilistic inference questions are challenging when observations are high-dimensional. In this paper, we show how these questions can have compact, closed form solutions in terms of learned representations. The key idea is to apply a variant of contrastive learning to time series data. Prior work already shows that the representations learned by contrastive learning encode a probability ratio. By extending prior work to show that the marginal distribution over representations is Gaussian, we can then prove that joint distribution of representations is also Gaussian. Taken together, these results show that representations learned via temporal contrastive learning follow a Gauss-Markov chain, a graphical model where inference (e.g., prediction, planning) over representations corresponds to inverting a low-",
    "link": "https://arxiv.org/abs/2403.04082",
    "context": "Title: Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference\nAbstract: arXiv:2403.04082v1 Announce Type: new  Abstract: Given time series data, how can we answer questions like \"what will happen in the future?\" and \"how did we get here?\" These sorts of probabilistic inference questions are challenging when observations are high-dimensional. In this paper, we show how these questions can have compact, closed form solutions in terms of learned representations. The key idea is to apply a variant of contrastive learning to time series data. Prior work already shows that the representations learned by contrastive learning encode a probability ratio. By extending prior work to show that the marginal distribution over representations is Gaussian, we can then prove that joint distribution of representations is also Gaussian. Taken together, these results show that representations learned via temporal contrastive learning follow a Gauss-Markov chain, a graphical model where inference (e.g., prediction, planning) over representations corresponds to inverting a low-",
    "path": "papers/24/03/2403.04082.json",
    "total_tokens": 790,
    "translated_title": "通过插值进行推断：对比表示可证明启用规划和推断",
    "translated_abstract": "给定时间序列数据，我们如何回答诸如“未来会发生什么？”和“我们是如何到达这里的？”这类概率推断问题在观测值为高维时具有挑战性。本文展示了这些问题如何通过学习表示的紧凑闭式解决方案。关键思想是将对比学习的变体应用于时间序列数据。之前的工作已经表明，通过对比学习学到的表示编码了概率比。通过将之前的工作扩展以表明表示的边际分布是高斯分布，我们随后证明表示的联合分布也是高斯分布。这些结果共同表明，通过时间对比学习学到的表示遵循高斯马尔可夫链，一种图形模型，其中对表示进行的推断（例如预测、规划）对应于反演低维分布。",
    "tldr": "通过对比学习学到的时间序列数据表示遵循高斯马尔可夫链，从而启用规划和推断",
    "en_tdlr": "Learned representations of time series data via contrastive learning follow a Gauss-Markov chain, enabling planning and inference."
}