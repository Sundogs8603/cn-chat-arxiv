{
    "title": "Model Parallelism on Distributed Infrastructure: A Literature Review from Theory to LLM Case-Studies",
    "abstract": "arXiv:2403.03699v1 Announce Type: cross  Abstract: Neural networks have become a cornerstone of machine learning. As the trend for these to get more and more complex continues, so does the underlying hardware and software infrastructure for training and deployment. In this survey we answer three research questions: \"What types of model parallelism exist?\", \"What are the challenges of model parallelism?\", and \"What is a modern use-case of model parallelism?\" We answer the first question by looking at how neural networks can be parallelised and expressing these as operator graphs while exploring the available dimensions. The dimensions along which neural networks can be parallelised are intra-operator and inter-operator. We answer the second question by collecting and listing both implementation challenges for the types of parallelism, as well as the problem of optimally partitioning the operator graph. We answer the last question by collecting and listing how parallelism is applied in m",
    "link": "https://arxiv.org/abs/2403.03699",
    "context": "Title: Model Parallelism on Distributed Infrastructure: A Literature Review from Theory to LLM Case-Studies\nAbstract: arXiv:2403.03699v1 Announce Type: cross  Abstract: Neural networks have become a cornerstone of machine learning. As the trend for these to get more and more complex continues, so does the underlying hardware and software infrastructure for training and deployment. In this survey we answer three research questions: \"What types of model parallelism exist?\", \"What are the challenges of model parallelism?\", and \"What is a modern use-case of model parallelism?\" We answer the first question by looking at how neural networks can be parallelised and expressing these as operator graphs while exploring the available dimensions. The dimensions along which neural networks can be parallelised are intra-operator and inter-operator. We answer the second question by collecting and listing both implementation challenges for the types of parallelism, as well as the problem of optimally partitioning the operator graph. We answer the last question by collecting and listing how parallelism is applied in m",
    "path": "papers/24/03/2403.03699.json",
    "total_tokens": 909,
    "translated_title": "分布式基础设施上的模型并行性：从理论到LLM案例研究的文献综述",
    "translated_abstract": "神经网络已经成为机器学习的基石。随着这些网络变得越来越复杂的趋势持续进行，用于训练和部署的基础硬件和软件基础设施也在不断发展。在这项调查中，我们回答了三个研究问题：“存在哪些模型并行性类型？”，“模型并行性的挑战是什么？”，以及“模型并行性的现代用例是什么？” 我们通过研究神经网络的并行化方式以及将其表达为操作图来回答第一个问题，同时探索可用的维度。神经网络可以并行化的维度包括操作内部并行和操作之间并行。我们通过收集和列出并行化类型的实施挑战以及操作图的最佳划分问题来回答第二个问题。最后，我们通过收集和列出并行化如何应用在m",
    "tldr": "这项研究从理论到LLM案例研究综述了分布式基础设施上的模型并行性，探讨了模型并行性类型、挑战和现代用例。神经网络可以通过操作内部和操作之间并行化，但实施挑战包括操作图的最佳划分。",
    "en_tdlr": "This study provides a literature review on model parallelism on distributed infrastructure, covering types, challenges, and modern use-cases. Neural networks can be parallelized along intra-operator and inter-operator dimensions, with challenges including optimal partitioning of operator graphs."
}