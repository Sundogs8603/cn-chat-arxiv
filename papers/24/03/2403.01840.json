{
    "title": "FreeA: Human-object Interaction Detection using Free Annotation Labels",
    "abstract": "arXiv:2403.01840v1 Announce Type: cross  Abstract: Recent human-object interaction (HOI) detection approaches rely on high cost of manpower and require comprehensive annotated image datasets. In this paper, we propose a novel self-adaption language-driven HOI detection method, termed as FreeA, without labeling by leveraging the adaptability of CLIP to generate latent HOI labels. To be specific, FreeA matches image features of human-object pairs with HOI text templates, and a priori knowledge-based mask method is developed to suppress improbable interactions. In addition, FreeA utilizes the proposed interaction correlation matching method to enhance the likelihood of actions related to a specified action, further refine the generated HOI labels. Experiments on two benchmark datasets show that FreeA achieves state-of-the-art performance among weakly supervised HOI models. Our approach is +8.58 mean Average Precision (mAP) on HICO-DET and +1.23 mAP on V-COCO more accurate in localizing an",
    "link": "https://arxiv.org/abs/2403.01840",
    "context": "Title: FreeA: Human-object Interaction Detection using Free Annotation Labels\nAbstract: arXiv:2403.01840v1 Announce Type: cross  Abstract: Recent human-object interaction (HOI) detection approaches rely on high cost of manpower and require comprehensive annotated image datasets. In this paper, we propose a novel self-adaption language-driven HOI detection method, termed as FreeA, without labeling by leveraging the adaptability of CLIP to generate latent HOI labels. To be specific, FreeA matches image features of human-object pairs with HOI text templates, and a priori knowledge-based mask method is developed to suppress improbable interactions. In addition, FreeA utilizes the proposed interaction correlation matching method to enhance the likelihood of actions related to a specified action, further refine the generated HOI labels. Experiments on two benchmark datasets show that FreeA achieves state-of-the-art performance among weakly supervised HOI models. Our approach is +8.58 mean Average Precision (mAP) on HICO-DET and +1.23 mAP on V-COCO more accurate in localizing an",
    "path": "papers/24/03/2403.01840.json",
    "total_tokens": 879,
    "translated_title": "使用自由注释标签进行人-物互动检测的FreeA方法",
    "translated_abstract": "最近的人-物互动（HOI）检测方法依赖于劳动力成本高昂，并需要全面注释的图像数据集。本文提出了一种新颖的自适应语言驱动的HOI检测方法FreeA，这种方法利用了CLIP的适应性来生成潜在的HOI标签，无需标记。具体而言，FreeA将人-物对的图像特征与HOI文本模板进行匹配，并开发了基于先验知识的掩模方法来抑制不太可能的交互作用。此外，FreeA利用了提出的交互相关性匹配方法来增强与指定动作相关的动作的可能性，进一步完善生成的HOI标签。在两个基准数据集上的实验证明，FreeA在弱监督HOI模型中实现了最先进的性能。我们的方法在HICO-DET上的平均精度（mAP）提高了+8.58，在V-COCO上提高了+1.23。",
    "tldr": "提出了一种新颖的自适应语言驱动的HOI检测方法FreeA，无需标记，利用了CLIP来生成潜在的HOI标签，并在两个基准数据集上取得了最先进的性能。",
    "en_tdlr": "Proposed a novel self-adaption language-driven HOI detection method FreeA, which does not require labeling and leverages CLIP to generate latent HOI labels, achieving state-of-the-art performance on two benchmark datasets."
}