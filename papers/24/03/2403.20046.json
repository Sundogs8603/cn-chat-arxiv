{
    "title": "Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to Boost for Reasoning",
    "abstract": "arXiv:2403.20046v1 Announce Type: new  Abstract: Recent works have shown the benefits to LLMs from fine-tuning golden-standard Chain-of-Thought (CoT) rationales or using them as correct examples in few-shot prompting. While humans can indeed imitate correct examples, learning from our mistakes is another vital aspect of human cognition. Hence, a question naturally arises: \\textit{can LLMs learn and benefit from their mistakes, especially for their reasoning? } This study investigates this problem from both the prompting and model-tuning perspectives. We begin by introducing \\textsc{CoTErrorSet}, a new benchmark with 609,432 questions, each designed with both correct and error references, and demonstrating the types and reasons for making such mistakes. To explore the effectiveness of those mistakes, we design two methods: (1) \\textbf{Self-rethinking} prompting guides LLMs to rethink whether they have made similar previous mistakes; and (2) \\textbf{Mistake tuning} involves finetuning mo",
    "link": "https://arxiv.org/abs/2403.20046",
    "context": "Title: Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to Boost for Reasoning\nAbstract: arXiv:2403.20046v1 Announce Type: new  Abstract: Recent works have shown the benefits to LLMs from fine-tuning golden-standard Chain-of-Thought (CoT) rationales or using them as correct examples in few-shot prompting. While humans can indeed imitate correct examples, learning from our mistakes is another vital aspect of human cognition. Hence, a question naturally arises: \\textit{can LLMs learn and benefit from their mistakes, especially for their reasoning? } This study investigates this problem from both the prompting and model-tuning perspectives. We begin by introducing \\textsc{CoTErrorSet}, a new benchmark with 609,432 questions, each designed with both correct and error references, and demonstrating the types and reasons for making such mistakes. To explore the effectiveness of those mistakes, we design two methods: (1) \\textbf{Self-rethinking} prompting guides LLMs to rethink whether they have made similar previous mistakes; and (2) \\textbf{Mistake tuning} involves finetuning mo",
    "path": "papers/24/03/2403.20046.json",
    "total_tokens": 902,
    "translated_title": "LLM能从以前的错误中学习吗？调查LLMs'错误以增强推理能力",
    "translated_abstract": "最近的研究表明，LLMs从微调黄金标准的思维链（CoT）解释或将其用作少量提示中的正确示例中受益。尽管人类确实可以模仿正确的例子，但从我们的错误中学习是人类认知的另一个至关重要的方面。因此，一个问题自然而然地出现：LLMs能否学习并受益于他们的错误，尤其是对于他们的推理？本研究从提示和模型调整的角度研究了这个问题。我们首先介绍了一个新的基准\\textsc{CoTErrorSet}，其中包含609,432个问题，每个问题都设计有正确和错误的参考文献，并展示了制造这些错误的类型和原因。为了探讨这些错误的有效性，我们设计了两种方法：（1）\\textbf{自我反思}提示指导LLMs重新考虑他们是否曾经犯过类似的错误；和（2）\\textbf{错误调整}包括对模型进行微调",
    "tldr": "通过新设计的基准测试\\textsc{CoTErrorSet}，研究了LLMs是否能够从以往的错误中学习，尤其是对于推理能力方面的提升。",
    "en_tdlr": "This study investigates whether LLMs can learn from their past mistakes, especially for boosting reasoning abilities, through a new benchmark named \\textsc{CoTErrorSet}."
}