{
    "title": "Reaching Consensus in Cooperative Multi-Agent Reinforcement Learning with Goal Imagination",
    "abstract": "arXiv:2403.03172v1 Announce Type: new  Abstract: Reaching consensus is key to multi-agent coordination. To accomplish a cooperative task, agents need to coherently select optimal joint actions to maximize the team reward. However, current cooperative multi-agent reinforcement learning (MARL) methods usually do not explicitly take consensus into consideration, which may cause miscoordination problem. In this paper, we propose a model-based consensus mechanism to explicitly coordinate multiple agents. The proposed Multi-agent Goal Imagination (MAGI) framework guides agents to reach consensus with an Imagined common goal. The common goal is an achievable state with high value, which is obtained by sampling from the distribution of future states. We directly model this distribution with a self-supervised generative model, thus alleviating the \"curse of dimensinality\" problem induced by multi-agent multi-step policy rollout commonly used in model-based methods. We show that such efficient c",
    "link": "https://arxiv.org/abs/2403.03172",
    "context": "Title: Reaching Consensus in Cooperative Multi-Agent Reinforcement Learning with Goal Imagination\nAbstract: arXiv:2403.03172v1 Announce Type: new  Abstract: Reaching consensus is key to multi-agent coordination. To accomplish a cooperative task, agents need to coherently select optimal joint actions to maximize the team reward. However, current cooperative multi-agent reinforcement learning (MARL) methods usually do not explicitly take consensus into consideration, which may cause miscoordination problem. In this paper, we propose a model-based consensus mechanism to explicitly coordinate multiple agents. The proposed Multi-agent Goal Imagination (MAGI) framework guides agents to reach consensus with an Imagined common goal. The common goal is an achievable state with high value, which is obtained by sampling from the distribution of future states. We directly model this distribution with a self-supervised generative model, thus alleviating the \"curse of dimensinality\" problem induced by multi-agent multi-step policy rollout commonly used in model-based methods. We show that such efficient c",
    "path": "papers/24/03/2403.03172.json",
    "total_tokens": 880,
    "translated_title": "使用目标想象在合作多智能体强化学习中实现共识",
    "translated_abstract": "达成一致意见对于多智能体协调至关重要。为了完成协作任务，智能体需要协调地选择最佳的联合动作，以最大化团队奖励。然而，当前的合作多智能体强化学习方法通常不明确考虑一致性，这可能导致协调问题。在本文中，我们提出了一种基于模型的共识机制，以明确协调多个智能体。提出的多智能体目标想象（MAGI）框架引导智能体通过想象出的共同目标达成一致。共同目标是一个具有高价值的可实现状态，通过从未来状态分布中采样获得。我们直接使用自我监督生成模型对此分布进行建模，从而缓解了模型方法中常用的多智能体多步骤策略展开引起的“维度灾难”问题。我们展示了这种高效的共识机制可以提高合作多智能体强化学习的性能。",
    "tldr": "提出了一种基于模型的共识机制，使用多智能体目标想象框架引导智能体达成共识，从而提高合作多智能体强化学习的性能。",
    "en_tdlr": "Introduced a model-based consensus mechanism using the Multi-agent Goal Imagination (MAGI) framework to guide agents to reach consensus and enhance the performance of cooperative multi-agent reinforcement learning."
}