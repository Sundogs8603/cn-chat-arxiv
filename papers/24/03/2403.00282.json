{
    "title": "Scale-Invariant Gradient Aggregation for Constrained Multi-Objective Reinforcement Learning",
    "abstract": "arXiv:2403.00282v1 Announce Type: new  Abstract: Multi-objective reinforcement learning (MORL) aims to find a set of Pareto optimal policies to cover various preferences. However, to apply MORL in real-world applications, it is important to find policies that are not only Pareto optimal but also satisfy pre-defined constraints for safety. To this end, we propose a constrained MORL (CMORL) algorithm called Constrained Multi-Objective Gradient Aggregator (CoMOGA). Recognizing the difficulty of handling multiple objectives and constraints concurrently, CoMOGA relaxes the original CMORL problem into a constrained optimization problem by transforming the objectives into additional constraints. This novel transformation process ensures that the converted constraints are invariant to the objective scales while having the same effect as the original objectives. We show that the proposed method converges to a local Pareto optimal policy while satisfying the predefined constraints. Empirical eva",
    "link": "https://arxiv.org/abs/2403.00282",
    "context": "Title: Scale-Invariant Gradient Aggregation for Constrained Multi-Objective Reinforcement Learning\nAbstract: arXiv:2403.00282v1 Announce Type: new  Abstract: Multi-objective reinforcement learning (MORL) aims to find a set of Pareto optimal policies to cover various preferences. However, to apply MORL in real-world applications, it is important to find policies that are not only Pareto optimal but also satisfy pre-defined constraints for safety. To this end, we propose a constrained MORL (CMORL) algorithm called Constrained Multi-Objective Gradient Aggregator (CoMOGA). Recognizing the difficulty of handling multiple objectives and constraints concurrently, CoMOGA relaxes the original CMORL problem into a constrained optimization problem by transforming the objectives into additional constraints. This novel transformation process ensures that the converted constraints are invariant to the objective scales while having the same effect as the original objectives. We show that the proposed method converges to a local Pareto optimal policy while satisfying the predefined constraints. Empirical eva",
    "path": "papers/24/03/2403.00282.json",
    "total_tokens": 844,
    "translated_title": "尺度不变梯度聚合用于受约束多目标强化学习",
    "translated_abstract": "多目标强化学习(MORL)的目标是找到一组帕累托最优策略，以涵盖各种偏好。然而，在实际应用中应用MORL，找到的策略不仅要帕累托最优，还要满足预定义的安全约束。为此，我们提出了一种名为约束多目标梯度聚合器(Constrained Multi-Objective Gradient Aggregator, CoMOGA)的约束MORL(CMORL)算法。CoMOGA意识到同时处理多个目标和约束的困难，通过将目标转换为额外的约束，将原始CMORL问题放松成一个约束优化问题。这种新颖的转换过程确保转换后的约束对目标尺度不变，同时具有与原始目标相同的效果。我们展示了所提方法收敛到一个局部帕累托最优策略，同时满足预定义约束。",
    "tldr": "提出了一种名为CoMOGA的约束多目标梯度聚合算法，通过将目标转换为约束，实现了对帕累托最优策略的求解，同时满足预定义约束。",
    "en_tdlr": "Proposed a constrained multi-objective gradient aggregation algorithm called CoMOGA, which converts objectives into constraints to solve for Pareto optimal policies while satisfying predefined constraints."
}