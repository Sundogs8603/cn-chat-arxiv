{
    "title": "Improving Cross-lingual Representation for Semantic Retrieval with Code-switching",
    "abstract": "arXiv:2403.01364v1 Announce Type: new  Abstract: Semantic Retrieval (SR) has become an indispensable part of the FAQ system in the task-oriented question-answering (QA) dialogue scenario. The demands for a cross-lingual smart-customer-service system for an e-commerce platform or some particular business conditions have been increasing recently. Most previous studies exploit cross-lingual pre-trained models (PTMs) for multi-lingual knowledge retrieval directly, while some others also leverage the continual pre-training before fine-tuning PTMs on the downstream tasks. However, no matter which schema is used, the previous work ignores to inform PTMs of some features of the downstream task, i.e. train their PTMs without providing any signals related to SR. To this end, in this work, we propose an Alternative Cross-lingual PTM for SR via code-switching. We are the first to utilize the code-switching approach for cross-lingual SR. Besides, we introduce the novel code-switched continual pre-t",
    "link": "https://arxiv.org/abs/2403.01364",
    "context": "Title: Improving Cross-lingual Representation for Semantic Retrieval with Code-switching\nAbstract: arXiv:2403.01364v1 Announce Type: new  Abstract: Semantic Retrieval (SR) has become an indispensable part of the FAQ system in the task-oriented question-answering (QA) dialogue scenario. The demands for a cross-lingual smart-customer-service system for an e-commerce platform or some particular business conditions have been increasing recently. Most previous studies exploit cross-lingual pre-trained models (PTMs) for multi-lingual knowledge retrieval directly, while some others also leverage the continual pre-training before fine-tuning PTMs on the downstream tasks. However, no matter which schema is used, the previous work ignores to inform PTMs of some features of the downstream task, i.e. train their PTMs without providing any signals related to SR. To this end, in this work, we propose an Alternative Cross-lingual PTM for SR via code-switching. We are the first to utilize the code-switching approach for cross-lingual SR. Besides, we introduce the novel code-switched continual pre-t",
    "path": "papers/24/03/2403.01364.json",
    "total_tokens": 880,
    "translated_title": "通过代码切换改进语义检索的跨语言表示",
    "translated_abstract": "arXiv:2403.01364v1 公告类型：新 提要：语义检索（SR）已成为任务导向问答（QA）对话场景中FAQ系统中不可或缺的部分。最近，对于电子商务平台或某些特定业务环境的跨语言智能客户服务系统的需求日益增加。大多数先前的研究直接利用跨语言预训练模型（PTMs）用于多语言知识检索，而其他一些研究也利用持续预训练在对下游任务的PTMs进行微调之前。然而，无论使用哪种模式，先前的工作都忽略了向PTMs告知与SR相关的一些特征，即在不提供与SR相关的任何信号的情况下训练他们的PTMs。为此，在这项工作中，我们提出了一种通过代码切换的交替跨语言PTM用于SR。我们是第一个为跨语言SR使用代码切换方法的研究。此外，我们还介绍了新颖的代码切换持续预训练方法。",
    "tldr": "提出了一种通过代码切换的交替跨语言PTM，首次将代码切换方法应用于跨语言语义检索。"
}