{
    "title": "LimGen: Probing the LLMs for Generating Suggestive Limitations of Research Papers",
    "abstract": "arXiv:2403.15529v1 Announce Type: cross  Abstract: Examining limitations is a crucial step in the scholarly research reviewing process, revealing aspects where a study might lack decisiveness or require enhancement. This aids readers in considering broader implications for further research. In this article, we present a novel and challenging task of Suggestive Limitation Generation (SLG) for research papers. We compile a dataset called LimGen, encompassing 4068 research papers and their associated limitations from the ACL anthology. We investigate several approaches to harness large language models (LLMs) for producing suggestive limitations, by thoroughly examining the related challenges, practical insights, and potential opportunities. Our LimGen dataset and code can be accessed at https://github.com/armbf/LimGen.",
    "link": "https://arxiv.org/abs/2403.15529",
    "context": "Title: LimGen: Probing the LLMs for Generating Suggestive Limitations of Research Papers\nAbstract: arXiv:2403.15529v1 Announce Type: cross  Abstract: Examining limitations is a crucial step in the scholarly research reviewing process, revealing aspects where a study might lack decisiveness or require enhancement. This aids readers in considering broader implications for further research. In this article, we present a novel and challenging task of Suggestive Limitation Generation (SLG) for research papers. We compile a dataset called LimGen, encompassing 4068 research papers and their associated limitations from the ACL anthology. We investigate several approaches to harness large language models (LLMs) for producing suggestive limitations, by thoroughly examining the related challenges, practical insights, and potential opportunities. Our LimGen dataset and code can be accessed at https://github.com/armbf/LimGen.",
    "path": "papers/24/03/2403.15529.json",
    "total_tokens": 782,
    "translated_title": "LimGen: 探究用于生成研究论文建议性局限的LLMs",
    "translated_abstract": "检查局限是学术研究评审过程中的关键步骤，揭示了研究可能缺乏决定性或需要加强的方面。这有助于读者考虑进一步研究的更广泛影响。本文提出了研究论文建议性局限生成（SLG）的一项新颖且具有挑战性的任务。我们编制了一个名为LimGen的数据集，包含来自ACL文集的4068篇研究论文及其相关局限。我们调查了多种方法来利用大型语言模型（LLMs）生成建议性局限，通过彻底研究相关挑战、实践见解和潜在机会。我们的LimGen数据集和代码可以在https://github.com/armbf/LimGen 上获取。",
    "tldr": "本文提出了一个新颖而具有挑战性的任务，即为研究论文生成建议性局限，通过调查大型语言模型的多种方法来揭示相关挑战、实践见解和潜在机会。"
}