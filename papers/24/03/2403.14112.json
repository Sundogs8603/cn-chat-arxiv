{
    "title": "Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations",
    "abstract": "arXiv:2403.14112v1 Announce Type: new  Abstract: We introduce CHARM, the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense. We evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5 representative prompt strategies for improving LLMs' reasoning ability, such as Chain-of-Thought. Our findings indicate that the LLM's language orientation and the task's domain influence the effectiveness of the prompt strategy, which enriches previous research findings. We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability, while others show differences in reasoning despite similar memorization performance. We also evaluated the LLMs' memorization-independent reasoning abilities and analyzed the typical errors. Our study pr",
    "link": "https://arxiv.org/abs/2403.14112",
    "context": "Title: Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations\nAbstract: arXiv:2403.14112v1 Announce Type: new  Abstract: We introduce CHARM, the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense. We evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5 representative prompt strategies for improving LLMs' reasoning ability, such as Chain-of-Thought. Our findings indicate that the LLM's language orientation and the task's domain influence the effectiveness of the prompt strategy, which enriches previous research findings. We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability, while others show differences in reasoning despite similar memorization performance. We also evaluated the LLMs' memorization-independent reasoning abilities and analyzed the typical errors. Our study pr",
    "path": "papers/24/03/2403.14112.json",
    "total_tokens": 959,
    "translated_title": "评估大型语言模型中文常识推理能力：从中文特定到推理-记忆关联",
    "translated_abstract": "我们介绍了CHARM，这是第一个用于全面深入评估大型语言模型（LLMs）中文常识推理能力的基准，涵盖了全球已知和中文特有的常识。在CHARM上评估了7个英文和12个中文定向LLMs，采用了5种代表性提示策略来提高LLMs的推理能力，比如思维链。我们的研究结果表明，LLM的语言导向性和任务领域影响了提示策略的有效性，这丰富了以往的研究结果。我们构建了紧密关联的推理和记忆任务，并发现一些LLMs在记忆中文常识方面存在困难，影响了它们的推理能力，而其他一些LLMs在推理上表现存在差异，尽管记忆表现相似。我们还评估了LLMs的与记忆无关的推理能力，并分析了典型错误。",
    "tldr": "CHARM是第一个用于全面深入评估大型语言模型中文常识推理能力的基准，研究发现LLM的语言导向性和任务领域会影响提示策略的有效性，并指出一些LLMs在记忆中文常识方面存在困难，而其他一些LLMs在推理上表现存在差异。",
    "en_tdlr": "CHARM is the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, the study found that the language orientation and task domain of LLMs affect the effectiveness of prompt strategies, and some LLMs struggle with memorizing Chinese commonsense while others show differences in reasoning despite similar memorization performance."
}