{
    "title": "ProgGen: Generating Named Entity Recognition Datasets Step-by-step with Self-Reflexive Large Language Models",
    "abstract": "arXiv:2403.11103v1 Announce Type: new  Abstract: Although Large Language Models (LLMs) exhibit remarkable adaptability across domains, these models often fall short in structured knowledge extraction tasks such as named entity recognition (NER). This paper explores an innovative, cost-efficient strategy to harness LLMs with modest NER capabilities for producing superior NER datasets. Our approach diverges from the basic class-conditional prompts by instructing LLMs to self-reflect on the specific domain, thereby generating domain-relevant attributes (such as category and emotions for movie reviews), which are utilized for creating attribute-rich training data. Furthermore, we preemptively generate entity terms and then develop NER context data around these entities, effectively bypassing the LLMs' challenges with complex structures. Our experiments across both general and niche domains reveal significant performance enhancements over conventional data generation methods while being mor",
    "link": "https://arxiv.org/abs/2403.11103",
    "context": "Title: ProgGen: Generating Named Entity Recognition Datasets Step-by-step with Self-Reflexive Large Language Models\nAbstract: arXiv:2403.11103v1 Announce Type: new  Abstract: Although Large Language Models (LLMs) exhibit remarkable adaptability across domains, these models often fall short in structured knowledge extraction tasks such as named entity recognition (NER). This paper explores an innovative, cost-efficient strategy to harness LLMs with modest NER capabilities for producing superior NER datasets. Our approach diverges from the basic class-conditional prompts by instructing LLMs to self-reflect on the specific domain, thereby generating domain-relevant attributes (such as category and emotions for movie reviews), which are utilized for creating attribute-rich training data. Furthermore, we preemptively generate entity terms and then develop NER context data around these entities, effectively bypassing the LLMs' challenges with complex structures. Our experiments across both general and niche domains reveal significant performance enhancements over conventional data generation methods while being mor",
    "path": "papers/24/03/2403.11103.json",
    "total_tokens": 866,
    "translated_title": "ProgGen:通过自反大语言模型逐步生成命名实体识别数据集",
    "translated_abstract": "虽然大语言模型（LLMs）在跨领域上表现出卓越的适应性，但这些模型在结构化知识提取任务（如命名实体识别NER）方面经常表现不佳。本文探讨了一种创新的、具有成本效益的策略，利用具有适度NER能力的LLMs来生成优秀的NER数据集。我们的方法不同于基本的类条件提示，而是指导LLMs对特定领域进行自我反思，从而生成具有领域相关属性（例如影评的类别和情感）的属性丰富的训练数据。此外，我们预先生成实体术语，然后围绕这些实体开发NER上下文数据，有效规避了LLMs对复杂结构的挑战。我们在通用和专业领域展开的实验显示，相对于传统数据生成方法，性能得到了显著提升，同时成本更低。",
    "tldr": "通过让大语言模型自省特定领域，生成领域相关属性并创建属性丰富的训练数据，同时绕过复杂结构的挑战，实现了生成命名实体识别数据集的创新策略。",
    "en_tdlr": "An innovative strategy of utilizing self-reflection of large language models in domain-specific contexts to generate attribute-rich training data for named entity recognition, bypassing challenges with complex structures, leading to significant performance enhancements over conventional methods."
}