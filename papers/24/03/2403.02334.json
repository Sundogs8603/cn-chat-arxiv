{
    "title": "Gradient Correlation Subspace Learning against Catastrophic Forgetting",
    "abstract": "arXiv:2403.02334v1 Announce Type: cross  Abstract: Efficient continual learning techniques have been a topic of significant research over the last few years. A fundamental problem with such learning is severe degradation of performance on previously learned tasks, known also as catastrophic forgetting. This paper introduces a novel method to reduce catastrophic forgetting in the context of incremental class learning called Gradient Correlation Subspace Learning (GCSL). The method detects a subspace of the weights that is least affected by previous tasks and projects the weights to train for the new task into said subspace. The method can be applied to one or more layers of a given network architectures and the size of the subspace used can be altered from layer to layer and task to task. Code will be available at \\href{https://github.com/vgthengane/GCSL}{https://github.com/vgthengane/GCSL}",
    "link": "https://arxiv.org/abs/2403.02334",
    "context": "Title: Gradient Correlation Subspace Learning against Catastrophic Forgetting\nAbstract: arXiv:2403.02334v1 Announce Type: cross  Abstract: Efficient continual learning techniques have been a topic of significant research over the last few years. A fundamental problem with such learning is severe degradation of performance on previously learned tasks, known also as catastrophic forgetting. This paper introduces a novel method to reduce catastrophic forgetting in the context of incremental class learning called Gradient Correlation Subspace Learning (GCSL). The method detects a subspace of the weights that is least affected by previous tasks and projects the weights to train for the new task into said subspace. The method can be applied to one or more layers of a given network architectures and the size of the subspace used can be altered from layer to layer and task to task. Code will be available at \\href{https://github.com/vgthengane/GCSL}{https://github.com/vgthengane/GCSL}",
    "path": "papers/24/03/2403.02334.json",
    "total_tokens": 763,
    "translated_title": "梯度相关子空间学习抵抗灾难性遗忘",
    "translated_abstract": "在过去几年中，高效的持续学习技术一直是一个重要的研究课题。这样的学习面临的一个基本问题是之前学习的任务性能严重下降，也称为灾难性遗忘。本文介绍了一种新颖的方法，在增量类学习的情况下减少灾难性遗忘，名为梯度相关子空间学习（GCSL）。该方法检测到最不受以前任务影响的权重子空间，并将权重投影到该子空间中进行新任务的训练。该方法可以应用于给定网络架构的一个或多个层，并且所使用的子空间大小可以从层到层、任务到任务进行改变。",
    "tldr": "GCSL是一种用于减少灾难性遗忘的新颖方法，通过检测和利用不受以前任务影响的权重子空间来训练新任务。",
    "en_tdlr": "GCSL is a novel method for reducing catastrophic forgetting by detecting and utilizing weight subspaces least affected by previous tasks for training new tasks."
}