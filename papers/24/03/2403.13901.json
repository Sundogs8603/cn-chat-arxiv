{
    "title": "Train & Constrain: Phonologically Informed Tongue-Twister Generation from Topics and Paraphrases",
    "abstract": "arXiv:2403.13901v1 Announce Type: new  Abstract: Previous work in phonologically and phonetically grounded language generation has mainly focused on domains such as puns and poetry. In this article, we present new work on the generation of tongue-twisters - a form of language that is required to be conditioned on a phoneme level to maximize sound overlap, whilst maintaining semantic consistency with an input topic and still being grammatically correct. We present TwisterLister, a pipeline for generating phonologically informed tongue-twisters from Large Language Models (LLMs) that we use to generate TwistList 2.0, the largest annotated dataset of tongue-twisters to date, consisting of 17K+ examples from a combination of human and LLM authors. Our generation pipeline involves the use of a phonologically constrained vocabulary alongside LLM prompting to generate novel, non-derivative tongue-twister examples. We additionally present the results of automatic and human evaluation of smaller",
    "link": "https://arxiv.org/abs/2403.13901",
    "context": "Title: Train & Constrain: Phonologically Informed Tongue-Twister Generation from Topics and Paraphrases\nAbstract: arXiv:2403.13901v1 Announce Type: new  Abstract: Previous work in phonologically and phonetically grounded language generation has mainly focused on domains such as puns and poetry. In this article, we present new work on the generation of tongue-twisters - a form of language that is required to be conditioned on a phoneme level to maximize sound overlap, whilst maintaining semantic consistency with an input topic and still being grammatically correct. We present TwisterLister, a pipeline for generating phonologically informed tongue-twisters from Large Language Models (LLMs) that we use to generate TwistList 2.0, the largest annotated dataset of tongue-twisters to date, consisting of 17K+ examples from a combination of human and LLM authors. Our generation pipeline involves the use of a phonologically constrained vocabulary alongside LLM prompting to generate novel, non-derivative tongue-twister examples. We additionally present the results of automatic and human evaluation of smaller",
    "path": "papers/24/03/2403.13901.json",
    "total_tokens": 877,
    "translated_title": "训练与限制：从主题和释义生成基于音韵学的绕口令",
    "translated_abstract": "过去在音韵和语音基础的语言生成方面的工作主要集中在领域，如双关语和诗歌。在本文中，我们提出了产生绕口令的新工作-这种语言形式需要在音素级别上进行条件约束，以最大程度地实现声音重叠，同时与输入主题保持语义一致，仍然保持语法正确。我们提出了TwisterLister，这是一个从大型语言模型（LLMs）中生成基于音韵学的绕口令的流程，我们用它来生成TwistList 2.0，到目前为止最大的一个已标记数据集，包含来自人类和LLM作者合作的超过17K个例子。我们的生成流程涉及使用音韵受限词汇以及LLM提示来生成新颖的、非衍生的绕口令实例。此外，我们还提出了对较小规模的自动和人工评估结果。",
    "tldr": "本文提出了一种从主题和释义生成基于音韵学的绕口令的新方法，生成了迄今为止最大的绕口令数据集TwistList 2.0，并进行了自动和人工评估。",
    "en_tdlr": "This article presents a new approach to generating phonologically informed tongue-twisters from topics and paraphrases, creating the largest tongue-twister dataset to date, TwistList 2.0, and evaluating it through both automatic and human assessment."
}