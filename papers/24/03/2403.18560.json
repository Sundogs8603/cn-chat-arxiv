{
    "title": "Noise-Robust Keyword Spotting through Self-supervised Pretraining",
    "abstract": "arXiv:2403.18560v1 Announce Type: cross  Abstract: Voice assistants are now widely available, and to activate them a keyword spotting (KWS) algorithm is used. Modern KWS systems are mainly trained using supervised learning methods and require a large amount of labelled data to achieve a good performance. Leveraging unlabelled data through self-supervised learning (SSL) has been shown to increase the accuracy in clean conditions. This paper explores how SSL pretraining such as Data2Vec can be used to enhance the robustness of KWS models in noisy conditions, which is under-explored.   Models of three different sizes are pretrained using different pretraining approaches and then fine-tuned for KWS. These models are then tested and compared to models trained using two baseline supervised learning methods, one being standard training using clean data and the other one being multi-style training (MTR). The results show that pretraining and fine-tuning on clean data is superior to supervised ",
    "link": "https://arxiv.org/abs/2403.18560",
    "context": "Title: Noise-Robust Keyword Spotting through Self-supervised Pretraining\nAbstract: arXiv:2403.18560v1 Announce Type: cross  Abstract: Voice assistants are now widely available, and to activate them a keyword spotting (KWS) algorithm is used. Modern KWS systems are mainly trained using supervised learning methods and require a large amount of labelled data to achieve a good performance. Leveraging unlabelled data through self-supervised learning (SSL) has been shown to increase the accuracy in clean conditions. This paper explores how SSL pretraining such as Data2Vec can be used to enhance the robustness of KWS models in noisy conditions, which is under-explored.   Models of three different sizes are pretrained using different pretraining approaches and then fine-tuned for KWS. These models are then tested and compared to models trained using two baseline supervised learning methods, one being standard training using clean data and the other one being multi-style training (MTR). The results show that pretraining and fine-tuning on clean data is superior to supervised ",
    "path": "papers/24/03/2403.18560.json",
    "total_tokens": 915,
    "translated_title": "通过自监督预训练实现抗噪声关键词检测",
    "translated_abstract": "语音助手现在已经广泛可用，为了启动它们，使用关键词检测（KWS）算法。现代KWS系统主要使用监督学习方法进行训练，并需要大量标注数据才能达到良好性能。利用自监督学习（SSL）通过未标记数据已经被证明可以提高在干净条件下的准确性。本文探讨了如何利用Data2Vec等SSL预训练方法来增强KWS模型在嘈杂条件下的稳健性，这方面研究尚未充分探讨。对三种不同规模的模型使用不同的预训练方法进行预训练，然后进行KWS的微调。这些模型然后被测试并与使用两种基准监督学习方法训练的模型进行比较，其中一种是使用干净数据进行标准训练，另一种是多样式训练（MTR）。结果表明，在干净数据上进行预训练和微调优于监督学习方法。",
    "tldr": "本研究探索了如何利用自监督学习预训练来增强关键词检测模型在嘈杂条件下的稳健性，发现在干净数据上进行预训练和微调优于监督学习方法。",
    "en_tdlr": "This study explores how to enhance the robustness of keyword spotting models in noisy conditions through self-supervised pretraining, and finds that pretraining and fine-tuning on clean data is superior to supervised learning methods."
}