{
    "title": "RewardBench: Evaluating Reward Models for Language Modeling",
    "abstract": "arXiv:2403.13787v1 Announce Type: new  Abstract: Reward models (RMs) are at the crux of successful RLHF to align pretrained models to human preferences, yet there has been relatively little study that focuses on evaluation of those reward models. Evaluating reward models presents an opportunity to understand the opaque technologies used for alignment of language models and which values are embedded in them. To date, very few descriptors of capabilities, training methods, or open-source reward models exist. In this paper, we present RewardBench, a benchmark dataset and code-base for evaluation, to enhance scientific understanding of reward models. The RewardBench dataset is a collection of prompt-win-lose trios spanning chat, reasoning, and safety, to benchmark how reward models perform on challenging, structured and out-of-distribution queries. We created specific comparison datasets for RMs that have subtle, but verifiable reasons (e.g. bugs, incorrect facts) why one answer should be ",
    "link": "https://arxiv.org/abs/2403.13787",
    "context": "Title: RewardBench: Evaluating Reward Models for Language Modeling\nAbstract: arXiv:2403.13787v1 Announce Type: new  Abstract: Reward models (RMs) are at the crux of successful RLHF to align pretrained models to human preferences, yet there has been relatively little study that focuses on evaluation of those reward models. Evaluating reward models presents an opportunity to understand the opaque technologies used for alignment of language models and which values are embedded in them. To date, very few descriptors of capabilities, training methods, or open-source reward models exist. In this paper, we present RewardBench, a benchmark dataset and code-base for evaluation, to enhance scientific understanding of reward models. The RewardBench dataset is a collection of prompt-win-lose trios spanning chat, reasoning, and safety, to benchmark how reward models perform on challenging, structured and out-of-distribution queries. We created specific comparison datasets for RMs that have subtle, but verifiable reasons (e.g. bugs, incorrect facts) why one answer should be ",
    "path": "papers/24/03/2403.13787.json",
    "total_tokens": 854,
    "translated_title": "RewardBench：评估用于语言建模的奖励模型",
    "translated_abstract": "奖励模型（RMs）是成功RLHF的关键，用于对预训练模型与人类偏好进行对齐，然而相对较少的研究关注对这些奖励模型的评估。评估奖励模型提供了一个了解用于对齐语言模型的不透明技术及其嵌入什么价值的机会。到目前为止，几乎没有关于能力描述、训练方法或开源奖励模型的描述。本文提出了RewardBench，一个用于评估的基准数据集和代码库，以增强对奖励模型的科学理解。RewardBench数据集是一个跨对话、推理和安全性的提示-赢-输三元组集合，用于评估奖励模型在具有挑战性、结构化和超分布查询上的性能。我们为RMs创建了特定的比较数据集，其中有微妙但可验证的原因（例如错误、不正确的事实），解释为什么一个答案应该",
    "tldr": "本论文提出了RewardBench, 一个用于评估奖励模型的基准数据集和代码库，旨在增强对奖励模型的科学理解。",
    "en_tdlr": "This paper introduces RewardBench, a benchmark dataset and code-base for evaluating reward models, aiming to enhance scientific understanding of reward models."
}