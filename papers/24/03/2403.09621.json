{
    "title": "Minimax Optimal and Computationally Efficient Algorithms for Distributionally Robust Offline Reinforcement Learning",
    "abstract": "arXiv:2403.09621v1 Announce Type: cross  Abstract: Distributionally robust offline reinforcement learning (RL), which seeks robust policy training against environment perturbation by modeling dynamics uncertainty, calls for function approximations when facing large state-action spaces. However, the consideration of dynamics uncertainty introduces essential nonlinearity and computational burden, posing unique challenges for analyzing and practically employing function approximation. Focusing on a basic setting where the nominal model and perturbed models are linearly parameterized, we propose minimax optimal and computationally efficient algorithms realizing function approximation and initiate the study on instance-dependent suboptimality analysis in the context of robust offline RL. Our results uncover that function approximation in robust offline RL is essentially distinct from and probably harder than that in standard offline RL. Our algorithms and theoretical results crucially depen",
    "link": "https://arxiv.org/abs/2403.09621",
    "context": "Title: Minimax Optimal and Computationally Efficient Algorithms for Distributionally Robust Offline Reinforcement Learning\nAbstract: arXiv:2403.09621v1 Announce Type: cross  Abstract: Distributionally robust offline reinforcement learning (RL), which seeks robust policy training against environment perturbation by modeling dynamics uncertainty, calls for function approximations when facing large state-action spaces. However, the consideration of dynamics uncertainty introduces essential nonlinearity and computational burden, posing unique challenges for analyzing and practically employing function approximation. Focusing on a basic setting where the nominal model and perturbed models are linearly parameterized, we propose minimax optimal and computationally efficient algorithms realizing function approximation and initiate the study on instance-dependent suboptimality analysis in the context of robust offline RL. Our results uncover that function approximation in robust offline RL is essentially distinct from and probably harder than that in standard offline RL. Our algorithms and theoretical results crucially depen",
    "path": "papers/24/03/2403.09621.json",
    "total_tokens": 829,
    "translated_title": "最小化最优和计算高效的分布鲁棒离线强化学习算法",
    "translated_abstract": "分布鲁棒离线强化学习（RL）寻求针对环境扰动的鲁棒策略训练，通过建模动态不确定性来调用函数逼近，当面对庞大的状态-动作空间时，这种RL需要考虑到动态不确定性，引入了基本的非线性和计算负担，这给分析和实际应用函数逼近提出了独特挑战。在基本设置下，提议最小化最优和计算高效的算法，实现函数逼近，并在鲁棒离线RL的背景下启动对实例相关次优性分析的研究。我们的结果揭示了鲁棒离线RL中的函数逼近本质上与标准离线RL中的函数逼近有明显区别，可能更加困难。我们的算法和理论结果至关重要地依赖于",
    "tldr": "研究提出了最小化最优和计算高效的算法，为鲁棒离线强化学习中的函数逼近带来新颖视角，并展示了其与标准离线强化学习中函数逼近的区别。",
    "en_tdlr": "The study introduces minimax optimal and computationally efficient algorithms for novel perspectives on function approximation in distributionally robust offline reinforcement learning, highlighting the differences with standard offline reinforcement learning."
}