{
    "title": "Defending Against Indirect Prompt Injection Attacks With Spotlighting",
    "abstract": "arXiv:2403.14720v1 Announce Type: cross  Abstract: Large Language Models (LLMs), while powerful, are built and trained to process a single text input. In common applications, multiple inputs can be processed by concatenating them together into a single stream of text. However, the LLM is unable to distinguish which sections of prompt belong to various input sources. Indirect prompt injection attacks take advantage of this vulnerability by embedding adversarial instructions into untrusted data being processed alongside user commands. Often, the LLM will mistake the adversarial instructions as user commands to be followed, creating a security vulnerability in the larger system. We introduce spotlighting, a family of prompt engineering techniques that can be used to improve LLMs' ability to distinguish among multiple sources of input. The key insight is to utilize transformations of an input to provide a reliable and continuous signal of its provenance. We evaluate spotlighting as a defen",
    "link": "https://arxiv.org/abs/2403.14720",
    "context": "Title: Defending Against Indirect Prompt Injection Attacks With Spotlighting\nAbstract: arXiv:2403.14720v1 Announce Type: cross  Abstract: Large Language Models (LLMs), while powerful, are built and trained to process a single text input. In common applications, multiple inputs can be processed by concatenating them together into a single stream of text. However, the LLM is unable to distinguish which sections of prompt belong to various input sources. Indirect prompt injection attacks take advantage of this vulnerability by embedding adversarial instructions into untrusted data being processed alongside user commands. Often, the LLM will mistake the adversarial instructions as user commands to be followed, creating a security vulnerability in the larger system. We introduce spotlighting, a family of prompt engineering techniques that can be used to improve LLMs' ability to distinguish among multiple sources of input. The key insight is to utilize transformations of an input to provide a reliable and continuous signal of its provenance. We evaluate spotlighting as a defen",
    "path": "papers/24/03/2403.14720.json",
    "total_tokens": 834,
    "translated_title": "使用聚焦技术抵御间接提示注入攻击",
    "translated_abstract": "大型语言模型(LLMs)虽然强大，却是建立和训练用于处理单个文本输入的。在常见应用中，可以通过将多个输入连接在一起形成单个文本流来进行处理。然而，LLM无法区分提示的哪些部分属于不同的输入源。间接提示注入攻击利用这一漏洞，将对手指令嵌入到与用户命令一起处理的不受信任数据中。通常情况下，LLM会误将对手指令作为要遵循的用户指令，从而在整个系统中创建安全漏洞。我们引入了聚焦技术，这是一种用于改进LLMs区分多个输入源能力的提示工程技术系列。关键的见解是利用输入的变换提供其来源的可靠连续信号。我们将聚焦技术作为一种防御手段进行评估。",
    "tldr": "引入了聚焦技术，一种提示工程技术，用于改进大型语言模型在处理多个输入源时的能力，通过提供可靠的输入来源信号来防御间接提示注入攻击。",
    "en_tdlr": "Introducing spotlighting, a prompt engineering technique, to enhance the capability of Large Language Models in distinguishing among multiple sources of input, defending against indirect prompt injection attacks by providing a reliable signal of input source."
}