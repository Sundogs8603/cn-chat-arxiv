{
    "title": "Multiscale Matching Driven by Cross-Modal Similarity Consistency for Audio-Text Retrieval",
    "abstract": "arXiv:2403.10146v1 Announce Type: cross  Abstract: Audio-text retrieval (ATR), which retrieves a relevant caption given an audio clip (A2T) and vice versa (T2A), has recently attracted much research attention. Existing methods typically aggregate information from each modality into a single vector for matching, but this sacrifices local details and can hardly capture intricate relationships within and between modalities. Furthermore, current ATR datasets lack comprehensive alignment information, and simple binary contrastive learning labels overlook the measurement of fine-grained semantic differences between samples. To counter these challenges, we present a novel ATR framework that comprehensively captures the matching relationships of multimodal information from different perspectives and finer granularities. Specifically, a fine-grained alignment method is introduced, achieving a more detail-oriented matching through a multiscale process from local to global levels to capture metic",
    "link": "https://arxiv.org/abs/2403.10146",
    "context": "Title: Multiscale Matching Driven by Cross-Modal Similarity Consistency for Audio-Text Retrieval\nAbstract: arXiv:2403.10146v1 Announce Type: cross  Abstract: Audio-text retrieval (ATR), which retrieves a relevant caption given an audio clip (A2T) and vice versa (T2A), has recently attracted much research attention. Existing methods typically aggregate information from each modality into a single vector for matching, but this sacrifices local details and can hardly capture intricate relationships within and between modalities. Furthermore, current ATR datasets lack comprehensive alignment information, and simple binary contrastive learning labels overlook the measurement of fine-grained semantic differences between samples. To counter these challenges, we present a novel ATR framework that comprehensively captures the matching relationships of multimodal information from different perspectives and finer granularities. Specifically, a fine-grained alignment method is introduced, achieving a more detail-oriented matching through a multiscale process from local to global levels to capture metic",
    "path": "papers/24/03/2403.10146.json",
    "total_tokens": 842,
    "translated_title": "基于跨模态相似性一致性驱动的多尺度匹配用于音频-文本检索",
    "translated_abstract": "音频-文本检索（ATR）近年来受到了广泛的研究关注，它可以在给定音频剪辑（A2T）时检索相关字幕，反之亦然（T2A）。现有方法通常将每种模态的信息聚合到单个向量中进行匹配，但这样做会牺牲局部细节，并且很难捕获模态内部和之间复杂的关系。此外，当前ATR数据集缺乏全面的对齐信息，并且简单的二元对比学习标签忽略了样本间细粒度语义差异的衡量。为了应对这些挑战，我们提出了一个新颖的ATR框架，该框架全面捕捉了不同视角和更细的粒度中多模态信息的匹配关系。具体而言，引入了一种细粒度对齐方法，通过从局部到全局层次的多尺度过程实现更详细的匹配，以捕获metic",
    "tldr": "提出了一种能够从不同视角和细粒度多尺度匹配多模态信息的新型ATR框架"
}