{
    "title": "DiPrompT: Disentangled Prompt Tuning for Multiple Latent Domain Generalization in Federated Learning",
    "abstract": "arXiv:2403.08506v1 Announce Type: cross  Abstract: Federated learning (FL) has emerged as a powerful paradigm for learning from decentralized data, and federated domain generalization further considers the test dataset (target domain) is absent from the decentralized training data (source domains). However, most existing FL methods assume that domain labels are provided during training, and their evaluation imposes explicit constraints on the number of domains, which must strictly match the number of clients. Because of the underutilization of numerous edge devices and additional cross-client domain annotations in the real world, such restrictions may be impractical and involve potential privacy leaks. In this paper, we propose an efficient and novel approach, called Disentangled Prompt Tuning (DiPrompT), a method that tackles the above restrictions by learning adaptive prompts for domain generalization in a distributed manner. Specifically, we first design two types of prompts, i.e., ",
    "link": "https://arxiv.org/abs/2403.08506",
    "context": "Title: DiPrompT: Disentangled Prompt Tuning for Multiple Latent Domain Generalization in Federated Learning\nAbstract: arXiv:2403.08506v1 Announce Type: cross  Abstract: Federated learning (FL) has emerged as a powerful paradigm for learning from decentralized data, and federated domain generalization further considers the test dataset (target domain) is absent from the decentralized training data (source domains). However, most existing FL methods assume that domain labels are provided during training, and their evaluation imposes explicit constraints on the number of domains, which must strictly match the number of clients. Because of the underutilization of numerous edge devices and additional cross-client domain annotations in the real world, such restrictions may be impractical and involve potential privacy leaks. In this paper, we propose an efficient and novel approach, called Disentangled Prompt Tuning (DiPrompT), a method that tackles the above restrictions by learning adaptive prompts for domain generalization in a distributed manner. Specifically, we first design two types of prompts, i.e., ",
    "path": "papers/24/03/2403.08506.json",
    "total_tokens": 898,
    "translated_title": "DiPrompT: 多潜在领域泛化的解耦提示调整在联邦学习中",
    "translated_abstract": "arXiv:2403.08506v1 公告类型: 跨领域  摘要: 联邦学习（FL）已经成为一种强大的学习范式，可以从分散的数据中学习，而联邦领域泛化进一步考虑测试数据集（目标领域）不存在于分散的训练数据（源领域）中。然而，大多数现有的FL方法假设在训练过程中提供了领域标签，并且它们的评估对领域数量施加明确的约束，这些约束必须严格匹配客户端的数量。由于现实世界中众多边缘设备的被低效利用以及额外的跨客户端领域注释，这些限制可能是不切实际的，并涉及潜在的隐私泄漏。在本文中，我们提出了一种高效而新颖的方法，称为解耦提示调整（DiPrompT），该方法通过分布式学习适应提示来处理上述限制，来实现领域泛化。具体而言，我们首先设计了两种提示类型，即",
    "tldr": "提出了一种名为DiPrompT的解耦提示调整方法，通过学习适应提示来解决在联邦学习中对领域泛化的限制。"
}