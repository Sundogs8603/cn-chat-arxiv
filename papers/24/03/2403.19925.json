{
    "title": "Decision Mamba: Reinforcement Learning via Sequence Modeling with Selective State Spaces",
    "abstract": "arXiv:2403.19925v1 Announce Type: cross  Abstract: Decision Transformer, a promising approach that applies Transformer architectures to reinforcement learning, relies on causal self-attention to model sequences of states, actions, and rewards. While this method has shown competitive results, this paper investigates the integration of the Mamba framework, known for its advanced capabilities in efficient and effective sequence modeling, into the Decision Transformer architecture, focusing on the potential performance enhancements in sequential decision-making tasks. Our study systematically evaluates this integration by conducting a series of experiments across various decision-making environments, comparing the modified Decision Transformer, Decision Mamba, with its traditional counterpart. This work contributes to the advancement of sequential decision-making models, suggesting that the architecture and training methodology of neural networks can significantly impact their performance ",
    "link": "https://arxiv.org/abs/2403.19925",
    "context": "Title: Decision Mamba: Reinforcement Learning via Sequence Modeling with Selective State Spaces\nAbstract: arXiv:2403.19925v1 Announce Type: cross  Abstract: Decision Transformer, a promising approach that applies Transformer architectures to reinforcement learning, relies on causal self-attention to model sequences of states, actions, and rewards. While this method has shown competitive results, this paper investigates the integration of the Mamba framework, known for its advanced capabilities in efficient and effective sequence modeling, into the Decision Transformer architecture, focusing on the potential performance enhancements in sequential decision-making tasks. Our study systematically evaluates this integration by conducting a series of experiments across various decision-making environments, comparing the modified Decision Transformer, Decision Mamba, with its traditional counterpart. This work contributes to the advancement of sequential decision-making models, suggesting that the architecture and training methodology of neural networks can significantly impact their performance ",
    "path": "papers/24/03/2403.19925.json",
    "total_tokens": 788,
    "translated_title": "决策巨蟒：通过选择性状态空间进行序列建模的强化学习",
    "translated_abstract": "Decision Transformer是一种将Transformer架构应用于强化学习的有前途的方法，它依赖因果自注意力来模拟状态、动作和奖励序列。本文研究了Mamba框架的整合，该框架以有效和高效的序列建模能力而闻名，将其整合到Decision Transformer架构中，关注在顺序决策任务中潜在的性能增强。我们通过在各种决策环境中进行一系列实验来系统评估这种整合，将修改后的Decision Transformer，Decision Mamba，与传统对应物进行比较。这项工作促进了顺序决策模型的发展，表明神经网络的架构和训练方法可以显著影响其性能",
    "tldr": "本研究将Mamba框架整合到Decision Transformer架构中，提出了Decision Mamba，通过在不同决策环境中进行一系列实验，表明了神经网络的架构和训练方法对性能的重要影响",
    "en_tdlr": "This study integrates the Mamba framework into the Decision Transformer architecture, proposing Decision Mamba, and demonstrates through a series of experiments in various decision-making environments that the architecture and training methodology of neural networks significantly impact their performance."
}