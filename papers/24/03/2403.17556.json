{
    "title": "m3P: Towards Multimodal Multilingual Translation with Multimodal Prompt",
    "abstract": "arXiv:2403.17556v1 Announce Type: cross  Abstract: Multilingual translation supports multiple translation directions by projecting all languages in a shared space, but the translation quality is undermined by the difference between languages in the text-only modality, especially when the number of languages is large. To bridge this gap, we introduce visual context as the universal language-independent representation to facilitate multilingual translation. In this paper, we propose a framework to leverage the multimodal prompt to guide the Multimodal Multilingual neural Machine Translation (m3P), which aligns the representations of different languages with the same meaning and generates the conditional vision-language memory for translation. We construct a multilingual multimodal instruction dataset (InstrMulti102) to support 102 languages. Our method aims to minimize the representation distance of different languages by regarding the image as a central language. Experimental results sh",
    "link": "https://arxiv.org/abs/2403.17556",
    "context": "Title: m3P: Towards Multimodal Multilingual Translation with Multimodal Prompt\nAbstract: arXiv:2403.17556v1 Announce Type: cross  Abstract: Multilingual translation supports multiple translation directions by projecting all languages in a shared space, but the translation quality is undermined by the difference between languages in the text-only modality, especially when the number of languages is large. To bridge this gap, we introduce visual context as the universal language-independent representation to facilitate multilingual translation. In this paper, we propose a framework to leverage the multimodal prompt to guide the Multimodal Multilingual neural Machine Translation (m3P), which aligns the representations of different languages with the same meaning and generates the conditional vision-language memory for translation. We construct a multilingual multimodal instruction dataset (InstrMulti102) to support 102 languages. Our method aims to minimize the representation distance of different languages by regarding the image as a central language. Experimental results sh",
    "path": "papers/24/03/2403.17556.json",
    "total_tokens": 872,
    "translated_title": "m3P:面向多模态多语言翻译的多语境提示",
    "translated_abstract": "多语言翻译通过将所有语言投影到一个共享空间来支持多个翻译方向，但由于文本模态中语言之间的差异，尤其是当语言数量较大时，翻译质量会受到影响。为了弥补这一差距，我们引入视觉上下文作为通用的语言无关表示，以促进多语言翻译。在本文中，我们提出了一个框架，利用多模态提示来指导Multimodal Multilingual神经机器翻译（m3P），通过将不同语言的表示与相同含义对齐，并生成用于翻译的条件视觉-语言记忆。我们构建了一个支持102种语言的多语言多模态指令数据集（InstrMulti102）。我们的方法旨在通过将图像视为中央语言来最小化不同语言之间的表示距离。实验结果表明，...",
    "tldr": "通过引入视觉上下文作为通用的语言无关表示，该论文提出了一种利用多模态提示来指导多模态多语言神经机器翻译的框架，以实现对不同语言表示的对齐，并生成条件视觉-语言记忆进行翻译。",
    "en_tdlr": "This paper proposes a framework that leverages multimodal prompt to guide Multimodal Multilingual neural Machine Translation (m3P) by introducing visual context as a universal language-independent representation to align representations of different languages and generate conditional vision-language memory for translation."
}