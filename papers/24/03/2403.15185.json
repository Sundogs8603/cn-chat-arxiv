{
    "title": "Investigating the Performance of Language Models for Completing Code in Functional Programming Languages: a Haskell Case Study",
    "abstract": "arXiv:2403.15185v1 Announce Type: new  Abstract: Language model-based code completion models have quickly grown in use, helping thousands of developers write code in many different programming languages. However, research on code completion models typically focuses on imperative languages such as Python and JavaScript, which results in a lack of representation for functional programming languages. Consequently, these models often perform poorly on functional languages such as Haskell. To investigate whether this can be alleviated, we evaluate the performance of two language models for code, CodeGPT and UniXcoder, on the functional programming language Haskell. We fine-tune and evaluate the models on Haskell functions sourced from a publicly accessible Haskell dataset on HuggingFace. Additionally, we manually evaluate the models using our novel translated HumanEval dataset. Our automatic evaluation shows that knowledge of imperative programming languages in the pre-training of LLMs may ",
    "link": "https://arxiv.org/abs/2403.15185",
    "context": "Title: Investigating the Performance of Language Models for Completing Code in Functional Programming Languages: a Haskell Case Study\nAbstract: arXiv:2403.15185v1 Announce Type: new  Abstract: Language model-based code completion models have quickly grown in use, helping thousands of developers write code in many different programming languages. However, research on code completion models typically focuses on imperative languages such as Python and JavaScript, which results in a lack of representation for functional programming languages. Consequently, these models often perform poorly on functional languages such as Haskell. To investigate whether this can be alleviated, we evaluate the performance of two language models for code, CodeGPT and UniXcoder, on the functional programming language Haskell. We fine-tune and evaluate the models on Haskell functions sourced from a publicly accessible Haskell dataset on HuggingFace. Additionally, we manually evaluate the models using our novel translated HumanEval dataset. Our automatic evaluation shows that knowledge of imperative programming languages in the pre-training of LLMs may ",
    "path": "papers/24/03/2403.15185.json",
    "total_tokens": 829,
    "translated_title": "研究语言模型在函数式编程语言中的代码补全性能：以Haskell为例",
    "translated_abstract": "基于语言模型的代码补全模型在许多不同的编程语言中帮助成千上万的开发人员编写代码的使用快速增长。然而，对于代码补全模型的研究通常集中在诸如Python和JavaScript等命令式语言，这导致对函数式编程语言的代表性不足。因此，这些模型在Haskell等函数式语言上通常表现不佳。为了调查是否可以缓解这一问题，我们评估了两种用于代码的语言模型CodeGPT和UniXcoder在函数式编程语言Haskell上的性能。我们在HuggingFace上的一个公开可访问的Haskell数据集上对这些模型进行微调和评估。此外，我们使用我们的新型翻译的HumanEval数据集手动评估了这些模型。我们的自动评估显示，LLMs的预训练中对命令式编程语言的知识可能会",
    "tldr": "语言模型在函数式编程语言中的代码补全性能研究以Haskell为例，发现命令式编程语言知识对提高性能有帮助",
    "en_tdlr": "Investigating the performance of language models for completing code in functional programming languages, using Haskell as a case study, finding that knowledge of imperative programming languages can help improve performance."
}