{
    "title": "Regret Minimization via Saddle Point Optimization",
    "abstract": "arXiv:2403.10379v1 Announce Type: new  Abstract: A long line of works characterizes the sample complexity of regret minimization in sequential decision-making by min-max programs. In the corresponding saddle-point game, the min-player optimizes the sampling distribution against an adversarial max-player that chooses confusing models leading to large regret. The most recent instantiation of this idea is the decision-estimation coefficient (DEC), which was shown to provide nearly tight lower and upper bounds on the worst-case expected regret in structured bandits and reinforcement learning. By re-parametrizing the offset DEC with the confidence radius and solving the corresponding min-max program, we derive an anytime variant of the Estimation-To-Decisions (E2D) algorithm. Importantly, the algorithm optimizes the exploration-exploitation trade-off online instead of via the analysis. Our formulation leads to a practical algorithm for finite model classes and linear feedback models. We fur",
    "link": "https://arxiv.org/abs/2403.10379",
    "context": "Title: Regret Minimization via Saddle Point Optimization\nAbstract: arXiv:2403.10379v1 Announce Type: new  Abstract: A long line of works characterizes the sample complexity of regret minimization in sequential decision-making by min-max programs. In the corresponding saddle-point game, the min-player optimizes the sampling distribution against an adversarial max-player that chooses confusing models leading to large regret. The most recent instantiation of this idea is the decision-estimation coefficient (DEC), which was shown to provide nearly tight lower and upper bounds on the worst-case expected regret in structured bandits and reinforcement learning. By re-parametrizing the offset DEC with the confidence radius and solving the corresponding min-max program, we derive an anytime variant of the Estimation-To-Decisions (E2D) algorithm. Importantly, the algorithm optimizes the exploration-exploitation trade-off online instead of via the analysis. Our formulation leads to a practical algorithm for finite model classes and linear feedback models. We fur",
    "path": "papers/24/03/2403.10379.json",
    "total_tokens": 844,
    "translated_title": "通过鞍点优化实现遗憾最小化",
    "translated_abstract": "一系列作品通过极小-极大程序表征了遗憾最小化在顺序决策中的样本复杂性。在相应的鞍点博弈中，极小玩家针对选择引发大遗憾的混乱模型进行采样分布的优化。这个想法的最新实例是决策-估计系数（DEC），已被证明在结构化赌博机和强化学习中几乎提供了最紧的下界和上界。通过重新参数化带有置信半径的DEC偏移量，并解决相应的极小-极大程序，我们得出了Estimation-To-Decisions（E2D）算法的任何时候变体。重要的是，该算法在线优化探索-利用权衡，而不是通过分析来进行。我们的公式导致了一个适用于有限模型类和线性反馈模型的实用算法。",
    "tldr": "通过重新参数化DEC并求解相应的极小-极大程序，提出了Estimation-To-Decisions（E2D）算法的任何时候变体，成功将在线优化探索-利用权衡转化为实用算法。",
    "en_tdlr": "A variant of the Estimation-To-Decisions (E2D) algorithm is proposed by re-parametrizing the DEC and solving the corresponding min-max program, successfully transforming the online exploration-exploitation trade-off optimization into a practical algorithm."
}