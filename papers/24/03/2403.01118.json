{
    "title": "Adversarial Testing for Visual Grounding via Image-Aware Property Reduction",
    "abstract": "arXiv:2403.01118v1 Announce Type: cross  Abstract: Due to the advantages of fusing information from various modalities, multimodal learning is gaining increasing attention. Being a fundamental task of multimodal learning, Visual Grounding (VG), aims to locate objects in images through natural language expressions. Ensuring the quality of VG models presents significant challenges due to the complex nature of the task. In the black box scenario, existing adversarial testing techniques often fail to fully exploit the potential of both modalities of information. They typically apply perturbations based solely on either the image or text information, disregarding the crucial correlation between the two modalities, which would lead to failures in test oracles or an inability to effectively challenge VG models. To this end, we propose PEELING, a text perturbation approach via image-aware property reduction for adversarial testing of the VG model. The core idea is to reduce the property-relate",
    "link": "https://arxiv.org/abs/2403.01118",
    "context": "Title: Adversarial Testing for Visual Grounding via Image-Aware Property Reduction\nAbstract: arXiv:2403.01118v1 Announce Type: cross  Abstract: Due to the advantages of fusing information from various modalities, multimodal learning is gaining increasing attention. Being a fundamental task of multimodal learning, Visual Grounding (VG), aims to locate objects in images through natural language expressions. Ensuring the quality of VG models presents significant challenges due to the complex nature of the task. In the black box scenario, existing adversarial testing techniques often fail to fully exploit the potential of both modalities of information. They typically apply perturbations based solely on either the image or text information, disregarding the crucial correlation between the two modalities, which would lead to failures in test oracles or an inability to effectively challenge VG models. To this end, we propose PEELING, a text perturbation approach via image-aware property reduction for adversarial testing of the VG model. The core idea is to reduce the property-relate",
    "path": "papers/24/03/2403.01118.json",
    "total_tokens": 780,
    "translated_title": "通过基于图像感知属性缩减的对抗性测试进行视觉定位",
    "translated_abstract": "由于融合多种模态信息的优势，多模态学习正在越来越受到关注。作为多模态学习的基本任务，视觉定位（VG）旨在通过自然语言表达在图像中定位对象。确保VG模型的质量面临着重大挑战，因为该任务具有复杂的特性。在黑盒场景下，现有的对抗性测试技术通常未能充分发挥信息两种模态的潜力。它们通常仅基于图像或文本信息之一应用扰动，忽视了两种模态之间的关键相关性，这将导致测试预言式失败或无法有效挑战VG模型。为此，我们提出了PEELING，这是一种通过基于图像感知属性缩减的文本扰动方法，用于对VG模型进行对抗性测试。",
    "tldr": "提出了一种通过基于图像感知属性缩减的文本扰动方法，用于对抗性测试VG模型",
    "en_tdlr": "Introducing a text perturbation approach via image-aware property reduction for adversarial testing of the VG model."
}