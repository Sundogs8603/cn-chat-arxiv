{
    "title": "Code Comparison Tuning for Code Large Language Models",
    "abstract": "arXiv:2403.19121v1 Announce Type: new  Abstract: We present Code Comparison Tuning (CCT), a simple and effective tuning method for code large language models (Code LLMs) to better handle subtle code errors. Specifically, we integrate the concept of comparison into instruction tuning, both at the token and sequence levels, enabling the model to discern even the slightest deviations in code. To compare the original code with an erroneous version containing manually added code errors, we use token-level preference loss for detailed token-level comparisons. Additionally, we combine code segments to create a new instruction tuning sample for sequence-level comparisons, enhancing the model's bug-fixing capability. Experimental results on the HumanEvalFix benchmark show that CCT surpasses instruction tuning in pass@1 scores by up to 4 points across diverse code LLMs, and extensive analysis demonstrates the effectiveness of our method.",
    "link": "https://arxiv.org/abs/2403.19121",
    "context": "Title: Code Comparison Tuning for Code Large Language Models\nAbstract: arXiv:2403.19121v1 Announce Type: new  Abstract: We present Code Comparison Tuning (CCT), a simple and effective tuning method for code large language models (Code LLMs) to better handle subtle code errors. Specifically, we integrate the concept of comparison into instruction tuning, both at the token and sequence levels, enabling the model to discern even the slightest deviations in code. To compare the original code with an erroneous version containing manually added code errors, we use token-level preference loss for detailed token-level comparisons. Additionally, we combine code segments to create a new instruction tuning sample for sequence-level comparisons, enhancing the model's bug-fixing capability. Experimental results on the HumanEvalFix benchmark show that CCT surpasses instruction tuning in pass@1 scores by up to 4 points across diverse code LLMs, and extensive analysis demonstrates the effectiveness of our method.",
    "path": "papers/24/03/2403.19121.json",
    "total_tokens": 765,
    "translated_title": "代码大型语言模型的代码比较调优",
    "translated_abstract": "我们提出了一种称为代码比较调优（CCT）的简单而有效的调优方法，用于改进代码大型语言模型（Code LLMs）以更好地处理微妙的代码错误。具体而言，我们在指令调优中集成了比较的概念，包括在标记和序列级别上，使模型能够分辨代码中甚至最细微的偏差。为了比较原始代码与包含手动添加的代码错误的错误版本，我们使用标记级别的优先损失进行详细的标记级别比较。此外，我们结合代码段创建新的指令调优样本，用于序列级别比较，增强模型的错误修复能力。在HumanEvalFix基准测试上的实验结果显示，CCT在各种代码LLMs上的pass@1分数比指令调优高出最多4分，并且广泛的分析证明了我们方法的有效性。",
    "tldr": "提出了一种简单有效的代码比较调优方法，用于改进代码大型语言模型的bug-fixing能力。",
    "en_tdlr": "Proposed a simple and effective code comparison tuning method to enhance bug-fixing capability of code large language models."
}