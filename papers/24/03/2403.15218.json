{
    "title": "Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment Anything Model for Crowd-Sourcing Medical Image Annotations",
    "abstract": "arXiv:2403.15218v1 Announce Type: cross  Abstract: Curating annotations for medical image segmentation is a labor-intensive and time-consuming task that requires domain expertise, resulting in \"narrowly\" focused deep learning (DL) models with limited translational utility. Recently, foundation models like the Segment Anything Model (SAM) have revolutionized semantic segmentation with exceptional zero-shot generalizability across various domains, including medical imaging, and hold a lot of promise for streamlining the annotation process. However, SAM has yet to be evaluated in a crowd-sourced setting to curate annotations for training 3D DL segmentation models. In this work, we explore the potential of SAM for crowd-sourcing \"sparse\" annotations from non-experts to generate \"dense\" segmentation masks for training 3D nnU-Net models, a state-of-the-art DL segmentation model. Our results indicate that while SAM-generated annotations exhibit high mean Dice scores compared to ground-truth a",
    "link": "https://arxiv.org/abs/2403.15218",
    "context": "Title: Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment Anything Model for Crowd-Sourcing Medical Image Annotations\nAbstract: arXiv:2403.15218v1 Announce Type: cross  Abstract: Curating annotations for medical image segmentation is a labor-intensive and time-consuming task that requires domain expertise, resulting in \"narrowly\" focused deep learning (DL) models with limited translational utility. Recently, foundation models like the Segment Anything Model (SAM) have revolutionized semantic segmentation with exceptional zero-shot generalizability across various domains, including medical imaging, and hold a lot of promise for streamlining the annotation process. However, SAM has yet to be evaluated in a crowd-sourced setting to curate annotations for training 3D DL segmentation models. In this work, we explore the potential of SAM for crowd-sourcing \"sparse\" annotations from non-experts to generate \"dense\" segmentation masks for training 3D nnU-Net models, a state-of-the-art DL segmentation model. Our results indicate that while SAM-generated annotations exhibit high mean Dice scores compared to ground-truth a",
    "path": "papers/24/03/2403.15218.json",
    "total_tokens": 935,
    "translated_title": "无论何时、何地、谁人：研究用于众包医学图像标注的部分任意模型的可行性",
    "translated_abstract": "医学图像分割注释的策划是一项耗时且劳动密集的任务，需要领域专业知识，导致\"狭窄\"专注的深度学习（DL）模型具有有限的转化效用。最近，像部分任意模型（SAM）这样的基础模型通过出色的零样本泛化能力彻底改变了语义分割，跨各个领域包括医学成像，对于简化注释过程有很大希望。然而，SAM尚未在众包环境中进行评估，以策划注释来训练3D DL分割模型。在这项工作中，我们探索了SAM用于从非专业人员中众包\"稀疏\"注释，产生用于训练3D nnU-Net模型（一种最先进的DL分割模型）的\"密集\"分割掩模的潜力。我们的结果表明，与地面真实度相比，SAM生成的注释展现出高平均Dice分数。",
    "tldr": "该研究探讨了使用部分任意模型（SAM）在众包环境中从非专家处策划医学图像标注的可行性，以生成用于训练3D DL分割模型的\"密集\"分割掩模。",
    "en_tdlr": "This study investigates the feasibility of using the Segment Anything Model (SAM) in a crowd-sourced setting to curate medical image annotations from non-experts, generating \"dense\" segmentation masks for training 3D DL segmentation models."
}