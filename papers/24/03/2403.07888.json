{
    "title": "Cross-modality debiasing: using language to mitigate sub-population shifts in imaging",
    "abstract": "arXiv:2403.07888v1 Announce Type: cross  Abstract: Sub-population shift is a specific type of domain shift that highlights changes in data distribution within specific sub-groups or populations between training and testing. Sub-population shift accounts for a significant source of algorithmic bias and calls for distributional robustness. Recent studies found inherent distributional robustness in multi-modality foundation models, such as the vision-language model CLIP, yet this robustness is vulnerable through parameter fine-tuning. In this paper, we propose leveraging the connection of robustness among different modalities and reshaping the distributional robustness of one modality with another. Specifically, in the context of the distributional robustness of CLIP, we propose to leverage natural language inputs to debias the image feature representations, to improve worst-case performance on sub-populations. Our extensive empirical studies show that image representations debiased by na",
    "link": "https://arxiv.org/abs/2403.07888",
    "context": "Title: Cross-modality debiasing: using language to mitigate sub-population shifts in imaging\nAbstract: arXiv:2403.07888v1 Announce Type: cross  Abstract: Sub-population shift is a specific type of domain shift that highlights changes in data distribution within specific sub-groups or populations between training and testing. Sub-population shift accounts for a significant source of algorithmic bias and calls for distributional robustness. Recent studies found inherent distributional robustness in multi-modality foundation models, such as the vision-language model CLIP, yet this robustness is vulnerable through parameter fine-tuning. In this paper, we propose leveraging the connection of robustness among different modalities and reshaping the distributional robustness of one modality with another. Specifically, in the context of the distributional robustness of CLIP, we propose to leverage natural language inputs to debias the image feature representations, to improve worst-case performance on sub-populations. Our extensive empirical studies show that image representations debiased by na",
    "path": "papers/24/03/2403.07888.json",
    "total_tokens": 829,
    "translated_title": "跨模态去偏见: 使用语言减轻影像中的子群体转变",
    "translated_abstract": "子群体转变是一种特定类型的领域转变，突显了在训练和测试之间特定子群体或人口的数据分布的变化。子群体转变占据了算法偏见的一个重要来源，并需要分布鲁棒性。最近的研究发现，多模态基础模型，如视觉-语言模型CLIP，具有固有的分布鲁棒性，但这种鲁棒性对参数微调是脆弱的。在本文中，我们提出利用不同模态之间的鲁棒性连接，重新塑造一个模态的分布鲁棒性。具体地，在CLIP的分布鲁棒性上下文中，我们提出利用自然语言输入来去偏置图像特征表示，以改善在子群体上的最坏情况表现。我们的广泛实证研究表明，通过自然语言输入进行去偏见处理的图像表示能够在子群体上改善最坏情况的性能。",
    "tldr": "使用自然语言输入去偏置图像特征表示，以改善在子群体上的最坏情况表现。",
    "en_tdlr": "Debiasing image feature representations using natural language inputs to improve worst-case performance on sub-populations."
}