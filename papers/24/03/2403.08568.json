{
    "title": "Consistent Prompting for Rehearsal-Free Continual Learning",
    "abstract": "arXiv:2403.08568v1 Announce Type: cross  Abstract: Continual learning empowers models to adapt autonomously to the ever-changing environment or data streams without forgetting old knowledge. Prompt-based approaches are built on frozen pre-trained models to learn the task-specific prompts and classifiers efficiently. Existing prompt-based methods are inconsistent between training and testing, limiting their effectiveness. Two types of inconsistency are revealed. Test predictions are made from all classifiers while training only focuses on the current task classifier without holistic alignment, leading to Classifier inconsistency. Prompt inconsistency indicates that the prompt selected during testing may not correspond to the one associated with this task during training. In this paper, we propose a novel prompt-based method, Consistent Prompting (CPrompt), for more aligned training and testing. Specifically, all existing classifiers are exposed to prompt training, resulting in classifie",
    "link": "https://arxiv.org/abs/2403.08568",
    "context": "Title: Consistent Prompting for Rehearsal-Free Continual Learning\nAbstract: arXiv:2403.08568v1 Announce Type: cross  Abstract: Continual learning empowers models to adapt autonomously to the ever-changing environment or data streams without forgetting old knowledge. Prompt-based approaches are built on frozen pre-trained models to learn the task-specific prompts and classifiers efficiently. Existing prompt-based methods are inconsistent between training and testing, limiting their effectiveness. Two types of inconsistency are revealed. Test predictions are made from all classifiers while training only focuses on the current task classifier without holistic alignment, leading to Classifier inconsistency. Prompt inconsistency indicates that the prompt selected during testing may not correspond to the one associated with this task during training. In this paper, we propose a novel prompt-based method, Consistent Prompting (CPrompt), for more aligned training and testing. Specifically, all existing classifiers are exposed to prompt training, resulting in classifie",
    "path": "papers/24/03/2403.08568.json",
    "total_tokens": 808,
    "translated_title": "无需复习的一致提示用于持续学习",
    "translated_abstract": "持续学习使模型能够自主适应不断变化的环境或数据流，而不会忘记旧知识。基于提示的方法建立在冻结的预训练模型上，以高效地学习任务特定的提示和分类器。现有的基于提示的方法在训练和测试之间存在不一致，从而限制了它们的有效性。这种不一致性揭示了两种类型。测试预测是从所有分类器中进行的，而训练只关注当前任务分类器而没有进行整体对齐，导致分类器的不一致性。提示的不一致性表示测试期间选择的提示可能与训练期间与该任务关联的提示不对应。在本文中，我们提出了一种新颖的基于提示的方法，一致提示（CPrompt），用于更加对齐的训练和测试。具体来说，所有现有的分类器都接受提示训练，从而形成分类",
    "tldr": "提出了一种新颖的一致提示（CPrompt）方法，通过训练期间所有现有分类器接受提示训练，实现更加对齐的训练和测试。",
    "en_tdlr": "A novel approach of Consistent Prompting (CPrompt) is proposed, where all existing classifiers undergo prompt training during training to achieve more aligned training and testing."
}