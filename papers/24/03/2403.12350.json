{
    "title": "Friendly Sharpness-Aware Minimization",
    "abstract": "arXiv:2403.12350v1 Announce Type: new  Abstract: Sharpness-Aware Minimization (SAM) has been instrumental in improving deep neural network training by minimizing both training loss and loss sharpness. Despite the practical success, the mechanisms behind SAM's generalization enhancements remain elusive, limiting its progress in deep learning optimization. In this work, we investigate SAM's core components for generalization improvement and introduce \"Friendly-SAM\" (F-SAM) to further enhance SAM's generalization. Our investigation reveals the key role of batch-specific stochastic gradient noise within the adversarial perturbation, i.e., the current minibatch gradient, which significantly influences SAM's generalization performance. By decomposing the adversarial perturbation in SAM into full gradient and stochastic gradient noise components, we discover that relying solely on the full gradient component degrades generalization while excluding it leads to improved performance. The possibl",
    "link": "https://arxiv.org/abs/2403.12350",
    "context": "Title: Friendly Sharpness-Aware Minimization\nAbstract: arXiv:2403.12350v1 Announce Type: new  Abstract: Sharpness-Aware Minimization (SAM) has been instrumental in improving deep neural network training by minimizing both training loss and loss sharpness. Despite the practical success, the mechanisms behind SAM's generalization enhancements remain elusive, limiting its progress in deep learning optimization. In this work, we investigate SAM's core components for generalization improvement and introduce \"Friendly-SAM\" (F-SAM) to further enhance SAM's generalization. Our investigation reveals the key role of batch-specific stochastic gradient noise within the adversarial perturbation, i.e., the current minibatch gradient, which significantly influences SAM's generalization performance. By decomposing the adversarial perturbation in SAM into full gradient and stochastic gradient noise components, we discover that relying solely on the full gradient component degrades generalization while excluding it leads to improved performance. The possibl",
    "path": "papers/24/03/2403.12350.json",
    "total_tokens": 915,
    "translated_title": "友好的锐度感知最小化",
    "translated_abstract": "锐度感知最小化（SAM）在改善深度神经网络训练方面发挥了重要作用，通过最小化训练损失和损失的锐度。尽管在实际中取得了成功，但SAM背后的增强泛化的机制仍然不清楚，限制了其在深度学习优化中的进展。在这项工作中，我们研究了SAM的核心组件以提升泛化，引入了“友好SAM”（F-SAM）来进一步增强SAM的泛化。我们的研究揭示了批特定随机梯度噪声在对抗扰动中的关键作用，即当前小批量梯度，这显著影响了SAM的泛化性能。通过将SAM中的对抗性扰动分解为完整梯度和随机梯度噪声组件，我们发现仅依赖完整梯度组件会降低泛化能力，而排除完整梯度组件会导致性能提升。",
    "tldr": "通过研究锐度感知最小化（SAM）的核心组件，引入“友好SAM”（F-SAM）进一步增强泛化性能，发现了批特定随机梯度噪声在对抗性扰动中扮演的关键角色，从而提升SAM的泛化能力。",
    "en_tdlr": "By studying the core components of Sharpness-Aware Minimization (SAM) and introducing \"Friendly-SAM\" (F-SAM) to further enhance generalization, we uncover the crucial role of batch-specific stochastic gradient noise in adversarial perturbation, leading to improved generalization performance of SAM."
}