{
    "title": "The Effectiveness of Local Updates for Decentralized Learning under Data Heterogeneity",
    "abstract": "arXiv:2403.15654v1 Announce Type: new  Abstract: We revisit two fundamental decentralized optimization methods, Decentralized Gradient Tracking (DGT) and Decentralized Gradient Descent (DGD), with multiple local updates. We consider two settings and demonstrate that incorporating $K > 1$ local update steps can reduce communication complexity. Specifically, for $\\mu$-strongly convex and $L$-smooth loss functions, we proved that local DGT achieves communication complexity $\\tilde{\\mathcal{O}} \\Big(\\frac{L}{\\mu K} + \\frac{\\delta}{\\mu (1 - \\rho)} + \\frac{\\rho }{(1 - \\rho)^2} \\cdot \\frac{L+ \\delta}{\\mu}\\Big)$, where $\\rho$ measures the network connectivity and $\\delta$ measures the second-order heterogeneity of the local loss. Our result reveals the tradeoff between communication and computation and shows increasing $K$ can effectively reduce communication costs when the data heterogeneity is low and the network is well-connected. We then consider the over-parameterization regime where the ",
    "link": "https://arxiv.org/abs/2403.15654",
    "context": "Title: The Effectiveness of Local Updates for Decentralized Learning under Data Heterogeneity\nAbstract: arXiv:2403.15654v1 Announce Type: new  Abstract: We revisit two fundamental decentralized optimization methods, Decentralized Gradient Tracking (DGT) and Decentralized Gradient Descent (DGD), with multiple local updates. We consider two settings and demonstrate that incorporating $K > 1$ local update steps can reduce communication complexity. Specifically, for $\\mu$-strongly convex and $L$-smooth loss functions, we proved that local DGT achieves communication complexity $\\tilde{\\mathcal{O}} \\Big(\\frac{L}{\\mu K} + \\frac{\\delta}{\\mu (1 - \\rho)} + \\frac{\\rho }{(1 - \\rho)^2} \\cdot \\frac{L+ \\delta}{\\mu}\\Big)$, where $\\rho$ measures the network connectivity and $\\delta$ measures the second-order heterogeneity of the local loss. Our result reveals the tradeoff between communication and computation and shows increasing $K$ can effectively reduce communication costs when the data heterogeneity is low and the network is well-connected. We then consider the over-parameterization regime where the ",
    "path": "papers/24/03/2403.15654.json",
    "total_tokens": 908,
    "translated_title": "基于数据异质性的本地更新对分散式学习的有效性研究",
    "translated_abstract": "我们重新审视了两种基本的分散式优化方法，即Decentralized Gradient Tracking (DGT) 和 Decentralized Gradient Descent (DGD)，并引入了多个本地更新步骤。我们考虑了两种情境，并且证明了加入 $K > 1$ 个本地更新步骤能够降低通信复杂度。具体而言，对于 $\\mu$-强凸和 $L$-光滑损失函数，我们证明了本地 DGT 方法实现了通信复杂度为 $\\tilde{\\mathcal{O}} \\Big(\\frac{L}{\\mu K} + \\frac{\\delta}{\\mu (1 - \\rho)} + \\frac{\\rho }{(1 - \\rho)^2} \\cdot \\frac{L+ \\delta}{\\mu}\\Big)$，其中 $\\rho$ 衡量网络连通性，$\\delta$ 表示本地损失的二阶异质性。我们的结果揭示了通信和计算之间的权衡，并表明在数据异质性低且网络高度连通时，增加 $K$ 能有效降低通信成本。",
    "tldr": "通过在分散式学习中引入多个本地更新步骤，可以降低通信复杂度，从而在数据异质性低且网络高度连通时有效降低通信成本。",
    "en_tdlr": "Introducing multiple local update steps in decentralized learning can reduce communication complexity, effectively lowering communication costs when data heterogeneity is low and the network is well-connected."
}