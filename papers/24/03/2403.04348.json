{
    "title": "LoCoDL: Communication-Efficient Distributed Learning with Local Training and Compression",
    "abstract": "arXiv:2403.04348v1 Announce Type: cross  Abstract: In Distributed optimization and Learning, and even more in the modern framework of federated learning, communication, which is slow and costly, is critical. We introduce LoCoDL, a communication-efficient algorithm that leverages the two popular and effective techniques of Local training, which reduces the communication frequency, and Compression, in which short bitstreams are sent instead of full-dimensional vectors of floats. LoCoDL works with a large class of unbiased compressors that includes widely-used sparsification and quantization methods. LoCoDL provably benefits from local training and compression and enjoys a doubly-accelerated communication complexity, with respect to the condition number of the functions and the model dimension, in the general heterogenous regime with strongly convex functions. This is confirmed in practice, with LoCoDL outperforming existing algorithms.",
    "link": "https://arxiv.org/abs/2403.04348",
    "context": "Title: LoCoDL: Communication-Efficient Distributed Learning with Local Training and Compression\nAbstract: arXiv:2403.04348v1 Announce Type: cross  Abstract: In Distributed optimization and Learning, and even more in the modern framework of federated learning, communication, which is slow and costly, is critical. We introduce LoCoDL, a communication-efficient algorithm that leverages the two popular and effective techniques of Local training, which reduces the communication frequency, and Compression, in which short bitstreams are sent instead of full-dimensional vectors of floats. LoCoDL works with a large class of unbiased compressors that includes widely-used sparsification and quantization methods. LoCoDL provably benefits from local training and compression and enjoys a doubly-accelerated communication complexity, with respect to the condition number of the functions and the model dimension, in the general heterogenous regime with strongly convex functions. This is confirmed in practice, with LoCoDL outperforming existing algorithms.",
    "path": "papers/24/03/2403.04348.json",
    "total_tokens": 845,
    "translated_title": "LoCoDL: 具有本地训练和压缩的通信高效分布式学习",
    "translated_abstract": "在分布式优化和学习中，甚至在现代联邦学习框架中，由于通信速度慢且成本高，通信至关重要。我们介绍了LoCoDL，这是一种通信高效的算法，它利用了本地训练和压缩这两种流行且有效的技术，本地训练降低了通信频率，压缩则是发送短的比特流而不是完整的浮点数向量。LoCoDL适用于大类别的无偏压缩器，其中包括广泛使用的稀疏化和量化方法。LoCoDL在一般异构条件下具有双倍加速的通信复杂度优势，这取决于函数的条件数和模型维度，特别是在强凸函数的情况下。在实践中得到了验证，LoCoDL胜过了现有的算法。",
    "tldr": "LoCoDL是一种通信高效的分布式学习算法，结合了本地训练和压缩技术，具有双倍加速的通信复杂度优势，特别适用于一般异构条件下的强凸函数。",
    "en_tdlr": "LoCoDL is a communication-efficient distributed learning algorithm that combines local training and compression techniques, with a doubly-accelerated communication complexity advantage, especially suitable for strongly convex functions in the general heterogeneous regime."
}