{
    "title": "Global Convergence Guarantees for Federated Policy Gradient Methods with Adversaries",
    "abstract": "arXiv:2403.09940v1 Announce Type: cross  Abstract: Federated Reinforcement Learning (FRL) allows multiple agents to collaboratively build a decision making policy without sharing raw trajectories. However, if a small fraction of these agents are adversarial, it can lead to catastrophic results. We propose a policy gradient based approach that is robust to adversarial agents which can send arbitrary values to the server. Under this setting, our results form the first global convergence guarantees with general parametrization. These results demonstrate resilience with adversaries, while achieving sample complexity of order $\\tilde{\\mathcal{O}}\\left( \\frac{1}{\\epsilon^2} \\left( \\frac{1}{N-f} + \\frac{f^2}{(N-f)^2}\\right)\\right)$, where $N$ is the total number of agents and $f$ is the number of adversarial agents.",
    "link": "https://arxiv.org/abs/2403.09940",
    "context": "Title: Global Convergence Guarantees for Federated Policy Gradient Methods with Adversaries\nAbstract: arXiv:2403.09940v1 Announce Type: cross  Abstract: Federated Reinforcement Learning (FRL) allows multiple agents to collaboratively build a decision making policy without sharing raw trajectories. However, if a small fraction of these agents are adversarial, it can lead to catastrophic results. We propose a policy gradient based approach that is robust to adversarial agents which can send arbitrary values to the server. Under this setting, our results form the first global convergence guarantees with general parametrization. These results demonstrate resilience with adversaries, while achieving sample complexity of order $\\tilde{\\mathcal{O}}\\left( \\frac{1}{\\epsilon^2} \\left( \\frac{1}{N-f} + \\frac{f^2}{(N-f)^2}\\right)\\right)$, where $N$ is the total number of agents and $f$ is the number of adversarial agents.",
    "path": "papers/24/03/2403.09940.json",
    "total_tokens": 804,
    "translated_title": "具有对手的联邦策略梯度方法的全局收敛保证",
    "translated_abstract": "Federated Reinforcement Learning (FRL)允许多个代理共同构建决策制定策略，而无需共享原始轨迹。然而，如果这些代理中只有少部分是对手，可能会导致灾难性结果。我们提出了一种基于策略梯度的方法，该方法对对手代理具有鲁棒性，可以向服务器发送任意值。在这种设置下，我们的结果形成了具有一般参数化的首个全局收敛保证。这些结果展示了对手的弹性，同时达到了样本复杂度的$\\tilde{\\mathcal{O}}\\left( \\frac{1}{\\epsilon^2} \\left( \\frac{1}{N-f} + \\frac{f^2}{(N-f)^2}\\right)\\right)$，其中$N$是代理的总数，$f$是对手代理的数量。",
    "tldr": "该方法提出了一种基于策略梯度的联邦学习方法，可以在存在对手代理的情况下实现全局收敛保证，并具有对对手的鲁棒性。"
}