{
    "title": "Contrastive Continual Learning with Importance Sampling and Prototype-Instance Relation Distillation",
    "abstract": "arXiv:2403.04599v1 Announce Type: new  Abstract: Recently, because of the high-quality representations of contrastive learning methods, rehearsal-based contrastive continual learning has been proposed to explore how to continually learn transferable representation embeddings to avoid the catastrophic forgetting issue in traditional continual settings. Based on this framework, we propose Contrastive Continual Learning via Importance Sampling (CCLIS) to preserve knowledge by recovering previous data distributions with a new strategy for Replay Buffer Selection (RBS), which minimize estimated variance to save hard negative samples for representation learning with high quality. Furthermore, we present the Prototype-instance Relation Distillation (PRD) loss, a technique designed to maintain the relationship between prototypes and sample representations using a self-distillation process. Experiments on standard continual learning benchmarks reveal that our method notably outperforms existing",
    "link": "https://arxiv.org/abs/2403.04599",
    "context": "Title: Contrastive Continual Learning with Importance Sampling and Prototype-Instance Relation Distillation\nAbstract: arXiv:2403.04599v1 Announce Type: new  Abstract: Recently, because of the high-quality representations of contrastive learning methods, rehearsal-based contrastive continual learning has been proposed to explore how to continually learn transferable representation embeddings to avoid the catastrophic forgetting issue in traditional continual settings. Based on this framework, we propose Contrastive Continual Learning via Importance Sampling (CCLIS) to preserve knowledge by recovering previous data distributions with a new strategy for Replay Buffer Selection (RBS), which minimize estimated variance to save hard negative samples for representation learning with high quality. Furthermore, we present the Prototype-instance Relation Distillation (PRD) loss, a technique designed to maintain the relationship between prototypes and sample representations using a self-distillation process. Experiments on standard continual learning benchmarks reveal that our method notably outperforms existing",
    "path": "papers/24/03/2403.04599.json",
    "total_tokens": 899,
    "translated_title": "具有重要性采样和原型实例关系蒸馏的对比持续学习",
    "translated_abstract": "最近，由于对比学习方法的高质量表示，提出了基于重复训练的对比持续学习，以探索如何持续学习可传递的表示嵌入，避免传统持续设置中的灾难性遗忘问题。基于这一框架，我们提出了通过重要性采样进行对比持续学习（CCLIS），通过一种新的重播缓冲区选择（RBS）策略来恢复先前的数据分布，最小化估计方差以保存高质量的用于表示学习的难负样本。此外，我们提出了原型-实例关系蒸馏（PRD）损失，一种通过自蒸馏过程来维护原型和样本表示之间关系的技术。在标准持续学习基准上的实验表明，我们的方法明显优于现有方法。",
    "tldr": "提出了Contrastive Continual Learning via Importance Sampling (CCLIS)以保留知识，其中通过重播缓冲区选择（RBS）策略恢复先前的数据分布，最小化估计方差以保存高质量的负样本，同时引入了原型-实例关系蒸馏（PRD）损失来维护原型和样本表示之间的关系。",
    "en_tdlr": "Proposed Contrastive Continual Learning via Importance Sampling (CCLIS) to preserve knowledge, which recovers previous data distributions through Replay Buffer Selection (RBS) strategy, minimizing estimated variance to save high-quality negative samples, and introduced Prototype-instance Relation Distillation (PRD) loss to maintain the relationship between prototypes and sample representations."
}