{
    "title": "Investigating the Benefits of Projection Head for Representation Learning",
    "abstract": "arXiv:2403.11391v1 Announce Type: new  Abstract: An effective technique for obtaining high-quality representations is adding a projection head on top of the encoder during training, then discarding it and using the pre-projection representations. Despite its proven practical effectiveness, the reason behind the success of this technique is poorly understood. The pre-projection representations are not directly optimized by the loss function, raising the question: what makes them better? In this work, we provide a rigorous theoretical answer to this question. We start by examining linear models trained with self-supervised contrastive loss. We reveal that the implicit bias of training algorithms leads to layer-wise progressive feature weighting, where features become increasingly unequal as we go deeper into the layers. Consequently, lower layers tend to have more normalized and less specialized representations. We theoretically characterize scenarios where such representations are more ",
    "link": "https://arxiv.org/abs/2403.11391",
    "context": "Title: Investigating the Benefits of Projection Head for Representation Learning\nAbstract: arXiv:2403.11391v1 Announce Type: new  Abstract: An effective technique for obtaining high-quality representations is adding a projection head on top of the encoder during training, then discarding it and using the pre-projection representations. Despite its proven practical effectiveness, the reason behind the success of this technique is poorly understood. The pre-projection representations are not directly optimized by the loss function, raising the question: what makes them better? In this work, we provide a rigorous theoretical answer to this question. We start by examining linear models trained with self-supervised contrastive loss. We reveal that the implicit bias of training algorithms leads to layer-wise progressive feature weighting, where features become increasingly unequal as we go deeper into the layers. Consequently, lower layers tend to have more normalized and less specialized representations. We theoretically characterize scenarios where such representations are more ",
    "path": "papers/24/03/2403.11391.json",
    "total_tokens": 749,
    "translated_title": "探究投影头对表示学习的益处",
    "translated_abstract": "一种获取高质量表示的有效技术是在训练过程中在编码器顶部添加一个投影头，然后丢弃它并使用预投影表示。尽管该技术被证明在实践中有效，但其成功背后的原因尚不明确。在这项工作中，我们对这个问题提供了严谨的理论回答。我们首先研究了使用自监督对比损失训练的线性模型。我们揭示了训练算法的隐式偏差导致了逐层渐进的特征加权，随着层深入，特征变得越来越不均衡。因此，较低层往往具有更多归一化和更少专门化表示。我们从理论上表征了这种表示更为适用的情况。",
    "tldr": "投影头技术能够通过逐层渐进的特征加权和更为归一化的表示，提高表示学习效果。",
    "en_tdlr": "Projection head technique improves representation learning by progressively weighting features layer-wise and achieving more normalized representations."
}