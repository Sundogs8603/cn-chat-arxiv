{
    "title": "The Impact of Syntactic and Semantic Proximity on Machine Translation with Back-Translation",
    "abstract": "arXiv:2403.18031v1 Announce Type: new  Abstract: Unsupervised on-the-fly back-translation, in conjunction with multilingual pretraining, is the dominant method for unsupervised neural machine translation. Theoretically, however, the method should not work in general. We therefore conduct controlled experiments with artificial languages to determine what properties of languages make back-translation an effective training method, covering lexical, syntactic, and semantic properties. We find, contrary to popular belief, that (i) parallel word frequency distributions, (ii) partially shared vocabulary, and (iii) similar syntactic structure across languages are not sufficient to explain the success of back-translation. We show however that even crude semantic signal (similar lexical fields across languages) does improve alignment of two languages through back-translation. We conjecture that rich semantic dependencies, parallel across languages, are at the root of the success of unsupervised ",
    "link": "https://arxiv.org/abs/2403.18031",
    "context": "Title: The Impact of Syntactic and Semantic Proximity on Machine Translation with Back-Translation\nAbstract: arXiv:2403.18031v1 Announce Type: new  Abstract: Unsupervised on-the-fly back-translation, in conjunction with multilingual pretraining, is the dominant method for unsupervised neural machine translation. Theoretically, however, the method should not work in general. We therefore conduct controlled experiments with artificial languages to determine what properties of languages make back-translation an effective training method, covering lexical, syntactic, and semantic properties. We find, contrary to popular belief, that (i) parallel word frequency distributions, (ii) partially shared vocabulary, and (iii) similar syntactic structure across languages are not sufficient to explain the success of back-translation. We show however that even crude semantic signal (similar lexical fields across languages) does improve alignment of two languages through back-translation. We conjecture that rich semantic dependencies, parallel across languages, are at the root of the success of unsupervised ",
    "path": "papers/24/03/2403.18031.json",
    "total_tokens": 897,
    "translated_title": "机器翻译中句法和语义接近对反向翻译的影响",
    "translated_abstract": "arXiv:2403.18031v1公告类型：新摘要：无监督即时反向翻译，结合多语言预训练，是无监督神经机器翻译的主要方法。然而，在理论上，该方法一般不应起作用。因此，我们进行了针对人工语言的受控实验，以确定哪些语言属性使反向翻译成为有效的训练方法，覆盖了词汇、句法和语义属性。我们发现，与普遍认为的相反，（i）平行词频分布，（ii）部分共享词汇和（iii）不同语种之间的类似句法结构并不足以解释反向翻译的成功。然而，我们显示，即使粗糙的语义信号（跨语言相似的词汇领域）也确实通过反向翻译改善了两种语言的对齐。我们猜测，富含语义依赖性并且跨语种平行的语义对成功的无监督基础。",
    "tldr": "机器翻译中，语义接近对反向翻译的影响很重要，推测跨语种平行的语义依赖性是无监督神经机器翻译成功的关键。",
    "en_tdlr": "In machine translation, semantic proximity plays a crucial role in back-translation, suggesting that rich semantic dependencies parallel across languages are key to the success of unsupervised neural machine translation."
}