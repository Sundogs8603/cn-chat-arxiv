{
    "title": "Adaptive Random Feature Regularization on Fine-tuning Deep Neural Networks",
    "abstract": "arXiv:2403.10097v1 Announce Type: cross  Abstract: While fine-tuning is a de facto standard method for training deep neural networks, it still suffers from overfitting when using small target datasets. Previous methods improve fine-tuning performance by maintaining knowledge of the source datasets or introducing regularization terms such as contrastive loss. However, these methods require auxiliary source information (e.g., source labels or datasets) or heavy additional computations. In this paper, we propose a simple method called adaptive random feature regularization (AdaRand). AdaRand helps the feature extractors of training models to adaptively change the distribution of feature vectors for downstream classification tasks without auxiliary source information and with reasonable computation costs. To this end, AdaRand minimizes the gap between feature vectors and random reference vectors that are sampled from class conditional Gaussian distributions. Furthermore, AdaRand dynamicall",
    "link": "https://arxiv.org/abs/2403.10097",
    "context": "Title: Adaptive Random Feature Regularization on Fine-tuning Deep Neural Networks\nAbstract: arXiv:2403.10097v1 Announce Type: cross  Abstract: While fine-tuning is a de facto standard method for training deep neural networks, it still suffers from overfitting when using small target datasets. Previous methods improve fine-tuning performance by maintaining knowledge of the source datasets or introducing regularization terms such as contrastive loss. However, these methods require auxiliary source information (e.g., source labels or datasets) or heavy additional computations. In this paper, we propose a simple method called adaptive random feature regularization (AdaRand). AdaRand helps the feature extractors of training models to adaptively change the distribution of feature vectors for downstream classification tasks without auxiliary source information and with reasonable computation costs. To this end, AdaRand minimizes the gap between feature vectors and random reference vectors that are sampled from class conditional Gaussian distributions. Furthermore, AdaRand dynamicall",
    "path": "papers/24/03/2403.10097.json",
    "total_tokens": 798,
    "translated_title": "在微调深度神经网络上的自适应随机特征正则化",
    "translated_abstract": "虽然微调是训练深度神经网络的一种事实标准方法，但在使用小型目标数据集时仍然存在过拟合问题。先前的方法通过保持对源数据集的知识或引入诸如对比损失之类的正则化项来提高微调性能。然而，这些方法需要辅助源信息（例如，源标签或数据集）或重复附加计算。在本文中，我们提出了一种称为自适应随机特征正则化（AdaRand）的简单方法。AdaRand可以帮助训练模型的特征提取器在没有辅助源信息的情况下，通过适度的计算成本，自适应地改变下游分类任务的特征向量分布。为此，AdaRand通过最小化特征向量和从类条件高斯分布中采样的随机参考向量之间的差距。",
    "tldr": "提出了一种名为AdaRand的简单方法，在微调深度神经网络时可以自适应地改变特征向量分布，从而提高性能而不需要辅助源信息。",
    "en_tdlr": "Introduced AdaRand, a simple method that adaptively changes the distribution of feature vectors during fine-tuning deep neural networks to improve performance without requiring auxiliary source information."
}