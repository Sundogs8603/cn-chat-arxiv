{
    "title": "Exploring LLMs as a Source of Targeted Synthetic Textual Data to Minimize High Confidence Misclassifications",
    "abstract": "arXiv:2403.17860v1 Announce Type: new  Abstract: Natural Language Processing (NLP) models optimized for predictive performance often make high confidence errors and suffer from vulnerability to adversarial and out-of-distribution data. Existing work has mainly focused on mitigation of such errors using either humans or an automated approach. In this study, we explore the usage of large language models (LLMs) for data augmentation as a potential solution to the issue of NLP models making wrong predictions with high confidence during classification tasks. We compare the effectiveness of synthetic data generated by LLMs with that of human data obtained via the same procedure. For mitigation, humans or LLMs provide natural language characterizations of high confidence misclassifications to generate synthetic data, which are then used to extend the training set. We conduct an extensive evaluation of our approach on three classification tasks and demonstrate its effectiveness in reducing the",
    "link": "https://arxiv.org/abs/2403.17860",
    "context": "Title: Exploring LLMs as a Source of Targeted Synthetic Textual Data to Minimize High Confidence Misclassifications\nAbstract: arXiv:2403.17860v1 Announce Type: new  Abstract: Natural Language Processing (NLP) models optimized for predictive performance often make high confidence errors and suffer from vulnerability to adversarial and out-of-distribution data. Existing work has mainly focused on mitigation of such errors using either humans or an automated approach. In this study, we explore the usage of large language models (LLMs) for data augmentation as a potential solution to the issue of NLP models making wrong predictions with high confidence during classification tasks. We compare the effectiveness of synthetic data generated by LLMs with that of human data obtained via the same procedure. For mitigation, humans or LLMs provide natural language characterizations of high confidence misclassifications to generate synthetic data, which are then used to extend the training set. We conduct an extensive evaluation of our approach on three classification tasks and demonstrate its effectiveness in reducing the",
    "path": "papers/24/03/2403.17860.json",
    "total_tokens": 761,
    "translated_title": "探究LLMs作为目标合成文本数据来源，以减少高置信度误分类",
    "translated_abstract": "自然语言处理（NLP）模型经过优化以提高预测性能时，常常存在高置信度错误并容易受到对抗性和超出分布数据的影响。本研究探讨使用大型语言模型（LLMs）进行数据增强，作为解决NLP模型在分类任务中产生高置信度错误预测问题的潜在解决方案。我们比较了由LLMs生成的合成数据与通过相同过程获得的人工数据的有效性。为了减轻错误，人类或LLMs提供高置信度误分类的自然语言描述以生成合成数据，然后用于扩展训练集。我们对我们的方法在三个分类任务上进行了广泛评估，并展示了其在减少方面的有效性。",
    "tldr": "探索使用大型语言模型（LLMs）生成合成数据以减少NLP模型高置信度误分类问题的研究。",
    "en_tdlr": "Exploring the use of large language models (LLMs) to generate synthetic data to reduce high confidence misclassifications in NLP models."
}