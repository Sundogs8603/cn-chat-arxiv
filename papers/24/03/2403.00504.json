{
    "title": "Learning and Leveraging World Models in Visual Representation Learning",
    "abstract": "arXiv:2403.00504v1 Announce Type: cross  Abstract: Joint-Embedding Predictive Architecture (JEPA) has emerged as a promising self-supervised approach that learns by leveraging a world model. While previously limited to predicting missing parts of an input, we explore how to generalize the JEPA prediction task to a broader set of corruptions. We introduce Image World Models, an approach that goes beyond masked image modeling and learns to predict the effect of global photometric transformations in latent space. We study the recipe of learning performant IWMs and show that it relies on three key aspects: conditioning, prediction difficulty, and capacity. Additionally, we show that the predictive world model learned by IWM can be adapted through finetuning to solve diverse tasks; a fine-tuned IWM world model matches or surpasses the performance of previous self-supervised methods. Finally, we show that learning with an IWM allows one to control the abstraction level of the learned represe",
    "link": "https://arxiv.org/abs/2403.00504",
    "context": "Title: Learning and Leveraging World Models in Visual Representation Learning\nAbstract: arXiv:2403.00504v1 Announce Type: cross  Abstract: Joint-Embedding Predictive Architecture (JEPA) has emerged as a promising self-supervised approach that learns by leveraging a world model. While previously limited to predicting missing parts of an input, we explore how to generalize the JEPA prediction task to a broader set of corruptions. We introduce Image World Models, an approach that goes beyond masked image modeling and learns to predict the effect of global photometric transformations in latent space. We study the recipe of learning performant IWMs and show that it relies on three key aspects: conditioning, prediction difficulty, and capacity. Additionally, we show that the predictive world model learned by IWM can be adapted through finetuning to solve diverse tasks; a fine-tuned IWM world model matches or surpasses the performance of previous self-supervised methods. Finally, we show that learning with an IWM allows one to control the abstraction level of the learned represe",
    "path": "papers/24/03/2403.00504.json",
    "total_tokens": 901,
    "translated_title": "学习和利用视觉表示学习中的世界模型",
    "translated_abstract": "通过联合嵌入预测架构（JEPA），我们探讨了如何将JEPA预测任务概括为更广泛的数据损坏形式。我们引入了图像世界模型（IWM），这种方法超越了遮罩图像建模，学会在潜在空间中预测全局光度变换的影响。我们研究了学习性能良好的IWM的关键方面：条件、预测困难度和容量。此外，我们表明通过微调可以将IWM学到的预测世界模型用于解决各种任务；经过微调的IWM世界模型的性能与甚至超过以往的自监督方法。最后，我们表明通过IWM学习可以控制所学习表示的抽象级别。",
    "tldr": "通过联合嵌入预测架构（JEPA）以及引入图像世界模型（IWM），本研究探讨了如何将JEPA预测任务概括为更广泛的数据损坏形式，并研究了学习性能良好的IWM的关键方面。此外，通过微调可以将IWM学到的预测世界模型用于解决各种任务，最终控制所学习表示的抽象级别。",
    "en_tdlr": "This paper explores how to generalize the JEPA prediction task to a broader set of corruptions and studies the key aspects of learning performant IWMs, which can be adapted through finetuning to solve diverse tasks and control the abstraction level of the learned representations."
}