{
    "title": "Improving Sampling Methods for Fine-tuning SentenceBERT in Text Streams",
    "abstract": "arXiv:2403.15455v1 Announce Type: new  Abstract: The proliferation of textual data on the Internet presents a unique opportunity for institutions and companies to monitor public opinion about their services and products. Given the rapid generation of such data, the text stream mining setting, which handles sequentially arriving, potentially infinite text streams, is often more suitable than traditional batch learning. While pre-trained language models are commonly employed for their high-quality text vectorization capabilities in streaming contexts, they face challenges adapting to concept drift - the phenomenon where the data distribution changes over time, adversely affecting model performance. Addressing the issue of concept drift, this study explores the efficacy of seven text sampling methods designed to selectively fine-tune language models, thereby mitigating performance degradation. We precisely assess the impact of these methods on fine-tuning the SBERT model using four differ",
    "link": "https://arxiv.org/abs/2403.15455",
    "context": "Title: Improving Sampling Methods for Fine-tuning SentenceBERT in Text Streams\nAbstract: arXiv:2403.15455v1 Announce Type: new  Abstract: The proliferation of textual data on the Internet presents a unique opportunity for institutions and companies to monitor public opinion about their services and products. Given the rapid generation of such data, the text stream mining setting, which handles sequentially arriving, potentially infinite text streams, is often more suitable than traditional batch learning. While pre-trained language models are commonly employed for their high-quality text vectorization capabilities in streaming contexts, they face challenges adapting to concept drift - the phenomenon where the data distribution changes over time, adversely affecting model performance. Addressing the issue of concept drift, this study explores the efficacy of seven text sampling methods designed to selectively fine-tune language models, thereby mitigating performance degradation. We precisely assess the impact of these methods on fine-tuning the SBERT model using four differ",
    "path": "papers/24/03/2403.15455.json",
    "total_tokens": 824,
    "translated_title": "改进文本流中用于微调SentenceBERT的采样方法",
    "translated_abstract": "互联网上文本数据的激增为机构和公司提供了一个独特的机会，可以监测公众对其服务和产品的意见。考虑到这些数据的快速生成，处理依次到达、潜在无限的文本流的文本流挖掘设置通常比传统的批量学习更合适。虽然预训练语言模型通常因其在流式内容中高质量的文本向量化能力而被广泛采用，但它们在适应概念漂移（数据分布随时间发生变化，从而对模型性能产生负面影响的现象）方面面临挑战。本研究解决了概念漂移问题，探讨了七种文本采样方法对精心微调语言模型的效果，从而减轻性能下降。我们准确评估了这些方法对使用四种不同方式进行微调的SBERT模型的影响。",
    "tldr": "本研究旨在解决概念漂移问题，通过探索七种文本采样方法的有效性，精细调整语言模型，从而减轻性能下降。",
    "en_tdlr": "This study aims to address the issue of concept drift by exploring the efficacy of seven text sampling methods to selectively fine-tune language models, thereby mitigating performance degradation."
}