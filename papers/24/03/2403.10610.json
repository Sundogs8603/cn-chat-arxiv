{
    "title": "Sequential Monte Carlo for Inclusive KL Minimization in Amortized Variational Inference",
    "abstract": "arXiv:2403.10610v1 Announce Type: new  Abstract: For training an encoder network to perform amortized variational inference, the Kullback-Leibler (KL) divergence from the exact posterior to its approximation, known as the inclusive or forward KL, is an increasingly popular choice of variational objective due to the mass-covering property of its minimizer. However, minimizing this objective is challenging. A popular existing approach, Reweighted Wake-Sleep (RWS), suffers from heavily biased gradients and a circular pathology that results in highly concentrated variational distributions. As an alternative, we propose SMC-Wake, a procedure for fitting an amortized variational approximation that uses likelihood-tempered sequential Monte Carlo samplers to estimate the gradient of the inclusive KL divergence. We propose three gradient estimators, all of which are asymptotically unbiased in the number of iterations and two of which are strongly consistent. Our method interleaves stochastic gr",
    "link": "https://arxiv.org/abs/2403.10610",
    "context": "Title: Sequential Monte Carlo for Inclusive KL Minimization in Amortized Variational Inference\nAbstract: arXiv:2403.10610v1 Announce Type: new  Abstract: For training an encoder network to perform amortized variational inference, the Kullback-Leibler (KL) divergence from the exact posterior to its approximation, known as the inclusive or forward KL, is an increasingly popular choice of variational objective due to the mass-covering property of its minimizer. However, minimizing this objective is challenging. A popular existing approach, Reweighted Wake-Sleep (RWS), suffers from heavily biased gradients and a circular pathology that results in highly concentrated variational distributions. As an alternative, we propose SMC-Wake, a procedure for fitting an amortized variational approximation that uses likelihood-tempered sequential Monte Carlo samplers to estimate the gradient of the inclusive KL divergence. We propose three gradient estimators, all of which are asymptotically unbiased in the number of iterations and two of which are strongly consistent. Our method interleaves stochastic gr",
    "path": "papers/24/03/2403.10610.json",
    "total_tokens": 818,
    "translated_title": "顺序蒙特卡洛在摊销变分推理中包容KL最小化的应用",
    "translated_abstract": "用来训练编码器网络执行摊销变分推理，从精确后验分布到其近似分布的KL散度，即包容或正向KL，是因其极小化值的区域覆盖性质而成为变分目标的越来越流行选择。然而，最小化这一目标是具有挑战性的。作为一种替代方法，我们提出了SMC-Wake，该过程用于拟合一种摊销变分近似，其使用调节似然的顺序蒙特卡罗采样器来估计包容KL散度的梯度。我们提出了三种梯度估计器，其中对迭代次数是渐近无偏的两种是强一致的。我们的方法交替使用随机梯度",
    "tldr": "用顺序蒙特卡洛采样器估计inclusive KL散度梯度，提出了三种梯度估计器，解决了现有方法的偏差梯度和高度集中变分分布问题。",
    "en_tdlr": "Estimating the gradient of inclusive KL divergence using sequential Monte Carlo samplers, proposing three gradient estimators, addressing biased gradients and highly concentrated variational distributions in existing methods."
}