{
    "title": "FL-GUARD: A Holistic Framework for Run-Time Detection and Recovery of Negative Federated Learning",
    "abstract": "arXiv:2403.04146v1 Announce Type: cross  Abstract: Federated learning (FL) is a promising approach for learning a model from data distributed on massive clients without exposing data privacy. It works effectively in the ideal federation where clients share homogeneous data distribution and learning behavior. However, FL may fail to function appropriately when the federation is not ideal, amid an unhealthy state called Negative Federated Learning (NFL), in which most clients gain no benefit from participating in FL. Many studies have tried to address NFL. However, their solutions either (1) predetermine to prevent NFL in the entire learning life-cycle or (2) tackle NFL in the aftermath of numerous learning rounds. Thus, they either (1) indiscriminately incur extra costs even if FL can perform well without such costs or (2) waste numerous learning rounds. Additionally, none of the previous work takes into account the clients who may be unwilling/unable to follow the proposed NFL solution",
    "link": "https://arxiv.org/abs/2403.04146",
    "context": "Title: FL-GUARD: A Holistic Framework for Run-Time Detection and Recovery of Negative Federated Learning\nAbstract: arXiv:2403.04146v1 Announce Type: cross  Abstract: Federated learning (FL) is a promising approach for learning a model from data distributed on massive clients without exposing data privacy. It works effectively in the ideal federation where clients share homogeneous data distribution and learning behavior. However, FL may fail to function appropriately when the federation is not ideal, amid an unhealthy state called Negative Federated Learning (NFL), in which most clients gain no benefit from participating in FL. Many studies have tried to address NFL. However, their solutions either (1) predetermine to prevent NFL in the entire learning life-cycle or (2) tackle NFL in the aftermath of numerous learning rounds. Thus, they either (1) indiscriminately incur extra costs even if FL can perform well without such costs or (2) waste numerous learning rounds. Additionally, none of the previous work takes into account the clients who may be unwilling/unable to follow the proposed NFL solution",
    "path": "papers/24/03/2403.04146.json",
    "total_tokens": 882,
    "translated_title": "FL-GUARD: 一个用于负面联邦学习运行时检测和恢复的全面框架",
    "translated_abstract": "联邦学习（FL）是一种从分布在大量客户端上的数据中学习模型且不暴露数据隐私的有前途的方法。在理想的联邦学习中效果显著，其中客户端分享同质的数据分布和学习行为。然而，当联邦学习不理想时，FL可能无法正常运作，导致负面联邦学习（NFL）的不良状态。许多研究尝试解决NFL问题。然而，它们的解决方案要么（1）预先防止整个学习生命周期中的NFL，要么（2）在大量学习轮次之后解决NFL。因此，它们要么（1）在FL没有这些成本的情况下无差别增加额外成本，要么（2）浪费大量学习轮次。另外，先前的工作没有考虑可能不愿意/无法遵循提出的NFL解决方案的客户端。",
    "tldr": "FL-GUARD提出了一个全面框架，用于在负面联邦学习情况下检测和恢复，解决了先前解决方案中的额外成本和浪费学习轮次的问题",
    "en_tdlr": "FL-GUARD proposes a holistic framework for detecting and recovering from Negative Federated Learning, addressing the issues of additional costs and wasted learning rounds in previous solutions."
}