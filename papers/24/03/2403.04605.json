{
    "title": "In-n-Out: Calibrating Graph Neural Networks for Link Prediction",
    "abstract": "arXiv:2403.04605v1 Announce Type: new  Abstract: Deep neural networks are notoriously miscalibrated, i.e., their outputs do not reflect the true probability of the event we aim to predict. While networks for tabular or image data are usually overconfident, recent works have shown that graph neural networks (GNNs) show the opposite behavior for node-level classification. But what happens when we are predicting links? We show that, in this case, GNNs often exhibit a mixed behavior. More specifically, they may be overconfident in negative predictions while being underconfident in positive ones. Based on this observation, we propose IN-N-OUT, the first-ever method to calibrate GNNs for link prediction. IN-N-OUT is based on two simple intuitions: i) attributing true/false labels to an edge while respecting a GNNs prediction should cause but small fluctuations in that edge's embedding; and, conversely, ii) if we label that same edge contradicting our GNN, embeddings should change more substa",
    "link": "https://arxiv.org/abs/2403.04605",
    "context": "Title: In-n-Out: Calibrating Graph Neural Networks for Link Prediction\nAbstract: arXiv:2403.04605v1 Announce Type: new  Abstract: Deep neural networks are notoriously miscalibrated, i.e., their outputs do not reflect the true probability of the event we aim to predict. While networks for tabular or image data are usually overconfident, recent works have shown that graph neural networks (GNNs) show the opposite behavior for node-level classification. But what happens when we are predicting links? We show that, in this case, GNNs often exhibit a mixed behavior. More specifically, they may be overconfident in negative predictions while being underconfident in positive ones. Based on this observation, we propose IN-N-OUT, the first-ever method to calibrate GNNs for link prediction. IN-N-OUT is based on two simple intuitions: i) attributing true/false labels to an edge while respecting a GNNs prediction should cause but small fluctuations in that edge's embedding; and, conversely, ii) if we label that same edge contradicting our GNN, embeddings should change more substa",
    "path": "papers/24/03/2403.04605.json",
    "total_tokens": 831,
    "translated_title": "In-n-Out: 用于链接预测的图神经网络校准",
    "translated_abstract": "深度神经网络通常存在校准偏差，即它们的输出不能反映我们打算预测的事件的真实概率。我们展示了在链接预测中，图神经网络通常表现出混合的行为，即在负预测上可能过于自信，在正预测上可能不够自信。基于这一观察，我们提出了IN-N-OUT，这是第一个用于链接预测的校准图神经网络的方法。IN-N-OUT基于两个简单的直觉：i) 给边标注真/假标签，同时遵循GNN的预测应导致该边嵌入的微小波动；ii) 相反地，如果我们标记相同的边与我们的GNN预测相悖，那么嵌入应该发生更大的变化。",
    "tldr": "IN-N-OUT提出了第一个用于链接预测的图神经网络校准方法，基于对GNN校准偏差的观察，通过简单直觉实现校准",
    "en_tdlr": "IN-N-OUT proposes the first calibration method for link prediction in graph neural networks, based on the observation of calibration bias in GNNs, and achieves calibration through simple intuitions."
}