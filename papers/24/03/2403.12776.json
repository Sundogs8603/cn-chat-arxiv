{
    "title": "Automated Data Curation for Robust Language Model Fine-Tuning",
    "abstract": "arXiv:2403.12776v1 Announce Type: new  Abstract: Large Language Models have become the de facto approach to sequence-to-sequence text generation tasks, but for specialized tasks/domains, a pretrained LLM lacks specific capabilities to produce accurate or well-formatted responses. Supervised fine-tuning specializes a LLM by training it on dataset of example prompts with target responses, but real-world data tends to be noisy. While many fine-tuning algorithms exist, here we consider a \\emph{data-centric AI} perspective on LLM fine-tuning, studying how to \\emph{systematically} curate the training dataset to improve the LLM produced via \\emph{any} fine-tuning algorithm.   We introduce an automated data curation pipeline CLEAR (Confidence-based LLM Evaluation And Rectification) for instruction tuning datasets, that can be used with any LLM and fine-tuning procedure. CLEAR estimates which training data is low-quality and either filters or corrects it. Automatically identifying which data to",
    "link": "https://arxiv.org/abs/2403.12776",
    "context": "Title: Automated Data Curation for Robust Language Model Fine-Tuning\nAbstract: arXiv:2403.12776v1 Announce Type: new  Abstract: Large Language Models have become the de facto approach to sequence-to-sequence text generation tasks, but for specialized tasks/domains, a pretrained LLM lacks specific capabilities to produce accurate or well-formatted responses. Supervised fine-tuning specializes a LLM by training it on dataset of example prompts with target responses, but real-world data tends to be noisy. While many fine-tuning algorithms exist, here we consider a \\emph{data-centric AI} perspective on LLM fine-tuning, studying how to \\emph{systematically} curate the training dataset to improve the LLM produced via \\emph{any} fine-tuning algorithm.   We introduce an automated data curation pipeline CLEAR (Confidence-based LLM Evaluation And Rectification) for instruction tuning datasets, that can be used with any LLM and fine-tuning procedure. CLEAR estimates which training data is low-quality and either filters or corrects it. Automatically identifying which data to",
    "path": "papers/24/03/2403.12776.json",
    "total_tokens": 766,
    "translated_title": "用于强大语言模型微调的自动化数据管理",
    "translated_abstract": "大型语言模型已成为序列到序列文本生成任务的事实标准，但对于专门的任务/领域，预训练的语言模型缺乏产生准确或格式良好响应的特定能力。监督微调通过训练模型在具有目标响应的示例提示数据集上进行专门化微调，但现实世界的数据往往存在噪声。虽然存在许多微调算法，但在这里，我们考虑了一种“以数据为中心的AI”视角下的语言模型微调，研究如何“系统地”筛选训练数据集以改进通过“任何”微调算法产生的语言模型。",
    "tldr": "介绍了一个自动化数据管理流水线CLEAR（基于置信度的LLM评估和纠正）用于指令微调数据集，可与任何LLM和微调程序一起使用。"
}