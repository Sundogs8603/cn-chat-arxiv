{
    "title": "Explainability through uncertainty: Trustworthy decision-making with neural networks",
    "abstract": "arXiv:2403.10168v1 Announce Type: new  Abstract: Uncertainty is a key feature of any machine learning model and is particularly important in neural networks, which tend to be overconfident. This overconfidence is worrying under distribution shifts, where the model performance silently degrades as the data distribution diverges from the training data distribution. Uncertainty estimation offers a solution to overconfident models, communicating when the output should (not) be trusted. Although methods for uncertainty estimation have been developed, they have not been explicitly linked to the field of explainable artificial intelligence (XAI). Furthermore, literature in operations research ignores the actionability component of uncertainty estimation and does not consider distribution shifts. This work proposes a general uncertainty framework, with contributions being threefold: (i) uncertainty estimation in ML models is positioned as an XAI technique, giving local and model-specific expla",
    "link": "https://arxiv.org/abs/2403.10168",
    "context": "Title: Explainability through uncertainty: Trustworthy decision-making with neural networks\nAbstract: arXiv:2403.10168v1 Announce Type: new  Abstract: Uncertainty is a key feature of any machine learning model and is particularly important in neural networks, which tend to be overconfident. This overconfidence is worrying under distribution shifts, where the model performance silently degrades as the data distribution diverges from the training data distribution. Uncertainty estimation offers a solution to overconfident models, communicating when the output should (not) be trusted. Although methods for uncertainty estimation have been developed, they have not been explicitly linked to the field of explainable artificial intelligence (XAI). Furthermore, literature in operations research ignores the actionability component of uncertainty estimation and does not consider distribution shifts. This work proposes a general uncertainty framework, with contributions being threefold: (i) uncertainty estimation in ML models is positioned as an XAI technique, giving local and model-specific expla",
    "path": "papers/24/03/2403.10168.json",
    "total_tokens": 863,
    "translated_title": "通过不确定性实现可解释性：神经网络中可信赖的决策制定",
    "translated_abstract": "不确定性是任何机器学习模型的一个关键特征，尤其在神经网络中尤为重要，因为神经网络往往过于自信。不确定性在数据分布发生变化时尤为令人担忧，当数据分布偏离训练数据分布时，模型性能会悄无声息地下降。不确定性估计提供了解决过于自信模型的方法，指示何时应该（不应该）信任输出结果。虽然关于不确定性估计的方法已经得到发展，但尚未明确定义与可解释人工智能（XAI）领域的关系。此外，运筹学领域的文献忽略了不确定性估计的可操作性组成部分，并且未考虑到数据分布的变化。本文提出了一个通用的不确定性框架，贡献主要体现在三个方面：（i）将机器学习模型中的不确定性估计定位为XAI技术，提供局部的，模型特定的解释",
    "tldr": "本文提出了一个通用的不确定性框架，将机器学习模型中的不确定性估计定位为XAI技术，并提供了解释输出结果时是否应该信任的方法。",
    "en_tdlr": "This paper proposes a general framework for uncertainty estimation in machine learning models positioning it as an XAI technique, providing a method to determine when the output should be trusted."
}