{
    "title": "Poly-View Contrastive Learning",
    "abstract": "arXiv:2403.05490v1 Announce Type: cross  Abstract: Contrastive learning typically matches pairs of related views among a number of unrelated negative views. Views can be generated (e.g. by augmentations) or be observed. We investigate matching when there are more than two related views which we call poly-view tasks, and derive new representation learning objectives using information maximization and sufficient statistics. We show that with unlimited computation, one should maximize the number of related views, and with a fixed compute budget, it is beneficial to decrease the number of unique samples whilst increasing the number of views of those samples. In particular, poly-view contrastive models trained for 128 epochs with batch size 256 outperform SimCLR trained for 1024 epochs at batch size 4096 on ImageNet1k, challenging the belief that contrastive models require large batch sizes and many training epochs.",
    "link": "https://arxiv.org/abs/2403.05490",
    "context": "Title: Poly-View Contrastive Learning\nAbstract: arXiv:2403.05490v1 Announce Type: cross  Abstract: Contrastive learning typically matches pairs of related views among a number of unrelated negative views. Views can be generated (e.g. by augmentations) or be observed. We investigate matching when there are more than two related views which we call poly-view tasks, and derive new representation learning objectives using information maximization and sufficient statistics. We show that with unlimited computation, one should maximize the number of related views, and with a fixed compute budget, it is beneficial to decrease the number of unique samples whilst increasing the number of views of those samples. In particular, poly-view contrastive models trained for 128 epochs with batch size 256 outperform SimCLR trained for 1024 epochs at batch size 4096 on ImageNet1k, challenging the belief that contrastive models require large batch sizes and many training epochs.",
    "path": "papers/24/03/2403.05490.json",
    "total_tokens": 835,
    "translated_title": "多视图对比学习",
    "translated_abstract": "对比学习通常会匹配一组不相关的负视图中相关视图的配对。视图可以是生成的（例如通过增强）或被观察到的。本文研究了当存在多于两个相关视图时的匹配，我们称之为多视图任务，并利用信息最大化和充分统计导出了新的表示学习目标。我们表明，在计算资源无限时，应最大化相关视图的数量；而在固定计算预算的情况下，减少独特样本的数量同时增加这些样本的视图数量是有益的。特别地，以256的批大小训练128轮的多视图对比模型在ImageNet1k上表现优于在批大小为4096且进行1024轮训练的SimCLR模型，挑战了对比模型需要大批大小和多次训练轮数的信念。",
    "tldr": "本研究提出了多视图对比学习方法，通过新的表示学习目标优化匹配多个相关视图，在ImageNet1k数据集上的实验结果显示，相比于SimCLR模型，多视图对比模型在更少的训练轮数和更小的批大小下表现更优。",
    "en_tdlr": "This study proposes poly-view contrastive learning, optimizing the matching of multiple related views with new representation learning objectives. Experimental results on the ImageNet1k dataset demonstrate that the poly-view contrastive models outperform SimCLR models with fewer training epochs and smaller batch sizes."
}