{
    "title": "Breaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models",
    "abstract": "arXiv:2403.04786v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have become a cornerstone in the field of Natural Language Processing (NLP), offering transformative capabilities in understanding and generating human-like text. However, with their rising prominence, the security and vulnerability aspects of these models have garnered significant attention. This paper presents a comprehensive survey of the various forms of attacks targeting LLMs, discussing the nature and mechanisms of these attacks, their potential impacts, and current defense strategies. We delve into topics such as adversarial attacks that aim to manipulate model outputs, data poisoning that affects model training, and privacy concerns related to training data exploitation. The paper also explores the effectiveness of different attack methodologies, the resilience of LLMs against these attacks, and the implications for model integrity and user trust. By examining the latest research, we provide insight",
    "link": "https://arxiv.org/abs/2403.04786",
    "context": "Title: Breaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models\nAbstract: arXiv:2403.04786v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have become a cornerstone in the field of Natural Language Processing (NLP), offering transformative capabilities in understanding and generating human-like text. However, with their rising prominence, the security and vulnerability aspects of these models have garnered significant attention. This paper presents a comprehensive survey of the various forms of attacks targeting LLMs, discussing the nature and mechanisms of these attacks, their potential impacts, and current defense strategies. We delve into topics such as adversarial attacks that aim to manipulate model outputs, data poisoning that affects model training, and privacy concerns related to training data exploitation. The paper also explores the effectiveness of different attack methodologies, the resilience of LLMs against these attacks, and the implications for model integrity and user trust. By examining the latest research, we provide insight",
    "path": "papers/24/03/2403.04786.json",
    "total_tokens": 860,
    "translated_title": "分解防御：大型语言模型攻击的比较调查",
    "translated_abstract": "大型语言模型（LLMs）已经成为自然语言处理（NLP）领域的基石，在理解和生成类似人类文本方面提供了革命性的能力。然而，随着它们日益重要，这些模型的安全性和脆弱性方面已经引起了重要关注。本文对针对LLMs的各种形式攻击进行了全面调查，讨论了这些攻击的性质和机制、它们的潜在影响以及当前的防御策略。我们深入探讨了旨在操纵模型输出的对抗性攻击、影响模型训练的数据中毒以及与训练数据利用相关的隐私问题。本文还探讨了不同攻击方法的有效性、LLMs抵抗这些攻击的韧性以及对模型完整性和用户信任的影响。通过审视最新研究，我们提供了见解。",
    "tldr": "本文通过全面调查各种攻击形式，探讨了大型语言模型受攻击的性质、机制、潜在影响以及当前防御策略，为模型完整性和用户信任提供了重要见解。",
    "en_tdlr": "This paper presents a comprehensive survey of attacks targeting Large Language Models (LLMs), discussing the nature, mechanisms, potential impacts, and current defense strategies, providing important insights into model integrity and user trust."
}