{
    "title": "Noise misleads rotation invariant algorithms on sparse targets",
    "abstract": "arXiv:2403.02697v1 Announce Type: cross  Abstract: It is well known that the class of rotation invariant algorithms are suboptimal even for learning sparse linear problems when the number of examples is below the \"dimension\" of the problem. This class includes any gradient descent trained neural net with a fully-connected input layer (initialized with a rotationally symmetric distribution). The simplest sparse problem is learning a single feature out of $d$ features. In that case the classification error or regression loss grows with $1-k/n$ where $k$ is the number of examples seen. These lower bounds become vacuous when the number of examples $k$ reaches the dimension $d$.   We show that when noise is added to this sparse linear problem, rotation invariant algorithms are still suboptimal after seeing $d$ or more examples. We prove this via a lower bound for the Bayes optimal algorithm on a rotationally symmetrized problem. We then prove much lower upper bounds on the same problem for ",
    "link": "https://arxiv.org/abs/2403.02697",
    "context": "Title: Noise misleads rotation invariant algorithms on sparse targets\nAbstract: arXiv:2403.02697v1 Announce Type: cross  Abstract: It is well known that the class of rotation invariant algorithms are suboptimal even for learning sparse linear problems when the number of examples is below the \"dimension\" of the problem. This class includes any gradient descent trained neural net with a fully-connected input layer (initialized with a rotationally symmetric distribution). The simplest sparse problem is learning a single feature out of $d$ features. In that case the classification error or regression loss grows with $1-k/n$ where $k$ is the number of examples seen. These lower bounds become vacuous when the number of examples $k$ reaches the dimension $d$.   We show that when noise is added to this sparse linear problem, rotation invariant algorithms are still suboptimal after seeing $d$ or more examples. We prove this via a lower bound for the Bayes optimal algorithm on a rotationally symmetrized problem. We then prove much lower upper bounds on the same problem for ",
    "path": "papers/24/03/2403.02697.json",
    "total_tokens": 841,
    "translated_title": "噪声误导稀疏目标上的旋转不变算法",
    "translated_abstract": "众所周知，即使对于学习稀疏线性问题，当样本数低于问题的“维度”时，旋转不变算法也是次优的。这个类别包括任何使用全连接输入层的梯度下降训练的神经网络（初始化为旋转对称分布）。最简单的稀疏问题是学习$d$个特征中的一个特征。在这种情况下，分类误差或回归损失随着$1-k/n$增长，其中$k$是观察到的样本数。当样本数$k$达到维度$d$时，这些下界变得空泛。我们表明，当将噪声添加到这个稀疏线性问题时，即使在观察到$d$个或更多样本后，旋转不变算法仍然是次优的。我们通过针对一个旋转对称化问题的贝叶斯最优算法的一个下界证明了这一点。然后，我们证明了相同问题的更低的上界",
    "tldr": "噪声添加后，稀疏线性问题上的旋转不变算法仍然次优，我们证明了这一点。",
    "en_tdlr": "Rotation invariant algorithms are still suboptimal on sparse linear problems even when noise is added, as we prove after observing d or more examples."
}