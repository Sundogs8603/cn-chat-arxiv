{
    "title": "Revisiting Meta-evaluation for Grammatical Error Correction",
    "abstract": "arXiv:2403.02674v1 Announce Type: new  Abstract: Metrics are the foundation for automatic evaluation in grammatical error correction (GEC), with their evaluation of the metrics (meta-evaluation) relying on their correlation with human judgments. However, conventional meta-evaluations in English GEC encounter several challenges including biases caused by inconsistencies in evaluation granularity, and an outdated setup using classical systems. These problems can lead to misinterpretation of metrics and potentially hinder the applicability of GEC techniques. To address these issues, this paper proposes SEEDA, a new dataset for GEC meta-evaluation. SEEDA consists of corrections with human ratings along two different granularities: edit-based and sentence-based, covering 12 state-of-the-art systems including large language models (LLMs), and two human corrections with different focuses. The results of improved correlations by aligning the granularity in the sentence-level meta-evaluation, s",
    "link": "https://arxiv.org/abs/2403.02674",
    "context": "Title: Revisiting Meta-evaluation for Grammatical Error Correction\nAbstract: arXiv:2403.02674v1 Announce Type: new  Abstract: Metrics are the foundation for automatic evaluation in grammatical error correction (GEC), with their evaluation of the metrics (meta-evaluation) relying on their correlation with human judgments. However, conventional meta-evaluations in English GEC encounter several challenges including biases caused by inconsistencies in evaluation granularity, and an outdated setup using classical systems. These problems can lead to misinterpretation of metrics and potentially hinder the applicability of GEC techniques. To address these issues, this paper proposes SEEDA, a new dataset for GEC meta-evaluation. SEEDA consists of corrections with human ratings along two different granularities: edit-based and sentence-based, covering 12 state-of-the-art systems including large language models (LLMs), and two human corrections with different focuses. The results of improved correlations by aligning the granularity in the sentence-level meta-evaluation, s",
    "path": "papers/24/03/2403.02674.json",
    "total_tokens": 730,
    "translated_title": "重新审视语法错误修正的元评估",
    "translated_abstract": "Metrics在语法错误修正（GEC）中自动评估的基础。其中，元评估依赖于它们与人类判断的相关性。然而，英语GEC中常规的元评估面临一些挑战，包括由于评估粒度不一致而导致的偏见，以及使用传统系统的过时设置。针对这些问题，本文提出了SEEDA，这是一个用于GEC元评估的新数据集，包括校正和两种不同粒度（基于编辑和基于句子）的人类评级。",
    "tldr": "该论文提出了SEEDA，一个用于语法错误修正的新数据集，提供了对12种最先进系统进行元评估的校正，通过在句子级别元评估中对粒度进行对齐，提高了相关性。",
    "en_tdlr": "The paper introduces SEEDA, a new dataset for grammatical error correction meta-evaluation, providing corrections for 12 state-of-the-art systems and improving correlations by aligning granularity in sentence-level meta-evaluation."
}