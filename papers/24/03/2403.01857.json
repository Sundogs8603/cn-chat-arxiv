{
    "title": "Reward Model Learning vs. Direct Policy Optimization: A Comparative Analysis of Learning from Human Preferences",
    "abstract": "arXiv:2403.01857v1 Announce Type: new  Abstract: In this paper, we take a step towards a deeper understanding of learning from human preferences by systematically comparing the paradigm of reinforcement learning from human feedback (RLHF) with the recently proposed paradigm of direct preference optimization (DPO). We focus our attention on the class of loglinear policy parametrization and linear reward functions. In order to compare the two paradigms, we first derive minimax statistical bounds on the suboptimality gap induced by both RLHF and DPO, assuming access to an oracle that exactly solves the optimization problems. We provide a detailed discussion on the relative comparison between the two paradigms, simultaneously taking into account the sample size, policy and reward class dimensions, and the regularization temperature. Moreover, we extend our analysis to the approximate optimization setting and derive exponentially decaying convergence rates for both RLHF and DPO. Next, we an",
    "link": "https://arxiv.org/abs/2403.01857",
    "context": "Title: Reward Model Learning vs. Direct Policy Optimization: A Comparative Analysis of Learning from Human Preferences\nAbstract: arXiv:2403.01857v1 Announce Type: new  Abstract: In this paper, we take a step towards a deeper understanding of learning from human preferences by systematically comparing the paradigm of reinforcement learning from human feedback (RLHF) with the recently proposed paradigm of direct preference optimization (DPO). We focus our attention on the class of loglinear policy parametrization and linear reward functions. In order to compare the two paradigms, we first derive minimax statistical bounds on the suboptimality gap induced by both RLHF and DPO, assuming access to an oracle that exactly solves the optimization problems. We provide a detailed discussion on the relative comparison between the two paradigms, simultaneously taking into account the sample size, policy and reward class dimensions, and the regularization temperature. Moreover, we extend our analysis to the approximate optimization setting and derive exponentially decaying convergence rates for both RLHF and DPO. Next, we an",
    "path": "papers/24/03/2403.01857.json",
    "total_tokens": 878,
    "translated_title": "奖励模型学习与直接策略优化：从人类偏好学习的比较分析",
    "translated_abstract": "本文通过系统比较从人类偏好学习的强化学习（RLHF）范式与最近提出的直接偏好优化（DPO）范式，迈向对学习人类偏好的更深入理解。我们以对数线性策略参数化和线性奖励函数的类为重点。为了比较这两种范式，我们首先对由RLHF和DPO引起的次优差距推导出极小-最大统计界限，假设可以访问确切解决优化问题的预言。我们就相对比较两种范式进行了详细讨论，同时考虑样本大小、策略和奖励类维数以及正则化温度。此外，我们将分析扩展到近似优化设置，并为RLHF和DPO分别推导出指数衰减的收敛速度。",
    "tldr": "本文通过比较强化学习从人类反馈学习（RLHF）范式与最近提出的直接偏好优化（DPO）范式，对学习人类偏好进行了深入探讨，推导出了次优差距的统计界限，并提供了收敛速度的分析。"
}