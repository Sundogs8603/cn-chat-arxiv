{
    "title": "Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval: Evolving Coding Benchmarks via LLM",
    "abstract": "arXiv:2403.19114v1 Announce Type: cross  Abstract: LLMs have become the go-to choice for code generation tasks, with an exponential increase in the training, development, and usage of LLMs specifically for code generation. To evaluate the ability of LLMs on code, both academic and industry practitioners rely on popular handcrafted benchmarks. However, prior benchmarks contain only a very limited set of problems, both in quantity and variety. Further, due to popularity and age, many benchmarks are prone to data leakage where example solutions can be readily found on the web and thus potentially in training data. Such limitations inevitably lead us to inquire: Is the leaderboard performance on existing benchmarks reliable and comprehensive enough to measure the program synthesis ability of LLMs? To address this, we introduce EvoEval -- a program synthesis benchmark suite created by evolving existing benchmarks into different targeted domains for a comprehensive evaluation of LLM coding a",
    "link": "https://arxiv.org/abs/2403.19114",
    "context": "Title: Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval: Evolving Coding Benchmarks via LLM\nAbstract: arXiv:2403.19114v1 Announce Type: cross  Abstract: LLMs have become the go-to choice for code generation tasks, with an exponential increase in the training, development, and usage of LLMs specifically for code generation. To evaluate the ability of LLMs on code, both academic and industry practitioners rely on popular handcrafted benchmarks. However, prior benchmarks contain only a very limited set of problems, both in quantity and variety. Further, due to popularity and age, many benchmarks are prone to data leakage where example solutions can be readily found on the web and thus potentially in training data. Such limitations inevitably lead us to inquire: Is the leaderboard performance on existing benchmarks reliable and comprehensive enough to measure the program synthesis ability of LLMs? To address this, we introduce EvoEval -- a program synthesis benchmark suite created by evolving existing benchmarks into different targeted domains for a comprehensive evaluation of LLM coding a",
    "path": "papers/24/03/2403.19114.json",
    "total_tokens": 862,
    "translated_title": "顶级排行榜 = 顶级编程能力，永远吗？EvoEval: 通过LLM演化编码基准",
    "translated_abstract": "LLM已成为生成代码任务的首选，LLM的训练、开发和使用随着专门用于生成代码的LLM的指数增长而增加。为了评估LLM在编码上的能力，学术界和行业从业者依赖于流行的人工制定的基准。然而，之前的基准只包含了数量和种类非常有限的问题。此外，由于流行度和年龄，许多基准容易发生数据泄漏，其中示例解决方案可以很容易地在网络上找到，因此可能出现在训练数据中。这些限制不可避免地导致我们要探讨：现有基准的排行榜表现是否可靠且全面足以衡量LLM的程序合成能力？为了解决这个问题，我们引入EvoEval--一个通过将现有基准演化为不同的目标领域而创建的程序合成基准套件，用于全面评估LLM编码。",
    "tldr": "EvoEval通过将现有基准演化为不同的目标领域，创建了一个新的程序合成基准套件，以充分评估LLM编码能力。",
    "en_tdlr": "EvoEval introduces a new program synthesis benchmark suite by evolving existing benchmarks into different targeted domains to comprehensively evaluate the coding ability of LLMs."
}