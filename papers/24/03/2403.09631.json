{
    "title": "3D-VLA: A 3D Vision-Language-Action Generative World Model",
    "abstract": "arXiv:2403.09631v1 Announce Type: cross  Abstract: Recent vision-language-action (VLA) models rely on 2D inputs, lacking integration with the broader realm of the 3D physical world. Furthermore, they perform action prediction by learning a direct mapping from perception to action, neglecting the vast dynamics of the world and the relations between actions and dynamics. In contrast, human beings are endowed with world models that depict imagination about future scenarios to plan actions accordingly. To this end, we propose 3D-VLA by introducing a new family of embodied foundation models that seamlessly link 3D perception, reasoning, and action through a generative world model. Specifically, 3D-VLA is built on top of a 3D-based large language model (LLM), and a set of interaction tokens is introduced to engage with the embodied environment. Furthermore, to inject generation abilities into the model, we train a series of embodied diffusion models and align them into the LLM for predicting",
    "link": "https://arxiv.org/abs/2403.09631",
    "context": "Title: 3D-VLA: A 3D Vision-Language-Action Generative World Model\nAbstract: arXiv:2403.09631v1 Announce Type: cross  Abstract: Recent vision-language-action (VLA) models rely on 2D inputs, lacking integration with the broader realm of the 3D physical world. Furthermore, they perform action prediction by learning a direct mapping from perception to action, neglecting the vast dynamics of the world and the relations between actions and dynamics. In contrast, human beings are endowed with world models that depict imagination about future scenarios to plan actions accordingly. To this end, we propose 3D-VLA by introducing a new family of embodied foundation models that seamlessly link 3D perception, reasoning, and action through a generative world model. Specifically, 3D-VLA is built on top of a 3D-based large language model (LLM), and a set of interaction tokens is introduced to engage with the embodied environment. Furthermore, to inject generation abilities into the model, we train a series of embodied diffusion models and align them into the LLM for predicting",
    "path": "papers/24/03/2403.09631.json",
    "total_tokens": 907,
    "translated_title": "3D-VLA: 一个3D视觉-语言-动作生成世界模型",
    "translated_abstract": "最近的视觉-语言-动作（VLA）模型依赖于2D输入，缺乏与更广阔的3D物理世界融合。此外，它们通过学习从感知到动作的直接映射来执行动作预测，忽略了世界的广泛动态和动作与动态之间的关系。相反，人类拥有描绘关于未来场景的想象，以相应地规划行动的世界模型。为此，我们通过引入一系列新的具身基础模型，无缝地将3D感知、推理和动作通过一个生成世界模型相连，提出了3D-VLA。具体地，3D-VLA建立在基于3D的大型语言模型（LLM）之上，并引入一组交互标记以与具身环境进行交互。此外，为了将生成能力注入模型，我们训练了一系列具身扩散模型，并将它们与LLM对齐以进行预测。",
    "tldr": "提出了3D-VLA，通过将3D感知、推理和动作无缝连接，建立一个生成世界模型，弥补了现有VLA模型只能处理2D输入且忽视世界动态与动作之间关系的不足。",
    "en_tdlr": "Introducing 3D-VLA, a generative world model that seamlessly links 3D perception, reasoning, and action to address the limitations of existing VLA models that can only handle 2D inputs and neglect the relations between world dynamics and actions."
}