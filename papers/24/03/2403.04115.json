{
    "title": "DNAct: Diffusion Guided Multi-Task 3D Policy Learning",
    "abstract": "arXiv:2403.04115v1 Announce Type: cross  Abstract: This paper presents DNAct, a language-conditioned multi-task policy framework that integrates neural rendering pre-training and diffusion training to enforce multi-modality learning in action sequence spaces. To learn a generalizable multi-task policy with few demonstrations, the pre-training phase of DNAct leverages neural rendering to distill 2D semantic features from foundation models such as Stable Diffusion to a 3D space, which provides a comprehensive semantic understanding regarding the scene. Consequently, it allows various applications to challenging robotic tasks requiring rich 3D semantics and accurate geometry. Furthermore, we introduce a novel approach utilizing diffusion training to learn a vision and language feature that encapsulates the inherent multi-modality in the multi-task demonstrations. By reconstructing the action sequences from different tasks via the diffusion process, the model is capable of distinguishing d",
    "link": "https://arxiv.org/abs/2403.04115",
    "context": "Title: DNAct: Diffusion Guided Multi-Task 3D Policy Learning\nAbstract: arXiv:2403.04115v1 Announce Type: cross  Abstract: This paper presents DNAct, a language-conditioned multi-task policy framework that integrates neural rendering pre-training and diffusion training to enforce multi-modality learning in action sequence spaces. To learn a generalizable multi-task policy with few demonstrations, the pre-training phase of DNAct leverages neural rendering to distill 2D semantic features from foundation models such as Stable Diffusion to a 3D space, which provides a comprehensive semantic understanding regarding the scene. Consequently, it allows various applications to challenging robotic tasks requiring rich 3D semantics and accurate geometry. Furthermore, we introduce a novel approach utilizing diffusion training to learn a vision and language feature that encapsulates the inherent multi-modality in the multi-task demonstrations. By reconstructing the action sequences from different tasks via the diffusion process, the model is capable of distinguishing d",
    "path": "papers/24/03/2403.04115.json",
    "total_tokens": 836,
    "translated_title": "DNAct：扩散引导的多任务3D策略学习",
    "translated_abstract": "本文提出了DNAct，这是一个以语言为条件的多任务策略框架，它整合了神经渲染预训练和扩散训练，以在动作序列空间中实现多模态学习。DNAct的预训练阶段利用神经渲染从诸如Stable Diffusion之类的基础模型中提取2D语义特征到3D空间，从而提供了关于场景的全面语义理解。这使得可以应用于需要丰富的3D语义和准确几何的挑战性机器人任务。此外，我们介绍了一种利用扩散训练来学习包含多任务演示中固有多模态的视觉和语言特征的新方法。通过扩散过程从不同任务的动作序列重构，该模型能够区分。",
    "tldr": "本文提出了DNAct框架，结合神经渲染和扩散训练，实现在动作序列空间中的多模态学习，可应用于挑战性机器人任务，同时通过扩散过程实现多任务动作序列的重构。",
    "en_tdlr": "This paper introduces the DNAct framework, which integrates neural rendering and diffusion training to achieve multi-modality learning in action sequence spaces, applicable to challenging robotic tasks, and reconstructs multi-task action sequences through the diffusion process."
}