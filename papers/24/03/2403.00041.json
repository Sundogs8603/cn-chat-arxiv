{
    "title": "Global and Local Prompts Cooperation via Optimal Transport for Federated Learning",
    "abstract": "arXiv:2403.00041v1 Announce Type: cross  Abstract: Prompt learning in pretrained visual-language models has shown remarkable flexibility across various downstream tasks. Leveraging its inherent lightweight nature, recent research attempted to integrate the powerful pretrained models into federated learning frameworks to simultaneously reduce communication costs and promote local training on insufficient data. Despite these efforts, current federated prompt learning methods lack specialized designs to systematically address severe data heterogeneities, e.g., data distribution with both label and feature shifts involved. To address this challenge, we present Federated Prompts Cooperation via Optimal Transport (FedOTP), which introduces efficient collaborative prompt learning strategies to capture diverse category traits on a per-client basis. Specifically, for each client, we learn a global prompt to extract consensus knowledge among clients, and a local prompt to capture client-specific",
    "link": "https://arxiv.org/abs/2403.00041",
    "context": "Title: Global and Local Prompts Cooperation via Optimal Transport for Federated Learning\nAbstract: arXiv:2403.00041v1 Announce Type: cross  Abstract: Prompt learning in pretrained visual-language models has shown remarkable flexibility across various downstream tasks. Leveraging its inherent lightweight nature, recent research attempted to integrate the powerful pretrained models into federated learning frameworks to simultaneously reduce communication costs and promote local training on insufficient data. Despite these efforts, current federated prompt learning methods lack specialized designs to systematically address severe data heterogeneities, e.g., data distribution with both label and feature shifts involved. To address this challenge, we present Federated Prompts Cooperation via Optimal Transport (FedOTP), which introduces efficient collaborative prompt learning strategies to capture diverse category traits on a per-client basis. Specifically, for each client, we learn a global prompt to extract consensus knowledge among clients, and a local prompt to capture client-specific",
    "path": "papers/24/03/2403.00041.json",
    "total_tokens": 829,
    "translated_title": "通过最优输运实现全局和本地提示的合作，用于联邦学习",
    "translated_abstract": "预训练的视觉-语言模型中的提示学习在各种下游任务中表现出了卓越的灵活性。最近的研究尝试将这种强大的预训练模型整合到联邦学习框架中，以同时降低通信成本并促进对数据不足的局部训练。为了应对当前联邦提示学习方法在系统化解决严重的数据异质性方面的不足，即涉及标签和特征转移的数据分布，我们提出了通过最优输运实现联邦提示合作（FedOTP），它引入了高效的协作提示学习策略，以在每个客户端基础上捕捉不同的类别特征。具体而言，对于每个客户端，我们学习一个全局提示来提取客户端之间的共识知识，还学习一个本地提示来捕获特定客户端的特征。",
    "tldr": "提出了联邦提示合作 via Optimal Transport（FedOTP）方法，通过最优输运实现全局和本地提示的合作，针对数据异质性设计了高效的协作提示学习策略。",
    "en_tdlr": "Introduced Federated Prompts Cooperation via Optimal Transport (FedOTP) method, which collaborates global and local prompts through optimal transport, offering efficient prompt learning strategies tailored for data heterogeneities."
}