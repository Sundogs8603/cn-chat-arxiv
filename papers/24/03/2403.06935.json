{
    "title": "Naming, Describing, and Quantifying Visual Objects in Humans and LLMs",
    "abstract": "arXiv:2403.06935v1 Announce Type: new  Abstract: While human speakers use a variety of different expressions when describing the same object in an image, giving rise to a distribution of plausible labels driven by pragmatic constraints, the extent to which current Vision \\& Language Large Language Models (VLLMs) can mimic this crucial feature of language use is an open question. This applies to common, everyday objects, but it is particularly interesting for uncommon or novel objects for which a category label may be lacking or fuzzy. Furthermore, humans show clear production preferences for highly context-sensitive expressions, such as the quantifiers `few' or `most'. In our work, we evaluate VLLMs (FROMAGe, BLIP-2, LLaVA) on three categories (nouns, attributes, and quantifiers) where humans show great subjective variability concerning the distribution over plausible labels, using datasets and resources mostly under-explored in previous work. Our results reveal mixed evidence on the a",
    "link": "https://arxiv.org/abs/2403.06935",
    "context": "Title: Naming, Describing, and Quantifying Visual Objects in Humans and LLMs\nAbstract: arXiv:2403.06935v1 Announce Type: new  Abstract: While human speakers use a variety of different expressions when describing the same object in an image, giving rise to a distribution of plausible labels driven by pragmatic constraints, the extent to which current Vision \\& Language Large Language Models (VLLMs) can mimic this crucial feature of language use is an open question. This applies to common, everyday objects, but it is particularly interesting for uncommon or novel objects for which a category label may be lacking or fuzzy. Furthermore, humans show clear production preferences for highly context-sensitive expressions, such as the quantifiers `few' or `most'. In our work, we evaluate VLLMs (FROMAGe, BLIP-2, LLaVA) on three categories (nouns, attributes, and quantifiers) where humans show great subjective variability concerning the distribution over plausible labels, using datasets and resources mostly under-explored in previous work. Our results reveal mixed evidence on the a",
    "path": "papers/24/03/2403.06935.json",
    "total_tokens": 733,
    "translated_title": "人类和语言大型语言模型中的视觉对象命名、描述和量化",
    "translated_abstract": "人类讲话者在描述图像中的同一对象时使用各种不同的表达方式，这产生了由语用约束驱动的合理标签分布，当前视觉与语言大语言模型（VLLMs）能够模仿语言使用中这一关键特征的程度尚不明确。我们评估了VLLMs（FROMAGe、BLIP-2、LLaVA）在人类在可能标签的分布上显示出极大主观变异性的三个类别（名词、属性和量词）上的性能。",
    "tldr": "评估了当前视觉与语言大语言模型在人类在可能标签的分布上显示出极大主观变异性的情况下，对视觉对象的命名、描述和量化的能力",
    "en_tdlr": "Evaluated the ability of current Vision & Language Large Language Models to name, describe, and quantify visual objects in the presence of high subjective variability in the distribution of plausible labels shown by humans"
}