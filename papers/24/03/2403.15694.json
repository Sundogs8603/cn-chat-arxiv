{
    "title": "Group Benefits Instances Selection for Data Purification",
    "abstract": "arXiv:2403.15694v1 Announce Type: new  Abstract: Manually annotating datasets for training deep models is very labor-intensive and time-consuming. To overcome such inferiority, directly leveraging web images to conduct training data becomes a natural choice. Nevertheless, the presence of label noise in web data usually degrades the model performance. Existing methods for combating label noise are typically designed and tested on synthetic noisy datasets. However, they tend to fail to achieve satisfying results on real-world noisy datasets. To this end, we propose a method named GRIP to alleviate the noisy label problem for both synthetic and real-world datasets. Specifically, GRIP utilizes a group regularization strategy that estimates class soft labels to improve noise robustness. Soft label supervision reduces overfitting on noisy labels and learns inter-class similarities to benefit classification. Furthermore, an instance purification operation globally identifies noisy labels by m",
    "link": "https://arxiv.org/abs/2403.15694",
    "context": "Title: Group Benefits Instances Selection for Data Purification\nAbstract: arXiv:2403.15694v1 Announce Type: new  Abstract: Manually annotating datasets for training deep models is very labor-intensive and time-consuming. To overcome such inferiority, directly leveraging web images to conduct training data becomes a natural choice. Nevertheless, the presence of label noise in web data usually degrades the model performance. Existing methods for combating label noise are typically designed and tested on synthetic noisy datasets. However, they tend to fail to achieve satisfying results on real-world noisy datasets. To this end, we propose a method named GRIP to alleviate the noisy label problem for both synthetic and real-world datasets. Specifically, GRIP utilizes a group regularization strategy that estimates class soft labels to improve noise robustness. Soft label supervision reduces overfitting on noisy labels and learns inter-class similarities to benefit classification. Furthermore, an instance purification operation globally identifies noisy labels by m",
    "path": "papers/24/03/2403.15694.json",
    "total_tokens": 848,
    "translated_title": "数据净化中的群体福利实例选择",
    "translated_abstract": "人工为训练深度模型注释数据集是非常费时费力的。为了克服这种劣势，直接利用网络图像来进行训练数据成为一种自然选择。然而，网络数据中的标签噪声通常会降低模型性能。现有的对抗标签噪声的方法通常是在合成嘈杂数据集上设计和测试的。然而，它们往往无法在真实世界嘈杂数据集上取得令人满意的结果。为此，我们提出了一种名为GRIP的方法，用于缓解合成和真实世界数据集的标签噪声问题。具体来说，GRIP利用一种群体正则化策略，估计类别软标签以提高噪声鲁棒性。软标签监督降低了对嘈杂标签的过拟合，并学习了有利于分类的类间相似性。此外，一个全局实例净化操作通过m",
    "tldr": "提出了一种名为GRIP的方法，通过群体正则化策略估计类别软标签以提高噪声鲁棒性，减少对嘈杂标签的过拟合并学习类间相似性以改善分类结果。",
    "en_tdlr": "Proposed a method named GRIP that estimates class soft labels using a group regularization strategy to improve noise robustness, reduce overfitting on noisy labels, and learn inter-class similarities to benefit classification."
}