{
    "title": "Directional Smoothness and Gradient Methods: Convergence and Adaptivity",
    "abstract": "arXiv:2403.04081v1 Announce Type: new  Abstract: We develop new sub-optimality bounds for gradient descent (GD) that depend on the conditioning of the objective along the path of optimization, rather than on global, worst-case constants. Key to our proofs is directional smoothness, a measure of gradient variation that we use to develop upper-bounds on the objective. Minimizing these upper-bounds requires solving implicit equations to obtain a sequence of strongly adapted step-sizes; we show that these equations are straightforward to solve for convex quadratics and lead to new guarantees for two classical step-sizes. For general functions, we prove that the Polyak step-size and normalized GD obtain fast, path-dependent rates despite using no knowledge of the directional smoothness. Experiments on logistic regression show our convergence guarantees are tighter than the classical theory based on L-smoothness.",
    "link": "https://arxiv.org/abs/2403.04081",
    "context": "Title: Directional Smoothness and Gradient Methods: Convergence and Adaptivity\nAbstract: arXiv:2403.04081v1 Announce Type: new  Abstract: We develop new sub-optimality bounds for gradient descent (GD) that depend on the conditioning of the objective along the path of optimization, rather than on global, worst-case constants. Key to our proofs is directional smoothness, a measure of gradient variation that we use to develop upper-bounds on the objective. Minimizing these upper-bounds requires solving implicit equations to obtain a sequence of strongly adapted step-sizes; we show that these equations are straightforward to solve for convex quadratics and lead to new guarantees for two classical step-sizes. For general functions, we prove that the Polyak step-size and normalized GD obtain fast, path-dependent rates despite using no knowledge of the directional smoothness. Experiments on logistic regression show our convergence guarantees are tighter than the classical theory based on L-smoothness.",
    "path": "papers/24/03/2403.04081.json",
    "total_tokens": 815,
    "translated_title": "方向平滑度与梯度方法：收敛性和自适应性",
    "translated_abstract": "我们为梯度下降（GD）开发了一种新的次优性界限，其取决于沿优化路径的目标条件性，而不是全局最坏情况常数。我们证明的关键是方向平滑度，这是我们用来开发目标上界的梯度变化度量。最小化这些上界需要解决隐式方程以获得一系列强适应的步长；我们展示了对于凸二次函数, 这些方程很容易求解，并为两种经典步长提供了新的保证。对于一般函数, 我们证明Polyak步长和归一化GD获得快速的、路径相关的速率，尽管不使用方向平滑度的任何知识。 Logistic回归实验表明，我们的收敛保证比基于L平滑度的经典理论更紧密。",
    "tldr": "提出了一种新的次优性界限方法以解决梯度下降问题，利用方向平滑度开发上界并获得收敛保证，在实验中证明优于传统基于L平滑度的理论。"
}