{
    "title": "Distributed Momentum Methods Under Biased Gradient Estimations",
    "abstract": "arXiv:2403.00853v1 Announce Type: new  Abstract: Distributed stochastic gradient methods are gaining prominence in solving large-scale machine learning problems that involve data distributed across multiple nodes. However, obtaining unbiased stochastic gradients, which have been the focus of most theoretical research, is challenging in many distributed machine learning applications. The gradient estimations easily become biased, for example, when gradients are compressed or clipped, when data is shuffled, and in meta-learning and reinforcement learning. In this work, we establish non-asymptotic convergence bounds on distributed momentum methods under biased gradient estimation on both general non-convex and $\\mu$-PL non-convex problems. Our analysis covers general distributed optimization problems, and we work out the implications for special cases where gradient estimates are biased, i.e., in meta-learning and when the gradients are compressed or clipped. Our numerical experiments on ",
    "link": "https://arxiv.org/abs/2403.00853",
    "context": "Title: Distributed Momentum Methods Under Biased Gradient Estimations\nAbstract: arXiv:2403.00853v1 Announce Type: new  Abstract: Distributed stochastic gradient methods are gaining prominence in solving large-scale machine learning problems that involve data distributed across multiple nodes. However, obtaining unbiased stochastic gradients, which have been the focus of most theoretical research, is challenging in many distributed machine learning applications. The gradient estimations easily become biased, for example, when gradients are compressed or clipped, when data is shuffled, and in meta-learning and reinforcement learning. In this work, we establish non-asymptotic convergence bounds on distributed momentum methods under biased gradient estimation on both general non-convex and $\\mu$-PL non-convex problems. Our analysis covers general distributed optimization problems, and we work out the implications for special cases where gradient estimates are biased, i.e., in meta-learning and when the gradients are compressed or clipped. Our numerical experiments on ",
    "path": "papers/24/03/2403.00853.json",
    "total_tokens": 803,
    "translated_title": "在偏差梯度估计下的分布式动量方法",
    "translated_abstract": "分布式随机梯度方法在解决涉及分布在多个节点上的数据的大规模机器学习问题中日益受到重视。然而，在许多分布式机器学习应用中，获得无偏的随机梯度，这是大多数理论研究的重点，是具有挑战性的。梯度估计很容易变得有偏，例如，在梯度被压缩或剪切时，数据被洗牌时，以及在元学习和强化学习中。",
    "tldr": "本文在偏差梯度估计下建立了关于一般非凸和$\\mu$-PL非凸问题的分布式动量方法的非渐近收敛界限，覆盖了一般分布式优化问题的分析，并揭示了梯度估计有偏时的特殊情况下的影响，即在元学习和梯度被压缩或剪切时。",
    "en_tdlr": "This work establishes non-asymptotic convergence bounds on distributed momentum methods under biased gradient estimation for both general non-convex and $\\mu$-PL non-convex problems, covering general distributed optimization problems and revealing implications for special cases where gradient estimates are biased, such as in meta-learning and when the gradients are compressed or clipped."
}