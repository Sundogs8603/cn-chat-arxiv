{
    "title": "RL-CFR: Improving Action Abstraction for Imperfect Information Extensive-Form Games with Reinforcement Learning",
    "abstract": "arXiv:2403.04344v1 Announce Type: cross  Abstract: Effective action abstraction is crucial in tackling challenges associated with large action spaces in Imperfect Information Extensive-Form Games (IIEFGs). However, due to the vast state space and computational complexity in IIEFGs, existing methods often rely on fixed abstractions, resulting in sub-optimal performance. In response, we introduce RL-CFR, a novel reinforcement learning (RL) approach for dynamic action abstraction. RL-CFR builds upon our innovative Markov Decision Process (MDP) formulation, with states corresponding to public information and actions represented as feature vectors indicating specific action abstractions. The reward is defined as the expected payoff difference between the selected and default action abstractions. RL-CFR constructs a game tree with RL-guided action abstractions and utilizes counterfactual regret minimization (CFR) for strategy derivation. Impressively, it can be trained from scratch, achievin",
    "link": "https://arxiv.org/abs/2403.04344",
    "context": "Title: RL-CFR: Improving Action Abstraction for Imperfect Information Extensive-Form Games with Reinforcement Learning\nAbstract: arXiv:2403.04344v1 Announce Type: cross  Abstract: Effective action abstraction is crucial in tackling challenges associated with large action spaces in Imperfect Information Extensive-Form Games (IIEFGs). However, due to the vast state space and computational complexity in IIEFGs, existing methods often rely on fixed abstractions, resulting in sub-optimal performance. In response, we introduce RL-CFR, a novel reinforcement learning (RL) approach for dynamic action abstraction. RL-CFR builds upon our innovative Markov Decision Process (MDP) formulation, with states corresponding to public information and actions represented as feature vectors indicating specific action abstractions. The reward is defined as the expected payoff difference between the selected and default action abstractions. RL-CFR constructs a game tree with RL-guided action abstractions and utilizes counterfactual regret minimization (CFR) for strategy derivation. Impressively, it can be trained from scratch, achievin",
    "path": "papers/24/03/2403.04344.json",
    "total_tokens": 922,
    "translated_title": "RL-CFR: 使用强化学习改进不完全信息广义形式博弈的动作抽象",
    "translated_abstract": "有效的动作抽象在应对不完全信息广义形式博弈（IIEFGs）中大规模动作空间所带来的挑战方面至关重要。然而，由于IIEFGs中庞大的状态空间和计算复杂性，现有方法通常依赖于固定的抽象，导致性能次优。为此，我们引入RL-CFR，这是一种新颖的强化学习（RL）方法，用于动态动作抽象。RL-CFR基于我们创新的马尔可夫决策过程（MDP）的表述，其中状态对应于公共信息，动作表示为指示特定动作抽象的特征向量。奖励被定义为所选动作抽象和默认动作抽象之间的预期收益差异。RL-CFR构建一个带有RL引导动作抽象的游戏树，并利用对盘悔恨最小化（CFR）进行策略推导。令人印象深刻的是，它可以从头开始进行训练，取得了优秀的性能。",
    "tldr": "RL-CFR是一种用于动态动作抽象的强化学习方法，通过创新的MDP公式构建游戏树，并利用CFR进行策略推导，能够在不完全信息广义形式博弈中取得优秀性能。",
    "en_tdlr": "RL-CFR is a reinforcement learning method for dynamic action abstraction, building game trees with innovative MDP formulation and utilizing CFR for strategy derivation, achieving excellent performance in Imperfect Information Extensive-Form Games."
}