{
    "title": "Imitating Cost-Constrained Behaviors in Reinforcement Learning",
    "abstract": "arXiv:2403.17456v1 Announce Type: cross  Abstract: Complex planning and scheduling problems have long been solved using various optimization or heuristic approaches. In recent years, imitation learning that aims to learn from expert demonstrations has been proposed as a viable alternative to solving these problems. Generally speaking, imitation learning is designed to learn either the reward (or preference) model or directly the behavioral policy by observing the behavior of an expert. Existing work in imitation learning and inverse reinforcement learning has focused on imitation primarily in unconstrained settings (e.g., no limit on fuel consumed by the vehicle). However, in many real-world domains, the behavior of an expert is governed not only by reward (or preference) but also by constraints. For instance, decisions on self-driving delivery vehicles are dependent not only on the route preferences/rewards (depending on past demand data) but also on the fuel in the vehicle and the ti",
    "link": "https://arxiv.org/abs/2403.17456",
    "context": "Title: Imitating Cost-Constrained Behaviors in Reinforcement Learning\nAbstract: arXiv:2403.17456v1 Announce Type: cross  Abstract: Complex planning and scheduling problems have long been solved using various optimization or heuristic approaches. In recent years, imitation learning that aims to learn from expert demonstrations has been proposed as a viable alternative to solving these problems. Generally speaking, imitation learning is designed to learn either the reward (or preference) model or directly the behavioral policy by observing the behavior of an expert. Existing work in imitation learning and inverse reinforcement learning has focused on imitation primarily in unconstrained settings (e.g., no limit on fuel consumed by the vehicle). However, in many real-world domains, the behavior of an expert is governed not only by reward (or preference) but also by constraints. For instance, decisions on self-driving delivery vehicles are dependent not only on the route preferences/rewards (depending on past demand data) but also on the fuel in the vehicle and the ti",
    "path": "papers/24/03/2403.17456.json",
    "total_tokens": 865,
    "translated_title": "在强化学习中模仿受成本约束的行为",
    "translated_abstract": "长期以来，复杂的计划和调度问题一直通过各种优化或启发式方法来解决。最近，提出了从专家演示中学习的模仿学习作为解决这些问题的一种可行替代方法。模仿学习旨在通过观察专家的行为来学习奖励（或偏好）模型或直接行为策略。现有的模仿学习和逆向强化学习工作主要集中在无限制设置下的模仿（例如，车辆消耗的燃油量没有限制）。然而，在许多实际应用中，专家的行为不仅受奖励（或偏好）的影响，还受约束的影响。例如，自动驾驶送货车的决策不仅取决于路径偏好/奖励（根据过去的需求数据），还取决于车辆内的燃油和送达时间等约束。",
    "tldr": "该论文介绍了在强化学习中模仿受成本约束的行为的重要性，提出了模仿学习在受约束设置下的应用，并探讨了在实际领域中专家行为受限制因素影响的问题。",
    "en_tdlr": "The paper discusses the importance of imitating cost-constrained behaviors in reinforcement learning, proposes the application of imitation learning in constrained settings, and explores the influence of constraints on expert behaviors in real-world domains."
}