{
    "title": "MIP: CLIP-based Image Reconstruction from PEFT Gradients",
    "abstract": "arXiv:2403.07901v1 Announce Type: cross  Abstract: Contrastive Language-Image Pre-training (CLIP) model, as an effective pre-trained multimodal neural network, has been widely used in distributed machine learning tasks, especially Federated Learning (FL). Typically, CLIP-based FL adopts Parameter-Efficient Fine-Tuning (PEFT) for model training, which only fine-tunes adapter parameters or soft prompts rather than the full parameters. Although PEFT is different from the traditional training mode, in this paper, we theoretically analyze that the gradients of adapters or soft prompts can still be used to perform image reconstruction attacks. Based on our theoretical analysis, we propose Multm-In-Parvo (MIP), a proprietary reconstruction attack method targeting CLIP-based distributed machine learning architecture. Specifically, MIP can reconstruct CLIP training images according to the gradients of soft prompts or an adapter. In addition, MIP includes a label prediction strategy to accelerat",
    "link": "https://arxiv.org/abs/2403.07901",
    "context": "Title: MIP: CLIP-based Image Reconstruction from PEFT Gradients\nAbstract: arXiv:2403.07901v1 Announce Type: cross  Abstract: Contrastive Language-Image Pre-training (CLIP) model, as an effective pre-trained multimodal neural network, has been widely used in distributed machine learning tasks, especially Federated Learning (FL). Typically, CLIP-based FL adopts Parameter-Efficient Fine-Tuning (PEFT) for model training, which only fine-tunes adapter parameters or soft prompts rather than the full parameters. Although PEFT is different from the traditional training mode, in this paper, we theoretically analyze that the gradients of adapters or soft prompts can still be used to perform image reconstruction attacks. Based on our theoretical analysis, we propose Multm-In-Parvo (MIP), a proprietary reconstruction attack method targeting CLIP-based distributed machine learning architecture. Specifically, MIP can reconstruct CLIP training images according to the gradients of soft prompts or an adapter. In addition, MIP includes a label prediction strategy to accelerat",
    "path": "papers/24/03/2403.07901.json",
    "total_tokens": 838,
    "translated_title": "MIP: 从PEFT梯度中基于CLIP进行图像重构",
    "translated_abstract": "CLIP模型作为一种有效的预训练多模态神经网络，在分布式机器学习任务中被广泛使用，尤其是联邦学习。通常，基于CLIP的联邦学习采用参数高效微调（PEFT）进行模型训练，只微调适配器参数或软提示，而不是完整的参数。尽管PEFT与传统训练模式不同，但在本文中，我们从理论上分析了适配器或软提示的梯度仍然可以用于执行图像重构攻击。基于我们的理论分析，我们提出了Multm-In-Parvo（MIP），一种针对基于CLIP的分布式机器学习架构的专有重构攻击方法。具体而言，MIP可以根据软提示或适配器的梯度重构CLIP训练图像。此外，MIP包括一个标签预测策略来加速攻击过程。",
    "tldr": "本文分析了基于CLIP的联邦学习中PEFT的梯度仍可用于进行图像重构攻击，并提出了针对CLIP的重构攻击方法MIP。",
    "en_tdlr": "This paper analyzes that the gradients of PEFT in CLIP-based federated learning can still be used for image reconstruction attacks, and proposes a reconstruction attack method MIP targeting CLIP."
}