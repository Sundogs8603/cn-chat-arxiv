{
    "title": "SMART: Automatically Scaling Down Language Models with Accuracy Guarantees for Reduced Processing Fees",
    "abstract": "arXiv:2403.13835v1 Announce Type: cross  Abstract: The advancement of Large Language Models (LLMs) has significantly boosted performance in natural language processing (NLP) tasks. However, the deployment of high-performance LLMs incurs substantial costs, primarily due to the increased number of parameters aimed at enhancing model performance. This has made the use of state-of-the-art LLMs more expensive for end-users. AI service providers, such as OpenAI and Anthropic, often offer multiple versions of LLMs with varying prices and performance. However, end-users still face challenges in choosing the appropriate LLM for their tasks that balance result quality with cost.   We introduce SMART, Scaling Models Adaptively for Reduced Token Fees, a novel LLM framework designed to minimize the inference costs of NLP tasks while ensuring sufficient result quality. It enables users to specify an accuracy constraint in terms of the equivalence of outputs to those of the most powerful LLM. SMART t",
    "link": "https://arxiv.org/abs/2403.13835",
    "context": "Title: SMART: Automatically Scaling Down Language Models with Accuracy Guarantees for Reduced Processing Fees\nAbstract: arXiv:2403.13835v1 Announce Type: cross  Abstract: The advancement of Large Language Models (LLMs) has significantly boosted performance in natural language processing (NLP) tasks. However, the deployment of high-performance LLMs incurs substantial costs, primarily due to the increased number of parameters aimed at enhancing model performance. This has made the use of state-of-the-art LLMs more expensive for end-users. AI service providers, such as OpenAI and Anthropic, often offer multiple versions of LLMs with varying prices and performance. However, end-users still face challenges in choosing the appropriate LLM for their tasks that balance result quality with cost.   We introduce SMART, Scaling Models Adaptively for Reduced Token Fees, a novel LLM framework designed to minimize the inference costs of NLP tasks while ensuring sufficient result quality. It enables users to specify an accuracy constraint in terms of the equivalence of outputs to those of the most powerful LLM. SMART t",
    "path": "papers/24/03/2403.13835.json",
    "total_tokens": 750,
    "translated_title": "使用准确性保证自动缩减语言模型规模以降低处理费用的SMART",
    "translated_abstract": "大型语言模型（LLMs）的进步显著提高了自然语言处理（NLP）任务的性能。然而，部署高性能LLMs会产生巨大的成本，主要是由于增加的参数数量旨在提升模型性能。这使得最先进的LLMs对终端用户而言变得更加昂贵。我们引入了SMART，即为降低标记费用而自适应缩放模型，这是一个新颖的LLM框架，旨在最大程度地降低NLP任务的推理成本，同时确保足够的结果质量。它使用户能够以输出的等效性指定准确性约束与最强大的LLM。",
    "tldr": "提出了SMART框架，可通过准确性约束最小化自然语言处理任务的推理成本，同时确保结果质量。",
    "en_tdlr": "Introduced the SMART framework to minimize the inference costs of NLP tasks by specifying an accuracy constraint while ensuring result quality."
}