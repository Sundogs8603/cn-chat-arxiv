{
    "title": "Improving Acoustic Word Embeddings through Correspondence Training of Self-supervised Speech Representations",
    "abstract": "arXiv:2403.08738v1 Announce Type: new  Abstract: Acoustic word embeddings (AWEs) are vector representations of spoken words. An effective method for obtaining AWEs is the Correspondence Auto-Encoder (CAE). In the past, the CAE method has been associated with traditional MFCC features. Representations obtained from self-supervised learning (SSL)-based speech models such as HuBERT, Wav2vec2, etc., are outperforming MFCC in many downstream tasks. However, they have not been well studied in the context of learning AWEs. This work explores the effectiveness of CAE with SSL-based speech representations to obtain improved AWEs. Additionally, the capabilities of SSL-based speech models are explored in cross-lingual scenarios for obtaining AWEs. Experiments are conducted on five languages: Polish, Portuguese, Spanish, French, and English. HuBERT-based CAE model achieves the best results for word discrimination in all languages, despite Hu-BERT being pre-trained on English only. Also, the HuBERT",
    "link": "https://arxiv.org/abs/2403.08738",
    "context": "Title: Improving Acoustic Word Embeddings through Correspondence Training of Self-supervised Speech Representations\nAbstract: arXiv:2403.08738v1 Announce Type: new  Abstract: Acoustic word embeddings (AWEs) are vector representations of spoken words. An effective method for obtaining AWEs is the Correspondence Auto-Encoder (CAE). In the past, the CAE method has been associated with traditional MFCC features. Representations obtained from self-supervised learning (SSL)-based speech models such as HuBERT, Wav2vec2, etc., are outperforming MFCC in many downstream tasks. However, they have not been well studied in the context of learning AWEs. This work explores the effectiveness of CAE with SSL-based speech representations to obtain improved AWEs. Additionally, the capabilities of SSL-based speech models are explored in cross-lingual scenarios for obtaining AWEs. Experiments are conducted on five languages: Polish, Portuguese, Spanish, French, and English. HuBERT-based CAE model achieves the best results for word discrimination in all languages, despite Hu-BERT being pre-trained on English only. Also, the HuBERT",
    "path": "papers/24/03/2403.08738.json",
    "total_tokens": 923,
    "translated_title": "通过对自监督语音表示进行对应训练来改进声学词嵌入",
    "translated_abstract": "声学词嵌入（AWEs）是口语词汇的向量表示。获得AWEs的一种有效方法是通过对应自动编码器（CAE）。过去，CAE方法一直与传统MFCC特征相关联。从自监督学习（SSL）为基础的语音模型（如HuBERT、Wav2vec2等）中获得的表示在许多下游任务中优于MFCC。但是，在学习AWEs的情况下，它们并未得到充分研究。本文探讨了利用CAE与基于SSL的语音表示来获得改进的AWEs的有效性。此外，还探讨了在跨语言环境中利用基于SSL的语音模型获取AWEs的能力。在波兰语、葡萄牙语、西班牙语、法语和英语中进行了实验。基于HuBERT的CAE模型在所有语言中都实现了最佳的词辨别结果，尽管HuBERT仅在英语上进行了预训练。此外，HuBERT 在跨语言环境中展示了优异的性能。",
    "tldr": "通过对应自动编码器与基于自监督学习的语音表示相结合，探索了一种改进声学词嵌入的方法，并在跨语言环境中取得了优异的结果。",
    "en_tdlr": "By combining Correspondence Auto-Encoder with SSL-based speech representations, this work explores a method to improve acoustic word embeddings and achieves excellent results in cross-lingual scenarios."
}