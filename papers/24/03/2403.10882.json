{
    "title": "Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean",
    "abstract": "arXiv:2403.10882v1 Announce Type: cross  Abstract: Large language models (LLMs) use pretraining to predict the subsequent word; however, their expansion requires significant computing resources. Numerous big tech companies and research institutes have developed multilingual LLMs (MLLMs) to meet current demands, overlooking less-resourced languages (LRLs). This study proposed three strategies to enhance the performance of LRLs based on the publicly available MLLMs. First, the MLLM vocabularies of LRLs were expanded to enhance expressiveness. Second, bilingual data were used for pretraining to align the high- and less-resourced languages. Third, a high-quality small-scale instruction dataset was constructed and instruction-tuning was performed to augment the LRL. The experiments employed the Llama2 model and Korean was used as the LRL, which was quantitatively evaluated against other developed LLMs across eight tasks. Furthermore, a qualitative assessment was performed based on human eva",
    "link": "https://arxiv.org/abs/2403.10882",
    "context": "Title: Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean\nAbstract: arXiv:2403.10882v1 Announce Type: cross  Abstract: Large language models (LLMs) use pretraining to predict the subsequent word; however, their expansion requires significant computing resources. Numerous big tech companies and research institutes have developed multilingual LLMs (MLLMs) to meet current demands, overlooking less-resourced languages (LRLs). This study proposed three strategies to enhance the performance of LRLs based on the publicly available MLLMs. First, the MLLM vocabularies of LRLs were expanded to enhance expressiveness. Second, bilingual data were used for pretraining to align the high- and less-resourced languages. Third, a high-quality small-scale instruction dataset was constructed and instruction-tuning was performed to augment the LRL. The experiments employed the Llama2 model and Korean was used as the LRL, which was quantitatively evaluated against other developed LLMs across eight tasks. Furthermore, a qualitative assessment was performed based on human eva",
    "path": "papers/24/03/2403.10882.json",
    "total_tokens": 879,
    "translated_title": "优化多语言大语言模型的语言增强：以韩语为例的案例研究",
    "translated_abstract": "大型语言模型（LLMs）使用预训练来预测下一个单词；然而，它们的扩展需要大量计算资源。许多大型科技公司和研究机构已经开发了多语言LLMs（MLLMs）以满足当前需求，但忽视了资源较少的语言（LRLs）。本研究提出了三种策略来增强基于公开可用MLLMs的LRLs的性能。首先，扩展LRLs的MLLM词汇以增强表达性。其次，使用双语数据进行预训练以对齐高资源语言和低资源语言。第三，构建高质量的小规模指导数据集，并进行指导微调以增强LRL。实验采用了Llama2模型，以韩语作为LRL，并在八项任务中对其与其他已开发的LLMs进行了定量评估。此外，基于人类评估进行了定性评估。",
    "tldr": "该研究提出了三种策略来增强基于公开可用MLLMs的资源较少的语言的性能，包括扩展词汇、双语数据预训练和指导微调。",
    "en_tdlr": "This study proposed three strategies to enhance the performance of less-resourced languages based on publicly available MLLMs, including vocabulary expansion, bilingual data pretraining, and instruction-tuning."
}