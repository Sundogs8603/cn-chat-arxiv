{
    "title": "DA-Net: A Disentangled and Adaptive Network for Multi-Source Cross-Lingual Transfer Learning",
    "abstract": "arXiv:2403.04158v1 Announce Type: cross  Abstract: Multi-Source cross-lingual transfer learning deals with the transfer of task knowledge from multiple labelled source languages to an unlabeled target language under the language shift. Existing methods typically focus on weighting the predictions produced by language-specific classifiers of different sources that follow a shared encoder. However, all source languages share the same encoder, which is updated by all these languages. The extracted representations inevitably contain different source languages' information, which may disturb the learning of the language-specific classifiers. Additionally, due to the language gap, language-specific classifiers trained with source labels are unable to make accurate predictions for the target language. Both facts impair the model's performance. To address these challenges, we propose a Disentangled and Adaptive Network (DA-Net). Firstly, we devise a feedback-guided collaborative disentanglemen",
    "link": "https://arxiv.org/abs/2403.04158",
    "context": "Title: DA-Net: A Disentangled and Adaptive Network for Multi-Source Cross-Lingual Transfer Learning\nAbstract: arXiv:2403.04158v1 Announce Type: cross  Abstract: Multi-Source cross-lingual transfer learning deals with the transfer of task knowledge from multiple labelled source languages to an unlabeled target language under the language shift. Existing methods typically focus on weighting the predictions produced by language-specific classifiers of different sources that follow a shared encoder. However, all source languages share the same encoder, which is updated by all these languages. The extracted representations inevitably contain different source languages' information, which may disturb the learning of the language-specific classifiers. Additionally, due to the language gap, language-specific classifiers trained with source labels are unable to make accurate predictions for the target language. Both facts impair the model's performance. To address these challenges, we propose a Disentangled and Adaptive Network (DA-Net). Firstly, we devise a feedback-guided collaborative disentanglemen",
    "path": "papers/24/03/2403.04158.json",
    "total_tokens": 801,
    "translated_title": "DA-Net: 一种用于多源跨语言迁移学习的解耦自适应网络",
    "translated_abstract": "多源跨语言迁移学习涉及从多个已标记源语言向一个未标记目标语言在语言转移下的任务知识传输。现有方法通常关注于加权不同源语言的特定语言分类器生成的预测，这些分类器遵循共享编码器。然而，所有源语言共享相同的编码器，这个编码器被所有这些语言更新。提取出的表示不可避免地包含不同源语言的信息，这可能干扰语言特定分类器的学习。此外，由于语言差距，使用源标签训练的语言特定分类器无法准确预测目标语言。这两个事实损害了模型的性能。为了解决这些挑战，我们提出了一种解耦自适应网络 (DA-Net)。",
    "tldr": "DA-Net 提出了一种解决多源跨语言迁移学习中共享编码器导致学习困扰和语言特定分类器性能下降的新方法。",
    "en_tdlr": "DA-Net proposes a new method to address the learning disturbance caused by shared encoders and the decrease in performance of language-specific classifiers in multi-source cross-lingual transfer learning."
}