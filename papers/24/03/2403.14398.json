{
    "title": "Regularized Adaptive Momentum Dual Averaging with an Efficient Inexact Subproblem Solver for Training Structured Neural Network",
    "abstract": "arXiv:2403.14398v1 Announce Type: new  Abstract: We propose a Regularized Adaptive Momentum Dual Averaging (RAMDA) algorithm for training structured neural networks. Similar to existing regularized adaptive methods, the subproblem for computing the update direction of RAMDA involves a nonsmooth regularizer and a diagonal preconditioner, and therefore does not possess a closed-form solution in general. We thus also carefully devise an implementable inexactness condition that retains convergence guarantees similar to the exact versions, and propose a companion efficient solver for the subproblems of both RAMDA and existing methods to make them practically feasible. We leverage the theory of manifold identification in variational analysis to show that, even in the presence of such inexactness, the iterates of RAMDA attain the ideal structure induced by the regularizer at the stationary point of asymptotic convergence. This structure is locally optimal near the point of convergence, so RAM",
    "link": "https://arxiv.org/abs/2403.14398",
    "context": "Title: Regularized Adaptive Momentum Dual Averaging with an Efficient Inexact Subproblem Solver for Training Structured Neural Network\nAbstract: arXiv:2403.14398v1 Announce Type: new  Abstract: We propose a Regularized Adaptive Momentum Dual Averaging (RAMDA) algorithm for training structured neural networks. Similar to existing regularized adaptive methods, the subproblem for computing the update direction of RAMDA involves a nonsmooth regularizer and a diagonal preconditioner, and therefore does not possess a closed-form solution in general. We thus also carefully devise an implementable inexactness condition that retains convergence guarantees similar to the exact versions, and propose a companion efficient solver for the subproblems of both RAMDA and existing methods to make them practically feasible. We leverage the theory of manifold identification in variational analysis to show that, even in the presence of such inexactness, the iterates of RAMDA attain the ideal structure induced by the regularizer at the stationary point of asymptotic convergence. This structure is locally optimal near the point of convergence, so RAM",
    "path": "papers/24/03/2403.14398.json",
    "total_tokens": 838,
    "translated_title": "用效率低下的近似子问题解算器训练结构化神经网络的正则化自适应动量双平均",
    "translated_abstract": "我们提出了一种用于训练结构化神经网络的正则化自适应动量双平均（RAMDA）算法。与现有的正则化自适应方法类似，RAMDA的更新方向计算子问题涉及非光滑正则化项和对角预处理器，因此一般而言没有封闭形式的解。我们精心设计了一个可实现的近似条件，保留了类似于精确版本的收敛性保证，并提出了一个配套的高效求解器，用于使RAMDA和现有方法的子问题在实践中可行。我们利用变分分析中的流形识别理论表明，即使存在这种近似性，RAMDA的迭代在渐近收敛的稳定点处达到由正则化项诱导的理想结构。在收敛点附近，这种结构在局部上是最优的。",
    "tldr": "提出了RAMDA算法用于训练结构化神经网络，引入了使用近似解的方法，并证明在收敛点附近RAMDA的迭代达到了最优结构。",
    "en_tdlr": "RAMDA algorithm is proposed for training structured neural networks, utilizing approximate solutions and demonstrating that the iterates of RAMDA attain the optimal structure near the convergence point."
}