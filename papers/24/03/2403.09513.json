{
    "title": "AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting",
    "abstract": "arXiv:2403.09513v1 Announce Type: cross  Abstract: With the advent and widespread deployment of Multimodal Large Language Models (MLLMs), the imperative to ensure their safety has become increasingly pronounced. However, with the integration of additional modalities, MLLMs are exposed to new vulnerabilities, rendering them prone to structured-based jailbreak attacks, where semantic content (e.g., \"harmful text\") has been injected into the images to mislead MLLMs. In this work, we aim to defend against such threats. Specifically, we propose \\textbf{Ada}ptive \\textbf{Shield} Prompting (\\textbf{AdaShield}), which prepends inputs with defense prompts to defend MLLMs against structure-based jailbreak attacks without fine-tuning MLLMs or training additional modules (e.g., post-stage content detector). Initially, we present a manually designed static defense prompt, which thoroughly examines the image and instruction content step by step and specifies response methods to malicious queries. Fu",
    "link": "https://arxiv.org/abs/2403.09513",
    "context": "Title: AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting\nAbstract: arXiv:2403.09513v1 Announce Type: cross  Abstract: With the advent and widespread deployment of Multimodal Large Language Models (MLLMs), the imperative to ensure their safety has become increasingly pronounced. However, with the integration of additional modalities, MLLMs are exposed to new vulnerabilities, rendering them prone to structured-based jailbreak attacks, where semantic content (e.g., \"harmful text\") has been injected into the images to mislead MLLMs. In this work, we aim to defend against such threats. Specifically, we propose \\textbf{Ada}ptive \\textbf{Shield} Prompting (\\textbf{AdaShield}), which prepends inputs with defense prompts to defend MLLMs against structure-based jailbreak attacks without fine-tuning MLLMs or training additional modules (e.g., post-stage content detector). Initially, we present a manually designed static defense prompt, which thoroughly examines the image and instruction content step by step and specifies response methods to malicious queries. Fu",
    "path": "papers/24/03/2403.09513.json",
    "total_tokens": 907,
    "translated_title": "AdaShield：通过自适应盾牌提示保护多模态大型语言模型免受基于结构的攻击",
    "translated_abstract": "随着多模态大型语言模型（MLLMs）的出现和广泛部署，确保它们的安全性变得愈发重要。然而，随着额外模态的整合，MLLMs暴露于新的漏洞，使其容易遭受基于结构的越狱攻击，即向图像中注入语义内容（例如“有害文本”）以误导MLLMs。在这项工作中，我们旨在抵御此类威胁。具体来说，我们提出了\\textbf{Ada}ptive \\textbf{Shield} Prompting（\\textbf{AdaShield}），它在输入前添加防御提示，以在不微调MLLMs或训练额外模块（例如后阶段内容检测器）的情况下，保护MLLMs免受基于结构的越狱攻击。最初，我们呈现了一个手动设计的静态防御提示，它逐步彻底检查图像和指令内容，并指定响应恶意查询的方法。",
    "tldr": "AdaShield 提出了自适应盾牌提示方法，无需微调或额外训练模块，即可保护多模态大型语言模型免受基于结构的越狱攻击。"
}