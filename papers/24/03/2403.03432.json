{
    "title": "Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language Models",
    "abstract": "arXiv:2403.03432v1 Announce Type: cross  Abstract: Instruction Tuning has the potential to stimulate or enhance specific capabilities of large language models (LLMs). However, achieving the right balance of data is crucial to prevent catastrophic forgetting and interference between tasks. To address these limitations and enhance training flexibility, we propose the Mixture-of-LoRAs (MoA) architecture which is a novel and parameter-efficient tuning method designed for multi-task learning with LLMs. In this paper, we start by individually training multiple domain-specific LoRA modules using corresponding supervised corpus data. These LoRA modules can be aligned with the expert design principles observed in Mixture-of-Experts (MoE). Subsequently, we combine the multiple LoRAs using an explicit routing strategy and introduce domain labels to facilitate multi-task learning, which help prevent interference between tasks and ultimately enhances the performance of each individual task. Further",
    "link": "https://arxiv.org/abs/2403.03432",
    "context": "Title: Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language Models\nAbstract: arXiv:2403.03432v1 Announce Type: cross  Abstract: Instruction Tuning has the potential to stimulate or enhance specific capabilities of large language models (LLMs). However, achieving the right balance of data is crucial to prevent catastrophic forgetting and interference between tasks. To address these limitations and enhance training flexibility, we propose the Mixture-of-LoRAs (MoA) architecture which is a novel and parameter-efficient tuning method designed for multi-task learning with LLMs. In this paper, we start by individually training multiple domain-specific LoRA modules using corresponding supervised corpus data. These LoRA modules can be aligned with the expert design principles observed in Mixture-of-Experts (MoE). Subsequently, we combine the multiple LoRAs using an explicit routing strategy and introduce domain labels to facilitate multi-task learning, which help prevent interference between tasks and ultimately enhances the performance of each individual task. Further",
    "path": "papers/24/03/2403.03432.json",
    "total_tokens": 892,
    "translated_title": "混合LoRAs：大型语言模型的高效多任务调优",
    "translated_abstract": "指导调优有潜力激发或增强大型语言模型（LLMs）的特定能力。然而，实现数据的正确平衡对于防止灾难性遗忘和任务之间的干扰至关重要。为了解决这些限制并增强训练灵活性，我们提出了适用于LLMs的新颖和参数高效的调优方法——混合LoRAs（MoA）架构。在本文中，我们通过使用相应的监督语料库数据单独训练多个特定领域的LoRA模块。这些LoRA模块可以与在专家设计原则中观察到的专家混合（MoE）相一致。随后，我们利用显式路由策略组合多个LoRAs，并引入领域标签以促进多任务学习，这有助于防止任务之间的干扰，并最终提升每个个别任务的性能。",
    "tldr": "提出了一种新颖且参数高效的混合LoRAs（MoA）架构，通过引入领域标签和显式路由策略，解决了大型语言模型多任务学习中的灾难性遗忘和任务干扰问题，从而提升了每个任务的性能。",
    "en_tdlr": "Proposed a novel and parameter-efficient Mixture-of-LoRAs (MoA) architecture, which addresses catastrophic forgetting and task interference issues in multi-task learning of large language models by introducing domain labels and explicit routing strategy, ultimately enhancing the performance of each individual task."
}