{
    "title": "A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives",
    "abstract": "arXiv:2403.03037v1 Announce Type: cross  Abstract: Human comprehension of a video stream is naturally broad: in a few instants, we are able to understand what is happening, the relevance and relationship of objects, and forecast what will follow in the near future, everything all at once. We believe that - to effectively transfer such an holistic perception to intelligent machines - an important role is played by learning to correlate concepts and to abstract knowledge coming from different tasks, to synergistically exploit them when learning novel skills. To accomplish this, we seek for a unified approach to video understanding which combines shared temporal modelling of human actions with minimal overhead, to support multiple downstream tasks and enable cooperation when learning novel skills. We then propose EgoPack, a solution that creates a collection of task perspectives that can be carried across downstream tasks and used as a potential source of additional insights, as a backpac",
    "link": "https://arxiv.org/abs/2403.03037",
    "context": "Title: A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives\nAbstract: arXiv:2403.03037v1 Announce Type: cross  Abstract: Human comprehension of a video stream is naturally broad: in a few instants, we are able to understand what is happening, the relevance and relationship of objects, and forecast what will follow in the near future, everything all at once. We believe that - to effectively transfer such an holistic perception to intelligent machines - an important role is played by learning to correlate concepts and to abstract knowledge coming from different tasks, to synergistically exploit them when learning novel skills. To accomplish this, we seek for a unified approach to video understanding which combines shared temporal modelling of human actions with minimal overhead, to support multiple downstream tasks and enable cooperation when learning novel skills. We then propose EgoPack, a solution that creates a collection of task perspectives that can be carried across downstream tasks and used as a potential source of additional insights, as a backpac",
    "path": "papers/24/03/2403.03037.json",
    "total_tokens": 869,
    "translated_title": "一个充满技能的背包：具有多元任务视角的自我中心视频理解",
    "translated_abstract": "arXiv：2403.03037v1 公告类型：交叉摘要：人类对视频流的理解自然而然是广泛的：在短短的瞬间内，我们能够理解发生了什么，对象的相关性和关系，并预测接下来的将来，所有这些一次性完成。我们认为，要有效地将这样的整体感知转移到智能机器中，学习相关概念和提炼来自不同任务的知识之间的关系扮演了重要角色，以便在学习新技能时协同利用它们。为了实现这一点，我们寻求一种统一的视频理解方法，将人类行为的共享时间建模与最小开销相结合，以支持多个下游任务并在学习新技能时进行合作。然后，我们提出了EgoPack，这是一个解决方案，创建了在下游任务中可以携带的任务视角集合，并可用作额外见解的潜在来源，就像一个背包。",
    "tldr": "提出了EgoPack，这是一个统一的视频理解方法，结合共享时间建模和最小开销，支持多个下游任务，并在学习新技能时进行合作，为智能机器提供全面的视频理解能力。"
}