{
    "title": "Generalization Error Analysis for Sparse Mixture-of-Experts: A Preliminary Study",
    "abstract": "arXiv:2403.17404v1 Announce Type: new  Abstract: Mixture-of-Experts (MoE) represents an ensemble methodology that amalgamates predictions from several specialized sub-models (referred to as experts). This fusion is accomplished through a router mechanism, dynamically assigning weights to each expert's contribution based on the input data. Conventional MoE mechanisms select all available experts, incurring substantial computational costs. In contrast, Sparse Mixture-of-Experts (Sparse MoE) selectively engages only a limited number, or even just one expert, significantly reducing computation overhead while empirically preserving, and sometimes even enhancing, performance. Despite its wide-ranging applications and these advantageous characteristics, MoE's theoretical underpinnings have remained elusive. In this paper, we embark on an exploration of Sparse MoE's generalization error concerning various critical factors. Specifically, we investigate the impact of the number of data samples, ",
    "link": "https://arxiv.org/abs/2403.17404",
    "context": "Title: Generalization Error Analysis for Sparse Mixture-of-Experts: A Preliminary Study\nAbstract: arXiv:2403.17404v1 Announce Type: new  Abstract: Mixture-of-Experts (MoE) represents an ensemble methodology that amalgamates predictions from several specialized sub-models (referred to as experts). This fusion is accomplished through a router mechanism, dynamically assigning weights to each expert's contribution based on the input data. Conventional MoE mechanisms select all available experts, incurring substantial computational costs. In contrast, Sparse Mixture-of-Experts (Sparse MoE) selectively engages only a limited number, or even just one expert, significantly reducing computation overhead while empirically preserving, and sometimes even enhancing, performance. Despite its wide-ranging applications and these advantageous characteristics, MoE's theoretical underpinnings have remained elusive. In this paper, we embark on an exploration of Sparse MoE's generalization error concerning various critical factors. Specifically, we investigate the impact of the number of data samples, ",
    "path": "papers/24/03/2403.17404.json",
    "total_tokens": 805,
    "translated_title": "稀疏专家混合的泛化误差分析: 一项初步研究",
    "translated_abstract": "Mixture-of-Experts (MoE)代表了一种整合预测来自几个专门子模型（称为专家）的方法。这种融合是通过一个路由机制实现的，根据输入数据动态分配权重给每个专家的贡献。传统的MoE机制选择所有可用的专家，带来了可观的计算成本。相反，稀疏专家混合（Sparse MoE）只选择有限数量，甚至只有一个专家，显着降低计算开销，同时在经验上保留，有时甚至增强性能。尽管MoE具有广泛的应用和这些优点，但其理论基础仍然难以捉摸。本文探讨了稀疏MoE在各种关键因素方面的泛化误差。具体来说，我们研究了数据样本数量的影响",
    "tldr": "本文讨论了稀疏专家混合模型在泛化误差方面的探索，特别关注了数据样本数量的影响。",
    "en_tdlr": "This paper explores the generalization error of sparse mixture-of-experts model, focusing on the impact of the number of data samples."
}