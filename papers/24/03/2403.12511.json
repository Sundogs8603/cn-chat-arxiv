{
    "title": "Forward Gradient-Based Frank-Wolfe Optimization for Memory Efficient Deep Neural Network Training",
    "abstract": "arXiv:2403.12511v1 Announce Type: new  Abstract: Training a deep neural network using gradient-based methods necessitates the calculation of gradients at each level. However, using backpropagation or reverse mode differentiation, to calculate the gradients necessities significant memory consumption, rendering backpropagation an inefficient method for computing gradients. This paper focuses on analyzing the performance of the well-known Frank-Wolfe algorithm, a.k.a. conditional gradient algorithm by having access to the forward mode of automatic differentiation to compute gradients. We provide in-depth technical details that show the proposed Algorithm does converge to the optimal solution with a sub-linear rate of convergence by having access to the noisy estimate of the true gradient obtained in the forward mode of automated differentiation, referred to as the Projected Forward Gradient. In contrast, the standard Frank-Wolfe algorithm, when provided with access to the Projected Forwar",
    "link": "https://arxiv.org/abs/2403.12511",
    "context": "Title: Forward Gradient-Based Frank-Wolfe Optimization for Memory Efficient Deep Neural Network Training\nAbstract: arXiv:2403.12511v1 Announce Type: new  Abstract: Training a deep neural network using gradient-based methods necessitates the calculation of gradients at each level. However, using backpropagation or reverse mode differentiation, to calculate the gradients necessities significant memory consumption, rendering backpropagation an inefficient method for computing gradients. This paper focuses on analyzing the performance of the well-known Frank-Wolfe algorithm, a.k.a. conditional gradient algorithm by having access to the forward mode of automatic differentiation to compute gradients. We provide in-depth technical details that show the proposed Algorithm does converge to the optimal solution with a sub-linear rate of convergence by having access to the noisy estimate of the true gradient obtained in the forward mode of automated differentiation, referred to as the Projected Forward Gradient. In contrast, the standard Frank-Wolfe algorithm, when provided with access to the Projected Forwar",
    "path": "papers/24/03/2403.12511.json",
    "total_tokens": 768,
    "translated_title": "基于前向梯度的Frank-Wolfe优化用于高效训练深度神经网络",
    "translated_abstract": "使用基于梯度的方法训练深度神经网络需要在每个级别计算梯度。然而，使用反向传播或反向模式微分计算梯度需要消耗大量内存，使反向传播成为计算梯度的一种低效方法。本文重点分析了著名的Frank-Wolfe算法的性能，即有条件的梯度算法，通过访问前向自动微分以计算梯度。我们提供了深入的技术细节，显示所提出的算法通过访问在前向自动微分中获得的真梯度的有噪声估计， 即称为Projected Forward Gradient，收敛于最优解，收敛速度为次线性。相比之下，标准的Frank-Wolfe算法，在提供Projected Fors",
    "tldr": "本文利用前向自动微分计算梯度，提出了基于Frank-Wolfe算法的优化方法，收敛速度为次线性。",
    "en_tdlr": "This paper proposes an optimization method based on the Frank-Wolfe algorithm using forward automatic differentiation to compute gradients, achieving sub-linear convergence rate."
}