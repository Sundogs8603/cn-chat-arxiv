{
    "title": "Meta-operators for Enabling Parallel Planning Using Deep Reinforcement Learning",
    "abstract": "arXiv:2403.08910v1 Announce Type: new  Abstract: There is a growing interest in the application of Reinforcement Learning (RL) techniques to AI planning with the aim to come up with general policies. Typically, the mapping of the transition model of AI planning to the state transition system of a Markov Decision Process is established by assuming a one-to-one correspondence of the respective action spaces. In this paper, we introduce the concept of meta-operator as the result of simultaneously applying multiple planning operators, and we show that including meta-operators in the RL action space enables new planning perspectives to be addressed using RL, such as parallel planning. Our research aims to analyze the performance and complexity of including meta-operators in the RL process, concretely in domains where satisfactory outcomes have not been previously achieved using usual generalized planning models. The main objective of this article is thus to pave the way towards a redefiniti",
    "link": "https://arxiv.org/abs/2403.08910",
    "context": "Title: Meta-operators for Enabling Parallel Planning Using Deep Reinforcement Learning\nAbstract: arXiv:2403.08910v1 Announce Type: new  Abstract: There is a growing interest in the application of Reinforcement Learning (RL) techniques to AI planning with the aim to come up with general policies. Typically, the mapping of the transition model of AI planning to the state transition system of a Markov Decision Process is established by assuming a one-to-one correspondence of the respective action spaces. In this paper, we introduce the concept of meta-operator as the result of simultaneously applying multiple planning operators, and we show that including meta-operators in the RL action space enables new planning perspectives to be addressed using RL, such as parallel planning. Our research aims to analyze the performance and complexity of including meta-operators in the RL process, concretely in domains where satisfactory outcomes have not been previously achieved using usual generalized planning models. The main objective of this article is thus to pave the way towards a redefiniti",
    "path": "papers/24/03/2403.08910.json",
    "total_tokens": 827,
    "translated_title": "通过深度强化学习实现并行规划的元算子",
    "translated_abstract": "最近人们对使用强化学习（RL）技术进行AI规划的应用越发感兴趣，旨在提出通用策略。通常，通过假设各自动作空间具有一一对应关系，将AI规划的转移模型映射到马尔可夫决策过程的状态转移系统。本文介绍了元算子的概念，它是同时应用多个规划算子的结果，并且我们展示了将元算子包括在RL动作空间中如何使得新的规划视角能够通过RL来解决，例如并行规划。我们的研究旨在分析在RL过程中包括元算子的性能和复杂性，具体应用于以往使用传统广义规划模型未曾取得满意结果的领域。因此，本文的主要目标是为重定义跨深层规划和强化学习之间的界限铺平道路。",
    "tldr": "引入了元算子的概念，使得RL动作空间能够同时处理多个规划操作符，从而实现新的规划视角，例如并行规划。",
    "en_tdlr": "Introducing the concept of meta-operators allows the RL action space to simultaneously handle multiple planning operators, enabling new planning perspectives such as parallel planning."
}