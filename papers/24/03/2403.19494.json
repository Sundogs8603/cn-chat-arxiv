{
    "title": "Regression with Multi-Expert Deferral",
    "abstract": "arXiv:2403.19494v1 Announce Type: new  Abstract: Learning to defer with multiple experts is a framework where the learner can choose to defer the prediction to several experts. While this problem has received significant attention in classification contexts, it presents unique challenges in regression due to the infinite and continuous nature of the label space. In this work, we introduce a novel framework of regression with deferral, which involves deferring the prediction to multiple experts. We present a comprehensive analysis for both the single-stage scenario, where there is simultaneous learning of predictor and deferral functions, and the two-stage scenario, which involves a pre-trained predictor with a learned deferral function. We introduce new surrogate loss functions for both scenarios and prove that they are supported by $H$-consistency bounds. These bounds provide consistency guarantees that are stronger than Bayes consistency, as they are non-asymptotic and hypothesis set",
    "link": "https://arxiv.org/abs/2403.19494",
    "context": "Title: Regression with Multi-Expert Deferral\nAbstract: arXiv:2403.19494v1 Announce Type: new  Abstract: Learning to defer with multiple experts is a framework where the learner can choose to defer the prediction to several experts. While this problem has received significant attention in classification contexts, it presents unique challenges in regression due to the infinite and continuous nature of the label space. In this work, we introduce a novel framework of regression with deferral, which involves deferring the prediction to multiple experts. We present a comprehensive analysis for both the single-stage scenario, where there is simultaneous learning of predictor and deferral functions, and the two-stage scenario, which involves a pre-trained predictor with a learned deferral function. We introduce new surrogate loss functions for both scenarios and prove that they are supported by $H$-consistency bounds. These bounds provide consistency guarantees that are stronger than Bayes consistency, as they are non-asymptotic and hypothesis set",
    "path": "papers/24/03/2403.19494.json",
    "total_tokens": 847,
    "translated_title": "具有多专家推迟的回归",
    "translated_abstract": "学习与多个专家推迟是一个框架，其中学习者可以选择将预测推迟给多个专家。虽然在分类情境中该问题得到了重视，但由于标签空间的无限和连续特性，它在回归中面临独特挑战。在这项工作中，我们引入了一个具有推迟的回归新框架，其中涉及将预测推迟给多个专家。我们针对单阶段情景和双阶段情景进行了全面分析，前者涉及预测器和推迟函数的同时学习，后者涉及具有已训练预测器和学习推迟函数。我们为两种情景引入了新的代理损失函数，并证明它们受到$H$-一致性界限的支持。这些界限提供了比贝叶斯一致性更强的一致性保证，因为它们是非渐近的，且假设集",
    "tldr": "这是一个回归问题的新框架，涉及将预测推迟给多个专家，提出了单阶段和双阶段情景的全面分析，并引入了新的代理损失函数及其支持的一致性界限。",
    "en_tdlr": "This is a new framework for regression problems involving deferring predictions to multiple experts, with comprehensive analyses for both single-stage and two-stage scenarios, introducing new surrogate loss functions and their supported consistency bounds."
}