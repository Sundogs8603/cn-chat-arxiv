{
    "title": "How to be fair? A study of label and selection bias",
    "abstract": "arXiv:2403.14282v1 Announce Type: cross  Abstract: It is widely accepted that biased data leads to biased and thus potentially unfair models. Therefore, several measures for bias in data and model predictions have been proposed, as well as bias mitigation techniques whose aim is to learn models that are fair by design. Despite the myriad of mitigation techniques developed in the past decade, however, it is still poorly understood under what circumstances which methods work. Recently, Wick et al. showed, with experiments on synthetic data, that there exist situations in which bias mitigation techniques lead to more accurate models when measured on unbiased data. Nevertheless, in the absence of a thorough mathematical analysis, it remains unclear which techniques are effective under what circumstances. We propose to address this problem by establishing relationships between the type of bias and the effectiveness of a mitigation technique, where we categorize the mitigation techniques by ",
    "link": "https://arxiv.org/abs/2403.14282",
    "context": "Title: How to be fair? A study of label and selection bias\nAbstract: arXiv:2403.14282v1 Announce Type: cross  Abstract: It is widely accepted that biased data leads to biased and thus potentially unfair models. Therefore, several measures for bias in data and model predictions have been proposed, as well as bias mitigation techniques whose aim is to learn models that are fair by design. Despite the myriad of mitigation techniques developed in the past decade, however, it is still poorly understood under what circumstances which methods work. Recently, Wick et al. showed, with experiments on synthetic data, that there exist situations in which bias mitigation techniques lead to more accurate models when measured on unbiased data. Nevertheless, in the absence of a thorough mathematical analysis, it remains unclear which techniques are effective under what circumstances. We propose to address this problem by establishing relationships between the type of bias and the effectiveness of a mitigation technique, where we categorize the mitigation techniques by ",
    "path": "papers/24/03/2403.14282.json",
    "total_tokens": 798,
    "translated_title": "如何做到公平？标签和选择偏差研究",
    "translated_abstract": "众所周知，偏见数据会导致偏见、潜在不公平的模型。因此，已经提出了几种用于数据和模型预测偏见的措施，以及其目标是通过设计公平的偏见缓解技术来学习模型。近十年来已经发展了大量的缓解技术，然而，在什么情况下哪些方法起作用仍然知之甚少。最近，Wick等人在合成数据上的实验表明，存在一些情况，其中偏见缓解技术导致在无偏数据上测量时更精确的模型。然而，在缺乏彻底的数学分析的情况下，仍不清楚在何种情况下哪些技术是有效的。我们提出通过建立偏见类型与缓解技术有效性之间的关系来解决这个问题，我们将缓解技术按",
    "tldr": "研究探讨数据偏见如何影响模型公平性，提出了建立偏见类型与缓解技术有效性之间关系的方法",
    "en_tdlr": "The study investigates how biased data affects model fairness and proposes a method to establish the relationship between types of bias and the effectiveness of mitigation techniques."
}