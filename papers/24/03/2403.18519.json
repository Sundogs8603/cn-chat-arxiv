{
    "title": "Improving Line Search Methods for Large Scale Neural Network Training",
    "abstract": "arXiv:2403.18519v1 Announce Type: cross  Abstract: In recent studies, line search methods have shown significant improvements in the performance of traditional stochastic gradient descent techniques, eliminating the need for a specific learning rate schedule. In this paper, we identify existing issues in state-of-the-art line search methods, propose enhancements, and rigorously evaluate their effectiveness. We test these methods on larger datasets and more complex data domains than before. Specifically, we improve the Armijo line search by integrating the momentum term from ADAM in its search direction, enabling efficient large-scale training, a task that was previously prone to failure using Armijo line search methods. Our optimization approach outperforms both the previous Armijo implementation and tuned learning rate schedules for Adam. Our evaluation focuses on Transformers and CNNs in the domains of NLP and image data. Our work is publicly available as a Python package, which prov",
    "link": "https://arxiv.org/abs/2403.18519",
    "context": "Title: Improving Line Search Methods for Large Scale Neural Network Training\nAbstract: arXiv:2403.18519v1 Announce Type: cross  Abstract: In recent studies, line search methods have shown significant improvements in the performance of traditional stochastic gradient descent techniques, eliminating the need for a specific learning rate schedule. In this paper, we identify existing issues in state-of-the-art line search methods, propose enhancements, and rigorously evaluate their effectiveness. We test these methods on larger datasets and more complex data domains than before. Specifically, we improve the Armijo line search by integrating the momentum term from ADAM in its search direction, enabling efficient large-scale training, a task that was previously prone to failure using Armijo line search methods. Our optimization approach outperforms both the previous Armijo implementation and tuned learning rate schedules for Adam. Our evaluation focuses on Transformers and CNNs in the domains of NLP and image data. Our work is publicly available as a Python package, which prov",
    "path": "papers/24/03/2403.18519.json",
    "total_tokens": 784,
    "translated_title": "改进大规模神经网络训练的线搜索方法",
    "translated_abstract": "在最近的研究中，线搜索方法在传统随机梯度下降技术的性能方面取得了显著进展，消除了需要特定学习率调度的需求。本文识别了现有最先进线搜索方法中存在的问题，提出了增强措施，并对其效果进行了严格评估。我们在比以往更大的数据集和更复杂的数据领域上测试了这些方法。具体来说，我们通过将ADAM的动量项集成到Armijo线搜索中的搜索方向中，改进了Armijo线搜索，实现了高效的大规模训练，这是以前使用Armijo线搜索方法容易失败的任务。我们的优化方法胜过以前的Armijo实现和Adam的调整学习率调度。我们的评估重点放在NLP和图像数据领域的Transformer和CNN上。我们的工作以Python包的形式公开发布，可以下载使用。",
    "tldr": "本文改进了大规模神经网络训练的线搜索方法，通过将ADAM的动量项集成到Armijo线搜索中，实现了高效的大规模训练，并且优于以往的方法和Adam的调整学习率。"
}