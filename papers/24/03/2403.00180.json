{
    "title": "\"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models",
    "abstract": "arXiv:2403.00180v1 Announce Type: new  Abstract: Model editing has emerged as a cost-effective strategy to update knowledge stored in language models. However, model editing can have unintended consequences after edits are applied: information unrelated to the edits can also be changed, and other general behaviors of the model can be wrongly altered. In this work, we investigate how model editing methods unexpectedly amplify model biases post-edit. We introduce a novel benchmark dataset, Seesaw-CF, for measuring bias-related harms of model editing and conduct the first in-depth investigation of how different weight-editing methods impact model bias. Specifically, we focus on biases with respect to demographic attributes such as race, geographic origin, and gender, as well as qualitative flaws in long-form texts generated by edited language models. We find that edited models exhibit, to various degrees, more biased behavior as they become less confident in attributes for Asian, African,",
    "link": "https://arxiv.org/abs/2403.00180",
    "context": "Title: \"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models\nAbstract: arXiv:2403.00180v1 Announce Type: new  Abstract: Model editing has emerged as a cost-effective strategy to update knowledge stored in language models. However, model editing can have unintended consequences after edits are applied: information unrelated to the edits can also be changed, and other general behaviors of the model can be wrongly altered. In this work, we investigate how model editing methods unexpectedly amplify model biases post-edit. We introduce a novel benchmark dataset, Seesaw-CF, for measuring bias-related harms of model editing and conduct the first in-depth investigation of how different weight-editing methods impact model bias. Specifically, we focus on biases with respect to demographic attributes such as race, geographic origin, and gender, as well as qualitative flaws in long-form texts generated by edited language models. We find that edited models exhibit, to various degrees, more biased behavior as they become less confident in attributes for Asian, African,",
    "path": "papers/24/03/2403.00180.json",
    "total_tokens": 858,
    "translated_title": "\"Flex Tape不能修复这个\": 编辑语言模型中的偏见和错误信息",
    "translated_abstract": "模型编辑已经成为更新存储在语言模型中的知识的一种具有成本效益的策略。然而，在编辑应用后，模型编辑可能会产生意想不到的后果：与编辑无关的信息也可能被更改，并且模型的其他一般行为可能被错误地改变。在这项工作中，我们调查了模型编辑方法如何意外地加剧了模型后编辑的偏见。我们引入了一个新的基准数据集Seesaw-CF，用于衡量模型编辑的偏见相关伤害，并进行了首次深入研究不同权重编辑方法如何影响模型偏见。具体而言，我们专注于与种族、地理来源和性别等人口属性相关的偏见，以及由编辑语言模型生成的长文本中的定性缺陷。我们发现，编辑模型在变得对亚洲、非洲等属性的属性不确定度愈高时表现出不同程度的更为偏见行为。",
    "tldr": "该研究调查了编辑语言模型中偏见放大的问题，引入了一个新的基准数据集Seesaw-CF，首次深入研究了权重编辑方法对模型偏见的影响。"
}