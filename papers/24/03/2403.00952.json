{
    "title": "MediSwift: Efficient Sparse Pre-trained Biomedical Language Models",
    "abstract": "arXiv:2403.00952v1 Announce Type: new  Abstract: Large language models (LLMs) are typically trained on general source data for various domains, but a recent surge in domain-specific LLMs has shown their potential to outperform general-purpose models in domain-specific tasks (e.g., biomedicine). Although domain-specific pre-training enhances efficiency and leads to smaller models, the computational costs of training these LLMs remain high, posing budgeting challenges. We introduce MediSwift, a suite of biomedical LMs that leverage sparse pre-training on domain-specific biomedical text data. By inducing up to 75% weight sparsity during the pre-training phase, MediSwift achieves a 2-2.5x reduction in training FLOPs. Notably, all sparse pre-training was performed on the Cerebras CS-2 system, which is specifically designed to realize the acceleration benefits from unstructured weight sparsity, thereby significantly enhancing the efficiency of the MediSwift models. Through subsequent dense f",
    "link": "https://arxiv.org/abs/2403.00952",
    "context": "Title: MediSwift: Efficient Sparse Pre-trained Biomedical Language Models\nAbstract: arXiv:2403.00952v1 Announce Type: new  Abstract: Large language models (LLMs) are typically trained on general source data for various domains, but a recent surge in domain-specific LLMs has shown their potential to outperform general-purpose models in domain-specific tasks (e.g., biomedicine). Although domain-specific pre-training enhances efficiency and leads to smaller models, the computational costs of training these LLMs remain high, posing budgeting challenges. We introduce MediSwift, a suite of biomedical LMs that leverage sparse pre-training on domain-specific biomedical text data. By inducing up to 75% weight sparsity during the pre-training phase, MediSwift achieves a 2-2.5x reduction in training FLOPs. Notably, all sparse pre-training was performed on the Cerebras CS-2 system, which is specifically designed to realize the acceleration benefits from unstructured weight sparsity, thereby significantly enhancing the efficiency of the MediSwift models. Through subsequent dense f",
    "path": "papers/24/03/2403.00952.json",
    "total_tokens": 914,
    "translated_title": "MediSwift：高效稀疏预训练生物医学语言模型",
    "translated_abstract": "大型语言模型（LLMs）通常在通用源数据上进行训练，用于各种领域，但最近领域特定的LLMs激增表明它们在领域特定任务（例如生物医学）中的潜力超过了通用型模型。虽然领域特定的预训练提高了效率并导致模型更小，但这些LLMs的训练计算成本仍然很高，构成了预算挑战。我们引入了MediSwift，一套利用领域特定生物医学文本数据上的稀疏预训练的生物医学LM。通过在预训练阶段引入高达75％的权重稀疏性，MediSwift在训练FLOPs方面实现了2-2.5倍的减少。值得注意的是，所有的稀疏预训练均在专门设计用于实现来自非结构化权重稀疏性的加速好处的Cerebras CS-2系统上进行，从而显着提高了MediSwift模型的效率。",
    "tldr": "MediSwift在生物医学领域引入了高效稀疏预训练模型，通过75%的权重稀疏性实现了2-2.5倍的训练FLOPs减少，从而显著提高了效率。",
    "en_tdlr": "MediSwift introduces efficient sparse pre-trained models in the biomedical domain, achieving a 2-2.5x reduction in training FLOPs through 75% weight sparsity, significantly enhancing efficiency."
}