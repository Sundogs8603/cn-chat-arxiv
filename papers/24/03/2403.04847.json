{
    "title": "Solving Inverse Problems with Model Mismatch using Untrained Neural Networks within Model-based Architectures",
    "abstract": "arXiv:2403.04847v1 Announce Type: new  Abstract: Model-based deep learning methods such as \\emph{loop unrolling} (LU) and \\emph{deep equilibrium model} (DEQ) extensions offer outstanding performance in solving inverse problems (IP). These methods unroll the optimization iterations into a sequence of neural networks that in effect learn a regularization function from data. While these architectures are currently state-of-the-art in numerous applications, their success heavily relies on the accuracy of the forward model. This assumption can be limiting in many physical applications due to model simplifications or uncertainties in the apparatus. To address forward model mismatch, we introduce an untrained forward model residual block within the model-based architecture to match the data consistency in the measurement domain for each instance. We propose two variants in well-known model-based architectures (LU and DEQ) and prove convergence under mild conditions. The experiments show signi",
    "link": "https://arxiv.org/abs/2403.04847",
    "context": "Title: Solving Inverse Problems with Model Mismatch using Untrained Neural Networks within Model-based Architectures\nAbstract: arXiv:2403.04847v1 Announce Type: new  Abstract: Model-based deep learning methods such as \\emph{loop unrolling} (LU) and \\emph{deep equilibrium model} (DEQ) extensions offer outstanding performance in solving inverse problems (IP). These methods unroll the optimization iterations into a sequence of neural networks that in effect learn a regularization function from data. While these architectures are currently state-of-the-art in numerous applications, their success heavily relies on the accuracy of the forward model. This assumption can be limiting in many physical applications due to model simplifications or uncertainties in the apparatus. To address forward model mismatch, we introduce an untrained forward model residual block within the model-based architecture to match the data consistency in the measurement domain for each instance. We propose two variants in well-known model-based architectures (LU and DEQ) and prove convergence under mild conditions. The experiments show signi",
    "path": "papers/24/03/2403.04847.json",
    "total_tokens": 852,
    "translated_title": "使用未训练的神经网络在基于模型的架构中解决模型不匹配的反问题",
    "translated_abstract": "基于模型的深度学习方法，如“展开迭代”（LU）和“深度平衡模型”（DEQ）扩展，在解决反问题（IP）方面表现出色。这些方法将优化迭代展开为一系列神经网络，实际上从数据中学习正则化函数。尽管这些架构目前在许多应用中处于最前沿，但它们的成功在很大程度上取决于正向模型的准确性。这一假设可能在许多物理应用中受限于模型简化或仪器不确定性。为解决正向模型不匹配问题，我们在基于模型的架构中引入了一个未经训练的正向模型残差块，以匹配每个实例在测量域中的数据一致性。我们在已知的基于模型的架构（LU和DEQ）中提出了两种变体，并证明在较轻的条件下收敛。实验表明显著",
    "tldr": "提出了一种在基于模型架构中使用未经训练的神经网络来解决模型不匹配的方法，证明了其在一定条件下的收敛性能"
}