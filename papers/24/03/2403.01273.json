{
    "title": "NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free Attention",
    "abstract": "arXiv:2403.01273v1 Announce Type: cross  Abstract: Large language model inference on Central Processing Units (CPU) is challenging due to the vast quantities of expensive Multiply-Add (MAD) matrix operations in the attention computations. In this paper, we argue that there is a rare gem in modern CPUs, Single-Instruction-Multiple-Data (SIMD) registers, which allow for ultra-low-latency lookups in batch. We leverage this unique capability of CPUs to propose NoMAD-Attention, an efficient attention algorithm that replaces MAD operations with in-register lookups. Through hardware-aware algorithmic designs, NoMAD-Attention achieves the computation of attention scores using repeated fast accesses to SIMD registers despite their highly limited sizes. Moreover, NoMAD-Attention works with pre-trained attention-based LLMs without model finetuning. Empirical evaluations demonstrate that NoMAD-Attention maintains the quality of the original LLMs well, and speeds up the 4-bit quantized LLaMA-7B-bas",
    "link": "https://arxiv.org/abs/2403.01273",
    "context": "Title: NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free Attention\nAbstract: arXiv:2403.01273v1 Announce Type: cross  Abstract: Large language model inference on Central Processing Units (CPU) is challenging due to the vast quantities of expensive Multiply-Add (MAD) matrix operations in the attention computations. In this paper, we argue that there is a rare gem in modern CPUs, Single-Instruction-Multiple-Data (SIMD) registers, which allow for ultra-low-latency lookups in batch. We leverage this unique capability of CPUs to propose NoMAD-Attention, an efficient attention algorithm that replaces MAD operations with in-register lookups. Through hardware-aware algorithmic designs, NoMAD-Attention achieves the computation of attention scores using repeated fast accesses to SIMD registers despite their highly limited sizes. Moreover, NoMAD-Attention works with pre-trained attention-based LLMs without model finetuning. Empirical evaluations demonstrate that NoMAD-Attention maintains the quality of the original LLMs well, and speeds up the 4-bit quantized LLaMA-7B-bas",
    "path": "papers/24/03/2403.01273.json",
    "total_tokens": 865,
    "translated_title": "NoMAD-Attention: 通过无MAD操作实现CPU上高效LLM推断",
    "translated_abstract": "在中央处理单元（CPU）上进行大型语言模型推断具有挑战性，因为注意力计算中存在大量昂贵的MAD矩阵操作。本文认为现代CPU中的单指令多数据（SIMD）寄存器是一种珍贵的宝石，它允许在批处理中进行超低延迟查找。我们利用CPU的这一独特能力提出了NoMAD-Attention，这是一种高效的注意力算法，用于将MAD操作替换为寄存器内查找。通过硬件感知的算法设计，NoMAD-Attention实现了通过重复快速访问SIMD寄存器来计算注意力分数，尽管它们的大小非常有限。此外，NoMAD-Attention适用于预训练的基于注意力的LLM，无需对模型进行微调。实证评估表明，NoMAD-Attention很好地保持了原始LLM的质量，并加速了4位量化的LLaMA-7B-bas。",
    "tldr": "NoMAD-Attention提出了一种高效的注意力算法，通过在CPU上使用寄存器内查找取代MAD操作，以实现LLM推断的快速计算。",
    "en_tdlr": "NoMAD-Attention proposes an efficient attention algorithm that achieves fast computation for LLM inference by replacing MAD operations with in-register lookups on CPUs."
}