{
    "title": "IBCB: Efficient Inverse Batched Contextual Bandit for Behavioral Evolution History",
    "abstract": "arXiv:2403.16075v1 Announce Type: new  Abstract: Traditional imitation learning focuses on modeling the behavioral mechanisms of experts, which requires a large amount of interaction history generated by some fixed expert. However, in many streaming applications, such as streaming recommender systems, online decision-makers typically engage in online learning during the decision-making process, meaning that the interaction history generated by online decision-makers includes their behavioral evolution from novice expert to experienced expert. This poses a new challenge for existing imitation learning approaches that can only utilize data from experienced experts. To address this issue, this paper proposes an inverse batched contextual bandit (IBCB) framework that can efficiently perform estimations of environment reward parameters and learned policy based on the expert's behavioral evolution history. Specifically, IBCB formulates the inverse problem into a simple quadratic programming ",
    "link": "https://arxiv.org/abs/2403.16075",
    "context": "Title: IBCB: Efficient Inverse Batched Contextual Bandit for Behavioral Evolution History\nAbstract: arXiv:2403.16075v1 Announce Type: new  Abstract: Traditional imitation learning focuses on modeling the behavioral mechanisms of experts, which requires a large amount of interaction history generated by some fixed expert. However, in many streaming applications, such as streaming recommender systems, online decision-makers typically engage in online learning during the decision-making process, meaning that the interaction history generated by online decision-makers includes their behavioral evolution from novice expert to experienced expert. This poses a new challenge for existing imitation learning approaches that can only utilize data from experienced experts. To address this issue, this paper proposes an inverse batched contextual bandit (IBCB) framework that can efficiently perform estimations of environment reward parameters and learned policy based on the expert's behavioral evolution history. Specifically, IBCB formulates the inverse problem into a simple quadratic programming ",
    "path": "papers/24/03/2403.16075.json",
    "total_tokens": 816,
    "translated_title": "IBCB: 高效的逆批处理上下文强化学习用于行为演变历史",
    "translated_abstract": "传统的模仿学习关注专家的行为机制建模，需要大量由某个固定专家生成的交互历史。然而，在许多流式应用中，如流式推荐系统，在线决策者通常在决策过程中进行在线学习，这意味着在线决策者生成的交互历史包括他们从新手专家到有经验专家的行为演变。这给现有的只能利用有经验专家数据的模仿学习方法带来了新挑战。为了解决这个问题，本文提出了一种逆批处理上下文强化学习（IBCB）框架，能够高效地进行基于专家行为演变历史的环境奖励参数和学习策略的估计。具体来说，IBCB将逆问题形式化为简单的二次规划。",
    "tldr": "提出了一种逆批处理上下文强化学习（IBCB）框架，可以高效地根据专家的行为演变历史对环境奖励参数和学习策略进行估计。",
    "en_tdlr": "Proposed an inverse batched contextual bandit (IBCB) framework that efficiently estimates environment reward parameters and learned policy based on expert's behavioral evolution history."
}