{
    "title": "The Value of Reward Lookahead in Reinforcement Learning",
    "abstract": "arXiv:2403.11637v1 Announce Type: new  Abstract: In reinforcement learning (RL), agents sequentially interact with changing environments while aiming to maximize the obtained rewards. Usually, rewards are observed only after acting, and so the goal is to maximize the expected cumulative reward. Yet, in many practical settings, reward information is observed in advance -- prices are observed before performing transactions; nearby traffic information is partially known; and goals are oftentimes given to agents prior to the interaction. In this work, we aim to quantifiably analyze the value of such future reward information through the lens of competitive analysis. In particular, we measure the ratio between the value of standard RL agents and that of agents with partial future-reward lookahead. We characterize the worst-case reward distribution and derive exact ratios for the worst-case reward expectations. Surprisingly, the resulting ratios relate to known quantities in offline RL and r",
    "link": "https://arxiv.org/abs/2403.11637",
    "context": "Title: The Value of Reward Lookahead in Reinforcement Learning\nAbstract: arXiv:2403.11637v1 Announce Type: new  Abstract: In reinforcement learning (RL), agents sequentially interact with changing environments while aiming to maximize the obtained rewards. Usually, rewards are observed only after acting, and so the goal is to maximize the expected cumulative reward. Yet, in many practical settings, reward information is observed in advance -- prices are observed before performing transactions; nearby traffic information is partially known; and goals are oftentimes given to agents prior to the interaction. In this work, we aim to quantifiably analyze the value of such future reward information through the lens of competitive analysis. In particular, we measure the ratio between the value of standard RL agents and that of agents with partial future-reward lookahead. We characterize the worst-case reward distribution and derive exact ratios for the worst-case reward expectations. Surprisingly, the resulting ratios relate to known quantities in offline RL and r",
    "path": "papers/24/03/2403.11637.json",
    "total_tokens": 838,
    "translated_title": "强化学习中未来奖励先知的价值",
    "translated_abstract": "在强化学习（RL）中，代理们与不断变化的环境进行顺序交互，旨在最大化获得的奖励。通常情况下，奖励仅在行动后被观察到，因此目标是最大化预期累积奖励。然而，在许多实际场景中，奖励信息是提前观察到的 -- 交易前观察到价格；了解部分附近交通信息；经常在互动之前为代理分配目标。在这项工作中，我们旨在通过竞争性分析的视角，定量分析这种未来奖励信息的价值。特别地，我们测量了标准RL代理的价值与具有部分未来奖励先知的代理之间的比率。我们刻画了最坏情况下的奖励分布，并推导出最坏情况下奖励期望的精确比率。令人惊讶的是，结果比率与离线RL和r中已知的数量有关。",
    "tldr": "分析了在强化学习中利用部分未来奖励先知的价值，通过竞争性分析得出了最坏情况下奖励期望的精确比率。"
}