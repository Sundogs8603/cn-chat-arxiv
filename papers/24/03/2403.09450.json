{
    "title": "Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative Privacy Risk",
    "abstract": "arXiv:2403.09450v1 Announce Type: new  Abstract: While diffusion models have recently demonstrated remarkable progress in generating realistic images, privacy risks also arise: published models or APIs could generate training images and thus leak privacy-sensitive training information. In this paper, we reveal a new risk, Shake-to-Leak (S2L), that fine-tuning the pre-trained models with manipulated data can amplify the existing privacy risks. We demonstrate that S2L could occur in various standard fine-tuning strategies for diffusion models, including concept-injection methods (DreamBooth and Textual Inversion) and parameter-efficient methods (LoRA and Hypernetwork), as well as their combinations. In the worst case, S2L can amplify the state-of-the-art membership inference attack (MIA) on diffusion models by $5.4\\%$ (absolute difference) AUC and can increase extracted private samples from almost $0$ samples to $16.3$ samples on average per target domain. This discovery underscores that",
    "link": "https://arxiv.org/abs/2403.09450",
    "context": "Title: Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative Privacy Risk\nAbstract: arXiv:2403.09450v1 Announce Type: new  Abstract: While diffusion models have recently demonstrated remarkable progress in generating realistic images, privacy risks also arise: published models or APIs could generate training images and thus leak privacy-sensitive training information. In this paper, we reveal a new risk, Shake-to-Leak (S2L), that fine-tuning the pre-trained models with manipulated data can amplify the existing privacy risks. We demonstrate that S2L could occur in various standard fine-tuning strategies for diffusion models, including concept-injection methods (DreamBooth and Textual Inversion) and parameter-efficient methods (LoRA and Hypernetwork), as well as their combinations. In the worst case, S2L can amplify the state-of-the-art membership inference attack (MIA) on diffusion models by $5.4\\%$ (absolute difference) AUC and can increase extracted private samples from almost $0$ samples to $16.3$ samples on average per target domain. This discovery underscores that",
    "path": "papers/24/03/2403.09450.json",
    "total_tokens": 937,
    "translated_title": "震动泄密：微调扩散模型可能增加生成隐私风险",
    "translated_abstract": "虽然扩散模型最近在生成逼真图像方面取得了显著进展，但也存在隐私风险：已发布的模型或API可能生成训练图像，从而泄露涉及隐私的训练信息。本文揭示了一种新风险，即Shake-to-Leak (S2L)，即使用操纵数据微调预训练模型可能会增加现有的隐私风险。我们证明了S2L可能发生在各种标准的扩散模型微调策略中，包括概念注入方法（DreamBooth和Textual Inversion）和参数高效方法（LoRA和Hypernetwork），以及它们的组合。在最糟糕的情况下，S2L可以将扩散模型上的最新成员推理攻击（MIA）的AUC提高5.4％（绝对差异），并且可以将每个目标域的提取私有样本从几乎0个样本增加到平均16.3个样本。这一发现强调了",
    "tldr": "微调扩散模型可能会增加生成隐私风险，甚至使最先进的成员推理攻击对扩散模型的攻击效果提高5.4％，并可将提取的私有样本数量从几乎0个增加到平均16.3个。",
    "en_tdlr": "Fine-tuning diffusion models can increase generative privacy risk, amplifying state-of-the-art membership inference attacks by 5.4% and increasing the extracted private samples from nearly 0 to an average of 16.3 per target domain."
}