{
    "title": "Towards a theory of model distillation",
    "abstract": "arXiv:2403.09053v1 Announce Type: cross  Abstract: Distillation is the task of replacing a complicated machine learning model with a simpler model that approximates the original [BCNM06,HVD15]. Despite many practical applications, basic questions about the extent to which models can be distilled, and the runtime and amount of data needed to distill, remain largely open.   To study these questions, we initiate a general theory of distillation, defining PAC-distillation in an analogous way to PAC-learning [Val84]. As applications of this theory: (1) we propose new algorithms to extract the knowledge stored in the trained weights of neural networks -- we show how to efficiently distill neural networks into succinct, explicit decision tree representations when possible by using the ``linear representation hypothesis''; and (2) we prove that distillation can be much cheaper than learning from scratch, and make progress on characterizing its complexity.",
    "link": "https://arxiv.org/abs/2403.09053",
    "context": "Title: Towards a theory of model distillation\nAbstract: arXiv:2403.09053v1 Announce Type: cross  Abstract: Distillation is the task of replacing a complicated machine learning model with a simpler model that approximates the original [BCNM06,HVD15]. Despite many practical applications, basic questions about the extent to which models can be distilled, and the runtime and amount of data needed to distill, remain largely open.   To study these questions, we initiate a general theory of distillation, defining PAC-distillation in an analogous way to PAC-learning [Val84]. As applications of this theory: (1) we propose new algorithms to extract the knowledge stored in the trained weights of neural networks -- we show how to efficiently distill neural networks into succinct, explicit decision tree representations when possible by using the ``linear representation hypothesis''; and (2) we prove that distillation can be much cheaper than learning from scratch, and make progress on characterizing its complexity.",
    "path": "papers/24/03/2403.09053.json",
    "total_tokens": 811,
    "translated_title": "走向模型蒸馏理论",
    "translated_abstract": "蒸馏是将复杂的机器学习模型替换为简化模型来近似原模型的任务。尽管有许多实际应用，关于模型蒸馏的程度、所需运行时间和数据量的基本问题仍然大多未解。为了研究这些问题，我们开始了蒸馏的一般理论，以类似的方式定义了PAC-蒸馏 [Val84]，提出了提取训练权重中存储的知识的新算法，展示了如何通过使用“线性表示假设”将神经网络高效地蒸馏成简明明了的决策树表示，还证明了蒸馏可以比从头开始学习便宜得多，并在表征其复杂性方面取得了进展。",
    "tldr": "提出了模型蒸馏的一般理论，通过PAC-蒸馏定义，提出了抽取神经网络训练权重知识的新算法，并证明了蒸馏比从头学习更便宜且有助于理解其复杂性。",
    "en_tdlr": "Proposed a general theory of model distillation, defined PAC-distillation, introduced new algorithms for extracting knowledge stored in neural network weights, demonstrated that distillation can be cheaper than learning from scratch and made progress in characterizing its complexity."
}