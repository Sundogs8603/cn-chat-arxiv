{
    "title": "On Optimizing Hyperparameters for Quantum Neural Networks",
    "abstract": "arXiv:2403.18579v1 Announce Type: new  Abstract: The increasing capabilities of Machine Learning (ML) models go hand in hand with an immense amount of data and computational power required for training. Therefore, training is usually outsourced into HPC facilities, where we have started to experience limits in scaling conventional HPC hardware, as theorized by Moore's law. Despite heavy parallelization and optimization efforts, current state-of-the-art ML models require weeks for training, which is associated with an enormous $CO_2$ footprint. Quantum Computing, and specifically Quantum Machine Learning (QML), can offer significant theoretical speed-ups and enhanced expressive power. However, training QML models requires tuning various hyperparameters, which is a nontrivial task and suboptimal choices can highly affect the trainability and performance of the models. In this study, we identify the most impactful hyperparameters and collect data about the performance of QML models. We co",
    "link": "https://arxiv.org/abs/2403.18579",
    "context": "Title: On Optimizing Hyperparameters for Quantum Neural Networks\nAbstract: arXiv:2403.18579v1 Announce Type: new  Abstract: The increasing capabilities of Machine Learning (ML) models go hand in hand with an immense amount of data and computational power required for training. Therefore, training is usually outsourced into HPC facilities, where we have started to experience limits in scaling conventional HPC hardware, as theorized by Moore's law. Despite heavy parallelization and optimization efforts, current state-of-the-art ML models require weeks for training, which is associated with an enormous $CO_2$ footprint. Quantum Computing, and specifically Quantum Machine Learning (QML), can offer significant theoretical speed-ups and enhanced expressive power. However, training QML models requires tuning various hyperparameters, which is a nontrivial task and suboptimal choices can highly affect the trainability and performance of the models. In this study, we identify the most impactful hyperparameters and collect data about the performance of QML models. We co",
    "path": "papers/24/03/2403.18579.json",
    "total_tokens": 811,
    "translated_title": "关于量子神经网络超参数优化的研究",
    "translated_abstract": "机器学习模型日益增强的能力与训练所需的海量数据和计算能力密不可分。因此，训练通常被外包到高性能计算设施中，我们在那里开始经历到传统高性能计算硬件扩展的限制，正如摩尔定律所预测的那样。尽管进行了大量的并行化和优化工作，当前最先进的机器学习模型需要数周的训练时间，这与巨大的 $CO_2$ 排放有关。量子计算，特别是量子机器学习（QML），可以提供显著的理论加速和增强的表现力。然而，训练 QML 模型需要调整各种超参数，这是一项非常困难的任务，次优选择可能会极大影响模型的可训练性和性能。在这项研究中，我们确定了最具影响力的超参数，并收集了关于 QML 模型性能的数据。",
    "tldr": "本研究确定了对量子机器学习模型性能影响最大的超参数，并收集了相关数据。",
    "en_tdlr": "This study identifies the most impactful hyperparameters for Quantum Machine Learning models and collects data on their performance."
}