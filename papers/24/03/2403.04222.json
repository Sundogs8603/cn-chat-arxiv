{
    "title": "Self-Evaluation of Large Language Model based on Glass-box Features",
    "abstract": "arXiv:2403.04222v1 Announce Type: new  Abstract: The proliferation of open-source Large Language Models (LLMs) underscores the pressing need for evaluation methods. Existing works primarily rely on external evaluators, focusing on training and prompting strategies. However, a crucial aspect - model-aware glass-box features - is overlooked. In this study, we explore the utility of glass-box features under the scenario of self-evaluation, namely applying an LLM to evaluate its own output. We investigate various glass-box feature groups and discovered that the softmax distribution serves as a reliable indicator for quality evaluation. Furthermore, we propose two strategies to enhance the evaluation by incorporating features derived from references. Experimental results on public benchmarks validate the feasibility of self-evaluation of LLMs using glass-box features.",
    "link": "https://arxiv.org/abs/2403.04222",
    "context": "Title: Self-Evaluation of Large Language Model based on Glass-box Features\nAbstract: arXiv:2403.04222v1 Announce Type: new  Abstract: The proliferation of open-source Large Language Models (LLMs) underscores the pressing need for evaluation methods. Existing works primarily rely on external evaluators, focusing on training and prompting strategies. However, a crucial aspect - model-aware glass-box features - is overlooked. In this study, we explore the utility of glass-box features under the scenario of self-evaluation, namely applying an LLM to evaluate its own output. We investigate various glass-box feature groups and discovered that the softmax distribution serves as a reliable indicator for quality evaluation. Furthermore, we propose two strategies to enhance the evaluation by incorporating features derived from references. Experimental results on public benchmarks validate the feasibility of self-evaluation of LLMs using glass-box features.",
    "path": "papers/24/03/2403.04222.json",
    "total_tokens": 892,
    "translated_title": "基于玻璃箱特征的大型语言模型的自我评估",
    "translated_abstract": "arXiv:2403.04222v1 公告类型：新摘要：开源大型语言模型（LLMs）的蓬勃发展凸显了对评估方法的迫切需求。现有作品主要依赖于外部评估者，侧重于训练和提示策略。然而，一个关键方面——模型感知的玻璃箱特征——被忽视了。在这项研究中，我们探讨了在自我评估情境下使用玻璃箱特征的效用，即应用LLM评估其自身输出。我们研究了各种玻璃箱特征组，并发现softmax分布作为质量评估的可靠指标。此外，我们提出了通过合并从参考文献中提取的特征来增强评估的两种策略。在公共基准测试上的实验结果验证了使用玻璃箱特征进行LLMs的自我评估的可行性。",
    "tldr": "研究探讨了大型语言模型在自我评估中利用玻璃箱特征的实用性，发现softmax分布在质量评估中可靠，提出了通过引入参考特征增强评估的策略，并验证了使用玻璃箱特征进行大型语言模型自我评估的可行性。",
    "en_tdlr": "The study explores the utility of glass-box features in self-evaluation of large language models, finding that the softmax distribution is a reliable indicator for quality evaluation, proposing strategies to enhance evaluation by incorporating reference features, and validating the feasibility of self-evaluation of large language models using glass-box features."
}