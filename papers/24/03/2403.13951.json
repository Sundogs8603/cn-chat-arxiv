{
    "title": "ACDG-VTON: Accurate and Contained Diffusion Generation for Virtual Try-On",
    "abstract": "arXiv:2403.13951v1 Announce Type: cross  Abstract: Virtual Try-on (VTON) involves generating images of a person wearing selected garments. Diffusion-based methods, in particular, can create high-quality images, but they struggle to maintain the identities of the input garments. We identified this problem stems from the specifics in the training formulation for diffusion. To address this, we propose a unique training scheme that limits the scope in which diffusion is trained. We use a control image that perfectly aligns with the target image during training. In turn, this accurately preserves garment details during inference. We demonstrate our method not only effectively conserves garment details but also allows for layering, styling, and shoe try-on. Our method runs multi-garment try-on in a single inference cycle and can support high-quality zoomed-in generations without training in higher resolutions. Finally, we show our method surpasses prior methods in accuracy and quality.",
    "link": "https://arxiv.org/abs/2403.13951",
    "context": "Title: ACDG-VTON: Accurate and Contained Diffusion Generation for Virtual Try-On\nAbstract: arXiv:2403.13951v1 Announce Type: cross  Abstract: Virtual Try-on (VTON) involves generating images of a person wearing selected garments. Diffusion-based methods, in particular, can create high-quality images, but they struggle to maintain the identities of the input garments. We identified this problem stems from the specifics in the training formulation for diffusion. To address this, we propose a unique training scheme that limits the scope in which diffusion is trained. We use a control image that perfectly aligns with the target image during training. In turn, this accurately preserves garment details during inference. We demonstrate our method not only effectively conserves garment details but also allows for layering, styling, and shoe try-on. Our method runs multi-garment try-on in a single inference cycle and can support high-quality zoomed-in generations without training in higher resolutions. Finally, we show our method surpasses prior methods in accuracy and quality.",
    "path": "papers/24/03/2403.13951.json",
    "total_tokens": 953,
    "translated_title": "ACDG-VTON: 准确且受限的扩散生成用于虚拟试穿",
    "translated_abstract": "虚拟试穿（VTON）涉及生成穿着选定服装的人物图像。基于扩散的方法尤其能够生成高质量图像，但往往难以保持输入服装的身份。我们发现这一问题源于扩散训练公式中的细节。为了解决这一问题，我们提出了一种独特的训练方案，限制了扩散训练的范围。我们在训练过程中使用一个与目标图像完全对齐的控制图像。因此，在推理过程中能够准确地保留服装细节。我们展示了我们的方法不仅有效地保留了服装细节，还能够进行分层、风格和鞋子试穿。我们的方法在单个推理周期内运行多服装试穿，并且能够支持高质量的放大生成，而无需在更高分辨率下进行训练。最后，我们展示了我们的方法在准确性和质量方面超越了先前的方法。",
    "tldr": "提出了一种独特的训练方案，限制了扩散训练的范围，有效保留了服装细节，并实现了多服装试穿、分层、风格和鞋子试穿，无需在更高分辨率下进行训练。最终，方法在准确性和质量方面超越了之前的方法。",
    "en_tdlr": "Proposing a unique training scheme that limits the scope of diffusion training, effectively preserving garment details, enabling multi-garment try-on, layering, styling, and shoe try-on without the need for training in higher resolutions, ultimately surpassing prior methods in accuracy and quality."
}