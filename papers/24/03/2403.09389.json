{
    "title": "Learning to optimize with convergence guarantees using nonlinear system theory",
    "abstract": "arXiv:2403.09389v1 Announce Type: cross  Abstract: The increasing reliance on numerical methods for controlling dynamical systems and training machine learning models underscores the need to devise algorithms that dependably and efficiently navigate complex optimization landscapes. Classical gradient descent methods offer strong theoretical guarantees for convex problems; however, they demand meticulous hyperparameter tuning for non-convex ones. The emerging paradigm of learning to optimize (L2O) automates the discovery of algorithms with optimized performance leveraging learning models and data - yet, it lacks a theoretical framework to analyze convergence and robustness of the learned algorithms. In this paper, we fill this gap by harnessing nonlinear system theory. Specifically, we propose an unconstrained parametrization of all convergent algorithms for smooth non-convex objective functions. Notably, our framework is directly compatible with automatic differentiation tools, ensurin",
    "link": "https://arxiv.org/abs/2403.09389",
    "context": "Title: Learning to optimize with convergence guarantees using nonlinear system theory\nAbstract: arXiv:2403.09389v1 Announce Type: cross  Abstract: The increasing reliance on numerical methods for controlling dynamical systems and training machine learning models underscores the need to devise algorithms that dependably and efficiently navigate complex optimization landscapes. Classical gradient descent methods offer strong theoretical guarantees for convex problems; however, they demand meticulous hyperparameter tuning for non-convex ones. The emerging paradigm of learning to optimize (L2O) automates the discovery of algorithms with optimized performance leveraging learning models and data - yet, it lacks a theoretical framework to analyze convergence and robustness of the learned algorithms. In this paper, we fill this gap by harnessing nonlinear system theory. Specifically, we propose an unconstrained parametrization of all convergent algorithms for smooth non-convex objective functions. Notably, our framework is directly compatible with automatic differentiation tools, ensurin",
    "path": "papers/24/03/2403.09389.json",
    "total_tokens": 773,
    "translated_title": "使用非线性系统理论学习具有收敛保证的优化",
    "translated_abstract": "越来越多地依赖于数字方法来控制动态系统和训练机器学习模型，突显了需要设计可靠且高效地遍历复杂优化空间的算法。传统的梯度下降方法对凸问题提供了强大的理论保证；然而，对于非凸问题则需要精细调整超参数。学习优化(L2O)的新兴范式自动发现具有优化性能的算法，利用学习模型和数据，但缺乏分析所学算法的收敛性和稳健性的理论框架。本文通过利用非线性系统理论填补了这一空白。具体来说，我们提出了对于平滑非凸目标函数的所有收敛算法的无约束参数化。值得注意的是，我们的框架与自动微分工具直接兼容，确保...",
    "tldr": "基于非线性系统理论，提出了一种对于平滑非凸目标函数的所有收敛算法的无约束参数化方法",
    "en_tdlr": "Proposed an unconstrained parametrization of all convergent algorithms for smooth non-convex objective functions based on nonlinear system theory."
}