{
    "title": "Be Yourself: Bounded Attention for Multi-Subject Text-to-Image Generation",
    "abstract": "arXiv:2403.16990v1 Announce Type: cross  Abstract: Text-to-image diffusion models have an unprecedented ability to generate diverse and high-quality images. However, they often struggle to faithfully capture the intended semantics of complex input prompts that include multiple subjects. Recently, numerous layout-to-image extensions have been introduced to improve user control, aiming to localize subjects represented by specific tokens. Yet, these methods often produce semantically inaccurate images, especially when dealing with multiple semantically or visually similar subjects. In this work, we study and analyze the causes of these limitations. Our exploration reveals that the primary issue stems from inadvertent semantic leakage between subjects in the denoising process. This leakage is attributed to the diffusion model's attention layers, which tend to blend the visual features of different subjects. To address these issues, we introduce Bounded Attention, a training-free method for",
    "link": "https://arxiv.org/abs/2403.16990",
    "context": "Title: Be Yourself: Bounded Attention for Multi-Subject Text-to-Image Generation\nAbstract: arXiv:2403.16990v1 Announce Type: cross  Abstract: Text-to-image diffusion models have an unprecedented ability to generate diverse and high-quality images. However, they often struggle to faithfully capture the intended semantics of complex input prompts that include multiple subjects. Recently, numerous layout-to-image extensions have been introduced to improve user control, aiming to localize subjects represented by specific tokens. Yet, these methods often produce semantically inaccurate images, especially when dealing with multiple semantically or visually similar subjects. In this work, we study and analyze the causes of these limitations. Our exploration reveals that the primary issue stems from inadvertent semantic leakage between subjects in the denoising process. This leakage is attributed to the diffusion model's attention layers, which tend to blend the visual features of different subjects. To address these issues, we introduce Bounded Attention, a training-free method for",
    "path": "papers/24/03/2403.16990.json",
    "total_tokens": 865,
    "translated_title": "做自己：多主体文本到图像生成的有界注意力",
    "translated_abstract": "arXiv:2403.16990v1 公告类型：跨主体文本到图像扩散模型具有生成多样且高质量图像的前所未有能力。然而，它们常常难以忠实地捕捉包含多个主题的复杂输入提示的预期语义。最近，许多布局到图像的扩展已被引入以提高用户控制，旨在定位由特定令牌表示的主题。然而，这些方法通常会产生语义不准确的图像，特别是在处理多个在语义上或视觉上相似的主题时。在这项工作中，我们研究并分析了这些限制的原因。我们的探索揭示了这个主要问题起源于去噪过程中主题之间的无意义语义泄漏。这种泄漏归因于扩散模型的注意力层，这些层倾向于混合不同主体的视觉特征。为了解决这些问题，我们引入了有界注意力，一种无需训练的方法",
    "tldr": "该论文研究了多主体文本到图像生成中的有界注意力方法，解决了扩散模型注意力层混合不同主体视觉特征导致的语义泄漏问题。",
    "en_tdlr": "This paper investigates the bounded attention method in multi-subject text-to-image generation, addressing the issue of semantic leakage caused by diffusion model attention layers mixing visual features of different subjects."
}