{
    "title": "InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions",
    "abstract": "arXiv:2403.11435v1 Announce Type: new  Abstract: Instruction tuning effectively optimizes Large Language Models (LLMs) for downstream tasks. Due to the changing environment in real-life applications, LLMs necessitate continual task-specific adaptation without catastrophic forgetting. Considering the heavy computational cost, replay-based Continual Learning (CL) methods are the simplest and most widely used for LLMs to address the forgetting issue. However, traditional replay-based methods do not fully utilize instructions to customize the replay strategy. In this work, we propose a novel paradigm called Instruction-based Continual Learning (InsCL). InsCL dynamically replays previous data based on task similarity, calculated by Wasserstein Distance with instructions. Moreover, we further introduce an Instruction Information Metric (InsInfo) to quantify the complexity and diversity of instructions. According to InsInfo, InsCL guides the replay process more inclined to high-quality data. ",
    "link": "https://arxiv.org/abs/2403.11435",
    "context": "Title: InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions\nAbstract: arXiv:2403.11435v1 Announce Type: new  Abstract: Instruction tuning effectively optimizes Large Language Models (LLMs) for downstream tasks. Due to the changing environment in real-life applications, LLMs necessitate continual task-specific adaptation without catastrophic forgetting. Considering the heavy computational cost, replay-based Continual Learning (CL) methods are the simplest and most widely used for LLMs to address the forgetting issue. However, traditional replay-based methods do not fully utilize instructions to customize the replay strategy. In this work, we propose a novel paradigm called Instruction-based Continual Learning (InsCL). InsCL dynamically replays previous data based on task similarity, calculated by Wasserstein Distance with instructions. Moreover, we further introduce an Instruction Information Metric (InsInfo) to quantify the complexity and diversity of instructions. According to InsInfo, InsCL guides the replay process more inclined to high-quality data. ",
    "path": "papers/24/03/2403.11435.json",
    "total_tokens": 877,
    "translated_title": "InsCL: 一种用指导信息细调大型语言模型的数据高效持续学习范式",
    "translated_abstract": "由于实际应用中环境的不断变化，大型语言模型(LLMs)需要在不产生灾难性遗忘的情况下持续进行特定任务的适应。由于高昂的计算成本，基于重播的持续学习(CL)方法是解决LLMs遗忘问题的最简单且广泛使用的方式。然而，传统的基于重播的方法并没有充分利用指导信息来定制重播策略。在这项工作中，我们提出了一种名为基于指导信息的持续学习(InsCL)的新范式。InsCL根据任务相似性（通过指导信息的Wasserstein Distance计算）动态重播先前的数据。此外，我们进一步引入了一个指导信息度量标准(InsInfo)来量化指导信息的复杂性和多样性。根据InsInfo，InsCL引导重播过程更倾向于高质量的数据。",
    "tldr": "InsCL提出了一种基于指导信息的新型持续学习范式，通过Wasserstein距离计算任务相似性，并引入InsInfo指标来定量指导信息的复杂性和多样性，从而指导重播过程更倾向于高质量数据。",
    "en_tdlr": "InsCL introduces a novel continual learning paradigm based on instructions, dynamically replaying previous data based on task similarity calculated by Wasserstein Distance and introducing an InsInfo metric to quantify the complexity and diversity of instructions, guiding the replay process towards higher-quality data."
}