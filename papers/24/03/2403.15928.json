{
    "title": "Safe Reinforcement Learning for Constrained Markov Decision Processes with Stochastic Stopping Time",
    "abstract": "arXiv:2403.15928v1 Announce Type: new  Abstract: In this paper, we present an online reinforcement learning algorithm for constrained Markov decision processes with a safety constraint. Despite the necessary attention of the scientific community, considering stochastic stopping time, the problem of learning optimal policy without violating safety constraints during the learning phase is yet to be addressed. To this end, we propose an algorithm based on linear programming that does not require a process model. We show that the learned policy is safe with high confidence. We also propose a method to compute a safe baseline policy, which is central in developing algorithms that do not violate the safety constraints. Finally, we provide simulation results to show the efficacy of the proposed algorithm. Further, we demonstrate that efficient exploration can be achieved by defining a subset of the state-space called proxy set.",
    "link": "https://arxiv.org/abs/2403.15928",
    "context": "Title: Safe Reinforcement Learning for Constrained Markov Decision Processes with Stochastic Stopping Time\nAbstract: arXiv:2403.15928v1 Announce Type: new  Abstract: In this paper, we present an online reinforcement learning algorithm for constrained Markov decision processes with a safety constraint. Despite the necessary attention of the scientific community, considering stochastic stopping time, the problem of learning optimal policy without violating safety constraints during the learning phase is yet to be addressed. To this end, we propose an algorithm based on linear programming that does not require a process model. We show that the learned policy is safe with high confidence. We also propose a method to compute a safe baseline policy, which is central in developing algorithms that do not violate the safety constraints. Finally, we provide simulation results to show the efficacy of the proposed algorithm. Further, we demonstrate that efficient exploration can be achieved by defining a subset of the state-space called proxy set.",
    "path": "papers/24/03/2403.15928.json",
    "total_tokens": 851,
    "translated_title": "带有随机停止时间的约束马尔可夫决策过程的安全强化学习",
    "translated_abstract": "在本文中，我们提出了一种用于带有安全性约束的马尔可夫决策过程的在线强化学习算法。尽管科学界已经引起必要的关注，但考虑到随机停止时间，学习在学习阶段不违反安全性约束的最优策略的问题尚未得到解决。为此，我们提出了一种基于线性规划的算法，不需要一个过程模型。我们展示了学到的策略具有较高的安全性保证。我们还提出了一种计算安全基线策略的方法，这对于开发不违反安全性约束的算法至关重要。最后，我们提供了模拟结果来展示所提出算法的有效性。此外，我们证明了通过定义一个称为代理集的状态空间子集可以实现有效的探索。",
    "tldr": "提出了一种在线强化学习算法，用于带有安全性约束的马尔可夫决策过程，能够学习到安全的最优策略，同时提出了计算安全基线策略的方法，证明了算法的有效性，并展示了通过定义代理集实现有效探索。",
    "en_tdlr": "Proposed an online reinforcement learning algorithm for constrained Markov decision processes with safety constraint, capable of learning safe optimal policy, introduced method to compute safe baseline policy, demonstrated algorithm effectiveness, and showed efficient exploration through defining proxy set."
}