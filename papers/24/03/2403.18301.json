{
    "title": "Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives",
    "abstract": "arXiv:2403.18301v1 Announce Type: cross  Abstract: The rise in internet usage has led to the generation of massive amounts of data, resulting in the adoption of various supervised and semi-supervised machine learning algorithms, which can effectively utilize the colossal amount of data to train models. However, before deploying these models in the real world, these must be strictly evaluated on performance measures like worst-case recall and satisfy constraints such as fairness. We find that current state-of-the-art empirical techniques offer sub-optimal performance on these practical, non-decomposable performance objectives. On the other hand, the theoretical techniques necessitate training a new model from scratch for each performance objective. To bridge the gap, we propose SelMix, a selective mixup-based inexpensive fine-tuning technique for pre-trained models, to optimize for the desired objective. The core idea of our framework is to determine a sampling distribution to perform a",
    "link": "https://arxiv.org/abs/2403.18301",
    "context": "Title: Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives\nAbstract: arXiv:2403.18301v1 Announce Type: cross  Abstract: The rise in internet usage has led to the generation of massive amounts of data, resulting in the adoption of various supervised and semi-supervised machine learning algorithms, which can effectively utilize the colossal amount of data to train models. However, before deploying these models in the real world, these must be strictly evaluated on performance measures like worst-case recall and satisfy constraints such as fairness. We find that current state-of-the-art empirical techniques offer sub-optimal performance on these practical, non-decomposable performance objectives. On the other hand, the theoretical techniques necessitate training a new model from scratch for each performance objective. To bridge the gap, we propose SelMix, a selective mixup-based inexpensive fine-tuning technique for pre-trained models, to optimize for the desired objective. The core idea of our framework is to determine a sampling distribution to perform a",
    "path": "papers/24/03/2403.18301.json",
    "total_tokens": 785,
    "translated_title": "选择性混合微调以优化非可分解目标",
    "translated_abstract": "互联网使用的增加导致了大量数据的生成，从而采用了各种监督和半监督机器学习算法，这些算法可以有效利用大量数据来训练模型。然而，在将这些模型部署到现实世界之前，必须严格评估它们在诸如最坏情况召回率之类的性能指标上的表现，并满足公平性等约束条件。我们发现，当前最先进的经验技术在这些实际的、非可分解的性能目标上提供了次优性能。另一方面，理论技术需要为每个性能目标从头开始训练一个新模型。为了弥合这一差距，我们提出了SelMix，这是一种基于选择性混合的廉价微调技术，用于针对所需目标进行优化。",
    "tldr": "提出了SelMix，一种选择性混合的廉价微调技术，用于优化预训练模型以实现所需的非可分解目标。",
    "en_tdlr": "Introduced SelMix, a selective mixup-based inexpensive fine-tuning technique for optimizing pre-trained models towards desired non-decomposable objectives."
}