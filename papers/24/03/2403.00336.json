{
    "title": "Never-Ending Embodied Robot Learning",
    "abstract": "arXiv:2403.00336v1 Announce Type: cross  Abstract: Relying on large language models (LLMs), embodied robots could perform complex multimodal robot manipulation tasks from visual observations with powerful generalization ability. However, most visual behavior-cloning agents suffer from manipulation performance degradation and skill knowledge forgetting when adapting into a series of challenging unseen tasks. We here investigate the above challenge with NBCagent in embodied robots, a pioneering language-conditioned Never-ending Behavior-Cloning agent, which can continually learn observation knowledge of novel robot manipulation skills from skill-specific and skill-shared attributes. Specifically, we establish a skill-specific evolving planner to perform knowledge decoupling, which can continually embed novel skill-specific knowledge in our NBCagent agent from latent and low-rank space. Meanwhile, we propose a skill-shared semantics rendering module and a skill-shared representation disti",
    "link": "https://arxiv.org/abs/2403.00336",
    "context": "Title: Never-Ending Embodied Robot Learning\nAbstract: arXiv:2403.00336v1 Announce Type: cross  Abstract: Relying on large language models (LLMs), embodied robots could perform complex multimodal robot manipulation tasks from visual observations with powerful generalization ability. However, most visual behavior-cloning agents suffer from manipulation performance degradation and skill knowledge forgetting when adapting into a series of challenging unseen tasks. We here investigate the above challenge with NBCagent in embodied robots, a pioneering language-conditioned Never-ending Behavior-Cloning agent, which can continually learn observation knowledge of novel robot manipulation skills from skill-specific and skill-shared attributes. Specifically, we establish a skill-specific evolving planner to perform knowledge decoupling, which can continually embed novel skill-specific knowledge in our NBCagent agent from latent and low-rank space. Meanwhile, we propose a skill-shared semantics rendering module and a skill-shared representation disti",
    "path": "papers/24/03/2403.00336.json",
    "total_tokens": 860,
    "translated_title": "永不停止的具身机器人学习",
    "translated_abstract": "依赖于大型语言模型（LLM），具身机器人可以通过强大的泛化能力，从视觉观测中执行复杂的多模态机器人操作任务。然而，大多数视觉行为克隆代理在适应一系列具有挑战性的未见任务时，会遭受操纵性能下降以及技能知识遗忘的困扰。在本研究中，我们通过NBCagent在具身机器人中探讨了上述挑战，这是一种开创性的、以语言为条件的永不停止行为克隆代理，可以不断从特定技能和共享技能属性中学习新的机器人操作技能的观察知识。具体来说，我们建立了一个特定技能不断演化的规划器来进行知识解耦，这可以从潜在和低秩空间中不断向我们的NBCagent代理嵌入新的技能特定知识。与此同时，我们提出了一个技能共享语义渲染模块和一个技能共享表示区分",
    "tldr": "提出了一种具身机器人学习代理NBCagent，通过技能特定的演化规划器和技能共享的语义渲染模块，实现从视觉观测中连续学习新的机器人操作技能知识。"
}