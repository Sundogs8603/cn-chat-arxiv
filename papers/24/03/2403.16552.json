{
    "title": "QKFormer: Hierarchical Spiking Transformer using Q-K Attention",
    "abstract": "arXiv:2403.16552v1 Announce Type: cross  Abstract: Spiking Transformers, which integrate Spiking Neural Networks (SNNs) with Transformer architectures, have attracted significant attention due to their potential for energy efficiency and high performance. However, existing models in this domain still suffer from suboptimal performance. We introduce several innovations to improve the performance: i) We propose a novel spike-form Q-K attention mechanism, tailored for SNNs, which efficiently models the importance of token or channel dimensions through binary vectors with linear complexity. ii) We incorporate the hierarchical structure, which significantly benefits the performance of both the brain and artificial neural networks, into spiking transformers to obtain multi-scale spiking representation. iii) We design a versatile and powerful patch embedding module with a deformed shortcut specifically for spiking transformers. Together, we develop QKFormer, a hierarchical spiking transformer",
    "link": "https://arxiv.org/abs/2403.16552",
    "context": "Title: QKFormer: Hierarchical Spiking Transformer using Q-K Attention\nAbstract: arXiv:2403.16552v1 Announce Type: cross  Abstract: Spiking Transformers, which integrate Spiking Neural Networks (SNNs) with Transformer architectures, have attracted significant attention due to their potential for energy efficiency and high performance. However, existing models in this domain still suffer from suboptimal performance. We introduce several innovations to improve the performance: i) We propose a novel spike-form Q-K attention mechanism, tailored for SNNs, which efficiently models the importance of token or channel dimensions through binary vectors with linear complexity. ii) We incorporate the hierarchical structure, which significantly benefits the performance of both the brain and artificial neural networks, into spiking transformers to obtain multi-scale spiking representation. iii) We design a versatile and powerful patch embedding module with a deformed shortcut specifically for spiking transformers. Together, we develop QKFormer, a hierarchical spiking transformer",
    "path": "papers/24/03/2403.16552.json",
    "total_tokens": 859,
    "translated_title": "QKFormer: 使用Q-K注意力的分层脉冲变压器",
    "translated_abstract": "脉冲变压器将脉冲神经网络（SNNs）与变压器架构相结合，由于其节能高性能的潜力，吸引了很多关注。然而，该领域现有模型仍然存在性能不佳的问题。为了提高性能，我们引入了几项创新：i）我们提出了一种为SNNs量身定制的新型脉冲形式Q-K注意力机制，通过具有线性复杂性的二进制向量有效地建模令牌或通道维度的重要性。ii）我们将具有显著性能优势的分层结构引入脉冲变压器，从而获得多尺度脉冲表示，这对大脑和人工神经网络的性能都有显着好处。iii）我们设计了一个通用且强大的补丁嵌入模块，其中包含了一个专门为脉冲变压器设计的变形快捷方式。总之，我们开发了QKFormer，一种分层脉冲变压器。",
    "tldr": "QKFormer引入了新颖的脉冲形式Q-K注意力机制、分层结构和补丁嵌入模块，以提高脉冲变压器的性能。",
    "en_tdlr": "QKFormer introduces novel spike-form Q-K attention mechanism, hierarchical structure, and patch embedding module to enhance the performance of spiking transformers."
}