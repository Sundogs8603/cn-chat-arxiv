{
    "title": "FBPT: A Fully Binary Point Transformer",
    "abstract": "arXiv:2403.09998v1 Announce Type: cross  Abstract: This paper presents a novel Fully Binary Point Cloud Transformer (FBPT) model which has the potential to be widely applied and expanded in the fields of robotics and mobile devices. By compressing the weights and activations of a 32-bit full-precision network to 1-bit binary values, the proposed binary point cloud Transformer network significantly reduces the storage footprint and computational resource requirements of neural network models for point cloud processing tasks, compared to full-precision point cloud networks. However, achieving a fully binary point cloud Transformer network, where all parts except the modules specific to the task are binary, poses challenges and bottlenecks in quantizing the activations of Q, K, V and self-attention in the attention module, as they do not adhere to simple probability distributions and can vary with input data. Furthermore, in our network, the binary attention module undergoes a degradation",
    "link": "https://arxiv.org/abs/2403.09998",
    "context": "Title: FBPT: A Fully Binary Point Transformer\nAbstract: arXiv:2403.09998v1 Announce Type: cross  Abstract: This paper presents a novel Fully Binary Point Cloud Transformer (FBPT) model which has the potential to be widely applied and expanded in the fields of robotics and mobile devices. By compressing the weights and activations of a 32-bit full-precision network to 1-bit binary values, the proposed binary point cloud Transformer network significantly reduces the storage footprint and computational resource requirements of neural network models for point cloud processing tasks, compared to full-precision point cloud networks. However, achieving a fully binary point cloud Transformer network, where all parts except the modules specific to the task are binary, poses challenges and bottlenecks in quantizing the activations of Q, K, V and self-attention in the attention module, as they do not adhere to simple probability distributions and can vary with input data. Furthermore, in our network, the binary attention module undergoes a degradation",
    "path": "papers/24/03/2403.09998.json",
    "total_tokens": 850,
    "translated_title": "FBPT：一个完全二进制点云Transformer",
    "translated_abstract": "本文提出了一种新颖的Fully Binary Point Cloud Transformer（FBPT）模型，该模型在机器人和移动设备领域具有广泛的应用和扩展潜力。通过将32位全精度网络的权重和激活压缩为1位二进制值，所提出的二进制点云Transformer网络显著降低了用于点云处理任务的神经网络模型的存储占用和计算资源需求，相较于全精度点云网络。然而，实现完全的二进制点云Transformer网络，其中除了与任务特定的模块外其他所有部分都是二进制的，会在量化注意力模块中的Q、K、V和自注意力激活时面临挑战和瓶颈，因为它们不符合简单的概率分布，并且可能随输入数据变化而变化。此外，在我们的网络中，二进制注意力模块经历了衰减",
    "tldr": "该论文提出了一个全新的完全二进制点云Transformer模型，通过压缩网络的权重和激活为1位二进制值，显著降低了点云处理任务神经网络模型的存储和计算资源需求。",
    "en_tdlr": "This paper introduces a novel Fully Binary Point Cloud Transformer (FBPT) model that significantly reduces the storage and computational resource requirements of neural network models for point cloud processing tasks by compressing weights and activations to 1-bit binary values."
}