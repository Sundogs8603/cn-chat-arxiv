{
    "title": "Subspace Defense: Discarding Adversarial Perturbations by Learning a Subspace for Clean Signals",
    "abstract": "arXiv:2403.16176v1 Announce Type: cross  Abstract: Deep neural networks (DNNs) are notoriously vulnerable to adversarial attacks that place carefully crafted perturbations on normal examples to fool DNNs. To better understand such attacks, a characterization of the features carried by adversarial examples is needed. In this paper, we tackle this challenge by inspecting the subspaces of sample features through spectral analysis. We first empirically show that the features of either clean signals or adversarial perturbations are redundant and span in low-dimensional linear subspaces respectively with minimal overlap, and the classical low-dimensional subspace projection can suppress perturbation features out of the subspace of clean signals. This makes it possible for DNNs to learn a subspace where only features of clean signals exist while those of perturbations are discarded, which can facilitate the distinction of adversarial examples. To prevent the residual perturbations that is ine",
    "link": "https://arxiv.org/abs/2403.16176",
    "context": "Title: Subspace Defense: Discarding Adversarial Perturbations by Learning a Subspace for Clean Signals\nAbstract: arXiv:2403.16176v1 Announce Type: cross  Abstract: Deep neural networks (DNNs) are notoriously vulnerable to adversarial attacks that place carefully crafted perturbations on normal examples to fool DNNs. To better understand such attacks, a characterization of the features carried by adversarial examples is needed. In this paper, we tackle this challenge by inspecting the subspaces of sample features through spectral analysis. We first empirically show that the features of either clean signals or adversarial perturbations are redundant and span in low-dimensional linear subspaces respectively with minimal overlap, and the classical low-dimensional subspace projection can suppress perturbation features out of the subspace of clean signals. This makes it possible for DNNs to learn a subspace where only features of clean signals exist while those of perturbations are discarded, which can facilitate the distinction of adversarial examples. To prevent the residual perturbations that is ine",
    "path": "papers/24/03/2403.16176.json",
    "total_tokens": 886,
    "translated_title": "子空间防御：通过学习干净信号的子空间来丢弃对抗性扰动",
    "translated_abstract": "深度神经网络(DNNs)极易受到对抗性攻击的影响，即在正常示例上放置精心制作的扰动以欺骗DNNs。为了更好地理解这类攻击，需要对对抗性示例携带的特征进行刻画。本文通过对样本特征子空间进行谱分析来解决这一挑战。我们首先经验性地展示，无论是干净信号还是对抗性扰动的特征都是冗余的，并分别在低维线性子空间中展开，互相之间重叠很小，而经典的低维子空间投影可以将扰动特征排除在干净信号子空间之外。这使得DNNs能够学习一个仅存在干净信号特征的子空间，而丢弃扰动特征，这有助于区分对抗性示例。为了防止残余的扰动，提出了一种二次最优方向残差网络(QPRN)。",
    "tldr": "通过学习一个仅存在干净信号特征的子空间并丢弃扰动特征，使得深度神经网络能够更好地区分对抗性示例。",
    "en_tdlr": "By learning a subspace where only features of clean signals exist and discarding perturbation features, deep neural networks are able to better distinguish adversarial examples."
}