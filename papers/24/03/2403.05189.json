{
    "title": "Tracing the Roots of Facts in Multilingual Language Models: Independent, Shared, and Transferred Knowledge",
    "abstract": "arXiv:2403.05189v1 Announce Type: cross  Abstract: Acquiring factual knowledge for language models (LMs) in low-resource languages poses a serious challenge, thus resorting to cross-lingual transfer in multilingual LMs (ML-LMs). In this study, we ask how ML-LMs acquire and represent factual knowledge. Using the multilingual factual knowledge probing dataset, mLAMA, we first conducted a neuron investigation of ML-LMs (specifically, multilingual BERT). We then traced the roots of facts back to the knowledge source (Wikipedia) to identify the ways in which ML-LMs acquire specific facts. We finally identified three patterns of acquiring and representing facts in ML-LMs: language-independent, cross-lingual shared and transferred, and devised methods for differentiating them. Our findings highlight the challenge of maintaining consistent factual knowledge across languages, underscoring the need for better fact representation learning in ML-LMs.",
    "link": "https://arxiv.org/abs/2403.05189",
    "context": "Title: Tracing the Roots of Facts in Multilingual Language Models: Independent, Shared, and Transferred Knowledge\nAbstract: arXiv:2403.05189v1 Announce Type: cross  Abstract: Acquiring factual knowledge for language models (LMs) in low-resource languages poses a serious challenge, thus resorting to cross-lingual transfer in multilingual LMs (ML-LMs). In this study, we ask how ML-LMs acquire and represent factual knowledge. Using the multilingual factual knowledge probing dataset, mLAMA, we first conducted a neuron investigation of ML-LMs (specifically, multilingual BERT). We then traced the roots of facts back to the knowledge source (Wikipedia) to identify the ways in which ML-LMs acquire specific facts. We finally identified three patterns of acquiring and representing facts in ML-LMs: language-independent, cross-lingual shared and transferred, and devised methods for differentiating them. Our findings highlight the challenge of maintaining consistent factual knowledge across languages, underscoring the need for better fact representation learning in ML-LMs.",
    "path": "papers/24/03/2403.05189.json",
    "total_tokens": 917,
    "translated_title": "追踪多语言语言模型中事实的根源：独立的、共享的和转移的知识",
    "translated_abstract": "获取低资源语言模型（LMs）中的事实知识是一个严峻的挑战，因此需要在多语言LMs（ML-LMs）中进行跨语言转移。本研究探讨了ML-LMs如何获取和表示事实知识。我们首先使用多语言事实知识探测数据集mLAMA对ML-LMs（特别是多语言BERT）进行神经元调查。然后我们追溯事实的根源（维基百科），以确定ML-LMs获取特定事实的方式。最后，我们确定了ML-LMs获取和表示事实的三种模式：语言独立、跨语言共享和转移，并制定了区分它们的方法。我们的发现突显了跨语言保持一致的事实知识的挑战，强调了在ML-LMs中进行更好的事实表示学习的必要性。",
    "tldr": "本研究追踪了多语言语言模型中事实的来源，发现了三种模式：语言独立、跨语言共享和转移，为区分它们提出了方法，凸显了在多语言LMs中保持一致事实知识的挑战，强调了需要在ML-LMs中改进事实表示学习。",
    "en_tdlr": "This study traced the roots of facts in multilingual language models, identified three patterns of acquiring and representing facts (language-independent, cross-lingual shared and transferred), devised methods for differentiating them, highlighted the challenge of maintaining consistent factual knowledge across languages, and underscored the need for better fact representation learning in ML-LMs."
}