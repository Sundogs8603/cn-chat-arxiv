{
    "title": "Retentive Decision Transformer with Adaptive Masking for Reinforcement Learning based Recommendation Systems",
    "abstract": "arXiv:2403.17634v1 Announce Type: cross  Abstract: Reinforcement Learning-based Recommender Systems (RLRS) have shown promise across a spectrum of applications, from e-commerce platforms to streaming services. Yet, they grapple with challenges, notably in crafting reward functions and harnessing large pre-existing datasets within the RL framework. Recent advancements in offline RLRS provide a solution for how to address these two challenges. However, existing methods mainly rely on the transformer architecture, which, as sequence lengths increase, can introduce challenges associated with computational resources and training costs. Additionally, the prevalent methods employ fixed-length input trajectories, restricting their capacity to capture evolving user preferences. In this study, we introduce a new offline RLRS method to deal with the above problems. We reinterpret the RLRS challenge by modeling sequential decision-making as an inference task, leveraging adaptive masking configurat",
    "link": "https://arxiv.org/abs/2403.17634",
    "context": "Title: Retentive Decision Transformer with Adaptive Masking for Reinforcement Learning based Recommendation Systems\nAbstract: arXiv:2403.17634v1 Announce Type: cross  Abstract: Reinforcement Learning-based Recommender Systems (RLRS) have shown promise across a spectrum of applications, from e-commerce platforms to streaming services. Yet, they grapple with challenges, notably in crafting reward functions and harnessing large pre-existing datasets within the RL framework. Recent advancements in offline RLRS provide a solution for how to address these two challenges. However, existing methods mainly rely on the transformer architecture, which, as sequence lengths increase, can introduce challenges associated with computational resources and training costs. Additionally, the prevalent methods employ fixed-length input trajectories, restricting their capacity to capture evolving user preferences. In this study, we introduce a new offline RLRS method to deal with the above problems. We reinterpret the RLRS challenge by modeling sequential decision-making as an inference task, leveraging adaptive masking configurat",
    "path": "papers/24/03/2403.17634.json",
    "total_tokens": 836,
    "translated_title": "具有自适应遮罩的保留决策变压器用于基于强化学习的推荐系统",
    "translated_abstract": "强化学习推荐系统（RLRS）在一系列应用中显示出潜力，从电子商务平台到流媒体服务。然而，它们在制定奖励函数和利用RL框架中的大型现有数据集方面面临挑战。最近的离线RLRS的技术进步为解决这两个挑战提供了解决方案。然而，现有方法主要依赖于转换器架构，在序列长度增加时可能会引入与计算资源和训练成本相关的挑战。此外，主流方法使用固定长度的输入轨迹，限制了它们捕获不断变化的用户喜好的能力。在本研究中，我们介绍了一种新的离线RLRS方法来解决以上问题。我们通过将顺序决策建模为推理任务，利用自适应遮罩配置来重新解释RLRS挑战。",
    "tldr": "本研究提出了一种新的离线RL推荐系统方法，通过将顺序决策建模为推理任务，利用自适应遮罩配置来重新解释RLRS挑战。",
    "en_tdlr": "This study introduces a new offline RL recommendation system method that reinterprets the RLRS challenge by modeling sequential decision-making as an inference task, leveraging adaptive masking configuration."
}