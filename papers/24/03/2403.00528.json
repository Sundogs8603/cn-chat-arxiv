{
    "title": "Large Language Models for Simultaneous Named Entity Extraction and Spelling Correction",
    "abstract": "arXiv:2403.00528v1 Announce Type: new  Abstract: Language Models (LMs) such as BERT, have been shown to perform well on the task of identifying Named Entities (NE) in text. A BERT LM is typically used as a classifier to classify individual tokens in the input text, or to classify spans of tokens, as belonging to one of a set of possible NE categories.   In this paper, we hypothesise that decoder-only Large Language Models (LLMs) can also be used generatively to extract both the NE, as well as potentially recover the correct surface form of the NE, where any spelling errors that were present in the input text get automatically corrected.   We fine-tune two BERT LMs as baselines, as well as eight open-source LLMs, on the task of producing NEs from text that was obtained by applying Optical Character Recognition (OCR) to images of Japanese shop receipts; in this work, we do not attempt to find or evaluate the location of NEs in the text.   We show that the best fine-tuned LLM performs as ",
    "link": "https://arxiv.org/abs/2403.00528",
    "context": "Title: Large Language Models for Simultaneous Named Entity Extraction and Spelling Correction\nAbstract: arXiv:2403.00528v1 Announce Type: new  Abstract: Language Models (LMs) such as BERT, have been shown to perform well on the task of identifying Named Entities (NE) in text. A BERT LM is typically used as a classifier to classify individual tokens in the input text, or to classify spans of tokens, as belonging to one of a set of possible NE categories.   In this paper, we hypothesise that decoder-only Large Language Models (LLMs) can also be used generatively to extract both the NE, as well as potentially recover the correct surface form of the NE, where any spelling errors that were present in the input text get automatically corrected.   We fine-tune two BERT LMs as baselines, as well as eight open-source LLMs, on the task of producing NEs from text that was obtained by applying Optical Character Recognition (OCR) to images of Japanese shop receipts; in this work, we do not attempt to find or evaluate the location of NEs in the text.   We show that the best fine-tuned LLM performs as ",
    "path": "papers/24/03/2403.00528.json",
    "total_tokens": 822,
    "translated_title": "用于同时进行命名实体提取和拼写校正的大型语言模型",
    "translated_abstract": "语言模型（LMs）如BERT已被证明在识别文本中的命名实体（NE）任务上表现良好。BERT LM通常被用作分类器，用于将输入文本中的单个标记分类，或将标记范围分类为可能的NE类别之一。本文假设仅解码器的大型语言模型（LLMs）也可以被生成性地用于提取NE以及可能恢复NE的正确表面形式，其中输入文本中存在的任何拼写错误都将被自动纠正。我们对两个BERT LMs进行微调作为基线，以及八个开源LLMs，在通过将光学字符识别（OCR）应用于日本商店收据图像而获得的文本上进行NE生成任务的微调，本工作中，我们不尝试找到或评估文本中NE的位置。我们表明最优微调的LLM的表现如何",
    "tldr": "本文研究使用解码器模型来生成地提取命名实体，并自动校正输入文本中的拼写错误。"
}