{
    "title": "Cross-Lingual Transfer Robustness to Lower-Resource Languages on Adversarial Datasets",
    "abstract": "arXiv:2403.20056v1 Announce Type: new  Abstract: Multilingual Language Models (MLLMs) exhibit robust cross-lingual transfer capabilities, or the ability to leverage information acquired in a source language and apply it to a target language. These capabilities find practical applications in well-established Natural Language Processing (NLP) tasks such as Named Entity Recognition (NER). This study aims to investigate the effectiveness of a source language when applied to a target language, particularly in the context of perturbing the input test set. We evaluate on 13 pairs of languages, each including one high-resource language (HRL) and one low-resource language (LRL) with a geographic, genetic, or borrowing relationship. We evaluate two well-known MLLMs--MBERT and XLM-R--on these pairs, in native LRL and cross-lingual transfer settings, in two tasks, under a set of different perturbations. Our findings indicate that NER cross-lingual transfer depends largely on the overlap of entity ",
    "link": "https://arxiv.org/abs/2403.20056",
    "context": "Title: Cross-Lingual Transfer Robustness to Lower-Resource Languages on Adversarial Datasets\nAbstract: arXiv:2403.20056v1 Announce Type: new  Abstract: Multilingual Language Models (MLLMs) exhibit robust cross-lingual transfer capabilities, or the ability to leverage information acquired in a source language and apply it to a target language. These capabilities find practical applications in well-established Natural Language Processing (NLP) tasks such as Named Entity Recognition (NER). This study aims to investigate the effectiveness of a source language when applied to a target language, particularly in the context of perturbing the input test set. We evaluate on 13 pairs of languages, each including one high-resource language (HRL) and one low-resource language (LRL) with a geographic, genetic, or borrowing relationship. We evaluate two well-known MLLMs--MBERT and XLM-R--on these pairs, in native LRL and cross-lingual transfer settings, in two tasks, under a set of different perturbations. Our findings indicate that NER cross-lingual transfer depends largely on the overlap of entity ",
    "path": "papers/24/03/2403.20056.json",
    "total_tokens": 900,
    "translated_title": "在对抗数据集上跨语言传输对低资源语言的鲁棒性",
    "translated_abstract": "多语言语言模型（MLLMs）展示了强大的跨语言传输能力，即利用在源语言中获取的信息并将其应用于目标语言的能力。这些能力在已建立的自然语言处理（NLP）任务中找到实际应用，如命名实体识别（NER）。该研究旨在调查将源语言应用于目标语言时的有效性，特别是在扰动输入测试集的情况下。我们评估了13对语言，每对语言包括一个高资源语言（HRL）和一个低资源语言（LRL），它们之间存在地理、遗传或借用关系。我们在这些语言对上评估了两个知名的MLLMs--MBERT和XLM-R，在本机LRL和跨语言传输设置中，在两个任务中，在一组不同的扰动下。我们的研究结果表明，NER跨语言传输在很大程度上取决于实体的重叠",
    "tldr": "多语言语言模型展示了强大的跨语言传输能力，对低资源语言的鲁棒性具有实际应用，研究发现命名实体识别跨语言传输在很大程度上取决于实体的重叠",
    "en_tdlr": "Multilingual Language Models show robust cross-lingual transfer capabilities with practical applications for low-resource languages, and the study found that Named Entity Recognition cross-lingual transfer largely depends on the overlap of entities."
}