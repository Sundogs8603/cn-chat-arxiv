{
    "title": "Recall-Oriented Continual Learning with Generative Adversarial Meta-Model",
    "abstract": "arXiv:2403.03082v1 Announce Type: cross  Abstract: The stability-plasticity dilemma is a major challenge in continual learning, as it involves balancing the conflicting objectives of maintaining performance on previous tasks while learning new tasks. In this paper, we propose the recall-oriented continual learning framework to address this challenge. Inspired by the human brain's ability to separate the mechanisms responsible for stability and plasticity, our framework consists of a two-level architecture where an inference network effectively acquires new knowledge and a generative network recalls past knowledge when necessary. In particular, to maximize the stability of past knowledge, we investigate the complexity of knowledge depending on different representations, and thereby introducing generative adversarial meta-model (GAMM) that incrementally learns task-specific parameters instead of input data samples of the task. Through our experiments, we show that our framework not only ",
    "link": "https://arxiv.org/abs/2403.03082",
    "context": "Title: Recall-Oriented Continual Learning with Generative Adversarial Meta-Model\nAbstract: arXiv:2403.03082v1 Announce Type: cross  Abstract: The stability-plasticity dilemma is a major challenge in continual learning, as it involves balancing the conflicting objectives of maintaining performance on previous tasks while learning new tasks. In this paper, we propose the recall-oriented continual learning framework to address this challenge. Inspired by the human brain's ability to separate the mechanisms responsible for stability and plasticity, our framework consists of a two-level architecture where an inference network effectively acquires new knowledge and a generative network recalls past knowledge when necessary. In particular, to maximize the stability of past knowledge, we investigate the complexity of knowledge depending on different representations, and thereby introducing generative adversarial meta-model (GAMM) that incrementally learns task-specific parameters instead of input data samples of the task. Through our experiments, we show that our framework not only ",
    "path": "papers/24/03/2403.03082.json",
    "total_tokens": 798,
    "translated_title": "具有生成对抗元模型的召回导向的持续学习",
    "translated_abstract": "稳定性-可塑性困境是持续学习中的主要挑战，因为它涉及在学习新任务的同时保持对以前任务性能的平衡。本文提出了召回导向的持续学习框架来解决这一挑战。灵感来自于人类大脑分离稳定性和可塑性机制的能力，我们的框架包括一个两级体系结构，其中推理网络有效地获取新知识，而生成网络在需要时回顾过去的知识。具体地，为了最大化过去知识的稳定性，我们研究了不同表示取决于知识复杂度，从而引入了增量学习任务特定参数而不是任务的输入数据样本的生成对抗元模型（GAMM）。通过实验证明，我们的框架不仅",
    "tldr": "框架提出了召回导向的持续学习方法，通过生成对抗元模型(GAMM)在学习新任务时最大化过去知识的稳定性。",
    "en_tdlr": "The framework introduces a recall-oriented continual learning approach, maximizing the stability of past knowledge through Generative Adversarial Meta-Model (GAMM) when learning new tasks."
}