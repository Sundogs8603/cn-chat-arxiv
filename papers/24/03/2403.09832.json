{
    "title": "Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks",
    "abstract": "arXiv:2403.09832v1 Announce Type: new  Abstract: Large Language Models (LLMs) are increasingly becoming the preferred foundation platforms for many Natural Language Processing tasks such as Machine Translation, owing to their quality often comparable to or better than task-specific models, and the simplicity of specifying the task through natural language instructions or in-context examples. Their generality, however, opens them up to subversion by end users who may embed into their requests instructions that cause the model to behave in unauthorized and possibly unsafe ways. In this work we study these Prompt Injection Attacks (PIAs) on multiple families of LLMs on a Machine Translation task, focusing on the effects of model size on the attack success rates. We introduce a new benchmark data set and we discover that on multiple language pairs and injected prompts written in English, larger models under certain conditions may become more susceptible to successful attacks, an instance o",
    "link": "https://arxiv.org/abs/2403.09832",
    "context": "Title: Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks\nAbstract: arXiv:2403.09832v1 Announce Type: new  Abstract: Large Language Models (LLMs) are increasingly becoming the preferred foundation platforms for many Natural Language Processing tasks such as Machine Translation, owing to their quality often comparable to or better than task-specific models, and the simplicity of specifying the task through natural language instructions or in-context examples. Their generality, however, opens them up to subversion by end users who may embed into their requests instructions that cause the model to behave in unauthorized and possibly unsafe ways. In this work we study these Prompt Injection Attacks (PIAs) on multiple families of LLMs on a Machine Translation task, focusing on the effects of model size on the attack success rates. We introduce a new benchmark data set and we discover that on multiple language pairs and injected prompts written in English, larger models under certain conditions may become more susceptible to successful attacks, an instance o",
    "path": "papers/24/03/2403.09832.json",
    "total_tokens": 871,
    "translated_title": "使用大型语言模型在提示注入攻击下进行的机器翻译的规模行为研究",
    "translated_abstract": "大型语言模型(LLMs)越来越成为许多自然语言处理任务的首选平台，如机器翻译，因为它们的质量往往与特定任务模型相比具有可比性或更好，并且通过自然语言指令或上下文示例指定任务的简单性。然而，它们的普遍性使它们容易受到最终用户的颠覆，后者可能将导致模型以未经授权且可能不安全的方式行为的指令嵌入到他们的请求中。在这项工作中，我们研究了在机器翻译任务上对多个家族的LLMs进行的提示注入攻击(PIAs)，重点关注模型大小对攻击成功率的影响。我们引入了一个新的基准数据集，并发现在多种语言对和英文注入提示的情况下，更大的模型在某些条件下可能更容易受到成功攻击的影响，这是一种情况",
    "tldr": "该研究探讨了在机器翻译任务上对多个家族的大型语言模型进行的提示注入攻击的影响，发现在某些条件下，更大的模型可能更容易受到成功攻击。",
    "en_tdlr": "This study explores the effects of prompt injection attacks on multiple families of large language models in machine translation tasks, finding that in certain conditions, larger models may be more susceptible to successful attacks."
}