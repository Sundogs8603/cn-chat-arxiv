{
    "title": "Region-Transformer: Self-Attention Region Based Class-Agnostic Point Cloud Segmentation",
    "abstract": "arXiv:2403.01407v1 Announce Type: cross  Abstract: Point cloud segmentation, which helps us understand the environment of specific structures and objects, can be performed in class-specific and class-agnostic ways. We propose a novel region-based transformer model called Region-Transformer for performing class-agnostic point cloud segmentation. The model utilizes a region-growth approach and self-attention mechanism to iteratively expand or contract a region by adding or removing points. It is trained on simulated point clouds with instance labels only, avoiding semantic labels. Attention-based networks have succeeded in many previous methods of performing point cloud segmentation. However, a region-growth approach with attention-based networks has yet to be used to explore its performance gain. To our knowledge, we are the first to use a self-attention mechanism in a region-growth approach. With the introduction of self-attention to region-growth that can utilize local contextual info",
    "link": "https://arxiv.org/abs/2403.01407",
    "context": "Title: Region-Transformer: Self-Attention Region Based Class-Agnostic Point Cloud Segmentation\nAbstract: arXiv:2403.01407v1 Announce Type: cross  Abstract: Point cloud segmentation, which helps us understand the environment of specific structures and objects, can be performed in class-specific and class-agnostic ways. We propose a novel region-based transformer model called Region-Transformer for performing class-agnostic point cloud segmentation. The model utilizes a region-growth approach and self-attention mechanism to iteratively expand or contract a region by adding or removing points. It is trained on simulated point clouds with instance labels only, avoiding semantic labels. Attention-based networks have succeeded in many previous methods of performing point cloud segmentation. However, a region-growth approach with attention-based networks has yet to be used to explore its performance gain. To our knowledge, we are the first to use a self-attention mechanism in a region-growth approach. With the introduction of self-attention to region-growth that can utilize local contextual info",
    "path": "papers/24/03/2403.01407.json",
    "total_tokens": 851,
    "translated_title": "区域-Transformer: 基于自注意力区域的无关类别点云分割",
    "translated_abstract": "点云分割可以以特定结构和对象视角以特定类别或无关类别的方式进行。我们提出了一种名为区域-Transformer的新型基于区域的Transformer模型，用于执行无关类别的点云分割。该模型利用了区域增长方法和自注意力机制，通过添加或删除点来迭代地扩展或收缩区域。模型仅在虚拟点云上进行训练，仅使用实例标签，避免使用语义标签。基于注意力的网络在许多以往的点云分割方法中取得了成功。然而，使用具有关注网络的区域增长方法尚未被用于探索其性能提升。据我们所知，我们是第一个在区域增长方法中使用自注意力机制的研究。通过将自注意引入到可以利用局部上下文信息的区域增长中",
    "tldr": "该论文提出了一种名为区域-Transformer的新型区域基Transformer模型，使用区域增长方法和自注意力机制进行无关类别的点云分割训练，首次将自注意力机制应用于区域增长方法。",
    "en_tdlr": "This paper introduces a novel region-based Transformer model called Region-Transformer for class-agnostic point cloud segmentation, utilizing region-growth approach and self-attention mechanism, marking the first application of self-attention mechanism in region-growth methods."
}