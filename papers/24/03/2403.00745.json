{
    "title": "AtP*: An efficient and scalable method for localizing LLM behaviour to components",
    "abstract": "arXiv:2403.00745v1 Announce Type: cross  Abstract: Activation Patching is a method of directly computing causal attributions of behavior to model components. However, applying it exhaustively requires a sweep with cost scaling linearly in the number of model components, which can be prohibitively expensive for SoTA Large Language Models (LLMs). We investigate Attribution Patching (AtP), a fast gradient-based approximation to Activation Patching and find two classes of failure modes of AtP which lead to significant false negatives. We propose a variant of AtP called AtP*, with two changes to address these failure modes while retaining scalability. We present the first systematic study of AtP and alternative methods for faster activation patching and show that AtP significantly outperforms all other investigated methods, with AtP* providing further significant improvement. Finally, we provide a method to bound the probability of remaining false negatives of AtP* estimates.",
    "link": "https://arxiv.org/abs/2403.00745",
    "context": "Title: AtP*: An efficient and scalable method for localizing LLM behaviour to components\nAbstract: arXiv:2403.00745v1 Announce Type: cross  Abstract: Activation Patching is a method of directly computing causal attributions of behavior to model components. However, applying it exhaustively requires a sweep with cost scaling linearly in the number of model components, which can be prohibitively expensive for SoTA Large Language Models (LLMs). We investigate Attribution Patching (AtP), a fast gradient-based approximation to Activation Patching and find two classes of failure modes of AtP which lead to significant false negatives. We propose a variant of AtP called AtP*, with two changes to address these failure modes while retaining scalability. We present the first systematic study of AtP and alternative methods for faster activation patching and show that AtP significantly outperforms all other investigated methods, with AtP* providing further significant improvement. Finally, we provide a method to bound the probability of remaining false negatives of AtP* estimates.",
    "path": "papers/24/03/2403.00745.json",
    "total_tokens": 858,
    "translated_title": "AtP*：一种将LLM行为定位到组件的高效可扩展方法",
    "translated_abstract": "Activation Patching是一种直接计算行为因果归因于模型组件的方法。然而，要全面应用该方法，需要进行一次成本随模型组件数量线性增加的扫描，这可能对SoTA大型语言模型（LLMs）来说成本过高。我们研究了Attribution Patching（AtP），这是对Activation Patching的一种快速基于梯度的近似方法，并发现了两类导致AtP出现显著假阴性的故障模式。我们提出了AtP*的变体，通过两种改变来解决这些故障模式，同时保持可扩展性。我们首次系统研究了AtP及其他快速激活修补方法的对比，并表明AtP明显优于所有其他研究方法，而AtP*进一步提供了显著改进。最后，我们提供了一种方法来限制AtP*估计的假阴性剩余概率。",
    "tldr": "AtP*是一种将LLM行为准确定位到组件的高效可扩展方法，通过解决Attribution Patching存在的显著假阴性问题，提供了显著改进以及进一步的性能提升。",
    "en_tdlr": "AtP* is an efficient and scalable method for accurately localizing LLM behavior to components, addressing significant false negatives in Attribution Patching and providing substantial improvement and further performance upgrades."
}