{
    "title": "Interactive Continual Learning: Fast and Slow Thinking",
    "abstract": "arXiv:2403.02628v1 Announce Type: cross  Abstract: Advanced life forms, sustained by the synergistic interaction of neural cognitive mechanisms, continually acquire and transfer knowledge throughout their lifespan. In contrast, contemporary machine learning paradigms exhibit limitations in emulating the facets of continual learning (CL). Nonetheless, the emergence of large language models (LLMs) presents promising avenues for realizing CL via interactions with these models. Drawing on Complementary Learning System theory, this paper presents a novel Interactive Continual Learning (ICL) framework, enabled by collaborative interactions among models of various sizes. Specifically, we assign the ViT model as System1 and multimodal LLM as System2. To enable the memory module to deduce tasks from class information and enhance Set2Set retrieval, we propose the Class-Knowledge-Task Multi-Head Attention (CKT-MHA). Additionally, to improve memory retrieval in System1 through enhanced geometric r",
    "link": "https://arxiv.org/abs/2403.02628",
    "context": "Title: Interactive Continual Learning: Fast and Slow Thinking\nAbstract: arXiv:2403.02628v1 Announce Type: cross  Abstract: Advanced life forms, sustained by the synergistic interaction of neural cognitive mechanisms, continually acquire and transfer knowledge throughout their lifespan. In contrast, contemporary machine learning paradigms exhibit limitations in emulating the facets of continual learning (CL). Nonetheless, the emergence of large language models (LLMs) presents promising avenues for realizing CL via interactions with these models. Drawing on Complementary Learning System theory, this paper presents a novel Interactive Continual Learning (ICL) framework, enabled by collaborative interactions among models of various sizes. Specifically, we assign the ViT model as System1 and multimodal LLM as System2. To enable the memory module to deduce tasks from class information and enhance Set2Set retrieval, we propose the Class-Knowledge-Task Multi-Head Attention (CKT-MHA). Additionally, to improve memory retrieval in System1 through enhanced geometric r",
    "path": "papers/24/03/2403.02628.json",
    "total_tokens": 811,
    "translated_title": "交互式持续学习: 快速与缓慢思考",
    "translated_abstract": "高级生命形式通过神经认知机制的协同互动，终身不断地获取和传递知识。与此相反，当代机器学习范式在模拟持续学习的方面存在局限性。然而，大型语言模型的出现为通过与这些模型的交互实现持续学习提供了希望。本文基于补充学习系统理论，提出了一种新颖的交互式持续学习（ICL）框架，通过各种规模模型之间的协作交互实现。具体而言，我们将ViT模型指定为第一系统，将多模态LLM指定为第二系统。为了使内存模块能够从类信息中推导任务并增强Set2Set检索，我们提出了Class-Knowledge-Task Multi-Head Attention (CKT-MHA)。此外，为了通过增强的几何检索改进第一系统的内存检索",
    "tldr": "本文提出了一种基于交互的持续学习框架，通过多模型之间的合作交互，实现了更好的任务推导和内存检索。",
    "en_tdlr": "This paper presents an interactive continual learning framework that achieves better task deduction and memory retrieval through collaborative interactions among multiple models."
}