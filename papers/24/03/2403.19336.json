{
    "title": "IVLMap: Instance-Aware Visual Language Grounding for Consumer Robot Navigation",
    "abstract": "arXiv:2403.19336v1 Announce Type: cross  Abstract: Vision-and-Language Navigation (VLN) is a challenging task that requires a robot to navigate in photo-realistic environments with human natural language promptings. Recent studies aim to handle this task by constructing the semantic spatial map representation of the environment, and then leveraging the strong ability of reasoning in large language models for generalizing code for guiding the robot navigation. However, these methods face limitations in instance-level and attribute-level navigation tasks as they cannot distinguish different instances of the same object. To address this challenge, we propose a new method, namely, Instance-aware Visual Language Map (IVLMap), to empower the robot with instance-level and attribute-level semantic mapping, where it is autonomously constructed by fusing the RGBD video data collected from the robot agent with special-designed natural language map indexing in the bird's-in-eye view. Such indexing",
    "link": "https://arxiv.org/abs/2403.19336",
    "context": "Title: IVLMap: Instance-Aware Visual Language Grounding for Consumer Robot Navigation\nAbstract: arXiv:2403.19336v1 Announce Type: cross  Abstract: Vision-and-Language Navigation (VLN) is a challenging task that requires a robot to navigate in photo-realistic environments with human natural language promptings. Recent studies aim to handle this task by constructing the semantic spatial map representation of the environment, and then leveraging the strong ability of reasoning in large language models for generalizing code for guiding the robot navigation. However, these methods face limitations in instance-level and attribute-level navigation tasks as they cannot distinguish different instances of the same object. To address this challenge, we propose a new method, namely, Instance-aware Visual Language Map (IVLMap), to empower the robot with instance-level and attribute-level semantic mapping, where it is autonomously constructed by fusing the RGBD video data collected from the robot agent with special-designed natural language map indexing in the bird's-in-eye view. Such indexing",
    "path": "papers/24/03/2403.19336.json",
    "total_tokens": 832,
    "translated_title": "IVLMap：针对消费级机器人导航的实例感知视觉语言基础",
    "translated_abstract": "arXiv:2403.19336v1 公告类型：跨界摘要：视觉与语言导航（VLN）是一项具有挑战性的任务，需要机器人在真实环境中使用人类自然语言提示进行导航。最近的研究旨在通过构建环境的语义空间地图表示，然后利用大型语言模型在推理方面的强大能力来推广用于引导机器人导航的代码。然而，这些方法在实例级和属性级导航任务中面临限制，因为它们无法区分同一对象的不同实例。为解决这一挑战，我们提出了一种新方法，即实例感知视觉语言地图（IVLMap），以赋予机器人实例级和属性级语义映射，其中通过将机器人代理收集的RGBD视频数据与鸟瞰视角中特别设计的自然语言地图索引融合来自动构建。",
    "tldr": "IVLMap为机器人导航提供了实例级和属性级语义映射能力，通过将RGBD视频数据与特定设计的自然语言地图索引相融合而实现。",
    "en_tdlr": "IVLMap empowers robot navigation with instance-level and attribute-level semantic mapping by fusing RGBD video data with specially-designed natural language map indexing."
}