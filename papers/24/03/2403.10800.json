{
    "title": "Model Reprogramming Outperforms Fine-tuning on Out-of-distribution Data in Text-Image Encoders",
    "abstract": "arXiv:2403.10800v1 Announce Type: new  Abstract: When evaluating the performance of a pre-trained model transferred to a downstream task, it is imperative to assess not only the in-distribution (ID) accuracy of the downstream model but also its capacity to generalize and identify out-of-distribution (OOD) samples. In this paper, we unveil the hidden costs associated with intrusive fine-tuning techniques. Specifically, we demonstrate that commonly used fine-tuning methods not only distort the representations necessary for generalizing to covariate-shifted OOD samples (OOD generalization) but also distort the representations necessary for detecting semantically-shifted OOD samples (OOD detection). To address these challenges, we introduce a new model reprogramming approach for fine-tuning, which we name Reprogrammer. Reprogrammer aims to improve the holistic performance of the downstream model across ID, OOD generalization, and OOD detection tasks. Our empirical evidence reveals that Rep",
    "link": "https://arxiv.org/abs/2403.10800",
    "context": "Title: Model Reprogramming Outperforms Fine-tuning on Out-of-distribution Data in Text-Image Encoders\nAbstract: arXiv:2403.10800v1 Announce Type: new  Abstract: When evaluating the performance of a pre-trained model transferred to a downstream task, it is imperative to assess not only the in-distribution (ID) accuracy of the downstream model but also its capacity to generalize and identify out-of-distribution (OOD) samples. In this paper, we unveil the hidden costs associated with intrusive fine-tuning techniques. Specifically, we demonstrate that commonly used fine-tuning methods not only distort the representations necessary for generalizing to covariate-shifted OOD samples (OOD generalization) but also distort the representations necessary for detecting semantically-shifted OOD samples (OOD detection). To address these challenges, we introduce a new model reprogramming approach for fine-tuning, which we name Reprogrammer. Reprogrammer aims to improve the holistic performance of the downstream model across ID, OOD generalization, and OOD detection tasks. Our empirical evidence reveals that Rep",
    "path": "papers/24/03/2403.10800.json",
    "total_tokens": 872,
    "translated_title": "模型重新编程优于在文本图像编码器中针对分布外数据进行微调",
    "translated_abstract": "在评估将预训练模型转移到下游任务的性能时，不仅需要评估下游模型的分布内（ID）准确性，还需要评估其泛化能力并识别分布外（OOD）样本。本文揭示了侵入性微调技术所带来的隐藏成本。具体来说，我们证明了常用的微调方法不仅扭曲了用于泛化到协变量转移的OOD样本（OOD泛化）所需的表示，还扭曲了用于检测在语义上转移的OOD样本（OOD检测）所需的表示。为了解决这些挑战，我们引入了一种新的模型重新编程方法用于微调，我们将其命名为重新编程器。重新编程器旨在提高下游模型在ID、OOD泛化和OOD检测任务中的整体性能。我们的实证证据显示重新编程器优于对抗性微调与原型微调方法，是一种通用、有效且强大的工具来应对OOD挑战。",
    "tldr": "模型重新编程方法 Reprogrammer 在文本图像编码器中的应用优于传统微调方法，能够提高下游模型在分布内和分布外数据中的性能表现",
    "en_tdlr": "Reprogrammer model reprogramming method outperforms traditional fine-tuning methods in text-image encoders, improving the performance of downstream models on in-distribution and out-of-distribution data."
}