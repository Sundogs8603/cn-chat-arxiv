{
    "title": "Inexact Unlearning Needs More Careful Evaluations to Avoid a False Sense of Privacy",
    "abstract": "arXiv:2403.01218v1 Announce Type: new  Abstract: The high cost of model training makes it increasingly desirable to develop techniques for unlearning. These techniques seek to remove the influence of a training example without having to retrain the model from scratch. Intuitively, once a model has unlearned, an adversary that interacts with the model should no longer be able to tell whether the unlearned example was included in the model's training set or not. In the privacy literature, this is known as membership inference. In this work, we discuss adaptations of Membership Inference Attacks (MIAs) to the setting of unlearning (leading to their ``U-MIA'' counterparts). We propose a categorization of existing U-MIAs into ``population U-MIAs'', where the same attacker is instantiated for all examples, and ``per-example U-MIAs'', where a dedicated attacker is instantiated for each example. We show that the latter category, wherein the attacker tailors its membership prediction to each ex",
    "link": "https://arxiv.org/abs/2403.01218",
    "context": "Title: Inexact Unlearning Needs More Careful Evaluations to Avoid a False Sense of Privacy\nAbstract: arXiv:2403.01218v1 Announce Type: new  Abstract: The high cost of model training makes it increasingly desirable to develop techniques for unlearning. These techniques seek to remove the influence of a training example without having to retrain the model from scratch. Intuitively, once a model has unlearned, an adversary that interacts with the model should no longer be able to tell whether the unlearned example was included in the model's training set or not. In the privacy literature, this is known as membership inference. In this work, we discuss adaptations of Membership Inference Attacks (MIAs) to the setting of unlearning (leading to their ``U-MIA'' counterparts). We propose a categorization of existing U-MIAs into ``population U-MIAs'', where the same attacker is instantiated for all examples, and ``per-example U-MIAs'', where a dedicated attacker is instantiated for each example. We show that the latter category, wherein the attacker tailors its membership prediction to each ex",
    "path": "papers/24/03/2403.01218.json",
    "total_tokens": 887,
    "translated_title": "粗糙反学习需要更加谨慎的评估以避免虚假隐私感知",
    "translated_abstract": "模型训练的高成本使得开发反学习技术变得越来越有吸引力。这些技术旨在删除训练样本的影响，而无需从头重新训练模型。从直觉上讲，一旦模型完成反学习，与该模型交互的对手就不应再能够判断反学习的样本是否包含在模型的训练集中。在隐私领域，这被称为成员推断。在这项工作中，我们讨论了成员推断攻击（MIAs）对反学习设置的调整（导致它们的“U-MIA”对应）。我们提出了现有U-MIA的分类，将其分为“人口U-MIA”，其中同一攻击者适用于所有示例，和“每个示例U-MIA”，其中为每个示例实例化了专用攻击者。我们展示了后一类别，在这种情况下，攻击者为每个实例定制其成员预测。",
    "tldr": "论文讨论了针对反学习环境的成员推断攻击的调整，并提出了现有U-MIA的分类，对每个示例实例化了专用攻击者。",
    "en_tdlr": "This paper discusses adaptations of Membership Inference Attacks (MIAs) to the setting of unlearning, and proposes a categorization of existing U-MIAs where a dedicated attacker is instantiated for each example."
}