{
    "title": "TablePuppet: A Generic Framework for Relational Federated Learning",
    "abstract": "arXiv:2403.15839v1 Announce Type: new  Abstract: Current federated learning (FL) approaches view decentralized training data as a single table, divided among participants either horizontally (by rows) or vertically (by columns). However, these approaches are inadequate for handling distributed relational tables across databases. This scenario requires intricate SQL operations like joins and unions to obtain the training data, which is either costly or restricted by privacy concerns. This raises the question: can we directly run FL on distributed relational tables?   In this paper, we formalize this problem as relational federated learning (RFL). We propose TablePuppet, a generic framework for RFL that decomposes the learning process into two steps: (1) learning over join (LoJ) followed by (2) learning over union (LoU). In a nutshell, LoJ pushes learning down onto the vertical tables being joined, and LoU further pushes learning down onto the horizontal partitions of each vertical table",
    "link": "https://arxiv.org/abs/2403.15839",
    "context": "Title: TablePuppet: A Generic Framework for Relational Federated Learning\nAbstract: arXiv:2403.15839v1 Announce Type: new  Abstract: Current federated learning (FL) approaches view decentralized training data as a single table, divided among participants either horizontally (by rows) or vertically (by columns). However, these approaches are inadequate for handling distributed relational tables across databases. This scenario requires intricate SQL operations like joins and unions to obtain the training data, which is either costly or restricted by privacy concerns. This raises the question: can we directly run FL on distributed relational tables?   In this paper, we formalize this problem as relational federated learning (RFL). We propose TablePuppet, a generic framework for RFL that decomposes the learning process into two steps: (1) learning over join (LoJ) followed by (2) learning over union (LoU). In a nutshell, LoJ pushes learning down onto the vertical tables being joined, and LoU further pushes learning down onto the horizontal partitions of each vertical table",
    "path": "papers/24/03/2403.15839.json",
    "total_tokens": 868,
    "translated_title": "TablePuppet：关系数据联邦学习的通用框架",
    "translated_abstract": "当前的联邦学习（FL）方法将分散的训练数据视为单个表，分为水平方式（按行）或垂直方式（按列）分配给参与者。然而，这些方法无法处理分布在数据库中的关系表。这种情况需要进行复杂的SQL操作，如连接和合并操作，以获得训练数据，这些操作要么成本高昂，要么受到隐私问题的限制。这引出了一个问题：我们能否直接在分布式关系表上运行FL呢？在本文中，我们将这一问题形式化为关系数据联邦学习（RFL）。我们提出了TablePuppet，一个用于RFL的通用框架，将学习过程分解为两个步骤：（1）连接中的学习（LoJ），然后是（2）联合中的学习（LoU）。简而言之，LoJ将学习推向加入的垂直表，LoU进一步将学习推向每个垂直表的水平分区",
    "tldr": "TablePuppet提出了关系数据联邦学习（RFL）的通用框架，使用连接中的学习（LoJ）和联合中的学习（LoU）来处理分布式关系表中的训练数据"
}