{
    "title": "Masked Generative Story Transformer with Character Guidance and Caption Augmentation",
    "abstract": "arXiv:2403.08502v1 Announce Type: cross  Abstract: Story Visualization (SV) is a challenging generative vision task, that requires both visual quality and consistency between different frames in generated image sequences. Previous approaches either employ some kind of memory mechanism to maintain context throughout an auto-regressive generation of the image sequence, or model the generation of the characters and their background separately, to improve the rendering of characters. On the contrary, we embrace a completely parallel transformer-based approach, exclusively relying on Cross-Attention with past and future captions to achieve consistency. Additionally, we propose a Character Guidance technique to focus on the generation of characters in an implicit manner, by forming a combination of text-conditional and character-conditional logits in the logit space. We also employ a caption-augmentation technique, carried out by a Large Language Model (LLM), to enhance the robustness of our",
    "link": "https://arxiv.org/abs/2403.08502",
    "context": "Title: Masked Generative Story Transformer with Character Guidance and Caption Augmentation\nAbstract: arXiv:2403.08502v1 Announce Type: cross  Abstract: Story Visualization (SV) is a challenging generative vision task, that requires both visual quality and consistency between different frames in generated image sequences. Previous approaches either employ some kind of memory mechanism to maintain context throughout an auto-regressive generation of the image sequence, or model the generation of the characters and their background separately, to improve the rendering of characters. On the contrary, we embrace a completely parallel transformer-based approach, exclusively relying on Cross-Attention with past and future captions to achieve consistency. Additionally, we propose a Character Guidance technique to focus on the generation of characters in an implicit manner, by forming a combination of text-conditional and character-conditional logits in the logit space. We also employ a caption-augmentation technique, carried out by a Large Language Model (LLM), to enhance the robustness of our",
    "path": "papers/24/03/2403.08502.json",
    "total_tokens": 705,
    "translated_title": "具有字符引导和标题增强的遮蔽生成故事变换器",
    "translated_abstract": "Story Visualization (SV)是一项具有挑战性的生成视觉任务，既要求生成图像序列的视觉质量，又要求不同帧之间的一致性。我们提出了一种全新的并行变换器方法，通过过去和未来的标题之间的交叉注意力来实现一致性。此外，我们提出了一种字符引导技术，通过在logit空间中形成文本条件和字符条件logits的组合，以隐式地聚焦于角色的生成。我们还采用了一种由大型语言模型（LLM）执行的标题增强技术，以增强我们方法的鲁棒性。",
    "tldr": "该论文提出了一种遮蔽生成故事变换器，利用字符引导和标题增强来实现一致性生成。",
    "en_tdlr": "This paper introduces a masked generative story transformer that achieves consistency generation through character guidance and caption augmentation."
}