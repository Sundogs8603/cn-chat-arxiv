{
    "title": "Teaching MLP More Graph Information: A Three-stage Multitask Knowledge Distillation Framework",
    "abstract": "arXiv:2403.01079v1 Announce Type: cross  Abstract: We study the challenging problem for inference tasks on large-scale graph datasets of Graph Neural Networks: huge time and memory consumption, and try to overcome it by reducing reliance on graph structure. Even though distilling graph knowledge to student MLP is an excellent idea, it faces two major problems of positional information loss and low generalization. To solve the problems, we propose a new three-stage multitask distillation framework. In detail, we use Positional Encoding to capture positional information. Also, we introduce Neural Heat Kernels responsible for graph data processing in GNN and utilize hidden layer outputs matching for better performance of student MLP's hidden layers. To the best of our knowledge, it is the first work to include hidden layer distillation for student MLP on graphs and to combine graph Positional Encoding with MLP. We test its performance and robustness with several settings and draw the conc",
    "link": "https://arxiv.org/abs/2403.01079",
    "context": "Title: Teaching MLP More Graph Information: A Three-stage Multitask Knowledge Distillation Framework\nAbstract: arXiv:2403.01079v1 Announce Type: cross  Abstract: We study the challenging problem for inference tasks on large-scale graph datasets of Graph Neural Networks: huge time and memory consumption, and try to overcome it by reducing reliance on graph structure. Even though distilling graph knowledge to student MLP is an excellent idea, it faces two major problems of positional information loss and low generalization. To solve the problems, we propose a new three-stage multitask distillation framework. In detail, we use Positional Encoding to capture positional information. Also, we introduce Neural Heat Kernels responsible for graph data processing in GNN and utilize hidden layer outputs matching for better performance of student MLP's hidden layers. To the best of our knowledge, it is the first work to include hidden layer distillation for student MLP on graphs and to combine graph Positional Encoding with MLP. We test its performance and robustness with several settings and draw the conc",
    "path": "papers/24/03/2403.01079.json",
    "total_tokens": 883,
    "translated_title": "教授多层感知机更多图信息：三阶段多任务知识蒸馏框架",
    "translated_abstract": "我们研究了图神经网络在大规模图数据集上进行推理任务时面临的挑战：巨大的时间和内存消耗，并尝试通过减少对图结构的依赖来克服这一问题。尽管将图知识蒸馏到学生多层感知机是一个不错的想法，但它面临两个主要问题：位置信息丢失和泛化能力低。为了解决这些问题，我们提出了一种新的三阶段多任务蒸馏框架。具体地，我们使用位置编码来捕捉位置信息。此外，我们引入神经热核来负责图数据处理，在GNN中利用隐藏层输出匹配来提高学生多层感知机的性能。据我们所知，这是首次在图上引入隐藏层蒸馏用于学生多层感知机，并结合图位置编码和多层感知机。我们通过多种设置测试了其性能和稳健性，并得出结论......",
    "tldr": "提出了一个新的三阶段多任务知识蒸馏框架，使用位置编码来捕捉位置信息，引入神经热核处理图数据，通过隐藏层输出匹配提高学生多层感知机的性能。",
    "en_tdlr": "Proposed a new three-stage multitask knowledge distillation framework, using positional encoding to capture positional information, introducing neural heat kernels for graph data processing, and improving student MLP performance through hidden layer output matching."
}