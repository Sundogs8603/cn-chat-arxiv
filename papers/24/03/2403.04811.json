{
    "title": "Quantifying Contamination in Evaluating Code Generation Capabilities of Language Models",
    "abstract": "arXiv:2403.04811v1 Announce Type: cross  Abstract: While large language models have achieved remarkable performance on various code generation benchmarks, there have been growing concerns regarding potential contamination of these benchmarks as they may be leaked into pretraining and finetuning data. While recent work has investigated contamination in natural language generation and understanding tasks, there has been less extensive research into how data contamination impacts the evaluation of code generation, which is critical for understanding the robustness and reliability of LLMs in programming contexts. In this work, we perform a comprehensive study of data contamination of popular code generation benchmarks, and precisely quantify their overlap with pretraining corpus through both surface-level and semantic-level matching. In our experiments, we show that there are substantial overlap between popular code generation benchmarks and open training corpus, and models perform signifi",
    "link": "https://arxiv.org/abs/2403.04811",
    "context": "Title: Quantifying Contamination in Evaluating Code Generation Capabilities of Language Models\nAbstract: arXiv:2403.04811v1 Announce Type: cross  Abstract: While large language models have achieved remarkable performance on various code generation benchmarks, there have been growing concerns regarding potential contamination of these benchmarks as they may be leaked into pretraining and finetuning data. While recent work has investigated contamination in natural language generation and understanding tasks, there has been less extensive research into how data contamination impacts the evaluation of code generation, which is critical for understanding the robustness and reliability of LLMs in programming contexts. In this work, we perform a comprehensive study of data contamination of popular code generation benchmarks, and precisely quantify their overlap with pretraining corpus through both surface-level and semantic-level matching. In our experiments, we show that there are substantial overlap between popular code generation benchmarks and open training corpus, and models perform signifi",
    "path": "papers/24/03/2403.04811.json",
    "total_tokens": 715,
    "translated_title": "量化污染评估语言模型的代码生成能力",
    "translated_abstract": "尽管大型语言模型在各种代码生成基准测试中取得了显著的性能，但人们对这些基准测试的潜在污染日益关注，因为它们可能泄漏到预训练和微调数据中。本文对流行的代码生成基准测试进行了全面研究，准确量化了它们与预训练语料库之间的重叠，通过表面级和语义级匹配。在我们的实验中，我们展示了流行的代码生成基准测试与公开训练语料库之间存在重叠，并且模型表现显着。",
    "tldr": "研究量化了流行的代码生成基准测试的数据污染程度，揭示了它们与预训练语料库之间的重叠，并展示模型的显著性能重叠。",
    "en_tdlr": "This paper quantifies the contamination levels of popular code generation benchmarks, reveals their overlap with pretraining corpus, and demonstrates significant performance overlap in models."
}