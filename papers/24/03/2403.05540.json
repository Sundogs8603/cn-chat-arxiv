{
    "title": "Extinction Risks from AI: Invisible to Science?",
    "abstract": "arXiv:2403.05540v1 Announce Type: cross  Abstract: In an effort to inform the discussion surrounding existential risks from AI, we formulate Extinction-level Goodhart's Law as \"Virtually any goal specification, pursued to the extreme, will result in the extinction of humanity\", and we aim to understand which formal models are suitable for investigating this hypothesis. Note that we remain agnostic as to whether Extinction-level Goodhart's Law holds or not. As our key contribution, we identify a set of conditions that are necessary for a model that aims to be informative for evaluating specific arguments for Extinction-level Goodhart's Law. Since each of the conditions seems to significantly contribute to the complexity of the resulting model, formally evaluating the hypothesis might be exceedingly difficult. This raises the possibility that whether the risk of extinction from artificial intelligence is real or not, the underlying dynamics might be invisible to current scientific method",
    "link": "https://arxiv.org/abs/2403.05540",
    "context": "Title: Extinction Risks from AI: Invisible to Science?\nAbstract: arXiv:2403.05540v1 Announce Type: cross  Abstract: In an effort to inform the discussion surrounding existential risks from AI, we formulate Extinction-level Goodhart's Law as \"Virtually any goal specification, pursued to the extreme, will result in the extinction of humanity\", and we aim to understand which formal models are suitable for investigating this hypothesis. Note that we remain agnostic as to whether Extinction-level Goodhart's Law holds or not. As our key contribution, we identify a set of conditions that are necessary for a model that aims to be informative for evaluating specific arguments for Extinction-level Goodhart's Law. Since each of the conditions seems to significantly contribute to the complexity of the resulting model, formally evaluating the hypothesis might be exceedingly difficult. This raises the possibility that whether the risk of extinction from artificial intelligence is real or not, the underlying dynamics might be invisible to current scientific method",
    "path": "papers/24/03/2403.05540.json",
    "total_tokens": 930,
    "translated_title": "AI的灭绝风险：对科学是否隐形的思考",
    "translated_abstract": "为了讨论AI所带来的灭绝风险，我们提出了灭绝级古哈特定律，即“几乎任何目标规范，如果过度追求，将导致人类的灭绝”，并试图了解哪些形式模型适合研究这一假设。我们对灭绝级古哈特定律是否成立持中立态度。作为我们的主要贡献，我们确定了一组必要的条件，这些条件对于一个旨在评估灭绝级古哈特定律具体论点的模型是必要的。由于每个条件似乎在相当程度上增加了最终模型的复杂性，形式评估这一假设可能会异常困难。这引发了一个可能性，即无论来自人工智能的灭绝风险是否真实，其潜在动态可能在当前科学方法下无法观察到。",
    "tldr": "论文探讨了AI的灭绝风险，并提出了灭绝级古哈特定律，指出任何目标规范过度追求都可能导致人类灭绝，同时指出评估此假设需要满足一定条件，但可能由于模型复杂性而难以实现，暗示了人工智能带来的灭绝风险可能在当前科学方法下难以观察到。",
    "en_tdlr": "The paper discusses the extinction risks from AI and proposes Extinction-level Goodhart's Law, which states that excessively pursuing any goal specification may lead to human extinction. It identifies necessary conditions for evaluating this hypothesis, indicating that the complexity of models may pose challenges, suggesting that the extinction risk from artificial intelligence could be invisible to current scientific methods."
}