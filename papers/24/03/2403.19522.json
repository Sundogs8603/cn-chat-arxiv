{
    "title": "Model Stock: All we need is just a few fine-tuned models",
    "abstract": "arXiv:2403.19522v1 Announce Type: new  Abstract: This paper introduces an efficient fine-tuning method for large pre-trained models, offering strong in-distribution (ID) and out-of-distribution (OOD) performance. Breaking away from traditional practices that need a multitude of fine-tuned models for averaging, our approach employs significantly fewer models to achieve final weights yet yield superior accuracy. Drawing from key insights in the weight space of fine-tuned weights, we uncover a strong link between the performance and proximity to the center of weight space. Based on this, we introduce a method that approximates a center-close weight using only two fine-tuned models, applicable during or after training. Our innovative layer-wise weight averaging technique surpasses state-of-the-art model methods such as Model Soup, utilizing only two fine-tuned models. This strategy can be aptly coined Model Stock, highlighting its reliance on selecting a minimal number of models to draw a ",
    "link": "https://arxiv.org/abs/2403.19522",
    "context": "Title: Model Stock: All we need is just a few fine-tuned models\nAbstract: arXiv:2403.19522v1 Announce Type: new  Abstract: This paper introduces an efficient fine-tuning method for large pre-trained models, offering strong in-distribution (ID) and out-of-distribution (OOD) performance. Breaking away from traditional practices that need a multitude of fine-tuned models for averaging, our approach employs significantly fewer models to achieve final weights yet yield superior accuracy. Drawing from key insights in the weight space of fine-tuned weights, we uncover a strong link between the performance and proximity to the center of weight space. Based on this, we introduce a method that approximates a center-close weight using only two fine-tuned models, applicable during or after training. Our innovative layer-wise weight averaging technique surpasses state-of-the-art model methods such as Model Soup, utilizing only two fine-tuned models. This strategy can be aptly coined Model Stock, highlighting its reliance on selecting a minimal number of models to draw a ",
    "path": "papers/24/03/2403.19522.json",
    "total_tokens": 829,
    "translated_title": "模型库：我们只需要几个经过良好调整的模型",
    "translated_abstract": "本文介绍了一种高效的大型预训练模型微调方法，提供强大的内分布（ID）和外分布（OOD）性能。与需要大量微调模型进行平均的传统做法不同，我们的方法使用更少的模型来获得最终权重，同时产生更高的准确性。从微调权重的权重空间中汲取关键见解，我们揭示了性能和接近权重空间中心的强连接。基于此，我们引入一种方法，通过仅使用两个微调模型来近似中心接近的权重，可在训练期间或之后应用。我们的创新的逐层权重平均技术超越了Model Soup等最先进的模型方法，仅利用两个微调模型。这种策略可以被称为模型库，突出了它依赖于选择少量模型来进行综合的特点。",
    "tldr": "本文提出了一种高效的微调方法，只使用少量模型就能获得优越的性能，通过权重空间和层次加权平均技术超越了现有的模型方法。",
    "en_tdlr": "This paper introduces an efficient fine-tuning method that achieves superior performance using only a few models, surpassing existing model methods through insights into weight space and layer-wise averaging technique."
}