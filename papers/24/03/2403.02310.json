{
    "title": "Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve",
    "abstract": "arXiv:2403.02310v1 Announce Type: new  Abstract: Each LLM serving request goes through two phases. The first is prefill which processes the entire input prompt to produce one output token and the second is decode which generates the rest of output tokens, one-at-a-time. Prefill iterations have high latency but saturate GPU compute due to parallel processing of the input prompt. In contrast, decode iterations have low latency but also low compute utilization because a decode iteration processes only a single token per request. This makes batching highly effective for decodes and consequently for overall throughput. However, batching multiple requests leads to an interleaving of prefill and decode iterations which makes it challenging to achieve both high throughput and low latency.   We introduce an efficient LLM inference scheduler Sarathi-Serve inspired by the techniques we originally proposed for optimizing throughput in Sarathi. Sarathi-Serve leverages chunked-prefills from Sarathi ",
    "link": "https://arxiv.org/abs/2403.02310",
    "context": "Title: Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve\nAbstract: arXiv:2403.02310v1 Announce Type: new  Abstract: Each LLM serving request goes through two phases. The first is prefill which processes the entire input prompt to produce one output token and the second is decode which generates the rest of output tokens, one-at-a-time. Prefill iterations have high latency but saturate GPU compute due to parallel processing of the input prompt. In contrast, decode iterations have low latency but also low compute utilization because a decode iteration processes only a single token per request. This makes batching highly effective for decodes and consequently for overall throughput. However, batching multiple requests leads to an interleaving of prefill and decode iterations which makes it challenging to achieve both high throughput and low latency.   We introduce an efficient LLM inference scheduler Sarathi-Serve inspired by the techniques we originally proposed for optimizing throughput in Sarathi. Sarathi-Serve leverages chunked-prefills from Sarathi ",
    "path": "papers/24/03/2403.02310.json",
    "total_tokens": 889,
    "translated_title": "在LLM推理中平衡吞吐量和延迟权衡的研究：Sarathi-Serve方法",
    "translated_abstract": "每个LLM服务请求经历两个阶段。首先是prefill阶段，处理整个输入提示以生成一个输出标记；第二个是decode阶段，逐个生成其余的输出标记。Prefill迭代具有较高的延迟，但由于输入提示的并行处理，可以使GPU计算饱和。相比之下，decode迭代具有较低的延迟，但也仅使用较低的计算资源，因为每个请求只处理一个标记。这使得对解码来说批处理非常有效，因此对整体吞吐量也很有效。然而，批量处理多个请求会导致prefill和decode迭代交错进行，这使得在实现高吞吐量和低延迟之间的平衡变得具有挑战性。我们引入了一个高效的LLM推理调度程序Sarathi-Serve，灵感来自我们最初为优化Sarathi的吞吐量提出的技术。Sarathi-Serve利用了从Sarathi中引入的分块prefill技术。",
    "tldr": "引入了一种有效的LLM推理调度程序Sarathi-Serve，通过分块预装填技术平衡了GPU计算饱和和单个标记处理的挑战，实现了高吞吐量和低延迟。",
    "en_tdlr": "Introduced an efficient LLM inference scheduler Sarathi-Serve that balances GPU compute saturation and single token processing challenge through chunked-prefill technique, achieving high throughput and low latency."
}