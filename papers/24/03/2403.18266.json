{
    "title": "Branch-Tuning: Balancing Stability and Plasticity for Continual Self-Supervised Learning",
    "abstract": "arXiv:2403.18266v1 Announce Type: new  Abstract: Self-supervised learning (SSL) has emerged as an effective paradigm for deriving general representations from vast amounts of unlabeled data. However, as real-world applications continually integrate new content, the high computational and resource demands of SSL necessitate continual learning rather than complete retraining. This poses a challenge in striking a balance between stability and plasticity when adapting to new information. In this paper, we employ Centered Kernel Alignment for quantitatively analyzing model stability and plasticity, revealing the critical roles of batch normalization layers for stability and convolutional layers for plasticity. Motivated by this, we propose Branch-tuning, an efficient and straightforward method that achieves a balance between stability and plasticity in continual SSL. Branch-tuning consists of branch expansion and compression, and can be easily applied to various SSL methods without the need",
    "link": "https://arxiv.org/abs/2403.18266",
    "context": "Title: Branch-Tuning: Balancing Stability and Plasticity for Continual Self-Supervised Learning\nAbstract: arXiv:2403.18266v1 Announce Type: new  Abstract: Self-supervised learning (SSL) has emerged as an effective paradigm for deriving general representations from vast amounts of unlabeled data. However, as real-world applications continually integrate new content, the high computational and resource demands of SSL necessitate continual learning rather than complete retraining. This poses a challenge in striking a balance between stability and plasticity when adapting to new information. In this paper, we employ Centered Kernel Alignment for quantitatively analyzing model stability and plasticity, revealing the critical roles of batch normalization layers for stability and convolutional layers for plasticity. Motivated by this, we propose Branch-tuning, an efficient and straightforward method that achieves a balance between stability and plasticity in continual SSL. Branch-tuning consists of branch expansion and compression, and can be easily applied to various SSL methods without the need",
    "path": "papers/24/03/2403.18266.json",
    "total_tokens": 782,
    "translated_title": "分支调整：在持续自监督学习中平衡稳定性和可塑性",
    "translated_abstract": "自监督学习（SSL）已经成为从大量未标记数据中得出通用表示的有效范式。然而，随着现实应用不断整合新内容，SSL的高计算和资源需求需要持续学习而不是完全重新训练。本文利用中心核对齐来定量分析模型的稳定性和可塑性，揭示了批归一化层对稳定性和卷积层对可塑性的关键作用。在此基础上，我们提出了一种高效简单的方法，即分支调整，实现了在持续SSL中稳定性和可塑性之间的平衡。分支调整由分支扩展和压缩组成，并且可以轻松应用于各种SSL方法而不需要",
    "tldr": "本文提出了一种名为分支调整的方法，在持续自监督学习中实现了稳定性和可塑性的平衡，通过分支扩展和压缩来实现。",
    "en_tdlr": "This paper proposes a method called Branch-tuning that achieves a balance between stability and plasticity in continual self-supervised learning through branch expansion and compression."
}