{
    "title": "Boosting Adversarial Training via Fisher-Rao Norm-based Regularization",
    "abstract": "arXiv:2403.17520v1 Announce Type: new  Abstract: Adversarial training is extensively utilized to improve the adversarial robustness of deep neural networks. Yet, mitigating the degradation of standard generalization performance in adversarial-trained models remains an open problem. This paper attempts to resolve this issue through the lens of model complexity. First, We leverage the Fisher-Rao norm, a geometrically invariant metric for model complexity, to establish the non-trivial bounds of the Cross-Entropy Loss-based Rademacher complexity for a ReLU-activated Multi-Layer Perceptron. Then we generalize a complexity-related variable, which is sensitive to the changes in model width and the trade-off factors in adversarial training. Moreover, intensive empirical evidence validates that this variable highly correlates with the generalization gap of Cross-Entropy loss between adversarial-trained and standard-trained models, especially during the initial and final phases of the training p",
    "link": "https://arxiv.org/abs/2403.17520",
    "context": "Title: Boosting Adversarial Training via Fisher-Rao Norm-based Regularization\nAbstract: arXiv:2403.17520v1 Announce Type: new  Abstract: Adversarial training is extensively utilized to improve the adversarial robustness of deep neural networks. Yet, mitigating the degradation of standard generalization performance in adversarial-trained models remains an open problem. This paper attempts to resolve this issue through the lens of model complexity. First, We leverage the Fisher-Rao norm, a geometrically invariant metric for model complexity, to establish the non-trivial bounds of the Cross-Entropy Loss-based Rademacher complexity for a ReLU-activated Multi-Layer Perceptron. Then we generalize a complexity-related variable, which is sensitive to the changes in model width and the trade-off factors in adversarial training. Moreover, intensive empirical evidence validates that this variable highly correlates with the generalization gap of Cross-Entropy loss between adversarial-trained and standard-trained models, especially during the initial and final phases of the training p",
    "path": "papers/24/03/2403.17520.json",
    "total_tokens": 853,
    "translated_title": "通过Fisher-Rao范数正则化提升对抗训练",
    "translated_abstract": "对抗训练被广泛应用于提高深度神经网络的对抗鲁棒性。然而，在对抗训练模型中减轻标准泛化性能的下降仍然是一个悬而未决的问题。本文试图通过模型复杂性的视角解决这个问题。首先，我们利用Fisher-Rao范数，一个在模型复杂性方面的几何不变度量，建立了基于ReLU激活的多层感知器的Cross-Entropy Loss-based Rademacher复杂度的非平凡界限。然后我们推广了一个与模型宽度变化和对抗训练的权衡因素敏感相关的复杂性相关变量。此外，大量实证证据表明，此变量与对抗训练和标准训练模型之间的Cross-Entropy loss的泛化差距高度相关，特别是在训练的初始和最终阶段。",
    "tldr": "通过Fisher-Rao范数正则化，本研究在对抗训练中提出了一种解决标准泛化性能下降问题的方法，并通过模型复杂性角度对此进行了理论和实证分析。",
    "en_tdlr": "This study proposes a method to address the degradation of standard generalization performance in adversarial training using Fisher-Rao norm-based regularization, analyzed theoretically and empirically from the perspective of model complexity."
}