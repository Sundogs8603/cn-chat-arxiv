{
    "title": "Simulating Weighted Automata over Sequences and Trees with Transformers",
    "abstract": "arXiv:2403.09728v1 Announce Type: cross  Abstract: Transformers are ubiquitous models in the natural language processing (NLP) community and have shown impressive empirical successes in the past few years. However, little is understood about how they reason and the limits of their computational capabilities. These models do not process data sequentially, and yet outperform sequential neural models such as RNNs. Recent work has shown that these models can compactly simulate the sequential reasoning abilities of deterministic finite automata (DFAs). This leads to the following question: can transformers simulate the reasoning of more complex finite state machines? In this work, we show that transformers can simulate weighted finite automata (WFAs), a class of models which subsumes DFAs, as well as weighted tree automata (WTA), a generalization of weighted automata to tree structured inputs. We prove these claims formally and provide upper bounds on the sizes of the transformer models nee",
    "link": "https://arxiv.org/abs/2403.09728",
    "context": "Title: Simulating Weighted Automata over Sequences and Trees with Transformers\nAbstract: arXiv:2403.09728v1 Announce Type: cross  Abstract: Transformers are ubiquitous models in the natural language processing (NLP) community and have shown impressive empirical successes in the past few years. However, little is understood about how they reason and the limits of their computational capabilities. These models do not process data sequentially, and yet outperform sequential neural models such as RNNs. Recent work has shown that these models can compactly simulate the sequential reasoning abilities of deterministic finite automata (DFAs). This leads to the following question: can transformers simulate the reasoning of more complex finite state machines? In this work, we show that transformers can simulate weighted finite automata (WFAs), a class of models which subsumes DFAs, as well as weighted tree automata (WTA), a generalization of weighted automata to tree structured inputs. We prove these claims formally and provide upper bounds on the sizes of the transformer models nee",
    "path": "papers/24/03/2403.09728.json",
    "total_tokens": 805,
    "translated_title": "使用变压器模拟序列和树上的加权自动机",
    "translated_abstract": "变压器是自然语言处理（NLP）社区中无处不在的模型，在过去几年中展示出令人印象深刻的经验成功。然而，关于它们推理的方式以及计算能力的限制，人们对此知之甚少。这些模型不是按顺序处理数据，却胜过诸如RNN的顺序神经模型。最近的研究表明，这些模型可以紧凑地模拟确定性有限自动机（DFAs）的序列推理能力。这带来了一个问题：变压器能否模拟更复杂的有限状态机的推理？在这项工作中，我们展示变压器可以模拟加权有限自动机（WFAs），这是一类包含DFAs的模型，以及加权树自动机（WTA），一种加权自动机推广到树形输入的模型。我们正式证明了这些说法，并给出了所需变压器模型大小的上界。",
    "tldr": "变压器可以模拟加权有限自动机和加权树自动机，拓展了它们的应用范围。"
}