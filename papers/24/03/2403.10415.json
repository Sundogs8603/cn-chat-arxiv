{
    "title": "Gradient based Feature Attribution in Explainable AI: A Technical Review",
    "abstract": "arXiv:2403.10415v1 Announce Type: new  Abstract: The surge in black-box AI models has prompted the need to explain the internal mechanism and justify their reliability, especially in high-stakes applications, such as healthcare and autonomous driving. Due to the lack of a rigorous definition of explainable AI (XAI), a plethora of research related to explainability, interpretability, and transparency has been developed to explain and analyze the model from various perspectives. Consequently, with an exhaustive list of papers, it becomes challenging to have a comprehensive overview of XAI research from all aspects. Considering the popularity of neural networks in AI research, we narrow our focus to a specific area of XAI research: gradient based explanations, which can be directly adopted for neural network models. In this review, we systematically explore gradient based explanation methods to date and introduce a novel taxonomy to categorize them into four distinct classes. Then, we pre",
    "link": "https://arxiv.org/abs/2403.10415",
    "context": "Title: Gradient based Feature Attribution in Explainable AI: A Technical Review\nAbstract: arXiv:2403.10415v1 Announce Type: new  Abstract: The surge in black-box AI models has prompted the need to explain the internal mechanism and justify their reliability, especially in high-stakes applications, such as healthcare and autonomous driving. Due to the lack of a rigorous definition of explainable AI (XAI), a plethora of research related to explainability, interpretability, and transparency has been developed to explain and analyze the model from various perspectives. Consequently, with an exhaustive list of papers, it becomes challenging to have a comprehensive overview of XAI research from all aspects. Considering the popularity of neural networks in AI research, we narrow our focus to a specific area of XAI research: gradient based explanations, which can be directly adopted for neural network models. In this review, we systematically explore gradient based explanation methods to date and introduce a novel taxonomy to categorize them into four distinct classes. Then, we pre",
    "path": "papers/24/03/2403.10415.json",
    "total_tokens": 863,
    "translated_title": "基于梯度的特征归因在可解释人工智能中的技术评估",
    "translated_abstract": "arXiv:2403.10415v1 公布类型：新摘要：黑盒人工智能模型的激增促使了解释内部机制并证明其可靠性的需求，特别是在高风险应用领域，如医疗保健和自动驾驶。由于缺乏可解释人工智能（XAI）的严格定义，已经出现了大量与解释性、可解释性和透明性相关的研究，以从各种角度解释和分析模型。因此，由于存在大量的论文，全面了解XAI研究的各个方面变得具有挑战性。考虑到神经网络在人工智能研究中的流行，我们将焦点缩小到XAI研究的一个特定领域：基于梯度的解释，这可以直接应用于神经网络模型。在本评估中，我们系统地探索了迄今为止基于梯度的解释方法，并引入了一个新颖的分类法将其归类为四个不同类别。",
    "tldr": "这项技术评估系统地探讨了基于梯度的解释方法，并引入了一个新颖的分类法将其归类为四个不同类别"
}