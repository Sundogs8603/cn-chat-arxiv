{
    "title": "Multi-Stage Multi-Modal Pre-Training for Automatic Speech Recognition",
    "abstract": "arXiv:2403.19822v1 Announce Type: cross  Abstract: Recent advances in machine learning have demonstrated that multi-modal pre-training can improve automatic speech recognition (ASR) performance compared to randomly initialized models, even when models are fine-tuned on uni-modal tasks. Existing multi-modal pre-training methods for the ASR task have primarily focused on single-stage pre-training where a single unsupervised task is used for pre-training followed by fine-tuning on the downstream task. In this work, we introduce a novel method combining multi-modal and multi-task unsupervised pre-training with a translation-based supervised mid-training approach. We empirically demonstrate that such a multi-stage approach leads to relative word error rate (WER) improvements of up to 38.45% over baselines on both Librispeech and SUPERB. Additionally, we share several important findings for choosing pre-training methods and datasets.",
    "link": "https://arxiv.org/abs/2403.19822",
    "context": "Title: Multi-Stage Multi-Modal Pre-Training for Automatic Speech Recognition\nAbstract: arXiv:2403.19822v1 Announce Type: cross  Abstract: Recent advances in machine learning have demonstrated that multi-modal pre-training can improve automatic speech recognition (ASR) performance compared to randomly initialized models, even when models are fine-tuned on uni-modal tasks. Existing multi-modal pre-training methods for the ASR task have primarily focused on single-stage pre-training where a single unsupervised task is used for pre-training followed by fine-tuning on the downstream task. In this work, we introduce a novel method combining multi-modal and multi-task unsupervised pre-training with a translation-based supervised mid-training approach. We empirically demonstrate that such a multi-stage approach leads to relative word error rate (WER) improvements of up to 38.45% over baselines on both Librispeech and SUPERB. Additionally, we share several important findings for choosing pre-training methods and datasets.",
    "path": "papers/24/03/2403.19822.json",
    "total_tokens": 827,
    "translated_title": "多阶段多模态预训练用于自动语音识别",
    "translated_abstract": "机器学习的最新进展表明，与随机初始化模型相比，多模态预训练可以提高自动语音识别（ASR）性能，即使模型在单模态任务上进行微调。现有的用于ASR任务的多模态预训练方法主要集中在单阶段预训练，即使用单个无监督任务进行预训练，然后在下游任务上进行微调。在这项工作中，我们介绍了一种将多模态和多任务无监督预训练与基于翻译的监督中间训练方法相结合的新方法。我们在实验中证明，这种多阶段方法在Librispeech和SUPERB上相对词错误率（WER）的改进最高达38.45％。此外，我们分享了选择预训练方法和数据集的一些重要发现。",
    "tldr": "提出了一种结合多模态和多任务无监督预训练以及基于翻译的监督中间训练方法的新方法，能够在Librispeech和SUPERB上相对提高高达38.45%的词错误率（WER）。",
    "en_tdlr": "Introducing a novel method combining multi-modal and multi-task unsupervised pre-training with a translation-based supervised mid-training approach, leading to relative word error rate (WER) improvements of up to 38.45% on both Librispeech and SUPERB."
}