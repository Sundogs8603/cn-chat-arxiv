{
    "title": "Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning",
    "abstract": "arXiv:2403.19962v1 Announce Type: cross  Abstract: Open-source pre-trained Large Language Models (LLMs) exhibit strong language understanding and generation capabilities, making them highly successful in a variety of tasks. However, when used as agents for dealing with complex problems in the real world, their performance is far inferior to large commercial models such as ChatGPT and GPT-4. As intelligent agents, LLMs need to have the capabilities of task planning, long-term memory, and the ability to leverage external tools to achieve satisfactory performance. Various methods have been proposed to enhance the agent capabilities of LLMs. On the one hand, methods involve constructing agent-specific data and fine-tuning the models. On the other hand, some methods focus on designing prompts that effectively activate the reasoning abilities of the LLMs. We explore both strategies on the 7B and 13B models. We propose a comprehensive method for constructing agent-specific data using GPT-4. T",
    "link": "https://arxiv.org/abs/2403.19962",
    "context": "Title: Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning\nAbstract: arXiv:2403.19962v1 Announce Type: cross  Abstract: Open-source pre-trained Large Language Models (LLMs) exhibit strong language understanding and generation capabilities, making them highly successful in a variety of tasks. However, when used as agents for dealing with complex problems in the real world, their performance is far inferior to large commercial models such as ChatGPT and GPT-4. As intelligent agents, LLMs need to have the capabilities of task planning, long-term memory, and the ability to leverage external tools to achieve satisfactory performance. Various methods have been proposed to enhance the agent capabilities of LLMs. On the one hand, methods involve constructing agent-specific data and fine-tuning the models. On the other hand, some methods focus on designing prompts that effectively activate the reasoning abilities of the LLMs. We explore both strategies on the 7B and 13B models. We propose a comprehensive method for constructing agent-specific data using GPT-4. T",
    "path": "papers/24/03/2403.19962.json",
    "total_tokens": 886,
    "translated_title": "通过调整和多支路推理增强低参数LLMs的通用代理功能",
    "translated_abstract": "arXiv:2403.19962v1 声明类型: 跨领域 摘要: 开源预训练的大型语言模型（LLM）表现出强大的语言理解和生成能力，使它们在各种任务中非常成功。然而，当将它们用作处理现实世界复杂问题的代理时，它们的性能远远不及ChatGPT和GPT-4等大型商用模型。作为智能代理，LLMs需要具备任务规划、长期记忆以及利用外部工具实现令人满意的性能的能力。各种方法已被提出来增强LLMs的代理能力。一方面，有些方法涉及构建特定于代理的数据和微调模型。另一方面，一些方法集中于设计能有效激活LLMs推理能力的提示。我们在7B和13B模型上同时探讨了这两种策略。我们提出了一种使用GPT-4构建特定于代理数据的全面方法。",
    "tldr": "通过构建特定于代理的数据并细调模型以及设计能够有效激活LLMs推理能力的提示，提出了一种综合方法来增强低参数LLMs的通用代理功能。",
    "en_tdlr": "A comprehensive method is proposed to enhance the general agent capabilities of low-parameter LLMs by constructing agent-specific data, fine-tuning models, and designing prompts that effectively activate the reasoning abilities of the LLMs."
}