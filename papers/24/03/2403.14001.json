{
    "title": "Evaluating Unsupervised Dimensionality Reduction Methods for Pretrained Sentence Embeddings",
    "abstract": "arXiv:2403.14001v1 Announce Type: new  Abstract: Sentence embeddings produced by Pretrained Language Models (PLMs) have received wide attention from the NLP community due to their superior performance when representing texts in numerous downstream applications. However, the high dimensionality of the sentence embeddings produced by PLMs is problematic when representing large numbers of sentences in memory- or compute-constrained devices. As a solution, we evaluate unsupervised dimensionality reduction methods to reduce the dimensionality of sentence embeddings produced by PLMs. Our experimental results show that simple methods such as Principal Component Analysis (PCA) can reduce the dimensionality of sentence embeddings by almost $50\\%$, without incurring a significant loss in performance in multiple downstream tasks. Surprisingly, reducing the dimensionality further improves performance over the original high-dimensional versions for the sentence embeddings produced by some PLMs in s",
    "link": "https://arxiv.org/abs/2403.14001",
    "context": "Title: Evaluating Unsupervised Dimensionality Reduction Methods for Pretrained Sentence Embeddings\nAbstract: arXiv:2403.14001v1 Announce Type: new  Abstract: Sentence embeddings produced by Pretrained Language Models (PLMs) have received wide attention from the NLP community due to their superior performance when representing texts in numerous downstream applications. However, the high dimensionality of the sentence embeddings produced by PLMs is problematic when representing large numbers of sentences in memory- or compute-constrained devices. As a solution, we evaluate unsupervised dimensionality reduction methods to reduce the dimensionality of sentence embeddings produced by PLMs. Our experimental results show that simple methods such as Principal Component Analysis (PCA) can reduce the dimensionality of sentence embeddings by almost $50\\%$, without incurring a significant loss in performance in multiple downstream tasks. Surprisingly, reducing the dimensionality further improves performance over the original high-dimensional versions for the sentence embeddings produced by some PLMs in s",
    "path": "papers/24/03/2403.14001.json",
    "total_tokens": 814,
    "translated_title": "评估预训练句子嵌入的无监督降维方法",
    "translated_abstract": "预训练语言模型（PLMs）生成的句子嵌入由于在众多下游应用中代表文本时表现出色，受到NLP社区的广泛关注。然而，PLMs生成的句子嵌入的高维度在代表大量句子时存在问题，特别是在内存或计算受限设备上。作为解决方案，我们评估了无监督降维方法，以减小PLMs生成的句子嵌入的维度。实验结果表明，简单的方法如主成分分析（PCA）可以将句子嵌入的维度减少约50％，在多个下游任务中性能未受到显着损失。令人惊讶的是，对一些PLMs生成的句子嵌入进一步降低维度可以改善性能，超过原始高维版本的性能。",
    "tldr": "评估使用无监督降维方法减小预训练语言模型生成句子嵌入维度，并发现对于一些模型，在降维后性能反而提高了。",
    "en_tdlr": "Evaluating unsupervised dimensionality reduction methods for reducing the dimensionality of sentence embeddings produced by pretrained models, and finding that for some models, performance actually improves after dimensionality reduction."
}