{
    "title": "Deep Submodular Peripteral Network",
    "abstract": "arXiv:2403.08199v1 Announce Type: cross  Abstract: Submodular functions, crucial for various applications, often lack practical learning methods for their acquisition. Seemingly unrelated, learning a scaling from oracles offering graded pairwise preferences (GPC) is underexplored, despite a rich history in psychometrics. In this paper, we introduce deep submodular peripteral networks (DSPNs), a novel parametric family of submodular functions, and methods for their training using a contrastive-learning inspired GPC-ready strategy to connect and then tackle both of the above challenges. We introduce newly devised GPC-style \"peripteral\" loss which leverages numerically graded relationships between pairs of objects (sets in our case). Unlike traditional contrastive learning, our method utilizes graded comparisons, extracting more nuanced information than just binary-outcome comparisons, and contrasts sets of any size (not just two). We also define a novel suite of automatic sampling strate",
    "link": "https://arxiv.org/abs/2403.08199",
    "context": "Title: Deep Submodular Peripteral Network\nAbstract: arXiv:2403.08199v1 Announce Type: cross  Abstract: Submodular functions, crucial for various applications, often lack practical learning methods for their acquisition. Seemingly unrelated, learning a scaling from oracles offering graded pairwise preferences (GPC) is underexplored, despite a rich history in psychometrics. In this paper, we introduce deep submodular peripteral networks (DSPNs), a novel parametric family of submodular functions, and methods for their training using a contrastive-learning inspired GPC-ready strategy to connect and then tackle both of the above challenges. We introduce newly devised GPC-style \"peripteral\" loss which leverages numerically graded relationships between pairs of objects (sets in our case). Unlike traditional contrastive learning, our method utilizes graded comparisons, extracting more nuanced information than just binary-outcome comparisons, and contrasts sets of any size (not just two). We also define a novel suite of automatic sampling strate",
    "path": "papers/24/03/2403.08199.json",
    "total_tokens": 664,
    "translated_title": "深度子模逆点网络",
    "translated_abstract": "子模函数对各种应用至关重要，但通常缺乏实用的学习方法来获取它们。本文引入了深度子模逆点网络（DSPNs），一种新颖的子模函数参数化族，并提出了使用对比学习启发的GPC-ready策略对其进行训练的方法，以连接并解决上述两个挑战。",
    "tldr": "引入了深度子模逆点网络（DSPNs），并提出了一种使用对比学习启发的GPC-ready策略进行训练的方法，以应对子模函数学习中的两大挑战。",
    "en_tdlr": "Introduced deep submodular peripteral networks (DSPNs) and proposed a training method using a contrastive-learning inspired GPC-ready strategy to address the two main challenges in submodular function learning."
}