{
    "title": "Parametric Encoding with Attention and Convolution Mitigate Spectral Bias of Neural Partial Differential Equation Solvers",
    "abstract": "arXiv:2403.15652v1 Announce Type: new  Abstract: Deep neural networks (DNNs) are increasingly used to solve partial differential equations (PDEs) that naturally arise while modeling a wide range of systems and physical phenomena. However, the accuracy of such DNNs decreases as the PDE complexity increases and they also suffer from spectral bias as they tend to learn the low-frequency solution characteristics. To address these issues, we introduce Parametric Grid Convolutional Attention Networks (PGCANs) that can solve PDE systems without leveraging any labeled data in the domain. The main idea of PGCAN is to parameterize the input space with a grid-based encoder whose parameters are connected to the output via a DNN decoder that leverages attention to prioritize feature training. Our encoder provides a localized learning ability and uses convolution layers to avoid overfitting and improve information propagation rate from the boundaries to the interior of the domain. We test the perfor",
    "link": "https://arxiv.org/abs/2403.15652",
    "context": "Title: Parametric Encoding with Attention and Convolution Mitigate Spectral Bias of Neural Partial Differential Equation Solvers\nAbstract: arXiv:2403.15652v1 Announce Type: new  Abstract: Deep neural networks (DNNs) are increasingly used to solve partial differential equations (PDEs) that naturally arise while modeling a wide range of systems and physical phenomena. However, the accuracy of such DNNs decreases as the PDE complexity increases and they also suffer from spectral bias as they tend to learn the low-frequency solution characteristics. To address these issues, we introduce Parametric Grid Convolutional Attention Networks (PGCANs) that can solve PDE systems without leveraging any labeled data in the domain. The main idea of PGCAN is to parameterize the input space with a grid-based encoder whose parameters are connected to the output via a DNN decoder that leverages attention to prioritize feature training. Our encoder provides a localized learning ability and uses convolution layers to avoid overfitting and improve information propagation rate from the boundaries to the interior of the domain. We test the perfor",
    "path": "papers/24/03/2403.15652.json",
    "total_tokens": 879,
    "translated_title": "使用注意力和卷积的参数化编码缓解神经偏微分方程求解器的谱偏差",
    "translated_abstract": "深度神经网络(DNNs)越来越多地用于解决在建模各种系统和物理现象时自然产生的偏微分方程(PDEs)。然而，随着PDE复杂性的增加，这些DNN的准确性会降低，它们也会受到谱偏差的影响，因为它们倾向于学习低频解的特征。为了解决这些问题，我们引入了Parametric Grid Convolutional Attention Networks (PGCANs)，可以在领域不依赖任何标记数据的情况下解决PDE系统。PGCAN的主要思想是用基于网格的编码器对输入空间进行参数化，其参数通过一个DNN解码器与输出相连接，后者利用注意力来优先选择特征进行训练。我们的编码器提供了本地化学习能力，并使用卷积层来避免过拟合，改善从边界到域内部的信息传播速率。",
    "tldr": "引入了Parametric Grid Convolutional Attention Networks（PGCANs），通过使用参数化编码器和注意力来解决偏微分方程系统，提高解决器的泛化性能和信息传播速率",
    "en_tdlr": "Introduced Parametric Grid Convolutional Attention Networks (PGCANs) to solve PDE systems by utilizing a parametric grid-based encoder and attention mechanism, improving the generalization performance and information propagation rate of the solver."
}