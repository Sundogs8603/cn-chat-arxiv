{
    "title": "Forward Learning of Graph Neural Networks",
    "abstract": "arXiv:2403.11004v1 Announce Type: new  Abstract: Graph neural networks (GNNs) have achieved remarkable success across a wide range of applications, such as recommendation, drug discovery, and question answering. Behind the success of GNNs lies the backpropagation (BP) algorithm, which is the de facto standard for training deep neural networks (NNs). However, despite its effectiveness, BP imposes several constraints, which are not only biologically implausible, but also limit the scalability, parallelism, and flexibility in learning NNs. Examples of such constraints include storage of neural activities computed in the forward pass for use in the subsequent backward pass, and the dependence of parameter updates on non-local signals. To address these limitations, the forward-forward algorithm (FF) was recently proposed as an alternative to BP in the image classification domain, which trains NNs by performing two forward passes over positive and negative data. Inspired by this advance, we ",
    "link": "https://arxiv.org/abs/2403.11004",
    "context": "Title: Forward Learning of Graph Neural Networks\nAbstract: arXiv:2403.11004v1 Announce Type: new  Abstract: Graph neural networks (GNNs) have achieved remarkable success across a wide range of applications, such as recommendation, drug discovery, and question answering. Behind the success of GNNs lies the backpropagation (BP) algorithm, which is the de facto standard for training deep neural networks (NNs). However, despite its effectiveness, BP imposes several constraints, which are not only biologically implausible, but also limit the scalability, parallelism, and flexibility in learning NNs. Examples of such constraints include storage of neural activities computed in the forward pass for use in the subsequent backward pass, and the dependence of parameter updates on non-local signals. To address these limitations, the forward-forward algorithm (FF) was recently proposed as an alternative to BP in the image classification domain, which trains NNs by performing two forward passes over positive and negative data. Inspired by this advance, we ",
    "path": "papers/24/03/2403.11004.json",
    "total_tokens": 732,
    "translated_title": "图神经网络的前向学习",
    "translated_abstract": "图神经网络（GNNs）在推荐系统、药物发现和问答等领域取得了显著的成功。在GNNs的成功背后，是反向传播（BP）算法，这是训练深度神经网络（NNs）的事实标准。然而，尽管BP的有效性，它还是存在一些限制，不仅在生物上不合理，而且限制了学习NNs的可扩展性、并行性和灵活性。为了解决这些限制，最近在图像分类领域提出了前向正向（FF）算法作为BP的替代方法，通过在正负数据上执行两次前向传递来训练NNs。",
    "tldr": "图神经网络的成功依赖于反向传播算法，但其存在一些限制，为此提出了前向正向算法作为一种替代方法。",
    "en_tdlr": "The success of graph neural networks relies on the backpropagation algorithm, which has limitations addressed by the proposed forward-forward algorithm as an alternative."
}