{
    "title": "Efficient Reinforcement Learning for Global Decision Making in the Presence of Local Agents at Scale",
    "abstract": "arXiv:2403.00222v1 Announce Type: new  Abstract: We study reinforcement learning for global decision-making in the presence of many local agents, where the global decision-maker makes decisions affecting all local agents, and the objective is to learn a policy that maximizes the rewards of both the global and the local agents. Such problems find many applications, e.g. demand response, EV charging, queueing, etc. In this setting, scalability has been a long-standing challenge due to the size of the state/action space which can be exponential in the number of agents. This work proposes the SUB-SAMPLE-Q algorithm where the global agent subsamples $k\\leq n$ local agents to compute an optimal policy in time that is only exponential in $k$, providing an exponential speedup from standard methods that are exponential in $n$. We show that the learned policy converges to the optimal policy in the order of $\\tilde{O}(1/\\sqrt{k}+\\epsilon_{k,m})$ as the number of sub-sampled agents $k$ increases, ",
    "link": "https://arxiv.org/abs/2403.00222",
    "context": "Title: Efficient Reinforcement Learning for Global Decision Making in the Presence of Local Agents at Scale\nAbstract: arXiv:2403.00222v1 Announce Type: new  Abstract: We study reinforcement learning for global decision-making in the presence of many local agents, where the global decision-maker makes decisions affecting all local agents, and the objective is to learn a policy that maximizes the rewards of both the global and the local agents. Such problems find many applications, e.g. demand response, EV charging, queueing, etc. In this setting, scalability has been a long-standing challenge due to the size of the state/action space which can be exponential in the number of agents. This work proposes the SUB-SAMPLE-Q algorithm where the global agent subsamples $k\\leq n$ local agents to compute an optimal policy in time that is only exponential in $k$, providing an exponential speedup from standard methods that are exponential in $n$. We show that the learned policy converges to the optimal policy in the order of $\\tilde{O}(1/\\sqrt{k}+\\epsilon_{k,m})$ as the number of sub-sampled agents $k$ increases, ",
    "path": "papers/24/03/2403.00222.json",
    "total_tokens": 882,
    "translated_title": "存在大规模局部代理的全局决策高效强化学习",
    "translated_abstract": "我们研究了存在许多局部代理的全局决策的强化学习问题，其中全局决策者做出影响所有局部代理的决策，目标是学习一个最大化全局和局部代理奖励的策略。在这种情况下，可扩展性一直是一个长期存在的挑战，因为状态/动作空间的大小可能会随代理数量指数增长。本文提出了SUB-SAMPLE-Q算法，在此算法中，全局代理对$k\\leq n$个局部代理进行子采样以在仅指数于$k$的时间内计算出最佳策略，从而提供了与指数于$n$的标准方法相比的指数加速。我们展示了随着子采样代理数$k$的增加，学到的策略将收敛于顺序为$\\tilde{O}(1/\\sqrt{k}+\\epsilon_{k,m})$的最优策略。",
    "tldr": "该研究提出了SUB-SAMPLE-Q算法，通过对局部代理进行子采样，在指数级别的时间内计算出最佳策略，从而实现了与标准方法相比的指数加速。",
    "en_tdlr": "The study introduces the SUB-SAMPLE-Q algorithm, which achieves an exponential speedup compared to standard methods by subsampling local agents and computing the optimal policy in exponential time only dependent on the subsampled agents."
}