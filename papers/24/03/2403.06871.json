{
    "title": "On the Generalization Ability of Unsupervised Pretraining",
    "abstract": "arXiv:2403.06871v1 Announce Type: new  Abstract: Recent advances in unsupervised learning have shown that unsupervised pre-training, followed by fine-tuning, can improve model generalization. However, a rigorous understanding of how the representation function learned on an unlabeled dataset affects the generalization of the fine-tuned model is lacking. Existing theoretical research does not adequately account for the heterogeneity of the distribution and tasks in pre-training and fine-tuning stage. To bridge this gap, this paper introduces a novel theoretical framework that illuminates the critical factor influencing the transferability of knowledge acquired during unsupervised pre-training to the subsequent fine-tuning phase, ultimately affecting the generalization capabilities of the fine-tuned model on downstream tasks. We apply our theoretical framework to analyze generalization bound of two distinct scenarios: Context Encoder pre-training with deep neural networks and Masked Auto",
    "link": "https://arxiv.org/abs/2403.06871",
    "context": "Title: On the Generalization Ability of Unsupervised Pretraining\nAbstract: arXiv:2403.06871v1 Announce Type: new  Abstract: Recent advances in unsupervised learning have shown that unsupervised pre-training, followed by fine-tuning, can improve model generalization. However, a rigorous understanding of how the representation function learned on an unlabeled dataset affects the generalization of the fine-tuned model is lacking. Existing theoretical research does not adequately account for the heterogeneity of the distribution and tasks in pre-training and fine-tuning stage. To bridge this gap, this paper introduces a novel theoretical framework that illuminates the critical factor influencing the transferability of knowledge acquired during unsupervised pre-training to the subsequent fine-tuning phase, ultimately affecting the generalization capabilities of the fine-tuned model on downstream tasks. We apply our theoretical framework to analyze generalization bound of two distinct scenarios: Context Encoder pre-training with deep neural networks and Masked Auto",
    "path": "papers/24/03/2403.06871.json",
    "total_tokens": 782,
    "translated_title": "关于无监督预训练的泛化能力",
    "translated_abstract": "无监督学习的最新进展表明，无监督预训练，然后进行微调，可以提高模型的泛化能力。然而，目前对于在未标记数据集上学习的表示函数如何影响微调模型的泛化能力缺乏严格的理解。现有理论研究未能充分考虑预训练和微调阶段的分布和任务的异质性。为填补这一空白，本文引入了一个新颖的理论框架，阐明了影响从无监督预训练获得的知识在随后的微调阶段的可传递性的关键因素，最终影响了微调模型在下游任务上的泛化能力。我们应用我们的理论框架来分析两种不同情景的泛化界限：使用深度神经网络进行上下文编码器预训练和蒙版自编码预",
    "tldr": "无监督预训练如何影响模型泛化能力的关键因素的新理论框架"
}