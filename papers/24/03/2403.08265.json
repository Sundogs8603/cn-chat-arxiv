{
    "title": "Random Search as a Baseline for Sparse Neural Network Architecture Search",
    "abstract": "arXiv:2403.08265v1 Announce Type: cross  Abstract: Sparse neural networks have shown similar or better generalization performance than their dense counterparts while having higher parameter efficiency. This has motivated a number of works to learn, induce, or search for high performing sparse networks. While reports of quality or efficiency gains are impressive, standard baselines are lacking, therefore hindering having reliable comparability and reproducibility across methods. In this work, we provide an evaluation approach and a naive Random Search baseline method for finding good sparse configurations. We apply Random Search on the node space of an overparameterized network with the goal of finding better initialized sparse sub-networks that are positioned more advantageously in the loss landscape. We record sparse network post-training performances at various levels of sparsity and compare against both their fully connected parent networks and random sparse configurations at the sa",
    "link": "https://arxiv.org/abs/2403.08265",
    "context": "Title: Random Search as a Baseline for Sparse Neural Network Architecture Search\nAbstract: arXiv:2403.08265v1 Announce Type: cross  Abstract: Sparse neural networks have shown similar or better generalization performance than their dense counterparts while having higher parameter efficiency. This has motivated a number of works to learn, induce, or search for high performing sparse networks. While reports of quality or efficiency gains are impressive, standard baselines are lacking, therefore hindering having reliable comparability and reproducibility across methods. In this work, we provide an evaluation approach and a naive Random Search baseline method for finding good sparse configurations. We apply Random Search on the node space of an overparameterized network with the goal of finding better initialized sparse sub-networks that are positioned more advantageously in the loss landscape. We record sparse network post-training performances at various levels of sparsity and compare against both their fully connected parent networks and random sparse configurations at the sa",
    "path": "papers/24/03/2403.08265.json",
    "total_tokens": 830,
    "translated_title": "随机搜索作为稀疏神经网络架构搜索的基准线",
    "translated_abstract": "稀疏神经网络在参数效率更高的情况下展现出与密集网络类似甚至更好的泛化性能，这促使许多工作学习、诱导或搜索性能高的稀疏网络。然而，尽管质量或效率的提升值得注意，但标准基线缺乏，因此妨碍了方法之间的可靠比较和可重现性。在这项工作中，我们提供了一种评估方法和一个简单的随机搜索基线方法，用于发现良好的稀疏配置。我们在过度参数化网络的节点空间上应用随机搜索，目标是找到在损失景观中位置更有优势的更好初始化的稀疏子网络。我们记录了不同稀疏程度下稀疏网络的训练后性能，并与它们的完全连接父网络以及随机稀疏配置进行比较。",
    "tldr": "论文提出了一种评估方法和基于随机搜索的基线方法，用于发现高质量的稀疏神经网络配置，以解决当前缺乏可靠比较和可重现性的问题。",
    "en_tdlr": "The paper introduces an evaluation approach and a baseline method based on random search for discovering high-quality sparse neural network configurations, aiming to address the current lack of reliable comparability and reproducibility."
}