{
    "title": "LaB-GATr: geometric algebra transformers for large biomedical surface and volume meshes",
    "abstract": "arXiv:2403.07536v1 Announce Type: cross  Abstract: Many anatomical structures can be described by surface or volume meshes. Machine learning is a promising tool to extract information from these 3D models. However, high-fidelity meshes often contain hundreds of thousands of vertices, which creates unique challenges in building deep neural network architectures. Furthermore, patient-specific meshes may not be canonically aligned which limits the generalisation of machine learning algorithms. We propose LaB-GATr, a transfomer neural network with geometric tokenisation that can effectively learn with large-scale (bio-)medical surface and volume meshes through sequence compression and interpolation. Our method extends the recently proposed geometric algebra transformer (GATr) and thus respects all Euclidean symmetries, i.e. rotation, translation and reflection, effectively mitigating the problem of canonical alignment between patients. LaB-GATr achieves state-of-the-art results on three ta",
    "link": "https://arxiv.org/abs/2403.07536",
    "context": "Title: LaB-GATr: geometric algebra transformers for large biomedical surface and volume meshes\nAbstract: arXiv:2403.07536v1 Announce Type: cross  Abstract: Many anatomical structures can be described by surface or volume meshes. Machine learning is a promising tool to extract information from these 3D models. However, high-fidelity meshes often contain hundreds of thousands of vertices, which creates unique challenges in building deep neural network architectures. Furthermore, patient-specific meshes may not be canonically aligned which limits the generalisation of machine learning algorithms. We propose LaB-GATr, a transfomer neural network with geometric tokenisation that can effectively learn with large-scale (bio-)medical surface and volume meshes through sequence compression and interpolation. Our method extends the recently proposed geometric algebra transformer (GATr) and thus respects all Euclidean symmetries, i.e. rotation, translation and reflection, effectively mitigating the problem of canonical alignment between patients. LaB-GATr achieves state-of-the-art results on three ta",
    "path": "papers/24/03/2403.07536.json",
    "total_tokens": 889,
    "translated_title": "LaB-GATr：大规模生物医学表面和体积网格的几何代数变换器",
    "translated_abstract": "许多解剖结构可以用表面或体积网格来描述。机器学习是从这些3D模型中提取信息的一种有前途的工具。然而，高保真度的网格通常包含成千上万个顶点，这在构建深度神经网络架构时带来了独特的挑战。此外，患者特异性网格可能没有经典对齐，这限制了机器学习算法的泛化。我们提出了LaB-GATr，一种具有几何标记化的转换器神经网络，通过序列压缩和插值有效地学习大规模（生物）医学表面和体积网格。我们的方法扩展了最近提出的几何代数变换器（GATr），因此尊重所有欧几里得对称性，即旋转、平移和反射，有效地缓解了患者之间经典对齐的问题。",
    "tldr": "LaB-GATr 是一种几何代数变换器神经网络，通过序列压缩和插值有效地学习大规模生物医学表面和体积网格，扩展了传统的 GATr 方法并尊重了欧几里得对称性，达到了最先进的结果。",
    "en_tdlr": "LaB-GATr is a geometric algebra transformer neural network that effectively learns with large-scale biomedical surface and volume meshes through sequence compression and interpolation, extending the traditional GATr method and respecting Euclidean symmetries, achieving state-of-the-art results."
}