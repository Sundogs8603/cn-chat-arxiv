{
    "title": "Is Mamba Effective for Time Series Forecasting?",
    "abstract": "arXiv:2403.11144v1 Announce Type: new  Abstract: In the realm of time series forecasting (TSF), the Transformer has consistently demonstrated robust performance due to its ability to focus on the global context and effectively capture long-range dependencies within time, as well as discern correlations between multiple variables. However, due to the inefficiencies of the Transformer model and questions surrounding its ability to capture dependencies, ongoing efforts to refine the Transformer architecture persist. Recently, state space models (SSMs), e.g. Mamba, have gained traction due to their ability to capture complex dependencies in sequences, similar to the Transformer, while maintaining near-linear complexity. In text and image tasks, Mamba-based models can improve performance and cost savings, creating a win-win situation. This has piqued our interest in exploring SSM's potential in TSF tasks. In this paper, we introduce two straightforward SSM-based models for TSF, S-Mamba and ",
    "link": "https://arxiv.org/abs/2403.11144",
    "context": "Title: Is Mamba Effective for Time Series Forecasting?\nAbstract: arXiv:2403.11144v1 Announce Type: new  Abstract: In the realm of time series forecasting (TSF), the Transformer has consistently demonstrated robust performance due to its ability to focus on the global context and effectively capture long-range dependencies within time, as well as discern correlations between multiple variables. However, due to the inefficiencies of the Transformer model and questions surrounding its ability to capture dependencies, ongoing efforts to refine the Transformer architecture persist. Recently, state space models (SSMs), e.g. Mamba, have gained traction due to their ability to capture complex dependencies in sequences, similar to the Transformer, while maintaining near-linear complexity. In text and image tasks, Mamba-based models can improve performance and cost savings, creating a win-win situation. This has piqued our interest in exploring SSM's potential in TSF tasks. In this paper, we introduce two straightforward SSM-based models for TSF, S-Mamba and ",
    "path": "papers/24/03/2403.11144.json",
    "total_tokens": 833,
    "translated_title": "Mamba在时间序列预测中的有效性如何？",
    "translated_abstract": "在时间序列预测（TSF）领域中，由于Transformer模型能够聚焦全局环境，有效捕捉时间序列中长距离依赖关系以及辨别多变量之间的相关性，因此它一直展现出强大的性能。然而，由于Transformer模型的低效率和关于其捕捉依赖关系能力的质疑，对Transformer架构的不断完善工作仍在进行中。最近，状态空间模型（SSMs）如Mamba因其能够像Transformer一样捕捉序列中的复杂依赖关系，同时又保持近线性的复杂度而备受推崇。在文本和图像任务中，基于Mamba的模型可以提高性能并节约成本，实现双赢局面。这引起了我们对探索SSM在TSF任务中潜力的兴趣。在本文中，我们介绍了两种基于SSM的简单模型，S-Mamba和......",
    "tldr": "Mamba模型作为一种状态空间模型在时间序列预测中具有捕捉复杂依赖关系、近线性复杂度以及性能优势的潜力。",
    "en_tdlr": "The Mamba model, as a state space model, shows potential for capturing complex dependencies, near-linear complexity, and performance advantages in time series forecasting."
}