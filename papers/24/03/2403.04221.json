{
    "title": "Why Online Reinforcement Learning is Causal",
    "abstract": "arXiv:2403.04221v1 Announce Type: cross  Abstract: Reinforcement learning (RL) and causal modelling naturally complement each other. The goal of causal modelling is to predict the effects of interventions in an environment, while the goal of reinforcement learning is to select interventions that maximize the rewards the agent receives from the environment. Reinforcement learning includes the two most powerful sources of information for estimating causal relationships: temporal ordering and the ability to act on an environment. This paper examines which reinforcement learning settings we can expect to benefit from causal modelling, and how. In online learning, the agent has the ability to interact directly with their environment, and learn from exploring it. Our main argument is that in online learning, conditional probabilities are causal, and therefore offline RL is the setting where causal learning has the most potential to make a difference. Essentially, the reason is that when an a",
    "link": "https://arxiv.org/abs/2403.04221",
    "context": "Title: Why Online Reinforcement Learning is Causal\nAbstract: arXiv:2403.04221v1 Announce Type: cross  Abstract: Reinforcement learning (RL) and causal modelling naturally complement each other. The goal of causal modelling is to predict the effects of interventions in an environment, while the goal of reinforcement learning is to select interventions that maximize the rewards the agent receives from the environment. Reinforcement learning includes the two most powerful sources of information for estimating causal relationships: temporal ordering and the ability to act on an environment. This paper examines which reinforcement learning settings we can expect to benefit from causal modelling, and how. In online learning, the agent has the ability to interact directly with their environment, and learn from exploring it. Our main argument is that in online learning, conditional probabilities are causal, and therefore offline RL is the setting where causal learning has the most potential to make a difference. Essentially, the reason is that when an a",
    "path": "papers/24/03/2403.04221.json",
    "total_tokens": 828,
    "translated_title": "在线强化学习为何具有因果性",
    "translated_abstract": "强化学习（RL）和因果建模自然互补。因果建模的目标是预测在环境中进行干预的效果，而强化学习的目标是选择最大化代理从环境中接收的奖励的干预。强化学习包括用于估计因果关系的两个最强大信息源：时间顺序和对环境进行操作的能力。本文研究了我们可以期望在哪些强化学习设置中从因果建模中受益，以及如何受益。在线学习中，代理有能力直接与环境进行交互，并从探索中学习。我们的主要论点是，在在线学习中，条件概率是因果的，因此离线RL是因果学习有潜力产生差异的环境。基本上，原因在于当代理与环境互动时，代理的行为是由其对环境的认识所推动的。",
    "tldr": "强化学习和因果建模相互补充，论文主要指出在线学习环境下，条件概率具有因果性，离线RL是因果学习潜力最大的环境。",
    "en_tdlr": "Reinforcement learning and causal modeling complement each other, and the paper argues that in online learning settings, conditional probabilities are causal, making offline RL the environment with the highest potential for causal learning."
}