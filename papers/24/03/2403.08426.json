{
    "title": "Language-Driven Visual Consensus for Zero-Shot Semantic Segmentation",
    "abstract": "arXiv:2403.08426v1 Announce Type: cross  Abstract: The pre-trained vision-language model, exemplified by CLIP, advances zero-shot semantic segmentation by aligning visual features with class embeddings through a transformer decoder to generate semantic masks. Despite its effectiveness, prevailing methods within this paradigm encounter challenges, including overfitting on seen classes and small fragmentation in masks. To mitigate these issues, we propose a Language-Driven Visual Consensus (LDVC) approach, fostering improved alignment of semantic and visual information.Specifically, we leverage class embeddings as anchors due to their discrete and abstract nature, steering vision features toward class embeddings. Moreover, to circumvent noisy alignments from the vision part due to its redundant nature, we introduce route attention into self-attention for finding visual consensus, thereby enhancing semantic consistency within the same object. Equipped with a vision-language prompting stra",
    "link": "https://arxiv.org/abs/2403.08426",
    "context": "Title: Language-Driven Visual Consensus for Zero-Shot Semantic Segmentation\nAbstract: arXiv:2403.08426v1 Announce Type: cross  Abstract: The pre-trained vision-language model, exemplified by CLIP, advances zero-shot semantic segmentation by aligning visual features with class embeddings through a transformer decoder to generate semantic masks. Despite its effectiveness, prevailing methods within this paradigm encounter challenges, including overfitting on seen classes and small fragmentation in masks. To mitigate these issues, we propose a Language-Driven Visual Consensus (LDVC) approach, fostering improved alignment of semantic and visual information.Specifically, we leverage class embeddings as anchors due to their discrete and abstract nature, steering vision features toward class embeddings. Moreover, to circumvent noisy alignments from the vision part due to its redundant nature, we introduce route attention into self-attention for finding visual consensus, thereby enhancing semantic consistency within the same object. Equipped with a vision-language prompting stra",
    "path": "papers/24/03/2403.08426.json",
    "total_tokens": 830,
    "translated_title": "基于语言驱动的视觉一致性方法用于零样本语义分割",
    "translated_abstract": "通过将视觉特征与类别嵌入对齐，借助变换器解码器生成语义掩模，预训练的视觉-语言模型（如CLIP）推动了零样本语义分割的发展。然而，目前这一范式内的方法遇到了一些挑战，包括在已见类别上过度拟合和掩模中的小碎片化。为了缓解这些问题，我们提出了一种基于语言驱动的视觉一致性（LDVC）方法，促进了语义和视觉信息的改进对齐。具体来说，我们利用类别嵌入作为锚点，引导视觉特征朝向类别嵌入。此外，为了避免由于视觉部分的冗余性而导致的嘈杂对齐，我们将路由注意引入到自注意力中，用于找到视觉一致性，从而增强同一物体内的语义一致性。",
    "tldr": "基于语言驱动的 LDVC 方法通过引入路由注意力机制，实现了零样本语义分割中的语义和视觉信息更好的对齐",
    "en_tdlr": "The Language-Driven Visual Consensus (LDVC) approach proposes route attention to better align semantic and visual information in zero-shot semantic segmentation."
}