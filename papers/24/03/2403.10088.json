{
    "title": "Intent-conditioned and Non-toxic Counterspeech Generation using Multi-Task Instruction Tuning with RLAIF",
    "abstract": "arXiv:2403.10088v1 Announce Type: cross  Abstract: Counterspeech, defined as a response to mitigate online hate speech, is increasingly used as a non-censorial solution. Addressing hate speech effectively involves dispelling the stereotypes, prejudices, and biases often subtly implied in brief, single-sentence statements or abuses. These implicit expressions challenge language models, especially in seq2seq tasks, as model performance typically excels with longer contexts. Our study introduces CoARL, a novel framework enhancing counterspeech generation by modeling the pragmatic implications underlying social biases in hateful statements. CoARL's first two phases involve sequential multi-instruction tuning, teaching the model to understand intents, reactions, and harms of offensive statements, and then learning task-specific low-rank adapter weights for generating intent-conditioned counterspeech. The final phase uses reinforcement learning to fine-tune outputs for effectiveness and non-",
    "link": "https://arxiv.org/abs/2403.10088",
    "context": "Title: Intent-conditioned and Non-toxic Counterspeech Generation using Multi-Task Instruction Tuning with RLAIF\nAbstract: arXiv:2403.10088v1 Announce Type: cross  Abstract: Counterspeech, defined as a response to mitigate online hate speech, is increasingly used as a non-censorial solution. Addressing hate speech effectively involves dispelling the stereotypes, prejudices, and biases often subtly implied in brief, single-sentence statements or abuses. These implicit expressions challenge language models, especially in seq2seq tasks, as model performance typically excels with longer contexts. Our study introduces CoARL, a novel framework enhancing counterspeech generation by modeling the pragmatic implications underlying social biases in hateful statements. CoARL's first two phases involve sequential multi-instruction tuning, teaching the model to understand intents, reactions, and harms of offensive statements, and then learning task-specific low-rank adapter weights for generating intent-conditioned counterspeech. The final phase uses reinforcement learning to fine-tune outputs for effectiveness and non-",
    "path": "papers/24/03/2403.10088.json",
    "total_tokens": 898,
    "translated_title": "利用RLAIF进行意图调节和无毒对抗生成的多任务指导调节",
    "translated_abstract": "对抗性言论被定义为减缓网络仇恨言论的回应，越来越被用作一种非审查解决方案。有效应对仇恨言论涉及消除通常在简短的单句陈述或虐待中暗示的刻板印象、偏见和偏见。这些隐含的表达挑战语言模型，特别是在seq2seq任务中，因为模型性能通常在更长上下文中表现出色。我们的研究引入了CoARL，这是一种新颖的框架，通过建模在仇恨言论中暗含的社会偏见的实用含义来增强对抗性言论生成。CoARL的前两个阶段涉及顺序多指导调节，教导模型理解攻击性陈述的意图、反应和危害，然后学习生成意图调节的对抗性言论的特定任务低秩适配器权重。最后一个阶段使用强化学习对输出进行微调，以提高效果和无毒性。",
    "tldr": "该研究引入了CoARL框架，通过模拟社会偏见中的语用启示来增强对抗性言论生成，利用顺序多指导调节和强化学习生成意图调节的对抗性言论。",
    "en_tdlr": "This study introduces the CoARL framework to enhance counterspeech generation by modeling the pragmatic implications underlying social biases, using sequential multi-instruction tuning and reinforcement learning to generate intent-conditioned counterspeech."
}