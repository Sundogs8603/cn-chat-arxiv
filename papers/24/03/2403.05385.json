{
    "title": "Switching the Loss Reduces the Cost in Batch Reinforcement Learning",
    "abstract": "arXiv:2403.05385v1 Announce Type: new  Abstract: We propose training fitted Q-iteration with log-loss (FQI-LOG) for batch reinforcement learning (RL). We show that the number of samples needed to learn a near-optimal policy with FQI-LOG scales with the accumulated cost of the optimal policy, which is zero in problems where acting optimally achieves the goal and incurs no cost. In doing so, we provide a general framework for proving $\\textit{small-cost}$ bounds, i.e. bounds that scale with the optimal achievable cost, in batch RL. Moreover, we empirically verify that FQI-LOG uses fewer samples than FQI trained with squared loss on problems where the optimal policy reliably achieves the goal.",
    "link": "https://arxiv.org/abs/2403.05385",
    "context": "Title: Switching the Loss Reduces the Cost in Batch Reinforcement Learning\nAbstract: arXiv:2403.05385v1 Announce Type: new  Abstract: We propose training fitted Q-iteration with log-loss (FQI-LOG) for batch reinforcement learning (RL). We show that the number of samples needed to learn a near-optimal policy with FQI-LOG scales with the accumulated cost of the optimal policy, which is zero in problems where acting optimally achieves the goal and incurs no cost. In doing so, we provide a general framework for proving $\\textit{small-cost}$ bounds, i.e. bounds that scale with the optimal achievable cost, in batch RL. Moreover, we empirically verify that FQI-LOG uses fewer samples than FQI trained with squared loss on problems where the optimal policy reliably achieves the goal.",
    "path": "papers/24/03/2403.05385.json",
    "total_tokens": 884,
    "translated_title": "在批强化学习中，通过切换损失函数来降低成本",
    "translated_abstract": "我们提出了一种使用对数损失（FQI-LOG）来训练适合的Q迭代的批强化学习（RL）方法。我们展示了使用FQI-LOG学习接近最优策略所需的样本数量与最优策略的累积成本成比例，对于那些通过最优行为实现目标且不产生成本的问题，最优策略的累积成本为零。通过这种方式，我们提供了一种在批RL中证明具有与最优可达成本成比例的“小成本”界限的一般框架。此外，我们从经验上验证，FQI-LOG在那些最优策略可靠地实现目标的问题上使用的样本比使用平方损失训练的FQI要少。",
    "tldr": "使用对数损失函数来训练适合的Q迭代的批强化学习方法，在实现目标时不产生成本的问题中，其样本数量需求与最优策略的累积成本成比例，能够提供与最优可达成本成比例的“小成本”界限，并在实验中验证在那些最优策略可靠实现目标的问题中，FQI-LOG比使用平方损失训练的FQI使用更少的样本。",
    "en_tdlr": "Using log-loss to train fitted Q-iteration in batch reinforcement learning, the sample complexity scales with the accumulated cost of the optimal policy in zero-cost problems, providing small-cost bounds proportional to the optimal achievable cost and empirically showing that FQI-LOG uses fewer samples in problems where the optimal policy reliably achieves the goal."
}