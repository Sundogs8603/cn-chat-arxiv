{
    "title": "Overestimation, Overfitting, and Plasticity in Actor-Critic: the Bitter Lesson of Reinforcement Learning",
    "abstract": "arXiv:2403.00514v1 Announce Type: new  Abstract: Recent advancements in off-policy Reinforcement Learning (RL) have significantly improved sample efficiency, primarily due to the incorporation of various forms of regularization that enable more gradient update steps than traditional agents. However, many of these techniques have been tested in limited settings, often on tasks from single simulation benchmarks and against well-known algorithms rather than a range of regularization approaches. This limits our understanding of the specific mechanisms driving RL improvements. To address this, we implemented over 60 different off-policy agents, each integrating established regularization techniques from recent state-of-the-art algorithms. We tested these agents across 14 diverse tasks from 2 simulation benchmarks. Our findings reveal that while the effectiveness of a specific regularization setup varies with the task, certain combinations consistently demonstrate robust and superior perform",
    "link": "https://arxiv.org/abs/2403.00514",
    "context": "Title: Overestimation, Overfitting, and Plasticity in Actor-Critic: the Bitter Lesson of Reinforcement Learning\nAbstract: arXiv:2403.00514v1 Announce Type: new  Abstract: Recent advancements in off-policy Reinforcement Learning (RL) have significantly improved sample efficiency, primarily due to the incorporation of various forms of regularization that enable more gradient update steps than traditional agents. However, many of these techniques have been tested in limited settings, often on tasks from single simulation benchmarks and against well-known algorithms rather than a range of regularization approaches. This limits our understanding of the specific mechanisms driving RL improvements. To address this, we implemented over 60 different off-policy agents, each integrating established regularization techniques from recent state-of-the-art algorithms. We tested these agents across 14 diverse tasks from 2 simulation benchmarks. Our findings reveal that while the effectiveness of a specific regularization setup varies with the task, certain combinations consistently demonstrate robust and superior perform",
    "path": "papers/24/03/2403.00514.json",
    "total_tokens": 850,
    "translated_title": "Actor-Critic中的过度估计、过拟合和可塑性：强化学习的苦涩教训",
    "translated_abstract": "最近，强化学习（RL）中离策略的进展显著提高了样本效率，主要是由于各种形式的正则化的应用，使其比传统的代理更能进行梯度更新步骤。然而，许多这些技术都在有限的情景下进行了测试，通常只在单个仿真基准任务上测试，与众所周知的算法相比，而不是与一系列正则化方法相比。这限制了我们对推动RL改进的具体机制的理解。为了解决这个问题，我们实现了超过60种不同的离策略代理，每个代理都整合了最近最先进算法中的已建立的正则化技术。我们在来自2个仿真基准的14个不同任务上测试了这些代理。我们的研究结果表明，尽管特定的正则化设置的效果因任务而异，但某些组合始终表现出稳健和优越的性能。",
    "tldr": "实施超过60种不同的离策略代理，发现某些组合表现出稳健和优越的性能，揭示了特定正则化设置与任务的关联性约几维多多。",
    "en_tdlr": "Implementing over 60 different off-policy agents, certain combinations consistently demonstrate robust and superior performance, revealing the association between specific regularization setups and tasks."
}