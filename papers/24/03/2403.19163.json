{
    "title": "D'OH: Decoder-Only random Hypernetworks for Implicit Neural Representations",
    "abstract": "arXiv:2403.19163v1 Announce Type: new  Abstract: Deep implicit functions have been found to be an effective tool for efficiently encoding all manner of natural signals. Their attractiveness stems from their ability to compactly represent signals with little to no off-line training data. Instead, they leverage the implicit bias of deep networks to decouple hidden redundancies within the signal. In this paper, we explore the hypothesis that additional compression can be achieved by leveraging the redundancies that exist between layers. We propose to use a novel run-time decoder-only hypernetwork - that uses no offline training data - to better model this cross-layer parameter redundancy. Previous applications of hyper-networks with deep implicit functions have applied feed-forward encoder/decoder frameworks that rely on large offline datasets that do not generalize beyond the signals they were trained on. We instead present a strategy for the initialization of run-time deep implicit func",
    "link": "https://arxiv.org/abs/2403.19163",
    "context": "Title: D'OH: Decoder-Only random Hypernetworks for Implicit Neural Representations\nAbstract: arXiv:2403.19163v1 Announce Type: new  Abstract: Deep implicit functions have been found to be an effective tool for efficiently encoding all manner of natural signals. Their attractiveness stems from their ability to compactly represent signals with little to no off-line training data. Instead, they leverage the implicit bias of deep networks to decouple hidden redundancies within the signal. In this paper, we explore the hypothesis that additional compression can be achieved by leveraging the redundancies that exist between layers. We propose to use a novel run-time decoder-only hypernetwork - that uses no offline training data - to better model this cross-layer parameter redundancy. Previous applications of hyper-networks with deep implicit functions have applied feed-forward encoder/decoder frameworks that rely on large offline datasets that do not generalize beyond the signals they were trained on. We instead present a strategy for the initialization of run-time deep implicit func",
    "path": "papers/24/03/2403.19163.json",
    "total_tokens": 819,
    "translated_title": "D'OH: 仅解码器随机超网络用于隐式神经表示",
    "translated_abstract": "深度隐式函数被发现是一种有效的工具，可以高效地编码各种自然信号。它们的吸引力在于能够紧凑地表示信号，几乎不需要离线训练数据。相反，它们利用深度网络的隐式偏差来解耦信号中的隐藏冗余。在本文中，我们探讨了这样一个假设：通过利用层之间存在的冗余可以实现更好的压缩。我们提出使用一种新颖的仅运行时解码器的超网络 - 它不使用离线训练数据 - 来更好地建模跨层参数冗余。先前在深度隐式函数中应用超网络的应用都采用了依赖大量离线数据集的前馈编码器/解码器框架，这些数据集无法泛化到训练信号之外。相反，我们提出一种用于初始化运行时深度隐式函数的策略",
    "tldr": "本文提出使用仅运行时解码器的超网络，不依赖离线数据训练，以更好地模拟跨层参数冗余。",
    "en_tdlr": "This paper proposes using decoder-only hypernetwork at runtime without relying on offline training data to better model cross-layer parameter redundancy."
}