{
    "title": "Motif distribution and function of sparse deep neural networks",
    "abstract": "arXiv:2403.00974v1 Announce Type: new  Abstract: We characterize the connectivity structure of feed-forward, deep neural networks (DNNs) using network motif theory. To address whether a particular motif distribution is characteristic of the training task, or function of the DNN, we compare the connectivity structure of 350 DNNs trained to simulate a bio-mechanical flight control system with different randomly initialized parameters. We develop and implement algorithms for counting second- and third-order motifs and calculate their significance using their Z-score. The DNNs are trained to solve the inverse problem of the flight dynamics model in Bustamante, et al. (2022) (i.e., predict the controls necessary for controlled flight from the initial and final state-space inputs) and are sparsified through an iterative pruning and retraining algorithm Zahn, et al. (2022). We show that, despite random initialization of network parameters, enforced sparsity causes DNNs to converge to similar ",
    "link": "https://arxiv.org/abs/2403.00974",
    "context": "Title: Motif distribution and function of sparse deep neural networks\nAbstract: arXiv:2403.00974v1 Announce Type: new  Abstract: We characterize the connectivity structure of feed-forward, deep neural networks (DNNs) using network motif theory. To address whether a particular motif distribution is characteristic of the training task, or function of the DNN, we compare the connectivity structure of 350 DNNs trained to simulate a bio-mechanical flight control system with different randomly initialized parameters. We develop and implement algorithms for counting second- and third-order motifs and calculate their significance using their Z-score. The DNNs are trained to solve the inverse problem of the flight dynamics model in Bustamante, et al. (2022) (i.e., predict the controls necessary for controlled flight from the initial and final state-space inputs) and are sparsified through an iterative pruning and retraining algorithm Zahn, et al. (2022). We show that, despite random initialization of network parameters, enforced sparsity causes DNNs to converge to similar ",
    "path": "papers/24/03/2403.00974.json",
    "total_tokens": 866,
    "translated_title": "稀疏深度神经网络的模式分布和功能",
    "translated_abstract": "我们使用网络模式理论表征前馈式深度神经网络（DNNs）的连接结构。为了解特定模式分布是否是训练任务的特征，还是DNN的功能，我们比较了训练以模拟生物力学飞行控制系统的350个DNN的连接结构，这些DNN拥有不同的随机初始化参数。我们开发并实施了用于计算二阶和三阶模式并使用它们的Z分数计算其显着性的算法。这些DNN被训练来解决 Bustamante 等人（2022年）中的飞行动力学模型的反问题（即从初始和最终状态空间输入预测控制所需）。通过一个迭代修剪和重新训练算法 Zahn 等人（2022年），我们对它们进行了稀疏化处理。我们展示了，尽管网络参数随机初始化，但强制稀疏会导致DNN收敛到类似的结构。",
    "tldr": "通过网络模式理论，研究了稀疏深度神经网络的连接结构，证明了尽管随机初始化参数，强制稀疏会导致这些网络收敛到相似的结构",
    "en_tdlr": "Investigated the connectivity structure of sparse deep neural networks using network motif theory, showing that enforced sparsity leads to similar network convergence despite random initialization of parameters."
}