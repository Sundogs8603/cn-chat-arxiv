{
    "title": "Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings",
    "abstract": "arXiv:2403.07750v1 Announce Type: cross  Abstract: The creation of high-quality human-labeled image-caption datasets presents a significant bottleneck in the development of Visual-Language Models (VLMs). We propose a novel approach that leverages the strengths of Large Language Models (LLMs) and image generation models to create synthetic image-text pairs for efficient and effective VLM training. Our method employs pretraining a text-to-image model to synthesize image embeddings starting from captions generated by an LLM. These synthetic pairs are then used to train a VLM. Extensive experiments demonstrate that the VLM trained with synthetic data exhibits comparable performance on image captioning, while requiring a fraction of the data used by models trained solely on human-annotated data. In particular, we outperform the baseline by 17% through augmentation with a synthetic dataset. Furthermore, we show that synthesizing in the image embedding space is 25% faster than in the pixel sp",
    "link": "https://arxiv.org/abs/2403.07750",
    "context": "Title: Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings\nAbstract: arXiv:2403.07750v1 Announce Type: cross  Abstract: The creation of high-quality human-labeled image-caption datasets presents a significant bottleneck in the development of Visual-Language Models (VLMs). We propose a novel approach that leverages the strengths of Large Language Models (LLMs) and image generation models to create synthetic image-text pairs for efficient and effective VLM training. Our method employs pretraining a text-to-image model to synthesize image embeddings starting from captions generated by an LLM. These synthetic pairs are then used to train a VLM. Extensive experiments demonstrate that the VLM trained with synthetic data exhibits comparable performance on image captioning, while requiring a fraction of the data used by models trained solely on human-annotated data. In particular, we outperform the baseline by 17% through augmentation with a synthetic dataset. Furthermore, we show that synthesizing in the image embedding space is 25% faster than in the pixel sp",
    "path": "papers/24/03/2403.07750.json",
    "total_tokens": 865,
    "translated_title": "Synth$^2$: 用合成标题和图像嵌入提升视觉-语言模型",
    "translated_abstract": "高质量的人工标记图像标题数据集的创建在视觉-语言模型（VLMs）的发展中构成了一个重大瓶颈。我们提出了一种新方法，利用大型语言模型（LLMs）和图像生成模型的优势，为高效有效的VLM训练创建合成图像-文本对。我们的方法利用预训练的文本到图像模型，从LLM生成的标题开始合成图像嵌入。然后，这些合成对用于训练VLM。大量实验证明，通过合成数据训练的VLM在图像字幕方面表现出可比较的性能，而所需的数据量仅为仅使用人类注释数据训练的模型的一小部分。特别是，通过合成数据集的增强，我们超过基线17%。此外，我们展示，在图像嵌入空间合成比在像素空间中快25%。",
    "tldr": "这项研究提出了一种新方法，通过合成数据训练的VLM在图像字幕方面表现出可比较的性能，而所需的数据量仅为仅使用人类注释数据训练的模型的一小部分。",
    "en_tdlr": "This study presents a novel approach where a VLM trained with synthetic data exhibits comparable performance on image captioning, while requiring a fraction of the data used by models trained solely on human-annotated data."
}