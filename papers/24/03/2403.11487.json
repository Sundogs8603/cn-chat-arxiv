{
    "title": "Can LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis",
    "abstract": "arXiv:2403.11487v1 Announce Type: cross  Abstract: We present a novel approach to automatically synthesize \"wayfinding instructions\" for an embodied robot agent. In contrast to prior approaches that are heavily reliant on human-annotated datasets designed exclusively for specific simulation platforms, our algorithm uses in-context learning to condition an LLM to generate instructions using just a few references. Using an LLM-based Visual Question Answering strategy, we gather detailed information about the environment which is used by the LLM for instruction synthesis. We implement our approach on multiple simulation platforms including Matterport3D, AI Habitat and ThreeDWorld, thereby demonstrating its platform-agnostic nature. We subjectively evaluate our approach via a user study and observe that 83.3% of users find the synthesized instructions accurately capture the details of the environment and show characteristics similar to those of human-generated instructions. Further, we con",
    "link": "https://arxiv.org/abs/2403.11487",
    "context": "Title: Can LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis\nAbstract: arXiv:2403.11487v1 Announce Type: cross  Abstract: We present a novel approach to automatically synthesize \"wayfinding instructions\" for an embodied robot agent. In contrast to prior approaches that are heavily reliant on human-annotated datasets designed exclusively for specific simulation platforms, our algorithm uses in-context learning to condition an LLM to generate instructions using just a few references. Using an LLM-based Visual Question Answering strategy, we gather detailed information about the environment which is used by the LLM for instruction synthesis. We implement our approach on multiple simulation platforms including Matterport3D, AI Habitat and ThreeDWorld, thereby demonstrating its platform-agnostic nature. We subjectively evaluate our approach via a user study and observe that 83.3% of users find the synthesized instructions accurately capture the details of the environment and show characteristics similar to those of human-generated instructions. Further, we con",
    "path": "papers/24/03/2403.11487.json",
    "total_tokens": 817,
    "translated_title": "LLM能生成类人行路指示吗？走向跨平台的具身指令综合",
    "translated_abstract": "我们提出了一种新颖的方法，用于自动合成“行路指示”以指导具身机器人。与先前的方法相比，这种方法不再依赖于仅设计用于特定模拟平台的人工注释数据集，而是使用上下文学习来调节LLM，以使用少量参考生成指示。我们使用基于LLM的视觉问答策略收集环境的详细信息，LLM用于指令合成。我们将我们的方法实现在多个模拟平台上，包括Matterport3D、AI Habitat和ThreeDWorld，从而展示了其跨平台特性。我们通过用户研究主观评估了我们的方法，观察到83.3%的用户认为合成的指示准确捕捉了环境的细节，并表现出与人类生成的指示类似的特征。",
    "tldr": "提出了一种新方法，利用LLM以及上下文学习，实现了自动生成具身机器人的“行路指示”，并且在多个模拟平台上展示出跨平台特性。",
    "en_tdlr": "A novel method is proposed to automatically synthesize \"wayfinding instructions\" for embodied robots using LLM and contextual learning, demonstrating platform-agnostic nature across multiple simulation platforms."
}