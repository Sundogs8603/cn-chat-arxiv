{
    "title": "STaR-GATE: Teaching Language Models to Ask Clarifying Questions",
    "abstract": "arXiv:2403.19154v1 Announce Type: cross  Abstract: When prompting language models to complete a task, users often leave important aspects unsaid. While asking questions could resolve this ambiguity \\citep[GATE;][]{li2023eliciting}, models often struggle to ask good questions. We explore a language model's ability to self-improve \\citep[STaR;][]{zelikman2022star} by rewarding the model for generating useful questions -- a simple method we dub STaR-GATE. We generate a synthetic dataset of 25,500 unique persona-task prompts to simulate conversations between a pretrained language model -- the \\texttt{Questioner} -- and a \\texttt{Roleplayer} whose preferences are unknown to the \\texttt{Questioner}. By asking questions, the \\texttt{Questioner} elicits preferences from the \\texttt{Roleplayer}. The \\texttt{Questioner} is iteratively finetuned on questions that increase the probability of high-quality responses to the task, which are generated by an \\texttt{Oracle} with access to the \\texttt{Ro",
    "link": "https://arxiv.org/abs/2403.19154",
    "context": "Title: STaR-GATE: Teaching Language Models to Ask Clarifying Questions\nAbstract: arXiv:2403.19154v1 Announce Type: cross  Abstract: When prompting language models to complete a task, users often leave important aspects unsaid. While asking questions could resolve this ambiguity \\citep[GATE;][]{li2023eliciting}, models often struggle to ask good questions. We explore a language model's ability to self-improve \\citep[STaR;][]{zelikman2022star} by rewarding the model for generating useful questions -- a simple method we dub STaR-GATE. We generate a synthetic dataset of 25,500 unique persona-task prompts to simulate conversations between a pretrained language model -- the \\texttt{Questioner} -- and a \\texttt{Roleplayer} whose preferences are unknown to the \\texttt{Questioner}. By asking questions, the \\texttt{Questioner} elicits preferences from the \\texttt{Roleplayer}. The \\texttt{Questioner} is iteratively finetuned on questions that increase the probability of high-quality responses to the task, which are generated by an \\texttt{Oracle} with access to the \\texttt{Ro",
    "path": "papers/24/03/2403.19154.json",
    "total_tokens": 865,
    "translated_title": "STaR-GATE: 教授语言模型询问澄清问题",
    "translated_abstract": "当提示语言模型完成任务时，用户通常会遗漏重要的细节。虽然提问可以解决这种歧义，但模型往往很难提出好问题。我们探讨了语言模型通过奖励模型生成有用问题来自我改进的能力，这是一种简单方法，我们称之为STaR-GATE。我们生成了一个包含25,500个独特人物-任务提示的合成数据集，以模拟预训练语言模型--提问者--与一个其偏好未知的角色扮演者之间的对话。通过提问，提问者从角色扮演者那里引出偏好。提问者在那些增加高质量响应概率的问题上进行迭代微调，这些问题是由具有对角色扮演者访问权限的预言者生成的。",
    "tldr": "通过奖励语言模型生成有用问题来自我改进的方法，提问者通过询问角色扮演者来引出偏好，从而迭代微调以增加任务高质量响应的概率。",
    "en_tdlr": "Rewarding language models for generating useful questions to self-improve, the Questioner elicits preferences from the Roleplayer through questioning and iteratively fine-tunes on questions that increase the probability of high-quality responses to the task."
}