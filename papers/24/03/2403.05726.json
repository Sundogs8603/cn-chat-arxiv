{
    "title": "Augmentations vs Algorithms: What Works in Self-Supervised Learning",
    "abstract": "arXiv:2403.05726v1 Announce Type: new  Abstract: We study the relative effects of data augmentations, pretraining algorithms, and model architectures in Self-Supervised Learning (SSL). While the recent literature in this space leaves the impression that the pretraining algorithm is of critical importance to performance, understanding its effect is complicated by the difficulty in making objective and direct comparisons between methods. We propose a new framework which unifies many seemingly disparate SSL methods into a single shared template. Using this framework, we identify aspects in which methods differ and observe that in addition to changing the pretraining algorithm, many works also use new data augmentations or more powerful model architectures. We compare several popular SSL methods using our framework and find that many algorithmic additions, such as prediction networks or new losses, have a minor impact on downstream task performance (often less than $1\\%$), while enhanced a",
    "link": "https://arxiv.org/abs/2403.05726",
    "context": "Title: Augmentations vs Algorithms: What Works in Self-Supervised Learning\nAbstract: arXiv:2403.05726v1 Announce Type: new  Abstract: We study the relative effects of data augmentations, pretraining algorithms, and model architectures in Self-Supervised Learning (SSL). While the recent literature in this space leaves the impression that the pretraining algorithm is of critical importance to performance, understanding its effect is complicated by the difficulty in making objective and direct comparisons between methods. We propose a new framework which unifies many seemingly disparate SSL methods into a single shared template. Using this framework, we identify aspects in which methods differ and observe that in addition to changing the pretraining algorithm, many works also use new data augmentations or more powerful model architectures. We compare several popular SSL methods using our framework and find that many algorithmic additions, such as prediction networks or new losses, have a minor impact on downstream task performance (often less than $1\\%$), while enhanced a",
    "path": "papers/24/03/2403.05726.json",
    "total_tokens": 884,
    "translated_title": "增强 vs 算法：自监督学习中的有效因素",
    "translated_abstract": "我们研究了数据增强、预训练算法和模型架构在自监督学习（SSL）中的相对影响。虽然最近这一领域的文献让人觉得预训练算法对性能至关重要，但要理解其影响受制于很难进行客观和直接的比较。我们提出了一个新的框架，将许多看似不同的SSL方法统一到一个共享的模板中。利用这个框架，我们确定了方法之间的差异之处，并观察到除了改变预训练算法外，许多作品还使用了新的数据增强或更强大的模型架构。我们使用我们的框架比较了几种流行的SSL方法，发现许多算法添加，如预测网络或新的损失，对下游任务性能影响较小（通常低于1%），而增强型",
    "tldr": "研究了自监督学习中数据增强、预训练算法和模型架构的相对影响，提出了统一SSL方法的新框架，发现除了改变预训练算法外，新数据增强和更强大的模型架构也起到重要作用",
    "en_tdlr": "Studied the relative effects of data augmentations, pretraining algorithms, and model architectures in Self-Supervised Learning and proposed a new framework unifying SSL methods, finding that besides changing pretraining algorithms, new data augmentations and more powerful model architectures also play important roles."
}