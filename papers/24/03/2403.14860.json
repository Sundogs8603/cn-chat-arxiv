{
    "title": "Robust Model Based Reinforcement Learning Using $\\mathcal{L}_1$ Adaptive Control",
    "abstract": "arXiv:2403.14860v1 Announce Type: cross  Abstract: We introduce $\\mathcal{L}_1$-MBRL, a control-theoretic augmentation scheme for Model-Based Reinforcement Learning (MBRL) algorithms. Unlike model-free approaches, MBRL algorithms learn a model of the transition function using data and use it to design a control input. Our approach generates a series of approximate control-affine models of the learned transition function according to the proposed switching law. Using the approximate model, control input produced by the underlying MBRL is perturbed by the $\\mathcal{L}_1$ adaptive control, which is designed to enhance the robustness of the system against uncertainties. Importantly, this approach is agnostic to the choice of MBRL algorithm, enabling the use of the scheme with various MBRL algorithms. MBRL algorithms with $\\mathcal{L}_1$ augmentation exhibit enhanced performance and sample efficiency across multiple MuJoCo environments, outperforming the original MBRL algorithms, both with ",
    "link": "https://arxiv.org/abs/2403.14860",
    "context": "Title: Robust Model Based Reinforcement Learning Using $\\mathcal{L}_1$ Adaptive Control\nAbstract: arXiv:2403.14860v1 Announce Type: cross  Abstract: We introduce $\\mathcal{L}_1$-MBRL, a control-theoretic augmentation scheme for Model-Based Reinforcement Learning (MBRL) algorithms. Unlike model-free approaches, MBRL algorithms learn a model of the transition function using data and use it to design a control input. Our approach generates a series of approximate control-affine models of the learned transition function according to the proposed switching law. Using the approximate model, control input produced by the underlying MBRL is perturbed by the $\\mathcal{L}_1$ adaptive control, which is designed to enhance the robustness of the system against uncertainties. Importantly, this approach is agnostic to the choice of MBRL algorithm, enabling the use of the scheme with various MBRL algorithms. MBRL algorithms with $\\mathcal{L}_1$ augmentation exhibit enhanced performance and sample efficiency across multiple MuJoCo environments, outperforming the original MBRL algorithms, both with ",
    "path": "papers/24/03/2403.14860.json",
    "total_tokens": 887,
    "translated_title": "使用$\\mathcal{L}_1$自适应控制的鲁棒性基于模型的强化学习",
    "translated_abstract": "我们引入了$\\mathcal{L}_1$-MBRL，这是一种用于模型基强化学习（MBRL）算法的控制理论增强方案。与无模型方法不同，MBRL算法通过数据学习转移函数模型，并用它设计控制输入。我们的方法根据提出的切换规律生成一系列学习转移函数的近似控制仿射模型。通过近似模型，底层MBRL生成的控制输入被$\\mathcal{L}_1$自适应控制所扰动，旨在增强系统对不确定性的鲁棒性。重要的是，这种方法对MBRL算法的选择是不可知的，可以与各种MBRL算法一起使用。具有$\\mathcal{L}_1$增强的MBRL算法在多个MuJoCo环境中表现出更佳的性能和样本效率，优于原始MBRL算法。",
    "tldr": "本研究引入了一种使用$\\mathcal{L}_1$自适应控制的鲁棒性基于模型的强化学习方法，通过增强系统对不确定性的鲁棒性，提高了MBRL算法的性能和样本效率。",
    "en_tdlr": "This paper introduces a robust model-based reinforcement learning method using $\\mathcal{L}_1$ adaptive control, which enhances the system's robustness against uncertainties and improves the performance and sample efficiency of MBRL algorithms."
}