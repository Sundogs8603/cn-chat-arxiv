{
    "title": "Snapshot Reinforcement Learning: Leveraging Prior Trajectories for Efficiency",
    "abstract": "arXiv:2403.00673v1 Announce Type: new  Abstract: Deep reinforcement learning (DRL) algorithms require substantial samples and computational resources to achieve higher performance, which restricts their practical application and poses challenges for further development. Given the constraint of limited resources, it is essential to leverage existing computational work (e.g., learned policies, samples) to enhance sample efficiency and reduce the computational resource consumption of DRL algorithms. Previous works to leverage existing computational work require intrusive modifications to existing algorithms and models, designed specifically for specific algorithms, lacking flexibility and universality. In this paper, we present the Snapshot Reinforcement Learning (SnapshotRL) framework, which enhances sample efficiency by simply altering environments, without making any modifications to algorithms and models. By allowing student agents to choose states in teacher trajectories as the initi",
    "link": "https://arxiv.org/abs/2403.00673",
    "context": "Title: Snapshot Reinforcement Learning: Leveraging Prior Trajectories for Efficiency\nAbstract: arXiv:2403.00673v1 Announce Type: new  Abstract: Deep reinforcement learning (DRL) algorithms require substantial samples and computational resources to achieve higher performance, which restricts their practical application and poses challenges for further development. Given the constraint of limited resources, it is essential to leverage existing computational work (e.g., learned policies, samples) to enhance sample efficiency and reduce the computational resource consumption of DRL algorithms. Previous works to leverage existing computational work require intrusive modifications to existing algorithms and models, designed specifically for specific algorithms, lacking flexibility and universality. In this paper, we present the Snapshot Reinforcement Learning (SnapshotRL) framework, which enhances sample efficiency by simply altering environments, without making any modifications to algorithms and models. By allowing student agents to choose states in teacher trajectories as the initi",
    "path": "papers/24/03/2403.00673.json",
    "total_tokens": 753,
    "translated_title": "快照强化学习：利用先前轨迹提高效率",
    "translated_abstract": "深度强化学习（DRL）算法需要大量样本和计算资源才能实现更高的性能，这限制了它们的实际应用并对进一步发展构成挑战。鉴于资源有限的约束，利用现有的计算工作（例如学习策略、样本）来增强样本效率和减少DRL算法的计算资源消耗至关重要。以前利用现有计算工作的研究需要对现有算法和模型进行干扰性修改，专门为特定算法设计，缺乏灵活性和通用性。本文提出了快照强化学习（SnapshotRL）框架，通过简单改变环境来增强样本效率，而无需对算法和模型进行任何修改。",
    "tldr": "提出了快照强化学习（SnapshotRL）框架，通过简单改变环境来增强样本效率，而无需对算法和模型进行任何修改",
    "en_tdlr": "The paper introduces the Snapshot Reinforcement Learning (SnapshotRL) framework, which enhances sample efficiency by simply altering environments, without making any modifications to algorithms and models."
}