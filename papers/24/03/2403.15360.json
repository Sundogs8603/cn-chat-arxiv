{
    "title": "SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time series",
    "abstract": "arXiv:2403.15360v1 Announce Type: cross  Abstract: Transformers have widely adopted attention networks for sequence mixing and MLPs for channel mixing, playing a pivotal role in achieving breakthroughs across domains. However, recent literature highlights issues with attention networks, including low inductive bias and quadratic complexity concerning input sequence length. State Space Models (SSMs) like S4 and others (Hippo, Global Convolutions, liquid S4, LRU, Mega, and Mamba), have emerged to address the above issues to help handle longer sequence lengths. Mamba, while being the state-of-the-art SSM, has a stability issue when scaled to large networks for computer vision datasets. We propose SiMBA, a new architecture that introduces Einstein FFT (EinFFT) for channel modeling by specific eigenvalue computations and uses the Mamba block for sequence modeling. Extensive performance studies across image and time-series benchmarks demonstrate that SiMBA outperforms existing SSMs, bridging",
    "link": "https://arxiv.org/abs/2403.15360",
    "context": "Title: SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time series\nAbstract: arXiv:2403.15360v1 Announce Type: cross  Abstract: Transformers have widely adopted attention networks for sequence mixing and MLPs for channel mixing, playing a pivotal role in achieving breakthroughs across domains. However, recent literature highlights issues with attention networks, including low inductive bias and quadratic complexity concerning input sequence length. State Space Models (SSMs) like S4 and others (Hippo, Global Convolutions, liquid S4, LRU, Mega, and Mamba), have emerged to address the above issues to help handle longer sequence lengths. Mamba, while being the state-of-the-art SSM, has a stability issue when scaled to large networks for computer vision datasets. We propose SiMBA, a new architecture that introduces Einstein FFT (EinFFT) for channel modeling by specific eigenvalue computations and uses the Mamba block for sequence modeling. Extensive performance studies across image and time-series benchmarks demonstrate that SiMBA outperforms existing SSMs, bridging",
    "path": "papers/24/03/2403.15360.json",
    "total_tokens": 853,
    "translated_title": "SiMBA：用于视觉和多变量时间序列的简化Mamba架构",
    "translated_abstract": "Transformers广泛采用了注意力网络进行序列混合和MLPs进行通道混合，在各个领域取得了突破性进展。然而，最近的文献强调了关于注意力网络的问题，包括对输入序列长度的低归纳偏差和二次复杂度。状态空间模型（SSMs）如S4和其他模型（Hippo，Global Convolutions，liquid S4，LRU，Mega和Mamba），已经出现来解决以上问题，以帮助处理更长的序列长度。Mamba是当前最先进的SSM，但在扩展到大型计算机视觉数据集时存在稳定性问题。我们提出了SiMBA，一种新的架构，通过特定的特征值计算引入Einstein FFT（EinFFT）来进行通道建模，并使用Mamba块进行序列建模。对图像和时间序列基准的广泛性能研究表明，SiMBA优于现有的SSMs，架起了",
    "tldr": "SiMBA是一种引入Einstein FFT进行通道建模并使用Mamba块进行序列建模的新架构，在图像和时间序列基准上优于现有的SSMs。",
    "en_tdlr": "SiMBA is a new architecture that introduces Einstein FFT for channel modeling and uses the Mamba block for sequence modeling, outperforming existing SSMs on image and time-series benchmarks."
}