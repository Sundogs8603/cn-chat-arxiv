{
    "title": "LayerNorm: A key component in parameter-efficient fine-tuning",
    "abstract": "arXiv:2403.20284v1 Announce Type: new  Abstract: Fine-tuning a pre-trained model, such as Bidirectional Encoder Representations from Transformers (BERT), has been proven to be an effective method for solving many natural language processing (NLP) tasks. However, due to the large number of parameters in many state-of-the-art NLP models, including BERT, the process of fine-tuning is computationally expensive. One attractive solution to this issue is parameter-efficient fine-tuning, which involves modifying only a minimal segment of the model while keeping the remainder unchanged. Yet, it remains unclear which segment of the BERT model is crucial for fine-tuning. In this paper, we first analyze different components in the BERT model to pinpoint which one undergoes the most significant changes after fine-tuning. We find that output LayerNorm changes more than any other components when fine-tuned for different General Language Understanding Evaluation (GLUE) tasks. Then we show that only fi",
    "link": "https://arxiv.org/abs/2403.20284",
    "context": "Title: LayerNorm: A key component in parameter-efficient fine-tuning\nAbstract: arXiv:2403.20284v1 Announce Type: new  Abstract: Fine-tuning a pre-trained model, such as Bidirectional Encoder Representations from Transformers (BERT), has been proven to be an effective method for solving many natural language processing (NLP) tasks. However, due to the large number of parameters in many state-of-the-art NLP models, including BERT, the process of fine-tuning is computationally expensive. One attractive solution to this issue is parameter-efficient fine-tuning, which involves modifying only a minimal segment of the model while keeping the remainder unchanged. Yet, it remains unclear which segment of the BERT model is crucial for fine-tuning. In this paper, we first analyze different components in the BERT model to pinpoint which one undergoes the most significant changes after fine-tuning. We find that output LayerNorm changes more than any other components when fine-tuned for different General Language Understanding Evaluation (GLUE) tasks. Then we show that only fi",
    "path": "papers/24/03/2403.20284.json",
    "total_tokens": 842,
    "translated_title": "LayerNorm：参数高效微调中的关键组件",
    "translated_abstract": "细调预训练模型，如双向编码器表示来自转换器（BERT），已被证明是解决许多自然语言处理（NLP）任务的有效方法。然而，由于许多最先进的NLP模型（包括BERT）中的参数数量庞大，微调过程耗费了大量计算资源。解决这一问题的一种吸引人方法是参数高效微调，即仅修改模型的最小部分，同时保持其余部分不变。然而，目前仍不清楚BERT模型的哪个部分对微调至关重要。在本文中，我们首先分析BERT模型中的不同组件，以查明在微调后哪些组件发生了最显著的变化。我们发现，在针对不同General Language Understanding Evaluation（GLUE）任务进行微调时，输出的LayerNorm发生的变化比其他组件都要大。然后我们展示仅微调output LayerNorm而保持其他部分不变就足以在多个GLUE任务中取得竞争力。",
    "tldr": "LayerNorm在参数高效微调中扮演关键角色，微调output LayerNorm而保持其他部分不变足以在多个GLUE任务中取得竞争力",
    "en_tdlr": "LayerNorm plays a crucial role in parameter-efficient fine-tuning, and fine-tuning only the output LayerNorm while keeping the rest unchanged is sufficient to be competitive in multiple GLUE tasks."
}