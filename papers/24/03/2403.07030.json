{
    "title": "AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge Distillation",
    "abstract": "arXiv:2403.07030v1 Announce Type: new  Abstract: Due to privacy or patent concerns, a growing number of large models are released without granting access to their training data, making transferring their knowledge inefficient and problematic. In response, Data-Free Knowledge Distillation (DFKD) methods have emerged as direct solutions. However, simply adopting models derived from DFKD for real-world applications suffers significant performance degradation, due to the discrepancy between teachers' training data and real-world scenarios (student domain). The degradation stems from the portions of teachers' knowledge that are not applicable to the student domain. They are specific to the teacher domain and would undermine students' performance. Hence, selectively transferring teachers' appropriate knowledge becomes the primary challenge in DFKD. In this work, we propose a simple but effective method AuG-KD. It utilizes an uncertainty-guided and sample-specific anchor to align student-doma",
    "link": "https://arxiv.org/abs/2403.07030",
    "context": "Title: AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge Distillation\nAbstract: arXiv:2403.07030v1 Announce Type: new  Abstract: Due to privacy or patent concerns, a growing number of large models are released without granting access to their training data, making transferring their knowledge inefficient and problematic. In response, Data-Free Knowledge Distillation (DFKD) methods have emerged as direct solutions. However, simply adopting models derived from DFKD for real-world applications suffers significant performance degradation, due to the discrepancy between teachers' training data and real-world scenarios (student domain). The degradation stems from the portions of teachers' knowledge that are not applicable to the student domain. They are specific to the teacher domain and would undermine students' performance. Hence, selectively transferring teachers' appropriate knowledge becomes the primary challenge in DFKD. In this work, we propose a simple but effective method AuG-KD. It utilizes an uncertainty-guided and sample-specific anchor to align student-doma",
    "path": "papers/24/03/2403.07030.json",
    "total_tokens": 887,
    "translated_title": "AuG-KD: 基于锚点的混合生成用于领域之外知识蒸馏",
    "translated_abstract": "由于隐私或专利问题，越来越多的大型模型发布时不提供其训练数据的访问权限，这使得将它们的知识转移变得低效且问题复杂。针对这一问题，出现了无数据知识蒸馏（DFKD）方法作为直接解决方案。然而，简单地采用从DFKD派生的模型用于实际应用会导致显著的性能下降，这是因为教师训练数据与实际场景（学生领域）之间存在巨大差异。这种性能下降源于教师知识中不适用于学生领域的部分，这些知识是特定于教师领域的，会削弱学生的性能。因此，在DFKD中，有选择地转移适用于学生领域的教师知识成为主要挑战。在本研究中，我们提出了一种简单而有效的方法AuG-KD。它利用一种基于不确定性和样本特定的锚点来对齐学生领域。",
    "tldr": "提出了一种名为AuG-KD的方法，通过利用基于锚点的混合生成，解决了无数据知识蒸馏中的知识转移挑战。",
    "en_tdlr": "Introduced AuG-KD, a method that addresses the knowledge transfer challenge in data-free knowledge distillation through anchor-based mixup generation."
}