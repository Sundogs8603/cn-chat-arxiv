{
    "title": "On the Convergence of Federated Learning Algorithms without Data Similarity",
    "abstract": "arXiv:2403.02347v1 Announce Type: new  Abstract: Data similarity assumptions have traditionally been relied upon to understand the convergence behaviors of federated learning methods. Unfortunately, this approach often demands fine-tuning step sizes based on the level of data similarity. When data similarity is low, these small step sizes result in an unacceptably slow convergence speed for federated methods. In this paper, we present a novel and unified framework for analyzing the convergence of federated learning algorithms without the need for data similarity conditions. Our analysis centers on an inequality that captures the influence of step sizes on algorithmic convergence performance. By applying our theorems to well-known federated algorithms, we derive precise expressions for three widely used step size schedules: fixed, diminishing, and step-decay step sizes, which are independent of data similarity conditions. Finally, we conduct comprehensive evaluations of the performance ",
    "link": "https://arxiv.org/abs/2403.02347",
    "context": "Title: On the Convergence of Federated Learning Algorithms without Data Similarity\nAbstract: arXiv:2403.02347v1 Announce Type: new  Abstract: Data similarity assumptions have traditionally been relied upon to understand the convergence behaviors of federated learning methods. Unfortunately, this approach often demands fine-tuning step sizes based on the level of data similarity. When data similarity is low, these small step sizes result in an unacceptably slow convergence speed for federated methods. In this paper, we present a novel and unified framework for analyzing the convergence of federated learning algorithms without the need for data similarity conditions. Our analysis centers on an inequality that captures the influence of step sizes on algorithmic convergence performance. By applying our theorems to well-known federated algorithms, we derive precise expressions for three widely used step size schedules: fixed, diminishing, and step-decay step sizes, which are independent of data similarity conditions. Finally, we conduct comprehensive evaluations of the performance ",
    "path": "papers/24/03/2403.02347.json",
    "total_tokens": 857,
    "translated_title": "关于无需数据相似性条件的联邦学习算法收敛性",
    "translated_abstract": "数据相似性假设传统上被广泛依赖于理解联邦学习方法的收敛行为。不幸的是，这种方法通常要求根据数据相似性程度微调步长。当数据相似性较低时，这些小步长会导致联邦方法的收敛速度不可接受地慢。本文提出了一种新颖和统一的框架，用于分析联邦学习算法的收敛性，无需数据相似性条件。我们的分析集中在一个不等式上，这个不等式捕捉了步长对算法收敛性能的影响。通过将我们的定理应用于众所周知的联邦算法，我们推导出了三种广泛使用的步长调度的精确表达式：固定步长、递减步长和步衰减步长，这些表达式独立于数据相似性条件。最后，我们对性能进行了全面评估。",
    "tldr": "本文提出了一种无需数据相似性条件的联邦学习算法收敛性分析框架，通过推导出三种常用步长调度的精确表达式，实现了对算法收敛性能的全面评估。",
    "en_tdlr": "This paper introduces a novel framework for analyzing the convergence of federated learning algorithms without the need for data similarity conditions, deriving precise expressions for three commonly used step size schedules and conducting comprehensive evaluations of algorithmic convergence performance."
}