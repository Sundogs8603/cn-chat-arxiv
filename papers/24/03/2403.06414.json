{
    "title": "Evolving Knowledge Distillation with Large Language Models and Active Learning",
    "abstract": "arXiv:2403.06414v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across various NLP tasks. However, their computational costs are prohibitively high. To address this issue, previous research has attempted to distill the knowledge of LLMs into smaller models by generating annotated data. Nonetheless, these works have mainly focused on the direct use of LLMs for text generation and labeling, without fully exploring their potential to comprehend the target task and acquire valuable knowledge. In this paper, we propose EvoKD: Evolving Knowledge Distillation, which leverages the concept of active learning to interactively enhance the process of data generation using large language models, simultaneously improving the task capabilities of small domain model (student model). Different from previous work, we actively analyze the student model's weaknesses, and then synthesize labeled samples based on the analysis. In addition, we provide i",
    "link": "https://arxiv.org/abs/2403.06414",
    "context": "Title: Evolving Knowledge Distillation with Large Language Models and Active Learning\nAbstract: arXiv:2403.06414v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across various NLP tasks. However, their computational costs are prohibitively high. To address this issue, previous research has attempted to distill the knowledge of LLMs into smaller models by generating annotated data. Nonetheless, these works have mainly focused on the direct use of LLMs for text generation and labeling, without fully exploring their potential to comprehend the target task and acquire valuable knowledge. In this paper, we propose EvoKD: Evolving Knowledge Distillation, which leverages the concept of active learning to interactively enhance the process of data generation using large language models, simultaneously improving the task capabilities of small domain model (student model). Different from previous work, we actively analyze the student model's weaknesses, and then synthesize labeled samples based on the analysis. In addition, we provide i",
    "path": "papers/24/03/2403.06414.json",
    "total_tokens": 827,
    "translated_title": "利用大型语言模型和主动学习演进知识蒸馏",
    "translated_abstract": "大型语言模型（LLMs）在各种自然语言处理任务中展示了显著的能力。然而，它们的计算成本过高。为了解决这个问题，先前的研究尝试将LLMs的知识蒸馏到更小的模型中，通过生成带标注的数据来实现。然而，这些工作主要集中在直接利用LLMs进行文本生成和标注，而没有充分探索它们理解目标任务和获取有价值知识的潜力。在本文中，我们提出了EvoKD：演进知识蒸馏，利用主动学习的概念与大型语言模型交互地增强数据生成过程，同时改进小领域模型（学生模型）的任务能力。与以往的工作不同，我们主动分析学生模型的不足之处，然后基于该分析综合标记样本。此外，我们提供了i",
    "tldr": "本文提出了EvoKD，利用主动学习与大型语言模型交互地增强数据生成过程，同时改进小领域模型的任务能力。",
    "en_tdlr": "This paper introduces EvoKD, which leverages active learning to enhance the data generation process interactively with large language models while improving the task capabilities of small domain models."
}