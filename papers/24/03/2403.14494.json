{
    "title": "Learning to Project for Cross-Task Knowledge Distillation",
    "abstract": "arXiv:2403.14494v1 Announce Type: cross  Abstract: Traditional knowledge distillation (KD) relies on a proficient teacher trained on the target task, which is not always available. In this setting, cross-task distillation can be used, enabling the use of any teacher model trained on a different task. However, many KD methods prove ineffective when applied to this cross-task setting. To address this limitation, we propose a simple modification: the use of an inverted projection. We show that this drop-in replacement for a standard projector is effective by learning to disregard any task-specific features which might degrade the student's performance. We find that this simple modification is sufficient for extending many KD methods to the cross-task setting, where the teacher and student tasks can be very different. In doing so, we obtain up to a 1.9% improvement in the cross-task setting compared to the traditional projection, at no additional cost. Our method can obtain significant per",
    "link": "https://arxiv.org/abs/2403.14494",
    "context": "Title: Learning to Project for Cross-Task Knowledge Distillation\nAbstract: arXiv:2403.14494v1 Announce Type: cross  Abstract: Traditional knowledge distillation (KD) relies on a proficient teacher trained on the target task, which is not always available. In this setting, cross-task distillation can be used, enabling the use of any teacher model trained on a different task. However, many KD methods prove ineffective when applied to this cross-task setting. To address this limitation, we propose a simple modification: the use of an inverted projection. We show that this drop-in replacement for a standard projector is effective by learning to disregard any task-specific features which might degrade the student's performance. We find that this simple modification is sufficient for extending many KD methods to the cross-task setting, where the teacher and student tasks can be very different. In doing so, we obtain up to a 1.9% improvement in the cross-task setting compared to the traditional projection, at no additional cost. Our method can obtain significant per",
    "path": "papers/24/03/2403.14494.json",
    "total_tokens": 852,
    "translated_title": "学习投影以进行跨任务知识蒸馏",
    "translated_abstract": "传统知识蒸馏(KD)依赖于在目标任务上训练过的熟练教师，而这并不总是可用的。在这种情况下，可以使用跨任务蒸馏，使得可以利用在不同任务上训练过的任何教师模型。然而，许多知识蒸馏方法在应用于这种跨任务设置时被证明是无效的。为了解决这一限制，我们提出了一个简单的修改：使用反向投影。我们展示了这种对标准投影的插入式替代是有效的，通过学习排除可能降低学生表现的任何任务特定特征。我们发现，这个简单的修改足以将许多知识蒸馏方法扩展到跨任务设置，其中教师和学生任务可能非常不同。这样一来，在跨任务设置中，我们相比于传统投影，可获得最高1.9%的改进，而无需额外成本。我们的方法可以获得显著的性能提升",
    "tldr": "提出了一种通过学习投影，以有效地将传统知识蒸馏方法应用于跨任务设置的方法，取得了显著的性能提升",
    "en_tdlr": "Proposes a method of learning to project to effectively apply traditional knowledge distillation methods to cross-task settings, achieving significant performance improvement."
}