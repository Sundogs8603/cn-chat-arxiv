{
    "title": "Population-aware Online Mirror Descent for Mean-Field Games by Deep Reinforcement Learning",
    "abstract": "arXiv:2403.03552v1 Announce Type: cross  Abstract: Mean Field Games (MFGs) have the ability to handle large-scale multi-agent systems, but learning Nash equilibria in MFGs remains a challenging task. In this paper, we propose a deep reinforcement learning (DRL) algorithm that achieves population-dependent Nash equilibrium without the need for averaging or sampling from history, inspired by Munchausen RL and Online Mirror Descent. Through the design of an additional inner-loop replay buffer, the agents can effectively learn to achieve Nash equilibrium from any distribution, mitigating catastrophic forgetting. The resulting policy can be applied to various initial distributions. Numerical experiments on four canonical examples demonstrate our algorithm has better convergence properties than SOTA algorithms, in particular a DRL version of Fictitious Play for population-dependent policies.",
    "link": "https://arxiv.org/abs/2403.03552",
    "context": "Title: Population-aware Online Mirror Descent for Mean-Field Games by Deep Reinforcement Learning\nAbstract: arXiv:2403.03552v1 Announce Type: cross  Abstract: Mean Field Games (MFGs) have the ability to handle large-scale multi-agent systems, but learning Nash equilibria in MFGs remains a challenging task. In this paper, we propose a deep reinforcement learning (DRL) algorithm that achieves population-dependent Nash equilibrium without the need for averaging or sampling from history, inspired by Munchausen RL and Online Mirror Descent. Through the design of an additional inner-loop replay buffer, the agents can effectively learn to achieve Nash equilibrium from any distribution, mitigating catastrophic forgetting. The resulting policy can be applied to various initial distributions. Numerical experiments on four canonical examples demonstrate our algorithm has better convergence properties than SOTA algorithms, in particular a DRL version of Fictitious Play for population-dependent policies.",
    "path": "papers/24/03/2403.03552.json",
    "total_tokens": 879,
    "translated_title": "通过深度强化学习的人口感知在线镜像下降解均场博弈",
    "translated_abstract": "均场博弈（MFGs）具有处理大规模多智体系统的能力，但在MFGs中学习纳什均衡仍然是一项具有挑战性的任务。本文提出了一种深度强化学习（DRL）算法，通过Munchausen RL和在线镜像下降的启发，实现了基于人口的纳什均衡，无需对历史进行平均或抽样。通过设计额外的内循环重播缓冲区，智能体可以有效地学习从任何分布实现纳什均衡，减轻了灾难性遗忘的影响。由此产生的策略可以应用于各种初始分布。对四个经典示例的数值实验表明，我们的算法具有比SOTA算法更好的收敛特性，特别是基于人口的策略的DRL版本虚构博弈。",
    "tldr": "本文提出了一种通过深度强化学习算法实现基于人口的纳什均衡的方法，通过设计额外的内循环重播缓冲区，智能体可以有效地学习从任何分布实现纳什均衡，从而在多代理系统中展现出更好的收敛特性。",
    "en_tdlr": "This paper proposes a method to achieve population-dependent Nash equilibrium using deep reinforcement learning algorithm, where agents can effectively learn to achieve Nash equilibrium from any distribution through the design of an additional inner-loop replay buffer, showing better convergence properties in multi-agent systems."
}