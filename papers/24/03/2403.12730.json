{
    "title": "What Does Evaluation of Explainable Artificial Intelligence Actually Tell Us? A Case for Compositional and Contextual Validation of XAI Building Blocks",
    "abstract": "arXiv:2403.12730v1 Announce Type: cross  Abstract: Despite significant progress, evaluation of explainable artificial intelligence remains elusive and challenging. In this paper we propose a fine-grained validation framework that is not overly reliant on any one facet of these sociotechnical systems, and that recognises their inherent modular structure: technical building blocks, user-facing explanatory artefacts and social communication protocols. While we concur that user studies are invaluable in assessing the quality and effectiveness of explanation presentation and delivery strategies from the explainees' perspective in a particular deployment context, the underlying explanation generation mechanisms require a separate, predominantly algorithmic validation strategy that accounts for the technical and human-centred desiderata of their (numerical) outputs. Such a comprehensive sociotechnical utility-based evaluation framework could allow to systematically reason about the properties",
    "link": "https://arxiv.org/abs/2403.12730",
    "context": "Title: What Does Evaluation of Explainable Artificial Intelligence Actually Tell Us? A Case for Compositional and Contextual Validation of XAI Building Blocks\nAbstract: arXiv:2403.12730v1 Announce Type: cross  Abstract: Despite significant progress, evaluation of explainable artificial intelligence remains elusive and challenging. In this paper we propose a fine-grained validation framework that is not overly reliant on any one facet of these sociotechnical systems, and that recognises their inherent modular structure: technical building blocks, user-facing explanatory artefacts and social communication protocols. While we concur that user studies are invaluable in assessing the quality and effectiveness of explanation presentation and delivery strategies from the explainees' perspective in a particular deployment context, the underlying explanation generation mechanisms require a separate, predominantly algorithmic validation strategy that accounts for the technical and human-centred desiderata of their (numerical) outputs. Such a comprehensive sociotechnical utility-based evaluation framework could allow to systematically reason about the properties",
    "path": "papers/24/03/2403.12730.json",
    "total_tokens": 893,
    "translated_title": "论可解释人工智能评估实际告诉我们什么？支持对XAI构建模块进行组合和情境验证的案例",
    "translated_abstract": "尽管取得了显著进展，但解释人工智能的评估仍然难以捉摸和具有挑战性。在本文中，我们提出了一个精细的验证框架，不过度依赖于这些社会技术系统的任何一个方面，并认识到它们固有的模块化结构：技术构建模块、面向用户的解释工件和社交通信协议。虽然我们同意用户研究在评估解释呈现和交付策略的质量和有效性方面具有不可估量的价值，特别是从特定部署情境中解释者的角度来看，但潜在的解释生成机制需要一个单独的、主要是算法的验证策略，考虑到它们（数值）输出的技术和以人为中心的期望。这样一个全面的社会技术实用度评估框架可以让我们系统地推理有关这些性质的内容",
    "tldr": "本文提出了一个细粒度的验证框架，旨在不过度依赖于任何一个社会技术系统的方面，并承认其固有的模块化结构，从而使我们能够系统地推理有关这些性质的内容",
    "en_tdlr": "This paper proposes a fine-grained validation framework that aims not to overly rely on any one facet of sociotechnical systems and recognizes their inherent modular structure, allowing us to systematically reason about the properties."
}