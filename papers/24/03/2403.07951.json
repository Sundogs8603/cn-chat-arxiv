{
    "title": "SAMDA: Leveraging SAM on Few-Shot Domain Adaptation for Electronic Microscopy Segmentation",
    "abstract": "arXiv:2403.07951v1 Announce Type: cross  Abstract: It has been shown that traditional deep learning methods for electronic microscopy segmentation usually suffer from low transferability when samples and annotations are limited, while large-scale vision foundation models are more robust when transferring between different domains but facing sub-optimal improvement under fine-tuning. In this work, we present a new few-shot domain adaptation framework SAMDA, which combines the Segment Anything Model(SAM) with nnUNet in the embedding space to achieve high transferability and accuracy. Specifically, we choose the Unet-based network as the \"expert\" component to learn segmentation features efficiently and design a SAM-based adaptation module as the \"generic\" component for domain transfer. By amalgamating the \"generic\" and \"expert\" components, we mitigate the modality imbalance in the complex pre-training knowledge inherent to large-scale Vision Foundation models and the challenge of transfer",
    "link": "https://arxiv.org/abs/2403.07951",
    "context": "Title: SAMDA: Leveraging SAM on Few-Shot Domain Adaptation for Electronic Microscopy Segmentation\nAbstract: arXiv:2403.07951v1 Announce Type: cross  Abstract: It has been shown that traditional deep learning methods for electronic microscopy segmentation usually suffer from low transferability when samples and annotations are limited, while large-scale vision foundation models are more robust when transferring between different domains but facing sub-optimal improvement under fine-tuning. In this work, we present a new few-shot domain adaptation framework SAMDA, which combines the Segment Anything Model(SAM) with nnUNet in the embedding space to achieve high transferability and accuracy. Specifically, we choose the Unet-based network as the \"expert\" component to learn segmentation features efficiently and design a SAM-based adaptation module as the \"generic\" component for domain transfer. By amalgamating the \"generic\" and \"expert\" components, we mitigate the modality imbalance in the complex pre-training knowledge inherent to large-scale Vision Foundation models and the challenge of transfer",
    "path": "papers/24/03/2403.07951.json",
    "total_tokens": 865,
    "translated_title": "SAMDA：利用SAM进行电子显微镜分割的少样本领域自适应",
    "translated_abstract": "传统的电子显微镜分割深度学习方法通常在样本和注释有限时存在转移性较低的问题，而大规模视觉基础模型在不同领域之间转移时更具鲁棒性，但在微调下面临亚最优改进。在这项工作中，我们提出了一种新的少样本领域自适应框架SAMDA，它将“Segment Anything Model (SAM)”与nnUNet相结合到嵌入空间中以实现高转移性和准确性。具体来说，我们选择基于Unet的网络作为“专家”组件高效学习分割特征，并设计了基于SAM的自适应模块作为“通用”组件用于领域转移。通过融合“通用”和“专家”组件，我们缓解了大规模视觉基础模型中复杂预训练知识中的模态不平衡和转移挑战。",
    "tldr": "SAMDA框架结合了SAM和nnUNet，通过融合“专家”和“通用”组件，在少样本领域自适应中解决了大规模视觉基础模型面临的转移性和精度问题"
}