{
    "title": "ChainLM: Empowering Large Language Models with Improved Chain-of-Thought Prompting",
    "abstract": "arXiv:2403.14312v1 Announce Type: new  Abstract: Chain-of-Thought (CoT) prompting can enhance the reasoning capabilities of large language models (LLMs), establishing itself as a primary approach to solving complex reasoning tasks. Existing CoT synthesis approaches usually focus on simpler reasoning tasks and thus result in low-quality and inconsistent CoT prompts. In response to this challenge, we present an empirical investigation of CoT prompting and introduce CoTGenius, a novel framework designed for the automatic generation of superior CoT prompts. CoTGenius is developed based on three major evolution strategies, i.e., complicate, diversify, and specify-alongside two filtering mechanisms: evolutionary success judgement and correctness verification. We further employ CoTGenius to create an extensive CoT dataset, and subsequently fine-tune the Llama 2-Chat 7B and 13B models on this dataset. We call the resulting model ChainLM. To deal with the cumulative error issue in reasoning ste",
    "link": "https://arxiv.org/abs/2403.14312",
    "context": "Title: ChainLM: Empowering Large Language Models with Improved Chain-of-Thought Prompting\nAbstract: arXiv:2403.14312v1 Announce Type: new  Abstract: Chain-of-Thought (CoT) prompting can enhance the reasoning capabilities of large language models (LLMs), establishing itself as a primary approach to solving complex reasoning tasks. Existing CoT synthesis approaches usually focus on simpler reasoning tasks and thus result in low-quality and inconsistent CoT prompts. In response to this challenge, we present an empirical investigation of CoT prompting and introduce CoTGenius, a novel framework designed for the automatic generation of superior CoT prompts. CoTGenius is developed based on three major evolution strategies, i.e., complicate, diversify, and specify-alongside two filtering mechanisms: evolutionary success judgement and correctness verification. We further employ CoTGenius to create an extensive CoT dataset, and subsequently fine-tune the Llama 2-Chat 7B and 13B models on this dataset. We call the resulting model ChainLM. To deal with the cumulative error issue in reasoning ste",
    "path": "papers/24/03/2403.14312.json",
    "total_tokens": 829,
    "translated_title": "ChainLM：借助改进的思维链提示赋能大型语言模型",
    "translated_abstract": "Chain-of-Thought (CoT)提示可以增强大型语言模型（LLMs）的推理能力，是解决复杂推理任务的主要方法之一。现有的CoT合成方法通常专注于更简单的推理任务，因此导致CoT提示质量低且不一致。针对这一挑战，我们对CoT提示进行了实证研究，并提出了CoTGenius，这是一个旨在自动生成优质CoT提示的新型框架。CoTGenius基于三种主要的进化策略发展而来，即复杂化、多样化和具体化，同时配备两种过滤机制：进化成功评判和正确性验证。我们进一步利用CoTGenius创建了一个庞大的CoT数据集，并随后在该数据集上对Llama 2-Chat 7B和13B模型进行微调。最终得到的模型被命名为ChainLM。",
    "tldr": "提出了CoTGenius框架，用于自动生成优质CoT提示，并通过它创建了庞大的CoT数据集以提升大型语言模型的推理能力",
    "en_tdlr": "Introduced the CoTGenius framework for automatically generating high-quality CoT prompts and created a large CoT dataset to enhance the reasoning capabilities of large language models."
}