{
    "title": "NovelQA: A Benchmark for Long-Range Novel Question Answering",
    "abstract": "arXiv:2403.12766v1 Announce Type: new  Abstract: The rapid advancement of Large Language Models (LLMs) has introduced a new frontier in natural language processing, particularly in understanding and processing long-context information. However, the evaluation of these models' long-context abilities remains a challenge due to the limitations of current benchmarks. To address this gap, we introduce NovelQA, a benchmark specifically designed to test the capabilities of LLMs with extended texts. Constructed from English novels, NovelQA offers a unique blend of complexity, length, and narrative coherence, making it an ideal tool for assessing deep textual understanding in LLMs. This paper presents the design and construction of NovelQA, highlighting its manual annotation, and diverse question types. Our evaluation of Long-context LLMs on NovelQA reveals significant insights into the models' performance, particularly emphasizing the challenges they face with multi-hop reasoning, detail-orien",
    "link": "https://arxiv.org/abs/2403.12766",
    "context": "Title: NovelQA: A Benchmark for Long-Range Novel Question Answering\nAbstract: arXiv:2403.12766v1 Announce Type: new  Abstract: The rapid advancement of Large Language Models (LLMs) has introduced a new frontier in natural language processing, particularly in understanding and processing long-context information. However, the evaluation of these models' long-context abilities remains a challenge due to the limitations of current benchmarks. To address this gap, we introduce NovelQA, a benchmark specifically designed to test the capabilities of LLMs with extended texts. Constructed from English novels, NovelQA offers a unique blend of complexity, length, and narrative coherence, making it an ideal tool for assessing deep textual understanding in LLMs. This paper presents the design and construction of NovelQA, highlighting its manual annotation, and diverse question types. Our evaluation of Long-context LLMs on NovelQA reveals significant insights into the models' performance, particularly emphasizing the challenges they face with multi-hop reasoning, detail-orien",
    "path": "papers/24/03/2403.12766.json",
    "total_tokens": 878,
    "translated_title": "NovelQA：用于长距离小说问答的基准",
    "translated_abstract": "大型语言模型（LLM）的快速发展引入了自然语言处理的新领域，特别是在理解和处理长文本信息方面。然而，由于当前基准的局限性，评估这些模型的长文本能力仍然是一个挑战。为了填补这一空白，我们引入了NovelQA，这是一个专门设计用于测试具有扩展文本的LLM能力的基准。NovelQA由英文小说构建，提供了复杂性、长度和叙述连贯性的独特组合，使其成为评估LLM中深度文本理解的理想工具。本文介绍了NovelQA的设计与构建，突出了其手动注释和多样的问题类型。我们在NovelQA上对长文本LLM进行评估，揭示了模型性能的重要见解，特别强调了它们在多跳推理、细节导向等方面所面临的挑战。",
    "tldr": "NovelQA是一个专门设计用于测试大型语言模型（LLMs）在长文本上的能力的基准，通过英文小说构建，提供了复杂性、长度和叙述连贯性的独特组合，可用于评估LLMs在深度文本理解方面的性能表现。",
    "en_tdlr": "NovelQA is a benchmark specifically designed to test the capabilities of Large Language Models (LLMs) with extended texts, constructed from English novels, offering a unique blend of complexity, length, and narrative coherence for assessing deep textual understanding in LLMs."
}