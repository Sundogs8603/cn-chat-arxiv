{
    "title": "RuBia: A Russian Language Bias Detection Dataset",
    "abstract": "arXiv:2403.17553v1 Announce Type: new  Abstract: Warning: this work contains upsetting or disturbing content.   Large language models (LLMs) tend to learn the social and cultural biases present in the raw pre-training data. To test if an LLM's behavior is fair, functional datasets are employed, and due to their purpose, these datasets are highly language and culture-specific. In this paper, we address a gap in the scope of multilingual bias evaluation by presenting a bias detection dataset specifically designed for the Russian language, dubbed as RuBia. The RuBia dataset is divided into 4 domains: gender, nationality, socio-economic status, and diverse, each of the domains is further divided into multiple fine-grained subdomains. Every example in the dataset consists of two sentences with the first reinforcing a potentially harmful stereotype or trope and the second contradicting it. These sentence pairs were first written by volunteers and then validated by native-speaking crowdsourci",
    "link": "https://arxiv.org/abs/2403.17553",
    "context": "Title: RuBia: A Russian Language Bias Detection Dataset\nAbstract: arXiv:2403.17553v1 Announce Type: new  Abstract: Warning: this work contains upsetting or disturbing content.   Large language models (LLMs) tend to learn the social and cultural biases present in the raw pre-training data. To test if an LLM's behavior is fair, functional datasets are employed, and due to their purpose, these datasets are highly language and culture-specific. In this paper, we address a gap in the scope of multilingual bias evaluation by presenting a bias detection dataset specifically designed for the Russian language, dubbed as RuBia. The RuBia dataset is divided into 4 domains: gender, nationality, socio-economic status, and diverse, each of the domains is further divided into multiple fine-grained subdomains. Every example in the dataset consists of two sentences with the first reinforcing a potentially harmful stereotype or trope and the second contradicting it. These sentence pairs were first written by volunteers and then validated by native-speaking crowdsourci",
    "path": "papers/24/03/2403.17553.json",
    "total_tokens": 861,
    "translated_title": "RuBia: 一个俄语语言偏见检测数据集",
    "translated_abstract": "大型语言模型（LLMs）往往会学习原始预训练数据中存在的社会文化偏见。为了测试LLM的行为是否公平，需要使用功能性数据集，而这些数据集由于设计目的，通常高度依赖于语言和文化。本文通过提出一种特别针对俄语设计的偏见检测数据集RuBia，填补了多语言偏见评估范围的空白。RuBia数据集分为4个领域：性别、国籍、社会经济地位和多元化，每个领域又进一步分为多个细粒度子域。数据集中的每个示例包含两个句子，第一个句子强化了一个潜在的有害刻板印象或模式，第二个句子则与之相矛盾。这些句对首先由志愿者编写，然后由母语人士的众包验证。",
    "tldr": "本论文提出了一个针对俄语的偏见检测数据集RuBia，填补了多语言偏见评估范围的空白，对于测试大型语言模型在俄语中是否存在偏见具有重要意义。"
}