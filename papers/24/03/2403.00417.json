{
    "title": "Rethinking Tokenization: Crafting Better Tokenizers for Large Language Models",
    "abstract": "arXiv:2403.00417v1 Announce Type: new  Abstract: Tokenization significantly influences language models(LMs)' performance. This paper traces the evolution of tokenizers from word-level to subword-level, analyzing how they balance tokens and types to enhance model adaptability while controlling complexity. Despite subword tokenizers like Byte Pair Encoding (BPE) overcoming many word tokenizer limitations, they encounter difficulties in handling non-Latin languages and depend heavily on extensive training data and computational resources to grasp the nuances of multiword expressions (MWEs). This article argues that tokenizers, more than mere technical tools, should drawing inspiration from the cognitive science about human language processing. This study then introduces the \"Principle of Least Effort\" from cognitive science, that humans naturally seek to reduce cognitive effort, and discusses the benefits of this principle for tokenizer development. Based on this principle, the paper prop",
    "link": "https://arxiv.org/abs/2403.00417",
    "context": "Title: Rethinking Tokenization: Crafting Better Tokenizers for Large Language Models\nAbstract: arXiv:2403.00417v1 Announce Type: new  Abstract: Tokenization significantly influences language models(LMs)' performance. This paper traces the evolution of tokenizers from word-level to subword-level, analyzing how they balance tokens and types to enhance model adaptability while controlling complexity. Despite subword tokenizers like Byte Pair Encoding (BPE) overcoming many word tokenizer limitations, they encounter difficulties in handling non-Latin languages and depend heavily on extensive training data and computational resources to grasp the nuances of multiword expressions (MWEs). This article argues that tokenizers, more than mere technical tools, should drawing inspiration from the cognitive science about human language processing. This study then introduces the \"Principle of Least Effort\" from cognitive science, that humans naturally seek to reduce cognitive effort, and discusses the benefits of this principle for tokenizer development. Based on this principle, the paper prop",
    "path": "papers/24/03/2403.00417.json",
    "total_tokens": 893,
    "translated_title": "重新思考标记化：为大型语言模型打造更好的标记化器",
    "translated_abstract": "标记化显著影响语言模型(LMs)的性能。本文追溯了标记化器从单词级别到子词级别的演变，分析它们如何平衡标记和类型以增强模型的适应性同时控制复杂性。尽管像字节对编码(BPE)这样的子词标记器克服了许多单词标记器的局限，但它们在处理非拉丁语言方面遇到困难，并且严重依赖大量的训练数据和计算资源来掌握多词表达的微妙之处。本文认为，标记化器不仅仅是技术工具，还应该从关于人类语言处理的认知科学中汲取灵感。该研究随后介绍了认知科学中的“最小努力原则”，即人类自然寻求减少认知努力，并讨论了该原则对标记器发展的益处。基于这一原则，本文提出...",
    "tldr": "标记化显著影响语言模型的性能，本研究从单词级别到子词级别追溯了标记化器的演变并提出了从认知科学中的“最小努力原则”获得启示，助力标记化器发展。",
    "en_tdlr": "Tokenization significantly influences language model performance. This study traces the evolution of tokenizers from word-level to subword-level and introduces insights from the \"Principle of Least Effort\" in cognitive science to aid in the development of tokenizers."
}