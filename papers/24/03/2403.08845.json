{
    "title": "Bifurcated Attention for Single-Context Large-Batch Sampling",
    "abstract": "arXiv:2403.08845v1 Announce Type: cross  Abstract: In our study, we present bifurcated attention, a method developed for language model inference in single-context batch sampling contexts. This approach aims to reduce redundant memory IO costs, a significant factor in latency for high batch sizes and long context lengths. Bifurcated attention achieves this by dividing the attention mechanism during incremental decoding into two distinct GEMM operations, focusing on the KV cache from prefill and the decoding process. This method ensures precise computation and maintains the usual computational load (FLOPs) of standard attention mechanisms, but with reduced memory IO. Bifurcated attention is also compatible with multi-query attention mechanism known for reduced memory IO for KV cache, further enabling higher batch size and context length. The resulting efficiency leads to lower latency, improving suitability for real-time applications, e.g., enabling massively-parallel answer generation ",
    "link": "https://arxiv.org/abs/2403.08845",
    "context": "Title: Bifurcated Attention for Single-Context Large-Batch Sampling\nAbstract: arXiv:2403.08845v1 Announce Type: cross  Abstract: In our study, we present bifurcated attention, a method developed for language model inference in single-context batch sampling contexts. This approach aims to reduce redundant memory IO costs, a significant factor in latency for high batch sizes and long context lengths. Bifurcated attention achieves this by dividing the attention mechanism during incremental decoding into two distinct GEMM operations, focusing on the KV cache from prefill and the decoding process. This method ensures precise computation and maintains the usual computational load (FLOPs) of standard attention mechanisms, but with reduced memory IO. Bifurcated attention is also compatible with multi-query attention mechanism known for reduced memory IO for KV cache, further enabling higher batch size and context length. The resulting efficiency leads to lower latency, improving suitability for real-time applications, e.g., enabling massively-parallel answer generation ",
    "path": "papers/24/03/2403.08845.json",
    "total_tokens": 870,
    "translated_title": "单上下文大批量抽样的分叉注意力",
    "translated_abstract": "在我们的研究中，我们提出了一种称为分叉注意力的方法，用于单上下文批量抽样环境下的语言模型推断。这种方法旨在减少冗余的内存IO成本，这是高批量大小和长上下文长度的延迟的重要因素。分叉注意力通过在增量解码期间将注意力机制划分为两个不同的GEMM操作，分别专注于来自预填充的KV缓存以及解码过程，从而实现这一目标。该方法确保了精确的计算，并维持常规注意力机制的计算负载（FLOPs），但减少了内存IO。分叉注意力还与减少KV缓存内存IO已知的多查询注意力机制兼容，进一步实现更高的批量大小和上下文长度。由此带来的效率导致更低的延迟，改善了实时应用的适用性，例如实现大规模并行的答案生成。",
    "tldr": "分叉注意力是针对语言模型推断中单上下文批量抽样环境开发的方法，通过将注意力机制分成两个独立的操作来减少冗余内存IO成本，提高效率并降低延迟。",
    "en_tdlr": "Bifurcated attention is a method developed for language model inference in single-context batch sampling environments, aiming to reduce redundant memory IO costs by dividing the attention mechanism into two distinct operations, improving efficiency and reducing latency."
}