{
    "title": "VTG-GPT: Tuning-Free Zero-Shot Video Temporal Grounding with GPT",
    "abstract": "arXiv:2403.02076v1 Announce Type: cross  Abstract: Video temporal grounding (VTG) aims to locate specific temporal segments from an untrimmed video based on a linguistic query. Most existing VTG models are trained on extensive annotated video-text pairs, a process that not only introduces human biases from the queries but also incurs significant computational costs. To tackle these challenges, we propose VTG-GPT, a GPT-based method for zero-shot VTG without training or fine-tuning. To reduce prejudice in the original query, we employ Baichuan2 to generate debiased queries. To lessen redundant information in videos, we apply MiniGPT-v2 to transform visual content into more precise captions. Finally, we devise the proposal generator and post-processing to produce accurate segments from debiased queries and image captions. Extensive experiments demonstrate that VTG-GPT significantly outperforms SOTA methods in zero-shot settings and surpasses unsupervised approaches. More notably, it achi",
    "link": "https://arxiv.org/abs/2403.02076",
    "context": "Title: VTG-GPT: Tuning-Free Zero-Shot Video Temporal Grounding with GPT\nAbstract: arXiv:2403.02076v1 Announce Type: cross  Abstract: Video temporal grounding (VTG) aims to locate specific temporal segments from an untrimmed video based on a linguistic query. Most existing VTG models are trained on extensive annotated video-text pairs, a process that not only introduces human biases from the queries but also incurs significant computational costs. To tackle these challenges, we propose VTG-GPT, a GPT-based method for zero-shot VTG without training or fine-tuning. To reduce prejudice in the original query, we employ Baichuan2 to generate debiased queries. To lessen redundant information in videos, we apply MiniGPT-v2 to transform visual content into more precise captions. Finally, we devise the proposal generator and post-processing to produce accurate segments from debiased queries and image captions. Extensive experiments demonstrate that VTG-GPT significantly outperforms SOTA methods in zero-shot settings and surpasses unsupervised approaches. More notably, it achi",
    "path": "papers/24/03/2403.02076.json",
    "total_tokens": 870,
    "translated_title": "VTG-GPT：使用GPT实现免调优零样本视频时间定位",
    "translated_abstract": "视频时间定位（VTG）旨在根据语言查询从未经剪辑的视频中定位特定的时间段。大多数现有的VTG模型都是在大量带注释的视频文本对上进行训练的，这个过程不仅引入了来自查询的人为偏见，还带来了显著的计算成本。为了解决这些挑战，我们提出了VTG-GPT，这是一种基于GPT的零调优VTG方法。为了减少原始查询中的偏见，我们使用Baichuan2生成无偏查询。为了减少视频中的冗余信息，我们应用MiniGPT-v2将视觉内容转换为更精确的字幕。最后，我们设计了提案生成器和后处理来从无偏查询和图像字幕中生成准确的段。大量实验证明，VTG-GPT在零样本设置中明显优于SOTA方法，并超越了无监督方法。",
    "tldr": "提出了一种基于GPT的零调优视频时间定位方法VTG-GPT，通过生成无偏查询和更精确的视觉描述，实现了在零样本设置中明显优于现有方法和无监督方法的性能提升",
    "en_tdlr": "Introduced a tuning-free video temporal grounding method VTG-GPT based on GPT, which significantly outperforms existing methods and unsupervised approaches in zero-shot settings by generating unbiased queries and more precise visual descriptions."
}