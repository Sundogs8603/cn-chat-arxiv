{
    "title": "Feature CAM: Interpretable AI in Image Classification",
    "abstract": "arXiv:2403.05658v1 Announce Type: cross  Abstract: Deep Neural Networks have often been called the black box because of the complex, deep architecture and non-transparency presented by the inner layers. There is a lack of trust to use Artificial Intelligence in critical and high-precision fields such as security, finance, health, and manufacturing industries. A lot of focused work has been done to provide interpretable models, intending to deliver meaningful insights into the thoughts and behavior of neural networks. In our research, we compare the state-of-the-art methods in the Activation-based methods (ABM) for interpreting predictions of CNN models, specifically in the application of Image Classification. We then extend the same for eight CNN-based architectures to compare the differences in visualization and thus interpretability. We introduced a novel technique Feature CAM, which falls in the perturbation-activation combination, to create fine-grained, class-discriminative visual",
    "link": "https://arxiv.org/abs/2403.05658",
    "context": "Title: Feature CAM: Interpretable AI in Image Classification\nAbstract: arXiv:2403.05658v1 Announce Type: cross  Abstract: Deep Neural Networks have often been called the black box because of the complex, deep architecture and non-transparency presented by the inner layers. There is a lack of trust to use Artificial Intelligence in critical and high-precision fields such as security, finance, health, and manufacturing industries. A lot of focused work has been done to provide interpretable models, intending to deliver meaningful insights into the thoughts and behavior of neural networks. In our research, we compare the state-of-the-art methods in the Activation-based methods (ABM) for interpreting predictions of CNN models, specifically in the application of Image Classification. We then extend the same for eight CNN-based architectures to compare the differences in visualization and thus interpretability. We introduced a novel technique Feature CAM, which falls in the perturbation-activation combination, to create fine-grained, class-discriminative visual",
    "path": "papers/24/03/2403.05658.json",
    "total_tokens": 760,
    "translated_title": "Feature CAM: 图像分类中的可解释AI",
    "translated_abstract": "深度神经网络常被称为黑盒子，因为其复杂的深层结构和内部层的不透明性。人们在将人工智能应用于安全、金融、健康和制造业等关键和高精度领域时常常缺乏信任。我们研究了激活方法（ABM）的最新方法，用于解释CNN模型对图像分类应用的预测。我们扩展了同样的方法，比较了八种基于CNN的体系结构的差异，从而提高了可解释性。我们引入了一种新颖的特征CAM技术，它属于扰动激活组合，用于创建细粒度的、具有类别区分性的可视化。",
    "tldr": "本研究比较了激活方法在CNN模型图像分类预测解释中的应用，提出了一种新颖的Feature CAM技术，用于提高模型的可解释性。"
}