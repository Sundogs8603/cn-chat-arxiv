{
    "title": "NTK-Guided Few-Shot Class Incremental Learning",
    "abstract": "arXiv:2403.12486v1 Announce Type: cross  Abstract: While anti-amnesia FSCIL learners often excel in incremental sessions, they tend to prioritize mitigating knowledge attrition over harnessing the model's potential for knowledge acquisition. In this paper, we delve into the foundations of model generalization in FSCIL through the lens of the Neural Tangent Kernel (NTK). Our primary design focus revolves around ensuring optimal NTK convergence and NTK-related generalization error, serving as the theoretical bedrock for exceptional generalization. To attain globally optimal NTK convergence, we employ a meta-learning mechanism grounded in mathematical principles to guide the optimization process within an expanded network. Furthermore, to reduce the NTK-related generalization error, we commence from the foundational level, optimizing the relevant factors constituting its generalization loss. Specifically, we initiate self-supervised pre-training on the base session to shape the initial ne",
    "link": "https://arxiv.org/abs/2403.12486",
    "context": "Title: NTK-Guided Few-Shot Class Incremental Learning\nAbstract: arXiv:2403.12486v1 Announce Type: cross  Abstract: While anti-amnesia FSCIL learners often excel in incremental sessions, they tend to prioritize mitigating knowledge attrition over harnessing the model's potential for knowledge acquisition. In this paper, we delve into the foundations of model generalization in FSCIL through the lens of the Neural Tangent Kernel (NTK). Our primary design focus revolves around ensuring optimal NTK convergence and NTK-related generalization error, serving as the theoretical bedrock for exceptional generalization. To attain globally optimal NTK convergence, we employ a meta-learning mechanism grounded in mathematical principles to guide the optimization process within an expanded network. Furthermore, to reduce the NTK-related generalization error, we commence from the foundational level, optimizing the relevant factors constituting its generalization loss. Specifically, we initiate self-supervised pre-training on the base session to shape the initial ne",
    "path": "papers/24/03/2403.12486.json",
    "total_tokens": 826,
    "translated_title": "基于NTK引导的少样本类增量学习",
    "translated_abstract": "尽管反遗忘FSCIL学习者在增量会话中表现出色，但他们往往更注重减少知识流失，而忽视了模型潜在获取知识的能力。本文通过神经切向核（NTK）的视角深入探讨了FSCIL模型泛化的基础。我们主要的设计重点在于确保最优NTK收敛和NTK相关的泛化误差，作为卓越泛化的理论基础。为了达到全局最优的NTK收敛，我们采用了一个植根于数学原理的元学习机制，指导扩展网络内的优化过程。此外，为了减少NTK相关的泛化误差，我们从基础层面开始，优化构成其泛化损失的相关因素。具体地，我们通过在基础会话上启动自监督预训练来塑造初始ne",
    "tldr": "本文通过NTK对FSCIL模型的指导，致力于在增量学习中实现卓越泛化，通过优化NTK收敛和降低泛化误差来确保最佳性能。",
    "en_tdlr": "This paper focuses on achieving exceptional generalization in incremental learning through NTK guidance, aiming to ensure optimal performance by optimizing NTK convergence and reducing generalization error."
}