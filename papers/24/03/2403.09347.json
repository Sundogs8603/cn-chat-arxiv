{
    "title": "BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences",
    "abstract": "arXiv:2403.09347v1 Announce Type: cross  Abstract: Effective attention modules have played a crucial role in the success of Transformer-based large language models (LLMs), but the quadratic time and memory complexities of these attention modules also pose a challenge when processing long sequences. One potential solution for the long sequence problem is to utilize distributed clusters to parallelize the computation of attention modules across multiple devices (e.g., GPUs). However, adopting a distributed approach inevitably introduces extra memory overheads to store local attention results and incurs additional communication costs to aggregate local results into global ones. In this paper, we propose a distributed attention framework named ``BurstAttention'' to optimize memory access and communication operations at both the global cluster and local device levels. In our experiments, we compare BurstAttention with other competitive distributed attention solutions for long sequence proce",
    "link": "https://arxiv.org/abs/2403.09347",
    "context": "Title: BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences\nAbstract: arXiv:2403.09347v1 Announce Type: cross  Abstract: Effective attention modules have played a crucial role in the success of Transformer-based large language models (LLMs), but the quadratic time and memory complexities of these attention modules also pose a challenge when processing long sequences. One potential solution for the long sequence problem is to utilize distributed clusters to parallelize the computation of attention modules across multiple devices (e.g., GPUs). However, adopting a distributed approach inevitably introduces extra memory overheads to store local attention results and incurs additional communication costs to aggregate local results into global ones. In this paper, we propose a distributed attention framework named ``BurstAttention'' to optimize memory access and communication operations at both the global cluster and local device levels. In our experiments, we compare BurstAttention with other competitive distributed attention solutions for long sequence proce",
    "path": "papers/24/03/2403.09347.json",
    "total_tokens": 624,
    "translated_title": "BurstAttention：一种用于处理极长序列的高效分布式注意力框架",
    "translated_abstract": "有效的注意力模块在基于Transformer的大型语言模型（LLMs）的成功中起着关键作用，但这些注意力模块的二次时间和内存复杂度在处理长序列时也构成了挑战。本文提出了一种名为“BurstAttention”的分布式注意力框架，以优化全局集群和本地设备级别的内存访问和通信操作。",
    "tldr": "BurstAttention是一种用于优化内存访问和通信操作的分布式注意力框架，适用于处理极长序列。",
    "en_tdlr": "BurstAttention is a distributed attention framework optimized for memory access and communication operations, suitable for processing extremely long sequences."
}