{
    "title": "On the Benefits of Fine-Grained Loss Truncation: A Case Study on Factuality in Summarization",
    "abstract": "arXiv:2403.05788v1 Announce Type: cross  Abstract: Text summarization and simplification are among the most widely used applications of AI. However, models developed for such tasks are often prone to hallucination, which can result from training on unaligned data. One efficient approach to address this issue is Loss Truncation (LT) (Kang and Hashimoto, 2020), an approach to modify the standard log loss to adaptively remove noisy examples during training. However, we find that LT alone yields a considerable number of hallucinated entities on various datasets. We study the behavior of the underlying losses between factual and non-factual examples, to understand and refine the performance of LT. We demonstrate that LT's performance is limited when the underlying assumption that noisy targets have higher NLL loss is not satisfied, and find that word-level NLL among entities provides better signal for distinguishing factuality. We then leverage this to propose a fine-grained NLL loss and fi",
    "link": "https://arxiv.org/abs/2403.05788",
    "context": "Title: On the Benefits of Fine-Grained Loss Truncation: A Case Study on Factuality in Summarization\nAbstract: arXiv:2403.05788v1 Announce Type: cross  Abstract: Text summarization and simplification are among the most widely used applications of AI. However, models developed for such tasks are often prone to hallucination, which can result from training on unaligned data. One efficient approach to address this issue is Loss Truncation (LT) (Kang and Hashimoto, 2020), an approach to modify the standard log loss to adaptively remove noisy examples during training. However, we find that LT alone yields a considerable number of hallucinated entities on various datasets. We study the behavior of the underlying losses between factual and non-factual examples, to understand and refine the performance of LT. We demonstrate that LT's performance is limited when the underlying assumption that noisy targets have higher NLL loss is not satisfied, and find that word-level NLL among entities provides better signal for distinguishing factuality. We then leverage this to propose a fine-grained NLL loss and fi",
    "path": "papers/24/03/2403.05788.json",
    "total_tokens": 914,
    "translated_title": "对细粒度损失截断效益的研究：以摘要中的事实性为例",
    "translated_abstract": "arXiv:2403.05788v1 公告类型：跨项 摘要：文本摘要和简化是人工智能中最广泛使用的应用之一。然而，针对这些任务开发的模型往往容易出现幻觉，这可能是因为在未对齐数据上进行训练。解决这一问题的一种有效方法是损失截断（LT）（Kang和Hashimoto，2020），这是一种修改标准对数损失以在训练过程中自适应地去除嘈杂示例的方法。然而，我们发现仅使用LT在各种数据集上会产生大量幻觉实体。我们研究了事实和非事实示例之间基础损失的行为，以了解并改进LT的性能。我们证明了当嘈杂目标具有较高NLL损失的基础假设不被满足时，LT的性能是有限的，并发现实体之间的单词级NLL为区分事实性提供了更好的信号。然后我们利用这一点提出了一种细粒度NLL损失和fi",
    "tldr": "通过细粒度NLL损失和fi以更好地区分事实性，改进了细粒度损失截断对于摘要中事实性的影响。",
    "en_tdlr": "Improved the impact of fine-grained loss truncation on factuality in summarization by utilizing fine-grained NLL loss and fi to better differentiate between factual and non-factual examples."
}