{
    "title": "Multi-Frame, Lightweight & Efficient Vision-Language Models for Question Answering in Autonomous Driving",
    "abstract": "arXiv:2403.19838v1 Announce Type: cross  Abstract: Vision-Language Models (VLMs) and Multi-Modal Language models (MMLMs) have become prominent in autonomous driving research, as these models can provide interpretable textual reasoning and responses for end-to-end autonomous driving safety tasks using traffic scene images and other data modalities. However, current approaches to these systems use expensive large language model (LLM) backbones and image encoders, making such systems unsuitable for real-time autonomous driving systems where tight memory constraints exist and fast inference time is necessary. To address these previous issues, we develop EM-VLM4AD, an efficient, lightweight, multi-frame vision language model which performs Visual Question Answering for autonomous driving. In comparison to previous approaches, EM-VLM4AD requires at least 10 times less memory and floating point operations, while also achieving higher BLEU-4, METEOR, CIDEr, and ROGUE scores than the existing b",
    "link": "https://arxiv.org/abs/2403.19838",
    "context": "Title: Multi-Frame, Lightweight & Efficient Vision-Language Models for Question Answering in Autonomous Driving\nAbstract: arXiv:2403.19838v1 Announce Type: cross  Abstract: Vision-Language Models (VLMs) and Multi-Modal Language models (MMLMs) have become prominent in autonomous driving research, as these models can provide interpretable textual reasoning and responses for end-to-end autonomous driving safety tasks using traffic scene images and other data modalities. However, current approaches to these systems use expensive large language model (LLM) backbones and image encoders, making such systems unsuitable for real-time autonomous driving systems where tight memory constraints exist and fast inference time is necessary. To address these previous issues, we develop EM-VLM4AD, an efficient, lightweight, multi-frame vision language model which performs Visual Question Answering for autonomous driving. In comparison to previous approaches, EM-VLM4AD requires at least 10 times less memory and floating point operations, while also achieving higher BLEU-4, METEOR, CIDEr, and ROGUE scores than the existing b",
    "path": "papers/24/03/2403.19838.json",
    "total_tokens": 887,
    "translated_title": "多帧、轻量级和高效的视觉-语言模型用于自动驾驶问答",
    "translated_abstract": "视觉-语言模型（VLMs）和多模态语言模型（MMLMs）已经在自动驾驶研究中变得突出，因为这些模型可以利用交通场景图像和其他数据模态提供可解释的文本推理和响应，用于端到端自动驾驶安全任务。然而，当前针对这些系统的方法使用昂贵的大型语言模型（LLM）骨干和图像编码器，使得这些系统不适合具有严格内存限制和需要快速推理时间的实时自动驾驶系统。为解决这些先前问题，我们开发了EM-VLM4AD，一种高效、轻量级、多帧视觉语言模型，用于执行自动驾驶的视觉问答。",
    "tldr": "提出了一种高效、轻量级、多帧视觉语言模型 EM-VLM4AD，用于自动驾驶的视觉问答，相比现有方法，内存和浮点运算需求至少减少十倍，并且在BLEU-4、METEOR、CIDEr和ROGUE分数上均取得更高的表现。",
    "en_tdlr": "Proposed an efficient, lightweight, multi-frame vision language model EM-VLM4AD for visual question answering in autonomous driving. Compared to existing methods, it requires at least 10 times less memory and floating point operations, while achieving higher scores in BLEU-4, METEOR, CIDEr, and ROGUE."
}