{
    "title": "Mixed Preference Optimization: Reinforcement Learning with Data Selection and Better Reference Model",
    "abstract": "arXiv:2403.19443v1 Announce Type: new  Abstract: Large Language Models (LLMs) have become increasingly popular due to their ability to process and generate natural language. However, as they are trained on massive datasets of text, LLMs can inherit harmful biases and produce outputs that are not aligned with human values. This paper studies two main approaches to LLM alignment: Reinforcement Learning with Human Feedback (RLHF) and contrastive learning-based methods like Direct Preference Optimization (DPO). By analyzing the stability and robustness of RLHF and DPO, we propose MPO (Mixed Preference Optimization), a novel method that mitigates the weaknesses of both approaches. Specifically, we propose a two-stage training procedure: first train DPO on an easy dataset, and then perform RLHF on a difficult set with DPO model being the reference model. Here, the easy and difficult sets are constructed by a well-trained reward model that splits response pairs into those with large gaps of r",
    "link": "https://arxiv.org/abs/2403.19443",
    "context": "Title: Mixed Preference Optimization: Reinforcement Learning with Data Selection and Better Reference Model\nAbstract: arXiv:2403.19443v1 Announce Type: new  Abstract: Large Language Models (LLMs) have become increasingly popular due to their ability to process and generate natural language. However, as they are trained on massive datasets of text, LLMs can inherit harmful biases and produce outputs that are not aligned with human values. This paper studies two main approaches to LLM alignment: Reinforcement Learning with Human Feedback (RLHF) and contrastive learning-based methods like Direct Preference Optimization (DPO). By analyzing the stability and robustness of RLHF and DPO, we propose MPO (Mixed Preference Optimization), a novel method that mitigates the weaknesses of both approaches. Specifically, we propose a two-stage training procedure: first train DPO on an easy dataset, and then perform RLHF on a difficult set with DPO model being the reference model. Here, the easy and difficult sets are constructed by a well-trained reward model that splits response pairs into those with large gaps of r",
    "path": "papers/24/03/2403.19443.json",
    "total_tokens": 931,
    "translated_title": "混合偏好优化：强化学习中的数据选择与更好的参考模型",
    "translated_abstract": "大型语言模型（LLMs）因其处理和生成自然语言的能力而日益受到青睐。然而，由于它们是在大规模文本数据集上训练的，LLMs可能会继承有害偏见，并产生与人类价值观不一致的输出。本文研究了LLM对齐的两种主要方法：带人类反馈的强化学习（RLHF）和基于对比学习的方法如直接偏好优化（DPO）。通过分析RLHF和DPO的稳定性和鲁棒性，我们提出了MPO（混合偏好优化），这是一种缓解两种方法弱点的新方法。具体而言，我们提出了一个两阶段训练过程：首先在一个简单数据集上训练DPO，然后再在带有DPO模型作为参考模型的困难集上执行RLHF。在这里，简单和困难集是由训练良好的奖励模型构建的，将响应对分成具有较大差距的对。",
    "tldr": "提出了一种混合偏好优化（MPO）方法，通过在简单数据集上训练Direct Preference Optimization（DPO），然后在困难数据集上执行Reinforcement Learning with Human Feedback（RLHF），从而减轻了两种方法的弱点。"
}