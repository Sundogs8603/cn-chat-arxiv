{
    "title": "A Safe Screening Rule with Bi-level Optimization of $\\nu$ Support Vector Machine",
    "abstract": "arXiv:2403.01769v1 Announce Type: cross  Abstract: Support vector machine (SVM) has achieved many successes in machine learning, especially for a small sample problem. As a famous extension of the traditional SVM, the $\\nu$ support vector machine ($\\nu$-SVM) has shown outstanding performance due to its great model interpretability. However, it still faces challenges in training overhead for large-scale problems. To address this issue, we propose a safe screening rule with bi-level optimization for $\\nu$-SVM (SRBO-$\\nu$-SVM) which can screen out inactive samples before training and reduce the computational cost without sacrificing the prediction accuracy. Our SRBO-$\\nu$-SVM is strictly deduced by integrating the Karush-Kuhn-Tucker (KKT) conditions, the variational inequalities of convex problems and the $\\nu$-property. Furthermore, we develop an efficient dual coordinate descent method (DCDM) to further improve computational speed. Finally, a unified framework for SRBO is proposed to ac",
    "link": "https://arxiv.org/abs/2403.01769",
    "context": "Title: A Safe Screening Rule with Bi-level Optimization of $\\nu$ Support Vector Machine\nAbstract: arXiv:2403.01769v1 Announce Type: cross  Abstract: Support vector machine (SVM) has achieved many successes in machine learning, especially for a small sample problem. As a famous extension of the traditional SVM, the $\\nu$ support vector machine ($\\nu$-SVM) has shown outstanding performance due to its great model interpretability. However, it still faces challenges in training overhead for large-scale problems. To address this issue, we propose a safe screening rule with bi-level optimization for $\\nu$-SVM (SRBO-$\\nu$-SVM) which can screen out inactive samples before training and reduce the computational cost without sacrificing the prediction accuracy. Our SRBO-$\\nu$-SVM is strictly deduced by integrating the Karush-Kuhn-Tucker (KKT) conditions, the variational inequalities of convex problems and the $\\nu$-property. Furthermore, we develop an efficient dual coordinate descent method (DCDM) to further improve computational speed. Finally, a unified framework for SRBO is proposed to ac",
    "path": "papers/24/03/2403.01769.json",
    "total_tokens": 905,
    "translated_title": "具有双层优化的$\\nu$支持向量机安全筛选规则",
    "translated_abstract": "支持向量机（SVM）在机器学习中取得了许多成功，尤其是在小样本问题上。作为传统SVM的一个著名扩展，$\\nu$支持向量机（$\\nu$-SVM）由于其出色的模型可解释性而表现卓越。然而，对于大规模问题，它仍面临训练开销的挑战。为了解决这个问题，我们提出了一种具有双层优化的$\\nu$-SVM安全筛选规则（SRBO-$\\nu$-SVM），可以在训练之前筛选出不活跃的样本，并降低计算成本，而不会牺牲预测准确性。我们的SRBO-$\\nu$-SVM严格地通过整合KKT条件、凸问题的变分不等式和$\\nu$属性推导而来。此外，我们开发了一种高效的对偶坐标下降方法（DCDM）来进一步提高计算速度。最后，提出了一个用于SRBO的统一框架。",
    "tldr": "提出了一种具有双层优化的安全筛选规则的$\\nu$支持向量机方法，可以在训练前筛选出不活跃样本，降低计算成本，同时不损失预测准确性。",
    "en_tdlr": "Proposed a safe screening rule with bi-level optimization for $\\nu$ Support Vector Machine, which can screen out inactive samples before training, reduce computational cost, and maintain prediction accuracy."
}