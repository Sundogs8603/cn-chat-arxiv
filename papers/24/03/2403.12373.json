{
    "title": "RankPrompt: Step-by-Step Comparisons Make Language Models Better Reasoners",
    "abstract": "arXiv:2403.12373v1 Announce Type: new  Abstract: Large Language Models (LLMs) have achieved impressive performance across various reasoning tasks. However, even state-of-the-art LLMs such as ChatGPT are prone to logical errors during their reasoning processes. Existing solutions, which include deploying task-specific verifiers or voting over multiple reasoning paths, either require extensive human annotations or fail in scenarios with inconsistent responses. To address these challenges, we introduce RankPrompt, a new prompting method that enables LLMs to self-rank their responses without additional resources. RankPrompt breaks down the ranking problem into a series of comparisons among diverse responses, leveraging the inherent capabilities of LLMs to generate chains of comparison as contextual exemplars. Our experiments across 11 arithmetic and commonsense reasoning tasks show that RankPrompt significantly enhances the reasoning performance of ChatGPT and GPT-4, with improvements of u",
    "link": "https://arxiv.org/abs/2403.12373",
    "context": "Title: RankPrompt: Step-by-Step Comparisons Make Language Models Better Reasoners\nAbstract: arXiv:2403.12373v1 Announce Type: new  Abstract: Large Language Models (LLMs) have achieved impressive performance across various reasoning tasks. However, even state-of-the-art LLMs such as ChatGPT are prone to logical errors during their reasoning processes. Existing solutions, which include deploying task-specific verifiers or voting over multiple reasoning paths, either require extensive human annotations or fail in scenarios with inconsistent responses. To address these challenges, we introduce RankPrompt, a new prompting method that enables LLMs to self-rank their responses without additional resources. RankPrompt breaks down the ranking problem into a series of comparisons among diverse responses, leveraging the inherent capabilities of LLMs to generate chains of comparison as contextual exemplars. Our experiments across 11 arithmetic and commonsense reasoning tasks show that RankPrompt significantly enhances the reasoning performance of ChatGPT and GPT-4, with improvements of u",
    "path": "papers/24/03/2403.12373.json",
    "total_tokens": 790,
    "translated_title": "RankPrompt：逐步比较使语言模型成为更好的推理者",
    "translated_abstract": "大型语言模型（LLMs）在各种推理任务中取得了令人印象深刻的表现。然而，即使像ChatGPT这样的最先进的LLMs在推理过程中也容易出现逻辑错误。现有的解决方案，包括部署特定于任务的验证器或在多个推理路径上投票，要么需要大量人类注释，要么在存在不一致响应的场景中失败。为了解决这些挑战，我们介绍了RankPrompt，这是一种新的提示方法，使LLMs能够自行对其响应进行排序而无需额外资源。RankPrompt将排序问题分解为多个响应之间的一系列比较，利用LLMs自动生成比较链作为上下文示例的固有能力。我们在11个算术推理和常识推理任务上的实验表明，RankPrompt显著提高了ChatGPT和GPT-4的推理性能。",
    "tldr": "RankPrompt 提出了一种新的提示方法，可以通过自我排序来提高大型语言模型在推理任务中的性能。"
}