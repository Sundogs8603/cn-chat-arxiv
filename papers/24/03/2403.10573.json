{
    "title": "Medical Unlearnable Examples: Securing Medical Data from Unauthorized Traning via Sparsity-Aware Local Masking",
    "abstract": "arXiv:2403.10573v1 Announce Type: cross  Abstract: With the rapid growth of artificial intelligence (AI) in healthcare, there has been a significant increase in the generation and storage of sensitive medical data. This abundance of data, in turn, has propelled the advancement of medical AI technologies. However, concerns about unauthorized data exploitation, such as training commercial AI models, often deter researchers from making their invaluable datasets publicly available. In response to the need to protect this hard-to-collect data while still encouraging medical institutions to share it, one promising solution is to introduce imperceptible noise into the data. This method aims to safeguard the data against unauthorized training by inducing degradation in model generalization. Although existing methods have shown commendable data protection capabilities in general domains, they tend to fall short when applied to biomedical data, mainly due to their failure to account for the spar",
    "link": "https://arxiv.org/abs/2403.10573",
    "context": "Title: Medical Unlearnable Examples: Securing Medical Data from Unauthorized Traning via Sparsity-Aware Local Masking\nAbstract: arXiv:2403.10573v1 Announce Type: cross  Abstract: With the rapid growth of artificial intelligence (AI) in healthcare, there has been a significant increase in the generation and storage of sensitive medical data. This abundance of data, in turn, has propelled the advancement of medical AI technologies. However, concerns about unauthorized data exploitation, such as training commercial AI models, often deter researchers from making their invaluable datasets publicly available. In response to the need to protect this hard-to-collect data while still encouraging medical institutions to share it, one promising solution is to introduce imperceptible noise into the data. This method aims to safeguard the data against unauthorized training by inducing degradation in model generalization. Although existing methods have shown commendable data protection capabilities in general domains, they tend to fall short when applied to biomedical data, mainly due to their failure to account for the spar",
    "path": "papers/24/03/2403.10573.json",
    "total_tokens": 875,
    "translated_title": "医学不可学习的示例：通过稀疏感知本地蒙版保护医学数据免受未经授权训练",
    "translated_abstract": "随着人工智能在医疗保健领域的快速增长，敏感医学数据的生成和存储显著增加。这种数据的丰富量推动了医学人工智能技术的进步。然而，对于未经授权的数据利用，例如用于训练商业人工智能模型，常常使研究人员望而却步，因为他们不愿公开其宝贵的数据集。为了保护这些难以收集的数据，同时鼓励医疗机构分享数据，一个有前途的解决方案是向数据中引入难以察觉的噪声。这种方法旨在通过在模型泛化中引入退化来保护数据，防止未经授权的训练。尽管现有方法在一般领域显示出令人钦佩的数据保护能力，但当应用于生物医学数据时往往表现不佳，主要是因为它们未能考虑到稀疏性。",
    "tldr": "引入医学数据中的难以察觉噪声来保护数据，防止未经授权的训练，尤其适用于生物医学数据领域。",
    "en_tdlr": "Protect medical data from unauthorized training by introducing imperceptible noise, particularly effective in the field of biomedical data."
}