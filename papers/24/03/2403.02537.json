{
    "title": "Demolition and Reinforcement of Memories in Spin-Glass-like Neural Networks",
    "abstract": "arXiv:2403.02537v1 Announce Type: cross  Abstract: Statistical mechanics has made significant contributions to the study of biological neural systems by modeling them as recurrent networks of interconnected units with adjustable interactions. Several algorithms have been proposed to optimize the neural connections to enable network tasks such as information storage (i.e. associative memory) and learning probability distributions from data (i.e. generative modeling). Among these methods, the Unlearning algorithm, aligned with emerging theories of synaptic plasticity, was introduced by John Hopfield and collaborators. The primary objective of this thesis is to understand the effectiveness of Unlearning in both associative memory models and generative models. Initially, we demonstrate that the Unlearning algorithm can be simplified to a linear perceptron model which learns from noisy examples featuring specific internal correlations. The selection of structured training data enables an as",
    "link": "https://arxiv.org/abs/2403.02537",
    "context": "Title: Demolition and Reinforcement of Memories in Spin-Glass-like Neural Networks\nAbstract: arXiv:2403.02537v1 Announce Type: cross  Abstract: Statistical mechanics has made significant contributions to the study of biological neural systems by modeling them as recurrent networks of interconnected units with adjustable interactions. Several algorithms have been proposed to optimize the neural connections to enable network tasks such as information storage (i.e. associative memory) and learning probability distributions from data (i.e. generative modeling). Among these methods, the Unlearning algorithm, aligned with emerging theories of synaptic plasticity, was introduced by John Hopfield and collaborators. The primary objective of this thesis is to understand the effectiveness of Unlearning in both associative memory models and generative models. Initially, we demonstrate that the Unlearning algorithm can be simplified to a linear perceptron model which learns from noisy examples featuring specific internal correlations. The selection of structured training data enables an as",
    "path": "papers/24/03/2403.02537.json",
    "total_tokens": 845,
    "translated_title": "自旋玻璃样神经网络中记忆的拆除和强化",
    "translated_abstract": "统计力学通过将生物神经系统建模为具有可调交互作用的互联单元的循环网络，对研究生物神经系统做出了重要贡献。已经提出了几种算法来优化神经连接，以实现网络任务，如信息存储（即联想记忆）和从数据中学习概率分布（即生成建模）。在这些方法中，与突触可塑性新兴理论保持一致，约翰·霍普菲尔德和合作者引入了Unlearning算法。本文的主要目标是了解Unlearning在联想记忆模型和生成模型中的有效性。最初，我们展示了Unlearning算法可以简化为从具有特定内部相关性的噪声示例中学习的线性感知器模型。结构化训练数据的选择使得在得益于Unlearning的同时，网络也遭遇了较大的高误差容限",
    "tldr": "本论文研究了Unlearning算法在联想记忆模型和生成模型中的有效性，将其简化为线性感知器模型，并通过选择结构化训练数据，使网络在较高误差容限下依然有效。"
}