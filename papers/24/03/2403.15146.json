{
    "title": "On the Convergence of Adam under Non-uniform Smoothness: Separability from SGDM and Beyond",
    "abstract": "arXiv:2403.15146v1 Announce Type: new  Abstract: This paper aims to clearly distinguish between Stochastic Gradient Descent with Momentum (SGDM) and Adam in terms of their convergence rates. We demonstrate that Adam achieves a faster convergence compared to SGDM under the condition of non-uniformly bounded smoothness. Our findings reveal that: (1) in deterministic environments, Adam can attain the known lower bound for the convergence rate of deterministic first-order optimizers, whereas the convergence rate of Gradient Descent with Momentum (GDM) has higher order dependence on the initial function value; (2) in stochastic setting, Adam's convergence rate upper bound matches the lower bounds of stochastic first-order optimizers, considering both the initial function value and the final error, whereas there are instances where SGDM fails to converge with any learning rate. These insights distinctly differentiate Adam and SGDM regarding their convergence rates. Additionally, by introduci",
    "link": "https://arxiv.org/abs/2403.15146",
    "context": "Title: On the Convergence of Adam under Non-uniform Smoothness: Separability from SGDM and Beyond\nAbstract: arXiv:2403.15146v1 Announce Type: new  Abstract: This paper aims to clearly distinguish between Stochastic Gradient Descent with Momentum (SGDM) and Adam in terms of their convergence rates. We demonstrate that Adam achieves a faster convergence compared to SGDM under the condition of non-uniformly bounded smoothness. Our findings reveal that: (1) in deterministic environments, Adam can attain the known lower bound for the convergence rate of deterministic first-order optimizers, whereas the convergence rate of Gradient Descent with Momentum (GDM) has higher order dependence on the initial function value; (2) in stochastic setting, Adam's convergence rate upper bound matches the lower bounds of stochastic first-order optimizers, considering both the initial function value and the final error, whereas there are instances where SGDM fails to converge with any learning rate. These insights distinctly differentiate Adam and SGDM regarding their convergence rates. Additionally, by introduci",
    "path": "papers/24/03/2403.15146.json",
    "total_tokens": 820,
    "translated_title": "Adam在非均匀光滑情况下的收敛性：与SGDM的可分性及其拓展",
    "translated_abstract": "本文旨在明确区分动量随机梯度下降（SGDM）和Adam在收敛速率方面的差异。我们证明了在非均匀有界光滑度条件下，Adam相对于SGDM实现了更快的收敛。我们的研究发现：（1）在确定性环境中，Adam可以达到确定性一阶优化器收敛速率的已知下界，而动量梯度下降（GDM）的收敛速率对初始函数值具有更高阶的依赖性；（2）在随机设置下，Adam的收敛速率上界匹配了随机一阶优化器的下界，考虑到初始函数值和最终误差，而SGDM存在学习率下失败收敛的情况。这些发现清晰地区分了Adam和SGDM在收敛速率方面的差异。",
    "tldr": "Adam在非均匀光滑情况下与SGDM的收敛速率有明显区别，Adam更快地达到收敛，并具有更低的下界。",
    "en_tdlr": "Distinction is made between Adam and SGDM in terms of convergence rates under conditions of non-uniform smoothness, with Adam showing faster convergence and achieving lower bounds."
}