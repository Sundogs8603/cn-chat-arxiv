{
    "title": "DreamReward: Text-to-3D Generation with Human Preference",
    "abstract": "arXiv:2403.14613v1 Announce Type: cross  Abstract: 3D content creation from text prompts has shown remarkable success recently. However, current text-to-3D methods often generate 3D results that do not align well with human preferences. In this paper, we present a comprehensive framework, coined DreamReward, to learn and improve text-to-3D models from human preference feedback. To begin with, we collect 25k expert comparisons based on a systematic annotation pipeline including rating and ranking. Then, we build Reward3D -- the first general-purpose text-to-3D human preference reward model to effectively encode human preferences. Building upon the 3D reward model, we finally perform theoretical analysis and present the Reward3D Feedback Learning (DreamFL), a direct tuning algorithm to optimize the multi-view diffusion models with a redefined scorer. Grounded by theoretical proof and extensive experiment comparisons, our DreamReward successfully generates high-fidelity and 3D consistent ",
    "link": "https://arxiv.org/abs/2403.14613",
    "context": "Title: DreamReward: Text-to-3D Generation with Human Preference\nAbstract: arXiv:2403.14613v1 Announce Type: cross  Abstract: 3D content creation from text prompts has shown remarkable success recently. However, current text-to-3D methods often generate 3D results that do not align well with human preferences. In this paper, we present a comprehensive framework, coined DreamReward, to learn and improve text-to-3D models from human preference feedback. To begin with, we collect 25k expert comparisons based on a systematic annotation pipeline including rating and ranking. Then, we build Reward3D -- the first general-purpose text-to-3D human preference reward model to effectively encode human preferences. Building upon the 3D reward model, we finally perform theoretical analysis and present the Reward3D Feedback Learning (DreamFL), a direct tuning algorithm to optimize the multi-view diffusion models with a redefined scorer. Grounded by theoretical proof and extensive experiment comparisons, our DreamReward successfully generates high-fidelity and 3D consistent ",
    "path": "papers/24/03/2403.14613.json",
    "total_tokens": 886,
    "translated_title": "DreamReward: 文本到3D生成与人类偏好",
    "translated_abstract": "最近，从文本提示生成3D内容取得了显著成功。然而，当前的文本到3D方法通常生成的3D结果与人类偏好不一致。本文提出了一个全面的框架，称为DreamReward，从人类偏好反馈中学习和改进文本到3D模型。首先，我们收集了基于系统注释流水线，包括打分和排名的25k专家比较。然后，我们构建了Reward3D—第一个通用的文本到3D人类偏好奖励模型，以有效地编码人类偏好。基于3D奖励模型，最终进行理论分析并提出了Reward3D反馈学习（DreamFL），一个直接调优算法，用重新定义的评分者优化多视图扩散模型。通过理论证明和大量实验比较，我们的DreamReward成功地生成了高保真度和一致的3D结果。",
    "tldr": "DreamReward提出了一个名为DreamFL的框架，通过学习和改进文本到3D模型，基于人类偏好反馈创建了第一个通用的文本到3D人类偏好奖励模型，并引入了Reward3D反馈学习算法，成功生成高保真度和一致的3D结果。",
    "en_tdlr": "DreamReward introduced a framework called DreamFL aiming to learn and improve text-to-3D models based on human preference feedback, created the first general-purpose text-to-3D human preference reward model, and presented the Reward3D Feedback Learning algorithm, achieving high-fidelity and consistent 3D results."
}