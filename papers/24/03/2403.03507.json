{
    "title": "GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection",
    "abstract": "arXiv:2403.03507v1 Announce Type: new  Abstract: Training Large Language Models (LLMs) presents significant memory challenges, predominantly due to the growing size of weights and optimizer states. Common memory-reduction approaches, such as low-rank adaptation (LoRA), add a trainable low-rank matrix to the frozen pre-trained weight in each layer, reducing trainable parameters and optimizer states. However, such approaches typically underperform training with full-rank weights in both pre-training and fine-tuning stages since they limit the parameter search to a low-rank subspace and alter the training dynamics, and further, may require full-rank warm start. In this work, we propose Gradient Low-Rank Projection (GaLore), a training strategy that allows full-parameter learning but is more memory-efficient than common low-rank adaptation methods such as LoRA. Our approach reduces memory usage by up to 65.5% in optimizer states while maintaining both efficiency and performance for pre-tra",
    "link": "https://arxiv.org/abs/2403.03507",
    "context": "Title: GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection\nAbstract: arXiv:2403.03507v1 Announce Type: new  Abstract: Training Large Language Models (LLMs) presents significant memory challenges, predominantly due to the growing size of weights and optimizer states. Common memory-reduction approaches, such as low-rank adaptation (LoRA), add a trainable low-rank matrix to the frozen pre-trained weight in each layer, reducing trainable parameters and optimizer states. However, such approaches typically underperform training with full-rank weights in both pre-training and fine-tuning stages since they limit the parameter search to a low-rank subspace and alter the training dynamics, and further, may require full-rank warm start. In this work, we propose Gradient Low-Rank Projection (GaLore), a training strategy that allows full-parameter learning but is more memory-efficient than common low-rank adaptation methods such as LoRA. Our approach reduces memory usage by up to 65.5% in optimizer states while maintaining both efficiency and performance for pre-tra",
    "path": "papers/24/03/2403.03507.json",
    "total_tokens": 914,
    "translated_title": "GaLore: 通过梯度低秩投影实现高效的LLM训练",
    "translated_abstract": "训练大型语言模型(LLMs)存在显着的内存挑战，主要是由于权重和优化器状态的不断增加。通常的内存减少方法，如低秩适应（LoRA），在每个层中向冻结的预训练权重添加可训练的低秩矩阵，从而减少了可训练的参数和优化器状态。然而，这样的方法通常在预训练和微调阶段的表现都不如完整秩权重的训练，因为它们将参数搜索限制在低秩子空间并改变了训练动态，而且可能需要完整秩的热启动。在这项工作中，我们提出了Gradient Low-Rank Projection (GaLore)，这是一种训练策略，允许完全参数学习，但比LoRA等常见低秩适应方法更节省内存。我们的方法在优化器状态上将内存使用降低了高达65.5%，同时保持了预训练和精调的效率和性能。",
    "tldr": "GaLore提出了一种名为Gradient Low-Rank Projection (GaLore)的训练策略，相比于一般低秩适应方法，它能够实现更高效的LLM训练，大幅降低内存使用同时保持性能。"
}