{
    "title": "Prior-dependent analysis of posterior sampling reinforcement learning with function approximation",
    "abstract": "arXiv:2403.11175v1 Announce Type: cross  Abstract: This work advances randomized exploration in reinforcement learning (RL) with function approximation modeled by linear mixture MDPs. We establish the first prior-dependent Bayesian regret bound for RL with function approximation; and refine the Bayesian regret analysis for posterior sampling reinforcement learning (PSRL), presenting an upper bound of ${\\mathcal{O}}(d\\sqrt{H^3 T \\log T})$, where $d$ represents the dimensionality of the transition kernel, $H$ the planning horizon, and $T$ the total number of interactions. This signifies a methodological enhancement by optimizing the $\\mathcal{O}(\\sqrt{\\log T})$ factor over the previous benchmark (Osband and Van Roy, 2014) specified to linear mixture MDPs. Our approach, leveraging a value-targeted model learning perspective, introduces a decoupling argument and a variance reduction technique, moving beyond traditional analyses reliant on confidence sets and concentration inequalities to f",
    "link": "https://arxiv.org/abs/2403.11175",
    "context": "Title: Prior-dependent analysis of posterior sampling reinforcement learning with function approximation\nAbstract: arXiv:2403.11175v1 Announce Type: cross  Abstract: This work advances randomized exploration in reinforcement learning (RL) with function approximation modeled by linear mixture MDPs. We establish the first prior-dependent Bayesian regret bound for RL with function approximation; and refine the Bayesian regret analysis for posterior sampling reinforcement learning (PSRL), presenting an upper bound of ${\\mathcal{O}}(d\\sqrt{H^3 T \\log T})$, where $d$ represents the dimensionality of the transition kernel, $H$ the planning horizon, and $T$ the total number of interactions. This signifies a methodological enhancement by optimizing the $\\mathcal{O}(\\sqrt{\\log T})$ factor over the previous benchmark (Osband and Van Roy, 2014) specified to linear mixture MDPs. Our approach, leveraging a value-targeted model learning perspective, introduces a decoupling argument and a variance reduction technique, moving beyond traditional analyses reliant on confidence sets and concentration inequalities to f",
    "path": "papers/24/03/2403.11175.json",
    "total_tokens": 909,
    "translated_title": "先验依赖性分析基于函数逼近的后验抽样强化学习",
    "translated_abstract": "这项研究在对线性混合MDPs建模的函数逼近强化学习（RL）中推进了随机探索。我们为具有函数逼近的RL建立了首个先验依赖性贝叶斯遗憾上界；并且改进了用于后验抽样强化学习（PSRL）的贝叶斯遗憾分析，提出了一个上界为${\\mathcal{O}}(d\\sqrt{H^3 T \\log T})$的结果，其中$d$表示转移核的维度，$H$表示规划视野，$T$表示总交互次数。 这表示通过优化$\\mathcal{O}(\\sqrt{\\log T})$因子，我们在之前针对线性混合MDPs的基准（Osband和Van Roy，2014）上取得了方法论上的提升。我们的方法，利用价值定向模型学习的视角，引入了一种解耦论证和方差缩减技术，超越了传统分析依赖于置信区间和集中不等式的限制。",
    "tldr": "该研究提出了首个先验依赖性贝叶斯遗憾上界，并对后验抽样强化学习进行了改进分析，提出了一个新的上界结果，实现了对先前基准的方法论提升。",
    "en_tdlr": "This work introduces the first prior-dependent Bayesian regret bound for reinforcement learning with function approximation and refines the analysis for posterior sampling reinforcement learning, achieving a methodological enhancement over previous benchmarks."
}