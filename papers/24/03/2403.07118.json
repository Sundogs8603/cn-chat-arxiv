{
    "title": "Narrating Causal Graphs with Large Language Models",
    "abstract": "arXiv:2403.07118v1 Announce Type: new  Abstract: The use of generative AI to create text descriptions from graphs has mostly focused on knowledge graphs, which connect concepts using facts. In this work we explore the capability of large pretrained language models to generate text from causal graphs, where salient concepts are represented as nodes and causality is represented via directed, typed edges. The causal reasoning encoded in these graphs can support applications as diverse as healthcare or marketing. Using two publicly available causal graph datasets, we empirically investigate the performance of four GPT-3 models under various settings. Our results indicate that while causal text descriptions improve with training data, compared to fact-based graphs, they are harder to generate under zero-shot settings. Results further suggest that users of generative AI can deploy future applications faster since similar performances are obtained when training a model with only a few example",
    "link": "https://arxiv.org/abs/2403.07118",
    "context": "Title: Narrating Causal Graphs with Large Language Models\nAbstract: arXiv:2403.07118v1 Announce Type: new  Abstract: The use of generative AI to create text descriptions from graphs has mostly focused on knowledge graphs, which connect concepts using facts. In this work we explore the capability of large pretrained language models to generate text from causal graphs, where salient concepts are represented as nodes and causality is represented via directed, typed edges. The causal reasoning encoded in these graphs can support applications as diverse as healthcare or marketing. Using two publicly available causal graph datasets, we empirically investigate the performance of four GPT-3 models under various settings. Our results indicate that while causal text descriptions improve with training data, compared to fact-based graphs, they are harder to generate under zero-shot settings. Results further suggest that users of generative AI can deploy future applications faster since similar performances are obtained when training a model with only a few example",
    "path": "papers/24/03/2403.07118.json",
    "total_tokens": 867,
    "translated_title": "利用大型语言模型叙述因果图",
    "translated_abstract": "利用生成式人工智能从图形生成文本描述的应用主要集中在知识图谱上，其通过事实连接概念。本研究探讨了大型预训练语言模型生成从因果图中的文本的能力，其中显著概念表示为节点，因果关系通过有向、类型化边表示。这些图中编码的因果推理可以支持诸如医疗保健或营销等各种应用。我们通过两个公开可用的因果图数据集，在各种设置下实证研究了四个GPT-3模型的性能。我们的结果表明，与基于事实的图形相比，虽然因果文本描述随着训练数据的增加而改善，但在零热身设置下更难生成。结果进一步表明，生成式AI的用户可以更快地部署未来应用程序，因为当仅使用少量示例训练模型时，可以获得类似的性能。",
    "tldr": "本研究探讨了大型预训练语言模型生成从因果图中的文本的能力，结果表明因果文本描述在训练数据增加时有所改善，但在零热身设置下更难生成，用户可以更快部署未来应用程序。",
    "en_tdlr": "This study explores the capability of large pretrained language models to generate text from causal graphs, indicating improvements in causal text descriptions with increased training data but making it harder to generate under zero-shot settings, allowing users to deploy future applications faster."
}