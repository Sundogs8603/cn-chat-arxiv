{
    "title": "Enhancing Privacy in Federated Learning through Local Training",
    "abstract": "arXiv:2403.17572v1 Announce Type: new  Abstract: In this paper we propose the federated private local training algorithm (Fed-PLT) for federated learning, to overcome the challenges of (i) expensive communications and (ii) privacy preservation. We address (i) by allowing for both partial participation and local training, which significantly reduce the number of communication rounds between the central coordinator and computing agents. The algorithm matches the state of the art in the sense that the use of local training demonstrably does not impact accuracy. Additionally, agents have the flexibility to choose from various local training solvers, such as (stochastic) gradient descent and accelerated gradient descent. Further, we investigate how employing local training can enhance privacy, addressing point (ii). In particular, we derive differential privacy bounds and highlight their dependence on the number of local training epochs. We assess the effectiveness of the proposed algorithm",
    "link": "https://arxiv.org/abs/2403.17572",
    "context": "Title: Enhancing Privacy in Federated Learning through Local Training\nAbstract: arXiv:2403.17572v1 Announce Type: new  Abstract: In this paper we propose the federated private local training algorithm (Fed-PLT) for federated learning, to overcome the challenges of (i) expensive communications and (ii) privacy preservation. We address (i) by allowing for both partial participation and local training, which significantly reduce the number of communication rounds between the central coordinator and computing agents. The algorithm matches the state of the art in the sense that the use of local training demonstrably does not impact accuracy. Additionally, agents have the flexibility to choose from various local training solvers, such as (stochastic) gradient descent and accelerated gradient descent. Further, we investigate how employing local training can enhance privacy, addressing point (ii). In particular, we derive differential privacy bounds and highlight their dependence on the number of local training epochs. We assess the effectiveness of the proposed algorithm",
    "path": "papers/24/03/2403.17572.json",
    "total_tokens": 895,
    "translated_title": "通过本地训练增强联邦学习的隐私性",
    "translated_abstract": "在本文中，我们提出了一种用于联邦学习的联邦私有本地训练算法（Fed-PLT），以克服（i）昂贵的通信和（ii）隐私保护的挑战。我们通过允许部分参与和本地训练来解决（i），这显著减少了中央协调员和计算代理之间的通信轮次。算法在本地训练的使用上达到了目前技术水平，可以证明不会影响准确性。此外，代理可以灵活选择各种本地训练求解器，如（随机）梯度下降和加速梯度下降。此外，我们研究了如何通过使用本地训练来增强隐私性，解决了点（ii）。具体而言，我们推导出差分隐私界限，并强调它们对本地训练纪元数的依赖性。我们评估了所提出算法的有效性。",
    "tldr": "提出了一种用于联邦学习的联邦私有本地训练算法（Fed-PLT），通过允许部分参与和本地训练，显著减少了通信轮次，同时不影响准确性，并研究了如何通过本地训练来增强隐私性。",
    "en_tdlr": "Proposed a federated private local training algorithm (Fed-PLT) for federated learning, reducing communication rounds significantly without affecting accuracy by allowing partial participation and local training, and investigated how to enhance privacy through local training."
}