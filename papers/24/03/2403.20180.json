{
    "title": "Measuring Taiwanese Mandarin Language Understanding",
    "abstract": "arXiv:2403.20180v1 Announce Type: new  Abstract: The evaluation of large language models (LLMs) has drawn substantial attention in the field recently. This work focuses on evaluating LLMs in a Chinese context, specifically, for Traditional Chinese which has been largely underrepresented in existing benchmarks. We present TMLU, a holistic evaluation suit tailored for assessing the advanced knowledge and reasoning capability in LLMs, under the context of Taiwanese Mandarin. TMLU consists of an array of 37 subjects across social science, STEM, humanities, Taiwan-specific content, and others, ranging from middle school to professional levels. In addition, we curate chain-of-thought-like few-shot explanations for each subject to facilitate the evaluation of complex reasoning skills. To establish a comprehensive baseline, we conduct extensive experiments and analysis on 24 advanced LLMs. The results suggest that Chinese open-weight models demonstrate inferior performance comparing to multili",
    "link": "https://arxiv.org/abs/2403.20180",
    "context": "Title: Measuring Taiwanese Mandarin Language Understanding\nAbstract: arXiv:2403.20180v1 Announce Type: new  Abstract: The evaluation of large language models (LLMs) has drawn substantial attention in the field recently. This work focuses on evaluating LLMs in a Chinese context, specifically, for Traditional Chinese which has been largely underrepresented in existing benchmarks. We present TMLU, a holistic evaluation suit tailored for assessing the advanced knowledge and reasoning capability in LLMs, under the context of Taiwanese Mandarin. TMLU consists of an array of 37 subjects across social science, STEM, humanities, Taiwan-specific content, and others, ranging from middle school to professional levels. In addition, we curate chain-of-thought-like few-shot explanations for each subject to facilitate the evaluation of complex reasoning skills. To establish a comprehensive baseline, we conduct extensive experiments and analysis on 24 advanced LLMs. The results suggest that Chinese open-weight models demonstrate inferior performance comparing to multili",
    "path": "papers/24/03/2403.20180.json",
    "total_tokens": 944,
    "translated_title": "测量台湾普通话理解能力",
    "translated_abstract": "最近，对大型语言模型（LLMs）的评估引起了领域内的重视。本研究致力于在汉语环境下评估LLMs，具体而言，针对传统中文，在现有基准中一直存在着较大的欠表征。我们提出了TMLU，这是一个为评估LLMs的高级知识和推理能力量身定制的综合评估套件，适用于台湾普通话环境。TMLU由37个科目组成，涵盖社会科学、STEM、人文学科、台湾特定内容等，涉及初中到专业水平。此外，我们为每个科目精心策划了一些类似于思维链的少样本解释，以促进复杂推理能力的评估。为建立全面的基准线，我们对24个高级LLMs进行了广泛的实验和分析。研究结果表明，相较于多项式开重量模型，中国开放权重模型表现出较低的性能。",
    "tldr": "该研究致力于在评估汉语环境中的大型语言模型，提出了一个综合评估套件TMLU，覆盖37个科目，通过精心策划的少样本解释促进复杂推理能力的评估，并对24个高级LLMs进行了广泛实验和分析，结果显示中国开放权重模型表现较差。",
    "en_tdlr": "This study focuses on evaluating large language models in a Chinese context, presents a comprehensive evaluation suite TMLU covering 37 subjects, facilitates the assessment of complex reasoning skills through carefully crafted few-shot explanations for each subject, conducts extensive experiments and analysis on 24 advanced LLMs, and finds that Chinese open-weight models demonstrate inferior performance."
}