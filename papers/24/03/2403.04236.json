{
    "title": "Regularized DeepIV with Model Selection",
    "abstract": "arXiv:2403.04236v1 Announce Type: new  Abstract: In this paper, we study nonparametric estimation of instrumental variable (IV) regressions. While recent advancements in machine learning have introduced flexible methods for IV estimation, they often encounter one or more of the following limitations: (1) restricting the IV regression to be uniquely identified; (2) requiring minimax computation oracle, which is highly unstable in practice; (3) absence of model selection procedure. In this paper, we present the first method and analysis that can avoid all three limitations, while still enabling general function approximation. Specifically, we propose a minimax-oracle-free method called Regularized DeepIV (RDIV) regression that can converge to the least-norm IV solution. Our method consists of two stages: first, we learn the conditional distribution of covariates, and by utilizing the learned distribution, we learn the estimator by minimizing a Tikhonov-regularized loss function. We furth",
    "link": "https://arxiv.org/abs/2403.04236",
    "context": "Title: Regularized DeepIV with Model Selection\nAbstract: arXiv:2403.04236v1 Announce Type: new  Abstract: In this paper, we study nonparametric estimation of instrumental variable (IV) regressions. While recent advancements in machine learning have introduced flexible methods for IV estimation, they often encounter one or more of the following limitations: (1) restricting the IV regression to be uniquely identified; (2) requiring minimax computation oracle, which is highly unstable in practice; (3) absence of model selection procedure. In this paper, we present the first method and analysis that can avoid all three limitations, while still enabling general function approximation. Specifically, we propose a minimax-oracle-free method called Regularized DeepIV (RDIV) regression that can converge to the least-norm IV solution. Our method consists of two stages: first, we learn the conditional distribution of covariates, and by utilizing the learned distribution, we learn the estimator by minimizing a Tikhonov-regularized loss function. We furth",
    "path": "papers/24/03/2403.04236.json",
    "total_tokens": 884,
    "translated_title": "具有模型选择的正则化DeepIV",
    "translated_abstract": "在这篇论文中，我们研究了工具变量（IV）回归的非参数估计。虽然机器学习的最新进展已经引入了灵活的IV估计方法，但它们往往会遇到以下一个或多个限制：（1）将IV回归限制为唯一标识；（2）需要极小极大计算预言，这在实践中非常不稳定；（3）缺乏模型选择过程。在本文中，我们提出了一种可以避免所有三个限制的第一种方法和分析，同时仍然能够实现通用函数逼近。具体而言，我们提出了一种名为正则化DeepIV（RDIV）回归的无极小极大计算预言的方法，可以收敛到最小范数IV解。我们的方法分为两个阶段：首先，我们学习协变量的条件分布，并通过利用所学到的分布，通过最小化Tikhonov正则化损失函数来学习估计值。我们进一步...",
    "tldr": "本文提出了一种名为正则化DeepIV（RDIV）回归的新方法，能够避免IV回归唯一标识、极小极大计算预言和缺乏模型选择过程等限制，同时实现了通用函数逼近。",
    "en_tdlr": "This paper introduces a new method called Regularized DeepIV (RDIV) regression, which avoids limitations such as uniquely identified IV regression, minimax computation oracle requirement, and absence of model selection procedure, while enabling general function approximation."
}