{
    "title": "State-Separated SARSA: A Practical Sequential Decision-Making Algorithm with Recovering Rewards",
    "abstract": "arXiv:2403.11520v1 Announce Type: new  Abstract: While many multi-armed bandit algorithms assume that rewards for all arms are constant across rounds, this assumption does not hold in many real-world scenarios. This paper considers the setting of recovering bandits (Pike-Burke & Grunewalder, 2019), where the reward depends on the number of rounds elapsed since the last time an arm was pulled. We propose a new reinforcement learning (RL) algorithm tailored to this setting, named the State-Separate SARSA (SS-SARSA) algorithm, which treats rounds as states. The SS-SARSA algorithm achieves efficient learning by reducing the number of state combinations required for Q-learning/SARSA, which often suffers from combinatorial issues for large-scale RL problems. Additionally, it makes minimal assumptions about the reward structure and offers lower computational complexity. Furthermore, we prove asymptotic convergence to an optimal policy under mild assumptions. Simulation studies demonstrate the",
    "link": "https://arxiv.org/abs/2403.11520",
    "context": "Title: State-Separated SARSA: A Practical Sequential Decision-Making Algorithm with Recovering Rewards\nAbstract: arXiv:2403.11520v1 Announce Type: new  Abstract: While many multi-armed bandit algorithms assume that rewards for all arms are constant across rounds, this assumption does not hold in many real-world scenarios. This paper considers the setting of recovering bandits (Pike-Burke & Grunewalder, 2019), where the reward depends on the number of rounds elapsed since the last time an arm was pulled. We propose a new reinforcement learning (RL) algorithm tailored to this setting, named the State-Separate SARSA (SS-SARSA) algorithm, which treats rounds as states. The SS-SARSA algorithm achieves efficient learning by reducing the number of state combinations required for Q-learning/SARSA, which often suffers from combinatorial issues for large-scale RL problems. Additionally, it makes minimal assumptions about the reward structure and offers lower computational complexity. Furthermore, we prove asymptotic convergence to an optimal policy under mild assumptions. Simulation studies demonstrate the",
    "path": "papers/24/03/2403.11520.json",
    "total_tokens": 910,
    "translated_title": "分离状态SARSA: 一种具有恢复奖励的实用序贯决策算法",
    "translated_abstract": "虽然许多多臂老虎机算法假设所有臂的奖励在各轮之间保持不变，但在许多现实场景中，这种假设并不成立。本文考虑了恢复老虎机的设置，其中奖励取决于自上次拉动臂以来经过的轮数。我们提出了一种新的适用于这种情况的强化学习（RL）算法，名为分离状态SARSA（SS-SARSA）算法，其中将各轮视为状态。 SS-SARSA算法通过减少Q-learning/SARSA所需的状态组合数量来实现高效学习，这在大规模RL问题中经常遇到组合问题。此外，它对奖励结构进行最少假设并提供较低的计算复杂度。此外，我们证明在温和假设下渐近收敛至最优策略。模拟研究证明",
    "tldr": "提出了分离状态SARSA（SS-SARSA）算法，针对恢复老虎机场景设计，通过将轮数视为状态，降低Q-learning/SARSA所需的状态组合数量，实现有效学习，并在温和假设下证明了渐近收敛至最优策略。",
    "en_tdlr": "Introduced the State-Separate SARSA (SS-SARSA) algorithm tailored to recovering bandit scenario, reducing the number of state combinations required for Q-learning/SARSA, achieving efficient learning, and proving asymptotic convergence to an optimal policy under mild assumptions."
}