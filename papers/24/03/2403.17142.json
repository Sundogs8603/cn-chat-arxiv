{
    "title": "Approximation with Random Shallow ReLU Networks with Applications to Model Reference Adaptive Control",
    "abstract": "arXiv:2403.17142v1 Announce Type: cross  Abstract: Neural networks are regularly employed in adaptive control of nonlinear systems and related methods o reinforcement learning. A common architecture uses a neural network with a single hidden layer (i.e. a shallow network), in which the weights and biases are fixed in advance and only the output layer is trained. While classical results show that there exist neural networks of this type that can approximate arbitrary continuous functions over bounded regions, they are non-constructive, and the networks used in practice have no approximation guarantees. Thus, the approximation properties required for control with neural networks are assumed, rather than proved. In this paper, we aim to fill this gap by showing that for sufficiently smooth functions, ReLU networks with randomly generated weights and biases achieve $L_{\\infty}$ error of $O(m^{-1/2})$ with high probability, where $m$ is the number of neurons. It suffices to generate the wei",
    "link": "https://arxiv.org/abs/2403.17142",
    "context": "Title: Approximation with Random Shallow ReLU Networks with Applications to Model Reference Adaptive Control\nAbstract: arXiv:2403.17142v1 Announce Type: cross  Abstract: Neural networks are regularly employed in adaptive control of nonlinear systems and related methods o reinforcement learning. A common architecture uses a neural network with a single hidden layer (i.e. a shallow network), in which the weights and biases are fixed in advance and only the output layer is trained. While classical results show that there exist neural networks of this type that can approximate arbitrary continuous functions over bounded regions, they are non-constructive, and the networks used in practice have no approximation guarantees. Thus, the approximation properties required for control with neural networks are assumed, rather than proved. In this paper, we aim to fill this gap by showing that for sufficiently smooth functions, ReLU networks with randomly generated weights and biases achieve $L_{\\infty}$ error of $O(m^{-1/2})$ with high probability, where $m$ is the number of neurons. It suffices to generate the wei",
    "path": "papers/24/03/2403.17142.json",
    "total_tokens": 847,
    "translated_title": "利用随机浅层ReLU网络来进行逼近及其在模型参考自适应控制中的应用",
    "translated_abstract": "神经网络常用于非线性系统的自适应控制以及相关的强化学习方法。一种常见的结构是使用具有单个隐藏层的神经网络（即浅层网络），其中权重和偏置提前固定，只有输出层被训练。尽管经典结果表明，存在这种类型的神经网络可以逼近有界区域上的任意连续函数，但这些结果是非构造性的，实际使用的网络没有逼近保证。因此，用于神经网络控制的逼近性质是假设的，而不是被证明的。本文旨在通过展示对于足够光滑的函数，具有随机生成的权重和偏置的ReLU网络可以在高概率下实现$O(m^{-1/2})$的$L_{\\infty}$误差，其中$m$是神经元的数量。",
    "tldr": "针对足够光滑的函数，本文证明使用随机生成的权重和偏置的ReLU网络可以在高概率下实现$O(m^{-1/2})$的$L_{\\infty}$误差。",
    "en_tdlr": "The paper demonstrates that for sufficiently smooth functions, ReLU networks with randomly generated weights and biases can achieve $L_{\\infty}$ error of $O(m^{-1/2})$ with high probability."
}