{
    "title": "Beyond the Labels: Unveiling Text-Dependency in Paralinguistic Speech Recognition Datasets",
    "abstract": "arXiv:2403.07767v1 Announce Type: cross  Abstract: Paralinguistic traits like cognitive load and emotion are increasingly recognized as pivotal areas in speech recognition research, often examined through specialized datasets like CLSE and IEMOCAP. However, the integrity of these datasets is seldom scrutinized for text-dependency. This paper critically evaluates the prevalent assumption that machine learning models trained on such datasets genuinely learn to identify paralinguistic traits, rather than merely capturing lexical features. By examining the lexical overlap in these datasets and testing the performance of machine learning models, we expose significant text-dependency in trait-labeling. Our results suggest that some machine learning models, especially large pre-trained models like HuBERT, might inadvertently focus on lexical characteristics rather than the intended paralinguistic features. The study serves as a call to action for the research community to reevaluate the relia",
    "link": "https://arxiv.org/abs/2403.07767",
    "context": "Title: Beyond the Labels: Unveiling Text-Dependency in Paralinguistic Speech Recognition Datasets\nAbstract: arXiv:2403.07767v1 Announce Type: cross  Abstract: Paralinguistic traits like cognitive load and emotion are increasingly recognized as pivotal areas in speech recognition research, often examined through specialized datasets like CLSE and IEMOCAP. However, the integrity of these datasets is seldom scrutinized for text-dependency. This paper critically evaluates the prevalent assumption that machine learning models trained on such datasets genuinely learn to identify paralinguistic traits, rather than merely capturing lexical features. By examining the lexical overlap in these datasets and testing the performance of machine learning models, we expose significant text-dependency in trait-labeling. Our results suggest that some machine learning models, especially large pre-trained models like HuBERT, might inadvertently focus on lexical characteristics rather than the intended paralinguistic features. The study serves as a call to action for the research community to reevaluate the relia",
    "path": "papers/24/03/2403.07767.json",
    "total_tokens": 889,
    "translated_title": "超越标签：揭示语音识别数据集中的文本依赖性",
    "translated_abstract": "类似认知负荷和情绪等语音交际特征越来越被认可为语音识别研究中的关键领域，通常通过专门的数据集（如CLSE和IEMOCAP）进行研究。然而，很少有人审查这些数据集是否存在文本依赖性。本文批判性地评估了机器学习模型在这些数据集上训练时真正学会识别语音交际特征，而不仅仅是捕捉词汇特征的普遍假设。通过检查这些数据集中的词汇重叠并测试机器学习模型的性能，我们揭示了特征标签中的显著文本依赖性。我们的结果表明，一些机器学习模型，特别是像HuBERT这样的大型预训练模型，可能无意中专注于词汇特征，而不是预期的语音交际特征。本研究号召研究界重新评估数据集的可靠性。",
    "tldr": "本研究批判性评估了语音识别数据集中的文本依赖性，揭示了一些机器学习模型可能会过于关注词汇特征而非预期的语音交际特征。",
    "en_tdlr": "This study critically evaluates the text-dependency in speech recognition datasets, revealing that some machine learning models may focus too much on lexical features rather than the intended paralinguistic traits."
}