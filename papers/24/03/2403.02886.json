{
    "title": "Revisiting Confidence Estimation: Towards Reliable Failure Prediction",
    "abstract": "arXiv:2403.02886v1 Announce Type: cross  Abstract: Reliable confidence estimation is a challenging yet fundamental requirement in many risk-sensitive applications. However, modern deep neural networks are often overconfident for their incorrect predictions, i.e., misclassified samples from known classes, and out-of-distribution (OOD) samples from unknown classes. In recent years, many confidence calibration and OOD detection methods have been developed. In this paper, we find a general, widely existing but actually-neglected phenomenon that most confidence estimation methods are harmful for detecting misclassification errors. We investigate this problem and reveal that popular calibration and OOD detection methods often lead to worse confidence separation between correctly classified and misclassified examples, making it difficult to decide whether to trust a prediction or not. Finally, we propose to enlarge the confidence gap by finding flat minima, which yields state-of-the-art failu",
    "link": "https://arxiv.org/abs/2403.02886",
    "context": "Title: Revisiting Confidence Estimation: Towards Reliable Failure Prediction\nAbstract: arXiv:2403.02886v1 Announce Type: cross  Abstract: Reliable confidence estimation is a challenging yet fundamental requirement in many risk-sensitive applications. However, modern deep neural networks are often overconfident for their incorrect predictions, i.e., misclassified samples from known classes, and out-of-distribution (OOD) samples from unknown classes. In recent years, many confidence calibration and OOD detection methods have been developed. In this paper, we find a general, widely existing but actually-neglected phenomenon that most confidence estimation methods are harmful for detecting misclassification errors. We investigate this problem and reveal that popular calibration and OOD detection methods often lead to worse confidence separation between correctly classified and misclassified examples, making it difficult to decide whether to trust a prediction or not. Finally, we propose to enlarge the confidence gap by finding flat minima, which yields state-of-the-art failu",
    "path": "papers/24/03/2403.02886.json",
    "total_tokens": 852,
    "translated_title": "重新审视置信度估计：朝向可靠的故障预测",
    "translated_abstract": "可靠的置信度估计在许多风险敏感应用中是一个具有挑战性但基本的要求。然而，现代深度神经网络往往对于它们的错误预测过于自信，即，来自已知类别的被错误分类的样本和来自未知类别的超出分布（OOD）样本。近年来，许多置信度校准和OOD检测方法已经被开发出来。在本文中，我们发现了一个普遍存在但实际上被忽视的现象，即大多数置信度估计方法对于检测错误分类错误是有害的。我们调查了这个问题，并揭示了流行的校准和OOD检测方法通常导致更糟糕的置信度分离正确分类和错误分类的事例，从而使得难以决定是否信任一个预测。最后，我们提出通过寻找平坦的最小值来扩大置信度间隔，从而获得最先进的失败预测。",
    "tldr": "大多数置信度估计方法对于检测错误分类错误是有害的，我们提出通过寻找平坦的最小值来扩大置信度间隔，从而取得了最先进的失败预测。",
    "en_tdlr": "Most confidence estimation methods are harmful for detecting misclassification errors; we propose to enlarge the confidence gap by finding flat minima, resulting in state-of-the-art failure prediction."
}