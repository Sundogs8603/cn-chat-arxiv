{
    "title": "Learning with SASQuaTCh: a Novel Variational Quantum Transformer Architecture with Kernel-Based Self-Attention",
    "abstract": "arXiv:2403.14753v1 Announce Type: cross  Abstract: The widely popular transformer network popularized by the generative pre-trained transformer (GPT) has a large field of applicability, including predicting text and images, classification, and even predicting solutions to the dynamics of physical systems. In the latter context, the continuous analog of the self-attention mechanism at the heart of transformer networks has been applied to learning the solutions of partial differential equations and reveals a convolution kernel nature that can be exploited by the Fourier transform. It is well known that many quantum algorithms that have provably demonstrated a speedup over classical algorithms utilize the quantum Fourier transform. In this work, we explore quantum circuits that can efficiently express a self-attention mechanism through the perspective of kernel-based operator learning. In this perspective, we are able to represent deep layers of a vision transformer network using simple g",
    "link": "https://arxiv.org/abs/2403.14753",
    "context": "Title: Learning with SASQuaTCh: a Novel Variational Quantum Transformer Architecture with Kernel-Based Self-Attention\nAbstract: arXiv:2403.14753v1 Announce Type: cross  Abstract: The widely popular transformer network popularized by the generative pre-trained transformer (GPT) has a large field of applicability, including predicting text and images, classification, and even predicting solutions to the dynamics of physical systems. In the latter context, the continuous analog of the self-attention mechanism at the heart of transformer networks has been applied to learning the solutions of partial differential equations and reveals a convolution kernel nature that can be exploited by the Fourier transform. It is well known that many quantum algorithms that have provably demonstrated a speedup over classical algorithms utilize the quantum Fourier transform. In this work, we explore quantum circuits that can efficiently express a self-attention mechanism through the perspective of kernel-based operator learning. In this perspective, we are able to represent deep layers of a vision transformer network using simple g",
    "path": "papers/24/03/2403.14753.json",
    "total_tokens": 719,
    "translated_title": "与SASQuaTCh学习：基于核自注意力的新型变分量子变压器架构",
    "translated_abstract": "由生成式预训练变压器（GPT）普及的广泛流行的变压器网络在许多领域都有广泛的应用，包括预测文本和图像、分类，甚至预测物理系统动力学的解。本工作中，我们探讨了能够通过基于核的运算符学习视角高效表达自我注意机制的量子电路。在这个视角下，我们能够使用简单的卷积核来表示视觉变压器网络的深层。",
    "tldr": "本研究提出了一种通过核自注意力来学习的新型变分量子变压器架构，可以用简单的卷积核表示深层的视觉变压器网络。"
}