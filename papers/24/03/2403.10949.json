{
    "title": "SelfIE: Self-Interpretation of Large Language Model Embeddings",
    "abstract": "arXiv:2403.10949v1 Announce Type: cross  Abstract: How do large language models (LLMs) obtain their answers? The ability to explain and control an LLM's reasoning process is key for reliability, transparency, and future model developments. We propose SelfIE (Self-Interpretation of Embeddings), a framework that enables LLMs to interpret their own embeddings in natural language by leveraging their ability to respond inquiry about a given passage. Capable of interpreting open-world concepts in the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such as making ethical decisions, internalizing prompt injection, and recalling harmful knowledge. SelfIE's text descriptions on hidden embeddings also open up new avenues to control LLM reasoning. We propose Supervised Control, which allows editing open-ended concepts while only requiring gradient computation of individual layer. We extend RLHF to hidden embeddings and propose Reinforcement Control that erases harmful knowledge i",
    "link": "https://arxiv.org/abs/2403.10949",
    "context": "Title: SelfIE: Self-Interpretation of Large Language Model Embeddings\nAbstract: arXiv:2403.10949v1 Announce Type: cross  Abstract: How do large language models (LLMs) obtain their answers? The ability to explain and control an LLM's reasoning process is key for reliability, transparency, and future model developments. We propose SelfIE (Self-Interpretation of Embeddings), a framework that enables LLMs to interpret their own embeddings in natural language by leveraging their ability to respond inquiry about a given passage. Capable of interpreting open-world concepts in the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such as making ethical decisions, internalizing prompt injection, and recalling harmful knowledge. SelfIE's text descriptions on hidden embeddings also open up new avenues to control LLM reasoning. We propose Supervised Control, which allows editing open-ended concepts while only requiring gradient computation of individual layer. We extend RLHF to hidden embeddings and propose Reinforcement Control that erases harmful knowledge i",
    "path": "papers/24/03/2403.10949.json",
    "total_tokens": 865,
    "translated_title": "SelfIE：大型语言模型嵌入的自我解释",
    "translated_abstract": "arXiv:2403.10949v1 公告类型：交叉摘要：大型语言模型（LLMs）如何获得答案？解释和控制LLM的推理过程对于可靠性、透明度和未来模型发展至关重要。我们提出了SelfIE（嵌入的自我解释），这是一个框架，能够利用LLMs响应关于给定段落的查询的能力，以自然语言解释它们自己的嵌入。SelfIE能够解释隐藏嵌入中的开放世界概念，在案例中揭示LLM的内部推理，如做出道德决策、内化提示注入和回想有害知识。SelfIE对隐藏嵌入的文本描述也开辟了控制LLM推理的新途径。我们提出了监督控制，它允许编辑开放式概念，而只需要计算单个层的梯度。我们将RLHF扩展到隐藏的嵌入，并提出了强化控制来消除有害知识。",
    "tldr": "提出了SelfIE框架，使大型语言模型能够自解释其嵌入，揭示内部推理，包括道德决策、提示注入和消除有害知识。"
}