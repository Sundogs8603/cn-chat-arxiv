{
    "title": "HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild",
    "abstract": "arXiv:2403.04307v1 Announce Type: new  Abstract: Hallucinations pose a significant challenge to the reliability of large language models (LLMs) in critical domains. Recent benchmarks designed to assess LLM hallucinations within conventional NLP tasks, such as knowledge-intensive question answering (QA) and summarization, are insufficient for capturing the complexities of user-LLM interactions in dynamic, real-world settings. To address this gap, we introduce HaluEval-Wild, the first benchmark specifically designed to evaluate LLM hallucinations in the wild. We meticulously collect challenging (adversarially filtered by Alpaca) user queries from existing real-world user-LLM interaction datasets, including ShareGPT, to evaluate the hallucination rates of various LLMs. Upon analyzing the collected queries, we categorize them into five distinct types, which enables a fine-grained analysis of the types of hallucinations LLMs exhibit, and synthesize the reference answers with the powerful GP",
    "link": "https://arxiv.org/abs/2403.04307",
    "context": "Title: HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild\nAbstract: arXiv:2403.04307v1 Announce Type: new  Abstract: Hallucinations pose a significant challenge to the reliability of large language models (LLMs) in critical domains. Recent benchmarks designed to assess LLM hallucinations within conventional NLP tasks, such as knowledge-intensive question answering (QA) and summarization, are insufficient for capturing the complexities of user-LLM interactions in dynamic, real-world settings. To address this gap, we introduce HaluEval-Wild, the first benchmark specifically designed to evaluate LLM hallucinations in the wild. We meticulously collect challenging (adversarially filtered by Alpaca) user queries from existing real-world user-LLM interaction datasets, including ShareGPT, to evaluate the hallucination rates of various LLMs. Upon analyzing the collected queries, we categorize them into five distinct types, which enables a fine-grained analysis of the types of hallucinations LLMs exhibit, and synthesize the reference answers with the powerful GP",
    "path": "papers/24/03/2403.04307.json",
    "total_tokens": 911,
    "translated_title": "HaluEval-Wild：在实际环境中评估语言模型的幻觉",
    "translated_abstract": "幻觉对于关键领域中大型语言模型（LLMs）的可靠性构成了重大挑战。最近设计用于评估LLM在传统NLP任务中的幻觉的基准测试，如知识密集型问答（QA）和摘要，不足以捕捉动态实际环境中用户-LLM交互的复杂性。为了弥补这一空白，我们介绍了HaluEval-Wild，这是第一个专门设计用于评估实际环境中LLM幻觉的基准测试。我们精心收集了来自现有实际用户-LLM交互数据集（包括ShareGPT）中具有挑战性的（经Alpaca对抗性过滤的）用户查询，以评估各种LLM的幻觉率。在分析收集到的查询后，我们将其分类为五种不同类型，这使得可以对LLM表现出的幻觉类型进行细粒度分析，并将引用答案与强大的GP合成。",
    "tldr": "HaluEval-Wild是第一个专门设计用于评估实际环境中LLM幻觉的基准测试，收集了具有挑战性的用户查询并分类为五种不同类型，可以对LLM表现出的幻觉类型进行细粒度分析。",
    "en_tdlr": "HaluEval-Wild is the first benchmark specifically designed to evaluate LLM hallucinations in the wild, collecting challenging user queries and categorizing them into five distinct types for fine-grained analysis of the types of hallucinations exhibited by LLMs."
}