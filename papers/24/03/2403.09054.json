{
    "title": "Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference",
    "abstract": "arXiv:2403.09054v1 Announce Type: cross  Abstract: Transformers have emerged as the underpinning architecture for Large Language Models (LLMs). In generative language models, the inference process involves two primary phases: prompt processing and token generation. Token generation, which constitutes the majority of the computational workload, primarily entails vector-matrix multiplications and interactions with the Key-Value (KV) Cache. This phase is constrained by memory bandwidth due to the overhead of transferring weights and KV cache values from the memory system to the computing units. This memory bottleneck becomes particularly pronounced in applications that require long-context and extensive text generation, both of which are increasingly crucial for LLMs.   This paper introduces \"Keyformer\", an innovative inference-time approach, to mitigate the challenges associated with KV cache size and memory bandwidth utilization. Keyformer leverages the observation that approximately 90",
    "link": "https://arxiv.org/abs/2403.09054",
    "context": "Title: Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference\nAbstract: arXiv:2403.09054v1 Announce Type: cross  Abstract: Transformers have emerged as the underpinning architecture for Large Language Models (LLMs). In generative language models, the inference process involves two primary phases: prompt processing and token generation. Token generation, which constitutes the majority of the computational workload, primarily entails vector-matrix multiplications and interactions with the Key-Value (KV) Cache. This phase is constrained by memory bandwidth due to the overhead of transferring weights and KV cache values from the memory system to the computing units. This memory bottleneck becomes particularly pronounced in applications that require long-context and extensive text generation, both of which are increasingly crucial for LLMs.   This paper introduces \"Keyformer\", an innovative inference-time approach, to mitigate the challenges associated with KV cache size and memory bandwidth utilization. Keyformer leverages the observation that approximately 90",
    "path": "papers/24/03/2403.09054.json",
    "total_tokens": 798,
    "translated_title": "Keyformer：通过关键标记选择减少KV缓存以实现高效的生成推断",
    "translated_abstract": "Transformer已经成为大型语言模型(LLMs)的基础架构。在生成语言模型中，推断过程涉及两个主要阶段：提示处理和标记生成。标记生成，构成了大部分计算工作量，主要涉及向量-矩阵乘法和与键-值(KV)缓存交互。由于从存储系统传输权重和KV缓存值到计算单元的开销，这一阶段受到内存带宽的限制。这种内存瓶颈在需要长上下文和大量文本生成的应用中尤为突出，这两者对LLMs越来越重要。  本文介绍了一种创新的推断时间方法“Keyformer”，以缓解与KV缓存大小和内存带宽利用相关的挑战。Keyformer利用了这样的观察结果，大约90",
    "tldr": "本文提出了一种名为“Keyformer”的创新推断时间方法，旨在通过选择关键标记来减少KV缓存的挑战，提高内存带宽利用率。",
    "en_tdlr": "This paper introduces an innovative inference-time approach called \"Keyformer\" aiming to mitigate the challenges associated with KV cache size and memory bandwidth utilization by selecting key tokens."
}