{
    "title": "Achieving Pareto Optimality using Efficient Parameter Reduction for DNNs in Resource-Constrained Edge Environment",
    "abstract": "arXiv:2403.10569v1 Announce Type: cross  Abstract: This paper proposes an optimization of an existing Deep Neural Network (DNN) that improves its hardware utilization and facilitates on-device training for resource-constrained edge environments. We implement efficient parameter reduction strategies on Xception that shrink the model size without sacrificing accuracy, thus decreasing memory utilization during training. We evaluate our model in two experiments: Caltech-101 image classification and PCB defect detection and compare its performance against the original Xception and lightweight models, EfficientNetV2B1 and MobileNetV2. The results of the Caltech-101 image classification show that our model has a better test accuracy (76.21%) than Xception (75.89%), uses less memory on average (847.9MB) than Xception (874.6MB), and has faster training and inference times. The lightweight models overfit with EfficientNetV2B1 having a 30.52% test accuracy and MobileNetV2 having a 58.11% test acc",
    "link": "https://arxiv.org/abs/2403.10569",
    "context": "Title: Achieving Pareto Optimality using Efficient Parameter Reduction for DNNs in Resource-Constrained Edge Environment\nAbstract: arXiv:2403.10569v1 Announce Type: cross  Abstract: This paper proposes an optimization of an existing Deep Neural Network (DNN) that improves its hardware utilization and facilitates on-device training for resource-constrained edge environments. We implement efficient parameter reduction strategies on Xception that shrink the model size without sacrificing accuracy, thus decreasing memory utilization during training. We evaluate our model in two experiments: Caltech-101 image classification and PCB defect detection and compare its performance against the original Xception and lightweight models, EfficientNetV2B1 and MobileNetV2. The results of the Caltech-101 image classification show that our model has a better test accuracy (76.21%) than Xception (75.89%), uses less memory on average (847.9MB) than Xception (874.6MB), and has faster training and inference times. The lightweight models overfit with EfficientNetV2B1 having a 30.52% test accuracy and MobileNetV2 having a 58.11% test acc",
    "path": "papers/24/03/2403.10569.json",
    "total_tokens": 977,
    "translated_title": "在资源受限的边缘环境中利用高效参数缩减实现DNN的帕累托最优性",
    "translated_abstract": "本文提出了对现有深度神经网络（DNN）进行优化，改善其硬件利用率，并为资源受限的边缘环境提供便于设备上训练的条件。我们在Xception上实施了高效的参数缩减策略，缩小模型大小而不损失准确性，从而减少训练过程中的内存利用。我们在两个实验中评估了我们的模型：Caltech-101图像分类和PCB缺陷检测，并将其性能与原始的Xception和轻量级模型EfficientNetV2B1和MobileNetV2进行了比较。Caltech-101图像分类的结果显示，我们的模型具有更好的测试准确度（76.21%），比Xception（75.89%）平均使用更少的内存（847.9MB比Xception的874.6MB），并且训练和推理时间更快。轻量级模型存在过拟合问题，EfficientNetV2B1的测试准确度为30.52%，MobileNetV2的测试准确度为58.11%。",
    "tldr": "通过在Xception上实施高效的参数缩减策略，该研究在资源受限的边缘环境中实现了DNN的帕累托最优性，提高了模型的准确性，减少了内存利用，且在Caltech-101图像分类中表现优于原始Xception和轻量级模型。",
    "en_tdlr": "By implementing efficient parameter reduction strategies on Xception, this research achieves Pareto optimality for DNNs in resource-constrained edge environments, improving model accuracy, reducing memory utilization, and outperforming the original Xception and lightweight models in Caltech-101 image classification."
}