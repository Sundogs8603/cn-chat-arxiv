{
    "title": "Level Set Teleportation: An Optimization Perspective",
    "abstract": "arXiv:2403.03362v1 Announce Type: new  Abstract: We study level set teleportation, an optimization sub-routine which seeks to accelerate gradient methods by maximizing the gradient norm on a level-set of the objective function. Since the descent lemma implies that gradient descent (GD) decreases the objective proportional to the squared norm of the gradient, level-set teleportation maximizes this one-step progress guarantee. For convex functions satisfying Hessian stability, we prove that GD with level-set teleportation obtains a combined sub-linear/linear convergence rate which is strictly faster than standard GD when the optimality gap is small. This is in sharp contrast to the standard (strongly) convex setting, where we show level-set teleportation neither improves nor worsens convergence rates. To evaluate teleportation in practice, we develop a projected-gradient-type method requiring only Hessian-vector products. We use this method to show that gradient methods with access to a ",
    "link": "https://arxiv.org/abs/2403.03362",
    "context": "Title: Level Set Teleportation: An Optimization Perspective\nAbstract: arXiv:2403.03362v1 Announce Type: new  Abstract: We study level set teleportation, an optimization sub-routine which seeks to accelerate gradient methods by maximizing the gradient norm on a level-set of the objective function. Since the descent lemma implies that gradient descent (GD) decreases the objective proportional to the squared norm of the gradient, level-set teleportation maximizes this one-step progress guarantee. For convex functions satisfying Hessian stability, we prove that GD with level-set teleportation obtains a combined sub-linear/linear convergence rate which is strictly faster than standard GD when the optimality gap is small. This is in sharp contrast to the standard (strongly) convex setting, where we show level-set teleportation neither improves nor worsens convergence rates. To evaluate teleportation in practice, we develop a projected-gradient-type method requiring only Hessian-vector products. We use this method to show that gradient methods with access to a ",
    "path": "papers/24/03/2403.03362.json",
    "total_tokens": 886,
    "translated_title": "水平集传输：优化的视角",
    "translated_abstract": "我们研究水平集传输，这是一种优化子程序，旨在通过在目标函数的水平集上最大化梯度范数来加速梯度方法。由于下降引理暗示梯度下降（GD）使目标函数按梯度的平方范数下降，水平集传输最大化了这一步进保证。对于满足Hessian稳定性的凸函数，我们证明了GD与水平集传输获得了综合的次线性/线性收敛速度，这比在最优性差距较小时标准GD要快得多。与标准（强）凸设置形成鲜明对比的是，我们展示了水平集传输既不改善也不恶化收敛速度。为了实际评估传输，我们开发了一种只需要Hessian-向量乘积的投影梯度类型方法。我们使用这种方法显示，如果提供了梯度访问权限，水平集传输梯度方法具有更快的收敛速度。",
    "tldr": "该论文从优化的角度研究了水平集传输，证明了在某些条件下水平集传输可以加速梯度方法的收敛速度，且提出了一种只需要Hessian-vector products的方法验证了该技术在实践中的有效性。",
    "en_tdlr": "This paper examines level set teleportation from an optimization perspective, showing under certain conditions it can accelerate the convergence rate of gradient methods. Additionally, a method that only requires Hessian-vector products is proposed to evaluate the effectiveness of teleportation in practice."
}