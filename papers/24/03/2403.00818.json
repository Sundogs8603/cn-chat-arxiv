{
    "title": "DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models",
    "abstract": "arXiv:2403.00818v1 Announce Type: new  Abstract: Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% ac",
    "link": "https://arxiv.org/abs/2403.00818",
    "context": "Title: DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models\nAbstract: arXiv:2403.00818v1 Announce Type: new  Abstract: Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% ac",
    "path": "papers/24/03/2403.00818.json",
    "total_tokens": 874,
    "translated_title": "DenseMamba: 具有密集隐藏连接的状态空间模型，用于高效大型语言模型",
    "translated_abstract": "大型语言模型(LLMs)面临着由普遍使用的Transformer架构过高的计算和内存需求而带来的巨大挑战。而状态空间模型(SSM)是一种新型基础网络架构，具有较低的计算复杂度，但其性能尚未完全能与Transformer相媲美。本文引入了DenseSSM，一种增强SSMs中各层之间隐藏信息流动的新方法。通过有选择地将浅层隐藏状态集成到更深层，DenseSSM保留了对最终输出至关重要的细粒度信息。密集连接增强的DenseSSM仍保持了训练的并行性和推理效率。该方法可以广泛适用于RetNet和Mamba等各种SSM类型。在相似的模型大小下，DenseSSM取得了显著的改进，例如DenseRetNet比原始RetNet提高了高达5%的准确率。",
    "tldr": "DenseSSM是一种新方法，通过密集连接增强了状态空间模型(SSM)，有效地提升了各层之间隐藏信息的流动，在保持训练并行性和推理效率的同时，取得了显著的性能提升。",
    "en_tdlr": "DenseSSM is a novel approach that enhances state space models (SSMs) with dense connections, effectively improving the flow of hidden information between layers, achieving significant performance gains while maintaining training parallelizability and inference efficiency."
}