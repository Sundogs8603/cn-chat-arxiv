{
    "title": "Aligning Large Language Models for Controllable Recommendations",
    "abstract": "arXiv:2403.05063v1 Announce Type: cross  Abstract: Inspired by the exceptional general intelligence of Large Language Models (LLMs), researchers have begun to explore their application in pioneering the next generation of recommender systems - systems that are conversational, explainable, and controllable. However, existing literature primarily concentrates on integrating domain-specific knowledge into LLMs to enhance accuracy, often neglecting the ability to follow instructions. To address this gap, we initially introduce a collection of supervised learning tasks, augmented with labels derived from a conventional recommender model, aimed at explicitly improving LLMs' proficiency in adhering to recommendation-specific instructions. Subsequently, we develop a reinforcement learning-based alignment procedure to further strengthen LLMs' aptitude in responding to users' intentions and mitigating formatting errors. Through extensive experiments on two real-world datasets, our method markedl",
    "link": "https://arxiv.org/abs/2403.05063",
    "context": "Title: Aligning Large Language Models for Controllable Recommendations\nAbstract: arXiv:2403.05063v1 Announce Type: cross  Abstract: Inspired by the exceptional general intelligence of Large Language Models (LLMs), researchers have begun to explore their application in pioneering the next generation of recommender systems - systems that are conversational, explainable, and controllable. However, existing literature primarily concentrates on integrating domain-specific knowledge into LLMs to enhance accuracy, often neglecting the ability to follow instructions. To address this gap, we initially introduce a collection of supervised learning tasks, augmented with labels derived from a conventional recommender model, aimed at explicitly improving LLMs' proficiency in adhering to recommendation-specific instructions. Subsequently, we develop a reinforcement learning-based alignment procedure to further strengthen LLMs' aptitude in responding to users' intentions and mitigating formatting errors. Through extensive experiments on two real-world datasets, our method markedl",
    "path": "papers/24/03/2403.05063.json",
    "total_tokens": 817,
    "translated_title": "调整大型语言模型以实现可控的推荐",
    "translated_abstract": "受到大型语言模型（LLMs）异常的智能启发，研究人员已开始探索将它们应用于开创下一代推荐系统 - 这些系统具有对话、可解释和可控的特性。然而，现有文献主要集中在将领域特定知识整合到LLMs中以提高准确性，通常忽略了遵循指令的能力。为填补这一空白，我们首先引入一组监督学习任务，标记来源于传统推荐模型的标签，旨在明确改善LLMs遵循特定推荐指令的熟练程度。随后，我们开发了一种基于强化学习的对齐程序，进一步加强了LLMs在响应用户意图和减少格式错误方面的能力。通过在两个真实世界数据集上进行广泛实验，我们的方法标记着",
    "tldr": "通过引入监督学习任务和强化学习对齐程序，研究人员提出了一种方法来改善大型语言模型适应推荐指令和减少格式错误的能力。",
    "en_tdlr": "Researchers propose a method to enhance the ability of Large Language Models to adhere to recommendation instructions and mitigate formatting errors by introducing supervised learning tasks and reinforcement learning alignment procedures."
}