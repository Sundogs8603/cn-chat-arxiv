{
    "title": "Optimal convex $M$-estimation via score matching",
    "abstract": "arXiv:2403.16688v1 Announce Type: cross  Abstract: In the context of linear regression, we construct a data-driven convex loss function with respect to which empirical risk minimisation yields optimal asymptotic variance in the downstream estimation of the regression coefficients. Our semiparametric approach targets the best decreasing approximation of the derivative of the log-density of the noise distribution. At the population level, this fitting process is a nonparametric extension of score matching, corresponding to a log-concave projection of the noise distribution with respect to the Fisher divergence. The procedure is computationally efficient, and we prove that our procedure attains the minimal asymptotic covariance among all convex $M$-estimators. As an example of a non-log-concave setting, for Cauchy errors, the optimal convex loss function is Huber-like, and our procedure yields an asymptotic efficiency greater than 0.87 relative to the oracle maximum likelihood estimator o",
    "link": "https://arxiv.org/abs/2403.16688",
    "context": "Title: Optimal convex $M$-estimation via score matching\nAbstract: arXiv:2403.16688v1 Announce Type: cross  Abstract: In the context of linear regression, we construct a data-driven convex loss function with respect to which empirical risk minimisation yields optimal asymptotic variance in the downstream estimation of the regression coefficients. Our semiparametric approach targets the best decreasing approximation of the derivative of the log-density of the noise distribution. At the population level, this fitting process is a nonparametric extension of score matching, corresponding to a log-concave projection of the noise distribution with respect to the Fisher divergence. The procedure is computationally efficient, and we prove that our procedure attains the minimal asymptotic covariance among all convex $M$-estimators. As an example of a non-log-concave setting, for Cauchy errors, the optimal convex loss function is Huber-like, and our procedure yields an asymptotic efficiency greater than 0.87 relative to the oracle maximum likelihood estimator o",
    "path": "papers/24/03/2403.16688.json",
    "total_tokens": 871,
    "translated_title": "通过得分匹配实现最佳凸$M$-估计",
    "translated_abstract": "在线性回归的背景下，我们构建了一个数据驱动的凸损失函数，通过该函数进行经验风险最小化可以在回归系数的下游估计中实现最佳的渐近方差。我们的半参数方法旨在最佳逼近噪声分布对数密度的导数。在总体层面上，这个拟合过程是对得分匹配的非参数拓展，对应于根据Fisher散度进行噪声分布的对数凹映射。该过程在计算上是高效的，我们证明我们的程序达到了所有凸$M$-估计中最小的渐近协方差。作为非对数凹设置的一个例子，对于柯西误差，最佳凸损失函数类似于Huber函数，并且我们的过程相对于oracle最大似然估计器实现了大于0.87的渐近效率。",
    "tldr": "该论文提出了一种通过得分匹配实现最佳凸$M$-估计的方法，在线性回归中能够达到最佳的渐近方差，并且在计算上高效，证明具有所有凸$M$-估计中最小的渐近协方差。",
    "en_tdlr": "The paper introduces a method for optimal convex $M$-estimation via score matching in linear regression, achieving optimal asymptotic variance and minimal asymptotic covariance among all convex $M$-estimators, while being computationally efficient."
}