{
    "title": "Provable Policy Gradient Methods for Average-Reward Markov Potential Games",
    "abstract": "arXiv:2403.05738v1 Announce Type: new  Abstract: We study Markov potential games under the infinite horizon average reward criterion. Most previous studies have been for discounted rewards. We prove that both algorithms based on independent policy gradient and independent natural policy gradient converge globally to a Nash equilibrium for the average reward criterion. To set the stage for gradient-based methods, we first establish that the average reward is a smooth function of policies and provide sensitivity bounds for the differential value functions, under certain conditions on ergodicity and the second largest eigenvalue of the underlying Markov decision process (MDP). We prove that three algorithms, policy gradient, proximal-Q, and natural policy gradient (NPG), converge to an $\\epsilon$-Nash equilibrium with time complexity $O(\\frac{1}{\\epsilon^2})$, given a gradient/differential Q function oracle. When policy gradients have to be estimated, we propose an algorithm with $\\tilde{",
    "link": "https://arxiv.org/abs/2403.05738",
    "context": "Title: Provable Policy Gradient Methods for Average-Reward Markov Potential Games\nAbstract: arXiv:2403.05738v1 Announce Type: new  Abstract: We study Markov potential games under the infinite horizon average reward criterion. Most previous studies have been for discounted rewards. We prove that both algorithms based on independent policy gradient and independent natural policy gradient converge globally to a Nash equilibrium for the average reward criterion. To set the stage for gradient-based methods, we first establish that the average reward is a smooth function of policies and provide sensitivity bounds for the differential value functions, under certain conditions on ergodicity and the second largest eigenvalue of the underlying Markov decision process (MDP). We prove that three algorithms, policy gradient, proximal-Q, and natural policy gradient (NPG), converge to an $\\epsilon$-Nash equilibrium with time complexity $O(\\frac{1}{\\epsilon^2})$, given a gradient/differential Q function oracle. When policy gradients have to be estimated, we propose an algorithm with $\\tilde{",
    "path": "papers/24/03/2403.05738.json",
    "total_tokens": 912,
    "translated_title": "可证明的平均回报马尔可夫潜在博弈的策略梯度方法",
    "translated_abstract": "我们研究了在无限时间段平均回报准则下的马尔可夫潜在博弈。大多数先前的研究都是针对折扣回报的。我们证明基于独立策略梯度和独立自然策略梯度的算法均在平均回报准则下全局收敛到纳什均衡。为了为基于梯度的方法奠定基础，我们首先建立平均回报是策略的光滑函数，并在马尔可夫决策过程 (MDP) 的遍历性和次大特征值的一定条件下提供差分价值函数的敏感度界限。我们证明了三种算法，包括策略梯度、近端-Q 和自然策略梯度 (NPG)，在给定梯度/差分Q函数oracle的情况下，收敛到$\\epsilon$-Nash均衡的时间复杂度为$O(\\frac{1}{\\epsilon^2})$。当必须估计策略梯度时，我们提出了一个算法，具有$\\tilde{...}$",
    "tldr": "提出了针对平均回报准则下马尔可夫潜在博弈的可证明的策略梯度方法，并展示了算法的全局收敛性和时间复杂度。",
    "en_tdlr": "Introduces provable policy gradient methods for average-reward Markov potential games and demonstrates the global convergence and time complexity of the algorithms."
}