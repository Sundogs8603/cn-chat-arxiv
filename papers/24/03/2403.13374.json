{
    "title": "Byzantine-resilient Federated Learning With Adaptivity to Data Heterogeneity",
    "abstract": "arXiv:2403.13374v1 Announce Type: new  Abstract: This paper deals with federated learning (FL) in the presence of malicious Byzantine attacks and data heterogeneity. A novel Robust Average Gradient Algorithm (RAGA) is proposed, which leverages the geometric median for aggregation and can freely select the round number for local updating. Different from most existing resilient approaches, which perform convergence analysis based on strongly-convex loss function or homogeneously distributed dataset, we conduct convergence analysis for not only strongly-convex but also non-convex loss function over heterogeneous dataset. According to our theoretical analysis, as long as the fraction of dataset from malicious users is less than half, RAGA can achieve convergence at rate $\\mathcal{O}({1}/{T^{2/3- \\delta}})$ where $T$ is the iteration number and $\\delta \\in (0, 2/3)$ for non-convex loss function, and at linear rate for strongly-convex loss function. Moreover, stationary point or global optim",
    "link": "https://arxiv.org/abs/2403.13374",
    "context": "Title: Byzantine-resilient Federated Learning With Adaptivity to Data Heterogeneity\nAbstract: arXiv:2403.13374v1 Announce Type: new  Abstract: This paper deals with federated learning (FL) in the presence of malicious Byzantine attacks and data heterogeneity. A novel Robust Average Gradient Algorithm (RAGA) is proposed, which leverages the geometric median for aggregation and can freely select the round number for local updating. Different from most existing resilient approaches, which perform convergence analysis based on strongly-convex loss function or homogeneously distributed dataset, we conduct convergence analysis for not only strongly-convex but also non-convex loss function over heterogeneous dataset. According to our theoretical analysis, as long as the fraction of dataset from malicious users is less than half, RAGA can achieve convergence at rate $\\mathcal{O}({1}/{T^{2/3- \\delta}})$ where $T$ is the iteration number and $\\delta \\in (0, 2/3)$ for non-convex loss function, and at linear rate for strongly-convex loss function. Moreover, stationary point or global optim",
    "path": "papers/24/03/2403.13374.json",
    "total_tokens": 941,
    "translated_title": "具有对数据异构性的自适应的拜占庭弹性联邦学习",
    "translated_abstract": "本文处理了在存在恶意拜占庭攻击和数据异构性的情况下的联邦学习（FL）。提出了一种新颖的鲁棒平均梯度算法（RAGA），该算法利用几何中位数进行聚合，并可以自由选择本地更新的轮数。与大多数现有的弹性方法不同，这些方法基于强凸损失函数或均匀分布的数据集进行收敛分析，我们进行了对强凸和非凸损失函数在异构数据集上的收敛分析。根据我们的理论分析，只要恶意用户数据集的比例小于一半，RAGA就可以以$\\mathcal{O}({1}/{T^{2/3- \\delta}})$的速度实现非凸损失函数的收敛，其中$T$为迭代次数，$\\delta \\in (0, 2/3)$，对于强凸损失函数则呈线性收敛。此外，稳定点或全局最优解",
    "tldr": "通过提出新的Robust Average Gradient Algorithm（RAGA），本研究在联邦学习中解决了恶意拜占庭攻击和数据异构性的问题，实现了在非凸损失函数和异构数据集上的收敛性分析，并展示了RAGA的良好收敛性能。",
    "en_tdlr": "By proposing the novel Robust Average Gradient Algorithm (RAGA), this research addresses malicious Byzantine attacks and data heterogeneity in federated learning, achieving convergence analysis on non-convex loss functions and heterogeneous datasets while demonstrating robust convergence performance of RAGA."
}