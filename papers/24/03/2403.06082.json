{
    "title": "FrameQuant: Flexible Low-Bit Quantization for Transformers",
    "abstract": "arXiv:2403.06082v1 Announce Type: cross  Abstract: Transformers are the backbone of powerful foundation models for many Vision and Natural Language Processing tasks. But their compute and memory/storage footprint is large, and so, serving such models is expensive often requiring high-end hardware. To mitigate this difficulty, Post-Training Quantization seeks to modify a pre-trained model and quantize it to eight bits or lower, significantly boosting compute/memory/latency efficiency. Such models have been successfully quantized to four bits with some performance loss. In this work, we outline a simple scheme to quantize Transformer-based models to just two bits (plus some overhead) with only a small drop in accuracy. Key to our formulation is a concept borrowed from Harmonic analysis called Fusion Frames. Our main finding is that the quantization must take place not in the original weight space, but instead in the Fusion Frame representations. If quantization is interpreted as the addi",
    "link": "https://arxiv.org/abs/2403.06082",
    "context": "Title: FrameQuant: Flexible Low-Bit Quantization for Transformers\nAbstract: arXiv:2403.06082v1 Announce Type: cross  Abstract: Transformers are the backbone of powerful foundation models for many Vision and Natural Language Processing tasks. But their compute and memory/storage footprint is large, and so, serving such models is expensive often requiring high-end hardware. To mitigate this difficulty, Post-Training Quantization seeks to modify a pre-trained model and quantize it to eight bits or lower, significantly boosting compute/memory/latency efficiency. Such models have been successfully quantized to four bits with some performance loss. In this work, we outline a simple scheme to quantize Transformer-based models to just two bits (plus some overhead) with only a small drop in accuracy. Key to our formulation is a concept borrowed from Harmonic analysis called Fusion Frames. Our main finding is that the quantization must take place not in the original weight space, but instead in the Fusion Frame representations. If quantization is interpreted as the addi",
    "path": "papers/24/03/2403.06082.json",
    "total_tokens": 829,
    "translated_title": "FrameQuant: Transformer的灵活低比特量化方法",
    "translated_abstract": "Transformer是许多视觉和自然语言处理任务强大基础模型的支柱。然而，它们的计算和内存/存储空间占用较大，因此为这些模型提供服务往往需要昂贵的高端硬件。为了缓解这一困难，后训练量化试图修改预训练模型并将其量化为八位或更低的位数，显着提高计算/内存/延迟效率。既可以成功将这些模型量化为四位，但性能有所损失。在这项工作中，我们概述了一个简单的方案，将基于Transformer的模型量化为仅两位（加一些额外开销），仅会有轻微的精度下降。我们的制定关键在于从谐波分析中借鉴了一种称为融合框架的概念。我们的主要发现是，量化不应该在原始权重空间中进行，而是应该在融合框架表示中进行。",
    "tldr": "提出一种简单的方案，通过融合框架将Transformer模型量化为仅两位，仅有轻微精度下降。",
    "en_tdlr": "Propose a simple scheme to quantize Transformer-based models to just two bits using Fusion Frames, with only a small drop in accuracy."
}