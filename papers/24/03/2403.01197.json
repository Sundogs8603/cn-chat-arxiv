{
    "title": "DMoERM: Recipes of Mixture-of-Experts for Effective Reward Modeling",
    "abstract": "arXiv:2403.01197v1 Announce Type: new  Abstract: The performance of the reward model (RM) is a critical factor in improving the effectiveness of the large language model (LLM) during alignment fine-tuning. There remain two challenges in RM training: 1) training the same RM using various categories of data may cause its generalization performance to suffer from multi-task disturbance, and 2) the human annotation consistency rate is generally only $60\\%$ to $75\\%$, causing training data to contain a lot of noise. To tackle these two challenges, we introduced the idea of Mixture-of-Experts (MoE) into the field of RM for the first time. We propose the Double-Layer MoE RM (DMoERM). The outer layer MoE is a sparse model. After classifying an input into task categories, we route it to the corresponding inner layer task-specific model. The inner layer MoE is a dense model. We decompose the specific task into multiple capability dimensions and individually fine-tune a LoRA expert on each one. T",
    "link": "https://arxiv.org/abs/2403.01197",
    "context": "Title: DMoERM: Recipes of Mixture-of-Experts for Effective Reward Modeling\nAbstract: arXiv:2403.01197v1 Announce Type: new  Abstract: The performance of the reward model (RM) is a critical factor in improving the effectiveness of the large language model (LLM) during alignment fine-tuning. There remain two challenges in RM training: 1) training the same RM using various categories of data may cause its generalization performance to suffer from multi-task disturbance, and 2) the human annotation consistency rate is generally only $60\\%$ to $75\\%$, causing training data to contain a lot of noise. To tackle these two challenges, we introduced the idea of Mixture-of-Experts (MoE) into the field of RM for the first time. We propose the Double-Layer MoE RM (DMoERM). The outer layer MoE is a sparse model. After classifying an input into task categories, we route it to the corresponding inner layer task-specific model. The inner layer MoE is a dense model. We decompose the specific task into multiple capability dimensions and individually fine-tune a LoRA expert on each one. T",
    "path": "papers/24/03/2403.01197.json",
    "total_tokens": 951,
    "translated_title": "DMoERM: 有效奖励建模的混合专家配方",
    "translated_abstract": "奖励模型（RM）的性能是改善大型语言模型（LLM）在对齐微调过程中效果的关键因素。 RM训练仍然存在两个挑战：1）使用各种类别数据训练相同的RM可能导致其泛化性能受到多任务干扰的影响；2）人类注释的一致性率通常仅为60%至75%，导致训练数据包含大量噪声。 为了解决这两个挑战，我们首次将专家混合（MoE）的概念引入到RM领域。 我们提出了双层MoE RM（DMoERM）。 外层MoE是一种稀疏模型。 将输入分类成任务类别后，我们将其路由到相应的内层任务特定模型中。 内层MoE是一种密集模型。 我们将特定任务分解为多个能力维度，并分别在每个维度上对LoRA专家进行微调。",
    "tldr": "DMoERM首次将专家混合（MoE）的概念引入奖励建模领域，提出了双层MoE RM（DMoERM），通过稀疏的外层MoE和密集的内层MoE对特定任务进行微调，解决了训练中的多任务干扰和数据噪声问题。"
}