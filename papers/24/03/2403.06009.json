{
    "title": "Detectors for Safe and Reliable LLMs: Implementations, Uses, and Limitations",
    "abstract": "arXiv:2403.06009v1 Announce Type: new  Abstract: Large language models (LLMs) are susceptible to a variety of risks, from non-faithful output to biased and toxic generations. Due to several limiting factors surrounding LLMs (training cost, API access, data availability, etc.), it may not always be feasible to impose direct safety constraints on a deployed model. Therefore, an efficient and reliable alternative is required. To this end, we present our ongoing efforts to create and deploy a library of detectors: compact and easy-to-build classification models that provide labels for various harms. In addition to the detectors themselves, we discuss a wide range of uses for these detector models - from acting as guardrails to enabling effective AI governance. We also deep dive into inherent challenges in their development and discuss future work aimed at making the detectors more reliable and broadening their scope.",
    "link": "https://arxiv.org/abs/2403.06009",
    "context": "Title: Detectors for Safe and Reliable LLMs: Implementations, Uses, and Limitations\nAbstract: arXiv:2403.06009v1 Announce Type: new  Abstract: Large language models (LLMs) are susceptible to a variety of risks, from non-faithful output to biased and toxic generations. Due to several limiting factors surrounding LLMs (training cost, API access, data availability, etc.), it may not always be feasible to impose direct safety constraints on a deployed model. Therefore, an efficient and reliable alternative is required. To this end, we present our ongoing efforts to create and deploy a library of detectors: compact and easy-to-build classification models that provide labels for various harms. In addition to the detectors themselves, we discuss a wide range of uses for these detector models - from acting as guardrails to enabling effective AI governance. We also deep dive into inherent challenges in their development and discuss future work aimed at making the detectors more reliable and broadening their scope.",
    "path": "papers/24/03/2403.06009.json",
    "total_tokens": 839,
    "translated_title": "用于安全可靠LLM的检测器：实现、用途和局限性",
    "translated_abstract": "大型语言模型（LLMs）容易受到各种风险的影响，从输出不忠实到有偏见和有毒的生成。由于围绕LLMs存在的几个限制性因素（训练成本、API访问、数据可用性等），在部署模型时可能并非总是可行施加直接安全约束。因此，需要一个高效可靠的替代方案。为此，我们正在努力创建和部署一系列检测器库：紧凑且易于构建的分类模型，为各种危害提供标签。除了检测器本身，我们还讨论了这些检测器模型的广泛用途——从充当防护栏到促进有效的AI治理。我们还深入探讨了它们的开发中固有的挑战，并讨论了未来工作，旨在使检测器更可靠并拓展其范围。",
    "tldr": "创建了一系列检测器库，其中包含紧凑且易于构建的分类模型，为各种危害提供标签，可作为大型语言模型（LLMs）的有效替代方案。",
    "en_tdlr": "Developed a library of detectors consisting of compact and easy-to-build classification models that provide labels for various harms, serving as an efficient alternative for Large Language Models (LLMs)."
}