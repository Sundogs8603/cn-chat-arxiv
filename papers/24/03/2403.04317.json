{
    "title": "Online Adaptation of Language Models with a Memory of Amortized Contexts",
    "abstract": "arXiv:2403.04317v1 Announce Type: cross  Abstract: Due to the rapid generation and dissemination of information, large language models (LLMs) quickly run out of date despite enormous development costs. Due to this crucial need to keep models updated, online learning has emerged as a critical necessity when utilizing LLMs for real-world applications. However, given the ever-expanding corpus of unseen documents and the large parameter space of modern LLMs, efficient adaptation is essential. To address these challenges, we propose Memory of Amortized Contexts (MAC), an efficient and effective online adaptation framework for LLMs with strong knowledge retention. We propose an amortized feature extraction and memory-augmentation approach to compress and extract information from new documents into compact modulations stored in a memory bank. When answering questions, our model attends to and extracts relevant knowledge from this memory bank. To learn informative modulations in an efficient m",
    "link": "https://arxiv.org/abs/2403.04317",
    "context": "Title: Online Adaptation of Language Models with a Memory of Amortized Contexts\nAbstract: arXiv:2403.04317v1 Announce Type: cross  Abstract: Due to the rapid generation and dissemination of information, large language models (LLMs) quickly run out of date despite enormous development costs. Due to this crucial need to keep models updated, online learning has emerged as a critical necessity when utilizing LLMs for real-world applications. However, given the ever-expanding corpus of unseen documents and the large parameter space of modern LLMs, efficient adaptation is essential. To address these challenges, we propose Memory of Amortized Contexts (MAC), an efficient and effective online adaptation framework for LLMs with strong knowledge retention. We propose an amortized feature extraction and memory-augmentation approach to compress and extract information from new documents into compact modulations stored in a memory bank. When answering questions, our model attends to and extracts relevant knowledge from this memory bank. To learn informative modulations in an efficient m",
    "path": "papers/24/03/2403.04317.json",
    "total_tokens": 840,
    "translated_title": "带有分摊上下文记忆的语言模型的在线适应",
    "translated_abstract": "由于信息的快速生成和传播，即使开发成本巨大，大型语言模型（LLMs）也很快过时。鉴于保持模型更新的重要性，当在现实世界应用LLMs时，在线学习已成为一项至关重要的需求。然而，鉴于不断扩大的未见文档语料库和现代LLMs的大参数空间，高效的适应至关重要。为了解决这些挑战，我们提出了Memory of Amortized Contexts（MAC），这是一个针对LLMs的高效且有效的在线适应框架，具有较强的知识保留能力。我们提出了一种摊销特征提取和记忆增强方法，将新文档中的信息压缩并提取为存储在记忆库中的紧凑调制。在回答问题时，我们的模型关注并从该记忆库中提取相关知识。为了有效地学习有信息量的调制…",
    "tldr": "提出了一种带有分摊上下文记忆的在线适应框架，可有效地提取、压缩并存储信息以保持强大的知识保留能力",
    "en_tdlr": "Proposed an online adaptation framework with a memory of amortized contexts for efficient extraction, compression, and storage of information to maintain strong knowledge retention."
}