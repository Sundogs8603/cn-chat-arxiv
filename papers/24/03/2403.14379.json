{
    "title": "Tensor network compressibility of convolutional models",
    "abstract": "arXiv:2403.14379v1 Announce Type: cross  Abstract: Convolutional neural networks (CNNs) represent one of the most widely used neural network architectures, showcasing state-of-the-art performance in computer vision tasks. Although larger CNNs generally exhibit higher accuracy, their size can be effectively reduced by \"tensorization\" while maintaining accuracy. Tensorization consists of replacing the convolution kernels with compact decompositions such as Tucker, Canonical Polyadic decompositions, or quantum-inspired decompositions such as matrix product states, and directly training the factors in the decompositions to bias the learning towards low-rank decompositions. But why doesn't tensorization seem to impact the accuracy adversely? We explore this by assessing how truncating the convolution kernels of dense (untensorized) CNNs impact their accuracy. Specifically, we truncated the kernels of (i) a vanilla four-layer CNN and (ii) ResNet-50 pre-trained for image classification on CIF",
    "link": "https://arxiv.org/abs/2403.14379",
    "context": "Title: Tensor network compressibility of convolutional models\nAbstract: arXiv:2403.14379v1 Announce Type: cross  Abstract: Convolutional neural networks (CNNs) represent one of the most widely used neural network architectures, showcasing state-of-the-art performance in computer vision tasks. Although larger CNNs generally exhibit higher accuracy, their size can be effectively reduced by \"tensorization\" while maintaining accuracy. Tensorization consists of replacing the convolution kernels with compact decompositions such as Tucker, Canonical Polyadic decompositions, or quantum-inspired decompositions such as matrix product states, and directly training the factors in the decompositions to bias the learning towards low-rank decompositions. But why doesn't tensorization seem to impact the accuracy adversely? We explore this by assessing how truncating the convolution kernels of dense (untensorized) CNNs impact their accuracy. Specifically, we truncated the kernels of (i) a vanilla four-layer CNN and (ii) ResNet-50 pre-trained for image classification on CIF",
    "path": "papers/24/03/2403.14379.json",
    "total_tokens": 847,
    "translated_title": "卷积模型的张量网络可压缩性",
    "translated_abstract": "卷积神经网络（CNNs）代表了最广泛使用的神经网络架构之一，在计算机视觉任务中展示了最先进的性能。尽管一般情况下更大的CNNs通常表现出更高的准确性，但通过“张量化”可以有效地减小它们的大小，同时保持准确性。张量化包括将卷积核替换为如Tucker、Canonical Polyadic分解或受量子启发的分解（如矩阵乘积状态）等紧凑的分解，并直接训练分解中的因子，以偏向于低秩分解。但为什么张量化似乎对准确性没有不利影响？我们通过评估截断密集（非张量化）CNNs的卷积核对其准确性的影响来探讨这一点。",
    "tldr": "张量化是将卷积神经网络中的卷积核替换为紧凑分解，并直接训练分解因子以偏向于低秩分解的方法，该研究探讨了张量化如何通过评估截断卷积核来保持准确性。"
}