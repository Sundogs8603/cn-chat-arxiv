{
    "title": "KIT-19: A Comprehensive Korean Instruction Toolkit on 19 Tasks for Fine-Tuning Korean Large Language Models",
    "abstract": "arXiv:2403.16444v1 Announce Type: new  Abstract: Instruction Tuning on Large Language Models is an essential process for model to function well and achieve high performance in specific tasks. Accordingly, in mainstream languages such as English, instruction-based datasets are being constructed and made publicly available. In the case of Korean, publicly available models and datasets all rely on using the output of ChatGPT or translating datasets built in English. In this paper, We introduce \\textit{KIT-19} as an instruction dataset for the development of LLM in Korean. \\textit{KIT-19} is a dataset created in an instruction format, comprising 19 existing open-source datasets for Korean NLP tasks. In this paper, we train a Korean Pretrained LLM using \\textit{KIT-19} to demonstrate its effectiveness. The experimental results show that the model trained on \\textit{KIT-19} significantly outperforms existing Korean LLMs. Based on the its quality and empirical results, this paper proposes tha",
    "link": "https://arxiv.org/abs/2403.16444",
    "context": "Title: KIT-19: A Comprehensive Korean Instruction Toolkit on 19 Tasks for Fine-Tuning Korean Large Language Models\nAbstract: arXiv:2403.16444v1 Announce Type: new  Abstract: Instruction Tuning on Large Language Models is an essential process for model to function well and achieve high performance in specific tasks. Accordingly, in mainstream languages such as English, instruction-based datasets are being constructed and made publicly available. In the case of Korean, publicly available models and datasets all rely on using the output of ChatGPT or translating datasets built in English. In this paper, We introduce \\textit{KIT-19} as an instruction dataset for the development of LLM in Korean. \\textit{KIT-19} is a dataset created in an instruction format, comprising 19 existing open-source datasets for Korean NLP tasks. In this paper, we train a Korean Pretrained LLM using \\textit{KIT-19} to demonstrate its effectiveness. The experimental results show that the model trained on \\textit{KIT-19} significantly outperforms existing Korean LLMs. Based on the its quality and empirical results, this paper proposes tha",
    "path": "papers/24/03/2403.16444.json",
    "total_tokens": 883,
    "translated_title": "KIT-19：一套涵盖19个任务的韩文指令工具包，用于微调韩文大型语言模型",
    "translated_abstract": "Instruction Tuning on Large Language Models是模型表现良好、在特定任务中取得高性能的必要过程。在主流语言如英语中，正在构建和公开提供基于指令的数据集。对于韩语，公开可用的模型和数据集都依赖于使用ChatGPT的输出或翻译英文构建的数据集。本文介绍了\\textit{KIT-19}作为用于开发韩文LLM的指令数据集。 \\textit{KIT-19}是以指令格式创建的数据集，包括19个韩文NLP任务的现有开源数据集。我们在本文中使用\\textit{KIT-19}训练韩文预训练LLM，以展示其有效性。实验结果表明，在\\textit{KIT-19}上训练的模型明显优于现有的韩文LLM。基于其质量和实证结果，本文提出了",
    "tldr": "这项研究介绍了一套名为KIT-19的韩文指令数据集，用于开发韩文大型语言模型，在19个韩文自然语言处理任务上取得了显著的优越性能。",
    "en_tdlr": "This research introduces a Korean instruction dataset named KIT-19 for the development of Korean large language models, achieving significantly better performance in 19 Korean natural language processing tasks."
}