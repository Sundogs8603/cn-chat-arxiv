{
    "title": "Distributed Learning based on 1-Bit Gradient Coding in the Presence of Stragglers",
    "abstract": "arXiv:2403.14716v1 Announce Type: new  Abstract: This paper considers the problem of distributed learning (DL) in the presence of stragglers. For this problem, DL methods based on gradient coding have been widely investigated, which redundantly distribute the training data to the workers to guarantee convergence when some workers are stragglers. However, these methods require the workers to transmit real-valued vectors during the process of learning, which induces very high communication burden. To overcome this drawback, we propose a novel DL method based on 1-bit gradient coding (1-bit GCDL), where 1-bit data encoded from the locally computed gradients are transmitted by the workers to reduce the communication overhead. We theoretically provide the convergence guarantees of the proposed method for both the convex loss functions and nonconvex loss functions. It is shown empirically that 1-bit GC-DL outperforms the baseline methods, which attains better learning performance under the s",
    "link": "https://arxiv.org/abs/2403.14716",
    "context": "Title: Distributed Learning based on 1-Bit Gradient Coding in the Presence of Stragglers\nAbstract: arXiv:2403.14716v1 Announce Type: new  Abstract: This paper considers the problem of distributed learning (DL) in the presence of stragglers. For this problem, DL methods based on gradient coding have been widely investigated, which redundantly distribute the training data to the workers to guarantee convergence when some workers are stragglers. However, these methods require the workers to transmit real-valued vectors during the process of learning, which induces very high communication burden. To overcome this drawback, we propose a novel DL method based on 1-bit gradient coding (1-bit GCDL), where 1-bit data encoded from the locally computed gradients are transmitted by the workers to reduce the communication overhead. We theoretically provide the convergence guarantees of the proposed method for both the convex loss functions and nonconvex loss functions. It is shown empirically that 1-bit GC-DL outperforms the baseline methods, which attains better learning performance under the s",
    "path": "papers/24/03/2403.14716.json",
    "total_tokens": 903,
    "translated_title": "基于1比特梯度编码的存在滞后者的分布式学习",
    "translated_abstract": "本文考虑了在存在滞后者的情况下的分布式学习（DL）问题。针对这个问题，研究了基于梯度编码的DL方法，这些方法通过冗余地将训练数据分发给工作节点，以确保在一些工作节点是滞后者时收敛。然而，这些方法要求工作节点在学习过程中传输实值向量，这导致了非常高的通信负担。为了克服这一缺点，我们提出了一种基于1比特梯度编码（1-bit GCDL）的新型DL方法，其中工作节点传输从本地计算的梯度编码而成的1比特数据，以减少通信开销。我们在理论上为所提出的方法在凸损失函数和非凸损失函数下提供了收敛保证。实验证明，1比特GC-DL优于基准方法，其在存在滞后者的情况下实现更好的学习性能。",
    "tldr": "提出了一种基于1比特梯度编码的新型分布式学习方法，能够在存在滞后者的情况下降低通信负担，并在凸损失函数和非凸损失函数下具有收敛保证，实验证明其性能优于基准方法。",
    "en_tdlr": "Proposed a novel distributed learning method based on 1-bit gradient coding, which reduces communication overhead in the presence of stragglers, provides convergence guarantees for both convex and nonconvex loss functions, and empirically outperforms baseline methods."
}