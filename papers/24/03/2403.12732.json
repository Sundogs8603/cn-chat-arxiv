{
    "title": "Tighter Confidence Bounds for Sequential Kernel Regression",
    "abstract": "arXiv:2403.12732v1 Announce Type: cross  Abstract: Confidence bounds are an essential tool for rigorously quantifying the uncertainty of predictions. In this capacity, they can inform the exploration-exploitation trade-off and form a core component in many sequential learning and decision-making algorithms. Tighter confidence bounds give rise to algorithms with better empirical performance and better performance guarantees. In this work, we use martingale tail bounds and finite-dimensional reformulations of infinite-dimensional convex programs to establish new confidence bounds for sequential kernel regression. We prove that our new confidence bounds are always tighter than existing ones in this setting. We apply our confidence bounds to the kernel bandit problem, where future actions depend on the previous history. When our confidence bounds replace existing ones, the KernelUCB (GP-UCB) algorithm has better empirical performance, a matching worst-case performance guarantee and compara",
    "link": "https://arxiv.org/abs/2403.12732",
    "context": "Title: Tighter Confidence Bounds for Sequential Kernel Regression\nAbstract: arXiv:2403.12732v1 Announce Type: cross  Abstract: Confidence bounds are an essential tool for rigorously quantifying the uncertainty of predictions. In this capacity, they can inform the exploration-exploitation trade-off and form a core component in many sequential learning and decision-making algorithms. Tighter confidence bounds give rise to algorithms with better empirical performance and better performance guarantees. In this work, we use martingale tail bounds and finite-dimensional reformulations of infinite-dimensional convex programs to establish new confidence bounds for sequential kernel regression. We prove that our new confidence bounds are always tighter than existing ones in this setting. We apply our confidence bounds to the kernel bandit problem, where future actions depend on the previous history. When our confidence bounds replace existing ones, the KernelUCB (GP-UCB) algorithm has better empirical performance, a matching worst-case performance guarantee and compara",
    "path": "papers/24/03/2403.12732.json",
    "total_tokens": 851,
    "translated_title": "对于序贯核回归的更紧凑置信区间",
    "translated_abstract": "置信区间是严格量化预测不确定性的重要工具。它们可以指导探索与开发的权衡，并构成许多序贯学习和决策算法的核心组成部分。更紧凑的置信区间带来了具有更好经验性能和更好性能保证的算法。在这项工作中，我们使用鞅尾巴界限和无限维凸规划的有限维重构来建立序贯核回归的新置信区间。我们证明在这一设置中，我们的新置信区间始终比现有的更紧凑。我们将我们的置信区间应用于核赌博问题，其中未来的行动取决于先前的历史。当我们的置信区间取代现有的置信区间时，KernelUCB（GP-UCB）算法具有更好的经验性能，匹配的最坏情况性能保证和可比性。",
    "tldr": "通过使用鞅尾巴界限和无限维凸规划的有限维重构，建立了序贯核回归的新置信区间，证明其始终比现有的置信区间更紧凑，并将其应用于核赌博问题，提高了算法的性能表现。",
    "en_tdlr": "New confidence bounds for sequential kernel regression are established using martingale tail bounds and finite-dimensional reformulations of infinite-dimensional convex programs, which are always tighter than existing bounds and show improved algorithm performance in the kernel bandit problem."
}