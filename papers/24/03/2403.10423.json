{
    "title": "Quantization Avoids Saddle Points in Distributed Optimization",
    "abstract": "arXiv:2403.10423v1 Announce Type: cross  Abstract: Distributed nonconvex optimization underpins key functionalities of numerous distributed systems, ranging from power systems, smart buildings, cooperative robots, vehicle networks to sensor networks. Recently, it has also merged as a promising solution to handle the enormous growth in data and model sizes in deep learning. A fundamental problem in distributed nonconvex optimization is avoiding convergence to saddle points, which significantly degrade optimization accuracy. We discover that the process of quantization, which is necessary for all digital communications, can be exploited to enable saddle-point avoidance. More specifically, we propose a stochastic quantization scheme and prove that it can effectively escape saddle points and ensure convergence to a second-order stationary point in distributed nonconvex optimization. With an easily adjustable quantization granularity, the approach allows a user to control the number of bits",
    "link": "https://arxiv.org/abs/2403.10423",
    "context": "Title: Quantization Avoids Saddle Points in Distributed Optimization\nAbstract: arXiv:2403.10423v1 Announce Type: cross  Abstract: Distributed nonconvex optimization underpins key functionalities of numerous distributed systems, ranging from power systems, smart buildings, cooperative robots, vehicle networks to sensor networks. Recently, it has also merged as a promising solution to handle the enormous growth in data and model sizes in deep learning. A fundamental problem in distributed nonconvex optimization is avoiding convergence to saddle points, which significantly degrade optimization accuracy. We discover that the process of quantization, which is necessary for all digital communications, can be exploited to enable saddle-point avoidance. More specifically, we propose a stochastic quantization scheme and prove that it can effectively escape saddle points and ensure convergence to a second-order stationary point in distributed nonconvex optimization. With an easily adjustable quantization granularity, the approach allows a user to control the number of bits",
    "path": "papers/24/03/2403.10423.json",
    "total_tokens": 776,
    "translated_title": "分布式优化中避免鞍点的量化方法",
    "translated_abstract": "分布式非凸优化支撑着众多分布式系统的关键功能，从电力系统、智能建筑、协作机器人、车辆网络到传感器网络等等。最近，它也作为一个有前途的解决方案，用来处理深度学习中数据量和模型规模的巨大增长。分布式非凸优化中的一个基本问题是避免收敛到鞍点，鞍点会显著降低优化精度。我们发现量化过程，对于所有数字通信都是必需的，可以被利用来实现避免鞍点。具体来说，我们提出了一种随机量化方案，并证明它可以有效地避开鞍点，确保在分布式非凸优化中收敛到二阶稳定点。这种方法允许用户通过轻松调整量化粒度来控制位数。",
    "tldr": "量化方法在分布式非凸优化中能够避免收敛到鞍点，确保收敛到二阶稳定点。",
    "en_tdlr": "Quantization method can avoid convergence to saddle points in distributed nonconvex optimization, ensuring convergence to a second-order stationary point."
}