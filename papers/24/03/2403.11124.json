{
    "title": "Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment",
    "abstract": "arXiv:2403.11124v1 Announce Type: cross  Abstract: Alignment with human preference prevents large language models (LLMs) from generating misleading or toxic content while requiring high-cost human feedback. Assuming resources of human annotation are limited, there are two different ways of allocating considered: more diverse PROMPTS or more diverse RESPONSES to be labeled. Nonetheless, a straightforward comparison between their impact is absent. In this work, we first control the diversity of both sides according to the number of samples for fine-tuning, which can directly reflect their influence. We find that instead of numerous prompts, more responses but fewer prompts better trigger LLMs for human alignment. Additionally, the concept of diversity for prompts can be more complex than responses that are typically quantified by single digits. Consequently, a new formulation of prompt diversity is proposed, further implying a linear correlation with the final performance of LLMs after f",
    "link": "https://arxiv.org/abs/2403.11124",
    "context": "Title: Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment\nAbstract: arXiv:2403.11124v1 Announce Type: cross  Abstract: Alignment with human preference prevents large language models (LLMs) from generating misleading or toxic content while requiring high-cost human feedback. Assuming resources of human annotation are limited, there are two different ways of allocating considered: more diverse PROMPTS or more diverse RESPONSES to be labeled. Nonetheless, a straightforward comparison between their impact is absent. In this work, we first control the diversity of both sides according to the number of samples for fine-tuning, which can directly reflect their influence. We find that instead of numerous prompts, more responses but fewer prompts better trigger LLMs for human alignment. Additionally, the concept of diversity for prompts can be more complex than responses that are typically quantified by single digits. Consequently, a new formulation of prompt diversity is proposed, further implying a linear correlation with the final performance of LLMs after f",
    "path": "papers/24/03/2403.11124.json",
    "total_tokens": 818,
    "translated_title": "在人类对齐中扩展数据多样性以微调语言模型",
    "translated_abstract": "与人类偏好对齐可以防止大型语言模型（LLMs）生成误导性或有毒内容，同时需要高成本的人类反馈。假设人工注释资源有限，则可以考虑两种不同的分配方式：更多样化的提示或更多样化的待标记响应。然而，它们对结果的影响的直接比较尚不存在。在这项工作中，我们首先根据微调样本数量控制双方的多样性，这可以直接反映它们的影响。我们发现，与大量提示不同，更多的响应但是更少的提示更能激发LLMs进行人类对齐。此外，提示的多样性概念可能比通常由单个数字量化的响应更复杂。因此，提出了提示多样性的新公式，进一步暗示与微调后LLMs最终性能的线性相关。",
    "tldr": "更多的响应但更少的提示可以更好地触发大型语言模型进行人类对齐，此外，提出了一种新的提示多样性公式，可以进一步影响LLMs的最终性能。",
    "en_tdlr": "More responses but fewer prompts can better trigger large language models for human alignment. Additionally, a new formulation of prompt diversity is proposed, which further impacts the final performance of LLMs."
}