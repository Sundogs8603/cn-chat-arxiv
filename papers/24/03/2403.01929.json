{
    "title": "Analyzing and Adapting Large Language Models for Few-Shot Multilingual NLU: Are We There Yet?",
    "abstract": "arXiv:2403.01929v1 Announce Type: new  Abstract: Supervised fine-tuning (SFT), supervised instruction tuning (SIT) and in-context learning (ICL) are three alternative, de facto standard approaches to few-shot learning. ICL has gained popularity recently with the advent of LLMs due to its simplicity and sample efficiency. Prior research has conducted only limited investigation into how these approaches work for multilingual few-shot learning, and the focus so far has been mostly on their performance. In this work, we present an extensive and systematic comparison of the three approaches, testing them on 6 high- and low-resource languages, three different NLU tasks, and a myriad of language and domain setups. Importantly, performance is only one aspect of the comparison, where we also analyse the approaches through the optics of their computational, inference and financial costs. Our observations show that supervised instruction tuning has the best trade-off between performance and resou",
    "link": "https://arxiv.org/abs/2403.01929",
    "context": "Title: Analyzing and Adapting Large Language Models for Few-Shot Multilingual NLU: Are We There Yet?\nAbstract: arXiv:2403.01929v1 Announce Type: new  Abstract: Supervised fine-tuning (SFT), supervised instruction tuning (SIT) and in-context learning (ICL) are three alternative, de facto standard approaches to few-shot learning. ICL has gained popularity recently with the advent of LLMs due to its simplicity and sample efficiency. Prior research has conducted only limited investigation into how these approaches work for multilingual few-shot learning, and the focus so far has been mostly on their performance. In this work, we present an extensive and systematic comparison of the three approaches, testing them on 6 high- and low-resource languages, three different NLU tasks, and a myriad of language and domain setups. Importantly, performance is only one aspect of the comparison, where we also analyse the approaches through the optics of their computational, inference and financial costs. Our observations show that supervised instruction tuning has the best trade-off between performance and resou",
    "path": "papers/24/03/2403.01929.json",
    "total_tokens": 872,
    "translated_title": "分析和调整大型语言模型以用于少样本多语言自然语言理解：我们到达了吗？",
    "translated_abstract": "监督微调（SFT）、监督指导调整（SIT）和上下文学习（ICL）是三种少样本学习的替代且实际标准方法。ICL由于其简单性和样本效率，最近由于LLM的出现而变得流行。先前的研究仅对这些方法如何用于多语种少样本学习进行了有限的调查，到目前为止，重点主要都是它们的性能。在这项工作中，我们对这三种方法进行了广泛而系统的比较，将它们应用于6种高资源和低资源语言、三种不同的自然语言理解任务以及多种语言和领域设置。重要的是，性能只是比较的一个方面，我们还通过计算成本、推理成本和财务成本的视角来分析这些方法。我们的观察表明，监督指导调整在性能和资源之间具有最佳的平衡。",
    "tldr": "本研究对监督微调、监督指导调整和上下文学习三种方法进行了广泛比较，发现监督指导调整在性能和资源之间具有最佳的平衡。",
    "en_tdlr": "This study provides an extensive comparison of supervised fine-tuning, supervised instruction tuning, and in-context learning, showing that supervised instruction tuning has the best trade-off between performance and resources."
}