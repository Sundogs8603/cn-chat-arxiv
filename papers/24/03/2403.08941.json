{
    "title": "Towards Model-Agnostic Posterior Approximation for Fast and Accurate Variational Autoencoders",
    "abstract": "arXiv:2403.08941v1 Announce Type: cross  Abstract: Inference for Variational Autoencoders (VAEs) consists of learning two models: (1) a generative model, which transforms a simple distribution over a latent space into the distribution over observed data, and (2) an inference model, which approximates the posterior of the latent codes given data. The two components are learned jointly via a lower bound to the generative model's log marginal likelihood. In early phases of joint training, the inference model poorly approximates the latent code posteriors. Recent work showed that this leads optimization to get stuck in local optima, negatively impacting the learned generative model. As such, recent work suggests ensuring a high-quality inference model via iterative training: maximizing the objective function relative to the inference model before every update to the generative model. Unfortunately, iterative training is inefficient, requiring heuristic criteria for reverting from iterative",
    "link": "https://arxiv.org/abs/2403.08941",
    "context": "Title: Towards Model-Agnostic Posterior Approximation for Fast and Accurate Variational Autoencoders\nAbstract: arXiv:2403.08941v1 Announce Type: cross  Abstract: Inference for Variational Autoencoders (VAEs) consists of learning two models: (1) a generative model, which transforms a simple distribution over a latent space into the distribution over observed data, and (2) an inference model, which approximates the posterior of the latent codes given data. The two components are learned jointly via a lower bound to the generative model's log marginal likelihood. In early phases of joint training, the inference model poorly approximates the latent code posteriors. Recent work showed that this leads optimization to get stuck in local optima, negatively impacting the learned generative model. As such, recent work suggests ensuring a high-quality inference model via iterative training: maximizing the objective function relative to the inference model before every update to the generative model. Unfortunately, iterative training is inefficient, requiring heuristic criteria for reverting from iterative",
    "path": "papers/24/03/2403.08941.json",
    "total_tokens": 876,
    "translated_title": "面向模型无关后验逼近的快速准确变分自编码器",
    "translated_abstract": "变分自编码器（VAEs）的推断包括学习两个模型：（1）生成模型，将潜在空间上的简单分布转换为观测数据分布，以及（2）推断模型，近似给定数据的潜在编码后验。这两个组件通过对生成模型对数边际似然的下界进行联合学习。在联合训练的早期阶段，推断模型很差地近似了潜在编码后验。最近的研究表明，这导致优化陷入局部最优解，对学习到的生成模型造成负面影响。因此，最近的研究建议通过迭代训练确保高质量的推断模型：相对于生成模型的每次更新之前最大化与推断模型相关的目标函数。不幸的是，迭代训练效率低，需要启发式标准来从迭代中恢复。",
    "tldr": "最近的研究表明，为了确保高质量的推断模型，可以通过迭代训练最大化与推断模型相关的目标函数，以解决变分自编码器中推断模型近似不准确导致的局部最优解问题。",
    "en_tdlr": "Recent research suggests maximizing the objective function relative to the inference model through iterative training to address the issue of local optima caused by inaccurate approximation of the inference model in variational autoencoders."
}