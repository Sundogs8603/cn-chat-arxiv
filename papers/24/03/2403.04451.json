{
    "title": "Membership Inference Attacks and Privacy in Topic Modeling",
    "abstract": "arXiv:2403.04451v1 Announce Type: cross  Abstract: Recent research shows that large language models are susceptible to privacy attacks that infer aspects of the training data. However, it is unclear if simpler generative models, like topic models, share similar vulnerabilities. In this work, we propose an attack against topic models that can confidently identify members of the training data in Latent Dirichlet Allocation. Our results suggest that the privacy risks associated with generative modeling are not restricted to large neural models. Additionally, to mitigate these vulnerabilities, we explore differentially private (DP) topic modeling. We propose a framework for private topic modeling that incorporates DP vocabulary selection as a pre-processing step, and show that it improves privacy while having limited effects on practical utility.",
    "link": "https://arxiv.org/abs/2403.04451",
    "context": "Title: Membership Inference Attacks and Privacy in Topic Modeling\nAbstract: arXiv:2403.04451v1 Announce Type: cross  Abstract: Recent research shows that large language models are susceptible to privacy attacks that infer aspects of the training data. However, it is unclear if simpler generative models, like topic models, share similar vulnerabilities. In this work, we propose an attack against topic models that can confidently identify members of the training data in Latent Dirichlet Allocation. Our results suggest that the privacy risks associated with generative modeling are not restricted to large neural models. Additionally, to mitigate these vulnerabilities, we explore differentially private (DP) topic modeling. We propose a framework for private topic modeling that incorporates DP vocabulary selection as a pre-processing step, and show that it improves privacy while having limited effects on practical utility.",
    "path": "papers/24/03/2403.04451.json",
    "total_tokens": 735,
    "translated_title": "会员推理攻击与主题建模中的隐私",
    "translated_abstract": "最近的研究表明，大型语言模型容易受到推理训练数据方面的隐私攻击。然而，目前还不清楚更简单的生成模型，例如主题模型，是否存在类似的漏洞。在这项工作中，我们提出了一种针对主题模型的攻击，可以自信地识别Latent Dirichlet Allocation中训练数据的成员。我们的结果表明，与大型神经模型相关联的隐私风险并不仅限于大型神经模型。此外，为了减轻这些漏洞，我们探讨了差分隐私（DP）主题建模。我们提出了一个私密主题建模框架，将DP词汇选择作为预处理步骤，并展示它不仅改善了隐私性，而且在实用性方面的影响有限。",
    "tldr": "主题建模中提出了会员推理攻击，通过差分隐私词汇选择来改善隐私风险"
}