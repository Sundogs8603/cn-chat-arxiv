{
    "title": "Hawk: Accurate and Fast Privacy-Preserving Machine Learning Using Secure Lookup Table Computation",
    "abstract": "arXiv:2403.17296v1 Announce Type: cross  Abstract: Training machine learning models on data from multiple entities without direct data sharing can unlock applications otherwise hindered by business, legal, or ethical constraints. In this work, we design and implement new privacy-preserving machine learning protocols for logistic regression and neural network models. We adopt a two-server model where data owners secret-share their data between two servers that train and evaluate the model on the joint data. A significant source of inefficiency and inaccuracy in existing methods arises from using Yao's garbled circuits to compute non-linear activation functions. We propose new methods for computing non-linear functions based on secret-shared lookup tables, offering both computational efficiency and improved accuracy.   Beyond introducing leakage-free techniques, we initiate the exploration of relaxed security measures for privacy-preserving machine learning. Instead of claiming that the ",
    "link": "https://arxiv.org/abs/2403.17296",
    "context": "Title: Hawk: Accurate and Fast Privacy-Preserving Machine Learning Using Secure Lookup Table Computation\nAbstract: arXiv:2403.17296v1 Announce Type: cross  Abstract: Training machine learning models on data from multiple entities without direct data sharing can unlock applications otherwise hindered by business, legal, or ethical constraints. In this work, we design and implement new privacy-preserving machine learning protocols for logistic regression and neural network models. We adopt a two-server model where data owners secret-share their data between two servers that train and evaluate the model on the joint data. A significant source of inefficiency and inaccuracy in existing methods arises from using Yao's garbled circuits to compute non-linear activation functions. We propose new methods for computing non-linear functions based on secret-shared lookup tables, offering both computational efficiency and improved accuracy.   Beyond introducing leakage-free techniques, we initiate the exploration of relaxed security measures for privacy-preserving machine learning. Instead of claiming that the ",
    "path": "papers/24/03/2403.17296.json",
    "total_tokens": 859,
    "translated_title": "Hawk: 使用安全查找表计算的准确快速隐私保护机器学习",
    "translated_abstract": "在没有直接数据共享的情况下对来自多个实体的数据进行机器学习模型训练可以解锁受到业务、法律或伦理约束的应用。本文设计并实现了新的隐私保护机器学习协议，用于逻辑回归和神经网络模型。我们采用数据所有者在两个服务器之间进行数据秘密共享，这两个服务器对联合数据进行模型训练和评估。现有方法中存在的一个重要的低效和不准确之处在于使用Yao的混淆电路来计算非线性激活函数。我们提出了基于秘密共享查找表计算非线性函数的新方法，既提高了计算效率又提高了准确性。除了引入无泄漏技术，我们还开展了对隐私保护机器学习的放松安全措施的探索。",
    "tldr": "本研究提出了用于逻辑回归和神经网络模型的新隐私保护机器学习协议，通过采用秘密共享查找表计算非线性函数，提高了计算效率和准确性，并探索了放松安全措施的可能性。",
    "en_tdlr": "This study proposes new privacy-preserving machine learning protocols for logistic regression and neural network models, which enhance computational efficiency and accuracy by utilizing secret-shared lookup tables for computing nonlinear functions, and explores the possibility of relaxed security measures."
}