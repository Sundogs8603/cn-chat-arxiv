{
    "title": "mALBERT: Is a Compact Multilingual BERT Model Still Worth It?",
    "abstract": "arXiv:2403.18338v1 Announce Type: new  Abstract: Within the current trend of Pretained Language Models (PLM), emerge more and more criticisms about the ethical andecological impact of such models. In this article, considering these critical remarks, we propose to focus on smallermodels, such as compact models like ALBERT, which are more ecologically virtuous than these PLM. However,PLMs enable huge breakthroughs in Natural Language Processing tasks, such as Spoken and Natural LanguageUnderstanding, classification, Question--Answering tasks. PLMs also have the advantage of being multilingual, and,as far as we know, a multilingual version of compact ALBERT models does not exist. Considering these facts, wepropose the free release of the first version of a multilingual compact ALBERT model, pre-trained using Wikipediadata, which complies with the ethical aspect of such a language model. We also evaluate the model against classicalmultilingual PLMs in classical NLP tasks. Finally, this pap",
    "link": "https://arxiv.org/abs/2403.18338",
    "context": "Title: mALBERT: Is a Compact Multilingual BERT Model Still Worth It?\nAbstract: arXiv:2403.18338v1 Announce Type: new  Abstract: Within the current trend of Pretained Language Models (PLM), emerge more and more criticisms about the ethical andecological impact of such models. In this article, considering these critical remarks, we propose to focus on smallermodels, such as compact models like ALBERT, which are more ecologically virtuous than these PLM. However,PLMs enable huge breakthroughs in Natural Language Processing tasks, such as Spoken and Natural LanguageUnderstanding, classification, Question--Answering tasks. PLMs also have the advantage of being multilingual, and,as far as we know, a multilingual version of compact ALBERT models does not exist. Considering these facts, wepropose the free release of the first version of a multilingual compact ALBERT model, pre-trained using Wikipediadata, which complies with the ethical aspect of such a language model. We also evaluate the model against classicalmultilingual PLMs in classical NLP tasks. Finally, this pap",
    "path": "papers/24/03/2403.18338.json",
    "total_tokens": 872,
    "translated_title": "mALBERT：小型多语言BERT模型仍然值得吗？",
    "translated_abstract": "在当前预训练语言模型（PLM）的趋势中，越来越多的批评涉及这些模型的道德和生态影响。本文考虑到这些批评意见，提出关注更小型的模型，如ALBERT这样的小型模型，这些模型在生态上比PLM更有优势。然而，PLM在自然语言处理任务中取得了巨大突破，如口语和自然语言理解、分类、问答任务。此外，PLM还具有多语言的优势，据我们所知，小型ALBERT模型的多语言版本尚不存在。基于这些事实，我们提出免费释放第一个使用Wikipedia数据预训练的多语言小型ALBERT模型的第一版，以符合这种语言模型的道德方面。我们还对模型在经典NLP任务中与传统多语言PLM进行评估。最后，这篇论文...",
    "tldr": "提出了第一个版本的多语言小型ALBERT模型，旨在解决预训练语言模型对道德和生态的影响，并通过与传统多语言PLM的比较，在经典NLP任务中评估了该模型的性能。",
    "en_tdlr": "Introduced the first version of a multilingual compact ALBERT model to address the ethical and ecological impact of pre-trained language models, and evaluated its performance in classical NLP tasks compared to traditional multilingual PLMs."
}