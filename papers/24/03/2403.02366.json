{
    "title": "Human Evaluation of English--Irish Transformer-Based NMT",
    "abstract": "arXiv:2403.02366v1 Announce Type: cross  Abstract: In this study, a human evaluation is carried out on how hyperparameter settings impact the quality of Transformer-based Neural Machine Translation (NMT) for the low-resourced English--Irish pair. SentencePiece models using both Byte Pair Encoding (BPE) and unigram approaches were appraised. Variations in model architectures included modifying the number of layers, evaluating the optimal number of heads for attention and testing various regularisation techniques. The greatest performance improvement was recorded for a Transformer-optimized model with a 16k BPE subword model. Compared with a baseline Recurrent Neural Network (RNN) model, a Transformer-optimized model demonstrated a BLEU score improvement of 7.8 points. When benchmarked against Google Translate, our translation engines demonstrated significant improvements. Furthermore, a quantitative fine-grained manual evaluation was conducted which compared the performance of machine t",
    "link": "https://arxiv.org/abs/2403.02366",
    "context": "Title: Human Evaluation of English--Irish Transformer-Based NMT\nAbstract: arXiv:2403.02366v1 Announce Type: cross  Abstract: In this study, a human evaluation is carried out on how hyperparameter settings impact the quality of Transformer-based Neural Machine Translation (NMT) for the low-resourced English--Irish pair. SentencePiece models using both Byte Pair Encoding (BPE) and unigram approaches were appraised. Variations in model architectures included modifying the number of layers, evaluating the optimal number of heads for attention and testing various regularisation techniques. The greatest performance improvement was recorded for a Transformer-optimized model with a 16k BPE subword model. Compared with a baseline Recurrent Neural Network (RNN) model, a Transformer-optimized model demonstrated a BLEU score improvement of 7.8 points. When benchmarked against Google Translate, our translation engines demonstrated significant improvements. Furthermore, a quantitative fine-grained manual evaluation was conducted which compared the performance of machine t",
    "path": "papers/24/03/2403.02366.json",
    "total_tokens": 928,
    "translated_title": "人类评估英爱变压器基于NMT的翻译质量",
    "translated_abstract": "在本研究中，对超参数设置如何影响低资源英语-爱尔兰文对的变压器神经机器翻译（NMT）质量进行了人类评估。使用Byte Pair Encoding（BPE）和unigram方法的SentencePiece模型受到了评价。模型架构的变化包括修改层数，评估注意力的最佳头数以及测试各种正则化技术。在一个优化了的16k BPE子词模型下，Transformer优化模型的性能表现得最好。与基准循环神经网络（RNN）模型相比，Transformer优化模型的BLEU分数提高了7.8个点。与谷歌翻译相比，我们的翻译引擎展示了显著的改进。此外，进行了定量细粒度的手动评估，比较了机器翻译的性能。",
    "tldr": "本研究评估了超参数设置对低资源英-爱变压器神经机器翻译质量的影响，并发现优化的Transformer模型在16k BPE子词模型下表现最佳，相较于基准RNN模型提高了7.8个BLEU分数，并在与谷歌翻译的比较中展示出显著的改进。",
    "en_tdlr": "This study evaluated the impact of hyperparameter settings on the quality of Transformer-based Neural Machine Translation (NMT) for the low-resourced English-Irish pair, and found that the optimized Transformer model performed best with a 16k BPE subword model, achieving a 7.8-point improvement in BLEU score compared to the baseline RNN model, showing significant enhancements when benchmarked against Google Translate."
}