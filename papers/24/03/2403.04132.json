{
    "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
    "abstract": "arXiv:2403.04132v1 Announce Type: new  Abstract: Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of",
    "link": "https://arxiv.org/abs/2403.04132",
    "context": "Title: Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference\nAbstract: arXiv:2403.04132v1 Announce Type: new  Abstract: Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of",
    "path": "papers/24/03/2403.04132.json",
    "total_tokens": 861,
    "translated_title": "Chatbot Arena：一个通过人类偏好评估LLM的开放平台",
    "translated_abstract": "大型语言模型（LLMs）解锁了新的能力和应用；然而，评估其与人类偏好的一致性仍然面临着重大挑战。为了解决这个问题，我们介绍了Chatbot Arena，这是一个基于人类偏好评估LLMs的开放平台。我们的方法采用了一种两两比较的方式，并通过众包利用来自不同用户群体的输入。该平台已经运营了几个月，获得了超过24万个投票。本文描述了该平台，分析了我们迄今收集的数据，并解释了我们正在使用的经过验证的统计方法，以便对模型进行高效准确的评估和排名。我们确认众包问题足够多样化和区分化，并且众包人类投票与专家评级者的投票基本一致。这些分析共同为平台的可信度奠定了坚实的基础。",
    "tldr": "Chatbot Arena是一个开放平台，采用两两比较的方式通过众包利用人类偏好评估LLMs。研究表明众包问题多样且具有区分性，并且人类投票与专家评级者的投票基本一致。",
    "en_tdlr": "Chatbot Arena is an open platform that evaluates LLMs based on human preferences using pairwise comparison through crowdsourcing. The study shows that the crowdsourced questions are diverse and discriminating, and human votes are in good agreement with expert raters."
}