{
    "title": "Sampling-based Safe Reinforcement Learning for Nonlinear Dynamical Systems",
    "abstract": "arXiv:2403.04007v1 Announce Type: new  Abstract: We develop provably safe and convergent reinforcement learning (RL) algorithms for control of nonlinear dynamical systems, bridging the gap between the hard safety guarantees of control theory and the convergence guarantees of RL theory. Recent advances at the intersection of control and RL follow a two-stage, safety filter approach to enforcing hard safety constraints: model-free RL is used to learn a potentially unsafe controller, whose actions are projected onto safe sets prescribed, for example, by a control barrier function. Though safe, such approaches lose any convergence guarantees enjoyed by the underlying RL methods. In this paper, we develop a single-stage, sampling-based approach to hard constraint satisfaction that learns RL controllers enjoying classical convergence guarantees while satisfying hard safety constraints throughout training and deployment. We validate the efficacy of our approach in simulation, including safe c",
    "link": "https://arxiv.org/abs/2403.04007",
    "context": "Title: Sampling-based Safe Reinforcement Learning for Nonlinear Dynamical Systems\nAbstract: arXiv:2403.04007v1 Announce Type: new  Abstract: We develop provably safe and convergent reinforcement learning (RL) algorithms for control of nonlinear dynamical systems, bridging the gap between the hard safety guarantees of control theory and the convergence guarantees of RL theory. Recent advances at the intersection of control and RL follow a two-stage, safety filter approach to enforcing hard safety constraints: model-free RL is used to learn a potentially unsafe controller, whose actions are projected onto safe sets prescribed, for example, by a control barrier function. Though safe, such approaches lose any convergence guarantees enjoyed by the underlying RL methods. In this paper, we develop a single-stage, sampling-based approach to hard constraint satisfaction that learns RL controllers enjoying classical convergence guarantees while satisfying hard safety constraints throughout training and deployment. We validate the efficacy of our approach in simulation, including safe c",
    "path": "papers/24/03/2403.04007.json",
    "total_tokens": 684,
    "translated_title": "基于采样的非线性动力系统安全强化学习",
    "translated_abstract": "我们为控制非线性动力系统开发了具有可证明安全性和收敛性的强化学习（RL）算法，弥合了控制理论的严格安全性保证和RL理论的收敛性保证之间的差距。我们发展了一种单阶段、基于采样的方法，用于硬约束满足，学习RL控制器在整个训练和部署过程中满足严格的安全约束，同时享有经典收敛保证。",
    "tldr": "该论文提出了一种基于采样的安全强化学习方法，可以在控制非线性动力系统时同时满足硬性安全约束和收敛性保证。"
}