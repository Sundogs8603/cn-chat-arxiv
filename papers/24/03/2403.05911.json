{
    "title": "Towards Optimizing Human-Centric Objectives in AI-Assisted Decision-Making With Offline Reinforcement Learning",
    "abstract": "arXiv:2403.05911v1 Announce Type: cross  Abstract: As AI assistance is increasingly infused into decision-making processes, we may seek to optimize human-centric objectives beyond decision accuracy, such as skill improvement or task enjoyment of individuals interacting with these systems. With this aspiration in mind, we propose offline reinforcement learning (RL) as a general approach for modeling human-AI decision-making to optimize such human-centric objectives. Our approach seeks to optimize different objectives by adaptively providing decision support to humans -- the right type of assistance, to the right person, at the right time. We instantiate our approach with two objectives: human-AI accuracy on the decision-making task and human learning about the task, and learn policies that optimize these two objectives from previous human-AI interaction data. We compare the optimized policies against various baselines in AI-assisted decision-making. Across two experiments (N = 316 and N",
    "link": "https://arxiv.org/abs/2403.05911",
    "context": "Title: Towards Optimizing Human-Centric Objectives in AI-Assisted Decision-Making With Offline Reinforcement Learning\nAbstract: arXiv:2403.05911v1 Announce Type: cross  Abstract: As AI assistance is increasingly infused into decision-making processes, we may seek to optimize human-centric objectives beyond decision accuracy, such as skill improvement or task enjoyment of individuals interacting with these systems. With this aspiration in mind, we propose offline reinforcement learning (RL) as a general approach for modeling human-AI decision-making to optimize such human-centric objectives. Our approach seeks to optimize different objectives by adaptively providing decision support to humans -- the right type of assistance, to the right person, at the right time. We instantiate our approach with two objectives: human-AI accuracy on the decision-making task and human learning about the task, and learn policies that optimize these two objectives from previous human-AI interaction data. We compare the optimized policies against various baselines in AI-assisted decision-making. Across two experiments (N = 316 and N",
    "path": "papers/24/03/2403.05911.json",
    "total_tokens": 890,
    "translated_title": "优化人类中心目标：AI辅助决策中的离线强化学习研究",
    "translated_abstract": "随着人工智能辅助渗透到决策过程中，我们可能希望优化人类中心目标，超越决策准确性，如那些与这些系统互动的个体的技能提升或任务享受。考虑到这一愿景，我们提出了离线强化学习（RL）作为建模人机决策以优化这些人类中心目标的一般方法。我们的方法通过灵活地为人类提供决策支持来优化不同的目标--在正确的时间、向正确的人提供正确类型的帮助。我们用两个目标实例化我们的方法：人工智能在决策任务中的准确性和人类对该任务的学习，并从先前的人机交互数据中学习优化这两个目标的策略。我们将优化的策略与在AI辅助决策中的各种基线进行比较。在两个实验中（N = 316 和 N",
    "tldr": "该研究提出了使用离线强化学习来优化人类中心目标的方法，通过提供适当类型的决策支持，针对特定人员、在适当时间，来优化决策准确性和人类学习能力这两个目标。",
    "en_tdlr": "This study proposes a method to optimize human-centric objectives using offline reinforcement learning, by providing tailored decision support to specific individuals at the right time to optimize decision accuracy and human learning ability."
}