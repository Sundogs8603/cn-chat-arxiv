{
    "title": "Pre-Trained Model Recommendation for Downstream Fine-tuning",
    "abstract": "arXiv:2403.06382v1 Announce Type: cross  Abstract: As a fundamental problem in transfer learning, model selection aims to rank off-the-shelf pre-trained models and select the most suitable one for the new target task. Existing model selection techniques are often constrained in their scope and tend to overlook the nuanced relationships between models and tasks. In this paper, we present a pragmatic framework \\textbf{Fennec}, delving into a diverse, large-scale model repository while meticulously considering the intricate connections between tasks and models. The key insight is to map all models and historical tasks into a transfer-related subspace, where the distance between model vectors and task vectors represents the magnitude of transferability. A large vision model, as a proxy, infers a new task's representation in the transfer space, thereby circumventing the computational burden of extensive forward passes. We also investigate the impact of the inherent inductive bias of models ",
    "link": "https://arxiv.org/abs/2403.06382",
    "context": "Title: Pre-Trained Model Recommendation for Downstream Fine-tuning\nAbstract: arXiv:2403.06382v1 Announce Type: cross  Abstract: As a fundamental problem in transfer learning, model selection aims to rank off-the-shelf pre-trained models and select the most suitable one for the new target task. Existing model selection techniques are often constrained in their scope and tend to overlook the nuanced relationships between models and tasks. In this paper, we present a pragmatic framework \\textbf{Fennec}, delving into a diverse, large-scale model repository while meticulously considering the intricate connections between tasks and models. The key insight is to map all models and historical tasks into a transfer-related subspace, where the distance between model vectors and task vectors represents the magnitude of transferability. A large vision model, as a proxy, infers a new task's representation in the transfer space, thereby circumventing the computational burden of extensive forward passes. We also investigate the impact of the inherent inductive bias of models ",
    "path": "papers/24/03/2403.06382.json",
    "total_tokens": 853,
    "translated_title": "针对下游微调的预训练模型推荐",
    "translated_abstract": "作为迁移学习中的一个基本问题，模型选择旨在对现成的预训练模型进行排名，并选择最适合新目标任务的模型。现有的模型选择技术通常在范围上受限，并倾向于忽视模型与任务之间微妙的关系。在本文中，我们提出了一个务实的框架 Fennec，深入研究了一个多样化、大规模的模型库，同时细致考虑了任务与模型之间的复杂联系。关键洞见在于将所有模型和历史任务映射到一个与迁移相关的子空间中，模型向量和任务向量之间的距离代表了可迁移性的大小。一个大型视觉模型作为代理人，在迁移空间中推断新任务的表示，从而避开了进行大量前向传播的计算负担。我们还调查了模型固有归纳偏差的影响。",
    "tldr": "本文提出了一个名为Fennec的框架，通过将所有模型和历史任务映射到一个迁移相关的子空间，以便推断新任务在迁移空间中的表征，从而改善模型选择技术。"
}