{
    "title": "Metric Learning from Limited Pairwise Preference Comparisons",
    "abstract": "arXiv:2403.19629v1 Announce Type: new  Abstract: We study metric learning from preference comparisons under the ideal point model, in which a user prefers an item over another if it is closer to their latent ideal item. These items are embedded into $\\mathbb{R}^d$ equipped with an unknown Mahalanobis distance shared across users. While recent work shows that it is possible to simultaneously recover the metric and ideal items given $\\mathcal{O}(d)$ pairwise comparisons per user, in practice we often have a limited budget of $o(d)$ comparisons. We study whether the metric can still be recovered, even though it is known that learning individual ideal items is now no longer possible. We show that in general, $o(d)$ comparisons reveals no information about the metric, even with infinitely many users. However, when comparisons are made over items that exhibit low-dimensional structure, each user can contribute to learning the metric restricted to a low-dimensional subspace so that the metric",
    "link": "https://arxiv.org/abs/2403.19629",
    "context": "Title: Metric Learning from Limited Pairwise Preference Comparisons\nAbstract: arXiv:2403.19629v1 Announce Type: new  Abstract: We study metric learning from preference comparisons under the ideal point model, in which a user prefers an item over another if it is closer to their latent ideal item. These items are embedded into $\\mathbb{R}^d$ equipped with an unknown Mahalanobis distance shared across users. While recent work shows that it is possible to simultaneously recover the metric and ideal items given $\\mathcal{O}(d)$ pairwise comparisons per user, in practice we often have a limited budget of $o(d)$ comparisons. We study whether the metric can still be recovered, even though it is known that learning individual ideal items is now no longer possible. We show that in general, $o(d)$ comparisons reveals no information about the metric, even with infinitely many users. However, when comparisons are made over items that exhibit low-dimensional structure, each user can contribute to learning the metric restricted to a low-dimensional subspace so that the metric",
    "path": "papers/24/03/2403.19629.json",
    "total_tokens": 896,
    "translated_title": "有限成对偏好比较下的度量学习",
    "translated_abstract": "我们研究了在理想点模型下的偏好比较中的度量学习，其中用户如果一个项目比其潜在理想项目更接近，则更喜欢该项目。这些项目嵌入到具有未知马氏距离的$\\mathbb{R}^d$中，该距离在用户间共享。尽管最近的工作表明，通过每个用户$\\mathcal{O}(d)$个成对比较可以同时恢复度量和理想项目，但在实践中，我们经常有$o(d)$的有限比较预算。我们研究了即使已知学习单个理想项目现在不再可能，度量是否仍然可以恢复。我们发现一般来说，$o(d)$比较不会揭示有关度量的信息，即使用户数量无限。然而，当比较的项目表现出低维结构时，每个用户都可以有助于学习限制在低维子空间中的度量，这样度量就可以被恢复。",
    "tldr": "在有限成对偏好比较下研究度量学习，表明虽然无法学习单个理想项目，但当比较对象表现出低维结构时，每个用户可以帮助学习限制在低维子空间中的度量。",
    "en_tdlr": "Investigating metric learning from limited pairwise preference comparisons, revealing that while learning individual ideal items is no longer feasible, each user can contribute to learning the metric restricted to a low-dimensional subspace when comparisons are made over items that exhibit low-dimensional structure."
}