{
    "title": "Recovery Guarantees of Unsupervised Neural Networks for Inverse Problems trained with Gradient Descent",
    "abstract": "arXiv:2403.05395v1 Announce Type: new  Abstract: Advanced machine learning methods, and more prominently neural networks, have become standard to solve inverse problems over the last years. However, the theoretical recovery guarantees of such methods are still scarce and difficult to achieve. Only recently did unsupervised methods such as Deep Image Prior (DIP) get equipped with convergence and recovery guarantees for generic loss functions when trained through gradient flow with an appropriate initialization. In this paper, we extend these results by proving that these guarantees hold true when using gradient descent with an appropriately chosen step-size/learning rate. We also show that the discretization only affects the overparametrization bound for a two-layer DIP network by a constant and thus that the different guarantees found for the gradient flow will hold for gradient descent.",
    "link": "https://arxiv.org/abs/2403.05395",
    "context": "Title: Recovery Guarantees of Unsupervised Neural Networks for Inverse Problems trained with Gradient Descent\nAbstract: arXiv:2403.05395v1 Announce Type: new  Abstract: Advanced machine learning methods, and more prominently neural networks, have become standard to solve inverse problems over the last years. However, the theoretical recovery guarantees of such methods are still scarce and difficult to achieve. Only recently did unsupervised methods such as Deep Image Prior (DIP) get equipped with convergence and recovery guarantees for generic loss functions when trained through gradient flow with an appropriate initialization. In this paper, we extend these results by proving that these guarantees hold true when using gradient descent with an appropriately chosen step-size/learning rate. We also show that the discretization only affects the overparametrization bound for a two-layer DIP network by a constant and thus that the different guarantees found for the gradient flow will hold for gradient descent.",
    "path": "papers/24/03/2403.05395.json",
    "total_tokens": 757,
    "translated_title": "通过梯度下降训练的无监督神经网络在反问题中的恢复保证",
    "translated_abstract": "近年来，先进的机器学习方法，尤其是神经网络，已成为解决反问题的标准工具。然而，这些方法的理论恢复保证仍然稀缺且难以实现。最近，通过适当初始化使用梯度流训练的无监督方法（如Deep Image Prior，DIP）仅在通用损失函数上获得了收敛和恢复保证。本文通过证明，当选择适当的步长/学习率使用梯度下降时，这些保证仍然成立。我们还表明，离散化仅以常数影响两层DIP网络的过参数化界限，因此梯度下降所找到的不同保证也适用于梯度流。",
    "tldr": "本研究证明了使用梯度下降训练的无监督神经网络在反问题中的恢复保证与使用梯度流时的保证相同。",
    "en_tdlr": "This study establishes that unsupervised neural networks trained with gradient descent have the same recovery guarantees for inverse problems as those trained with gradient flow."
}