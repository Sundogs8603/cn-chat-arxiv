{
    "title": "Team Trifecta at Factify5WQA: Setting the Standard in Fact Verification with Fine-Tuning",
    "abstract": "arXiv:2403.10281v1 Announce Type: cross  Abstract: In this paper, we present Pre-CoFactv3, a comprehensive framework comprised of Question Answering and Text Classification components for fact verification. Leveraging In-Context Learning, Fine-tuned Large Language Models (LLMs), and the FakeNet model, we address the challenges of fact verification. Our experiments explore diverse approaches, comparing different Pre-trained LLMs, introducing FakeNet, and implementing various ensemble methods. Notably, our team, Trifecta, secured first place in the AAAI-24 Factify 3.0 Workshop, surpassing the baseline accuracy by 103% and maintaining a 70% lead over the second competitor. This success underscores the efficacy of our approach and its potential contributions to advancing fact verification research.",
    "link": "https://arxiv.org/abs/2403.10281",
    "context": "Title: Team Trifecta at Factify5WQA: Setting the Standard in Fact Verification with Fine-Tuning\nAbstract: arXiv:2403.10281v1 Announce Type: cross  Abstract: In this paper, we present Pre-CoFactv3, a comprehensive framework comprised of Question Answering and Text Classification components for fact verification. Leveraging In-Context Learning, Fine-tuned Large Language Models (LLMs), and the FakeNet model, we address the challenges of fact verification. Our experiments explore diverse approaches, comparing different Pre-trained LLMs, introducing FakeNet, and implementing various ensemble methods. Notably, our team, Trifecta, secured first place in the AAAI-24 Factify 3.0 Workshop, surpassing the baseline accuracy by 103% and maintaining a 70% lead over the second competitor. This success underscores the efficacy of our approach and its potential contributions to advancing fact verification research.",
    "path": "papers/24/03/2403.10281.json",
    "total_tokens": 810,
    "translated_title": "Team Trifecta在Factify 5WQA上设定了细化调整中事实验证的标准",
    "translated_abstract": "在本文中，我们介绍了Pre-CoFactv3，这是一个由问答和文本分类组件组成的全面框架，用于事实验证。通过利用上下文学习、微调大型语言模型（LLMs）和FakeNet模型，我们解决了事实验证面临的挑战。我们的实验探讨了不同的方法，比较了不同的预训练LLMs，引入了FakeNet，并实施了各种集成方法。值得注意的是，我们的团队Trifecta在AAAI-24 Factify 3.0研讨会上获得了第一名，比基准准确率高出103%，并保持了对第二名竞争对手的70%领先优势。这一成功突显了我们方法的有效性及其对推进事实验证研究的潜在贡献。",
    "tldr": "Team Trifecta在Factify 5WQA上以Fine-Tuning取得了首要地位，成功超越基准准确率103％，并保持了对第二名竞争者的70%领先优势。",
    "en_tdlr": "Team Trifecta set the standard in fact verification with fine-tuning at Factify 5WQA, surpassing the baseline accuracy by 103% and maintaining a 70% lead over the second competitor."
}