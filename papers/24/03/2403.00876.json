{
    "title": "Word Order and World Knowledge",
    "abstract": "arXiv:2403.00876v1 Announce Type: cross  Abstract: Word order is an important concept in natural language, and in this work, we study how word order affects the induction of world knowledge from raw text using language models. We use word analogies to probe for such knowledge. Specifically, in addition to the natural word order, we first respectively extract texts of six fixed word orders from five languages and then pretrain the language models on these texts. Finally, we analyze the experimental results of the fixed word orders on word analogies and show that i) certain fixed word orders consistently outperform or underperform others, though the specifics vary across languages, and ii) the Wov2Lex hypothesis is not hold in pre-trained language models, and the natural word order typically yields mediocre results. The source code will be made publicly available at https://github.com/lshowway/probing_by_analogy.",
    "link": "https://arxiv.org/abs/2403.00876",
    "context": "Title: Word Order and World Knowledge\nAbstract: arXiv:2403.00876v1 Announce Type: cross  Abstract: Word order is an important concept in natural language, and in this work, we study how word order affects the induction of world knowledge from raw text using language models. We use word analogies to probe for such knowledge. Specifically, in addition to the natural word order, we first respectively extract texts of six fixed word orders from five languages and then pretrain the language models on these texts. Finally, we analyze the experimental results of the fixed word orders on word analogies and show that i) certain fixed word orders consistently outperform or underperform others, though the specifics vary across languages, and ii) the Wov2Lex hypothesis is not hold in pre-trained language models, and the natural word order typically yields mediocre results. The source code will be made publicly available at https://github.com/lshowway/probing_by_analogy.",
    "path": "papers/24/03/2403.00876.json",
    "total_tokens": 837,
    "translated_title": "词序与世界知识",
    "translated_abstract": "词序是自然语言中的一个重要概念，在这项工作中，我们研究了词序如何影响使用语言模型从原始文本中归纳世界知识。我们使用词类比来探究这种知识。具体来说，除了自然词序外，我们分别从五种语言中提取了六种固定词序的文本，并在这些文本上对语言模型进行了预训练。最后，我们分析了固定词序在词类比上的实验结果，表明某些固定词序在不同语言中始终表现出色或不佳，尽管具体情况因语言而异，以及ii）Wov2Lex假设在预训练语言模型中不成立，自然词序通常产生平庸的结果。源代码将公开在 https://github.com/lshowway/probing_by_analogy。",
    "tldr": "本研究探讨了词序如何影响语言模型从原始文本中归纳世界知识，发现一些固定词序在不同语言中表现更好或更差，而预训练语言模型中的Wov2Lex假设不成立。",
    "en_tdlr": "This study explores how word order affects the induction of world knowledge from raw text using language models, revealing certain fixed word orders perform better or worse in different languages, and the Wov2Lex hypothesis does not hold in pre-trained language models."
}