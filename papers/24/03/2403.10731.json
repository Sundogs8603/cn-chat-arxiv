{
    "title": "Giving a Hand to Diffusion Models: a Two-Stage Approach to Improving Conditional Human Image Generation",
    "abstract": "arXiv:2403.10731v1 Announce Type: cross  Abstract: Recent years have seen significant progress in human image generation, particularly with the advancements in diffusion models. However, existing diffusion methods encounter challenges when producing consistent hand anatomy and the generated images often lack precise control over the hand pose. To address this limitation, we introduce a novel approach to pose-conditioned human image generation, dividing the process into two stages: hand generation and subsequent body out-painting around the hands. We propose training the hand generator in a multi-task setting to produce both hand images and their corresponding segmentation masks, and employ the trained model in the first stage of generation. An adapted ControlNet model is then used in the second stage to outpaint the body around the generated hands, producing the final result. A novel blending technique is introduced to preserve the hand details during the second stage that combines the",
    "link": "https://arxiv.org/abs/2403.10731",
    "context": "Title: Giving a Hand to Diffusion Models: a Two-Stage Approach to Improving Conditional Human Image Generation\nAbstract: arXiv:2403.10731v1 Announce Type: cross  Abstract: Recent years have seen significant progress in human image generation, particularly with the advancements in diffusion models. However, existing diffusion methods encounter challenges when producing consistent hand anatomy and the generated images often lack precise control over the hand pose. To address this limitation, we introduce a novel approach to pose-conditioned human image generation, dividing the process into two stages: hand generation and subsequent body out-painting around the hands. We propose training the hand generator in a multi-task setting to produce both hand images and their corresponding segmentation masks, and employ the trained model in the first stage of generation. An adapted ControlNet model is then used in the second stage to outpaint the body around the generated hands, producing the final result. A novel blending technique is introduced to preserve the hand details during the second stage that combines the",
    "path": "papers/24/03/2403.10731.json",
    "total_tokens": 867,
    "translated_title": "一种为扩散模型提供帮助的方法：改进条件人类图像生成的两阶段方法",
    "translated_abstract": "近年来，人类图像生成取得了显著进展，特别是在扩散模型的进步方面。然而，现有的扩散方法在生成一致的手部解剖结构时遇到挑战，并且生成的图像通常缺乏对手部姿势的精确控制。为了解决这一局限性，我们引入了一种新颖的方法来进行姿势条件的人类图像生成，将过程分为两个阶段：手的生成和随后围绕手部进行身体外部绘制。我们提出通过多任务设置训练手部生成器来生成手部图像及其对应的分割掩模，并在第一阶段中使用训练好的模型。然后在第二阶段使用适应的 ControlNet 模型来绘制周围的身体，生成最终结果。我们引入了一种新颖的混合技术，在第二阶段保留手部细节。",
    "tldr": "提出了一种改进条件人类图像生成的两阶段方法，首先训练手部生成器产生手部图像和分割掩模，在第二阶段使用改进的 ControlNet 模型绘制生成手部周围的身体。",
    "en_tdlr": "Proposed a two-stage approach to improving conditional human image generation, training the hand generator to produce hand images and segmentation masks in the first stage, and utilizing an adapted ControlNet model to outpaint the body around the generated hands in the second stage."
}