{
    "title": "MEP: Multiple Kernel Learning Enhancing Relative Positional Encoding Length Extrapolation",
    "abstract": "arXiv:2403.17698v1 Announce Type: cross  Abstract: When the predicted sequence length exceeds the length seen during training, the transformer's inference accuracy diminishes. Existing relative position encoding methods, such as those based on the ALiBi technique, address the length extrapolation challenge exclusively through the implementation of a single kernel function, which introduces a constant bias to every post-softmax attention scores according to their distance. These approaches do not investigate or employ multiple kernel functions to address the extrapolation challenge. Drawing on the ALiBi approach, this study proposes a novel relative positional encoding method, called MEP, which employs a weighted average to combine distinct kernel functions(such as the exponential kernel and the Gaussian kernel) to generate a bias that is applied to post-softmax attention scores. Initially, the framework utilizes various kernel functions to construct multiple kernel functions. Each kern",
    "link": "https://arxiv.org/abs/2403.17698",
    "context": "Title: MEP: Multiple Kernel Learning Enhancing Relative Positional Encoding Length Extrapolation\nAbstract: arXiv:2403.17698v1 Announce Type: cross  Abstract: When the predicted sequence length exceeds the length seen during training, the transformer's inference accuracy diminishes. Existing relative position encoding methods, such as those based on the ALiBi technique, address the length extrapolation challenge exclusively through the implementation of a single kernel function, which introduces a constant bias to every post-softmax attention scores according to their distance. These approaches do not investigate or employ multiple kernel functions to address the extrapolation challenge. Drawing on the ALiBi approach, this study proposes a novel relative positional encoding method, called MEP, which employs a weighted average to combine distinct kernel functions(such as the exponential kernel and the Gaussian kernel) to generate a bias that is applied to post-softmax attention scores. Initially, the framework utilizes various kernel functions to construct multiple kernel functions. Each kern",
    "path": "papers/24/03/2403.17698.json",
    "total_tokens": 762,
    "translated_title": "MEP: 多核学习增强相对位置编码长度外推",
    "translated_abstract": "当预测的序列长度超过训练中看到的长度时，变压器的推理准确性会降低。现有的相对位置编码方法，如基于ALiBi技术的方法，仅通过实现单个核函数来解决长度外推挑战，这会根据它们之间的距离为每个后Softmax注意力分数引入恒定偏差。这些方法未探讨或使用多个核函数来应对外推挑战。借鉴ALiBi方法，本研究提出了一种新颖的相对位置编码方法，称为MEP，它采用加权平均来结合不同的核函数（如指数核和高斯核）产生一个应用于后Softmax注意力分数的偏差。最初，该框架利用各种核函数构建多个核函数。每个核",
    "tldr": "提出了MEP方法，通过结合不同核函数生成偏差来解决变压器模型在长度外推时的准确性降低问题"
}