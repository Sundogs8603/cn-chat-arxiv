{
    "title": "Faster Convergence for Transformer Fine-tuning with Line Search Methods",
    "abstract": "arXiv:2403.18506v1 Announce Type: cross  Abstract: Recent works have shown that line search methods greatly increase performance of traditional stochastic gradient descent methods on a variety of datasets and architectures [1], [2]. In this work we succeed in extending line search methods to the novel and highly popular Transformer architecture and dataset domains in natural language processing. More specifically, we combine the Armijo line search with the Adam optimizer and extend it by subdividing the networks architecture into sensible units and perform the line search separately on these local units. Our optimization method outperforms the traditional Adam optimizer and achieves significant performance improvements for small data sets or small training budgets, while performing equal or better for other tested cases. Our work is publicly available as a python package, which provides a hyperparameter-free pytorch optimizer that is compatible with arbitrary network architectures.",
    "link": "https://arxiv.org/abs/2403.18506",
    "context": "Title: Faster Convergence for Transformer Fine-tuning with Line Search Methods\nAbstract: arXiv:2403.18506v1 Announce Type: cross  Abstract: Recent works have shown that line search methods greatly increase performance of traditional stochastic gradient descent methods on a variety of datasets and architectures [1], [2]. In this work we succeed in extending line search methods to the novel and highly popular Transformer architecture and dataset domains in natural language processing. More specifically, we combine the Armijo line search with the Adam optimizer and extend it by subdividing the networks architecture into sensible units and perform the line search separately on these local units. Our optimization method outperforms the traditional Adam optimizer and achieves significant performance improvements for small data sets or small training budgets, while performing equal or better for other tested cases. Our work is publicly available as a python package, which provides a hyperparameter-free pytorch optimizer that is compatible with arbitrary network architectures.",
    "path": "papers/24/03/2403.18506.json",
    "total_tokens": 755,
    "translated_title": "使用线搜索方法加速Transformer微调的收敛速度",
    "translated_abstract": "最近的研究表明，线搜索方法极大地提高了传统随机梯度下降方法在各种数据集和架构上的性能。在这项工作中，我们成功将线搜索方法扩展到了新颖且备受欢迎的Transformer架构和自然语言处理领域的数据集。具体来说，我们将Armijo线搜索与Adam优化器相结合，并通过将网络架构细分为合理的单元，在这些本地单元上分别执行线搜索。我们的优化方法优于传统的Adam优化器，在小数据集或小训练预算的情况下实现了显著的性能改进，同时在其他测试案例中表现相等或更好。我们的工作作为一个Python包公开可用，提供了一个无需超参数的PyTorch优化器，与任意网络架构兼容。",
    "tldr": "将Armijo线搜索与Adam优化器相结合，通过在本地单元执行线搜索，实现了在Transformer微调中收敛速度更快的优化方法。",
    "en_tdlr": "By combining Armijo line search with the Adam optimizer and performing line search on local units, we achieve faster convergence for Transformer fine-tuning."
}