{
    "title": "Beyond Single-Model Views for Deep Learning: Optimization versus Generalizability of Stochastic Optimization Algorithms",
    "abstract": "arXiv:2403.00574v1 Announce Type: new  Abstract: Despite an extensive body of literature on deep learning optimization, our current understanding of what makes an optimization algorithm effective is fragmented. In particular, we do not understand well whether enhanced optimization translates to improved generalizability. Current research overlooks the inherent stochastic nature of stochastic gradient descent (SGD) and its variants, resulting in a lack of comprehensive benchmarking and insight into their statistical performance. This paper aims to address this gap by adopting a novel approach. Rather than solely evaluating the endpoint of individual optimization trajectories, we draw from an ensemble of trajectories to estimate the stationary distribution of stochastic optimizers. Our investigation encompasses a wide array of techniques, including SGD and its variants, flat-minima optimizers, and new algorithms we propose under the Basin Hopping framework. Through our evaluation, which ",
    "link": "https://arxiv.org/abs/2403.00574",
    "context": "Title: Beyond Single-Model Views for Deep Learning: Optimization versus Generalizability of Stochastic Optimization Algorithms\nAbstract: arXiv:2403.00574v1 Announce Type: new  Abstract: Despite an extensive body of literature on deep learning optimization, our current understanding of what makes an optimization algorithm effective is fragmented. In particular, we do not understand well whether enhanced optimization translates to improved generalizability. Current research overlooks the inherent stochastic nature of stochastic gradient descent (SGD) and its variants, resulting in a lack of comprehensive benchmarking and insight into their statistical performance. This paper aims to address this gap by adopting a novel approach. Rather than solely evaluating the endpoint of individual optimization trajectories, we draw from an ensemble of trajectories to estimate the stationary distribution of stochastic optimizers. Our investigation encompasses a wide array of techniques, including SGD and its variants, flat-minima optimizers, and new algorithms we propose under the Basin Hopping framework. Through our evaluation, which ",
    "path": "papers/24/03/2403.00574.json",
    "total_tokens": 875,
    "translated_title": "深度学习的单模型观点的发展：优化与随机优化算法的泛化能力",
    "translated_abstract": "尽管关于深度学习优化的文献内容很丰富，但我们对于什么使优化算法有效的理解仍然零散。特别是，我们不太清楚增强的优化是否会转化为更好的泛化能力。目前的研究忽视了随机梯度下降（SGD）及其变体固有的随机性，导致缺乏全面的基准测试和对它们统计性能的洞察。本文旨在通过采用一种新颖的方法来填补这一差距。我们不仅仅评估单个优化轨迹的终点，而是从一系列轨迹中汲取，以估计随机优化器的稳态分布。我们的研究涵盖了广泛的技术，包括SGD及其变体、平坦最小值优化器以及我们在Basin Hopping框架下提出的新算法。通过我们的评估，我们发现...",
    "tldr": "本文研究通过采用新颖方法，从一系列轨迹中估计随机优化器的稳态分布，填补了深度学习优化中关于优化和泛化能力之间关系的理解空白。",
    "en_tdlr": "This paper addresses the gap in understanding the relationship between optimization and generalizability in deep learning by estimating the stationary distribution of stochastic optimizers from a series of trajectories using a novel approach."
}