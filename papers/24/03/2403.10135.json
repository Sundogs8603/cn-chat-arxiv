{
    "title": "The Whole is Better than the Sum: Using Aggregated Demonstrations in In-Context Learning for Sequential Recommendation",
    "abstract": "arXiv:2403.10135v1 Announce Type: cross  Abstract: Large language models (LLMs) have shown excellent performance on various NLP tasks. To use LLMs as strong sequential recommenders, we explore the in-context learning approach to sequential recommendation. We investigate the effects of instruction format, task consistency, demonstration selection, and number of demonstrations. As increasing the number of demonstrations in ICL does not improve accuracy despite using a long prompt, we propose a novel method called LLMSRec-Syn that incorporates multiple demonstration users into one aggregated demonstration. Our experiments on three recommendation datasets show that LLMSRec-Syn outperforms state-of-the-art LLM-based sequential recommendation methods. In some cases, LLMSRec-Syn can perform on par with or even better than supervised learning methods. Our code is publicly available at https://github.com/demoleiwang/LLMSRec_Syn.",
    "link": "https://arxiv.org/abs/2403.10135",
    "context": "Title: The Whole is Better than the Sum: Using Aggregated Demonstrations in In-Context Learning for Sequential Recommendation\nAbstract: arXiv:2403.10135v1 Announce Type: cross  Abstract: Large language models (LLMs) have shown excellent performance on various NLP tasks. To use LLMs as strong sequential recommenders, we explore the in-context learning approach to sequential recommendation. We investigate the effects of instruction format, task consistency, demonstration selection, and number of demonstrations. As increasing the number of demonstrations in ICL does not improve accuracy despite using a long prompt, we propose a novel method called LLMSRec-Syn that incorporates multiple demonstration users into one aggregated demonstration. Our experiments on three recommendation datasets show that LLMSRec-Syn outperforms state-of-the-art LLM-based sequential recommendation methods. In some cases, LLMSRec-Syn can perform on par with or even better than supervised learning methods. Our code is publicly available at https://github.com/demoleiwang/LLMSRec_Syn.",
    "path": "papers/24/03/2403.10135.json",
    "total_tokens": 810,
    "translated_title": "整体优于总和：在上下文学习中使用聚合演示进行顺序推荐",
    "translated_abstract": "大型语言模型（LLMs）在各种自然语言处理任务中展现出优秀性能。为了将LLMs作为强大的顺序推荐系统，我们探索了上下文学习方法用于顺序推荐。我们研究了指导格式、任务一致性、演示选择和演示数量对模型的影响。我们提出了一种新颖的方法LLMSRec-Syn，通过将多个演示用户整合成一个聚合演示来提高准确性。我们在三个推荐数据集上进行了实验证明，LLMSRec-Syn优于最先进的基于LLM的顺序推荐方法。在某些情况下，LLMSRec-Syn可以与甚至优于监督学习方法。我们的代码公开在https://github.com/demoleiwang/LLMSRec_Syn。",
    "tldr": "探索在顺序推荐中使用上下文学习的方法，提出了一种聚合演示的新颖方法LLMSRec-Syn，在多个数据集上实验证明其优于现有基于LLM的方法。"
}