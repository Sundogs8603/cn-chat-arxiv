{
    "title": "FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware Graph Transformer",
    "abstract": "arXiv:2403.12821v1 Announce Type: cross  Abstract: The success of a specific neural network architecture is closely tied to the dataset and task it tackles; there is no one-size-fits-all solution. Thus, considerable efforts have been made to quickly and accurately estimate the performances of neural architectures, without full training or evaluation, for given tasks and datasets. Neural architecture encoding has played a crucial role in the estimation, and graphbased methods, which treat an architecture as a graph, have shown prominent performance. For enhanced representation learning of neural architectures, we introduce FlowerFormer, a powerful graph transformer that incorporates the information flows within a neural architecture. FlowerFormer consists of two key components: (a) bidirectional asynchronous message passing, inspired by the flows; (b) global attention built on flow-based masking. Our extensive experiments demonstrate the superiority of FlowerFormer over existing neural ",
    "link": "https://arxiv.org/abs/2403.12821",
    "context": "Title: FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware Graph Transformer\nAbstract: arXiv:2403.12821v1 Announce Type: cross  Abstract: The success of a specific neural network architecture is closely tied to the dataset and task it tackles; there is no one-size-fits-all solution. Thus, considerable efforts have been made to quickly and accurately estimate the performances of neural architectures, without full training or evaluation, for given tasks and datasets. Neural architecture encoding has played a crucial role in the estimation, and graphbased methods, which treat an architecture as a graph, have shown prominent performance. For enhanced representation learning of neural architectures, we introduce FlowerFormer, a powerful graph transformer that incorporates the information flows within a neural architecture. FlowerFormer consists of two key components: (a) bidirectional asynchronous message passing, inspired by the flows; (b) global attention built on flow-based masking. Our extensive experiments demonstrate the superiority of FlowerFormer over existing neural ",
    "path": "papers/24/03/2403.12821.json",
    "total_tokens": 803,
    "translated_title": "FlowerFormer: 使用基于流感知的图变换器增强神经结构编码",
    "translated_abstract": "特定神经网络架构的成功与其处理的数据集和任务密切相关；没有一种适合所有情况的解决方案。因此，人们付出了大量努力，以快速准确地估计神经结构在特定任务和数据集上的表现，而无需进行完整的训练或评估。神经结构编码在估计中起着至关重要的作用，而将架构视为图的基于图的方法表现出色。为了增强神经结构的表征学习，我们介绍了FlowerFormer，一种强大的图变换器，它融入了神经结构内的信息流。 FlowerFormer由两个关键组件组成：（a）受流程启发的双向异步消息传递；（b）建立在基于流程的掩码上的全局关注。我们广泛的实验表明，FlowerFormer优于现有神经结构。",
    "tldr": "FlowerFormer是一种强大的图变换器，通过双向异步消息传递和基于流程的全局注意力，可以增强神经结构的表征学习。",
    "en_tdlr": "FlowerFormer is a powerful graph transformer that enhances representation learning of neural architectures through bidirectional asynchronous message passing and flow-based global attention."
}