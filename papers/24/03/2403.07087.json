{
    "title": "LSTM-Based Text Generation: A Study on Historical Datasets",
    "abstract": "arXiv:2403.07087v1 Announce Type: cross  Abstract: This paper presents an exploration of Long Short-Term Memory (LSTM) networks in the realm of text generation, focusing on the utilization of historical datasets for Shakespeare and Nietzsche. LSTMs, known for their effectiveness in handling sequential data, are applied here to model complex language patterns and structures inherent in historical texts. The study demonstrates that LSTM-based models, when trained on historical datasets, can not only generate text that is linguistically rich and contextually relevant but also provide insights into the evolution of language patterns over time. The finding presents models that are highly accurate and efficient in predicting text from works of Nietzsche, with low loss values and a training time of 100 iterations. The accuracy of the model is 0.9521, indicating high accuracy. The loss of the model is 0.2518, indicating its effectiveness. The accuracy of the model in predicting text from the w",
    "link": "https://arxiv.org/abs/2403.07087",
    "context": "Title: LSTM-Based Text Generation: A Study on Historical Datasets\nAbstract: arXiv:2403.07087v1 Announce Type: cross  Abstract: This paper presents an exploration of Long Short-Term Memory (LSTM) networks in the realm of text generation, focusing on the utilization of historical datasets for Shakespeare and Nietzsche. LSTMs, known for their effectiveness in handling sequential data, are applied here to model complex language patterns and structures inherent in historical texts. The study demonstrates that LSTM-based models, when trained on historical datasets, can not only generate text that is linguistically rich and contextually relevant but also provide insights into the evolution of language patterns over time. The finding presents models that are highly accurate and efficient in predicting text from works of Nietzsche, with low loss values and a training time of 100 iterations. The accuracy of the model is 0.9521, indicating high accuracy. The loss of the model is 0.2518, indicating its effectiveness. The accuracy of the model in predicting text from the w",
    "path": "papers/24/03/2403.07087.json",
    "total_tokens": 842,
    "translated_title": "基于LSTM的文本生成：对历史数据集的研究",
    "translated_abstract": "本文探讨了长短期记忆（LSTM）网络在文本生成领域的应用，重点在于利用莎士比亚和尼采的历史数据集。LSTMs以其处理序列数据的有效性而闻名，这里应用它们来建模历史文本中固有的复杂语言模式和结构。研究表明，基于历史数据集训练的LSTM模型不仅能够生成语言丰富且语境相关的文本，还能提供关于语言模式随时间演变的见解。研究结果展示了基于LSTM的模型在预测尼采作品文本时的高准确性和高效率，具有较低的损失值和100次迭代的训练时间。模型的准确性为0.9521，表明其高准确性。模型的损失为0.2518，显示其有效性。",
    "tldr": "本研究探讨了基于LSTM的文本生成在历史数据集上的应用，展示了这些模型在生成语言丰富、语境相关的文本方面的高准确性和高效率。",
    "en_tdlr": "This study explores the application of LSTM-based text generation on historical datasets, demonstrating the high accuracy and efficiency of these models in generating linguistically rich and contextually relevant text."
}