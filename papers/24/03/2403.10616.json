{
    "title": "DiPaCo: Distributed Path Composition",
    "abstract": "arXiv:2403.10616v1 Announce Type: cross  Abstract: Progress in machine learning (ML) has been fueled by scaling neural network models. This scaling has been enabled by ever more heroic feats of engineering, necessary for accommodating ML approaches that require high bandwidth communication between devices working in parallel. In this work, we propose a co-designed modular architecture and training approach for ML models, dubbed DIstributed PAth COmposition (DiPaCo). During training, DiPaCo distributes computation by paths through a set of shared modules. Together with a Local-SGD inspired optimization (DiLoCo) that keeps modules in sync with drastically reduced communication, Our approach facilitates training across poorly connected and heterogeneous workers, with a design that ensures robustness to worker failures and preemptions. At inference time, only a single path needs to be executed for each input, without the need for any model compression. We consider this approach as a first ",
    "link": "https://arxiv.org/abs/2403.10616",
    "context": "Title: DiPaCo: Distributed Path Composition\nAbstract: arXiv:2403.10616v1 Announce Type: cross  Abstract: Progress in machine learning (ML) has been fueled by scaling neural network models. This scaling has been enabled by ever more heroic feats of engineering, necessary for accommodating ML approaches that require high bandwidth communication between devices working in parallel. In this work, we propose a co-designed modular architecture and training approach for ML models, dubbed DIstributed PAth COmposition (DiPaCo). During training, DiPaCo distributes computation by paths through a set of shared modules. Together with a Local-SGD inspired optimization (DiLoCo) that keeps modules in sync with drastically reduced communication, Our approach facilitates training across poorly connected and heterogeneous workers, with a design that ensures robustness to worker failures and preemptions. At inference time, only a single path needs to be executed for each input, without the need for any model compression. We consider this approach as a first ",
    "path": "papers/24/03/2403.10616.json",
    "total_tokens": 822,
    "translated_title": "DiPaCo: 分布式路径组合",
    "translated_abstract": "机器学习（ML）领域的进展得益于扩展神经网络模型。这种扩展是通过不断壮举的工程努力实现的，以适应需要设备之间高带宽通信的并行ML方法。在这项工作中，我们提出了一个为ML模型设计的协同模块化架构和训练方法，称为DIstributed PAth COmposition（DiPaCo）。在训练过程中，DiPaCo通过一组共享模块的路径进行计算分发。结合一种受Local-SGD启发的优化方法（DiLoCo），该方法通过大幅减少通信来确保模块同步，促进了跨连接不佳且异构的工作节点的训练，其设计确保了对工作节点故障和抢占的稳健性。在推断时，每个输入只需要执行一条路径，无需任何模型压缩。我们认为这是一种首创性的方法。",
    "tldr": "DiPaCo提出了一种协同模块化架构和训练方法，可以通过路径分发计算，实现机器学习模型的训练，并在推断时无需模型压缩。",
    "en_tdlr": "DiPaCo proposes a co-designed modular architecture and training approach that distributes computation by paths, facilitating training of machine learning models without the need for model compression during inference."
}