{
    "title": "Monotonic Paraphrasing Improves Generalization of Language Model Prompting",
    "abstract": "arXiv:2403.16038v1 Announce Type: new  Abstract: Performance of large language models (LLMs) may vary with different prompts or instructions of even the same task. One commonly recognized factor for this phenomenon is the model's familiarity with the given prompt or instruction, which is typically estimated by its perplexity. However, finding the prompt with the lowest perplexity is challenging, given the enormous space of possible prompting phrases. In this paper, we propose monotonic paraphrasing (MonoPara), an end-to-end decoding strategy that paraphrases given prompts or instructions into their lower perplexity counterparts based on an ensemble of a paraphrase LM for prompt (or instruction) rewriting, and a target LM (i.e. the prompt or instruction executor) that constrains the generation for lower perplexity. The ensemble decoding process can efficiently paraphrase the original prompt without altering its semantic meaning, while monotonically decreasing the perplexity of each gene",
    "link": "https://arxiv.org/abs/2403.16038",
    "context": "Title: Monotonic Paraphrasing Improves Generalization of Language Model Prompting\nAbstract: arXiv:2403.16038v1 Announce Type: new  Abstract: Performance of large language models (LLMs) may vary with different prompts or instructions of even the same task. One commonly recognized factor for this phenomenon is the model's familiarity with the given prompt or instruction, which is typically estimated by its perplexity. However, finding the prompt with the lowest perplexity is challenging, given the enormous space of possible prompting phrases. In this paper, we propose monotonic paraphrasing (MonoPara), an end-to-end decoding strategy that paraphrases given prompts or instructions into their lower perplexity counterparts based on an ensemble of a paraphrase LM for prompt (or instruction) rewriting, and a target LM (i.e. the prompt or instruction executor) that constrains the generation for lower perplexity. The ensemble decoding process can efficiently paraphrase the original prompt without altering its semantic meaning, while monotonically decreasing the perplexity of each gene",
    "path": "papers/24/03/2403.16038.json",
    "total_tokens": 807,
    "translated_title": "单调释义提高语言模型提示的泛化能力",
    "translated_abstract": "大型语言模型（LLMs）的表现可能会随着同一任务的不同提示或指令而变化。这种现象的一个公认因素是模型对给定提示或指令的熟悉程度，通常通过其困惑度来估计。然而，鉴于可能提示短语的巨大空间，找到困惑度最低的提示是具有挑战性的。在本文中，我们提出了单调释义（MonoPara），一种端到端解码策略，根据释义LM和目标LM（即提示或指令执行器）的集合来将给定提示或指令释义化为其低困惑度的对应物。集合解码过程可以有效地释义原始提示而不改变其语义含义，同时单调地降低每个生成物的困惑度。",
    "tldr": "提出了单调释义（MonoPara）方法，通过释义LM和目标LM集成解码过程，将提示或指令释义为低困惑度的版本，从而提高语言模型的泛化能力",
    "en_tdlr": "Proposed monotonic paraphrasing (MonoPara) method enhances the generalization of language models by paraphrasing prompts or instructions into lower perplexity versions through an ensemble decoding process of a paraphrase LM and a target LM."
}