{
    "title": "Offline Imitation Learning from Multiple Baselines with Applications to Compiler Optimization",
    "abstract": "arXiv:2403.19462v1 Announce Type: new  Abstract: This work studies a Reinforcement Learning (RL) problem in which we are given a set of trajectories collected with K baseline policies. Each of these policies can be quite suboptimal in isolation, and have strong performance in complementary parts of the state space. The goal is to learn a policy which performs as well as the best combination of baselines on the entire state space. We propose a simple imitation learning based algorithm, show a sample complexity bound on its accuracy and prove that the the algorithm is minimax optimal by showing a matching lower bound. Further, we apply the algorithm in the setting of machine learning guided compiler optimization to learn policies for inlining programs with the objective of creating a small binary. We demonstrate that we can learn a policy that outperforms an initial policy learned via standard RL through a few iterations of our approach.",
    "link": "https://arxiv.org/abs/2403.19462",
    "context": "Title: Offline Imitation Learning from Multiple Baselines with Applications to Compiler Optimization\nAbstract: arXiv:2403.19462v1 Announce Type: new  Abstract: This work studies a Reinforcement Learning (RL) problem in which we are given a set of trajectories collected with K baseline policies. Each of these policies can be quite suboptimal in isolation, and have strong performance in complementary parts of the state space. The goal is to learn a policy which performs as well as the best combination of baselines on the entire state space. We propose a simple imitation learning based algorithm, show a sample complexity bound on its accuracy and prove that the the algorithm is minimax optimal by showing a matching lower bound. Further, we apply the algorithm in the setting of machine learning guided compiler optimization to learn policies for inlining programs with the objective of creating a small binary. We demonstrate that we can learn a policy that outperforms an initial policy learned via standard RL through a few iterations of our approach.",
    "path": "papers/24/03/2403.19462.json",
    "total_tokens": 837,
    "translated_title": "离线多基线模仿学习及其在编译器优化中的应用",
    "translated_abstract": "这项工作研究了一个强化学习（RL）问题，我们在其中给定了使用K个基线策略收集的一组轨迹。这些策略中的每一个在单独的情况下可能相当次优，并且在状态空间的互补部分表现出较强的性能。旨在学习一个策略，使其在整个状态空间上的表现与最佳组合基线相当。我们提出了一种基于简单模仿学习的算法，证明了其准确性的样本复杂度界限，并通过展示一种匹配的下界证明了算法的极小-极大最优性。此外，在机器学习引导编译器优化的设置中应用该算法，学习用于内联程序的策略，目标是创建一个小的二进制文件。我们证明可以通过我们的方法的几次迭代学习到一个优于通过标准RL学到的初始策略的策略。",
    "tldr": "通过离线多基线模仿学习算法，在编译器优化领域实现了超越标准RL的策略学习效果",
    "en_tdlr": "Aiming at the problem of Reinforcement Learning with multiple baselines, this work introduces an offline imitation learning algorithm and demonstrates its superior performance in machine learning guided compiler optimization over traditional RL methods."
}