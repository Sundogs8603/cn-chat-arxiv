{
    "title": "Learning Physical Dynamics for Object-centric Visual Prediction",
    "abstract": "arXiv:2403.10079v1 Announce Type: cross  Abstract: The ability to model the underlying dynamics of visual scenes and reason about the future is central to human intelligence. Many attempts have been made to empower intelligent systems with such physical understanding and prediction abilities. However, most existing methods focus on pixel-to-pixel prediction, which suffers from heavy computational costs while lacking a deep understanding of the physical dynamics behind videos. Recently, object-centric prediction methods have emerged and attracted increasing interest. Inspired by it, this paper proposes an unsupervised object-centric prediction model that makes future predictions by learning visual dynamics between objects. Our model consists of two modules, perceptual, and dynamic module. The perceptual module is utilized to decompose images into several objects and synthesize images with a set of object-centric representations. The dynamic module fuses contextual information, takes env",
    "link": "https://arxiv.org/abs/2403.10079",
    "context": "Title: Learning Physical Dynamics for Object-centric Visual Prediction\nAbstract: arXiv:2403.10079v1 Announce Type: cross  Abstract: The ability to model the underlying dynamics of visual scenes and reason about the future is central to human intelligence. Many attempts have been made to empower intelligent systems with such physical understanding and prediction abilities. However, most existing methods focus on pixel-to-pixel prediction, which suffers from heavy computational costs while lacking a deep understanding of the physical dynamics behind videos. Recently, object-centric prediction methods have emerged and attracted increasing interest. Inspired by it, this paper proposes an unsupervised object-centric prediction model that makes future predictions by learning visual dynamics between objects. Our model consists of two modules, perceptual, and dynamic module. The perceptual module is utilized to decompose images into several objects and synthesize images with a set of object-centric representations. The dynamic module fuses contextual information, takes env",
    "path": "papers/24/03/2403.10079.json",
    "total_tokens": 860,
    "translated_title": "学习面向物体的视觉预测中的物理动力学",
    "translated_abstract": "arXiv:2403.10079v1 公告类型: 跨领域 摘要: 模拟视觉场景的潜在动力学并推断未来情况对于人类智能至关重要。许多尝试赋予智能系统这种物理理解和预测能力。然而，大多数现有方法侧重于像素到像素的预测，这种方法计算成本高昂，同时缺乏对视频背后物理动力学的深入理解。最近，出现了面向物体的预测方法，并引起了越来越多的关注。受此启发，本文提出了一种无监督的面向物体的预测模型，通过学习物体之间的视觉动力学进行未来预测。我们的模型由两个模块组成，感知模块和动力模块。感知模块用于将图像分解为多个物体，并使用一组面向物体的表示合成图像。动力模块融合了上下文信息，进行环境建模。",
    "tldr": "本文提出了一种学习面向物体的视觉预测中的物理动力学的模型，通过学习物体之间的视觉动力学进行未来预测，具有两个关键模块：感知模块和动力模块。",
    "en_tdlr": "This paper proposes a model for learning physical dynamics in object-centric visual prediction, making future predictions by learning the visual dynamics between objects, with two key modules: perceptual module and dynamic module."
}