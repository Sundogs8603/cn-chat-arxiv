{
    "title": "LITA: Language Instructed Temporal-Localization Assistant",
    "abstract": "arXiv:2403.19046v1 Announce Type: cross  Abstract: There has been tremendous progress in multimodal Large Language Models (LLMs). Recent works have extended these models to video input with promising instruction following capabilities. However, an important missing piece is temporal localization. These models cannot accurately answer the \"When?\" questions. We identify three key aspects that limit their temporal localization capabilities: (i) time representation, (ii) architecture, and (iii) data. We address these shortcomings by proposing Language Instructed Temporal-Localization Assistant (LITA) with the following features: (1) We introduce time tokens that encode timestamps relative to the video length to better represent time in videos. (2) We introduce SlowFast tokens in the architecture to capture temporal information at fine temporal resolution. (3) We emphasize temporal localization data for LITA. In addition to leveraging existing video datasets with timestamps, we propose a ne",
    "link": "https://arxiv.org/abs/2403.19046",
    "context": "Title: LITA: Language Instructed Temporal-Localization Assistant\nAbstract: arXiv:2403.19046v1 Announce Type: cross  Abstract: There has been tremendous progress in multimodal Large Language Models (LLMs). Recent works have extended these models to video input with promising instruction following capabilities. However, an important missing piece is temporal localization. These models cannot accurately answer the \"When?\" questions. We identify three key aspects that limit their temporal localization capabilities: (i) time representation, (ii) architecture, and (iii) data. We address these shortcomings by proposing Language Instructed Temporal-Localization Assistant (LITA) with the following features: (1) We introduce time tokens that encode timestamps relative to the video length to better represent time in videos. (2) We introduce SlowFast tokens in the architecture to capture temporal information at fine temporal resolution. (3) We emphasize temporal localization data for LITA. In addition to leveraging existing video datasets with timestamps, we propose a ne",
    "path": "papers/24/03/2403.19046.json",
    "total_tokens": 828,
    "translated_title": "LITA: 语言指导的时间定位助手",
    "translated_abstract": "多模态大语言模型（LLMs）取得了巨大进展。最近的研究将这些模型扩展到视频输入，并具有有前途的指令跟随能力。然而，一项重要缺失的部分是时间定位。这些模型无法准确回答“什么时候？”的问题。我们确定了三个限制它们时间定位能力的关键方面：（i）时间表示，（ii）架构和（iii）数据。我们通过提出具有以下功能的语言指导的时间定位助手（LITA）来解决这些缺点: (1) 我们引入了时间令牌，用于编码相对于视频长度的时间戳，以更好地表示视频中的时间。(2) 我们在架构中引入了SlowFast令牌，以捕获细粒度的时间信息。(3) 我们强调LITA的时间定位数据。除了利用具有时间戳的现有视频数据集外，我们还提出一个新的",
    "tldr": "提出了一种名为LITA的语言指导的时间定位助手，通过引入时间令牌、SlowFast令牌和强调时间定位数据来改善视频中的时间定位能力",
    "en_tdlr": "Introduced a language instructed temporal-localization assistant called LITA to enhance temporal localization in videos by introducing time tokens, SlowFast tokens, and emphasizing temporal localization data."
}