{
    "title": "A Novel Loss Function-based Support Vector Machine for Binary Classification",
    "abstract": "arXiv:2403.16654v1 Announce Type: new  Abstract: The previous support vector machine(SVM) including $0/1$ loss SVM, hinge loss SVM, ramp loss SVM, truncated pinball loss SVM, and others, overlooked the degree of penalty for the correctly classified samples within the margin. This oversight affects the generalization ability of the SVM classifier to some extent. To address this limitation, from the perspective of confidence margin, we propose a novel Slide loss function ($\\ell_s$) to construct the support vector machine classifier($\\ell_s$-SVM). By introducing the concept of proximal stationary point, and utilizing the property of Lipschitz continuity, we derive the first-order optimality conditions for $\\ell_s$-SVM. Based on this, we define the $\\ell_s$ support vectors and working set of $\\ell_s$-SVM. To efficiently handle $\\ell_s$-SVM, we devise a fast alternating direction method of multipliers with the working set ($\\ell_s$-ADMM), and provide the convergence analysis. The numerical ",
    "link": "https://arxiv.org/abs/2403.16654",
    "context": "Title: A Novel Loss Function-based Support Vector Machine for Binary Classification\nAbstract: arXiv:2403.16654v1 Announce Type: new  Abstract: The previous support vector machine(SVM) including $0/1$ loss SVM, hinge loss SVM, ramp loss SVM, truncated pinball loss SVM, and others, overlooked the degree of penalty for the correctly classified samples within the margin. This oversight affects the generalization ability of the SVM classifier to some extent. To address this limitation, from the perspective of confidence margin, we propose a novel Slide loss function ($\\ell_s$) to construct the support vector machine classifier($\\ell_s$-SVM). By introducing the concept of proximal stationary point, and utilizing the property of Lipschitz continuity, we derive the first-order optimality conditions for $\\ell_s$-SVM. Based on this, we define the $\\ell_s$ support vectors and working set of $\\ell_s$-SVM. To efficiently handle $\\ell_s$-SVM, we devise a fast alternating direction method of multipliers with the working set ($\\ell_s$-ADMM), and provide the convergence analysis. The numerical ",
    "path": "papers/24/03/2403.16654.json",
    "total_tokens": 918,
    "translated_title": "一种基于新型损失函数的二分类支持向量机",
    "translated_abstract": "先前的支持向量机（SVM）包括0/1损失SVM、铰链损失SVM、坡度损失SVM、截断针垫损失SVM等，在正确分类的样本在边际内的惩罚程度上存在一定的疏忽。这种疏忽在一定程度上影响了SVM分类器的泛化能力。为了解决这一限制，我们从置信边际的角度提出了一种新型Slide损失函数（$\\ell_s$）来构建支持向量机分类器（$\\ell_s$-SVM）。通过引入近端稳定点的概念，并利用Lipschitz连续性的属性，推导出了$\\ell_s$-SVM的一阶最优性条件。基于此，我们定义了$\\ell_s$支持向量和$\\ell_s$-SVM的工作集。为了高效处理$\\ell_s$-SVM，我们设计了一个具有工作集的快速交替方向乘法器方法（$\\ell_s$-ADMM），并提供了收敛性分析。",
    "tldr": "通过引入新型Slide损失函数（$\\ell_s$），我们提出了一种$\\ell_s$-SVM分类器，有效地解决了传统SVM在边缘正确分类样本惩罚程度方面的不足，提升了泛化能力。",
    "en_tdlr": "By introducing the novel Slide loss function ($\\ell_s$), we propose an $\\ell_s$-SVM classifier, effectively addressing the limitation of traditional SVM in penalizing correctly classified samples within the margin and enhancing generalization ability."
}