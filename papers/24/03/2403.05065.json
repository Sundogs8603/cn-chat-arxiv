{
    "title": "Can we obtain significant success in RST discourse parsing by using Large Language Models?",
    "abstract": "arXiv:2403.05065v1 Announce Type: new  Abstract: Recently, decoder-only pre-trained large language models (LLMs), with several tens of billion parameters, have significantly impacted a wide range of natural language processing (NLP) tasks. While encoder-only or encoder-decoder pre-trained language models have already proved to be effective in discourse parsing, the extent to which LLMs can perform this task remains an open research question. Therefore, this paper explores how beneficial such LLMs are for Rhetorical Structure Theory (RST) discourse parsing. Here, the parsing process for both fundamental top-down and bottom-up strategies is converted into prompts, which LLMs can work with. We employ Llama 2 and fine-tune it with QLoRA, which has fewer parameters that can be tuned. Experimental results on three benchmark datasets, RST-DT, Instr-DT, and the GUM corpus, demonstrate that Llama 2 with 70 billion parameters in the bottom-up strategy obtained state-of-the-art (SOTA) results wit",
    "link": "https://arxiv.org/abs/2403.05065",
    "context": "Title: Can we obtain significant success in RST discourse parsing by using Large Language Models?\nAbstract: arXiv:2403.05065v1 Announce Type: new  Abstract: Recently, decoder-only pre-trained large language models (LLMs), with several tens of billion parameters, have significantly impacted a wide range of natural language processing (NLP) tasks. While encoder-only or encoder-decoder pre-trained language models have already proved to be effective in discourse parsing, the extent to which LLMs can perform this task remains an open research question. Therefore, this paper explores how beneficial such LLMs are for Rhetorical Structure Theory (RST) discourse parsing. Here, the parsing process for both fundamental top-down and bottom-up strategies is converted into prompts, which LLMs can work with. We employ Llama 2 and fine-tune it with QLoRA, which has fewer parameters that can be tuned. Experimental results on three benchmark datasets, RST-DT, Instr-DT, and the GUM corpus, demonstrate that Llama 2 with 70 billion parameters in the bottom-up strategy obtained state-of-the-art (SOTA) results wit",
    "path": "papers/24/03/2403.05065.json",
    "total_tokens": 850,
    "translated_title": "使用大型语言模型在RST语篇解析中能否取得显著成功？",
    "translated_abstract": "最近，只有解码器的预训练大型语言模型（LLMs）对各种自然语言处理（NLP）任务产生了显著影响。虽然已经证明仅编码器或编码器-解码器预训练语言模型在语篇解析中是有效的，但LLMs能够执行这项任务的程度仍然是一个开放的研究问题。因此，本文探讨了LLMs对修辞结构理论（RST）语篇解析的益处。在这里，基本自顶向下和自底向上策略的解析过程被转换为LLMs可以使用的提示。我们使用Llama 2，并用QLoRA进行微调，后者具有可以调节的更少参数。对RST-DT、Instr-DT和GUM语料库的三个基准数据集的实验结果表明，底部策略中Llama 2的70亿参数获得了最新成果。",
    "tldr": "研究探索了如何利用大型语言模型（LLMs）来进行RST语篇解析，并在底部策略中取得了最新的最先进结果。",
    "en_tdlr": "This paper explores how to use Large Language Models (LLMs) for RST discourse parsing and achieves state-of-the-art results in the bottom-up strategy."
}