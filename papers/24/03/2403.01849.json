{
    "title": "One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models",
    "abstract": "arXiv:2403.01849v1 Announce Type: cross  Abstract: Large pre-trained Vision-Language Models (VLMs) like CLIP, despite having remarkable generalization ability, are highly vulnerable to adversarial examples. This work studies the adversarial robustness of VLMs from the novel perspective of the text prompt instead of the extensively studied model weights (frozen in this work). We first show that the effectiveness of both adversarial attack and defense are sensitive to the used text prompt. Inspired by this, we propose a method to improve resilience to adversarial attacks by learning a robust text prompt for VLMs. The proposed method, named Adversarial Prompt Tuning (APT), is effective while being both computationally and data efficient. Extensive experiments are conducted across 15 datasets and 4 data sparsity schemes (from 1-shot to full training data settings) to show APT's superiority over hand-engineered prompts and other state-of-the-art adaption methods. APT demonstrated excellent ",
    "link": "https://arxiv.org/abs/2403.01849",
    "context": "Title: One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models\nAbstract: arXiv:2403.01849v1 Announce Type: cross  Abstract: Large pre-trained Vision-Language Models (VLMs) like CLIP, despite having remarkable generalization ability, are highly vulnerable to adversarial examples. This work studies the adversarial robustness of VLMs from the novel perspective of the text prompt instead of the extensively studied model weights (frozen in this work). We first show that the effectiveness of both adversarial attack and defense are sensitive to the used text prompt. Inspired by this, we propose a method to improve resilience to adversarial attacks by learning a robust text prompt for VLMs. The proposed method, named Adversarial Prompt Tuning (APT), is effective while being both computationally and data efficient. Extensive experiments are conducted across 15 datasets and 4 data sparsity schemes (from 1-shot to full training data settings) to show APT's superiority over hand-engineered prompts and other state-of-the-art adaption methods. APT demonstrated excellent ",
    "path": "papers/24/03/2403.01849.json",
    "total_tokens": 947,
    "translated_title": "一个提示词就足以提升预训练视觉-语言模型的对抗鲁棒性",
    "translated_abstract": "像CLIP这样的大型预训练视觉-语言模型（VLMs），尽管具有显着的泛化能力，但对于对抗样本非常脆弱。本文从文本提示的新颖视角而非传统研究的模型权重（在本文中冻结）研究了VLMs的对抗鲁棒性。我们首先展示了对抗攻击和防御的有效性对使用的文本提示敏感。在此启发下，我们提出了一种通过学习VLMs的强韧文本提示来提高对抗攻击韧性的方法。提出的方法，称为Adversarial Prompt Tuning（APT），在计算和数据效率方面都非常有效。通过在15个数据集和4种数据稀疏方案（从1-shot到完全训练数据设置）上进行了大量实验，以展示APT相对于手工设计提示和其他最先进的适应方法的优越性。APT表现出色",
    "tldr": "本文研究了预训练视觉-语言模型的对抗鲁棒性，提出了一种通过学习强韧文本提示来改善对抗攻击韧性的方法，称为Adversarial Prompt Tuning（APT），并在多个数据集和数据稀疏方案上进行了全面实验验证。",
    "en_tdlr": "This paper studies the adversarial robustness of pre-trained Vision-Language Models (VLMs), proposes a method named Adversarial Prompt Tuning (APT) to improve resilience to adversarial attacks by learning a robust text prompt, and conducts extensive experiments across multiple datasets and data sparsity schemes."
}