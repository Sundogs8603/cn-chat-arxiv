{
    "title": "Measuring Political Bias in Large Language Models: What Is Said and How It Is Said",
    "abstract": "arXiv:2403.18932v1 Announce Type: cross  Abstract: We propose to measure political bias in LLMs by analyzing both the content and style of their generated content regarding political issues. Existing benchmarks and measures focus on gender and racial biases. However, political bias exists in LLMs and can lead to polarization and other harms in downstream applications. In order to provide transparency to users, we advocate that there should be fine-grained and explainable measures of political biases generated by LLMs. Our proposed measure looks at different political issues such as reproductive rights and climate change, at both the content (the substance of the generation) and the style (the lexical polarity) of such bias. We measured the political bias in eleven open-sourced LLMs and showed that our proposed framework is easily scalable to other topics and is explainable.",
    "link": "https://arxiv.org/abs/2403.18932",
    "context": "Title: Measuring Political Bias in Large Language Models: What Is Said and How It Is Said\nAbstract: arXiv:2403.18932v1 Announce Type: cross  Abstract: We propose to measure political bias in LLMs by analyzing both the content and style of their generated content regarding political issues. Existing benchmarks and measures focus on gender and racial biases. However, political bias exists in LLMs and can lead to polarization and other harms in downstream applications. In order to provide transparency to users, we advocate that there should be fine-grained and explainable measures of political biases generated by LLMs. Our proposed measure looks at different political issues such as reproductive rights and climate change, at both the content (the substance of the generation) and the style (the lexical polarity) of such bias. We measured the political bias in eleven open-sourced LLMs and showed that our proposed framework is easily scalable to other topics and is explainable.",
    "path": "papers/24/03/2403.18932.json",
    "total_tokens": 825,
    "translated_title": "在大型语言模型中测量政治偏见：言论内容和表达方式分析",
    "translated_abstract": "我们提出通过分析大型语言模型生成的政治议题内容和风格来测量其政治偏见。现有的基准和测量方法关注性别和种族偏见，但是大型语言模型中存在政治偏见，可能导致下游应用中的极化和其他危害。为了向用户提供透明度，我们主张应该有由大型语言模型生成的政治偏见的细粒度和可解释性测量。我们提出的测量方法既考虑了生殖权利和气候变化等不同政治议题的内容（生成物的实质），也考虑了这种偏见的风格（词汇的极性）。我们测量了十一个开源大型语言模型的政治偏见，并展示了我们提出的框架在扩展到其他主题时既易于扩展又具有可解释性。",
    "tldr": "提出通过分析大型语言模型生成的政治议题内容和风格来测量其政治偏见，主张应该有由大型语言模型生成的政治偏见的细粒度和可解释性衡量。",
    "en_tdlr": "Propose to measure political bias in large language models by analyzing the content and style of their generated content regarding political issues, advocating for fine-grained and explainable measures of political biases generated by large language models."
}