{
    "title": "Pre-Trained Language Models Represent Some Geographic Populations Better Than Others",
    "abstract": "arXiv:2403.11025v1 Announce Type: new  Abstract: This paper measures the skew in how well two families of LLMs represent diverse geographic populations. A spatial probing task is used with geo-referenced corpora to measure the degree to which pre-trained language models from the OPT and BLOOM series represent diverse populations around the world. Results show that these models perform much better for some populations than others. In particular, populations across the US and the UK are represented quite well while those in South and Southeast Asia are poorly represented. Analysis shows that both families of models largely share the same skew across populations. At the same time, this skew cannot be fully explained by sociolinguistic factors, economic factors, or geographic factors. The basic conclusion from this analysis is that pre-trained models do not equally represent the world's population: there is a strong skew towards specific geographic populations. This finding challenges the ",
    "link": "https://arxiv.org/abs/2403.11025",
    "context": "Title: Pre-Trained Language Models Represent Some Geographic Populations Better Than Others\nAbstract: arXiv:2403.11025v1 Announce Type: new  Abstract: This paper measures the skew in how well two families of LLMs represent diverse geographic populations. A spatial probing task is used with geo-referenced corpora to measure the degree to which pre-trained language models from the OPT and BLOOM series represent diverse populations around the world. Results show that these models perform much better for some populations than others. In particular, populations across the US and the UK are represented quite well while those in South and Southeast Asia are poorly represented. Analysis shows that both families of models largely share the same skew across populations. At the same time, this skew cannot be fully explained by sociolinguistic factors, economic factors, or geographic factors. The basic conclusion from this analysis is that pre-trained models do not equally represent the world's population: there is a strong skew towards specific geographic populations. This finding challenges the ",
    "path": "papers/24/03/2403.11025.json",
    "total_tokens": 850,
    "translated_title": "预训练语言模型在代表某些地理人口方面较其他人更好",
    "translated_abstract": "这篇论文衡量了两种LLMs家族在代表多样化地理人口方面的偏向程度。使用空间探测任务和地理参考语料库衡量了OPT和BLOOM系列预训练语言模型在全球不同人口中的代表性程度。结果显示，这些模型对某些人口表现得比其他人口更好。特别是，美国和英国的人口被非常好地代表，而南亚和东南亚地区的人口则被较差地代表。分析表明，这两个模型家族在不同人口中基本上共享相同的偏向。同时，这种偏向不能完全通过社会语言因素、经济因素或地理因素来解释。从这个分析中得出的基本结论是，预训练模型并不平等地代表了世界人口：存在着特定地理人口的明显偏向。这一发现挑战了",
    "tldr": "预训练语言模型在代表地理人口方面存在明显偏向，表现优秀的人口主要集中在美国和英国，而南亚和东南亚地区的人口则被较差地代表。",
    "en_tdlr": "Pre-trained language models exhibit a strong skew in representing various geographic populations, performing well for populations in the US and UK while poorly representing those in South and Southeast Asia."
}