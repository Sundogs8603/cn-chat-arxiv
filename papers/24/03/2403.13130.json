{
    "title": "Self-generated Replay Memories for Continual Neural Machine Translation",
    "abstract": "arXiv:2403.13130v1 Announce Type: new  Abstract: Modern Neural Machine Translation systems exhibit strong performance in several different languages and are constantly improving. Their ability to learn continuously is, however, still severely limited by the catastrophic forgetting issue. In this work, we leverage a key property of encoder-decoder Transformers, i.e. their generative ability, to propose a novel approach to continually learning Neural Machine Translation systems. We show how this can effectively learn on a stream of experiences comprising different languages, by leveraging a replay memory populated by using the model itself as a generator of parallel sentences. We empirically demonstrate that our approach can counteract catastrophic forgetting without requiring explicit memorization of training data. Code will be publicly available upon publication. Code: https://github.com/m-resta/sg-rep",
    "link": "https://arxiv.org/abs/2403.13130",
    "context": "Title: Self-generated Replay Memories for Continual Neural Machine Translation\nAbstract: arXiv:2403.13130v1 Announce Type: new  Abstract: Modern Neural Machine Translation systems exhibit strong performance in several different languages and are constantly improving. Their ability to learn continuously is, however, still severely limited by the catastrophic forgetting issue. In this work, we leverage a key property of encoder-decoder Transformers, i.e. their generative ability, to propose a novel approach to continually learning Neural Machine Translation systems. We show how this can effectively learn on a stream of experiences comprising different languages, by leveraging a replay memory populated by using the model itself as a generator of parallel sentences. We empirically demonstrate that our approach can counteract catastrophic forgetting without requiring explicit memorization of training data. Code will be publicly available upon publication. Code: https://github.com/m-resta/sg-rep",
    "path": "papers/24/03/2403.13130.json",
    "total_tokens": 749,
    "translated_title": "自生成的回放记忆对持续神经机器翻译的影响",
    "translated_abstract": "现代神经机器翻译系统在多种不同语言中表现出色，并且不断改进。然而，它们对持续学习的能力仍然受到灾难性遗忘问题的严重限制。本研究利用编码器-解码器Transformer的关键属性，即其生成能力，提出了一种新颖的方法来持续学习神经机器翻译系统。我们展示了如何通过利用模型自身作为并行句子生成器来有效地学习由不同语言组成的经验流。我们在实证上证明了我们的方法可以抵消灾难性遗忘，而无需显式记忆训练数据。代码将在发表后公开提供。",
    "tldr": "提出了一种利用神经机器翻译系统的生成能力来构建自生成回放记忆的方法，可以有效解决持续学习过程中的灾难性遗忘问题。",
    "en_tdlr": "Proposed a method that leverages the generative ability of neural machine translation systems to build self-generated replay memories, effectively addressing the catastrophic forgetting issue during continual learning."
}