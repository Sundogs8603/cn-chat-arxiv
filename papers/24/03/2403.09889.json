{
    "title": "Generalization of Scaled Deep ResNets in the Mean-Field Regime",
    "abstract": "arXiv:2403.09889v1 Announce Type: new  Abstract: Despite the widespread empirical success of ResNet, the generalization properties of deep ResNet are rarely explored beyond the lazy training regime. In this work, we investigate \\emph{scaled} ResNet in the limit of infinitely deep and wide neural networks, of which the gradient flow is described by a partial differential equation in the large-neural network limit, i.e., the \\emph{mean-field} regime. To derive the generalization bounds under this setting, our analysis necessitates a shift from the conventional time-invariant Gram matrix employed in the lazy training regime to a time-variant, distribution-dependent version. To this end, we provide a global lower bound on the minimum eigenvalue of the Gram matrix under the mean-field regime. Besides, for the traceability of the dynamic of Kullback-Leibler (KL) divergence, we establish the linear convergence of the empirical error and estimate the upper bound of the KL divergence over param",
    "link": "https://arxiv.org/abs/2403.09889",
    "context": "Title: Generalization of Scaled Deep ResNets in the Mean-Field Regime\nAbstract: arXiv:2403.09889v1 Announce Type: new  Abstract: Despite the widespread empirical success of ResNet, the generalization properties of deep ResNet are rarely explored beyond the lazy training regime. In this work, we investigate \\emph{scaled} ResNet in the limit of infinitely deep and wide neural networks, of which the gradient flow is described by a partial differential equation in the large-neural network limit, i.e., the \\emph{mean-field} regime. To derive the generalization bounds under this setting, our analysis necessitates a shift from the conventional time-invariant Gram matrix employed in the lazy training regime to a time-variant, distribution-dependent version. To this end, we provide a global lower bound on the minimum eigenvalue of the Gram matrix under the mean-field regime. Besides, for the traceability of the dynamic of Kullback-Leibler (KL) divergence, we establish the linear convergence of the empirical error and estimate the upper bound of the KL divergence over param",
    "path": "papers/24/03/2403.09889.json",
    "total_tokens": 934,
    "translated_title": "在均场极限中缩放的深度ResNets的泛化",
    "translated_abstract": "尽管ResNet在经验上取得了广泛的成功，但深度ResNet的泛化特性在懒惰训练阶段之外很少被探索。在这项工作中，我们研究了在无限深和宽神经网络的极限下的\\emph{缩放}ResNet，其中梯度流被描述为大神经网络极限下的偏微分方程，即\\emph{均场}极限。为了在这种设置下推导出泛化界限，我们的分析需要从懒惰训练阶段采用的传统时间不变Gram矩阵转变为一个时间变量、依赖于分布的版本。为此，我们在均场极限下提供了Gram矩阵最小特征值的全局下界。此外，为了追踪Kullback-Leibler（KL）散度的动态，我们建立了经验误差的线性收敛性，并估计了KL散度在参数上的上界。",
    "tldr": "本研究通过研究在无限深和宽神经网络的极限下的缩放ResNet，推导出了在均场极限中泛化界限的全局下界和Kullback-Leibler散度的动态跟踪，为深度ResNet的泛化性质提供了新的认识。",
    "en_tdlr": "This study derives a global lower bound on the generalization bounds in the mean-field regime and tracks the dynamics of Kullback-Leibler divergence by investigating scaled ResNet in the limit of infinitely deep and wide neural networks, providing new insights into the generalization properties of deep ResNets."
}