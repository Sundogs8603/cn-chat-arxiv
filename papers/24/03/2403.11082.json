{
    "title": "RobustSentEmbed: Robust Sentence Embeddings Using Adversarial Self-Supervised Contrastive Learning",
    "abstract": "arXiv:2403.11082v1 Announce Type: cross  Abstract: Pre-trained language models (PLMs) have consistently demonstrated outstanding performance across a diverse spectrum of natural language processing tasks. Nevertheless, despite their success with unseen data, current PLM-based representations often exhibit poor robustness in adversarial settings. In this paper, we introduce RobustSentEmbed, a self-supervised sentence embedding framework designed to improve both generalization and robustness in diverse text representation tasks and against a diverse set of adversarial attacks. Through the generation of high-risk adversarial perturbations and their utilization in a novel objective function, RobustSentEmbed adeptly learns high-quality and robust sentence embeddings. Our experiments confirm the superiority of RobustSentEmbed over state-of-the-art representations. Specifically, Our framework achieves a significant reduction in the success rate of various adversarial attacks, notably reducing",
    "link": "https://arxiv.org/abs/2403.11082",
    "context": "Title: RobustSentEmbed: Robust Sentence Embeddings Using Adversarial Self-Supervised Contrastive Learning\nAbstract: arXiv:2403.11082v1 Announce Type: cross  Abstract: Pre-trained language models (PLMs) have consistently demonstrated outstanding performance across a diverse spectrum of natural language processing tasks. Nevertheless, despite their success with unseen data, current PLM-based representations often exhibit poor robustness in adversarial settings. In this paper, we introduce RobustSentEmbed, a self-supervised sentence embedding framework designed to improve both generalization and robustness in diverse text representation tasks and against a diverse set of adversarial attacks. Through the generation of high-risk adversarial perturbations and their utilization in a novel objective function, RobustSentEmbed adeptly learns high-quality and robust sentence embeddings. Our experiments confirm the superiority of RobustSentEmbed over state-of-the-art representations. Specifically, Our framework achieves a significant reduction in the success rate of various adversarial attacks, notably reducing",
    "path": "papers/24/03/2403.11082.json",
    "total_tokens": 849,
    "translated_title": "RobustSentEmbed：使用对抗自监督对比学习的稳健句子嵌入",
    "translated_abstract": "预训练语言模型（PLMs）在各种自然语言处理任务中展现出了出色的性能。然而，尽管它们在未见数据上取得成功，但目前基于PLM的表示通常在对抗设置中表现出较差的稳健性。本文引入了RobustSentEmbed，这是一个自监督句子嵌入框架，旨在改善不同文本表示任务中的泛化性和稳健性，以及对抗各种对抗攻击。通过生成高风险的对抗扰动并将其用于新颖的目标函数中，RobustSentEmbed巧妙地学习高质量和稳健的句子嵌入。我们的实验证实了RobustSentEmbed优于最先进表示方法的优越性。具体来说，我们的框架显著降低了各种对抗攻击的成功率，特别是降低了",
    "tldr": "RobustSentEmbed是一个自监督句子嵌入框架，通过对抗对比学习提高了文本表示任务中的泛化性和稳健性，实现了在对抗攻击中的优越表现。",
    "en_tdlr": "RobustSentEmbed is a self-supervised sentence embedding framework that enhances generalization and robustness in text representation tasks through adversarial contrastive learning, achieving superior performance in adversarial attacks."
}