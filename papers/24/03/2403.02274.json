{
    "title": "NatSGD: A Dataset with Speech, Gestures, and Demonstrations for Robot Learning in Natural Human-Robot Interaction",
    "abstract": "arXiv:2403.02274v1 Announce Type: cross  Abstract: Recent advancements in multimodal Human-Robot Interaction (HRI) datasets have highlighted the fusion of speech and gesture, expanding robots' capabilities to absorb explicit and implicit HRI insights. However, existing speech-gesture HRI datasets often focus on elementary tasks, like object pointing and pushing, revealing limitations in scaling to intricate domains and prioritizing human command data over robot behavior records. To bridge these gaps, we introduce NatSGD, a multimodal HRI dataset encompassing human commands through speech and gestures that are natural, synchronized with robot behavior demonstrations. NatSGD serves as a foundational resource at the intersection of machine learning and HRI research, and we demonstrate its effectiveness in training robots to understand tasks through multimodal human commands, emphasizing the significance of jointly considering speech and gestures. We have released our dataset, simulator, a",
    "link": "https://arxiv.org/abs/2403.02274",
    "context": "Title: NatSGD: A Dataset with Speech, Gestures, and Demonstrations for Robot Learning in Natural Human-Robot Interaction\nAbstract: arXiv:2403.02274v1 Announce Type: cross  Abstract: Recent advancements in multimodal Human-Robot Interaction (HRI) datasets have highlighted the fusion of speech and gesture, expanding robots' capabilities to absorb explicit and implicit HRI insights. However, existing speech-gesture HRI datasets often focus on elementary tasks, like object pointing and pushing, revealing limitations in scaling to intricate domains and prioritizing human command data over robot behavior records. To bridge these gaps, we introduce NatSGD, a multimodal HRI dataset encompassing human commands through speech and gestures that are natural, synchronized with robot behavior demonstrations. NatSGD serves as a foundational resource at the intersection of machine learning and HRI research, and we demonstrate its effectiveness in training robots to understand tasks through multimodal human commands, emphasizing the significance of jointly considering speech and gestures. We have released our dataset, simulator, a",
    "path": "papers/24/03/2403.02274.json",
    "total_tokens": 876,
    "translated_title": "NatSGD: 一个包含语音、手势和演示的数据集，用于自然人机交互中机器人学习",
    "translated_abstract": "最近多模式人机交互(HRI)数据集的发展突出了语音和手势的融合，扩展了机器人吸收显式和隐式人机交互洞见的能力。然而，现有的语音-手势HRI数据集通常专注于基本任务，如物体指向和推动，揭示了在扩展到复杂领域和优先考虑人类命令数据而不是机器人行为记录方面的限制。为了弥合这些差距，我们引入了NatSGD，一个多模式HRI数据集，涵盖了通过语音和手势传达的自然人类命令，这些命令与机器人行为演示同步。NatSGD作为机器学习和HRI研究交叉点上的基础资源，并展示了训练机器人通过多模式人类命令理解任务的有效性，强调了共同考虑语音和手势的重要性。我们已发布我们的数据集、模拟器等。",
    "tldr": "NatSGD是一个包含语音、手势和演示的数据集，旨在帮助机器人通过多模式人类命令理解任务，并强调了共同考虑语音和手势的重要性。"
}