{
    "title": "Near-optimal Per-Action Regret Bounds for Sleeping Bandits",
    "abstract": "arXiv:2403.01315v1 Announce Type: new  Abstract: We derive near-optimal per-action regret bounds for sleeping bandits, in which both the sets of available arms and their losses in every round are chosen by an adversary. In a setting with $K$ total arms and at most $A$ available arms in each round over $T$ rounds, the best known upper bound is $O(K\\sqrt{TA\\ln{K}})$, obtained indirectly via minimizing internal sleeping regrets. Compared to the minimax $\\Omega(\\sqrt{TA})$ lower bound, this upper bound contains an extra multiplicative factor of $K\\ln{K}$. We address this gap by directly minimizing the per-action regret using generalized versions of EXP3, EXP3-IX and FTRL with Tsallis entropy, thereby obtaining near-optimal bounds of order $O(\\sqrt{TA\\ln{K}})$ and $O(\\sqrt{T\\sqrt{AK}})$. We extend our results to the setting of bandits with advice from sleeping experts, generalizing EXP4 along the way. This leads to new proofs for a number of existing adaptive and tracking regret bounds for ",
    "link": "https://arxiv.org/abs/2403.01315",
    "context": "Title: Near-optimal Per-Action Regret Bounds for Sleeping Bandits\nAbstract: arXiv:2403.01315v1 Announce Type: new  Abstract: We derive near-optimal per-action regret bounds for sleeping bandits, in which both the sets of available arms and their losses in every round are chosen by an adversary. In a setting with $K$ total arms and at most $A$ available arms in each round over $T$ rounds, the best known upper bound is $O(K\\sqrt{TA\\ln{K}})$, obtained indirectly via minimizing internal sleeping regrets. Compared to the minimax $\\Omega(\\sqrt{TA})$ lower bound, this upper bound contains an extra multiplicative factor of $K\\ln{K}$. We address this gap by directly minimizing the per-action regret using generalized versions of EXP3, EXP3-IX and FTRL with Tsallis entropy, thereby obtaining near-optimal bounds of order $O(\\sqrt{TA\\ln{K}})$ and $O(\\sqrt{T\\sqrt{AK}})$. We extend our results to the setting of bandits with advice from sleeping experts, generalizing EXP4 along the way. This leads to new proofs for a number of existing adaptive and tracking regret bounds for ",
    "path": "papers/24/03/2403.01315.json",
    "total_tokens": 929,
    "translated_title": "睡眠臂决策问题中接近最优的每次行动遗憾界",
    "translated_abstract": "我们推导了针对睡眠臂决策问题的接近最优每次行动遗憾界，其中敌手选择每轮可用臂的集合和它们的损失。在每轮至多有 $A$ 个可用臂的 $K$ 个总臂的情况下，已知的最好上界为 $O(K\\sqrt{TA\\ln{K}})$，通过间接最小化内部睡眠遗憾获得。与极小值 $\\Omega(\\sqrt{TA})$ 下界相比，这个上界包含额外的乘数因子 $K\\ln{K}$。我们通过直接最小化每次行动遗憾，使用EXP3、EXP3-IX和带有Tsallis熵的FTRL的推广版本，从而获得了顺序为 $O(\\sqrt{TA\\ln{K}})$ 和 $O(\\sqrt{T\\sqrt{AK}})$ 的接近最优界。我们将结果扩展到了从睡眠专家获得建议的臂决策问题设置，同时推广了EXP4。这为现有的多个自适应和跟踪遗憾界的新证明铺平了道路。",
    "tldr": "该论文提出了针对睡眠臂决策问题的接近最优每次行动遗憾界，通过直接最小化每次行动遗憾，使用Generalized EXP3、EXP3-IX和Tsallis entropy下的FTRL方法，获得了较之现有方法更好的界。"
}