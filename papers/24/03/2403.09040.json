{
    "title": "RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems",
    "abstract": "arXiv:2403.09040v1 Announce Type: new  Abstract: Retrieval-augmented generation (RAG) greatly benefits language models (LMs) by providing additional context for tasks such as document-based question answering (DBQA). Despite its potential, the power of RAG is highly dependent on its configuration, raising the question: What is the optimal RAG configuration? To answer this, we introduce the RAGGED framework to analyze and optimize RAG systems. On a set of representative DBQA tasks, we study two classic sparse and dense retrievers, and four top-performing LMs in encoder-decoder and decoder-only architectures. Through RAGGED, we uncover that different models suit substantially varied RAG setups. While encoder-decoder models monotonically improve with more documents, we find decoder-only models can only effectively use < 5 documents, despite often having a longer context window. RAGGED offers further insights into LMs' context utilization habits, where we find that encoder-decoder models r",
    "link": "https://arxiv.org/abs/2403.09040",
    "context": "Title: RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems\nAbstract: arXiv:2403.09040v1 Announce Type: new  Abstract: Retrieval-augmented generation (RAG) greatly benefits language models (LMs) by providing additional context for tasks such as document-based question answering (DBQA). Despite its potential, the power of RAG is highly dependent on its configuration, raising the question: What is the optimal RAG configuration? To answer this, we introduce the RAGGED framework to analyze and optimize RAG systems. On a set of representative DBQA tasks, we study two classic sparse and dense retrievers, and four top-performing LMs in encoder-decoder and decoder-only architectures. Through RAGGED, we uncover that different models suit substantially varied RAG setups. While encoder-decoder models monotonically improve with more documents, we find decoder-only models can only effectively use < 5 documents, despite often having a longer context window. RAGGED offers further insights into LMs' context utilization habits, where we find that encoder-decoder models r",
    "path": "papers/24/03/2403.09040.json",
    "total_tokens": 964,
    "translated_title": "RAGGED:朝着基于检索增强生成系统的知情设计",
    "translated_abstract": "arXiv:2403.09040v1 声明类型: 新的 摘要: 检索增强生成（RAG）通过为文档型问答等任务提供附加上下文，极大地提升了语言模型（LMs）的性能。尽管具有潜力，但RAG的效力高度依赖于其配置，从而引发一个问题：什么是最佳RAG配置？为了回答这个问题，我们引入了RAGGED框架来分析和优化RAG系统。在一组代表性的文档型问答任务上，我们研究了两种经典的稀疏和密集检索器，以及四种在编码器-解码器和仅解码器结构中表现优异的LMs。通过RAGGED，我们发现不同模型适合完全不同的RAG设置。虽然编码器-解码器模型随着更多文档的增加而单调提升，但我们发现仅解码器模型只能有效地使用<5个文档，尽管通常具有更长的上下文窗口。RAGGED进一步揭示了LMs的上下文利用习惯，我们发现编码器-解码器模型...",
    "tldr": "RAGGED框架分析和优化了检索增强生成系统，揭示了不同模型适合不同RAG设置的事实，编码器-解码器模型随文档数量增加而改善，而仅解码器模型只能有效利用少量文档。",
    "en_tdlr": "The RAGGED framework analyzed and optimized the retrieval augmented generation systems, revealing that different models suit substantially varied RAG setups, with encoder-decoder models improving monotonically with more documents while decoder-only models can only effectively use a small number of documents."
}