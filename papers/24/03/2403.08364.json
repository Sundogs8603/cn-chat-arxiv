{
    "title": "Decoupled Federated Learning on Long-Tailed and Non-IID data with Feature Statistics",
    "abstract": "arXiv:2403.08364v1 Announce Type: cross  Abstract: Federated learning is designed to enhance data security and privacy, but faces challenges when dealing with heterogeneous data in long-tailed and non-IID distributions. This paper explores an overlooked scenario where tail classes are sparsely distributed over a few clients, causing the models trained with these classes to have a lower probability of being selected during client aggregation, leading to slower convergence rates and poorer model performance. To address this issue, we propose a two-stage Decoupled Federated learning framework using Feature Statistics (DFL-FS). In the first stage, the server estimates the client's class coverage distributions through masked local feature statistics clustering to select models for aggregation to accelerate convergence and enhance feature learning without privacy leakage. In the second stage, DFL-FS employs federated feature regeneration based on global feature statistics and utilizes resamp",
    "link": "https://arxiv.org/abs/2403.08364",
    "context": "Title: Decoupled Federated Learning on Long-Tailed and Non-IID data with Feature Statistics\nAbstract: arXiv:2403.08364v1 Announce Type: cross  Abstract: Federated learning is designed to enhance data security and privacy, but faces challenges when dealing with heterogeneous data in long-tailed and non-IID distributions. This paper explores an overlooked scenario where tail classes are sparsely distributed over a few clients, causing the models trained with these classes to have a lower probability of being selected during client aggregation, leading to slower convergence rates and poorer model performance. To address this issue, we propose a two-stage Decoupled Federated learning framework using Feature Statistics (DFL-FS). In the first stage, the server estimates the client's class coverage distributions through masked local feature statistics clustering to select models for aggregation to accelerate convergence and enhance feature learning without privacy leakage. In the second stage, DFL-FS employs federated feature regeneration based on global feature statistics and utilizes resamp",
    "path": "papers/24/03/2403.08364.json",
    "total_tokens": 885,
    "translated_title": "针对长尾和非独立同分布数据的特征统计分开式联邦学习",
    "translated_abstract": "联邦学习旨在增强数据安全性和隐私性，但在处理长尾和非独立同分布的异构数据时面临挑战。本文探讨了一个被忽视的情景，即尾部类别在少数客户端上稀疏分布，导致使用这些类别训练的模型在客户端聚合过程中被选择的概率较低，从而导致收敛速度较慢和模型性能较差。为了解决这个问题，我们提出了一个使用特征统计的两阶段分开式联邦学习框架（DFL-FS）。在第一阶段，服务器通过蒙版局部特征统计聚类估计客户端的类别覆盖分布，以加快收敛速度和增强特征学习而不泄露隐私。在第二阶段，DFL-FS基于全局特征统计采用联邦特征再生，并利用重采样",
    "tldr": "本文提出了针对长尾和非独立同分布数据的特征统计分开式联邦学习框架，通过两阶段方式解决尾部类别稀疏分布导致的模型性能下降问题",
    "en_tdlr": "This paper introduces a decoupled federated learning framework for long-tailed and non-IID data, addressing the issue of model performance degradation caused by sparse distribution of tail classes through a two-stage approach."
}