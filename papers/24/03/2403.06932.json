{
    "title": "ERA-CoT: Improving Chain-of-Thought through Entity Relationship Analysis",
    "abstract": "arXiv:2403.06932v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved commendable accomplishments in various natural language processing tasks. However, LLMs still encounter significant challenges when dealing with complex scenarios involving multiple entities. These challenges arise from the presence of implicit relationships that demand multi-step reasoning. In this paper, we propose a novel approach ERA-CoT, which aids LLMs in understanding context by capturing relationships between entities and supports the reasoning of diverse tasks through Chain-of-Thoughts (CoT). Experimental results show that ERA-CoT demonstrates the superior performance of our proposed method compared to current CoT prompting methods, achieving a significant improvement of an average of 5.1\\% on GPT3.5 compared to previous SOTA baselines. Our analysis indicates that ERA-CoT increases the LLM's understanding of entity relationships, significantly improves the accuracy of question answering",
    "link": "https://arxiv.org/abs/2403.06932",
    "context": "Title: ERA-CoT: Improving Chain-of-Thought through Entity Relationship Analysis\nAbstract: arXiv:2403.06932v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved commendable accomplishments in various natural language processing tasks. However, LLMs still encounter significant challenges when dealing with complex scenarios involving multiple entities. These challenges arise from the presence of implicit relationships that demand multi-step reasoning. In this paper, we propose a novel approach ERA-CoT, which aids LLMs in understanding context by capturing relationships between entities and supports the reasoning of diverse tasks through Chain-of-Thoughts (CoT). Experimental results show that ERA-CoT demonstrates the superior performance of our proposed method compared to current CoT prompting methods, achieving a significant improvement of an average of 5.1\\% on GPT3.5 compared to previous SOTA baselines. Our analysis indicates that ERA-CoT increases the LLM's understanding of entity relationships, significantly improves the accuracy of question answering",
    "path": "papers/24/03/2403.06932.json",
    "total_tokens": 828,
    "translated_title": "ERA-CoT: 通过实体关系分析改进思维链",
    "translated_abstract": "大型语言模型在各种自然语言处理任务中取得了可观的成就。然而，当处理涉及多个实体的复杂场景时，LLMs 仍然面临重大挑战。这些挑战源于存在需要多步推理的隐式关系。在本文中，我们提出了一种新颖的方法 ERA-CoT，通过捕获实体之间的关系来帮助 LLMs 理解上下文，并通过思维链（CoT）支持不同任务的推理。实验结果表明，与当前的 CoT 提示方法相比，ERA-CoT 表现出我们提出的方法在 GPT3.5 上平均比以前的 SOTA 基线实现了显著的 5.1% 改进的卓越性能。我们的分析表明，ERA-CoT 提高了LLM对实体关系的理解，显著提高了问题回答的准确性。",
    "tldr": "ERA-CoT 提出了一种新颖的方法，通过捕获实体之间的关系和支持思维链，帮助大型语言模型(LLMs)理解上下文，提高了多样任务的推理准确性。",
    "en_tdlr": "ERA-CoT proposes a novel approach to help large language models (LLMs) understand context by capturing relationships between entities and supporting chain-of-thought, which leads to improved reasoning accuracy for diverse tasks."
}