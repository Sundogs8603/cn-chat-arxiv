{
    "title": "Better Schedules for Low Precision Training of Deep Neural Networks",
    "abstract": "arXiv:2403.02243v1 Announce Type: cross  Abstract: Low precision training can significantly reduce the computational overhead of training deep neural networks (DNNs). Though many such techniques exist, cyclic precision training (CPT), which dynamically adjusts precision throughout train- ing according to a cyclic schedule, achieves particularly impressive improvements in training efficiency, while actually improving DNN performance. Existing CPT implementations take common learning rate schedules (e.g., cyclical cosine sched- ules) and use them for low precision training without adequate comparisons to alternative scheduling options. We define a diverse suite of CPT schedules and analyze their performance across a variety of DNN training regimes, some of which are unexplored in the low precision training literature (e.g., node classification with graph neural networks). From these experiments, we discover alternative CPT schedules that offer further improvements in training efficiency ",
    "link": "https://arxiv.org/abs/2403.02243",
    "context": "Title: Better Schedules for Low Precision Training of Deep Neural Networks\nAbstract: arXiv:2403.02243v1 Announce Type: cross  Abstract: Low precision training can significantly reduce the computational overhead of training deep neural networks (DNNs). Though many such techniques exist, cyclic precision training (CPT), which dynamically adjusts precision throughout train- ing according to a cyclic schedule, achieves particularly impressive improvements in training efficiency, while actually improving DNN performance. Existing CPT implementations take common learning rate schedules (e.g., cyclical cosine sched- ules) and use them for low precision training without adequate comparisons to alternative scheduling options. We define a diverse suite of CPT schedules and analyze their performance across a variety of DNN training regimes, some of which are unexplored in the low precision training literature (e.g., node classification with graph neural networks). From these experiments, we discover alternative CPT schedules that offer further improvements in training efficiency ",
    "path": "papers/24/03/2403.02243.json",
    "total_tokens": 812,
    "translated_title": "深度神经网络低精度训练的更好调度",
    "translated_abstract": "低精度训练可以显著减少训练深度神经网络(DNNs)的计算开销。尽管存在许多这样的技术，但循环精度训练(CPT)，根据循环调度通过动态调整精度的方式进行训练，实现了训练效率的显著提升，同时实际上提高了DNN的性能。现有的CPT实现采用常见的学习率调度（例如，周期余弦调度），并在低精度训练中使用它们，但未与其他调度选项进行充分比较。我们定义了一套多样化的CPT调度，并分析它们在各种DNN训练方案中的性能，其中一些在低精度训练文献中尚未探索（例如，使用图神经网络进行节点分类）。通过这些实验，我们发现了提供进一步提升训练效率的替代CPT调度。",
    "tldr": "该研究发现了用于低精度训练的循环精度训练调度的更好选择，进一步提高了训练效率",
    "en_tdlr": "This study identifies better choices of cyclic precision training schedules for low precision training, further enhancing training efficiency."
}