{
    "title": "Dissipative Gradient Descent Ascent Method: A Control Theory Inspired Algorithm for Min-max Optimization",
    "abstract": "arXiv:2403.09090v1 Announce Type: cross  Abstract: Gradient Descent Ascent (GDA) methods for min-max optimization problems typically produce oscillatory behavior that can lead to instability, e.g., in bilinear settings. To address this problem, we introduce a dissipation term into the GDA updates to dampen these oscillations. The proposed Dissipative GDA (DGDA) method can be seen as performing standard GDA on a state-augmented and regularized saddle function that does not strictly introduce additional convexity/concavity. We theoretically show the linear convergence of DGDA in the bilinear and strongly convex-strongly concave settings and assess its performance by comparing DGDA with other methods such as GDA, Extra-Gradient (EG), and Optimistic GDA. Our findings demonstrate that DGDA surpasses these methods, achieving superior convergence rates. We support our claims with two numerical examples that showcase DGDA's effectiveness in solving saddle point problems.",
    "link": "https://arxiv.org/abs/2403.09090",
    "context": "Title: Dissipative Gradient Descent Ascent Method: A Control Theory Inspired Algorithm for Min-max Optimization\nAbstract: arXiv:2403.09090v1 Announce Type: cross  Abstract: Gradient Descent Ascent (GDA) methods for min-max optimization problems typically produce oscillatory behavior that can lead to instability, e.g., in bilinear settings. To address this problem, we introduce a dissipation term into the GDA updates to dampen these oscillations. The proposed Dissipative GDA (DGDA) method can be seen as performing standard GDA on a state-augmented and regularized saddle function that does not strictly introduce additional convexity/concavity. We theoretically show the linear convergence of DGDA in the bilinear and strongly convex-strongly concave settings and assess its performance by comparing DGDA with other methods such as GDA, Extra-Gradient (EG), and Optimistic GDA. Our findings demonstrate that DGDA surpasses these methods, achieving superior convergence rates. We support our claims with two numerical examples that showcase DGDA's effectiveness in solving saddle point problems.",
    "path": "papers/24/03/2403.09090.json",
    "total_tokens": 886,
    "translated_title": "耗散梯度下降方法：一种受控制论启发的极小-极大优化算法",
    "translated_abstract": "梯度下降上升(GDA)方法用于极小-极大优化问题通常会产生可能导致不稳定性的振荡行为，例如在双线性设置中。为了解决这个问题，我们在GDA更新中引入了一个耗散项，以减弱这些振荡。所提出的耗散梯度下降（DGDA）方法可以被视为在一个状态增广和正则化的鞍点函数上执行标准的GDA，而不严格引入额外的凸性/凹性。我们在双线性和强凸-强凹设置中理论上展示了DGDA的线性收敛性，并通过将DGDA与其他方法（如GDA、Extra-Gradient (EG) 和乐观性GDA）进行比较来评估其性能。我们的研究结果表明，DGDA超越了这些方法，实现了更优越的收敛速度。我们通过两个数值示例支持我们的说法，展示了DGDA在解决鞍点问题中的有效性。",
    "tldr": "DGDA方法通过在GDA更新中引入耗散项，解决了极小-极大优化问题中的振荡行为，实现了超越其他方法的更优越收敛速度。",
    "en_tdlr": "DGDA method addresses oscillatory behavior in min-max optimization problems by introducing a dissipative term in GDA updates, achieving superior convergence rates compared to other methods."
}