{
    "title": "The Case for Evaluating Multimodal Translation Models on Text Datasets",
    "abstract": "arXiv:2403.03014v1 Announce Type: new  Abstract: A good evaluation framework should evaluate multimodal machine translation (MMT) models by measuring 1) their use of visual information to aid in the translation task and 2) their ability to translate complex sentences such as done for text-only machine translation. However, most current work in MMT is evaluated against the Multi30k testing sets, which do not measure these properties. Namely, the use of visual information by the MMT model cannot be shown directly from the Multi30k test set results and the sentences in Multi30k are are image captions, i.e., short, descriptive sentences, as opposed to complex sentences that typical text-only machine translation models are evaluated against.   Therefore, we propose that MMT models be evaluated using 1) the CoMMuTE evaluation framework, which measures the use of visual information by MMT models, 2) the text-only WMT news translation task test sets, which evaluates translation performance aga",
    "link": "https://arxiv.org/abs/2403.03014",
    "context": "Title: The Case for Evaluating Multimodal Translation Models on Text Datasets\nAbstract: arXiv:2403.03014v1 Announce Type: new  Abstract: A good evaluation framework should evaluate multimodal machine translation (MMT) models by measuring 1) their use of visual information to aid in the translation task and 2) their ability to translate complex sentences such as done for text-only machine translation. However, most current work in MMT is evaluated against the Multi30k testing sets, which do not measure these properties. Namely, the use of visual information by the MMT model cannot be shown directly from the Multi30k test set results and the sentences in Multi30k are are image captions, i.e., short, descriptive sentences, as opposed to complex sentences that typical text-only machine translation models are evaluated against.   Therefore, we propose that MMT models be evaluated using 1) the CoMMuTE evaluation framework, which measures the use of visual information by MMT models, 2) the text-only WMT news translation task test sets, which evaluates translation performance aga",
    "path": "papers/24/03/2403.03014.json",
    "total_tokens": 723,
    "translated_title": "在文本数据集上评估多模式翻译模型的必要性",
    "translated_abstract": "一个良好的评估框架应该通过衡量多模式机器翻译（MMT）模型在翻译任务中利用视觉信息的能力以及它们翻译复杂句子的能力来评估它们。然而，大多数当前的MMT工作是根据Multi30k测试集进行评估的，而这些测试集并不能衡量这些属性。因此，我们建议使用CoMMuTE评估框架来评估MMT模型的性能。",
    "tldr": "评估多模式翻译模型时，应考虑其利用视觉信息的能力和翻译复杂句子的表现，建议使用CoMMuTE评估框架。"
}