{
    "title": "Less is More: Hop-Wise Graph Attention for Scalable and Generalizable Learning on Circuits",
    "abstract": "arXiv:2403.01317v1 Announce Type: new  Abstract: While graph neural networks (GNNs) have gained popularity for learning circuit representations in various electronic design automation (EDA) tasks, they face challenges in scalability when applied to large graphs and exhibit limited generalizability to new designs. These limitations make them less practical for addressing large-scale, complex circuit problems. In this work we propose HOGA, a novel attention-based model for learning circuit representations in a scalable and generalizable manner. HOGA first computes hop-wise features per node prior to model training. Subsequently, the hop-wise features are solely used to produce node representations through a gated self-attention module, which adaptively learns important features among different hops without involving the graph topology. As a result, HOGA is adaptive to various structures across different circuits and can be efficiently trained in a distributed manner. To demonstrate the e",
    "link": "https://arxiv.org/abs/2403.01317",
    "context": "Title: Less is More: Hop-Wise Graph Attention for Scalable and Generalizable Learning on Circuits\nAbstract: arXiv:2403.01317v1 Announce Type: new  Abstract: While graph neural networks (GNNs) have gained popularity for learning circuit representations in various electronic design automation (EDA) tasks, they face challenges in scalability when applied to large graphs and exhibit limited generalizability to new designs. These limitations make them less practical for addressing large-scale, complex circuit problems. In this work we propose HOGA, a novel attention-based model for learning circuit representations in a scalable and generalizable manner. HOGA first computes hop-wise features per node prior to model training. Subsequently, the hop-wise features are solely used to produce node representations through a gated self-attention module, which adaptively learns important features among different hops without involving the graph topology. As a result, HOGA is adaptive to various structures across different circuits and can be efficiently trained in a distributed manner. To demonstrate the e",
    "path": "papers/24/03/2403.01317.json",
    "total_tokens": 903,
    "translated_title": "少即是多：面向可扩展和通用学习的跳数图注意力在电路上的应用",
    "translated_abstract": "虽然图神经网络（GNNs）在各种电子设计自动化（EDA）任务中学习电路表示方面变得流行，但当应用于大图时，它们面临可扩展性挑战，并且对新设计的泛化能力有限。这些限制使它们在解决大规模复杂电路问题时不太实用。在这项工作中，我们提出了HOGA，一种新颖的基于注意力的模型，用于以可扩展和通用的方式学习电路表示。HOGA首先在模型训练之前针对每个节点计算跳数特征。随后，跳数特征仅用于通过门控自注意力模块生成节点表示，该模块自适应地学习不同跳数之间的重要特征，而不涉及图拓扑。因此，HOGA能够适应不同电路之间的各种结构，并可以以分布式的方式高效训练。",
    "tldr": "提出了一种名为HOGA的基于注意力的模型，能够在电路中以可扩展和通用的方式学习电路表示，通过跳数特征和门控自注意力模块的方式，实现了对不同电路结构的自适应学习，并可以进行高效的分布式训练。",
    "en_tdlr": "Introduced a novel attention-based model called HOGA for scalable and generalizable learning of circuit representations, which adaptively learns important features among different circuit structures through hop-wise features and a gated self-attention module, enabling efficient distributed training."
}