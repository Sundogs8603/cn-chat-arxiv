{
    "title": "Towards Personalized Evaluation of Large Language Models with An Anonymous Crowd-Sourcing Platform",
    "abstract": "arXiv:2403.08305v1 Announce Type: new  Abstract: Large language model evaluation plays a pivotal role in the enhancement of its capacity. Previously, numerous methods for evaluating large language models have been proposed in this area. Despite their effectiveness, these existing works mainly focus on assessing objective questions, overlooking the capability to evaluate subjective questions which is extremely common for large language models. Additionally, these methods predominantly utilize centralized datasets for evaluation, with question banks concentrated within the evaluation platforms themselves. Moreover, the evaluation processes employed by these platforms often overlook personalized factors, neglecting to consider the individual characteristics of both the evaluators and the models being evaluated. To address these limitations, we propose a novel anonymous crowd-sourcing evaluation platform, BingJian, for large language models that employs a competitive scoring mechanism wher",
    "link": "https://arxiv.org/abs/2403.08305",
    "context": "Title: Towards Personalized Evaluation of Large Language Models with An Anonymous Crowd-Sourcing Platform\nAbstract: arXiv:2403.08305v1 Announce Type: new  Abstract: Large language model evaluation plays a pivotal role in the enhancement of its capacity. Previously, numerous methods for evaluating large language models have been proposed in this area. Despite their effectiveness, these existing works mainly focus on assessing objective questions, overlooking the capability to evaluate subjective questions which is extremely common for large language models. Additionally, these methods predominantly utilize centralized datasets for evaluation, with question banks concentrated within the evaluation platforms themselves. Moreover, the evaluation processes employed by these platforms often overlook personalized factors, neglecting to consider the individual characteristics of both the evaluators and the models being evaluated. To address these limitations, we propose a novel anonymous crowd-sourcing evaluation platform, BingJian, for large language models that employs a competitive scoring mechanism wher",
    "path": "papers/24/03/2403.08305.json",
    "total_tokens": 836,
    "translated_title": "基于匿名众包平台的大型语言模型个性化评估方法研究",
    "translated_abstract": "大型语言模型的评估在提升其性能方面起着关键作用。以往已经提出了许多用于评估大型语言模型的方法。然而，这些现有方法主要集中在评估客观问题，忽视了对大型语言模型而言非常普遍的主观问题的评估能力。此外，这些方法主要利用中心化数据集进行评估，问题库集中在评估平台本身。而且，这些平台采用的评估过程经常忽视了个性化因素，未考虑评估者和被评估模型的个体特征。为了解决这些问题，我们提出了一个新颖的匿名众包评估平台——炳剑，用于大型语言模型，采用了竞争性评分机制。",
    "tldr": "提出了一个新颖的匿名众包评估平台，BingJian，用于大型语言模型，采用了竞争性评分机制，解决了现有方法主要评估客观问题、忽视主观问题、使用中心化数据集、忽视个性化因素的局限性。",
    "en_tdlr": "Proposed a novel anonymous crowd-sourcing evaluation platform, BingJian, for large language models with a competitive scoring mechanism, addressing the limitations of existing methods that mainly focus on objective questions, overlook subjective questions, use centralized datasets, and neglect personalized factors."
}