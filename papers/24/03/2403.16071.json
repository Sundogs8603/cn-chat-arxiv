{
    "title": "Landmark-Guided Cross-Speaker Lip Reading with Mutual Information Regularization",
    "abstract": "arXiv:2403.16071v1 Announce Type: new  Abstract: Lip reading, the process of interpreting silent speech from visual lip movements, has gained rising attention for its wide range of realistic applications. Deep learning approaches greatly improve current lip reading systems. However, lip reading in cross-speaker scenarios where the speaker identity changes, poses a challenging problem due to inter-speaker variability. A well-trained lip reading system may perform poorly when handling a brand new speaker. To learn a speaker-robust lip reading model, a key insight is to reduce visual variations across speakers, avoiding the model overfitting to specific speakers. In this work, in view of both input visual clues and latent representations based on a hybrid CTC/attention architecture, we propose to exploit the lip landmark-guided fine-grained visual clues instead of frequently-used mouth-cropped images as input features, diminishing speaker-specific appearance characteristics. Furthermore, ",
    "link": "https://arxiv.org/abs/2403.16071",
    "context": "Title: Landmark-Guided Cross-Speaker Lip Reading with Mutual Information Regularization\nAbstract: arXiv:2403.16071v1 Announce Type: new  Abstract: Lip reading, the process of interpreting silent speech from visual lip movements, has gained rising attention for its wide range of realistic applications. Deep learning approaches greatly improve current lip reading systems. However, lip reading in cross-speaker scenarios where the speaker identity changes, poses a challenging problem due to inter-speaker variability. A well-trained lip reading system may perform poorly when handling a brand new speaker. To learn a speaker-robust lip reading model, a key insight is to reduce visual variations across speakers, avoiding the model overfitting to specific speakers. In this work, in view of both input visual clues and latent representations based on a hybrid CTC/attention architecture, we propose to exploit the lip landmark-guided fine-grained visual clues instead of frequently-used mouth-cropped images as input features, diminishing speaker-specific appearance characteristics. Furthermore, ",
    "path": "papers/24/03/2403.16071.json",
    "total_tokens": 847,
    "translated_title": "通过互信息正则化进行基准引导的跨说话人唇读",
    "translated_abstract": "Lip reading，即通过视觉唇部运动解释无声语音的过程，由于其广泛的实际应用而引起人们的广泛关注。深度学习方法极大地改进了当前的唇读系统。然而，在说话人变化的交叉说话人场景中进行唇读，由于说话人之间的变异性，存在挑战性问题。一个训练良好的唇读系统在处理全新的说话人时可能表现不佳。为了学习一个适应说话人的唇读模型，一个关键的见解是减少说话人之间的视觉变化，避免模型过度拟合特定说话人。本研究针对基于混合CTC/attention架构的输入视觉线索和基于隐变量表示，提出利用唇部地标引导的细粒度视觉线索，而不是频繁使用的裁剪嘴巴图片作为输入特征，减少说话人特定的外观特征。",
    "tldr": "通过利用唇部地标引导的细粒度视觉线索，提出了一种适应说话人的唇读模型，有效降低说话人之间的视觉变化。",
    "en_tdlr": "The paper proposes a speaker-robust lip reading model by utilizing fine-grained visual clues guided by lip landmarks, effectively reducing visual variations across speakers."
}