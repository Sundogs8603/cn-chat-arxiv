{
    "title": "Genetic Quantization-Aware Approximation for Non-Linear Operations in Transformers",
    "abstract": "arXiv:2403.19591v1 Announce Type: new  Abstract: Non-linear functions are prevalent in Transformers and their lightweight variants, incurring substantial and frequently underestimated hardware costs. Previous state-of-the-art works optimize these operations by piece-wise linear approximation and store the parameters in look-up tables (LUT), but most of them require unfriendly high-precision arithmetics such as FP/INT 32 and lack consideration of integer-only INT quantization. This paper proposed a genetic LUT-Approximation algorithm namely GQA-LUT that can automatically determine the parameters with quantization awareness. The results demonstrate that GQA-LUT achieves negligible degradation on the challenging semantic segmentation task for both vanilla and linear Transformer models. Besides, proposed GQA-LUT enables the employment of INT8-based LUT-Approximation that achieves an area savings of 81.3~81.7% and a power reduction of 79.3~80.2% compared to the high-precision FP/INT 32 alte",
    "link": "https://arxiv.org/abs/2403.19591",
    "context": "Title: Genetic Quantization-Aware Approximation for Non-Linear Operations in Transformers\nAbstract: arXiv:2403.19591v1 Announce Type: new  Abstract: Non-linear functions are prevalent in Transformers and their lightweight variants, incurring substantial and frequently underestimated hardware costs. Previous state-of-the-art works optimize these operations by piece-wise linear approximation and store the parameters in look-up tables (LUT), but most of them require unfriendly high-precision arithmetics such as FP/INT 32 and lack consideration of integer-only INT quantization. This paper proposed a genetic LUT-Approximation algorithm namely GQA-LUT that can automatically determine the parameters with quantization awareness. The results demonstrate that GQA-LUT achieves negligible degradation on the challenging semantic segmentation task for both vanilla and linear Transformer models. Besides, proposed GQA-LUT enables the employment of INT8-based LUT-Approximation that achieves an area savings of 81.3~81.7% and a power reduction of 79.3~80.2% compared to the high-precision FP/INT 32 alte",
    "path": "papers/24/03/2403.19591.json",
    "total_tokens": 870,
    "translated_title": "基因量化感知逼近用于变压器中的非线性操作",
    "translated_abstract": "在变压器及其轻量级变体中，非线性函数普遍存在，导致硬件成本显著且经常被低估。先前的最先进作品通过分段线性逼近来优化这些操作，并将参数存储在查找表（LUT）中，但大多数需要不友好的高精度算术，如FP/INT 32，并且缺乏对纯整数INT量化的考虑。本文提出了一种遗传LUT-逼近算法，即GQA-LUT，它可以自动确定具有量化意识的参数。结果表明，对于普通和线性Transformer模型的挑战性语义分割任务，GQA-LUT实现了可忽略的性能降级。此外，提出的GQA-LUT使得能够使用基于INT8的LUT逼近，相比高精度FP/INT 32，可以实现81.3~81.7%的面积节约和79.3~80.2%的功耗降低。",
    "tldr": "本文提出的GQA-LUT算法在变压器中的非线性操作中具有量化感知性，并实现了对INT8-based LUT逼近的应用，节约了大量硬件和功耗",
    "en_tdlr": "The proposed GQA-LUT algorithm exhibits quantization awareness in non-linear operations in transformers and enables the utilization of INT8-based LUT-Approximation, resulting in significant hardware and power savings."
}