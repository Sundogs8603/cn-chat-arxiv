{
    "title": "HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain Reinforcement Learning From AI Feedback",
    "abstract": "arXiv:2403.08309v1 Announce Type: cross  Abstract: Reinforcement Learning from AI Feedback (RLAIF) has the advantages of shorter annotation cycles and lower costs over Reinforcement Learning from Human Feedback (RLHF), making it highly efficient during the rapid strategy iteration periods of large language model (LLM) training. Using ChatGPT as a labeler to provide feedback on open-domain prompts in RLAIF training, we observe an increase in human evaluators' preference win ratio for model responses, but a decrease in evaluators' satisfaction rate. Analysis suggests that the decrease in satisfaction rate is mainly due to some responses becoming less helpful, particularly in terms of correctness and truthfulness, highlighting practical limitations of basic RLAIF. In this paper, we propose Hybrid Reinforcement Learning from AI Feedback (HRLAIF). This method enhances the accuracy of AI annotations for responses, making the model's helpfulness more robust in training process. Additionally, ",
    "link": "https://arxiv.org/abs/2403.08309",
    "context": "Title: HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain Reinforcement Learning From AI Feedback\nAbstract: arXiv:2403.08309v1 Announce Type: cross  Abstract: Reinforcement Learning from AI Feedback (RLAIF) has the advantages of shorter annotation cycles and lower costs over Reinforcement Learning from Human Feedback (RLHF), making it highly efficient during the rapid strategy iteration periods of large language model (LLM) training. Using ChatGPT as a labeler to provide feedback on open-domain prompts in RLAIF training, we observe an increase in human evaluators' preference win ratio for model responses, but a decrease in evaluators' satisfaction rate. Analysis suggests that the decrease in satisfaction rate is mainly due to some responses becoming less helpful, particularly in terms of correctness and truthfulness, highlighting practical limitations of basic RLAIF. In this paper, we propose Hybrid Reinforcement Learning from AI Feedback (HRLAIF). This method enhances the accuracy of AI annotations for responses, making the model's helpfulness more robust in training process. Additionally, ",
    "path": "papers/24/03/2403.08309.json",
    "total_tokens": 894,
    "translated_title": "HRLAIF: 通过AI反馈改进开放域强化学习中的帮助性和无害性",
    "translated_abstract": "强化学习从AI反馈（RLAIF）相比从人类反馈中学习（RLHF）具有更短的注释周期和更低的成本优势，使其在大型语言模型（LLM）训练的快速策略迭代阶段非常高效。使用ChatGPT作为标注员，在RLAIF训练中为开放域提示提供反馈，我们观察到人类评估者对模型响应的偏好胜率增加，但评估者的满意度下降。分析表明，满意度下降主要是因为一些响应变得不够有帮助，特别是在正确性和真实性方面，突显了基本RLAIF的实际局限性。在本文中，我们提出了混合强化学习从AI反馈（HRLAIF）。该方法增强了AI注释响应的准确性，在训练过程中使模型的帮助性更加稳健。",
    "tldr": "提出了HRLAIF方法来改善开放域强化学习中的模型响应帮助性，通过增强AI注释响应的准确性来提高模型在训练过程中的鲁棒性",
    "en_tdlr": "Introducing HRLAIF method to enhance the helpfulness of model responses in open-domain reinforcement learning by improving the accuracy of AI annotated responses to increase the robustness of the model during training."
}