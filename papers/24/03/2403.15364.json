{
    "title": "Towards Knowledge-Grounded Natural Language Understanding and Generation",
    "abstract": "arXiv:2403.15364v1 Announce Type: new  Abstract: This thesis investigates how natural language understanding and generation with transformer models can benefit from grounding the models with knowledge representations and addresses the following key research questions: (i) Can knowledge of entities extend its benefits beyond entity-centric tasks, such as entity linking? (ii) How can we faithfully and effectively extract such structured knowledge from raw text, especially noisy web text? (iii) How do other types of knowledge, beyond structured knowledge, contribute to improving NLP tasks?   Studies in this thesis find that incorporating relevant and up-to-date knowledge of entities benefits fake news detection, and entity-focused code-switching significantly enhances zero-shot cross-lingual transfer on entity-centric tasks. In terms of effective and faithful approaches to extracting structured knowledge, it is observed that integrating negative examples and training with entity planning ",
    "link": "https://arxiv.org/abs/2403.15364",
    "context": "Title: Towards Knowledge-Grounded Natural Language Understanding and Generation\nAbstract: arXiv:2403.15364v1 Announce Type: new  Abstract: This thesis investigates how natural language understanding and generation with transformer models can benefit from grounding the models with knowledge representations and addresses the following key research questions: (i) Can knowledge of entities extend its benefits beyond entity-centric tasks, such as entity linking? (ii) How can we faithfully and effectively extract such structured knowledge from raw text, especially noisy web text? (iii) How do other types of knowledge, beyond structured knowledge, contribute to improving NLP tasks?   Studies in this thesis find that incorporating relevant and up-to-date knowledge of entities benefits fake news detection, and entity-focused code-switching significantly enhances zero-shot cross-lingual transfer on entity-centric tasks. In terms of effective and faithful approaches to extracting structured knowledge, it is observed that integrating negative examples and training with entity planning ",
    "path": "papers/24/03/2403.15364.json",
    "total_tokens": 848,
    "translated_title": "实现知识驱动的自然语言理解和生成",
    "translated_abstract": "这篇论文调查了如何通过使用transformer模型的知识表示来受益自然语言理解和生成，并探讨了以下关键研究问题：(i) 实体知识能否扩展其优势至实体链接等不限于实体的任务? (ii) 我们如何忠实有效地从原始文本中提取这种结构化知识，尤其是在嘈杂的网络文本中? (iii) 除了结构化知识之外，其他类型的知识如何有助于改善自然语言处理任务? 这篇论文的研究发现，整合与实体相关和最新的知识有助于假新闻检测，而以实体为焦点的代码切换显著提高了实体相关任务的零样例跨语言转移。在提取结构化知识的有效和忠实方法方面，观察到通过整合负例和进行实体规划训练可以取得良好效果。",
    "tldr": "本论文研究了如何利用知识表示来改进自然语言理解和生成，发现整合实体相关知识有助于假新闻检测，实体焦点代码切换改善了跨语言转移性能。",
    "en_tdlr": "This paper investigates how natural language understanding and generation can benefit from grounding transformer models with knowledge representations. It finds that incorporating entity-specific knowledge improves fake news detection, and entity-focused code-switching enhances cross-lingual transfer performance."
}