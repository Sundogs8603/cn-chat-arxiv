{
    "title": "SCOD: From Heuristics to Theory",
    "abstract": "arXiv:2403.16916v1 Announce Type: new  Abstract: This paper addresses the problem of designing reliable prediction models that abstain from predictions when faced with uncertain or out-of-distribution samples - a recently proposed problem known as Selective Classification in the presence of Out-of-Distribution data (SCOD). We make three key contributions to SCOD. Firstly, we demonstrate that the optimal SCOD strategy involves a Bayes classifier for in-distribution (ID) data and a selector represented as a stochastic linear classifier in a 2D space, using i) the conditional risk of the ID classifier, and ii) the likelihood ratio of ID and out-of-distribution (OOD) data as input. This contrasts with suboptimal strategies from current OOD detection methods and the Softmax Information Retaining Combination (SIRC), specifically developed for SCOD. Secondly, we establish that in a distribution-free setting, the SCOD problem is not Probably Approximately Correct learnable when relying solely ",
    "link": "https://arxiv.org/abs/2403.16916",
    "context": "Title: SCOD: From Heuristics to Theory\nAbstract: arXiv:2403.16916v1 Announce Type: new  Abstract: This paper addresses the problem of designing reliable prediction models that abstain from predictions when faced with uncertain or out-of-distribution samples - a recently proposed problem known as Selective Classification in the presence of Out-of-Distribution data (SCOD). We make three key contributions to SCOD. Firstly, we demonstrate that the optimal SCOD strategy involves a Bayes classifier for in-distribution (ID) data and a selector represented as a stochastic linear classifier in a 2D space, using i) the conditional risk of the ID classifier, and ii) the likelihood ratio of ID and out-of-distribution (OOD) data as input. This contrasts with suboptimal strategies from current OOD detection methods and the Softmax Information Retaining Combination (SIRC), specifically developed for SCOD. Secondly, we establish that in a distribution-free setting, the SCOD problem is not Probably Approximately Correct learnable when relying solely ",
    "path": "papers/24/03/2403.16916.json",
    "total_tokens": 828,
    "translated_title": "从启发式到理论：SCOD",
    "translated_abstract": "本文解决了设计可靠的预测模型，当面对不确定或离群样本时避免预测的问题 - 一种最近提出的被称为Selective Classification in the presence of Out-of-Distribution data (SCOD)的问题。我们对SCOD做出了三个关键贡献。首先，我们证明了最优的SCOD策略涉及基于贝叶斯分类器进行内部分布（ID）数据和在2D空间中表示为随机线性分类器的选择器，使用ID分类器的条件风险和ID与离群分布（OOD）数据的似然比作为输入。这与当前OOD检测方法和专为SCOD开发的Softmax Information Retaining Combination (SIRC)的次优策略形成对比。其次，我们建立了在一个无分布设置中，当仅依赖于条件分布和IID样本的逼近可能性时，SCOD问题不可能被正确学习。",
    "tldr": "本文提出了针对SCOD问题的最优策略，包括基于贝叶斯分类器和随机线性分类器的选择器，以解决不确定或离群样本预测可靠性问题。",
    "en_tdlr": "This paper presents the optimal strategy for the SCOD problem, including a Bayes classifier and a stochastic linear classifier, to address the reliability of predictions for uncertain or out-of-distribution samples."
}