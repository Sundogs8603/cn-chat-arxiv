{
    "title": "A Multimodal Approach to Device-Directed Speech Detection with Large Language Models",
    "abstract": "arXiv:2403.14438v1 Announce Type: new  Abstract: Interactions with virtual assistants typically start with a predefined trigger phrase followed by the user command. To make interactions with the assistant more intuitive, we explore whether it is feasible to drop the requirement that users must begin each command with a trigger phrase. We explore this task in three ways: First, we train classifiers using only acoustic information obtained from the audio waveform. Second, we take the decoder outputs of an automatic speech recognition (ASR) system, such as 1-best hypotheses, as input features to a large language model (LLM). Finally, we explore a multimodal system that combines acoustic and lexical features, as well as ASR decoder signals in an LLM. Using multimodal information yields relative equal-error-rate improvements over text-only and audio-only models of up to 39% and 61%. Increasing the size of the LLM and training with low-rank adaption leads to further relative EER reductions o",
    "link": "https://arxiv.org/abs/2403.14438",
    "context": "Title: A Multimodal Approach to Device-Directed Speech Detection with Large Language Models\nAbstract: arXiv:2403.14438v1 Announce Type: new  Abstract: Interactions with virtual assistants typically start with a predefined trigger phrase followed by the user command. To make interactions with the assistant more intuitive, we explore whether it is feasible to drop the requirement that users must begin each command with a trigger phrase. We explore this task in three ways: First, we train classifiers using only acoustic information obtained from the audio waveform. Second, we take the decoder outputs of an automatic speech recognition (ASR) system, such as 1-best hypotheses, as input features to a large language model (LLM). Finally, we explore a multimodal system that combines acoustic and lexical features, as well as ASR decoder signals in an LLM. Using multimodal information yields relative equal-error-rate improvements over text-only and audio-only models of up to 39% and 61%. Increasing the size of the LLM and training with low-rank adaption leads to further relative EER reductions o",
    "path": "papers/24/03/2403.14438.json",
    "total_tokens": 858,
    "translated_title": "一种利用大型语言模型进行设备定向语音检测的多模态方法",
    "translated_abstract": "虚拟助手的交互通常从预定义触发短语开始，然后是用户命令。为了使与助手的交互更直观，我们探讨了是否可以放弃用户必须用触发短语开始每个命令的要求。我们通过三种方式探索了这个任务：首先，我们仅使用从音频波形中获得的声学信息训练分类器。其次，我们将自动语音识别（ASR）系统的解码器输出，例如1-best假设，作为输入特征输入到大型语言模型（LLM）中。最后，我们探讨了一种多模态系统，将声学和词汇特征以及ASR解码器信号结合在LLM中。使用多模态信息相对于仅文本和仅音频模型提高了相等错误率高达39%和61%。增加LLM的大小并通过低秩调整进行训练进一步减少了相对EER值的减少",
    "tldr": "探索了一种利用大型语言模型进行设备定向语音检测的多模态方法，相比于文本和音频模型，使用多模态信息能够显著提高相等错误率。",
    "en_tdlr": "Explored a multimodal approach to device-directed speech detection using large language models, which significantly improves equal-error-rate compared to text-only and audio-only models."
}