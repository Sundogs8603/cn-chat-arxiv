{
    "title": "Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMs",
    "abstract": "arXiv:2403.05434v1 Announce Type: new  Abstract: Large Language Models (LLMs) exhibit impressive zero/few-shot inference and generation quality for high-resource languages(HRLs). A few of them have been trained in low-resource languages (LRLs) and give decent performance. Owing to the prohibitive costs of training LLMs, they are usually used as a network service, with the client charged by the count of input and output tokens. The number of tokens strongly depends on the script and language, as well as the LLM's sub-word vocabulary. We show that LRLs are at a pricing disadvantage, because the well-known LLMs produce more tokens for LRLs than HRLs. This is because most currently popular LLMs are optimized for HRL vocabularies. Our objective is to level the playing field: reduce the cost of processing LRLs in contemporary LLMs while ensuring that predictive and generative qualities are not compromised. As means to reduce the number of tokens processed by the LLM, we consider code-mixing,",
    "link": "https://arxiv.org/abs/2403.05434",
    "context": "Title: Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMs\nAbstract: arXiv:2403.05434v1 Announce Type: new  Abstract: Large Language Models (LLMs) exhibit impressive zero/few-shot inference and generation quality for high-resource languages(HRLs). A few of them have been trained in low-resource languages (LRLs) and give decent performance. Owing to the prohibitive costs of training LLMs, they are usually used as a network service, with the client charged by the count of input and output tokens. The number of tokens strongly depends on the script and language, as well as the LLM's sub-word vocabulary. We show that LRLs are at a pricing disadvantage, because the well-known LLMs produce more tokens for LRLs than HRLs. This is because most currently popular LLMs are optimized for HRL vocabularies. Our objective is to level the playing field: reduce the cost of processing LRLs in contemporary LLMs while ensuring that predictive and generative qualities are not compromised. As means to reduce the number of tokens processed by the LLM, we consider code-mixing,",
    "path": "papers/24/03/2403.05434.json",
    "total_tokens": 899,
    "translated_title": "使用商业语言模型优化处理低资源语言任务的成本与性能",
    "translated_abstract": "大型语言模型(LLMs)在高资源语言上展现出令人印象深刻的零/少轮推理和生成质量。其中有一些在低资源语言(LRLs)上训练并表现出不错的性能。由于训练LLMs的成本极高，它们通常被用作网络服务，客户根据输入和输出令牌的数量付费。令牌数量强烈依赖于脚本和语言，以及LLM的子词汇表。我们表明LRLs在定价上处于不利位置，因为众所周知，对于LRLs，知名LLMs产生的令牌比HRLs多。这是因为目前大多数流行的LLMs都针对HRL词汇表进行了优化。我们的目标是在保证预测和生成质量不受损的同时，调整平衡：降低在当代LLMs中处理LRLs的成本。作为减少LLM处理的令牌数量的手段，我们考虑代码混合",
    "tldr": "该论文旨在通过考虑代码混合等手段，降低在当代LLMs中处理低资源语言任务的成本，以确保预测和生成质量不受损。",
    "en_tdlr": "This paper aims to reduce the cost of processing low-resource language tasks in contemporary LLMs by considering code-mixing and other methods, while ensuring that predictive and generative qualities are not compromised."
}