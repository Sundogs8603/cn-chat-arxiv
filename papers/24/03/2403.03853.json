{
    "title": "ShortGPT: Layers in Large Language Models are More Redundant Than You Expect",
    "abstract": "arXiv:2403.03853v1 Announce Type: new  Abstract: As Large Language Models (LLMs) continue to advance in performance, their size has escalated significantly, with current LLMs containing billions or even trillions of parameters. However, in this study, we discovered that many layers of LLMs exhibit high similarity, and some layers play a negligible role in network functionality. Based on this observation, we define a metric called Block Influence (BI) to gauge the significance of each layer in LLMs. We then propose a straightforward pruning approach: layer removal, in which we directly delete the redundant layers in LLMs based on their BI scores. Experiments demonstrate that our method, which we call ShortGPT, significantly outperforms previous state-of-the-art (SOTA) methods in model pruning. Moreover, ShortGPT is orthogonal to quantization-like methods, enabling further reduction in parameters and computation. The ability to achieve better results through simple layer removal, as oppo",
    "link": "https://arxiv.org/abs/2403.03853",
    "context": "Title: ShortGPT: Layers in Large Language Models are More Redundant Than You Expect\nAbstract: arXiv:2403.03853v1 Announce Type: new  Abstract: As Large Language Models (LLMs) continue to advance in performance, their size has escalated significantly, with current LLMs containing billions or even trillions of parameters. However, in this study, we discovered that many layers of LLMs exhibit high similarity, and some layers play a negligible role in network functionality. Based on this observation, we define a metric called Block Influence (BI) to gauge the significance of each layer in LLMs. We then propose a straightforward pruning approach: layer removal, in which we directly delete the redundant layers in LLMs based on their BI scores. Experiments demonstrate that our method, which we call ShortGPT, significantly outperforms previous state-of-the-art (SOTA) methods in model pruning. Moreover, ShortGPT is orthogonal to quantization-like methods, enabling further reduction in parameters and computation. The ability to achieve better results through simple layer removal, as oppo",
    "path": "papers/24/03/2403.03853.json",
    "total_tokens": 897,
    "translated_title": "ShortGPT: 大语言模型中的层级比您想象的更冗余",
    "translated_abstract": "随着大语言模型（LLMs）在性能上不断取得进展，其规模显著增加，当前的LLMs包含数十亿甚至数万亿个参数。然而，在这项研究中，我们发现许多LLMs的层之间存在高度相似性，并且一些层在网络功能中起到了可忽略的作用。基于这一观察，我们定义了一种称为区块影响（BI）的度量衡量LLMs中每个层的重要性。然后，我们提出了一种简单的修剪方法：层删除，即根据它们的BI得分直接删除LLMs中的冗余层。实验证明，我们的方法ShortGPT在模型修剪方面明显优于以往的最先进方法。此外，ShortGPT与量化等方法正交，可以进一步减少参数和计算。通过简单的层删除即可获得更好的结果的能力，与传统的精确修剪方法截然不同。",
    "tldr": "大语言模型中的层级存在较高相似性，有些层对网络功能几乎无影响。研究提出一种称为区块影响的度量，并通过层删除方法显著优于以往的模型修剪方法。",
    "en_tdlr": "Layers in large language models exhibit high similarity, with some layers playing a negligible role in network functionality. The study introduces a metric called Block Influence and proposes a layer removal method that significantly outperforms previous model pruning approaches."
}