{
    "title": "IndicLLMSuite: A Blueprint for Creating Pre-training and Fine-Tuning Datasets for Indian Languages",
    "abstract": "arXiv:2403.06350v1 Announce Type: new  Abstract: Despite the considerable advancements in English LLMs, the progress in building comparable models for other languages has been hindered due to the scarcity of tailored resources. Our work aims to bridge this divide by introducing an expansive suite of resources specifically designed for the development of Indic LLMs, covering 22 languages, containing a total of 251B tokens and 74.8M instruction-response pairs. Recognizing the importance of both data quality and quantity, our approach combines highly curated manually verified data, unverified yet valuable data, and synthetic data. We build a clean, open-source pipeline for curating pre-training data from diverse sources, including websites, PDFs, and videos, incorporating best practices for crawling, cleaning, flagging, and deduplication. For instruction-fine tuning, we amalgamate existing Indic datasets, translate/transliterate English datasets into Indian languages, and utilize LLaMa2 a",
    "link": "https://arxiv.org/abs/2403.06350",
    "context": "Title: IndicLLMSuite: A Blueprint for Creating Pre-training and Fine-Tuning Datasets for Indian Languages\nAbstract: arXiv:2403.06350v1 Announce Type: new  Abstract: Despite the considerable advancements in English LLMs, the progress in building comparable models for other languages has been hindered due to the scarcity of tailored resources. Our work aims to bridge this divide by introducing an expansive suite of resources specifically designed for the development of Indic LLMs, covering 22 languages, containing a total of 251B tokens and 74.8M instruction-response pairs. Recognizing the importance of both data quality and quantity, our approach combines highly curated manually verified data, unverified yet valuable data, and synthetic data. We build a clean, open-source pipeline for curating pre-training data from diverse sources, including websites, PDFs, and videos, incorporating best practices for crawling, cleaning, flagging, and deduplication. For instruction-fine tuning, we amalgamate existing Indic datasets, translate/transliterate English datasets into Indian languages, and utilize LLaMa2 a",
    "path": "papers/24/03/2403.06350.json",
    "total_tokens": 951,
    "translated_title": "IndicLLMSuite: 为印度语言创建预训练和微调数据集提供了蓝图",
    "translated_abstract": "尽管英文LLM（Large Language Models）取得了显著进展，但由于缺乏定制资源，构建其他语言的可比模型的进展受阻。我们的工作旨在通过引入一个专门为发展印度语言LLM而设计的大量资源套件来弥合这一鸿沟，涵盖了22种语言，包含总共251B标记和7480万个指导-响应对。我们认识到数据质量和数量的重要性，我们的方法结合了经过精心筛选的手动验证数据、尚未验证但有价值的数据和合成数据。我们构建了一个干净的、开源的流水线，用于从各种来源筛选预训练数据，包括网站、PDF和视频，融入了爬取、清理、标记和去重的最佳实践。对于指导微调，我们汇集了现有的印度数据集，将英文数据集翻译/转写成印度语言，并利用了LLaMa2的技术。",
    "tldr": "为印度语言创建了一个覆盖22种语言、包含251B标记和74.8M指导-响应对的资源套件，结合高度筛选的数据、有价值的未验证数据和合成数据，建立了用于筛选预训练数据的干净开源流水线，以及用于指导微调的方法。",
    "en_tdlr": "Created a suite of resources for Indian languages covering 22 languages, containing 251B tokens and 74.8M instruction-response pairs, combining curated data, valuable unverified data, and synthetic data, and established clean open-source pipeline for curating pre-training data and methods for instruction fine-tuning."
}