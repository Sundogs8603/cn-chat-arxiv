{
    "title": "Measuring Gender and Racial Biases in Large Language Models",
    "abstract": "arXiv:2403.15281v1 Announce Type: new  Abstract: In traditional decision making processes, social biases of human decision makers can lead to unequal economic outcomes for underrepresented social groups, such as women, racial or ethnic minorities. Recently, the increasing popularity of Large language model based artificial intelligence suggests a potential transition from human to AI based decision making. How would this impact the distributional outcomes across social groups? Here we investigate the gender and racial biases of OpenAIs GPT, a widely used LLM, in a high stakes decision making setting, specifically assessing entry level job candidates from diverse social groups. Instructing GPT to score approximately 361000 resumes with randomized social identities, we find that the LLM awards higher assessment scores for female candidates with similar work experience, education, and skills, while lower scores for black male candidates with comparable qualifications. These biases may res",
    "link": "https://arxiv.org/abs/2403.15281",
    "context": "Title: Measuring Gender and Racial Biases in Large Language Models\nAbstract: arXiv:2403.15281v1 Announce Type: new  Abstract: In traditional decision making processes, social biases of human decision makers can lead to unequal economic outcomes for underrepresented social groups, such as women, racial or ethnic minorities. Recently, the increasing popularity of Large language model based artificial intelligence suggests a potential transition from human to AI based decision making. How would this impact the distributional outcomes across social groups? Here we investigate the gender and racial biases of OpenAIs GPT, a widely used LLM, in a high stakes decision making setting, specifically assessing entry level job candidates from diverse social groups. Instructing GPT to score approximately 361000 resumes with randomized social identities, we find that the LLM awards higher assessment scores for female candidates with similar work experience, education, and skills, while lower scores for black male candidates with comparable qualifications. These biases may res",
    "path": "papers/24/03/2403.15281.json",
    "total_tokens": 886,
    "translated_title": "在大型语言模型中测量性别和种族偏见",
    "translated_abstract": "在传统的决策过程中，人类决策者的社会偏见可能导致妇女、种族或少数民族等代表性社会群体获得不平等的经济成果。最近，基于大型语言模型的人工智能的增长暗示着潜在的从基于人类到基于 AI 的决策过程的转变。这将如何影响社会群体之间的分配结果？在这里，我们在一个高风险的决策环境中调查了 OpenAI 的 GPT（一种广泛使用的 LLM）的性别和种族偏见，具体评估了来自不同社会群体的应聘者。指示 GPT对约361000份具有随机社会身份的简历进行评分后，我们发现该 LLM 对具有类似工作经验、教育和技能的女性候选人给予更高的评分，而对具有相当资质的黑人男性候选人给予较低的评分。这些偏见可能导致...（文本截断）",
    "tldr": "研究发现在高风险决策中，OpenAI的GPT语言模型对于具有类似背景和技能的女性候选人评分偏高，而对于类似资质的黑人男性候选人评分较低。",
    "en_tdlr": "The study found that in high-stakes decision making, OpenAI's GPT language model tends to give higher assessment scores to female candidates with similar backgrounds and skills, while giving lower scores to black male candidates with similar qualifications."
}