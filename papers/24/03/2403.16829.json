{
    "title": "Convergence of a model-free entropy-regularized inverse reinforcement learning algorithm",
    "abstract": "arXiv:2403.16829v1 Announce Type: cross  Abstract: Given a dataset of expert demonstrations, inverse reinforcement learning (IRL) aims to recover a reward for which the expert is optimal. This work proposes a model-free algorithm to solve entropy-regularized IRL problem. In particular, we employ a stochastic gradient descent update for the reward and a stochastic soft policy iteration update for the policy. Assuming access to a generative model, we prove that our algorithm is guaranteed to recover a reward for which the expert is $\\varepsilon$-optimal using $\\mathcal{O}(1/\\varepsilon^{2})$ samples of the Markov decision process (MDP). Furthermore, with $\\mathcal{O}(1/\\varepsilon^{4})$ samples we prove that the optimal policy corresponding to the recovered reward is $\\varepsilon$-close to the expert policy in total variation distance.",
    "link": "https://arxiv.org/abs/2403.16829",
    "context": "Title: Convergence of a model-free entropy-regularized inverse reinforcement learning algorithm\nAbstract: arXiv:2403.16829v1 Announce Type: cross  Abstract: Given a dataset of expert demonstrations, inverse reinforcement learning (IRL) aims to recover a reward for which the expert is optimal. This work proposes a model-free algorithm to solve entropy-regularized IRL problem. In particular, we employ a stochastic gradient descent update for the reward and a stochastic soft policy iteration update for the policy. Assuming access to a generative model, we prove that our algorithm is guaranteed to recover a reward for which the expert is $\\varepsilon$-optimal using $\\mathcal{O}(1/\\varepsilon^{2})$ samples of the Markov decision process (MDP). Furthermore, with $\\mathcal{O}(1/\\varepsilon^{4})$ samples we prove that the optimal policy corresponding to the recovered reward is $\\varepsilon$-close to the expert policy in total variation distance.",
    "path": "papers/24/03/2403.16829.json",
    "total_tokens": 862,
    "translated_title": "一个无模型的熵正则化逆强化学习算法的收敛性",
    "translated_abstract": "在给定一组专家演示数据集的情况下，逆强化学习旨在恢复一个专家表现最佳的奖励。本文提出了一个无模型的算法来解决熵正则化逆强化学习问题。具体而言，我们采用随机梯度下降更新奖励，采用随机软策略迭代更新策略。假设可以访问一个生成模型，我们证明了我们的算法能够保证使用$\\mathcal{O}(1/\\varepsilon^{2})$个马尔可夫决策过程（MDP）样本恢复出一个使专家表现最佳的奖励。此外，通过$\\mathcal{O}(1/\\varepsilon^{4})$个样本，我们证明了与恢复奖励对应的最优策略在总变差距离上与专家策略$\\varepsilon$-接近。",
    "tldr": "提出一个无模型的算法来解决熵正则化的逆强化学习问题，该算法能够使用有限样本恢复出专家表现最佳的奖励，并且最终得到的最优策略与专家策略非常接近。",
    "en_tdlr": "Introducing a model-free algorithm to address the entropy-regularized inverse reinforcement learning problem, which can recover the reward yielding expert optimality with limited samples and result in an optimal policy close to the expert's policy."
}