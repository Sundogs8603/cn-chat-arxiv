{
    "title": "Approximation of RKHS Functionals by Neural Networks",
    "abstract": "arXiv:2403.12187v1 Announce Type: cross  Abstract: Motivated by the abundance of functional data such as time series and images, there has been a growing interest in integrating such data into neural networks and learning maps from function spaces to R (i.e., functionals). In this paper, we study the approximation of functionals on reproducing kernel Hilbert spaces (RKHS's) using neural networks. We establish the universality of the approximation of functionals on the RKHS's. Specifically, we derive explicit error bounds for those induced by inverse multiquadric, Gaussian, and Sobolev kernels. Moreover, we apply our findings to functional regression, proving that neural networks can accurately approximate the regression maps in generalized functional linear models. Existing works on functional learning require integration-type basis function expansions with a set of pre-specified basis functions. By leveraging the interpolating orthogonal projections in RKHS's, our proposed network is ",
    "link": "https://arxiv.org/abs/2403.12187",
    "context": "Title: Approximation of RKHS Functionals by Neural Networks\nAbstract: arXiv:2403.12187v1 Announce Type: cross  Abstract: Motivated by the abundance of functional data such as time series and images, there has been a growing interest in integrating such data into neural networks and learning maps from function spaces to R (i.e., functionals). In this paper, we study the approximation of functionals on reproducing kernel Hilbert spaces (RKHS's) using neural networks. We establish the universality of the approximation of functionals on the RKHS's. Specifically, we derive explicit error bounds for those induced by inverse multiquadric, Gaussian, and Sobolev kernels. Moreover, we apply our findings to functional regression, proving that neural networks can accurately approximate the regression maps in generalized functional linear models. Existing works on functional learning require integration-type basis function expansions with a set of pre-specified basis functions. By leveraging the interpolating orthogonal projections in RKHS's, our proposed network is ",
    "path": "papers/24/03/2403.12187.json",
    "total_tokens": 885,
    "translated_title": "神经网络逼近RKHS函数型",
    "translated_abstract": "受到时间序列和图像等丰富功能性数据的启发，人们越来越感兴趣将这些数据整合到神经网络中，并从函数空间到R（即函数型）学习映射。本文研究了使用神经网络逼近再生核希尔伯特空间（RKHS）上的函数型。我们建立了对RKHS上函数型逼近的普适性。具体来说，我们推导了通过逆多重二次、高斯和Sobolev核引起的明确误差界限。此外，我们将我们的研究应用于函数回归，证明了神经网络可以准确逼近广义函数线性模型中的回归映射。现有的功能性学习作品需要积分型基函数展开与一组预定义的基函数。通过在RKHS中利用插值正交投影，我们提出的网络是...",
    "tldr": "本文研究了使用神经网络逼近再生核希尔伯特空间（RKHS）上的函数型，并建立了逼近的普适性，推导了逆多重二次、高斯和Sobolev核引起的误差界限，证明神经网络可以准确逼近广义函数线性模型中的回归映射。",
    "en_tdlr": "This paper studies the approximation of functionals on reproducing kernel Hilbert spaces (RKHS) using neural networks, establishes the universality of the approximation, derives explicit error bounds induced by inverse multiquadric, Gaussian, and Sobolev kernels, and proves that neural networks can accurately approximate the regression maps in generalized functional linear models."
}