{
    "title": "Counting-Stars: A Simple, Efficient, and Reasonable Strategy for Evaluating Long-Context Large Language Models",
    "abstract": "arXiv:2403.11802v1 Announce Type: new  Abstract: While recent research endeavors have concentrated on developing Large Language Models (LLMs) with robust long-context capabilities, due to the lack of appropriate evaluation strategies, relatively little is known about how well the long-context processing abilities and performance of leading LLMs (e.g., ChatGPT and KimiChat). To address this gap, we propose a simple, efficient, and reasonable strategy for evaluating long-context LLMs as a new benchmark, named Counting-Stars. The Counting-Stars is designed to require LLMs to fully understand and capture long dependencies in long contexts and be able to collect inter-dependency across multiple pieces of evidence spanning the entire context to finish the task. Based on the Counting-Stars, we conduct experiments to evaluate the two leading long-context LLMs, i.e., GPT-4 Turbo and Kimi Chat. The experimental results indicate that GPT-4 Turbo and Kimi Chat achieve significant performance in th",
    "link": "https://arxiv.org/abs/2403.11802",
    "context": "Title: Counting-Stars: A Simple, Efficient, and Reasonable Strategy for Evaluating Long-Context Large Language Models\nAbstract: arXiv:2403.11802v1 Announce Type: new  Abstract: While recent research endeavors have concentrated on developing Large Language Models (LLMs) with robust long-context capabilities, due to the lack of appropriate evaluation strategies, relatively little is known about how well the long-context processing abilities and performance of leading LLMs (e.g., ChatGPT and KimiChat). To address this gap, we propose a simple, efficient, and reasonable strategy for evaluating long-context LLMs as a new benchmark, named Counting-Stars. The Counting-Stars is designed to require LLMs to fully understand and capture long dependencies in long contexts and be able to collect inter-dependency across multiple pieces of evidence spanning the entire context to finish the task. Based on the Counting-Stars, we conduct experiments to evaluate the two leading long-context LLMs, i.e., GPT-4 Turbo and Kimi Chat. The experimental results indicate that GPT-4 Turbo and Kimi Chat achieve significant performance in th",
    "path": "papers/24/03/2403.11802.json",
    "total_tokens": 904,
    "translated_title": "Counting-Stars：一种评估长上下文大型语言模型的简单、高效、合理策略",
    "translated_abstract": "近期的研究主要集中在开发具有强大长上下文能力的大型语言模型（LLMs），由于缺乏适当的评估策略，对领先的LLMs（例如ChatGPT和KimiChat）的长上下文处理能力和性能了解甚少。为了填补这一空白，我们提出了一个简单、高效、合理的长上下文LLMs评估策略作为一个新的基准，名为Counting-Stars。Counting-Stars旨在要求LLMs充分理解和捕捉长上下文中的长依赖关系，并能够收集跨越整个上下文的多个证据之间的相互依赖来完成任务。基于Counting-Stars，我们进行实验评估了两个领先的长上下文LLMs，即GPT-4 Turbo和Kimi Chat。实验结果表明，GPT-4 Turbo和Kimi Chat在Counting-Stars任务上取得了显著的表现。",
    "tldr": "提出了一种名为Counting-Stars的简单、高效、合理策略，用于评估长上下文大型语言模型的能力，并在实验中发现GPT-4 Turbo和Kimi Chat在此任务上取得显著性能。",
    "en_tdlr": "Introduced a simple, efficient, and reasonable strategy named Counting-Stars for evaluating the capabilities of long-context large language models, and experimentally found that GPT-4 Turbo and Kimi Chat achieved significant performance in this task."
}