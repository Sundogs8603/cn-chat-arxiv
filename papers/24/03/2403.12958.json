{
    "title": "Dated Data: Tracing Knowledge Cutoffs in Large Language Models",
    "abstract": "arXiv:2403.12958v1 Announce Type: new  Abstract: Released Large Language Models (LLMs) are often paired with a claimed knowledge cutoff date, or the dates at which training data was gathered. Such information is crucial for applications where the LLM must provide up to date information. However, this statement only scratches the surface: do all resources in the training data share the same knowledge cutoff date? Does the model's demonstrated knowledge for these subsets closely align to their cutoff dates? In this work, we define the notion of an effective cutoff. This is distinct from the LLM designer reported cutoff and applies separately to sub-resources and topics. We propose a simple approach to estimate effective cutoffs on the resource-level temporal alignment of an LLM by probing across versions of the data. Using this analysis, we find that effective cutoffs often differ from reported cutoffs. To understand the root cause of this observation, we conduct a direct large-scale ana",
    "link": "https://arxiv.org/abs/2403.12958",
    "context": "Title: Dated Data: Tracing Knowledge Cutoffs in Large Language Models\nAbstract: arXiv:2403.12958v1 Announce Type: new  Abstract: Released Large Language Models (LLMs) are often paired with a claimed knowledge cutoff date, or the dates at which training data was gathered. Such information is crucial for applications where the LLM must provide up to date information. However, this statement only scratches the surface: do all resources in the training data share the same knowledge cutoff date? Does the model's demonstrated knowledge for these subsets closely align to their cutoff dates? In this work, we define the notion of an effective cutoff. This is distinct from the LLM designer reported cutoff and applies separately to sub-resources and topics. We propose a simple approach to estimate effective cutoffs on the resource-level temporal alignment of an LLM by probing across versions of the data. Using this analysis, we find that effective cutoffs often differ from reported cutoffs. To understand the root cause of this observation, we conduct a direct large-scale ana",
    "path": "papers/24/03/2403.12958.json",
    "total_tokens": 852,
    "translated_title": "数据的时效性：在大型语言模型中追踪知识截止日期",
    "translated_abstract": "发布的大型语言模型通常配有声称的知识截止日期，即获取训练数据的日期。这些信息对于需要语言模型提供最新信息的应用至关重要。然而，这一说法只是表面现象：训练数据中的所有资源是否都具有相同的知识截止日期？模型对这些子集的展示知识是否与它们的截止日期密切相关？在这项工作中，我们定义了有效截止日期的概念。这与语言模型设计者报告的截止日期不同，分别适用于子资源和主题。我们提出了一种简单的方法，通过探测数据版本之间的时间对齐性来估计语言模型在资源级别的有效截止日期。通过这项分析，我们发现有效截止日期通常与报告的截止日期不同。为了了解这一观察结果的根本原因，我们进行了直接的大规模分析。",
    "tldr": "本文提出了在大型语言模型中追踪知识截止日期的概念，通过资源级别的时间对齐性估计有效截止日期，并发现这些截止日期通常与报道的不同。",
    "en_tdlr": "This paper introduces the concept of tracing knowledge cutoffs in large language models, estimates effective cutoff dates through resource-level temporal alignment, and finds that these cutoff dates often differ from the reported ones."
}