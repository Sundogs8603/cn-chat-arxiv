{
    "title": "On Uncertainty Quantification for Near-Bayes Optimal Algorithms",
    "abstract": "arXiv:2403.19381v1 Announce Type: cross  Abstract: Bayesian modelling allows for the quantification of predictive uncertainty which is crucial in safety-critical applications. Yet for many machine learning (ML) algorithms, it is difficult to construct or implement their Bayesian counterpart. In this work we present a promising approach to address this challenge, based on the hypothesis that commonly used ML algorithms are efficient across a wide variety of tasks and may thus be near Bayes-optimal w.r.t. an unknown task distribution. We prove that it is possible to recover the Bayesian posterior defined by the task distribution, which is unknown but optimal in this setting, by building a martingale posterior using the algorithm. We further propose a practical uncertainty quantification method that apply to general ML algorithms. Experiments based on a variety of non-NN and NN algorithms demonstrate the efficacy of our method.",
    "link": "https://arxiv.org/abs/2403.19381",
    "context": "Title: On Uncertainty Quantification for Near-Bayes Optimal Algorithms\nAbstract: arXiv:2403.19381v1 Announce Type: cross  Abstract: Bayesian modelling allows for the quantification of predictive uncertainty which is crucial in safety-critical applications. Yet for many machine learning (ML) algorithms, it is difficult to construct or implement their Bayesian counterpart. In this work we present a promising approach to address this challenge, based on the hypothesis that commonly used ML algorithms are efficient across a wide variety of tasks and may thus be near Bayes-optimal w.r.t. an unknown task distribution. We prove that it is possible to recover the Bayesian posterior defined by the task distribution, which is unknown but optimal in this setting, by building a martingale posterior using the algorithm. We further propose a practical uncertainty quantification method that apply to general ML algorithms. Experiments based on a variety of non-NN and NN algorithms demonstrate the efficacy of our method.",
    "path": "papers/24/03/2403.19381.json",
    "total_tokens": 804,
    "translated_title": "关于近贝叶斯最优算法的不确定性量化",
    "translated_abstract": "贝叶斯建模允许对预测不确定性进行量化，在安全关键应用中至关重要。然而，对于许多机器学习（ML）算法，构建或实现它们的贝叶斯对应是困难的。 在这项工作中，我们提出了一种解决这一挑战的有前途的方法，该方法基于常用的ML算法在各种任务中高效，并且可能在未知任务分布下接近贝叶斯最优。我们证明了通过使用该算法构建一个鞅后验，可以恢复由任务分布定义的贝叶斯后验，在这种设置中是未知但最优的。我们进一步提出了一种适用于通用ML算法的实用不确定性量化方法。基于各种非NN和NN算法的实验表明了我们方法的效果。",
    "tldr": "该论文提出了一种基于常用机器学习算法的近似贝叶斯最优方法，可以恢复由未知任务分布定义的贝叶斯后验，并提出了一种通用的不确定性量化方法。",
    "en_tdlr": "This paper proposes an approximate Bayes-optimal method based on commonly used machine learning algorithms, which can recover the Bayesian posterior defined by an unknown task distribution, and introduces a general uncertainty quantification method."
}