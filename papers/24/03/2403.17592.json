{
    "title": "On the Benefits of Over-parameterization for Out-of-Distribution Generalization",
    "abstract": "arXiv:2403.17592v1 Announce Type: new  Abstract: In recent years, machine learning models have achieved success based on the independently and identically distributed assumption. However, this assumption can be easily violated in real-world applications, leading to the Out-of-Distribution (OOD) problem. Understanding how modern over-parameterized DNNs behave under non-trivial natural distributional shifts is essential, as current theoretical understanding is insufficient. Existing theoretical works often provide meaningless results for over-parameterized models in OOD scenarios or even contradict empirical findings. To this end, we are investigating the performance of the over-parameterized model in terms of OOD generalization under the general benign overfitting conditions. Our analysis focuses on a random feature model and examines non-trivial natural distributional shifts, where the benign overfitting estimators demonstrate a constant excess OOD loss, despite achieving zero excess i",
    "link": "https://arxiv.org/abs/2403.17592",
    "context": "Title: On the Benefits of Over-parameterization for Out-of-Distribution Generalization\nAbstract: arXiv:2403.17592v1 Announce Type: new  Abstract: In recent years, machine learning models have achieved success based on the independently and identically distributed assumption. However, this assumption can be easily violated in real-world applications, leading to the Out-of-Distribution (OOD) problem. Understanding how modern over-parameterized DNNs behave under non-trivial natural distributional shifts is essential, as current theoretical understanding is insufficient. Existing theoretical works often provide meaningless results for over-parameterized models in OOD scenarios or even contradict empirical findings. To this end, we are investigating the performance of the over-parameterized model in terms of OOD generalization under the general benign overfitting conditions. Our analysis focuses on a random feature model and examines non-trivial natural distributional shifts, where the benign overfitting estimators demonstrate a constant excess OOD loss, despite achieving zero excess i",
    "path": "papers/24/03/2403.17592.json",
    "total_tokens": 824,
    "translated_title": "对超参数化对于超出分布泛化的益处",
    "translated_abstract": "在最近几年，基于独立同分布假设的机器学习模型取得了成功。然而，这一假设在现实世界的应用中很容易被违反，导致了超出分布（OOD）问题。理解现代超参数化深度神经网络在非平凡自然分布偏移下的行为是至关重要的，因为目前对其在理论上的理解是不足的。现有的理论工作常常为OOD场景中的超参数化模型提供无意义的结果，甚至与实证结果相矛盾。为此，我们正在研究在一般良性过拟合条件下，超参数化模型在OOD泛化方面的性能。我们的分析集中在随机特征模型上，并研究非平凡自然分布偏移，其中良性过拟合估计器展示出恒定的过大OOD损失，尽管达到了零过大i",
    "tldr": "研究了超参数化模型在超出分布泛化方面的表现，探讨了在良性过拟合条件下的表现，并发现了恒定的超出分布损失。"
}