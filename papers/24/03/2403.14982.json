{
    "title": "MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of Chain-of-Thoughts",
    "abstract": "arXiv:2403.14982v1 Announce Type: new  Abstract: Our paper presents team MasonTigers submission to the SemEval-2024 Task 9 - which provides a dataset of puzzles for testing natural language understanding. We employ large language models (LLMs) to solve this task through several prompting techniques. Zero-shot and few-shot prompting generate reasonably good results when tested with proprietary LLMs, compared to the open-source models. We obtain further improved results with chain-of-thought prompting, an iterative prompting method that breaks down the reasoning process step-by-step. We obtain our best results by utilizing an ensemble of chain-of-thought prompts, placing 2nd in the word puzzle subtask and 13th in the sentence puzzle subtask. The strong performance of prompted LLMs demonstrates their capability for complex reasoning when provided with a decomposition of the thought process. Our work sheds light on how step-wise explanatory prompts can unlock more of the knowledge encoded ",
    "link": "https://arxiv.org/abs/2403.14982",
    "context": "Title: MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of Chain-of-Thoughts\nAbstract: arXiv:2403.14982v1 Announce Type: new  Abstract: Our paper presents team MasonTigers submission to the SemEval-2024 Task 9 - which provides a dataset of puzzles for testing natural language understanding. We employ large language models (LLMs) to solve this task through several prompting techniques. Zero-shot and few-shot prompting generate reasonably good results when tested with proprietary LLMs, compared to the open-source models. We obtain further improved results with chain-of-thought prompting, an iterative prompting method that breaks down the reasoning process step-by-step. We obtain our best results by utilizing an ensemble of chain-of-thought prompts, placing 2nd in the word puzzle subtask and 13th in the sentence puzzle subtask. The strong performance of prompted LLMs demonstrates their capability for complex reasoning when provided with a decomposition of the thought process. Our work sheds light on how step-wise explanatory prompts can unlock more of the knowledge encoded ",
    "path": "papers/24/03/2403.14982.json",
    "total_tokens": 920,
    "translated_title": "MasonTigers在SemEval-2024 Task 9中：使用一系列思维链解决谜题",
    "translated_abstract": "我们的论文介绍了团队MasonTigers提交给SemEval-2024 Task 9的作品，该任务提供了一组谜题数据集用于测试自然语言理解。我们利用大型语言模型（LLMs）通过几种提示技术来解决这个任务。零参考和少量参考提示通过专有LLMs测试时取得了相当不错的结果，相较于开源模型。通过思维链提示进一步提升了结果，这是一种迭代提示方法，逐步分解推理过程。通过利用一系列思维链提示，我们获得最好的结果，在单词谜题子任务中排名第二，在句子谜题子任务中排名第13。提示LLMs的强大表现显示了它们在提供思考过程分解时进行复杂推理的能力。我们的工作揭示了逐步解释性提示如何能够更多地揭示已编码的知识。",
    "tldr": "这项研究使用大型语言模型解决SemEval-2024 Task 9的谜题任务，通过一系列思维链提示技术，包括零参考和少量参考提示，以及思维链提示，最终取得了显著的结果，展示了逐步解释性提示如何可以更好地揭示已编码的知识。",
    "en_tdlr": "This study tackles the SemEval-2024 Task 9 puzzles using large language models and a series of prompting techniques, including zero-shot and few-shot prompting, as well as chain-of-thought prompting, achieving significant results and demonstrating how step-wise explanatory prompts can uncover encoded knowledge."
}