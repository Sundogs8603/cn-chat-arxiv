{
    "title": "Conservative DDPG -- Pessimistic RL without Ensemble",
    "abstract": "arXiv:2403.05732v1 Announce Type: new  Abstract: DDPG is hindered by the overestimation bias problem, wherein its $Q$-estimates tend to overstate the actual $Q$-values. Traditional solutions to this bias involve ensemble-based methods, which require significant computational resources, or complex log-policy-based approaches, which are difficult to understand and implement. In contrast, we propose a straightforward solution using a $Q$-target and incorporating a behavioral cloning (BC) loss penalty. This solution, acting as an uncertainty measure, can be easily implemented with minimal code and without the need for an ensemble. Our empirical findings strongly support the superiority of Conservative DDPG over DDPG across various MuJoCo and Bullet tasks. We consistently observe better performance in all evaluated tasks and even competitive or superior performance compared to TD3 and TD7, all achieved with significantly reduced computational requirements.",
    "link": "https://arxiv.org/abs/2403.05732",
    "context": "Title: Conservative DDPG -- Pessimistic RL without Ensemble\nAbstract: arXiv:2403.05732v1 Announce Type: new  Abstract: DDPG is hindered by the overestimation bias problem, wherein its $Q$-estimates tend to overstate the actual $Q$-values. Traditional solutions to this bias involve ensemble-based methods, which require significant computational resources, or complex log-policy-based approaches, which are difficult to understand and implement. In contrast, we propose a straightforward solution using a $Q$-target and incorporating a behavioral cloning (BC) loss penalty. This solution, acting as an uncertainty measure, can be easily implemented with minimal code and without the need for an ensemble. Our empirical findings strongly support the superiority of Conservative DDPG over DDPG across various MuJoCo and Bullet tasks. We consistently observe better performance in all evaluated tasks and even competitive or superior performance compared to TD3 and TD7, all achieved with significantly reduced computational requirements.",
    "path": "papers/24/03/2403.05732.json",
    "total_tokens": 883,
    "translated_title": "保守DDPG - 无集成的悲观强化学习",
    "translated_abstract": "DDPG受到高估偏差问题的阻碍，其中其$Q$-估计倾向于夸大实际$Q$值。传统解决这一偏见的方法涉及基于集成的方法，需要大量计算资源，或者基于复杂对数策略的方法，难以理解和实施。相比之下，我们提出了一种简单的解决方案，使用$Q$-目标并结合行为克隆（BC）损失惩罚。这种解决方案作为一种不确定性度量，可以很容易地用较少的代码实现，而无需集成。我们的实证结果强烈支持保守DDPG在各种MuJoCo和Bullet任务上优于DDPG。我们始终观察到在所有评估任务中表现更好，甚至在与TD3和TD7相比性能更有竞争力或更优越，所有这些都是以显著降低的计算要求实现的。",
    "tldr": "提出了一种新的保守DDPG方法，通过引入$Q$-目标和行为克隆损失惩罚来解决DDPG中的高估偏差问题，可以在不需要集成的情况下轻松实现，并且在各种任务中表现优异。",
    "en_tdlr": "A new Conservative DDPG method is proposed to address the overestimation bias problem in DDPG by introducing a Q-target and a behavioral cloning loss penalty, which can be easily implemented without the need for an ensemble and shows superior performance across various tasks."
}