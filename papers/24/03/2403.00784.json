{
    "title": "Utilizing BERT for Information Retrieval: Survey, Applications, Resources, and Challenges",
    "abstract": "arXiv:2403.00784v1 Announce Type: cross  Abstract: Recent years have witnessed a substantial increase in the use of deep learning to solve various natural language processing (NLP) problems. Early deep learning models were constrained by their sequential or unidirectional nature, such that they struggled to capture the contextual relationships across text inputs. The introduction of bidirectional encoder representations from transformers (BERT) leads to a robust encoder for the transformer model that can understand the broader context and deliver state-of-the-art performance across various NLP tasks. This has inspired researchers and practitioners to apply BERT to practical problems, such as information retrieval (IR). A survey that focuses on a comprehensive analysis of prevalent approaches that apply pretrained transformer encoders like BERT to IR can thus be useful for academia and the industry. In light of this, we revisit a variety of BERT-based methods in this survey, cover a wid",
    "link": "https://arxiv.org/abs/2403.00784",
    "context": "Title: Utilizing BERT for Information Retrieval: Survey, Applications, Resources, and Challenges\nAbstract: arXiv:2403.00784v1 Announce Type: cross  Abstract: Recent years have witnessed a substantial increase in the use of deep learning to solve various natural language processing (NLP) problems. Early deep learning models were constrained by their sequential or unidirectional nature, such that they struggled to capture the contextual relationships across text inputs. The introduction of bidirectional encoder representations from transformers (BERT) leads to a robust encoder for the transformer model that can understand the broader context and deliver state-of-the-art performance across various NLP tasks. This has inspired researchers and practitioners to apply BERT to practical problems, such as information retrieval (IR). A survey that focuses on a comprehensive analysis of prevalent approaches that apply pretrained transformer encoders like BERT to IR can thus be useful for academia and the industry. In light of this, we revisit a variety of BERT-based methods in this survey, cover a wid",
    "path": "papers/24/03/2403.00784.json",
    "total_tokens": 859,
    "translated_title": "利用BERT进行信息检索：调研、应用、资源和挑战",
    "translated_abstract": "近年来，深度学习在解决各种自然语言处理（NLP）问题方面得到了显著增长。最初的深度学习模型受到它们顺序或单向性质的限制，因此难以捕捉文本输入之间的上下文关系。从变压器（BERT）中引入的双向编码器表征提供了变压器模型的强大编码器，可以理解更广泛的上下文，并在各种NLP任务中获得最先进的性能。这激发了研究人员和从业者将BERT应用于实际问题，如信息检索（IR）。因此，一项关注将预训练的变压器编码器如BERT应用于IR的普遍方法的综合分析的调查对学术界和工业界都有用。鉴于此，本调查重新审视了各种基于BERT的方法，涵盖了各种方法",
    "tldr": "BERT的引入为信息检索领域带来了突破，研究者们将其应用于解决实际问题，并通过综合分析其在信息检索中的应用方法，为学术界和工业界提供了有益的参考。",
    "en_tdlr": "The introduction of BERT has brought breakthroughs in the field of information retrieval, as researchers apply it to solve practical problems and provide a comprehensive analysis of its applications in information retrieval for academia and industry."
}