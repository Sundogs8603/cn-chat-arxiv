{
    "title": "Vid2Robot: End-to-end Video-conditioned Policy Learning with Cross-Attention Transformers",
    "abstract": "arXiv:2403.12943v1 Announce Type: cross  Abstract: While large-scale robotic systems typically rely on textual instructions for tasks, this work explores a different approach: can robots infer the task directly from observing humans? This shift necessitates the robot's ability to decode human intent and translate it into executable actions within its physical constraints and environment. We introduce Vid2Robot, a novel end-to-end video-based learning framework for robots. Given a video demonstration of a manipulation task and current visual observations, Vid2Robot directly produces robot actions. This is achieved through a unified representation model trained on a large dataset of human video and robot trajectory. The model leverages cross-attention mechanisms to fuse prompt video features to the robot's current state and generate appropriate actions that mimic the observed task. To further improve policy performance, we propose auxiliary contrastive losses that enhance the alignment b",
    "link": "https://arxiv.org/abs/2403.12943",
    "context": "Title: Vid2Robot: End-to-end Video-conditioned Policy Learning with Cross-Attention Transformers\nAbstract: arXiv:2403.12943v1 Announce Type: cross  Abstract: While large-scale robotic systems typically rely on textual instructions for tasks, this work explores a different approach: can robots infer the task directly from observing humans? This shift necessitates the robot's ability to decode human intent and translate it into executable actions within its physical constraints and environment. We introduce Vid2Robot, a novel end-to-end video-based learning framework for robots. Given a video demonstration of a manipulation task and current visual observations, Vid2Robot directly produces robot actions. This is achieved through a unified representation model trained on a large dataset of human video and robot trajectory. The model leverages cross-attention mechanisms to fuse prompt video features to the robot's current state and generate appropriate actions that mimic the observed task. To further improve policy performance, we propose auxiliary contrastive losses that enhance the alignment b",
    "path": "papers/24/03/2403.12943.json",
    "total_tokens": 853,
    "translated_title": "Vid2Robot：基于视频条件化策略学习的端到端交叉注意力变换器",
    "translated_abstract": "尽管大规模机器人系统通常依赖文本指令进行任务，但这项工作探索了一种不同的方法：机器人能否直接从观察人类推断任务？这种转变要求机器人能够解码人类意图，并将其转化为可在其物理约束和环境内执行的动作。我们引入了Vid2Robot，这是一种新颖的面向机器人的端到端基于视频的学习框架。给定一个操作任务的视频演示和当前的视觉观察，Vid2Robot直接生成机器人动作。这是通过在大规模人类视频和机器人轨迹数据集上训练的统一表示模型实现的。该模型利用交叉注意力机制来融合提示视频特征与机器人的当前状态，并生成模仿所观察任务的适当动作。为了进一步提高策略性能，我们提出了辅助对比损失，以增强对齐",
    "tldr": "Vid2Robot提出了一种新颖的端到端视频条件化策略学习框架，通过交叉注意力机制融合视频特征和机器人状态，直接生成模仿所观察任务的动作。",
    "en_tdlr": "Vid2Robot introduces a novel end-to-end video-conditioned policy learning framework that directly generates actions mimicking the observed task by fusing video features and robot state through cross-attention mechanisms."
}