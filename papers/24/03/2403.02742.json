{
    "title": "Towards Training A Chinese Large Language Model for Anesthesiology",
    "abstract": "arXiv:2403.02742v1 Announce Type: new  Abstract: Medical large language models (LLMs) have gained popularity recently due to their significant practical utility. However, most existing research focuses on general medicine, and there is a need for in-depth study of LLMs in specific fields like anesthesiology. To fill the gap, we introduce Hypnos, a Chinese Anesthesia model built upon existing LLMs, e.g., Llama. Hypnos' contributions have three aspects: 1) The data, such as utilizing Self-Instruct, acquired from current LLMs likely includes inaccuracies. Hypnos implements a cross-filtering strategy to improve the data quality. This strategy involves using one LLM to assess the quality of the generated data from another LLM and filtering out the data with low quality. 2) Hypnos employs a general-to-specific training strategy that starts by fine-tuning LLMs using the general medicine data and subsequently improving the fine-tuned LLMs using data specifically from Anesthesiology. The genera",
    "link": "https://arxiv.org/abs/2403.02742",
    "context": "Title: Towards Training A Chinese Large Language Model for Anesthesiology\nAbstract: arXiv:2403.02742v1 Announce Type: new  Abstract: Medical large language models (LLMs) have gained popularity recently due to their significant practical utility. However, most existing research focuses on general medicine, and there is a need for in-depth study of LLMs in specific fields like anesthesiology. To fill the gap, we introduce Hypnos, a Chinese Anesthesia model built upon existing LLMs, e.g., Llama. Hypnos' contributions have three aspects: 1) The data, such as utilizing Self-Instruct, acquired from current LLMs likely includes inaccuracies. Hypnos implements a cross-filtering strategy to improve the data quality. This strategy involves using one LLM to assess the quality of the generated data from another LLM and filtering out the data with low quality. 2) Hypnos employs a general-to-specific training strategy that starts by fine-tuning LLMs using the general medicine data and subsequently improving the fine-tuned LLMs using data specifically from Anesthesiology. The genera",
    "path": "papers/24/03/2403.02742.json",
    "total_tokens": 895,
    "translated_title": "面向麻醉学的中文大型语言模型训练",
    "translated_abstract": "最近，由于其显著的实用价值，医学大型语言模型（LLMs）受到了广泛关注。然而，大多数现有研究集中在一般医学领域，需要深入研究特定领域如麻醉学中的LLMs。为此，我们介绍了Hypnos，一种基于现有LLMs（如Llama）构建的中文麻醉模型。Hypnos的贡献包括三个方面：1）数据，例如来自当前LLMs的Self-Instruct获得的数据可能存在不准确性。Hypnos实施了交叉过滤策略来提高数据质量。该策略涉及使用一个LLM来评估另一个LLM生成的数据的质量，并过滤出质量较低的数据。2）Hypnos采用了一种由一般到特定的训练策略，即通过使用一般医学数据微调LLMs，然后再使用来自麻醉学的数据进一步改进微调的LLMs。",
    "tldr": "Hypnos通过改善数据质量和采用由一般到特定的训练策略，为面向麻醉学的中文大型语言模型的建设做出了重要贡献。"
}