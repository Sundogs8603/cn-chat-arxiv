{
    "title": "ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching",
    "abstract": "arXiv:2403.17312v1 Announce Type: new  Abstract: The Transformer architecture has significantly advanced natural language processing (NLP) and has been foundational in developing large language models (LLMs) such as LLaMA and OPT, which have come to dominate a broad range of NLP tasks. Despite their superior accuracy, LLMs present unique challenges in practical inference, concerning the compute and memory-intensive nature. Thanks to the autoregressive characteristic of LLM inference, KV caching for the attention layers in Transformers can effectively accelerate LLM inference by substituting quadratic-complexity computation with linear-complexity memory accesses. Yet, this approach requires increasing memory as demand grows for processing longer sequences. The overhead leads to reduced throughput due to I/O bottlenecks and even out-of-memory errors, particularly on resource-constrained systems like a single commodity GPU. In this paper, we propose ALISA, a novel algorithm-system co-desi",
    "link": "https://arxiv.org/abs/2403.17312",
    "context": "Title: ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching\nAbstract: arXiv:2403.17312v1 Announce Type: new  Abstract: The Transformer architecture has significantly advanced natural language processing (NLP) and has been foundational in developing large language models (LLMs) such as LLaMA and OPT, which have come to dominate a broad range of NLP tasks. Despite their superior accuracy, LLMs present unique challenges in practical inference, concerning the compute and memory-intensive nature. Thanks to the autoregressive characteristic of LLM inference, KV caching for the attention layers in Transformers can effectively accelerate LLM inference by substituting quadratic-complexity computation with linear-complexity memory accesses. Yet, this approach requires increasing memory as demand grows for processing longer sequences. The overhead leads to reduced throughput due to I/O bottlenecks and even out-of-memory errors, particularly on resource-constrained systems like a single commodity GPU. In this paper, we propose ALISA, a novel algorithm-system co-desi",
    "path": "papers/24/03/2403.17312.json",
    "total_tokens": 814,
    "translated_title": "ALISA: 通过稀疏感知KV缓存加速大型语言模型推理",
    "translated_abstract": "Transformer架构显著推动了自然语言处理（NLP）的发展，并且在开发大型语言模型（LLMs）方面具有基础性作用，如LLaMA和OPT，这些模型已经在广泛的NLP任务中占据主导地位。尽管它们具有优越的准确性，但LLMs在实际推理中存在独特挑战，涉及计算和占用大量内存。由于LLM推理具有自回归特性，Transformer中的注意层的KV缓存可以通过将二次复杂度计算替换为线性复杂度内存访问，从而有效加速LLM推理。然而，随着对处理更长序列的需求增加，这种方法需要增加内存。这种开销导致由于I/O瓶颈和甚至是内存不足错误而导致吞吐量降低，特别是在资源受限的系统上，如单个通用GPU上。",
    "tldr": "提出了ALISA，一种通过稀疏感知KV缓存加速大型语言模型推理的新算法系统设计。",
    "en_tdlr": "Proposed ALISA, a novel algorithm-system design to accelerate large language model inference via sparsity-aware KV caching."
}