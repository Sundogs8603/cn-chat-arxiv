{
    "title": "Detecting Hallucination and Coverage Errors in Retrieval Augmented Generation for Controversial Topics",
    "abstract": "arXiv:2403.08904v1 Announce Type: new  Abstract: We explore a strategy to handle controversial topics in LLM-based chatbots based on Wikipedia's Neutral Point of View (NPOV) principle: acknowledge the absence of a single true answer and surface multiple perspectives. We frame this as retrieval augmented generation, where perspectives are retrieved from a knowledge base and the LLM is tasked with generating a fluent and faithful response from the given perspectives. As a starting point, we use a deterministic retrieval system and then focus on common LLM failure modes that arise during this approach to text generation, namely hallucination and coverage errors. We propose and evaluate three methods to detect such errors based on (1) word-overlap, (2) salience, and (3) LLM-based classifiers. Our results demonstrate that LLM-based classifiers, even when trained only on synthetic errors, achieve high error detection performance, with ROC AUC scores of 95.3% for hallucination and 90.5% for c",
    "link": "https://arxiv.org/abs/2403.08904",
    "context": "Title: Detecting Hallucination and Coverage Errors in Retrieval Augmented Generation for Controversial Topics\nAbstract: arXiv:2403.08904v1 Announce Type: new  Abstract: We explore a strategy to handle controversial topics in LLM-based chatbots based on Wikipedia's Neutral Point of View (NPOV) principle: acknowledge the absence of a single true answer and surface multiple perspectives. We frame this as retrieval augmented generation, where perspectives are retrieved from a knowledge base and the LLM is tasked with generating a fluent and faithful response from the given perspectives. As a starting point, we use a deterministic retrieval system and then focus on common LLM failure modes that arise during this approach to text generation, namely hallucination and coverage errors. We propose and evaluate three methods to detect such errors based on (1) word-overlap, (2) salience, and (3) LLM-based classifiers. Our results demonstrate that LLM-based classifiers, even when trained only on synthetic errors, achieve high error detection performance, with ROC AUC scores of 95.3% for hallucination and 90.5% for c",
    "path": "papers/24/03/2403.08904.json",
    "total_tokens": 932,
    "translated_title": "在争议性话题中检测检索增强生成中的虚构和覆盖错误",
    "translated_abstract": "我们探讨了一种处理基于LLM的聊天机器人中争议性话题的策略，该策略基于维基百科的中立观点（NPOV）原则：承认不存在一个真实答案，并呈现多个观点。我们将此称为检索增强生成，在此方法中，从知识库中检索观点，然后LLM负责从给定的观点生成流畅且忠实的响应。我们首先使用确定性检索系统，然后专注于在这种文本生成方法中出现的常见LLM失败模式，即虚构和覆盖错误。我们提出并评估了三种基于（1）词重叠，（2）显著性和（3）基于LLM的分类器的方法来检测此类错误。我们的结果表明，基于LLM的分类器，即使只在合成错误上进行训练，也能实现高错误检测性能，虚构错误的ROC AUC分数为95.3%，覆盖错误的ROC AUC分数为90.5%。",
    "tldr": "提出了一种检测LLM在生成文本过程中虚构和覆盖错误的方法，通过基于词重叠、显著性和分类器的三种策略，即使在合成错误上训练，也能实现高错误检测性能，表现出快速和有效的能力。",
    "en_tdlr": "A method for detecting hallucination and coverage errors in text generation by LLM was proposed, achieving high error detection performance through three strategies based on word-overlap, salience, and classifiers, even when trained on synthetic errors."
}