{
    "title": "AttentionStitch: How Attention Solves the Speech Editing Problem",
    "abstract": "arXiv:2403.04804v1 Announce Type: cross  Abstract: The generation of natural and high-quality speech from text is a challenging problem in the field of natural language processing. In addition to speech generation, speech editing is also a crucial task, which requires the seamless and unnoticeable integration of edited speech into synthesized speech. We propose a novel approach to speech editing by leveraging a pre-trained text-to-speech (TTS) model, such as FastSpeech 2, and incorporating a double attention block network on top of it to automatically merge the synthesized mel-spectrogram with the mel-spectrogram of the edited text. We refer to this model as AttentionStitch, as it harnesses attention to stitch audio samples together. We evaluate the proposed AttentionStitch model against state-of-the-art baselines on both single and multi-speaker datasets, namely LJSpeech and VCTK. We demonstrate its superior performance through an objective and a subjective evaluation test involving 1",
    "link": "https://arxiv.org/abs/2403.04804",
    "context": "Title: AttentionStitch: How Attention Solves the Speech Editing Problem\nAbstract: arXiv:2403.04804v1 Announce Type: cross  Abstract: The generation of natural and high-quality speech from text is a challenging problem in the field of natural language processing. In addition to speech generation, speech editing is also a crucial task, which requires the seamless and unnoticeable integration of edited speech into synthesized speech. We propose a novel approach to speech editing by leveraging a pre-trained text-to-speech (TTS) model, such as FastSpeech 2, and incorporating a double attention block network on top of it to automatically merge the synthesized mel-spectrogram with the mel-spectrogram of the edited text. We refer to this model as AttentionStitch, as it harnesses attention to stitch audio samples together. We evaluate the proposed AttentionStitch model against state-of-the-art baselines on both single and multi-speaker datasets, namely LJSpeech and VCTK. We demonstrate its superior performance through an objective and a subjective evaluation test involving 1",
    "path": "papers/24/03/2403.04804.json",
    "total_tokens": 820,
    "translated_title": "AttentionStitch：关注如何解决语音编辑问题",
    "translated_abstract": "自然高质量的语音生成是自然语言处理领域中一个具有挑战性的问题。除了语音生成之外，语音编辑也是一个关键任务，需要将编辑后的语音无缝、不易察觉地整合到合成语音中。我们提出了一种新颖的语音编辑方法，利用预训练的文本到语音（TTS）模型，如FastSpeech 2，并在其之上加入双注意力块网络，以自动将合成的mel频谱图与编辑文本的mel频谱图融合在一起。我们将这个模型称为AttentionStitch，因为它利用注意力来将音频样本拼接在一起。我们在单个和多个说话者数据集（LJSpeech和VCTK）上对所提出的AttentionStitch模型与最先进的基线模型进行评估。通过客观和主观评估测试，我们证明了其优越的性能。",
    "tldr": "AttentionStitch模型通过引入双注意力块网络，并将编辑文本的mel频谱图与合成的mel频谱图自动融合，实现了语音编辑的无缝整合。",
    "en_tdlr": "The AttentionStitch model achieves seamless integration of speech editing by introducing a double attention block network to automatically merge the mel-spectrogram of edited text with the synthesized mel-spectrogram."
}