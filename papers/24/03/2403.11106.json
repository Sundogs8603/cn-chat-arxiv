{
    "title": "Self-Supervised Quantization-Aware Knowledge Distillation",
    "abstract": "arXiv:2403.11106v1 Announce Type: cross  Abstract: Quantization-aware training (QAT) and Knowledge Distillation (KD) are combined to achieve competitive performance in creating low-bit deep learning models. However, existing works applying KD to QAT require tedious hyper-parameter tuning to balance the weights of different loss terms, assume the availability of labeled training data, and require complex, computationally intensive training procedures for good performance. To address these limitations, this paper proposes a novel Self-Supervised Quantization-Aware Knowledge Distillation (SQAKD) framework. SQAKD first unifies the forward and backward dynamics of various quantization functions, making it flexible for incorporating various QAT works. Then it formulates QAT as a co-optimization problem that simultaneously minimizes the KL-Loss between the full-precision and low-bit models for KD and the discretization error for quantization, without supervision from labels. A comprehensive e",
    "link": "https://arxiv.org/abs/2403.11106",
    "context": "Title: Self-Supervised Quantization-Aware Knowledge Distillation\nAbstract: arXiv:2403.11106v1 Announce Type: cross  Abstract: Quantization-aware training (QAT) and Knowledge Distillation (KD) are combined to achieve competitive performance in creating low-bit deep learning models. However, existing works applying KD to QAT require tedious hyper-parameter tuning to balance the weights of different loss terms, assume the availability of labeled training data, and require complex, computationally intensive training procedures for good performance. To address these limitations, this paper proposes a novel Self-Supervised Quantization-Aware Knowledge Distillation (SQAKD) framework. SQAKD first unifies the forward and backward dynamics of various quantization functions, making it flexible for incorporating various QAT works. Then it formulates QAT as a co-optimization problem that simultaneously minimizes the KL-Loss between the full-precision and low-bit models for KD and the discretization error for quantization, without supervision from labels. A comprehensive e",
    "path": "papers/24/03/2403.11106.json",
    "total_tokens": 893,
    "translated_title": "自监督量化感知知识蒸馏",
    "translated_abstract": "遗憾地，现有工作将知识蒸馏应用于量化感知训练(QAT)需要繁琐的超参数调整来平衡不同损失项的权重，假定有标记的训练数据可用，并且需要复杂、计算密集的训练程序以获得良好的性能。为了解决这些限制，本文提出了一种新颖的自监督量化感知知识蒸馏(SQAKD)框架。SQAKD首先统一了各种量化函数的前向和反向动态，使其可以灵活地整合各种QAT工作。然后，它将QAT形式化为一个联合优化问题，同时最小化了用于KD的全精度模型和低比特模型之间的KL损失，以及用于量化的离散化误差，而无需来自标签的监督。",
    "tldr": "提出了一种自监督量化感知知识蒸馏(SQAKD)框架，可以在不需要标记的监督情况下，同时最小化全精度和低比特模型之间的KL损失以及量化的离散化误差，从而避免了繁琐的超参数调整和复杂的训练过程。",
    "en_tdlr": "Proposed a Self-Supervised Quantization-Aware Knowledge Distillation (SQAKD) framework that can simultaneously minimize the KL loss between full-precision and low-bit models for KD and the discretization error for quantization without supervision from labels, avoiding tedious hyper-parameter tuning and complex training procedures."
}