{
    "title": "JEP-KD: Joint-Embedding Predictive Architecture Based Knowledge Distillation for Visual Speech Recognition",
    "abstract": "arXiv:2403.18843v1 Announce Type: cross  Abstract: Visual Speech Recognition (VSR) tasks are generally recognized to have a lower theoretical performance ceiling than Automatic Speech Recognition (ASR), owing to the inherent limitations of conveying semantic information visually. To mitigate this challenge, this paper introduces an advanced knowledge distillation approach using a Joint-Embedding Predictive Architecture (JEPA), named JEP-KD, designed to more effectively utilize audio features during model training. Central to JEP-KD is the inclusion of a generative network within the embedding layer, which enhances the video encoder's capacity for semantic feature extraction and brings it into closer alignment with the audio features from a pre-trained ASR model's encoder. This approach aims to progressively reduce the performance gap between VSR and ASR. Moreover, a comprehensive multimodal, multistage training regimen for the JEP-KD framework is established, bolstering the robustness ",
    "link": "https://arxiv.org/abs/2403.18843",
    "context": "Title: JEP-KD: Joint-Embedding Predictive Architecture Based Knowledge Distillation for Visual Speech Recognition\nAbstract: arXiv:2403.18843v1 Announce Type: cross  Abstract: Visual Speech Recognition (VSR) tasks are generally recognized to have a lower theoretical performance ceiling than Automatic Speech Recognition (ASR), owing to the inherent limitations of conveying semantic information visually. To mitigate this challenge, this paper introduces an advanced knowledge distillation approach using a Joint-Embedding Predictive Architecture (JEPA), named JEP-KD, designed to more effectively utilize audio features during model training. Central to JEP-KD is the inclusion of a generative network within the embedding layer, which enhances the video encoder's capacity for semantic feature extraction and brings it into closer alignment with the audio features from a pre-trained ASR model's encoder. This approach aims to progressively reduce the performance gap between VSR and ASR. Moreover, a comprehensive multimodal, multistage training regimen for the JEP-KD framework is established, bolstering the robustness ",
    "path": "papers/24/03/2403.18843.json",
    "total_tokens": 951,
    "translated_title": "JEP-KD：基于联合嵌入预测架构的视觉语音识别知识蒸馏",
    "translated_abstract": "视觉语音识别（VSR）任务通常被认为具有比自动语音识别（ASR）更低的理论性能上限，这是由于通过视觉方式传达语义信息的固有限制。为了减轻这一挑战，本文介绍了一种先进的知识蒸馏方法，使用一个名为JEP-KD的Joint-Embedding Predictive Architecture（JEPA），旨在更有效地利用音频特征进行模型训练。JEP-KD的核心在于在嵌入层内包含一个生成网络，增强了视频编码器对语义特征提取的能力，并使其与预先训练的ASR模型编码器的音频特征更加接近。该方法旨在逐渐减少VSR和ASR之间的性能差距。此外，还建立了一套全面的多模态、多阶段训练方案，以增强JEP-KD框架的稳健性。",
    "tldr": "该论文提出了一种基于联合嵌入预测架构的知识蒸馏方法JEP-KD，旨在通过引入生成网络在嵌入层内增强视频编码器的语义特征提取能力，从而更有效地利用音频特征，逐步减少视觉语音识别与自动语音识别之间的性能差距。",
    "en_tdlr": "This paper introduces a knowledge distillation approach JEP-KD based on Joint-Embedding Predictive Architecture to enhance the semantic feature extraction capability of the video encoder by introducing a generative network within the embedding layer, aiming to utilize audio features more effectively and progressively reduce the performance gap between visual speech recognition and automatic speech recognition."
}