{
    "title": "Deep Bayesian Future Fusion for Self-Supervised, High-Resolution, Off-Road Mapping",
    "abstract": "arXiv:2403.11876v1 Announce Type: cross  Abstract: The limited sensing resolution of resource-constrained off-road vehicles poses significant challenges towards reliable off-road autonomy. To overcome this limitation, we propose a general framework based on fusing the future information (i.e. future fusion) for self-supervision. Recent approaches exploit this future information alongside the hand-crafted heuristics to directly supervise the targeted downstream tasks (e.g. traversability estimation). However, in this paper, we opt for a more general line of development - time-efficient completion of the highest resolution (i.e. 2cm per pixel) BEV map in a self-supervised manner via future fusion, which can be used for any downstream tasks for better longer range prediction. To this end, first, we create a high-resolution future-fusion dataset containing pairs of (RGB / height) raw sparse and noisy inputs and map-based dense labels. Next, to accommodate the noise and sparsity of the sens",
    "link": "https://arxiv.org/abs/2403.11876",
    "context": "Title: Deep Bayesian Future Fusion for Self-Supervised, High-Resolution, Off-Road Mapping\nAbstract: arXiv:2403.11876v1 Announce Type: cross  Abstract: The limited sensing resolution of resource-constrained off-road vehicles poses significant challenges towards reliable off-road autonomy. To overcome this limitation, we propose a general framework based on fusing the future information (i.e. future fusion) for self-supervision. Recent approaches exploit this future information alongside the hand-crafted heuristics to directly supervise the targeted downstream tasks (e.g. traversability estimation). However, in this paper, we opt for a more general line of development - time-efficient completion of the highest resolution (i.e. 2cm per pixel) BEV map in a self-supervised manner via future fusion, which can be used for any downstream tasks for better longer range prediction. To this end, first, we create a high-resolution future-fusion dataset containing pairs of (RGB / height) raw sparse and noisy inputs and map-based dense labels. Next, to accommodate the noise and sparsity of the sens",
    "path": "papers/24/03/2403.11876.json",
    "total_tokens": 898,
    "translated_title": "深度贝叶斯未来融合用于自监督、高分辨率、越野地图制作",
    "translated_abstract": "资源受限的越野车辆的传感器分辨率有限，这给可靠的越野自主性带来了巨大挑战。为了克服这一局限性，我们提出了一个基于融合未来信息（即未来融合）进行自监督的通用框架。最近的方法利用未来信息以及手工制作的启发式方法来直接监督目标下游任务（例如可穿越性估计）。然而，在本文中，我们选择了一个更为通用的发展方向 - 通过未来融合以自监督的方式时间高效地完成最高分辨率（即每像素2厘米）BEV地图，可用于任何下游任务以获得更好的长程预测。为此，首先，我们创建了一个高分辨率未来融合数据集，其中包含（RGB / 高度）原始稀疏噪音输入和基于地图的密集标签的成对数据。接下来，为了适应传感器的噪声和稀疏性",
    "tldr": "该论文提出了一种深度贝叶斯未来融合的方法，通过自监督的方式实现高分辨率越野地图的制作，为长程预测提供更好的支持。",
    "en_tdlr": "The paper proposes a Deep Bayesian Future Fusion method to create high-resolution off-road maps in a self-supervised manner, providing better support for long-range prediction."
}