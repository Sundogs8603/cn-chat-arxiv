{
    "title": "VariErr NLI: Separating Annotation Error from Human Label Variation",
    "abstract": "arXiv:2403.01931v1 Announce Type: new  Abstract: Human label variation arises when annotators assign different labels to the same item for valid reasons, while annotation errors occur when labels are assigned for invalid reasons. These two issues are prevalent in NLP benchmarks, yet existing research has studied them in isolation. To the best of our knowledge, there exists no prior work that focuses on teasing apart error from signal, especially in cases where signal is beyond black-and-white. To fill this gap, we introduce a systematic methodology and a new dataset, VariErr (variation versus error), focusing on the NLI task in English. We propose a 2-round annotation scheme with annotators explaining each label and subsequently judging the validity of label-explanation pairs. \\name{} contains 7,574 validity judgments on 1,933 explanations for 500 re-annotated NLI items. We assess the effectiveness of various automatic error detection (AED) methods and GPTs in uncovering errors versus ",
    "link": "https://arxiv.org/abs/2403.01931",
    "context": "Title: VariErr NLI: Separating Annotation Error from Human Label Variation\nAbstract: arXiv:2403.01931v1 Announce Type: new  Abstract: Human label variation arises when annotators assign different labels to the same item for valid reasons, while annotation errors occur when labels are assigned for invalid reasons. These two issues are prevalent in NLP benchmarks, yet existing research has studied them in isolation. To the best of our knowledge, there exists no prior work that focuses on teasing apart error from signal, especially in cases where signal is beyond black-and-white. To fill this gap, we introduce a systematic methodology and a new dataset, VariErr (variation versus error), focusing on the NLI task in English. We propose a 2-round annotation scheme with annotators explaining each label and subsequently judging the validity of label-explanation pairs. \\name{} contains 7,574 validity judgments on 1,933 explanations for 500 re-annotated NLI items. We assess the effectiveness of various automatic error detection (AED) methods and GPTs in uncovering errors versus ",
    "path": "papers/24/03/2403.01931.json",
    "total_tokens": 865,
    "translated_title": "VariErr NLI: 将注释错误与人类标签变化区分开来",
    "translated_abstract": "人类标签变化是由于注释者出于有效原因将不同标签分配给同一项而产生的，而注释错误是指由于无效原因分配标签。这两个问题在自然语言处理基准中普遍存在，但现有研究通常是孤立研究它们。据我们所知，以前没有专注于区分错误与信号的先前工作，特别是在信号超越黑白之处。为了填补这一空白，我们介绍了一种系统方法和一个新数据集VariErr（变异与错误），重点关注英语NLI任务。我们提出了一个包含两轮注释方案的方法，注释者解释每个标签，然后判断标签解释对的有效性。VariErr包含对500个重新注释的NLI项目上的1,933个解释进行的7,574个有效性判断。我们评估了各种自动错误检测（AED）方法和GPT在揭示错误与信号之间的有效性。",
    "tldr": "该研究提出了一个新的方法和数据集VariErr，专注于NLI任务中的注释错误和人类标签变化的区分。研究填补了在处理信号非黑白情况下的先前空白。",
    "en_tdlr": "This study introduces a new methodology and dataset VariErr, focusing on distinguishing annotation errors from human label variation in the NLI task, addressing the gap in handling non-black-and-white signals."
}