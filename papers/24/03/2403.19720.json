{
    "title": "Meta-Learning with Generalized Ridge Regression: High-dimensional Asymptotics, Optimality and Hyper-covariance Estimation",
    "abstract": "arXiv:2403.19720v1 Announce Type: cross  Abstract: Meta-learning involves training models on a variety of training tasks in a way that enables them to generalize well on new, unseen test tasks. In this work, we consider meta-learning within the framework of high-dimensional multivariate random-effects linear models and study generalized ridge-regression based predictions. The statistical intuition of using generalized ridge regression in this setting is that the covariance structure of the random regression coefficients could be leveraged to make better predictions on new tasks. Accordingly, we first characterize the precise asymptotic behavior of the predictive risk for a new test task when the data dimension grows proportionally to the number of samples per task. We next show that this predictive risk is optimal when the weight matrix in generalized ridge regression is chosen to be the inverse of the covariance matrix of random coefficients. Finally, we propose and analyze an estimat",
    "link": "https://arxiv.org/abs/2403.19720",
    "context": "Title: Meta-Learning with Generalized Ridge Regression: High-dimensional Asymptotics, Optimality and Hyper-covariance Estimation\nAbstract: arXiv:2403.19720v1 Announce Type: cross  Abstract: Meta-learning involves training models on a variety of training tasks in a way that enables them to generalize well on new, unseen test tasks. In this work, we consider meta-learning within the framework of high-dimensional multivariate random-effects linear models and study generalized ridge-regression based predictions. The statistical intuition of using generalized ridge regression in this setting is that the covariance structure of the random regression coefficients could be leveraged to make better predictions on new tasks. Accordingly, we first characterize the precise asymptotic behavior of the predictive risk for a new test task when the data dimension grows proportionally to the number of samples per task. We next show that this predictive risk is optimal when the weight matrix in generalized ridge regression is chosen to be the inverse of the covariance matrix of random coefficients. Finally, we propose and analyze an estimat",
    "path": "papers/24/03/2403.19720.json",
    "total_tokens": 923,
    "translated_title": "具有广义岭回归的元学习：高维渐近性、最优性和超协方差估计",
    "translated_abstract": "Meta-learning即元学习，指的是以一种方式在多个训练任务上训练模型，使之能够在新的、未见过的测试任务上有很好的泛化能力。本文将元学习纳入高维多元随机效应线性模型框架中，并研究基于广义岭回归的预测。在该设定下使用广义岭回归的统计直觉是，随机回归系数的协方差结构可以被利用来在新任务上做出更好的预测。我们首先详细描述了在数据维度与每个任务样本数成比例增长时，对于新测试任务的预测风险的精确渐近行为。接着我们证明了当广义岭回归中的权重矩阵选择为随机系数的协方差矩阵的逆时，这种预测风险是最优的。最后，我们提出并分析了一种估计方法。",
    "tldr": "本研究在高维多元随机效应线性模型框架下研究了元学习，在使用广义岭回归进行预测时发现，利用随机回归系数的协方差结构可以在新任务上做出更好的预测，并提出了一种最优的权重矩阵选择方法。",
    "en_tdlr": "This study investigates meta-learning within the framework of high-dimensional multivariate random-effects linear models, finding that leveraging the covariance structure of random regression coefficients with generalized ridge regression leads to better predictions on new tasks, and proposes an optimal weight matrix selection method."
}