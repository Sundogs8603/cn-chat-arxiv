{
    "title": "A prediction rigidity formalism for low-cost uncertainties in trained neural networks",
    "abstract": "arXiv:2403.02251v1 Announce Type: cross  Abstract: Regression methods are fundamental for scientific and technological applications. However, fitted models can be highly unreliable outside of their training domain, and hence the quantification of their uncertainty is crucial in many of their applications. Based on the solution of a constrained optimization problem, we propose \"prediction rigidities\" as a method to obtain uncertainties of arbitrary pre-trained regressors. We establish a strong connection between our framework and Bayesian inference, and we develop a last-layer approximation that allows the new method to be applied to neural networks. This extension affords cheap uncertainties without any modification to the neural network itself or its training procedure. We show the effectiveness of our method on a wide range of regression tasks, ranging from simple toy models to applications in chemistry and meteorology.",
    "link": "https://arxiv.org/abs/2403.02251",
    "context": "Title: A prediction rigidity formalism for low-cost uncertainties in trained neural networks\nAbstract: arXiv:2403.02251v1 Announce Type: cross  Abstract: Regression methods are fundamental for scientific and technological applications. However, fitted models can be highly unreliable outside of their training domain, and hence the quantification of their uncertainty is crucial in many of their applications. Based on the solution of a constrained optimization problem, we propose \"prediction rigidities\" as a method to obtain uncertainties of arbitrary pre-trained regressors. We establish a strong connection between our framework and Bayesian inference, and we develop a last-layer approximation that allows the new method to be applied to neural networks. This extension affords cheap uncertainties without any modification to the neural network itself or its training procedure. We show the effectiveness of our method on a wide range of regression tasks, ranging from simple toy models to applications in chemistry and meteorology.",
    "path": "papers/24/03/2403.02251.json",
    "total_tokens": 814,
    "translated_title": "一个用于训练神经网络中低成本不确定性的预测刚性形式主义",
    "translated_abstract": "回归方法对科学和技术应用至关重要。然而，拟合模型在其训练领域之外可能极不可靠，因此在许多应用中，量化其不确定性是至关重要的。基于受限优化问题的解，我们提出“预测刚性”作为一种获得任意预先训练回归器不确定性的方法。我们建立了我们的框架与贝叶斯推断之间的强连接，并开发了一个允许新方法应用于神经网络的最后一层逼近。这种扩展提供了不需要对神经网络本身或其训练过程进行任何修改的低成本不确定性。我们展示了我们的方法在从简单玩具模型到化学和气象学应用的广泛回归任务中的有效性。",
    "tldr": "通过解决受限优化问题，提出了“预测刚性”作为一种获得任意预先训练回归器不确定性的方法，扩展了方法应用于神经网络，并在多种回归任务上展示了其有效性。",
    "en_tdlr": "\"Proposed \"prediction rigidities\" as a method to obtain uncertainties of arbitrary pre-trained regressors through solving a constrained optimization problem, extended the method to be applied to neural networks, and demonstrated its effectiveness on various regression tasks.\""
}