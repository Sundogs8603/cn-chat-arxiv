{
    "title": "SUN Team's Contribution to ABAW 2024 Competition: Audio-visual Valence-Arousal Estimation and Expression Recognition",
    "abstract": "arXiv:2403.12609v1 Announce Type: new  Abstract: As emotions play a central role in human communication, automatic emotion recognition has attracted increasing attention in the last two decades. While multimodal systems enjoy high performances on lab-controlled data, they are still far from providing ecological validity on non-lab-controlled, namely 'in-the-wild' data. This work investigates audiovisual deep learning approaches for emotion recognition in-the-wild problem. We particularly explore the effectiveness of architectures based on fine-tuned Convolutional Neural Networks (CNN) and Public Dimensional Emotion Model (PDEM), for video and audio modality, respectively. We compare alternative temporal modeling and fusion strategies using the embeddings from these multi-stage trained modality-specific Deep Neural Networks (DNN). We report results on the AffWild2 dataset under Affective Behavior Analysis in-the-Wild 2024 (ABAW'24) challenge protocol.",
    "link": "https://arxiv.org/abs/2403.12609",
    "context": "Title: SUN Team's Contribution to ABAW 2024 Competition: Audio-visual Valence-Arousal Estimation and Expression Recognition\nAbstract: arXiv:2403.12609v1 Announce Type: new  Abstract: As emotions play a central role in human communication, automatic emotion recognition has attracted increasing attention in the last two decades. While multimodal systems enjoy high performances on lab-controlled data, they are still far from providing ecological validity on non-lab-controlled, namely 'in-the-wild' data. This work investigates audiovisual deep learning approaches for emotion recognition in-the-wild problem. We particularly explore the effectiveness of architectures based on fine-tuned Convolutional Neural Networks (CNN) and Public Dimensional Emotion Model (PDEM), for video and audio modality, respectively. We compare alternative temporal modeling and fusion strategies using the embeddings from these multi-stage trained modality-specific Deep Neural Networks (DNN). We report results on the AffWild2 dataset under Affective Behavior Analysis in-the-Wild 2024 (ABAW'24) challenge protocol.",
    "path": "papers/24/03/2403.12609.json",
    "total_tokens": 925,
    "translated_title": "SUN团队对ABAW 2024比赛的贡献：音视频Valence-Arousal估计和表情识别",
    "translated_abstract": "随着情绪在人类交流中起着核心作用，自动情绪识别在过去二十年中吸引了越来越多的关注。尽管多模态系统在实验室控制的数据上表现出较高的性能，但它们仍远未能在非实验室控制的“野外”数据上提供生态效度。本文研究了用于野外情绪识别的音视频深度学习方法。我们特别探讨基于微调的卷积神经网络（CNN）和Public Dimensional Emotion Model (PDEM) 的架构对视频和音频模态的有效性。我们比较了使用这些多阶段训练的模态特定深度神经网络（DNN）的嵌入进行替代时间建模和融合策略。我们报告了在Affective Behavior Analysis in-the-Wild 2024 (ABAW'24) 挑战协议下使用AffWild2数据集的结果。",
    "tldr": "该研究探索了音视频深度学习方法在野外情绪识别问题中的有效性，针对视频和音频模态分别基于微调的CNN和PDEM架构，比较了不同的时间建模和融合策略，并在ABAW'24挑战中取得了实验结果。",
    "en_tdlr": "This study investigates the effectiveness of audiovisual deep learning approaches for emotion recognition in-the-wild, using fine-tuned CNN and PDEM architectures for video and audio modalities respectively, comparing various temporal modeling and fusion strategies, and reporting results under the ABAW'24 challenge protocol."
}