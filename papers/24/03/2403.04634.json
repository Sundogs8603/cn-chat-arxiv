{
    "title": "Pix2Gif: Motion-Guided Diffusion for GIF Generation",
    "abstract": "arXiv:2403.04634v1 Announce Type: cross  Abstract: We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video) generation. We tackle this problem differently by formulating the task as an image translation problem steered by text and motion magnitude prompts, as shown in teaser fig. To ensure that the model adheres to motion guidance, we propose a new motion-guided warping module to spatially transform the features of the source image conditioned on the two types of prompts. Furthermore, we introduce a perceptual loss to ensure the transformed feature map remains within the same space as the target image, ensuring content consistency and coherence. In preparation for the model training, we meticulously curated data by extracting coherent image frames from the TGIF video-caption dataset, which provides rich information about the temporal changes of subjects. After pretraining, we apply our model in a zero-shot manner to a number of video datasets. Extensive qualitative ",
    "link": "https://arxiv.org/abs/2403.04634",
    "context": "Title: Pix2Gif: Motion-Guided Diffusion for GIF Generation\nAbstract: arXiv:2403.04634v1 Announce Type: cross  Abstract: We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video) generation. We tackle this problem differently by formulating the task as an image translation problem steered by text and motion magnitude prompts, as shown in teaser fig. To ensure that the model adheres to motion guidance, we propose a new motion-guided warping module to spatially transform the features of the source image conditioned on the two types of prompts. Furthermore, we introduce a perceptual loss to ensure the transformed feature map remains within the same space as the target image, ensuring content consistency and coherence. In preparation for the model training, we meticulously curated data by extracting coherent image frames from the TGIF video-caption dataset, which provides rich information about the temporal changes of subjects. After pretraining, we apply our model in a zero-shot manner to a number of video datasets. Extensive qualitative ",
    "path": "papers/24/03/2403.04634.json",
    "total_tokens": 884,
    "translated_title": "Pix2Gif：基于运动引导扩散的GIF生成模型",
    "translated_abstract": "我们提出了Pix2Gif，这是一个基于运动引导的扩散模型，用于图像到GIF（视频）的生成。我们通过将任务构建为由文本和运动大小提示指导的图像转换问题来不同地解决这一问题，如teaser fig所示。为了确保模型遵循运动引导，我们提出了一个新的运动引导变形模块，以在两种类型的提示条件下空间变换源图像的特征。此外，我们引入了一个感知损失，以确保转换的特征图保持在与目标图像相同的空间中，确保内容一致性和连贯性。为了为模型训练做准备，我们通过从TGIF视频字幕数据集中提取连贯的图像帧来精心筛选数据，该数据集提供了有关主题的时间变化丰富信息。在预训练之后，我们以零射样的方式将我们的模型应用于多个视频数据集。",
    "tldr": "提出了Pix2Gif，一种基于运动引导的扩散模型，通过图像转换问题来实现图像到GIF的生成，引入了新的运动引导变形模块和感知损失以确保模型遵循运动引导并保持内容一致性和连贯性。",
    "en_tdlr": "Pix2Gif is a motion-guided diffusion model for generating GIFs from images, introducing a new motion-guided warping module and perceptual loss to ensure adherence to motion guidance and content consistency and coherence."
}