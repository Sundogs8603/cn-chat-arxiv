{
    "title": "NASH: Neural Architecture Search for Hardware-Optimized Machine Learning Models",
    "abstract": "arXiv:2403.01845v1 Announce Type: cross  Abstract: As machine learning (ML) algorithms get deployed in an ever-increasing number of applications, these algorithms need to achieve better trade-offs between high accuracy, high throughput and low latency. This paper introduces NASH, a novel approach that applies neural architecture search to machine learning hardware. Using NASH, hardware designs can achieve not only high throughput and low latency but also superior accuracy performance. We present four versions of the NASH strategy in this paper, all of which show higher accuracy than the original models. The strategy can be applied to various convolutional neural networks, selecting specific model operations among many to guide the training process toward higher accuracy. Experimental results show that applying NASH on ResNet18 or ResNet34 achieves a top 1 accuracy increase of up to 3.1% and a top 5 accuracy increase of up to 2.2% compared to the non-NASH version when tested on the Imag",
    "link": "https://arxiv.org/abs/2403.01845",
    "context": "Title: NASH: Neural Architecture Search for Hardware-Optimized Machine Learning Models\nAbstract: arXiv:2403.01845v1 Announce Type: cross  Abstract: As machine learning (ML) algorithms get deployed in an ever-increasing number of applications, these algorithms need to achieve better trade-offs between high accuracy, high throughput and low latency. This paper introduces NASH, a novel approach that applies neural architecture search to machine learning hardware. Using NASH, hardware designs can achieve not only high throughput and low latency but also superior accuracy performance. We present four versions of the NASH strategy in this paper, all of which show higher accuracy than the original models. The strategy can be applied to various convolutional neural networks, selecting specific model operations among many to guide the training process toward higher accuracy. Experimental results show that applying NASH on ResNet18 or ResNet34 achieves a top 1 accuracy increase of up to 3.1% and a top 5 accuracy increase of up to 2.2% compared to the non-NASH version when tested on the Imag",
    "path": "papers/24/03/2403.01845.json",
    "total_tokens": 862,
    "translated_title": "NASH：用于硬件优化机器学习模型的神经架构搜索",
    "translated_abstract": "随着机器学习（ML）算法在越来越多的应用中部署，这些算法需要在高准确性、高吞吐量和低延迟之间取得更好的权衡。本文介绍了一种名为NASH的新方法，将神经架构搜索应用于机器学习硬件。使用NASH，硬件设计不仅可以实现高吞吐量和低延迟，还可以实现优越的准确性表现。本文提出了四个版本的NASH策略，所有这些策略显示出比原始模型更高的准确性。该策略可以应用于各种卷积神经网络，从众多模型操作中选择特定操作，引导训练过程朝向更高的准确性。实验结果显示，在 ResNet18 或 ResNet34 上应用NASH，与非NASH版本相比，可使Top1准确率提高高达3.1%，Top5准确率提高高达2.2%。",
    "tldr": "NASH是一种将神经架构搜索应用于机器学习硬件的新方法，可以帮助硬件设计实现高吞吐量、低延迟和优越的准确性表现。"
}