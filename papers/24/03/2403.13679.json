{
    "title": "RoleInteract: Evaluating the Social Interaction of Role-Playing Agents",
    "abstract": "arXiv:2403.13679v1 Announce Type: new  Abstract: Large language models (LLMs) have advanced the development of various AI conversational agents, including role-playing conversational agents that mimic diverse characters and human behaviors. While prior research has predominantly focused on enhancing the conversational capability, role-specific knowledge, and stylistic attributes of these agents, there has been a noticeable gap in assessing their social intelligence. In this paper, we introduce RoleInteract, the first benchmark designed to systematically evaluate the sociality of role-playing conversational agents at both individual and group levels of social interactions. The benchmark is constructed from a variety of sources and covers a wide range of 500 characters and over 6,000 question prompts and 30,800 multi-turn role-playing utterances. We conduct comprehensive evaluations on this benchmark using mainstream open-source and closed-source LLMs. We find that agents excelling in in",
    "link": "https://arxiv.org/abs/2403.13679",
    "context": "Title: RoleInteract: Evaluating the Social Interaction of Role-Playing Agents\nAbstract: arXiv:2403.13679v1 Announce Type: new  Abstract: Large language models (LLMs) have advanced the development of various AI conversational agents, including role-playing conversational agents that mimic diverse characters and human behaviors. While prior research has predominantly focused on enhancing the conversational capability, role-specific knowledge, and stylistic attributes of these agents, there has been a noticeable gap in assessing their social intelligence. In this paper, we introduce RoleInteract, the first benchmark designed to systematically evaluate the sociality of role-playing conversational agents at both individual and group levels of social interactions. The benchmark is constructed from a variety of sources and covers a wide range of 500 characters and over 6,000 question prompts and 30,800 multi-turn role-playing utterances. We conduct comprehensive evaluations on this benchmark using mainstream open-source and closed-source LLMs. We find that agents excelling in in",
    "path": "papers/24/03/2403.13679.json",
    "total_tokens": 706,
    "translated_title": "RoleInteract：评估角色扮演代理的社交互动",
    "translated_abstract": "大型语言模型（LLMs）推动了各种AI对话代理的发展，包括模仿不同角色和人类行为的角色扮演对话代理。本文引入了RoleInteract，这是第一个旨在系统评估角色扮演对话代理在社交方面表现的基准。该基准从各种来源构建，涵盖了超过500个角色、6000多个问题提示和30800个多轮角色扮演话语。",
    "tldr": "该论文介绍了RoleInteract，一个旨在评估角色扮演对话代理社交性的基准，覆盖了500个角色、6000多个问题提示和30800个对话话语。"
}