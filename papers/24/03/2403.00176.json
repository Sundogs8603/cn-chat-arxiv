{
    "title": "SoD$^2$: Statically Optimizing Dynamic Deep Neural Network",
    "abstract": "arXiv:2403.00176v1 Announce Type: cross  Abstract: Though many compilation and runtime systems have been developed for DNNs in recent years, the focus has largely been on static DNNs. Dynamic DNNs, where tensor shapes and sizes and even the set of operators used are dependent upon the input and/or execution, are becoming common. This paper presents SoD$^2$, a comprehensive framework for optimizing Dynamic DNNs. The basis of our approach is a classification of common operators that form DNNs, and the use of this classification towards a Rank and Dimension Propagation (RDP) method. This framework statically determines the shapes of operators as known constants, symbolic constants, or operations on these. Next, using RDP we enable a series of optimizations, like fused code generation, execution (order) planning, and even runtime memory allocation plan generation. By evaluating the framework on 10 emerging Dynamic DNNs and comparing it against several existing systems, we demonstrate both ",
    "link": "https://arxiv.org/abs/2403.00176",
    "context": "Title: SoD$^2$: Statically Optimizing Dynamic Deep Neural Network\nAbstract: arXiv:2403.00176v1 Announce Type: cross  Abstract: Though many compilation and runtime systems have been developed for DNNs in recent years, the focus has largely been on static DNNs. Dynamic DNNs, where tensor shapes and sizes and even the set of operators used are dependent upon the input and/or execution, are becoming common. This paper presents SoD$^2$, a comprehensive framework for optimizing Dynamic DNNs. The basis of our approach is a classification of common operators that form DNNs, and the use of this classification towards a Rank and Dimension Propagation (RDP) method. This framework statically determines the shapes of operators as known constants, symbolic constants, or operations on these. Next, using RDP we enable a series of optimizations, like fused code generation, execution (order) planning, and even runtime memory allocation plan generation. By evaluating the framework on 10 emerging Dynamic DNNs and comparing it against several existing systems, we demonstrate both ",
    "path": "papers/24/03/2403.00176.json",
    "total_tokens": 906,
    "translated_title": "SoD$^2$: 静态优化动态深度神经网络",
    "translated_abstract": "虽然近年来已开发了许多针对DNN的编译和运行时系统，但主要集中在静态DNN上。动态DNN，其中张量形状和大小甚至使用的操作符集取决于输入和/或执行，正在变得常见。本文提出了SoD$^2$，一个用于优化动态DNN的综合框架。我们方法的基础是对构成DNN的常见操作符进行分类，并利用这一分类方法来实现秩和维度传播（RDP）方法。该框架静态确定操作符的形状为已知常量、符号常量或这些操作的运算。接下来，使用RDP我们实现一系列优化，如融合代码生成、执行（顺序）计划，甚至运行时内存分配计划生成。通过在 10 个新兴动态DNN 上评估该框架，并将其与几个现有系统进行比较，我们展示了",
    "tldr": "本文提出了SoD$^2$框架，用于静态优化动态深度神经网络，通过秩和维度传播（RDP）方法实现了操作符的形状静态确定，进而进行一系列优化，包括融合代码生成、执行计划和运行时内存分配计划生成。",
    "en_tdlr": "This paper presents the SoD$^2$ framework for statically optimizing dynamic deep neural networks, which statically determines the shapes of operators using Rank and Dimension Propagation (RDP) method, enabling a series of optimizations including fused code generation, execution planning, and runtime memory allocation planning."
}