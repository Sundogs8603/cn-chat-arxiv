{
    "title": "Federated Linear Contextual Bandits with Heterogeneous Clients",
    "abstract": "arXiv:2403.00116v1 Announce Type: cross  Abstract: The demand for collaborative and private bandit learning across multiple agents is surging due to the growing quantity of data generated from distributed systems. Federated bandit learning has emerged as a promising framework for private, efficient, and decentralized online learning. However, almost all previous works rely on strong assumptions of client homogeneity, i.e., all participating clients shall share the same bandit model; otherwise, they all would suffer linear regret. This greatly restricts the application of federated bandit learning in practice. In this work, we introduce a new approach for federated bandits for heterogeneous clients, which clusters clients for collaborative bandit learning under the federated learning setting. Our proposed algorithm achieves non-trivial sub-linear regret and communication cost for all clients, subject to the communication protocol under federated learning that at anytime only one model c",
    "link": "https://arxiv.org/abs/2403.00116",
    "context": "Title: Federated Linear Contextual Bandits with Heterogeneous Clients\nAbstract: arXiv:2403.00116v1 Announce Type: cross  Abstract: The demand for collaborative and private bandit learning across multiple agents is surging due to the growing quantity of data generated from distributed systems. Federated bandit learning has emerged as a promising framework for private, efficient, and decentralized online learning. However, almost all previous works rely on strong assumptions of client homogeneity, i.e., all participating clients shall share the same bandit model; otherwise, they all would suffer linear regret. This greatly restricts the application of federated bandit learning in practice. In this work, we introduce a new approach for federated bandits for heterogeneous clients, which clusters clients for collaborative bandit learning under the federated learning setting. Our proposed algorithm achieves non-trivial sub-linear regret and communication cost for all clients, subject to the communication protocol under federated learning that at anytime only one model c",
    "path": "papers/24/03/2403.00116.json",
    "total_tokens": 866,
    "translated_title": "具有异构客户的联邦线性上下文赌臂",
    "translated_abstract": "由于分布式系统产生的数据量不断增加，对跨多个代理进行协同和私密的赌臂学习的需求正在增长。联邦赌臂学习已经成为一种有前途的框架，用于私密、高效和去中心化的在线学习。然而，几乎所有先前的工作都依赖于客户同质性的强假设，即所有参与客户都应共享相同的赌臂模型；否则，它们将遭受线性后悔。这严重限制了联邦赌臂学习在实践中的应用。在这项工作中，我们为异构客户引入了一种新的联邦赌臂方法，该方法在联邦学习环境下对客户进行聚类，用于协同赌臂学习。我们提出的算法实现了对所有客户而言的非平凡次线性后悔和通信成本，符合联邦学习中的通信协议，在任何时候只有一个模型",
    "tldr": "提出了一种适用于异构客户的联邦赌臂学习方法，通过在联邦学习设置下为客户进行聚类，实现了非平凡次线性后悔和通信成本的优化",
    "en_tdlr": "Introduced a federated bandit learning approach for heterogeneous clients, which optimizes non-trivial sub-linear regret and communication cost by clustering clients for collaborative bandit learning under the federated learning setting."
}