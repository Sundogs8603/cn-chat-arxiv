{
    "title": "MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning",
    "abstract": "arXiv:2403.20320v1 Announce Type: cross  Abstract: Adapting models pre-trained on large-scale datasets to a variety of downstream tasks is a common strategy in deep learning. Consequently, parameter-efficient fine-tuning methods have emerged as a promising way to adapt pre-trained models to different tasks while training only a minimal number of parameters. While most of these methods are designed for single-task adaptation, parameter-efficient training in Multi-Task Learning (MTL) architectures is still unexplored. In this paper, we introduce MTLoRA, a novel framework for parameter-efficient training of MTL models. MTLoRA employs Task-Agnostic and Task-Specific Low-Rank Adaptation modules, which effectively disentangle the parameter space in MTL fine-tuning, thereby enabling the model to adeptly handle both task specialization and interaction within MTL contexts. We applied MTLoRA to hierarchical-transformer-based MTL architectures, adapting them to multiple downstream dense predictio",
    "link": "https://arxiv.org/abs/2403.20320",
    "context": "Title: MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning\nAbstract: arXiv:2403.20320v1 Announce Type: cross  Abstract: Adapting models pre-trained on large-scale datasets to a variety of downstream tasks is a common strategy in deep learning. Consequently, parameter-efficient fine-tuning methods have emerged as a promising way to adapt pre-trained models to different tasks while training only a minimal number of parameters. While most of these methods are designed for single-task adaptation, parameter-efficient training in Multi-Task Learning (MTL) architectures is still unexplored. In this paper, we introduce MTLoRA, a novel framework for parameter-efficient training of MTL models. MTLoRA employs Task-Agnostic and Task-Specific Low-Rank Adaptation modules, which effectively disentangle the parameter space in MTL fine-tuning, thereby enabling the model to adeptly handle both task specialization and interaction within MTL contexts. We applied MTLoRA to hierarchical-transformer-based MTL architectures, adapting them to multiple downstream dense predictio",
    "path": "papers/24/03/2403.20320.json",
    "total_tokens": 929,
    "translated_title": "MTLoRA: 一种用于高效多任务学习的低秩适应方法",
    "translated_abstract": "适应在大规模数据集上预训练的模型到各种下游任务是深度学习中常见的策略。因此，参数高效的微调方法已经成为将预训练模型适应到不同任务的一种有前景的方式，同时仅训练少量参数。虽然大多数这些方法是为单任务适应而设计的，但在多任务学习（MTL）架构中进行参数高效训练仍未被探索。在本文中，我们介绍了MTLoRA，一种用于多任务学习模型参数高效训练的新框架。MTLoRA采用任务不可知和任务特定的低秩适应模块，有效地分开了MTL微调中的参数空间，从而使模型能够熟练处理MTL上下文中的任务专门化和交互。我们将MTLoRA应用于基于分层变压器的MTL架构，将它们调整到多个下游密集预测",
    "tldr": "MTLoRA提出了一种新颖的多任务学习模型参数高效训练框架，通过任务不可知和任务特定的低秩适应模块有效地实现了参数空间的分离，使得模型能够灵活处理任务专业化和在多任务学习环境中的相互作用",
    "en_tdlr": "MTLoRA introduces a novel framework for parameter-efficient training of Multi-Task Learning models, utilizing Task-Agnostic and Task-Specific Low-Rank Adaptation modules to effectively disentangle the parameter space, enabling the model to adeptly handle task specialization and interaction within Multi-Task Learning contexts."
}