{
    "title": "BootTOD: Bootstrap Task-oriented Dialogue Representations by Aligning Diverse Responses",
    "abstract": "arXiv:2403.01163v1 Announce Type: new  Abstract: Pre-trained language models have been successful in many scenarios. However, their usefulness in task-oriented dialogues is limited due to the intrinsic linguistic differences between general text and task-oriented dialogues. Current task-oriented dialogue pre-training methods rely on a contrastive framework, which faces challenges such as selecting true positives and hard negatives, as well as lacking diversity. In this paper, we propose a novel dialogue pre-training model called BootTOD. It learns task-oriented dialogue representations via a self-bootstrapping framework. Unlike contrastive counterparts, BootTOD aligns context and context+response representations and dismisses the requirements of contrastive pairs. BootTOD also uses multiple appropriate response targets to model the intrinsic one-to-many diversity of human conversations. Experimental results show that BootTOD outperforms strong TOD baselines on diverse downstream dialog",
    "link": "https://arxiv.org/abs/2403.01163",
    "context": "Title: BootTOD: Bootstrap Task-oriented Dialogue Representations by Aligning Diverse Responses\nAbstract: arXiv:2403.01163v1 Announce Type: new  Abstract: Pre-trained language models have been successful in many scenarios. However, their usefulness in task-oriented dialogues is limited due to the intrinsic linguistic differences between general text and task-oriented dialogues. Current task-oriented dialogue pre-training methods rely on a contrastive framework, which faces challenges such as selecting true positives and hard negatives, as well as lacking diversity. In this paper, we propose a novel dialogue pre-training model called BootTOD. It learns task-oriented dialogue representations via a self-bootstrapping framework. Unlike contrastive counterparts, BootTOD aligns context and context+response representations and dismisses the requirements of contrastive pairs. BootTOD also uses multiple appropriate response targets to model the intrinsic one-to-many diversity of human conversations. Experimental results show that BootTOD outperforms strong TOD baselines on diverse downstream dialog",
    "path": "papers/24/03/2403.01163.json",
    "total_tokens": 824,
    "translated_title": "通过对齐多样化回复来引导任务导向对话表示的BootstrapTOD",
    "translated_abstract": "预训练语言模型在许多场景中取得了成功。然而，在任务导向对话中，由于通用文本和任务导向对话之间固有的语言差异，它们的实用性受到限制。当前的任务导向对话预训练方法依赖于对比框架，面临着诸如选择真正例和难负例，以及缺乏多样性等挑战。本文提出了一种称为BootTOD的新型对话预训练模型。它通过自助引导框架学习任务导向对话表示。与对比对应物相反，BootTOD通过对齐上下文和上下文+回复表示来消除对比对的要求。BootTOD还使用多个适当的回复目标来建模人类对话固有的一对多多样性。实验结果表明BootTOD在多样的下游对话任务上优于强大的TOD基线。",
    "tldr": "BootTOD通过自助引导框架学习任务导向对话表示，消除了对比对的要求，并使用多个回复目标来模拟人类对话的多样性。",
    "en_tdlr": "BootTOD learns task-oriented dialogue representations via a self-bootstrapping framework, dismisses the requirements of contrastive pairs, and uses multiple response targets to model the diversity of human conversations."
}