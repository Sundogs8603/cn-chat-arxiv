{
    "title": "SpikeGraphormer: A High-Performance Graph Transformer with Spiking Graph Attention",
    "abstract": "arXiv:2403.15480v1 Announce Type: cross  Abstract: Recently, Graph Transformers have emerged as a promising solution to alleviate the inherent limitations of Graph Neural Networks (GNNs) and enhance graph representation performance. Unfortunately, Graph Transformers are computationally expensive due to the quadratic complexity inherent in self-attention when applied over large-scale graphs, especially for node tasks. In contrast, spiking neural networks (SNNs), with event-driven and binary spikes properties, can perform energy-efficient computation. In this work, we propose a novel insight into integrating SNNs with Graph Transformers and design a Spiking Graph Attention (SGA) module. The matrix multiplication is replaced by sparse addition and mask operations. The linear complexity enables all-pair node interactions on large-scale graphs with limited GPU memory. To our knowledge, our work is the first attempt to introduce SNNs into Graph Transformers. Furthermore, we design SpikeGraph",
    "link": "https://arxiv.org/abs/2403.15480",
    "context": "Title: SpikeGraphormer: A High-Performance Graph Transformer with Spiking Graph Attention\nAbstract: arXiv:2403.15480v1 Announce Type: cross  Abstract: Recently, Graph Transformers have emerged as a promising solution to alleviate the inherent limitations of Graph Neural Networks (GNNs) and enhance graph representation performance. Unfortunately, Graph Transformers are computationally expensive due to the quadratic complexity inherent in self-attention when applied over large-scale graphs, especially for node tasks. In contrast, spiking neural networks (SNNs), with event-driven and binary spikes properties, can perform energy-efficient computation. In this work, we propose a novel insight into integrating SNNs with Graph Transformers and design a Spiking Graph Attention (SGA) module. The matrix multiplication is replaced by sparse addition and mask operations. The linear complexity enables all-pair node interactions on large-scale graphs with limited GPU memory. To our knowledge, our work is the first attempt to introduce SNNs into Graph Transformers. Furthermore, we design SpikeGraph",
    "path": "papers/24/03/2403.15480.json",
    "total_tokens": 889,
    "translated_title": "SpikeGraphormer：一种具有脉冲图注意力的高性能图变换器",
    "translated_abstract": "最近，图变换器已经成为一种有希望的解决方案，能够缓解图神经网络（GNN）的固有限制并增强图表示性能。不幸的是，由于自我注意力在大规模图上的二次复杂性，特别是对于节点任务，图变换器在计算上是昂贵的。相比之下，脉冲神经网络（SNN）具有事件驱动和二进制脉冲属性，可以进行高效的能量计算。在这项工作中，我们对将SNN与图变换器进行集成提出了一种新的见解，并设计了一个脉冲图注意力（SGA）模块。矩阵乘法被稀疏加法和掩码操作所取代。线性复杂性使得可以在具有有限GPU内存的大规模图上进行所有节点间的交互。据我们所知，我们的工作是第一次尝试将SNN引入图变换器。此外，我们设计了SpikeGraph。",
    "tldr": "本研究将脉冲神经网络与图变换器集成，设计了脉冲图注意力模块，通过稀疏加法和掩码操作取代矩阵乘法，实现了线性复杂度，从而在大规模图上实现了所有节点之间的交互。"
}