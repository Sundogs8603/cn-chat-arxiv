{
    "title": "Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding",
    "abstract": "arXiv:2403.04797v1 Announce Type: new  Abstract: This paper aims to overcome the \"lost-in-the-middle\" challenge of large language models (LLMs). While recent advancements have successfully enabled LLMs to perform stable language modeling with up to 4 million tokens, the persistent difficulty faced by most LLMs in identifying relevant information situated in the middle of the context has not been adequately tackled. To address this problem, this paper introduces Multi-scale Positional Encoding (Ms-PoE) which is a simple yet effective plug-and-play approach to enhance the capacity of LLMs to handle the relevant information located in the middle of the context, without fine-tuning or introducing any additional overhead. Ms-PoE leverages the position indice rescaling to relieve the long-term decay effect introduced by RoPE, while meticulously assigning distinct scaling ratios to different attention heads to preserve essential knowledge learned during the pre-training step, forming a multi-",
    "link": "https://arxiv.org/abs/2403.04797",
    "context": "Title: Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding\nAbstract: arXiv:2403.04797v1 Announce Type: new  Abstract: This paper aims to overcome the \"lost-in-the-middle\" challenge of large language models (LLMs). While recent advancements have successfully enabled LLMs to perform stable language modeling with up to 4 million tokens, the persistent difficulty faced by most LLMs in identifying relevant information situated in the middle of the context has not been adequately tackled. To address this problem, this paper introduces Multi-scale Positional Encoding (Ms-PoE) which is a simple yet effective plug-and-play approach to enhance the capacity of LLMs to handle the relevant information located in the middle of the context, without fine-tuning or introducing any additional overhead. Ms-PoE leverages the position indice rescaling to relieve the long-term decay effect introduced by RoPE, while meticulously assigning distinct scaling ratios to different attention heads to preserve essential knowledge learned during the pre-training step, forming a multi-",
    "path": "papers/24/03/2403.04797.json",
    "total_tokens": 853,
    "translated_title": "在中间被发现: 语言模型如何通过即插即用位置编码更好地使用长上下文",
    "translated_abstract": "本文旨在克服大型语言模型（LLMs）面临的“中间丢失”挑战。尽管最近的进展成功实现了LLMs对包含400万令牌的稳定语言建模，但大多数LLMs在识别位于上下文中间的相关信息方面仍然存在持续困难。为解决这一问题，本文引入了多尺度位置编码（Ms-PoE），这是一种简单而有效的即插即用方法，可以增强LLMs处理位于上下文中间的相关信息的能力，无需微调或引入任何额外开销。Ms-PoE利用位置指数重新缩放以减轻RoPE引入的长期衰减效应，同时精心为不同注意力头分配不同的缩放比以保留预训练阶段学到的基本知识，形成多",
    "tldr": "通过引入多尺度位置编码（Ms-PoE）来增强大型语言模型（LLMs）对上下文中间相关信息的处理能力，解决了LLMs面临的“中间丢失”挑战。",
    "en_tdlr": "Introducing Multi-scale Positional Encoding (Ms-PoE) enhances the capability of Large Language Models (LLMs) to handle relevant information in the middle of the context, thus addressing the \"lost-in-the-middle\" challenge faced by LLMs."
}