{
    "title": "A Study in Dataset Pruning for Image Super-Resolution",
    "abstract": "arXiv:2403.17083v1 Announce Type: cross  Abstract: In image Super-Resolution (SR), relying on large datasets for training is a double-edged sword. While offering rich training material, they also demand substantial computational and storage resources. In this work, we analyze dataset pruning as a solution to these challenges. We introduce a novel approach that reduces a dataset to a core-set of training samples, selected based on their loss values as determined by a simple pre-trained SR model. By focusing the training on just 50% of the original dataset, specifically on the samples characterized by the highest loss values, we achieve results comparable to or even surpassing those obtained from training on the entire dataset. Interestingly, our analysis reveals that the top 5% of samples with the highest loss values negatively affect the training process. Excluding these samples and adjusting the selection to favor easier samples further enhances training outcomes. Our work opens new p",
    "link": "https://arxiv.org/abs/2403.17083",
    "context": "Title: A Study in Dataset Pruning for Image Super-Resolution\nAbstract: arXiv:2403.17083v1 Announce Type: cross  Abstract: In image Super-Resolution (SR), relying on large datasets for training is a double-edged sword. While offering rich training material, they also demand substantial computational and storage resources. In this work, we analyze dataset pruning as a solution to these challenges. We introduce a novel approach that reduces a dataset to a core-set of training samples, selected based on their loss values as determined by a simple pre-trained SR model. By focusing the training on just 50% of the original dataset, specifically on the samples characterized by the highest loss values, we achieve results comparable to or even surpassing those obtained from training on the entire dataset. Interestingly, our analysis reveals that the top 5% of samples with the highest loss values negatively affect the training process. Excluding these samples and adjusting the selection to favor easier samples further enhances training outcomes. Our work opens new p",
    "path": "papers/24/03/2403.17083.json",
    "total_tokens": 889,
    "translated_title": "数据集修剪在图像超分辨率中的研究",
    "translated_abstract": "在图像超分辨率（SR）中，依赖大型数据集进行训练是一把双刃剑。尽管提供丰富的训练素材，但也需要大量的计算和存储资源。在本工作中，我们分析了数据集修剪作为应对这些挑战的解决方案。我们引入了一种新颖的方法，将数据集缩减到基于其损失值而选择的一组核心训练样本。通过仅将训练重点放在原始数据集的50%上，特别是那些损失值最高的样本上，我们实现了与或甚至超过整个数据集训练的结果相媲美的效果。有趣的是，我们的分析显示，具有最高损失值的前5％样本会对训练过程产生负面影响。排除这些样本并调整选择以偏好更容易的样本进一步提高了训练结果。我们的工作开辟了新的研究方向。",
    "tldr": "本研究针对图像超分辨率中数据集训练资源需求大的问题，提出了一种数据集修剪的解决方案，通过基于损失值的选择，将训练集缩减至原始数据集的50%，取得了令人满意的结果。",
    "en_tdlr": "This study addresses the issue of resource-intensive training in image super-resolution by proposing a dataset pruning solution that reduces the training set to 50% of the original dataset based on loss values, achieving satisfactory results."
}