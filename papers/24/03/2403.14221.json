{
    "title": "Improving the Robustness of Large Language Models via Consistency Alignment",
    "abstract": "arXiv:2403.14221v1 Announce Type: new  Abstract: Large language models (LLMs) have shown tremendous success in following user instructions and generating helpful responses. Nevertheless, their robustness is still far from optimal, as they may generate significantly inconsistent responses due to minor changes in the verbalized instructions. Recent literature has explored this inconsistency issue, highlighting the importance of continued improvement in the robustness of response generation. However, systematic analysis and solutions are still lacking. In this paper, we quantitatively define the inconsistency problem and propose a two-stage training framework consisting of instruction-augmented supervised fine-tuning and consistency alignment training. The first stage helps a model generalize on following instructions via similar instruction augmentations. In the second stage, we improve the diversity and help the model understand which responses are more aligned with human expectations b",
    "link": "https://arxiv.org/abs/2403.14221",
    "context": "Title: Improving the Robustness of Large Language Models via Consistency Alignment\nAbstract: arXiv:2403.14221v1 Announce Type: new  Abstract: Large language models (LLMs) have shown tremendous success in following user instructions and generating helpful responses. Nevertheless, their robustness is still far from optimal, as they may generate significantly inconsistent responses due to minor changes in the verbalized instructions. Recent literature has explored this inconsistency issue, highlighting the importance of continued improvement in the robustness of response generation. However, systematic analysis and solutions are still lacking. In this paper, we quantitatively define the inconsistency problem and propose a two-stage training framework consisting of instruction-augmented supervised fine-tuning and consistency alignment training. The first stage helps a model generalize on following instructions via similar instruction augmentations. In the second stage, we improve the diversity and help the model understand which responses are more aligned with human expectations b",
    "path": "papers/24/03/2403.14221.json",
    "total_tokens": 816,
    "translated_title": "通过一致性对齐提高大型语言模型的鲁棒性",
    "translated_abstract": "大型语言模型(LLMs)在遵循用户指令和生成有用响应方面取得了巨大成功。然而，它们的鲁棒性仍远未达到最佳状态，因为可能由于口头指令中的细微更改而产生明显不一致的响应。最近的文献探讨了这种不一致性问题，强调了继续改善响应生成的鲁棒性的重要性。然而，对系统性分析和解决方案仍然缺乏。在本文中，我们定量定义了不一致性问题，并提出了一个由指令增强监督微调和一致性对齐训练组成的两阶段训练框架。第一阶段通过类似指令增强帮助模型在遵循指令时泛化。在第二阶段，我们提高了多样性，并帮助模型理解哪些响应与人类期望更一致。",
    "tldr": "通过一致性对齐训练的两阶段框架，有助于提高大型语言模型的鲁棒性和对指令的理解。"
}