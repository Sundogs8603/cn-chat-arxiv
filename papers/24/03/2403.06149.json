{
    "title": "Can Large Language Models Automatically Score Proficiency of Written Essays?",
    "abstract": "arXiv:2403.06149v1 Announce Type: cross  Abstract: Although several methods were proposed to address the problem of automated essay scoring (AES) in the last 50 years, there is still much to desire in terms of effectiveness. Large Language Models (LLMs) are transformer-based models that demonstrate extraordinary capabilities on various tasks. In this paper, we test the ability of LLMs, given their powerful linguistic knowledge, to analyze and effectively score written essays. We experimented with two popular LLMs, namely ChatGPT and Llama. We aim to check if these models can do this task and, if so, how their performance is positioned among the state-of-the-art (SOTA) models across two levels, holistically and per individual writing trait. We utilized prompt-engineering tactics in designing four different prompts to bring their maximum potential to this task. Our experiments conducted on the ASAP dataset revealed several interesting observations. First, choosing the right prompt depend",
    "link": "https://arxiv.org/abs/2403.06149",
    "context": "Title: Can Large Language Models Automatically Score Proficiency of Written Essays?\nAbstract: arXiv:2403.06149v1 Announce Type: cross  Abstract: Although several methods were proposed to address the problem of automated essay scoring (AES) in the last 50 years, there is still much to desire in terms of effectiveness. Large Language Models (LLMs) are transformer-based models that demonstrate extraordinary capabilities on various tasks. In this paper, we test the ability of LLMs, given their powerful linguistic knowledge, to analyze and effectively score written essays. We experimented with two popular LLMs, namely ChatGPT and Llama. We aim to check if these models can do this task and, if so, how their performance is positioned among the state-of-the-art (SOTA) models across two levels, holistically and per individual writing trait. We utilized prompt-engineering tactics in designing four different prompts to bring their maximum potential to this task. Our experiments conducted on the ASAP dataset revealed several interesting observations. First, choosing the right prompt depend",
    "path": "papers/24/03/2403.06149.json",
    "total_tokens": 842,
    "translated_title": "大型语言模型能否自动评分写作文章的能力？",
    "translated_abstract": "虽然在过去50年中提出了几种方法来解决自动评分作文（AES）的问题，但在效果方面仍有许多不足之处。大型语言模型（LLMs）是基于Transformer的模型，在各种任务上展示了非凡的能力。本文测试了LLMs的能力，鉴于它们强大的语言知识，来分析和有效评分书面作文。我们对两种流行的LLMs进行了实验，分别是ChatGPT和Llama。我们旨在检查这些模型是否能够完成这项任务，以及它们在两个层面上的表现如何，即在整体上和在个体写作特征上。我们利用提示工程策略设计了四个不同的提示，以发挥它们在这项任务中的最大潜力。我们在ASAP数据集上进行的实验揭示了几个有趣的观察结果。",
    "tldr": "本研究旨在测试大型语言模型在分析和评分书面作文方面的能力，通过对两种流行的LLMs进行实验，设计不同提示并在ASAP数据集上进行实验，揭示了有趣的观察结果。",
    "en_tdlr": "This study aims to test the ability of large language models to analyze and score written essays, conducting experiments on two popular LLMs with different prompts on the ASAP dataset, revealing interesting observations."
}