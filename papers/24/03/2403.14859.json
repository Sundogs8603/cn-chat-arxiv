{
    "title": "Comparing Plausibility Estimates in Base and Instruction-Tuned Large Language Models",
    "abstract": "arXiv:2403.14859v1 Announce Type: cross  Abstract: Instruction-tuned LLMs can respond to explicit queries formulated as prompts, which greatly facilitates interaction with human users. However, prompt-based approaches might not always be able to tap into the wealth of implicit knowledge acquired by LLMs during pre-training. This paper presents a comprehensive study of ways to evaluate semantic plausibility in LLMs. We compare base and instruction-tuned LLM performance on an English sentence plausibility task via (a) explicit prompting and (b) implicit estimation via direct readout of the probabilities models assign to strings. Experiment 1 shows that, across model architectures and plausibility datasets, (i) log likelihood ($\\textit{LL}$) scores are the most reliable indicator of sentence plausibility, with zero-shot prompting yielding inconsistent and typically poor results; (ii) $\\textit{LL}$-based performance is still inferior to human performance; (iii) instruction-tuned models hav",
    "link": "https://arxiv.org/abs/2403.14859",
    "context": "Title: Comparing Plausibility Estimates in Base and Instruction-Tuned Large Language Models\nAbstract: arXiv:2403.14859v1 Announce Type: cross  Abstract: Instruction-tuned LLMs can respond to explicit queries formulated as prompts, which greatly facilitates interaction with human users. However, prompt-based approaches might not always be able to tap into the wealth of implicit knowledge acquired by LLMs during pre-training. This paper presents a comprehensive study of ways to evaluate semantic plausibility in LLMs. We compare base and instruction-tuned LLM performance on an English sentence plausibility task via (a) explicit prompting and (b) implicit estimation via direct readout of the probabilities models assign to strings. Experiment 1 shows that, across model architectures and plausibility datasets, (i) log likelihood ($\\textit{LL}$) scores are the most reliable indicator of sentence plausibility, with zero-shot prompting yielding inconsistent and typically poor results; (ii) $\\textit{LL}$-based performance is still inferior to human performance; (iii) instruction-tuned models hav",
    "path": "papers/24/03/2403.14859.json",
    "total_tokens": 879,
    "translated_title": "在基础模型和指令调优的大型语言模型中比较可信度估计",
    "translated_abstract": "指令调优的LLM可以响应明确制定为提示的查询，这极大地促进了与人类用户的交互。然而，基于提示的方法可能并不总是能够利用LLM在预训练期间获得的隐式知识。本文对评估LLM中语义可信度的方法进行了全面研究。我们通过（a）明确提示和（b）直接读取模型分配给字符串的概率的隐式估计，在英语句子可信度任务中比较了基础和指令调优LLM的性能。实验1表明，跨模型架构和可信度数据集，（i）对数似然（LL）分数是句子可信度最可靠的指标，零照射提示产生不一致且通常效果不佳的结果；（ii）基于LL的性能仍低于人类表现；（iii）指令调优模型有",
    "tldr": "通过比较基础和指令调优的大型语言模型在英语句子可信度任务中的表现，发现对数似然（LL）分数是最可靠的句子可信度指标，但仍低于人类表现。",
    "en_tdlr": "Comparing base and instruction-tuned large language models on an English sentence plausibility task, the study found that log likelihood (LL) scores are the most reliable indicator of sentence plausibility, yet still inferior to human performance."
}