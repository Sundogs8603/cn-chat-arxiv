{
    "title": "A comparative analysis of embedding models for patent similarity",
    "abstract": "arXiv:2403.16630v1 Announce Type: new  Abstract: This paper makes two contributions to the field of text-based patent similarity. First, it compares the performance of different kinds of patent-specific pretrained embedding models, namely static word embeddings (such as word2vec and doc2vec models) and contextual word embeddings (such as transformers based models), on the task of patent similarity calculation. Second, it compares specifically the performance of Sentence Transformers (SBERT) architectures with different training phases on the patent similarity task. To assess the models' performance, we use information about patent interferences, a phenomenon in which two or more patent claims belonging to different patent applications are proven to be overlapping by patent examiners. Therefore, we use these interferences cases as a proxy for maximum similarity between two patents, treating them as ground-truth to evaluate the performance of the different embedding models. Our results p",
    "link": "https://arxiv.org/abs/2403.16630",
    "context": "Title: A comparative analysis of embedding models for patent similarity\nAbstract: arXiv:2403.16630v1 Announce Type: new  Abstract: This paper makes two contributions to the field of text-based patent similarity. First, it compares the performance of different kinds of patent-specific pretrained embedding models, namely static word embeddings (such as word2vec and doc2vec models) and contextual word embeddings (such as transformers based models), on the task of patent similarity calculation. Second, it compares specifically the performance of Sentence Transformers (SBERT) architectures with different training phases on the patent similarity task. To assess the models' performance, we use information about patent interferences, a phenomenon in which two or more patent claims belonging to different patent applications are proven to be overlapping by patent examiners. Therefore, we use these interferences cases as a proxy for maximum similarity between two patents, treating them as ground-truth to evaluate the performance of the different embedding models. Our results p",
    "path": "papers/24/03/2403.16630.json",
    "total_tokens": 813,
    "translated_title": "专利相似性嵌入模型的比较分析",
    "translated_abstract": "本文对基于文本的专利相似性领域做出了两个贡献。首先，它比较了不同类型的专利特定预训练嵌入模型（如word2vec和doc2vec模型）和上下文词嵌入模型（如基于transformers的模型）在专利相似性计算任务上的表现。其次，它具体比较了具有不同训练阶段的Sentence Transformers（SBERT）架构在专利相似性任务上的性能。为评估模型的性能，我们使用关于专利干涉的信息，即两个或多个专利申请中的专利要求被专利审查员证明存在重叠的现象。因此，我们将这些干涉案例视为两个专利之间的最大相似性的代理，并用它们作为基准来评估不同嵌入模型的性能。",
    "tldr": "本文比较了不同类型的专利嵌入模型在专利相似性计算任务上的表现，并具体探讨了Sentence Transformers (SBERT) 架构在专利相似性任务中的性能。",
    "en_tdlr": "This paper compares the performance of different types of patent embedding models on the task of patent similarity calculation and specifically explores the performance of Sentence Transformers (SBERT) architecture in patent similarity task."
}