{
    "title": "In-context Exploration-Exploitation for Reinforcement Learning",
    "abstract": "arXiv:2403.06826v1 Announce Type: cross  Abstract: In-context learning is a promising approach for online policy learning of offline reinforcement learning (RL) methods, which can be achieved at inference time without gradient optimization. However, this method is hindered by significant computational costs resulting from the gathering of large training trajectory sets and the need to train large Transformer models. We address this challenge by introducing an In-context Exploration-Exploitation (ICEE) algorithm, designed to optimize the efficiency of in-context policy learning. Unlike existing models, ICEE performs an exploration-exploitation trade-off at inference time within a Transformer model, without the need for explicit Bayesian inference. Consequently, ICEE can solve Bayesian optimization problems as efficiently as Gaussian process biased methods do, but in significantly less time. Through experiments in grid world environments, we demonstrate that ICEE can learn to solve new R",
    "link": "https://arxiv.org/abs/2403.06826",
    "context": "Title: In-context Exploration-Exploitation for Reinforcement Learning\nAbstract: arXiv:2403.06826v1 Announce Type: cross  Abstract: In-context learning is a promising approach for online policy learning of offline reinforcement learning (RL) methods, which can be achieved at inference time without gradient optimization. However, this method is hindered by significant computational costs resulting from the gathering of large training trajectory sets and the need to train large Transformer models. We address this challenge by introducing an In-context Exploration-Exploitation (ICEE) algorithm, designed to optimize the efficiency of in-context policy learning. Unlike existing models, ICEE performs an exploration-exploitation trade-off at inference time within a Transformer model, without the need for explicit Bayesian inference. Consequently, ICEE can solve Bayesian optimization problems as efficiently as Gaussian process biased methods do, but in significantly less time. Through experiments in grid world environments, we demonstrate that ICEE can learn to solve new R",
    "path": "papers/24/03/2403.06826.json",
    "total_tokens": 811,
    "translated_title": "基于上下文的探索-利用用于强化学习",
    "translated_abstract": "在-context学习是在线策略学习离线强化学习（RL）方法的一种有前途的方法，可以在推理时间内实现，无需梯度优化。然而，由于需要收集大量训练轨迹集并训练大型Transformer模型，这种方法所带来的显著计算成本。我们通过引入一种基于In-context Exploration-Exploitation（ICEE）的算法来解决这一挑战，该算法旨在优化在-context策略学习的效率。ICEE在推理时间内在Transformer模型中执行探索-利用权衡，不需要显式贝叶斯推断。因此，ICEE可以像高斯过程偏差方法那样有效地解决贝叶斯优化问题，但时间显着较短。通过在网格世界环境中的实验，我们证明ICEE能够学习解决新的R",
    "tldr": "引入了In-context Exploration-Exploitation (ICEE)算法，通过在Transformer模型内部进行探索-利用权衡，提高了在-context策略学习的效率。",
    "en_tdlr": "Introduced the In-context Exploration-Exploitation (ICEE) algorithm, which improves the efficiency of in-context policy learning by performing exploration-exploitation trade-off within the Transformer model."
}