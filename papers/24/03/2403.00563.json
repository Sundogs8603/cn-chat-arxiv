{
    "title": "Indirectly Parameterized Concrete Autoencoders",
    "abstract": "arXiv:2403.00563v1 Announce Type: new  Abstract: Feature selection is a crucial task in settings where data is high-dimensional or acquiring the full set of features is costly. Recent developments in neural network-based embedded feature selection show promising results across a wide range of applications. Concrete Autoencoders (CAEs), considered state-of-the-art in embedded feature selection, may struggle to achieve stable joint optimization, hurting their training time and generalization. In this work, we identify that this instability is correlated with the CAE learning duplicate selections. To remedy this, we propose a simple and effective improvement: Indirectly Parameterized CAEs (IP-CAEs). IP-CAEs learn an embedding and a mapping from it to the Gumbel-Softmax distributions' parameters. Despite being simple to implement, IP-CAE exhibits significant and consistent improvements over CAE in both generalization and training time across several datasets for reconstruction and classifi",
    "link": "https://arxiv.org/abs/2403.00563",
    "context": "Title: Indirectly Parameterized Concrete Autoencoders\nAbstract: arXiv:2403.00563v1 Announce Type: new  Abstract: Feature selection is a crucial task in settings where data is high-dimensional or acquiring the full set of features is costly. Recent developments in neural network-based embedded feature selection show promising results across a wide range of applications. Concrete Autoencoders (CAEs), considered state-of-the-art in embedded feature selection, may struggle to achieve stable joint optimization, hurting their training time and generalization. In this work, we identify that this instability is correlated with the CAE learning duplicate selections. To remedy this, we propose a simple and effective improvement: Indirectly Parameterized CAEs (IP-CAEs). IP-CAEs learn an embedding and a mapping from it to the Gumbel-Softmax distributions' parameters. Despite being simple to implement, IP-CAE exhibits significant and consistent improvements over CAE in both generalization and training time across several datasets for reconstruction and classifi",
    "path": "papers/24/03/2403.00563.json",
    "total_tokens": 851,
    "translated_title": "间接参数化具体自编码器",
    "translated_abstract": "特征选择在数据高维或获取完整特征集成本高昂的情况下至关重要。最近基于神经网络的嵌入式特征选择的发展在广泛应用中表现出有希望的结果。具体自编码器（CAEs）被认为是嵌入式特征选择中的最先进技术，但可能难以实现稳定的联合优化，从而影响其训练时间和泛化能力。本文发现这种不稳定性与CAE学习重复选择有关。为了解决这个问题，我们提出了一种简单有效的改进：间接参数化CAEs（IP-CAEs）。IP-CAEs学习一个嵌入和从它到Gumbel-Softmax分布参数的映射。尽管实现简单，IP-CAE在多个数据集上的重构和分类任务中均表现出显著且一致的改进，无论是在泛化还是训练时间上。",
    "tldr": "本文提出了间接参数化CAEs（IP-CAEs）来解决具体自编码器（CAEs）在稳定联合优化方面的问题，IP-CAEs在多个数据集上表现出显著且一致的改进。",
    "en_tdlr": "This paper introduces Indirectly Parameterized CAEs (IP-CAEs) to address the instability issue in joint optimization of Concrete Autoencoders (CAEs), showing significant and consistent improvements across multiple datasets."
}