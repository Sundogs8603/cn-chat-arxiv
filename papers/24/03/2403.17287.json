{
    "title": "Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study",
    "abstract": "arXiv:2403.17287v1 Announce Type: new  Abstract: Federated Learning (FL) emerged as a practical approach to training a model from decentralized data. The proliferation of FL led to the development of numerous FL algorithms and mechanisms. Many prior efforts have given their primary focus on accuracy of those approaches, but there exists little understanding of other aspects such as computational overheads, performance and training stability, etc. To bridge this gap, we conduct extensive performance evaluation on several canonical FL algorithms (FedAvg, FedProx, FedYogi, FedAdam, SCAFFOLD, and FedDyn) by leveraging an open-source federated learning framework called Flame. Our comprehensive measurement study reveals that no single algorithm works best across different performance metrics. A few key observations are: (1) While some state-of-the-art algorithms achieve higher accuracy than others, they incur either higher computation overheads (FedDyn) or communication overheads (SCAFFOLD).",
    "link": "https://arxiv.org/abs/2403.17287",
    "context": "Title: Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study\nAbstract: arXiv:2403.17287v1 Announce Type: new  Abstract: Federated Learning (FL) emerged as a practical approach to training a model from decentralized data. The proliferation of FL led to the development of numerous FL algorithms and mechanisms. Many prior efforts have given their primary focus on accuracy of those approaches, but there exists little understanding of other aspects such as computational overheads, performance and training stability, etc. To bridge this gap, we conduct extensive performance evaluation on several canonical FL algorithms (FedAvg, FedProx, FedYogi, FedAdam, SCAFFOLD, and FedDyn) by leveraging an open-source federated learning framework called Flame. Our comprehensive measurement study reveals that no single algorithm works best across different performance metrics. A few key observations are: (1) While some state-of-the-art algorithms achieve higher accuracy than others, they incur either higher computation overheads (FedDyn) or communication overheads (SCAFFOLD).",
    "path": "papers/24/03/2403.17287.json",
    "total_tokens": 896,
    "translated_title": "并非所有联邦学习算法都一视同仁：一项性能评估研究",
    "translated_abstract": "《联邦学习(FL)作为一种从分散数据训练模型的实际方法崛起。FL的流行导致了众多FL算法和机制的发展。许多先前的努力主要集中在这些方法的准确性上，但对其他方面，如计算开销，性能和训练稳定性等的理解却很少。为了弥补这一差距，我们利用一款名为Flame的开源联邦学习框架，对几种传统FL算法（FedAvg，FedProx，FedYogi，FedAdam，SCAFFOLD和FedDyn）进行了广泛的性能评估。我们的全面测量研究表明，没有一种算法可以在不同的性能指标上表现最好。一些关键观察结果是：（1）虽然一些最先进的算法达到了比其他算法更高的准确性，但它们会带来更高的计算开销（FedDyn）或通信开销（SCAFFOLD）。",
    "tldr": "联邦学习算法的性能评估研究揭示出，没有一种算法可以在所有性能指标上表现最佳，一些最先进的算法虽然准确性更高，却伴随着更高的计算或通信开销。",
    "en_tdlr": "The performance evaluation study of federated learning algorithms reveals that no single algorithm performs best across all metrics, with some state-of-the-art algorithms achieving higher accuracy at the cost of increased computational or communication overheads."
}