{
    "title": "Localized Zeroth-Order Prompt Optimization",
    "abstract": "arXiv:2403.02993v1 Announce Type: new  Abstract: The efficacy of large language models (LLMs) in understanding and generating natural language has aroused a wide interest in developing prompt-based methods to harness the power of black-box LLMs. Existing methodologies usually prioritize a global optimization for finding the global optimum, which however will perform poorly in certain tasks. This thus motivates us to re-think the necessity of finding a global optimum in prompt optimization. To answer this, we conduct a thorough empirical study on prompt optimization and draw two major insights. Contrasting with the rarity of global optimum, local optima are usually prevalent and well-performed, which can be more worthwhile for efficient prompt optimization (Insight I). The choice of the input domain, covering both the generation and the representation of prompts, affects the identification of well-performing local optima (Insight II). Inspired by these insights, we propose a novel algor",
    "link": "https://arxiv.org/abs/2403.02993",
    "context": "Title: Localized Zeroth-Order Prompt Optimization\nAbstract: arXiv:2403.02993v1 Announce Type: new  Abstract: The efficacy of large language models (LLMs) in understanding and generating natural language has aroused a wide interest in developing prompt-based methods to harness the power of black-box LLMs. Existing methodologies usually prioritize a global optimization for finding the global optimum, which however will perform poorly in certain tasks. This thus motivates us to re-think the necessity of finding a global optimum in prompt optimization. To answer this, we conduct a thorough empirical study on prompt optimization and draw two major insights. Contrasting with the rarity of global optimum, local optima are usually prevalent and well-performed, which can be more worthwhile for efficient prompt optimization (Insight I). The choice of the input domain, covering both the generation and the representation of prompts, affects the identification of well-performing local optima (Insight II). Inspired by these insights, we propose a novel algor",
    "path": "papers/24/03/2403.02993.json",
    "total_tokens": 822,
    "translated_title": "本地化的零阶提示优化",
    "translated_abstract": "大型语言模型（LLMs）在理解和生成自然语言方面的有效性引起了广泛兴趣，促使人们开发了基于提示的方法来利用黑盒LLMs的能力。现有方法通常优先考虑全局优化以寻找全局最优解，然而在某些任务中表现不佳。这促使我们重新考虑提示优化中寻找全局最优解的必要性。为了回答这个问题，我们对提示优化进行了彻底的实证研究，并得出了两个主要见解。与全局最优解的罕见性形成对比，局部最优解通常普遍存在且表现良好，这对于高效的提示优化可能更具价值（见解I）。输入领域的选择，包括提示的生成和表示，影响了优化良好的局部最优解的识别（见解II）。受到这些见解的启发，我们提出了一种新颖的算法。",
    "tldr": "提出了一种本地化的零阶提示优化方法，通过研究发现，局部最优解通常普遍存在且表现良好，有助于高效的提示优化。",
    "en_tdlr": "Proposed a localized zeroth-order prompt optimization method, which through the study found that local optima are usually prevalent and well-performed, contributing to efficient prompt optimization."
}