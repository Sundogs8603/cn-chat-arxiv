{
    "title": "Don't Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models",
    "abstract": "arXiv:2403.17336v1 Announce Type: cross  Abstract: Recent advancements in generative AI have enabled ubiquitous access to large language models (LLMs). Empowered by their exceptional capabilities to understand and generate human-like text, these models are being increasingly integrated into our society. At the same time, there are also concerns on the potential misuse of this powerful technology, prompting defensive measures from service providers. To overcome such protection, jailbreaking prompts have recently emerged as one of the most effective mechanisms to circumvent security restrictions and elicit harmful content originally designed to be prohibited.   Due to the rapid development of LLMs and their ease of access via natural languages, the frontline of jailbreak prompts is largely seen in online forums and among hobbyists. To gain a better understanding of the threat landscape of semantically meaningful jailbreak prompts, we systemized existing prompts and measured their jailbre",
    "link": "https://arxiv.org/abs/2403.17336",
    "context": "Title: Don't Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models\nAbstract: arXiv:2403.17336v1 Announce Type: cross  Abstract: Recent advancements in generative AI have enabled ubiquitous access to large language models (LLMs). Empowered by their exceptional capabilities to understand and generate human-like text, these models are being increasingly integrated into our society. At the same time, there are also concerns on the potential misuse of this powerful technology, prompting defensive measures from service providers. To overcome such protection, jailbreaking prompts have recently emerged as one of the most effective mechanisms to circumvent security restrictions and elicit harmful content originally designed to be prohibited.   Due to the rapid development of LLMs and their ease of access via natural languages, the frontline of jailbreak prompts is largely seen in online forums and among hobbyists. To gain a better understanding of the threat landscape of semantically meaningful jailbreak prompts, we systemized existing prompts and measured their jailbre",
    "path": "papers/24/03/2403.17336.json",
    "total_tokens": 858,
    "translated_title": "不要听我的话：理解和探索大型语言模型的越狱提示",
    "translated_abstract": "生成式人工智能的最新进展使得大型语言模型（LLMs）能够无处不在地被访问。凭借其出色的理解和生成类似人类文本的能力，这些模型正日益融入我们的社会。与此同时，人们也对这种强大技术的潜在滥用表示担忧，并促使服务提供商采取了防御措施。为了克服这种保护机制，越狱提示最近已成为规避安全限制和引诱最初设计为被禁止的有害内容的最有效机制之一。由于LLM的快速发展及通过自然语言轻松获取的便利性，越狱提示的前沿主要出现在在线论坛和爱好者中。为了更好地了解语义上具有意义的越狱提示的威胁格局，我们系统化了现有提示并测量它们的越狱",
    "tldr": "该论文系统化了关于大型语言模型越狱提示的存在形式，并衡量了它们的越狱潜力，以更好地理解语义上具有意义的越狱提示的威胁格局。",
    "en_tdlr": "This paper systemized the existence forms of jailbreak prompts related to large language models and measured their jailbreaking potential to better understand the threat landscape of semantically meaningful jailbreak prompts."
}