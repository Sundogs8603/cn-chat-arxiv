{
    "title": "COMQ: A Backpropagation-Free Algorithm for Post-Training Quantization",
    "abstract": "arXiv:2403.07134v1 Announce Type: new  Abstract: Post-training quantization (PTQ) has emerged as a practical approach to compress large neural networks, making them highly efficient for deployment. However, effectively reducing these models to their low-bit counterparts without compromising the original accuracy remains a key challenge. In this paper, we propose an innovative PTQ algorithm termed COMQ, which sequentially conducts coordinate-wise minimization of the layer-wise reconstruction errors. We consider the widely used integer quantization, where every quantized weight can be decomposed into a shared floating-point scalar and an integer bit-code. Within a fixed layer, COMQ treats all the scaling factor(s) and bit-codes as the variables of the reconstruction error. Every iteration improves this error along a single coordinate while keeping all other variables constant. COMQ is easy to use and requires no hyper-parameter tuning. It instead involves only dot products and rounding o",
    "link": "https://arxiv.org/abs/2403.07134",
    "context": "Title: COMQ: A Backpropagation-Free Algorithm for Post-Training Quantization\nAbstract: arXiv:2403.07134v1 Announce Type: new  Abstract: Post-training quantization (PTQ) has emerged as a practical approach to compress large neural networks, making them highly efficient for deployment. However, effectively reducing these models to their low-bit counterparts without compromising the original accuracy remains a key challenge. In this paper, we propose an innovative PTQ algorithm termed COMQ, which sequentially conducts coordinate-wise minimization of the layer-wise reconstruction errors. We consider the widely used integer quantization, where every quantized weight can be decomposed into a shared floating-point scalar and an integer bit-code. Within a fixed layer, COMQ treats all the scaling factor(s) and bit-codes as the variables of the reconstruction error. Every iteration improves this error along a single coordinate while keeping all other variables constant. COMQ is easy to use and requires no hyper-parameter tuning. It instead involves only dot products and rounding o",
    "path": "papers/24/03/2403.07134.json",
    "total_tokens": 839,
    "translated_title": "COMQ: 一种无需反向传播的后训练量化算法",
    "translated_abstract": "后训练量化（PTQ）已经成为一种将大型神经网络压缩的实用方法，使其在部署时高度高效。然而，有效地将这些模型降至低比特表示而不损害原始准确性仍然是一个关键挑战。在本文中，我们提出了一种创新的PTQ算法称为COMQ，它通过依次减小逐层重构误差来进行坐标方向上的最小化。我们考虑了广泛使用的整数量化，其中每个量化权重可以分解为一个共享的浮点标量和一个整数位编码。在固定层内，COMQ将所有缩放因子和位编码视为重构误差的变量。每次迭代都会沿着一个坐标轴改进这个错误，同时保持所有其他变量恒定。COMQ易于使用，无需调整超参数。它只涉及点乘和四舍五入。",
    "tldr": "提出了一种名为COMQ的创新后训练量化算法，通过逐层减小重构误差来有效降低大型神经网络的存储要求，同时保持原始准确性。",
    "en_tdlr": "Introduced an innovative post-training quantization algorithm called COMQ, which effectively reduces storage requirements of large neural networks by minimizing reconstruction errors layer-wise while preserving the original accuracy."
}