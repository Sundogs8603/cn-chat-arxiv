{
    "title": "SignSGD with Federated Voting",
    "abstract": "arXiv:2403.16372v1 Announce Type: new  Abstract: Distributed learning is commonly used for accelerating model training by harnessing the computational capabilities of multiple-edge devices. However, in practical applications, the communication delay emerges as a bottleneck due to the substantial information exchange required between workers and a central parameter server. SignSGD with majority voting (signSGD-MV) is an effective distributed learning algorithm that can significantly reduce communication costs by one-bit quantization. However, due to heterogeneous computational capabilities, it fails to converge when the mini-batch sizes differ among workers. To overcome this, we propose a novel signSGD optimizer with \\textit{federated voting} (signSGD-FV). The idea of federated voting is to exploit learnable weights to perform weighted majority voting. The server learns the weights assigned to the edge devices in an online fashion based on their computational capabilities. Subsequently,",
    "link": "https://arxiv.org/abs/2403.16372",
    "context": "Title: SignSGD with Federated Voting\nAbstract: arXiv:2403.16372v1 Announce Type: new  Abstract: Distributed learning is commonly used for accelerating model training by harnessing the computational capabilities of multiple-edge devices. However, in practical applications, the communication delay emerges as a bottleneck due to the substantial information exchange required between workers and a central parameter server. SignSGD with majority voting (signSGD-MV) is an effective distributed learning algorithm that can significantly reduce communication costs by one-bit quantization. However, due to heterogeneous computational capabilities, it fails to converge when the mini-batch sizes differ among workers. To overcome this, we propose a novel signSGD optimizer with \\textit{federated voting} (signSGD-FV). The idea of federated voting is to exploit learnable weights to perform weighted majority voting. The server learns the weights assigned to the edge devices in an online fashion based on their computational capabilities. Subsequently,",
    "path": "papers/24/03/2403.16372.json",
    "total_tokens": 810,
    "translated_title": "具有联邦投票的SignSGD",
    "translated_abstract": "分布式学习通常用于通过利用多个边缘设备的计算能力来加速模型训练。然而，在实际应用中，由于工作者和中央参数服务器之间需要大量信息交换，通信延迟成为瓶颈。SignSGD与多数投票（signSGD-MV）是一种有效的分布式学习算法，通过一位量化可以显著减少通信成本。然而，由于异构的计算能力，当工作者之间的小批量大小不同时，它无法收敛。为了克服这一问题，我们提出了一种新颖的具有联邦投票的signSGD优化器（signSGD-FV）。联邦投票的理念在于利用可学习权重执行加权多数投票。服务器根据边缘设备的计算能力在线学习分配给这些设备的权重。",
    "tldr": "signSGD-FV是一种具有联邦投票的优化器，通过在线学习边缘设备的权重分配以克服SignSGD-MV在小批量大小不同的工作者间收敛困难的问题。",
    "en_tdlr": "signSGD-FV is an optimizer with federated voting that overcomes the convergence difficulty of SignSGD-MV among workers with different mini-batch sizes by online learning the weights assigned to edge devices."
}