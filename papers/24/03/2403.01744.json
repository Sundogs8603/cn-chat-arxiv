{
    "title": "NoteLLM: A Retrievable Large Language Model for Note Recommendation",
    "abstract": "arXiv:2403.01744v1 Announce Type: new  Abstract: People enjoy sharing \"notes\" including their experiences within online communities. Therefore, recommending notes aligned with user interests has become a crucial task. Existing online methods only input notes into BERT-based models to generate note embeddings for assessing similarity. However, they may underutilize some important cues, e.g., hashtags or categories, which represent the key concepts of notes. Indeed, learning to generate hashtags/categories can potentially enhance note embeddings, both of which compress key note information into limited content. Besides, Large Language Models (LLMs) have significantly outperformed BERT in understanding natural languages. It is promising to introduce LLMs into note recommendation. In this paper, we propose a novel unified framework called NoteLLM, which leverages LLMs to address the item-to-item (I2I) note recommendation. Specifically, we utilize Note Compression Prompt to compress a note ",
    "link": "https://arxiv.org/abs/2403.01744",
    "context": "Title: NoteLLM: A Retrievable Large Language Model for Note Recommendation\nAbstract: arXiv:2403.01744v1 Announce Type: new  Abstract: People enjoy sharing \"notes\" including their experiences within online communities. Therefore, recommending notes aligned with user interests has become a crucial task. Existing online methods only input notes into BERT-based models to generate note embeddings for assessing similarity. However, they may underutilize some important cues, e.g., hashtags or categories, which represent the key concepts of notes. Indeed, learning to generate hashtags/categories can potentially enhance note embeddings, both of which compress key note information into limited content. Besides, Large Language Models (LLMs) have significantly outperformed BERT in understanding natural languages. It is promising to introduce LLMs into note recommendation. In this paper, we propose a novel unified framework called NoteLLM, which leverages LLMs to address the item-to-item (I2I) note recommendation. Specifically, we utilize Note Compression Prompt to compress a note ",
    "path": "papers/24/03/2403.01744.json",
    "total_tokens": 939,
    "translated_title": "NoteLLM: 一种可检索的大型语言模型，用于笔记推荐",
    "translated_abstract": "人们喜欢在在线社区内分享“笔记”，包括他们的经验。因此，推荐与用户兴趣相符的笔记已经成为一项关键任务。现有的在线方法只将笔记输入到基于BERT的模型中，用于生成笔记嵌入以评估相似性。然而，它们可能未充分利用一些重要的线索，例如哈希标签或类别，这些代表了笔记的关键概念。事实上，学习生成哈希标签/类别可以潜在地增强笔记嵌入，二者都将重要的笔记信息压缩为有限内容。此外，大型语言模型（LLMs）在理解自然语言方面明显优于BERT。将LLMs引入笔记推荐是很有前途的。在本文中，我们提出了一种名为NoteLLM的新颖统一框架，利用LLMs来处理物品到物品（I2I）笔记推荐。具体来说，我们利用笔记压缩提示来压缩一条笔记",
    "tldr": "本文提出了一种名为NoteLLM的新颖统一框架，利用大型语言模型(LLMs)来实现物品到物品(I2I)的笔记推荐，通过学习生成哈希标签/类别潜在地增强笔记嵌入，提高了对关键笔记信息的压缩。"
}