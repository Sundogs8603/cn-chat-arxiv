{
    "title": "A spatiotemporal style transfer algorithm for dynamic visual stimulus generation",
    "abstract": "arXiv:2403.04940v1 Announce Type: cross  Abstract: Understanding how visual information is encoded in biological and artificial systems often requires vision scientists to generate appropriate stimuli to test specific hypotheses. Although deep neural network models have revolutionized the field of image generation with methods such as image style transfer, available methods for video generation are scarce. Here, we introduce the Spatiotemporal Style Transfer (STST) algorithm, a dynamic visual stimulus generation framework that allows powerful manipulation and synthesis of video stimuli for vision research. It is based on a two-stream deep neural network model that factorizes spatial and temporal features to generate dynamic visual stimuli whose model layer activations are matched to those of input videos. As an example, we show that our algorithm enables the generation of model metamers, dynamic stimuli whose layer activations within our two-stream model are matched to those of natural",
    "link": "https://arxiv.org/abs/2403.04940",
    "context": "Title: A spatiotemporal style transfer algorithm for dynamic visual stimulus generation\nAbstract: arXiv:2403.04940v1 Announce Type: cross  Abstract: Understanding how visual information is encoded in biological and artificial systems often requires vision scientists to generate appropriate stimuli to test specific hypotheses. Although deep neural network models have revolutionized the field of image generation with methods such as image style transfer, available methods for video generation are scarce. Here, we introduce the Spatiotemporal Style Transfer (STST) algorithm, a dynamic visual stimulus generation framework that allows powerful manipulation and synthesis of video stimuli for vision research. It is based on a two-stream deep neural network model that factorizes spatial and temporal features to generate dynamic visual stimuli whose model layer activations are matched to those of input videos. As an example, we show that our algorithm enables the generation of model metamers, dynamic stimuli whose layer activations within our two-stream model are matched to those of natural",
    "path": "papers/24/03/2403.04940.json",
    "total_tokens": 770,
    "translated_title": "用于动态视觉刺激生成的时空风格转移算法",
    "translated_abstract": "了解视觉信息如何在生物和人工系统中编码通常需要视觉科学家生成适当的刺激来测试特定的假设。尽管深度神经网络模型已经在图像生成领域引起革命，例如图像风格转移，但视频生成的方法却很少。在这里，我们介绍了时空风格转移（STST）算法，这是一个动态的视觉刺激生成框架，允许强大地操作和合成视频刺激用于视觉研究。它基于一个双流深度神经网络模型，分解空间和时间特征以生成动态视觉刺激，其模型层激活与输入视频的匹配。",
    "tldr": "提出了Spatiotemporal Style Transfer (STST)算法，基于双流深度神经网络模型，允许生成强大的动态视觉刺激，用于视觉研究。"
}