{
    "title": "Towards Provable Log Density Policy Gradient",
    "abstract": "arXiv:2403.01605v1 Announce Type: cross  Abstract: Policy gradient methods are a vital ingredient behind the success of modern reinforcement learning. Modern policy gradient methods, although successful, introduce a residual error in gradient estimation. In this work, we argue that this residual term is significant and correcting for it could potentially improve sample-complexity of reinforcement learning methods. To that end, we propose log density gradient to estimate the policy gradient, which corrects for this residual error term. Log density gradient method computes policy gradient by utilising the state-action discounted distributional formulation. We first present the equations needed to exactly find the log density gradient for a tabular Markov Decision Processes (MDPs). For more complex environments, we propose a temporal difference (TD) method that approximates log density gradient by utilizing backward on-policy samples. Since backward sampling from a Markov chain is highly ",
    "link": "https://arxiv.org/abs/2403.01605",
    "context": "Title: Towards Provable Log Density Policy Gradient\nAbstract: arXiv:2403.01605v1 Announce Type: cross  Abstract: Policy gradient methods are a vital ingredient behind the success of modern reinforcement learning. Modern policy gradient methods, although successful, introduce a residual error in gradient estimation. In this work, we argue that this residual term is significant and correcting for it could potentially improve sample-complexity of reinforcement learning methods. To that end, we propose log density gradient to estimate the policy gradient, which corrects for this residual error term. Log density gradient method computes policy gradient by utilising the state-action discounted distributional formulation. We first present the equations needed to exactly find the log density gradient for a tabular Markov Decision Processes (MDPs). For more complex environments, we propose a temporal difference (TD) method that approximates log density gradient by utilizing backward on-policy samples. Since backward sampling from a Markov chain is highly ",
    "path": "papers/24/03/2403.01605.json",
    "total_tokens": 832,
    "translated_title": "朝向可证明的对数密度策略梯度",
    "translated_abstract": "策略梯度方法是现代强化学习成功的关键要素。现代策略梯度方法虽然成功，但在梯度估计中引入了一个残差误差。本文认为这个残差项很重要，纠正它有可能改善强化学习方法的样本复杂度。为此，我们提出了对数密度梯度来估计策略梯度，可以纠正这个残差误差项。对数密度梯度方法通过利用状态-动作折扣分布形式来计算策略梯度。我们首先给出了准确找到标签马尔可夫决策过程（MDPs）的对数密度梯度所需的方程式。对于更复杂的环境，我们提出了一种利用后向即时（TD）方法来近似计算对数密度梯度的方法，通过利用后向的同策略样本。由于从马尔可夫链中进行后向采样是高度",
    "tldr": "提出对数密度梯度方法来估计策略梯度，修正残差误差，有望改善强化学习方法的样本复杂度。",
    "en_tdlr": "Introducing log density gradient method to estimate policy gradient, correcting residual errors, potentially improving sample-complexity of reinforcement learning methods."
}