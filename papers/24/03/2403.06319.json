{
    "title": "Fake or Compromised? Making Sense of Malicious Clients in Federated Learning",
    "abstract": "arXiv:2403.06319v1 Announce Type: new  Abstract: Federated learning (FL) is a distributed machine learning paradigm that enables training models on decentralized data. The field of FL security against poisoning attacks is plagued with confusion due to the proliferation of research that makes different assumptions about the capabilities of adversaries and the adversary models they operate under. Our work aims to clarify this confusion by presenting a comprehensive analysis of the various poisoning attacks and defensive aggregation rules (AGRs) proposed in the literature, and connecting them under a common framework. To connect existing adversary models, we present a hybrid adversary model, which lies in the middle of the spectrum of adversaries, where the adversary compromises a few clients, trains a generative (e.g., DDPM) model with their compromised samples, and generates new synthetic data to solve an optimization for a stronger (e.g., cheaper, more practical) attack against differe",
    "link": "https://arxiv.org/abs/2403.06319",
    "context": "Title: Fake or Compromised? Making Sense of Malicious Clients in Federated Learning\nAbstract: arXiv:2403.06319v1 Announce Type: new  Abstract: Federated learning (FL) is a distributed machine learning paradigm that enables training models on decentralized data. The field of FL security against poisoning attacks is plagued with confusion due to the proliferation of research that makes different assumptions about the capabilities of adversaries and the adversary models they operate under. Our work aims to clarify this confusion by presenting a comprehensive analysis of the various poisoning attacks and defensive aggregation rules (AGRs) proposed in the literature, and connecting them under a common framework. To connect existing adversary models, we present a hybrid adversary model, which lies in the middle of the spectrum of adversaries, where the adversary compromises a few clients, trains a generative (e.g., DDPM) model with their compromised samples, and generates new synthetic data to solve an optimization for a stronger (e.g., cheaper, more practical) attack against differe",
    "path": "papers/24/03/2403.06319.json",
    "total_tokens": 878,
    "translated_title": "伪造还是被篡改? 理解联邦学习中的恶意客户",
    "translated_abstract": "联邦学习（FL）是一种分布式机器学习范例，可以在分散的数据上训练模型。在防毒攻击方面，FL安全领域存在混乱，因为有许多研究假设对手的能力和对手模型存在不同。我们的工作旨在通过对文献中提出的各种毒害攻击和防御聚合规则（AGRs）进行全面分析，并在一个共同框架下予以连接来澄清这种混乱。为了联结现有的对手模型，我们提出了一个混合对手模型，位于对手光谱的中间位置，对手会损害一些客户，使用他们受损的样本训练生成（例如DDPM）模型，并生成新的合成数据以解决更强（例如更便宜，更实际）攻击的优化问题。",
    "tldr": "该研究致力于澄清联邦学习中对恶意客户的混淆，通过提出混合对手模型来连接现有的对手模型，分析各种毒害攻击和防御聚合规则，从而为该领域的安全研究提供指导.",
    "en_tdlr": "This study aims to clarify the confusion around malicious clients in federated learning by proposing a hybrid adversary model to connect existing adversary models, analyzing various poisoning attacks and defensive aggregation rules, providing guidance for security research in the field."
}