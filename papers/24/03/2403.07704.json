{
    "title": "Symmetric Q-learning: Reducing Skewness of Bellman Error in Online Reinforcement Learning",
    "abstract": "arXiv:2403.07704v1 Announce Type: cross  Abstract: In deep reinforcement learning, estimating the value function to evaluate the quality of states and actions is essential. The value function is often trained using the least squares method, which implicitly assumes a Gaussian error distribution. However, a recent study suggested that the error distribution for training the value function is often skewed because of the properties of the Bellman operator, and violates the implicit assumption of normal error distribution in the least squares method. To address this, we proposed a method called Symmetric Q-learning, in which the synthetic noise generated from a zero-mean distribution is added to the target values to generate a Gaussian error distribution. We evaluated the proposed method on continuous control benchmark tasks in MuJoCo. It improved the sample efficiency of a state-of-the-art reinforcement learning method by reducing the skewness of the error distribution.",
    "link": "https://arxiv.org/abs/2403.07704",
    "context": "Title: Symmetric Q-learning: Reducing Skewness of Bellman Error in Online Reinforcement Learning\nAbstract: arXiv:2403.07704v1 Announce Type: cross  Abstract: In deep reinforcement learning, estimating the value function to evaluate the quality of states and actions is essential. The value function is often trained using the least squares method, which implicitly assumes a Gaussian error distribution. However, a recent study suggested that the error distribution for training the value function is often skewed because of the properties of the Bellman operator, and violates the implicit assumption of normal error distribution in the least squares method. To address this, we proposed a method called Symmetric Q-learning, in which the synthetic noise generated from a zero-mean distribution is added to the target values to generate a Gaussian error distribution. We evaluated the proposed method on continuous control benchmark tasks in MuJoCo. It improved the sample efficiency of a state-of-the-art reinforcement learning method by reducing the skewness of the error distribution.",
    "path": "papers/24/03/2403.07704.json",
    "total_tokens": 805,
    "translated_title": "对称 Q-learning：减少在线强化学习中贝尔曼误差的偏斜",
    "translated_abstract": "在深度强化学习中，估计值函数以评估状态和动作的质量是至关重要的。该值函数通常使用最小二乘法进行训练，隐含地假定一个高斯误差分布。然而，最近的研究表明，由于贝尔曼算子的特性，用于训练值函数的误差分布通常是倾斜的，违反了最小二乘法中对正态误差分布的隐含假设。为了解决这个问题，我们提出了一种称为对称 Q-learning 的方法，其中从零均值分布生成的合成噪声被添加到目标值中，以生成高斯误差分布。我们在MuJoCo的连续控制基准任务上评估了所提出的方法。通过减少错误分布的偏斜，它提高了一种最先进的强化学习方法的样本效率。",
    "tldr": "对称 Q-learning方法通过添加合成噪声来减少贝尔曼误差的偏斜，在在线强化学习中提高了样本效率。",
    "en_tdlr": "Symmetric Q-learning method reduces skewness of Bellman error by adding synthetic noise, improving sample efficiency in online reinforcement learning."
}