{
    "title": "Localizing Paragraph Memorization in Language Models",
    "abstract": "arXiv:2403.19851v1 Announce Type: new  Abstract: Can we localize the weights and mechanisms used by a language model to memorize and recite entire paragraphs of its training data? In this paper, we show that while memorization is spread across multiple layers and model components, gradients of memorized paragraphs have a distinguishable spatial pattern, being larger in lower model layers than gradients of non-memorized examples. Moreover, the memorized examples can be unlearned by fine-tuning only the high-gradient weights. We localize a low-layer attention head that appears to be especially involved in paragraph memorization. This head is predominantly focusing its attention on distinctive, rare tokens that are least frequent in a corpus-level unigram distribution. Next, we study how localized memorization is across the tokens in the prefix by perturbing tokens and measuring the caused change in the decoding. A few distinctive tokens early in a prefix can often corrupt the entire cont",
    "link": "https://arxiv.org/abs/2403.19851",
    "context": "Title: Localizing Paragraph Memorization in Language Models\nAbstract: arXiv:2403.19851v1 Announce Type: new  Abstract: Can we localize the weights and mechanisms used by a language model to memorize and recite entire paragraphs of its training data? In this paper, we show that while memorization is spread across multiple layers and model components, gradients of memorized paragraphs have a distinguishable spatial pattern, being larger in lower model layers than gradients of non-memorized examples. Moreover, the memorized examples can be unlearned by fine-tuning only the high-gradient weights. We localize a low-layer attention head that appears to be especially involved in paragraph memorization. This head is predominantly focusing its attention on distinctive, rare tokens that are least frequent in a corpus-level unigram distribution. Next, we study how localized memorization is across the tokens in the prefix by perturbing tokens and measuring the caused change in the decoding. A few distinctive tokens early in a prefix can often corrupt the entire cont",
    "path": "papers/24/03/2403.19851.json",
    "total_tokens": 899,
    "translated_title": "将语言模型中的段落记忆本地化",
    "translated_abstract": "我们能否将语言模型用于记忆和背诵整个训练数据段的权重和机制本地化？本文表明，虽然记忆分布在多个层次和模型组件中，但记忆段落的梯度具有可区分的空间模式，较低模型层次中的梯度比非记忆示例的梯度更大。此外，这些记忆示例可以通过仅微调高梯度权重来取消学习。我们定位了一个似乎特别参与段落记忆的低层注意头。这个头部主要将注意力集中在在语料库级别的单语分布中最不频繁的独特、罕见的令牌上。接下来，我们通过扰动令牌并测量对解码造成的改变来研究记忆在前缀中的本地化程度。前缀中的一些独特令牌经常会使整个内容受损。",
    "tldr": "论文展示了语言模型中段落记忆的梯度具有可区分的空间模式，通过微调高梯度权重可以取消学习，定位了特别参与段落记忆的低层注意头，并研究了记忆在前缀中的本地化程度。",
    "en_tdlr": "The paper demonstrates that the gradients of paragraph memorization in language models have distinguishable spatial patterns, the memorization can be unlearned by fine-tuning high-gradient weights, a low-layer attention head is involved in paragraph memorization, and the localization of memorization across tokens in the prefix is studied."
}