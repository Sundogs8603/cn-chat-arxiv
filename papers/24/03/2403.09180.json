{
    "title": "Online and Offline Evaluation in Search Clarification",
    "abstract": "arXiv:2403.09180v1 Announce Type: new  Abstract: The effectiveness of clarification question models in engaging users within search systems is currently constrained, casting doubt on their overall usefulness. To improve the performance of these models, it is crucial to employ assessment approaches that encompass both real-time feedback from users (online evaluation) and the characteristics of clarification questions evaluated through human assessment (offline evaluation). However, the relationship between online and offline evaluations has been debated in information retrieval. This study aims to investigate how this discordance holds in search clarification. We use user engagement as ground truth and employ several offline labels to investigate to what extent the offline ranked lists of clarification resemble the ideal ranked lists based on online user engagement.",
    "link": "https://arxiv.org/abs/2403.09180",
    "context": "Title: Online and Offline Evaluation in Search Clarification\nAbstract: arXiv:2403.09180v1 Announce Type: new  Abstract: The effectiveness of clarification question models in engaging users within search systems is currently constrained, casting doubt on their overall usefulness. To improve the performance of these models, it is crucial to employ assessment approaches that encompass both real-time feedback from users (online evaluation) and the characteristics of clarification questions evaluated through human assessment (offline evaluation). However, the relationship between online and offline evaluations has been debated in information retrieval. This study aims to investigate how this discordance holds in search clarification. We use user engagement as ground truth and employ several offline labels to investigate to what extent the offline ranked lists of clarification resemble the ideal ranked lists based on online user engagement.",
    "path": "papers/24/03/2403.09180.json",
    "total_tokens": 752,
    "translated_title": "搜索澄清中的在线和离线评估",
    "translated_abstract": "目前，在搜索系统中，澄清问题模型在吸引用户方面的有效性受到限制，对其整体有用性产生了怀疑。要改善这些模型的性能，关键是采用既包括来自用户的实时反馈（在线评估），又通过人工评估评估澄清问题特征的评估方法。然而，在信息检索领域，关于在线和离线评估之间的关系存在争议。本研究旨在调查这种不一致在搜索澄清中的持续情况。我们以用户参与度作为基本事实，并使用多个离线标签来调查离线排名的澄清问题在多大程度上类似于基于在线用户参与度的理想排名列表。",
    "tldr": "研究调查搜索澄清中在线和离线评估之间的不一致情况，以用户参与度作为真实情况，探讨离线排名列表如何与基于在线用户参与度的理想排名列表相似。",
    "en_tdlr": "Investigating the discordance between online and offline evaluations in search clarification, using user engagement as ground truth to explore how offline ranked lists of clarification questions resemble ideal ranked lists based on online user engagement."
}