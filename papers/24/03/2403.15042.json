{
    "title": "LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement",
    "abstract": "arXiv:2403.15042v1 Announce Type: new  Abstract: Pretrained large language models (LLMs) are currently state-of-the-art for solving the vast majority of natural language processing tasks. While many real-world applications still require fine-tuning to reach satisfactory levels of performance, many of them are in the low-data regime, making fine-tuning challenging. To address this, we propose LLM2LLM, a targeted and iterative data augmentation strategy that uses a teacher LLM to enhance a small seed dataset by augmenting additional data that can be used for fine-tuning on a specific task. LLM2LLM (1) fine-tunes a baseline student LLM on the initial seed data, (2) evaluates and extracts data points that the model gets wrong, and (3) uses a teacher LLM to generate synthetic data based on these incorrect data points, which are then added back into the training data. This approach amplifies the signal from incorrectly predicted data points by the LLM during training and reintegrates them in",
    "link": "https://arxiv.org/abs/2403.15042",
    "context": "Title: LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement\nAbstract: arXiv:2403.15042v1 Announce Type: new  Abstract: Pretrained large language models (LLMs) are currently state-of-the-art for solving the vast majority of natural language processing tasks. While many real-world applications still require fine-tuning to reach satisfactory levels of performance, many of them are in the low-data regime, making fine-tuning challenging. To address this, we propose LLM2LLM, a targeted and iterative data augmentation strategy that uses a teacher LLM to enhance a small seed dataset by augmenting additional data that can be used for fine-tuning on a specific task. LLM2LLM (1) fine-tunes a baseline student LLM on the initial seed data, (2) evaluates and extracts data points that the model gets wrong, and (3) uses a teacher LLM to generate synthetic data based on these incorrect data points, which are then added back into the training data. This approach amplifies the signal from incorrectly predicted data points by the LLM during training and reintegrates them in",
    "path": "papers/24/03/2403.15042.json",
    "total_tokens": 901,
    "translated_title": "LLM2LLM: 利用新的迭代数据增强技术增强LLM",
    "translated_abstract": "预训练的大型语言模型（LLMs）目前是解决绝大多数自然语言处理任务的最先进技术。尽管许多现实世界应用仍需要微调以达到令人满意的性能水平，但其中许多应用处于低数据范围，使得微调变得具有挑战性。为了解决这个问题，我们提出了LLM2LLM，这是一种有针对性和迭代的数据增强策略，利用一个教师LLM来增强一个小的种子数据集，通过增加额外的数据用于针对特定任务的微调。LLM2LLM（1）在初始种子数据上微调基线学生LLM，（2）评估和提取模型错误的数据点，（3）使用教师LLM根据这些错误的数据点生成合成数据，然后将其添加回训练数据中。这种方法通过在训练过程中增强LLM对错误预测数据点的信号，并重新整合它们。",
    "tldr": "LLM2LLM 提出了一种迭代数据增强策略，通过使用教师LLM生成合成数据并将其添加回训练数据，从而帮助低数据环境下的LLM进行微调。",
    "en_tdlr": "LLM2LLM proposes an iterative data enhancement strategy that leverages a teacher LLM to generate synthetic data and add it back to the training data, aiding in fine-tuning LLM in low-data settings."
}