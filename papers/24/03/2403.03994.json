{
    "title": "Video Relationship Detection Using Mixture of Experts",
    "abstract": "arXiv:2403.03994v1 Announce Type: cross  Abstract: Machine comprehension of visual information from images and videos by neural networks faces two primary challenges. Firstly, there exists a computational and inference gap in connecting vision and language, making it difficult to accurately determine which object a given agent acts on and represent it through language. Secondly, classifiers trained by a single, monolithic neural network often lack stability and generalization. To overcome these challenges, we introduce MoE-VRD, a novel approach to visual relationship detection utilizing a mixture of experts. MoE-VRD identifies language triplets in the form of < subject, predicate, object> tuples to extract relationships from visual processing. Leveraging recent advancements in visual relationship detection, MoE-VRD addresses the requirement for action recognition in establishing relationships between subjects (acting) and objects (being acted upon). In contrast to single monolithic net",
    "link": "https://arxiv.org/abs/2403.03994",
    "context": "Title: Video Relationship Detection Using Mixture of Experts\nAbstract: arXiv:2403.03994v1 Announce Type: cross  Abstract: Machine comprehension of visual information from images and videos by neural networks faces two primary challenges. Firstly, there exists a computational and inference gap in connecting vision and language, making it difficult to accurately determine which object a given agent acts on and represent it through language. Secondly, classifiers trained by a single, monolithic neural network often lack stability and generalization. To overcome these challenges, we introduce MoE-VRD, a novel approach to visual relationship detection utilizing a mixture of experts. MoE-VRD identifies language triplets in the form of < subject, predicate, object> tuples to extract relationships from visual processing. Leveraging recent advancements in visual relationship detection, MoE-VRD addresses the requirement for action recognition in establishing relationships between subjects (acting) and objects (being acted upon). In contrast to single monolithic net",
    "path": "papers/24/03/2403.03994.json",
    "total_tokens": 796,
    "translated_title": "使用专家混合模型进行视频关系检测",
    "translated_abstract": "从图像和视频中通过神经网络对视觉信息进行机器理解面临两个主要挑战。首先，连接视觉和语言存在计算和推理差距，难以精确确定给定代理作用于哪个对象并通过语言表示。其次，由单一的整体神经网络训练的分类器通常缺乏稳定性和泛化能力。为了克服这些挑战，我们引入MoE-VRD，一种利用专家混合模型进行视觉关系检测的新方法。MoE-VRD通过在视觉处理中提取以<主语，谓词，宾语>元组形式的语言三元组来识别关系。利用视觉关系检测的最新进展，MoE-VRD解决了在建立主体（执行动作）和客体（受到作用）之间的关系时对动作识别的要求。与单一整体网络相比，",
    "tldr": "使用专家混合模型进行视频关系检测，解决了连接视觉和语言的计算和推理差距，提高了稳定性和泛化能力。",
    "en_tdlr": "Utilizing a mixture of experts for video relationship detection addresses the computational and inference gap between vision and language, improving stability and generalization."
}