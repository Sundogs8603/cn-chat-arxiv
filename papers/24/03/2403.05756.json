{
    "title": "Model-Free Local Recalibration of Neural Networks",
    "abstract": "arXiv:2403.05756v1 Announce Type: cross  Abstract: Artificial neural networks (ANNs) are highly flexible predictive models. However, reliably quantifying uncertainty for their predictions is a continuing challenge. There has been much recent work on \"recalibration\" of predictive distributions for ANNs, so that forecast probabilities for events of interest are consistent with certain frequency evaluations of them. Uncalibrated probabilistic forecasts are of limited use for many important decision-making tasks. To address this issue, we propose a localized recalibration of ANN predictive distributions using the dimension-reduced representation of the input provided by the ANN hidden layers. Our novel method draws inspiration from recalibration techniques used in the literature on approximate Bayesian computation and likelihood-free inference methods. Most existing calibration methods for ANNs can be thought of as calibrating either on the input layer, which is difficult when the input is",
    "link": "https://arxiv.org/abs/2403.05756",
    "context": "Title: Model-Free Local Recalibration of Neural Networks\nAbstract: arXiv:2403.05756v1 Announce Type: cross  Abstract: Artificial neural networks (ANNs) are highly flexible predictive models. However, reliably quantifying uncertainty for their predictions is a continuing challenge. There has been much recent work on \"recalibration\" of predictive distributions for ANNs, so that forecast probabilities for events of interest are consistent with certain frequency evaluations of them. Uncalibrated probabilistic forecasts are of limited use for many important decision-making tasks. To address this issue, we propose a localized recalibration of ANN predictive distributions using the dimension-reduced representation of the input provided by the ANN hidden layers. Our novel method draws inspiration from recalibration techniques used in the literature on approximate Bayesian computation and likelihood-free inference methods. Most existing calibration methods for ANNs can be thought of as calibrating either on the input layer, which is difficult when the input is",
    "path": "papers/24/03/2403.05756.json",
    "total_tokens": 842,
    "translated_title": "无模型本地重新校准神经网络",
    "translated_abstract": "人工神经网络（ANNs）是高度灵活的预测模型。然而，可靠地量化其预测的不确定性仍然是一个持续的挑战。最近关于对ANNs的预测分布进行“重新校准”的工作很多，使得感兴趣事件的预测概率与对它们的某些频率评估一致。未校准的概率预测对于许多重要的决策任务有限用处。为了解决这个问题，我们提出了一种使用ANN隐层提供的输入的降维表示来对ANN预测分布进行本地重新校准的方法。我们的新颖方法受到了文献中用于近似贝叶斯计算和无似然推断方法的重新校准技术的启发。大多数现有的ANN校准方法可以被认为是在输入层上进行校准，但当输入是...",
    "tldr": "提出了一种使用ANN隐层提供的输入的降维表示来对ANN预测分布进行本地重新校准的方法，从文献中近似贝叶斯计算和无似然推断方法的重新校准技术中获取灵感。",
    "en_tdlr": "Proposed a method of locally recalibrating ANN predictive distributions using the dimension-reduced representation of the input provided by the ANN hidden layers, drawing inspiration from recalibration techniques in the literature on approximate Bayesian computation and likelihood-free inference methods."
}