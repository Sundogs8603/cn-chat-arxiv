{
    "title": "SEVEN: Pruning Transformer Model by Reserving Sentinels",
    "abstract": "arXiv:2403.12688v1 Announce Type: new  Abstract: Large-scale Transformer models (TM) have demonstrated outstanding performance across various tasks. However, their considerable parameter size restricts their applicability, particularly on mobile devices. Due to the dynamic and intricate nature of gradients on TM compared to Convolutional Neural Networks, commonly used pruning methods tend to retain weights with larger gradient noise. This results in pruned models that are sensitive to sparsity and datasets, exhibiting suboptimal performance. Symbolic Descent (SD) is a general approach for training and fine-tuning TM. In this paper, we attempt to describe the noisy batch gradient sequences on TM through the cumulative process of SD. We utilize this design to dynamically assess the importance scores of weights.SEVEN is introduced by us, which particularly favors weights with consistently high sensitivity, i.e., weights with small gradient noise. These weights are tended to be preserved b",
    "link": "https://arxiv.org/abs/2403.12688",
    "context": "Title: SEVEN: Pruning Transformer Model by Reserving Sentinels\nAbstract: arXiv:2403.12688v1 Announce Type: new  Abstract: Large-scale Transformer models (TM) have demonstrated outstanding performance across various tasks. However, their considerable parameter size restricts their applicability, particularly on mobile devices. Due to the dynamic and intricate nature of gradients on TM compared to Convolutional Neural Networks, commonly used pruning methods tend to retain weights with larger gradient noise. This results in pruned models that are sensitive to sparsity and datasets, exhibiting suboptimal performance. Symbolic Descent (SD) is a general approach for training and fine-tuning TM. In this paper, we attempt to describe the noisy batch gradient sequences on TM through the cumulative process of SD. We utilize this design to dynamically assess the importance scores of weights.SEVEN is introduced by us, which particularly favors weights with consistently high sensitivity, i.e., weights with small gradient noise. These weights are tended to be preserved b",
    "path": "papers/24/03/2403.12688.json",
    "total_tokens": 807,
    "translated_title": "SEVEN: 通过保留哨兵来剪枝Transformer模型",
    "translated_abstract": "大规模Transformer模型已经在各种任务中展现出卓越的性能。然而，由于其可观的参数规模，它们的适用性受到限制，尤其是在移动设备上。鉴于Transformer模型相对于卷积神经网络的梯度是动态且错综复杂的，常用的剪枝方法往往会保留具有较大梯度噪声的权重。这导致被剪枝的模型对稀疏性和数据集敏感，表现出次优性能。符号下降（SD）是一种用于训练和微调Transformer模型的通用方法。在本文中，我们试图通过SD的累积过程描述Transformer模型上的噪声批梯度序列。我们利用这一设计动态评估权重的重要性分数。我们引入了SEVEN，特别偏向于具有持续高敏感度的权重，即梯度噪声较小的权重。",
    "tldr": "SEVEN通过保留梯度噪声较小的权重，在剪枝Transformer模型时取得了优异的效果。",
    "en_tdlr": "SEVEN achieves excellent results in pruning Transformer models by preserving weights with small gradient noise."
}