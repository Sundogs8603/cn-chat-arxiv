{
    "title": "UniTS: Building a Unified Time Series Model",
    "abstract": "arXiv:2403.00131v1 Announce Type: cross  Abstract: Foundation models, especially LLMs, are profoundly transforming deep learning. Instead of training many task-specific models, we can adapt a single pretrained model to many tasks via fewshot prompting or fine-tuning. However, current foundation models apply to sequence data but not to time series, which present unique challenges due to the inherent diverse and multidomain time series datasets, diverging task specifications across forecasting, classification and other types of tasks, and the apparent need for task-specialized models. We developed UNITS, a unified time series model that supports a universal task specification, accommodating classification, forecasting, imputation, and anomaly detection tasks. This is achieved through a novel unified network backbone, which incorporates sequence and variable attention along with a dynamic linear operator and is trained as a unified model. Across 38 multi-domain datasets, UNITS demonstrate",
    "link": "https://arxiv.org/abs/2403.00131",
    "context": "Title: UniTS: Building a Unified Time Series Model\nAbstract: arXiv:2403.00131v1 Announce Type: cross  Abstract: Foundation models, especially LLMs, are profoundly transforming deep learning. Instead of training many task-specific models, we can adapt a single pretrained model to many tasks via fewshot prompting or fine-tuning. However, current foundation models apply to sequence data but not to time series, which present unique challenges due to the inherent diverse and multidomain time series datasets, diverging task specifications across forecasting, classification and other types of tasks, and the apparent need for task-specialized models. We developed UNITS, a unified time series model that supports a universal task specification, accommodating classification, forecasting, imputation, and anomaly detection tasks. This is achieved through a novel unified network backbone, which incorporates sequence and variable attention along with a dynamic linear operator and is trained as a unified model. Across 38 multi-domain datasets, UNITS demonstrate",
    "path": "papers/24/03/2403.00131.json",
    "total_tokens": 840,
    "translated_title": "UniTS: 构建统一的时间序列模型",
    "translated_abstract": "基础模型，特别是LLMs，正在深度学习中产生深远影响。我们可以通过少量提示或微调将单个预训练模型适应于许多任务，而不是训练许多特定任务的模型。然而，当前的基础模型适用于序列数据，但不适用于时间序列，因为时间序列具有独特的挑战，包括固有多样性和多领域时间序列数据集，预测、分类和其他类型任务之间的任务规范分歧，以及对任务专用模型的明显需求。我们开发了UNITS，一种支持通用任务规范的统一时间序列模型，可容纳分类、预测、插补和异常检测任务。这是通过一种新颖的统一网络骨干实现的，该骨干结合了序列和变量注意力以及动态线性算子，并作为统一模型进行训练。在38个多领域数据集上，UNITS展示",
    "tldr": "UNITS是一种统一的时间序列模型，通过独特的统一网络骨干实现了通用任务规范，并成功支持多种任务，包括分类、预测、插补和异常检测。"
}