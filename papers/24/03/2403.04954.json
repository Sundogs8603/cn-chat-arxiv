{
    "title": "Fooling Neural Networks for Motion Forecasting via Adversarial Attacks",
    "abstract": "arXiv:2403.04954v1 Announce Type: cross  Abstract: Human motion prediction is still an open problem, which is extremely important for autonomous driving and safety applications. Although there are great advances in this area, the widely studied topic of adversarial attacks has not been applied to multi-regression models such as GCNs and MLP-based architectures in human motion prediction. This work intends to reduce this gap using extensive quantitative and qualitative experiments in state-of-the-art architectures similar to the initial stages of adversarial attacks in image classification. The results suggest that models are susceptible to attacks even on low levels of perturbation. We also show experiments with 3D transformations that affect the model performance, in particular, we show that most models are sensitive to simple rotations and translations which do not alter joint distances. We conclude that similar to earlier CNN models, motion forecasting tasks are susceptible to small",
    "link": "https://arxiv.org/abs/2403.04954",
    "context": "Title: Fooling Neural Networks for Motion Forecasting via Adversarial Attacks\nAbstract: arXiv:2403.04954v1 Announce Type: cross  Abstract: Human motion prediction is still an open problem, which is extremely important for autonomous driving and safety applications. Although there are great advances in this area, the widely studied topic of adversarial attacks has not been applied to multi-regression models such as GCNs and MLP-based architectures in human motion prediction. This work intends to reduce this gap using extensive quantitative and qualitative experiments in state-of-the-art architectures similar to the initial stages of adversarial attacks in image classification. The results suggest that models are susceptible to attacks even on low levels of perturbation. We also show experiments with 3D transformations that affect the model performance, in particular, we show that most models are sensitive to simple rotations and translations which do not alter joint distances. We conclude that similar to earlier CNN models, motion forecasting tasks are susceptible to small",
    "path": "papers/24/03/2403.04954.json",
    "total_tokens": 871,
    "translated_title": "通过对抗性攻击欺骗神经网络进行动作预测",
    "translated_abstract": "人体动作预测仍然是一个需要解决的开放问题，对于自动驾驶和安全应用非常重要。尽管该领域取得了巨大进展，但广泛研究的对抗性攻击主题尚未应用于人体动作预测中的多回归模型，如GCNs和基于MLP的架构。该工作旨在通过对类似于图像分类中对抗性攻击初始阶段的最先进架构进行广泛的定量和定性实验来缩小这一差距。结果表明，即使在低水平扰动上，模型也容易受到攻击。我们还展示了影响模型性能的三维变换实验，特别是我们展示了大多数模型对简单的旋转和平移敏感，这些变换不会改变关节距离。我们得出结论，类似早期CNN模型一样，动作预测任务易受到小的攻击。",
    "tldr": "该研究在人体动作预测领域引入了对抗性攻击，通过实验证实模型即使在低水平的扰动下也容易受到攻击，并展示了对简单旋转和平移敏感的模型性能受影响。",
    "en_tdlr": "This study introduces adversarial attacks into human motion prediction, demonstrating through experiments that models are susceptible to attacks even on low levels of perturbation, and shows how the performance of models sensitive to simple rotations and translations is affected."
}