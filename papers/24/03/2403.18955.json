{
    "title": "Structurally Prune Anything: Any Architecture, Any Framework, Any Time",
    "abstract": "arXiv:2403.18955v1 Announce Type: new  Abstract: Neural network pruning serves as a critical technique for enhancing the efficiency of deep learning models. Unlike unstructured pruning, which only sets specific parameters to zero, structured pruning eliminates entire channels, thus yielding direct computational and storage benefits. However, the diverse patterns for coupling parameters, such as residual connections and group convolutions, the diverse deep learning frameworks, and the various time stages at which pruning can be performed make existing pruning methods less adaptable to different architectures, frameworks, and pruning criteria. To address this, we introduce Structurally Prune Anything (SPA), a versatile structured pruning framework that can prune neural networks with any architecture, from any framework, and at any stage of training. SPA leverages a standardized computational graph and ONNX representation to prune diverse neural network architectures without the need for ",
    "link": "https://arxiv.org/abs/2403.18955",
    "context": "Title: Structurally Prune Anything: Any Architecture, Any Framework, Any Time\nAbstract: arXiv:2403.18955v1 Announce Type: new  Abstract: Neural network pruning serves as a critical technique for enhancing the efficiency of deep learning models. Unlike unstructured pruning, which only sets specific parameters to zero, structured pruning eliminates entire channels, thus yielding direct computational and storage benefits. However, the diverse patterns for coupling parameters, such as residual connections and group convolutions, the diverse deep learning frameworks, and the various time stages at which pruning can be performed make existing pruning methods less adaptable to different architectures, frameworks, and pruning criteria. To address this, we introduce Structurally Prune Anything (SPA), a versatile structured pruning framework that can prune neural networks with any architecture, from any framework, and at any stage of training. SPA leverages a standardized computational graph and ONNX representation to prune diverse neural network architectures without the need for ",
    "path": "papers/24/03/2403.18955.json",
    "total_tokens": 855,
    "translated_title": "任何结构裁剪：任何架构，任何框架，任何时候",
    "translated_abstract": "arXiv:2403.18955v1 公告类型：新  摘要：神经网络剪枝是增强深度学习模型效率的关键技术。与无结构剪枝不同，后者仅将特定参数设置为零，结构剪枝消除了整个通道，从而产生直接的计算和存储优势。然而，不同的耦合参数模式，如残差连接和组卷积，不同的深度学习框架，以及可以执行剪枝的各种时间阶段使得现有的剪枝方法对不同架构、框架和剪枝准则 less adaptable。为了解决这个问题，我们引入了Structurally Prune Anything（SPA），这是一个多功能的结构剪枝框架，可以对任何架构，任何框架的神经网络进行剪枝，而且可以在训练的任何阶段进行剪枝。SPA利用标准化的计算图和ONNX表示来对不同的神经网络架构进行剪枝，而无需",
    "tldr": "SPA是一个多功能的结构剪枝框架，可以对任何架构、任何框架的神经网络进行剪枝，而且可以在任何训练阶段进行剪枝。",
    "en_tdlr": "SPA is a versatile structured pruning framework that can prune neural networks with any architecture, from any framework, and at any stage of training."
}