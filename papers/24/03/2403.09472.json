{
    "title": "Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision",
    "abstract": "arXiv:2403.09472v1 Announce Type: cross  Abstract: Current AI alignment methodologies rely on human-provided demonstrations or judgments, and the learned capabilities of AI systems would be upper-bounded by human capabilities as a result. This raises a challenging research question: How can we keep improving the systems when their capabilities have surpassed the levels of humans? This paper answers this question in the context of tackling hard reasoning tasks (e.g., level 4-5 MATH problems) via learning from human annotations on easier tasks (e.g., level 1-3 MATH problems), which we term as \\textit{easy-to-hard generalization}. Our key insight is that an evaluator (reward model) trained on supervisions for easier tasks can be effectively used for scoring candidate solutions of harder tasks and hence facilitating easy-to-hard generalization over different levels of tasks. Based on this insight, we propose a novel approach to scalable alignment, which firstly trains the process-supervise",
    "link": "https://arxiv.org/abs/2403.09472",
    "context": "Title: Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision\nAbstract: arXiv:2403.09472v1 Announce Type: cross  Abstract: Current AI alignment methodologies rely on human-provided demonstrations or judgments, and the learned capabilities of AI systems would be upper-bounded by human capabilities as a result. This raises a challenging research question: How can we keep improving the systems when their capabilities have surpassed the levels of humans? This paper answers this question in the context of tackling hard reasoning tasks (e.g., level 4-5 MATH problems) via learning from human annotations on easier tasks (e.g., level 1-3 MATH problems), which we term as \\textit{easy-to-hard generalization}. Our key insight is that an evaluator (reward model) trained on supervisions for easier tasks can be effectively used for scoring candidate solutions of harder tasks and hence facilitating easy-to-hard generalization over different levels of tasks. Based on this insight, we propose a novel approach to scalable alignment, which firstly trains the process-supervise",
    "path": "papers/24/03/2403.09472.json",
    "total_tokens": 838,
    "translated_title": "易于难的泛化：超越人类监督的可扩展对齐",
    "translated_abstract": "当前人工智能对齐方法依赖于人类提供的演示或判断，由于这种方法，AI系统学习到的能力将受到人类能力的上界限制。这就带来了一个具有挑战性的研究问题：当系统的能力超过人类水平时，我们如何继续改进这些系统？本文在解决难度推理任务（如4-5级数学问题）的背景下回答了这个问题，通过从更简单的任务（如1-3级数学问题）中学习人类注释，我们将其称为“易于难的泛化”。我们的关键观点是，一个在更简单任务的监督下训练的评估器（奖励模型）可以有效地用于评分更难任务的候选解决方案，从而促进在不同难度任务间的易于难的泛化。基于这一观点，我们提出了一种新的可扩展对齐方法，首先训练处理督导",
    "tldr": "通过从更简单的任务学习，实现对更难推理任务的有效泛化，提出了一种可扩展对齐方法。",
    "en_tdlr": "Achieving effective generalization to harder reasoning tasks by learning from easier tasks, proposing a scalable alignment approach."
}