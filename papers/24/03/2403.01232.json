{
    "title": "Polynormer: Polynomial-Expressive Graph Transformer in Linear Time",
    "abstract": "arXiv:2403.01232v1 Announce Type: cross  Abstract: Graph transformers (GTs) have emerged as a promising architecture that is theoretically more expressive than message-passing graph neural networks (GNNs). However, typical GT models have at least quadratic complexity and thus cannot scale to large graphs. While there are several linear GTs recently proposed, they still lag behind GNN counterparts on several popular graph datasets, which poses a critical concern on their practical expressivity. To balance the trade-off between expressivity and scalability of GTs, we propose Polynormer, a polynomial-expressive GT model with linear complexity. Polynormer is built upon a novel base model that learns a high-degree polynomial on input features. To enable the base model permutation equivariant, we integrate it with graph topology and node features separately, resulting in local and global equivariant attention models. Consequently, Polynormer adopts a linear local-to-global attention scheme t",
    "link": "https://arxiv.org/abs/2403.01232",
    "context": "Title: Polynormer: Polynomial-Expressive Graph Transformer in Linear Time\nAbstract: arXiv:2403.01232v1 Announce Type: cross  Abstract: Graph transformers (GTs) have emerged as a promising architecture that is theoretically more expressive than message-passing graph neural networks (GNNs). However, typical GT models have at least quadratic complexity and thus cannot scale to large graphs. While there are several linear GTs recently proposed, they still lag behind GNN counterparts on several popular graph datasets, which poses a critical concern on their practical expressivity. To balance the trade-off between expressivity and scalability of GTs, we propose Polynormer, a polynomial-expressive GT model with linear complexity. Polynormer is built upon a novel base model that learns a high-degree polynomial on input features. To enable the base model permutation equivariant, we integrate it with graph topology and node features separately, resulting in local and global equivariant attention models. Consequently, Polynormer adopts a linear local-to-global attention scheme t",
    "path": "papers/24/03/2403.01232.json",
    "total_tokens": 862,
    "translated_title": "Polynormer: 多项式表达的线性时间图转换器",
    "translated_abstract": "图转换器（GTs）已经成为一种有前途的架构，理论上它比消息传递图神经网络（GNNs）更具表现力。然而，典型的GT模型至少具有二次复杂度，因此无法扩展到大型图。虽然最近提出了几种线性GTs，但它们在几个热门图数据集上仍落后于GNN对应模型，这对于它们的实际表现力构成了一个重要关注点。为了平衡GTs的表现力和可扩展性之间的权衡，我们提出了Polynormer，一个具有线性复杂度的多项式表达GT模型。Polynormer构建在一个新颖的基础模型上，该模型在输入特征上学习高次多项式。为了使基础模型具有置换等变性，我们将其与图拓扑和节点特征分开集成，从而产生本地和全局等变关注模型。因此，Polynormer采用了线性的局部到全局关注方案。",
    "tldr": "Polynormer提出了一种多项式表达GT模型，具有线性复杂度，结合本地和全局等变注意力模型，平衡了表现力和可扩展性。",
    "en_tdlr": "Polynormer introduces a polynomial-expressive GT model with linear complexity, incorporating local and global equivariant attention models to balance expressiveness and scalability."
}