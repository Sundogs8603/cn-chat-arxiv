{
    "title": "Towards Efficient Replay in Federated Incremental Learning",
    "abstract": "arXiv:2403.05890v1 Announce Type: new  Abstract: In Federated Learning (FL), the data in each client is typically assumed fixed or static. However, data often comes in an incremental manner in real-world applications, where the data domain may increase dynamically. In this work, we study catastrophic forgetting with data heterogeneity in Federated Incremental Learning (FIL) scenarios where edge clients may lack enough storage space to retain full data. We propose to employ a simple, generic framework for FIL named Re-Fed, which can coordinate each client to cache important samples for replay. More specifically, when a new task arrives, each client first caches selected previous samples based on their global and local importance. Then, the client trains the local model with both the cached samples and the samples from the new task. Theoretically, we analyze the ability of Re-Fed to discover important samples for replay thus alleviating the catastrophic forgetting problem. Moreover, we e",
    "link": "https://arxiv.org/abs/2403.05890",
    "context": "Title: Towards Efficient Replay in Federated Incremental Learning\nAbstract: arXiv:2403.05890v1 Announce Type: new  Abstract: In Federated Learning (FL), the data in each client is typically assumed fixed or static. However, data often comes in an incremental manner in real-world applications, where the data domain may increase dynamically. In this work, we study catastrophic forgetting with data heterogeneity in Federated Incremental Learning (FIL) scenarios where edge clients may lack enough storage space to retain full data. We propose to employ a simple, generic framework for FIL named Re-Fed, which can coordinate each client to cache important samples for replay. More specifically, when a new task arrives, each client first caches selected previous samples based on their global and local importance. Then, the client trains the local model with both the cached samples and the samples from the new task. Theoretically, we analyze the ability of Re-Fed to discover important samples for replay thus alleviating the catastrophic forgetting problem. Moreover, we e",
    "path": "papers/24/03/2403.05890.json",
    "total_tokens": 877,
    "translated_title": "朝着联邦增量学习中高效的重播",
    "translated_abstract": "在联邦学习（FL）中，通常假定每个客户端的数据是固定或静态的。然而，在现实世界的应用中，数据通常以增量方式到来，其中数据领域可能动态增加。在这项工作中，我们研究了在边缘客户端在联邦增量学习（FIL）场景中因数据异构性而可能缺乏足够存储空间以保留完整数据的灾难性遗忘。我们提出了一种名为Re-Fed的简单、通用的FIL框架，它可以协调每个客户端缓存重播的重要样本。具体而言，当出现新任务时，每个客户端首先基于它们的全局和本地重要性缓存选定的先前样本。然后，客户端使用既缓存的样本又使用新任务的样本训练本地模型。在理论上，我们分析了Re-Fed发现重播重要样本的能力，从而缓解了灾难性遗忘问题。",
    "tldr": "本研究提出了一种名为Re-Fed的简单通用框架，用于联邦增量学习中的重播，通过协调每个客户端缓存重要样本以减轻灾难性遗忘问题。"
}