{
    "title": "Detecting Bias in Large Language Models: Fine-tuned KcBERT",
    "abstract": "arXiv:2403.10774v1 Announce Type: new  Abstract: The rapid advancement of large language models (LLMs) has enabled natural language processing capabilities similar to those of humans, and LLMs are being widely utilized across various societal domains such as education and healthcare. While the versatility of these models has increased, they have the potential to generate subjective and normative language, leading to discriminatory treatment or outcomes among social groups, especially due to online offensive language. In this paper, we define such harm as societal bias and assess ethnic, gender, and racial biases in a model fine-tuned with Korean comments using Bidirectional Encoder Representations from Transformers (KcBERT) and KOLD data through template-based Masked Language Modeling (MLM). To quantitatively evaluate biases, we employ LPBS and CBS metrics. Compared to KcBERT, the fine-tuned model shows a reduction in ethnic bias but demonstrates significant changes in gender and racia",
    "link": "https://arxiv.org/abs/2403.10774",
    "context": "Title: Detecting Bias in Large Language Models: Fine-tuned KcBERT\nAbstract: arXiv:2403.10774v1 Announce Type: new  Abstract: The rapid advancement of large language models (LLMs) has enabled natural language processing capabilities similar to those of humans, and LLMs are being widely utilized across various societal domains such as education and healthcare. While the versatility of these models has increased, they have the potential to generate subjective and normative language, leading to discriminatory treatment or outcomes among social groups, especially due to online offensive language. In this paper, we define such harm as societal bias and assess ethnic, gender, and racial biases in a model fine-tuned with Korean comments using Bidirectional Encoder Representations from Transformers (KcBERT) and KOLD data through template-based Masked Language Modeling (MLM). To quantitatively evaluate biases, we employ LPBS and CBS metrics. Compared to KcBERT, the fine-tuned model shows a reduction in ethnic bias but demonstrates significant changes in gender and racia",
    "path": "papers/24/03/2403.10774.json",
    "total_tokens": 888,
    "translated_title": "检测大型语言模型中的偏见：Fine-tuned KcBERT",
    "translated_abstract": "大型语言模型（LLMs）的快速发展实现了类似于人类的自然语言处理能力，LLMs被广泛应用于教育和医疗等各个社会领域。然而，这些模型的多功能性增加了，但它们有可能产生主观和规范性语言，导致在社会群体中出现歧视性对待或结果，特别是由于在线侮辱性语言。本文将此类伤害定义为社会偏见，并评估了使用KcBERT和KOLD数据通过基于模板的遮蔽语言建模（MLM）对韩语评论进行微调的模型中的种族、性别和种族偏见。为了定量评估偏见，我们采用了LPBS和CBS指标。与KcBERT相比，微调模型减少了种族偏见，但在性别和种族方面显示出显著变化。",
    "tldr": "在本研究中，作者通过使用KcBERT和KOLD数据对韩语评论进行微调，通过遮蔽语言建模来评估大型语言模型中的社会偏见，发现微调后的模型降低了种族偏见，但在性别和种族方面表现出显著变化。",
    "en_tdlr": "In this study, the authors evaluate societal bias in large language models by fine-tuning with Korean comments using KcBERT and KOLD data through masked language modeling, finding a reduction in ethnic bias but significant changes in gender and racial biases in the fine-tuned model."
}