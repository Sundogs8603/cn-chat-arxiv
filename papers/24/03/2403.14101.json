{
    "title": "Text-Enhanced Data-free Approach for Federated Class-Incremental Learning",
    "abstract": "arXiv:2403.14101v1 Announce Type: cross  Abstract: Federated Class-Incremental Learning (FCIL) is an underexplored yet pivotal issue, involving the dynamic addition of new classes in the context of federated learning. In this field, Data-Free Knowledge Transfer (DFKT) plays a crucial role in addressing catastrophic forgetting and data privacy problems. However, prior approaches lack the crucial synergy between DFKT and the model training phases, causing DFKT to encounter difficulties in generating high-quality data from a non-anchored latent space of the old task model. In this paper, we introduce LANDER (Label Text Centered Data-Free Knowledge Transfer) to address this issue by utilizing label text embeddings (LTE) produced by pretrained language models. Specifically, during the model training phase, our approach treats LTE as anchor points and constrains the feature embeddings of corresponding training samples around them, enriching the surrounding area with more meaningful informati",
    "link": "https://arxiv.org/abs/2403.14101",
    "context": "Title: Text-Enhanced Data-free Approach for Federated Class-Incremental Learning\nAbstract: arXiv:2403.14101v1 Announce Type: cross  Abstract: Federated Class-Incremental Learning (FCIL) is an underexplored yet pivotal issue, involving the dynamic addition of new classes in the context of federated learning. In this field, Data-Free Knowledge Transfer (DFKT) plays a crucial role in addressing catastrophic forgetting and data privacy problems. However, prior approaches lack the crucial synergy between DFKT and the model training phases, causing DFKT to encounter difficulties in generating high-quality data from a non-anchored latent space of the old task model. In this paper, we introduce LANDER (Label Text Centered Data-Free Knowledge Transfer) to address this issue by utilizing label text embeddings (LTE) produced by pretrained language models. Specifically, during the model training phase, our approach treats LTE as anchor points and constrains the feature embeddings of corresponding training samples around them, enriching the surrounding area with more meaningful informati",
    "path": "papers/24/03/2403.14101.json",
    "total_tokens": 901,
    "translated_title": "文本增强的面向联邦类增量学习的无数据方法",
    "translated_abstract": "联邦类增量学习（FCIL）是一个尚未充分探讨但至关重要的问题，涉及在联邦学习环境中动态添加新类别。在这一领域，无数据知识迁移（DFKT）在解决灾难性遗忘和数据隐私问题方面发挥着关键作用。然而，以往的方法缺乏DFKT与模型训练阶段之间的关键协同作用，导致DFKT在生成旧任务模型的非锚定潜在空间中的高质量数据时遇到困难。在本文中，我们介绍了LANDER（标签文本中心化无数据知识迁移），通过利用预训练语言模型产生的标签文本嵌入（LTE）来解决这一问题。具体而言，在模型训练阶段，我们的方法将LTE视为锚点，并约束相应训练样本的特征嵌入围绕其周围，以更丰富的信息丰富周围区域",
    "tldr": "通过使用预训练语言模型产生的标签文本嵌入，本文提出了一种称为LANDER的方法，即面向联邦类增量学习的无数据方法，以解决DFKT在生成高质量数据时遇到的困难。",
    "en_tdlr": "This paper introduces a method called LANDER for Federated Class-Incremental Learning that utilizes label text embeddings generated by pretrained language models to address the difficulties encountered by DFKT in generating high-quality data."
}