{
    "title": "Is Context Helpful for Chat Translation Evaluation?",
    "abstract": "arXiv:2403.08314v1 Announce Type: new  Abstract: Despite the recent success of automatic metrics for assessing translation quality, their application in evaluating the quality of machine-translated chats has been limited. Unlike more structured texts like news, chat conversations are often unstructured, short, and heavily reliant on contextual information. This poses questions about the reliability of existing sentence-level metrics in this domain as well as the role of context in assessing the translation quality. Motivated by this, we conduct a meta-evaluation of existing sentence-level automatic metrics, primarily designed for structured domains such as news, to assess the quality of machine-translated chats. We find that reference-free metrics lag behind reference-based ones, especially when evaluating translation quality in out-of-English settings. We then investigate how incorporating conversational contextual information in these metrics affects their performance. Our findings s",
    "link": "https://arxiv.org/abs/2403.08314",
    "context": "Title: Is Context Helpful for Chat Translation Evaluation?\nAbstract: arXiv:2403.08314v1 Announce Type: new  Abstract: Despite the recent success of automatic metrics for assessing translation quality, their application in evaluating the quality of machine-translated chats has been limited. Unlike more structured texts like news, chat conversations are often unstructured, short, and heavily reliant on contextual information. This poses questions about the reliability of existing sentence-level metrics in this domain as well as the role of context in assessing the translation quality. Motivated by this, we conduct a meta-evaluation of existing sentence-level automatic metrics, primarily designed for structured domains such as news, to assess the quality of machine-translated chats. We find that reference-free metrics lag behind reference-based ones, especially when evaluating translation quality in out-of-English settings. We then investigate how incorporating conversational contextual information in these metrics affects their performance. Our findings s",
    "path": "papers/24/03/2403.08314.json",
    "total_tokens": 920,
    "translated_title": "聊天翻译评估是否受上下文帮助？",
    "translated_abstract": "虽然自动评估翻译质量的度量标准取得了近期成功，但它们在评估机器翻译聊天质量方面的应用却受到了限制。与新闻等更为结构化的文本不同，聊天对话通常是非结构化的、短小并且严重依赖于上下文信息。这给现有句子级度量标准在该领域的可靠性以及上下文在评估翻译质量中的作用提出了问题。受此启发，我们对现有主要用于新闻等结构化领域的句子级自动度量标准进行了元评估，以评估机器翻译聊天的质量。我们发现，在评估非英语环境下的翻译质量时，无参考度量标准落后于有参考度量标准，然后我们调查了如何将会话上下文信息结合到这些度量标准中，以影响它们的性能。",
    "tldr": "自动度量标准在评估机器翻译聊天质量方面的应用受到限制，无参考度量标准在评估翻译质量时落后于有参考度量标准，尤其在非英语环境下评估时。研究发现将会话上下文信息结合到度量标准中可以改善其性能。",
    "en_tdlr": "The application of automatic metrics for evaluating the quality of machine-translated chats is limited, reference-free metrics lag behind reference-based ones, especially in evaluating translation quality in out-of-English settings. The study found that incorporating conversational contextual information into metrics can improve their performance."
}