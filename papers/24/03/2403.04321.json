{
    "title": "Discriminative Probing and Tuning for Text-to-Image Generation",
    "abstract": "arXiv:2403.04321v1 Announce Type: cross  Abstract: Despite advancements in text-to-image generation (T2I), prior methods often face text-image misalignment problems such as relation confusion in generated images. Existing solutions involve cross-attention manipulation for better compositional understanding or integrating large language models for improved layout planning. However, the inherent alignment capabilities of T2I models are still inadequate. By reviewing the link between generative and discriminative modeling, we posit that T2I models' discriminative abilities may reflect their text-image alignment proficiency during generation. In this light, we advocate bolstering the discriminative abilities of T2I models to achieve more precise text-to-image alignment for generation. We present a discriminative adapter built on T2I models to probe their discriminative abilities on two representative tasks and leverage discriminative fine-tuning to improve their text-image alignment. As a ",
    "link": "https://arxiv.org/abs/2403.04321",
    "context": "Title: Discriminative Probing and Tuning for Text-to-Image Generation\nAbstract: arXiv:2403.04321v1 Announce Type: cross  Abstract: Despite advancements in text-to-image generation (T2I), prior methods often face text-image misalignment problems such as relation confusion in generated images. Existing solutions involve cross-attention manipulation for better compositional understanding or integrating large language models for improved layout planning. However, the inherent alignment capabilities of T2I models are still inadequate. By reviewing the link between generative and discriminative modeling, we posit that T2I models' discriminative abilities may reflect their text-image alignment proficiency during generation. In this light, we advocate bolstering the discriminative abilities of T2I models to achieve more precise text-to-image alignment for generation. We present a discriminative adapter built on T2I models to probe their discriminative abilities on two representative tasks and leverage discriminative fine-tuning to improve their text-image alignment. As a ",
    "path": "papers/24/03/2403.04321.json",
    "total_tokens": 808,
    "translated_title": "磨具探测和调整用于文本到图像生成",
    "translated_abstract": "尽管文本到图像生成（T2I）取得了进展，但先前的方法经常面临文本图像不对齐等问题，如生成图像中的关系混淆。现有解决方案包括交叉注意力操作以实现更好的组合理解，或者集成大型语言模型以改进布局规划。然而，T2I模型的固有对齐能力仍然不足。通过审视生成模型和判别模型之间的联系，我们认为T2I模型的判别能力可能反映了它们在生成过程中的文本图像对齐熟练度。基于这一观点，我们主张加强T2I模型的判别能力，以实现更精确的文本到图像对齐生成。我们提出了一个建立在T2I模型上的判别适配器，以探测它们在两项代表性任务上的判别能力，并利用判别微调来改善它们的文本图像对齐。",
    "tldr": "加强T2I模型的判别能力，以实现更精确的文本到图像对齐生成。",
    "en_tdlr": "Reinforcing the discriminative abilities of T2I models to achieve more precise text-to-image alignment for generation."
}