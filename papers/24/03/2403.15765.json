{
    "title": "Towards Human-Like Machine Comprehension: Few-Shot Relational Learning in Visually-Rich Documents",
    "abstract": "arXiv:2403.15765v1 Announce Type: cross  Abstract: Key-value relations are prevalent in Visually-Rich Documents (VRDs), often depicted in distinct spatial regions accompanied by specific color and font styles. These non-textual cues serve as important indicators that greatly enhance human comprehension and acquisition of such relation triplets. However, current document AI approaches often fail to consider this valuable prior information related to visual and spatial features, resulting in suboptimal performance, particularly when dealing with limited examples. To address this limitation, our research focuses on few-shot relational learning, specifically targeting the extraction of key-value relation triplets in VRDs. Given the absence of a suitable dataset for this task, we introduce two new few-shot benchmarks built upon existing supervised benchmark datasets. Furthermore, we propose a variational approach that incorporates relational 2D-spatial priors and prototypical rectification ",
    "link": "https://arxiv.org/abs/2403.15765",
    "context": "Title: Towards Human-Like Machine Comprehension: Few-Shot Relational Learning in Visually-Rich Documents\nAbstract: arXiv:2403.15765v1 Announce Type: cross  Abstract: Key-value relations are prevalent in Visually-Rich Documents (VRDs), often depicted in distinct spatial regions accompanied by specific color and font styles. These non-textual cues serve as important indicators that greatly enhance human comprehension and acquisition of such relation triplets. However, current document AI approaches often fail to consider this valuable prior information related to visual and spatial features, resulting in suboptimal performance, particularly when dealing with limited examples. To address this limitation, our research focuses on few-shot relational learning, specifically targeting the extraction of key-value relation triplets in VRDs. Given the absence of a suitable dataset for this task, we introduce two new few-shot benchmarks built upon existing supervised benchmark datasets. Furthermore, we propose a variational approach that incorporates relational 2D-spatial priors and prototypical rectification ",
    "path": "papers/24/03/2403.15765.json",
    "total_tokens": 901,
    "translated_title": "朝向类人机理解的方向：在视觉丰富文档中进行少样本关系学习",
    "translated_abstract": "关键-值关系在视觉丰富文档（VRDs）中普遍存在，通常在不同的空间区域中呈现，伴随特定的颜色和字体风格。这些非文本线索作为重要指示器，极大增强了人类对这种关系三元组的理解和获取。然而，当前的文档AI方法往往未考虑与视觉和空间特征相关的这些有价值的先验信息，导致性能不佳，特别是在处理有限示例时。为了解决这一限制，我们的研究聚焦于少样本关系学习，具体针对在VRDs中提取关键-值关系三元组。鉴于缺乏适用于这一任务的数据集，我们引入了基于现有监督基准数据集构建的两个新的少样本基准。此外，我们提出了一种包含关系二维空间先验和样本矫正的变分方法",
    "tldr": "该研究聚焦于在视觉丰富文档中进行少样本关系学习，引入了基于现有监督基准数据集构建的两个新的少样本基准，提出了一种包含关系二维空间先验和样本矫正的变分方法",
    "en_tdlr": "This research focuses on few-shot relational learning in visually-rich documents, introducing two new few-shot benchmarks built upon existing supervised benchmark datasets, and proposing a variational approach that incorporates relational 2D-spatial priors and prototypical rectification."
}