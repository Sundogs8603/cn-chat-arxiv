{
    "title": "Robust optimization for adversarial learning with finite sample complexity guarantees",
    "abstract": "arXiv:2403.15207v1 Announce Type: new  Abstract: Decision making and learning in the presence of uncertainty has attracted significant attention in view of the increasing need to achieve robust and reliable operations. In the case where uncertainty stems from the presence of adversarial attacks this need is becoming more prominent. In this paper we focus on linear and nonlinear classification problems and propose a novel adversarial training method for robust classifiers, inspired by Support Vector Machine (SVM) margins. We view robustness under a data driven lens, and derive finite sample complexity bounds for both linear and non-linear classifiers in binary and multi-class scenarios. Notably, our bounds match natural classifiers' complexity. Our algorithm minimizes a worst-case surrogate loss using Linear Programming (LP) and Second Order Cone Programming (SOCP) for linear and non-linear models. Numerical experiments on the benchmark MNIST and CIFAR10 datasets show our approach's com",
    "link": "https://arxiv.org/abs/2403.15207",
    "context": "Title: Robust optimization for adversarial learning with finite sample complexity guarantees\nAbstract: arXiv:2403.15207v1 Announce Type: new  Abstract: Decision making and learning in the presence of uncertainty has attracted significant attention in view of the increasing need to achieve robust and reliable operations. In the case where uncertainty stems from the presence of adversarial attacks this need is becoming more prominent. In this paper we focus on linear and nonlinear classification problems and propose a novel adversarial training method for robust classifiers, inspired by Support Vector Machine (SVM) margins. We view robustness under a data driven lens, and derive finite sample complexity bounds for both linear and non-linear classifiers in binary and multi-class scenarios. Notably, our bounds match natural classifiers' complexity. Our algorithm minimizes a worst-case surrogate loss using Linear Programming (LP) and Second Order Cone Programming (SOCP) for linear and non-linear models. Numerical experiments on the benchmark MNIST and CIFAR10 datasets show our approach's com",
    "path": "papers/24/03/2403.15207.json",
    "total_tokens": 863,
    "translated_title": "对抗学习的鲁棒优化与有限样本复杂性保证",
    "translated_abstract": "在存在不确定性的决策和学习过程中，越来越需要实现鲁棒和可靠的操作。当不确定性来自对抗性攻击时，这种需求变得更加突出。本文聚焦于线性和非线性分类问题，提出了一种新颖的对抗训练方法，用于强大的分类器，灵感来自支持向量机（SVM）边界。我们通过数据驱动的角度看待鲁棒性，并针对二元和多类场景推导出了线性和非线性分类器的有限样本复杂性界限。值得注意的是，我们的界限与自然分类器的复杂性相匹配。我们的算法使用线性规划（LP）和二阶锥规划（SOCP）最小化最坏情况的替代损失，适用于线性和非线性模型。在基准MNIST和CIFAR10数据集上的数值实验展示了我们方法的性能。",
    "tldr": "本文提出了一种针对对抗学习的鲁棒分类器的新型训练方法，通过最小化最坏情况下的替代损失来实现，同时在线性和非线性模型上推导出了有限样本复杂性界限。"
}