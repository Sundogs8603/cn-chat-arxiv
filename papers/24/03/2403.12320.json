{
    "title": "Approximated Likelihood Ratio: A Forward-Only and Parallel Framework for Boosting Neural Network Training",
    "abstract": "arXiv:2403.12320v1 Announce Type: cross  Abstract: Efficient and biologically plausible alternatives to backpropagation in neural network training remain a challenge due to issues such as high computational complexity and additional assumptions about neural networks, which limit scalability to deeper networks. The likelihood ratio method offers a promising gradient estimation strategy but is constrained by significant memory consumption, especially when deploying multiple copies of data to reduce estimation variance. In this paper, we introduce an approximation technique for the likelihood ratio (LR) method to alleviate computational and memory demands in gradient estimation. By exploiting the natural parallelism during the backward pass using LR, we further provide a high-performance training strategy, which pipelines both the forward and backward pass, to make it more suitable for the computation on specialized hardware. Extensive experiments demonstrate the effectiveness of the appr",
    "link": "https://arxiv.org/abs/2403.12320",
    "context": "Title: Approximated Likelihood Ratio: A Forward-Only and Parallel Framework for Boosting Neural Network Training\nAbstract: arXiv:2403.12320v1 Announce Type: cross  Abstract: Efficient and biologically plausible alternatives to backpropagation in neural network training remain a challenge due to issues such as high computational complexity and additional assumptions about neural networks, which limit scalability to deeper networks. The likelihood ratio method offers a promising gradient estimation strategy but is constrained by significant memory consumption, especially when deploying multiple copies of data to reduce estimation variance. In this paper, we introduce an approximation technique for the likelihood ratio (LR) method to alleviate computational and memory demands in gradient estimation. By exploiting the natural parallelism during the backward pass using LR, we further provide a high-performance training strategy, which pipelines both the forward and backward pass, to make it more suitable for the computation on specialized hardware. Extensive experiments demonstrate the effectiveness of the appr",
    "path": "papers/24/03/2403.12320.json",
    "total_tokens": 879,
    "translated_title": "近似似然比：Boosting神经网络训练的前向并行框架",
    "translated_abstract": "arXiv:2403.12320v1 发布类型: 跨领域 摘要: 在神经网络训练中，与反向传播相比，高计算复杂性和对神经网络的额外假设等问题使得高效、符合生物学的替代方案仍然是一个挑战，这些限制了对更深层次网络的可扩展性。似然比方法提供了一种有前途的梯度估计策略，但在部署多个数据副本以减少估计方差时，受到显著的内存消耗的限制。在本文中，我们引入了一种似然比（LR）方法的近似技术，以减轻梯度估计中的计算和内存需求。通过利用LR在反向传递过程中的自然并行性，我们进一步提供了一种高性能训练策略，该策略同时管道化了前向和反向传递，使其更适用于专用硬件的计算。大量实验证明了这种方法的有效性。",
    "tldr": "该论文提出了一种近似似然比方法，通过在反向传递过程中利用自然并行性，提供了一种高性能训练策略，以减轻神经网络训练中的计算和内存需求。",
    "en_tdlr": "This paper introduces an approximated likelihood ratio method that utilizes natural parallelism in the backward pass to provide a high-performance training strategy, alleviating computational and memory demands in neural network training."
}