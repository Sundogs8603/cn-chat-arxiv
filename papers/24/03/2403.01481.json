{
    "title": "Infusing Knowledge into Large Language Models with Contextual Prompts",
    "abstract": "arXiv:2403.01481v1 Announce Type: new  Abstract: Knowledge infusion is a promising method for enhancing Large Language Models for domain-specific NLP tasks rather than pre-training models over large data from scratch. These augmented LLMs typically depend on additional pre-training or knowledge prompts from an existing knowledge graph, which is impractical in many applications. In contrast, knowledge infusion directly from relevant documents is more generalisable and alleviates the need for structured knowledge graphs while also being useful for entities that are usually not found in any knowledge graph. With this motivation, we propose a simple yet generalisable approach for knowledge infusion by generating prompts from the context in the input text. Our experiments show the effectiveness of our approach which we evaluate by probing the fine-tuned LLMs.",
    "link": "https://arxiv.org/abs/2403.01481",
    "context": "Title: Infusing Knowledge into Large Language Models with Contextual Prompts\nAbstract: arXiv:2403.01481v1 Announce Type: new  Abstract: Knowledge infusion is a promising method for enhancing Large Language Models for domain-specific NLP tasks rather than pre-training models over large data from scratch. These augmented LLMs typically depend on additional pre-training or knowledge prompts from an existing knowledge graph, which is impractical in many applications. In contrast, knowledge infusion directly from relevant documents is more generalisable and alleviates the need for structured knowledge graphs while also being useful for entities that are usually not found in any knowledge graph. With this motivation, we propose a simple yet generalisable approach for knowledge infusion by generating prompts from the context in the input text. Our experiments show the effectiveness of our approach which we evaluate by probing the fine-tuned LLMs.",
    "path": "papers/24/03/2403.01481.json",
    "total_tokens": 756,
    "translated_title": "通过上下文提示向大型语言模型灌输知识",
    "translated_abstract": "知识注入是增强大型语言模型用于特定领域NLP任务的一种有效方法，而不是从头开始对模型进行大规模数据的预训练。这种增强的LLM通常依赖于来自现有知识图的额外预训练或知识提示，但在许多应用中是不可行的。相反，直接从相关文档中注入知识更具一般性，减轻了对结构化知识图的需求，同时对于通常不在任何知识图中找到的实体也很有用。鉴于这一动机，我们提出了一种简单但具有一般性的知识注入方法，通过从输入文本的上下文中生成提示。我们的实验表明了我们的方法的有效性，我们通过对微调后的LLM进行探测来评估该方法。",
    "tldr": "提出了一种通过从输入文本的上下文中生成提示来进行知识注入的简单但通用方法，能够有效增强大型语言模型的性能。",
    "en_tdlr": "Proposing a simple yet generalizable approach for infusing knowledge by generating prompts from the context in the input text, which enhances the performance of Large Language Models effectively."
}