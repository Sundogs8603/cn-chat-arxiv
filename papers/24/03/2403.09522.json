{
    "title": "MT-PATCHER: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation",
    "abstract": "arXiv:2403.09522v1 Announce Type: new  Abstract: Large Language Models (LLM) have demonstrated their strong ability in the field of machine translation (MT), yet they suffer from high computational cost and latency. Therefore, transferring translation knowledge from giant LLMs to medium-sized machine translation models is a promising research direction. However, traditional knowledge distillation methods do not take the capability of student and teacher models into consideration, therefore repeatedly teaching student models on the knowledge they have learned, and failing to extend to novel contexts and knowledge. In this paper, we propose a framework called MT-Patcher, which transfers knowledge from LLMs to existing MT models in a selective, comprehensive and proactive manner. Considering the current translation ability of student MT models, we only identify and correct their translation errors, instead of distilling the whole translation from the teacher. Leveraging the strong languag",
    "link": "https://arxiv.org/abs/2403.09522",
    "context": "Title: MT-PATCHER: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation\nAbstract: arXiv:2403.09522v1 Announce Type: new  Abstract: Large Language Models (LLM) have demonstrated their strong ability in the field of machine translation (MT), yet they suffer from high computational cost and latency. Therefore, transferring translation knowledge from giant LLMs to medium-sized machine translation models is a promising research direction. However, traditional knowledge distillation methods do not take the capability of student and teacher models into consideration, therefore repeatedly teaching student models on the knowledge they have learned, and failing to extend to novel contexts and knowledge. In this paper, we propose a framework called MT-Patcher, which transfers knowledge from LLMs to existing MT models in a selective, comprehensive and proactive manner. Considering the current translation ability of student MT models, we only identify and correct their translation errors, instead of distilling the whole translation from the teacher. Leveraging the strong languag",
    "path": "papers/24/03/2403.09522.json",
    "total_tokens": 768,
    "translated_title": "MT-PATCHER：来自大型语言模型的有选择性和可扩展的知识蒸馏用于机器翻译",
    "translated_abstract": "大型语言模型（LLM）在机器翻译（MT）领域展现出强大的能力，但它们面临着高计算成本和延迟的问题。因此，将翻译知识从巨型LLM转移到中等规模的机器翻译模型是一个有前途的研究方向。本文提出了一个名为MT-Patcher的框架，以选择性、全面和主动的方式将知识从LLMs转移到现有的MT模型中。考虑到学生MT模型当前的翻译能力，我们仅识别和纠正其翻译错误，而不是从老师那里蒸馏整个翻译。",
    "tldr": "提出了MT-Patcher框架，实现了从大型语言模型到中等规模机器翻译模型的有选择性、全面和主动的知识迁移",
    "en_tdlr": "Introduced the MT-Patcher framework, enabling selective, comprehensive, and proactive knowledge transfer from large language models to medium-sized machine translation models."
}