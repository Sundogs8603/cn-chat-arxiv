{
    "title": "Fostering the Ecosystem of Open Neural Encoders for Portuguese with Albertina PT* Family",
    "abstract": "arXiv:2403.01897v1 Announce Type: new  Abstract: To foster the neural encoding of Portuguese, this paper contributes foundation encoder models that represent an expansion of the still very scarce ecosystem of large language models specifically developed for this language that are fully open, in the sense that they are open source and openly distributed for free under an open license for any purpose, thus including research and commercial usages. Like most languages other than English, Portuguese is low-resourced in terms of these foundational language resources, there being the inaugural 900 million parameter Albertina and 335 million Bertimbau. Taking this couple of models as an inaugural set, we present the extension of the ecosystem of state-of-the-art open encoders for Portuguese with a larger, top performance-driven model with 1.5 billion parameters, and a smaller, efficiency-driven model with 100 million parameters. While achieving this primary goal, further results that are rele",
    "link": "https://arxiv.org/abs/2403.01897",
    "context": "Title: Fostering the Ecosystem of Open Neural Encoders for Portuguese with Albertina PT* Family\nAbstract: arXiv:2403.01897v1 Announce Type: new  Abstract: To foster the neural encoding of Portuguese, this paper contributes foundation encoder models that represent an expansion of the still very scarce ecosystem of large language models specifically developed for this language that are fully open, in the sense that they are open source and openly distributed for free under an open license for any purpose, thus including research and commercial usages. Like most languages other than English, Portuguese is low-resourced in terms of these foundational language resources, there being the inaugural 900 million parameter Albertina and 335 million Bertimbau. Taking this couple of models as an inaugural set, we present the extension of the ecosystem of state-of-the-art open encoders for Portuguese with a larger, top performance-driven model with 1.5 billion parameters, and a smaller, efficiency-driven model with 100 million parameters. While achieving this primary goal, further results that are rele",
    "path": "papers/24/03/2403.01897.json",
    "total_tokens": 938,
    "translated_title": "促进葡萄牙语的开放神经编码器生态系统与Albertina PT*家族",
    "translated_abstract": "为了促进葡萄牙语的神经编码，本文贡献了代表基础编码器模型，代表了一个仍然非常稀缺的针对该语言特别开发的大型语言模型生态系统的扩展，这些模型完全是开放的，即它们是开源的，并在一个开放许可下免费分发，可用于任何目的，包括研究和商业用途。与英语以外的大多数语言一样，葡萄牙语在这些基础语言资源方面资源匮乏，这里有首届拥有 9 亿个参数的 Albertina 和 3.35 亿个参数的 Bertimbau。在以这对模型为首次集合的基础上，我们介绍了最先进的开放式葡萄牙语编码器生态系统的扩展，其中包括一个拥有 15 亿参数的更大型、性能驱动的模型，以及一个拥有 1 亿参数的更小型、效率驱动的模型。在实现这一主要目标的同时，还得到了一些进一步的成果",
    "tldr": "本文为葡萄牙语的神经编码作出了贡献，扩展了大型语言模型生态系统，并发布了包括亿级参数 Albertina 和 Bertimbau 在内的开源编码器模型，进一步推进了葡萄牙语的神经编码器技术。",
    "en_tdlr": "This paper contributes to Portuguese neural encoding by expanding the ecosystem of large language models and releasing open encoder models including Albertina and Bertimbau with billions of parameters, further advancing the neural encoding technology for Portuguese."
}