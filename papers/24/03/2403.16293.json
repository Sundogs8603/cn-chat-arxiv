{
    "title": "Interpretable Modeling of Deep Reinforcement Learning Driven Scheduling",
    "abstract": "arXiv:2403.16293v1 Announce Type: new  Abstract: In the field of high-performance computing (HPC), there has been recent exploration into the use of deep reinforcement learning for cluster scheduling (DRL scheduling), which has demonstrated promising outcomes. However, a significant challenge arises from the lack of interpretability in deep neural networks (DNN), rendering them as black-box models to system managers. This lack of model interpretability hinders the practical deployment of DRL scheduling. In this work, we present a framework called IRL (Interpretable Reinforcement Learning) to address the issue of interpretability of DRL scheduling. The core idea is to interpret DNN (i.e., the DRL policy) as a decision tree by utilizing imitation learning. Unlike DNN, decision tree models are non-parametric and easily comprehensible to humans. To extract an effective and efficient decision tree, IRL incorporates the Dataset Aggregation (DAgger) algorithm and introduces the notion of crit",
    "link": "https://arxiv.org/abs/2403.16293",
    "context": "Title: Interpretable Modeling of Deep Reinforcement Learning Driven Scheduling\nAbstract: arXiv:2403.16293v1 Announce Type: new  Abstract: In the field of high-performance computing (HPC), there has been recent exploration into the use of deep reinforcement learning for cluster scheduling (DRL scheduling), which has demonstrated promising outcomes. However, a significant challenge arises from the lack of interpretability in deep neural networks (DNN), rendering them as black-box models to system managers. This lack of model interpretability hinders the practical deployment of DRL scheduling. In this work, we present a framework called IRL (Interpretable Reinforcement Learning) to address the issue of interpretability of DRL scheduling. The core idea is to interpret DNN (i.e., the DRL policy) as a decision tree by utilizing imitation learning. Unlike DNN, decision tree models are non-parametric and easily comprehensible to humans. To extract an effective and efficient decision tree, IRL incorporates the Dataset Aggregation (DAgger) algorithm and introduces the notion of crit",
    "path": "papers/24/03/2403.16293.json",
    "total_tokens": 830,
    "translated_title": "深度强化学习驱动调度可解释建模",
    "translated_abstract": "在高性能计算（HPC）领域，最近探索了使用深度强化学习进行集群调度（DRL调度），取得了良好的成果。然而，深度神经网络（DNN）缺乏可解释性，使其成为系统管理员看不懂的黑匣模型，这导致了DRL调度的实际部署困难。本文提出了一个名为IRL（可解释强化学习）的框架，以解决DRL调度的可解释性问题。其核心思想是通过利用模仿学习将DNN（即DRL策略）解释为决策树。与DNN不同，决策树模型是非参数化的，易于人类理解。为了提取一个有效且高效的决策树，IRL结合了数据集聚合（DAgger）算法，并引入了评论概念。",
    "tldr": "该研究提出了一个名为IRL的框架，通过将DNN解释为决策树，解决了DRL调度中深度神经网络缺乏可解释性的问题。",
    "en_tdlr": "This study introduces a framework called IRL, which addresses the lack of interpretability in deep neural networks in DRL scheduling by interpreting DNN as decision trees."
}