{
    "title": "Approximations to the Fisher Information Metric of Deep Generative Models for Out-Of-Distribution Detection",
    "abstract": "arXiv:2403.01485v1 Announce Type: cross  Abstract: Likelihood-based deep generative models such as score-based diffusion models and variational autoencoders are state-of-the-art machine learning models approximating high-dimensional distributions of data such as images, text, or audio. One of many downstream tasks they can be naturally applied to is out-of-distribution (OOD) detection. However, seminal work by Nalisnick et al. which we reproduce showed that deep generative models consistently infer higher log-likelihoods for OOD data than data they were trained on, marking an open problem. In this work, we analyse using the gradient of a data point with respect to the parameters of the deep generative model for OOD detection, based on the simple intuition that OOD data should have larger gradient norms than training data. We formalise measuring the size of the gradient as approximating the Fisher information metric. We show that the Fisher information matrix (FIM) has large absolute di",
    "link": "https://arxiv.org/abs/2403.01485",
    "context": "Title: Approximations to the Fisher Information Metric of Deep Generative Models for Out-Of-Distribution Detection\nAbstract: arXiv:2403.01485v1 Announce Type: cross  Abstract: Likelihood-based deep generative models such as score-based diffusion models and variational autoencoders are state-of-the-art machine learning models approximating high-dimensional distributions of data such as images, text, or audio. One of many downstream tasks they can be naturally applied to is out-of-distribution (OOD) detection. However, seminal work by Nalisnick et al. which we reproduce showed that deep generative models consistently infer higher log-likelihoods for OOD data than data they were trained on, marking an open problem. In this work, we analyse using the gradient of a data point with respect to the parameters of the deep generative model for OOD detection, based on the simple intuition that OOD data should have larger gradient norms than training data. We formalise measuring the size of the gradient as approximating the Fisher information metric. We show that the Fisher information matrix (FIM) has large absolute di",
    "path": "papers/24/03/2403.01485.json",
    "total_tokens": 927,
    "translated_title": "对深度生成模型的费舍尔信息度量进行近似用于检测离群分布",
    "translated_abstract": "基于概率似然的深度生成模型，如基于评分的扩散模型和变分自动编码器，是近年来用于拟合高维数据分布（如图像、文本或音频）的先进机器学习模型之一。它们可以自然地应用于许多下游任务之一，即离群分布（OOD）检测。然而，Nalisnick等人的开创性工作表明，深度生成模型始终为OOD数据推断出比它们训练过的数据更高的对数似然，标志着一个悬而未决的问题。在这项工作中，我们分析了使用数据点对深度生成模型的参数梯度进行OOD检测，基于这样的简单直觉，即OOD数据的梯度范数应该大于训练数据。我们形式化地将梯度大小的度量量化为近似费舍尔信息度量。我们展示了费舍尔信息矩阵（FIM）具有较大的绝对值",
    "tldr": "本文分析了一种使用数据点关于深度生成模型参数的梯度进行离群分布检测的方法，基于对OOD数据应具有更大梯度范数的简单直觉，通过近似费舍尔信息度量实现该方法",
    "en_tdlr": "This work analyzes a method of using the gradient of a data point with respect to the parameters of deep generative models for out-of-distribution detection, based on the simple intuition that out-of-distribution data should have larger gradient norms, formalizing the measurement of gradient size as approximating the Fisher information metric."
}