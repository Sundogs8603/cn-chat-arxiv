{
    "title": "ClinicalMamba: A Generative Clinical Language Model on Longitudinal Clinical Notes",
    "abstract": "arXiv:2403.05795v1 Announce Type: new  Abstract: The advancement of natural language processing (NLP) systems in healthcare hinges on language model ability to interpret the intricate information contained within clinical notes. This process often requires integrating information from various time points in a patient's medical history. However, most earlier clinical language models were pretrained with a context length limited to roughly one clinical document. In this study, We introduce ClinicalMamba, a specialized version of the Mamba language model, pretrained on a vast corpus of longitudinal clinical notes to address the unique linguistic characteristics and information processing needs of the medical domain. ClinicalMamba, with 130 million and 2.8 billion parameters, demonstrates a superior performance in modeling clinical language across extended text lengths compared to Mamba and clinical Llama. With few-shot learning, ClinicalMamba achieves notable benchmarks in speed and accur",
    "link": "https://arxiv.org/abs/2403.05795",
    "context": "Title: ClinicalMamba: A Generative Clinical Language Model on Longitudinal Clinical Notes\nAbstract: arXiv:2403.05795v1 Announce Type: new  Abstract: The advancement of natural language processing (NLP) systems in healthcare hinges on language model ability to interpret the intricate information contained within clinical notes. This process often requires integrating information from various time points in a patient's medical history. However, most earlier clinical language models were pretrained with a context length limited to roughly one clinical document. In this study, We introduce ClinicalMamba, a specialized version of the Mamba language model, pretrained on a vast corpus of longitudinal clinical notes to address the unique linguistic characteristics and information processing needs of the medical domain. ClinicalMamba, with 130 million and 2.8 billion parameters, demonstrates a superior performance in modeling clinical language across extended text lengths compared to Mamba and clinical Llama. With few-shot learning, ClinicalMamba achieves notable benchmarks in speed and accur",
    "path": "papers/24/03/2403.05795.json",
    "total_tokens": 704,
    "translated_title": "ClinicalMamba：基于纵向临床记录的生成型临床语言模型",
    "translated_abstract": "在医疗保健领域，自然语言处理（NLP）系统的进步取决于语言模型解释临床笔记中包含的复杂信息的能力。本研究引入了ClinicalMamba，这是Mamba语言模型的专门版本，它在大量纵向临床记录语料库上进行了预训练，以解决医学领域的独特语言特征和信息处理需求。",
    "tldr": "ClinicalMamba是基于纵向临床记录的生成型临床语言模型，具有130亿和28亿参数，相对于Mamba和临床Llama，在建模较长文本时表现出更优异的性能。",
    "en_tdlr": "ClinicalMamba is a generative clinical language model based on longitudinal clinical notes, with 1.3 billion and 2.8 billion parameters, demonstrating superior performance in modeling clinical language across extended text lengths compared to Mamba and clinical Llama."
}