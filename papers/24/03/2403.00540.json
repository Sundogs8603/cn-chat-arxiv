{
    "title": "Epsilon-Greedy Thompson Sampling to Bayesian Optimization",
    "abstract": "arXiv:2403.00540v1 Announce Type: new  Abstract: Thompson sampling (TS) serves as a solution for addressing the exploitation-exploration dilemma in Bayesian optimization (BO). While it prioritizes exploration by randomly generating and maximizing sample paths of Gaussian process (GP) posteriors, TS weakly manages its exploitation by gathering information about the true objective function after each exploration is performed. In this study, we incorporate the epsilon-greedy ($\\varepsilon$-greedy) policy, a well-established selection strategy in reinforcement learning, into TS to improve its exploitation. We first delineate two extremes of TS applied for BO, namely the generic TS and a sample-average TS. The former and latter promote exploration and exploitation, respectively. We then use $\\varepsilon$-greedy policy to randomly switch between the two extremes. A small value of $\\varepsilon \\in (0,1)$ prioritizes exploitation, and vice versa. We empirically show that $\\varepsilon$-greedy T",
    "link": "https://arxiv.org/abs/2403.00540",
    "context": "Title: Epsilon-Greedy Thompson Sampling to Bayesian Optimization\nAbstract: arXiv:2403.00540v1 Announce Type: new  Abstract: Thompson sampling (TS) serves as a solution for addressing the exploitation-exploration dilemma in Bayesian optimization (BO). While it prioritizes exploration by randomly generating and maximizing sample paths of Gaussian process (GP) posteriors, TS weakly manages its exploitation by gathering information about the true objective function after each exploration is performed. In this study, we incorporate the epsilon-greedy ($\\varepsilon$-greedy) policy, a well-established selection strategy in reinforcement learning, into TS to improve its exploitation. We first delineate two extremes of TS applied for BO, namely the generic TS and a sample-average TS. The former and latter promote exploration and exploitation, respectively. We then use $\\varepsilon$-greedy policy to randomly switch between the two extremes. A small value of $\\varepsilon \\in (0,1)$ prioritizes exploitation, and vice versa. We empirically show that $\\varepsilon$-greedy T",
    "path": "papers/24/03/2403.00540.json",
    "total_tokens": 842,
    "translated_title": "Epsilon-Greedy Thompson Sampling用于贝叶斯优化",
    "translated_abstract": "Thompson采样（TS）被认为是解决贝叶斯优化中开发-探索困境的解决方案。 虽然它通过随机生成和最大化高斯过程（GP）后验的样本路径来优先进行探索，但TS在每次执行探索后通过收集关于真实目标函数的信息来弱化其开发功能。 本研究将在TS中引入$\\varepsilon$-greedy策略，这是一种在强化学习中被广泛应用的选择策略，以改进其开发功能。 我们首先描述了TS应用于BO的两个极端，即通用TS和样本平均TS。前者和后者分别提倡探索和开发。 然后我们使用$\\varepsilon$-greedy策略在两个极端之间随机切换。 $\\varepsilon \\in (0,1)$的小值优先考虑开发，反之亦然。 我们实证表明$\\varepsilon$-greedy T",
    "tldr": "将$\\varepsilon$-greedy策略引入Thompson采样以改进贝叶斯优化中的开发功能，并实证表明其有效性。",
    "en_tdlr": "Incorporating the $\\varepsilon$-greedy policy into Thompson Sampling improves exploitation in Bayesian optimization and is empirically shown to be effective."
}