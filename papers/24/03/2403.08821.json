{
    "title": "Effective Gradient Sample Size via Variation Estimation for Accelerating Sharpness aware Minimization",
    "abstract": "arXiv:2403.08821v1 Announce Type: cross  Abstract: Sharpness-aware Minimization (SAM) has been proposed recently to improve model generalization ability. However, SAM calculates the gradient twice in each optimization step, thereby doubling the computation costs compared to stochastic gradient descent (SGD). In this paper, we propose a simple yet efficient sampling method to significantly accelerate SAM. Concretely, we discover that the gradient of SAM is a combination of the gradient of SGD and the Projection of the Second-order gradient matrix onto the First-order gradient (PSF). PSF exhibits a gradually increasing frequency of change during the training process. To leverage this observation, we propose an adaptive sampling method based on the variation of PSF, and we reuse the sampled PSF for non-sampling iterations. Extensive empirical results illustrate that the proposed method achieved state-of-the-art accuracies comparable to SAM on diverse network architectures.",
    "link": "https://arxiv.org/abs/2403.08821",
    "context": "Title: Effective Gradient Sample Size via Variation Estimation for Accelerating Sharpness aware Minimization\nAbstract: arXiv:2403.08821v1 Announce Type: cross  Abstract: Sharpness-aware Minimization (SAM) has been proposed recently to improve model generalization ability. However, SAM calculates the gradient twice in each optimization step, thereby doubling the computation costs compared to stochastic gradient descent (SGD). In this paper, we propose a simple yet efficient sampling method to significantly accelerate SAM. Concretely, we discover that the gradient of SAM is a combination of the gradient of SGD and the Projection of the Second-order gradient matrix onto the First-order gradient (PSF). PSF exhibits a gradually increasing frequency of change during the training process. To leverage this observation, we propose an adaptive sampling method based on the variation of PSF, and we reuse the sampled PSF for non-sampling iterations. Extensive empirical results illustrate that the proposed method achieved state-of-the-art accuracies comparable to SAM on diverse network architectures.",
    "path": "papers/24/03/2403.08821.json",
    "total_tokens": 849,
    "translated_title": "通过变化估计实现有效的梯度样本大小，加速敏锐度感知最小化",
    "translated_abstract": "近期提出了“敏锐度感知最小化”（SAM）以改善模型的泛化能力。然而，SAM在每次优化步骤中都会计算梯度两次，从而使计算成本比随机梯度下降（SGD）增加一倍。本文提出了一种简单而高效的抽样方法，显著加速SAM。具体而言，我们发现SAM的梯度是SGD梯度和第一阶梯度上的二阶梯度矩阵投影（PSF）的组合。PSF在训练过程中表现出逐渐增加的变化频率。为了利用这一观察结果，我们基于PSF的变化提出了一种自适应抽样方法，并将已采样的PSF重用于非抽样迭代。大量实证结果表明，所提出的方法在各种网络架构上达到了与SAM相当的最先进准确性。",
    "tldr": "本文提出了一种简单而高效的抽样方法，通过基于PSF的变化来显著加速Sharpness-aware Minimization（SAM），实现了与SAM相当的最先进准确性。",
    "en_tdlr": "This paper proposes a simple and efficient sampling method to significantly accelerate Sharpness-aware Minimization (SAM) by leveraging the variation of PSF, achieving state-of-the-art accuracies comparable to SAM."
}