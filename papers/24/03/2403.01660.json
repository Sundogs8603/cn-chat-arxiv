{
    "title": "Geometry and Stability of Supervised Learning Problems",
    "abstract": "arXiv:2403.01660v1 Announce Type: new  Abstract: We introduce a notion of distance between supervised learning problems, which we call the Risk distance. This optimal-transport-inspired distance facilitates stability results; one can quantify how seriously issues like sampling bias, noise, limited data, and approximations might change a given problem by bounding how much these modifications can move the problem under the Risk distance. With the distance established, we explore the geometry of the resulting space of supervised learning problems, providing explicit geodesics and proving that the set of classification problems is dense in a larger class of problems. We also provide two variants of the Risk distance: one that incorporates specified weights on a problem's predictors, and one that is more sensitive to the contours of a problem's risk landscape.",
    "link": "https://arxiv.org/abs/2403.01660",
    "context": "Title: Geometry and Stability of Supervised Learning Problems\nAbstract: arXiv:2403.01660v1 Announce Type: new  Abstract: We introduce a notion of distance between supervised learning problems, which we call the Risk distance. This optimal-transport-inspired distance facilitates stability results; one can quantify how seriously issues like sampling bias, noise, limited data, and approximations might change a given problem by bounding how much these modifications can move the problem under the Risk distance. With the distance established, we explore the geometry of the resulting space of supervised learning problems, providing explicit geodesics and proving that the set of classification problems is dense in a larger class of problems. We also provide two variants of the Risk distance: one that incorporates specified weights on a problem's predictors, and one that is more sensitive to the contours of a problem's risk landscape.",
    "path": "papers/24/03/2403.01660.json",
    "total_tokens": 764,
    "translated_title": "监督学习问题的几何和稳定性",
    "translated_abstract": "我们引入了一种监督学习问题之间的距离概念，我们称之为风险距离。这种以最优传输为灵感的距离促进了稳定性结果；我们可以通过限制这些修改可以将问题移动多少来量化诸如采样偏差、噪声、有限数据和逼近等问题在风险距离下如何改变给定问题。在建立了距离之后，我们探索了产生的监督学习问题空间的几何结构，提供了明确的测地线并证明分类问题集在更大类的问题中是密集的。我们还提供了风险距离的两个变体：一个在问题的预测变量上结合了指定的权重，另一个对问题的风险景观轮廓更为敏感。",
    "tldr": "引入了监督学习问题之间的风险距离概念，通过风险距离可以量化问题的稳定性变化，并探索了监督学习问题空间的几何结构。",
    "en_tdlr": "Introduced the Risk distance concept between supervised learning problems to quantify stability changes and explored the geometry of the space of supervised learning problems."
}