{
    "title": "Probabilistic Model Checking of Stochastic Reinforcement Learning Policies",
    "abstract": "arXiv:2403.18725v1 Announce Type: new  Abstract: We introduce a method to verify stochastic reinforcement learning (RL) policies. This approach is compatible with any RL algorithm as long as the algorithm and its corresponding environment collectively adhere to the Markov property. In this setting, the future state of the environment should depend solely on its current state and the action executed, independent of any previous states or actions. Our method integrates a verification technique, referred to as model checking, with RL, leveraging a Markov decision process, a trained RL policy, and a probabilistic computation tree logic (PCTL) formula to build a formal model that can be subsequently verified via the model checker Storm. We demonstrate our method's applicability across multiple benchmarks, comparing it to baseline methods called deterministic safety estimates and naive monolithic model checking. Our results show that our method is suited to verify stochastic RL policies.",
    "link": "https://arxiv.org/abs/2403.18725",
    "context": "Title: Probabilistic Model Checking of Stochastic Reinforcement Learning Policies\nAbstract: arXiv:2403.18725v1 Announce Type: new  Abstract: We introduce a method to verify stochastic reinforcement learning (RL) policies. This approach is compatible with any RL algorithm as long as the algorithm and its corresponding environment collectively adhere to the Markov property. In this setting, the future state of the environment should depend solely on its current state and the action executed, independent of any previous states or actions. Our method integrates a verification technique, referred to as model checking, with RL, leveraging a Markov decision process, a trained RL policy, and a probabilistic computation tree logic (PCTL) formula to build a formal model that can be subsequently verified via the model checker Storm. We demonstrate our method's applicability across multiple benchmarks, comparing it to baseline methods called deterministic safety estimates and naive monolithic model checking. Our results show that our method is suited to verify stochastic RL policies.",
    "path": "papers/24/03/2403.18725.json",
    "total_tokens": 820,
    "translated_title": "随机强化学习策略的概率模型检验",
    "translated_abstract": "我们引入了一种验证随机强化学习（RL）策略的方法。该方法与任何RL算法兼容，只要算法及其对应的环境共同遵守马尔可夫性质。在这种情况下，环境的未来状态应仅取决于其当前状态和执行的动作，而与任何先前的状态或动作无关。我们的方法将一种称为模型检验的验证技术与RL相结合，利用马尔可夫决策过程、训练后的RL策略和概率计算树逻辑（PCTL）公式构建一个形式模型，随后可以通过模型检验器Storm进行验证。我们展示了我们的方法在多个基准测试中的适用性，并将其与称为确定性安全估计和天真的整体模型检验的基线方法进行比较。我们的结果表明我们的方法适用于验证随机RL策略。",
    "tldr": "该方法介绍了一种验证随机强化学习策略的方法，通过将模型检验技术与RL相结合，建立了能够通过模型检验器验证的形式模型。"
}