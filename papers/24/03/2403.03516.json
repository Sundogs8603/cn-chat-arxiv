{
    "title": "Unsupervised Multilingual Dense Retrieval via Generative Pseudo Labeling",
    "abstract": "arXiv:2403.03516v1 Announce Type: new  Abstract: Dense retrieval methods have demonstrated promising performance in multilingual information retrieval, where queries and documents can be in different languages. However, dense retrievers typically require a substantial amount of paired data, which poses even greater challenges in multilingual scenarios. This paper introduces UMR, an Unsupervised Multilingual dense Retriever trained without any paired data. Our approach leverages the sequence likelihood estimation capabilities of multilingual language models to acquire pseudo labels for training dense retrievers. We propose a two-stage framework which iteratively improves the performance of multilingual dense retrievers. Experimental results on two benchmark datasets show that UMR outperforms supervised baselines, showcasing the potential of training multilingual retrievers without paired data, thereby enhancing their practicality. Our source code, data, and models are publicly available",
    "link": "https://arxiv.org/abs/2403.03516",
    "context": "Title: Unsupervised Multilingual Dense Retrieval via Generative Pseudo Labeling\nAbstract: arXiv:2403.03516v1 Announce Type: new  Abstract: Dense retrieval methods have demonstrated promising performance in multilingual information retrieval, where queries and documents can be in different languages. However, dense retrievers typically require a substantial amount of paired data, which poses even greater challenges in multilingual scenarios. This paper introduces UMR, an Unsupervised Multilingual dense Retriever trained without any paired data. Our approach leverages the sequence likelihood estimation capabilities of multilingual language models to acquire pseudo labels for training dense retrievers. We propose a two-stage framework which iteratively improves the performance of multilingual dense retrievers. Experimental results on two benchmark datasets show that UMR outperforms supervised baselines, showcasing the potential of training multilingual retrievers without paired data, thereby enhancing their practicality. Our source code, data, and models are publicly available",
    "path": "papers/24/03/2403.03516.json",
    "total_tokens": 772,
    "translated_title": "通过生成伪标签实现的无监督多语言稠密检索",
    "translated_abstract": "稠密检索方法在多语言信息检索中表现出色，但通常需要大量配对数据，这在多语言场景下更具挑战性。本文介绍了UMR，一种无需任何配对数据训练的无监督多语言稠密检索器。我们的方法利用多语言语言模型的序列似然估计能力来获取用于训练稠密检索器的伪标签。我们提出了一个两阶段框架，通过迭代改善多语言稠密检索器的性能。对两个基准数据集的实验结果表明，UMR的性能优于监督基线，展示了无需配对数据训练多语言检索器的潜力，从而提高了其实用性。我们的源代码、数据和模型已公开可用。",
    "tldr": "通过生成伪标签实现的无监督多语言稠密检索方法能够在多语言信息检索中取得优异性能，提高了多语言检索器的实用性",
    "en_tdlr": "Unsupervised multilingual dense retrieval method using generative pseudo labeling demonstrates superior performance in multilingual information retrieval, enhancing the practicality of multilingual retrievers."
}