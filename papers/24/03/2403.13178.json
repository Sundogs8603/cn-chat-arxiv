{
    "title": "Fast Value Tracking for Deep Reinforcement Learning",
    "abstract": "arXiv:2403.13178v1 Announce Type: cross  Abstract: Reinforcement learning (RL) tackles sequential decision-making problems by creating agents that interacts with their environment. However, existing algorithms often view these problem as static, focusing on point estimates for model parameters to maximize expected rewards, neglecting the stochastic dynamics of agent-environment interactions and the critical role of uncertainty quantification. Our research leverages the Kalman filtering paradigm to introduce a novel and scalable sampling algorithm called Langevinized Kalman Temporal-Difference (LKTD) for deep reinforcement learning. This algorithm, grounded in Stochastic Gradient Markov Chain Monte Carlo (SGMCMC), efficiently draws samples from the posterior distribution of deep neural network parameters. Under mild conditions, we prove that the posterior samples generated by the LKTD algorithm converge to a stationary distribution. This convergence not only enables us to quantify uncer",
    "link": "https://arxiv.org/abs/2403.13178",
    "context": "Title: Fast Value Tracking for Deep Reinforcement Learning\nAbstract: arXiv:2403.13178v1 Announce Type: cross  Abstract: Reinforcement learning (RL) tackles sequential decision-making problems by creating agents that interacts with their environment. However, existing algorithms often view these problem as static, focusing on point estimates for model parameters to maximize expected rewards, neglecting the stochastic dynamics of agent-environment interactions and the critical role of uncertainty quantification. Our research leverages the Kalman filtering paradigm to introduce a novel and scalable sampling algorithm called Langevinized Kalman Temporal-Difference (LKTD) for deep reinforcement learning. This algorithm, grounded in Stochastic Gradient Markov Chain Monte Carlo (SGMCMC), efficiently draws samples from the posterior distribution of deep neural network parameters. Under mild conditions, we prove that the posterior samples generated by the LKTD algorithm converge to a stationary distribution. This convergence not only enables us to quantify uncer",
    "path": "papers/24/03/2403.13178.json",
    "total_tokens": 881,
    "translated_title": "深度强化学习的快速价值跟踪",
    "translated_abstract": "强化学习（RL）通过创建与环境互动的Agent来解决顺序决策问题。然而，现有算法通常将这些问题视为静态问题，专注于模型参数的点估计以最大化预期奖励，忽视了Agent-Environment互动的随机动力学和不确定性量化的关键作用。我们的研究利用卡尔曼滤波范式引入了一种新颖且可扩展的采样算法，称为Langevinized Kalman Temporal-Difference（LKTD）用于深度强化学习。这种算法基于随机梯度马尔科夫链蒙特卡罗（SGMCMC），能够有效地从深度神经网络参数的后验分布中抽取样本。在温和条件下，我们证明了LKTD算法生成的后验样本会收敛到一个稳定分布。这种收敛不仅使我们能够量化不确定性，",
    "tldr": "我们的研究提出一种基于Kalman滤波范式的新颖和可扩展的采样算法LKTD，用于深度强化学习，能够有效地从深度神经网络参数的后验分布中抽取样本，并证明这些后验样本会收敛到一个稳定分布。"
}