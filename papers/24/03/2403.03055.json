{
    "title": "Distributed Policy Gradient for Linear Quadratic Networked Control with Limited Communication Range",
    "abstract": "arXiv:2403.03055v1 Announce Type: cross  Abstract: This paper proposes a scalable distributed policy gradient method and proves its convergence to near-optimal solution in multi-agent linear quadratic networked systems. The agents engage within a specified network under local communication constraints, implying that each agent can only exchange information with a limited number of neighboring agents. On the underlying graph of the network, each agent implements its control input depending on its nearby neighbors' states in the linear quadratic control setting. We show that it is possible to approximate the exact gradient only using local information. Compared with the centralized optimal controller, the performance gap decreases to zero exponentially as the communication and control ranges increase. We also demonstrate how increasing the communication range enhances system stability in the gradient descent process, thereby elucidating a critical trade-off. The simulation results verify",
    "link": "https://arxiv.org/abs/2403.03055",
    "context": "Title: Distributed Policy Gradient for Linear Quadratic Networked Control with Limited Communication Range\nAbstract: arXiv:2403.03055v1 Announce Type: cross  Abstract: This paper proposes a scalable distributed policy gradient method and proves its convergence to near-optimal solution in multi-agent linear quadratic networked systems. The agents engage within a specified network under local communication constraints, implying that each agent can only exchange information with a limited number of neighboring agents. On the underlying graph of the network, each agent implements its control input depending on its nearby neighbors' states in the linear quadratic control setting. We show that it is possible to approximate the exact gradient only using local information. Compared with the centralized optimal controller, the performance gap decreases to zero exponentially as the communication and control ranges increase. We also demonstrate how increasing the communication range enhances system stability in the gradient descent process, thereby elucidating a critical trade-off. The simulation results verify",
    "path": "papers/24/03/2403.03055.json",
    "total_tokens": 902,
    "translated_title": "具有有限通信范围的线性二次网络控制的分布式策略梯度",
    "translated_abstract": "本文提出了一种可扩展的分布式策略梯度方法，并证明其在多智能体线性二次网络系统中收敛到近似最优解。智能体在特定网络中进行交互，受限于本地通信约束，意味着每个智能体只能与有限数量的相邻智能体交换信息。在网络的底层图中，每个智能体在线性二次控制设置中根据其附近邻居的状态实施其控制输入。我们展示了只使用局部信息即可近似精确梯度是可能的。与集中式最优控制器相比，随着通信和控制范围的增加，性能差距指数级减小为零。我们还展示了如何增加通信范围可以增强梯度下降过程中系统稳定性，从而阐明了一个关键的权衡。模拟结果验证了",
    "tldr": "提出了一种在多智能体线性二次网络系统中收敛到近似最优解的可扩展分布式策略梯度方法，证明了随着通信和控制范围的增加，性能差距会指数级减小为零，并展示了增加通信范围如何增强系统稳定性和揭示关键权衡。",
    "en_tdlr": "Proposed a scalable distributed policy gradient method that converges to near-optimal solution in multi-agent linear quadratic networked systems, showing that the performance gap decreases exponentially to zero as communication and control ranges increase, and demonstrating how increasing communication range enhances system stability and reveals a critical trade-off."
}