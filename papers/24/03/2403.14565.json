{
    "title": "A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students' Formative Assessment Responses in Science",
    "abstract": "arXiv:2403.14565v1 Announce Type: new  Abstract: This paper explores the use of large language models (LLMs) to score and explain short-answer assessments in K-12 science. While existing methods can score more structured math and computer science assessments, they often do not provide explanations for the scores. Our study focuses on employing GPT-4 for automated assessment in middle school Earth Science, combining few-shot and active learning with chain-of-thought reasoning. Using a human-in-the-loop approach, we successfully score and provide meaningful explanations for formative assessment responses. A systematic analysis of our method's pros and cons sheds light on the potential for human-in-the-loop techniques to enhance automated grading for open-ended science assessments.",
    "link": "https://arxiv.org/abs/2403.14565",
    "context": "Title: A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students' Formative Assessment Responses in Science\nAbstract: arXiv:2403.14565v1 Announce Type: new  Abstract: This paper explores the use of large language models (LLMs) to score and explain short-answer assessments in K-12 science. While existing methods can score more structured math and computer science assessments, they often do not provide explanations for the scores. Our study focuses on employing GPT-4 for automated assessment in middle school Earth Science, combining few-shot and active learning with chain-of-thought reasoning. Using a human-in-the-loop approach, we successfully score and provide meaningful explanations for formative assessment responses. A systematic analysis of our method's pros and cons sheds light on the potential for human-in-the-loop techniques to enhance automated grading for open-ended science assessments.",
    "path": "papers/24/03/2403.14565.json",
    "total_tokens": 784,
    "translated_title": "基于LLMs的思维链提示方法用于评估科学中学生形成性评估回答",
    "translated_abstract": "这篇论文探讨了使用大型语言模型（LLMs）对K-12科学中的简答测评进行评分和解释。虽然现有方法可以为更结构化的数学和计算机科学测评打分，但它们通常不提供分数的解释。我们的研究侧重于在中学地球科学中应用GPT-4进行自动评估，将少样本学习和主动学习与思维链推理相结合。通过人机协同方法，我们成功对形成性评估回答进行评分并提供有意义的解释。我们方法的系统分析揭示了人机协同技术增强开放性科学测评的自动评分潜力。",
    "tldr": "使用大型语言模型（LLMs）进行自动评估中学生科学形成性评估回答，结合思维链推理，人机协同方法成功评分并提供有意义解释，有望提升开放性科学测评的自动评分。",
    "en_tdlr": "Using large language models (LLMs) for automated assessment of K-12 science formative assessment responses, incorporating chain-of-thought reasoning, the human-in-the-loop approach successfully scores and provides meaningful explanations, showing potential to enhance automated grading for open-ended science assessments."
}