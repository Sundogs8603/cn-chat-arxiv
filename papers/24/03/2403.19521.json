{
    "title": "Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models",
    "abstract": "arXiv:2403.19521v1 Announce Type: cross  Abstract: In this paper, we deeply explore the mechanisms employed by Transformer-based language models in factual recall tasks. In zero-shot scenarios, given a prompt like \"The capital of France is,\" task-specific attention heads extract the topic entity, such as \"France,\" from the context and pass it to subsequent MLPs to recall the required answer such as \"Paris.\" We introduce a novel analysis method aimed at decomposing the outputs of the MLP into components understandable by humans. Through this method, we quantify the function of the MLP layer following these task-specific heads. In the residual stream, it either erases or amplifies the information originating from individual heads. Moreover, it generates a component that redirects the residual stream towards the direction of its expected answer. These zero-shot mechanisms are also employed in few-shot scenarios. Additionally, we observed a widely existent anti-overconfidence mechanism in ",
    "link": "https://arxiv.org/abs/2403.19521",
    "context": "Title: Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models\nAbstract: arXiv:2403.19521v1 Announce Type: cross  Abstract: In this paper, we deeply explore the mechanisms employed by Transformer-based language models in factual recall tasks. In zero-shot scenarios, given a prompt like \"The capital of France is,\" task-specific attention heads extract the topic entity, such as \"France,\" from the context and pass it to subsequent MLPs to recall the required answer such as \"Paris.\" We introduce a novel analysis method aimed at decomposing the outputs of the MLP into components understandable by humans. Through this method, we quantify the function of the MLP layer following these task-specific heads. In the residual stream, it either erases or amplifies the information originating from individual heads. Moreover, it generates a component that redirects the residual stream towards the direction of its expected answer. These zero-shot mechanisms are also employed in few-shot scenarios. Additionally, we observed a widely existent anti-overconfidence mechanism in ",
    "path": "papers/24/03/2403.19521.json",
    "total_tokens": 861,
    "translated_title": "解释基于Transformer模型的语言模型在事实回忆中的关键机制",
    "translated_abstract": "本文深入探讨了Transformer-based语言模型在事实回忆任务中所采用的机制。在零次样本情况下，给定类似“法国的首都是”的提示，特定任务的注意力头会从上下文中提取主题实体，如“法国”，并将其传递给后续的MLP以回忆所需的答案，如“巴黎”。我们引入了一种新颖的分析方法，旨在将MLP的输出分解为人类可理解的组件。通过这种方法，我们量化了跟随这些特定任务头的MLP层的功能。在残差流中，它会擦除或放大来自各个头的信息。此外，它会生成一个组件，将残差流重新定向到预期答案的方向。这些零次机制也适用于少次样本情况。此外，我们观察到一种广泛存在的抗过度自信机制。",
    "tldr": "通过深入研究Transformer-based语言模型在事实回忆任务中的机制，我们发现了零/少次样本情况下的特定任务头、MLP层和残差流的功能，以及抗过度自信机制。",
    "en_tdlr": "By deeply exploring the mechanisms of Transformer-based language models in factual recall tasks, we identified the functions of task-specific heads, MLP layers, and residual streams in zero/few-shot scenarios, as well as an anti-overconfidence mechanism."
}