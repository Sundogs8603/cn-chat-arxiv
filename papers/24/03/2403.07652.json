{
    "title": "Harder Tasks Need More Experts: Dynamic Routing in MoE Models",
    "abstract": "arXiv:2403.07652v1 Announce Type: cross  Abstract: In this paper, we introduce a novel dynamic expert selection framework for Mixture of Experts (MoE) models, aiming to enhance computational efficiency and model performance by adjusting the number of activated experts based on input difficulty. Unlike traditional MoE approaches that rely on fixed Top-K routing, which activates a predetermined number of experts regardless of the input's complexity, our method dynamically selects experts based on the confidence level in expert selection for each input. This allows for a more efficient utilization of computational resources, activating more experts for complex tasks requiring advanced reasoning and fewer for simpler tasks. Through extensive evaluations, our dynamic routing method demonstrates substantial improvements over conventional Top-2 routing across various benchmarks, achieving an average improvement of 0.7% with less than 90% activated parameters. Further analysis shows our model ",
    "link": "https://arxiv.org/abs/2403.07652",
    "context": "Title: Harder Tasks Need More Experts: Dynamic Routing in MoE Models\nAbstract: arXiv:2403.07652v1 Announce Type: cross  Abstract: In this paper, we introduce a novel dynamic expert selection framework for Mixture of Experts (MoE) models, aiming to enhance computational efficiency and model performance by adjusting the number of activated experts based on input difficulty. Unlike traditional MoE approaches that rely on fixed Top-K routing, which activates a predetermined number of experts regardless of the input's complexity, our method dynamically selects experts based on the confidence level in expert selection for each input. This allows for a more efficient utilization of computational resources, activating more experts for complex tasks requiring advanced reasoning and fewer for simpler tasks. Through extensive evaluations, our dynamic routing method demonstrates substantial improvements over conventional Top-2 routing across various benchmarks, achieving an average improvement of 0.7% with less than 90% activated parameters. Further analysis shows our model ",
    "path": "papers/24/03/2403.07652.json",
    "total_tokens": 824,
    "translated_title": "较困难的任务需要更多专家：MoE模型中的动态路由",
    "translated_abstract": "在本文中，我们引入了一种新颖的动态专家选择框架，用于Mixture of Experts（MoE）模型，旨在通过根据输入难度调整激活的专家数量，增强计算效率和模型性能。与依赖于固定Top-K路由的传统MoE方法不同，该方法根据对每个输入的专家选择的置信水平动态选择专家。这允许更有效地利用计算资源，对需要高级推理的复杂任务激活更多的专家，对较简单的任务激活更少的专家。通过广泛的评估，我们的动态路由方法在各种基准测试中表现出明显的改进，与常规Top-2路由相比，实现了平均改进0.7%的效果，且激活参数少于90%。进一步的分析显示我们的模型",
    "tldr": "通过动态选择专家来提高计算效率和模型性能，针对不同难度的任务激活不同数量的专家，相比传统的Top-K路由方法，我们的动态路由方法在各种基准测试中取得了明显的改进。",
    "en_tdlr": "By dynamically selecting experts to improve computational efficiency and model performance, activating different numbers of experts for tasks of varying difficulty, our dynamic routing method has shown significant improvements over traditional Top-K routing methods across various benchmarks."
}