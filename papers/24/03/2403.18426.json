{
    "title": "TriviaHG: A Dataset for Automatic Hint Generation from Factoid Questions",
    "abstract": "arXiv:2403.18426v1 Announce Type: new  Abstract: Nowadays, individuals tend to engage in dialogues with Large Language Models, seeking answers to their questions. In times when such answers are readily accessible to anyone, the stimulation and preservation of human's cognitive abilities, as well as the assurance of maintaining good reasoning skills by humans becomes crucial. This study addresses such needs by proposing hints (instead of final answers or before giving answers) as a viable solution. We introduce a framework for the automatic hint generation for factoid questions, employing it to construct TriviaHG, a novel large-scale dataset featuring 160,230 hints corresponding to 16,645 questions from the TriviaQA dataset. Additionally, we present an automatic evaluation method that measures the Convergence and Familiarity quality attributes of hints. To evaluate the TriviaHG dataset and the proposed evaluation method, we enlisted 10 individuals to annotate 2,791 hints and tasked 6 hu",
    "link": "https://arxiv.org/abs/2403.18426",
    "context": "Title: TriviaHG: A Dataset for Automatic Hint Generation from Factoid Questions\nAbstract: arXiv:2403.18426v1 Announce Type: new  Abstract: Nowadays, individuals tend to engage in dialogues with Large Language Models, seeking answers to their questions. In times when such answers are readily accessible to anyone, the stimulation and preservation of human's cognitive abilities, as well as the assurance of maintaining good reasoning skills by humans becomes crucial. This study addresses such needs by proposing hints (instead of final answers or before giving answers) as a viable solution. We introduce a framework for the automatic hint generation for factoid questions, employing it to construct TriviaHG, a novel large-scale dataset featuring 160,230 hints corresponding to 16,645 questions from the TriviaQA dataset. Additionally, we present an automatic evaluation method that measures the Convergence and Familiarity quality attributes of hints. To evaluate the TriviaHG dataset and the proposed evaluation method, we enlisted 10 individuals to annotate 2,791 hints and tasked 6 hu",
    "path": "papers/24/03/2403.18426.json",
    "total_tokens": 857,
    "translated_title": "TriviaHG：用于从事实性问题生成自动提示的数据集",
    "translated_abstract": "现在，个人倾向于与大型语言模型进行对话，寻找他们问题的答案。在这样的答案对任何人都很容易获得的时代，刺激和保持人类的认知能力，以及确保人类保持良好推理能力变得至关重要。本研究通过提出提示（而不是最终答案或在给出答案之前）作为一种可行的解决方案来满足这些需求。我们介绍了一个用于事实性问题的自动提示生成框架，利用它构建了TriviaHG，这是一个新颖的大规模数据集，包含来自TriviaQA数据集的16,645个问题对应的160,230个提示。此外，我们提出了一种自动评估方法，用于衡量提示的收敛性和熟悉度质量属性。为了评估TriviaHG数据集和所提出的评估方法，我们邀请了10名个体注释2,791个提示，并分配了6名研究人员",
    "tldr": "提出了一个用于自动提示生成的框架，构建了一个包含160,230个提示的大规模数据集TriviaHG，并提出了一种评估方法来衡量提示的质量属性。",
    "en_tdlr": "Introduced a framework for automatic hint generation, constructed a large-scale dataset TriviaHG containing 160,230 hints, and proposed an evaluation method to measure the quality attributes of hints."
}