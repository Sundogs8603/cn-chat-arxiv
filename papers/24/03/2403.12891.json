{
    "title": "Adaptive Visual Imitation Learning for Robotic Assisted Feeding Across Varied Bowl Configurations and Food Types",
    "abstract": "arXiv:2403.12891v1 Announce Type: cross  Abstract: In this study, we introduce a novel visual imitation network with a spatial attention module for robotic assisted feeding (RAF). The goal is to acquire (i.e., scoop) food items from a bowl. However, achieving robust and adaptive food manipulation is particularly challenging. To deal with this, we propose a framework that integrates visual perception with imitation learning to enable the robot to handle diverse scenarios during scooping. Our approach, named AVIL (adaptive visual imitation learning), exhibits adaptability and robustness across different bowl configurations in terms of material, size, and position, as well as diverse food types including granular, semi-solid, and liquid, even in the presence of distractors. We validate the effectiveness of our approach by conducting experiments on a real robot. We also compare its performance with a baseline. The results demonstrate improvement over the baseline across all scenarios, with",
    "link": "https://arxiv.org/abs/2403.12891",
    "context": "Title: Adaptive Visual Imitation Learning for Robotic Assisted Feeding Across Varied Bowl Configurations and Food Types\nAbstract: arXiv:2403.12891v1 Announce Type: cross  Abstract: In this study, we introduce a novel visual imitation network with a spatial attention module for robotic assisted feeding (RAF). The goal is to acquire (i.e., scoop) food items from a bowl. However, achieving robust and adaptive food manipulation is particularly challenging. To deal with this, we propose a framework that integrates visual perception with imitation learning to enable the robot to handle diverse scenarios during scooping. Our approach, named AVIL (adaptive visual imitation learning), exhibits adaptability and robustness across different bowl configurations in terms of material, size, and position, as well as diverse food types including granular, semi-solid, and liquid, even in the presence of distractors. We validate the effectiveness of our approach by conducting experiments on a real robot. We also compare its performance with a baseline. The results demonstrate improvement over the baseline across all scenarios, with",
    "path": "papers/24/03/2403.12891.json",
    "total_tokens": 939,
    "translated_title": "自适应视觉模仿学习在不同碗配置和食物类型下的机器人辅助喂食中的应用",
    "translated_abstract": "在这项研究中，我们引入了一种带有空间注意模块的新型视觉模仿网络，用于机器人辅助喂食（RAF）。我们的目标是从碗中获取（即铲取）食物。然而，实现强健和灵活的食物操纵尤其具有挑战性。为了解决这个问题，我们提出了一个框架，将视觉感知与模仿学习相结合，使机器人能够在铲取过程中处理各种不同的情景。我们的方法称为AVIL（自适应视觉模仿学习），在材料、大小和位置等方面的不同碗配置以及包括颗粒状、半固体和液体在内的各种食物类型中展现了适应性和稳健性，甚至在干扰物存在的情况下也是如此。我们通过在实际机器人上进行实验来验证我们方法的有效性。我们还将其与基准线性能进行比较。结果表明，在所有情境下，我们的方法相比基准线性能有所提升。",
    "tldr": "提出了一种名为AVIL的自适应视觉模仿学习方法，通过将视觉感知与模仿学习相结合，实现了机器人在不同碗配置和食物类型下的灵活和稳健的辅助喂食。",
    "en_tdlr": "Introduced AVIL, a novel adaptive visual imitation learning method that integrates visual perception with imitation learning to enable the robot to achieve flexible and robust assisted feeding across varied bowl configurations and food types."
}