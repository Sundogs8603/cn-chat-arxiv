{
    "title": "Personalized Negative Reservoir for Incremental Learning in Recommender Systems",
    "abstract": "arXiv:2403.03993v1 Announce Type: cross  Abstract: Recommender systems have become an integral part of online platforms. Every day the volume of training data is expanding and the number of user interactions is constantly increasing. The exploration of larger and more expressive models has become a necessary pursuit to improve user experience. However, this progression carries with it an increased computational burden. In commercial settings, once a recommendation system model has been trained and deployed it typically needs to be updated frequently as new client data arrive. Cumulatively, the mounting volume of data is guaranteed to eventually make full batch retraining of the model from scratch computationally infeasible. Naively fine-tuning solely on the new data runs into the well-documented problem of catastrophic forgetting. Despite the fact that negative sampling is a crucial part of training with implicit feedback, no specialized technique exists that is tailored to the increme",
    "link": "https://arxiv.org/abs/2403.03993",
    "context": "Title: Personalized Negative Reservoir for Incremental Learning in Recommender Systems\nAbstract: arXiv:2403.03993v1 Announce Type: cross  Abstract: Recommender systems have become an integral part of online platforms. Every day the volume of training data is expanding and the number of user interactions is constantly increasing. The exploration of larger and more expressive models has become a necessary pursuit to improve user experience. However, this progression carries with it an increased computational burden. In commercial settings, once a recommendation system model has been trained and deployed it typically needs to be updated frequently as new client data arrive. Cumulatively, the mounting volume of data is guaranteed to eventually make full batch retraining of the model from scratch computationally infeasible. Naively fine-tuning solely on the new data runs into the well-documented problem of catastrophic forgetting. Despite the fact that negative sampling is a crucial part of training with implicit feedback, no specialized technique exists that is tailored to the increme",
    "path": "papers/24/03/2403.03993.json",
    "total_tokens": 792,
    "translated_title": "个性化负采样在推荐系统增量学习中的应用",
    "translated_abstract": "推荐系统已成为在线平台的重要组成部分。每天训练数据量不断扩大，用户互动次数不断增加。探索更大更具表现力的模型已成为改善用户体验的必要追求。然而，这种进展带来了更大的计算负担。在商业环境中，一旦推荐系统模型被训练和部署，通常需要频繁更新以适应新的客户数据。累积起来，数据量的增加必将使得从头开始进行全量重训练变得计算上不可行。仅仅在新数据上进行简单微调会遇到已被广泛记录的遗忘灾难问题。尽管负采样在使用隐式反馈进行训练中是至关重要的一部分，但目前并不存在专门针对增量学习的技术。",
    "tldr": "推荐系统中的个性化负采样技术在增量学习中的应用，解决了更新推荐系统模型时遇到的遗忘灾难问题。",
    "en_tdlr": "The personalized negative sampling technique in recommender systems addresses the problem of catastrophic forgetting when updating recommendation system models."
}