{
    "title": "Phasic Diversity Optimization for Population-Based Reinforcement Learning",
    "abstract": "arXiv:2403.11114v1 Announce Type: cross  Abstract: Reviewing the previous work of diversity Rein-forcement Learning,diversity is often obtained via an augmented loss function,which requires a balance between reward and diversity.Generally,diversity optimization algorithms use Multi-armed Bandits algorithms to select the coefficient in the pre-defined space. However, the dynamic distribution of reward signals for MABs or the conflict between quality and diversity limits the performance of these methods. We introduce the Phasic Diversity Optimization (PDO) algorithm, a Population-Based Training framework that separates reward and diversity training into distinct phases instead of optimizing a multi-objective function. In the auxiliary phase, agents with poor performance diversified via determinants will not replace the better agents in the archive. The decoupling of reward and diversity allows us to use an aggressive diversity optimization in the auxiliary phase without performance degra",
    "link": "https://arxiv.org/abs/2403.11114",
    "context": "Title: Phasic Diversity Optimization for Population-Based Reinforcement Learning\nAbstract: arXiv:2403.11114v1 Announce Type: cross  Abstract: Reviewing the previous work of diversity Rein-forcement Learning,diversity is often obtained via an augmented loss function,which requires a balance between reward and diversity.Generally,diversity optimization algorithms use Multi-armed Bandits algorithms to select the coefficient in the pre-defined space. However, the dynamic distribution of reward signals for MABs or the conflict between quality and diversity limits the performance of these methods. We introduce the Phasic Diversity Optimization (PDO) algorithm, a Population-Based Training framework that separates reward and diversity training into distinct phases instead of optimizing a multi-objective function. In the auxiliary phase, agents with poor performance diversified via determinants will not replace the better agents in the archive. The decoupling of reward and diversity allows us to use an aggressive diversity optimization in the auxiliary phase without performance degra",
    "path": "papers/24/03/2403.11114.json",
    "total_tokens": 711,
    "translated_title": "种群强化学习的相位多样性优化",
    "translated_abstract": "本文介绍了Phasic Diversity Optimization (PDO)算法，这是一个基于种群的训练框架，将奖励和多样性训练分为不同的阶段，而不是优化多目标函数。在辅助阶段，表现较差的agent通过决策者进行多样化，不会取代存档中更好的agent。奖励和多样性的解耦使我们能够在辅助阶段使用激进的多样性优化，而不会降低性能。",
    "tldr": "引入了Phasic Diversity Optimization (PDO)算法，采用种群训练框架，将奖励和多样性训练分为不同阶段，并在辅助阶段实现激进的多样性优化。",
    "en_tdlr": "Introducing the Phasic Diversity Optimization (PDO) algorithm, which utilizes a population-based training framework to separate reward and diversity training into different phases, allowing for aggressive diversity optimization in the auxiliary phase."
}