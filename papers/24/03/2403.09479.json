{
    "title": "Laying the Foundation First? Investigating the Generalization from Atomic Skills to Complex Reasoning Tasks",
    "abstract": "arXiv:2403.09479v1 Announce Type: new  Abstract: Current language models have demonstrated their capability to develop basic reasoning, but struggle in more complicated reasoning tasks that require a combination of atomic skills, such as math word problem requiring skills like arithmetic and unit conversion. Previous methods either do not improve the inherent atomic skills of models or not attempt to generalize the atomic skills to complex reasoning tasks. In this paper, we first propose a probing framework to investigate whether the atomic skill can spontaneously generalize to complex reasoning tasks. Then, we introduce a hierarchical curriculum learning training strategy to achieve better skill generalization. In our experiments, we find that atomic skills can not spontaneously generalize to compositional tasks. By leveraging hierarchical curriculum learning, we successfully induce generalization, significantly improve the performance of open-source LMs on complex reasoning tasks. Pr",
    "link": "https://arxiv.org/abs/2403.09479",
    "context": "Title: Laying the Foundation First? Investigating the Generalization from Atomic Skills to Complex Reasoning Tasks\nAbstract: arXiv:2403.09479v1 Announce Type: new  Abstract: Current language models have demonstrated their capability to develop basic reasoning, but struggle in more complicated reasoning tasks that require a combination of atomic skills, such as math word problem requiring skills like arithmetic and unit conversion. Previous methods either do not improve the inherent atomic skills of models or not attempt to generalize the atomic skills to complex reasoning tasks. In this paper, we first propose a probing framework to investigate whether the atomic skill can spontaneously generalize to complex reasoning tasks. Then, we introduce a hierarchical curriculum learning training strategy to achieve better skill generalization. In our experiments, we find that atomic skills can not spontaneously generalize to compositional tasks. By leveraging hierarchical curriculum learning, we successfully induce generalization, significantly improve the performance of open-source LMs on complex reasoning tasks. Pr",
    "path": "papers/24/03/2403.09479.json",
    "total_tokens": 768,
    "translated_title": "从基本技能到复杂推理任务：先打好基础再说？",
    "translated_abstract": "当前的语言模型已经展示了它们发展基本推理的能力，但在需要结合诸如算术和单位转换等基本技能的更复杂推理任务中往往困难重重。本文首先提出了一个探测框架，来研究基本技能能否自发地推广到复杂推理任务。然后，我们引入了一个分级课程学习训练策略，以实现更好的技能推广。在实验中，我们发现基本技能不能自发地推广到合成任务。通过利用分层课程学习，我们成功诱导了推广，显著改善了开源语言模型在复杂推理任务上的表现。",
    "tldr": "本文研究了语言模型从基本技能到复杂推理任务的泛化能力，并提出了分级课程学习训练策略，成功改进了模型在复杂推理任务上的性能。",
    "en_tdlr": "This paper investigates the generalization capability of language models from basic skills to complex reasoning tasks and proposes a hierarchical curriculum learning training strategy, which significantly improves the performance of the models on complex reasoning tasks."
}