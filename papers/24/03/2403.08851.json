{
    "title": "PAPERCLIP: Associating Astronomical Observations and Natural Language with Multi-Modal Models",
    "abstract": "arXiv:2403.08851v1 Announce Type: cross  Abstract: We present PAPERCLIP (Proposal Abstracts Provide an Effective Representation for Contrastive Language-Image Pre-training), a method which associates astronomical observations imaged by telescopes with natural language using a neural network model. The model is fine-tuned from a pre-trained Contrastive Language-Image Pre-training (CLIP) model using successful observing proposal abstracts and corresponding downstream observations, with the abstracts optionally summarized via guided generation using large language models (LLMs). Using observations from the Hubble Space Telescope (HST) as an example, we show that the fine-tuned model embodies a meaningful joint representation between observations and natural language through tests targeting image retrieval (i.e., finding the most relevant observations using natural language queries) and description retrieval (i.e., querying for astrophysical object classes and use cases most relevant to a ",
    "link": "https://arxiv.org/abs/2403.08851",
    "context": "Title: PAPERCLIP: Associating Astronomical Observations and Natural Language with Multi-Modal Models\nAbstract: arXiv:2403.08851v1 Announce Type: cross  Abstract: We present PAPERCLIP (Proposal Abstracts Provide an Effective Representation for Contrastive Language-Image Pre-training), a method which associates astronomical observations imaged by telescopes with natural language using a neural network model. The model is fine-tuned from a pre-trained Contrastive Language-Image Pre-training (CLIP) model using successful observing proposal abstracts and corresponding downstream observations, with the abstracts optionally summarized via guided generation using large language models (LLMs). Using observations from the Hubble Space Telescope (HST) as an example, we show that the fine-tuned model embodies a meaningful joint representation between observations and natural language through tests targeting image retrieval (i.e., finding the most relevant observations using natural language queries) and description retrieval (i.e., querying for astrophysical object classes and use cases most relevant to a ",
    "path": "papers/24/03/2403.08851.json",
    "total_tokens": 837,
    "translated_title": "PAPERCLIP：使用多模态模型将天文观测和自然语言关联起来",
    "translated_abstract": "我们提出了PAPERCLIP（Proposal Abstracts Provide an Effective Representation for Contrastive Language-Image Pre-training），一种使用神经网络模型将由望远镜成像的天文观测与自然语言关联起来的方法。该模型是从经过预训练的对比语言-图像预训练（CLIP）模型微调而来，使用成功的观测提案摘要和相应的下游观测，其中摘要可选择通过使用大型语言模型（LLMs）进行引导生成来进行总结。以哈勃空间望远镜（HST）的观测为例，我们展示了微调的模型通过针对图像检索（即使用自然语言查询找到最相关的观测）和描述检索（即查询与天文物体类别和用例最相关的内容）的测试，体现了观测和自然语言之间的有意义的联合表示。",
    "tldr": "该研究提出了PAPERCLIP方法，通过将天文观测与自然语言关联起来，利用预训练的CLIP模型进行微调，实现了观测和自然语言之间的有意义的联合表示。",
    "en_tdlr": "This paper introduces the PAPERCLIP method, which associates astronomical observations with natural language, fine-tuning a pre-trained CLIP model to achieve a meaningful joint representation between observations and natural language."
}