{
    "title": "Federated reinforcement learning for robot motion planning with zero-shot generalization",
    "abstract": "arXiv:2403.13245v1 Announce Type: cross  Abstract: This paper considers the problem of learning a control policy for robot motion planning with zero-shot generalization, i.e., no data collection and policy adaptation is needed when the learned policy is deployed in new environments. We develop a federated reinforcement learning framework that enables collaborative learning of multiple learners and a central server, i.e., the Cloud, without sharing their raw data. In each iteration, each learner uploads its local control policy and the corresponding estimated normalized arrival time to the Cloud, which then computes the global optimum among the learners and broadcasts the optimal policy to the learners. Each learner then selects between its local control policy and that from the Cloud for next iteration. The proposed framework leverages on the derived zero-shot generalization guarantees on arrival time and safety. Theoretical guarantees on almost-sure convergence, almost consensus, Pare",
    "link": "https://arxiv.org/abs/2403.13245",
    "context": "Title: Federated reinforcement learning for robot motion planning with zero-shot generalization\nAbstract: arXiv:2403.13245v1 Announce Type: cross  Abstract: This paper considers the problem of learning a control policy for robot motion planning with zero-shot generalization, i.e., no data collection and policy adaptation is needed when the learned policy is deployed in new environments. We develop a federated reinforcement learning framework that enables collaborative learning of multiple learners and a central server, i.e., the Cloud, without sharing their raw data. In each iteration, each learner uploads its local control policy and the corresponding estimated normalized arrival time to the Cloud, which then computes the global optimum among the learners and broadcasts the optimal policy to the learners. Each learner then selects between its local control policy and that from the Cloud for next iteration. The proposed framework leverages on the derived zero-shot generalization guarantees on arrival time and safety. Theoretical guarantees on almost-sure convergence, almost consensus, Pare",
    "path": "papers/24/03/2403.13245.json",
    "total_tokens": 860,
    "translated_title": "机器人运动规划的联邦强化学习与零次通用化",
    "translated_abstract": "本文考虑了使用零次通用化学习控制策略进行机器人运动规划的问题，即在部署学习策略到新环境时不需要数据收集和策略调整。我们开发了一个联邦强化学习框架，实现了多个学习者和中央服务器（云端）的协作学习，而不分享原始数据。在每次迭代中，每个学习者将其本地控制策略和相应的估计归一化到达时间上传至云端，然后云端在学习者间计算全局最优并将最优策略广播给学习者。每个学习者然后在下一次迭代中从其本地控制策略和云端中选择。提出的框架利用了到达时间和安全性的零次通用化保证。对于几乎必然收敛，几乎一致性，Pare的理论保证//}",
    "tldr": "该论文提出了一个联邦强化学习框架，实现了机器人运动规划中的零次通用化，通过协作学习多个学习者和中央服务器，在不共享原始数据的情况下达到全局最优解。",
    "en_tdlr": "This paper introduces a federated reinforcement learning framework for robot motion planning with zero-shot generalization, enabling collaborative learning of multiple learners and a central server to achieve global optimum without sharing raw data."
}