{
    "title": "Proxy-RLHF: Decoupling Generation and Alignment in Large Language Model with Proxy",
    "abstract": "arXiv:2403.04283v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) is the prevailing approach to ensure Large Language Models (LLMs) align with human values. However, existing RLHF methods require a high computational cost, one main reason being that RLHF assigns both the generation and alignment tasks to the LLM simultaneously. In this paper, we introduce Proxy-RLHF, which decouples the generation and alignment processes of LLMs, achieving alignment with human values at a much lower computational cost. We start with a novel Markov Decision Process (MDP) designed for the alignment process and employ Reinforcement Learning (RL) to train a streamlined proxy model that oversees the token generation of the LLM, without altering the LLM itself. Experiments show that our method achieves a comparable level of alignment with only 1\\% of the training parameters of other methods.",
    "link": "https://arxiv.org/abs/2403.04283",
    "context": "Title: Proxy-RLHF: Decoupling Generation and Alignment in Large Language Model with Proxy\nAbstract: arXiv:2403.04283v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) is the prevailing approach to ensure Large Language Models (LLMs) align with human values. However, existing RLHF methods require a high computational cost, one main reason being that RLHF assigns both the generation and alignment tasks to the LLM simultaneously. In this paper, we introduce Proxy-RLHF, which decouples the generation and alignment processes of LLMs, achieving alignment with human values at a much lower computational cost. We start with a novel Markov Decision Process (MDP) designed for the alignment process and employ Reinforcement Learning (RL) to train a streamlined proxy model that oversees the token generation of the LLM, without altering the LLM itself. Experiments show that our method achieves a comparable level of alignment with only 1\\% of the training parameters of other methods.",
    "path": "papers/24/03/2403.04283.json",
    "total_tokens": 849,
    "translated_title": "Proxy-RLHF：在大型语言模型中通过代理解耦生成和对齐",
    "translated_abstract": "强化学习从人类反馈中学习（RLHF）是确保大型语言模型（LLMs）与人类价值观保持一致的主流方法。然而，现有的RLHF方法需要高昂的计算成本，主要原因之一是RLHF同时将生成和对齐任务分配给LLM。本文介绍了Proxy-RLHF，它解耦了LLMs的生成和对齐流程，以更低的计算成本实现与人类价值的对齐。我们从为对齐过程设计的新型马尔可夫决策过程（MDP）开始，并使用强化学习（RL）训练了一个简化的代理模型，监督LLM的标记生成，而不改变LLM本身。实验证明，我们的方法仅使用其他方法的1%训练参数即可实现可比水平的对齐度。",
    "tldr": "该论文提出Proxy-RLHF方法，通过将大型语言模型的生成和对齐过程解耦，实现了以更低计算成本对齐人类价值观，仅使用其他方法的1%训练参数即可达到可比水平的对齐度。",
    "en_tdlr": "This paper introduces the Proxy-RLHF method, which decouples the generation and alignment processes of Large Language Models, achieving alignment with human values at a much lower computational cost with only 1% of the training parameters needed by other methods."
}