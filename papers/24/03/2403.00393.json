{
    "title": "Private Benchmarking to Prevent Contamination and Improve Comparative Evaluation of LLMs",
    "abstract": "arXiv:2403.00393v1 Announce Type: cross  Abstract: Benchmarking is the de-facto standard for evaluating LLMs, due to its speed, replicability and low cost. However, recent work has pointed out that the majority of the open source benchmarks available today have been contaminated or leaked into LLMs, meaning that LLMs have access to test data during pretraining and/or fine-tuning. This raises serious concerns about the validity of benchmarking studies conducted so far and the future of evaluation using benchmarks. To solve this problem, we propose Private Benchmarking, a solution where test datasets are kept private and models are evaluated without revealing the test data to the model. We describe various scenarios (depending on the trust placed on model owners or dataset owners), and present solutions to avoid data contamination using private benchmarking. For scenarios where the model weights need to be kept private, we describe solutions from confidential computing and cryptography t",
    "link": "https://arxiv.org/abs/2403.00393",
    "context": "Title: Private Benchmarking to Prevent Contamination and Improve Comparative Evaluation of LLMs\nAbstract: arXiv:2403.00393v1 Announce Type: cross  Abstract: Benchmarking is the de-facto standard for evaluating LLMs, due to its speed, replicability and low cost. However, recent work has pointed out that the majority of the open source benchmarks available today have been contaminated or leaked into LLMs, meaning that LLMs have access to test data during pretraining and/or fine-tuning. This raises serious concerns about the validity of benchmarking studies conducted so far and the future of evaluation using benchmarks. To solve this problem, we propose Private Benchmarking, a solution where test datasets are kept private and models are evaluated without revealing the test data to the model. We describe various scenarios (depending on the trust placed on model owners or dataset owners), and present solutions to avoid data contamination using private benchmarking. For scenarios where the model weights need to be kept private, we describe solutions from confidential computing and cryptography t",
    "path": "papers/24/03/2403.00393.json",
    "total_tokens": 864,
    "translated_title": "防止污染和提高LLM比较评估的私人基准设定",
    "translated_abstract": "基准设定是评估LLM的事实标准，因为它速度快、可复制且成本低廉。然而，最近的研究指出，今天大多数开源基准设定已经被污染或泄露到LLM中，这意味着LLM在预训练和/或微调期间可以访问测试数据。这对迄今为止进行的基准研究的有效性以及未来使用基准进行评估提出了严重关切。为了解决这个问题，我们提出了Private Benchmarking，这是一个方案，其中测试数据集保持私密，模型在不向模型透露测试数据的情况下进行评估。我们描述了各种场景（取决于对模型所有者或数据集所有者的信任），并提出了使用私人基准设定避免数据污染的解决方案。对于需要保护模型权重的情况，我们描述了来自机密计算和密码学的解决方案。",
    "tldr": "本论文提出了私人基准设定方法，通过保持测试数据的私密性，使模型在不揭露测试数据的情况下进行评估，解决了当前基准设定中存在数据污染的问题。",
    "en_tdlr": "This paper proposes a private benchmarking approach to evaluate models without revealing test data, addressing the issue of data contamination in current benchmarking practices."
}