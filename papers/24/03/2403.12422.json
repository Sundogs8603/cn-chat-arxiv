{
    "title": "Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization",
    "abstract": "arXiv:2403.12422v1 Announce Type: new  Abstract: Pretraining transformers are generally time-consuming. Fully quantized training (FQT) is a promising approach to speed up pretraining. However, most FQT methods adopt a quantize-compute-dequantize procedure, which often leads to suboptimal speedup and significant performance degradation when used in transformers due to the high memory access overheads and low-precision computations. In this work, we propose Jetfire, an efficient and accurate INT8 training method specific to transformers. Our method features an INT8 data flow to optimize memory access and a per-block quantization method to maintain the accuracy of pretrained transformers. Extensive experiments demonstrate that our INT8 FQT method achieves comparable accuracy to the FP16 training baseline and outperforms the existing INT8 training works for transformers. Moreover, for a standard transformer block, our method offers an end-to-end training speedup of 1.42x and a 1.49x memory",
    "link": "https://arxiv.org/abs/2403.12422",
    "context": "Title: Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization\nAbstract: arXiv:2403.12422v1 Announce Type: new  Abstract: Pretraining transformers are generally time-consuming. Fully quantized training (FQT) is a promising approach to speed up pretraining. However, most FQT methods adopt a quantize-compute-dequantize procedure, which often leads to suboptimal speedup and significant performance degradation when used in transformers due to the high memory access overheads and low-precision computations. In this work, we propose Jetfire, an efficient and accurate INT8 training method specific to transformers. Our method features an INT8 data flow to optimize memory access and a per-block quantization method to maintain the accuracy of pretrained transformers. Extensive experiments demonstrate that our INT8 FQT method achieves comparable accuracy to the FP16 training baseline and outperforms the existing INT8 training works for transformers. Moreover, for a standard transformer block, our method offers an end-to-end training speedup of 1.42x and a 1.49x memory",
    "path": "papers/24/03/2403.12422.json",
    "total_tokens": 920,
    "translated_title": "Jetfire: 使用INT8数据流和按块量化的高效准确Transformer预训练方法",
    "translated_abstract": "预训练transformer通常耗时较长。完全量化训练（FQT）是一种加速预训练的有前途的方法。然而，大多数FQT方法采用量化-计算-反量化的过程，这往往导致在transformers中使用时出现次优的加速和显著的性能降级，原因是高内存访问开销和低精度计算。在这项工作中，我们提出了Jetfire，一种针对transformers的高效准确INT8训练方法。我们的方法采用INT8数据流来优化内存访问，并采用按块量化方法来保持预先训练的transformers的准确性。大量实验证明，我们的INT8 FQT方法达到了与FP16训练基线相当的精度，并且在transformers的INT8训练方面优于现有方法。此外，针对标准的transformer块，我们的方法提供了1.42倍的端到端训练加速和1.49倍的内存节省。",
    "tldr": "Jetfire提出了一种针对transformers的高效准确的INT8训练方法，实现了与FP16训练基线相当的精度，为标准transformer块提供了1.42倍的训练加速和1.49倍的内存节省。",
    "en_tdlr": "Jetfire proposes an efficient and accurate INT8 training method specific to transformers, achieving comparable accuracy to the FP16 training baseline and offering a 1.42x training speedup and 1.49x memory savings for standard transformer blocks."
}