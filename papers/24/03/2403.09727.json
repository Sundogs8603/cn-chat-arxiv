{
    "title": "Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems",
    "abstract": "arXiv:2403.09727v1 Announce Type: cross  Abstract: The development of generative large language models (G-LLM) opened up new opportunities for the development of new types of knowledge-based systems similar to ChatGPT, Bing, or Gemini. Fine-tuning (FN) and Retrieval-Augmented Generation (RAG) are the techniques that can be used to implement domain adaptation for the development of G-LLM-based knowledge systems. In our study, using ROUGE, BLEU, METEOR scores, and cosine similarity, we compare and examine the performance of RAG and FN for the GPT-J-6B, OPT-6.7B, LlaMA, LlaMA-2 language models. Based on measurements shown on different datasets, we demonstrate that RAG-based constructions are more efficient than models produced with FN. We point out that connecting RAG and FN is not trivial, because connecting FN models with RAG can cause a decrease in performance. Furthermore, we outline a simple RAG-based architecture which, on average, outperforms the FN models by 16% in terms of the RO",
    "link": "https://arxiv.org/abs/2403.09727",
    "context": "Title: Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems\nAbstract: arXiv:2403.09727v1 Announce Type: cross  Abstract: The development of generative large language models (G-LLM) opened up new opportunities for the development of new types of knowledge-based systems similar to ChatGPT, Bing, or Gemini. Fine-tuning (FN) and Retrieval-Augmented Generation (RAG) are the techniques that can be used to implement domain adaptation for the development of G-LLM-based knowledge systems. In our study, using ROUGE, BLEU, METEOR scores, and cosine similarity, we compare and examine the performance of RAG and FN for the GPT-J-6B, OPT-6.7B, LlaMA, LlaMA-2 language models. Based on measurements shown on different datasets, we demonstrate that RAG-based constructions are more efficient than models produced with FN. We point out that connecting RAG and FN is not trivial, because connecting FN models with RAG can cause a decrease in performance. Furthermore, we outline a simple RAG-based architecture which, on average, outperforms the FN models by 16% in terms of the RO",
    "path": "papers/24/03/2403.09727.json",
    "total_tokens": 883,
    "translated_title": "探究检索增强生成和微调在发展基于人工智能的知识系统中的表现",
    "translated_abstract": "arXiv:2403.09727v1 公告类型: 交叉领域 摘要: 生成式大型语言模型(G-LLM)的发展为类似ChatGPT、Bing或Gemini的新型知识系统的开发打开了新的机会。微调(FN)和检索增强生成(RAG)是可用于实现基于G-LLM的知识系统领域自适应的技术。在我们的研究中，利用ROUGE、BLEU、METEOR分数和余弦相似度，我们比较并检验了GPT-J-6B、OPT-6.7B、LlaMA、LlaMA-2语言模型的RAG和FN的表现。基于在不同数据集上展示的测量结果，我们展示了基于RAG的构建比使用FN产生的模型更有效。我们指出将RAG和FN连接起来并不是轻而易举的，因为将FN模型与RAG连接可能会导致性能下降。此外，我们概述了一个简单的基于RAG的架构，平均在RO方面优于FN模型16%",
    "tldr": "RAG-based constructions are more efficient than models produced with FN for the development of AI-driven knowledge-based systems.",
    "en_tdlr": "RAG-based constructions are more efficient than models produced with FN for the development of AI-driven knowledge-based systems."
}