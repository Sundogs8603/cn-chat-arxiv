{
    "title": "The Topos of Transformer Networks",
    "abstract": "arXiv:2403.18415v1 Announce Type: new  Abstract: The transformer neural network has significantly out-shined all other neural network architectures as the engine behind large language models. We provide a theoretical analysis of the expressivity of the transformer architecture through the lens of topos theory. From this viewpoint, we show that many common neural network architectures, such as the convolutional, recurrent and graph convolutional networks, can be embedded in a pretopos of piecewise-linear functions, but that the transformer necessarily lives in its topos completion. In particular, this suggests that the two network families instantiate different fragments of logic: the former are first order, whereas transformers are higher-order reasoners. Furthermore, we draw parallels with architecture search and gradient descent, integrating our analysis in the framework of cybernetic agents.",
    "link": "https://arxiv.org/abs/2403.18415",
    "context": "Title: The Topos of Transformer Networks\nAbstract: arXiv:2403.18415v1 Announce Type: new  Abstract: The transformer neural network has significantly out-shined all other neural network architectures as the engine behind large language models. We provide a theoretical analysis of the expressivity of the transformer architecture through the lens of topos theory. From this viewpoint, we show that many common neural network architectures, such as the convolutional, recurrent and graph convolutional networks, can be embedded in a pretopos of piecewise-linear functions, but that the transformer necessarily lives in its topos completion. In particular, this suggests that the two network families instantiate different fragments of logic: the former are first order, whereas transformers are higher-order reasoners. Furthermore, we draw parallels with architecture search and gradient descent, integrating our analysis in the framework of cybernetic agents.",
    "path": "papers/24/03/2403.18415.json",
    "total_tokens": 759,
    "translated_title": "Transformer网络的拓扑结构",
    "translated_abstract": "Transformer神经网络已经远远超越所有其他神经网络架构，成为大型语言模型背后的引擎。我们通过拓扑理论的视角提供了对Transformer架构表达能力的理论分析。从这个观点出发，我们展示了许多常见的神经网络架构，如卷积网络、循环网络和图卷积网络，可以嵌入在分段线性函数的预拓扑中，但Transformer必然存在于其拓扑完备性中。特别地，这表明这两个网络家族实例化了不同的逻辑片段：前者是一阶的，而Transformers是高阶推理机。此外，我们还将拓扑理论与架构搜索和梯度下降进行了类比，将我们的分析纳入了控制论代理的框架中。",
    "tldr": "通过拓扑理论的视角，我们对Transformer架构的表达能力进行了理论分析，发现它具有高阶推理的特点，并与其他常见神经网络架构进行了对比。"
}