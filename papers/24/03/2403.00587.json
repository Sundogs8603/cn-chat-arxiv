{
    "title": "Improving Explicit Spatial Relationships in Text-to-Image Generation through an Automatically Derived Dataset",
    "abstract": "arXiv:2403.00587v1 Announce Type: cross  Abstract: Existing work has observed that current text-to-image systems do not accurately reflect explicit spatial relations between objects such as 'left of' or 'below'. We hypothesize that this is because explicit spatial relations rarely appear in the image captions used to train these models. We propose an automatic method that, given existing images, generates synthetic captions that contain 14 explicit spatial relations. We introduce the Spatial Relation for Generation (SR4G) dataset, which contains 9.9 millions image-caption pairs for training, and more than 60 thousand captions for evaluation. In order to test generalization we also provide an 'unseen' split, where the set of objects in the train and test captions are disjoint. SR4G is the first dataset that can be used to spatially fine-tune text-to-image systems. We show that fine-tuning two different Stable Diffusion models (denoted as SD$_{SR4G}$) yields up to 9 points improvements i",
    "link": "https://arxiv.org/abs/2403.00587",
    "context": "Title: Improving Explicit Spatial Relationships in Text-to-Image Generation through an Automatically Derived Dataset\nAbstract: arXiv:2403.00587v1 Announce Type: cross  Abstract: Existing work has observed that current text-to-image systems do not accurately reflect explicit spatial relations between objects such as 'left of' or 'below'. We hypothesize that this is because explicit spatial relations rarely appear in the image captions used to train these models. We propose an automatic method that, given existing images, generates synthetic captions that contain 14 explicit spatial relations. We introduce the Spatial Relation for Generation (SR4G) dataset, which contains 9.9 millions image-caption pairs for training, and more than 60 thousand captions for evaluation. In order to test generalization we also provide an 'unseen' split, where the set of objects in the train and test captions are disjoint. SR4G is the first dataset that can be used to spatially fine-tune text-to-image systems. We show that fine-tuning two different Stable Diffusion models (denoted as SD$_{SR4G}$) yields up to 9 points improvements i",
    "path": "papers/24/03/2403.00587.json",
    "total_tokens": 894,
    "translated_title": "通过自动生成的数据集改进文本到图像生成中的显式空间关系",
    "translated_abstract": "现有研究发现，当前的文本到图像系统未能准确反映物体之间的明确空间关系，比如“在左侧”或“在下方”。 我们假设这是因为用于训练这些模型的图像标题中很少出现明确的空间关系。我们提出了一种自动方法，通过给定现有图像生成包含14个明确空间关系的合成标题。我们引入了“生成空间关系”(SR4G)数据集，其中包含990万个用于训练的图像标题对和超过6万个用于评估的标题。为了测试泛化性，我们还提供了一个“未见过”的切分，其中训练和测试标题中的对象集是不相交的。 SR4G是可以用来在文本到图像系统中进行空间微调的首个数据集。我们展示了通过微调两个不同的稳定扩散模型（标记为SD$_{SR4G}$）可以获得多达9个分数的改进。",
    "tldr": "该论文提出了一种自动生成包含明确空间关系的合成标题的方法，介绍了用于训练和评估的SR4G数据集，以及展示了通过微调稳定扩散模型可以获得高达9分的改进。",
    "en_tdlr": "This paper presents a method to automatically generate synthetic captions containing explicit spatial relations, introduces the SR4G dataset for training and evaluation, and shows up to 9 points improvements through fine-tuning Stable Diffusion models."
}