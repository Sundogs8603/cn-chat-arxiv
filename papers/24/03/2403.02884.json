{
    "title": "MathScale: Scaling Instruction Tuning for Mathematical Reasoning",
    "abstract": "arXiv:2403.02884v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in problem-solving. However, their proficiency in solving mathematical problems remains inadequate. We propose MathScale, a simple and scalable method to create high-quality mathematical reasoning data using frontier LLMs (e.g., {\\tt GPT-3.5}). Inspired by the cognitive mechanism in human mathematical learning, it first extracts topics and knowledge points from seed math questions and then build a concept graph, which is subsequently used to generate new math questions. MathScale exhibits effective scalability along the size axis of the math dataset that we generate. As a result, we create a mathematical reasoning dataset (MathScaleQA) containing two million math question-answer pairs. To evaluate mathematical reasoning abilities of LLMs comprehensively, we construct {\\sc MwpBench}, a benchmark of Math Word Problems, which is a collection of ten datasets (including ",
    "link": "https://arxiv.org/abs/2403.02884",
    "context": "Title: MathScale: Scaling Instruction Tuning for Mathematical Reasoning\nAbstract: arXiv:2403.02884v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in problem-solving. However, their proficiency in solving mathematical problems remains inadequate. We propose MathScale, a simple and scalable method to create high-quality mathematical reasoning data using frontier LLMs (e.g., {\\tt GPT-3.5}). Inspired by the cognitive mechanism in human mathematical learning, it first extracts topics and knowledge points from seed math questions and then build a concept graph, which is subsequently used to generate new math questions. MathScale exhibits effective scalability along the size axis of the math dataset that we generate. As a result, we create a mathematical reasoning dataset (MathScaleQA) containing two million math question-answer pairs. To evaluate mathematical reasoning abilities of LLMs comprehensively, we construct {\\sc MwpBench}, a benchmark of Math Word Problems, which is a collection of ten datasets (including ",
    "path": "papers/24/03/2403.02884.json",
    "total_tokens": 804,
    "translated_title": "MathScale: 数学推理的指导优化尺度",
    "translated_abstract": "大型语言模型（LLMs）在问题解决方面展现了出色的能力，但它们在解决数学问题方面的熟练程度仍然不足。我们提出了MathScale，这是一种简单且可扩展的方法，使用前沿的LLMs（例如GPT-3.5）创建高质量的数学推理数据。受人类数学学习中的认知机制启发，它首先从种子数学问题中提取主题和知识点，然后构建一个概念图，随后用于生成新的数学问题。MathScale在我们生成的数学数据集的大小方面展现出了有效的可扩展性。因此，我们创建了一个包含两百万数学问题-答案对的数学推理数据集（MathScaleQA）。为了全面评估LLMs的数学推理能力，我们构建了MwpBench，这是一个数学问题词汇问题基准，包括十个数据集。",
    "tldr": "MathScale提出了一种简单可扩展的方法来创建高质量的数学推理数据，展现出在数学数据集大小方面的有效可扩展性。"
}