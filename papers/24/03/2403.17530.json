{
    "title": "Boosting Few-Shot Learning with Disentangled Self-Supervised Learning and Meta-Learning for Medical Image Classification",
    "abstract": "arXiv:2403.17530v1 Announce Type: cross  Abstract: Background and objective: Employing deep learning models in critical domains such as medical imaging poses challenges associated with the limited availability of training data. We present a strategy for improving the performance and generalization capabilities of models trained in low-data regimes. Methods: The proposed method starts with a pre-training phase, where features learned in a self-supervised learning setting are disentangled to improve the robustness of the representations for downstream tasks. We then introduce a meta-fine-tuning step, leveraging related classes between meta-training and meta-testing phases but varying the granularity level. This approach aims to enhance the model's generalization capabilities by exposing it to more challenging classification tasks during meta-training and evaluating it on easier tasks but holding greater clinical relevance during meta-testing. We demonstrate the effectiveness of the propo",
    "link": "https://arxiv.org/abs/2403.17530",
    "context": "Title: Boosting Few-Shot Learning with Disentangled Self-Supervised Learning and Meta-Learning for Medical Image Classification\nAbstract: arXiv:2403.17530v1 Announce Type: cross  Abstract: Background and objective: Employing deep learning models in critical domains such as medical imaging poses challenges associated with the limited availability of training data. We present a strategy for improving the performance and generalization capabilities of models trained in low-data regimes. Methods: The proposed method starts with a pre-training phase, where features learned in a self-supervised learning setting are disentangled to improve the robustness of the representations for downstream tasks. We then introduce a meta-fine-tuning step, leveraging related classes between meta-training and meta-testing phases but varying the granularity level. This approach aims to enhance the model's generalization capabilities by exposing it to more challenging classification tasks during meta-training and evaluating it on easier tasks but holding greater clinical relevance during meta-testing. We demonstrate the effectiveness of the propo",
    "path": "papers/24/03/2403.17530.json",
    "total_tokens": 824,
    "translated_title": "利用解缠的自监督学习和元学习增强医学图像分类的少样本学习",
    "translated_abstract": "背景和目标：在医学成像等关键领域使用深度学习模型面临着训练数据有限的挑战。我们提出了一种改进在低数据环境中训练模型性能和泛化能力的策略。方法：所提出的方法从一个预训练阶段开始，其中在自监督学习设置中学到的特征被解缠以提高表示对下游任务的鲁棒性。然后引入元微调步骤，利用元训练和元测试阶段之间的相关类别但变化粒度级别。这种方法旨在通过在元训练期间将模型暴露于更具挑战性的分类任务，然后在元测试期间评估它在更简单但在临床上更具重要性的任务上的表现，从而增强模型的泛化能力。我们展示了该方法的有效性。",
    "tldr": "通过解缠的自监督学习和元学习，在低数据环境中训练医学图像分类模型，提高了模型的性能和泛化能力。"
}