{
    "title": "Few-Shot Relation Extraction with Hybrid Visual Evidence",
    "abstract": "arXiv:2403.00724v1 Announce Type: new  Abstract: The goal of few-shot relation extraction is to predict relations between name entities in a sentence when only a few labeled instances are available for training. Existing few-shot relation extraction methods focus on uni-modal information such as text only. This reduces performance when there are no clear contexts between the name entities described in text. We propose a multi-modal few-shot relation extraction model (MFS-HVE) that leverages both textual and visual semantic information to learn a multi-modal representation jointly. The MFS-HVE includes semantic feature extractors and multi-modal fusion components. The MFS-HVE semantic feature extractors are developed to extract both textual and visual features. The visual features include global image features and local object features within the image. The MFS-HVE multi-modal fusion unit integrates information from various modalities using image-guided attention, object-guided attentio",
    "link": "https://arxiv.org/abs/2403.00724",
    "context": "Title: Few-Shot Relation Extraction with Hybrid Visual Evidence\nAbstract: arXiv:2403.00724v1 Announce Type: new  Abstract: The goal of few-shot relation extraction is to predict relations between name entities in a sentence when only a few labeled instances are available for training. Existing few-shot relation extraction methods focus on uni-modal information such as text only. This reduces performance when there are no clear contexts between the name entities described in text. We propose a multi-modal few-shot relation extraction model (MFS-HVE) that leverages both textual and visual semantic information to learn a multi-modal representation jointly. The MFS-HVE includes semantic feature extractors and multi-modal fusion components. The MFS-HVE semantic feature extractors are developed to extract both textual and visual features. The visual features include global image features and local object features within the image. The MFS-HVE multi-modal fusion unit integrates information from various modalities using image-guided attention, object-guided attentio",
    "path": "papers/24/03/2403.00724.json",
    "total_tokens": 805,
    "translated_title": "基于混合视觉证据的少样本关系抽取",
    "translated_abstract": "少样本关系抽取的目标是在只有少量标记实例可供训练时，预测句子中命名实体之间的关系。现有的少样本关系抽取方法主要关注单一模态信息，例如仅文本。这会降低性能，当文本中描述的命名实体之间没有明确的上下文时。我们提出了一种多模态少样本关系抽取模型(MFS-HVE)，该模型利用文本和视觉语义信息共同学习多模态表示。MFS-HVE包括语义特征提取器和多模态融合组件。MFS-HVE语义特征提取器被设计用于提取文本和视觉特征。视觉特征包括全局图像特征和图像内的局部物体特征。MFS-HVE多模态融合单元利用图像引导注意力和目标引导注意力集成来自各种模态的信息。",
    "tldr": "提出了一种利用文本和视觉语义信息联合学习多模态表示的多模态少样本关系抽取模型(MFS-HVE)。"
}