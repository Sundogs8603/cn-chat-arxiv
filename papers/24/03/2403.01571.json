{
    "title": "Limits to classification performance by relating Kullback-Leibler divergence to Cohen's Kappa",
    "abstract": "arXiv:2403.01571v1 Announce Type: cross  Abstract: The performance of machine learning classification algorithms are evaluated by estimating metrics, often from the confusion matrix, using training data and cross-validation. However, these do not prove that the best possible performance has been achieved. Fundamental limits to error rates can be estimated using information distance measures. To this end, the confusion matrix has been formulated to comply with the Chernoff-Stein Lemma. This links the error rates to the Kullback-Leibler divergences between the probability density functions describing the two classes. This leads to a key result that relates Cohen's Kappa to the Resistor Average Distance which is the parallel resistor combination of the two Kullback-Leibler divergences. The Resistor Average Distance has units of bits and is estimated from the same training data used by the classification algorithm, using kNN estimates of the KullBack-Leibler divergences. The classification",
    "link": "https://arxiv.org/abs/2403.01571",
    "context": "Title: Limits to classification performance by relating Kullback-Leibler divergence to Cohen's Kappa\nAbstract: arXiv:2403.01571v1 Announce Type: cross  Abstract: The performance of machine learning classification algorithms are evaluated by estimating metrics, often from the confusion matrix, using training data and cross-validation. However, these do not prove that the best possible performance has been achieved. Fundamental limits to error rates can be estimated using information distance measures. To this end, the confusion matrix has been formulated to comply with the Chernoff-Stein Lemma. This links the error rates to the Kullback-Leibler divergences between the probability density functions describing the two classes. This leads to a key result that relates Cohen's Kappa to the Resistor Average Distance which is the parallel resistor combination of the two Kullback-Leibler divergences. The Resistor Average Distance has units of bits and is estimated from the same training data used by the classification algorithm, using kNN estimates of the KullBack-Leibler divergences. The classification",
    "path": "papers/24/03/2403.01571.json",
    "total_tokens": 793,
    "translated_title": "将Kullback-Leibler散度与Cohen's Kappa相关联，限制分类性能",
    "translated_abstract": "机器学习分类算法的性能是通过估计指标来评估的，通常是从混淆矩阵中使用训练数据和交叉验证得出的。然而，这些并不证明已经实现了最佳性能。可以使用信息距离度量来估计错误率的基本限制。为此，混淆矩阵已被制定为符合Chernoff-Stein引理。这将错误率与描述两个类别的概率密度函数之间的Kullback-Leibler散度相关联。这导致了一个关键结果，将Cohen's Kappa与电阻器平均距离联系起来，这是两个Kullback-Leibler散度的并联电阻器组合。电阻器平均距离具有比特单位，可以从同一训练数据中使用分类算法估计的KullBack-Leibler散度的kNN估计中得出。",
    "tldr": "通过将Kullback-Leibler散度与Cohen's Kappa相关联，限制了分类性能的最大限度",
    "en_tdlr": "The classification performance limits are constrained by relating Kullback-Leibler divergence to Cohen's Kappa."
}