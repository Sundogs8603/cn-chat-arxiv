{
    "title": "An Intermediate Fusion ViT Enables Efficient Text-Image Alignment in Diffusion Models",
    "abstract": "arXiv:2403.16530v1 Announce Type: cross  Abstract: Diffusion models have been widely used for conditional data cross-modal generation tasks such as text-to-image and text-to-video. However, state-of-the-art models still fail to align the generated visual concepts with high-level semantics in a language such as object count, spatial relationship, etc. We approach this problem from a multimodal data fusion perspective and investigate how different fusion strategies can affect vision-language alignment. We discover that compared to the widely used early fusion of conditioning text in a pretrained image feature space, a specially designed intermediate fusion can: (i) boost text-to-image alignment with improved generation quality and (ii) improve training and inference efficiency by reducing low-rank text-to-image attention calculations. We perform experiments using a text-to-image generation task on the MS-COCO dataset. We compare our intermediate fusion mechanism with the classic early fu",
    "link": "https://arxiv.org/abs/2403.16530",
    "context": "Title: An Intermediate Fusion ViT Enables Efficient Text-Image Alignment in Diffusion Models\nAbstract: arXiv:2403.16530v1 Announce Type: cross  Abstract: Diffusion models have been widely used for conditional data cross-modal generation tasks such as text-to-image and text-to-video. However, state-of-the-art models still fail to align the generated visual concepts with high-level semantics in a language such as object count, spatial relationship, etc. We approach this problem from a multimodal data fusion perspective and investigate how different fusion strategies can affect vision-language alignment. We discover that compared to the widely used early fusion of conditioning text in a pretrained image feature space, a specially designed intermediate fusion can: (i) boost text-to-image alignment with improved generation quality and (ii) improve training and inference efficiency by reducing low-rank text-to-image attention calculations. We perform experiments using a text-to-image generation task on the MS-COCO dataset. We compare our intermediate fusion mechanism with the classic early fu",
    "path": "papers/24/03/2403.16530.json",
    "total_tokens": 851,
    "translated_title": "一种中间融合的ViT在扩散模型中实现了文本-图像的高效对齐",
    "translated_abstract": "扩散模型被广泛应用于条件数据跨模态生成任务，如文本到图像和文本到视频。然而，最先进的模型仍然未能将生成的视觉概念与高级语义（如物体数量、空间关系等）进行对齐。我们从多模态数据融合的角度来解决这个问题，并研究了不同融合策略如何影响视觉-语言对齐。我们发现，与在预训练图像特征空间中广泛使用的早期融合相比，一种特别设计的中间融合可以：(i) 提高文本到图像对齐的生成质量；(ii) 通过减少低秩文本到图像注意力计算来提高训练和推理效率。我们在MS-COCO数据集上进行了文本到图像生成任务的实验。我们将我们的中间融合机制与经典的早期融合机制进行了比较。",
    "tldr": "通过一种中间融合的策略，可以提高文本到图像对齐的生成质量，并通过减少低秩文本到图像注意力计算来提高训练和推理效率。"
}