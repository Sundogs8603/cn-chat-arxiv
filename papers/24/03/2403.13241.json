{
    "title": "Tackling Noisy Labels with Network Parameter Additive Decomposition",
    "abstract": "arXiv:2403.13241v1 Announce Type: new  Abstract: Given data with noisy labels, over-parameterized deep networks suffer overfitting mislabeled data, resulting in poor generalization. The memorization effect of deep networks shows that although the networks have the ability to memorize all noisy data, they would first memorize clean training data, and then gradually memorize mislabeled training data. A simple and effective method that exploits the memorization effect to combat noisy labels is early stopping. However, early stopping cannot distinguish the memorization of clean data and mislabeled data, resulting in the network still inevitably overfitting mislabeled data in the early training stage.In this paper, to decouple the memorization of clean data and mislabeled data, and further reduce the side effect of mislabeled data, we perform additive decomposition on network parameters. Namely, all parameters are additively decomposed into two groups, i.e., parameters $\\mathbf{w}$ are deco",
    "link": "https://arxiv.org/abs/2403.13241",
    "context": "Title: Tackling Noisy Labels with Network Parameter Additive Decomposition\nAbstract: arXiv:2403.13241v1 Announce Type: new  Abstract: Given data with noisy labels, over-parameterized deep networks suffer overfitting mislabeled data, resulting in poor generalization. The memorization effect of deep networks shows that although the networks have the ability to memorize all noisy data, they would first memorize clean training data, and then gradually memorize mislabeled training data. A simple and effective method that exploits the memorization effect to combat noisy labels is early stopping. However, early stopping cannot distinguish the memorization of clean data and mislabeled data, resulting in the network still inevitably overfitting mislabeled data in the early training stage.In this paper, to decouple the memorization of clean data and mislabeled data, and further reduce the side effect of mislabeled data, we perform additive decomposition on network parameters. Namely, all parameters are additively decomposed into two groups, i.e., parameters $\\mathbf{w}$ are deco",
    "path": "papers/24/03/2403.13241.json",
    "total_tokens": 871,
    "translated_title": "使用网络参数附加分解解决有嘈杂标签问题",
    "translated_abstract": "考虑到具有嘈杂标签的数据，过参性深度网络会因为过度拟合错误标记的数据而导致泛化能力不佳。深度网络的记忆效应表明，尽管网络能够记忆所有嘈杂数据，但它们首先会记忆干净的训练数据，然后逐渐记忆错误标记的训练数据。一种利用记忆效应来对抗嘈杂标签的简单有效方法是早停止。然而，早停止无法区分对干净数据和错误标记数据的记忆，导致网络仍然在早期训练阶段不可避免地过度拟合错误标记的数据。在本文中，为了解耦干净数据和错误标记数据的记忆，并进一步减少错误标记数据的副作用，我们对网络参数进行了附加分解。即，将所有参数分解为两组，即参数 $\\mathbf{w}$ 被分开始解",
    "tldr": "本论文提出了一种使用网络参数附加分解来解耦干净数据和错误标记数据的记忆，从而减少嘈杂标签对深度网络训练的副作用。",
    "en_tdlr": "This paper proposes a method of using network parameter additive decomposition to decouple the memorization of clean data and mislabeled data, thus reducing the side effects of noisy labels on deep network training."
}