{
    "title": "Towards Multimodal Human Intention Understanding Debiasing via Subject-Deconfounding",
    "abstract": "arXiv:2403.05025v1 Announce Type: new  Abstract: Multimodal intention understanding (MIU) is an indispensable component of human expression analysis (e.g., sentiment or humor) from heterogeneous modalities, including visual postures, linguistic contents, and acoustic behaviors. Existing works invariably focus on designing sophisticated structures or fusion strategies to achieve impressive improvements. Unfortunately, they all suffer from the subject variation problem due to data distribution discrepancies among subjects. Concretely, MIU models are easily misled by distinct subjects with different expression customs and characteristics in the training data to learn subject-specific spurious correlations, significantly limiting performance and generalizability across uninitiated subjects.Motivated by this observation, we introduce a recapitulative causal graph to formulate the MIU procedure and analyze the confounding effect of subjects. Then, we propose SuCI, a simple yet effective caus",
    "link": "https://arxiv.org/abs/2403.05025",
    "context": "Title: Towards Multimodal Human Intention Understanding Debiasing via Subject-Deconfounding\nAbstract: arXiv:2403.05025v1 Announce Type: new  Abstract: Multimodal intention understanding (MIU) is an indispensable component of human expression analysis (e.g., sentiment or humor) from heterogeneous modalities, including visual postures, linguistic contents, and acoustic behaviors. Existing works invariably focus on designing sophisticated structures or fusion strategies to achieve impressive improvements. Unfortunately, they all suffer from the subject variation problem due to data distribution discrepancies among subjects. Concretely, MIU models are easily misled by distinct subjects with different expression customs and characteristics in the training data to learn subject-specific spurious correlations, significantly limiting performance and generalizability across uninitiated subjects.Motivated by this observation, we introduce a recapitulative causal graph to formulate the MIU procedure and analyze the confounding effect of subjects. Then, we propose SuCI, a simple yet effective caus",
    "path": "papers/24/03/2403.05025.json",
    "total_tokens": 885,
    "translated_title": "通过主题去相关实现多模态人类意图理解去偏见",
    "translated_abstract": "arXiv:2403.05025v1 公告类型: 新摘要: 多模态意图理解(MIU)是人类表达分析(例如情感或幽默)不可或缺的组成部分，涉及视觉姿势、语言内容和声学行为等异构模态。现有工作始终专注于设计复杂的结构或融合策略，取得显著进展。然而，它们都受到主题变异问题的困扰，因为不同主题之间的数据分布差异导致。具体而言，由于训练数据中具有不同表达习惯和特征的不同主题，MIU模型很容易被误导，以学习特定于主题的伪相关性，从而显着限制了跨未接触主题的性能和泛化能力。受这一观察启发，我们引入了一个概括性因果图来制定MIU过程，并分析主题的混淆效应。然后，我们提出了SuCI，一个简单而有效的因果",
    "tldr": "通过引入概括性因果图和分析主题混淆效应，本文提出了SuCI，实现了多模态人类意图理解的去偏见，解决了MIU模型受主体变异问题困扰的挑战。",
    "en_tdlr": "By introducing a recapitulative causal graph and analyzing the confounding effect of subjects, this paper proposes SuCI to achieve debiasing in multimodal human intention understanding, addressing the challenge of subject variation problem in MIU models."
}