{
    "title": "Poisoning Programs by Un-Repairing Code: Security Concerns of AI-generated Code",
    "abstract": "arXiv:2403.06675v1 Announce Type: cross  Abstract: AI-based code generators have gained a fundamental role in assisting developers in writing software starting from natural language (NL). However, since these large language models are trained on massive volumes of data collected from unreliable online sources (e.g., GitHub, Hugging Face), AI models become an easy target for data poisoning attacks, in which an attacker corrupts the training data by injecting a small amount of poison into it, i.e., astutely crafted malicious samples. In this position paper, we address the security of AI code generators by identifying a novel data poisoning attack that results in the generation of vulnerable code. Next, we devise an extensive evaluation of how these attacks impact state-of-the-art models for code generation. Lastly, we discuss potential solutions to overcome this threat.",
    "link": "https://arxiv.org/abs/2403.06675",
    "context": "Title: Poisoning Programs by Un-Repairing Code: Security Concerns of AI-generated Code\nAbstract: arXiv:2403.06675v1 Announce Type: cross  Abstract: AI-based code generators have gained a fundamental role in assisting developers in writing software starting from natural language (NL). However, since these large language models are trained on massive volumes of data collected from unreliable online sources (e.g., GitHub, Hugging Face), AI models become an easy target for data poisoning attacks, in which an attacker corrupts the training data by injecting a small amount of poison into it, i.e., astutely crafted malicious samples. In this position paper, we address the security of AI code generators by identifying a novel data poisoning attack that results in the generation of vulnerable code. Next, we devise an extensive evaluation of how these attacks impact state-of-the-art models for code generation. Lastly, we discuss potential solutions to overcome this threat.",
    "path": "papers/24/03/2403.06675.json",
    "total_tokens": 851,
    "translated_title": "通过不修复代码对程序进行毒化：AI生成的代码的安全问题",
    "translated_abstract": "基于AI的代码生成器在协助开发人员从自然语言（NL）开始编写软件方面发挥了根本作用。然而，由于这些大型语言模型是在从不可靠的在线来源（例如GitHub，Hugging Face）收集的大量数据上进行训练的，AI模型成为数据毒化攻击的容易目标，即攻击者通过向训练数据中注入少量毒素（即巧妙制作的恶意样本）来破坏训练数据。在这篇立场论文中，我们通过识别一种导致生成易受攻击的代码的新型数据毒化攻击来讨论AI代码生成器的安全性。接下来，我们对这些攻击如何影响用于代码生成的最新模型进行了广泛评估。最后，我们讨论了解决这一威胁的潜在解决方案。",
    "tldr": "本文章针对AI代码生成器面临的安全挑战，提出了一种新颖的数据毒化攻击，通过向训练数据中注入毒素来生成易受攻击的代码，并对其对代码生成模型的影响进行了评估，并讨论了潜在的解决方案。",
    "en_tdlr": "This paper discusses the security challenges of AI code generators, introduces a novel data poisoning attack to generate vulnerable code by injecting poison into the training data, evaluates its impact on models for code generation, and discusses potential solutions to overcome this threat."
}