{
    "title": "A Teacher-Free Graph Knowledge Distillation Framework with Dual Self-Distillation",
    "abstract": "arXiv:2403.03483v1 Announce Type: new  Abstract: Recent years have witnessed great success in handling graph-related tasks with Graph Neural Networks (GNNs). Despite their great academic success, Multi-Layer Perceptrons (MLPs) remain the primary workhorse for practical industrial applications. One reason for such an academic-industry gap is the neighborhood-fetching latency incurred by data dependency in GNNs. To reduce their gaps, Graph Knowledge Distillation (GKD) is proposed, usually based on a standard teacher-student architecture, to distill knowledge from a large teacher GNN into a lightweight student GNN or MLP. However, we found in this paper that neither teachers nor GNNs are necessary for graph knowledge distillation. We propose a Teacher-Free Graph Self-Distillation (TGS) framework that does not require any teacher model or GNNs during both training and inference. More importantly, the proposed TGS framework is purely based on MLPs, where structural information is only impli",
    "link": "https://arxiv.org/abs/2403.03483",
    "context": "Title: A Teacher-Free Graph Knowledge Distillation Framework with Dual Self-Distillation\nAbstract: arXiv:2403.03483v1 Announce Type: new  Abstract: Recent years have witnessed great success in handling graph-related tasks with Graph Neural Networks (GNNs). Despite their great academic success, Multi-Layer Perceptrons (MLPs) remain the primary workhorse for practical industrial applications. One reason for such an academic-industry gap is the neighborhood-fetching latency incurred by data dependency in GNNs. To reduce their gaps, Graph Knowledge Distillation (GKD) is proposed, usually based on a standard teacher-student architecture, to distill knowledge from a large teacher GNN into a lightweight student GNN or MLP. However, we found in this paper that neither teachers nor GNNs are necessary for graph knowledge distillation. We propose a Teacher-Free Graph Self-Distillation (TGS) framework that does not require any teacher model or GNNs during both training and inference. More importantly, the proposed TGS framework is purely based on MLPs, where structural information is only impli",
    "path": "papers/24/03/2403.03483.json",
    "total_tokens": 793,
    "translated_title": "一种双自蒸馏的无教师图知识蒸馏框架",
    "translated_abstract": "近年来，使用图神经网络（GNNs）处理与图相关的任务取得了巨大成功。尽管在学术上取得了巨大成功，但多层感知器（MLPs）仍然是实际工业应用的主要工具。本文发现，无论是教师还是GNN对图知识蒸馏都不是必需的。我们提出了一种无教师图自蒸馏（TGS）框架，既不需要教师模型也不需要GNNs，在训练和推理过程中都不需要。更重要的是，所提出的TGS框架纯粹基于MLP，其中结构信息仅被暗示。",
    "tldr": "本文提出了一种无教师图自蒸馏框架，不需要教师模型或GNN，仅基于MLP，有助于缩小学术和工业之间的差距。",
    "en_tdlr": "This paper introduces a teacher-free graph self-distillation framework that does not require teacher models or GNNs, purely based on MLPs, helping to bridge the gap between academia and industry."
}