{
    "title": "LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images",
    "abstract": "arXiv:2403.11703v1 Announce Type: cross  Abstract: Visual encoding constitutes the basis of large multimodal models (LMMs) in understanding the visual world. Conventional LMMs process images in fixed sizes and limited resolutions, while recent explorations in this direction are limited in adaptivity, efficiency, and even correctness. In this work, we first take GPT-4V and LLaVA-1.5 as representative examples and expose systematic flaws rooted in their visual encoding strategy. To address the challenges, we present LLaVA-UHD, a large multimodal model that can efficiently perceive images in any aspect ratio and high resolution. LLaVA-UHD includes three key components: (1) An image modularization strategy that divides native-resolution images into smaller variable-sized slices for efficient and extensible encoding, (2) a compression module that further condenses image tokens from visual encoders, and (3) a spatial schema to organize slice tokens for LLMs. Comprehensive experiments show th",
    "link": "https://arxiv.org/abs/2403.11703",
    "context": "Title: LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images\nAbstract: arXiv:2403.11703v1 Announce Type: cross  Abstract: Visual encoding constitutes the basis of large multimodal models (LMMs) in understanding the visual world. Conventional LMMs process images in fixed sizes and limited resolutions, while recent explorations in this direction are limited in adaptivity, efficiency, and even correctness. In this work, we first take GPT-4V and LLaVA-1.5 as representative examples and expose systematic flaws rooted in their visual encoding strategy. To address the challenges, we present LLaVA-UHD, a large multimodal model that can efficiently perceive images in any aspect ratio and high resolution. LLaVA-UHD includes three key components: (1) An image modularization strategy that divides native-resolution images into smaller variable-sized slices for efficient and extensible encoding, (2) a compression module that further condenses image tokens from visual encoders, and (3) a spatial schema to organize slice tokens for LLMs. Comprehensive experiments show th",
    "path": "papers/24/03/2403.11703.json",
    "total_tokens": 964,
    "translated_title": "LLaVA-UHD：一个能感知任意长宽比和高分辨率图像的LMM",
    "translated_abstract": "arXiv:2403.11703v1 公告类型：跨文摘要：视觉编码构成了大型多模态模型（LMM）理解视觉世界的基础。传统的 LMM 处理固定大小和有限分辨率的图像，而最近在这个方向上的探索在适应性、效率甚至正确性方面有所限制。在这项工作中，我们首先以 GPT-4V 和 LLaVA-1.5 为代表例，并揭示了根植于它们的视觉编码策略中的系统性缺陷。为了解决这些挑战，我们提出了LLaVA-UHD，这是一个大型多模态模型，能够高效地感知任意长宽比和高分辨率的图像。LLaVA-UHD 包括三个关键组件：(1)一种图像模块化策略，将原始分辨率的图像分成更小的可变大小片段，以便进行高效和可扩展的编码，(2)一个压缩模块，进一步压缩来自视觉编码器的图像令牌，以及(3)一个用于为 LLMs 组织片段令牌的空间模式。全面的实验表明LLaVA-UHD",
    "tldr": "提出了LLaVA-UHD，一个大型多模态模型，通过图像模块化策略、压缩模块和空间模式，可以高效地感知任意长宽比和高分辨率的图像"
}