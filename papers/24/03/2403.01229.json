{
    "title": "REWIND Dataset: Privacy-preserving Speaking Status Segmentation from Multimodal Body Movement Signals in the Wild",
    "abstract": "arXiv:2403.01229v1 Announce Type: cross  Abstract: Recognizing speaking in humans is a central task towards understanding social interactions. Ideally, speaking would be detected from individual voice recordings, as done previously for meeting scenarios. However, individual voice recordings are hard to obtain in the wild, especially in crowded mingling scenarios due to cost, logistics, and privacy concerns. As an alternative, machine learning models trained on video and wearable sensor data make it possible to recognize speech by detecting its related gestures in an unobtrusive, privacy-preserving way. These models themselves should ideally be trained using labels obtained from the speech signal. However, existing mingling datasets do not contain high quality audio recordings. Instead, speaking status annotations have often been inferred by human annotators from video, without validation of this approach against audio-based ground truth. In this paper we revisit no-audio speaking statu",
    "link": "https://arxiv.org/abs/2403.01229",
    "context": "Title: REWIND Dataset: Privacy-preserving Speaking Status Segmentation from Multimodal Body Movement Signals in the Wild\nAbstract: arXiv:2403.01229v1 Announce Type: cross  Abstract: Recognizing speaking in humans is a central task towards understanding social interactions. Ideally, speaking would be detected from individual voice recordings, as done previously for meeting scenarios. However, individual voice recordings are hard to obtain in the wild, especially in crowded mingling scenarios due to cost, logistics, and privacy concerns. As an alternative, machine learning models trained on video and wearable sensor data make it possible to recognize speech by detecting its related gestures in an unobtrusive, privacy-preserving way. These models themselves should ideally be trained using labels obtained from the speech signal. However, existing mingling datasets do not contain high quality audio recordings. Instead, speaking status annotations have often been inferred by human annotators from video, without validation of this approach against audio-based ground truth. In this paper we revisit no-audio speaking statu",
    "path": "papers/24/03/2403.01229.json",
    "total_tokens": 858,
    "translated_title": "REWIND数据集：在野外多模态身体运动信号中隐私保护的语音状态分割",
    "translated_abstract": "识别人类的说话是理解社会互动的一个核心任务。通常情况下，会从个人录音中检测说话，就像之前为会议场景所做的那样。然而，在拥挤的聚会场景中，由于成本、后勤和隐私问题，很难获取个人录音。作为一种替代方案，通过训练在视频和可穿戴传感器数据上的机器学习模型可以实现通过检测其相关手势来识别语音，这种方式既不引人注目又保护隐私。然而，这些模型本身理想情况下应该使用从语音信号中获取的标签进行训练。然而，现有的聚会数据集中没有包含高质量的音频记录。相反，对说话状态的注释通常是通过人类标注者从视频中推断出来的，而没有对这种方法针对基于音频的地面真实性进行验证。本文重新审视了非音频说话状态标签",
    "tldr": "通过视频和可穿戴传感器数据训练的机器学习模型可以隐私保护地识别说话状态，解决了在野外获取个人录音困难的问题",
    "en_tdlr": "Machine learning models trained on video and wearable sensor data enable privacy-preserving speech recognition, addressing the challenge of obtaining individual voice recordings in the wild."
}