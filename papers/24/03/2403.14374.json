{
    "title": "FIT-RAG: Black-Box RAG with Factual Information and Token Reduction",
    "abstract": "arXiv:2403.14374v1 Announce Type: new  Abstract: Due to the extraordinarily large number of parameters, fine-tuning Large Language Models (LLMs) to update long-tail or out-of-date knowledge is impractical in lots of applications. To avoid fine-tuning, we can alternatively treat a LLM as a black-box (i.e., freeze the parameters of the LLM) and augment it with a Retrieval-Augmented Generation (RAG) system, namely black-box RAG. Recently, black-box RAG has achieved success in knowledge-intensive tasks and has gained much attention. Existing black-box RAG methods typically fine-tune the retriever to cater to LLMs' preferences and concatenate all the retrieved documents as the input, which suffers from two issues: (1) Ignorance of Factual Information. The LLM preferred documents may not contain the factual information for the given question, which can mislead the retriever and hurt the effectiveness of black-box RAG; (2) Waste of Tokens. Simply concatenating all the retrieved documents brin",
    "link": "https://arxiv.org/abs/2403.14374",
    "context": "Title: FIT-RAG: Black-Box RAG with Factual Information and Token Reduction\nAbstract: arXiv:2403.14374v1 Announce Type: new  Abstract: Due to the extraordinarily large number of parameters, fine-tuning Large Language Models (LLMs) to update long-tail or out-of-date knowledge is impractical in lots of applications. To avoid fine-tuning, we can alternatively treat a LLM as a black-box (i.e., freeze the parameters of the LLM) and augment it with a Retrieval-Augmented Generation (RAG) system, namely black-box RAG. Recently, black-box RAG has achieved success in knowledge-intensive tasks and has gained much attention. Existing black-box RAG methods typically fine-tune the retriever to cater to LLMs' preferences and concatenate all the retrieved documents as the input, which suffers from two issues: (1) Ignorance of Factual Information. The LLM preferred documents may not contain the factual information for the given question, which can mislead the retriever and hurt the effectiveness of black-box RAG; (2) Waste of Tokens. Simply concatenating all the retrieved documents brin",
    "path": "papers/24/03/2403.14374.json",
    "total_tokens": 850,
    "translated_title": "FIT-RAG: 具有事实信息和令牌减少的黑匣子RAG",
    "translated_abstract": "由于参数数量庞大，对大型语言模型（LLMs）进行微调以更新长尾或过时知识在许多应用中都是不切实际的。为了避免微调，我们可以将LLM视为一个黑匣子（即，冻结LLM的参数）并增加一个检索增强生成（RAG）系统，即黑匣子RAG。最近，黑匣子RAG在知识密集型任务中取得了成功并引起了广泛关注。现有的黑匣子RAG方法通常微调检索器以迎合LLMs的偏好，并将所有检索到的文档串联在一起作为输入，但存在两个问题:（1）对事实信息的忽视。LLM偏好的文档可能不包含给定问题的事实信息，这可能会误导检索器，并损害黑匣子RAG的有效性;（2）令牌的浪费。简单地将所有检索到的文档串联在一起会带来...",
    "tldr": "FIT-RAG使用事实信息和令牌减少解决了黑匣子RAG系统中存在的事实信息忽视和令牌浪费问题",
    "en_tdlr": "FIT-RAG addresses the issues of factual information ignorance and token waste in black-box RAG systems by incorporating factual information and reducing tokens."
}