{
    "title": "Can Your Model Tell a Negation from an Implicature? Unravelling Challenges With Intent Encoders",
    "abstract": "arXiv:2403.04314v1 Announce Type: new  Abstract: Conversational systems often rely on embedding models for intent classification and intent clustering tasks. The advent of Large Language Models (LLMs), which enable instructional embeddings allowing one to adjust semantics over the embedding space using prompts, are being viewed as a panacea for these downstream conversational tasks. However, traditional evaluation benchmarks rely solely on task metrics that don't particularly measure gaps related to semantic understanding. Thus, we propose an intent semantic toolkit that gives a more holistic view of intent embedding models by considering three tasks-- (1) intent classification, (2) intent clustering, and (3) a novel triplet task. The triplet task gauges the model's understanding of two semantic concepts paramount in real-world conversational systems-- negation and implicature. We observe that current embedding models fare poorly in semantic understanding of these concepts. To address ",
    "link": "https://arxiv.org/abs/2403.04314",
    "context": "Title: Can Your Model Tell a Negation from an Implicature? Unravelling Challenges With Intent Encoders\nAbstract: arXiv:2403.04314v1 Announce Type: new  Abstract: Conversational systems often rely on embedding models for intent classification and intent clustering tasks. The advent of Large Language Models (LLMs), which enable instructional embeddings allowing one to adjust semantics over the embedding space using prompts, are being viewed as a panacea for these downstream conversational tasks. However, traditional evaluation benchmarks rely solely on task metrics that don't particularly measure gaps related to semantic understanding. Thus, we propose an intent semantic toolkit that gives a more holistic view of intent embedding models by considering three tasks-- (1) intent classification, (2) intent clustering, and (3) a novel triplet task. The triplet task gauges the model's understanding of two semantic concepts paramount in real-world conversational systems-- negation and implicature. We observe that current embedding models fare poorly in semantic understanding of these concepts. To address ",
    "path": "papers/24/03/2403.04314.json",
    "total_tokens": 849,
    "translated_title": "您的模型能分辨否定和含义推断吗？揭示意图编码器的挑战",
    "translated_abstract": "论文摘要：对话系统通常依赖于嵌入模型进行意图分类和意图聚类任务。大型语言模型（LLM）的出现使得调整嵌入空间上的语义成为可能，通过提示来进行。然而，传统评估基准仅依赖于任务度量，不能很好地衡量与语义理解有关的差距。因此，我们提出了一个意图语义工具包，通过考虑意图嵌入模型的三个任务（1）意图分类，（2）意图聚类，以及（3）一个新颖的三元任务，来更全面地展示意图嵌入模型。三元任务评估模型对实际对话系统中至关重要的两个语义概念--否定和含义推断的理解情况。我们观察到当前的嵌入模型在这些概念的语义理解方面表现不佳。为了解决这一问题",
    "tldr": "该论文提出了一个意图语义工具包，通过引入三元任务来评估模型对于否定和含义推断这两个实际对话系统中重要的语义概念的理解情况",
    "en_tdlr": "This paper introduces an intent semantic toolkit that evaluates the model's understanding of the critical semantic concepts of negation and implicature in real-world conversational systems through a triplet task."
}