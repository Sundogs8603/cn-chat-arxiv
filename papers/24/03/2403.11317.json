{
    "title": "Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches",
    "abstract": "arXiv:2403.11317v1 Announce Type: new  Abstract: Two approaches have emerged to input images into large language models (LLMs). The first is to caption images into natural language. The second is to map image feature embeddings into the domain of the LLM and pass the mapped embeddings directly to the LLM. The majority of recent few-shot multimodal work reports performance using architectures that employ variations of one of these two approaches. But they overlook an important comparison between them. We design a controlled and focused experiment to compare these two approaches to few-shot visual question answering (VQA) with LLMs. Our findings indicate that for Flan-T5 XL, a 3B parameter LLM, connecting visual embeddings directly to the LLM embedding space does not guarantee improved performance over using image captions. In the zero-shot regime, we find using textual image captions is better. In the few-shot regimes, how the in-context examples are selected determines which is better.",
    "link": "https://arxiv.org/abs/2403.11317",
    "context": "Title: Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches\nAbstract: arXiv:2403.11317v1 Announce Type: new  Abstract: Two approaches have emerged to input images into large language models (LLMs). The first is to caption images into natural language. The second is to map image feature embeddings into the domain of the LLM and pass the mapped embeddings directly to the LLM. The majority of recent few-shot multimodal work reports performance using architectures that employ variations of one of these two approaches. But they overlook an important comparison between them. We design a controlled and focused experiment to compare these two approaches to few-shot visual question answering (VQA) with LLMs. Our findings indicate that for Flan-T5 XL, a 3B parameter LLM, connecting visual embeddings directly to the LLM embedding space does not guarantee improved performance over using image captions. In the zero-shot regime, we find using textual image captions is better. In the few-shot regimes, how the in-context examples are selected determines which is better.",
    "path": "papers/24/03/2403.11317.json",
    "total_tokens": 948,
    "translated_title": "冻结LLMs的少样本VQA：两种方法的故事",
    "translated_abstract": "两种输入图像到大型语言模型（LLMs）的方法已经出现。第一种是将图像描述成自然语言。第二种是将图像特征嵌入映射到LLMs的领域，并直接将映射后的嵌入传递给LLMs。大多数最近的少样本多模态工作使用了采用这两种方法变体的架构来报告性能。但它们忽视了它们之间的一个重要比较。我们设计了一个受控的、专注的实验来比较这两种方法在使用LLMs进行少样本视觉问答（VQA）方面的表现。我们的研究表明对于Flan-T5 XL，一个3B参数的LLM，将视觉嵌入直接连接到LLM嵌入空间不能保证比使用图像描述获得更好的性能。在零样本情况下，我们发现使用文本图像描述更好。在少样本情况下，所选上下文例子如何选择决定了哪种方法更好。",
    "tldr": "本论文对比了在使用LLMs进行少样本视觉问答（VQA）时，将视觉嵌入直接连接到LLM嵌入空间和使用图像描述两种方法的性能，发现对于特定LLM模型，在零样本情况下使用图像描述更好，在少样本情况下则取决于所选的上下文例子。",
    "en_tdlr": "This paper compares the performance of connecting visual embeddings directly to the LLM embedding space and using image captions in few-shot visual question answering (VQA) with LLMs, finding that for a specific LLM model, using image captions performs better in zero-shot scenarios, while in few-shot scenarios, the choice depends on the selected in-context examples."
}