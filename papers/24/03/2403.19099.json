{
    "title": "Optimizing Quantum Convolutional Neural Network Architectures for Arbitrary Data Dimension",
    "abstract": "arXiv:2403.19099v1 Announce Type: cross  Abstract: Quantum convolutional neural networks (QCNNs) represent a promising approach in quantum machine learning, paving new directions for both quantum and classical data analysis. This approach is particularly attractive due to the absence of the barren plateau problem, a fundamental challenge in training quantum neural networks (QNNs), and its feasibility. However, a limitation arises when applying QCNNs to classical data. The network architecture is most natural when the number of input qubits is a power of two, as this number is reduced by a factor of two in each pooling layer. The number of input qubits determines the dimensions (i.e. the number of features) of the input data that can be processed, restricting the applicability of QCNN algorithms to real-world data. To address this issue, we propose a QCNN architecture capable of handling arbitrary input data dimensions while optimizing the allocation of quantum resources such as ancilla",
    "link": "https://arxiv.org/abs/2403.19099",
    "context": "Title: Optimizing Quantum Convolutional Neural Network Architectures for Arbitrary Data Dimension\nAbstract: arXiv:2403.19099v1 Announce Type: cross  Abstract: Quantum convolutional neural networks (QCNNs) represent a promising approach in quantum machine learning, paving new directions for both quantum and classical data analysis. This approach is particularly attractive due to the absence of the barren plateau problem, a fundamental challenge in training quantum neural networks (QNNs), and its feasibility. However, a limitation arises when applying QCNNs to classical data. The network architecture is most natural when the number of input qubits is a power of two, as this number is reduced by a factor of two in each pooling layer. The number of input qubits determines the dimensions (i.e. the number of features) of the input data that can be processed, restricting the applicability of QCNN algorithms to real-world data. To address this issue, we propose a QCNN architecture capable of handling arbitrary input data dimensions while optimizing the allocation of quantum resources such as ancilla",
    "path": "papers/24/03/2403.19099.json",
    "total_tokens": 656,
    "translated_title": "为任意数据维度优化量子卷积神经网络架构",
    "translated_abstract": "Quantum Convolutional Neural Networks (QCNNs)代表了在量子机器学习中的一种有前途的方法，为量子和经典数据分析开辟了新的方向。然而，将QCNNs应用于经典数据时会出现限制。为解决这一问题，我们提出了一种能够处理任意输入数据维度的QCNN架构，同时优化了诸如辅助量子资源分配的功能。",
    "tldr": "优化量子卷积神经网络架构，使其能处理任意输入数据维度，并同时优化了辅助量子资源分配",
    "en_tdlr": "Optimizing quantum convolutional neural network architectures to handle arbitrary input data dimensions while optimizing the allocation of quantum resources"
}