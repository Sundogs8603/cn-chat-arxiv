{
    "title": "Automatic Annotation of Grammaticality in Child-Caregiver Conversations",
    "abstract": "arXiv:2403.14208v1 Announce Type: new  Abstract: The acquisition of grammar has been a central question to adjudicate between theories of language acquisition. In order to conduct faster, more reproducible, and larger-scale corpus studies on grammaticality in child-caregiver conversations, tools for automatic annotation can offer an effective alternative to tedious manual annotation. We propose a coding scheme for context-dependent grammaticality in child-caregiver conversations and annotate more than 4,000 utterances from a large corpus of transcribed conversations. Based on these annotations, we train and evaluate a range of NLP models. Our results show that fine-tuned Transformer-based models perform best, achieving human inter-annotation agreement levels.As a first application and sanity check of this tool, we use the trained models to annotate a corpus almost two orders of magnitude larger than the manually annotated data and verify that children's grammaticality shows a steady in",
    "link": "https://arxiv.org/abs/2403.14208",
    "context": "Title: Automatic Annotation of Grammaticality in Child-Caregiver Conversations\nAbstract: arXiv:2403.14208v1 Announce Type: new  Abstract: The acquisition of grammar has been a central question to adjudicate between theories of language acquisition. In order to conduct faster, more reproducible, and larger-scale corpus studies on grammaticality in child-caregiver conversations, tools for automatic annotation can offer an effective alternative to tedious manual annotation. We propose a coding scheme for context-dependent grammaticality in child-caregiver conversations and annotate more than 4,000 utterances from a large corpus of transcribed conversations. Based on these annotations, we train and evaluate a range of NLP models. Our results show that fine-tuned Transformer-based models perform best, achieving human inter-annotation agreement levels.As a first application and sanity check of this tool, we use the trained models to annotate a corpus almost two orders of magnitude larger than the manually annotated data and verify that children's grammaticality shows a steady in",
    "path": "papers/24/03/2403.14208.json",
    "total_tokens": 791,
    "translated_title": "儿童-看护者对话中语法正确性的自动注释",
    "translated_abstract": "文法的习得一直是语言习得理论之间的一项核心问题。为了在儿童-看护者对话中进行更快速、更可重复、更大规模的语法研究，自动注释工具可以为繁琐的手动注释提供有效的替代方法。我们提出了一个针对儿童-看护者对话中的上下文依赖语法的编码方案，并对大量转录对话语料库中的4000多个话语进行了注释。基于这些注释，我们训练和评估了一系列自然语言处理模型。我们的结果表明，经过微调的基于Transformer的模型表现最佳，达到了人类之间的注释一致性水平。",
    "tldr": "提出了针对儿童-看护者对话中上下文依赖语法的编码方案，并训练并评估了一系列自然语言处理模型，结果显示经过微调的Transformer模型表现最佳。",
    "en_tdlr": "Proposed a coding scheme for context-dependent grammaticality in child-caregiver conversations, trained and evaluated a range of NLP models, and found that fine-tuned Transformer-based models performed the best."
}