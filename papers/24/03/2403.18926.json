{
    "title": "Enhancing Efficiency in Sparse Models with Sparser Selection",
    "abstract": "arXiv:2403.18926v1 Announce Type: cross  Abstract: Sparse models, including sparse Mixture-of-Experts (MoE) models, have emerged as an effective approach for scaling Transformer models. However, they often suffer from computational inefficiency since a significant number of parameters are unnecessarily involved in computations via multiplying values by zero or low activation values. To address this issue, we present \\tool, a novel MoE designed to enhance both the efficacy and efficiency of sparse MoE models. \\tool leverages small experts and a threshold-based router to enable tokens to selectively engage only essential parameters. Our extensive experiments on language modeling and machine translation tasks demonstrate that \\tool can enhance model performance while decreasing the computation load at MoE layers by over 50\\% without sacrificing performance. Furthermore, we present the versatility of \\tool by applying it to dense models, enabling sparse computation during inference. We pro",
    "link": "https://arxiv.org/abs/2403.18926",
    "context": "Title: Enhancing Efficiency in Sparse Models with Sparser Selection\nAbstract: arXiv:2403.18926v1 Announce Type: cross  Abstract: Sparse models, including sparse Mixture-of-Experts (MoE) models, have emerged as an effective approach for scaling Transformer models. However, they often suffer from computational inefficiency since a significant number of parameters are unnecessarily involved in computations via multiplying values by zero or low activation values. To address this issue, we present \\tool, a novel MoE designed to enhance both the efficacy and efficiency of sparse MoE models. \\tool leverages small experts and a threshold-based router to enable tokens to selectively engage only essential parameters. Our extensive experiments on language modeling and machine translation tasks demonstrate that \\tool can enhance model performance while decreasing the computation load at MoE layers by over 50\\% without sacrificing performance. Furthermore, we present the versatility of \\tool by applying it to dense models, enabling sparse computation during inference. We pro",
    "path": "papers/24/03/2403.18926.json",
    "total_tokens": 864,
    "translated_title": "用更稀疏的选择提高稀疏模型的效率",
    "translated_abstract": "稀疏模型，包括稀疏的专家混合（MoE）模型，已经成为缩放Transformer模型的有效方法。然而，它们通常存在计算效率低的问题，因为大量参数通过将值乘以零或低激活值无谓参与计算。为了解决这一问题，我们提出了一种名为\\tool 的新颖MoE模型，旨在提升稀疏MoE模型的功效和效率。 \\tool 利用小型专家和基于阈值的路由器，使标记能够选择性地仅涉及到必要的参数。我们在语言建模和机器翻译任务上进行了大量实验，结果表明\\tool 可以在不牺牲性能的情况下，将MoE层的计算负载减少50\\%以上，同时提高模型性能。此外，我们展示了\\tool 的通用性，通过将其应用于密集模型，在推断期间实现稀疏计算。",
    "tldr": "提出了一种新颖的MoE模型\\tool，通过利用小型专家和基于阈值的路由器，使标记能够选择性地仅涉及到必要的参数，从而在减少MoE层计算负载50%以上的同时提高模型性能。",
    "en_tdlr": "Introduced a novel MoE model \\tool that utilizes small experts and threshold-based router to selectively engage only essential parameters for over 50% reduction in computation load at MoE layers with improved model performance."
}