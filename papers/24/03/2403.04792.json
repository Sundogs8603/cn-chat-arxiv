{
    "title": "Breaking the Language Barrier: Can Direct Inference Outperform Pre-Translation in Multilingual LLM Applications?",
    "abstract": "arXiv:2403.04792v1 Announce Type: new  Abstract: Large language models hold significant promise in multilingual applications. However, inherent biases stemming from predominantly English-centric pre-training have led to the widespread practice of pre-translation, i.e., translating non-English inputs to English before inference, leading to complexity and information loss. This study re-evaluates the need for pre-translation in the context of PaLM2 models (Anil et al., 2023), which have been established as highly performant in multilingual tasks. We offer a comprehensive investigation across 108 languages and 6 diverse benchmarks, including open-end generative tasks, which were excluded from previous similar studies. Our findings challenge the pre-translation paradigm established in prior research, highlighting the advantages of direct inference in PaLM2. Specifically, PaLM2-L consistently outperforms pre-translation in 94 out of 108 languages. These findings pave the way for more effici",
    "link": "https://arxiv.org/abs/2403.04792",
    "context": "Title: Breaking the Language Barrier: Can Direct Inference Outperform Pre-Translation in Multilingual LLM Applications?\nAbstract: arXiv:2403.04792v1 Announce Type: new  Abstract: Large language models hold significant promise in multilingual applications. However, inherent biases stemming from predominantly English-centric pre-training have led to the widespread practice of pre-translation, i.e., translating non-English inputs to English before inference, leading to complexity and information loss. This study re-evaluates the need for pre-translation in the context of PaLM2 models (Anil et al., 2023), which have been established as highly performant in multilingual tasks. We offer a comprehensive investigation across 108 languages and 6 diverse benchmarks, including open-end generative tasks, which were excluded from previous similar studies. Our findings challenge the pre-translation paradigm established in prior research, highlighting the advantages of direct inference in PaLM2. Specifically, PaLM2-L consistently outperforms pre-translation in 94 out of 108 languages. These findings pave the way for more effici",
    "path": "papers/24/03/2403.04792.json",
    "total_tokens": 912,
    "translated_title": "打破语言障碍：在多语言LLM应用中，直接推断能否胜过预翻译？",
    "translated_abstract": "大型语言模型在多语言应用中具有重要潜力。然而，由于主要以英文为中心的预训练会导致固有偏见，因此已经普遍采用预翻译的做法，即在推断之前将非英文输入翻译成英文，从而导致复杂性和信息丢失。本研究重新评估了在PaLM2模型中的预翻译需求（Anil等人，2023年），这些模型已被证明在多语言任务中表现出色。我们在108种语言和6个不同基准测试中进行了全面调查，包括开放式生成式任务，在此类任务中之前的研究中被排除在外。我们的发现挑战了以前研究中建立的预翻译范式，突出了在PaLM2中直接推断的优势。具体而言，PaLM2-L在108种语言中的94种中始终优于预翻译。这些发现为更高效的翻译方法铺平了道路。",
    "tldr": "本研究挑战了以往研究中建立的预翻译范式，并在108种语言中的94种语言中表明PaLM2-L在直接推断中优于预翻译。",
    "en_tdlr": "The study challenges the established pre-translation paradigm and demonstrates that PaLM2-L outperforms pre-translation in direct inference in 94 out of 108 languages."
}