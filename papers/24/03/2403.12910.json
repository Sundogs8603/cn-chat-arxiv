{
    "title": "Yell At Your Robot: Improving On-the-Fly from Language Corrections",
    "abstract": "arXiv:2403.12910v1 Announce Type: cross  Abstract: Hierarchical policies that combine language and low-level control have been shown to perform impressively long-horizon robotic tasks, by leveraging either zero-shot high-level planners like pretrained language and vision-language models (LLMs/VLMs) or models trained on annotated robotic demonstrations. However, for complex and dexterous skills, attaining high success rates on long-horizon tasks still represents a major challenge -- the longer the task is, the more likely it is that some stage will fail. Can humans help the robot to continuously improve its long-horizon task performance through intuitive and natural feedback? In this paper, we make the following observation: high-level policies that index into sufficiently rich and expressive low-level language-conditioned skills can be readily supervised with human feedback in the form of language corrections. We show that even fine-grained corrections, such as small movements (\"move a",
    "link": "https://arxiv.org/abs/2403.12910",
    "context": "Title: Yell At Your Robot: Improving On-the-Fly from Language Corrections\nAbstract: arXiv:2403.12910v1 Announce Type: cross  Abstract: Hierarchical policies that combine language and low-level control have been shown to perform impressively long-horizon robotic tasks, by leveraging either zero-shot high-level planners like pretrained language and vision-language models (LLMs/VLMs) or models trained on annotated robotic demonstrations. However, for complex and dexterous skills, attaining high success rates on long-horizon tasks still represents a major challenge -- the longer the task is, the more likely it is that some stage will fail. Can humans help the robot to continuously improve its long-horizon task performance through intuitive and natural feedback? In this paper, we make the following observation: high-level policies that index into sufficiently rich and expressive low-level language-conditioned skills can be readily supervised with human feedback in the form of language corrections. We show that even fine-grained corrections, such as small movements (\"move a",
    "path": "papers/24/03/2403.12910.json",
    "total_tokens": 798,
    "translated_title": "对您的机器人大喊：从语言纠正中实时改进",
    "translated_abstract": "合并语言和低级控制的分层策略已经被证明可以实现令人印象深刻的长视野机器人任务，通过利用预训练语言和视觉-语言模型（LLMs/VLMs）或在注释的机器人演示上训练的模型。但是，对于复杂和灵巧的技能，实现在长视野任务上高成功率仍然是一个主要挑战——任务越长，某个阶段失败的可能性就越大。人类可以通过直观自然的反馈帮助机器人持续改进其长视野任务性能吗？本文中我们发现：可以通过富含表达力的低级语言条件技能索引到高水平策略，并能够通过语言纠正的形式进行人类监督。我们表明，甚至精细的纠正，如小动作（“移动",
    "tldr": "该论文发现，通过人类提供的语言纠正，可以帮助机器人持续改进长久任务表现",
    "en_tdlr": "The paper demonstrates that continuous improvement of long-horizon task performance of robots can be achieved through human-provided language corrections."
}