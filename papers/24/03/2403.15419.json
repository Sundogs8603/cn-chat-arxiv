{
    "title": "Attention is all you need for boosting graph convolutional neural network",
    "abstract": "arXiv:2403.15419v1 Announce Type: new  Abstract: Graph Convolutional Neural Networks (GCNs) possess strong capabilities for processing graph data in non-grid domains. They can capture the topological logical structure and node features in graphs and integrate them into nodes' final representations. GCNs have been extensively studied in various fields, such as recommendation systems, social networks, and protein molecular structures. With the increasing application of graph neural networks, research has focused on improving their performance while compressing their size. In this work, a plug-in module named Graph Knowledge Enhancement and Distillation Module (GKEDM) is proposed. GKEDM can enhance node representations and improve the performance of GCNs by extracting and aggregating graph information via multi-head attention mechanism. Furthermore, GKEDM can serve as an auxiliary transferor for knowledge distillation. With a specially designed attention distillation method, GKEDM can dis",
    "link": "https://arxiv.org/abs/2403.15419",
    "context": "Title: Attention is all you need for boosting graph convolutional neural network\nAbstract: arXiv:2403.15419v1 Announce Type: new  Abstract: Graph Convolutional Neural Networks (GCNs) possess strong capabilities for processing graph data in non-grid domains. They can capture the topological logical structure and node features in graphs and integrate them into nodes' final representations. GCNs have been extensively studied in various fields, such as recommendation systems, social networks, and protein molecular structures. With the increasing application of graph neural networks, research has focused on improving their performance while compressing their size. In this work, a plug-in module named Graph Knowledge Enhancement and Distillation Module (GKEDM) is proposed. GKEDM can enhance node representations and improve the performance of GCNs by extracting and aggregating graph information via multi-head attention mechanism. Furthermore, GKEDM can serve as an auxiliary transferor for knowledge distillation. With a specially designed attention distillation method, GKEDM can dis",
    "path": "papers/24/03/2403.15419.json",
    "total_tokens": 805,
    "translated_title": "仅需注意力即可提升图卷积神经网络",
    "translated_abstract": "图卷积神经网络（GCNs）在处理非网格域的图数据方面具有强大的能力。它们能够捕捉图中的拓扑逻辑结构和节点特征，并将它们融合到节点的最终表示中。GCNs在推荐系统、社交网络和蛋白质分子结构等各个领域得到了广泛研究。本文提出了一种名为图知识增强和蒸馏模块（GKEDM）的插件模块。GKEDM可以通过多头注意力机制提取和聚合图信息，增强节点表示并提高GCN的性能。此外，GKEDM还可以作为知识蒸馏的辅助转移器。通过一种特殊设计的注意力蒸馏方法，GKEDM可以...",
    "tldr": "GKEDM插件模块通过多头注意力机制提取和聚合图信息，增强节点表示并提高GCN的性能，同时可以作为知识蒸馏的辅助转移器。",
    "en_tdlr": "The GKEDM plug-in module enhances node representations and improves the performance of GCNs by extracting and aggregating graph information via a multi-head attention mechanism, serving as an auxiliary transferor for knowledge distillation."
}