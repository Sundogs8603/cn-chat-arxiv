{
    "title": "LM4OPT: Unveiling the Potential of Large Language Models in Formulating Mathematical Optimization Problems",
    "abstract": "arXiv:2403.01342v1 Announce Type: new  Abstract: In the rapidly evolving field of natural language processing, the translation of linguistic descriptions into mathematical formulation of optimization problems presents a formidable challenge, demanding intricate understanding and processing capabilities from Large Language Models (LLMs). This study compares prominent LLMs, including GPT-3.5, GPT-4, and Llama-2-7b, in zero-shot and one-shot settings for this task. Our findings show GPT-4's superior performance, particularly in the one-shot scenario. A central part of this research is the introduction of `LM4OPT,' a progressive fine-tuning framework for Llama-2-7b that utilizes noisy embeddings and specialized datasets. However, this research highlights a notable gap in the contextual understanding capabilities of smaller models such as Llama-2-7b compared to larger counterparts, especially in processing lengthy and complex input contexts. Our empirical investigation, utilizing the NL4Opt",
    "link": "https://arxiv.org/abs/2403.01342",
    "context": "Title: LM4OPT: Unveiling the Potential of Large Language Models in Formulating Mathematical Optimization Problems\nAbstract: arXiv:2403.01342v1 Announce Type: new  Abstract: In the rapidly evolving field of natural language processing, the translation of linguistic descriptions into mathematical formulation of optimization problems presents a formidable challenge, demanding intricate understanding and processing capabilities from Large Language Models (LLMs). This study compares prominent LLMs, including GPT-3.5, GPT-4, and Llama-2-7b, in zero-shot and one-shot settings for this task. Our findings show GPT-4's superior performance, particularly in the one-shot scenario. A central part of this research is the introduction of `LM4OPT,' a progressive fine-tuning framework for Llama-2-7b that utilizes noisy embeddings and specialized datasets. However, this research highlights a notable gap in the contextual understanding capabilities of smaller models such as Llama-2-7b compared to larger counterparts, especially in processing lengthy and complex input contexts. Our empirical investigation, utilizing the NL4Opt",
    "path": "papers/24/03/2403.01342.json",
    "total_tokens": 912,
    "translated_title": "LM4OPT：揭示大型语言模型在制定数学优化问题中潜力",
    "translated_abstract": "在自然语言处理这一快速发展的领域中，将语言描述翻译成数学优化问题的数学公式是一项巨大挑战，要求大型语言模型（LLMs）具备复杂的理解和处理能力。本研究比较了几种知名的LLMs，包括GPT-3.5、GPT-4和Llama-2-7b，在零次和一次设置中对这一任务的表现。我们的发现显示出GPT-4在一次场景中的卓越表现。其中心部分是引入了“LM4OPT”，这是一个利用噪声嵌入和专门数据集进行Llama-2-7b渐进微调的框架。然而，这项研究突出了小型模型（如Llama-2-7b）在处理冗长和复杂输入上的上下文理解能力与更大型对应模型之间存在显著差距。我们的实证研究利用了NL4Opt",
    "tldr": "本研究比较了几种知名的大型语言模型在翻译语言描述为数学优化问题中的表现，发现GPT-4在一次场景中表现出色，并引入了“LM4OPT”框架进行Llama-2-7b的渐进微调。",
    "en_tdlr": "This study compares the performance of prominent Large Language Models in translating linguistic descriptions into mathematical optimization problems, finding GPT-4 excelling in one-shot scenario and introducing a progressive fine-tuning framework 'LM4OPT' for Llama-2-7b."
}