{
    "title": "Axe the X in XAI: A Plea for Understandable AI",
    "abstract": "arXiv:2403.00315v1 Announce Type: new  Abstract: In a recent paper, Erasmus et al. (2021) defend the idea that the ambiguity of the term \"explanation\" in explainable AI (XAI) can be solved by adopting any of four different extant accounts of explanation in the philosophy of science: the Deductive Nomological, Inductive Statistical, Causal Mechanical, and New Mechanist models. In this chapter, I show that the authors' claim that these accounts can be applied to deep neural networks as they would to any natural phenomenon is mistaken. I also provide a more general argument as to why the notion of explainability as it is currently used in the XAI literature bears little resemblance to the traditional concept of scientific explanation. It would be more fruitful to use the label \"understandable AI\" to avoid the confusion that surrounds the goal and purposes of XAI. In the second half of the chapter, I argue for a pragmatic conception of understanding that is better suited to play the centra",
    "link": "https://arxiv.org/abs/2403.00315",
    "context": "Title: Axe the X in XAI: A Plea for Understandable AI\nAbstract: arXiv:2403.00315v1 Announce Type: new  Abstract: In a recent paper, Erasmus et al. (2021) defend the idea that the ambiguity of the term \"explanation\" in explainable AI (XAI) can be solved by adopting any of four different extant accounts of explanation in the philosophy of science: the Deductive Nomological, Inductive Statistical, Causal Mechanical, and New Mechanist models. In this chapter, I show that the authors' claim that these accounts can be applied to deep neural networks as they would to any natural phenomenon is mistaken. I also provide a more general argument as to why the notion of explainability as it is currently used in the XAI literature bears little resemblance to the traditional concept of scientific explanation. It would be more fruitful to use the label \"understandable AI\" to avoid the confusion that surrounds the goal and purposes of XAI. In the second half of the chapter, I argue for a pragmatic conception of understanding that is better suited to play the centra",
    "path": "papers/24/03/2403.00315.json",
    "total_tokens": 902,
    "translated_title": "切掉XAI中的X: 为可理解的人工智能辩护",
    "translated_abstract": "在最近的一篇论文中，Erasmus等人(2021)辩护了在可解释的人工智能(XAI)中术语“解释”存在的歧义可以通过采用哲学科学中四种不同的现存解释模式之一来解决：演绎-规范、归纳-统计、因果机械和新机制主义模式。在本章中，我展示了作者声称这些模式可以像对待任何自然现象一样应用于深度神经网络的说法是错误的。我还提出了更一般的论点，说明了XAI文献中目前使用的可解释性概念与传统科学解释概念几乎没有相似之处。更有成效的做法是使用“可理解的人工智能”标签，以避免围绕XAI目标和目的的困惑。在章节的后半部分，我主张采用更适合扮演核心角色的实用理解概念。",
    "tldr": "论文辩护了采用“可理解的人工智能”标签作为替代“XAI”，以避免围绕XAI目标和目的的混乱，并主张采用更适合的实用理解概念。",
    "en_tdlr": "The paper advocates for using the label \"understandable AI\" as an alternative to \"XAI\" to avoid confusion surrounding the goals and purposes of XAI, and argues for adopting a more suitable pragmatic conception of understanding."
}