{
    "title": "Intrinsic Subgraph Generation for Interpretable Graph based Visual Question Answering",
    "abstract": "arXiv:2403.17647v1 Announce Type: new  Abstract: The large success of deep learning based methods in Visual Question Answering (VQA) has concurrently increased the demand for explainable methods. Most methods in Explainable Artificial Intelligence (XAI) focus on generating post-hoc explanations rather than taking an intrinsic approach, the latter characterizing an interpretable model. In this work, we introduce an interpretable approach for graph-based VQA and demonstrate competitive performance on the GQA dataset. This approach bridges the gap between interpretability and performance. Our model is designed to intrinsically produce a subgraph during the question-answering process as its explanation, providing insight into the decision making. To evaluate the quality of these generated subgraphs, we compare them against established post-hoc explainability methods for graph neural networks, and perform a human evaluation. Moreover, we present quantitative metrics that correlate with the ",
    "link": "https://arxiv.org/abs/2403.17647",
    "context": "Title: Intrinsic Subgraph Generation for Interpretable Graph based Visual Question Answering\nAbstract: arXiv:2403.17647v1 Announce Type: new  Abstract: The large success of deep learning based methods in Visual Question Answering (VQA) has concurrently increased the demand for explainable methods. Most methods in Explainable Artificial Intelligence (XAI) focus on generating post-hoc explanations rather than taking an intrinsic approach, the latter characterizing an interpretable model. In this work, we introduce an interpretable approach for graph-based VQA and demonstrate competitive performance on the GQA dataset. This approach bridges the gap between interpretability and performance. Our model is designed to intrinsically produce a subgraph during the question-answering process as its explanation, providing insight into the decision making. To evaluate the quality of these generated subgraphs, we compare them against established post-hoc explainability methods for graph neural networks, and perform a human evaluation. Moreover, we present quantitative metrics that correlate with the ",
    "path": "papers/24/03/2403.17647.json",
    "total_tokens": 800,
    "translated_title": "用于可解释图像问答的内在子图生成",
    "translated_abstract": "深度学习在视觉问答（VQA）中取得了巨大成功，同时也增加了对可解释方法的需求。大多数可解释人工智能（XAI）方法侧重于生成事后解释，而非采取内在方法，后者特征化了可解释模型。在这项工作中，我们介绍了一种用于基于图的VQA的可解释方法，并在GQA数据集上展示了竞争性能。这种方法弥合了解释性和性能之间的差距。我们的模型被设计成在问答过程中本质上生成一个子图作为解释，提供决策制定的洞察。为了评估这些生成的子图的质量，我们将它们与建立的用于图神经网络的事后解释能力方法进行比较，并进行人类评估。此外，我们提出了与...",
    "tldr": "该论文介绍了一种用于图像问答的可解释方法，通过内在生成子图来提供决策洞察，并在GQA数据集上取得了竞争性能。"
}