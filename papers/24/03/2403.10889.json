{
    "title": "List Sample Compression and Uniform Convergence",
    "abstract": "arXiv:2403.10889v1 Announce Type: new  Abstract: List learning is a variant of supervised classification where the learner outputs multiple plausible labels for each instance rather than just one. We investigate classical principles related to generalization within the context of list learning. Our primary goal is to determine whether classical principles in the PAC setting retain their applicability in the domain of list PAC learning. We focus on uniform convergence (which is the basis of Empirical Risk Minimization) and on sample compression (which is a powerful manifestation of Occam's Razor). In classical PAC learning, both uniform convergence and sample compression satisfy a form of `completeness': whenever a class is learnable, it can also be learned by a learning rule that adheres to these principles. We ask whether the same completeness holds true in the list learning setting.   We show that uniform convergence remains equivalent to learnability in the list PAC learning setting",
    "link": "https://arxiv.org/abs/2403.10889",
    "context": "Title: List Sample Compression and Uniform Convergence\nAbstract: arXiv:2403.10889v1 Announce Type: new  Abstract: List learning is a variant of supervised classification where the learner outputs multiple plausible labels for each instance rather than just one. We investigate classical principles related to generalization within the context of list learning. Our primary goal is to determine whether classical principles in the PAC setting retain their applicability in the domain of list PAC learning. We focus on uniform convergence (which is the basis of Empirical Risk Minimization) and on sample compression (which is a powerful manifestation of Occam's Razor). In classical PAC learning, both uniform convergence and sample compression satisfy a form of `completeness': whenever a class is learnable, it can also be learned by a learning rule that adheres to these principles. We ask whether the same completeness holds true in the list learning setting.   We show that uniform convergence remains equivalent to learnability in the list PAC learning setting",
    "path": "papers/24/03/2403.10889.json",
    "total_tokens": 840,
    "translated_title": "列表样本压缩和均匀收敛",
    "translated_abstract": "列表学习是监督分类的一个变种，在这种学习中，学习器为每个实例输出多个可能的标签，而不仅仅是一个。我们研究了与列表学习上的泛化相关的经典原则。我们的主要目标是确定在列表PAC学习领域，PAC设置中的经典原则是否保留其适用性。我们重点关注均匀收敛（这是经验风险最小化的基础）和样本压缩（这是Occam's Razor的一个强大体现）。在经典PAC学习中，均匀收敛和样本压缩都满足一种“完备性”形式：每当一个类是可学习的时候，也可以通过遵循这些原则的学习规则来学习它。我们探讨在列表学习环境中是否也存在相同的完备性。我们表明在列表PAC学习环境中，均匀收敛仍然等价于可学习性。",
    "tldr": "研究在列表学习中均匀收敛和样本压缩原则的适用性，证明了在列表PAC学习中均匀收敛仍然等价于可学习性",
    "en_tdlr": "Investigating the applicability of uniform convergence and sample compression principles in list learning, showing that uniform convergence remains equivalent to learnability in list PAC learning."
}