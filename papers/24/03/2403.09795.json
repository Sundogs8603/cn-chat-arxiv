{
    "title": "Helpful or Harmful? Exploring the Efficacy of Large Language Models for Online Grooming Prevention",
    "abstract": "arXiv:2403.09795v1 Announce Type: cross  Abstract: Powerful generative Large Language Models (LLMs) are becoming popular tools amongst the general public as question-answering systems, and are being utilised by vulnerable groups such as children. With children increasingly interacting with these tools, it is imperative for researchers to scrutinise the safety of LLMs, especially for applications that could lead to serious outcomes, such as online child safety queries. In this paper, the efficacy of LLMs for online grooming prevention is explored both for identifying and avoiding grooming through advice generation, and the impact of prompt design on model performance is investigated by varying the provided context and prompt specificity. In results reflecting over 6,000 LLM interactions, we find that no models were clearly appropriate for online grooming prevention, with an observed lack of consistency in behaviours, and potential for harmful answer generation, especially from open-sour",
    "link": "https://arxiv.org/abs/2403.09795",
    "context": "Title: Helpful or Harmful? Exploring the Efficacy of Large Language Models for Online Grooming Prevention\nAbstract: arXiv:2403.09795v1 Announce Type: cross  Abstract: Powerful generative Large Language Models (LLMs) are becoming popular tools amongst the general public as question-answering systems, and are being utilised by vulnerable groups such as children. With children increasingly interacting with these tools, it is imperative for researchers to scrutinise the safety of LLMs, especially for applications that could lead to serious outcomes, such as online child safety queries. In this paper, the efficacy of LLMs for online grooming prevention is explored both for identifying and avoiding grooming through advice generation, and the impact of prompt design on model performance is investigated by varying the provided context and prompt specificity. In results reflecting over 6,000 LLM interactions, we find that no models were clearly appropriate for online grooming prevention, with an observed lack of consistency in behaviours, and potential for harmful answer generation, especially from open-sour",
    "path": "papers/24/03/2403.09795.json",
    "total_tokens": 858,
    "translated_title": "探讨大型语言模型在在线诱拐预防中的效力：有益还是有害？",
    "translated_abstract": "强大的生成式大型语言模型(LLMs)作为问答系统正在成为普通大众和脆弱群体（如儿童）之间流行的工具。随着儿童与这些工具的互动日益增多，对研究人员来说，审视LLMs的安全性至关重要，特别是对可能导致严重后果的应用，比如在线儿童安全查询。本文探讨了LLMs在在线诱拐预防中的效力，既包括通过建议生成来识别和避免诱拐，也通过改变提供的上下文和提示特定性来调查提示设计对模型性能的影响。通过对超过6,000次LLMs互动的结果反映，我们发现没有模型明显适用于在线诱拐预防，在行为一致性方面存在缺乏一致性，并且存在有害答案生成的潜力，特别是来自开源",
    "tldr": "研究人员探索大型语言模型在在线诱拐预防中的有效性，发现没有模型明显适用于此目的，存在潜在的有害答案生成。",
    "en_tdlr": "Researchers investigated the efficacy of large language models for online grooming prevention and found that no models were clearly suitable for this purpose, with a potential for harmful answer generation."
}