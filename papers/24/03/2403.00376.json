{
    "title": "Invariant Test-Time Adaptation for Vision-Language Model Generalization",
    "abstract": "arXiv:2403.00376v1 Announce Type: cross  Abstract: Vision-language foundation models have exhibited remarkable success across a multitude of downstream tasks due to their scalability on extensive image-text paired datasets. However, these models display significant limitations when applied to long-tail tasks, such as fine-grained image classification, as a result of \"decision shortcuts\" that hinders their generalization capabilities. In this work, we find that the CLIP model possesses a rich set of features, encompassing both \\textit{desired invariant causal features} and \\textit{undesired decision shortcuts}. Moreover, the underperformance of CLIP on downstream tasks originates from its inability to effectively utilize pre-trained features in accordance with specific task requirements. To address this challenge, this paper introduces a test-time prompt tuning paradigm that optimizes a learnable prompt, thereby compelling the model to exploit genuine causal invariant features while dis",
    "link": "https://arxiv.org/abs/2403.00376",
    "context": "Title: Invariant Test-Time Adaptation for Vision-Language Model Generalization\nAbstract: arXiv:2403.00376v1 Announce Type: cross  Abstract: Vision-language foundation models have exhibited remarkable success across a multitude of downstream tasks due to their scalability on extensive image-text paired datasets. However, these models display significant limitations when applied to long-tail tasks, such as fine-grained image classification, as a result of \"decision shortcuts\" that hinders their generalization capabilities. In this work, we find that the CLIP model possesses a rich set of features, encompassing both \\textit{desired invariant causal features} and \\textit{undesired decision shortcuts}. Moreover, the underperformance of CLIP on downstream tasks originates from its inability to effectively utilize pre-trained features in accordance with specific task requirements. To address this challenge, this paper introduces a test-time prompt tuning paradigm that optimizes a learnable prompt, thereby compelling the model to exploit genuine causal invariant features while dis",
    "path": "papers/24/03/2403.00376.json",
    "total_tokens": 879,
    "translated_title": "视觉-语言模型泛化的不变测试时适应性",
    "translated_abstract": "arXiv:2403.00376v1 公告类型: 交叉摘要: 视觉-语言基础模型在大量图像-文本配对数据集上的可扩展性使其在众多下游任务中展现出卓越成功。然而，这些模型在应用于长尾任务（如细粒度图像分类）时显示出明显局限，这是由于“决策捷径”导致了它们的泛化能力受限。本文发现CLIP模型具有丰富的特征集，涵盖了既有的\\textit{期望不变因果特征}又有的\\textit{不希望的决策捷径}。此外，CLIP在下游任务中的表现不佳源自其无法有效利用预训练特征以符合特定任务要求。为解决这一挑战，本文引入一种测试时提示调优范式，优化一个可学习的提示，从而促使模型利用真正的因果不变特征。",
    "tldr": "本文提出了一个测试时提示调优范式，通过优化可学习的提示，迫使模型利用真正的因果不变特征，以解决视觉-语言模型在特定任务需求上无法有效利用预训练特征的挑战。",
    "en_tdlr": "This paper introduces a test-time prompt tuning paradigm to compel the model to exploit genuine causal invariant features, addressing the challenge of the vision-language model's inability to effectively utilize pre-trained features according to specific task requirements."
}