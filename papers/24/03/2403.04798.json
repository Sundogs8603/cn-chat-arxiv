{
    "title": "JMI at SemEval 2024 Task 3: Two-step approach for multimodal ECAC using in-context learning with GPT and instruction-tuned Llama models",
    "abstract": "arXiv:2403.04798v1 Announce Type: new  Abstract: This paper presents our system development for SemEval-2024 Task 3: \"The Competition of Multimodal Emotion Cause Analysis in Conversations\". Effectively capturing emotions in human conversations requires integrating multiple modalities such as text, audio, and video. However, the complexities of these diverse modalities pose challenges for developing an efficient multimodal emotion cause analysis (ECA) system. Our proposed approach addresses these challenges by a two-step framework. We adopt two different approaches in our implementation. In Approach 1, we employ instruction-tuning with two separate Llama 2 models for emotion and cause prediction. In Approach 2, we use GPT-4V for conversation-level video description and employ in-context learning with annotated conversation using GPT 3.5. Our system wins rank 4, and system ablation experiments demonstrate that our proposed solutions achieve significant performance gains. All the experime",
    "link": "https://arxiv.org/abs/2403.04798",
    "context": "Title: JMI at SemEval 2024 Task 3: Two-step approach for multimodal ECAC using in-context learning with GPT and instruction-tuned Llama models\nAbstract: arXiv:2403.04798v1 Announce Type: new  Abstract: This paper presents our system development for SemEval-2024 Task 3: \"The Competition of Multimodal Emotion Cause Analysis in Conversations\". Effectively capturing emotions in human conversations requires integrating multiple modalities such as text, audio, and video. However, the complexities of these diverse modalities pose challenges for developing an efficient multimodal emotion cause analysis (ECA) system. Our proposed approach addresses these challenges by a two-step framework. We adopt two different approaches in our implementation. In Approach 1, we employ instruction-tuning with two separate Llama 2 models for emotion and cause prediction. In Approach 2, we use GPT-4V for conversation-level video description and employ in-context learning with annotated conversation using GPT 3.5. Our system wins rank 4, and system ablation experiments demonstrate that our proposed solutions achieve significant performance gains. All the experime",
    "path": "papers/24/03/2403.04798.json",
    "total_tokens": 901,
    "translated_title": "JMI在SemEval 2024任务3中的应用：使用GPT和instruction-tuned Llama模型进行多模态情感因果分析的两步法",
    "translated_abstract": "本文介绍了我们针对SemEval-2024任务3：“对话中的多模态情感因果分析竞赛”开发的系统。有效捕捉人类对话中的情感需要整合文本、音频和视频等多种模态。然而，这些多样性模态的复杂性给开发高效的多模态情感因果分析系统带来了挑战。我们提出的方法通过两步框架来解决这些挑战。我们在实现中采用了两种不同的方法。在方法1中，我们使用两个单独的Llama 2模型进行情感和原因预测的instruction-tuning。在方法2中，我们使用GPT-4V进行会话级视频描述，并使用带有GPT 3.5的上下文学习对注释对话进行处理。我们的系统获得了第4名，系统消融实验表明，我们提出的解决方案取得了显著的性能增益。",
    "tldr": "本文介绍了针对SemEval-2024任务3开发的多模态情感因果分析系统，提出了通过两步框架解决多模态情感因果分析挑战的方法，并在实验中取得显著性能提升。",
    "en_tdlr": "This paper presents a two-step framework for developing a multimodal emotion cause analysis system for SemEval-2024 Task 3, which addresses the challenges of integrating multiple modalities and achieves significant performance gains in experiments."
}