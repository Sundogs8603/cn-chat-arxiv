{
    "title": "Risk-Sensitive RL with Optimized Certainty Equivalents via Reduction to Standard RL",
    "abstract": "arXiv:2403.06323v1 Announce Type: new  Abstract: We study Risk-Sensitive Reinforcement Learning (RSRL) with the Optimized Certainty Equivalent (OCE) risk, which generalizes Conditional Value-at-risk (CVaR), entropic risk and Markowitz's mean-variance. Using an augmented Markov Decision Process (MDP), we propose two general meta-algorithms via reductions to standard RL: one based on optimistic algorithms and another based on policy optimization. Our optimistic meta-algorithm generalizes almost all prior RSRL theory with entropic risk or CVaR. Under discrete rewards, our optimistic theory also certifies the first RSRL regret bounds for MDPs with bounded coverability, e.g., exogenous block MDPs. Under discrete rewards, our policy optimization meta-algorithm enjoys both global convergence and local improvement guarantees in a novel metric that lower bounds the true OCE risk. Finally, we instantiate our framework with PPO, construct an MDP, and show that it learns the optimal risk-sensitive",
    "link": "https://arxiv.org/abs/2403.06323",
    "context": "Title: Risk-Sensitive RL with Optimized Certainty Equivalents via Reduction to Standard RL\nAbstract: arXiv:2403.06323v1 Announce Type: new  Abstract: We study Risk-Sensitive Reinforcement Learning (RSRL) with the Optimized Certainty Equivalent (OCE) risk, which generalizes Conditional Value-at-risk (CVaR), entropic risk and Markowitz's mean-variance. Using an augmented Markov Decision Process (MDP), we propose two general meta-algorithms via reductions to standard RL: one based on optimistic algorithms and another based on policy optimization. Our optimistic meta-algorithm generalizes almost all prior RSRL theory with entropic risk or CVaR. Under discrete rewards, our optimistic theory also certifies the first RSRL regret bounds for MDPs with bounded coverability, e.g., exogenous block MDPs. Under discrete rewards, our policy optimization meta-algorithm enjoys both global convergence and local improvement guarantees in a novel metric that lower bounds the true OCE risk. Finally, we instantiate our framework with PPO, construct an MDP, and show that it learns the optimal risk-sensitive",
    "path": "papers/24/03/2403.06323.json",
    "total_tokens": 1016,
    "translated_title": "使用优化等价证明降低到标准强化学习中的风险敏感RL",
    "translated_abstract": "我们研究了具有优化等价证明（OCE）风险的风险敏感强化学习（RSRL），该风险概括了条件值风险（CVaR）、熵风险和马科维茨的均值-方差。通过增强马尔可夫决策过程（MDP），我们提出了两个通用的元算法，通过将其降低为标准RL：一个基于乐观算法，另一个基于策略优化。我们的乐观元算法概括了几乎所有之前RSRL理论，该理论使用熵风险或CVaR。在离散奖励下，我们的乐观理论还证明了具有有界可覆盖性的MDP（例如外生块MDP）的第一个RSRL遗憾上界。在离散奖励下，我们的策略优化元算法在一个新颖的度量中享有全局收敛性和局部改进保证，该度量下界为真实的OCE风险。最后，我们使用PPO实例化我们的框架，构建一个MDP，并展示它学习了最优的风险敏感。",
    "tldr": "本研究通过将其降低为标准强化学习提出了两个通用的元算法，一个基于乐观算法，另一个基于策略优化，概括了以往的风险敏感强化学习理论并证实了新的理论在具有有界可覆盖性的MDP中的有效性。",
    "en_tdlr": "This study proposes two general meta-algorithms by reducing to standard reinforcement learning, one based on optimistic algorithms and another based on policy optimization, generalizing prior risk-sensitive reinforcement learning theories and validating the effectiveness of the new theory in MDPs with bounded coverability."
}