{
    "title": "Guardrail Baselines for Unlearning in LLMs",
    "abstract": "arXiv:2403.03329v1 Announce Type: new  Abstract: Recent work has demonstrated that fine-tuning is a promising approach to `unlearn' concepts from large language models. However, fine-tuning can be expensive, as it requires both generating a set of examples and running iterations of fine-tuning to update the model. In this work, we show that simple guardrail-based approaches such as prompting and filtering can achieve unlearning results comparable to fine-tuning. We recommend that researchers investigate these lightweight baselines when evaluating the performance of more computationally intensive fine-tuning methods. While we do not claim that methods such as prompting or filtering are universal solutions to the problem of unlearning, our work suggests the need for evaluation metrics that can better separate the power of guardrails vs. fine-tuning, and highlights scenarios where guardrails themselves may be advantageous for unlearning, such as in generating examples for fine-tuning or u",
    "link": "https://arxiv.org/abs/2403.03329",
    "context": "Title: Guardrail Baselines for Unlearning in LLMs\nAbstract: arXiv:2403.03329v1 Announce Type: new  Abstract: Recent work has demonstrated that fine-tuning is a promising approach to `unlearn' concepts from large language models. However, fine-tuning can be expensive, as it requires both generating a set of examples and running iterations of fine-tuning to update the model. In this work, we show that simple guardrail-based approaches such as prompting and filtering can achieve unlearning results comparable to fine-tuning. We recommend that researchers investigate these lightweight baselines when evaluating the performance of more computationally intensive fine-tuning methods. While we do not claim that methods such as prompting or filtering are universal solutions to the problem of unlearning, our work suggests the need for evaluation metrics that can better separate the power of guardrails vs. fine-tuning, and highlights scenarios where guardrails themselves may be advantageous for unlearning, such as in generating examples for fine-tuning or u",
    "path": "papers/24/03/2403.03329.json",
    "total_tokens": 825,
    "translated_title": "Guardrail Baselines for Unlearning in LLMs",
    "translated_abstract": "最近的研究表明fine-tuning是从大型语言模型中“unlearn”概念的一种有前途的方法。然而，fine-tuning可能很昂贵，因为它既需要生成一组示例，又需要运行多次迭代的fine-tuning来更新模型。在这项工作中，我们展示了简单的基于guardrail的方法，如提示和过滤，可以实现与fine-tuning相媲美的unlearning结果。我们建议研究人员在评估更消耗计算资源的fine-tuning方法的性能时，调查这些轻量级基线。虽然我们并不声称提示或过滤等方法是unlearning问题的通用解决方案，但我们的工作表明需要更好地区分guardrails与fine-tuning的强大之处的评估指标，并强调guardrails本身可能为unlearning具有优势的场景，例如生成示例用于fine-tuning或u",
    "tldr": "简单的基于guardrail的方法如提示和过滤可以实现与fine-tuning相媲美的unlearning结果，建议研究人员在评估更消耗计算资源的fine-tuning方法时考虑这些轻量级基线。",
    "en_tdlr": "Simple guardrail-based approaches such as prompting and filtering can achieve unlearning results comparable to fine-tuning, recommending researchers to explore these lightweight baselines when evaluating more computationally intensive fine-tuning methods."
}