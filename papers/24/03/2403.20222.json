{
    "title": "Shallow Cross-Encoders for Low-Latency Retrieval",
    "abstract": "arXiv:2403.20222v1 Announce Type: cross  Abstract: Transformer-based Cross-Encoders achieve state-of-the-art effectiveness in text retrieval. However, Cross-Encoders based on large transformer models (such as BERT or T5) are computationally expensive and allow for scoring only a small number of documents within a reasonably small latency window. However, keeping search latencies low is important for user satisfaction and energy usage. In this paper, we show that weaker shallow transformer models (i.e., transformers with a limited number of layers) actually perform better than full-scale models when constrained to these practical low-latency settings since they can estimate the relevance of more documents in the same time budget. We further show that shallow transformers may benefit from the generalized Binary Cross-Entropy (gBCE) training scheme, which has recently demonstrated success for recommendation tasks. Our experiments with TREC Deep Learning passage ranking query sets demonstr",
    "link": "https://arxiv.org/abs/2403.20222",
    "context": "Title: Shallow Cross-Encoders for Low-Latency Retrieval\nAbstract: arXiv:2403.20222v1 Announce Type: cross  Abstract: Transformer-based Cross-Encoders achieve state-of-the-art effectiveness in text retrieval. However, Cross-Encoders based on large transformer models (such as BERT or T5) are computationally expensive and allow for scoring only a small number of documents within a reasonably small latency window. However, keeping search latencies low is important for user satisfaction and energy usage. In this paper, we show that weaker shallow transformer models (i.e., transformers with a limited number of layers) actually perform better than full-scale models when constrained to these practical low-latency settings since they can estimate the relevance of more documents in the same time budget. We further show that shallow transformers may benefit from the generalized Binary Cross-Entropy (gBCE) training scheme, which has recently demonstrated success for recommendation tasks. Our experiments with TREC Deep Learning passage ranking query sets demonstr",
    "path": "papers/24/03/2403.20222.json",
    "total_tokens": 852,
    "translated_title": "用于低延迟检索的浅层交叉编码器",
    "translated_abstract": "基于Transformer的交叉编码器在文本检索中取得了最先进的效果。然而，基于大型Transformer模型（如BERT或T5）的交叉编码器在计算上是昂贵的，且只允许在相对较小的延迟时间窗口内评分少量文档。本文表明，用于这些实际低延迟设置的较弱的浅层Transformer模型（即具有有限层数的Transformer）实际上比完整模型表现更好，因为它们可以在同样的时间预算内估算出更多文档的相关性。我们进一步表明，浅层Transformer可能会受益于最近在推荐任务中展示成功的广义二元交叉熵（gBCE）训练方案。我们在TREC深度学习段落排序查询集上的实验证明。",
    "tldr": "张海洋这里是中文总结出的一句话要点：本文展示了在低延迟设置下，较弱的浅层Transformer模型在文本检索中的表现优于完整模型，并且可能受益于广义二元交叉熵（gBCE）训练方案。",
    "en_tdlr": "Here is the TLDR in English: This paper demonstrates that weaker shallow transformer models perform better in text retrieval than full-scale models under low-latency settings and may benefit from the generalized Binary Cross-Entropy (gBCE) training scheme."
}