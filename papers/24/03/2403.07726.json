{
    "title": "SemEval-2024 Shared Task 6: SHROOM, a Shared-task on Hallucinations and Related Observable Overgeneration Mistakes",
    "abstract": "arXiv:2403.07726v1 Announce Type: new  Abstract: This paper presents the results of the SHROOM, a shared task focused on detecting hallucinations: outputs from natural language generation (NLG) systems that are fluent, yet inaccurate. Such cases of overgeneration put in jeopardy many NLG applications, where correctness is often mission-critical. The shared task was conducted with a newly constructed dataset of 4000 model outputs labeled by 5 annotators each, spanning 3 NLP tasks: machine translation, paraphrase generation and definition modeling.   The shared task was tackled by a total of 58 different users grouped in 42 teams, out of which 27 elected to write a system description paper; collectively, they submitted over 300 prediction sets on both tracks of the shared task. We observe a number of key trends in how this approach was tackled -- many participants rely on a handful of model, and often rely either on synthetic data for fine-tuning or zero-shot prompting strategies. While ",
    "link": "https://arxiv.org/abs/2403.07726",
    "context": "Title: SemEval-2024 Shared Task 6: SHROOM, a Shared-task on Hallucinations and Related Observable Overgeneration Mistakes\nAbstract: arXiv:2403.07726v1 Announce Type: new  Abstract: This paper presents the results of the SHROOM, a shared task focused on detecting hallucinations: outputs from natural language generation (NLG) systems that are fluent, yet inaccurate. Such cases of overgeneration put in jeopardy many NLG applications, where correctness is often mission-critical. The shared task was conducted with a newly constructed dataset of 4000 model outputs labeled by 5 annotators each, spanning 3 NLP tasks: machine translation, paraphrase generation and definition modeling.   The shared task was tackled by a total of 58 different users grouped in 42 teams, out of which 27 elected to write a system description paper; collectively, they submitted over 300 prediction sets on both tracks of the shared task. We observe a number of key trends in how this approach was tackled -- many participants rely on a handful of model, and often rely either on synthetic data for fine-tuning or zero-shot prompting strategies. While ",
    "path": "papers/24/03/2403.07726.json",
    "total_tokens": 840,
    "translated_title": "SemEval-2024共享任务6: SHROOM，一个关于幻觉及相关可观察过度生成错误的共享任务",
    "translated_abstract": "本文介绍了SHROOM的结果，这是一个专注于检测幻觉的共享任务：即自然语言生成（NLG）系统的输出流畅但不准确。这种过度生成的情况可能危及许多NLG应用，其中正确性往往至关重要。共享任务使用了一个新构建的数据集，包含4000个由5个标注者标记的模型输出，涵盖了3个NLP任务：机器翻译、释义生成和定义建模。 共享任务由58个不同用户组成的42支团队共同解决，其中27支选择撰写系统描述论文；他们共提交了超过300个预测集在共享任务的两个跟踪上。我们观察到这种方法如何被处理的一些关键趋势--许多参与者依赖少数模型，并经常依赖合成数据进行微调或零样本提示策略。",
    "tldr": "本文介绍了SHROOM共享任务，重点关注检测幻觉，以及参与者使用的模型和策略。"
}