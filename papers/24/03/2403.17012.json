{
    "title": "Evolution and Efficiency in Neural Architecture Search: Bridging the Gap Between Expert Design and Automated Optimization",
    "abstract": "arXiv:2403.17012v1 Announce Type: cross  Abstract: The paper provides a comprehensive overview of Neural Architecture Search (NAS), emphasizing its evolution from manual design to automated, computationally-driven approaches. It covers the inception and growth of NAS, highlighting its application across various domains, including medical imaging and natural language processing. The document details the shift from expert-driven design to algorithm-driven processes, exploring initial methodologies like reinforcement learning and evolutionary algorithms. It also discusses the challenges of computational demands and the emergence of efficient NAS methodologies, such as Differentiable Architecture Search and hardware-aware NAS. The paper further elaborates on NAS's application in computer vision, NLP, and beyond, demonstrating its versatility and potential for optimizing neural network architectures across different tasks. Future directions and challenges, including computational efficiency",
    "link": "https://arxiv.org/abs/2403.17012",
    "context": "Title: Evolution and Efficiency in Neural Architecture Search: Bridging the Gap Between Expert Design and Automated Optimization\nAbstract: arXiv:2403.17012v1 Announce Type: cross  Abstract: The paper provides a comprehensive overview of Neural Architecture Search (NAS), emphasizing its evolution from manual design to automated, computationally-driven approaches. It covers the inception and growth of NAS, highlighting its application across various domains, including medical imaging and natural language processing. The document details the shift from expert-driven design to algorithm-driven processes, exploring initial methodologies like reinforcement learning and evolutionary algorithms. It also discusses the challenges of computational demands and the emergence of efficient NAS methodologies, such as Differentiable Architecture Search and hardware-aware NAS. The paper further elaborates on NAS's application in computer vision, NLP, and beyond, demonstrating its versatility and potential for optimizing neural network architectures across different tasks. Future directions and challenges, including computational efficiency",
    "path": "papers/24/03/2403.17012.json",
    "total_tokens": 840,
    "translated_title": "神经架构搜索中的进化与效率：弥合专家设计与自动优化之间的鸿沟",
    "translated_abstract": "本文全面介绍了神经架构搜索（NAS），强调了它从手动设计到自动化、计算驱动方法的演变。它涵盖了NAS的起源和发展，突出了其在各个领域的应用，包括医学影像和自然语言处理。文章详细阐述了从专家驱动设计到算法驱动过程的转变，探讨了强化学习和进化算法等初始方法。还讨论了计算需求的挑战以及高效NAS方法的出现，如可微架构搜索和硬件感知NAS。该论文进一步阐述了NAS在计算机视觉、NLP等领域的应用，展示了其在不同任务中优化神经网络架构的多功能性和潜力。探讨了未来的方向和挑战，包括计算效率。",
    "tldr": "本文综述了神经架构搜索（NAS）领域的进化历程，介绍了从手动设计到自动化优化的演变过程，探讨了NAS在各个领域的应用，以及针对计算效率挑战提出的高效NAS方法。",
    "en_tdlr": "The paper provides a comprehensive overview of the evolution in Neural Architecture Search (NAS), covering its transition from manual design to automated optimization, discussing its applications across domains, and the introduction of efficient NAS methodologies to tackle computational challenges."
}