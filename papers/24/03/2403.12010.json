{
    "title": "VideoMV: Consistent Multi-View Generation Based on Large Video Generative Model",
    "abstract": "arXiv:2403.12010v1 Announce Type: cross  Abstract: Generating multi-view images based on text or single-image prompts is a critical capability for the creation of 3D content. Two fundamental questions on this topic are what data we use for training and how to ensure multi-view consistency. This paper introduces a novel framework that makes fundamental contributions to both questions. Unlike leveraging images from 2D diffusion models for training, we propose a dense consistent multi-view generation model that is fine-tuned from off-the-shelf video generative models. Images from video generative models are more suitable for multi-view generation because the underlying network architecture that generates them employs a temporal module to enforce frame consistency. Moreover, the video data sets used to train these models are abundant and diverse, leading to a reduced train-finetuning domain gap. To enhance multi-view consistency, we introduce a 3D-Aware Denoising Sampling, which first empl",
    "link": "https://arxiv.org/abs/2403.12010",
    "context": "Title: VideoMV: Consistent Multi-View Generation Based on Large Video Generative Model\nAbstract: arXiv:2403.12010v1 Announce Type: cross  Abstract: Generating multi-view images based on text or single-image prompts is a critical capability for the creation of 3D content. Two fundamental questions on this topic are what data we use for training and how to ensure multi-view consistency. This paper introduces a novel framework that makes fundamental contributions to both questions. Unlike leveraging images from 2D diffusion models for training, we propose a dense consistent multi-view generation model that is fine-tuned from off-the-shelf video generative models. Images from video generative models are more suitable for multi-view generation because the underlying network architecture that generates them employs a temporal module to enforce frame consistency. Moreover, the video data sets used to train these models are abundant and diverse, leading to a reduced train-finetuning domain gap. To enhance multi-view consistency, we introduce a 3D-Aware Denoising Sampling, which first empl",
    "path": "papers/24/03/2403.12010.json",
    "total_tokens": 848,
    "translated_title": "基于大型视频生成模型的一致性多视角生成",
    "translated_abstract": "生成基于文本或单图像提示的多视角图像是创建3D内容的关键能力。本文介绍了一个新颖的框架，对于数据用于训练和如何确保多视角一致性这两个基本问题都做出了贡献。与利用2D扩散模型的图像进行训练不同，我们提出了一种密集的一致性多视角生成模型，该模型是从现成的视频生成模型微调而来的。视频生成模型生成的图像更适合于多视角生成，因为生成这些图像的基础网络架构采用了时间模块来强制帧一致性。此外，用于训练这些模型的视频数据集丰富多样，导致减少了训练微调领域差距。为了增强多视角一致性，我们引入了一个3D感知去噪采样。",
    "tldr": "提出了基于大型视频生成模型的一致性多视角生成框架，通过微调视频生成模型并引入3D感知去噪采样方法来解决多视角图像生成中的数据训练和一致性问题。"
}