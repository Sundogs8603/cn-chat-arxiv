{
    "title": "GTC: GNN-Transformer Co-contrastive Learning for Self-supervised Heterogeneous Graph Representation",
    "abstract": "arXiv:2403.15520v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have emerged as the most powerful weapon for various graph tasks due to the message-passing mechanism's great local information aggregation ability. However, over-smoothing has always hindered GNNs from going deeper and capturing multi-hop neighbors. Unlike GNNs, Transformers can model global information and multi-hop interactions via multi-head self-attention and a proper Transformer structure can show more immunity to the over-smoothing problem. So, can we propose a novel framework to combine GNN and Transformer, integrating both GNN's local information aggregation and Transformer's global information modeling ability to eliminate the over-smoothing problem? To realize this, this paper proposes a collaborative learning scheme for GNN-Transformer and constructs GTC architecture. GTC leverages the GNN and Transformer branch to encode node information from different views respectively, and establishes contrast",
    "link": "https://arxiv.org/abs/2403.15520",
    "context": "Title: GTC: GNN-Transformer Co-contrastive Learning for Self-supervised Heterogeneous Graph Representation\nAbstract: arXiv:2403.15520v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have emerged as the most powerful weapon for various graph tasks due to the message-passing mechanism's great local information aggregation ability. However, over-smoothing has always hindered GNNs from going deeper and capturing multi-hop neighbors. Unlike GNNs, Transformers can model global information and multi-hop interactions via multi-head self-attention and a proper Transformer structure can show more immunity to the over-smoothing problem. So, can we propose a novel framework to combine GNN and Transformer, integrating both GNN's local information aggregation and Transformer's global information modeling ability to eliminate the over-smoothing problem? To realize this, this paper proposes a collaborative learning scheme for GNN-Transformer and constructs GTC architecture. GTC leverages the GNN and Transformer branch to encode node information from different views respectively, and establishes contrast",
    "path": "papers/24/03/2403.15520.json",
    "total_tokens": 854,
    "translated_title": "GTC：GNN-Transformer自监督异构图表示的共轭对比学习",
    "translated_abstract": "图神经网络（GNN）由于具有传递信息的机制，能够很好地聚合本地信息，在各种图任务中已经成为最强大的工具。然而，过度平滑一直阻碍着GNN进一步深入和捕获多跳邻居。与GNN不同，Transformer可以通过多头自注意力和适当的Transformer结构来建模全局信息和多跳交互，并且能够更好地解决过度平滑问题。因此，我们是否可以提出一个新颖的框架，将GNN和Transformer结合起来，整合GNN的本地信息聚合和Transformer的全局信息建模能力，以消除过度平滑问题？为了实现这一点，本文提出了一个用于GNN-Transformer的协同学习方案，并构建了GTC架构。GTC利用GNN和Transformer分支分别对来自不同视图的节点信息进行编码，并建立了对比",
    "tldr": "该论文提出了一个用于GNN和Transformer的合作学习方案，并构建了GTC架构，通过整合GNN的本地信息聚合和Transformer的全局信息建模能力，解决了过度平滑问题。",
    "en_tdlr": "This paper introduces a collaborative learning scheme for GNN and Transformer, and constructs the GTC architecture to address the over-smoothing problem by integrating the local information aggregation of GNN and the global information modeling capability of Transformer."
}