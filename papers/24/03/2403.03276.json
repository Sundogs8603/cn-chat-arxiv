{
    "title": "ARNN: Attentive Recurrent Neural Network for Multi-channel EEG Signals to Identify Epileptic Seizures",
    "abstract": "arXiv:2403.03276v1 Announce Type: cross  Abstract: We proposed an Attentive Recurrent Neural Network (ARNN), which recurrently applies attention layers along a sequence and has linear complexity with respect to the sequence length. The proposed model operates on multi-channel EEG signals rather than single channel signals and leverages parallel computation. In this cell, the attention layer is a computational unit that efficiently applies self-attention and cross-attention mechanisms to compute a recurrent function over a wide number of state vectors and input signals. Our architecture is inspired in part by the attention layer and long short-term memory (LSTM) cells, and it uses long-short style gates, but it scales this typical cell up by several orders to parallelize for multi-channel EEG signals. It inherits the advantages of attention layers and LSTM gate while avoiding their respective drawbacks. We evaluated the model effectiveness through extensive experiments with heterogeneou",
    "link": "https://arxiv.org/abs/2403.03276",
    "context": "Title: ARNN: Attentive Recurrent Neural Network for Multi-channel EEG Signals to Identify Epileptic Seizures\nAbstract: arXiv:2403.03276v1 Announce Type: cross  Abstract: We proposed an Attentive Recurrent Neural Network (ARNN), which recurrently applies attention layers along a sequence and has linear complexity with respect to the sequence length. The proposed model operates on multi-channel EEG signals rather than single channel signals and leverages parallel computation. In this cell, the attention layer is a computational unit that efficiently applies self-attention and cross-attention mechanisms to compute a recurrent function over a wide number of state vectors and input signals. Our architecture is inspired in part by the attention layer and long short-term memory (LSTM) cells, and it uses long-short style gates, but it scales this typical cell up by several orders to parallelize for multi-channel EEG signals. It inherits the advantages of attention layers and LSTM gate while avoiding their respective drawbacks. We evaluated the model effectiveness through extensive experiments with heterogeneou",
    "path": "papers/24/03/2403.03276.json",
    "total_tokens": 876,
    "translated_title": "ARNN: 用于识别癫痫发作的多通道脑电图信号的注意力循环神经网络",
    "translated_abstract": "我们提出了一种注意力循环神经网络（ARNN），其沿着序列循环应用注意力层，并且具有与序列长度相关的线性复杂度。该模型在多通道脑电图信号上运行，而不是单通道信号，并利用并行计算。在该模型中，注意力层是一种计算单元，可以有效地应用自注意力机制和交叉注意力机制来计算一组广泛数量的状态向量和输入信号的递归函数。我们的架构在某种程度上受到了注意力层和长短期记忆（LSTM）单元的启发，并使用长短风格门，但通过多个阶段将这种典型单元扩展到多通道脑电图信号的并行化。它继承了注意力层和LSTM门的优势，同时避免了它们各自的缺点。我们通过对异质实验进行了广泛的模型有效性评估。",
    "tldr": "ARNN提出了一种注意力循环神经网络，用于处理多通道脑电图信号，具有线性复杂度和并行计算，结合注意力和LSTM gate的优势，并避免了它们的缺点。",
    "en_tdlr": "ARNN proposed an attentive recurrent neural network for multi-channel EEG signals, with linear complexity and parallel computation, combining the advantages of attention and LSTM gate while avoiding their drawbacks."
}