{
    "title": "Click to Grasp: Zero-Shot Precise Manipulation via Visual Diffusion Descriptors",
    "abstract": "arXiv:2403.14526v1 Announce Type: cross  Abstract: Precise manipulation that is generalizable across scenes and objects remains a persistent challenge in robotics. Current approaches for this task heavily depend on having a significant number of training instances to handle objects with pronounced visual and/or geometric part ambiguities. Our work explores the grounding of fine-grained part descriptors for precise manipulation in a zero-shot setting by utilizing web-trained text-to-image diffusion-based generative models. We tackle the problem by framing it as a dense semantic part correspondence task. Our model returns a gripper pose for manipulating a specific part, using as reference a user-defined click from a source image of a visually different instance of the same object. We require no manual grasping demonstrations as we leverage the intrinsic object geometry and features. Practical experiments in a real-world tabletop scenario validate the efficacy of our approach, demonstrati",
    "link": "https://arxiv.org/abs/2403.14526",
    "context": "Title: Click to Grasp: Zero-Shot Precise Manipulation via Visual Diffusion Descriptors\nAbstract: arXiv:2403.14526v1 Announce Type: cross  Abstract: Precise manipulation that is generalizable across scenes and objects remains a persistent challenge in robotics. Current approaches for this task heavily depend on having a significant number of training instances to handle objects with pronounced visual and/or geometric part ambiguities. Our work explores the grounding of fine-grained part descriptors for precise manipulation in a zero-shot setting by utilizing web-trained text-to-image diffusion-based generative models. We tackle the problem by framing it as a dense semantic part correspondence task. Our model returns a gripper pose for manipulating a specific part, using as reference a user-defined click from a source image of a visually different instance of the same object. We require no manual grasping demonstrations as we leverage the intrinsic object geometry and features. Practical experiments in a real-world tabletop scenario validate the efficacy of our approach, demonstrati",
    "path": "papers/24/03/2403.14526.json",
    "total_tokens": 826,
    "translated_title": "点击抓取: 通过视觉扩散描述进行零样本精确操作",
    "translated_abstract": "精确操控在机器人领域一直是一个持久的挑战，特别是在能够在不同场景和不同物体之间泛化的情况下。我们的工作通过利用基于网络训练的文本到图像扩散生成模型，探索了在零样本情况下将细粒度部件描述符用于精确操控领域。我们将问题框定为密集语义部件对应任务，模型通过参考源图像中与目标物体不同实例的点击，返回一个用于操控特定部件的夹具姿势。我们不需要手动抓取示范，因为我们利用了固有的物体几何和特征。在真实世界桌面情境中的实用实验验证了我们方法的有效性。",
    "tldr": "通过利用 web 训练文本到图像扩散生成模型，在零样本情况下利用细粒度部件描述符进行精确操作，通过点击源图像不同实例的引用，返回夹具姿势，实现了机器人精确操控。",
    "en_tdlr": "Leveraging web-trained text-to-image diffusion-based generative models, our work achieves precise manipulation in a zero-shot setting by utilizing fine-grained part descriptors, returning gripper pose by referencing clicks from visually different instances of the same object."
}