{
    "title": "ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys",
    "abstract": "arXiv:2403.02352v1 Announce Type: cross  Abstract: We propose a new attention mechanism with linear complexity, ATP, that fixates \\textbf{A}ttention on \\textbf{T}op \\textbf{P}rincipal keys, rather than on each individual token. Particularly, ATP is driven by an important observation that input sequences are typically low-rank, i.e., input sequences can be represented by a few principal bases. Therefore, instead of directly iterating over all the input tokens, ATP transforms inputs into an orthogonal space and computes attention only on the top principal bases (keys). Owing to the observed low-rank structure in input sequences, ATP is able to capture semantic relationships in input sequences with a few principal keys. Furthermore, the attention complexity is reduced from \\emph{quadratic} to \\emph{linear} without incurring a noticeable performance drop. ATP further reduces complexity for other linear layers with low-rank inputs, leading to more speedup compared to prior works that solely",
    "link": "https://arxiv.org/abs/2403.02352",
    "context": "Title: ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys\nAbstract: arXiv:2403.02352v1 Announce Type: cross  Abstract: We propose a new attention mechanism with linear complexity, ATP, that fixates \\textbf{A}ttention on \\textbf{T}op \\textbf{P}rincipal keys, rather than on each individual token. Particularly, ATP is driven by an important observation that input sequences are typically low-rank, i.e., input sequences can be represented by a few principal bases. Therefore, instead of directly iterating over all the input tokens, ATP transforms inputs into an orthogonal space and computes attention only on the top principal bases (keys). Owing to the observed low-rank structure in input sequences, ATP is able to capture semantic relationships in input sequences with a few principal keys. Furthermore, the attention complexity is reduced from \\emph{quadratic} to \\emph{linear} without incurring a noticeable performance drop. ATP further reduces complexity for other linear layers with low-rank inputs, leading to more speedup compared to prior works that solely",
    "path": "papers/24/03/2403.02352.json",
    "total_tokens": 860,
    "translated_title": "ATP: 通过对顶级主要键进行关注实现快速LLM服务",
    "translated_abstract": "我们提出了一种具有线性复杂度的新型注意机制 ATP，该机制将注意力集中在顶级主要键上，而不是每个单独的标记上。特别地，ATP受到一个重要观察的驱动，即输入序列通常具有低秩结构，即输入序列可以由少量主要基表示。因此，ATP将输入转换为正交空间，并仅在顶级主要基上计算注意力。由于输入序列中观察到的低秩结构，ATP能够仅通过少量主要基捕捉输入序列中的语义关系。此外，注意复杂度从二次降低到线性，而不会引起明显的性能下降。ATP进一步为具有低秩输入的其他线性层减少了复杂度，与仅单纯进行注意力计算的先前工作相比，实现了更大的加速。",
    "tldr": "提出了一种新的注意机制ATP，通过关注顶级主要键而非每个标记，以实现对输入序列的快速处理，并能够在降低注意力复杂度的同时捕捉输入序列的语义关系。",
    "en_tdlr": "A new attention mechanism ATP is proposed to enable fast processing of input sequences by focusing on top principal keys instead of individual tokens, capturing semantic relationships in the input sequences while reducing attention complexity."
}