{
    "title": "Automated Feature Selection for Inverse Reinforcement Learning",
    "abstract": "arXiv:2403.15079v1 Announce Type: new  Abstract: Inverse reinforcement learning (IRL) is an imitation learning approach to learning reward functions from expert demonstrations. Its use avoids the difficult and tedious procedure of manual reward specification while retaining the generalization power of reinforcement learning. In IRL, the reward is usually represented as a linear combination of features. In continuous state spaces, the state variables alone are not sufficiently rich to be used as features, but which features are good is not known in general. To address this issue, we propose a method that employs polynomial basis functions to form a candidate set of features, which are shown to allow the matching of statistical moments of state distributions. Feature selection is then performed for the candidates by leveraging the correlation between trajectory probabilities and feature expectations. We demonstrate the approach's effectiveness by recovering reward functions that capture ",
    "link": "https://arxiv.org/abs/2403.15079",
    "context": "Title: Automated Feature Selection for Inverse Reinforcement Learning\nAbstract: arXiv:2403.15079v1 Announce Type: new  Abstract: Inverse reinforcement learning (IRL) is an imitation learning approach to learning reward functions from expert demonstrations. Its use avoids the difficult and tedious procedure of manual reward specification while retaining the generalization power of reinforcement learning. In IRL, the reward is usually represented as a linear combination of features. In continuous state spaces, the state variables alone are not sufficiently rich to be used as features, but which features are good is not known in general. To address this issue, we propose a method that employs polynomial basis functions to form a candidate set of features, which are shown to allow the matching of statistical moments of state distributions. Feature selection is then performed for the candidates by leveraging the correlation between trajectory probabilities and feature expectations. We demonstrate the approach's effectiveness by recovering reward functions that capture ",
    "path": "papers/24/03/2403.15079.json",
    "total_tokens": 814,
    "translated_title": "逆强化学习的自动特征选择",
    "translated_abstract": "逆强化学习（IRL）是一种从专家示范中学习奖励函数的模仿学习方法。它的使用避免了手动指定奖励的困难和繁琐过程，同时保留了强化学习的泛化能力。在IRL中，奖励通常被表示为特征的线性组合。在连续状态空间中，仅使用状态变量作为特征不够丰富，但通常不清楚哪些特征是好的。为了解决这个问题，我们提出了一种方法，该方法利用多项式基函数来形成候选特征集合，这些特征被证明可以匹配状态分布的统计矩。然后通过利用轨迹概率和特征期望之间的相关性对候选特征进行特征选择。我们通过恢复捕获奖励函数的方式展示了该方法的有效性。",
    "tldr": "提出了一种使用多项式基函数进行特征选择的方法，通过匹配状态分布的统计矩和利用轨迹概率与特征期望的相关性，有效地恢复奖励函数。",
    "en_tdlr": "Introduced a method for feature selection using polynomial basis functions, which effectively recovers reward functions by matching statistical moments of state distributions and leveraging the correlation between trajectory probabilities and feature expectations."
}