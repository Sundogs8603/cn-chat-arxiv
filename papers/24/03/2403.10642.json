{
    "title": "Using Uncertainty Quantification to Characterize and Improve Out-of-Domain Learning for PDEs",
    "abstract": "arXiv:2403.10642v1 Announce Type: new  Abstract: Existing work in scientific machine learning (SciML) has shown that data-driven learning of solution operators can provide a fast approximate alternative to classical numerical partial differential equation (PDE) solvers. Of these, Neural Operators (NOs) have emerged as particularly promising. We observe that several uncertainty quantification (UQ) methods for NOs fail for test inputs that are even moderately out-of-domain (OOD), even when the model approximates the solution well for in-domain tasks. To address this limitation, we show that ensembling several NOs can identify high-error regions and provide good uncertainty estimates that are well-correlated with prediction errors. Based on this, we propose a cost-effective alternative, DiverseNO, that mimics the properties of the ensemble by encouraging diverse predictions from its multiple heads in the last feed-forward layer. We then introduce Operator-ProbConserv, a method that uses t",
    "link": "https://arxiv.org/abs/2403.10642",
    "context": "Title: Using Uncertainty Quantification to Characterize and Improve Out-of-Domain Learning for PDEs\nAbstract: arXiv:2403.10642v1 Announce Type: new  Abstract: Existing work in scientific machine learning (SciML) has shown that data-driven learning of solution operators can provide a fast approximate alternative to classical numerical partial differential equation (PDE) solvers. Of these, Neural Operators (NOs) have emerged as particularly promising. We observe that several uncertainty quantification (UQ) methods for NOs fail for test inputs that are even moderately out-of-domain (OOD), even when the model approximates the solution well for in-domain tasks. To address this limitation, we show that ensembling several NOs can identify high-error regions and provide good uncertainty estimates that are well-correlated with prediction errors. Based on this, we propose a cost-effective alternative, DiverseNO, that mimics the properties of the ensemble by encouraging diverse predictions from its multiple heads in the last feed-forward layer. We then introduce Operator-ProbConserv, a method that uses t",
    "path": "papers/24/03/2403.10642.json",
    "total_tokens": 842,
    "translated_title": "使用不确定性量化来表征和改进偏微分方程的区域外学习",
    "translated_abstract": "存在于科学机器学习（SciML）领域中的现有工作表明，通过数据驱动学习解算符可以为经典数值偏微分方程（PDE）求解器提供一个快速的近似替代方案。在其中，神经算子（NOs）已经被认为尤为具有前景。我们观察到，对于区域外（OOD）测试输入，几种NOs的不确定性量化（UQ）方法甚至在模型对于域内任务的解近似良好时也会失败。为了解决这个限制，我们展示了集成几个NOs可以识别高误差区域，并提供良好与预测误差相关的不确定性估计。基于此，我们提出了一种经济有效的替代方案，DiverseNO，通过鼓励其最后前向传播层中的多个头部进行多样化预测来模拟集成的属性。然后，我们介绍了一种使用t",
    "tldr": "通过集成多个神经算子来提高对区域外学习的不确定性估计，从而解决现有方法在OOD测试输入上的失败",
    "en_tdlr": "Improving uncertainty estimates for out-of-domain learning by ensembling multiple neural operators to address the failure of existing methods on OOD test inputs."
}