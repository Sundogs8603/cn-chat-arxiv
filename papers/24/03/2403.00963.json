{
    "title": "Tree-Regularized Tabular Embeddings",
    "abstract": "arXiv:2403.00963v1 Announce Type: new  Abstract: Tabular neural network (NN) has attracted remarkable attentions and its recent advances have gradually narrowed the performance gap with respect to tree-based models on many public datasets. While the mainstreams focus on calibrating NN to fit tabular data, we emphasize the importance of homogeneous embeddings and alternately concentrate on regularizing tabular inputs through supervised pretraining. Specifically, we extend a recent work (DeepTLF) and utilize the structure of pretrained tree ensembles to transform raw variables into a single vector (T2V), or an array of tokens (T2T). Without loss of space efficiency, these binarized embeddings can be consumed by canonical tabular NN with fully-connected or attention-based building blocks. Through quantitative experiments on 88 OpenML datasets with binary classification task, we validated that the proposed tree-regularized representation not only tapers the difference with respect to tree-",
    "link": "https://arxiv.org/abs/2403.00963",
    "context": "Title: Tree-Regularized Tabular Embeddings\nAbstract: arXiv:2403.00963v1 Announce Type: new  Abstract: Tabular neural network (NN) has attracted remarkable attentions and its recent advances have gradually narrowed the performance gap with respect to tree-based models on many public datasets. While the mainstreams focus on calibrating NN to fit tabular data, we emphasize the importance of homogeneous embeddings and alternately concentrate on regularizing tabular inputs through supervised pretraining. Specifically, we extend a recent work (DeepTLF) and utilize the structure of pretrained tree ensembles to transform raw variables into a single vector (T2V), or an array of tokens (T2T). Without loss of space efficiency, these binarized embeddings can be consumed by canonical tabular NN with fully-connected or attention-based building blocks. Through quantitative experiments on 88 OpenML datasets with binary classification task, we validated that the proposed tree-regularized representation not only tapers the difference with respect to tree-",
    "path": "papers/24/03/2403.00963.json",
    "total_tokens": 838,
    "translated_title": "树正则化的表格嵌入",
    "translated_abstract": "表格神经网络（NN）引起了显着的关注，其最新进展逐渐缩小了与许多公共数据集上树模型之间的性能差距。虽然主流关注于校准NN以适应表格数据，但我们强调同质嵌入的重要性，并交替集中在通过监督预训练来正则化表格输入。具体来说，我们扩展了最近的一个工作（DeepTLF），并利用预训练树集合的结构将原始变量转换为单个向量（T2V）或一系列标记（T2T）。这些二元化嵌入可以通过带有全连接或基于注意力的构建块的经典表格NN消耗，而不会损失空间效率。通过对88个OpenML数据集进行数量实验，其中包括二元分类任务，我们验证了所提出的树正则化表征不仅缩小了与树模型之间的差距",
    "tldr": "提出了一种通过树正则化表征来正则化表格输入的方法，将原始变量转换为单个向量或一系列标记，有效缩小了与树模型之间的性能差距。",
    "en_tdlr": "Proposed a method that regularizes tabular inputs through tree-regularized representation, transforming raw variables into a single vector or a series of tokens, effectively narrowing the performance gap with tree-based models."
}