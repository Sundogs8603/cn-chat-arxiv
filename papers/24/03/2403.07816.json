{
    "title": "Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM",
    "abstract": "arXiv:2403.07816v1 Announce Type: cross  Abstract: We investigate efficient methods for training Large Language Models (LLMs) to possess capabilities in multiple specialized domains, such as coding, math reasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts from a seed model, which is branched to train experts in embarrassingly parallel fashion with high throughput and reduced communication cost. After individual experts are asynchronously trained, BTX brings together their feedforward parameters as experts in Mixture-of-Expert (MoE) layers and averages the remaining parameters, followed by an MoE-finetuning stage to learn token-level routing. BTX generalizes two special cases, the Branch-Train-Merge method, which does not have the MoE finetuning stage to learn routing, and sparse upcycling, which omits the stage of training experts asynchronously. Compared to alternative approaches, BTX achieves the best accuracy-efficiency tradeoff.",
    "link": "https://arxiv.org/abs/2403.07816",
    "context": "Title: Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM\nAbstract: arXiv:2403.07816v1 Announce Type: cross  Abstract: We investigate efficient methods for training Large Language Models (LLMs) to possess capabilities in multiple specialized domains, such as coding, math reasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts from a seed model, which is branched to train experts in embarrassingly parallel fashion with high throughput and reduced communication cost. After individual experts are asynchronously trained, BTX brings together their feedforward parameters as experts in Mixture-of-Expert (MoE) layers and averages the remaining parameters, followed by an MoE-finetuning stage to learn token-level routing. BTX generalizes two special cases, the Branch-Train-Merge method, which does not have the MoE finetuning stage to learn routing, and sparse upcycling, which omits the stage of training experts asynchronously. Compared to alternative approaches, BTX achieves the best accuracy-efficiency tradeoff.",
    "path": "papers/24/03/2403.07816.json",
    "total_tokens": 954,
    "translated_title": "Branch-Train-MiX: 将专家LLMs混合到混合专家LLM中",
    "translated_abstract": "我们研究了训练大型语言模型（LLMs）以具备多个专业领域能力的高效方法，例如编码、数学推理和世界知识。我们的方法名为Branch-Train-MiX（BTX），从一个种子模型开始，将其分支训练成专家，以高吞吐量和降低通信成本的尴尬并行方式。在各个专家异步训练后，BTX将它们作为专家在Mixture-of-Expert（MoE）层中汇集其前馈参数，并平均其他参数，随后是一个MoE微调阶段来学习基于标记的路由。BTX概括了两个特殊情况，即Branch-Train-Merge方法，它没有MoE微调阶段来学习路由，以及稀疏升级，它省略了异步训练专家的阶段。与其他方法相比，BTX实现了最佳精度和效率的权衡。",
    "tldr": "BTX方法提供了一种高效的训练大型语言模型以具备多个专业领域能力的方法，通过将种子模型的分支培训成专家，然后在Mixture-of-Expert层中将它们汇集为专家，并利用MoE微调阶段学习基于标记的路由，从而实现了最佳的精度和效率权衡。",
    "en_tdlr": "The BTX method provides an efficient approach to train Large Language Models with capabilities in multiple specialized domains, by branching a seed model to train experts, then assembling them as experts in Mixture-of-Expert layers and using an MoE finetuning stage to learn token-level routing, achieving the best accuracy-efficiency tradeoff compared to other methods."
}