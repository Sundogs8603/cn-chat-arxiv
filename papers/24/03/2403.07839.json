{
    "title": "MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric",
    "abstract": "arXiv:2403.07839v1 Announce Type: cross  Abstract: Vision-language pre-trained models have achieved impressive performance on various downstream tasks. However, their large model sizes hinder their utilization on platforms with limited computational resources. We find that directly using smaller pre-trained models and applying magnitude-based pruning on CLIP models leads to inflexibility and inferior performance. Recent efforts for VLP compression either adopt uni-modal compression metrics resulting in limited performance or involve costly mask-search processes with learnable masks. In this paper, we first propose the Module-wise Pruning Error (MoPE) metric, accurately assessing CLIP module importance by performance decline on cross-modal tasks. Using the MoPE metric, we introduce a unified pruning framework applicable to both pre-training and task-specific fine-tuning compression stages. For pre-training, MoPE-CLIP effectively leverages knowledge from the teacher model, significantly ",
    "link": "https://arxiv.org/abs/2403.07839",
    "context": "Title: MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric\nAbstract: arXiv:2403.07839v1 Announce Type: cross  Abstract: Vision-language pre-trained models have achieved impressive performance on various downstream tasks. However, their large model sizes hinder their utilization on platforms with limited computational resources. We find that directly using smaller pre-trained models and applying magnitude-based pruning on CLIP models leads to inflexibility and inferior performance. Recent efforts for VLP compression either adopt uni-modal compression metrics resulting in limited performance or involve costly mask-search processes with learnable masks. In this paper, we first propose the Module-wise Pruning Error (MoPE) metric, accurately assessing CLIP module importance by performance decline on cross-modal tasks. Using the MoPE metric, we introduce a unified pruning framework applicable to both pre-training and task-specific fine-tuning compression stages. For pre-training, MoPE-CLIP effectively leverages knowledge from the teacher model, significantly ",
    "path": "papers/24/03/2403.07839.json",
    "total_tokens": 907,
    "translated_title": "MoPE-CLIP: 结构化剪枝用于高效的视觉-语言模型，带有基于模块的剪枝误差度量",
    "translated_abstract": "视觉-语言预训练模型在各种下游任务中取得了令人印象深刻的性能，然而，它们庞大的模型大小阻碍了在计算资源有限的平台上的利用。我们发现，直接使用更小的预训练模型并对CLIP模型应用基于大小的剪枝会导致僵化和性能较差。最近关于VLP压缩的努力要么采用单模态压缩指标导致性能有限，要么涉及昂贵的掩码搜索过程和可学习掩码。在本文中，我们首次提出了基于模块的剪枝误差（MoPE）指标，通过跨模态任务性能下降准确评估CLIP模块重要性。利用MoPE指标，我们引入了一个统一的剪枝框架，适用于预训练和任务特定微调压缩阶段。对于预训练，MoPE-CLIP有效利用了来自教师模型的知识，显著提高了",
    "tldr": "提出了Module-wise Pruning Error（MoPE）指标，用于评估CLIP模块重要性，在预训练和任务特定微调压缩阶段提出了统一的剪枝框架。",
    "en_tdlr": "Introduced the Module-wise Pruning Error (MoPE) metric to assess the importance of CLIP modules, and proposed a unified pruning framework for both pre-training and task-specific fine-tuning compression stages."
}