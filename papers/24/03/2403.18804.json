{
    "title": "Is Modularity Transferable? A Case Study through the Lens of Knowledge Distillation",
    "abstract": "arXiv:2403.18804v1 Announce Type: new  Abstract: The rise of Modular Deep Learning showcases its potential in various Natural Language Processing applications. Parameter-efficient fine-tuning (PEFT) modularity has been shown to work for various use cases, from domain adaptation to multilingual setups. However, all this work covers the case where the modular components are trained and deployed within one single Pre-trained Language Model (PLM). This model-specific setup is a substantial limitation on the very modularity that modular architectures are trying to achieve. We ask whether current modular approaches are transferable between models and whether we can transfer the modules from more robust and larger PLMs to smaller ones. In this work, we aim to fill this gap via a lens of Knowledge Distillation, commonly used for model compression, and present an extremely straightforward approach to transferring pre-trained, task-specific PEFT modules between same-family PLMs. Moreover, we pro",
    "link": "https://arxiv.org/abs/2403.18804",
    "context": "Title: Is Modularity Transferable? A Case Study through the Lens of Knowledge Distillation\nAbstract: arXiv:2403.18804v1 Announce Type: new  Abstract: The rise of Modular Deep Learning showcases its potential in various Natural Language Processing applications. Parameter-efficient fine-tuning (PEFT) modularity has been shown to work for various use cases, from domain adaptation to multilingual setups. However, all this work covers the case where the modular components are trained and deployed within one single Pre-trained Language Model (PLM). This model-specific setup is a substantial limitation on the very modularity that modular architectures are trying to achieve. We ask whether current modular approaches are transferable between models and whether we can transfer the modules from more robust and larger PLMs to smaller ones. In this work, we aim to fill this gap via a lens of Knowledge Distillation, commonly used for model compression, and present an extremely straightforward approach to transferring pre-trained, task-specific PEFT modules between same-family PLMs. Moreover, we pro",
    "path": "papers/24/03/2403.18804.json",
    "total_tokens": 894,
    "translated_title": "模块化可迁移吗？以知识蒸馏视角的案例研究",
    "translated_abstract": "深度学习模块化的兴起展示了它在各种自然语言处理应用中的潜力。参数高效微调（PEFT）模块化已被证明适用于各种用例，从领域自适应到多语言设置。然而，所有这项工作涵盖了模块组件在单个预训练语言模型（PLM）内训练和部署的情况。这种特定于模型的设置对模块化架构试图实现的模块化构成了实质性限制。我们探讨了当前模块化方法是否可以在模型之间可迁移，以及我们是否可以将更强大和更大的PLM中的模块转移到更小的PLM中。在这项工作中，我们旨在通过常用于模型压缩的知识蒸馏镜头填补这一空白，并提出了一种非常直观的方法，将预训练的任务特定PEFT模块在同系列PLM之间进行转移。",
    "tldr": "本研究通过知识蒸馏的视角，探讨了模块化方法在不同模型之间的可迁移性，提出了一种简单直接的方法，将预训练的任务特定模块在同系列PLM之间进行转移。",
    "en_tdlr": "This study investigates the transferability of modular approaches between different models through the lens of knowledge distillation, proposing a simple and straightforward method to transfer pre-trained task-specific modules between PLMs of the same family."
}