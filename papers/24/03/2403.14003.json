{
    "title": "Multi-Modal Hallucination Control by Visual Information Grounding",
    "abstract": "arXiv:2403.14003v1 Announce Type: cross  Abstract: Generative Vision-Language Models (VLMs) are prone to generate plausible-sounding textual answers that, however, are not always grounded in the input image. We investigate this phenomenon, usually referred to as \"hallucination\" and show that it stems from an excessive reliance on the language prior. In particular, we show that as more tokens are generated, the reliance on the visual prompt decreases, and this behavior strongly correlates with the emergence of hallucinations. To reduce hallucinations, we introduce Multi-Modal Mutual-Information Decoding (M3ID), a new sampling method for prompt amplification. M3ID amplifies the influence of the reference image over the language prior, hence favoring the generation of tokens with higher mutual information with the visual prompt. M3ID can be applied to any pre-trained autoregressive VLM at inference time without necessitating further training and with minimal computational overhead. If tra",
    "link": "https://arxiv.org/abs/2403.14003",
    "context": "Title: Multi-Modal Hallucination Control by Visual Information Grounding\nAbstract: arXiv:2403.14003v1 Announce Type: cross  Abstract: Generative Vision-Language Models (VLMs) are prone to generate plausible-sounding textual answers that, however, are not always grounded in the input image. We investigate this phenomenon, usually referred to as \"hallucination\" and show that it stems from an excessive reliance on the language prior. In particular, we show that as more tokens are generated, the reliance on the visual prompt decreases, and this behavior strongly correlates with the emergence of hallucinations. To reduce hallucinations, we introduce Multi-Modal Mutual-Information Decoding (M3ID), a new sampling method for prompt amplification. M3ID amplifies the influence of the reference image over the language prior, hence favoring the generation of tokens with higher mutual information with the visual prompt. M3ID can be applied to any pre-trained autoregressive VLM at inference time without necessitating further training and with minimal computational overhead. If tra",
    "path": "papers/24/03/2403.14003.json",
    "total_tokens": 861,
    "translated_title": "多模态视觉信息基础下的幻觉控制",
    "translated_abstract": "生成式视觉-语言模型（VLMs）往往会生成听起来合理的文本答案，但这些答案并不总是与输入图像相匹配。我们研究了这种现象，通常称为“幻觉”，并表明它源于对语言先验的过度依赖。特别是，我们发现随着生成的标记数量增加，对视觉提示的依赖性减少，这种行为与幻觉的出现强烈相关。为了减少幻觉，我们引入了一种新的抽样方法Multi-Modal Mutual-Information Decoding（M3ID）用于提示放大。M3ID增加了参考图像对语言先验的影响，从而有利于生成与视觉提示具有更高互信息的标记。M3ID可以应用于任何预训练的自回归VLM，在推断阶段而无需进行进一步的训练，并且计算开销最小。",
    "tldr": "引入了一种新的抽样方法M3ID来减少生成式视觉-语言模型中的幻觉，通过放大参考图像对语言先验的影响，从而更好地生成与视觉提示相关的标记。",
    "en_tdlr": "Introduced a new sampling method M3ID to reduce hallucinations in generative vision-language models by amplifying the influence of the reference image over the language prior, resulting in better generation of tokens related to the visual prompt."
}