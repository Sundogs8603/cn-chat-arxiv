{
    "title": "VidLA: Video-Language Alignment at Scale",
    "abstract": "arXiv:2403.14870v1 Announce Type: cross  Abstract: In this paper, we propose VidLA, an approach for video-language alignment at scale. There are two major limitations of previous video-language alignment approaches. First, they do not capture both short-range and long-range temporal dependencies and typically employ complex hierarchical deep network architectures that are hard to integrate with existing pretrained image-text foundation models. To effectively address this limitation, we instead keep the network architecture simple and use a set of data tokens that operate at different temporal resolutions in a hierarchical manner, accounting for the temporally hierarchical nature of videos. By employing a simple two-tower architecture, we are able to initialize our video-language model with pretrained image-text foundation models, thereby boosting the final performance. Second, existing video-language alignment works struggle due to the lack of semantically aligned large-scale training ",
    "link": "https://arxiv.org/abs/2403.14870",
    "context": "Title: VidLA: Video-Language Alignment at Scale\nAbstract: arXiv:2403.14870v1 Announce Type: cross  Abstract: In this paper, we propose VidLA, an approach for video-language alignment at scale. There are two major limitations of previous video-language alignment approaches. First, they do not capture both short-range and long-range temporal dependencies and typically employ complex hierarchical deep network architectures that are hard to integrate with existing pretrained image-text foundation models. To effectively address this limitation, we instead keep the network architecture simple and use a set of data tokens that operate at different temporal resolutions in a hierarchical manner, accounting for the temporally hierarchical nature of videos. By employing a simple two-tower architecture, we are able to initialize our video-language model with pretrained image-text foundation models, thereby boosting the final performance. Second, existing video-language alignment works struggle due to the lack of semantically aligned large-scale training ",
    "path": "papers/24/03/2403.14870.json",
    "total_tokens": 810,
    "translated_title": "VidLA: 规模化视频语言对齐",
    "translated_abstract": "在这篇论文中，我们提出了VidLA，一种用于规模化视频语言对齐的方法。我们解决了以往视频语言对齐方法的两个主要局限。首先，它们没有捕捉到短程和长程时间依赖关系，并且通常采用复杂的分层深度网络架构，难以与现有的预训练图像-文本基础模型集成。为了有效解决这一限制，我们保持网络架构简单，使用一组在不同时间分辨率下以分层方式运行的数据令牌，从而考虑视频的时间分层性质。通过使用简单的双塔架构，我们能够使用预训练的图像-文本基础模型初始化我们的视频-语言模型，从而提高最终性能。",
    "tldr": "VidLA 提出了一种规模化视频语言对齐方法，通过简化网络架构和使用分层数据令牌来捕捉短程和长程时间依赖关系，从而成功融合预训练图像-文本基础模型，提高了最终性能。"
}