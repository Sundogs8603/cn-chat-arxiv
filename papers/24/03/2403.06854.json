{
    "title": "Quantifying the Sensitivity of Inverse Reinforcement Learning to Misspecification",
    "abstract": "arXiv:2403.06854v1 Announce Type: new  Abstract: Inverse reinforcement learning (IRL) aims to infer an agent's preferences (represented as a reward function $R$) from their behaviour (represented as a policy $\\pi$). To do this, we need a behavioural model of how $\\pi$ relates to $R$. In the current literature, the most common behavioural models are optimality, Boltzmann-rationality, and causal entropy maximisation. However, the true relationship between a human's preferences and their behaviour is much more complex than any of these behavioural models. This means that the behavioural models are misspecified, which raises the concern that they may lead to systematic errors if applied to real data. In this paper, we analyse how sensitive the IRL problem is to misspecification of the behavioural model. Specifically, we provide necessary and sufficient conditions that completely characterise how the observed data may differ from the assumed behavioural model without incurring an error abov",
    "link": "https://arxiv.org/abs/2403.06854",
    "context": "Title: Quantifying the Sensitivity of Inverse Reinforcement Learning to Misspecification\nAbstract: arXiv:2403.06854v1 Announce Type: new  Abstract: Inverse reinforcement learning (IRL) aims to infer an agent's preferences (represented as a reward function $R$) from their behaviour (represented as a policy $\\pi$). To do this, we need a behavioural model of how $\\pi$ relates to $R$. In the current literature, the most common behavioural models are optimality, Boltzmann-rationality, and causal entropy maximisation. However, the true relationship between a human's preferences and their behaviour is much more complex than any of these behavioural models. This means that the behavioural models are misspecified, which raises the concern that they may lead to systematic errors if applied to real data. In this paper, we analyse how sensitive the IRL problem is to misspecification of the behavioural model. Specifically, we provide necessary and sufficient conditions that completely characterise how the observed data may differ from the assumed behavioural model without incurring an error abov",
    "path": "papers/24/03/2403.06854.json",
    "total_tokens": 817,
    "translated_title": "量化逆强化学习对误差规定的敏感性",
    "translated_abstract": "逆强化学习（IRL）旨在从代理的行为（表示为策略$\\pi$）中推断其偏好（表示为奖励函数$R$）。为此，我们需要一个描述$\\pi$与$R$关系的行为模型。当前文献中，最常见的行为模型是最优性、Boltzmann-理性和因果熵最大化。然而，人类偏好与其行为之间的真实关系要比任何这些行为模型复杂得多。这意味着行为模型存在规定错误的可能性，从而引发对真实数据的系统误差担忧。本文分析了IRL问题对行为模型误差的敏感性。具体而言，我们提供了完全描述观测数据如何可能与假定的行为模型不同而不会产生错误的必要和充分条件。",
    "tldr": "本文分析了逆强化学习问题对行为模型误差的敏感性，并提供了观测数据与假定行为模型不同但不引发错误的条件。",
    "en_tdlr": "The paper analyzes the sensitivity of Inverse Reinforcement Learning to behavioral model misspecification, providing necessary and sufficient conditions for how observed data may differ from assumed behavioral model without incurring errors."
}