{
    "title": "CoLLEGe: Concept Embedding Generation for Large Language Models",
    "abstract": "arXiv:2403.15362v1 Announce Type: cross  Abstract: Current language models are unable to quickly learn new concepts on the fly, often requiring a more involved finetuning process to learn robustly. Prompting in-context is not robust to context distractions, and often fails to confer much information about the new concepts. Classic methods for few-shot word learning in NLP, relying on global word vectors, are less applicable to large language models. In this paper, we introduce a novel approach named CoLLEGe (Concept Learning with Language Embedding Generation) to modernize few-shot concept learning. CoLLEGe is a meta-learning framework capable of generating flexible embeddings for new concepts using a small number of example sentences or definitions. Our primary meta-learning objective is simply to facilitate a language model to make next word predictions in forthcoming sentences, making it compatible with language model pretraining. We design a series of tasks to test new concept lear",
    "link": "https://arxiv.org/abs/2403.15362",
    "context": "Title: CoLLEGe: Concept Embedding Generation for Large Language Models\nAbstract: arXiv:2403.15362v1 Announce Type: cross  Abstract: Current language models are unable to quickly learn new concepts on the fly, often requiring a more involved finetuning process to learn robustly. Prompting in-context is not robust to context distractions, and often fails to confer much information about the new concepts. Classic methods for few-shot word learning in NLP, relying on global word vectors, are less applicable to large language models. In this paper, we introduce a novel approach named CoLLEGe (Concept Learning with Language Embedding Generation) to modernize few-shot concept learning. CoLLEGe is a meta-learning framework capable of generating flexible embeddings for new concepts using a small number of example sentences or definitions. Our primary meta-learning objective is simply to facilitate a language model to make next word predictions in forthcoming sentences, making it compatible with language model pretraining. We design a series of tasks to test new concept lear",
    "path": "papers/24/03/2403.15362.json",
    "total_tokens": 731,
    "translated_title": "CoLLEGe: 大型语言模型的概念嵌入生成",
    "translated_abstract": "当前语言模型无法快速学习新概念，通常需要更复杂的微调过程才能学习得更稳健。本文引入了一种名为CoLLEGe（Concept Learning with Language Embedding Generation）的新方法，用于现代化的少样本概念学习。CoLLEGe是一个元学习框架，能够使用少量示例句子或定义生成新概念的灵活嵌入。我们的主要元学习目标只是促进语言模型在随后的句子中进行下一个词预测，使其与语言模型的预训练兼容。",
    "tldr": "CoLLEGe是一个元学习框架，能够为大型语言模型生成灵活的新概念嵌入，用于现代化少样本概念学习。",
    "en_tdlr": "CoLLEGe is a meta-learning framework that generates flexible embeddings for new concepts in large language models, modernizing few-shot concept learning."
}