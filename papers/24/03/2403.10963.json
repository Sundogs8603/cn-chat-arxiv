{
    "title": "Pointer-Generator Networks for Low-Resource Machine Translation: Don't Copy That!",
    "abstract": "arXiv:2403.10963v1 Announce Type: new  Abstract: While Transformer-based neural machine translation (NMT) is very effective in high-resource settings, many languages lack the necessary large parallel corpora to benefit from it. In the context of low-resource (LR) MT between two closely-related languages, a natural intuition is to seek benefits from structural \"shortcuts\", such as copying subwords from the source to the target, given that such language pairs often share a considerable number of identical words, cognates, and borrowings. We test Pointer-Generator Networks for this purpose for six language pairs over a variety of resource ranges, and find weak improvements for most settings. However, analysis shows that the model does not show greater improvements for closely-related vs. more distant language pairs, or for lower resource ranges, and that the models do not exhibit the expected usage of the mechanism for shared subwords. Our discussion of the reasons for this behaviour high",
    "link": "https://arxiv.org/abs/2403.10963",
    "context": "Title: Pointer-Generator Networks for Low-Resource Machine Translation: Don't Copy That!\nAbstract: arXiv:2403.10963v1 Announce Type: new  Abstract: While Transformer-based neural machine translation (NMT) is very effective in high-resource settings, many languages lack the necessary large parallel corpora to benefit from it. In the context of low-resource (LR) MT between two closely-related languages, a natural intuition is to seek benefits from structural \"shortcuts\", such as copying subwords from the source to the target, given that such language pairs often share a considerable number of identical words, cognates, and borrowings. We test Pointer-Generator Networks for this purpose for six language pairs over a variety of resource ranges, and find weak improvements for most settings. However, analysis shows that the model does not show greater improvements for closely-related vs. more distant language pairs, or for lower resource ranges, and that the models do not exhibit the expected usage of the mechanism for shared subwords. Our discussion of the reasons for this behaviour high",
    "path": "papers/24/03/2403.10963.json",
    "total_tokens": 859,
    "translated_title": "Pointer-Generator网络用于低资源机器翻译：不要复制那个！",
    "translated_abstract": "虽然基于Transformer的神经机器翻译（NMT）在高资源环境中非常有效，但许多语言缺乏必要的大规模平行语料库来受益。在两种密切相关语言之间的低资源（LR）机器翻译中，一种自然的直觉是寻求从结构“捷径”中获益，例如从源语言复制子词到目标语言，因为这样的语言对通常共享相当数量的相同单词、同源词和借词。我们测试了针对六种语言对的指针生成器网络在各种资源范围下的用途，并发现在大多数情况下都有轻微改进。然而，分析显示，模型对于密切相关的语言对与较远的语言对，或者资源范围较低与较高的语言对并没有展现出更大的改进，并且模型并未展示出对于共享子词机制的预期用法。我们讨论了这种行为的原因。",
    "tldr": "Pointer-Generator Networks在低资源机器翻译中未展现出预期的优势，模型在不同资源范围和语言之间的关系下表现一般。",
    "en_tdlr": "Pointer-Generator Networks do not show the expected advantages in low-resource machine translation, with the model performing similarly across different resource ranges and language relations."
}