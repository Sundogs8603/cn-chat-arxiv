{
    "title": "Reasoning in Transformers - Mitigating Spurious Correlations and Reasoning Shortcuts",
    "abstract": "arXiv:2403.11314v1 Announce Type: cross  Abstract: Transformer language models are neural networks used for a wide variety of tasks concerning natural language, including some that also require logical reasoning. However, a transformer model may easily learn spurious patterns in the data, short-circuiting actual reasoning. In this paper we investigate to what extent transformers can be trained to a) approximate reasoning in propositional logic while b) avoiding known reasoning shortcuts via spurious correlations in the training data. To do so, we use a dataset with known spurious correlation between truth and e.g. the number of rules in the problem. We augment the data with proofs, and train two models: a generative transformer, WP-BART, trained on problems and their whole proofs, and a neuro-symbolic model, SIP-BART, trained on individual proof steps and combining the generative transformer model BART with a symbolic proof checker. We find that SIP-BART succeeds in avoiding reasoning ",
    "link": "https://arxiv.org/abs/2403.11314",
    "context": "Title: Reasoning in Transformers - Mitigating Spurious Correlations and Reasoning Shortcuts\nAbstract: arXiv:2403.11314v1 Announce Type: cross  Abstract: Transformer language models are neural networks used for a wide variety of tasks concerning natural language, including some that also require logical reasoning. However, a transformer model may easily learn spurious patterns in the data, short-circuiting actual reasoning. In this paper we investigate to what extent transformers can be trained to a) approximate reasoning in propositional logic while b) avoiding known reasoning shortcuts via spurious correlations in the training data. To do so, we use a dataset with known spurious correlation between truth and e.g. the number of rules in the problem. We augment the data with proofs, and train two models: a generative transformer, WP-BART, trained on problems and their whole proofs, and a neuro-symbolic model, SIP-BART, trained on individual proof steps and combining the generative transformer model BART with a symbolic proof checker. We find that SIP-BART succeeds in avoiding reasoning ",
    "path": "papers/24/03/2403.11314.json",
    "total_tokens": 853,
    "translated_title": "在Transformer中进行推理-减少伪相关性和推理捷径",
    "translated_abstract": "Transformer语言模型是用于处理自然语言的神经网络，在许多需要逻辑推理的任务中被使用。然而，Transformer模型可能会轻易学习到数据中的伪模式，从而绕过实际推理过程。本文研究了Transformer在多大程度上可以被训练来a) 近似命题逻辑推理，同时b) 避免通过训练数据中的伪相关性引起的已知推理捷径。为此，我们使用了一个数据集，其中真实性与问题中规则数量等之间存在已知的伪相关性。我们通过证明增强了数据，并训练了两个模型：一个生成式Transformer，WP-BART，它在问题及其完整证明上进行训练；一个神经符号模型，SIP-BART，它在单个证明步骤上训练，并将生成式Transformer模型BART与符号推理检查器结合起来。我们发现SIP-BART成功地避免了推理捷径。",
    "tldr": "这项研究探讨了如何在Transformer模型中进行逻辑推理，通过在数据集中引入证明来训练两种模型，成功避免了伪相关性和推理捷径。"
}