{
    "title": "Repoformer: Selective Retrieval for Repository-Level Code Completion",
    "abstract": "arXiv:2403.10059v1 Announce Type: cross  Abstract: Recent advances in retrieval-augmented generation (RAG) have initiated a new era in repository-level code completion. However, the invariable use of retrieval in existing methods exposes issues in both efficiency and robustness, with a large proportion of the retrieved contexts proving unhelpful or harmful to code language models (code LMs). To tackle the challenges, this paper proposes a selective RAG framework where retrieval is avoided when unnecessary. To power this framework, we design a self-supervised learning approach that enables a code LM to accurately self-evaluate whether retrieval can improve its output quality and robustly leverage the potentially noisy retrieved contexts. Using this LM as both the selective retrieval policy and the generation model, our framework consistently outperforms the state-of-the-art prompting with an invariable retrieval approach on diverse benchmarks including RepoEval, CrossCodeEval, and a new",
    "link": "https://arxiv.org/abs/2403.10059",
    "context": "Title: Repoformer: Selective Retrieval for Repository-Level Code Completion\nAbstract: arXiv:2403.10059v1 Announce Type: cross  Abstract: Recent advances in retrieval-augmented generation (RAG) have initiated a new era in repository-level code completion. However, the invariable use of retrieval in existing methods exposes issues in both efficiency and robustness, with a large proportion of the retrieved contexts proving unhelpful or harmful to code language models (code LMs). To tackle the challenges, this paper proposes a selective RAG framework where retrieval is avoided when unnecessary. To power this framework, we design a self-supervised learning approach that enables a code LM to accurately self-evaluate whether retrieval can improve its output quality and robustly leverage the potentially noisy retrieved contexts. Using this LM as both the selective retrieval policy and the generation model, our framework consistently outperforms the state-of-the-art prompting with an invariable retrieval approach on diverse benchmarks including RepoEval, CrossCodeEval, and a new",
    "path": "papers/24/03/2403.10059.json",
    "total_tokens": 832,
    "translated_title": "Repoformer：面向存储库级代码补全的选择性检索",
    "translated_abstract": "arXiv:2403.10059v1 公告类型：跨文摘：检索增强生成（RAG）的最新进展开启了存储库级代码补全的新时代。但是，现有方法中检索的不变使用暴露了效率和鲁棒性方面的问题，大部分检索到的上下文对于代码语言模型（code LM）来说既无效又有害。为了解决这些挑战，本文提出了一种选择性RAG框架，在不必要时避免使用检索。为了支持这一框架，我们设计了一种自监督学习方法，使代码LM能够准确自我评估检索是否可以提高其输出质量，并能够稳健地利用潜在含噪声的检索上下文。使用这种LM作为选择性检索策略和生成模型，我们的框架在包括RepoEval、CrossCodeEval和一个新...",
    "tldr": "本文提出了一种选择性的检索增强生成框架，通过自监督学习方法使代码LM能够避免不必要的检索，并在各种基准测试上始终优于现有方法。",
    "en_tdlr": "This paper presents a selective retrieval-augmented generation framework, enabling a code LM to avoid unnecessary retrieval through self-supervised learning, and consistently outperform existing methods on diverse benchmarks."
}