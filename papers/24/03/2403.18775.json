{
    "title": "ImageNet-D: Benchmarking Neural Network Robustness on Diffusion Synthetic Object",
    "abstract": "arXiv:2403.18775v1 Announce Type: cross  Abstract: We establish rigorous benchmarks for visual perception robustness. Synthetic images such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific type of evaluation over synthetic corruptions, backgrounds, and textures, yet those robustness benchmarks are restricted in specified variations and have low synthetic quality. In this work, we introduce generative model as a data source for synthesizing hard images that benchmark deep models' robustness. Leveraging diffusion models, we are able to generate images with more diversified backgrounds, textures, and materials than any prior work, where we term this benchmark as ImageNet-D. Experimental results show that ImageNet-D results in a significant accuracy drop to a range of vision models, from the standard ResNet visual classifier to the latest foundation models like CLIP and MiniGPT-4, significantly reducing their accuracy by up to 60\\%. Our work suggests that diffusion models ",
    "link": "https://arxiv.org/abs/2403.18775",
    "context": "Title: ImageNet-D: Benchmarking Neural Network Robustness on Diffusion Synthetic Object\nAbstract: arXiv:2403.18775v1 Announce Type: cross  Abstract: We establish rigorous benchmarks for visual perception robustness. Synthetic images such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific type of evaluation over synthetic corruptions, backgrounds, and textures, yet those robustness benchmarks are restricted in specified variations and have low synthetic quality. In this work, we introduce generative model as a data source for synthesizing hard images that benchmark deep models' robustness. Leveraging diffusion models, we are able to generate images with more diversified backgrounds, textures, and materials than any prior work, where we term this benchmark as ImageNet-D. Experimental results show that ImageNet-D results in a significant accuracy drop to a range of vision models, from the standard ResNet visual classifier to the latest foundation models like CLIP and MiniGPT-4, significantly reducing their accuracy by up to 60\\%. Our work suggests that diffusion models ",
    "path": "papers/24/03/2403.18775.json",
    "total_tokens": 937,
    "translated_title": "ImageNet-D: 在扩散合成对象上评估神经网络的鲁棒性基准",
    "translated_abstract": "我们为视觉感知鲁棒性建立了严格的基准。合成图像，如ImageNet-C、ImageNet-9和Stylized ImageNet，提供了对合成破坏、背景和纹理的特定类型评估，然而这些鲁棒性基准受限于指定的变体，并具有较低的合成质量。在这项工作中，我们引入生成模型作为合成难图像的数据源来评估深度模型的鲁棒性。利用扩散模型，我们能够生成比任何先前工作更多样化的背景、纹理和材料图像，我们将这个基准称为ImageNet-D。实验结果表明，ImageNet-D导致了一系列视觉模型的显著准确性下降，从标准ResNet视觉分类器到最新的基础模型，如CLIP和MiniGPT-4，将它们的准确性显著降低了高达60\\%。我们的工作表明，扩散模型...",
    "tldr": "本研究引入生成模型作为数据源，通过扩散模型生成了更多样化的背景、纹理和材料图像，建立了ImageNet-D基准评估深度模型的鲁棒性，在一系列视觉模型中显著降低了准确性高达60%。",
    "en_tdlr": "Introducing generative models as data sources, generating diversified background, texture, and material images through diffusion models, establishing the ImageNet-D benchmark for evaluating deep model robustness, significantly reducing accuracy by up to 60% in a range of vision models."
}