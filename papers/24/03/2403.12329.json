{
    "title": "FedFisher: Leveraging Fisher Information for One-Shot Federated Learning",
    "abstract": "arXiv:2403.12329v1 Announce Type: new  Abstract: Standard federated learning (FL) algorithms typically require multiple rounds of communication between the server and the clients, which has several drawbacks, including requiring constant network connectivity, repeated investment of computational resources, and susceptibility to privacy attacks. One-Shot FL is a new paradigm that aims to address this challenge by enabling the server to train a global model in a single round of communication. In this work, we present FedFisher, a novel algorithm for one-shot FL that makes use of Fisher information matrices computed on local client models, motivated by a Bayesian perspective of FL. First, we theoretically analyze FedFisher for two-layer over-parameterized ReLU neural networks and show that the error of our one-shot FedFisher global model becomes vanishingly small as the width of the neural networks and amount of local training at clients increases. Next, we propose practical variants of F",
    "link": "https://arxiv.org/abs/2403.12329",
    "context": "Title: FedFisher: Leveraging Fisher Information for One-Shot Federated Learning\nAbstract: arXiv:2403.12329v1 Announce Type: new  Abstract: Standard federated learning (FL) algorithms typically require multiple rounds of communication between the server and the clients, which has several drawbacks, including requiring constant network connectivity, repeated investment of computational resources, and susceptibility to privacy attacks. One-Shot FL is a new paradigm that aims to address this challenge by enabling the server to train a global model in a single round of communication. In this work, we present FedFisher, a novel algorithm for one-shot FL that makes use of Fisher information matrices computed on local client models, motivated by a Bayesian perspective of FL. First, we theoretically analyze FedFisher for two-layer over-parameterized ReLU neural networks and show that the error of our one-shot FedFisher global model becomes vanishingly small as the width of the neural networks and amount of local training at clients increases. Next, we propose practical variants of F",
    "path": "papers/24/03/2403.12329.json",
    "total_tokens": 925,
    "translated_title": "FedFisher：利用Fisher信息进行一次性联邦学习",
    "translated_abstract": "标准的联邦学习(FL)算法通常需要服务器和客户端之间的多轮通信，这具有几个缺点，包括需要恒定的网络连通性，重复投入计算资源，以及容易受到隐私攻击的影响。一次性FL是一种新的范例，旨在通过使服务器在一轮通信中训练全局模型来解决这一挑战。在这项工作中，我们提出了FedFisher，一种新颖的用于一次性FL的算法，该算法利用在本地客户端模型上计算的Fisher信息矩阵，受到FL的贝叶斯视角的启发。首先，我们从两层过参数化的ReLU神经网络的理论角度对FedFisher进行了分析，并展示了我们的一次性FedFisher全局模型的误差会在神经网络的宽度和客户端本地训练量增加时变得无限小。接下来，我们提出了实用的F的变种",
    "tldr": "该论文提出了FedFisher算法，通过利用Fisher信息矩阵进行一次性联邦学习，能够在一次通信中训练出全局模型，并在理论上证明了随着神经网络宽度和客户端本地训练量的增加，FedFisher全局模型的误差会变得非常小。",
    "en_tdlr": "The paper introduces the FedFisher algorithm for one-shot federated learning, leveraging Fisher information matrices to train a global model in a single communication round. Theoretically, it shows that the error of the FedFisher global model becomes vanishingly small as the width of neural networks and local training at clients increases."
}