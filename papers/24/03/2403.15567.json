{
    "title": "Do not trust what you trust: Miscalibration in Semi-supervised Learning",
    "abstract": "arXiv:2403.15567v1 Announce Type: new  Abstract: State-of-the-art semi-supervised learning (SSL) approaches rely on highly confident predictions to serve as pseudo-labels that guide the training on unlabeled samples. An inherent drawback of this strategy stems from the quality of the uncertainty estimates, as pseudo-labels are filtered only based on their degree of uncertainty, regardless of the correctness of their predictions. Thus, assessing and enhancing the uncertainty of network predictions is of paramount importance in the pseudo-labeling process. In this work, we empirically demonstrate that SSL methods based on pseudo-labels are significantly miscalibrated, and formally demonstrate the minimization of the min-entropy, a lower bound of the Shannon entropy, as a potential cause for miscalibration. To alleviate this issue, we integrate a simple penalty term, which enforces the logit distances of the predictions on unlabeled samples to remain low, preventing the network prediction",
    "link": "https://arxiv.org/abs/2403.15567",
    "context": "Title: Do not trust what you trust: Miscalibration in Semi-supervised Learning\nAbstract: arXiv:2403.15567v1 Announce Type: new  Abstract: State-of-the-art semi-supervised learning (SSL) approaches rely on highly confident predictions to serve as pseudo-labels that guide the training on unlabeled samples. An inherent drawback of this strategy stems from the quality of the uncertainty estimates, as pseudo-labels are filtered only based on their degree of uncertainty, regardless of the correctness of their predictions. Thus, assessing and enhancing the uncertainty of network predictions is of paramount importance in the pseudo-labeling process. In this work, we empirically demonstrate that SSL methods based on pseudo-labels are significantly miscalibrated, and formally demonstrate the minimization of the min-entropy, a lower bound of the Shannon entropy, as a potential cause for miscalibration. To alleviate this issue, we integrate a simple penalty term, which enforces the logit distances of the predictions on unlabeled samples to remain low, preventing the network prediction",
    "path": "papers/24/03/2403.15567.json",
    "total_tokens": 828,
    "translated_title": "不要相信你所信任的：半监督学习中的误校准问题",
    "translated_abstract": "最先进的半监督学习（SSL）方法依赖于高度自信的预测，作为伪标签，引导对未标记样本的训练。这种策略的固有缺点来自于不确定性估计的质量，因为伪标签仅基于它们的不确定性程度进行过滤，而不考虑其预测的正确性。因此，在伪标记过程中评估和增强网络预测的不确定性非常重要。在这项工作中，我们在实证上证明了基于伪标签的SSL方法存在显著的误校准问题，并形式化地证明了最小化最小熵（Shannon熵的下界）可能是误校准的一个潜在原因。为了缓解这个问题，我们集成了一个简单的惩罚项，强制未标记样本的预测的logit距离保持较低，从而防止网络预测",
    "tldr": "SSL方法基于伪标签存在显著的误校准问题，通过最小化最小熵和引入一个惩罚项，可以缓解这一问题。"
}