{
    "title": "Understanding the training of infinitely deep and wide ResNets with Conditional Optimal Transport",
    "abstract": "arXiv:2403.12887v1 Announce Type: new  Abstract: We study the convergence of gradient flow for the training of deep neural networks. If Residual Neural Networks are a popular example of very deep architectures, their training constitutes a challenging optimization problem due notably to the non-convexity and the non-coercivity of the objective. Yet, in applications, those tasks are successfully solved by simple optimization algorithms such as gradient descent. To better understand this phenomenon, we focus here on a ``mean-field'' model of infinitely deep and arbitrarily wide ResNet, parameterized by probability measures over the product set of layers and parameters and with constant marginal on the set of layers. Indeed, in the case of shallow neural networks, mean field models have proven to benefit from simplified loss-landscapes and good theoretical guarantees when trained with gradient flow for the Wasserstein metric on the set of probability measures. Motivated by this approach, ",
    "link": "https://arxiv.org/abs/2403.12887",
    "context": "Title: Understanding the training of infinitely deep and wide ResNets with Conditional Optimal Transport\nAbstract: arXiv:2403.12887v1 Announce Type: new  Abstract: We study the convergence of gradient flow for the training of deep neural networks. If Residual Neural Networks are a popular example of very deep architectures, their training constitutes a challenging optimization problem due notably to the non-convexity and the non-coercivity of the objective. Yet, in applications, those tasks are successfully solved by simple optimization algorithms such as gradient descent. To better understand this phenomenon, we focus here on a ``mean-field'' model of infinitely deep and arbitrarily wide ResNet, parameterized by probability measures over the product set of layers and parameters and with constant marginal on the set of layers. Indeed, in the case of shallow neural networks, mean field models have proven to benefit from simplified loss-landscapes and good theoretical guarantees when trained with gradient flow for the Wasserstein metric on the set of probability measures. Motivated by this approach, ",
    "path": "papers/24/03/2403.12887.json",
    "total_tokens": 920,
    "translated_title": "理解具有条件最优运输的无限深度和宽度ResNets的训练",
    "translated_abstract": "我们研究了深度神经网络训练的梯度流的收敛性。 如果残差神经网络是非常深的架构的一个常见例子，那么由于目标的非凸性和非强凸性，它们的训练构成了一个具有挑战性的优化问题。 Yet, 在应用中，这些任务可以通过诸如梯度下降等简单的优化算法成功解决。 为了更好地理解这一现象，我们在这里专注于一个无限深度和任意宽度的ResNet的“均场”模型，其参数由层和参数的乘积集上的概率测度参数化，并在层集上具有常数边际。 实际上，在浅层神经网络的情况下，均场模型已被证明在用梯度流训练概率测度集上的Wasserstein度量时受益于简化的损失景观和良好的理论保证。 受这种方法的启发。",
    "tldr": "该研究通过研究无限深度和任意宽度的ResNet的“均场”模型，探讨了深度神经网络训练过程中的梯度流收敛性，以更好地理解简单优化算法如何成功解决这一具有挑战性的优化问题。",
    "en_tdlr": "This study investigates the convergence of gradient flow during the training process of infinitely deep and arbitrarily wide ResNets through a \"mean-field\" model, aiming to better understand how simple optimization algorithms can successfully tackle this challenging optimization problem."
}