{
    "title": "Exploring Large Language Models and Hierarchical Frameworks for Classification of Large Unstructured Legal Documents",
    "abstract": "arXiv:2403.06872v1 Announce Type: cross  Abstract: Legal judgment prediction suffers from the problem of long case documents exceeding tens of thousands of words, in general, and having a non-uniform structure. Predicting judgments from such documents becomes a challenging task, more so on documents with no structural annotation. We explore the classification of these large legal documents and their lack of structural information with a deep-learning-based hierarchical framework which we call MESc; \"Multi-stage Encoder-based Supervised with-clustering\"; for judgment prediction. Specifically, we divide a document into parts to extract their embeddings from the last four layers of a custom fine-tuned Large Language Model, and try to approximate their structure through unsupervised clustering. Which we use in another set of transformer encoder layers to learn the inter-chunk representations. We analyze the adaptability of Large Language Models (LLMs) with multi-billion parameters (GPT-Neo",
    "link": "https://arxiv.org/abs/2403.06872",
    "context": "Title: Exploring Large Language Models and Hierarchical Frameworks for Classification of Large Unstructured Legal Documents\nAbstract: arXiv:2403.06872v1 Announce Type: cross  Abstract: Legal judgment prediction suffers from the problem of long case documents exceeding tens of thousands of words, in general, and having a non-uniform structure. Predicting judgments from such documents becomes a challenging task, more so on documents with no structural annotation. We explore the classification of these large legal documents and their lack of structural information with a deep-learning-based hierarchical framework which we call MESc; \"Multi-stage Encoder-based Supervised with-clustering\"; for judgment prediction. Specifically, we divide a document into parts to extract their embeddings from the last four layers of a custom fine-tuned Large Language Model, and try to approximate their structure through unsupervised clustering. Which we use in another set of transformer encoder layers to learn the inter-chunk representations. We analyze the adaptability of Large Language Models (LLMs) with multi-billion parameters (GPT-Neo",
    "path": "papers/24/03/2403.06872.json",
    "total_tokens": 833,
    "translated_title": "探索大型语言模型和分层框架用于大型非结构化法律文件的分类",
    "translated_abstract": "法律判决预测受长达数万字的案例文件和非均匀结构的问题困扰，尤其是对于没有结构标注的文件。本研究通过一个基于深度学习的分层框架(MESc)，即“基于多阶段编码器的带聚类的监督学习”，来探索这些大型法律文件的分类和它们缺乏结构信息的情况，用于判决预测。具体来说，我们将文件分成部分，从自定义精调的大型语言模型的最后四层中提取它们的嵌入，并尝试通过无监督聚类来近似它们的结构。然后在另一组变压器编码器层中使用这些表示来学习部分间的表示。我们分析了具有数十亿参数的大型语言模型(LLM)的适应性(GPT-Neo)。",
    "tldr": "使用MESc框架探索大型法律文件的分类，通过大型语言模型提取文件部分的嵌入并使用聚类近似结构，进而预测判决。",
    "en_tdlr": "Exploring classification of large legal documents using the MESc framework, extracting embeddings of document parts with large language models and approximating structure through clustering for judgment prediction."
}