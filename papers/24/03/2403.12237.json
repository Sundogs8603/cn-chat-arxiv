{
    "title": "Efficient Transformer-based Hyper-parameter Optimization for Resource-constrained IoT Environments",
    "abstract": "arXiv:2403.12237v1 Announce Type: cross  Abstract: The hyper-parameter optimization (HPO) process is imperative for finding the best-performing Convolutional Neural Networks (CNNs). The automation process of HPO is characterized by its sizable computational footprint and its lack of transparency; both important factors in a resource-constrained Internet of Things (IoT) environment. In this paper, we address these problems by proposing a novel approach that combines transformer architecture and actor-critic Reinforcement Learning (RL) model, TRL-HPO, equipped with multi-headed attention that enables parallelization and progressive generation of layers. These assumptions are founded empirically by evaluating TRL-HPO on the MNIST dataset and comparing it with state-of-the-art approaches that build CNN models from scratch. The results show that TRL-HPO outperforms the classification results of these approaches by 6.8% within the same time frame, demonstrating the efficiency of TRL-HPO for ",
    "link": "https://arxiv.org/abs/2403.12237",
    "context": "Title: Efficient Transformer-based Hyper-parameter Optimization for Resource-constrained IoT Environments\nAbstract: arXiv:2403.12237v1 Announce Type: cross  Abstract: The hyper-parameter optimization (HPO) process is imperative for finding the best-performing Convolutional Neural Networks (CNNs). The automation process of HPO is characterized by its sizable computational footprint and its lack of transparency; both important factors in a resource-constrained Internet of Things (IoT) environment. In this paper, we address these problems by proposing a novel approach that combines transformer architecture and actor-critic Reinforcement Learning (RL) model, TRL-HPO, equipped with multi-headed attention that enables parallelization and progressive generation of layers. These assumptions are founded empirically by evaluating TRL-HPO on the MNIST dataset and comparing it with state-of-the-art approaches that build CNN models from scratch. The results show that TRL-HPO outperforms the classification results of these approaches by 6.8% within the same time frame, demonstrating the efficiency of TRL-HPO for ",
    "path": "papers/24/03/2403.12237.json",
    "total_tokens": 854,
    "translated_title": "面向资源受限的IoT环境的高效基于Transformer的超参数优化",
    "translated_abstract": "超参数优化（HPO）过程对于找到表现最佳的卷积神经网络（CNNs）至关重要。HPO的自动化过程以其可观的计算占用和缺乏透明度而闻名；这两个因素在资源受限的物联网（IoT）环境中至关重要。本文通过提出一种结合Transformer架构和演员-评论家强化学习（RL）模型的新方法TRL-HPO，旨在解决这些问题，TRL-HPO配备了多头注意力，实现了并行化和渐进生成层。我们通过在MNIST数据集上评估TRL-HPO，并将其与从头开始构建CNN模型的最新方法进行比较，从而从经验上验证了这些假设。结果显示，在相同时间范围内，TRL-HPO的分类结果优于这些方法的结果6.8%，证明了TRL-HPO的高效性。",
    "tldr": "本文提出了一种通过将Transformer架构和演员-评论家强化学习模型相结合的新方法TRL-HPO，在资源受限的IoT环境中实现了高效的超参数优化，该方法在MNIST数据集上表现优良。",
    "en_tdlr": "This paper introduces a novel approach TRL-HPO, which combines Transformer architecture and actor-critic Reinforcement Learning model, achieving efficient hyper-parameter optimization in resource-constrained IoT environments, with superior performance on the MNIST dataset."
}