{
    "title": "The Human Factor in Detecting Errors of Large Language Models: A Systematic Literature Review and Future Research Directions",
    "abstract": "arXiv:2403.09743v1 Announce Type: cross  Abstract: The launch of ChatGPT by OpenAI in November 2022 marked a pivotal moment for Artificial Intelligence, introducing Large Language Models (LLMs) to the mainstream and setting new records in user adoption. LLMs, particularly ChatGPT, trained on extensive internet data, demonstrate remarkable conversational capabilities across various domains, suggesting a significant impact on the workforce. However, these models are susceptible to errors - \"hallucinations\" and omissions, generating incorrect or incomplete information. This poses risks especially in contexts where accuracy is crucial, such as legal compliance, medicine or fine-grained process frameworks.   There are both technical and human solutions to cope with this isse. This paper explores the human factors that enable users to detect errors in LLM outputs, a critical component in mitigating risks associated with their use in professional settings. Understanding these factors is essen",
    "link": "https://arxiv.org/abs/2403.09743",
    "context": "Title: The Human Factor in Detecting Errors of Large Language Models: A Systematic Literature Review and Future Research Directions\nAbstract: arXiv:2403.09743v1 Announce Type: cross  Abstract: The launch of ChatGPT by OpenAI in November 2022 marked a pivotal moment for Artificial Intelligence, introducing Large Language Models (LLMs) to the mainstream and setting new records in user adoption. LLMs, particularly ChatGPT, trained on extensive internet data, demonstrate remarkable conversational capabilities across various domains, suggesting a significant impact on the workforce. However, these models are susceptible to errors - \"hallucinations\" and omissions, generating incorrect or incomplete information. This poses risks especially in contexts where accuracy is crucial, such as legal compliance, medicine or fine-grained process frameworks.   There are both technical and human solutions to cope with this isse. This paper explores the human factors that enable users to detect errors in LLM outputs, a critical component in mitigating risks associated with their use in professional settings. Understanding these factors is essen",
    "path": "papers/24/03/2403.09743.json",
    "total_tokens": 793,
    "translated_title": "大型语言模型中的人为因素：系统文献综述与未来研究方向",
    "translated_abstract": "OpenAI在2022年11月推出的ChatGPT标志着人工智能的一个关键时刻，将大型语言模型（LLMs）引入主流，并在用户采用方面创造了新记录。尤其是ChatGPT，经过广泛的互联网数据训练，展示出在各个领域具有显著的对话能力，暗示对劳动力产生了重大影响。然而，这些模型容易出现错误-“幻觉”和遗漏，产生不正确或不完整的信息。这在准确性至关重要的环境中尤为危险，比如法律合规、医学或精细的流程框架。",
    "tldr": "人工智能领域中，这项研究探索了人类因素在检测大型语言模型的错误输出中的作用，有助于减轻其在专业环境中使用时所带来的风险。",
    "en_tdlr": "This paper explores the human factors that enable users to detect errors in LLM outputs, a critical component in mitigating risks associated with their use in professional settings."
}