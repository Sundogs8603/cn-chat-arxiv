{
    "title": "To Generate or to Retrieve? On the Effectiveness of Artificial Contexts for Medical Open-Domain Question Answering",
    "abstract": "arXiv:2403.01924v1 Announce Type: cross  Abstract: Medical open-domain question answering demands substantial access to specialized knowledge. Recent efforts have sought to decouple knowledge from model parameters, counteracting architectural scaling and allowing for training on common low-resource hardware. The retrieve-then-read paradigm has become ubiquitous, with model predictions grounded on relevant knowledge pieces from external repositories such as PubMed, textbooks, and UMLS. An alternative path, still under-explored but made possible by the advent of domain-specific large language models, entails constructing artificial contexts through prompting. As a result, \"to generate or to retrieve\" is the modern equivalent of Hamlet's dilemma. This paper presents MedGENIE, the first generate-then-read framework for multiple-choice question answering in medicine. We conduct extensive experiments on MedQA-USMLE, MedMCQA, and MMLU, incorporating a practical perspective by assuming a maxim",
    "link": "https://arxiv.org/abs/2403.01924",
    "context": "Title: To Generate or to Retrieve? On the Effectiveness of Artificial Contexts for Medical Open-Domain Question Answering\nAbstract: arXiv:2403.01924v1 Announce Type: cross  Abstract: Medical open-domain question answering demands substantial access to specialized knowledge. Recent efforts have sought to decouple knowledge from model parameters, counteracting architectural scaling and allowing for training on common low-resource hardware. The retrieve-then-read paradigm has become ubiquitous, with model predictions grounded on relevant knowledge pieces from external repositories such as PubMed, textbooks, and UMLS. An alternative path, still under-explored but made possible by the advent of domain-specific large language models, entails constructing artificial contexts through prompting. As a result, \"to generate or to retrieve\" is the modern equivalent of Hamlet's dilemma. This paper presents MedGENIE, the first generate-then-read framework for multiple-choice question answering in medicine. We conduct extensive experiments on MedQA-USMLE, MedMCQA, and MMLU, incorporating a practical perspective by assuming a maxim",
    "path": "papers/24/03/2403.01924.json",
    "total_tokens": 824,
    "translated_title": "生成还是检索？关于人工环境在医学开放域问答效果的研究",
    "translated_abstract": "医学领域的开放域问答需要大量专业知识的支持。近期的努力致力于将知识与模型参数分离，对抗架构规模化，并允许在常见的低资源硬件上进行训练。检索然后阅读的范式已变得普遍，模型预测依赖于来自外部知识库（如PubMed、教科书和UMLS）的相关知识片段。另一条尚未得到充分探索但由于领域特定大型语言模型的出现变得可能的路径是通过提示构建人工环境。因此，“生成还是检索”成为了现代版的哈姆雷特困境。本文提出了MedGENIE，这是医学领域多项选择问答的生成然后阅读框架。我们在MedQA-USMLE、MedMCQA和MMLU上进行了广泛实验，从实践的角度出发，假设最大",
    "tldr": "本文介绍了MedGENIE，这是医学领域多项选择问题回答的第一个生成后阅读框架。",
    "en_tdlr": "This paper presents MedGENIE, the first generate-then-read framework for multiple-choice question answering in medicine."
}