{
    "title": "ILLUMINER: Instruction-tuned Large Language Models as Few-shot Intent Classifier and Slot Filler",
    "abstract": "arXiv:2403.17536v1 Announce Type: new  Abstract: State-of-the-art intent classification (IC) and slot filling (SF) methods often rely on data-intensive deep learning models, limiting their practicality for industry applications. Large language models on the other hand, particularly instruction-tuned models (Instruct-LLMs), exhibit remarkable zero-shot performance across various natural language tasks. This study evaluates Instruct-LLMs on popular benchmark datasets for IC and SF, emphasizing their capacity to learn from fewer examples. We introduce ILLUMINER, an approach framing IC and SF as language generation tasks for Instruct-LLMs, with a more efficient SF-prompting method compared to prior work. A comprehensive comparison with multiple baselines shows that our approach, using the FLAN-T5 11B model, outperforms the state-of-the-art joint IC+SF method and in-context learning with GPT3.5 (175B), particularly in slot filling by 11.1--32.2 percentage points. Additionally, our in-depth ",
    "link": "https://arxiv.org/abs/2403.17536",
    "context": "Title: ILLUMINER: Instruction-tuned Large Language Models as Few-shot Intent Classifier and Slot Filler\nAbstract: arXiv:2403.17536v1 Announce Type: new  Abstract: State-of-the-art intent classification (IC) and slot filling (SF) methods often rely on data-intensive deep learning models, limiting their practicality for industry applications. Large language models on the other hand, particularly instruction-tuned models (Instruct-LLMs), exhibit remarkable zero-shot performance across various natural language tasks. This study evaluates Instruct-LLMs on popular benchmark datasets for IC and SF, emphasizing their capacity to learn from fewer examples. We introduce ILLUMINER, an approach framing IC and SF as language generation tasks for Instruct-LLMs, with a more efficient SF-prompting method compared to prior work. A comprehensive comparison with multiple baselines shows that our approach, using the FLAN-T5 11B model, outperforms the state-of-the-art joint IC+SF method and in-context learning with GPT3.5 (175B), particularly in slot filling by 11.1--32.2 percentage points. Additionally, our in-depth ",
    "path": "papers/24/03/2403.17536.json",
    "total_tokens": 936,
    "translated_title": "ILLUMINER: 指令调整的大型语言模型作为少样本意图分类器和槽位填充器",
    "translated_abstract": "最先进的意图分类（IC）和槽位填充（SF）方法通常依赖于数据密集型的深度学习模型，限制了它们在工业应用中的实用性。另一方面，大型语言模型，特别是指令调整模型（Instruct-LLMs），在各种自然语言任务中表现出卓越的零样本性能。本研究评估了Instruct-LLMs在流行的IC和SF基准数据集上的表现，强调它们从更少示例中学习的能力。我们引入了 ILLUMINER，一种将IC和SF构建为Instruct-LLMs中的语言生成任务的方法，相比之前的工作，具有更高效的SF提示方法。与多个基线方法的全面比较显示，我们的方法使用FLAN-T5 11B模型，在槽位填充方面比最先进的联合IC + SF方法和GPT3.5 (175B)的上下文学习表现更好，槽位填充方面提高了11.1-32.2个百分点。此外，我们的深入研究",
    "tldr": "使用指令调整模型的ILLUMINER方法在意图分类和槽位填充任务上表现出更高效的学习能力，并在槽位填充方面优于目前最先进的方法。",
    "en_tdlr": "ILLUMINER method utilizing instruction-tuned models demonstrates more efficient learning capabilities in intent classification and slot filling tasks, outperforming state-of-the-art methods particularly in slot filling."
}