{
    "title": "S2DM: Sector-Shaped Diffusion Models for Video Generation",
    "abstract": "arXiv:2403.13408v2 Announce Type: replace-cross  Abstract: Diffusion models have achieved great success in image generation. However, when leveraging this idea for video generation, we face significant challenges in maintaining the consistency and continuity across video frames. This is mainly caused by the lack of an effective framework to align frames of videos with desired temporal features while preserving consistent semantic and stochastic features. In this work, we propose a novel Sector-Shaped Diffusion Model (S2DM) whose sector-shaped diffusion region is formed by a set of ray-shaped reverse diffusion processes starting at the same noise point. S2DM can generate a group of intrinsically related data sharing the same semantic and stochastic features while varying on temporal features with appropriate guided conditions. We apply S2DM to video generation tasks, and explore the use of optical flow as temporal conditions. Our experimental results show that S2DM outperforms many exis",
    "link": "https://arxiv.org/abs/2403.13408",
    "context": "Title: S2DM: Sector-Shaped Diffusion Models for Video Generation\nAbstract: arXiv:2403.13408v2 Announce Type: replace-cross  Abstract: Diffusion models have achieved great success in image generation. However, when leveraging this idea for video generation, we face significant challenges in maintaining the consistency and continuity across video frames. This is mainly caused by the lack of an effective framework to align frames of videos with desired temporal features while preserving consistent semantic and stochastic features. In this work, we propose a novel Sector-Shaped Diffusion Model (S2DM) whose sector-shaped diffusion region is formed by a set of ray-shaped reverse diffusion processes starting at the same noise point. S2DM can generate a group of intrinsically related data sharing the same semantic and stochastic features while varying on temporal features with appropriate guided conditions. We apply S2DM to video generation tasks, and explore the use of optical flow as temporal conditions. Our experimental results show that S2DM outperforms many exis",
    "path": "papers/24/03/2403.13408.json",
    "total_tokens": 850,
    "translated_title": "S2DM：面向视频生成的扇形扩散模型",
    "translated_abstract": "扩散模型在图像生成方面取得了巨大成功。然而，当将这一思想应用于视频生成时，我们面临着保持视频帧一致性和连续性的重大挑战。这主要是由于缺乏一个有效的框架来将视频帧与期望的时间特征对齐，同时保持一致的语义和随机特征所致。在本工作中，我们提出了一种新颖的Sector-Shaped Diffusion Model（S2DM），其扇形扩散区域由一组以相同噪声点为起点的射线状反向扩散过程形成。S2DM能够生成一组在语义和随机特征上共享相同特征的内在相关数据，同时在适当的引导条件下在时间特征上变化。我们将S2DM应用于视频生成任务，并探讨了光流作为时间条件的使用。我们的实验结果表明，S2DM优于许多已存在的...",
    "tldr": "S2DM提出了一种新颖的Sector-Shaped Diffusion Model，能够生成具有一致语义和随机特征的一组相关数据，同时在时间特征上变化，在视频生成任务上表现优异。",
    "en_tdlr": "S2DM introduces a novel Sector-Shaped Diffusion Model that can generate a group of intrinsically related data sharing the same semantic and stochastic features while varying on temporal features, showing superior performance in video generation tasks."
}