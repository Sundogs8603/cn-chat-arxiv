{
    "title": "MAMBA: an Effective World Model Approach for Meta-Reinforcement Learning",
    "abstract": "arXiv:2403.09859v1 Announce Type: new  Abstract: Meta-reinforcement learning (meta-RL) is a promising framework for tackling challenging domains requiring efficient exploration. Existing meta-RL algorithms are characterized by low sample efficiency, and mostly focus on low-dimensional task distributions. In parallel, model-based RL methods have been successful in solving partially observable MDPs, of which meta-RL is a special case. In this work, we leverage this success and propose a new model-based approach to meta-RL, based on elements from existing state-of-the-art model-based and meta-RL methods. We demonstrate the effectiveness of our approach on common meta-RL benchmark domains, attaining greater return with better sample efficiency (up to $15\\times$) while requiring very little hyperparameter tuning. In addition, we validate our approach on a slate of more challenging, higher-dimensional domains, taking a step towards real-world generalizing agents.",
    "link": "https://arxiv.org/abs/2403.09859",
    "context": "Title: MAMBA: an Effective World Model Approach for Meta-Reinforcement Learning\nAbstract: arXiv:2403.09859v1 Announce Type: new  Abstract: Meta-reinforcement learning (meta-RL) is a promising framework for tackling challenging domains requiring efficient exploration. Existing meta-RL algorithms are characterized by low sample efficiency, and mostly focus on low-dimensional task distributions. In parallel, model-based RL methods have been successful in solving partially observable MDPs, of which meta-RL is a special case. In this work, we leverage this success and propose a new model-based approach to meta-RL, based on elements from existing state-of-the-art model-based and meta-RL methods. We demonstrate the effectiveness of our approach on common meta-RL benchmark domains, attaining greater return with better sample efficiency (up to $15\\times$) while requiring very little hyperparameter tuning. In addition, we validate our approach on a slate of more challenging, higher-dimensional domains, taking a step towards real-world generalizing agents.",
    "path": "papers/24/03/2403.09859.json",
    "total_tokens": 858,
    "translated_title": "MAMBA：一种用于元强化学习的有效世界模型方法",
    "translated_abstract": "Meta强化学习(meta-RL)是解决需要高效探索的具有挑战性领域的有前途的框架。现有的meta-RL算法以低样本效率为特征，主要关注低维任务分布。与此同时，基于模型的RL方法在解决部分可观察MDP方面取得了成功，其中meta-RL是一个特殊情况。在这项工作中，我们利用这一成功并提出了一种新的基于模型的meta-RL方法，该方法基于现有的最先进的基于模型和meta-RL方法的元素。我们在常见的meta-RL基准领域上展示了我们方法的有效性，实现了更大回报和更好样本效率(最高提升$15\\times$)，同时需要非常少的超参数调整。此外，我们在一系列更具挑战性的、更高维度的领域上验证了我们的方法，迈出了通向真实世界泛化代理的一步。",
    "tldr": "MAMBA提出了一种新的基于模型的元强化学习方法，能够在常见的基准测试领域上实现更大的回报和更好的样本效率，同时只需很少的超参数调整。",
    "en_tdlr": "MAMBA proposes a new model-based approach for meta-reinforcement learning, achieving greater returns and better sample efficiency in common benchmark domains with minimal hyperparameter tuning."
}