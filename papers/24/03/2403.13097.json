{
    "title": "Simple Ingredients for Offline Reinforcement Learning",
    "abstract": "arXiv:2403.13097v1 Announce Type: new  Abstract: Offline reinforcement learning algorithms have proven effective on datasets highly connected to the target downstream task. Yet, leveraging a novel testbed (MOOD) in which trajectories come from heterogeneous sources, we show that existing methods struggle with diverse data: their performance considerably deteriorates as data collected for related but different tasks is simply added to the offline buffer. In light of this finding, we conduct a large empirical study where we formulate and test several hypotheses to explain this failure. Surprisingly, we find that scale, more than algorithmic considerations, is the key factor influencing performance. We show that simple methods like AWAC and IQL with increased network size overcome the paradoxical failure modes from the inclusion of additional data in MOOD, and notably outperform prior state-of-the-art algorithms on the canonical D4RL benchmark.",
    "link": "https://arxiv.org/abs/2403.13097",
    "context": "Title: Simple Ingredients for Offline Reinforcement Learning\nAbstract: arXiv:2403.13097v1 Announce Type: new  Abstract: Offline reinforcement learning algorithms have proven effective on datasets highly connected to the target downstream task. Yet, leveraging a novel testbed (MOOD) in which trajectories come from heterogeneous sources, we show that existing methods struggle with diverse data: their performance considerably deteriorates as data collected for related but different tasks is simply added to the offline buffer. In light of this finding, we conduct a large empirical study where we formulate and test several hypotheses to explain this failure. Surprisingly, we find that scale, more than algorithmic considerations, is the key factor influencing performance. We show that simple methods like AWAC and IQL with increased network size overcome the paradoxical failure modes from the inclusion of additional data in MOOD, and notably outperform prior state-of-the-art algorithms on the canonical D4RL benchmark.",
    "path": "papers/24/03/2403.13097.json",
    "total_tokens": 831,
    "translated_title": "离线强化学习的简单要素",
    "translated_abstract": "离线强化学习算法已被证明在与目标下游任务高度相关的数据集上非常有效。然而，在一个新的测试平台（MOOD）中，轨迹来自不同来源，我们展示现有方法在处理多样化数据时存在困难：当为相关但不同的任务收集的数据被简单添加到离线缓冲区时，它们的性能会明显恶化。鉴于这一发现，我们进行了一项大型实证研究，制定并测试了几个假设以解释这种失败。令人惊讶的是，我们发现规模，而不是算法考虑，是影响性能的关键因素。我们表明，像AWAC和IQL这样的简单方法，通过增加网络大小，克服了在MOOD中包含额外数据引起的矛盾故障模式，并明显优于传统最先进的算法在经典的D4RL基准上。",
    "tldr": "规模是影响离线强化学习性能的关键因素，简单方法如AWAC和IQL通过增加网络大小克服了在多源数据情况下的失败模式，并在D4RL基准上表现出色。",
    "en_tdlr": "Scale is the key factor influencing the performance of offline reinforcement learning, simple methods like AWAC and IQL overcome the failure modes in handling diverse data by increasing network size, and notably outperform prior state-of-the-art algorithms on the D4RL benchmark."
}