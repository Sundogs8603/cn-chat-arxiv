{
    "title": "Adapprox: Adaptive Approximation in Adam Optimization via Randomized Low-Rank Matrices",
    "abstract": "arXiv:2403.14958v1 Announce Type: cross  Abstract: As deep learning models exponentially increase in size, optimizers such as Adam encounter significant memory consumption challenges due to the storage of first and second moment data. Current memory-efficient methods like Adafactor and CAME often compromise accuracy with their matrix factorization techniques. Addressing this, we introduce Adapprox, a novel approach that employs randomized low-rank matrix approximation for a more effective and accurate approximation of Adam's second moment. Adapprox features an adaptive rank selection mechanism, finely balancing accuracy and memory efficiency, and includes an optional cosine similarity guidance strategy to enhance stability and expedite convergence. In GPT-2 training and downstream tasks, Adapprox surpasses AdamW by achieving 34.5% to 49.9% and 33.8% to 49.9% memory savings for the 117M and 345M models, respectively, with the first moment enabled, and further increases these savings wit",
    "link": "https://arxiv.org/abs/2403.14958",
    "context": "Title: Adapprox: Adaptive Approximation in Adam Optimization via Randomized Low-Rank Matrices\nAbstract: arXiv:2403.14958v1 Announce Type: cross  Abstract: As deep learning models exponentially increase in size, optimizers such as Adam encounter significant memory consumption challenges due to the storage of first and second moment data. Current memory-efficient methods like Adafactor and CAME often compromise accuracy with their matrix factorization techniques. Addressing this, we introduce Adapprox, a novel approach that employs randomized low-rank matrix approximation for a more effective and accurate approximation of Adam's second moment. Adapprox features an adaptive rank selection mechanism, finely balancing accuracy and memory efficiency, and includes an optional cosine similarity guidance strategy to enhance stability and expedite convergence. In GPT-2 training and downstream tasks, Adapprox surpasses AdamW by achieving 34.5% to 49.9% and 33.8% to 49.9% memory savings for the 117M and 345M models, respectively, with the first moment enabled, and further increases these savings wit",
    "path": "papers/24/03/2403.14958.json",
    "total_tokens": 1031,
    "translated_title": "Adapprox:自适应近似在Adam优化中使用随机低秩矩阵",
    "translated_abstract": "随着深度学习模型的规模呈指数增长，诸如Adam之类的优化器在存储一、二阶矩数据时遇到了显著的内存消耗挑战。当前的内存高效方法如Adafactor和CAME通常通过其矩阵因式分解技术来牺牲准确性。针对这一问题，我们引入了Adapprox，一种新颖的方法，它采用随机低秩矩阵近似来更有效和准确地近似Adam的二阶矩。Adapprox具有自适应秩选择机制，精细平衡准确性和内存效率，并包括一个可选的余弦相似性指导策略，以增强稳定性并加快收敛速度。在GPT-2训练和下游任务中，Adapprox通过实现对117M和345M模型的34.5%至49.9%和33.8%至49.9%内存节约（分别启用了第一阶矩），超越了AdamW，并进一步增加了这些节约。",
    "tldr": "Adapprox是一种采用随机低秩矩阵近似的自适应方法，用于更有效和准确地逼近Adam优化算法的二阶矩。在GPT-2的训练和下游任务中，Adapprox相比AdamW能够实现34.5%至49.9%和33.8%至49.9%的内存节约，并通过余弦相似性指导策略提高了稳定性和加快了收敛速度。",
    "en_tdlr": "Adapprox is an adaptive approach that uses randomized low-rank matrix approximation to more effectively and accurately approximate the second moment of Adam optimization. In GPT-2 training and downstream tasks, Adapprox achieves 34.5% to 49.9% and 33.8% to 49.9% memory savings compared to AdamW, surpassing it, and further enhances stability and convergence speed with a cosine similarity guidance strategy."
}