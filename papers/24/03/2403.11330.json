{
    "title": "Improving Dialogue Agents by Decomposing One Global Explicit Annotation with Local Implicit Multimodal Feedback",
    "abstract": "arXiv:2403.11330v1 Announce Type: cross  Abstract: We describe an approach for aligning an LLM-based dialogue agent based on global (i.e., dialogue-level) rewards, while also taking into account naturally-occurring multimodal signals. At a high level, our approach (dubbed GELI) learns a local, turn-level reward model by decomposing the human-provided Global Explicit (GE) session-level reward, using Local Implicit (LI} multimodal reward signals to crossmodally shape the reward decomposition step. This decomposed reward model is then used as part of the standard RHLF pipeline improve an LLM-based dialog agent. We run quantitative and qualitative human studies to evaluate the performance of our GELI approach, and find that it shows consistent improvements across various conversational metrics compared to baseline methods.",
    "link": "https://arxiv.org/abs/2403.11330",
    "context": "Title: Improving Dialogue Agents by Decomposing One Global Explicit Annotation with Local Implicit Multimodal Feedback\nAbstract: arXiv:2403.11330v1 Announce Type: cross  Abstract: We describe an approach for aligning an LLM-based dialogue agent based on global (i.e., dialogue-level) rewards, while also taking into account naturally-occurring multimodal signals. At a high level, our approach (dubbed GELI) learns a local, turn-level reward model by decomposing the human-provided Global Explicit (GE) session-level reward, using Local Implicit (LI} multimodal reward signals to crossmodally shape the reward decomposition step. This decomposed reward model is then used as part of the standard RHLF pipeline improve an LLM-based dialog agent. We run quantitative and qualitative human studies to evaluate the performance of our GELI approach, and find that it shows consistent improvements across various conversational metrics compared to baseline methods.",
    "path": "papers/24/03/2403.11330.json",
    "total_tokens": 788,
    "translated_title": "通过将一个全局明确标注拆解成本地隐式多模态反馈来改进对话代理",
    "translated_abstract": "我们描述了一种方法，通过全局（即，对话级）奖励对齐基于LLM的对话代理，同时考虑到自然发生的多模态信号。在高层次上，我们的方法（名为GELI）通过将人类提供的全局明确（GE）会话级奖励拆分，利用本地隐式（LI）多模态奖励信号来跨模态地塑造奖励分解步骤。然后将这种分解的奖励模型作为标准RHLF流程的一部分，来改进基于LLM的对话代理。我们进行了定量和定性的人类研究，评估了我们的GELI方法的性能，并发现与基线方法相比，它在各种对话度量方面都表现出一致的改进。",
    "tldr": "通过将全局明确标注拆解成本地隐式多模态反馈，提出了一种改进对话代理的方法，并在各种对话度量方面展现出一致的改进",
    "en_tdlr": "A method for improving dialogue agents by decomposing one global explicit annotation into local implicit multimodal feedback was proposed, showing consistent improvements across various conversational metrics."
}