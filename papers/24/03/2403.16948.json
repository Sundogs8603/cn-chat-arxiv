{
    "title": "Reinforcement Learning-based Recommender Systems with Large Language Models for State Reward and Action Modeling",
    "abstract": "arXiv:2403.16948v1 Announce Type: new  Abstract: Reinforcement Learning (RL)-based recommender systems have demonstrated promising performance in meeting user expectations by learning to make accurate next-item recommendations from historical user-item interactions. However, existing offline RL-based sequential recommendation methods face the challenge of obtaining effective user feedback from the environment. Effectively modeling the user state and shaping an appropriate reward for recommendation remains a challenge. In this paper, we leverage language understanding capabilities and adapt large language models (LLMs) as an environment (LE) to enhance RL-based recommenders. The LE is learned from a subset of user-item interaction data, thus reducing the need for large training data, and can synthesise user feedback for offline data by: (i) acting as a state model that produces high quality states that enrich the user representation, and (ii) functioning as a reward model to accurately ",
    "link": "https://arxiv.org/abs/2403.16948",
    "context": "Title: Reinforcement Learning-based Recommender Systems with Large Language Models for State Reward and Action Modeling\nAbstract: arXiv:2403.16948v1 Announce Type: new  Abstract: Reinforcement Learning (RL)-based recommender systems have demonstrated promising performance in meeting user expectations by learning to make accurate next-item recommendations from historical user-item interactions. However, existing offline RL-based sequential recommendation methods face the challenge of obtaining effective user feedback from the environment. Effectively modeling the user state and shaping an appropriate reward for recommendation remains a challenge. In this paper, we leverage language understanding capabilities and adapt large language models (LLMs) as an environment (LE) to enhance RL-based recommenders. The LE is learned from a subset of user-item interaction data, thus reducing the need for large training data, and can synthesise user feedback for offline data by: (i) acting as a state model that produces high quality states that enrich the user representation, and (ii) functioning as a reward model to accurately ",
    "path": "papers/24/03/2403.16948.json",
    "total_tokens": 709,
    "translated_title": "利用大语言模型进行基于强化学习的推荐系统，用于状态、奖励和动作建模",
    "translated_abstract": "基于强化学习的推荐系统在从历史用户-物品交互中学习准确的下一个物品推荐方面表现出色，但现有的离线强化学习方法在获取有效用户反馈方面面临挑战。本文利用语言理解能力，将大型语言模型(LLM)调整为环境(LE)以增强基于强化学习的推荐系统，从而减少了对大量训练数据的需求。",
    "tldr": "本文利用大型语言模型(LLM)作为环境(LE)优化了基于强化学习的推荐系统，提高了状态建模和奖励设置的准确性。",
    "en_tdlr": "This paper leverages large language models (LLMs) as an environment (LE) to optimize reinforcement learning-based recommender systems, improving the accuracy of state modeling and reward setting."
}