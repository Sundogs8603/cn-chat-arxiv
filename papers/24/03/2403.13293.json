{
    "title": "Building Optimal Neural Architectures using Interpretable Knowledge",
    "abstract": "arXiv:2403.13293v1 Announce Type: cross  Abstract: Neural Architecture Search is a costly practice. The fact that a search space can span a vast number of design choices with each architecture evaluation taking nontrivial overhead makes it hard for an algorithm to sufficiently explore candidate networks. In this paper, we propose AutoBuild, a scheme which learns to align the latent embeddings of operations and architecture modules with the ground-truth performance of the architectures they appear in. By doing so, AutoBuild is capable of assigning interpretable importance scores to architecture modules, such as individual operation features and larger macro operation sequences such that high-performance neural networks can be constructed without any need for search. Through experiments performed on state-of-the-art image classification, segmentation, and Stable Diffusion models, we show that by mining a relatively small set of evaluated architectures, AutoBuild can learn to build high-q",
    "link": "https://arxiv.org/abs/2403.13293",
    "context": "Title: Building Optimal Neural Architectures using Interpretable Knowledge\nAbstract: arXiv:2403.13293v1 Announce Type: cross  Abstract: Neural Architecture Search is a costly practice. The fact that a search space can span a vast number of design choices with each architecture evaluation taking nontrivial overhead makes it hard for an algorithm to sufficiently explore candidate networks. In this paper, we propose AutoBuild, a scheme which learns to align the latent embeddings of operations and architecture modules with the ground-truth performance of the architectures they appear in. By doing so, AutoBuild is capable of assigning interpretable importance scores to architecture modules, such as individual operation features and larger macro operation sequences such that high-performance neural networks can be constructed without any need for search. Through experiments performed on state-of-the-art image classification, segmentation, and Stable Diffusion models, we show that by mining a relatively small set of evaluated architectures, AutoBuild can learn to build high-q",
    "path": "papers/24/03/2403.13293.json",
    "total_tokens": 819,
    "translated_title": "利用可解释知识构建最佳神经架构",
    "translated_abstract": "神经架构搜索是一种昂贵的做法。一个搜索空间涵盖了大量的设计选择，每个架构评估都需要非常大的开销，这使得算法很难充分探索候选网络。在本文中，我们提出AutoBuild，一个方案，它学习将操作和架构模块的潜在嵌入与它们所出现的架构的实际性能进行对齐。通过这样做，AutoBuild能够为架构模块分配可解释的重要性分数，例如个别操作特征和更大的宏操作序列，从而可以构建高性能神经网络而无需进行搜索。通过在最先进的图像分类、分割和稳定扩散模型上进行的实验，我们展示了通过挖掘一个相对较小的评估架构集，AutoBuild可以学会构建高效的神经架构。",
    "tldr": "AutoBuild通过学习将操作和架构模块的潜在嵌入与其性能对齐，赋予架构模块可解释的重要性分数，从而实现高性能神经网络的构建。",
    "en_tdlr": "AutoBuild aligns the latent embeddings of operations and architecture modules with their performance, assigning interpretable importance scores to modules, enabling the construction of high-performance neural networks without the need for search."
}