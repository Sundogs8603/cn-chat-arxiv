{
    "title": "Data-Efficient Contrastive Language-Image Pretraining: Prioritizing Data Quality over Quantity",
    "abstract": "arXiv:2403.12267v1 Announce Type: cross  Abstract: Contrastive Language-Image Pre-training (CLIP) on large-scale image-caption datasets learns representations that can achieve remarkable zero-shot generalization. However, such models require a massive amount of pre-training data. Improving the quality of the pre-training data has been shown to be much more effective in improving CLIP's performance than increasing its volume. Nevertheless, finding small subsets of training data that provably generalize the best has remained an open question. In this work, we propose the first theoretically rigorous data selection method for CLIP. We show that subsets that closely preserve the cross-covariance of the images and captions of the full data provably achieve a superior generalization performance. Our extensive experiments on ConceptualCaptions3M and ConceptualCaptions12M demonstrate that subsets found by \\method\\ achieve over 2.7x and 1.4x the accuracy of the next best baseline on ImageNet an",
    "link": "https://arxiv.org/abs/2403.12267",
    "context": "Title: Data-Efficient Contrastive Language-Image Pretraining: Prioritizing Data Quality over Quantity\nAbstract: arXiv:2403.12267v1 Announce Type: cross  Abstract: Contrastive Language-Image Pre-training (CLIP) on large-scale image-caption datasets learns representations that can achieve remarkable zero-shot generalization. However, such models require a massive amount of pre-training data. Improving the quality of the pre-training data has been shown to be much more effective in improving CLIP's performance than increasing its volume. Nevertheless, finding small subsets of training data that provably generalize the best has remained an open question. In this work, we propose the first theoretically rigorous data selection method for CLIP. We show that subsets that closely preserve the cross-covariance of the images and captions of the full data provably achieve a superior generalization performance. Our extensive experiments on ConceptualCaptions3M and ConceptualCaptions12M demonstrate that subsets found by \\method\\ achieve over 2.7x and 1.4x the accuracy of the next best baseline on ImageNet an",
    "path": "papers/24/03/2403.12267.json",
    "total_tokens": 881,
    "translated_title": "数据高效的对比语言-图像预训练：优先考虑数据质量而非数量",
    "translated_abstract": "对比语言-图像预训练（CLIP）是在大规模图像字幕数据集上学习表示，能够实现显著的零次通用化。然而，这样的模型需要大量的预训练数据。改进预训练数据的质量已被证明比增加数量更有效地提高了CLIP的性能。然而，找到能够证明达到最佳泛化效果的小训练数据子集一直是一个悬而未决的问题。在本文中，我们提出了第一个针对CLIP的理论严谨的数据选择方法。我们展示了能够证明实现卓越泛化性能的子集接近保留完整数据的图像和字幕的交叉协方差。我们在ConceptualCaptions3M和ConceptualCaptions12M上进行了大量实验证明，\\method\\找到的子集在ImageNet上的准确性比下一个最佳基线提高了2.7倍和1.4倍。",
    "tldr": "改进预训练数据质量对于提高CLIP性能比增加数据量更为有效，本研究提出了首个针对CLIP的理论严谨的数据选择方法，大幅提升了泛化性能。",
    "en_tdlr": "Improving the quality of pre-training data is more effective for enhancing CLIP performance than increasing the quantity, and this study introduces the first theoretically rigorous data selection method for CLIP, significantly boosting generalization performance."
}