{
    "title": "Unimodal Multi-Task Fusion for Emotional Mimicry Prediciton",
    "abstract": "arXiv:2403.11879v1 Announce Type: cross  Abstract: In this study, we propose a methodology for the Emotional Mimicry Intensity (EMI) Estimation task within the context of the 6th Workshop and Competition on Affective Behavior Analysis in-the-wild. Our approach leverages the Wav2Vec 2.0 framework, pre-trained on a comprehensive podcast dataset, to extract a broad range of audio features encompassing both linguistic and paralinguistic elements. We enhance feature representation through a fusion technique that integrates individual features with a global mean vector, introducing global contextual insights into our analysis. Additionally, we incorporate a pre-trained valence- arousal-dominance (VAD) module from the Wav2Vec 2.0 model. Our fusion employs a Long Short-Term Memory (LSTM) architecture for efficient temporal analysis of audio data. Utilizing only the provided audio data, our approach demonstrates significant improvements over the established baseline.",
    "link": "https://arxiv.org/abs/2403.11879",
    "context": "Title: Unimodal Multi-Task Fusion for Emotional Mimicry Prediciton\nAbstract: arXiv:2403.11879v1 Announce Type: cross  Abstract: In this study, we propose a methodology for the Emotional Mimicry Intensity (EMI) Estimation task within the context of the 6th Workshop and Competition on Affective Behavior Analysis in-the-wild. Our approach leverages the Wav2Vec 2.0 framework, pre-trained on a comprehensive podcast dataset, to extract a broad range of audio features encompassing both linguistic and paralinguistic elements. We enhance feature representation through a fusion technique that integrates individual features with a global mean vector, introducing global contextual insights into our analysis. Additionally, we incorporate a pre-trained valence- arousal-dominance (VAD) module from the Wav2Vec 2.0 model. Our fusion employs a Long Short-Term Memory (LSTM) architecture for efficient temporal analysis of audio data. Utilizing only the provided audio data, our approach demonstrates significant improvements over the established baseline.",
    "path": "papers/24/03/2403.11879.json",
    "total_tokens": 860,
    "translated_title": "单模态多任务融合用于情感模仿预测",
    "translated_abstract": "在这项研究中，我们提出了一种方法，用于在第六届户外情感行为分析研讨会和竞赛中进行情感模仿强度（EMI）估计任务。我们的方法利用了Wav2Vec 2.0框架，在一个全面的播客数据集上进行了预训练，以提取涵盖语言和语外元素的广泛音频特征。我们通过一种融合技术增强了特征表示，该技术将个体特征与全局均值向量相结合，引入全局背景信息到我们的分析中。此外，我们从Wav2Vec 2.0模型中引入了一个预训练的valence-arousal-dominance（VAD）模块。我们的融合采用了一种长短期记忆（LSTM）架构，用于对音频数据进行高效的时间分析。仅利用所提供的音频数据，我们的方法在已建立的基准线上表现出显著的改进。",
    "tldr": "通过融合技术整合全局背景信息和采用LSTM架构进行时间分析，我们的方法在情感模仿强度预测任务上取得了显着的改进。",
    "en_tdlr": "By integrating global contextual insights with fusion technique and utilizing LSTM architecture for temporal analysis, our approach demonstrates significant improvements in emotional mimicry intensity prediction task."
}