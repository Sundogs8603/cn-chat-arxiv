{
    "title": "TriSum: Learning Summarization Ability from Large Language Models with Structured Rationale",
    "abstract": "arXiv:2403.10351v1 Announce Type: new  Abstract: The advent of large language models (LLMs) has significantly advanced natural language processing tasks like text summarization. However, their large size and computational demands, coupled with privacy concerns in data transmission, limit their use in resource-constrained and privacy-centric settings. To overcome this, we introduce TriSum, a framework for distilling LLMs' text summarization abilities into a compact, local model. Initially, LLMs extract a set of aspect-triple rationales and summaries, which are refined using a dual-scoring method for quality. Next, a smaller local model is trained with these tasks, employing a curriculum learning strategy that evolves from simple to complex tasks. Our method enhances local model performance on various benchmarks (CNN/DailyMail, XSum, and ClinicalTrial), outperforming baselines by 4.5%, 8.5%, and 7.4%, respectively. It also improves interpretability by providing insights into the summariz",
    "link": "https://arxiv.org/abs/2403.10351",
    "context": "Title: TriSum: Learning Summarization Ability from Large Language Models with Structured Rationale\nAbstract: arXiv:2403.10351v1 Announce Type: new  Abstract: The advent of large language models (LLMs) has significantly advanced natural language processing tasks like text summarization. However, their large size and computational demands, coupled with privacy concerns in data transmission, limit their use in resource-constrained and privacy-centric settings. To overcome this, we introduce TriSum, a framework for distilling LLMs' text summarization abilities into a compact, local model. Initially, LLMs extract a set of aspect-triple rationales and summaries, which are refined using a dual-scoring method for quality. Next, a smaller local model is trained with these tasks, employing a curriculum learning strategy that evolves from simple to complex tasks. Our method enhances local model performance on various benchmarks (CNN/DailyMail, XSum, and ClinicalTrial), outperforming baselines by 4.5%, 8.5%, and 7.4%, respectively. It also improves interpretability by providing insights into the summariz",
    "path": "papers/24/03/2403.10351.json",
    "total_tokens": 885,
    "translated_title": "TriSum: 从大型语言模型中学习总结能力与结构化理由",
    "translated_abstract": "大型语言模型（LLMs）的出现显著推动了文本总结等自然语言处理任务。然而，它们的庞大大小和计算需求，加上数据传输中的隐私问题，限制了它们在资源受限和隐私为中心的环境中的使用。为了克服这一问题，我们引入了TriSum，一个将LLMs的文本总结能力提炼到一个紧凑的本地模型中的框架。最初，LLMs提取一组方面三元理由和总结，然后使用双评分方法对其进行优化。接下来，使用这些任务训练一个规模较小的本地模型，采用从简单到复杂任务的课程学习策略。我们的方法提升了本地模型在各种基准测试上的性能（CNN/DailyMail，XSum和ClinicalTrial），分别比基线提高了4.5％，8.5％和7.4％。它还通过提供对总结的见解来改善可解释性。",
    "tldr": "TriSum是一个框架，通过将大型语言模型的文本总结能力提炼到一个紧凑的本地模型中，从而在各种基准测试中提高了性能，并提高了可解释性。",
    "en_tdlr": "TriSum is a framework that distills the text summarization abilities of large language models into a compact local model, improving performance on various benchmarks and enhancing interpretability."
}