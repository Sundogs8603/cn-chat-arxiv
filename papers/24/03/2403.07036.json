{
    "title": "A Converting Autoencoder Toward Low-latency and Energy-efficient DNN Inference at the Edge",
    "abstract": "arXiv:2403.07036v1 Announce Type: new  Abstract: Reducing inference time and energy usage while maintaining prediction accuracy has become a significant concern for deep neural networks (DNN) inference on resource-constrained edge devices. To address this problem, we propose a novel approach based on \"converting\" autoencoder and lightweight DNNs. This improves upon recent work such as early-exiting framework and DNN partitioning. Early-exiting frameworks spend different amounts of computation power for different input data depending upon their complexity. However, they can be inefficient in real-world scenarios that deal with many hard image samples. On the other hand, DNN partitioning algorithms that utilize the computation power of both the cloud and edge devices can be affected by network delays and intermittent connections between the cloud and the edge. We present CBNet, a low-latency and energy-efficient DNN inference framework tailored for edge devices. It utilizes a \"converting",
    "link": "https://arxiv.org/abs/2403.07036",
    "context": "Title: A Converting Autoencoder Toward Low-latency and Energy-efficient DNN Inference at the Edge\nAbstract: arXiv:2403.07036v1 Announce Type: new  Abstract: Reducing inference time and energy usage while maintaining prediction accuracy has become a significant concern for deep neural networks (DNN) inference on resource-constrained edge devices. To address this problem, we propose a novel approach based on \"converting\" autoencoder and lightweight DNNs. This improves upon recent work such as early-exiting framework and DNN partitioning. Early-exiting frameworks spend different amounts of computation power for different input data depending upon their complexity. However, they can be inefficient in real-world scenarios that deal with many hard image samples. On the other hand, DNN partitioning algorithms that utilize the computation power of both the cloud and edge devices can be affected by network delays and intermittent connections between the cloud and the edge. We present CBNet, a low-latency and energy-efficient DNN inference framework tailored for edge devices. It utilizes a \"converting",
    "path": "papers/24/03/2403.07036.json",
    "total_tokens": 692,
    "translated_title": "一种面向边缘低延迟和高能效DNN推断的转换自编码器",
    "translated_abstract": "降低推断时间和能量使用，同时保持预测准确性，已成为资源受限边缘设备上深度神经网络（DNN）推断的一个重要关注点。为了解决这个问题，我们提出了一种基于“转换”自编码器和轻量级DNN的新方法。这一方法改进了最近的工作，如提前退出框架和DNN分区。",
    "tldr": "提出了一种面向边缘设备的低延迟和高能效DNN推断框架，利用“转换”自编码器和轻量级DNN，改进了现有的方法。"
}