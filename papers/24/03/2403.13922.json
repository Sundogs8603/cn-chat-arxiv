{
    "title": "Visually Grounded Speech Models have a Mutual Exclusivity Bias",
    "abstract": "arXiv:2403.13922v1 Announce Type: new  Abstract: When children learn new words, they employ constraints such as the mutual exclusivity (ME) bias: a novel word is mapped to a novel object rather than a familiar one. This bias has been studied computationally, but only in models that use discrete word representations as input, ignoring the high variability of spoken words. We investigate the ME bias in the context of visually grounded speech models that learn from natural images and continuous speech audio. Concretely, we train a model on familiar words and test its ME bias by asking it to select between a novel and a familiar object when queried with a novel word. To simulate prior acoustic and visual knowledge, we experiment with several initialisation strategies using pretrained speech and vision networks. Our findings reveal the ME bias across the different initialisation approaches, with a stronger bias in models with more prior (in particular, visual) knowledge. Additional tests co",
    "link": "https://arxiv.org/abs/2403.13922",
    "context": "Title: Visually Grounded Speech Models have a Mutual Exclusivity Bias\nAbstract: arXiv:2403.13922v1 Announce Type: new  Abstract: When children learn new words, they employ constraints such as the mutual exclusivity (ME) bias: a novel word is mapped to a novel object rather than a familiar one. This bias has been studied computationally, but only in models that use discrete word representations as input, ignoring the high variability of spoken words. We investigate the ME bias in the context of visually grounded speech models that learn from natural images and continuous speech audio. Concretely, we train a model on familiar words and test its ME bias by asking it to select between a novel and a familiar object when queried with a novel word. To simulate prior acoustic and visual knowledge, we experiment with several initialisation strategies using pretrained speech and vision networks. Our findings reveal the ME bias across the different initialisation approaches, with a stronger bias in models with more prior (in particular, visual) knowledge. Additional tests co",
    "path": "papers/24/03/2403.13922.json",
    "total_tokens": 877,
    "translated_title": "视觉引导的语音模型存在相互排他性偏见",
    "translated_abstract": "当孩子学习新单词时，他们会运用相互排他性（ME）偏见这样的约束：一个新单词会映射到一个新对象而不是一个熟悉的对象。这种偏见已经在计算模型中进行了研究，但只在使用离散词表示作为输入的模型中进行了研究，忽略了口语词的高变异性。我们研究了在从自然图像和连续语音音频中学习的视觉引导的语音模型中的ME偏见。具体来说，我们训练一个模型以熟悉的单词，然后通过询问模型选择一个新对象和一个熟悉对象的方式来测试其ME偏见。为了模拟先前的声学和视觉知识，我们尝试了几种使用预训练语音和视觉网络的初始化策略。我们的研究结果揭示了不同初始化方法下的ME偏见，以及在具有更多先前知识（特别是视觉知识）的模型中更强的偏见。额外的测试",
    "tldr": "研究探讨了视觉引导的语音模型中的相互排他性偏见，并发现在具有更多视觉知识的模型中存在更强的偏见。",
    "en_tdlr": "The study investigates the mutual exclusivity bias in visually grounded speech models, revealing a stronger bias in models with more visual knowledge."
}