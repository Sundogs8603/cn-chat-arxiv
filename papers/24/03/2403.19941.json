{
    "title": "Diverse Feature Learning by Self-distillation and Reset",
    "abstract": "arXiv:2403.19941v1 Announce Type: new  Abstract: Our paper addresses the problem of models struggling to learn diverse features, due to either forgetting previously learned features or failing to learn new ones. To overcome this problem, we introduce Diverse Feature Learning (DFL), a method that combines an important feature preservation algorithm with a new feature learning algorithm. Specifically, for preserving important features, we utilize self-distillation in ensemble models by selecting the meaningful model weights observed during training. For learning new features, we employ reset that involves periodically re-initializing part of the model. As a result, through experiments with various models on the image classification, we have identified the potential for synergistic effects between self-distillation and reset.",
    "link": "https://arxiv.org/abs/2403.19941",
    "context": "Title: Diverse Feature Learning by Self-distillation and Reset\nAbstract: arXiv:2403.19941v1 Announce Type: new  Abstract: Our paper addresses the problem of models struggling to learn diverse features, due to either forgetting previously learned features or failing to learn new ones. To overcome this problem, we introduce Diverse Feature Learning (DFL), a method that combines an important feature preservation algorithm with a new feature learning algorithm. Specifically, for preserving important features, we utilize self-distillation in ensemble models by selecting the meaningful model weights observed during training. For learning new features, we employ reset that involves periodically re-initializing part of the model. As a result, through experiments with various models on the image classification, we have identified the potential for synergistic effects between self-distillation and reset.",
    "path": "papers/24/03/2403.19941.json",
    "total_tokens": 767,
    "translated_title": "通过自蒸馏和重置实现多样特征学习",
    "translated_abstract": "我们的论文解决了模型难以学习多样特征的问题，原因是它们要么忘记了先前学习的特征，要么无法学习新的特征。为了克服这个问题，我们引入了一种称为Diverse Feature Learning (DFL)的方法，它将重要特征保留算法与新特征学习算法相结合。具体地，为了保留重要特征，我们在训练过程中选择有意义的模型权重，通过自蒸馏在集成模型中进行。为了学习新的特征，我们采用了定期重新初始化模型的重置方法。通过在图像分类上对各种模型进行实验，我们发现了自蒸馏和重置之间的协同效应潜力。",
    "tldr": "提出了一种名为Diverse Feature Learning (DFL)的方法，通过结合重要特征保留算法和新特征学习算法，利用自蒸馏和重置来解决模型学习多样特征时遇到的问题。",
    "en_tdlr": "Introducing a method called Diverse Feature Learning (DFL) that addresses the issue of models struggling to learn diverse features by combining important feature preservation algorithm with a new feature learning algorithm, utilizing self-distillation and reset."
}