{
    "title": "Large, Small or Both: A Novel Data Augmentation Framework Based on Language Models for Debiasing Opinion Summarization",
    "abstract": "arXiv:2403.07693v1 Announce Type: cross  Abstract: As more than 70$\\%$ of reviews in the existing opinion summary data set are positive, current opinion summarization approaches are reluctant to generate negative summaries given the input of negative texts. To address such sentiment bias, a direct approach without the over-reliance on a specific framework is to generate additional data based on large language models to balance the emotional distribution of the dataset. However, data augmentation based on large language models faces two disadvantages: 1) the potential issues or toxicity in the augmented data; 2) the expensive costs. Therefore, in this paper, we propose a novel data augmentation framework based on both large and small language models for debiasing opinion summarization. In specific, a small size of synthesized negative reviews is obtained by rewriting the positive text via a large language model. Then, a disentangle reconstruction model is trained based on the generated ",
    "link": "https://arxiv.org/abs/2403.07693",
    "context": "Title: Large, Small or Both: A Novel Data Augmentation Framework Based on Language Models for Debiasing Opinion Summarization\nAbstract: arXiv:2403.07693v1 Announce Type: cross  Abstract: As more than 70$\\%$ of reviews in the existing opinion summary data set are positive, current opinion summarization approaches are reluctant to generate negative summaries given the input of negative texts. To address such sentiment bias, a direct approach without the over-reliance on a specific framework is to generate additional data based on large language models to balance the emotional distribution of the dataset. However, data augmentation based on large language models faces two disadvantages: 1) the potential issues or toxicity in the augmented data; 2) the expensive costs. Therefore, in this paper, we propose a novel data augmentation framework based on both large and small language models for debiasing opinion summarization. In specific, a small size of synthesized negative reviews is obtained by rewriting the positive text via a large language model. Then, a disentangle reconstruction model is trained based on the generated ",
    "path": "papers/24/03/2403.07693.json",
    "total_tokens": 869,
    "translated_title": "基于语言模型的新型数据增强框架用于去偏见观点摘要",
    "translated_abstract": "现有观点摘要数据集中超过70％的评论是积极的，当前的观点摘要方法在给定负面文本的情况下不愿生成负面摘要，造成情感偏见。为了解决这种情感偏见，一个直接的方法是基于大型语言模型生成额外的数据，平衡数据集的情感分布，而不过分依赖特定框架。然而，基于大型语言模型的数据增强面临两个缺点：1）增强数据中的潜在问题或毒性；2）昂贵的成本。因此，在本文中，我们提出了一个基于大型和小型语言模型的新型数据增强框架，用于去偏见观点摘要。具体而言，通过大型语言模型重写正面文本获得了小规模合成的负面评论。然后，基于生成的内容训练一个解耦重构模型。",
    "tldr": "使用大型和小型语言模型的新型数据增强框架，通过重新生成评论来实现观点摘要的去偏见化，避免了大型语言模型数据增强可能存在的问题和高昂成本。"
}