{
    "title": "Deep Backward and Galerkin Methods for the Finite State Master Equation",
    "abstract": "arXiv:2403.04975v1 Announce Type: cross  Abstract: This paper proposes and analyzes two neural network methods to solve the master equation for finite-state mean field games (MFGs). Solving MFGs provides approximate Nash equilibria for stochastic, differential games with finite but large populations of agents. The master equation is a partial differential equation (PDE) whose solution characterizes MFG equilibria for any possible initial distribution. The first method we propose relies on backward induction in a time component while the second method directly tackles the PDE without discretizing time. For both approaches, we prove two types of results: there exist neural networks that make the algorithms' loss functions arbitrarily small, and conversely, if the losses are small, then the neural networks are good approximations of the master equation's solution. We conclude the paper with numerical experiments on benchmark problems from the literature up to dimension 15, and a compariso",
    "link": "https://arxiv.org/abs/2403.04975",
    "context": "Title: Deep Backward and Galerkin Methods for the Finite State Master Equation\nAbstract: arXiv:2403.04975v1 Announce Type: cross  Abstract: This paper proposes and analyzes two neural network methods to solve the master equation for finite-state mean field games (MFGs). Solving MFGs provides approximate Nash equilibria for stochastic, differential games with finite but large populations of agents. The master equation is a partial differential equation (PDE) whose solution characterizes MFG equilibria for any possible initial distribution. The first method we propose relies on backward induction in a time component while the second method directly tackles the PDE without discretizing time. For both approaches, we prove two types of results: there exist neural networks that make the algorithms' loss functions arbitrarily small, and conversely, if the losses are small, then the neural networks are good approximations of the master equation's solution. We conclude the paper with numerical experiments on benchmark problems from the literature up to dimension 15, and a compariso",
    "path": "papers/24/03/2403.04975.json",
    "total_tokens": 801,
    "translated_title": "深度反向和Galerkin方法用于有限状态主方程",
    "translated_abstract": "本论文提出并分析了两种神经网络方法，用于解决有限状态均场博弈的主方程。解决MFGs为具有有限但大量代理人群的随机、微分博弈提供近似纳什均衡。主方程是一个偏微分方程（PDE），其解表征任何可能的初始分布下的MFG均衡。我们提出的第一种方法依赖于在时间分量上的反向归纳，而第二种方法直接解决了PDE，而无需离散化时间。对于两种方法，我们证明了两种类型的结果：存在神经网络，可以使算法的损失函数任意小，并且反之，如果损失很小，则神经网络是主方程解的良好近似。我们通过对文献中的基准问题进行了维度为15的数值实验，并做出对比。",
    "tldr": "该论文提出了两种神经网络方法来解决有限状态均场博弈的主方程，通过数值实验验证了这两种方法的有效性。",
    "en_tdlr": "This paper introduces two neural network methods to solve the master equation for finite-state mean field games and validates their effectiveness through numerical experiments."
}