{
    "title": "Taming Cross-Domain Representation Variance in Federated Prototype Learning with Heterogeneous Data Domains",
    "abstract": "arXiv:2403.09048v1 Announce Type: new  Abstract: Federated learning (FL) allows collaborative machine learning training without sharing private data. While most FL methods assume identical data domains across clients, real-world scenarios often involve heterogeneous data domains. Federated Prototype Learning (FedPL) addresses this issue, using mean feature vectors as prototypes to enhance model generalization. However, existing FedPL methods create the same number of prototypes for each client, leading to cross-domain performance gaps and disparities for clients with varied data distributions. To mitigate cross-domain feature representation variance, we introduce FedPLVM, which establishes variance-aware dual-level prototypes clustering and employs a novel $\\alpha$-sparsity prototype loss. The dual-level prototypes clustering strategy creates local clustered prototypes based on private data features, then performs global prototypes clustering to reduce communication complexity and pres",
    "link": "https://arxiv.org/abs/2403.09048",
    "context": "Title: Taming Cross-Domain Representation Variance in Federated Prototype Learning with Heterogeneous Data Domains\nAbstract: arXiv:2403.09048v1 Announce Type: new  Abstract: Federated learning (FL) allows collaborative machine learning training without sharing private data. While most FL methods assume identical data domains across clients, real-world scenarios often involve heterogeneous data domains. Federated Prototype Learning (FedPL) addresses this issue, using mean feature vectors as prototypes to enhance model generalization. However, existing FedPL methods create the same number of prototypes for each client, leading to cross-domain performance gaps and disparities for clients with varied data distributions. To mitigate cross-domain feature representation variance, we introduce FedPLVM, which establishes variance-aware dual-level prototypes clustering and employs a novel $\\alpha$-sparsity prototype loss. The dual-level prototypes clustering strategy creates local clustered prototypes based on private data features, then performs global prototypes clustering to reduce communication complexity and pres",
    "path": "papers/24/03/2403.09048.json",
    "total_tokens": 811,
    "translated_title": "驯服异构数据域中联邦原型学习中的跨领域表示差异",
    "translated_abstract": "联邦学习（FL）允许在不共享私人数据的情况下进行协作机器学习训练。虽然大多数FL方法假设客户端之间具有相同的数据领域，但现实场景中通常涉及异构数据领域。联邦原型学习（FedPL）解决了这个问题，使用平均特征向量作为原型来增强模型泛化能力。然而，现有的FedPL方法为每个客户端创建相同数量的原型，导致跨领域性能差距，并使数据分布不同的客户端存在差异。为了减轻跨领域特征表示差异，我们引入了FedPLVM，它建立了方差感知的双层原型聚类，并采用了一种新颖的$\\alpha$-稀疏原型损失。",
    "tldr": "引入FedPLVM通过建立方差感知的双层原型聚类和使用新型$\\alpha$-稀疏原型损失，以减少跨领域特征表示差异。",
    "en_tdlr": "Introducing FedPLVM to reduce cross-domain feature representation variance by establishing variance-aware dual-level prototypes clustering and using a novel $\\alpha$-sparsity prototype loss."
}