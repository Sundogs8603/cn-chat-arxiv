{
    "title": "Joint-Embedding Masked Autoencoder for Self-supervised Learning of Dynamic Functional Connectivity from the Human Brain",
    "abstract": "arXiv:2403.06432v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have shown promise in learning dynamic functional connectivity for distinguishing phenotypes from human brain networks. However, obtaining extensive labeled clinical data for training is often resource-intensive, making practical application difficult. Leveraging unlabeled data thus becomes crucial for representation learning in a label-scarce setting. Although generative self-supervised learning techniques, especially masked autoencoders, have shown promising results in representation learning in various domains, their application to dynamic graphs for dynamic functional connectivity remains underexplored, facing challenges in capturing high-level semantic representations. Here, we introduce the Spatio-Temporal Joint Embedding Masked Autoencoder (ST-JEMA), drawing inspiration from the Joint Embedding Predictive Architecture (JEPA) in computer vision. ST-JEMA employs a JEPA-inspired strategy for reconstructin",
    "link": "https://arxiv.org/abs/2403.06432",
    "context": "Title: Joint-Embedding Masked Autoencoder for Self-supervised Learning of Dynamic Functional Connectivity from the Human Brain\nAbstract: arXiv:2403.06432v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have shown promise in learning dynamic functional connectivity for distinguishing phenotypes from human brain networks. However, obtaining extensive labeled clinical data for training is often resource-intensive, making practical application difficult. Leveraging unlabeled data thus becomes crucial for representation learning in a label-scarce setting. Although generative self-supervised learning techniques, especially masked autoencoders, have shown promising results in representation learning in various domains, their application to dynamic graphs for dynamic functional connectivity remains underexplored, facing challenges in capturing high-level semantic representations. Here, we introduce the Spatio-Temporal Joint Embedding Masked Autoencoder (ST-JEMA), drawing inspiration from the Joint Embedding Predictive Architecture (JEPA) in computer vision. ST-JEMA employs a JEPA-inspired strategy for reconstructin",
    "path": "papers/24/03/2403.06432.json",
    "total_tokens": 914,
    "translated_title": "人类大脑动态功能连接的自监督学习中的联合嵌入掩蔽自编码器",
    "translated_abstract": "arXiv:2403.06432v1 通告类型: 新的 摘要: 图神经网络（GNNs）在学习动态功能连接方面表现出潜力，可以区分人脑网络中的表现型。然而，获得用于训练的大量标记临床数据通常具有资源密集性，这使得实际应用变得困难。因此，在标签稀缺设置中，利用未标记数据对于表示学习变得至关重要。尽管生成式自监督学习技术，特别是掩蔽自编码器，在各个领域的表示学习中展现出了有希望的结果，但它们在动态图形上的应用以及动态功能连接方面仍未得到充分探讨，面临着捕捉高级语义表示方面的挑战。在这里，我们介绍了时空联合嵌入掩蔽自编码器（ST-JEMA），受到计算机视觉中联合嵌入预测架构（JEPA）的启发。ST-JEMA采用了一种受JEPA启发的策略来重构",
    "tldr": "提出了一种受到计算机视觉中 JEPA 架构启发的 Spatio-Temporal Joint Embedding Masked Autoencoder（ST-JEMA）用于动态功能连接的自监督学习。",
    "en_tdlr": "Introduced a Spatio-Temporal Joint Embedding Masked Autoencoder (ST-JEMA) inspired by the Joint Embedding Predictive Architecture (JEPA) in computer vision for self-supervised learning of dynamic functional connectivity in the human brain."
}