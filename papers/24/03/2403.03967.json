{
    "title": "Effect of Ambient-Intrinsic Dimension Gap on Adversarial Vulnerability",
    "abstract": "arXiv:2403.03967v1 Announce Type: new  Abstract: The existence of adversarial attacks on machine learning models imperceptible to a human is still quite a mystery from a theoretical perspective. In this work, we introduce two notions of adversarial attacks: natural or on-manifold attacks, which are perceptible by a human/oracle, and unnatural or off-manifold attacks, which are not. We argue that the existence of the off-manifold attacks is a natural consequence of the dimension gap between the intrinsic and ambient dimensions of the data. For 2-layer ReLU networks, we prove that even though the dimension gap does not affect generalization performance on samples drawn from the observed data space, it makes the clean-trained model more vulnerable to adversarial perturbations in the off-manifold direction of the data space. Our main results provide an explicit relationship between the $\\ell_2,\\ell_{\\infty}$ attack strength of the on/off-manifold attack and the dimension gap.",
    "link": "https://arxiv.org/abs/2403.03967",
    "context": "Title: Effect of Ambient-Intrinsic Dimension Gap on Adversarial Vulnerability\nAbstract: arXiv:2403.03967v1 Announce Type: new  Abstract: The existence of adversarial attacks on machine learning models imperceptible to a human is still quite a mystery from a theoretical perspective. In this work, we introduce two notions of adversarial attacks: natural or on-manifold attacks, which are perceptible by a human/oracle, and unnatural or off-manifold attacks, which are not. We argue that the existence of the off-manifold attacks is a natural consequence of the dimension gap between the intrinsic and ambient dimensions of the data. For 2-layer ReLU networks, we prove that even though the dimension gap does not affect generalization performance on samples drawn from the observed data space, it makes the clean-trained model more vulnerable to adversarial perturbations in the off-manifold direction of the data space. Our main results provide an explicit relationship between the $\\ell_2,\\ell_{\\infty}$ attack strength of the on/off-manifold attack and the dimension gap.",
    "path": "papers/24/03/2403.03967.json",
    "total_tokens": 913,
    "translated_title": "环境-固有维度差异对对抗脆弱性的影响",
    "translated_abstract": "论文介绍了对机器学习模型的对抗攻击存在且对人类来说几乎无法察觉这一事实，在理论上仍然相当神秘。文章引入了两种对抗攻击的概念：自然或在流形上的攻击，这些攻击是可以被人类/神谕感知到的；非自然或脱离流形的攻击，这些攻击则无法被感知到。文章认为脱离流形的攻击存在是数据固有维度与环境维度之间的差异的必然结果。对于2层ReLU网络，我们证明了即使维度差异不影响从观测数据空间中抽取样本的泛化性能，它仍会使干净训练的模型更容易受到数据空间脱离流形方向的对抗扰动攻击。我们的主要结果提供了在/脱离流形攻击的$\\ell_2,\\ell_{\\infty}$攻击强度与维度差异之间明确的关系。",
    "tldr": "通过环境-固有维度差异的概念，论文证明了维度差异使干净训练的模型更容易受到数据空间脱离流形方向的对抗扰动攻击。",
    "en_tdlr": "The paper demonstrates that the dimension gap between intrinsic and ambient dimensions makes clean-trained models more vulnerable to adversarial perturbations in the off-manifold direction of the data space."
}