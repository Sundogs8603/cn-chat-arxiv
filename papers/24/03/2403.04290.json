{
    "title": "MedM2G: Unifying Medical Multi-Modal Generation via Cross-Guided Diffusion with Visual Invariant",
    "abstract": "arXiv:2403.04290v1 Announce Type: cross  Abstract: Medical generative models, acknowledged for their high-quality sample generation ability, have accelerated the fast growth of medical applications. However, recent works concentrate on separate medical generation models for distinct medical tasks and are restricted to inadequate medical multi-modal knowledge, constraining medical comprehensive diagnosis. In this paper, we propose MedM2G, a Medical Multi-Modal Generative framework, with the key innovation to align, extract, and generate medical multi-modal within a unified model. Extending beyond single or two medical modalities, we efficiently align medical multi-modal through the central alignment approach in the unified space. Significantly, our framework extracts valuable clinical knowledge by preserving the medical visual invariant of each imaging modal, thereby enhancing specific medical information for multi-modal generation. By conditioning the adaptive cross-guided parameters i",
    "link": "https://arxiv.org/abs/2403.04290",
    "context": "Title: MedM2G: Unifying Medical Multi-Modal Generation via Cross-Guided Diffusion with Visual Invariant\nAbstract: arXiv:2403.04290v1 Announce Type: cross  Abstract: Medical generative models, acknowledged for their high-quality sample generation ability, have accelerated the fast growth of medical applications. However, recent works concentrate on separate medical generation models for distinct medical tasks and are restricted to inadequate medical multi-modal knowledge, constraining medical comprehensive diagnosis. In this paper, we propose MedM2G, a Medical Multi-Modal Generative framework, with the key innovation to align, extract, and generate medical multi-modal within a unified model. Extending beyond single or two medical modalities, we efficiently align medical multi-modal through the central alignment approach in the unified space. Significantly, our framework extracts valuable clinical knowledge by preserving the medical visual invariant of each imaging modal, thereby enhancing specific medical information for multi-modal generation. By conditioning the adaptive cross-guided parameters i",
    "path": "papers/24/03/2403.04290.json",
    "total_tokens": 862,
    "translated_title": "MedM2G：通过交叉引导扩散统一医学多模态生成与视觉不变性",
    "translated_abstract": "医学生成模型以其高质量样本生成能力而闻名，加速了医学应用的快速发展。然而，最近的研究集中于针对不同医学任务的单独医学生成模型，并且受到不足的医学多模态知识的限制，从而限制了医学全面诊断。在本文中，我们提出了MedM2G，一种医学多模态生成框架，其关键创新在于通过统一模型对医学多模态进行对齐、提取和生成。我们通过统一空间中的中心对齐方法高效地对医学多模态进行对齐，不仅限于单一或两种医学模态。值得注意的是，我们的框架通过保留每种成像模态的医学视觉不变性来提取有价值的临床知识，从而增强多模态生成的特定医学信息。通过调节自适应的交叉引导参数",
    "tldr": "MedM2G提出了一种医学多模态生成框架，通过统一模型对医学多模态进行对齐、提取和生成，提取有价值的临床知识并增强特定医学信息。",
    "en_tdlr": "MedM2G proposes a medical multi-modal generative framework that aligns, extracts, and generates medical multi-modal within a unified model, extracting valuable clinical knowledge and enhancing specific medical information."
}