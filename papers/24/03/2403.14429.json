{
    "title": "Style-Extracting Diffusion Models for Semi-Supervised Histopathology Segmentation",
    "abstract": "arXiv:2403.14429v1 Announce Type: cross  Abstract: Deep learning-based image generation has seen significant advancements with diffusion models, notably improving the quality of generated images. Despite these developments, generating images with unseen characteristics beneficial for downstream tasks has received limited attention. To bridge this gap, we propose Style-Extracting Diffusion Models, featuring two conditioning mechanisms. Specifically, we utilize 1) a style conditioning mechanism which allows to inject style information of previously unseen images during image generation and 2) a content conditioning which can be targeted to a downstream task, e.g., layout for segmentation. We introduce a trainable style encoder to extract style information from images, and an aggregation block that merges style information from multiple style inputs. This architecture enables the generation of images with unseen styles in a zero-shot manner, by leveraging styles from unseen images, result",
    "link": "https://arxiv.org/abs/2403.14429",
    "context": "Title: Style-Extracting Diffusion Models for Semi-Supervised Histopathology Segmentation\nAbstract: arXiv:2403.14429v1 Announce Type: cross  Abstract: Deep learning-based image generation has seen significant advancements with diffusion models, notably improving the quality of generated images. Despite these developments, generating images with unseen characteristics beneficial for downstream tasks has received limited attention. To bridge this gap, we propose Style-Extracting Diffusion Models, featuring two conditioning mechanisms. Specifically, we utilize 1) a style conditioning mechanism which allows to inject style information of previously unseen images during image generation and 2) a content conditioning which can be targeted to a downstream task, e.g., layout for segmentation. We introduce a trainable style encoder to extract style information from images, and an aggregation block that merges style information from multiple style inputs. This architecture enables the generation of images with unseen styles in a zero-shot manner, by leveraging styles from unseen images, result",
    "path": "papers/24/03/2403.14429.json",
    "total_tokens": 884,
    "translated_title": "风格提取扩散模型用于半监督组织学分割",
    "translated_abstract": "arXiv:2403.14429v1 公告类型:跨领域 摘要:基于深度学习的图像生成在扩散模型的显着进展下取得了重要进展，明显改善了生成图像的质量。尽管取得了这些进展，但生成具有对下游任务有益的未见特征的图像却受到了较少关注。为了弥补这一差距，我们提出了风格提取扩散模型，其中包含两种调节机制。具体来说，我们利用1)风格调制机制在图像生成过程中注入先前未见图像的风格信息，2)内容调制机制可以针对下游任务进行定位，例如布局用于分割。我们引入了可训练的风格编码器，从图像中提取风格信息，并引入了一个聚合块，用于合并来自多个风格输入的风格信息。这种架构使得通过利用来自未见图像的风格，在零-shot方式下生成具有未见风格的图像成为可能。",
    "tldr": "提出了风格提取扩散模型，利用风格调节机制和内容调节机制，实现了在图像生成过程中注入未见图像风格信息，从而以零-shot方式生成具有未见风格的图像。",
    "en_tdlr": "Proposed Style-Extracting Diffusion Models that utilize style conditioning and content conditioning mechanisms to inject style information of unseen images during image generation, enabling the generation of images with unseen styles in a zero-shot manner."
}