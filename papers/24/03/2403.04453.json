{
    "title": "Vlearn: Off-Policy Learning with Efficient State-Value Function Estimation",
    "abstract": "arXiv:2403.04453v1 Announce Type: new  Abstract: Existing off-policy reinforcement learning algorithms typically necessitate an explicit state-action-value function representation, which becomes problematic in high-dimensional action spaces. These algorithms often encounter challenges where they struggle with the curse of dimensionality, as maintaining a state-action-value function in such spaces becomes data-inefficient. In this work, we propose a novel off-policy trust region optimization approach, called Vlearn, that eliminates the requirement for an explicit state-action-value function. Instead, we demonstrate how to efficiently leverage just a state-value function as the critic, thus overcoming several limitations of existing methods. By doing so, Vlearn addresses the computational challenges posed by high-dimensional action spaces. Furthermore, Vlearn introduces an efficient approach to address the challenges associated with pure state-value function learning in the off-policy se",
    "link": "https://arxiv.org/abs/2403.04453",
    "context": "Title: Vlearn: Off-Policy Learning with Efficient State-Value Function Estimation\nAbstract: arXiv:2403.04453v1 Announce Type: new  Abstract: Existing off-policy reinforcement learning algorithms typically necessitate an explicit state-action-value function representation, which becomes problematic in high-dimensional action spaces. These algorithms often encounter challenges where they struggle with the curse of dimensionality, as maintaining a state-action-value function in such spaces becomes data-inefficient. In this work, we propose a novel off-policy trust region optimization approach, called Vlearn, that eliminates the requirement for an explicit state-action-value function. Instead, we demonstrate how to efficiently leverage just a state-value function as the critic, thus overcoming several limitations of existing methods. By doing so, Vlearn addresses the computational challenges posed by high-dimensional action spaces. Furthermore, Vlearn introduces an efficient approach to address the challenges associated with pure state-value function learning in the off-policy se",
    "path": "papers/24/03/2403.04453.json",
    "total_tokens": 846,
    "translated_title": "Vlearn：使用高效状态值函数估计的离策略学习",
    "translated_abstract": "存在的离策略强化学习算法通常需要明确状态-动作-值函数表示，这在高维动作空间中变得棘手。这些算法经常遇到挑战，即它们在处理维度灾难时遇到困难，因为在这样的空间中维护状态-动作-值函数变得数据效率低下。在这项工作中，我们提出了一种名为Vlearn的新型离策略信任区域优化方法，它消除了对明确状态-动作-值函数的要求。相反，我们展示了如何有效地利用状态值函数作为评论家，从而克服现有方法的几个局限性。通过这样做，Vlearn解决了高维动作空间所带来的计算挑战。此外，Vlearn引入了一种有效的方法来解决与纯状态值函数学习相关的离策略学习中的挑战。",
    "tldr": "Vlearn提出了一种新颖的离策略优化方法，称为Vlearn，它通过仅利用状态值函数作为评论家，消除了对明确状态-动作-值函数的需求，从而解决了高维动作空间中的计算挑战。",
    "en_tdlr": "Vlearn introduces a novel off-policy optimization approach that eliminates the need for an explicit state-action-value function by efficiently leveraging just a state-value function as the critic, addressing the computational challenges posed by high-dimensional action spaces."
}