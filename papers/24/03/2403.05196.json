{
    "title": "Denoising Autoregressive Representation Learning",
    "abstract": "arXiv:2403.05196v1 Announce Type: new  Abstract: In this paper, we explore a new generative approach for learning visual representations. Our method, DARL, employs a decoder-only Transformer to predict image patches autoregressively. We find that training with Mean Squared Error (MSE) alone leads to strong representations. To enhance the image generation ability, we replace the MSE loss with the diffusion objective by using a denoising patch decoder. We show that the learned representation can be improved by using tailored noise schedules and longer training in larger models. Notably, the optimal schedule differs significantly from the typical ones used in standard image diffusion models. Overall, despite its simple architecture, DARL delivers performance remarkably close to state-of-the-art masked prediction models under the fine-tuning protocol. This marks an important step towards a unified model capable of both visual perception and generation, effectively combining the strengths o",
    "link": "https://arxiv.org/abs/2403.05196",
    "context": "Title: Denoising Autoregressive Representation Learning\nAbstract: arXiv:2403.05196v1 Announce Type: new  Abstract: In this paper, we explore a new generative approach for learning visual representations. Our method, DARL, employs a decoder-only Transformer to predict image patches autoregressively. We find that training with Mean Squared Error (MSE) alone leads to strong representations. To enhance the image generation ability, we replace the MSE loss with the diffusion objective by using a denoising patch decoder. We show that the learned representation can be improved by using tailored noise schedules and longer training in larger models. Notably, the optimal schedule differs significantly from the typical ones used in standard image diffusion models. Overall, despite its simple architecture, DARL delivers performance remarkably close to state-of-the-art masked prediction models under the fine-tuning protocol. This marks an important step towards a unified model capable of both visual perception and generation, effectively combining the strengths o",
    "path": "papers/24/03/2403.05196.json",
    "total_tokens": 857,
    "translated_title": "降噪自回归表示学习",
    "translated_abstract": "在本文中，我们探索了一种用于学习视觉表示的新的生成方法。我们的方法DARL采用了仅解码器的Transformer来自回归地预测图像补丁。我们发现仅使用均方误差（MSE）进行训练会得到强大的表示。为了增强图像生成能力，我们使用去噪补丁解码器将MSE损失替换为扩散目标。我们展示了通过使用量身定制的噪声计划和更大模型的更长训练可以改善学习到的表示。值得注意的是，最佳计划与标准图像扩散模型中常用的计划有显著差异。总体而言，尽管其简单的架构，DARL在微调协议下的性能接近最先进的蒙版预测模型。这标志着向能够同时进行视觉感知和生成的统一模型迈出了重要的一步，有效地结合了其优势。",
    "tldr": "DARL使用仅解码器的Transformer进行自回归预测，通过使用去噪补丁解码器和特定噪声计划改善图像生成能力，实现了与蒙版预测模型相近的性能。",
    "en_tdlr": "DARL utilizes a decoder-only Transformer for autoregressive prediction, improves image generation ability by employing a denoising patch decoder and specific noise schedules, achieving performance close to state-of-the-art masked prediction models."
}