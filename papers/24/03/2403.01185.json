{
    "title": "Balancing Exploration and Exploitation in LLM using Soft RLLF for Enhanced Negation Understanding",
    "abstract": "arXiv:2403.01185v1 Announce Type: cross  Abstract: Finetuning approaches in NLP often focus on exploitation rather than exploration, which may lead to suboptimal models. Given the vast search space of natural language, this limited exploration can restrict their performance in complex, high-stakes domains, where accurate negation understanding and logical reasoning abilities are crucial. To address this issue, we leverage Reinforcement Learning from Logical Feedback (RLLF) to create an effective balance between exploration and exploitation in LLMs. Our approach employs an appropriate benchmark dataset for training and evaluation, highlighting the importance of exploration in enhancing negation understanding capabilities. We compare the performance of our RLLF-enhanced LLMs with baseline models trained without RLLF, demonstrating the value of this balanced approach. Furthermore, we showcase the potential of our method in legal AI applications by employing transfer learning and evaluatin",
    "link": "https://arxiv.org/abs/2403.01185",
    "context": "Title: Balancing Exploration and Exploitation in LLM using Soft RLLF for Enhanced Negation Understanding\nAbstract: arXiv:2403.01185v1 Announce Type: cross  Abstract: Finetuning approaches in NLP often focus on exploitation rather than exploration, which may lead to suboptimal models. Given the vast search space of natural language, this limited exploration can restrict their performance in complex, high-stakes domains, where accurate negation understanding and logical reasoning abilities are crucial. To address this issue, we leverage Reinforcement Learning from Logical Feedback (RLLF) to create an effective balance between exploration and exploitation in LLMs. Our approach employs an appropriate benchmark dataset for training and evaluation, highlighting the importance of exploration in enhancing negation understanding capabilities. We compare the performance of our RLLF-enhanced LLMs with baseline models trained without RLLF, demonstrating the value of this balanced approach. Furthermore, we showcase the potential of our method in legal AI applications by employing transfer learning and evaluatin",
    "path": "papers/24/03/2403.01185.json",
    "total_tokens": 864,
    "translated_title": "在LLM中使用Soft RLLF实现探索和开发的平衡，以增强否定理解",
    "translated_abstract": "在NLP中，调整方法通常侧重于开发而不是探索，这可能导致次优模型。考虑到自然语言的广阔搜索空间，这种有限的探索可能限制它们在复杂、高风险领域中的表现，那里准确的否定理解和逻辑推理能力至关重要。为解决这一问题，我们利用逻辑反馈的强化学习（RLLF）在LLMs中实现探索和开发的有效平衡。我们的方法采用适当的基准数据集进行训练和评估，突出了通过增强否定理解能力来强调探索的重要性。我们将使用RLLF增强的LLMs的性能与未使用RLLF训练的基线模型进行比较，展示了这种平衡方法的价值。此外，我们通过迁移学习展示了我们的方法在法律AI应用中的潜力，并进行了评价。",
    "tldr": "通过利用逻辑反馈的强化学习（RLLF）在LLMs中实现探索和开发的平衡，以增强否定理解能力，并通过比较性能验证了这种平衡方法的价值。",
    "en_tdlr": "Achieving a balance between exploration and exploitation in LLMs using Reinforcement Learning from Logical Feedback (RLLF) enhances negation understanding capabilities, as demonstrated by comparing performance with baseline models."
}