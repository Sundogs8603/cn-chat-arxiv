{
    "title": "Alignment Studio: Aligning Large Language Models to Particular Contextual Regulations",
    "abstract": "arXiv:2403.09704v1 Announce Type: cross  Abstract: The alignment of large language models is usually done by model providers to add or control behaviors that are common or universally understood across use cases and contexts. In contrast, in this article, we present an approach and architecture that empowers application developers to tune a model to their particular values, social norms, laws and other regulations, and orchestrate between potentially conflicting requirements in context. We lay out three main components of such an Alignment Studio architecture: Framers, Instructors, and Auditors that work in concert to control the behavior of a language model. We illustrate this approach with a running example of aligning a company's internal-facing enterprise chatbot to its business conduct guidelines.",
    "link": "https://arxiv.org/abs/2403.09704",
    "context": "Title: Alignment Studio: Aligning Large Language Models to Particular Contextual Regulations\nAbstract: arXiv:2403.09704v1 Announce Type: cross  Abstract: The alignment of large language models is usually done by model providers to add or control behaviors that are common or universally understood across use cases and contexts. In contrast, in this article, we present an approach and architecture that empowers application developers to tune a model to their particular values, social norms, laws and other regulations, and orchestrate between potentially conflicting requirements in context. We lay out three main components of such an Alignment Studio architecture: Framers, Instructors, and Auditors that work in concert to control the behavior of a language model. We illustrate this approach with a running example of aligning a company's internal-facing enterprise chatbot to its business conduct guidelines.",
    "path": "papers/24/03/2403.09704.json",
    "total_tokens": 744,
    "translated_title": "对齐大型语言模型至特定情境规范的 Alignment Studio",
    "translated_abstract": "大型语言模型的对齐通常由模型提供者进行，以添加或控制跨用例和情境中通用或普遍理解的行为。相比之下，本文提出了一种方法和架构，赋予应用开发者调整模型至其特定价值观、社会规范、法律和其他法规的能力，并在情境中协调潜在冲突的需求。我们阐述了这种对齐工作室架构的三个主要组成部分：构架者、指导者和审核者共同作用于控制语言模型的行为。我们通过一个企业内部聊天机器人对齐到业务行为准则的实例来说明这种方法。",
    "tldr": "本文提出了 Alignment Studio 架构，使应用开发者能够调整大型语言模型至他们特定的价值观、社会规范、法律和其他法规，并协调潜在冲突的需求。",
    "en_tdlr": "The paper introduces the Alignment Studio architecture, allowing application developers to adjust large language models to their specific values, social norms, laws, and other regulations, while orchestrating potentially conflicting requirements."
}