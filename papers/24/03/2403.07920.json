{
    "title": "ProtLLM: An Interleaved Protein-Language LLM with Protein-as-Word Pre-Training",
    "abstract": "arXiv:2403.07920v1 Announce Type: cross  Abstract: We propose ProtLLM, a versatile cross-modal large language model (LLM) for both protein-centric and protein-language tasks. ProtLLM features a unique dynamic protein mounting mechanism, enabling it to handle complex inputs where the natural language text is interspersed with an arbitrary number of proteins. Besides, we propose the protein-as-word language modeling approach to train ProtLLM. By developing a specialized protein vocabulary, we equip the model with the capability to predict not just natural language but also proteins from a vast pool of candidates. Additionally, we construct a large-scale interleaved protein-text dataset, named InterPT, for pre-training. This dataset comprehensively encompasses both (1) structured data sources like protein annotations and (2) unstructured data sources like biological research papers, thereby endowing ProtLLM with crucial knowledge for understanding proteins. We evaluate ProtLLM on classic ",
    "link": "https://arxiv.org/abs/2403.07920",
    "context": "Title: ProtLLM: An Interleaved Protein-Language LLM with Protein-as-Word Pre-Training\nAbstract: arXiv:2403.07920v1 Announce Type: cross  Abstract: We propose ProtLLM, a versatile cross-modal large language model (LLM) for both protein-centric and protein-language tasks. ProtLLM features a unique dynamic protein mounting mechanism, enabling it to handle complex inputs where the natural language text is interspersed with an arbitrary number of proteins. Besides, we propose the protein-as-word language modeling approach to train ProtLLM. By developing a specialized protein vocabulary, we equip the model with the capability to predict not just natural language but also proteins from a vast pool of candidates. Additionally, we construct a large-scale interleaved protein-text dataset, named InterPT, for pre-training. This dataset comprehensively encompasses both (1) structured data sources like protein annotations and (2) unstructured data sources like biological research papers, thereby endowing ProtLLM with crucial knowledge for understanding proteins. We evaluate ProtLLM on classic ",
    "path": "papers/24/03/2403.07920.json",
    "total_tokens": 971,
    "translated_title": "ProtLLM：一种具有蛋白质作为单词预训练的交织式蛋白质-语言LLM",
    "translated_abstract": "我们提出了ProtLLM，一种多功能的跨模式大型语言模型（LLM），用于处理既有蛋白质为中心又有蛋白质-语言任务。ProtLLM具有独特的动态蛋白质装配机制，使其能够处理自然语言文本与任意数量的蛋白质交织在一起的复杂输入。此外，我们提出了蛋白质作为单词语言建模方法来训练ProtLLM。通过开发专门的蛋白质词汇表，我们赋予该模型不仅预测自然语言而且预测来自大量候选蛋白质的能力。此外，我们构建了一个大规模的交织式蛋白质-文本数据集，命名为InterPT，用于预训练。该数据集全面涵盖了结构化数据源（如蛋白质注释）和非结构化数据源（如生物研究论文），从而赋予ProtLLM理解蛋白质的关键知识。我们在经典数据集上对ProtLLM进行了评估。",
    "tldr": "提出了ProtLLM，一种具有独特动态蛋白质装配机制及蛋白质作为单词语言建模方法的交织式蛋白质-语言LLM，并构建了大规模的交织式蛋白质-文本数据集用于预训练。"
}