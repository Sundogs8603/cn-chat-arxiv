{
    "title": "Semantic Text Transmission via Prediction with Small Language Models: Cost-Similarity Trade-off",
    "abstract": "arXiv:2403.00290v1 Announce Type: cross  Abstract: We consider the communication of natural language text from a source to a destination over noiseless and character-erasure channels. We exploit language's inherent correlations and predictability to constrain transmission costs by allowing the destination to predict or complete words with potential dissimilarity with the source text. Concretely, our objective is to obtain achievable $(\\bar{c}, \\bar{s})$ pairs, where $\\bar{c}$ is the average transmission cost at the source and $\\bar{s}$ is the average semantic similarity measured via cosine similarity between vector embedding of words at the source and those predicted/completed at the destination. We obtain $(\\bar{c}, \\bar{s})$ pairs for neural language and first-order Markov chain-based small language models (SLM) for prediction, using both a threshold policy that transmits a word if its cosine similarity with that predicted/completed at the destination is below a threshold, and a peri",
    "link": "https://arxiv.org/abs/2403.00290",
    "context": "Title: Semantic Text Transmission via Prediction with Small Language Models: Cost-Similarity Trade-off\nAbstract: arXiv:2403.00290v1 Announce Type: cross  Abstract: We consider the communication of natural language text from a source to a destination over noiseless and character-erasure channels. We exploit language's inherent correlations and predictability to constrain transmission costs by allowing the destination to predict or complete words with potential dissimilarity with the source text. Concretely, our objective is to obtain achievable $(\\bar{c}, \\bar{s})$ pairs, where $\\bar{c}$ is the average transmission cost at the source and $\\bar{s}$ is the average semantic similarity measured via cosine similarity between vector embedding of words at the source and those predicted/completed at the destination. We obtain $(\\bar{c}, \\bar{s})$ pairs for neural language and first-order Markov chain-based small language models (SLM) for prediction, using both a threshold policy that transmits a word if its cosine similarity with that predicted/completed at the destination is below a threshold, and a peri",
    "path": "papers/24/03/2403.00290.json",
    "total_tokens": 827,
    "translated_title": "基于小型语言模型的语义文本传输：成本-相似度权衡",
    "translated_abstract": "我们考虑在无噪音和字符擦除通道上从源到目的地传输自然语言文本。我们利用语言的固有相关性和可预测性，通过允许目的地预测或补全与源文本可能不相似的单词来限制传输成本。我们的目标是获得可实现的$(\\bar{c}, \\bar{s})$对，其中$\\bar{c}$是源头的平均传输成本，$\\bar{s}$是通过余弦相似度测量的源头词向量和目的地预测/补全词向量之间的平均语义相似度。我们使用神经语言模型和基于一阶马尔可夫链的小型语言模型(SLM)进行预测，为传输使用了阈值策略，即如果单词与目的地预测/补全的单词的余弦相似度低于阈值，则传输该单词。",
    "tldr": "该研究通过使用小型语言模型进行预测，实现了在语义文本传输中的成本和相似度之间的权衡。"
}