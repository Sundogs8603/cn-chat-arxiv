{
    "title": "One-Shot Averaging for Distributed TD($\\lambda$) Under Markov Sampling",
    "abstract": "arXiv:2403.08896v1 Announce Type: new  Abstract: We consider a distributed setup for reinforcement learning, where each agent has a copy of the same Markov Decision Process but transitions are sampled from the corresponding Markov chain independently by each agent. We show that in this setting, we can achieve a linear speedup for TD($\\lambda$), a family of popular methods for policy evaluation, in the sense that $N$ agents can evaluate a policy $N$ times faster provided the target accuracy is small enough. Notably, this speedup is achieved by ``one shot averaging,'' a procedure where the agents run TD($\\lambda$) with Markov sampling independently and only average their results after the final step. This significantly reduces the amount of communication required to achieve a linear speedup relative to previous work.",
    "link": "https://arxiv.org/abs/2403.08896",
    "context": "Title: One-Shot Averaging for Distributed TD($\\lambda$) Under Markov Sampling\nAbstract: arXiv:2403.08896v1 Announce Type: new  Abstract: We consider a distributed setup for reinforcement learning, where each agent has a copy of the same Markov Decision Process but transitions are sampled from the corresponding Markov chain independently by each agent. We show that in this setting, we can achieve a linear speedup for TD($\\lambda$), a family of popular methods for policy evaluation, in the sense that $N$ agents can evaluate a policy $N$ times faster provided the target accuracy is small enough. Notably, this speedup is achieved by ``one shot averaging,'' a procedure where the agents run TD($\\lambda$) with Markov sampling independently and only average their results after the final step. This significantly reduces the amount of communication required to achieve a linear speedup relative to previous work.",
    "path": "papers/24/03/2403.08896.json",
    "total_tokens": 839,
    "translated_title": "一次性平均化在马尔可夫采样下的分布式TD($\\lambda$)",
    "translated_abstract": "我们考虑一种分布式强化学习设置，每个agent都拥有相同的马尔可夫决策过程副本，但是转换是独立地从相应的马尔可夫链中由每个agent采样的。我们展示在这种设置下，我们可以实现对于TD($\\lambda$)的线性加速，这是一系列流行的用于策略评估的方法，即若目标精度足够小，$N$个agents可以以$N$倍速度评估一个策略。值得注意的是，这种加速是通过“一次性平均化”实现的，即agent们独立地使用马尔可夫采样运行TD($\\lambda$)，并且仅在最后一步之后对他们的结果进行平均。相对于以前的工作，这显著减少了实现线性加速所需的通信量。",
    "tldr": "在分布式强化学习中，通过一次性平均化的方法，每个agent独立进行TD($\\lambda$)运算，并最终在结果上进行平均，实现了相对于以往工作更少的通信量要求的线性加速。",
    "en_tdlr": "In distributed reinforcement learning, a linear speedup is achieved by independently running TD($\\lambda$) with Markov sampling for each agent and averaging the results afterwards using a one-shot averaging approach, leading to reduced communication requirements compared to prior work."
}