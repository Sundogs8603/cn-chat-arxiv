{
    "title": "Efficiently Assemble Normalization Layers and Regularization for Federated Domain Generalization",
    "abstract": "arXiv:2403.15605v1 Announce Type: cross  Abstract: Domain shift is a formidable issue in Machine Learning that causes a model to suffer from performance degradation when tested on unseen domains. Federated Domain Generalization (FedDG) attempts to train a global model using collaborative clients in a privacy-preserving manner that can generalize well to unseen clients possibly with domain shift. However, most existing FedDG methods either cause additional privacy risks of data leakage or induce significant costs in client communication and computation, which are major concerns in the Federated Learning paradigm. To circumvent these challenges, here we introduce a novel architectural method for FedDG, namely gPerXAN, which relies on a normalization scheme working with a guiding regularizer. In particular, we carefully design Personalized eXplicitly Assembled Normalization to enforce client models selectively filtering domain-specific features that are biased towards local data while ret",
    "link": "https://arxiv.org/abs/2403.15605",
    "context": "Title: Efficiently Assemble Normalization Layers and Regularization for Federated Domain Generalization\nAbstract: arXiv:2403.15605v1 Announce Type: cross  Abstract: Domain shift is a formidable issue in Machine Learning that causes a model to suffer from performance degradation when tested on unseen domains. Federated Domain Generalization (FedDG) attempts to train a global model using collaborative clients in a privacy-preserving manner that can generalize well to unseen clients possibly with domain shift. However, most existing FedDG methods either cause additional privacy risks of data leakage or induce significant costs in client communication and computation, which are major concerns in the Federated Learning paradigm. To circumvent these challenges, here we introduce a novel architectural method for FedDG, namely gPerXAN, which relies on a normalization scheme working with a guiding regularizer. In particular, we carefully design Personalized eXplicitly Assembled Normalization to enforce client models selectively filtering domain-specific features that are biased towards local data while ret",
    "path": "papers/24/03/2403.15605.json",
    "total_tokens": 885,
    "translated_title": "为联邦领域泛化高效组合规范化层与正则化方法",
    "translated_abstract": "领域转移是机器学习中一个严峻的问题，会导致模型在未知领域测试时性能下降。联邦领域泛化（FedDG）旨在以隐私保护的方式使用协作客户端训练全局模型，能够很好地泛化到可能存在领域转移的未知客户端。然而，大多数现有的FedDG方法可能会导致额外的数据泄露隐私风险，或者在客户端通信和计算成本方面产生显著开销，这在联邦学习范式中是主要关注的问题。为了解决这些挑战，我们引入了一种新颖的FedDG架构方法，即gPerXAN，它依赖于一个规范化方案与引导正则化器配合工作。具体来说，我们精心设计了个性化显式组装规范化，以强制客户端模型有选择地过滤对本地数据有偏向的特定领域特征。",
    "tldr": "引入了一种新颖的FedDG架构方法gPerXAN，通过规范化方案和引导正则化器配合工作，实现了个性化显式组装规范化，有助于客户端模型对领域特征进行有选择性过滤。"
}