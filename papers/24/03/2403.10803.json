{
    "title": "Enhancing Out-of-Distribution Detection with Multitesting-based Layer-wise Feature Fusion",
    "abstract": "arXiv:2403.10803v1 Announce Type: cross  Abstract: Deploying machine learning in open environments presents the challenge of encountering diverse test inputs that differ significantly from the training data. These out-of-distribution samples may exhibit shifts in local or global features compared to the training distribution. The machine learning (ML) community has responded with a number of methods aimed at distinguishing anomalous inputs from original training data. However, the majority of previous studies have primarily focused on the output layer or penultimate layer of pre-trained deep neural networks. In this paper, we propose a novel framework, Multitesting-based Layer-wise Out-of-Distribution (OOD) Detection (MLOD), to identify distributional shifts in test samples at different levels of features through rigorous multiple testing procedure. Our approach distinguishes itself from existing methods as it does not require modifying the structure or fine-tuning of the pre-trained c",
    "link": "https://arxiv.org/abs/2403.10803",
    "context": "Title: Enhancing Out-of-Distribution Detection with Multitesting-based Layer-wise Feature Fusion\nAbstract: arXiv:2403.10803v1 Announce Type: cross  Abstract: Deploying machine learning in open environments presents the challenge of encountering diverse test inputs that differ significantly from the training data. These out-of-distribution samples may exhibit shifts in local or global features compared to the training distribution. The machine learning (ML) community has responded with a number of methods aimed at distinguishing anomalous inputs from original training data. However, the majority of previous studies have primarily focused on the output layer or penultimate layer of pre-trained deep neural networks. In this paper, we propose a novel framework, Multitesting-based Layer-wise Out-of-Distribution (OOD) Detection (MLOD), to identify distributional shifts in test samples at different levels of features through rigorous multiple testing procedure. Our approach distinguishes itself from existing methods as it does not require modifying the structure or fine-tuning of the pre-trained c",
    "path": "papers/24/03/2403.10803.json",
    "total_tokens": 754,
    "translated_title": "利用基于多测试的逐层特征融合增强超出分布检测",
    "translated_abstract": "在开放环境中部署机器学习会遇到一个挑战，即遇到与训练数据显著不同的各种测试输入，这些超出分布的样本可能在局部或全局特征上与训练分布有所偏移。本文提出了一种新颖的框架，名为基于多测试的逐层超出分布（OOD）检测（MLOD），通过严格的多个测试过程在不同级别的特征中鉴别测试样本中的分布偏移。我们的方法不同于现有方法，因为它不需要修改预训练的深度神经网络的结构或微调。",
    "tldr": "提出了一种名为MLOD的新框架，利用多测试过程在不同级别的特征中识别测试样本中的分布偏移，无需修改预训练模型结构。"
}