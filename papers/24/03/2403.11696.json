{
    "title": "Generalization error of spectral algorithms",
    "abstract": "arXiv:2403.11696v1 Announce Type: new  Abstract: The asymptotically precise estimation of the generalization of kernel methods has recently received attention due to the parallels between neural networks and their associated kernels. However, prior works derive such estimates for training by kernel ridge regression (KRR), whereas neural networks are typically trained with gradient descent (GD). In the present work, we consider the training of kernels with a family of $\\textit{spectral algorithms}$ specified by profile $h(\\lambda)$, and including KRR and GD as special cases. Then, we derive the generalization error as a functional of learning profile $h(\\lambda)$ for two data models: high-dimensional Gaussian and low-dimensional translation-invariant model. Under power-law assumptions on the spectrum of the kernel and target, we use our framework to (i) give full loss asymptotics for both noisy and noiseless observations (ii) show that the loss localizes on certain spectral scales, givi",
    "link": "https://arxiv.org/abs/2403.11696",
    "context": "Title: Generalization error of spectral algorithms\nAbstract: arXiv:2403.11696v1 Announce Type: new  Abstract: The asymptotically precise estimation of the generalization of kernel methods has recently received attention due to the parallels between neural networks and their associated kernels. However, prior works derive such estimates for training by kernel ridge regression (KRR), whereas neural networks are typically trained with gradient descent (GD). In the present work, we consider the training of kernels with a family of $\\textit{spectral algorithms}$ specified by profile $h(\\lambda)$, and including KRR and GD as special cases. Then, we derive the generalization error as a functional of learning profile $h(\\lambda)$ for two data models: high-dimensional Gaussian and low-dimensional translation-invariant model. Under power-law assumptions on the spectrum of the kernel and target, we use our framework to (i) give full loss asymptotics for both noisy and noiseless observations (ii) show that the loss localizes on certain spectral scales, givi",
    "path": "papers/24/03/2403.11696.json",
    "total_tokens": 851,
    "translated_title": "光谱算法的泛化误差",
    "translated_abstract": "近期关注核方法泛化的渐近精确估计，源于神经网络及其相关核之间的类比。然而，以往的研究是通过核岭回归（KRR）推导出这些估计值，而神经网络通常是通过梯度下降（GD）进行训练。本研究考虑了使用由配置文件$h(\\lambda)$指定的一系列“光谱算法”来训练核，其中包括KRR和GD作为特例。然后，我们推导出关于两种数据模型的学习配置文件$h(\\lambda)$的泛化误差函数：高维高斯模型和低维平移不变模型。在对核和目标的频谱进行幂律假设的情况下，我们利用我们的框架来(i)为有噪和无噪观测提供完整的损失渐近行为；(ii)展示损失出现在某些频谱尺度上的局部化，",
    "tldr": "本研究考虑了使用光谱算法来训练核，推导出泛化误差函数，提供了完整的损失渐近行为，展示了损失在特定频谱尺度上的局部化。",
    "en_tdlr": "This study considers training kernels with spectral algorithms, derives the generalization error function, provides complete loss asymptotics, and shows the localization of loss on certain spectral scales."
}