{
    "title": "Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models",
    "abstract": "arXiv:2403.12881v1 Announce Type: new  Abstract: Open-sourced Large Language Models (LLMs) have achieved great success in various NLP tasks, however, they are still far inferior to API-based models when acting as agents. How to integrate agent ability into general LLMs becomes a crucial and urgent problem. This paper first delivers three key observations: (1) the current agent training corpus is entangled with both formats following and agent reasoning, which significantly shifts from the distribution of its pre-training data; (2) LLMs exhibit different learning speeds on the capabilities required by agent tasks; and (3) current approaches have side-effects when improving agent abilities by introducing hallucinations. Based on the above findings, we propose Agent-FLAN to effectively Fine-tune LANguage models for Agents. Through careful decomposition and redesign of the training corpus, Agent-FLAN enables Llama2-7B to outperform prior best works by 3.5\\% across various agent evaluation ",
    "link": "https://arxiv.org/abs/2403.12881",
    "context": "Title: Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models\nAbstract: arXiv:2403.12881v1 Announce Type: new  Abstract: Open-sourced Large Language Models (LLMs) have achieved great success in various NLP tasks, however, they are still far inferior to API-based models when acting as agents. How to integrate agent ability into general LLMs becomes a crucial and urgent problem. This paper first delivers three key observations: (1) the current agent training corpus is entangled with both formats following and agent reasoning, which significantly shifts from the distribution of its pre-training data; (2) LLMs exhibit different learning speeds on the capabilities required by agent tasks; and (3) current approaches have side-effects when improving agent abilities by introducing hallucinations. Based on the above findings, we propose Agent-FLAN to effectively Fine-tune LANguage models for Agents. Through careful decomposition and redesign of the training corpus, Agent-FLAN enables Llama2-7B to outperform prior best works by 3.5\\% across various agent evaluation ",
    "path": "papers/24/03/2403.12881.json",
    "total_tokens": 889,
    "translated_title": "Agent-FLAN: 为了大型语言模型的有效代理调节设计数据和方法",
    "translated_abstract": "开源的大型语言模型（LLMs）在各种自然语言处理任务中取得了巨大成功，然而，与基于API的模型相比，它们在充当代理时仍然明显逊色。如何将代理能力整合到一般的LLMs中成为一个关键且紧迫的问题。本文首先提出三个关键观察：（1）当前的代理训练语料库与其预训练数据的分布明显不同，同时涉及遵循格式和代理推理；（2）LLMs在代理任务所需的能力上展现出不同的学习速度；以及（3）通过引入幻觉来提高代理能力的当前方法存在副作用。基于以上发现，我们提出Agent-FLAN，用于有效地为代理调试语言模型。通过对训练语料库进行精心的分解和重新设计，Agent-FLAN使得Llama2-7B在各种代理评估中超过先前的最佳工作3.5％。",
    "tldr": "本文提出了Agent-FLAN，通过精心分解和重新设计训练语料库，使得Llama2-7B在各种代理评估中超过先前的最佳工作3.5％",
    "en_tdlr": "This paper introduces Agent-FLAN, which through careful decomposition and redesign of the training corpus, enables Llama2-7B to outperform prior best works by 3.5% across various agent evaluations."
}