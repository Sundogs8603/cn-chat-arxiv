{
    "title": "Near-Optimal Solutions of Constrained Learning Problems",
    "abstract": "arXiv:2403.11844v1 Announce Type: new  Abstract: With the widespread adoption of machine learning systems, the need to curtail their behavior has become increasingly apparent. This is evidenced by recent advancements towards developing models that satisfy robustness, safety, and fairness requirements. These requirements can be imposed (with generalization guarantees) by formulating constrained learning problems that can then be tackled by dual ascent algorithms. Yet, though these algorithms converge in objective value, even in non-convex settings, they cannot guarantee that their outcome is feasible. Doing so requires randomizing over all iterates, which is impractical in virtually any modern applications. Still, final iterates have been observed to perform well in practice. In this work, we address this gap between theory and practice by characterizing the constraint violation of Lagrangian minimizers associated with optimal dual variables, despite lack of convexity. To do this, we le",
    "link": "https://arxiv.org/abs/2403.11844",
    "context": "Title: Near-Optimal Solutions of Constrained Learning Problems\nAbstract: arXiv:2403.11844v1 Announce Type: new  Abstract: With the widespread adoption of machine learning systems, the need to curtail their behavior has become increasingly apparent. This is evidenced by recent advancements towards developing models that satisfy robustness, safety, and fairness requirements. These requirements can be imposed (with generalization guarantees) by formulating constrained learning problems that can then be tackled by dual ascent algorithms. Yet, though these algorithms converge in objective value, even in non-convex settings, they cannot guarantee that their outcome is feasible. Doing so requires randomizing over all iterates, which is impractical in virtually any modern applications. Still, final iterates have been observed to perform well in practice. In this work, we address this gap between theory and practice by characterizing the constraint violation of Lagrangian minimizers associated with optimal dual variables, despite lack of convexity. To do this, we le",
    "path": "papers/24/03/2403.11844.json",
    "total_tokens": 843,
    "translated_title": "受限制学习问题的近似最优解决方案",
    "translated_abstract": "随着机器学习系统的广泛应用，限制它们行为的需求变得日益明显。最近的进展表明，对满足鲁棒性、安全性和公平性要求的模型的开发已经引起了广泛关注。这些要求可以通过制定受限制的学习问题并通过对偶上升算法来解决，以实现泛化保证。然而，即使在非凸设置中，这些算法收敛于目标值，也无法保证其结果是可行的。为了做到这一点，需要在所有迭代上进行随机化，这在任何现代应用中几乎是不切实际的。尽管如此，实践中观察到的最终迭代表现良好。在这项工作中，我们通过表征与最优对偶变量相关的Lagrange最小化器的约束违反，尽管缺乏凸性，来解决实践与理论之间的差距。",
    "tldr": "本文研究了在非凸设置中，通过表征与最优对偶变量相关的Lagrange最小化器的约束违反来弥合实践与理论之间的差距。",
    "en_tdlr": "This paper bridges the gap between theory and practice by characterizing the constraint violation of Lagrangian minimizers associated with optimal dual variables in non-convex settings."
}