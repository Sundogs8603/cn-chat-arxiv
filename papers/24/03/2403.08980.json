{
    "title": "Architectural Implications of Neural Network Inference for High Data-Rate, Low-Latency Scientific Applications",
    "abstract": "arXiv:2403.08980v1 Announce Type: new  Abstract: With more scientific fields relying on neural networks (NNs) to process data incoming at extreme throughputs and latencies, it is crucial to develop NNs with all their parameters stored on-chip. In many of these applications, there is not enough time to go off-chip and retrieve weights. Even more so, off-chip memory such as DRAM does not have the bandwidth required to process these NNs as fast as the data is being produced (e.g., every 25 ns). As such, these extreme latency and bandwidth requirements have architectural implications for the hardware intended to run these NNs: 1) all NN parameters must fit on-chip, and 2) codesigning custom/reconfigurable logic is often required to meet these latency and bandwidth constraints. In our work, we show that many scientific NN applications must run fully on chip, in the extreme case requiring a custom chip to meet such stringent constraints.",
    "link": "https://arxiv.org/abs/2403.08980",
    "context": "Title: Architectural Implications of Neural Network Inference for High Data-Rate, Low-Latency Scientific Applications\nAbstract: arXiv:2403.08980v1 Announce Type: new  Abstract: With more scientific fields relying on neural networks (NNs) to process data incoming at extreme throughputs and latencies, it is crucial to develop NNs with all their parameters stored on-chip. In many of these applications, there is not enough time to go off-chip and retrieve weights. Even more so, off-chip memory such as DRAM does not have the bandwidth required to process these NNs as fast as the data is being produced (e.g., every 25 ns). As such, these extreme latency and bandwidth requirements have architectural implications for the hardware intended to run these NNs: 1) all NN parameters must fit on-chip, and 2) codesigning custom/reconfigurable logic is often required to meet these latency and bandwidth constraints. In our work, we show that many scientific NN applications must run fully on chip, in the extreme case requiring a custom chip to meet such stringent constraints.",
    "path": "papers/24/03/2403.08980.json",
    "total_tokens": 926,
    "translated_title": "神经网络推断对高数据速率、低延迟科学应用的架构影响",
    "translated_abstract": "随着越来越多的科学领域依赖神经网络（NNs）处理极高吞吐量和延迟的数据，开发所有参数存储在芯片上的神经网络是至关重要的。在许多应用中，没有足够的时间去从芯片外检索权重。此外，例如DRAM的芯片外内存没有足够的带宽来按照数据产生的速度快速处理这些神经网络（例如，每25纳秒）。因此，这些极端的延迟和带宽要求对旨在运行这些神经网络的硬件有架构上的影响：1）所有神经网络参数必须适合在芯片上，2）通常需要协同设计自定义/可重构逻辑以满足这些延迟和带宽约束。在我们的工作中，我们展示了许多科学神经网络应用必须完全在芯片上运行，极端情况下需要定制芯片来满足如此严格的约束。",
    "tldr": "神经网络推断在高数据速率、低延迟科学应用中的架构要求：所有参数存储在芯片上，需要协同设计自定义/可重构逻辑以满足极端的延迟和带宽约束"
}