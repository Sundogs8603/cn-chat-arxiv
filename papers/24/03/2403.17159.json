{
    "title": "Less Is More - On the Importance of Sparsification for Transformers and Graph Neural Networks for TSP",
    "abstract": "arXiv:2403.17159v1 Announce Type: cross  Abstract: Most of the recent studies tackling routing problems like the Traveling Salesman Problem (TSP) with machine learning use a transformer or Graph Neural Network (GNN) based encoder architecture. However, many of them apply these encoders naively by allowing them to aggregate information over the whole TSP instances. We, on the other hand, propose a data preprocessing method that allows the encoders to focus on the most relevant parts of the TSP instances only. In particular, we propose graph sparsification for TSP graph representations passed to GNNs and attention masking for TSP instances passed to transformers where the masks correspond to the adjacency matrices of the sparse TSP graph representations. Furthermore, we propose ensembles of different sparsification levels allowing models to focus on the most promising parts while also allowing information flow between all nodes of a TSP instance. In the experimental studies, we show that",
    "link": "https://arxiv.org/abs/2403.17159",
    "context": "Title: Less Is More - On the Importance of Sparsification for Transformers and Graph Neural Networks for TSP\nAbstract: arXiv:2403.17159v1 Announce Type: cross  Abstract: Most of the recent studies tackling routing problems like the Traveling Salesman Problem (TSP) with machine learning use a transformer or Graph Neural Network (GNN) based encoder architecture. However, many of them apply these encoders naively by allowing them to aggregate information over the whole TSP instances. We, on the other hand, propose a data preprocessing method that allows the encoders to focus on the most relevant parts of the TSP instances only. In particular, we propose graph sparsification for TSP graph representations passed to GNNs and attention masking for TSP instances passed to transformers where the masks correspond to the adjacency matrices of the sparse TSP graph representations. Furthermore, we propose ensembles of different sparsification levels allowing models to focus on the most promising parts while also allowing information flow between all nodes of a TSP instance. In the experimental studies, we show that",
    "path": "papers/24/03/2403.17159.json",
    "total_tokens": 864,
    "translated_title": "少即是多 - 关于稀疏化在Transformers和图神经网络在TSP问题中的重要性",
    "translated_abstract": "大多数最近研究处理旅行商问题（TSP）等路由问题的机器学习方法使用基于transformer或图神经网络（GNN）的编码器架构。然而，其中许多研究直接应用这些编码器，允许它们在整个TSP实例上聚合信息。相反，我们提出了一种数据预处理方法，使编码器仅关注TSP实例的最相关部分。具体来说，我们为传递给GNN的TSP图表示提出了图稀疏化，并为传递给transformers的TSP实例提出了注意力屏蔽，其中mask对应于稀疏TSP图表示的邻接矩阵。此外，我们提出了不同稀疏化级别的集合，使模型能够专注于最有前途的部分，同时还允许TSP实例的所有节点之间的信息流动。在实验研究中，我们展示了",
    "tldr": "我们提出了一种数据预处理方法，通过稀疏化TSP图表示和注意力掩码，使编码器集中于TSP实例的关键部分，同时允许信息在所有节点之间自由流动。"
}