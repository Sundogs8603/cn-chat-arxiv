{
    "title": "CrossTune: Black-Box Few-Shot Classification with Label Enhancement",
    "abstract": "arXiv:2403.12468v1 Announce Type: new  Abstract: Training or finetuning large-scale language models (LLMs) requires substantial computation resources, motivating recent efforts to explore parameter-efficient adaptation to downstream tasks. One approach is to treat these models as black boxes and use forward passes (Inference APIs) to interact with them. Current research focuses on adapting these black-box models to downstream tasks using gradient-free prompt optimization, but this often involves an expensive process of searching task-specific prompts. Therefore, we are motivated to study black-box language model adaptation without prompt search. Specifically, we introduce a label-enhanced cross-attention network called CrossTune, which models the semantic relatedness between the input text sequence and task-specific label descriptions. Its effectiveness is examined in the context of few-shot text classification. To improve the generalization of CrossTune, we utilize ChatGPT to generate",
    "link": "https://arxiv.org/abs/2403.12468",
    "context": "Title: CrossTune: Black-Box Few-Shot Classification with Label Enhancement\nAbstract: arXiv:2403.12468v1 Announce Type: new  Abstract: Training or finetuning large-scale language models (LLMs) requires substantial computation resources, motivating recent efforts to explore parameter-efficient adaptation to downstream tasks. One approach is to treat these models as black boxes and use forward passes (Inference APIs) to interact with them. Current research focuses on adapting these black-box models to downstream tasks using gradient-free prompt optimization, but this often involves an expensive process of searching task-specific prompts. Therefore, we are motivated to study black-box language model adaptation without prompt search. Specifically, we introduce a label-enhanced cross-attention network called CrossTune, which models the semantic relatedness between the input text sequence and task-specific label descriptions. Its effectiveness is examined in the context of few-shot text classification. To improve the generalization of CrossTune, we utilize ChatGPT to generate",
    "path": "papers/24/03/2403.12468.json",
    "total_tokens": 808,
    "translated_title": "CrossTune: 带有标签增强的黑盒少样本分类",
    "translated_abstract": "训练或微调大规模语言模型（LLMs）需要大量计算资源，鼓励最近的努力探索对下游任务进行参数高效适应的方法。一种方法是将这些模型视为黑盒，并使用前向传递（推理API）与它们进行交互。目前的研究集中在使用无梯度提示优化将这些黑盒模型适应到下游任务上，但这通常涉及一个昂贵的搜索特定任务提示的过程。因此，我们受到动机去研究无需搜索提示的黑盒语言模型适应。具体来说，我们引入了一个名为CrossTune的标签增强跨注意力网络，它模拟了输入文本序列与特定任务标签描述之间的语义相关性。其有效性在少样本文本分类的背景下得到检验。为了改进CrossTune的泛化能力，我们利用ChatGPT生成",
    "tldr": "CrossTune是一种带有标签增强的黑盒少样本分类网络，通过模拟输入文本序列与任务标签的语义相关性来提高泛化能力。",
    "en_tdlr": "CrossTune is a black-box few-shot classification network with label enhancement, improving generalization by modeling the semantic relatedness between input text sequences and task labels."
}