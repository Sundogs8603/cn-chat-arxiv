{
    "title": "Reinforcement Learning with Token-level Feedback for Controllable Text Generation",
    "abstract": "arXiv:2403.11558v1 Announce Type: cross  Abstract: To meet the requirements of real-world applications, it is essential to control generations of large language models (LLMs). Prior research has tried to introduce reinforcement learning (RL) into controllable text generation while most existing methods suffer from overfitting issues (finetuning-based methods) or semantic collapse (post-processing methods). However, current RL methods are generally guided by coarse-grained (sentence/paragraph-level) feedback, which may lead to suboptimal performance owing to semantic twists or progressions within sentences. To tackle that, we propose a novel reinforcement learning algorithm named TOLE which formulates TOken-LEvel rewards for controllable text generation, and employs a \"first-quantize-then-noise\" paradigm to enhance the robustness of the RL algorithm.Furthermore, TOLE can be flexibly extended to multiple constraints with little computational expense. Experimental results show that our al",
    "link": "https://arxiv.org/abs/2403.11558",
    "context": "Title: Reinforcement Learning with Token-level Feedback for Controllable Text Generation\nAbstract: arXiv:2403.11558v1 Announce Type: cross  Abstract: To meet the requirements of real-world applications, it is essential to control generations of large language models (LLMs). Prior research has tried to introduce reinforcement learning (RL) into controllable text generation while most existing methods suffer from overfitting issues (finetuning-based methods) or semantic collapse (post-processing methods). However, current RL methods are generally guided by coarse-grained (sentence/paragraph-level) feedback, which may lead to suboptimal performance owing to semantic twists or progressions within sentences. To tackle that, we propose a novel reinforcement learning algorithm named TOLE which formulates TOken-LEvel rewards for controllable text generation, and employs a \"first-quantize-then-noise\" paradigm to enhance the robustness of the RL algorithm.Furthermore, TOLE can be flexibly extended to multiple constraints with little computational expense. Experimental results show that our al",
    "path": "papers/24/03/2403.11558.json",
    "total_tokens": 921,
    "translated_title": "使用标记级反馈的强化学习用于可控文本生成",
    "translated_abstract": "为了满足现实世界应用的需求，控制大型语言模型（LLMs）的生成是至关重要的。先前的研究试图将强化学习（RL）引入可控文本生成，而大多数现有方法存在过拟合问题（微调基础方法）或语义崩溃（后处理方法）。然而，当前的RL方法通常是由粗粒度（句子/段落级别）的反馈引导的，这可能导致句子内语义扭曲或进展，从而导致性能次优。为了解决这个问题，我们提出了一种名为TOLE的新型强化学习算法，该算法制定了TOken-LEvel奖励用于可控文本生成，并采用“先量子化，然后加噪声”范式来增强强化学习算法的鲁棒性。此外，TOLE可以灵活地扩展到多个约束条件，计算成本很低。实验结果表明，我们的al",
    "tldr": "提出了一种新的强化学习算法TOLE，通过使用标记级别奖励进行文本生成，采用了\"先量子化，然后加噪声\"的方法来增强算法的鲁棒性。可以在多个约束条件下灵活扩展，避免了过拟合和语义崩溃问题。",
    "en_tdlr": "Proposed a new reinforcement learning algorithm TOLE for text generation with token-level rewards, employing a \"first-quantize-then-noise\" paradigm to enhance algorithm robustness. It can be flexibly extended to multiple constraints, avoiding overfitting and semantic collapse issues."
}