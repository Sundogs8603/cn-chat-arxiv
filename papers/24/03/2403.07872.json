{
    "title": "Rethinking Generative Large Language Model Evaluation for Semantic Comprehension",
    "abstract": "arXiv:2403.07872v1 Announce Type: new  Abstract: Despite their sophisticated capabilities, large language models (LLMs) encounter a major hurdle in effective assessment. This paper first revisits the prevalent evaluation method-multiple choice question answering (MCQA), which allows for straightforward accuracy measurement. Through a comprehensive evaluation of 24 models across 11 benchmarks, we highlight several potential drawbacks of MCQA, for instance, the inconsistency between the MCQA evaluation and the generation of open-ended responses in practical scenarios. In response, we introduce an RWQ-Elo rating system, engaging 24 LLMs such as GPT-4, GPT-3.5, Google-Gemini-Pro and LLaMA-1/-2, in a two-player competitive format, with GPT-4 serving as the judge. Each LLM receives an Elo rating thereafter. This system is designed to mirror real-world usage, and for this purpose, we have compiled a new benchmark called ``Real-world questions'' (RWQ), comprising 20,772 authentic user inquirie",
    "link": "https://arxiv.org/abs/2403.07872",
    "context": "Title: Rethinking Generative Large Language Model Evaluation for Semantic Comprehension\nAbstract: arXiv:2403.07872v1 Announce Type: new  Abstract: Despite their sophisticated capabilities, large language models (LLMs) encounter a major hurdle in effective assessment. This paper first revisits the prevalent evaluation method-multiple choice question answering (MCQA), which allows for straightforward accuracy measurement. Through a comprehensive evaluation of 24 models across 11 benchmarks, we highlight several potential drawbacks of MCQA, for instance, the inconsistency between the MCQA evaluation and the generation of open-ended responses in practical scenarios. In response, we introduce an RWQ-Elo rating system, engaging 24 LLMs such as GPT-4, GPT-3.5, Google-Gemini-Pro and LLaMA-1/-2, in a two-player competitive format, with GPT-4 serving as the judge. Each LLM receives an Elo rating thereafter. This system is designed to mirror real-world usage, and for this purpose, we have compiled a new benchmark called ``Real-world questions'' (RWQ), comprising 20,772 authentic user inquirie",
    "path": "papers/24/03/2403.07872.json",
    "total_tokens": 909,
    "translated_title": "对大型生成语言模型的语义理解重新评估",
    "translated_abstract": "尽管大型语言模型（LLMs）具有复杂的功能，但在有效评估方面仍面临重大障碍。本文首先重新审视主流的评估方法 - 多项选择问题回答（MCQA），这种方法允许进行直观的准确性评估。通过对11个基准测试上的24个模型进行全面评估，我们突出显示了MCQA的多个潜在缺点，例如MCQA评估与实际情景中开放式回答的生成之间的不一致性。为此，我们引入了一种RWQ-Elo评级系统，利用GPT-4、GPT-3.5、Google-Gemini-Pro和LLaMA-1/-2等24个LLMs，采用二人竞争格式，其中GPT-4作为裁判。每个LLM随后收到一个Elo评级。该系统旨在模拟真实世界的使用情况，出于此目的，我们编制了一个名为“真实世界问题”（RWQ）的新基准，包含20,772个真实用户查询。",
    "tldr": "通过引入RWQ-Elo评级系统，在新基准“真实世界问题”上对24种大型语言模型进行两人竞争评估，以解决多选问题回答方法在语义理解评估中的潜在缺陷。",
    "en_tdlr": "Introducing the RWQ-Elo rating system and a new benchmark \"Real-world questions,\" this paper addresses potential drawbacks of multiple choice question answering for semantic comprehension evaluation by competitively assessing 24 large language models."
}