{
    "title": "Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning",
    "abstract": "arXiv:2403.01112v1 Announce Type: new  Abstract: In cooperative multi-agent reinforcement learning (MARL), agents aim to achieve a common goal, such as defeating enemies or scoring a goal. Existing MARL algorithms are effective but still require significant learning time and often get trapped in local optima by complex tasks, subsequently failing to discover a goal-reaching policy. To address this, we introduce Efficient episodic Memory Utilization (EMU) for MARL, with two primary objectives: (a) accelerating reinforcement learning by leveraging semantically coherent memory from an episodic buffer and (b) selectively promoting desirable transitions to prevent local convergence. To achieve (a), EMU incorporates a trainable encoder/decoder structure alongside MARL, creating coherent memory embeddings that facilitate exploratory memory recall. To achieve (b), EMU introduces a novel reward structure called episodic incentive based on the desirability of states. This reward improves the TD ",
    "link": "https://arxiv.org/abs/2403.01112",
    "context": "Title: Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning\nAbstract: arXiv:2403.01112v1 Announce Type: new  Abstract: In cooperative multi-agent reinforcement learning (MARL), agents aim to achieve a common goal, such as defeating enemies or scoring a goal. Existing MARL algorithms are effective but still require significant learning time and often get trapped in local optima by complex tasks, subsequently failing to discover a goal-reaching policy. To address this, we introduce Efficient episodic Memory Utilization (EMU) for MARL, with two primary objectives: (a) accelerating reinforcement learning by leveraging semantically coherent memory from an episodic buffer and (b) selectively promoting desirable transitions to prevent local convergence. To achieve (a), EMU incorporates a trainable encoder/decoder structure alongside MARL, creating coherent memory embeddings that facilitate exploratory memory recall. To achieve (b), EMU introduces a novel reward structure called episodic incentive based on the desirability of states. This reward improves the TD ",
    "path": "papers/24/03/2403.01112.json",
    "total_tokens": 888,
    "translated_title": "高效的合作多智能体强化学习的情节记忆利用",
    "translated_abstract": "在合作多智能体强化学习（MARL）中，智能体旨在实现共同目标，比如击败敌人或进球。现有的MARL算法虽然有效，但仍需要大量学习时间，通常会在复杂任务中陷入局部最优解，随后未能发现达成目标的策略。为了解决这一问题，我们引入了用于MARL的高效情节记忆利用（EMU），其两个主要目标是：（a）通过利用来自情节缓冲区的语义一致内存加速强化学习，以及（b）有选择地促进理想的转换以防止局部收敛。为实现（a），EMU在MARL旁引入了可训练的编码器/解码器结构，创建了有助于探索性内存回忆的连贯记忆嵌入。为实现（b），EMU引入了一种基于状态愿望性的新颖奖励结构，称为情节激励。这种奖励改善了TD",
    "tldr": "提出了用于合作多智能体强化学习的高效情节记忆利用（EMU），利用语义一致内存加速学习，有选择地促进理想的转换，避免陷入局部最优解。",
    "en_tdlr": "Introduced Efficient episodic Memory Utilization (EMU) for cooperative multi-agent reinforcement learning, which accelerates learning by leveraging semantically coherent memory, selectively promotes desirable transitions, and prevents local optima."
}