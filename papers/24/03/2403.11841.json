{
    "title": "Pessimistic Causal Reinforcement Learning with Mediators for Confounded Offline Data",
    "abstract": "arXiv:2403.11841v1 Announce Type: cross  Abstract: In real-world scenarios, datasets collected from randomized experiments are often constrained by size, due to limitations in time and budget. As a result, leveraging large observational datasets becomes a more attractive option for achieving high-quality policy learning. However, most existing offline reinforcement learning (RL) methods depend on two key assumptions--unconfoundedness and positivity--which frequently do not hold in observational data contexts. Recognizing these challenges, we propose a novel policy learning algorithm, PESsimistic CAusal Learning (PESCAL). We utilize the mediator variable based on front-door criterion to remove the confounding bias; additionally, we adopt the pessimistic principle to address the distributional shift between the action distributions induced by candidate policies, and the behavior policy that generates the observational data. Our key observation is that, by incorporating auxiliary variable",
    "link": "https://arxiv.org/abs/2403.11841",
    "context": "Title: Pessimistic Causal Reinforcement Learning with Mediators for Confounded Offline Data\nAbstract: arXiv:2403.11841v1 Announce Type: cross  Abstract: In real-world scenarios, datasets collected from randomized experiments are often constrained by size, due to limitations in time and budget. As a result, leveraging large observational datasets becomes a more attractive option for achieving high-quality policy learning. However, most existing offline reinforcement learning (RL) methods depend on two key assumptions--unconfoundedness and positivity--which frequently do not hold in observational data contexts. Recognizing these challenges, we propose a novel policy learning algorithm, PESsimistic CAusal Learning (PESCAL). We utilize the mediator variable based on front-door criterion to remove the confounding bias; additionally, we adopt the pessimistic principle to address the distributional shift between the action distributions induced by candidate policies, and the behavior policy that generates the observational data. Our key observation is that, by incorporating auxiliary variable",
    "path": "papers/24/03/2403.11841.json",
    "total_tokens": 877,
    "translated_title": "基于中介因素的悲观因果强化学习用于混杂的离线数据",
    "translated_abstract": "在现实场景中，由随机实验收集的数据集往往受到时间和预算限制而规模有限。因此，利用大规模的观测数据集成为实现高质量策略学习更具吸引力的选择。然而，大多数现有的离线强化学习（RL）方法依赖于两个关键假设-- 非混杂性和正性-- 这两个假设在观测数据环境中经常不成立。鉴于这些挑战，我们提出了一种新颖的策略学习算法，称为悲观因果学习（PESCAL）。我们利用基于前门标准的中介变量来消除混杂偏差；此外，我们采用悲观原则来解决由候选策略引起的动作分布与生成观测数据的行为策略之间的分布变化。我们的关键观察是，通过融合辅助变量",
    "tldr": "提出了一种新的策略学习算法，PESCAL，利用基于前门标准的中介变量消除混杂偏差，并采用悲观原则处理候选策略引起的分布变化。",
    "en_tdlr": "Proposed a new policy learning algorithm, PESCAL, that utilizes a mediator variable based on the front-door criterion to remove confounding bias and adopts a pessimistic principle to address distributional shifts induced by candidate policies."
}