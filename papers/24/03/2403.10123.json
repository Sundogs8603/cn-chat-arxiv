{
    "title": "Regularization-Based Efficient Continual Learning in Deep State-Space Models",
    "abstract": "arXiv:2403.10123v1 Announce Type: new  Abstract: Deep state-space models (DSSMs) have gained popularity in recent years due to their potent modeling capacity for dynamic systems. However, existing DSSM works are limited to single-task modeling, which requires retraining with historical task data upon revisiting a forepassed task. To address this limitation, we propose continual learning DSSMs (CLDSSMs), which are capable of adapting to evolving tasks without catastrophic forgetting. Our proposed CLDSSMs integrate mainstream regularization-based continual learning (CL) methods, ensuring efficient updates with constant computational and memory costs for modeling multiple dynamic systems. We also conduct a comprehensive cost analysis of each CL method applied to the respective CLDSSMs, and demonstrate the efficacy of CLDSSMs through experiments on real-world datasets. The results corroborate that while various competing CL methods exhibit different merits, the proposed CLDSSMs consistentl",
    "link": "https://arxiv.org/abs/2403.10123",
    "context": "Title: Regularization-Based Efficient Continual Learning in Deep State-Space Models\nAbstract: arXiv:2403.10123v1 Announce Type: new  Abstract: Deep state-space models (DSSMs) have gained popularity in recent years due to their potent modeling capacity for dynamic systems. However, existing DSSM works are limited to single-task modeling, which requires retraining with historical task data upon revisiting a forepassed task. To address this limitation, we propose continual learning DSSMs (CLDSSMs), which are capable of adapting to evolving tasks without catastrophic forgetting. Our proposed CLDSSMs integrate mainstream regularization-based continual learning (CL) methods, ensuring efficient updates with constant computational and memory costs for modeling multiple dynamic systems. We also conduct a comprehensive cost analysis of each CL method applied to the respective CLDSSMs, and demonstrate the efficacy of CLDSSMs through experiments on real-world datasets. The results corroborate that while various competing CL methods exhibit different merits, the proposed CLDSSMs consistentl",
    "path": "papers/24/03/2403.10123.json",
    "total_tokens": 864,
    "translated_title": "正则化驱动的深度状态空间模型中的高效持续学习",
    "translated_abstract": "最近几年，由于其对动态系统具有强大的建模能力，深度状态空间模型（DSSMs）已经变得越来越受欢迎。然而，现有的DSSM工作局限于单任务建模，这需要在重新访问之前的任务时利用历史任务数据进行重新训练。为了解决这一局限性，我们提出了持续学习DSSMs（CLDSSMs），能够适应不断变化的任务而不会发生灾难性遗忘。我们提出的CLDSSMs集成了主流基于正则化的持续学习（CL）方法，确保在对多个动态系统建模时高效更新，保持不变的计算和内存成本。我们还对应用于各自CLDSSMs的每种CL方法进行了全面的成本分析，并通过对真实世界数据集的实验来展示CLDSSMs的有效性。结果证实，虽然各种竞争的CL方法具有不同的优点，但所提出的CLDSSMs始终保持一致。",
    "tldr": "提出了一种正则化驱动的深度状态空间模型，实现了高效的持续学习，能够在多个动态系统建模时进行有效更新，并且通过实验证实了其有效性",
    "en_tdlr": "Proposed a regularization-based deep state-space model for efficient continual learning, allowing for efficient updates in modeling multiple dynamic systems, with demonstrated efficacy through experiments."
}