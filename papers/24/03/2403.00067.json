{
    "title": "Query-OPT: Optimizing Inference of Large Language Models via Multi-Query Instructions in Meeting Summarization",
    "abstract": "arXiv:2403.00067v1 Announce Type: new  Abstract: This work focuses on the task of query-based meeting summarization in which the summary of a context (meeting transcript) is generated in response to a specific query. When using Large Language Models (LLMs) for this task, a new call to the LLM inference endpoint/API is required for each new query even if the context stays the same. However, repeated calls to the LLM inference endpoints would significantly increase the costs of using them in production, making LLMs impractical for many real-world use cases. To address this problem, in this paper, we investigate whether combining the queries for the same input context in a single prompt to minimize repeated calls can be successfully used in meeting summarization. In this regard, we conduct extensive experiments by comparing the performance of various popular LLMs: GPT-4, PaLM-2, LLaMA-2, Mistral, and FLAN-T5 in single-query and multi-query settings. We observe that while most LLMs tend to",
    "link": "https://arxiv.org/abs/2403.00067",
    "context": "Title: Query-OPT: Optimizing Inference of Large Language Models via Multi-Query Instructions in Meeting Summarization\nAbstract: arXiv:2403.00067v1 Announce Type: new  Abstract: This work focuses on the task of query-based meeting summarization in which the summary of a context (meeting transcript) is generated in response to a specific query. When using Large Language Models (LLMs) for this task, a new call to the LLM inference endpoint/API is required for each new query even if the context stays the same. However, repeated calls to the LLM inference endpoints would significantly increase the costs of using them in production, making LLMs impractical for many real-world use cases. To address this problem, in this paper, we investigate whether combining the queries for the same input context in a single prompt to minimize repeated calls can be successfully used in meeting summarization. In this regard, we conduct extensive experiments by comparing the performance of various popular LLMs: GPT-4, PaLM-2, LLaMA-2, Mistral, and FLAN-T5 in single-query and multi-query settings. We observe that while most LLMs tend to",
    "path": "papers/24/03/2403.00067.json",
    "total_tokens": 852,
    "translated_title": "Query-OPT：通过多查询指令优化大型语言模型在会议摘要中的推理",
    "translated_abstract": "这项工作关注基于查询的会议摘要任务，在此任务中，针对特定查询对上下文（会议记录）生成摘要。使用大型语言模型（LLMs）进行此任务时，即使上下文保持不变，每个新查询也需要对LLM推理端点/API进行一次新调用。然而，反复调用LLM推理端点会显著增加在生产中使用它们的成本，这使得许多实际用例中LLMs都不切实际。为解决这一问题，在本文中，我们研究了是否可以成功地将相同输入上下文的查询组合为单个提示以最小化重复调用，在会议摘要中使用。在这方面，我们通过比较各种流行的LLM（GPT-4、PaLM-2、LLaMA-2、Mistral和FLAN-T5）在单查询和多查询设置中的表现进行了广泛实验。",
    "tldr": "本研究旨在通过将相同输入上下文的查询组合为单个提示，以最小化重复调用来优化使用大型语言模型在会议摘要中的推理。"
}