{
    "title": "A Single Linear Layer Yields Task-Adapted Low-Rank Matrices",
    "abstract": "arXiv:2403.14946v1 Announce Type: cross  Abstract: Low-Rank Adaptation (LoRA) is a widely used Parameter-Efficient Fine-Tuning (PEFT) method that updates an initial weight matrix $W_0$ with a delta matrix $\\Delta W$ consisted by two low-rank matrices $A$ and $B$. A previous study suggested that there is correlation between $W_0$ and $\\Delta W$. In this study, we aim to delve deeper into relationships between $W_0$ and low-rank matrices $A$ and $B$ to further comprehend the behavior of LoRA. In particular, we analyze a conversion matrix that transform $W_0$ into low-rank matrices, which encapsulates information about the relationships. Our analysis reveals that the conversion matrices are similar across each layer. Inspired by these findings, we hypothesize that a single linear layer, which takes each layer's $W_0$ as input, can yield task-adapted low-rank matrices. To confirm this hypothesis, we devise a method named Conditionally Parameterized LoRA (CondLoRA) that updates initial weig",
    "link": "https://arxiv.org/abs/2403.14946",
    "context": "Title: A Single Linear Layer Yields Task-Adapted Low-Rank Matrices\nAbstract: arXiv:2403.14946v1 Announce Type: cross  Abstract: Low-Rank Adaptation (LoRA) is a widely used Parameter-Efficient Fine-Tuning (PEFT) method that updates an initial weight matrix $W_0$ with a delta matrix $\\Delta W$ consisted by two low-rank matrices $A$ and $B$. A previous study suggested that there is correlation between $W_0$ and $\\Delta W$. In this study, we aim to delve deeper into relationships between $W_0$ and low-rank matrices $A$ and $B$ to further comprehend the behavior of LoRA. In particular, we analyze a conversion matrix that transform $W_0$ into low-rank matrices, which encapsulates information about the relationships. Our analysis reveals that the conversion matrices are similar across each layer. Inspired by these findings, we hypothesize that a single linear layer, which takes each layer's $W_0$ as input, can yield task-adapted low-rank matrices. To confirm this hypothesis, we devise a method named Conditionally Parameterized LoRA (CondLoRA) that updates initial weig",
    "path": "papers/24/03/2403.14946.json",
    "total_tokens": 951,
    "translated_title": "一个线性层生成任务自适应低秩矩阵",
    "translated_abstract": "低秩适应（LoRA）是一种广泛使用的参数高效调整（PEFT）方法，它通过由两个低秩矩阵$ A $和$ B $组成的增量矩阵$ \\Delta W $更新初始权重矩阵$ W_0 $。先前的研究表明$ W_0 $和$ \\Delta W $之间存在关联。在这项研究中，我们旨在深入探讨$ W_0 $与低秩矩阵$ A $和$ B $之间的关系，以进一步理解LoRA的行为。特别地，我们分析了一个将$ W_0 $转换为低秩矩阵的转换矩阵，其中蕴含了关系的信息。我们的分析表明转换矩阵在每一层之间是相似的。受到这些发现的启发，我们假设一个单一线性层，将每一层的$ W_0 $作为输入，可以生成任务自适应的低秩矩阵。为了验证这一假设，我们设计了一种名为有条件参数化的LoRA (CondLoRA) 方法，来更新初始权重...",
    "tldr": "通过研究转换矩阵将$ W_0 $转换为低秩矩阵的关系信息，我们提出单一线性层可以生成任务自适应的低秩矩阵。"
}