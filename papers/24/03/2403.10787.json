{
    "title": "Time Series Representation Learning with Supervised Contrastive Temporal Transformer",
    "abstract": "arXiv:2403.10787v1 Announce Type: cross  Abstract: Finding effective representations for time series data is a useful but challenging task. Several works utilize self-supervised or unsupervised learning methods to address this. However, there still remains the open question of how to leverage available label information for better representations. To answer this question, we exploit pre-existing techniques in time series and representation learning domains and develop a simple, yet novel fusion model, called: \\textbf{S}upervised \\textbf{CO}ntrastive \\textbf{T}emporal \\textbf{T}ransformer (SCOTT). We first investigate suitable augmentation methods for various types of time series data to assist with learning change-invariant representations. Secondly, we combine Transformer and Temporal Convolutional Networks in a simple way to efficiently learn both global and local features. Finally, we simplify Supervised Contrastive Loss for representation learning of labelled time series data. We p",
    "link": "https://arxiv.org/abs/2403.10787",
    "context": "Title: Time Series Representation Learning with Supervised Contrastive Temporal Transformer\nAbstract: arXiv:2403.10787v1 Announce Type: cross  Abstract: Finding effective representations for time series data is a useful but challenging task. Several works utilize self-supervised or unsupervised learning methods to address this. However, there still remains the open question of how to leverage available label information for better representations. To answer this question, we exploit pre-existing techniques in time series and representation learning domains and develop a simple, yet novel fusion model, called: \\textbf{S}upervised \\textbf{CO}ntrastive \\textbf{T}emporal \\textbf{T}ransformer (SCOTT). We first investigate suitable augmentation methods for various types of time series data to assist with learning change-invariant representations. Secondly, we combine Transformer and Temporal Convolutional Networks in a simple way to efficiently learn both global and local features. Finally, we simplify Supervised Contrastive Loss for representation learning of labelled time series data. We p",
    "path": "papers/24/03/2403.10787.json",
    "total_tokens": 874,
    "translated_title": "具有监督对比时间变换器的时间序列表示学习",
    "translated_abstract": "找到时间序列数据的有效表示是一项有用但具有挑战性的任务。有些工作利用自监督或无监督学习方法来解决这个问题。然而，如何利用可用的标签信息来获得更好的表示仍然是一个悬而未决的问题。为了回答这个问题，我们利用时间序列和表示学习领域中的现有技术，开发了一个简单但新颖的融合模型，称为：\\textbf{S}upervised \\textbf{CO}ntrastive \\textbf{T}emporal \\textbf{T}ransformer (SCOTT)。我们首先研究了适用于各种类型时间序列数据的合适增强方法，以帮助学习具有变化不变性的表示。其次，我们以简单的方式结合了Transformer和Temporal Convolutional Networks，以有效地学习全局和局部特征。最后，我们简化了用于标记时间序列数据表示学习的监督对比损失。",
    "tldr": "提出了一种名为SCOTT的具有监督对比变换器的时间序列表示学习模型，结合了Transformer和Temporal Convolutional Networks以学习全局和局部特征，并简化了用于标记时间序列数据的监督对比损失。"
}