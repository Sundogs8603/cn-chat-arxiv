{
    "title": "Explaining Pre-Trained Language Models with Attribution Scores: An Analysis in Low-Resource Settings",
    "abstract": "arXiv:2403.05338v1 Announce Type: new  Abstract: Attribution scores indicate the importance of different input parts and can, thus, explain model behaviour. Currently, prompt-based models are gaining popularity, i.a., due to their easier adaptability in low-resource settings. However, the quality of attribution scores extracted from prompt-based models has not been investigated yet. In this work, we address this topic by analyzing attribution scores extracted from prompt-based models w.r.t. plausibility and faithfulness and comparing them with attribution scores extracted from fine-tuned models and large language models. In contrast to previous work, we introduce training size as another dimension into the analysis. We find that using the prompting paradigm (with either encoder-based or decoder-based models) yields more plausible explanations than fine-tuning the models in low-resource settings and Shapley Value Sampling consistently outperforms attention and Integrated Gradients in te",
    "link": "https://arxiv.org/abs/2403.05338",
    "context": "Title: Explaining Pre-Trained Language Models with Attribution Scores: An Analysis in Low-Resource Settings\nAbstract: arXiv:2403.05338v1 Announce Type: new  Abstract: Attribution scores indicate the importance of different input parts and can, thus, explain model behaviour. Currently, prompt-based models are gaining popularity, i.a., due to their easier adaptability in low-resource settings. However, the quality of attribution scores extracted from prompt-based models has not been investigated yet. In this work, we address this topic by analyzing attribution scores extracted from prompt-based models w.r.t. plausibility and faithfulness and comparing them with attribution scores extracted from fine-tuned models and large language models. In contrast to previous work, we introduce training size as another dimension into the analysis. We find that using the prompting paradigm (with either encoder-based or decoder-based models) yields more plausible explanations than fine-tuning the models in low-resource settings and Shapley Value Sampling consistently outperforms attention and Integrated Gradients in te",
    "path": "papers/24/03/2403.05338.json",
    "total_tokens": 876,
    "translated_title": "用归因分数解释预训练语言模型：在低资源环境中的分析",
    "translated_abstract": "归因分数指示不同输入部分的重要性，因此可以解释模型行为。目前，基于提示的模型正变得越来越受欢迎，部分原因是它们在低资源环境中更容易适应。然而，从基于提示的模型中提取的归因分数的质量尚未得到研究。在这项工作中，我们通过分析从基于提示的模型中提取的归因分数关于可信度和忠实度，并将它们与从微调模型和大型语言模型中提取的归因分数进行比较来解决这个问题。与以前的工作相反，我们将训练规模作为分析的另一个维度引入。我们发现在低资源环境中使用提示范式（无论是基于编码器还是基于解码器的模型）比微调模型产生更合理的解释，Shapley值采样在可解释性和一致性方面均优于注意力和集成梯度。",
    "tldr": "本研究分析在低资源环境中从预备训练模型中提取的归因分数，并发现使用提示范式产生的解释比微调模型更合理，Shapley值采样表现出色。",
    "en_tdlr": "This study analyzes attribution scores extracted from pre-trained models in low-resource settings, finding that explanations generated using the prompting paradigm are more plausible than fine-tuned models, with Shapley Value Sampling performing well."
}