{
    "title": "FedSPU: Personalized Federated Learning for Resource-constrained Devices with Stochastic Parameter Update",
    "abstract": "arXiv:2403.11464v1 Announce Type: new  Abstract: Personalized Federated Learning (PFL) is widely employed in IoT applications to handle high-volume, non-iid client data while ensuring data privacy. However, heterogeneous edge devices owned by clients may impose varying degrees of resource constraints, causing computation and communication bottlenecks for PFL. Federated Dropout has emerged as a popular strategy to address this challenge, wherein only a subset of the global model, i.e. a \\textit{sub-model}, is trained on a client's device, thereby reducing computation and communication overheads. Nevertheless, the dropout-based model-pruning strategy may introduce bias, particularly towards non-iid local data. When biased sub-models absorb highly divergent parameters from other clients, performance degradation becomes inevitable. In response, we propose federated learning with stochastic parameter update (FedSPU). Unlike dropout that tailors the global model to small-size local sub-model",
    "link": "https://arxiv.org/abs/2403.11464",
    "context": "Title: FedSPU: Personalized Federated Learning for Resource-constrained Devices with Stochastic Parameter Update\nAbstract: arXiv:2403.11464v1 Announce Type: new  Abstract: Personalized Federated Learning (PFL) is widely employed in IoT applications to handle high-volume, non-iid client data while ensuring data privacy. However, heterogeneous edge devices owned by clients may impose varying degrees of resource constraints, causing computation and communication bottlenecks for PFL. Federated Dropout has emerged as a popular strategy to address this challenge, wherein only a subset of the global model, i.e. a \\textit{sub-model}, is trained on a client's device, thereby reducing computation and communication overheads. Nevertheless, the dropout-based model-pruning strategy may introduce bias, particularly towards non-iid local data. When biased sub-models absorb highly divergent parameters from other clients, performance degradation becomes inevitable. In response, we propose federated learning with stochastic parameter update (FedSPU). Unlike dropout that tailors the global model to small-size local sub-model",
    "path": "papers/24/03/2403.11464.json",
    "total_tokens": 887,
    "translated_title": "FedSPU：具有随机参数更新的资源受限设备个性化联邦学习",
    "translated_abstract": "个性化的联邦学习（PFL）被广泛应用于物联网应用中，用于处理大量的非iid客户端数据，同时确保数据隐私。然而，客户拥有的异构边缘设备可能施加不同程度的资源约束，给PFL造成计算和通信瓶颈。联邦Dropout已成为应对这一挑战的一种流行策略，其中仅在客户端设备上训练全局模型的一个子模型，从而降低计算和通信开销。然而，基于Dropout的模型修剪策略可能引入偏差，特别是对非iid本地数据。当有偏见的子模型吸收来自其他客户端的高度分散参数时，性能下降是不可避免的。为了应对这一情况，我们提出了具有随机参数更新的联邦学习（FedSPU）。与专门为小型本地子模型定制全局模型的Dropout不同，FedSPU引入了个性化的随机参数更新机制，以在保持数据隐私的同时降低计算和通信开销。",
    "tldr": "提出了一种具有随机参数更新机制的个性化联邦学习方法，以解决资源受限设备在非iid数据场景下的性能下降问题。",
    "en_tdlr": "Proposed a personalized federated learning method with stochastic parameter update to address performance degradation on resource-constrained devices in non-iid data scenarios."
}