{
    "title": "Training morphological neural networks with gradient descent: some theoretical insights",
    "abstract": "arXiv:2403.12975v1 Announce Type: cross  Abstract: Morphological neural networks, or layers, can be a powerful tool to boost the progress in mathematical morphology, either on theoretical aspects such as the representation of complete lattice operators, or in the development of image processing pipelines. However, these architectures turn out to be difficult to train when they count more than a few morphological layers, at least within popular machine learning frameworks which use gradient descent based optimization algorithms. In this paper we investigate the potential and limitations of differentiation based approaches and back-propagation applied to morphological networks, in light of the non-smooth optimization concept of Bouligand derivative. We provide insights and first theoretical guidelines, in particular regarding initialization and learning rates.",
    "link": "https://arxiv.org/abs/2403.12975",
    "context": "Title: Training morphological neural networks with gradient descent: some theoretical insights\nAbstract: arXiv:2403.12975v1 Announce Type: cross  Abstract: Morphological neural networks, or layers, can be a powerful tool to boost the progress in mathematical morphology, either on theoretical aspects such as the representation of complete lattice operators, or in the development of image processing pipelines. However, these architectures turn out to be difficult to train when they count more than a few morphological layers, at least within popular machine learning frameworks which use gradient descent based optimization algorithms. In this paper we investigate the potential and limitations of differentiation based approaches and back-propagation applied to morphological networks, in light of the non-smooth optimization concept of Bouligand derivative. We provide insights and first theoretical guidelines, in particular regarding initialization and learning rates.",
    "path": "papers/24/03/2403.12975.json",
    "total_tokens": 767,
    "translated_title": "用梯度下降训练形态神经网络：一些理论见解",
    "translated_abstract": "形态神经网络或层可以成为提升数学形态学进展的强大工具，无论是在理论方面，如完整格算子的表示，还是在图像处理流程的开发方面。然而，当这些架构包含多层形态学时，至少在使用基于梯度下降的优化算法的流行机器学习框架内，这些网络很难进行训练。在本文中，我们探讨了基于微分方法和反向传播应用于形态网络的潜力和局限性，考虑到Bouligand导数的非光滑优化概念。我们提供了见解和首个理论指南，特别是关于初始化和学习率。",
    "tldr": "形态神经网络的训练存在挑战，本文通过使用基于梯度下降的优化算法，探讨了基于微分方法和反向传播对形态网络的潜力和局限性，提供了关于初始化和学习率的理论指导。",
    "en_tdlr": "Training morphological neural networks poses challenges, and this paper investigates the potential and limitations of differentiation based approaches and back-propagation in light of the non-smooth optimization concept of Bouligand derivative, providing theoretical guidelines on initialization and learning rates."
}