{
    "title": "Auto-Train-Once: Controller Network Guided Automatic Network Pruning from Scratch",
    "abstract": "arXiv:2403.14729v1 Announce Type: cross  Abstract: Current techniques for deep neural network (DNN) pruning often involve intricate multi-step processes that require domain-specific expertise, making their widespread adoption challenging. To address the limitation, the Only-Train-Once (OTO) and OTOv2 are proposed to eliminate the need for additional fine-tuning steps by directly training and compressing a general DNN from scratch. Nevertheless, the static design of optimizers (in OTO) can lead to convergence issues of local optima. In this paper, we proposed the Auto-Train-Once (ATO), an innovative network pruning algorithm designed to automatically reduce the computational and storage costs of DNNs. During the model training phase, our approach not only trains the target model but also leverages a controller network as an architecture generator to guide the learning of target model weights. Furthermore, we developed a novel stochastic gradient algorithm that enhances the coordination ",
    "link": "https://arxiv.org/abs/2403.14729",
    "context": "Title: Auto-Train-Once: Controller Network Guided Automatic Network Pruning from Scratch\nAbstract: arXiv:2403.14729v1 Announce Type: cross  Abstract: Current techniques for deep neural network (DNN) pruning often involve intricate multi-step processes that require domain-specific expertise, making their widespread adoption challenging. To address the limitation, the Only-Train-Once (OTO) and OTOv2 are proposed to eliminate the need for additional fine-tuning steps by directly training and compressing a general DNN from scratch. Nevertheless, the static design of optimizers (in OTO) can lead to convergence issues of local optima. In this paper, we proposed the Auto-Train-Once (ATO), an innovative network pruning algorithm designed to automatically reduce the computational and storage costs of DNNs. During the model training phase, our approach not only trains the target model but also leverages a controller network as an architecture generator to guide the learning of target model weights. Furthermore, we developed a novel stochastic gradient algorithm that enhances the coordination ",
    "path": "papers/24/03/2403.14729.json",
    "total_tokens": 868,
    "translated_title": "Auto-Train-Once：从零开始指导的控制器网络自动网络剪枝",
    "translated_abstract": "现有的深度神经网络（DNN）剪枝技术通常涉及复杂的多步骤过程，需要领域专业知识，使其普遍采用具有挑战性。为了解决这一限制，提出了一次训练（OTO）和OTOv2，它们通过直接训练和压缩通用DNN来消除额外的微调步骤。然而，优化器的静态设计（在OTO中）可能导致局部最优解的收敛问题。本文提出了Auto-Train-Once（ATO），一种创新的网络剪枝算法，旨在自动减少DNN的计算和存储成本。在模型训练阶段，我们的方法不仅训练目标模型，还利用控制器网络作为架构生成器来引导目标模型权重的学习。此外，我们还开发了一种新颖的随机梯度算法，以增强协调性。",
    "tldr": "提出了Auto-Train-Once（ATO）网络剪枝算法，利用控制器网络自动降低DNN的计算和存储成本，并通过新颖的随机梯度算法增强协调性。"
}