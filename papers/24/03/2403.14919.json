{
    "title": "Hierarchical Skip Decoding for Efficient Autoregressive Text Generation",
    "abstract": "arXiv:2403.14919v1 Announce Type: cross  Abstract: Autoregressive decoding strategy is a commonly used method for text generation tasks with pre-trained language models, while early-exiting is an effective approach to speedup the inference stage. In this work, we propose a novel decoding strategy named Hierarchical Skip Decoding (HSD) for efficient autoregressive text generation. Different from existing methods that require additional trainable components, HSD is a plug-and-play method applicable to autoregressive text generation models, it adaptively skips decoding layers in a hierarchical manner based on the current sequence length, thereby reducing computational workload and allocating computation resources. Comprehensive experiments on five text generation datasets with pre-trained language models demonstrate HSD's advantages in balancing efficiency and text quality. With almost half of the layers skipped, HSD can sustain 90% of the text quality compared to vanilla autoregressive d",
    "link": "https://arxiv.org/abs/2403.14919",
    "context": "Title: Hierarchical Skip Decoding for Efficient Autoregressive Text Generation\nAbstract: arXiv:2403.14919v1 Announce Type: cross  Abstract: Autoregressive decoding strategy is a commonly used method for text generation tasks with pre-trained language models, while early-exiting is an effective approach to speedup the inference stage. In this work, we propose a novel decoding strategy named Hierarchical Skip Decoding (HSD) for efficient autoregressive text generation. Different from existing methods that require additional trainable components, HSD is a plug-and-play method applicable to autoregressive text generation models, it adaptively skips decoding layers in a hierarchical manner based on the current sequence length, thereby reducing computational workload and allocating computation resources. Comprehensive experiments on five text generation datasets with pre-trained language models demonstrate HSD's advantages in balancing efficiency and text quality. With almost half of the layers skipped, HSD can sustain 90% of the text quality compared to vanilla autoregressive d",
    "path": "papers/24/03/2403.14919.json",
    "total_tokens": 851,
    "translated_title": "用于高效自回归文本生成的分层跳跃解码",
    "translated_abstract": "自回归解码策略是一种常用的文本生成任务方法，适用于预训练语言模型，而提前结束是一种有效的加速推断阶段的方法。在本研究中，我们提出了一种名为Hierarchical Skip Decoding（HSD）的新型解码策略，用于高效的自回归文本生成。与需要额外可训练组件的现有方法不同，HSD是一种即插即用的方法，适用于自回归文本生成模型，它根据当前序列长度以分层的方式自适应地跳过解码层，从而减少计算负载并分配计算资源。在五个带有预先训练语言模型的文本生成数据集上进行的全面实验显示，HSD在平衡效率和文本质量方面具有优势。几乎跳过一半的层，HSD可以与原始自回归d模型相比保持90%的文本质量。",
    "tldr": "提出了一种名为Hierarchical Skip Decoding（HSD）的新型解码策略，用于高效的自回归文本生成，通过分层地自适应跳过解码层来减少计算负载和分配计算资源。",
    "en_tdlr": "Introduced a novel decoding strategy named Hierarchical Skip Decoding (HSD) for efficient autoregressive text generation, reducing computational workload and allocating computation resources by adaptively skipping decoding layers in a hierarchical manner."
}