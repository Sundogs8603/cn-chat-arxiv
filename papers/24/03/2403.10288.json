{
    "title": "Rough Transformers for Continuous and Efficient Time-Series Modelling",
    "abstract": "arXiv:2403.10288v1 Announce Type: cross  Abstract: Time-series data in real-world medical settings typically exhibit long-range dependencies and are observed at non-uniform intervals. In such contexts, traditional sequence-based recurrent models struggle. To overcome this, researchers replace recurrent architectures with Neural ODE-based models to model irregularly sampled data and use Transformer-based architectures to account for long-range dependencies. Despite the success of these two approaches, both incur very high computational costs for input sequences of moderate lengths and greater. To mitigate this, we introduce the Rough Transformer, a variation of the Transformer model which operates on continuous-time representations of input sequences and incurs significantly reduced computational costs, critical for addressing long-range dependencies common in medical contexts. In particular, we propose multi-view signature attention, which uses path signatures to augment vanilla attent",
    "link": "https://arxiv.org/abs/2403.10288",
    "context": "Title: Rough Transformers for Continuous and Efficient Time-Series Modelling\nAbstract: arXiv:2403.10288v1 Announce Type: cross  Abstract: Time-series data in real-world medical settings typically exhibit long-range dependencies and are observed at non-uniform intervals. In such contexts, traditional sequence-based recurrent models struggle. To overcome this, researchers replace recurrent architectures with Neural ODE-based models to model irregularly sampled data and use Transformer-based architectures to account for long-range dependencies. Despite the success of these two approaches, both incur very high computational costs for input sequences of moderate lengths and greater. To mitigate this, we introduce the Rough Transformer, a variation of the Transformer model which operates on continuous-time representations of input sequences and incurs significantly reduced computational costs, critical for addressing long-range dependencies common in medical contexts. In particular, we propose multi-view signature attention, which uses path signatures to augment vanilla attent",
    "path": "papers/24/03/2403.10288.json",
    "total_tokens": 821,
    "translated_title": "用于连续和高效时间序列建模的粗糙Transformer",
    "translated_abstract": "在真实世界的医疗环境中，时间序列数据通常表现出长程依赖性，并且以不均匀间隔观察到。在这种情况下，传统的基于序列的循环模型很难处理。为了克服这一问题，研究人员用基于神经ODE的模型替换循环架构来建模非均匀采样的数据，并使用Transformer架构来考虑长程依赖。尽管这两种方法取得了成功，但对于中等长度及更长输入序列，两者都需要非常高的计算成本。为了缓解这一问题，我们引入了粗糙Transformer，这是Transformer模型的一种变体，其在输入序列的连续时间表示上运行，并且减少了计算成本，对于处理医疗情境中常见的长程依赖性至关重要。特别地，我们提出了多视图签名注意力，利用路径签名来增强传统的注意力。",
    "tldr": "提出了粗糙Transformer，用于在连续时间表示的输入序列上进行操作，大大降低了计算成本，对于处理医疗情境中的长程依赖性至关重要。"
}