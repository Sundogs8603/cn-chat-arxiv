{
    "title": "Can't Remember Details in Long Documents? You Need Some R&R",
    "abstract": "arXiv:2403.05004v1 Announce Type: cross  Abstract: Long-context large language models (LLMs) hold promise for tasks such as question-answering (QA) over long documents, but they tend to miss important information in the middle of context documents (arXiv:2307.03172v3). Here, we introduce $\\textit{R&R}$ -- a combination of two novel prompt-based methods called $\\textit{reprompting}$ and $\\textit{in-context retrieval}$ (ICR) -- to alleviate this effect in document-based QA. In reprompting, we repeat the prompt instructions periodically throughout the context document to remind the LLM of its original task. In ICR, rather than instructing the LLM to answer the question directly, we instruct it to retrieve the top $k$ passage numbers most relevant to the given question, which are then used as an abbreviated context in a second QA prompt. We test R&R with GPT-4 Turbo and Claude-2.1 on documents up to 80k tokens in length and observe a 16-point boost in QA accuracy on average. Our further an",
    "link": "https://arxiv.org/abs/2403.05004",
    "context": "Title: Can't Remember Details in Long Documents? You Need Some R&R\nAbstract: arXiv:2403.05004v1 Announce Type: cross  Abstract: Long-context large language models (LLMs) hold promise for tasks such as question-answering (QA) over long documents, but they tend to miss important information in the middle of context documents (arXiv:2307.03172v3). Here, we introduce $\\textit{R&R}$ -- a combination of two novel prompt-based methods called $\\textit{reprompting}$ and $\\textit{in-context retrieval}$ (ICR) -- to alleviate this effect in document-based QA. In reprompting, we repeat the prompt instructions periodically throughout the context document to remind the LLM of its original task. In ICR, rather than instructing the LLM to answer the question directly, we instruct it to retrieve the top $k$ passage numbers most relevant to the given question, which are then used as an abbreviated context in a second QA prompt. We test R&R with GPT-4 Turbo and Claude-2.1 on documents up to 80k tokens in length and observe a 16-point boost in QA accuracy on average. Our further an",
    "path": "papers/24/03/2403.05004.json",
    "total_tokens": 895,
    "translated_title": "无法记住长文档中的细节？您需要一些R&R",
    "translated_abstract": "长上下文大型语言模型（LLMs）在诸如长篇文档上的问答（QA）等任务中表现出潜力，但它们往往会错过上下文文档中间的重要信息。在这里，我们介绍了一个名为$\\textit{R&R}$的方法，它结合了两种新型基于提示的方法，称为$\\textit{reprompting}$和$\\textit{in-context retrieval}$（ICR），以减轻文档型QA中的这种影响。在$\\textit{reprompting}$中，我们周期性地在整个上下文文档中重复提示说明，以提醒LLM其原始任务。在ICR中，我们并不指示LLM直接回答问题，而是指示它检索与给定问题最相关的前$k$个段落编号，然后将其用作第二个QA提示中的缩略上下文。我们使用GPT-4 Turbo和Claude-2.1在长度达到80k标记的文档上测试了R&R，并平均观察到QA准确率提升了16个百分点。",
    "tldr": "引入R&R方法，结合reprompting和in-context retrieval两种新型提示方式，提高了在长文档上的问答任务的准确性。",
    "en_tdlr": "Introducing R&R method, combining novel prompt-based methods of reprompting and in-context retrieval, to enhance accuracy in question-answering tasks over long documents."
}