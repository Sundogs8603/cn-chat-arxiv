{
    "title": "Mind the GAP: Improving Robustness to Subpopulation Shifts with Group-Aware Priors",
    "abstract": "arXiv:2403.09869v1 Announce Type: cross  Abstract: Machine learning models often perform poorly under subpopulation shifts in the data distribution. Developing methods that allow machine learning models to better generalize to such shifts is crucial for safe deployment in real-world settings. In this paper, we develop a family of group-aware prior (GAP) distributions over neural network parameters that explicitly favor models that generalize well under subpopulation shifts. We design a simple group-aware prior that only requires access to a small set of data with group information and demonstrate that training with this prior yields state-of-the-art performance -- even when only retraining the final layer of a previously trained non-robust model. Group aware-priors are conceptually simple, complementary to existing approaches, such as attribute pseudo labeling and data reweighting, and open up promising new avenues for harnessing Bayesian inference to enable robustness to subpopulation",
    "link": "https://arxiv.org/abs/2403.09869",
    "context": "Title: Mind the GAP: Improving Robustness to Subpopulation Shifts with Group-Aware Priors\nAbstract: arXiv:2403.09869v1 Announce Type: cross  Abstract: Machine learning models often perform poorly under subpopulation shifts in the data distribution. Developing methods that allow machine learning models to better generalize to such shifts is crucial for safe deployment in real-world settings. In this paper, we develop a family of group-aware prior (GAP) distributions over neural network parameters that explicitly favor models that generalize well under subpopulation shifts. We design a simple group-aware prior that only requires access to a small set of data with group information and demonstrate that training with this prior yields state-of-the-art performance -- even when only retraining the final layer of a previously trained non-robust model. Group aware-priors are conceptually simple, complementary to existing approaches, such as attribute pseudo labeling and data reweighting, and open up promising new avenues for harnessing Bayesian inference to enable robustness to subpopulation",
    "path": "papers/24/03/2403.09869.json",
    "total_tokens": 908,
    "translated_title": "对亚群体偏移的鲁棒性改进：使用组感知先验",
    "translated_abstract": "机器学习模型在数据分布的亚群体偏移下往往表现不佳。开发能够让机器学习模型更好地泛化到这种偏移的方法对于在现实世界中安全部署至关重要。在这篇论文中，我们提出了一族针对神经网络参数的组感知先验（GAP）分布，明确支持在数据分布的亚群体偏移下泛化良好的模型。我们设计了一个简单的组感知先验，只需要访问一小部分包含组信息的数据，证明了在此先验下训练会获得最先进的性能——即使只重新训练先前训练的非鲁棒模型的最后一层。组感知先验在概念上简单，与现有方法（如属性伪标记和数据重新加权）互补，为利用贝叶斯推断以实现对亚群体偏移的鲁棒性开辟了有前景的新途径。",
    "tldr": "开发了一族组感知先验分布，可以改进神经网络模型在数据分布的亚群体偏移下的泛化能力，并展示了即使只重新训练非鲁棒模型的最后一层，使用这种先验进行训练也能获得最先进的性能。",
    "en_tdlr": "Developed a family of group-aware prior distributions to improve the neural network model's generalization ability under subpopulation shifts in the data distribution, demonstrating state-of-the-art performance even when retraining only the final layer of a previously trained non-robust model."
}