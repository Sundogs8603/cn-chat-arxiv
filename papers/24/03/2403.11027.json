{
    "title": "Reward Guided Latent Consistency Distillation",
    "abstract": "arXiv:2403.11027v1 Announce Type: cross  Abstract: Latent Consistency Distillation (LCD) has emerged as a promising paradigm for efficient text-to-image synthesis. By distilling a latent consistency model (LCM) from a pre-trained teacher latent diffusion model (LDM), LCD facilitates the generation of high-fidelity images within merely 2 to 4 inference steps. However, the LCM's efficient inference is obtained at the cost of the sample quality. In this paper, we propose compensating the quality loss by aligning LCM's output with human preference during training. Specifically, we introduce Reward Guided LCD (RG-LCD), which integrates feedback from a reward model (RM) into the LCD process by augmenting the original LCD loss with the objective of maximizing the reward associated with LCM's single-step generation. As validated through human evaluation, when trained with the feedback of a good RM, the 2-step generations from our RG-LCM are favored by humans over the 50-step DDIM samples from ",
    "link": "https://arxiv.org/abs/2403.11027",
    "context": "Title: Reward Guided Latent Consistency Distillation\nAbstract: arXiv:2403.11027v1 Announce Type: cross  Abstract: Latent Consistency Distillation (LCD) has emerged as a promising paradigm for efficient text-to-image synthesis. By distilling a latent consistency model (LCM) from a pre-trained teacher latent diffusion model (LDM), LCD facilitates the generation of high-fidelity images within merely 2 to 4 inference steps. However, the LCM's efficient inference is obtained at the cost of the sample quality. In this paper, we propose compensating the quality loss by aligning LCM's output with human preference during training. Specifically, we introduce Reward Guided LCD (RG-LCD), which integrates feedback from a reward model (RM) into the LCD process by augmenting the original LCD loss with the objective of maximizing the reward associated with LCM's single-step generation. As validated through human evaluation, when trained with the feedback of a good RM, the 2-step generations from our RG-LCM are favored by humans over the 50-step DDIM samples from ",
    "path": "papers/24/03/2403.11027.json",
    "total_tokens": 890,
    "translated_title": "奖励引导的潜在一致性蒸馏",
    "translated_abstract": "潜在一致性蒸馏(LCD)已成为一种有效的文本到图像合成范式。通过从预训练的教师潜在扩散模型(LDM)中蒸馏出潜在一致性模型(LCM)，LCD在仅需2到4个推理步骤内促进了高保真图像的生成。然而，LCM的高效推理是以样本质量为代价的。本文提出通过在训练过程中将LCM的输出与人类偏好对齐来补偿质量损失。具体而言，我们引入奖励引导的LCD(RG-LCD)，通过将奖励模型(RM)的反馈整合到LCD过程中，通过将原始LCD损失与最大化与LCM单步生成相关联的奖励的目标相结合，来最大化奖励。通过人类评估验证，当使用良好RM的反馈进行训练时，我们的RG-LCM的2步生成被人类青睐，超过了50步DDIM样本。",
    "tldr": "该论文提出了一种奖励引导的潜在一致性蒸馏方法，通过在LCD过程中整合奖励模型的反馈，从而有效提高高保真图像生成时的样本质量。",
    "en_tdlr": "The paper introduces a reward guided latent consistency distillation method, which effectively improves the sample quality of high-fidelity image generation by integrating feedback from a reward model into the LCD process."
}