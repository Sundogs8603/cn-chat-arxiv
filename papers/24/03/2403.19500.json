{
    "title": "Tensor Network-Constrained Kernel Machines as Gaussian Processes",
    "abstract": "arXiv:2403.19500v1 Announce Type: new  Abstract: Tensor Networks (TNs) have recently been used to speed up kernel machines by constraining the model weights, yielding exponential computational and storage savings. In this paper we prove that the outputs of Canonical Polyadic Decomposition (CPD) and Tensor Train (TT)-constrained kernel machines recover a Gaussian Process (GP), which we fully characterize, when placing i.i.d. priors over their parameters. We analyze the convergence of both CPD and TT-constrained models, and show how TT yields models exhibiting more GP behavior compared to CPD, for the same number of model parameters. We empirically observe this behavior in two numerical experiments where we respectively analyze the convergence to the GP and the performance at prediction. We thereby establish a connection between TN-constrained kernel machines and GPs.",
    "link": "https://arxiv.org/abs/2403.19500",
    "context": "Title: Tensor Network-Constrained Kernel Machines as Gaussian Processes\nAbstract: arXiv:2403.19500v1 Announce Type: new  Abstract: Tensor Networks (TNs) have recently been used to speed up kernel machines by constraining the model weights, yielding exponential computational and storage savings. In this paper we prove that the outputs of Canonical Polyadic Decomposition (CPD) and Tensor Train (TT)-constrained kernel machines recover a Gaussian Process (GP), which we fully characterize, when placing i.i.d. priors over their parameters. We analyze the convergence of both CPD and TT-constrained models, and show how TT yields models exhibiting more GP behavior compared to CPD, for the same number of model parameters. We empirically observe this behavior in two numerical experiments where we respectively analyze the convergence to the GP and the performance at prediction. We thereby establish a connection between TN-constrained kernel machines and GPs.",
    "path": "papers/24/03/2403.19500.json",
    "total_tokens": 815,
    "translated_title": "张量网络约束的核机器作为高斯过程",
    "translated_abstract": "张量网络（TNs）最近被用来通过约束模型权重加快核机器的速度，产生了指数级的计算和存储节约。在本文中，我们证明Canonical Polyadic Decomposition（CPD）和Tensor Train（TT）约束的核机器的输出会在对参数进行i.i.d.先验分布的情况下恢复为高斯过程（GP），我们完全表征了这一过程。我们分析了CPD和TT约束模型的收敛性，并展示了TT相对于CPD具有更多GP行为的模型，而模型参数的数量相同。我们通过两个数值实验在两个方面实证观察了这一行为，分别是分析到GP的收敛性和预测性能。因此，我们建立了张量网络约束的核机器和高斯过程之间的联系。",
    "tldr": "本文证明了Canonical Polyadic Decomposition和Tensor Train约束的核机器的输出会在对参数进行i.i.d.先验分布的情况下恢复为高斯过程，并且通过实验证明了Tensor Train模型相对于Canonical Polyadic Decomposition模型具有更多高斯过程行为。",
    "en_tdlr": "The paper proves that the outputs of Canonical Polyadic Decomposition and Tensor Train constrained kernel machines recover a Gaussian Process when placing i.i.d. priors over their parameters, and empirically demonstrates that the Tensor Train model exhibits more Gaussian Process behavior compared to the Canonical Polyadic Decomposition model."
}