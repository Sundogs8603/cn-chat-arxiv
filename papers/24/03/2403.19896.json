{
    "title": "Nonlinearity Enhanced Adaptive Activation Function",
    "abstract": "arXiv:2403.19896v1 Announce Type: new  Abstract: A simply implemented activation function with even cubic nonlinearity is introduced that increases the accuracy of neural networks without substantial additional computational resources. This is partially enabled through an apparent tradeoff between convergence and accuracy. The activation function generalizes the standard RELU function by introducing additional degrees of freedom through optimizable parameters that enable the degree of nonlinearity to be adjusted. The associated accuracy enhancement is quantified in the context of the MNIST digit data set through a comparison with standard techniques.",
    "link": "https://arxiv.org/abs/2403.19896",
    "context": "Title: Nonlinearity Enhanced Adaptive Activation Function\nAbstract: arXiv:2403.19896v1 Announce Type: new  Abstract: A simply implemented activation function with even cubic nonlinearity is introduced that increases the accuracy of neural networks without substantial additional computational resources. This is partially enabled through an apparent tradeoff between convergence and accuracy. The activation function generalizes the standard RELU function by introducing additional degrees of freedom through optimizable parameters that enable the degree of nonlinearity to be adjusted. The associated accuracy enhancement is quantified in the context of the MNIST digit data set through a comparison with standard techniques.",
    "path": "papers/24/03/2403.19896.json",
    "total_tokens": 647,
    "translated_title": "非线性增强自适应激活函数",
    "translated_abstract": "引入一种简单实现的激活函数，具有甚至立方非线性，可以提高神经网络的准确性，而不需要太多额外的计算资源。通过一种明显的收敛与准确性之间的权衡来实现。该激活函数通过引入可优化参数来泛化标准RELU函数，从而增加了额外的自由度，使得非线性程度可以被调整。通过与标准技术进行比较，将在MNIST数字数据集的背景下量化相关准确性的提升。",
    "tldr": "引入了一种带有甚至立方非线性的简单实现激活函数，通过引入可优化参数使得激活函数具有更大的自由度，可以提高神经网络的准确性，同时不需要太多额外的计算资源。"
}