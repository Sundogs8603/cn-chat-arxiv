{
    "title": "Fairness Feedback Loops: Training on Synthetic Data Amplifies Bias",
    "abstract": "arXiv:2403.07857v1 Announce Type: new  Abstract: Model-induced distribution shifts (MIDS) occur as previous model outputs pollute new model training sets over generations of models. This is known as model collapse in the case of generative models, and performative prediction or unfairness feedback loops for supervised models. When a model induces a distribution shift, it also encodes its mistakes, biases, and unfairnesses into the ground truth of its data ecosystem. We introduce a framework that allows us to track multiple MIDS over many generations, finding that they can lead to loss in performance, fairness, and minoritized group representation, even in initially unbiased datasets. Despite these negative consequences, we identify how models might be used for positive, intentional, interventions in their data ecosystems, providing redress for historical discrimination through a framework called algorithmic reparation (AR). We simulate AR interventions by curating representative traini",
    "link": "https://arxiv.org/abs/2403.07857",
    "context": "Title: Fairness Feedback Loops: Training on Synthetic Data Amplifies Bias\nAbstract: arXiv:2403.07857v1 Announce Type: new  Abstract: Model-induced distribution shifts (MIDS) occur as previous model outputs pollute new model training sets over generations of models. This is known as model collapse in the case of generative models, and performative prediction or unfairness feedback loops for supervised models. When a model induces a distribution shift, it also encodes its mistakes, biases, and unfairnesses into the ground truth of its data ecosystem. We introduce a framework that allows us to track multiple MIDS over many generations, finding that they can lead to loss in performance, fairness, and minoritized group representation, even in initially unbiased datasets. Despite these negative consequences, we identify how models might be used for positive, intentional, interventions in their data ecosystems, providing redress for historical discrimination through a framework called algorithmic reparation (AR). We simulate AR interventions by curating representative traini",
    "path": "papers/24/03/2403.07857.json",
    "total_tokens": 885,
    "translated_title": "公平反馈循环：在合成数据上训练会放大偏见",
    "translated_abstract": "模型诱导的分布转移(MIDS)会导致先前模型输出污染新模型训练集，随着模型的演变。对于生成模型来说，这被称为模型崩溃，对于监督模型来说则是表现预测或不公平反馈循环。当一个模型诱导了分布的转移，它也将其错误、偏见和不公平性编码到数据生态系统的基本事实中。我们引入了一个框架，使我们能够跟踪多个MIDS长时间的演变，发现它们可能导致性能、公平性和边缘群体表现的损失，即使在最初是无偏的数据集中也是如此。尽管存在这些负面后果，我们确定了模型如何可以用于积极、有意的干预其数据生态系统，通过一种名为算法修复(AR)的框架为历史上的歧视提供补救。我们通过策划代表性培训来模拟AR干预。",
    "tldr": "模型诱导的分布转移可能导致性能、公平性和边缘群体表现的损失，提出了算法修复(AR)框架以通过积极干预实现对历史歧视的补救",
    "en_tdlr": "Model-induced distribution shifts can lead to loss in performance, fairness, and minoritized group representation, proposing the framework of algorithmic reparation (AR) for remedying historical discrimination through positive interventions."
}