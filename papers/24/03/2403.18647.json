{
    "title": "SDSAT: Accelerating LLM Inference through Speculative Decoding with Semantic Adaptive Tokens",
    "abstract": "arXiv:2403.18647v1 Announce Type: new  Abstract: We propose an acceleration scheme for large language models (LLMs) through Speculative Decoding with Semantic Adaptive Tokens (SDSAT). The primary objective of this design is to enhance the LLM model's ability to generate draft tokens more accurately without compromising the model's accuracy. The core strategies involve: 1) Fine-tune the model by incorporating semantic adaptive tokens that possess flexible decoding capabilities without changing its structure, allowing them to generate high-quality draft tokens. 2) By employing a training method that does not affect the standard tokens, the model can acquire parallel decoding abilities atop its original framework with minimal training overhead. 3) We have designed the \"two-step-draft-then-verify\" generation strategies using both greedy search and nucleus sampling. Experiments conducted on the CodeLlama-13B and 7B models have yielded speed increases of over 3.5X and 3.0X, respectively. Ple",
    "link": "https://arxiv.org/abs/2403.18647",
    "context": "Title: SDSAT: Accelerating LLM Inference through Speculative Decoding with Semantic Adaptive Tokens\nAbstract: arXiv:2403.18647v1 Announce Type: new  Abstract: We propose an acceleration scheme for large language models (LLMs) through Speculative Decoding with Semantic Adaptive Tokens (SDSAT). The primary objective of this design is to enhance the LLM model's ability to generate draft tokens more accurately without compromising the model's accuracy. The core strategies involve: 1) Fine-tune the model by incorporating semantic adaptive tokens that possess flexible decoding capabilities without changing its structure, allowing them to generate high-quality draft tokens. 2) By employing a training method that does not affect the standard tokens, the model can acquire parallel decoding abilities atop its original framework with minimal training overhead. 3) We have designed the \"two-step-draft-then-verify\" generation strategies using both greedy search and nucleus sampling. Experiments conducted on the CodeLlama-13B and 7B models have yielded speed increases of over 3.5X and 3.0X, respectively. Ple",
    "path": "papers/24/03/2403.18647.json",
    "total_tokens": 901,
    "translated_title": "SDSAT：通过具有语义自适应令牌的推测解码加速LLM推断",
    "translated_abstract": "我们提出了一种用于大型语言模型（LLMs）的加速方案，通过具有语义自适应令牌（SDSAT）进行推测解码。该设计的主要目标是增强LLM模型生成草稿标记的能力，而不影响模型的准确性。核心策略包括：1）通过合并具有灵活解码能力的语义自适应令牌来微调模型，而不改变其结构，使其能够生成高质量的草稿标记。2）通过使用不影响标准令牌的训练方法，模型可以在其原始框架之上获得并行解码能力，而训练开销最小。3）我们设计了使用贪婪搜索和核采样的“两步起草然后验证”生成策略。在CodeLlama-13B和7B模型上进行的实验结果显示，速度分别提高了3.5倍和3.0倍以上。",
    "tldr": "SDSAT提出了一种加速大型语言模型推断的方案，通过使用具有灵活解码能力的语义自适应令牌，可以增强模型生成高质量草稿令牌的能力，并实现超过3.5倍和3.0倍的速度提升。",
    "en_tdlr": "SDSAT proposes an acceleration scheme for large language models by enhancing the ability to generate high-quality draft tokens through the use of semantic adaptive tokens with flexible decoding capabilities, achieving speed increases of over 3.5X and 3.0X."
}