{
    "title": "Masked Autoencoders are PDE Learners",
    "abstract": "arXiv:2403.17728v1 Announce Type: new  Abstract: Neural solvers for partial differential equations (PDEs) have great potential, yet their practicality is currently limited by their generalizability. PDEs evolve over broad scales and exhibit diverse behaviors; predicting these phenomena will require learning representations across a wide variety of inputs, which may encompass different coefficients, geometries, or equations. As a step towards generalizable PDE modeling, we adapt masked pretraining for PDEs. Through self-supervised learning across PDEs, masked autoencoders can learn useful latent representations for downstream tasks. In particular, masked pretraining can improve coefficient regression and timestepping performance of neural solvers on unseen equations. We hope that masked pretraining can emerge as a unifying method across large, unlabeled, and heterogeneous datasets to learn latent physics at scale.",
    "link": "https://arxiv.org/abs/2403.17728",
    "context": "Title: Masked Autoencoders are PDE Learners\nAbstract: arXiv:2403.17728v1 Announce Type: new  Abstract: Neural solvers for partial differential equations (PDEs) have great potential, yet their practicality is currently limited by their generalizability. PDEs evolve over broad scales and exhibit diverse behaviors; predicting these phenomena will require learning representations across a wide variety of inputs, which may encompass different coefficients, geometries, or equations. As a step towards generalizable PDE modeling, we adapt masked pretraining for PDEs. Through self-supervised learning across PDEs, masked autoencoders can learn useful latent representations for downstream tasks. In particular, masked pretraining can improve coefficient regression and timestepping performance of neural solvers on unseen equations. We hope that masked pretraining can emerge as a unifying method across large, unlabeled, and heterogeneous datasets to learn latent physics at scale.",
    "path": "papers/24/03/2403.17728.json",
    "total_tokens": 815,
    "translated_title": "掩码自动编码器是PDE学习者",
    "translated_abstract": "神经求解器用于偏微分方程（PDE）具有巨大潜力，但实用性目前受到其泛化能力的限制。 PDE在广泛的尺度上演变并展示出多样化的行为；预测这些现象将需要学习跨越各种输入的表示，这些输入可能涵盖不同的系数、几何图形或方程。作为通向可泛化PDE建模的一步，我们为PDEs调整了掩码预训练。通过自监督学习跨越PDEs，掩码自动编码器可以学习有用的潜在表示，以用于下游任务。特别是，掩码预训练可以改善神经求解器对未见方程的系数回归和时间步骤性能。我们希望掩码预训练能成为一种通用方法，可以在大型、未标记和异构数据集上学习规模化的潜在物理学。",
    "tldr": "掩码自动编码器在偏微分方程求解器中表现出色，通过自监督学习跨越PDEs，可以学习用于下游任务的有用潜在表示。"
}