{
    "title": "Making Pre-trained Language Models Great on Tabular Prediction",
    "abstract": "arXiv:2403.01841v1 Announce Type: new  Abstract: The transferability of deep neural networks (DNNs) has made significant progress in image and language processing. However, due to the heterogeneity among tables, such DNN bonus is still far from being well exploited on tabular data prediction (e.g., regression or classification tasks). Condensing knowledge from diverse domains, language models (LMs) possess the capability to comprehend feature names from various tables, potentially serving as versatile learners in transferring knowledge across distinct tables and diverse prediction tasks, but their discrete text representation space is inherently incompatible with numerical feature values in tables. In this paper, we present TP-BERTa, a specifically pre-trained LM model for tabular data prediction. Concretely, a novel relative magnitude tokenization converts scalar numerical feature values to finely discrete, high-dimensional tokens, and an intra-feature attention approach integrates fe",
    "link": "https://arxiv.org/abs/2403.01841",
    "context": "Title: Making Pre-trained Language Models Great on Tabular Prediction\nAbstract: arXiv:2403.01841v1 Announce Type: new  Abstract: The transferability of deep neural networks (DNNs) has made significant progress in image and language processing. However, due to the heterogeneity among tables, such DNN bonus is still far from being well exploited on tabular data prediction (e.g., regression or classification tasks). Condensing knowledge from diverse domains, language models (LMs) possess the capability to comprehend feature names from various tables, potentially serving as versatile learners in transferring knowledge across distinct tables and diverse prediction tasks, but their discrete text representation space is inherently incompatible with numerical feature values in tables. In this paper, we present TP-BERTa, a specifically pre-trained LM model for tabular data prediction. Concretely, a novel relative magnitude tokenization converts scalar numerical feature values to finely discrete, high-dimensional tokens, and an intra-feature attention approach integrates fe",
    "path": "papers/24/03/2403.01841.json",
    "total_tokens": 772,
    "translated_title": "在表格预测上优化预训练语言模型的方法",
    "translated_abstract": "深度神经网络（DNN）的可迁移性在图像和语言处理领域取得了显著进展。然而，由于表格之间的异质性，这种DNN的优势在表格数据预测（例如回归或分类任务）上仍未得到充分利用。本文提出了TP-BERTa，这是一种专门为表格数据预测而预训练的语言模型。具体而言，一种新颖的相对大小标记化方法将标量数值特征值转换为离散度高、高维度的标记，并且一种内部特征关注方法整合了特征名称和数值特征值。",
    "tldr": "提出了一种专门为表格数据预测而预训练的语言模型TP-BERTa，通过新颖的相对大小标记化方法和内部特征关注方法解决了预训练语言模型在数值特征值上的不兼容性问题",
    "en_tdlr": "Introduced a pre-trained language model TP-BERTa specifically designed for tabular data prediction, addressing the incompatibility issue of pre-trained language models with numerical feature values through novel relative magnitude tokenization and intra-feature attention approach."
}