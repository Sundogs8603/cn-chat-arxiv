{
    "title": "Shuffling Momentum Gradient Algorithm for Convex Optimization",
    "abstract": "arXiv:2403.03180v1 Announce Type: cross  Abstract: The Stochastic Gradient Descent method (SGD) and its stochastic variants have become methods of choice for solving finite-sum optimization problems arising from machine learning and data science thanks to their ability to handle large-scale applications and big datasets. In the last decades, researchers have made substantial effort to study the theoretical performance of SGD and its shuffling variants. However, only limited work has investigated its shuffling momentum variants, including shuffling heavy-ball momentum schemes for non-convex problems and Nesterov's momentum for convex settings. In this work, we extend the analysis of the shuffling momentum gradient method developed in [Tran et al (2021)] to both finite-sum convex and strongly convex optimization problems. We provide the first analysis of shuffling momentum-based methods for the strongly convex setting, attaining a convergence rate of $O(1/nT^2)$, where $n$ is the number ",
    "link": "https://arxiv.org/abs/2403.03180",
    "context": "Title: Shuffling Momentum Gradient Algorithm for Convex Optimization\nAbstract: arXiv:2403.03180v1 Announce Type: cross  Abstract: The Stochastic Gradient Descent method (SGD) and its stochastic variants have become methods of choice for solving finite-sum optimization problems arising from machine learning and data science thanks to their ability to handle large-scale applications and big datasets. In the last decades, researchers have made substantial effort to study the theoretical performance of SGD and its shuffling variants. However, only limited work has investigated its shuffling momentum variants, including shuffling heavy-ball momentum schemes for non-convex problems and Nesterov's momentum for convex settings. In this work, we extend the analysis of the shuffling momentum gradient method developed in [Tran et al (2021)] to both finite-sum convex and strongly convex optimization problems. We provide the first analysis of shuffling momentum-based methods for the strongly convex setting, attaining a convergence rate of $O(1/nT^2)$, where $n$ is the number ",
    "path": "papers/24/03/2403.03180.json",
    "total_tokens": 879,
    "translated_title": "用于凸优化的洗牌动量梯度算法",
    "translated_abstract": "随机梯度下降方法（SGD）及其随机变体已成为解决机器学习和数据科学中由大规模应用和大数据集产生的有限和优化问题的首选方法，由于其能够处理大规模应用和大型数据集。过去几十年，研究人员已经付出了大量努力来研究SGD及其洗牌变体的理论性能。然而，只有有限的工作涉及了其洗牌动量变体，包括用于非凸问题的洗牌重量球动量方案和用于凸设置的Nesterov动量。在这项工作中，我们将Tran等人（2021年）所开发的洗牌动量梯度方法的分析拓展到有限和强凸优化问题，我们首次提供了针对强凸设置的洗牌动量方法的分析，达到了收敛速度为$O(1/nT^2)$，其中$n$是数量",
    "tldr": "本研究将洗牌动量梯度方法扩展到有限和强凸优化问题，首次提供了针对强凸设置的洗牌动量方法的分析，达到了收敛速度为$O(1/nT^2)$。",
    "en_tdlr": "This study extends the shuffling momentum gradient method to finite-sum convex and strongly convex optimization problems, providing the first analysis of shuffling momentum-based methods for the strongly convex setting, achieving a convergence rate of $O(1/nT^2)$."
}