{
    "title": "Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models",
    "abstract": "arXiv:2403.11838v1 Announce Type: cross  Abstract: Large Language Models (LLMs) exhibit impressive capabilities but also present risks such as biased content generation and privacy issues. One of the current alignment techniques includes principle-driven integration, but it faces challenges arising from the imprecision of manually crafted rules and inadequate risk perception in models without safety training. To address these, we introduce Guide-Align, a two-stage approach. Initially, a safety-trained model identifies potential risks and formulates specific guidelines for various inputs, thereby establishing a comprehensive library of guidelines and models for input-guidelines retrieval. Subsequently, the retrieval model correlates new inputs with pertinent guidelines, guiding LLMs in response generation to ensure safe and high-quality outputs, thus aligning with human values. An additional optional stage involves fine-tuning a model with new well-aligned datasets generated through the",
    "link": "https://arxiv.org/abs/2403.11838",
    "context": "Title: Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models\nAbstract: arXiv:2403.11838v1 Announce Type: cross  Abstract: Large Language Models (LLMs) exhibit impressive capabilities but also present risks such as biased content generation and privacy issues. One of the current alignment techniques includes principle-driven integration, but it faces challenges arising from the imprecision of manually crafted rules and inadequate risk perception in models without safety training. To address these, we introduce Guide-Align, a two-stage approach. Initially, a safety-trained model identifies potential risks and formulates specific guidelines for various inputs, thereby establishing a comprehensive library of guidelines and models for input-guidelines retrieval. Subsequently, the retrieval model correlates new inputs with pertinent guidelines, guiding LLMs in response generation to ensure safe and high-quality outputs, thus aligning with human values. An additional optional stage involves fine-tuning a model with new well-aligned datasets generated through the",
    "path": "papers/24/03/2403.11838.json",
    "total_tokens": 853,
    "translated_title": "确保安全和高质量输出：面向语言模型的指南库方法",
    "translated_abstract": "大型语言模型(LLMs)展示了令人印象深刻的能力，但也存在偏见内容生成和隐私问题等风险。当前的对齐技术之一包括基于原则的集成，但面临由于手工制定规则的不精确性和未经安全训练的模型对风险感知不足而产生的挑战。为了解决这些问题，我们引入了Guide-Align，这是一种两阶段方法。最初，一个经过安全训练的模型识别潜在风险，并为各种输入制定具体指南，从而建立了全面的指南库和用于输入指南检索的模型。随后，检索模型将新输入与相关指南相关联，引导LLMs在响应生成中确保安全和高质量输出，从而与人类价值观一致。另一个额外可选阶段涉及使用经过细致对齐的新数据集对模型进行微调。",
    "tldr": "引入Guide-Align，一种两阶段方法，通过安全训练模型识别潜在风险，并制定特定指南，从而建立全面的指导库，用于指导LLMs生成安全和高质量输出。",
    "en_tdlr": "Introducing Guide-align, a two-stage approach to identify potential risks with safety-trained models, formulate specific guidelines, establish a comprehensive library for guiding LLMs to generate safe and high-quality outputs."
}