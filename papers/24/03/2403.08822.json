{
    "title": "LoRA-SP: Streamlined Partial Parameter Adaptation for Resource-Efficient Fine-Tuning of Large Language Models",
    "abstract": "arXiv:2403.08822v1 Announce Type: cross  Abstract: In addressing the computational and memory demands of fine-tuning Large Language Models(LLMs), we propose LoRA-SP(Streamlined Partial Parameter Adaptation), a novel approach utilizing randomized half-selective parameter freezing within the Low-Rank Adaptation(LoRA)framework. This method efficiently balances pre-trained knowledge retention and adaptability for task-specific optimizations. Through a randomized mechanism, LoRA-SP determines which parameters to update or freeze, significantly reducing computational and memory requirements without compromising model performance. We evaluated LoRA-SP across several benchmark NLP tasks, demonstrating its ability to achieve competitive performance with substantially lower resource consumption compared to traditional full-parameter fine-tuning and other parameter-efficient techniques. LoRA-SP innovative approach not only facilitates the deployment of advanced NLP models in resource-limited sett",
    "link": "https://arxiv.org/abs/2403.08822",
    "context": "Title: LoRA-SP: Streamlined Partial Parameter Adaptation for Resource-Efficient Fine-Tuning of Large Language Models\nAbstract: arXiv:2403.08822v1 Announce Type: cross  Abstract: In addressing the computational and memory demands of fine-tuning Large Language Models(LLMs), we propose LoRA-SP(Streamlined Partial Parameter Adaptation), a novel approach utilizing randomized half-selective parameter freezing within the Low-Rank Adaptation(LoRA)framework. This method efficiently balances pre-trained knowledge retention and adaptability for task-specific optimizations. Through a randomized mechanism, LoRA-SP determines which parameters to update or freeze, significantly reducing computational and memory requirements without compromising model performance. We evaluated LoRA-SP across several benchmark NLP tasks, demonstrating its ability to achieve competitive performance with substantially lower resource consumption compared to traditional full-parameter fine-tuning and other parameter-efficient techniques. LoRA-SP innovative approach not only facilitates the deployment of advanced NLP models in resource-limited sett",
    "path": "papers/24/03/2403.08822.json",
    "total_tokens": 874,
    "translated_title": "LoRA-SP：用于资源高效微调大型语言模型的简化部分参数适应",
    "translated_abstract": "在解决大型语言模型（LLMs）微调的计算和内存需求方面，我们提出了 LoRA-SP（简化部分参数适应），这是一种新颖的方法，利用低秩适应（LoRA）框架内的随机半选择参数冻结。该方法有效地平衡了预训练知识的保留和任务特定优化的适应性。通过随机机制，LoRA-SP 确定要更新或冻结哪些参数，显著降低了计算和内存需求，而不会影响模型性能。我们在几个基准 NLP 任务中评估了 LoRA-SP，展示了它与传统的全参数微调和其他参数高效技术相比，能够以大大降低的资源消耗实现竞争性性能。LoRA-SP 的创新方法不仅有助于在资源有限的情况下部署先进的 NLP 模型。",
    "tldr": "LoRA-SP利用随机半选择参数冻结的新颖方法，在微调大型语言模型时有效平衡预训练知识的保留和任务特定优化的适应性，显著降低了计算和内存需求，同时实现了竞争性性能。",
    "en_tdlr": "LoRA-SP proposes a novel approach of utilizing randomized half-selective parameter freezing to effectively balance pre-trained knowledge retention and task-specific optimization in fine-tuning large language models, significantly reducing computational and memory demands while achieving competitive performance."
}