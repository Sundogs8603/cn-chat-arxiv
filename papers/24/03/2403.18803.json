{
    "title": "Projective Methods for Mitigating Gender Bias in Pre-trained Language Models",
    "abstract": "arXiv:2403.18803v1 Announce Type: new  Abstract: Mitigation of gender bias in NLP has a long history tied to debiasing static word embeddings. More recently, attention has shifted to debiasing pre-trained language models. We study to what extent the simplest projective debiasing methods, developed for word embeddings, can help when applied to BERT's internal representations. Projective methods are fast to implement, use a small number of saved parameters, and make no updates to the existing model parameters. We evaluate the efficacy of the methods in reducing both intrinsic bias, as measured by BERT's next sentence prediction task, and in mitigating observed bias in a downstream setting when fine-tuned. To this end, we also provide a critical analysis of a popular gender-bias assessment test for quantifying intrinsic bias, resulting in an enhanced test set and new bias measures. We find that projective methods can be effective at both intrinsic bias and downstream bias mitigation, but ",
    "link": "https://arxiv.org/abs/2403.18803",
    "context": "Title: Projective Methods for Mitigating Gender Bias in Pre-trained Language Models\nAbstract: arXiv:2403.18803v1 Announce Type: new  Abstract: Mitigation of gender bias in NLP has a long history tied to debiasing static word embeddings. More recently, attention has shifted to debiasing pre-trained language models. We study to what extent the simplest projective debiasing methods, developed for word embeddings, can help when applied to BERT's internal representations. Projective methods are fast to implement, use a small number of saved parameters, and make no updates to the existing model parameters. We evaluate the efficacy of the methods in reducing both intrinsic bias, as measured by BERT's next sentence prediction task, and in mitigating observed bias in a downstream setting when fine-tuned. To this end, we also provide a critical analysis of a popular gender-bias assessment test for quantifying intrinsic bias, resulting in an enhanced test set and new bias measures. We find that projective methods can be effective at both intrinsic bias and downstream bias mitigation, but ",
    "path": "papers/24/03/2403.18803.json",
    "total_tokens": 888,
    "translated_title": "用于缓解预训练语言模型性别偏见的投影方法",
    "translated_abstract": "在自然语言处理中缓解性别偏见的方法与去偏置静态词嵌入有着悠久的历史。最近，注意力转向了去偏置预训练语言模型。本文研究了最简单的投影去偏置方法在应用于BERT内部表示时能够帮助到什么程度。投影方法实现快速，使用少量保存的参数，并且不对现有模型参数进行更新。我们评估了这些方法在减少BERT对内在偏见的效果，通过BERT下一个句子预测任务进行测量，以及在微调下游任务时缓解观察到的偏见。为此，我们还对用于量化内在偏见的流行性别偏见评估测试进行了批判性分析，从而得到了一个增强的测试集和新的偏见测量。我们发现投影方法在减少内在偏见和缓解下游偏见方面都可以取得良好效果。",
    "tldr": "研究探讨了将用于静态词嵌入的最简单投影去偏置方法应用于BERT内部表示的效果，发现这种方法既可以减少内在偏见，又可以缓解下游任务中观察到的偏见。",
    "en_tdlr": "Investigated the efficacy of applying simple projective debiasing methods developed for word embeddings to BERT's internal representations, showing effectiveness in reducing intrinsic bias and mitigating observed bias in downstream tasks."
}