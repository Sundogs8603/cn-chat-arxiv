{
    "title": "Learning with Logical Constraints but without Shortcut Satisfaction",
    "abstract": "arXiv:2403.00329v1 Announce Type: new  Abstract: Recent studies in neuro-symbolic learning have explored the integration of logical knowledge into deep learning via encoding logical constraints as an additional loss function. However, existing approaches tend to vacuously satisfy logical constraints through shortcuts, failing to fully exploit the knowledge. In this paper, we present a new framework for learning with logical constraints. Specifically, we address the shortcut satisfaction issue by introducing dual variables for logical connectives, encoding how the constraint is satisfied. We further propose a variational framework where the encoded logical constraint is expressed as a distributional loss that is compatible with the model's original training loss. The theoretical analysis shows that the proposed approach bears salient properties, and the experimental evaluations demonstrate its superior performance in both model generalizability and constraint satisfaction.",
    "link": "https://arxiv.org/abs/2403.00329",
    "context": "Title: Learning with Logical Constraints but without Shortcut Satisfaction\nAbstract: arXiv:2403.00329v1 Announce Type: new  Abstract: Recent studies in neuro-symbolic learning have explored the integration of logical knowledge into deep learning via encoding logical constraints as an additional loss function. However, existing approaches tend to vacuously satisfy logical constraints through shortcuts, failing to fully exploit the knowledge. In this paper, we present a new framework for learning with logical constraints. Specifically, we address the shortcut satisfaction issue by introducing dual variables for logical connectives, encoding how the constraint is satisfied. We further propose a variational framework where the encoded logical constraint is expressed as a distributional loss that is compatible with the model's original training loss. The theoretical analysis shows that the proposed approach bears salient properties, and the experimental evaluations demonstrate its superior performance in both model generalizability and constraint satisfaction.",
    "path": "papers/24/03/2403.00329.json",
    "total_tokens": 820,
    "translated_title": "在不满足捷径的情况下学习逻辑约束",
    "translated_abstract": "最近的神经符号学习研究探讨了通过将逻辑知识编码为额外的损失函数将逻辑约束整合到深度学习中。然而，现有方法往往通过捷径虚假地满足了逻辑约束，未能充分利用知识。本文提出了一个新的学习逻辑约束的框架。具体而言，我们通过引入逻辑连接词的双重变量来解决捷径满足问题，对约束的满足方式进行编码。我们进一步提出了一个变分框架，其中编码的逻辑约束被表达为一个分布损失，与模型的原始训练损失兼容。理论分析表明，所提出的方法具有显著的特性，实验评估显示其在模型的普适性和约束满足方面性能优越。",
    "tldr": "引入了逻辑连接词的双重变量来解决捷径满足问题，提出了一个新的学习逻辑约束的框架，实验证明其在模型的普适性和约束满足方面性能优越。",
    "en_tdlr": "Introducing dual variables for logical connectives to address shortcut satisfaction issue, a new framework for learning with logical constraints is proposed, showing superior performance in model generalizability and constraint satisfaction through experimental evaluations."
}