{
    "title": "Bridge the Modality and Capacity Gaps in Vision-Language Model Selection",
    "abstract": "arXiv:2403.13797v1 Announce Type: new  Abstract: Vision Language Models (VLMs) excel in zero-shot image classification by pairing images with textual category names. The expanding variety of Pre-Trained VLMs enhances the likelihood of identifying a suitable VLM for specific tasks. Thus, a promising zero-shot image classification strategy is selecting the most appropriate Pre-Trained VLM from the VLM Zoo, relying solely on the text data of the target dataset without access to the dataset's images. In this paper, we analyze two inherent challenges in assessing the ability of a VLM in this Language-Only VLM selection: the \"Modality Gap\" -- the disparity in VLM's embeddings across two different modalities, making text a less reliable substitute for images; and the \"Capability Gap\" -- the discrepancy between the VLM's overall ranking and its ranking for target dataset, hindering direct prediction of a model's dataset-specific performance from its general performance. We propose VLM Selectio",
    "link": "https://arxiv.org/abs/2403.13797",
    "context": "Title: Bridge the Modality and Capacity Gaps in Vision-Language Model Selection\nAbstract: arXiv:2403.13797v1 Announce Type: new  Abstract: Vision Language Models (VLMs) excel in zero-shot image classification by pairing images with textual category names. The expanding variety of Pre-Trained VLMs enhances the likelihood of identifying a suitable VLM for specific tasks. Thus, a promising zero-shot image classification strategy is selecting the most appropriate Pre-Trained VLM from the VLM Zoo, relying solely on the text data of the target dataset without access to the dataset's images. In this paper, we analyze two inherent challenges in assessing the ability of a VLM in this Language-Only VLM selection: the \"Modality Gap\" -- the disparity in VLM's embeddings across two different modalities, making text a less reliable substitute for images; and the \"Capability Gap\" -- the discrepancy between the VLM's overall ranking and its ranking for target dataset, hindering direct prediction of a model's dataset-specific performance from its general performance. We propose VLM Selectio",
    "path": "papers/24/03/2403.13797.json",
    "total_tokens": 877,
    "translated_title": "弥合视觉-语言模型选择中的模态差距和能力差距",
    "translated_abstract": "视觉语言模型（VLMs）通过将图像与文本类别名称配对，在零样本图像分类方面表现出色。预训练的VLMs的不断增加使得特定任务的VLM选择更有可能标识出适合的VLM。因此，一种有前途的零样本图像分类策略是从VLM动物园中选择最合适的预训练VLM，仅依赖目标数据集的文本数据而无需访问数据集的图像。本文分析了这种仅语言VLM选择中两个固有挑战：「模态差距」——VLM在两个不同模态下的嵌入之间的差异，使得文本成为图像的一个不太可靠的替代品；「能力差距」——VLM的整体排名与其在目标数据集的排名之间存在差异，阻碍了直接从模型的整体表现来预测其数据集特定性能。我们提出了VLM选择",
    "tldr": "本文分析了在语言-Only VLM选择中的两个固有挑战：「模态差距」和「能力差距」，并提出了VLM选择中弥合这两个差距的方法"
}