{
    "title": "LayoutLLM: Large Language Model Instruction Tuning for Visually Rich Document Understanding",
    "abstract": "arXiv:2403.14252v1 Announce Type: cross  Abstract: This paper proposes LayoutLLM, a more flexible document analysis method for understanding imaged documents. Visually Rich Document Understanding tasks, such as document image classification and information extraction, have gained significant attention due to their importance. Existing methods have been developed to enhance document comprehension by incorporating pre-training awareness of images, text, and layout structure. However, these methods require fine-tuning for each task and dataset, and the models are expensive to train and operate. To overcome this limitation, we propose a new LayoutLLM that integrates these with large-scale language models (LLMs). By leveraging the strengths of existing research in document image understanding and LLMs' superior language understanding capabilities, the proposed model, fine-tuned with multimodal instruction datasets, performs an understanding of document images in a single model. Our experime",
    "link": "https://arxiv.org/abs/2403.14252",
    "context": "Title: LayoutLLM: Large Language Model Instruction Tuning for Visually Rich Document Understanding\nAbstract: arXiv:2403.14252v1 Announce Type: cross  Abstract: This paper proposes LayoutLLM, a more flexible document analysis method for understanding imaged documents. Visually Rich Document Understanding tasks, such as document image classification and information extraction, have gained significant attention due to their importance. Existing methods have been developed to enhance document comprehension by incorporating pre-training awareness of images, text, and layout structure. However, these methods require fine-tuning for each task and dataset, and the models are expensive to train and operate. To overcome this limitation, we propose a new LayoutLLM that integrates these with large-scale language models (LLMs). By leveraging the strengths of existing research in document image understanding and LLMs' superior language understanding capabilities, the proposed model, fine-tuned with multimodal instruction datasets, performs an understanding of document images in a single model. Our experime",
    "path": "papers/24/03/2403.14252.json",
    "total_tokens": 804,
    "translated_title": "LayoutLLM：大规模语言模型指令调整用于视觉丰富文档理解",
    "translated_abstract": "这篇论文提出了LayoutLLM，一种更灵活的文档分析方法，用于理解图像文档。视觉丰富文档理解任务，如文档图像分类和信息提取，由于其重要性而受到重视。现有方法旨在通过整合对图像、文本和布局结构的预训练意识来提升文档理解能力。然而，这些方法需要针对每个任务和数据集进行微调，而且模型训练和操作成本高昂。为了克服这一限制，我们提出了一种新的LayoutLLM，将这些与大规模语言模型（LLMs）相集成。通过利用现有研究在文档图像理解和LLMs卓越的语言理解能力方面的优势，所提出的模型在多模态指令数据集的微调下，可以通过单个模型理解文档图像。",
    "tldr": "提出了一种新的LayoutLLM模型，通过结合大规模语言模型和文档图像理解的优势，实现了对文档图像的理解。",
    "en_tdlr": "Proposed a new LayoutLLM model that leverages the advantages of large language models and document image understanding to achieve document image comprehension."
}