{
    "title": "CEAT: Continual Expansion and Absorption Transformer for Non-Exemplar Class-Incremental Learnin",
    "abstract": "arXiv:2403.06670v1 Announce Type: cross  Abstract: In real-world applications, dynamic scenarios require the models to possess the capability to learn new tasks continuously without forgetting the old knowledge. Experience-Replay methods store a subset of the old images for joint training. In the scenario of more strict privacy protection, storing the old images becomes infeasible, which leads to a more severe plasticity-stability dilemma and classifier bias. To meet the above challenges, we propose a new architecture, named continual expansion and absorption transformer~(CEAT). The model can learn the novel knowledge by extending the expanded-fusion layers in parallel with the frozen previous parameters. After the task ends, we losslessly absorb the extended parameters into the backbone to ensure that the number of parameters remains constant. To improve the learning ability of the model, we designed a novel prototype contrastive loss to reduce the overlap between old and new classes ",
    "link": "https://arxiv.org/abs/2403.06670",
    "context": "Title: CEAT: Continual Expansion and Absorption Transformer for Non-Exemplar Class-Incremental Learnin\nAbstract: arXiv:2403.06670v1 Announce Type: cross  Abstract: In real-world applications, dynamic scenarios require the models to possess the capability to learn new tasks continuously without forgetting the old knowledge. Experience-Replay methods store a subset of the old images for joint training. In the scenario of more strict privacy protection, storing the old images becomes infeasible, which leads to a more severe plasticity-stability dilemma and classifier bias. To meet the above challenges, we propose a new architecture, named continual expansion and absorption transformer~(CEAT). The model can learn the novel knowledge by extending the expanded-fusion layers in parallel with the frozen previous parameters. After the task ends, we losslessly absorb the extended parameters into the backbone to ensure that the number of parameters remains constant. To improve the learning ability of the model, we designed a novel prototype contrastive loss to reduce the overlap between old and new classes ",
    "path": "papers/24/03/2403.06670.json",
    "total_tokens": 857,
    "translated_title": "CEAT：用于非示范类增量学习的持续扩展和吸收变压器",
    "translated_abstract": "在现实世界的应用中，动态场景要求模型具备不断学习新任务而不忘记旧知识的能力。经验重放方法存储一部分旧图像进行联合训练。在更严格的隐私保护场景中，存储旧图像变得不可行，这导致了更为严重的可塑性-稳定性困境和分类器偏差。为应对上述挑战，我们提出了一种新的架构，称为持续扩展和吸收变压器（CEAT）。模型可以通过将扩展-融合层与冻结前期参数并行扩展来学习新知识。任务结束后，我们无损地吸收扩展的参数到主干，以确保参数数量保持恒定。为提高模型的学习能力，我们设计了一种新颖的原型对比损失，以减少旧类和新类之间的重叠。",
    "tldr": "CEAT提出了一种用于非示范类增量学习的新架构，通过持续扩展和吸收参数的方式解决了可塑性-稳定性困境和分类器偏差问题",
    "en_tdlr": "CEAT proposes a new architecture for non-exemplar class-incremental learning, addressing the plasticity-stability dilemma and classifier bias through continual expansion and absorption of parameters."
}