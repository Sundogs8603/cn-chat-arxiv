{
    "title": "SLFNet: Generating Semantic Logic Forms from Natural Language Using Semantic Probability Graphs",
    "abstract": "arXiv:2403.19936v1 Announce Type: new  Abstract: Building natural language interfaces typically uses a semantic parser to parse the user's natural language and convert it into structured \\textbf{S}emantic \\textbf{L}ogic \\textbf{F}orms (SLFs). The mainstream approach is to adopt a sequence-to-sequence framework, which requires that natural language commands and SLFs must be represented serially. Since a single natural language may have multiple SLFs or multiple natural language commands may have the same SLF, training a sequence-to-sequence model is sensitive to the choice among them, a phenomenon recorded as \"order matters\". To solve this problem, we propose a novel neural network, SLFNet, which firstly incorporates dependent syntactic information as prior knowledge and can capture the long-range interactions between contextual information and words. Secondly construct semantic probability graphs to obtain local dependencies between predictor variables. Finally we propose the Multi-Hea",
    "link": "https://arxiv.org/abs/2403.19936",
    "context": "Title: SLFNet: Generating Semantic Logic Forms from Natural Language Using Semantic Probability Graphs\nAbstract: arXiv:2403.19936v1 Announce Type: new  Abstract: Building natural language interfaces typically uses a semantic parser to parse the user's natural language and convert it into structured \\textbf{S}emantic \\textbf{L}ogic \\textbf{F}orms (SLFs). The mainstream approach is to adopt a sequence-to-sequence framework, which requires that natural language commands and SLFs must be represented serially. Since a single natural language may have multiple SLFs or multiple natural language commands may have the same SLF, training a sequence-to-sequence model is sensitive to the choice among them, a phenomenon recorded as \"order matters\". To solve this problem, we propose a novel neural network, SLFNet, which firstly incorporates dependent syntactic information as prior knowledge and can capture the long-range interactions between contextual information and words. Secondly construct semantic probability graphs to obtain local dependencies between predictor variables. Finally we propose the Multi-Hea",
    "path": "papers/24/03/2403.19936.json",
    "total_tokens": 862,
    "translated_title": "SLFNet: 使用语义概率图从自然语言生成语义逻辑形式",
    "translated_abstract": "构建自然语言接口通常使用语义解析器来解析用户的自然语言并将其转换为结构化的语义逻辑形式(SLFs)。 主流方法是采用序列到序列框架，这要求自然语言命令和SLFs必须按顺序表示。 由于单个自然语言可能具有多个SLF或多个自然语言命令可能具有相同的SLF，因此训练序列到序列模型对它们之间的选择敏感，这一现象被记录为“顺序重要性”。 为了解决这个问题，我们提出了一种新颖的神经网络SLFNet，首先将依赖句法信息作为先验知识并能够捕获上下文信息和单词之间的长距离交互作用。 其次构建语义概率图以获得预测变量之间的局部依赖性。",
    "tldr": "SLFNet提出了一种新颖的神经网络架构，利用依赖句法信息和语义概率图来从自然语言生成语义逻辑形式，以解决序列到序列模型中的“顺序重要性”问题。",
    "en_tdlr": "SLFNet proposes a novel neural network architecture that utilizes dependent syntactic information and semantic probability graphs to generate Semantic Logic Forms from natural language, addressing the issue of \"order matters\" in sequence-to-sequence models."
}