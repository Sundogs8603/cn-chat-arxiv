{
    "title": "Fine-Tuning Language Models with Reward Learning on Policy",
    "abstract": "arXiv:2403.19279v1 Announce Type: cross  Abstract: Reinforcement learning from human feedback (RLHF) has emerged as an effective approach to aligning large language models (LLMs) to human preferences. RLHF contains three steps, i.e., human preference collecting, reward learning, and policy optimization, which are usually performed serially. Despite its popularity, however, (fixed) reward models may suffer from inaccurate off-distribution, since policy optimization continuously shifts LLMs' data distribution. Repeatedly collecting new preference data from the latest LLMs may alleviate this issue, which unfortunately makes the resulting system more complicated and difficult to optimize. In this paper, we propose reward learning on policy (RLP), an unsupervised framework that refines a reward model using policy samples to keep it on-distribution. Specifically, an unsupervised multi-view learning method is introduced to learn robust representations of policy samples. Meanwhile, a synthetic",
    "link": "https://arxiv.org/abs/2403.19279",
    "context": "Title: Fine-Tuning Language Models with Reward Learning on Policy\nAbstract: arXiv:2403.19279v1 Announce Type: cross  Abstract: Reinforcement learning from human feedback (RLHF) has emerged as an effective approach to aligning large language models (LLMs) to human preferences. RLHF contains three steps, i.e., human preference collecting, reward learning, and policy optimization, which are usually performed serially. Despite its popularity, however, (fixed) reward models may suffer from inaccurate off-distribution, since policy optimization continuously shifts LLMs' data distribution. Repeatedly collecting new preference data from the latest LLMs may alleviate this issue, which unfortunately makes the resulting system more complicated and difficult to optimize. In this paper, we propose reward learning on policy (RLP), an unsupervised framework that refines a reward model using policy samples to keep it on-distribution. Specifically, an unsupervised multi-view learning method is introduced to learn robust representations of policy samples. Meanwhile, a synthetic",
    "path": "papers/24/03/2403.19279.json",
    "total_tokens": 838,
    "translated_title": "使用奖励学习在策略上微调语言模型",
    "translated_abstract": "强化学习从人类反馈（RLHF）作为一种有效的方法出现，用于使大型语言模型（LLMs）与人类偏好保持一致。RLHF包含三个步骤，即收集人类偏好、奖励学习和策略优化，通常是串行执行的。然而，（固定的）奖励模型可能会因为策略优化不断改变LLMs的数据分布而遭受不准确的离分布情况。从最新的LLMs重复收集新的偏好数据可能会缓解这个问题，但不幸的是，这会使得结果系统更加复杂和难以优化。在本文中，我们提出了在策略上的奖励学习（RLP），这是一个无监督的框架，使用策略样本来优化奖励模型以保持其分布上的一致性。具体而言，引入了一种无监督的多视图学习方法来学习策略样本的稳健表示。",
    "tldr": "提出了在策略上的奖励学习框架，使用策略样本优化奖励模型以保持其分布上的一致性",
    "en_tdlr": "Introduced a reward learning framework on policy that optimizes reward models using policy samples to maintain their on-distribution consistency."
}