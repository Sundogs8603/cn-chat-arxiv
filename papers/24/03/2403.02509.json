{
    "title": "SPUQ: Perturbation-Based Uncertainty Quantification for Large Language Models",
    "abstract": "arXiv:2403.02509v1 Announce Type: cross  Abstract: In recent years, large language models (LLMs) have become increasingly prevalent, offering remarkable text generation capabilities. However, a pressing challenge is their tendency to make confidently wrong predictions, highlighting the critical need for uncertainty quantification (UQ) in LLMs. While previous works have mainly focused on addressing aleatoric uncertainty, the full spectrum of uncertainties, including epistemic, remains inadequately explored. Motivated by this gap, we introduce a novel UQ method, sampling with perturbation for UQ (SPUQ), designed to tackle both aleatoric and epistemic uncertainties. The method entails generating a set of perturbations for LLM inputs, sampling outputs for each perturbation, and incorporating an aggregation module that generalizes the sampling uncertainty approach for text generation tasks. Through extensive experiments on various datasets, we investigated different perturbation and aggrega",
    "link": "https://arxiv.org/abs/2403.02509",
    "context": "Title: SPUQ: Perturbation-Based Uncertainty Quantification for Large Language Models\nAbstract: arXiv:2403.02509v1 Announce Type: cross  Abstract: In recent years, large language models (LLMs) have become increasingly prevalent, offering remarkable text generation capabilities. However, a pressing challenge is their tendency to make confidently wrong predictions, highlighting the critical need for uncertainty quantification (UQ) in LLMs. While previous works have mainly focused on addressing aleatoric uncertainty, the full spectrum of uncertainties, including epistemic, remains inadequately explored. Motivated by this gap, we introduce a novel UQ method, sampling with perturbation for UQ (SPUQ), designed to tackle both aleatoric and epistemic uncertainties. The method entails generating a set of perturbations for LLM inputs, sampling outputs for each perturbation, and incorporating an aggregation module that generalizes the sampling uncertainty approach for text generation tasks. Through extensive experiments on various datasets, we investigated different perturbation and aggrega",
    "path": "papers/24/03/2403.02509.json",
    "total_tokens": 834,
    "translated_title": "SPUQ：基于扰动的大型语言模型不确定性量化",
    "translated_abstract": "近年来，大型语言模型（LLMs）越来越普遍，提供了出色的文本生成能力。然而，它们倾向于做出自信错误的预测，突显了在LLMs中进行不确定性量化（UQ）的重要性。尽管先前的工作主要集中在解决现象性不确定性，但对包括认知性在内的不确定性的全谱尚未充分探索。受到这一差距的启发，我们引入了一种新颖的UQ方法，即适用于UQ的扰动采样（SPUQ），旨在应对现象性和认知性不确定性。该方法涉及为LLM输入生成一组扰动，对每个扰动进行输出采样，并结合一个聚合模块，该聚合模块概括了用于文本生成任务的采样不确定性方法。通过对各种数据集上的广泛实验，我们研究了不同的扰动和聚合方式。",
    "tldr": "SPUQ是一种新颖的基于扰动的大型语言模型不确定性量化方法，旨在同时处理现象性和认知性不确定性",
    "en_tdlr": "SPUQ is a novel perturbation-based uncertainty quantification method for large language models, designed to tackle both aleatoric and epistemic uncertainties."
}