{
    "title": "Calibrating Multi-modal Representations: A Pursuit of Group Robustness without Annotations",
    "abstract": "arXiv:2403.07241v1 Announce Type: cross  Abstract: Fine-tuning pre-trained vision-language models, like CLIP, has yielded success on diverse downstream tasks. However, several pain points persist for this paradigm: (i) directly tuning entire pre-trained models becomes both time-intensive and computationally costly. Additionally, these tuned models tend to become highly specialized, limiting their practicality for real-world deployment; (ii) recent studies indicate that pre-trained vision-language classifiers may overly depend on spurious features -- patterns that correlate with the target in training data, but are not related to the true labeling function; and (iii) existing studies on mitigating the reliance on spurious features, largely based on the assumption that we can identify such features, does not provide definitive assurance for real-world applications. As a piloting study, this work focuses on exploring mitigating the reliance on spurious features for CLIP without using any ",
    "link": "https://arxiv.org/abs/2403.07241",
    "context": "Title: Calibrating Multi-modal Representations: A Pursuit of Group Robustness without Annotations\nAbstract: arXiv:2403.07241v1 Announce Type: cross  Abstract: Fine-tuning pre-trained vision-language models, like CLIP, has yielded success on diverse downstream tasks. However, several pain points persist for this paradigm: (i) directly tuning entire pre-trained models becomes both time-intensive and computationally costly. Additionally, these tuned models tend to become highly specialized, limiting their practicality for real-world deployment; (ii) recent studies indicate that pre-trained vision-language classifiers may overly depend on spurious features -- patterns that correlate with the target in training data, but are not related to the true labeling function; and (iii) existing studies on mitigating the reliance on spurious features, largely based on the assumption that we can identify such features, does not provide definitive assurance for real-world applications. As a piloting study, this work focuses on exploring mitigating the reliance on spurious features for CLIP without using any ",
    "path": "papers/24/03/2403.07241.json",
    "total_tokens": 883,
    "translated_title": "校准多模态表示：在不使用注释的情况下追求群体鲁棒性",
    "translated_abstract": "arXiv:2403.07241v1 公告类型：交叉 摘要：微调预训练的视觉-语言模型，如CLIP，在多样的下游任务上取得成功。然而，这种范式存在一些痛点：(i) 直接微调整个预训练模型既时间密集又计算成本高。此外，这些调整后的模型往往变得高度专业化，限制了它们在实际部署中的实用性；(ii) 最近的研究表明，预训练的视觉-语言分类器可能过度依赖于伪特征-在训练数据中与目标相关的模式，但与真实标签函数无关；(iii) 现有关于减少对伪特征依赖的研究，主要基于我们能够识别这些特征的假设，对于实际应用并没有提供确切的保证。作为一项试点研究，本工作侧重于探索在不使用任何注释的情况下减少CLIP对伪特征依赖的方法。",
    "tldr": "本文旨在探索如何减少CLIP对伪特征的依赖，从而提高群体鲁棒性，而无需使用注释数据。",
    "en_tdlr": "This paper aims to explore methods for reducing CLIP's reliance on spurious features to enhance group robustness without the need for annotated data."
}