{
    "title": "KnFu: Effective Knowledge Fusion",
    "abstract": "arXiv:2403.11892v1 Announce Type: new  Abstract: Federated Learning (FL) has emerged as a prominent alternative to the traditional centralized learning approach. Generally speaking, FL is a decentralized approach that allows for collaborative training of Machine Learning (ML) models across multiple local nodes, ensuring data privacy and security while leveraging diverse datasets. Conventional FL, however, is susceptible to gradient inversion attacks, restrictively enforces a uniform architecture on local models, and suffers from model heterogeneity (model drift) due to non-IID local datasets. To mitigate some of these challenges, the new paradigm of Federated Knowledge Distillation (FKD) has emerged. FDK is developed based on the concept of Knowledge Distillation (KD), which involves extraction and transfer of a large and well-trained teacher model's knowledge to lightweight student models. FKD, however, still faces the model drift issue. Intuitively speaking, not all knowledge is univ",
    "link": "https://arxiv.org/abs/2403.11892",
    "context": "Title: KnFu: Effective Knowledge Fusion\nAbstract: arXiv:2403.11892v1 Announce Type: new  Abstract: Federated Learning (FL) has emerged as a prominent alternative to the traditional centralized learning approach. Generally speaking, FL is a decentralized approach that allows for collaborative training of Machine Learning (ML) models across multiple local nodes, ensuring data privacy and security while leveraging diverse datasets. Conventional FL, however, is susceptible to gradient inversion attacks, restrictively enforces a uniform architecture on local models, and suffers from model heterogeneity (model drift) due to non-IID local datasets. To mitigate some of these challenges, the new paradigm of Federated Knowledge Distillation (FKD) has emerged. FDK is developed based on the concept of Knowledge Distillation (KD), which involves extraction and transfer of a large and well-trained teacher model's knowledge to lightweight student models. FKD, however, still faces the model drift issue. Intuitively speaking, not all knowledge is univ",
    "path": "papers/24/03/2403.11892.json",
    "total_tokens": 888,
    "translated_title": "KnFu: 有效的知识融合",
    "translated_abstract": "arXiv:2403.11892v1 公告类型：新  摘要：联邦学习（FL）已经成为传统集中学习方法的一种突出替代方案。一般来说，FL是一种去中心化方法，允许跨多个本地节点协作训练机器学习（ML）模型，确保数据隐私和安全，同时利用各种多样化的数据集。然而，传统的FL容易受到梯度反转攻击，强制在本地模型上实施统一架构，并且由于非独立同分布的本地数据集，导致模型异构性（模型漂移）。为了减轻其中一些挑战，出现了联邦知识蒸馏（FKD）的新范式。FKD基于知识蒸馏（KD）的概念开发，其中涉及从经验丰富并训练良好的大型教师模型中提取和转移知识到轻量级学生模型。然而，FKD仍然面临模型漂移问题。直观地说，并不是所有的知识都是统一的。",
    "tldr": "FKD是一个新的联邦学习范式，基于知识蒸馏概念，尝试在解决FL中的模型异构性和梯度反转攻击方面取得进展。",
    "en_tdlr": "FKD is a new paradigm of federated learning based on the concept of knowledge distillation, aiming to make progress in addressing model heterogeneity and gradient inversion attacks in FL."
}