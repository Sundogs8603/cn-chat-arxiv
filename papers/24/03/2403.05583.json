{
    "title": "A Cross-Modal Approach to Silent Speech with LLM-Enhanced Recognition",
    "abstract": "arXiv:2403.05583v1 Announce Type: cross  Abstract: Silent Speech Interfaces (SSIs) offer a noninvasive alternative to brain-computer interfaces for soundless verbal communication. We introduce Multimodal Orofacial Neural Audio (MONA), a system that leverages cross-modal alignment through novel loss functions--cross-contrast (crossCon) and supervised temporal contrast (supTcon)--to train a multimodal model with a shared latent representation. This architecture enables the use of audio-only datasets like LibriSpeech to improve silent speech recognition. Additionally, our introduction of Large Language Model (LLM) Integrated Scoring Adjustment (LISA) significantly improves recognition accuracy. Together, MONA LISA reduces the state-of-the-art word error rate (WER) from 28.8% to 12.2% in the Gaddy (2020) benchmark dataset for silent speech on an open vocabulary. For vocal EMG recordings, our method improves the state-of-the-art from 23.3% to 3.7% WER. In the Brain-to-Text 2024 competition,",
    "link": "https://arxiv.org/abs/2403.05583",
    "context": "Title: A Cross-Modal Approach to Silent Speech with LLM-Enhanced Recognition\nAbstract: arXiv:2403.05583v1 Announce Type: cross  Abstract: Silent Speech Interfaces (SSIs) offer a noninvasive alternative to brain-computer interfaces for soundless verbal communication. We introduce Multimodal Orofacial Neural Audio (MONA), a system that leverages cross-modal alignment through novel loss functions--cross-contrast (crossCon) and supervised temporal contrast (supTcon)--to train a multimodal model with a shared latent representation. This architecture enables the use of audio-only datasets like LibriSpeech to improve silent speech recognition. Additionally, our introduction of Large Language Model (LLM) Integrated Scoring Adjustment (LISA) significantly improves recognition accuracy. Together, MONA LISA reduces the state-of-the-art word error rate (WER) from 28.8% to 12.2% in the Gaddy (2020) benchmark dataset for silent speech on an open vocabulary. For vocal EMG recordings, our method improves the state-of-the-art from 23.3% to 3.7% WER. In the Brain-to-Text 2024 competition,",
    "path": "papers/24/03/2403.05583.json",
    "total_tokens": 912,
    "translated_title": "一种基于LLM增强识别的跨模态静默语音方法",
    "translated_abstract": "静默语音界面（SSIs）为无声口头交流提供了一种非侵入式的替代方案，相较于脑机接口。我们引入了Multimodal Orofacial Neural Audio（MONA），该系统通过新颖的损失函数——跨对比（crossCon）和监督时间对比（supTcon）利用跨模态对齐来训练具有共享潜在表示的多模态模型。这种架构使得可以利用类似LibriSpeech的仅音频数据集来改善静默语音识别。此外，我们引入的大语言模型（LLM）整合评分调整（LISA）显著提高了识别准确性。综合而言，MONA LISA在Gaddy（2020年）静默语音基准数据集上将词错误率（WER）从28.8%降至12.2%，并且在开放词汇表上进行了静默语音的改进。对于声音EMG记录，我们的方法将最先进的识别率从23.3%提高到3.7% WER。",
    "tldr": "通过引入跨模态对齐和大语言模型，提出了一种Multimodal Orofacial Neural Audio系统，成功减少静默语音识别中的词错误率。",
    "en_tdlr": "A Multimodal Orofacial Neural Audio (MONA) system is proposed, leveraging cross-modal alignment and Large Language Model (LLM) to significantly reduce word error rate in silent speech recognition."
}