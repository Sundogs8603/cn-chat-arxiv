{
    "title": "Enhancing Image Caption Generation Using Reinforcement Learning with Human Feedback",
    "abstract": "arXiv:2403.06735v1 Announce Type: cross  Abstract: Research on generative models to produce human-aligned / human-preferred outputs has seen significant recent contributions. Between text and image-generative models, we narrowed our focus to text-based generative models, particularly to produce captions for images that align with human preferences. In this research, we explored a potential method to amplify the performance of the Deep Neural Network Model to generate captions that are preferred by humans. This was achieved by integrating Supervised Learning and Reinforcement Learning with Human Feedback (RLHF) using the Flickr8k dataset. Also, a novel loss function that is capable of optimizing the model based on human feedback is introduced. In this paper, we provide a concise sketch of our approach and results, hoping to contribute to the ongoing advances in the field of human-aligned generative AI models.",
    "link": "https://arxiv.org/abs/2403.06735",
    "context": "Title: Enhancing Image Caption Generation Using Reinforcement Learning with Human Feedback\nAbstract: arXiv:2403.06735v1 Announce Type: cross  Abstract: Research on generative models to produce human-aligned / human-preferred outputs has seen significant recent contributions. Between text and image-generative models, we narrowed our focus to text-based generative models, particularly to produce captions for images that align with human preferences. In this research, we explored a potential method to amplify the performance of the Deep Neural Network Model to generate captions that are preferred by humans. This was achieved by integrating Supervised Learning and Reinforcement Learning with Human Feedback (RLHF) using the Flickr8k dataset. Also, a novel loss function that is capable of optimizing the model based on human feedback is introduced. In this paper, we provide a concise sketch of our approach and results, hoping to contribute to the ongoing advances in the field of human-aligned generative AI models.",
    "path": "papers/24/03/2403.06735.json",
    "total_tokens": 809,
    "translated_title": "利用强化学习和人类反馈增强图像标题生成",
    "translated_abstract": "最近关于生成模型以产生符合人类偏好输出的研究取得了显著进展。在文本和图像生成模型之间，我们将重点放在文本生成模型上，特别是为图像生成符合人类偏好的标题。在这项研究中，我们探讨了一种潜在的方法，通过集成监督学习、强化学习和人类反馈（RLHF）来提高深度神经网络模型的性能，生成人类喜欢的标题。我们使用Flickr8k数据集，并引入了一种能够根据人类反馈优化模型的新型损失函数。在本文中，我们提供了我们的方法和结果的简明概述，希望为人类偏好生成AI模型领域的持续进展做出贡献。",
    "tldr": "通过整合监督学习和强化学习与人类反馈（RLHF）并引入新型损失函数，本研究提出增强深度神经网络模型性能的方法，以生成符合人类偏好的图像标题。",
    "en_tdlr": "By integrating supervised learning and reinforcement learning with human feedback (RLHF) and introducing a novel loss function, this research proposes a method to enhance the performance of deep neural network models for generating image captions aligned with human preferences."
}