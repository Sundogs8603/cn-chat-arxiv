{
    "title": "Addressing Social Misattributions of Large Language Models: An HCXAI-based Approach",
    "abstract": "arXiv:2403.17873v1 Announce Type: new  Abstract: Human-centered explainable AI (HCXAI) advocates for the integration of social aspects into AI explanations. Central to the HCXAI discourse is the Social Transparency (ST) framework, which aims to make the socio-organizational context of AI systems accessible to their users. In this work, we suggest extending the ST framework to address the risks of social misattributions in Large Language Models (LLMs), particularly in sensitive areas like mental health. In fact LLMs, which are remarkably capable of simulating roles and personas, may lead to mismatches between designers' intentions and users' perceptions of social attributes, risking to promote emotional manipulation and dangerous behaviors, cases of epistemic injustice, and unwarranted trust. To address these issues, we propose enhancing the ST framework with a fifth 'W-question' to clarify the specific social attributions assigned to LLMs by its designers and users. This addition aims ",
    "link": "https://arxiv.org/abs/2403.17873",
    "context": "Title: Addressing Social Misattributions of Large Language Models: An HCXAI-based Approach\nAbstract: arXiv:2403.17873v1 Announce Type: new  Abstract: Human-centered explainable AI (HCXAI) advocates for the integration of social aspects into AI explanations. Central to the HCXAI discourse is the Social Transparency (ST) framework, which aims to make the socio-organizational context of AI systems accessible to their users. In this work, we suggest extending the ST framework to address the risks of social misattributions in Large Language Models (LLMs), particularly in sensitive areas like mental health. In fact LLMs, which are remarkably capable of simulating roles and personas, may lead to mismatches between designers' intentions and users' perceptions of social attributes, risking to promote emotional manipulation and dangerous behaviors, cases of epistemic injustice, and unwarranted trust. To address these issues, we propose enhancing the ST framework with a fifth 'W-question' to clarify the specific social attributions assigned to LLMs by its designers and users. This addition aims ",
    "path": "papers/24/03/2403.17873.json",
    "total_tokens": 901,
    "translated_title": "处理大型语言模型的社会误归属性：一种基于HCXAI的方法",
    "translated_abstract": "人类中心可解释AI（HCXAI）倡导将社会因素融入AI解释中。HCXAI讨论的核心是社会透明（ST）框架，旨在使AI系统的社会组织背景对其用户可见。在这项工作中，我们建议将ST框架扩展到解决大型语言模型（LLMs）中的社会误归风险，特别是在敏感领域如心理健康方面。事实上，LLMs在模拟角色和人物方面具有显著能力，可能导致设计者意图与用户对社会属性的感知之间的不匹配，从而可能促成情感操纵和危险行为，出现认识上的不公正情况以及不合理的信任。为了解决这些问题，我们提出通过向ST框架添加第五个“W问题”来加强该框架，以澄清设计者和用户对LLMs赋予的具体社会属性。",
    "tldr": "本研究通过将ST框架扩展到大型语言模型，特别是在心理健康等敏感领域，以解决社会误归属性的风险，并提出通过增加第五个“W问题”的方式来增强该框架。",
    "en_tdlr": "This study extends the ST framework to large language models, especially in sensitive areas like mental health, to address the risks of social misattributions, proposing to enhance the framework by adding a fifth 'W-question'."
}