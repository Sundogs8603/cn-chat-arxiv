{
    "title": "MAGID: An Automated Pipeline for Generating Synthetic Multi-modal Datasets",
    "abstract": "arXiv:2403.03194v1 Announce Type: new  Abstract: Development of multimodal interactive systems is hindered by the lack of rich, multimodal (text, images) conversational data, which is needed in large quantities for LLMs. Previous approaches augment textual dialogues with retrieved images, posing privacy, diversity, and quality constraints. In this work, we introduce \\textbf{M}ultimodal \\textbf{A}ugmented \\textbf{G}enerative \\textbf{I}mages \\textbf{D}ialogues (MAGID), a framework to augment text-only dialogues with diverse and high-quality images. Subsequently, a diffusion model is applied to craft corresponding images, ensuring alignment with the identified text. Finally, MAGID incorporates an innovative feedback loop between an image description generation module (textual LLM) and image quality modules (addressing aesthetics, image-text matching, and safety), that work in tandem to generate high-quality and multi-modal dialogues. We compare MAGID to other SOTA baselines on three dialo",
    "link": "https://arxiv.org/abs/2403.03194",
    "context": "Title: MAGID: An Automated Pipeline for Generating Synthetic Multi-modal Datasets\nAbstract: arXiv:2403.03194v1 Announce Type: new  Abstract: Development of multimodal interactive systems is hindered by the lack of rich, multimodal (text, images) conversational data, which is needed in large quantities for LLMs. Previous approaches augment textual dialogues with retrieved images, posing privacy, diversity, and quality constraints. In this work, we introduce \\textbf{M}ultimodal \\textbf{A}ugmented \\textbf{G}enerative \\textbf{I}mages \\textbf{D}ialogues (MAGID), a framework to augment text-only dialogues with diverse and high-quality images. Subsequently, a diffusion model is applied to craft corresponding images, ensuring alignment with the identified text. Finally, MAGID incorporates an innovative feedback loop between an image description generation module (textual LLM) and image quality modules (addressing aesthetics, image-text matching, and safety), that work in tandem to generate high-quality and multi-modal dialogues. We compare MAGID to other SOTA baselines on three dialo",
    "path": "papers/24/03/2403.03194.json",
    "total_tokens": 898,
    "translated_title": "MAGID：用于生成合成多模态数据集的自动化流水线",
    "translated_abstract": "多模态交互系统的发展受限于缺乏丰富的多模态（文本、图像）对话数据，这些数据对LLMs而言需要大量。先前的方法通过检索图像来增强文本对话，存在隐私、多样性和质量等约束。在这项工作中，我们介绍了MAGID（\\textbf{M}ultimodal \\textbf{A}ugmented \\textbf{G}enerative \\textbf{I}mages \\textbf{D}ialogues）, 一个框架，用于用各种多样性和高质量图像增强仅限于文本的对话。随后，应用扩散模型来创建相应的图像，确保与确定的文本保持一致。最后，MAGID包含了一个创新性的反馈循环，介于图像描述生成模块（文本LLM）和图像质量模块（解决美学、图像文本匹配和安全性），二者协作生成高质量和多模态对话。我们将MAGID与其他SOTA基线在三个对话方面进行了比较。",
    "tldr": "MAGID是一个用于将仅文本对话增强为多样性和高质量图像的框架，通过创新的反馈循环生成高质量和多模态对话。",
    "en_tdlr": "MAGID is a framework for augmenting text-only dialogues with diverse and high-quality images, generating high-quality and multi-modal dialogues through an innovative feedback loop."
}