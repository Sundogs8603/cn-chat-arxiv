{
    "title": "Do Not Worry if You Do Not Have Data: Building Pretrained Language Models Using Translationese",
    "abstract": "arXiv:2403.13638v1 Announce Type: new  Abstract: In this paper, we explore the utility of \\textit{Translationese} as synthetic data created using machine translation for pre-training language models (LMs). Pre-training requires vast amounts of monolingual data, which is mostly unavailable for languages other than English. Recently, there has been a growing interest in using synthetic data to address this data scarcity. We take the case of English and Indic languages and translate web-crawled monolingual documents (clean) into the target language. Then, we train language models containing 28M and 85M parameters on this translationese data (synthetic). We show that their performance on downstream natural language understanding and generative tasks is only 3.56\\% poorer on NLU tasks and 1.51\\% on NLG tasks than LMs pre-trained on clean data. Further, we propose the use of lightweight \\textit{TinyLMs} pre-trained on clean data to filter synthetic data efficiently which significantly improv",
    "link": "https://arxiv.org/abs/2403.13638",
    "context": "Title: Do Not Worry if You Do Not Have Data: Building Pretrained Language Models Using Translationese\nAbstract: arXiv:2403.13638v1 Announce Type: new  Abstract: In this paper, we explore the utility of \\textit{Translationese} as synthetic data created using machine translation for pre-training language models (LMs). Pre-training requires vast amounts of monolingual data, which is mostly unavailable for languages other than English. Recently, there has been a growing interest in using synthetic data to address this data scarcity. We take the case of English and Indic languages and translate web-crawled monolingual documents (clean) into the target language. Then, we train language models containing 28M and 85M parameters on this translationese data (synthetic). We show that their performance on downstream natural language understanding and generative tasks is only 3.56\\% poorer on NLU tasks and 1.51\\% on NLG tasks than LMs pre-trained on clean data. Further, we propose the use of lightweight \\textit{TinyLMs} pre-trained on clean data to filter synthetic data efficiently which significantly improv",
    "path": "papers/24/03/2403.13638.json",
    "total_tokens": 954,
    "translated_title": "不必担心如果您没有数据：利用Translationese构建预训练语言模型",
    "translated_abstract": "在本文中，我们探讨了将机器翻译创建的合成数据Translationese用作预训练语言模型（LMs）的实用性。预训练需要大量的单语数据，对于英语以外的语言，这些数据大部分是不可用的。近年来，人们越来越关注使用合成数据来解决这种数据稀缺性问题。我们以英语和Indic语言为例，将网络抓取的单语文档（干净的）翻译成目标语言。然后，我们在这些Translationese数据（合成数据）上训练包含28M和85M参数的语言模型。我们展示了它们在下游自然语言理解和生成任务中的性能与在干净数据上预训练的LMs相比，NLU任务的性能仅差3.56％，NLG任务的差异为1.51％。此外，我们提出了使用在干净数据上预训练的轻量级TinyLMs来高效过滤合成数据的方法，这显著提高了性能。",
    "tldr": "本文探讨了使用Translationese合成数据作为预训练语言模型的实用性，展示了在英语以外的语言中使用机器翻译创建的合成数据进行LMs预训练的有效性，并提出了通过使用轻量级TinyLMs预训练来过滤合成数据的方法。",
    "en_tdlr": "This paper explores the utility of using Translationese as synthetic data for pre-training language models, demonstrates the effectiveness of pre-training LM on non-English languages using machine-translated synthetic data, and proposes a method to filter synthetic data efficiently using lightweight TinyLMs pre-training."
}