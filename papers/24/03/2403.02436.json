{
    "title": "How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider Transformer Models",
    "abstract": "arXiv:2403.02436v1 Announce Type: new  Abstract: Pre-trained language models have been proven to possess strong base capabilities, which not only excel in in-distribution language modeling but also show powerful abilities in out-of-distribution language modeling, transfer learning and few-shot learning. Unlike existing work focusing on the influence of scale on base capabilities, our work examines the influence of architecture on those. Specifically, our concern is: How does architecture influence the base capabilities of pre-trained language models? In this work, we attempt to explain and reverse the decline in base capabilities caused by the architecture of FFN-Wider Transformers, seeking to provide some insights. Through analysis, we found the contribution ratio of Multi-Head Attention (a combination function) to pre-trained language modeling is a key factor affecting base capabilities. FFN-Wider Transformers reduce the contribution ratio of this combination function, leading to a d",
    "link": "https://arxiv.org/abs/2403.02436",
    "context": "Title: How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider Transformer Models\nAbstract: arXiv:2403.02436v1 Announce Type: new  Abstract: Pre-trained language models have been proven to possess strong base capabilities, which not only excel in in-distribution language modeling but also show powerful abilities in out-of-distribution language modeling, transfer learning and few-shot learning. Unlike existing work focusing on the influence of scale on base capabilities, our work examines the influence of architecture on those. Specifically, our concern is: How does architecture influence the base capabilities of pre-trained language models? In this work, we attempt to explain and reverse the decline in base capabilities caused by the architecture of FFN-Wider Transformers, seeking to provide some insights. Through analysis, we found the contribution ratio of Multi-Head Attention (a combination function) to pre-trained language modeling is a key factor affecting base capabilities. FFN-Wider Transformers reduce the contribution ratio of this combination function, leading to a d",
    "path": "papers/24/03/2403.02436.json",
    "total_tokens": 904,
    "translated_title": "建筑如何影响预训练语言模型的基本能力？基于FFN-Wider变压器模型的案例研究",
    "translated_abstract": "预训练语言模型已被证明具有强大的基本能力，不仅在分布式语言建模方面表现出色，而且在超出分布式语言建模、迁移学习和少样本学习方面也展现出强大的能力。与现有研究侧重于规模对基本能力的影响不同，我们的工作将重点放在了架构对其影响。具体地，我们关心的是：建筑如何影响预训练语言模型的基本能力？在这项工作中，我们试图解释并逆转FFN-Wider变压器的架构导致基本能力下降的情况，力求提供一些见解。通过分析，我们发现多头注意力（一种组合函数）对预训练语言建模的贡献比是影响基本能力的关键因素。FFN-Wider变压器减少了这种组合函数的贡献比，导致一种",
    "tldr": "本研究探讨了建筑如何影响预训练语言模型的基本能力，发现了FFN-Wider变压器模型降低了多头注意力的贡献比，从而导致基本能力的下降。"
}