{
    "title": "A Generalized Acquisition Function for Preference-based Reward Learning",
    "abstract": "arXiv:2403.06003v1 Announce Type: cross  Abstract: Preference-based reward learning is a popular technique for teaching robots and autonomous systems how a human user wants them to perform a task. Previous works have shown that actively synthesizing preference queries to maximize information gain about the reward function parameters improves data efficiency. The information gain criterion focuses on precisely identifying all parameters of the reward function. This can potentially be wasteful as many parameters may result in the same reward, and many rewards may result in the same behavior in the downstream tasks. Instead, we show that it is possible to optimize for learning the reward function up to a behavioral equivalence class, such as inducing the same ranking over behaviors, distribution over choices, or other related definitions of what makes two rewards similar. We introduce a tractable framework that can capture such definitions of similarity. Our experiments in a synthetic env",
    "link": "https://arxiv.org/abs/2403.06003",
    "context": "Title: A Generalized Acquisition Function for Preference-based Reward Learning\nAbstract: arXiv:2403.06003v1 Announce Type: cross  Abstract: Preference-based reward learning is a popular technique for teaching robots and autonomous systems how a human user wants them to perform a task. Previous works have shown that actively synthesizing preference queries to maximize information gain about the reward function parameters improves data efficiency. The information gain criterion focuses on precisely identifying all parameters of the reward function. This can potentially be wasteful as many parameters may result in the same reward, and many rewards may result in the same behavior in the downstream tasks. Instead, we show that it is possible to optimize for learning the reward function up to a behavioral equivalence class, such as inducing the same ranking over behaviors, distribution over choices, or other related definitions of what makes two rewards similar. We introduce a tractable framework that can capture such definitions of similarity. Our experiments in a synthetic env",
    "path": "papers/24/03/2403.06003.json",
    "total_tokens": 780,
    "translated_title": "一种推广的用于基于偏好的奖励学习的收益函数",
    "translated_abstract": "基于偏好的奖励学习是一种用于教导机器人和自主系统如何执行任务的流行技术。 先前的研究表明，积极合成偏好查询以最大化关于奖励函数参数的信息增益可以提高数据效率。 信息增益标准侧重于精确识别奖励函数的所有参数。 这可能是低效的，因为许多参数可能导致相同的奖励，并且许多奖励可能导致下游任务中的相同行为。 相反，我们展示了可以优化学习奖励函数直到行为等效类，例如诱导出相同的行为排序，选择分布，或其他相关定义使得两个奖励看起来相似。 我们引入了一个可以捕捉这种相似性定义的可处理框架。 我们在一个.synthetic env的实验中展示了这一点",
    "tldr": "通过引入一种可以捕捉奖励函数相似性定义的框架，优化了奖励函数的学习效率。",
    "en_tdlr": "Optimized learning efficiency of the reward function by introducing a framework that captures the similarity definitions of rewards."
}