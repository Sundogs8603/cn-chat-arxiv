{
    "title": "Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs",
    "abstract": "arXiv:2403.20041v1 Announce Type: new  Abstract: The Large Language Model (LLM) is widely employed for tasks such as intelligent assistants, text summarization, translation, and multi-modality on mobile phones. However, the current methods for on-device LLM deployment maintain slow inference speed, which causes poor user experience. To facilitate high-efficiency LLM deployment on device GPUs, we propose four optimization techniques: (a) a symbolic expression-based approach to support dynamic shape model inference; (b) operator optimizations and execution priority setting to enhance inference speed and reduce phone lagging; (c) an FP4 quantization method termed M0E4 to reduce dequantization overhead; (d) a sub-tensor-based technique to eliminate the need for copying KV cache after LLM inference. Furthermore, we implement these methods in our mobile inference engine, Transformer-Lite, which is compatible with both Qualcomm and MTK processors. We evaluated Transformer-Lite's performance u",
    "link": "https://arxiv.org/abs/2403.20041",
    "context": "Title: Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs\nAbstract: arXiv:2403.20041v1 Announce Type: new  Abstract: The Large Language Model (LLM) is widely employed for tasks such as intelligent assistants, text summarization, translation, and multi-modality on mobile phones. However, the current methods for on-device LLM deployment maintain slow inference speed, which causes poor user experience. To facilitate high-efficiency LLM deployment on device GPUs, we propose four optimization techniques: (a) a symbolic expression-based approach to support dynamic shape model inference; (b) operator optimizations and execution priority setting to enhance inference speed and reduce phone lagging; (c) an FP4 quantization method termed M0E4 to reduce dequantization overhead; (d) a sub-tensor-based technique to eliminate the need for copying KV cache after LLM inference. Furthermore, we implement these methods in our mobile inference engine, Transformer-Lite, which is compatible with both Qualcomm and MTK processors. We evaluated Transformer-Lite's performance u",
    "path": "papers/24/03/2403.20041.json",
    "total_tokens": 852,
    "translated_title": "Transformer-Lite: 在手机GPU上高效部署大型语言模型",
    "translated_abstract": "大型语言模型（LLM）被广泛应用于智能助手、文本摘要、翻译和手机上的多模任务。然而，当前的设备上LLM部署方法速度较慢，导致用户体验不佳。为了实现在设备GPU上高效部署LLM，我们提出了四种优化技术：（a）基于符号表达的方法支持动态形状模型推断；（b）操作优化和执行优先级设置以提高推断速度并减少手机滞后；（c）一种名为M0E4的FP4量化方法以减少去量化开销；（d）一种基于子张量的技术来在LLM推断后消除复制KV缓存的需要。此外，我们在我们的移动推断引擎Transformer-Lite中实现了这些方法，该引擎与高通和MTK处理器兼容。我们评估了Transformer-Lite的性能。",
    "tldr": "提出了四种优化技术来在手机GPU上高效部署大型语言模型，并通过实现在移动推断引擎Transformer-Lite中，提高了推断速度和降低了手机滞后，从而改善用户体验。",
    "en_tdlr": "Proposed four optimization techniques for efficient deployment of large language models on mobile phone GPUs, implemented in the mobile inference engine Transformer-Lite to improve inference speed and reduce phone lagging, enhancing user experience."
}