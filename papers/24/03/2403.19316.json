{
    "title": "Hypergraph-based Multi-View Action Recognition using Event Cameras",
    "abstract": "arXiv:2403.19316v1 Announce Type: cross  Abstract: Action recognition from video data forms a cornerstone with wide-ranging applications. Single-view action recognition faces limitations due to its reliance on a single viewpoint. In contrast, multi-view approaches capture complementary information from various viewpoints for improved accuracy. Recently, event cameras have emerged as innovative bio-inspired sensors, leading to advancements in event-based action recognition. However, existing works predominantly focus on single-view scenarios, leaving a gap in multi-view event data exploitation, particularly in challenges like information deficit and semantic misalignment. To bridge this gap, we introduce HyperMV, a multi-view event-based action recognition framework. HyperMV converts discrete event data into frame-like representations and extracts view-related features using a shared convolutional network. By treating segments as vertices and constructing hyperedges using rule-based and",
    "link": "https://arxiv.org/abs/2403.19316",
    "context": "Title: Hypergraph-based Multi-View Action Recognition using Event Cameras\nAbstract: arXiv:2403.19316v1 Announce Type: cross  Abstract: Action recognition from video data forms a cornerstone with wide-ranging applications. Single-view action recognition faces limitations due to its reliance on a single viewpoint. In contrast, multi-view approaches capture complementary information from various viewpoints for improved accuracy. Recently, event cameras have emerged as innovative bio-inspired sensors, leading to advancements in event-based action recognition. However, existing works predominantly focus on single-view scenarios, leaving a gap in multi-view event data exploitation, particularly in challenges like information deficit and semantic misalignment. To bridge this gap, we introduce HyperMV, a multi-view event-based action recognition framework. HyperMV converts discrete event data into frame-like representations and extracts view-related features using a shared convolutional network. By treating segments as vertices and constructing hyperedges using rule-based and",
    "path": "papers/24/03/2403.19316.json",
    "total_tokens": 802,
    "translated_title": "基于超图的多视角事件相机动作识别",
    "translated_abstract": "视频数据的动作识别是具有广泛应用的基石。单视角动作识别由于依赖单一视角而面临限制。相比之下，多视角方法从不同视角捕获互补信息以提高准确性。最近，事件相机作为创新的仿生传感器崭露头角，为基于事件的动作识别带来了进展。然而，现有作品主要关注单一视角场景，在多视角事件数据利用方面存在空白，特别是在信息不足和语义错配等挑战方面。为了弥补这一空白，我们引入了HyperMV，这是一个多视角基于事件的动作识别框架。HyperMV将离散事件数据转换成类似帧的表示，并使用共享的卷积网络提取与视角相关的特征。通过将段视为顶点并使用基于规则的方法构建超边",
    "tldr": "提出了一个名为HyperMV的多视角事件动作识别框架，实现了将离散事件数据转换成帧状表示，并利用共享卷积网络提取视角相关特征。",
    "en_tdlr": "Introduced a multi-view event-based action recognition framework called HyperMV, which converts discrete event data into frame-like representations and extracts view-related features using a shared convolutional network."
}