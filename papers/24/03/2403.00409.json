{
    "title": "Provably Robust DPO: Aligning Language Models with Noisy Feedback",
    "abstract": "arXiv:2403.00409v1 Announce Type: cross  Abstract: Learning from preference-based feedback has recently gained traction as a promising approach to align language models with human interests. While these aligned generative models have demonstrated impressive capabilities across various tasks, their dependence on high-quality human preference data poses a bottleneck in practical applications. Specifically, noisy (incorrect and ambiguous) preference pairs in the dataset might restrict the language models from capturing human intent accurately. While practitioners have recently proposed heuristics to mitigate the effect of noisy preferences, a complete theoretical understanding of their workings remain elusive.   In this work, we aim to bridge this gap by by introducing a general framework for policy optimization in the presence of random preference flips. We focus on the direct preference optimization (DPO) algorithm in particular since it assumes that preferences adhere to the Bradley-Te",
    "link": "https://arxiv.org/abs/2403.00409",
    "context": "Title: Provably Robust DPO: Aligning Language Models with Noisy Feedback\nAbstract: arXiv:2403.00409v1 Announce Type: cross  Abstract: Learning from preference-based feedback has recently gained traction as a promising approach to align language models with human interests. While these aligned generative models have demonstrated impressive capabilities across various tasks, their dependence on high-quality human preference data poses a bottleneck in practical applications. Specifically, noisy (incorrect and ambiguous) preference pairs in the dataset might restrict the language models from capturing human intent accurately. While practitioners have recently proposed heuristics to mitigate the effect of noisy preferences, a complete theoretical understanding of their workings remain elusive.   In this work, we aim to bridge this gap by by introducing a general framework for policy optimization in the presence of random preference flips. We focus on the direct preference optimization (DPO) algorithm in particular since it assumes that preferences adhere to the Bradley-Te",
    "path": "papers/24/03/2403.00409.json",
    "total_tokens": 885,
    "translated_title": "可证明鲁棒的DPO: 用有噪反馈对齐语言模型",
    "translated_abstract": "最近，从基于喜好反馈学习作为一种与人类兴趣对齐的有前景方法已经引起了广泛关注。虽然这些对齐的生成模型在各种任务中展示出令人印象深刻的能力，但它们对高质量人类喜好数据的依赖在实际应用中构成了瓶颈。具体来说，数据集中有噪（不正确和模糊）的偏好对可能会限制语言模型准确捕捉人类意图。虽然从业者最近提出了启发式方法来减轻噪声偏好的影响，但对它们的工作完整理论理解仍然难以捉摸。在这项工作中，我们旨在通过引入一个面向在随机偏好翻转存在的策略优化的通用框架来弥合这一差距。我们特别关注直接偏好优化（DPO）算法，因为它假设偏好遵循 Bradley-Te",
    "tldr": "通过引入面向随机偏好翻转的策略优化通用框架，本研究旨在理解存在嘈杂反馈时的DPO算法，从而解决语言模型对齐人类兴趣中的挑战。",
    "en_tdlr": "This study aims to understand the DPO algorithm in the presence of noisy feedback by introducing a general framework for policy optimization targeting random preference flips, addressing the challenges in aligning language models with human interests."
}