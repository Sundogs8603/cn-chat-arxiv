{
    "title": "Can Generative Models Improve Self-Supervised Representation Learning?",
    "abstract": "arXiv:2403.05966v1 Announce Type: cross  Abstract: The rapid advancement in self-supervised learning (SSL) has highlighted its potential to leverage unlabeled data for learning powerful visual representations. However, existing SSL approaches, particularly those employing different views of the same image, often rely on a limited set of predefined data augmentations. This constrains the diversity and quality of transformations, which leads to sub-optimal representations. In this paper, we introduce a novel framework that enriches the SSL paradigm by utilizing generative models to produce semantically consistent image augmentations. By directly conditioning generative models on a source image representation, our method enables the generation of diverse augmentations while maintaining the semantics of the source image, thus offering a richer set of data for self-supervised learning. Our experimental results demonstrate that our framework significantly enhances the quality of learned visu",
    "link": "https://arxiv.org/abs/2403.05966",
    "context": "Title: Can Generative Models Improve Self-Supervised Representation Learning?\nAbstract: arXiv:2403.05966v1 Announce Type: cross  Abstract: The rapid advancement in self-supervised learning (SSL) has highlighted its potential to leverage unlabeled data for learning powerful visual representations. However, existing SSL approaches, particularly those employing different views of the same image, often rely on a limited set of predefined data augmentations. This constrains the diversity and quality of transformations, which leads to sub-optimal representations. In this paper, we introduce a novel framework that enriches the SSL paradigm by utilizing generative models to produce semantically consistent image augmentations. By directly conditioning generative models on a source image representation, our method enables the generation of diverse augmentations while maintaining the semantics of the source image, thus offering a richer set of data for self-supervised learning. Our experimental results demonstrate that our framework significantly enhances the quality of learned visu",
    "path": "papers/24/03/2403.05966.json",
    "total_tokens": 807,
    "translated_title": "能生成模型改进自监督表示学习吗？",
    "translated_abstract": "自监督学习的快速发展突显了其利用无标签数据学习强大视觉表示的潜力。然而，现有的自监督学习方法，特别是那些利用同一图像的不同视图的方法，通常依赖于一组预定义的数据增强，这限制了变换的多样性和质量，导致表示不够优化。本文引入了一个新颖的框架，通过利用生成模型产生语义一致的图像增强，丰富了自监督学习范式。通过直接在源图像表示上进行条件生成模型，我们的方法能够生成多样的增强，同时保持源图像的语义，为自监督学习提供更丰富的数据集。我们的实验结果表明，我们的框架显著提高了学到的视觉的质量。",
    "tldr": "本文引入了一个新的框架，通过利用生成模型生成语义一致的图像增强，丰富了自监督学习的方法，实验结果表明提高了学到的视觉质量。",
    "en_tdlr": "This paper introduces a novel framework that enriches self-supervised learning by utilizing generative models to generate semantically consistent image augmentations, and the experimental results demonstrate an enhancement in the quality of learned visual representations."
}