{
    "title": "Perceptual Quality-based Model Training under Annotator Label Uncertainty",
    "abstract": "arXiv:2403.10190v1 Announce Type: cross  Abstract: Annotators exhibit disagreement during data labeling, which can be termed as annotator label uncertainty. Annotator label uncertainty manifests in variations of labeling quality. Training with a single low-quality annotation per sample induces model reliability degradations. In this work, we first examine the effects of annotator label uncertainty in terms of the model's generalizability and prediction uncertainty. We observe that the model's generalizability and prediction uncertainty degrade with the presence of low-quality noisy labels. Meanwhile, our evaluation of existing uncertainty estimation algorithms indicates their incapability in response to annotator label uncertainty. To mitigate performance degradation, prior methods show that training models with labels collected from multiple independent annotators can enhance generalizability. However, they require massive annotations. Hence, we introduce a novel perceptual quality-ba",
    "link": "https://arxiv.org/abs/2403.10190",
    "context": "Title: Perceptual Quality-based Model Training under Annotator Label Uncertainty\nAbstract: arXiv:2403.10190v1 Announce Type: cross  Abstract: Annotators exhibit disagreement during data labeling, which can be termed as annotator label uncertainty. Annotator label uncertainty manifests in variations of labeling quality. Training with a single low-quality annotation per sample induces model reliability degradations. In this work, we first examine the effects of annotator label uncertainty in terms of the model's generalizability and prediction uncertainty. We observe that the model's generalizability and prediction uncertainty degrade with the presence of low-quality noisy labels. Meanwhile, our evaluation of existing uncertainty estimation algorithms indicates their incapability in response to annotator label uncertainty. To mitigate performance degradation, prior methods show that training models with labels collected from multiple independent annotators can enhance generalizability. However, they require massive annotations. Hence, we introduce a novel perceptual quality-ba",
    "path": "papers/24/03/2403.10190.json",
    "total_tokens": 868,
    "translated_title": "基于感知质量的模型训练在标注者标签不确定性下",
    "translated_abstract": "arXiv:2403.10190v1 公告类型:跨领域. 标注者在数据标记过程中存在分歧，可称为标注者标签不确定性。标注者标签不确定性表现为标记质量的变化。每个样本使用单个低质量标注进行训练会导致模型可靠性下降。本研究首先考察了标注者标签不确定性对模型的泛化能力和预测不确定性的影响。我们观察到，模型的泛化能力和预测不确定性会随着低质量的嘈杂标签的存在而降低。同时，我们评估现有的不确定性估计算法表明它们无法应对标注者标签不确定性。为了减轻性能下降，先前的方法表明使用来自多个独立标注者收集的标签进行训练可以增强泛化能力。然而，它们需要大量标注。因此，我们引入一种新的基于感知质量的方法",
    "tldr": "标注者标签不确定性影响模型泛化能力和预测不确定性，现有不确定性估计算法无法应对，提出一种基于感知质量的训练方法以缓解性能下降",
    "en_tdlr": "Annotator label uncertainty affects model generalizability and prediction uncertainty, existing uncertainty estimation algorithms are incapable of addressing it, proposing a perceptual quality-based training method to alleviate performance degradation."
}