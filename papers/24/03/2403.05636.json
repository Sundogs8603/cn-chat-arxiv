{
    "title": "Tuning-Free Accountable Intervention for LLM Deployment -- A Metacognitive Approach",
    "abstract": "arXiv:2403.05636v1 Announce Type: new  Abstract: Large Language Models (LLMs) have catalyzed transformative advances across a spectrum of natural language processing tasks through few-shot or zero-shot prompting, bypassing the need for parameter tuning. While convenient, this modus operandi aggravates ``hallucination'' concerns, particularly given the enigmatic ``black-box'' nature behind their gigantic model sizes. Such concerns are exacerbated in high-stakes applications (e.g., healthcare), where unaccountable decision errors can lead to devastating consequences. In contrast, human decision-making relies on nuanced cognitive processes, such as the ability to sense and adaptively correct misjudgments through conceptual understanding. Drawing inspiration from human cognition, we propose an innovative \\textit{metacognitive} approach, dubbed \\textbf{CLEAR}, to equip LLMs with capabilities for self-aware error identification and correction. Our framework facilitates the construction of co",
    "link": "https://arxiv.org/abs/2403.05636",
    "context": "Title: Tuning-Free Accountable Intervention for LLM Deployment -- A Metacognitive Approach\nAbstract: arXiv:2403.05636v1 Announce Type: new  Abstract: Large Language Models (LLMs) have catalyzed transformative advances across a spectrum of natural language processing tasks through few-shot or zero-shot prompting, bypassing the need for parameter tuning. While convenient, this modus operandi aggravates ``hallucination'' concerns, particularly given the enigmatic ``black-box'' nature behind their gigantic model sizes. Such concerns are exacerbated in high-stakes applications (e.g., healthcare), where unaccountable decision errors can lead to devastating consequences. In contrast, human decision-making relies on nuanced cognitive processes, such as the ability to sense and adaptively correct misjudgments through conceptual understanding. Drawing inspiration from human cognition, we propose an innovative \\textit{metacognitive} approach, dubbed \\textbf{CLEAR}, to equip LLMs with capabilities for self-aware error identification and correction. Our framework facilitates the construction of co",
    "path": "papers/24/03/2403.05636.json",
    "total_tokens": 823,
    "translated_title": "无需调参的LLM部署负责干预--一种元认知方法",
    "translated_abstract": "大型语言模型（LLMs）通过少量或零-shot提示在一系列自然语言处理任务中催生了变革性进展，绕过了参数调整的必要性。然而，这种便利的操作方式加剧了“幻觉”问题，特别是考虑到它们庞大模型规模背后的神秘“黑匣子”性质。这些担忧在高风险应用（如医疗保健）中变得更加严重，因为不负责任的决策错误可能导致灾难性后果。相比之下，人类决策依赖于微妙的认知过程，如通过概念理解感知和自适应地纠正错误判断的能力。受人类认知的启发，我们提出了一种创新的元认知方法，称为CLEAR，为LLMs提供自我意识的错误识别和纠正能力。我们的框架有助于构建co",
    "tldr": "提出了一种创新的元认知方法，名为CLEAR，旨在为LLMs提供自我意识的错误识别和纠正能力",
    "en_tdlr": "Proposed an innovative metacognitive approach, named CLEAR, to equip LLMs with capabilities for self-aware error identification and correction."
}