{
    "title": "Benchmarking Zero-Shot Robustness of Multimodal Foundation Models: A Pilot Study",
    "abstract": "arXiv:2403.10499v1 Announce Type: cross  Abstract: Pre-training image representations from the raw text about images enables zero-shot vision transfer to downstream tasks. Through pre-training on millions of samples collected from the internet, multimodal foundation models, such as CLIP, produce state-of-the-art zero-shot results that often reach competitiveness with fully supervised methods without the need for task-specific training. Besides the encouraging performance on classification accuracy, it is reported that these models close the robustness gap by matching the performance of supervised models trained on ImageNet under natural distribution shift. Because robustness is critical to real-world applications, especially safety-critical ones, in this paper, we present a comprehensive evaluation based on a large-scale robustness benchmark covering 7 natural, 3 synthetic distribution shifts, and 11 adversarial attacks. We use CLIP as a pilot study. We show that CLIP leads to a signif",
    "link": "https://arxiv.org/abs/2403.10499",
    "context": "Title: Benchmarking Zero-Shot Robustness of Multimodal Foundation Models: A Pilot Study\nAbstract: arXiv:2403.10499v1 Announce Type: cross  Abstract: Pre-training image representations from the raw text about images enables zero-shot vision transfer to downstream tasks. Through pre-training on millions of samples collected from the internet, multimodal foundation models, such as CLIP, produce state-of-the-art zero-shot results that often reach competitiveness with fully supervised methods without the need for task-specific training. Besides the encouraging performance on classification accuracy, it is reported that these models close the robustness gap by matching the performance of supervised models trained on ImageNet under natural distribution shift. Because robustness is critical to real-world applications, especially safety-critical ones, in this paper, we present a comprehensive evaluation based on a large-scale robustness benchmark covering 7 natural, 3 synthetic distribution shifts, and 11 adversarial attacks. We use CLIP as a pilot study. We show that CLIP leads to a signif",
    "path": "papers/24/03/2403.10499.json",
    "total_tokens": 917,
    "translated_title": "基于多模态基础模型的零样本鲁棒性基准测试：一项试点研究",
    "translated_abstract": "通过从关于图像的原始文本中预训练图像表示，使得零样本视觉传输至下游任务成为可能。通过在互联网上采集的数百万样本上进行预训练，如CLIP之类的多模态基础模型产生了最先进的零样本结果，通常在无需任务特定训练的情况下达到与完全监督方法竞争力相当的水平。除了在分类准确性上表现鼓舞人心之外，报道称这些模型通过在自然分布偏移下与在ImageNet上训练的监督模型的表现相匹配来缩小鲁棒性差距。由于鲁棒性对于现实世界的应用至关重要，特别是对于安全关键的应用，本文提出了基于涵盖7种自然、3种合成分布偏移和11种对抗攻击的大规模鲁棒性基准测试的全面评估。我们以CLIP作为试点研究。我们展示了CLIP导致了显著",
    "tldr": "本研究通过对多模态基础模型CLIP进行大规模鲁棒性基准测试，揭示了其在涵盖自然分布偏移、合成分布偏移和对抗攻击等多个方面的优异表现。",
    "en_tdlr": "This study conducts a large-scale benchmark evaluation on the multimodal foundation model CLIP, revealing its outstanding performance in various aspects including natural distribution shift, synthetic distribution shift, and adversarial attacks."
}