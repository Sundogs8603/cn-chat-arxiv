{
    "title": "Efficient first-order algorithms for large-scale, non-smooth maximum entropy models with application to wildfire science",
    "abstract": "arXiv:2403.06816v1 Announce Type: cross  Abstract: Maximum entropy (Maxent) models are a class of statistical models that use the maximum entropy principle to estimate probability distributions from data. Due to the size of modern data sets, Maxent models need efficient optimization algorithms to scale well for big data applications. State-of-the-art algorithms for Maxent models, however, were not originally designed to handle big data sets; these algorithms either rely on technical devices that may yield unreliable numerical results, scale poorly, or require smoothness assumptions that many practical Maxent models lack. In this paper, we present novel optimization algorithms that overcome the shortcomings of state-of-the-art algorithms for training large-scale, non-smooth Maxent models. Our proposed first-order algorithms leverage the Kullback-Leibler divergence to train large-scale and non-smooth Maxent models efficiently. For Maxent models with discrete probability distribution of $",
    "link": "https://arxiv.org/abs/2403.06816",
    "context": "Title: Efficient first-order algorithms for large-scale, non-smooth maximum entropy models with application to wildfire science\nAbstract: arXiv:2403.06816v1 Announce Type: cross  Abstract: Maximum entropy (Maxent) models are a class of statistical models that use the maximum entropy principle to estimate probability distributions from data. Due to the size of modern data sets, Maxent models need efficient optimization algorithms to scale well for big data applications. State-of-the-art algorithms for Maxent models, however, were not originally designed to handle big data sets; these algorithms either rely on technical devices that may yield unreliable numerical results, scale poorly, or require smoothness assumptions that many practical Maxent models lack. In this paper, we present novel optimization algorithms that overcome the shortcomings of state-of-the-art algorithms for training large-scale, non-smooth Maxent models. Our proposed first-order algorithms leverage the Kullback-Leibler divergence to train large-scale and non-smooth Maxent models efficiently. For Maxent models with discrete probability distribution of $",
    "path": "papers/24/03/2403.06816.json",
    "total_tokens": 708,
    "translated_title": "大规模、非光滑最大熵模型的高效一阶算法及其在野火科学中的应用",
    "translated_abstract": "最大熵（Maxent）模型是一类利用最大熵原理从数据中估计概率分布的统计模型。由于现代数据集的规模，Maxent模型需要高效的优化算法来适应大数据应用。本文提出了一种新颖的优化算法，克服了训练大规模、非光滑Maxent模型的现有算法的缺点。我们提出的一阶算法利用Kullback-Leibler散度，可以高效地训练大规模且非光滑的Maxent模型。",
    "tldr": "提出了一种新颖的优化算法，利用Kullback-Leibler散度训练大规模、非光滑的Maxent模型",
    "en_tdlr": "Novel optimization algorithms leveraging Kullback-Leibler divergence were proposed to efficiently train large-scale, non-smooth Maxent models."
}