{
    "title": "GlossLM: Multilingual Pretraining for Low-Resource Interlinear Glossing",
    "abstract": "arXiv:2403.06399v1 Announce Type: new  Abstract: A key aspect of language documentation is the creation of annotated text in a format such as interlinear glossed text (IGT), which captures fine-grained morphosyntactic analyses in a morpheme-by-morpheme format. Prior work has explored methods to automatically generate IGT in order to reduce the time cost of language analysis. However, many languages (particularly those requiring preservation) lack sufficient IGT data to train effective models, and crosslingual transfer has been proposed as a method to overcome this limitation.   We compile the largest existing corpus of IGT data from a variety of sources, covering over 450k examples across 1.8k languages, to enable research on crosslingual transfer and IGT generation. Then, we pretrain a large multilingual model on a portion of this corpus, and further finetune it to specific languages. Our model is competitive with state-of-the-art methods for segmented data and large monolingual datas",
    "link": "https://arxiv.org/abs/2403.06399",
    "context": "Title: GlossLM: Multilingual Pretraining for Low-Resource Interlinear Glossing\nAbstract: arXiv:2403.06399v1 Announce Type: new  Abstract: A key aspect of language documentation is the creation of annotated text in a format such as interlinear glossed text (IGT), which captures fine-grained morphosyntactic analyses in a morpheme-by-morpheme format. Prior work has explored methods to automatically generate IGT in order to reduce the time cost of language analysis. However, many languages (particularly those requiring preservation) lack sufficient IGT data to train effective models, and crosslingual transfer has been proposed as a method to overcome this limitation.   We compile the largest existing corpus of IGT data from a variety of sources, covering over 450k examples across 1.8k languages, to enable research on crosslingual transfer and IGT generation. Then, we pretrain a large multilingual model on a portion of this corpus, and further finetune it to specific languages. Our model is competitive with state-of-the-art methods for segmented data and large monolingual datas",
    "path": "papers/24/03/2403.06399.json",
    "total_tokens": 866,
    "translated_title": "GlossLM: 低资源语言文字间注释的多语言预训练",
    "translated_abstract": "语言文献学的一个关键方面是以形式如文字间注释文本（IGT）的方式创建带注释的文本，IGT以逐词素的格式捕捉了精细的形态句法分析。先前的研究已探索了自动生成IGT的方法，以减少语言分析的时间成本。然而，许多语言（尤其是需要保护的语言）缺乏足够的IGT数据来训练有效的模型，跨语言转移被提出作为克服这一局限的方法。我们编制了来自各种来源的最大已有IGT数据语料库，涵盖了来自1.8k种语言的超过45万个例子，以便进行跨语言转移和IGT生成方面的研究。然后，我们在部分语料库上对一个大型多语言模型进行预训练，并进一步对特定语言进行微调。我们的模型在分割数据和大型单语数据方面与最先进的方法相竞争。",
    "tldr": "该论文提出了GlossLM模型，通过利用跨语言转移和大规模多语言预训练，实现了低资源语言文字间注释的有效生成。"
}