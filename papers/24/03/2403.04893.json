{
    "title": "A Safe Harbor for AI Evaluation and Red Teaming",
    "abstract": "arXiv:2403.04893v1 Announce Type: new  Abstract: Independent evaluation and red teaming are critical for identifying the risks posed by generative AI systems. However, the terms of service and enforcement strategies used by prominent AI companies to deter model misuse have disincentives on good faith safety evaluations. This causes some researchers to fear that conducting such research or releasing their findings will result in account suspensions or legal reprisal. Although some companies offer researcher access programs, they are an inadequate substitute for independent research access, as they have limited community representation, receive inadequate funding, and lack independence from corporate incentives. We propose that major AI developers commit to providing a legal and technical safe harbor, indemnifying public interest safety research and protecting it from the threat of account suspensions or legal reprisal. These proposals emerged from our collective experience conducting sa",
    "link": "https://arxiv.org/abs/2403.04893",
    "context": "Title: A Safe Harbor for AI Evaluation and Red Teaming\nAbstract: arXiv:2403.04893v1 Announce Type: new  Abstract: Independent evaluation and red teaming are critical for identifying the risks posed by generative AI systems. However, the terms of service and enforcement strategies used by prominent AI companies to deter model misuse have disincentives on good faith safety evaluations. This causes some researchers to fear that conducting such research or releasing their findings will result in account suspensions or legal reprisal. Although some companies offer researcher access programs, they are an inadequate substitute for independent research access, as they have limited community representation, receive inadequate funding, and lack independence from corporate incentives. We propose that major AI developers commit to providing a legal and technical safe harbor, indemnifying public interest safety research and protecting it from the threat of account suspensions or legal reprisal. These proposals emerged from our collective experience conducting sa",
    "path": "papers/24/03/2403.04893.json",
    "total_tokens": 827,
    "translated_title": "一种用于AI评估和红队测试的安全港",
    "translated_abstract": "独立评估和红队测试对于发现生成式AI系统所带来的风险至关重要。然而，主要AI公司用于阻止模型误用的服务条款和执行策略会对善意的安全评估造成打击。这导致一些研究人员担心进行此类研究或发布其发现将导致账户被暂停或面临法律报复。虽然一些公司提供研究人员访问计划，但它们无法替代独立研究访问，因为它们的社区代表性有限，资金不足，并且缺乏独立于企业激励的性质。我们提出，主要AI开发者承诺提供法律和技术上的安全港，使公共利益的安全研究免受账户暂停或法律报复的威胁。这些提议源自我们进行安全评估的集体经验。",
    "tldr": "主要AI开发者应承诺提供法律和技术上的安全港，使公共利益的安全研究免受账户暂停或法律报复的威胁。",
    "en_tdlr": "Major AI developers should commit to providing a legal and technical safe harbor to protect public interest safety research from account suspensions or legal reprisal."
}