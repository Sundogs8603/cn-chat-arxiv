{
    "title": "Multi-conditioned Graph Diffusion for Neural Architecture Search",
    "abstract": "arXiv:2403.06020v1 Announce Type: new  Abstract: Neural architecture search automates the design of neural network architectures usually by exploring a large and thus complex architecture search space. To advance the architecture search, we present a graph diffusion-based NAS approach that uses discrete conditional graph diffusion processes to generate high-performing neural network architectures. We then propose a multi-conditioned classifier-free guidance approach applied to graph diffusion networks to jointly impose constraints such as high accuracy and low hardware latency. Unlike the related work, our method is completely differentiable and requires only a single model training. In our evaluations, we show promising results on six standard benchmarks, yielding novel and unique architectures at a fast speed, i.e. less than 0.2 seconds per architecture. Furthermore, we demonstrate the generalisability and efficiency of our method through experiments on ImageNet dataset.",
    "link": "https://arxiv.org/abs/2403.06020",
    "context": "Title: Multi-conditioned Graph Diffusion for Neural Architecture Search\nAbstract: arXiv:2403.06020v1 Announce Type: new  Abstract: Neural architecture search automates the design of neural network architectures usually by exploring a large and thus complex architecture search space. To advance the architecture search, we present a graph diffusion-based NAS approach that uses discrete conditional graph diffusion processes to generate high-performing neural network architectures. We then propose a multi-conditioned classifier-free guidance approach applied to graph diffusion networks to jointly impose constraints such as high accuracy and low hardware latency. Unlike the related work, our method is completely differentiable and requires only a single model training. In our evaluations, we show promising results on six standard benchmarks, yielding novel and unique architectures at a fast speed, i.e. less than 0.2 seconds per architecture. Furthermore, we demonstrate the generalisability and efficiency of our method through experiments on ImageNet dataset.",
    "path": "papers/24/03/2403.06020.json",
    "total_tokens": 835,
    "translated_title": "多条件图扩散用于神经架构搜索",
    "translated_abstract": "神经架构搜索通过探索庞大且复杂的架构搜索空间来自动设计神经网络架构。为了推动架构搜索，我们提出了一种基于图扩散的NAS方法，该方法使用离散条件图扩散过程生成性能优越的神经网络架构。我们随后提出了一种多条件无分类器指导方法，应用于图扩散网络，共同施加诸如高准确性和低硬件延迟等约束。与相关工作不同，我们的方法完全可微分，并且仅需要单模型训练。在我们的评估中，我们展示了在六个标准基准上取得了有希望的结果，以快速速度生成新颖且独特的架构，即每种架构少于0.2秒。此外，我们通过在ImageNet数据集上的实验展示了我们方法的泛化能力和效率。",
    "tldr": "提出了一种基于图扩散的NAS方法，结合多条件无分类器指导方法，能在架构搜索中生成快速且性能优越的神经网络架构，并在多个标准基准和ImageNet数据集上取得了有希望的结果",
    "en_tdlr": "Introduced a graph diffusion-based NAS approach combined with a multi-conditioned classifier-free guidance method to generate fast and high-performing neural network architectures in architecture search, showing promising results on multiple standard benchmarks and ImageNet dataset."
}