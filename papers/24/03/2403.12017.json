{
    "title": "Supervised Fine-Tuning as Inverse Reinforcement Learning",
    "abstract": "arXiv:2403.12017v1 Announce Type: cross  Abstract: The prevailing approach to aligning Large Language Models (LLMs) typically relies on human or AI feedback and assumes access to specific types of preference datasets. In our work, we question the efficacy of such datasets and explore various scenarios where alignment with expert demonstrations proves more realistic. We build a sequential decision-making framework to formulate the problem of aligning LLMs using demonstration datasets. Drawing insights from inverse reinforcement learning and imitation learning, we introduce various approaches for divergence minimization in the LLM alignment tasks. Our analysis highlights the mass-covering and mode-seeking behaviors of these different approaches. Inclusively, we examine the pros and cons of the classical supervised fine-tuning method, elaborating on scenarios where different methods shine.",
    "link": "https://arxiv.org/abs/2403.12017",
    "context": "Title: Supervised Fine-Tuning as Inverse Reinforcement Learning\nAbstract: arXiv:2403.12017v1 Announce Type: cross  Abstract: The prevailing approach to aligning Large Language Models (LLMs) typically relies on human or AI feedback and assumes access to specific types of preference datasets. In our work, we question the efficacy of such datasets and explore various scenarios where alignment with expert demonstrations proves more realistic. We build a sequential decision-making framework to formulate the problem of aligning LLMs using demonstration datasets. Drawing insights from inverse reinforcement learning and imitation learning, we introduce various approaches for divergence minimization in the LLM alignment tasks. Our analysis highlights the mass-covering and mode-seeking behaviors of these different approaches. Inclusively, we examine the pros and cons of the classical supervised fine-tuning method, elaborating on scenarios where different methods shine.",
    "path": "papers/24/03/2403.12017.json",
    "total_tokens": 775,
    "translated_title": "监督微调作为逆强化学习",
    "translated_abstract": "大语言模型（LLMs）对齐的主流方法通常依赖于人类或AI反馈，并假设可以访问特定类型的偏好数据集。在我们的工作中，我们质疑这些数据集的有效性，并探讨了各种情景下与专家演示对齐更为现实的方法。我们构建了一个顺序决策框架，以演示数据集为基础来规划对齐LLMs的问题。借鉴逆强化学习和模仿学习的见解，我们引入了各种方法来最小化LLM对齐任务中的差异。我们的分析突出了这些不同方法的覆盖率和寻找模式行为。此外，我们考察了经典监督微调方法的利弊，并详细阐述了不同方法表现突出的情景。",
    "tldr": "本论文提出将逆强化学习和模仿学习的见解结合，探讨了使用演示数据集对齐大语言模型的方法，并对不同方法的性能进行了分析。"
}