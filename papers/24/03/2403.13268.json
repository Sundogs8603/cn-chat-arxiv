{
    "title": "Unifews: Unified Entry-Wise Sparsification for Efficient Graph Neural Network",
    "abstract": "arXiv:2403.13268v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have shown promising performance in various graph learning tasks, but at the cost of resource-intensive computations. The primary overhead of GNN update stems from graph propagation and weight transformation, both involving operations on graph-scale matrices. Previous studies attempt to reduce the computational budget by leveraging graph-level or network-level sparsification techniques, resulting in downsized graph or weights. In this work, we propose Unifews, which unifies the two operations in an entry-wise manner considering individual matrix elements, and conducts joint edge-weight sparsification to enhance learning efficiency. The entry-wise design of Unifews enables adaptive compression across GNN layers with progressively increased sparsity, and is applicable to a variety of architectural designs with on-the-fly operation simplification. Theoretically, we establish a novel framework to characterize spa",
    "link": "https://arxiv.org/abs/2403.13268",
    "context": "Title: Unifews: Unified Entry-Wise Sparsification for Efficient Graph Neural Network\nAbstract: arXiv:2403.13268v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have shown promising performance in various graph learning tasks, but at the cost of resource-intensive computations. The primary overhead of GNN update stems from graph propagation and weight transformation, both involving operations on graph-scale matrices. Previous studies attempt to reduce the computational budget by leveraging graph-level or network-level sparsification techniques, resulting in downsized graph or weights. In this work, we propose Unifews, which unifies the two operations in an entry-wise manner considering individual matrix elements, and conducts joint edge-weight sparsification to enhance learning efficiency. The entry-wise design of Unifews enables adaptive compression across GNN layers with progressively increased sparsity, and is applicable to a variety of architectural designs with on-the-fly operation simplification. Theoretically, we establish a novel framework to characterize spa",
    "path": "papers/24/03/2403.13268.json",
    "total_tokens": 855,
    "translated_title": "Unifews：用于高效图神经网络的统一逐条稀疏化",
    "translated_abstract": "图神经网络（GNNs）在各种图学习任务中表现出了有希望的性能，但代价是资源密集型的计算。GNN更新的主要开销来自图传播和权重变换，两者都涉及对图规模矩阵的操作。先前的研究尝试通过利用图级别或网络级别的稀疏化技术来减少计算预算，从而产生缩小的图或权重。在这项工作中，我们提出了Unifews，它以逐个矩阵元素的方式统一了这两种操作，并进行联合边权重稀疏化以增强学习效率。Unifews的逐条设计使其能够在GNN层之间进行自适应压缩，稀疏度逐渐增加，并适用于各种架构设计，具有即时操作简化。在理论上，我们建立了一个新颖的框架来表征稀疏",
    "tldr": "Unifews通过统一逐条稀疏化的方式，联合边权重稀疏化以提高学习效率，适用于不同架构设计并具有逐渐增加稀疏度的自适应压缩。",
    "en_tdlr": "Unifews unifies entry-wise sparsification, conducts joint edge-weight sparsification to enhance learning efficiency, applicable to various architectural designs with progressively increased sparsity."
}