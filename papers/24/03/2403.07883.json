{
    "title": "Efficient Vision-and-Language Pre-training with Text-Relevant Image Patch Selection",
    "abstract": "arXiv:2403.07883v1 Announce Type: cross  Abstract: Vision Transformers (ViTs) have become increasingly popular in large-scale Vision and Language Pre-training (VLP) models. Although previous VLP research has demonstrated the efficacy of ViTs, these efforts still struggle with computational inefficiencies caused by lengthy visual sequences. To address this challenge, we introduce an efficient VLP approach called TRIPS, which stands for Text-Relevant Image Patch Selection. TRIPS progressively reduces the visual sequence using a text-guided patch-selection layer in the visual backbone, thereby accelerating both training and inference processes. This patch-selection layer dynamically computes text-dependent visual attention, enabling it to identify attentive image tokens with text guidance and fuse inattentive ones in an end-to-end fashion. Importantly, TRIPS does not add any extra parameters and generalizes to most ViT-based VLP models. We incorporate TRIPS into three representative VLP m",
    "link": "https://arxiv.org/abs/2403.07883",
    "context": "Title: Efficient Vision-and-Language Pre-training with Text-Relevant Image Patch Selection\nAbstract: arXiv:2403.07883v1 Announce Type: cross  Abstract: Vision Transformers (ViTs) have become increasingly popular in large-scale Vision and Language Pre-training (VLP) models. Although previous VLP research has demonstrated the efficacy of ViTs, these efforts still struggle with computational inefficiencies caused by lengthy visual sequences. To address this challenge, we introduce an efficient VLP approach called TRIPS, which stands for Text-Relevant Image Patch Selection. TRIPS progressively reduces the visual sequence using a text-guided patch-selection layer in the visual backbone, thereby accelerating both training and inference processes. This patch-selection layer dynamically computes text-dependent visual attention, enabling it to identify attentive image tokens with text guidance and fuse inattentive ones in an end-to-end fashion. Importantly, TRIPS does not add any extra parameters and generalizes to most ViT-based VLP models. We incorporate TRIPS into three representative VLP m",
    "path": "papers/24/03/2403.07883.json",
    "total_tokens": 871,
    "translated_title": "使用与文本相关的图像块选择进行高效的视觉与语言预训练",
    "translated_abstract": "Vision Transformers (ViTs)在大规模的视觉与语言预训练(VLP)模型中变得越来越受欢迎。尽管先前的VLP研究已经证明了ViTs的有效性，但这些努力仍然受到由于冗长的视觉序列引起的计算效率低下的困扰。为了解决这一挑战，我们引入了一种高效的VLP方法，称为TRIPS，全称为Text-Relevant Image Patch Selection。TRIPS通过在视觉主干网络中逐步减少视觉序列，使用一个文本引导的图像块选择层，从而加速训练和推理过程。这个块选择层动态计算文本相关的视觉注意力，使其能够在端到端的方式中通过文本引导识别出关注的图像记号并融合不关注的记号。重要的是，TRIPS不添加任何额外的参数，并且适用于大多数基于ViT的VLP模型。",
    "tldr": "TRIPS是一种高效的视觉与语言预训练方法，通过使用文本引导的图像块选择层，动态计算文本相关的视觉注意力，加速训练和推理过程，而且不增加额外参数。",
    "en_tdlr": "TRIPS is an efficient vision-and-language pre-training approach that accelerates training and inference processes by dynamically computing text-relevant visual attention using a text-guided image patch selection layer, without adding extra parameters."
}