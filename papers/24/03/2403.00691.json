{
    "title": "Tri-Modal Motion Retrieval by Learning a Joint Embedding Space",
    "abstract": "arXiv:2403.00691v1 Announce Type: cross  Abstract: Information retrieval is an ever-evolving and crucial research domain. The substantial demand for high-quality human motion data especially in online acquirement has led to a surge in human motion research works. Prior works have mainly concentrated on dual-modality learning, such as text and motion tasks, but three-modality learning has been rarely explored. Intuitively, an extra introduced modality can enrich a model's application scenario, and more importantly, an adequate choice of the extra modality can also act as an intermediary and enhance the alignment between the other two disparate modalities. In this work, we introduce LAVIMO (LAnguage-VIdeo-MOtion alignment), a novel framework for three-modality learning integrating human-centric videos as an additional modality, thereby effectively bridging the gap between text and motion. Moreover, our approach leverages a specially designed attention mechanism to foster enhanced alignme",
    "link": "https://arxiv.org/abs/2403.00691",
    "context": "Title: Tri-Modal Motion Retrieval by Learning a Joint Embedding Space\nAbstract: arXiv:2403.00691v1 Announce Type: cross  Abstract: Information retrieval is an ever-evolving and crucial research domain. The substantial demand for high-quality human motion data especially in online acquirement has led to a surge in human motion research works. Prior works have mainly concentrated on dual-modality learning, such as text and motion tasks, but three-modality learning has been rarely explored. Intuitively, an extra introduced modality can enrich a model's application scenario, and more importantly, an adequate choice of the extra modality can also act as an intermediary and enhance the alignment between the other two disparate modalities. In this work, we introduce LAVIMO (LAnguage-VIdeo-MOtion alignment), a novel framework for three-modality learning integrating human-centric videos as an additional modality, thereby effectively bridging the gap between text and motion. Moreover, our approach leverages a specially designed attention mechanism to foster enhanced alignme",
    "path": "papers/24/03/2403.00691.json",
    "total_tokens": 893,
    "translated_title": "通过学习联合嵌入空间实现三模态运动检索",
    "translated_abstract": "信息检索是一个不断发展且至关重要的研究领域。对高质量人体运动数据的巨大需求尤其在在线获取方面导致了人体运动研究工作的激增。以往的研究主要集中在双模态学习上，如文本和运动任务，但很少探索三模态学习。直觉上，引入额外的模态可以丰富模型的应用场景，更重要的是，对额外模态的充分选择还可以作为中介并增强其他两个不同模态之间的对齐。在这项工作中，我们提出了LAVIMO（LAnguage-VIdeo-MOtion对齐），这是一个新颖的三模态学习框架，将以人为中心的视频作为额外的模态集成，从而有效地弥合文本和运动之间的差距。此外，我们的方法利用了一个特别设计的注意机制来促进增强的对齐。",
    "tldr": "通过引入人为中心的视频作为额外模态，该文章提出了一个新颖的三模态学习框架LAVIMO，有效地减少了文本和运动之间的差距，同时利用特别设计的注意机制促进了增强的对齐。",
    "en_tdlr": "This paper introduces a novel three-modality learning framework LAVIMO by incorporating human-centric videos as an additional modality, effectively bridging the gap between text and motion while leveraging a specially designed attention mechanism to foster enhanced alignment."
}