{
    "title": "Automatic Question-Answer Generation for Long-Tail Knowledge",
    "abstract": "arXiv:2403.01382v1 Announce Type: new  Abstract: Pretrained Large Language Models (LLMs) have gained significant attention for addressing open-domain Question Answering (QA). While they exhibit high accuracy in answering questions related to common knowledge, LLMs encounter difficulties in learning about uncommon long-tail knowledge (tail entities). Since manually constructing QA datasets demands substantial human resources, the types of existing QA datasets are limited, leaving us with a scarcity of datasets to study the performance of LLMs on tail entities. In this paper, we propose an automatic approach to generate specialized QA datasets for tail entities and present the associated research challenges. We conduct extensive experiments by employing pretrained LLMs on our newly generated long-tail QA datasets, comparing their performance with and without external resources including Wikipedia and Wikidata knowledge graphs.",
    "link": "https://arxiv.org/abs/2403.01382",
    "context": "Title: Automatic Question-Answer Generation for Long-Tail Knowledge\nAbstract: arXiv:2403.01382v1 Announce Type: new  Abstract: Pretrained Large Language Models (LLMs) have gained significant attention for addressing open-domain Question Answering (QA). While they exhibit high accuracy in answering questions related to common knowledge, LLMs encounter difficulties in learning about uncommon long-tail knowledge (tail entities). Since manually constructing QA datasets demands substantial human resources, the types of existing QA datasets are limited, leaving us with a scarcity of datasets to study the performance of LLMs on tail entities. In this paper, we propose an automatic approach to generate specialized QA datasets for tail entities and present the associated research challenges. We conduct extensive experiments by employing pretrained LLMs on our newly generated long-tail QA datasets, comparing their performance with and without external resources including Wikipedia and Wikidata knowledge graphs.",
    "path": "papers/24/03/2403.01382.json",
    "total_tokens": 719,
    "translated_title": "长尾知识的自动问答生成",
    "translated_abstract": "预训练的大型语言模型（LLMs）在开放领域问答中取得了显著的关注。虽然它们在回答与常见知识相关的问题时表现出高准确性，但在学习罕见的长尾知识（尾部实体）时遇到困难。本文提出了一种自动生成专门用于尾部实体的问答数据集的方法，并介绍了相关的研究挑战。我们通过利用预训练的LLMs在我们新生成的长尾问答数据集上进行了大量实验，比较了它们在有无外部资源（包括维基百科和维基数据知识图谱）的情况下的表现。",
    "tldr": "提出了一种自动生成长尾知识问答数据集的自动化方法，并通过预训练的大型语言模型对其进行了实验验证。"
}