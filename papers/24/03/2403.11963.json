{
    "title": "Transfer Learning Beyond Bounded Density Ratios",
    "abstract": "arXiv:2403.11963v1 Announce Type: new  Abstract: We study the fundamental problem of transfer learning where a learning algorithm collects data from some source distribution $P$ but needs to perform well with respect to a different target distribution $Q$. A standard change of measure argument implies that transfer learning happens when the density ratio $dQ/dP$ is bounded. Yet, prior thought-provoking works by Kpotufe and Martinet (COLT, 2018) and Hanneke and Kpotufe (NeurIPS, 2019) demonstrate cases where the ratio $dQ/dP$ is unbounded, but transfer learning is possible.   In this work, we focus on transfer learning over the class of low-degree polynomial estimators. Our main result is a general transfer inequality over the domain $\\mathbb{R}^n$, proving that non-trivial transfer learning for low-degree polynomials is possible under very mild assumptions, going well beyond the classical assumption that $dQ/dP$ is bounded. For instance, it always applies if $Q$ is a log-concave measur",
    "link": "https://arxiv.org/abs/2403.11963",
    "context": "Title: Transfer Learning Beyond Bounded Density Ratios\nAbstract: arXiv:2403.11963v1 Announce Type: new  Abstract: We study the fundamental problem of transfer learning where a learning algorithm collects data from some source distribution $P$ but needs to perform well with respect to a different target distribution $Q$. A standard change of measure argument implies that transfer learning happens when the density ratio $dQ/dP$ is bounded. Yet, prior thought-provoking works by Kpotufe and Martinet (COLT, 2018) and Hanneke and Kpotufe (NeurIPS, 2019) demonstrate cases where the ratio $dQ/dP$ is unbounded, but transfer learning is possible.   In this work, we focus on transfer learning over the class of low-degree polynomial estimators. Our main result is a general transfer inequality over the domain $\\mathbb{R}^n$, proving that non-trivial transfer learning for low-degree polynomials is possible under very mild assumptions, going well beyond the classical assumption that $dQ/dP$ is bounded. For instance, it always applies if $Q$ is a log-concave measur",
    "path": "papers/24/03/2403.11963.json",
    "total_tokens": 919,
    "translated_title": "超越有界密度比的迁移学习",
    "translated_abstract": "我们研究了迁移学习的基本问题，即学习算法从某个源分布$P$收集数据，但需要在不同的目标分布$Q$上表现良好。标准的测度变换论证表明，当密度比$dQ/dP$有界时发生迁移学习。然而，Kpotufe和Martinet(2018年COLT)以及Hanneke和Kpotufe(2019年NeurIPS)之前引人深思的作品展示了一些情况，其中比率$dQ/dP$是无界的，但迁移学习是可能的。在这项工作中，我们专注于在低次多项式估计类上进行迁移学习。我们的主要结果是在定义域$\\mathbb{R}^n$上的一般迁移不等式，证明了在非常温和的假设下，对于低次多项式来说非平凡的迁移学习是可能的，远远超出了$dQ/dP$被有界的经典假设。例如，如果$Q$是对数凹测度，则始终适用。",
    "tldr": "低次多项式估计类上的迁移学习，证明了在非常温和的假设下，对于低次多项式来说非平凡的迁移学习是可能的，超越了$dQ/dP$有界的经典假设",
    "en_tdlr": "Transfer learning over low-degree polynomial estimators demonstrates non-trivial transfer learning is possible under very mild assumptions, well beyond the classical bounded density ratio assumption."
}