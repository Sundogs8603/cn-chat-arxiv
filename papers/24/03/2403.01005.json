{
    "title": "Policy Optimization for PDE Control with a Warm Start",
    "abstract": "arXiv:2403.01005v1 Announce Type: cross  Abstract: Dimensionality reduction is crucial for controlling nonlinear partial differential equations (PDE) through a \"reduce-then-design\" strategy, which identifies a reduced-order model and then implements model-based control solutions. However, inaccuracies in the reduced-order modeling can substantially degrade controller performance, especially in PDEs with chaotic behavior. To address this issue, we augment the reduce-then-design procedure with a policy optimization (PO) step. The PO step fine-tunes the model-based controller to compensate for the modeling error from dimensionality reduction. This augmentation shifts the overall strategy into reduce-then-design-then-adapt, where the model-based controller serves as a warm start for PO. Specifically, we study the state-feedback tracking control of PDEs that aims to align the PDE state with a specific constant target subject to a linear-quadratic cost. Through extensive experiments, we show",
    "link": "https://arxiv.org/abs/2403.01005",
    "context": "Title: Policy Optimization for PDE Control with a Warm Start\nAbstract: arXiv:2403.01005v1 Announce Type: cross  Abstract: Dimensionality reduction is crucial for controlling nonlinear partial differential equations (PDE) through a \"reduce-then-design\" strategy, which identifies a reduced-order model and then implements model-based control solutions. However, inaccuracies in the reduced-order modeling can substantially degrade controller performance, especially in PDEs with chaotic behavior. To address this issue, we augment the reduce-then-design procedure with a policy optimization (PO) step. The PO step fine-tunes the model-based controller to compensate for the modeling error from dimensionality reduction. This augmentation shifts the overall strategy into reduce-then-design-then-adapt, where the model-based controller serves as a warm start for PO. Specifically, we study the state-feedback tracking control of PDEs that aims to align the PDE state with a specific constant target subject to a linear-quadratic cost. Through extensive experiments, we show",
    "path": "papers/24/03/2403.01005.json",
    "total_tokens": 905,
    "translated_title": "具有热启动的PDE控制策略优化",
    "translated_abstract": "维度约简对通过“减少-然后设计”策略控制非线性偏微分方程（PDE）至关重要，该策略确定降阶模型然后实施基于模型的控制解决方案。然而，降阶建模的不准确性可能会严重降低控制器的性能，尤其是在具有混沌行为的PDE中。为了解决这个问题，我们在减少-然后设计过程中增加了一个策略优化（PO）步骤。PO步骤微调基于模型的控制器，以补偿由维度约简引起的建模误差。这种增强将整体策略转变为减少-然后设计-然后调整，其中基于模型的控制器作为PO的热启动。具体来说，我们研究了旨在将PDE状态与特定恒定目标对齐的PDE状态反馈跟踪控制，受线性二次成本约束。通过大量实验，我们展示",
    "tldr": "通过在减少-然后设计过程中增加策略优化步骤，来微调模型-based 控制器以补偿维度约简引起的建模错误，并将整体策略转变为减少-然后设计-然后适应的PDE控制方法。",
    "en_tdlr": "By adding a policy optimization step in the reduce-then-design process to fine-tune the model-based controller for compensating the modeling error from dimensionality reduction, the overall strategy shifts into reduce-then-design-then-adapt for controlling PDEs."
}