{
    "title": "A Natural Extension To Online Algorithms For Hybrid RL With Limited Coverage",
    "abstract": "arXiv:2403.09701v1 Announce Type: new  Abstract: Hybrid Reinforcement Learning (RL), leveraging both online and offline data, has garnered recent interest, yet research on its provable benefits remains sparse. Additionally, many existing hybrid RL algorithms (Song et al., 2023; Nakamoto et al., 2023; Amortila et al., 2024) impose coverage assumptions on the offline dataset, but we show that this is unnecessary. A well-designed online algorithm should \"fill in the gaps\" in the offline dataset, exploring states and actions that the behavior policy did not explore. Unlike previous approaches that focus on estimating the offline data distribution to guide online exploration (Li et al., 2023b), we show that a natural extension to standard optimistic online algorithms -- warm-starting them by including the offline dataset in the experience replay buffer -- achieves similar provable gains from hybrid data even when the offline dataset does not have single-policy concentrability. We accomplish",
    "link": "https://arxiv.org/abs/2403.09701",
    "context": "Title: A Natural Extension To Online Algorithms For Hybrid RL With Limited Coverage\nAbstract: arXiv:2403.09701v1 Announce Type: new  Abstract: Hybrid Reinforcement Learning (RL), leveraging both online and offline data, has garnered recent interest, yet research on its provable benefits remains sparse. Additionally, many existing hybrid RL algorithms (Song et al., 2023; Nakamoto et al., 2023; Amortila et al., 2024) impose coverage assumptions on the offline dataset, but we show that this is unnecessary. A well-designed online algorithm should \"fill in the gaps\" in the offline dataset, exploring states and actions that the behavior policy did not explore. Unlike previous approaches that focus on estimating the offline data distribution to guide online exploration (Li et al., 2023b), we show that a natural extension to standard optimistic online algorithms -- warm-starting them by including the offline dataset in the experience replay buffer -- achieves similar provable gains from hybrid data even when the offline dataset does not have single-policy concentrability. We accomplish",
    "path": "papers/24/03/2403.09701.json",
    "total_tokens": 868,
    "translated_title": "一种对有限覆盖的混合RL在线算法的自然扩展",
    "translated_abstract": "混合强化学习（RL）结合在线和离线数据，近年来引起了广泛关注，但关于其可证明益处的研究仍然很少。许多现有的混合RL算法对离线数据集施加覆盖假设，但我们表明这是不必要的。一个设计良好的在线算法应该在离线数据集中“填补空白”，探索行为策略未探索的状态和动作。与先前侧重于估计离线数据分布以引导在线探索的方法不同，我们表明对标准乐观在线算法的一个自然扩展——通过将离线数据集包含在经验重放缓冲区中来启动它们——即使离线数据集没有单一策略可集中性，也可实现混合数据的类似可证明收益。我们完成",
    "tldr": "混合强化学习算法中，通过将离线数据集包含在在线算法的经验重放缓冲区中进行启动，可以实现类似于基于离线数据分布引导在线探索的可证明收益，即使离线数据集没有单一策略可集中性。"
}