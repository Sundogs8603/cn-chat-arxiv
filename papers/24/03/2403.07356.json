{
    "title": "Premonition: Using Generative Models to Preempt Future Data Changes in Continual Learning",
    "abstract": "arXiv:2403.07356v1 Announce Type: cross  Abstract: Continual learning requires a model to adapt to ongoing changes in the data distribution, and often to the set of tasks to be performed. It is rare, however, that the data and task changes are completely unpredictable. Given a description of an overarching goal or data theme, which we call a realm, humans can often guess what concepts are associated with it. We show here that the combination of a large language model and an image generation model can similarly provide useful premonitions as to how a continual learning challenge might develop over time. We use the large language model to generate text descriptions of semantically related classes that might potentially appear in the data stream in future. These descriptions are then rendered using Stable Diffusion to generate new labelled image samples. The resulting synthetic dataset is employed for supervised pre-training, but is discarded prior to commencing continual learning, along ",
    "link": "https://arxiv.org/abs/2403.07356",
    "context": "Title: Premonition: Using Generative Models to Preempt Future Data Changes in Continual Learning\nAbstract: arXiv:2403.07356v1 Announce Type: cross  Abstract: Continual learning requires a model to adapt to ongoing changes in the data distribution, and often to the set of tasks to be performed. It is rare, however, that the data and task changes are completely unpredictable. Given a description of an overarching goal or data theme, which we call a realm, humans can often guess what concepts are associated with it. We show here that the combination of a large language model and an image generation model can similarly provide useful premonitions as to how a continual learning challenge might develop over time. We use the large language model to generate text descriptions of semantically related classes that might potentially appear in the data stream in future. These descriptions are then rendered using Stable Diffusion to generate new labelled image samples. The resulting synthetic dataset is employed for supervised pre-training, but is discarded prior to commencing continual learning, along ",
    "path": "papers/24/03/2403.07356.json",
    "total_tokens": 805,
    "translated_title": "预感：利用生成模型预测持续学习中未来数据变化",
    "translated_abstract": "持续学习要求模型能够适应数据分布的持续变化，通常也要适应要执行的任务集。然而，数据和任务变化很少是完全不可预测的。鉴于一个概括性目标或数据主题的描述，我们称之为领域，人类通常可以猜测与之相关的概念。我们在这里展示，大型语言模型和图像生成模型的组合可以类似地提供有用的预感，以预测持续学习挑战随时间如何发展。我们使用大型语言模型生成未来可能出现在数据流中的语义相关类别的文本描述。然后利用稳定扩散来生成新的带标签图像样本。生成的合成数据集用于监督预训练，但在开始持续学习之前会被丢弃。",
    "tldr": "结合大型语言模型和图像生成模型，在持续学习中利用预感来预测数据变化，为监督预训练提供了新的途径。",
    "en_tdlr": "By combining a large language model and an image generation model, using premonitions in continual learning to anticipate data changes provides a new approach for supervised pre-training."
}