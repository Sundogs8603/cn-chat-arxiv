{
    "title": "On permutation-invariant neural networks",
    "abstract": "arXiv:2403.17410v1 Announce Type: cross  Abstract: Conventional machine learning algorithms have traditionally been designed under the assumption that input data follows a vector-based format, with an emphasis on vector-centric paradigms. However, as the demand for tasks involving set-based inputs has grown, there has been a paradigm shift in the research community towards addressing these challenges. In recent years, the emergence of neural network architectures such as Deep Sets and Transformers has presented a significant advancement in the treatment of set-based data. These architectures are specifically engineered to naturally accommodate sets as input, enabling more effective representation and processing of set structures. Consequently, there has been a surge of research endeavors dedicated to exploring and harnessing the capabilities of these architectures for various tasks involving the approximation of set functions. This comprehensive survey aims to provide an overview of th",
    "link": "https://arxiv.org/abs/2403.17410",
    "context": "Title: On permutation-invariant neural networks\nAbstract: arXiv:2403.17410v1 Announce Type: cross  Abstract: Conventional machine learning algorithms have traditionally been designed under the assumption that input data follows a vector-based format, with an emphasis on vector-centric paradigms. However, as the demand for tasks involving set-based inputs has grown, there has been a paradigm shift in the research community towards addressing these challenges. In recent years, the emergence of neural network architectures such as Deep Sets and Transformers has presented a significant advancement in the treatment of set-based data. These architectures are specifically engineered to naturally accommodate sets as input, enabling more effective representation and processing of set structures. Consequently, there has been a surge of research endeavors dedicated to exploring and harnessing the capabilities of these architectures for various tasks involving the approximation of set functions. This comprehensive survey aims to provide an overview of th",
    "path": "papers/24/03/2403.17410.json",
    "total_tokens": 710,
    "translated_title": "论排列不变神经网络",
    "translated_abstract": "传统机器学习算法通常在假设输入数据遵循基于向量的格式的前提下设计，着重于基于向量的范式。然而，随着需求涉及基于集合的任务的增长，研究界对解决这些挑战的兴趣发生了范式转变。近年来，Deep Sets和Transformers等神经网络架构的出现在处理基于集合的数据方面取得了重大进展。这些架构专门设计为自然容纳集合作为输入，从而更有效地表示和处理集合结构。因此，近年来出现了大量致力于探索和利用这些架构能力的研究努力，以逼近集合函数的各种任务。这项综合调查旨在概述th",
    "tldr": "神经网络如Deep Sets和Transformers的出现显著推动了基于集合的数据处理的进展",
    "en_tdlr": "The emergence of neural network architectures like Deep Sets and Transformers has significantly advanced the treatment of set-based data."
}