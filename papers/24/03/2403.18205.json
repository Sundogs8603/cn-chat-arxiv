{
    "title": "Exploring the Privacy Protection Capabilities of Chinese Large Language Models",
    "abstract": "arXiv:2403.18205v1 Announce Type: new  Abstract: Large language models (LLMs), renowned for their impressive capabilities in various tasks, have significantly advanced artificial intelligence. Yet, these advancements have raised growing concerns about privacy and security implications. To address these issues and explain the risks inherent in these models, we have devised a three-tiered progressive framework tailored for evaluating privacy in language systems. This framework consists of progressively complex and in-depth privacy test tasks at each tier. Our primary objective is to comprehensively evaluate the sensitivity of large language models to private information, examining how effectively they discern, manage, and safeguard sensitive data in diverse scenarios. This systematic evaluation helps us understand the degree to which these models comply with privacy protection guidelines and the effectiveness of their inherent safeguards against privacy breaches. Our observations indicat",
    "link": "https://arxiv.org/abs/2403.18205",
    "context": "Title: Exploring the Privacy Protection Capabilities of Chinese Large Language Models\nAbstract: arXiv:2403.18205v1 Announce Type: new  Abstract: Large language models (LLMs), renowned for their impressive capabilities in various tasks, have significantly advanced artificial intelligence. Yet, these advancements have raised growing concerns about privacy and security implications. To address these issues and explain the risks inherent in these models, we have devised a three-tiered progressive framework tailored for evaluating privacy in language systems. This framework consists of progressively complex and in-depth privacy test tasks at each tier. Our primary objective is to comprehensively evaluate the sensitivity of large language models to private information, examining how effectively they discern, manage, and safeguard sensitive data in diverse scenarios. This systematic evaluation helps us understand the degree to which these models comply with privacy protection guidelines and the effectiveness of their inherent safeguards against privacy breaches. Our observations indicat",
    "path": "papers/24/03/2403.18205.json",
    "total_tokens": 830,
    "translated_title": "探究中国大型语言模型的隐私保护能力",
    "translated_abstract": "大型语言模型（LLMs）以其在各种任务中出色表现而闻名，在推动人工智能方面取得了重大进展。然而，这些进步引发了人们对隐私和安全影响日益增长的关注。为了解决这些问题并解释这些模型固有风险，我们设计了一个分为三层的渐进式框架，专门用于评估语言系统中的隐私。该框架在每个层次都包含逐渐复杂和深入的隐私测试任务。我们的主要目标是全面评估大型语言模型对私人信息的敏感性，考察它们在不同情境下如何有效辨别、管理和保护敏感数据。这种系统化评估有助于我们了解这些模型遵守隐私保护准则的程度，以及其固有的防范隐私侵犯措施的有效性。我们的观察表明，",
    "tldr": "设计了一个三层渐进式框架来评估语言系统中的隐私，全面评估大型语言模型对私人信息的敏感性以及其防范隐私侵犯的有效性",
    "en_tdlr": "Developed a three-tiered progressive framework to evaluate privacy in language systems, comprehensively assessing the sensitivity of large language models to private information and the effectiveness of their safeguards against privacy breaches."
}