{
    "title": "FLGuard: Byzantine-Robust Federated Learning via Ensemble of Contrastive Models",
    "abstract": "arXiv:2403.02846v1 Announce Type: cross  Abstract: Federated Learning (FL) thrives in training a global model with numerous clients by only sharing the parameters of their local models trained with their private training datasets. Therefore, without revealing the private dataset, the clients can obtain a deep learning (DL) model with high performance. However, recent research proposed poisoning attacks that cause a catastrophic loss in the accuracy of the global model when adversaries, posed as benign clients, are present in a group of clients. Therefore, recent studies suggested byzantine-robust FL methods that allow the server to train an accurate global model even with the adversaries present in the system. However, many existing methods require the knowledge of the number of malicious clients or the auxiliary (clean) dataset or the effectiveness reportedly decreased hugely when the private dataset was non-independently and identically distributed (non-IID). In this work, we propose",
    "link": "https://arxiv.org/abs/2403.02846",
    "context": "Title: FLGuard: Byzantine-Robust Federated Learning via Ensemble of Contrastive Models\nAbstract: arXiv:2403.02846v1 Announce Type: cross  Abstract: Federated Learning (FL) thrives in training a global model with numerous clients by only sharing the parameters of their local models trained with their private training datasets. Therefore, without revealing the private dataset, the clients can obtain a deep learning (DL) model with high performance. However, recent research proposed poisoning attacks that cause a catastrophic loss in the accuracy of the global model when adversaries, posed as benign clients, are present in a group of clients. Therefore, recent studies suggested byzantine-robust FL methods that allow the server to train an accurate global model even with the adversaries present in the system. However, many existing methods require the knowledge of the number of malicious clients or the auxiliary (clean) dataset or the effectiveness reportedly decreased hugely when the private dataset was non-independently and identically distributed (non-IID). In this work, we propose",
    "path": "papers/24/03/2403.02846.json",
    "total_tokens": 804,
    "translated_title": "FLGuard: 通过对比模型集合实现拜占庭-鲁棒的联邦学习",
    "translated_abstract": "联邦学习（FL）通过仅共享本地模型的参数来训练全局模型，从而在训练具有私人训练数据集的众多客户时蓬勃发展。然而，最近的研究提出了投毒攻击，当对手假扮为良性客户并存在于一组客户中时，会导致全局模型准确性的灾难性损失。因此，最近的研究提出了拜占庭-鲁棒的FL方法，允许服务器即使在系统中存在对手也能训练准确的全局模型。然而，许多现有方法要求知道恶意客户的数量或辅助（干净）数据集，或者在私有数据集非独立同分布（non-IID）时其有效性报告明显降低。在这项工作中，我们提出",
    "tldr": "通过对比模型集合，提出了FLGuard方法增强联邦学习的拜占庭-鲁棒性。",
    "en_tdlr": "FLGuard is proposed to enhance the Byzantine-robustness of Federated Learning via ensemble of contrastive models."
}