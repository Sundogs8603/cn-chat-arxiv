{
    "title": "Continuous Mean-Zero Disagreement-Regularized Imitation Learning (CMZ-DRIL)",
    "abstract": "arXiv:2403.01059v1 Announce Type: new  Abstract: Machine-learning paradigms such as imitation learning and reinforcement learning can generate highly performant agents in a variety of complex environments. However, commonly used methods require large quantities of data and/or a known reward function. This paper presents a method called Continuous Mean-Zero Disagreement-Regularized Imitation Learning (CMZ-DRIL) that employs a novel reward structure to improve the performance of imitation-learning agents that have access to only a handful of expert demonstrations. CMZ-DRIL uses reinforcement learning to minimize uncertainty among an ensemble of agents trained to model the expert demonstrations. This method does not use any environment-specific rewards, but creates a continuous and mean-zero reward function from the action disagreement of the agent ensemble. As demonstrated in a waypoint-navigation environment and in two MuJoCo environments, CMZ-DRIL can generate performant agents that be",
    "link": "https://arxiv.org/abs/2403.01059",
    "context": "Title: Continuous Mean-Zero Disagreement-Regularized Imitation Learning (CMZ-DRIL)\nAbstract: arXiv:2403.01059v1 Announce Type: new  Abstract: Machine-learning paradigms such as imitation learning and reinforcement learning can generate highly performant agents in a variety of complex environments. However, commonly used methods require large quantities of data and/or a known reward function. This paper presents a method called Continuous Mean-Zero Disagreement-Regularized Imitation Learning (CMZ-DRIL) that employs a novel reward structure to improve the performance of imitation-learning agents that have access to only a handful of expert demonstrations. CMZ-DRIL uses reinforcement learning to minimize uncertainty among an ensemble of agents trained to model the expert demonstrations. This method does not use any environment-specific rewards, but creates a continuous and mean-zero reward function from the action disagreement of the agent ensemble. As demonstrated in a waypoint-navigation environment and in two MuJoCo environments, CMZ-DRIL can generate performant agents that be",
    "path": "papers/24/03/2403.01059.json",
    "total_tokens": 870,
    "translated_title": "连续零均值争议正则化模仿学习（CMZ-DRIL）",
    "translated_abstract": "机器学习范式，如模仿学习和强化学习，可以在各种复杂环境中生成高性能智能体。然而，常用方法需要大量数据和/或已知的奖励函数。本文提出了一种称为连续零均值争议正则化模仿学习（CMZ-DRIL）的方法，它采用一种新颖的奖励结构来提高只有少量专家演示的模仿学习智能体的性能。CMZ-DRIL使用强化学习来最小化训练为模仿专家演示的智能体集合之间的不确定性。该方法不使用任何特定于环境的奖励，而是通过智能体集合的动作不一致性创建连续的零均值奖励函数。如在路径导航环境和两个MuJoCo环境中展示的，CMZ-DRIL可以生成具有优越性能的智能体。",
    "tldr": "CMZ-DRIL是一种新颖的模仿学习方法，通过连续的零均值奖励函数和智能体集合之间的不确定性最小化，提高了只有少量专家演示的智能体性能。",
    "en_tdlr": "CMZ-DRIL is a novel imitation learning method that improves the performance of agents with access to only a few expert demonstrations by minimizing uncertainty among an ensemble of agents through a continuous mean-zero reward function."
}