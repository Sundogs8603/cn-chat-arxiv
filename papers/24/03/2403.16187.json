{
    "title": "ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models",
    "abstract": "arXiv:2403.16187v1 Announce Type: new  Abstract: Parameter-efficient fine-tuning (PEFT) is widely studied for its effectiveness and efficiency in the era of large language models. Low-rank adaptation (LoRA) has demonstrated commendable performance as a popular and representative method. However, it is implemented with a fixed intrinsic rank that might not be the ideal setting for the downstream tasks. Recognizing the need for more flexible downstream task adaptation, we extend the methodology of LoRA to an innovative approach we call allocating low-rank adaptation (ALoRA) that enables dynamic adjustments to the intrinsic rank during the adaptation process. First, we propose a novel method, AB-LoRA, that can effectively estimate the importance score of each LoRA rank. Second, guided by AB-LoRA, we gradually prune abundant and negatively impacting LoRA ranks and allocate the pruned LoRA budgets to important Transformer modules needing higher ranks. We have conducted experiments on variou",
    "link": "https://arxiv.org/abs/2403.16187",
    "context": "Title: ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models\nAbstract: arXiv:2403.16187v1 Announce Type: new  Abstract: Parameter-efficient fine-tuning (PEFT) is widely studied for its effectiveness and efficiency in the era of large language models. Low-rank adaptation (LoRA) has demonstrated commendable performance as a popular and representative method. However, it is implemented with a fixed intrinsic rank that might not be the ideal setting for the downstream tasks. Recognizing the need for more flexible downstream task adaptation, we extend the methodology of LoRA to an innovative approach we call allocating low-rank adaptation (ALoRA) that enables dynamic adjustments to the intrinsic rank during the adaptation process. First, we propose a novel method, AB-LoRA, that can effectively estimate the importance score of each LoRA rank. Second, guided by AB-LoRA, we gradually prune abundant and negatively impacting LoRA ranks and allocate the pruned LoRA budgets to important Transformer modules needing higher ranks. We have conducted experiments on variou",
    "path": "papers/24/03/2403.16187.json",
    "total_tokens": 910,
    "translated_title": "ALoRA: 为大型语言模型微调分配低秩适应性",
    "translated_abstract": "参数高效的微调（PEFT）因其在大型语言模型时代的有效性和效率而被广泛研究。低秩适应性（LoRA）已经展示出作为一种流行和代表性方法的令人钦佩的性能。然而，它是使用固定的固有秩来实现的，这可能不是下游任务的理想设置。认识到需要更灵活的下游任务适应性，我们将LoRA的方法学扩展到一种创新的方法，我们将其称为分配低秩适应性（ALoRA），这样可以在适应过程中动态调整固有秩。首先，我们提出了一种新颖的方法，AB-LoRA，可以有效地估计每个LoRA等级的重要性得分。其次，受AB-LoRA的指导，我们逐渐修剪了过多且产生负面影响的LoRA等级，并将被修剪的LoRA预算分配给需要更高等级的重要Transformer模块。我们已经在各种实验上进行了研究。",
    "tldr": "ALoRA创新地提出了分配低秩适应性（ALoRA）方法，通过动态调整适应过程中的固有秩，从而解决了在微调大型语言模型时固定固有秩带来的问题。",
    "en_tdlr": "ALoRA innovatively proposes the method of allocating low-rank adaptation (ALoRA), which dynamically adjusts the intrinsic rank during the adaptation process, addressing the issue of fixed intrinsic rank when fine-tuning large language models."
}