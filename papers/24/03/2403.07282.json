{
    "title": "Enhancing Transfer Learning with Flexible Nonparametric Posterior Sampling",
    "abstract": "arXiv:2403.07282v1 Announce Type: new  Abstract: Transfer learning has recently shown significant performance across various tasks involving deep neural networks. In these transfer learning scenarios, the prior distribution for downstream data becomes crucial in Bayesian model averaging (BMA). While previous works proposed the prior over the neural network parameters centered around the pre-trained solution, such strategies have limitations when dealing with distribution shifts between upstream and downstream data. This paper introduces nonparametric transfer learning (NPTL), a flexible posterior sampling method to address the distribution shift issue within the context of nonparametric learning. The nonparametric learning (NPL) method is a recent approach that employs a nonparametric prior for posterior sampling, efficiently accounting for model misspecification scenarios, which is suitable for transfer learning scenarios that may involve the distribution shift between upstream and do",
    "link": "https://arxiv.org/abs/2403.07282",
    "context": "Title: Enhancing Transfer Learning with Flexible Nonparametric Posterior Sampling\nAbstract: arXiv:2403.07282v1 Announce Type: new  Abstract: Transfer learning has recently shown significant performance across various tasks involving deep neural networks. In these transfer learning scenarios, the prior distribution for downstream data becomes crucial in Bayesian model averaging (BMA). While previous works proposed the prior over the neural network parameters centered around the pre-trained solution, such strategies have limitations when dealing with distribution shifts between upstream and downstream data. This paper introduces nonparametric transfer learning (NPTL), a flexible posterior sampling method to address the distribution shift issue within the context of nonparametric learning. The nonparametric learning (NPL) method is a recent approach that employs a nonparametric prior for posterior sampling, efficiently accounting for model misspecification scenarios, which is suitable for transfer learning scenarios that may involve the distribution shift between upstream and do",
    "path": "papers/24/03/2403.07282.json",
    "total_tokens": 851,
    "translated_title": "通过灵活的非参数后验抽样增强迁移学习",
    "translated_abstract": "近期工作表明，涉及深度神经网络的各种任务中，迁移学习显示出了显著的性能。在这些迁移学习场景中，下游数据的先验分布在贝叶斯模型平均（BMA）中变得至关重要。虽然先前的工作提出了以围绕预训练解决方案为中心的神经网络参数的先验，但在处理上游数据和下游数据之间的分布转移时，这样的策略存在局限性。本文介绍了非参数迁移学习（NPTL），一种灵活的后验抽样方法，以解决在非参数学习背景下的分布转移问题。非参数学习（NPL）方法是一种最近引入的方法，它采用了非参数先验进行后验抽样，有效地解决了模型错误规范情况，适用于涉及上游和下游数据之间分布转移的迁移学习场景。",
    "tldr": "这项研究引入了灵活的非参数后验抽样方法，命名为非参数迁移学习（NPTL），用以解决在非参数学习背景下的分布转移问题，并提高了迁移学习的性能。",
    "en_tdlr": "This study introduces a flexible nonparametric posterior sampling method, named Nonparametric Transfer Learning (NPTL), to address distribution shift issues in the nonparametric learning context and enhances the performance of transfer learning."
}