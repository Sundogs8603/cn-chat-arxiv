{
    "title": "Graph Expansion in Pruned Recurrent Neural Network Layers Preserve Performance",
    "abstract": "arXiv:2403.11100v1 Announce Type: new  Abstract: Expansion property of a graph refers to its strong connectivity as well as sparseness. It has been reported that deep neural networks can be pruned to a high degree of sparsity while maintaining their performance. Such pruning is essential for performing real time sequence learning tasks using recurrent neural networks in resource constrained platforms. We prune recurrent networks such as RNNs and LSTMs, maintaining a large spectral gap of the underlying graphs and ensuring their layerwise expansion properties. We also study the time unfolded recurrent network graphs in terms of the properties of their bipartite layers. Experimental results for the benchmark sequence MNIST, CIFAR-10, and Google speech command data show that expander graph properties are key to preserving classification accuracy of RNN and LSTM.",
    "link": "https://arxiv.org/abs/2403.11100",
    "context": "Title: Graph Expansion in Pruned Recurrent Neural Network Layers Preserve Performance\nAbstract: arXiv:2403.11100v1 Announce Type: new  Abstract: Expansion property of a graph refers to its strong connectivity as well as sparseness. It has been reported that deep neural networks can be pruned to a high degree of sparsity while maintaining their performance. Such pruning is essential for performing real time sequence learning tasks using recurrent neural networks in resource constrained platforms. We prune recurrent networks such as RNNs and LSTMs, maintaining a large spectral gap of the underlying graphs and ensuring their layerwise expansion properties. We also study the time unfolded recurrent network graphs in terms of the properties of their bipartite layers. Experimental results for the benchmark sequence MNIST, CIFAR-10, and Google speech command data show that expander graph properties are key to preserving classification accuracy of RNN and LSTM.",
    "path": "papers/24/03/2403.11100.json",
    "total_tokens": 774,
    "translated_title": "在修剪的循环神经网络层中保持性能的图扩展",
    "translated_abstract": "图的扩展性质指的是其强连通性和稀疏性。已经报告称可以对深度神经网络进行修剪，使其在保持性能的同时具有高度的稀疏性。这种修剪对于在资源受限平台上使用循环神经网络执行实时序列学习任务至关重要。我们对诸如RNN和LSTM的循环网络进行修剪，保持底层图的较大谱间隔，并确保它们的逐层扩展性质。我们还从其二分图层面研究时间展开的循环网络图的性质。针对基准序列MNIST、CIFAR-10和Google语音命令数据的实验结果表明，扩展器图形属性是保持RNN和LSTM分类准确性的关键。",
    "tldr": "图扩展的性质是该研究的关键，研究发现在修剪的循环神经网络层中保持图的扩展性能可以维持RNN和LSTM的分类准确性。",
    "en_tdlr": "The key finding of this study is that preserving graph expansion properties in pruned recurrent neural network layers can maintain the classification accuracy of RNN and LSTM."
}