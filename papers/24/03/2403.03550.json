{
    "title": "Emotional Manipulation Through Prompt Engineering Amplifies Disinformation Generation in AI Large Language Models",
    "abstract": "arXiv:2403.03550v1 Announce Type: new  Abstract: This study investigates the generation of synthetic disinformation by OpenAI's Large Language Models (LLMs) through prompt engineering and explores their responsiveness to emotional prompting. Leveraging various LLM iterations using davinci-002, davinci-003, gpt-3.5-turbo and gpt-4, we designed experiments to assess their success in producing disinformation. Our findings, based on a corpus of 19,800 synthetic disinformation social media posts, reveal that all LLMs by OpenAI can successfully produce disinformation, and that they effectively respond to emotional prompting, indicating their nuanced understanding of emotional cues in text generation. When prompted politely, all examined LLMs consistently generate disinformation at a high frequency. Conversely, when prompted impolitely, the frequency of disinformation production diminishes, as the models often refuse to generate disinformation and instead caution users that the tool is not in",
    "link": "https://arxiv.org/abs/2403.03550",
    "context": "Title: Emotional Manipulation Through Prompt Engineering Amplifies Disinformation Generation in AI Large Language Models\nAbstract: arXiv:2403.03550v1 Announce Type: new  Abstract: This study investigates the generation of synthetic disinformation by OpenAI's Large Language Models (LLMs) through prompt engineering and explores their responsiveness to emotional prompting. Leveraging various LLM iterations using davinci-002, davinci-003, gpt-3.5-turbo and gpt-4, we designed experiments to assess their success in producing disinformation. Our findings, based on a corpus of 19,800 synthetic disinformation social media posts, reveal that all LLMs by OpenAI can successfully produce disinformation, and that they effectively respond to emotional prompting, indicating their nuanced understanding of emotional cues in text generation. When prompted politely, all examined LLMs consistently generate disinformation at a high frequency. Conversely, when prompted impolitely, the frequency of disinformation production diminishes, as the models often refuse to generate disinformation and instead caution users that the tool is not in",
    "path": "papers/24/03/2403.03550.json",
    "total_tokens": 908,
    "translated_title": "通过提示工程加大了人工智能大语言模型中生成虚假信息的情绪操纵",
    "translated_abstract": "这项研究通过提示工程调查了OpenAI的大型语言模型（LLMs）生成合成虚假信息，并探讨了它们对情绪提示的响应。利用davinci-002、davinci-003、gpt-3.5-turbo和gpt-4等各种LLM迭代，我们设计了实验来评估它们在生成虚假信息方面的成功率。基于一组包含19,800条合成虚假社交媒体帖子的语料库，我们的研究发现OpenAI的所有LLMs都能成功生成虚假信息，并且它们有效地响应情绪提示，表明它们对文本生成中的情绪线索有微妙的理解。在礼貌提示下，所有研究中的LLMs都以高频率一致地生成虚假信息。相反，当以不礼貌方式提示时，虚假信息生成的频率会降低，因为这些模型通常会拒绝生成虚假信息，而是警告用户该工具不适用",
    "tldr": "研究通过提示工程和情绪操纵研究了OpenAI的大型语言模型在生成虚假信息方面的反应，发现它们能够成功生成虚假信息并对情绪提示有细致的理解。",
    "en_tdlr": "This study explores OpenAI's Large Language Models' generation of synthetic disinformation through prompt engineering and emotional prompting, finding that they can successfully produce disinformation and demonstrate nuanced understanding of emotional cues."
}