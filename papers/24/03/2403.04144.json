{
    "title": "FedClust: Optimizing Federated Learning on Non-IID Data through Weight-Driven Client Clustering",
    "abstract": "arXiv:2403.04144v1 Announce Type: cross  Abstract: Federated learning (FL) is an emerging distributed machine learning paradigm enabling collaborative model training on decentralized devices without exposing their local data. A key challenge in FL is the uneven data distribution across client devices, violating the well-known assumption of independent-and-identically-distributed (IID) training samples in conventional machine learning. Clustered federated learning (CFL) addresses this challenge by grouping clients based on the similarity of their data distributions. However, existing CFL approaches require a large number of communication rounds for stable cluster formation and rely on a predefined number of clusters, thus limiting their flexibility and adaptability. This paper proposes FedClust, a novel CFL approach leveraging correlations between local model weights and client data distributions. FedClust groups clients into clusters in a one-shot manner using strategically selected pa",
    "link": "https://arxiv.org/abs/2403.04144",
    "context": "Title: FedClust: Optimizing Federated Learning on Non-IID Data through Weight-Driven Client Clustering\nAbstract: arXiv:2403.04144v1 Announce Type: cross  Abstract: Federated learning (FL) is an emerging distributed machine learning paradigm enabling collaborative model training on decentralized devices without exposing their local data. A key challenge in FL is the uneven data distribution across client devices, violating the well-known assumption of independent-and-identically-distributed (IID) training samples in conventional machine learning. Clustered federated learning (CFL) addresses this challenge by grouping clients based on the similarity of their data distributions. However, existing CFL approaches require a large number of communication rounds for stable cluster formation and rely on a predefined number of clusters, thus limiting their flexibility and adaptability. This paper proposes FedClust, a novel CFL approach leveraging correlations between local model weights and client data distributions. FedClust groups clients into clusters in a one-shot manner using strategically selected pa",
    "path": "papers/24/03/2403.04144.json",
    "total_tokens": 873,
    "translated_title": "FedClust：通过基于权重驱动的客户端聚类优化非独立同分布数据上的联邦学习",
    "translated_abstract": "联邦学习（FL）是一种新兴的分布式机器学习范式，实现了在分散设备上进行协作模型训练，并不会暴露它们的本地数据。在FL中，一个关键挑战是客户端设备之间的数据分布不均匀，违反了传统机器学习中独立同分布（IID）训练样本的假设。集群化的联邦学习（CFL）通过根据数据分布的相似性对客户端进行分组来解决这一挑战。然而，现有的CFL方法需要大量的通信往返来稳定集群形成，并且依赖预定义的集群数量，从而限制了它们的灵活性和适应性。本文提出了FedClust，一种新颖的CFL方法，利用本地模型权重与客户端数据分布之间的相关性。FedClust使用策略性选择的参数一次性将客户端分组成集群。",
    "tldr": "该论文提出了FedClust，一种新颖的集群化联邦学习方法，通过利用本地模型权重和客户端数据分布之间的相关性，在一次性的操作中将客户端分组成集群。",
    "en_tdlr": "This paper introduces FedClust, a novel clustered federated learning method that groups clients into clusters in a one-shot manner by leveraging correlations between local model weights and client data distributions."
}