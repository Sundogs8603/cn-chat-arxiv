{
    "title": "Towards Optimal Customized Architecture for Heterogeneous Federated Learning with Contrastive Cloud-Edge Model Decoupling",
    "abstract": "arXiv:2403.02360v1 Announce Type: cross  Abstract: Federated learning, as a promising distributed learning paradigm, enables collaborative training of a global model across multiple network edge clients without the need for central data collecting. However, the heterogeneity of edge data distribution drags the model towards the local minima, which can be distant from the global optimum. Such heterogeneity often leads to slow convergence and substantial communication overhead. To address these issues, we propose a novel federated learning framework called FedCMD, a model decoupling tailored to the Cloud-edge supported federated learning that separates deep neural networks into a body for capturing shared representations in Cloud and a personalized head for migrating data heterogeneity. Our motivation is that, by the deep investigation of the performance of selecting different neural network layers as the personalized head, we found rigidly assigning the last layer as the personalized he",
    "link": "https://arxiv.org/abs/2403.02360",
    "context": "Title: Towards Optimal Customized Architecture for Heterogeneous Federated Learning with Contrastive Cloud-Edge Model Decoupling\nAbstract: arXiv:2403.02360v1 Announce Type: cross  Abstract: Federated learning, as a promising distributed learning paradigm, enables collaborative training of a global model across multiple network edge clients without the need for central data collecting. However, the heterogeneity of edge data distribution drags the model towards the local minima, which can be distant from the global optimum. Such heterogeneity often leads to slow convergence and substantial communication overhead. To address these issues, we propose a novel federated learning framework called FedCMD, a model decoupling tailored to the Cloud-edge supported federated learning that separates deep neural networks into a body for capturing shared representations in Cloud and a personalized head for migrating data heterogeneity. Our motivation is that, by the deep investigation of the performance of selecting different neural network layers as the personalized head, we found rigidly assigning the last layer as the personalized he",
    "path": "papers/24/03/2403.02360.json",
    "total_tokens": 926,
    "translated_title": "面向异构联邦学习的定制架构研究: 对比云边模型解耦",
    "translated_abstract": "联邦学习作为一种有前途的分布式学习范例，实现了多个网络边缘客户端之间全局模型的协作训练，而无需进行中央数据收集。然而，边缘数据分布的异构性使得模型倾向于局部极小值，这些极小值可能远离全局最优。这种异构性通常导致收敛速度缓慢且通信开销巨大。为解决这些问题，我们提出了一种名为FedCMD的新型联邦学习框架，即适应云边支持的联邦学习的模型解耦，将深度神经网络分为用于获取云端共享表示的主体和用于迁移数据异构性的个性化头部。我们的动机是，通过深入研究选择不同神经网络层作为个性化头部的性能，发现将最后一层刚性分配为个性化头部可能无法最大化改善异构性数据性能，从而提出了一种自动化选择个性化头部的方法。",
    "tldr": "通过对比云边模型解耦，提出了一种为异构联邦学习定制架构，该架构将深度神经网络分为捕获共享表示的主体和处理数据异构性的个性化头部，以优化联邦学习性能。",
    "en_tdlr": "A customized architecture for heterogeneous federated learning is proposed by decoupling the cloud-edge model, separating deep neural networks into a body for capturing shared representations and a personalized head for handling data heterogeneity to optimize federated learning performance."
}