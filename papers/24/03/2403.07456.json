{
    "title": "A tutorial on multi-view autoencoders using the multi-view-AE library",
    "abstract": "arXiv:2403.07456v1 Announce Type: new  Abstract: There has been a growing interest in recent years in modelling multiple modalities (or views) of data to for example, understand the relationship between modalities or to generate missing data. Multi-view autoencoders have gained significant traction for their adaptability and versatility in modelling multi-modal data, demonstrating an ability to tailor their approach to suit the characteristics of the data at hand. However, most multi-view autoencoders have inconsistent notation and are often implemented using different coding frameworks. To address this, we present a unified mathematical framework for multi-view autoencoders, consolidating their formulations. Moreover, we offer insights into the motivation and theoretical advantages of each model. To facilitate accessibility and practical use, we extend the documentation and functionality of the previously introduced \\texttt{multi-view-AE} library. This library offers Python implementa",
    "link": "https://arxiv.org/abs/2403.07456",
    "context": "Title: A tutorial on multi-view autoencoders using the multi-view-AE library\nAbstract: arXiv:2403.07456v1 Announce Type: new  Abstract: There has been a growing interest in recent years in modelling multiple modalities (or views) of data to for example, understand the relationship between modalities or to generate missing data. Multi-view autoencoders have gained significant traction for their adaptability and versatility in modelling multi-modal data, demonstrating an ability to tailor their approach to suit the characteristics of the data at hand. However, most multi-view autoencoders have inconsistent notation and are often implemented using different coding frameworks. To address this, we present a unified mathematical framework for multi-view autoencoders, consolidating their formulations. Moreover, we offer insights into the motivation and theoretical advantages of each model. To facilitate accessibility and practical use, we extend the documentation and functionality of the previously introduced \\texttt{multi-view-AE} library. This library offers Python implementa",
    "path": "papers/24/03/2403.07456.json",
    "total_tokens": 818,
    "translated_title": "使用 multi-view-AE 库的多视图自编码器教程",
    "translated_abstract": "近年来，对建模数据的多个模态（或视图）以便理解模态之间的关系或生成缺失数据引起了越来越多的关注。多视图自编码器因其能够适应和灵活建模多模态数据的能力而备受关注，表明其具有根据手头数据特征调整方法的能力。然而，大多数多视图自编码器存在一致性符号标注不一的问题，并且通常使用不同的编码框架实现。为解决这个问题，我们提出了一个统一的多视图自编码器数学框架，整合了它们的公式。此外，我们提供了对每个模型动机和理论优势的见解。为了方便访问和实际使用，我们扩展了先前介绍的 multi-view-AE 库的文档和功能。该库提供了 Python 实现。",
    "tldr": "提出了一个统一的多视图自编码器数学框架，整合了各种公式，并拓展了 \\texttt{multi-view-AE} 库的文档和功能。",
    "en_tdlr": "A unified mathematical framework for multi-view autoencoders, consolidating various formulations, and an extension of the documentation and functionality of the \\texttt{multi-view-AE} library were presented to address the inconsistent notation and different coding frameworks in current multi-view autoencoder implementations."
}