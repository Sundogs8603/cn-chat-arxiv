{
    "title": "Read between the lines -- Functionality Extraction From READMEs",
    "abstract": "arXiv:2403.10205v1 Announce Type: cross  Abstract: While text summarization is a well-known NLP task, in this paper, we introduce a novel and useful variant of it called functionality extraction from Git README files. Though this task is a text2text generation at an abstract level, it involves its own peculiarities and challenges making existing text2text generation systems not very useful. The motivation behind this task stems from a recent surge in research and development activities around the use of large language models for code-related tasks, such as code refactoring, code summarization, etc. We also release a human-annotated dataset called FuncRead, and develop a battery of models for the task. Our exhaustive experimentation shows that small size fine-tuned models beat any baseline models that can be designed using popular black-box or white-box large language models (LLMs) such as ChatGPT and Bard. Our best fine-tuned 7 Billion CodeLlama model exhibit 70% and 20% gain on the F1",
    "link": "https://arxiv.org/abs/2403.10205",
    "context": "Title: Read between the lines -- Functionality Extraction From READMEs\nAbstract: arXiv:2403.10205v1 Announce Type: cross  Abstract: While text summarization is a well-known NLP task, in this paper, we introduce a novel and useful variant of it called functionality extraction from Git README files. Though this task is a text2text generation at an abstract level, it involves its own peculiarities and challenges making existing text2text generation systems not very useful. The motivation behind this task stems from a recent surge in research and development activities around the use of large language models for code-related tasks, such as code refactoring, code summarization, etc. We also release a human-annotated dataset called FuncRead, and develop a battery of models for the task. Our exhaustive experimentation shows that small size fine-tuned models beat any baseline models that can be designed using popular black-box or white-box large language models (LLMs) such as ChatGPT and Bard. Our best fine-tuned 7 Billion CodeLlama model exhibit 70% and 20% gain on the F1",
    "path": "papers/24/03/2403.10205.json",
    "total_tokens": 892,
    "translated_title": "从 README 中提取功能",
    "translated_abstract": "虽然文本摘要是一项众所周知的自然语言处理任务，但在本文中，我们介绍了一种称为从 Git README 文件中提取功能的新颖而有用的变体。虽然这个任务在抽象层面上是一个文本生成任务，但它涉及到自己的特殊性和挑战，使得现有的文本生成系统并不十分有用。这一任务的动机源自最近围绕着使用大型语言模型进行代码相关任务（如代码重构、代码摘要等）的研究和开发活动的激增。我们还发布了一个名为FuncRead的人工注释数据集，并为这一任务开发了一系列模型。我们进行了详尽的实验，结果表明，小型微调模型击败了可以使用流行的黑盒或白盒大型语言模型（LLMs）（如ChatGPT和Bard）设计的任何基线模型。我们的最佳微调的70亿CodeLlama模型在F1上取得了70%和20%的增益。",
    "tldr": "本文介绍了一种新颖的文本处理任务——从 Git README 文件中提取功能，研究动机源自对大型语言模型在代码相关任务中应用的兴趣，通过开发小型微调模型，取得了70%和20%的性能提升。",
    "en_tdlr": "This paper introduces a novel text processing task - functionality extraction from Git README files, motivated by the interest in applying large language models to code-related tasks, achieving 70% and 20% performance improvement by developing small fine-tuned models."
}