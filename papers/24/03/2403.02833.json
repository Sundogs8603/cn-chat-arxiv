{
    "title": "SOFIM: Stochastic Optimization Using Regularized Fisher Information Matrix",
    "abstract": "arXiv:2403.02833v1 Announce Type: new  Abstract: This paper introduces a new stochastic optimization method based on the regularized Fisher information matrix (FIM), named SOFIM, which can efficiently utilize the FIM to approximate the Hessian matrix for finding Newton's gradient update in large-scale stochastic optimization of machine learning models. It can be viewed as a variant of natural gradient descent (NGD), where the challenge of storing and calculating the full FIM is addressed through making use of the regularized FIM and directly finding the gradient update direction via Sherman-Morrison matrix inversion. Additionally, like the popular Adam method, SOFIM uses the first moment of the gradient to address the issue of non-stationary objectives across mini-batches due to heterogeneous data. The utilization of the regularized FIM and Sherman-Morrison matrix inversion leads to the improved convergence rate with the same space and time complexities as stochastic gradient descent (",
    "link": "https://arxiv.org/abs/2403.02833",
    "context": "Title: SOFIM: Stochastic Optimization Using Regularized Fisher Information Matrix\nAbstract: arXiv:2403.02833v1 Announce Type: new  Abstract: This paper introduces a new stochastic optimization method based on the regularized Fisher information matrix (FIM), named SOFIM, which can efficiently utilize the FIM to approximate the Hessian matrix for finding Newton's gradient update in large-scale stochastic optimization of machine learning models. It can be viewed as a variant of natural gradient descent (NGD), where the challenge of storing and calculating the full FIM is addressed through making use of the regularized FIM and directly finding the gradient update direction via Sherman-Morrison matrix inversion. Additionally, like the popular Adam method, SOFIM uses the first moment of the gradient to address the issue of non-stationary objectives across mini-batches due to heterogeneous data. The utilization of the regularized FIM and Sherman-Morrison matrix inversion leads to the improved convergence rate with the same space and time complexities as stochastic gradient descent (",
    "path": "papers/24/03/2403.02833.json",
    "total_tokens": 868,
    "translated_title": "SOFIM: 使用正则化Fisher信息矩阵的随机优化",
    "translated_abstract": "这篇论文介绍了一种新的基于正则化Fisher信息矩阵（FIM）的随机优化方法，称为SOFIM，可以有效利用FIM来逼近Hessian矩阵，以找到大规模随机优化机器学习模型中的牛顿梯度更新。可以视为自然梯度下降（NGD）的一种变体，通过使用正则化FIM和直接通过Sherman-Morrison矩阵求逆找到梯度更新方向来解决存储和计算完整FIM的挑战。此外，像广受欢迎的Adam方法一样，SOFIM利用梯度的第一时刻来处理由异构数据引起的跨小批次非平稳目标的问题。正则化FIM和Sherman-Morrison矩阵求逆的利用导致了收敛速率的改善，同时space和time复杂度与随机梯度下降相同。",
    "tldr": "SOFIM利用正则化Fisher信息矩阵和Sherman-Morrison矩阵求逆改善了大规模随机优化中梯度更新的收敛速率，同时解决了数据异质性带来的非平稳目标问题。",
    "en_tdlr": "SOFIM improves the convergence rate of gradient updates in large-scale stochastic optimization by utilizing the regularized Fisher information matrix and Sherman-Morrison matrix inversion, while addressing the issue of non-stationary objectives due to heterogeneous data."
}