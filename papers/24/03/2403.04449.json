{
    "title": "Feedback-Generation for Programming Exercises With GPT-4",
    "abstract": "arXiv:2403.04449v1 Announce Type: new  Abstract: Ever since Large Language Models (LLMs) and related applications have become broadly available, several studies investigated their potential for assisting educators and supporting students in higher education. LLMs such as Codex, GPT-3.5, and GPT 4 have shown promising results in the context of large programming courses, where students can benefit from feedback and hints if provided timely and at scale. This paper explores the quality of GPT-4 Turbo's generated output for prompts containing both the programming task specification and a student's submission as input. Two assignments from an introductory programming course were selected, and GPT-4 was asked to generate feedback for 55 randomly chosen, authentic student programming submissions. The output was qualitatively analyzed regarding correctness, personalization, fault localization, and other features identified in the material. Compared to prior work and analyses of GPT-3.5, GPT-4 ",
    "link": "https://arxiv.org/abs/2403.04449",
    "context": "Title: Feedback-Generation for Programming Exercises With GPT-4\nAbstract: arXiv:2403.04449v1 Announce Type: new  Abstract: Ever since Large Language Models (LLMs) and related applications have become broadly available, several studies investigated their potential for assisting educators and supporting students in higher education. LLMs such as Codex, GPT-3.5, and GPT 4 have shown promising results in the context of large programming courses, where students can benefit from feedback and hints if provided timely and at scale. This paper explores the quality of GPT-4 Turbo's generated output for prompts containing both the programming task specification and a student's submission as input. Two assignments from an introductory programming course were selected, and GPT-4 was asked to generate feedback for 55 randomly chosen, authentic student programming submissions. The output was qualitatively analyzed regarding correctness, personalization, fault localization, and other features identified in the material. Compared to prior work and analyses of GPT-3.5, GPT-4 ",
    "path": "papers/24/03/2403.04449.json",
    "total_tokens": 823,
    "translated_title": "使用GPT-4生成编程练习反馈",
    "translated_abstract": "自从大型语言模型（LLMs）及相关应用广泛可用以来，有几项研究调查了它们在协助教育工作者和支持高等教育学生方面的潜力。像Codex、GPT-3.5和GPT 4这样的LLMs在大型编程课程的背景下展示了有希望的结果，学生可以从及时且大规模提供的反馈和提示中受益。本文探讨了GPT-4 Turbo为包含编程任务规范和学生提交内容的提示生成输出的质量。从一门入门级编程课程中选择了两项作业，并要求GPT-4为随机选择的55份真实学生编程提交生成反馈。对输出进行了关于正确性、个性化、故障定位和材料中识别的其他特征的定性分析。与之前的工作以及GPT-3.5的分析相比，GPT-4",
    "tldr": "本文研究了GPT-4生成编程练习反馈的质量，对包含编程任务规范和学生提交内容的提示进行了分析。",
    "en_tdlr": "This paper explores the quality of GPT-4 Turbo's generated feedback for programming exercises by analyzing prompts containing both the programming task specification and a student's submission."
}