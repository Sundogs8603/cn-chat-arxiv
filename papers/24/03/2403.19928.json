{
    "title": "DiJiang: Efficient Large Language Models through Compact Kernelization",
    "abstract": "arXiv:2403.19928v1 Announce Type: new  Abstract: In an effort to reduce the computational load of Transformers, research on linear attention has gained significant momentum. However, the improvement strategies for attention mechanisms typically necessitate extensive retraining, which is impractical for large language models with a vast array of parameters. In this paper, we present DiJiang, a novel Frequency Domain Kernelization approach that enables the transformation of a pre-trained vanilla Transformer into a linear complexity model with little training costs. By employing a weighted Quasi-Monte Carlo method for sampling, the proposed approach theoretically offers superior approximation efficiency. To further reduce the training computational complexity, our kernelization is based on Discrete Cosine Transform (DCT) operations. Extensive experiments demonstrate that the proposed method achieves comparable performance to the original Transformer, but with significantly reduced trainin",
    "link": "https://arxiv.org/abs/2403.19928",
    "context": "Title: DiJiang: Efficient Large Language Models through Compact Kernelization\nAbstract: arXiv:2403.19928v1 Announce Type: new  Abstract: In an effort to reduce the computational load of Transformers, research on linear attention has gained significant momentum. However, the improvement strategies for attention mechanisms typically necessitate extensive retraining, which is impractical for large language models with a vast array of parameters. In this paper, we present DiJiang, a novel Frequency Domain Kernelization approach that enables the transformation of a pre-trained vanilla Transformer into a linear complexity model with little training costs. By employing a weighted Quasi-Monte Carlo method for sampling, the proposed approach theoretically offers superior approximation efficiency. To further reduce the training computational complexity, our kernelization is based on Discrete Cosine Transform (DCT) operations. Extensive experiments demonstrate that the proposed method achieves comparable performance to the original Transformer, but with significantly reduced trainin",
    "path": "papers/24/03/2403.19928.json",
    "total_tokens": 813,
    "translated_title": "DiJiang：通过紧凑的核方法实现高效的大型语言模型",
    "translated_abstract": "为了减少Transformers的计算负荷，线性注意力的研究已经取得了显著的进展。然而，注意机制的改进策略通常需要经过大量的重新训练，在具有大量参数的大型语言模型上是不切实际的。本文介绍了DiJiang，一种新颖的频域核方法，可将预训练的基本Transformer转化为具有较小训练成本的线性复杂度模型。通过采用加权拟随机采样法，所提出的方法在理论上提供了更好的逼近效率。为了进一步降低训练的计算复杂度，我们的核方法基于离散余弦变换（DCT）操作。大量实验证明，所提出的方法达到了与原始Transformer相当的性能，但训练时间大大减少。",
    "tldr": "DiJiang提出了一种新颖的频域核方法，可以将预训练的基本Transformer模型转化为具有线性复杂度的模型，大大减少训练成本，并在理论上提供更好的逼近效率。"
}