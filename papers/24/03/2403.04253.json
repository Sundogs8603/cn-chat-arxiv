{
    "title": "Mastering Memory Tasks with World Models",
    "abstract": "arXiv:2403.04253v1 Announce Type: new  Abstract: Current model-based reinforcement learning (MBRL) agents struggle with long-term dependencies. This limits their ability to effectively solve tasks involving extended time gaps between actions and outcomes, or tasks demanding the recalling of distant observations to inform current actions. To improve temporal coherence, we integrate a new family of state space models (SSMs) in world models of MBRL agents to present a new method, Recall to Imagine (R2I). This integration aims to enhance both long-term memory and long-horizon credit assignment. Through a diverse set of illustrative tasks, we systematically demonstrate that R2I not only establishes a new state-of-the-art for challenging memory and credit assignment RL tasks, such as BSuite and POPGym, but also showcases superhuman performance in the complex memory domain of Memory Maze. At the same time, it upholds comparable performance in classic RL tasks, such as Atari and DMC, suggestin",
    "link": "https://arxiv.org/abs/2403.04253",
    "context": "Title: Mastering Memory Tasks with World Models\nAbstract: arXiv:2403.04253v1 Announce Type: new  Abstract: Current model-based reinforcement learning (MBRL) agents struggle with long-term dependencies. This limits their ability to effectively solve tasks involving extended time gaps between actions and outcomes, or tasks demanding the recalling of distant observations to inform current actions. To improve temporal coherence, we integrate a new family of state space models (SSMs) in world models of MBRL agents to present a new method, Recall to Imagine (R2I). This integration aims to enhance both long-term memory and long-horizon credit assignment. Through a diverse set of illustrative tasks, we systematically demonstrate that R2I not only establishes a new state-of-the-art for challenging memory and credit assignment RL tasks, such as BSuite and POPGym, but also showcases superhuman performance in the complex memory domain of Memory Maze. At the same time, it upholds comparable performance in classic RL tasks, such as Atari and DMC, suggestin",
    "path": "papers/24/03/2403.04253.json",
    "total_tokens": 930,
    "translated_title": "用世界模型掌握记忆任务",
    "translated_abstract": "当前基于模型的强化学习（MBRL）代理在处理长期依赖性方面存在困难，这限制了它们有效解决涉及动作和结果之间存在时间间隔或需要回想远距离观察以指导当前动作的任务的能力。为了改善时间上的一致性，我们将一类新的状态空间模型（SSMs）整合到MBRL代理的世界模型中，提出了一种新方法，Recall to Imagine（R2I）。这种整合旨在增强长期记忆和长距离奖励分配。通过一系列多样化的示例任务，我们系统地展示了R2I不仅在具有挑战性的记忆和奖励分配强化学习任务中确立了一个新的最先进水平，如BSuite和POPGym，还展示了在Memory Maze这一复杂的记忆领域中超越人类的表现。同时，在经典的强化学习任务，例如Atari和DMC中，它保持了可以媲美的性能。",
    "tldr": "通过将一类新的状态空间模型整合到基于模型的强化学习代理的世界模型中，提出了一种新方法，Recall to Imagine（R2I），旨在增强长期记忆和长距离奖励分配，实现了在记忆任务和奖励分配方面的超越表现。",
    "en_tdlr": "By integrating a new family of state space models into the world models of model-based reinforcement learning agents, a new method called Recall to Imagine (R2I) is proposed to enhance long-term memory and long-horizon credit assignment, achieving superhuman performance in memory tasks and credit assignment."
}