{
    "title": "A Dataset for the Validation of Truth Inference Algorithms Suitable for Online Deployment",
    "abstract": "arXiv:2403.08826v1 Announce Type: cross  Abstract: For the purpose of efficient and cost-effective large-scale data labeling, crowdsourcing is increasingly being utilized. To guarantee the quality of data labeling, multiple annotations need to be collected for each data sample, and truth inference algorithms have been developed to accurately infer the true labels. Despite previous studies having released public datasets to evaluate the efficacy of truth inference algorithms, these have typically focused on a single type of crowdsourcing task and neglected the temporal information associated with workers' annotation activities. These limitations significantly restrict the practical applicability of these algorithms, particularly in the context of long-term and online truth inference. In this paper, we introduce a substantial crowdsourcing annotation dataset collected from a real-world crowdsourcing platform. This dataset comprises approximately two thousand workers, one million tasks, a",
    "link": "https://arxiv.org/abs/2403.08826",
    "context": "Title: A Dataset for the Validation of Truth Inference Algorithms Suitable for Online Deployment\nAbstract: arXiv:2403.08826v1 Announce Type: cross  Abstract: For the purpose of efficient and cost-effective large-scale data labeling, crowdsourcing is increasingly being utilized. To guarantee the quality of data labeling, multiple annotations need to be collected for each data sample, and truth inference algorithms have been developed to accurately infer the true labels. Despite previous studies having released public datasets to evaluate the efficacy of truth inference algorithms, these have typically focused on a single type of crowdsourcing task and neglected the temporal information associated with workers' annotation activities. These limitations significantly restrict the practical applicability of these algorithms, particularly in the context of long-term and online truth inference. In this paper, we introduce a substantial crowdsourcing annotation dataset collected from a real-world crowdsourcing platform. This dataset comprises approximately two thousand workers, one million tasks, a",
    "path": "papers/24/03/2403.08826.json",
    "total_tokens": 771,
    "translated_title": "适用于在线部署的真相推断算法验证数据集",
    "translated_abstract": "为了实现高效和具有成本效益的大规模数据标注，越来越多地利用众包。为了保证数据标注的质量，需要为每个数据样本收集多个注释，并开发了真相推断算法来准确推断真实标签。尽管先前的研究已经发布了用于评估真相推断算法有效性的公共数据集，但这些数据集通常集中在单一类型的众包任务，并忽略了与工作者注释活动相关的时间信息。这些限制严重限制了这些算法的实际适用性，特别是在长期和在线真相推断的背景下。本文介绍了从一家真实众包平台收集的大量众包标注数据集。该数据集包括约两千名工作者、一百万个任务、一",
    "tldr": "介绍了一个大规模真实众包标注数据集，弥补了先前真相推断算法验证数据集的局限性，为长期和在线真相推断提供了实用性。",
    "en_tdlr": "Introduces a large-scale real-world crowdsourcing annotation dataset to address the limitations of previous datasets for validating truth inference algorithms, enabling practical long-term and online truth inference."
}