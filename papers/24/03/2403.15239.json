{
    "title": "Guided Decoding for Robot Motion Generation and Adaption",
    "abstract": "arXiv:2403.15239v1 Announce Type: cross  Abstract: We address motion generation for high-DoF robot arms in complex settings with obstacles, via points, etc. A significant advancement in this domain is achieved by integrating Learning from Demonstration (LfD) into the motion generation process. This integration facilitates rapid adaptation to new tasks and optimizes the utilization of accumulated expertise by allowing robots to learn and generalize from demonstrated trajectories.   We train a transformer architecture on a large dataset of simulated trajectories. This architecture, based on a conditional variational autoencoder transformer, learns essential motion generation skills and adapts these to meet auxiliary tasks and constraints. Our auto-regressive approach enables real-time integration of feedback from the physical system, enhancing the adaptability and efficiency of motion generation. We show that our model can generate motion from initial and target points, but also that it ",
    "link": "https://arxiv.org/abs/2403.15239",
    "context": "Title: Guided Decoding for Robot Motion Generation and Adaption\nAbstract: arXiv:2403.15239v1 Announce Type: cross  Abstract: We address motion generation for high-DoF robot arms in complex settings with obstacles, via points, etc. A significant advancement in this domain is achieved by integrating Learning from Demonstration (LfD) into the motion generation process. This integration facilitates rapid adaptation to new tasks and optimizes the utilization of accumulated expertise by allowing robots to learn and generalize from demonstrated trajectories.   We train a transformer architecture on a large dataset of simulated trajectories. This architecture, based on a conditional variational autoencoder transformer, learns essential motion generation skills and adapts these to meet auxiliary tasks and constraints. Our auto-regressive approach enables real-time integration of feedback from the physical system, enhancing the adaptability and efficiency of motion generation. We show that our model can generate motion from initial and target points, but also that it ",
    "path": "papers/24/03/2403.15239.json",
    "total_tokens": 793,
    "translated_title": "引导解码用于机器人运动生成和适应",
    "translated_abstract": "我们针对具有障碍物、通过点等复杂环境下的高自由度机器人臂运动生成问题进行了探讨。通过将演示学习（LfD）集成到运动生成过程中，取得了该领域的重大进展。这种集成支持机器人快速适应新任务，并通过允许机器人从演示轨迹中学习和泛化来优化积累的经验利用。我们在大量模拟轨迹数据集上训练了一个变分自动编码器变换器的transformer架构。这种基于条件变分自动编码器变换器的架构学习了基本的运动生成技能，并将其调整以满足辅助任务和约束条件。我们的自回归方法实现了物理系统反馈的实时集成，增强了运动生成的适应性和效率。我们展示了我们的模型能够从初始点和目标点生成运动，同时",
    "tldr": "通过将演示学习集成到运动生成中，使机器人能够实时生成适应复杂环境的运动",
    "en_tdlr": "By integrating learning from demonstration into motion generation, robots can generate motions in complex environments in real-time and adapt to new tasks."
}