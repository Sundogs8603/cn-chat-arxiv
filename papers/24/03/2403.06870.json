{
    "title": "Semantic Residual Prompts for Continual Learning",
    "abstract": "arXiv:2403.06870v1 Announce Type: new  Abstract: Prompt-tuning methods for Continual Learning (CL) freeze a large pre-trained model and focus training on a few parameter vectors termed prompts. Most of these methods organize these vectors in a pool of key-value pairs, and use the input image as query to retrieve the prompts (values). However, as keys are learned while tasks progress, the prompting selection strategy is itself subject to catastrophic forgetting, an issue often overlooked by existing approaches. For instance, prompts introduced to accommodate new tasks might end up interfering with previously learned prompts. To make the selection strategy more stable, we ask a foundational model (CLIP) to select our prompt within a two-level adaptation mechanism. Specifically, the first level leverages standard textual prompts for the CLIP textual encoder, leading to stable class prototypes. The second level, instead, uses these prototypes along with the query image as keys to index a s",
    "link": "https://arxiv.org/abs/2403.06870",
    "context": "Title: Semantic Residual Prompts for Continual Learning\nAbstract: arXiv:2403.06870v1 Announce Type: new  Abstract: Prompt-tuning methods for Continual Learning (CL) freeze a large pre-trained model and focus training on a few parameter vectors termed prompts. Most of these methods organize these vectors in a pool of key-value pairs, and use the input image as query to retrieve the prompts (values). However, as keys are learned while tasks progress, the prompting selection strategy is itself subject to catastrophic forgetting, an issue often overlooked by existing approaches. For instance, prompts introduced to accommodate new tasks might end up interfering with previously learned prompts. To make the selection strategy more stable, we ask a foundational model (CLIP) to select our prompt within a two-level adaptation mechanism. Specifically, the first level leverages standard textual prompts for the CLIP textual encoder, leading to stable class prototypes. The second level, instead, uses these prototypes along with the query image as keys to index a s",
    "path": "papers/24/03/2403.06870.json",
    "total_tokens": 782,
    "translated_title": "语义剩余提示用于持续学习",
    "translated_abstract": "持续学习（CL）的提示调整方法冻结了一个大型预训练模型，并侧重于训练一些称为提示的参数向量。这些方法中的大多数将这些向量组织在一个键-值对池中，并使用输入图像作为查询来检索提示（值）。然而，随着任务的进行，由于键是学习的，提示选择策略本身也会面临灾难性遗忘，这是现有方法经常忽视的问题。为了使选择策略更加稳定，我们请求一个基础模型（CLIP）来在两级适应机制中选择我们的提示。具体而言，第一级利用标准文本提示来调整CLIP文本编码器，形成稳定的类原型。而第二级则将这些原型与查询图像一起用作键来索引一个s",
    "tldr": "通过引入语义剩余提示，作者提出了一种稳定的选择策略，利用两级适应机制来在持续学习中解决提示冲突的问题。",
    "en_tdlr": "The authors propose a stable selection strategy by introducing semantic residual prompts, leveraging a two-level adaptation mechanism to address prompt interference in continual learning."
}