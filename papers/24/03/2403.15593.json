{
    "title": "FairerCLIP: Debiasing CLIP's Zero-Shot Predictions using Functions in RKHSs",
    "abstract": "arXiv:2403.15593v1 Announce Type: cross  Abstract: Large pre-trained vision-language models such as CLIP provide compact and general-purpose representations of text and images that are demonstrably effective across multiple downstream zero-shot prediction tasks. However, owing to the nature of their training process, these models have the potential to 1) propagate or amplify societal biases in the training data and 2) learn to rely on spurious features. This paper proposes FairerCLIP, a general approach for making zero-shot predictions of CLIP more fair and robust to spurious correlations. We formulate the problem of jointly debiasing CLIP's image and text representations in reproducing kernel Hilbert spaces (RKHSs), which affords multiple benefits: 1) Flexibility: Unlike existing approaches, which are specialized to either learn with or without ground-truth labels, FairerCLIP is adaptable to learning in both scenarios. 2) Ease of Optimization: FairerCLIP lends itself to an iterative o",
    "link": "https://arxiv.org/abs/2403.15593",
    "context": "Title: FairerCLIP: Debiasing CLIP's Zero-Shot Predictions using Functions in RKHSs\nAbstract: arXiv:2403.15593v1 Announce Type: cross  Abstract: Large pre-trained vision-language models such as CLIP provide compact and general-purpose representations of text and images that are demonstrably effective across multiple downstream zero-shot prediction tasks. However, owing to the nature of their training process, these models have the potential to 1) propagate or amplify societal biases in the training data and 2) learn to rely on spurious features. This paper proposes FairerCLIP, a general approach for making zero-shot predictions of CLIP more fair and robust to spurious correlations. We formulate the problem of jointly debiasing CLIP's image and text representations in reproducing kernel Hilbert spaces (RKHSs), which affords multiple benefits: 1) Flexibility: Unlike existing approaches, which are specialized to either learn with or without ground-truth labels, FairerCLIP is adaptable to learning in both scenarios. 2) Ease of Optimization: FairerCLIP lends itself to an iterative o",
    "path": "papers/24/03/2403.15593.json",
    "total_tokens": 898,
    "translated_title": "FairerCLIP: 在RKHSs中使用函数去除CLIP的零样本预测偏见",
    "translated_abstract": "大型预训练的视觉-语言模型（如CLIP）提供了文本和图像的紧凑通用表示，已被证明在多个下游零样本预测任务中有效。然而，由于它们训练过程的性质，这些模型可能存在以下问题：1）传播或放大社会偏见和2）学习依赖虚假特征。本文提出FairerCLIP，一种通用方法，使CLIP的零样本预测更加公平且更能抵抗虚假相关性。我们在再生核希尔伯特空间（RKHSs）中联合去偏CLIP的图像和文本表示问题，这带来了多个好处：1）灵活性：与现有方法不同，现有方法要么专门用于学习有地面真相标签的情况，要么专门用于学习没有地面真相标签的情况，FairerCLIP能够适应两种学习情况。2）优化便利性：FairerCLIP对于迭代优化非常合适。",
    "tldr": "FairerCLIP提出了一种在RKHSs中使用函数去除CLIP的零样本预测偏见的通用方法，使得预测更公平且更能抵抗虚假相关性。"
}