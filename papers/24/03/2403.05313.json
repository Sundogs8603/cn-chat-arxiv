{
    "title": "RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation",
    "abstract": "arXiv:2403.05313v1 Announce Type: cross  Abstract: We explore how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination. In particular, the proposed method -- *retrieval-augmented thoughts* (RAT) -- revises each thought step one by one with retrieved information relevant to the task query, the current and the past thought steps, after the initial zero-shot CoT is generated. Applying RAT to GPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on various long-horizon generation tasks; on average of relatively increasing rating scores by 13.63% on code generation, 16.96% on mathematical reasoning, 19.2% on creative writing, and 42.78% on embodied task planning. The demo page can be found at https://craftjarvis.github.io/RAT",
    "link": "https://arxiv.org/abs/2403.05313",
    "context": "Title: RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation\nAbstract: arXiv:2403.05313v1 Announce Type: cross  Abstract: We explore how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination. In particular, the proposed method -- *retrieval-augmented thoughts* (RAT) -- revises each thought step one by one with retrieved information relevant to the task query, the current and the past thought steps, after the initial zero-shot CoT is generated. Applying RAT to GPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on various long-horizon generation tasks; on average of relatively increasing rating scores by 13.63% on code generation, 16.96% on mathematical reasoning, 19.2% on creative writing, and 42.78% on embodied task planning. The demo page can be found at https://craftjarvis.github.io/RAT",
    "path": "papers/24/03/2403.05313.json",
    "total_tokens": 855,
    "translated_title": "RAT：检索增强思维在长视角生成中引发了上下文感知推理",
    "translated_abstract": "我们探讨了如何通过信息检索迭代修订一系列思维，显著改善大型语言模型在长视角生成任务中的推理和生成能力，同时极大减轻了幻觉。具体来说，所提出的方法——*检索增强思维* (RAT)——在生成初始的零射 CoT 后，逐步修订每个思维步骤，与任务查询、当前和过去的思维步骤相关的检索信息。将 RAT 应用于 GPT-3.5、GPT-4 和 CodeLLaMA-7b，在各种长视角生成任务上显著提高它们的性能；平均而言，代码生成评分增加了 13.63%，数学推理增加了 16.96%，创意写作增加了 19.2%，具象任务规划增加了 42.78%。演示页面链接：https://craftjarvis.github.io/RAT",
    "tldr": "RAT方法通过检索增强思维，在长视角生成中改善大型语言模型的推理和生成能力，显著降低了幻觉，并取得了显著的性能提升"
}