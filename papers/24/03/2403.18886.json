{
    "title": "Self-Expansion of Pre-trained Models with Mixture of Adapters for Continual Learning",
    "abstract": "arXiv:2403.18886v1 Announce Type: new  Abstract: Continual learning aims to learn from a stream of continuously arriving data with minimum forgetting of previously learned knowledge. While previous works have explored the effectiveness of leveraging the generalizable knowledge from pre-trained models in continual learning, existing parameter-efficient fine-tuning approaches focus on the use of a predetermined or task-wise set of adapters or prompts. However, these approaches still suffer from forgetting due to task interference on jointly used parameters or restricted flexibility. The reliance on a static model architecture may lead to the allocation of excessive parameters that are not essential or, conversely, inadequate adaptation for downstream tasks, given that the scale and distribution of incoming data are unpredictable in continual learning. We propose Self-Expansion of pre-trained models with Modularized Adaptation (SEMA), a novel fine-tuning approach which automatically decid",
    "link": "https://arxiv.org/abs/2403.18886",
    "context": "Title: Self-Expansion of Pre-trained Models with Mixture of Adapters for Continual Learning\nAbstract: arXiv:2403.18886v1 Announce Type: new  Abstract: Continual learning aims to learn from a stream of continuously arriving data with minimum forgetting of previously learned knowledge. While previous works have explored the effectiveness of leveraging the generalizable knowledge from pre-trained models in continual learning, existing parameter-efficient fine-tuning approaches focus on the use of a predetermined or task-wise set of adapters or prompts. However, these approaches still suffer from forgetting due to task interference on jointly used parameters or restricted flexibility. The reliance on a static model architecture may lead to the allocation of excessive parameters that are not essential or, conversely, inadequate adaptation for downstream tasks, given that the scale and distribution of incoming data are unpredictable in continual learning. We propose Self-Expansion of pre-trained models with Modularized Adaptation (SEMA), a novel fine-tuning approach which automatically decid",
    "path": "papers/24/03/2403.18886.json",
    "total_tokens": 702,
    "translated_title": "使用混合适配器进行预训练模型的自我扩展以实现持续学习",
    "translated_abstract": "持续学习旨在从连续到达的数据流中学习，最大限度地减少先前学到的知识的遗忘。本文提出了一种名为SEMA的新型微调方法，称为自我扩展预训练模型与模块化适配，自动决定...（摘要未完整）",
    "tldr": "提出了一种名为SEMA的新型微调方法，旨在通过自我扩展预训练模型与模块化适配，实现持续学习过程中的最小遗忘，解决先前针对静态模型架构情况下存在的过多参数分配或适应性不足等问题。",
    "en_tdlr": "Proposed a novel fine-tuning approach called SEMA to address the minimum forgetting in continual learning by self-expanding pre-trained models with modularized adaptation, tackling issues such as excessive parameter allocation or inadequate adaptation under static model architectures."
}