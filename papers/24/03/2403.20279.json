{
    "title": "LUQ: Long-text Uncertainty Quantification for LLMs",
    "abstract": "arXiv:2403.20279v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated remarkable capability in a variety of NLP tasks. Despite their effectiveness, these models are prone to generate nonfactual content. Uncertainty Quantification (UQ) is pivotal in enhancing our understanding of a model's confidence in its generated content, thereby aiding in the mitigation of nonfactual outputs. Existing research on UQ predominantly targets short text generation, typically yielding brief, word-limited responses. However, real-world applications frequently necessitate much longer responses. Our study first highlights the limitations of current UQ methods in handling long text generation. We then introduce \\textsc{Luq}, a novel sampling-based UQ approach specifically designed for long text. Our findings reveal that \\textsc{Luq} outperforms existing baseline methods in correlating with the model's factuality scores (negative coefficient of -0.85 observed for Gemini Pro). With \\t",
    "link": "https://arxiv.org/abs/2403.20279",
    "context": "Title: LUQ: Long-text Uncertainty Quantification for LLMs\nAbstract: arXiv:2403.20279v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated remarkable capability in a variety of NLP tasks. Despite their effectiveness, these models are prone to generate nonfactual content. Uncertainty Quantification (UQ) is pivotal in enhancing our understanding of a model's confidence in its generated content, thereby aiding in the mitigation of nonfactual outputs. Existing research on UQ predominantly targets short text generation, typically yielding brief, word-limited responses. However, real-world applications frequently necessitate much longer responses. Our study first highlights the limitations of current UQ methods in handling long text generation. We then introduce \\textsc{Luq}, a novel sampling-based UQ approach specifically designed for long text. Our findings reveal that \\textsc{Luq} outperforms existing baseline methods in correlating with the model's factuality scores (negative coefficient of -0.85 observed for Gemini Pro). With \\t",
    "path": "papers/24/03/2403.20279.json",
    "total_tokens": 842,
    "translated_title": "LUQ：LLM模型的长文本不确定性量化",
    "translated_abstract": "大型语言模型（LLMs）在各种自然语言处理任务中展现出了显著的能力。尽管它们有效，但这些模型倾向于生成非事实内容。不确定性量化（UQ）对于增强我们对模型在生成内容上的信心至关重要，从而有助于减轻非事实输出。现有的UQ研究主要针对短文本生成，通常产生简短的、受词限制的响应。然而，现实世界中的应用往往需要更长的响应。我们的研究首先强调了当前UQ方法在处理长文本生成中的局限性。然后，我们介绍了一种名为\\textsc{Luq}的新型基于抽样的UQ方法，专门设计用于长文本。我们的研究结果显示，\\textsc{Luq}在与模型的事实得分相关方面优于现有的基准方法（Gemini Pro观察到-0.85的负相关系数）。",
    "tldr": "LUQ提出了一种针对长文本设计的新型采样UQ方法，优于现有基准方法在与模型的事实得分相关方面。",
    "en_tdlr": "LUQ proposes a novel sampling-based UQ approach specifically designed for long text, outperforming existing baseline methods in correlating with the model's factuality scores."
}