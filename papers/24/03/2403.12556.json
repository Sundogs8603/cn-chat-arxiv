{
    "title": "Factorized Learning Assisted with Large Language Model for Gloss-free Sign Language Translation",
    "abstract": "arXiv:2403.12556v1 Announce Type: new  Abstract: Previous Sign Language Translation (SLT) methods achieve superior performance by relying on gloss annotations. However, labeling high-quality glosses is a labor-intensive task, which limits the further development of SLT. Although some approaches work towards gloss-free SLT through jointly training the visual encoder and translation network, these efforts still suffer from poor performance and inefficient use of the powerful Large Language Model (LLM). Most seriously, we find that directly introducing LLM into SLT will lead to insufficient learning of visual representations as LLM dominates the learning curve. To address these problems, we propose Factorized Learning assisted with Large Language Model (FLa-LLM) for gloss-free SLT. Concretely, we factorize the training process into two stages. In the visual initialing stage, we employ a lightweight translation model after the visual encoder to pre-train the visual encoder. In the LLM fine",
    "link": "https://arxiv.org/abs/2403.12556",
    "context": "Title: Factorized Learning Assisted with Large Language Model for Gloss-free Sign Language Translation\nAbstract: arXiv:2403.12556v1 Announce Type: new  Abstract: Previous Sign Language Translation (SLT) methods achieve superior performance by relying on gloss annotations. However, labeling high-quality glosses is a labor-intensive task, which limits the further development of SLT. Although some approaches work towards gloss-free SLT through jointly training the visual encoder and translation network, these efforts still suffer from poor performance and inefficient use of the powerful Large Language Model (LLM). Most seriously, we find that directly introducing LLM into SLT will lead to insufficient learning of visual representations as LLM dominates the learning curve. To address these problems, we propose Factorized Learning assisted with Large Language Model (FLa-LLM) for gloss-free SLT. Concretely, we factorize the training process into two stages. In the visual initialing stage, we employ a lightweight translation model after the visual encoder to pre-train the visual encoder. In the LLM fine",
    "path": "papers/24/03/2403.12556.json",
    "total_tokens": 973,
    "translated_title": "利用大型语言模型辅助分解学习的无词汇手语翻译方法",
    "translated_abstract": "先前的手语翻译方法通过依赖术语标注实现了卓越的性能，然而，标记高质量的术语是一项劳动密集型的任务，限制了手语翻译的进一步发展。尽管一些方法通过联合训练视觉编码器和翻译网络实现了无术语的手语翻译，但这些努力仍然面临性能不佳和无效使用强大的大型语言模型（LLM）的问题。我们发现，直接引入LLM到手语翻译中会导致视觉表示学习不足，因为LLM主导了学习曲线。为了解决这些问题，我们提出了一种利用大型语言模型辅助分解学习的无词汇手语翻译方法（FLa-LLM）。具体而言，我们将训练过程分解为两个阶段。在视觉初始化阶段，我们在视觉编码器后使用轻量级翻译模型来预训练视觉编码器。",
    "tldr": "提出了一种利用大型语言模型辅助分解学习的无词汇手语翻译方法，通过将训练过程分解为两个阶段，在视觉初始化阶段采用轻量级翻译模型预训练视觉编码器，解决了直接引入大型语言模型导致学习不足的问题。",
    "en_tdlr": "Introduced a gloss-free sign language translation method assisted with a large language model by factorizing the training process into two stages and using a lightweight translation model to pre-train the visual encoder in the visual initialing stage, addressing the issue of insufficient learning due to the direct introduction of the large language model."
}