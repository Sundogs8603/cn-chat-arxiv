{
    "title": "FAGH: Accelerating Federated Learning with Approximated Global Hessian",
    "abstract": "arXiv:2403.11041v1 Announce Type: new  Abstract: In federated learning (FL), the significant communication overhead due to the slow convergence speed of training the global model poses a great challenge. Specifically, a large number of communication rounds are required to achieve the convergence in FL. One potential solution is to employ the Newton-based optimization method for training, known for its quadratic convergence rate. However, the existing Newton-based FL training methods suffer from either memory inefficiency or high computational costs for local clients or the server. To address this issue, we propose an FL with approximated global Hessian (FAGH) method to accelerate FL training. FAGH leverages the first moment of the approximated global Hessian and the first moment of the global gradient to train the global model. By harnessing the approximated global Hessian curvature, FAGH accelerates the convergence of global model training, leading to the reduced number of communicati",
    "link": "https://arxiv.org/abs/2403.11041",
    "context": "Title: FAGH: Accelerating Federated Learning with Approximated Global Hessian\nAbstract: arXiv:2403.11041v1 Announce Type: new  Abstract: In federated learning (FL), the significant communication overhead due to the slow convergence speed of training the global model poses a great challenge. Specifically, a large number of communication rounds are required to achieve the convergence in FL. One potential solution is to employ the Newton-based optimization method for training, known for its quadratic convergence rate. However, the existing Newton-based FL training methods suffer from either memory inefficiency or high computational costs for local clients or the server. To address this issue, we propose an FL with approximated global Hessian (FAGH) method to accelerate FL training. FAGH leverages the first moment of the approximated global Hessian and the first moment of the global gradient to train the global model. By harnessing the approximated global Hessian curvature, FAGH accelerates the convergence of global model training, leading to the reduced number of communicati",
    "path": "papers/24/03/2403.11041.json",
    "total_tokens": 700,
    "translated_title": "FAGH：利用近似全局Hessian加速联邦学习",
    "translated_abstract": "在联邦学习（FL）中，由于全局模型收敛速度慢而导致的显着通信开销构成了一项重大挑战。本文提出了一种名为FAGH的FL方法，该方法利用近似全局Hessian的一阶矩和全局梯度的一阶矩来训练全局模型，通过利用全局Hessian的曲率，加速全局模型训练的收敛，减少了通信次数。",
    "tldr": "提出了一种FL方法FAGH，利用近似全局Hessian和全局梯度的一阶矩来加速全局模型训练，降低通信次数。",
    "en_tdlr": "Proposed a FL method FAGH that utilizes the first moment of the approximated global Hessian and the first moment of the global gradient to accelerate global model training, reducing the number of communication rounds."
}