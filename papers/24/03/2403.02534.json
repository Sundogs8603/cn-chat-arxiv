{
    "title": "Towards Foundation Time Series Model: To Synthesize Or Not To Synthesize?",
    "abstract": "arXiv:2403.02534v1 Announce Type: new  Abstract: The industry is rich in cases when we are required to make forecasting for large amounts of time series at once. However, we might be in a situation where we can not afford to train a separate model for each of them. Such issue in time series modeling remains without due attention. The remedy for this setting is the establishment of a foundation model. Such a model is expected to work in zero-shot and few-shot regimes. However, what should we take as a training dataset for such kind of model?   Witnessing the benefits from the enrichment of NLP datasets with artificially-generated data, we might want to adopt their experience for time series. In contrast to natural language, the process of generation of synthetic time series data is even more favorable because it provides full control of series patterns, time horizons, and number of samples. In this work, we consider the essential question if it is advantageous to train a foundation mode",
    "link": "https://arxiv.org/abs/2403.02534",
    "context": "Title: Towards Foundation Time Series Model: To Synthesize Or Not To Synthesize?\nAbstract: arXiv:2403.02534v1 Announce Type: new  Abstract: The industry is rich in cases when we are required to make forecasting for large amounts of time series at once. However, we might be in a situation where we can not afford to train a separate model for each of them. Such issue in time series modeling remains without due attention. The remedy for this setting is the establishment of a foundation model. Such a model is expected to work in zero-shot and few-shot regimes. However, what should we take as a training dataset for such kind of model?   Witnessing the benefits from the enrichment of NLP datasets with artificially-generated data, we might want to adopt their experience for time series. In contrast to natural language, the process of generation of synthetic time series data is even more favorable because it provides full control of series patterns, time horizons, and number of samples. In this work, we consider the essential question if it is advantageous to train a foundation mode",
    "path": "papers/24/03/2403.02534.json",
    "total_tokens": 823,
    "translated_title": "迈向基础时间序列模型：合成还是不合成？",
    "translated_abstract": "产业中经常有需要同时对大量时间序列进行预测的情况。然而，我们可能会遇到无法为每个时间序列训练单独模型的情况。时间序列建模中这一问题一直未得到充分重视。针对这种情况的解决办法是建立一个基础模型。这种模型预计能在零样本和少样本情况下工作。然而，我们应该将什么作为这种模型的训练数据集呢？从NLP数据集的扩充中获益，我们可能希望借鉴他们的经验用于时间序列。与自然语言不同，合成时间序列数据生成过程更为有利，因为它提供了全面控制系列模式、时间跨度和样本数量的可能性。",
    "tldr": "通常需要对大量时间序列进行预测，但可能无法为每个序列单独训练模型，提出了建立基础时间序列模型的解决方案，并探讨使用合成数据作为训练集的优势。"
}