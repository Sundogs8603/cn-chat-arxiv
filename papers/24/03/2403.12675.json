{
    "title": "Pragmatic Competence Evaluation of Large Language Models for Korean",
    "abstract": "arXiv:2403.12675v1 Announce Type: new  Abstract: The current evaluation of Large Language Models (LLMs) predominantly relies on benchmarks focusing on their embedded knowledge by testing through multiple-choice questions (MCQs), a format inherently suited for automated evaluation. Our study extends this evaluation to explore LLMs' pragmatic competence--a facet previously underexamined before the advent of sophisticated LLMs, specifically in the context of Korean. We employ two distinct evaluation setups: the conventional MCQ format, adapted for automatic evaluation, and Open-Ended Questions (OEQs), assessed by human experts, to examine LLMs' narrative response capabilities without predefined options. Our findings reveal that GPT-4 excels, scoring 81.11 and 85.69 in the MCQ and OEQ setups, respectively, with HyperCLOVA X, an LLM optimized for Korean, closely following, especially in the OEQ setup, demonstrating a score of 81.56 with a marginal difference of 4.13 points compared to GPT-4",
    "link": "https://arxiv.org/abs/2403.12675",
    "context": "Title: Pragmatic Competence Evaluation of Large Language Models for Korean\nAbstract: arXiv:2403.12675v1 Announce Type: new  Abstract: The current evaluation of Large Language Models (LLMs) predominantly relies on benchmarks focusing on their embedded knowledge by testing through multiple-choice questions (MCQs), a format inherently suited for automated evaluation. Our study extends this evaluation to explore LLMs' pragmatic competence--a facet previously underexamined before the advent of sophisticated LLMs, specifically in the context of Korean. We employ two distinct evaluation setups: the conventional MCQ format, adapted for automatic evaluation, and Open-Ended Questions (OEQs), assessed by human experts, to examine LLMs' narrative response capabilities without predefined options. Our findings reveal that GPT-4 excels, scoring 81.11 and 85.69 in the MCQ and OEQ setups, respectively, with HyperCLOVA X, an LLM optimized for Korean, closely following, especially in the OEQ setup, demonstrating a score of 81.56 with a marginal difference of 4.13 points compared to GPT-4",
    "path": "papers/24/03/2403.12675.json",
    "total_tokens": 945,
    "translated_title": "对韩语大型语言模型的语用能力评估",
    "translated_abstract": "目前对大型语言模型（LLMs）的评估主要依赖于着重于测试其嵌入知识的基准，通过多项选择题（MCQs）来进行评估，这种格式非常适合自动评估。我们的研究将此评估拓展到探索LLM的语用能力--在先进的LLM出现之前鲜有研究，特别是在韩语环境下。我们采用两种不同的评估设置：传统的自动评估适配的MCQ格式，以及由人类专家评估的开放式问题（OEQs），用以检查LLM的叙事回应能力，而无需预先定义选项。我们的研究发现，GPT-4表现优异，在MCQ和OEQ设置中得分分别为81.11和85.69，而以韩语为优化目标的HyperCLOVA X在OEQ设置中表现出色，得分为81.56，与GPT-4相比，仅有4.13分的微小差距。",
    "tldr": "该研究将大型语言模型的评估从对嵌入知识的基准拓展到了探索语用能力，结果显示在韩语环境下，GPT-4在传统和人工评估设置中表现出色，而HyperCLOVA X也在开放式问题评估中取得了不错的成绩。",
    "en_tdlr": "This study extends the evaluation of large language models from embedded knowledge benchmarks to explore pragmatic competence, showing that in the context of Korean, GPT-4 performs well in both traditional and human evaluation setups, while HyperCLOVA X also achieves good results in open-ended question assessment."
}