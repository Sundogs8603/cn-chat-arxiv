{
    "title": "Where does In-context Translation Happen in Large Language Models",
    "abstract": "arXiv:2403.04510v1 Announce Type: cross  Abstract: Self-supervised large language models have demonstrated the ability to perform Machine Translation (MT) via in-context learning, but little is known about where the model performs the task with respect to prompt instructions and demonstration examples. In this work, we attempt to characterize the region where large language models transition from in-context learners to translation models. Through a series of layer-wise context-masking experiments on \\textsc{GPTNeo2.7B}, \\textsc{Bloom3B}, \\textsc{Llama7b} and \\textsc{Llama7b-chat}, we demonstrate evidence of a \"task recognition\" point where the translation task is encoded into the input representations and attention to context is no longer necessary. We further observe correspondence between the low performance when masking out entire layers, and the task recognition layers. Taking advantage of this redundancy results in 45\\% computational savings when prompting with 5 examples, and tas",
    "link": "https://arxiv.org/abs/2403.04510",
    "context": "Title: Where does In-context Translation Happen in Large Language Models\nAbstract: arXiv:2403.04510v1 Announce Type: cross  Abstract: Self-supervised large language models have demonstrated the ability to perform Machine Translation (MT) via in-context learning, but little is known about where the model performs the task with respect to prompt instructions and demonstration examples. In this work, we attempt to characterize the region where large language models transition from in-context learners to translation models. Through a series of layer-wise context-masking experiments on \\textsc{GPTNeo2.7B}, \\textsc{Bloom3B}, \\textsc{Llama7b} and \\textsc{Llama7b-chat}, we demonstrate evidence of a \"task recognition\" point where the translation task is encoded into the input representations and attention to context is no longer necessary. We further observe correspondence between the low performance when masking out entire layers, and the task recognition layers. Taking advantage of this redundancy results in 45\\% computational savings when prompting with 5 examples, and tas",
    "path": "papers/24/03/2403.04510.json",
    "total_tokens": 849,
    "translated_title": "大型语言模型中的上下文翻译发生在哪里",
    "translated_abstract": "自监督大型语言模型已经展示出能够通过上下文学习执行机器翻译（MT）的能力，但关于模型在何处执行这一任务相对于提示指令和演示示例的情况知之甚少。在这项工作中，我们试图表征大型语言模型从上下文学习者转变为翻译模型的区域。通过一系列在GPTNeo2.7B、Bloom3B、Llama7b和Llama7b-chat上的逐层上下文屏蔽实验，我们展示了\"任务识别\"点的证据，即翻译任务被编码到输入表示中，并且不再需要关注上下文。我们进一步观察到完全屏蔽层时低性能与任务识别层之间的对应关系。利用这种冗余性在提示5个示例时节约了45%的计算量。",
    "tldr": "该研究在大型语言模型中探索了从上下文学习者到翻译模型的转变过程，并发现了\"任务识别\"点以及利用该点的冗余性可节约计算量。",
    "en_tdlr": "This study explores the transition from in-context learners to translation models in large language models, identifies a \"task recognition\" point, and shows that utilizing the redundancy at this point leads to computational savings."
}