{
    "title": "Which LLM to Play? Convergence-Aware Online Model Selection with Time-Increasing Bandits",
    "abstract": "arXiv:2403.07213v1 Announce Type: new  Abstract: Web-based applications such as chatbots, search engines and news recommendations continue to grow in scale and complexity with the recent surge in the adoption of LLMs. Online model selection has thus garnered increasing attention due to the need to choose the best model among a diverse set while balancing task reward and exploration cost. Organizations faces decisions like whether to employ a costly API-based LLM or a locally finetuned small LLM, weighing cost against performance. Traditional selection methods often evaluate every candidate model before choosing one, which are becoming impractical given the rising costs of training and finetuning LLMs. Moreover, it is undesirable to allocate excessive resources towards exploring poor-performing models. While some recent works leverage online bandit algorithm to manage such exploration-exploitation trade-off in model selection, they tend to overlook the increasing-then-converging trend i",
    "link": "https://arxiv.org/abs/2403.07213",
    "context": "Title: Which LLM to Play? Convergence-Aware Online Model Selection with Time-Increasing Bandits\nAbstract: arXiv:2403.07213v1 Announce Type: new  Abstract: Web-based applications such as chatbots, search engines and news recommendations continue to grow in scale and complexity with the recent surge in the adoption of LLMs. Online model selection has thus garnered increasing attention due to the need to choose the best model among a diverse set while balancing task reward and exploration cost. Organizations faces decisions like whether to employ a costly API-based LLM or a locally finetuned small LLM, weighing cost against performance. Traditional selection methods often evaluate every candidate model before choosing one, which are becoming impractical given the rising costs of training and finetuning LLMs. Moreover, it is undesirable to allocate excessive resources towards exploring poor-performing models. While some recent works leverage online bandit algorithm to manage such exploration-exploitation trade-off in model selection, they tend to overlook the increasing-then-converging trend i",
    "path": "papers/24/03/2403.07213.json",
    "total_tokens": 793,
    "translated_title": "选择哪个LLM？具有收敛意识的增量时间臂的在线模型选择",
    "translated_abstract": "Web-based应用，如聊天机器人、搜索引擎和新闻推荐，随着LLMs的日益普及，在规模和复杂性上继续增长。在线模型选择因需要在平衡任务奖励和探索成本的同时选择最佳模型而引起了越来越多的关注。传统的选择方法通常在选择一个模型之前评估每个候选模型，随着训练和微调LLMs成本的上升，这些方法变得不切实际。此外，分配过多资源去探索表现不佳的模型是不可取的。尽管一些最新的工作利用在线臂算法来管理模型选择中的这种探索-开发权衡，但它们往往忽视了增长然后收敛趋势。",
    "tldr": "该论文提出了一种具有收敛意识的增量时间臂在线模型选择方法，用于在选择最佳模型时平衡任务奖励和探索成本。",
    "en_tdlr": "This paper introduces a convergence-aware online model selection approach with time-increasing bandits to balance task reward and exploration cost when selecting the best model."
}