{
    "title": "Cyclic Data Parallelism for Efficient Parallelism of Deep Neural Networks",
    "abstract": "arXiv:2403.08837v1 Announce Type: cross  Abstract: Training large deep learning models requires parallelization techniques to scale. In existing methods such as Data Parallelism or ZeRO-DP, micro-batches of data are processed in parallel, which creates two drawbacks: the total memory required to store the model's activations peaks at the end of the forward pass, and gradients must be simultaneously averaged at the end of the backpropagation step. We propose Cyclic Data Parallelism, a novel paradigm shifting the execution of the micro-batches from simultaneous to sequential, with a uniform delay. At the cost of a slight gradient delay, the total memory taken by activations is constant, and the gradient communications are balanced during the training step. With Model Parallelism, our technique reduces the number of GPUs needed, by sharing GPUs across micro-batches. Within the ZeRO-DP framework, our technique allows communication of the model states with point-to-point operations rather t",
    "link": "https://arxiv.org/abs/2403.08837",
    "context": "Title: Cyclic Data Parallelism for Efficient Parallelism of Deep Neural Networks\nAbstract: arXiv:2403.08837v1 Announce Type: cross  Abstract: Training large deep learning models requires parallelization techniques to scale. In existing methods such as Data Parallelism or ZeRO-DP, micro-batches of data are processed in parallel, which creates two drawbacks: the total memory required to store the model's activations peaks at the end of the forward pass, and gradients must be simultaneously averaged at the end of the backpropagation step. We propose Cyclic Data Parallelism, a novel paradigm shifting the execution of the micro-batches from simultaneous to sequential, with a uniform delay. At the cost of a slight gradient delay, the total memory taken by activations is constant, and the gradient communications are balanced during the training step. With Model Parallelism, our technique reduces the number of GPUs needed, by sharing GPUs across micro-batches. Within the ZeRO-DP framework, our technique allows communication of the model states with point-to-point operations rather t",
    "path": "papers/24/03/2403.08837.json",
    "total_tokens": 877,
    "translated_title": "用于深度神经网络高效并行化的循环数据并行性",
    "translated_abstract": "训练大型深度学习模型需要并行化技术以扩展规模。在现有方法中，如数据并行性或ZeRO-DP，微批量数据被并行处理，这产生了两个缺点：在前向传递结束时模型激活所需的总内存峰值，并且梯度必须在反向传播步骤结束时同时平均。我们提出了循环数据并行性，这是一种新颖的范式，将微批量的执行从同时变为顺序执行，带有均匀的延迟。以略微梯度延迟为代价，激活所占的总内存是恒定的，并且梯度通信在训练步骤期间是平衡的。通过模型并行性，我们的技术减少了所需的GPU数量，通过在微批量之间共享GPU。在ZeRO-DP框架内，我们的技术允许使用点对点操作进行模型状态的通信，而非 t",
    "tldr": "提出循环数据并行性，通过将微批量执行从同时改为顺序执行，以解决数据并行化中激活内存峰值和梯度平均的问题，同时还能减少所需GPU数量。",
    "en_tdlr": "Introducing Cyclic Data Parallelism to address issues in data parallelism by shifting micro-batch execution from simultaneous to sequential, reducing peak memory usage for activations and balancing gradient averaging, while also decreasing the number of required GPUs."
}