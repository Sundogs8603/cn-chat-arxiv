{
    "title": "DPAdapter: Improving Differentially Private Deep Learning through Noise Tolerance Pre-training",
    "abstract": "arXiv:2403.02571v1 Announce Type: new  Abstract: Recent developments have underscored the critical role of \\textit{differential privacy} (DP) in safeguarding individual data for training machine learning models. However, integrating DP oftentimes incurs significant model performance degradation due to the perturbation introduced into the training process, presenting a formidable challenge in the {differentially private machine learning} (DPML) field. To this end, several mitigative efforts have been proposed, typically revolving around formulating new DPML algorithms or relaxing DP definitions to harmonize with distinct contexts. In spite of these initiatives, the diminishment induced by DP on models, particularly large-scale models, remains substantial and thus, necessitates an innovative solution that adeptly circumnavigates the consequential impairment of model utility.   In response, we introduce DPAdapter, a pioneering technique designed to amplify the model performance of DPML al",
    "link": "https://arxiv.org/abs/2403.02571",
    "context": "Title: DPAdapter: Improving Differentially Private Deep Learning through Noise Tolerance Pre-training\nAbstract: arXiv:2403.02571v1 Announce Type: new  Abstract: Recent developments have underscored the critical role of \\textit{differential privacy} (DP) in safeguarding individual data for training machine learning models. However, integrating DP oftentimes incurs significant model performance degradation due to the perturbation introduced into the training process, presenting a formidable challenge in the {differentially private machine learning} (DPML) field. To this end, several mitigative efforts have been proposed, typically revolving around formulating new DPML algorithms or relaxing DP definitions to harmonize with distinct contexts. In spite of these initiatives, the diminishment induced by DP on models, particularly large-scale models, remains substantial and thus, necessitates an innovative solution that adeptly circumnavigates the consequential impairment of model utility.   In response, we introduce DPAdapter, a pioneering technique designed to amplify the model performance of DPML al",
    "path": "papers/24/03/2403.02571.json",
    "total_tokens": 841,
    "translated_title": "DPAdapter: 通过噪声容忍预训练改进差分隐私深度学习",
    "translated_abstract": "近期的研究强调了差分隐私在保护个人数据训练机器学习模型中的关键作用。然而，集成差分隐私通常会导致模型性能严重降低，这是差分隐私机器学习领域面临的重要挑战。为此，提出了几项缓解措施，通常围绕制定新的差分隐私机器学习算法或放宽差分隐私定义以适应不同情境。尽管有这些努力，差分隐私对模型的减弱，特别是大规模模型，仍然很大，因此需要一种创新性解决方案，巧妙地规避模型效用的重要损害。为此，我们引入了DPAdapter，这是一个旨在提升DPML模型性能的开创性技术。",
    "tldr": "DPAdapter通过噪声容忍预训练的方法，旨在增强差分隐私深度学习模型性能，有效解决了DP引入的模型性能降低挑战。",
    "en_tdlr": "DPAdapter aims to enhance the model performance of differentially private deep learning through noise tolerance pre-training, effectively addressing the challenge of model performance degradation introduced by differential privacy."
}