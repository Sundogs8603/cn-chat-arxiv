{
    "title": "Exploring Task Unification in Graph Representation Learning via Generative Approach",
    "abstract": "arXiv:2403.14340v1 Announce Type: cross  Abstract: Graphs are ubiquitous in real-world scenarios and encompass a diverse range of tasks, from node-, edge-, and graph-level tasks to transfer learning. However, designing specific tasks for each type of graph data is often costly and lacks generalizability. Recent endeavors under the \"Pre-training + Fine-tuning\" or \"Pre-training + Prompt\" paradigms aim to design a unified framework capable of generalizing across multiple graph tasks. Among these, graph autoencoders (GAEs), generative self-supervised models, have demonstrated their potential in effectively addressing various graph tasks. Nevertheless, these methods typically employ multi-stage training and require adaptive designs, which on one hand make it difficult to be seamlessly applied to diverse graph tasks and on the other hand overlook the negative impact caused by discrepancies in task objectives between the different stages. To address these challenges, we propose GA^2E, a unifi",
    "link": "https://arxiv.org/abs/2403.14340",
    "context": "Title: Exploring Task Unification in Graph Representation Learning via Generative Approach\nAbstract: arXiv:2403.14340v1 Announce Type: cross  Abstract: Graphs are ubiquitous in real-world scenarios and encompass a diverse range of tasks, from node-, edge-, and graph-level tasks to transfer learning. However, designing specific tasks for each type of graph data is often costly and lacks generalizability. Recent endeavors under the \"Pre-training + Fine-tuning\" or \"Pre-training + Prompt\" paradigms aim to design a unified framework capable of generalizing across multiple graph tasks. Among these, graph autoencoders (GAEs), generative self-supervised models, have demonstrated their potential in effectively addressing various graph tasks. Nevertheless, these methods typically employ multi-stage training and require adaptive designs, which on one hand make it difficult to be seamlessly applied to diverse graph tasks and on the other hand overlook the negative impact caused by discrepancies in task objectives between the different stages. To address these challenges, we propose GA^2E, a unifi",
    "path": "papers/24/03/2403.14340.json",
    "total_tokens": 901,
    "translated_title": "通过生成方法探索图表示学习中的任务统一化",
    "translated_abstract": "图在现实场景中无处不在，并涵盖了从节点级、边级和图级任务到迁移学习的各种任务。然而，为每种类型的图数据设计特定任务通常代价高昂且缺乏泛化能力。最近的研究致力于“预训练+微调”或“预训练+提示”范式，旨在设计一个能够泛化多种图任务的统一框架。在这些方法中，图自编码器（GAEs）、生成自监督模型已经证明了它们在有效解决各种图任务方面的潜力。然而，这些方法通常使用多阶段训练并需要自适应设计，这一方面使得将其无缝应用于不同的图任务变得困难，另一方面忽略了不同阶段任务目标之间的差异造成的负面影响。为了解决这些挑战，我们提出了GA^2E，一个统一的框架，通过统一的生成式半监督学习，可以在单个训练阶段中同时实现图生成、图判别和图预测任务。",
    "tldr": "通过提出GA^2E，一个统一的框架，通过统一的生成式半监督学习，在单个训练阶段中实现了图生成、图判别和图预测任务。",
    "en_tdlr": "The paper proposes GA^2E, a unified framework that achieves graph generation, graph discrimination, and graph prediction tasks in a single training stage through unified generative semi-supervised learning."
}