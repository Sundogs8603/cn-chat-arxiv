{
    "title": "InternLM2 Technical Report",
    "abstract": "arXiv:2403.17297v1 Announce Type: cross  Abstract: The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack\" test. InternLM2 is further aligned using Supervised Fi",
    "link": "https://arxiv.org/abs/2403.17297",
    "context": "Title: InternLM2 Technical Report\nAbstract: arXiv:2403.17297v1 Announce Type: cross  Abstract: The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack\" test. InternLM2 is further aligned using Supervised Fi",
    "path": "papers/24/03/2403.17297.json",
    "total_tokens": 687,
    "translated_title": "InternLM2技术报告",
    "translated_abstract": "大语言模型（LLMs）的发展，如ChatGPT和GPT-4，引发了关于人工通用智能（AGI）即将到来的讨论。然而，在开源模型中复制这样的进展一直是具有挑战性的。本文介绍了InternLM2，一个开源的LLM，在6个维度和30个基准测试中胜过其前辈，在长文本建模和主观评估方面优异，通过创新的预训练和优化技术。",
    "tldr": "InternLM2是一个开源的大语言模型，在全面评估、长文本建模以及创新的预训练和优化技术下表现出色，超越了其前任模型。"
}