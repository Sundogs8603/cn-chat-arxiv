{
    "title": "Does generalization performance of $l^q$ regularization learning depend on $q$? A negative example. (arXiv:1307.6616v2 [cs.LG] UPDATED)",
    "abstract": "$l^q$-regularization has been demonstrated to be an attractive technique in machine learning and statistical modeling. It attempts to improve the generalization (prediction) capability of a machine (model) through appropriately shrinking its coefficients. The shape of a $l^q$ estimator differs in varying choices of the regularization order $q$. In particular, $l^1$ leads to the LASSO estimate, while $l^{2}$ corresponds to the smooth ridge regression. This makes the order $q$ a potential tuning parameter in applications. To facilitate the use of $l^{q}$-regularization, we intend to seek for a modeling strategy where an elaborative selection on $q$ is avoidable. In this spirit, we place our investigation within a general framework of $l^{q}$-regularized kernel learning under a sample dependent hypothesis space (SDHS). For a designated class of kernel functions, we show that all $l^{q}$ estimators for $0< q < \\infty$ attain similar generalization error bounds. These estimated bounds are a",
    "link": "http://arxiv.org/abs/1307.6616",
    "context": "Title: Does generalization performance of $l^q$ regularization learning depend on $q$? A negative example. (arXiv:1307.6616v2 [cs.LG] UPDATED)\nAbstract: $l^q$-regularization has been demonstrated to be an attractive technique in machine learning and statistical modeling. It attempts to improve the generalization (prediction) capability of a machine (model) through appropriately shrinking its coefficients. The shape of a $l^q$ estimator differs in varying choices of the regularization order $q$. In particular, $l^1$ leads to the LASSO estimate, while $l^{2}$ corresponds to the smooth ridge regression. This makes the order $q$ a potential tuning parameter in applications. To facilitate the use of $l^{q}$-regularization, we intend to seek for a modeling strategy where an elaborative selection on $q$ is avoidable. In this spirit, we place our investigation within a general framework of $l^{q}$-regularized kernel learning under a sample dependent hypothesis space (SDHS). For a designated class of kernel functions, we show that all $l^{q}$ estimators for $0< q < \\infty$ attain similar generalization error bounds. These estimated bounds are a",
    "path": "papers/13/07/1307.6616.json",
    "total_tokens": 949,
    "translated_title": "$l^q$正则化学习的泛化性能是否依赖于$q$？一个否定的例子。",
    "translated_abstract": "$l^q$-正则化已经被证明是机器学习和统计建模中一种有吸引力的技术。它通过适当缩小系数来提高机器（模型）的泛化（预测）能力。在不同的正则化阶数 $q$ 选择下，$l^q$ 估计器的形状不同。特别地，$l^1$ 导致 LASSO 估计，而 $l^{2}$ 对应于平滑的岭回归。这使得阶数 $q$ 成为应用中的一个潜在调参参数。为了促进 $l^{q}$-正则化的使用，我们打算寻找一种建模策略，可以避免在 $q$ 上进行精细的选择。在这样的精神下，我们将我们的研究置于一个样本相关假设空间（SDHS）下的 $l^{q}$-正则化核学习的一般框架中。对于一类指定的核函数，在 $0<q<\\infty$ 的所有 $l^{q}$ 估计值都具有类似的泛化误差界限。这些估计边界是一个...",
    "tldr": "该研究表明，在特定的核函数类中，$l^{q}$ 正则化学习在不同阶数 $q$ 下都具有相似的泛化误差界限。"
}