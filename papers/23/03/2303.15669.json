{
    "title": "Unsupervised Pre-Training For Data-Efficient Text-to-Speech On Low Resource Languages. (arXiv:2303.15669v1 [eess.AS])",
    "abstract": "Neural text-to-speech (TTS) models can synthesize natural human speech when trained on large amounts of transcribed speech. However, collecting such large-scale transcribed data is expensive. This paper proposes an unsupervised pre-training method for a sequence-to-sequence TTS model by leveraging large untranscribed speech data. With our pre-training, we can remarkably reduce the amount of paired transcribed data required to train the model for the target downstream TTS task. The main idea is to pre-train the model to reconstruct de-warped mel-spectrograms from warped ones, which may allow the model to learn proper temporal assignment relation between input and output sequences. In addition, we propose a data augmentation method that further improves the data efficiency in fine-tuning. We empirically demonstrate the effectiveness of our proposed method in low-resource language scenarios, achieving outstanding performance compared to competing methods. The code and audio samples are av",
    "link": "http://arxiv.org/abs/2303.15669",
    "context": "Title: Unsupervised Pre-Training For Data-Efficient Text-to-Speech On Low Resource Languages. (arXiv:2303.15669v1 [eess.AS])\nAbstract: Neural text-to-speech (TTS) models can synthesize natural human speech when trained on large amounts of transcribed speech. However, collecting such large-scale transcribed data is expensive. This paper proposes an unsupervised pre-training method for a sequence-to-sequence TTS model by leveraging large untranscribed speech data. With our pre-training, we can remarkably reduce the amount of paired transcribed data required to train the model for the target downstream TTS task. The main idea is to pre-train the model to reconstruct de-warped mel-spectrograms from warped ones, which may allow the model to learn proper temporal assignment relation between input and output sequences. In addition, we propose a data augmentation method that further improves the data efficiency in fine-tuning. We empirically demonstrate the effectiveness of our proposed method in low-resource language scenarios, achieving outstanding performance compared to competing methods. The code and audio samples are av",
    "path": "papers/23/03/2303.15669.json",
    "total_tokens": 969,
    "translated_title": "低资源语言下的无监督预训练文本转语音模型",
    "translated_abstract": "当大量转录音频数据用于训练时，神经文本到语音（TTS）模型可以合成自然的人类语音。但是，收集这样的大规模转录数据很昂贵。本文提出了一种无监督预训练方法，用于对序列到序列的TTS模型进行预训练，利用大量未转录的语音数据。通过我们的预训练，我们可以显着减少训练模型所需的匹配转录数据量，用于目标下游TTS任务的训练。主要思想是预先训练模型，以从扭曲的mel频谱图中重建出去扭曲的mel频谱图，这可能使模型学会了输入和输出序列之间的适当时间分配关系。此外，我们提出了一种数据增强方法，可进一步提高微调中的数据效率。我们在低资源语言场景中实验证明了我们提出的方法的有效性，与竞争方法相比表现出色。代码和音频样本可以在我们的项目页面上找到。",
    "tldr": "本文提出了一种针对低资源语言下无监督预训练的文本转语音模型，利用大量未转录的语音数据进行预训练，可显著减少训练模型所需的匹配转录数据量，进一步提升了数据效率，实验证明方法有效性。",
    "en_tdlr": "This paper proposes an unsupervised pre-training method for text-to-speech (TTS) models in low-resource languages, which leverages large untranscribed speech data to significantly reduce the amount of paired transcribed data required to train the model for downstream tasks. The method achieves outstanding performance compared to competing methods, and further improves data efficiency through data augmentation."
}