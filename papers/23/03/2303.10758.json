{
    "title": "Lower Generalization Bounds for GD and SGD in Smooth Stochastic Convex Optimization. (arXiv:2303.10758v1 [cs.LG])",
    "abstract": "Recent progress was made in characterizing the generalization error of gradient methods for general convex loss by the learning theory community. In this work, we focus on how training longer might affect generalization in smooth stochastic convex optimization (SCO) problems. We first provide tight lower bounds for general non-realizable SCO problems. Furthermore, existing upper bound results suggest that sample complexity can be improved by assuming the loss is realizable, i.e. an optimal solution simultaneously minimizes all the data points. However, this improvement is compromised when training time is long and lower bounds are lacking. Our paper examines this observation by providing excess risk lower bounds for gradient descent (GD) and stochastic gradient descent (SGD) in two realizable settings: 1) realizable with $T = O(n)$, and (2) realizable with $T = \\Omega(n)$, where $T$ denotes the number of training iterations and $n$ is the size of the training dataset. These bounds are ",
    "link": "http://arxiv.org/abs/2303.10758",
    "context": "Title: Lower Generalization Bounds for GD and SGD in Smooth Stochastic Convex Optimization. (arXiv:2303.10758v1 [cs.LG])\nAbstract: Recent progress was made in characterizing the generalization error of gradient methods for general convex loss by the learning theory community. In this work, we focus on how training longer might affect generalization in smooth stochastic convex optimization (SCO) problems. We first provide tight lower bounds for general non-realizable SCO problems. Furthermore, existing upper bound results suggest that sample complexity can be improved by assuming the loss is realizable, i.e. an optimal solution simultaneously minimizes all the data points. However, this improvement is compromised when training time is long and lower bounds are lacking. Our paper examines this observation by providing excess risk lower bounds for gradient descent (GD) and stochastic gradient descent (SGD) in two realizable settings: 1) realizable with $T = O(n)$, and (2) realizable with $T = \\Omega(n)$, where $T$ denotes the number of training iterations and $n$ is the size of the training dataset. These bounds are ",
    "path": "papers/23/03/2303.10758.json",
    "total_tokens": 1066,
    "translated_title": "平稳随机凸优化中GD和SGD的泛化下界降低",
    "translated_abstract": "最近，学习理论界在刻画一般凸损失梯度方法的泛化误差方面取得了进展。本文侧重于讨论在泛化光滑随机凸优化（SCO）问题中训练时间如何影响泛化能力。我们首先为一般的不可实现SCO问题提供了严格的下界。此外，现有的上界结果表明，假设损失可实现（即最优解同时最小化所有数据点）可以提高样本复杂度。但是，当训练时间长且缺乏下界时，这种改进会受到损害。我们对此进行了研究，提供了对于梯度下降（GD）和随机梯度下降（SGD）在两种可实现情况下的过量风险下界：1）实现需$T = O(n)$，和（2）实现需$T = \\Omega(n)$，其中$T$表示训练迭代次数，$n$为训练数据集的大小。这些下界的证明使用了来自优化的现代工具，包括对偶理论和镜像下降。我们的结果表明，在可实现的SCO中，更长的训练时间可能会导致更差的泛化，这与文献中的先前发现形成鲜明对比。我们还提供了支持我们结果的数值实验。",
    "tldr": "本论文证明了在平稳随机凸优化中，GD和SGD的泛化下界可以降低，并且长时间的训练可能导致更差的泛化能力，这与其他研究成果不同。"
}