{
    "title": "Fix the Noise: Disentangling Source Feature for Controllable Domain Translation. (arXiv:2303.11545v1 [cs.CV])",
    "abstract": "Recent studies show strong generative performance in domain translation especially by using transfer learning techniques on the unconditional generator. However, the control between different domain features using a single model is still challenging. Existing methods often require additional models, which is computationally demanding and leads to unsatisfactory visual quality. In addition, they have restricted control steps, which prevents a smooth transition. In this paper, we propose a new approach for high-quality domain translation with better controllability. The key idea is to preserve source features within a disentangled subspace of a target feature space. This allows our method to smoothly control the degree to which it preserves source features while generating images from an entirely new domain using only a single model. Our extensive experiments show that the proposed method can produce more consistent and realistic images than previous works and maintain precise controllab",
    "link": "http://arxiv.org/abs/2303.11545",
    "context": "Title: Fix the Noise: Disentangling Source Feature for Controllable Domain Translation. (arXiv:2303.11545v1 [cs.CV])\nAbstract: Recent studies show strong generative performance in domain translation especially by using transfer learning techniques on the unconditional generator. However, the control between different domain features using a single model is still challenging. Existing methods often require additional models, which is computationally demanding and leads to unsatisfactory visual quality. In addition, they have restricted control steps, which prevents a smooth transition. In this paper, we propose a new approach for high-quality domain translation with better controllability. The key idea is to preserve source features within a disentangled subspace of a target feature space. This allows our method to smoothly control the degree to which it preserves source features while generating images from an entirely new domain using only a single model. Our extensive experiments show that the proposed method can produce more consistent and realistic images than previous works and maintain precise controllab",
    "path": "papers/23/03/2303.11545.json",
    "total_tokens": 949,
    "translated_title": "修正噪声：为可控领域翻译分解源特征",
    "translated_abstract": "最近的研究表明，在无条件生成器上使用转移学习技术，特别是在领域翻译方面，表现出了强大的生成能力。但是，使用单个模型控制不同领域特征之间的控制仍然具有挑战性。现有方法通常需要额外的模型，这在计算上是要求很高的，而且会导致不令人满意的视觉质量。此外，它们具有受限控制步骤，从而防止平滑过渡。在本文中，我们提出了一种新的高质量领域翻译方法，具有更好的可控性。其关键思想是在目标特征空间的已分解子空间中保留源特征。这使得我们的方法能够在只使用单个模型的情况下，平滑地控制保留源特征的程度，同时从完全新的领域生成图像。我们广泛的实验表明，所提出的方法可以产生比先前的工作更一致、更逼真的图像，并保持精确的可控性。",
    "tldr": "本文提出了一种新的可控领域翻译方法，通过在目标特征空间的已分解子空间中保留源特征，使得只使用单个模型就能平滑地控制保留源特征的程度，产生更一致、更逼真的图像。",
    "en_tdlr": "This paper proposes a new approach for controllable domain translation, which preserves source features within a disentangled subspace of a target feature space. This allows for smooth control over the degree to which source features are preserved, resulting in more consistent and realistic images produced with only a single model."
}