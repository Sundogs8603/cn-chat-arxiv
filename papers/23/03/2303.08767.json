{
    "title": "Highly Personalized Text Embedding for Image Manipulation by Stable Diffusion. (arXiv:2303.08767v1 [cs.CV])",
    "abstract": "Diffusion models have shown superior performance in image generation and manipulation, but the inherent stochasticity presents challenges in preserving and manipulating image content and identity. While previous approaches like DreamBooth and Textual Inversion have proposed model or latent representation personalization to maintain the content, their reliance on multiple reference images and complex training limits their practicality. In this paper, we present a simple yet highly effective approach to personalization using highly personalized (HiPer) text embedding by decomposing the CLIP embedding space for personalization and content manipulation. Our method does not require model fine-tuning or identifiers, yet still enables manipulation of background, texture, and motion with just a single image and target text. Through experiments on diverse target texts, we demonstrate that our approach produces highly personalized and complex semantic image edits across a wide range of tasks. We",
    "link": "http://arxiv.org/abs/2303.08767",
    "context": "Title: Highly Personalized Text Embedding for Image Manipulation by Stable Diffusion. (arXiv:2303.08767v1 [cs.CV])\nAbstract: Diffusion models have shown superior performance in image generation and manipulation, but the inherent stochasticity presents challenges in preserving and manipulating image content and identity. While previous approaches like DreamBooth and Textual Inversion have proposed model or latent representation personalization to maintain the content, their reliance on multiple reference images and complex training limits their practicality. In this paper, we present a simple yet highly effective approach to personalization using highly personalized (HiPer) text embedding by decomposing the CLIP embedding space for personalization and content manipulation. Our method does not require model fine-tuning or identifiers, yet still enables manipulation of background, texture, and motion with just a single image and target text. Through experiments on diverse target texts, we demonstrate that our approach produces highly personalized and complex semantic image edits across a wide range of tasks. We",
    "path": "papers/23/03/2303.08767.json",
    "total_tokens": 895,
    "translated_title": "基于稳定扩散的高度个性化文本嵌入图像操作",
    "translated_abstract": "稳定扩散模型已经展现出在图像生成和操作方面的卓越性能，但内部随机性带来的挑战在于如何保留和操作图像内容和身份。虽然之前的方法如“梦境相机”和“文本反转”提出了使用模型或潜在表示个性化来保持内容，但它们对多个参考图像和复杂训练的依赖限制了它们的实用性。本文提出了一种简单而又非常有效的个性化方法，使用高度个性化（HiPer）文本嵌入通过分解CLIP嵌入空间实现个性化和内容操作。我们的方法不需要模型微调或识别符，但仍可以仅通过单个图像和目标文本来实现背景、纹理和动作的操作。通过对多样化的目标文本的实验，我们证明了我们的方法在各种任务中都能产生高度个性化和复杂的语义图像编辑。",
    "tldr": "本论文提出了一种简单但高效的个性化方法——使用高度个性化文本嵌入来进行图像操作，可以运用于图像的背景、纹理和动作的编辑，不需要多个参考图像或复杂训练，能实现复杂语义图像编辑。"
}