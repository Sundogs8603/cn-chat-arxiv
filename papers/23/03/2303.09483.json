{
    "title": "Achieving a Better Stability-Plasticity Trade-off via Auxiliary Networks in Continual Learning. (arXiv:2303.09483v1 [cs.LG])",
    "abstract": "In contrast to the natural capabilities of humans to learn new tasks in a sequential fashion, neural networks are known to suffer from catastrophic forgetting, where the model's performances on old tasks drop dramatically after being optimized for a new task. Since then, the continual learning (CL) community has proposed several solutions aiming to equip the neural network with the ability to learn the current task (plasticity) while still achieving high accuracy on the previous tasks (stability). Despite remarkable improvements, the plasticity-stability trade-off is still far from being solved and its underlying mechanism is poorly understood. In this work, we propose Auxiliary Network Continual Learning (ANCL), a novel method that applies an additional auxiliary network which promotes plasticity to the continually learned model which mainly focuses on stability. More concretely, the proposed framework materializes in a regularizer that naturally interpolates between plasticity and st",
    "link": "http://arxiv.org/abs/2303.09483",
    "context": "Title: Achieving a Better Stability-Plasticity Trade-off via Auxiliary Networks in Continual Learning. (arXiv:2303.09483v1 [cs.LG])\nAbstract: In contrast to the natural capabilities of humans to learn new tasks in a sequential fashion, neural networks are known to suffer from catastrophic forgetting, where the model's performances on old tasks drop dramatically after being optimized for a new task. Since then, the continual learning (CL) community has proposed several solutions aiming to equip the neural network with the ability to learn the current task (plasticity) while still achieving high accuracy on the previous tasks (stability). Despite remarkable improvements, the plasticity-stability trade-off is still far from being solved and its underlying mechanism is poorly understood. In this work, we propose Auxiliary Network Continual Learning (ANCL), a novel method that applies an additional auxiliary network which promotes plasticity to the continually learned model which mainly focuses on stability. More concretely, the proposed framework materializes in a regularizer that naturally interpolates between plasticity and st",
    "path": "papers/23/03/2303.09483.json",
    "total_tokens": 1031,
    "translated_title": "辅助网络在持续学习中实现更好的稳定性-可塑性平衡",
    "translated_abstract": "与人类顺序学习新任务的自然能力相比，神经网络被认为容易出现灾难性遗忘，即模型在被优化为新任务后，在旧任务上的表现急剧下降。为此，持续学习（CL）社区提出了几种解决方案，旨在使神经网络具有学习当前任务（可塑性）的能力，同时在以前的任务上实现高精度（稳定性）。尽管取得了显着的进展，但稳定性-可塑性平衡还远未得到解决，其基本机制尚不清楚。在这项工作中，我们提出了一种新方法——辅助网络持续学习（ANCL），它将一个额外的辅助网络应用于主要关注稳定性的持续学习模型中，从而促进模型的可塑性。更具体地说，所提出的框架通过控制主要网络和辅助网络之间信息的流动来自然地插值可塑性和稳定性之间的差异。多个数据集的实验结果表明，ANCL在可塑性和稳定性方面优于现有持续学习方法，实现了更好的平衡。",
    "tldr": "本文提出了一种辅助网络持续学习方法（ANCL），通过对流信息的控制，自然插值可塑性和稳定性之间的差异，有助于在神经网络中实现更好的稳定性-可塑性平衡。",
    "en_tdlr": "This paper proposes a novel method called Auxiliary Network Continual Learning (ANCL) which promotes plasticity to the continually learned model by controlling the flow of information between the primary and auxiliary networks, achieving a better trade-off between stability and plasticity in neural networks."
}