{
    "title": "Learning Spatial-Temporal Implicit Neural Representations for Event-Guided Video Super-Resolution. (arXiv:2303.13767v1 [cs.CV])",
    "abstract": "Event cameras sense the intensity changes asynchronously and produce event streams with high dynamic range and low latency. This has inspired research endeavors utilizing events to guide the challenging video superresolution (VSR) task. In this paper, we make the first attempt to address a novel problem of achieving VSR at random scales by taking advantages of the high temporal resolution property of events. This is hampered by the difficulties of representing the spatial-temporal information of events when guiding VSR. To this end, we propose a novel framework that incorporates the spatial-temporal interpolation of events to VSR in a unified framework. Our key idea is to learn implicit neural representations from queried spatial-temporal coordinates and features from both RGB frames and events. Our method contains three parts. Specifically, the Spatial-Temporal Fusion (STF) module first learns the 3D features from events and RGB frames. Then, the Temporal Filter (TF) module unlocks mo",
    "link": "http://arxiv.org/abs/2303.13767",
    "context": "Title: Learning Spatial-Temporal Implicit Neural Representations for Event-Guided Video Super-Resolution. (arXiv:2303.13767v1 [cs.CV])\nAbstract: Event cameras sense the intensity changes asynchronously and produce event streams with high dynamic range and low latency. This has inspired research endeavors utilizing events to guide the challenging video superresolution (VSR) task. In this paper, we make the first attempt to address a novel problem of achieving VSR at random scales by taking advantages of the high temporal resolution property of events. This is hampered by the difficulties of representing the spatial-temporal information of events when guiding VSR. To this end, we propose a novel framework that incorporates the spatial-temporal interpolation of events to VSR in a unified framework. Our key idea is to learn implicit neural representations from queried spatial-temporal coordinates and features from both RGB frames and events. Our method contains three parts. Specifically, the Spatial-Temporal Fusion (STF) module first learns the 3D features from events and RGB frames. Then, the Temporal Filter (TF) module unlocks mo",
    "path": "papers/23/03/2303.13767.json",
    "total_tokens": 951,
    "translated_title": "学习时空隐式神经表示来实现事件引导的视频超分辨率",
    "translated_abstract": "事件相机异步感知强度变化，产生具有高动态范围和低延迟的事件流。这激发了利用事件引导具有挑战性的视频超分辨率（VSR）任务的研究。在本文中，我们首次尝试利用事件的高时序分辨率性质，通过利用事件的时空插值来实现随机尺度下的VSR。当引导VSR时，事件的时空信息的表示具有困难。为此，我们提出了一个新的框架，将事件的时空插值与VSR结合在一个统一的框架中。我们的关键思想是从RGB帧和事件的查询空间-时间坐标和特征中学习隐式神经表示。我们的方法分为三部分。具体而言，空时融合（STF）模块首先学习事件和RGB帧的3D特征。然后，时域滤波器（TF）模块解锁了更多特征并给出了精细的VSR结果。最后，我们提供了一个大规模的VSR数据集，以便深度学习的VSR研究人员可以评估其方法。",
    "tldr": "本文提出了一种新的框架，利用事件的时空插值来实现随机尺度下的视频超分辨率。方法包括从RGB帧和事件的查询空间-时间坐标和特征中学习隐式神经表示。",
    "en_tdlr": "This paper proposes a novel framework using spatial-temporal interpolation of events to achieve video superresolution at random scales. The method involves learning implicit neural representations from queried spatial-temporal coordinates and features from both RGB frames and events."
}