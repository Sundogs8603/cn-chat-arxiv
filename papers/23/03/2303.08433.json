{
    "title": "The Benefits of Mixup for Feature Learning. (arXiv:2303.08433v1 [cs.LG])",
    "abstract": "Mixup, a simple data augmentation method that randomly mixes two data points via linear interpolation, has been extensively applied in various deep learning applications to gain better generalization. However, the theoretical underpinnings of its efficacy are not yet fully understood. In this paper, we aim to seek a fundamental understanding of the benefits of Mixup. We first show that Mixup using different linear interpolation parameters for features and labels can still achieve similar performance to the standard Mixup. This indicates that the intuitive linearity explanation in Zhang et al., (2018) may not fully explain the success of Mixup. Then we perform a theoretical study of Mixup from the feature learning perspective. We consider a feature-noise data model and show that Mixup training can effectively learn the rare features (appearing in a small fraction of data) from its mixture with the common features (appearing in a large fraction of data). In contrast, standard training ca",
    "link": "http://arxiv.org/abs/2303.08433",
    "context": "Title: The Benefits of Mixup for Feature Learning. (arXiv:2303.08433v1 [cs.LG])\nAbstract: Mixup, a simple data augmentation method that randomly mixes two data points via linear interpolation, has been extensively applied in various deep learning applications to gain better generalization. However, the theoretical underpinnings of its efficacy are not yet fully understood. In this paper, we aim to seek a fundamental understanding of the benefits of Mixup. We first show that Mixup using different linear interpolation parameters for features and labels can still achieve similar performance to the standard Mixup. This indicates that the intuitive linearity explanation in Zhang et al., (2018) may not fully explain the success of Mixup. Then we perform a theoretical study of Mixup from the feature learning perspective. We consider a feature-noise data model and show that Mixup training can effectively learn the rare features (appearing in a small fraction of data) from its mixture with the common features (appearing in a large fraction of data). In contrast, standard training ca",
    "path": "papers/23/03/2303.08433.json",
    "total_tokens": 900,
    "translated_title": "混合数据增强方法Mixup对于特征学习的益处",
    "translated_abstract": "Mixup是一种简单的数据增强方法，通过线性插值随机混合两个数据点，已广泛应用于各种深度学习应用中，以获得更好的泛化效果。然而，其有效性的理论基础尚未完全被理解。本文旨在寻求对Mixup益处的基本理解。首先，我们展示Mixup在特征和标签使用不同的线性插值参数时仍可实现类似于标准Mixup的性能。这表明，Zhang等人（2018）提出的直观线性解释可能并不能完全解释Mixup的成功。然后，我们从特征学习的角度对Mixup进行理论研究。我们考虑一个特征噪声数据模型，并展示Mixup训练可以有效地从其与常见特征（出现在大部分数据中）混合中学习罕见特征（出现在少部分数据中）。相比之下，标准训练可能会漏掉这些罕见特征。",
    "tldr": "本论文介绍了数据增强方法Mixup对于特征学习的益处。混合训练可以有效地从混合数据中学习罕见特征，相比之下，标准训练可能会漏掉这些罕见特征。",
    "en_tdlr": "This paper presents the benefits of Mixup, a data augmentation method, for feature learning. Mixup training can effectively learn rare features from its mixture with common features, while standard training may miss these rare features."
}