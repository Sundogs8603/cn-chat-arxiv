{
    "title": "Towards Better Dynamic Graph Learning: New Architecture and Unified Library. (arXiv:2303.13047v1 [cs.LG])",
    "abstract": "We propose DyGFormer, a new Transformer-based architecture for dynamic graph learning that solely learns from the sequences of nodes' historical first-hop interactions. DyGFormer incorporates two distinct designs: a neighbor co-occurrence encoding scheme that explores the correlations of the source node and destination node based on their sequences; a patching technique that divides each sequence into multiple patches and feeds them to Transformer, allowing the model to effectively and efficiently benefit from longer histories. We also introduce DyGLib, a unified library with standard training pipelines, extensible coding interfaces, and comprehensive evaluating protocols to promote reproducible, scalable, and credible dynamic graph learning research. By performing extensive experiments on thirteen datasets from various domains for transductive/inductive dynamic link prediction and dynamic node classification tasks, we observe that: DyGFormer achieves state-of-the-art performance on mo",
    "link": "http://arxiv.org/abs/2303.13047",
    "context": "Title: Towards Better Dynamic Graph Learning: New Architecture and Unified Library. (arXiv:2303.13047v1 [cs.LG])\nAbstract: We propose DyGFormer, a new Transformer-based architecture for dynamic graph learning that solely learns from the sequences of nodes' historical first-hop interactions. DyGFormer incorporates two distinct designs: a neighbor co-occurrence encoding scheme that explores the correlations of the source node and destination node based on their sequences; a patching technique that divides each sequence into multiple patches and feeds them to Transformer, allowing the model to effectively and efficiently benefit from longer histories. We also introduce DyGLib, a unified library with standard training pipelines, extensible coding interfaces, and comprehensive evaluating protocols to promote reproducible, scalable, and credible dynamic graph learning research. By performing extensive experiments on thirteen datasets from various domains for transductive/inductive dynamic link prediction and dynamic node classification tasks, we observe that: DyGFormer achieves state-of-the-art performance on mo",
    "path": "papers/23/03/2303.13047.json",
    "total_tokens": 964,
    "translated_title": "向更好的动态图学习迈进：新的架构和统一库",
    "translated_abstract": "我们提出了DyGFormer，这是一种基于Transformer的新型动态图学习架构，仅从节点历史的第一跳交互序列中学习。DyGFormer结合了两种不同的设计：一种邻居共现编码方案，探索源节点和目标节点基于它们的序列的相关性；一种分块技术，将每个序列分成多个块并将其馈送给Transformer，使模型能够有效而高效地受益于更长期的历史。我们还引入了DyGLib，这是一个统一的库，具有标准的训练管道、可扩展的编码接口和综合的评估协议，以促进可重复、可伸缩和可信的动态图学习研究。通过在来自各个领域的13个数据集上执行广泛的实验，进行推导/归纳动态链接预测和动态节点分类任务，我们观察到：DyGFormer在mo上实现了最先进的性能",
    "tldr": "我们提出了一种基于Transformer的新型动态图学习架构DyGFormer，并引入了统一的库DyGLib，以促进可重复、可扩展和可信的动态图学习研究。DyGFormer通过邻居共现编码方案和分块技术实现更长期历史的高效推理，在13个不同领域的数据集上实现了最先进的性能。"
}