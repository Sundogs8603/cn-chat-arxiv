{
    "title": "When does Privileged Information Explain Away Label Noise?. (arXiv:2303.01806v2 [cs.LG] UPDATED)",
    "abstract": "Leveraging privileged information (PI), or features available during training but not at test time, has recently been shown to be an effective method for addressing label noise. However, the reasons for its effectiveness are not well understood. In this study, we investigate the role played by different properties of the PI in explaining away label noise. Through experiments on multiple datasets with real PI (CIFAR-N/H) and a new large-scale benchmark ImageNet-PI, we find that PI is most helpful when it allows networks to easily distinguish clean from noisy data, while enabling a learning shortcut to memorize the noisy examples. Interestingly, when PI becomes too predictive of the target label, PI methods often perform worse than their no-PI baselines. Based on these findings, we propose several enhancements to the state-of-the-art PI methods and demonstrate the potential of PI as a means of tackling label noise. Finally, we show how we can easily combine the resulting PI approaches wi",
    "link": "http://arxiv.org/abs/2303.01806",
    "context": "Title: When does Privileged Information Explain Away Label Noise?. (arXiv:2303.01806v2 [cs.LG] UPDATED)\nAbstract: Leveraging privileged information (PI), or features available during training but not at test time, has recently been shown to be an effective method for addressing label noise. However, the reasons for its effectiveness are not well understood. In this study, we investigate the role played by different properties of the PI in explaining away label noise. Through experiments on multiple datasets with real PI (CIFAR-N/H) and a new large-scale benchmark ImageNet-PI, we find that PI is most helpful when it allows networks to easily distinguish clean from noisy data, while enabling a learning shortcut to memorize the noisy examples. Interestingly, when PI becomes too predictive of the target label, PI methods often perform worse than their no-PI baselines. Based on these findings, we propose several enhancements to the state-of-the-art PI methods and demonstrate the potential of PI as a means of tackling label noise. Finally, we show how we can easily combine the resulting PI approaches wi",
    "path": "papers/23/03/2303.01806.json",
    "total_tokens": 1001,
    "translated_title": "当特权信息能够解释标签噪音的时候？",
    "translated_abstract": "近期研究表明，利用训练过程中可用但测试时无法获取的特权信息（PI）来处理标签噪音是一种有效的方法，然而其有效性的原因尚不清楚。本研究通过在CIFAR-N/H多个数据集和新的大规模基准ImageNet-PI上的实验发现，当特权信息能够帮助神经网络轻松分辨出干净和嘈杂的点时，并能够为记忆嘈杂的例子提供学习的快捷方式时，它是最有帮助的。有趣的是，当PI变得过于预测目标标签时，PI方法往往比无PI基线表现更差。基于这些发现，我们提出了几种改进最先进的PI方法的方式，并展示了PI作为解决标签噪音的手段的潜力。最后，我们展示了如何轻松地将结果PI方法与常规方法相结合。",
    "tldr": "本文研究了在什么情况下，使用特权信息可以解决标签噪声问题，发现当PI帮助神经网络轻松分辨出嘈杂和干净的数据，与提供记忆嘈杂数据的学习快捷方式时，它是最有用的，在PI过于预测目标标签时，PI方法表现会更差。在此基础上，提出了一些增强方法，用于处理标签噪音。",
    "en_tdlr": "This paper investigates when and how privileged information (PI) can explain label noise, and finds that PI is most helpful when it allows the neural network to distinguish clean from noisy data, while enabling a learning shortcut to memorize the noisy examples. However, when PI becomes too predictive of the target label, PI methods may perform worse than their no-PI baselines. Based on these findings, the paper proposes enhancements to PI methods to tackle label noise."
}