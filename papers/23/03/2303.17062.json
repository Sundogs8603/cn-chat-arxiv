{
    "title": "Ideal Abstractions for Decision-Focused Learning. (arXiv:2303.17062v1 [cs.LG])",
    "abstract": "We present a methodology for formulating simplifying abstractions in machine learning systems by identifying and harnessing the utility structure of decisions. Machine learning tasks commonly involve high-dimensional output spaces (e.g., predictions for every pixel in an image or node in a graph), even though a coarser output would often suffice for downstream decision-making (e.g., regions of an image instead of pixels). Developers often hand-engineer abstractions of the output space, but numerous abstractions are possible and it is unclear how the choice of output space for a model impacts its usefulness in downstream decision-making. We propose a method that configures the output space automatically in order to minimize the loss of decision-relevant information. Taking a geometric perspective, we formulate a step of the algorithm as a projection of the probability simplex, termed fold, that minimizes the total loss of decision-related information in the H-entropy sense. Crucially, l",
    "link": "http://arxiv.org/abs/2303.17062",
    "context": "Title: Ideal Abstractions for Decision-Focused Learning. (arXiv:2303.17062v1 [cs.LG])\nAbstract: We present a methodology for formulating simplifying abstractions in machine learning systems by identifying and harnessing the utility structure of decisions. Machine learning tasks commonly involve high-dimensional output spaces (e.g., predictions for every pixel in an image or node in a graph), even though a coarser output would often suffice for downstream decision-making (e.g., regions of an image instead of pixels). Developers often hand-engineer abstractions of the output space, but numerous abstractions are possible and it is unclear how the choice of output space for a model impacts its usefulness in downstream decision-making. We propose a method that configures the output space automatically in order to minimize the loss of decision-relevant information. Taking a geometric perspective, we formulate a step of the algorithm as a projection of the probability simplex, termed fold, that minimizes the total loss of decision-related information in the H-entropy sense. Crucially, l",
    "path": "papers/23/03/2303.17062.json",
    "total_tokens": 829,
    "translated_title": "决策导向学习中的理想抽象",
    "translated_abstract": "我们提出了一种通过识别和利用决策的效用结构来制定简化抽象的机器学习系统的方法。机器学习任务通常涉及高维输出空间（例如图像中每个像素或图中节点的预测），尽管对于下游的决策制定来说，粗略的输出空间通常已经足够了（例如图像中的区域而不是像素）。开发者常常手工制定输出空间的抽象，但存在着众多的抽象形式，而且选择模型输出空间的影响对其在下游决策制定方面的实用性尚不清楚。我们提出了一种方法，自动配置输出空间以最小化与决策相关信息的损失。采用几何角度，我们将算法的一步作为概率单纯形的投影，称之为fold，以最小化决策相关信息在H-熵意义下的总损失。关键是，L…",
    "tldr": "本论文提出了一种通过自动配置输出空间以最小化与决策相关信息的损失来制定简化抽象的机器学习系统的方法。",
    "en_tdlr": "This paper proposes a methodology for formulating simplifying abstractions in machine learning systems by automatically configuring the output space to minimize the loss of decision-relevant information."
}