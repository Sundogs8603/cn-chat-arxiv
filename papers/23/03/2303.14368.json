{
    "title": "FlexNeRF: Photorealistic Free-viewpoint Rendering of Moving Humans from Sparse Views. (arXiv:2303.14368v1 [cs.CV])",
    "abstract": "We present FlexNeRF, a method for photorealistic freeviewpoint rendering of humans in motion from monocular videos. Our approach works well with sparse views, which is a challenging scenario when the subject is exhibiting fast/complex motions. We propose a novel approach which jointly optimizes a canonical time and pose configuration, with a pose-dependent motion field and pose-independent temporal deformations complementing each other. Thanks to our novel temporal and cyclic consistency constraints along with additional losses on intermediate representation such as segmentation, our approach provides high quality outputs as the observed views become sparser. We empirically demonstrate that our method significantly outperforms the state-of-the-art on public benchmark datasets as well as a self-captured fashion dataset. The project page is available at: https://flex-nerf.github.io/",
    "link": "http://arxiv.org/abs/2303.14368",
    "context": "Title: FlexNeRF: Photorealistic Free-viewpoint Rendering of Moving Humans from Sparse Views. (arXiv:2303.14368v1 [cs.CV])\nAbstract: We present FlexNeRF, a method for photorealistic freeviewpoint rendering of humans in motion from monocular videos. Our approach works well with sparse views, which is a challenging scenario when the subject is exhibiting fast/complex motions. We propose a novel approach which jointly optimizes a canonical time and pose configuration, with a pose-dependent motion field and pose-independent temporal deformations complementing each other. Thanks to our novel temporal and cyclic consistency constraints along with additional losses on intermediate representation such as segmentation, our approach provides high quality outputs as the observed views become sparser. We empirically demonstrate that our method significantly outperforms the state-of-the-art on public benchmark datasets as well as a self-captured fashion dataset. The project page is available at: https://flex-nerf.github.io/",
    "path": "papers/23/03/2303.14368.json",
    "total_tokens": 924,
    "translated_title": "FlexNeRF：从稀疏视角中实现运动人体的真实自由视角渲染",
    "translated_abstract": "本文提出了FlexNeRF方法，用于从单目视频中实现运动人体的真实自由视角渲染。我们的方法适用于稀疏视角，尤其是当主题表现出快速/复杂运动时，需要克服的挑战。我们提出了一种新颖的方法，同时优化规范时间和姿态配置，使姿态相关的运动场和姿态无关的时间变形互补。通过我们的新颖的时间和循环一致性约束以及对中间表示的额外损失（如分割），我们的方法提供高质量的输出，甚至在观察到的视角变得更稀疏时也是如此。我们实证演示了我们的方法在公开基准数据集以及自行捕获的时尚数据集上显着优于最先进技术。项目页面网址为：https://flex-nerf.github.io/",
    "tldr": "本文提出了FlexNeRF方法，用于从单目视频中实现运动人体的真实自由视角渲染。通过对时间和姿态配置的优化以及额外的损失，可在观察视角变得更稀疏时提供高质量的输出，这在公开基准数据集以及自行捕获的时尚数据集上都表现出优越性。",
    "en_tdlr": "FlexNeRF is proposed for photorealistic free-viewpoint rendering of humans in motion from monocular videos. The method optimizes canonical time and pose configuration with pose-dependent motion field and pose-independent temporal deformations. High quality outputs are achieved with temporal and cyclic consistency constraints and additional losses even when observed views become sparser, outperforming state-of-the-art on public benchmark datasets and a self-captured fashion dataset."
}