{
    "title": "EvalAttAI: A Holistic Approach to Evaluating Attribution Maps in Robust and Non-Robust Models. (arXiv:2303.08866v1 [cs.LG])",
    "abstract": "The expansion of explainable artificial intelligence as a field of research has generated numerous methods of visualizing and understanding the black box of a machine learning model. Attribution maps are generally used to highlight the parts of the input image that influence the model to make a specific decision. On the other hand, the robustness of machine learning models to natural noise and adversarial attacks is also being actively explored. This paper focuses on evaluating methods of attribution mapping to find whether robust neural networks are more explainable. We explore this problem within the application of classification for medical imaging. Explainability research is at an impasse. There are many methods of attribution mapping, but no current consensus on how to evaluate them and determine the ones that are the best. Our experiments on multiple datasets (natural and medical imaging) and various attribution methods reveal that two popular evaluation metrics, Deletion and Ins",
    "link": "http://arxiv.org/abs/2303.08866",
    "context": "Title: EvalAttAI: A Holistic Approach to Evaluating Attribution Maps in Robust and Non-Robust Models. (arXiv:2303.08866v1 [cs.LG])\nAbstract: The expansion of explainable artificial intelligence as a field of research has generated numerous methods of visualizing and understanding the black box of a machine learning model. Attribution maps are generally used to highlight the parts of the input image that influence the model to make a specific decision. On the other hand, the robustness of machine learning models to natural noise and adversarial attacks is also being actively explored. This paper focuses on evaluating methods of attribution mapping to find whether robust neural networks are more explainable. We explore this problem within the application of classification for medical imaging. Explainability research is at an impasse. There are many methods of attribution mapping, but no current consensus on how to evaluate them and determine the ones that are the best. Our experiments on multiple datasets (natural and medical imaging) and various attribution methods reveal that two popular evaluation metrics, Deletion and Ins",
    "path": "papers/23/03/2303.08866.json",
    "total_tokens": 1084,
    "translated_title": "EvalAttAI：一种综合评估鲁棒和非鲁棒模型中的归因映射方法的方法",
    "translated_abstract": "可解释的人工智能作为一个研究领域的扩张，已经产生了许多可视化和理解机器学习模型黑盒的方法。归因映射通常用于突出显示影响模型做出特定决策的输入图像的部分。另一方面，机器学习模型对自然噪声和对抗攻击的鲁棒性也正在积极探索。本文重点评估归因映射方法，以找到鲁棒神经网络是否更可解释。我们将这个问题探索在医学成像的分类应用中。可解释性研究已经陷入了僵局。虽然有许多归因映射方法，但目前并没有共识如何评估它们并确定最好的方法。我们在多个数据集（自然和医学成像）和各种归因方法上进行的实验证明，两种流行的评估指标，删除和插入稳健性，不足以评估鲁棒模型中的归因映射。相反，我们提出了一种综合方法EvalAttAI，在各种条件下考虑归因映射的稳健性和保真度。EvalAttAI可以帮助研究人员和实践者更好地评估归因映射的性能，并选择适合其特定应用的方法。",
    "tldr": "该论文提出了一个综合方法EvalAttAI，旨在同时考虑归因映射在各种条件下的稳健性和保真度，以更好地评估归因映射的性能并选择适合特定应用的方法。",
    "en_tdlr": "This paper proposes a holistic method, EvalAttAI, to better evaluate the performance of attribution maps and choose appropriate methods for specific applications by considering both the robustness and fidelity of attribution maps under various conditions."
}