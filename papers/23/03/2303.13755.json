{
    "title": "Sparsifiner: Learning Sparse Instance-Dependent Attention for Efficient Vision Transformers. (arXiv:2303.13755v1 [cs.CV])",
    "abstract": "Vision Transformers (ViT) have shown their competitive advantages performance-wise compared to convolutional neural networks (CNNs) though they often come with high computational costs. To this end, previous methods explore different attention patterns by limiting a fixed number of spatially nearby tokens to accelerate the ViT's multi-head self-attention (MHSA) operations. However, such structured attention patterns limit the token-to-token connections to their spatial relevance, which disregards learned semantic connections from a full attention mask. In this work, we propose a novel approach to learn instance-dependent attention patterns, by devising a lightweight connectivity predictor module to estimate the connectivity score of each pair of tokens. Intuitively, two tokens have high connectivity scores if the features are considered relevant either spatially or semantically. As each token only attends to a small number of other tokens, the binarized connectivity masks are often ver",
    "link": "http://arxiv.org/abs/2303.13755",
    "context": "Title: Sparsifiner: Learning Sparse Instance-Dependent Attention for Efficient Vision Transformers. (arXiv:2303.13755v1 [cs.CV])\nAbstract: Vision Transformers (ViT) have shown their competitive advantages performance-wise compared to convolutional neural networks (CNNs) though they often come with high computational costs. To this end, previous methods explore different attention patterns by limiting a fixed number of spatially nearby tokens to accelerate the ViT's multi-head self-attention (MHSA) operations. However, such structured attention patterns limit the token-to-token connections to their spatial relevance, which disregards learned semantic connections from a full attention mask. In this work, we propose a novel approach to learn instance-dependent attention patterns, by devising a lightweight connectivity predictor module to estimate the connectivity score of each pair of tokens. Intuitively, two tokens have high connectivity scores if the features are considered relevant either spatially or semantically. As each token only attends to a small number of other tokens, the binarized connectivity masks are often ver",
    "path": "papers/23/03/2303.13755.json",
    "total_tokens": 830,
    "translated_title": "Sparsifiner：学习稀疏的实例相关注意力用于高效的视觉Transformer",
    "translated_abstract": "视觉Transformer相比卷积神经网络在性能方面具有竞争优势，但往往伴随着高计算成本。为此，先前的方法通过限制一定数量的空间相邻令牌来探索不同的注意力模式，以加速ViT的多头自注意力（MHSA）操作。但是，这种结构化的注意力模式将令牌与其空间相关性的令牌之间的令牌 - 令牌连接限制在了一定范围内，这不考虑从完整的注意力掩码中学习的语义连接。在这项工作中，我们提出了一种新方法，通过设计一个轻量级的连接性预测模块来学习实例相关的注意力模式。直观的说，如果认为特征在空间或语义上是相关的，则两个标记具有高的连接得分。由于每个标记只与少量其他标记相关，因此二元化连接掩码通常是有效的 。",
    "tldr": "本文分析了ViT算法的计算成本高的问题，并且通过学习实例相关的注意力模式，来提高计算效率",
    "en_tdlr": "This paper addresses the high computational cost of the ViT algorithm and proposes a method to improve computational efficiency by learning instance-dependent attention patterns."
}