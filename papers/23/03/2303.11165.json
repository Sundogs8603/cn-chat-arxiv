{
    "title": "Computationally Budgeted Continual Learning: What Does Matter?. (arXiv:2303.11165v2 [cs.LG] UPDATED)",
    "abstract": "Continual Learning (CL) aims to sequentially train models on streams of incoming data that vary in distribution by preserving previous knowledge while adapting to new data. Current CL literature focuses on restricted access to previously seen data, while imposing no constraints on the computational budget for training. This is unreasonable for applications in-the-wild, where systems are primarily constrained by computational and time budgets, not storage. We revisit this problem with a large-scale benchmark and analyze the performance of traditional CL approaches in a compute-constrained setting, where effective memory samples used in training can be implicitly restricted as a consequence of limited computation. We conduct experiments evaluating various CL sampling strategies, distillation losses, and partial fine-tuning on two large-scale datasets, namely ImageNet2K and Continual Google Landmarks V2 in data incremental, class incremental, and time incremental settings. Through extensi",
    "link": "http://arxiv.org/abs/2303.11165",
    "context": "Title: Computationally Budgeted Continual Learning: What Does Matter?. (arXiv:2303.11165v2 [cs.LG] UPDATED)\nAbstract: Continual Learning (CL) aims to sequentially train models on streams of incoming data that vary in distribution by preserving previous knowledge while adapting to new data. Current CL literature focuses on restricted access to previously seen data, while imposing no constraints on the computational budget for training. This is unreasonable for applications in-the-wild, where systems are primarily constrained by computational and time budgets, not storage. We revisit this problem with a large-scale benchmark and analyze the performance of traditional CL approaches in a compute-constrained setting, where effective memory samples used in training can be implicitly restricted as a consequence of limited computation. We conduct experiments evaluating various CL sampling strategies, distillation losses, and partial fine-tuning on two large-scale datasets, namely ImageNet2K and Continual Google Landmarks V2 in data incremental, class incremental, and time incremental settings. Through extensi",
    "path": "papers/23/03/2303.11165.json",
    "total_tokens": 977,
    "translated_title": "计算预算的持续学习：什么才是重要的？",
    "translated_abstract": "持续学习旨在通过保持以前的知识并适应新数据的方式，对流入的数据流进行顺序训练模型，其数据分布不断变化。当前的持续学习文献主要关注对以前的数据有限访问的问题，而对训练的计算预算没有施加任何限制。这在野外应用中是不合理的，因为系统主要受到计算和时间预算的限制，而不是存储。我们通过一个大规模的基准测试来重新审视这个问题，并分析传统持续学习方法在计算受限环境中的性能，其中由于有限的计算，训练中使用的有效内存样本可以被隐式限制。我们对两个大规模数据集（ImageNet2K和Continual Google Landmarks V2）进行了实验，评估了各种持续学习采样策略、蒸馏损失和部分微调，在数据递增、类递增和时间递增设置中进行了评估。通过扩展实验评价了各种持续学习采样策略、蒸馏损失和部分微调等方法。",
    "tldr": "本文针对持续学习中计算预算的限制问题进行了研究，通过大规模基准测试评估了传统持续学习方法在计算受限环境中的性能，并探讨了不同采样策略、蒸馏损失和部分微调方法在数据递增、类递增和时间递增设置中的效果。",
    "en_tdlr": "This paper investigates the issue of computational budget in continual learning, evaluates the performance of traditional CL methods in a compute-constrained setting, and explores the effects of different sampling strategies, distillation losses, and partial fine-tuning methods in data incremental, class incremental, and time incremental settings through a large-scale benchmark."
}