{
    "title": "Deep Ranking Ensembles for Hyperparameter Optimization. (arXiv:2303.15212v2 [cs.LG] UPDATED)",
    "abstract": "Automatically optimizing the hyperparameters of Machine Learning algorithms is one of the primary open questions in AI. Existing work in Hyperparameter Optimization (HPO) trains surrogate models for approximating the response surface of hyperparameters as a regression task. In contrast, we hypothesize that the optimal strategy for training surrogates is to preserve the ranks of the performances of hyperparameter configurations as a Learning to Rank problem. As a result, we present a novel method that meta-learns neural network surrogates optimized for ranking the configurations' performances while modeling their uncertainty via ensembling. In a large-scale experimental protocol comprising 12 baselines, 16 HPO search spaces and 86 datasets/tasks, we demonstrate that our method achieves new state-of-the-art results in HPO.",
    "link": "http://arxiv.org/abs/2303.15212",
    "context": "Title: Deep Ranking Ensembles for Hyperparameter Optimization. (arXiv:2303.15212v2 [cs.LG] UPDATED)\nAbstract: Automatically optimizing the hyperparameters of Machine Learning algorithms is one of the primary open questions in AI. Existing work in Hyperparameter Optimization (HPO) trains surrogate models for approximating the response surface of hyperparameters as a regression task. In contrast, we hypothesize that the optimal strategy for training surrogates is to preserve the ranks of the performances of hyperparameter configurations as a Learning to Rank problem. As a result, we present a novel method that meta-learns neural network surrogates optimized for ranking the configurations' performances while modeling their uncertainty via ensembling. In a large-scale experimental protocol comprising 12 baselines, 16 HPO search spaces and 86 datasets/tasks, we demonstrate that our method achieves new state-of-the-art results in HPO.",
    "path": "papers/23/03/2303.15212.json",
    "total_tokens": 772,
    "translated_title": "深度排名集成用于超参数优化",
    "translated_abstract": "自动优化机器学习算法的超参数是人工智能中的主要问题之一。现有的超参数优化工作训练了代理模型来近似超参数响应面作为回归任务。相反，我们假设训练代理的最佳策略是保持超参数配置的性能排名作为一个学习排名问题。因此，我们提出了一种新颖的方法，元学习神经网络代理来优化配置的性能排名，同时通过集成的方式对其不确定性进行建模。在一个大规模的实验协议中，包含12个基线，16个HPO搜索空间和86个数据集/任务，我们证明了我们的方法在HPO方面取得了新的最先进的结果。",
    "tldr": "研究提出了一种新型方法，元学习神经网络代理来优化超参数配置的性能排名，并通过集成来建模不确定性，该方法在超参数优化方面取得了最新最先进的结果。",
    "en_tdlr": "This paper proposes a novel method to meta-learn neural network surrogates optimized for ranking the performances of hyperparameter configurations while modeling their uncertainty via ensembling, achieving state-of-the-art results in large-scale Hyperparameter Optimization experiments with 12 baselines, 16 HPO search spaces and 86 datasets/tasks."
}