{
    "title": "Model-Agnostic Federated Learning. (arXiv:2303.04906v2 [cs.LG] UPDATED)",
    "abstract": "Since its debut in 2016, Federated Learning (FL) has been tied to the inner workings of Deep Neural Networks (DNNs). On the one hand, this allowed its development and widespread use as DNNs proliferated. On the other hand, it neglected all those scenarios in which using DNNs is not possible or advantageous. The fact that most current FL frameworks only allow training DNNs reinforces this problem. To address the lack of FL solutions for non-DNN-based use cases, we propose MAFL (Model-Agnostic Federated Learning). MAFL marries a model-agnostic FL algorithm, AdaBoost.F, with an open industry-grade FL framework: Intel OpenFL. MAFL is the first FL system not tied to any specific type of machine learning model, allowing exploration of FL scenarios beyond DNNs and trees. We test MAFL from multiple points of view, assessing its correctness, flexibility and scaling properties up to 64 nodes. We optimised the base software achieving a 5.5x speedup on a standard FL scenario. MAFL is compatible wi",
    "link": "http://arxiv.org/abs/2303.04906",
    "context": "Title: Model-Agnostic Federated Learning. (arXiv:2303.04906v2 [cs.LG] UPDATED)\nAbstract: Since its debut in 2016, Federated Learning (FL) has been tied to the inner workings of Deep Neural Networks (DNNs). On the one hand, this allowed its development and widespread use as DNNs proliferated. On the other hand, it neglected all those scenarios in which using DNNs is not possible or advantageous. The fact that most current FL frameworks only allow training DNNs reinforces this problem. To address the lack of FL solutions for non-DNN-based use cases, we propose MAFL (Model-Agnostic Federated Learning). MAFL marries a model-agnostic FL algorithm, AdaBoost.F, with an open industry-grade FL framework: Intel OpenFL. MAFL is the first FL system not tied to any specific type of machine learning model, allowing exploration of FL scenarios beyond DNNs and trees. We test MAFL from multiple points of view, assessing its correctness, flexibility and scaling properties up to 64 nodes. We optimised the base software achieving a 5.5x speedup on a standard FL scenario. MAFL is compatible wi",
    "path": "papers/23/03/2303.04906.json",
    "total_tokens": 975,
    "translated_title": "模型无关的联邦学习",
    "translated_abstract": "自从2016年引入以来，联邦学习（FL）一直与深度神经网络（DNN）的内部工作紧密相关。一方面，这使得其得以在DNN广泛使用的同时发展和推广。另一方面，这忽视了所有那些不可行或不具优势使用DNN的情况。目前大部分FL框架仅允许训练DNN，加剧了这个问题。为了解决非DNN场景下缺乏FL解决方案的问题，我们提出了MAFL（模型无关的联邦学习）。MAFL将一个模型无关的FL算法AdaBoost.F与一种开放的工业级FL框架Intel OpenFL结合起来。MAFL是第一个不与任何特定类型的机器学习模型绑定的FL系统，允许探索超越DNN和树的FL场景。我们从多个角度测试了MAFL，评估了其正确性、灵活性和可扩展性，最多达到64个节点。我们对基础软件进行了优化，在标准FL场景中实现了5.5倍的加速。MAFL与",
    "tldr": "这篇论文提出了一种模型无关的联邦学习系统（MAFL），该系统不仅适用于深度神经网络（DNN）和决策树等特定类型的机器学习模型，还能在非DNN场景中应用，并且通过优化实现了较高的性能。"
}