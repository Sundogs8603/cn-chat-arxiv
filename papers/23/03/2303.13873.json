{
    "title": "Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation. (arXiv:2303.13873v1 [cs.CV])",
    "abstract": "Automatic 3D content creation has achieved rapid progress recently due to the availability of pre-trained, large language models and image diffusion models, forming the emerging topic of text-to-3D content creation. Existing text-to-3D methods commonly use implicit scene representations, which couple the geometry and appearance via volume rendering and are suboptimal in terms of recovering finer geometries and achieving photorealistic rendering; consequently, they are less effective for generating high-quality 3D assets. In this work, we propose a new method of Fantasia3D for high-quality text-to-3D content creation. Key to Fantasia3D is the disentangled modeling and learning of geometry and appearance. For geometry learning, we rely on a hybrid scene representation, and propose to encode surface normal extracted from the representation as the input of the image diffusion model. For appearance modeling, we introduce the spatially varying bidirectional reflectance distribution function ",
    "link": "http://arxiv.org/abs/2303.13873",
    "context": "Title: Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation. (arXiv:2303.13873v1 [cs.CV])\nAbstract: Automatic 3D content creation has achieved rapid progress recently due to the availability of pre-trained, large language models and image diffusion models, forming the emerging topic of text-to-3D content creation. Existing text-to-3D methods commonly use implicit scene representations, which couple the geometry and appearance via volume rendering and are suboptimal in terms of recovering finer geometries and achieving photorealistic rendering; consequently, they are less effective for generating high-quality 3D assets. In this work, we propose a new method of Fantasia3D for high-quality text-to-3D content creation. Key to Fantasia3D is the disentangled modeling and learning of geometry and appearance. For geometry learning, we rely on a hybrid scene representation, and propose to encode surface normal extracted from the representation as the input of the image diffusion model. For appearance modeling, we introduce the spatially varying bidirectional reflectance distribution function ",
    "path": "papers/23/03/2303.13873.json",
    "total_tokens": 1051,
    "translated_title": "Fantasia3D: 用于高质量文本生成3D内容的几何和外观分离方法",
    "translated_abstract": "最近，由于预训练的大型语言模型和图像扩散模型的提供，自动3D内容的创建取得了快速进展，形成了文本生成3D内容的新兴话题。现有的文本生成3D方法通常使用隐式场景表示，这些表示使用体积渲染将几何和外观耦合在一起，对于恢复更精细的几何和实现照片般逼真的渲染是次优的；因此，它们在产生高质量3D资产方面不够有效。在这项工作中，我们提出了一种名为Fantasia3D的新方法，用于高质量文本生成3D内容。Fantasia3D的关键在于几何和外观的分离建模和学习。对于几何学习，我们依靠混合场景表示，并建议将从表示中提取的表面法线编码作为图像扩散模型的输入。对于外观建模，我们引入了空间可变双向反射率分布函数（SVBRDF）来分离材料和光照属性。实验结果表明，我们的方法在几何细节和逼真渲染方面超越了现有技术，并且在从自然语言文本中生成高质量的3D内容方面更为有效和高效。",
    "tldr": "Fantasia3D是一种新的文本生成3D内容的方法，通过分离几何和外观建模和学习，提高了几何细节和逼真渲染，并更有效和高效。",
    "en_tdlr": "Fantasia3D is a new method for text-to-3D content creation that improves geometric details and photorealistic rendering by disentangling geometry and appearance through modeling and learning, leading to more effectiveness and efficiency for generating high-quality 3D assets from natural language text."
}