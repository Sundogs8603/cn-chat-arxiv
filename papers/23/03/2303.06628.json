{
    "title": "Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language Models. (arXiv:2303.06628v2 [cs.CV] UPDATED)",
    "abstract": "Continual learning (CL) can help pre-trained vision-language models efficiently adapt to new or under-trained data distributions without re-training. Nevertheless, during the continual training of the Contrastive Language-Image Pre-training (CLIP) model, we observe that the model's zero-shot transfer ability significantly degrades due to catastrophic forgetting. Existing CL methods can mitigate forgetting by replaying previous data. However, since the CLIP dataset is private, replay methods cannot access the pre-training dataset. In addition, replaying data of previously learned downstream tasks can enhance their performance but comes at the cost of sacrificing zero-shot performance. To address this challenge, we propose a novel method ZSCL to prevent zero-shot transfer degradation in the continual learning of vision-language models in both feature and parameter space. In the feature space, a reference dataset is introduced for distillation between the current and initial models. The r",
    "link": "http://arxiv.org/abs/2303.06628",
    "context": "Title: Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language Models. (arXiv:2303.06628v2 [cs.CV] UPDATED)\nAbstract: Continual learning (CL) can help pre-trained vision-language models efficiently adapt to new or under-trained data distributions without re-training. Nevertheless, during the continual training of the Contrastive Language-Image Pre-training (CLIP) model, we observe that the model's zero-shot transfer ability significantly degrades due to catastrophic forgetting. Existing CL methods can mitigate forgetting by replaying previous data. However, since the CLIP dataset is private, replay methods cannot access the pre-training dataset. In addition, replaying data of previously learned downstream tasks can enhance their performance but comes at the cost of sacrificing zero-shot performance. To address this challenge, we propose a novel method ZSCL to prevent zero-shot transfer degradation in the continual learning of vision-language models in both feature and parameter space. In the feature space, a reference dataset is introduced for distillation between the current and initial models. The r",
    "path": "papers/23/03/2303.06628.json",
    "total_tokens": 952,
    "translated_title": "防止视觉语言模型在不断学习中出现零样例转移降级",
    "translated_abstract": "连续学习（CL）可以帮助预训练的视觉语言模型在不重新训练的情况下高效地适应新的或未经训练的数据分布。然而，在对比语言-图像预训练（CLIP）模型的连续训练过程中，我们观察到由于灾难性遗忘，模型的零样例转移能力显著降低。现有的CL方法可以通过回放先前的数据来减轻遗忘。然而，由于CLIP数据集是私有的，回放方法无法访问预训练数据集。此外，回放先前学习的下游任务数据可以提高它们的性能，但会损耗零样例性能。为了解决这个挑战，在特征空间和参数空间中，我们提出了一种新颖的方法ZSCL，以防止在连续学习视觉语言模型时出现零样例转移降级。在特征空间中，引入了一个参考数据集，用于当前和初始模型之间的蒸馏。",
    "tldr": "本文提出了一种新的方法ZSCL，旨在解决连续学习视觉语言模型中零样例转移降级的问题。通过在特征空间中引入参考数据集进行蒸馏，该方法能够有效地防止模型的零样例转移能力的降低。"
}