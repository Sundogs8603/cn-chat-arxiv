{
    "title": "NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation. (arXiv:2303.12346v1 [cs.CV])",
    "abstract": "In this paper, we propose NUWA-XL, a novel Diffusion over Diffusion architecture for eXtremely Long video generation. Most current work generates long videos segment by segment sequentially, which normally leads to the gap between training on short videos and inferring long videos, and the sequential generation is inefficient. Instead, our approach adopts a ``coarse-to-fine'' process, in which the video can be generated in parallel at the same granularity. A global diffusion model is applied to generate the keyframes across the entire time range, and then local diffusion models recursively fill in the content between nearby frames. This simple yet effective strategy allows us to directly train on long videos (3376 frames) to reduce the training-inference gap, and makes it possible to generate all segments in parallel. To evaluate our model, we build FlintstonesHD dataset, a new benchmark for long video generation. Experiments show that our model not only generates high-quality long vid",
    "link": "http://arxiv.org/abs/2303.12346",
    "context": "Title: NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation. (arXiv:2303.12346v1 [cs.CV])\nAbstract: In this paper, we propose NUWA-XL, a novel Diffusion over Diffusion architecture for eXtremely Long video generation. Most current work generates long videos segment by segment sequentially, which normally leads to the gap between training on short videos and inferring long videos, and the sequential generation is inefficient. Instead, our approach adopts a ``coarse-to-fine'' process, in which the video can be generated in parallel at the same granularity. A global diffusion model is applied to generate the keyframes across the entire time range, and then local diffusion models recursively fill in the content between nearby frames. This simple yet effective strategy allows us to directly train on long videos (3376 frames) to reduce the training-inference gap, and makes it possible to generate all segments in parallel. To evaluate our model, we build FlintstonesHD dataset, a new benchmark for long video generation. Experiments show that our model not only generates high-quality long vid",
    "path": "papers/23/03/2303.12346.json",
    "total_tokens": 761,
    "translated_title": "NUWA-XL: 扩散过程在极长视频生成中的应用",
    "translated_abstract": "本文提出了一种新颖的NUWA-XL扩散过程架构，用于极长视频的生成。我们提出了一个“由粗到细”的过程，应用全局扩散模型在整个时间范围内生成关键帧，并且递归地应用本地扩散模型填充相邻帧之间的内容。这种简单而有效的策略使我们能够直接在长视频（3376帧）上进行训练，从而减小了训练-推断之间的差距，并且使得所有的段落都可以并行生成。",
    "tldr": "NUWA-XL采用扩散过程，在极长视频的生成中实现了由粗到细的过程，减小了训练-推断之间的差距，并且使得所有的段落都可以并行生成。",
    "en_tdlr": "NUWA-XL applies diffusion process in a coarse-to-fine process, reducing the training-inference gap and enabling parallel generation of all segments in extremely long video generation."
}