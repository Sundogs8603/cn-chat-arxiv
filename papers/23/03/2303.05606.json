{
    "title": "Variance-aware robust reinforcement learning with linear function approximation with heavy-tailed rewards. (arXiv:2303.05606v1 [cs.LG])",
    "abstract": "This paper presents two algorithms, AdaOFUL and VARA, for online sequential decision-making in the presence of heavy-tailed rewards with only finite variances. For linear stochastic bandits, we address the issue of heavy-tailed rewards by modifying the adaptive Huber regression and proposing AdaOFUL. AdaOFUL achieves a state-of-the-art regret bound of $\\widetilde{\\mathcal{O}}\\big(d\\big(\\sum_{t=1}^T \\nu_{t}^2\\big)^{1/2}+d\\big)$ as if the rewards were uniformly bounded, where $\\nu_{t}^2$ is the observed conditional variance of the reward at round $t$, $d$ is the feature dimension, and $\\widetilde{\\mathcal{O}}(\\cdot)$ hides logarithmic dependence. Building upon AdaOFUL, we propose VARA for linear MDPs, which achieves a tighter variance-aware regret bound of $\\widetilde{\\mathcal{O}}(d\\sqrt{H\\mathcal{G}^*K})$. Here, $H$ is the length of episodes, $K$ is the number of episodes, and $\\mathcal{G}^*$ is a smaller instance-dependent quantity that can be bounded by other instance-dependent quanti",
    "link": "http://arxiv.org/abs/2303.05606",
    "raw_ret": "{\n    \"translated_title\": \"具有重尾奖励的带线性函数逼近的方差感知鲁棒强化学习\",\n    \"translated_abstract\": \"本文提出了两种算法AdaOFUL和VARA，用于在线顺序决策，处理仅有有限方差的重尾奖励。针对线性随机赌博机，我们通过修改自适应Huber回归和提出AdaOFUL来解决重尾奖励的问题。 AdaOFUL在使奖励服从均匀边界的情况下，实现了一个最先进的遗憾度绑定，其中νt2是回合t的奖励的观察条件方差，d是特征维度，而$ \\widetilde {\\ mathcal {O}}(\\cdot) $隐藏对数依赖性。在此基础上，我们提出了VARA用于线性MDP，它实现了一个更紧的方差感知遗憾度绑定，即$ \\widetilde{\\mathcal{O}}(d\\sqrt{H\\mathcal{G}^*K})$。这里，H是剧集长度，K是剧集数量，而$\\mathcal{G}^*$是一个小得多的依赖于实例的量，可以通过其他依赖于实例的量来限制。\",\n    \"tldr\": \"该论文提出了AdaOFUL和VARA两种算法，用于处理仅有有限方差的重尾奖励，并在带线性函数逼近的情况下实现了具有竞争力的遗憾度绑定。\" \n}",
    "total_tokens": 989
}