{
    "title": "$\\alpha$-divergence Improves the Entropy Production Estimation via Machine Learning. (arXiv:2303.02901v2 [cond-mat.stat-mech] UPDATED)",
    "abstract": "Recent years have seen a surge of interest in the algorithmic estimation of stochastic entropy production (EP) from trajectory data via machine learning. A crucial element of such algorithms is the identification of a loss function whose minimization guarantees the accurate EP estimation. In this study, we show that there exists a host of loss functions, namely those implementing a variational representation of the $\\alpha$-divergence, which can be used for the EP estimation. By fixing $\\alpha$ to a value between $-1$ and $0$, the $\\alpha$-NEEP (Neural Estimator for Entropy Production) exhibits a much more robust performance against strong nonequilibrium driving or slow dynamics, which adversely affects the existing method based on the Kullback-Leibler divergence ($\\alpha = 0$). In particular, the choice of $\\alpha = -0.5$ tends to yield the optimal results. To corroborate our findings, we present an exactly solvable simplification of the EP estimation problem, whose loss function land",
    "link": "http://arxiv.org/abs/2303.02901",
    "context": "Title: $\\alpha$-divergence Improves the Entropy Production Estimation via Machine Learning. (arXiv:2303.02901v2 [cond-mat.stat-mech] UPDATED)\nAbstract: Recent years have seen a surge of interest in the algorithmic estimation of stochastic entropy production (EP) from trajectory data via machine learning. A crucial element of such algorithms is the identification of a loss function whose minimization guarantees the accurate EP estimation. In this study, we show that there exists a host of loss functions, namely those implementing a variational representation of the $\\alpha$-divergence, which can be used for the EP estimation. By fixing $\\alpha$ to a value between $-1$ and $0$, the $\\alpha$-NEEP (Neural Estimator for Entropy Production) exhibits a much more robust performance against strong nonequilibrium driving or slow dynamics, which adversely affects the existing method based on the Kullback-Leibler divergence ($\\alpha = 0$). In particular, the choice of $\\alpha = -0.5$ tends to yield the optimal results. To corroborate our findings, we present an exactly solvable simplification of the EP estimation problem, whose loss function land",
    "path": "papers/23/03/2303.02901.json",
    "total_tokens": 966,
    "translated_title": "$\\alpha$-散度通过机器学习改进了熵产生估计",
    "translated_abstract": "最近几年，通过机器学习从轨迹数据估计随机熵产生（EP）的算法引起了极大兴趣。这类算法的关键是找到一个损失函数，使其最小化能够保证准确的EP估计。本研究展示了存在一类损失函数，即那些实现了$\\alpha$-散度的变分表示的函数，可以用于EP估计。通过将$\\alpha$固定为在-1到0之间的值，$\\alpha$-NEEP（Entropy Production的神经估计器）在强非平衡驱动或者缓慢动力学的情况下表现出更为稳健的性能，而这些情况对基于Kullback-Leibler散度（$\\alpha=0$）的现有方法产生不利影响。特别地，选择$\\alpha=-0.5$往往能得到最优结果。为了证实我们的发现，我们还提供了一个可精确求解的EP估计问题简化模型，其损失函数为land",
    "tldr": "本研究通过机器学习提出了一种基于$\\alpha$-散度的损失函数，在估计随机熵产生时表现出更加稳健的性能，尤其在强非平衡驱动或者缓慢动力学的情况下。选择$\\alpha=-0.5$能获得最优结果。"
}