{
    "title": "Generating Adversarial Samples in Mini-Batches May Be Detrimental To Adversarial Robustness. (arXiv:2303.17720v1 [cs.LG])",
    "abstract": "Neural networks have been proven to be both highly effective within computer vision, and highly vulnerable to adversarial attacks. Consequently, as the use of neural networks increases due to their unrivaled performance, so too does the threat posed by adversarial attacks. In this work, we build towards addressing the challenge of adversarial robustness by exploring the relationship between the mini-batch size used during adversarial sample generation and the strength of the adversarial samples produced. We demonstrate that an increase in mini-batch size results in a decrease in the efficacy of the samples produced, and we draw connections between these observations and the phenomenon of vanishing gradients. Next, we formulate loss functions such that adversarial sample strength is not degraded by mini-batch size. Our findings highlight a potential risk for underestimating the true (practical) strength of adversarial attacks, and a risk of overestimating a model's robustness. We share ",
    "link": "http://arxiv.org/abs/2303.17720",
    "context": "Title: Generating Adversarial Samples in Mini-Batches May Be Detrimental To Adversarial Robustness. (arXiv:2303.17720v1 [cs.LG])\nAbstract: Neural networks have been proven to be both highly effective within computer vision, and highly vulnerable to adversarial attacks. Consequently, as the use of neural networks increases due to their unrivaled performance, so too does the threat posed by adversarial attacks. In this work, we build towards addressing the challenge of adversarial robustness by exploring the relationship between the mini-batch size used during adversarial sample generation and the strength of the adversarial samples produced. We demonstrate that an increase in mini-batch size results in a decrease in the efficacy of the samples produced, and we draw connections between these observations and the phenomenon of vanishing gradients. Next, we formulate loss functions such that adversarial sample strength is not degraded by mini-batch size. Our findings highlight a potential risk for underestimating the true (practical) strength of adversarial attacks, and a risk of overestimating a model's robustness. We share ",
    "path": "papers/23/03/2303.17720.json",
    "total_tokens": 981,
    "translated_title": "在小批量生成对抗样本可能会损害对抗性鲁棒性",
    "translated_abstract": "神经网络在计算机视觉中的效果得到了证明，但同时也容易受到对抗性攻击的影响。因此，随着神经网络的使用增加，对抗性攻击带来的威胁也随之增加。本文通过探索在对抗性样本生成过程中使用的小批量大小与生成的对抗性样本强度之间的关系，来解决对抗性鲁棒性的挑战。我们证明了小批量大小的增加会导致生成的对抗性样本效果的下降，并将这些观察结果与梯度消失现象联系起来。接下来，我们制定了损失函数，使得小批量大小不会降低对抗性样本强度。我们的研究结果突出了低估对抗性攻击真实（实际）强度和高估模型鲁棒性的潜在风险。为了提高研究的可重复性、模型的可用性和鲁棒性，我们分享了代码。",
    "tldr": "本文探究小批量在生成对抗样本中的作用，发现小批量大小的增加会导致生成的对抗样本效果下降，进一步提醒人们低估真实的对抗攻击强度并高估模型的鲁棒性。",
    "en_tdlr": "This paper explores the role of mini-batches in generating adversarial samples and finds that an increase in mini-batch size leads to a decrease in the efficacy of the samples produced, highlighting a potential risk of underestimating the true strength of adversarial attacks and overestimating a model's robustness."
}