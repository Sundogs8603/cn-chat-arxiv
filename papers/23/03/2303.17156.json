{
    "title": "MAHALO: Unifying Offline Reinforcement Learning and Imitation Learning from Observations. (arXiv:2303.17156v1 [cs.LG])",
    "abstract": "We study a new paradigm for sequential decision making, called offline Policy Learning from Observation (PLfO). Offline PLfO aims to learn policies using datasets with substandard qualities: 1) only a subset of trajectories is labeled with rewards, 2) labeled trajectories may not contain actions, 3) labeled trajectories may not be of high quality, and 4) the overall data may not have full coverage. Such imperfection is common in real-world learning scenarios, so offline PLfO encompasses many existing offline learning setups, including offline imitation learning (IL), ILfO, and reinforcement learning (RL). In this work, we present a generic approach, called Modality-agnostic Adversarial Hypothesis Adaptation for Learning from Observations (MAHALO), for offline PLfO. Built upon the pessimism concept in offline RL, MAHALO optimizes the policy using a performance lower bound that accounts for uncertainty due to the dataset's insufficient converge. We implement this idea by adversarially tr",
    "link": "http://arxiv.org/abs/2303.17156",
    "context": "Title: MAHALO: Unifying Offline Reinforcement Learning and Imitation Learning from Observations. (arXiv:2303.17156v1 [cs.LG])\nAbstract: We study a new paradigm for sequential decision making, called offline Policy Learning from Observation (PLfO). Offline PLfO aims to learn policies using datasets with substandard qualities: 1) only a subset of trajectories is labeled with rewards, 2) labeled trajectories may not contain actions, 3) labeled trajectories may not be of high quality, and 4) the overall data may not have full coverage. Such imperfection is common in real-world learning scenarios, so offline PLfO encompasses many existing offline learning setups, including offline imitation learning (IL), ILfO, and reinforcement learning (RL). In this work, we present a generic approach, called Modality-agnostic Adversarial Hypothesis Adaptation for Learning from Observations (MAHALO), for offline PLfO. Built upon the pessimism concept in offline RL, MAHALO optimizes the policy using a performance lower bound that accounts for uncertainty due to the dataset's insufficient converge. We implement this idea by adversarially tr",
    "path": "papers/23/03/2303.17156.json",
    "total_tokens": 1037,
    "translated_title": "MAHALO: 统一离线强化学习和基于观测的模仿学习",
    "translated_abstract": "本文研究了一种名为离线观测学习（PLfO）的新的顺序决策制定范例。离线PLfO旨在使用质量不佳的数据集来学习策略：1）仅有一部分轨迹被标记为奖励，2）标记轨迹可能不包含动作，3）标记轨迹可能质量不高，4）总体数据可能不具备全面性。这些缺陷在真实的学习场景中很常见，因此离线PLfO包括许多现有的离线学习设置，包括离线模仿学习（IL），ILfO和强化学习（RL）。在本文中，我们提出了一种名为模态不可知对抗假设适应学习的离线PLfO的通用方法（MAHALO）。 MAHALO基于离线RL中的悲观主义概念，使用考虑由于数据集不足的不确定性的性能下界来优化策略。我们通过对有关观察和动作模态的有限假设进行对抗变换来实现这一想法。实验结果表明，MAHALO在各种离线PLfO任务上都很有效。",
    "tldr": "本文提出了一种名为MAHALO的方法，可以统一离线强化学习和基于观测的模仿学习，帮助处理数据集质量不佳的情况下的顺序决策制定问题，并在实验中证明了其有效性。",
    "en_tdlr": "This article proposes a generic method called MAHALO that unifies offline reinforcement learning and imitation learning from observations, which is helpful for sequential decision making problems with substandard data quality, and its effectiveness is demonstrated in experiments."
}