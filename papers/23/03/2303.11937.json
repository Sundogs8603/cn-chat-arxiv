{
    "title": "High Probability Bounds for Stochastic Continuous Submodular Maximization. (arXiv:2303.11937v1 [cs.DS])",
    "abstract": "We consider maximization of stochastic monotone continuous submodular functions (CSF) with a diminishing return property. Existing algorithms only guarantee the performance \\textit{in expectation}, and do not bound the probability of getting a bad solution. This implies that for a particular run of the algorithms, the solution may be much worse than the provided guarantee in expectation. In this paper, we first empirically verify that this is indeed the case. Then, we provide the first \\textit{high-probability} analysis of the existing methods for stochastic CSF maximization, namely PGA, boosted PGA, SCG, and SCG++. Finally, we provide an improved high-probability bound for SCG, under slightly stronger assumptions, with a better convergence rate than that of the expected solution. Through extensive experiments on non-concave quadratic programming (NQP) and optimal budget allocation, we confirm the validity of our bounds and show that even in the worst-case, PGA converges to $OPT/2$, an",
    "link": "http://arxiv.org/abs/2303.11937",
    "context": "Title: High Probability Bounds for Stochastic Continuous Submodular Maximization. (arXiv:2303.11937v1 [cs.DS])\nAbstract: We consider maximization of stochastic monotone continuous submodular functions (CSF) with a diminishing return property. Existing algorithms only guarantee the performance \\textit{in expectation}, and do not bound the probability of getting a bad solution. This implies that for a particular run of the algorithms, the solution may be much worse than the provided guarantee in expectation. In this paper, we first empirically verify that this is indeed the case. Then, we provide the first \\textit{high-probability} analysis of the existing methods for stochastic CSF maximization, namely PGA, boosted PGA, SCG, and SCG++. Finally, we provide an improved high-probability bound for SCG, under slightly stronger assumptions, with a better convergence rate than that of the expected solution. Through extensive experiments on non-concave quadratic programming (NQP) and optimal budget allocation, we confirm the validity of our bounds and show that even in the worst-case, PGA converges to $OPT/2$, an",
    "path": "papers/23/03/2303.11937.json",
    "total_tokens": 888,
    "translated_title": "随机连续次模最大化问题高概率边界研究",
    "translated_abstract": "本论文研究了具有递减收益特性的随机单调连续次模函数（CSF）最大化问题。现有算法只能提供期望表现保证，并不能限制得到不好解的概率。这意味着对于算法的某个运行，得到的解可能要比期望保证的更糟糕。在本文中，我们首先通过实验证实了这一点。然后，我们提供了现有随机CSF最大化方法的第一分析，即PGA，boosted PGA，SCG和SCG ++。最后，我们在稍微强一点的假设下提供了SCG的改进高概率界，其收敛速度比预期解更快。通过对非凸二次规划（NQP）和最优预算分配的广泛实验，我们确认了我们的界限的有效性，并表明即使在最坏情况下，PGA也会收敛到$OPT / 2$，一个。",
    "tldr": "本论文提出了针对随机CSF最大化问题的高概率边界算法，取得了比现有方法更好的收敛速度，经过验证在最坏情况下也能收敛到较优解。",
    "en_tdlr": "This paper proposes a high-probability bound algorithm for stochastic CSF maximization problem, with better convergence rate than existing methods, which can converge to a better solution even in worst cases, as verified by experiments on NQP and optimal budget allocation."
}