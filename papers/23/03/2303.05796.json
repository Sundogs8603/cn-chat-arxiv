{
    "title": "Training, Architecture, and Prior for Deterministic Uncertainty Methods. (arXiv:2303.05796v2 [cs.LG] UPDATED)",
    "abstract": "Accurate and efficient uncertainty estimation is crucial to build reliable Machine Learning (ML) models capable to provide calibrated uncertainty estimates, generalize and detect Out-Of-Distribution (OOD) datasets. To this end, Deterministic Uncertainty Methods (DUMs) is a promising model family capable to perform uncertainty estimation in a single forward pass. This work investigates important design choices in DUMs: (1) we show that training schemes decoupling the core architecture and the uncertainty head schemes can significantly improve uncertainty performances. (2) we demonstrate that the core architecture expressiveness is crucial for uncertainty performance and that additional architecture constraints to avoid feature collapse can deteriorate the trade-off between OOD generalization and detection. (3) Contrary to other Bayesian models, we show that the prior defined by DUMs do not have a strong effect on the final performances.",
    "link": "http://arxiv.org/abs/2303.05796",
    "context": "Title: Training, Architecture, and Prior for Deterministic Uncertainty Methods. (arXiv:2303.05796v2 [cs.LG] UPDATED)\nAbstract: Accurate and efficient uncertainty estimation is crucial to build reliable Machine Learning (ML) models capable to provide calibrated uncertainty estimates, generalize and detect Out-Of-Distribution (OOD) datasets. To this end, Deterministic Uncertainty Methods (DUMs) is a promising model family capable to perform uncertainty estimation in a single forward pass. This work investigates important design choices in DUMs: (1) we show that training schemes decoupling the core architecture and the uncertainty head schemes can significantly improve uncertainty performances. (2) we demonstrate that the core architecture expressiveness is crucial for uncertainty performance and that additional architecture constraints to avoid feature collapse can deteriorate the trade-off between OOD generalization and detection. (3) Contrary to other Bayesian models, we show that the prior defined by DUMs do not have a strong effect on the final performances.",
    "path": "papers/23/03/2303.05796.json",
    "total_tokens": 965,
    "translated_title": "确定性不确定性方法中的训练、架构和先验",
    "translated_abstract": "准确高效的不确定性估计对于构建可靠的机器学习模型至关重要，而确定性不确定性方法是一种有前途的模型，能够在单次前向传播中执行不确定性估计。本文研究了DUMs的重要设计选择：（1）我们展示了将核心架构和不确定性头方案解耦的训练方案，可以显著提高不确定性性能。（2）我们证明了核心架构表达能力对不确定性性能至关重要，并且避免特征崩溃的额外架构约束可能破坏OOD泛化和检测之间的折衷。 （3）与其他贝叶斯模型相反，我们展示了DUMs定义的先验对最终性能没有强烈影响。",
    "tldr": "本文研究了确定性不确定性方法中的重要设计选择，包括解耦核心架构和不确定性头方案的训练、核心架构表达能力、避免特征崩溃的额外架构约束和先验影响等。研究发现，训练方案和核心架构表达能力对不确定性性能至关重要，而避免特征崩溃的额外架构约束可能会破坏OOD泛化和检测之间的折衷，DUMs定义的先验则对最终性能影响较小。",
    "en_tdlr": "This work investigates important design choices in Deterministic Uncertainty Methods (DUMs), including the decoupling of training between the core architecture and uncertainty head schemes, the importance of core architecture expressiveness, the potential trade-off between OOD generalization and detection when avoiding feature collapse via additional architecture constraints, and the minimal impact of prior defined by DUMs on final performance."
}