{
    "title": "Deep Kernel Methods Learn Better: From Cards to Process Optimization. (arXiv:2303.14554v2 [cs.LG] UPDATED)",
    "abstract": "The ability of deep learning methods to perform classification and regression tasks relies heavily on their capacity to uncover manifolds in high-dimensional data spaces and project them into low-dimensional representation spaces. In this study, we investigate the structure and character of the manifolds generated by classical variational autoencoder (VAE) approaches and deep kernel learning (DKL). In the former case, the structure of the latent space is determined by the properties of the input data alone, while in the latter, the latent manifold forms as a result of an active learning process that balances the data distribution and target functionalities. We show that DKL with active learning can produce a more compact and smooth latent space which is more conducive to optimization compared to previously reported methods, such as the VAE. We demonstrate this behavior using a simple cards data set and extend it to the optimization of domain-generated trajectories in physical systems. ",
    "link": "http://arxiv.org/abs/2303.14554",
    "context": "Title: Deep Kernel Methods Learn Better: From Cards to Process Optimization. (arXiv:2303.14554v2 [cs.LG] UPDATED)\nAbstract: The ability of deep learning methods to perform classification and regression tasks relies heavily on their capacity to uncover manifolds in high-dimensional data spaces and project them into low-dimensional representation spaces. In this study, we investigate the structure and character of the manifolds generated by classical variational autoencoder (VAE) approaches and deep kernel learning (DKL). In the former case, the structure of the latent space is determined by the properties of the input data alone, while in the latter, the latent manifold forms as a result of an active learning process that balances the data distribution and target functionalities. We show that DKL with active learning can produce a more compact and smooth latent space which is more conducive to optimization compared to previously reported methods, such as the VAE. We demonstrate this behavior using a simple cards data set and extend it to the optimization of domain-generated trajectories in physical systems. ",
    "path": "papers/23/03/2303.14554.json",
    "total_tokens": 895,
    "translated_title": "深度核方法学习更好：从纸牌到过程优化",
    "tldr": "本研究探索了经典的变分自动编码器（VAE）方法和深度核学习（DKL）方法生成的流形的结构和特征，发现使用DKL的主动学习能够产生更紧凑和平滑的潜在空间，有利于优化。"
}