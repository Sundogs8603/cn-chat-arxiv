{
    "title": "Improved Differentially Private Regression via Gradient Boosting. (arXiv:2303.03451v2 [cs.LG] UPDATED)",
    "abstract": "We revisit the problem of differentially private squared error linear regression. We observe that existing state-of-the-art methods are sensitive to the choice of hyperparameters -- including the ``clipping threshold'' that cannot be set optimally in a data-independent way. We give a new algorithm for private linear regression based on gradient boosting. We show that our method consistently improves over the previous state of the art when the clipping threshold is taken to be fixed without knowledge of the data, rather than optimized in a non-private way -- and that even when we optimize the hyperparameters of competitor algorithms non-privately, our algorithm is no worse and often better. In addition to a comprehensive set of experiments, we give theoretical insights to explain this behavior.",
    "link": "http://arxiv.org/abs/2303.03451",
    "context": "Title: Improved Differentially Private Regression via Gradient Boosting. (arXiv:2303.03451v2 [cs.LG] UPDATED)\nAbstract: We revisit the problem of differentially private squared error linear regression. We observe that existing state-of-the-art methods are sensitive to the choice of hyperparameters -- including the ``clipping threshold'' that cannot be set optimally in a data-independent way. We give a new algorithm for private linear regression based on gradient boosting. We show that our method consistently improves over the previous state of the art when the clipping threshold is taken to be fixed without knowledge of the data, rather than optimized in a non-private way -- and that even when we optimize the hyperparameters of competitor algorithms non-privately, our algorithm is no worse and often better. In addition to a comprehensive set of experiments, we give theoretical insights to explain this behavior.",
    "path": "papers/23/03/2303.03451.json",
    "total_tokens": 767,
    "translated_title": "利用梯度提升改进差分隐私回归",
    "translated_abstract": "我们重新研究了差分隐私平方误差线性回归问题。我们发现现有的最先进方法对于超参数的选择是敏感的，包括“剪切阈值”，不能以数据独立的方式进行最佳设置。我们提出了一种基于梯度提升的差分隐私线性回归新算法。我们表明，当将剪切阈值固定为不知道数据的情况下时，我们的方法始终优于先前的最先进方法。而且，即使我们不以隐私方式优化竞争算法的超参数，我们的算法也不会更差，而且通常更好。除了全面的实验外，我们还提供了理论洞察以解释这种行为。",
    "tldr": "本文提出了一种基于梯度提升的差分隐私线性回归新算法，能够在剪切阈值固定的情况下始终优于先前的最先进方法。",
    "en_tdlr": "This paper proposes a new differentially private linear regression algorithm based on gradient boosting, which consistently outperforms previous state-of-the-art methods under fixed clipping threshold without knowledge of data. Theoretical insights are also provided to explain this behavior."
}