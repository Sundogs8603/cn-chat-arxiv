{
    "title": "Cross-Modal Entity Matching for Visually Rich Documents",
    "abstract": "arXiv:2303.00720v2 Announce Type: replace  Abstract: Visually rich documents (e.g. leaflets, banners, magazine articles) are physical or digital documents that utilize visual cues to augment their semantics. Information contained in these documents are ad-hoc and often incomplete. Existing works that enable structured querying on these documents do not take this into account. This makes it difficult to contextualize the information retrieved from querying these documents and gather actionable insights from them. We propose Juno -- a cross-modal entity matching framework to address this limitation. It augments heterogeneous documents with supplementary information by matching a text span in the document with semantically similar tuples from an external database. Our main contribution in this is a deep neural network with attention that goes beyond traditional keyword-based matching and finds matching tuples by aligning text spans and relational tuples on a multimodal encoding space with",
    "link": "https://arxiv.org/abs/2303.00720",
    "context": "Title: Cross-Modal Entity Matching for Visually Rich Documents\nAbstract: arXiv:2303.00720v2 Announce Type: replace  Abstract: Visually rich documents (e.g. leaflets, banners, magazine articles) are physical or digital documents that utilize visual cues to augment their semantics. Information contained in these documents are ad-hoc and often incomplete. Existing works that enable structured querying on these documents do not take this into account. This makes it difficult to contextualize the information retrieved from querying these documents and gather actionable insights from them. We propose Juno -- a cross-modal entity matching framework to address this limitation. It augments heterogeneous documents with supplementary information by matching a text span in the document with semantically similar tuples from an external database. Our main contribution in this is a deep neural network with attention that goes beyond traditional keyword-based matching and finds matching tuples by aligning text spans and relational tuples on a multimodal encoding space with",
    "path": "papers/23/03/2303.00720.json",
    "total_tokens": 893,
    "translated_title": "视觉丰富文档的跨模态实体匹配",
    "translated_abstract": "视觉丰富文档（如传单、横幅、杂志文章）是利用视觉线索来增强语义的实体或数字化文档。这些文档中包含的信息往往是临时的，经常是不完整的。现有的允许对这些文档进行结构化查询的方法没有考虑到这一点。这使得在从这些文档中进行查询并从中获取可操作见解时，很难将检索到的信息置于上下文中。我们提出了Juno - 一个跨模态实体匹配框架，以解决这一局限。它通过将文档中的文本跨度与外部数据库中语义类似的元组进行匹配，从而为异构文档提供补充信息。我们的主要贡献是一个带注意力机制的深度神经网络，它超越了传统的基于关键字的匹配，通过在多模态编码空间上对齐文本跨度和关系元组来找到匹配的元组。",
    "tldr": "提出了一个跨模态实体匹配框架 Juno，通过深度神经网络和注意力机制，实现了文档中文本跨度与外部数据库中语义相似元组的匹配，从而解决了视觉丰富文档信息检索中缺乏上下文和见解的问题。",
    "en_tdlr": "Proposed a cross-modal entity matching framework Juno that addresses the lack of context and insights in visually rich document information retrieval by matching text spans in documents with semantically similar tuples from an external database using a deep neural network with attention mechanism."
}