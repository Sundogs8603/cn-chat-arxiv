{
    "title": "Task-oriented Memory-efficient Pruning-Adapter. (arXiv:2303.14704v2 [cs.CL] UPDATED)",
    "abstract": "The Outstanding performance and growing size of Large Language Models has led to increased attention in parameter efficient learning. The two predominant approaches are Adapters and Pruning. Adapters are to freeze the model and give it a new weight matrix on the side, which can significantly reduce the time and memory of training, but the cost is that the evaluation and testing will increase the time and memory consumption. Pruning is to cut off some weight and re-distribute the remaining weight, which sacrifices the complexity of training at the cost of extremely high memory and training time, making the cost of evaluation and testing relatively low. So efficiency of training and inference can't be obtained in the same time. In this work, we propose a task-oriented Pruning-Adapter method that achieve a high memory efficiency of training and memory, and speeds up training time and ensures no significant decrease in accuracy in GLUE tasks, achieving training and inference efficiency at ",
    "link": "http://arxiv.org/abs/2303.14704",
    "context": "Title: Task-oriented Memory-efficient Pruning-Adapter. (arXiv:2303.14704v2 [cs.CL] UPDATED)\nAbstract: The Outstanding performance and growing size of Large Language Models has led to increased attention in parameter efficient learning. The two predominant approaches are Adapters and Pruning. Adapters are to freeze the model and give it a new weight matrix on the side, which can significantly reduce the time and memory of training, but the cost is that the evaluation and testing will increase the time and memory consumption. Pruning is to cut off some weight and re-distribute the remaining weight, which sacrifices the complexity of training at the cost of extremely high memory and training time, making the cost of evaluation and testing relatively low. So efficiency of training and inference can't be obtained in the same time. In this work, we propose a task-oriented Pruning-Adapter method that achieve a high memory efficiency of training and memory, and speeds up training time and ensures no significant decrease in accuracy in GLUE tasks, achieving training and inference efficiency at ",
    "path": "papers/23/03/2303.14704.json",
    "total_tokens": 883,
    "translated_title": "面向任务的内存高效剪枝适配器",
    "translated_abstract": "大型语言模型的出色性能和不断增长的规模导致了对参数高效学习的 increased attention。主要的两种方法是适配器和剪枝。适配器是在模型上冻结并给它一个新的权重矩阵，在训练时间和内存方面可以显著降低成本，但这样会增加评估和测试的时间和内存消耗。剪枝是截断一些权重并重新分配剩余的权重，这样可以牺牲训练的复杂度，以极高的内存和训练时间为代价，使评估和测试的成本相对较低。因此，训练和推理的效率无法同时得到。在这项工作中，我们提出了一种面向任务的剪枝适配器方法，实现了训练和内存的高内存效率，加快了训练时间，并确保在 GLUE 任务中准确性没有显著下降，实现了训练和推理的效率。",
    "tldr": "本文提出了一种面向任务的剪枝适配器方法，既实现了训练和内存的高效率，又加快了训练时间，并且在 GLUE 任务中没有显著降低准确性。",
    "en_tdlr": "This paper proposes a task-oriented Pruning-Adapter method that achieves high efficiency in both training and memory, accelerates training time, and ensures no significant decrease in accuracy in GLUE tasks."
}