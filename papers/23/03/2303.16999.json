{
    "title": "PopSparse: Accelerated block sparse matrix multiplication on IPU. (arXiv:2303.16999v1 [cs.LG])",
    "abstract": "Reducing the computational cost of running large scale neural networks using sparsity has attracted great attention in the deep learning community. While much success has been achieved in reducing FLOP and parameter counts while maintaining acceptable task performance, achieving actual speed improvements has typically been much more difficult, particularly on general purpose accelerators (GPAs) such as NVIDIA GPUs using low precision number formats. In this work we introduce PopSparse, a library that enables fast sparse operations on Graphcore IPUs by leveraging both the unique hardware characteristics of IPUs as well as any block structure defined in the data. We target two different types of sparsity: static, where the sparsity pattern is fixed at compile-time; and dynamic, where it can change each time the model is run. We present benchmark results for matrix multiplication for both of these modes on IPU with a range of block sizes, matrix sizes and densities. Results indicate that ",
    "link": "http://arxiv.org/abs/2303.16999",
    "context": "Title: PopSparse: Accelerated block sparse matrix multiplication on IPU. (arXiv:2303.16999v1 [cs.LG])\nAbstract: Reducing the computational cost of running large scale neural networks using sparsity has attracted great attention in the deep learning community. While much success has been achieved in reducing FLOP and parameter counts while maintaining acceptable task performance, achieving actual speed improvements has typically been much more difficult, particularly on general purpose accelerators (GPAs) such as NVIDIA GPUs using low precision number formats. In this work we introduce PopSparse, a library that enables fast sparse operations on Graphcore IPUs by leveraging both the unique hardware characteristics of IPUs as well as any block structure defined in the data. We target two different types of sparsity: static, where the sparsity pattern is fixed at compile-time; and dynamic, where it can change each time the model is run. We present benchmark results for matrix multiplication for both of these modes on IPU with a range of block sizes, matrix sizes and densities. Results indicate that ",
    "path": "papers/23/03/2303.16999.json",
    "total_tokens": 929,
    "translated_title": "PopSparse：在IPU上加速的块稀疏矩阵乘法",
    "translated_abstract": "在深度学习社区中，使用稀疏性降低大规模神经网络的计算成本引起了广泛关注。虽然在维持可接受的任务性能的同时减少FLOP和参数数量取得了许多成功，但实际上获得加速改进通常更加困难，特别是在通用加速器（GPA）上，如使用低精度数字格式的NVIDIA GPU。在本文中，我们介绍了PopSparse，一种利用IPU的独特硬件特性以及数据中定义的任何块结构实现在Graphcore IPUs上快速稀疏操作的库。我们针对两种不同类型的稀疏性：静态稀疏性，其中稀疏模式在编译时固定；动态稀疏性，其中每次运行模型时都可以改变。我们针对各种块大小、矩阵大小和密度在IPU上进行矩阵乘法的基准测试。结果表明：",
    "tldr": "将大规模神经网络的计算成本降低到可接受的任务性能和加速改进是一个具有挑战的问题。本文介绍了一种利用IPU独特的硬件特性和数据中定义的任何块结构实现在Graphcore IPUs上快速稀疏操作的库——PopSparse。",
    "en_tdlr": "Reducing the computational cost of large scale neural networks while maintaining acceptable task performance and achieving speed improvements poses a challenge. This paper introduces PopSparse, a library that leverages both the unique hardware characteristics of IPUs and any block structure defined in the data to enable fast sparse operations on Graphcore IPUs."
}