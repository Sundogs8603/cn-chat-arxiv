{
    "title": "Context-faithful Prompting for Large Language Models. (arXiv:2303.11315v2 [cs.CL] UPDATED)",
    "abstract": "Large language models (LLMs) encode parametric knowledge about world facts and have shown remarkable performance in knowledge-driven NLP tasks. However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks). In this paper, we seek to assess and enhance LLMs' contextual faithfulness in two aspects: knowledge conflict and prediction with abstention. We demonstrate that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies. In particular, we identify opinion-based prompts and counterfactual demonstrations as the most effective methods. Opinion-based prompts reframe the context as a narrator's statement and inquire about the narrator's opinions, while counterfactual demonstrations use instances containing false facts to improve faithfulness in knowledge conflict situations. Neither technique requires additional training. W",
    "link": "http://arxiv.org/abs/2303.11315",
    "context": "Title: Context-faithful Prompting for Large Language Models. (arXiv:2303.11315v2 [cs.CL] UPDATED)\nAbstract: Large language models (LLMs) encode parametric knowledge about world facts and have shown remarkable performance in knowledge-driven NLP tasks. However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks). In this paper, we seek to assess and enhance LLMs' contextual faithfulness in two aspects: knowledge conflict and prediction with abstention. We demonstrate that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies. In particular, we identify opinion-based prompts and counterfactual demonstrations as the most effective methods. Opinion-based prompts reframe the context as a narrator's statement and inquire about the narrator's opinions, while counterfactual demonstrations use instances containing false facts to improve faithfulness in knowledge conflict situations. Neither technique requires additional training. W",
    "path": "papers/23/03/2303.11315.json",
    "total_tokens": 933,
    "translated_title": "大型语言模型的上下文准确提示",
    "translated_abstract": "大型语言模型（LLMs）编码了关于世界事实的参数化知识，在知识驱动的自然语言处理任务中表现出了非凡的性能。然而，它们对参数化知识的依赖可能导致它们忽视上下文线索，在上下文敏感的自然语言处理任务（例如知识获取任务）中做出错误的预测。本文旨在评估和增强LLMs在两个方面的上下文准确性：知识冲突和预测的弃权。我们证明可以通过精心设计的提示策略显著提高LLMs的准确性。特别是，我们确定了基于观点的提示和虚构演示是最有效的方法。基于观点的提示将上下文重新构造为讲述者的陈述，并询问讲述者的意见，而虚构演示使用包含错误事实的实例，以提高在知识冲突情况下的准确性。两种技术都不需要额外的训练。",
    "tldr": "大型语言模型的准确提示对于解决上下文敏感的自然语言处理任务非常重要。本文通过研究两个方面的上下文准确性（知识冲突和预测的弃权），发现了基于观点的提示和虚构演示两种有效的方法，提高了大型语言模型的上下文准确性。"
}