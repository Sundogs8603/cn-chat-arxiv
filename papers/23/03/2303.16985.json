{
    "title": "Adapting to the Low-Resource Double-Bind: Investigating Low-Compute Methods on Low-Resource African Languages. (arXiv:2303.16985v1 [cs.CL])",
    "abstract": "Many natural language processing (NLP) tasks make use of massively pre-trained language models, which are computationally expensive. However, access to high computational resources added to the issue of data scarcity of African languages constitutes a real barrier to research experiments on these languages. In this work, we explore the applicability of low-compute approaches such as language adapters in the context of this low-resource double-bind. We intend to answer the following question: do language adapters allow those who are doubly bound by data and compute to practically build useful models? Through fine-tuning experiments on African languages, we evaluate their effectiveness as cost-effective approaches to low-resource African NLP. Using solely free compute resources, our results show that language adapters achieve comparable performances to massive pre-trained language models which are heavy on computational resources. This opens the door to further experimentation and explor",
    "link": "http://arxiv.org/abs/2303.16985",
    "context": "Title: Adapting to the Low-Resource Double-Bind: Investigating Low-Compute Methods on Low-Resource African Languages. (arXiv:2303.16985v1 [cs.CL])\nAbstract: Many natural language processing (NLP) tasks make use of massively pre-trained language models, which are computationally expensive. However, access to high computational resources added to the issue of data scarcity of African languages constitutes a real barrier to research experiments on these languages. In this work, we explore the applicability of low-compute approaches such as language adapters in the context of this low-resource double-bind. We intend to answer the following question: do language adapters allow those who are doubly bound by data and compute to practically build useful models? Through fine-tuning experiments on African languages, we evaluate their effectiveness as cost-effective approaches to low-resource African NLP. Using solely free compute resources, our results show that language adapters achieve comparable performances to massive pre-trained language models which are heavy on computational resources. This opens the door to further experimentation and explor",
    "path": "papers/23/03/2303.16985.json",
    "total_tokens": 888,
    "translated_title": "适应低资源双连通性: 探究对非洲低资源语言采用低计算方法的有效性",
    "translated_abstract": "自然语言处理(NLP)的许多任务都使用大规模的预训练语言模型，但这些模型计算成本高昂。然而，非洲语言数据的稀缺性和高计算资源的获取限制了对这些语言进行研究的可能性。本文探讨了在这种低资源双连通性背景下，语言适配器等低计算方法的适用性。我们试图回答以下问题：语言适配器是否允许那些在数据和计算方面双重受限的人实际上构建有用的模型？通过在非洲语言上进行微调实验，我们评估了它们作为低资源非洲NLP的成本效益方法的有效性。使用全部免费计算资源，我们的结果显示，与计算资源损耗大的大规模预训练语言模型相比，语言适配器实现了可比的性能。这为进一步的实验和探索打开了大门。",
    "tldr": "本文探讨了在低资源条件下，采用语言适配器等低计算方法在非洲语言上进行NLP研究的有效性，并通过微调实验得出了实现可比性能的结论。",
    "en_tdlr": "This paper investigates the effectiveness of low-compute methods such as language adapters on NLP research for low-resource African languages, and concludes that these methods can achieve comparable performance to computationally expensive pre-trained language models, opening doors for further experimentation and exploration in this area."
}