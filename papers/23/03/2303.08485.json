{
    "title": "Can Fairness be Automated? Guidelines and Opportunities for Fairness-aware AutoML. (arXiv:2303.08485v1 [cs.AI])",
    "abstract": "The field of automated machine learning (AutoML) introduces techniques that automate parts of the development of machine learning (ML) systems, accelerating the process and reducing barriers for novices. However, decisions derived from ML models can reproduce, amplify, or even introduce unfairness in our societies, causing harm to (groups of) individuals. In response, researchers have started to propose AutoML systems that jointly optimize fairness and predictive performance to mitigate fairness-related harm. However, fairness is a complex and inherently interdisciplinary subject, and solely posing it as an optimization problem can have adverse side effects. With this work, we aim to raise awareness among developers of AutoML systems about such limitations of fairness-aware AutoML, while also calling attention to the potential of AutoML as a tool for fairness research. We present a comprehensive overview of different ways in which fairness-related harm can arise and the ensuing implica",
    "link": "http://arxiv.org/abs/2303.08485",
    "context": "Title: Can Fairness be Automated? Guidelines and Opportunities for Fairness-aware AutoML. (arXiv:2303.08485v1 [cs.AI])\nAbstract: The field of automated machine learning (AutoML) introduces techniques that automate parts of the development of machine learning (ML) systems, accelerating the process and reducing barriers for novices. However, decisions derived from ML models can reproduce, amplify, or even introduce unfairness in our societies, causing harm to (groups of) individuals. In response, researchers have started to propose AutoML systems that jointly optimize fairness and predictive performance to mitigate fairness-related harm. However, fairness is a complex and inherently interdisciplinary subject, and solely posing it as an optimization problem can have adverse side effects. With this work, we aim to raise awareness among developers of AutoML systems about such limitations of fairness-aware AutoML, while also calling attention to the potential of AutoML as a tool for fairness research. We present a comprehensive overview of different ways in which fairness-related harm can arise and the ensuing implica",
    "path": "papers/23/03/2303.08485.json",
    "total_tokens": 992,
    "translated_title": "自动化学习中的公平性问题：公平性感知的AutoML的指南与机会",
    "translated_abstract": "自动化机器学习（AutoML）领域引入了一些自动化机器学习（ML）系统开发的技术，加速了该过程，降低了新手的门槛。然而，基于ML模型的决策可能会在我们的社会中复制、放大或甚至引入不公平性，对（群体中的）个人造成伤害。为此，研究人员已经开始提出联合优化公平性和预测性能的AutoML系统，以减轻与公平性相关的损害。然而，公平性是一个复杂且固有的跨学科主题，仅将其作为优化问题可能会产生不良副作用。本文旨在提高AutoML系统开发者对公平性感知AutoML的这些局限性的认识，同时引起AutoML作为公平研究工具的潜力的注意。我们给出了不同方式的公平相关危害及其随之而来的影响的综合概述。",
    "tldr": "自动化机器学习技术（AutoML）的发展加速了机器学习模型的开发，但由于模型的决策可能会引发不公平问题，因此研究者提出了联合优化公平性和预测性能的AutoML系统，本文呼吁AutoML系统开发者应该认识到公平性处理未必是纯粹的优化问题，并提醒此类算法也具备成为公平研究工具的潜力。",
    "en_tdlr": "AutoML accelerates the development of machine learning models, but decisions derived from ML models can cause unfairness and harm to individuals. Researchers proposed fairness-aware AutoML systems to mitigate the harm, but treating fairness solely as an optimization problem can lead to adverse side effects. This article calls for awareness of the limitations of fairness-aware AutoML among developers, while highlighting the potential of AutoML as a tool for fairness research."
}