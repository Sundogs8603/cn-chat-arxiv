{
    "title": "MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks. (arXiv:2303.16839v1 [cs.CV])",
    "abstract": "The development of language models have moved from encoder-decoder to decoder-only designs. In addition, the common knowledge has it that the two most popular multimodal tasks, the generative and contrastive tasks, tend to conflict with one another, are hard to accommodate in one architecture, and further need complex adaptations for downstream tasks. We propose a novel paradigm of training with a decoder-only model for multimodal tasks, which is surprisingly effective in jointly learning of these disparate vision-language tasks. This is done with a simple model, called MaMMUT. It consists of a single vision encoder and a text decoder, and is able to accommodate contrastive and generative learning by a novel two-pass approach on the text decoder. We demonstrate that joint training of these diverse-objective tasks is simple, effective, and maximizes the weight-sharing of the model. Furthermore, the same architecture enables straightforward extensions to open-vocabulary object detection ",
    "link": "http://arxiv.org/abs/2303.16839",
    "context": "Title: MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks. (arXiv:2303.16839v1 [cs.CV])\nAbstract: The development of language models have moved from encoder-decoder to decoder-only designs. In addition, the common knowledge has it that the two most popular multimodal tasks, the generative and contrastive tasks, tend to conflict with one another, are hard to accommodate in one architecture, and further need complex adaptations for downstream tasks. We propose a novel paradigm of training with a decoder-only model for multimodal tasks, which is surprisingly effective in jointly learning of these disparate vision-language tasks. This is done with a simple model, called MaMMUT. It consists of a single vision encoder and a text decoder, and is able to accommodate contrastive and generative learning by a novel two-pass approach on the text decoder. We demonstrate that joint training of these diverse-objective tasks is simple, effective, and maximizes the weight-sharing of the model. Furthermore, the same architecture enables straightforward extensions to open-vocabulary object detection ",
    "path": "papers/23/03/2303.16839.json",
    "total_tokens": 872,
    "translated_title": "MaMMUT: 一种用于多模态任务联合学习的简单架构",
    "translated_abstract": "语言模型的发展已从编码-解码转向仅解码的设计。此外，普遍认为，最流行的两种多模态任务，生成任务和对比任务，往往互相冲突，难以在一个架构中容纳，并进一步需要用于下游任务的复杂调整。我们提出了一种新的培训范式，用于多模态任务的仅解码模型，这在联合学习这些不同的视觉语言任务方面非常有效。这是通过一个简单的模型MaMMUT实现的。它由单一的视觉编码器和一个文本解码器组成，并能够通过文本解码器上的新的两步方法容纳对比和生成学习。我们证明这些不同目标任务的联合训练是简单的，有效的，并最大化了模型的权重共享。此外，相同的架构使得对开放词汇对象检测的简单扩展成为可能。",
    "tldr": "提出了一种名为MaMMUT的简单模型，可以通过两步方法容纳对比和生成学习，并在联合训练不同的视觉语言任务时表现出很高的效力。",
    "en_tdlr": "The paper proposes a simple model called MaMMUT, which accommodates contrastive and generative learning by a novel two-pass approach on the text decoder, and exhibits high effectiveness in joint training of diverse-objective tasks in vision-language."
}