{
    "title": "Analyzing the Generalizability of Deep Contextualized Language Representations For Text Classification. (arXiv:2303.12936v1 [cs.CL])",
    "abstract": "This study evaluates the robustness of two state-of-the-art deep contextual language representations, ELMo and DistilBERT, on supervised learning of binary protest news classification and sentiment analysis of product reviews. A \"cross-context\" setting is enabled using test sets that are distinct from the training data. Specifically, in the news classification task, the models are developed on local news from India and tested on the local news from China. In the sentiment analysis task, the models are trained on movie reviews and tested on customer reviews. This comparison is aimed at exploring the limits of the representative power of today's Natural Language Processing systems on the path to the systems that are generalizable to real-life scenarios. The models are fine-tuned and fed into a Feed-Forward Neural Network and a Bidirectional Long Short Term Memory network. Multinomial Naive Bayes and Linear Support Vector Machine are used as traditional baselines. The results show that, i",
    "link": "http://arxiv.org/abs/2303.12936",
    "context": "Title: Analyzing the Generalizability of Deep Contextualized Language Representations For Text Classification. (arXiv:2303.12936v1 [cs.CL])\nAbstract: This study evaluates the robustness of two state-of-the-art deep contextual language representations, ELMo and DistilBERT, on supervised learning of binary protest news classification and sentiment analysis of product reviews. A \"cross-context\" setting is enabled using test sets that are distinct from the training data. Specifically, in the news classification task, the models are developed on local news from India and tested on the local news from China. In the sentiment analysis task, the models are trained on movie reviews and tested on customer reviews. This comparison is aimed at exploring the limits of the representative power of today's Natural Language Processing systems on the path to the systems that are generalizable to real-life scenarios. The models are fine-tuned and fed into a Feed-Forward Neural Network and a Bidirectional Long Short Term Memory network. Multinomial Naive Bayes and Linear Support Vector Machine are used as traditional baselines. The results show that, i",
    "path": "papers/23/03/2303.12936.json",
    "total_tokens": 948,
    "translated_title": "深度上下文化语言表示对文本分类可泛化性的分析",
    "translated_abstract": "本研究评估了两种最先进的深度上下文语言表示——ELMo和DistilBERT在二元抗议新闻分类和产品评论情感分析的监督学习中的稳健性，采用了“交叉环境”设置，使用测试集不同于训练数据。具体来说，在新闻分类任务中，这些模型是在印度本地新闻上开发的，并在中国本地新闻上进行测试。在情感分析任务中，这些模型是在电影评论方面进行训练的，而在客户评论方面进行测试的。本次比较旨在探索当今自然语言处理系统的代表能力极限，以达到通用于真实场景的系统。模型经过微调，使用前馈神经网络和双向长短期记忆网络。多项式朴素贝叶斯和线性支持向量机用作传统基线。结果表明，ELMo和DistilBERT表现优于传统基线，但在跨环境测试中的表现较差。",
    "tldr": "本研究通过使用“交叉环境”设置，评估了ELMo和DistilBERT在新闻分类和情感分析任务上的监督学习性能表现。虽然两种模型超过传统基线，但在跨环境测试中表现较差，暴露了现代自然语言处理系统的代表能力极限。",
    "en_tdlr": "This study evaluates the supervised learning performance of ELMo and DistilBERT in news classification and sentiment analysis tasks using a \"cross-context\" setting. Although both models outperformed traditional baselines, they showed poor performance in cross-context tests, revealing the representative power limits of modern natural language processing systems."
}