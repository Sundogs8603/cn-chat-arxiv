{
    "title": "Solving Regularized Exp, Cosh and Sinh Regression Problems. (arXiv:2303.15725v1 [cs.LG])",
    "abstract": "In modern machine learning, attention computation is a fundamental task for training large language models such as Transformer, GPT-4 and ChatGPT. In this work, we study exponential regression problem which is inspired by the softmax/exp unit in the attention mechanism in large language models. The standard exponential regression is non-convex. We study the regularization version of exponential regression problem which is a convex problem. We use approximate newton method to solve in input sparsity time.  Formally, in this problem, one is given matrix $A \\in \\mathbb{R}^{n \\times d}$, $b \\in \\mathbb{R}^n$, $w \\in \\mathbb{R}^n$ and any of functions $\\exp, \\cosh$ and $\\sinh$ denoted as $f$. The goal is to find the optimal $x$ that minimize $ 0.5 \\| f(Ax) - b \\|_2^2 + 0.5 \\| \\mathrm{diag}(w) A x \\|_2^2$. The straightforward method is to use the naive Newton's method. Let $\\mathrm{nnz}(A)$ denote the number of non-zeros entries in matrix $A$. Let $\\omega$ denote the exponent of matrix multi",
    "link": "http://arxiv.org/abs/2303.15725",
    "context": "Title: Solving Regularized Exp, Cosh and Sinh Regression Problems. (arXiv:2303.15725v1 [cs.LG])\nAbstract: In modern machine learning, attention computation is a fundamental task for training large language models such as Transformer, GPT-4 and ChatGPT. In this work, we study exponential regression problem which is inspired by the softmax/exp unit in the attention mechanism in large language models. The standard exponential regression is non-convex. We study the regularization version of exponential regression problem which is a convex problem. We use approximate newton method to solve in input sparsity time.  Formally, in this problem, one is given matrix $A \\in \\mathbb{R}^{n \\times d}$, $b \\in \\mathbb{R}^n$, $w \\in \\mathbb{R}^n$ and any of functions $\\exp, \\cosh$ and $\\sinh$ denoted as $f$. The goal is to find the optimal $x$ that minimize $ 0.5 \\| f(Ax) - b \\|_2^2 + 0.5 \\| \\mathrm{diag}(w) A x \\|_2^2$. The straightforward method is to use the naive Newton's method. Let $\\mathrm{nnz}(A)$ denote the number of non-zeros entries in matrix $A$. Let $\\omega$ denote the exponent of matrix multi",
    "path": "papers/23/03/2303.15725.json",
    "total_tokens": 1098,
    "translated_title": "求解正则化的exp、cosh和sinh回归问题",
    "translated_abstract": "在现代机器学习中，注意力计算是训练大型语言模型（如Transformer、GPT-4和ChatGPT）的基本任务。该文研究了受softmax/exp单元启发的指数回归问题。标准指数回归是非凸的。我们研究了指数回归问题的正则化版本，这是一个凸问题。我们使用近似牛顿方法以输入稀疏时间解决问题。形式上，给定矩阵$A\\in \\mathbb{R}^{n\\times d}$，$b\\in \\mathbb{R}^n$，$w\\in\\mathbb{R}^n$和任何函数$\\exp,\\cosh$和$\\sinh$，记作$f$。目标是找到最优$x$，使得$0.5 \\| f(Ax) - b \\|_2^2 + 0.5 \\| \\mathrm{diag}(w) A x \\|_2^2$最小化。研究并解决了正则化指数回归问题，其中函数为$\\exp, \\cosh$和$\\sinh$，并使用近似牛顿法在输入稀疏时间内求解，灵感来自大型语言模型中的注意力计算。",
    "tldr": "该文研究和解决了正则化指数回归问题，根据大型语言模型注意力计算的灵感，使用近似牛顿方法在输入稀疏时间内求解。",
    "en_tdlr": "This paper studies and solves the regularized exponential regression problem with functions $\\exp, \\cosh$ and $\\sinh$, inspired by attention computation in large language models, using approximate Newton method in input sparsity time."
}