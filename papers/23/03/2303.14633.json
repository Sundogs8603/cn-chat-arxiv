{
    "title": "An Evaluation of Memory Optimization Methods for Training Neural Networks. (arXiv:2303.14633v2 [cs.LG] UPDATED)",
    "abstract": "As models continue to grow in size, the development of memory optimization methods (MOMs) has emerged as a solution to address the memory bottleneck encountered when training large models. To comprehensively examine the practical value of various MOMs, we have conducted a thorough analysis of existing literature from a systems perspective. Our analysis has revealed a notable challenge within the research community: the absence of standardized metrics for effectively evaluating the efficacy of MOMs. The scarcity of informative evaluation metrics hinders the ability of researchers and practitioners to compare and benchmark different approaches reliably. Consequently, drawing definitive conclusions and making informed decisions regarding the selection and application of MOMs becomes a challenging endeavor. To address the challenge, this paper summarizes the scenarios in which MOMs prove advantageous for model training. We propose the use of distinct evaluation metrics under different scen",
    "link": "http://arxiv.org/abs/2303.14633",
    "context": "Title: An Evaluation of Memory Optimization Methods for Training Neural Networks. (arXiv:2303.14633v2 [cs.LG] UPDATED)\nAbstract: As models continue to grow in size, the development of memory optimization methods (MOMs) has emerged as a solution to address the memory bottleneck encountered when training large models. To comprehensively examine the practical value of various MOMs, we have conducted a thorough analysis of existing literature from a systems perspective. Our analysis has revealed a notable challenge within the research community: the absence of standardized metrics for effectively evaluating the efficacy of MOMs. The scarcity of informative evaluation metrics hinders the ability of researchers and practitioners to compare and benchmark different approaches reliably. Consequently, drawing definitive conclusions and making informed decisions regarding the selection and application of MOMs becomes a challenging endeavor. To address the challenge, this paper summarizes the scenarios in which MOMs prove advantageous for model training. We propose the use of distinct evaluation metrics under different scen",
    "path": "papers/23/03/2303.14633.json",
    "total_tokens": 1047,
    "translated_title": "训练神经网络的内存优化方法评估",
    "translated_abstract": "随着模型规模的不断增长，内存优化方法（MOMs）的开发已成为解决训练大型模型时遇到的内存瓶颈的解决方案。为全面检查各种MOM的实际价值，我们从系统角度对现有文献进行了彻底分析。我们的分析揭示了研究界一个显著的挑战:缺少标准度量来有效评估MOM的功效。信息有限的评估指标妨碍了研究人员和从业者可靠地比较和基准不同方法。因此，得出明确的结论并做出关于选择和应用MOMs的知情决策变得困难。为解决这个挑战，本文总结了MOM在模型训练中证明优势的场景。我们提议在不同场景下使用不同的评估指标，并对各种MOM进行全面比较，以提供有关其相对优势和局限性的见解。我们的评估结果证明了MOM在减少模型训练记忆需求的同时保持训练性能方面的实际价值。通过我们的分析，我们希望为研究人员和从业人员提供选择和应用MOMs的指导。",
    "tldr": "本文从系统角度对现有内存优化方法（MOMs）进行全面分析和评估，提出了在不同场景下选择不同的评估指标并对各种MOMs进行全面比较的方案，为研究人员和从业人员提供了选择和应用MOMs的指导。",
    "en_tdlr": "This paper provides a comprehensive analysis and evaluation of memory optimization methods (MOMs) for training neural networks from a systems perspective. It proposes different evaluation metrics for different scenarios and compares various MOMs to provide guidance for researchers and practitioners on selecting and applying MOMs."
}