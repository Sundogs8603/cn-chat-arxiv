{
    "title": "ALOFT: A Lightweight MLP-like Architecture with Dynamic Low-frequency Transform for Domain Generalization. (arXiv:2303.11674v1 [cs.CV])",
    "abstract": "Domain generalization (DG) aims to learn a model that generalizes well to unseen target domains utilizing multiple source domains without re-training. Most existing DG works are based on convolutional neural networks (CNNs). However, the local operation of the convolution kernel makes the model focus too much on local representations (e.g., texture), which inherently causes the model more prone to overfit to the source domains and hampers its generalization ability. Recently, several MLP-based methods have achieved promising results in supervised learning tasks by learning global interactions among different patches of the image. Inspired by this, in this paper, we first analyze the difference between CNN and MLP methods in DG and find that MLP methods exhibit a better generalization ability because they can better capture the global representations (e.g., structure) than CNN methods. Then, based on a recent lightweight MLP method, we obtain a strong baseline that outperforms most stat",
    "link": "http://arxiv.org/abs/2303.11674",
    "context": "Title: ALOFT: A Lightweight MLP-like Architecture with Dynamic Low-frequency Transform for Domain Generalization. (arXiv:2303.11674v1 [cs.CV])\nAbstract: Domain generalization (DG) aims to learn a model that generalizes well to unseen target domains utilizing multiple source domains without re-training. Most existing DG works are based on convolutional neural networks (CNNs). However, the local operation of the convolution kernel makes the model focus too much on local representations (e.g., texture), which inherently causes the model more prone to overfit to the source domains and hampers its generalization ability. Recently, several MLP-based methods have achieved promising results in supervised learning tasks by learning global interactions among different patches of the image. Inspired by this, in this paper, we first analyze the difference between CNN and MLP methods in DG and find that MLP methods exhibit a better generalization ability because they can better capture the global representations (e.g., structure) than CNN methods. Then, based on a recent lightweight MLP method, we obtain a strong baseline that outperforms most stat",
    "path": "papers/23/03/2303.11674.json",
    "total_tokens": 912,
    "translated_title": "ALOFT：一种轻量化的类MLP架构，配合动态低频变换用于域泛化",
    "translated_abstract": "域泛化旨在学习一个模型，它在不重新训练的情况下利用多个源域来很好地推广到看不见的目标域。大多数现有的域泛化工作都基于卷积神经网络（CNN）。然而，卷积核的局部操作使得模型过于关注局部表示（例如纹理），这从本质上使得模型更容易过拟合源域并阻碍其泛化能力。最近，几种基于MLP的方法通过学习图像不同块之间的全局交互，在监督学习任务中取得了有希望的结果。本文在此受到启发，首先分析了CNN和MLP方法在DG中的差异，并发现MLP方法表现出更好的泛化能力，因为它们可以比CNN方法更好地捕捉全局表示（例如结构）。然后，基于最近的一种轻量级MLP方法，我们获得了一个强大的基准线，其性能优于大多数统计学方法。",
    "tldr": "本文介绍了一种轻量级的类MLP架构ALOFT，它可使用动态低频变换用于域泛化。与CNN相比，该架构更好地捕捉全局表示，因此具有更好的泛化能力。",
    "en_tdlr": "This paper introduces a lightweight MLP-like architecture called ALOFT, which utilizes dynamic low-frequency transform for domain generalization. Compared with CNN, this architecture better captures global representations and thus has better generalization ability."
}