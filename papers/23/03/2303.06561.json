{
    "title": "Phase Diagram of Initial Condensation for Two-layer Neural Networks. (arXiv:2303.06561v2 [cs.LG] UPDATED)",
    "abstract": "The phenomenon of distinct behaviors exhibited by neural networks under varying scales of initialization remains an enigma in deep learning research. In this paper, based on the earlier work by Luo et al.~\\cite{luo2021phase}, we present a phase diagram of initial condensation for two-layer neural networks. Condensation is a phenomenon wherein the weight vectors of neural networks concentrate on isolated orientations during the training process, and it is a feature in non-linear learning process that enables neural networks to possess better generalization abilities. Our phase diagram serves to provide a comprehensive understanding of the dynamical regimes of neural networks and their dependence on the choice of hyperparameters related to initialization. Furthermore, we demonstrate in detail the underlying mechanisms by which small initialization leads to condensation at the initial training stage.",
    "link": "http://arxiv.org/abs/2303.06561",
    "context": "Title: Phase Diagram of Initial Condensation for Two-layer Neural Networks. (arXiv:2303.06561v2 [cs.LG] UPDATED)\nAbstract: The phenomenon of distinct behaviors exhibited by neural networks under varying scales of initialization remains an enigma in deep learning research. In this paper, based on the earlier work by Luo et al.~\\cite{luo2021phase}, we present a phase diagram of initial condensation for two-layer neural networks. Condensation is a phenomenon wherein the weight vectors of neural networks concentrate on isolated orientations during the training process, and it is a feature in non-linear learning process that enables neural networks to possess better generalization abilities. Our phase diagram serves to provide a comprehensive understanding of the dynamical regimes of neural networks and their dependence on the choice of hyperparameters related to initialization. Furthermore, we demonstrate in detail the underlying mechanisms by which small initialization leads to condensation at the initial training stage.",
    "path": "papers/23/03/2303.06561.json",
    "total_tokens": 814,
    "translated_title": "两层神经网络初始凝聚的相图",
    "translated_abstract": "神经网络在不同初始化比例下呈现出不同行为的现象一直是深度学习领域中的难题。本文基于Luo等人早期的工作，提出了一个两层神经网络初始凝聚的相图。凝聚是神经网络在训练过程中权重向量集中于独立方向的现象，在非线性学习过程中是一种特性，使神经网络拥有更好的泛化能力。我们的相图旨在提供对神经网络动力学区域及其与初始化相关的超参数选择的综合理解。此外，我们详细展示了小初始化导致在初始训练阶段出现凝聚的基本机制。",
    "tldr": "本文提出了一个两层神经网络初始凝聚的相图，旨在提供对神经网络动力学区域及其与初始化相关的超参数选择的综合理解。同时，我们详细解释了小初始化导致在初始训练阶段出现凝聚的基本机制。",
    "en_tdlr": "This paper presents a phase diagram of initial condensation for two-layer neural networks, aiming to provide a comprehensive understanding of the dynamical regimes of neural networks and their dependence on the choice of hyperparameters related to initialization. Moreover, the underlying mechanisms by which small initialization leads to condensation at the initial training stage are demonstrated in detail."
}