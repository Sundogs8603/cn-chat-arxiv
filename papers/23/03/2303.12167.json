{
    "title": "Training and Deploying Spiking NN Applications to the Mixed-Signal Neuromorphic Chip Dynap-SE2 with Rockpool. (arXiv:2303.12167v1 [cs.ET])",
    "abstract": "Mixed-signal neuromorphic processors provide extremely low-power operation for edge inference workloads, taking advantage of sparse asynchronous computation within Spiking Neural Networks (SNNs). However, deploying robust applications to these devices is complicated by limited controllability over analog hardware parameters, unintended parameter and dynamics variations of analog circuits due to fabrication non-idealities. Here we demonstrate a novel methodology for offline training and deployment of spiking neural networks (SNNs) to the mixed-signal neuromorphic processor Dynap-SE2. The methodology utilizes an unsupervised weight quantization method to optimize the network's parameters, coupled with adversarial parameter noise injection during training. The optimized network is shown to be robust to the effects of quantization and device mismatch, making the method a promising candidate for real-world applications with hardware constraints. This work extends Rockpool, an open-source de",
    "link": "http://arxiv.org/abs/2303.12167",
    "context": "Title: Training and Deploying Spiking NN Applications to the Mixed-Signal Neuromorphic Chip Dynap-SE2 with Rockpool. (arXiv:2303.12167v1 [cs.ET])\nAbstract: Mixed-signal neuromorphic processors provide extremely low-power operation for edge inference workloads, taking advantage of sparse asynchronous computation within Spiking Neural Networks (SNNs). However, deploying robust applications to these devices is complicated by limited controllability over analog hardware parameters, unintended parameter and dynamics variations of analog circuits due to fabrication non-idealities. Here we demonstrate a novel methodology for offline training and deployment of spiking neural networks (SNNs) to the mixed-signal neuromorphic processor Dynap-SE2. The methodology utilizes an unsupervised weight quantization method to optimize the network's parameters, coupled with adversarial parameter noise injection during training. The optimized network is shown to be robust to the effects of quantization and device mismatch, making the method a promising candidate for real-world applications with hardware constraints. This work extends Rockpool, an open-source de",
    "path": "papers/23/03/2303.12167.json",
    "total_tokens": 978,
    "translated_title": "使用Rockpool将脉冲神经网络应用程序训练部署到混合信号神经形态芯片Dynap-SE2上",
    "translated_abstract": "混合信号神经形态处理器利用脉冲神经网络（SNN）内的稀疏异步计算提供极低功耗的边缘推理负载。然而，由于模拟硬件参数的受限可控性以及由于制造非理想性所导致的模拟电路的无意参数和动态变化，将稳健的应用程序部署到这些设备是复杂的。本文展示了一种用于将SNN应用程序离线训练和部署到混合信号神经形态处理器Dynap-SE2的新型方法。该方法利用一种无监督的重量量化方法来优化网络的参数，并结合在训练过程中注入对抗性参数噪声。优化的网络表现出很强的鲁棒性，可以抵御量化和设备不匹配的影响，使该方法成为具有硬件约束的真实世界应用程序的有前景的候选方法。这项工作扩展了开源设计工具Rockpool。",
    "tldr": "本文介绍了一种通过优化网络参数和注入对抗性参数噪声，将SNN应用程序离线训练和部署到Dynap-SE2混合信号神经形态处理器的新方法。优化后的网络表现出很强的鲁棒性，对于硬件约束的真实世界应用程序有很大的潜力。",
    "en_tdlr": "This paper introduces a novel method to offline train and deploy SNN applications to the mixed-signal neuromorphic processor Dynap-SE2 by optimizing network parameters and injecting adversarial parameter noise. The optimized network shows strong robustness and has great potential for real-world applications with hardware constraints."
}