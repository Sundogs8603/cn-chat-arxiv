{
    "title": "OFA$^2$: A Multi-Objective Perspective for the Once-for-All Neural Architecture Search. (arXiv:2303.13683v1 [cs.NE])",
    "abstract": "Once-for-All (OFA) is a Neural Architecture Search (NAS) framework designed to address the problem of searching efficient architectures for devices with different resources constraints by decoupling the training and the searching stages. The computationally expensive process of training the OFA neural network is done only once, and then it is possible to perform multiple searches for subnetworks extracted from this trained network according to each deployment scenario. In this work we aim to give one step further in the search for efficiency by explicitly conceiving the search stage as a multi-objective optimization problem. A Pareto frontier is then populated with efficient, and already trained, neural architectures exhibiting distinct trade-offs among the conflicting objectives. This could be achieved by using any multi-objective evolutionary algorithm during the search stage, such as NSGA-II and SMS-EMOA. In other words, the neural network is trained once, the searching for subnetwo",
    "link": "http://arxiv.org/abs/2303.13683",
    "context": "Title: OFA$^2$: A Multi-Objective Perspective for the Once-for-All Neural Architecture Search. (arXiv:2303.13683v1 [cs.NE])\nAbstract: Once-for-All (OFA) is a Neural Architecture Search (NAS) framework designed to address the problem of searching efficient architectures for devices with different resources constraints by decoupling the training and the searching stages. The computationally expensive process of training the OFA neural network is done only once, and then it is possible to perform multiple searches for subnetworks extracted from this trained network according to each deployment scenario. In this work we aim to give one step further in the search for efficiency by explicitly conceiving the search stage as a multi-objective optimization problem. A Pareto frontier is then populated with efficient, and already trained, neural architectures exhibiting distinct trade-offs among the conflicting objectives. This could be achieved by using any multi-objective evolutionary algorithm during the search stage, such as NSGA-II and SMS-EMOA. In other words, the neural network is trained once, the searching for subnetwo",
    "path": "papers/23/03/2303.13683.json",
    "total_tokens": 915,
    "translated_title": "OFA$^2$: 一种基于多目标的Once-for-All神经架构搜索",
    "translated_abstract": "Once-for-All（OFA）是一个神经架构搜索（NAS）框架，旨在通过分离训练和搜索阶段来解决为不同资源约束的设备搜索高效架构的问题。 Ofa神经网络的训练过程只需要进行一次，然后可以根据每个部署方案从此训练好的网络中提取多个子网络进行多次搜索。本文旨在通过将搜索阶段明确构想为多目标优化问题，进一步寻求效率。 然后使用任何多目标进化算法（例如NSGA-II和SMS-EMOA）在搜索阶段填充Pareto前沿，其中包含具有不同权衡的高效预训练神经结构。换句话说，神经网络只需训练一次，然后以多目标优化的形式执行子网搜索，并获得一组高效、预训练且多样化的子网络。",
    "tldr": "OFA$^2$是一个基于多目标优化的神经架构搜索模型，它通过将搜索阶段构想为一个多目标优化问题，并使用已经训练好的神经网络，从而在多个权衡目标之间找到高效和多样化的子网络。",
    "en_tdlr": "OFA$^2$ is a multi-objective neural architecture search model that explicitly formulates the search as a multi-objective optimization problem, and finds efficient and diverse subnetworks among multiple conflicting objectives by using pre-trained neural network."
}