{
    "title": "Preserving Linear Separability in Continual Learning by Backward Feature Projection. (arXiv:2303.14595v2 [cs.LG] UPDATED)",
    "abstract": "Catastrophic forgetting has been a major challenge in continual learning, where the model needs to learn new tasks with limited or no access to data from previously seen tasks. To tackle this challenge, methods based on knowledge distillation in feature space have been proposed and shown to reduce forgetting. However, most feature distillation methods directly constrain the new features to match the old ones, overlooking the need for plasticity. To achieve a better stability-plasticity trade-off, we propose Backward Feature Projection (BFP), a method for continual learning that allows the new features to change up to a learnable linear transformation of the old features. BFP preserves the linear separability of the old classes while allowing the emergence of new feature directions to accommodate new classes. BFP can be integrated with existing experience replay methods and boost performance by a significant margin. We also demonstrate that BFP helps learn a better representation space,",
    "link": "http://arxiv.org/abs/2303.14595",
    "context": "Title: Preserving Linear Separability in Continual Learning by Backward Feature Projection. (arXiv:2303.14595v2 [cs.LG] UPDATED)\nAbstract: Catastrophic forgetting has been a major challenge in continual learning, where the model needs to learn new tasks with limited or no access to data from previously seen tasks. To tackle this challenge, methods based on knowledge distillation in feature space have been proposed and shown to reduce forgetting. However, most feature distillation methods directly constrain the new features to match the old ones, overlooking the need for plasticity. To achieve a better stability-plasticity trade-off, we propose Backward Feature Projection (BFP), a method for continual learning that allows the new features to change up to a learnable linear transformation of the old features. BFP preserves the linear separability of the old classes while allowing the emergence of new feature directions to accommodate new classes. BFP can be integrated with existing experience replay methods and boost performance by a significant margin. We also demonstrate that BFP helps learn a better representation space,",
    "path": "papers/23/03/2303.14595.json",
    "total_tokens": 938,
    "translated_title": "通过反向特征投影在不断学习中维护线性可分性",
    "translated_abstract": "在不断学习中，灾难性遗忘一直是一个重大挑战，因为模型需要在有限或没有以前查看任务的数据情况下学习新任务。为了解决这个挑战，基于特征空间知识蒸馏的方法已被提出并证明可以减少遗忘。然而，大多数特征蒸馏方法直接约束新特征以匹配旧特征，忽视了可塑性的需求。为了实现更好的稳定性-可塑性平衡，我们提出了Backward Feature Projection（BFP），这是一种连续学习方法，允许新特征在旧特征的可学习线性变换中发生变化。BFP保留旧类别的线性可分性，同时允许新的特征方向出现以适应新的类别。BFP可以与现有的经验重播方法集成，并显著提高性能。我们还证明，BFP有助于学习更好的表示空间。",
    "tldr": "提出了一种名为BFP的连续学习方法，它通过将新特征变换为旧特征的线性变换来维护线性可分性，从而允许新特征方向的出现以适应新任务，同时保留旧任务的信息。",
    "en_tdlr": "A method called Backward Feature Projection (BFP) is proposed for continual learning, which allows new features to change through a learnable linear transformation of old features, preserving the linear separability of old classes and accommodating the emergence of new feature directions for new tasks. BFP can be integrated with existing experience replay methods and significantly improves performance, while also helping to learn a better representation space."
}