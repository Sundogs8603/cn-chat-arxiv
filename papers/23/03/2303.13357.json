{
    "title": "POTTER: Pooling Attention Transformer for Efficient Human Mesh Recovery. (arXiv:2303.13357v1 [cs.CV])",
    "abstract": "Transformer architectures have achieved SOTA performance on the human mesh recovery (HMR) from monocular images. However, the performance gain has come at the cost of substantial memory and computational overhead. A lightweight and efficient model to reconstruct accurate human mesh is needed for real-world applications. In this paper, we propose a pure transformer architecture named POoling aTtention TransformER (POTTER) for the HMR task from single images. Observing that the conventional attention module is memory and computationally expensive, we propose an efficient pooling attention module, which significantly reduces the memory and computational cost without sacrificing performance. Furthermore, we design a new transformer architecture by integrating a High-Resolution (HR) stream for the HMR task. The high-resolution local and global features from the HR stream can be utilized for recovering more accurate human mesh. Our POTTER outperforms the SOTA method METRO by only requiring 7",
    "link": "http://arxiv.org/abs/2303.13357",
    "context": "Title: POTTER: Pooling Attention Transformer for Efficient Human Mesh Recovery. (arXiv:2303.13357v1 [cs.CV])\nAbstract: Transformer architectures have achieved SOTA performance on the human mesh recovery (HMR) from monocular images. However, the performance gain has come at the cost of substantial memory and computational overhead. A lightweight and efficient model to reconstruct accurate human mesh is needed for real-world applications. In this paper, we propose a pure transformer architecture named POoling aTtention TransformER (POTTER) for the HMR task from single images. Observing that the conventional attention module is memory and computationally expensive, we propose an efficient pooling attention module, which significantly reduces the memory and computational cost without sacrificing performance. Furthermore, we design a new transformer architecture by integrating a High-Resolution (HR) stream for the HMR task. The high-resolution local and global features from the HR stream can be utilized for recovering more accurate human mesh. Our POTTER outperforms the SOTA method METRO by only requiring 7",
    "path": "papers/23/03/2303.13357.json",
    "total_tokens": 927,
    "translated_title": "POTTER: 基于池化注意力变换器的高效人体网格恢复",
    "translated_abstract": "在单目图像中，变换器架构已经在人体网格恢复（HMR）上取得SOTA表现。然而，这种性能的提升是以巨大的内存和计算开销为代价的。为了实现实际应用中精确的人体网格重建，需要一种轻量级和高效的模型。在本文中，我们提出了一种纯变换器架构，名为POTTER（池化注意力变换器），用于单图像HMR任务。我们提出了一种高效的池化注意力模块，可以显著降低内存和计算成本，而不会牺牲性能。此外，我们设计了一种新的变换器架构，通过集成高分辨率（HR）数据流进行HMR任务。来自HR流的高分辨率局部和全局特征可以用于恢复更精确的人体网格。我们的POTTER仅需要7个英伟达显卡，超越了SOTA方法METRO，同时具有更高的精度和更好的性能表现。",
    "tldr": "POTTER是一种针对人体网格恢复任务的纯变换器架构，其有效的池化注意力模块可以提高性能并降低计算成本。此外，集成HR数据流可以用于恢复更精确的人体网格。",
    "en_tdlr": "POTTER is a pure transformer architecture for efficient human mesh recovery. By proposing an efficient pooling attention module, POTTER significantly reduces memory and computational cost without sacrificing performance. Furthermore, integrating an HR stream can recover more accurate human mesh."
}