{
    "title": "Evolutionary Augmentation Policy Optimization for Self-supervised Learning. (arXiv:2303.01584v2 [cs.CV] UPDATED)",
    "abstract": "Self-supervised Learning (SSL) is a machine learning algorithm for pretraining Deep Neural Networks (DNNs) without requiring manually labeled data. The central idea of this learning technique is based on an auxiliary stage aka pretext task in which labeled data are created automatically through data augmentation and exploited for pretraining the DNN. However, the effect of each pretext task is not well studied or compared in the literature. In this paper, we study the contribution of augmentation operators on the performance of self supervised learning algorithms in a constrained settings. We propose an evolutionary search method for optimization of data augmentation pipeline in pretext tasks and measure the impact of augmentation operators in several SOTA SSL algorithms. By encoding different combination of augmentation operators in chromosomes we seek the optimal augmentation policies through an evolutionary optimization mechanism. We further introduce methods for analyzing and expla",
    "link": "http://arxiv.org/abs/2303.01584",
    "context": "Title: Evolutionary Augmentation Policy Optimization for Self-supervised Learning. (arXiv:2303.01584v2 [cs.CV] UPDATED)\nAbstract: Self-supervised Learning (SSL) is a machine learning algorithm for pretraining Deep Neural Networks (DNNs) without requiring manually labeled data. The central idea of this learning technique is based on an auxiliary stage aka pretext task in which labeled data are created automatically through data augmentation and exploited for pretraining the DNN. However, the effect of each pretext task is not well studied or compared in the literature. In this paper, we study the contribution of augmentation operators on the performance of self supervised learning algorithms in a constrained settings. We propose an evolutionary search method for optimization of data augmentation pipeline in pretext tasks and measure the impact of augmentation operators in several SOTA SSL algorithms. By encoding different combination of augmentation operators in chromosomes we seek the optimal augmentation policies through an evolutionary optimization mechanism. We further introduce methods for analyzing and expla",
    "path": "papers/23/03/2303.01584.json",
    "total_tokens": 880,
    "translated_title": "进化增强策略优化用于自监督学习",
    "translated_abstract": "自监督学习是一种无需手动标记数据的深度神经网络预训练的机器学习算法。该学习技术的核心思想是通过辅助阶段（也称为预训练任务），通过数据增强自动生成标记数据，并用于预训练深度神经网络。然而，文献中对于每个预训练任务的影响还未得到充分研究和比较。本文研究了数据增强操作对约束条件下自监督学习算法性能的贡献。我们提出了一种进化搜索方法，用于优化预训练任务中的数据增强流程，并测量了几种先进的自监督学习算法中数据增强操作的影响。通过在染色体中编码不同组合的增强操作，我们通过进化优化机制寻求最优的增强策略。此外，我们还引入了用于分析和解释数据增强策略在模型性能上的影响的方法。",
    "tldr": "本文研究了数据增强操作对自监督学习算法性能的影响，提出了一种进化搜索方法来优化数据增强策略，并通过实验比较了几种现有自监督学习算法的性能。",
    "en_tdlr": "This paper studies the impact of augmentation operators on the performance of self-supervised learning algorithms and proposes an evolutionary search method to optimize data augmentation policies. It also compares the performance of several existing self-supervised learning algorithms through experiments."
}