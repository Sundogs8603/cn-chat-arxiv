{
    "title": "Latent-Conditioned Policy Gradient for Multi-Objective Deep Reinforcement Learning. (arXiv:2303.08909v1 [cs.LG])",
    "abstract": "Sequential decision making in the real world often requires finding a good balance of conflicting objectives. In general, there exist a plethora of Pareto-optimal policies that embody different patterns of compromises between objectives, and it is technically challenging to obtain them exhaustively using deep neural networks. In this work, we propose a novel multi-objective reinforcement learning (MORL) algorithm that trains a single neural network via policy gradient to approximately obtain the entire Pareto set in a single run of training, without relying on linear scalarization of objectives. The proposed method works in both continuous and discrete action spaces with no design change of the policy network. Numerical experiments in benchmark environments demonstrate the practicality and efficacy of our approach in comparison to standard MORL baselines.",
    "link": "http://arxiv.org/abs/2303.08909",
    "context": "Title: Latent-Conditioned Policy Gradient for Multi-Objective Deep Reinforcement Learning. (arXiv:2303.08909v1 [cs.LG])\nAbstract: Sequential decision making in the real world often requires finding a good balance of conflicting objectives. In general, there exist a plethora of Pareto-optimal policies that embody different patterns of compromises between objectives, and it is technically challenging to obtain them exhaustively using deep neural networks. In this work, we propose a novel multi-objective reinforcement learning (MORL) algorithm that trains a single neural network via policy gradient to approximately obtain the entire Pareto set in a single run of training, without relying on linear scalarization of objectives. The proposed method works in both continuous and discrete action spaces with no design change of the policy network. Numerical experiments in benchmark environments demonstrate the practicality and efficacy of our approach in comparison to standard MORL baselines.",
    "path": "papers/23/03/2303.08909.json",
    "total_tokens": 849,
    "translated_title": "多目标深度强化学习中的潜在条件策略梯度",
    "translated_abstract": "在现实世界中进行序列决策通常需要找到平衡相互矛盾的目标的良好平衡点。一般来说，存在大量的帕累托最优策略，它们体现了不同的目标权衡模式，并且使用深度神经网络全面获得它们具有技术挑战性。在本文中，我们提出了一种新的多目标强化学习（MORL）算法，通过策略梯度训练单个神经网络，以在单次训练运行中近似获取整个帕累托集，而不依赖于目标的线性标量化。该方法适用于连续和离散的行动空间，并且不需要修改策略网络的设计。在基准环境中的数字实验证明了我们的方法与标准MORL基线相比的实用性和有效性。",
    "tldr": "该论文提出了一种新的多目标深度强化学习算法，通过策略梯度训练单个神经网络，以在单次训练运行中近似获取整个帕累托集，而不依赖于目标的线性标量化。",
    "en_tdlr": "This paper proposes a new multi-objective deep reinforcement learning algorithm that trains a single neural network via policy gradient to approximately obtain the entire Pareto set in a single run of training, without relying on linear scalarization of objectives."
}