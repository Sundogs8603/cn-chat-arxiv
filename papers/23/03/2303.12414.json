{
    "title": "Delay-Aware Hierarchical Federated Learning. (arXiv:2303.12414v1 [cs.LG])",
    "abstract": "Federated learning has gained popularity as a means of training models distributed across the wireless edge. The paper introduces delay-aware federated learning (DFL) to improve the efficiency of distributed machine learning (ML) model training by addressing communication delays between edge and cloud. DFL employs multiple stochastic gradient descent iterations on device datasets during each global aggregation interval and intermittently aggregates model parameters through edge servers in local subnetworks. The cloud server synchronizes the local models with the global deployed model computed via a local-global combiner at global synchronization. The convergence behavior of DFL is theoretically investigated under a generalized data heterogeneity metric. A set of conditions is obtained to achieve the sub-linear convergence rate of O(1/k). Based on these findings, an adaptive control algorithm is developed for DFL, implementing policies to mitigate energy consumption and edge-to-cloud co",
    "link": "http://arxiv.org/abs/2303.12414",
    "context": "Title: Delay-Aware Hierarchical Federated Learning. (arXiv:2303.12414v1 [cs.LG])\nAbstract: Federated learning has gained popularity as a means of training models distributed across the wireless edge. The paper introduces delay-aware federated learning (DFL) to improve the efficiency of distributed machine learning (ML) model training by addressing communication delays between edge and cloud. DFL employs multiple stochastic gradient descent iterations on device datasets during each global aggregation interval and intermittently aggregates model parameters through edge servers in local subnetworks. The cloud server synchronizes the local models with the global deployed model computed via a local-global combiner at global synchronization. The convergence behavior of DFL is theoretically investigated under a generalized data heterogeneity metric. A set of conditions is obtained to achieve the sub-linear convergence rate of O(1/k). Based on these findings, an adaptive control algorithm is developed for DFL, implementing policies to mitigate energy consumption and edge-to-cloud co",
    "path": "papers/23/03/2303.12414.json",
    "total_tokens": 886,
    "translated_title": "延迟感知的分层联邦学习",
    "translated_abstract": "联邦学习作为一种在分布式环境下训练模型的方法，已经越来越受到关注。本文引入了延迟感知的联邦学习(DFL)，通过解决边缘和云之间的通信延迟，提高了分布式机器学习模型训练的效率。DFL在每个全局聚合间隔期间对设备数据集执行多个随机梯度下降迭代，并通过边缘服务器在本地子网络中间断地聚合模型参数。云服务器通过局部-全局合并器将本地模型与全局部署模型同步。DFL的收敛行为在广义数据异质性度量下进行了理论研究。得出了一组条件，以实现O(1/k)的次线性收敛率。基于这些发现，开发了一个自适应控制算法来实现DFL，并实现了一些政策以减少能量消耗和边缘到云端的通信。",
    "tldr": "本论文提出了延迟感知的联邦学习(DFL)，通过解决边缘和云之间的通信延迟，提高了分布式机器学习模型训练的效率，并实现了一些政策以减少能量消耗和边缘到云端的通信。",
    "en_tdlr": "The paper proposes delay-aware federated learning (DFL) which enhances the efficiency of distributed machine learning by addressing communication delays between edge and cloud, and implements policies to reduce energy consumption and edge-to-cloud communication."
}