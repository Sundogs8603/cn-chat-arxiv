{
    "title": "Identification of Negative Transfers in Multitask Learning Using Surrogate Models. (arXiv:2303.14582v1 [cs.LG])",
    "abstract": "Multitask learning is widely used in practice to train a low-resource target task by augmenting it with multiple related source tasks. Yet, naively combining all the source tasks with a target task does not always improve the prediction performance for the target task due to negative transfers. Thus, a critical problem in multitask learning is identifying subsets of source tasks that would benefit the target task. This problem is computationally challenging since the number of subsets grows exponentially with the number of source tasks; efficient heuristics for subset selection does not always capture the relationship between task subsets and multitask learning performances. In this paper, we introduce an efficient procedure to address this problem via surrogate modeling. In surrogate modeling, we sample (random) subsets of source tasks and precompute their multitask learning performances; Then, we approximate the precomputed performances with a linear regression model that can also be",
    "link": "http://arxiv.org/abs/2303.14582",
    "context": "Title: Identification of Negative Transfers in Multitask Learning Using Surrogate Models. (arXiv:2303.14582v1 [cs.LG])\nAbstract: Multitask learning is widely used in practice to train a low-resource target task by augmenting it with multiple related source tasks. Yet, naively combining all the source tasks with a target task does not always improve the prediction performance for the target task due to negative transfers. Thus, a critical problem in multitask learning is identifying subsets of source tasks that would benefit the target task. This problem is computationally challenging since the number of subsets grows exponentially with the number of source tasks; efficient heuristics for subset selection does not always capture the relationship between task subsets and multitask learning performances. In this paper, we introduce an efficient procedure to address this problem via surrogate modeling. In surrogate modeling, we sample (random) subsets of source tasks and precompute their multitask learning performances; Then, we approximate the precomputed performances with a linear regression model that can also be",
    "path": "papers/23/03/2303.14582.json",
    "total_tokens": 879,
    "translated_title": "利用代理模型识别多任务学习中的负迁移",
    "translated_abstract": "多任务学习广泛应用于通过增加多个相关源任务来训练低资源目标任务。然而，将所有源任务与目标任务简单组合并不总是能提高目标任务的预测性能，因为会存在负迁移。因此，多任务学习的一个关键问题是识别哪些源任务的子集会对目标任务有益。这个问题在计算上很具有挑战性，因为子集的数量随着源任务的数量呈指数级增长。在本文中，我们介绍了一种通过代理建模来解决此问题的有效方法。在代理建模中，我们对源任务进行采样（随机），并预先计算它们的多任务学习表现；然后，我们用线性回归模型来逼近预先计算的表现，该模型也可用于预测未采样的子集的表现。我们在几个合成示例和一个现实世界的多语言情感分析任务上证明了我们方法的有效性。",
    "tldr": "本文提出了一种通过代理建模来解决多任务学习中负迁移问题的方法，能够识别哪些源任务的子集会对目标任务有帮助。",
    "en_tdlr": "This paper proposes an efficient way to identify subsets of source tasks that would benefit a low-resource target task by using surrogate modeling to address the problem of negative transfers in multitask learning."
}