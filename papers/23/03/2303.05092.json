{
    "title": "Task Aware Dreamer for Task Generalization in Reinforcement Learning. (arXiv:2303.05092v2 [cs.LG] UPDATED)",
    "abstract": "A long-standing goal of reinforcement learning is to acquire agents that can learn on training tasks and generalize well on unseen tasks that may share a similar dynamic but with different reward functions. A general challenge is to quantitatively measure the similarities between these different tasks, which is vital for analyzing the task distribution and further designing algorithms with stronger generalization. To address this, we present a novel metric named Task Distribution Relevance (TDR) via optimal Q functions of different tasks to capture the relevance of the task distribution quantitatively. In the case of tasks with a high TDR, i.e., the tasks differ significantly, we show that the Markovian policies cannot differentiate them, leading to poor performance. Based on this insight, we encode all historical information into policies for distinguishing different tasks and propose Task Aware Dreamer (TAD), which extends world models into our reward-informed world models to capture",
    "link": "http://arxiv.org/abs/2303.05092",
    "context": "Title: Task Aware Dreamer for Task Generalization in Reinforcement Learning. (arXiv:2303.05092v2 [cs.LG] UPDATED)\nAbstract: A long-standing goal of reinforcement learning is to acquire agents that can learn on training tasks and generalize well on unseen tasks that may share a similar dynamic but with different reward functions. A general challenge is to quantitatively measure the similarities between these different tasks, which is vital for analyzing the task distribution and further designing algorithms with stronger generalization. To address this, we present a novel metric named Task Distribution Relevance (TDR) via optimal Q functions of different tasks to capture the relevance of the task distribution quantitatively. In the case of tasks with a high TDR, i.e., the tasks differ significantly, we show that the Markovian policies cannot differentiate them, leading to poor performance. Based on this insight, we encode all historical information into policies for distinguishing different tasks and propose Task Aware Dreamer (TAD), which extends world models into our reward-informed world models to capture",
    "path": "papers/23/03/2303.05092.json",
    "total_tokens": 848,
    "translated_title": "Task Aware Dreamer用于强化学习中的任务泛化",
    "translated_abstract": "强化学习的一个长期目标是获得能够在训练任务上学习并且在不同奖励函数下可以很好地泛化到未见任务的代理。一个通用的挑战是定量地衡量这些不同任务之间的相似性，这对于分析任务分布并进一步设计具有更强泛化能力的算法至关重要。为了解决这个问题，我们提出了一种新的度量方法，名为任务分布相关性（TDR），通过不同任务的最优Q函数来量化任务分布的相关性。在具有高TDR的任务情况下，即任务之间显著不同，我们发现马尔可夫策略无法区分它们，导致性能较差。基于这一观察，我们将所有历史信息编码到策略中以区分不同任务，并提出了Task Aware Dreamer（TAD），它将世界模型扩展为我们的奖励感知世界模型以捕捉任务的相关性。",
    "tldr": "本文提出了一种名为Task Aware Dreamer（TAD）的方法用于强化学习中的任务泛化。通过量化任务分布的相关性，TAD能够将历史信息编码到策略中，以便区分不同任务，并在泛化到未见任务时具有较好的性能。"
}