{
    "title": "An Over-parameterized Exponential Regression. (arXiv:2303.16504v1 [cs.LG])",
    "abstract": "Over the past few years, there has been a significant amount of research focused on studying the ReLU activation function, with the aim of achieving neural network convergence through over-parametrization. However, recent developments in the field of Large Language Models (LLMs) have sparked interest in the use of exponential activation functions, specifically in the attention mechanism.  Mathematically, we define the neural function $F: \\mathbb{R}^{d \\times m} \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}$ using an exponential activation function. Given a set of data points with labels $\\{(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\} \\subset \\mathbb{R}^d \\times \\mathbb{R}$ where $n$ denotes the number of the data. Here $F(W(t),x)$ can be expressed as $F(W(t),x) := \\sum_{r=1}^m a_r \\exp(\\langle w_r, x \\rangle)$, where $m$ represents the number of neurons, and $w_r(t)$ are weights at time $t$. It's standard in literature that $a_r$ are the fixed weights and it's never changed during the trai",
    "link": "http://arxiv.org/abs/2303.16504",
    "context": "Title: An Over-parameterized Exponential Regression. (arXiv:2303.16504v1 [cs.LG])\nAbstract: Over the past few years, there has been a significant amount of research focused on studying the ReLU activation function, with the aim of achieving neural network convergence through over-parametrization. However, recent developments in the field of Large Language Models (LLMs) have sparked interest in the use of exponential activation functions, specifically in the attention mechanism.  Mathematically, we define the neural function $F: \\mathbb{R}^{d \\times m} \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}$ using an exponential activation function. Given a set of data points with labels $\\{(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\} \\subset \\mathbb{R}^d \\times \\mathbb{R}$ where $n$ denotes the number of the data. Here $F(W(t),x)$ can be expressed as $F(W(t),x) := \\sum_{r=1}^m a_r \\exp(\\langle w_r, x \\rangle)$, where $m$ represents the number of neurons, and $w_r(t)$ are weights at time $t$. It's standard in literature that $a_r$ are the fixed weights and it's never changed during the trai",
    "path": "papers/23/03/2303.16504.json",
    "total_tokens": 955,
    "translated_title": "一种过参数化指数回归模型",
    "translated_abstract": "近年来，对ReLU激活函数的研究引发了许多关注，旨在通过过参数化实现神经网络收敛。然而，最近在大型语言模型 (LLM) 领域的发展引起了人们对指数激活函数的兴趣，特别是在注意力机制中的应用。在数学上，我们使用指数激活函数定义神经函数 $F: \\mathbb{R}^{d \\times m} \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}$。给定一组带标签的数据点 $\\{(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\} \\subset \\mathbb{R}^d \\times \\mathbb{R}$，其中 $n$ 表示数据的数量。这里 $F(W(t),x)$ 可以用 $F(W(t),x) := \\sum_{r=1}^m a_r \\exp(\\langle w_r, x \\rangle)$ 表示，其中 $m$ 表示神经元的数量，$w_r(t)$ 是时间 $t$ 上的权重。固定权重 $a_r$ 在训练期间不会改变，这是文献中的标准。",
    "tldr": "该论文介绍了一种使用指数激活函数定义的神经函数来实现过参数化指数回归模型，并提出了一种新的注意力机制。",
    "en_tdlr": "This paper introduces an over-parameterized exponential regression model using an exponential activation function defined neural function, and proposes a new attention mechanism."
}