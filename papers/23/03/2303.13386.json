{
    "title": "Compositional Zero-Shot Domain Transfer with Text-to-Text Models. (arXiv:2303.13386v1 [cs.CL])",
    "abstract": "Label scarcity is a bottleneck for improving task performance in specialised domains. We propose a novel compositional transfer learning framework (DoT5 domain compositional zero-shot T5) for zero-shot domain transfer. Without access to in-domain labels, DoT5 jointly learns domain knowledge (from MLM of unlabelled in-domain free text) and task knowledge (from task training on more readily available general-domain data) in a multi-task manner. To improve the transferability of task training, we design a strategy named NLGU: we simultaneously train NLG for in-domain label-to-data generation which enables data augmentation for self-finetuning and NLU for label prediction. We evaluate DoT5 on the biomedical domain and the resource-lean subdomain of radiology, focusing on NLI, text summarisation and embedding learning. DoT5 demonstrates the effectiveness of compositional transfer learning through multi-task learning. In particular, DoT5 outperforms the current SOTA in zero-shot transfer b",
    "link": "http://arxiv.org/abs/2303.13386",
    "context": "Title: Compositional Zero-Shot Domain Transfer with Text-to-Text Models. (arXiv:2303.13386v1 [cs.CL])\nAbstract: Label scarcity is a bottleneck for improving task performance in specialised domains. We propose a novel compositional transfer learning framework (DoT5 domain compositional zero-shot T5) for zero-shot domain transfer. Without access to in-domain labels, DoT5 jointly learns domain knowledge (from MLM of unlabelled in-domain free text) and task knowledge (from task training on more readily available general-domain data) in a multi-task manner. To improve the transferability of task training, we design a strategy named NLGU: we simultaneously train NLG for in-domain label-to-data generation which enables data augmentation for self-finetuning and NLU for label prediction. We evaluate DoT5 on the biomedical domain and the resource-lean subdomain of radiology, focusing on NLI, text summarisation and embedding learning. DoT5 demonstrates the effectiveness of compositional transfer learning through multi-task learning. In particular, DoT5 outperforms the current SOTA in zero-shot transfer b",
    "path": "papers/23/03/2303.13386.json",
    "total_tokens": 1138,
    "translated_title": "基于文本模型的组合零样本领域转移",
    "translated_abstract": "在专业领域中，标签稀缺是提高任务性能的瓶颈。我们提出了一种新的组合转移学习框架（DoT5 领域组合零样本 T5），用于零样本领域转移。在没有访问领域标签的情况下，DoT5以多任务的方式共同学习领域知识（从未标记的领域自由文本的 MLM 中学习）和任务知识（从更容易获取的通用领域数据的任务训练中学习）。为了提高任务训练的可转移性，我们设计了一种名为 NLGU 的策略：我们同时为领域标签到数据生成训练 NLG，从而实现用于自我微调的数据增强和用于标签预测的 NLU 训练。我们在生物医学领域和放射学的资源贫乏子领域上评估了 DoT5，重点关注 NLI、文本摘要和嵌入学习。通过多任务学习，DoT5证明了组合转移学习的有效性，尤其是在零样本转移方面胜过了现有的 SOTA。",
    "tldr": "提出了一种组合转移学习框架，用于专业领域中的零样本领域转移，使用未标记的领域自由文本进行领域和任务知识的共同学习，通过 NLGU 策略实现领域数据增强和标签预测，经实验证明在生物医学领域和放射学子领域具有优异的性能，胜过了现有的 SOTA。",
    "en_tdlr": "A compositional transfer learning framework is proposed for zero-shot domain transfer in specialised domains, jointly learning domain knowledge and task knowledge without access to in-domain labels. The NLGU strategy is introduced to improve transferability through in-domain data augmentation and label prediction. DoT5 outperforms the current state-of-the-art in zero-shot transfer in biomedical and radiology subdomains, demonstrating the effectiveness of compositional transfer learning."
}