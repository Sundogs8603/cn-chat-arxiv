{
    "title": "Exploring the Limits of Model-Targeted Indiscriminate Data Poisoning Attacks. (arXiv:2303.03592v3 [cs.LG] UPDATED)",
    "abstract": "Indiscriminate data poisoning attacks aim to decrease a model's test accuracy by injecting a small amount of corrupted training data. Despite significant interest, existing attacks remain relatively ineffective against modern machine learning (ML) architectures. In this work, we introduce the notion of model poisoning reachability as a technical tool to explore the intrinsic limits of data poisoning attacks towards target parameters (i.e., model-targeted attacks). We derive an easily computable threshold to establish and quantify a surprising phase transition phenomenon among popular ML models: data poisoning attacks can achieve certain target parameters only when the poisoning ratio exceeds our threshold. Building on existing parameter corruption attacks and refining the Gradient Canceling attack, we perform extensive experiments to confirm our theoretical findings, test the predictability of our transition threshold, and significantly improve existing indiscriminate data poisoning ba",
    "link": "http://arxiv.org/abs/2303.03592",
    "context": "Title: Exploring the Limits of Model-Targeted Indiscriminate Data Poisoning Attacks. (arXiv:2303.03592v3 [cs.LG] UPDATED)\nAbstract: Indiscriminate data poisoning attacks aim to decrease a model's test accuracy by injecting a small amount of corrupted training data. Despite significant interest, existing attacks remain relatively ineffective against modern machine learning (ML) architectures. In this work, we introduce the notion of model poisoning reachability as a technical tool to explore the intrinsic limits of data poisoning attacks towards target parameters (i.e., model-targeted attacks). We derive an easily computable threshold to establish and quantify a surprising phase transition phenomenon among popular ML models: data poisoning attacks can achieve certain target parameters only when the poisoning ratio exceeds our threshold. Building on existing parameter corruption attacks and refining the Gradient Canceling attack, we perform extensive experiments to confirm our theoretical findings, test the predictability of our transition threshold, and significantly improve existing indiscriminate data poisoning ba",
    "path": "papers/23/03/2303.03592.json",
    "total_tokens": 952,
    "translated_title": "探究模型定向无差别数据污染攻击的极限",
    "translated_abstract": "无差别数据污染攻击旨在通过注入少量损坏的训练数据来降低模型的测试准确性。尽管受到了相当大的关注，但现有攻击对现代机器学习（ML）架构仍然相对无效。在这项工作中，我们引入了模型污染可达性的概念作为探索数据污染攻击向目标参数（即模型定向攻击）的固有限制的技术工具。我们推导出一个易于计算的阈值，以确定和量化受欢迎的ML模型之间的惊人相变现象：只有当污染比超过我们的阈值时，数据污染攻击才能实现某些目标参数。在现有参数损坏攻击的基础上，并改进了梯度取消攻击，我们进行了广泛的实验，以验证我们的理论发现，测试我们的转换阈值的可预测性，并显着改善现有的无差别数据污染攻击。",
    "tldr": "该研究探究了模型定向无差别数据污染攻击的极限。作者引入了模型污染可达性的概念，推导出了易于计算的阈值，并发现了一种惊人的相变现象：只有当污染比超过阈值时，攻击才能实现某些目标参数。",
    "en_tdlr": "This study explores the limits of model-targeted indiscriminate data poisoning attacks. The authors introduce the concept of model poisoning reachability as a tool to explore the intrinsic limits of data poisoning attacks and derive a computable threshold for popular ML models. They discover a surprising phase transition phenomenon where attacks can only achieve certain target parameters when the poisoning ratio exceeds the threshold."
}