{
    "title": "Improved Sample Complexity Bounds for Distributionally Robust Reinforcement Learning. (arXiv:2303.02783v2 [cs.LG] UPDATED)",
    "abstract": "We consider the problem of learning a control policy that is robust against the parameter mismatches between the training environment and testing environment. We formulate this as a distributionally robust reinforcement learning (DR-RL) problem where the objective is to learn the policy which maximizes the value function against the worst possible stochastic model of the environment in an uncertainty set. We focus on the tabular episodic learning setting where the algorithm has access to a generative model of the nominal (training) environment around which the uncertainty set is defined. We propose the Robust Phased Value Learning (RPVL) algorithm to solve this problem for the uncertainty sets specified by four different divergences: total variation, chi-square, Kullback-Leibler, and Wasserstein. We show that our algorithm achieves $\\tilde{\\mathcal{O}}(|\\mathcal{S}||\\mathcal{A}| H^{5})$ sample complexity, which is uniformly better than the existing results by a factor of $|\\mathcal{S}|",
    "link": "http://arxiv.org/abs/2303.02783",
    "context": "Title: Improved Sample Complexity Bounds for Distributionally Robust Reinforcement Learning. (arXiv:2303.02783v2 [cs.LG] UPDATED)\nAbstract: We consider the problem of learning a control policy that is robust against the parameter mismatches between the training environment and testing environment. We formulate this as a distributionally robust reinforcement learning (DR-RL) problem where the objective is to learn the policy which maximizes the value function against the worst possible stochastic model of the environment in an uncertainty set. We focus on the tabular episodic learning setting where the algorithm has access to a generative model of the nominal (training) environment around which the uncertainty set is defined. We propose the Robust Phased Value Learning (RPVL) algorithm to solve this problem for the uncertainty sets specified by four different divergences: total variation, chi-square, Kullback-Leibler, and Wasserstein. We show that our algorithm achieves $\\tilde{\\mathcal{O}}(|\\mathcal{S}||\\mathcal{A}| H^{5})$ sample complexity, which is uniformly better than the existing results by a factor of $|\\mathcal{S}|",
    "path": "papers/23/03/2303.02783.json",
    "total_tokens": 926,
    "translated_title": "分布式鲁棒强化学习的样本复杂性界限的改进",
    "translated_abstract": "本文考虑了在训练环境与测试环境之间参数不匹配的情况下学习控制策略的问题。我们将其制定为一个分布式鲁棒强化学习(DR-RL)问题，其中目标是学习在不确定集中针对环境最坏的随机模型下最大化价值函数的策略。我们专注于表格剧情学习环境，在不确定集被定义在名义（训练）环境的生成模型周围的情况下，算法可以访问该环境。我们提出了稳健分阶段价值学习(RPVL)算法来解决用四种不同发散度指定的不确定集的问题: 全变分、卡方、Kullback-Leibler和Wasserstein。我们证明了我们的算法达到了 $\\tilde{\\mathcal{O}}(|\\mathcal{S}||\\mathcal{A}| H^{5})$ 样本复杂性，这比现有结果平均好了一倍的 $|\\mathcal{S}|$",
    "tldr": "本文改进了分布式鲁棒强化学习的样本复杂度界限，提出了稳健分阶段价值学习（RPVL）算法来解决表格剧情学习环境下的不确定性问题。",
    "en_tdlr": "This paper improves the sample complexity bounds for Distributionally Robust Reinforcement Learning by proposing the Robust Phased Value Learning (RPVL) algorithm for uncertainty sets specified by four different divergences and achieving a lower sample complexity than existing results."
}