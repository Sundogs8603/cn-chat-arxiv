{
    "title": "CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning. (arXiv:2303.03323v3 [cs.CV] UPDATED)",
    "abstract": "Multimodal contrastive pretraining has been used to train multimodal representation models, such as CLIP, on large amounts of paired image-text data. However, previous studies have revealed that such models are vulnerable to backdoor attacks. Specifically, when trained on backdoored examples, CLIP learns spurious correlations between the embedded backdoor trigger and the target label, aligning their representations in the joint embedding space. Injecting even a small number of poisoned examples, such as 75 examples in 3 million pretraining data, can significantly manipulate the model's behavior, making it difficult to detect or unlearn such correlations. To address this issue, we propose CleanCLIP, a finetuning framework that weakens the learned spurious associations introduced by backdoor attacks by independently re-aligning the representations for individual modalities. We demonstrate that unsupervised finetuning using a combination of multimodal contrastive and unimodal self-supervi",
    "link": "http://arxiv.org/abs/2303.03323",
    "context": "Title: CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning. (arXiv:2303.03323v3 [cs.CV] UPDATED)\nAbstract: Multimodal contrastive pretraining has been used to train multimodal representation models, such as CLIP, on large amounts of paired image-text data. However, previous studies have revealed that such models are vulnerable to backdoor attacks. Specifically, when trained on backdoored examples, CLIP learns spurious correlations between the embedded backdoor trigger and the target label, aligning their representations in the joint embedding space. Injecting even a small number of poisoned examples, such as 75 examples in 3 million pretraining data, can significantly manipulate the model's behavior, making it difficult to detect or unlearn such correlations. To address this issue, we propose CleanCLIP, a finetuning framework that weakens the learned spurious associations introduced by backdoor attacks by independently re-aligning the representations for individual modalities. We demonstrate that unsupervised finetuning using a combination of multimodal contrastive and unimodal self-supervi",
    "path": "papers/23/03/2303.03323.json",
    "total_tokens": 891,
    "translated_title": "清洁CLIP: 缓解多模态对比学习中的数据污染攻击",
    "translated_abstract": "多模态对比预训练已被用于在大量配对的图文数据上训练多模态表示模型，如CLIP。然而，先前的研究揭示了这类模型容易受到后门攻击的影响。具体而言，当在含有后门的示例上进行训练时，CLIP学习到了嵌入式后门触发器与目标标签之间的虚假相关性，并将它们在联合嵌入空间中进行了对齐。即使注入了少量的毒化示例，例如在3000000个预训练数据中注入了75个示例，也能显著操纵模型的行为，使其难以检测或忘记这种相关性。为了解决这个问题，我们提出了CleanCLIP，一种通过独立重新对齐个别模态的表示来削弱后门攻击引入的学习到的虚假关联的微调框架。我们通过使用多模态对比和单模态自监督的组合进行无监督微调来证明这一点。",
    "tldr": "CleanCLIP是一个通过独立重新对齐个别模态的表示来削弱后门攻击引入的虚假关联的微调框架。",
    "en_tdlr": "CleanCLIP is a finetuning framework that weakens the learned spurious associations introduced by backdoor attacks by independently re-aligning the representations for individual modalities."
}