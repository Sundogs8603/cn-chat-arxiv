{
    "title": "Dense Network Expansion for Class Incremental Learning. (arXiv:2303.12696v1 [cs.CV])",
    "abstract": "The problem of class incremental learning (CIL) is considered. State-of-the-art approaches use a dynamic architecture based on network expansion (NE), in which a task expert is added per task. While effective from a computational standpoint, these methods lead to models that grow quickly with the number of tasks. A new NE method, dense network expansion (DNE), is proposed to achieve a better trade-off between accuracy and model complexity. This is accomplished by the introduction of dense connections between the intermediate layers of the task expert networks, that enable the transfer of knowledge from old to new tasks via feature sharing and reusing. This sharing is implemented with a cross-task attention mechanism, based on a new task attention block (TAB), that fuses information across tasks. Unlike traditional attention mechanisms, TAB operates at the level of the feature mixing and is decoupled with spatial attentions. This is shown more effective than a joint spatial-and-task att",
    "link": "http://arxiv.org/abs/2303.12696",
    "context": "Title: Dense Network Expansion for Class Incremental Learning. (arXiv:2303.12696v1 [cs.CV])\nAbstract: The problem of class incremental learning (CIL) is considered. State-of-the-art approaches use a dynamic architecture based on network expansion (NE), in which a task expert is added per task. While effective from a computational standpoint, these methods lead to models that grow quickly with the number of tasks. A new NE method, dense network expansion (DNE), is proposed to achieve a better trade-off between accuracy and model complexity. This is accomplished by the introduction of dense connections between the intermediate layers of the task expert networks, that enable the transfer of knowledge from old to new tasks via feature sharing and reusing. This sharing is implemented with a cross-task attention mechanism, based on a new task attention block (TAB), that fuses information across tasks. Unlike traditional attention mechanisms, TAB operates at the level of the feature mixing and is decoupled with spatial attentions. This is shown more effective than a joint spatial-and-task att",
    "path": "papers/23/03/2303.12696.json",
    "total_tokens": 952,
    "translated_title": "密集网络扩展用于类别增量学习",
    "translated_abstract": "本文研究了类别增量学习的问题。现有的方法采用基于网络扩展（NE）的动态架构，每个任务都需要增加一个专家。虽然这种方法在计算上很有效，但会导致模型随着任务数量的增加而快速增长。我们提出了一种新的网络扩展方法，即密集网络扩展（DNE），以在精度和模型复杂度之间实现更好的平衡。我们引入了任务专家网络中间层之间的密集连接，通过特征共享和复用，实现了从旧任务到新任务的知识传递。这种共享是通过跨任务注意机制来实现的，基于新任务注意块（TAB），它可以在任务之间融合信息。与传统注意机制不同，TAB 在特征混合的层次上操作，并且与空间注意力解耦。与联合空间和任务关注机制相比，这种方法在精度和计算效率方面都更有效。该方法在多个基准数据集上实现了最先进的性能。",
    "tldr": "本文提出了一种新的网络扩展方法，称为密集网络扩展（DNE），通过跨任务注意机制和密集连接来实现从旧任务到新任务的知识传递。在精度和模型复杂度之间实现更好的平衡，达到了最先进的性能。"
}