{
    "title": "Sequential training of GANs against GAN-classifiers reveals correlated \"knowledge gaps\" present among independently trained GAN instances. (arXiv:2303.15533v1 [cs.LG])",
    "abstract": "Modern Generative Adversarial Networks (GANs) generate realistic images remarkably well. Previous work has demonstrated the feasibility of \"GAN-classifiers\" that are distinct from the co-trained discriminator, and operate on images generated from a frozen GAN. That such classifiers work at all affirms the existence of \"knowledge gaps\" (out-of-distribution artifacts across samples) present in GAN training. We iteratively train GAN-classifiers and train GANs that \"fool\" the classifiers (in an attempt to fill the knowledge gaps), and examine the effect on GAN training dynamics, output quality, and GAN-classifier generalization. We investigate two settings, a small DCGAN architecture trained on low dimensional images (MNIST), and StyleGAN2, a SOTA GAN architecture trained on high dimensional images (FFHQ). We find that the DCGAN is unable to effectively fool a held-out GAN-classifier without compromising the output quality. However, StyleGAN2 can fool held-out classifiers with no change in",
    "link": "http://arxiv.org/abs/2303.15533",
    "context": "Title: Sequential training of GANs against GAN-classifiers reveals correlated \"knowledge gaps\" present among independently trained GAN instances. (arXiv:2303.15533v1 [cs.LG])\nAbstract: Modern Generative Adversarial Networks (GANs) generate realistic images remarkably well. Previous work has demonstrated the feasibility of \"GAN-classifiers\" that are distinct from the co-trained discriminator, and operate on images generated from a frozen GAN. That such classifiers work at all affirms the existence of \"knowledge gaps\" (out-of-distribution artifacts across samples) present in GAN training. We iteratively train GAN-classifiers and train GANs that \"fool\" the classifiers (in an attempt to fill the knowledge gaps), and examine the effect on GAN training dynamics, output quality, and GAN-classifier generalization. We investigate two settings, a small DCGAN architecture trained on low dimensional images (MNIST), and StyleGAN2, a SOTA GAN architecture trained on high dimensional images (FFHQ). We find that the DCGAN is unable to effectively fool a held-out GAN-classifier without compromising the output quality. However, StyleGAN2 can fool held-out classifiers with no change in",
    "path": "papers/23/03/2303.15533.json",
    "total_tokens": 1136,
    "translated_title": "对抗生成网络的顺序训练与GAN分类器揭示了独立训练的GAN实例之间存在的相关“知识盲区”",
    "translated_abstract": "现代生成对抗网络（GAN）可以生成非常逼真的图像。先前的工作已经证明，可以训练与协同训练的鉴别器不同，对从冻结GAN中生成的图像进行分类的“GAN分类器”。这样的分类器能够工作表明GAN训练中存在“知识盲区”（样本之间的分布外伪特征）。本文迭代地训练GAN分类器和训练GAN，以“欺骗”分类器（尝试填补“知识盲区”），并研究其对GAN训练动态、输出质量和GAN分类器泛化能力的影响。我们研究了两种情况，一个是小型的低维图像（MNIST）的DCGAN架构，另一个是StyleGAN2，一种在高维图像（FFHQ）上训练的SOTA GAN架构。我们发现DCGAN无法有效地欺骗一个分开的GAN分类器而不影响输出质量。然而，StyleGAN2可以欺骗分开的分类器而不影响输出质量，表明它已经学会了生成更多样化和鲁棒的特征。我们的实验揭示了顺序训练GAN对抗GAN分类器可以揭示独立训练的GAN实例之间存在的相关“知识盲区”，并有机会改善GAN训练的稳定性和输出质量。",
    "tldr": "本文研究发现，顺序训练GAN对抗GAN分类器可以揭示独立训练的GAN实例之间存在的相关“知识盲区”，有机会改善GAN训练的稳定性和输出质量。",
    "en_tdlr": "This paper finds that sequentially training GANs against GAN-classifiers can reveal correlated \"knowledge gaps\" present among independently trained GAN instances, which provides opportunities for improving GAN training stability and output quality in the future."
}