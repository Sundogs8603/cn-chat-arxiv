{
    "title": "Describe me an Aucklet: Generating Grounded Perceptual Category Descriptions. (arXiv:2303.04053v3 [cs.CL] UPDATED)",
    "abstract": "Human speakers can generate descriptions of perceptual concepts, abstracted from the instance-level. Moreover, such descriptions can be used by other speakers to learn provisional representations of those concepts. Learning and using abstract perceptual concepts is under-investigated in the language-and-vision field. The problem is also highly relevant to the field of representation learning in multi-modal NLP. In this paper, we introduce a framework for testing category-level perceptual grounding in multi-modal language models. In particular, we train separate neural networks to generate and interpret descriptions of visual categories. We measure the communicative success of the two models with the zero-shot classification performance of the interpretation model, which we argue is an indicator of perceptual grounding. Using this framework, we compare the performance of prototype- and exemplar-based representations. Finally, we show that communicative success exposes performance issues",
    "link": "http://arxiv.org/abs/2303.04053",
    "context": "Title: Describe me an Aucklet: Generating Grounded Perceptual Category Descriptions. (arXiv:2303.04053v3 [cs.CL] UPDATED)\nAbstract: Human speakers can generate descriptions of perceptual concepts, abstracted from the instance-level. Moreover, such descriptions can be used by other speakers to learn provisional representations of those concepts. Learning and using abstract perceptual concepts is under-investigated in the language-and-vision field. The problem is also highly relevant to the field of representation learning in multi-modal NLP. In this paper, we introduce a framework for testing category-level perceptual grounding in multi-modal language models. In particular, we train separate neural networks to generate and interpret descriptions of visual categories. We measure the communicative success of the two models with the zero-shot classification performance of the interpretation model, which we argue is an indicator of perceptual grounding. Using this framework, we compare the performance of prototype- and exemplar-based representations. Finally, we show that communicative success exposes performance issues",
    "path": "papers/23/03/2303.04053.json",
    "total_tokens": 782,
    "translated_title": "描述一个Aucklet：生成基于感知的类别描述",
    "translated_abstract": "人类演讲者可以生成从实例级别抽象出来的感知概念描述。此外，其他演讲者可以利用这些描述来学习这些概念的临时表示。在语言和视觉领域，学习和使用抽象感知概念的问题还不够研究。这个问题对多模态自然语言处理中的表示学习领域也非常相关。在本文中，我们介绍了一个用于测试多模态语言模型中基于类别的感知基础的框架。特别地，我们训练了不同的神经网络来生成和解释视觉类别的描述。我们用解释模型的零样本分类性能来衡量两个模型的交流成功度，我们认为这是感知基础的一个指标。利用这个框架，我们比较了基于原型和样本的表示的性能。最后，我们展示了交流成功揭示了性能问题。",
    "tldr": "本文介绍了一个用于测试多模态语言模型中基于类别的感知基础的框架，并比较了基于原型和样本的表示的性能。"
}