{
    "title": "Fast computation of permutation equivariant layers with the partition algebra. (arXiv:2303.06208v1 [cs.LG])",
    "abstract": "Linear neural network layers that are either equivariant or invariant to permutations of their inputs form core building blocks of modern deep learning architectures. Examples include the layers of DeepSets, as well as linear layers occurring in attention blocks of transformers and some graph neural networks. The space of permutation equivariant linear layers can be identified as the invariant subspace of a certain symmetric group representation, and recent work parameterized this space by exhibiting a basis whose vectors are sums over orbits of standard basis elements with respect to the symmetric group action. A parameterization opens up the possibility of learning the weights of permutation equivariant linear layers via gradient descent. The space of permutation equivariant linear layers is a generalization of the partition algebra, an object first discovered in statistical physics with deep connections to the representation theory of the symmetric group, and the basis described abo",
    "link": "http://arxiv.org/abs/2303.06208",
    "total_tokens": 1217,
    "translated_title": "利用分区代数快速计算置换等变层",
    "translated_abstract": "线性神经网络层，无论是等变还是不变于其输入的排列，都是现代深度学习架构的核心构建块。例如DeepSets的层，以及出现在transformers和一些图神经网络的注意力块中的线性层。置换等变线性层的空间可以被识别为某个对称群表示的不变子空间，并且最近的工作通过展示一组基础，其向量是标准基础元素在对称群作用下轨道的总和，来参数化这个空间。参数化打开了通过梯度下降学习置换等变线性层权重的可能性。置换等变线性层的空间是分区代数的一般化，这是一种在统计物理学中首次发现的对象，与对称群的表示论有着深刻的联系，而上述基础与分区代数的基础密切相关。在本文中，我们展示了如何利用这种联系，在输入大小的线性时间内计算置换等变线性层的输出，并在输入大小的二次时间内计算损失相对于权重的梯度。我们的方法基于一种计算分区代数在向量上作用的新算法，我们称之为“分区卷积”。我们展示了分区卷积可以在输入向量大小的线性时间内计算，并且可以用于在输入大小的线性和二次时间内计算置换等变线性层的输出和梯度，分别。我们还展示了如何使用分区卷积来计算某些非线性置换等变层的输出和梯度，并在几个基准数据集上展示了我们方法的有效性。",
    "tldr": "本文提出了一种利用分区代数计算置换等变层输出和梯度的新算法，可以在输入大小的线性和二次时间内计算，有效性得到了在几个基准数据集上的证明。",
    "en_tldr": "This paper proposes a new algorithm for computing the output and gradient of permutation equivariant linear layers using the partition algebra, which can be computed in time linear and quadratic in the input size, respectively. The effectiveness of the approach is demonstrated on several benchmark datasets."
}