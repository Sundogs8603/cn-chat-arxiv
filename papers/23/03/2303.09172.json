{
    "title": "Learning Logic Specifications for Soft Policy Guidance in POMCP. (arXiv:2303.09172v1 [cs.AI])",
    "abstract": "Partially Observable Monte Carlo Planning (POMCP) is an efficient solver for Partially Observable Markov Decision Processes (POMDPs). It allows scaling to large state spaces by computing an approximation of the optimal policy locally and online, using a Monte Carlo Tree Search based strategy. However, POMCP suffers from sparse reward function, namely, rewards achieved only when the final goal is reached, particularly in environments with large state spaces and long horizons. Recently, logic specifications have been integrated into POMCP to guide exploration and to satisfy safety requirements. However, such policy-related rules require manual definition by domain experts, especially in real-world scenarios. In this paper, we use inductive logic programming to learn logic specifications from traces of POMCP executions, i.e., sets of belief-action pairs generated by the planner. Specifically, we learn rules expressed in the paradigm of answer set programming. We then integrate them inside",
    "link": "http://arxiv.org/abs/2303.09172",
    "context": "Title: Learning Logic Specifications for Soft Policy Guidance in POMCP. (arXiv:2303.09172v1 [cs.AI])\nAbstract: Partially Observable Monte Carlo Planning (POMCP) is an efficient solver for Partially Observable Markov Decision Processes (POMDPs). It allows scaling to large state spaces by computing an approximation of the optimal policy locally and online, using a Monte Carlo Tree Search based strategy. However, POMCP suffers from sparse reward function, namely, rewards achieved only when the final goal is reached, particularly in environments with large state spaces and long horizons. Recently, logic specifications have been integrated into POMCP to guide exploration and to satisfy safety requirements. However, such policy-related rules require manual definition by domain experts, especially in real-world scenarios. In this paper, we use inductive logic programming to learn logic specifications from traces of POMCP executions, i.e., sets of belief-action pairs generated by the planner. Specifically, we learn rules expressed in the paradigm of answer set programming. We then integrate them inside",
    "path": "papers/23/03/2303.09172.json",
    "total_tokens": 942,
    "translated_title": "POMCP中学习逻辑规范以实现软政策指导",
    "translated_abstract": "部分可观察的蒙特卡洛规划（POMCP）是一种有效的部分可观察马尔可夫决策过程（POMDP）的解决器。它通过使用基于蒙特卡洛树搜索的策略，在本地和在线计算最优策略的近似，从而使得规模上的扩展成为可能。然而，POMCP在稀疏奖励函数方面存在问题，即仅在达到最终目标时获得奖励，尤其是在具有大状态空间和长时间跨度的环境中。最近，已经将逻辑规范集成到POMCP中，以指导探索并满足安全性要求。然而，在真实世界的情况下，这些与策略相关的规则需要由领域专家手动定义。在本文中，我们使用归纳逻辑编程从POMCP执行的跟踪中学习逻辑规范，即由规划器生成的信念-行为对集合。具体来说，我们学习了用答案集编程范式表示的规则。然后我们将它们集成到POMCP中利用它以实现软指导政策。",
    "tldr": "论文提出了一种从POMCP执行中学习逻辑规范的方法，以实现软政策指导，代替手动定义的策略相关规则，并用于解决环境状态空间大的问题。",
    "en_tdlr": "This paper proposes a method of learning logic specifications from POMCP execution to achieve soft policy guidance, instead of manually defined policy-related rules, and applies them to solve the problem of large state space in the environment."
}