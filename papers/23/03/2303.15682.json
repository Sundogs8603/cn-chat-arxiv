{
    "title": "Pre-training Transformers for Knowledge Graph Completion. (arXiv:2303.15682v1 [cs.CL])",
    "abstract": "Learning transferable representation of knowledge graphs (KGs) is challenging due to the heterogeneous, multi-relational nature of graph structures. Inspired by Transformer-based pretrained language models' success on learning transferable representation for texts, we introduce a novel inductive KG representation model (iHT) for KG completion by large-scale pre-training. iHT consists of a entity encoder (e.g., BERT) and a neighbor-aware relational scoring function both parameterized by Transformers. We first pre-train iHT on a large KG dataset, Wikidata5M. Our approach achieves new state-of-the-art results on matched evaluations, with a relative improvement of more than 25% in mean reciprocal rank over previous SOTA models. When further fine-tuned on smaller KGs with either entity and relational shifts, pre-trained iHT representations are shown to be transferable, significantly improving the performance on FB15K-237 and WN18RR.",
    "link": "http://arxiv.org/abs/2303.15682",
    "context": "Title: Pre-training Transformers for Knowledge Graph Completion. (arXiv:2303.15682v1 [cs.CL])\nAbstract: Learning transferable representation of knowledge graphs (KGs) is challenging due to the heterogeneous, multi-relational nature of graph structures. Inspired by Transformer-based pretrained language models' success on learning transferable representation for texts, we introduce a novel inductive KG representation model (iHT) for KG completion by large-scale pre-training. iHT consists of a entity encoder (e.g., BERT) and a neighbor-aware relational scoring function both parameterized by Transformers. We first pre-train iHT on a large KG dataset, Wikidata5M. Our approach achieves new state-of-the-art results on matched evaluations, with a relative improvement of more than 25% in mean reciprocal rank over previous SOTA models. When further fine-tuned on smaller KGs with either entity and relational shifts, pre-trained iHT representations are shown to be transferable, significantly improving the performance on FB15K-237 and WN18RR.",
    "path": "papers/23/03/2303.15682.json",
    "total_tokens": 869,
    "translated_title": "面向知识图谱补全的Transformer预训练。",
    "translated_abstract": "由于图结构的异构性和多关系性，学习知识图谱（KGs）的可转移表示是具有挑战性的。受Transformer基于预训练语言模型在学习文本方面的成功启发，我们引入了一种新型的面向KG补全的归纳式表示模型（iHT），通过大规模预训练，iHT由实体编码器（例如BERT）和邻居感知的关系评分函数组成，两者都由Transformer参数化。我们首先在大型KG数据集Wikidata5M上预训练iHT。我们的方法在匹配评估上实现了新的最先进结果，相对于先前SOTA模型，平均倒数排名提高了25％以上。当在具有实体和关系移位的较小KG上进一步微调时，预训练的iHT表示被证明是可转移的，显着提高了FB15K-237和WN18RR的性能。",
    "tldr": "该论文介绍了一种面向知识图谱补全的归纳式表示模型（iHT），通过大规模预训练，iHT表示可转移且在多个数据集上取得了最先进的结果。"
}