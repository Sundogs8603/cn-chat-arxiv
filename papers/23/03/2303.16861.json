{
    "title": "Beyond Empirical Risk Minimization: Local Structure Preserving Regularization for Improving Adversarial Robustness. (arXiv:2303.16861v1 [cs.LG])",
    "abstract": "It is broadly known that deep neural networks are susceptible to being fooled by adversarial examples with perturbations imperceptible by humans. Various defenses have been proposed to improve adversarial robustness, among which adversarial training methods are most effective. However, most of these methods treat the training samples independently and demand a tremendous amount of samples to train a robust network, while ignoring the latent structural information among these samples. In this work, we propose a novel Local Structure Preserving (LSP) regularization, which aims to preserve the local structure of the input space in the learned embedding space. In this manner, the attacking effect of adversarial samples lying in the vicinity of clean samples can be alleviated. We show strong empirical evidence that with or without adversarial training, our method consistently improves the performance of adversarial robustness on several image classification datasets compared to the baseline",
    "link": "http://arxiv.org/abs/2303.16861",
    "context": "Title: Beyond Empirical Risk Minimization: Local Structure Preserving Regularization for Improving Adversarial Robustness. (arXiv:2303.16861v1 [cs.LG])\nAbstract: It is broadly known that deep neural networks are susceptible to being fooled by adversarial examples with perturbations imperceptible by humans. Various defenses have been proposed to improve adversarial robustness, among which adversarial training methods are most effective. However, most of these methods treat the training samples independently and demand a tremendous amount of samples to train a robust network, while ignoring the latent structural information among these samples. In this work, we propose a novel Local Structure Preserving (LSP) regularization, which aims to preserve the local structure of the input space in the learned embedding space. In this manner, the attacking effect of adversarial samples lying in the vicinity of clean samples can be alleviated. We show strong empirical evidence that with or without adversarial training, our method consistently improves the performance of adversarial robustness on several image classification datasets compared to the baseline",
    "path": "papers/23/03/2303.16861.json",
    "total_tokens": 900,
    "translated_title": "超越经验风险最小化：局部结构保持正则化提高对抗鲁棒性",
    "translated_abstract": "深度神经网络容易被对人类不可感知的扰动所欺骗。许多防御措施已被提出以提高对抗鲁棒性，其中对抗训练方法最为有效。然而，大多数这些方法独立地处理训练样本并要求大量的样本来训练鲁棒性网络，同时忽略这些样本之间的潜在结构信息。在本文中，我们提出了一种新颖的局部结构保持（LSP）正则化方法，旨在在学习的嵌入空间中保持输入空间的局部结构。这样，可缓解干净样本附近存在的对抗样本的攻击效果。我们展示了强有力的实证证据，在几个图像分类数据集中，我们的方法不论是否进行对抗训练，均相对于基线显著提高了对抗鲁棒性的性能。",
    "tldr": "提出一种新颖的局部结构保持正则化方法，旨在在学习的嵌入空间中保持输入空间的局部结构，与基线相比，该方法显著提高了对抗鲁棒性的性能。",
    "en_tdlr": "A novel Local Structure Preserving (LSP) regularization method is proposed to preserve the local structure of the input space in the learned embedding space for improving adversarial robustness. The method consistently outperforms the baseline in terms of adversarial robustness on several image classification datasets."
}