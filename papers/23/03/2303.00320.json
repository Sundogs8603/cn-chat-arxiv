{
    "title": "TimeMAE: Self-Supervised Representations of Time Series with Decoupled Masked Autoencoders. (arXiv:2303.00320v3 [cs.LG] UPDATED)",
    "abstract": "Enhancing the expressive capacity of deep learning-based time series models with self-supervised pre-training has become ever-increasingly prevalent in time series classification. Even though numerous efforts have been devoted to developing self-supervised models for time series data, we argue that the current methods are not sufficient to learn optimal time series representations due to solely unidirectional encoding over sparse point-wise input units. In this work, we propose TimeMAE, a novel self-supervised paradigm for learning transferrable time series representations based on transformer networks. The distinct characteristics of the TimeMAE lie in processing each time series into a sequence of non-overlapping sub-series via window-slicing partitioning, followed by random masking strategies over the semantic units of localized sub-series. Such a simple yet effective setting can help us achieve the goal of killing three birds with one stone, i.e., (1) learning enriched contextual r",
    "link": "http://arxiv.org/abs/2303.00320",
    "context": "Title: TimeMAE: Self-Supervised Representations of Time Series with Decoupled Masked Autoencoders. (arXiv:2303.00320v3 [cs.LG] UPDATED)\nAbstract: Enhancing the expressive capacity of deep learning-based time series models with self-supervised pre-training has become ever-increasingly prevalent in time series classification. Even though numerous efforts have been devoted to developing self-supervised models for time series data, we argue that the current methods are not sufficient to learn optimal time series representations due to solely unidirectional encoding over sparse point-wise input units. In this work, we propose TimeMAE, a novel self-supervised paradigm for learning transferrable time series representations based on transformer networks. The distinct characteristics of the TimeMAE lie in processing each time series into a sequence of non-overlapping sub-series via window-slicing partitioning, followed by random masking strategies over the semantic units of localized sub-series. Such a simple yet effective setting can help us achieve the goal of killing three birds with one stone, i.e., (1) learning enriched contextual r",
    "path": "papers/23/03/2303.00320.json",
    "total_tokens": 900,
    "translated_title": "TimeMAE: 基于解耦掩码自编码器的自监督时间序列表示",
    "translated_abstract": "在时间序列分类中，利用自监督预训练提高深度学习模型的表达能力正在变得越来越普遍。虽然已经有很多工作致力于开发面向时间序列数据的自监督模型，但由于仅在稀疏逐点输入单元上进行单向编码，当前方法不能学习到最优时间序列表示。在这项工作中，我们提出了TimeMAE，一种基于transformer网络的学习可传递时间序列表示的新型自监督范式。TimeMAE的独特特点在于将每个时间序列通过窗口切片分区处理成一系列不重叠的子序列，然后通过随机掩码策略覆盖本地化子序列的语义单元。这种简单而有效的设置可以帮助我们达到一举三得的目标，即（1）学习丰富的上下文信息；",
    "tldr": "TimeMAE是一种新型自监督模型，利用transformer网络将每个时间序列处理成一系列不重叠的子序列，并通过随机掩码策略覆盖本地化子序列的语义单元，以学习到丰富的上下文信息和可传递的时间序列表示。",
    "en_tdlr": "TimeMAE is a novel self-supervised model that uses transformer networks to process each time series into a series of non-overlapping sub-series and randomly masks the semantic units of localized sub-series to learn enriched contextual information and transferrable representations of time series."
}