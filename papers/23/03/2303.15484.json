{
    "title": "Regularize implicit neural representation by itself. (arXiv:2303.15484v1 [cs.LG])",
    "abstract": "This paper proposes a regularizer called Implicit Neural Representation Regularizer (INRR) to improve the generalization ability of the Implicit Neural Representation (INR). The INR is a fully connected network that can represent signals with details not restricted by grid resolution. However, its generalization ability could be improved, especially with non-uniformly sampled data. The proposed INRR is based on learned Dirichlet Energy (DE) that measures similarities between rows/columns of the matrix. The smoothness of the Laplacian matrix is further integrated by parameterizing DE with a tiny INR. INRR improves the generalization of INR in signal representation by perfectly integrating the signal's self-similarity with the smoothness of the Laplacian matrix. Through well-designed numerical experiments, the paper also reveals a series of properties derived from INRR, including momentum methods like convergence trajectory and multi-scale similarity. Moreover, the proposed method could ",
    "link": "http://arxiv.org/abs/2303.15484",
    "context": "Title: Regularize implicit neural representation by itself. (arXiv:2303.15484v1 [cs.LG])\nAbstract: This paper proposes a regularizer called Implicit Neural Representation Regularizer (INRR) to improve the generalization ability of the Implicit Neural Representation (INR). The INR is a fully connected network that can represent signals with details not restricted by grid resolution. However, its generalization ability could be improved, especially with non-uniformly sampled data. The proposed INRR is based on learned Dirichlet Energy (DE) that measures similarities between rows/columns of the matrix. The smoothness of the Laplacian matrix is further integrated by parameterizing DE with a tiny INR. INRR improves the generalization of INR in signal representation by perfectly integrating the signal's self-similarity with the smoothness of the Laplacian matrix. Through well-designed numerical experiments, the paper also reveals a series of properties derived from INRR, including momentum methods like convergence trajectory and multi-scale similarity. Moreover, the proposed method could ",
    "path": "papers/23/03/2303.15484.json",
    "total_tokens": 941,
    "translated_title": "自身正则化隐式神经表示",
    "translated_abstract": "本文提出了一种名为隐式神经表示正则化器（INRR）的正则化器，以提高隐式神经表示（INR）的泛化能力。INR是一种完全连接的网络，可以表示信号的细节，并不受网格分辨率的限制。然而，尤其是在非均匀采样数据的情况下，其泛化能力需要提高。所提出的INRR基于所学得的狄利克雷能量（DE），以测量矩阵的行/列之间的相似性。将拉普拉斯矩阵的平滑性进一步集成到将DE参数化为微小INR中。INRR通过将信号的自相似性与拉普拉斯矩阵的平滑度完美集成，提高了INR在信号表示中的泛化能力。通过精心设计的数值实验，本文还揭示了一系列从INRR中得出的共性，包括收敛轨迹和多尺度相似性等动量方法。此外，所提出的方法也可用于其他模型的正则化。",
    "tldr": "本文提出了一种正则化器INRR，可以提高隐式神经表示INR的泛化能力，通过将信号的自相似性与拉普拉斯矩阵的平滑度完美集成。此外，研究了INRR的一系列共性，可用于其他模型的正则化。",
    "en_tdlr": "This paper proposes a regularizer called INRR to improve the generalization ability of INR. INRR perfectly integrates the self-similarity of signals with the smoothness of the Laplacian matrix. The paper also reveals a series of properties derived from INRR and it could be used for regularization of other models."
}