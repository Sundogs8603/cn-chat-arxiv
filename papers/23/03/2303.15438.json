{
    "title": "On the Stepwise Nature of Self-Supervised Learning. (arXiv:2303.15438v2 [cs.LG] UPDATED)",
    "abstract": "We present a simple picture of the training process of joint embedding self-supervised learning methods. We find that these methods learn their high-dimensional embeddings one dimension at a time in a sequence of discrete, well-separated steps. We arrive at this conclusion via the study of a linearized model of Barlow Twins applicable to the case in which the trained network is infinitely wide. We solve the training dynamics of this model from small initialization, finding that the model learns the top eigenmodes of a certain contrastive kernel in a stepwise fashion, and obtain a closed-form expression for the final learned representations. Remarkably, we then see the same stepwise learning phenomenon when training deep ResNets using the Barlow Twins, SimCLR, and VICReg losses. Our theory suggests that, just as kernel regression can be thought of as a model of supervised learning, kernel PCA may serve as a useful model of self-supervised learning.",
    "link": "http://arxiv.org/abs/2303.15438",
    "context": "Title: On the Stepwise Nature of Self-Supervised Learning. (arXiv:2303.15438v2 [cs.LG] UPDATED)\nAbstract: We present a simple picture of the training process of joint embedding self-supervised learning methods. We find that these methods learn their high-dimensional embeddings one dimension at a time in a sequence of discrete, well-separated steps. We arrive at this conclusion via the study of a linearized model of Barlow Twins applicable to the case in which the trained network is infinitely wide. We solve the training dynamics of this model from small initialization, finding that the model learns the top eigenmodes of a certain contrastive kernel in a stepwise fashion, and obtain a closed-form expression for the final learned representations. Remarkably, we then see the same stepwise learning phenomenon when training deep ResNets using the Barlow Twins, SimCLR, and VICReg losses. Our theory suggests that, just as kernel regression can be thought of as a model of supervised learning, kernel PCA may serve as a useful model of self-supervised learning.",
    "path": "papers/23/03/2303.15438.json",
    "total_tokens": 962,
    "translated_title": "自监督学习的阶梯式本质研究",
    "translated_abstract": "我们提出了一个简单的训练过程图，阐述联合嵌入式自监督学习方法的训练过程。我们发现这些方法以离散、互相独立的步骤一次学习一维高维嵌入。我们通过Barlow Twins的线性化模型研究得出了这个结论，该模型适用于训练网络永远是无限宽的情况。我们从小的初始化开始解决了这个模型的训练动力学问题，发现该模型以阶梯式的方式学习某个对比核的顶部特征向量，并获得了最终所学嵌入的闭合表达式。有趣的是，我们还发现在使用Barlow Twins、SimCLR和VICReg损失训练深层ResNet时出现了相同的阶梯式学习现象。我们的理论表明，就像核回归可以被认为是监督学习的模型一样，核PCA可以作为自监督学习的有用模型。",
    "tldr": "本文研究了联合嵌入式自监督学习方法的阶梯式本质，通过研究Barlow Twins的线性化模型发现，这些方法是通过离散的步骤依次学习高维嵌入中的每一个维度。我们认为，核PCA可以作为自监督学习的有用模型。",
    "en_tdlr": "This paper investigates the stepwise nature of joint embedding self-supervised learning methods. Through studying a linearized model of Barlow Twins, the authors find that these methods learn their high-dimensional embeddings one dimension at a time in discrete and well-separated steps. The study suggests that kernel PCA may serve as a useful model of self-supervised learning."
}