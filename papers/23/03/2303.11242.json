{
    "title": "Make Landscape Flatter in Differentially Private Federated Learning. (arXiv:2303.11242v2 [cs.LG] UPDATED)",
    "abstract": "To defend the inference attacks and mitigate the sensitive information leakages in Federated Learning (FL), client-level Differentially Private FL (DPFL) is the de-facto standard for privacy protection by clipping local updates and adding random noise. However, existing DPFL methods tend to make a sharper loss landscape and have poorer weight perturbation robustness, resulting in severe performance degradation. To alleviate these issues, we propose a novel DPFL algorithm named DP-FedSAM, which leverages gradient perturbation to mitigate the negative impact of DP. Specifically, DP-FedSAM integrates Sharpness Aware Minimization (SAM) optimizer to generate local flatness models with better stability and weight perturbation robustness, which results in the small norm of local updates and robustness to DP noise, thereby improving the performance. From the theoretical perspective, we analyze in detail how DP-FedSAM mitigates the performance degradation induced by DP. Meanwhile, we give rigor",
    "link": "http://arxiv.org/abs/2303.11242",
    "context": "Title: Make Landscape Flatter in Differentially Private Federated Learning. (arXiv:2303.11242v2 [cs.LG] UPDATED)\nAbstract: To defend the inference attacks and mitigate the sensitive information leakages in Federated Learning (FL), client-level Differentially Private FL (DPFL) is the de-facto standard for privacy protection by clipping local updates and adding random noise. However, existing DPFL methods tend to make a sharper loss landscape and have poorer weight perturbation robustness, resulting in severe performance degradation. To alleviate these issues, we propose a novel DPFL algorithm named DP-FedSAM, which leverages gradient perturbation to mitigate the negative impact of DP. Specifically, DP-FedSAM integrates Sharpness Aware Minimization (SAM) optimizer to generate local flatness models with better stability and weight perturbation robustness, which results in the small norm of local updates and robustness to DP noise, thereby improving the performance. From the theoretical perspective, we analyze in detail how DP-FedSAM mitigates the performance degradation induced by DP. Meanwhile, we give rigor",
    "path": "papers/23/03/2303.11242.json",
    "total_tokens": 1071,
    "translated_title": "在差分隐私联邦学习中降低梯度平面化",
    "translated_abstract": "为了防御推理攻击并减轻联邦学习中的敏感信息泄漏，客户端级别的差分隐私联邦学习（DPFL）通过裁剪本地更新和添加随机噪声成为隐私保护的事实标准。然而，现有的DPFL方法往往会导致更尖锐的损失平面，并具有较差的权重扰动鲁棒性，从而导致严重的性能下降。为了缓解这些问题，我们提出了一种新的DPFL算法DP-FedSAM，它利用梯度扰动来减轻DP的负面影响。具体来说，DP-FedSAM将Sharpness Aware Minimization（SAM）优化器集成到其中，生成更稳定和具有更好的权重扰动鲁棒性的本地平坦模型，这导致本地更新的范数小且对DP噪声有鲁棒性，从而提高性能。从理论上来说，我们详细分析了DP-FedSAM如何减轻DP导致的性能下降。与此同时，我们基于Rényi差分隐私框架给出了严格的隐私分析，以保证我们提出的算法具有强隐私保证。在多个基准数据集上的广泛实验证明，在各种敌对设置下，DP-FedSAM相对于最先进的DPFL方法具有卓越的性能。",
    "tldr": "提出了一个新的差分隐私联邦学习算法DP-FedSAM，它利用梯度扰动生成本地平坦模型，从而提高性能和隐私保证。",
    "en_tdlr": "Proposed a novel DPFL algorithm named DP-FedSAM that leverages gradient perturbation to generate local flatness models with better stability and weight perturbation robustness, thereby improving the performance and privacy guarantee in differentially private federated learning."
}