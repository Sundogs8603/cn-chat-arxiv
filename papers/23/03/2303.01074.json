{
    "title": "Learning not to Regret",
    "abstract": "arXiv:2303.01074v2 Announce Type: replace-cross  Abstract: The literature on game-theoretic equilibrium finding predominantly focuses on single games or their repeated play. Nevertheless, numerous real-world scenarios feature playing a game sampled from a distribution of similar, but not identical games, such as playing poker with different public cards or trading correlated assets on the stock market. As these similar games feature similar equilibra, we investigate a way to accelerate equilibrium finding on such a distribution. We present a novel \"learning not to regret\" framework, enabling us to meta-learn a regret minimizer tailored to a specific distribution. Our key contribution, Neural Predictive Regret Matching, is uniquely meta-learned to converge rapidly for the chosen distribution of games, while having regret minimization guarantees on any game. We validated our algorithms' faster convergence on a distribution of river poker games. Our experiments show that the meta-learned ",
    "link": "https://arxiv.org/abs/2303.01074",
    "context": "Title: Learning not to Regret\nAbstract: arXiv:2303.01074v2 Announce Type: replace-cross  Abstract: The literature on game-theoretic equilibrium finding predominantly focuses on single games or their repeated play. Nevertheless, numerous real-world scenarios feature playing a game sampled from a distribution of similar, but not identical games, such as playing poker with different public cards or trading correlated assets on the stock market. As these similar games feature similar equilibra, we investigate a way to accelerate equilibrium finding on such a distribution. We present a novel \"learning not to regret\" framework, enabling us to meta-learn a regret minimizer tailored to a specific distribution. Our key contribution, Neural Predictive Regret Matching, is uniquely meta-learned to converge rapidly for the chosen distribution of games, while having regret minimization guarantees on any game. We validated our algorithms' faster convergence on a distribution of river poker games. Our experiments show that the meta-learned ",
    "path": "papers/23/03/2303.01074.json",
    "total_tokens": 862,
    "translated_title": "学习不后悔",
    "translated_abstract": "博弈论均衡研究文献主要集中在单个游戏或其重复游戏上。然而，许多现实世界的场景涉及玩一个取自类似但不相同游戏分布的游戏，比如用不同公共牌玩扑克或在股票市场上交易相关资产。由于这些相似游戏具有相似的均衡，我们研究了在这种分布上加速寻找均衡的方法。我们提出了一个新颖的“学会不后悔”的框架，使我们能够元学习针对特定分布量身定制的减小后悔的算法。我们的主要贡献，即神经预测后悔匹配，是独特地元学习，能够在所选择的游戏分布上快速收敛，同时在任何游戏上具有减小后悔的保证。我们验证了算法在一组公共牌扑克游戏分布上更快地收敛。我们的实验表明，元学习的...",
    "tldr": "提出了一个“学习不后悔”的框架，可加速在类似但不相同游戏分布上的均衡寻找，同时在任何游戏上具有减小后悔的保证。",
    "en_tdlr": "Introduced a \"learning not to regret\" framework to accelerate equilibrium finding on a distribution of similar but not identical games, with regret minimization guarantees on any game."
}