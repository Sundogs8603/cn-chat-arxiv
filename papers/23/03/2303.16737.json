{
    "title": "Multi-Agent Reinforcement Learning with Action Masking for UAV-enabled Mobile Communications. (arXiv:2303.16737v1 [cs.MA])",
    "abstract": "Unmanned Aerial Vehicles (UAVs) are increasingly used as aerial base stations to provide ad hoc communications infrastructure. Building upon prior research efforts which consider either static nodes, 2D trajectories or single UAV systems, this paper focuses on the use of multiple UAVs for providing wireless communication to mobile users in the absence of terrestrial communications infrastructure. In particular, we jointly optimize UAV 3D trajectory and NOMA power allocation to maximize system throughput. Firstly, a weighted K-means-based clustering algorithm establishes UAV-user associations at regular intervals. The efficacy of training a novel Shared Deep Q-Network (SDQN) with action masking is then explored. Unlike training each UAV separately using DQN, the SDQN reduces training time by using the experiences of multiple UAVs instead of a single agent. We also show that SDQN can be used to train a multi-agent system with differing action spaces. Simulation results confirm that: 1) t",
    "link": "http://arxiv.org/abs/2303.16737",
    "context": "Title: Multi-Agent Reinforcement Learning with Action Masking for UAV-enabled Mobile Communications. (arXiv:2303.16737v1 [cs.MA])\nAbstract: Unmanned Aerial Vehicles (UAVs) are increasingly used as aerial base stations to provide ad hoc communications infrastructure. Building upon prior research efforts which consider either static nodes, 2D trajectories or single UAV systems, this paper focuses on the use of multiple UAVs for providing wireless communication to mobile users in the absence of terrestrial communications infrastructure. In particular, we jointly optimize UAV 3D trajectory and NOMA power allocation to maximize system throughput. Firstly, a weighted K-means-based clustering algorithm establishes UAV-user associations at regular intervals. The efficacy of training a novel Shared Deep Q-Network (SDQN) with action masking is then explored. Unlike training each UAV separately using DQN, the SDQN reduces training time by using the experiences of multiple UAVs instead of a single agent. We also show that SDQN can be used to train a multi-agent system with differing action spaces. Simulation results confirm that: 1) t",
    "path": "papers/23/03/2303.16737.json",
    "total_tokens": 1032,
    "translated_abstract": "无人机被越来越多地用作提供即席通信基础设施的空中基站。在之前的研究中，通常仅考虑静态节点、二维轨迹或单一无人机系统。而本文则将重点放在利用多个无人机为移动用户提供无地面通信基础设施的领域上。本文中，我们联合优化了无人机的三维轨迹和NOMA功率分配以最大化系统吞吐量。首先，基于加权K均值聚类算法建立了无人机-用户关联。其次，本文探讨了一种新颖的共享深度Q网络（SDQN）并采用动作屏蔽进行训练。与单独使用DQN为每个无人机进行训练不同，SDQN通过使用多个无人机的经验而不是单个智能体，减少了培训时间。我们还展示了SDQN可用于训练具有不同动作空间的多智能体系统。仿真结果证实了：1）通过SDQN进行训练所得到的推出性能优于DQN；2）通过使用动作屏蔽，SDQN能够在忽略不必要的动作的同时减少训练时间。",
    "tldr": "本文针对无地面通信基础设施下，利用多个无人机为移动用户提供通信的问题进行研究。采用共享深度Q网络（SDQN）和动作屏蔽技术训练多智能体系统，优化无人机的三维轨迹和功率分配以最大化系统吞吐量。",
    "en_tdlr": "This paper studies the use of multiple UAVs for providing communication to mobile users in the absence of terrestrial communications infrastructure. It optimizes UAV trajectory and NOMA power allocation using a novel Shared Deep Q-Network (SDQN) with action masking to train a multi-agent system and maximize system throughput."
}