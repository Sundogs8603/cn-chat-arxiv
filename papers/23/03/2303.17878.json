{
    "title": "Fused Depthwise Tiling for Memory Optimization in TinyML Deep Neural Network Inference. (arXiv:2303.17878v1 [cs.LG])",
    "abstract": "Memory optimization for deep neural network (DNN) inference gains high relevance with the emergence of TinyML, which refers to the deployment of DNN inference tasks on tiny, low-power microcontrollers. Applications such as audio keyword detection or radar-based gesture recognition are heavily constrained by the limited memory on such tiny devices because DNN inference requires large intermediate run-time buffers to store activations and other intermediate data, which leads to high memory usage. In this paper, we propose a new Fused Depthwise Tiling (FDT) method for the memory optimization of DNNs, which, compared to existing tiling methods, reduces memory usage without inducing any run time overhead. FDT applies to a larger variety of network layers than existing tiling methods that focus on convolutions. It improves TinyML memory optimization significantly by reducing memory of models where this was not possible before and additionally providing alternative design points for models th",
    "link": "http://arxiv.org/abs/2303.17878",
    "context": "Title: Fused Depthwise Tiling for Memory Optimization in TinyML Deep Neural Network Inference. (arXiv:2303.17878v1 [cs.LG])\nAbstract: Memory optimization for deep neural network (DNN) inference gains high relevance with the emergence of TinyML, which refers to the deployment of DNN inference tasks on tiny, low-power microcontrollers. Applications such as audio keyword detection or radar-based gesture recognition are heavily constrained by the limited memory on such tiny devices because DNN inference requires large intermediate run-time buffers to store activations and other intermediate data, which leads to high memory usage. In this paper, we propose a new Fused Depthwise Tiling (FDT) method for the memory optimization of DNNs, which, compared to existing tiling methods, reduces memory usage without inducing any run time overhead. FDT applies to a larger variety of network layers than existing tiling methods that focus on convolutions. It improves TinyML memory optimization significantly by reducing memory of models where this was not possible before and additionally providing alternative design points for models th",
    "path": "papers/23/03/2303.17878.json",
    "total_tokens": 942,
    "translated_title": "嵌入式深度神经网络中的融合深度分块瓦片技术提高内存利用率",
    "translated_abstract": "随着微型、低功耗控制器的出现，深度神经网络（DNN）推理的内存优化变得越来越重要。在这样的小型设备上运行DNN推理任务，如音频关键词检测或基于雷达的手势识别，由于DNN推理需要大量的中间运行时缓冲区来存储激活和其他中间数据，从而导致内存使用量高度限制。本文提出了一种新的融合深度分块瓦片（FDT）方法，用于DNN的内存优化。与现有的分块方法相比，FDT可以在不引入任何运行时开销的情况下降低内存使用量，并适用于比现有分块方法更多种类的网络层。FDT显著提高了TinyML的内存优化，减少了以前无法减少内存的模型，并为模型的替代设计提供了其他设计方案。",
    "tldr": "本文提出了一种新的融合深度分块瓦片（FDT）方法，可以在不引入任何运行时开销的情况下降低DNN推理的内存使用量，进一步提高了在嵌入式设备上进行的TinyML模型的内存利用率。",
    "en_tdlr": "This paper proposes a new Fused Depthwise Tiling (FDT) method to optimize memory usage of DNN inference on tiny, low-power microcontrollers for TinyML applications. Compared to existing tiling methods, FDT reduces memory usage without inducing any run time overhead and applies to a larger variety of network layers. This improves the memory optimization of TinyML models and provides alternative design points."
}