{
    "title": "SoftCLIP: Softer Cross-modal Alignment Makes CLIP Stronger. (arXiv:2303.17561v1 [cs.CV])",
    "abstract": "During the preceding biennium, vision-language pre-training has achieved noteworthy success on several downstream tasks. Nevertheless, acquiring high-quality image-text pairs, where the pairs are entirely exclusive of each other, remains a challenging task, and noise exists in the commonly used datasets. To address this issue, we propose SoftCLIP, a novel approach that relaxes the strict one-to-one constraint and achieves a soft cross-modal alignment by introducing a softened target, which is generated from the fine-grained intra-modal self-similarity. The intra-modal guidance is indicative to enable two pairs have some local similarities and model many-to-many relationships between the two modalities. Besides, since the positive still dominates in the softened target distribution, we disentangle the negatives in the distribution to further boost the relation alignment with the negatives in the cross-modal learning. Extensive experiments demonstrate the effectiveness of SoftCLIP. In pa",
    "link": "http://arxiv.org/abs/2303.17561",
    "context": "Title: SoftCLIP: Softer Cross-modal Alignment Makes CLIP Stronger. (arXiv:2303.17561v1 [cs.CV])\nAbstract: During the preceding biennium, vision-language pre-training has achieved noteworthy success on several downstream tasks. Nevertheless, acquiring high-quality image-text pairs, where the pairs are entirely exclusive of each other, remains a challenging task, and noise exists in the commonly used datasets. To address this issue, we propose SoftCLIP, a novel approach that relaxes the strict one-to-one constraint and achieves a soft cross-modal alignment by introducing a softened target, which is generated from the fine-grained intra-modal self-similarity. The intra-modal guidance is indicative to enable two pairs have some local similarities and model many-to-many relationships between the two modalities. Besides, since the positive still dominates in the softened target distribution, we disentangle the negatives in the distribution to further boost the relation alignment with the negatives in the cross-modal learning. Extensive experiments demonstrate the effectiveness of SoftCLIP. In pa",
    "path": "papers/23/03/2303.17561.json",
    "total_tokens": 813,
    "translated_title": "SoftCLIP: 更柔和的跨模态对齐使 CLIP 更强大",
    "translated_abstract": "在过去两年中，视觉-语言预训练在多个下游任务上取得了令人瞩目的成功。然而，获取高质量的图像-文本配对，其中配对完全互不干扰，仍然是一项具有挑战性的任务，并且常用数据集中存在噪声。为了解决这个问题，我们提出了 SoftCLIP，这是一种新颖的方法，通过引入来自细粒度内模态自相似性生成的“柔性目标”，实现了柔性跨模态对齐。内模态引导能够使得两个配对之间存在一些局部相似性，并且模型之间存在多对多的关系。此外，由于正样本在柔性目标分布中仍然占主导地位，我们通过分离分布中的负样本来进一步提高跨模态学习中的关系对齐性。大量实验表明 SoftCLIP 的有效性。",
    "tldr": "本文提出了一种柔性跨模态对齐的方法 SoftCLIP，在配对任务中取得了很好的效果。"
}