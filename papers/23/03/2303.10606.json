{
    "title": "CTRAN: CNN-Transformer-based Network for Natural Language Understanding. (arXiv:2303.10606v1 [cs.CL])",
    "abstract": "Intent-detection and slot-filling are the two main tasks in natural language understanding. In this study, we propose CTRAN, a novel encoder-decoder CNN-Transformer-based architecture for intent-detection and slot-filling. In the encoder, we use BERT, followed by several convolutional layers, and rearrange the output using window feature sequence. We use stacked Transformer encoders after the window feature sequence. For the intent-detection decoder, we utilize self-attention followed by a linear layer. In the slot-filling decoder, we introduce the aligned Transformer decoder, which utilizes a zero diagonal mask, aligning output tags with input tokens. We apply our network on ATIS and SNIPS, and surpass the current state-of-the-art in slot-filling on both datasets. Furthermore, we incorporate the language model as word embeddings, and show that this strategy yields a better result when compared to the language model as an encoder.",
    "link": "http://arxiv.org/abs/2303.10606",
    "context": "Title: CTRAN: CNN-Transformer-based Network for Natural Language Understanding. (arXiv:2303.10606v1 [cs.CL])\nAbstract: Intent-detection and slot-filling are the two main tasks in natural language understanding. In this study, we propose CTRAN, a novel encoder-decoder CNN-Transformer-based architecture for intent-detection and slot-filling. In the encoder, we use BERT, followed by several convolutional layers, and rearrange the output using window feature sequence. We use stacked Transformer encoders after the window feature sequence. For the intent-detection decoder, we utilize self-attention followed by a linear layer. In the slot-filling decoder, we introduce the aligned Transformer decoder, which utilizes a zero diagonal mask, aligning output tags with input tokens. We apply our network on ATIS and SNIPS, and surpass the current state-of-the-art in slot-filling on both datasets. Furthermore, we incorporate the language model as word embeddings, and show that this strategy yields a better result when compared to the language model as an encoder.",
    "path": "papers/23/03/2303.10606.json",
    "total_tokens": 972,
    "translated_title": "CTRAN：基于CNN-Transformer的自然语言理解网络",
    "translated_abstract": "意图检测和插槽填充是自然语言理解中的两个主要任务。在本研究中，我们提出了CTRAN，一种新的基于CNN-Transformer的编码器-解码器架构，用于意图检测和插槽填充。在编码器中，我们使用BERT，接着几个卷积层，并使用窗口特征序列重新排列输出。我们在窗口特征序列后使用堆叠的Transformer编码器。对于意图检测解码器，我们利用自注意力后跟一个线性层。在插槽填充解码器中，我们介绍了对齐的Transformer解码器，它利用了零对角线掩码，将输出标签与输入标记对齐。我们将我们的网络应用于ATIS和SNIPS，并在两个数据集的插槽填充中超越了当前的最新结果。此外，我们将语言模型作为词嵌入并将其应用于网络中，结果表明这种策略比将语言模型作为编码器更好。",
    "tldr": "CTRAN是一种新颖的基于CNN-Transformer的编码器-解码器架构，用于自然语言理解的意图检测和插槽填充。该网络利用了BERT和多层卷积和Transformer编码器，采用自注意力和对齐的Transformer解码器。该网络在ATIS和SNIPS数据集上的插槽填充任务中超越了当前的最新结果。",
    "en_tdlr": "CTRAN is a novel CNN-Transformer-based encoder-decoder architecture for intent-detection and slot-filling in natural language understanding. The network uses BERT, convolutional and Transformer encoders, self-attention, and an aligned Transformer decoder. The network surpasses the current state-of-the-art in slot-filling on both ATIS and SNIPS datasets."
}