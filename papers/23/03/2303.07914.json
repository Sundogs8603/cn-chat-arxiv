{
    "title": "Adapting Offline Speech Translation Models for Streaming with Future-Aware Distillation and Inference. (arXiv:2303.07914v1 [cs.CL])",
    "abstract": "A popular approach to streaming speech translation is to employ a single offline model with a \\textit{wait-$k$} policy to support different latency requirements, which is simpler than training multiple online models with different latency constraints. However, there is a mismatch problem in using a model trained with complete utterances for streaming inference with partial input. We demonstrate that speech representations extracted at the end of a streaming input are significantly different from those extracted from a complete utterance. To address this issue, we propose a new approach called Future-Aware Streaming Translation (FAST) that adapts an offline ST model for streaming input. FAST includes a Future-Aware Inference (FAI) strategy that incorporates future context through a trainable masked embedding, and a Future-Aware Distillation (FAD) framework that transfers future context from an approximation of full speech to streaming input. Our experiments on the MuST-C EnDe, EnEs, and",
    "link": "http://arxiv.org/abs/2303.07914",
    "context": "Title: Adapting Offline Speech Translation Models for Streaming with Future-Aware Distillation and Inference. (arXiv:2303.07914v1 [cs.CL])\nAbstract: A popular approach to streaming speech translation is to employ a single offline model with a \\textit{wait-$k$} policy to support different latency requirements, which is simpler than training multiple online models with different latency constraints. However, there is a mismatch problem in using a model trained with complete utterances for streaming inference with partial input. We demonstrate that speech representations extracted at the end of a streaming input are significantly different from those extracted from a complete utterance. To address this issue, we propose a new approach called Future-Aware Streaming Translation (FAST) that adapts an offline ST model for streaming input. FAST includes a Future-Aware Inference (FAI) strategy that incorporates future context through a trainable masked embedding, and a Future-Aware Distillation (FAD) framework that transfers future context from an approximation of full speech to streaming input. Our experiments on the MuST-C EnDe, EnEs, and",
    "path": "papers/23/03/2303.07914.json",
    "total_tokens": 981,
    "tldr": "本论文提出了一种称为FAST的方法，用于将离线语音翻译模型适应流式输入，在推理中融合未来上下文，以解决使用完整话语训练的模型进行部分输入的流式推理不匹配的问题。"
}