{
    "title": "Fuzziness-tuned: Improving the Transferability of Adversarial Examples. (arXiv:2303.10078v1 [cs.LG])",
    "abstract": "With the development of adversarial attacks, adversairal examples have been widely used to enhance the robustness of the training models on deep neural networks. Although considerable efforts of adversarial attacks on improving the transferability of adversarial examples have been developed, the attack success rate of the transfer-based attacks on the surrogate model is much higher than that on victim model under the low attack strength (e.g., the attack strength $\\epsilon=8/255$). In this paper, we first systematically investigated this issue and found that the enormous difference of attack success rates between the surrogate model and victim model is caused by the existence of a special area (known as fuzzy domain in our paper), in which the adversarial examples in the area are classified wrongly by the surrogate model while correctly by the victim model. Then, to eliminate such enormous difference of attack success rates for improving the transferability of generated adversarial exa",
    "link": "http://arxiv.org/abs/2303.10078",
    "context": "Title: Fuzziness-tuned: Improving the Transferability of Adversarial Examples. (arXiv:2303.10078v1 [cs.LG])\nAbstract: With the development of adversarial attacks, adversairal examples have been widely used to enhance the robustness of the training models on deep neural networks. Although considerable efforts of adversarial attacks on improving the transferability of adversarial examples have been developed, the attack success rate of the transfer-based attacks on the surrogate model is much higher than that on victim model under the low attack strength (e.g., the attack strength $\\epsilon=8/255$). In this paper, we first systematically investigated this issue and found that the enormous difference of attack success rates between the surrogate model and victim model is caused by the existence of a special area (known as fuzzy domain in our paper), in which the adversarial examples in the area are classified wrongly by the surrogate model while correctly by the victim model. Then, to eliminate such enormous difference of attack success rates for improving the transferability of generated adversarial exa",
    "path": "papers/23/03/2303.10078.json",
    "total_tokens": 1161,
    "translated_title": "模糊调谐：提高对抗样本的可迁移性",
    "translated_abstract": "随着对抗攻击的发展，对抗样本已广泛应用于提高深度神经网络模型的鲁棒性。尽管针对提高对抗样本可迁移性的对抗攻击做出了可观的努力，但在低攻击力度(例如，攻击强度ϵ=8/255)，基于迁移的攻击在代理模型上的攻击成功率远高于受害者模型的攻击成功率。本文首先系统地研究了这个问题，并发现代理模型和受害者模型之间攻击成功率的巨大差异是由一个特殊区域（我们在本文中称之为模糊领域）的存在导致的，该区域中的对抗样本被代理模型错误分类，而被受害者模型正确分类。然后，为了消除这种攻击成功率的巨大差异以提高生成的对抗性样本的可迁移性，我们提出了一种模糊度调谐方法来生成对抗性样本。具体而言，我们引入了一个名为“模糊度”的新参数来控制对抗性样本生成的扰动程度，这可以有效减少模糊领域并促进对抗性样本的可迁移性。多个数据集上的实验结果证明了我们方法相对于现有技术的卓越性能，进一步分析表明，我们的方法可以提高深度神经网络对基于迁移的攻击的鲁棒性。",
    "tldr": "本文提出了一种模糊度调谐方法生成对抗性样本，通过减少模糊领域有效地提高对抗样本的可迁移性，实验证明其相对于现有技术具有卓越性能。",
    "en_tdlr": "This paper proposes a fuzziness-tuned approach for generating adversarial examples, which effectively improves the transferability of adversarial examples by reducing the fuzzy domain through controlling the degree of perturbation. Experimental results demonstrate the superior performance of our approach compared with state-of-the-art methods."
}