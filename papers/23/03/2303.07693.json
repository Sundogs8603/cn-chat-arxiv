{
    "title": "Adaptive Policy Learning for Offline-to-Online Reinforcement Learning. (arXiv:2303.07693v1 [cs.LG])",
    "abstract": "Conventional reinforcement learning (RL) needs an environment to collect fresh data, which is impractical when online interactions are costly. Offline RL provides an alternative solution by directly learning from the previously collected dataset. However, it will yield unsatisfactory performance if the quality of the offline datasets is poor. In this paper, we consider an offline-to-online setting where the agent is first learned from the offline dataset and then trained online, and propose a framework called Adaptive Policy Learning for effectively taking advantage of offline and online data. Specifically, we explicitly consider the difference between the online and offline data and apply an adaptive update scheme accordingly, that is, a pessimistic update strategy for the offline dataset and an optimistic/greedy update scheme for the online dataset. Such a simple and effective method provides a way to mix the offline and online RL and achieve the best of both worlds. We further provi",
    "link": "http://arxiv.org/abs/2303.07693",
    "total_tokens": 988,
    "translated_title": "离线到在线强化学习的自适应策略学习",
    "translated_abstract": "传统强化学习需要一个环境来收集新鲜的数据，但当在线交互成本高昂时不切实际。离线强化学习通过直接从以前收集的数据集中学习提供了一种替代方法。但是，如果离线数据集的质量差，将导致性能不佳。本文考虑了一种离线到在线的场景，在该场景中，代理首先从离线数据集中学习，然后再进行在线训练，并提出了一种名为自适应策略学习的框架，以有效地利用离线和在线数据。具体来说，我们显式考虑了在线和离线数据之间的差异，并相应地应用了自适应更新策略，即对离线数据集采用悲观更新策略，而对在线数据集采用乐观/贪心更新策略。这种简单而有效的方法提供了一种混合离线和在线强化学习并实现两者最佳效果的方法。我们进一步通过理论分析表明，我们的算法实现了亚线性后悔界，与有同时访问离线和在线数据的oracle算法的性能相匹配。",
    "tldr": "本文提出了一种自适应策略学习框架，以有效地利用离线和在线数据，实现了离线到在线强化学习的最佳效果。",
    "en_tdlr": "This paper proposes an adaptive policy learning framework to effectively use both offline and online data for achieving the best performance for offline-to-online reinforcement learning. The approach considers the difference between online and offline data and uses an adaptive update strategy to mix and optimize both. The theoretical analysis shows that the algorithm produces a sublinear regret bound matching the performance of an oracle algorithm that can access both offline and online data."
}