{
    "title": "Multi-modal reward for visual relationships-based image captioning. (arXiv:2303.10766v2 [cs.CV] UPDATED)",
    "abstract": "Deep neural networks have achieved promising results in automatic image captioning due to their effective representation learning and context-based content generation capabilities. As a prominent type of deep features used in many of the recent image captioning methods, the well-known bottomup features provide a detailed representation of different objects of the image in comparison with the feature maps directly extracted from the raw image. However, the lack of high-level semantic information about the relationships between these objects is an important drawback of bottom-up features, despite their expensive and resource-demanding extraction procedure. To take advantage of visual relationships in caption generation, this paper proposes a deep neural network architecture for image captioning based on fusing the visual relationships information extracted from an image's scene graph with the spatial feature maps of the image. A multi-modal reward function is then introduced for deep rei",
    "link": "http://arxiv.org/abs/2303.10766",
    "context": "Title: Multi-modal reward for visual relationships-based image captioning. (arXiv:2303.10766v2 [cs.CV] UPDATED)\nAbstract: Deep neural networks have achieved promising results in automatic image captioning due to their effective representation learning and context-based content generation capabilities. As a prominent type of deep features used in many of the recent image captioning methods, the well-known bottomup features provide a detailed representation of different objects of the image in comparison with the feature maps directly extracted from the raw image. However, the lack of high-level semantic information about the relationships between these objects is an important drawback of bottom-up features, despite their expensive and resource-demanding extraction procedure. To take advantage of visual relationships in caption generation, this paper proposes a deep neural network architecture for image captioning based on fusing the visual relationships information extracted from an image's scene graph with the spatial feature maps of the image. A multi-modal reward function is then introduced for deep rei",
    "path": "papers/23/03/2303.10766.json",
    "total_tokens": 984,
    "translated_title": "基于视觉关系的图像字幕生成中的多模态奖励",
    "translated_abstract": "深度神经网络在自动图像字幕生成中取得了很好的效果，其有效的表示学习和基于上下文的内容生成能力是关键原因。然而，虽然 bottom-up 特征是许多最近图像字幕生成的深度功能的重要组成部分，这些特征与从原始图像直接提取的特征图相比提供了不同对象的详细表示，但是它们关于这些对象之间高层次语义信息的缺乏是它们的重要缺点，尽管其提取过程耗时且需耗费大量资源。为了利用视觉关系在字幕生成中的优势，本文提出了一种深度神经网络架构，它基于提取的图像场景图的视觉关系信息与图像的空间特征映射进行融合，然后引入了一个多模态奖励函数来进行深度强化学习，以优化图像字幕生成任务。实验结果表明，我们提出的方法在客观和主观评估指标上均优于基准。",
    "tldr": "本文提出了一种基于融合图像场景图的视觉关系信息与图像的空间特征映射的深度神经网络，引入多模态奖励函数进行深度强化学习，以优化图像字幕生成任务。实验结果表明，该方法在客观和主观评估指标上均优于基准。"
}