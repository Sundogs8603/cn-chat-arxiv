{
    "title": "$P^{3}O$: Transferring Visual Representations for Reinforcement Learning via Prompting. (arXiv:2303.12371v1 [cs.CV])",
    "abstract": "It is important for deep reinforcement learning (DRL) algorithms to transfer their learned policies to new environments that have different visual inputs. In this paper, we introduce Prompt based Proximal Policy Optimization ($P^{3}O$), a three-stage DRL algorithm that transfers visual representations from a target to a source environment by applying prompting. The process of $P^{3}O$ consists of three stages: pre-training, prompting, and predicting. In particular, we specify a prompt-transformer for representation conversion and propose a two-step training process to train the prompt-transformer for the target environment, while the rest of the DRL pipeline remains unchanged. We implement $P^{3}O$ and evaluate it on the OpenAI CarRacing video game. The experimental results show that $P^{3}O$ outperforms the state-of-the-art visual transferring schemes. In particular, $P^{3}O$ allows the learned policies to perform well in environments with different visual inputs, which is much more e",
    "link": "http://arxiv.org/abs/2303.12371",
    "context": "Title: $P^{3}O$: Transferring Visual Representations for Reinforcement Learning via Prompting. (arXiv:2303.12371v1 [cs.CV])\nAbstract: It is important for deep reinforcement learning (DRL) algorithms to transfer their learned policies to new environments that have different visual inputs. In this paper, we introduce Prompt based Proximal Policy Optimization ($P^{3}O$), a three-stage DRL algorithm that transfers visual representations from a target to a source environment by applying prompting. The process of $P^{3}O$ consists of three stages: pre-training, prompting, and predicting. In particular, we specify a prompt-transformer for representation conversion and propose a two-step training process to train the prompt-transformer for the target environment, while the rest of the DRL pipeline remains unchanged. We implement $P^{3}O$ and evaluate it on the OpenAI CarRacing video game. The experimental results show that $P^{3}O$ outperforms the state-of-the-art visual transferring schemes. In particular, $P^{3}O$ allows the learned policies to perform well in environments with different visual inputs, which is much more e",
    "path": "papers/23/03/2303.12371.json",
    "total_tokens": 882,
    "translated_title": "$P^{3}O$: Prompting方法中用于强化学习的视觉表示迁移",
    "translated_abstract": "在深度强化学习算法中，将学到的策略转移至视觉输入不同的新环境是非常重要的。本文提出了一种基于Prompt的近端策略优化($P^{3}O$)的三阶段DRL算法，通过应用Prompt使得视觉表示从目标环境传递到源环境。$P^{3}O$的过程包括三个阶段:预训练、Prompting和预测。我们提出了一个Prompt-Transformer用于表示转换，并针对目标环境提出了一个两步训练过程，训练Prompt-Transformer，而DRL管道的其余部分保持不变。我们在OpenAI CarRacing视频游戏上实施$P^{3}O$并进行评估。实验结果表明，$P^{3}O$优于最先进的视觉转移方案，而且能让学到的策略在视觉输入不同的环境中表现良好，这是非常重要的。",
    "tldr": "本论文提出一种名为$P^{3}O$的三阶段DRL算法，通过提问来转移视觉表示，显著优于现有的视觉传输方案。",
    "en_tdlr": "This paper proposes a three-stage DRL algorithm called $P^{3}O$, which transfers visual representations through prompting and outperforms existing visual transferring schemes."
}