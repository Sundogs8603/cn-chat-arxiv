{
    "title": "An Extended Study of Human-like Behavior under Adversarial Training. (arXiv:2303.12669v1 [cs.CV])",
    "abstract": "Neural networks have a number of shortcomings. Amongst the severest ones is the sensitivity to distribution shifts which allows models to be easily fooled into wrong predictions by small perturbations to inputs that are often imperceivable to humans and do not have to carry semantic meaning. Adversarial training poses a partial solution to address this issue by training models on worst-case perturbations. Yet, recent work has also pointed out that the reasoning in neural networks is different from humans. Humans identify objects by shape, while neural nets mainly employ texture cues. Exemplarily, a model trained on photographs will likely fail to generalize to datasets containing sketches. Interestingly, it was also shown that adversarial training seems to favorably increase the shift toward shape bias. In this work, we revisit this observation and provide an extensive analysis of this effect on various architectures, the common $\\ell_2$- and $\\ell_\\infty$-training, and Transformer-bas",
    "link": "http://arxiv.org/abs/2303.12669",
    "context": "Title: An Extended Study of Human-like Behavior under Adversarial Training. (arXiv:2303.12669v1 [cs.CV])\nAbstract: Neural networks have a number of shortcomings. Amongst the severest ones is the sensitivity to distribution shifts which allows models to be easily fooled into wrong predictions by small perturbations to inputs that are often imperceivable to humans and do not have to carry semantic meaning. Adversarial training poses a partial solution to address this issue by training models on worst-case perturbations. Yet, recent work has also pointed out that the reasoning in neural networks is different from humans. Humans identify objects by shape, while neural nets mainly employ texture cues. Exemplarily, a model trained on photographs will likely fail to generalize to datasets containing sketches. Interestingly, it was also shown that adversarial training seems to favorably increase the shift toward shape bias. In this work, we revisit this observation and provide an extensive analysis of this effect on various architectures, the common $\\ell_2$- and $\\ell_\\infty$-training, and Transformer-bas",
    "path": "papers/23/03/2303.12669.json",
    "total_tokens": 1120,
    "translated_title": "对抗训练下基于人类行为的研究扩展的实验",
    "translated_abstract": "神经网络存在许多缺陷，其中最严重的之一是对分布偏差的敏感性，这允许模型轻易被小型扰动欺骗并做出错误预测，而这些扰动通常对人类来说不易察觉并不必须具有语义含义。 对抗性训练通过在最坏情况下对模型进行扰动来训练模型来解决这个问题。 然而，最近的工作也指出，神经网络的推理方式不同于人类。 人类通过形状识别对象，而神经网络主要利用纹理线索。 例如，受过照片训练的模型可能无法推广到包含素描的数据集。 有趣的是，还表明对抗性训练似乎有利于增加转向形状偏差的趋势。 本文重新审视了这一观察结果，并就各种架构，常见的$\\ell_2$和$\\ell_\\infty$-training，以及基于Transformer的自然语言处理模型的效应进行了广泛分析。 我们在物体分类和自然语言推理上的实验表明，转向形状偏差的变换不仅限于视觉领域，而且也适用于语言处理。 具体而言，我们观察到对抗性训练导致模型更多地依赖组合结构来识别对象和进行预测，从而具有更接近人类的行为。",
    "tldr": "对抗训练可以使模型更倾向于人类形状识别，而非仅仅利用纹理线索。在数据集中包含素描的情况下，这种变换效果仍然有效，并且在语言处理领域也适用。",
    "en_tdlr": "Adversarial training can shift models towards shape-based representations, which are more human-like compared to texture-based ones. This effect holds even when datasets contain sketches and is also observed in language processing."
}