{
    "title": "Measuring Improvement of F$_1$-Scores in Detection of Self-Admitted Technical Debt. (arXiv:2303.09617v1 [cs.SE])",
    "abstract": "Artificial Intelligence and Machine Learning have witnessed rapid, significant improvements in Natural Language Processing (NLP) tasks. Utilizing Deep Learning, researchers have taken advantage of repository comments in Software Engineering to produce accurate methods for detecting Self-Admitted Technical Debt (SATD) from 20 open-source Java projects' code. In this work, we improve SATD detection with a novel approach that leverages the Bidirectional Encoder Representations from Transformers (BERT) architecture. For comparison, we re-evaluated previous deep learning methods and applied stratified 10-fold cross-validation to report reliable F$_1$-scores. We examine our model in both cross-project and intra-project contexts. For each context, we use re-sampling and duplication as augmentation strategies to account for data imbalance. We find that our trained BERT model improves over the best performance of all previous methods in 19 of the 20 projects in cross-project scenarios. However,",
    "link": "http://arxiv.org/abs/2303.09617",
    "context": "Title: Measuring Improvement of F$_1$-Scores in Detection of Self-Admitted Technical Debt. (arXiv:2303.09617v1 [cs.SE])\nAbstract: Artificial Intelligence and Machine Learning have witnessed rapid, significant improvements in Natural Language Processing (NLP) tasks. Utilizing Deep Learning, researchers have taken advantage of repository comments in Software Engineering to produce accurate methods for detecting Self-Admitted Technical Debt (SATD) from 20 open-source Java projects' code. In this work, we improve SATD detection with a novel approach that leverages the Bidirectional Encoder Representations from Transformers (BERT) architecture. For comparison, we re-evaluated previous deep learning methods and applied stratified 10-fold cross-validation to report reliable F$_1$-scores. We examine our model in both cross-project and intra-project contexts. For each context, we use re-sampling and duplication as augmentation strategies to account for data imbalance. We find that our trained BERT model improves over the best performance of all previous methods in 19 of the 20 projects in cross-project scenarios. However,",
    "path": "papers/23/03/2303.09617.json",
    "total_tokens": 863,
    "translated_title": "论检测自我承认技术债务F$_1$-得分的改进方法的研究",
    "translated_abstract": "人工智能和机器学习在自然语言处理（NLP）任务中取得了快速而显著的进展。利用深度学习，研究人员利用软件工程中的版本库注释，提出了一种准确检测20个开源Java项目代码中自我承认技术债务（SATD）的方法。本文提出了一种新的方法，利用转换器中的双向编码器表示（BERT）架构改进SATD检测。为了进行比较，我们重新评估了以前的深度学习方法，并应用分层的10折交叉验证来报告可靠的F$_{1}$-得分。我们在跨项目和内部项目上检验了我们的模型。针对每个上下文，我们使用重新抽样和复制作为增广策略，以考虑数据不平衡的影响。我们发现，我们的训练BERT模型在19个项目的跨项目情况下优于所有先前方法的最佳表现。",
    "tldr": "本研究提出了一种利用BERT架构改进SATD检测的方法，在跨项目情况下表现优于所有先前方法的最佳表现。",
    "en_tdlr": "This research proposes a novel approach utilizing BERT architecture to improve SATD detection and achieves better performance than all previous methods in cross-project scenarios."
}