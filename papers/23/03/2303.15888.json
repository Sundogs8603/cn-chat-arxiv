{
    "title": "Projected Latent Distillation for Data-Agnostic Consolidation in Distributed Continual Learning. (arXiv:2303.15888v1 [cs.LG])",
    "abstract": "Distributed learning on the edge often comprises self-centered devices (SCD) which learn local tasks independently and are unwilling to contribute to the performance of other SDCs. How do we achieve forward transfer at zero cost for the single SCDs? We formalize this problem as a Distributed Continual Learning scenario, where SCD adapt to local tasks and a CL model consolidates the knowledge from the resulting stream of models without looking at the SCD's private data. Unfortunately, current CL methods are not directly applicable to this scenario. We propose Data-Agnostic Consolidation (DAC), a novel double knowledge distillation method that consolidates the stream of SC models without using the original data. DAC performs distillation in the latent space via a novel Projected Latent Distillation loss. Experimental results show that DAC enables forward transfer between SCDs and reaches state-of-the-art accuracy on Split CIFAR100, CORe50 and Split TinyImageNet, both in reharsal-free and",
    "link": "http://arxiv.org/abs/2303.15888",
    "context": "Title: Projected Latent Distillation for Data-Agnostic Consolidation in Distributed Continual Learning. (arXiv:2303.15888v1 [cs.LG])\nAbstract: Distributed learning on the edge often comprises self-centered devices (SCD) which learn local tasks independently and are unwilling to contribute to the performance of other SDCs. How do we achieve forward transfer at zero cost for the single SCDs? We formalize this problem as a Distributed Continual Learning scenario, where SCD adapt to local tasks and a CL model consolidates the knowledge from the resulting stream of models without looking at the SCD's private data. Unfortunately, current CL methods are not directly applicable to this scenario. We propose Data-Agnostic Consolidation (DAC), a novel double knowledge distillation method that consolidates the stream of SC models without using the original data. DAC performs distillation in the latent space via a novel Projected Latent Distillation loss. Experimental results show that DAC enables forward transfer between SCDs and reaches state-of-the-art accuracy on Split CIFAR100, CORe50 and Split TinyImageNet, both in reharsal-free and",
    "path": "papers/23/03/2303.15888.json",
    "total_tokens": 987,
    "translated_title": "无需数据的分布式连续学习中的投影潜空间蒸馏",
    "translated_abstract": "边缘分布式学习通常由自我中心的设备（SCD）组成，它们独立学习本地任务并不愿意为其他SCD的性能做出贡献。我们如何以零成本实现单个SCD的前向转移？我们将这个问题形式化为分布式连续学习场景，在这个场景中，SCD适应本地任务，CL模型将由这些模型产生的知识合并而无需查看SCD的私有数据。不幸的是，目前的CL方法并不直接适用于这种情况。我们提出了一个新的双重知识蒸馏方法Data-Agnostic Consolidation（DAC），该方法无需使用原始数据即可合并SC模型的流。DAC通过一种新的投影潜空间蒸馏损失在潜空间中执行蒸馏。实验结果表明，DAC使SCD之间的前向转移成为可能，并在Split CIFAR100，CORe50和Split TinyImageNet上达到了最先进的准确性，无需排练。",
    "tldr": "该论文提出了一种双重知识蒸馏方法Data-Agnostic Consolidation（DAC），在不使用原始数据的情况下，通过一种新的投影潜空间蒸馏损失，在分布式连续学习中实现了SCD之间的前向转移并取得了最先进的准确性。",
    "en_tdlr": "This paper proposes a novel double knowledge distillation method, Data-Agnostic Consolidation (DAC), which consolidates the stream of SC models without using the original data via a new Projected Latent Distillation loss. DAC enables forward transfer between SCDs and achieves state-of-the-art accuracy in distributed continual learning without rehearsal on Split CIFAR100, CORe50, and Split TinyImageNet."
}