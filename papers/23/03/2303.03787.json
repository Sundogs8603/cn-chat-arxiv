{
    "title": "Sample-efficient Real-time Planning with Curiosity Cross-Entropy Method and Contrastive Learning. (arXiv:2303.03787v2 [cs.LG] UPDATED)",
    "abstract": "Model-based reinforcement learning (MBRL) with real-time planning has shown great potential in locomotion and manipulation control tasks. However, the existing planning methods, such as the Cross-Entropy Method (CEM), do not scale well to complex high-dimensional environments. One of the key reasons for underperformance is the lack of exploration, as these planning methods only aim to maximize the cumulative extrinsic reward over the planning horizon. Furthermore, planning inside the compact latent space in the absence of observations makes it challenging to use curiosity-based intrinsic motivation. We propose Curiosity CEM (CCEM), an improved version of the CEM algorithm for encouraging exploration via curiosity. Our proposed method maximizes the sum of state-action Q values over the planning horizon, in which these Q values estimate the future extrinsic and intrinsic reward, hence encouraging reaching novel observations. In addition, our model uses contrastive representation learning",
    "link": "http://arxiv.org/abs/2303.03787",
    "context": "Title: Sample-efficient Real-time Planning with Curiosity Cross-Entropy Method and Contrastive Learning. (arXiv:2303.03787v2 [cs.LG] UPDATED)\nAbstract: Model-based reinforcement learning (MBRL) with real-time planning has shown great potential in locomotion and manipulation control tasks. However, the existing planning methods, such as the Cross-Entropy Method (CEM), do not scale well to complex high-dimensional environments. One of the key reasons for underperformance is the lack of exploration, as these planning methods only aim to maximize the cumulative extrinsic reward over the planning horizon. Furthermore, planning inside the compact latent space in the absence of observations makes it challenging to use curiosity-based intrinsic motivation. We propose Curiosity CEM (CCEM), an improved version of the CEM algorithm for encouraging exploration via curiosity. Our proposed method maximizes the sum of state-action Q values over the planning horizon, in which these Q values estimate the future extrinsic and intrinsic reward, hence encouraging reaching novel observations. In addition, our model uses contrastive representation learning",
    "path": "papers/23/03/2303.03787.json",
    "total_tokens": 972,
    "translated_title": "用好奇心交叉熵方法和对比学习进行的样本高效实时规划",
    "translated_abstract": "基于模型的强化学习（MBRL）与实时规划在运动和操作控制任务中显示出巨大潜力。然而，现有的规划方法，如交叉熵方法（CEM），在复杂的高维环境中无法很好地扩展。导致性能不佳的一个关键原因是缺乏探索，因为这些规划方法只旨在最大化规划视角内的累积外在奖励。此外，在没有观察到的紧凑潜在空间内进行规划使得使用基于好奇心的内在动机变得具有挑战性。我们提出了好奇心交叉熵方法（CCEM），这是对CEM算法的改进版本，用于通过好奇心鼓励探索。我们的方法在规划视角内最大化状态-动作Q值的和，其中这些Q值估计未来的外在和内在奖励，从而鼓励达到新颖的观察。此外，我们的模型使用对比表示学习。",
    "tldr": "本文提出了一种用好奇心触发探索的改进版本的交叉熵方法（CCEM），以解决现有规划方法在复杂高维环境下无法扩展的问题。通过最大化规划视角内状态-动作Q值的和，我们的方法鼓励达到新颖的观察，并利用对比表示学习提高性能。",
    "en_tdlr": "This paper proposes an improved version of the Cross-Entropy Method (CEM) called Curiosity CEM (CCEM), which addresses the scalability issue of existing planning methods in complex high-dimensional environments. By maximizing the sum of state-action Q values, our method encourages exploration and leverages contrastive representation learning to enhance performance."
}