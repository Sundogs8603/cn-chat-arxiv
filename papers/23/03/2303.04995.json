{
    "title": "Text-Visual Prompting for Efficient 2D Temporal Video Grounding. (arXiv:2303.04995v2 [cs.CV] UPDATED)",
    "abstract": "In this paper, we study the problem of temporal video grounding (TVG), which aims to predict the starting/ending time points of moments described by a text sentence within a long untrimmed video. Benefiting from fine-grained 3D visual features, the TVG techniques have achieved remarkable progress in recent years. However, the high complexity of 3D convolutional neural networks (CNNs) makes extracting dense 3D visual features time-consuming, which calls for intensive memory and computing resources. Towards efficient TVG, we propose a novel text-visual prompting (TVP) framework, which incorporates optimized perturbation patterns (that we call 'prompts') into both visual inputs and textual features of a TVG model. In sharp contrast to 3D CNNs, we show that TVP allows us to effectively co-train vision encoder and language encoder in a 2D TVG model and improves the performance of crossmodal feature fusion using only low-complexity sparse 2D visual features. Further, we propose a Temporal-Di",
    "link": "http://arxiv.org/abs/2303.04995",
    "context": "Title: Text-Visual Prompting for Efficient 2D Temporal Video Grounding. (arXiv:2303.04995v2 [cs.CV] UPDATED)\nAbstract: In this paper, we study the problem of temporal video grounding (TVG), which aims to predict the starting/ending time points of moments described by a text sentence within a long untrimmed video. Benefiting from fine-grained 3D visual features, the TVG techniques have achieved remarkable progress in recent years. However, the high complexity of 3D convolutional neural networks (CNNs) makes extracting dense 3D visual features time-consuming, which calls for intensive memory and computing resources. Towards efficient TVG, we propose a novel text-visual prompting (TVP) framework, which incorporates optimized perturbation patterns (that we call 'prompts') into both visual inputs and textual features of a TVG model. In sharp contrast to 3D CNNs, we show that TVP allows us to effectively co-train vision encoder and language encoder in a 2D TVG model and improves the performance of crossmodal feature fusion using only low-complexity sparse 2D visual features. Further, we propose a Temporal-Di",
    "path": "papers/23/03/2303.04995.json",
    "total_tokens": 1197,
    "translated_title": "文本-视觉提示用于高效的二维时间视频定位",
    "translated_abstract": "本文研究了时间视频定位（TVG）的问题，旨在预测文字描述的时刻在长时间未修剪的视频中的开始/结束时间点。受益于细粒度的三维视觉特征，TVG技术在近年来取得了显着进展。然而，三维卷积神经网络（CNNs）的高复杂性使得提取密集的三维视觉特征是耗时的，这需要大量的内存和计算资源。针对高效的TVG，作者提出了一种新颖的文本-视觉提示（TVP）框架，并将优化的扰动模式（称为“提示”）结合到TVG模型的视觉输入和文本特征中。与三维CNN形成鲜明对比，作者表明TVP允许我们在二维TVG模型中有效地共同训练视觉编码器和语言编码器，并使用只有低复杂度稀疏二维视觉特征来提高跨模态特征融合的性能。此外，作者提出了一种时态对话排名（TDR）训练策略，以监督TVP的学习并促进视频段与相应文本查询的对齐。在两个基准数据集上的实验结果验证了TVP模型的有效性和高效性。",
    "tldr": "本文提出了一种新颖的文本-视觉提示（TVP）框架来解决时间视频定位问题，该框架有效地共同训练视觉编码器和语言编码器，且使用只有低复杂度稀疏二维视觉特征来提高跨模态特征融合的性能，并提出了一种时态对话排名（TDR）训练策略用于监督TVP的学习，实验证明该框架有效且高效。",
    "en_tdlr": "This paper proposes a novel text-visual prompting (TVP) framework for efficient 2D temporal video grounding, which allows us to effectively co-train vision encoder and language encoder in a 2D model using only low-complexity sparse 2D visual features. The proposed Temporal-Dialogue-Ranking (TDR) training strategy facilitates the alignment of the video segments and the corresponding text query. The experiment results validate the efficiency and effectiveness of the TVP model."
}