{
    "title": "Exploiting Multiple Abstractions in Episodic RL via Reward Shaping. (arXiv:2303.00516v2 [cs.LG] UPDATED)",
    "abstract": "One major limitation to the applicability of Reinforcement Learning (RL) to many practical domains is the large number of samples required to learn an optimal policy. To address this problem and improve learning efficiency, we consider a linear hierarchy of abstraction layers of the Markov Decision Process (MDP) underlying the target domain. Each layer is an MDP representing a coarser model of the one immediately below in the hierarchy. In this work, we propose a novel form of Reward Shaping where the solution obtained at the abstract level is used to offer rewards to the more concrete MDP, in such a way that the abstract solution guides the learning in the more complex domain. In contrast with other works in Hierarchical RL, our technique has few requirements in the design of the abstract models and it is also tolerant to modeling errors, thus making the proposed approach practical. We formally analyze the relationship between the abstract models and the exploration heuristic induced ",
    "link": "http://arxiv.org/abs/2303.00516",
    "context": "Title: Exploiting Multiple Abstractions in Episodic RL via Reward Shaping. (arXiv:2303.00516v2 [cs.LG] UPDATED)\nAbstract: One major limitation to the applicability of Reinforcement Learning (RL) to many practical domains is the large number of samples required to learn an optimal policy. To address this problem and improve learning efficiency, we consider a linear hierarchy of abstraction layers of the Markov Decision Process (MDP) underlying the target domain. Each layer is an MDP representing a coarser model of the one immediately below in the hierarchy. In this work, we propose a novel form of Reward Shaping where the solution obtained at the abstract level is used to offer rewards to the more concrete MDP, in such a way that the abstract solution guides the learning in the more complex domain. In contrast with other works in Hierarchical RL, our technique has few requirements in the design of the abstract models and it is also tolerant to modeling errors, thus making the proposed approach practical. We formally analyze the relationship between the abstract models and the exploration heuristic induced ",
    "path": "papers/23/03/2303.00516.json",
    "total_tokens": 907,
    "translated_title": "通过奖励塑形在基于情节的RL中利用多重抽象",
    "translated_abstract": "强化学习（RL）在许多实际领域应用的一个主要限制是需要大量的样本来学习最优策略。为了解决这个问题并提高学习效率，我们考虑了基于目标领域的马尔可夫决策过程（MDP）的线性层次的抽象层次。每个层次都是一个表示比层次结构下方即刻模型更粗糙的模型的MDP。在这项工作中，我们提出了一种新颖的奖励塑形形式，其中在抽象层面获得的解决方案用于向更具体的MDP提供奖励，以使抽象解决方案指导更复杂领域中的学习。与层次RL中的其他工作相比，我们的技术在抽象模型的设计方面有很少的要求，并且也对建模错误具有容忍性，从而使所提出的方法变得实用。我们正式分析了抽象模型与引发探索启发式的关系",
    "tldr": "在这项工作中，我们通过引入一种新颖的奖励塑形形式，利用多层次的抽象来改善强化学习的效率，并且对抽象模型的设计要求较少，具有容忍性。"
}