{
    "title": "Distributed Graph Embedding with Information-Oriented Random Walks. (arXiv:2303.15702v1 [cs.DC])",
    "abstract": "Graph embedding maps graph nodes to low-dimensional vectors, and is widely adopted in machine learning tasks. The increasing availability of billion-edge graphs underscores the importance of learning efficient and effective embeddings on large graphs, such as link prediction on Twitter with over one billion edges. Most existing graph embedding methods fall short of reaching high data scalability. In this paper, we present a general-purpose, distributed, information-centric random walk-based graph embedding framework, DistGER, which can scale to embed billion-edge graphs. DistGER incrementally computes information-centric random walks. It further leverages a multi-proximity-aware, streaming, parallel graph partitioning strategy, simultaneously achieving high local partition quality and excellent workload balancing across machines. DistGER also improves the distributed Skip-Gram learning model to generate node embeddings by optimizing the access locality, CPU throughput, and synchronizat",
    "link": "http://arxiv.org/abs/2303.15702",
    "context": "Title: Distributed Graph Embedding with Information-Oriented Random Walks. (arXiv:2303.15702v1 [cs.DC])\nAbstract: Graph embedding maps graph nodes to low-dimensional vectors, and is widely adopted in machine learning tasks. The increasing availability of billion-edge graphs underscores the importance of learning efficient and effective embeddings on large graphs, such as link prediction on Twitter with over one billion edges. Most existing graph embedding methods fall short of reaching high data scalability. In this paper, we present a general-purpose, distributed, information-centric random walk-based graph embedding framework, DistGER, which can scale to embed billion-edge graphs. DistGER incrementally computes information-centric random walks. It further leverages a multi-proximity-aware, streaming, parallel graph partitioning strategy, simultaneously achieving high local partition quality and excellent workload balancing across machines. DistGER also improves the distributed Skip-Gram learning model to generate node embeddings by optimizing the access locality, CPU throughput, and synchronizat",
    "path": "papers/23/03/2303.15702.json",
    "total_tokens": 781,
    "translated_title": "基于信息导向随机游走的分布式图嵌入方法",
    "translated_abstract": "图嵌入将图节点映射到低维向量中，被广泛应用于机器学习任务中。本文提出了一种分布式的、基于信息导向随机游走的通用图嵌入框架DistGER，可用于处理十亿级别的图嵌入。该方法通过增量式地计算信息导向的随机游走，并利用了多种邻近感知、流式、并行的图划分策略，实现了在不同机器上既高质量的本地划分，又出色的负载均衡。同时DistGER改进了分布式Skip-Gram学习模型以生成节点嵌入，并优化了在分布式环境中的访问局部性、CPU吞吐量和同步效率。",
    "tldr": "本文提出了一种名为DistGER的分布式图嵌入方法，基于信息导向随机游走策略，利用多种优化技术实现了高效的十亿级别图嵌入。",
    "en_tdlr": "The paper presents a distributed graph embedding method named DistGER, using information-oriented random walks and various optimization techniques, to achieve efficient embedding on billion-edge graphs."
}