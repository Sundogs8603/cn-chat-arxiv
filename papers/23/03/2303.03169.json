{
    "title": "A Unified Algebraic Perspective on Lipschitz Neural Networks. (arXiv:2303.03169v2 [cs.LG] UPDATED)",
    "abstract": "Important research efforts have focused on the design and training of neural networks with a controlled Lipschitz constant. The goal is to increase and sometimes guarantee the robustness against adversarial attacks. Recent promising techniques draw inspirations from different backgrounds to design 1-Lipschitz neural networks, just to name a few: convex potential layers derive from the discretization of continuous dynamical systems, Almost-Orthogonal-Layer proposes a tailored method for matrix rescaling. However, it is today important to consider the recent and promising contributions in the field under a common theoretical lens to better design new and improved layers. This paper introduces a novel algebraic perspective unifying various types of 1-Lipschitz neural networks, including the ones previously mentioned, along with methods based on orthogonality and spectral methods. Interestingly, we show that many existing techniques can be derived and generalized via finding analytical sol",
    "link": "http://arxiv.org/abs/2303.03169",
    "context": "Title: A Unified Algebraic Perspective on Lipschitz Neural Networks. (arXiv:2303.03169v2 [cs.LG] UPDATED)\nAbstract: Important research efforts have focused on the design and training of neural networks with a controlled Lipschitz constant. The goal is to increase and sometimes guarantee the robustness against adversarial attacks. Recent promising techniques draw inspirations from different backgrounds to design 1-Lipschitz neural networks, just to name a few: convex potential layers derive from the discretization of continuous dynamical systems, Almost-Orthogonal-Layer proposes a tailored method for matrix rescaling. However, it is today important to consider the recent and promising contributions in the field under a common theoretical lens to better design new and improved layers. This paper introduces a novel algebraic perspective unifying various types of 1-Lipschitz neural networks, including the ones previously mentioned, along with methods based on orthogonality and spectral methods. Interestingly, we show that many existing techniques can be derived and generalized via finding analytical sol",
    "path": "papers/23/03/2303.03169.json",
    "total_tokens": 877,
    "translated_title": "关于Lipschitz神经网络的统一代数视角",
    "translated_abstract": "这项研究致力于设计和训练具有控制的Lipschitz常数的神经网络。目标是提高并有时保证对抗攻击的鲁棒性。最近的一些有希望的技术从不同的背景中汲取灵感来设计1-Lipschitz神经网络，比如凸潜在层从连续动力系统的离散化中得出，近正交层则提出了一种定制的矩阵重新缩放方法。然而，现在重要的是在通用的理论视角下考虑该领域的最新和有希望的贡献，以更好地设计新的和改进的层次。本文引入了一种新的代数视角，统一了各种类型的1-Lipschitz神经网络，包括之前提到的方法，以及基于正交性和谱方法的方法。有趣的是，我们展示了许多现有的技术可以通过找到解析解来推导和推广。",
    "tldr": "本文介绍了一种统一的代数视角，融合了各种类型的1-Lipschitz神经网络，包括凸潜在层和近正交层，并利用解析解推导和推广了许多现有的技术。",
    "en_tdlr": "This paper presents a unified algebraic perspective that integrates various types of 1-Lipschitz neural networks, including convex potential layers and almost-orthogonal layers, and derives and generalizes existing techniques through analytical solutions."
}