{
    "title": "NoiseCAM: Explainable AI for the Boundary Between Noise and Adversarial Attacks. (arXiv:2303.06151v1 [cs.LG])",
    "abstract": "Deep Learning (DL) and Deep Neural Networks (DNNs) are widely used in various domains. However, adversarial attacks can easily mislead a neural network and lead to wrong decisions. Defense mechanisms are highly preferred in safety-critical applications. In this paper, firstly, we use the gradient class activation map (GradCAM) to analyze the behavior deviation of the VGG-16 network when its inputs are mixed with adversarial perturbation or Gaussian noise. In particular, our method can locate vulnerable layers that are sensitive to adversarial perturbation and Gaussian noise. We also show that the behavior deviation of vulnerable layers can be used to detect adversarial examples. Secondly, we propose a novel NoiseCAM algorithm that integrates information from globally and pixel-level weighted class activation maps. Our algorithm is susceptible to adversarial perturbations and will not respond to Gaussian random noise mixed in the inputs. Third, we compare detecting adversarial examples ",
    "link": "http://arxiv.org/abs/2303.06151",
    "total_tokens": 880,
    "translated_title": "NoiseCAM: 用于噪声和对抗攻击边界的可解释人工智能",
    "translated_abstract": "深度学习和深度神经网络在各个领域得到了广泛应用。然而，对抗攻击很容易误导神经网络并导致错误决策。在安全关键应用中高度需要防御机制。本文首先使用梯度类激活映射（GradCAM）分析VGG-16网络在其输入与对抗扰动或高斯噪声混合时的行为偏差。特别地，我们的方法可以定位对抗扰动和高斯噪声敏感的易受攻击层。我们还展示易受攻击层的行为偏差可以用于检测对抗性示例。其次，我们提出了一种新的NoiseCAM算法，该算法集成了全局和像素级加权类激活映射的信息。我们的算法容易受到对抗扰动的影响，不会对混入输入的高斯随机噪声做出反应。第三，我们比较了检测对抗性示例的方法。",
    "tldr": "本文提出了一种名为NoiseCAM的算法，该算法可以定位易受攻击层并检测对抗性示例，同时不会对混入输入的高斯随机噪声做出反应。",
    "en_tldr": "This paper proposes a NoiseCAM algorithm that can locate vulnerable layers and detect adversarial examples, while not responding to Gaussian random noise mixed in the inputs."
}