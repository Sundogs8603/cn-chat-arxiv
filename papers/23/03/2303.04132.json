{
    "title": "Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction. (arXiv:2303.04132v2 [cs.CL] UPDATED)",
    "abstract": "Large language models (LLMs) have great potential for synthetic data generation. This work shows that useful data can be synthetically generated even for tasks that cannot be solved directly by LLMs: for problems with structured outputs, it is possible to prompt an LLM to perform the task in the reverse direction, by generating plausible input text for a target output structure. Leveraging this asymmetry in task difficulty makes it possible to produce large-scale, high-quality data for complex tasks. We demonstrate the effectiveness of this approach on closed information extraction, where collecting ground-truth data is challenging, and no satisfactory dataset exists to date. We synthetically generate a dataset of 1.8M data points, establish its superior quality compared to existing datasets in a human evaluation, and use it to finetune small models (220M and 770M parameters), termed SynthIE, that outperform the prior state of the art (with equal model size) by a substantial margin of ",
    "link": "http://arxiv.org/abs/2303.04132",
    "context": "Title: Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction. (arXiv:2303.04132v2 [cs.CL] UPDATED)\nAbstract: Large language models (LLMs) have great potential for synthetic data generation. This work shows that useful data can be synthetically generated even for tasks that cannot be solved directly by LLMs: for problems with structured outputs, it is possible to prompt an LLM to perform the task in the reverse direction, by generating plausible input text for a target output structure. Leveraging this asymmetry in task difficulty makes it possible to produce large-scale, high-quality data for complex tasks. We demonstrate the effectiveness of this approach on closed information extraction, where collecting ground-truth data is challenging, and no satisfactory dataset exists to date. We synthetically generate a dataset of 1.8M data points, establish its superior quality compared to existing datasets in a human evaluation, and use it to finetune small models (220M and 770M parameters), termed SynthIE, that outperform the prior state of the art (with equal model size) by a substantial margin of ",
    "path": "papers/23/03/2303.04132.json",
    "total_tokens": 960,
    "translated_title": "利用不对称性进行合成训练数据生成：SynthIE和信息提取案例",
    "translated_abstract": "大型语言模型（LLM）在合成数据生成方面有着巨大的潜力。这项工作表明，即使对于LLM无法直接解决的任务，也可以合成生成有用的数据：对于具有结构化输出的问题，可以提示LLM在反向方向上执行任务，通过为目标输出结构生成合理的输入文本。利用任务困难度的不对称性，可以生成大规模、高质量的复杂任务数据。我们在封闭信息提取方面展示了这种方法的有效性，该领域难以收集到真实数据，至今没有令人满意的数据集存在。我们合成生成了一个包含180万数据点的数据集，并在人工评估中证明其与现有数据集相比具有更好的质量，并利用该数据集对小型模型（220M和770M参数）进行微调，这些模型被称为SynthIE，以远远超过先前领先技术的水平（具有相同的模型大小）。",
    "tldr": "本文展示了利用不对称性进行合成训练数据生成的方法，可以针对结构化输出问题产生大规模、高质量的数据。在封闭信息提取任务中，我们合成生成了一个包含180万数据点的数据集，并利用此数据集对小型模型进行微调，取得了远超先前领先技术的成果。",
    "en_tdlr": "This study demonstrates a method for synthetic data generation by leveraging asymmetry, which can produce large-scale and high-quality data for problems with structured outputs. In the case of closed information extraction, a dataset of 1.8M data points was synthetically generated, and using this dataset to fine-tune small models led to substantial improvements over prior state-of-the-art approaches."
}