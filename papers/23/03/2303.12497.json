{
    "title": "Lower Bound on the Bayesian Risk via Information Measure. (arXiv:2303.12497v1 [cs.IT])",
    "abstract": "This paper focuses on parameter estimation and introduces a new method for lower bounding the Bayesian risk. The method allows for the use of virtually \\emph{any} information measure, including R\\'enyi's $\\alpha$, $\\varphi$-Divergences, and Sibson's $\\alpha$-Mutual Information. The approach considers divergences as functionals of measures and exploits the duality between spaces of measures and spaces of functions. In particular, we show that one can lower bound the risk with any information measure by upper bounding its dual via Markov's inequality. We are thus able to provide estimator-independent impossibility results thanks to the Data-Processing Inequalities that divergences satisfy. The results are then applied to settings of interest involving both discrete and continuous parameters, including the ``Hide-and-Seek'' problem, and compared to the state-of-the-art techniques. An important observation is that the behaviour of the lower bound in the number of samples is influenced by t",
    "link": "http://arxiv.org/abs/2303.12497",
    "context": "Title: Lower Bound on the Bayesian Risk via Information Measure. (arXiv:2303.12497v1 [cs.IT])\nAbstract: This paper focuses on parameter estimation and introduces a new method for lower bounding the Bayesian risk. The method allows for the use of virtually \\emph{any} information measure, including R\\'enyi's $\\alpha$, $\\varphi$-Divergences, and Sibson's $\\alpha$-Mutual Information. The approach considers divergences as functionals of measures and exploits the duality between spaces of measures and spaces of functions. In particular, we show that one can lower bound the risk with any information measure by upper bounding its dual via Markov's inequality. We are thus able to provide estimator-independent impossibility results thanks to the Data-Processing Inequalities that divergences satisfy. The results are then applied to settings of interest involving both discrete and continuous parameters, including the ``Hide-and-Seek'' problem, and compared to the state-of-the-art techniques. An important observation is that the behaviour of the lower bound in the number of samples is influenced by t",
    "path": "papers/23/03/2303.12497.json",
    "total_tokens": 924,
    "translated_title": "通过信息度量的下界贝叶斯风险",
    "translated_abstract": "本文关注参数估计，介绍了一种新的方法来计算贝叶斯风险下界。该方法允许使用几乎任何信息度量，包括Rényi的α，φ-分歧和Sibson的α-互信息。该 方法将分歧视为度量的函数，并利用度量空间和函数空间之间的对偶性。特别地，我们展示了通过马尔可夫不等式对其对偶进行上界限制，就可以用任何信息度量计算风险的下界。因此，由于分歧满足数据处理不等式，我们能够提供与估计器无关的不可能结果。然后将这些结果应用于涉及离散和连续参数的有趣问题，包括“捉迷藏”问题，并与最先进的技术进行比较。重要的观察是下界在样本数上的行为受到t的影响。",
    "tldr": "新提出一种方法计算贝叶斯风险下界，允许使用几乎任何信息度量，能提供与估计器无关的不可能结果。已应用于离散和连续参数问题，与最先进的技术进行了比较。",
    "en_tdlr": "This paper proposes a new method for computing the lower bound of Bayesian risk using virtually any information measure. The approach considers divergences as functionals of measures and exploits the duality between spaces of measures and spaces of functions to bound the risk. The method has been successfully applied to both discrete and continuous parameter settings, including the \"Hide-and-Seek\" problem. The important observation is that the behavior of the lower bound in the number of samples is influenced by t."
}