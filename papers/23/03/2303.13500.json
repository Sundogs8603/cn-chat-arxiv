{
    "title": "A Closer Look at Model Adaptation using Feature Distortion and Simplicity Bias. (arXiv:2303.13500v1 [cs.LG])",
    "abstract": "Advances in the expressivity of pretrained models have increased interest in the design of adaptation protocols which enable safe and effective transfer learning. Going beyond conventional linear probing (LP) and fine tuning (FT) strategies, protocols that can effectively control feature distortion, i.e., the failure to update features orthogonal to the in-distribution, have been found to achieve improved out-of-distribution generalization (OOD). In order to limit this distortion, the LP+FT protocol, which first learns a linear probe and then uses this initialization for subsequent FT, was proposed. However, in this paper, we find when adaptation protocols (LP, FT, LP+FT) are also evaluated on a variety of safety objectives (e.g., calibration, robustness, etc.), a complementary perspective to feature distortion is helpful to explain protocol behavior. To this end, we study the susceptibility of protocols to simplicity bias (SB), i.e. the well-known propensity of deep neural networks to",
    "link": "http://arxiv.org/abs/2303.13500",
    "context": "Title: A Closer Look at Model Adaptation using Feature Distortion and Simplicity Bias. (arXiv:2303.13500v1 [cs.LG])\nAbstract: Advances in the expressivity of pretrained models have increased interest in the design of adaptation protocols which enable safe and effective transfer learning. Going beyond conventional linear probing (LP) and fine tuning (FT) strategies, protocols that can effectively control feature distortion, i.e., the failure to update features orthogonal to the in-distribution, have been found to achieve improved out-of-distribution generalization (OOD). In order to limit this distortion, the LP+FT protocol, which first learns a linear probe and then uses this initialization for subsequent FT, was proposed. However, in this paper, we find when adaptation protocols (LP, FT, LP+FT) are also evaluated on a variety of safety objectives (e.g., calibration, robustness, etc.), a complementary perspective to feature distortion is helpful to explain protocol behavior. To this end, we study the susceptibility of protocols to simplicity bias (SB), i.e. the well-known propensity of deep neural networks to",
    "path": "papers/23/03/2303.13500.json",
    "total_tokens": 1114,
    "translated_title": "通过特征扭曲和简单性偏差来适应模型的更深入研究",
    "translated_abstract": "预训练模型表达能力的提高增加了对启用安全有效的迁移学习的适应协议设计的兴趣。在传统的线性探测（LP）和微调（FT）策略之外，发现可以有效控制特征扭曲（即无法更新正交于分布内部的特征）的协议可以实现改进的越界泛化（OOD）。为了限制这种扭曲，提出了LP+FT协议，该协议首先学习线性探测，然后使用此初始化进行后续FT。但是，在本文中，我们发现当适应协议（LP、FT、LP+FT）也在多种安全目标（例如校准、鲁棒性等）上进行评估时，对特征扭曲的互补视角有助于解释协议行为。为此，我们研究了协议对简单性偏见（SB）的易感性，即深度神经网络过度拟合数据中简单模式的倾向，并提出了一种新的受控简单性正则化（CSR）方法，该方法鼓励对更复杂的特征进行泛化并限制过度拟合。我们的实验证明，CSR可以有效地补充旨在限制特征扭曲的协议，从而实现更强的OOD泛化和改进的安全性。",
    "tldr": "研究对深度神经网络过度拟合数据中简单模式的倾向的易感性，提出了一种受控简单性正则化（CSR）方法来限制过度拟合，完成对模型的更深入研究，提高了模型的泛化能力和安全性。",
    "en_tdlr": "This paper proposes a controlled simplicity regularization method (CSR) to limit overfitting and improve the generalization ability and safety of models by studying the susceptibility of deep neural networks to overfitting simple patterns in data, providing an in-depth understanding of model adaptation using feature distortion and simplicity bias."
}