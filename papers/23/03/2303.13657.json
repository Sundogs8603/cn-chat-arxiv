{
    "title": "Policy Evaluation in Distributional LQR. (arXiv:2303.13657v1 [math.OC])",
    "abstract": "Distributional reinforcement learning (DRL) enhances the understanding of the effects of the randomness in the environment by letting agents learn the distribution of a random return, rather than its expected value as in standard RL. At the same time, a main challenge in DRL is that policy evaluation in DRL typically relies on the representation of the return distribution, which needs to be carefully designed. In this paper, we address this challenge for a special class of DRL problems that rely on linear quadratic regulator (LQR) for control, advocating for a new distributional approach to LQR, which we call \\emph{distributional LQR}. Specifically, we provide a closed-form expression of the distribution of the random return which, remarkably, is applicable to all exogenous disturbances on the dynamics, as long as they are independent and identically distributed (i.i.d.). While the proposed exact return distribution consists of infinitely many random variables, we show that this distri",
    "link": "http://arxiv.org/abs/2303.13657",
    "context": "Title: Policy Evaluation in Distributional LQR. (arXiv:2303.13657v1 [math.OC])\nAbstract: Distributional reinforcement learning (DRL) enhances the understanding of the effects of the randomness in the environment by letting agents learn the distribution of a random return, rather than its expected value as in standard RL. At the same time, a main challenge in DRL is that policy evaluation in DRL typically relies on the representation of the return distribution, which needs to be carefully designed. In this paper, we address this challenge for a special class of DRL problems that rely on linear quadratic regulator (LQR) for control, advocating for a new distributional approach to LQR, which we call \\emph{distributional LQR}. Specifically, we provide a closed-form expression of the distribution of the random return which, remarkably, is applicable to all exogenous disturbances on the dynamics, as long as they are independent and identically distributed (i.i.d.). While the proposed exact return distribution consists of infinitely many random variables, we show that this distri",
    "path": "papers/23/03/2303.13657.json",
    "total_tokens": 1011,
    "translated_title": "分布式LQR中的策略评估",
    "translated_abstract": "分布式强化学习增强了对环境中随机性影响的理解，通过让代理学习随机回报的分布，而不是像标准强化学习一样学习期望值。但是，在分布式强化学习中，策略评估通常依赖于对回报分布的表示，需要仔细设计。本文针对一类基于线性二次调节器（LQR）进行控制的DRL问题的这一挑战提出了一种新的分布式方法，称为“分布式LQR”。我们提供了随机回报分布的闭合表达式，适用于所有外部扰动的动力学。虽然所提出的精确回报分布包含无限多个随机变量，但我们表明这种分布可以通过有限的一组时刻有效逼近，并且我们证明这些逼近是准确且可计算的。最后，我们通过提供实证证据表明，在控制具有随机扰动的线性定常系统时，分布式LQR的效果优于竞争方法。",
    "tldr": "本文提出了一种新的分布式LQR方法，通过提供一种有效逼近无限多个随机变量的有限一组时刻，解决了DRL中策略评估的挑战，其效果优于竞争方法。",
    "en_tdlr": "This paper proposes a new distributional approach to LQR, called \"distributional LQR\", which provides a closed-form expression of the distribution of the random return applicable to all exogenous disturbances on the dynamics. The paper solves the challenge of policy evaluation in DRL by efficiently approximating infinitely many random variables with a finite set of moments, and shows the effectiveness of the proposed method in controlling a linear time-invariant system with random disturbances."
}