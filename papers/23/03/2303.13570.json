{
    "title": "Return of the RNN: Residual Recurrent Networks for Invertible Sentence Embeddings. (arXiv:2303.13570v1 [cs.CL])",
    "abstract": "This study presents a novel model for invertible sentence embeddings using a residual recurrent network trained on an unsupervised encoding task. Rather than the probabilistic outputs common to neural machine translation models, our approach employs a regression-based output layer to reconstruct the input sequence's word vectors. The model achieves high accuracy and fast training with the ADAM optimizer, a significant finding given that RNNs typically require memory units, such as LSTMs, or second-order optimization methods. We incorporate residual connections and introduce a \"match drop\" technique, where gradients are calculated only for incorrect words. Our approach demonstrates potential for various natural language processing applications, particularly in neural network-based systems that require high-quality sentence embeddings.",
    "link": "http://arxiv.org/abs/2303.13570",
    "context": "Title: Return of the RNN: Residual Recurrent Networks for Invertible Sentence Embeddings. (arXiv:2303.13570v1 [cs.CL])\nAbstract: This study presents a novel model for invertible sentence embeddings using a residual recurrent network trained on an unsupervised encoding task. Rather than the probabilistic outputs common to neural machine translation models, our approach employs a regression-based output layer to reconstruct the input sequence's word vectors. The model achieves high accuracy and fast training with the ADAM optimizer, a significant finding given that RNNs typically require memory units, such as LSTMs, or second-order optimization methods. We incorporate residual connections and introduce a \"match drop\" technique, where gradients are calculated only for incorrect words. Our approach demonstrates potential for various natural language processing applications, particularly in neural network-based systems that require high-quality sentence embeddings.",
    "path": "papers/23/03/2303.13570.json",
    "total_tokens": 887,
    "translated_title": "RNN 的回归：用可逆句嵌入的残差循环神经网络",
    "translated_abstract": "本研究提出了一种新型模型，使用残差循环神经网络在无监督编码任务上进行训练，以生成可逆的句子嵌入。相比于神经机器翻译模型中常见的概率输出，我们的方法采用基于回归的输出层来重建输入序列的单词向量。该模型在使用 ADAM 优化器进行快速训练的同时，取得了高准确度的结果。我们引入了残差连接和“match drop”技术，即只计算错误单词的梯度。我们的方法在各种自然语言处理应用中表现出潜在优势，特别是在需要高质量句嵌入的神经网络系统中。",
    "tldr": "本研究提出了一种使用残差循环神经网络的新型模型，实现了可逆的句子嵌入。与其他神经机器翻译模型不同，该方法使用基于回归的输出层重建输入序列的单词向量，其具有高准确度和快速训练速度。这种方法适合各种自然语言处理应用，特别是对需要高质量句嵌入的神经网络系统的使用具有潜在优势。",
    "en_tdlr": "This study proposes a novel model that uses residual recurrent networks to generate invertible sentence embeddings. The approach uses a regression-based output layer to reconstruct word vectors, achieving high accuracy and faster training. The technique of “match drop” is introduced and has potential for various natural language processing applications, particularly in neural network-based systems that require high-quality sentence embeddings."
}