{
    "title": "Tradeoff of generalization error in unsupervised learning. (arXiv:2303.05718v2 [cond-mat.stat-mech] UPDATED)",
    "abstract": "Finding the optimal model complexity that minimizes the generalization error (GE) is a key issue of machine learning. For the conventional supervised learning, this task typically involves the bias-variance tradeoff: lowering the bias by making the model more complex entails an increase in the variance. Meanwhile, little has been studied about whether the same tradeoff exists for unsupervised learning. In this study, we propose that unsupervised learning generally exhibits a two-component tradeoff of the GE, namely the model error and the data error -- using a more complex model reduces the model error at the cost of the data error, with the data error playing a more significant role for a smaller training dataset. This is corroborated by training the restricted Boltzmann machine to generate the configurations of the two-dimensional Ising model at a given temperature and the totally asymmetric simple exclusion process with given entry and exit rates. Our results also indicate that the ",
    "link": "http://arxiv.org/abs/2303.05718",
    "context": "Title: Tradeoff of generalization error in unsupervised learning. (arXiv:2303.05718v2 [cond-mat.stat-mech] UPDATED)\nAbstract: Finding the optimal model complexity that minimizes the generalization error (GE) is a key issue of machine learning. For the conventional supervised learning, this task typically involves the bias-variance tradeoff: lowering the bias by making the model more complex entails an increase in the variance. Meanwhile, little has been studied about whether the same tradeoff exists for unsupervised learning. In this study, we propose that unsupervised learning generally exhibits a two-component tradeoff of the GE, namely the model error and the data error -- using a more complex model reduces the model error at the cost of the data error, with the data error playing a more significant role for a smaller training dataset. This is corroborated by training the restricted Boltzmann machine to generate the configurations of the two-dimensional Ising model at a given temperature and the totally asymmetric simple exclusion process with given entry and exit rates. Our results also indicate that the ",
    "path": "papers/23/03/2303.05718.json",
    "total_tokens": 863,
    "translated_title": "无监督学习中泛化错误的权衡",
    "translated_abstract": "寻找最小化泛化错误（GE）的最佳模型复杂度是机器学习的关键问题。对于传统的监督学习，这个任务通常涉及偏差-方差权衡：通过使模型更复杂来降低偏差会导致方差的增加。与此同时，关于无监督学习是否存在相同的权衡问题研究较少。在本研究中，我们提出无监督学习通常表现出GE的两个组成要素的权衡，即模型误差和数据误差 - 使用更复杂的模型减小模型误差的同时以数据误差为代价，数据误差对于更小的训练数据集起到更重要的作用。通过训练受限玻尔兹曼机生成给定温度下的二维伊辛模型的配置以及给定入口和出口速率的完全非对称简单排斥过程，我们证实了这一点。我们的结果还表明，",
    "tldr": "无监督学习中存在一个权衡，使用更复杂的模型可以降低模型误差，但会增加数据误差，特别是在训练数据集较小的情况下。"
}