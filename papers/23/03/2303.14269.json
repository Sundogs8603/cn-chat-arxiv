{
    "title": "The Exact Sample Complexity Gain from Invariances for Kernel Regression on Manifolds. (arXiv:2303.14269v1 [cs.LG])",
    "abstract": "In practice, encoding invariances into models helps sample complexity. In this work, we tighten and generalize theoretical results on how invariances improve sample complexity. In particular, we provide minimax optimal rates for kernel ridge regression on any manifold, with a target function that is invariant to an arbitrary group action on the manifold. Our results hold for (almost) any group action, even groups of positive dimension. For a finite group, the gain increases the \"effective\" number of samples by the group size. For groups of positive dimension, the gain is observed by a reduction in the manifold's dimension, in addition to a factor proportional to the volume of the quotient space. Our proof takes the viewpoint of differential geometry, in contrast to the more common strategy of using invariant polynomials. Hence, this new geometric viewpoint on learning with invariances may be of independent interest.",
    "link": "http://arxiv.org/abs/2303.14269",
    "context": "Title: The Exact Sample Complexity Gain from Invariances for Kernel Regression on Manifolds. (arXiv:2303.14269v1 [cs.LG])\nAbstract: In practice, encoding invariances into models helps sample complexity. In this work, we tighten and generalize theoretical results on how invariances improve sample complexity. In particular, we provide minimax optimal rates for kernel ridge regression on any manifold, with a target function that is invariant to an arbitrary group action on the manifold. Our results hold for (almost) any group action, even groups of positive dimension. For a finite group, the gain increases the \"effective\" number of samples by the group size. For groups of positive dimension, the gain is observed by a reduction in the manifold's dimension, in addition to a factor proportional to the volume of the quotient space. Our proof takes the viewpoint of differential geometry, in contrast to the more common strategy of using invariant polynomials. Hence, this new geometric viewpoint on learning with invariances may be of independent interest.",
    "path": "papers/23/03/2303.14269.json",
    "total_tokens": 903,
    "translated_title": "内容不变性对流形核回归的精确样本复杂度增益",
    "translated_abstract": "在实践中，将内容不变性编码进模型可以提高样本复杂度。本文对内容不变性如何改善样本复杂度的理论结果进行了细化和推广，特别地，在任何流形上，对于一个在流形上任意群作用下不变的目标函数，我们提供了核岭回归的最小化最优率。我们的结果适用于（几乎）任何群作用，甚至是正维度的群。对于有限群，增益通过将“有效”样本数量扩大到群的大小来实现。对于正维度的群，增益表现为降低流形的维数，同时还与商空间体积成比例。我们的证明从微分几何的角度来看，与使用不变多项式的更常见策略不同。因此，这个在具有不变性的学习中的新几何视角可能具有独立的兴趣。",
    "tldr": "本文提供了在任何流形上，对于一个在流形上任意群作用下不变的目标函数，核岭回归的最小化最优率，从而增加了有效样本数量或降低了流形的维数。"
}