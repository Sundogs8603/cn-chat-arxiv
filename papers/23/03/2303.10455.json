{
    "title": "Learn, Unlearn and Relearn: An Online Learning Paradigm for Deep Neural Networks. (arXiv:2303.10455v1 [cs.LG])",
    "abstract": "Deep neural networks (DNNs) are often trained on the premise that the complete training data set is provided ahead of time. However, in real-world scenarios, data often arrive in chunks over time. This leads to important considerations about the optimal strategy for training DNNs, such as whether to fine-tune them with each chunk of incoming data (warm-start) or to retrain them from scratch with the entire corpus of data whenever a new chunk is available. While employing the latter for training can be resource-intensive, recent work has pointed out the lack of generalization in warm-start models. Therefore, to strike a balance between efficiency and generalization, we introduce Learn, Unlearn, and Relearn (LURE) an online learning paradigm for DNNs. LURE interchanges between the unlearning phase, which selectively forgets the undesirable information in the model through weight reinitialization in a data-dependent manner, and the relearning phase, which emphasizes learning on generaliza",
    "link": "http://arxiv.org/abs/2303.10455",
    "context": "Title: Learn, Unlearn and Relearn: An Online Learning Paradigm for Deep Neural Networks. (arXiv:2303.10455v1 [cs.LG])\nAbstract: Deep neural networks (DNNs) are often trained on the premise that the complete training data set is provided ahead of time. However, in real-world scenarios, data often arrive in chunks over time. This leads to important considerations about the optimal strategy for training DNNs, such as whether to fine-tune them with each chunk of incoming data (warm-start) or to retrain them from scratch with the entire corpus of data whenever a new chunk is available. While employing the latter for training can be resource-intensive, recent work has pointed out the lack of generalization in warm-start models. Therefore, to strike a balance between efficiency and generalization, we introduce Learn, Unlearn, and Relearn (LURE) an online learning paradigm for DNNs. LURE interchanges between the unlearning phase, which selectively forgets the undesirable information in the model through weight reinitialization in a data-dependent manner, and the relearning phase, which emphasizes learning on generaliza",
    "path": "papers/23/03/2303.10455.json",
    "total_tokens": 1006,
    "translated_title": "学习、遗忘和重学：深度神经网络的在线学习范式",
    "translated_abstract": "深度神经网络（DNN）通常是在提前提供完整的训练数据集的前提下进行训练的。然而，在现实世界的情况下，数据经常会随着时间的推移而以块的形式出现。这就引发了关于训练DNN的最佳策略的重要考虑，例如是否在每个新数据块到达时使用微调的方法（温启动）或者每当有新的数据块可用时从头开始重新训练它们。虽然后者用于训练可以消耗更多的资源，但最近的研究指出了温启动模型的泛化能力缺乏。因此，为了在效率和泛化之间取得平衡，我们介绍了一种深度神经网络的在线学习范式——学习、遗忘和重学（LURE）。LURE在遗忘阶段和重学阶段之间进行交替，遗忘阶段通过数据依赖方式的权重重新初始化有选择地遗忘模型中的不良信息，而重学阶段则强调对泛化性的学习。",
    "tldr": "学习、遗忘和重学（LURE）是一种深度神经网络的在线学习范式，它在遗忘阶段和重学阶段之间进行交替，通过有选择地遗忘模型中的不良信息和对泛化性的学习来在效率和泛化之间取得平衡。",
    "en_tdlr": "Learn, Unlearn, and Relearn (LURE) is an online learning paradigm for deep neural networks. It strikes a balance between efficiency and generalization by interchanging between the unlearning phase, which selectively forgets the undesirable information in the model through weight reinitialization in a data-dependent manner, and the relearning phase, which emphasizes learning on generalization."
}