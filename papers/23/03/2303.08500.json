{
    "title": "The Devil's Advocate: Shattering the Illusion of Unexploitable Data using Diffusion Models. (arXiv:2303.08500v1 [cs.LG])",
    "abstract": "Protecting personal data against the exploitation of machine learning models is of paramount importance. Recently, availability attacks have shown great promise to provide an extra layer of protection against the unauthorized use of data to train neural networks. These methods aim to add imperceptible noise to clean data so that the neural networks cannot extract meaningful patterns from the protected data, claiming that they can make personal data \"unexploitable.\" In this paper, we provide a strong countermeasure against such approaches, showing that unexploitable data might only be an illusion. In particular, we leverage the power of diffusion models and show that a carefully designed denoising process can defuse the ramifications of the data-protecting perturbations. We rigorously analyze our algorithm, and theoretically prove that the amount of required denoising is directly related to the magnitude of the data-protecting perturbations. Our approach, called AVATAR, delivers state-o",
    "link": "http://arxiv.org/abs/2303.08500",
    "context": "Title: The Devil's Advocate: Shattering the Illusion of Unexploitable Data using Diffusion Models. (arXiv:2303.08500v1 [cs.LG])\nAbstract: Protecting personal data against the exploitation of machine learning models is of paramount importance. Recently, availability attacks have shown great promise to provide an extra layer of protection against the unauthorized use of data to train neural networks. These methods aim to add imperceptible noise to clean data so that the neural networks cannot extract meaningful patterns from the protected data, claiming that they can make personal data \"unexploitable.\" In this paper, we provide a strong countermeasure against such approaches, showing that unexploitable data might only be an illusion. In particular, we leverage the power of diffusion models and show that a carefully designed denoising process can defuse the ramifications of the data-protecting perturbations. We rigorously analyze our algorithm, and theoretically prove that the amount of required denoising is directly related to the magnitude of the data-protecting perturbations. Our approach, called AVATAR, delivers state-o",
    "path": "papers/23/03/2303.08500.json",
    "total_tokens": 1029,
    "translated_title": "规避扩散模型中添加的噪声对数据进行保护的挑战",
    "translated_abstract": "保护个人数据免受机器学习模型的利用至关重要。最近，可用性攻击展现出提供额外保护措施的巨大潜力，以防止未经授权地使用数据来训练神经网络。这些方法旨在向干净数据添加难以察觉的噪声，使神经网络无法从受保护的数据中提取有意义的模式，声称可以使个人数据“无法利用”。在本文中，我们针对这种方法提供了一个强有力的对抗措施，表明不可利用的数据可能只是一种幻觉。特别地，我们利用扩散模型的威力，并展示精心设计的去噪过程可以消除数据保护扰动的影响。我们严谨地分析了我们的算法，并在理论上证明了所需去噪的量直接与数据保护扰动的数量成正比。我们的方法名为AVATAR，在包括CelebA数据集在内的多个数据集上提供了最先进的性能，其中它以巨大的优势胜出现有方法。",
    "tldr": "保护个人隐私信息是很重要的，但规避扩散模型中添加的噪声对数据进行保护存在挑战。AVATAR算法借助扩散模型的威力，提供了一种精心设计的去噪过程来消除数据保护扰动的影响，并获得了在多个数据集上的最先进的性能。"
}