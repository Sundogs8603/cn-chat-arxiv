{
    "title": "BC-IRL: Learning Generalizable Reward Functions from Demonstrations. (arXiv:2303.16194v1 [cs.LG])",
    "abstract": "How well do reward functions learned with inverse reinforcement learning (IRL) generalize? We illustrate that state-of-the-art IRL algorithms, which maximize a maximum-entropy objective, learn rewards that overfit to the demonstrations. Such rewards struggle to provide meaningful rewards for states not covered by the demonstrations, a major detriment when using the reward to learn policies in new situations. We introduce BC-IRL a new inverse reinforcement learning method that learns reward functions that generalize better when compared to maximum-entropy IRL approaches. In contrast to the MaxEnt framework, which learns to maximize rewards around demonstrations, BC-IRL updates reward parameters such that the policy trained with the new reward matches the expert demonstrations better. We show that BC-IRL learns rewards that generalize better on an illustrative simple task and two continuous robotic control tasks, achieving over twice the success rate of baselines in challenging generaliz",
    "link": "http://arxiv.org/abs/2303.16194",
    "context": "Title: BC-IRL: Learning Generalizable Reward Functions from Demonstrations. (arXiv:2303.16194v1 [cs.LG])\nAbstract: How well do reward functions learned with inverse reinforcement learning (IRL) generalize? We illustrate that state-of-the-art IRL algorithms, which maximize a maximum-entropy objective, learn rewards that overfit to the demonstrations. Such rewards struggle to provide meaningful rewards for states not covered by the demonstrations, a major detriment when using the reward to learn policies in new situations. We introduce BC-IRL a new inverse reinforcement learning method that learns reward functions that generalize better when compared to maximum-entropy IRL approaches. In contrast to the MaxEnt framework, which learns to maximize rewards around demonstrations, BC-IRL updates reward parameters such that the policy trained with the new reward matches the expert demonstrations better. We show that BC-IRL learns rewards that generalize better on an illustrative simple task and two continuous robotic control tasks, achieving over twice the success rate of baselines in challenging generaliz",
    "path": "papers/23/03/2303.16194.json",
    "total_tokens": 887,
    "translated_title": "BC-IRL: 从示范中学习可泛化奖励函数",
    "translated_abstract": "逆强化学习（IRL）学习的奖励函数能够很好地泛化吗？我们展示了最先进的IRL算法，该算法最大化最大熵目标，学习到的奖励函数过度拟合于示范。这样的奖励函数难以为未被示范覆盖的状态提供有意义的奖励，这在使用奖励来学习新情况下的策略时是一个重大劣势。我们引入了BC-IRL，一种新的逆强化学习方法，该方法学习的奖励函数在与最大熵IRL方法相比更具有泛化性。与MaxEnt框架不同，该算法更新奖励参数，以便所训练的策略更好地匹配示范。我们展示了BC-IRL在一个简单任务和两个连续的机器人控制任务上学习到更具有泛化性的奖励函数，在挑战性的泛化场景中成功率是基线的两倍以上。",
    "tldr": "本文介绍了一种新的逆强化学习方法BC-IRL，能够更好地学习泛化性更强的奖励函数并在挑战性的泛化场景中取得两倍以上的成功率。",
    "en_tdlr": "This paper introduces a new inverse reinforcement learning method, BC-IRL, which learns reward functions with better generalization ability, achieving over twice the success rate of baselines in challenging generalization scenarios."
}