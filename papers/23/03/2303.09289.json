{
    "title": "Image Classifiers Leak Sensitive Attributes About Their Classes. (arXiv:2303.09289v1 [cs.LG])",
    "abstract": "Neural network-based image classifiers are powerful tools for computer vision tasks, but they inadvertently reveal sensitive attribute information about their classes, raising concerns about their privacy. To investigate this privacy leakage, we introduce the first Class Attribute Inference Attack (Caia), which leverages recent advances in text-to-image synthesis to infer sensitive attributes of individual classes in a black-box setting, while remaining competitive with related white-box attacks. Our extensive experiments in the face recognition domain show that Caia can accurately infer undisclosed sensitive attributes, such as an individual's hair color, gender and racial appearance, which are not part of the training labels. Interestingly, we demonstrate that adversarial robust models are even more vulnerable to such privacy leakage than standard models, indicating that a trade-off between robustness and privacy exists.",
    "link": "http://arxiv.org/abs/2303.09289",
    "context": "Title: Image Classifiers Leak Sensitive Attributes About Their Classes. (arXiv:2303.09289v1 [cs.LG])\nAbstract: Neural network-based image classifiers are powerful tools for computer vision tasks, but they inadvertently reveal sensitive attribute information about their classes, raising concerns about their privacy. To investigate this privacy leakage, we introduce the first Class Attribute Inference Attack (Caia), which leverages recent advances in text-to-image synthesis to infer sensitive attributes of individual classes in a black-box setting, while remaining competitive with related white-box attacks. Our extensive experiments in the face recognition domain show that Caia can accurately infer undisclosed sensitive attributes, such as an individual's hair color, gender and racial appearance, which are not part of the training labels. Interestingly, we demonstrate that adversarial robust models are even more vulnerable to such privacy leakage than standard models, indicating that a trade-off between robustness and privacy exists.",
    "path": "papers/23/03/2303.09289.json",
    "total_tokens": 881,
    "translated_title": "图像分类器泄露其类别的敏感属性",
    "translated_abstract": "基于神经网络的图像分类器是计算机视觉任务的有力工具，但它们无意中透露了有关其类别的敏感属性信息，引起了对它们的隐私的关注。为了研究这种隐私泄漏，我们引入了第一个Class Attribute Inference Attack（Caia），利用最近在文本到图像合成方面的进展，在黑盒设置中推断出单个类别的敏感属性，同时与相关的白盒攻击相竞争。在人脸识别领域进行的广泛实验表明，Caia能够准确地推断出未公开的敏感属性，例如个人的发色、性别和种族外貌，这些属性不属于训练标签。有趣的是，我们证明了对抗性鲁棒模型比标准模型更容易泄露隐私，表明在鲁棒性和隐私之间存在权衡。",
    "tldr": "该论文探讨了图像分类器存在的隐私泄露问题，提出的Class Attribute Inference Attack（Caia）能够从黑盒设置中准确地推断出敏感属性，包括个人的发色、性别和种族，这表明在鲁棒性和隐私之间存在权衡。",
    "en_tdlr": "The paper discusses the privacy leakage issue for image classifiers and introduces the first Class Attribute Inference Attack (Caia) that can accurately infer sensitive attributes, such as hair color, gender, and race, from a black-box setting. The study indicates a trade-off between robustness and privacy, and the adversarial robust models are even more vulnerable to such privacy leakage."
}