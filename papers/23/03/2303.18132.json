{
    "title": "A Desynchronization-Based Countermeasure Against Side-Channel Analysis of Neural Networks. (arXiv:2303.18132v1 [cs.CR])",
    "abstract": "Model extraction attacks have been widely applied, which can normally be used to recover confidential parameters of neural networks for multiple layers. Recently, side-channel analysis of neural networks allows parameter extraction even for networks with several multiple deep layers with high effectiveness. It is therefore of interest to implement a certain level of protection against these attacks. In this paper, we propose a desynchronization-based countermeasure that makes the timing analysis of activation functions harder. We analyze the timing properties of several activation functions and design the desynchronization in a way that the dependency on the input and the activation type is hidden. We experimentally verify the effectiveness of the countermeasure on a 32-bit ARM Cortex-M4 microcontroller and employ a t-test to show the side-channel information leakage. The overhead ultimately depends on the number of neurons in the fully-connected layer, for example, in the case of 4096",
    "link": "http://arxiv.org/abs/2303.18132",
    "context": "Title: A Desynchronization-Based Countermeasure Against Side-Channel Analysis of Neural Networks. (arXiv:2303.18132v1 [cs.CR])\nAbstract: Model extraction attacks have been widely applied, which can normally be used to recover confidential parameters of neural networks for multiple layers. Recently, side-channel analysis of neural networks allows parameter extraction even for networks with several multiple deep layers with high effectiveness. It is therefore of interest to implement a certain level of protection against these attacks. In this paper, we propose a desynchronization-based countermeasure that makes the timing analysis of activation functions harder. We analyze the timing properties of several activation functions and design the desynchronization in a way that the dependency on the input and the activation type is hidden. We experimentally verify the effectiveness of the countermeasure on a 32-bit ARM Cortex-M4 microcontroller and employ a t-test to show the side-channel information leakage. The overhead ultimately depends on the number of neurons in the fully-connected layer, for example, in the case of 4096",
    "path": "papers/23/03/2303.18132.json",
    "total_tokens": 935,
    "translated_title": "基于脱同步的神经网络侧信道攻击对抗措施",
    "translated_abstract": "模型提取攻击已经被广泛应用，通常可以用于恢复多层神经网络的机密参数。最近，神经网络的侧信道分析即使对于具有多个深层网络的网络也可以进行参数提取，并且非常有效。因此，实现一定程度的保护以抵御这些攻击非常重要。在本文中，我们提出了一种基于脱同步的对抗措施，使激活函数的时序分析变得更加困难。我们分析了几种激活函数的时序特性，并设计了脱同步方式，以隐藏输入和激活类型的依赖性。我们在32位ARM Cortex-M4微控制器上实验验证了对抗措施的有效性，并使用t检验显示了侧信道信息泄漏。最终开销取决于完全连接层中神经元的数量，例如，对于4096个神经元的情况，我们的方案开销不到1%。",
    "tldr": "本文提出了一种基于脱同步的对抗措施，使激活函数的时序分析变得更加困难，以防止模型提取和神经网络侧信道分析攻击。实验结果表明，在4096个神经元的情况下，我们的方案仅产生不到1%的开销。",
    "en_tdlr": "This paper proposes a desynchronization-based countermeasure to prevent model extraction and side-channel analysis attacks on neural networks by making the timing analysis of activation functions more difficult. The experimental results show that our approach generates less than 1% overhead in the case of 4096 neurons."
}