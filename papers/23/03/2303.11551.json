{
    "title": "ModEFormer: Modality-Preserving Embedding for Audio-Video Synchronization using Transformers. (arXiv:2303.11551v1 [cs.CV])",
    "abstract": "Lack of audio-video synchronization is a common problem during television broadcasts and video conferencing, leading to an unsatisfactory viewing experience. A widely accepted paradigm is to create an error detection mechanism that identifies the cases when audio is leading or lagging. We propose ModEFormer, which independently extracts audio and video embeddings using modality-specific transformers. Different from the other transformer-based approaches, ModEFormer preserves the modality of the input streams which allows us to use a larger batch size with more negative audio samples for contrastive learning. Further, we propose a trade-off between the number of negative samples and number of unique samples in a batch to significantly exceed the performance of previous methods. Experimental results show that ModEFormer achieves state-of-the-art performance, 94.5% for LRS2 and 90.9% for LRS3. Finally, we demonstrate how ModEFormer can be used for offset detection for test clips.",
    "link": "http://arxiv.org/abs/2303.11551",
    "context": "Title: ModEFormer: Modality-Preserving Embedding for Audio-Video Synchronization using Transformers. (arXiv:2303.11551v1 [cs.CV])\nAbstract: Lack of audio-video synchronization is a common problem during television broadcasts and video conferencing, leading to an unsatisfactory viewing experience. A widely accepted paradigm is to create an error detection mechanism that identifies the cases when audio is leading or lagging. We propose ModEFormer, which independently extracts audio and video embeddings using modality-specific transformers. Different from the other transformer-based approaches, ModEFormer preserves the modality of the input streams which allows us to use a larger batch size with more negative audio samples for contrastive learning. Further, we propose a trade-off between the number of negative samples and number of unique samples in a batch to significantly exceed the performance of previous methods. Experimental results show that ModEFormer achieves state-of-the-art performance, 94.5% for LRS2 and 90.9% for LRS3. Finally, we demonstrate how ModEFormer can be used for offset detection for test clips.",
    "path": "papers/23/03/2303.11551.json",
    "total_tokens": 906,
    "translated_title": "ModEFormer：使用变压器进行音视频同步的模态保留嵌入",
    "translated_abstract": "电视广播和视频会议中音视频不同步是一个常见的问题，会导致观看体验不佳。一种广泛接受的范式是创建一个误差检测机制来识别音频超前或滞后的情况。我们提出了ModEFormer，通过模态特定的变压器独立提取音频和视频嵌入。与其他基于变压器的方法不同的是，ModEFormer保留输入流的模态，这使我们可以使用更大的批量和更多的负音频样本进行对比学习。此外，我们提出了批处理中负样本数量和唯一样本数量之间的权衡，以明显超过以前方法的性能。实验结果表明，ModEFormer实现了最先进的性能，对于LRS2和LRS3分别达到了94.5％和90.9％。最后，我们演示了如何使用ModEFormer对测试片段进行偏移检测。",
    "tldr": "ModEFormer使用变压器独立提取音频和视频嵌入，并保留输入流的模态，使用更大的批量和更多的负音频样本进行对比学习，这使得其在检测电视广播和视频会议中的音视频同步方面取得了最先进的性能。",
    "en_tdlr": "ModEFormer uses transformers to independently extract audio and video embeddings while preserving the modality of input streams, allowing for larger batch sizes and more negative audio samples for contrastive learning. This leads to state-of-the-art performance for detecting audio-video synchronization in television broadcasts and video conferencing."
}