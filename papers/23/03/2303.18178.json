{
    "title": "Robust and IP-Protecting Vertical Federated Learning against Unexpected Quitting of Parties. (arXiv:2303.18178v1 [cs.CR])",
    "abstract": "Vertical federated learning (VFL) enables a service provider (i.e., active party) who owns labeled features to collaborate with passive parties who possess auxiliary features to improve model performance. Existing VFL approaches, however, have two major vulnerabilities when passive parties unexpectedly quit in the deployment phase of VFL - severe performance degradation and intellectual property (IP) leakage of the active party's labels. In this paper, we propose \\textbf{Party-wise Dropout} to improve the VFL model's robustness against the unexpected exit of passive parties and a defense method called \\textbf{DIMIP} to protect the active party's IP in the deployment phase. We evaluate our proposed methods on multiple datasets against different inference attacks. The results show that Party-wise Dropout effectively maintains model performance after the passive party quits, and DIMIP successfully disguises label information from the passive party's feature extractor, thereby mitigating I",
    "link": "http://arxiv.org/abs/2303.18178",
    "context": "Title: Robust and IP-Protecting Vertical Federated Learning against Unexpected Quitting of Parties. (arXiv:2303.18178v1 [cs.CR])\nAbstract: Vertical federated learning (VFL) enables a service provider (i.e., active party) who owns labeled features to collaborate with passive parties who possess auxiliary features to improve model performance. Existing VFL approaches, however, have two major vulnerabilities when passive parties unexpectedly quit in the deployment phase of VFL - severe performance degradation and intellectual property (IP) leakage of the active party's labels. In this paper, we propose \\textbf{Party-wise Dropout} to improve the VFL model's robustness against the unexpected exit of passive parties and a defense method called \\textbf{DIMIP} to protect the active party's IP in the deployment phase. We evaluate our proposed methods on multiple datasets against different inference attacks. The results show that Party-wise Dropout effectively maintains model performance after the passive party quits, and DIMIP successfully disguises label information from the passive party's feature extractor, thereby mitigating I",
    "path": "papers/23/03/2303.18178.json",
    "total_tokens": 912,
    "translated_title": "鲁棒性与知识产权保护的竖直联邦学习防止参与方意外退出",
    "translated_abstract": "竖直联邦学习（VFL）使具有标记特征的服务提供商（即活动方）能够与拥有辅助特征的被动方合作，以提高模型性能。然而，现有的VFL方法存在两个主要漏洞，当被动方在VFL部署阶段意外退出时 - 严重的性能降低和活动方标签的知识产权泄露。本文提出了Party-wise Dropout来提高VFL模型对被动方意外退出的鲁棒性，以及一种名为DIMIP的防御方法，用于保护活动方在部署阶段的知识产权。我们对多个数据集以及不同的推理攻击对我们提出的方法进行了评估。结果表明，Party-wise Dropout可以在被动方退出后有效地保持模型性能，DIMIP可以成功地掩盖被动方特征提取器中的标签信息，从而减轻知识产权侵权。",
    "tldr": "本文提出了一种名为Party-wise Dropout的方法，该方法可以提高竖直联邦学习模型对被动方意外退出的鲁棒性，并提出了一种名为DIMIP的防御方法，用于保护活动方在部署阶段的知识产权。",
    "en_tdlr": "This paper proposes a method called Party-wise Dropout to improve the robustness of the vertical federated learning model against the unexpected exit of passive parties, and a defense method called DIMIP to protect the intellectual property of the active party in the deployment phase."
}