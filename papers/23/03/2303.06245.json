{
    "title": "AUTODIAL: Efficient Asynchronous Task-Oriented Dialogue Model. (arXiv:2303.06245v1 [cs.CL])",
    "abstract": "As large dialogue models become commonplace in practice, the problems surrounding high compute requirements for training, inference and larger memory footprint still persists. In this work, we present AUTODIAL, a multi-task dialogue model that addresses the challenges of deploying dialogue model. AUTODIAL utilizes parallel decoders to perform tasks such as dialogue act prediction, domain prediction, intent prediction, and dialogue state tracking. Using classification decoders over generative decoders allows AUTODIAL to significantly reduce memory footprint and achieve faster inference times compared to existing generative approach namely SimpleTOD. We demonstrate that AUTODIAL provides 3-6x speedups during inference while having 11x fewer parameters on three dialogue tasks compared to SimpleTOD. Our results show that extending current dialogue models to have parallel decoders can be a viable alternative for deploying them in resource-constrained environments.",
    "link": "http://arxiv.org/abs/2303.06245",
    "total_tokens": 916,
    "translated_title": "AUTODIAL: 高效异步任务导向的对话模型",
    "translated_abstract": "随着大型对话模型在实践中变得普遍，训练、推理和更大的内存占用的高计算要求问题仍然存在。在这项工作中，我们提出了AUTODIAL，一种多任务对话模型，解决了部署对话模型的挑战。AUTODIAL利用并行解码器执行诸如对话行为预测、领域预测、意图预测和对话状态跟踪等任务。使用分类解码器而不是生成解码器使AUTODIAL能够显著减少内存占用，并在推理时间上实现比现有生成方法（即SimpleTOD）更快的速度。我们证明，将当前的对话模型扩展为具有并行解码器可以成为在资源受限环境中部署它们的可行替代方案。",
    "tldr": "AUTODIAL是一种多任务对话模型，通过使用并行解码器来执行对话任务，从而显著减少内存占用并实现更快的推理时间。与现有的生成方法相比，AUTODIAL在三个对话任务上提供了3-6倍的速度提升，同时具有11倍的参数减少。这表明将当前的对话模型扩展为具有并行解码器可以成为在资源受限环境中部署它们的可行替代方案。",
    "en_tldr": "AUTODIAL is a multi-task dialogue model that significantly reduces memory footprint and achieves faster inference times by using parallel decoders to perform dialogue tasks. Compared to existing generative approach, AUTODIAL provides 3-6x speedups during inference while having 11x fewer parameters on three dialogue tasks. This suggests that extending current dialogue models to have parallel decoders can be a viable alternative for deploying them in resource-constrained environments."
}