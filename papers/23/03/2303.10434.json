{
    "title": "Byzantine-Resilient Federated Learning at Edge. (arXiv:2303.10434v1 [cs.DC])",
    "abstract": "Both Byzantine resilience and communication efficiency have attracted tremendous attention recently for their significance in edge federated learning. However, most existing algorithms may fail when dealing with real-world irregular data that behaves in a heavy-tailed manner. To address this issue, we study the stochastic convex and non-convex optimization problem for federated learning at edge and show how to handle heavy-tailed data while retaining the Byzantine resilience, communication efficiency and the optimal statistical error rates simultaneously. Specifically, we first present a Byzantine-resilient distributed gradient descent algorithm that can handle the heavy-tailed data and meanwhile converge under the standard assumptions. To reduce the communication overhead, we further propose another algorithm that incorporates gradient compression techniques to save communication costs during the learning process. Theoretical analysis shows that our algorithms achieve order-optimal st",
    "link": "http://arxiv.org/abs/2303.10434",
    "context": "Title: Byzantine-Resilient Federated Learning at Edge. (arXiv:2303.10434v1 [cs.DC])\nAbstract: Both Byzantine resilience and communication efficiency have attracted tremendous attention recently for their significance in edge federated learning. However, most existing algorithms may fail when dealing with real-world irregular data that behaves in a heavy-tailed manner. To address this issue, we study the stochastic convex and non-convex optimization problem for federated learning at edge and show how to handle heavy-tailed data while retaining the Byzantine resilience, communication efficiency and the optimal statistical error rates simultaneously. Specifically, we first present a Byzantine-resilient distributed gradient descent algorithm that can handle the heavy-tailed data and meanwhile converge under the standard assumptions. To reduce the communication overhead, we further propose another algorithm that incorporates gradient compression techniques to save communication costs during the learning process. Theoretical analysis shows that our algorithms achieve order-optimal st",
    "path": "papers/23/03/2303.10434.json",
    "total_tokens": 916,
    "translated_title": "边缘计算中拜占庭鲁棒的联邦学习",
    "translated_abstract": "近年来，拜占庭容错和通信效率在边缘联邦学习中备受关注。然而，现有算法在处理真实世界中表现具有重尾性质的不规则数据时可能会失败。为了解决这个问题，我们研究了边缘联邦学习中的随机凸和非凸优化问题，并展示了如何同时保持拜占庭容错、通信效率和最优统计误差率来处理重尾数据。具体来说，我们首先提出了一种支持处理重尾数据、同时在标准假设下收敛的拜占庭容错分布式梯度下降算法。为了减少通信开销，我们进一步提出了另一种算法，采用梯度压缩技术在学习过程中节省通信成本。理论分析表明，我们的算法在保持对拜占庭攻击的鲁棒性的同时，能够高概率地实现最优统计速率。我们对合成和现实世界数据集进行了广泛的实验验证。",
    "tldr": "本文提出了一种适用于处理重尾数据并在拜占庭攻击下具备鲁棒性的联邦学习算法，同时采用梯度压缩技术来减少通信开销。",
    "en_tdlr": "This paper proposes a Byzantine-resilient federated learning algorithm that handles heavy-tailed data and maintains robustness against Byzantine attacks, while incorporating gradient compression techniques to reduce communication costs."
}