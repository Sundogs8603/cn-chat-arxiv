{
    "title": "STDLens: Model Hijacking-resilient Federated Learning for Object Detection. (arXiv:2303.11511v1 [cs.CR])",
    "abstract": "Federated Learning (FL) has been gaining popularity as a collaborative learning framework to train deep learning-based object detection models over a distributed population of clients. Despite its advantages, FL is vulnerable to model hijacking. The attacker can control how the object detection system should misbehave by implanting Trojaned gradients using only a small number of compromised clients in the collaborative learning process. This paper introduces STDLens, a principled approach to safeguarding FL against such attacks. We first investigate existing mitigation mechanisms and analyze their failures caused by the inherent errors in spatial clustering analysis on gradients. Based on the insights, we introduce a three-tier forensic framework to identify and expel Trojaned gradients and reclaim the performance over the course of FL. We consider three types of adaptive attacks and demonstrate the robustness of STDLens against advanced adversaries. Extensive experiments show that STD",
    "link": "http://arxiv.org/abs/2303.11511",
    "context": "Title: STDLens: Model Hijacking-resilient Federated Learning for Object Detection. (arXiv:2303.11511v1 [cs.CR])\nAbstract: Federated Learning (FL) has been gaining popularity as a collaborative learning framework to train deep learning-based object detection models over a distributed population of clients. Despite its advantages, FL is vulnerable to model hijacking. The attacker can control how the object detection system should misbehave by implanting Trojaned gradients using only a small number of compromised clients in the collaborative learning process. This paper introduces STDLens, a principled approach to safeguarding FL against such attacks. We first investigate existing mitigation mechanisms and analyze their failures caused by the inherent errors in spatial clustering analysis on gradients. Based on the insights, we introduce a three-tier forensic framework to identify and expel Trojaned gradients and reclaim the performance over the course of FL. We consider three types of adaptive attacks and demonstrate the robustness of STDLens against advanced adversaries. Extensive experiments show that STD",
    "path": "papers/23/03/2303.11511.json",
    "total_tokens": 1029,
    "translated_title": "STDLens：基于模型挟持的物体检测联邦学习的安全防护方法",
    "translated_abstract": "联邦学习（FL）作为协同学习框架在分布式客户端中训练基于深度学习的物体检测模型已经越来越受欢迎。尽管它具有诸多优点，FL容易受到模型挟持的攻击。攻击者可以仅仅利用一小部分可以被攻击的客户端控制物体检测系统的正确性，通过植入特殊梯度实现攻击。本文提出了一种名为STDLens的安全方法以保护FL免受此类攻击。我们首先调查现有的缓解机制并分析它们在空间聚类分析梯度时由于固有误差而产生的失败情况。基于这些洞见，我们提出了一个三层的取证框架来识别和排除这种特殊的梯度，并在FL过程中恢复性能。我们考虑了三种类型的自适应攻击，并展示了STDLens对高级对手具有的稳健性。大量实验表明，STDLens在物体检测方面实现了最先进的性能，并且具有防止模型挟持的鲁棒性。",
    "tldr": "STDLens 是一种可以防止FL受到模型挟持的攻击的安全方法。它基于三层的取证框架来识别和排除特殊的梯度，并恢复FL的性能。STDLens在物体检测方面实现了最先进的性能并且具有防止模型挟持的鲁棒性。",
    "en_tdlr": "STDLens is a secure method to protect federated learning (FL) against model hijacking attacks. It uses a three-tier forensic framework to identify and expel Trojaned gradients and reclaim performance over the course of FL. STDLens achieves state-of-the-art performance in object detection while being resilient to model hijacking attacks."
}