{
    "title": "Prototype Helps Federated Learning: Towards Faster Convergence. (arXiv:2303.12296v1 [cs.LG])",
    "abstract": "Federated learning (FL) is a distributed machine learning technique in which multiple clients cooperate to train a shared model without exchanging their raw data. However, heterogeneity of data distribution among clients usually leads to poor model inference. In this paper, a prototype-based federated learning framework is proposed, which can achieve better inference performance with only a few changes to the last global iteration of the typical federated learning process. In the last iteration, the server aggregates the prototypes transmitted from distributed clients and then sends them back to local clients for their respective model inferences. Experiments on two baseline datasets show that our proposal can achieve higher accuracy (at least 1%) and relatively efficient communication than two popular baselines under different heterogeneous settings.",
    "link": "http://arxiv.org/abs/2303.12296",
    "context": "Title: Prototype Helps Federated Learning: Towards Faster Convergence. (arXiv:2303.12296v1 [cs.LG])\nAbstract: Federated learning (FL) is a distributed machine learning technique in which multiple clients cooperate to train a shared model without exchanging their raw data. However, heterogeneity of data distribution among clients usually leads to poor model inference. In this paper, a prototype-based federated learning framework is proposed, which can achieve better inference performance with only a few changes to the last global iteration of the typical federated learning process. In the last iteration, the server aggregates the prototypes transmitted from distributed clients and then sends them back to local clients for their respective model inferences. Experiments on two baseline datasets show that our proposal can achieve higher accuracy (at least 1%) and relatively efficient communication than two popular baselines under different heterogeneous settings.",
    "path": "papers/23/03/2303.12296.json",
    "total_tokens": 796,
    "translated_title": "原型有助于联邦学习：实现更快的收敛",
    "translated_abstract": "联邦学习（FL）是一种分布式机器学习技术，多个客户端合作训练共享模型而不交换原始数据。然而，不同客户端数据分布的异质性通常导致模型推断能力不佳。本文提出一种基于原型的联邦学习框架，而只需对典型的联邦学习过程的最后一个全局迭代进行少量更改，即可实现更好的推断性能。在最后一次迭代中，服务器聚合从分布式客户端传输的原型，然后将其发送回本地客户端，以用于各自的模型推断。在两个基准数据集上的实验表明，我们的提议在不同的异质性设置下，可以实现比两个流行基准更高的准确性（至少1％）和相对高效的通信。",
    "tldr": "本文提出一种基于原型的联邦学习框架，可以在只对联邦学习进行少量更改的情况下，提高模型推断的性能和精度，并实现高效的通信。",
    "en_tdlr": "This paper proposes a prototype-based federated learning framework, which can achieve better inference performance and accuracy with only a few changes to the typical federated learning process, and also achieve relatively efficient communication."
}