{
    "title": "No Fear of Classifier Biases: Neural Collapse Inspired Federated Learning with Synthetic and Fixed Classifier. (arXiv:2303.10058v1 [cs.LG])",
    "abstract": "Data heterogeneity is an inherent challenge that hinders the performance of federated learning (FL). Recent studies have identified the biased classifiers of local models as the key bottleneck. Previous attempts have used classifier calibration after FL training, but this approach falls short in improving the poor feature representations caused by training-time classifier biases. Resolving the classifier bias dilemma in FL requires a full understanding of the mechanisms behind the classifier. Recent advances in neural collapse have shown that the classifiers and feature prototypes under perfect training scenarios collapse into an optimal structure called simplex equiangular tight frame (ETF). Building on this neural collapse insight, we propose a solution to the FL's classifier bias problem by utilizing a synthetic and fixed ETF classifier during training. The optimal classifier structure enables all clients to learn unified and optimal feature representations even under extremely hete",
    "link": "http://arxiv.org/abs/2303.10058",
    "context": "Title: No Fear of Classifier Biases: Neural Collapse Inspired Federated Learning with Synthetic and Fixed Classifier. (arXiv:2303.10058v1 [cs.LG])\nAbstract: Data heterogeneity is an inherent challenge that hinders the performance of federated learning (FL). Recent studies have identified the biased classifiers of local models as the key bottleneck. Previous attempts have used classifier calibration after FL training, but this approach falls short in improving the poor feature representations caused by training-time classifier biases. Resolving the classifier bias dilemma in FL requires a full understanding of the mechanisms behind the classifier. Recent advances in neural collapse have shown that the classifiers and feature prototypes under perfect training scenarios collapse into an optimal structure called simplex equiangular tight frame (ETF). Building on this neural collapse insight, we propose a solution to the FL's classifier bias problem by utilizing a synthetic and fixed ETF classifier during training. The optimal classifier structure enables all clients to learn unified and optimal feature representations even under extremely hete",
    "path": "papers/23/03/2303.10058.json",
    "total_tokens": 883,
    "translated_title": "不怕分类器偏差：以神经崩溃为灵感的合作学习中使用合成和固定分类器",
    "translated_abstract": "数据异构性是困扰合作学习性能的内在挑战。最近的研究已经确定了本地模型的偏置分类器是关键瓶颈。以前的尝试利用FL训练后进行分类器校准，但这种方法未能改善训练时分类器偏差导致的差劣特征表示。解决FL中分类器偏差困境需要充分理解分类器背后的机制。神经崩溃的最新进展表明，在完美的训练场景下，分类器和特征原型崩溃为一种称为simplex equiangular tight frame(ETF)的最优结构。基于这种神经崩溃的见解，我们提出了一种解决FL分类器偏差问题的解决方案，即在训练过程中利用合成和固定的ETF分类器。最优分类器结构使得所有客户端甚至在极端异构数据下也能学到统一的和最优的特征表示。",
    "tldr": "本文提出一种解决合作学习中分类器偏差问题的方案，即在训练过程中使用合成的ETF分类器，使得所有客户端能够学习到统一的最优特征表示。",
    "en_tdlr": "This paper proposes a solution to the classifier bias problem in federated learning by utilizing a synthetic and fixed ETF classifier during training. The optimal classifier structure enables all clients to learn unified and optimal feature representations even under extremely heterogeneous data."
}