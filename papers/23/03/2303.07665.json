{
    "title": "RenewNAT: Renewing Potential Translation for Non-Autoregressive Transformer. (arXiv:2303.07665v1 [cs.CL])",
    "abstract": "Non-autoregressive neural machine translation (NAT) models are proposed to accelerate the inference process while maintaining relatively high performance. However, existing NAT models are difficult to achieve the desired efficiency-quality trade-off. For one thing, fully NAT models with efficient inference perform inferior to their autoregressive counterparts. For another, iterative NAT models can, though, achieve comparable performance while diminishing the advantage of speed. In this paper, we propose RenewNAT, a flexible framework with high efficiency and effectiveness, to incorporate the merits of fully and iterative NAT models. RenewNAT first generates the potential translation results and then renews them in a single pass. It can achieve significant performance improvements at the same expense as traditional NAT models (without introducing additional model parameters and decoding latency). Experimental results on various translation benchmarks (e.g., \\textbf{4} WMT) show that our",
    "link": "http://arxiv.org/abs/2303.07665",
    "context": "Title: RenewNAT: Renewing Potential Translation for Non-Autoregressive Transformer. (arXiv:2303.07665v1 [cs.CL])\nAbstract: Non-autoregressive neural machine translation (NAT) models are proposed to accelerate the inference process while maintaining relatively high performance. However, existing NAT models are difficult to achieve the desired efficiency-quality trade-off. For one thing, fully NAT models with efficient inference perform inferior to their autoregressive counterparts. For another, iterative NAT models can, though, achieve comparable performance while diminishing the advantage of speed. In this paper, we propose RenewNAT, a flexible framework with high efficiency and effectiveness, to incorporate the merits of fully and iterative NAT models. RenewNAT first generates the potential translation results and then renews them in a single pass. It can achieve significant performance improvements at the same expense as traditional NAT models (without introducing additional model parameters and decoding latency). Experimental results on various translation benchmarks (e.g., \\textbf{4} WMT) show that our",
    "path": "papers/23/03/2303.07665.json",
    "total_tokens": 915,
    "translated_title": "RenewNAT: 非自回归Transformer的更新潜在翻译",
    "translated_abstract": "非自回归神经机器翻译(NAT)模型被提出来加速推理过程同时保持相对高的性能。然而，现有的NAT模型很难实现所期望的效率-质量权衡。一方面，完全NAT模型在高效推理方面表现不及其自回归对应物。另一方面，迭代NAT模型可以实现可比的性能，但减弱了速度优势。在本文中，我们提出了RenewNAT，一个高效有效的灵活框架，结合了完全和迭代NAT模型的优点。RenewNAT首先生成潜在的翻译结果，然后在单次传递中更新它们。它可以在不引入额外的模型参数和解码延迟的情况下实现显著的性能改进。在各种翻译基准测试中的实验结果（例如，\\textbf{4} WMT）表明，我们的...",
    "tldr": "研究提出了一个非自回归神经机器翻译模型——RenewNAT，它结合了完全和迭代NAT模型的优点，通过一次传递以高效和有效的方式生成潜在翻译结果并更新它们，实现了相对于传统NAT模型的显著性能改进。",
    "en_tdlr": "The RenewNAT is a non-autoregressive neural machine translation model that combines the advantages of fully and iterative NAT models to efficiently and effectively generate and update potential translation results in a single pass, resulting in significant performance improvements relative to traditional NAT models without introducing additional model parameters and decoding latency."
}