{
    "title": "Towards provably efficient quantum algorithms for large-scale machine-learning models. (arXiv:2303.03428v2 [quant-ph] UPDATED)",
    "abstract": "Large machine learning models are revolutionary technologies of artificial intelligence whose bottlenecks include huge computational expenses, power, and time used both in the pre-training and fine-tuning process. In this work, we show that fault-tolerant quantum computing could possibly provide provably efficient resolutions for generic (stochastic) gradient descent algorithms, scaling as $\\mathcal{O}(T^2 \\times \\text{polylog}(n))$, where $n$ is the size of the models and $T$ is the number of iterations in the training, as long as the models are both sufficiently dissipative and sparse, with small learning rates. Based on earlier efficient quantum algorithms for dissipative differential equations, we find and prove that similar algorithms work for (stochastic) gradient descent, the primary algorithm for machine learning. In practice, we benchmark instances of large machine learning models from 7 million to 103 million parameters. We find that, in the context of sparse training, a quan",
    "link": "http://arxiv.org/abs/2303.03428",
    "context": "Title: Towards provably efficient quantum algorithms for large-scale machine-learning models. (arXiv:2303.03428v2 [quant-ph] UPDATED)\nAbstract: Large machine learning models are revolutionary technologies of artificial intelligence whose bottlenecks include huge computational expenses, power, and time used both in the pre-training and fine-tuning process. In this work, we show that fault-tolerant quantum computing could possibly provide provably efficient resolutions for generic (stochastic) gradient descent algorithms, scaling as $\\mathcal{O}(T^2 \\times \\text{polylog}(n))$, where $n$ is the size of the models and $T$ is the number of iterations in the training, as long as the models are both sufficiently dissipative and sparse, with small learning rates. Based on earlier efficient quantum algorithms for dissipative differential equations, we find and prove that similar algorithms work for (stochastic) gradient descent, the primary algorithm for machine learning. In practice, we benchmark instances of large machine learning models from 7 million to 103 million parameters. We find that, in the context of sparse training, a quan",
    "path": "papers/23/03/2303.03428.json",
    "total_tokens": 1035,
    "translated_title": "面向大规模机器学习模型的可证明高效量子算法",
    "translated_abstract": "大型机器学习模型是人工智能的革命性技术，其瓶颈包括巨大的计算开销、功耗和时间，既用于预训练，也用于微调过程。本研究表明，容错量子计算可能会针对通用（随机）梯度下降算法提供可证明的高效解决方案，其缩放为 $\\mathcal{O}(T^2 \\times \\text{polylog}(n))$，其中 $n$ 是模型的大小，$T$ 是训练中的迭代次数，只要模型足够耗散和稀疏，并具有较小的学习率。基于早期用于耗散微分方程的高效量子算法，我们发现并证明了类似的算法可用于（随机）梯度下降，这是机器学习的主要算法。在实践中，我们对拥有从700万到1.03亿个参数的大型机器学习模型进行了基准测试。我们发现，在稀疏训练的情况下，量子计算显然可以在一定程度上提高效率。",
    "tldr": "本论文提出了一种可能的针对通用（随机）梯度下降算法的高效量子解决方案，只要模型足够耗散和稀疏，具有小的学习率，并且可以缩放至 $O(T^2 \\times \\text{polylog}(n))$。在实践中，证明了在稀疏训练的情况下，量子计算可以显著提高效率。",
    "en_tdlr": "This paper proposes a possible efficient quantum solution for generic (stochastic) gradient descent algorithms, scaling as $\\mathcal{O}(T^2 \\times \\text{polylog}(n))$, as long as the models are sufficiently dissipative and sparse, with small learning rates. In practice, it has been shown that quantum computing can significantly improve efficiency in sparse training."
}