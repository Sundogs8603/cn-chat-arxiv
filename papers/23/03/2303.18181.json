{
    "title": "A Closer Look at Parameter-Efficient Tuning in Diffusion Models. (arXiv:2303.18181v1 [cs.CV])",
    "abstract": "Large-scale diffusion models like Stable Diffusion are powerful and find various real-world applications while customizing such models by fine-tuning is both memory and time inefficient. Motivated by the recent progress in natural language processing, we investigate parameter-efficient tuning in large diffusion models by inserting small learnable modules (termed adapters). In particular, we decompose the design space of adapters into orthogonal factors -- the input position, the output position as well as the function form, and perform Analysis of Variance (ANOVA), a classical statistical approach for analyzing the correlation between discrete (design options) and continuous variables (evaluation metrics). Our analysis suggests that the input position of adapters is the critical factor influencing the performance of downstream tasks. Then, we carefully study the choice of the input position, and we find that putting the input position after the cross-attention block can lead to the bes",
    "link": "http://arxiv.org/abs/2303.18181",
    "context": "Title: A Closer Look at Parameter-Efficient Tuning in Diffusion Models. (arXiv:2303.18181v1 [cs.CV])\nAbstract: Large-scale diffusion models like Stable Diffusion are powerful and find various real-world applications while customizing such models by fine-tuning is both memory and time inefficient. Motivated by the recent progress in natural language processing, we investigate parameter-efficient tuning in large diffusion models by inserting small learnable modules (termed adapters). In particular, we decompose the design space of adapters into orthogonal factors -- the input position, the output position as well as the function form, and perform Analysis of Variance (ANOVA), a classical statistical approach for analyzing the correlation between discrete (design options) and continuous variables (evaluation metrics). Our analysis suggests that the input position of adapters is the critical factor influencing the performance of downstream tasks. Then, we carefully study the choice of the input position, and we find that putting the input position after the cross-attention block can lead to the bes",
    "path": "papers/23/03/2303.18181.json",
    "total_tokens": 894,
    "translated_title": "关于扩散模型参数效率微调的进一步探究",
    "translated_abstract": "大规模扩散模型如Stable Diffusion在许多实际应用中都表现出足够的强大，然而在对这些模型进行个性化微调时却需要耗费大量内存和时间。受自然语言处理领域近期的进展启发，我们通过插入小型可学习模块(称作适配器)来研究大规模扩散模型的参数效率微调。具体来说，我们将适配器的设计空间分解为正交因子——输入位置、输出位置以及函数形式，并进行ANOVA分析，一种分析离散(设计选项)与连续变量(评估指标)之间相关性的经典统计方法。我们的分析表明，适配器的输入位置是影响下游任务性能的关键因素。接着，我们仔细研究了输入位置的选择，发现将输入位置放在交叉注意力块后可以使性能达到最佳。",
    "tldr": "本文研究了大规模扩散模型的参数效率微调，通过插入小型可学习模块来实现。研究表明，适配器的输入位置是影响下游任务性能的关键因素，而将输入位置放在交叉注意力块之后可获得最佳性能。"
}