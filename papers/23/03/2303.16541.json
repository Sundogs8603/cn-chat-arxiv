{
    "title": "Sounding Video Generator: A Unified Framework for Text-guided Sounding Video Generation. (arXiv:2303.16541v1 [cs.CV] CROSS LISTED)",
    "abstract": "As a combination of visual and audio signals, video is inherently multi-modal. However, existing video generation methods are primarily intended for the synthesis of visual frames, whereas audio signals in realistic videos are disregarded. In this work, we concentrate on a rarely investigated problem of text guided sounding video generation and propose the Sounding Video Generator (SVG), a unified framework for generating realistic videos along with audio signals. Specifically, we present the SVG-VQGAN to transform visual frames and audio melspectrograms into discrete tokens. SVG-VQGAN applies a novel hybrid contrastive learning method to model inter-modal and intra-modal consistency and improve the quantized representations. A cross-modal attention module is employed to extract associated features of visual frames and audio signals for contrastive learning. Then, a Transformer-based decoder is used to model associations between texts, visual frames, and audio signals at token level fo",
    "link": "http://arxiv.org/abs/2303.16541",
    "context": "Title: Sounding Video Generator: A Unified Framework for Text-guided Sounding Video Generation. (arXiv:2303.16541v1 [cs.CV] CROSS LISTED)\nAbstract: As a combination of visual and audio signals, video is inherently multi-modal. However, existing video generation methods are primarily intended for the synthesis of visual frames, whereas audio signals in realistic videos are disregarded. In this work, we concentrate on a rarely investigated problem of text guided sounding video generation and propose the Sounding Video Generator (SVG), a unified framework for generating realistic videos along with audio signals. Specifically, we present the SVG-VQGAN to transform visual frames and audio melspectrograms into discrete tokens. SVG-VQGAN applies a novel hybrid contrastive learning method to model inter-modal and intra-modal consistency and improve the quantized representations. A cross-modal attention module is employed to extract associated features of visual frames and audio signals for contrastive learning. Then, a Transformer-based decoder is used to model associations between texts, visual frames, and audio signals at token level fo",
    "path": "papers/23/03/2303.16541.json",
    "total_tokens": 906,
    "tldr": "本文提出了一种用于文本引导的声音视频生成的框架，通过采用混合对比学习和跨模态注意力模块来生成逼真的视频和音频信号，并使用Transformer解码器在令牌级别上建模它们之间的关联。"
}