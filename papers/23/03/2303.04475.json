{
    "title": "RACCER: Towards Reachable and Certain Counterfactual Explanations for Reinforcement Learning. (arXiv:2303.04475v2 [cs.AI] UPDATED)",
    "abstract": "While reinforcement learning (RL) algorithms have been successfully applied to numerous tasks, their reliance on neural networks makes their behavior difficult to understand and trust. Counterfactual explanations are human-friendly explanations that offer users actionable advice on how to alter the model inputs to achieve the desired output from a black-box system. However, current approaches to generating counterfactuals in RL ignore the stochastic and sequential nature of RL tasks and can produce counterfactuals that are difficult to obtain or do not deliver the desired outcome. In this work, we propose RACCER, the first RL-specific approach to generating counterfactual explanations for the behavior of RL agents. We first propose and implement a set of RL-specific counterfactual properties that ensure easily reachable counterfactuals with highly probable desired outcomes. We use a heuristic tree search of the agent's execution trajectories to find the most suitable counterfactuals ba",
    "link": "http://arxiv.org/abs/2303.04475",
    "context": "Title: RACCER: Towards Reachable and Certain Counterfactual Explanations for Reinforcement Learning. (arXiv:2303.04475v2 [cs.AI] UPDATED)\nAbstract: While reinforcement learning (RL) algorithms have been successfully applied to numerous tasks, their reliance on neural networks makes their behavior difficult to understand and trust. Counterfactual explanations are human-friendly explanations that offer users actionable advice on how to alter the model inputs to achieve the desired output from a black-box system. However, current approaches to generating counterfactuals in RL ignore the stochastic and sequential nature of RL tasks and can produce counterfactuals that are difficult to obtain or do not deliver the desired outcome. In this work, we propose RACCER, the first RL-specific approach to generating counterfactual explanations for the behavior of RL agents. We first propose and implement a set of RL-specific counterfactual properties that ensure easily reachable counterfactuals with highly probable desired outcomes. We use a heuristic tree search of the agent's execution trajectories to find the most suitable counterfactuals ba",
    "path": "papers/23/03/2303.04475.json",
    "total_tokens": 893,
    "translated_title": "RACCER: 面向可达和确证的强化学习可追溯解释",
    "translated_abstract": "尽管强化学习算法在许多任务上取得了成功，但其对神经网络的依赖使其行为难以理解和信任。对抗事实解释是一种人性化的解释，为用户提供了关于如何改变模型输入以达到预期输出的可行建议。然而，当前在强化学习中生成对抗事实的方法忽略了强化学习任务的随机性和顺序性，可能产生难以获得或无法实现预期结果的对抗事实。在这项工作中，我们提出了RACCER，这是一种针对强化学习智能体行为生成对抗事实解释的首个专用方法。我们首先提出并实现了一组针对强化学习的特定对抗事实属性，确保易于实现且具有高概率预期结果的对抗事实。我们使用启发式树搜索代理执行轨迹，以找到最合适的对抗事实。",
    "tldr": "RACCER是第一个针对强化学习智能体行为生成对抗事实解释的专用方法，通过使用一组针对强化学习的特定对抗事实属性，保证易于实现且具有高概率预期结果的对抗事实。",
    "en_tdlr": "RACCER is the first RL-specific approach to generating counterfactual explanations for the behavior of RL agents, ensuring easily reachable counterfactuals with highly probable desired outcomes by using a set of RL-specific counterfactual properties."
}