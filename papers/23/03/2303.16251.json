{
    "title": "Function Approximation with Randomly Initialized Neural Networks for Approximate Model Reference Adaptive Control. (arXiv:2303.16251v1 [math.OC])",
    "abstract": "Classical results in neural network approximation theory show how arbitrary continuous functions can be approximated by networks with a single hidden layer, under mild assumptions on the activation function. However, the classical theory does not give a constructive means to generate the network parameters that achieve a desired accuracy. Recent results have demonstrated that for specialized activation functions, such as ReLUs and some classes of analytic functions, high accuracy can be achieved via linear combinations of randomly initialized activations. These recent works utilize specialized integral representations of target functions that depend on the specific activation functions used. This paper defines mollified integral representations, which provide a means to form integral representations of target functions using activations for which no direct integral representation is currently known. The new construction enables approximation guarantees for randomly initialized networks",
    "link": "http://arxiv.org/abs/2303.16251",
    "context": "Title: Function Approximation with Randomly Initialized Neural Networks for Approximate Model Reference Adaptive Control. (arXiv:2303.16251v1 [math.OC])\nAbstract: Classical results in neural network approximation theory show how arbitrary continuous functions can be approximated by networks with a single hidden layer, under mild assumptions on the activation function. However, the classical theory does not give a constructive means to generate the network parameters that achieve a desired accuracy. Recent results have demonstrated that for specialized activation functions, such as ReLUs and some classes of analytic functions, high accuracy can be achieved via linear combinations of randomly initialized activations. These recent works utilize specialized integral representations of target functions that depend on the specific activation functions used. This paper defines mollified integral representations, which provide a means to form integral representations of target functions using activations for which no direct integral representation is currently known. The new construction enables approximation guarantees for randomly initialized networks",
    "path": "papers/23/03/2303.16251.json",
    "total_tokens": 818,
    "translated_title": "随机初始化神经网络进行模型参考自适应控制的函数逼近",
    "translated_abstract": "神经网络逼近理论中的经典结果表明，在激活函数满足某些温和的假设条件下，可以通过单层隐藏层的网络逼近任意连续函数。然而，经典理论并没有给出一种构造性方法来生成网络参数以实现所需的精度。最近的研究结果表明，对于专门的激活函数（如ReLU函数和某些类的解析函数），可以通过随机初始化激活的线性组合实现高精度逼近。这些最近的工作利用了特殊的积分表示，这些表示依赖于所使用的具体的激活函数。本文定义了平滑的积分表示，提供了一种使用激活形成目标函数积分表示的方法，而对于这些激活，目前还没有直接的积分表示。新的构造使得可以为随机初始化网络提供逼近保证。",
    "tldr": "本论文提出一种新的构造方法——平滑的积分表示，使得可以使用随机初始化的神经网络保证逼近精度，并且可以适用于更广泛的激活函数。",
    "en_tdlr": "This paper proposes a new construction method, namely the mollified integral representations, which provides a means to use randomly initialized neural networks to guarantee approximation accuracy and is applicable to a wider range of activation functions."
}