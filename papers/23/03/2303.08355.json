{
    "title": "Efficient and Secure Federated Learning for Financial Applications. (arXiv:2303.08355v1 [cs.LG])",
    "abstract": "The conventional machine learning (ML) and deep learning approaches need to share customers' sensitive information with an external credit bureau to generate a prediction model that opens the door to privacy leakage. This leakage risk makes financial companies face an enormous challenge in their cooperation. Federated learning is a machine learning setting that can protect data privacy, but the high communication cost is often the bottleneck of the federated systems, especially for large neural networks. Limiting the number and size of communications is necessary for the practical training of large neural structures. Gradient sparsification has received increasing attention as a method to reduce communication cost, which only updates significant gradients and accumulates insignificant gradients locally. However, the secure aggregation framework cannot directly use gradient sparsification. This article proposes two sparsification methods to reduce communication cost in federated learnin",
    "link": "http://arxiv.org/abs/2303.08355",
    "context": "Title: Efficient and Secure Federated Learning for Financial Applications. (arXiv:2303.08355v1 [cs.LG])\nAbstract: The conventional machine learning (ML) and deep learning approaches need to share customers' sensitive information with an external credit bureau to generate a prediction model that opens the door to privacy leakage. This leakage risk makes financial companies face an enormous challenge in their cooperation. Federated learning is a machine learning setting that can protect data privacy, but the high communication cost is often the bottleneck of the federated systems, especially for large neural networks. Limiting the number and size of communications is necessary for the practical training of large neural structures. Gradient sparsification has received increasing attention as a method to reduce communication cost, which only updates significant gradients and accumulates insignificant gradients locally. However, the secure aggregation framework cannot directly use gradient sparsification. This article proposes two sparsification methods to reduce communication cost in federated learnin",
    "path": "papers/23/03/2303.08355.json",
    "total_tokens": 959,
    "translated_title": "金融应用的高效安全联合学习",
    "translated_abstract": "传统的机器学习和深度学习方法需要与外部征信局共享客户的敏感信息来生成预测模型，从而导致隐私泄露的风险。联合学习是一种可以保护数据隐私的机器学习设置，但高通信成本经常成为联合系统的瓶颈，特别是对于大型神经网络而言。本文提出了两种稀疏化方法来降低联合学习中的通信成本，即Top-K梯度稀疏化和基于Delta的稀疏化。此外，本文提出了一种隐私保护和安全的聚合框架，用于聚合梯度更新，该框架可以在进行聚合时保护每个参与者的数据隐私。实验结果表明，这两种稀疏化方法可以在保持期望的收敛率和预测精度的同时，减少多达90%的通信开销。",
    "tldr": "本文提出了一种金融应用的高效安全联合学习方法，其中包括Top-K梯度稀疏化和基于Delta的稀疏化，以及一种可保护数据隐私的聚合框架，这些方法可以大幅降低通信成本并保证预测精度。",
    "en_tdlr": "This article proposes an efficient and secure federated learning approach for financial applications, which includes Top-K gradient sparsification and delta-based sparsification, and a privacy-preserving and secure aggregation framework, which can significantly reduce communication overhead while maintaining prediction accuracy."
}