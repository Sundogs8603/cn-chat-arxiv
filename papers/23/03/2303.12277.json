{
    "title": "Stochastic Nonsmooth Convex Optimization with Heavy-Tailed Noises. (arXiv:2303.12277v1 [math.OC])",
    "abstract": "Recently, several studies consider the stochastic optimization problem but in a heavy-tailed noise regime, i.e., the difference between the stochastic gradient and the true gradient is assumed to have a finite $p$-th moment (say being upper bounded by $\\sigma^{p}$ for some $\\sigma\\geq0$) where $p\\in(1,2]$, which not only generalizes the traditional finite variance assumption ($p=2$) but also has been observed in practice for several different tasks. Under this challenging assumption, lots of new progress has been made for either convex or nonconvex problems, however, most of which only consider smooth objectives. In contrast, people have not fully explored and well understood this problem when functions are nonsmooth. This paper aims to fill this crucial gap by providing a comprehensive analysis of stochastic nonsmooth convex optimization with heavy-tailed noises. We revisit a simple clipping-based algorithm, whereas, which is only proved to converge in expectation but under the additi",
    "link": "http://arxiv.org/abs/2303.12277",
    "context": "Title: Stochastic Nonsmooth Convex Optimization with Heavy-Tailed Noises. (arXiv:2303.12277v1 [math.OC])\nAbstract: Recently, several studies consider the stochastic optimization problem but in a heavy-tailed noise regime, i.e., the difference between the stochastic gradient and the true gradient is assumed to have a finite $p$-th moment (say being upper bounded by $\\sigma^{p}$ for some $\\sigma\\geq0$) where $p\\in(1,2]$, which not only generalizes the traditional finite variance assumption ($p=2$) but also has been observed in practice for several different tasks. Under this challenging assumption, lots of new progress has been made for either convex or nonconvex problems, however, most of which only consider smooth objectives. In contrast, people have not fully explored and well understood this problem when functions are nonsmooth. This paper aims to fill this crucial gap by providing a comprehensive analysis of stochastic nonsmooth convex optimization with heavy-tailed noises. We revisit a simple clipping-based algorithm, whereas, which is only proved to converge in expectation but under the additi",
    "path": "papers/23/03/2303.12277.json",
    "total_tokens": 916,
    "translated_title": "带有重尾噪声的随机非光滑凸优化",
    "translated_abstract": "最近，一些研究将随机优化问题考虑在重尾噪声范式下，即假设随机梯度和真实梯度之间的差异具有有限的 $p$ 阶矩（例如被某个 $\\sigma \\geq0$ 上界限制为 $\\sigma^{p}$），其中 $p\\in (1,2]$，这不仅泛化了传统的有限方差假设（$p=2$），而且在许多不同的任务中都被观察到。在这个具有挑战性的假设下，针对凸或非凸问题已经取得了很多新进展，然而，大多数只考虑光滑的目标函数。相反，在函数非光滑时，人们尚未充分探索并完全理解这个问题。本文旨在通过对带有重尾噪声的随机非光滑凸优化提供全面分析来填补这一关键空白。我们重新考虑了一个简单的基于裁剪的算法，然而，这个算法只被证明能以期望方式收敛，但在附加",
    "tldr": "本文分析了具有重尾噪声的随机非光滑凸优化问题，并填补了在函数非光滑场景下的研究空白。"
}