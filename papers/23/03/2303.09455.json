{
    "title": "Learning Cross-lingual Visual Speech Representations. (arXiv:2303.09455v1 [cs.CL])",
    "abstract": "Cross-lingual self-supervised learning has been a growing research topic in the last few years. However, current works only explored the use of audio signals to create representations. In this work, we study cross-lingual self-supervised visual representation learning. We use the recently-proposed Raw Audio-Visual Speech Encoders (RAVEn) framework to pre-train an audio-visual model with unlabelled multilingual data, and then fine-tune the visual model on labelled transcriptions. Our experiments show that: (1) multi-lingual models with more data outperform monolingual ones, but, when keeping the amount of data fixed, monolingual models tend to reach better performance; (2) multi-lingual outperforms English-only pre-training; (3) using languages which are more similar yields better results; and (4) fine-tuning on unseen languages is competitive to using the target language in the pre-training set. We hope our study inspires future research on non-English-only speech representation learni",
    "link": "http://arxiv.org/abs/2303.09455",
    "context": "Title: Learning Cross-lingual Visual Speech Representations. (arXiv:2303.09455v1 [cs.CL])\nAbstract: Cross-lingual self-supervised learning has been a growing research topic in the last few years. However, current works only explored the use of audio signals to create representations. In this work, we study cross-lingual self-supervised visual representation learning. We use the recently-proposed Raw Audio-Visual Speech Encoders (RAVEn) framework to pre-train an audio-visual model with unlabelled multilingual data, and then fine-tune the visual model on labelled transcriptions. Our experiments show that: (1) multi-lingual models with more data outperform monolingual ones, but, when keeping the amount of data fixed, monolingual models tend to reach better performance; (2) multi-lingual outperforms English-only pre-training; (3) using languages which are more similar yields better results; and (4) fine-tuning on unseen languages is competitive to using the target language in the pre-training set. We hope our study inspires future research on non-English-only speech representation learni",
    "path": "papers/23/03/2303.09455.json",
    "total_tokens": 985,
    "translated_title": "跨语言视觉语音表示的学习",
    "translated_abstract": "跨语言自监督学习是近年来逐渐流行的研究课题。当前相关工作仅限于利用音频信号进行表示学习。在本工作中，我们研究跨语言自监督的视觉表示学习。我们利用最近提出的RAVEn框架，对未标记的多语种数据进行音频-视觉预训练，然后在标记的转录上对视觉模型进行微调。我们的实验证明：（1）具有更多数据的多语种模型优于单语种模型，但当数据量固定时，单语种模型往往达到更好的性能；（2）多语种优于仅英语预训练；（3）使用更相似的语言可以得到更好的结果；（4）在看不见的语言上进行微调与在预训练集中使用目标语言相当竞争力。我们希望我们的研究能启发未来关于非英语语音表示学习的研究。",
    "tldr": "本研究使用跨语言自监督学习方法，利用未标记的多语种数据进行音频-视觉预训练，然后在标记的转录上对视觉模型进行微调，实验证明多语种模型性能优越，使用更相似的语言可以得到更好的结果，同时在看不见的语言上进行微调竞争力相当。",
    "en_tdlr": "This paper proposes a cross-lingual self-supervised learning method to pre-train an audio-visual model with unlabelled multilingual data and fine-tune the visual model on labelled transcriptions. The experiments show that multi-lingual models outperform monolingual ones with more data, and using more similar languages yields better results. Fine-tuning on unseen languages is competitive to using the target language in the pre-training set."
}