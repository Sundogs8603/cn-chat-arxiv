{
    "title": "On the Risks of Stealing the Decoding Algorithms of Language Models. (arXiv:2303.04729v3 [cs.LG] UPDATED)",
    "abstract": "A key component of generating text from modern language models (LM) is the selection and tuning of decoding algorithms. These algorithms determine how to generate text from the internal probability distribution generated by the LM. The process of choosing a decoding algorithm and tuning its hyperparameters takes significant time, manual effort, and computation, and it also requires extensive human evaluation. Therefore, the identity and hyperparameters of such decoding algorithms are considered to be extremely valuable to their owners. In this work, we show, for the first time, that an adversary with typical API access to an LM can steal the type and hyperparameters of its decoding algorithms at very low monetary costs. Our attack is effective against popular LMs used in text generation APIs, including GPT-2 and GPT-3. We demonstrate the feasibility of stealing such information with only a few dollars, e.g., $\\$0.8$, $\\$1$, $\\$4$, and $\\$40$ for the four versions of GPT-3.",
    "link": "http://arxiv.org/abs/2303.04729",
    "context": "Title: On the Risks of Stealing the Decoding Algorithms of Language Models. (arXiv:2303.04729v3 [cs.LG] UPDATED)\nAbstract: A key component of generating text from modern language models (LM) is the selection and tuning of decoding algorithms. These algorithms determine how to generate text from the internal probability distribution generated by the LM. The process of choosing a decoding algorithm and tuning its hyperparameters takes significant time, manual effort, and computation, and it also requires extensive human evaluation. Therefore, the identity and hyperparameters of such decoding algorithms are considered to be extremely valuable to their owners. In this work, we show, for the first time, that an adversary with typical API access to an LM can steal the type and hyperparameters of its decoding algorithms at very low monetary costs. Our attack is effective against popular LMs used in text generation APIs, including GPT-2 and GPT-3. We demonstrate the feasibility of stealing such information with only a few dollars, e.g., $\\$0.8$, $\\$1$, $\\$4$, and $\\$40$ for the four versions of GPT-3.",
    "path": "papers/23/03/2303.04729.json",
    "total_tokens": 845,
    "translated_title": "论盗用语言模型解码算法的风险",
    "translated_abstract": "现代语言模型（LM）生成文本的关键组成部分是选择和调整解码算法。这些算法确定如何从LM生成的内部概率分布中生成文本。选择解码算法并调整其超参数的过程需要显著的时间、手动工作和计算，还需要进行广泛的人类评估。因此，解码算法的身份和超参数被认为是极其有价值的。在这项工作中，我们首次展示了一个拥有典型API访问权限的对手可以以极低的金钱成本窃取其解码算法的类型和超参数。我们的攻击对用于文本生成API的流行LM有效，包括GPT-2和GPT-3。我们证明了只需花费几美元，例如0.8美元、1美元、4美元和40美元，就可以盗取此类信息。",
    "tldr": "这项工作首次展示，一个拥有典型API访问权限的对手可以以极低的金钱成本窃取GPT-2和GPT-3等LM的解码算法的类型和超参数。",
    "en_tdlr": "This work demonstrates for the first time that an adversary with typical API access to popular LMs such as GPT-2 and GPT-3 can steal the type and hyperparameters of their decoding algorithms at very low monetary costs."
}