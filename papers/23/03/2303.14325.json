{
    "title": "Backdoor Attacks with Input-unique Triggers in NLP. (arXiv:2303.14325v1 [cs.CL])",
    "abstract": "Backdoor attack aims at inducing neural models to make incorrect predictions for poison data while keeping predictions on the clean dataset unchanged, which creates a considerable threat to current natural language processing (NLP) systems. Existing backdoor attacking systems face two severe issues:firstly, most backdoor triggers follow a uniform and usually input-independent pattern, e.g., insertion of specific trigger words, synonym replacement. This significantly hinders the stealthiness of the attacking model, leading the trained backdoor model being easily identified as malicious by model probes. Secondly, trigger-inserted poisoned sentences are usually disfluent, ungrammatical, or even change the semantic meaning from the original sentence, making them being easily filtered in the pre-processing stage. To resolve these two issues, in this paper, we propose an input-unique backdoor attack(NURA), where we generate backdoor triggers unique to inputs. IDBA generates context-related t",
    "link": "http://arxiv.org/abs/2303.14325",
    "context": "Title: Backdoor Attacks with Input-unique Triggers in NLP. (arXiv:2303.14325v1 [cs.CL])\nAbstract: Backdoor attack aims at inducing neural models to make incorrect predictions for poison data while keeping predictions on the clean dataset unchanged, which creates a considerable threat to current natural language processing (NLP) systems. Existing backdoor attacking systems face two severe issues:firstly, most backdoor triggers follow a uniform and usually input-independent pattern, e.g., insertion of specific trigger words, synonym replacement. This significantly hinders the stealthiness of the attacking model, leading the trained backdoor model being easily identified as malicious by model probes. Secondly, trigger-inserted poisoned sentences are usually disfluent, ungrammatical, or even change the semantic meaning from the original sentence, making them being easily filtered in the pre-processing stage. To resolve these two issues, in this paper, we propose an input-unique backdoor attack(NURA), where we generate backdoor triggers unique to inputs. IDBA generates context-related t",
    "path": "papers/23/03/2303.14325.json",
    "total_tokens": 897,
    "translated_abstract": "后门攻击旨在诱导神经模型对有毒数据做出不正确的预测，同时保持对于干净数据集的预测不变，这给当前的自然语言处理（NLP）系统带来了相当大的威胁。现有的后门攻击系统面临两个严重问题：首先，大多数后门触发器遵循统一且通常是与输入无关的模式，例如插入特定触发词，同义词替换。这严重阻碍了攻击模型的潜伏性，导致受过训练的后门模型很容易被模型探测器识别为恶意的。其次，插入触发器的有毒句子通常是不流利的、不符合语法的，甚至改变了原始句子的语义含义，这使得它们很容易在预处理阶段被过滤掉。为了解决这两个问题，本文提出了一种基于输入唯一触发器的后门攻击（NURA），其中我们生成了与输入相关的后门触发器。",
    "tldr": "本论文提出了一种新的后门攻击方法，名为NURA，通过生成与输入相关的触发器，解决了现有方法中触发器通用性和生成有毒句子低效的问题。",
    "en_tdlr": "This paper proposes a new backdoor attack method called NURA, which generates input-specific triggers to tackle the issues of trigger uniformity and inefficiency in generating toxic sentences in existing methods."
}