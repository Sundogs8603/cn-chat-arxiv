{
    "title": "Do Backdoors Assist Membership Inference Attacks?. (arXiv:2303.12589v1 [cs.CR])",
    "abstract": "When an adversary provides poison samples to a machine learning model, privacy leakage, such as membership inference attacks that infer whether a sample was included in the training of the model, becomes effective by moving the sample to an outlier. However, the attacks can be detected because inference accuracy deteriorates due to poison samples. In this paper, we discuss a \\textit{backdoor-assisted membership inference attack}, a novel membership inference attack based on backdoors that return the adversary's expected output for a triggered sample. We found three crucial insights through experiments with an academic benchmark dataset. We first demonstrate that the backdoor-assisted membership inference attack is unsuccessful. Second, when we analyzed loss distributions to understand the reason for the unsuccessful results, we found that backdoors cannot separate loss distributions of training and non-training samples. In other words, backdoors cannot affect the distribution of clean ",
    "link": "http://arxiv.org/abs/2303.12589",
    "context": "Title: Do Backdoors Assist Membership Inference Attacks?. (arXiv:2303.12589v1 [cs.CR])\nAbstract: When an adversary provides poison samples to a machine learning model, privacy leakage, such as membership inference attacks that infer whether a sample was included in the training of the model, becomes effective by moving the sample to an outlier. However, the attacks can be detected because inference accuracy deteriorates due to poison samples. In this paper, we discuss a \\textit{backdoor-assisted membership inference attack}, a novel membership inference attack based on backdoors that return the adversary's expected output for a triggered sample. We found three crucial insights through experiments with an academic benchmark dataset. We first demonstrate that the backdoor-assisted membership inference attack is unsuccessful. Second, when we analyzed loss distributions to understand the reason for the unsuccessful results, we found that backdoors cannot separate loss distributions of training and non-training samples. In other words, backdoors cannot affect the distribution of clean ",
    "path": "papers/23/03/2303.12589.json",
    "total_tokens": 836,
    "translated_title": "后门是否有助于成员推理攻击？",
    "translated_abstract": "当对机器学习模型提供了毒瘤样本时，数据隐私可能会泄漏，例如会员推理攻击会推断样本是否包含在模型的训练之中，从而有效地将样本移动到一个异常值之中。然而，攻击可能会被检测到，因为由于毒瘤样本而导致的推断准确性下降。本文中，我们讨论了一种基于后门的新型成员推理攻击，其通过后门返回攻击者预期的输出来进行攻击。通过对一个学术基准数据集的实验，我们发现了三个关键的见解。首先，我们证明了后门辅助的成员推理攻击是不成功的。其次，当我们分析损失分布以了解不成功结果的原因时，我们发现后门不能分离训练和非训练样本的损失分布。换句话说，后门不能影响干净样本的分布。",
    "tldr": "本文探讨了一种基于后门的新型成员推理攻击，通过实验发现后门并不能成功进行攻击，因为后门不能分离训练和非训练样本的损失分布。",
    "en_tdlr": "This paper discusses a novel membership inference attack based on backdoors, but experiments show that it is unsuccessful because backdoors cannot separate loss distributions of training and non-training samples."
}