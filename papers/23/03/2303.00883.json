{
    "title": "Variance-reduced Clipping for Non-convex Optimization. (arXiv:2303.00883v2 [cs.LG] UPDATED)",
    "abstract": "Gradient clipping is a standard training technique used in deep learning applications such as large-scale language modeling to mitigate exploding gradients. Recent experimental studies have demonstrated a fairly special behavior in the smoothness of the training objective along its trajectory when trained with gradient clipping. That is, the smoothness grows with the gradient norm. This is in clear contrast to the well-established assumption in folklore non-convex optimization, a.k.a. $L$--smoothness, where the smoothness is assumed to be bounded by a constant $L$ globally. The recently introduced $(L_0,L_1)$--smoothness is a more relaxed notion that captures such behavior in non-convex optimization. In particular, it has been shown that under this relaxed smoothness assumption, SGD with clipping requires $O(\\epsilon^{-4})$ stochastic gradient computations to find an $\\epsilon$--stationary solution. In this paper, we employ a variance reduction technique, namely SPIDER, and demonstrate",
    "link": "http://arxiv.org/abs/2303.00883",
    "context": "Title: Variance-reduced Clipping for Non-convex Optimization. (arXiv:2303.00883v2 [cs.LG] UPDATED)\nAbstract: Gradient clipping is a standard training technique used in deep learning applications such as large-scale language modeling to mitigate exploding gradients. Recent experimental studies have demonstrated a fairly special behavior in the smoothness of the training objective along its trajectory when trained with gradient clipping. That is, the smoothness grows with the gradient norm. This is in clear contrast to the well-established assumption in folklore non-convex optimization, a.k.a. $L$--smoothness, where the smoothness is assumed to be bounded by a constant $L$ globally. The recently introduced $(L_0,L_1)$--smoothness is a more relaxed notion that captures such behavior in non-convex optimization. In particular, it has been shown that under this relaxed smoothness assumption, SGD with clipping requires $O(\\epsilon^{-4})$ stochastic gradient computations to find an $\\epsilon$--stationary solution. In this paper, we employ a variance reduction technique, namely SPIDER, and demonstrate",
    "path": "papers/23/03/2303.00883.json",
    "total_tokens": 913,
    "translated_title": "非凸优化中的方差缩减裁剪",
    "translated_abstract": "梯度裁剪是深度学习应用中的标准训练技术，用于减轻梯度爆炸等问题，最近的实验研究表明，当使用梯度裁剪进行训练时，训练目标沿着其轨迹的平滑性具有一种特殊的行为，即平滑性随着梯度范数增长而增长。这与民间非凸优化中广泛流传的$L$-平滑假设形成明显对比，即全局平滑性被假定为由常数$L$上界。最近引入的$(L_0,L_1)$-平滑度是一个更放松的概念，它捕捉到非凸优化中的这种特征。特别是，在这种放松的平滑性假设下，在SGD裁剪的情况下需要$O(\\epsilon^{-4})$随机梯度计算才能找到一个$\\epsilon$-稳定解。本文采用方差缩减技术SPIDER，并演示如何在理论上分析该方法的性质。",
    "tldr": "本文提出了一种非凸优化中的方差缩减裁剪方法SPIDER，可以实现在较少的随机梯度计算次数下找到一个较稳定的解决方案。",
    "en_tdlr": "This paper proposes a variance-reduced clipping method, SPIDER, for non-convex optimization, which can find a relatively stable solution with fewer stochastic gradient computations."
}