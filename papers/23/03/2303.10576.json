{
    "title": "Efficiently Counting Substructures by Subgraph GNNs without Running GNN on Subgraphs. (arXiv:2303.10576v1 [cs.LG])",
    "abstract": "Using graph neural networks (GNNs) to approximate specific functions such as counting graph substructures is a recent trend in graph learning. Among these works, a popular way is to use subgraph GNNs, which decompose the input graph into a collection of subgraphs and enhance the representation of the graph by applying GNN to individual subgraphs. Although subgraph GNNs are able to count complicated substructures, they suffer from high computational and memory costs. In this paper, we address a non-trivial question: can we count substructures efficiently with GNNs? To answer the question, we first theoretically show that the distance to the rooted nodes within subgraphs is key to boosting the counting power of subgraph GNNs. We then encode such information into structural embeddings, and precompute the embeddings to avoid extracting information over all subgraphs via GNNs repeatedly. Experiments on various benchmarks show that the proposed model can preserve the counting power of subgra",
    "link": "http://arxiv.org/abs/2303.10576",
    "context": "Title: Efficiently Counting Substructures by Subgraph GNNs without Running GNN on Subgraphs. (arXiv:2303.10576v1 [cs.LG])\nAbstract: Using graph neural networks (GNNs) to approximate specific functions such as counting graph substructures is a recent trend in graph learning. Among these works, a popular way is to use subgraph GNNs, which decompose the input graph into a collection of subgraphs and enhance the representation of the graph by applying GNN to individual subgraphs. Although subgraph GNNs are able to count complicated substructures, they suffer from high computational and memory costs. In this paper, we address a non-trivial question: can we count substructures efficiently with GNNs? To answer the question, we first theoretically show that the distance to the rooted nodes within subgraphs is key to boosting the counting power of subgraph GNNs. We then encode such information into structural embeddings, and precompute the embeddings to avoid extracting information over all subgraphs via GNNs repeatedly. Experiments on various benchmarks show that the proposed model can preserve the counting power of subgra",
    "path": "papers/23/03/2303.10576.json",
    "total_tokens": 910,
    "translated_title": "不需要在子图上运行 GNN，使用子图 GNN 高效计数子结构",
    "translated_abstract": "近来，在图学习中使用图神经网络 (GNN) 来近似计算特定函数，如计数图的子结构，是一个热门趋势。在这些工作中，一种常用的方法是使用子图 GNN，将输入图分解为一系列子图，并通过对每个子图应用 GNN 来增强图的表示。尽管子图 GNN 能够计数复杂的子结构，但它们会遭受高计算和内存成本的困扰。本文提出了一个非常规的问题：我们是否能够使用 GNN 高效地计数子结构？为了回答这个问题，我们首先理论上证明，在子图中到根节点的距离是提高子图 GNN 计数能力的关键。然后，我们将这种信息编码为结构嵌入，并预先计算这些嵌入，以避免通过 GNN 反复提取所有子图中的信息。在各种基准测试上的实验表明，所提出的模型可以保持子图 GNN 的计数能力，同时显著降低计算和内存成本。",
    "tldr": "本文提出了一种使用结构嵌入和预计算的方法，以减少计算和内存成本，并实现了在子图 GNN 上高效计数子结构的目的。"
}