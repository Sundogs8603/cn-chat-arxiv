{
    "title": "Safe and Sample-efficient Reinforcement Learning for Clustered Dynamic Environments. (arXiv:2303.14265v1 [cs.LG])",
    "abstract": "This study proposes a safe and sample-efficient reinforcement learning (RL) framework to address two major challenges in developing applicable RL algorithms: satisfying safety constraints and efficiently learning with limited samples. To guarantee safety in real-world complex environments, we use the safe set algorithm (SSA) to monitor and modify the nominal controls, and evaluate SSA+RL in a clustered dynamic environment which is challenging to be solved by existing RL algorithms. However, the SSA+RL framework is usually not sample-efficient especially in reward-sparse environments, which has not been addressed in previous safe RL works. To improve the learning efficiency, we propose three techniques: (1) avoiding behaving overly conservative by adapting the SSA; (2) encouraging safe exploration using random network distillation with safety constraints; (3) improving policy convergence by treating SSA as expert demonstrations and directly learn from that. The experimental results show",
    "link": "http://arxiv.org/abs/2303.14265",
    "context": "Title: Safe and Sample-efficient Reinforcement Learning for Clustered Dynamic Environments. (arXiv:2303.14265v1 [cs.LG])\nAbstract: This study proposes a safe and sample-efficient reinforcement learning (RL) framework to address two major challenges in developing applicable RL algorithms: satisfying safety constraints and efficiently learning with limited samples. To guarantee safety in real-world complex environments, we use the safe set algorithm (SSA) to monitor and modify the nominal controls, and evaluate SSA+RL in a clustered dynamic environment which is challenging to be solved by existing RL algorithms. However, the SSA+RL framework is usually not sample-efficient especially in reward-sparse environments, which has not been addressed in previous safe RL works. To improve the learning efficiency, we propose three techniques: (1) avoiding behaving overly conservative by adapting the SSA; (2) encouraging safe exploration using random network distillation with safety constraints; (3) improving policy convergence by treating SSA as expert demonstrations and directly learn from that. The experimental results show",
    "path": "papers/23/03/2303.14265.json",
    "total_tokens": 910,
    "translated_title": "面向聚类动态环境的安全、高效强化学习",
    "translated_abstract": "本研究提出了一种安全、高效的强化学习框架，以解决应用强化学习算法的两个主要挑战：满足安全约束和在有限样本下有效学习。为了在复杂的真实世界环境中保证安全性，我们使用安全集算法（SSA）来监测和修改标准控制，并在聚类动态环境下评估SSA+RL框架，这在现有的RL算法中具有挑战性。然而，SSA+RL框架通常在奖励稀疏的环境中不够高效，这在以前的安全RL工作中没有得到解决。为了提高学习效率，我们提出了三个技术：（1）通过适应SSA来避免过度保守的行为；（2）使用具有安全约束的随机网络精炼来鼓励安全探索；（3）通过将SSA视为专家演示并直接从中学习来提高策略的收敛性。实验结果显示：",
    "tldr": "本研究提出了一种安全、高效的强化学习框架，使用安全集算法监测和修改标准控制，并在聚类动态环境下实现，同时使用三项技术提高智能体的学习效率。",
    "en_tdlr": "This study proposes a safe and sample-efficient RL framework for clustered dynamic environments, using the safe set algorithm to monitor and modify the nominal controls, and improving learning efficiency with three techniques. The experimental results demonstrate its effectiveness."
}