{
    "title": "X-ReCoSa: Multi-Scale Context Aggregation For Multi-Turn Dialogue Generation. (arXiv:2303.07833v1 [cs.CL])",
    "abstract": "In multi-turn dialogue generation, responses are not only related to the topic and background of the context but also related to words and phrases in the sentences of the context. However, currently widely used hierarchical dialog models solely rely on context representations from the utterance-level encoder, ignoring the sentence representations output by the word-level encoder. This inevitably results in a loss of information while decoding and generating. In this paper, we propose a new dialog model X-ReCoSa to tackle this problem which aggregates multi-scale context information for hierarchical dialog models. Specifically, we divide the generation decoder into upper and lower parts, namely the intention part and the generation part. Firstly, the intention part takes context representations as input to generate the intention of the response. Then the generation part generates words depending on sentence representations. Therefore, the hierarchical information has been fused into res",
    "link": "http://arxiv.org/abs/2303.07833",
    "context": "Title: X-ReCoSa: Multi-Scale Context Aggregation For Multi-Turn Dialogue Generation. (arXiv:2303.07833v1 [cs.CL])\nAbstract: In multi-turn dialogue generation, responses are not only related to the topic and background of the context but also related to words and phrases in the sentences of the context. However, currently widely used hierarchical dialog models solely rely on context representations from the utterance-level encoder, ignoring the sentence representations output by the word-level encoder. This inevitably results in a loss of information while decoding and generating. In this paper, we propose a new dialog model X-ReCoSa to tackle this problem which aggregates multi-scale context information for hierarchical dialog models. Specifically, we divide the generation decoder into upper and lower parts, namely the intention part and the generation part. Firstly, the intention part takes context representations as input to generate the intention of the response. Then the generation part generates words depending on sentence representations. Therefore, the hierarchical information has been fused into res",
    "path": "papers/23/03/2303.07833.json",
    "total_tokens": 810,
    "translated_title": "X-ReCoSa: 多层次上下文聚合的多轮对话生成",
    "translated_abstract": "在多轮对话生成中，回复不仅与上下文的主题和背景有关，还与上下文中句子中的单词和短语有关。然而，目前广泛使用的分层对话模型仅依靠话语级别编码器的上下文表示，忽略了单词级别编码器的句子表示。这必然会在解码和生成的过程中丢失信息。本文提出了一种新的对话模型X-ReCoSa，以解决这个问题，该模型聚合了多层次上下文信息，适用于分层对话模型。具体来说，我们将生成解码器分为上部和下部，即意图部分和生成部分。首先，意图部分将上下文表示作为输入，生成回复的意图。然后，生成部分根据句子表示生成单词。因此，分层信息已被融合到结果中。",
    "tldr": "提出了一种新的对话模型 X-ReCoSa，实现了多层次上下文聚合，将上下文表示和句子表示结合，以提高分层对话模型的效果。"
}