{
    "title": "Treat Different Negatives Differently: Enriching Loss Functions with Domain and Range Constraints for Link Prediction. (arXiv:2303.00286v3 [cs.LG] UPDATED)",
    "abstract": "Knowledge graph embedding models (KGEMs) are used for various tasks related to knowledge graphs (KGs), including link prediction. They are trained with loss functions that are computed considering a batch of scored triples and their corresponding labels. Traditional approaches consider the label of a triple to be either true or false. However, recent works suggest that all negative triples should not be valued equally. In line with this recent assumption, we posit that negative triples that are semantically valid w.r.t. domain and range constraints might be high-quality negative triples. As such, loss functions should treat them differently from semantically invalid negative ones. To this aim, we propose semantic-driven versions for the three main loss functions for link prediction. In an extensive and controlled experimental setting, we show that the proposed loss functions systematically provide satisfying results on three public benchmark KGs underpinned with different schemas, whic",
    "link": "http://arxiv.org/abs/2303.00286",
    "context": "Title: Treat Different Negatives Differently: Enriching Loss Functions with Domain and Range Constraints for Link Prediction. (arXiv:2303.00286v3 [cs.LG] UPDATED)\nAbstract: Knowledge graph embedding models (KGEMs) are used for various tasks related to knowledge graphs (KGs), including link prediction. They are trained with loss functions that are computed considering a batch of scored triples and their corresponding labels. Traditional approaches consider the label of a triple to be either true or false. However, recent works suggest that all negative triples should not be valued equally. In line with this recent assumption, we posit that negative triples that are semantically valid w.r.t. domain and range constraints might be high-quality negative triples. As such, loss functions should treat them differently from semantically invalid negative ones. To this aim, we propose semantic-driven versions for the three main loss functions for link prediction. In an extensive and controlled experimental setting, we show that the proposed loss functions systematically provide satisfying results on three public benchmark KGs underpinned with different schemas, whic",
    "path": "papers/23/03/2303.00286.json",
    "total_tokens": 879,
    "translated_title": "针对链接预测，对待不同的负样本有差异性：利用领域和范围约束丰富损失函数",
    "translated_abstract": "知识图谱嵌入模型（KGEMs）用于与知识图谱（KGs）相关的各种任务，包括链接预测。它们使用考虑了一批得分三元组及其相应标签的损失函数进行训练。传统方法认为三元组的标签要么为真，要么为假。然而，最近的研究表明，并非所有的负样本应该被平等对待。与这一最近的假设一致，我们认为基于领域和范围约束在语义上有效的负样本可能是高质量的负样本。因此，损失函数应该将它们与语义上无效的负样本区别对待。为此，我们针对链接预测的三个主要损失函数提出了基于语义的版本。通过广泛和受控的实验设置，我们展示了所提出的损失函数在三个具有不同模式的公共基准KG上系统地提供了令人满意的结果。",
    "tldr": "通过引入领域和范围约束，我们提出了基于语义的损失函数来区分不同质量的负样本，实验证明在链接预测任务上有效。"
}