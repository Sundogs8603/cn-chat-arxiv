{
    "title": "Revisiting Adversarial Training for ImageNet: Architectures, Training and Generalization across Threat Models. (arXiv:2303.01870v2 [cs.CV] UPDATED)",
    "abstract": "While adversarial training has been extensively studied for ResNet architectures and low resolution datasets like CIFAR, much less is known for ImageNet. Given the recent debate about whether transformers are more robust than convnets, we revisit adversarial training on ImageNet comparing ViTs and ConvNeXts. Extensive experiments show that minor changes in architecture, most notably replacing PatchStem with ConvStem, and training scheme have a significant impact on the achieved robustness. These changes not only increase robustness in the seen $\\ell_\\infty$-threat model, but even more so improve generalization to unseen $\\ell_1/\\ell_2$-attacks. Our modified ConvNeXt, ConvNeXt + ConvStem, yields the most robust $\\ell_\\infty$-models across different ranges of model parameters and FLOPs, while our ViT + ConvStem yields the best generalization to unseen threat models.",
    "link": "http://arxiv.org/abs/2303.01870",
    "context": "Title: Revisiting Adversarial Training for ImageNet: Architectures, Training and Generalization across Threat Models. (arXiv:2303.01870v2 [cs.CV] UPDATED)\nAbstract: While adversarial training has been extensively studied for ResNet architectures and low resolution datasets like CIFAR, much less is known for ImageNet. Given the recent debate about whether transformers are more robust than convnets, we revisit adversarial training on ImageNet comparing ViTs and ConvNeXts. Extensive experiments show that minor changes in architecture, most notably replacing PatchStem with ConvStem, and training scheme have a significant impact on the achieved robustness. These changes not only increase robustness in the seen $\\ell_\\infty$-threat model, but even more so improve generalization to unseen $\\ell_1/\\ell_2$-attacks. Our modified ConvNeXt, ConvNeXt + ConvStem, yields the most robust $\\ell_\\infty$-models across different ranges of model parameters and FLOPs, while our ViT + ConvStem yields the best generalization to unseen threat models.",
    "path": "papers/23/03/2303.01870.json",
    "total_tokens": 1016,
    "translated_title": "对于ImageNet的对抗训练再探：架构、训练和跨威胁模型的泛化",
    "translated_abstract": "虽然对于ResNet架构和低分辨率数据集如CIFAR来说，对抗训练已有广泛研究，但对于ImageNet而言，了解较少。鉴于最近有关transformers是否比卷积网络更坚固的争论，我们重新研究了在ImageNet上的对抗训练，并比较了ViTs和ConvNeXts的性能。大量实验证明，架构的微小改变，尤其是用ConvStem替换PatchStem以及训练方案，对所获得的鲁棒性有显著影响。这些改变不仅提高了在已见$\\ell_\\infty$威胁模型下的鲁棒性，而且更进一步改善了对未见$\\ell_1/\\ell_2$攻击的泛化能力。我们修改后的ConvNeXt，在不同模型参数和FLOPs范围内获得了最鲁棒的$\\ell_\\infty$模型，而我们的ViT + ConvStem在未见威胁模型上实现了最佳的泛化效果。",
    "tldr": "本论文重新研究了在ImageNet上的对抗训练，发现通过轻微改变架构和训练方案可显著提高模型的鲁棒性和泛化能力。修改后的ConvNeXt在已见威胁模型下获得了最鲁棒的结果，而ViT + ConvStem在未见威胁模型下的泛化效果最好。",
    "en_tdlr": "This paper revisits adversarial training on ImageNet and finds that making minor changes to the architecture and training scheme significantly improves both model robustness and generalization. The modified ConvNeXt achieves the most robust results in seen threat models, while ViT + ConvStem shows the best generalization to unseen threat models."
}