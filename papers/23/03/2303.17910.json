{
    "title": "Selective Knowledge Distillation for Non-Autoregressive Neural Machine Translation. (arXiv:2303.17910v1 [cs.CL])",
    "abstract": "Benefiting from the sequence-level knowledge distillation, the Non-Autoregressive Transformer (NAT) achieves great success in neural machine translation tasks. However, existing knowledge distillation has side effects, such as propagating errors from the teacher to NAT students, which may limit further improvements of NAT models and are rarely discussed in existing research. In this paper, we introduce selective knowledge distillation by introducing an NAT evaluator to select NAT-friendly targets that are of high quality and easy to learn. In addition, we introduce a simple yet effective progressive distillation method to boost NAT performance. Experiment results on multiple WMT language directions and several representative NAT models show that our approach can realize a flexible trade-off between the quality and complexity of training data for NAT models, achieving strong performances. Further analysis shows that distilling only 5% of the raw translations can help an NAT outperform i",
    "link": "http://arxiv.org/abs/2303.17910",
    "context": "Title: Selective Knowledge Distillation for Non-Autoregressive Neural Machine Translation. (arXiv:2303.17910v1 [cs.CL])\nAbstract: Benefiting from the sequence-level knowledge distillation, the Non-Autoregressive Transformer (NAT) achieves great success in neural machine translation tasks. However, existing knowledge distillation has side effects, such as propagating errors from the teacher to NAT students, which may limit further improvements of NAT models and are rarely discussed in existing research. In this paper, we introduce selective knowledge distillation by introducing an NAT evaluator to select NAT-friendly targets that are of high quality and easy to learn. In addition, we introduce a simple yet effective progressive distillation method to boost NAT performance. Experiment results on multiple WMT language directions and several representative NAT models show that our approach can realize a flexible trade-off between the quality and complexity of training data for NAT models, achieving strong performances. Further analysis shows that distilling only 5% of the raw translations can help an NAT outperform i",
    "path": "papers/23/03/2303.17910.json",
    "total_tokens": 924,
    "translated_title": "非自回归神经机器翻译中的选择性知识蒸馏",
    "translated_abstract": "通过序列级知识蒸馏的方式，非自回归变压器（NAT）在神经机器翻译任务中取得了巨大的成功。然而，现有的知识蒸馏存在副作用，如将教师机中的错误传播到NAT学生中，这可能限制NAT模型的进一步改进，并且很少在现有研究中讨论。本文通过引入一个NAT评估器来进行选择性知识蒸馏，选择高质量且易于学习的NAT友好目标。此外，我们引入了一种简单而有效的渐进蒸馏方法，以提高NAT性能。在多个WMT语言方向和几个代表性的NAT模型上进行实验结果显示，我们的方法可以实现NAT模型训练数据质量和复杂度之间的灵活权衡，达到了强大的性能。进一步的分析表明，只蒸馏5％的原始翻译就可以帮助NAT超越基线。",
    "tldr": "本论文介绍在非自回归神经机器翻译中引入选择性知识蒸馏和渐进蒸馏方法，并在实验中证明该方法可以在NAT模型的训练数据质量和复杂度之间实现灵活权衡，有助于NAT超越基线。",
    "en_tdlr": "This paper introduces selective knowledge distillation and progressive distillation methods in non-autoregressive neural machine translation, demonstrating its flexibility in balancing the quality and complexity of training data for NAT models and helping NAT surpass the baseline."
}