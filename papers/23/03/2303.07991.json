{
    "title": "Finding the Needle in a Haystack: Unsupervised Rationale Extraction from Long Text Classifiers. (arXiv:2303.07991v1 [cs.CL])",
    "abstract": "Long-sequence transformers are designed to improve the representation of longer texts by language models and their performance on downstream document-level tasks. However, not much is understood about the quality of token-level predictions in long-form models. We investigate the performance of such architectures in the context of document classification with unsupervised rationale extraction. We find standard soft attention methods to perform significantly worse when combined with the Longformer language model. We propose a compositional soft attention architecture that applies RoBERTa sentence-wise to extract plausible rationales at the token-level. We find this method to significantly outperform Longformer-driven baselines on sentiment classification datasets, while also exhibiting significantly lower runtimes.",
    "link": "http://arxiv.org/abs/2303.07991",
    "context": "Title: Finding the Needle in a Haystack: Unsupervised Rationale Extraction from Long Text Classifiers. (arXiv:2303.07991v1 [cs.CL])\nAbstract: Long-sequence transformers are designed to improve the representation of longer texts by language models and their performance on downstream document-level tasks. However, not much is understood about the quality of token-level predictions in long-form models. We investigate the performance of such architectures in the context of document classification with unsupervised rationale extraction. We find standard soft attention methods to perform significantly worse when combined with the Longformer language model. We propose a compositional soft attention architecture that applies RoBERTa sentence-wise to extract plausible rationales at the token-level. We find this method to significantly outperform Longformer-driven baselines on sentiment classification datasets, while also exhibiting significantly lower runtimes.",
    "path": "papers/23/03/2303.07991.json",
    "total_tokens": 766,
    "translated_title": "在长文本分类器中无监督地提取理由",
    "translated_abstract": "长序列转换器旨在通过语言模型改进较长文本的表示，并提高它们在下游文档级任务中的性能。然而，人们对长格式模型中令牌级别预测的质量尚不太了解。我们研究了这种架构在无监督理由提取的文档分类背景下的性能。我们发现当与Longformer语言模型结合使用时，标准软量注意方法表现显着较差。我们提出了一种组合软量关注架构，它将RoBERTa应用于句子，以在令牌级别提取可信理由。我们发现这种方法在情感分类数据集上明显优于Longformer衍生基线，并且展现出明显更低的运行时间。",
    "tldr": "该论文研究了无监督理由提取的文档分类背景下，长文本分类器中的表现，并提出了一种比Longformer驱动的基线明显更好的RoBERTa句子级组合软注意力结构。",
    "en_tdlr": "This paper investigates the performance of long text classifiers in the context of document classification with unsupervised rationale extraction and proposes a compositional soft attention architecture that significantly outperforms Longformer-driven baselines on sentiment classification datasets."
}