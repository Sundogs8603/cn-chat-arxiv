{
    "title": "Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos. (arXiv:2303.16897v1 [cs.CV])",
    "abstract": "Modeling sounds emitted from physical object interactions is critical for immersive perceptual experiences in real and virtual worlds. Traditional methods of impact sound synthesis use physics simulation to obtain a set of physics parameters that could represent and synthesize the sound. However, they require fine details of both the object geometries and impact locations, which are rarely available in the real world and can not be applied to synthesize impact sounds from common videos. On the other hand, existing video-driven deep learning-based approaches could only capture the weak correspondence between visual content and impact sounds since they lack of physics knowledge. In this work, we propose a physics-driven diffusion model that can synthesize high-fidelity impact sound for a silent video clip. In addition to the video content, we propose to use additional physics priors to guide the impact sound synthesis procedure. The physics priors include both physics parameters that are",
    "link": "http://arxiv.org/abs/2303.16897",
    "context": "Title: Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos. (arXiv:2303.16897v1 [cs.CV])\nAbstract: Modeling sounds emitted from physical object interactions is critical for immersive perceptual experiences in real and virtual worlds. Traditional methods of impact sound synthesis use physics simulation to obtain a set of physics parameters that could represent and synthesize the sound. However, they require fine details of both the object geometries and impact locations, which are rarely available in the real world and can not be applied to synthesize impact sounds from common videos. On the other hand, existing video-driven deep learning-based approaches could only capture the weak correspondence between visual content and impact sounds since they lack of physics knowledge. In this work, we propose a physics-driven diffusion model that can synthesize high-fidelity impact sound for a silent video clip. In addition to the video content, we propose to use additional physics priors to guide the impact sound synthesis procedure. The physics priors include both physics parameters that are",
    "path": "papers/23/03/2303.16897.json",
    "total_tokens": 868,
    "translated_title": "物理驱动的扩散模型用于从视频中合成冲击声",
    "translated_abstract": "对物体相互作用发出的声音进行建模对于实际世界和虚拟世界中的沉浸式感官体验至关重要。传统的冲击声合成方法使用物理模拟来获得一组能够表示和合成声音的物理参数。然而，它们需要物体的细节和冲击位置，这在真实世界中很少可用，并且无法应用于从普通视频中合成冲击声。另一方面，现有的视频驱动深度学习方法只能捕捉到视觉内容和冲击声之间的弱对应关系，因为它们缺乏物理知识。在这项工作中，我们提出了一种物理驱动的扩散模型，可以为静态视频剪辑合成高保真的冲击声。除了视频内容外，我们还提出使用额外的物理先验知识来指导冲击声合成过程，这些先验包括既可控制物理参数，同时也能保证音效质量的噪声扰动。",
    "tldr": "该论文提出了一种物理驱动扩散模型，可以为silent视频剪辑合成高保真的冲击声，并使用额外的物理先验知识来指导冲击声合成过程。"
}