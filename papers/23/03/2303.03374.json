{
    "title": "To Stay or Not to Stay in the Pre-train Basin: Insights on Ensembling in Transfer Learning. (arXiv:2303.03374v2 [cs.LG] UPDATED)",
    "abstract": "Transfer learning and ensembling are two popular techniques for improving the performance and robustness of neural networks. Due to the high cost of pre-training, ensembles of models fine-tuned from a single pre-trained checkpoint are often used in practice. Such models end up in the same basin of the loss landscape, which we call the pre-train basin, and thus have limited diversity. In this work, we show that ensembles trained from a single pre-trained checkpoint may be improved by better exploring the pre-train basin, however, leaving the basin results in losing the benefits of transfer learning and in degradation of the ensemble quality. Based on the analysis of existing exploration methods, we propose a more effective modification of the Snapshot Ensembles (SSE) for transfer learning setup, StarSSE, which results in stronger ensembles and uniform model soups.",
    "link": "http://arxiv.org/abs/2303.03374",
    "context": "Title: To Stay or Not to Stay in the Pre-train Basin: Insights on Ensembling in Transfer Learning. (arXiv:2303.03374v2 [cs.LG] UPDATED)\nAbstract: Transfer learning and ensembling are two popular techniques for improving the performance and robustness of neural networks. Due to the high cost of pre-training, ensembles of models fine-tuned from a single pre-trained checkpoint are often used in practice. Such models end up in the same basin of the loss landscape, which we call the pre-train basin, and thus have limited diversity. In this work, we show that ensembles trained from a single pre-trained checkpoint may be improved by better exploring the pre-train basin, however, leaving the basin results in losing the benefits of transfer learning and in degradation of the ensemble quality. Based on the analysis of existing exploration methods, we propose a more effective modification of the Snapshot Ensembles (SSE) for transfer learning setup, StarSSE, which results in stronger ensembles and uniform model soups.",
    "path": "papers/23/03/2303.03374.json",
    "total_tokens": 980,
    "translated_title": "停留还是离开预训练基域：关于集成学习在迁移学习中的洞见",
    "translated_abstract": "迁移学习和集成学习是改善神经网络性能和鲁棒性的两种热门技术。由于预训练成本高昂，通常实践中使用从单个预训练检查点微调的模型集合。这些模型最终会进入损失函数梯度下降空间的相同区域，我们称之为预训练基域，因此具有有限的多样性。在这项工作中，我们展示了从单个预训练检查点训练的集成模型可以通过更好地探索预训练基域来改进，然而，离开基域会导致失去迁移学习的好处并导致集成质量的下降。基于对现有探索方法的分析，我们提出了一种更有效的修改Transfer Learning Setup中的Snapshot Ensembles（SSE）方法，名为StarSSE，它能产生更强的集成模型和均匀的模型混合。",
    "tldr": "该论文研究了在迁移学习中使用单个预训练检查点微调的模型集合，发现通过更好地探索预训练基域可以改进集成模型，但离开基域会导致失去迁移学习的好处，并且降低集成质量。作者提出了一种更有效的修改方法StarSSE，可以产生更强的集成模型和均匀的模型混合。",
    "en_tdlr": "This paper investigates ensemble models fine-tuned from a single pre-trained checkpoint in transfer learning. It shows that better exploration of the pre-train basin can improve ensemble models, but leaving the basin results in losing the benefits of transfer learning and degrading ensemble quality. The authors propose a more effective modification called StarSSE, which produces stronger ensembles and uniform model mixtures."
}