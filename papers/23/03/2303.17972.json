{
    "title": "$\\mathcal{E}$ K\\'U [MASK]: Integrating Yor\\`ub\\'a cultural greetings into machine translation. (arXiv:2303.17972v1 [cs.CL])",
    "abstract": "This paper investigates the performance of massively multilingual neural machine translation (NMT) systems in translating Yor\\`ub\\'a greetings ($\\mathcal{E}$ k\\'u [MASK]), which are a big part of Yor\\`ub\\'a language and culture, into English. To evaluate these models, we present IkiniYor\\`ub\\'a, a Yor\\`ub\\'a-English translation dataset containing some Yor\\`ub\\'a greetings, and sample use cases. We analysed the performance of different multilingual NMT systems including Google and NLLB and show that these models struggle to accurately translate Yor\\`ub\\'a greetings into English. In addition, we trained a Yor\\`ub\\'a-English model by finetuning an existing NMT model on the training split of IkiniYor\\`ub\\'a and this achieved better performance when compared to the pre-trained multilingual NMT models, although they were trained on a large volume of data.",
    "link": "http://arxiv.org/abs/2303.17972",
    "context": "Title: $\\mathcal{E}$ K\\'U [MASK]: Integrating Yor\\`ub\\'a cultural greetings into machine translation. (arXiv:2303.17972v1 [cs.CL])\nAbstract: This paper investigates the performance of massively multilingual neural machine translation (NMT) systems in translating Yor\\`ub\\'a greetings ($\\mathcal{E}$ k\\'u [MASK]), which are a big part of Yor\\`ub\\'a language and culture, into English. To evaluate these models, we present IkiniYor\\`ub\\'a, a Yor\\`ub\\'a-English translation dataset containing some Yor\\`ub\\'a greetings, and sample use cases. We analysed the performance of different multilingual NMT systems including Google and NLLB and show that these models struggle to accurately translate Yor\\`ub\\'a greetings into English. In addition, we trained a Yor\\`ub\\'a-English model by finetuning an existing NMT model on the training split of IkiniYor\\`ub\\'a and this achieved better performance when compared to the pre-trained multilingual NMT models, although they were trained on a large volume of data.",
    "path": "papers/23/03/2303.17972.json",
    "total_tokens": 1041,
    "translated_title": "$\\mathcal{E}$ K\\'U [MASK]: 将尤鲁巴文化问候语整合到机器翻译中",
    "translated_abstract": "本文研究了大规模多语言神经机器翻译（NMT）系统在将尤鲁巴语问候语（$\\mathcal{E}$ k\\'u [MASK]）翻译成英文时的表现。为了评估这些模型，我们介绍了一个尤鲁巴语-英语翻译数据集IkiniYor\\`ub\\'a，其中包含尤鲁巴语问候语和样例用例。我们分析了包括Google和NLLB在内的不同多语言NMT系统的表现，并显示这些模型在准确翻译尤鲁巴语问候语到英语时存在困难。此外，我们通过在IkiniYor\\`ub\\'a的训练集上微调现有的NMT模型来训练一个尤鲁巴语-英语模型，与预训练的多语言NMT模型相比，其表现更好，尽管它们经过大量数据的训练。",
    "tldr": "本文研究了将尤鲁巴文化问候语（$\\mathcal{E}$ k\\'u [MASK]）整合到机器翻译中，通过IkiniYor\\`ub\\'a数据集，我们发现大规模多语言神经机器翻译（NMT）系统无法准确翻译，而微调现有的NMT模型在翻译中有更好的表现。",
    "en_tdlr": "This paper explores integrating Yor\\`ub\\'a cultural greetings ($\\mathcal{E}$ k\\'u [MASK]) into machine translation and shows that existing massively multilingual neural machine translation (NMT) systems struggle to accurately translate them into English. The study trains a Yor\\`ub\\'a-English model by finetuning an existing NMT model on the IkiniYor\\`ub\\'a dataset and demonstrates that the fine-tuned model outperforms pre-trained multilingual NMT models."
}