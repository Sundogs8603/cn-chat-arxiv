{
    "title": "MultiModal Bias: Introducing a Framework for Stereotypical Bias Assessment beyond Gender and Race in Vision Language Models. (arXiv:2303.12734v1 [cs.CV])",
    "abstract": "Recent breakthroughs in self supervised training have led to a new class of pretrained vision language models. While there have been investigations of bias in multimodal models, they have mostly focused on gender and racial bias, giving much less attention to other relevant groups, such as minorities with regard to religion, nationality, sexual orientation, or disabilities. This is mainly due to lack of suitable benchmarks for such groups. We seek to address this gap by providing a visual and textual bias benchmark called MMBias, consisting of around 3,800 images and phrases covering 14 population subgroups. We utilize this dataset to assess bias in several prominent self supervised multimodal models, including CLIP, ALBEF, and ViLT. Our results show that these models demonstrate meaningful bias favoring certain groups. Finally, we introduce a debiasing method designed specifically for such large pre-trained models that can be applied as a post-processing step to mitigate bias, while p",
    "link": "http://arxiv.org/abs/2303.12734",
    "context": "Title: MultiModal Bias: Introducing a Framework for Stereotypical Bias Assessment beyond Gender and Race in Vision Language Models. (arXiv:2303.12734v1 [cs.CV])\nAbstract: Recent breakthroughs in self supervised training have led to a new class of pretrained vision language models. While there have been investigations of bias in multimodal models, they have mostly focused on gender and racial bias, giving much less attention to other relevant groups, such as minorities with regard to religion, nationality, sexual orientation, or disabilities. This is mainly due to lack of suitable benchmarks for such groups. We seek to address this gap by providing a visual and textual bias benchmark called MMBias, consisting of around 3,800 images and phrases covering 14 population subgroups. We utilize this dataset to assess bias in several prominent self supervised multimodal models, including CLIP, ALBEF, and ViLT. Our results show that these models demonstrate meaningful bias favoring certain groups. Finally, we introduce a debiasing method designed specifically for such large pre-trained models that can be applied as a post-processing step to mitigate bias, while p",
    "path": "papers/23/03/2303.12734.json",
    "total_tokens": 1086,
    "translated_title": "多模态偏差：引入一种框架以评估视觉语言模型中的刻板印象偏差，超越性别和种族。",
    "translated_abstract": "最近自主训练的突破为一类预先训练的视觉语言模型带来了新的机遇。虽然对多模态模型中的偏见进行了一些调查，但主要集中在性别和种族偏见上，对于其他相关群体，如宗教、国籍、性取向或残疾人群，给予的关注较少，这主要是由于缺乏适当的基准。我们通过提供一个称为MMBias的视觉和文本偏差基准来解决这一差距，包括约3800个图像和短语，涵盖14个人口子群。我们利用这个数据集来评估几个著名的自我监督多模态模型，包括CLIP、ALBEF和ViLT。我们的结果显示，这些模型表现出有意义的偏差，偏向某些群体。最后，我们引入了一种针对这种大规模预先训练模型设计的去偏置方法，可以作为后处理步骤应用于减轻偏差，同时保持模型性能。",
    "tldr": "本文提出了一个视觉和文本偏置基准MMBias，涵盖14个人口子群，并利用该基准评估了多个自我监督多模态模型（包括CLIP、ALBEF和ViLT），结果表明这些模型表现出偏向某些群体的有意义的偏见。同时，本文引入了一种针对大规模预先训练模型设计的去偏置方法作为后处理步骤，可以减轻偏差的影响，同时保证模型的性能。",
    "en_tdlr": "This paper introduces a visual and textual bias benchmark called MMBias, covering 14 population subgroups, and evaluates several prominent self supervised multimodal models (including CLIP, ALBEF and ViLT) using this dataset to find meaningful bias favoring certain groups. A debiasing method is also proposed as a post-processing step to address the bias of large pre-trained models while maintaining their performance."
}