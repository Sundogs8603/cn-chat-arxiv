{
    "title": "Going Beyond Nouns With Vision & Language Models Using Synthetic Data. (arXiv:2303.17590v1 [cs.CV])",
    "abstract": "Large-scale pre-trained Vision & Language (VL) models have shown remarkable performance in many applications, enabling replacing a fixed set of supported classes with zero-shot open vocabulary reasoning over (almost arbitrary) natural language prompts. However, recent works have uncovered a fundamental weakness of these models. For example, their difficulty to understand Visual Language Concepts (VLC) that go 'beyond nouns' such as the meaning of non-object words (e.g., attributes, actions, relations, states, etc.), or difficulty in performing compositional reasoning such as understanding the significance of the order of the words in a sentence. In this work, we investigate to which extent purely synthetic data could be leveraged to teach these models to overcome such shortcomings without compromising their zero-shot capabilities. We contribute Synthetic Visual Concepts (SyViC) - a million-scale synthetic dataset and data generation codebase allowing to generate additional suitable dat",
    "link": "http://arxiv.org/abs/2303.17590",
    "context": "Title: Going Beyond Nouns With Vision & Language Models Using Synthetic Data. (arXiv:2303.17590v1 [cs.CV])\nAbstract: Large-scale pre-trained Vision & Language (VL) models have shown remarkable performance in many applications, enabling replacing a fixed set of supported classes with zero-shot open vocabulary reasoning over (almost arbitrary) natural language prompts. However, recent works have uncovered a fundamental weakness of these models. For example, their difficulty to understand Visual Language Concepts (VLC) that go 'beyond nouns' such as the meaning of non-object words (e.g., attributes, actions, relations, states, etc.), or difficulty in performing compositional reasoning such as understanding the significance of the order of the words in a sentence. In this work, we investigate to which extent purely synthetic data could be leveraged to teach these models to overcome such shortcomings without compromising their zero-shot capabilities. We contribute Synthetic Visual Concepts (SyViC) - a million-scale synthetic dataset and data generation codebase allowing to generate additional suitable dat",
    "path": "papers/23/03/2303.17590.json",
    "total_tokens": 858,
    "translated_title": "利用合成数据，视觉语言模型突破名词局限",
    "translated_abstract": "大规模预训练的视觉语言（VL）模型在许多应用中表现出了显着的性能，使得可以通过（几乎任意）自然语言提示进行零样本开放词汇推理，取代了一组支持的类别。然而，最近的研究揭示了这些模型的一个根本性弱点。本文研究了纯合成数据在多大程度上可以教会这些模型克服这些缺点，而不损害它们的零样本能力。作者贡献了一个数百万规模的合成数据集SyViC，以及数据生成代码库，允许在现有的VL基准数据集中生成额外的合适数据，实现'noun'以外的理解任务学习。",
    "tldr": "本文在现有的VL模型中加入合成数据集SyViC，成功实现对'名词以外'的理解任务。",
    "en_tdlr": "This paper leverages synthetic data to teach visual language models to understand concepts beyond nouns and improve their performance on such tasks. The proposed SyViC dataset and data generation codebase enable generating additional suitable data for learning beyond standard training data, achieving success in a purely zero-shot transfer learning setting."
}