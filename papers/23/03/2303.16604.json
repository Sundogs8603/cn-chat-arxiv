{
    "title": "Bi-directional Training for Composed Image Retrieval via Text Prompt Learning. (arXiv:2303.16604v1 [cs.CV])",
    "abstract": "Composed image retrieval searches for a target image based on a multi-modal user query comprised of a reference image and modification text describing the desired changes. Existing approaches to solving this challenging task learn a mapping from the (reference image, modification text)-pair to an image embedding that is then matched against a large image corpus. One area that has not yet been explored is the reverse direction, which asks the question, what reference image when modified as describe by the text would produce the given target image? In this work we propose a bi-directional training scheme that leverages such reversed queries and can be applied to existing composed image retrieval architectures. To encode the bi-directional query we prepend a learnable token to the modification text that designates the direction of the query and then finetune the parameters of the text embedding module. We make no other changes to the network architecture. Experiments on two standard datas",
    "link": "http://arxiv.org/abs/2303.16604",
    "context": "Title: Bi-directional Training for Composed Image Retrieval via Text Prompt Learning. (arXiv:2303.16604v1 [cs.CV])\nAbstract: Composed image retrieval searches for a target image based on a multi-modal user query comprised of a reference image and modification text describing the desired changes. Existing approaches to solving this challenging task learn a mapping from the (reference image, modification text)-pair to an image embedding that is then matched against a large image corpus. One area that has not yet been explored is the reverse direction, which asks the question, what reference image when modified as describe by the text would produce the given target image? In this work we propose a bi-directional training scheme that leverages such reversed queries and can be applied to existing composed image retrieval architectures. To encode the bi-directional query we prepend a learnable token to the modification text that designates the direction of the query and then finetune the parameters of the text embedding module. We make no other changes to the network architecture. Experiments on two standard datas",
    "path": "papers/23/03/2303.16604.json",
    "total_tokens": 879,
    "translated_title": "基于文本提示学习和双向训练的组成图像检索方法",
    "translated_abstract": "组成图像检索是根据包含参考图像和描述所需更改的修改文本的多模态用户查询来搜索目标图像的方法。现有的解决这个具有挑战性的任务的方法学习从（参考图像，修改文本）对到图像嵌入的映射，然后将其与大型图像语料库进行匹配。本文提出了一种双向训练方案，利用了这种反向查询，并可应用于现有的组成图像检索体系结构。为了编码双向查询，我们在修改文本前面添加一个可学习的令牌，指定查询的方向，然后微调文本嵌入模块的参数。我们没有对网络架构进行其他更改。在两个标准数据集上的实验表明，双向训练在提高组成图像检索性能方面是有效的，特别是在修改文本存在噪声或歧义的情况下。",
    "tldr": "本文提出了一种基于文本提示学习和双向训练的组成图像检索方法，可以应用于现有的体系结构，并且在修改文本存在噪声或歧义的情况下特别有效。",
    "en_tdlr": "This paper proposes a composed image retrieval method based on text prompt learning and bi-directional training that can be applied to existing architectures, and is particularly effective when the modification text is noisy or ambiguous."
}