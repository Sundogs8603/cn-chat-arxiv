{
    "title": "Dexterity from Touch: Self-Supervised Pre-Training of Tactile Representations with Robotic Play. (arXiv:2303.12076v1 [cs.RO])",
    "abstract": "Teaching dexterity to multi-fingered robots has been a longstanding challenge in robotics. Most prominent work in this area focuses on learning controllers or policies that either operate on visual observations or state estimates derived from vision. However, such methods perform poorly on fine-grained manipulation tasks that require reasoning about contact forces or about objects occluded by the hand itself. In this work, we present T-Dex, a new approach for tactile-based dexterity, that operates in two phases. In the first phase, we collect 2.5 hours of play data, which is used to train self-supervised tactile encoders. This is necessary to bring high-dimensional tactile readings to a lower-dimensional embedding. In the second phase, given a handful of demonstrations for a dexterous task, we learn non-parametric policies that combine the tactile observations with visual ones. Across five challenging dexterous tasks, we show that our tactile-based dexterity models outperform purely vi",
    "link": "http://arxiv.org/abs/2303.12076",
    "context": "Title: Dexterity from Touch: Self-Supervised Pre-Training of Tactile Representations with Robotic Play. (arXiv:2303.12076v1 [cs.RO])\nAbstract: Teaching dexterity to multi-fingered robots has been a longstanding challenge in robotics. Most prominent work in this area focuses on learning controllers or policies that either operate on visual observations or state estimates derived from vision. However, such methods perform poorly on fine-grained manipulation tasks that require reasoning about contact forces or about objects occluded by the hand itself. In this work, we present T-Dex, a new approach for tactile-based dexterity, that operates in two phases. In the first phase, we collect 2.5 hours of play data, which is used to train self-supervised tactile encoders. This is necessary to bring high-dimensional tactile readings to a lower-dimensional embedding. In the second phase, given a handful of demonstrations for a dexterous task, we learn non-parametric policies that combine the tactile observations with visual ones. Across five challenging dexterous tasks, we show that our tactile-based dexterity models outperform purely vi",
    "path": "papers/23/03/2303.12076.json",
    "total_tokens": 1004,
    "translated_title": "触觉训练下的机器人灵巧性",
    "translated_abstract": "在机器人领域，让具有多指的机器人具有灵巧性一直是一个长期的挑战。此前，最重要的工作都集中在学习控制器或策略上，这些控制器或策略要么基于视觉观测，要么基于从视觉推断得到的状态估计。然而，这些方法在需要推理接触力或通过手本身遮挡的物体的细粒度操作任务上表现不佳。本研究提出了一种新的基于触觉的灵巧性方法T-Dex，该方法分为两个阶段：在第一阶段，收集2.5小时的游戏数据并使用这些数据训练自我监督型触觉编码器；在第二阶段，利用少量的灵巧性任务演示，学习将触觉观测和视觉观测相结合的非参数化策略。通过五个具有挑战性的灵巧性任务，我们展示了我们的基于触觉的灵巧性模型比纯视觉方法更有效，并且能够推广到现实世界中视觉方法失败的情况。",
    "tldr": "T-Dex是一种基于触觉的灵巧性方法，可在自我监督式的触觉编码器和一些灵巧性任务演示的指导下，将触觉和视觉结合起来，相比于传统的纯视觉方法更有效，并在现实世界中具有更好的应用性。",
    "en_tdlr": "T-Dex is a tactile-based dexterity approach that combines touch and vision under the guidance of self-supervised tactile encoders and some demonstrations of dexterity tasks, and outperforms traditional visual-based methods with better practicality in the real world."
}