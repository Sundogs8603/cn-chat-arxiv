{
    "title": "RGI : Regularized Graph Infomax for self-supervised learning on graphs. (arXiv:2303.08644v1 [cs.LG])",
    "abstract": "Self-supervised learning is gaining considerable attention as a solution to avoid the requirement of extensive annotations in representation learning on graphs. We introduce \\textit{Regularized Graph Infomax (RGI)}, a simple yet effective framework for node level self-supervised learning on graphs that trains a graph neural network encoder by maximizing the mutual information between node level local and global views, in contrast to previous works that employ graph level global views. The method promotes the predictability between views while regularizing the covariance matrices of the representations. Therefore, RGI is non-contrastive, does not depend on complex asymmetric architectures nor training tricks, is augmentation-free and does not rely on a two branch architecture. We run RGI on both transductive and inductive settings with popular graph benchmarks and show that it can achieve state-of-the-art performance regardless of its simplicity.",
    "link": "http://arxiv.org/abs/2303.08644",
    "context": "Title: RGI : Regularized Graph Infomax for self-supervised learning on graphs. (arXiv:2303.08644v1 [cs.LG])\nAbstract: Self-supervised learning is gaining considerable attention as a solution to avoid the requirement of extensive annotations in representation learning on graphs. We introduce \\textit{Regularized Graph Infomax (RGI)}, a simple yet effective framework for node level self-supervised learning on graphs that trains a graph neural network encoder by maximizing the mutual information between node level local and global views, in contrast to previous works that employ graph level global views. The method promotes the predictability between views while regularizing the covariance matrices of the representations. Therefore, RGI is non-contrastive, does not depend on complex asymmetric architectures nor training tricks, is augmentation-free and does not rely on a two branch architecture. We run RGI on both transductive and inductive settings with popular graph benchmarks and show that it can achieve state-of-the-art performance regardless of its simplicity.",
    "path": "papers/23/03/2303.08644.json",
    "total_tokens": 869,
    "translated_title": "RGI: 基于正则化的图形Infomax自监督学习",
    "translated_abstract": "自监督学习作为在图形表示学习中避免大量注释的解决方案，正受到越来越多的关注。我们引入了“正则化图形Infomax（RGI）”，这是一个简单而又有效的框架，用于在图形上进行节点级自监督学习，通过最大化节点级局部和全局视图之间的互信息来训练图神经网络编码器，与以前采用图级全局视图的方法不同。该方法促进了视图之间的可预测性，同时规范化了表示的协方差矩阵。因此，RGI是非对比的，不依赖于复杂的不对称体系结构或训练技巧，无需增强和不依赖于双分支架构。我们在流行的图形基准上运行RGI，并展示它可以实现最先进的性能，而不管它的简单性如何。",
    "tldr": "RGI是一个用于图上节点级自监督学习的简单而有效的框架，通过最大化节点级局部和全局视图之间的互信息来训练图神经网络编码器，并规范化了表示的协方差矩阵。",
    "en_tdlr": "RGI is a simple and effective framework for node-level self-supervised learning on graphs, which trains a graph neural network encoder by maximizing the mutual information between node-level local and global views, and regularizes the covariance matrices of the representations."
}