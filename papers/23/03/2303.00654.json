{
    "title": "How to DP-fy ML: A Practical Guide to Machine Learning with Differential Privacy. (arXiv:2303.00654v3 [cs.LG] UPDATED)",
    "abstract": "ML models are ubiquitous in real world applications and are a constant focus of research. At the same time, the community has started to realize the importance of protecting the privacy of ML training data.  Differential Privacy (DP) has become a gold standard for making formal statements about data anonymization. However, while some adoption of DP has happened in industry, attempts to apply DP to real world complex ML models are still few and far between. The adoption of DP is hindered by limited practical guidance of what DP protection entails, what privacy guarantees to aim for, and the difficulty of achieving good privacy-utility-computation trade-offs for ML models. Tricks for tuning and maximizing performance are scattered among papers or stored in the heads of practitioners. Furthermore, the literature seems to present conflicting evidence on how and whether to apply architectural adjustments and which components are \"safe\" to use with DP.  This work is a self-contained guide th",
    "link": "http://arxiv.org/abs/2303.00654",
    "context": "Title: How to DP-fy ML: A Practical Guide to Machine Learning with Differential Privacy. (arXiv:2303.00654v3 [cs.LG] UPDATED)\nAbstract: ML models are ubiquitous in real world applications and are a constant focus of research. At the same time, the community has started to realize the importance of protecting the privacy of ML training data.  Differential Privacy (DP) has become a gold standard for making formal statements about data anonymization. However, while some adoption of DP has happened in industry, attempts to apply DP to real world complex ML models are still few and far between. The adoption of DP is hindered by limited practical guidance of what DP protection entails, what privacy guarantees to aim for, and the difficulty of achieving good privacy-utility-computation trade-offs for ML models. Tricks for tuning and maximizing performance are scattered among papers or stored in the heads of practitioners. Furthermore, the literature seems to present conflicting evidence on how and whether to apply architectural adjustments and which components are \"safe\" to use with DP.  This work is a self-contained guide th",
    "path": "papers/23/03/2303.00654.json",
    "total_tokens": 985,
    "translated_title": "如何用差分隐私实现机器学习：机器学习与差分隐私实用指南",
    "translated_abstract": "机器学习模型在现实世界应用广泛，并且是研究的重点。与此同时，社区开始意识到保护机器学习训练数据的隐私的重要性。差分隐私已经成为对数据匿名化做出正式陈述的黄金标准。然而，尽管在工业界已经有一些应用差分隐私的尝试，但将差分隐私应用于现实世界中的复杂机器学习模型仍然很少。差分隐私的应用受限于缺乏实际指导，不清楚需要什么样的隐私保证，并且在机器学习模型的隐私保护、效用和计算之间存在良好的平衡。调整和优化性能的技巧散布在论文中或者存在于从业者的头脑中。此外，文献似乎对于如何以及是否应用架构调整以及哪些组件在应用差分隐私时是“安全”的问题存在着相互矛盾的证据。本工作是一份自包含的指南，旨在填补这些空白并提供实际指导，帮助实现机器学习与差分隐私的结合。",
    "tldr": "这篇论文提供了关于如何将差分隐私应用于复杂机器学习模型的实用指南，填补了现有实践中的空白，为实现机器学习与差分隐私的结合提供了实际指导。",
    "en_tdlr": "This paper provides a practical guide on how to apply differential privacy to complex machine learning models, filling the gaps in current practices and providing practical guidance on integrating machine learning with differential privacy."
}