{
    "title": "Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization. (arXiv:2303.03108v2 [cs.LG] UPDATED)",
    "abstract": "Recently, flat minima are proven to be effective for improving generalization and sharpness-aware minimization (SAM) achieves state-of-the-art performance. Yet the current definition of flatness discussed in SAM and its follow-ups are limited to the zeroth-order flatness (i.e., the worst-case loss within a perturbation radius). We show that the zeroth-order flatness can be insufficient to discriminate minima with low generalization error from those with high generalization error both when there is a single minimum or multiple minima within the given perturbation radius. Thus we present first-order flatness, a stronger measure of flatness focusing on the maximal gradient norm within a perturbation radius which bounds both the maximal eigenvalue of Hessian at local minima and the regularization function of SAM. We also present a novel training procedure named Gradient norm Aware Minimization (GAM) to seek minima with uniformly small curvature across all directions. Experimental results s",
    "link": "http://arxiv.org/abs/2303.03108",
    "context": "Title: Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization. (arXiv:2303.03108v2 [cs.LG] UPDATED)\nAbstract: Recently, flat minima are proven to be effective for improving generalization and sharpness-aware minimization (SAM) achieves state-of-the-art performance. Yet the current definition of flatness discussed in SAM and its follow-ups are limited to the zeroth-order flatness (i.e., the worst-case loss within a perturbation radius). We show that the zeroth-order flatness can be insufficient to discriminate minima with low generalization error from those with high generalization error both when there is a single minimum or multiple minima within the given perturbation radius. Thus we present first-order flatness, a stronger measure of flatness focusing on the maximal gradient norm within a perturbation radius which bounds both the maximal eigenvalue of Hessian at local minima and the regularization function of SAM. We also present a novel training procedure named Gradient norm Aware Minimization (GAM) to seek minima with uniformly small curvature across all directions. Experimental results s",
    "path": "papers/23/03/2303.03108.json",
    "total_tokens": 940,
    "translated_title": "梯度范数感知最小化在寻找一阶平坦度中改进了广义化能力",
    "translated_abstract": "最近，已经证明了平坦的极小值有效地提高了泛化能力，而锐度感知最小化 (SAM) 实现了最先进的性能。然而，SAM 及其后续讨论中当前关于平坦性的定义仅限于零阶平坦性 (即扰动半径内最坏损失)。我们表明，当存在单一最小值或给定扰动半径内的多个最小值时，零阶平坦度可能不足以区分具有低泛化误差和高泛化误差的极小值。因此，我们提出了一阶平坦度，这是一种更强的平坦度测量，重点关注扰动半径内的最大梯度范数，其限制了局部极小值的 Hessian 的最大特征值和 SAM 的正则化函数。我们还提出了一种名为 Gradient norm Aware Minimization (GAM) 的新型训练过程，以寻找所有方向上曲率均匀小的最小值。实验结果表明，GAM 显着改进了广义化能力和测试损失",
    "tldr": "研究提出了一阶平坦度的概念，使用梯度范数感知最小化算法寻找在所有方向上具有均匀小曲率的极小值，提高了广义化能力和测试损失。",
    "en_tdlr": "The paper proposes the concept of first-order flatness and presents a novel training procedure named Gradient norm Aware Minimization (GAM) to seek minima with uniformly small curvature across all directions, improving both generalization ability and testing loss."
}