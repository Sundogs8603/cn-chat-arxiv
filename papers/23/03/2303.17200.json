{
    "title": "SynthVSR: Scaling Up Visual Speech Recognition With Synthetic Supervision. (arXiv:2303.17200v1 [cs.CV])",
    "abstract": "Recently reported state-of-the-art results in visual speech recognition (VSR) often rely on increasingly large amounts of video data, while the publicly available transcribed video datasets are limited in size. In this paper, for the first time, we study the potential of leveraging synthetic visual data for VSR. Our method, termed SynthVSR, substantially improves the performance of VSR systems with synthetic lip movements. The key idea behind SynthVSR is to leverage a speech-driven lip animation model that generates lip movements conditioned on the input speech. The speech-driven lip animation model is trained on an unlabeled audio-visual dataset and could be further optimized towards a pre-trained VSR model when labeled videos are available. As plenty of transcribed acoustic data and face images are available, we are able to generate large-scale synthetic data using the proposed lip animation model for semi-supervised VSR training. We evaluate the performance of our approach on the la",
    "link": "http://arxiv.org/abs/2303.17200",
    "context": "Title: SynthVSR: Scaling Up Visual Speech Recognition With Synthetic Supervision. (arXiv:2303.17200v1 [cs.CV])\nAbstract: Recently reported state-of-the-art results in visual speech recognition (VSR) often rely on increasingly large amounts of video data, while the publicly available transcribed video datasets are limited in size. In this paper, for the first time, we study the potential of leveraging synthetic visual data for VSR. Our method, termed SynthVSR, substantially improves the performance of VSR systems with synthetic lip movements. The key idea behind SynthVSR is to leverage a speech-driven lip animation model that generates lip movements conditioned on the input speech. The speech-driven lip animation model is trained on an unlabeled audio-visual dataset and could be further optimized towards a pre-trained VSR model when labeled videos are available. As plenty of transcribed acoustic data and face images are available, we are able to generate large-scale synthetic data using the proposed lip animation model for semi-supervised VSR training. We evaluate the performance of our approach on the la",
    "path": "papers/23/03/2303.17200.json",
    "total_tokens": 958,
    "translated_title": "SynthVSR：使用合成监督实现可视语音识别的规模化提升",
    "translated_abstract": "最近在可视语音识别（VSR）领域中报道的最新成果通常依赖于越来越多的视频数据，而公开可用的转录视频数据集大小有限。本文首次研究了利用合成视觉数据进行VSR的潜力。我们的方法“SynthVSR”通过合成嘴唇动作显著提高了VSR系统的性能。SynthVSR背后的关键思想是利用一个基于语音驱动的唇部动画模型，该模型根据输入语音生成唇部动作。该语音驱动的唇部动画模型是在未标记的音视频数据集上训练的，并且可以在有标记的视频可用时进一步优化为预训练的VSR模型。由于存在大量的转录声学数据和面部图像，我们能够使用所提出的唇部动画模型生成大规模的合成数据用于半监督VSR训练。我们在“Lip Reading in the Wild”（Lrw）评测基准上评估了我们方法的性能。",
    "tldr": "本文首次探讨利用合成视觉数据进行可视语音识别（VSR）的潜力，提出的方法SynthVSR通过利用语音驱动的唇部动画模型合成大规模数据，极大地提高了VSR系统的性能。",
    "en_tdlr": "This paper explores the potential of leveraging synthetic visual data for visual speech recognition (VSR), and proposes a method called SynthVSR, which uses a speech-driven lip animation model to generate large-scale synthetic data for semi-supervised VSR training. The SynthVSR substantially improves the performance of VSR systems and achieves state-of-the-art results on the \"Lip Reading in the Wild\" benchmark."
}