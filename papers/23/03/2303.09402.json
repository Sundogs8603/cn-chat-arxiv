{
    "title": "ToxVis: Enabling Interpretability of Implicit vs. Explicit Toxicity Detection Models with Interactive Visualization. (arXiv:2303.09402v1 [cs.CL])",
    "abstract": "The rise of hate speech on online platforms has led to an urgent need for effective content moderation. However, the subjective and multi-faceted nature of hateful online content, including implicit hate speech, poses significant challenges to human moderators and content moderation systems. To address this issue, we developed ToxVis, a visually interactive and explainable tool for classifying hate speech into three categories: implicit, explicit, and non-hateful. We fine-tuned two transformer-based models using RoBERTa, XLNET, and GPT-3 and used deep learning interpretation techniques to provide explanations for the classification results. ToxVis enables users to input potentially hateful text and receive a classification result along with a visual explanation of which words contributed most to the decision. By making the classification process explainable, ToxVis provides a valuable tool for understanding the nuances of hateful content and supporting more effective content moderation",
    "link": "http://arxiv.org/abs/2303.09402",
    "context": "Title: ToxVis: Enabling Interpretability of Implicit vs. Explicit Toxicity Detection Models with Interactive Visualization. (arXiv:2303.09402v1 [cs.CL])\nAbstract: The rise of hate speech on online platforms has led to an urgent need for effective content moderation. However, the subjective and multi-faceted nature of hateful online content, including implicit hate speech, poses significant challenges to human moderators and content moderation systems. To address this issue, we developed ToxVis, a visually interactive and explainable tool for classifying hate speech into three categories: implicit, explicit, and non-hateful. We fine-tuned two transformer-based models using RoBERTa, XLNET, and GPT-3 and used deep learning interpretation techniques to provide explanations for the classification results. ToxVis enables users to input potentially hateful text and receive a classification result along with a visual explanation of which words contributed most to the decision. By making the classification process explainable, ToxVis provides a valuable tool for understanding the nuances of hateful content and supporting more effective content moderation",
    "path": "papers/23/03/2303.09402.json",
    "total_tokens": 934,
    "translated_title": "ToxVis：通过交互式可视化增强隐式与显式毒性检测模型的可解释性",
    "translated_abstract": "在线平台上仇恨言论的崛起导致了有效内容审核的迫切需求。然而，包括隐式仇恨言论在内的令人反感的在线内容具有主观性和多方面性，对于人类审核员和内容审核系统都存在重大挑战。为了解决这个问题，我们开发了ToxVis，一个可视化互动和可解释的工具，用于将令人反感的言论分为三类：隐式的、显式的和非令人反感的。我们使用RoBERTa、XLNET和GPT-3 Fine-tune了两个基于transformer的模型，并使用深度学习解释技术来提供分类结果的解释。ToxVis使用户能够输入可能的令人反感的文本，并获得分类结果以及哪些单词对该决策做出了最大的贡献的可视化解释。通过使分类过程具有可解释性，ToxVis为了解令人反感的内容的微妙之处以及支持更有效的内容审核提供了有价值的工具。",
    "tldr": "ToxVis是一个可视化互动的工具，用于将在线内容分为隐式、显式和非令人反感的三类，利用深度学习解释技术提供分类结果解释，并为了解令人反感内容和支持更有效的内容审核提供了有价值的工具。",
    "en_tdlr": "ToxVis is an interactive tool that visually classifies online content into three categories: implicit, explicit, and non-hateful, with explainable classification results using deep learning interpretation techniques, providing a valuable tool for understanding hateful content and supporting more effective content moderation."
}