{
    "title": "On the Importance of Feature Separability in Predicting Out-Of-Distribution Error. (arXiv:2303.15488v1 [cs.LG])",
    "abstract": "Estimating the generalization performance is practically challenging on out-of-distribution (OOD) data without ground truth labels. While previous methods emphasize the connection between distribution difference and OOD accuracy, we show that a large domain gap not necessarily leads to a low test accuracy. In this paper, we investigate this problem from the perspective of feature separability, and propose a dataset-level score based upon feature dispersion to estimate the test accuracy under distribution shift. Our method is inspired by desirable properties of features in representation learning: high inter-class dispersion and high intra-class compactness. Our analysis shows that inter-class dispersion is strongly correlated with the model accuracy, while intra-class compactness does not reflect the generalization performance on OOD data. Extensive experiments demonstrate the superiority of our method in both prediction performance and computational efficiency.",
    "link": "http://arxiv.org/abs/2303.15488",
    "context": "Title: On the Importance of Feature Separability in Predicting Out-Of-Distribution Error. (arXiv:2303.15488v1 [cs.LG])\nAbstract: Estimating the generalization performance is practically challenging on out-of-distribution (OOD) data without ground truth labels. While previous methods emphasize the connection between distribution difference and OOD accuracy, we show that a large domain gap not necessarily leads to a low test accuracy. In this paper, we investigate this problem from the perspective of feature separability, and propose a dataset-level score based upon feature dispersion to estimate the test accuracy under distribution shift. Our method is inspired by desirable properties of features in representation learning: high inter-class dispersion and high intra-class compactness. Our analysis shows that inter-class dispersion is strongly correlated with the model accuracy, while intra-class compactness does not reflect the generalization performance on OOD data. Extensive experiments demonstrate the superiority of our method in both prediction performance and computational efficiency.",
    "path": "papers/23/03/2303.15488.json",
    "total_tokens": 855,
    "translated_title": "论特征可分性在预测分布外误差中的重要性",
    "translated_abstract": "在没有基准标签的分布外数据的泛化性能估计实际上很有挑战性。虽然以前的方法强调分布差异与分布外精度之间的联系，但我们表明，大的域间差异并不一定导致低的测试准确度。本文从特征可分性的角度研究了这个问题，并提出了一种基于特征离散度的数据集级别的分数，以估计在分布移位下的测试准确度。我们的方法是受表征学习中特征良好属性的启示：高内类离散度和高内类紧致度。我们的分析表明，内类离散度与模型准确度强相关，而内类紧致度不反映分布外数据的泛化性能。大量实验证明了我们的方法在预测性能和计算效率方面的优越性。",
    "tldr": "本文研究发现，特征可分性对于模型在分布移位下的测试准确度有着重要作用。作者提出了一种基于特征离散度的分数用于估计测试准确度并在实验证明了该方法的优越性。",
    "en_tdlr": "This paper shows that feature separability plays an important role in the test accuracy of models under distribution shift. The authors propose a dataset-level score based on feature dispersion to estimate the test accuracy, and extensive experiments demonstrate the superiority of this method."
}