{
    "title": "Dynamic Update-to-Data Ratio: Minimizing World Model Overfitting. (arXiv:2303.10144v1 [cs.LG])",
    "abstract": "Early stopping based on the validation set performance is a popular approach to find the right balance between under- and overfitting in the context of supervised learning. However, in reinforcement learning, even for supervised sub-problems such as world model learning, early stopping is not applicable as the dataset is continually evolving. As a solution, we propose a new general method that dynamically adjusts the update to data (UTD) ratio during training based on under- and overfitting detection on a small subset of the continuously collected experience not used for training. We apply our method to DreamerV2, a state-of-the-art model-based reinforcement learning algorithm, and evaluate it on the DeepMind Control Suite and the Atari $100$k benchmark. The results demonstrate that one can better balance under- and overestimation by adjusting the UTD ratio with our approach compared to the default setting in DreamerV2 and that it is competitive with an extensive hyperparameter search ",
    "link": "http://arxiv.org/abs/2303.10144",
    "context": "Title: Dynamic Update-to-Data Ratio: Minimizing World Model Overfitting. (arXiv:2303.10144v1 [cs.LG])\nAbstract: Early stopping based on the validation set performance is a popular approach to find the right balance between under- and overfitting in the context of supervised learning. However, in reinforcement learning, even for supervised sub-problems such as world model learning, early stopping is not applicable as the dataset is continually evolving. As a solution, we propose a new general method that dynamically adjusts the update to data (UTD) ratio during training based on under- and overfitting detection on a small subset of the continuously collected experience not used for training. We apply our method to DreamerV2, a state-of-the-art model-based reinforcement learning algorithm, and evaluate it on the DeepMind Control Suite and the Atari $100$k benchmark. The results demonstrate that one can better balance under- and overestimation by adjusting the UTD ratio with our approach compared to the default setting in DreamerV2 and that it is competitive with an extensive hyperparameter search ",
    "path": "papers/23/03/2303.10144.json",
    "total_tokens": 974,
    "translated_title": "动态更新数据比率：减少世界模型过拟合",
    "translated_abstract": "在监督学习的情境下，基于验证集表现的早期停止是一种流行的方法，可以找到欠拟合和过拟合之间的平衡。然而，在强化学习中，即使在诸如世界模型学习之类的监督子问题中，也不能应用早期停止，因为数据集在不断演变。为此，我们提出了一种新的通用方法，根据在未参与训练的一小部分连续收集的经验上检测欠拟合和过拟合来动态调整训练中的更新到数据比率（UTD）。我们将该方法应用于DreamerV2，这是一种最先进的基于模型的强化学习算法，并在DeepMind Control Suite和Atari $100$k基准测试上进行评估。结果表明，与DreamerV2中的默认设置相比，通过调整UTD比率来平衡欠拟合与过拟合的效果更好，而且具有与广泛超参数搜索竞争的能力。",
    "tldr": "提出了一种动态调整更新到数据比率（UTD）的方法，根据小规模的未用于训练的连续收集的经验上检测欠拟合和过拟合。该方法应用于最先进的基于模型的强化学习算法DreamerV2，可以更好地平衡欠拟合和过拟合，并且与广泛的超参数搜索具有竞争力。",
    "en_tdlr": "A new method is proposed to dynamically adjust the update to data (UTD) ratio during training based on under and overfitting detection on a small subset of continuously collected experience. The proposed method is applied to the state-of-the-art model-based reinforcement learning algorithm DreamerV2, demonstrating competitive performance with extensive hyperparameter search while better balancing under and overfitting."
}