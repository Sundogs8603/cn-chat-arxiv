{
    "title": "Efficient Parallel Split Learning over Resource-constrained Wireless Edge Networks. (arXiv:2303.15991v1 [cs.LG])",
    "abstract": "The increasingly deeper neural networks hinder the democratization of privacy-enhancing distributed learning, such as federated learning (FL), to resource-constrained devices. To overcome this challenge, in this paper, we advocate the integration of edge computing paradigm and parallel split learning (PSL), allowing multiple client devices to offload substantial training workloads to an edge server via layer-wise model split. By observing that existing PSL schemes incur excessive training latency and large volume of data transmissions, we propose an innovative PSL framework, namely, efficient parallel split learning (EPSL), to accelerate model training. To be specific, EPSL parallelizes client-side model training and reduces the dimension of local gradients for back propagation (BP) via last-layer gradient aggregation, leading to a significant reduction in server-side training and communication latency. Moreover, by considering the heterogeneous channel conditions and computing capabil",
    "link": "http://arxiv.org/abs/2303.15991",
    "context": "Title: Efficient Parallel Split Learning over Resource-constrained Wireless Edge Networks. (arXiv:2303.15991v1 [cs.LG])\nAbstract: The increasingly deeper neural networks hinder the democratization of privacy-enhancing distributed learning, such as federated learning (FL), to resource-constrained devices. To overcome this challenge, in this paper, we advocate the integration of edge computing paradigm and parallel split learning (PSL), allowing multiple client devices to offload substantial training workloads to an edge server via layer-wise model split. By observing that existing PSL schemes incur excessive training latency and large volume of data transmissions, we propose an innovative PSL framework, namely, efficient parallel split learning (EPSL), to accelerate model training. To be specific, EPSL parallelizes client-side model training and reduces the dimension of local gradients for back propagation (BP) via last-layer gradient aggregation, leading to a significant reduction in server-side training and communication latency. Moreover, by considering the heterogeneous channel conditions and computing capabil",
    "path": "papers/23/03/2303.15991.json",
    "total_tokens": 1179,
    "translated_title": "面向资源受限的无线边缘网络的高效并行分裂学习",
    "translated_abstract": "随着神经网络越来越深，这阻碍了联合学习等隐私增强分布式学习方式（如联邦学习）在资源受限的设备上的民主化。为了解决这个挑战，本文倡导将边缘计算范式和并行分裂学习（PSL）相结合，允许多个客户端设备通过逐层模型分裂将大量的训练工作负载卸载到边缘服务器上。通过观察到现有的PSL方案会产生过多的训练延迟和大量的数据传输，我们提出了一种创新的PSL框架——高效并行分裂学习（EPSL），以加速模型训练。具体而言，EPSL将客户端模型训练并行化，并通过最后一层梯度聚合降低了反向传播（BP）的局部梯度维度，从而显著减少了服务器端的训练和通信延迟。此外，通过考虑边缘设备的异构通道条件和计算能力，我们设计了资源分配算法以优化计算和通信资源分配。实验结果表明，EPSL通过将通信成本和训练时间分别降低76％和63％，优于最先进的PSL方法。",
    "tldr": "本文提出了面向资源受限的无线边缘网络的高效并行分裂学习（EPSL）框架，旨在加速模型训练。EPSL并行化客户端模型训练，通过聚合梯度降低了反向传播的局部梯度维度，从而显著减少了服务器端的训练和通信延迟。同时，EPSL还设计了资源分配算法以优化计算和通信资源分配。",
    "en_tdlr": "This paper proposes an efficient parallel split learning (EPSL) framework for resource-constrained wireless edge networks, which aims to accelerate model training. EPSL parallelizes client-side model training and reduces the dimension of local gradients for backpropagation via last-layer gradient aggregation, significantly reducing server-side training and communication latency. Additionally, EPSL designs a resource allocation algorithm to optimize the computation and communication resource allocation, outperforming state-of-the-art PSL methods by up to 76% and 63% in terms of reducing communication cost and training time."
}