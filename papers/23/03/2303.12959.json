{
    "title": "Variantional autoencoder with decremental information bottleneck for disentanglement. (arXiv:2303.12959v1 [cs.LG])",
    "abstract": "One major challenge of disentanglement learning with variational autoencoders is the trade-off between disentanglement and reconstruction fidelity. Previous incremental methods with only on latent space cannot optimize these two targets simultaneously, so they expand the Information Bottleneck while training to {optimize from disentanglement to reconstruction. However, a large bottleneck will lose the constraint of disentanglement, causing the information diffusion problem. To tackle this issue, we present a novel decremental variational autoencoder with disentanglement-invariant transformations to optimize multiple objectives in different layers, termed DeVAE, for balancing disentanglement and reconstruction fidelity by decreasing the information bottleneck of diverse latent spaces gradually. Benefiting from the multiple latent spaces, DeVAE allows simultaneous optimization of multiple objectives to optimize reconstruction while keeping the constraint of disentanglement, avoiding info",
    "link": "http://arxiv.org/abs/2303.12959",
    "context": "Title: Variantional autoencoder with decremental information bottleneck for disentanglement. (arXiv:2303.12959v1 [cs.LG])\nAbstract: One major challenge of disentanglement learning with variational autoencoders is the trade-off between disentanglement and reconstruction fidelity. Previous incremental methods with only on latent space cannot optimize these two targets simultaneously, so they expand the Information Bottleneck while training to {optimize from disentanglement to reconstruction. However, a large bottleneck will lose the constraint of disentanglement, causing the information diffusion problem. To tackle this issue, we present a novel decremental variational autoencoder with disentanglement-invariant transformations to optimize multiple objectives in different layers, termed DeVAE, for balancing disentanglement and reconstruction fidelity by decreasing the information bottleneck of diverse latent spaces gradually. Benefiting from the multiple latent spaces, DeVAE allows simultaneous optimization of multiple objectives to optimize reconstruction while keeping the constraint of disentanglement, avoiding info",
    "path": "papers/23/03/2303.12959.json",
    "total_tokens": 892,
    "translated_title": "变分自编码器中逐步减少信息瓶颈的去纠缠方法",
    "translated_abstract": "变分自编码器中去纠缠学习的一个主要挑战是在权衡去纠缠和重构保真度之间的平衡。之前仅在一个潜在空间中进行的逐步方法无法同时优化这两个目标，因此在训练过程中扩展了信息瓶颈，以从去纠缠到重构进行优化。然而，大型瓶颈会失去去纠缠的约束，导致信息扩散问题。为了解决这个问题，我们提出了一种新颖的逐步减少信息瓶颈的变分自编码器方法，使用去纠缠不变变换来优化不同层的多个目标，称为DeVAE。通过逐渐减小不同潜在空间的信息瓶颈，DeVAE 平衡了去纠缠和重构保真度。由于具有多个潜在空间，DeVAE 允许同时优化多个目标，以在保持去纠缠约束的同时优化重构，避免信息扩散问题。",
    "tldr": "本论文提出了一种逐步减少信息瓶颈的变分自编码器方法，使用去纠缠不变变换来平衡去纠缠和重构保真度，避免信息扩散问题。",
    "en_tdlr": "This paper proposes a decremental variational autoencoder method with disentanglement-invariant transformations to balance disentanglement and reconstruction fidelity by gradually decreasing the information bottleneck of diverse latent spaces, avoiding the problem of information diffusion."
}