{
    "title": "Zero-shot Referring Image Segmentation with Global-Local Context Features. (arXiv:2303.17811v1 [cs.CV])",
    "abstract": "Referring image segmentation (RIS) aims to find a segmentation mask given a referring expression grounded to a region of the input image. Collecting labelled datasets for this task, however, is notoriously costly and labor-intensive. To overcome this issue, we propose a simple yet effective zero-shot referring image segmentation method by leveraging the pre-trained cross-modal knowledge from CLIP. In order to obtain segmentation masks grounded to the input text, we propose a mask-guided visual encoder that captures global and local contextual information of an input image. By utilizing instance masks obtained from off-the-shelf mask proposal techniques, our method is able to segment fine-detailed Istance-level groundings. We also introduce a global-local text encoder where the global feature captures complex sentence-level semantics of the entire input expression while the local feature focuses on the target noun phrase extracted by a dependency parser. In our experiments, the proposed",
    "link": "http://arxiv.org/abs/2303.17811",
    "context": "Title: Zero-shot Referring Image Segmentation with Global-Local Context Features. (arXiv:2303.17811v1 [cs.CV])\nAbstract: Referring image segmentation (RIS) aims to find a segmentation mask given a referring expression grounded to a region of the input image. Collecting labelled datasets for this task, however, is notoriously costly and labor-intensive. To overcome this issue, we propose a simple yet effective zero-shot referring image segmentation method by leveraging the pre-trained cross-modal knowledge from CLIP. In order to obtain segmentation masks grounded to the input text, we propose a mask-guided visual encoder that captures global and local contextual information of an input image. By utilizing instance masks obtained from off-the-shelf mask proposal techniques, our method is able to segment fine-detailed Istance-level groundings. We also introduce a global-local text encoder where the global feature captures complex sentence-level semantics of the entire input expression while the local feature focuses on the target noun phrase extracted by a dependency parser. In our experiments, the proposed",
    "path": "papers/23/03/2303.17811.json",
    "total_tokens": 1052,
    "tldr": "该论文提出了一种通过利用预训练的CLIP跨模态知识的零样本引用图像分割方法, 通过mask-guided visual encoder捕获全局和本地上下文信息, 引入全局-局部文本编码器捕获整个输入引用表达的复杂语义和目标名词短语。",
    "en_tdlr": "This paper proposes a zero-shot referring image segmentation method by leveraging pre-trained cross-modal knowledge from CLIP. A mask-guided visual encoder is introduced to capture global and local contextual information, and a global-local text encoder is utilized to capture complex sentence-level semantics and target noun phrase. The proposed method achieves fine-detailed Instance-level groundings."
}