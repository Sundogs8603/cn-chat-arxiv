{
    "title": "Visual Chain-of-Thought Diffusion Models. (arXiv:2303.16187v1 [cs.CV])",
    "abstract": "Recent progress with conditional image diffusion models has been stunning, and this holds true whether we are speaking about models conditioned on a text description, a scene layout, or a sketch. Unconditional image diffusion models are also improving but lag behind, as do diffusion models which are conditioned on lower-dimensional features like class labels. We propose to close the gap between conditional and unconditional models using a two-stage sampling procedure. In the first stage we sample an embedding describing the semantic content of the image. In the second stage we sample the image conditioned on this embedding and then discard the embedding. Doing so lets us leverage the power of conditional diffusion models on the unconditional generation task, which we show improves FID by 25-50% compared to standard unconditional generation.",
    "link": "http://arxiv.org/abs/2303.16187",
    "context": "Title: Visual Chain-of-Thought Diffusion Models. (arXiv:2303.16187v1 [cs.CV])\nAbstract: Recent progress with conditional image diffusion models has been stunning, and this holds true whether we are speaking about models conditioned on a text description, a scene layout, or a sketch. Unconditional image diffusion models are also improving but lag behind, as do diffusion models which are conditioned on lower-dimensional features like class labels. We propose to close the gap between conditional and unconditional models using a two-stage sampling procedure. In the first stage we sample an embedding describing the semantic content of the image. In the second stage we sample the image conditioned on this embedding and then discard the embedding. Doing so lets us leverage the power of conditional diffusion models on the unconditional generation task, which we show improves FID by 25-50% compared to standard unconditional generation.",
    "path": "papers/23/03/2303.16187.json",
    "total_tokens": 773,
    "translated_title": "可视化思维传递模型",
    "translated_abstract": "条件图像扩散模型的近期进展是惊人的，这适用于以文本描述、场景布局或素描为条件的模型。无条件图像扩散模型也在改进，但落后于条件模型，以类标签为条件的扩散模型亦是如此。本文提出使用两阶段采样过程，缩小条件和无条件模型之间的差距。在第一阶段中，我们采样描述图像语义内容的嵌入。在第二阶段中，我们在这个嵌入的条件下采样图像，然后丢弃这个嵌入。这样做让我们利用条件扩散模型的强大功能来进行无条件生成任务，我们证明相对于标准无条件生成，FID（Frechet inception distance）最多提高了25-50%。",
    "tldr": "本文提出了一种两阶段采样过程，使用可视化思维传递模型来缩小条件和无条件模型之间的差距，相对标准无条件生成，FID提高25-50%。",
    "en_tdlr": "The paper proposes a two-stage sampling process using Visual Chain-of-Thought Diffusion Models to narrow the gap between conditional and unconditional generation models. The FID can improve by 25-50% compared to standard unconditional generation."
}