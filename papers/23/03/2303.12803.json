{
    "title": "Evolving Populations of Diverse RL Agents with MAP-Elites. (arXiv:2303.12803v1 [cs.NE])",
    "abstract": "Quality Diversity (QD) has emerged as a powerful alternative optimization paradigm that aims at generating large and diverse collections of solutions, notably with its flagship algorithm MAP-ELITES (ME) which evolves solutions through mutations and crossovers. While very effective for some unstructured problems, early ME implementations relied exclusively on random search to evolve the population of solutions, rendering them notoriously sample-inefficient for high-dimensional problems, such as when evolving neural networks. Follow-up works considered exploiting gradient information to guide the search in order to address these shortcomings through techniques borrowed from either Black-Box Optimization (BBO) or Reinforcement Learning (RL). While mixing RL techniques with ME unlocked state-of-the-art performance for robotics control problems that require a good amount of exploration, it also plagued these ME variants with limitations common among RL algorithms that ME was free of, such a",
    "link": "http://arxiv.org/abs/2303.12803",
    "context": "Title: Evolving Populations of Diverse RL Agents with MAP-Elites. (arXiv:2303.12803v1 [cs.NE])\nAbstract: Quality Diversity (QD) has emerged as a powerful alternative optimization paradigm that aims at generating large and diverse collections of solutions, notably with its flagship algorithm MAP-ELITES (ME) which evolves solutions through mutations and crossovers. While very effective for some unstructured problems, early ME implementations relied exclusively on random search to evolve the population of solutions, rendering them notoriously sample-inefficient for high-dimensional problems, such as when evolving neural networks. Follow-up works considered exploiting gradient information to guide the search in order to address these shortcomings through techniques borrowed from either Black-Box Optimization (BBO) or Reinforcement Learning (RL). While mixing RL techniques with ME unlocked state-of-the-art performance for robotics control problems that require a good amount of exploration, it also plagued these ME variants with limitations common among RL algorithms that ME was free of, such a",
    "path": "papers/23/03/2303.12803.json",
    "total_tokens": 856,
    "translated_title": "基于MAP-Elites的多样化强化学习智能体群体进化",
    "translated_abstract": "品质多样性(QD)已成为一种强大的替代优化模式，旨在生成大量和多样的解决方案，其代表算法MAP-Elites(ME)通过变异和交叉进化解决方案。虽然ME对于某些非结构化问题非常有效，但早期的ME实现仅依赖于随机搜索来进化解决方案的种群，因此在高维问题中如进化神经网络时常常无法有效地进行，效率极低。后续的研究考虑利用梯度信息通过黑盒优化（BBO）或强化学习（RL）中的技术来引导搜索，以解决这些问题。将RL技巧与ME结合在机器人控制问题中实现了最先进的性能，但是也在ME变体中引入了这些算法的共同限制，例如对探索需求的要求。",
    "tldr": "本文介绍了一种尝试在进化计算与强化学习中相结合解决机器人控制问题的方法，并通过改进ME算法提高了效率和多样性，但也出现了一些常见强化学习算法的限制。",
    "en_tdlr": "The paper proposes a method that combines evolutionary computation and reinforcement learning to solve robotics control problems, and improves the efficiency and diversity of the MAP-Elites algorithm. However, it also introduces some common limitations of RL algorithms."
}