{
    "title": "Dynamic-Aware Loss for Learning with Label Noise. (arXiv:2303.11562v1 [cs.LG])",
    "abstract": "Label noise poses a serious threat to deep neural networks (DNNs). Employing robust loss function which reconciles fitting ability with robustness is a simple but effective strategy to handle this problem. However, the widely-used static trade-off between these two factors contradicts the dynamic nature of DNNs learning with label noise, leading to inferior performance. Therefore, we propose a dynamics-aware loss (DAL) to solve this problem. Considering that DNNs tend to first learn generalized patterns, then gradually overfit label noise, DAL strengthens the fitting ability initially, then gradually increases the weight of robustness. Moreover, at the later stage, we let DNNs put more emphasis on easy examples which are more likely to be correctly labeled than hard ones and introduce a bootstrapping term to further reduce the negative impact of label noise. Both the detailed theoretical analyses and extensive experimental results demonstrate the superiority of our method.",
    "link": "http://arxiv.org/abs/2303.11562",
    "context": "Title: Dynamic-Aware Loss for Learning with Label Noise. (arXiv:2303.11562v1 [cs.LG])\nAbstract: Label noise poses a serious threat to deep neural networks (DNNs). Employing robust loss function which reconciles fitting ability with robustness is a simple but effective strategy to handle this problem. However, the widely-used static trade-off between these two factors contradicts the dynamic nature of DNNs learning with label noise, leading to inferior performance. Therefore, we propose a dynamics-aware loss (DAL) to solve this problem. Considering that DNNs tend to first learn generalized patterns, then gradually overfit label noise, DAL strengthens the fitting ability initially, then gradually increases the weight of robustness. Moreover, at the later stage, we let DNNs put more emphasis on easy examples which are more likely to be correctly labeled than hard ones and introduce a bootstrapping term to further reduce the negative impact of label noise. Both the detailed theoretical analyses and extensive experimental results demonstrate the superiority of our method.",
    "path": "papers/23/03/2303.11562.json",
    "total_tokens": 915,
    "translated_title": "动态感知损失函数用于标签噪声学习",
    "translated_abstract": "标签噪声对深度神经网络(DNN)构成严重威胁。使用既能拟合又具有鲁棒性的强健损失函数是处理此问题的一种简单而有效的策略。然而，这两个因素之间的静态权衡与DNN学习标签噪声的动态性相矛盾，导致性能不佳。因此，我们提出动态感知损失(DAL)来解决这个问题。考虑到DNN倾向于先学习一般化的模式，然后逐渐过拟合标签噪声，DAL最初增强了拟合能力，然后逐渐增加了鲁棒性的权重。此外，在后期阶段，我们让DNN更加重视容易的样例，这些样例更容易标记为正确的标签，并引入自举项来进一步减少标签噪声的负面影响。详细的理论分析和广泛的实验结果都证明了我们方法的优越性。",
    "tldr": "本文提出一种动态感知损失函数，采用增强拟合能力，逐渐增加鲁棒性的权重来处理标签噪声。在后期阶段，引入自举项，让DNN更加重视容易的样例，证明了这种方法的优越性。",
    "en_tdlr": "This paper proposes a dynamic-aware loss function to handle label noise, which starts with boosting fitting ability and gradually increasing the weight of robustness. In the later stage, a bootstrapping term is introduced to emphasize easy examples, demonstrating its superiority."
}