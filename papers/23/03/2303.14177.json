{
    "title": "Scaling Expert Language Models with Unsupervised Domain Discovery. (arXiv:2303.14177v1 [cs.CL])",
    "abstract": "Large language models are typically trained densely: all parameters are updated with respect to all inputs. This requires synchronization of billions of parameters across thousands of GPUs. We introduce a simple but effective method to asynchronously train large, sparse language models on arbitrary text corpora. Our method clusters a corpus into sets of related documents, trains a separate expert language model on each cluster, and combines them in a sparse ensemble for inference. This approach generalizes embarrassingly parallel training by automatically discovering the domains for each expert, and eliminates nearly all the communication overhead of existing sparse language models. Our technique outperforms dense baselines on multiple corpora and few-shot tasks, and our analysis shows that specializing experts to meaningful clusters is key to these gains. Performance also improves with the number of experts and size of training data, suggesting this is a highly efficient and accessibl",
    "link": "http://arxiv.org/abs/2303.14177",
    "context": "Title: Scaling Expert Language Models with Unsupervised Domain Discovery. (arXiv:2303.14177v1 [cs.CL])\nAbstract: Large language models are typically trained densely: all parameters are updated with respect to all inputs. This requires synchronization of billions of parameters across thousands of GPUs. We introduce a simple but effective method to asynchronously train large, sparse language models on arbitrary text corpora. Our method clusters a corpus into sets of related documents, trains a separate expert language model on each cluster, and combines them in a sparse ensemble for inference. This approach generalizes embarrassingly parallel training by automatically discovering the domains for each expert, and eliminates nearly all the communication overhead of existing sparse language models. Our technique outperforms dense baselines on multiple corpora and few-shot tasks, and our analysis shows that specializing experts to meaningful clusters is key to these gains. Performance also improves with the number of experts and size of training data, suggesting this is a highly efficient and accessibl",
    "path": "papers/23/03/2303.14177.json",
    "total_tokens": 976,
    "translated_title": "用无监督的领域发现方法扩展专家语言模型",
    "translated_abstract": "大型语言模型通常进行密集训练：所有参数均对所有输入进行更新。这要求在数千个GPU之间同步数十亿个参数。我们引入了一种简单而有效的方法，能够异步地在任意文本语料库上训练大型稀疏语言模型。我们的方法将一个语料库聚类成相关文档集，对每个集群训练一个单独的专家语言模型，然后在推理时将它们组合成稀疏的集合。这种方法通过自动发现每个专家的领域来推广了尴尬平行训练，并消除了现有稀疏语言模型中几乎所有的通信开销。我们的技术在多个语料库和少量训练任务上优于密集基线，并且我们的分析表明将专家特化到有意义的集群是取得这些增益的关键。性能还随着专家数量和训练数据的大小而提高，这表明这是一种高效且可访问的缩放专家语言模型的方法。",
    "tldr": "该论文提出了一种用于扩展专家语言模型的简单而有效的方法，使用无监督的领域发现来自动化训练并消除通信开销，通过将语料库聚类成相关文档集来训练单独的专家语言模型，并将它们组合成一个稀疏的集合进行推理。",
    "en_tdlr": "This paper proposes a simple and effective method for scaling expert language models using unsupervised domain discovery, which automates training and eliminates communication overhead by clustering a corpus into related document sets and training separate expert language models on each cluster, combining them into a sparse ensemble for inference. The technique outperforms dense baselines on multiple corpora and few-shot tasks, with improved performance as the number of experts and size of training data increase."
}