{
    "title": "$\\mathcal{C}^k$-continuous Spline Approximation with TensorFlow Gradient Descent Optimizers. (arXiv:2303.12454v1 [cs.LG])",
    "abstract": "In this work we present an \"out-of-the-box\" application of Machine Learning (ML) optimizers for an industrial optimization problem. We introduce a piecewise polynomial model (spline) for fitting of $\\mathcal{C}^k$-continuos functions, which can be deployed in a cam approximation setting. We then use the gradient descent optimization context provided by the machine learning framework TensorFlow to optimize the model parameters with respect to approximation quality and $\\mathcal{C}^k$-continuity and evaluate available optimizers. Our experiments show that the problem solution is feasible using TensorFlow gradient tapes and that AMSGrad and SGD show the best results among available TensorFlow optimizers. Furthermore, we introduce a novel regularization approach to improve SGD convergence. Although experiments show that remaining discontinuities after optimization are small, we can eliminate these errors using a presented algorithm which has impact only on affected derivatives in the local",
    "link": "http://arxiv.org/abs/2303.12454",
    "context": "Title: $\\mathcal{C}^k$-continuous Spline Approximation with TensorFlow Gradient Descent Optimizers. (arXiv:2303.12454v1 [cs.LG])\nAbstract: In this work we present an \"out-of-the-box\" application of Machine Learning (ML) optimizers for an industrial optimization problem. We introduce a piecewise polynomial model (spline) for fitting of $\\mathcal{C}^k$-continuos functions, which can be deployed in a cam approximation setting. We then use the gradient descent optimization context provided by the machine learning framework TensorFlow to optimize the model parameters with respect to approximation quality and $\\mathcal{C}^k$-continuity and evaluate available optimizers. Our experiments show that the problem solution is feasible using TensorFlow gradient tapes and that AMSGrad and SGD show the best results among available TensorFlow optimizers. Furthermore, we introduce a novel regularization approach to improve SGD convergence. Although experiments show that remaining discontinuities after optimization are small, we can eliminate these errors using a presented algorithm which has impact only on affected derivatives in the local",
    "path": "papers/23/03/2303.12454.json",
    "total_tokens": 986,
    "translated_title": "用TensorFlow梯度下降优化器实现$\\mathcal{C}^k$连续的样条函数逼近",
    "translated_abstract": "本文介绍了一种使用机器学习优化器解决工业优化问题的方法。我们提出了一个分段式多项式模型（样条函数）来拟合$\\mathcal{C}^k$连续函数，并将其应用于磨齿传动装置近似问题。然后，我们利用TensorFlow机器学习框架提供的梯度下降优化上下文，优化模型参数，以达到逼近质量和$\\mathcal{C}^k$连续性较好的效果，并评估可用的优化器。实验结果表明，使用TensorFlow梯度磁带可以解决该问题，而AMSGrad和SGD是可用的TensorFlow优化器中表现最佳的。此外，我们还介绍了一种新颖的正则化方法来提高SGD收敛性。尽管实验表明，优化后剩余的不连续性很小，但我们仍然可以使用一种算法来消除这些错误，该算法只对局部受影响的导数产生影响。",
    "tldr": "本研究利用TensorFlow梯度下降优化器，提出了一种满足$\\mathcal{C}^k$连续的样条函数逼近方法，并评估了不同的优化器。实验结果表明，SGD和AMSGrad是表现最佳的TensorFlow优化器，并引入了正则化方法以提高收敛性。通过一个算法，我们还可以消除局部受影响的导数上的误差。",
    "en_tdlr": "This study proposes a TensorFlow-gradient descent optimizer to approximate $\\mathcal{C}^k$-continuous spline functions and evaluates available optimizers. Experimental results show that both SGD and AMSGrad perform well and a novel regularization method is introduced to improve convergence. An algorithm is presented to eliminate errors on affected derivatives."
}