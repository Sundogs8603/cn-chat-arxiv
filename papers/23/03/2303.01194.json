{
    "title": "UZH_CLyp at SemEval-2023 Task 9: Head-First Fine-Tuning and ChatGPT Data Generation for Cross-Lingual Learning in Tweet Intimacy Prediction. (arXiv:2303.01194v2 [cs.CL] UPDATED)",
    "abstract": "This paper describes the submission of UZH_CLyp for the SemEval 2023 Task 9 \"Multilingual Tweet Intimacy Analysis\". We achieved second-best results in all 10 languages according to the official Pearson's correlation regression evaluation measure. Our cross-lingual transfer learning approach explores the benefits of using a Head-First Fine-Tuning method (HeFiT) that first updates only the regression head parameters and then also updates the pre-trained transformer encoder parameters at a reduced learning rate. Additionally, we study the impact of using a small set of automatically generated examples (in our case, from ChatGPT) for low-resource settings where no human-labeled data is available. Our study shows that HeFiT stabilizes training and consistently improves results for pre-trained models that lack domain adaptation to tweets. Our study also shows a noticeable performance increase in cross-lingual learning when synthetic data is used, confirming the usefulness of current text gen",
    "link": "http://arxiv.org/abs/2303.01194",
    "context": "Title: UZH_CLyp at SemEval-2023 Task 9: Head-First Fine-Tuning and ChatGPT Data Generation for Cross-Lingual Learning in Tweet Intimacy Prediction. (arXiv:2303.01194v2 [cs.CL] UPDATED)\nAbstract: This paper describes the submission of UZH_CLyp for the SemEval 2023 Task 9 \"Multilingual Tweet Intimacy Analysis\". We achieved second-best results in all 10 languages according to the official Pearson's correlation regression evaluation measure. Our cross-lingual transfer learning approach explores the benefits of using a Head-First Fine-Tuning method (HeFiT) that first updates only the regression head parameters and then also updates the pre-trained transformer encoder parameters at a reduced learning rate. Additionally, we study the impact of using a small set of automatically generated examples (in our case, from ChatGPT) for low-resource settings where no human-labeled data is available. Our study shows that HeFiT stabilizes training and consistently improves results for pre-trained models that lack domain adaptation to tweets. Our study also shows a noticeable performance increase in cross-lingual learning when synthetic data is used, confirming the usefulness of current text gen",
    "path": "papers/23/03/2303.01194.json",
    "total_tokens": 1160,
    "translated_title": "UZH_CLyp在SemEval-2023任务9中的表现：基于Head-First Fine-Tuning和ChatGPT数据生成的跨语言学习方法用于推文亲密度预测",
    "translated_abstract": "本文介绍了UZH_CLyp在SemEval 2023任务9“多语言推文亲密度分析”中的表现。我们在所有10种语言中均取得了第二好的结果，根据官方的Pearson相关系数回归评估指标。我们的跨语言迁移学习方法探索了使用Head-First Fine-Tuning方法（HeFiT）的益处，该方法首先仅更新回归头参数，然后再以降低的学习率更新预训练的transformer编码器参数。此外，我们研究了在低资源设置中使用一小组自动生成的示例（在我们的情况下，来自ChatGPT）对没有人工标记数据的情况的影响。我们的研究表明，HeFiT稳定了培训并且对于缺乏推文领域适应的预训练模型一致地提高了结果。我们的研究还表明，当使用合成数据时，跨语言学习的性能显着提高，证实了当前文本生成模型用于解决低资源情况的实用性。",
    "tldr": "本文介绍了UZH_CLyp在SemEval 2023任务9中的表现。我们的跨语言迁移学习方法包含了先使用Head-First Fine-Tuning（HeFiT）方法更新回归头参数，再降低学习率更新预训练transformer的参数。同时，我们研究了在没有人工标记数据的情况下，使用ChatGPT生成的自动小型样例来解决低资源问题。研究发现，HeFiT稳定训练并显著提高预训练模型的性能，使用合成数据时也能提高跨语言学习的性能。"
}