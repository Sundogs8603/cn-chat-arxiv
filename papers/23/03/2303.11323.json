{
    "title": "Tangent Bundle Convolutional Learning: from Manifolds to Cellular Sheaves and Back",
    "abstract": "arXiv:2303.11323v2 Announce Type: replace-cross  Abstract: In this work we introduce a convolution operation over the tangent bundle of Riemann manifolds in terms of exponentials of the Connection Laplacian operator. We define tangent bundle filters and tangent bundle neural networks (TNNs) based on this convolution operation, which are novel continuous architectures operating on tangent bundle signals, i.e. vector fields over the manifolds. Tangent bundle filters admit a spectral representation that generalizes the ones of scalar manifold filters, graph filters and standard convolutional filters in continuous time. We then introduce a discretization procedure, both in the space and time domains, to make TNNs implementable, showing that their discrete counterpart is a novel principled variant of the very recently introduced sheaf neural networks. We formally prove that this discretized architecture converges to the underlying continuous TNN. Finally, we numerically evaluate the effecti",
    "link": "https://arxiv.org/abs/2303.11323",
    "context": "Title: Tangent Bundle Convolutional Learning: from Manifolds to Cellular Sheaves and Back\nAbstract: arXiv:2303.11323v2 Announce Type: replace-cross  Abstract: In this work we introduce a convolution operation over the tangent bundle of Riemann manifolds in terms of exponentials of the Connection Laplacian operator. We define tangent bundle filters and tangent bundle neural networks (TNNs) based on this convolution operation, which are novel continuous architectures operating on tangent bundle signals, i.e. vector fields over the manifolds. Tangent bundle filters admit a spectral representation that generalizes the ones of scalar manifold filters, graph filters and standard convolutional filters in continuous time. We then introduce a discretization procedure, both in the space and time domains, to make TNNs implementable, showing that their discrete counterpart is a novel principled variant of the very recently introduced sheaf neural networks. We formally prove that this discretized architecture converges to the underlying continuous TNN. Finally, we numerically evaluate the effecti",
    "path": "papers/23/03/2303.11323.json",
    "total_tokens": 930,
    "translated_title": "Tangent Bundle Convolutional Learning: 从流形到细胞层状结构再回去",
    "translated_abstract": "在这项工作中，我们介绍了一种在黎曼流形的切丛上进行卷积操作的方法，这种方法是使用连接拉普拉斯算子的指数定义的。我们基于这种卷积操作定义了切丛滤波器和切丛神经网络（TNNs），这些是在切丛信号上操作的新型连续架构，即流形上的矢量场。切丛滤波器具有广义的谱表示，推广了标量流形滤波器、图滤波器和连续时间标准卷积滤波器的表示。然后我们介绍了一种离散化过程，涉及空间和时间域，使得TNNs可实现，并展示了它们的离散对应物是最近介绍的层神经网络的一个新的合理变种。我们正式证明这种离散化架构收敛于基础的连续TNN。最后，我们对这种方法的有效性进行了数值评估。",
    "tldr": "本文介绍了一种在黎曼流形的切丛上进行卷积操作的方法，并基于此定义了切丛滤波器和切丛神经网络（TNNs），这为连续架构提供了新颖的操作方式，最后证明了离散化后的架构收敛于连续架构。",
    "en_tdlr": "This paper introduces a convolution operation on the tangent bundle of Riemann manifolds, defining tangent bundle filters and tangent bundle neural networks (TNNs), providing a novel approach for continuous architectures, and proving the convergence of the discrete architecture to the continuous one."
}