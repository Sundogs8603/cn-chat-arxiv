{
    "title": "Deconstructing deep active inference. (arXiv:2303.01618v2 [cs.AI] UPDATED)",
    "abstract": "Active inference is a theory of perception, learning and decision making, which can be applied to neuroscience, robotics, and machine learning. Recently, reasearch has been taking place to scale up this framework using Monte-Carlo tree search and deep learning. The goal of this activity is to solve more complicated tasks using deep active inference. First, we review the existing literature, then, we progresively build a deep active inference agent. For two agents, we have experimented with five definitions of the expected free energy and three different action selection strategies. According to our experiments, the models able to solve the dSprites environment are the ones that maximise rewards. Finally, we compare the similarity of the representation learned by the layers of various agents using centered kernel alignment. Importantly, the agent maximising reward and the agent minimising expected free energy learn very similar representations except for the last layer of the critic net",
    "link": "http://arxiv.org/abs/2303.01618",
    "context": "Title: Deconstructing deep active inference. (arXiv:2303.01618v2 [cs.AI] UPDATED)\nAbstract: Active inference is a theory of perception, learning and decision making, which can be applied to neuroscience, robotics, and machine learning. Recently, reasearch has been taking place to scale up this framework using Monte-Carlo tree search and deep learning. The goal of this activity is to solve more complicated tasks using deep active inference. First, we review the existing literature, then, we progresively build a deep active inference agent. For two agents, we have experimented with five definitions of the expected free energy and three different action selection strategies. According to our experiments, the models able to solve the dSprites environment are the ones that maximise rewards. Finally, we compare the similarity of the representation learned by the layers of various agents using centered kernel alignment. Importantly, the agent maximising reward and the agent minimising expected free energy learn very similar representations except for the last layer of the critic net",
    "path": "papers/23/03/2303.01618.json",
    "total_tokens": 818,
    "translated_title": "解构深度主动推断",
    "translated_abstract": "主动推断是一种有关感知、学习和决策的理论，可应用于神经科学、机器人学和机器学习。最近，人们开始使用蒙特卡罗树搜索和深度学习来扩展这一框架，以使用深度主动推断解决更加复杂的任务。本文首先回顾了现有的文献，然后逐步构建了一个深度主动推断代理。对于两个代理，我们尝试了五种期望自由能定义和三种不同的动作选择策略。根据实验，能够解决 dSprites 环境的模型是那些最大化奖励的模型。最后，我们使用中心核对其来比较各个代理的层所学习的表示的相似性。重要的是，最大化奖励的代理和最小化期望自由能的代理在最后一个批评网络层之外学习到非常相似的表示。",
    "tldr": "本论文探讨了深度主动推断，实验研究表明最大化奖励的代理模型能够较好地解决复杂任务。"
}