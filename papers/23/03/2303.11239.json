{
    "title": "Training Invertible Neural Networks as Autoencoders. (arXiv:2303.11239v2 [cs.LG] UPDATED)",
    "abstract": "Autoencoders are able to learn useful data representations in an unsupervised matter and have been widely used in various machine learning and computer vision tasks. In this work, we present methods to train Invertible Neural Networks (INNs) as (variational) autoencoders which we call INN (variational) autoencoders. Our experiments on MNIST, CIFAR and CelebA show that for low bottleneck sizes our INN autoencoder achieves results similar to the classical autoencoder. However, for large bottleneck sizes our INN autoencoder outperforms its classical counterpart. Based on the empirical results, we hypothesize that INN autoencoders might not have any intrinsic information loss and thereby are not bounded to a maximal number of layers (depth) after which only suboptimal results can be achieved.",
    "link": "http://arxiv.org/abs/2303.11239",
    "context": "Title: Training Invertible Neural Networks as Autoencoders. (arXiv:2303.11239v2 [cs.LG] UPDATED)\nAbstract: Autoencoders are able to learn useful data representations in an unsupervised matter and have been widely used in various machine learning and computer vision tasks. In this work, we present methods to train Invertible Neural Networks (INNs) as (variational) autoencoders which we call INN (variational) autoencoders. Our experiments on MNIST, CIFAR and CelebA show that for low bottleneck sizes our INN autoencoder achieves results similar to the classical autoencoder. However, for large bottleneck sizes our INN autoencoder outperforms its classical counterpart. Based on the empirical results, we hypothesize that INN autoencoders might not have any intrinsic information loss and thereby are not bounded to a maximal number of layers (depth) after which only suboptimal results can be achieved.",
    "path": "papers/23/03/2303.11239.json",
    "total_tokens": 811,
    "translated_title": "将可逆神经网络作为自动编码器进行训练",
    "translated_abstract": "自动编码器能够以无监督的方式学习有用的数据表示，并已被广泛应用于各种机器学习和计算机视觉任务中。在本文中，我们提出了一种方法，将可逆神经网络（INN）作为（变分）自动编码器进行训练，称为INN（变分）自动编码器。我们在MNIST，CIFAR和CelebA数据集上的实验表明，在瓶颈大小较小的情况下，我们的INN自动编码器实现了与经典自动编码器类似的结果。然而，在大瓶颈大小的情况下，我们的INN自动编码器优于其经典对应物。基于实证结果，我们假设INN自动编码器可能没有任何固有信息损失，因此不受最大层数（深度）的限制，达到该层数后只能实现次优结果。",
    "tldr": "本文提出了使用INN自动编码器进行训练的方法，实验证明其在大瓶颈大小的情况下优于经典自动编码器。",
    "en_tdlr": "This paper proposes a method of training INN autoencoders and shows that it outperforms classical autoencoders for large bottleneck sizes, possibly due to no intrinsic information loss and no limitation on maximal number of layers."
}