{
    "title": "Does Sparsity Help in Learning Misspecified Linear Bandits?. (arXiv:2303.16998v1 [cs.LG])",
    "abstract": "Recently, the study of linear misspecified bandits has generated intriguing implications of the hardness of learning in bandits and reinforcement learning (RL). In particular, Du et al. (2020) show that even if a learner is given linear features in $\\mathbb{R}^d$ that approximate the rewards in a bandit or RL with a uniform error of $\\varepsilon$, searching for an $O(\\varepsilon)$-optimal action requires pulling at least $\\Omega(\\exp(d))$ queries. Furthermore, Lattimore et al. (2020) show that a degraded $O(\\varepsilon\\sqrt{d})$-optimal solution can be learned within $\\operatorname{poly}(d/\\varepsilon)$ queries. Yet it is unknown whether a structural assumption on the ground-truth parameter, such as sparsity, could break the $\\varepsilon\\sqrt{d}$ barrier. In this paper, we address this question by showing that algorithms can obtain $O(\\varepsilon)$-optimal actions by querying $O(\\varepsilon^{-s}d^s)$ actions, where $s$ is the sparsity parameter, removing the $\\exp(d)$-dependence. We th",
    "link": "http://arxiv.org/abs/2303.16998",
    "context": "Title: Does Sparsity Help in Learning Misspecified Linear Bandits?. (arXiv:2303.16998v1 [cs.LG])\nAbstract: Recently, the study of linear misspecified bandits has generated intriguing implications of the hardness of learning in bandits and reinforcement learning (RL). In particular, Du et al. (2020) show that even if a learner is given linear features in $\\mathbb{R}^d$ that approximate the rewards in a bandit or RL with a uniform error of $\\varepsilon$, searching for an $O(\\varepsilon)$-optimal action requires pulling at least $\\Omega(\\exp(d))$ queries. Furthermore, Lattimore et al. (2020) show that a degraded $O(\\varepsilon\\sqrt{d})$-optimal solution can be learned within $\\operatorname{poly}(d/\\varepsilon)$ queries. Yet it is unknown whether a structural assumption on the ground-truth parameter, such as sparsity, could break the $\\varepsilon\\sqrt{d}$ barrier. In this paper, we address this question by showing that algorithms can obtain $O(\\varepsilon)$-optimal actions by querying $O(\\varepsilon^{-s}d^s)$ actions, where $s$ is the sparsity parameter, removing the $\\exp(d)$-dependence. We th",
    "path": "papers/23/03/2303.16998.json",
    "total_tokens": 1042,
    "translated_title": "稀疏性是否有助于学习不正确的线性赌博机？",
    "translated_abstract": "最近，学习线性不正确赌博机已经产生了对学习赌博机和强化学习（RL）的难度的有趣影响。具体而言，Du等人（2020）表明，即使学习者被赋予在$ \\mathbb{R}^d$ 中近似赌博机或RL奖励的线性特征，且误差在$\\varepsilon$的范围内，寻找一个$ O（\\varepsilon）$ -最优行动需要至少拉出$ \\Omega(\\exp(d)) $的查询。此外，Lattimore等人（2020）展示了如何在$\\operatorname{poly}(d/\\varepsilon)$的查询中学习得到退化的$O(\\varepsilon\\sqrt{d})$ -最优解决方案。然而，关于实际参数的结构假设，如稀疏性，是否能打破$\\varepsilon\\sqrt{d}$的障碍仍不清楚。本文通过展示算法可以通过查询$ O(\\varepsilon^{-s}d^s) $个操作来获得$O(\\varepsilon)$-最优行动，其中$s$是稀疏性参数，以消除$ \\exp(d)$-依赖性解决了这个问题。",
    "tldr": "本文研究了稀疏性在解决不正确线性赌博机问题中的作用，证明了算法可以通过查询$ O(\\varepsilon^{-s}d^s) $个操作来获得$O(\\varepsilon)$-最优行动，其中$s$是稀疏性参数。",
    "en_tdlr": "This paper investigates the role of sparsity in solving misspecified linear bandit problems, and shows that algorithms can achieve $O(\\varepsilon)$-optimal actions by querying $O(\\varepsilon^{-s}d^s)$ actions, where $s$ is the sparsity parameter."
}