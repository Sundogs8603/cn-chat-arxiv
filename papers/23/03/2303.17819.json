{
    "title": "An Efficient Off-Policy Reinforcement Learning Algorithm for the Continuous-Time LQR Problem. (arXiv:2303.17819v1 [eess.SY])",
    "abstract": "In this paper, an off-policy reinforcement learning algorithm is designed to solve the continuous-time LQR problem using only input-state data measured from the system. Different from other algorithms in the literature, we propose the use of a specific persistently exciting input as the exploration signal during the data collection step. We then show that, using this persistently excited data, the solution of the matrix equation in our algorithm is guaranteed to exist and to be unique at every iteration. Convergence of the algorithm to the optimal control input is also proven. Moreover, we formulate the policy evaluation step as the solution of a Sylvester-transpose equation, which increases the efficiency of its solution. Finally, a method to determine a stabilizing policy to initialize the algorithm using only measured data is proposed.",
    "link": "http://arxiv.org/abs/2303.17819",
    "context": "Title: An Efficient Off-Policy Reinforcement Learning Algorithm for the Continuous-Time LQR Problem. (arXiv:2303.17819v1 [eess.SY])\nAbstract: In this paper, an off-policy reinforcement learning algorithm is designed to solve the continuous-time LQR problem using only input-state data measured from the system. Different from other algorithms in the literature, we propose the use of a specific persistently exciting input as the exploration signal during the data collection step. We then show that, using this persistently excited data, the solution of the matrix equation in our algorithm is guaranteed to exist and to be unique at every iteration. Convergence of the algorithm to the optimal control input is also proven. Moreover, we formulate the policy evaluation step as the solution of a Sylvester-transpose equation, which increases the efficiency of its solution. Finally, a method to determine a stabilizing policy to initialize the algorithm using only measured data is proposed.",
    "path": "papers/23/03/2303.17819.json",
    "total_tokens": 848,
    "translated_title": "一种用于连续时间LQR问题的高效离策略强化学习算法",
    "translated_abstract": "本文设计了一种离策略强化学习算法，仅利用从系统测量到的输入状态数据来解决连续时间LQR问题。不同于文献中的其他算法，我们提出在数据收集步骤中使用特定的持续激励输入作为探索信号。我们随后展示，利用这种持续激励的数据，我们算法中矩阵方程的解保证每次迭代都存在且唯一。同时，算法收敛到最优控制输入也得到了证明。此外，我们将策略评估步骤制定为一个Sylvester转置方程的解，从而提高了其求解的效率。最后，我们提出了一种仅使用测量数据来确定稳定策略以初始化算法的方法。",
    "tldr": "本文提出了一种高效的离策略强化学习算法，使用特定的持续激励输入作为探索信号来解决连续时间LQR问题，并证明了算法在每次迭代中可收敛到最优控制输入，同时提高了求解的效率。",
    "en_tdlr": "This paper proposes an efficient off-policy reinforcement learning algorithm which uses a specific persistently exciting input as the exploration signal to solve the continuous-time LQR problem. The algorithm converges to the optimal control input and increases the efficiency of its solution by formulating the policy evaluation step as the solution of a Sylvester-transpose equation."
}