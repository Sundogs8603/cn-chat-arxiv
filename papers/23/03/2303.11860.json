{
    "title": "Online Transformers with Spiking Neurons for Fast Prosthetic Hand Control. (arXiv:2303.11860v1 [cs.NE])",
    "abstract": "Transformers are state-of-the-art networks for most sequence processing tasks. However, the self-attention mechanism often used in Transformers requires large time windows for each computation step and thus makes them less suitable for online signal processing compared to Recurrent Neural Networks (RNNs). In this paper, instead of the self-attention mechanism, we use a sliding window attention mechanism. We show that this mechanism is more efficient for continuous signals with finite-range dependencies between input and target, and that we can use it to process sequences element-by-element, this making it compatible with online processing. We test our model on a finger position regression dataset (NinaproDB8) with Surface Electromyographic (sEMG) signals measured on the forearm skin to estimate muscle activities. Our approach sets the new state-of-the-art in terms of accuracy on this dataset while requiring only very short time windows of 3.5 ms at each inference step. Moreover, we inc",
    "link": "http://arxiv.org/abs/2303.11860",
    "context": "Title: Online Transformers with Spiking Neurons for Fast Prosthetic Hand Control. (arXiv:2303.11860v1 [cs.NE])\nAbstract: Transformers are state-of-the-art networks for most sequence processing tasks. However, the self-attention mechanism often used in Transformers requires large time windows for each computation step and thus makes them less suitable for online signal processing compared to Recurrent Neural Networks (RNNs). In this paper, instead of the self-attention mechanism, we use a sliding window attention mechanism. We show that this mechanism is more efficient for continuous signals with finite-range dependencies between input and target, and that we can use it to process sequences element-by-element, this making it compatible with online processing. We test our model on a finger position regression dataset (NinaproDB8) with Surface Electromyographic (sEMG) signals measured on the forearm skin to estimate muscle activities. Our approach sets the new state-of-the-art in terms of accuracy on this dataset while requiring only very short time windows of 3.5 ms at each inference step. Moreover, we inc",
    "path": "papers/23/03/2303.11860.json",
    "total_tokens": 944,
    "translated_title": "使用脉冲神经元的在线Transformer用于快速假肢手控制",
    "translated_abstract": "Transformer网络是大多数序列处理任务的最先进网络。然而，Transformer中经常使用的自注意机制需要大的时间窗口来进行每个计算步骤，因此与递归神经网络(RNN)相比，使得它们不太适用于在线信号处理。在本文中，我们使用滑动窗口注意机制来代替自注意机制。我们展示了这种机制对于在输入和目标之间存在有限范围依赖的连续信号更为高效，并且可以用它来逐个元素地处理序列，因此使其适用于在线处理。我们在一个指尖位置回归数据集(NinaproDB8)上测试了模型，该数据集使用在前臂皮肤上测量的Surface Electromyographic (sEMG)信号来估计肌肉活动。我们的方法在每个推理步骤中仅需要非常短的时间窗口(3.5毫秒)就能在这个数据集上取得最新的准确性纪录。",
    "tldr": "该论文采用了滑动窗口注意机制来替代Transformer中的自注意机制，从而使得使用逐个元素地处理序列，更加适用于在线信号处理，并且在手指位置回归数据集上实现了最好的准确性纪录，每个推理步骤仅需要非常短的时间窗口(3.5毫秒)。",
    "en_tdlr": "This paper adopts sliding window attention mechanism to replace the self-attention mechanism in Transformers, which makes it more suitable for online signal processing by processing sequences element-by-element, and achieves the best accuracy record on finger position regression dataset with short time windows of 3.5 ms per inference step."
}