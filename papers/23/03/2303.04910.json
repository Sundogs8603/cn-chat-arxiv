{
    "title": "Baldur: Whole-Proof Generation and Repair with Large Language Models. (arXiv:2303.04910v2 [cs.LG] UPDATED)",
    "abstract": "Formally verifying software properties is a highly desirable but labor-intensive task. Recent work has developed methods to automate formal verification using proof assistants, such as Coq and Isabelle/HOL, e.g., by training a model to predict one proof step at a time, and using that model to search through the space of possible proofs. This paper introduces a new method to automate formal verification: We use large language models, trained on natural language text and code and fine-tuned on proofs, to generate whole proofs for theorems at once, rather than one step at a time. We combine this proof generation model with a fine-tuned repair model to repair generated proofs, further increasing proving power. As its main contributions, this paper demonstrates for the first time that: (1) Whole-proof generation using transformers is possible and is as effective as search-based techniques without requiring costly search. (2) Giving the learned model additional context, such as a prior faile",
    "link": "http://arxiv.org/abs/2303.04910",
    "context": "Title: Baldur: Whole-Proof Generation and Repair with Large Language Models. (arXiv:2303.04910v2 [cs.LG] UPDATED)\nAbstract: Formally verifying software properties is a highly desirable but labor-intensive task. Recent work has developed methods to automate formal verification using proof assistants, such as Coq and Isabelle/HOL, e.g., by training a model to predict one proof step at a time, and using that model to search through the space of possible proofs. This paper introduces a new method to automate formal verification: We use large language models, trained on natural language text and code and fine-tuned on proofs, to generate whole proofs for theorems at once, rather than one step at a time. We combine this proof generation model with a fine-tuned repair model to repair generated proofs, further increasing proving power. As its main contributions, this paper demonstrates for the first time that: (1) Whole-proof generation using transformers is possible and is as effective as search-based techniques without requiring costly search. (2) Giving the learned model additional context, such as a prior faile",
    "path": "papers/23/03/2303.04910.json",
    "total_tokens": 903,
    "translated_title": "Baldur：使用大语言模型进行全证明生成和修复",
    "translated_abstract": "正式验证软件属性是一项非常可取但却需要耗费大量工作的任务。最近的工作已经开发出了使用证明助手（如Coq和Isabelle/HOL）自动化形式验证的方法，例如通过训练模型一次预测一个证明步骤，并使用该模型在可能的证明空间中进行搜索。本文介绍了一种新的方法来自动化形式验证：我们使用基于自然语言文本和代码的大型语言模型，并在证明上进行微调，一次性生成整个定理的证明，而不是一步步进行。我们将此证明生成模型与经过微调的修复模型相结合，以修复生成的证明，进一步增强证明能力。作为本文的主要贡献，本文首次表明：（1）使用transformers进行全证明生成是可能的，并且与不需要昂贵搜索的基于搜索的技术一样有效。 （2）给所学模型额外的上下文，例如之前失败的证明尝试，可以提高证明的效率。",
    "tldr": "本文介绍了一种新的自动化形式验证方法，使用经过微调的大型语言模型一次性生成整个定理的证明，而不是一步步进行，从而提高证明效率。",
    "en_tdlr": "This paper introduces a new method of automating formal verification using large language models, which generates whole proofs for theorems at once, rather than one step at a time. The approach has been demonstrated to be as effective as search-based techniques but without the need for costly searches."
}