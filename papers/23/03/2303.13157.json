{
    "title": "Adiabatic replay for continual learning. (arXiv:2303.13157v1 [cs.LG])",
    "abstract": "Conventional replay-based approaches to continual learning (CL) require, for each learning phase with new data, the replay of samples representing all of the previously learned knowledge in order to avoid catastrophic forgetting. Since the amount of learned knowledge grows over time in CL problems, generative replay spends an increasing amount of time just re-learning what is already known. In this proof-of-concept study, we propose a replay-based CL strategy that we term adiabatic replay (AR), which derives its efficiency from the (reasonable) assumption that each new learning phase is adiabatic, i.e., represents only a small addition to existing knowledge. Each new learning phase triggers a sampling process that selectively replays, from the body of existing knowledge, just such samples that are similar to the new data, in contrast to replaying all of it. Complete replay is not required since AR represents the data distribution by GMMs, which are capable of selectively updating their",
    "link": "http://arxiv.org/abs/2303.13157",
    "context": "Title: Adiabatic replay for continual learning. (arXiv:2303.13157v1 [cs.LG])\nAbstract: Conventional replay-based approaches to continual learning (CL) require, for each learning phase with new data, the replay of samples representing all of the previously learned knowledge in order to avoid catastrophic forgetting. Since the amount of learned knowledge grows over time in CL problems, generative replay spends an increasing amount of time just re-learning what is already known. In this proof-of-concept study, we propose a replay-based CL strategy that we term adiabatic replay (AR), which derives its efficiency from the (reasonable) assumption that each new learning phase is adiabatic, i.e., represents only a small addition to existing knowledge. Each new learning phase triggers a sampling process that selectively replays, from the body of existing knowledge, just such samples that are similar to the new data, in contrast to replaying all of it. Complete replay is not required since AR represents the data distribution by GMMs, which are capable of selectively updating their",
    "path": "papers/23/03/2303.13157.json",
    "total_tokens": 885,
    "translated_title": "连续学习的绝热重放",
    "translated_abstract": "传统的基于重放的连续学习方法需要在每个新数据的学习阶段重放代表先前学习到的所有知识的样本，以避免灾难性遗忘。由于在连续学习问题中学到的知识量随时间增长，生成式重放会花费越来越多的时间来重新学习已知内容。在这个概念验证的研究中，我们提出了一种我们称之为绝热重放（AR）的重放连续学习策略，其效率来自于（合理的）假设每个新的学习阶段都是绝热的，即仅代表现有知识的小幅增加。每个新的学习阶段会触发一个选择性重放的采样过程，从现有知识库中选择相似于新数据的样本进行重放，而不是全部重放。完全重放不是必须的，因为AR通过GMMs表示数据分布，这些分布能够有选择性地更新它们的分布。",
    "tldr": "本研究提出了一种称为绝热重放的重放连续学习策略，它能够有选择性地重放与新数据相似的样本，从而提高学习效率。",
    "en_tdlr": "This study proposes a replay-based continual learning strategy called adiabatic replay (AR), which selectively replays samples similar to new data to improve efficiency. AR represents data distribution by GMMs to avoid complete replay."
}