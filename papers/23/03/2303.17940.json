{
    "title": "Per-Example Gradient Regularization Improves Learning Signals from Noisy Data. (arXiv:2303.17940v1 [stat.ML])",
    "abstract": "Gradient regularization, as described in \\citet{barrett2021implicit}, is a highly effective technique for promoting flat minima during gradient descent. Empirical evidence suggests that this regularization technique can significantly enhance the robustness of deep learning models against noisy perturbations, while also reducing test error. In this paper, we explore the per-example gradient regularization (PEGR) and present a theoretical analysis that demonstrates its effectiveness in improving both test error and robustness against noise perturbations. Specifically, we adopt a signal-noise data model from \\citet{cao2022benign} and show that PEGR can learn signals effectively while suppressing noise. In contrast, standard gradient descent struggles to distinguish the signal from the noise, leading to suboptimal generalization performance. Our analysis reveals that PEGR penalizes the variance of pattern learning, thus effectively suppressing the memorization of noises from the training d",
    "link": "http://arxiv.org/abs/2303.17940",
    "context": "Title: Per-Example Gradient Regularization Improves Learning Signals from Noisy Data. (arXiv:2303.17940v1 [stat.ML])\nAbstract: Gradient regularization, as described in \\citet{barrett2021implicit}, is a highly effective technique for promoting flat minima during gradient descent. Empirical evidence suggests that this regularization technique can significantly enhance the robustness of deep learning models against noisy perturbations, while also reducing test error. In this paper, we explore the per-example gradient regularization (PEGR) and present a theoretical analysis that demonstrates its effectiveness in improving both test error and robustness against noise perturbations. Specifically, we adopt a signal-noise data model from \\citet{cao2022benign} and show that PEGR can learn signals effectively while suppressing noise. In contrast, standard gradient descent struggles to distinguish the signal from the noise, leading to suboptimal generalization performance. Our analysis reveals that PEGR penalizes the variance of pattern learning, thus effectively suppressing the memorization of noises from the training d",
    "path": "papers/23/03/2303.17940.json",
    "total_tokens": 795,
    "translated_title": "每个示例的梯度正则化改进了从嘈杂数据中学习的信号",
    "translated_abstract": "本文研究了每个示例的梯度正则化 (PEGR) 技术，并通过理论分析证明了其在提高测试误差和抗噪声扰动方面的有效性。具体来说，我们采用了 \\citet {cao2022benign} 的信号噪音数据模型，并展示了 PEGR 可以有效地学习信号并抑制噪音。与传统的梯度下降方法不同，PEGR 可以区分信号和噪音，从而提高泛化性能。我们的分析揭示了 PEGR 惩罚模式学习的方差，从而有效地抑制了从训练数据中记忆噪声。",
    "tldr": "本文研究了每个示例的梯度正则化 (PEGR) 技术，证明其可以有效地学习信号并抑制噪音，从而提高测试误差和抗噪声扰动能力。",
    "en_tdlr": "This paper explores per-example gradient regularization (PEGR) technique, demonstrating its effectiveness in learning signals and suppressing noise, leading to improved test error and robustness against noise perturbations."
}