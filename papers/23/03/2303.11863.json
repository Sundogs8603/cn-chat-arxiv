{
    "title": "Continual Learning in the Presence of Spurious Correlation. (arXiv:2303.11863v1 [cs.LG])",
    "abstract": "Most continual learning (CL) algorithms have focused on tackling the stability-plasticity dilemma, that is, the challenge of preventing the forgetting of previous tasks while learning new ones. However, they have overlooked the impact of the knowledge transfer when the dataset in a certain task is biased - namely, when some unintended spurious correlations of the tasks are learned from the biased dataset. In that case, how would they affect learning future tasks or the knowledge already learned from the past tasks? In this work, we carefully design systematic experiments using one synthetic and two real-world datasets to answer the question from our empirical findings. Specifically, we first show through two-task CL experiments that standard CL methods, which are unaware of dataset bias, can transfer biases from one task to another, both forward and backward, and this transfer is exacerbated depending on whether the CL methods focus on the stability or the plasticity. We then present t",
    "link": "http://arxiv.org/abs/2303.11863",
    "context": "Title: Continual Learning in the Presence of Spurious Correlation. (arXiv:2303.11863v1 [cs.LG])\nAbstract: Most continual learning (CL) algorithms have focused on tackling the stability-plasticity dilemma, that is, the challenge of preventing the forgetting of previous tasks while learning new ones. However, they have overlooked the impact of the knowledge transfer when the dataset in a certain task is biased - namely, when some unintended spurious correlations of the tasks are learned from the biased dataset. In that case, how would they affect learning future tasks or the knowledge already learned from the past tasks? In this work, we carefully design systematic experiments using one synthetic and two real-world datasets to answer the question from our empirical findings. Specifically, we first show through two-task CL experiments that standard CL methods, which are unaware of dataset bias, can transfer biases from one task to another, both forward and backward, and this transfer is exacerbated depending on whether the CL methods focus on the stability or the plasticity. We then present t",
    "path": "papers/23/03/2303.11863.json",
    "total_tokens": 1016,
    "translated_abstract": "大多数持续学习(CL)算法都是解决稳定性-可塑性两难问题，即如何防止学习新任务时忘记以前的任务。然而，它们忽视了当某些任务的数据集被偏倚，即从偏倚的数据集中学习了一些非预期的虚假相关性时，知识传递的影响。在这种情况下，它们将如何影响未来任务的学习或已从过去任务中学到的知识呢？在这项工作中，我们使用一个合成数据集和两个真实世界的数据集来设计系统实验，以回答从实证发现中得出的问题。具体而言，我们首先通过两个任务的CL实验表明，不知道数据集偏差的标准CL方法可以将偏差从一个任务传递到另一个任务，而这种传递取决于CL方法是专注于稳定性还是可塑性，并且还会加剧这种传递。然后我们提出一种使用反重链的方法，能够有效地消除这种虚假相关性，从而达到更好地持续学习的效果。",
    "tldr": "大多数持续学习算法忽略了存在虚假相关性的数据集偏倚对持续学习的影响。本文设计了实验，表明标准CL方法会向前和向后传递偏差，取决于CL方法专注于稳定性还是可塑性。同时，提出的方法通过反重链有效地消除虚假相关性，从而实现更好的持续学习效果。",
    "en_tdlr": "Most continual learning algorithms have overlooked the impact of spurious correlation when the dataset in a certain task is biased. The results of systematic experiments showed that standard CL methods can transfer biases from one task to another, both forward and backward, depending on whether the CL methods focus on stability or plasticity. A novel method using anti-chain was proposed to effectively eliminate the spurious correlation and achieve better continual learning performance."
}