{
    "title": "BERT4ETH: A Pre-trained Transformer for Ethereum Fraud Detection. (arXiv:2303.18138v1 [cs.CR])",
    "abstract": "As various forms of fraud proliferate on Ethereum, it is imperative to safeguard against these malicious activities to protect susceptible users from being victimized. While current studies solely rely on graph-based fraud detection approaches, it is argued that they may not be well-suited for dealing with highly repetitive, skew-distributed and heterogeneous Ethereum transactions. To address these challenges, we propose BERT4ETH, a universal pre-trained Transformer encoder that serves as an account representation extractor for detecting various fraud behaviors on Ethereum. BERT4ETH features the superior modeling capability of Transformer to capture the dynamic sequential patterns inherent in Ethereum transactions, and addresses the challenges of pre-training a BERT model for Ethereum with three practical and effective strategies, namely repetitiveness reduction, skew alleviation and heterogeneity modeling. Our empirical evaluation demonstrates that BERT4ETH outperforms state-of-the-ar",
    "link": "http://arxiv.org/abs/2303.18138",
    "context": "Title: BERT4ETH: A Pre-trained Transformer for Ethereum Fraud Detection. (arXiv:2303.18138v1 [cs.CR])\nAbstract: As various forms of fraud proliferate on Ethereum, it is imperative to safeguard against these malicious activities to protect susceptible users from being victimized. While current studies solely rely on graph-based fraud detection approaches, it is argued that they may not be well-suited for dealing with highly repetitive, skew-distributed and heterogeneous Ethereum transactions. To address these challenges, we propose BERT4ETH, a universal pre-trained Transformer encoder that serves as an account representation extractor for detecting various fraud behaviors on Ethereum. BERT4ETH features the superior modeling capability of Transformer to capture the dynamic sequential patterns inherent in Ethereum transactions, and addresses the challenges of pre-training a BERT model for Ethereum with three practical and effective strategies, namely repetitiveness reduction, skew alleviation and heterogeneity modeling. Our empirical evaluation demonstrates that BERT4ETH outperforms state-of-the-ar",
    "path": "papers/23/03/2303.18138.json",
    "total_tokens": 912,
    "translated_title": "BERT4ETH：用于以太坊欺诈检测的预训练Transformer模型",
    "translated_abstract": "随着以太坊上各种欺诈行为的激增，保护易受攻击用户免受受害是非常必要的。虽然目前的研究仅依赖于基于图的欺诈检测方法，但有人认为它们可能不适合处理高度重复、偏斜分布和异构以太坊交易。为了应对这些挑战，我们提出了BERT4ETH，这是一个通用的预训练Transformer编码器，用于检测以太坊上的各种欺诈行为。BERT4ETH具有Transformer的优秀建模能力，能够捕捉以太坊交易中固有的动态顺序模式，并通过减少重复性、减轻偏斜和建模异构性等三种实用有效策略来解决为以太坊预训练BERT模型带来的挑战。我们的实证评估表明，BERT4ETH在欺诈检测方面优于现有技术。",
    "tldr": "BERT4ETH是一个用于以太坊欺诈检测的预训练Transformer编码器，它通过三种实用有效策略来解决针对高度重复、偏斜分布和异构以太坊交易这些挑战，并且比现有技术更为优秀。",
    "en_tdlr": "BERT4ETH is a pre-trained Transformer encoder for Ethereum fraud detection, which addresses the challenges of dealing with highly repetitive, skew-distributed, and heterogeneous Ethereum transactions with three effective strategies. It outperforms state-of-the-art techniques in fraud detection."
}