{
    "title": "Improved Sample Complexity for Reward-free Reinforcement Learning under Low-rank MDPs. (arXiv:2303.10859v1 [cs.LG])",
    "abstract": "In reward-free reinforcement learning (RL), an agent explores the environment first without any reward information, in order to achieve certain learning goals afterwards for any given reward. In this paper we focus on reward-free RL under low-rank MDP models, in which both the representation and linear weight vectors are unknown. Although various algorithms have been proposed for reward-free low-rank MDPs, the corresponding sample complexity is still far from being satisfactory. In this work, we first provide the first known sample complexity lower bound that holds for any algorithm under low-rank MDPs. This lower bound implies it is strictly harder to find a near-optimal policy under low-rank MDPs than under linear MDPs. We then propose a novel model-based algorithm, coined RAFFLE, and show it can both find an $\\epsilon$-optimal policy and achieve an $\\epsilon$-accurate system identification via reward-free exploration, with a sample complexity significantly improving the previous res",
    "link": "http://arxiv.org/abs/2303.10859",
    "context": "Title: Improved Sample Complexity for Reward-free Reinforcement Learning under Low-rank MDPs. (arXiv:2303.10859v1 [cs.LG])\nAbstract: In reward-free reinforcement learning (RL), an agent explores the environment first without any reward information, in order to achieve certain learning goals afterwards for any given reward. In this paper we focus on reward-free RL under low-rank MDP models, in which both the representation and linear weight vectors are unknown. Although various algorithms have been proposed for reward-free low-rank MDPs, the corresponding sample complexity is still far from being satisfactory. In this work, we first provide the first known sample complexity lower bound that holds for any algorithm under low-rank MDPs. This lower bound implies it is strictly harder to find a near-optimal policy under low-rank MDPs than under linear MDPs. We then propose a novel model-based algorithm, coined RAFFLE, and show it can both find an $\\epsilon$-optimal policy and achieve an $\\epsilon$-accurate system identification via reward-free exploration, with a sample complexity significantly improving the previous res",
    "path": "papers/23/03/2303.10859.json",
    "total_tokens": 932,
    "translated_title": "低秩MDP下无奖励强化学习样本复杂度的改进",
    "translated_abstract": "在无奖励强化学习中，智能体首先在没有奖励信息的情况下探索环境，以便在任何给定的奖励下实现某些学习目标。本文侧重于低秩MDP模型下的无奖励强化学习，其中表示和线性权向量均未知。虽然针对无奖励低秩MDP提出了各种算法，但相应的样本复杂度仍远未达到令人满意的水平。本文首先提供了低秩MDP下的首个已知的样本复杂度下界，该下界意味着在低秩MDP下找到接近最优策略比在线性MDP下更加困难。随后，我们提出一种新颖的基于模型的算法RAFFLE，并且通过无奖励探索，它能够获得$\\epsilon$-最优策略和$\\epsilon$-准确的系统识别，且其样本复杂度显著优于以前的结果。",
    "tldr": "本文提出了一种新的基于模型的算法RAFFLE，能够在低秩MDP下实现无奖励探索，并且具有显著改进的样本复杂度。",
    "en_tdlr": "This paper proposes a novel model-based algorithm, coined RAFFLE, for reward-free reinforcement learning under low-rank MDP models. RAFFLE achieves significant improvement in sample complexity and can obtain an $\\epsilon$-optimal policy and $\\epsilon$-accurate system identification via reward-free exploration."
}