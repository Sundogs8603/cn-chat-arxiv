{
    "title": "Split-Et-Impera: A Framework for the Design of Distributed Deep Learning Applications. (arXiv:2303.12524v1 [cs.DC])",
    "abstract": "Many recent pattern recognition applications rely on complex distributed architectures in which sensing and computational nodes interact together through a communication network. Deep neural networks (DNNs) play an important role in this scenario, furnishing powerful decision mechanisms, at the price of a high computational effort. Consequently, powerful state-of-the-art DNNs are frequently split over various computational nodes, e.g., a first part stays on an embedded device and the rest on a server. Deciding where to split a DNN is a challenge in itself, making the design of deep learning applications even more complicated. Therefore, we propose Split-Et-Impera, a novel and practical framework that i) determines the set of the best-split points of a neural network based on deep network interpretability principles without performing a tedious try-and-test approach, ii) performs a communication-aware simulation for the rapid evaluation of different neural network rearrangements, and ii",
    "link": "http://arxiv.org/abs/2303.12524",
    "context": "Title: Split-Et-Impera: A Framework for the Design of Distributed Deep Learning Applications. (arXiv:2303.12524v1 [cs.DC])\nAbstract: Many recent pattern recognition applications rely on complex distributed architectures in which sensing and computational nodes interact together through a communication network. Deep neural networks (DNNs) play an important role in this scenario, furnishing powerful decision mechanisms, at the price of a high computational effort. Consequently, powerful state-of-the-art DNNs are frequently split over various computational nodes, e.g., a first part stays on an embedded device and the rest on a server. Deciding where to split a DNN is a challenge in itself, making the design of deep learning applications even more complicated. Therefore, we propose Split-Et-Impera, a novel and practical framework that i) determines the set of the best-split points of a neural network based on deep network interpretability principles without performing a tedious try-and-test approach, ii) performs a communication-aware simulation for the rapid evaluation of different neural network rearrangements, and ii",
    "path": "papers/23/03/2303.12524.json",
    "total_tokens": 946,
    "translated_title": "Split-Et-Impera: 一种用于设计分布式深度学习应用程序的框架",
    "translated_abstract": "最近许多模式识别应用都依赖于复杂的分布式架构，其中感知和计算节点通过通信网络相互交互。深度神经网络在这种情况下扮演着重要角色，提供强大的决策机制，但也需要高计算量。因此，我们提出了Split-Et-Impera框架，它可以确定神经网络的最佳分割点，并通过通信感知仿真进行快速评估不同的神经网络重新排列，同时考虑目标硬件平台的特性，以适应神经网络拓扑。我们的框架可以提供一个易于使用的工具箱，设计，验证和优化分布式深度学习应用程序，而不需要对分布式系统或网络通信协议有深入的了解。",
    "tldr": "Split-Et-Impera是一种针对分布式深度学习应用程序的框架，可根据神经网络可解释性原则确定最佳的网络分割点、通过通信感知仿真进行快速评估不同的神经网络重新排列，并根据目标硬件平台的特性适应神经网络拓扑。"
}