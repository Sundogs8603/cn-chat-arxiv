{
    "title": "One Adapter for All Programming Languages? Adapter Tuning for Code Search and Summarization. (arXiv:2303.15822v1 [cs.SE])",
    "abstract": "As pre-trained models automate many code intelligence tasks, a widely used paradigm is to fine-tune a model on the task dataset for each programming language. A recent study reported that multilingual fine-tuning benefits a range of tasks and models. However, we find that multilingual fine-tuning leads to performance degradation on recent models UniXcoder and CodeT5.  To alleviate the potentially catastrophic forgetting issue in multilingual models, we fix all pre-trained model parameters, insert the parameter-efficient structure adapter, and fine-tune it. Updating only 0.6\\% of the overall parameters compared to full-model fine-tuning for each programming language, adapter tuning yields consistent improvements on code search and summarization tasks, achieving state-of-the-art results. In addition, we experimentally show its effectiveness in cross-lingual and low-resource scenarios. Multilingual fine-tuning with 200 samples per programming language approaches the results fine-tuned wit",
    "link": "http://arxiv.org/abs/2303.15822",
    "context": "Title: One Adapter for All Programming Languages? Adapter Tuning for Code Search and Summarization. (arXiv:2303.15822v1 [cs.SE])\nAbstract: As pre-trained models automate many code intelligence tasks, a widely used paradigm is to fine-tune a model on the task dataset for each programming language. A recent study reported that multilingual fine-tuning benefits a range of tasks and models. However, we find that multilingual fine-tuning leads to performance degradation on recent models UniXcoder and CodeT5.  To alleviate the potentially catastrophic forgetting issue in multilingual models, we fix all pre-trained model parameters, insert the parameter-efficient structure adapter, and fine-tune it. Updating only 0.6\\% of the overall parameters compared to full-model fine-tuning for each programming language, adapter tuning yields consistent improvements on code search and summarization tasks, achieving state-of-the-art results. In addition, we experimentally show its effectiveness in cross-lingual and low-resource scenarios. Multilingual fine-tuning with 200 samples per programming language approaches the results fine-tuned wit",
    "path": "papers/23/03/2303.15822.json",
    "total_tokens": 974,
    "translated_title": "一种适用于所有编程语言的适配器吗？用于代码搜索和摘要的适配器调整",
    "translated_abstract": "随着预训练模型对许多代码智能任务的自动化，一种广泛使用的范式是在每种编程语言的任务数据集上微调模型。最近的一项研究报告称，多语言微调有益于各种任务和模型。然而，我们发现多语言微调会导致UniXcoder和CodeT5最近模型的性能下降。为了缓解多语言模型中可能发生的灾难性遗忘问题，我们固定所有预训练模型参数，插入参数高效结构适配器，并对其进行微调。与为每种编程语言进行完全模型微调相比，适配器微调仅更新了整体参数的0.6％，在代码搜索和摘要任务上产生了一致的改进，取得了最先进的结果。此外，我们通过实验展示了适配器在跨语言和低资源场景中的有效性。每种编程语言使用200个样本进行的多语言微调接近于微调结果。",
    "tldr": "本文探讨在适配器微调中固定所有预训练模型参数并插入参数高效结构适配器的方法，旨在缓解多语言模型中遗忘问题。在代码搜索和摘要任务中实现了一致的改进并取得了最先进的结果，同时还展示了适配器在跨语言和低资源场景中的有效性。"
}