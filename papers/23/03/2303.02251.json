{
    "title": "Certified Robust Neural Networks: Generalization and Corruption Resistance. (arXiv:2303.02251v2 [stat.ML] UPDATED)",
    "abstract": "Recent work have demonstrated that robustness (to \"corruption\") can be at odds with generalization. Adversarial training, for instance, aims to reduce the problematic susceptibility of modern neural networks to small data perturbations. Surprisingly, overfitting is a major concern in adversarial training despite being mostly absent in standard training. We provide here theoretical evidence for this peculiar \"robust overfitting\" phenomenon. Subsequently, we advance a novel distributionally robust loss function bridging robustness and generalization. We demonstrate both theoretically as well as empirically the loss to enjoy a certified level of robustness against two common types of corruption--data evasion and poisoning attacks--while ensuring guaranteed generalization. We show through careful numerical experiments that our resulting holistic robust (HR) training procedure yields SOTA performance. Finally, we indicate that HR training can be interpreted as a direct extension of adversar",
    "link": "http://arxiv.org/abs/2303.02251",
    "context": "Title: Certified Robust Neural Networks: Generalization and Corruption Resistance. (arXiv:2303.02251v2 [stat.ML] UPDATED)\nAbstract: Recent work have demonstrated that robustness (to \"corruption\") can be at odds with generalization. Adversarial training, for instance, aims to reduce the problematic susceptibility of modern neural networks to small data perturbations. Surprisingly, overfitting is a major concern in adversarial training despite being mostly absent in standard training. We provide here theoretical evidence for this peculiar \"robust overfitting\" phenomenon. Subsequently, we advance a novel distributionally robust loss function bridging robustness and generalization. We demonstrate both theoretically as well as empirically the loss to enjoy a certified level of robustness against two common types of corruption--data evasion and poisoning attacks--while ensuring guaranteed generalization. We show through careful numerical experiments that our resulting holistic robust (HR) training procedure yields SOTA performance. Finally, we indicate that HR training can be interpreted as a direct extension of adversar",
    "path": "papers/23/03/2303.02251.json",
    "total_tokens": 984,
    "translated_title": "论文标题：认证鲁棒神经网络：泛化和抗污染性",
    "translated_abstract": "最近的研究表明，鲁棒性（对“污染”的抵抗能力）可能与泛化存在矛盾。例如，对抗性训练旨在减少现代神经网络对小数据扰动的敏感性。令人惊讶的是，在对抗训练中，过拟合是一个主要问题，尽管在标准训练中几乎不存在。我们在这里提供了关于这种奇特的“鲁棒过拟合”现象的理论证据。随后，我们提出了一种新颖的分布鲁棒损失函数，将鲁棒性和泛化相结合。我们理论上和实证地证明了该损失具有认证级别的鲁棒性，可以抵抗两种常见的污染类型——数据逃避和攻击——同时确保泛化保证。通过精心设计的数字实验，我们展示了所得到的完整鲁棒（HR）训练程序具有SOTA的性能。最后，我们指出HR训练可以被解释为对抗性训练的直接扩展，并可以自然地应用于GAN和RL。",
    "tldr": "该论文提出了一种新颖的分布鲁棒损失函数，该函数通过认证级别的鲁棒性对两种常见的污染类型进行抵抗，并确保泛化保证，从而解决了鲁棒性和泛化之间的矛盾，具有极高的实用性。",
    "en_tdlr": "This paper proposes a novel distributionally robust loss function that bridges robustness and generalization, providing certified level of robustness against common types of corruption while ensuring guaranteed generalization. It solves the conflict between robustness and generalization and has high practicality."
}