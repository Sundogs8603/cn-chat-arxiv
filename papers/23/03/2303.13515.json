{
    "title": "Persistent Nature: A Generative Model of Unbounded 3D Worlds. (arXiv:2303.13515v1 [cs.CV])",
    "abstract": "Despite increasingly realistic image quality, recent 3D image generative models often operate on 3D volumes of fixed extent with limited camera motions. We investigate the task of unconditionally synthesizing unbounded nature scenes, enabling arbitrarily large camera motion while maintaining a persistent 3D world model. Our scene representation consists of an extendable, planar scene layout grid, which can be rendered from arbitrary camera poses via a 3D decoder and volume rendering, and a panoramic skydome. Based on this representation, we learn a generative world model solely from single-view internet photos. Our method enables simulating long flights through 3D landscapes, while maintaining global scene consistency--for instance, returning to the starting point yields the same view of the scene. Our approach enables scene extrapolation beyond the fixed bounds of current 3D generative models, while also supporting a persistent, camera-independent world representation that stands in c",
    "link": "http://arxiv.org/abs/2303.13515",
    "context": "Title: Persistent Nature: A Generative Model of Unbounded 3D Worlds. (arXiv:2303.13515v1 [cs.CV])\nAbstract: Despite increasingly realistic image quality, recent 3D image generative models often operate on 3D volumes of fixed extent with limited camera motions. We investigate the task of unconditionally synthesizing unbounded nature scenes, enabling arbitrarily large camera motion while maintaining a persistent 3D world model. Our scene representation consists of an extendable, planar scene layout grid, which can be rendered from arbitrary camera poses via a 3D decoder and volume rendering, and a panoramic skydome. Based on this representation, we learn a generative world model solely from single-view internet photos. Our method enables simulating long flights through 3D landscapes, while maintaining global scene consistency--for instance, returning to the starting point yields the same view of the scene. Our approach enables scene extrapolation beyond the fixed bounds of current 3D generative models, while also supporting a persistent, camera-independent world representation that stands in c",
    "path": "papers/23/03/2303.13515.json",
    "total_tokens": 1077,
    "translated_title": "持续的自然：一个生成无界3D世界的模型",
    "translated_abstract": "近年来的3D图像生成模型，尽管图像质量越来越逼真，但其操作的3D体积通常是固定的，且摄像机运动受限。我们探索了无条件合成无界自然场景的任务，使得在保持持久的3D世界模型的同时可以进行任意大的摄像机运动。我们的场景表示包括可扩展的平面场景布局网格，可以通过3D解码器和体积渲染从任意摄像机姿态进行渲染，以及全景天空穹顶。基于这种表示，我们仅从单视图互联网照片中学习生成世界模型。我们的方法可以模拟在3D景观中长时间的飞行，同时保持全局场景的一致性——例如，返回起点会产生相同的场景视图。我们的方法使得场景推断超越了当前3D生成模型的固定范围，同时支持一个持续的、摄像机独立的世界表示，而这种表示在计算效率上是非常高效的。",
    "tldr": "本文介绍了一个生成无界3D世界的模型，探索了无条件合成无界自然场景的任务，使得在保持持久的3D世界模型的同时可以进行任意大的摄像机运动。其场景表示包括可扩展的平面场景布局网格以及全景天空穹顶，并仅从单视图互联网照片中学习生成世界模型。该模型可模拟在3D景观中长时间的飞行，同时支持一个持续的、摄像机独立的世界表示，具有高效的计算性能。",
    "en_tdlr": "This paper presents a generative model of unbounded 3D worlds, exploring the task of unconditionally synthesizing unbounded nature scenes, enabling arbitrarily large camera motion while maintaining a persistent 3D world model. The scene representation consists of an extendable, planar scene layout grid and a panoramic skydome, and the model is learned solely from single-view internet photos. This model enables simulating long flights through 3D landscapes while maintaining global scene consistency, and supports a persistent, camera-independent world representation with efficient computational performance."
}