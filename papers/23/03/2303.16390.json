{
    "title": "Are Data-driven Explanations Robust against Out-of-distribution Data?. (arXiv:2303.16390v1 [cs.LG])",
    "abstract": "As black-box models increasingly power high-stakes applications, a variety of data-driven explanation methods have been introduced. Meanwhile, machine learning models are constantly challenged by distributional shifts. A question naturally arises: Are data-driven explanations robust against out-of-distribution data? Our empirical results show that even though predict correctly, the model might still yield unreliable explanations under distributional shifts. How to develop robust explanations against out-of-distribution data? To address this problem, we propose an end-to-end model-agnostic learning framework Distributionally Robust Explanations (DRE). The key idea is, inspired by self-supervised learning, to fully utilizes the inter-distribution information to provide supervisory signals for the learning of explanations without human annotation. Can robust explanations benefit the model's generalization capability? We conduct extensive experiments on a wide range of tasks and data types",
    "link": "http://arxiv.org/abs/2303.16390",
    "context": "Title: Are Data-driven Explanations Robust against Out-of-distribution Data?. (arXiv:2303.16390v1 [cs.LG])\nAbstract: As black-box models increasingly power high-stakes applications, a variety of data-driven explanation methods have been introduced. Meanwhile, machine learning models are constantly challenged by distributional shifts. A question naturally arises: Are data-driven explanations robust against out-of-distribution data? Our empirical results show that even though predict correctly, the model might still yield unreliable explanations under distributional shifts. How to develop robust explanations against out-of-distribution data? To address this problem, we propose an end-to-end model-agnostic learning framework Distributionally Robust Explanations (DRE). The key idea is, inspired by self-supervised learning, to fully utilizes the inter-distribution information to provide supervisory signals for the learning of explanations without human annotation. Can robust explanations benefit the model's generalization capability? We conduct extensive experiments on a wide range of tasks and data types",
    "path": "papers/23/03/2303.16390.json",
    "total_tokens": 1000,
    "translated_title": "数据驱动的解释是否对越界数据具有稳健性？",
    "translated_abstract": "随着黑盒模型越来越多地应用于高风险应用，引入了各种数据驱动的解释方法。与此同时，机器学习模型不断受到分布转移的挑战。自然而然地出现了一个问题：数据驱动的解释是否对越界数据具有稳健性？我们的实证结果表明，即使模型预测正确，也可能在分布转移下产生不可靠的解释。如何开发针对越界数据的强健解释？为了解决这个问题，我们提出了一个端到端的模型无关学习框架：分布式稳健解释(DRE)。关键思路是受到自监督学习的启发，充分利用相互分布的信息，为解释的学习提供监督信号，无需人类注释。强健的解释能否提高模型的泛化能力？我们在广泛的任务和数据类型上进行了大量实验。",
    "tldr": "随着黑盒模型在高风险应用中越来越被广泛应用，各种数据驱动的解释方法应运而生，但是，实证结果表明，即使模型预测正确，也可能在分布转移下产生不可靠的解释。为了解决这个问题，我们提出了一个模型无关的解释学习框架：分布稳健解释(DRE)，它可以在越界数据的情况下提供强健的解释信号，并提高模型的泛化能力。",
    "en_tdlr": "As black-box models become increasingly common in high-risk applications, various data-driven explanation methods have been introduced. However, empirical results show that even when the model's predictions are correct, unreliable explanations may still result from distributional shifts. To address this problem, we propose a model-agnostic learning framework called Distributionally Robust Explanations (DRE), which provides robust explanation signals even for out-of-distribution data, potentially improving the model's generalization ability."
}