{
    "title": "Data-dependent Generalization Bounds via Variable-Size Compressibility. (arXiv:2303.05369v2 [stat.ML] UPDATED)",
    "abstract": "In this paper, we establish novel data-dependent upper bounds on the generalization error through the lens of a \"variable-size compressibility\" framework that we introduce newly here. In this framework, the generalization error of an algorithm is linked to a variable-size 'compression rate' of its input data. This is shown to yield bounds that depend on the empirical measure of the given input data at hand, rather than its unknown distribution. Our new generalization bounds that we establish are tail bounds, tail bounds on the expectation, and in-expectations bounds. Moreover, it is shown that our framework also allows to derive general bounds on any function of the input data and output hypothesis random variables. In particular, these general bounds are shown to subsume and possibly improve over several existing PAC-Bayes and data-dependent intrinsic dimension-based bounds that are recovered as special cases, thus unveiling a unifying character of our approach. For instance, a new da",
    "link": "http://arxiv.org/abs/2303.05369",
    "context": "Title: Data-dependent Generalization Bounds via Variable-Size Compressibility. (arXiv:2303.05369v2 [stat.ML] UPDATED)\nAbstract: In this paper, we establish novel data-dependent upper bounds on the generalization error through the lens of a \"variable-size compressibility\" framework that we introduce newly here. In this framework, the generalization error of an algorithm is linked to a variable-size 'compression rate' of its input data. This is shown to yield bounds that depend on the empirical measure of the given input data at hand, rather than its unknown distribution. Our new generalization bounds that we establish are tail bounds, tail bounds on the expectation, and in-expectations bounds. Moreover, it is shown that our framework also allows to derive general bounds on any function of the input data and output hypothesis random variables. In particular, these general bounds are shown to subsume and possibly improve over several existing PAC-Bayes and data-dependent intrinsic dimension-based bounds that are recovered as special cases, thus unveiling a unifying character of our approach. For instance, a new da",
    "path": "papers/23/03/2303.05369.json",
    "total_tokens": 978,
    "translated_title": "通过可变大小的压缩性建立数据相关的泛化界限",
    "translated_abstract": "本文通过引入“可变大小压缩性”框架，建立了一种新的数据相关泛化误差的上界。在这个框架中，算法的泛化误差与其输入数据的可变大小“压缩率”相关联。通过这种方式，我们得到的界限依赖于手头给定输入数据的经验分布，而不是其未知分布。我们建立的新的泛化界限包括尾部界限、期望值的尾部界限和期望界限。此外，我们的框架还可以推导出对输入数据和输出假设随机变量的任何函数的泛化界限。特别是，这些泛化界限包含并可能优于几种现有的基于PAC-Bayes和数据相关内在维度的界限，这些界限作为特殊情况得到复原，从而揭示出我们方法的统一特性。",
    "tldr": "本文通过引入可变大小压缩性框架，建立了一种新的数据相关的泛化误差上界。该方法将算法的泛化误差与其输入数据的可变大小压缩率相关联，并提供了依赖于经验分布而非未知分布的界限。此外，该方法还可以推导出输入数据和输出假设随机变量的任何函数的泛化界限，并包含并可能优于现有的基于PAC-Bayes和数据相关内在维度的界限。",
    "en_tdlr": "This paper establishes novel data-dependent upper bounds on the generalization error using a variable-size compressibility framework. The framework links the generalization error of an algorithm to the compression rate of its input data, allowing for bounds that rely on the empirical measure rather than the unknown distribution. The proposed approach also extends to deriving general bounds on any function of the input data and output hypothesis random variables, improving upon existing results based on PAC-Bayes and data-dependent intrinsic dimension bounds."
}