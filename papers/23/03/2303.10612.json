{
    "title": "Bangla Grammatical Error Detection Using T5 Transformer Model. (arXiv:2303.10612v1 [cs.CL])",
    "abstract": "This paper presents a method for detecting grammatical errors in Bangla using a Text-to-Text Transfer Transformer (T5) Language Model, using the small variant of BanglaT5, fine-tuned on a corpus of 9385 sentences where errors were bracketed by the dedicated demarcation symbol. The T5 model was primarily designed for translation and is not specifically designed for this task, so extensive post-processing was necessary to adapt it to the task of error detection. Our experiments show that the T5 model can achieve low Levenshtein Distance in detecting grammatical errors in Bangla, but post-processing is essential to achieve optimal performance. The final average Levenshtein Distance after post-processing the output of the fine-tuned model was 1.0394 on a test set of 5000 sentences. This paper also presents a detailed analysis of the errors detected by the model and discusses the challenges of adapting a translation model for grammar. Our approach can be extended to other languages, demonst",
    "link": "http://arxiv.org/abs/2303.10612",
    "context": "Title: Bangla Grammatical Error Detection Using T5 Transformer Model. (arXiv:2303.10612v1 [cs.CL])\nAbstract: This paper presents a method for detecting grammatical errors in Bangla using a Text-to-Text Transfer Transformer (T5) Language Model, using the small variant of BanglaT5, fine-tuned on a corpus of 9385 sentences where errors were bracketed by the dedicated demarcation symbol. The T5 model was primarily designed for translation and is not specifically designed for this task, so extensive post-processing was necessary to adapt it to the task of error detection. Our experiments show that the T5 model can achieve low Levenshtein Distance in detecting grammatical errors in Bangla, but post-processing is essential to achieve optimal performance. The final average Levenshtein Distance after post-processing the output of the fine-tuned model was 1.0394 on a test set of 5000 sentences. This paper also presents a detailed analysis of the errors detected by the model and discusses the challenges of adapting a translation model for grammar. Our approach can be extended to other languages, demonst",
    "path": "papers/23/03/2303.10612.json",
    "total_tokens": 977,
    "translated_title": "使用T5 Transformer模型的孟加拉语语法错误检测",
    "translated_abstract": "本文提出了一种使用基于文本的转换变压器（T5）语言模型检测孟加拉语语法错误的方法，使用精细调整过的BanglaT5的小变种，在一个包含9385个句子的语料库上进行训练，其中错误被专用分界符括起来。T5模型主要设计用于翻译，而不是特别为这个任务设计的，因此需要进行广泛的后处理来使其适应错误检测任务。我们的实验表明，T5模型在检测孟加拉语语法错误时可以实现较低的Levenshtein距离，但是必须进行后处理才能达到最佳性能。在对精细调整的模型输出进行后处理后，最终测试集的平均Levenshtein距离为1.0394。本文还对模型检测到的错误进行了详细分析，并讨论了将翻译模型适应语法检测任务的挑战。我们的方法可以扩展到其他语言，这在成功检测孟加拉语语法错误时得到了证明。",
    "tldr": "本文使用T5 Transformer模型成功地在孟加拉语中检测语法错误，并对模型检测到的错误进行了详细分析，同时探讨了将翻译模型适应语法检测任务的挑战。",
    "en_tdlr": "This paper presents a successful method of detecting grammatical errors in Bangla using T5 Transformer model, which was primarily designed for translation. The approach involved extensive post-processing to achieve optimal performance. The paper provides detailed analysis of errors detected by the model and discusses the challenges of adapting a translation model for grammar. The method can be extended to other languages, as demonstrated by successful detection of grammatical errors in Bangla."
}