{
    "title": "Encoding Binary Concepts in the Latent Space of Generative Models for Enhancing Data Representation. (arXiv:2303.12255v1 [cs.LG])",
    "abstract": "Binary concepts are empirically used by humans to generalize efficiently. And they are based on Bernoulli distribution which is the building block of information. These concepts span both low-level and high-level features such as \"large vs small\" and \"a neuron is active or inactive\". Binary concepts are ubiquitous features and can be used to transfer knowledge to improve model generalization. We propose a novel binarized regularization to facilitate learning of binary concepts to improve the quality of data generation in autoencoders. We introduce a binarizing hyperparameter $r$ in data generation process to disentangle the latent space symmetrically. We demonstrate that this method can be applied easily to existing variational autoencoder (VAE) variants to encourage symmetric disentanglement, improve reconstruction quality, and prevent posterior collapse without computation overhead. We also demonstrate that this method can boost existing models to learn more transferable representati",
    "link": "http://arxiv.org/abs/2303.12255",
    "context": "Title: Encoding Binary Concepts in the Latent Space of Generative Models for Enhancing Data Representation. (arXiv:2303.12255v1 [cs.LG])\nAbstract: Binary concepts are empirically used by humans to generalize efficiently. And they are based on Bernoulli distribution which is the building block of information. These concepts span both low-level and high-level features such as \"large vs small\" and \"a neuron is active or inactive\". Binary concepts are ubiquitous features and can be used to transfer knowledge to improve model generalization. We propose a novel binarized regularization to facilitate learning of binary concepts to improve the quality of data generation in autoencoders. We introduce a binarizing hyperparameter $r$ in data generation process to disentangle the latent space symmetrically. We demonstrate that this method can be applied easily to existing variational autoencoder (VAE) variants to encourage symmetric disentanglement, improve reconstruction quality, and prevent posterior collapse without computation overhead. We also demonstrate that this method can boost existing models to learn more transferable representati",
    "path": "papers/23/03/2303.12255.json",
    "total_tokens": 875,
    "translated_title": "在生成模型的潜空间中编码二元概念以增强数据表示",
    "translated_abstract": "人类经验主义地使用二元概念来高效地推广。这些概念基于伯努利分布，是信息的基本组成部分。这些概念涵盖了低级和高级特征，如“大 vs 小”和“神经元处于活跃或非活跃状态”。二元概念是无处不在的特征，可以用于传递知识，以改进模型的泛化性能。我们提出了一种新的二元正则化方法，以便于学习二元概念，从而提高自编码器生成数据的质量。我们在数据生成过程中引入了二元化超参数$r$，以对称地解开潜在空间。我们证明了这种方法可以轻松应用于现有变分自编码器（VAE）变体，以鼓励对称解缠、提高重建质量并防止后验崩溃而无需计算开销。我们还证明了这种方法可以提高现有模型的泛化性能和学习可传递性表示。",
    "tldr": "本文提出了一种新的二元正则化方法以便于学习二元概念，从而提高自编码器生成数据的质量并改进模型的泛化性能。",
    "en_tdlr": "This paper proposes a novel binarized regularization method to facilitate learning of binary concepts, improving the quality of data generation in autoencoders and enhancing the generalization performance of the model."
}