{
    "title": "Masked Images Are Counterfactual Samples for Robust Fine-tuning. (arXiv:2303.03052v2 [cs.CV] UPDATED)",
    "abstract": "Deep learning models are challenged by the distribution shift between the training data and test data. Recently, the large models pre-trained on diverse data demonstrate unprecedented robustness to various distribution shifts. However, fine-tuning on these models can lead to a trade-off between in-distribution (ID) performance and out-of-distribution (OOD) robustness. Existing methods for tackling this trade-off do not explicitly address the OOD robustness problem. In this paper, based on causal analysis on the aforementioned problems, we propose a novel fine-tuning method, which use masked images as counterfactual samples that help improving the robustness of the fine-tuning model. Specifically, we mask either the semantics-related or semantics-unrelated patches of the images based on class activation map to break the spurious correlation, and refill the masked patches with patches from other images. The resulting counterfactual samples are used in feature-based distillation with the ",
    "link": "http://arxiv.org/abs/2303.03052",
    "context": "Title: Masked Images Are Counterfactual Samples for Robust Fine-tuning. (arXiv:2303.03052v2 [cs.CV] UPDATED)\nAbstract: Deep learning models are challenged by the distribution shift between the training data and test data. Recently, the large models pre-trained on diverse data demonstrate unprecedented robustness to various distribution shifts. However, fine-tuning on these models can lead to a trade-off between in-distribution (ID) performance and out-of-distribution (OOD) robustness. Existing methods for tackling this trade-off do not explicitly address the OOD robustness problem. In this paper, based on causal analysis on the aforementioned problems, we propose a novel fine-tuning method, which use masked images as counterfactual samples that help improving the robustness of the fine-tuning model. Specifically, we mask either the semantics-related or semantics-unrelated patches of the images based on class activation map to break the spurious correlation, and refill the masked patches with patches from other images. The resulting counterfactual samples are used in feature-based distillation with the ",
    "path": "papers/23/03/2303.03052.json",
    "total_tokens": 853,
    "translated_title": "掩蔽图像是鲁棒微调的反事实样本",
    "translated_abstract": "深度学习模型由于训练数据和测试数据之间的分布差异而受到挑战。最近，基于多样化数据预训练的大型模型展现了空前的鲁棒性来应对各种分布差异。然而，在这些模型上进行微调可能会导致在分布内性能和分布外鲁棒性之间的权衡。现有的方法并没有明确处理分布外鲁棒性问题。在本文中，我们基于对上述问题的因果分析，提出了一种新颖的微调方法，利用掩蔽图像作为反事实样本，有助于提高微调模型的鲁棒性。具体而言，我们基于类激活图对图像的语义相关或语义无关补丁进行掩蔽，以打破虚假相关性，并用其他图像的补丁来重新填充掩蔽的补丁。这些反事实样本则用于特征蒸馏。",
    "tldr": "本文提出了一种新颖的深度学习模型微调方法，利用掩蔽图像作为反事实样本，提高模型的鲁棒性。",
    "en_tdlr": "This paper proposes a novel fine-tuning method for deep learning models using masked images as counterfactual samples to improve the model's robustness."
}