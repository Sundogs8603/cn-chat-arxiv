{
    "title": "EVA-02: A Visual Representation for Neon Genesis. (arXiv:2303.11331v2 [cs.CV] UPDATED)",
    "abstract": "We launch EVA-02, a next-generation Transformer-based visual representation pre-trained to reconstruct strong and robust language-aligned vision features via masked image modeling. With an updated plain Transformer architecture as well as extensive pre-training from an open & accessible giant CLIP vision encoder, EVA-02 demonstrates superior performance compared to prior state-of-the-art approaches across various representative vision tasks, while utilizing significantly fewer parameters and compute budgets. Notably, using exclusively publicly accessible training data, EVA-02 with only 304M parameters achieves a phenomenal 90.0 fine-tuning top-1 accuracy on ImageNet-1K val set. Additionally, our EVA-02-CLIP can reach up to 80.4 zero-shot top-1 on ImageNet-1K, outperforming the previous largest & best open-sourced CLIP with only ~1/6 parameters and ~1/6 image-text training data. We offer four EVA-02 variants in various model sizes, ranging from 6M to 304M parameters, all with impressive",
    "link": "http://arxiv.org/abs/2303.11331",
    "context": "Title: EVA-02: A Visual Representation for Neon Genesis. (arXiv:2303.11331v2 [cs.CV] UPDATED)\nAbstract: We launch EVA-02, a next-generation Transformer-based visual representation pre-trained to reconstruct strong and robust language-aligned vision features via masked image modeling. With an updated plain Transformer architecture as well as extensive pre-training from an open & accessible giant CLIP vision encoder, EVA-02 demonstrates superior performance compared to prior state-of-the-art approaches across various representative vision tasks, while utilizing significantly fewer parameters and compute budgets. Notably, using exclusively publicly accessible training data, EVA-02 with only 304M parameters achieves a phenomenal 90.0 fine-tuning top-1 accuracy on ImageNet-1K val set. Additionally, our EVA-02-CLIP can reach up to 80.4 zero-shot top-1 on ImageNet-1K, outperforming the previous largest & best open-sourced CLIP with only ~1/6 parameters and ~1/6 image-text training data. We offer four EVA-02 variants in various model sizes, ranging from 6M to 304M parameters, all with impressive",
    "path": "papers/23/03/2303.11331.json",
    "total_tokens": 964,
    "translated_title": "EVA-02：新世纪福音战士的视觉表现",
    "translated_abstract": "我们发布EVA-02，这是一种基于Transformer的下一代视觉表征，经过预训练，通过掩蔽图像建模重建强大且稳健的特征，实现语言和视觉的对齐。使用更新的普通Transformer架构以及来自开放且易于访问的巨型CLIP视觉编码器的广泛预训练，EVA-02在各种代表性视觉任务方面表现优异，同时使用的参数和计算预算显著较少。值得注意的是，仅使用公开可访问的训练数据，具有304M参数的EVA-02在ImageNet-1K val集上实现了惊人的90.0微调top-1精度。此外，我们的EVA-02-CLIP在ImageNet-1K上的零样本top-1可达80.4，胜过了以前最大且最好的开源CLIP，仅使用了约1/6的参数和图像文本训练数据。我们提供了四种EVA-02变体，其模型大小各不相同，范围从6M到304M参数，均具有令人印象深刻的性能。",
    "tldr": "EVA-02是一种基于Transformer的下一代视觉表征，具有重建强大且稳健特征的能力，并在各种代表性视觉任务中表现出色，同时使用的参数和计算预算显著较少。",
    "en_tdlr": "EVA-02 is a next-generation visual representation based on Transformer, capable of reconstructing strong and robust features, and demonstrating superior performance across various representative vision tasks, while utilizing significantly fewer parameters and compute budgets."
}