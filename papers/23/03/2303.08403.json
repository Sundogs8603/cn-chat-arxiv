{
    "title": "DualFair: Fair Representation Learning at Both Group and Individual Levels via Contrastive Self-supervision. (arXiv:2303.08403v1 [cs.LG])",
    "abstract": "Algorithmic fairness has become an important machine learning problem, especially for mission-critical Web applications. This work presents a self-supervised model, called DualFair, that can debias sensitive attributes like gender and race from learned representations. Unlike existing models that target a single type of fairness, our model jointly optimizes for two fairness criteria - group fairness and counterfactual fairness - and hence makes fairer predictions at both the group and individual levels. Our model uses contrastive loss to generate embeddings that are indistinguishable for each protected group, while forcing the embeddings of counterfactual pairs to be similar. It then uses a self-knowledge distillation method to maintain the quality of representation for the downstream tasks. Extensive analysis over multiple datasets confirms the model's validity and further shows the synergy of jointly addressing two fairness criteria, suggesting the model's potential value in fair int",
    "link": "http://arxiv.org/abs/2303.08403",
    "context": "Title: DualFair: Fair Representation Learning at Both Group and Individual Levels via Contrastive Self-supervision. (arXiv:2303.08403v1 [cs.LG])\nAbstract: Algorithmic fairness has become an important machine learning problem, especially for mission-critical Web applications. This work presents a self-supervised model, called DualFair, that can debias sensitive attributes like gender and race from learned representations. Unlike existing models that target a single type of fairness, our model jointly optimizes for two fairness criteria - group fairness and counterfactual fairness - and hence makes fairer predictions at both the group and individual levels. Our model uses contrastive loss to generate embeddings that are indistinguishable for each protected group, while forcing the embeddings of counterfactual pairs to be similar. It then uses a self-knowledge distillation method to maintain the quality of representation for the downstream tasks. Extensive analysis over multiple datasets confirms the model's validity and further shows the synergy of jointly addressing two fairness criteria, suggesting the model's potential value in fair int",
    "path": "papers/23/03/2303.08403.json",
    "total_tokens": 950,
    "translated_title": "双重公平性：通过对比自监督实现群体和个体级别的公平表示学习",
    "translated_abstract": "算法公平性已成为重要的机器学习问题，特别是对于关键任务的Web应用。本文提出了一种自我监督模型DualFair，该模型可以使学习表示不带有性别和种族等敏感属性。与现有的针对单一公平性类型的模型不同，我们的模型联合优化了两个公平性标准，即群体公平性和反事实公平性，从而在群体和个体级别上做出更公正的预测。我们的模型使用对比损失来生成对每个受保护群体不可区分的嵌入，同时迫使反事实对的嵌入相似。然后，我们使用自我知识蒸馏方法来维护下游任务的表示质量。多个数据集的广泛分析证实了该模型的有效性，并进一步展示了联合处理两个公平性标准的协同效应，表明该模型在公平智能领域中具有潜在的价值。",
    "tldr": "本文提出了一种自我监督模型DualFair，该模型使用对比损失来生成对每个受保护群体不可区分的嵌入，并联合优化了群体公平性和反事实公平性两个公平性标准，使得在群体和个体级别上做出更公正的预测，该模型在公平智能领域中具有潜在价值。",
    "en_tdlr": "This paper proposes a self-supervised model called DualFair, which uses contrastive loss to generate embeddings that are indistinguishable for each protected group and optimizes for both group fairness and counterfactual fairness. DualFair shows potential value in fair intelligence field."
}