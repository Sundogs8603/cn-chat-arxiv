{
    "title": "Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations. (arXiv:2303.02536v2 [cs.AI] UPDATED)",
    "abstract": "Causal abstraction is a promising theoretical framework for explainable artificial intelligence that defines when an interpretable high-level causal model is a faithful simplification of a low-level deep learning system. However, existing causal abstraction methods have two major limitations: they require a brute-force search over alignments between the high-level model and the low-level one, and they presuppose that variables in the high-level model will align with disjoint sets of neurons in the low-level one. In this paper, we present distributed alignment search (DAS), which overcomes these limitations. In DAS, we find the alignment between high-level and low-level models using gradient descent rather than conducting a brute-force search, and we allow individual neurons to play multiple distinct roles by analyzing representations in non-standard bases-distributed representations. Our experiments show that DAS can discover internal structure that prior approaches miss. Overall, DAS ",
    "link": "http://arxiv.org/abs/2303.02536",
    "context": "Title: Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations. (arXiv:2303.02536v2 [cs.AI] UPDATED)\nAbstract: Causal abstraction is a promising theoretical framework for explainable artificial intelligence that defines when an interpretable high-level causal model is a faithful simplification of a low-level deep learning system. However, existing causal abstraction methods have two major limitations: they require a brute-force search over alignments between the high-level model and the low-level one, and they presuppose that variables in the high-level model will align with disjoint sets of neurons in the low-level one. In this paper, we present distributed alignment search (DAS), which overcomes these limitations. In DAS, we find the alignment between high-level and low-level models using gradient descent rather than conducting a brute-force search, and we allow individual neurons to play multiple distinct roles by analyzing representations in non-standard bases-distributed representations. Our experiments show that DAS can discover internal structure that prior approaches miss. Overall, DAS ",
    "path": "papers/23/03/2303.02536.json",
    "total_tokens": 977,
    "translated_title": "在可解释的因果变量和分布式神经表示之间寻找对齐方法",
    "translated_abstract": "因果抽象是可解释的人工智能的一个有前途的理论框架，它定义了可解释的高层因果模型何时是低层深度学习系统的可信简化。然而，现有的因果抽象方法存在两个主要限制：它们需要在高层模型和低层模型之间进行暴力搜索对齐，并且它们预设高层模型中的变量将与低层模型中的不相交的神经元集对齐。在本文中，我们提出了分布式对齐搜索（DAS），它克服了这些限制。在DAS中，我们使用梯度下降找到高层模型和低层模型之间的对齐方法，允许个体神经元在非传统基底分布表示中发挥多个不同的角色。我们的实验表明，DAS可以发现先前方法忽略的内部结构。总体而言，DAS有潜力实现对复杂深度学习系统的更好解释和理解。",
    "tldr": "本文提出了分布式对齐搜索（DAS）算法，可以在不使用暴力搜索的情况下找到高层因果模型和低层深度学习系统之间的对齐方法，并且DAS可以发现先前方法忽略的内部结构。DAS算法有潜力实现对复杂深度学习系统的更好解释和理解。",
    "en_tdlr": "This paper presents a distributed alignment search (DAS) algorithm that can find the alignment between high-level causal models and low-level deep learning systems without using brute-force search. DAS can discover internal structures that prior approaches miss and has the potential to enable better interpretability and understanding of complex deep learning systems."
}