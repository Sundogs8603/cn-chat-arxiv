{
    "title": "Rewarding Chatbots for Real-World Engagement with Millions of Users. (arXiv:2303.06135v2 [cs.CL] UPDATED)",
    "abstract": "The emergence of pretrained large language models has led to the deployment of a range of social chatbots for chitchat. Although these chatbots demonstrate language ability and fluency, they are not guaranteed to be engaging and can struggle to retain users. This work investigates the development of social chatbots that prioritize user engagement to enhance retention, specifically examining the use of human feedback to efficiently develop highly engaging chatbots. The proposed approach uses automatic pseudo-labels collected from user interactions to train a reward model that can be used to reject low-scoring sample responses generated by the chatbot model at inference time. Intuitive evaluation metrics, such as mean conversation length (MCL), are introduced as proxies to measure the level of engagement of deployed chatbots. A/B testing on groups of 10,000 new daily chatbot users on the Chai Research platform shows that this approach increases the MCL by up to 70%, which translates to a",
    "link": "http://arxiv.org/abs/2303.06135",
    "context": "Title: Rewarding Chatbots for Real-World Engagement with Millions of Users. (arXiv:2303.06135v2 [cs.CL] UPDATED)\nAbstract: The emergence of pretrained large language models has led to the deployment of a range of social chatbots for chitchat. Although these chatbots demonstrate language ability and fluency, they are not guaranteed to be engaging and can struggle to retain users. This work investigates the development of social chatbots that prioritize user engagement to enhance retention, specifically examining the use of human feedback to efficiently develop highly engaging chatbots. The proposed approach uses automatic pseudo-labels collected from user interactions to train a reward model that can be used to reject low-scoring sample responses generated by the chatbot model at inference time. Intuitive evaluation metrics, such as mean conversation length (MCL), are introduced as proxies to measure the level of engagement of deployed chatbots. A/B testing on groups of 10,000 new daily chatbot users on the Chai Research platform shows that this approach increases the MCL by up to 70%, which translates to a",
    "path": "papers/23/03/2303.06135.json",
    "total_tokens": 1013,
    "translated_title": "基于百万用户的现实世界互动来奖励聊天机器人",
    "translated_abstract": "预先训练的大型语言模型的出现，导致部署了一系列的社交聊天机器人。虽然这些聊天机器人展示了其语言能力和流畅性，但它们并不能保证很有吸引力，很容易失去用户。本文研究了开发优先考虑用户参与度以增强留存的社交聊天机器人，具体探讨了使用人工反馈以高效地开发高度有吸引力的聊天机器人。提出的方法使用从用户交互中收集的自动伪标签来训练奖励模型，该模型可用于在推理时拒绝低得分的样本响应，以提高用户参与度。引入了直观的评估指标，例如平均对话长度（MCL），作为衡量已部署聊天机器人参与度水平的代理。在Chai Research平台上对每日的10,000个新聊天机器人用户进行A/B测试，结果表明，这种方法可使MCL增加70％，这相当于将留存时间延长1.5倍。",
    "tldr": "本文研究了如何通过利用用户反馈来提高聊天机器人的参与度，从而增强其留存能力。具体方法是使用自动伪标签来训练奖励模型，并使用平均对话长度一类的指标来衡量其效果。在试验中，该方法可将聊天机器人的平均对话长度提高70%。"
}