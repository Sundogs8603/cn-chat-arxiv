{
    "title": "Building High-accuracy Multilingual ASR with Gated Language Experts and Curriculum Training. (arXiv:2303.00786v2 [cs.CL] UPDATED)",
    "abstract": "We propose gated language experts and curriculum training to enhance multilingual transformer transducer models without requiring language identification (LID) input from users during inference. Our method incorporates a gating mechanism and LID loss, enabling transformer experts to learn language-specific information. By combining gated transformer experts with shared transformer layers, we construct multilingual transformer blocks and utilize linear experts to effectively regularize the joint network. The curriculum training scheme leverages LID to guide the gated experts in improving their respective language performance. Experimental results on a bilingual task involving English and Spanish demonstrate significant improvements, with average relative word error reductions of 12.5% and 7.3% compared to the baseline bilingual and monolingual models, respectively. Notably, our method achieves performance comparable to the upper-bound model trained and inferred with oracle LID. Extendin",
    "link": "http://arxiv.org/abs/2303.00786",
    "context": "Title: Building High-accuracy Multilingual ASR with Gated Language Experts and Curriculum Training. (arXiv:2303.00786v2 [cs.CL] UPDATED)\nAbstract: We propose gated language experts and curriculum training to enhance multilingual transformer transducer models without requiring language identification (LID) input from users during inference. Our method incorporates a gating mechanism and LID loss, enabling transformer experts to learn language-specific information. By combining gated transformer experts with shared transformer layers, we construct multilingual transformer blocks and utilize linear experts to effectively regularize the joint network. The curriculum training scheme leverages LID to guide the gated experts in improving their respective language performance. Experimental results on a bilingual task involving English and Spanish demonstrate significant improvements, with average relative word error reductions of 12.5% and 7.3% compared to the baseline bilingual and monolingual models, respectively. Notably, our method achieves performance comparable to the upper-bound model trained and inferred with oracle LID. Extendin",
    "path": "papers/23/03/2303.00786.json",
    "total_tokens": 926,
    "translated_title": "使用门控语言专家和课程训练构建高准确度的多语言ASR",
    "translated_abstract": "我们提出使用门控语言专家和课程训练来增强多语言变压器传导模型，在推断过程中无需用户提供语言识别（LID）输入。我们的方法采用门控机制和LID损失，使变压器专家学习语言特定信息。通过将门控变压器专家与共享变压器层相结合，我们构建了多语言变压器块，并利用线性专家有效地规范化联合网络。课程训练方案利用LID引导门控专家改进各自的语言性能。在涉及英语和西班牙语的双语任务上的实验证明了显著的改进，与基线的双语和单语模型相比，平均相对词错误率分别降低了12.5％和7.3％。值得注意的是，我们的方法的性能与使用oracle LID训练和推断的上限模型相当。",
    "tldr": "本文提出了一种使用门控语言专家和课程训练来增强多语言ASR的方法，无需用户提供LID输入。在双语任务中，与基线模型相比，平均词错误率分别降低了12.5％和7.3％，并且与使用oracle LID训练的上限模型相当。",
    "en_tdlr": "This paper proposes a method to enhance multilingual ASR using gated language experts and curriculum training, without requiring LID input from users. Experimental results show significant improvements compared to baseline models, with average word error rate reductions of 12.5% and 7.3% in bilingual tasks, achieving performance comparable to the upper-bound model trained with oracle LID."
}