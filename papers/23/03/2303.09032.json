{
    "title": "Conditionally Optimistic Exploration for Cooperative Deep Multi-Agent Reinforcement Learning. (arXiv:2303.09032v1 [cs.LG])",
    "abstract": "Efficient exploration is critical in cooperative deep Multi-Agent Reinforcement Learning (MARL). In this paper, we propose an exploration method that efficiently encourages cooperative exploration based on the idea of the theoretically justified tree search algorithm UCT (Upper Confidence bounds applied to Trees). The high-level intuition is that to perform optimism-based exploration, agents would achieve cooperative strategies if each agent's optimism estimate captures a structured dependency relationship with other agents. At each node (i.e., action) of the search tree, UCT performs optimism-based exploration using a bonus derived by conditioning on the visitation count of its parent node. We provide a perspective to view MARL as tree search iterations and develop a method called Conditionally Optimistic Exploration (COE). We assume agents take actions following a sequential order, and consider nodes at the same depth of the search tree as actions of one individual agent. COE compute",
    "link": "http://arxiv.org/abs/2303.09032",
    "context": "Title: Conditionally Optimistic Exploration for Cooperative Deep Multi-Agent Reinforcement Learning. (arXiv:2303.09032v1 [cs.LG])\nAbstract: Efficient exploration is critical in cooperative deep Multi-Agent Reinforcement Learning (MARL). In this paper, we propose an exploration method that efficiently encourages cooperative exploration based on the idea of the theoretically justified tree search algorithm UCT (Upper Confidence bounds applied to Trees). The high-level intuition is that to perform optimism-based exploration, agents would achieve cooperative strategies if each agent's optimism estimate captures a structured dependency relationship with other agents. At each node (i.e., action) of the search tree, UCT performs optimism-based exploration using a bonus derived by conditioning on the visitation count of its parent node. We provide a perspective to view MARL as tree search iterations and develop a method called Conditionally Optimistic Exploration (COE). We assume agents take actions following a sequential order, and consider nodes at the same depth of the search tree as actions of one individual agent. COE compute",
    "path": "papers/23/03/2303.09032.json",
    "total_tokens": 899,
    "translated_title": "条件乐观探索用于深度多智能体强化学习",
    "translated_abstract": "高效的探索对于协作深度多智能体强化学习(MARL)至关重要。本文提出了一种基于UCT(应用于树的置信上限)的探索方法，该方法有效地鼓励协同探索。高层次的思路是，为了进行基于乐观主义的探索，如果每个智能体的乐观估计捕获了与其他智能体的结构化依赖关系，则智能体将实现协作策略。在搜索树的每个节点（即动作）上，UCT使用通过对其父节点的访问计数进行条件推导的奖励来执行基于乐观主义的探索。我们提出了一种将MARL视为树搜索迭代的方法，并开发了一种名为条件乐观探索(COE)的方法。我们假设智能体按顺序执行动作，并将搜索树同一深度的节点视为一个单独智能体的动作。COE计算基于乐观主义的奖励，以鼓励智能体通过尝试新的行动来探索未知的状态。",
    "tldr": "本文提出了一种名为条件乐观探索(COE)的基于UCT的探索方法，通过结构依赖关系鼓励智能体进行基于乐观主义的协同探索。"
}