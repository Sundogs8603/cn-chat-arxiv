{
    "title": "MCR-DL: Mix-and-Match Communication Runtime for Deep Learning. (arXiv:2303.08374v1 [cs.DC])",
    "abstract": "In recent years, the training requirements of many state-of-the-art Deep Learning (DL) models have scaled beyond the compute and memory capabilities of a single processor, and necessitated distribution among processors. Training such massive models necessitates advanced parallelism strategies to maintain efficiency. However, such distributed DL parallelism strategies require a varied mixture of collective and point-to-point communication operations across a broad range of message sizes and scales. Examples of models using advanced parallelism strategies include Deep Learning Recommendation Models (DLRM) and Mixture-of-Experts (MoE). Communication libraries' performance varies wildly across different communication operations, scales, and message sizes. We propose MCR-DL: an extensible DL communication framework that supports all point-to-point and collective operations while enabling users to dynamically mix-and-match communication backends for a given operation without deadlocks. MCR-D",
    "link": "http://arxiv.org/abs/2303.08374",
    "context": "Title: MCR-DL: Mix-and-Match Communication Runtime for Deep Learning. (arXiv:2303.08374v1 [cs.DC])\nAbstract: In recent years, the training requirements of many state-of-the-art Deep Learning (DL) models have scaled beyond the compute and memory capabilities of a single processor, and necessitated distribution among processors. Training such massive models necessitates advanced parallelism strategies to maintain efficiency. However, such distributed DL parallelism strategies require a varied mixture of collective and point-to-point communication operations across a broad range of message sizes and scales. Examples of models using advanced parallelism strategies include Deep Learning Recommendation Models (DLRM) and Mixture-of-Experts (MoE). Communication libraries' performance varies wildly across different communication operations, scales, and message sizes. We propose MCR-DL: an extensible DL communication framework that supports all point-to-point and collective operations while enabling users to dynamically mix-and-match communication backends for a given operation without deadlocks. MCR-D",
    "path": "papers/23/03/2303.08374.json",
    "total_tokens": 899,
    "translated_title": "MCR-DL: 用于深度学习的混合通信运行时",
    "translated_abstract": "最近几年，许多最先进的深度学习模型的训练需求超出了单个处理器的计算和内存能力，并需要在多个处理器之间分配。训练这样的大型模型需要先进的并行策略以保持效率。然而，这种分布式的深度学习并行策略需要在各种消息大小和规模下进行各种集合和点对点通信操作的混合。使用高级并行策略的模型包括：深度学习推荐模型（DLRM）和专家混合（MoE）。通信库在不同的通信操作、规模和消息大小下的性能差异巨大。我们提出了 MCR-DL：一种可扩展的深度学习通信框架，支持所有点对点和集体操作，同时使用户能够动态地混合和匹配通信后端以进行给定操作而不会出现死锁。",
    "tldr": "该论文提出了MCR-DL，一种用于深度学习的可扩展混合通信框架，能够支持各种集合和点对点操作，并允许用户动态地混合和匹配通信后端以进行给定操作，从而提高深度学习模型的训练效率。",
    "en_tdlr": "The paper proposes MCR-DL, an extensible mixed communication framework for deep learning, which supports a variety of collective and point-to-point operations and allows users to dynamically mix and match communication backends for a given operation, thus improving the training efficiency of deep learning models."
}