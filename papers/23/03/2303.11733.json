{
    "title": "DIPPM: a Deep Learning Inference Performance Predictive Model using Graph Neural Networks. (arXiv:2303.11733v1 [cs.PF])",
    "abstract": "Deep Learning (DL) has developed to become a corner-stone in many everyday applications that we are now relying on. However, making sure that the DL model uses the underlying hardware efficiently takes a lot of effort. Knowledge about inference characteristics can help to find the right match so that enough resources are given to the model, but not too much. We have developed a DL Inference Performance Predictive Model (DIPPM) that predicts the inference latency, energy, and memory usage of a given input DL model on the NVIDIA A100 GPU. We also devised an algorithm to suggest the appropriate A100 Multi-Instance GPU profile from the output of DIPPM. We developed a methodology to convert DL models expressed in multiple frameworks to a generalized graph structure that is used in DIPPM. It means DIPPM can parse input DL models from various frameworks. Our DIPPM can be used not only helps to find suitable hardware configurations but also helps to perform rapid design-space exploration for t",
    "link": "http://arxiv.org/abs/2303.11733",
    "context": "Title: DIPPM: a Deep Learning Inference Performance Predictive Model using Graph Neural Networks. (arXiv:2303.11733v1 [cs.PF])\nAbstract: Deep Learning (DL) has developed to become a corner-stone in many everyday applications that we are now relying on. However, making sure that the DL model uses the underlying hardware efficiently takes a lot of effort. Knowledge about inference characteristics can help to find the right match so that enough resources are given to the model, but not too much. We have developed a DL Inference Performance Predictive Model (DIPPM) that predicts the inference latency, energy, and memory usage of a given input DL model on the NVIDIA A100 GPU. We also devised an algorithm to suggest the appropriate A100 Multi-Instance GPU profile from the output of DIPPM. We developed a methodology to convert DL models expressed in multiple frameworks to a generalized graph structure that is used in DIPPM. It means DIPPM can parse input DL models from various frameworks. Our DIPPM can be used not only helps to find suitable hardware configurations but also helps to perform rapid design-space exploration for t",
    "path": "papers/23/03/2303.11733.json",
    "total_tokens": 936,
    "translated_abstract": "深度学习已经成为我们现在所依赖的许多日常应用程序的基础。然而，确保DL模型有效地利用底层硬件需要付出很多努力。了解推理特性可以帮助找到合适的匹配，使足够的资源分配给模型，但不会过度分配。本文提出了一种DL推理性能预测模型 (DIPPM)，该模型可以预测给定输入DL模型在NVIDIA A100 GPU上的推理延迟、能耗和内存使用情况。我们还设计了一种算法，根据DIPPM输出来建议适当的A100多实例GPU配置文件。我们开发了一种方法，将用多个框架表示的DL模型转换为用于DIPPM的广义图形结构。这意味着DIPPM可以解析来自各种框架的DL模型。我们的DIPPM不仅可以帮助找到适合的硬件配置，还可以帮助快速进行设计空间的探索。",
    "tldr": "本文提出了一种DIPPM深度学习推理性能预测模型，它可以预测输入DL模型在NVIDIA A100 GPU上的推理延迟、能耗和内存使用情况，并为A100多实例GPU提供适当的配置文件。"
}