{
    "title": "Improving Code Generation by Training with Natural Language Feedback. (arXiv:2303.16749v1 [cs.SE])",
    "abstract": "The potential for pre-trained large language models (LLMs) to use natural language feedback at inference time has been an exciting recent development. We build upon this observation by formalizing an algorithm for learning from natural language feedback at training time instead, which we call Imitation learning from Language Feedback (ILF). ILF requires only a small amount of human-written feedback during training and does not require the same feedback at test time, making it both user-friendly and sample-efficient. We further show that ILF can be seen as a form of minimizing the KL divergence to the ground truth distribution and demonstrate a proof-of-concept on a neural program synthesis task. We use ILF to improve a Codegen-Mono 6.1B model's pass@1 rate by 38% relative (and 10% absolute) on the Mostly Basic Python Problems (MBPP) benchmark, outperforming both fine-tuning on MBPP and fine-tuning on repaired programs written by humans. Overall, our results suggest that learning from h",
    "link": "http://arxiv.org/abs/2303.16749",
    "context": "Title: Improving Code Generation by Training with Natural Language Feedback. (arXiv:2303.16749v1 [cs.SE])\nAbstract: The potential for pre-trained large language models (LLMs) to use natural language feedback at inference time has been an exciting recent development. We build upon this observation by formalizing an algorithm for learning from natural language feedback at training time instead, which we call Imitation learning from Language Feedback (ILF). ILF requires only a small amount of human-written feedback during training and does not require the same feedback at test time, making it both user-friendly and sample-efficient. We further show that ILF can be seen as a form of minimizing the KL divergence to the ground truth distribution and demonstrate a proof-of-concept on a neural program synthesis task. We use ILF to improve a Codegen-Mono 6.1B model's pass@1 rate by 38% relative (and 10% absolute) on the Mostly Basic Python Problems (MBPP) benchmark, outperforming both fine-tuning on MBPP and fine-tuning on repaired programs written by humans. Overall, our results suggest that learning from h",
    "path": "papers/23/03/2303.16749.json",
    "total_tokens": 936,
    "translated_title": "利用自然语言反馈进行代码生成的改进",
    "translated_abstract": "预先训练好的大型语言模型（LLM）在推理时使用自然语言反馈的潜力是最近的一个令人兴奋的发展。我们在此基础上提出一种名为Language Feedback（ILF）的算法，用于从自然语言反馈中进行学习。ILF在训练期间仅需要少量的人工编写反馈，并且在测试时不需要相同的反馈，因此使用起来既方便又高效。此外，我们进一步证明ILF可以被视为最小化与基准分布的KL散度的一种形式，并在神经程序合成任务上进行了概念验证。我们使用ILF在Mostly Basic Python Problems(MBPP)基准测试上将Codegen-Mono 6.1B模型的pass @ 1覆盖率相对提高了38%（绝对提高了10%），胜过了在MBPP上微调和在人类修复的程序上微调的模型。总的来说，我们的结果表明，即使只有少量反馈，从人类编写的自然语言反馈中进行学习也可以显著改进代码生成模型。",
    "tldr": "该论文提出了一种新算法ILF，通过从自然语言反馈中进行学习来显著提高代码生成模型的性能，即使只有少量反馈，也可以获得很好的效果。"
}