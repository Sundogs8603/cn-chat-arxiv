{
    "title": "Task-Attentive Transformer Architecture for Continual Learning of Vision-and-Language Tasks Using Knowledge Distillation. (arXiv:2303.14423v1 [cs.LG])",
    "abstract": "The size and the computational load of fine-tuning large-scale pre-trained neural network are becoming two major obstacles in adopting machine learning in many applications. Continual learning (CL) can serve as a remedy through enabling knowledge-transfer across sequentially arriving tasks which relaxes the need to fine-tune all network weights from scratch. However, existing CL algorithms primarily consider learning unimodal vision-only or language-only tasks. We develop a transformer-based CL architecture for learning bimodal vision-and-language tasks based on increasing the number of the learnable parameters dynamically and using knowledge distillation. The new additional parameters are used to specialize the network for each task. Our approach enables sharing information between the tasks while addressing the challenge of catastrophic forgetting. Our approach is scalable learning to a large number of tasks because it requires little memory and time overhead. Our model reaches state",
    "link": "http://arxiv.org/abs/2303.14423",
    "context": "Title: Task-Attentive Transformer Architecture for Continual Learning of Vision-and-Language Tasks Using Knowledge Distillation. (arXiv:2303.14423v1 [cs.LG])\nAbstract: The size and the computational load of fine-tuning large-scale pre-trained neural network are becoming two major obstacles in adopting machine learning in many applications. Continual learning (CL) can serve as a remedy through enabling knowledge-transfer across sequentially arriving tasks which relaxes the need to fine-tune all network weights from scratch. However, existing CL algorithms primarily consider learning unimodal vision-only or language-only tasks. We develop a transformer-based CL architecture for learning bimodal vision-and-language tasks based on increasing the number of the learnable parameters dynamically and using knowledge distillation. The new additional parameters are used to specialize the network for each task. Our approach enables sharing information between the tasks while addressing the challenge of catastrophic forgetting. Our approach is scalable learning to a large number of tasks because it requires little memory and time overhead. Our model reaches state",
    "path": "papers/23/03/2303.14423.json",
    "total_tokens": 934,
    "translated_title": "基于任务关注变换器架构的知识蒸馏连续学习视觉语言任务",
    "translated_abstract": "对于在许多应用中采用机器学习而言，预训练神经网络的规模和计算负载成为两个主要障碍。而连续学习 (CL) 可以作为一种补救措施，通过使顺序到达的任务之间进行知识转移，从而减轻了需要从头开始微调所有网络权重的需要。然而，现有的 CL 算法主要考虑学习单模态的仅视觉或仅语言任务。我们开发了一种基于变换器的 CL 架构，用于学习双模态的视觉语言任务，通过动态增加可学习参数的数量并使用知识蒸馏。新添加的参数用于为每个任务专门定制网络。我们的方法使任务之间可以共享信息，同时解决了灾难性遗忘的挑战。我们的方法可扩展到大量任务的学习，因为它需要很少的内存和时间开销。我们的模型达到了最先进的性能水平。",
    "tldr": "本文提出一种基于变换器的连续学习架构，用于学习双模态的视觉语言任务，并采用动态增加参数和知识蒸馏的方法，实现在多任务学习中共享信息和解决灾难性遗忘的问题。该方法具有扩展性和高效性能。",
    "en_tdlr": "This paper proposes a transformer-based continual learning architecture for bimodal vision-and-language tasks, which dynamically increases the number of learnable parameters and uses knowledge distillation to enable information sharing between tasks and address catastrophic forgetting. It achieves state-of-the-art performance and is highly scalable and efficient."
}