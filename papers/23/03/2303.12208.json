{
    "title": "MAGVLT: Masked Generative Vision-and-Language Transformer. (arXiv:2303.12208v1 [cs.CV])",
    "abstract": "While generative modeling on multimodal image-text data has been actively developed with large-scale paired datasets, there have been limited attempts to generate both image and text data by a single model rather than a generation of one fixed modality conditioned on the other modality. In this paper, we explore a unified generative vision-and-language (VL) model that can produce both images and text sequences. Especially, we propose a generative VL transformer based on the non-autoregressive mask prediction, named MAGVLT, and compare it with an autoregressive generative VL transformer (ARGVLT). In comparison to ARGVLT, the proposed MAGVLT enables bidirectional context encoding, fast decoding by parallel token predictions in an iterative refinement, and extended editing capabilities such as image and text infilling. For rigorous training of our MAGVLT with image-text pairs from scratch, we combine the image-to-text, text-to-image, and joint image-and-text mask prediction tasks. Moreove",
    "link": "http://arxiv.org/abs/2303.12208",
    "context": "Title: MAGVLT: Masked Generative Vision-and-Language Transformer. (arXiv:2303.12208v1 [cs.CV])\nAbstract: While generative modeling on multimodal image-text data has been actively developed with large-scale paired datasets, there have been limited attempts to generate both image and text data by a single model rather than a generation of one fixed modality conditioned on the other modality. In this paper, we explore a unified generative vision-and-language (VL) model that can produce both images and text sequences. Especially, we propose a generative VL transformer based on the non-autoregressive mask prediction, named MAGVLT, and compare it with an autoregressive generative VL transformer (ARGVLT). In comparison to ARGVLT, the proposed MAGVLT enables bidirectional context encoding, fast decoding by parallel token predictions in an iterative refinement, and extended editing capabilities such as image and text infilling. For rigorous training of our MAGVLT with image-text pairs from scratch, we combine the image-to-text, text-to-image, and joint image-and-text mask prediction tasks. Moreove",
    "path": "papers/23/03/2303.12208.json",
    "total_tokens": 872,
    "translated_title": "MAGVLT: 带掩码的生成视觉语言变压器",
    "translated_abstract": "虽然在多模态图像文本数据上的生成建模已经得到了广泛的发展，但仍有许多限制，例如仅生成一种模态的固定模型。在本文中，我们探讨了一种可以生成图像和文本序列的统一生成式视觉语言（VL）模型。特别地，我们提出了一种基于非自回归掩码预测的生成VL变压器，名为MAGVLT，并将其与自回归生成VL变压器（ARGVLT）进行了比较。与ARGVLT相比，所提出的MAGVLT实现了双向上下文编码，通过迭代细化的并行标记预测实现了快速解码，具有图像和文本填充等扩展编辑功能。为了从头开始严格训练我们的MAGVLT模型，我们结合了从图像到文本、从文本到图像、以及联合图像和文本的掩码预测任务。",
    "tldr": "本文提出 MGVLT 模型用于生成图像和文本序列，通过非自回归掩码预测实现了双向上下文编码和快速解码等特点。",
    "en_tdlr": "The paper presents a unified generative vision-and-language model (VL) called MAGVLT, which can generate both images and text sequences. The proposed model enables bidirectional context encoding, fast decoding through non-autoregressive mask prediction, and extended editing capabilities such as image and text infilling."
}