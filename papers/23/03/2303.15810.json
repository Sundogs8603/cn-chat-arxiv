{
    "title": "Offline RL with No OOD Actions: In-Sample Learning via Implicit Value Regularization. (arXiv:2303.15810v1 [cs.LG])",
    "abstract": "Most offline reinforcement learning (RL) methods suffer from the trade-off between improving the policy to surpass the behavior policy and constraining the policy to limit the deviation from the behavior policy as computing $Q$-values using out-of-distribution (OOD) actions will suffer from errors due to distributional shift. The recently proposed \\textit{In-sample Learning} paradigm (i.e., IQL), which improves the policy by quantile regression using only data samples, shows great promise because it learns an optimal policy without querying the value function of any unseen actions. However, it remains unclear how this type of method handles the distributional shift in learning the value function. In this work, we make a key finding that the in-sample learning paradigm arises under the \\textit{Implicit Value Regularization} (IVR) framework. This gives a deeper understanding of why the in-sample learning paradigm works, i.e., it applies implicit value regularization to the policy. Based ",
    "link": "http://arxiv.org/abs/2303.15810",
    "context": "Title: Offline RL with No OOD Actions: In-Sample Learning via Implicit Value Regularization. (arXiv:2303.15810v1 [cs.LG])\nAbstract: Most offline reinforcement learning (RL) methods suffer from the trade-off between improving the policy to surpass the behavior policy and constraining the policy to limit the deviation from the behavior policy as computing $Q$-values using out-of-distribution (OOD) actions will suffer from errors due to distributional shift. The recently proposed \\textit{In-sample Learning} paradigm (i.e., IQL), which improves the policy by quantile regression using only data samples, shows great promise because it learns an optimal policy without querying the value function of any unseen actions. However, it remains unclear how this type of method handles the distributional shift in learning the value function. In this work, we make a key finding that the in-sample learning paradigm arises under the \\textit{Implicit Value Regularization} (IVR) framework. This gives a deeper understanding of why the in-sample learning paradigm works, i.e., it applies implicit value regularization to the policy. Based ",
    "path": "papers/23/03/2303.15810.json",
    "total_tokens": 1142,
    "translated_title": "没有OOD动作的离线强化学习：基于隐式价值正则化的样本内学习",
    "translated_abstract": "大多数离线强化学习 (RL) 方法面临一个折衷问题：改善策略以超越行为策略与限制策略以限制与行为策略的偏差之间的平衡。由于使用超出分布范围 (OOD) 的动作计算 Q 值会受到分布偏移错误的影响。最近提出的基于样本内学习范式（IQL）通过对数据样本进行分位回归来改善策略，表现出很大的潜力，因为它可以学习最优策略而不查询任何未见动作的值函数。然而，目前尚不清楚这种方法如何处理学习价值函数时的分布偏移。本文发现，样本内学习范例在隐式价值正则化 (IVR) 框架下得以产生。这给了我们更深刻的理解为什么样本内学习范例有效，即它将隐式价值正则化应用于策略。基于这个洞见，我们提出了一个新算法，IVR-Q，它通过规范化策略的价值函数以避免OOD动作，并通过最小化IVR loss来改善策略。多个基准任务的实验结果表明，IVR-Q优于现有方法，并在离线RL中实现了最佳性能。",
    "tldr": "本文提出了一种新算法，IVR-Q，用于离线强化学习，其通过隐式价值正则化的方式避免了OOD动作带来的价值函数偏移，并通过最小化IVR loss来改善策略。在多个基准任务上的实验结果证明其优于现有方法并实现了最佳性能。",
    "en_tdlr": "This paper proposes a new algorithm, IVR-Q, for offline reinforcement learning, which avoids the value function shift caused by OOD actions through implicit value regularization and improves the policy by minimizing IVR loss. Experimental results on multiple benchmark tasks demonstrate its superiority over existing methods and achieve state-of-the-art performance in offline RL."
}