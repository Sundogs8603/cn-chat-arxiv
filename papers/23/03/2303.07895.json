{
    "title": "The Learnability of In-Context Learning. (arXiv:2303.07895v1 [cs.CL])",
    "abstract": "In-context learning is a surprising and important phenomenon that emerged when modern language models were scaled to billions of learned parameters. Without modifying a large language model's weights, it can be tuned to perform various downstream natural language tasks simply by including concatenated training examples of these tasks in its input. Though disruptive for many practical applications of large language models, this emergent learning paradigm is not well understood from a theoretical perspective. In this paper, we propose a first-of-its-kind PAC based framework for in-context learnability, and use it to provide the first finite sample complexity results for the in-context learning setup. Our framework includes an initial pretraining phase, which fits a function to the pretraining distribution, and then a second in-context learning phase, which keeps this function constant and concatenates training examples of the downstream task in its input. We use our framework in order to",
    "link": "http://arxiv.org/abs/2303.07895",
    "context": "Title: The Learnability of In-Context Learning. (arXiv:2303.07895v1 [cs.CL])\nAbstract: In-context learning is a surprising and important phenomenon that emerged when modern language models were scaled to billions of learned parameters. Without modifying a large language model's weights, it can be tuned to perform various downstream natural language tasks simply by including concatenated training examples of these tasks in its input. Though disruptive for many practical applications of large language models, this emergent learning paradigm is not well understood from a theoretical perspective. In this paper, we propose a first-of-its-kind PAC based framework for in-context learnability, and use it to provide the first finite sample complexity results for the in-context learning setup. Our framework includes an initial pretraining phase, which fits a function to the pretraining distribution, and then a second in-context learning phase, which keeps this function constant and concatenates training examples of the downstream task in its input. We use our framework in order to",
    "path": "papers/23/03/2303.07895.json",
    "total_tokens": 821,
    "translated_title": "在上下文学习的可学习性",
    "translated_abstract": "在现代语言模型参数扩展到数十亿个的情况下出现了令人惊讶且重要的上下文学习现象。在不修改大型语言模型的权重的情况下，只需将这些任务的训练样例与其输入连接即可将其调整为执行各种下游自然语言任务。虽然对于大型语言模型的许多实际应用具有破坏性，但这种新兴的学习范式从理论角度尚不为人所知。本文提出了一个首次基于PAC的上下文可学习性框架，并利用它为上下文学习设置提供了首个有限样本复杂度结果。我们的框架包括一个初始预训练阶段，它将一个函数拟合到预训练分布中，然后是第二个上下文学习阶段，它保持该函数不变，并将下游任务的训练样例连接到其输入中。我们使用我们的框架来",
    "tldr": "本文提出了首个基于PAC的上下文可学习性框架，并使用其为上下文学习设置提供了首个有限样本复杂度结果。"
}