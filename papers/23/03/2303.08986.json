{
    "title": "Deep Learning Weight Pruning with RMT-SVD: Increasing Accuracy and Reducing Overfitting. (arXiv:2303.08986v1 [cs.LG])",
    "abstract": "In this work, we present some applications of random matrix theory for the training of deep neural networks. Recently, random matrix theory (RMT) has been applied to the overfitting problem in deep learning. Specifically, it has been shown that the spectrum of the weight layers of a deep neural network (DNN) can be studied and understood using techniques from RMT. In this work, these RMT techniques will be used to determine which and how many singular values should be removed from the weight layers of a DNN during training, via singular value decomposition (SVD), so as to reduce overfitting and increase accuracy. We show the results on a simple DNN model trained on MNIST. In general, these techniques may be applied to any fully connected layer of a pretrained DNN to reduce the number of parameters in the layer while preserving and sometimes increasing the accuracy of the DNN.",
    "link": "http://arxiv.org/abs/2303.08986",
    "context": "Title: Deep Learning Weight Pruning with RMT-SVD: Increasing Accuracy and Reducing Overfitting. (arXiv:2303.08986v1 [cs.LG])\nAbstract: In this work, we present some applications of random matrix theory for the training of deep neural networks. Recently, random matrix theory (RMT) has been applied to the overfitting problem in deep learning. Specifically, it has been shown that the spectrum of the weight layers of a deep neural network (DNN) can be studied and understood using techniques from RMT. In this work, these RMT techniques will be used to determine which and how many singular values should be removed from the weight layers of a DNN during training, via singular value decomposition (SVD), so as to reduce overfitting and increase accuracy. We show the results on a simple DNN model trained on MNIST. In general, these techniques may be applied to any fully connected layer of a pretrained DNN to reduce the number of parameters in the layer while preserving and sometimes increasing the accuracy of the DNN.",
    "path": "papers/23/03/2303.08986.json",
    "total_tokens": 738,
    "translated_title": "RMT-SVD实现深度学习权重剪枝：提高准确性和减少过拟合",
    "translated_abstract": "本文提出了使用随机矩阵理论(RMT)训练深度神经网络的应用。我们利用RMT技术来确定在训练DNN过程中应该去除哪些和多少奇异值，以通过奇异值分解(SVD)减少过拟合和提高准确性。在MNIST数据集上实验结果表明，这些技术可以减少全连接层的参数数量而保持或提高DNN的准确性。",
    "tldr": "本文提出了一种使用随机矩阵理论技术进行深度学习权重剪枝的方法，可以通过奇异值分解技术去除一些特定奇异值，从而减少过拟合和提高准确性。",
    "en_tdlr": "This paper presents a method of deep learning weight pruning using random matrix theory, which utilizes singular value decomposition to remove certain singular values during the training process, reducing overfitting and increasing accuracy."
}