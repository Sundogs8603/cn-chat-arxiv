{
    "title": "Optimizing Quantum Federated Learning Based on Federated Quantum Natural Gradient Descent. (arXiv:2303.08116v1 [quant-ph])",
    "abstract": "Quantum federated learning (QFL) is a quantum extension of the classical federated learning model across multiple local quantum devices. An efficient optimization algorithm is always expected to minimize the communication overhead among different quantum participants. In this work, we propose an efficient optimization algorithm, namely federated quantum natural gradient descent (FQNGD), and further, apply it to a QFL framework that is composed of a variational quantum circuit (VQC)-based quantum neural networks (QNN). Compared with stochastic gradient descent methods like Adam and Adagrad, the FQNGD algorithm admits much fewer training iterations for the QFL to get converged. Moreover, it can significantly reduce the total communication overhead among local quantum devices. Our experiments on a handwritten digit classification dataset justify the effectiveness of the FQNGD for the QFL framework in terms of a faster convergence rate on the training set and higher accuracy on the test se",
    "link": "http://arxiv.org/abs/2303.08116",
    "context": "Title: Optimizing Quantum Federated Learning Based on Federated Quantum Natural Gradient Descent. (arXiv:2303.08116v1 [quant-ph])\nAbstract: Quantum federated learning (QFL) is a quantum extension of the classical federated learning model across multiple local quantum devices. An efficient optimization algorithm is always expected to minimize the communication overhead among different quantum participants. In this work, we propose an efficient optimization algorithm, namely federated quantum natural gradient descent (FQNGD), and further, apply it to a QFL framework that is composed of a variational quantum circuit (VQC)-based quantum neural networks (QNN). Compared with stochastic gradient descent methods like Adam and Adagrad, the FQNGD algorithm admits much fewer training iterations for the QFL to get converged. Moreover, it can significantly reduce the total communication overhead among local quantum devices. Our experiments on a handwritten digit classification dataset justify the effectiveness of the FQNGD for the QFL framework in terms of a faster convergence rate on the training set and higher accuracy on the test se",
    "path": "papers/23/03/2303.08116.json",
    "total_tokens": 901,
    "translated_title": "基于联邦量子自然梯度下降算法的量子联邦学习优化",
    "translated_abstract": "量子联邦学习（QFL）是在多个本地量子设备之间进行的经典联邦学习模型的量子扩展。一个有效的优化算法可以最小化不同量子参与者之间的通信开销。本文提出了一种高效的优化算法，即联邦量子自然梯度下降（FQNGD），并将其应用于由基于变分量子电路（VQC）的量子神经网络（QNN）组成的QFL框架中。与Adam和Adagrad等随机梯度下降方法相比，FQNGD算法所需的训练迭代次数要少得多，可以显著减少本地量子设备之间的通信开销。我们在手写数字分类数据集上的实验证明了FQNGD对于QFL框架的有效性，具有较快的训练收敛速度和较高的测试精度。",
    "tldr": "本文提出了一种基于联邦量子自然梯度下降算法的量子联邦学习优化算法，该算法可以在减少本地量子设备之间的通信开销的同时，比传统的随机梯度下降方法更快地实现训练收敛和提高测试精度。",
    "en_tdlr": "This article proposes an optimization algorithm for quantum federated learning based on federated quantum natural gradient descent. It reduces the communication overhead among local quantum devices and achieves faster convergence in training and higher accuracy in testing compared to traditional stochastic gradient descent methods."
}