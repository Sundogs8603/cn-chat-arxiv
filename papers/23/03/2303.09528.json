{
    "title": "Reinforcement Learning for Omega-Regular Specifications on Continuous-Time MDP. (arXiv:2303.09528v1 [cs.LG])",
    "abstract": "Continuous-time Markov decision processes (CTMDPs) are canonical models to express sequential decision-making under dense-time and stochastic environments. When the stochastic evolution of the environment is only available via sampling, model-free reinforcement learning (RL) is the algorithm-of-choice to compute optimal decision sequence. RL, on the other hand, requires the learning objective to be encoded as scalar reward signals. Since doing such translations manually is both tedious and error-prone, a number of techniques have been proposed to translate high-level objectives (expressed in logic or automata formalism) to scalar rewards for discrete-time Markov decision processes (MDPs). Unfortunately, no automatic translation exists for CTMDPs.  We consider CTMDP environments against the learning objectives expressed as omega-regular languages. Omega-regular languages generalize regular languages to infinite-horizon specifications and can express properties given in popular linear-ti",
    "link": "http://arxiv.org/abs/2303.09528",
    "context": "Title: Reinforcement Learning for Omega-Regular Specifications on Continuous-Time MDP. (arXiv:2303.09528v1 [cs.LG])\nAbstract: Continuous-time Markov decision processes (CTMDPs) are canonical models to express sequential decision-making under dense-time and stochastic environments. When the stochastic evolution of the environment is only available via sampling, model-free reinforcement learning (RL) is the algorithm-of-choice to compute optimal decision sequence. RL, on the other hand, requires the learning objective to be encoded as scalar reward signals. Since doing such translations manually is both tedious and error-prone, a number of techniques have been proposed to translate high-level objectives (expressed in logic or automata formalism) to scalar rewards for discrete-time Markov decision processes (MDPs). Unfortunately, no automatic translation exists for CTMDPs.  We consider CTMDP environments against the learning objectives expressed as omega-regular languages. Omega-regular languages generalize regular languages to infinite-horizon specifications and can express properties given in popular linear-ti",
    "path": "papers/23/03/2303.09528.json",
    "total_tokens": 896,
    "translated_abstract": "连续时间马尔可夫决策过程（CTMDP）是表达在稠密时间和随机环境下的连续决策问题的典型模型。当环境的随机演化只能通过抽样得到时，模型无关的强化学习（RL）是计算最优决策序列的选择算法。强化学习要求将学习目标编码为标量奖励信号。由于手动进行此类翻译既繁琐又容易出错，因此已经提出了许多技术，可以将高级目标（用逻辑或自动机形式表达）转换为标量奖励用于离散时间马尔可夫决策过程（MDP）。然而，目前还不存在用于CTMDP的自动翻译方法。本文考虑了以omega-regular语言表达的学习目标的CTMDP环境。omega-regular语言将正则语言推广到无限时间范围，并且可以表达在流行的线性时变逻辑中给定的属性。",
    "tldr": "本研究针对以omega-regular语言表达的学习目标的连续时间马尔可夫决策过程（CTMDP）环境，探讨了强化学习方法，以便自动将高级目标转换为标量奖励信号。",
    "en_tdlr": "This study focuses on using reinforcement learning to automatically translate high-level objectives expressed as omega-regular languages to scalar reward signals in continuous-time Markov decision processes (CTMDPs)."
}