{
    "title": "Lifting uniform learners via distributional decomposition. (arXiv:2303.16208v1 [stat.ML])",
    "abstract": "We show how any PAC learning algorithm that works under the uniform distribution can be transformed, in a blackbox fashion, into one that works under an arbitrary and unknown distribution $\\mathcal{D}$. The efficiency of our transformation scales with the inherent complexity of $\\mathcal{D}$, running in $\\mathrm{poly}(n, (md)^d)$ time for distributions over $\\{\\pm 1\\}^n$ whose pmfs are computed by depth-$d$ decision trees, where $m$ is the sample complexity of the original algorithm. For monotone distributions our transformation uses only samples from $\\mathcal{D}$, and for general ones it uses subcube conditioning samples.  A key technical ingredient is an algorithm which, given the aforementioned access to $\\mathcal{D}$, produces an optimal decision tree decomposition of $\\mathcal{D}$: an approximation of $\\mathcal{D}$ as a mixture of uniform distributions over disjoint subcubes. With this decomposition in hand, we run the uniform-distribution learner on each subcube and combine the ",
    "link": "http://arxiv.org/abs/2303.16208",
    "context": "Title: Lifting uniform learners via distributional decomposition. (arXiv:2303.16208v1 [stat.ML])\nAbstract: We show how any PAC learning algorithm that works under the uniform distribution can be transformed, in a blackbox fashion, into one that works under an arbitrary and unknown distribution $\\mathcal{D}$. The efficiency of our transformation scales with the inherent complexity of $\\mathcal{D}$, running in $\\mathrm{poly}(n, (md)^d)$ time for distributions over $\\{\\pm 1\\}^n$ whose pmfs are computed by depth-$d$ decision trees, where $m$ is the sample complexity of the original algorithm. For monotone distributions our transformation uses only samples from $\\mathcal{D}$, and for general ones it uses subcube conditioning samples.  A key technical ingredient is an algorithm which, given the aforementioned access to $\\mathcal{D}$, produces an optimal decision tree decomposition of $\\mathcal{D}$: an approximation of $\\mathcal{D}$ as a mixture of uniform distributions over disjoint subcubes. With this decomposition in hand, we run the uniform-distribution learner on each subcube and combine the ",
    "path": "papers/23/03/2303.16208.json",
    "total_tokens": 1001,
    "translated_title": "利用分布分解提高均匀学习算法的性能",
    "translated_abstract": "我们展示了如何将任何在均匀分布下有效的PAC学习算法转换成一个在任意未知分布$\\mathcal{D}$下有效的算法。我们的转换效率随$\\mathcal{D}$的固有复杂性而变化，对于在$\\{\\pm 1\\}^n$上的分布，其pmf由深度为$d$的决策树计算，则时间复杂度为$\\mathrm{poly}(n, (md)^d)$，其中$m$是原始算法的样本复杂度。对于单调分布，我们的转换仅使用$\\mathcal{D}$中的样本，而对于一般分布，我们使用子立方体条件样本。其中一个关键技术是一个算法，它在给出$\\mathcal{D}$的访问权限的情况下，产生了一个最优决策树分解$\\mathcal{D}$：一个逼近了$\\mathcal{D}$的混合均匀分布的分离子立方体。通过这个分解，我们在每个子立方体上运行均匀分布学习器，并将结果合并起来。",
    "tldr": "本文介绍了一种方法，可以将任何在均匀分布下有效的PAC学习算法转换成一个在任意未知分布下有效的算法，而且对于单调分布，只需要用$\\mathcal{D}$中的样本。算法的核心是通过一个算法将$\\mathcal{D}$逼近成由子立方体混合而成的混合均匀分布。"
}