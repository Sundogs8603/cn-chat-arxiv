{
    "title": "Parameter-Efficient Sparse Retrievers and Rerankers using Adapters. (arXiv:2303.13220v1 [cs.IR])",
    "abstract": "Parameter-Efficient transfer learning with Adapters have been studied in Natural Language Processing (NLP) as an alternative to full fine-tuning. Adapters are memory-efficient and scale well with downstream tasks by training small bottle-neck layers added between transformer layers while keeping the large pretrained language model (PLMs) frozen. In spite of showing promising results in NLP, these methods are under-explored in Information Retrieval. While previous studies have only experimented with dense retriever or in a cross lingual retrieval scenario, in this paper we aim to complete the picture on the use of adapters in IR. First, we study adapters for SPLADE, a sparse retriever, for which adapters not only retain the efficiency and effectiveness otherwise achieved by finetuning, but are memory-efficient and orders of magnitude lighter to train. We observe that Adapters-SPLADE not only optimizes just 2\\% of training parameters, but outperforms fully fine-tuned counterpart and exis",
    "link": "http://arxiv.org/abs/2303.13220",
    "context": "Title: Parameter-Efficient Sparse Retrievers and Rerankers using Adapters. (arXiv:2303.13220v1 [cs.IR])\nAbstract: Parameter-Efficient transfer learning with Adapters have been studied in Natural Language Processing (NLP) as an alternative to full fine-tuning. Adapters are memory-efficient and scale well with downstream tasks by training small bottle-neck layers added between transformer layers while keeping the large pretrained language model (PLMs) frozen. In spite of showing promising results in NLP, these methods are under-explored in Information Retrieval. While previous studies have only experimented with dense retriever or in a cross lingual retrieval scenario, in this paper we aim to complete the picture on the use of adapters in IR. First, we study adapters for SPLADE, a sparse retriever, for which adapters not only retain the efficiency and effectiveness otherwise achieved by finetuning, but are memory-efficient and orders of magnitude lighter to train. We observe that Adapters-SPLADE not only optimizes just 2\\% of training parameters, but outperforms fully fine-tuned counterpart and exis",
    "path": "papers/23/03/2303.13220.json",
    "total_tokens": 944,
    "translated_title": "使用适配器的参数高效稀疏检索器和重排器",
    "translated_abstract": "使用适配器的参数高效迁移学习已在自然语言处理（NLP）中研究作为完全微调的替代方法。适配器是内存高效的，并通过在变压器层之间添加小瓶颈层进行训练，同时保持大型预训练语言模型（PLMs）冻结来与下游任务良好地进行缩放。尽管在NLP中表现出有希望的结果，但这些方法在信息检索方面尚未得到充分探索。本文旨在完善适配器在IR中的使用情况。首先，我们研究了适配器对于SPLADE（一种稀疏检索器）的应用，适配器不仅保留了通过完全微调实现的效率和效果，而且内存高效，训练轻量级。我们观察到，适配器-SPLADE仅优化2％的训练参数，但胜过完全微调的对应物以及已有的最佳稀疏检索器。",
    "tldr": "本文研究了在信息检索中使用适配器的效果，特别是在SPLADE这种稀疏检索器上。研究表明，适配器-SPLADE只需优化2%的训练参数，但在效率和效果方面均优于完全微调的方法。",
    "en_tdlr": "This paper explores the use of adapters in information retrieval, specifically on the sparse retriever SPLADE, and finds that Adapters-SPLADE not only optimizes just 2% of training parameters, but outperforms fully fine-tuned counterparts and existing state-of-the-art sparse retrievers."
}