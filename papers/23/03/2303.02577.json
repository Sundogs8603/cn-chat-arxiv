{
    "title": "Effectiveness of Data Augmentation for Parameter Efficient Tuning with Limited Data. (arXiv:2303.02577v2 [cs.CL] UPDATED)",
    "abstract": "Recent work has demonstrated that using parameter efficient tuning techniques such as prefix tuning (or P-tuning) on pretrained language models can yield performance that is comparable or superior to fine-tuning while dramatically reducing trainable parameters. Nevertheless, the effectiveness of such methods under the context of data augmentation, a common strategy to improve learning under low data regimes, has not been fully explored. In this paper, we examine the effectiveness of several popular task-agnostic data augmentation techniques, i.e., EDA, Back Translation, and Mixup, when using two general parameter efficient tuning methods, P-tuning v2 and LoRA, under data scarcity. We show that data augmentation can be used to boost the performance of P-tuning and LoRA models, but the effectiveness of each technique varies and certain methods can lead to a notable degradation in performance, particularly when using larger models and on harder tasks. We further analyze the sentence repre",
    "link": "http://arxiv.org/abs/2303.02577",
    "context": "Title: Effectiveness of Data Augmentation for Parameter Efficient Tuning with Limited Data. (arXiv:2303.02577v2 [cs.CL] UPDATED)\nAbstract: Recent work has demonstrated that using parameter efficient tuning techniques such as prefix tuning (or P-tuning) on pretrained language models can yield performance that is comparable or superior to fine-tuning while dramatically reducing trainable parameters. Nevertheless, the effectiveness of such methods under the context of data augmentation, a common strategy to improve learning under low data regimes, has not been fully explored. In this paper, we examine the effectiveness of several popular task-agnostic data augmentation techniques, i.e., EDA, Back Translation, and Mixup, when using two general parameter efficient tuning methods, P-tuning v2 and LoRA, under data scarcity. We show that data augmentation can be used to boost the performance of P-tuning and LoRA models, but the effectiveness of each technique varies and certain methods can lead to a notable degradation in performance, particularly when using larger models and on harder tasks. We further analyze the sentence repre",
    "path": "papers/23/03/2303.02577.json",
    "total_tokens": 906,
    "translated_title": "数据增强在有限数据下参数高效调整的有效性研究",
    "translated_abstract": "最近的研究表明，使用参数高效调整技术，如预训练语言模型上的前缀调整（或P-tuning），可以在大大减少可训练参数的同时，产生与微调相媲美或更好的性能。然而，在低数据情况下，这种方法在数据增强的背景下的效果尚未得到充分探讨。本文研究了在数据稀缺情况下，使用两种通用参数高效调整方法——P-tuning v2和LoRA时，几种常用的任务无关数据增强技术，即EDA，后翻译和混合，的有效性。我们展示了数据增强可以用于提升P-tuning和LoRA模型的性能，但各种技术的有效性有所不同，并且某些方法可能导致性能明显下降，特别是在使用较大模型和更难的任务时。我们进一步分析了句子的表示方法。",
    "tldr": "本文研究了在有限数据情况下，使用参数高效调整方法时，数据增强的有效性。研究表明，数据增强可以提升某些方法的性能，但效果因技术和任务而异，并且在使用较大模型和更难的任务时可能导致性能下降。",
    "en_tdlr": "This paper explores the effectiveness of data augmentation for parameter efficient tuning with limited data. The study shows that data augmentation can improve performance for certain methods, but the effectiveness varies depending on the technique and task, and may lead to performance degradation when using larger models and harder tasks."
}