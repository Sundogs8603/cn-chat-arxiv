{
    "title": "Heterogeneous-Branch Collaborative Learning for Dialogue Generation. (arXiv:2303.11621v1 [cs.CL])",
    "abstract": "With the development of deep learning, advanced dialogue generation methods usually require a greater amount of computational resources. One promising approach to obtaining a high-performance and lightweight model is knowledge distillation, which relies heavily on the pre-trained powerful teacher. Collaborative learning, also known as online knowledge distillation, is an effective way to conduct one-stage group distillation in the absence of a well-trained large teacher model. However, previous work has a severe branch homogeneity problem due to the same training objective and the independent identical training sets. To alleviate this problem, we consider the dialogue attributes in the training of network branches. Each branch learns the attribute-related features based on the selected subset. Furthermore, we propose a dual group-based knowledge distillation method, consisting of positive distillation and negative distillation, to further diversify the features of different branches in",
    "link": "http://arxiv.org/abs/2303.11621",
    "context": "Title: Heterogeneous-Branch Collaborative Learning for Dialogue Generation. (arXiv:2303.11621v1 [cs.CL])\nAbstract: With the development of deep learning, advanced dialogue generation methods usually require a greater amount of computational resources. One promising approach to obtaining a high-performance and lightweight model is knowledge distillation, which relies heavily on the pre-trained powerful teacher. Collaborative learning, also known as online knowledge distillation, is an effective way to conduct one-stage group distillation in the absence of a well-trained large teacher model. However, previous work has a severe branch homogeneity problem due to the same training objective and the independent identical training sets. To alleviate this problem, we consider the dialogue attributes in the training of network branches. Each branch learns the attribute-related features based on the selected subset. Furthermore, we propose a dual group-based knowledge distillation method, consisting of positive distillation and negative distillation, to further diversify the features of different branches in",
    "path": "papers/23/03/2303.11621.json",
    "total_tokens": 883,
    "translated_title": "异构分支协作学习用于对话生成",
    "translated_abstract": "随着深度学习的发展，高级对话生成方法通常需要更多的计算资源。一种有效的获得高性能和轻量级模型的方法是知识蒸馏，其依赖于预训练的强大教师模型。协作学习，也称为在线知识蒸馏，在缺乏训练良好的大型教师模型的情况下，是进行单阶段群体蒸馏的有效方法。然而，先前的工作由于相同的训练目标和独立相同的训练集存在严重的分支同质性问题。为了缓解这个问题，我们考虑在网络分支的训练中考虑对话属性。每个分支基于所选子集学习与属性相关的特征。此外，我们提出了一个双重基于群体的知识蒸馏方法，包括积极蒸馏和消极蒸馏，进一步使不同分支的特征多样化。",
    "tldr": "本文提出了一种异构分支协作学习模型，用于对话生成。该模型使用协作学习方法而非传统的知识蒸馏方法，在网络分支的训练中考虑到对话属性，使不同分支的特征多样化。",
    "en_tdlr": "This paper proposes a heterogeneous-branch collaborative learning model for dialogue generation, which uses collaborative learning instead of the traditional knowledge distillation method. The model considers the dialogue attributes in the training of network branches to diversify the features of different branches."
}