{
    "title": "PowerPruning: Selecting Weights and Activations for Power-Efficient Neural Network Acceleration. (arXiv:2303.13997v1 [cs.NE])",
    "abstract": "Deep neural networks (DNNs) have been successfully applied in various fields. A major challenge of deploying DNNs, especially on edge devices, is power consumption, due to the large number of multiply-and-accumulate (MAC) operations. To address this challenge, we propose PowerPruning, a novel method to reduce power consumption in digital neural network accelerators by selecting weights that lead to less power consumption in MAC operations. In addition, the timing characteristics of the selected weights together with all activation transitions are evaluated. The weights and activations that lead to small delays are further selected. Consequently, the maximum delay of the sensitized circuit paths in the MAC units is reduced even without modifying MAC units, which thus allows a flexible scaling of supply voltage to reduce power consumption further. Together with retraining, the proposed method can reduce power consumption of DNNs on hardware by up to 78.3% with only a slight accuracy loss",
    "link": "http://arxiv.org/abs/2303.13997",
    "context": "Title: PowerPruning: Selecting Weights and Activations for Power-Efficient Neural Network Acceleration. (arXiv:2303.13997v1 [cs.NE])\nAbstract: Deep neural networks (DNNs) have been successfully applied in various fields. A major challenge of deploying DNNs, especially on edge devices, is power consumption, due to the large number of multiply-and-accumulate (MAC) operations. To address this challenge, we propose PowerPruning, a novel method to reduce power consumption in digital neural network accelerators by selecting weights that lead to less power consumption in MAC operations. In addition, the timing characteristics of the selected weights together with all activation transitions are evaluated. The weights and activations that lead to small delays are further selected. Consequently, the maximum delay of the sensitized circuit paths in the MAC units is reduced even without modifying MAC units, which thus allows a flexible scaling of supply voltage to reduce power consumption further. Together with retraining, the proposed method can reduce power consumption of DNNs on hardware by up to 78.3% with only a slight accuracy loss",
    "path": "papers/23/03/2303.13997.json",
    "total_tokens": 937,
    "translated_title": "PowerPruning: 针对功耗优化的神经网络权重和激活选择方法",
    "translated_abstract": "深度神经网络在多个领域都获得了成功应用。然而，在将这些网络部署到边缘设备上，尤其是功耗问题上仍然是一个关键挑战，其中最主要的因素是大量的乘加（MAC）操作。为解决这个问题，本文提出了PowerPruning方法，通过选择导致MAC操作消耗更少功耗的权重来减少数字神经网络加速器的功耗。在此基础上，对所选权重及其与所有激活转换的时序特征进行评估，挑选出在引起较小延迟的权重和激活。因此，即使不修改MAC单元，MAC单元中敏感电路路径的最大延迟也将被减小，从而允许灵活缩小供电电压以进一步减少功耗。结合重新训练，本方法可以在不显著降低准确度的情况下，将DNN在硬件上的功耗降低高达78.3％。",
    "tldr": "PowerPruning是一种新颖的方法，通过选择能减少神经网络硬件中MAC操作功耗的权重来优化效率。该方法可以将DNN在硬件上的功耗降低高达78.3％，同时没有显著的准确度损失。",
    "en_tdlr": "PowerPruning proposes a method to reduce power consumption in digital neural network accelerators by selecting weights that lead to less power consumption in MAC operations. The method can reduce power consumption of DNNs on hardware by up to 78.3% with only a slight accuracy loss."
}