{
    "title": "Making Vision Transformers Efficient from A Token Sparsification View. (arXiv:2303.08685v1 [cs.CV])",
    "abstract": "The quadratic computational complexity to the number of tokens limits the practical applications of Vision Transformers (ViTs). Several works propose to prune redundant tokens to achieve efficient ViTs. However, these methods generally suffer from (i) dramatic accuracy drops, (ii) application difficulty in the local vision transformer, and (iii) non-general-purpose networks for downstream tasks. In this work, we propose a novel Semantic Token ViT (STViT), for efficient global and local vision transformers, which can also be revised to serve as backbone for downstream tasks. The semantic tokens represent cluster centers, and they are initialized by pooling image tokens in space and recovered by attention, which can adaptively represent global or local semantic information. Due to the cluster properties, a few semantic tokens can attain the same effect as vast image tokens, for both global and local vision transformers. For instance, only 16 semantic tokens on DeiT-(Tiny,Small,Base) can ",
    "link": "http://arxiv.org/abs/2303.08685",
    "context": "Title: Making Vision Transformers Efficient from A Token Sparsification View. (arXiv:2303.08685v1 [cs.CV])\nAbstract: The quadratic computational complexity to the number of tokens limits the practical applications of Vision Transformers (ViTs). Several works propose to prune redundant tokens to achieve efficient ViTs. However, these methods generally suffer from (i) dramatic accuracy drops, (ii) application difficulty in the local vision transformer, and (iii) non-general-purpose networks for downstream tasks. In this work, we propose a novel Semantic Token ViT (STViT), for efficient global and local vision transformers, which can also be revised to serve as backbone for downstream tasks. The semantic tokens represent cluster centers, and they are initialized by pooling image tokens in space and recovered by attention, which can adaptively represent global or local semantic information. Due to the cluster properties, a few semantic tokens can attain the same effect as vast image tokens, for both global and local vision transformers. For instance, only 16 semantic tokens on DeiT-(Tiny,Small,Base) can ",
    "path": "papers/23/03/2303.08685.json",
    "total_tokens": 986,
    "translated_title": "基于令牌稀疏化视角的视觉Transformer优化",
    "translated_abstract": "视觉Transformer (ViTs)的计算复杂度随着令牌数量呈二次增长，限制了其实际应用。为了实现高效的ViT，已有多种方法通过修剪冗余令牌来达到目的。然而，这些方法往往存在以下问题：(i) 显著的精度下降，(ii) 无法应用于本地视觉Transformer中，以及 (iii) 无法通用于下游任务的网络。本文提出了一种新颖的语义令牌ViT（STViT），用于实现全局和本地视觉Transformer的高效性能，并可作为下游任务的主干骨干进行修订。语义令牌代表聚类中心，其通过空间内的图像令牌汇集来初始化，并通过注意组件进行恢复，自适应地表示全局或本地的语义信息。由于其聚类性质，少量的语义令牌即可实现与众多图像令牌相同的效果，适用于全局和本地视觉Transformer。例如，对于DeiT-(Tiny, Small, Base)，仅需16个语义令牌即可达到相同效果。",
    "tldr": "本文提出了一种新的Semantic Token ViT (STViT)方法，实现了全局和本地视觉Transformer的高效性能，同时可用作下游任务的主干骨干。其通过聚类中心的语义令牌代表来代替图像令牌，实现较少的语义令牌即可达到同样的效果。",
    "en_tdlr": "This paper proposes a novel Semantic Token ViT (STViT) method that achieves efficient performance for both global and local vision transformers, while also serving as a backbone for downstream tasks. The method uses semantic tokens to represent cluster centers and replaces image tokens, allowing for the same effect to be achieved with fewer tokens."
}