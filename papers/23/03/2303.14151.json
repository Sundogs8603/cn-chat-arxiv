{
    "title": "Double Descent Demystified: Identifying, Interpreting & Ablating the Sources of a Deep Learning Puzzle. (arXiv:2303.14151v1 [cs.LG])",
    "abstract": "Double descent is a surprising phenomenon in machine learning, in which as the number of model parameters grows relative to the number of data, test error drops as models grow ever larger into the highly overparameterized (data undersampled) regime. This drop in test error flies against classical learning theory on overfitting and has arguably underpinned the success of large models in machine learning. This non-monotonic behavior of test loss depends on the number of data, the dimensionality of the data and the number of model parameters. Here, we briefly describe double descent, then provide an explanation of why double descent occurs in an informal and approachable manner, requiring only familiarity with linear algebra and introductory probability. We provide visual intuition using polynomial regression, then mathematically analyze double descent with ordinary linear regression and identify three interpretable factors that, when simultaneously all present, together create double des",
    "link": "http://arxiv.org/abs/2303.14151",
    "context": "Title: Double Descent Demystified: Identifying, Interpreting & Ablating the Sources of a Deep Learning Puzzle. (arXiv:2303.14151v1 [cs.LG])\nAbstract: Double descent is a surprising phenomenon in machine learning, in which as the number of model parameters grows relative to the number of data, test error drops as models grow ever larger into the highly overparameterized (data undersampled) regime. This drop in test error flies against classical learning theory on overfitting and has arguably underpinned the success of large models in machine learning. This non-monotonic behavior of test loss depends on the number of data, the dimensionality of the data and the number of model parameters. Here, we briefly describe double descent, then provide an explanation of why double descent occurs in an informal and approachable manner, requiring only familiarity with linear algebra and introductory probability. We provide visual intuition using polynomial regression, then mathematically analyze double descent with ordinary linear regression and identify three interpretable factors that, when simultaneously all present, together create double des",
    "path": "papers/23/03/2303.14151.json",
    "total_tokens": 1147,
    "translated_title": "双重下降的谜团：辨识、解释和消解深度学习之谜",
    "translated_abstract": "双重下降是机器学习中一个令人惊讶的现象，即随着模型参数数量相对于数据量的增长，测试误差在模型不断扩大而进入高度超参数化（数据未充分采样）阶段时下降。测试误差下降的这种情况与传统的关于过度拟合的学习理论相悖，可谓承载了机器学习中大型模型的成功。这种非单调的测试误差变化行为取决于数据的数量、数据的维度和模型参数的数量。在本文中，我们简要描述了双重下降现象，然后用易于理解和接受的方式对为何出现双重下降进行了解释，只需要了解线性代数和概率论的知识。我们使用多项式回归提供可视化的直观感受，然后通过普通线性回归数学分析双重下降，确定了三个解释性因素，当同时存在时，可以共同制造双重下降现象。随后，我们展示了这些因素如何被关闭在输出单个标量的简单神经网络中。最后，我们通过控制消融实验测试了这些因素在现代深度神经网络中的重要性，并证明双重下降现象不仅仅是超参数化模型所特有的，也出现在欠参数化模型和插值阈值中。",
    "tldr": "双重下降是机器学习中一个令人惊讶的现象，数据量、数据维度和模型参数是影响双重下降的关键因素。研究者找到了制造双重下降的三个解释性因素，并证明这些因素可以在简单神经网络中关闭。",
    "en_tdlr": "Double descent is a surprising phenomenon in machine learning where test error drops as models grow larger relative to the amount of data, contrary to the overfitting theory. The number of data, data dimensionality, and model parameters are crucial factors affecting double descent. Researchers have identified three interpretable factors that cause double descent and demonstrated that they can be turned off in simple neural networks."
}