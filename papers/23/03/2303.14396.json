{
    "title": "IFSeg: Image-free Semantic Segmentation via Vision-Language Model. (arXiv:2303.14396v1 [cs.CV])",
    "abstract": "Vision-language (VL) pre-training has recently gained much attention for its transferability and flexibility in novel concepts (e.g., cross-modality transfer) across various visual tasks. However, VL-driven segmentation has been under-explored, and the existing approaches still have the burden of acquiring additional training images or even segmentation annotations to adapt a VL model to downstream segmentation tasks. In this paper, we introduce a novel image-free segmentation task where the goal is to perform semantic segmentation given only a set of the target semantic categories, but without any task-specific images and annotations. To tackle this challenging task, our proposed method, coined IFSeg, generates VL-driven artificial image-segmentation pairs and updates a pre-trained VL model to a segmentation task. We construct this artificial training data by creating a 2D map of random semantic categories and another map of their corresponding word tokens. Given that a pre-trained VL",
    "link": "http://arxiv.org/abs/2303.14396",
    "context": "Title: IFSeg: Image-free Semantic Segmentation via Vision-Language Model. (arXiv:2303.14396v1 [cs.CV])\nAbstract: Vision-language (VL) pre-training has recently gained much attention for its transferability and flexibility in novel concepts (e.g., cross-modality transfer) across various visual tasks. However, VL-driven segmentation has been under-explored, and the existing approaches still have the burden of acquiring additional training images or even segmentation annotations to adapt a VL model to downstream segmentation tasks. In this paper, we introduce a novel image-free segmentation task where the goal is to perform semantic segmentation given only a set of the target semantic categories, but without any task-specific images and annotations. To tackle this challenging task, our proposed method, coined IFSeg, generates VL-driven artificial image-segmentation pairs and updates a pre-trained VL model to a segmentation task. We construct this artificial training data by creating a 2D map of random semantic categories and another map of their corresponding word tokens. Given that a pre-trained VL",
    "path": "papers/23/03/2303.14396.json",
    "total_tokens": 1083,
    "translated_title": "IFSeg：基于视觉语言模型的无图像语义分割",
    "translated_abstract": "近年来，视觉语言（VL）预训练因其在不同视觉任务中的可转移性和灵活性（例如跨模态转移）而备受关注。然而，VL驱动的分割任务尚未得到充分探索，并且现有方法仍需要获取额外的训练图像甚至分割注释来适应下游分割任务。本文介绍了一种新颖的无图像分割任务，其目标是在只有一组目标语义类别的情况下执行语义分割，但不使用任何特定于任务的图像和注释。为了解决这一具有挑战性的任务，我们提出了一种新颖的方法IFSeg，通过生成基于VL的人工图像分割对来更新预训练的VL模型以适应分割任务。我们通过创建一个随机语义类别的2D地图以及另一个地图的相应单词标记来构造这些人造训练数据。由于预训练的VL模型可以将语义短语与其视觉表示相关联，因此我们可以使用它生成带有地面真实语义分割掩模的图像。我们的方法在多个基准数据集上实现了最先进的性能，并且证明了对未见过的类别和不同级别噪声的鲁棒性。",
    "tldr": "IFSeg提出了一种全新的无图像分割任务，能够在没有特定图像和注释的情况下执行语义分割，实现了基于视觉语言模型的人工图像分割对更新，取得了在多个基准数据集上的最先进表现，对未知类别和噪声鲁棒性强。",
    "en_tdlr": "IFSeg proposes a novel image-free segmentation task using vision-language model, which achieves state-of-the-art performance on several benchmarks without the need for specific images or annotations. The proposed method generates VL-driven artificial image-segmentation pairs to update a pre-trained VL model, demonstrating robustness to unseen categories and varying levels of noise."
}