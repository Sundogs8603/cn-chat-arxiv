{
    "title": "NESS: Learning Node Embeddings from Static SubGraphs. (arXiv:2303.08958v1 [cs.LG])",
    "abstract": "We present a framework for learning Node Embeddings from Static Subgraphs (NESS) using a graph autoencoder (GAE) in a transductive setting. Moreover, we propose a novel approach for contrastive learning in the same setting. We demonstrate that using static subgraphs during training with a GAE improves node representation for link prediction tasks compared to current autoencoding methods using the entire graph or stochastic subgraphs. NESS consists of two steps: 1) Partitioning the training graph into subgraphs using random edge split (RES) during data pre-processing, and 2) Aggregating the node representations learned from each subgraph to obtain a joint representation of the graph at test time. Our experiments show that NESS improves the performance of a wide range of graph encoders and achieves state-of-the-art (SOTA) results for link prediction on multiple benchmark datasets.",
    "link": "http://arxiv.org/abs/2303.08958",
    "context": "Title: NESS: Learning Node Embeddings from Static SubGraphs. (arXiv:2303.08958v1 [cs.LG])\nAbstract: We present a framework for learning Node Embeddings from Static Subgraphs (NESS) using a graph autoencoder (GAE) in a transductive setting. Moreover, we propose a novel approach for contrastive learning in the same setting. We demonstrate that using static subgraphs during training with a GAE improves node representation for link prediction tasks compared to current autoencoding methods using the entire graph or stochastic subgraphs. NESS consists of two steps: 1) Partitioning the training graph into subgraphs using random edge split (RES) during data pre-processing, and 2) Aggregating the node representations learned from each subgraph to obtain a joint representation of the graph at test time. Our experiments show that NESS improves the performance of a wide range of graph encoders and achieves state-of-the-art (SOTA) results for link prediction on multiple benchmark datasets.",
    "path": "papers/23/03/2303.08958.json",
    "total_tokens": 802,
    "translated_title": "NESS：从静态子图学习节点嵌入",
    "translated_abstract": "我们提出了一个在转导设置下使用图自编码器（GAE）从静态子图（NESS）中学习节点嵌入的框架。此外，我们提出了一种新的对比学习方法。我们证明，与使用整个图或随机子图的当前自编码方法相比，在训练中使用静态子图加上GAE改善了节点表示，用于链接预测任务。NESS包括两个步骤：1）使用随机边缘拆分（RES）将训练图划分为子图，在数据预处理期间，2）聚合从每个子图学习的节点表示，以在测试时间获得图的联合表示。我们的实验表明，NESS改进了广泛的图编码器的性能，并在多个基准数据集上实现了链接预测的最新结果（SOTA）。",
    "tldr": "NESS提出了一种在转导设置下使用图自编码器（GAE）从静态子图（NESS）中学习节点嵌入的新方法，并在多个基准数据集上达到了最新链接预测结果。",
    "en_tdlr": "NESS proposes a novel approach for learning node embeddings from static subgraphs using a graph autoencoder in a transductive setting, and achieves state-of-the-art results for link prediction on multiple benchmark datasets."
}