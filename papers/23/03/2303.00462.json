{
    "title": "Hidden Gems: 4D Radar Scene Flow Learning Using Cross-Modal Supervision. (arXiv:2303.00462v3 [cs.CV] UPDATED)",
    "abstract": "This work proposes a novel approach to 4D radar-based scene flow estimation via cross-modal learning. Our approach is motivated by the co-located sensing redundancy in modern autonomous vehicles. Such redundancy implicitly provides various forms of supervision cues to the radar scene flow estimation. Specifically, we introduce a multi-task model architecture for the identified cross-modal learning problem and propose loss functions to opportunistically engage scene flow estimation using multiple cross-modal constraints for effective model training. Extensive experiments show the state-of-the-art performance of our method and demonstrate the effectiveness of cross-modal supervised learning to infer more accurate 4D radar scene flow. We also show its usefulness to two subtasks - motion segmentation and ego-motion estimation. Our source code will be available on https://github.com/Toytiny/CMFlow.",
    "link": "http://arxiv.org/abs/2303.00462",
    "context": "Title: Hidden Gems: 4D Radar Scene Flow Learning Using Cross-Modal Supervision. (arXiv:2303.00462v3 [cs.CV] UPDATED)\nAbstract: This work proposes a novel approach to 4D radar-based scene flow estimation via cross-modal learning. Our approach is motivated by the co-located sensing redundancy in modern autonomous vehicles. Such redundancy implicitly provides various forms of supervision cues to the radar scene flow estimation. Specifically, we introduce a multi-task model architecture for the identified cross-modal learning problem and propose loss functions to opportunistically engage scene flow estimation using multiple cross-modal constraints for effective model training. Extensive experiments show the state-of-the-art performance of our method and demonstrate the effectiveness of cross-modal supervised learning to infer more accurate 4D radar scene flow. We also show its usefulness to two subtasks - motion segmentation and ego-motion estimation. Our source code will be available on https://github.com/Toytiny/CMFlow.",
    "path": "papers/23/03/2303.00462.json",
    "total_tokens": 829,
    "translated_title": "隐藏的宝石：使用跨模态监督的4D雷达场景流学习",
    "translated_abstract": "本文提出一种通过跨模态学习进行4D雷达基础场景流量估计的新方法。我们的方法受到现代自动驾驶车辆中同一位置的传感器冗余的启发。这种冗余隐含地为雷达场景流估计提供了各种形式的监督线索。具体来说，我们提出了一个多任务模型，针对已确定的跨模态学习问题，提出了损失函数，以使用多个跨模态约束机会有效地进行场景流量估计进行模型训练。广泛的实验显示了我们方法的最先进性能，并证明了跨模态监督学习用于推断更准确的4D雷达场景流的有效性。我们还展示了它对两个子任务-运动分割和自我运动估计的有用性。我们的源代码将在https://github.com/Toytiny/CMFlow上提供。",
    "tldr": "本研究提出了一种使用跨模态监督学习的新方法，用于精确地估计4D雷达场景流，并在运动分割和自我运动估计等子任务中显示了其实用性。",
    "en_tdlr": "This paper proposes a novel method using cross-modal supervision for accurate 4D radar scene flow estimation and demonstrates its usefulness in subtasks such as motion segmentation and ego-motion estimation."
}