{
    "title": "Denoising Autoencoder-based Defensive Distillation as an Adversarial Robustness Algorithm. (arXiv:2303.15901v1 [cs.LG])",
    "abstract": "Adversarial attacks significantly threaten the robustness of deep neural networks (DNNs). Despite the multiple defensive methods employed, they are nevertheless vulnerable to poison attacks, where attackers meddle with the initial training data. In order to defend DNNs against such adversarial attacks, this work proposes a novel method that combines the defensive distillation mechanism with a denoising autoencoder (DAE). This technique tries to lower the sensitivity of the distilled model to poison attacks by spotting and reconstructing poisonous adversarial inputs in the training data. We added carefully created adversarial samples to the initial training data to assess the proposed method's performance. Our experimental findings demonstrate that our method successfully identified and reconstructed the poisonous inputs while also considering enhancing the DNN's resilience. The proposed approach provides a potent and robust defense mechanism for DNNs in various applications where data ",
    "link": "http://arxiv.org/abs/2303.15901",
    "context": "Title: Denoising Autoencoder-based Defensive Distillation as an Adversarial Robustness Algorithm. (arXiv:2303.15901v1 [cs.LG])\nAbstract: Adversarial attacks significantly threaten the robustness of deep neural networks (DNNs). Despite the multiple defensive methods employed, they are nevertheless vulnerable to poison attacks, where attackers meddle with the initial training data. In order to defend DNNs against such adversarial attacks, this work proposes a novel method that combines the defensive distillation mechanism with a denoising autoencoder (DAE). This technique tries to lower the sensitivity of the distilled model to poison attacks by spotting and reconstructing poisonous adversarial inputs in the training data. We added carefully created adversarial samples to the initial training data to assess the proposed method's performance. Our experimental findings demonstrate that our method successfully identified and reconstructed the poisonous inputs while also considering enhancing the DNN's resilience. The proposed approach provides a potent and robust defense mechanism for DNNs in various applications where data ",
    "path": "papers/23/03/2303.15901.json",
    "total_tokens": 905,
    "translated_title": "基于去噪自编码器的防御蒸馏作为对抗鲁棒性算法",
    "translated_abstract": "对抗攻击对深度神经网络(DNNs)的鲁棒性造成了极大威胁。尽管使用了多种防御方法，但它们仍然容易受到攻击者篡改的初始训练数据。为了防御这种对抗攻击，本研究提出一种新的方法，将防御蒸馏机制与去噪自编码器(DAE)相结合。该技术旨在通过检测和重构训练数据中有害的对抗输入来降低蒸馏模型对有毒攻击的敏感度。我们在初始训练数据中添加了精心创建的对抗样本以评估所提出方法的性能。实验证明，我们的方法成功识别和重构了有毒的输入，同时也考虑了增强DNN的鲁棒性。所提出的方法为在数据隐私保护等各种应用中提供了一种强大且鲁棒的DNN防御机制。",
    "tldr": "本论文提出了一种新的对抗攻击防御方法，通过将防御蒸馏机制与去噪自编码器相结合，来降低模型对有毒攻击的敏感度。",
    "en_tdlr": "This paper proposes a novel defense mechanism against adversarial attacks by combining defensive distillation mechanism with a denoising autoencoder, aiming to decrease the sensitivity of the distilled model to poison attacks by detecting and reconstructing poisonous adversarial inputs in the training data."
}