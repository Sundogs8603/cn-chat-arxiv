{
    "title": "Exploring Continual Learning of Diffusion Models. (arXiv:2303.15342v1 [cs.LG])",
    "abstract": "Diffusion models have achieved remarkable success in generating high-quality images thanks to their novel training procedures applied to unprecedented amounts of data. However, training a diffusion model from scratch is computationally expensive. This highlights the need to investigate the possibility of training these models iteratively, reusing computation while the data distribution changes. In this study, we take the first step in this direction and evaluate the continual learning (CL) properties of diffusion models. We begin by benchmarking the most common CL methods applied to Denoising Diffusion Probabilistic Models (DDPMs), where we note the strong performance of the experience replay with the reduced rehearsal coefficient. Furthermore, we provide insights into the dynamics of forgetting, which exhibit diverse behavior across diffusion timesteps. We also uncover certain pitfalls of using the bits-per-dimension metric for evaluating CL.",
    "link": "http://arxiv.org/abs/2303.15342",
    "context": "Title: Exploring Continual Learning of Diffusion Models. (arXiv:2303.15342v1 [cs.LG])\nAbstract: Diffusion models have achieved remarkable success in generating high-quality images thanks to their novel training procedures applied to unprecedented amounts of data. However, training a diffusion model from scratch is computationally expensive. This highlights the need to investigate the possibility of training these models iteratively, reusing computation while the data distribution changes. In this study, we take the first step in this direction and evaluate the continual learning (CL) properties of diffusion models. We begin by benchmarking the most common CL methods applied to Denoising Diffusion Probabilistic Models (DDPMs), where we note the strong performance of the experience replay with the reduced rehearsal coefficient. Furthermore, we provide insights into the dynamics of forgetting, which exhibit diverse behavior across diffusion timesteps. We also uncover certain pitfalls of using the bits-per-dimension metric for evaluating CL.",
    "path": "papers/23/03/2303.15342.json",
    "total_tokens": 868,
    "translated_title": "探索扩散模型的持续学习",
    "translated_abstract": "由于其新颖的训练程序应用于大量数据，扩散模型在生成高质量图像方面取得了显著的成功。然而，从头开始训练扩散模型是计算密集型的。这突出了需要研究在数据分布发生变化时重新使用计算的迭代训练可能性。在此研究中，我们迈出了这一方向的第一步，并评估了扩散模型的持续学习（CL）属性。我们首先对应用于去噪扩散概率模型（DDPM）的最常见CL方法进行基准测试，其中我们注意到经验重放与减少排练系数的性能强大。此外，我们提供了对遗忘动态的见解，它们表现出扩散时间步长的不同行为。我们还揭示了使用每维比特数评估CL的某些缺陷。",
    "tldr": "扩散模型需要进行计算密集型的从头开始训练，因此探索在数据分布发生变化时重新使用计算的迭代训练是有必要的。经验重放与减少排练系数的性能强大。使用每维比特数评估CL存在某些缺陷。",
    "en_tdlr": "It's computationally expensive to train diffusion models from scratch. Therefore, it's necessary to explore iterative training by reusing computation when the data distribution changes. Experience replay with reduced rehearsal coefficient shows strong performance. There are certain pitfalls of using the bits-per-dimension metric for evaluating CL."
}