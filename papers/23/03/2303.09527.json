{
    "title": "Fairness-aware Differentially Private Collaborative Filtering. (arXiv:2303.09527v1 [cs.IR])",
    "abstract": "Recently, there has been an increasing adoption of differential privacy guided algorithms for privacy-preserving machine learning tasks. However, the use of such algorithms comes with trade-offs in terms of algorithmic fairness, which has been widely acknowledged. Specifically, we have empirically observed that the classical collaborative filtering method, trained by differentially private stochastic gradient descent (DP-SGD), results in a disparate impact on user groups with respect to different user engagement levels. This, in turn, causes the original unfair model to become even more biased against inactive users. To address the above issues, we propose \\textbf{DP-Fair}, a two-stage framework for collaborative filtering based algorithms. Specifically, it combines differential privacy mechanisms with fairness constraints to protect user privacy while ensuring fair recommendations. The experimental results, based on Amazon datasets, and user history logs collected from Etsy, one of th",
    "link": "http://arxiv.org/abs/2303.09527",
    "context": "Title: Fairness-aware Differentially Private Collaborative Filtering. (arXiv:2303.09527v1 [cs.IR])\nAbstract: Recently, there has been an increasing adoption of differential privacy guided algorithms for privacy-preserving machine learning tasks. However, the use of such algorithms comes with trade-offs in terms of algorithmic fairness, which has been widely acknowledged. Specifically, we have empirically observed that the classical collaborative filtering method, trained by differentially private stochastic gradient descent (DP-SGD), results in a disparate impact on user groups with respect to different user engagement levels. This, in turn, causes the original unfair model to become even more biased against inactive users. To address the above issues, we propose \\textbf{DP-Fair}, a two-stage framework for collaborative filtering based algorithms. Specifically, it combines differential privacy mechanisms with fairness constraints to protect user privacy while ensuring fair recommendations. The experimental results, based on Amazon datasets, and user history logs collected from Etsy, one of th",
    "path": "papers/23/03/2303.09527.json",
    "total_tokens": 743,
    "translated_title": "公平感知的差分隐私协同过滤",
    "translated_abstract": "最近，越来越多的隐私保护机器学习任务采用差分隐私引导算法，然而，这样的算法使用在算法公平性方面有折衷，这一点被广泛认可。本文针对差分隐私随机梯度下降（DP-SGD）训练的经典协同过滤方法导致用户群体与不同用户参与水平之间存在不公平影响的问题，提出了一个两阶段框架DP-Fair，它将差分隐私机制与公平限制相结合，从而在保护用户隐私的同时确保公平推荐。",
    "tldr": "本文提出了DP-Fair，一个两阶段的协同过滤算法框架，它结合了差分隐私机制和公平约束，旨在保护用户隐私、确保公平推荐。",
    "en_tdlr": "This paper proposes a two-stage collaborative filtering framework DP-Fair, which combines differential privacy mechanisms with fairness constraints to protect user privacy and ensure fair recommendations."
}