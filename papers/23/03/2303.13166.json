{
    "title": "Take 5: Interpretable Image Classification with a Handful of Features. (arXiv:2303.13166v1 [cs.CV])",
    "abstract": "Deep Neural Networks use thousands of mostly incomprehensible features to identify a single class, a decision no human can follow. We propose an interpretable sparse and low dimensional final decision layer in a deep neural network with measurable aspects of interpretability and demonstrate it on fine-grained image classification. We argue that a human can only understand the decision of a machine learning model, if the features are interpretable and only very few of them are used for a single decision. For that matter, the final layer has to be sparse and, to make interpreting the features feasible, low dimensional. We call a model with a Sparse Low-Dimensional Decision SLDD-Model. We show that a SLDD-Model is easier to interpret locally and globally than a dense high-dimensional decision layer while being able to maintain competitive accuracy. Additionally, we propose a loss function that improves a model's feature diversity and accuracy. Our more interpretable SLDD-Model only uses 5",
    "link": "http://arxiv.org/abs/2303.13166",
    "context": "Title: Take 5: Interpretable Image Classification with a Handful of Features. (arXiv:2303.13166v1 [cs.CV])\nAbstract: Deep Neural Networks use thousands of mostly incomprehensible features to identify a single class, a decision no human can follow. We propose an interpretable sparse and low dimensional final decision layer in a deep neural network with measurable aspects of interpretability and demonstrate it on fine-grained image classification. We argue that a human can only understand the decision of a machine learning model, if the features are interpretable and only very few of them are used for a single decision. For that matter, the final layer has to be sparse and, to make interpreting the features feasible, low dimensional. We call a model with a Sparse Low-Dimensional Decision SLDD-Model. We show that a SLDD-Model is easier to interpret locally and globally than a dense high-dimensional decision layer while being able to maintain competitive accuracy. Additionally, we propose a loss function that improves a model's feature diversity and accuracy. Our more interpretable SLDD-Model only uses 5",
    "path": "papers/23/03/2303.13166.json",
    "total_tokens": 1133,
    "translated_title": "使用少量特征实现可解释的图像分类",
    "translated_abstract": "深度神经网络使用数千个大多不可理解的特征来识别单个类别，这是任何人都无法理解的决策。我们提出了在深度神经网络中采用可解释的稀疏低维决策层，能够量化可解释性，具有可解释性，并在细颗粒度图像分类中进行了演示。我们认为，只有当特征是可解释的，并且只有极少量的特征用于单个决策时，人才能理解机器学习模型的决策。为此，最后一层必须是稀疏且维数较低的。我们将具有稀疏低维决策的模型称为Sparse Low-Dimensional Decision（SLDD）模型。我们展示了，相比密集高维决策层，SLDD模型在本地和全局上更容易解释，并能够保持竞争性准确性。此外，我们提出了一种可以提高模型特征多样性和准确性的损失函数。",
    "tldr": "人们常常无法理解深度学习模型的决策过程，我们提出了一种Sparse Low-Dimensional Decision模型，它只使用一小部分可解释的特征进行决策，这使得该模型更容易被人理解，同时也具有与其他密集高维模型相似的准确性。",
    "en_tdlr": "Understanding the decision making process of deep learning models can be challenging. In this study, we propose a Sparse Low-Dimensional Decision (SLDD) model that uses only a small number of interpretable features for decision making, making it more easily understandable to humans while maintaining competitive accuracy compared to dense high-dimensional models. Our proposed model also includes a loss function that improves feature diversity and accuracy."
}