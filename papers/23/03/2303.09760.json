{
    "title": "Diffusing the Optimal Topology: A Generative Optimization Approach. (arXiv:2303.09760v1 [cs.LG])",
    "abstract": "Topology Optimization seeks to find the best design that satisfies a set of constraints while maximizing system performance. Traditional iterative optimization methods like SIMP can be computationally expensive and get stuck in local minima, limiting their applicability to complex or large-scale problems. Learning-based approaches have been developed to accelerate the topology optimization process, but these methods can generate designs with floating material and low performance when challenged with out-of-distribution constraint configurations. Recently, deep generative models, such as Generative Adversarial Networks and Diffusion Models, conditioned on constraints and physics fields have shown promise, but they require extensive pre-processing and surrogate models for improving performance. To address these issues, we propose a Generative Optimization method that integrates classic optimization like SIMP as a refining mechanism for the topology generated by a deep generative model. W",
    "link": "http://arxiv.org/abs/2303.09760",
    "context": "Title: Diffusing the Optimal Topology: A Generative Optimization Approach. (arXiv:2303.09760v1 [cs.LG])\nAbstract: Topology Optimization seeks to find the best design that satisfies a set of constraints while maximizing system performance. Traditional iterative optimization methods like SIMP can be computationally expensive and get stuck in local minima, limiting their applicability to complex or large-scale problems. Learning-based approaches have been developed to accelerate the topology optimization process, but these methods can generate designs with floating material and low performance when challenged with out-of-distribution constraint configurations. Recently, deep generative models, such as Generative Adversarial Networks and Diffusion Models, conditioned on constraints and physics fields have shown promise, but they require extensive pre-processing and surrogate models for improving performance. To address these issues, we propose a Generative Optimization method that integrates classic optimization like SIMP as a refining mechanism for the topology generated by a deep generative model. W",
    "path": "papers/23/03/2303.09760.json",
    "total_tokens": 926,
    "translated_title": "扩散最优拓扑结构：一种生成式优化方法",
    "translated_abstract": "拓扑优化旨在寻找在满足一系列约束条件的同时最大化系统性能的最佳设计。传统的迭代优化方法如SIMP可能计算成本高且陷入局部最小值，限制了它们在复杂或大规模问题中的适用性。已经开发了基于学习的方法来加速拓扑优化过程，但是当面临超出分布约束配置时，这些方法可能会生成具有浮动材料和低性能的设计。最近，基于条件约束和物理场的深度生成模型，如生成对抗网络和扩散模型，显示出很好的效果，但它们需要广泛的预处理和代理模型来改善性能。为解决这些问题，我们提出了一种生成式优化方法，将像SIMP这样的经典优化作为拓扑结构的精制机制整合到深度生成模型生成的拓扑结构中。我们还将在基准问题上展示其与传统拓扑优化方法和其他基于学习的模型相比的优良性能。",
    "tldr": "该论文提出了一种新的生成式优化方法，将SIMP等经典优化算法作为精制机制整合到深度生成模型生成的拓扑结构中。该方法在基准问题上展现出较传统拓扑优化和其他学习模型更优的性能。",
    "en_tdlr": "The paper proposes a new generative optimization method that integrates classic optimization algorithms like SIMP into the topology generated by deep generative models. It demonstrates superior performance compared to traditional topology optimization methods and other learning-based models on benchmark problems."
}