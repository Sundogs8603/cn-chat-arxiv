{
    "title": "Distribution-restrained Softmax Loss for the Model Robustness. (arXiv:2303.12363v1 [cs.LG])",
    "abstract": "Recently, the robustness of deep learning models has received widespread attention, and various methods for improving model robustness have been proposed, including adversarial training, model architecture modification, design of loss functions, certified defenses, and so on. However, the principle of the robustness to attacks is still not fully understood, also the related research is still not sufficient. Here, we have identified a significant factor that affects the robustness of models: the distribution characteristics of softmax values for non-real label samples. We found that the results after an attack are highly correlated with the distribution characteristics, and thus we proposed a loss function to suppress the distribution diversity of softmax. A large number of experiments have shown that our method can improve robustness without significant time consumption.",
    "link": "http://arxiv.org/abs/2303.12363",
    "context": "Title: Distribution-restrained Softmax Loss for the Model Robustness. (arXiv:2303.12363v1 [cs.LG])\nAbstract: Recently, the robustness of deep learning models has received widespread attention, and various methods for improving model robustness have been proposed, including adversarial training, model architecture modification, design of loss functions, certified defenses, and so on. However, the principle of the robustness to attacks is still not fully understood, also the related research is still not sufficient. Here, we have identified a significant factor that affects the robustness of models: the distribution characteristics of softmax values for non-real label samples. We found that the results after an attack are highly correlated with the distribution characteristics, and thus we proposed a loss function to suppress the distribution diversity of softmax. A large number of experiments have shown that our method can improve robustness without significant time consumption.",
    "path": "papers/23/03/2303.12363.json",
    "total_tokens": 795,
    "translated_title": "分布约束的softmax损失函数用于提高模型鲁棒性",
    "translated_abstract": "深度学习模型的鲁棒性近来引起了广泛关注，提出了许多提高模型鲁棒性的方法，包括对抗训练、模型架构修改、损失函数的设计、认证防御等。然而，鲁棒性原则仍未被充分理解，相关研究尚不足够。本文发现影响模型鲁棒性的一个显著因素是softmax对于非真实标签样本的值的分布特征。我们发现攻击后的结果与分布特征高度相关，因此我们提出了一种能够抑制softmax分布多样性的损失函数。大量实验证明了我们的方法能够提高模型的鲁棒性而不会显著增加时间消耗。",
    "tldr": "本文发现影响深度学习模型鲁棒性的因素是softmax对于非真实标签样本的值的分布特征，提出了一种分布约束的softmax损失函数以提高模型鲁棒性。"
}