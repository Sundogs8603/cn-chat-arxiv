{
    "title": "Practical self-supervised continual learning with continual fine-tuning. (arXiv:2303.17235v1 [cs.LG])",
    "abstract": "Self-supervised learning (SSL) has shown remarkable performance in computer vision tasks when trained offline. However, in a Continual Learning (CL) scenario where new data is introduced progressively, models still suffer from catastrophic forgetting. Retraining a model from scratch to adapt to newly generated data is time-consuming and inefficient. Previous approaches suggested re-purposing self-supervised objectives with knowledge distillation to mitigate forgetting across tasks, assuming that labels from all tasks are available during fine-tuning. In this paper, we generalize self-supervised continual learning in a practical setting where available labels can be leveraged in any step of the SSL process. With an increasing number of continual tasks, this offers more flexibility in the pre-training and fine-tuning phases. With Kaizen, we introduce a training architecture that is able to mitigate catastrophic forgetting for both the feature extractor and classifier with a carefully des",
    "link": "http://arxiv.org/abs/2303.17235",
    "context": "Title: Practical self-supervised continual learning with continual fine-tuning. (arXiv:2303.17235v1 [cs.LG])\nAbstract: Self-supervised learning (SSL) has shown remarkable performance in computer vision tasks when trained offline. However, in a Continual Learning (CL) scenario where new data is introduced progressively, models still suffer from catastrophic forgetting. Retraining a model from scratch to adapt to newly generated data is time-consuming and inefficient. Previous approaches suggested re-purposing self-supervised objectives with knowledge distillation to mitigate forgetting across tasks, assuming that labels from all tasks are available during fine-tuning. In this paper, we generalize self-supervised continual learning in a practical setting where available labels can be leveraged in any step of the SSL process. With an increasing number of continual tasks, this offers more flexibility in the pre-training and fine-tuning phases. With Kaizen, we introduce a training architecture that is able to mitigate catastrophic forgetting for both the feature extractor and classifier with a carefully des",
    "path": "papers/23/03/2303.17235.json",
    "total_tokens": 678,
    "translated_title": "实用的自监督连续学习方法：连续微调",
    "translated_abstract": "自监督学习在计算机视觉任务中表现出了出色的性能。而在连续学习情景中，模型仍然存在灾难性遗忘的问题。本文提出了一种能够使用任何步骤中的可用标签的方法，该方法能够在实际情况下泛化自监督连续学习，并且能够提供更多的灵活性。",
    "tldr": "本文提出了一种实用的自监督连续学习方法，使用可用的标签来泛化自监督学习，并通过连续微调来减轻灾难性遗忘。",
    "en_tdlr": "This paper proposes a practical self-supervised continual learning method that uses available labels to generalize self-supervised learning, and alleviates catastrophic forgetting through continual fine-tuning."
}