{
    "title": "Adversarial Attacks against Binary Similarity Systems. (arXiv:2303.11143v2 [cs.CR] UPDATED)",
    "abstract": "In recent years, binary analysis gained traction as a fundamental approach to inspect software and guarantee its security. Due to the exponential increase of devices running software, much research is now moving towards new autonomous solutions based on deep learning models, as they have been showing state-of-the-art performances in solving binary analysis problems. One of the hot topics in this context is binary similarity, which consists in determining if two functions in assembly code are compiled from the same source code. However, it is unclear how deep learning models for binary similarity behave in an adversarial context. In this paper, we study the resilience of binary similarity models against adversarial examples, showing that they are susceptible to both targeted and untargeted attacks (w.r.t. similarity goals) performed by black-box and white-box attackers. In more detail, we extensively test three current state-of-the-art solutions for binary similarity against two black-b",
    "link": "http://arxiv.org/abs/2303.11143",
    "context": "Title: Adversarial Attacks against Binary Similarity Systems. (arXiv:2303.11143v2 [cs.CR] UPDATED)\nAbstract: In recent years, binary analysis gained traction as a fundamental approach to inspect software and guarantee its security. Due to the exponential increase of devices running software, much research is now moving towards new autonomous solutions based on deep learning models, as they have been showing state-of-the-art performances in solving binary analysis problems. One of the hot topics in this context is binary similarity, which consists in determining if two functions in assembly code are compiled from the same source code. However, it is unclear how deep learning models for binary similarity behave in an adversarial context. In this paper, we study the resilience of binary similarity models against adversarial examples, showing that they are susceptible to both targeted and untargeted attacks (w.r.t. similarity goals) performed by black-box and white-box attackers. In more detail, we extensively test three current state-of-the-art solutions for binary similarity against two black-b",
    "path": "papers/23/03/2303.11143.json",
    "total_tokens": 888,
    "translated_title": "对抗二进制相似性系统的攻击",
    "translated_abstract": "最近几年，二进制分析作为一种检查软件并确保其安全性的基本方法得到了关注。由于运行软件的设备数量指数增长，许多研究现在正转向基于深度学习模型的新型自主解决方案，因为它们在解决二进制分析问题时表现出了最先进的性能。在这个背景下，二进制相似性是一个热门话题，它涉及确定汇编代码中的两个函数是否来自相同的源代码。然而，目前还不清楚深度学习模型在对抗性环境中对于二进制相似性的行为如何。本文研究了二进制相似性模型对抗对抗性示例的韧性，显示它们容易受到黑盒和白盒攻击者对于相似性目标的有针对性和无针对性攻击。具体来说，我们详细测试了三种当前最先进的二进制相似性解决方案对两种黑盒",
    "tldr": "本文研究了二进制相似性模型在对抗性环境中的韧性，表明它们容易受到有针对性和无针对性攻击，并向黑盒和白盒攻击者展示了这种易受攻击的情况。"
}