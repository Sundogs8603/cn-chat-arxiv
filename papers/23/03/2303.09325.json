{
    "title": "Can Generative Pre-trained Transformers (GPT) Pass Assessments in Higher Education Programming Courses?. (arXiv:2303.09325v1 [cs.AI])",
    "abstract": "We evaluated the capability of generative pre-trained transformers (GPT), to pass assessments in introductory and intermediate Python programming courses at the postsecondary level. Discussions of potential uses (e.g., exercise generation, code explanation) and misuses (e.g., cheating) of this emerging technology in programming education have intensified, but to date there has not been a rigorous analysis of the models' capabilities in the realistic context of a full-fledged programming course with diverse set of assessment instruments. We evaluated GPT on three Python courses that employ assessments ranging from simple multiple-choice questions (no code involved) to complex programming projects with code bases distributed into multiple files (599 exercises overall). Further, we studied if and how successfully GPT models leverage feedback provided by an auto-grader. We found that the current models are not capable of passing the full spectrum of assessments typically involved in a Pyth",
    "link": "http://arxiv.org/abs/2303.09325",
    "context": "Title: Can Generative Pre-trained Transformers (GPT) Pass Assessments in Higher Education Programming Courses?. (arXiv:2303.09325v1 [cs.AI])\nAbstract: We evaluated the capability of generative pre-trained transformers (GPT), to pass assessments in introductory and intermediate Python programming courses at the postsecondary level. Discussions of potential uses (e.g., exercise generation, code explanation) and misuses (e.g., cheating) of this emerging technology in programming education have intensified, but to date there has not been a rigorous analysis of the models' capabilities in the realistic context of a full-fledged programming course with diverse set of assessment instruments. We evaluated GPT on three Python courses that employ assessments ranging from simple multiple-choice questions (no code involved) to complex programming projects with code bases distributed into multiple files (599 exercises overall). Further, we studied if and how successfully GPT models leverage feedback provided by an auto-grader. We found that the current models are not capable of passing the full spectrum of assessments typically involved in a Pyth",
    "path": "papers/23/03/2303.09325.json",
    "total_tokens": 1148,
    "translated_title": "GPT是否能通过高等教育编程课程的评估？",
    "translated_abstract": "我们评估了生成式预训练变压器（GPT）在高等教育Python编程课程的初级和中级评估中的能力。人们对这种新兴技术在编程教育方面的潜在用途（例如，练习生成，代码解释）和不良用途（例如，作弊）的讨论已经加 intens 了，但到目前为止，该模型在具有多种评估工具的广泛编程课程的现实环境中的能力还没有得到严格分析。我们在使用评估从简单的多项选择题（不涉及代码）到代码分布在多个文件中的复杂编程项目的三个Python课程上评估了GPT（总共599个练习题）。此外，我们研究了GPT模型如何成功地利用自动评分器提供的反馈。我们发现，当前的模型不能通过在高等教育Python编程课程中通常涉及的完整评估工具的全谱。虽然GPT模型可以成功地生成简单练习的语法正确代码，但对于更复杂的练习，他们遇到了困难，并且无法生成满足多文件编程项目要求的代码。此外，GPT模型显示了有限的利用自动评分器提供的反馈的能力。我们的研究结果表明GPT模型可能在编程教育中应用受到限制，并强调需要在这个领域进行更多的研究和开发。",
    "tldr": "该研究评估了生成式预训练变压器（GPT）在高等教育Python编程课程的初级和中级评估中的能力，并发现其在解决复杂的编程问题上遇到困难，限制了其在编程教育中的应用。"
}