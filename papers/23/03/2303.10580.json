{
    "title": "Hierarchical Personalized Federated Learning Over Massive Mobile Edge Computing Networks. (arXiv:2303.10580v1 [cs.LG])",
    "abstract": "Personalized Federated Learning (PFL) is a new Federated Learning (FL) paradigm, particularly tackling the heterogeneity issues brought by various mobile user equipments (UEs) in mobile edge computing (MEC) networks. However, due to the ever-increasing number of UEs and the complicated administrative work it brings, it is desirable to switch the PFL algorithm from its conventional two-layer framework to a multiple-layer one. In this paper, we propose hierarchical PFL (HPFL), an algorithm for deploying PFL over massive MEC networks. The UEs in HPFL are divided into multiple clusters, and the UEs in each cluster forward their local updates to the edge server (ES) synchronously for edge model aggregation, while the ESs forward their edge models to the cloud server semi-asynchronously for global model aggregation. The above training manner leads to a tradeoff between the training loss in each round and the round latency. HPFL combines the objectives of training loss minimization and round ",
    "link": "http://arxiv.org/abs/2303.10580",
    "context": "Title: Hierarchical Personalized Federated Learning Over Massive Mobile Edge Computing Networks. (arXiv:2303.10580v1 [cs.LG])\nAbstract: Personalized Federated Learning (PFL) is a new Federated Learning (FL) paradigm, particularly tackling the heterogeneity issues brought by various mobile user equipments (UEs) in mobile edge computing (MEC) networks. However, due to the ever-increasing number of UEs and the complicated administrative work it brings, it is desirable to switch the PFL algorithm from its conventional two-layer framework to a multiple-layer one. In this paper, we propose hierarchical PFL (HPFL), an algorithm for deploying PFL over massive MEC networks. The UEs in HPFL are divided into multiple clusters, and the UEs in each cluster forward their local updates to the edge server (ES) synchronously for edge model aggregation, while the ESs forward their edge models to the cloud server semi-asynchronously for global model aggregation. The above training manner leads to a tradeoff between the training loss in each round and the round latency. HPFL combines the objectives of training loss minimization and round ",
    "path": "papers/23/03/2303.10580.json",
    "total_tokens": 1004,
    "translated_title": "巨型移动边缘计算网络中的分层个性化联邦学习",
    "translated_abstract": "个性化联邦学习（PFL）是一种新的联邦学习范 paradigm，特别是针对移动边缘计算（MEC）网络中各种移动用户设备（UE）带来的异构问题。但是，由于UE数量不断增加以及带来的复杂行政工作，将PFL算法从其传统的双层框架切换到多层框架是可行的。本文提出了分层个性化联邦学习（HPFL），一种用于在巨型MEC网络中部署PFL的算法。在HPFL中，UE被划分为多个集群，每个集群中的UE同步将其本地更新转发到边缘服务器（ES）进行边缘模型聚合，而ES半异步地将其边缘模型转发到云服务器进行全局模型聚合。上述训练方式在训练损失和轮延迟之间达到了一个权衡。HPFL以分层方式结合了训练损失最小化和轮延迟降低的目标，从而在培训效率和通信开销方面比传统的PFL算法实现了更好的性能。",
    "tldr": "本篇论文提出了一种分层个性化联邦学习算法，能够在巨型移动边缘计算网络中处理移动用户设备的异构性，并且实现了训练损失最小化和轮延迟降低的目标权衡。",
    "en_tdlr": "This paper proposes a hierarchical personalized federated learning algorithm, which can handle the heterogeneity of mobile user devices in massive mobile edge computing networks, and achieves a tradeoff between training loss minimization and round latency reduction."
}