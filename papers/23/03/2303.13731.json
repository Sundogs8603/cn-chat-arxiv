{
    "title": "How Does Attention Work in Vision Transformers? A Visual Analytics Attempt. (arXiv:2303.13731v1 [cs.LG])",
    "abstract": "Vision transformer (ViT) expands the success of transformer models from sequential data to images. The model decomposes an image into many smaller patches and arranges them into a sequence. Multi-head self-attentions are then applied to the sequence to learn the attention between patches. Despite many successful interpretations of transformers on sequential data, little effort has been devoted to the interpretation of ViTs, and many questions remain unanswered. For example, among the numerous attention heads, which one is more important? How strong are individual patches attending to their spatial neighbors in different heads? What attention patterns have individual heads learned? In this work, we answer these questions through a visual analytics approach. Specifically, we first identify what heads are more important in ViTs by introducing multiple pruning-based metrics. Then, we profile the spatial distribution of attention strengths between patches inside individual heads, as well as",
    "link": "http://arxiv.org/abs/2303.13731",
    "context": "Title: How Does Attention Work in Vision Transformers? A Visual Analytics Attempt. (arXiv:2303.13731v1 [cs.LG])\nAbstract: Vision transformer (ViT) expands the success of transformer models from sequential data to images. The model decomposes an image into many smaller patches and arranges them into a sequence. Multi-head self-attentions are then applied to the sequence to learn the attention between patches. Despite many successful interpretations of transformers on sequential data, little effort has been devoted to the interpretation of ViTs, and many questions remain unanswered. For example, among the numerous attention heads, which one is more important? How strong are individual patches attending to their spatial neighbors in different heads? What attention patterns have individual heads learned? In this work, we answer these questions through a visual analytics approach. Specifically, we first identify what heads are more important in ViTs by introducing multiple pruning-based metrics. Then, we profile the spatial distribution of attention strengths between patches inside individual heads, as well as",
    "path": "papers/23/03/2303.13731.json",
    "total_tokens": 865,
    "translated_title": "注意力在视觉Transformer中是如何工作的？一次视觉分析尝试",
    "translated_abstract": "视觉Transformer（ViT）扩展了将Transformer模型从序列数据应用到图像的成功。该模型将图像分解为许多较小的patch，然后将它们排列成一个序列。接下来对序列应用多头自注意力机制以学习patch之间的关注。尽管已经有很多成功解释序列数据上Transformer的研究，但对ViT的解释却鲜有研究，许多问题依然没有得到明确的回答。例如，在众多的注意力头中，哪个更重要？不同的头对其空间邻居的特定patch进行的关注有多强？每个注意力头学习了哪些关注模式？本文通过一种视觉分析方法来回答这些问题。具体而言，我们首先通过引入多个基于剪枝的度量来确定ViT中哪些头更重要。接着，我们对每个注意力头内部的patch之间的注意力强度进行空间分布分析，同时分析了每个头学习的注意力模式。",
    "tldr": "本文采用视觉分析方法回答了ViT中头的重要性、不同头对空间邻居的关注强度、以及每个头学习的注意力模式等问题。",
    "en_tdlr": "This article answers questions about the importance of heads in ViT, the strength of attention to spatial neighbors in different heads, and attention patterns learned by each head through a visual analytics approach."
}