{
    "title": "Trainable Projected Gradient Method for Robust Fine-tuning. (arXiv:2303.10720v2 [cs.CV] UPDATED)",
    "abstract": "Recent studies on transfer learning have shown that selectively fine-tuning a subset of layers or customizing different learning rates for each layer can greatly improve robustness to out-of-distribution (OOD) data and retain generalization capability in the pre-trained models. However, most of these methods employ manually crafted heuristics or expensive hyper-parameter searches, which prevent them from scaling up to large datasets and neural networks. To solve this problem, we propose Trainable Projected Gradient Method (TPGM) to automatically learn the constraint imposed for each layer for a fine-grained fine-tuning regularization. This is motivated by formulating fine-tuning as a bi-level constrained optimization problem. Specifically, TPGM maintains a set of projection radii, i.e., distance constraints between the fine-tuned model and the pre-trained model, for each layer, and enforces them through weight projections. To learn the constraints, we propose a bi-level optimization to",
    "link": "http://arxiv.org/abs/2303.10720",
    "context": "Title: Trainable Projected Gradient Method for Robust Fine-tuning. (arXiv:2303.10720v2 [cs.CV] UPDATED)\nAbstract: Recent studies on transfer learning have shown that selectively fine-tuning a subset of layers or customizing different learning rates for each layer can greatly improve robustness to out-of-distribution (OOD) data and retain generalization capability in the pre-trained models. However, most of these methods employ manually crafted heuristics or expensive hyper-parameter searches, which prevent them from scaling up to large datasets and neural networks. To solve this problem, we propose Trainable Projected Gradient Method (TPGM) to automatically learn the constraint imposed for each layer for a fine-grained fine-tuning regularization. This is motivated by formulating fine-tuning as a bi-level constrained optimization problem. Specifically, TPGM maintains a set of projection radii, i.e., distance constraints between the fine-tuned model and the pre-trained model, for each layer, and enforces them through weight projections. To learn the constraints, we propose a bi-level optimization to",
    "path": "papers/23/03/2303.10720.json",
    "total_tokens": 1039,
    "translated_title": "可训练的投影梯度方法用于鲁棒微调",
    "translated_abstract": "近期关于迁移学习的研究表明，选择性地微调子集层或为每个层自定义不同的学习率可以极大地提高对于区分度数据的健壮性，并保留预先训练的模型的泛化能力。然而，大多数这些方法都采用手工制作的启发式方法或昂贵的超参数搜索，这防止了它们在大数据集和神经网络上扩展。为了解决这个问题，我们提出了可训练的投影梯度方法（TPGM），以自动学习针对每个层的施加约束以进行细粒度的微调正则化。这是通过将微调表述为双层约束优化问题来实现的。具体来说，TPGM维护每个图层的投影半径集，即微调模型与预训练模型之间的距离约束，并通过权重投影强制执行它们。为了学习约束，我们提出了一个双层优化来最大化训练数据上的经验风险，其受到给定投影半径集的约束。实验证明，我们的方法在各种微调场景下都可以实现最先进的性能，同时具有计算效率和易用性。",
    "tldr": "该论文提出了可训练的投影梯度方法（TPGM），以自动学习针对每个层的施加约束以进行细粒度的微调正则化，同时具有计算效率和易用性，并在各种微调场景下实现了最先进的性能。",
    "en_tdlr": "The paper proposes a trainable projected gradient method (TPGM) to automatically learn the constraints imposed on each layer for fine-grained fine-tuning regularization, showing state-of-the-art performance in various fine-tuning scenarios and being computationally efficient and easy-to-use."
}