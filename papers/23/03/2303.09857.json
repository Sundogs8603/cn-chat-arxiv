{
    "title": "Dual-path Adaptation from Image to Video Transformers. (arXiv:2303.09857v1 [cs.CV])",
    "abstract": "In this paper, we efficiently transfer the surpassing representation power of the vision foundation models, such as ViT and Swin, for video understanding with only a few trainable parameters. Previous adaptation methods have simultaneously considered spatial and temporal modeling with a unified learnable module but still suffered from fully leveraging the representative capabilities of image transformers. We argue that the popular dual-path (two-stream) architecture in video models can mitigate this problem. We propose a novel DualPath adaptation separated into spatial and temporal adaptation paths, where a lightweight bottleneck adapter is employed in each transformer block. Especially for temporal dynamic modeling, we incorporate consecutive frames into a grid-like frameset to precisely imitate vision transformers' capability that extrapolates relationships between tokens. In addition, we extensively investigate the multiple baselines from a unified perspective in video understanding",
    "link": "http://arxiv.org/abs/2303.09857",
    "context": "Title: Dual-path Adaptation from Image to Video Transformers. (arXiv:2303.09857v1 [cs.CV])\nAbstract: In this paper, we efficiently transfer the surpassing representation power of the vision foundation models, such as ViT and Swin, for video understanding with only a few trainable parameters. Previous adaptation methods have simultaneously considered spatial and temporal modeling with a unified learnable module but still suffered from fully leveraging the representative capabilities of image transformers. We argue that the popular dual-path (two-stream) architecture in video models can mitigate this problem. We propose a novel DualPath adaptation separated into spatial and temporal adaptation paths, where a lightweight bottleneck adapter is employed in each transformer block. Especially for temporal dynamic modeling, we incorporate consecutive frames into a grid-like frameset to precisely imitate vision transformers' capability that extrapolates relationships between tokens. In addition, we extensively investigate the multiple baselines from a unified perspective in video understanding",
    "path": "papers/23/03/2303.09857.json",
    "total_tokens": 853,
    "translated_title": "图像到视频转换的双路径自适应",
    "translated_abstract": "本文提出了一种高效地将视觉基础模型（如ViT和Swin）的优越表示能力转移到视频理解中的方法，仅使用少量可训练参数。我们认为，在以前的自适应方法中，同时考虑空间和时间建模，采用一个可学习的统一模块，但仍然无法充分利用图像变形器的代表性能力。我们认为，在视频模型中广泛采用的双路径（两个流）架构可以缓解这个问题。我们提出了一种新的双路径自适应，分为空间和时间自适应路径，并在每个变形器块中采用轻量级瓶颈适配器。特别是对于时间动态建模，我们将连续的帧组合成类似于网格的帧集，并精确模仿视觉变换器的能力，即推断令牌之间的关系。此外，我们从统一的视角广泛地研究了多个基线在视频理解中的应用。",
    "tldr": "本文提出了一种新的双路径自适应方法，在视频理解中充分利用图像变形器的能力，特别是对于时间动态建模，提出了一个精确模仿视觉变换器的能力的方法。",
    "en_tdlr": "This paper proposes a novel dual-path adaptation method that efficiently leverages image transformers' capabilities in video understanding, particularly in temporal dynamic modeling, using lightweight bottleneck adapters and grid-like framesets to imitate the relationship between tokens in vision transformers."
}