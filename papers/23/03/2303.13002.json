{
    "title": "Planning Goals for Exploration. (arXiv:2303.13002v1 [cs.LG])",
    "abstract": "Dropped into an unknown environment, what should an agent do to quickly learn about the environment and how to accomplish diverse tasks within it? We address this question within the goal-conditioned reinforcement learning paradigm, by identifying how the agent should set its goals at training time to maximize exploration. We propose \"Planning Exploratory Goals\" (PEG), a method that sets goals for each training episode to directly optimize an intrinsic exploration reward. PEG first chooses goal commands such that the agent's goal-conditioned policy, at its current level of training, will end up in states with high exploration potential. It then launches an exploration policy starting at those promising states. To enable this direct optimization, PEG learns world models and adapts sampling-based planning algorithms to \"plan goal commands\". In challenging simulated robotics environments including a multi-legged ant robot in a maze, and a robot arm on a cluttered tabletop, PEG exploration",
    "link": "http://arxiv.org/abs/2303.13002",
    "context": "Title: Planning Goals for Exploration. (arXiv:2303.13002v1 [cs.LG])\nAbstract: Dropped into an unknown environment, what should an agent do to quickly learn about the environment and how to accomplish diverse tasks within it? We address this question within the goal-conditioned reinforcement learning paradigm, by identifying how the agent should set its goals at training time to maximize exploration. We propose \"Planning Exploratory Goals\" (PEG), a method that sets goals for each training episode to directly optimize an intrinsic exploration reward. PEG first chooses goal commands such that the agent's goal-conditioned policy, at its current level of training, will end up in states with high exploration potential. It then launches an exploration policy starting at those promising states. To enable this direct optimization, PEG learns world models and adapts sampling-based planning algorithms to \"plan goal commands\". In challenging simulated robotics environments including a multi-legged ant robot in a maze, and a robot arm on a cluttered tabletop, PEG exploration",
    "path": "papers/23/03/2303.13002.json",
    "total_tokens": 1051,
    "tldr": "该论文针对智能体在未知环境中如何快速学习到其环境并在其中完成任务的问题，提出了一种直接优化内在探索奖励的方法——\"计划探索目标\" (PEG)，PEG 可以选择具有高探索潜力的目标状态，并在此基础上启动探索过程，在具有挑战性的模拟机器人环境中表现良好。",
    "en_tdlr": "This paper proposes a method called \"Planning Exploratory Goals\" (PEG) to address the challenge of how an agent quickly learns about an unknown environment and completes tasks within it, by setting goals for each training episode to directly optimize an intrinsic exploration reward. PEG learns world models and adapts sampling-based planning algorithms to \"plan goal commands\" and shows promising performance in challenging simulated robotics environments."
}