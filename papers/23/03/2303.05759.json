{
    "title": "An Overview on Language Models: Recent Developments and Outlook. (arXiv:2303.05759v2 [cs.CL] UPDATED)",
    "abstract": "Language modeling studies the probability distributions over strings of texts. It is one of the most fundamental tasks in natural language processing (NLP). It has been widely used in text generation, speech recognition, machine translation, etc. Conventional language models (CLMs) aim to predict the probability of linguistic sequences in a causal manner, while pre-trained language models (PLMs) cover broader concepts and can be used in both causal sequential modeling and fine-tuning for downstream applications. PLMs have their own training paradigms (usually self-supervised) and serve as foundation models in modern NLP systems. This overview paper provides an introduction to both CLMs and PLMs from five aspects, i.e., linguistic units, architectures, training methods, evaluation methods, and applications. Furthermore, we discuss the relationship between CLMs and PLMs and shed light on the future directions of language modeling in the pre-trained era.",
    "link": "http://arxiv.org/abs/2303.05759",
    "context": "Title: An Overview on Language Models: Recent Developments and Outlook. (arXiv:2303.05759v2 [cs.CL] UPDATED)\nAbstract: Language modeling studies the probability distributions over strings of texts. It is one of the most fundamental tasks in natural language processing (NLP). It has been widely used in text generation, speech recognition, machine translation, etc. Conventional language models (CLMs) aim to predict the probability of linguistic sequences in a causal manner, while pre-trained language models (PLMs) cover broader concepts and can be used in both causal sequential modeling and fine-tuning for downstream applications. PLMs have their own training paradigms (usually self-supervised) and serve as foundation models in modern NLP systems. This overview paper provides an introduction to both CLMs and PLMs from five aspects, i.e., linguistic units, architectures, training methods, evaluation methods, and applications. Furthermore, we discuss the relationship between CLMs and PLMs and shed light on the future directions of language modeling in the pre-trained era.",
    "path": "papers/23/03/2303.05759.json",
    "total_tokens": 864,
    "translated_title": "语言模型概述：最新发展和展望",
    "translated_abstract": "语言模型研究文本字符串的概率分布，是自然语言处理中最基本的任务之一。它被广泛应用于文本生成、语音识别、机器翻译等领域。传统语言模型旨在以因果方式预测语言序列的概率，而预训练语言模型涵盖更广泛的概念，并可用于因果顺序建模和下游应用的微调。预训练语言模型具有自己的训练范例（通常是自监督的），并在现代NLP系统中作为基础模型。本综述论文从语言单元、架构、训练方法、评估方法和应用五个方面介绍了传统语言模型和预训练语言模型，同时讨论了它们之间的关系，并对预训练时代的语言建模未来方向进行了探讨。",
    "tldr": "这篇综述论文介绍了传统语言模型和预训练语言模型的概念、方法和应用，探讨了二者之间的关系，并展望了预训练时代语言建模的未来方向。",
    "en_tdlr": "This overview paper provides an introduction to both conventional language models (CLMs) and pre-trained language models (PLMs), covering their concepts, methods, and applications, discussing the relationship between them, and exploring the future directions of language modeling in the pre-trained era."
}