{
    "title": "Visual-Language Prompt Tuning with Knowledge-guided Context Optimization. (arXiv:2303.13283v1 [cs.CV])",
    "abstract": "Prompt tuning is an effective way to adapt the pre-trained visual-language model (VLM) to the downstream task using task-related textual tokens. Representative CoOp-based work combines the learnable textual tokens with the class tokens to obtain specific textual knowledge. However, the specific textual knowledge is the worse generalization to the unseen classes because it forgets the essential general textual knowledge having a strong generalization ability. To tackle this issue, we introduce a novel Knowledge-guided Context Optimization (KgCoOp) to enhance the generalization ability of the learnable prompt for unseen classes. The key insight of KgCoOp is that forgetting about essential knowledge can be alleviated by reducing the discrepancy between the learnable prompt and the hand-crafted prompt. Especially, KgCoOp minimizes the discrepancy between the textual embeddings generated by learned prompts and the hand-crafted prompts. Finally, adding the KgCoOp upon the contrastive loss ca",
    "link": "http://arxiv.org/abs/2303.13283",
    "context": "Title: Visual-Language Prompt Tuning with Knowledge-guided Context Optimization. (arXiv:2303.13283v1 [cs.CV])\nAbstract: Prompt tuning is an effective way to adapt the pre-trained visual-language model (VLM) to the downstream task using task-related textual tokens. Representative CoOp-based work combines the learnable textual tokens with the class tokens to obtain specific textual knowledge. However, the specific textual knowledge is the worse generalization to the unseen classes because it forgets the essential general textual knowledge having a strong generalization ability. To tackle this issue, we introduce a novel Knowledge-guided Context Optimization (KgCoOp) to enhance the generalization ability of the learnable prompt for unseen classes. The key insight of KgCoOp is that forgetting about essential knowledge can be alleviated by reducing the discrepancy between the learnable prompt and the hand-crafted prompt. Especially, KgCoOp minimizes the discrepancy between the textual embeddings generated by learned prompts and the hand-crafted prompts. Finally, adding the KgCoOp upon the contrastive loss ca",
    "path": "papers/23/03/2303.13283.json",
    "total_tokens": 842,
    "translated_title": "基于知识引导的上下文优化的视觉语言提示调整",
    "translated_abstract": "提示调整是将预训练的视觉语言模型(VLM)适应于下游任务的有效方法，使用与任务相关的文本标记。然而，由于缺少强大的泛化能力的通用文本知识，具有特定文本知识的代表性CoOp工作在未见过的类别上的泛化能力较差。为了解决这个问题，我们引入了一种新的基于知识引导的上下文优化(KgCoOp)来增强对未知类别的适应能力。KgCoOp的关键见解是，可以通过降低可学习提示与手工提示之间的差异来缓解遗忘重要知识的问题。特别是，KgCoOp最小化通过学习提示生成的文本嵌入和手工制作提示之间的差异。最后，在对比损失上添加KgCoOp。",
    "tldr": "提出一种基于知识引导的上下文优化方法，通过降低可学习提示与手工提示之间的差异来解决具有特定文本知识的模型在未知类别上泛化能力差的问题。",
    "en_tdlr": "A novel Knowledge-guided Context Optimization (KgCoOp) is proposed to enhance the generalization ability of the learnable prompt for unseen classes by reducing the discrepancy between the learned prompt and the hand-crafted prompt."
}