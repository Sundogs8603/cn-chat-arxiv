{
    "title": "Secure Aggregation in Federated Learning is not Private: Leaking User Data at Large Scale through Model Modification. (arXiv:2303.12233v1 [cs.LG])",
    "abstract": "Security and privacy are important concerns in machine learning. End user devices often contain a wealth of data and this information is sensitive and should not be shared with servers or enterprises. As a result, federated learning was introduced to enable machine learning over large decentralized datasets while promising privacy by eliminating the need for data sharing. However, prior work has shown that shared gradients often contain private information and attackers can gain knowledge either through malicious modification of the architecture and parameters or by using optimization to approximate user data from the shared gradients. Despite this, most attacks have so far been limited in scale of number of clients, especially failing when client gradients are aggregated together using secure model aggregation. The attacks that still function are strongly limited in the number of clients attacked, amount of training samples they leak, or number of iterations they take to be trained. I",
    "link": "http://arxiv.org/abs/2303.12233",
    "context": "Title: Secure Aggregation in Federated Learning is not Private: Leaking User Data at Large Scale through Model Modification. (arXiv:2303.12233v1 [cs.LG])\nAbstract: Security and privacy are important concerns in machine learning. End user devices often contain a wealth of data and this information is sensitive and should not be shared with servers or enterprises. As a result, federated learning was introduced to enable machine learning over large decentralized datasets while promising privacy by eliminating the need for data sharing. However, prior work has shown that shared gradients often contain private information and attackers can gain knowledge either through malicious modification of the architecture and parameters or by using optimization to approximate user data from the shared gradients. Despite this, most attacks have so far been limited in scale of number of clients, especially failing when client gradients are aggregated together using secure model aggregation. The attacks that still function are strongly limited in the number of clients attacked, amount of training samples they leak, or number of iterations they take to be trained. I",
    "path": "papers/23/03/2303.12233.json",
    "total_tokens": 874,
    "translated_title": "联邦学习中的安全聚合并非隐私：通过模型修改大规模泄露用户数据",
    "translated_abstract": "安全和隐私是机器学习中的重要问题。终端用户设备通常包含大量敏感数据，不应与服务器或企业分享。因此，联邦学习被引入以在大规模分散式数据集上进行机器学习，同时通过消除数据共享来保证隐私。然而，先前的研究表明，共享的梯度通常包含私密信息，攻击者可以通过恶意修改架构和参数或使用优化从共享梯度中近似用户数据来获得知识。尽管如此，大多数攻击至今仍受到限制，尤其是在使用安全模型聚合将客户端梯度聚合在一起时会失败。目前仍然可行的攻击在被攻击的客户端数量、泄漏的训练样本数量或训练所需的迭代次数方面都受到严格限制。",
    "tldr": "联邦学习虽然能够消除数据共享，但共享的梯度可能会包含私密信息，并且攻击者可以通过恶意修改架构和参数或使用优化从共享的梯度中近似用户数据，导致用户数据泄露。",
    "en_tdlr": "While federated learning promises privacy by eliminating the need for data sharing, shared gradients in the process can contain private information, making attacks possible through modification of architecture and parameters or optimization to approximate user data, resulting in potential data leakages."
}