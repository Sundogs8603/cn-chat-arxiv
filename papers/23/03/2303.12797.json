{
    "title": "An algorithmic framework for the optimization of deep neural networks architectures and hyperparameters. (arXiv:2303.12797v1 [cs.NE])",
    "abstract": "In this paper, we propose an algorithmic framework to automatically generate efficient deep neural networks and optimize their associated hyperparameters. The framework is based on evolving directed acyclic graphs (DAGs), defining a more flexible search space than the existing ones in the literature. It allows mixtures of different classical operations: convolutions, recurrences and dense layers, but also more newfangled operations such as self-attention. Based on this search space we propose neighbourhood and evolution search operators to optimize both the architecture and hyper-parameters of our networks. These search operators can be used with any metaheuristic capable of handling mixed search spaces. We tested our algorithmic framework with an evolutionary algorithm on a time series prediction benchmark. The results demonstrate that our framework was able to find models outperforming the established baseline on numerous datasets.",
    "link": "http://arxiv.org/abs/2303.12797",
    "context": "Title: An algorithmic framework for the optimization of deep neural networks architectures and hyperparameters. (arXiv:2303.12797v1 [cs.NE])\nAbstract: In this paper, we propose an algorithmic framework to automatically generate efficient deep neural networks and optimize their associated hyperparameters. The framework is based on evolving directed acyclic graphs (DAGs), defining a more flexible search space than the existing ones in the literature. It allows mixtures of different classical operations: convolutions, recurrences and dense layers, but also more newfangled operations such as self-attention. Based on this search space we propose neighbourhood and evolution search operators to optimize both the architecture and hyper-parameters of our networks. These search operators can be used with any metaheuristic capable of handling mixed search spaces. We tested our algorithmic framework with an evolutionary algorithm on a time series prediction benchmark. The results demonstrate that our framework was able to find models outperforming the established baseline on numerous datasets.",
    "path": "papers/23/03/2303.12797.json",
    "total_tokens": 882,
    "translated_title": "一种用于深度神经网络架构和超参数优化的算法框架",
    "translated_abstract": "本文提出一种算法框架，用于自动生成高效的深度神经网络并优化相关的超参数。该框架基于进化的有向无环图(DAG)，定义了比文献中现有的搜索空间更为灵活的搜索空间，允许混合使用传统操作，如卷积、循环和密集层，以及较为新颖的操作，如自注意力机制。基于该搜索空间，我们提出了邻域搜索算子和演化搜索算子，以优化网络的架构和超参数。这些搜索算子可与任何能够处理混合搜索空间的元启发式算法一起使用。我们在时间序列预测数据集上使用进化算法测试了我们的算法框架。结果表明，我们的框架能够找到在许多数据集上性能优于基准模型的模型。",
    "tldr": "本文提出一种基于进化的有向无环图的算法框架，用于自动生成高效且灵活的深度神经网络并优化相关的超参数。此框架可用于任何能够处理混合搜索空间的元启发式算法，并在时间序列预测数据集上表现出比已有模型更好的性能。",
    "en_tdlr": "This paper proposes an algorithmic framework based on evolving directed acyclic graphs (DAGs) for automatically generating efficient and flexible deep neural networks and optimizing their associated hyperparameters. The framework can be used with any metaheuristic capable of handling mixed search spaces and demonstrated better performance on time series prediction benchmark datasets than established baseline models."
}