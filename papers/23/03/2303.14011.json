{
    "title": "SPEC: Summary Preference Decomposition for Low-Resource Abstractive Summarization. (arXiv:2303.14011v1 [cs.CL])",
    "abstract": "Neural abstractive summarization has been widely studied and achieved great success with large-scale corpora. However, the considerable cost of annotating data motivates the need for learning strategies under low-resource settings. In this paper, we investigate the problems of learning summarizers with only few examples and propose corresponding methods for improvements. First, typical transfer learning methods are prone to be affected by data properties and learning objectives in the pretext tasks. Therefore, based on pretrained language models, we further present a meta learning framework to transfer few-shot learning processes from source corpora to the target corpus. Second, previous methods learn from training examples without decomposing the content and preference. The generated summaries could therefore be constrained by the preference bias in the training set, especially under low-resource settings. As such, we propose decomposing the contents and preferences during learning th",
    "link": "http://arxiv.org/abs/2303.14011",
    "context": "Title: SPEC: Summary Preference Decomposition for Low-Resource Abstractive Summarization. (arXiv:2303.14011v1 [cs.CL])\nAbstract: Neural abstractive summarization has been widely studied and achieved great success with large-scale corpora. However, the considerable cost of annotating data motivates the need for learning strategies under low-resource settings. In this paper, we investigate the problems of learning summarizers with only few examples and propose corresponding methods for improvements. First, typical transfer learning methods are prone to be affected by data properties and learning objectives in the pretext tasks. Therefore, based on pretrained language models, we further present a meta learning framework to transfer few-shot learning processes from source corpora to the target corpus. Second, previous methods learn from training examples without decomposing the content and preference. The generated summaries could therefore be constrained by the preference bias in the training set, especially under low-resource settings. As such, we propose decomposing the contents and preferences during learning th",
    "path": "papers/23/03/2303.14011.json",
    "total_tokens": 963,
    "translated_title": "SPEC: 低资源抽象文摘的总结偏好分解",
    "translated_abstract": "神经抽象摘要已经被广泛研究，并在大规模语料库中取得了巨大成功。 然而，注释数据的相当高的成本促使我们需要在低资源环境下学习策略。 本文研究了只有少量示例情况下学习摘要生成器的问题，并提出了相应的改进方法。",
    "tldr": "本文提出了一种低资源抽象文摘的总结偏好分解的方法，以在只有少量示例的情况下学习生成器。其中，以预训练的语言模型为基础，提出了一种元学习框架，将少量样本的学习过程从源语料库转移到目标语料库。"
}