{
    "title": "Decentralized Weakly Convex Optimization Over the Stiefel Manifold. (arXiv:2303.17779v1 [math.OC])",
    "abstract": "We focus on a class of non-smooth optimization problems over the Stiefel manifold in the decentralized setting, where a connected network of $n$ agents cooperatively minimize a finite-sum objective function with each component being weakly convex in the ambient Euclidean space. Such optimization problems, albeit frequently encountered in applications, are quite challenging due to their non-smoothness and non-convexity. To tackle them, we propose an iterative method called the decentralized Riemannian subgradient method (DRSM). The global convergence and an iteration complexity of $\\mathcal{O}(\\varepsilon^{-2} \\log^2(\\varepsilon^{-1}))$ for forcing a natural stationarity measure below $\\varepsilon$ are established via the powerful tool of proximal smoothness from variational analysis, which could be of independent interest. Besides, we show the local linear convergence of the DRSM using geometrically diminishing stepsizes when the problem at hand further possesses a sharpness property. ",
    "link": "http://arxiv.org/abs/2303.17779",
    "context": "Title: Decentralized Weakly Convex Optimization Over the Stiefel Manifold. (arXiv:2303.17779v1 [math.OC])\nAbstract: We focus on a class of non-smooth optimization problems over the Stiefel manifold in the decentralized setting, where a connected network of $n$ agents cooperatively minimize a finite-sum objective function with each component being weakly convex in the ambient Euclidean space. Such optimization problems, albeit frequently encountered in applications, are quite challenging due to their non-smoothness and non-convexity. To tackle them, we propose an iterative method called the decentralized Riemannian subgradient method (DRSM). The global convergence and an iteration complexity of $\\mathcal{O}(\\varepsilon^{-2} \\log^2(\\varepsilon^{-1}))$ for forcing a natural stationarity measure below $\\varepsilon$ are established via the powerful tool of proximal smoothness from variational analysis, which could be of independent interest. Besides, we show the local linear convergence of the DRSM using geometrically diminishing stepsizes when the problem at hand further possesses a sharpness property. ",
    "path": "papers/23/03/2303.17779.json",
    "total_tokens": 888,
    "translated_title": "分布式矩阵局部最小化问题的去中心化弱凸优化",
    "translated_abstract": "我们关注于分布式网络中Stiefel流形上的一类非光滑最小化问题，在该问题中，$n$个代理协作最小化一个有限和目标函数，其中每个部分在环境欧几里得空间中是弱凸的。我们提出了一种名为去中心化黎曼次梯度法（DRSM）的迭代方法来解决这些具有非光滑和非凸性质的优化问题，并通过变分分析的近端光滑性强大工具来建立自然稳定性度量小于$\\varepsilon$的全局收敛性和迭代复杂度为$\\mathcal{O}(\\varepsilon^{-2} \\log^2(\\varepsilon^{-1}))$，这对于研究者来说可能具有单独的价值。此外，在问题具有锐度属性时，当使用几何渐小的步长时，我们还说明了DRSM的局部线性收敛性。",
    "tldr": "本文提出了一种去中心化黎曼次梯度法（DRSM）来解决分布式网络中的一类非光滑最小化问题，该方法可以在全局稳定性和迭代复杂度上得到保证。",
    "en_tdlr": "This paper proposes a decentralized Riemannian subgradient method (DRSM) to solve a class of non-smooth optimization problems over the Stiefel manifold in a distributed network, with guaranteed global stability and iteration complexity."
}