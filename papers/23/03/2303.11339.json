{
    "title": "FedMAE: Federated Self-Supervised Learning with One-Block Masked Auto-Encoder. (arXiv:2303.11339v1 [cs.LG])",
    "abstract": "Latest federated learning (FL) methods started to focus on how to use unlabeled data in clients for training due to users' privacy concerns, high labeling costs, or lack of expertise. However, current Federated Semi-Supervised/Self-Supervised Learning (FSSL) approaches fail to learn large-scale images because of the limited computing resources of local clients. In this paper, we introduce a new framework FedMAE, which stands for Federated Masked AutoEncoder, to address the problem of how to utilize unlabeled large-scale images for FL. Specifically, FedMAE can pre-train one-block Masked AutoEncoder (MAE) using large images in lightweight client devices, and then cascades multiple pre-trained one-block MAEs in the server to build a multi-block ViT backbone for downstream tasks. Theoretical analysis and experimental results on image reconstruction and classification show that our FedMAE achieves superior performance compared to the state-of-the-art FSSL methods.",
    "link": "http://arxiv.org/abs/2303.11339",
    "context": "Title: FedMAE: Federated Self-Supervised Learning with One-Block Masked Auto-Encoder. (arXiv:2303.11339v1 [cs.LG])\nAbstract: Latest federated learning (FL) methods started to focus on how to use unlabeled data in clients for training due to users' privacy concerns, high labeling costs, or lack of expertise. However, current Federated Semi-Supervised/Self-Supervised Learning (FSSL) approaches fail to learn large-scale images because of the limited computing resources of local clients. In this paper, we introduce a new framework FedMAE, which stands for Federated Masked AutoEncoder, to address the problem of how to utilize unlabeled large-scale images for FL. Specifically, FedMAE can pre-train one-block Masked AutoEncoder (MAE) using large images in lightweight client devices, and then cascades multiple pre-trained one-block MAEs in the server to build a multi-block ViT backbone for downstream tasks. Theoretical analysis and experimental results on image reconstruction and classification show that our FedMAE achieves superior performance compared to the state-of-the-art FSSL methods.",
    "path": "papers/23/03/2303.11339.json",
    "total_tokens": 1018,
    "translated_title": "FedMAE：带有单块遮蔽自编码器的联邦自监督学习",
    "translated_abstract": "最新的联邦学习方法开始关注如何利用用户设备中未标记的数据进行训练，原因是用户关注隐私，成本高，或者缺乏专业知识。然而，当前的Federated Semi-Supervised/Self-Supervised Learning（FSSL）方法由于本地客户端的有限计算资源而无法学习大规模图像。本文提出了一个新的框架FedMAE，即Federated Masked AutoEncoder，以解决如何利用未标记的大尺度图像进行联邦学习的问题。具体来说，FedMAE可以使用轻量级客户端设备中的大型图像预训练单块遮蔽自编码器（MAE），然后在服务器中级联多个预训练的单块MAE以构建下游任务的多块ViT骨干。 图像重建和分类的理论分析和实验结果表明，与最先进的FSSL方法相比，我们的FedMAE获得了更优秀的性能。",
    "tldr": "本文提出了一个新的联邦自监督学习框架 FedMAE，可以利用轻量级设备上的大规模未标记图像进行联邦学习。FedMAE可以预训练一个单块遮蔽自编码器，并将多个预训练的单块MAE级联在服务器上构建用于下游任务的多块ViT骨干。实验结果表明，FedMAE相较于最先进的FSSL方法具有卓越的性能。",
    "en_tdlr": "This paper proposes a new federated self-supervised learning framework called FedMAE, which can utilize large unlabeled images on lightweight devices for training. FedMAE can pre-train a one-block masked autoencoder on large images and cascade multiple pre-trained ones on the server to build a multi-block ViT backbone for downstream tasks. Experimental results show that FedMAE outperforms state-of-the-art FSSL methods."
}