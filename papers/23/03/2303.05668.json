{
    "title": "UNFUSED: UNsupervised Finetuning Using SElf supervised Distillation. (arXiv:2303.05668v2 [eess.AS] UPDATED)",
    "abstract": "In this paper, we introduce UnFuSeD, a novel approach to leverage self-supervised learning and reduce the need for large amounts of labeled data for audio classification. Unlike prior works, which directly fine-tune a self-supervised pre-trained encoder on a target dataset, we use the encoder to generate pseudo-labels for unsupervised fine-tuning before the actual fine-tuning step. We first train an encoder using a novel self-supervised learning algorithm (SSL) on an unlabeled audio dataset. Then, we use that encoder to generate pseudo-labels on our target task dataset via clustering the extracted representations. These pseudo-labels are then used to guide self-distillation on a randomly initialized model, which we call unsupervised fine-tuning. Finally, the resultant encoder is then fine-tuned on our target task dataset. Through UnFuSeD, we propose the first system that moves away from generic SSL paradigms in literature, which pre-train and fine-tune the same encoder, and present a n",
    "link": "http://arxiv.org/abs/2303.05668",
    "context": "Title: UNFUSED: UNsupervised Finetuning Using SElf supervised Distillation. (arXiv:2303.05668v2 [eess.AS] UPDATED)\nAbstract: In this paper, we introduce UnFuSeD, a novel approach to leverage self-supervised learning and reduce the need for large amounts of labeled data for audio classification. Unlike prior works, which directly fine-tune a self-supervised pre-trained encoder on a target dataset, we use the encoder to generate pseudo-labels for unsupervised fine-tuning before the actual fine-tuning step. We first train an encoder using a novel self-supervised learning algorithm (SSL) on an unlabeled audio dataset. Then, we use that encoder to generate pseudo-labels on our target task dataset via clustering the extracted representations. These pseudo-labels are then used to guide self-distillation on a randomly initialized model, which we call unsupervised fine-tuning. Finally, the resultant encoder is then fine-tuned on our target task dataset. Through UnFuSeD, we propose the first system that moves away from generic SSL paradigms in literature, which pre-train and fine-tune the same encoder, and present a n",
    "path": "papers/23/03/2303.05668.json",
    "total_tokens": 928,
    "translated_title": "UNFUSED: 使用自监督蒸馏的无监督微调方法",
    "translated_abstract": "本文提出了一种名为 UnFuSeD 的新方法，利用自监督学习技术，减少音频分类所需的标注数据量。与以往的方法不同，直接在目标数据集上对自监督预训练的编码器进行微调，我们使用编码器为无监督微调产生伪标签。首先，我们使用一种新型的自监督学习算法在未标注的音频数据集上训练编码器。然后，通过提取的表示进行聚类，使用编码器生成伪标签。这些伪标签用于指导随机初始化模型上的自蒸馏，我们将其称为无监督微调。最终，结果编码器在目标任务数据集上进行微调。通过 UnFuSeD，我们提出了第一个与文献中的通用自监督学习方法不同的系统，即预训练和微调相同的编码器，提出了一种利用自监督学习和无监督微调方法减少对标注数据的依赖来进行音频分类的方法。",
    "tldr": "本文提出了一种使用自监督蒸馏的无监督微调方法，可以通过生成伪标签来减少音频分类所需的标注数据量。",
    "en_tdlr": "This paper proposes a novel approach, UNFUSED, for reducing the need for labeled data in audio classification by using self-supervised learning and unsupervised fine-tuning, generating pseudo-labels to guide distillation on a randomly initialized model."
}