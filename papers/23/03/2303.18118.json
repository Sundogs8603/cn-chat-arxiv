{
    "title": "A two-head loss function for deep Average-K classification. (arXiv:2303.18118v1 [cs.LG])",
    "abstract": "Average-K classification is an alternative to top-K classification in which the number of labels returned varies with the ambiguity of the input image but must average to K over all the samples. A simple method to solve this task is to threshold the softmax output of a model trained with the cross-entropy loss. This approach is theoretically proven to be asymptotically consistent, but it is not guaranteed to be optimal for a finite set of samples. In this paper, we propose a new loss function based on a multi-label classification head in addition to the classical softmax. This second head is trained using pseudo-labels generated by thresholding the softmax head while guaranteeing that K classes are returned on average. We show that this approach allows the model to better capture ambiguities between classes and, as a result, to return more consistent sets of possible classes. Experiments on two datasets from the literature demonstrate that our approach outperforms the softmax baseline,",
    "link": "http://arxiv.org/abs/2303.18118",
    "context": "Title: A two-head loss function for deep Average-K classification. (arXiv:2303.18118v1 [cs.LG])\nAbstract: Average-K classification is an alternative to top-K classification in which the number of labels returned varies with the ambiguity of the input image but must average to K over all the samples. A simple method to solve this task is to threshold the softmax output of a model trained with the cross-entropy loss. This approach is theoretically proven to be asymptotically consistent, but it is not guaranteed to be optimal for a finite set of samples. In this paper, we propose a new loss function based on a multi-label classification head in addition to the classical softmax. This second head is trained using pseudo-labels generated by thresholding the softmax head while guaranteeing that K classes are returned on average. We show that this approach allows the model to better capture ambiguities between classes and, as a result, to return more consistent sets of possible classes. Experiments on two datasets from the literature demonstrate that our approach outperforms the softmax baseline,",
    "path": "papers/23/03/2303.18118.json",
    "total_tokens": 845,
    "translated_title": "一种用于深度平均K分类的双头损失函数",
    "translated_abstract": "平均K分类是一种在输入图像不确定性影响下返回数量变化的K分类方法，但所有样本的返回结果需平均为K。解决这一问题的简单方法是用经过交叉熵损失训练的模型的softmax输出进行阈值处理。这种方法理论上证明渐近一致，但对于有限样本集合并不一定是最优选择。本文提出了一种新的损失函数，基于对经典softmax的多标签分类头而添加的第二个头。用软阈值对softmax的伪标签进行训练，保证平均返回K个类。我们展示了这种方法可以更好地捕捉类别之间的不确定性，并因此返回更一致的可能类别集。实验在两个已有数据集上证明了我们的方法胜过softmax基线。",
    "tldr": "本文提出了一种双头损失函数用于平均K分类，其中第二个头被用于多标签分类，模型采用了软阈值训练以保证平均返回K个类，实验在两个数据集上证明其优于softmax基线。"
}