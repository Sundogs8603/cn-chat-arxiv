{
    "title": "Scalable Bayesian Meta-Learning through Generalized Implicit Gradients. (arXiv:2303.17768v1 [cs.LG])",
    "abstract": "Meta-learning owns unique effectiveness and swiftness in tackling emerging tasks with limited data. Its broad applicability is revealed by viewing it as a bi-level optimization problem. The resultant algorithmic viewpoint however, faces scalability issues when the inner-level optimization relies on gradient-based iterations. Implicit differentiation has been considered to alleviate this challenge, but it is restricted to an isotropic Gaussian prior, and only favors deterministic meta-learning approaches. This work markedly mitigates the scalability bottleneck by cross-fertilizing the benefits of implicit differentiation to probabilistic Bayesian meta-learning. The novel implicit Bayesian meta-learning (iBaML) method not only broadens the scope of learnable priors, but also quantifies the associated uncertainty. Furthermore, the ultimate complexity is well controlled regardless of the inner-level optimization trajectory. Analytical error bounds are established to demonstrate the precisi",
    "link": "http://arxiv.org/abs/2303.17768",
    "context": "Title: Scalable Bayesian Meta-Learning through Generalized Implicit Gradients. (arXiv:2303.17768v1 [cs.LG])\nAbstract: Meta-learning owns unique effectiveness and swiftness in tackling emerging tasks with limited data. Its broad applicability is revealed by viewing it as a bi-level optimization problem. The resultant algorithmic viewpoint however, faces scalability issues when the inner-level optimization relies on gradient-based iterations. Implicit differentiation has been considered to alleviate this challenge, but it is restricted to an isotropic Gaussian prior, and only favors deterministic meta-learning approaches. This work markedly mitigates the scalability bottleneck by cross-fertilizing the benefits of implicit differentiation to probabilistic Bayesian meta-learning. The novel implicit Bayesian meta-learning (iBaML) method not only broadens the scope of learnable priors, but also quantifies the associated uncertainty. Furthermore, the ultimate complexity is well controlled regardless of the inner-level optimization trajectory. Analytical error bounds are established to demonstrate the precisi",
    "path": "papers/23/03/2303.17768.json",
    "total_tokens": 909,
    "translated_title": "可扩展的贝叶斯元学习：基于广义隐式梯度方法",
    "translated_abstract": "元学习在处理数据有限的新兴任务时具有独特的效率和速度。将其视为双层优化问题揭示出它的广泛适用性。然而，当内层优化依赖于基于梯度的迭代时，其算法视角面临可扩展性问题。本研究通过将隐式梯度的优势应用到概率贝叶斯元学习中，显著缓解了可扩展性的瓶颈。新颖的隐式贝叶斯元学习（iBaML）方法不仅扩展了可学习先验的范围，还量化了相关的不确定性。此外，无论内层优化轨迹如何，最终复杂性均得到良好控制。分析误差界定证明了该方法的精确性。",
    "tldr": "本研究提出了一种新颖的隐式贝叶斯元学习方法(iBaML)，通过将隐式梯度的优势应用到概率贝叶斯元学习中，显著缓解了可扩展性的瓶颈，并量化了相关的不确定性，具备良好的复杂性控制。"
}