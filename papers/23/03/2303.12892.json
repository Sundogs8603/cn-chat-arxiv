{
    "title": "A Small-Scale Switch Transformer and NLP-based Model for Clinical Narratives Classification. (arXiv:2303.12892v1 [cs.CL])",
    "abstract": "In recent years, Transformer-based models such as the Switch Transformer have achieved remarkable results in natural language processing tasks. However, these models are often too complex and require extensive pre-training, which limits their effectiveness for small clinical text classification tasks with limited data. In this study, we propose a simplified Switch Transformer framework and train it from scratch on a small French clinical text classification dataset at CHU Sainte-Justine hospital. Our results demonstrate that the simplified small-scale Transformer models outperform pre-trained BERT-based models, including DistillBERT, CamemBERT, FlauBERT, and FrALBERT. Additionally, using a mixture of expert mechanisms from the Switch Transformer helps capture diverse patterns; hence, the proposed approach achieves better results than a conventional Transformer with the self-attention mechanism. Finally, our proposed framework achieves an accuracy of 87\\%, precision at 87\\%, and recall ",
    "link": "http://arxiv.org/abs/2303.12892",
    "context": "Title: A Small-Scale Switch Transformer and NLP-based Model for Clinical Narratives Classification. (arXiv:2303.12892v1 [cs.CL])\nAbstract: In recent years, Transformer-based models such as the Switch Transformer have achieved remarkable results in natural language processing tasks. However, these models are often too complex and require extensive pre-training, which limits their effectiveness for small clinical text classification tasks with limited data. In this study, we propose a simplified Switch Transformer framework and train it from scratch on a small French clinical text classification dataset at CHU Sainte-Justine hospital. Our results demonstrate that the simplified small-scale Transformer models outperform pre-trained BERT-based models, including DistillBERT, CamemBERT, FlauBERT, and FrALBERT. Additionally, using a mixture of expert mechanisms from the Switch Transformer helps capture diverse patterns; hence, the proposed approach achieves better results than a conventional Transformer with the self-attention mechanism. Finally, our proposed framework achieves an accuracy of 87\\%, precision at 87\\%, and recall ",
    "path": "papers/23/03/2303.12892.json",
    "total_tokens": 1035,
    "translated_title": "用于临床叙述分类的小规模交换变压器和基于NLP的模型",
    "translated_abstract": "近年来，基于变压器的模型（如交换变压器）在自然语言处理任务中取得了显著的结果。然而，这些模型通常过于复杂并需要大量的预训练，这限制了它们在有限数据的小型临床文本分类任务中的有效性。在本研究中，我们提出了一个简化的Switch Transformer框架，并从头开始在CHU Sainte-Justine医院的小型法语临床文本分类数据集上进行了训练。我们的结果表明，简化的小规模变压器模型优于预训练的BERT模型，包括DistillBERT、CamemBERT、FlauBERT和FrALBERT。此外，使用Switch Transformer的专家混合机制有助于捕获多样的模式；因此，所提出的方法比具有自我注意机制的传统变压器获得更好的结果。最后，我们提出的框架在测试集上实现了87％的准确率，87％的精度和86％的召回率，突显了其在小型临床文本分类任务中的潜力。",
    "tldr": "本研究提出了一个简化的Switch Transformer框架，并从头开始训练，取得了在小型法语临床文本分类任务中比预训练的BERT模型更好的效果，采用Switch Transformer的专家混合机制有助于提高识别准确度，最终在测试集上实现了87％的准确率、87％的精度和86％的召回率。",
    "en_tdlr": "This study proposes a simplified Switch Transformer framework and trains it from scratch on a small French clinical text classification dataset at CHU Sainte-Justine hospital, achieving better results than pre-trained BERT-based models. The mixture of expert mechanisms from the Switch Transformer helps capture diverse patterns for improved accuracy, achieving an accuracy of 87\\%, precision at 87\\%, and recall at 86\\% on the test set."
}