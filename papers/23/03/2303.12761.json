{
    "title": "LSTM-based Video Quality Prediction Accounting for Temporal Distortions in Videoconferencing Calls. (arXiv:2303.12761v1 [eess.IV])",
    "abstract": "Current state-of-the-art video quality models, such as VMAF, give excellent prediction results by comparing the degraded video with its reference video. However, they do not consider temporal distortions (e.g., frame freezes or skips) that occur during videoconferencing calls. In this paper, we present a data-driven approach for modeling such distortions automatically by training an LSTM with subjective quality ratings labeled via crowdsourcing. The videos were collected from live videoconferencing calls in 83 different network conditions. We applied QR codes as markers on the source videos to create aligned references and compute temporal features based on the alignment vectors. Using these features together with VMAF core features, our proposed model achieves a PCC of 0.99 on the validation set. Furthermore, our model outputs per-frame quality that gives detailed insight into the cause of video quality impairments. The VCM model and dataset are open-sourced at https://github.com/micr",
    "link": "http://arxiv.org/abs/2303.12761",
    "context": "Title: LSTM-based Video Quality Prediction Accounting for Temporal Distortions in Videoconferencing Calls. (arXiv:2303.12761v1 [eess.IV])\nAbstract: Current state-of-the-art video quality models, such as VMAF, give excellent prediction results by comparing the degraded video with its reference video. However, they do not consider temporal distortions (e.g., frame freezes or skips) that occur during videoconferencing calls. In this paper, we present a data-driven approach for modeling such distortions automatically by training an LSTM with subjective quality ratings labeled via crowdsourcing. The videos were collected from live videoconferencing calls in 83 different network conditions. We applied QR codes as markers on the source videos to create aligned references and compute temporal features based on the alignment vectors. Using these features together with VMAF core features, our proposed model achieves a PCC of 0.99 on the validation set. Furthermore, our model outputs per-frame quality that gives detailed insight into the cause of video quality impairments. The VCM model and dataset are open-sourced at https://github.com/micr",
    "path": "papers/23/03/2303.12761.json",
    "total_tokens": 926,
    "translated_title": "考虑视频会议通话中的时间失真的基于LSTM的视频质量预测",
    "translated_abstract": "目前的视频质量模型（例如VMAF）能够通过将降质视频与参考视频进行比较，获得出色的预测结果。然而，它们没有考虑视频会议通话期间发生的时间失真（例如帧冻结或跳跃）。本文提出了一种数据驱动方法，通过使用经由群众智慧标记的主观质量评分训练LSTM自动建模这种失真。我们从83种不同的网络情况下的实时视频会议中收集了视频。我们在源视频上应用QR码作为标记来创建对齐的参考视频，并根据对齐向量计算时间特征。结合VMAF核心特征，我们提出的模型使用这些特征实现了验证集上0.99的PCC。此外，我们的模型输出每帧质量，可以详细解析导致视频质量损失的原因。VCM模型和数据集在https://github.com/micr上开源。",
    "tldr": "本文提出了一种基于数据的方法，通过训练一个LSTM模型实现了对视频会议通话中出现的时间失真的建模，并成功预测了视频质量，其中模型的每帧输出可以详细分析视频质量损失的原因。",
    "en_tdlr": "This paper proposes a data-driven approach to automatically model temporal distortions that occur during videoconferencing calls by training an LSTM with subjective quality ratings labeled via crowdsourcing. The model achieves a PCC of 0.99 on the validation set and outputs per-frame quality that provides detailed insight into the cause of video quality impairments."
}