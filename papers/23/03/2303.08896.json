{
    "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models. (arXiv:2303.08896v1 [cs.CL])",
    "abstract": "Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to token-level output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose \"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if a LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GP",
    "link": "http://arxiv.org/abs/2303.08896",
    "context": "Title: SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models. (arXiv:2303.08896v1 [cs.CL])\nAbstract: Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to token-level output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose \"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if a LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GP",
    "path": "papers/23/03/2303.08896.json",
    "total_tokens": 1017,
    "translated_title": "SelfCheckGPT: 零资源黑盒幻觉检测方法用于生成式大语言模型",
    "translated_abstract": "生成式大语言模型（LLM）例如GPT-3，能够对各种用户提示进行高度流畅的响应。然而，LLM已知会产生幻觉事实和非事实陈述，这可能会削弱对它们的输出的信任。现有的事实检查方法要么需要访问令牌级输出概率分布（这可能对于ChatGPT等系统来说不可用），要么需要通过单独的通常复杂的模块接口的外部数据库。在这项工作中，我们提出了一种简单的基于采样的方法，称为“SelfCheckGPT”，可以以零资源的方式检查黑盒模型，即不需要外部数据库。 SelfCheckGPT利用一个简单的思想：如果LLM具有特定概念的知识，则采样的响应可能类似并包含一致的事实。但是，对于幻觉的事实，随机采样的响应可能会发散并相互矛盾。我们通过使用GP-T-3模型为例来研究此方法，并在常见任务上进行广泛实验，结果表明SelfCheckGPT能够有效地检测模型的幻觉现象，且在保持准确性的同时保持良好的效率。",
    "tldr": "SelfCheckGPT是一种简单的基于采样的方法，可以以零资源的方式检查黑盒模型的幻觉现象。",
    "en_tdlr": "SelfCheckGPT is a simple sampling-based approach that can be used to fact-check black-box models in a zero-resource fashion and effectively detect hallucinations in Generative Large Language Models such as GPT-3."
}