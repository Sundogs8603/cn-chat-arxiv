{
    "title": "Exploiting Multilingualism in Low-resource Neural Machine Translation via Adversarial Learning. (arXiv:2303.18011v1 [cs.CL])",
    "abstract": "Generative Adversarial Networks (GAN) offer a promising approach for Neural Machine Translation (NMT). However, feeding multiple morphologically languages into a single model during training reduces the NMT's performance. In GAN, similar to bilingual models, multilingual NMT only considers one reference translation for each sentence during model training. This single reference translation limits the GAN model from learning sufficient information about the source sentence representation. Thus, in this article, we propose Denoising Adversarial Auto-encoder-based Sentence Interpolation (DAASI) approach to perform sentence interpolation by learning the intermediate latent representation of the source and target sentences of multilingual language pairs. Apart from latent representation, we also use the Wasserstein-GAN approach for the multilingual NMT model by incorporating the model generated sentences of multiple languages for reward computation. This computed reward optimizes the perform",
    "link": "http://arxiv.org/abs/2303.18011",
    "context": "Title: Exploiting Multilingualism in Low-resource Neural Machine Translation via Adversarial Learning. (arXiv:2303.18011v1 [cs.CL])\nAbstract: Generative Adversarial Networks (GAN) offer a promising approach for Neural Machine Translation (NMT). However, feeding multiple morphologically languages into a single model during training reduces the NMT's performance. In GAN, similar to bilingual models, multilingual NMT only considers one reference translation for each sentence during model training. This single reference translation limits the GAN model from learning sufficient information about the source sentence representation. Thus, in this article, we propose Denoising Adversarial Auto-encoder-based Sentence Interpolation (DAASI) approach to perform sentence interpolation by learning the intermediate latent representation of the source and target sentences of multilingual language pairs. Apart from latent representation, we also use the Wasserstein-GAN approach for the multilingual NMT model by incorporating the model generated sentences of multiple languages for reward computation. This computed reward optimizes the perform",
    "path": "papers/23/03/2303.18011.json",
    "total_tokens": 912,
    "translated_title": "通过对抗学习利用多语种来进行低资源神经机器翻译",
    "translated_abstract": "生成式对抗网络（GAN）为神经机器翻译（NMT）提供了一种有前途的方法。然而，在训练期间将多个形态语言提供给单个模型会降低NMT的性能。在GAN中，与双语模型类似，多语种NMT仅在模型训练期间考虑一个参考翻译的每个句子。这个单一的参考翻译限制了GAN模型从源句子表示中学习足够的信息。因此，在本文中，我们提出了一种基于去噪对抗自编码器的句子插值（DAASI）方法，通过学习多语种语言对的源句子和目标句子的中间潜在表示来执行句子插值。除了潜在表示之外，我们还使用Wasserstein-GAN方法进行多语种NMT模型，将多种语言的模型生成句子并与参考翻译一起用于奖励计算。这个计算出的奖励优化了表现。",
    "tldr": "本文提出了一种基于去噪对抗自编码器的句子插值方法，以及利用多语种模型生成句子进行奖励计算的Wasserstein-GAN方法来优化多语种神经机器翻译模型的性能。",
    "en_tdlr": "The paper proposes a Denoising Adversarial Auto-encoder-based Sentence Interpolation (DAASI) approach for performing sentence interpolation and a Wasserstein-GAN method using model-generated sentences of multiple languages for reward computation to optimize the performance of low-resource multilingual neural machine translation."
}