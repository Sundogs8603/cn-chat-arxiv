{
    "title": "EasyDGL: Encode, Train and Interpret for Continuous-time Dynamic Graph Learning. (arXiv:2303.12341v1 [cs.LG])",
    "abstract": "Dynamic graphs arise in various real-world applications, and it is often welcomed to model the dynamics directly in continuous time domain for its flexibility. This paper aims to design an easy-to-use pipeline (termed as EasyDGL which is also due to its implementation by DGL toolkit) composed of three key modules with both strong fitting ability and interpretability. Specifically the proposed pipeline which involves encoding, training and interpreting: i) a temporal point process (TPP) modulated attention architecture to endow the continuous-time resolution with the coupled spatiotemporal dynamics of the observed graph with edge-addition events; ii) a principled loss composed of task-agnostic TPP posterior maximization based on observed events on the graph, and a task-aware loss with a masking strategy over dynamic graph, where the covered tasks include dynamic link prediction, dynamic node classification and node traffic forecasting; iii) interpretation of the model outputs (e.g., rep",
    "link": "http://arxiv.org/abs/2303.12341",
    "context": "Title: EasyDGL: Encode, Train and Interpret for Continuous-time Dynamic Graph Learning. (arXiv:2303.12341v1 [cs.LG])\nAbstract: Dynamic graphs arise in various real-world applications, and it is often welcomed to model the dynamics directly in continuous time domain for its flexibility. This paper aims to design an easy-to-use pipeline (termed as EasyDGL which is also due to its implementation by DGL toolkit) composed of three key modules with both strong fitting ability and interpretability. Specifically the proposed pipeline which involves encoding, training and interpreting: i) a temporal point process (TPP) modulated attention architecture to endow the continuous-time resolution with the coupled spatiotemporal dynamics of the observed graph with edge-addition events; ii) a principled loss composed of task-agnostic TPP posterior maximization based on observed events on the graph, and a task-aware loss with a masking strategy over dynamic graph, where the covered tasks include dynamic link prediction, dynamic node classification and node traffic forecasting; iii) interpretation of the model outputs (e.g., rep",
    "path": "papers/23/03/2303.12341.json",
    "total_tokens": 1073,
    "translated_title": "EasyDGL: 连续时间动态图学习的编码、训练和解释",
    "translated_abstract": "动态图在各种实际应用中都很常见，直接在连续时间域中建模动态图以实现灵活性是被欢迎的选择。本文旨在设计一个易于使用的流水线（名为EasyDGL，也因其由DGL工具包实现而得名），它由三个关键模块组成，既具有强大的拟合能力，又易于解释。具体来说，所提出的流水线涉及编码、训练和解释：i）时间点过程（TPP）调制的注意力架构将连续时间分辨率赋予观察到的具有边添加事件的图的耦合时空动态；ii）一个合理的损失，由任务不可知的基于观察到的图中事件的TPP后验最大化和一个带遮蔽策略的任务感知损失组成，其中涵盖的任务包括动态链接预测、动态节点分类和节点流量预测；iii）通过敏感性分析解释模型输出（例如节点和边的表示），从而有助于理解每个边和节点对模型预测的贡献。",
    "tldr": "EasyDGL 提出了一个易于使用的连续时间动态图学习的流水线，其中包含编码、训练和解释三个关键步骤。它使用时间点过程调制的注意力架构来处理连续时间动态图，在任务不可知的损失和任务感知损失的组合下实现动态链接预测、动态节点分类和节点流量预测，并通过敏感性分析对模型输出进行解释。",
    "en_tdlr": "EasyDGL proposes an easy-to-use pipeline for continuous-time dynamic graph learning, consisting of encoding, training, and interpretation. It utilizes a temporal point process modulated attention architecture to capture the spatiotemporal dynamics of edge-addition events, and achieves dynamic link prediction, dynamic node classification, and node traffic forecasting through a principled loss composed of task-agnostic TPP posterior maximization and task-aware loss with a masking strategy. The interpretation of model outputs is performed through sensitivity analysis."
}