{
    "title": "Optimal Horizon-Free Reward-Free Exploration for Linear Mixture MDPs. (arXiv:2303.10165v1 [cs.LG])",
    "abstract": "We study reward-free reinforcement learning (RL) with linear function approximation, where the agent works in two phases: (1) in the exploration phase, the agent interacts with the environment but cannot access the reward; and (2) in the planning phase, the agent is given a reward function and is expected to find a near-optimal policy based on samples collected in the exploration phase. The sample complexities of existing reward-free algorithms have a polynomial dependence on the planning horizon, which makes them intractable for long planning horizon RL problems. In this paper, we propose a new reward-free algorithm for learning linear mixture Markov decision processes (MDPs), where the transition probability can be parameterized as a linear combination of known feature mappings. At the core of our algorithm is uncertainty-weighted value-targeted regression with exploration-driven pseudo-reward and a high-order moment estimator for the aleatoric and epistemic uncertainties. When the t",
    "link": "http://arxiv.org/abs/2303.10165",
    "context": "Title: Optimal Horizon-Free Reward-Free Exploration for Linear Mixture MDPs. (arXiv:2303.10165v1 [cs.LG])\nAbstract: We study reward-free reinforcement learning (RL) with linear function approximation, where the agent works in two phases: (1) in the exploration phase, the agent interacts with the environment but cannot access the reward; and (2) in the planning phase, the agent is given a reward function and is expected to find a near-optimal policy based on samples collected in the exploration phase. The sample complexities of existing reward-free algorithms have a polynomial dependence on the planning horizon, which makes them intractable for long planning horizon RL problems. In this paper, we propose a new reward-free algorithm for learning linear mixture Markov decision processes (MDPs), where the transition probability can be parameterized as a linear combination of known feature mappings. At the core of our algorithm is uncertainty-weighted value-targeted regression with exploration-driven pseudo-reward and a high-order moment estimator for the aleatoric and epistemic uncertainties. When the t",
    "path": "papers/23/03/2303.10165.json",
    "total_tokens": 910,
    "translated_abstract": "本论文探讨使用线性函数逼近的无奖励强化学习，代理在两个阶段工作：（1）在探索期间，代理与环境进行交互但不能接触到奖励；（2）在计划阶段，代理获得奖励函数，并基于在探索阶段收集的样本找到接近最优的策略。现有无奖励算法的样本复杂度与规划时间有多项式的相关性，这使得它们难以解决长期规划时间的强化学习问题。本文提出了一种新的无奖励算法，用于学习线性混合马尔可夫决策过程（MDPs），其中转移概率可以被参数化为已知特征映射的线性组合。我们的算法核心是带有探索驱动伪奖励和对杂项和认知不确定性进行高阶矩估计的不确定性加权价值目标回归。",
    "tldr": "本论文提出了一种针对线性混合MDPs的新的无奖励算法，采用不确定性加权价值目标回归和高阶矩估计。算法能避免现有算法中面临的规划时间多项式复杂度问题。",
    "en_tdlr": "This paper proposes a new reward-free algorithm for learning linear mixture Markov decision processes, which avoids the polynomial dependence on the planning horizon faced by existing algorithms by using uncertainty-weighted value-targeted regression and high-order moment estimation for uncertainties."
}