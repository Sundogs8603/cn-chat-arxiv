{
    "title": "Multimodal and multicontrast image fusion via deep generative models. (arXiv:2303.15963v1 [cs.LG])",
    "abstract": "Recently, it has become progressively more evident that classic diagnostic labels are unable to reliably describe the complexity and variability of several clinical phenotypes. This is particularly true for a broad range of neuropsychiatric illnesses (e.g., depression, anxiety disorders, behavioral phenotypes). Patient heterogeneity can be better described by grouping individuals into novel categories based on empirically derived sections of intersecting continua that span across and beyond traditional categorical borders. In this context, neuroimaging data carry a wealth of spatiotemporally resolved information about each patient's brain. However, they are usually heavily collapsed a priori through procedures which are not learned as part of model training, and consequently not optimized for the downstream prediction task. This is because every individual participant usually comes with multiple whole-brain 3D imaging modalities often accompanied by a deep genotypic and phenotypic char",
    "link": "http://arxiv.org/abs/2303.15963",
    "context": "Title: Multimodal and multicontrast image fusion via deep generative models. (arXiv:2303.15963v1 [cs.LG])\nAbstract: Recently, it has become progressively more evident that classic diagnostic labels are unable to reliably describe the complexity and variability of several clinical phenotypes. This is particularly true for a broad range of neuropsychiatric illnesses (e.g., depression, anxiety disorders, behavioral phenotypes). Patient heterogeneity can be better described by grouping individuals into novel categories based on empirically derived sections of intersecting continua that span across and beyond traditional categorical borders. In this context, neuroimaging data carry a wealth of spatiotemporally resolved information about each patient's brain. However, they are usually heavily collapsed a priori through procedures which are not learned as part of model training, and consequently not optimized for the downstream prediction task. This is because every individual participant usually comes with multiple whole-brain 3D imaging modalities often accompanied by a deep genotypic and phenotypic char",
    "path": "papers/23/03/2303.15963.json",
    "total_tokens": 1237,
    "translated_title": "基于深生成模型的多模态和多对比度图像融合",
    "translated_abstract": "最近，人们逐渐意识到传统的诊断标签无法可靠地描述多种临床表型的复杂性和变异性，尤其是广泛的神经精神疾病（例如抑郁症、焦虑症、行为表型）。患者的异质性可以通过将个体根据经验得出的交织连续的新类别分组来更好地描述，这些连续跨越并超越了传统的类别边界。在这种情况下，神经影像数据携带着关于每个患者大脑的富含时空的信息。然而，它们通常会经过一系列事先未学习为模型的训练部分的过程进行预处理，因此没有针对下游预测任务进行优化。这是因为每个个体通常都有多个全脑三维成像模态，常伴随着深层的基因型和表型特征描述。为应对这些挑战，我们提出了一种基于深度生成模型的图像融合方法，以组合多模态和多对比度的神经影像数据。我们的方法使用变分自动编码器（VAE）来学习不同成像模态的共同表示空间，同时强制执行模态特定和对比度特定的编码-解码过程。我们在来自OPENNeuro数据库的174名MDD患者的数据集上评估了我们的方法，包括结构T1加权MRI、扩散张量成像（DTI）和静息功能MRI（rsfMRI）数据。我们的结果表明，所提出的方法有效地整合了多模态和多对比度的成像数据，相对于传统方法，分类表现得到了改善。",
    "tldr": "该论文提出了一种基于变分自动编码器（VAE）的图像融合方法，可以整合多模态和多对比度的神经影像数据，以提高神经影像分析的分类性能。",
    "en_tdlr": "The paper proposes an image fusion approach based on variational autoencoder to integrate multimodal and multicontrast neuroimaging data, and shows improved classification performance compared to conventional methods."
}