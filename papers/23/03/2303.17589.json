{
    "title": "Polarity is all you need to learn and transfer faster. (arXiv:2303.17589v1 [cs.LG])",
    "abstract": "Natural intelligences (NIs) thrive in a dynamic world - they learn quickly, sometimes with only a few samples. In contrast, Artificial intelligences (AIs) typically learn with prohibitive amount of training samples and computational power. What design principle difference between NI and AI could contribute to such a discrepancy? Here, we propose an angle from weight polarity: development processes initialize NIs with advantageous polarity configurations; as NIs grow and learn, synapse magnitudes update yet polarities are largely kept unchanged. We demonstrate with simulation and image classification tasks that if weight polarities are adequately set $\\textit{a priori}$, then networks learn with less time and data. We also explicitly illustrate situations in which $\\textit{a priori}$ setting the weight polarities is disadvantageous for networks. Our work illustrates the value of weight polarities from the perspective of statistical and computational efficiency during learning.",
    "link": "http://arxiv.org/abs/2303.17589",
    "context": "Title: Polarity is all you need to learn and transfer faster. (arXiv:2303.17589v1 [cs.LG])\nAbstract: Natural intelligences (NIs) thrive in a dynamic world - they learn quickly, sometimes with only a few samples. In contrast, Artificial intelligences (AIs) typically learn with prohibitive amount of training samples and computational power. What design principle difference between NI and AI could contribute to such a discrepancy? Here, we propose an angle from weight polarity: development processes initialize NIs with advantageous polarity configurations; as NIs grow and learn, synapse magnitudes update yet polarities are largely kept unchanged. We demonstrate with simulation and image classification tasks that if weight polarities are adequately set $\\textit{a priori}$, then networks learn with less time and data. We also explicitly illustrate situations in which $\\textit{a priori}$ setting the weight polarities is disadvantageous for networks. Our work illustrates the value of weight polarities from the perspective of statistical and computational efficiency during learning.",
    "path": "papers/23/03/2303.17589.json",
    "total_tokens": 961,
    "translated_title": "极性是您学习和快速传递所需的全部",
    "translated_abstract": "自然智能在动态世界中茁壮成长——它们可以很快地学习，有时仅需要少量样本。相比之下，人工智能通常需要大量的训练样本和计算能力才能学习。什么设计原则使得自然智能和人工智能之间存在如此明显的差异？在这里，我们从权重极性的角度提出了一个思路：发育过程会初始化有优势极性配置的自然智能，当自然智能成长和学习时，突触的大小发生变化，但极性基本保持不变。通过模拟和图像分类任务，我们证明了如果权重极性被适当地设置在先，那么网络学习所需的时间和数据将会减少。我们还明确说明了某些情况下，先验设置权重极性会对网络产生不利的影响。我们的工作从学习的统计和计算效率的角度阐述了权重极性的价值。",
    "tldr": "本文从权重极性的角度提出了一个思路：发育过程会初始化有优势极性配置的自然智能，当自然智能成长和学习时，突触的大小发生变化，但极性基本保持不变，如果权重极性被适当地设置在先，那么网络学习所需的时间和数据将会减少，从而增加学习和转移的效率。",
    "en_tdlr": "This article proposes an idea from the perspective of weight polarity: development processes initialize natural intelligences with advantageous polarity configurations; as they grow and learn, synapse magnitudes update yet polarities are largely kept unchanged. If weight polarities are adequately set a priori, networks can learn with less time and data, thereby increasing learning and transfer efficiency."
}