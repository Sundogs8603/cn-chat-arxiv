{
    "title": "Recognition, recall, and retention of few-shot memories in large language models. (arXiv:2303.17557v1 [cs.CL])",
    "abstract": "The training of modern large language models (LLMs) takes place in a regime where most training examples are seen only a few times by the model during the course of training. What does a model remember about such examples seen only a few times during training and how long does that memory persist in the face of continuous training with new examples? Here, we investigate these questions through simple recognition, recall, and retention experiments with LLMs. In recognition experiments, we ask if the model can distinguish the seen example from a novel example; in recall experiments, we ask if the model can correctly recall the seen example when cued by a part of it; and in retention experiments, we periodically probe the model's memory for the original examples as the model is trained continuously with new examples. We find that a single exposure is generally sufficient for a model to achieve near perfect accuracy even in very challenging recognition experiments. We estimate that the rec",
    "link": "http://arxiv.org/abs/2303.17557",
    "context": "Title: Recognition, recall, and retention of few-shot memories in large language models. (arXiv:2303.17557v1 [cs.CL])\nAbstract: The training of modern large language models (LLMs) takes place in a regime where most training examples are seen only a few times by the model during the course of training. What does a model remember about such examples seen only a few times during training and how long does that memory persist in the face of continuous training with new examples? Here, we investigate these questions through simple recognition, recall, and retention experiments with LLMs. In recognition experiments, we ask if the model can distinguish the seen example from a novel example; in recall experiments, we ask if the model can correctly recall the seen example when cued by a part of it; and in retention experiments, we periodically probe the model's memory for the original examples as the model is trained continuously with new examples. We find that a single exposure is generally sufficient for a model to achieve near perfect accuracy even in very challenging recognition experiments. We estimate that the rec",
    "path": "papers/23/03/2303.17557.json",
    "total_tokens": 895,
    "translated_title": "大型语言模型的少样本记忆的识别、回忆和保持",
    "translated_abstract": "当代大型语言模型的训练在一个大多数训练样本仅在模型训练期间看到几次的模式下进行。本文通过对大型语言模型进行简单的识别、回忆和保持实验，探究模型对仅在训练期间少次观察到的样本的记忆及其在继续训练时持续的时间。实验结果表明，仅一个接触通常足以让模型在非常具有挑战性的识别实验中达到近乎完美的准确性。我们估计，在连续训练新样本的几个时期内，模型保持了已见样本的可识别特征。实验表明，大型语言模型具有记忆和泛化少样本例子的能力，这与它们在自然语言处理基准测试中的卓越表现一致。",
    "tldr": "本文通过对大型语言模型进行实验，揭示了这种模型能够有效地记忆和泛化仅在训练中少次观察到的例子。",
    "en_tdlr": "This paper investigates the memory and generalization of large language models on few-shot examples through recognition, recall, and retention experiments, and shows that these models are capable of retaining recognizable features of few-shot examples for several epochs of continuous training."
}