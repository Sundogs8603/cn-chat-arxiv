{
    "title": "The Quantization Model of Neural Scaling. (arXiv:2303.13506v1 [cs.LG])",
    "abstract": "We propose the $\\textit{Quantization Model}$ of neural scaling laws, explaining both the observed power law dropoff of loss with model and data size, and also the sudden emergence of new capabilities with scale. We derive this model from what we call the $\\textit{Quantization Hypothesis}$, where learned network capabilities are quantized into discrete chunks ($\\textit{quanta}$). We show that when quanta are learned in order of decreasing use frequency, then a power law in use frequencies explains observed power law scaling of loss. We validate this prediction on toy datasets, then study how scaling curves decompose for large language models. Using language model internals, we auto-discover diverse model capabilities (quanta) and find tentative evidence that the distribution over corresponding subproblems in the prediction of natural text is compatible with the power law predicted from the neural scaling exponent as predicted from our theory.",
    "link": "http://arxiv.org/abs/2303.13506",
    "context": "Title: The Quantization Model of Neural Scaling. (arXiv:2303.13506v1 [cs.LG])\nAbstract: We propose the $\\textit{Quantization Model}$ of neural scaling laws, explaining both the observed power law dropoff of loss with model and data size, and also the sudden emergence of new capabilities with scale. We derive this model from what we call the $\\textit{Quantization Hypothesis}$, where learned network capabilities are quantized into discrete chunks ($\\textit{quanta}$). We show that when quanta are learned in order of decreasing use frequency, then a power law in use frequencies explains observed power law scaling of loss. We validate this prediction on toy datasets, then study how scaling curves decompose for large language models. Using language model internals, we auto-discover diverse model capabilities (quanta) and find tentative evidence that the distribution over corresponding subproblems in the prediction of natural text is compatible with the power law predicted from the neural scaling exponent as predicted from our theory.",
    "path": "papers/23/03/2303.13506.json",
    "total_tokens": 914,
    "translated_title": "神经网络缩放的量化模型",
    "translated_abstract": "我们提出了神经网络缩放定律的量化模型，解释了观察到的损失函数随着模型和数据规模的幂律下降以及随着规模的增加出现新能力的突然突破。我们从所谓的“量化假设”中推导出这个模型，其中学习到的神经网络功能被量化为离散块（“量子”）。我们在降序学习频率中学习量子，并表明当量子被以递减使用频率的顺序学习时，在使用频率中使用幂律可以解释观察到的损失缩放定律。我们在玩具数据集上验证了这个预测，然后研究了大型语言模型的缩放曲线如何分解。使用语言模型的内部，我们自动发现多样的模型能力（量子），并发现对应子问题的分布与我们理论预测的神经缩放指数产生了兼容性证据。",
    "tldr": "该论文提出了神经网络缩放的量化模型，通过将神经网络学习到的功能分为量子并对量子进行递减使用频率的顺序学习，可以解释损失缩放定律，并自动发现语言模型的多样能力。",
    "en_tdlr": "The paper proposes a Quantization Model to explain the power law dropoff of loss with model and data size, and the sudden emergence of new capabilities with scale. By learning the network capabilities as discrete chunks (quanta) in decreasing frequency, the observed scaling laws are explained. The model is validated on toy datasets and applied to large language models to auto-discover diverse capabilities."
}