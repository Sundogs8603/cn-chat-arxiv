{
    "title": "Active Finetuning: Exploiting Annotation Budget in the Pretraining-Finetuning Paradigm. (arXiv:2303.14382v1 [cs.CV])",
    "abstract": "Given the large-scale data and the high annotation cost, pretraining-finetuning becomes a popular paradigm in multiple computer vision tasks. Previous research has covered both the unsupervised pretraining and supervised finetuning in this paradigm, while little attention is paid to exploiting the annotation budget for finetuning. To fill in this gap, we formally define this new active finetuning task focusing on the selection of samples for annotation in the pretraining-finetuning paradigm. We propose a novel method called ActiveFT for active finetuning task to select a subset of data distributing similarly with the entire unlabeled pool and maintaining enough diversity by optimizing a parametric model in the continuous space. We prove that the Earth Mover's distance between the distributions of the selected subset and the entire data pool is also reduced in this process. Extensive experiments show the leading performance and high efficiency of ActiveFT superior to baselines on both i",
    "link": "http://arxiv.org/abs/2303.14382",
    "context": "Title: Active Finetuning: Exploiting Annotation Budget in the Pretraining-Finetuning Paradigm. (arXiv:2303.14382v1 [cs.CV])\nAbstract: Given the large-scale data and the high annotation cost, pretraining-finetuning becomes a popular paradigm in multiple computer vision tasks. Previous research has covered both the unsupervised pretraining and supervised finetuning in this paradigm, while little attention is paid to exploiting the annotation budget for finetuning. To fill in this gap, we formally define this new active finetuning task focusing on the selection of samples for annotation in the pretraining-finetuning paradigm. We propose a novel method called ActiveFT for active finetuning task to select a subset of data distributing similarly with the entire unlabeled pool and maintaining enough diversity by optimizing a parametric model in the continuous space. We prove that the Earth Mover's distance between the distributions of the selected subset and the entire data pool is also reduced in this process. Extensive experiments show the leading performance and high efficiency of ActiveFT superior to baselines on both i",
    "path": "papers/23/03/2303.14382.json",
    "total_tokens": 944,
    "translated_title": "主动微调：在预训练-微调范式中利用注释预算",
    "translated_abstract": "针对大规模数据和高昂的注释成本，预训练-微调范式在多个计算机视觉任务中变得流行。以前的研究涵盖了无监督预训练和有监督微调，在这个范式中很少关注利用注释预算来微调。为了填补这个空白，本文正式定义了这个新的主动微调任务，重点是在预训练-微调范式中选择样本进行注释。我们提出了一种称为ActiveFT的新方法来进行主动微调任务，通过在连续空间中优化参数模型，选择与整个未标记数据池类似分布并保持足够多样性的数据子集。我们证明了，在此过程中，所选子集的分布和整个数据池之间的地球移动距离也被减小。广泛的实验表明，ActiveFT相对于基线具有卓越的性能和高效性。",
    "tldr": "本文提出了一种新的“主动微调”任务，旨在在预训练-微调范式中利用注释预算。作者提出了一种名为ActiveFT的方法，通过优化参数模型来选择一组数据子集进行注释，以减小所选子集的分布与整个数据池之间的地球移动距离。"
}