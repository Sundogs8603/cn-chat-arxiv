{
    "title": "Exploring the Limits of Deep Image Clustering using Pretrained Models. (arXiv:2303.17896v1 [cs.CV])",
    "abstract": "We present a general methodology that learns to classify images without labels by leveraging pretrained feature extractors. Our approach involves self-distillation training of clustering heads, based on the fact that nearest neighbors in the pretrained feature space are likely to share the same label. We propose a novel objective to learn associations between images by introducing a variant of pointwise mutual information together with instance weighting. We demonstrate that the proposed objective is able to attenuate the effect of false positive pairs while efficiently exploiting the structure in the pretrained feature space. As a result, we improve the clustering accuracy over $k$-means on $17$ different pretrained models by $6.1$\\% and $12.2$\\% on ImageNet and CIFAR100, respectively. Finally, using self-supervised pretrained vision transformers we push the clustering accuracy on ImageNet to $61.6$\\%. The code will be open-sourced.",
    "link": "http://arxiv.org/abs/2303.17896",
    "context": "Title: Exploring the Limits of Deep Image Clustering using Pretrained Models. (arXiv:2303.17896v1 [cs.CV])\nAbstract: We present a general methodology that learns to classify images without labels by leveraging pretrained feature extractors. Our approach involves self-distillation training of clustering heads, based on the fact that nearest neighbors in the pretrained feature space are likely to share the same label. We propose a novel objective to learn associations between images by introducing a variant of pointwise mutual information together with instance weighting. We demonstrate that the proposed objective is able to attenuate the effect of false positive pairs while efficiently exploiting the structure in the pretrained feature space. As a result, we improve the clustering accuracy over $k$-means on $17$ different pretrained models by $6.1$\\% and $12.2$\\% on ImageNet and CIFAR100, respectively. Finally, using self-supervised pretrained vision transformers we push the clustering accuracy on ImageNet to $61.6$\\%. The code will be open-sourced.",
    "path": "papers/23/03/2303.17896.json",
    "total_tokens": 1054,
    "translated_title": "利用预训练模型探索深度图像聚类的极限",
    "translated_abstract": "我们提出了一种通用方法，利用预训练的特征提取器学习在没有标签的情况下对图像进行分类的方法。我们的方法涉及到基于预训练特征空间中最近邻居共享相同标签的事实对聚类头进行自蒸馏训练。我们提出了一种新的目标函数，通过引入一种点对点的互信息变量以及实例加权来学习图像之间的关联性。我们证明了所提出的目标函数能够减弱假阳性对的影响，同时高效地利用预训练特征空间中的结构。因此，我们在ImageNet和CIFAR100的17个不同的预训练模型上将聚类精度相对于k-均值提高了6.1%和12.2%。最后，使用自监督的预训练视觉变换器，我们将在ImageNet上的聚类准确度提高到了61.6%。代码将公开源代码化。",
    "tldr": "本文提出了一种利用预训练模型实现无标签图像分类的方法，通过自蒸馏训练聚类头学习图像之间的关联性，并提出了一种新的目标函数，可以高效准确地通过预训练特征空间中的结构来学习。使用该方法在ImageNet和CIFAR100的17个不同的预训练模型上将聚类精度相对于k-均值提高了6.1%和12.2%。在ImageNet上，使用自监督的预训练视觉变换器能够将聚类准确度提高到61.6%。",
    "en_tdlr": "This paper proposes a method for label-free image classification using pretrained models by self-distillation training of clustering heads and introducing a novel objective function that can efficiently learn the associations between images in the pretrained feature space. The clustering accuracy is improved by 6.1% and 12.2% over k-means on 17 different pretrained models in ImageNet and CIFAR100, respectively. The clustering accuracy on ImageNet is pushed to 61.6% using self-supervised pretrained vision transformers."
}