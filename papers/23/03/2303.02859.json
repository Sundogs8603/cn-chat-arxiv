{
    "title": "Bayesian inference with finitely wide neural networks. (arXiv:2303.02859v2 [cond-mat.dis-nn] UPDATED)",
    "abstract": "The analytic inference, e.g. predictive distribution being in closed form, may be an appealing benefit for machine learning practitioners when they treat wide neural networks as Gaussian process in Bayesian setting. The realistic widths, however, are finite and cause weak deviation from the Gaussianity under which partial marginalization of random variables in a model is straightforward. On the basis of multivariate Edgeworth expansion, we propose a non-Gaussian distribution in differential form to model a finite set of outputs from a random neural network, and derive the corresponding marginal and conditional properties. Thus, we are able to derive the non-Gaussian posterior distribution in Bayesian regression task. In addition, in the bottlenecked deep neural networks, a weight space representation of deep Gaussian process, the non-Gaussianity is investigated through the marginal kernel.",
    "link": "http://arxiv.org/abs/2303.02859",
    "context": "Title: Bayesian inference with finitely wide neural networks. (arXiv:2303.02859v2 [cond-mat.dis-nn] UPDATED)\nAbstract: The analytic inference, e.g. predictive distribution being in closed form, may be an appealing benefit for machine learning practitioners when they treat wide neural networks as Gaussian process in Bayesian setting. The realistic widths, however, are finite and cause weak deviation from the Gaussianity under which partial marginalization of random variables in a model is straightforward. On the basis of multivariate Edgeworth expansion, we propose a non-Gaussian distribution in differential form to model a finite set of outputs from a random neural network, and derive the corresponding marginal and conditional properties. Thus, we are able to derive the non-Gaussian posterior distribution in Bayesian regression task. In addition, in the bottlenecked deep neural networks, a weight space representation of deep Gaussian process, the non-Gaussianity is investigated through the marginal kernel.",
    "path": "papers/23/03/2303.02859.json",
    "total_tokens": 787,
    "translated_title": "有限宽度神经网络的贝叶斯推断",
    "translated_abstract": "当机器学习从业者将宽度很大的神经网络视为贝叶斯设置中的高斯过程时，解析推断，例如预测分布以闭合形式给出，可能是一种吸引人的优势。但是，实际的宽度是有限的，并且在该宽度下，一些随机变量的边际化的高斯假设可能出现偏差。基于多元Edgeworth展开，我们提出了用微分形式表示的非高斯分布，来对来自随机神经网络的有限输出进行建模，并推导出相应的边际和条件属性，从而能够在贝叶斯回归任务中推导出非高斯后验分布。此外，在瓶颈式深度神经网络中，通过边缘核探究了深高斯过程的非高斯特性。",
    "tldr": "本文通过多元Edgeworth展开，提出用微分形式表示非高斯分布，来模拟有限宽度神经网络的非高斯后验分布。",
    "en_tdlr": "This paper proposes a non-Gaussian distribution in differential form using multivariate Edgeworth expansion to model the non-Gaussian posterior distribution in Bayesian regression task for finite-width neural networks."
}