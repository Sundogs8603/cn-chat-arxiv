{
    "title": "AdPE: Adversarial Positional Embeddings for Pretraining Vision Transformers via MAE+. (arXiv:2303.07598v1 [cs.CV])",
    "abstract": "Unsupervised learning of vision transformers seeks to pretrain an encoder via pretext tasks without labels. Among them is the Masked Image Modeling (MIM) aligned with pretraining of language transformers by predicting masked patches as a pretext task. A criterion in unsupervised pretraining is the pretext task needs to be sufficiently hard to prevent the transformer encoder from learning trivial low-level features not generalizable well to downstream tasks. For this purpose, we propose an Adversarial Positional Embedding (AdPE) approach -- It distorts the local visual structures by perturbing the position encodings so that the learned transformer cannot simply use the locally correlated patches to predict the missing ones. We hypothesize that it forces the transformer encoder to learn more discriminative features in a global context with stronger generalizability to downstream tasks. We will consider both absolute and relative positional encodings, where adversarial positions can be im",
    "link": "http://arxiv.org/abs/2303.07598",
    "context": "Title: AdPE: Adversarial Positional Embeddings for Pretraining Vision Transformers via MAE+. (arXiv:2303.07598v1 [cs.CV])\nAbstract: Unsupervised learning of vision transformers seeks to pretrain an encoder via pretext tasks without labels. Among them is the Masked Image Modeling (MIM) aligned with pretraining of language transformers by predicting masked patches as a pretext task. A criterion in unsupervised pretraining is the pretext task needs to be sufficiently hard to prevent the transformer encoder from learning trivial low-level features not generalizable well to downstream tasks. For this purpose, we propose an Adversarial Positional Embedding (AdPE) approach -- It distorts the local visual structures by perturbing the position encodings so that the learned transformer cannot simply use the locally correlated patches to predict the missing ones. We hypothesize that it forces the transformer encoder to learn more discriminative features in a global context with stronger generalizability to downstream tasks. We will consider both absolute and relative positional encodings, where adversarial positions can be im",
    "path": "papers/23/03/2303.07598.json",
    "total_tokens": 873,
    "translated_title": "AdPE：通过MAE+对视觉Transformer进行预训练的对抗位置嵌入",
    "translated_abstract": "无监督学习视觉Transformer旨在通过预设任务在没有标签的情况下预先训练编码器。其中一个任务是Masked Image Modeling（MIM），与预训练语言Transformer预测掩蔽补丁相对应。无监督预训练中的一个准则是预设任务需要足够难以防止Transformer学习不能很好地泛化到下游任务的微不足道的低层次特征。为此，我们提出了Adversarial Positional Embedding（AdPE）方法 - 通过扰动位置编码来扭曲局部视觉结构，以使得学习的Transformer不能仅使用局部相关的补丁来预测缺失的补丁。我们假设它迫使Transformer编码器在全局上下文中学习更具有差别性的特征，从而在下游任务中具有更强的泛化能力。我们将考虑绝对和相对位置编码，其中对抗位置可以模拟为...",
    "tldr": "AdPE方法通过对抗位置嵌入，扭曲局部结构，强制Transformer编码器在全局上下文中学习更具有差别性的特征，从而提高泛化能力。",
    "en_tdlr": "AdPE approach enhances the generalizability of the pretraining Vision Transformers by distorting the local visual structures with adversarial positional embeddings, which force the Transformer encoder to learn more discriminative features in a global context."
}