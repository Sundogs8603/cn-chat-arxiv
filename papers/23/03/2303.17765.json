{
    "title": "Learning from Similar Linear Representations: Adaptivity, Minimaxity, and Robustness. (arXiv:2303.17765v1 [stat.ML])",
    "abstract": "Representation multi-task learning (MTL) and transfer learning (TL) have achieved tremendous success in practice. However, the theoretical understanding of these methods is still lacking. Most existing theoretical works focus on cases where all tasks share the same representation, and claim that MTL and TL almost always improve performance. However, as the number of tasks grow, assuming all tasks share the same representation is unrealistic. Also, this does not always match empirical findings, which suggest that a shared representation may not necessarily improve single-task or target-only learning performance. In this paper, we aim to understand how to learn from tasks with \\textit{similar but not exactly the same} linear representations, while dealing with outlier tasks. We propose two algorithms that are \\textit{adaptive} to the similarity structure and \\textit{robust} to outlier tasks under both MTL and TL settings. Our algorithms outperform single-task or target-only learning when",
    "link": "http://arxiv.org/abs/2303.17765",
    "context": "Title: Learning from Similar Linear Representations: Adaptivity, Minimaxity, and Robustness. (arXiv:2303.17765v1 [stat.ML])\nAbstract: Representation multi-task learning (MTL) and transfer learning (TL) have achieved tremendous success in practice. However, the theoretical understanding of these methods is still lacking. Most existing theoretical works focus on cases where all tasks share the same representation, and claim that MTL and TL almost always improve performance. However, as the number of tasks grow, assuming all tasks share the same representation is unrealistic. Also, this does not always match empirical findings, which suggest that a shared representation may not necessarily improve single-task or target-only learning performance. In this paper, we aim to understand how to learn from tasks with \\textit{similar but not exactly the same} linear representations, while dealing with outlier tasks. We propose two algorithms that are \\textit{adaptive} to the similarity structure and \\textit{robust} to outlier tasks under both MTL and TL settings. Our algorithms outperform single-task or target-only learning when",
    "path": "papers/23/03/2303.17765.json",
    "total_tokens": 722,
    "translated_title": "学习相似的线性表示：适应性、极小化、以及稳健性",
    "translated_abstract": "表示多任务学习和迁移学习在实践中取得了巨大的成功，然而对这些方法的理论理解仍然欠缺。本文旨在理解从具有相似但并非完全相同的线性表示的任务中学习，同时处理异常值任务。我们提出了两种算法，适应相似性结构并对异常值任务具有稳健性，适用于表示多任务学习和迁移学习设置，我们的算法在单任务或仅目标学习时表现优异。",
    "tldr": "本文提出了两种算法，适应相似性结构并对异常值任务具有稳健性，适用于表示多任务学习和迁移学习设置。",
    "en_tdlr": "This paper proposes two algorithms that are adaptive to the similarity structure and robust to outlier tasks under both representation multi-task learning and transfer learning settings."
}