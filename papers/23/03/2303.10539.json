{
    "title": "Textless Speech-to-Music Retrieval Using Emotion Similarity. (arXiv:2303.10539v1 [cs.SD])",
    "abstract": "We introduce a framework that recommends music based on the emotions of speech. In content creation and daily life, speech contains information about human emotions, which can be enhanced by music. Our framework focuses on a cross-domain retrieval system to bridge the gap between speech and music via emotion labels. We explore different speech representations and report their impact on different speech types, including acting voice and wake-up words. We also propose an emotion similarity regularization term in cross-domain retrieval tasks. By incorporating the regularization term into training, similar speech-and-music pairs in the emotion space are closer in the joint embedding space. Our comprehensive experimental results show that the proposed model is effective in textless speech-to-music retrieval.",
    "link": "http://arxiv.org/abs/2303.10539",
    "context": "Title: Textless Speech-to-Music Retrieval Using Emotion Similarity. (arXiv:2303.10539v1 [cs.SD])\nAbstract: We introduce a framework that recommends music based on the emotions of speech. In content creation and daily life, speech contains information about human emotions, which can be enhanced by music. Our framework focuses on a cross-domain retrieval system to bridge the gap between speech and music via emotion labels. We explore different speech representations and report their impact on different speech types, including acting voice and wake-up words. We also propose an emotion similarity regularization term in cross-domain retrieval tasks. By incorporating the regularization term into training, similar speech-and-music pairs in the emotion space are closer in the joint embedding space. Our comprehensive experimental results show that the proposed model is effective in textless speech-to-music retrieval.",
    "path": "papers/23/03/2303.10539.json",
    "total_tokens": 806,
    "translated_title": "基于情感相似性的无语音文本的语音到音乐检索",
    "translated_abstract": "我们引入了一个框架，根据语音情绪推荐音乐。在内容创建和日常生活中，语音包含有关人类情感的信息，这些信息可以通过音乐来增强。我们的框架关注跨域检索系统，通过情感标签来弥合语音和音乐之间的差距。我们探索了不同的语音表示，并报告了它们对不同语音类型（包括表演语音和唤醒词）的影响。我们还在跨域检索任务中提出了情感相似性正则化项。通过将正则化项纳入训练中，情感空间中相似的语音和音乐对在联合嵌入空间中更加接近。我们广泛的实验结果表明，所提出的模型对于文本无关的语音到音乐检索非常有效。",
    "tldr": "该论文提出了一种基于情感相似性框架来进行无语音文本的语音到音乐检索，该框架通过跨域检索系统来弥合语音和音乐之间的差距，该模型是有效的。",
    "en_tdlr": "This paper proposes a framework for textless speech-to-music retrieval using emotion similarity, which helps bridge the gap between speech and music via a cross-domain retrieval system. The proposed model is effective in retrieving music based on speech emotions."
}