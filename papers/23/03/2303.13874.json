{
    "title": "Query-Dependent Video Representation for Moment Retrieval and Highlight Detection. (arXiv:2303.13874v1 [cs.CV])",
    "abstract": "Recently, video moment retrieval and highlight detection (MR/HD) are being spotlighted as the demand for video understanding is drastically increased. The key objective of MR/HD is to localize the moment and estimate clip-wise accordance level, i.e., saliency score, to the given text query. Although the recent transformer-based models brought some advances, we found that these methods do not fully exploit the information of a given query. For example, the relevance between text query and video contents is sometimes neglected when predicting the moment and its saliency. To tackle this issue, we introduce Query-Dependent DETR (QD-DETR), a detection transformer tailored for MR/HD. As we observe the insignificant role of a given query in transformer architectures, our encoding module starts with cross-attention layers to explicitly inject the context of text query into video representation. Then, to enhance the model's capability of exploiting the query information, we manipulate the video",
    "link": "http://arxiv.org/abs/2303.13874",
    "context": "Title: Query-Dependent Video Representation for Moment Retrieval and Highlight Detection. (arXiv:2303.13874v1 [cs.CV])\nAbstract: Recently, video moment retrieval and highlight detection (MR/HD) are being spotlighted as the demand for video understanding is drastically increased. The key objective of MR/HD is to localize the moment and estimate clip-wise accordance level, i.e., saliency score, to the given text query. Although the recent transformer-based models brought some advances, we found that these methods do not fully exploit the information of a given query. For example, the relevance between text query and video contents is sometimes neglected when predicting the moment and its saliency. To tackle this issue, we introduce Query-Dependent DETR (QD-DETR), a detection transformer tailored for MR/HD. As we observe the insignificant role of a given query in transformer architectures, our encoding module starts with cross-attention layers to explicitly inject the context of text query into video representation. Then, to enhance the model's capability of exploiting the query information, we manipulate the video",
    "path": "papers/23/03/2303.13874.json",
    "total_tokens": 910,
    "translated_title": "基于查询的视频表示用于时刻检索和亮点检测",
    "translated_abstract": "最近，随着对视频理解的需求急剧增加，视频时刻检索（MR）和亮点检测（HD）备受瞩目。MR / HD的关键目标是定位时刻并估计剪辑级别的符合程度，即突出显示的分数，以给定的文本查询为依据。尽管最近的基于Transformer的模型带来了一些进步，但我们发现这些方法没有充分利用给定查询的信息。例如，在预测时刻及其显著性时，有时会忽略文本查询和视频内容之间的相关性。为了解决这个问题，我们引入了一种针对MR / HD量身定制的检测Transformer——Query-Dependent DETR（QD-DETR）。由于我们观察到给定查询在Transformer架构中的重要性不太明显，我们的编码模块从交叉注意力层开始，以明确将文本查询的上下文注入视频表示。然后，为了增强模型利用查询信息的能力，我们操纵视频",
    "tldr": "该论文提出了一个基于查询的视频表示来解决视频时刻检索和亮点检测中存在的查询信息利用不充分的问题，该方法在编码模块中采用交叉注意力层将文本查询的上下文注入视频表示中。",
    "en_tdlr": "This paper proposes a query-dependent video representation to tackle the issue of inadequate utilization of query information in video moment retrieval and highlight detection. The method adopts cross-attention layers in the encoding module to explicitly inject the context of text query into video representation."
}