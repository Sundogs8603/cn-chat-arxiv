{
    "title": "GLASU: A Communication-Efficient Algorithm for Federated Learning with Vertically Distributed Graph Data. (arXiv:2303.09531v1 [cs.LG])",
    "abstract": "Vertical federated learning (VFL) is a distributed learning paradigm, where computing clients collectively train a model based on the partial features of the same set of samples they possess. Current research on VFL focuses on the case when samples are independent, but it rarely addresses an emerging scenario when samples are interrelated through a graph. For graph-structured data, graph neural networks (GNNs) are competitive machine learning models, but a naive implementation in the VFL setting causes a significant communication overhead. Moreover, the analysis of the training is faced with a challenge caused by the biased stochastic gradients. In this paper, we propose a model splitting method that splits a backbone GNN across the clients and the server and a communication-efficient algorithm, GLASU, to train such a model. GLASU adopts lazy aggregation and stale updates to skip aggregation when evaluating the model and skip feature exchanges during training, greatly reducing communic",
    "link": "http://arxiv.org/abs/2303.09531",
    "context": "Title: GLASU: A Communication-Efficient Algorithm for Federated Learning with Vertically Distributed Graph Data. (arXiv:2303.09531v1 [cs.LG])\nAbstract: Vertical federated learning (VFL) is a distributed learning paradigm, where computing clients collectively train a model based on the partial features of the same set of samples they possess. Current research on VFL focuses on the case when samples are independent, but it rarely addresses an emerging scenario when samples are interrelated through a graph. For graph-structured data, graph neural networks (GNNs) are competitive machine learning models, but a naive implementation in the VFL setting causes a significant communication overhead. Moreover, the analysis of the training is faced with a challenge caused by the biased stochastic gradients. In this paper, we propose a model splitting method that splits a backbone GNN across the clients and the server and a communication-efficient algorithm, GLASU, to train such a model. GLASU adopts lazy aggregation and stale updates to skip aggregation when evaluating the model and skip feature exchanges during training, greatly reducing communic",
    "path": "papers/23/03/2303.09531.json",
    "total_tokens": 943,
    "translated_abstract": "纵向联邦学习是一种分布式学习范式，其中计算客户端根据他们拥有的同一样本集的部分特征共同训练模型。目前对于纵向联邦学习的研究集中于样本相互独立的情况，但很少考虑样本存在图关系的情况。针对图结构数据，图神经网络是竞争性的机器学习模型，但在纵向联邦学习环境中的简单实现会导致显著的通信开销。此外，由于有偏的随机梯度下降，训练分析也面临挑战。本文提出了一种模型分割方法，将骨干GNN跨越客户端和服务器进行分割，以及一种通信高效的算法GLASU来训练这样的模型。GLASU采用懒惰的聚合和过期更新方法，在评估模型时跳过聚合、在训练期间跳过特征交换，极大地减少了通信开销。",
    "tldr": "提出了针对图数据的纵向联邦学习通信高效算法GLASU，采用模型分割、懒惰聚合和过期更新等方法来减少通信开销，对针对图结构数据的GNN进行分布式训练。",
    "en_tdlr": "A communication-efficient algorithm, GLASU, is proposed for vertically distributed graph data in federated learning. It adopts model splitting, lazy aggregation and stale updates to reduce communication overhead and enable distributed training of graph neural networks with interrelated samples."
}