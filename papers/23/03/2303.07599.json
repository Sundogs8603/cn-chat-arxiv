{
    "title": "A Contrastive Knowledge Transfer Framework for Model Compression and Transfer Learning. (arXiv:2303.07599v1 [cs.LG])",
    "abstract": "Knowledge Transfer (KT) achieves competitive performance and is widely used for image classification tasks in model compression and transfer learning. Existing KT works transfer the information from a large model (\"teacher\") to train a small model (\"student\") by minimizing the difference of their conditionally independent output distributions. However, these works overlook the high-dimension structural knowledge from the intermediate representations of the teacher, which leads to limited effectiveness, and they are motivated by various heuristic intuitions, which makes it difficult to generalize. This paper proposes a novel Contrastive Knowledge Transfer Framework (CKTF), which enables the transfer of sufficient structural knowledge from the teacher to the student by optimizing multiple contrastive objectives across the intermediate representations between them. Also, CKTF provides a generalized agreement to existing KT techniques and increases their performance significantly by derivi",
    "link": "http://arxiv.org/abs/2303.07599",
    "context": "Title: A Contrastive Knowledge Transfer Framework for Model Compression and Transfer Learning. (arXiv:2303.07599v1 [cs.LG])\nAbstract: Knowledge Transfer (KT) achieves competitive performance and is widely used for image classification tasks in model compression and transfer learning. Existing KT works transfer the information from a large model (\"teacher\") to train a small model (\"student\") by minimizing the difference of their conditionally independent output distributions. However, these works overlook the high-dimension structural knowledge from the intermediate representations of the teacher, which leads to limited effectiveness, and they are motivated by various heuristic intuitions, which makes it difficult to generalize. This paper proposes a novel Contrastive Knowledge Transfer Framework (CKTF), which enables the transfer of sufficient structural knowledge from the teacher to the student by optimizing multiple contrastive objectives across the intermediate representations between them. Also, CKTF provides a generalized agreement to existing KT techniques and increases their performance significantly by derivi",
    "path": "papers/23/03/2303.07599.json",
    "total_tokens": 862,
    "translated_title": "一种对比知识迁移框架用于模型压缩与迁移学习",
    "translated_abstract": "知识迁移（KT）在模型压缩和迁移学习中实现了竞争性能，并被广泛用于图像分类任务。现有的KT工作通过最小化“教师”模型和“学生”模型之间条件独立输出分布的差异来从大模型中传输信息。然而，这些方法忽略了教师模型中中间表示的高维结构知识，这导致有效性有限，并且这些方法受到各种启发式直觉的启发，使得普适性难以实现。本文提出了一种新颖的对比知识迁移框架（CKTF），它通过优化其之间的中间表示的多个对比目标，使教师向学生传递足够的结构化知识。此外，CKTF对现有的KT技术提供了一种广义协议，并通过导出高性能使其性能显著提高。",
    "tldr": "本论文提出了一个新的对比知识迁移框架（CKTF），通过优化教师和学生之间的中间表示的多个对比目标，使得CKTF能够传递足够的结构化知识，并提高了现有KT技术的性能。",
    "en_tdlr": "This paper proposes a novel Contrastive Knowledge Transfer Framework (CKTF), which enables the transfer of sufficient structural knowledge from the teacher to the student by optimizing multiple contrastive objectives across the intermediate representations between them."
}