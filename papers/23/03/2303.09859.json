{
    "title": "Trained on 100 million words and still in shape: BERT meets British National Corpus. (arXiv:2303.09859v1 [cs.CL])",
    "abstract": "While modern masked language models (LMs) are trained on ever larger corpora, we here explore the effects of down-scaling training to a modestly-sized but representative, well-balanced, and publicly available English text source -the British National Corpus. We show that pre-training on this carefully curated corpus can reach better performance than the original BERT model. We argue that this type of corpora has great potential as a language modeling benchmark. To showcase this potential, we present fair, reproducible and data-efficient comparative studies of LMs, in which we evaluate several training objectives and model architectures and replicate previous empirical results in a systematic way. We propose an optimized LM architecture called LTG-BERT.",
    "link": "http://arxiv.org/abs/2303.09859",
    "context": "Title: Trained on 100 million words and still in shape: BERT meets British National Corpus. (arXiv:2303.09859v1 [cs.CL])\nAbstract: While modern masked language models (LMs) are trained on ever larger corpora, we here explore the effects of down-scaling training to a modestly-sized but representative, well-balanced, and publicly available English text source -the British National Corpus. We show that pre-training on this carefully curated corpus can reach better performance than the original BERT model. We argue that this type of corpora has great potential as a language modeling benchmark. To showcase this potential, we present fair, reproducible and data-efficient comparative studies of LMs, in which we evaluate several training objectives and model architectures and replicate previous empirical results in a systematic way. We propose an optimized LM architecture called LTG-BERT.",
    "path": "papers/23/03/2303.09859.json",
    "total_tokens": 880,
    "translated_title": "经过训练的1亿单词仍然保持状态：BERT结合英国国家语料库",
    "translated_abstract": "当前，现代遮蔽语言模型（LMs）训练的语料库规模越来越大。在本文中，我们探讨了缩小训练规模到一个规模适中、代表性好、平衡性好且公开可用的英文文本源-英国国家语料库的效果。我们展示了在这个精心策划的语料库上预训练可以达到比原始BERT模型更好的表现。我们认为这种类型的语料库具有作为语言建模基准的巨大潜力。为了展示这种潜力，我们以公平、可重复和数据有效的比较研究为特色，在其中评估了几个训练目标和模型架构，并以系统性的方式复制了先前的经验结果。我们提出了一个经过优化的LM体系结构称为LTG-BERT。",
    "tldr": "本文探讨了在英国国家语料库上预训练的效果，并展示它可以比原始BERT模型达到更好的表现。在公平、可重复且数据有效的比较研究中，他们证明了这样的语料库有作为语言建模基准的巨大潜力。他们提出了一个经过优化的LM体系结构称为LTG-BERT。",
    "en_tdlr": "The paper explores pre-training on the British National Corpus and shows that it can achieve better performance than the original BERT model. They demonstrate the potential of such corpora as a language modeling benchmark and propose an optimized LM architecture called LTG-BERT through fair, reproducible and data-efficient comparative studies."
}