{
    "title": "Learning Generative Models with Goal-conditioned Reinforcement Learning. (arXiv:2303.14811v1 [cs.LG])",
    "abstract": "We present a novel, alternative framework for learning generative models with goal-conditioned reinforcement learning. We define two agents, a goal conditioned agent (GC-agent) and a supervised agent (S-agent). Given a user-input initial state, the GC-agent learns to reconstruct the training set. In this context, elements in the training set are the goals. During training, the S-agent learns to imitate the GC-agent while remaining agnostic of the goals. At inference we generate new samples with the S-agent. Following a similar route as in variational auto-encoders, we derive an upper bound on the negative log-likelihood that consists of a reconstruction term and a divergence between the GC-agent policy and the (goal-agnostic) S-agent policy. We empirically demonstrate that our method is able to generate diverse and high quality samples in the task of image synthesis.",
    "link": "http://arxiv.org/abs/2303.14811",
    "context": "Title: Learning Generative Models with Goal-conditioned Reinforcement Learning. (arXiv:2303.14811v1 [cs.LG])\nAbstract: We present a novel, alternative framework for learning generative models with goal-conditioned reinforcement learning. We define two agents, a goal conditioned agent (GC-agent) and a supervised agent (S-agent). Given a user-input initial state, the GC-agent learns to reconstruct the training set. In this context, elements in the training set are the goals. During training, the S-agent learns to imitate the GC-agent while remaining agnostic of the goals. At inference we generate new samples with the S-agent. Following a similar route as in variational auto-encoders, we derive an upper bound on the negative log-likelihood that consists of a reconstruction term and a divergence between the GC-agent policy and the (goal-agnostic) S-agent policy. We empirically demonstrate that our method is able to generate diverse and high quality samples in the task of image synthesis.",
    "path": "papers/23/03/2303.14811.json",
    "total_tokens": 810,
    "translated_title": "用目标条件的强化学习学习生成模型",
    "translated_abstract": "我们提出了一种新颖的、用目标条件的强化学习来学习生成模型的框架。我们定义了两个代理，一个是目标条件代理（GC-agent），另一个是监督代理（S-agent）。在给定用户输入的初始状态后，GC-agent学习重构训练集。在此情况下，训练集中的元素是目标。在训练过程中，S-agent学习在不知道目标的情况下模仿GC-agent。在推理阶段，我们使用S-agent生成新的样本。类似于变分自编码器，我们通过推导出一个上限来衡量负对数似然，它由重构项和GC-agent策略与（目标无关的）S-agent策略之间的差异组成。我们在图像合成任务中经验证明，我们的方法能够生成多样性和高质量的样本。",
    "tldr": "研究提出了用目标条件的强化学习来学习生成模型的框架，能够在训练集中生成多样性和高质量的样本。",
    "en_tdlr": "A novel framework for learning generative models with goal-conditioned reinforcement learning is proposed, which generates diverse and high-quality samples in the training set, and consists of a goal conditioned agent and a supervised agent."
}