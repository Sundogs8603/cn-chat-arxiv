{
    "title": "Gradient scarcity with Bilevel Optimization for Graph Learning. (arXiv:2303.13964v1 [cs.LG])",
    "abstract": "A common issue in graph learning under the semi-supervised setting is referred to as gradient scarcity. That is, learning graphs by minimizing a loss on a subset of nodes causes edges between unlabelled nodes that are far from labelled ones to receive zero gradients. The phenomenon was first described when optimizing the graph and the weights of a Graph Neural Network (GCN) with a joint optimization algorithm. In this work, we give a precise mathematical characterization of this phenomenon, and prove that it also emerges in bilevel optimization, where additional dependency exists between the parameters of the problem. While for GCNs gradient scarcity occurs due to their finite receptive field, we show that it also occurs with the Laplacian regularization model, in the sense that gradients amplitude decreases exponentially with distance to labelled nodes. To alleviate this issue, we study several solutions: we propose to resort to latent graph learning using a Graph-to-Graph model (G2G)",
    "link": "http://arxiv.org/abs/2303.13964",
    "context": "Title: Gradient scarcity with Bilevel Optimization for Graph Learning. (arXiv:2303.13964v1 [cs.LG])\nAbstract: A common issue in graph learning under the semi-supervised setting is referred to as gradient scarcity. That is, learning graphs by minimizing a loss on a subset of nodes causes edges between unlabelled nodes that are far from labelled ones to receive zero gradients. The phenomenon was first described when optimizing the graph and the weights of a Graph Neural Network (GCN) with a joint optimization algorithm. In this work, we give a precise mathematical characterization of this phenomenon, and prove that it also emerges in bilevel optimization, where additional dependency exists between the parameters of the problem. While for GCNs gradient scarcity occurs due to their finite receptive field, we show that it also occurs with the Laplacian regularization model, in the sense that gradients amplitude decreases exponentially with distance to labelled nodes. To alleviate this issue, we study several solutions: we propose to resort to latent graph learning using a Graph-to-Graph model (G2G)",
    "path": "papers/23/03/2303.13964.json",
    "total_tokens": 727,
    "translated_title": "基于二层优化的梯度稀缺问题在图学习中的应用",
    "translated_abstract": "在半监督学习下，图学习中普遍存在的问题是梯度稀缺现象。即学习一部分节点的损失会导致未标记节点之间的边缘收到零梯度。本文对这种现象进行了精确的数学刻画，并证明了它也存在于二层优化中，其中问题的参数存在额外的依赖关系。我们探讨了解决这个问题的几种方法，并提出了通过使用图到图模型（G2G）进行潜在图学习的方法。",
    "tldr": "本研究研究了二层优化算法在图学习中存在的梯度稀缺问题，并通过提出潜在图学习的方法来解决这一问题。",
    "en_tdlr": "This paper studies the gradient scarcity issue in graph learning with bilevel optimization, and proposes a latent graph learning method to alleviate the problem."
}