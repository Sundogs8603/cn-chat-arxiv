{
    "title": "TriPlaneNet: An Encoder for EG3D Inversion. (arXiv:2303.13497v1 [cs.CV])",
    "abstract": "Recent progress in NeRF-based GANs has introduced a number of approaches for high-resolution and high-fidelity generative modeling of human heads with a possibility for novel view rendering. At the same time, one must solve an inverse problem to be able to re-render or modify an existing image or video. Despite the success of universal optimization-based methods for 2D GAN inversion, those, applied to 3D GANs, may fail to produce 3D-consistent renderings. Fast encoder-based techniques, such as those developed for StyleGAN, may also be less appealing due to the lack of identity preservation. In our work, we introduce a real-time method that bridges the gap between the two approaches by directly utilizing the tri-plane representation introduced for EG3D generative model. In particular, we build upon a feed-forward convolutional encoder for the latent code and extend it with a fully-convolutional predictor of tri-plane numerical offsets. As shown in our work, the renderings are similar in",
    "link": "http://arxiv.org/abs/2303.13497",
    "context": "Title: TriPlaneNet: An Encoder for EG3D Inversion. (arXiv:2303.13497v1 [cs.CV])\nAbstract: Recent progress in NeRF-based GANs has introduced a number of approaches for high-resolution and high-fidelity generative modeling of human heads with a possibility for novel view rendering. At the same time, one must solve an inverse problem to be able to re-render or modify an existing image or video. Despite the success of universal optimization-based methods for 2D GAN inversion, those, applied to 3D GANs, may fail to produce 3D-consistent renderings. Fast encoder-based techniques, such as those developed for StyleGAN, may also be less appealing due to the lack of identity preservation. In our work, we introduce a real-time method that bridges the gap between the two approaches by directly utilizing the tri-plane representation introduced for EG3D generative model. In particular, we build upon a feed-forward convolutional encoder for the latent code and extend it with a fully-convolutional predictor of tri-plane numerical offsets. As shown in our work, the renderings are similar in",
    "path": "papers/23/03/2303.13497.json",
    "total_tokens": 989,
    "translated_title": "TriPlaneNet：一种EG3D反演的编码器",
    "translated_abstract": "最近，基于NeRF的GAN取得了很大的进展，在高分辨率和高保真度的生成建模中引入了许多方法，可以进行新颖的视角渲染。与此同时，为了能够重新渲染或修改现有的图像或视频，必须解决一个反问题。尽管对于2D GAN反演而言，通用的基于优化的方法取得了成功，但对于3D GAN反演，这些方法可能无法产生3D一致的渲染。而像StyleGAN这样的快速编码器技术，可能也不太吸引人，因为它们缺乏身份保留能力。在我们的工作中，我们介绍了一种实时方法，通过直接利用为EG3D生成模型引入的三平面表示，弥合了这两种方法之间的差距。具体而言，我们建立在一个用于潜在编码的前馈卷积编码器上，并扩展了一个完全卷积的三平面数值偏移预测器。正如我们的工作所显示的那样，渲染结果相似。",
    "tldr": "本研究介绍了一种实时方法TriPlaneNet，通过直接利用EG3D生成模型的三平面表示，建立在一个用于潜在编码的前馈卷积编码器上，并扩展了一个完全卷积的三平面数值偏移预测器，旨在弥合现有的GAN反演方法的差距。",
    "en_tdlr": "This paper introduces a real-time method TriPlaneNet, which bridges the gap between universal optimization-based methods and fast encoder-based techniques for 3D GAN inversion. It directly utilizes the tri-plane representation introduced for EG3D generative model and extends a feed-forward convolutional encoder for the latent code with a fully-convolutional predictor of tri-plane numerical offsets."
}