{
    "title": "Mask-Free Video Instance Segmentation. (arXiv:2303.15904v1 [cs.CV])",
    "abstract": "The recent advancement in Video Instance Segmentation (VIS) has largely been driven by the use of deeper and increasingly data-hungry transformer-based models. However, video masks are tedious and expensive to annotate, limiting the scale and diversity of existing VIS datasets. In this work, we aim to remove the mask-annotation requirement. We propose MaskFreeVIS, achieving highly competitive VIS performance, while only using bounding box annotations for the object state. We leverage the rich temporal mask consistency constraints in videos by introducing the Temporal KNN-patch Loss (TK-Loss), providing strong mask supervision without any labels. Our TK-Loss finds one-to-many matches across frames, through an efficient patch-matching step followed by a K-nearest neighbor selection. A consistency loss is then enforced on the found matches. Our mask-free objective is simple to implement, has no trainable parameters, is computationally efficient, yet outperforms baselines employing, e.g., ",
    "link": "http://arxiv.org/abs/2303.15904",
    "context": "Title: Mask-Free Video Instance Segmentation. (arXiv:2303.15904v1 [cs.CV])\nAbstract: The recent advancement in Video Instance Segmentation (VIS) has largely been driven by the use of deeper and increasingly data-hungry transformer-based models. However, video masks are tedious and expensive to annotate, limiting the scale and diversity of existing VIS datasets. In this work, we aim to remove the mask-annotation requirement. We propose MaskFreeVIS, achieving highly competitive VIS performance, while only using bounding box annotations for the object state. We leverage the rich temporal mask consistency constraints in videos by introducing the Temporal KNN-patch Loss (TK-Loss), providing strong mask supervision without any labels. Our TK-Loss finds one-to-many matches across frames, through an efficient patch-matching step followed by a K-nearest neighbor selection. A consistency loss is then enforced on the found matches. Our mask-free objective is simple to implement, has no trainable parameters, is computationally efficient, yet outperforms baselines employing, e.g., ",
    "path": "papers/23/03/2303.15904.json",
    "total_tokens": 954,
    "translated_abstract": "近年来，深度和数据需求更高的变换器模型驱动了视频实例分割（VIS）的发展。然而，视频掩膜的注释是冗杂和昂贵的，这限制了现有VIS数据集的规模和多样性。本文旨在消除遮罩注释的要求。我们提出了MaskFreeVIS，它通过仅使用对象状态的边界框注释，实现了竞争性的VIS性能。我们利用了视频中丰富的时间掩码一致性约束，引入了时间KNN补丁损失（TK-Loss），提供了强大的遮罩监督，而不需要任何标签。通过有效的补丁匹配步骤，我们的TK-Loss在帧之间找到一对多的匹配，随后进行K最近邻选择。然后在发现的匹配上强制执行一致性损失。我们的无掩膜目标易于实现，没有可训练参数，计算效率高，但胜过利用遮罩的基线。",
    "tldr": "本文提出了MaskFreeVIS，通过仅使用对象状态的边界框注释，消除了遮罩注释的要求，通过时间KNN补丁损失提供了无标签的强大遮罩监督，实现了竞争性的视频实例分割性能，对于解决遮罩注释的昂贵和冗杂问题具有重要意义。",
    "en_tdlr": "This paper proposes MaskFreeVIS, which eliminates the annotation requirement for masks by only using bounding box annotations for the object state. The use of Temporal KNN-patch Loss (TK-Loss) provides strong mask supervision without any labels and achieves competitive performance for video instance segmentation. It is significant in addressing the expensive and tedious issue of mask annotation."
}