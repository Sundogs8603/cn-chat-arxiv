{
    "title": "Memory-adaptive Depth-wise Heterogenous Federated Learning. (arXiv:2303.04887v2 [cs.LG] UPDATED)",
    "abstract": "Federated learning is a promising paradigm that allows multiple clients to collaboratively train a model without sharing the local data. However, the presence of heterogeneous devices in federated learning, such as mobile phones and IoT devices with varying memory capabilities, would limit the scale and hence the performance of the model could be trained. The mainstream approaches to address memory limitations focus on width-slimming techniques, where different clients train subnetworks with reduced widths locally and then the server aggregates the subnetworks. The global model produced from these methods suffers from performance degradation due to the negative impact of the actions taken to handle the varying subnetwork widths in the aggregation phase. In this paper, we introduce a memory-adaptive depth-wise learning solution in FL called FeDepth, which adaptively decomposes the full model into blocks according to the memory budgets of each client and trains blocks sequentially to obt",
    "link": "http://arxiv.org/abs/2303.04887",
    "context": "Title: Memory-adaptive Depth-wise Heterogenous Federated Learning. (arXiv:2303.04887v2 [cs.LG] UPDATED)\nAbstract: Federated learning is a promising paradigm that allows multiple clients to collaboratively train a model without sharing the local data. However, the presence of heterogeneous devices in federated learning, such as mobile phones and IoT devices with varying memory capabilities, would limit the scale and hence the performance of the model could be trained. The mainstream approaches to address memory limitations focus on width-slimming techniques, where different clients train subnetworks with reduced widths locally and then the server aggregates the subnetworks. The global model produced from these methods suffers from performance degradation due to the negative impact of the actions taken to handle the varying subnetwork widths in the aggregation phase. In this paper, we introduce a memory-adaptive depth-wise learning solution in FL called FeDepth, which adaptively decomposes the full model into blocks according to the memory budgets of each client and trains blocks sequentially to obt",
    "path": "papers/23/03/2303.04887.json",
    "total_tokens": 911,
    "translated_title": "可变深度异构联邦学习的内存自适应模型",
    "translated_abstract": "联邦学习是一种有前途的范式，允许多个客户端在不共享本地数据的情况下协同训练模型。然而，在联邦学习中存在异构设备，如手机和物联网设备的内存能力不同，会限制模型能够训练的规模和性能。主要解决内存限制的方法集中在减少宽度的技术上，即不同客户端在本地训练减宽度的子网络，然后服务器聚合这些子网络。由于处理聚合阶段中不同子网络宽度变化的负面影响，这些方法产生的全局模型会受到性能的降低。在本文中，我们介绍了一种称为FeDepth的内存自适应深度学习解决方案，它根据每个客户端的内存预算将完整模型自适应地分解成块，并依次训练这些块，以获取更好的性能和可扩展性。",
    "tldr": "这项研究介绍了一种名为FeDepth的内存自适应深度学习解决方案，它根据每个客户端的内存预算将完整模型自适应地分解成块，并依次训练这些块，以解决联邦学习中异构设备的内存限制问题。",
    "en_tdlr": "This research introduces a memory-adaptive deep learning solution called FeDepth, which decomposes the full model into blocks according to the memory budgets of each client and trains them sequentially to address the memory limitations of heterogeneous devices in federated learning."
}