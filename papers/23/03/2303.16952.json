{
    "title": "Meta-Learning Parameterized First-Order Optimizers using Differentiable Convex Optimization. (arXiv:2303.16952v1 [cs.LG])",
    "abstract": "Conventional optimization methods in machine learning and controls rely heavily on first-order update rules. Selecting the right method and hyperparameters for a particular task often involves trial-and-error or practitioner intuition, motivating the field of meta-learning. We generalize a broad family of preexisting update rules by proposing a meta-learning framework in which the inner loop optimization step involves solving a differentiable convex optimization (DCO). We illustrate the theoretical appeal of this approach by showing that it enables one-step optimization of a family of linear least squares problems, given that the meta-learner has sufficient exposure to similar tasks. Various instantiations of the DCO update rule are compared to conventional optimizers on a range of illustrative experimental settings.",
    "link": "http://arxiv.org/abs/2303.16952",
    "context": "Title: Meta-Learning Parameterized First-Order Optimizers using Differentiable Convex Optimization. (arXiv:2303.16952v1 [cs.LG])\nAbstract: Conventional optimization methods in machine learning and controls rely heavily on first-order update rules. Selecting the right method and hyperparameters for a particular task often involves trial-and-error or practitioner intuition, motivating the field of meta-learning. We generalize a broad family of preexisting update rules by proposing a meta-learning framework in which the inner loop optimization step involves solving a differentiable convex optimization (DCO). We illustrate the theoretical appeal of this approach by showing that it enables one-step optimization of a family of linear least squares problems, given that the meta-learner has sufficient exposure to similar tasks. Various instantiations of the DCO update rule are compared to conventional optimizers on a range of illustrative experimental settings.",
    "path": "papers/23/03/2303.16952.json",
    "total_tokens": 812,
    "translated_title": "使用可微凸优化元学习参数化的一阶优化器",
    "translated_abstract": "机器学习和控制中的传统优化方法主要依赖于一阶更新规则。针对某个任务选择合适方法和超参数常常需要试错或从业者直觉，这促进了元学习领域的发展。我们通过提出内循环优化步骤涉及可微凸优化(DCO)的元学习框架，推广了一个广泛的现有更新规则家族。我们通过展示此方法的理论吸引力，证明了在元学习者有足够的类似任务经验的情况下，它可以一步优化一系列线性最小二乘问题。在一系列说明性实验设置中，将DCO更新规则的各种实例与传统优化器进行了比较。",
    "tldr": "该研究提出了使用可微凸优化的元学习框架，将现有的一阶更新规则推广到更广的家族，证明在元学习者有足够类似任务的经验下，可以一步优化一系列线性最小二乘问题。",
    "en_tdlr": "This study proposes a meta-learning framework using differentiable convex optimization (DCO) to generalize a wide family of existing first-order update rules, demonstrating that, given sufficient experience with similar tasks, it is possible to optimize a family of linear least squares problems in one step. Various instantiations of the DCO update rule were compared to traditional optimizers in illustrative experimental settings."
}