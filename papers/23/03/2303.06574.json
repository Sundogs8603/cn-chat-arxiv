{
    "title": "Diffusion Models for Non-autoregressive Text Generation: A Survey. (arXiv:2303.06574v2 [cs.CL] UPDATED)",
    "abstract": "Non-autoregressive (NAR) text generation has attracted much attention in the field of natural language processing, which greatly reduces the inference latency but has to sacrifice the generation accuracy. Recently, diffusion models, a class of latent variable generative models, have been introduced into NAR text generation, showing an improved text generation quality. In this survey, we review the recent progress in diffusion models for NAR text generation. As the background, we first present the general definition of diffusion models and the text diffusion models, and then discuss their merits for NAR generation. As the core content, we further introduce two mainstream diffusion models in existing work of text diffusion, and review the key designs of the diffusion process. Moreover, we discuss the utilization of pre-trained language models (PLMs) for text diffusion models and introduce optimization techniques for text data. Finally, we discuss several promising directions and conclude",
    "link": "http://arxiv.org/abs/2303.06574",
    "context": "Title: Diffusion Models for Non-autoregressive Text Generation: A Survey. (arXiv:2303.06574v2 [cs.CL] UPDATED)\nAbstract: Non-autoregressive (NAR) text generation has attracted much attention in the field of natural language processing, which greatly reduces the inference latency but has to sacrifice the generation accuracy. Recently, diffusion models, a class of latent variable generative models, have been introduced into NAR text generation, showing an improved text generation quality. In this survey, we review the recent progress in diffusion models for NAR text generation. As the background, we first present the general definition of diffusion models and the text diffusion models, and then discuss their merits for NAR generation. As the core content, we further introduce two mainstream diffusion models in existing work of text diffusion, and review the key designs of the diffusion process. Moreover, we discuss the utilization of pre-trained language models (PLMs) for text diffusion models and introduce optimization techniques for text data. Finally, we discuss several promising directions and conclude",
    "path": "papers/23/03/2303.06574.json",
    "total_tokens": 885,
    "translated_title": "非自回归文本生成的扩散模型: 一项综述",
    "translated_abstract": "非自回归（NAR）文本生成在自然语言处理领域引起了广泛关注，大大降低了推理延迟，但不得不牺牲生成准确性。最近，一类潜变量生成模型——扩散模型已被引入到NAR文本生成中，展现了更好的文本生成质量。本综述对扩散模型在NAR文本生成中最近的进展进行了回顾。我们首先介绍了扩散模型的通用定义和文本扩散模型，并讨论了它们在NAR生成中的优点。随后，我们介绍了现有文本扩散工作中两种主流扩散模型，并回顾了扩散过程的关键设计。此外，我们讨论了语言预训练模型（PLMs）在文本扩散模型中的使用，并介绍了文本数据的优化技术。最后，我们讨论了几个有希望的方向，并作出结论。",
    "tldr": "本文综述了扩散模型在非自回归文本生成中的最新进展，包括两种主流扩散模型，以及语言预训练模型在文本扩散模型中的应用和文本数据的优化技术。"
}