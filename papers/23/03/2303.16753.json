{
    "title": "Scaling Pre-trained Language Models to Deeper via Parameter-efficient Architecture. (arXiv:2303.16753v1 [cs.CL])",
    "abstract": "In this paper, we propose a highly parameter-efficient approach to scaling pre-trained language models (PLMs) to a deeper model depth. Unlike prior work that shares all parameters or uses extra blocks, we design a more capable parameter-sharing architecture based on matrix product operator (MPO). MPO decomposition can reorganize and factorize the information of a parameter matrix into two parts: the major part that contains the major information (central tensor) and the supplementary part that only has a small proportion of parameters (auxiliary tensors). Based on such a decomposition, our architecture shares the central tensor across all layers for reducing the model size and meanwhile keeps layer-specific auxiliary tensors (also using adapters) for enhancing the adaptation flexibility. To improve the model training, we further propose a stable initialization algorithm tailored for the MPO-based architecture. Extensive experiments have demonstrated the effectiveness of our proposed mo",
    "link": "http://arxiv.org/abs/2303.16753",
    "context": "Title: Scaling Pre-trained Language Models to Deeper via Parameter-efficient Architecture. (arXiv:2303.16753v1 [cs.CL])\nAbstract: In this paper, we propose a highly parameter-efficient approach to scaling pre-trained language models (PLMs) to a deeper model depth. Unlike prior work that shares all parameters or uses extra blocks, we design a more capable parameter-sharing architecture based on matrix product operator (MPO). MPO decomposition can reorganize and factorize the information of a parameter matrix into two parts: the major part that contains the major information (central tensor) and the supplementary part that only has a small proportion of parameters (auxiliary tensors). Based on such a decomposition, our architecture shares the central tensor across all layers for reducing the model size and meanwhile keeps layer-specific auxiliary tensors (also using adapters) for enhancing the adaptation flexibility. To improve the model training, we further propose a stable initialization algorithm tailored for the MPO-based architecture. Extensive experiments have demonstrated the effectiveness of our proposed mo",
    "path": "papers/23/03/2303.16753.json",
    "total_tokens": 712,
    "translated_title": "使用参数有效的结构将预训练语言模型扩展至更深的深度",
    "translated_abstract": "本文提出了一种高度参数有效的方法，通过使用基于矩阵乘积算子（MPO）的更具能力的参数共享架构，将预训练语言模型（PLMs）扩展到更深的模型深度。通过MPO分解，我们的架构跨所有层共享中央张量以减少模型大小，并保持层特定的辅助张量以增强适应性灵活性。",
    "tldr": "本文提出了一种参数有效的结构，通过MPO分解共享中央张量并保持层特定的辅助张量，将预训练语言模型扩展到更深的深度。",
    "en_tdlr": "This paper proposes a parameter-efficient approach to scaling pre-trained language models (PLMs) to a deeper depth, by sharing the central tensor using matrix product operator (MPO) decomposition and keeping layer-specific auxiliary tensors, which has been demonstrated effective through extensive experiments."
}