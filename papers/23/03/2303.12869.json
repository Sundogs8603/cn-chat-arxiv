{
    "title": "JaCoText: A Pretrained Model for Java Code-Text Generation. (arXiv:2303.12869v1 [cs.CL])",
    "abstract": "Pretrained transformer-based models have shown high performance in natural language generation task. However, a new wave of interest has surged: automatic programming language generation. This task consists of translating natural language instructions to a programming code. Despite the fact that well-known pretrained models on language generation have achieved good performance in learning programming languages, effort is still needed in automatic code generation. In this paper, we introduce JaCoText, a model based on Transformers neural network. It aims to generate java source code from natural language text. JaCoText leverages advantages of both natural language and code generation models. More specifically, we study some findings from the state of the art and use them to (1) initialize our model from powerful pretrained models, (2) explore additional pretraining on our java dataset, (3) carry out experiments combining the unimodal and bimodal data in the training, and (4) scale the i",
    "link": "http://arxiv.org/abs/2303.12869",
    "context": "Title: JaCoText: A Pretrained Model for Java Code-Text Generation. (arXiv:2303.12869v1 [cs.CL])\nAbstract: Pretrained transformer-based models have shown high performance in natural language generation task. However, a new wave of interest has surged: automatic programming language generation. This task consists of translating natural language instructions to a programming code. Despite the fact that well-known pretrained models on language generation have achieved good performance in learning programming languages, effort is still needed in automatic code generation. In this paper, we introduce JaCoText, a model based on Transformers neural network. It aims to generate java source code from natural language text. JaCoText leverages advantages of both natural language and code generation models. More specifically, we study some findings from the state of the art and use them to (1) initialize our model from powerful pretrained models, (2) explore additional pretraining on our java dataset, (3) carry out experiments combining the unimodal and bimodal data in the training, and (4) scale the i",
    "path": "papers/23/03/2303.12869.json",
    "total_tokens": 644,
    "translated_title": "JaCoText: 一种用于Java代码文本生成的预训练模型",
    "translated_abstract": "基于预训练的transformer模型在自然语言生成任务中表现出高性能。然而，一股新的兴趣浪潮涌起：自动生成编程语言。这项任务包括将自然语言指令转换为编程代码。本文介绍了JaCoText，这是一种基于Transformers神经网络的模型，旨在从自然语言文本生成Java源代码。",
    "tldr": "JaCoText是一种使用Transformer神经网络从自然语言文本生成Java源代码的预训练模型。"
}