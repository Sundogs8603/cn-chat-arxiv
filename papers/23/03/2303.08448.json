{
    "title": "A Cross-institutional Evaluation on Breast Cancer Phenotyping NLP Algorithms on Electronic Health Records. (arXiv:2303.08448v1 [cs.CL])",
    "abstract": "Objective: The generalizability of clinical large language models is usually ignored during the model development process. This study evaluated the generalizability of BERT-based clinical NLP models across different clinical settings through a breast cancer phenotype extraction task.  Materials and Methods: Two clinical corpora of breast cancer patients were collected from the electronic health records from the University of Minnesota and the Mayo Clinic, and annotated following the same guideline. We developed three types of NLP models (i.e., conditional random field, bi-directional long short-term memory and CancerBERT) to extract cancer phenotypes from clinical texts. The models were evaluated for their generalizability on different test sets with different learning strategies (model transfer vs. locally trained). The entity coverage score was assessed with their association with the model performances.  Results: We manually annotated 200 and 161 clinical documents at UMN and MC, re",
    "link": "http://arxiv.org/abs/2303.08448",
    "context": "Title: A Cross-institutional Evaluation on Breast Cancer Phenotyping NLP Algorithms on Electronic Health Records. (arXiv:2303.08448v1 [cs.CL])\nAbstract: Objective: The generalizability of clinical large language models is usually ignored during the model development process. This study evaluated the generalizability of BERT-based clinical NLP models across different clinical settings through a breast cancer phenotype extraction task.  Materials and Methods: Two clinical corpora of breast cancer patients were collected from the electronic health records from the University of Minnesota and the Mayo Clinic, and annotated following the same guideline. We developed three types of NLP models (i.e., conditional random field, bi-directional long short-term memory and CancerBERT) to extract cancer phenotypes from clinical texts. The models were evaluated for their generalizability on different test sets with different learning strategies (model transfer vs. locally trained). The entity coverage score was assessed with their association with the model performances.  Results: We manually annotated 200 and 161 clinical documents at UMN and MC, re",
    "path": "papers/23/03/2303.08448.json",
    "total_tokens": 1158,
    "translated_title": "通过对电子病历的乳腺癌表型NLP算法进行跨机构评估",
    "translated_abstract": "目标：在模型开发过程中，通常忽略临床大型语言模型的泛化能力。本研究通过乳腺癌表型提取任务，评估了基于BERT的临床NLP模型在不同临床环境下的泛化能力。方法：从明尼苏达大学和梅奥诊所的电子病历中收集了两种乳腺癌患者的临床语料库，并按照同一指南进行注释。我们开发了三种类型的NLP模型（条件随机场、双向长短期记忆和CancerBERT），从临床文本中提取癌症表型。使用不同的学习策略（模型转移与本地训练）对模型在不同测试集上进行泛化能力评估。评估实体覆盖率与模型性能的相关性得分。结果：在UMN和MC手动注释了200和161份临床文档。CancerBERT模型达到了最高的F1分数（0.896）和实体覆盖率（98.8%），优于其他模型。模型转移方法在两个机构中产生了类似于本地训练模型的结果，表明跨机构存在潜在的泛化性。结论：本研究展示了在不同临床环境中评估NLP模型的重要性，并强调了使用转移学习开发广义临床NLP模型的潜力。",
    "tldr": "本研究通过对乳腺癌表型提取任务的评估，展示了基于BERT的临床NLP模型在不同临床环境中具有良好的泛化能力，并强调了使用转移学习开发广义临床NLP模型的潜力。",
    "en_tdlr": "This study evaluated the generalizability of BERT-based clinical NLP models across different clinical settings through a breast cancer phenotype extraction task, demonstrating their good potential for transfer learning in developing generalized clinical NLP models. The CancerBERT model achieved the best performance among all models tested."
}