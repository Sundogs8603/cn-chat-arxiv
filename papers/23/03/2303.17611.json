{
    "title": "Transformer-based Self-supervised Multimodal Representation Learning for Wearable Emotion Recognition. (arXiv:2303.17611v1 [cs.HC])",
    "abstract": "Recently, wearable emotion recognition based on peripheral physiological signals has drawn massive attention due to its less invasive nature and its applicability in real-life scenarios. However, how to effectively fuse multimodal data remains a challenging problem. Moreover, traditional fully-supervised based approaches suffer from overfitting given limited labeled data. To address the above issues, we propose a novel self-supervised learning (SSL) framework for wearable emotion recognition, where efficient multimodal fusion is realized with temporal convolution-based modality-specific encoders and a transformer-based shared encoder, capturing both intra-modal and inter-modal correlations. Extensive unlabeled data is automatically assigned labels by five signal transforms, and the proposed SSL model is pre-trained with signal transformation recognition as a pretext task, allowing the extraction of generalized multimodal representations for emotion-related downstream tasks. For evaluat",
    "link": "http://arxiv.org/abs/2303.17611",
    "context": "Title: Transformer-based Self-supervised Multimodal Representation Learning for Wearable Emotion Recognition. (arXiv:2303.17611v1 [cs.HC])\nAbstract: Recently, wearable emotion recognition based on peripheral physiological signals has drawn massive attention due to its less invasive nature and its applicability in real-life scenarios. However, how to effectively fuse multimodal data remains a challenging problem. Moreover, traditional fully-supervised based approaches suffer from overfitting given limited labeled data. To address the above issues, we propose a novel self-supervised learning (SSL) framework for wearable emotion recognition, where efficient multimodal fusion is realized with temporal convolution-based modality-specific encoders and a transformer-based shared encoder, capturing both intra-modal and inter-modal correlations. Extensive unlabeled data is automatically assigned labels by five signal transforms, and the proposed SSL model is pre-trained with signal transformation recognition as a pretext task, allowing the extraction of generalized multimodal representations for emotion-related downstream tasks. For evaluat",
    "path": "papers/23/03/2303.17611.json",
    "total_tokens": 895,
    "translated_title": "基于Transformer自监督多模态表示学习的可穿戴情感识别",
    "translated_abstract": "近年来，基于周边生理信号的可穿戴情感识别因其非侵入性和在实际场景中的应用性而受到广泛关注。然而，如何有效地融合多模态数据仍是一个具有挑战性的问题。另外，传统的完全监督式方法在有限的有标签数据下容易出现过拟合。针对上述问题，我们提出了一个新颖的自监督学习框架，其中通过基于时间卷积的模态特定编码器和基于Transformer的共享编码器实现了高效的多模态融合，捕获了模态内和模态间的相关性。利用5个信号变换，大量未标记的数据被自动赋予标签，所提出的SSL模型通过信号变换的识别作为掩饰任务进行预训练，允许提取用于情感相关下游任务的广义多模态表示。",
    "tldr": "提出了一种基于Transformer的自监督多模态表示学习方法，其中采用时间卷积特定编码器和共享编码器实现高效的多模态融合，利用自动标记的未标记数据对模型进行了预训练，可用于情感相关下游任务。",
    "en_tdlr": "A transformer-based self-supervised multimodal representation learning method is proposed for wearable emotion recognition, which adopts temporal convolution-based modality-specific encoders and a shared encoder to realize efficient multimodal fusion, and pre-trains with automatically labeled unlabeled data for downstream emotion-related tasks."
}