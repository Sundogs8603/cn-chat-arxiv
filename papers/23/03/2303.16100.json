{
    "title": "Energy-efficient Task Adaptation for NLP Edge Inference Leveraging Heterogeneous Memory Architectures. (arXiv:2303.16100v1 [cs.LG])",
    "abstract": "Executing machine learning inference tasks on resource-constrained edge devices requires careful hardware-software co-design optimizations. Recent examples have shown how transformer-based deep neural network models such as ALBERT can be used to enable the execution of natural language processing (NLP) inference on mobile systems-on-chip housing custom hardware accelerators. However, while these existing solutions are effective in alleviating the latency, energy, and area costs of running single NLP tasks, achieving multi-task inference requires running computations over multiple variants of the model parameters, which are tailored to each of the targeted tasks. This approach leads to either prohibitive on-chip memory requirements or paying the cost of off-chip memory access. This paper proposes adapter-ALBERT, an efficient model optimization for maximal data reuse across different tasks. The proposed model's performance and robustness to data compression methods are evaluated across s",
    "link": "http://arxiv.org/abs/2303.16100",
    "context": "Title: Energy-efficient Task Adaptation for NLP Edge Inference Leveraging Heterogeneous Memory Architectures. (arXiv:2303.16100v1 [cs.LG])\nAbstract: Executing machine learning inference tasks on resource-constrained edge devices requires careful hardware-software co-design optimizations. Recent examples have shown how transformer-based deep neural network models such as ALBERT can be used to enable the execution of natural language processing (NLP) inference on mobile systems-on-chip housing custom hardware accelerators. However, while these existing solutions are effective in alleviating the latency, energy, and area costs of running single NLP tasks, achieving multi-task inference requires running computations over multiple variants of the model parameters, which are tailored to each of the targeted tasks. This approach leads to either prohibitive on-chip memory requirements or paying the cost of off-chip memory access. This paper proposes adapter-ALBERT, an efficient model optimization for maximal data reuse across different tasks. The proposed model's performance and robustness to data compression methods are evaluated across s",
    "path": "papers/23/03/2303.16100.json",
    "total_tokens": 851,
    "translated_title": "基于异构内存结构的边缘NLP推理的节能任务适应性",
    "translated_abstract": "在资源受限的边缘设备上执行机器学习推理任务需要仔细的硬件-软件协同设计优化。最近的研究表明，基于变压器的深度神经网络模型，如ALBERT可以用于在移动系统级芯片上执行自然语言处理（NLP）推理，并配备自定义硬件加速器。然而，这些现有的解决方案虽然可以有效地缓解运行单个NLP任务的延迟、能量和面积成本，但对于实现多任务推理，需要在针对每个目标任务的多个模型参数的基础上执行计算。这种方法会导致过高的芯片内存需求或支付芯片外部存储器访问的成本。本文提出了一种高效的模型优化方法，即adapter-ALBERT，用于在不同任务之间实现最大化数据重用性。该模型的性能和对数据压缩方法的鲁棒性进行了评估。",
    "tldr": "adapter-ALBERT是一种高效的NLP模型优化方法，可以在实现多任务推理同时最大程度地实现数据重用，并提高对数据压缩方法的鲁棒性。"
}