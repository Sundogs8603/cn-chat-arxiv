{
    "title": "Generalist: Decoupling Natural and Robust Generalization. (arXiv:2303.13813v1 [cs.CV])",
    "abstract": "Deep neural networks obtained by standard training have been constantly plagued by adversarial examples. Although adversarial training demonstrates its capability to defend against adversarial examples, unfortunately, it leads to an inevitable drop in the natural generalization. To address the issue, we decouple the natural generalization and the robust generalization from joint training and formulate different training strategies for each one. Specifically, instead of minimizing a global loss on the expectation over these two generalization errors, we propose a bi-expert framework called \\emph{Generalist} where we simultaneously train base learners with task-aware strategies so that they can specialize in their own fields. The parameters of base learners are collected and combined to form a global learner at intervals during the training process. The global learner is then distributed to the base learners as initialized parameters for continued training. Theoretically, we prove that t",
    "link": "http://arxiv.org/abs/2303.13813",
    "context": "Title: Generalist: Decoupling Natural and Robust Generalization. (arXiv:2303.13813v1 [cs.CV])\nAbstract: Deep neural networks obtained by standard training have been constantly plagued by adversarial examples. Although adversarial training demonstrates its capability to defend against adversarial examples, unfortunately, it leads to an inevitable drop in the natural generalization. To address the issue, we decouple the natural generalization and the robust generalization from joint training and formulate different training strategies for each one. Specifically, instead of minimizing a global loss on the expectation over these two generalization errors, we propose a bi-expert framework called \\emph{Generalist} where we simultaneously train base learners with task-aware strategies so that they can specialize in their own fields. The parameters of base learners are collected and combined to form a global learner at intervals during the training process. The global learner is then distributed to the base learners as initialized parameters for continued training. Theoretically, we prove that t",
    "path": "papers/23/03/2303.13813.json",
    "total_tokens": 918,
    "translated_abstract": "标准训练得到的深度神经网络一直受到对抗性样本的影响。尽管对抗性训练展示了抵御对抗性样本的能力，但不幸的是，它导致了自然泛化的不可避免下降。为了解决这个问题，我们将自然泛化和鲁棒泛化从联合训练中解耦，并为每个泛化设计了不同的训练策略。具体来说，我们提出了一种名为\\emph{Generalist}的双专家框架，通过任务感知策略同时训练基学习器，使它们可以专注于自己的领域。在训练过程中，基学习器的参数被收集并组合起来形成全局学习器。然后将全局学习器分配给基学习器作为初始化参数进行持续训练。理论上，我们证明了该模型具有良好的性能。",
    "tldr": "该论文提出了一种名为Generalist的双专家框架，解耦了自然泛化和鲁棒泛化以并分别设计了不同的训练策略。在训练过程中，全局学习器被分配给基学习器作为初始化参数进行持续训练，从而提高模型的性能表现。",
    "en_tdlr": "The paper proposes a bi-expert framework named Generalist to decouple natural and robust generalization and formulate different training strategies for each one. During the training process, the global learner is distributed to the base learners as initialized parameters for continued training, which improves the model's performance."
}