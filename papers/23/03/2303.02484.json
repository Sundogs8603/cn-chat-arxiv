{
    "title": "Multi-Symmetry Ensembles: Improving Diversity and Generalization via Opposing Symmetries. (arXiv:2303.02484v2 [cs.LG] UPDATED)",
    "abstract": "Deep ensembles (DE) have been successful in improving model performance by learning diverse members via the stochasticity of random initialization. While recent works have attempted to promote further diversity in DE via hyperparameters or regularizing loss functions, these methods primarily still rely on a stochastic approach to explore the hypothesis space. In this work, we present Multi-Symmetry Ensembles (MSE), a framework for constructing diverse ensembles by capturing the multiplicity of hypotheses along symmetry axes, which explore the hypothesis space beyond stochastic perturbations of model weights and hyperparameters. We leverage recent advances in contrastive representation learning to create models that separately capture opposing hypotheses of invariant and equivariant functional classes and present a simple ensembling approach to efficiently combine appropriate hypotheses for a given task. We show that MSE effectively captures the multiplicity of conflicting hypotheses th",
    "link": "http://arxiv.org/abs/2303.02484",
    "context": "Title: Multi-Symmetry Ensembles: Improving Diversity and Generalization via Opposing Symmetries. (arXiv:2303.02484v2 [cs.LG] UPDATED)\nAbstract: Deep ensembles (DE) have been successful in improving model performance by learning diverse members via the stochasticity of random initialization. While recent works have attempted to promote further diversity in DE via hyperparameters or regularizing loss functions, these methods primarily still rely on a stochastic approach to explore the hypothesis space. In this work, we present Multi-Symmetry Ensembles (MSE), a framework for constructing diverse ensembles by capturing the multiplicity of hypotheses along symmetry axes, which explore the hypothesis space beyond stochastic perturbations of model weights and hyperparameters. We leverage recent advances in contrastive representation learning to create models that separately capture opposing hypotheses of invariant and equivariant functional classes and present a simple ensembling approach to efficiently combine appropriate hypotheses for a given task. We show that MSE effectively captures the multiplicity of conflicting hypotheses th",
    "path": "papers/23/03/2303.02484.json",
    "total_tokens": 887,
    "translated_title": "多对称集合：通过反向对称性提高多样性和泛化能力",
    "translated_abstract": "深度集合（DE）通过学习随机初始化的多样化成员成功地提高了模型性能。虽然最近的一些研究尝试通过超参数或正则化损失函数来促进进一步的多样性，但这些方法主要仍然依赖于随机方法来探索假设空间。在本文中，我们提出了多对称集合（MSE），通过捕捉对称轴上假设的多样性，构建了一个构建多样性集合的框架，它可以超越模型权重和超参数的随机扰动探索假设空间。我们利用对比表示学习的最新进展，创建了分别捕捉不变和等变函数类的对立假设的模型，并提出了一种简单的集成方法，以高效地组合适当的假设来完成给定的任务。我们证明，MSE有效地捕捉了相互矛盾的假设的多样性。",
    "tldr": "本研究提出了一种新的集成学习方法——多对称集合（MSE），它通过对称轴上假设的多样性来提高多样性和泛化能力，超越传统随机扰动的方法探索假设空间，并取得了良好效果。",
    "en_tdlr": "This paper proposes a new ensemble learning method, Multi-Symmetry Ensembles (MSE), which improves diversity and generalization by capturing the multiplicity of hypotheses along symmetry axes, explore the hypothesis space beyond traditional stochastic perturbations, and shows good performance."
}