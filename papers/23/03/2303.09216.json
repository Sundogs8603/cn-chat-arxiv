{
    "title": "Controlled Descent Training. (arXiv:2303.09216v1 [math.OC])",
    "abstract": "In this work, a novel and model-based artificial neural network (ANN) training method is developed supported by optimal control theory. The method augments training labels in order to robustly guarantee training loss convergence and improve training convergence rate. Dynamic label augmentation is proposed within the framework of gradient descent training where the convergence of training loss is controlled. First, we capture the training behavior with the help of empirical Neural Tangent Kernels (NTK) and borrow tools from systems and control theory to analyze both the local and global training dynamics (e.g. stability, reachability). Second, we propose to dynamically alter the gradient descent training mechanism via fictitious labels as control inputs and an optimal state feedback policy. In this way, we enforce locally $\\mathcal{H}_2$ optimal and convergent training behavior. The novel algorithm, \\textit{Controlled Descent Training} (CDT), guarantees local convergence. CDT unleashes ",
    "link": "http://arxiv.org/abs/2303.09216",
    "context": "Title: Controlled Descent Training. (arXiv:2303.09216v1 [math.OC])\nAbstract: In this work, a novel and model-based artificial neural network (ANN) training method is developed supported by optimal control theory. The method augments training labels in order to robustly guarantee training loss convergence and improve training convergence rate. Dynamic label augmentation is proposed within the framework of gradient descent training where the convergence of training loss is controlled. First, we capture the training behavior with the help of empirical Neural Tangent Kernels (NTK) and borrow tools from systems and control theory to analyze both the local and global training dynamics (e.g. stability, reachability). Second, we propose to dynamically alter the gradient descent training mechanism via fictitious labels as control inputs and an optimal state feedback policy. In this way, we enforce locally $\\mathcal{H}_2$ optimal and convergent training behavior. The novel algorithm, \\textit{Controlled Descent Training} (CDT), guarantees local convergence. CDT unleashes ",
    "path": "papers/23/03/2303.09216.json",
    "total_tokens": 946,
    "translated_title": "受控下降训练",
    "translated_abstract": "本研究设计了一种新颖的、基于模型的人工神经网络（ANN）训练方法，该方法使用最优控制理论支持。该方法通过增强训练标签，以可靠地保证训练损失收敛并提高训练收敛速率。在梯度下降训练框架内提出了动态标签增强方法，以控制训练损失的收敛。首先，我们借助经验神经切向核（NTK）来捕捉训练行为，并从系统和控制理论中借鉴工具来分析局部和全局的训练动态（如稳定性、可达性）。其次，我们建议通过虚构的标签作为控制输入和最优状态反馈策略来动态改变梯度下降训练机制。通过这种方式，我们强制在本地实现$\\mathcal{H}_2$最优和收敛的训练行为。这种新颖的算法“受控下降训练”（CDT）保证了局部收敛。",
    "tldr": "本文提出了一种新颖的神经网络（ANN）训练方法，使用最优控制理论支持。通过增强训练标签，以可靠地保证训练损失收敛并提高训练收敛速率，实现动态标签增强。这种新颖的算法保证了局部收敛。",
    "en_tdlr": "This paper proposes a novel artificial neural network (ANN) training method, supported by optimal control theory. By augmenting training labels and controlling the convergence of training loss, the method improves convergence rate. The proposed algorithm, Controlled Descent Training (CDT), guarantees local convergence."
}