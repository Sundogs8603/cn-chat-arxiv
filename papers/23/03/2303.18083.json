{
    "title": "Analysis and Comparison of Two-Level KFAC Methods for Training Deep Neural Networks. (arXiv:2303.18083v1 [cs.LG])",
    "abstract": "As a second-order method, the Natural Gradient Descent (NGD) has the ability to accelerate training of neural networks. However, due to the prohibitive computational and memory costs of computing and inverting the Fisher Information Matrix (FIM), efficient approximations are necessary to make NGD scalable to Deep Neural Networks (DNNs). Many such approximations have been attempted. The most sophisticated of these is KFAC, which approximates the FIM as a block-diagonal matrix, where each block corresponds to a layer of the neural network. By doing so, KFAC ignores the interactions between different layers. In this work, we investigate the interest of restoring some low-frequency interactions between the layers by means of two-level methods. Inspired from domain decomposition, several two-level corrections to KFAC using different coarse spaces are proposed and assessed. The obtained results show that incorporating the layer interactions in this fashion does not really improve the perform",
    "link": "http://arxiv.org/abs/2303.18083",
    "context": "Title: Analysis and Comparison of Two-Level KFAC Methods for Training Deep Neural Networks. (arXiv:2303.18083v1 [cs.LG])\nAbstract: As a second-order method, the Natural Gradient Descent (NGD) has the ability to accelerate training of neural networks. However, due to the prohibitive computational and memory costs of computing and inverting the Fisher Information Matrix (FIM), efficient approximations are necessary to make NGD scalable to Deep Neural Networks (DNNs). Many such approximations have been attempted. The most sophisticated of these is KFAC, which approximates the FIM as a block-diagonal matrix, where each block corresponds to a layer of the neural network. By doing so, KFAC ignores the interactions between different layers. In this work, we investigate the interest of restoring some low-frequency interactions between the layers by means of two-level methods. Inspired from domain decomposition, several two-level corrections to KFAC using different coarse spaces are proposed and assessed. The obtained results show that incorporating the layer interactions in this fashion does not really improve the perform",
    "path": "papers/23/03/2303.18083.json",
    "total_tokens": 821,
    "translated_title": "两种KFAC二级方法在深度神经网络训练中的分析与比较",
    "translated_abstract": "作为二阶方法，自然梯度下降（NGD）可以加速神经网络的训练。然而，由于计算和反演费舍尔信息矩阵（FIM）的代价过高，需要高效的近似方法，以使NGD可扩展到深度神经网络（DNN）。已经尝试了许多这样的近似方法。其中最复杂的是KFAC，它将FIM近似为一个块对角矩阵，其中每个块对应于神经网络的一层. 本文通过二级方法，探讨通过使用不同的粗略空间还原一些低频层间交互的方法的利益。实验结果表明，以这种方式将层间交互结合起来并不能真正提高性能。",
    "tldr": "本文研究了两种KFAC二级方法，用于在训练深度神经网络中恢复层间低频交互，研究结果发现这种方法并未显著提高性能。",
    "en_tdlr": "This paper analyzes and compares two-level KFAC methods for training deep neural networks, investigates the restoration of low-frequency interactions between layers, and finds that this method does not significantly improve performance."
}