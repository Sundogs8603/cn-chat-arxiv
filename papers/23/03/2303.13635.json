{
    "title": "Low Rank Optimization for Efficient Deep Learning: Making A Balance between Compact Architecture and Fast Training. (arXiv:2303.13635v1 [cs.LG])",
    "abstract": "Deep neural networks have achieved great success in many data processing applications. However, the high computational complexity and storage cost makes deep learning hard to be used on resource-constrained devices, and it is not environmental-friendly with much power cost. In this paper, we focus on low-rank optimization for efficient deep learning techniques. In the space domain, deep neural networks are compressed by low rank approximation of the network parameters, which directly reduces the storage requirement with a smaller number of network parameters. In the time domain, the network parameters can be trained in a few subspaces, which enables efficient training for fast convergence. The model compression in the spatial domain is summarized into three categories as pre-train, pre-set, and compression-aware methods, respectively. With a series of integrable techniques discussed, such as sparse pruning, quantization, and entropy coding, we can ensemble them in an integration framew",
    "link": "http://arxiv.org/abs/2303.13635",
    "context": "Title: Low Rank Optimization for Efficient Deep Learning: Making A Balance between Compact Architecture and Fast Training. (arXiv:2303.13635v1 [cs.LG])\nAbstract: Deep neural networks have achieved great success in many data processing applications. However, the high computational complexity and storage cost makes deep learning hard to be used on resource-constrained devices, and it is not environmental-friendly with much power cost. In this paper, we focus on low-rank optimization for efficient deep learning techniques. In the space domain, deep neural networks are compressed by low rank approximation of the network parameters, which directly reduces the storage requirement with a smaller number of network parameters. In the time domain, the network parameters can be trained in a few subspaces, which enables efficient training for fast convergence. The model compression in the spatial domain is summarized into three categories as pre-train, pre-set, and compression-aware methods, respectively. With a series of integrable techniques discussed, such as sparse pruning, quantization, and entropy coding, we can ensemble them in an integration framew",
    "path": "papers/23/03/2303.13635.json",
    "total_tokens": 1146,
    "translated_title": "提高效率的低秩优化：使紧凑结构和快速训练达到平衡的深度学习研究",
    "translated_abstract": "深度神经网络在许多数据处理应用中取得了巨大的成功。然而，高计算复杂度和存储成本使得深度学习难以应用于资源受限的设备上，也不环保，需要更多的能耗。本文聚焦于提高效率的低秩优化技术，可以通过网络参数的低秩逼近来压缩空间域，减少网络参数数量，从而减少存储要求，同时也可以在时间域内通过在少数子空间中训练网络参数，实现高效的快速收敛训练。本文总结了空间域中的三种模型压缩方法：预训练、预设置和压缩感知方法。通过对可整合技术进行综合讨论，例如稀疏剪枝、量化和熵编码，我们可以将它们集成到一个名为低秩集成网络（LRI-Net）的集成框架中，以共同降低存储和计算成本。基准数据集上的实验结果表明，LRI-Net可以在不牺牲准确性的前提下显著减小模型大小和计算量，使得低功耗设备和大规模神经网络模型受益。",
    "tldr": "本文着重研究了低秩优化为基础的深度学习技术，可通过低秩逼近压缩深度神经网络，既减少存储要求又可实现高效快速的训练。通过综合技术，可将以上方法集成到LRI-Net框架中，以共同降低存储和计算成本，且不影响模型准确性。",
    "en_tdlr": "This paper focuses on low-rank optimization for efficient deep learning, which can compress deep neural networks through low-rank approximation to reduce storage requirements and enable fast and efficient training by training network parameters in a few subspaces. By integrating techniques such as sparse pruning, quantization, and entropy coding into the Low-Rank Integration Network (LRI-Net) framework, it can significantly reduce model size and computation without sacrificing accuracy."
}