{
    "title": "Fully neuromorphic vision and control for autonomous drone flight. (arXiv:2303.08778v1 [cs.RO])",
    "abstract": "Biological sensing and processing is asynchronous and sparse, leading to low-latency and energy-efficient perception and action. In robotics, neuromorphic hardware for event-based vision and spiking neural networks promises to exhibit similar characteristics. However, robotic implementations have been limited to basic tasks with low-dimensional sensory inputs and motor actions due to the restricted network size in current embedded neuromorphic processors and the difficulties of training spiking neural networks. Here, we present the first fully neuromorphic vision-to-control pipeline for controlling a freely flying drone. Specifically, we train a spiking neural network that accepts high-dimensional raw event-based camera data and outputs low-level control actions for performing autonomous vision-based flight. The vision part of the network, consisting of five layers and 28.8k neurons, maps incoming raw events to ego-motion estimates and is trained with self-supervised learning on real e",
    "link": "http://arxiv.org/abs/2303.08778",
    "context": "Title: Fully neuromorphic vision and control for autonomous drone flight. (arXiv:2303.08778v1 [cs.RO])\nAbstract: Biological sensing and processing is asynchronous and sparse, leading to low-latency and energy-efficient perception and action. In robotics, neuromorphic hardware for event-based vision and spiking neural networks promises to exhibit similar characteristics. However, robotic implementations have been limited to basic tasks with low-dimensional sensory inputs and motor actions due to the restricted network size in current embedded neuromorphic processors and the difficulties of training spiking neural networks. Here, we present the first fully neuromorphic vision-to-control pipeline for controlling a freely flying drone. Specifically, we train a spiking neural network that accepts high-dimensional raw event-based camera data and outputs low-level control actions for performing autonomous vision-based flight. The vision part of the network, consisting of five layers and 28.8k neurons, maps incoming raw events to ego-motion estimates and is trained with self-supervised learning on real e",
    "path": "papers/23/03/2303.08778.json",
    "total_tokens": 908,
    "translated_title": "全神经形态视觉和控制的自主飞行执照。",
    "translated_abstract": "生物感知和处理是异步和稀疏的，导致低延迟和能量有效的感知和行动。在机器人学中，使用面向事件的神经形态硬件和尖峰神经网络承诺具有类似的特征。然而，由于当前嵌入式神经形态处理器中受限的网络规模以及训练尖峰神经网络的困难，机器人实现仅限于具有低维感官输入和运动执行的基本任务。在这里，我们提出了第一个全神经形态视觉到控制的流程，以控制自由飞行的无人机。具体而言，我们训练了一个尖峰神经网络，该神经网络接受高维原始事件相机数据并输出执行自主基于视觉的飞行所需的低级控制动作。网络的视觉部分由五层和 28.8k 神经元组成，将传入的原始事件映射到自我运动估计上，并使用真实环境的自我监督学习进行训练。",
    "tldr": "本文介绍了第一个全神经形态视觉到控制的流程，使无人机具备了执行自主基于视觉的飞行所需的能力，为机器人感知和行动提供了低延迟和能量有效的解决方案。",
    "en_tdlr": "This article describes the first fully neuromorphic vision-to-control pipeline, enabling drones to perform autonomous vision-based flight and providing a low-latency and energy-efficient solution for robotic perception and action."
}