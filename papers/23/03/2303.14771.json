{
    "title": "Prototype-Sample Relation Distillation: Towards Replay-Free Continual Learning. (arXiv:2303.14771v2 [cs.LG] UPDATED)",
    "abstract": "In Continual learning (CL) balancing effective adaptation while combating catastrophic forgetting is a central challenge. Many of the recent best-performing methods utilize various forms of prior task data, e.g. a replay buffer, to tackle the catastrophic forgetting problem. Having access to previous task data can be restrictive in many real-world scenarios, for example when task data is sensitive or proprietary. To overcome the necessity of using previous tasks' data, in this work, we start with strong representation learning methods that have been shown to be less prone to forgetting. We propose a holistic approach to jointly learn the representation and class prototypes while maintaining the relevance of old class prototypes and their embedded similarities. Specifically, samples are mapped to an embedding space where the representations are learned using a supervised contrastive loss. Class prototypes are evolved continually in the same latent space, enabling learning and prediction",
    "link": "http://arxiv.org/abs/2303.14771",
    "context": "Title: Prototype-Sample Relation Distillation: Towards Replay-Free Continual Learning. (arXiv:2303.14771v2 [cs.LG] UPDATED)\nAbstract: In Continual learning (CL) balancing effective adaptation while combating catastrophic forgetting is a central challenge. Many of the recent best-performing methods utilize various forms of prior task data, e.g. a replay buffer, to tackle the catastrophic forgetting problem. Having access to previous task data can be restrictive in many real-world scenarios, for example when task data is sensitive or proprietary. To overcome the necessity of using previous tasks' data, in this work, we start with strong representation learning methods that have been shown to be less prone to forgetting. We propose a holistic approach to jointly learn the representation and class prototypes while maintaining the relevance of old class prototypes and their embedded similarities. Specifically, samples are mapped to an embedding space where the representations are learned using a supervised contrastive loss. Class prototypes are evolved continually in the same latent space, enabling learning and prediction",
    "path": "papers/23/03/2303.14771.json",
    "total_tokens": 729,
    "translated_title": "原型样本关系蒸馏：实现无需回放的持续学习",
    "translated_abstract": "在持续学习中，平衡有效适应和抵御灾难性遗忘是一个核心难题。许多最近表现最佳的方法利用各种形式的先前任务数据来解决灾难性遗忘问题。然而，在许多实际场景中，访问以前的任务数据可能具有限制性，例如当任务数据是敏感的或专有的时候。本文提出了一种综合方法来共同学习特征表示和类原型，同时保持旧类原型及其内在相似性的相关性，从而克服了使用先前任务数据的必要性。",
    "tldr": "本文提出了一种无需回放以往数据的持续学习方法，通过共同学习特征表示和类原型来避免灾难性遗忘。",
    "en_tdlr": "This paper proposes a replay-free continual learning method that jointly learns feature representation and class prototypes to tackle catastrophic forgetting without relying on previous task data."
}