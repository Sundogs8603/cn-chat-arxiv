{
    "title": "Unit Scaling: Out-of-the-Box Low-Precision Training. (arXiv:2303.11257v2 [cs.LG] UPDATED)",
    "abstract": "We present unit scaling, a paradigm for designing deep learning models that simplifies the use of low-precision number formats. Training in FP16 or the recently proposed FP8 formats offers substantial efficiency gains, but can lack sufficient range for out-of-the-box training. Unit scaling addresses this by introducing a principled approach to model numerics: seeking unit variance of all weights, activations and gradients at initialisation. Unlike alternative methods, this approach neither requires multiple training runs to find a suitable scale nor has significant computational overhead. We demonstrate the efficacy of unit scaling across a range of models and optimisers. We further show that existing models can be adapted to be unit-scaled, training BERT-Large in FP16 and then FP8 with no degradation in accuracy.",
    "link": "http://arxiv.org/abs/2303.11257",
    "context": "Title: Unit Scaling: Out-of-the-Box Low-Precision Training. (arXiv:2303.11257v2 [cs.LG] UPDATED)\nAbstract: We present unit scaling, a paradigm for designing deep learning models that simplifies the use of low-precision number formats. Training in FP16 or the recently proposed FP8 formats offers substantial efficiency gains, but can lack sufficient range for out-of-the-box training. Unit scaling addresses this by introducing a principled approach to model numerics: seeking unit variance of all weights, activations and gradients at initialisation. Unlike alternative methods, this approach neither requires multiple training runs to find a suitable scale nor has significant computational overhead. We demonstrate the efficacy of unit scaling across a range of models and optimisers. We further show that existing models can be adapted to be unit-scaled, training BERT-Large in FP16 and then FP8 with no degradation in accuracy.",
    "path": "papers/23/03/2303.11257.json",
    "total_tokens": 889,
    "translated_title": "单位缩放：开箱即用的低精度训练方法。",
    "translated_abstract": "本论文提出了一种名为“单位缩放”的范式，用于设计神经网络模型，该方法简化了使用低精度数字格式的操作。在FP16或FP8格式下进行训练可以大大提高效率，但可能缺乏足够的范围进行训练。单位缩放通过引入基于模型数字的原则方法，寻求在初始化时所有权重、激活函数和梯度的单位方差来解决这个问题。与其他方法不同，这种方法既不需要多次运行以找到合适的比例，也没有显著的计算开销。我们展示了单位缩放在各种模型和优化器上的有效性。我们进一步展示了现有模型可以被调整为单位缩放，例如使用FP16对BERT-Large进行训练，然后使用FP8进行训练，而不会影响网络精度。",
    "tldr": "本论文提出了一种称为“单位缩放”的方法，该方法可简化低精度数字格式的操作并提高训练效率。该方法通过在初始化时将所有权重、激活函数和梯度的单位差变为1来解决低精度训练范围的问题，与其他方法不同，它不需要多次运行也没有显著的计算开销。",
    "en_tdlr": "This paper proposes a paradigm called \"unit scaling\" that simplifies low-precision training and improves efficiency. By seeking unit variance of weights, activations, and gradients during initialization, this method addresses the issue of low-precision training range. It does not require multiple runs or significant computational overhead, and can be adapted to existing models with no degradation in accuracy."
}