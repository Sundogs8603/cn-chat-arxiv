{
    "title": "Reducing Spurious Correlations for Aspect-Based Sentiment Analysis with Variational Information Bottleneck and Contrastive Learning. (arXiv:2303.02846v2 [cs.CL] UPDATED)",
    "abstract": "Deep learning techniques have dominated the literature on aspect-based sentiment analysis (ABSA), yielding state-of-the-art results. However, these deep models generally suffer from spurious correlation problems between input features and output labels, which creates significant barriers to robustness and generalization capability. In this paper, we propose a novel Contrastive Variational Information Bottleneck framework (called CVIB) to reduce spurious correlations for ABSA. The proposed CVIB framework is composed of an original network and a self-pruned network, and these two networks are optimized simultaneously via contrastive learning. Concretely, we employ the Variational Information Bottleneck (VIB) principle to learn an informative and compressed network (self-pruned network) from the original network, which discards the superfluous patterns or spurious correlations between input features and prediction labels. Then, self-pruning contrastive learning is devised to pull together",
    "link": "http://arxiv.org/abs/2303.02846",
    "total_tokens": 976,
    "translated_title": "通过变分信息瓶颈和对比学习减少方面情感分析中的虚假相关性",
    "translated_abstract": "深度学习技术在方面情感分析（ABSA）的文献中占据主导地位，取得了最先进的结果。然而，这些深度模型通常在输入特征和输出标签之间存在虚假相关性问题，这会给鲁棒性和泛化能力带来重大障碍。在本文中，我们提出了一种新颖的对比变分信息瓶颈框架（称为CVIB），以减少ABSA中的虚假相关性。所提出的CVIB框架由一个原始网络和一个自剪枝网络组成，这两个网络通过对比学习同时进行优化。具体而言，我们采用变分信息瓶颈（VIB）原则从原始网络中学习一个信息丰富且压缩的网络（自剪枝网络），该网络丢弃了输入特征和预测标签之间的多余模式或虚假相关性。然后，我们设计了自剪枝对比学习，以将两个网络拉在一起。",
    "tldr": "本文提出了一种新的对比变分信息瓶颈框架（CVIB），以减少方面情感分析（ABSA）中的虚假相关性。该框架由一个原始网络和一个自剪枝网络组成，通过对比学习同时进行优化，从而丢弃了输入特征和预测标签之间的多余模式或虚假相关性。",
    "en_tldr": "This paper proposes a novel Contrastive Variational Information Bottleneck framework (CVIB) to reduce spurious correlations for aspect-based sentiment analysis (ABSA). The proposed CVIB framework is composed of an original network and a self-pruned network, and these two networks are optimized simultaneously via contrastive learning, which discards the superfluous patterns or spurious correlations between input features and prediction labels."
}