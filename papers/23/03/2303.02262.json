{
    "title": "Locally Regularized Neural Differential Equations: Some Black Boxes Were Meant to Remain Closed!. (arXiv:2303.02262v3 [cs.LG] UPDATED)",
    "abstract": "Implicit layer deep learning techniques, like Neural Differential Equations, have become an important modeling framework due to their ability to adapt to new problems automatically. Training a neural differential equation is effectively a search over a space of plausible dynamical systems. However, controlling the computational cost for these models is difficult since it relies on the number of steps the adaptive solver takes. Most prior works have used higher-order methods to reduce prediction timings while greatly increasing training time or reducing both training and prediction timings by relying on specific training algorithms, which are harder to use as a drop-in replacement due to strict requirements on automatic differentiation. In this manuscript, we use internal cost heuristics of adaptive differential equation solvers at stochastic time points to guide the training toward learning a dynamical system that is easier to integrate. We \"close the black-box\" and allow the use of ou",
    "link": "http://arxiv.org/abs/2303.02262",
    "context": "Title: Locally Regularized Neural Differential Equations: Some Black Boxes Were Meant to Remain Closed!. (arXiv:2303.02262v3 [cs.LG] UPDATED)\nAbstract: Implicit layer deep learning techniques, like Neural Differential Equations, have become an important modeling framework due to their ability to adapt to new problems automatically. Training a neural differential equation is effectively a search over a space of plausible dynamical systems. However, controlling the computational cost for these models is difficult since it relies on the number of steps the adaptive solver takes. Most prior works have used higher-order methods to reduce prediction timings while greatly increasing training time or reducing both training and prediction timings by relying on specific training algorithms, which are harder to use as a drop-in replacement due to strict requirements on automatic differentiation. In this manuscript, we use internal cost heuristics of adaptive differential equation solvers at stochastic time points to guide the training toward learning a dynamical system that is easier to integrate. We \"close the black-box\" and allow the use of ou",
    "path": "papers/23/03/2303.02262.json",
    "total_tokens": 888,
    "translated_title": "局部正则化的神经微分方程：有些黑盒子是要保持封闭的！",
    "translated_abstract": "由于自动适应的能力，类似神经微分方程的隐式层深度学习技术已经成为重要的建模框架。但是，控制这些模型的计算成本是困难的，因为它依赖于自适应求解器的步骤数。本文中，我们在随机时间点使用自适应微分方程求解器的内部成本启发式来引导训练以学习更易于积分的动力系统。我们增加局部正则化项到目标函数中，使得训练过程遵循经验条件并提高预测速度，同时仍保留了模型的自适应特性。",
    "tldr": "该论文提出了一种局部正则化的神经微分方程模型，通过启发式成本控制训练过程以学习易于积分的动力系统，同时提高预测速度，并保留了模型的自适应特性。",
    "en_tdlr": "The paper proposes a locally regularized neural differential equation model that uses internal cost heuristics of adaptive differential equation solvers at stochastic time points to guide training towards learning a dynamical system that is easier to integrate. The approach improves prediction speed, maintains the model's adaptive properties and follows empirical conditions."
}