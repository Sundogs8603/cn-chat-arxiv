{
    "title": "Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?. (arXiv:2303.18240v1 [cs.CV])",
    "abstract": "We present the largest and most comprehensive empirical study of pre-trained visual representations (PVRs) or visual 'foundation models' for Embodied AI. First, we curate CortexBench, consisting of 17 different tasks spanning locomotion, navigation, dexterous, and mobile manipulation. Next, we systematically evaluate existing PVRs and find that none are universally dominant.  To study the effect of pre-training data scale and diversity, we combine over 4,000 hours of egocentric videos from 7 different sources (over 5.6M images) and ImageNet to train different-sized vision transformers using Masked Auto-Encoding (MAE) on slices of this data. Contrary to inferences from prior work, we find that scaling dataset size and diversity does not improve performance universally (but does so on average).  Our largest model, named VC-1, outperforms all prior PVRs on average but does not universally dominate either. Finally, we show that task or domain-specific adaptation of VC-1 leads to substantia",
    "link": "http://arxiv.org/abs/2303.18240",
    "context": "Title: Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?. (arXiv:2303.18240v1 [cs.CV])\nAbstract: We present the largest and most comprehensive empirical study of pre-trained visual representations (PVRs) or visual 'foundation models' for Embodied AI. First, we curate CortexBench, consisting of 17 different tasks spanning locomotion, navigation, dexterous, and mobile manipulation. Next, we systematically evaluate existing PVRs and find that none are universally dominant.  To study the effect of pre-training data scale and diversity, we combine over 4,000 hours of egocentric videos from 7 different sources (over 5.6M images) and ImageNet to train different-sized vision transformers using Masked Auto-Encoding (MAE) on slices of this data. Contrary to inferences from prior work, we find that scaling dataset size and diversity does not improve performance universally (but does so on average).  Our largest model, named VC-1, outperforms all prior PVRs on average but does not universally dominate either. Finally, we show that task or domain-specific adaptation of VC-1 leads to substantia",
    "path": "papers/23/03/2303.18240.json",
    "total_tokens": 998,
    "translated_title": "寻找具有身体智能的人工视觉皮层在哪里？",
    "translated_abstract": "我们提出了最大、最全面的预训练视觉表征（PVR）或视觉基础模型的经验研究，用于身体智能。首先，我们策划了 CortexBench，其中包括涵盖动力学、导航、熟练和移动操作的17种不同任务。接下来，我们系统评估了现有的PVR，发现没有一种是普遍优越的。为了研究预训练数据规模和多样性的影响，我们结合了来自7个不同来源的超过4000小时的自我中心视频（超过560万张图像）和ImageNet，使用切片数据的遮盖自编码（MAE）来训练不同大小的视觉变形器。与先前的工作推断相反，我们发现扩展数据集的规模和多样性并不能普遍改善性能（但平均性能有所提高）。我们最大的模型名为VC-1，平均表现超过所有先前的PVR，但也没有普遍优势。最后，我们展示了VC-1的特定于任务或领域的适应会带来实质性的改进。",
    "tldr": "该研究研究了使用预训练视觉表征来实现身体智能的最新进展。他们展示了最大、最全面的经验研究，发现没有一种表征是普遍优越的，并且数据集的大小和多样性并不能普遍改善性能。"
}