{
    "title": "Inducing Neural Collapse to a Fixed Hierarchy-Aware Frame for Reducing Mistake Severity. (arXiv:2303.05689v2 [cs.CV] UPDATED)",
    "abstract": "There is a recently discovered and intriguing phenomenon called Neural Collapse: at the terminal phase of training a deep neural network for classification, the within-class penultimate feature means and the associated classifier vectors of all flat classes collapse to the vertices of a simplex Equiangular Tight Frame (ETF). Recent work has tried to exploit this phenomenon by fixing the related classifier weights to a pre-computed ETF to induce neural collapse and maximize the separation of the learned features when training with imbalanced data. In this work, we propose to fix the linear classifier of a deep neural network to a Hierarchy-Aware Frame (HAFrame), instead of an ETF, and use a cosine similarity-based auxiliary loss to learn hierarchy-aware penultimate features that collapse to the HAFrame. We demonstrate that our approach reduces the mistake severity of the model's predictions while maintaining its top-1 accuracy on several datasets of varying scales with hierarchies of he",
    "link": "http://arxiv.org/abs/2303.05689",
    "context": "Title: Inducing Neural Collapse to a Fixed Hierarchy-Aware Frame for Reducing Mistake Severity. (arXiv:2303.05689v2 [cs.CV] UPDATED)\nAbstract: There is a recently discovered and intriguing phenomenon called Neural Collapse: at the terminal phase of training a deep neural network for classification, the within-class penultimate feature means and the associated classifier vectors of all flat classes collapse to the vertices of a simplex Equiangular Tight Frame (ETF). Recent work has tried to exploit this phenomenon by fixing the related classifier weights to a pre-computed ETF to induce neural collapse and maximize the separation of the learned features when training with imbalanced data. In this work, we propose to fix the linear classifier of a deep neural network to a Hierarchy-Aware Frame (HAFrame), instead of an ETF, and use a cosine similarity-based auxiliary loss to learn hierarchy-aware penultimate features that collapse to the HAFrame. We demonstrate that our approach reduces the mistake severity of the model's predictions while maintaining its top-1 accuracy on several datasets of varying scales with hierarchies of he",
    "path": "papers/23/03/2303.05689.json",
    "total_tokens": 933,
    "translated_title": "将神经衰竭引入到固定的等角紧框架中，以减少错误严重程度",
    "translated_abstract": "最近发现了一种有趣的现象，称为神经衰竭：在训练深度神经网络进行分类的末期阶段，所有平坦类的倒数第二特征均值和相关的分类器向量都会坍缩到一个简单的等角紧框架（ETF）的顶点上。近期的研究尝试利用这一现象，通过将相关的分类器权重固定为预先计算得到的ETF来引发神经衰竭，并在使用不平衡数据进行训练时最大化所学特征之间的分离度。在本文中，我们提出将深度神经网络的线性分类器固定为按层次结构设计的框架（HAFrame），而不是ETF，并使用基于余弦相似度的辅助损失来学习坍缩到HAFrame的层次感知的倒数第二特征。我们证明了我们的方法在保持模型在多个具有层次结构的数据集上的top-1准确率的同时，降低了模型预测的错误严重程度。",
    "tldr": "本文提出了一种将神经衰竭引入到固定的层次感知框架中的方法，通过学习层次感知的倒数第二特征，并将其坍缩到框架上，来减少模型预测的错误严重程度。",
    "en_tdlr": "This paper proposes a method to induce neural collapse to a fixed hierarchy-aware frame by learning hierarchy-aware penultimate features and collapsing them to the frame, aiming to reduce the mistake severity of model predictions."
}