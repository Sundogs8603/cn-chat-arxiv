{
    "title": "High-Dimensional Penalized Bernstein Support Vector Machines. (arXiv:2303.09066v1 [stat.ML])",
    "abstract": "The support vector machines (SVM) is a powerful classifier used for binary classification to improve the prediction accuracy. However, the non-differentiability of the SVM hinge loss function can lead to computational difficulties in high dimensional settings. To overcome this problem, we rely on Bernstein polynomial and propose a new smoothed version of the SVM hinge loss called the Bernstein support vector machine (BernSVM), which is suitable for the high dimension $p >> n$ regime. As the BernSVM objective loss function is of the class $C^2$, we propose two efficient algorithms for computing the solution of the penalized BernSVM. The first algorithm is based on coordinate descent with maximization-majorization (MM) principle and the second one is IRLS-type algorithm (iterative re-weighted least squares). Under standard assumptions, we derive a cone condition and a restricted strong convexity to establish an upper bound for the weighted Lasso BernSVM estimator. Using a local linear ap",
    "link": "http://arxiv.org/abs/2303.09066",
    "context": "Title: High-Dimensional Penalized Bernstein Support Vector Machines. (arXiv:2303.09066v1 [stat.ML])\nAbstract: The support vector machines (SVM) is a powerful classifier used for binary classification to improve the prediction accuracy. However, the non-differentiability of the SVM hinge loss function can lead to computational difficulties in high dimensional settings. To overcome this problem, we rely on Bernstein polynomial and propose a new smoothed version of the SVM hinge loss called the Bernstein support vector machine (BernSVM), which is suitable for the high dimension $p >> n$ regime. As the BernSVM objective loss function is of the class $C^2$, we propose two efficient algorithms for computing the solution of the penalized BernSVM. The first algorithm is based on coordinate descent with maximization-majorization (MM) principle and the second one is IRLS-type algorithm (iterative re-weighted least squares). Under standard assumptions, we derive a cone condition and a restricted strong convexity to establish an upper bound for the weighted Lasso BernSVM estimator. Using a local linear ap",
    "path": "papers/23/03/2303.09066.json",
    "total_tokens": 1015,
    "translated_title": "高维度惩罚伯恩斯坦支持向量机",
    "translated_abstract": "支持向量机(SVM)是一种用于二分类的强大分类器，以提高预测精度。然而，在高维设置中，SVM铰链损失函数的不可微性可能导致计算困难。为了克服这个问题，我们依赖伯恩斯坦多项式，提出了一种新的平滑的SVM铰链损失函数版本，称为Bernstein支持向量机（BernSVM），适用于高维$p>> n$情况。由于BernSVM目标损失函数属于$C^2$类，因此我们提出了两种计算惩罚BernSVM解的有效算法。第一个算法基于最大化-主导（MM）原理的坐标下降法，第二个算法是IRLS类型算法（迭代重新加权最小二乘法）。在标准假设下，我们推导出一个锥条件和一个限制性强凸性，以建立加权Lasso BernSVM估计器的上界。使用局部线性逼近，我们提出了两个模型选择标准，用于调整BernSVM超参数。进行了广泛的数值实验，以证明所提出的方法在现有竞争对手中具有优越性。",
    "tldr": "提出一种适用于高维度情况下的平滑支持向量机铰链损失函数，即Bernstein支持向量机（BernSVM），并提出两种有效算法求解该方法，实验结果表明该方法在现有竞争对手中具有优越性。",
    "en_tdlr": "A smoothed version of the SVM hinge loss called the Bernstein support vector machine (BernSVM) is proposed for high dimensional settings, with two efficient algorithms for computing the solution of the penalized BernSVM, and extensive numerical experiments showing the superiority of the proposed method over state-of-the-art competitors."
}