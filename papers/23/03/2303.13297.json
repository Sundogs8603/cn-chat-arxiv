{
    "title": "Improving Generalization with Domain Convex Game. (arXiv:2303.13297v1 [cs.CV])",
    "abstract": "Domain generalization (DG) tends to alleviate the poor generalization capability of deep neural networks by learning model with multiple source domains. A classical solution to DG is domain augmentation, the common belief of which is that diversifying source domains will be conducive to the out-of-distribution generalization. However, these claims are understood intuitively, rather than mathematically. Our explorations empirically reveal that the correlation between model generalization and the diversity of domains may be not strictly positive, which limits the effectiveness of domain augmentation. This work therefore aim to guarantee and further enhance the validity of this strand. To this end, we propose a new perspective on DG that recasts it as a convex game between domains. We first encourage each diversified domain to enhance model generalization by elaborately designing a regularization term based on supermodularity. Meanwhile, a sample filter is constructed to eliminate low-qua",
    "link": "http://arxiv.org/abs/2303.13297",
    "context": "Title: Improving Generalization with Domain Convex Game. (arXiv:2303.13297v1 [cs.CV])\nAbstract: Domain generalization (DG) tends to alleviate the poor generalization capability of deep neural networks by learning model with multiple source domains. A classical solution to DG is domain augmentation, the common belief of which is that diversifying source domains will be conducive to the out-of-distribution generalization. However, these claims are understood intuitively, rather than mathematically. Our explorations empirically reveal that the correlation between model generalization and the diversity of domains may be not strictly positive, which limits the effectiveness of domain augmentation. This work therefore aim to guarantee and further enhance the validity of this strand. To this end, we propose a new perspective on DG that recasts it as a convex game between domains. We first encourage each diversified domain to enhance model generalization by elaborately designing a regularization term based on supermodularity. Meanwhile, a sample filter is constructed to eliminate low-qua",
    "path": "papers/23/03/2303.13297.json",
    "total_tokens": 906,
    "translated_title": "基于域凸博弈的通用性改进研究",
    "translated_abstract": "域泛化（DG）通过学习具有多个源域的模型来缓解深度神经网络的差泛化能力。 DG的经典解决方案是域增强，它的普遍信仰是通过多样化源域有助于超出分布范围的泛化。然而，这些主张仅基于直观理解，缺乏数学证明。我们的探索实验表明模型泛化和域多样性之间的相关性可能不是严格正相关的，这限制了域增强的有效性。因此，本研究旨在保证和进一步增强该领域的有效性。为此，我们提出了一个新的DG视角，将其重新解释为域之间的凸博弈。通过基于超模性的巧妙设计正则化项来鼓励每个多样化的域增强模型泛化，并构建样本过滤器来消除低质量样本，同时有效地提高域增强的有效性。",
    "tldr": "本文提出了一种新的域泛化视角，将其重新解释为域之间的凸博弈，并通过鼓励每个多样化的域增强模型泛化和构建样本过滤器来提高域增强的有效性。",
    "en_tdlr": "This paper proposes a new perspective on domain generalization by recasting it as a convex game between domains, with the aim of enhancing the validity of domain augmentation. The approach includes encouraging diversified domains to enhance model generalization through elaborate regularization based on supermodularity and constructing a sample filter to eliminate low-quality samples."
}