{
    "title": "Connected Superlevel Set in (Deep) Reinforcement Learning and its Application to Minimax Theorems. (arXiv:2303.12981v1 [cs.LG])",
    "abstract": "The aim of this paper is to improve the understanding of the optimization landscape for policy optimization problems in reinforcement learning. Specifically, we show that the superlevel set of the objective function with respect to the policy parameter is always a connected set both in the tabular setting and under policies represented by a class of neural networks. In addition, we show that the optimization objective as a function of the policy parameter and reward satisfies a stronger \"equiconnectedness\" property. To our best knowledge, these are novel and previously unknown discoveries.  We present an application of the connectedness of these superlevel sets to the derivation of minimax theorems for robust reinforcement learning. We show that any minimax optimization program which is convex on one side and is equiconnected on the other side observes the minimax equality (i.e. has a Nash equilibrium). We find that this exact structure is exhibited by an interesting robust reinforceme",
    "link": "http://arxiv.org/abs/2303.12981",
    "context": "Title: Connected Superlevel Set in (Deep) Reinforcement Learning and its Application to Minimax Theorems. (arXiv:2303.12981v1 [cs.LG])\nAbstract: The aim of this paper is to improve the understanding of the optimization landscape for policy optimization problems in reinforcement learning. Specifically, we show that the superlevel set of the objective function with respect to the policy parameter is always a connected set both in the tabular setting and under policies represented by a class of neural networks. In addition, we show that the optimization objective as a function of the policy parameter and reward satisfies a stronger \"equiconnectedness\" property. To our best knowledge, these are novel and previously unknown discoveries.  We present an application of the connectedness of these superlevel sets to the derivation of minimax theorems for robust reinforcement learning. We show that any minimax optimization program which is convex on one side and is equiconnected on the other side observes the minimax equality (i.e. has a Nash equilibrium). We find that this exact structure is exhibited by an interesting robust reinforceme",
    "path": "papers/23/03/2303.12981.json",
    "total_tokens": 1063,
    "translated_title": "(深度)强化学习中的连接超水平集及其在极小极大定理中的应用",
    "translated_abstract": "本文的目的是改善强化学习中策略优化问题的优化函数图景理解。具体而言，我们证明了策略参数的优化目标函数的超水平集，在网络类策略和表格式下始终是连通的。同时，我们证明了奖励作为超水平集的函数满足更强的“等连通”性质。此外，我们还应用这些超水平集的连通性结果，导出了鲁棒性强化学习的极小极大定理。我们发现，任何一个在一侧为凸的，另一侧为等连通的极小极大优化问题都有纳什均衡。这些结论是新颖而且之前未知的。",
    "tldr": "本文研究了强化学习中的策略优化问题，并证明了优化函数超水平集在网络类策略和表格式下始终是连通的，并应用此结果导出了鲁棒性强化学习的极小极大定理。",
    "en_tdlr": "This paper investigates the optimization landscape for policy optimization problems in reinforcement learning, and proves that the superlevel set of the optimization objective function with respect to the policy parameter is always a connected set both in the tabular setting and under policies represented by a class of neural networks. The paper also presents an application of the connectedness of these superlevel sets to derive the minimax theorems for robust reinforcement learning."
}