{
    "title": "X-Former: In-Memory Acceleration of Transformers. (arXiv:2303.07470v1 [cs.LG])",
    "abstract": "Transformers have achieved great success in a wide variety of natural language processing (NLP) tasks due to the attention mechanism, which assigns an importance score for every word relative to other words in a sequence. However, these models are very large, often reaching hundreds of billions of parameters, and therefore require a large number of DRAM accesses. Hence, traditional deep neural network (DNN) accelerators such as GPUs and TPUs face limitations in processing Transformers efficiently. In-memory accelerators based on non-volatile memory promise to be an effective solution to this challenge, since they provide high storage density while performing massively parallel matrix vector multiplications within memory arrays. However, attention score computations, which are frequently used in Transformers (unlike CNNs and RNNs), require matrix vector multiplications (MVM) where both operands change dynamically for each input. As a result, conventional NVM-based accelerators incur hig",
    "link": "http://arxiv.org/abs/2303.07470",
    "context": "Title: X-Former: In-Memory Acceleration of Transformers. (arXiv:2303.07470v1 [cs.LG])\nAbstract: Transformers have achieved great success in a wide variety of natural language processing (NLP) tasks due to the attention mechanism, which assigns an importance score for every word relative to other words in a sequence. However, these models are very large, often reaching hundreds of billions of parameters, and therefore require a large number of DRAM accesses. Hence, traditional deep neural network (DNN) accelerators such as GPUs and TPUs face limitations in processing Transformers efficiently. In-memory accelerators based on non-volatile memory promise to be an effective solution to this challenge, since they provide high storage density while performing massively parallel matrix vector multiplications within memory arrays. However, attention score computations, which are frequently used in Transformers (unlike CNNs and RNNs), require matrix vector multiplications (MVM) where both operands change dynamically for each input. As a result, conventional NVM-based accelerators incur hig",
    "path": "papers/23/03/2303.07470.json",
    "total_tokens": 995,
    "translated_title": "在内存中加速Transformer",
    "translated_abstract": "由于注意力机制可以将每个单词相对于序列中的其他单词分配一个重要性分数，Transformer在各种自然语言处理（NLP）任务中取得了巨大的成功。然而，这些模型非常大，往往达到数千亿个参数，因此需要大量的DRAM访问。因此，传统的深度神经网络（DNN）加速器，如GPU和TPU，在高效处理Transformer方面面临限制。基于非易失性内存的内存加速器有望成为解决此挑战的有效解决方案，因为它们提供高存储密度，同时在存储器阵列内执行大规模并行矩阵向量乘法。然而，与CNN和RNN不同，Transformer中经常使用注意得分计算，这需要矩阵向量乘法（MVM），其中每个输入的两个操作数都会动态更改。因此，传统的基于NVM的加速器会由于所需的内存访问量增加而产生高开销。为了解决这个问题，我们提出了X-Former，这是一种内存加速器，利用注意矩阵中的稀疏性来减少MVM操作所需的内存访问次数。多个Transformer模型的实验结果表明，X-Former显着加速Transformer计算，同时保持高精度。",
    "tldr": "X-Former是一种内存加速器，它利用注意矩阵的稀疏性，显著加速Transformer计算，同时保持高精度。"
}