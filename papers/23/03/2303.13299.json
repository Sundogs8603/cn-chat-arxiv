{
    "title": "Reckoning with the Disagreement Problem: Explanation Consensus as a Training Objective. (arXiv:2303.13299v1 [cs.LG])",
    "abstract": "As neural networks increasingly make critical decisions in high-stakes settings, monitoring and explaining their behavior in an understandable and trustworthy manner is a necessity. One commonly used type of explainer is post hoc feature attribution, a family of methods for giving each feature in an input a score corresponding to its influence on a model's output. A major limitation of this family of explainers in practice is that they can disagree on which features are more important than others. Our contribution in this paper is a method of training models with this disagreement problem in mind. We do this by introducing a Post hoc Explainer Agreement Regularization (PEAR) loss term alongside the standard term corresponding to accuracy, an additional term that measures the difference in feature attribution between a pair of explainers. We observe on three datasets that we can train a model with this loss term to improve explanation consensus on unseen data, and see improved consensus",
    "link": "http://arxiv.org/abs/2303.13299",
    "context": "Title: Reckoning with the Disagreement Problem: Explanation Consensus as a Training Objective. (arXiv:2303.13299v1 [cs.LG])\nAbstract: As neural networks increasingly make critical decisions in high-stakes settings, monitoring and explaining their behavior in an understandable and trustworthy manner is a necessity. One commonly used type of explainer is post hoc feature attribution, a family of methods for giving each feature in an input a score corresponding to its influence on a model's output. A major limitation of this family of explainers in practice is that they can disagree on which features are more important than others. Our contribution in this paper is a method of training models with this disagreement problem in mind. We do this by introducing a Post hoc Explainer Agreement Regularization (PEAR) loss term alongside the standard term corresponding to accuracy, an additional term that measures the difference in feature attribution between a pair of explainers. We observe on three datasets that we can train a model with this loss term to improve explanation consensus on unseen data, and see improved consensus",
    "path": "papers/23/03/2303.13299.json",
    "total_tokens": 870,
    "translated_title": "论如何解决不同解释方法所带来的问题：通过训练目标达成解释一致性",
    "translated_abstract": "随着深度神经网络逐渐在高风险领域中做出关键决策，监控和解释其行为成为必需。后续特征归因方法是一种常用的解释方法，可为输入中的每个特征分配得分，以衡量其对模型输出的影响。在实践中，这种方法的一个主要局限是不同的解释方法对于哪些特征更重要可能有不同的看法。本文提出了一种考虑不同解释方法的训练模型方法，我们引入Post hoc Explainer Agreement Regularization (PEAR)损失项以提升解释一致性。我们在三个数据集上观察到我们可以使用此损失项训练模型，以在未看见的数据上获得解释一致性的提升。",
    "tldr": "本文提出针对后续特征归因方法所存在的不同解释的问题，引入PEAR损失项，从而提升模型的解释一致性，达到模型行为的可理解和可信任。",
    "en_tdlr": "The paper proposes a method of training models with post hoc feature attribution explainers that can disagree on which features are more important than others in mind. By introducing a Post hoc Explainer Agreement Regularization (PEAR) loss term alongside the standard term corresponding to accuracy, the proposed method improves the model's explanation consensus on unseen data, addressing the need for understandable and trustworthy behavior of neural networks in high-stakes settings."
}