{
    "title": "Eliciting Latent Predictions from Transformers with the Tuned Lens. (arXiv:2303.08112v1 [cs.LG])",
    "abstract": "We analyze transformers from the perspective of iterative inference, seeking to understand how model predictions are refined layer by layer. To do so, we train an affine probe for each block in a frozen pretrained model, making it possible to decode every hidden state into a distribution over the vocabulary. Our method, the \\emph{tuned lens}, is a refinement of the earlier ``logit lens'' technique, which yielded useful insights but is often brittle.  We test our method on various autoregressive language models with up to 20B parameters, showing it to be more predictive, reliable and unbiased than the logit lens. With causal experiments, we show the tuned lens uses similar features to the model itself. We also find the trajectory of latent predictions can be used to detect malicious inputs with high accuracy. All code needed to reproduce our results can be found at https://github.com/AlignmentResearch/tuned-lens.",
    "link": "http://arxiv.org/abs/2303.08112",
    "context": "Title: Eliciting Latent Predictions from Transformers with the Tuned Lens. (arXiv:2303.08112v1 [cs.LG])\nAbstract: We analyze transformers from the perspective of iterative inference, seeking to understand how model predictions are refined layer by layer. To do so, we train an affine probe for each block in a frozen pretrained model, making it possible to decode every hidden state into a distribution over the vocabulary. Our method, the \\emph{tuned lens}, is a refinement of the earlier ``logit lens'' technique, which yielded useful insights but is often brittle.  We test our method on various autoregressive language models with up to 20B parameters, showing it to be more predictive, reliable and unbiased than the logit lens. With causal experiments, we show the tuned lens uses similar features to the model itself. We also find the trajectory of latent predictions can be used to detect malicious inputs with high accuracy. All code needed to reproduce our results can be found at https://github.com/AlignmentResearch/tuned-lens.",
    "path": "papers/23/03/2303.08112.json",
    "total_tokens": 1031,
    "translated_title": "用调谐透镜从Transformer中获取潜在的预测能力",
    "translated_abstract": "本文从迭代推理的角度分析了transformers模型，旨在了解模型预测是如何逐层进行精化的。为了实现这一目的，我们为冻结的预训练模型中的每个块训练一个仿射探针，使得可以将每个隐藏状态解码成词汇分布。我们的方法“调谐透镜”，是“逻辑透镜”技术的改进版本，前者给出了有用的见解，但常常易碎。我们将其应用于各种具有多达20B参数的自回归语言模型，表明其比逻辑透镜更具有预测性、可靠性和无偏性。通过因果实验显示，调谐透镜使用的特征与模型本身类似。我们还发现，潜在预测的轨迹可以用于高精度地检测恶意输入。我们的所有代码都可以在https://github.com/AlignmentResearch/tuned-lens 找到。",
    "tldr": "本文提出了一种改进版的“逻辑透镜”技术——“调谐透镜”，通过训练一个仿射探针，可以将每个隐藏状态解码成词汇分布。这个方法被应用于各种自回归语言模型上，比逻辑透镜更具有预测性、可靠性和无偏性，并且通过因果实验验证使用的特征与模型本身类似。同时，本文发现潜在预测的轨迹可以用于高精度地检测恶意输入。",
    "en_tdlr": "This paper proposes an improved version of the \"logit lens\" technique - the \"tuned lens\", which decodes each hidden state into a distribution over the vocabulary by training an affine probe for each block in a frozen pretrained model. It applies to various autoregressive language models, is more predictive, reliable and unbiased than the logit lens, and uses similar features to the model itself according to causal experiments. Also, the trajectory of latent predictions can be used to detect malicious inputs with high accuracy."
}