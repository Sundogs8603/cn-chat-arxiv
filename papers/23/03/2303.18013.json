{
    "title": "LaCViT: A Label-aware Contrastive Training Framework for Vision Transformers. (arXiv:2303.18013v1 [cs.CV])",
    "abstract": "Vision Transformers have been incredibly effective when tackling computer vision tasks due to their ability to model long feature dependencies. By using large-scale training data and various self-supervised signals (e.g., masked random patches), vision transformers provide state-of-the-art performance on several benchmarking datasets, such as ImageNet-1k and CIFAR-10. However, these vision transformers pretrained over general large-scale image corpora could only produce an anisotropic representation space, limiting their generalizability and transferability to the target downstream tasks. In this paper, we propose a simple and effective Label-aware Contrastive Training framework LaCViT, which improves the isotropy of the pretrained representation space for vision transformers, thereby enabling more effective transfer learning amongst a wide range of image classification tasks. Through experimentation over five standard image classification datasets, we demonstrate that LaCViT-trained m",
    "link": "http://arxiv.org/abs/2303.18013",
    "context": "Title: LaCViT: A Label-aware Contrastive Training Framework for Vision Transformers. (arXiv:2303.18013v1 [cs.CV])\nAbstract: Vision Transformers have been incredibly effective when tackling computer vision tasks due to their ability to model long feature dependencies. By using large-scale training data and various self-supervised signals (e.g., masked random patches), vision transformers provide state-of-the-art performance on several benchmarking datasets, such as ImageNet-1k and CIFAR-10. However, these vision transformers pretrained over general large-scale image corpora could only produce an anisotropic representation space, limiting their generalizability and transferability to the target downstream tasks. In this paper, we propose a simple and effective Label-aware Contrastive Training framework LaCViT, which improves the isotropy of the pretrained representation space for vision transformers, thereby enabling more effective transfer learning amongst a wide range of image classification tasks. Through experimentation over five standard image classification datasets, we demonstrate that LaCViT-trained m",
    "path": "papers/23/03/2303.18013.json",
    "total_tokens": 911,
    "translated_title": "LaCViT：一种面向标签的对比训练框架，提高视觉Transformer的表示空间的等性",
    "translated_abstract": "视觉 Transformer 已经在处理计算机视觉任务时表现出了惊人的效果，这是由于其模拟长时间的特征依赖能力。通过使用大规模的训练数据和各种自监督信号（例如，遮蔽随机块），视觉 Transformer 在 ImageNet-1k 和 CIFAR-10 等几个基准数据集上提供了最先进的性能。然而，这些基于通用大规模图像语料库预训练的视觉Transformer只能产生各向异性表示空间，限制了它们在目标下游任务中的通用性和可转移性。在本文中，我们提出了一种简单而有效的面向标签的对比训练框架 LaCViT，它提高了视觉Transformer预训练表示空间的等性，从而实现了更有效的转移学习。通过对五个标准图像分类数据集的实验，我们证明了LaCViT训练的模型在各种图像分类任务中都具有卓越的性能。",
    "tldr": "LaCViT是一种针对视觉Transformer预训练表示空间的各向等性不足问题，提高其表示空间等性的面向标签的对比训练框架，经过实验证明其在五个标准图像分类数据集中具有卓越的性能。",
    "en_tdlr": "LaCViT is a label-aware contrastive training framework aimed at improving the isotropy of the representation space of pre-trained vision transformers, addressing the issue of anisotropy. Experimental results demonstrate its superior performance across five standard image classification datasets."
}