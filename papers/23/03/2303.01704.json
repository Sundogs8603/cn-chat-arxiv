{
    "title": "Feature Importance Disparities for Data Bias Investigations",
    "abstract": "It is widely held that one cause of downstream bias in classifiers is bias present in the training data. Rectifying such biases may involve context-dependent interventions such as training separate models on subgroups, removing features with bias in the collection process, or even conducting real-world experiments to ascertain sources of bias. Despite the need for such data bias investigations, few automated methods exist to assist practitioners in these efforts. In this paper, we present one such method that given a dataset $X$ consisting of protected and unprotected features, outcomes $y$, and a regressor $h$ that predicts $y$ given $X$, outputs a tuple $(f_j, g)$, with the following property: $g$ corresponds to a subset of the training dataset $(X, y)$, such that the $j^{th}$ feature $f_j$ has much larger (or smaller) influence in the subgroup $g$, than on the dataset overall, which we call feature importance disparity (FID). We show across $4$ datasets and $4$ common feature import",
    "link": "https://arxiv.org/abs/2303.01704",
    "context": "Title: Feature Importance Disparities for Data Bias Investigations\nAbstract: It is widely held that one cause of downstream bias in classifiers is bias present in the training data. Rectifying such biases may involve context-dependent interventions such as training separate models on subgroups, removing features with bias in the collection process, or even conducting real-world experiments to ascertain sources of bias. Despite the need for such data bias investigations, few automated methods exist to assist practitioners in these efforts. In this paper, we present one such method that given a dataset $X$ consisting of protected and unprotected features, outcomes $y$, and a regressor $h$ that predicts $y$ given $X$, outputs a tuple $(f_j, g)$, with the following property: $g$ corresponds to a subset of the training dataset $(X, y)$, such that the $j^{th}$ feature $f_j$ has much larger (or smaller) influence in the subgroup $g$, than on the dataset overall, which we call feature importance disparity (FID). We show across $4$ datasets and $4$ common feature import",
    "path": "papers/23/03/2303.01704.json",
    "total_tokens": 894,
    "translated_title": "数据偏差调查中的特征重要性差异",
    "translated_abstract": "广泛认为，分类器中的下游偏差的一种原因是训练数据中存在的偏差。纠正这种偏差可能涉及到依赖于上下文的干预措施，例如在子集上训练单独的模型，在收集过程中删除具有偏差的特征，甚至进行真实世界的实验以确定偏差源。尽管需要进行这样的数据偏差调查，但目前很少有自动化方法可以辅助从业人员进行这些工作。在本文中，我们提出了一种给定数据集$X$，包括保护和不保护的特征，结果$y$，以及一个预测给定$X$的回归器$h$的元组$(f_j, g)$, 其中$g$对应于训练数据集$(X, y)$的子集，使得第$j$个特征$f_j$在子组$g$中的影响要比整体数据集中大得多（或者小得多），我们将其称为特征重要性差异（FID）。我们在4个数据集和4个常见特征重要性中展示了这一点。",
    "tldr": "本文介绍了一种新的方法，通过给定一个数据集，利用特征重要性差异（FID）来调查数据偏差，并展示了实验证据支持该方法的有效性。",
    "en_tdlr": "This paper presents a new method that investigates data bias by utilizing feature importance disparities (FID) and provides experimental evidence supporting its effectiveness."
}