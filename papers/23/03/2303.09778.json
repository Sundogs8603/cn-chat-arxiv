{
    "title": "SE-GSL: A General and Effective Graph Structure Learning Framework through Structural Entropy Optimization. (arXiv:2303.09778v1 [cs.LG])",
    "abstract": "Graph Neural Networks (GNNs) are de facto solutions to structural data learning. However, it is susceptible to low-quality and unreliable structure, which has been a norm rather than an exception in real-world graphs. Existing graph structure learning (GSL) frameworks still lack robustness and interpretability. This paper proposes a general GSL framework, SE-GSL, through structural entropy and the graph hierarchy abstracted in the encoding tree. Particularly, we exploit the one-dimensional structural entropy to maximize embedded information content when auxiliary neighbourhood attributes are fused to enhance the original graph. A new scheme of constructing optimal encoding trees is proposed to minimize the uncertainty and noises in the graph whilst assuring proper community partition in hierarchical abstraction. We present a novel sample-based mechanism for restoring the graph structure via node structural entropy distribution. It increases the connectivity among nodes with larger unce",
    "link": "http://arxiv.org/abs/2303.09778",
    "context": "Title: SE-GSL: A General and Effective Graph Structure Learning Framework through Structural Entropy Optimization. (arXiv:2303.09778v1 [cs.LG])\nAbstract: Graph Neural Networks (GNNs) are de facto solutions to structural data learning. However, it is susceptible to low-quality and unreliable structure, which has been a norm rather than an exception in real-world graphs. Existing graph structure learning (GSL) frameworks still lack robustness and interpretability. This paper proposes a general GSL framework, SE-GSL, through structural entropy and the graph hierarchy abstracted in the encoding tree. Particularly, we exploit the one-dimensional structural entropy to maximize embedded information content when auxiliary neighbourhood attributes are fused to enhance the original graph. A new scheme of constructing optimal encoding trees is proposed to minimize the uncertainty and noises in the graph whilst assuring proper community partition in hierarchical abstraction. We present a novel sample-based mechanism for restoring the graph structure via node structural entropy distribution. It increases the connectivity among nodes with larger unce",
    "path": "papers/23/03/2303.09778.json",
    "total_tokens": 954,
    "translated_title": "SE-GSL：一种通过结构熵优化实现通用有效图结构学习的框架。",
    "translated_abstract": "图神经网络是学习结构化数据的实际解决方案。 然而，它容易受到低质量和不可靠结构的影响，这在真实世界的图中是常态而不是例外。现有的图结构学习框架仍然缺乏鲁棒性和可解释性。本文通过结构熵和编码树中抽象的图层次结构提出了一种通用的GSL框架SE-GSL。特别地，我们利用一维结构熵来最大化嵌入信息内容，当辅助邻域属性被融合以增强原始图时。提出了一种构建最优编码树的新方案，以在分层抽象中最小化图中的不确定性和噪音，同时确保适当的社区划分。我们提出了一个新颖的基于样本的机制，通过节点结构熵分布来恢复图结构。它增加了更大不确定性的节点之间的连通性。",
    "tldr": "本论文提出了一个通用且有效的图结构学习框架SE-GSL，通过利用结构熵和编码树中的层次结构来最大化嵌入信息内容，同时提出了一个声慢构建最优编码树的方案。该框架还提出了一个新颖的基于样本的机制，通过节点结构熵分布来恢复图结构。",
    "en_tdlr": "The paper proposes a general and effective graph structure learning framework, SE-GSL, which maximizes the embedded information content by using structural entropy and the hierarchy abstracted in the encoding tree, and presents a new scheme of constructing an optimal encoding tree. The framework also introduces a novel sample-based mechanism for restoring the graph structure via node structural entropy distribution."
}