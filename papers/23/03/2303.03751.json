{
    "title": "Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles",
    "abstract": "arXiv:2303.03751v2 Announce Type: replace-cross  Abstract: In this study, we delve into an emerging optimization challenge involving a black-box objective function that can only be gauged via a ranking oracle-a situation frequently encountered in real-world scenarios, especially when the function is evaluated by human judges. Such challenge is inspired from Reinforcement Learning with Human Feedback (RLHF), an approach recently employed to enhance the performance of Large Language Models (LLMs) using human guidance. We introduce ZO-RankSGD, an innovative zeroth-order optimization algorithm designed to tackle this optimization problem, accompanied by theoretical assurances. Our algorithm utilizes a novel rank-based random estimator to determine the descent direction and guarantees convergence to a stationary point. Moreover, ZO-RankSGD is readily applicable to policy optimization problems in Reinforcement Learning (RL), particularly when only ranking oracles for the episode reward are a",
    "link": "https://arxiv.org/abs/2303.03751",
    "context": "Title: Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles\nAbstract: arXiv:2303.03751v2 Announce Type: replace-cross  Abstract: In this study, we delve into an emerging optimization challenge involving a black-box objective function that can only be gauged via a ranking oracle-a situation frequently encountered in real-world scenarios, especially when the function is evaluated by human judges. Such challenge is inspired from Reinforcement Learning with Human Feedback (RLHF), an approach recently employed to enhance the performance of Large Language Models (LLMs) using human guidance. We introduce ZO-RankSGD, an innovative zeroth-order optimization algorithm designed to tackle this optimization problem, accompanied by theoretical assurances. Our algorithm utilizes a novel rank-based random estimator to determine the descent direction and guarantees convergence to a stationary point. Moreover, ZO-RankSGD is readily applicable to policy optimization problems in Reinforcement Learning (RL), particularly when only ranking oracles for the episode reward are a",
    "path": "papers/23/03/2303.03751.json",
    "total_tokens": 957,
    "translated_title": "零阶优化遇到人工反馈：通过排名预测实现可证明学习",
    "translated_abstract": "在这项研究中，我们探讨了一种新兴的优化挑战，其中涉及到一个只能通过排名预测来评估的黑盒目标函数-这种情况在实际场景中经常遇到，特别是当函数由人类评判员评估时。这种挑战受到了强化学习与人工反馈（RLHF）的启发，这是一种最近用来提高大型语言模型（LLMs）性能的方法。我们引入了一种创新的零阶优化算法ZO-RankSGD来解决这个优化问题，并提供了理论保证。我们的算法利用一种新颖的基于排名的随机估计器来确定下降方向，并保证收敛到一个稳定点。此外，ZO-RankSGD可以直接应用于增强学习中的策略优化问题，特别是当只有对于回报排名的排名预测时。",
    "tldr": "零阶优化算法ZO-RankSGD解决了一个新兴的优化挑战，即只能通过排名预测来评估黑盒目标函数。该算法利用一种新颖的随机估计器来确定下降方向，并保证收敛到一个稳定点。此外，该算法还可用于增强学习中的策略优化问题，特别是当只有对于回报排名的排名预测时。",
    "en_tdlr": "The zeroth-order optimization algorithm ZO-RankSGD tackles the emerging challenge of optimizing a black-box objective function using ranking oracles. The algorithm utilizes a novel rank-based random estimator for determining the descent direction and guarantees convergence to a stationary point. Additionally, it can be applied to policy optimization problems in reinforcement learning, particularly when only ranking oracles for the episode reward are available."
}