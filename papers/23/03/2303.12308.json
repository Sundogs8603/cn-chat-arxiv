{
    "title": "XWikiGen: Cross-lingual Summarization for Encyclopedic Text Generation in Low Resource Languages. (arXiv:2303.12308v1 [cs.CL])",
    "abstract": "Lack of encyclopedic text contributors, especially on Wikipedia, makes automated text generation for \\emph{low resource (LR) languages} a critical problem. Existing work on Wikipedia text generation has focused on \\emph{English only} where English reference articles are summarized to generate English Wikipedia pages. But, for low-resource languages, the scarcity of reference articles makes monolingual summarization ineffective in solving this problem. Hence, in this work, we propose \\task{}, which is the task of cross-lingual multi-document summarization of text from multiple reference articles, written in various languages, to generate Wikipedia-style text. Accordingly, we contribute a benchmark dataset, \\data{}, spanning $\\sim$69K Wikipedia articles covering five domains and eight languages. We harness this dataset to train a two-stage system where the input is a set of citations and a section title and the output is a section-specific LR summary. The proposed system is based on a no",
    "link": "http://arxiv.org/abs/2303.12308",
    "context": "Title: XWikiGen: Cross-lingual Summarization for Encyclopedic Text Generation in Low Resource Languages. (arXiv:2303.12308v1 [cs.CL])\nAbstract: Lack of encyclopedic text contributors, especially on Wikipedia, makes automated text generation for \\emph{low resource (LR) languages} a critical problem. Existing work on Wikipedia text generation has focused on \\emph{English only} where English reference articles are summarized to generate English Wikipedia pages. But, for low-resource languages, the scarcity of reference articles makes monolingual summarization ineffective in solving this problem. Hence, in this work, we propose \\task{}, which is the task of cross-lingual multi-document summarization of text from multiple reference articles, written in various languages, to generate Wikipedia-style text. Accordingly, we contribute a benchmark dataset, \\data{}, spanning $\\sim$69K Wikipedia articles covering five domains and eight languages. We harness this dataset to train a two-stage system where the input is a set of citations and a section title and the output is a section-specific LR summary. The proposed system is based on a no",
    "path": "papers/23/03/2303.12308.json",
    "total_tokens": 1100,
    "tldr": "本文提出了一种跨语言多文档摘要的任务，以解决低资源语言的百科全书文本生成问题。我们贡献了一个基准数据集，并提出了一个利用跨语言预训练模型和单语言模型的两阶段系统来生成LR摘要。相对于多个基线，我们的系统在低资源语言上表现更好。",
    "en_tdlr": "This paper proposes a cross-lingual multi-document summarization task to tackle the problem of automated text generation in low resource languages for encyclopedias, and contributes a benchmark dataset to train a two-stage system utilizing both cross-lingual pre-trained and single-language models to generate section-specific LR summaries. The proposed system outperforms multiple baselines, especially in low resource languages."
}