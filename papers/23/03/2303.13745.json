{
    "title": "EdgeTran: Co-designing Transformers for Efficient Inference on Mobile Edge Platforms. (arXiv:2303.13745v1 [cs.LG])",
    "abstract": "Automated design of efficient transformer models has recently attracted significant attention from industry and academia. However, most works only focus on certain metrics while searching for the best-performing transformer architecture. Furthermore, running traditional, complex, and large transformer models on low-compute edge platforms is a challenging problem. In this work, we propose a framework, called ProTran, to profile the hardware performance measures for a design space of transformer architectures and a diverse set of edge devices. We use this profiler in conjunction with the proposed co-design technique to obtain the best-performing models that have high accuracy on the given task and minimize latency, energy consumption, and peak power draw to enable edge deployment. We refer to our framework for co-optimizing accuracy and hardware performance measures as EdgeTran. It searches for the best transformer model and edge device pair. Finally, we propose GPTran, a multi-stage blo",
    "link": "http://arxiv.org/abs/2303.13745",
    "context": "Title: EdgeTran: Co-designing Transformers for Efficient Inference on Mobile Edge Platforms. (arXiv:2303.13745v1 [cs.LG])\nAbstract: Automated design of efficient transformer models has recently attracted significant attention from industry and academia. However, most works only focus on certain metrics while searching for the best-performing transformer architecture. Furthermore, running traditional, complex, and large transformer models on low-compute edge platforms is a challenging problem. In this work, we propose a framework, called ProTran, to profile the hardware performance measures for a design space of transformer architectures and a diverse set of edge devices. We use this profiler in conjunction with the proposed co-design technique to obtain the best-performing models that have high accuracy on the given task and minimize latency, energy consumption, and peak power draw to enable edge deployment. We refer to our framework for co-optimizing accuracy and hardware performance measures as EdgeTran. It searches for the best transformer model and edge device pair. Finally, we propose GPTran, a multi-stage blo",
    "path": "papers/23/03/2303.13745.json",
    "total_tokens": 900,
    "translated_title": "EdgeTran：在移动边缘平台上共同设计Transformer以实现高效推理",
    "translated_abstract": "自动化设计高效Transformer模型近来广受行业和学术界关注。但大多数工作只关注某些度量标准，而搜索最佳性能的Transformer架构。此外，在低计算边缘平台上运行传统的复杂大型Transformer模型是一个难题。本文提出了一种名为ProTran的框架，通过对Transformer体系结构的设计空间和不同种类的边缘设备进行硬件性能度量来削减延迟、能耗和波峰功耗，从而获得在给定任务上具有高准确性的最佳性能模型。我们称共同优化准确性和硬件性能度量的框架为EdgeTran。最后，我们提出了GPTran，一种多阶段的块稀疏训练算法，以帮助优化模型的内存占用和推理效率。",
    "tldr": "EdgeTran框架旨在设计适用于移动边缘设备且在准确性、延迟、能耗和波峰功耗方面表现优秀的Transformer模型。同时，作者提出了块稀疏训练算法GPTran以帮助优化模型的内存占用和推理效率。",
    "en_tdlr": "EdgeTran framework aims to design Transformer models that are suitable for mobile edge devices and perform well in accuracy, latency, energy consumption, and peak power draw. Meanwhile, the authors proposed the block sparse training algorithm GPTran to help optimize the memory consumption and inference efficiency of the model."
}