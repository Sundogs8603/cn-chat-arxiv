{
    "title": "Planning with Sequence Models through Iterative Energy Minimization. (arXiv:2303.16189v1 [cs.LG])",
    "abstract": "Recent works have shown that sequence modeling can be effectively used to train reinforcement learning (RL) policies. However, the success of applying existing sequence models to planning, in which we wish to obtain a trajectory of actions to reach some goal, is less straightforward. The typical autoregressive generation procedures of sequence models preclude sequential refinement of earlier steps, which limits the effectiveness of a predicted plan. In this paper, we suggest an approach towards integrating planning with sequence models based on the idea of iterative energy minimization, and illustrate how such a procedure leads to improved RL performance across different tasks. We train a masked language model to capture an implicit energy function over trajectories of actions, and formulate planning as finding a trajectory of actions with minimum energy. We illustrate how this procedure enables improved performance over recent approaches across BabyAI and Atari environments. We furthe",
    "link": "http://arxiv.org/abs/2303.16189",
    "context": "Title: Planning with Sequence Models through Iterative Energy Minimization. (arXiv:2303.16189v1 [cs.LG])\nAbstract: Recent works have shown that sequence modeling can be effectively used to train reinforcement learning (RL) policies. However, the success of applying existing sequence models to planning, in which we wish to obtain a trajectory of actions to reach some goal, is less straightforward. The typical autoregressive generation procedures of sequence models preclude sequential refinement of earlier steps, which limits the effectiveness of a predicted plan. In this paper, we suggest an approach towards integrating planning with sequence models based on the idea of iterative energy minimization, and illustrate how such a procedure leads to improved RL performance across different tasks. We train a masked language model to capture an implicit energy function over trajectories of actions, and formulate planning as finding a trajectory of actions with minimum energy. We illustrate how this procedure enables improved performance over recent approaches across BabyAI and Atari environments. We furthe",
    "path": "papers/23/03/2303.16189.json",
    "total_tokens": 828,
    "translated_title": "通过迭代能量最小化进行序列模型规划",
    "translated_abstract": "最近的研究表明，序列建模可以有效地用于训练强化学习（RL）策略。然而，将现有的序列模型应用于规划，即希望获得一系列动作的轨迹以达到某个目标，并不那么直接。序列模型的典型自回归生成过程排除了对较早步骤的顺序细化，这限制了预测计划的有效性。在本文中，我们提出了一种与迭代能量最小化相关的方法，将规划与序列模型进行集成，并说明这种过程如何在不同任务中导致改进的RL性能。我们训练了一个掩码语言模型，捕捉了动作轨迹上的隐式能量函数，并将规划形式化为寻找具有最小能量的动作轨迹。我们说明了这个过程如何在BabyAI和Atari环境下实现改进的性能。",
    "tldr": "本文提出了一种基于迭代能量最小化方法的序列模型规划集成方法，能够提高强化学习性能在BabyAI和Atari等不同任务中具有应用价值。",
    "en_tdlr": "This paper proposes an integrated method of planning with sequence models based on iterative energy minimization, which can improve RL performance in different tasks such as BabyAI and Atari."
}