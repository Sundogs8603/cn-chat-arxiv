{
    "title": "Variational Distribution Learning for Unsupervised Text-to-Image Generation. (arXiv:2303.16105v1 [cs.CV])",
    "abstract": "We propose a text-to-image generation algorithm based on deep neural networks when text captions for images are unavailable during training. In this work, instead of simply generating pseudo-ground-truth sentences of training images using existing image captioning methods, we employ a pretrained CLIP model, which is capable of properly aligning embeddings of images and corresponding texts in a joint space and, consequently, works well on zero-shot recognition tasks. We optimize a text-to-image generation model by maximizing the data log-likelihood conditioned on pairs of image-text CLIP embeddings. To better align data in the two domains, we employ a principled way based on a variational inference, which efficiently estimates an approximate posterior of the hidden text embedding given an image and its CLIP feature. Experimental results validate that the proposed framework outperforms existing approaches by large margins under unsupervised and semi-supervised text-to-image generation se",
    "link": "http://arxiv.org/abs/2303.16105",
    "context": "Title: Variational Distribution Learning for Unsupervised Text-to-Image Generation. (arXiv:2303.16105v1 [cs.CV])\nAbstract: We propose a text-to-image generation algorithm based on deep neural networks when text captions for images are unavailable during training. In this work, instead of simply generating pseudo-ground-truth sentences of training images using existing image captioning methods, we employ a pretrained CLIP model, which is capable of properly aligning embeddings of images and corresponding texts in a joint space and, consequently, works well on zero-shot recognition tasks. We optimize a text-to-image generation model by maximizing the data log-likelihood conditioned on pairs of image-text CLIP embeddings. To better align data in the two domains, we employ a principled way based on a variational inference, which efficiently estimates an approximate posterior of the hidden text embedding given an image and its CLIP feature. Experimental results validate that the proposed framework outperforms existing approaches by large margins under unsupervised and semi-supervised text-to-image generation se",
    "path": "papers/23/03/2303.16105.json",
    "total_tokens": 875,
    "translated_title": "无监督文本到图像生成的变分分布学习",
    "translated_abstract": "本文提出了一种基于深度神经网络的文本到图像生成算法，在训练过程中没有图像的文本说明。我们使用预训练的CLIP模型来生成图像与对应文本的嵌入，从而更好地将两个域中的数据对齐，并以数据对的图像-文本CLIP嵌入为条件，通过最大化数据对数似然来优化文本到图像生成模型。为了更好地对齐这两个域中的数据，我们基于变分推断提出了一种合理的方法，有效地估计了给定图像及其CLIP特征的隐藏文本嵌入的近似后验分布。实验结果表明，在无监督和半监督文本到图像生成中，所提出的框架比现有方法有着更好的表现。",
    "tldr": "本文基于CLIP模型，提出了一种无监督文本到图像生成的算法，通过最大化数据对数似然优化文本到图像生成模型。实验结果表明，该算法在无监督和半监督文本到图像生成任务中，优于现有方法。",
    "en_tdlr": "This paper proposes a variational distribution learning algorithm for unsupervised text-to-image generation, using a pretrained CLIP model to generate embeddings of images and corresponding texts. The proposed algorithm optimizes the text-to-image generation model by maximizing the data log-likelihood conditioned on pairs of image-text CLIP embeddings. Experimental results show that the proposed algorithm outperforms existing approaches in unsupervised and semi-supervised text-to-image generation tasks."
}