{
    "title": "An Empirical Analysis of the Shift and Scale Parameters in BatchNorm. (arXiv:2303.12818v1 [cs.LG])",
    "abstract": "Batch Normalization (BatchNorm) is a technique that improves the training of deep neural networks, especially Convolutional Neural Networks (CNN). It has been empirically demonstrated that BatchNorm increases performance, stability, and accuracy, although the reasons for such improvements are unclear. BatchNorm includes a normalization step as well as trainable shift and scale parameters. In this paper, we empirically examine the relative contribution to the success of BatchNorm of the normalization step, as compared to the re-parameterization via shifting and scaling. To conduct our experiments, we implement two new optimizers in PyTorch, namely, a version of BatchNorm that we refer to as AffineLayer, which includes the re-parameterization step without normalization, and a version with just the normalization step, that we call BatchNorm-minus. We compare the performance of our AffineLayer and BatchNorm-minus implementations to standard BatchNorm, and we also compare these to the case ",
    "link": "http://arxiv.org/abs/2303.12818",
    "context": "Title: An Empirical Analysis of the Shift and Scale Parameters in BatchNorm. (arXiv:2303.12818v1 [cs.LG])\nAbstract: Batch Normalization (BatchNorm) is a technique that improves the training of deep neural networks, especially Convolutional Neural Networks (CNN). It has been empirically demonstrated that BatchNorm increases performance, stability, and accuracy, although the reasons for such improvements are unclear. BatchNorm includes a normalization step as well as trainable shift and scale parameters. In this paper, we empirically examine the relative contribution to the success of BatchNorm of the normalization step, as compared to the re-parameterization via shifting and scaling. To conduct our experiments, we implement two new optimizers in PyTorch, namely, a version of BatchNorm that we refer to as AffineLayer, which includes the re-parameterization step without normalization, and a version with just the normalization step, that we call BatchNorm-minus. We compare the performance of our AffineLayer and BatchNorm-minus implementations to standard BatchNorm, and we also compare these to the case ",
    "path": "papers/23/03/2303.12818.json",
    "total_tokens": 826,
    "translated_title": "BatchNorm中Shift和Scale参数的实证分析",
    "translated_abstract": "Batch Normalization（BatchNorm）是一种改善深度神经网络训练的技术，特别是卷积神经网络（CNN）。虽然不清楚这种改进的原因，但已经经验证明BatchNorm可以增加性能，稳定性和准确性。BatchNorm包括归一化步骤以及可训练的Shift和Scale参数。在本文中，我们通过实证研究归一化步骤相对于移位和缩放的重新参数化对BatchNorm成功的贡献度。为了进行实验，我们在PyTorch中实现了两个新的优化器，分别为包含重新参数化步骤但不进行归一化（称为AffineLayer）和仅包含归一化步骤的版本（称为BatchNorm-minus）。我们将我们的AffineLayer和BatchNorm-minus的性能与标准BatchNorm进行比较，并将这些与不使用BatchNorm进行比较。",
    "tldr": "本文通过实验比较重新参数化步骤与归一化步骤对BatchNorm成功的贡献，以研究BatchNorm中Shift和Scale参数的作用。"
}