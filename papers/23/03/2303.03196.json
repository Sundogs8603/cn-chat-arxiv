{
    "title": "Population-based Evaluation in Repeated Rock-Paper-Scissors as a Benchmark for Multiagent Reinforcement Learning. (arXiv:2303.03196v2 [cs.GT] UPDATED)",
    "abstract": "Progress in fields of machine learning and adversarial planning has benefited significantly from benchmark domains, from checkers and the classic UCI data sets to Go and Diplomacy. In sequential decision-making, agent evaluation has largely been restricted to few interactions against experts, with the aim to reach some desired level of performance (e.g. beating a human professional player). We propose a benchmark for multiagent learning based on repeated play of the simple game Rock, Paper, Scissors along with a population of forty-three tournament entries, some of which are intentionally sub-optimal. We describe metrics to measure the quality of agents based both on average returns and exploitability. We then show that several RL, online learning, and language model approaches can learn good counter-strategies and generalize well, but ultimately lose to the top-performing bots, creating an opportunity for research in multiagent learning.",
    "link": "http://arxiv.org/abs/2303.03196",
    "context": "Title: Population-based Evaluation in Repeated Rock-Paper-Scissors as a Benchmark for Multiagent Reinforcement Learning. (arXiv:2303.03196v2 [cs.GT] UPDATED)\nAbstract: Progress in fields of machine learning and adversarial planning has benefited significantly from benchmark domains, from checkers and the classic UCI data sets to Go and Diplomacy. In sequential decision-making, agent evaluation has largely been restricted to few interactions against experts, with the aim to reach some desired level of performance (e.g. beating a human professional player). We propose a benchmark for multiagent learning based on repeated play of the simple game Rock, Paper, Scissors along with a population of forty-three tournament entries, some of which are intentionally sub-optimal. We describe metrics to measure the quality of agents based both on average returns and exploitability. We then show that several RL, online learning, and language model approaches can learn good counter-strategies and generalize well, but ultimately lose to the top-performing bots, creating an opportunity for research in multiagent learning.",
    "path": "papers/23/03/2303.03196.json",
    "total_tokens": 885,
    "translated_title": "作为多Agent强化学习基准的重复剪刀石头布的基于人口评估",
    "translated_abstract": "机器学习和对抗规划领域的进展，很大程度上受益于基准域，从国际象棋和经典的UCI数据集到围棋和外交。在顺序决策中，对Agent评估主要局限于与专家进行少量交互，旨在达到一定的性能水平（如击败人类专业玩家）。我们提出了一种基于剪刀石头布的多Agent学习基准，其中包括四十三个锦标赛参赛作品，其中一些是有意的次优作品。我们描述了基于平均回报和可开发性的代理质量度量标准。然后，我们展示了几种RL、在线学习和语言模型方法可以学习良好的反策略，并具有良好的泛化能力，但最终会输给表现最佳的机器人，为多Agent学习的研究提供了机会。",
    "tldr": "这项研究提出了一种基于重复剪刀石头布游戏的多Agent学习基准，展示了几种学习方法的泛化能力，为多Agent学习领域的研究提供了机会。",
    "en_tdlr": "This research proposes a benchmark for multiagent learning based on repeated play of Rock, Paper, Scissors, showing the generalization capability of several learning methods and providing an opportunity for research in the field of multiagent learning."
}