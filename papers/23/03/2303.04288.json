{
    "title": "Polynomial Time and Private Learning of Unbounded Gaussian Mixture Models. (arXiv:2303.04288v2 [stat.ML] UPDATED)",
    "abstract": "We study the problem of privately estimating the parameters of $d$-dimensional Gaussian Mixture Models (GMMs) with $k$ components. For this, we develop a technique to reduce the problem to its non-private counterpart. This allows us to privatize existing non-private algorithms in a blackbox manner, while incurring only a small overhead in the sample complexity and running time. As the main application of our framework, we develop an $(\\varepsilon, \\delta)$-differentially private algorithm to learn GMMs using the non-private algorithm of Moitra and Valiant [MV10] as a blackbox. Consequently, this gives the first sample complexity upper bound and first polynomial time algorithm for privately learning GMMs without any boundedness assumptions on the parameters. As part of our analysis, we prove a tight (up to a constant factor) lower bound on the total variation distance of high-dimensional Gaussians which can be of independent interest.",
    "link": "http://arxiv.org/abs/2303.04288",
    "context": "Title: Polynomial Time and Private Learning of Unbounded Gaussian Mixture Models. (arXiv:2303.04288v2 [stat.ML] UPDATED)\nAbstract: We study the problem of privately estimating the parameters of $d$-dimensional Gaussian Mixture Models (GMMs) with $k$ components. For this, we develop a technique to reduce the problem to its non-private counterpart. This allows us to privatize existing non-private algorithms in a blackbox manner, while incurring only a small overhead in the sample complexity and running time. As the main application of our framework, we develop an $(\\varepsilon, \\delta)$-differentially private algorithm to learn GMMs using the non-private algorithm of Moitra and Valiant [MV10] as a blackbox. Consequently, this gives the first sample complexity upper bound and first polynomial time algorithm for privately learning GMMs without any boundedness assumptions on the parameters. As part of our analysis, we prove a tight (up to a constant factor) lower bound on the total variation distance of high-dimensional Gaussians which can be of independent interest.",
    "path": "papers/23/03/2303.04288.json",
    "total_tokens": 900,
    "translated_title": "多项式时间和私有学习无限高斯混合模型",
    "translated_abstract": "本文研究了使用$k$个组件和$d$维高斯混合模型(GMMs)私下估计参数的问题。我们开发了一种技术，将此问题减少到非私有算法。这使我们能够以黑盒方式私有化现有的非私有算法，同时在样本复杂度和运行时间方面仅产生少量开销。作为我们框架的主要应用，我们使用Moitra和Valiant [MV10]的非私有算法开发了一个$(\\varepsilon, \\delta)$-差分隐私算法来学习GMMs。结果，这提供了第一个不对参数进行任何有界性假设的私有学习GMMs的样本复杂度上界和第一个多项式时间算法。作为我们分析的一部分，我们证明了高维高斯分布的总变差距离的紧密(高达常数因子)下界，这可以独立地得到研究。",
    "tldr": "本文开发了一种技术，将高维高斯混合模型的私有参数估计问题降低到非私有算法，从而使现有的非私有算法能够在隐私保护的条件下使用，提供了第一个多项式时间的私有学习GMMs算法。",
    "en_tdlr": "This paper develops a technique to reduce the problem of privately estimating the parameters of high-dimensional Gaussian Mixture Models to its non-private counterpart, providing the first polynomial time algorithm for privately learning GMMs without any boundedness assumptions on the parameters."
}