{
    "title": "Autoregressive Conditional Neural Processes. (arXiv:2303.14468v1 [stat.ML])",
    "abstract": "Conditional neural processes (CNPs; Garnelo et al., 2018a) are attractive meta-learning models which produce well-calibrated predictions and are trainable via a simple maximum likelihood procedure. Although CNPs have many advantages, they are unable to model dependencies in their predictions. Various works propose solutions to this, but these come at the cost of either requiring approximate inference or being limited to Gaussian predictions. In this work, we instead propose to change how CNPs are deployed at test time, without any modifications to the model or training procedure. Instead of making predictions independently for every target point, we autoregressively define a joint predictive distribution using the chain rule of probability, taking inspiration from the neural autoregressive density estimator (NADE) literature. We show that this simple procedure allows factorised Gaussian CNPs to model highly dependent, non-Gaussian predictive distributions. Perhaps surprisingly, in an e",
    "link": "http://arxiv.org/abs/2303.14468",
    "context": "Title: Autoregressive Conditional Neural Processes. (arXiv:2303.14468v1 [stat.ML])\nAbstract: Conditional neural processes (CNPs; Garnelo et al., 2018a) are attractive meta-learning models which produce well-calibrated predictions and are trainable via a simple maximum likelihood procedure. Although CNPs have many advantages, they are unable to model dependencies in their predictions. Various works propose solutions to this, but these come at the cost of either requiring approximate inference or being limited to Gaussian predictions. In this work, we instead propose to change how CNPs are deployed at test time, without any modifications to the model or training procedure. Instead of making predictions independently for every target point, we autoregressively define a joint predictive distribution using the chain rule of probability, taking inspiration from the neural autoregressive density estimator (NADE) literature. We show that this simple procedure allows factorised Gaussian CNPs to model highly dependent, non-Gaussian predictive distributions. Perhaps surprisingly, in an e",
    "path": "papers/23/03/2303.14468.json",
    "total_tokens": 623,
    "translated_title": "自回归条件神经过程",
    "translated_abstract": "有条件的神经过程是一种有吸引力元学习模型，能够产生良好校准的预测，并且可以通过简单的最大似然过程进行训练。本研究提出了一种改变有条件的神经过程在测试时部署方式的方法，从而能够建模高度相关的非高斯预测分布。",
    "tldr": "本论文提出了自回归条件神经过程模型，能够建模高度相关的非高斯预测分布。"
}