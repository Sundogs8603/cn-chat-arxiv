{
    "title": "Adversarially Contrastive Estimation of Conditional Neural Processes. (arXiv:2303.13004v1 [cs.LG])",
    "abstract": "Conditional Neural Processes~(CNPs) formulate distributions over functions and generate function observations with exact conditional likelihoods. CNPs, however, have limited expressivity for high-dimensional observations, since their predictive distribution is factorized into a product of unconstrained (typically) Gaussian outputs. Previously, this could be handled using latent variables or autoregressive likelihood, but at the expense of intractable training and quadratically increased complexity. Instead, we propose calibrating CNPs with an adversarial training scheme besides regular maximum likelihood estimates. Specifically, we train an energy-based model (EBM) with noise contrastive estimation, which enforces EBM to identify true observations from the generations of CNP. In this way, CNP must generate predictions closer to the ground-truth to fool EBM, instead of merely optimizing with respect to the fixed-form likelihood. From generative function reconstruction to downstream regr",
    "link": "http://arxiv.org/abs/2303.13004",
    "context": "Title: Adversarially Contrastive Estimation of Conditional Neural Processes. (arXiv:2303.13004v1 [cs.LG])\nAbstract: Conditional Neural Processes~(CNPs) formulate distributions over functions and generate function observations with exact conditional likelihoods. CNPs, however, have limited expressivity for high-dimensional observations, since their predictive distribution is factorized into a product of unconstrained (typically) Gaussian outputs. Previously, this could be handled using latent variables or autoregressive likelihood, but at the expense of intractable training and quadratically increased complexity. Instead, we propose calibrating CNPs with an adversarial training scheme besides regular maximum likelihood estimates. Specifically, we train an energy-based model (EBM) with noise contrastive estimation, which enforces EBM to identify true observations from the generations of CNP. In this way, CNP must generate predictions closer to the ground-truth to fool EBM, instead of merely optimizing with respect to the fixed-form likelihood. From generative function reconstruction to downstream regr",
    "path": "papers/23/03/2303.13004.json",
    "total_tokens": 948,
    "translated_title": "对抗性对比条件神经过程估计",
    "translated_abstract": "条件神经过程(CNPs)通过准确的条件似然生成函数观测值，形成函数分布。然而，由于它们的预测分布被分解成一组无约束(通常为高斯分布)的输出，因此CNPs的表达能力对于高维度观测值是有限的。以前，可以使用潜变量或自回归似然来处理这个问题，但代价是难以训练和复杂度的平方级增加。我们提出了一种对抗性训练方案，使CNPs与常规的最大似然估计相结合。具体来说，我们使用噪声对比估计训练出一个能量基模型(EBM)，其要求EBM将真实观测值与CNP生成的样本区分出来。通过这种方式，CNP必须生成更接近基准答案的预测结果来欺骗EBM，而不仅仅是优化与固定形式似然有关的部分。从生成函数重构到下游回归任务，我们的方法在各种数据集上均表现优于强基线模型，这表明CNPs在建模复杂高维数据方面具有潜力。",
    "tldr": "本文提出了一种对抗性训练方案，通过噪声对比估计共同训练CNPs和EBM，并在各种数据集上改进了表现。",
    "en_tdlr": "This paper proposes an adversarial training scheme that jointly trains conditional neural processes (CNPs) and an energy-based model (EBM) with noise contrastive estimation, improving performance on various datasets and addressing the limited expressivity of CNPs for high-dimensional observations."
}