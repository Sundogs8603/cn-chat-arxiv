{
    "title": "Federated Stochastic Bandit Learning with Unobserved Context. (arXiv:2303.17043v1 [cs.LG])",
    "abstract": "We study the problem of federated stochastic multi-arm contextual bandits with unknown contexts, in which M agents are faced with different bandits and collaborate to learn. The communication model consists of a central server and the agents share their estimates with the central server periodically to learn to choose optimal actions in order to minimize the total regret. We assume that the exact contexts are not observable and the agents observe only a distribution of the contexts. Such a situation arises, for instance, when the context itself is a noisy measurement or based on a prediction mechanism. Our goal is to develop a distributed and federated algorithm that facilitates collaborative learning among the agents to select a sequence of optimal actions so as to maximize the cumulative reward. By performing a feature vector transformation, we propose an elimination-based algorithm and prove the regret bound for linearly parametrized reward functions. Finally, we validated the perfo",
    "link": "http://arxiv.org/abs/2303.17043",
    "context": "Title: Federated Stochastic Bandit Learning with Unobserved Context. (arXiv:2303.17043v1 [cs.LG])\nAbstract: We study the problem of federated stochastic multi-arm contextual bandits with unknown contexts, in which M agents are faced with different bandits and collaborate to learn. The communication model consists of a central server and the agents share their estimates with the central server periodically to learn to choose optimal actions in order to minimize the total regret. We assume that the exact contexts are not observable and the agents observe only a distribution of the contexts. Such a situation arises, for instance, when the context itself is a noisy measurement or based on a prediction mechanism. Our goal is to develop a distributed and federated algorithm that facilitates collaborative learning among the agents to select a sequence of optimal actions so as to maximize the cumulative reward. By performing a feature vector transformation, we propose an elimination-based algorithm and prove the regret bound for linearly parametrized reward functions. Finally, we validated the perfo",
    "path": "papers/23/03/2303.17043.json",
    "total_tokens": 855,
    "translated_title": "无观测上下文的联邦随机赌博学习",
    "translated_abstract": "本文研究了具有未知上下文的联邦随机多臂上下文赌博问题，其中M个代理面临不同的赌博机并协作学习。通信模型由中央服务器组成，并且代理会定期与中央服务器共享其估计结果，以便选择最优动作以最小化总后悔。我们假设精确的上下文不可观察，代理仅观测上下文的分布。例如，当上下文本身是噪声测量或基于预测机制时，就会出现这种情况。我们的目标是开发一种分布式联邦算法，促进代理之间的协作学习，选择一系列最优动作以最大化累积奖励。通过执行特征向量转换，我们提出了一种基于消除的算法，并证明了线性参数化奖励函数的后悔界。最后，我们验证了算法的性能。",
    "tldr": "本文提出了一种联邦随机多臂上下文赌博算法以最大化累积奖励，针对未知上下文的情况通过执行特征向量转换解决问题。",
    "en_tdlr": "This paper proposes a federated stochastic multi-arm contextual bandit algorithm to maximize the cumulative reward. By addressing the issue of unknown context through feature vector transformation, the algorithm aims to facilitate collaborative learning among agents."
}