{
    "title": "Provable Convergence of Variational Monte Carlo Methods. (arXiv:2303.10599v1 [stat.ML])",
    "abstract": "The Variational Monte Carlo (VMC) is a promising approach for computing the ground state energy of many-body quantum problems and attracts more and more interests due to the development of machine learning. The recent paradigms in VMC construct neural networks as trial wave functions, sample quantum configurations using Markov chain Monte Carlo (MCMC) and train neural networks with stochastic gradient descent (SGD) method. However, the theoretical convergence of VMC is still unknown when SGD interacts with MCMC sampling given a well-designed trial wave function. Since MCMC reduces the difficulty of estimating gradients, it has inevitable bias in practice. Moreover, the local energy may be unbounded, which makes it harder to analyze the error of MCMC sampling. Therefore, we assume that the local energy is sub-exponential and use the Bernstein inequality for non-stationary Markov chains to derive error bounds of the MCMC estimator. Consequently, VMC is proven to have a first order conver",
    "link": "http://arxiv.org/abs/2303.10599",
    "context": "Title: Provable Convergence of Variational Monte Carlo Methods. (arXiv:2303.10599v1 [stat.ML])\nAbstract: The Variational Monte Carlo (VMC) is a promising approach for computing the ground state energy of many-body quantum problems and attracts more and more interests due to the development of machine learning. The recent paradigms in VMC construct neural networks as trial wave functions, sample quantum configurations using Markov chain Monte Carlo (MCMC) and train neural networks with stochastic gradient descent (SGD) method. However, the theoretical convergence of VMC is still unknown when SGD interacts with MCMC sampling given a well-designed trial wave function. Since MCMC reduces the difficulty of estimating gradients, it has inevitable bias in practice. Moreover, the local energy may be unbounded, which makes it harder to analyze the error of MCMC sampling. Therefore, we assume that the local energy is sub-exponential and use the Bernstein inequality for non-stationary Markov chains to derive error bounds of the MCMC estimator. Consequently, VMC is proven to have a first order conver",
    "path": "papers/23/03/2303.10599.json",
    "total_tokens": 1061,
    "translated_title": "可验证变分蒙特卡罗方法的收敛性",
    "translated_abstract": "变分蒙特卡罗（VMC）是一种用于计算量子多体问题基态能量的有前途的方法，并由于机器学习的发展而越来越受到关注。最近的VMC方法以神经网络构建试探波函数，使用马尔可夫链蒙特卡罗（MCMC）采样量子态，并用随机梯度下降（SGD）方法训练神经网络。然而，当SGD与MCMC采样与设计良好的试探波函数交互作用时，VMC的理论收敛性仍然未知。由于MCMC降低了梯度估计的难度，实际上不可避免地存在偏差。此外，局部能量可能是无界的，这使得分析MCMC采样的误差更加困难。因此，我们假设局部能量是次指数的，并使用非平稳马尔可夫链的Bernstein不等式推导出MCMC估计量的误差界限。因此，在温和假设下，VMC被证明具有一阶收敛速率。此外，我们还展示了在某些情况下，收敛速率是最优的。",
    "tldr": "本文提出了一种对变分蒙特卡罗（VMC）方法收敛性的可验证方法，在假设局部能量是次指数的条件下，使用非平稳马尔可夫链的Bernstein不等式推导出了MCMC估计量的误差界限，证明了VMC具有一阶收敛速率，在某些情况下，收敛速率是最优的。",
    "en_tdlr": "This paper proposes a verifiable approach to the convergence of the Variational Monte Carlo (VMC) method. Assuming the local energy is sub-exponential, the Bernstein inequality for non-stationary Markov chains is used to derive error bounds of the MCMC estimator. VMC is proven to have a first order convergent rate under mild assumptions, and the convergence rate is optimal in certain settings."
}