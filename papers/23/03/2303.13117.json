{
    "title": "RLOR: A Flexible Framework of Deep Reinforcement Learning for Operation Research. (arXiv:2303.13117v1 [math.OC])",
    "abstract": "Reinforcement learning has been applied in operation research and has shown promise in solving large combinatorial optimization problems. However, existing works focus on developing neural network architectures for certain problems. These works lack the flexibility to incorporate recent advances in reinforcement learning, as well as the flexibility of customizing model architectures for operation research problems. In this work, we analyze the end-to-end autoregressive models for vehicle routing problems and show that these models can benefit from the recent advances in reinforcement learning with a careful re-implementation of the model architecture. In particular, we re-implemented the Attention Model and trained it with Proximal Policy Optimization (PPO) in CleanRL, showing at least 8 times speed up in training time. We hereby introduce RLOR, a flexible framework for Deep Reinforcement Learning for Operation Research. We believe that a flexible framework is key to developing deep re",
    "link": "http://arxiv.org/abs/2303.13117",
    "context": "Title: RLOR: A Flexible Framework of Deep Reinforcement Learning for Operation Research. (arXiv:2303.13117v1 [math.OC])\nAbstract: Reinforcement learning has been applied in operation research and has shown promise in solving large combinatorial optimization problems. However, existing works focus on developing neural network architectures for certain problems. These works lack the flexibility to incorporate recent advances in reinforcement learning, as well as the flexibility of customizing model architectures for operation research problems. In this work, we analyze the end-to-end autoregressive models for vehicle routing problems and show that these models can benefit from the recent advances in reinforcement learning with a careful re-implementation of the model architecture. In particular, we re-implemented the Attention Model and trained it with Proximal Policy Optimization (PPO) in CleanRL, showing at least 8 times speed up in training time. We hereby introduce RLOR, a flexible framework for Deep Reinforcement Learning for Operation Research. We believe that a flexible framework is key to developing deep re",
    "path": "papers/23/03/2303.13117.json",
    "total_tokens": 944,
    "translated_title": "RLOR:一种灵活的深度强化学习框架，用于运筹学",
    "translated_abstract": "强化学习已经应用于运筹学中，显示出在解决大型组合优化问题方面的潜力。然而，现有的研究侧重于针对某些问题开发神经网络架构，这些研究缺乏将强化学习的最新进展和自定义模型架构用于运筹学问题的灵活性。在本研究中，我们分析了车辆路径问题的端到端自回归模型，并展示了这些模型可以通过仔细重新实施模型架构来获益于强化学习的最新进展。特别地，我们重新实现了注意力模型，并在CleanRL中使用近端策略优化(PPO)进行训练，展示出至少8倍的训练时间加速。我们在此引入了RLOR，一种用于运筹学的灵活深度强化学习框架。我们相信灵活的框架对于开发各种运筹学问题的深度强化学习模型至关重要。",
    "tldr": "本文提出了灵活的深度强化学习框架RLOR，能够用于各种运筹学问题。我们重新实现了车辆路径问题的自回归模型，并展示了这些模型可以从强化学习的最新进展中受益，同时也提高了训练速度。",
    "en_tdlr": "This paper proposes a flexible framework for deep reinforcement learning called RLOR, which can be applied to various operation research problems. The authors re-implemented the autoregressive models for vehicle routing problems and showed that these models can benefit from recent advances in reinforcement learning while also improving training speed."
}