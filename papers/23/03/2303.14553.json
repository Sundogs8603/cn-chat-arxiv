{
    "title": "Complexity-calibrated Benchmarks for Machine Learning Reveal When Next-Generation Reservoir Computer Predictions Succeed and Mislead. (arXiv:2303.14553v1 [cs.LG])",
    "abstract": "Recurrent neural networks are used to forecast time series in finance, climate, language, and from many other domains. Reservoir computers are a particularly easily trainable form of recurrent neural network. Recently, a \"next-generation\" reservoir computer was introduced in which the memory trace involves only a finite number of previous symbols. We explore the inherent limitations of finite-past memory traces in this intriguing proposal. A lower bound from Fano's inequality shows that, on highly non-Markovian processes generated by large probabilistic state machines, next-generation reservoir computers with reasonably long memory traces have an error probability that is at least ~ 60% higher than the minimal attainable error probability in predicting the next observation. More generally, it appears that popular recurrent neural networks fall far short of optimally predicting such complex processes. These results highlight the need for a new generation of optimized recurrent neural ne",
    "link": "http://arxiv.org/abs/2303.14553",
    "context": "Title: Complexity-calibrated Benchmarks for Machine Learning Reveal When Next-Generation Reservoir Computer Predictions Succeed and Mislead. (arXiv:2303.14553v1 [cs.LG])\nAbstract: Recurrent neural networks are used to forecast time series in finance, climate, language, and from many other domains. Reservoir computers are a particularly easily trainable form of recurrent neural network. Recently, a \"next-generation\" reservoir computer was introduced in which the memory trace involves only a finite number of previous symbols. We explore the inherent limitations of finite-past memory traces in this intriguing proposal. A lower bound from Fano's inequality shows that, on highly non-Markovian processes generated by large probabilistic state machines, next-generation reservoir computers with reasonably long memory traces have an error probability that is at least ~ 60% higher than the minimal attainable error probability in predicting the next observation. More generally, it appears that popular recurrent neural networks fall far short of optimally predicting such complex processes. These results highlight the need for a new generation of optimized recurrent neural ne",
    "path": "papers/23/03/2303.14553.json",
    "total_tokens": 961,
    "translated_title": "机器学习的复杂性校准基准揭示了下一代水库计算机预测成功和误导的时机。",
    "translated_abstract": "循环神经网络被用于预测金融、气候、语言和其他领域的时间序列。水库计算机是一种特别易于训练的循环神经网络形式。最近引入了“下一代”水库计算机，其中内存跟踪仅涉及有限数量的先前符号。我们探讨了这种有趣提议中有限历史记忆痕迹的固有限制。从范诺不等式的下界可以看出，在由大型概率状态机生成的高度非马尔可夫过程中，具有相当长存储痕迹的下一代水库计算机的错误概率比预测下一个观察的最小可达错误概率高至少~60%。总的来说，似乎流行的循环神经网络远远无法实现最佳预测这种复杂过程的目标。这些结果凸显了一种新一代优化循环神经网络的需求。",
    "tldr": "本文讨论了有限历史记忆痕迹在下一代水库计算机中存在的固有限制，并发现在高度非马尔可夫过程中，流行的循环神经网络远远无法实现最佳预测这种复杂过程的目标。这些结果凸显了一种新一代优化循环神经网络的需求。",
    "en_tdlr": "This paper discusses the inherent limitations of finite-past memory traces in the next-generation reservoir computer, and finds that popular recurrent neural networks fall far short of optimally predicting complex processes generated by large probabilistic state machines. These results highlight the need for a new generation of optimized recurrent neural networks."
}