{
    "title": "Addressing Class Variable Imbalance in Federated Semi-supervised Learning. (arXiv:2303.11809v1 [cs.LG])",
    "abstract": "Federated Semi-supervised Learning (FSSL) combines techniques from both fields of federated and semi-supervised learning to improve the accuracy and performance of models in a distributed environment by using a small fraction of labeled data and a large amount of unlabeled data. Without the need to centralize all data in one place for training, it collect updates of model training after devices train models at local, and thus can protect the privacy of user data. However, during the federal training process, some of the devices fail to collect enough data for local training, while new devices will be included to the group training. This leads to an unbalanced global data distribution and thus affect the performance of the global model training. Most of the current research is focusing on class imbalance with a fixed number of classes, while little attention is paid to data imbalance with a variable number of classes. Therefore, in this paper, we propose Federated Semi-supervised Learni",
    "link": "http://arxiv.org/abs/2303.11809",
    "context": "Title: Addressing Class Variable Imbalance in Federated Semi-supervised Learning. (arXiv:2303.11809v1 [cs.LG])\nAbstract: Federated Semi-supervised Learning (FSSL) combines techniques from both fields of federated and semi-supervised learning to improve the accuracy and performance of models in a distributed environment by using a small fraction of labeled data and a large amount of unlabeled data. Without the need to centralize all data in one place for training, it collect updates of model training after devices train models at local, and thus can protect the privacy of user data. However, during the federal training process, some of the devices fail to collect enough data for local training, while new devices will be included to the group training. This leads to an unbalanced global data distribution and thus affect the performance of the global model training. Most of the current research is focusing on class imbalance with a fixed number of classes, while little attention is paid to data imbalance with a variable number of classes. Therefore, in this paper, we propose Federated Semi-supervised Learni",
    "path": "papers/23/03/2303.11809.json",
    "total_tokens": 1117,
    "translated_title": "解决联邦半监督学习中的类变量不平衡问题",
    "translated_abstract": "联邦半监督学习（FSSL）结合联邦学习和半监督学习的技术，通过使用少量标注数据和大量未标注数据在分布式环境中提高模型的准确性和性能。在不需要将所有数据集中于一处进行训练的情况下，它会在设备本地训练模型后收集模型训练更新，从而可以保护用户数据的隐私。然而，在联邦训练过程中，一些设备无法收集足够的本地训练数据，同时新设备将被添加到组训练中。这导致不平衡的全局数据分布，从而影响全局模型训练的性能。大多数当前的研究着重于固定类别数量的类别不平衡问题，而很少有注意力放在具有可变类别数量的数据不平衡问题上。因此，在本文中，我们提出了联邦半监督学习与类变量不平衡（FSSL-CVI），它使用动态类权重方案来解决FSSL中的类变量不平衡问题。实验结果表明，我们提出的方法在具有类变量不平衡的各种数据集上的分类准确率方面优于现有的联邦学习和FSSL方法。",
    "tldr": "本文介绍了一种称为联邦半监督学习与类变量不平衡（FSSL-CVI）的新方法，它使用动态类别加权方案来处理FSSL中的类别变量不平衡问题，并且在多个数据集上进行了实验验证。通过实验结果，本文表明 FSSL-CVI 方法在各方面性能上优于现有的联邦学习和FSSL 方法。",
    "en_tdlr": "This paper proposes a new method called Federated Semi-supervised Learning with Class Variable Imbalance (FSSL-CVI) to address the problem of class variable imbalance in FSSL, and uses a dynamic class weighting scheme. The experimental results show that FSSL-CVI outperforms existing federated learning and FSSL methods on various datasets with class variable imbalance."
}