{
    "title": "Bayes Complexity of Learners vs Overfitting. (arXiv:2303.07874v1 [cs.LG])",
    "abstract": "We introduce a new notion of complexity of functions and we show that it has the following properties: (i) it governs a PAC Bayes-like generalization bound, (ii) for neural networks it relates to natural notions of complexity of functions (such as the variation), and (iii) it explains the generalization gap between neural networks and linear schemes. While there is a large set of papers which describes bounds that have each such property in isolation, and even some that have two, as far as we know, this is a first notion that satisfies all three of them. Moreover, in contrast to previous works, our notion naturally generalizes to neural networks with several layers.  Even though the computation of our complexity is nontrivial in general, an upper-bound is often easy to derive, even for higher number of layers and functions with structure, such as period functions. An upper-bound we derive allows to show a separation in the number of samples needed for good generalization between 2 and ",
    "link": "http://arxiv.org/abs/2303.07874",
    "context": "Title: Bayes Complexity of Learners vs Overfitting. (arXiv:2303.07874v1 [cs.LG])\nAbstract: We introduce a new notion of complexity of functions and we show that it has the following properties: (i) it governs a PAC Bayes-like generalization bound, (ii) for neural networks it relates to natural notions of complexity of functions (such as the variation), and (iii) it explains the generalization gap between neural networks and linear schemes. While there is a large set of papers which describes bounds that have each such property in isolation, and even some that have two, as far as we know, this is a first notion that satisfies all three of them. Moreover, in contrast to previous works, our notion naturally generalizes to neural networks with several layers.  Even though the computation of our complexity is nontrivial in general, an upper-bound is often easy to derive, even for higher number of layers and functions with structure, such as period functions. An upper-bound we derive allows to show a separation in the number of samples needed for good generalization between 2 and ",
    "path": "papers/23/03/2303.07874.json",
    "total_tokens": 1010,
    "translated_title": "学习者的贝叶斯复杂度与过拟合",
    "translated_abstract": "我们引入了一种新的函数复杂度概念，并展示了它具有以下特性：（i）它支配着PAC Bayes一样的概化界限，（ii）对于神经网络，它关联了函数复杂度的自然概念（如变量），（iii）它解释了神经网络和线性模型之间的概化差距。虽然已有许多针对这些特性的论文和界限，但据我们所知，这是第一个满足这三个条件的概念。此外，与以前的工作不同，我们的概念自然地推广到有多层的神经网络。虽然一般情况下计算我们的复杂度是非常困难的，但一般可导出一个上界，即使在更高层以及拥有结构化函数（如周期函数）的情况下也是如此。我们推导的上界允许展示样本数量在2和总样本数量之间的良好泛化分离差距。",
    "tldr": "本文引入了新的函数复杂度概念，它能够支配PAC Bayes一样的概化界限，关联神经网络的自然函数复杂度概念，并解释神经网络与线性模型之间的泛化差距。此外，我们的概念能够自然地推广到有多层的神经网络，并且可导出上界，在更高层以及拥有结构化函数的情况下也是如此。",
    "en_tdlr": "This paper proposes a new notion of function complexity, which governs a PAC Bayes-like generalization bound, relates to the natural notions of complexity of functions for neural networks, and explains the generalization gap between neural networks and linear schemes. The proposed notion can be applied to neural networks with multiple layers and an upper bound on its computation can be derived, even for higher number of layers and functions with structure, such as period functions."
}