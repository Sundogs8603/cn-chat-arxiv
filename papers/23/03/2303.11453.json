{
    "title": "Greedy Pruning with Group Lasso Provably Generalizes for Matrix Sensing and Neural Networks with Quadratic Activations. (arXiv:2303.11453v1 [cs.LG])",
    "abstract": "Pruning schemes have been widely used in practice to reduce the complexity of trained models with a massive number of parameters. Several practical studies have shown that pruning an overparameterized model and fine-tuning generalizes well to new samples. Although the above pipeline, which we refer to as pruning + fine-tuning, has been extremely successful in lowering the complexity of trained models, there is very little known about the theory behind this success. In this paper we address this issue by investigating the pruning + fine-tuning framework on the overparameterized matrix sensing problem, with the ground truth denoted $U_\\star \\in \\mathbb{R}^{d \\times r}$ and the overparameterized model $U \\in \\mathbb{R}^{d \\times k}$ with $k \\gg r$. We study the approximate local minima of the empirical mean square error, augmented with a smooth version of a group Lasso regularizer, $\\sum_{i=1}^k \\| U e_i \\|_2$ and show that pruning the low $\\ell_2$-norm columns results in a solution $U_{\\",
    "link": "http://arxiv.org/abs/2303.11453",
    "context": "Title: Greedy Pruning with Group Lasso Provably Generalizes for Matrix Sensing and Neural Networks with Quadratic Activations. (arXiv:2303.11453v1 [cs.LG])\nAbstract: Pruning schemes have been widely used in practice to reduce the complexity of trained models with a massive number of parameters. Several practical studies have shown that pruning an overparameterized model and fine-tuning generalizes well to new samples. Although the above pipeline, which we refer to as pruning + fine-tuning, has been extremely successful in lowering the complexity of trained models, there is very little known about the theory behind this success. In this paper we address this issue by investigating the pruning + fine-tuning framework on the overparameterized matrix sensing problem, with the ground truth denoted $U_\\star \\in \\mathbb{R}^{d \\times r}$ and the overparameterized model $U \\in \\mathbb{R}^{d \\times k}$ with $k \\gg r$. We study the approximate local minima of the empirical mean square error, augmented with a smooth version of a group Lasso regularizer, $\\sum_{i=1}^k \\| U e_i \\|_2$ and show that pruning the low $\\ell_2$-norm columns results in a solution $U_{\\",
    "path": "papers/23/03/2303.11453.json",
    "total_tokens": 960,
    "translated_title": "基于Group Lasso的贪婪剪枝在矩阵感知和二次激活神经网络上可证地泛化",
    "translated_abstract": "剪枝方案广泛用于降低具有大量参数的模型的复杂性。实践研究表明，修剪过度参数化模型并微调可很好地泛化到新样本上。虽然以上被称为剪枝+微调的流程在降低训练模型的复杂性方面非常成功，但其背后的理论仍然不甚了解。本文通过研究超参数化矩阵感知问题上的剪枝+微调框架来解决这个问题，其中真实结果表示为$U_\\star \\in \\mathbb{R}^{d \\times r}$，而超参数化模型表示为$U \\in \\mathbb{R}^{d \\times k}$，其中$k \\gg r$。我们研究加上Group Lasso正则化器的平滑版本$\\sum_{i=1}^k \\| U e_i \\|_2$的平均误差的近似局部极小值，证明修剪低$\\ell_2$范数列的解$U_{",
    "tldr": "本文在矩阵感知问题中研究了基于Group Lasso正则化器的贪婪剪枝方法，证明了修剪低$\\ell_2$范数列的解可以泛化到新样本上。",
    "en_tdlr": "This paper investigates the greedy pruning method based on the Group Lasso regularizer in the matrix sensing problem and proves that pruning the solution of the low $\\ell_2$-norm columns can generalize well to new samples."
}