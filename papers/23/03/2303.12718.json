{
    "title": "Strategy Synthesis in Markov Decision Processes Under Limited Sampling Access. (arXiv:2303.12718v1 [cs.LG])",
    "abstract": "A central task in control theory, artificial intelligence, and formal methods is to synthesize reward-maximizing strategies for agents that operate in partially unknown environments. In environments modeled by gray-box Markov decision processes (MDPs), the impact of the agents' actions are known in terms of successor states but not the stochastics involved. In this paper, we devise a strategy synthesis algorithm for gray-box MDPs via reinforcement learning that utilizes interval MDPs as internal model. To compete with limited sampling access in reinforcement learning, we incorporate two novel concepts into our algorithm, focusing on rapid and successful learning rather than on stochastic guarantees and optimality: lower confidence bound exploration reinforces variants of already learned practical strategies and action scoping reduces the learning action space to promising actions. We illustrate benefits of our algorithms by means of a prototypical implementation applied on examples fro",
    "link": "http://arxiv.org/abs/2303.12718",
    "context": "Title: Strategy Synthesis in Markov Decision Processes Under Limited Sampling Access. (arXiv:2303.12718v1 [cs.LG])\nAbstract: A central task in control theory, artificial intelligence, and formal methods is to synthesize reward-maximizing strategies for agents that operate in partially unknown environments. In environments modeled by gray-box Markov decision processes (MDPs), the impact of the agents' actions are known in terms of successor states but not the stochastics involved. In this paper, we devise a strategy synthesis algorithm for gray-box MDPs via reinforcement learning that utilizes interval MDPs as internal model. To compete with limited sampling access in reinforcement learning, we incorporate two novel concepts into our algorithm, focusing on rapid and successful learning rather than on stochastic guarantees and optimality: lower confidence bound exploration reinforces variants of already learned practical strategies and action scoping reduces the learning action space to promising actions. We illustrate benefits of our algorithms by means of a prototypical implementation applied on examples fro",
    "path": "papers/23/03/2303.12718.json",
    "total_tokens": 931,
    "translated_title": "有限采样下的马尔可夫决策过程策略综合",
    "translated_abstract": "在控制理论、人工智能和形式方法中，一个核心的任务是为在部分未知环境下操作的代理合成最大化回报的策略。在灰箱马尔可夫决策过程 (MDPs) 模型中，代理的行为影响以后的状态而不是涉及到的概率。在本文中，我们通过强化学习为灰箱 MDP 合成策略设计了一个内部模型为区间 MDP 的策略综合算法。为了应对强化学习中的有限采样问题，我们将两个新的概念引入到算法中，专注于快速成功的学习而不是随机保证和最优性：下置信区间探索加强已经学习的可行策略的变体，行动划分将学习行动空间缩小到有前途的行动。我们通过具有代表性的实例说明了算法的优点。",
    "tldr": "本文提出了一种针对灰箱 MDP 的策略综合算法，使用区间 MDP 作为内部模型，并通过强化学习，结合下置信区间探索和行动划分的方法，解决有限采样下的问题，用于合成最大化回报的实用策略。",
    "en_tdlr": "This paper proposes a strategy synthesis algorithm for gray-box Markov decision processes (MDPs) via reinforcement learning that utilizes interval MDPs as internal model. The algorithm addresses limited sampling access in reinforcement learning with lower confidence bound exploration and action scoping, and focuses on rapid and successful learning rather than stochastic guarantees and optimality. The benefits of the algorithm are illustrated through a prototypical implementation applied on examples. The algorithm aims to synthesize practical strategies that maximize the return."
}