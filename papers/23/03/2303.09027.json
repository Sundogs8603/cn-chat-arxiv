{
    "title": "Learning Rewards to Optimize Global Performance Metrics in Deep Reinforcement Learning. (arXiv:2303.09027v1 [cs.LG])",
    "abstract": "When applying reinforcement learning (RL) to a new problem, reward engineering is a necessary, but often difficult and error-prone task a system designer has to face. To avoid this step, we propose LR4GPM, a novel (deep) RL method that can optimize a global performance metric, which is supposed to be available as part of the problem description. LR4GPM alternates between two phases: (1) learning a (possibly vector) reward function used to fit the performance metric, and (2) training a policy to optimize an approximation of this performance metric based on the learned rewards. Such RL training is not straightforward since both the reward function and the policy are trained using non-stationary data. To overcome this issue, we propose several training tricks. We demonstrate the efficiency of LR4GPM on several domains. Notably, LR4GPM outperforms the winner of a recent autonomous driving competition organized at DAI'2020.",
    "link": "http://arxiv.org/abs/2303.09027",
    "context": "Title: Learning Rewards to Optimize Global Performance Metrics in Deep Reinforcement Learning. (arXiv:2303.09027v1 [cs.LG])\nAbstract: When applying reinforcement learning (RL) to a new problem, reward engineering is a necessary, but often difficult and error-prone task a system designer has to face. To avoid this step, we propose LR4GPM, a novel (deep) RL method that can optimize a global performance metric, which is supposed to be available as part of the problem description. LR4GPM alternates between two phases: (1) learning a (possibly vector) reward function used to fit the performance metric, and (2) training a policy to optimize an approximation of this performance metric based on the learned rewards. Such RL training is not straightforward since both the reward function and the policy are trained using non-stationary data. To overcome this issue, we propose several training tricks. We demonstrate the efficiency of LR4GPM on several domains. Notably, LR4GPM outperforms the winner of a recent autonomous driving competition organized at DAI'2020.",
    "path": "papers/23/03/2303.09027.json",
    "total_tokens": 1032,
    "translated_title": "在深度强化学习中学习奖励以优化全局性能指标",
    "translated_abstract": "当应用强化学习（RL）到一个新问题上时，奖励工程是一个必要但常常十分困难且容易出错的任务，系统设计师需要面对它。为了避免这一步，我们提出了LR4GPM，一种能够优化全局性能度量的新型（深度）RL方法，这个全局性能度量应该作为问题描述的一部分。LR4GPM交替执行两个阶段：（1）学习（可能是向量）的奖励函数，用于拟合性能度量，（2）训练策略以优化基于学习奖励的这个性能度量的近似值。这样的RL训练并不简单，因为奖励函数和策略都是使用非稳态数据进行训练的。为了克服这个问题，我们提出了几种训练技巧。我们在几个领域展示了LR4GPM的效率。值得注意的是，LR4GPM胜过了DAI'2020组织的最近一次自动驾驶竞赛的获胜者。",
    "tldr": "该论文提出了一种名为LR4GPM的（深度）强化学习方法，它可以在问题描述中提供的全局性能度量的基础上优化。LR4GPM在学习奖励函数和训练策略时使用了几种训练技巧，能够避免进行奖励工程，其效率高于DAI'2020组织最近举办的一次自动驾驶竞赛的获胜者。"
}