{
    "title": "Explaining Model Confidence Using Counterfactuals. (arXiv:2303.05729v1 [cs.AI])",
    "abstract": "Displaying confidence scores in human-AI interaction has been shown to help build trust between humans and AI systems. However, most existing research uses only the confidence score as a form of communication. As confidence scores are just another model output, users may want to understand why the algorithm is confident to determine whether to accept the confidence score. In this paper, we show that counterfactual explanations of confidence scores help study participants to better understand and better trust a machine learning model's prediction. We present two methods for understanding model confidence using counterfactual explanation: (1) based on counterfactual examples; and (2) based on visualisation of the counterfactual space. Both increase understanding and trust for study participants over a baseline of no explanation, but qualitative results show that they are used quite differently, leading to recommendations of when to use each one and directions of designing better explanat",
    "link": "http://arxiv.org/abs/2303.05729",
    "raw_ret": "import requests\nimport json\n\nurl='http://api.fanyi.baidu.com/api/trans/vip/translate'\ndata={\n    'from':'en',\n    'to':'zh',\n    'appid':'20210130000692483',\n    'salt':'1435660288',\n    'sign':'6de215ad6ed337534a4f72abeb25c86f',\n    'q':'Explaining Model Confidence Using Counterfactuals. (arXiv:2303.05729v1 [cs.AI]) Displaying confidence scores in human-AI interaction has been shown to help build trust between humans and AI systems. However, most existing research uses only the confidence score as a form of communication. As confidence scores are just another model output, users may want to understand why the algorithm is confident to determine whether to accept the confidence score. In this paper, we show that counterfactual explanations of confidence scores help study participants to better understand and better trust a machine learning model\\'s prediction. We present two methods for understanding model confidence using counterfactual explanation: (1) based on counterfactual examples; and (2) based on visualisation of the counterfactual space. Both increase understanding and trust for study participants over a baseline of no explanation, but qualitative results show that they are used quite differently, leading to recommendations of when to use each one and directions of designing better explanat',\n}\nheaders={\n    'Content-Type':'application/x-www-form-urlencoded'\n}\nresponse = requests.post(url=url,data=data,headers=headers)\nstrs=json.loads(response.content)\ntranslated_title=strs['trans_result'][0]['dst']\nresponse.close()\n\ndata={\n    'from':'en',\n    'to':'zh',\n    'appid':'20210130000692483',\n    'salt':'1435660288',\n    'sign':'6de215ad6ed337534a4f72abeb25c86f',\n    'q':'Displaying confidence scores in human-AI interaction has been shown to help build trust between humans and AI systems. However, most existing research uses only the confidence score as a form of communication. As confidence scores are just another model output, users may want to understand why the algorithm is confident to determine whether to accept the confidence score. In this paper, we show that counterfactual explanations of confidence scores help study participants to better understand and better trust a machine learning model\\'s prediction. We present two methods for understanding model confidence using counterfactual explanation: (1) based on counterfactual examples; and (2) based on visualisation of the counterfactual space. Both increase understanding and trust for study participants over a baseline of no explanation, but qualitative results show that they are used quite differently, leading to recommendations of when to use each one and directions of designing better explanat',\n}\nresponse = requests.post(url=url,data=data,headers=headers)\nstrs=json.loads(response.content)\ntranslated_abstract=strs['trans_result'][0]['dst']\nresponse.close()\n\ntldr='在人工智能与人类交互中，置信度分值的展示有助于建立人与人工智能系统间的信任。本文通过反事实解释模型的置信度，提高了研究对象对机器学习模型的预测的理解和信任。我们提出了两种解释模型置信度的方法：（1）基于反事实示例；（2）基于反事实空间的可视化。这两种方法相对于没有解释的情况下都可以提高研究对象对模型的理解和信任，但定性结果显示，它们使用上有着不同，我们推荐合理使用这两种方法，并提出了更好的解释设计方向。'\n\nresult={\n    \"translated_title\": translated_title,\n    \"translated_abstract\": translated_abstract,\n    \"tldr\": tldr\n}\nprint(json.dumps(result))<|im_sep|>",
    "total_tokens": 1288
}