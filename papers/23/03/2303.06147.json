{
    "title": "Exphormer: Sparse Transformers for Graphs. (arXiv:2303.06147v2 [cs.LG] UPDATED)",
    "abstract": "Graph transformers have emerged as a promising architecture for a variety of graph learning and representation tasks. Despite their successes, though, it remains challenging to scale graph transformers to large graphs while maintaining accuracy competitive with message-passing networks. In this paper, we introduce Exphormer, a framework for building powerful and scalable graph transformers. Exphormer consists of a sparse attention mechanism based on two mechanisms: virtual global nodes and expander graphs, whose mathematical characteristics, such as spectral expansion, pseduorandomness, and sparsity, yield graph transformers with complexity only linear in the size of the graph, while allowing us to prove desirable theoretical properties of the resulting transformer models. We show that incorporating Exphormer into the recently-proposed GraphGPS framework produces models with competitive empirical results on a wide variety of graph datasets, including state-of-the-art results on three d",
    "link": "http://arxiv.org/abs/2303.06147",
    "context": "Title: Exphormer: Sparse Transformers for Graphs. (arXiv:2303.06147v2 [cs.LG] UPDATED)\nAbstract: Graph transformers have emerged as a promising architecture for a variety of graph learning and representation tasks. Despite their successes, though, it remains challenging to scale graph transformers to large graphs while maintaining accuracy competitive with message-passing networks. In this paper, we introduce Exphormer, a framework for building powerful and scalable graph transformers. Exphormer consists of a sparse attention mechanism based on two mechanisms: virtual global nodes and expander graphs, whose mathematical characteristics, such as spectral expansion, pseduorandomness, and sparsity, yield graph transformers with complexity only linear in the size of the graph, while allowing us to prove desirable theoretical properties of the resulting transformer models. We show that incorporating Exphormer into the recently-proposed GraphGPS framework produces models with competitive empirical results on a wide variety of graph datasets, including state-of-the-art results on three d",
    "path": "papers/23/03/2303.06147.json",
    "total_tokens": 864,
    "translated_title": "Exphormer: 稀疏Transformer用于图形",
    "translated_abstract": "图形转换器已经成为各种图学习和表示任务的一种有前途的架构。尽管取得了成功，但是将图形转换器扩展到大型图形并同时保持与消息传递网络相媲美的准确性仍然具有挑战性。在本文中，我们介绍了Exphormer，一个构建强大且可扩展的图形转换器的框架。Exphormer包括基于两个机制的稀疏注意机制：虚拟全局节点和扩张图，其数学特性（如谱扩展、伪随机性和稀疏性）使得图形转换器的复杂度仅与图形大小线性相关，并且能够证明生成的转换器模型具有理想的理论特性。我们展示了将Exphormer纳入最近提出的GraphGPS框架中，可以在各种图数据集上获得具有竞争力的实证结果，包括在三个数据集上的最新结果。",
    "tldr": "Exphormer是一个稀疏Transformer框架，通过虚拟全局节点和扩张图的稀疏注意机制，在保持准确性的同时能够扩展到大型图形，并证明其拥有理想的理论特性。",
    "en_tdlr": "Exphormer is a sparse Transformer framework that uses a sparse attention mechanism based on virtual global nodes and expander graphs to scale to large graphs while maintaining accuracy, and has desirable theoretical properties."
}