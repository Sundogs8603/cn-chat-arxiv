{
    "title": "TextMI: Textualize Multimodal Information for Integrating Non-verbal Cues in Pre-trained Language Models. (arXiv:2303.15430v2 [cs.CL] UPDATED)",
    "abstract": "Pre-trained large language models have recently achieved ground-breaking performance in a wide variety of language understanding tasks. However, the same model can not be applied to multimodal behavior understanding tasks (e.g., video sentiment/humor detection) unless non-verbal features (e.g., acoustic and visual) can be integrated with language. Jointly modeling multiple modalities significantly increases the model complexity, and makes the training process data-hungry. While an enormous amount of text data is available via the web, collecting large-scale multimodal behavioral video datasets is extremely expensive, both in terms of time and money. In this paper, we investigate whether large language models alone can successfully incorporate non-verbal information when they are presented in textual form. We present a way to convert the acoustic and visual information into corresponding textual descriptions and concatenate them with the spoken text. We feed this augmented input to a pr",
    "link": "http://arxiv.org/abs/2303.15430",
    "context": "Title: TextMI: Textualize Multimodal Information for Integrating Non-verbal Cues in Pre-trained Language Models. (arXiv:2303.15430v2 [cs.CL] UPDATED)\nAbstract: Pre-trained large language models have recently achieved ground-breaking performance in a wide variety of language understanding tasks. However, the same model can not be applied to multimodal behavior understanding tasks (e.g., video sentiment/humor detection) unless non-verbal features (e.g., acoustic and visual) can be integrated with language. Jointly modeling multiple modalities significantly increases the model complexity, and makes the training process data-hungry. While an enormous amount of text data is available via the web, collecting large-scale multimodal behavioral video datasets is extremely expensive, both in terms of time and money. In this paper, we investigate whether large language models alone can successfully incorporate non-verbal information when they are presented in textual form. We present a way to convert the acoustic and visual information into corresponding textual descriptions and concatenate them with the spoken text. We feed this augmented input to a pr",
    "path": "papers/23/03/2303.15430.json",
    "total_tokens": 1118,
    "translated_title": "TextMI:将多模态信息转化为文本形式，使预训练语言模型集成非语言线索",
    "translated_abstract": "预训练大型语言模型最近在各种语言理解任务中取得了突破性的性能。然而，同一模型无法应用于多模式行为理解任务（例如，视频情感/幽默检测），除非可以将非语言特征（例如，声学和视觉）与语言集成。联合建模多个模态显着增加了模型复杂性，并使训练过程变得需数据量大。虽然通过网络可以获得大量的文本数据，但收集大规模的多模态行为视频数据集极其昂贵，无论是时间上还是金钱上。在本文中，我们研究了当以文本形式呈现的情况下，大型语言模型是否可以成功地将非语言信息纳入其中。我们提出了一种将声音和视觉信息转化为对应的文本描述，并将其与口语文本串联的方法。我们将这种增强的输入馈送给预先训练的语言模型，并显示其性能与直接使用非语言特征的相同模型相当。我们的方法TextMI不仅避免了昂贵的多模态数据收集和建模过程，而且实现了非语言线索与语言的无缝集成，这对于实际应用至关重要。",
    "tldr": "本文提出了一种将声学和视觉信息转化为文本描述并与口语文本串联，从而将非语言信息纳入预训练语言模型中的方法。该方法可以不需要昂贵的多模态数据收集和建模，并且可以使非语言线索与语言无缝集成，对于多模态行为理解任务具有实际应用意义。",
    "en_tdlr": "The paper proposes a method, TextMI, that converts acoustic and visual information into corresponding textual descriptions and concatenates them with spoken text to integrate non-verbal information into pre-trained language models. The approach avoids the need for expensive multimodal data collection and modeling, and enables seamless integration of non-verbal cues with language, which is essential for practical applications in multimodal behavior understanding tasks."
}