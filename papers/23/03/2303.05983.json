{
    "title": "Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation. (arXiv:2303.05983v2 [cs.CV] UPDATED)",
    "abstract": "The recent success of ChatGPT and GPT-4 has drawn widespread attention to multimodal dialogue systems. However, the academia community lacks a dataset that can validate the multimodal generation capabilities of Visual Language Models (VLMs) in textual-visual chat tasks. In this paper, we construct two new multimodal datasets: the synthetic CLEVR-ATVC dataset (620K) and the manually pictured Fruit-ATVC dataset (50K), both featuring visual and text-based inputs and outputs. Additionally, to enable the multimodal system to reject human requests (i.e., demonstrate accountability), as in language-based ChatGPT conversations, we develop and incorporate specific rules into the datasets as supervisory signals. This allows the trained VLM to provide a yes or no answer after visual and textual reasoning, accompanied by a language explanation as to why the human instruction cannot be excuted. In our method, we propose a two-state training procedure to train the image auto-encoder and auto-regress",
    "link": "http://arxiv.org/abs/2303.05983",
    "context": "Title: Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation. (arXiv:2303.05983v2 [cs.CV] UPDATED)\nAbstract: The recent success of ChatGPT and GPT-4 has drawn widespread attention to multimodal dialogue systems. However, the academia community lacks a dataset that can validate the multimodal generation capabilities of Visual Language Models (VLMs) in textual-visual chat tasks. In this paper, we construct two new multimodal datasets: the synthetic CLEVR-ATVC dataset (620K) and the manually pictured Fruit-ATVC dataset (50K), both featuring visual and text-based inputs and outputs. Additionally, to enable the multimodal system to reject human requests (i.e., demonstrate accountability), as in language-based ChatGPT conversations, we develop and incorporate specific rules into the datasets as supervisory signals. This allows the trained VLM to provide a yes or no answer after visual and textual reasoning, accompanied by a language explanation as to why the human instruction cannot be excuted. In our method, we propose a two-state training procedure to train the image auto-encoder and auto-regress",
    "path": "papers/23/03/2303.05983.json",
    "total_tokens": 879,
    "translated_title": "基于人类指令拒绝图像再创作的文本-视觉对话可追溯技术",
    "translated_abstract": "ChatGPT和GPT-4的成功引起了对多模态对话系统的广泛关注，但缺乏可以验证视觉语言模型在文本-视觉对话任务中多模态生成能力的数据集。为此，本文构建了两个新的多模态数据集：合成CLEVR-ATVC数据集（620K）和手动绘制的Fruit-ATVC数据集（50K），均具有基于视觉和文本的输入和输出。为了让多模态系统能够拒绝人类请求（即展示可追溯性），我们在数据集中开发和并入了特定规则作为监督信号。这允许训练后的VLM在视觉和文本推理后提供yes或no的答案，并附带说明语言为什么无法执行人类指令。我们提出了一个两阶段训练程序来训练图像自编码器和自回归神经网络。",
    "tldr": "本论文构建了两个多模态数据集来验证视觉语言模型在文本-视觉对话任务中的能力，并开发特定规则的监督信号来让系统展示可追溯性。",
    "en_tdlr": "This paper creates two multimodal datasets to validate visual language models' capabilities in textual-visual chat tasks and incorporates specific rules as supervisory signals to enable the system to reject human requests and exhibit accountability."
}