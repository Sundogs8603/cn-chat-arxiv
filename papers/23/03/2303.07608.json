{
    "title": "On the Implicit Geometry of Cross-Entropy Parameterizations for Label-Imbalanced Data. (arXiv:2303.07608v1 [cs.LG])",
    "abstract": "Various logit-adjusted parameterizations of the cross-entropy (CE) loss have been proposed as alternatives to weighted CE for training large models on label-imbalanced data far beyond the zero train error regime. The driving force behind those designs has been the theory of implicit bias, which for linear(ized) models, explains why they successfully induce bias on the optimization path towards solutions that favor minorities. Aiming to extend this theory to non-linear models, we investigate the implicit geometry of classifiers and embeddings that are learned by different CE parameterizations. Our main result characterizes the global minimizers of a non-convex cost-sensitive SVM classifier for the unconstrained features model, which serves as an abstraction of deep nets. We derive closed-form formulas for the angles and norms of classifiers and embeddings as a function of the number of classes, the imbalance and the minority ratios, and the loss hyperparameters. Using these, we show tha",
    "link": "http://arxiv.org/abs/2303.07608",
    "context": "Title: On the Implicit Geometry of Cross-Entropy Parameterizations for Label-Imbalanced Data. (arXiv:2303.07608v1 [cs.LG])\nAbstract: Various logit-adjusted parameterizations of the cross-entropy (CE) loss have been proposed as alternatives to weighted CE for training large models on label-imbalanced data far beyond the zero train error regime. The driving force behind those designs has been the theory of implicit bias, which for linear(ized) models, explains why they successfully induce bias on the optimization path towards solutions that favor minorities. Aiming to extend this theory to non-linear models, we investigate the implicit geometry of classifiers and embeddings that are learned by different CE parameterizations. Our main result characterizes the global minimizers of a non-convex cost-sensitive SVM classifier for the unconstrained features model, which serves as an abstraction of deep nets. We derive closed-form formulas for the angles and norms of classifiers and embeddings as a function of the number of classes, the imbalance and the minority ratios, and the loss hyperparameters. Using these, we show tha",
    "path": "papers/23/03/2303.07608.json",
    "total_tokens": 923,
    "translated_title": "关于基于交叉熵参数化的类别不平衡数据中的隐式几何",
    "translated_abstract": "针对在零训练误差区域以外的标签不平衡数据上训练大模型的方法，目前已经提出了多种经过逻辑调整的交叉熵（CE）损失参数化作为加权CE的替代方法。这些设计背后的主要驱动因素是隐式偏差理论，对于线性和线性化模型，解释了它们为什么能够成功地在优化路径上诱导出偏差，以使解决方案有利于少数人。为了将这个理论扩展到非线性模型，我们研究了不同CE参数化学习的分类器和嵌入的隐式几何。我们的主要成果是表征非凸敏感成本SVM分类器的全局最优解，该分类器是无约束特征模型的抽象。我们导出了分类器和嵌入的角度和范数的闭式公式，这些公式是类别数、失衡和少数人比例以及损失超参数的函数。借助这些公式，我们展示了。",
    "tldr": "本文探究了在标签不平衡数据上训练深度神经网络的CE损失函数参数化方法，结果得出了可以使模型偏向于少数类的隐式偏差理论并推导出了分类器和嵌入的闭式公式。",
    "en_tdlr": "This paper investigates the implicit bias theory for training deep neural networks on label-imbalanced data using cross-entropy loss function parameterization, and derives closed-form formulas for the classifiers and embeddings that can induce bias towards minority classes."
}