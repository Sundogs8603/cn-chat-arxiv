{
    "title": "ASIC: Aligning Sparse in-the-wild Image Collections. (arXiv:2303.16201v1 [cs.CV])",
    "abstract": "We present a method for joint alignment of sparse in-the-wild image collections of an object category. Most prior works assume either ground-truth keypoint annotations or a large dataset of images of a single object category. However, neither of the above assumptions hold true for the long-tail of the objects present in the world. We present a self-supervised technique that directly optimizes on a sparse collection of images of a particular object/object category to obtain consistent dense correspondences across the collection. We use pairwise nearest neighbors obtained from deep features of a pre-trained vision transformer (ViT) model as noisy and sparse keypoint matches and make them dense and accurate matches by optimizing a neural network that jointly maps the image collection into a learned canonical grid. Experiments on CUB and SPair-71k benchmarks demonstrate that our method can produce globally consistent and higher quality correspondences across the image collection when compa",
    "link": "http://arxiv.org/abs/2303.16201",
    "context": "Title: ASIC: Aligning Sparse in-the-wild Image Collections. (arXiv:2303.16201v1 [cs.CV])\nAbstract: We present a method for joint alignment of sparse in-the-wild image collections of an object category. Most prior works assume either ground-truth keypoint annotations or a large dataset of images of a single object category. However, neither of the above assumptions hold true for the long-tail of the objects present in the world. We present a self-supervised technique that directly optimizes on a sparse collection of images of a particular object/object category to obtain consistent dense correspondences across the collection. We use pairwise nearest neighbors obtained from deep features of a pre-trained vision transformer (ViT) model as noisy and sparse keypoint matches and make them dense and accurate matches by optimizing a neural network that jointly maps the image collection into a learned canonical grid. Experiments on CUB and SPair-71k benchmarks demonstrate that our method can produce globally consistent and higher quality correspondences across the image collection when compa",
    "path": "papers/23/03/2303.16201.json",
    "total_tokens": 901,
    "translated_title": "ASIC: 对野外稀疏图像集的对齐",
    "translated_abstract": "我们提出了一种针对物体类别的稀疏野外图像集进行联合对齐的方法。大多数先前的作品要么假定有ground-truth的关键点注释，要么假定有一个物体类别的大型图像数据集。然而，以上两个假设都不适用于存在于世界上的物体的尾部。我们提出了一种自我监督的技术，直接在特定物体/物体类别的稀疏图像集中进行优化，以获得整个集合的一致且稠密的对应关系。我们使用预训练的视觉变压器（ViT）模型的深度特征中获得的成对最近邻作为噪声和稀疏关键点匹配，并通过优化神经网络，将它们密集和精确匹配，同时将图像集合映射到学习到的规范网格中。在CUB和SPair-71k基准测试中进行实验，我们的方法可以产生全局一致性和更高质量的图像集对应关系。",
    "tldr": "该文提出了一种针对物体类别的稀疏野外图像集进行联合对齐的方法，可用于一致性和高质量的图像集对应关系。",
    "en_tdlr": "The paper presents a self-supervised technique to jointly align sparse in-the-wild image collections of an object category, using noisy and sparse keypoint matches obtained from deep features of a pre-trained vision transformer and optimizing a neural network to achieve globally consistent and higher quality correspondences across the image collection."
}