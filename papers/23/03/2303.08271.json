{
    "title": "Act-Then-Measure: Reinforcement Learning for Partially Observable Environments with Active Measuring. (arXiv:2303.08271v1 [cs.AI])",
    "abstract": "We study Markov decision processes (MDPs), where agents have direct control over when and how they gather information, as formalized by action-contingent noiselessly observable MDPs (ACNO-MPDs). In these models, actions consist of two components: a control action that affects the environment, and a measurement action that affects what the agent can observe. To solve ACNO-MDPs, we introduce the act-then-measure (ATM) heuristic, which assumes that we can ignore future state uncertainty when choosing control actions. We show how following this heuristic may lead to shorter policy computation times and prove a bound on the performance loss incurred by the heuristic. To decide whether or not to take a measurement action, we introduce the concept of measuring value. We develop a reinforcement learning algorithm based on the ATM heuristic, using a Dyna-Q variant adapted for partially observable domains, and showcase its superior performance compared to prior methods on a number of partially-o",
    "link": "http://arxiv.org/abs/2303.08271",
    "context": "Title: Act-Then-Measure: Reinforcement Learning for Partially Observable Environments with Active Measuring. (arXiv:2303.08271v1 [cs.AI])\nAbstract: We study Markov decision processes (MDPs), where agents have direct control over when and how they gather information, as formalized by action-contingent noiselessly observable MDPs (ACNO-MPDs). In these models, actions consist of two components: a control action that affects the environment, and a measurement action that affects what the agent can observe. To solve ACNO-MDPs, we introduce the act-then-measure (ATM) heuristic, which assumes that we can ignore future state uncertainty when choosing control actions. We show how following this heuristic may lead to shorter policy computation times and prove a bound on the performance loss incurred by the heuristic. To decide whether or not to take a measurement action, we introduce the concept of measuring value. We develop a reinforcement learning algorithm based on the ATM heuristic, using a Dyna-Q variant adapted for partially observable domains, and showcase its superior performance compared to prior methods on a number of partially-o",
    "path": "papers/23/03/2303.08271.json",
    "total_tokens": 1038,
    "translated_title": "Act-Then-Measure: 带主动测量的部分可观察环境中的强化学习",
    "translated_abstract": "本研究研究了马尔可夫决策过程 (MDPs)，其中代理有直接控制何时以及如何收集信息的能力，如 action-contingent noiselessly observable MDPs (ACNO-MPDs) 所形式化。在这些模型中，动作由两个组成部分组成：影响环境的控制动作和影响代理可以观察到什么的测量动作。为了解决 ACNO-MDPs，我们引入了行动后测量 (ATM) 策略，它假设在选择控制动作时可以忽略未来状态的不确定性。我们展示了遵循此策略可能导致较短的策略计算时间，并证明了该启发式方法引起的性能丧失的界限。为了确定是否采取测量行动，我们引入了测量价值的概念。我们基于 ATM 启发式方法开发了一个强化学习算法，使用针对部分可观察域的 Dyna-Q 变体，并展示了它在多个部分可观察环境上优于先前的方法的卓越性能。",
    "tldr": "本论文研究了对于代理有直接控制何时以及如何收集信息的能力的马尔可夫决策过程。我们引入了行动后测量 (ATM) 策略，并开发了一个基于 ATM 启发式方法的强化学习算法，展示了其在多个部分可观察环境上优于先前的方法的卓越性能。",
    "en_tdlr": "This paper studies Markov decision processes (MDPs) in which agents have direct control over when and how they gather information through action-contingent noiselessly observable MDPs (ACNO-MPDs). The paper introduces the act-then-measure (ATM) heuristic and develops a reinforcement learning algorithm based on it, showcasing superior performance compared to prior methods in multiple partially observable environments."
}