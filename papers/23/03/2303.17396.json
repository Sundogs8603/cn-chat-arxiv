{
    "title": "Finetuning from Offline Reinforcement Learning: Challenges, Trade-offs and Practical Solutions. (arXiv:2303.17396v1 [cs.LG])",
    "abstract": "Offline reinforcement learning (RL) allows for the training of competent agents from offline datasets without any interaction with the environment. Online finetuning of such offline models can further improve performance. But how should we ideally finetune agents obtained from offline RL training? While offline RL algorithms can in principle be used for finetuning, in practice, their online performance improves slowly. In contrast, we show that it is possible to use standard online off-policy algorithms for faster improvement. However, we find this approach may suffer from policy collapse, where the policy undergoes severe performance deterioration during initial online learning. We investigate the issue of policy collapse and how it relates to data diversity, algorithm choices and online replay distribution. Based on these insights, we propose a conservative policy optimization procedure that can achieve stable and sample-efficient online learning from offline pretraining.",
    "link": "http://arxiv.org/abs/2303.17396",
    "context": "Title: Finetuning from Offline Reinforcement Learning: Challenges, Trade-offs and Practical Solutions. (arXiv:2303.17396v1 [cs.LG])\nAbstract: Offline reinforcement learning (RL) allows for the training of competent agents from offline datasets without any interaction with the environment. Online finetuning of such offline models can further improve performance. But how should we ideally finetune agents obtained from offline RL training? While offline RL algorithms can in principle be used for finetuning, in practice, their online performance improves slowly. In contrast, we show that it is possible to use standard online off-policy algorithms for faster improvement. However, we find this approach may suffer from policy collapse, where the policy undergoes severe performance deterioration during initial online learning. We investigate the issue of policy collapse and how it relates to data diversity, algorithm choices and online replay distribution. Based on these insights, we propose a conservative policy optimization procedure that can achieve stable and sample-efficient online learning from offline pretraining.",
    "path": "papers/23/03/2303.17396.json",
    "total_tokens": 910,
    "translated_title": "离线强化学习的微调：挑战、平衡和实际解决方案",
    "translated_abstract": "离线强化学习（RL）允许在没有与环境交互的情况下通过离线数据集训练有能力的智能体。这些离线模型的在线微调可以进一步提高性能。但是，如何理想地微调从离线RL训练中获得的代理？虽然离线RL算法原则上可以用于微调，但在实践中，它们的在线性能提高缓慢。相反，我们发现可以使用标准的在线离策略算法进行更快的改进。然而，我们发现这种方法可能会遭受策略崩溃，即在初始在线学习过程中策略会严重退化。我们研究了策略崩溃的问题以及它与数据多样性、算法选择和在线回放分布之间的关系。基于这些洞见，我们提出了一个保守的策略优化过程，可以从离线预训练中实现稳定且样本有效的在线学习。",
    "tldr": "本文讨论了离线RL代理的在线微调问题。通过研究数据多样性、算法选择和在线回放分布，我们提出了一个保守的策略优化过程，可以从离线预训练中实现稳定且样本有效的在线学习。",
    "en_tdlr": "This paper discusses the problem of online finetuning for offline RL agents. By investigating data diversity, algorithm choices, and online replay distribution, we propose a conservative policy optimization procedure that can achieve stable and sample-efficient online learning from offline pretraining."
}