{
    "title": "Helpful, Misleading or Confusing: How Humans Perceive Fundamental Building Blocks of Artificial Intelligence Explanations. (arXiv:2303.00934v2 [cs.HC] UPDATED)",
    "abstract": "Explainable artificial intelligence techniques are developed at breakneck speed, but suitable evaluation approaches lag behind. With explainers becoming increasingly complex and a lack of consensus on how to assess their utility, it is challenging to judge the benefit and effectiveness of different explanations. To address this gap, we take a step back from sophisticated predictive algorithms and instead look into explainability of simple decision-making models. In this setting, we aim to assess how people perceive comprehensibility of their different representations such as mathematical formulation, graphical representation and textual summarisation (of varying complexity and scope). This allows us to capture how diverse stakeholders -engineers, researchers, consumers, regulators and the like -- judge intelligibility of fundamental concepts that more elaborate artificial intelligence explanations are built from. This position paper charts our approach to establishing appropriate eva",
    "link": "http://arxiv.org/abs/2303.00934",
    "context": "Title: Helpful, Misleading or Confusing: How Humans Perceive Fundamental Building Blocks of Artificial Intelligence Explanations. (arXiv:2303.00934v2 [cs.HC] UPDATED)\nAbstract: Explainable artificial intelligence techniques are developed at breakneck speed, but suitable evaluation approaches lag behind. With explainers becoming increasingly complex and a lack of consensus on how to assess their utility, it is challenging to judge the benefit and effectiveness of different explanations. To address this gap, we take a step back from sophisticated predictive algorithms and instead look into explainability of simple decision-making models. In this setting, we aim to assess how people perceive comprehensibility of their different representations such as mathematical formulation, graphical representation and textual summarisation (of varying complexity and scope). This allows us to capture how diverse stakeholders -engineers, researchers, consumers, regulators and the like -- judge intelligibility of fundamental concepts that more elaborate artificial intelligence explanations are built from. This position paper charts our approach to establishing appropriate eva",
    "path": "papers/23/03/2303.00934.json",
    "total_tokens": 893,
    "translated_title": "有帮助、引导还是让人困惑: 人们如何看待人工智能解释的基本构建块。",
    "translated_abstract": "可解释的人工智能技术正在以惊人的速度发展，但适当的评估方法落后。随着解释器变得越来越复杂，以及缺乏关于如何评估它们效用的共识，评判不同解释的好处和有效性变得具有挑战性。为了填补这一空白，我们退一步研究简单决策模型的可解释性，而不是复杂的预测算法。在这种环境下，我们旨在评估人们如何理解它们的不同表示形式，例如数学公式、图形表示和文本摘要（不同复杂度和范围）。这使我们能够捕捉到各种利益相关者（工程师、研究者、消费者、监管机构等）如何评判更复杂的人工智能解释所构建的基本概念的可理解性。本文介绍我们从以人为中心的角度建立适当的可解释性评估方法的方法。",
    "tldr": "该论文旨在以人为中心的角度评估人工智能中的解释可理解性，以及评估可解释性的适当方法落后于技术的发展速度。",
    "en_tdlr": "This paper aims to evaluate the explainability of artificial intelligence from a human-centered perspective and highlights the lagging evaluation methodologies in comparison to the rapid development of the technology."
}