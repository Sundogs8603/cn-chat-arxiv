{
    "title": "Optimal approximation of $C^k$-functions using shallow complex-valued neural networks. (arXiv:2303.16813v1 [math.FA])",
    "abstract": "We prove a quantitative result for the approximation of functions of regularity $C^k$ (in the sense of real variables) defined on the complex cube $\\Omega_n := [-1,1]^n +i[-1,1]^n\\subseteq \\mathbb{C}^n$ using shallow complex-valued neural networks. Precisely, we consider neural networks with a single hidden layer and $m$ neurons, i.e., networks of the form $z \\mapsto \\sum_{j=1}^m \\sigma_j \\cdot \\phi\\big(\\rho_j^T z + b_j\\big)$ and show that one can approximate every function in $C^k \\left( \\Omega_n; \\mathbb{C}\\right)$ using a function of that form with error of the order $m^{-k/(2n)}$ as $m \\to \\infty$, provided that the activation function $\\phi: \\mathbb{C} \\to \\mathbb{C}$ is smooth but not polyharmonic on some non-empty open set. Furthermore, we show that the selection of the weights $\\sigma_j, b_j \\in \\mathbb{C}$ and $\\rho_j \\in \\mathbb{C}^n$ is continuous with respect to $f$ and prove that the derived rate of approximation is optimal under this continuity assumption. We also discuss",
    "link": "http://arxiv.org/abs/2303.16813",
    "context": "Title: Optimal approximation of $C^k$-functions using shallow complex-valued neural networks. (arXiv:2303.16813v1 [math.FA])\nAbstract: We prove a quantitative result for the approximation of functions of regularity $C^k$ (in the sense of real variables) defined on the complex cube $\\Omega_n := [-1,1]^n +i[-1,1]^n\\subseteq \\mathbb{C}^n$ using shallow complex-valued neural networks. Precisely, we consider neural networks with a single hidden layer and $m$ neurons, i.e., networks of the form $z \\mapsto \\sum_{j=1}^m \\sigma_j \\cdot \\phi\\big(\\rho_j^T z + b_j\\big)$ and show that one can approximate every function in $C^k \\left( \\Omega_n; \\mathbb{C}\\right)$ using a function of that form with error of the order $m^{-k/(2n)}$ as $m \\to \\infty$, provided that the activation function $\\phi: \\mathbb{C} \\to \\mathbb{C}$ is smooth but not polyharmonic on some non-empty open set. Furthermore, we show that the selection of the weights $\\sigma_j, b_j \\in \\mathbb{C}$ and $\\rho_j \\in \\mathbb{C}^n$ is continuous with respect to $f$ and prove that the derived rate of approximation is optimal under this continuity assumption. We also discuss",
    "path": "papers/23/03/2303.16813.json",
    "total_tokens": 1054,
    "translated_title": "浅层复值神经网络对$C^k$-函数的最优逼近",
    "translated_abstract": "本文证明了使用浅层复值神经网络对复立方体上$C^k$（在实变量意义下）的函数进行逼近的量化结果。具体而言，我们考虑具有单层隐藏层和$m$个神经元的神经网络，即形如$z \\mapsto \\sum_{j=1}^m \\sigma_j \\cdot \\phi\\big(\\rho_j^T z + b_j\\big)$的网络，并且证明了可以使用这种形式的函数逼近$C^k \\left(\\Omega_n;\\mathbb{C}\\right)$中的任何函数，当$m\\to\\infty$时误差为$m^{-k/(2n)}$.此外，我们还证明选取权值$\\sigma_j,b_j\\in\\mathbb{C}$和$\\rho_j\\in\\mathbb{C}^n$对$f$连续并且在这种连续性假设下获得的逼近速率是最优的。",
    "tldr": "本文证明了对于$C^k$（在实变量意义下）的函数，使用具有单层隐藏层和$m$个神经元的神经网络可以以错误率$m^{-k/(2n)}$将其逼近。此外，如果选取权值$\\sigma_j,b_j\\in\\mathbb{C}$和$\\rho_j\\in\\mathbb{C}^n$对$f$连续，那么获得的逼近速率是最优的。",
    "en_tdlr": "This paper proves that a shallow complex-valued neural network with a single hidden layer and m neurons can approximate functions of regularity $C^k$ (in the sense of real variables) defined on the complex cube. The error is of the order $m^{-k/(2n)}$ when $m$ goes to infinity. Additionally, the selection of weights is shown to be continuous with respect to f and the derived rate of approximation is optimal under this continuity assumption."
}