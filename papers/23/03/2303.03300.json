{
    "title": "Chasing Fairness Under Distribution Shift: A Model Weight Perturbation Approach. (arXiv:2303.03300v2 [cs.LG] UPDATED)",
    "abstract": "Fairness in machine learning has attracted increasing attention in recent years. The fairness methods improving algorithmic fairness for in-distribution data may not perform well under distribution shifts. In this paper, we first theoretically demonstrate the inherent connection between distribution shift, data perturbation, and model weight perturbation. Subsequently, we analyze the sufficient conditions to guarantee fairness (i.e., low demographic parity) for the target dataset, including fairness for the source dataset, and low prediction difference between the source and target datasets for each sensitive attribute group. Motivated by these sufficient conditions, we propose robust fairness regularization (RFR) by considering the worst case within the model weight perturbation ball for each sensitive attribute group. We evaluate the effectiveness of our proposed RFR algorithm on synthetic and real distribution shifts across various datasets. Experimental results demonstrate that RFR",
    "link": "http://arxiv.org/abs/2303.03300",
    "context": "Title: Chasing Fairness Under Distribution Shift: A Model Weight Perturbation Approach. (arXiv:2303.03300v2 [cs.LG] UPDATED)\nAbstract: Fairness in machine learning has attracted increasing attention in recent years. The fairness methods improving algorithmic fairness for in-distribution data may not perform well under distribution shifts. In this paper, we first theoretically demonstrate the inherent connection between distribution shift, data perturbation, and model weight perturbation. Subsequently, we analyze the sufficient conditions to guarantee fairness (i.e., low demographic parity) for the target dataset, including fairness for the source dataset, and low prediction difference between the source and target datasets for each sensitive attribute group. Motivated by these sufficient conditions, we propose robust fairness regularization (RFR) by considering the worst case within the model weight perturbation ball for each sensitive attribute group. We evaluate the effectiveness of our proposed RFR algorithm on synthetic and real distribution shifts across various datasets. Experimental results demonstrate that RFR",
    "path": "papers/23/03/2303.03300.json",
    "total_tokens": 861,
    "translated_title": "追逐公平性在分布转移下：一种模型权重扰动的方法",
    "translated_abstract": "机器学习中的公平性近年来引起了越来越多的关注。针对分布转移下的数据，提高算法公平性的公平方法可能效果不佳。本文首先从理论上证明了分布转移、数据扰动和模型权重扰动之间的内在联系。随后，我们分析了确保目标数据集公平性（即低人口统计学平衡）的充分条件，包括源数据集的公平性，以及每个敏感属性组的源数据集和目标数据集之间的低预测差异。受到这些充分条件的启发，我们提出了鲁棒公平正则化（RFR），通过考虑每个敏感属性组在模型权重扰动球内的最坏情况来实现。我们在合成和真实分布转移数据集上评估了我们提出的RFR算法的有效性。实验结果表明，RFR算法在不同数据集上的分布转移下具有良好的性能表现。",
    "tldr": "本文提出了一种鲁棒公平正则化算法（RFR），通过对模型权重进行扰动来实现公平性。实验结果表明，RFR在不同的数据集和分布转移情况下都表现出良好的性能。"
}