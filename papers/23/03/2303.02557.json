{
    "title": "Bounding the Optimal Value Function in Compositional Reinforcement Learning. (arXiv:2303.02557v2 [cs.LG] UPDATED)",
    "abstract": "In the field of reinforcement learning (RL), agents are often tasked with solving a variety of problems differing only in their reward functions. In order to quickly obtain solutions to unseen problems with new reward functions, a popular approach involves functional composition of previously solved tasks. However, previous work using such functional composition has primarily focused on specific instances of composition functions whose limiting assumptions allow for exact zero-shot composition. Our work unifies these examples and provides a more general framework for compositionality in both standard and entropy-regularized RL. We find that, for a broad class of functions, the optimal solution for the composite task of interest can be related to the known primitive task solutions. Specifically, we present double-sided inequalities relating the optimal composite value function to the value functions for the primitive tasks. We also show that the regret of using a zero-shot policy can be",
    "link": "http://arxiv.org/abs/2303.02557",
    "context": "Title: Bounding the Optimal Value Function in Compositional Reinforcement Learning. (arXiv:2303.02557v2 [cs.LG] UPDATED)\nAbstract: In the field of reinforcement learning (RL), agents are often tasked with solving a variety of problems differing only in their reward functions. In order to quickly obtain solutions to unseen problems with new reward functions, a popular approach involves functional composition of previously solved tasks. However, previous work using such functional composition has primarily focused on specific instances of composition functions whose limiting assumptions allow for exact zero-shot composition. Our work unifies these examples and provides a more general framework for compositionality in both standard and entropy-regularized RL. We find that, for a broad class of functions, the optimal solution for the composite task of interest can be related to the known primitive task solutions. Specifically, we present double-sided inequalities relating the optimal composite value function to the value functions for the primitive tasks. We also show that the regret of using a zero-shot policy can be",
    "path": "papers/23/03/2303.02557.json",
    "total_tokens": 864,
    "translated_title": "在组合强化学习中限制最优值函数的界限",
    "translated_abstract": "在强化学习领域，智能体通常需要解决一系列仅在奖励函数上有所不同的问题。为了快速获得适用于新奖励函数的未知问题的解决方案，一种流行的方法涉及到以前解决任务的功能组合。然而，以前使用这种功能组合的工作主要集中在组合函数的具体实例上，这些实例的极限假设允许进行精确的零-shot组合。我们的工作统一了这些示例，并为标准和熵正则化RL中的组合性提供了更一般的框架。我们发现，对于一类广泛的函数，感兴趣的组合任务的最优解可以与已知的原始任务解决方案相关联。具体而言，我们提出了双侧不等式，将最优组合值函数与原始任务的值函数相关联。我们还展示了使用零-shot策略的遗憾可以得到界限。",
    "tldr": "本文在组合强化学习中提出了一种通用的框架，将感兴趣的组合任务的最优解与已知的原始任务解决方案相关联，并提出了双侧不等式将最优组合值函数与原始任务的值函数相关联。",
    "en_tdlr": "This paper proposes a general framework for compositional reinforcement learning by unifying examples of functional composition and presenting double-sided inequalities relating the optimal composite value function to the value functions for primitive tasks."
}