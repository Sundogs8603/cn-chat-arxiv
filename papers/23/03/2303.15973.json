{
    "title": "Do Neural Topic Models Really Need Dropout? Analysis of the Effect of Dropout in Topic Modeling. (arXiv:2303.15973v1 [cs.CL])",
    "abstract": "Dropout is a widely used regularization trick to resolve the overfitting issue in large feedforward neural networks trained on a small dataset, which performs poorly on the held-out test subset. Although the effectiveness of this regularization trick has been extensively studied for convolutional neural networks, there is a lack of analysis of it for unsupervised models and in particular, VAE-based neural topic models. In this paper, we have analyzed the consequences of dropout in the encoder as well as in the decoder of the VAE architecture in three widely used neural topic models, namely, contextualized topic model (CTM), ProdLDA, and embedded topic model (ETM) using four publicly available datasets. We characterize the dropout effect on these models in terms of the quality and predictive performance of the generated topics.",
    "link": "http://arxiv.org/abs/2303.15973",
    "context": "Title: Do Neural Topic Models Really Need Dropout? Analysis of the Effect of Dropout in Topic Modeling. (arXiv:2303.15973v1 [cs.CL])\nAbstract: Dropout is a widely used regularization trick to resolve the overfitting issue in large feedforward neural networks trained on a small dataset, which performs poorly on the held-out test subset. Although the effectiveness of this regularization trick has been extensively studied for convolutional neural networks, there is a lack of analysis of it for unsupervised models and in particular, VAE-based neural topic models. In this paper, we have analyzed the consequences of dropout in the encoder as well as in the decoder of the VAE architecture in three widely used neural topic models, namely, contextualized topic model (CTM), ProdLDA, and embedded topic model (ETM) using four publicly available datasets. We characterize the dropout effect on these models in terms of the quality and predictive performance of the generated topics.",
    "path": "papers/23/03/2303.15973.json",
    "total_tokens": 824,
    "translated_title": "神经主题模型真的需要使用dropout吗？关于dropout在主题建模中的影响分析",
    "translated_abstract": "Dropout是一种广泛使用的正则化技巧，用于解决在小数据集上训练的大型前馈神经网络过拟合问题，该问题在测试集上表现不佳。尽管这种正则化技巧在卷积神经网络中的有效性已经得到广泛研究，但对于无监督模型（特别是基于VAE的神经主题模型），缺乏对其的分析。本文在三个广泛使用的神经主题模型（即，情境主题模型（CTM），ProdLDA和嵌入式主题模型（ETM））中，利用四个公开可用数据集，分析了VAE架构的编码器和解码器中dropout的后果。我们从生成的主题的质量和预测性能的角度，表征了这些模型的dropout效应。",
    "tldr": "本研究分析了三种常见的神经主题模型（CTM、ProdLDA和ETM），利用四个公开数据集探讨了使用dropout对神经主题模型的质量和预测效果的影响。",
    "en_tdlr": "This paper analyzes the effect of dropout in three widely used neural topic models (CTM, ProdLDA, and ETM) using four publicly available datasets and characterizes its impact on the quality and predictive performance of the generated topics."
}