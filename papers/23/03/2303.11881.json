{
    "title": "Protective Self-Adaptive Pruning to Better Compress DNNs. (arXiv:2303.11881v1 [cs.CV])",
    "abstract": "Adaptive network pruning approach has recently drawn significant attention due to its excellent capability to identify the importance and redundancy of layers and filters and customize a suitable pruning solution. However, it remains unsatisfactory since current adaptive pruning methods rely mostly on an additional monitor to score layer and filter importance, and thus faces high complexity and weak interpretability. To tackle these issues, we have deeply researched the weight reconstruction process in iterative prune-train process and propose a Protective Self-Adaptive Pruning (PSAP) method. First of all, PSAP can utilize its own information, weight sparsity ratio, to adaptively adjust pruning ratio of layers before each pruning step. Moreover, we propose a protective reconstruction mechanism to prevent important filters from being pruned through supervising gradients and to avoid unrecoverable information loss as well. Our PSAP is handy and explicit because it merely depends on weigh",
    "link": "http://arxiv.org/abs/2303.11881",
    "context": "Title: Protective Self-Adaptive Pruning to Better Compress DNNs. (arXiv:2303.11881v1 [cs.CV])\nAbstract: Adaptive network pruning approach has recently drawn significant attention due to its excellent capability to identify the importance and redundancy of layers and filters and customize a suitable pruning solution. However, it remains unsatisfactory since current adaptive pruning methods rely mostly on an additional monitor to score layer and filter importance, and thus faces high complexity and weak interpretability. To tackle these issues, we have deeply researched the weight reconstruction process in iterative prune-train process and propose a Protective Self-Adaptive Pruning (PSAP) method. First of all, PSAP can utilize its own information, weight sparsity ratio, to adaptively adjust pruning ratio of layers before each pruning step. Moreover, we propose a protective reconstruction mechanism to prevent important filters from being pruned through supervising gradients and to avoid unrecoverable information loss as well. Our PSAP is handy and explicit because it merely depends on weigh",
    "path": "papers/23/03/2303.11881.json",
    "total_tokens": 951,
    "translated_title": "用保护性自适应剪枝来更好地压缩DNN",
    "translated_abstract": "自适应网络剪枝方法因其优秀的能力来识别层和过滤器的重要性和冗余性并定制合适的剪枝方案而受到了极大的关注。然而，由于当前的自适应剪枝方法大多依赖于额外的监视器来评分层和过滤器的重要性，因此仍然不尽人意，面临着复杂性和可解释性弱的问题。为了解决这些问题，我们深入研究了迭代修剪-训练过程中的重量重构过程，并提出了一种保护性自适应修剪（PSAP）方法。首先，PSAP可以利用自身信息，即重量稀疏比，以使每个修剪步骤之前自适应地调整层的修剪比率。此外，我们提出了一种保护性重建机制，通过监督梯度来防止重要过滤器被修剪并避免不可恢复的信息丢失。我们的PSAP非常便捷和明确，因为它仅依赖于权重。",
    "tldr": "本文提出了一种新的保护性自适应剪枝（PSAP）方法，该方法利用权重稀疏比自适应地调整层的剪枝比率，并通过监督梯度来避免重要过滤器被剪枝，从而避免不可恢复的信息丢失。",
    "en_tdlr": "This paper proposes a new Protective Self-Adaptive Pruning (PSAP) method that adaptively adjusts pruning ratio of layers using weight sparsity ratio and prevents important filters from being pruned through supervising gradients to avoid unrecoverable information loss."
}