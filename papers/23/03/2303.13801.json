{
    "title": "Toward Open-domain Slot Filling via Self-supervised Co-training. (arXiv:2303.13801v1 [cs.CL])",
    "abstract": "Slot filling is one of the critical tasks in modern conversational systems. The majority of existing literature employs supervised learning methods, which require labeled training data for each new domain. Zero-shot learning and weak supervision approaches, among others, have shown promise as alternatives to manual labeling. Nonetheless, these learning paradigms are significantly inferior to supervised learning approaches in terms of performance. To minimize this performance gap and demonstrate the possibility of open-domain slot filling, we propose a Self-supervised Co-training framework, called SCot, that requires zero in-domain manually labeled training examples and works in three phases. Phase one acquires two sets of complementary pseudo labels automatically. Phase two leverages the power of the pre-trained language model BERT, by adapting it for the slot filling task using these sets of pseudo labels. In phase three, we introduce a self-supervised cotraining mechanism, where both",
    "link": "http://arxiv.org/abs/2303.13801",
    "context": "Title: Toward Open-domain Slot Filling via Self-supervised Co-training. (arXiv:2303.13801v1 [cs.CL])\nAbstract: Slot filling is one of the critical tasks in modern conversational systems. The majority of existing literature employs supervised learning methods, which require labeled training data for each new domain. Zero-shot learning and weak supervision approaches, among others, have shown promise as alternatives to manual labeling. Nonetheless, these learning paradigms are significantly inferior to supervised learning approaches in terms of performance. To minimize this performance gap and demonstrate the possibility of open-domain slot filling, we propose a Self-supervised Co-training framework, called SCot, that requires zero in-domain manually labeled training examples and works in three phases. Phase one acquires two sets of complementary pseudo labels automatically. Phase two leverages the power of the pre-trained language model BERT, by adapting it for the slot filling task using these sets of pseudo labels. In phase three, we introduce a self-supervised cotraining mechanism, where both",
    "path": "papers/23/03/2303.13801.json",
    "total_tokens": 801,
    "translated_title": "通过自监督协同训练实现开放域槽填充",
    "translated_abstract": "槽填充是现代会话系统中的关键任务之一。现有大部分文献采用监督学习方法，需要每个新域的标记训练数据。零样本学习和弱监督方法等已表现出替代手动标注的前景，但是这些学习范例在性能方面明显逊于监督学习方法。为了最小化这种性能差距并展示开放域槽填充的可能性，我们提出了一种自监督协同训练框架，称为SCot，它不需要领域内手动标记训练示例并分为三个阶段进行。",
    "tldr": "本文提出了一种名为SCot的自监督协同训练框架，通过使用BERT模型和伪标签，实现开放域槽填充任务的零样本学习，克服了传统监督学习方法需要大量手动标注数据的问题。"
}