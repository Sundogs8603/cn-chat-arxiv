{
    "title": "Adaptive Federated Learning via New Entropy Approach. (arXiv:2303.14966v2 [cs.DC] UPDATED)",
    "abstract": "Federated Learning (FL) has recently emerged as a popular framework, which allows resource-constrained discrete clients to cooperatively learn the global model under the orchestration of a central server while storing privacy-sensitive data locally. However, due to the difference in equipment and data divergence of heterogeneous clients, there will be parameter deviation between local models, resulting in a slow convergence rate and a reduction of the accuracy of the global model. The current FL algorithms use the static client learning strategy pervasively and can not adapt to the dynamic training parameters of different clients. In this paper, by considering the deviation between different local model parameters, we propose an adaptive learning rate scheme for each client based on entropy theory to alleviate the deviation between heterogeneous clients and achieve fast convergence of the global model. It's difficult to design the optimal dynamic learning rate for each client as the lo",
    "link": "http://arxiv.org/abs/2303.14966",
    "context": "Title: Adaptive Federated Learning via New Entropy Approach. (arXiv:2303.14966v2 [cs.DC] UPDATED)\nAbstract: Federated Learning (FL) has recently emerged as a popular framework, which allows resource-constrained discrete clients to cooperatively learn the global model under the orchestration of a central server while storing privacy-sensitive data locally. However, due to the difference in equipment and data divergence of heterogeneous clients, there will be parameter deviation between local models, resulting in a slow convergence rate and a reduction of the accuracy of the global model. The current FL algorithms use the static client learning strategy pervasively and can not adapt to the dynamic training parameters of different clients. In this paper, by considering the deviation between different local model parameters, we propose an adaptive learning rate scheme for each client based on entropy theory to alleviate the deviation between heterogeneous clients and achieve fast convergence of the global model. It's difficult to design the optimal dynamic learning rate for each client as the lo",
    "path": "papers/23/03/2303.14966.json",
    "total_tokens": 879,
    "translated_title": "新熵方法的自适应联邦学习",
    "translated_abstract": "联邦学习 (FL) 是一种新兴的框架，它允许资源受限的离散客户端在中央服务器的协调下，通过在本地存储保护隐私数据的方式，共同学习全局模型。然而，由于异构客户端的设备和数据差异会导致本地模型参数的偏差，进而导致全局模型的收敛速度减慢和精度降低。当前的 FL 算法普遍采用静态客户端学习策略并不能适应不同客户端的动态训练参数。在本文中，我们根据熵理论考虑不同本地模型参数之间的偏差，为每个客户端提出了基于熵理论的自适应学习率方案，以缓解异构客户端之间的偏差，实现全局模型的快速收敛。但由于不同客户端的本地数据集和特征具有显著的差异，设计每个客户端的最优动态学习率是困难的。",
    "tldr": "本文提出了一种新的自适应学习率方案，基于熵理论缓解异构客户端之间的偏差，实现全局模型的快速收敛。",
    "en_tdlr": "This paper proposes a new adaptive learning rate scheme based on entropy theory, which alleviates the deviation between heterogeneous clients to achieve fast convergence of the global model in Federated Learning."
}