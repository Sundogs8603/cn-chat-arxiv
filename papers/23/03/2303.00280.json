{
    "title": "Label Attention Network for sequential multi-label classification: you were looking at a wrong self-attention. (arXiv:2303.00280v2 [cs.LG] UPDATED)",
    "abstract": "Most of the available user information can be represented as a sequence of timestamped events. Each event is assigned a set of categorical labels whose future structure is of great interest. For instance, our goal is to predict a group of items in the next customer's purchase or tomorrow's client transactions. This is a multi-label classification problem for sequential data. Modern approaches focus on transformer architecture for sequential data introducing self-attention for the elements in a sequence. In that case, we take into account events' time interactions but lose information on label inter-dependencies. Motivated by this shortcoming, we propose leveraging a self-attention mechanism over labels preceding the predicted step. As our approach is a Label-Attention NETwork, we call it LANET. Experimental evidence suggests that LANET outperforms the established models' performance and greatly captures interconnections between labels. For example, the micro-AUC of our approach is $0.9",
    "link": "http://arxiv.org/abs/2303.00280",
    "context": "Title: Label Attention Network for sequential multi-label classification: you were looking at a wrong self-attention. (arXiv:2303.00280v2 [cs.LG] UPDATED)\nAbstract: Most of the available user information can be represented as a sequence of timestamped events. Each event is assigned a set of categorical labels whose future structure is of great interest. For instance, our goal is to predict a group of items in the next customer's purchase or tomorrow's client transactions. This is a multi-label classification problem for sequential data. Modern approaches focus on transformer architecture for sequential data introducing self-attention for the elements in a sequence. In that case, we take into account events' time interactions but lose information on label inter-dependencies. Motivated by this shortcoming, we propose leveraging a self-attention mechanism over labels preceding the predicted step. As our approach is a Label-Attention NETwork, we call it LANET. Experimental evidence suggests that LANET outperforms the established models' performance and greatly captures interconnections between labels. For example, the micro-AUC of our approach is $0.9",
    "path": "papers/23/03/2303.00280.json",
    "total_tokens": 837,
    "translated_title": "序列多标签分类的标签注意力网络：你以前看错了自注意力",
    "translated_abstract": "大多数可用的用户信息可以表示为时间戳事件的序列。每个事件被分配了一组分类标签，其未来结构非常重要。例如，我们的目标是预测下一个客户购买的物品或明天的客户交易中的一组物品。这是一个序列数据的多标签分类问题。现代方法针对序列数据的转换器架构引入了元素的自关注。在这种情况下，我们考虑事件的时间交互作用，但是失去了标签之间的信息依赖关系。受到这个缺点的启发，我们建议利用先于预测步骤的标签的自注意机制。由于我们的方法是标签注意力网络，因此我们称之为LANET。实验证据表明，LANET优于已建立的模型的性能，并极大地捕捉了标签之间的相互关系。例如，我们的方法的微观AUC是0.9。",
    "tldr": "LANET是一种标签注意力网络，可用于解决序列多标签分类问题，能够更好地捕捉标签之间的相互关系，并且在实验中表现良好。",
    "en_tdlr": "LANET is a label attention network for sequential multi-label classification, which can capture interconnections between labels and outperforms established models in experiments."
}