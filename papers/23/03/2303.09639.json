{
    "title": "Neural Architecture Search for Effective Teacher-Student Knowledge Transfer in Language Models. (arXiv:2303.09639v1 [cs.CL])",
    "abstract": "Large pre-trained language models have achieved state-of-the-art results on a variety of downstream tasks. Knowledge Distillation (KD) of a smaller student model addresses their inefficiency, allowing for deployment in resource-constraint environments. KD however remains ineffective, as the student is manually selected from a set of existing options already pre-trained on large corpora, a sub-optimal choice within the space of all possible student architectures. This paper proposes KD-NAS, the use of Neural Architecture Search (NAS) guided by the Knowledge Distillation process to find the optimal student model for distillation from a teacher, for a given natural language task. In each episode of the search process, a NAS controller predicts a reward based on a combination of accuracy on the downstream task and latency of inference. The top candidate architectures are then distilled from the teacher on a small proxy set. Finally the architecture(s) with the highest reward is selected, a",
    "link": "http://arxiv.org/abs/2303.09639",
    "context": "Title: Neural Architecture Search for Effective Teacher-Student Knowledge Transfer in Language Models. (arXiv:2303.09639v1 [cs.CL])\nAbstract: Large pre-trained language models have achieved state-of-the-art results on a variety of downstream tasks. Knowledge Distillation (KD) of a smaller student model addresses their inefficiency, allowing for deployment in resource-constraint environments. KD however remains ineffective, as the student is manually selected from a set of existing options already pre-trained on large corpora, a sub-optimal choice within the space of all possible student architectures. This paper proposes KD-NAS, the use of Neural Architecture Search (NAS) guided by the Knowledge Distillation process to find the optimal student model for distillation from a teacher, for a given natural language task. In each episode of the search process, a NAS controller predicts a reward based on a combination of accuracy on the downstream task and latency of inference. The top candidate architectures are then distilled from the teacher on a small proxy set. Finally the architecture(s) with the highest reward is selected, a",
    "path": "papers/23/03/2303.09639.json",
    "total_tokens": 980,
    "translated_title": "语言模型中高效师生知识转移的神经架构搜索",
    "translated_abstract": "大型预训练的语言模型已经在各种下游任务上取得了最先进的成果。小型学生成为资源受限环境部署的有效解决方法。然而，现有的已经预训练语料库中选出的学生模型会导致知识转移的低效。本文提出了使用神经架构搜索（NAS）指导知识蒸馏（KD）过程从而找到最优学生模型的KD-NAS方法。在搜索过程的每个episode中，NAS控制器根据下游任务的准确性和推理延迟的组合预测奖励。然后，对排名靠前的候选架构进行蒸馏处理。最后选择最高奖励的架构并固定进行知识蒸馏，制作出学生模型。实验结果表明，KD-NAS能够找到有效的学生模型，这些模型在资源受限场景下表现超越所有手工设计的学生模型，甚至超越大型教师模型。",
    "tldr": "本论文提出一种KD-NAS方法，使用神经架构搜索指导知识蒸馏过程，并找到最优的学生模型，从而在资源受限环境下实现高效师生知识转移，超过手工设计的学生模型和大型教师模型。"
}