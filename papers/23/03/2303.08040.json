{
    "title": "Demographic Parity Inspector: Fairness Audits via the Explanation Space. (arXiv:2303.08040v1 [cs.LG])",
    "abstract": "Even if deployed with the best intentions, machine learning methods can perpetuate, amplify or even create social biases. Measures of (un-)fairness have been proposed as a way to gauge the (non-)discriminatory nature of machine learning models. However, proxies of protected attributes causing discriminatory effects remain challenging to address. In this work, we propose a new algorithmic approach that measures group-wise demographic parity violations and allows us to inspect the causes of inter-group discrimination. Our method relies on the novel idea of measuring the dependence of a model on the protected attribute based on the explanation space, an informative space that allows for more sensitive audits than the primary space of input data or prediction distributions, and allowing for the assertion of theoretical demographic parity auditing guarantees. We provide a mathematical analysis, synthetic examples, and experimental evaluation of real-world data. We release an open-source Pyt",
    "link": "http://arxiv.org/abs/2303.08040",
    "context": "Title: Demographic Parity Inspector: Fairness Audits via the Explanation Space. (arXiv:2303.08040v1 [cs.LG])\nAbstract: Even if deployed with the best intentions, machine learning methods can perpetuate, amplify or even create social biases. Measures of (un-)fairness have been proposed as a way to gauge the (non-)discriminatory nature of machine learning models. However, proxies of protected attributes causing discriminatory effects remain challenging to address. In this work, we propose a new algorithmic approach that measures group-wise demographic parity violations and allows us to inspect the causes of inter-group discrimination. Our method relies on the novel idea of measuring the dependence of a model on the protected attribute based on the explanation space, an informative space that allows for more sensitive audits than the primary space of input data or prediction distributions, and allowing for the assertion of theoretical demographic parity auditing guarantees. We provide a mathematical analysis, synthetic examples, and experimental evaluation of real-world data. We release an open-source Pyt",
    "path": "papers/23/03/2303.08040.json",
    "total_tokens": 908,
    "translated_title": "《人口统计平等检查员：通过解释空间进行公平审核》",
    "translated_abstract": "即使具有最好的意图，机器学习方法也可能延续、放大甚至创造社会偏见。衡量机器学习模型的歧视性（非歧视性）的方法已被提出。然而，导致歧视效果的受保护属性的代理仍然是一个具有挑战性的问题。我们提出了一种新的算法方法，它可以测量分组人口统计学平等的违规情况，并允许我们检查组间歧视的原因。我们的方法依赖于一种新颖的思想，即基于解释空间对模型对受保护属性的依赖度进行测量，解释空间是一种提供比输入数据或预测分布的原始空间更敏感审计的信息空间，从而允许断言理论上的人口统计审核保证。我们提供了数学分析、合成样例和实际数据的实验评估。我们还发布了一个开源的Pytorch实现和一个易于使用的Web应用程序。",
    "tldr": "这篇论文提出了一种基于解释空间的算法方法，测量分组人口统计学平等的违规情况，并允许我们检查组间歧视的原因，提高了审计公平性的敏感度。",
    "en_tdlr": "This paper proposes a novel algorithmic approach based on the explanation space to measure group-wise demographic parity violations and inspect the causes of inter-group discrimination, improving the sensitivity of fairness audits. The method provides theoretical demographic parity auditing guarantees and is accompanied by an open-source Pytorch implementation and a user-friendly web application."
}