{
    "title": "Sharing Low Rank Conformer Weights for Tiny Always-On Ambient Speech Recognition Models. (arXiv:2303.08343v1 [eess.AS])",
    "abstract": "Continued improvements in machine learning techniques offer exciting new opportunities through the use of larger models and larger training datasets. However, there is a growing need to offer these new capabilities on-board low-powered devices such as smartphones, wearables and other embedded environments where only low memory is available. Towards this, we consider methods to reduce the model size of Conformer-based speech recognition models which typically require models with greater than 100M parameters down to just $5$M parameters while minimizing impact on model quality. Such a model allows us to achieve always-on ambient speech recognition on edge devices with low-memory neural processors. We propose model weight reuse at different levels within our model architecture: (i) repeating full conformer block layers, (ii) sharing specific conformer modules across layers, (iii) sharing sub-components per conformer module, and (iv) sharing decomposed sub-component weights after low-rank ",
    "link": "http://arxiv.org/abs/2303.08343",
    "context": "Title: Sharing Low Rank Conformer Weights for Tiny Always-On Ambient Speech Recognition Models. (arXiv:2303.08343v1 [eess.AS])\nAbstract: Continued improvements in machine learning techniques offer exciting new opportunities through the use of larger models and larger training datasets. However, there is a growing need to offer these new capabilities on-board low-powered devices such as smartphones, wearables and other embedded environments where only low memory is available. Towards this, we consider methods to reduce the model size of Conformer-based speech recognition models which typically require models with greater than 100M parameters down to just $5$M parameters while minimizing impact on model quality. Such a model allows us to achieve always-on ambient speech recognition on edge devices with low-memory neural processors. We propose model weight reuse at different levels within our model architecture: (i) repeating full conformer block layers, (ii) sharing specific conformer modules across layers, (iii) sharing sub-components per conformer module, and (iv) sharing decomposed sub-component weights after low-rank ",
    "path": "papers/23/03/2303.08343.json",
    "total_tokens": 923,
    "translated_title": "基于低秩张量分享的Tiny Ambient Speech Recognition模型",
    "translated_abstract": "机器学习技术的持续改进为使用更大的模型和更大的训练数据集提供了令人兴奋的新机会。但是，在仅有低内存的智能手机、可穿戴设备和其他嵌入式环境等低功耗设备上提供这些新功能的需求与日俱增。为此，我们考虑了一些方法来减小基于Conformer的语音识别模型的模型大小，这些模型通常需要大于100M个参数的模型，将其缩小到仅$5$M个参数，同时最小化对模型质量的影响。这样的模型使我们能够在具有低内存神经处理器的边缘设备上实现始终处于运行状态的语音识别。我们提出在模型架构中的不同层次上重复使用模型权重: (i) 重复整个Conformer块层，(ii) 在层之间共享特定的Conformer模块，(iii) 在Conformer模块中共享子部件，(iv) 在低秩张量分解后共享分解的子部件权重。",
    "tldr": "本论文提出了一种基于低秩张量分享的模型方法，将大型语音识别模型缩小至5M参数，同时实现了在低内存神经处理器边缘设备上的始终处于运行状态的语音识别。",
    "en_tdlr": "This paper proposes a model approach based on low-rank tensor sharing to reduce the model size of Conformer-based speech recognition models to 5M parameters while achieving always-on ambient speech recognition on low-memory neural processors in edge devices."
}