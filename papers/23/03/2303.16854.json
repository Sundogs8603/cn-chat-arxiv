{
    "title": "AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators. (arXiv:2303.16854v1 [cs.CL])",
    "abstract": "Many natural language processing (NLP) tasks rely on labeled data to train machine learning models to achieve high performance. However, data annotation can be a time-consuming and expensive process, especially when the task involves a large amount of data or requires specialized domains. Recently, GPT-3.5 series models have demonstrated remarkable few-shot and zero-shot ability across various NLP tasks. In this paper, we first claim that large language models (LLMs), such as GPT-3.5, can serve as an excellent crowdsourced annotator by providing them with sufficient guidance and demonstrated examples. To make LLMs to be better annotators, we propose a two-step approach, 'explain-then-annotate'. To be more precise, we begin by creating prompts for every demonstrated example, which we subsequently utilize to prompt a LLM to provide an explanation for why the specific ground truth answer/label was chosen for that particular example. Following this, we construct the few-shot chain-of-thoug",
    "link": "http://arxiv.org/abs/2303.16854",
    "context": "Title: AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators. (arXiv:2303.16854v1 [cs.CL])\nAbstract: Many natural language processing (NLP) tasks rely on labeled data to train machine learning models to achieve high performance. However, data annotation can be a time-consuming and expensive process, especially when the task involves a large amount of data or requires specialized domains. Recently, GPT-3.5 series models have demonstrated remarkable few-shot and zero-shot ability across various NLP tasks. In this paper, we first claim that large language models (LLMs), such as GPT-3.5, can serve as an excellent crowdsourced annotator by providing them with sufficient guidance and demonstrated examples. To make LLMs to be better annotators, we propose a two-step approach, 'explain-then-annotate'. To be more precise, we begin by creating prompts for every demonstrated example, which we subsequently utilize to prompt a LLM to provide an explanation for why the specific ground truth answer/label was chosen for that particular example. Following this, we construct the few-shot chain-of-thoug",
    "path": "papers/23/03/2303.16854.json",
    "total_tokens": 964,
    "translated_title": "AnnoLLM：使大型语言模型成为更好的众包标注器",
    "translated_abstract": "许多自然语言处理（NLP）任务依赖于带标签的数据，以训练机器学习模型实现高性能。然而，数据注释可能是一个耗时且昂贵的过程，特别是当任务涉及大量数据或需要专业领域时。最近，GPT-3.5系列模型在各种NLP任务中展示出了令人瞩目的少样本和零样本能力。本文首先声称，大型语言模型（LLMs），如GPT-3.5，可以通过为它们提供充分的指导和演示示例来作为优秀的众包标注器。为了使LLMs成为更好的标注器，我们提出了一种两步方法，“先解释再注释”。更确切地说，我们首先为每个演示示例创建提示，随后利用这些提示提示LLM提供关于为什么对于特定示例选择了特定的基础真相回答/标签的解释。随后，我们构建了few-shot思维链。",
    "tldr": "本文提出了一种两步法，即“先解释再注释”，以使大型语言模型（LLMs）成为更好的众包标注器，首先为每个演示实例创建提示，随后利用这些提示提示LLM提供解释。",
    "en_tdlr": "This paper proposes a two-step approach, \"explain-then-annotate,\" to make large language models (LLMs) better crowdsourced annotators, and demonstrates that LLMs can function as excellent annotators when provided with sufficient guidance and demonstrated examples. The approach involves creating prompts for every demonstrated example, which are then used to prompt the LLM to provide an explanation for why a specific ground truth answer/label was chosen, followed by constructing a few-shot chain-of-thought."
}