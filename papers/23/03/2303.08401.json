{
    "title": "Implicit Ray-Transformers for Multi-view Remote Sensing Image Segmentation. (arXiv:2303.08401v1 [cs.CV])",
    "abstract": "The mainstream CNN-based remote sensing (RS) image semantic segmentation approaches typically rely on massive labeled training data. Such a paradigm struggles with the problem of RS multi-view scene segmentation with limited labeled views due to the lack of considering 3D information within the scene. In this paper, we propose ''Implicit Ray-Transformer (IRT)'' based on Implicit Neural Representation (INR), for RS scene semantic segmentation with sparse labels (such as 4-6 labels per 100 images). We explore a new way of introducing multi-view 3D structure priors to the task for accurate and view-consistent semantic segmentation. The proposed method includes a two-stage learning process. In the first stage, we optimize a neural field to encode the color and 3D structure of the remote sensing scene based on multi-view images. In the second stage, we design a Ray Transformer to leverage the relations between the neural field 3D features and 2D texture features for learning better semantic",
    "link": "http://arxiv.org/abs/2303.08401",
    "context": "Title: Implicit Ray-Transformers for Multi-view Remote Sensing Image Segmentation. (arXiv:2303.08401v1 [cs.CV])\nAbstract: The mainstream CNN-based remote sensing (RS) image semantic segmentation approaches typically rely on massive labeled training data. Such a paradigm struggles with the problem of RS multi-view scene segmentation with limited labeled views due to the lack of considering 3D information within the scene. In this paper, we propose ''Implicit Ray-Transformer (IRT)'' based on Implicit Neural Representation (INR), for RS scene semantic segmentation with sparse labels (such as 4-6 labels per 100 images). We explore a new way of introducing multi-view 3D structure priors to the task for accurate and view-consistent semantic segmentation. The proposed method includes a two-stage learning process. In the first stage, we optimize a neural field to encode the color and 3D structure of the remote sensing scene based on multi-view images. In the second stage, we design a Ray Transformer to leverage the relations between the neural field 3D features and 2D texture features for learning better semantic",
    "path": "papers/23/03/2303.08401.json",
    "total_tokens": 923,
    "translated_title": "基于Implicit Ray-Transformers的多视角遥感图像分割",
    "translated_abstract": "目前大多数基于CNN的遥感图像语义分割方法通常依赖于大量标记的训练数据，但这种范式因未考虑场景中的3D信息，而在仅有少量标记视图的情况下难以应对遥感多视角场景的分割问题。为此，本文基于Implicit Neural Representation（INR）提出了“Implicit Ray-Transformer（IRT）”，用于处理仅有稀疏标注（如100个图像中的4-6个标签）的遥感场景语义分割。我们探索了一种将多视图3D结构先验引入任务以获得准确且视角一致的语义分割的新方法。所提出的方法包括两个阶段的学习过程。在第一阶段，我们优化神经场以编码基于多视图图像的遥感场景的颜色和3D结构。在第二阶段，我们设计了一个Ray Transformer来利用神经场3D特征与2D纹理特征之间的关系，以学习更好的语义分割。",
    "tldr": "本文提出了一种基于Implicit Ray-Transformers的多视角遥感图像分割方法，借助神经场和Ray Transformer，有效处理稀疏标注情况下的遥感场景，实现准确且视角一致的语义分割。",
    "en_tdlr": "This paper proposes a multi-view remote sensing image segmentation approach based on Implicit Ray-Transformers, which utilizes neural field and Ray Transformer to deal with sparse labeling and achieve accurate and view-consistent semantic segmentation in remote sensing scenes."
}