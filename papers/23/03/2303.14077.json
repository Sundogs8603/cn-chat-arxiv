{
    "title": "Improved Adversarial Training Through Adaptive Instance-wise Loss Smoothing. (arXiv:2303.14077v1 [cs.CV])",
    "abstract": "Deep neural networks can be easily fooled into making incorrect predictions through corruption of the input by adversarial perturbations: human-imperceptible artificial noise. So far adversarial training has been the most successful defense against such adversarial attacks. This work focuses on improving adversarial training to boost adversarial robustness. We first analyze, from an instance-wise perspective, how adversarial vulnerability evolves during adversarial training. We find that during training an overall reduction of adversarial loss is achieved by sacrificing a considerable proportion of training samples to be more vulnerable to adversarial attack, which results in an uneven distribution of adversarial vulnerability among data. Such \"uneven vulnerability\", is prevalent across several popular robust training methods and, more importantly, relates to overfitting in adversarial training. Motivated by this observation, we propose a new adversarial training method: Instance-adapt",
    "link": "http://arxiv.org/abs/2303.14077",
    "context": "Title: Improved Adversarial Training Through Adaptive Instance-wise Loss Smoothing. (arXiv:2303.14077v1 [cs.CV])\nAbstract: Deep neural networks can be easily fooled into making incorrect predictions through corruption of the input by adversarial perturbations: human-imperceptible artificial noise. So far adversarial training has been the most successful defense against such adversarial attacks. This work focuses on improving adversarial training to boost adversarial robustness. We first analyze, from an instance-wise perspective, how adversarial vulnerability evolves during adversarial training. We find that during training an overall reduction of adversarial loss is achieved by sacrificing a considerable proportion of training samples to be more vulnerable to adversarial attack, which results in an uneven distribution of adversarial vulnerability among data. Such \"uneven vulnerability\", is prevalent across several popular robust training methods and, more importantly, relates to overfitting in adversarial training. Motivated by this observation, we propose a new adversarial training method: Instance-adapt",
    "path": "papers/23/03/2303.14077.json",
    "total_tokens": 1185,
    "translated_title": "自适应实例级损失平滑改进对抗训练",
    "translated_abstract": "通过对输入进行对抗扰动：即人类难以察觉的人造噪声，可以轻易地迷惑深度神经网络从而做出不正确的预测。目前对抗训练已成为最成功的对抗攻击防御方法，本文致力于改进对抗训练以提升对抗鲁棒性。首先从实例级别的角度分析了对抗训练期间对抗性脆弱性的演变。发现在训练期间，通过牺牲相当比例的训练样本来提高对抗攻击的脆弱性，从而实现对抗性损失的整体降低，这导致了不同数据的对抗性脆弱性分布不均衡。这种“不均衡脆弱性”在几种流行的鲁棒性训练方法中普遍存在，并且与对抗训练中的过拟合相关。基于此观察，我们提出了一种新的对抗训练方法：Instance-adaptive Adversarial Training (IAAT)。该方法在训练过程中平滑实例级别的对抗性损失，鼓励模型关注“难”的样本，同时避免牺牲特定的样本而偏爱其他样本。本方法在各种数据集下都取得了最新的最佳结果，并在白盒和黑盒攻击下均优于以前的方法。",
    "tldr": "本文提出了一种新的对抗训练方法(Instance-adaptive Adversarial Training, IAAT)通过平滑实例级别的对抗性损失，鼓励模型关注“难”的样本，同时避免牺牲特定的样本而偏爱其他样本，取得了在各种数据集下的最新、最佳结果，并在白盒和黑盒攻击下均优于以前的方法。",
    "en_tdlr": "This paper proposes a new adversarial training method, Instance-adaptive Adversarial Training (IAAT), which smooths the instance-wise adversarial loss during training, encourages the model to focus on the \"hard\" samples, while avoiding sacrificing any particular sample in favor of the others. Our approach achieves new state-of-the-art results on various datasets under both white-box and black-box attacks, outperforming previous methods."
}