{
    "title": "CLIP4MC: An RL-Friendly Vision-Language Model for Minecraft. (arXiv:2303.10571v1 [cs.LG])",
    "abstract": "One of the essential missions in the AI research community is to build an autonomous embodied agent that can attain high-level performance across a wide spectrum of tasks. However, acquiring reward/penalty in all open-ended tasks is unrealistic, making the Reinforcement Learning (RL) training procedure impossible. In this paper, we propose a novel cross-modal contrastive learning framework architecture, CLIP4MC, aiming to learn an RL-friendly vision-language model that serves as a reward function for open-ended tasks. Therefore, no further task-specific reward design is needed. Intuitively, it is more reasonable for the model to address the similarity between the video snippet and the language prompt at both the action and entity levels. To this end, a motion encoder is proposed to capture the motion embeddings across different intervals. The correlation scores are then used to construct the auxiliary reward signal for RL agents. Moreover, we construct a neat YouTube dataset based on t",
    "link": "http://arxiv.org/abs/2303.10571",
    "context": "Title: CLIP4MC: An RL-Friendly Vision-Language Model for Minecraft. (arXiv:2303.10571v1 [cs.LG])\nAbstract: One of the essential missions in the AI research community is to build an autonomous embodied agent that can attain high-level performance across a wide spectrum of tasks. However, acquiring reward/penalty in all open-ended tasks is unrealistic, making the Reinforcement Learning (RL) training procedure impossible. In this paper, we propose a novel cross-modal contrastive learning framework architecture, CLIP4MC, aiming to learn an RL-friendly vision-language model that serves as a reward function for open-ended tasks. Therefore, no further task-specific reward design is needed. Intuitively, it is more reasonable for the model to address the similarity between the video snippet and the language prompt at both the action and entity levels. To this end, a motion encoder is proposed to capture the motion embeddings across different intervals. The correlation scores are then used to construct the auxiliary reward signal for RL agents. Moreover, we construct a neat YouTube dataset based on t",
    "path": "papers/23/03/2303.10571.json",
    "total_tokens": 899,
    "translated_abstract": "AI研究的重要任务之一是构建一种可以在广泛任务上实现高水平性能的自主体。然而，在所有开放性任务中获得奖励/惩罚是不现实的，因此无法进行强化学习（RL）训练。本文提出了一种新颖的跨模态对比学习框架结构CLIP4MC，旨在学习一种RL友好的视觉语言模型，为开放性任务提供奖励函数，因此不需要进一步的任务特定奖励设计。直观地说，该模型更合理地处理视频片段和语言提示之间在动作和实体级别的相似性。为此，提出了一种运动编码器来捕获不同时间间隔内的运动嵌入。然后，相关性得分用于构建RL代理的辅助奖励信号。此外，我们基于整洁的YouTube数据集构建了数据集。",
    "tldr": "本文提出了一种名为CLIP4MC的跨模态对比学习框架，通过学习视觉与语言间的相似性来提供RL友好型的奖励函数，免去了任务特定奖励设计的需求。",
    "en_tdlr": "This paper proposes a novel cross-modal contrastive learning framework called CLIP4MC, which aims to provide an RL-friendly reward function for open-ended tasks by learning the similarity between vision and language, eliminating the need for task-specific reward design. The framework uses a motion encoder to capture motion embeddings and constructs an auxiliary reward signal using correlation scores for RL agents."
}