{
    "title": "An Analysis of Abstractive Text Summarization Using Pre-trained Models. (arXiv:2303.12796v1 [cs.CL])",
    "abstract": "People nowadays use search engines like Google, Yahoo, and Bing to find information on the Internet. Due to explosion in data, it is helpful for users if they are provided relevant summaries of the search results rather than just links to webpages. Text summarization has become a vital approach to help consumers swiftly grasp vast amounts of information.In this paper, different pre-trained models for text summarization are evaluated on different datasets. Specifically, we have used three different pre-trained models, namely, google/pegasus-cnn-dailymail, T5-base, facebook/bart-large-cnn. We have considered three different datasets, namely, CNN-dailymail, SAMSum and BillSum to get the output from the above three models. The pre-trained models are compared over these different datasets, each of 2000 examples, through ROUGH and BLEU metrics.",
    "link": "http://arxiv.org/abs/2303.12796",
    "context": "Title: An Analysis of Abstractive Text Summarization Using Pre-trained Models. (arXiv:2303.12796v1 [cs.CL])\nAbstract: People nowadays use search engines like Google, Yahoo, and Bing to find information on the Internet. Due to explosion in data, it is helpful for users if they are provided relevant summaries of the search results rather than just links to webpages. Text summarization has become a vital approach to help consumers swiftly grasp vast amounts of information.In this paper, different pre-trained models for text summarization are evaluated on different datasets. Specifically, we have used three different pre-trained models, namely, google/pegasus-cnn-dailymail, T5-base, facebook/bart-large-cnn. We have considered three different datasets, namely, CNN-dailymail, SAMSum and BillSum to get the output from the above three models. The pre-trained models are compared over these different datasets, each of 2000 examples, through ROUGH and BLEU metrics.",
    "path": "papers/23/03/2303.12796.json",
    "total_tokens": 790,
    "translated_title": "使用预训练模型进行抽象文本摘要的分析",
    "translated_abstract": "人们现在使用像谷歌、雅虎和必应这样的搜索引擎来查找互联网上的信息。由于数据爆炸，如果为用户提供相关的搜索结果摘要而不仅仅是网页链接将会很有帮助。文本摘要已成为帮助用户迅速掌握大量信息的关键方法。在本文中，对不同的预训练模型进行了在不同数据集上的评估。具体来说，我们使用了三个不同的预训练模型，分别是google/pegasus-cnn-dailymail、T5-base、facebook/bart-large-cnn。我们考虑了三个不同的数据集，分别是CNN-dailymail、SAMSum和BillSum，以从上述三个模型中获取输出。通过ROUGH和BLEU指标，在这些不同的数据集上比较了这些预训练模型，每个数据集有2000个示例。",
    "tldr": "本文对使用预训练模型进行文本摘要的方法进行了评估，并在不同数据集上进行了比较，结果表明......",
    "en_tdlr": "This paper evaluates the method of using pre-trained models for text summarization and compares them on different datasets, showing that..."
}