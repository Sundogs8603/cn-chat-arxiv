{
    "title": "On the Effect of Initialization: The Scaling Path of 2-Layer Neural Networks. (arXiv:2303.17805v1 [cs.LG])",
    "abstract": "In supervised learning, the regularization path is sometimes used as a convenient theoretical proxy for the optimization path of gradient descent initialized with zero. In this paper, we study a modification of the regularization path for infinite-width 2-layer ReLU neural networks with non-zero initial distribution of the weights at different scales. By exploiting a link with unbalanced optimal transport theory, we show that, despite the non-convexity of the 2-layer network training, this problem admits an infinite dimensional convex counterpart. We formulate the corresponding functional optimization problem and investigate its main properties. In particular, we show that as the scale of the initialization ranges between $0$ and $+\\infty$, the associated path interpolates continuously between the so-called kernel and rich regimes. The numerical experiments confirm that, in our setting, the scaling path and the final states of the optimization path behave similarly even beyond these ex",
    "link": "http://arxiv.org/abs/2303.17805",
    "context": "Title: On the Effect of Initialization: The Scaling Path of 2-Layer Neural Networks. (arXiv:2303.17805v1 [cs.LG])\nAbstract: In supervised learning, the regularization path is sometimes used as a convenient theoretical proxy for the optimization path of gradient descent initialized with zero. In this paper, we study a modification of the regularization path for infinite-width 2-layer ReLU neural networks with non-zero initial distribution of the weights at different scales. By exploiting a link with unbalanced optimal transport theory, we show that, despite the non-convexity of the 2-layer network training, this problem admits an infinite dimensional convex counterpart. We formulate the corresponding functional optimization problem and investigate its main properties. In particular, we show that as the scale of the initialization ranges between $0$ and $+\\infty$, the associated path interpolates continuously between the so-called kernel and rich regimes. The numerical experiments confirm that, in our setting, the scaling path and the final states of the optimization path behave similarly even beyond these ex",
    "path": "papers/23/03/2303.17805.json",
    "total_tokens": 891,
    "translated_title": "关于初始化的影响：2层神经网络的缩放路径",
    "translated_abstract": "在监督学习中，正则化路径有时被用作梯度下降的优化路径的方便理论代理。本文研究了具有不同尺度的非零权重的无限宽2层ReLU神经网络的正则化路径的修改。通过利用与不平衡最优输运理论的联系，我们表明，尽管2层网络训练的非凸性，这个问题具有一个无限维度的凸对应。我们制定了相应的函数优化问题并调查了其主要特性。特别地，我们展示了随着初始化的尺度在0到+∞范围内变化，关联路径在所谓的内核和丰富的区域之间连续插值。数值实验证实，在我们的设置中，缩放路径和优化路径的最终状态行为类似，甚至超越了这些范围。",
    "tldr": "本文研究了具有不同尺度的非零权重的无限宽2层ReLU神经网络的正则化路径，展示该问题具有一个无限维度的凸对应，随着初始化的尺度在0到+∞范围内变化，关联路径在所谓的内核和丰富的区域之间连续插值。",
    "en_tdlr": "This paper studies the regularization path of infinite-width 2-layer ReLU neural networks with non-zero initial distribution of weights, showing that it has an infinite dimensional convex counterpart and interpolates continuously between kernel and rich regimes as the initialization scale ranges from 0 to +∞."
}