{
    "title": "Attention is Not Always What You Need: Towards Efficient Classification of Domain-Specific Text. (arXiv:2303.17786v1 [cs.CL])",
    "abstract": "For large-scale IT corpora with hundreds of classes organized in a hierarchy, the task of accurate classification of classes at the higher level in the hierarchies is crucial to avoid errors propagating to the lower levels. In the business world, an efficient and explainable ML model is preferred over an expensive black-box model, especially if the performance increase is marginal. A current trend in the Natural Language Processing (NLP) community is towards employing huge pre-trained language models (PLMs) or what is known as self-attention models (e.g., BERT) for almost any kind of NLP task (e.g., question-answering, sentiment analysis, text classification). Despite the widespread use of PLMs and the impressive performance in a broad range of NLP tasks, there is a lack of a clear and well-justified need to as why these models are being employed for domain-specific text classification (TC) tasks, given the monosemic nature of specialized words (i.e., jargon) found in domain-specific t",
    "link": "http://arxiv.org/abs/2303.17786",
    "context": "Title: Attention is Not Always What You Need: Towards Efficient Classification of Domain-Specific Text. (arXiv:2303.17786v1 [cs.CL])\nAbstract: For large-scale IT corpora with hundreds of classes organized in a hierarchy, the task of accurate classification of classes at the higher level in the hierarchies is crucial to avoid errors propagating to the lower levels. In the business world, an efficient and explainable ML model is preferred over an expensive black-box model, especially if the performance increase is marginal. A current trend in the Natural Language Processing (NLP) community is towards employing huge pre-trained language models (PLMs) or what is known as self-attention models (e.g., BERT) for almost any kind of NLP task (e.g., question-answering, sentiment analysis, text classification). Despite the widespread use of PLMs and the impressive performance in a broad range of NLP tasks, there is a lack of a clear and well-justified need to as why these models are being employed for domain-specific text classification (TC) tasks, given the monosemic nature of specialized words (i.e., jargon) found in domain-specific t",
    "path": "papers/23/03/2303.17786.json",
    "total_tokens": 868,
    "translated_abstract": "对于有数百类分层组织的大规模IT语料库，准确分类上级层次中的类别对于避免错误传递到下一级别是至关重要的。在商业世界中，高效且易于解释的ML模型比昂贵的黑盒模型更受欢迎，特别是如果性能提高微不足道。自然语言处理（NLP）社区目前的趋势是使用大规模预训练语言模型（PLMs）或所谓的自我关注模型（例如BERT）几乎用于任何类型的NLP任务（例如问答、情感分析、文本分类）。尽管PLM广泛使用并在广泛的NLP任务中表现出色，但由于特定领域中特殊单义词（即行话）的单个含义，针对特定领域文本分类（TC）任务使用这些模型的明确和充分的需要仍然缺乏。",
    "tldr": "PLMs并不适用于处理特定领域中的单义词，使用高效且易于解释的ML模型对于特定领域的文本分类任务更实用。"
}