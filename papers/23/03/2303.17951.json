{
    "title": "FP8 versus INT8 for efficient deep learning inference. (arXiv:2303.17951v1 [cs.LG])",
    "abstract": "Recently, the idea of using FP8 as a number format for neural network training has been floating around the deep learning world. Given that most training is currently conducted with entire networks in FP32, or sometimes FP16 with mixed-precision, the step to having some parts of a network run in FP8 with 8-bit weights is an appealing potential speed-up for the generally costly and time-intensive training procedures in deep learning. A natural question arises regarding what this development means for efficient inference on edge devices. In the efficient inference device world, workloads are frequently executed in INT8. Sometimes going even as low as INT4 when efficiency calls for it. In this whitepaper, we compare the performance for both the FP8 and INT formats for efficient on-device inference. We theoretically show the difference between the INT and FP formats for neural networks and present a plethora of post-training quantization and quantization-aware-training results to show how ",
    "link": "http://arxiv.org/abs/2303.17951",
    "context": "Title: FP8 versus INT8 for efficient deep learning inference. (arXiv:2303.17951v1 [cs.LG])\nAbstract: Recently, the idea of using FP8 as a number format for neural network training has been floating around the deep learning world. Given that most training is currently conducted with entire networks in FP32, or sometimes FP16 with mixed-precision, the step to having some parts of a network run in FP8 with 8-bit weights is an appealing potential speed-up for the generally costly and time-intensive training procedures in deep learning. A natural question arises regarding what this development means for efficient inference on edge devices. In the efficient inference device world, workloads are frequently executed in INT8. Sometimes going even as low as INT4 when efficiency calls for it. In this whitepaper, we compare the performance for both the FP8 and INT formats for efficient on-device inference. We theoretically show the difference between the INT and FP formats for neural networks and present a plethora of post-training quantization and quantization-aware-training results to show how ",
    "path": "papers/23/03/2303.17951.json",
    "total_tokens": 842,
    "translated_title": "FP8和INT8在高效深度学习推理中的比较",
    "translated_abstract": "最近，使用FP8作为神经网络训练的数字格式的想法在深度学习世界中流传。鉴于目前大部分训练都是使用完整网络的FP32或者有时使用混合精度的FP16进行的，部分网络使用8位重量级的FP8可以加快深度学习中通常耗时昂贵的训练过程。这引发了人们对于此发展对于边缘设备高效推理的影响的自然问题。在高效推理设备中，工作负载通常在INT8中执行。有时为了保证效率，甚至低至INT4。在本文中，我们比较了FP8和INT格式在设备高效推理中的性能。我们理论上展示了神经网络中INT和FP格式之间的差异，并提供了大量的训练后量化和训练时量化结果来展示如何在不同格式下实现高效推理。",
    "tldr": "本文比较了FP8和INT8在设备高效推理中的性能，展示了量化和量化感知训练的成果，为选择正确的数字格式提供了参考。"
}