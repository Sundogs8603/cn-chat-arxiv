{
    "title": "Aligning benchmark datasets for table structure recognition. (arXiv:2303.00716v2 [cs.CV] UPDATED)",
    "abstract": "Benchmark datasets for table structure recognition (TSR) must be carefully processed to ensure they are annotated consistently. However, even if a dataset's annotations are self-consistent, there may be significant inconsistency across datasets, which can harm the performance of models trained and evaluated on them. In this work, we show that aligning these benchmarks$\\unicode{x2014}$removing both errors and inconsistency between them$\\unicode{x2014}$improves model performance significantly. We demonstrate this through a data-centric approach where we adopt one model architecture, the Table Transformer (TATR), that we hold fixed throughout. Baseline exact match accuracy for TATR evaluated on the ICDAR-2013 benchmark is 65% when trained on PubTables-1M, 42% when trained on FinTabNet, and 69% combined. After reducing annotation mistakes and inter-dataset inconsistency, performance of TATR evaluated on ICDAR-2013 increases substantially to 75% when trained on PubTables-1M, 65% when traine",
    "link": "http://arxiv.org/abs/2303.00716",
    "context": "Title: Aligning benchmark datasets for table structure recognition. (arXiv:2303.00716v2 [cs.CV] UPDATED)\nAbstract: Benchmark datasets for table structure recognition (TSR) must be carefully processed to ensure they are annotated consistently. However, even if a dataset's annotations are self-consistent, there may be significant inconsistency across datasets, which can harm the performance of models trained and evaluated on them. In this work, we show that aligning these benchmarks$\\unicode{x2014}$removing both errors and inconsistency between them$\\unicode{x2014}$improves model performance significantly. We demonstrate this through a data-centric approach where we adopt one model architecture, the Table Transformer (TATR), that we hold fixed throughout. Baseline exact match accuracy for TATR evaluated on the ICDAR-2013 benchmark is 65% when trained on PubTables-1M, 42% when trained on FinTabNet, and 69% combined. After reducing annotation mistakes and inter-dataset inconsistency, performance of TATR evaluated on ICDAR-2013 increases substantially to 75% when trained on PubTables-1M, 65% when traine",
    "path": "papers/23/03/2303.00716.json",
    "total_tokens": 929,
    "translated_title": "对齐表格结构识别基准数据集",
    "translated_abstract": "表格结构识别(TSR)基准数据集必须经过精心处理，以确保它们的注释具有一致性。但即使数据集的注释是自洽的，不同数据集之间也存在显著的不一致性，这可能会损害基于它们训练和评估的模型的性能。在这项工作中，我们展示了通过对齐这些基准数据集$\\unicode{x2014}$删除它们之间的错误和不一致性$\\unicode{x2014}$显著提高模型的性能。我们通过一种数据中心的方法来展示这一点，该方法采用了一个固定的模型架构，即表格变换器(TATR)。在PubTables-1M上训练的TATR的基准完全匹配精度为65%，在FinTabNet上训练为42%，组合为69%。在减少注释错误和数据集间不一致性后，TATR在ICDAR-2013上的性能显著提高，当在PubTables-1M上训练时为75%，在FinTabNet上训练时为65%，组合为76%。",
    "tldr": "本文提出通过对齐表格结构识别基准数据集来提高模型性能，有效减少数据集间的不一致性和注释错误。实验证明此方法显著提高了模型的性能。",
    "en_tdlr": "This paper proposes aligning benchmark datasets for table structure recognition (TSR) to improve model performance and reduce inconsistency and annotation errors. Experiments demonstrate a significant improvement in model performance with this approach."
}