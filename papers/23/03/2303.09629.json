{
    "title": "Online Reinforcement Learning in Periodic MDP. (arXiv:2303.09629v1 [cs.LG])",
    "abstract": "We study learning in periodic Markov Decision Process (MDP), a special type of non-stationary MDP where both the state transition probabilities and reward functions vary periodically, under the average reward maximization setting. We formulate the problem as a stationary MDP by augmenting the state space with the period index, and propose a periodic upper confidence bound reinforcement learning-2 (PUCRL2) algorithm. We show that the regret of PUCRL2 varies linearly with the period $N$ and as $\\mathcal{O}(\\sqrt{Tlog T})$ with the horizon length $T$. Utilizing the information about the sparsity of transition matrix of augmented MDP, we propose another algorithm PUCRLB which enhances upon PUCRL2, both in terms of regret ($O(\\sqrt{N})$ dependency on period) and empirical performance. Finally, we propose two other algorithms U-PUCRL2 and U-PUCRLB for extended uncertainty in the environment in which the period is unknown but a set of candidate periods are known. Numerical results demonstrate",
    "link": "http://arxiv.org/abs/2303.09629",
    "context": "Title: Online Reinforcement Learning in Periodic MDP. (arXiv:2303.09629v1 [cs.LG])\nAbstract: We study learning in periodic Markov Decision Process (MDP), a special type of non-stationary MDP where both the state transition probabilities and reward functions vary periodically, under the average reward maximization setting. We formulate the problem as a stationary MDP by augmenting the state space with the period index, and propose a periodic upper confidence bound reinforcement learning-2 (PUCRL2) algorithm. We show that the regret of PUCRL2 varies linearly with the period $N$ and as $\\mathcal{O}(\\sqrt{Tlog T})$ with the horizon length $T$. Utilizing the information about the sparsity of transition matrix of augmented MDP, we propose another algorithm PUCRLB which enhances upon PUCRL2, both in terms of regret ($O(\\sqrt{N})$ dependency on period) and empirical performance. Finally, we propose two other algorithms U-PUCRL2 and U-PUCRLB for extended uncertainty in the environment in which the period is unknown but a set of candidate periods are known. Numerical results demonstrate",
    "path": "papers/23/03/2303.09629.json",
    "total_tokens": 984,
    "translated_title": "周期性MDP中的在线强化学习",
    "translated_abstract": "我们研究了周期性马尔可夫决策过程（MDP）下的强化学习问题，这是一种特殊的非平稳MDP，其中状态转移概率和奖励函数都会周期性变化，在平均奖励最大化的设置下。我们通过在状态空间中添加周期索引来将问题归结为静态MDP，并提出了一种周期性上置信区间强化学习-2（PUCRL2）算法。我们证明了PUCRL2的遗憾值随周期$N$线性变化，并且随着时限长度$T$呈$\\mathcal{O}(\\sqrt{Tlog T})$的变化。利用增广MDP的转移矩阵的稀疏信息，我们提出另一个算法PUCRLB，它在遗憾（周期的$O(\\sqrt{N})$依赖关系）和经验表现方面都比PUCRL2更出色。最后，我们还提出了两个算法U-PUCRL2和U-PUCRLB，用于环境中扩展不确定性，其中周期未知，但已知一组候选周期。数值结果证明，",
    "tldr": "本文提出了用于解决周期性MDP中强化学习问题的四种算法，其中包括PUCRL2，PUCRLB，U-PUCRL2和U-PUCRLB。PUCRLB表现更好，且其遗憾随周期$N$的变化为$O(\\sqrt{N})$。",
    "en_tdlr": "This paper proposes four algorithms, including PUCRL2, PUCRLB, U-PUCRL2 and U-PUCRLB, for learning in periodic Markov decision processes. PUCRLB performs better and has a regret dependency on the period $N$ of $O(\\sqrt{N})$."
}