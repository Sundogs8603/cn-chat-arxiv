{
    "title": "Your representations are in the network: composable and parallel adaptation for large scale models. (arXiv:2303.04105v2 [cs.LG] UPDATED)",
    "abstract": "We propose InCA, a lightweight method for transfer learning that cross-attends to any activation layer of a pre-trained model. During training, InCA uses a single forward pass to extract multiple activations, which are passed to external cross-attention adapters, trained anew and combined or selected for downstream tasks. We show that, even when selecting a single top-scoring adapter, InCA achieves performance comparable to full fine-tuning, at a cost comparable to fine-tuning just the last layer. For example, with a cross-attention probe 1.3% the size of a pre-trained ViT-L/16 model, we achieve performance within 0.2% of the full fine-tuning paragon at a computational training cost of 51% of the baseline, on average across 11 downstream classification. Unlike other forms of efficient adaptation, InCA does not require backpropagating through the pre-trained model, thus leaving its execution unaltered at both training and inference. The versatility of InCA is best illustrated in fine-gr",
    "link": "http://arxiv.org/abs/2303.04105",
    "context": "Title: Your representations are in the network: composable and parallel adaptation for large scale models. (arXiv:2303.04105v2 [cs.LG] UPDATED)\nAbstract: We propose InCA, a lightweight method for transfer learning that cross-attends to any activation layer of a pre-trained model. During training, InCA uses a single forward pass to extract multiple activations, which are passed to external cross-attention adapters, trained anew and combined or selected for downstream tasks. We show that, even when selecting a single top-scoring adapter, InCA achieves performance comparable to full fine-tuning, at a cost comparable to fine-tuning just the last layer. For example, with a cross-attention probe 1.3% the size of a pre-trained ViT-L/16 model, we achieve performance within 0.2% of the full fine-tuning paragon at a computational training cost of 51% of the baseline, on average across 11 downstream classification. Unlike other forms of efficient adaptation, InCA does not require backpropagating through the pre-trained model, thus leaving its execution unaltered at both training and inference. The versatility of InCA is best illustrated in fine-gr",
    "path": "papers/23/03/2303.04105.json",
    "total_tokens": 952,
    "translated_title": "你的表示在网络中：可组合和并行适应大型模型",
    "translated_abstract": "我们提出了InCA，一种轻量级的迁移学习方法，在预训练模型的任何激活层中进行交互式关注。在训练过程中，InCA使用单次前向传递来提取多个激活，并将其传递给外部交叉注意力适配器进行重新训练和组合或选择用于下游任务。我们表明，即使选择一个最高分的适配器，InCA的性能也可以与全面微调相媲美，而成本仅相当于仅微调最后一层。例如，对于一个与预训练ViT-L/16模型大小相比仅为1.3%的交叉注意力探针，在平均11个下游分类任务中，我们的性能接近全面微调的参照，并且计算训练成本仅为基线的51%。与其他形式的高效适应不同，InCA不需要通过预训练模型进行反向传播，从而在训练和推理过程中保持其执行不变。InCA的多功能性在fine-tuning方面表现最好。",
    "tldr": "InCA是一种轻量级的迁移学习方法，可以在预训练模型的任何激活层进行交互式关注。与其他形式的适应方法相比，InCA的性能接近全面微调，但计算成本仅为基线的51%。"
}