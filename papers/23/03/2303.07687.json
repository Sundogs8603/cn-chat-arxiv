{
    "title": "Dynamic Alignment Mask CTC: Improved Mask-CTC with Aligned Cross Entropy. (arXiv:2303.07687v1 [cs.SD])",
    "abstract": "Because of predicting all the target tokens in parallel, the non-autoregressive models greatly improve the decoding efficiency of speech recognition compared with traditional autoregressive models. In this work, we present dynamic alignment Mask CTC, introducing two methods: (1) Aligned Cross Entropy (AXE), finding the monotonic alignment that minimizes the cross-entropy loss through dynamic programming, (2) Dynamic Rectification, creating new training samples by replacing some masks with model predicted tokens. The AXE ignores the absolute position alignment between prediction and ground truth sentence and focuses on tokens matching in relative order. The dynamic rectification method makes the model capable of simulating the non-mask but possible wrong tokens, even if they have high confidence. Our experiments on WSJ dataset demonstrated that not only AXE loss but also the rectification method could improve the WER performance of Mask CTC.",
    "link": "http://arxiv.org/abs/2303.07687",
    "context": "Title: Dynamic Alignment Mask CTC: Improved Mask-CTC with Aligned Cross Entropy. (arXiv:2303.07687v1 [cs.SD])\nAbstract: Because of predicting all the target tokens in parallel, the non-autoregressive models greatly improve the decoding efficiency of speech recognition compared with traditional autoregressive models. In this work, we present dynamic alignment Mask CTC, introducing two methods: (1) Aligned Cross Entropy (AXE), finding the monotonic alignment that minimizes the cross-entropy loss through dynamic programming, (2) Dynamic Rectification, creating new training samples by replacing some masks with model predicted tokens. The AXE ignores the absolute position alignment between prediction and ground truth sentence and focuses on tokens matching in relative order. The dynamic rectification method makes the model capable of simulating the non-mask but possible wrong tokens, even if they have high confidence. Our experiments on WSJ dataset demonstrated that not only AXE loss but also the rectification method could improve the WER performance of Mask CTC.",
    "path": "papers/23/03/2303.07687.json",
    "total_tokens": 906,
    "translated_title": "动态对齐遮罩CTC: 通过对齐交叉熵进行改进的遮罩CTC",
    "translated_abstract": "与传统的自回归模型相比，非自回归模型由于可以同时预测所有目标令牌，因此极大地提高了语音识别的解码效率。本文提出了动态对齐遮罩CTC，引入了两种方法：（1）对齐的交叉熵（AXE），通过动态规划找到最小化交叉熵损失的单调对齐；（2）动态矫正，通过将一些遮罩替换为模型预测的令牌，创建新的训练样本。AXE忽略了预测和实际句子之间的绝对位置对齐，而是关注以相对顺序匹配的令牌。动态矫正方法使模型能够模拟非遮罩但可能错误的令牌，即使它们具有较高的置信度。我们在WSJ数据集上的实验表明，不仅AXE损失，而且矫正方法都可以提高遮罩CTC的WER性能。",
    "tldr": "本文提出了动态对齐遮罩CTC算法，采用了动态规划的方法来最小化交叉熵损失，同时使用动态矫正方法创建新的训练样本，使得该算法能够提高语音识别性能。",
    "en_tdlr": "This paper proposes a dynamic alignment mask CTC algorithm, which uses dynamic programming to minimize cross-entropy loss and creates new training samples using dynamic rectification method, improving speech recognition performance."
}