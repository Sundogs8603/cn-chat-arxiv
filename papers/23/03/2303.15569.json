{
    "title": "Core-Periphery Principle Guided Redesign of Self-Attention in Transformers. (arXiv:2303.15569v1 [cs.LG])",
    "abstract": "Designing more efficient, reliable, and explainable neural network architectures is critical to studies that are based on artificial intelligence (AI) techniques. Previous studies, by post-hoc analysis, have found that the best-performing ANNs surprisingly resemble biological neural networks (BNN), which indicates that ANNs and BNNs may share some common principles to achieve optimal performance in either machine learning or cognitive/behavior tasks. Inspired by this phenomenon, we proactively instill organizational principles of BNNs to guide the redesign of ANNs. We leverage the Core-Periphery (CP) organization, which is widely found in human brain networks, to guide the information communication mechanism in the self-attention of vision transformer (ViT) and name this novel framework as CP-ViT. In CP-ViT, the attention operation between nodes is defined by a sparse graph with a Core-Periphery structure (CP graph), where the core nodes are redesigned and reorganized to play an integr",
    "link": "http://arxiv.org/abs/2303.15569",
    "context": "Title: Core-Periphery Principle Guided Redesign of Self-Attention in Transformers. (arXiv:2303.15569v1 [cs.LG])\nAbstract: Designing more efficient, reliable, and explainable neural network architectures is critical to studies that are based on artificial intelligence (AI) techniques. Previous studies, by post-hoc analysis, have found that the best-performing ANNs surprisingly resemble biological neural networks (BNN), which indicates that ANNs and BNNs may share some common principles to achieve optimal performance in either machine learning or cognitive/behavior tasks. Inspired by this phenomenon, we proactively instill organizational principles of BNNs to guide the redesign of ANNs. We leverage the Core-Periphery (CP) organization, which is widely found in human brain networks, to guide the information communication mechanism in the self-attention of vision transformer (ViT) and name this novel framework as CP-ViT. In CP-ViT, the attention operation between nodes is defined by a sparse graph with a Core-Periphery structure (CP graph), where the core nodes are redesigned and reorganized to play an integr",
    "path": "papers/23/03/2303.15569.json",
    "total_tokens": 1067,
    "translated_title": "基于核心-外围原则的变形金刚自注意力重构",
    "translated_abstract": "设计更高效、可靠和可解释的神经网络架构对于基于人工智能技术的研究至关重要。之前的研究通过事后分析发现，表现最佳的人工神经网络惊人地类似于生物神经网络，这表明人工神经网络和生物神经网络可能共享某些通用原则，以在机器学习或认知/行为任务中实现最佳性能。基于这一现象，我们积极注入生物神经网络的组织原则来指导人工神经网络的重构。我们利用在人脑网络中广泛存在的核心-外围（CP）结构来引导视觉变压器（ViT）中自我注意的信息传递机制，并将这个新颖的框架命名为CP-ViT。在CP-ViT中，节点之间的注意力操作由一个具有核心-外围结构（CP图）的稀疏图定义，其中核心节点被重新设计和重新组织以在信息处理中发挥综合和核心的作用。我们在三个视觉任务，包括图像分类、目标检测和语义分割方面验证了CP-ViT的有效性，并表明与原始ViT模型相比，在模型效率、精度和可解释性方面，CP-ViT都取得了显著的改进。",
    "tldr": "本文提出了一种新的框架CP-ViT，它利用核心-外围原则重构了变形金刚的自我注意力机制，在图像分类、目标检测和语义分割三个任务上取得了显著性能提升。"
}