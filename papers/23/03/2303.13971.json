{
    "title": "Optimal Transport for Offline Imitation Learning. (arXiv:2303.13971v1 [cs.LG])",
    "abstract": "With the advent of large datasets, offline reinforcement learning (RL) is a promising framework for learning good decision-making policies without the need to interact with the real environment. However, offline RL requires the dataset to be reward-annotated, which presents practical challenges when reward engineering is difficult or when obtaining reward annotations is labor-intensive. In this paper, we introduce Optimal Transport Reward labeling (OTR), an algorithm that assigns rewards to offline trajectories, with a few high-quality demonstrations. OTR's key idea is to use optimal transport to compute an optimal alignment between an unlabeled trajectory in the dataset and an expert demonstration to obtain a similarity measure that can be interpreted as a reward, which can then be used by an offline RL algorithm to learn the policy. OTR is easy to implement and computationally efficient. On D4RL benchmarks, we show that OTR with a single demonstration can consistently match the perfo",
    "link": "http://arxiv.org/abs/2303.13971",
    "context": "Title: Optimal Transport for Offline Imitation Learning. (arXiv:2303.13971v1 [cs.LG])\nAbstract: With the advent of large datasets, offline reinforcement learning (RL) is a promising framework for learning good decision-making policies without the need to interact with the real environment. However, offline RL requires the dataset to be reward-annotated, which presents practical challenges when reward engineering is difficult or when obtaining reward annotations is labor-intensive. In this paper, we introduce Optimal Transport Reward labeling (OTR), an algorithm that assigns rewards to offline trajectories, with a few high-quality demonstrations. OTR's key idea is to use optimal transport to compute an optimal alignment between an unlabeled trajectory in the dataset and an expert demonstration to obtain a similarity measure that can be interpreted as a reward, which can then be used by an offline RL algorithm to learn the policy. OTR is easy to implement and computationally efficient. On D4RL benchmarks, we show that OTR with a single demonstration can consistently match the perfo",
    "path": "papers/23/03/2303.13971.json",
    "total_tokens": 961,
    "translated_title": "离线模仿学习的最优输运方法",
    "translated_abstract": "随着大型数据集的出现，离线强化学习（RL）是学习良好决策策略的有前途的框架，无需与真实环境进行交互。然而，在线下RL中需要数据集进行奖励注释，这在奖励工程困难或获得奖励注释需要大量劳动力时，会带来实际挑战。本文提出了一种名为“最优输运奖励标记（OTR）”的算法，它使用最优输运来计算数据集中未注释的轨迹和专家演示之间的最佳对齐，从而得出一种可以解释为奖励的相似性度量，该度量可以由离线RL算法用于学习策略。OTR易于实现且计算效率高。在D4RL基准测试中，我们展示了OTR只使用一个演示时可以持续匹配性能。",
    "tldr": "本文提出了一种名为“最优输运奖励标记（OTR）”的算法，可用于离线模仿学习中为无标记的轨迹分配奖励，通过使用最优输运计算数据集中未注释的轨迹和专家演示之间的最佳对齐，得出可解释为奖励的相似性度量。在D4RL基准测试中，OTR只使用一个演示时可以持续匹配性能。",
    "en_tdlr": "This paper proposes an algorithm called Optimal Transport Reward labeling (OTR) for assigning rewards to unlabeled trajectories in offline imitation learning, which uses optimal transport to compute the optimal alignment between the unlabeled trajectory and an expert demonstration to obtain a similarity measure that can be interpreted as a reward. Results show that OTR with a single demonstration can consistently match performance on D4RL benchmarks."
}