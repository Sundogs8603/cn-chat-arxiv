{
    "title": "FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU. (arXiv:2303.06865v2 [cs.LG] UPDATED)",
    "abstract": "The high computational and memory requirements of large language model (LLM) inference make it feasible only with multiple high-end accelerators. Motivated by the emerging demand for latency-insensitive tasks with batched processing, this paper initiates the study of high-throughput LLM inference using limited resources, such as a single commodity GPU. We present FlexGen, a high-throughput generation engine for running LLMs with limited GPU memory. FlexGen can be flexibly configured under various hardware resource constraints by aggregating memory and computation from the GPU, CPU, and disk. By solving a linear programming problem, it searches for efficient patterns to store and access tensors. FlexGen further compresses the weights and the attention cache to 4 bits with negligible accuracy loss. These techniques enable FlexGen to have a larger space of batch size choices and thus significantly increase maximum throughput. As a result, when running OPT-175B on a single 16GB GPU, FlexGe",
    "link": "http://arxiv.org/abs/2303.06865",
    "context": "Title: FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU. (arXiv:2303.06865v2 [cs.LG] UPDATED)\nAbstract: The high computational and memory requirements of large language model (LLM) inference make it feasible only with multiple high-end accelerators. Motivated by the emerging demand for latency-insensitive tasks with batched processing, this paper initiates the study of high-throughput LLM inference using limited resources, such as a single commodity GPU. We present FlexGen, a high-throughput generation engine for running LLMs with limited GPU memory. FlexGen can be flexibly configured under various hardware resource constraints by aggregating memory and computation from the GPU, CPU, and disk. By solving a linear programming problem, it searches for efficient patterns to store and access tensors. FlexGen further compresses the weights and the attention cache to 4 bits with negligible accuracy loss. These techniques enable FlexGen to have a larger space of batch size choices and thus significantly increase maximum throughput. As a result, when running OPT-175B on a single 16GB GPU, FlexGe",
    "path": "papers/23/03/2303.06865.json",
    "total_tokens": 1076,
    "translated_title": "单个GPU的高吞吐大语言模型生成推理技术——FlexGen",
    "translated_abstract": "大语言模型(LLM)的高计算和内存需求使其只能在多个高端加速器上实现。本文着眼于对批处理不敏感的任务，并使用有限资源（如单个普通GPU）进行高吞吐量LLM推理的研究。我们提出了FlexGen，一种用于运行具有GPU内存限制的LLMs的高吞吐量生成引擎。通过聚合来自GPU、CPU和磁盘的内存和计算，可以在各种硬件资源约束下灵活配置FlexGen。通过解决线性规划问题，它可以搜索有效的张量存储和访问模式。FlexGen还将权重和注意力缓存压缩为4个位，几乎没有精度损失。这些技术使FlexGen具有更大的批量选择空间，从而显著增加了最大吞吐量。结果，当在单个16GB GPU上运行OPT-175B时，FlexGen的吞吐量为每秒90个序列，比Megatron-LM快4.5倍，比使用单个普通GPU的竞争者快7.5-19倍。",
    "tldr": "本论文提出了一种名为FlexGen的技术，使用单个GPU实现大型语言模型的高吞吐推理。FlexGen通过聚合来自GPU、CPU和磁盘的内存和计算，搜索有效的张量存储和访问模式，并将权重和注意力缓存压缩为4个位。这些技术使FlexGen具有更大的批量选择空间，从而显著增加了最大吞吐量。",
    "en_tdlr": "This paper introduces a technology called FlexGen, which achieves high-throughput inference of large language models using a single GPU. FlexGen aggregates memory and computation from GPU, CPU, and disk, and searches for efficient patterns to store and access tensors. The weights and attention cache are further compressed to 4 bits, enabling FlexGen to have a larger space of batch sizes and significantly increase maximum throughput."
}