{
    "title": "Improving Uncertainty Quantification of Deep Classifiers via Neighborhood Conformal Prediction: Novel Algorithm and Theoretical Analysis. (arXiv:2303.10694v1 [cs.LG])",
    "abstract": "Safe deployment of deep neural networks in high-stake real-world applications requires theoretically sound uncertainty quantification. Conformal prediction (CP) is a principled framework for uncertainty quantification of deep models in the form of prediction set for classification tasks with a user-specified coverage (i.e., true class label is contained with high probability). This paper proposes a novel algorithm referred to as Neighborhood Conformal Prediction (NCP) to improve the efficiency of uncertainty quantification from CP for deep classifiers (i.e., reduce prediction set size). The key idea behind NCP is to use the learned representation of the neural network to identify k nearest-neighbors calibration examples for a given testing input and assign them importance weights proportional to their distance to create adaptive prediction sets. We theoretically show that if the learned data representation of the neural network satisfies some mild conditions, NCP will produce smaller p",
    "link": "http://arxiv.org/abs/2303.10694",
    "context": "Title: Improving Uncertainty Quantification of Deep Classifiers via Neighborhood Conformal Prediction: Novel Algorithm and Theoretical Analysis. (arXiv:2303.10694v1 [cs.LG])\nAbstract: Safe deployment of deep neural networks in high-stake real-world applications requires theoretically sound uncertainty quantification. Conformal prediction (CP) is a principled framework for uncertainty quantification of deep models in the form of prediction set for classification tasks with a user-specified coverage (i.e., true class label is contained with high probability). This paper proposes a novel algorithm referred to as Neighborhood Conformal Prediction (NCP) to improve the efficiency of uncertainty quantification from CP for deep classifiers (i.e., reduce prediction set size). The key idea behind NCP is to use the learned representation of the neural network to identify k nearest-neighbors calibration examples for a given testing input and assign them importance weights proportional to their distance to create adaptive prediction sets. We theoretically show that if the learned data representation of the neural network satisfies some mild conditions, NCP will produce smaller p",
    "path": "papers/23/03/2303.10694.json",
    "total_tokens": 913,
    "translated_title": "通过领域一致性预测改进深度分类器的不确定性量化：新的算法和理论分析",
    "translated_abstract": "在高风险实际应用中安全部署深度神经网络需要有理论基础的确定性量化。对于分类任务，符合性预测（CP）是一种可以以用户指定的覆盖率（即真实类标签包含在高概率内）来确定深度模型的不确定性的原则性框架。本文提出了一种新的算法——领域一致性预测（NCP），以改进由CP进行的深度分类器的确定性量化效率（即减少预测集大小）。NCP的关键思想是使用神经网络学习到的表示来识别给定测试输入的k个最近邻校准示例，并分配与其距离成比例的重要性权重以创建自适应预测集。我们从理论上证明了，如果神经网络学习到的数据表示满足一些温和的条件，NCP将产生更小的预测集。",
    "tldr": "本文提出了一种新算法Neighborhood Conformal Prediction(NCP)，可以使用神经网络学习到的表示提高深度分类器的确定性量化效率；我们从理论上证明了NCP可以产生更小的预测集。",
    "en_tdlr": "The paper proposes a new algorithm, Neighborhood Conformal Prediction (NCP), that uses the learned representation of a neural network to improve the efficiency of uncertainty quantification for deep classifiers. The algorithm creates adaptive prediction sets by identifying k nearest-neighbors calibration examples and assigning them importance weights proportional to their distance. Theoretical analysis shows that NCP produces smaller prediction sets if the learned data representation of the neural network satisfies mild conditions."
}