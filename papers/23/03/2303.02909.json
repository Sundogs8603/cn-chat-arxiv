{
    "title": "Dynamic Prompting: A Unified Framework for Prompt Tuning. (arXiv:2303.02909v2 [cs.CL] UPDATED)",
    "abstract": "It has been demonstrated that the art of prompt tuning is highly effective in efficiently extracting knowledge from pretrained foundation models, encompassing pretrained language models (PLMs), vision pretrained models, and vision-language (V-L) models. However, the efficacy of employing fixed soft prompts with a predetermined position for concatenation with inputs for all instances, irrespective of their inherent disparities, remains uncertain. Variables such as the position, length, and representations of prompts across diverse instances and tasks can substantially influence the performance of prompt tuning. In this context, we provide a theoretical analysis, which reveals that optimizing the position of the prompt to encompass the input can capture additional semantic information that traditional prefix or postfix prompt tuning methods fail to capture. Building upon our analysis, we present a unified dynamic prompt (DP) tuning strategy that dynamically determines different factors o",
    "link": "http://arxiv.org/abs/2303.02909",
    "context": "Title: Dynamic Prompting: A Unified Framework for Prompt Tuning. (arXiv:2303.02909v2 [cs.CL] UPDATED)\nAbstract: It has been demonstrated that the art of prompt tuning is highly effective in efficiently extracting knowledge from pretrained foundation models, encompassing pretrained language models (PLMs), vision pretrained models, and vision-language (V-L) models. However, the efficacy of employing fixed soft prompts with a predetermined position for concatenation with inputs for all instances, irrespective of their inherent disparities, remains uncertain. Variables such as the position, length, and representations of prompts across diverse instances and tasks can substantially influence the performance of prompt tuning. In this context, we provide a theoretical analysis, which reveals that optimizing the position of the prompt to encompass the input can capture additional semantic information that traditional prefix or postfix prompt tuning methods fail to capture. Building upon our analysis, we present a unified dynamic prompt (DP) tuning strategy that dynamically determines different factors o",
    "path": "papers/23/03/2303.02909.json",
    "total_tokens": 805,
    "translated_title": "动态提示：用于提示调整的统一框架",
    "translated_abstract": "已经证明，提示调整技术可以高效地从基础预训练模型中提取知识，包括预训练语言模型（PLMs）、预训练视觉模型和视觉语言模型 (V-L)。然而，采用固定的软提示来与所有实例连接输入，而忽略它们的固有差异，其有效性仍不确定。例如提示的位置、长度和表示在不同实例和任务中的不同变量，可以显著影响提示调整的性能。在此背景下，我们提供了一个理论分析。这个分析发现，优化提示的位置可以捕获传统前缀或后缀提示调整方法无法捕获的额外语义信息。基于我们的分析，我们提出了一个统一的动态提示 (DP) 调整策略，可以动态地确定不同的提示变量，以优化提示调整的性能。",
    "tldr": "本论文提出了一个统一的动态提示（DP）调整策略用于优化提示调整的性能，该策略可以动态地确定不同的提示变量来捕获额外的语义信息。",
    "en_tdlr": "This paper proposes a unified dynamic prompt (DP) tuning strategy to optimize the performance of prompt tuning, which dynamically determines different prompt variables to capture additional semantic information."
}