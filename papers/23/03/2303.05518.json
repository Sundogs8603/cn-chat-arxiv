{
    "title": "Computably Continuous Reinforcement-Learning Objectives are PAC-learnable. (arXiv:2303.05518v2 [cs.LG] UPDATED)",
    "abstract": "In reinforcement learning, the classic objectives of maximizing discounted and finite-horizon cumulative rewards are PAC-learnable: There are algorithms that learn a near-optimal policy with high probability using a finite amount of samples and computation. In recent years, researchers have introduced objectives and corresponding reinforcement-learning algorithms beyond the classic cumulative rewards, such as objectives specified as linear temporal logic formulas. However, questions about the PAC-learnability of these new objectives have remained open.  This work demonstrates the PAC-learnability of general reinforcement-learning objectives through sufficient conditions for PAC-learnability in two analysis settings. In particular, for the analysis that considers only sample complexity, we prove that if an objective given as an oracle is uniformly continuous, then it is PAC-learnable. Further, for the analysis that considers computational complexity, we prove that if an objective is com",
    "link": "http://arxiv.org/abs/2303.05518",
    "context": "Title: Computably Continuous Reinforcement-Learning Objectives are PAC-learnable. (arXiv:2303.05518v2 [cs.LG] UPDATED)\nAbstract: In reinforcement learning, the classic objectives of maximizing discounted and finite-horizon cumulative rewards are PAC-learnable: There are algorithms that learn a near-optimal policy with high probability using a finite amount of samples and computation. In recent years, researchers have introduced objectives and corresponding reinforcement-learning algorithms beyond the classic cumulative rewards, such as objectives specified as linear temporal logic formulas. However, questions about the PAC-learnability of these new objectives have remained open.  This work demonstrates the PAC-learnability of general reinforcement-learning objectives through sufficient conditions for PAC-learnability in two analysis settings. In particular, for the analysis that considers only sample complexity, we prove that if an objective given as an oracle is uniformly continuous, then it is PAC-learnable. Further, for the analysis that considers computational complexity, we prove that if an objective is com",
    "path": "papers/23/03/2303.05518.json",
    "total_tokens": 985,
    "translated_title": "计算连续的强化学习目标是可PAC学习的",
    "translated_abstract": "在强化学习中，最大化折扣和有限时间步长累积奖励的经典目标是可PAC学习的：存在算法使用有限数量的样本和计算，高概率地学习到近似最优策略。近年来，研究人员引入了超越经典累积奖励的新目标和相应的强化学习算法，如用线性时态逻辑公式指定的目标。然而，这些新目标的PAC可学习性仍然存在问题。本论文通过在两种分析设置中充分条件的PAC可学习性，证明了强化学习目标的可PAC学习性。特别地，在仅考虑样本复杂性的分析中，我们证明如果一个目标作为一个oracle是一致连续的，那么它是可PAC学习的。此外，在考虑计算复杂度的分析中，我们证明如果一个目标是计算连续的，并满足“统一连续条件”，那么它是可PAC学习的。我们的结果为超越累积奖励的目标的可学习性提供了见解，并为设计这些目标的高效强化学习算法铺平了道路。",
    "tldr": "本研究证明了计算连续的强化学习目标是可PAC学习的，这对于设计高效的强化学习算法打开了新的突破口。",
    "en_tdlr": "The paper demonstrates the PAC-learnability of computably continuous reinforcement-learning objectives through sufficient conditions for PAC-learnability in two analysis settings, paving the way for efficient reinforcement-learning algorithms for objectives beyond cumulative rewards."
}