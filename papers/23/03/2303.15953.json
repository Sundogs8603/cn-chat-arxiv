{
    "title": "Randomly Initialized Subnetworks with Iterative Weight Recycling. (arXiv:2303.15953v1 [cs.LG])",
    "abstract": "The Multi-Prize Lottery Ticket Hypothesis posits that randomly initialized neural networks contain several subnetworks that achieve comparable accuracy to fully trained models of the same architecture. However, current methods require that the network is sufficiently overparameterized. In this work, we propose a modification to two state-of-the-art algorithms (Edge-Popup and Biprop) that finds high-accuracy subnetworks with no additional storage cost or scaling. The algorithm, Iterative Weight Recycling, identifies subsets of important weights within a randomly initialized network for intra-layer reuse. Empirically we show improvements on smaller network architectures and higher prune rates, finding that model sparsity can be increased through the \"recycling\" of existing weights. In addition to Iterative Weight Recycling, we complement the Multi-Prize Lottery Ticket Hypothesis with a reciprocal finding: high-accuracy, randomly initialized subnetwork's produce diverse masks, despite bei",
    "link": "http://arxiv.org/abs/2303.15953",
    "context": "Title: Randomly Initialized Subnetworks with Iterative Weight Recycling. (arXiv:2303.15953v1 [cs.LG])\nAbstract: The Multi-Prize Lottery Ticket Hypothesis posits that randomly initialized neural networks contain several subnetworks that achieve comparable accuracy to fully trained models of the same architecture. However, current methods require that the network is sufficiently overparameterized. In this work, we propose a modification to two state-of-the-art algorithms (Edge-Popup and Biprop) that finds high-accuracy subnetworks with no additional storage cost or scaling. The algorithm, Iterative Weight Recycling, identifies subsets of important weights within a randomly initialized network for intra-layer reuse. Empirically we show improvements on smaller network architectures and higher prune rates, finding that model sparsity can be increased through the \"recycling\" of existing weights. In addition to Iterative Weight Recycling, we complement the Multi-Prize Lottery Ticket Hypothesis with a reciprocal finding: high-accuracy, randomly initialized subnetwork's produce diverse masks, despite bei",
    "path": "papers/23/03/2303.15953.json",
    "total_tokens": 940,
    "translated_title": "随机初始化的子网络与迭代权重回收",
    "translated_abstract": "多重彩票假设认为，随机初始化的神经网络包含几个子网络，这些子网络的精度与同一架构的已经完全训练的模型相当。然而，目前的方法要求网络具有足够的过参数化。在本文中，我们提出了两种最先进的算法（Edge-Popup和Biprop）的修改版，它可以在没有额外存储成本或缩放的情况下找到高精度的子网络。算法，迭代式权重回收，识别随意初始化网络中的重要权重子集以进行层内重用。我们的实证研究表明，在更小的网络架构和更高的修剪率的情况下，我们可以通过“回收”现有权重来增加模型的稀疏度。除了迭代权重回收，我们还用相反的发现来补充多重彩票假设：高精度的随机初始化子网络产生不同的掩码，尽管它们共享权重。",
    "tldr": "本文提出了修改版的Edge-Popup和Biprop算法，名为迭代权重回收，识别随机初始化网络中的重要权重子集，以进行层内重用，并找到高精度的子网络，从而提高了模型稀疏度，并用相反的发现来补充多重彩票假设。",
    "en_tdlr": "This paper proposes a modification to Edge-Popup and Biprop algorithms called Iterative Weight Recycling, which identifies subsets of important weights within a randomly initialized network for intra-layer reuse, and finds high-accuracy subnetworks to increase model sparsity. It also complements the Multi-Prize Lottery Ticket Hypothesis with the finding that high-accuracy, randomly initialized subnetworks produce diverse masks despite sharing weights."
}