{
    "title": "BERTino: an Italian DistilBERT model. (arXiv:2303.18121v1 [cs.CL])",
    "abstract": "The recent introduction of Transformers language representation models allowed great improvements in many natural language processing (NLP) tasks. However, if on one hand the performances achieved by this kind of architectures are surprising, on the other their usability is limited by the high number of parameters which constitute their network, resulting in high computational and memory demands. In this work we present BERTino, a DistilBERT model which proposes to be the first lightweight alternative to the BERT architecture specific for the Italian language. We evaluated BERTino on the Italian ISDT, Italian ParTUT, Italian WikiNER and multiclass classification tasks, obtaining F1 scores comparable to those obtained by a BERTBASE with a remarkable improvement in training and inference speed.",
    "link": "http://arxiv.org/abs/2303.18121",
    "context": "Title: BERTino: an Italian DistilBERT model. (arXiv:2303.18121v1 [cs.CL])\nAbstract: The recent introduction of Transformers language representation models allowed great improvements in many natural language processing (NLP) tasks. However, if on one hand the performances achieved by this kind of architectures are surprising, on the other their usability is limited by the high number of parameters which constitute their network, resulting in high computational and memory demands. In this work we present BERTino, a DistilBERT model which proposes to be the first lightweight alternative to the BERT architecture specific for the Italian language. We evaluated BERTino on the Italian ISDT, Italian ParTUT, Italian WikiNER and multiclass classification tasks, obtaining F1 scores comparable to those obtained by a BERTBASE with a remarkable improvement in training and inference speed.",
    "path": "papers/23/03/2303.18121.json",
    "total_tokens": 752,
    "translated_title": "BERTino：一种意大利DistilBERT模型",
    "translated_abstract": "最近引入的Transformer语言表示模型在许多自然语言处理任务中取得了很大的改进。然而，这种体系结构的性能虽然惊人，但由于构成其网络的参数过多，导致计算和存储需求高，限制了它们的可用性。本文介绍了BERTino，一种DistilBERT模型，它是用于意大利语的第一个轻量级替代BERT体系结构的选择。我们对BERTino在意大利ISDT、意大利ParTUT、意大利WikiNER和多类分类任务上进行了评估，在训练和推理速度方面获得了显著提高，并获得了与BERTBASE相当的F1分数。",
    "tldr": "本文介绍了BERTino，一种轻量级的DistilBERT模型，是用于意大利语的第一个替代BERT体系结构的选择，其在多项任务中F1分数与BERTBASE相当并显著提高了训练和推理速度。",
    "en_tdlr": "This paper introduces BERTino, a lightweight DistilBERT model and the first alternative to the BERT architecture specific for the Italian language. It achieves comparable F1 scores with BERTBASE on multiple tasks and significantly improves training and inference speed."
}