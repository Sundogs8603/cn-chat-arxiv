{
    "title": "Sublinear Convergence Rates of Extragradient-Type Methods: A Survey on Classical and Recent Developments. (arXiv:2303.17192v1 [math.OC])",
    "abstract": "The extragradient (EG), introduced by G. M. Korpelevich in 1976, is a well-known method to approximate solutions of saddle-point problems and their extensions such as variational inequalities and monotone inclusions. Over the years, numerous variants of EG have been proposed and studied in the literature. Recently, these methods have gained popularity due to new applications in machine learning and robust optimization. In this work, we survey the latest developments in the EG method and its variants for approximating solutions of nonlinear equations and inclusions, with a focus on the monotonicity and co-hypomonotonicity settings. We provide a unified convergence analysis for different classes of algorithms, with an emphasis on sublinear best-iterate and last-iterate convergence rates. We also discuss recent accelerated variants of EG based on both Halpern fixed-point iteration and Nesterov's accelerated techniques. Our approach uses simple arguments and basic mathematical tools to mak",
    "link": "http://arxiv.org/abs/2303.17192",
    "context": "Title: Sublinear Convergence Rates of Extragradient-Type Methods: A Survey on Classical and Recent Developments. (arXiv:2303.17192v1 [math.OC])\nAbstract: The extragradient (EG), introduced by G. M. Korpelevich in 1976, is a well-known method to approximate solutions of saddle-point problems and their extensions such as variational inequalities and monotone inclusions. Over the years, numerous variants of EG have been proposed and studied in the literature. Recently, these methods have gained popularity due to new applications in machine learning and robust optimization. In this work, we survey the latest developments in the EG method and its variants for approximating solutions of nonlinear equations and inclusions, with a focus on the monotonicity and co-hypomonotonicity settings. We provide a unified convergence analysis for different classes of algorithms, with an emphasis on sublinear best-iterate and last-iterate convergence rates. We also discuss recent accelerated variants of EG based on both Halpern fixed-point iteration and Nesterov's accelerated techniques. Our approach uses simple arguments and basic mathematical tools to mak",
    "path": "papers/23/03/2303.17192.json",
    "total_tokens": 875,
    "translated_title": "Extragradient类型方法的次线性收敛速率：对经典和最新进展的调查",
    "translated_abstract": "其中，外推梯度（EG）是G. M. Korpelevich在1976年引入的一种广泛应用于近似解决最小斜率问题和其扩展的方法，如变分不等式和单调包含。多年来，文献中提出并研究了EG的各种变体。最近，由于在机器学习和鲁棒优化中的新应用，这些方法变得越来越流行。本文概述了EG方法及其变种的最新进展，用于近似求解非线性方程和包含，重点关注单调性和协同单调性设置。我们为不同类别的算法提供了统一的收敛分析，重点是次线性最佳迭代和最后迭代的收敛速率。我们还讨论了基于Halpern固定点迭代和Nesterov加速技术的最近加速变体的EG。我们使用简单的论证和基本的数学工具来使我们的方法易于理解和应用。",
    "tldr": "本文调查了外推梯度方法和其变种的最新进展，并提供了次线性最佳迭代和最后迭代的收敛速率。",
    "en_tdlr": "This paper surveys the latest developments in the extragradient method and its variants for approximating solutions of nonlinear equations and inclusions, with a focus on sublinear best-iterate and last-iterate convergence rates."
}