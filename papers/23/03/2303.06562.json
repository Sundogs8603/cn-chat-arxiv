{
    "title": "ContraNorm: A Contrastive Learning Perspective on Oversmoothing and Beyond. (arXiv:2303.06562v2 [cs.LG] UPDATED)",
    "abstract": "Oversmoothing is a common phenomenon in a wide range of Graph Neural Networks (GNNs) and Transformers, where performance worsens as the number of layers increases. Instead of characterizing oversmoothing from the view of complete collapse in which representations converge to a single point, we dive into a more general perspective of dimensional collapse in which representations lie in a narrow cone. Accordingly, inspired by the effectiveness of contrastive learning in preventing dimensional collapse, we propose a novel normalization layer called ContraNorm. Intuitively, ContraNorm implicitly shatters representations in the embedding space, leading to a more uniform distribution and a slighter dimensional collapse. On the theoretical analysis, we prove that ContraNorm can alleviate both complete collapse and dimensional collapse under certain conditions. Our proposed normalization layer can be easily integrated into GNNs and Transformers with negligible parameter overhead. Experiments o",
    "link": "http://arxiv.org/abs/2303.06562",
    "context": "Title: ContraNorm: A Contrastive Learning Perspective on Oversmoothing and Beyond. (arXiv:2303.06562v2 [cs.LG] UPDATED)\nAbstract: Oversmoothing is a common phenomenon in a wide range of Graph Neural Networks (GNNs) and Transformers, where performance worsens as the number of layers increases. Instead of characterizing oversmoothing from the view of complete collapse in which representations converge to a single point, we dive into a more general perspective of dimensional collapse in which representations lie in a narrow cone. Accordingly, inspired by the effectiveness of contrastive learning in preventing dimensional collapse, we propose a novel normalization layer called ContraNorm. Intuitively, ContraNorm implicitly shatters representations in the embedding space, leading to a more uniform distribution and a slighter dimensional collapse. On the theoretical analysis, we prove that ContraNorm can alleviate both complete collapse and dimensional collapse under certain conditions. Our proposed normalization layer can be easily integrated into GNNs and Transformers with negligible parameter overhead. Experiments o",
    "path": "papers/23/03/2303.06562.json",
    "total_tokens": 965,
    "translated_title": "ContraNorm: 对于过度平滑的对比学习视角和更多的研究",
    "translated_abstract": "过度平滑现象在各种图神经网络和变压器中普遍存在，当层数增加时，其性能会变差。我们从维度折叠的一个更一般的视角来描述过度平滑的现象，表示会聚到一个狭窄的锥形空间中，而不是表示会聚到一个点上。受到对抗性学习在防止维度折叠方面的有效性启发，我们提出了一种新的规范化层——ContraNorm。直观上，ContraNorm会在嵌入空间中隐式破坏表示，导致更均匀的分布和轻微的维度折叠。在理论分析中，我们证明了在某些条件下，ContraNorm可以缓解完全塌陷和维度塌陷的情况。我们提出的规范化层可以轻松地集成到GNNs和Transformers中，且参数开销很小。实验结果表明，我们的提议可以提高GNNs和Transformers的精度。",
    "tldr": "本研究提出了一种新的规范化层——ContraNorm，针对图神经网络和变压器中的过度平滑问题，通过对比学习的方式在嵌入空间中破坏表示，缓解了完全塌陷和维度塌陷的现象，并在实验中表现出较高的精度。"
}