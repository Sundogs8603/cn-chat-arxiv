{
    "title": "Is forgetting less a good inductive bias for forward transfer?. (arXiv:2303.08207v1 [cs.LG])",
    "abstract": "One of the main motivations of studying continual learning is that the problem setting allows a model to accrue knowledge from past tasks to learn new tasks more efficiently. However, recent studies suggest that the key metric that continual learning algorithms optimize, reduction in catastrophic forgetting, does not correlate well with the forward transfer of knowledge. We believe that the conclusion previous works reached is due to the way they measure forward transfer. We argue that the measure of forward transfer to a task should not be affected by the restrictions placed on the continual learner in order to preserve knowledge of previous tasks. Instead, forward transfer should be measured by how easy it is to learn a new task given a set of representations produced by continual learning on previous tasks. Under this notion of forward transfer, we evaluate different continual learning algorithms on a variety of image classification benchmarks. Our results indicate that less forgetf",
    "link": "http://arxiv.org/abs/2303.08207",
    "context": "Title: Is forgetting less a good inductive bias for forward transfer?. (arXiv:2303.08207v1 [cs.LG])\nAbstract: One of the main motivations of studying continual learning is that the problem setting allows a model to accrue knowledge from past tasks to learn new tasks more efficiently. However, recent studies suggest that the key metric that continual learning algorithms optimize, reduction in catastrophic forgetting, does not correlate well with the forward transfer of knowledge. We believe that the conclusion previous works reached is due to the way they measure forward transfer. We argue that the measure of forward transfer to a task should not be affected by the restrictions placed on the continual learner in order to preserve knowledge of previous tasks. Instead, forward transfer should be measured by how easy it is to learn a new task given a set of representations produced by continual learning on previous tasks. Under this notion of forward transfer, we evaluate different continual learning algorithms on a variety of image classification benchmarks. Our results indicate that less forgetf",
    "path": "papers/23/03/2303.08207.json",
    "total_tokens": 910,
    "translated_title": "遗忘是否是前向迁移的良好归纳偏差？",
    "translated_abstract": "持续学习的主要动机之一是，该问题设置允许模型从过去的任务中积累知识以更有效地学习新任务。然而，最近的研究表明，持续学习算法所优化的关键指标，即减少灾难性遗忘，并不与前向知识迁移相关。我们认为之前的研究结论是由于他们衡量前向迁移的方式所致。我们认为，衡量一个任务的前向迁移不应受到为保留先前任务知识而对持续学习器施加的限制的影响。相反，前向迁移应该通过持续学习产生的一组表示来评估给定一个新任务有多容易学习。在这种前向迁移概念下，我们评估了不同的持续学习算法在各种图像分类基准测试中的表现。我们的结果表明，较不遗忘的模型具有更好的性能，特别是当训练数据数量少时。",
    "tldr": "本文提出对于持续学习任务来说，遗忘不是一种良好的归纳偏差。之前的研究没有考虑到前向迁移的量度方式，本文提出了一种新的量度方式，发现较不遗忘的模型具有更好的性能。"
}