{
    "title": "Model Stitching: Looking For Functional Similarity Between Representations. (arXiv:2303.11277v2 [cs.LG] UPDATED)",
    "abstract": "Model stitching (Lenc & Vedaldi 2015) is a compelling methodology to compare different neural network representations, because it allows us to measure to what degree they may be interchanged. We expand on a previous work from Bansal, Nakkiran & Barak which used model stitching to compare representations of the same shapes learned by differently seeded and/or trained neural networks of the same architecture. Our contribution enables us to compare the representations learned by layers with different shapes from neural networks with different architectures. We subsequently reveal unexpected behavior of model stitching. Namely, we find that stitching, based on convolutions, for small ResNets, can reach high accuracy if those layers come later in the first (sender) network than in the second (receiver), even if those layers are far apart.",
    "link": "http://arxiv.org/abs/2303.11277",
    "context": "Title: Model Stitching: Looking For Functional Similarity Between Representations. (arXiv:2303.11277v2 [cs.LG] UPDATED)\nAbstract: Model stitching (Lenc & Vedaldi 2015) is a compelling methodology to compare different neural network representations, because it allows us to measure to what degree they may be interchanged. We expand on a previous work from Bansal, Nakkiran & Barak which used model stitching to compare representations of the same shapes learned by differently seeded and/or trained neural networks of the same architecture. Our contribution enables us to compare the representations learned by layers with different shapes from neural networks with different architectures. We subsequently reveal unexpected behavior of model stitching. Namely, we find that stitching, based on convolutions, for small ResNets, can reach high accuracy if those layers come later in the first (sender) network than in the second (receiver), even if those layers are far apart.",
    "path": "papers/23/03/2303.11277.json",
    "total_tokens": 889,
    "translated_title": "模型拼接：寻找表示间的功能相似性",
    "translated_abstract": "模型拼接是一种比较不同神经网络表示的有效方法，因为它允许我们衡量它们可以互换的程度。我们在Bansal, Nakkiran & Barak的先前工作基础上进行了扩展，该工作使用模型拼接来比较不同结构的神经网络学习到的相同形状的表示。我们的贡献使我们能够比较不同结构的神经网络中不同形状层学习到的表示。我们随后揭示了模型拼接的意外行为。具体来说，我们发现基于卷积的拼接对于小ResNets来说，如果这些层在第一个（发送者）网络中出现的位置晚于第二个（接收者）网络中的位置，即使这些层相距很远，也可以达到很高的准确度。",
    "tldr": "本研究扩展了模型拼接方法，使其能够比较不同神经网络架构中不同形状层学习到的表示。我们发现在小ResNets中使用基于卷积的拼接，在第一个网络中较晚出现的层与第二个网络中对应层距离较远的情况下也能达到较高的准确度。",
    "en_tdlr": "This research extends the model stitching method to compare representations learned by layers with different shapes from neural networks with different architectures. The study reveals unexpected behavior of model stitching, showing that for small ResNets, even if the layers appear later in the first network and are far apart from the corresponding layers in the second network, high accuracy can still be achieved using convolution-based stitching."
}