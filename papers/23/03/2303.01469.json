{
    "title": "Consistency Models. (arXiv:2303.01469v2 [cs.LG] UPDATED)",
    "abstract": "Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose consistency models, a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step sampling, achieving the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generatio",
    "link": "http://arxiv.org/abs/2303.01469",
    "context": "Title: Consistency Models. (arXiv:2303.01469v2 [cs.LG] UPDATED)\nAbstract: Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose consistency models, a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step sampling, achieving the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generatio",
    "path": "papers/23/03/2303.01469.json",
    "total_tokens": 868,
    "translated_title": "一种新的生成模型：一步生成且支持零样本编辑——一致性模型",
    "translated_abstract": "扩散模型在图像、音频和视频生成领域有了显著的进展，但它们依赖于一个迭代抽样过程，导致生成速度缓慢。为了克服这个限制，我们提出了一致性模型，这是一族通过直接将噪声映射到数据来生成高质量样本的新模型。它们通过设计支持快速的一步生成，同时仍允许多步抽样来以计算换取样本质量。它们还支持零样本数据编辑，如图像修复、上色和超分辨率，而无需明确训练这些任务。一致性模型可以通过蒸馏预训练的扩散模型来训练，也可以作为独立的生成模型进行训练。",
    "tldr": "提出了一种支持一步生成且支持零样本编辑的生成模型——一致性模型，它们能够通过直接将噪声映射到数据来生成高质量样本，支持快速的一步生成，且仍然支持多步抽样以提高样本质量。",
    "en_tdlr": "Consistency models are proposed as a new family of generative models that support fast one-step generation and zero-shot data editing, achieving high quality samples by directly mapping noise to data, and can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether."
}