{
    "title": "Character, Word, or Both? Revisiting the Segmentation Granularity for Chinese Pre-trained Language Models. (arXiv:2303.10893v2 [cs.CL] UPDATED)",
    "abstract": "Pretrained language models (PLMs) have shown marvelous improvements across various NLP tasks. Most Chinese PLMs simply treat an input text as a sequence of characters, and completely ignore word information. Although Whole Word Masking can alleviate this, the semantics in words is still not well represented. In this paper, we revisit the segmentation granularity of Chinese PLMs. We propose a mixed-granularity Chinese BERT (MigBERT) by considering both characters and words. To achieve this, we design objective functions for learning both character and word-level representations. We conduct extensive experiments on various Chinese NLP tasks to evaluate existing PLMs as well as the proposed MigBERT. Experimental results show that MigBERT achieves new SOTA performance on all these tasks. Further analysis demonstrates that words are semantically richer than characters. More interestingly, we show that MigBERT also works with Japanese. Our code and model have been released here~\\footnote{htt",
    "link": "http://arxiv.org/abs/2303.10893",
    "context": "Title: Character, Word, or Both? Revisiting the Segmentation Granularity for Chinese Pre-trained Language Models. (arXiv:2303.10893v2 [cs.CL] UPDATED)\nAbstract: Pretrained language models (PLMs) have shown marvelous improvements across various NLP tasks. Most Chinese PLMs simply treat an input text as a sequence of characters, and completely ignore word information. Although Whole Word Masking can alleviate this, the semantics in words is still not well represented. In this paper, we revisit the segmentation granularity of Chinese PLMs. We propose a mixed-granularity Chinese BERT (MigBERT) by considering both characters and words. To achieve this, we design objective functions for learning both character and word-level representations. We conduct extensive experiments on various Chinese NLP tasks to evaluate existing PLMs as well as the proposed MigBERT. Experimental results show that MigBERT achieves new SOTA performance on all these tasks. Further analysis demonstrates that words are semantically richer than characters. More interestingly, we show that MigBERT also works with Japanese. Our code and model have been released here~\\footnote{htt",
    "path": "papers/23/03/2303.10893.json",
    "total_tokens": 927,
    "translated_title": "字符，词还是两者兼备？——重访中文预训练语言模型的分词粒度",
    "translated_abstract": "预训练语言模型（PLMs）在各种自然语言处理任务中表现出了惊人的改进。大多数中文PLMs仅将输入文本视为一系列字符，并完全忽略单词信息。尽管整词遮盖可以缓解这个问题，但单词中的语义仍然无法良好地表现。在本文中，我们重新审视了中文PLMs的分词粒度，并提出了一种混合粒度的中文BERT（MigBERT），同时考虑字符和单词。为了实现这一点，我们设计了学习字符和单词级表示的目标函数。我们对各种中文NLP任务进行了广泛的实验，以评估现有的PLMs和所提出的MigBERT。实验结果表明，MigBERT在所有这些任务中均实现了新的SOTA性能。进一步的分析表明，单词在语义上比字符更丰富。更有趣的是，我们展示了MigBERT也可以在日语中工作。",
    "tldr": "该论文提出了一种混合粒度中文BERT（MigBERT），同时考虑字符和单词，以更好地表现单词的语义。实验证明MigBERT在各种中文NLP任务上均可实现新的SOTA性能。",
    "en_tdlr": "This paper proposes a mixed-granularity Chinese BERT, considering both characters and words, to better represent the semantics of words. Experimental results show that MigBERT achieves new SOTA performance on various Chinese NLP tasks."
}