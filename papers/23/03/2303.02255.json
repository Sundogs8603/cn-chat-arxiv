{
    "title": "Finite-Sample Analysis of Learning High-Dimensional Single ReLU Neuron. (arXiv:2303.02255v2 [cs.LG] UPDATED)",
    "abstract": "This paper considers the problem of learning a single ReLU neuron with squared loss (a.k.a., ReLU regression) in the overparameterized regime, where the input dimension can exceed the number of samples. We analyze a Perceptron-type algorithm called GLM-tron (Kakade et al., 2011) and provide its dimension-free risk upper bounds for high-dimensional ReLU regression in both well-specified and misspecified settings. Our risk bounds recover several existing results as special cases. Moreover, in the well-specified setting, we provide an instance-wise matching risk lower bound for GLM-tron. Our upper and lower risk bounds provide a sharp characterization of the high-dimensional ReLU regression problems that can be learned via GLM-tron. On the other hand, we provide some negative results for stochastic gradient descent (SGD) for ReLU regression with symmetric Bernoulli data: if the model is well-specified, the excess risk of SGD is provably no better than that of GLM-tron ignoring constant fa",
    "link": "http://arxiv.org/abs/2303.02255",
    "context": "Title: Finite-Sample Analysis of Learning High-Dimensional Single ReLU Neuron. (arXiv:2303.02255v2 [cs.LG] UPDATED)\nAbstract: This paper considers the problem of learning a single ReLU neuron with squared loss (a.k.a., ReLU regression) in the overparameterized regime, where the input dimension can exceed the number of samples. We analyze a Perceptron-type algorithm called GLM-tron (Kakade et al., 2011) and provide its dimension-free risk upper bounds for high-dimensional ReLU regression in both well-specified and misspecified settings. Our risk bounds recover several existing results as special cases. Moreover, in the well-specified setting, we provide an instance-wise matching risk lower bound for GLM-tron. Our upper and lower risk bounds provide a sharp characterization of the high-dimensional ReLU regression problems that can be learned via GLM-tron. On the other hand, we provide some negative results for stochastic gradient descent (SGD) for ReLU regression with symmetric Bernoulli data: if the model is well-specified, the excess risk of SGD is provably no better than that of GLM-tron ignoring constant fa",
    "path": "papers/23/03/2303.02255.json",
    "total_tokens": 994,
    "translated_title": "高维单个ReLU神经元的有限样本学习分析",
    "translated_abstract": "本文研究了在过参数化的情况下（即输入维度可能超出样本数），学习具有平方损失的单个ReLU神经元的问题。我们分析了称为GLM-tron（Kakade等人，2011）的感知器算法，并提供了其维度无关的风险上界，用于高维ReLU回归的良好规定和规定错误设置。我们的风险上界恢复了几个现有结果作为特例。此外，在良好规定的情况下，我们为GLM-tron提供了一个实例匹配风险下界。我们的上下风险界提供了对可以通过GLM-tron学习的高维ReLU回归问题的清晰刻画。另一方面，我们针对对称伯努利数据的ReLU回归提供了一些随机梯度下降（SGD）的负面结果：如果模型规定良好，则SGD的过多风险可证明不比无视常数因素的GLM-tron的过多风险好。",
    "tldr": "本文研究了高维单个ReLU神经元的有限样本学习问题，并提供了感知器算法GLM-tron的风险上下界，其中包括特殊情况，为高维ReLU回归问题提供了清晰的刻画。此外，对于对称伯努利数据的ReLU回归，随机梯度下降的过多风险不如GLM-tron。",
    "en_tdlr": "This paper analyzes the finite-sample learning of a single ReLU neuron in high dimensions, and provides risk bounds for the Perceptron-type algorithm GLM-tron, which characterizes the high-dimensional ReLU regression problem. Additionally, it presents negative results for stochastic gradient descent in ReLU regression with symmetric Bernoulli data."
}