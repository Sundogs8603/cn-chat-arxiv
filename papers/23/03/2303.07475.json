{
    "title": "General Loss Functions Lead to (Approximate) Interpolation in High Dimensions. (arXiv:2303.07475v1 [stat.ML])",
    "abstract": "We provide a unified framework, applicable to a general family of convex losses and across binary and multiclass settings in the overparameterized regime, to approximately characterize the implicit bias of gradient descent in closed form. Specifically, we show that the implicit bias is approximated (but not exactly equal to) the minimum-norm interpolation in high dimensions, which arises from training on the squared loss. In contrast to prior work which was tailored to exponentially-tailed losses and used the intermediate support-vector-machine formulation, our framework directly builds on the primal-dual analysis of Ji and Telgarsky (2021), allowing us to provide new approximate equivalences for general convex losses through a novel sensitivity analysis. Our framework also recovers existing exact equivalence results for exponentially-tailed losses across binary and multiclass settings. Finally, we provide evidence for the tightness of our techniques, which we use to demonstrate the ef",
    "link": "http://arxiv.org/abs/2303.07475",
    "context": "Title: General Loss Functions Lead to (Approximate) Interpolation in High Dimensions. (arXiv:2303.07475v1 [stat.ML])\nAbstract: We provide a unified framework, applicable to a general family of convex losses and across binary and multiclass settings in the overparameterized regime, to approximately characterize the implicit bias of gradient descent in closed form. Specifically, we show that the implicit bias is approximated (but not exactly equal to) the minimum-norm interpolation in high dimensions, which arises from training on the squared loss. In contrast to prior work which was tailored to exponentially-tailed losses and used the intermediate support-vector-machine formulation, our framework directly builds on the primal-dual analysis of Ji and Telgarsky (2021), allowing us to provide new approximate equivalences for general convex losses through a novel sensitivity analysis. Our framework also recovers existing exact equivalence results for exponentially-tailed losses across binary and multiclass settings. Finally, we provide evidence for the tightness of our techniques, which we use to demonstrate the ef",
    "path": "papers/23/03/2303.07475.json",
    "total_tokens": 886,
    "translated_title": "一般损失函数在高维情况下导致（近似）插值",
    "translated_abstract": "我们提供了一个统一的框架，适用于一般凸损失函数和超参数化阶段的二元和多元分类设置，以近似地表征梯度下降的隐含偏差。具体而言，我们展示了在高维情况下梯度下降的隐含偏差近似于最小范数插值，最小范数插值来自于对平方损失的训练。与之前专门针对指数尾损失并使用中间支持向量机公式的研究不同，我们的框架直接基于Ji和Telgarsky（2021）的原始-对偶分析方法，通过新颖的敏感度分析提供了一般凸损失的新近似等效性结果。我们的框架还恢复了二元和多元分类设置下指数尾损失的现有精确等效结果。最后，我们提供了我们技术的紧密性的证据，我们使用这些证据来演示我们的方法的有效性。",
    "tldr": "本研究提供了一般凸损失函数和超参数化阶段下梯度下降的隐含偏差的近似表征。具体而言是近似最小范数插值。"
}