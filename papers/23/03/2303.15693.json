{
    "title": "Large-scale pretraining on pathological images for fine-tuning of small pathological benchmarks. (arXiv:2303.15693v1 [cs.CV])",
    "abstract": "Pretraining a deep learning model on large image datasets is a standard step before fine-tuning the model on small targeted datasets. The large dataset is usually general images (e.g. imagenet2012) while the small dataset can be specialized datasets that have different distributions from the large dataset. However, this 'large-to-small' strategy is not well-validated when the large dataset is specialized and has a similar distribution to small datasets. We newly compiled three hematoxylin and eosin-stained image datasets, one large (PTCGA200) and two magnification-adjusted small datasets (PCam200 and segPANDA200). Major deep learning models were trained with supervised and self-supervised learning methods and fine-tuned on the small datasets for tumor classification and tissue segmentation benchmarks. ResNet50 pretrained with MoCov2, SimCLR, and BYOL on PTCGA200 was better than imagenet2012 pretraining when fine-tuned on PTCGA200 (accuracy of 83.94%, 86.41%, 84.91%, and 82.72%, respect",
    "link": "http://arxiv.org/abs/2303.15693",
    "context": "Title: Large-scale pretraining on pathological images for fine-tuning of small pathological benchmarks. (arXiv:2303.15693v1 [cs.CV])\nAbstract: Pretraining a deep learning model on large image datasets is a standard step before fine-tuning the model on small targeted datasets. The large dataset is usually general images (e.g. imagenet2012) while the small dataset can be specialized datasets that have different distributions from the large dataset. However, this 'large-to-small' strategy is not well-validated when the large dataset is specialized and has a similar distribution to small datasets. We newly compiled three hematoxylin and eosin-stained image datasets, one large (PTCGA200) and two magnification-adjusted small datasets (PCam200 and segPANDA200). Major deep learning models were trained with supervised and self-supervised learning methods and fine-tuned on the small datasets for tumor classification and tissue segmentation benchmarks. ResNet50 pretrained with MoCov2, SimCLR, and BYOL on PTCGA200 was better than imagenet2012 pretraining when fine-tuned on PTCGA200 (accuracy of 83.94%, 86.41%, 84.91%, and 82.72%, respect",
    "path": "papers/23/03/2303.15693.json",
    "total_tokens": 1186,
    "translated_title": "病理图像大规模预训练用于小规模病理基准微调",
    "translated_abstract": "在对小型目标数据集进行微调之前，在大型图像数据集上预训练深度学习模型是标准步骤。大型数据集通常是通用图像（例如imagenet2012），而小型数据集可以是具有与大型数据集不同分布的专业数据集。然而，当大型数据集是专业化的且具有与小型数据集相似的分布时，这种“大到小”的策略并未得到很好的验证。我们新编译了三个苏木精和伊红染色图像数据集，一个大型数据集（PTCGA200）和两个放大调整的小型数据集（PCam200和segPANDA200）。通过监督学习和自监督学习方法训练了主要的深度学习模型，并在小型数据集上进行了肿瘤分类和组织分割基准的微调。在PTCGA200上以MoCov2，SimCLR和BYOL预训练的ResNet50在微调时比imagenet2012预训练更好（精度分别为83.94％，86.41％，84.91％和82.72％）。此外，以MoCov2预训练的ResNet50在收敛速度更快的情况下实现了PCam200和segPANDA200的最优性能，而比imagenet2012预训练的ResNet50更快。",
    "tldr": "本文探讨了病理图像预训练对小规模病理基准微调的影响，通过自监督学习方法，以PTCGA200为训练集进行预训练的ResNet50在微调时表现更好，优于imagenet2012预训练。MoCov2预训练的ResNet50在PCam200和segPANDA200上表现优秀，且收敛速度更快。",
    "en_tdlr": "This paper investigates the impact of pretraining on pathological images for fine-tuning on small pathological datasets. ResNet50 pretrained with MoCov2 on PTCGA200 performed better than imagenet2012 pretraining when fine-tuned on PTCGA200. MoCov2-pretrained ResNet50 achieved state-of-the-art performance on PCam200 and segPANDA200 while being faster in convergence than imagenet2012-pretrained ResNet50."
}