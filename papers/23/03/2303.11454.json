{
    "title": "How (Implicit) Regularization of ReLU Neural Networks Characterizes the Learned Function -- Part II: the Multi-D Case of Two Layers with Random First Layer. (arXiv:2303.11454v1 [cs.LG])",
    "abstract": "Randomized neural networks (randomized NNs), where only the terminal layer's weights are optimized constitute a powerful model class to reduce computational time in training the neural network model. At the same time, these models generalize surprisingly well in various regression and classification tasks. In this paper, we give an exact macroscopic characterization (i.e., a characterization in function space) of the generalization behavior of randomized, shallow NNs with ReLU activation (RSNs). We show that RSNs correspond to a generalized additive model (GAM)-typed regression in which infinitely many directions are considered: the infinite generalized additive model (IGAM). The IGAM is formalized as solution to an optimization problem in function space for a specific regularization functional and a fairly general loss. This work is an extension to multivariate NNs of prior work, where we showed how wide RSNs with ReLU activation behave like spline regression under certain conditions ",
    "link": "http://arxiv.org/abs/2303.11454",
    "context": "Title: How (Implicit) Regularization of ReLU Neural Networks Characterizes the Learned Function -- Part II: the Multi-D Case of Two Layers with Random First Layer. (arXiv:2303.11454v1 [cs.LG])\nAbstract: Randomized neural networks (randomized NNs), where only the terminal layer's weights are optimized constitute a powerful model class to reduce computational time in training the neural network model. At the same time, these models generalize surprisingly well in various regression and classification tasks. In this paper, we give an exact macroscopic characterization (i.e., a characterization in function space) of the generalization behavior of randomized, shallow NNs with ReLU activation (RSNs). We show that RSNs correspond to a generalized additive model (GAM)-typed regression in which infinitely many directions are considered: the infinite generalized additive model (IGAM). The IGAM is formalized as solution to an optimization problem in function space for a specific regularization functional and a fairly general loss. This work is an extension to multivariate NNs of prior work, where we showed how wide RSNs with ReLU activation behave like spline regression under certain conditions ",
    "path": "papers/23/03/2303.11454.json",
    "total_tokens": 883,
    "translated_title": "如何描述ReLU神经网络的隐式正则化特性——第二部分：具有随机第一层的两层多维情况",
    "translated_abstract": "随机化神经网络是一种强大的模型，其中仅优化了最终层的权重，可降低神经网络模型的计算时间。同时，这些模型在各种回归和分类任务中的泛化能力惊人。本文对于具有ReLU激活的随机、浅层神经网络提出了一个宏观精确的特征描述，即广义加性模型（GAM）类型的回归问题，其中考虑了无限多个方向：无限广义加性模型（IGAM）。 IGAM被形式化为函数空间中特定正则化泛函和相当一般的损失函数的优化问题的解。本文是在先前研究的基础上对多元神经网络进行的扩展，我们在先前研究中展示了在某些条件下具有ReLU激活的宽式RSNs的行为类似于样条回归。",
    "tldr": "本文提出了具有ReLU激活的随机浅层神经网络的特征描述，对于回归问题，它们类似于无限广义加性模型（IGAM）",
    "en_tdlr": "This paper provides a macroscopic characterization of the generalization behavior of randomized, shallow neural networks with ReLU activation and shows that they correspond to an infinite generalized additive model (IGAM) for regression tasks."
}