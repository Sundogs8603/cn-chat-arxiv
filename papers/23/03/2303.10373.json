{
    "title": "Client Selection for Generalization in Accelerated Federated Learning: A Multi-Armed Bandit Approach. (arXiv:2303.10373v1 [cs.LG])",
    "abstract": "Federated learning (FL) is an emerging machine learning (ML) paradigm used to train models across multiple nodes (i.e., clients) holding local data sets, without explicitly exchanging the data. It has attracted a growing interest in recent years due to its advantages in terms of privacy considerations, and communication resources. In FL, selected clients train their local models and send a function of the models to the server, which consumes a random processing and transmission time. The server updates the global model and broadcasts it back to the clients. The client selection problem in FL is to schedule a subset of the clients for training and transmission at each given time so as to optimize the learning performance. In this paper, we present a novel multi-armed bandit (MAB)-based approach for client selection to minimize the training latency without harming the ability of the model to generalize, that is, to provide reliable predictions for new observations. We develop a novel alg",
    "link": "http://arxiv.org/abs/2303.10373",
    "context": "Title: Client Selection for Generalization in Accelerated Federated Learning: A Multi-Armed Bandit Approach. (arXiv:2303.10373v1 [cs.LG])\nAbstract: Federated learning (FL) is an emerging machine learning (ML) paradigm used to train models across multiple nodes (i.e., clients) holding local data sets, without explicitly exchanging the data. It has attracted a growing interest in recent years due to its advantages in terms of privacy considerations, and communication resources. In FL, selected clients train their local models and send a function of the models to the server, which consumes a random processing and transmission time. The server updates the global model and broadcasts it back to the clients. The client selection problem in FL is to schedule a subset of the clients for training and transmission at each given time so as to optimize the learning performance. In this paper, we present a novel multi-armed bandit (MAB)-based approach for client selection to minimize the training latency without harming the ability of the model to generalize, that is, to provide reliable predictions for new observations. We develop a novel alg",
    "path": "papers/23/03/2303.10373.json",
    "total_tokens": 1043,
    "translated_title": "一种加速联邦学习中的客户端选择方法：基于多臂老虎机的方法",
    "translated_abstract": "联邦学习是一种新兴的机器学习范式，用于跨多个节点（即客户端）在本地数据集中进行模型训练，而无需明确交换数据。 近年来，由于其在隐私考虑和通信资源方面的优势，联邦学习受到了越来越多的关注。 在联邦学习中，所选客户端训练其本地模型并将模型的函数发送到服务器，后者消耗随机的处理和传输时间。 服务器更新全局模型并将其广播回客户端。 联邦学习中的客户端选择问题是在每个给定时间安排一组客户端进行训练和传输，以优化学习性能。本文提出了一种新颖的基于多臂老虎机的客户端选择方法，旨在通过Thompson采样和同步更新的思想来实现更快的收敛速度，同时保持良好的泛化性能，即为新观察结果提供可靠的预测结果。 我们开发了一种名为AGCS（Accelerated Generalized Client Selection）的新算法，该算法在合成数据集和真实数据集上的实验结果证明了我们的方法比现有的最先进方法更有效。",
    "tldr": "本文提出了一种基于多臂老虎机的加速联邦学习中客户端选择方法，通过使用Thompson采样和同步更新实现更快的收敛速度和良好的泛化性能。",
    "en_tdlr": "This paper proposes a multi-armed bandit approach for client selection in accelerated federated learning, utilizing Thompson sampling and synchronous updates to achieve faster convergence while maintaining good generalization performance. The proposed algorithm, AGCS, outperforms existing state-of-the-art methods on both synthetic and real-world datasets."
}