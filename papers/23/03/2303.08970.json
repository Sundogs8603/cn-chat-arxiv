{
    "title": "Gated Compression Layers for Efficient Always-On Models. (arXiv:2303.08970v1 [cs.LG])",
    "abstract": "Mobile and embedded machine learning developers frequently have to compromise between two inferior on-device deployment strategies: sacrifice accuracy and aggressively shrink their models to run on dedicated low-power cores; or sacrifice battery by running larger models on more powerful compute cores such as neural processing units or the main application processor. In this paper, we propose a novel Gated Compression layer that can be applied to transform existing neural network architectures into Gated Neural Networks. Gated Neural Networks have multiple properties that excel for on-device use cases that help significantly reduce power, boost accuracy, and take advantage of heterogeneous compute cores. We provide results across five public image and audio datasets that demonstrate the proposed Gated Compression layer effectively stops up to 96% of negative samples, compresses 97% of positive samples, while maintaining or improving model accuracy.",
    "link": "http://arxiv.org/abs/2303.08970",
    "context": "Title: Gated Compression Layers for Efficient Always-On Models. (arXiv:2303.08970v1 [cs.LG])\nAbstract: Mobile and embedded machine learning developers frequently have to compromise between two inferior on-device deployment strategies: sacrifice accuracy and aggressively shrink their models to run on dedicated low-power cores; or sacrifice battery by running larger models on more powerful compute cores such as neural processing units or the main application processor. In this paper, we propose a novel Gated Compression layer that can be applied to transform existing neural network architectures into Gated Neural Networks. Gated Neural Networks have multiple properties that excel for on-device use cases that help significantly reduce power, boost accuracy, and take advantage of heterogeneous compute cores. We provide results across five public image and audio datasets that demonstrate the proposed Gated Compression layer effectively stops up to 96% of negative samples, compresses 97% of positive samples, while maintaining or improving model accuracy.",
    "path": "papers/23/03/2303.08970.json",
    "total_tokens": 887,
    "translated_title": "用于高效的持续模型的门控压缩层",
    "translated_abstract": "移动和嵌入式机器学习开发人员经常需要在牺牲精度和大幅度压缩模型以运行在专用低功耗核心、或在更强大的计算核心（如神经处理单元或主应用程序处理器）上运行较大模型之间进行妥协。在本文中，我们提出了一种新颖的门控压缩层，可以将现有神经网络架构转化为门控神经网络。门控神经网络具有多个用于设备上场景的优点，可大大降低功耗、提高准确性并利用异构计算核心。我们在五个公共图像和音频数据集上提供了结果，证明所提出的门控压缩层可以有效地阻止高达96％的负样本，压缩97％的正样本，同时保持或提高模型准确性。",
    "tldr": "本文提出了一种Gated Compression层，可将现有的神经网络架构转化为Gated Neural Networks，可大幅度降低功耗、提高准确性并利用异构计算核心。实验结果在五个公共数据集上证明所提出的方法有效地压缩了正样本，同时保持或提高了模型准确性。",
    "en_tdlr": "This paper proposes a Gated Compression layer that can transform existing neural network architectures into Gated Neural Networks for on-device use cases. The layer significantly reduces power consumption, boosts accuracy, and takes advantage of heterogeneous compute cores. Experimental results across five public datasets demonstrate the effectiveness of the proposed method in compressing positive samples while maintaining or improving model accuracy."
}