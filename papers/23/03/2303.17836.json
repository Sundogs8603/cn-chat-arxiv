{
    "title": "Rethinking interpretation: Input-agnostic saliency mapping of deep visual classifiers. (arXiv:2303.17836v1 [cs.CV])",
    "abstract": "Saliency methods provide post-hoc model interpretation by attributing input features to the model outputs. Current methods mainly achieve this using a single input sample, thereby failing to answer input-independent inquiries about the model. We also show that input-specific saliency mapping is intrinsically susceptible to misleading feature attribution. Current attempts to use 'general' input features for model interpretation assume access to a dataset containing those features, which biases the interpretation. Addressing the gap, we introduce a new perspective of input-agnostic saliency mapping that computationally estimates the high-level features attributed by the model to its outputs. These features are geometrically correlated, and are computed by accumulating model's gradient information with respect to an unrestricted data distribution. To compute these features, we nudge independent data points over the model loss surface towards the local minima associated by a human-understa",
    "link": "http://arxiv.org/abs/2303.17836",
    "context": "Title: Rethinking interpretation: Input-agnostic saliency mapping of deep visual classifiers. (arXiv:2303.17836v1 [cs.CV])\nAbstract: Saliency methods provide post-hoc model interpretation by attributing input features to the model outputs. Current methods mainly achieve this using a single input sample, thereby failing to answer input-independent inquiries about the model. We also show that input-specific saliency mapping is intrinsically susceptible to misleading feature attribution. Current attempts to use 'general' input features for model interpretation assume access to a dataset containing those features, which biases the interpretation. Addressing the gap, we introduce a new perspective of input-agnostic saliency mapping that computationally estimates the high-level features attributed by the model to its outputs. These features are geometrically correlated, and are computed by accumulating model's gradient information with respect to an unrestricted data distribution. To compute these features, we nudge independent data points over the model loss surface towards the local minima associated by a human-understa",
    "path": "papers/23/03/2303.17836.json",
    "total_tokens": 1030,
    "translated_title": "重新思考解释：深度视觉分类器的无特定输入显著性映射",
    "translated_abstract": "显著性方法通过将输入特征归属于模型输出，提供事后的模型解释。 当前的方法主要使用单个输入样本来实现这一点，因此无法回答有关模型的独立于输入的查询。 我们还表明，特定于输入的显著性映射本质上容易受到误导性特征归属的影响。试图使用“通用”输入特征来进行模型解释的现有尝试假定可以访问包含这些特征的数据集，这会导致解释的偏差。针对这一差距，我们提出了一种新的无特定输入显著性映射视角，该方法计算了模型对其输出所归属的高级特征。 这些特征是几何相关的，并通过积累模型相对于无限制数据分布的梯度信息来计算。 为了计算这些特征，我们将独立的数据点沿着人类可理解标签相关联的局部最小值推向模型损失面。 所提出的方法提供了一种新的解释深度分类器的方式，不依赖于特定输入信息，并且经过检验，在输入变化、噪声和对抗性攻击方面都很鲁棒。",
    "tldr": "提出了一种新的无特定输入显著性映射视角，它计算了模型对其输出所归属的高级特征，这种方法能够独立于输入进行模型解释，且鲁棒性较好。",
    "en_tdlr": "A new perspective of input-agnostic saliency mapping is introduced to calculate the high-level features attributed by the model to its outputs, providing a way to interpret deep classifiers that does not rely on input-specific information and is shown to be robust to input variations, noise, and adversarial attacks."
}