{
    "title": "A Unified Framework of Policy Learning for Contextual Bandit with Confounding Bias and Missing Observations. (arXiv:2303.11187v1 [cs.LG])",
    "abstract": "We study the offline contextual bandit problem, where we aim to acquire an optimal policy using observational data. However, this data usually contains two deficiencies: (i) some variables that confound actions are not observed, and (ii) missing observations exist in the collected data. Unobserved confounders lead to a confounding bias and missing observations cause bias and inefficiency problems. To overcome these challenges and learn the optimal policy from the observed dataset, we present a new algorithm called Causal-Adjusted Pessimistic (CAP) policy learning, which forms the reward function as the solution of an integral equation system, builds a confidence set, and greedily takes action with pessimism. With mild assumptions on the data, we develop an upper bound to the suboptimality of CAP for the offline contextual bandit problem.",
    "link": "http://arxiv.org/abs/2303.11187",
    "context": "Title: A Unified Framework of Policy Learning for Contextual Bandit with Confounding Bias and Missing Observations. (arXiv:2303.11187v1 [cs.LG])\nAbstract: We study the offline contextual bandit problem, where we aim to acquire an optimal policy using observational data. However, this data usually contains two deficiencies: (i) some variables that confound actions are not observed, and (ii) missing observations exist in the collected data. Unobserved confounders lead to a confounding bias and missing observations cause bias and inefficiency problems. To overcome these challenges and learn the optimal policy from the observed dataset, we present a new algorithm called Causal-Adjusted Pessimistic (CAP) policy learning, which forms the reward function as the solution of an integral equation system, builds a confidence set, and greedily takes action with pessimism. With mild assumptions on the data, we develop an upper bound to the suboptimality of CAP for the offline contextual bandit problem.",
    "path": "papers/23/03/2303.11187.json",
    "total_tokens": 955,
    "translated_title": "具有混杂偏差和缺失观测的情境赌博机策略学习的统一框架",
    "translated_abstract": "本文研究离线情境赌博机问题，旨在使用观察数据获得最优策略。但是，这些数据通常存在两个不足：(i)一些混淆行动的变量未被观察到，(ii)采集到的数据中存在缺失观测。未被观察到的混淆因素导致混淆偏差，缺失观测则导致偏差和低效问题。为了克服这些挑战，并从观察到的数据集中学习最优策略，我们提出了一种称为因果调整悲观（CAP）策略学习的新算法，它将奖励函数构建为积分方程系统的解，构建置信区间，并带有悲观主义地采取行动。在数据的温和假设下，我们为离线情境赌博机问题发展了CAP亚优解的上界。",
    "tldr": "本文提出了一个名为“因果调整悲观（CAP）策略学习”的算法，旨在克服离线情境赌博机问题中存在的混淆偏差和缺失观测问题，该算法通过构建奖励函数的积分方程系统的解并建立置信区间来达到目的，并带有悲观主义地采取行动。"
}