{
    "title": "Tubelet-Contrastive Self-Supervision for Video-Efficient Generalization. (arXiv:2303.11003v2 [cs.CV] UPDATED)",
    "abstract": "We propose a self-supervised method for learning motion-focused video representations. Existing approaches minimize distances between temporally augmented videos, which maintain high spatial similarity. We instead propose to learn similarities between videos with identical local motion dynamics but an otherwise different appearance. We do so by adding synthetic motion trajectories to videos which we refer to as tubelets. By simulating different tubelet motions and applying transformations, such as scaling and rotation, we introduce motion patterns beyond what is present in the pretraining data. This allows us to learn a video representation that is remarkably data efficient: our approach maintains performance when using only 25\\% of the pretraining videos. Experiments on 10 diverse downstream settings demonstrate our competitive performance and generalizability to new domains and fine-grained actions.",
    "link": "http://arxiv.org/abs/2303.11003",
    "context": "Title: Tubelet-Contrastive Self-Supervision for Video-Efficient Generalization. (arXiv:2303.11003v2 [cs.CV] UPDATED)\nAbstract: We propose a self-supervised method for learning motion-focused video representations. Existing approaches minimize distances between temporally augmented videos, which maintain high spatial similarity. We instead propose to learn similarities between videos with identical local motion dynamics but an otherwise different appearance. We do so by adding synthetic motion trajectories to videos which we refer to as tubelets. By simulating different tubelet motions and applying transformations, such as scaling and rotation, we introduce motion patterns beyond what is present in the pretraining data. This allows us to learn a video representation that is remarkably data efficient: our approach maintains performance when using only 25\\% of the pretraining videos. Experiments on 10 diverse downstream settings demonstrate our competitive performance and generalizability to new domains and fine-grained actions.",
    "path": "papers/23/03/2303.11003.json",
    "total_tokens": 898,
    "translated_title": "Tubelet-对比自监督方法用于视频高效泛化",
    "translated_abstract": "我们提出了一种自监督方法来学习以运动为重点的视频表示。现有方法通过最小化时间增强的视频之间的距离来保持高空间相似性。相反，我们提出通过在局部运动动态相同但外观不同的视频之间学习相似性来实现。我们在视频中添加了合成的运动轨迹，称之为tubelets，通过模拟不同的tubelet运动并应用缩放和旋转等变换，引入了超出预训练数据中存在的运动模式。这使我们能够学习出一种数据效率显著的视频表示：我们的方法在使用仅25％的预训练视频时仍能保持性能。在10个不同的下游设置上的实验证明了我们竞争力强的性能和对新领域和细粒度动作的泛化能力。",
    "tldr": "该论文提出了Tubelet-对比自监督方法，通过在局部运动动态相同但外观不同的视频之间学习相似性，引入超出预训练数据中存在的运动模式来学习高效的视频表示。实验证明该方法在使用仅25％的预训练视频时仍能保持性能，并在多个下游任务中具有竞争力和一定的泛化能力。",
    "en_tdlr": "This paper proposes the Tubelet-Contrastive self-supervised method for learning efficient video representations by learning similarities between videos with identical local motion dynamics but different appearances and introducing motion patterns beyond the pretraining data. The experiments show that the method maintains performance with only 25% of the pretraining videos and has competitive performance and generalizability in various downstream tasks."
}