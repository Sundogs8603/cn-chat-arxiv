{
    "title": "Fast Rates for Maximum Entropy Exploration. (arXiv:2303.08059v2 [stat.ML] UPDATED)",
    "abstract": "We address the challenge of exploration in reinforcement learning (RL) when the agent operates in an unknown environment with sparse or no rewards. In this work, we study the maximum entropy exploration problem of two different types. The first type is visitation entropy maximization previously considered by Hazan et al.(2019) in the discounted setting. For this type of exploration, we propose a game-theoretic algorithm that has $\\widetilde{\\mathcal{O}}(H^3S^2A/\\varepsilon^2)$ sample complexity thus improving the $\\varepsilon$-dependence upon existing results, where $S$ is a number of states, $A$ is a number of actions, $H$ is an episode length, and $\\varepsilon$ is a desired accuracy. The second type of entropy we study is the trajectory entropy. This objective function is closely related to the entropy-regularized MDPs, and we propose a simple algorithm that has a sample complexity of order $\\widetilde{\\mathcal{O}}(\\mathrm{poly}(S,A,H)/\\varepsilon)$. Interestingly, it is the first th",
    "link": "http://arxiv.org/abs/2303.08059",
    "context": "Title: Fast Rates for Maximum Entropy Exploration. (arXiv:2303.08059v2 [stat.ML] UPDATED)\nAbstract: We address the challenge of exploration in reinforcement learning (RL) when the agent operates in an unknown environment with sparse or no rewards. In this work, we study the maximum entropy exploration problem of two different types. The first type is visitation entropy maximization previously considered by Hazan et al.(2019) in the discounted setting. For this type of exploration, we propose a game-theoretic algorithm that has $\\widetilde{\\mathcal{O}}(H^3S^2A/\\varepsilon^2)$ sample complexity thus improving the $\\varepsilon$-dependence upon existing results, where $S$ is a number of states, $A$ is a number of actions, $H$ is an episode length, and $\\varepsilon$ is a desired accuracy. The second type of entropy we study is the trajectory entropy. This objective function is closely related to the entropy-regularized MDPs, and we propose a simple algorithm that has a sample complexity of order $\\widetilde{\\mathcal{O}}(\\mathrm{poly}(S,A,H)/\\varepsilon)$. Interestingly, it is the first th",
    "path": "papers/23/03/2303.08059.json",
    "total_tokens": 966,
    "translated_title": "快速率的最大熵探索方法",
    "translated_abstract": "当智能体在一个未知的、稀疏或没有奖励的环境中操作时，我们解决了强化学习（RL）中探索的挑战。在本文中，我们研究了两种不同类型的最大熵探索问题。第一种类型是回访熵最大化，这在折扣设置中已经由Hazan et al.（2019）考虑过。对于这种类型的探索，我们提出了一种博弈论算法，其样本复杂性为$\\widetilde{\\mathcal{O}}(H^3S^2A/\\varepsilon^2)$，从而改进了现有结果的$\\varepsilon$依赖关系，其中$S$是状态数，$A$是行动数，$H$是每个回合的长度，$\\varepsilon$是期望的精度。我们研究的第二种熵是轨迹熵。这个目标函数与熵正则化MDPs密切相关，我们提出了一个简单的算法，其样本复杂度为$\\widetilde{\\mathcal{O}}(\\mathrm{poly}(S,A,H)/\\varepsilon)$。有趣的是，这是第一个在具有$\\mathrm{poly}(S,A,H)$样本复杂度的情况下解决轨迹熵最大化问题的算法。",
    "tldr": "本文解决了强化学习中稀疏奖励下的最大熵探索问题，提出了两种类型的最大熵探索方法，并提高了其样本复杂度。"
}