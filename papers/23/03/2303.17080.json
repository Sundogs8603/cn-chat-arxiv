{
    "title": "Mole Recruitment: Poisoning of Image Classifiers via Selective Batch Sampling. (arXiv:2303.17080v1 [cs.LG])",
    "abstract": "In this work, we present a data poisoning attack that confounds machine learning models without any manipulation of the image or label. This is achieved by simply leveraging the most confounding natural samples found within the training data itself, in a new form of a targeted attack coined \"Mole Recruitment.\" We define moles as the training samples of a class that appear most similar to samples of another class, and show that simply restructuring training batches with an optimal number of moles can lead to significant degradation in the performance of the targeted class. We show the efficacy of this novel attack in an offline setting across several standard image classification datasets, and demonstrate the real-world viability of this attack in a continual learning (CL) setting. Our analysis reveals that state-of-the-art models are susceptible to Mole Recruitment, thereby exposing a previously undetected vulnerability of image classifiers.",
    "link": "http://arxiv.org/abs/2303.17080",
    "context": "Title: Mole Recruitment: Poisoning of Image Classifiers via Selective Batch Sampling. (arXiv:2303.17080v1 [cs.LG])\nAbstract: In this work, we present a data poisoning attack that confounds machine learning models without any manipulation of the image or label. This is achieved by simply leveraging the most confounding natural samples found within the training data itself, in a new form of a targeted attack coined \"Mole Recruitment.\" We define moles as the training samples of a class that appear most similar to samples of another class, and show that simply restructuring training batches with an optimal number of moles can lead to significant degradation in the performance of the targeted class. We show the efficacy of this novel attack in an offline setting across several standard image classification datasets, and demonstrate the real-world viability of this attack in a continual learning (CL) setting. Our analysis reveals that state-of-the-art models are susceptible to Mole Recruitment, thereby exposing a previously undetected vulnerability of image classifiers.",
    "path": "papers/23/03/2303.17080.json",
    "total_tokens": 903,
    "translated_title": "“选择性批量采样中的毒瘤招募：一种混淆机器学习模型的数据攻击”",
    "translated_abstract": "本文提出了一种数据毒瘤攻击，该攻击不需要对图像或标签进行任何操作就可以混淆机器学习模型。这是通过仅仅利用训练数据中最混淆的自然样本来实现的，这是一种新的有针对性的攻击形式，称为“毒瘤招募”。我们将毒瘤定义为一个类的训练样本，其与另一个类的样本最相似，并展示了通过优化毒瘤数量结构化训练批次，可以显著降低目标类的性能。我们在离线设置下展示了这种新型攻击的有效性，并在连续学习 (CL) 设置下展示了这种攻击的实际可行性。我们的分析揭示了最先进的模型容易受到“毒瘤招募”的影响，从而暴露了图像分类器以前未被发现的漏洞。",
    "tldr": "本文提出了一种名为“毒瘤招募”的数据攻击，通过选择性批量采样来混淆机器学习模型，即通过优化结构化训练批次的毒瘤数量来降低目标类的性能。",
    "en_tdlr": "This paper proposes a data poisoning attack called \"Mole Recruitment\" that confounds machine learning models by selectively sampling the most confounding natural samples found within the training data itself, and restructuring training batches with an optimal number of moles to significantly degrade the performance of the targeted class."
}