{
    "title": "Mixed Autoencoder for Self-supervised Visual Representation Learning. (arXiv:2303.17152v1 [cs.CV])",
    "abstract": "Masked Autoencoder (MAE) has demonstrated superior performance on various vision tasks via randomly masking image patches and reconstruction. However, effective data augmentation strategies for MAE still remain open questions, different from those in contrastive learning that serve as the most important part. This paper studies the prevailing mixing augmentation for MAE. We first demonstrate that naive mixing will in contrast degenerate model performance due to the increase of mutual information (MI). To address, we propose homologous recognition, an auxiliary pretext task, not only to alleviate the MI increasement by explicitly requiring each patch to recognize homologous patches, but also to perform object-aware self-supervised pre-training for better downstream dense perception performance. With extensive experiments, we demonstrate that our proposed Mixed Autoencoder (MixedAE) achieves the state-of-the-art transfer results among masked image modeling (MIM) augmentations on differen",
    "link": "http://arxiv.org/abs/2303.17152",
    "context": "Title: Mixed Autoencoder for Self-supervised Visual Representation Learning. (arXiv:2303.17152v1 [cs.CV])\nAbstract: Masked Autoencoder (MAE) has demonstrated superior performance on various vision tasks via randomly masking image patches and reconstruction. However, effective data augmentation strategies for MAE still remain open questions, different from those in contrastive learning that serve as the most important part. This paper studies the prevailing mixing augmentation for MAE. We first demonstrate that naive mixing will in contrast degenerate model performance due to the increase of mutual information (MI). To address, we propose homologous recognition, an auxiliary pretext task, not only to alleviate the MI increasement by explicitly requiring each patch to recognize homologous patches, but also to perform object-aware self-supervised pre-training for better downstream dense perception performance. With extensive experiments, we demonstrate that our proposed Mixed Autoencoder (MixedAE) achieves the state-of-the-art transfer results among masked image modeling (MIM) augmentations on differen",
    "path": "papers/23/03/2303.17152.json",
    "total_tokens": 938,
    "translated_title": "混合自编码器用于自监督视觉表示学习",
    "translated_abstract": "掩码自编码器（MAE）通过随机遮盖图像补丁和重建在各种视觉任务上展现出卓越的性能。然而，MAE的有效数据增强策略仍然是未解决的问题，不同于对比学习中的策略。本文研究了用于MAE的普遍混合增强。我们首先证明了朴素混合将由于相互信息的增加而降低模型性能。为了解决这个问题，我们提出了同源识别方法，一种辅助的预文本任务，不仅通过明确要求每个补丁识别同源补丁来缓解相互信息的增加，而且还可以执行面向对象的自监督预训练以获得更好的下游密集感知性能。通过大量的实验证明，我们的混合自编码器（MixedAE）在不同的遮蔽图像建模（MIM）增强中实现了最先进的转移结果。",
    "tldr": "本文提出混合自编码器（MixedAE）用于自监督视觉表示学习，在MAE构架下通过同源识别等辅助预文本任务，解决了数据增强下相互信息增加导致性能下降的问题，并取得了遮蔽图像建模（MIM）增强中最先进的转移结果。",
    "en_tdlr": "This paper proposes a Mixed Autoencoder (MixedAE) for self-supervised visual representation learning. By using homologous recognition as an auxiliary pretext task, MixedAE solves the problem of mutual information increase leading to performance degradation under data augmentation in the MAE framework, and achieves state-of-the-art transfer results among masked image modeling (MIM) augmentations."
}