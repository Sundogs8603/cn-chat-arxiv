{
    "title": "Distortion-Disentangled Contrastive Learning. (arXiv:2303.05066v2 [cs.CV] UPDATED)",
    "abstract": "Self-supervised learning is well known for its remarkable performance in representation learning and various downstream computer vision tasks. Recently, Positive-pair-Only Contrastive Learning (POCL) has achieved reliable performance without the need to construct positive-negative training sets. It reduces memory requirements by lessening the dependency on the batch size. The POCL method typically uses a single loss function to extract the distortion invariant representation (DIR) which describes the proximity of positive-pair representations affected by different distortions. This loss function implicitly enables the model to filter out or ignore the distortion variant representation (DVR) affected by different distortions. However, existing POCL methods do not explicitly enforce the disentanglement and exploitation of the actually valuable DVR. In addition, these POCL methods have been observed to be sensitive to augmentation strategies. To address these limitations, we propose a nov",
    "link": "http://arxiv.org/abs/2303.05066",
    "context": "Title: Distortion-Disentangled Contrastive Learning. (arXiv:2303.05066v2 [cs.CV] UPDATED)\nAbstract: Self-supervised learning is well known for its remarkable performance in representation learning and various downstream computer vision tasks. Recently, Positive-pair-Only Contrastive Learning (POCL) has achieved reliable performance without the need to construct positive-negative training sets. It reduces memory requirements by lessening the dependency on the batch size. The POCL method typically uses a single loss function to extract the distortion invariant representation (DIR) which describes the proximity of positive-pair representations affected by different distortions. This loss function implicitly enables the model to filter out or ignore the distortion variant representation (DVR) affected by different distortions. However, existing POCL methods do not explicitly enforce the disentanglement and exploitation of the actually valuable DVR. In addition, these POCL methods have been observed to be sensitive to augmentation strategies. To address these limitations, we propose a nov",
    "path": "papers/23/03/2303.05066.json",
    "total_tokens": 908,
    "translated_title": "扭曲-解缠对比学习",
    "translated_abstract": "自监督学习以其在表示学习和各种下游计算机视觉任务中的显著性能而闻名。最近，正对对比学习（POCL）在无需构建正负训练集的情况下实现了可靠的性能。它通过降低对批次大小的依赖来减少内存需求。POCL方法通常使用单个损失函数提取失真不变表示（DIR），该表示描述了受不同失真影响的正对表示的接近程度。这个损失函数隐式地使模型能够滤除或忽略受不同失真影响的失真变体表示（DVR）。然而，现有的POCL方法没有明确强制执行有价值的DVR的解缠和利用。此外，这些POCL方法对增强策略很敏感。为了解决这些限制，我们提出了一种新颖的方法",
    "tldr": "扭曲-解缠对比学习是一种自监督学习方法，通过使用单个损失函数提取失真不变表示并过滤掉失真变体表示，实现了可靠的性能，同时减少了对批次大小的依赖。该方法还解决了对宝贵的失真变体表示进行解缠和利用的问题，以及对增强策略的敏感性。",
    "en_tdlr": "Distortion-Disentangled Contrastive Learning is a self-supervised learning method that achieves reliable performance by using a single loss function to extract distortion invariant representations and filter out distortion variant representations, while reducing dependency on batch size. It also addresses the disentanglement and exploitation of valuable distortion variant representations, as well as the sensitivity to augmentation strategies."
}