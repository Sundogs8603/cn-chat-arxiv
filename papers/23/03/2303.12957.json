{
    "title": "Reinforcement Learning with Exogenous States and Rewards. (arXiv:2303.12957v1 [cs.LG])",
    "abstract": "Exogenous state variables and rewards can slow reinforcement learning by injecting uncontrolled variation into the reward signal. This paper formalizes exogenous state variables and rewards and shows that if the reward function decomposes additively into endogenous and exogenous components, the MDP can be decomposed into an exogenous Markov Reward Process (based on the exogenous reward) and an endogenous Markov Decision Process (optimizing the endogenous reward). Any optimal policy for the endogenous MDP is also an optimal policy for the original MDP, but because the endogenous reward typically has reduced variance, the endogenous MDP is easier to solve. We study settings where the decomposition of the state space into exogenous and endogenous state spaces is not given but must be discovered. The paper introduces and proves correctness of algorithms for discovering the exogenous and endogenous subspaces of the state space when they are mixed through linear combination. These algorithms",
    "link": "http://arxiv.org/abs/2303.12957",
    "context": "Title: Reinforcement Learning with Exogenous States and Rewards. (arXiv:2303.12957v1 [cs.LG])\nAbstract: Exogenous state variables and rewards can slow reinforcement learning by injecting uncontrolled variation into the reward signal. This paper formalizes exogenous state variables and rewards and shows that if the reward function decomposes additively into endogenous and exogenous components, the MDP can be decomposed into an exogenous Markov Reward Process (based on the exogenous reward) and an endogenous Markov Decision Process (optimizing the endogenous reward). Any optimal policy for the endogenous MDP is also an optimal policy for the original MDP, but because the endogenous reward typically has reduced variance, the endogenous MDP is easier to solve. We study settings where the decomposition of the state space into exogenous and endogenous state spaces is not given but must be discovered. The paper introduces and proves correctness of algorithms for discovering the exogenous and endogenous subspaces of the state space when they are mixed through linear combination. These algorithms",
    "path": "papers/23/03/2303.12957.json",
    "total_tokens": 884,
    "translated_title": "具有外部状态和奖励的强化学习",
    "translated_abstract": "外部状态变量和奖励会通过向奖励信号注入不可控的变化而减慢强化学习的速度。本文对外部状态变量和奖励进行了正式化，并表明如果奖励函数加法分解成内生和外生两个部分，MDP可以分解为一个外生马尔可夫奖励过程（基于外部奖励）和一个内生马尔可夫决策过程（优化内生奖励）。内生MDP的任何最优策略也是原始MDP的最优策略，但由于内生奖励通常具有降低的方差，因此内生MDP更容易求解。我们研究了状态空间分解为内外生状态空间的情况，而这种状态空间分解并没有给出，而是必须发现。本文介绍并证明了在线性组合下发现内生和外生状态空间的算法的正确性。",
    "tldr": "该研究提出了一种强化学习的方法，通过将MDP分解为外生和内生两个部分，优化内生奖励，在状态空间的内生和外生状态空间没有事先给出的情况下，提出了正确的算法进行自动发现。",
    "en_tdlr": "This paper proposes a reinforcement learning approach that decomposes the Markov Decision Process into exogenous and endogenous components, optimizes the endogenous reward, and introduces correct algorithms for discovering the exogenous and endogenous subspaces of the state space under linear combination."
}