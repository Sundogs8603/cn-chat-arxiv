{
    "title": "A General Theory of Correct, Incorrect, and Extrinsic Equivariance. (arXiv:2303.04745v2 [cs.LG] UPDATED)",
    "abstract": "Although equivariant machine learning has proven effective at many tasks, success depends heavily on the assumption that the ground truth function is symmetric over the entire domain matching the symmetry in an equivariant neural network. A missing piece in the equivariant learning literature is the analysis of equivariant networks when symmetry exists only partially in the domain. In this work, we present a general theory for such a situation. We propose pointwise definitions of correct, incorrect, and extrinsic equivariance, which allow us to quantify continuously the degree of each type of equivariance a function displays. We then study the impact of various degrees of incorrect or extrinsic symmetry on model error. We prove error lower bounds for invariant or equivariant networks in classification or regression settings with partially incorrect symmetry. We also analyze the potentially harmful effects of extrinsic equivariance. Experiments validate these results in three different ",
    "link": "http://arxiv.org/abs/2303.04745",
    "context": "Title: A General Theory of Correct, Incorrect, and Extrinsic Equivariance. (arXiv:2303.04745v2 [cs.LG] UPDATED)\nAbstract: Although equivariant machine learning has proven effective at many tasks, success depends heavily on the assumption that the ground truth function is symmetric over the entire domain matching the symmetry in an equivariant neural network. A missing piece in the equivariant learning literature is the analysis of equivariant networks when symmetry exists only partially in the domain. In this work, we present a general theory for such a situation. We propose pointwise definitions of correct, incorrect, and extrinsic equivariance, which allow us to quantify continuously the degree of each type of equivariance a function displays. We then study the impact of various degrees of incorrect or extrinsic symmetry on model error. We prove error lower bounds for invariant or equivariant networks in classification or regression settings with partially incorrect symmetry. We also analyze the potentially harmful effects of extrinsic equivariance. Experiments validate these results in three different ",
    "path": "papers/23/03/2303.04745.json",
    "total_tokens": 879,
    "translated_title": "一个关于正确、错误和外在等变性的普遍理论",
    "translated_abstract": "尽管等变机器学习在许多任务中证明是有效的，但成功很大程度上依赖于假设地面真相函数在整个域上是对称的，与等变神经网络的对称性匹配。等变学习文献中缺少的一块是在域中仅部分存在对称性时等变网络的分析。在这项工作中，我们提出了一个适用于这种情况的普遍理论。我们提出了正确、错误和外在等变性的逐点定义，这使我们能够连续地量化函数显示的每种类型等变性的程度。然后，我们研究了不正确或外在对称性的各种程度对模型错误的影响。我们证明了在部分不正确对称性的分类或回归设置中不变或等变网络存在错误的下界。我们还分析了外在等变性的潜在有害影响。实验证实了这些结果在三种不同的实验中。",
    "tldr": "该论文提出了一个关于正确、错误和外在等变性的普遍理论，通过逐点定义量化了函数表现的每种类型等变性的程度，并研究了不正确或外在对称性对模型错误的影响。实验证实了这些结果。 (230字符)",
    "en_tdlr": "This paper presents a general theory of correct, incorrect, and extrinsic equivariance. It defines pointwise notions of each type of equivariance and studies the impact of incorrect or extrinsic symmetry on model error. Experimental results validate the findings. (188 characters)"
}