{
    "title": "Zero-shot Clinical Entity Recognition using ChatGPT. (arXiv:2303.16416v1 [cs.CL])",
    "abstract": "In this study, we investigated the potential of ChatGPT, a large language model developed by OpenAI, for the clinical named entity recognition task defined in the 2010 i2b2 challenge, in a zero-shot setting with two different prompt strategies. We compared its performance with GPT-3 in a similar zero-shot setting, as well as a fine-tuned BioClinicalBERT model using a set of synthetic clinical notes from MTSamples. Our findings revealed that ChatGPT outperformed GPT-3 in the zero-shot setting, with F1 scores of 0.418 (vs.0.250) and 0.620 (vs. 0.480) for exact- and relaxed-matching, respectively. Moreover, prompts affected ChatGPT's performance greatly, with relaxed-matching F1 scores of 0.628 vs.0.541 for two different prompt strategies. Although ChatGPT's performance was still lower than that of the supervised BioClinicalBERT model (i.e., relaxed-matching F1 scores of 0.628 vs. 0.870), our study demonstrates the great potential of ChatGPT for clinical NER tasks in a zero-shot setting, ",
    "link": "http://arxiv.org/abs/2303.16416",
    "context": "Title: Zero-shot Clinical Entity Recognition using ChatGPT. (arXiv:2303.16416v1 [cs.CL])\nAbstract: In this study, we investigated the potential of ChatGPT, a large language model developed by OpenAI, for the clinical named entity recognition task defined in the 2010 i2b2 challenge, in a zero-shot setting with two different prompt strategies. We compared its performance with GPT-3 in a similar zero-shot setting, as well as a fine-tuned BioClinicalBERT model using a set of synthetic clinical notes from MTSamples. Our findings revealed that ChatGPT outperformed GPT-3 in the zero-shot setting, with F1 scores of 0.418 (vs.0.250) and 0.620 (vs. 0.480) for exact- and relaxed-matching, respectively. Moreover, prompts affected ChatGPT's performance greatly, with relaxed-matching F1 scores of 0.628 vs.0.541 for two different prompt strategies. Although ChatGPT's performance was still lower than that of the supervised BioClinicalBERT model (i.e., relaxed-matching F1 scores of 0.628 vs. 0.870), our study demonstrates the great potential of ChatGPT for clinical NER tasks in a zero-shot setting, ",
    "path": "papers/23/03/2303.16416.json",
    "total_tokens": 1080,
    "translated_title": "利用ChatGPT进行零样本临床实体识别",
    "translated_abstract": "本研究探讨了OpenAI开发的大型语言模型ChatGPT在2010年i2b2挑战中指定的临床命名实体识别任务中的潜力，使用两种不同的提示策略进行了零样本设置。同时，我们将其性能与GPT-3在类似的零样本设置下进行了比较，以及使用MTSamples的一组合成的临床笔记对BioClinicalBERT模型进行优化微调。研究结果显示，ChatGPT在零样本设置中表现优异，精确匹配和松弛匹配的F1分别为0.418（vs.0.250）和0.620（vs.0.480），相比之下，GPT-3的表现较差。另外，提示策略极大地影响了ChatGPT的性能，在两种不同提示策略下松弛匹配的F1分别为0.628和0.541。虽然ChatGPT的性能仍低于受监督的BioClinicalBERT模型（即松弛匹配F1分数分别为0.628和0.870），但我们的研究表明了ChatGPT在零样本设置下临床NER任务中的巨大潜力。",
    "tldr": "本研究探讨了使用 ChatGPT 进行零样本临床实体识别任务，并发现 ChatGPT 在松弛匹配 F1 分数方面显著优于 GPT-3。虽然其性能仍低于 BioClinicalBERT 模型，但我们的研究表明了 ChatGPT 在零样本设置下有很大的临床 NER 任务潜力。",
    "en_tdlr": "This study investigates the potential of using ChatGPT for zero-shot clinical named entity recognition and found that ChatGPT outperformed GPT-3 in relaxed-matching F1 score. Although its performance is still lower than BioClinicalBERT model, this study demonstrates the great potential of ChatGPT for clinical NER tasks in a zero-shot setting."
}