{
    "title": "MGTBench: Benchmarking Machine-Generated Text Detection. (arXiv:2303.14822v2 [cs.CR] UPDATED)",
    "abstract": "Nowadays large language models (LLMs) have shown revolutionary power in a variety of natural language processing (NLP) tasks such as text classification, sentiment analysis, language translation, and question-answering. In this way, detecting machine-generated texts (MGTs) is becoming increasingly important as LLMs become more advanced and prevalent. These models can generate human-like language that can be difficult to distinguish from text written by a human, which raises concerns about authenticity, accountability, and potential bias. However, existing detection methods against MGTs are evaluated under different model architectures, datasets, and experimental settings, resulting in a lack of a comprehensive evaluation framework across different methodologies  In this paper, we fill this gap by proposing the first benchmark framework for MGT detection, named MGTBench. Extensive evaluations on public datasets with curated answers generated by ChatGPT (the most representative and power",
    "link": "http://arxiv.org/abs/2303.14822",
    "context": "Title: MGTBench: Benchmarking Machine-Generated Text Detection. (arXiv:2303.14822v2 [cs.CR] UPDATED)\nAbstract: Nowadays large language models (LLMs) have shown revolutionary power in a variety of natural language processing (NLP) tasks such as text classification, sentiment analysis, language translation, and question-answering. In this way, detecting machine-generated texts (MGTs) is becoming increasingly important as LLMs become more advanced and prevalent. These models can generate human-like language that can be difficult to distinguish from text written by a human, which raises concerns about authenticity, accountability, and potential bias. However, existing detection methods against MGTs are evaluated under different model architectures, datasets, and experimental settings, resulting in a lack of a comprehensive evaluation framework across different methodologies  In this paper, we fill this gap by proposing the first benchmark framework for MGT detection, named MGTBench. Extensive evaluations on public datasets with curated answers generated by ChatGPT (the most representative and power",
    "path": "papers/23/03/2303.14822.json",
    "total_tokens": 1030,
    "translated_title": "MGTBench：机器生成文本检测基准测试",
    "translated_abstract": "现在，大型语言模型（LLM）在多种自然语言处理（NLP）任务中显示出了革命性的力量，例如文本分类，情感分析，语言翻译和问答。因此，检测机器生成文本（MGT）随着LLM变得越来越先进和普遍变得越来越重要。这些模型可以生成类似于人类写作的语言，很难与人类写的文本区分开来，这引发了关于真实性，问责和潜在偏见的担忧。然而，现有的MGT检测方法是在不同的模型体系结构，数据集和实验设置下进行评估的，导致缺乏跨不同方法学的全面评估框架。本文通过提出名为MGTBench的MGT检测基准框架来填补这个空白。对由ChatGPT生成的答案进行了广泛的评估，该模型是中国最具代表性和最强大的LLM模型。结果表明，MGTBench提供了公平和全面的MGT检测评估，并使研究人员比较了不同检测方法的有效性。",
    "tldr": "本文提出了MGTBench框架，旨在解决机器生成文本检测中现有评估方法的不足。该框架通过广泛评估和公开数据集，提供了全面的MGT检测评估，使研究人员能够比较不同检测方法的有效性。",
    "en_tdlr": "This paper proposes a benchmark framework named MGTBench to address the shortcomings of existing evaluation methods for machine-generated text (MGT) detection. Extensive evaluations on public datasets provide a comprehensive evaluation of MGT detection, enabling researchers to compare the effectiveness of different detection methodologies. The framework aims to provide fair and comprehensive MGT detection evaluation to better understand the authenticity, accountability, and potential bias of MGT."
}