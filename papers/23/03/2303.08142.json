{
    "title": "Performance Embeddings: A Similarity-based Approach to Automatic Performance Optimization. (arXiv:2303.08142v1 [cs.SE])",
    "abstract": "Performance optimization is an increasingly challenging but often repetitive task. While each platform has its quirks, the underlying code transformations rely on data movement and computational characteristics that recur across applications. This paper proposes to leverage those similarities by constructing an embedding space for subprograms. The continuous space captures both static and dynamic properties of loop nests via symbolic code analysis and performance profiling, respectively. Performance embeddings enable direct knowledge transfer of performance tuning between applications, which can result from autotuning or tailored improvements. We demonstrate this transfer tuning approach on case studies in deep neural networks, dense and sparse linear algebra compositions, and numerical weather prediction stencils. Transfer tuning reduces the search complexity by up to four orders of magnitude and outperforms the MKL library in sparse-dense matrix multiplication. The results exhibit cl",
    "link": "http://arxiv.org/abs/2303.08142",
    "context": "Title: Performance Embeddings: A Similarity-based Approach to Automatic Performance Optimization. (arXiv:2303.08142v1 [cs.SE])\nAbstract: Performance optimization is an increasingly challenging but often repetitive task. While each platform has its quirks, the underlying code transformations rely on data movement and computational characteristics that recur across applications. This paper proposes to leverage those similarities by constructing an embedding space for subprograms. The continuous space captures both static and dynamic properties of loop nests via symbolic code analysis and performance profiling, respectively. Performance embeddings enable direct knowledge transfer of performance tuning between applications, which can result from autotuning or tailored improvements. We demonstrate this transfer tuning approach on case studies in deep neural networks, dense and sparse linear algebra compositions, and numerical weather prediction stencils. Transfer tuning reduces the search complexity by up to four orders of magnitude and outperforms the MKL library in sparse-dense matrix multiplication. The results exhibit cl",
    "path": "papers/23/03/2303.08142.json",
    "total_tokens": 869,
    "translated_title": "性能嵌入：一种基于相似性的自动性能优化方法",
    "translated_abstract": "性能优化是一项越来越具有挑战性但经常重复的任务。虽然每个平台都有其独特之处，但基于数据移动和计算特性的底层代码转换在应用程序中会反复出现。本文提议利用这些相似性通过构建子程序的嵌入空间。该连续空间通过符号代码分析和性能分析捕捉循环嵌套的静态和动态特性。性能嵌入使性能调整的直接知识传输成为可能，这可以来自自动调整或量身定制的改进。我们在深度神经网络、密集和稀疏线性代数组合以及数值天气预报模板的案例研究中展示了这种传输调整方法。传输调整将搜索复杂度降低了多达四个数量级，并在稀疏-密集矩阵乘法中优于MKL库。结果表明...",
    "tldr": "本文提出了性能嵌入的方法，通过构建子程序的嵌入空间来实现性能优化的直接知识传输。传输调整将搜索复杂度降低了多达四个数量级，并在稀疏-密集矩阵乘法中优于MKL库。"
}