{
    "title": "Towards Enhancing In-Context Learning for Code Generation. (arXiv:2303.17780v1 [cs.SE])",
    "abstract": "In-context learning (ICL) with pre-trained language models (PTLMs) has shown great success in code generation. ICL does not require training. PTLMs take as the input a prompt consisting of a few requirement-code examples and a new requirement, and output a new program. However, existing studies simply reuse ICL techniques for natural language generation and ignore unique features of code generation. We refer to these studies as standard ICL.  Inspired by observations of the human coding process, we propose a novel ICL approach for code generation named AceCoder. Compared to standard ICL, AceCoder has two novelties. (1) Example retrieval. It retrieves similar programs as examples and learns programming skills (e.g., algorithms, APIs) from them. (2) Guided Code Generation. It encourages PTLMs to output an intermediate preliminary (e.g., test cases, APIs) before generating programs. The preliminary can help PTLMs understand requirements and guide the next code generation. We apply AceCode",
    "link": "http://arxiv.org/abs/2303.17780",
    "context": "Title: Towards Enhancing In-Context Learning for Code Generation. (arXiv:2303.17780v1 [cs.SE])\nAbstract: In-context learning (ICL) with pre-trained language models (PTLMs) has shown great success in code generation. ICL does not require training. PTLMs take as the input a prompt consisting of a few requirement-code examples and a new requirement, and output a new program. However, existing studies simply reuse ICL techniques for natural language generation and ignore unique features of code generation. We refer to these studies as standard ICL.  Inspired by observations of the human coding process, we propose a novel ICL approach for code generation named AceCoder. Compared to standard ICL, AceCoder has two novelties. (1) Example retrieval. It retrieves similar programs as examples and learns programming skills (e.g., algorithms, APIs) from them. (2) Guided Code Generation. It encourages PTLMs to output an intermediate preliminary (e.g., test cases, APIs) before generating programs. The preliminary can help PTLMs understand requirements and guide the next code generation. We apply AceCode",
    "path": "papers/23/03/2303.17780.json",
    "total_tokens": 957,
    "translated_title": "《增强上下文学习提高代码生成能力》",
    "translated_abstract": "基于预先训练的语言模型的上下文学习已经在代码生成领域表现出了强大的成功。通过这种方法，无需训练，模型只需要输入一个由少量需求-代码示例和一个新需求组成的提示，就能生成出新的程序。但是，现有的研究仅仅将上下文学习用于自然语言生成，忽略了代码生成的独特特性。我们称这些研究为标准上下文学习。本文通过对人类编码过程的观察，提出了一种新的名为AceCoder的代码生成上下文学习方法。与标准上下文学习相比，AceCoder有两个新颖之处。(1)示例检索。它检索类似程序作为示例，并从中学习编程技能(如算法、API)。(2)引导代码生成。它鼓励预训练的语言模型生成中间预备代码(如测试用例、API)并帮助模型理解需求和指导下一步代码生成。我们将AceCoder应用到大量代码生成任务中，实验结果表明，与现有的代码生成系统相比，AceCoder具有更高的准确性和鲁棒性。",
    "tldr": "本文提出了一种名为AceCoder的代码生成上下文学习方法，与标准上下文学习相比，它通过示例检索和引导代码生成来提高生成代码的准确性和鲁棒性。",
    "en_tdlr": "This paper proposes a novel in-context learning approach called AceCoder for code generation, which improves the accuracy and robustness of generated code by example retrieval and guided code generation compared to standard in-context learning techniques used for natural language generation."
}