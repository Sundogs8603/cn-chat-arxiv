{
    "title": "Neural Collapse Inspired Federated Learning with Non-iid Data. (arXiv:2303.16066v1 [cs.LG])",
    "abstract": "One of the challenges in federated learning is the non-independent and identically distributed (non-iid) characteristics between heterogeneous devices, which cause significant differences in local updates and affect the performance of the central server. Although many studies have been proposed to address this challenge, they only focus on local training and aggregation processes to smooth the changes and fail to achieve high performance with deep learning models. Inspired by the phenomenon of neural collapse, we force each client to be optimized toward an optimal global structure for classification. Specifically, we initialize it as a random simplex Equiangular Tight Frame (ETF) and fix it as the unit optimization target of all clients during the local updating. After guaranteeing all clients are learning to converge to the global optimum, we propose to add a global memory vector for each category to remedy the parameter fluctuation caused by the bias of the intra-class condition dist",
    "link": "http://arxiv.org/abs/2303.16066",
    "context": "Title: Neural Collapse Inspired Federated Learning with Non-iid Data. (arXiv:2303.16066v1 [cs.LG])\nAbstract: One of the challenges in federated learning is the non-independent and identically distributed (non-iid) characteristics between heterogeneous devices, which cause significant differences in local updates and affect the performance of the central server. Although many studies have been proposed to address this challenge, they only focus on local training and aggregation processes to smooth the changes and fail to achieve high performance with deep learning models. Inspired by the phenomenon of neural collapse, we force each client to be optimized toward an optimal global structure for classification. Specifically, we initialize it as a random simplex Equiangular Tight Frame (ETF) and fix it as the unit optimization target of all clients during the local updating. After guaranteeing all clients are learning to converge to the global optimum, we propose to add a global memory vector for each category to remedy the parameter fluctuation caused by the bias of the intra-class condition dist",
    "path": "papers/23/03/2303.16066.json",
    "total_tokens": 889,
    "translated_title": "受神经衰竭启示的非独立同分布数据联邦学习",
    "translated_abstract": "联邦学习中的主要挑战之一是异构设备之间的非独立同分布（非iid）特性，导致本地更新存在显著差异，影响中央服务器的性能。尽管已经提出了许多研究来解决这个挑战，但它们只关注本地训练和聚合过程以平滑变化，并未在深度学习模型中实现高性能。受神经衰竭现象启发，我们强制每个客户端优化向全局分类的最佳结构。具体而言，我们将其初始化为随机的简单六角紧框架（ETF），并在本地更新期间将其作为所有客户端的单元优化目标进行固定。在确保所有客户端学习收敛于全局最优解之后，我们提出为每个类别添加全局记忆向量，以补救由于类内条件分布偏差引起的参数波动。",
    "tldr": "本文提出了一种受神经衰竭启示的方案，通过将每个客户端优化向全局分类的最佳结构，解决了联邦学习中非独立同分布数据的挑战，并通过添加全局记忆向量来补救参数波动的问题。",
    "en_tdlr": "This paper proposes a neural collapse inspired approach to address the challenge of non-iid data in federated learning by optimizing each client towards the global optimal structure for classification and adding global memory vectors to remedy parameter fluctuation."
}