{
    "title": "Hindi as a Second Language: Improving Visually Grounded Speech with Semantically Similar Samples. (arXiv:2303.17517v1 [cs.CL])",
    "abstract": "The objective of this work is to explore the learning of visually grounded speech models (VGS) from multilingual perspective. Bilingual VGS models are generally trained with an equal number of spoken captions from both languages. However, in reality, there can be an imbalance among the languages for the available spoken captions. Our key contribution in this work is to leverage the power of a high-resource language in a bilingual visually grounded speech model to improve the performance of a low-resource language. We introduce two methods to distill the knowledge of high-resource language into low-resource languages: (1) incorporating a strong pre-trained high-resource language encoder and (2) using semantically similar spoken captions. Our experiments show that combining these two approaches effectively enables the low-resource language to surpass the performances of monolingual and bilingual counterparts for cross-modal retrieval tasks.",
    "link": "http://arxiv.org/abs/2303.17517",
    "context": "Title: Hindi as a Second Language: Improving Visually Grounded Speech with Semantically Similar Samples. (arXiv:2303.17517v1 [cs.CL])\nAbstract: The objective of this work is to explore the learning of visually grounded speech models (VGS) from multilingual perspective. Bilingual VGS models are generally trained with an equal number of spoken captions from both languages. However, in reality, there can be an imbalance among the languages for the available spoken captions. Our key contribution in this work is to leverage the power of a high-resource language in a bilingual visually grounded speech model to improve the performance of a low-resource language. We introduce two methods to distill the knowledge of high-resource language into low-resource languages: (1) incorporating a strong pre-trained high-resource language encoder and (2) using semantically similar spoken captions. Our experiments show that combining these two approaches effectively enables the low-resource language to surpass the performances of monolingual and bilingual counterparts for cross-modal retrieval tasks.",
    "path": "papers/23/03/2303.17517.json",
    "total_tokens": 836,
    "translated_title": "印地语作为第二语言：通过语义相似样本提高基于视觉的语音模型的性能",
    "translated_abstract": "本文旨在从多语言角度探索基于视觉的语音模型(VGS)的学习。双语VGS模型通常使用两种语言中平均数量的口语字幕进行训练。然而，实际上，可用口语字幕之间的语言可能存在不平衡。本文主要贡献在于利用高资源语言的力量在双语基于视觉的语音模型中提高低资源语言的性能。我们介绍了两种方法来将高资源语言的知识蒸馏到低资源语言中：(1)整合强大的预训练高资源语言编码器和(2)使用语义相似的口语字幕。我们的实验结果表明，这两种方法的组合有效地使低资源语言在跨模态检索任务中超过单语言和双语言模型的性能。",
    "tldr": "本文通过使用语义相似的口语字幕和预训练的语言编码器方法，利用高资源语言来提高低资源语言的基于视觉的语音模型性能。"
}