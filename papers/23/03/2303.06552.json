{
    "title": "Energy Regularized RNNs for Solving Non-Stationary Bandit Problems. (arXiv:2303.06552v2 [cs.LG] UPDATED)",
    "abstract": "We consider a Multi-Armed Bandit problem in which the rewards are non-stationary and are dependent on past actions and potentially on past contexts. At the heart of our method, we employ a recurrent neural network, which models these sequences. In order to balance between exploration and exploitation, we present an energy minimization term that prevents the neural network from becoming too confident in support of a certain action. This term provably limits the gap between the maximal and minimal probabilities assigned by the network. In a diverse set of experiments, we demonstrate that our method is at least as effective as methods suggested to solve the sub-problem of Rotting Bandits, and can solve intuitive extensions of various benchmark problems. We share our implementation at https://github.com/rotmanmi/Energy-Regularized-RNN.",
    "link": "http://arxiv.org/abs/2303.06552",
    "context": "Title: Energy Regularized RNNs for Solving Non-Stationary Bandit Problems. (arXiv:2303.06552v2 [cs.LG] UPDATED)\nAbstract: We consider a Multi-Armed Bandit problem in which the rewards are non-stationary and are dependent on past actions and potentially on past contexts. At the heart of our method, we employ a recurrent neural network, which models these sequences. In order to balance between exploration and exploitation, we present an energy minimization term that prevents the neural network from becoming too confident in support of a certain action. This term provably limits the gap between the maximal and minimal probabilities assigned by the network. In a diverse set of experiments, we demonstrate that our method is at least as effective as methods suggested to solve the sub-problem of Rotting Bandits, and can solve intuitive extensions of various benchmark problems. We share our implementation at https://github.com/rotmanmi/Energy-Regularized-RNN.",
    "path": "papers/23/03/2303.06552.json",
    "total_tokens": 861,
    "translated_title": "能量正则化RNN解决非平稳Bandit问题",
    "translated_abstract": "我们考虑一个多臂赌博机问题，其中奖励是非平稳的，并取决于过去的动作和可能的过去情境。在我们方法的核心，我们采用循环神经网络，对这些序列进行建模。为了平衡探索和利用之间的关系，我们提出了一个能量最小化项，以防止神经网络对支持某个操作过于自信。通过这个术语，网络所分配的最大和最小概率之间的差距受到合理限制。在一系列不同的实验中，我们证明了我们的方法至少与解决Rotting Bandits子问题的方法一样有效，并且可以解决各种基准问题的直观扩展性。我们在https://github.com/rotmanmi/Energy-Regularized-RNN上共享我们的实现。",
    "tldr": "本文提出了一种能量正则化的循环神经网络方法，解决了奖励非平稳的多臂赌博机问题。该方法平衡了探索和利用，通过能量最小化项限制了网络对支持某个操作的过度自信，有效性与同类方法相当。",
    "en_tdlr": "This paper proposes an energy regularized recurrent neural network method to solve non-stationary multi-armed bandit problems. It balances exploration and exploitation by minimizing energy and limits the network's confidence in a certain action. The method is proven to be at least as effective as similar ones in solving the sub-problem of Rotting Bandits and can solve intuitive extensions of various benchmark problems."
}