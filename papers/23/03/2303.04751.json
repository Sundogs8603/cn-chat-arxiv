{
    "title": "Multimodal Parameter-Efficient Few-Shot Class Incremental Learning. (arXiv:2303.04751v2 [cs.CV] UPDATED)",
    "abstract": "Few-Shot Class Incremental Learning (FSCIL) is a challenging continual learning task, where limited training examples are available during several learning sessions. To succeed in this task, it is necessary to avoid over-fitting new classes caused by biased distributions in the few-shot training sets. The general approach to address this issue involves enhancing the representational capability of a pre-defined backbone architecture by adding special modules for backward compatibility with older classes. However, this approach has not yet solved the dilemma of ensuring high classification accuracy over time while reducing the gap between the performance obtained on larger training sets and the smaller ones. In this work, we propose an alternative approach called Continual Parameter-Efficient CLIP (CPE-CLIP) to reduce the loss of information between different learning sessions. Instead of adapting additional modules to address information loss, we leverage the vast knowledge acquired by ",
    "link": "http://arxiv.org/abs/2303.04751",
    "context": "Title: Multimodal Parameter-Efficient Few-Shot Class Incremental Learning. (arXiv:2303.04751v2 [cs.CV] UPDATED)\nAbstract: Few-Shot Class Incremental Learning (FSCIL) is a challenging continual learning task, where limited training examples are available during several learning sessions. To succeed in this task, it is necessary to avoid over-fitting new classes caused by biased distributions in the few-shot training sets. The general approach to address this issue involves enhancing the representational capability of a pre-defined backbone architecture by adding special modules for backward compatibility with older classes. However, this approach has not yet solved the dilemma of ensuring high classification accuracy over time while reducing the gap between the performance obtained on larger training sets and the smaller ones. In this work, we propose an alternative approach called Continual Parameter-Efficient CLIP (CPE-CLIP) to reduce the loss of information between different learning sessions. Instead of adapting additional modules to address information loss, we leverage the vast knowledge acquired by ",
    "path": "papers/23/03/2303.04751.json",
    "total_tokens": 915,
    "translated_title": "多模态参数高效少样本类增量学习",
    "translated_abstract": "少样本类增量学习（FSCIL）是一项具有挑战性的连续学习任务，在多个学习会话期间只有有限的训练样本可用。为了在这个任务中取得成功，需要避免少样本训练集中存在的偏态分布导致对新类别过拟合的问题。通常解决这个问题的方法是通过为预定义的骨干架构添加特殊模块以增强表示能力，以实现与旧类别的向后兼容性。然而，这种方法尚未解决在减少训练集规模时同时确保高分类精度和减小较大训练集和较小训练集之间性能差距的困境。在这项工作中，我们提出了一种另类方法，称为连续参数高效CLIP（CPE-CLIP），以减少不同学习会话之间信息丢失的问题。我们不是通过适应附加模块来解决信息丢失问题，而是利用广泛的知识来提升模型的表示能力，并通过动态参数调整来适应新的类别。",
    "tldr": "提出了一种名为CPE-CLIP的连续参数高效学习方法，通过利用广泛的知识和动态参数调整来减少少样本类增量学习中不同学习会话之间的信息丢失问题。"
}