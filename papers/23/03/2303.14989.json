{
    "title": "Regularized EM algorithm. (arXiv:2303.14989v1 [stat.ML])",
    "abstract": "Expectation-Maximization (EM) algorithm is a widely used iterative algorithm for computing (local) maximum likelihood estimate (MLE). It can be used in an extensive range of problems, including the clustering of data based on the Gaussian mixture model (GMM). Numerical instability and convergence problems may arise in situations where the sample size is not much larger than the data dimensionality. In such low sample support (LSS) settings, the covariance matrix update in the EM-GMM algorithm may become singular or poorly conditioned, causing the algorithm to crash. On the other hand, in many signal processing problems, a priori information can be available indicating certain structures for different cluster covariance matrices. In this paper, we present a regularized EM algorithm for GMM-s that can make efficient use of such prior knowledge as well as cope with LSS situations. The method aims to maximize a penalized GMM likelihood where regularized estimation may be used to ensure pos",
    "link": "http://arxiv.org/abs/2303.14989",
    "context": "Title: Regularized EM algorithm. (arXiv:2303.14989v1 [stat.ML])\nAbstract: Expectation-Maximization (EM) algorithm is a widely used iterative algorithm for computing (local) maximum likelihood estimate (MLE). It can be used in an extensive range of problems, including the clustering of data based on the Gaussian mixture model (GMM). Numerical instability and convergence problems may arise in situations where the sample size is not much larger than the data dimensionality. In such low sample support (LSS) settings, the covariance matrix update in the EM-GMM algorithm may become singular or poorly conditioned, causing the algorithm to crash. On the other hand, in many signal processing problems, a priori information can be available indicating certain structures for different cluster covariance matrices. In this paper, we present a regularized EM algorithm for GMM-s that can make efficient use of such prior knowledge as well as cope with LSS situations. The method aims to maximize a penalized GMM likelihood where regularized estimation may be used to ensure pos",
    "path": "papers/23/03/2303.14989.json",
    "total_tokens": 872,
    "translated_title": "正则化EM算法",
    "translated_abstract": "期望最大化（EM）算法是一种广泛用于计算局部最大似然估计（MLE）的迭代算法。它可以应用于广泛的问题，包括基于高斯混合模型（GMM）进行数据聚类。当样本大小不大于数据维数时，可能会出现数值不稳定和收敛问题。在这种低样本支持（LSS）设置下，EM-GMM算法中的协方差矩阵更新可能会变得奇异或病态，从而导致算法崩溃。另一方面，在许多信号处理问题中，预先可用的先验信息可以指示不同聚类协方差矩阵的某些结构。在本文中，我们提出了一种正则化EM算法，用于GMM，可以有效利用这样的先验知识以及应对LSS情况。该方法旨在最大化惩罚的GMM似然，其中可以使用正则化估计来确保正定的协方差矩阵。",
    "tldr": "该论文提出了一种正则化EM算法，用于处理低样本支持情况下GMM的协方差矩阵更新问题，并且可以利用预先提供的先验知识来实现更高效的计算。",
    "en_tdlr": "This paper proposes a regularized EM algorithm for GMM that can handle the issues with covariance matrix updates in low sample support situations and make efficient use of available prior knowledge for better computation."
}