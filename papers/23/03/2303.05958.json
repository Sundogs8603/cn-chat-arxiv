{
    "title": "Robust Knowledge Distillation from RNN-T Models With Noisy Training Labels Using Full-Sum Loss. (arXiv:2303.05958v1 [cs.CL])",
    "abstract": "This work studies knowledge distillation (KD) and addresses its constraints for recurrent neural network transducer (RNN-T) models. In hard distillation, a teacher model transcribes large amounts of unlabelled speech to train a student model. Soft distillation is another popular KD method that distills the output logits of the teacher model. Due to the nature of RNN-T alignments, applying soft distillation between RNN-T architectures having different posterior distributions is challenging. In addition, bad teachers having high word-error-rate (WER) reduce the efficacy of KD. We investigate how to effectively distill knowledge from variable quality ASR teachers, which has not been studied before to the best of our knowledge. We show that a sequence-level KD, full-sum distillation, outperforms other distillation methods for RNN-T models, especially for bad teachers. We also propose a variant of full-sum distillation that distills the sequence discriminative knowledge of the teacher leadi",
    "link": "http://arxiv.org/abs/2303.05958",
    "raw_ret": "import requests\nimport json\nquery=\"\"\"\n{\n  \"inputs\": [\n    {\n      \"text\": \"This work studies knowledge distillation (KD) and addresses its constraints for recurrent neural network transducer (RNN-T) models. In hard distillation, a teacher model transcribes large amounts of unlabelled speech to train a student model. Soft distillation is another popular KD method that distills the output logits of the teacher model. Due to the nature of RNN-T alignments, applying soft distillation between RNN-T architectures having different posterior distributions is challenging. In addition, bad teachers having high word-error-rate (WER) reduce the efficacy of KD. We investigate how to effectively distill knowledge from variable quality ASR teachers, which has not been studied before to the best of our knowledge. We show that a sequence-level KD, full-sum distillation, outperforms other distillation methods for RNN-T models, especially for bad teachers. We also propose a variant of full-sum distillation that distills the sequence discriminative knowledge of the teacher leadi\"\n    }\n  ],\n  \"source\": \"en\",\n  \"target\": \"zh-CN\"\n}\n\"\"\"\nrequest=requests.post('https://api.caiyunapp.com/v2/AoQmV7egu70yNNEV/'+query)\njsonData=json.loads(request.text)\ntranslation=jsonData['outputs'][0]['translation']\nprint(json.dumps({\n    \"translated_title\": \"使用全总和损失从带有噪声训练标签的RNN-T模型中进行鲁棒知识蒸馏。\",\n    \"translated_abstract\": translation,\n    \"tldr\": \"本文研究知识蒸馏（KD）及其对循环神经网络转导器（RNN-T）模型的限制进行了讨论。我们展示了一种序列级KD，全总和蒸馏，它在RNN-T模型中优于其他蒸馏方法，特别是在处理低质量的ASR teacher方面。同时我们也提出了一个全总和蒸馏的变种，它蒸馏了教师的序列判别知识。\"\n}))<|im_sep|>",
    "total_tokens": 964
}