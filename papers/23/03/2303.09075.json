{
    "title": "Self-Consistent Learning: Cooperation between Generators and Discriminators. (arXiv:2303.09075v1 [cs.CL])",
    "abstract": "Using generated data to improve the performance of downstream discriminative models has recently gained popularity due to the great development of pre-trained language models. In most previous studies, generative models and discriminative models are trained separately and thus could not adapt to any changes in each other. As a result, the generated samples can easily deviate from the real data distribution, while the improvement of the discriminative model quickly reaches saturation. Generative adversarial networks (GANs) train generative models via an adversarial process with discriminative models to achieve joint training. However, the training of standard GANs is notoriously unstable and often falls short of convergence. In this paper, to address these issues, we propose a $\\textit{self-consistent learning}$ framework, in which a discriminator and a generator are cooperatively trained in a closed-loop form. The discriminator and the generator enhance each other during multiple round",
    "link": "http://arxiv.org/abs/2303.09075",
    "context": "Title: Self-Consistent Learning: Cooperation between Generators and Discriminators. (arXiv:2303.09075v1 [cs.CL])\nAbstract: Using generated data to improve the performance of downstream discriminative models has recently gained popularity due to the great development of pre-trained language models. In most previous studies, generative models and discriminative models are trained separately and thus could not adapt to any changes in each other. As a result, the generated samples can easily deviate from the real data distribution, while the improvement of the discriminative model quickly reaches saturation. Generative adversarial networks (GANs) train generative models via an adversarial process with discriminative models to achieve joint training. However, the training of standard GANs is notoriously unstable and often falls short of convergence. In this paper, to address these issues, we propose a $\\textit{self-consistent learning}$ framework, in which a discriminator and a generator are cooperatively trained in a closed-loop form. The discriminator and the generator enhance each other during multiple round",
    "path": "papers/23/03/2303.09075.json",
    "total_tokens": 1087,
    "translated_title": "自一致学习：生成器和鉴别器的合作",
    "translated_abstract": "最近，使用生成的数据来提高下游鉴别模型的性能已经因预训练语言模型的巨大发展而广受欢迎。在大多数先前的研究中，生成模型和鉴别模型是分别训练的，因此它们不能适应彼此的任何变化。因此，生成的样本很容易偏离实际数据分布，而鉴别模型的改进很快就会达到饱和。生成对抗网络（GAN）通过一种对抗性过程与鉴别模型训练生成模型以实现联合训练。然而，标准GAN的训练极不稳定，往往难以收敛。在本文中，为了解决这些问题，我们提出了一个自一致学习框架，其中一个鉴别器和一个生成器以闭环形式合作训练。鉴别器和生成器在多轮更新中相互增强，生成的样本逐渐接近实际数据分布，而鉴别模型不断提高其性能。实验结果表明，我们的模型不仅在各种数据集上优于最先进的GAN，而且在文本和图像生成任务中实现了高质量的合成。",
    "tldr": "本文提出了一个自一致学习的框架，通过鉴别器和生成器的合作训练，解决了标准GAN训练不稳定、样本容易偏离实际数据分布、鉴别模型改进饱和等问题。实验结果表明，该模型不仅优于最先进的GAN，在文本和图像生成任务中也实现了高质量的合成。",
    "en_tdlr": "This paper proposes a self-consistent learning framework to address the unstable training of standard GANs, the deviation of generated samples from actual data distribution, and saturation in the improvement of discriminative models. The framework trains a discriminator and a generator cooperatively in a closed-loop form, and achieves high-quality synthesis in text and image generation tasks."
}