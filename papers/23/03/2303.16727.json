{
    "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking. (arXiv:2303.16727v1 [cs.CV])",
    "abstract": "Scale is the primary factor for building a powerful foundation model that could well generalize to a variety of downstream tasks. However, it is still challenging to train video foundation models with billions of parameters. This paper shows that video masked autoencoder (VideoMAE) is a scalable and general self-supervised pre-trainer for building video foundation models. We scale the VideoMAE in both model and data with a core design. Specifically, we present a dual masking strategy for efficient pre-training, with an encoder operating on a subset of video tokens and a decoder processing another subset of video tokens. Although VideoMAE is very efficient due to high masking ratio in encoder, masking decoder can still further reduce the overall computational cost. This enables the efficient pre-training of billion-level models in video. We also use a progressive training paradigm that involves an initial pre-training on a diverse multi-sourced unlabeled dataset, followed by a post-pre-",
    "link": "http://arxiv.org/abs/2303.16727",
    "context": "Title: VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking. (arXiv:2303.16727v1 [cs.CV])\nAbstract: Scale is the primary factor for building a powerful foundation model that could well generalize to a variety of downstream tasks. However, it is still challenging to train video foundation models with billions of parameters. This paper shows that video masked autoencoder (VideoMAE) is a scalable and general self-supervised pre-trainer for building video foundation models. We scale the VideoMAE in both model and data with a core design. Specifically, we present a dual masking strategy for efficient pre-training, with an encoder operating on a subset of video tokens and a decoder processing another subset of video tokens. Although VideoMAE is very efficient due to high masking ratio in encoder, masking decoder can still further reduce the overall computational cost. This enables the efficient pre-training of billion-level models in video. We also use a progressive training paradigm that involves an initial pre-training on a diverse multi-sourced unlabeled dataset, followed by a post-pre-",
    "path": "papers/23/03/2303.16727.json",
    "total_tokens": 1005,
    "tldr": "本论文介绍了使用双重遮盖策略提高视频遮盖自编码器（VideoMAE）预训练效率的方法，进一步使其高效预训练数十亿级别的模型。最终建立出更好的视频识别模型。",
    "en_tdlr": "This paper presents a method to improve the efficiency of pre-training video masked autoencoder (VideoMAE) using dual masking strategy, making it possible to efficiently pre-train models on the scale of billions. The proposed approach results in better video recognition models built using a progressive training paradigm involving an initial pre-training on diverse unlabeled data followed by post-pre-training on a smaller labeled dataset."
}