{
    "title": "A Generalized Multi-Modal Fusion Detection Framework. (arXiv:2303.07064v2 [cs.CV] UPDATED)",
    "abstract": "LiDAR point clouds have become the most common data source in autonomous driving. However, due to the sparsity of point clouds, accurate and reliable detection cannot be achieved in specific scenarios. Because of their complementarity with point clouds, images are getting increasing attention. Although with some success, existing fusion methods either perform hard fusion or do not fuse in a direct manner. In this paper, we propose a generic 3D detection framework called MMFusion, using multi-modal features. The framework aims to achieve accurate fusion between LiDAR and images to improve 3D detection in complex scenes. Our framework consists of two separate streams: the LiDAR stream and the camera stream, which can be compatible with any single-modal feature extraction network. The Voxel Local Perception Module in the LiDAR stream enhances local feature representation, and then the Multi-modal Feature Fusion Module selectively combines feature output from different streams to achieve b",
    "link": "http://arxiv.org/abs/2303.07064",
    "context": "Title: A Generalized Multi-Modal Fusion Detection Framework. (arXiv:2303.07064v2 [cs.CV] UPDATED)\nAbstract: LiDAR point clouds have become the most common data source in autonomous driving. However, due to the sparsity of point clouds, accurate and reliable detection cannot be achieved in specific scenarios. Because of their complementarity with point clouds, images are getting increasing attention. Although with some success, existing fusion methods either perform hard fusion or do not fuse in a direct manner. In this paper, we propose a generic 3D detection framework called MMFusion, using multi-modal features. The framework aims to achieve accurate fusion between LiDAR and images to improve 3D detection in complex scenes. Our framework consists of two separate streams: the LiDAR stream and the camera stream, which can be compatible with any single-modal feature extraction network. The Voxel Local Perception Module in the LiDAR stream enhances local feature representation, and then the Multi-modal Feature Fusion Module selectively combines feature output from different streams to achieve b",
    "path": "papers/23/03/2303.07064.json",
    "total_tokens": 894,
    "translated_title": "一个通用的多模态融合检测框架",
    "translated_abstract": "激光雷达点云已成为自动驾驶中最常见的数据来源，但由于点云的稀疏性，在特定场景下无法实现准确可靠的检测。图像因其与点云的互补性而越来越受到关注。虽然已有一些成功案例，但现有的融合方法要么进行硬融合，要么不直接进行融合。本文提出了一种称为MMFusion的通用的3D检测框架，使用多模态特征。该框架旨在实现在复杂场景中激光雷达和图像的准确融合，从而提高3D检测的精度。我们的框架由两个独立的流组成：激光雷达流和相机流，可以与任何单模态特征提取网络兼容。激光雷达流中的体素局部感知模块增强了局部特征表示，然后多模态特征融合模块有选择地将来自不同流的特征输出进行融合，实现。",
    "tldr": "本文提出了一个通用的多模态融合检测框架，旨在在复杂场景中通过准确融合激光雷达和图像来提高3D检测的精度。",
    "en_tdlr": "A generalized multi-modal fusion detection framework is proposed in this paper, aiming to improve the accuracy of 3D detection in complex scenes by accurately fusing LiDAR and images."
}