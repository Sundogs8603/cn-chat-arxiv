{
    "title": "Revisiting Automatic Question Summarization Evaluation in the Biomedical Domain. (arXiv:2303.10328v1 [cs.CL])",
    "abstract": "Automatic evaluation metrics have been facilitating the rapid development of automatic summarization methods by providing instant and fair assessments of the quality of summaries. Most metrics have been developed for the general domain, especially news and meeting notes, or other language-generation tasks. However, these metrics are applied to evaluate summarization systems in different domains, such as biomedical question summarization. To better understand whether commonly used evaluation metrics are capable of evaluating automatic summarization in the biomedical domain, we conduct human evaluations of summarization quality from four different aspects of a biomedical question summarization task. Based on human judgments, we identify different noteworthy features for current automatic metrics and summarization systems as well. We also release a dataset of our human annotations to aid the research of summarization evaluation metrics in the biomedical domain.",
    "link": "http://arxiv.org/abs/2303.10328",
    "context": "Title: Revisiting Automatic Question Summarization Evaluation in the Biomedical Domain. (arXiv:2303.10328v1 [cs.CL])\nAbstract: Automatic evaluation metrics have been facilitating the rapid development of automatic summarization methods by providing instant and fair assessments of the quality of summaries. Most metrics have been developed for the general domain, especially news and meeting notes, or other language-generation tasks. However, these metrics are applied to evaluate summarization systems in different domains, such as biomedical question summarization. To better understand whether commonly used evaluation metrics are capable of evaluating automatic summarization in the biomedical domain, we conduct human evaluations of summarization quality from four different aspects of a biomedical question summarization task. Based on human judgments, we identify different noteworthy features for current automatic metrics and summarization systems as well. We also release a dataset of our human annotations to aid the research of summarization evaluation metrics in the biomedical domain.",
    "path": "papers/23/03/2303.10328.json",
    "total_tokens": 839,
    "translated_title": "重新审视生物医学领域自动问题摘要的评估",
    "translated_abstract": "自动化评估指标通过提供快速且公正的摘要质量评估，促进了摘要方法的快速发展。然而，这些指标大多是为一般领域，特别是新闻和会议记录，或其他语言生成任务开发的。这些指标被应用于评估在不同领域中的摘要系统，例如生物医学问题摘要。为了更好地了解常用的评估指标是否能够评估生物医学领域中的自动化摘要，我们进行了人工评估，并从生物医学问题摘要任务的四个不同方面评估了摘要质量。根据人类评判，我们发现当前自动评估指标和摘要系统存在不同值得关注的特征。我们还发布了我们的人工注释数据集，以促进生物医学领域摘要评估指标的研究。",
    "tldr": "本研究重新审视了生物医学领域中自动问题摘要的评估方法，通过人工评估发现当前自动评估指标和摘要系统存在不同值得关注的特征，并发布了一个数据集以促进未来的研究。",
    "en_tdlr": "This study revisits the evaluation of automatic question summarization in the biomedical domain and identifies noteworthy features for current metrics and systems through human evaluations, also releasing a dataset for future research."
}