{
    "title": "One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale. (arXiv:2303.06555v2 [cs.LG] UPDATED)",
    "abstract": "This paper proposes a unified diffusion framework (dubbed UniDiffuser) to fit all distributions relevant to a set of multi-modal data in one model. Our key insight is -- learning diffusion models for marginal, conditional, and joint distributions can be unified as predicting the noise in the perturbed data, where the perturbation levels (i.e. timesteps) can be different for different modalities. Inspired by the unified view, UniDiffuser learns all distributions simultaneously with a minimal modification to the original diffusion model -perturbs data in all modalities instead of a single modality, inputs individual timesteps in different modalities, and predicts the noise of all modalities instead of a single modality. UniDiffuser is parameterized by a transformer for diffusion models to handle input types of different modalities. Implemented on large-scale paired image-text data, UniDiffuser is able to perform image, text, text-to-image, image-to-text, and image-text pair generation ",
    "link": "http://arxiv.org/abs/2303.06555",
    "context": "Title: One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale. (arXiv:2303.06555v2 [cs.LG] UPDATED)\nAbstract: This paper proposes a unified diffusion framework (dubbed UniDiffuser) to fit all distributions relevant to a set of multi-modal data in one model. Our key insight is -- learning diffusion models for marginal, conditional, and joint distributions can be unified as predicting the noise in the perturbed data, where the perturbation levels (i.e. timesteps) can be different for different modalities. Inspired by the unified view, UniDiffuser learns all distributions simultaneously with a minimal modification to the original diffusion model -perturbs data in all modalities instead of a single modality, inputs individual timesteps in different modalities, and predicts the noise of all modalities instead of a single modality. UniDiffuser is parameterized by a transformer for diffusion models to handle input types of different modalities. Implemented on large-scale paired image-text data, UniDiffuser is able to perform image, text, text-to-image, image-to-text, and image-text pair generation ",
    "path": "papers/23/03/2303.06555.json",
    "total_tokens": 874,
    "translated_title": "多模态扩散中的一个Transformer适应所有分布",
    "translated_abstract": "本文提出了一个统一的扩散框架(UniDiffuser)，用于在一个模型中拟合与一组多模态数据相关的所有分布。我们的关键见解是：针对边缘、条件和联合分布学习扩散模型可以被统一为预测扰动数据中的噪声，其中对不同模态的扰动级别（即时间步长）可能是不同的。在统一观点的启发下，UniDiffuser使用了最小的修改来同时学习所有分布，即对所有模态扰动数据，输入不同模态中的单独时间步长，并预测所有模态的噪声而不是单一模态的噪声。UniDiffuser是使用Transformer参数化的扩散模型，以处理不同模态的输入类型。在大规模的图文配对数据上实现，UniDiffuser能够执行图像、文本、文本到图像、图像到文本、图像文本对的生成。",
    "tldr": "本论文提出了UniDiffuser框架，采用一个Transformer模型来统一处理多模态数据的分布拟合问题，覆盖了边缘、条件和联合分布，并且能够在图文配对数据上实现多种生成任务。",
    "en_tdlr": "This paper proposes UniDiffuser, a framework using a Transformer model to unify the distribution fitting problem for multi-modal data, covering marginal, conditional and joint distributions, and able to perform various generation tasks on image-text paired data."
}