{
    "title": "Optimal Input Gain: All You Need to Supercharge a Feed-Forward Neural Network. (arXiv:2303.17732v1 [cs.LG])",
    "abstract": "Linear transformation of the inputs alters the training performance of feed-forward networks that are otherwise equivalent. However, most linear transforms are viewed as a pre-processing operation separate from the actual training. Starting from equivalent networks, it is shown that pre-processing inputs using linear transformation are equivalent to multiplying the negative gradient matrix with an autocorrelation matrix per training iteration. Second order method is proposed to find the autocorrelation matrix that maximizes learning in a given iteration. When the autocorrelation matrix is diagonal, the method optimizes input gains. This optimal input gain (OIG) approach is used to improve two first-order two-stage training algorithms, namely back-propagation (BP) and hidden weight optimization (HWO), which alternately update the input weights and solve linear equations for output weights. Results show that the proposed OIG approach greatly enhances the performance of the first-order al",
    "link": "http://arxiv.org/abs/2303.17732",
    "context": "Title: Optimal Input Gain: All You Need to Supercharge a Feed-Forward Neural Network. (arXiv:2303.17732v1 [cs.LG])\nAbstract: Linear transformation of the inputs alters the training performance of feed-forward networks that are otherwise equivalent. However, most linear transforms are viewed as a pre-processing operation separate from the actual training. Starting from equivalent networks, it is shown that pre-processing inputs using linear transformation are equivalent to multiplying the negative gradient matrix with an autocorrelation matrix per training iteration. Second order method is proposed to find the autocorrelation matrix that maximizes learning in a given iteration. When the autocorrelation matrix is diagonal, the method optimizes input gains. This optimal input gain (OIG) approach is used to improve two first-order two-stage training algorithms, namely back-propagation (BP) and hidden weight optimization (HWO), which alternately update the input weights and solve linear equations for output weights. Results show that the proposed OIG approach greatly enhances the performance of the first-order al",
    "path": "papers/23/03/2303.17732.json",
    "total_tokens": 823,
    "translated_title": "最优输入增益：超级前馈神经网络所需的全部。",
    "translated_abstract": "输入的线性转换改变了等效的前馈网络的训练性能。然而，大多数线性变换被视为与实际训练分离的预处理操作。从等效网络开始，通过线性转换对输入进行预处理等效于在每次训练迭代中将负梯度矩阵与自相关矩阵相乘。提出了一个二阶方法，用于找到在给定迭代中最大化学习的自相关矩阵。当自相关矩阵为对角线矩阵时，该方法优化了输入增益。该最优输入增益（OIG）方法用于改进两个一阶二级训练算法，即反向传播（BP）和隐藏权重优化（HWO），这两种算法交替更新输入权重并解线性方程以确定输出权重。结果证明，所提出的OIG方法极大地提高了第一顺序算法的性能。",
    "tldr": "通过优化输入增益，可以显著提高前馈神经网络的性能，特别是在使用反向传播和隐藏权重优化等算法时。",
    "en_tdlr": "By optimizing input gain, significant performance improvements can be achieved in feed-forward neural networks, especially when using algorithms such as back-propagation and hidden weight optimization."
}