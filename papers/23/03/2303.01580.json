{
    "title": "Mixture of Soft Prompts for Controllable Data Generation. (arXiv:2303.01580v2 [cs.CL] UPDATED)",
    "abstract": "Large language models (LLMs) effectively generate fluent text when the target output follows natural language patterns. However, structured prediction tasks confine the output format to a limited ontology, causing even very large models to struggle since they were never trained with such restrictions in mind. The difficulty of using LLMs for direct prediction is exacerbated in few-shot learning scenarios, which commonly arise due to domain shift and resource limitations. We flip the problem on its head by leveraging the LLM as a tool for data augmentation rather than direct prediction. Our proposed Mixture of Soft Prompts (MSP) serves as a parameter-efficient procedure for generating data in a controlled manner. Denoising mechanisms are further applied to improve the quality of synthesized data. Automatic metrics show our method is capable of producing diverse and natural text, while preserving label semantics. Moreover, MSP achieves state-of-the-art results on three benchmarks when co",
    "link": "http://arxiv.org/abs/2303.01580",
    "context": "Title: Mixture of Soft Prompts for Controllable Data Generation. (arXiv:2303.01580v2 [cs.CL] UPDATED)\nAbstract: Large language models (LLMs) effectively generate fluent text when the target output follows natural language patterns. However, structured prediction tasks confine the output format to a limited ontology, causing even very large models to struggle since they were never trained with such restrictions in mind. The difficulty of using LLMs for direct prediction is exacerbated in few-shot learning scenarios, which commonly arise due to domain shift and resource limitations. We flip the problem on its head by leveraging the LLM as a tool for data augmentation rather than direct prediction. Our proposed Mixture of Soft Prompts (MSP) serves as a parameter-efficient procedure for generating data in a controlled manner. Denoising mechanisms are further applied to improve the quality of synthesized data. Automatic metrics show our method is capable of producing diverse and natural text, while preserving label semantics. Moreover, MSP achieves state-of-the-art results on three benchmarks when co",
    "path": "papers/23/03/2303.01580.json",
    "total_tokens": 911,
    "translated_title": "可控数据生成的软提示混合模型",
    "translated_abstract": "大型语言模型（LLM）在目标输出遵循自然语言模式时有效地生成流畅的文本。然而，结构化预测任务将输出格式限制在有限的本体上，导致即使是非常大的模型也难以应对，因为它们从未被训练在考虑这种限制的情况下。在少样本学习场景中，LLM的直接预测问题变得更加困难，这常常是由于领域移位和资源限制造成的。我们通过将LLM作为数据增强的工具而不是直接预测，从而颠覆了这个问题。我们提出的软提示混合模型（MSP）是一种参数高效的生成数据的方法。进一步应用去噪机制来提高合成数据的质量。自动评估指标显示我们的方法能够产生多样且自然的文本，同时保留标签语义。此外，MSP在三个基准测试中取得了最先进的结果。",
    "tldr": "这项研究提出了一种称为软提示混合模型（MSP）的方法，将大型语言模型（LLM）用于数据增强而不是直接预测。实验结果显示，MSP能够生成多样且自然的文本，同时保留标签语义，并在三个基准测试中取得了最先进的结果。"
}