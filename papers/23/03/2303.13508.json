{
    "title": "DreamBooth3D: Subject-Driven Text-to-3D Generation. (arXiv:2303.13508v1 [cs.CV])",
    "abstract": "We present DreamBooth3D, an approach to personalize text-to-3D generative models from as few as 3-6 casually captured images of a subject. Our approach combines recent advances in personalizing text-to-image models (DreamBooth) with text-to-3D generation (DreamFusion). We find that naively combining these methods fails to yield satisfactory subject-specific 3D assets due to personalized text-to-image models overfitting to the input viewpoints of the subject. We overcome this through a 3-stage optimization strategy where we jointly leverage the 3D consistency of neural radiance fields together with the personalization capability of text-to-image models. Our method can produce high-quality, subject-specific 3D assets with text-driven modifications such as novel poses, colors and attributes that are not seen in any of the input images of the subject.",
    "link": "http://arxiv.org/abs/2303.13508",
    "context": "Title: DreamBooth3D: Subject-Driven Text-to-3D Generation. (arXiv:2303.13508v1 [cs.CV])\nAbstract: We present DreamBooth3D, an approach to personalize text-to-3D generative models from as few as 3-6 casually captured images of a subject. Our approach combines recent advances in personalizing text-to-image models (DreamBooth) with text-to-3D generation (DreamFusion). We find that naively combining these methods fails to yield satisfactory subject-specific 3D assets due to personalized text-to-image models overfitting to the input viewpoints of the subject. We overcome this through a 3-stage optimization strategy where we jointly leverage the 3D consistency of neural radiance fields together with the personalization capability of text-to-image models. Our method can produce high-quality, subject-specific 3D assets with text-driven modifications such as novel poses, colors and attributes that are not seen in any of the input images of the subject.",
    "path": "papers/23/03/2303.13508.json",
    "total_tokens": 907,
    "translated_title": "DreamBooth3D：主体驱动的文本到3D生成",
    "translated_abstract": "我们提出了DreamBooth3D方法，该方法可以从3-6个随意拍摄的主体图像个性化生成文本到3D模型。我们的方法将个性化文本到图像模型(DreamBooth)与文本到3D生成(DreamFusion)的最新进展相结合。我们发现，简单地将这些方法组合起来无法产生令人满意的主体特定的3D素材，因为个性化的文本到图像模型会过度拟合主体图像的输入视角。我们通过三阶段的优化策略解决了这个问题，其中我们同时利用了神经辐射场的3D一致性和文本到图像模型的个性化能力。我们的方法可以产生高质量、主体特定的3D素材，具有文本驱动的修改，如新颖的姿势、颜色和属性，这些修改在主体的任何输入图像中都没有看到。",
    "tldr": "DreamBooth3D是一种可从3-6张图片中生成主体特定3D素材的方法，通过结合文本到图像模型和文本到3D生成模型，使用一种三阶段的优化策略来产生高质量3D素材。",
    "en_tdlr": "DreamBooth3D is a subject-driven text-to-3D generation approach that can personalize 3D assets from as few as 3-6 casually captured images of a subject, by combining recent advances in personalizing text-to-image models and text-to-3D generation. A three-stage optimization strategy is utilized to overcome the overfitting issue of personalized text-to-image models, resulting in subject-specific 3D assets with text-driven modifications such as novel poses, colors and attributes."
}