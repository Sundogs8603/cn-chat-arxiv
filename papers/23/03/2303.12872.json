{
    "title": "Human Uncertainty in Concept-Based AI Systems. (arXiv:2303.12872v1 [cs.HC])",
    "abstract": "Placing a human in the loop may abate the risks of deploying AI systems in safety-critical settings (e.g., a clinician working with a medical AI system). However, mitigating risks arising from human error and uncertainty within such human-AI interactions is an important and understudied issue. In this work, we study human uncertainty in the context of concept-based models, a family of AI systems that enable human feedback via concept interventions where an expert intervenes on human-interpretable concepts relevant to the task. Prior work in this space often assumes that humans are oracles who are always certain and correct. Yet, real-world decision-making by humans is prone to occasional mistakes and uncertainty. We study how existing concept-based models deal with uncertain interventions from humans using two novel datasets: UMNIST, a visual dataset with controlled simulated uncertainty based on the MNIST dataset, and CUB-S, a relabeling of the popular CUB concept dataset with rich, d",
    "link": "http://arxiv.org/abs/2303.12872",
    "context": "Title: Human Uncertainty in Concept-Based AI Systems. (arXiv:2303.12872v1 [cs.HC])\nAbstract: Placing a human in the loop may abate the risks of deploying AI systems in safety-critical settings (e.g., a clinician working with a medical AI system). However, mitigating risks arising from human error and uncertainty within such human-AI interactions is an important and understudied issue. In this work, we study human uncertainty in the context of concept-based models, a family of AI systems that enable human feedback via concept interventions where an expert intervenes on human-interpretable concepts relevant to the task. Prior work in this space often assumes that humans are oracles who are always certain and correct. Yet, real-world decision-making by humans is prone to occasional mistakes and uncertainty. We study how existing concept-based models deal with uncertain interventions from humans using two novel datasets: UMNIST, a visual dataset with controlled simulated uncertainty based on the MNIST dataset, and CUB-S, a relabeling of the popular CUB concept dataset with rich, d",
    "path": "papers/23/03/2303.12872.json",
    "total_tokens": 883,
    "translated_title": "概念驱动的AI系统中的人类不确定性",
    "translated_abstract": "在安全关键领域中部署AI系统（如医疗AI系统与临床医生一起工作）时，将人类放入其中可能会减轻一些风险。然而，缓解人间误差和不确定因素在此类人工智能交互中引起的风险，是一个重要的且未被研究充分的问题。在本文中，我们研究了在概念驱动模型中人类不确定性的问题，这是一类在AI系统中启用概念干预功能的模型。该功能是指在与任务相关的人类可解释概念上，专家对其进行干预以获得人类反馈。之前的工作已经对此进行了研究，但通常假设人类是预言家，总是确定和正确的。然而，实际中人类的决策过程往往也会出现偶尔的错误和不确定性。我们通过两个新型数据集（UMNIST和CUB-S）探讨了现有的概念驱动模型如何处理来自人类的不确定干预。",
    "tldr": "本研究探讨了人类不确定性对概念驱动AI系统的影响，通过控制数据集的干扰因素，分析了现有模型的处理方法。",
    "en_tdlr": "This study examines the impact of human uncertainty on concept-based AI systems and analyzes the handling methods of existing models through controlled interference factors in datasets."
}