{
    "title": "Policy Mirror Descent Inherently Explores Action Space. (arXiv:2303.04386v2 [cs.LG] UPDATED)",
    "abstract": "Explicit exploration in the action space was assumed to be indispensable for online policy gradient methods to avoid a drastic degradation in sample complexity, for solving general reinforcement learning problems over finite state and action spaces. In this paper, we establish for the first time an $\\tilde{\\mathcal{O}}(1/\\epsilon^2)$ sample complexity for online policy gradient methods without incorporating any exploration strategies. The essential development consists of two new on-policy evaluation operators and a novel analysis of the stochastic policy mirror descent method (SPMD). SPMD with the first evaluation operator, called value-based estimation, tailors to the Kullback-Leibler divergence. Provided the Markov chains on the state space of generated policies are uniformly mixing with non-diminishing minimal visitation measure, an $\\tilde{\\mathcal{O}}(1/\\epsilon^2)$ sample complexity is obtained with a linear dependence on the size of the action space. SPMD with the second evalua",
    "link": "http://arxiv.org/abs/2303.04386",
    "context": "Title: Policy Mirror Descent Inherently Explores Action Space. (arXiv:2303.04386v2 [cs.LG] UPDATED)\nAbstract: Explicit exploration in the action space was assumed to be indispensable for online policy gradient methods to avoid a drastic degradation in sample complexity, for solving general reinforcement learning problems over finite state and action spaces. In this paper, we establish for the first time an $\\tilde{\\mathcal{O}}(1/\\epsilon^2)$ sample complexity for online policy gradient methods without incorporating any exploration strategies. The essential development consists of two new on-policy evaluation operators and a novel analysis of the stochastic policy mirror descent method (SPMD). SPMD with the first evaluation operator, called value-based estimation, tailors to the Kullback-Leibler divergence. Provided the Markov chains on the state space of generated policies are uniformly mixing with non-diminishing minimal visitation measure, an $\\tilde{\\mathcal{O}}(1/\\epsilon^2)$ sample complexity is obtained with a linear dependence on the size of the action space. SPMD with the second evalua",
    "path": "papers/23/03/2303.04386.json",
    "total_tokens": 1046,
    "translated_title": "策略镜像下降方法固有地探索行动空间",
    "translated_abstract": "本文研究了如何在没有加入探索策略的情况下，采用在线策略梯度方法来解决有限状态和行动空间下的通用增强学习问题，原以为在行动空间内进行显式探索是不可或缺的，以避免样本复杂度的剧烈降低。本文首次确定了在线策略梯度方法的$\\tilde{\\mathcal{O}}(1/\\epsilon^2)$的样本复杂度，并提出了两个新的策略评估算子和一种新的分析随机策略镜像下降方法（ SPMD）。结果表明，SPMD通过其在线评估算子固有地探索行动空间，明确的行动空间探索并不一定是策略梯度方法实现最佳样本复杂度的必要条件。其中一个评估算子称为基于价值函数的估计，另一个评估算子称为基于策略的估计。",
    "tldr": "本文研究了在线策略梯度方法如何固有地探索行动空间，并首次确定了不需要探索策略的情况下，这种方法的最优样本复杂度，进一步展示了明确的行动空间探索并不一定是必要条件。",
    "en_tdlr": "This paper investigates how to explore the action space inherently using policy mirror descent without explicit exploration strategies, and for the first time establishes the optimal sample complexity of this method without the assumption of explicit exploration. It shows that explicit exploration in the action space is not necessarily required for policy gradient methods to achieve optimal sample complexity."
}