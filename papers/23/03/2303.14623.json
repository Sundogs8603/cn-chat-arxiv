{
    "title": "Inverse Reinforcement Learning without Reinforcement Learning. (arXiv:2303.14623v2 [cs.LG] UPDATED)",
    "abstract": "Inverse Reinforcement Learning (IRL) is a powerful set of techniques for imitation learning that aims to learn a reward function that rationalizes expert demonstrations. Unfortunately, traditional IRL methods suffer from a computational weakness: they require repeatedly solving a hard reinforcement learning (RL) problem as a subroutine. This is counter-intuitive from the viewpoint of reductions: we have reduced the easier problem of imitation learning to repeatedly solving the harder problem of RL. Another thread of work has proved that access to the side-information of the distribution of states where a strong policy spends time can dramatically reduce the sample and computational complexities of solving an RL problem. In this work, we demonstrate for the first time a more informed imitation learning reduction where we utilize the state distribution of the expert to alleviate the global exploration component of the RL subroutine, providing an exponential speedup in theory. In practice",
    "link": "http://arxiv.org/abs/2303.14623",
    "context": "Title: Inverse Reinforcement Learning without Reinforcement Learning. (arXiv:2303.14623v2 [cs.LG] UPDATED)\nAbstract: Inverse Reinforcement Learning (IRL) is a powerful set of techniques for imitation learning that aims to learn a reward function that rationalizes expert demonstrations. Unfortunately, traditional IRL methods suffer from a computational weakness: they require repeatedly solving a hard reinforcement learning (RL) problem as a subroutine. This is counter-intuitive from the viewpoint of reductions: we have reduced the easier problem of imitation learning to repeatedly solving the harder problem of RL. Another thread of work has proved that access to the side-information of the distribution of states where a strong policy spends time can dramatically reduce the sample and computational complexities of solving an RL problem. In this work, we demonstrate for the first time a more informed imitation learning reduction where we utilize the state distribution of the expert to alleviate the global exploration component of the RL subroutine, providing an exponential speedup in theory. In practice",
    "path": "papers/23/03/2303.14623.json",
    "total_tokens": 948,
    "translated_title": "无需强化学习的逆强化学习",
    "translated_abstract": "逆强化学习 (IRL) 是一种强大的模仿学习技术，旨在学习合乎逻辑的专家演示的奖励函数。然而，传统的IRL方法存在计算上的弱点：它们需要将解决难度高的强化学习（RL）问题作为子例程进行反复求解。这与归约的观点相矛盾：我们已将模仿学习的较易问题归约为反复解决强化学习的更难问题。另一方面的工作证明，访问强策略花费时间的状态分布的侧面信息可以大大降低解决RL问题的样本和计算复杂度。在本研究中，我们首次展示了一种更加明智的模仿学习简化方法，利用专家的状态分布来缓解RL子例程的全局探索部分，理论上提供了指数级的加速。实际上，我们的算法在多个基准任务中在样本复杂度和时间复杂度方面都显著优于现有的IRL方法。",
    "tldr": "该论文提出了一种新的逆强化学习简化方法，通过利用专家的状态分布来减少强化学习子例程的全局探索部分，实现了指数级的加速，大大提高了样本复杂度和时间复杂度的效果。",
    "en_tdlr": "This paper proposes a new simplification method for inverse reinforcement learning, which utilizes the expert's state distribution to reduce the global exploration component of the reinforcement learning subroutine, achieving an exponential speedup and significantly improving the sample and time complexities on several benchmark tasks."
}