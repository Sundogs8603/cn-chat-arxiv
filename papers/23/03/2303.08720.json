{
    "title": "Practicality of generalization guarantees for unsupervised domain adaptation with neural networks. (arXiv:2303.08720v1 [cs.LG])",
    "abstract": "Understanding generalization is crucial to confidently engineer and deploy machine learning models, especially when deployment implies a shift in the data domain. For such domain adaptation problems, we seek generalization bounds which are tractably computable and tight. If these desiderata can be reached, the bounds can serve as guarantees for adequate performance in deployment. However, in applications where deep neural networks are the models of choice, deriving results which fulfill these remains an unresolved challenge; most existing bounds are either vacuous or has non-estimable terms, even in favorable conditions. In this work, we evaluate existing bounds from the literature with potential to satisfy our desiderata on domain adaptation image classification tasks, where deep neural networks are preferred. We find that all bounds are vacuous and that sample generalization terms account for much of the observed looseness, especially when these terms interact with measures of domain",
    "link": "http://arxiv.org/abs/2303.08720",
    "context": "Title: Practicality of generalization guarantees for unsupervised domain adaptation with neural networks. (arXiv:2303.08720v1 [cs.LG])\nAbstract: Understanding generalization is crucial to confidently engineer and deploy machine learning models, especially when deployment implies a shift in the data domain. For such domain adaptation problems, we seek generalization bounds which are tractably computable and tight. If these desiderata can be reached, the bounds can serve as guarantees for adequate performance in deployment. However, in applications where deep neural networks are the models of choice, deriving results which fulfill these remains an unresolved challenge; most existing bounds are either vacuous or has non-estimable terms, even in favorable conditions. In this work, we evaluate existing bounds from the literature with potential to satisfy our desiderata on domain adaptation image classification tasks, where deep neural networks are preferred. We find that all bounds are vacuous and that sample generalization terms account for much of the observed looseness, especially when these terms interact with measures of domain",
    "path": "papers/23/03/2303.08720.json",
    "total_tokens": 962,
    "translated_title": "神经网络无监督域适应的泛化保证的实用性研究",
    "translated_abstract": "理解泛化对于自信地设计和部署机器学习模型至关重要，特别是当部署意味着数据域的转移时。对于这样的域适应问题，我们寻求可计算和紧密的泛化界限。如果可以实现这些要求，这些界限可以作为部署中充足性能的保证。然而，在深度神经网络是首选模型的应用中，推导出满足这些要求的结果仍是一项未解决的挑战；大多数现有的界限要么是空泛的，要么有不可估计的术语，即使在有利条件下也是如此。在本文中，我们评估了现有文献中有潜力满足我们要求的域适应图像分类任务的界限，深度神经网络是首选。我们发现所有界限都是空泛的，并且样本泛化术语占据了观察到的松弛程度的很大部分，特别是当这些术语与域的度量互动时。",
    "tldr": "研究评估了现有文献中有潜力满足我们要求的域适应图像分类任务的界限，发现所有界限都是空泛的，样本泛化术语占据了观察到的松弛程度的很大部分，特别是当这些术语与域的度量互动时。",
    "en_tdlr": "This paper evaluates existing bounds from the literature on domain adaptation image classification tasks with deep neural networks for potential to satisfy tractably computable and tight generalization bounds. The study finds that all bounds are vacuous and that sample generalization terms account for much of the observed looseness, especially when these terms interact with measures of domain."
}