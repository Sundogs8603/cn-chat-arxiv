{
    "title": "A Contrastive Learning Scheme with Transformer Innate Patches. (arXiv:2303.14806v2 [cs.CV] UPDATED)",
    "abstract": "This paper presents Contrastive Transformer, a contrastive learning scheme using the Transformer innate patches. Contrastive Transformer enables existing contrastive learning techniques, often used for image classification, to benefit dense downstream prediction tasks such as semantic segmentation. The scheme performs supervised patch-level contrastive learning, selecting the patches based on the ground truth mask, subsequently used for hard-negative and hard-positive sampling. The scheme applies to all vision-transformer architectures, is easy to implement, and introduces minimal additional memory footprint. Additionally, the scheme removes the need for huge batch sizes, as each patch is treated as an image.  We apply and test Contrastive Transformer for the case of aerial image segmentation, known for low-resolution data, large class imbalance, and similar semantic classes. We perform extensive experiments to show the efficacy of the Contrastive Transformer scheme on the ISPRS Potsda",
    "link": "http://arxiv.org/abs/2303.14806",
    "context": "Title: A Contrastive Learning Scheme with Transformer Innate Patches. (arXiv:2303.14806v2 [cs.CV] UPDATED)\nAbstract: This paper presents Contrastive Transformer, a contrastive learning scheme using the Transformer innate patches. Contrastive Transformer enables existing contrastive learning techniques, often used for image classification, to benefit dense downstream prediction tasks such as semantic segmentation. The scheme performs supervised patch-level contrastive learning, selecting the patches based on the ground truth mask, subsequently used for hard-negative and hard-positive sampling. The scheme applies to all vision-transformer architectures, is easy to implement, and introduces minimal additional memory footprint. Additionally, the scheme removes the need for huge batch sizes, as each patch is treated as an image.  We apply and test Contrastive Transformer for the case of aerial image segmentation, known for low-resolution data, large class imbalance, and similar semantic classes. We perform extensive experiments to show the efficacy of the Contrastive Transformer scheme on the ISPRS Potsda",
    "path": "papers/23/03/2303.14806.json",
    "total_tokens": 868,
    "translated_title": "基于Transformer内在补丁的对比学习方案",
    "translated_abstract": "本文提出了一种使用Transformer内在补丁的对比学习方案，称为对比Transformer。对比Transformer能够使得现有的对比学习技术在密集下游预测任务（如语义分割）中受益。该方案通过监督补丁级对比学习，在选择补丁时基于地面真值掩膜，后续用于难负样本和难正样本采样。该方案适用于所有视觉Transformer架构，易于实现，并且引入的额外内存开销很小。此外，该方案无需大规模批处理，因为每个补丁被视为一张图像。我们将对比Transformer应用于航空图像分割的案例中，这种情况下存在低分辨率数据、大类别不平衡和相似语义类别。我们进行了大量实验证明了对比Transformer方案的有效性。",
    "tldr": "本文提出了一种使用Transformer内在补丁的对比学习方案，可在密集下游预测任务中受益。该方案适用于所有视觉Transformer架构，易于实现，并且无需大规模批处理。对比Transformer方案在航空图像分割中有效。"
}