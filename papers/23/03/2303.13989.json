{
    "title": "Paraphrase Detection: Human vs. Machine Content. (arXiv:2303.13989v1 [cs.CL])",
    "abstract": "The growing prominence of large language models, such as GPT-4 and ChatGPT, has led to increased concerns over academic integrity due to the potential for machine-generated content and paraphrasing. Although studies have explored the detection of human- and machine-paraphrased content, the comparison between these types of content remains underexplored. In this paper, we conduct a comprehensive analysis of various datasets commonly employed for paraphrase detection tasks and evaluate an array of detection methods. Our findings highlight the strengths and limitations of different detection methods in terms of performance on individual datasets, revealing a lack of suitable machine-generated datasets that can be aligned with human expectations. Our main finding is that human-authored paraphrases exceed machine-generated ones in terms of difficulty, diversity, and similarity implying that automatically generated texts are not yet on par with human-level performance. Transformers emerged a",
    "link": "http://arxiv.org/abs/2303.13989",
    "context": "Title: Paraphrase Detection: Human vs. Machine Content. (arXiv:2303.13989v1 [cs.CL])\nAbstract: The growing prominence of large language models, such as GPT-4 and ChatGPT, has led to increased concerns over academic integrity due to the potential for machine-generated content and paraphrasing. Although studies have explored the detection of human- and machine-paraphrased content, the comparison between these types of content remains underexplored. In this paper, we conduct a comprehensive analysis of various datasets commonly employed for paraphrase detection tasks and evaluate an array of detection methods. Our findings highlight the strengths and limitations of different detection methods in terms of performance on individual datasets, revealing a lack of suitable machine-generated datasets that can be aligned with human expectations. Our main finding is that human-authored paraphrases exceed machine-generated ones in terms of difficulty, diversity, and similarity implying that automatically generated texts are not yet on par with human-level performance. Transformers emerged a",
    "path": "papers/23/03/2303.13989.json",
    "total_tokens": 936,
    "translated_title": "人类与机器内容的释义检测比较研究",
    "translated_abstract": "大型语言模型（如 GPT-4 和 ChatGPT）日益重要，但也引起了学术诚信问题，因为存在机器生成的内容和释义。虽然有研究探讨了人类和机器释义内容的检测，但这些类型内容之间的比较仍未得到广泛研究。本文对常用的各种数据集进行了全面分析，并评估了各种检测方法。我们的研究发现，相对于机器生成的数据集缺乏，现有的评估方法在不同数据集上表现各有优劣。我们的主要发现是，人类释义超过机器释义的难度、多样性和相似性，暗示自动生成的文本还没有达到人类水平。Transformer 是最有效的检测方法，BERT 和 RoBERTa 变体在所有数据集上都取得了显著的结果，而基于 SVM 的方法则落后于它们。",
    "tldr": "本研究分析了不同数据集和检测方法，对人类和机器释义内容进行了比较，发现人类释义难度、多样性和相似性超过机器释义，机器生成的数据集不足。Transformer 是最有效的检测方法，而 SVM-based 方法不及 BERT 和 RoBERTa 变体。",
    "en_tdlr": "This study conducted a comparative analysis of human- and machine-generated paraphrased content using various datasets and detection methods. The findings suggest that automatically generated texts are not yet on par with human-level performance in terms of difficulty, diversity, and similarity. The Transformer model emerged as the most effective detection method, with BERT and RoBERTa variants achieving notable results across all datasets while SVM-based methods lagged behind."
}