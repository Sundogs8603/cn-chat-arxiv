{
    "title": "TOFA: Transfer-Once-for-All. (arXiv:2303.15485v1 [cs.LG])",
    "abstract": "Weight-sharing neural architecture search aims to optimize a configurable neural network model (supernet) for a variety of deployment scenarios across many devices with different resource constraints. Existing approaches use evolutionary search to extract a number of models from a supernet trained on a very large data set, and then fine-tune the extracted models on the typically small, real-world data set of interest. The computational cost of training thus grows linearly with the number of different model deployment scenarios. Hence, we propose Transfer-Once-For-All (TOFA) for supernet-style training on small data sets with constant computational training cost over any number of edge deployment scenarios. Given a task, TOFA obtains custom neural networks, both the topology and the weights, optimized for any number of edge deployment scenarios. To overcome the challenges arising from small data, TOFA utilizes a unified semi-supervised training loss to simultaneously train all subnets w",
    "link": "http://arxiv.org/abs/2303.15485",
    "context": "Title: TOFA: Transfer-Once-for-All. (arXiv:2303.15485v1 [cs.LG])\nAbstract: Weight-sharing neural architecture search aims to optimize a configurable neural network model (supernet) for a variety of deployment scenarios across many devices with different resource constraints. Existing approaches use evolutionary search to extract a number of models from a supernet trained on a very large data set, and then fine-tune the extracted models on the typically small, real-world data set of interest. The computational cost of training thus grows linearly with the number of different model deployment scenarios. Hence, we propose Transfer-Once-For-All (TOFA) for supernet-style training on small data sets with constant computational training cost over any number of edge deployment scenarios. Given a task, TOFA obtains custom neural networks, both the topology and the weights, optimized for any number of edge deployment scenarios. To overcome the challenges arising from small data, TOFA utilizes a unified semi-supervised training loss to simultaneously train all subnets w",
    "path": "papers/23/03/2303.15485.json",
    "total_tokens": 948,
    "translated_title": "TOFA：一次转移全能的神经架构搜索",
    "translated_abstract": "权重共享神经架构搜索旨在为不同资源约束的许多设备优化可配置的神经网络模型（超网）以满足各种部署场景。现有方法使用进化搜索从在非常大的数据集上训练的超网中提取多个模型，然后对感兴趣的通常很小的真实数据集上提取的模型进行微调。因此，训练的计算成本随着不同模型部署方案的数量而线性增长。因此，我们提出了Transfer-Once-For-All（TOFA），用于在小数据集上进行超网风格的训练，在任意数量的边缘部署方案上具有恒定的计算训练成本。给定任务，TOFA获得定制的神经网络，优化任意数量的边缘部署方案的拓扑和权重。为了克服小数据带来的挑战，TOFA利用统一的半监督训练损失同时训练超网内的所有子网。",
    "tldr": "TOFA使用权重共享来进行神经架构搜索，以优化超网以适应各种设备的各种部署情况。与现有方法不同，TOFA在小数据集上进行训练，计算训练成本与部署方案数量无关。TOFA使用统一的半监督训练方法来解决小数据集带来的挑战。",
    "en_tdlr": "TOFA utilizes weight-sharing to perform neural architecture search and optimize a supernet for a variety of deployment scenarios across many devices. Unlike existing methods, TOFA trains on small datasets with constant training cost regardless of the number of deployment scenarios. TOFA employs a unified semi-supervised training approach to overcome challenges posed by small datasets."
}