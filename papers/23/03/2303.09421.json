{
    "title": "Team SheffieldVeraAI at SemEval-2023 Task 3: Mono and multilingual approaches for news genre, topic and persuasion technique classification. (arXiv:2303.09421v1 [cs.CL])",
    "abstract": "This paper describes our approach for SemEval-2023 Task 3: Detecting the category, the framing, and the persuasion techniques in online news in a multi-lingual setup. For Subtask 1 (News Genre), we propose an ensemble of fully trained and adapter mBERT models which was ranked joint-first for German, and had the highest mean rank of multi-language teams. For Subtask 2 (Framing), we achieved first place in 3 languages, and the best average rank across all the languages, by using two separate ensembles: a monolingual RoBERTa-MUPPETLARGE and an ensemble of XLM-RoBERTaLARGE with adapters and task adaptive pretraining. For Subtask 3 (Persuasion Techniques), we train a monolingual RoBERTa-Base model for English and a multilingual mBERT model for the remaining languages, which achieved top 10 for all languages, including 2nd for English. For each subtask, we compare monolingual and multilingual approaches, and consider class imbalance techniques.",
    "link": "http://arxiv.org/abs/2303.09421",
    "context": "Title: Team SheffieldVeraAI at SemEval-2023 Task 3: Mono and multilingual approaches for news genre, topic and persuasion technique classification. (arXiv:2303.09421v1 [cs.CL])\nAbstract: This paper describes our approach for SemEval-2023 Task 3: Detecting the category, the framing, and the persuasion techniques in online news in a multi-lingual setup. For Subtask 1 (News Genre), we propose an ensemble of fully trained and adapter mBERT models which was ranked joint-first for German, and had the highest mean rank of multi-language teams. For Subtask 2 (Framing), we achieved first place in 3 languages, and the best average rank across all the languages, by using two separate ensembles: a monolingual RoBERTa-MUPPETLARGE and an ensemble of XLM-RoBERTaLARGE with adapters and task adaptive pretraining. For Subtask 3 (Persuasion Techniques), we train a monolingual RoBERTa-Base model for English and a multilingual mBERT model for the remaining languages, which achieved top 10 for all languages, including 2nd for English. For each subtask, we compare monolingual and multilingual approaches, and consider class imbalance techniques.",
    "path": "papers/23/03/2303.09421.json",
    "total_tokens": 997,
    "translated_title": "SemEval-2023任务3中的单语和多语言方法：Team SheffieldVeraAI在新闻类型、主题和说服技巧分类方面的表现",
    "translated_abstract": "本文描述了我们在多语言环境下应用的方法，用于SemEval-2023任务3：在在线新闻中检测类别、框架和说服技巧。 对于子任务1（新闻类型），我们提出了一个完全训练和适配器mBERT模型的集成，其在德语中排名第一，并且具有多语言团队中最高的平均排名。 对于子任务2（框架），我们使用两个单独的集成：一个单语RoBERTa-MUPPETLARGE和一个XLM-RoBERTaLARGE的集成，分别使用适配器和任务自适应预训练，在3种语言中获得第一名，并在所有语言中获得最佳平均排名。 对于子任务3（说服技巧），我们为英语训练了一个单语言RoBERTa-Base模型和一个适用于剩余语言的多语言mBERT模型，其在所有语言中均排名前10，其中英语排名第二。 对于每个子任务，我们比较了单语和多语言方法，并考虑了类别不平衡技术。",
    "tldr": "本文介绍了Team SheffieldVeraAI在SemEval-2023任务3中的表现。他们提出了用于新闻类型、框架和说服技巧分类的单语和多语言方法。该团队使用多种模型和适配器，取得了在不同语言下的好成绩。",
    "en_tdlr": "This paper describes Team SheffieldVeraAI's mono and multilingual approaches for news genre, topic and persuasion technique classification in SemEval-2023 Task 3. The team achieved top rankings with various models and adapters in different languages."
}