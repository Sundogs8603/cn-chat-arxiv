{
    "title": "Training Deep Boltzmann Networks with Sparse Ising Machines. (arXiv:2303.10728v2 [cs.ET] UPDATED)",
    "abstract": "The slowing down of Moore's law has driven the development of unconventional computing paradigms, such as specialized Ising machines tailored to solve combinatorial optimization problems. In this paper, we show a new application domain for probabilistic bit (p-bit) based Ising machines by training deep generative AI models with them. Using sparse, asynchronous, and massively parallel Ising machines we train deep Boltzmann networks in a hybrid probabilistic-classical computing setup. We use the full MNIST and Fashion MNIST (FMNIST) dataset without any downsampling and a reduced version of CIFAR-10 dataset in hardware-aware network topologies implemented in moderately sized Field Programmable Gate Arrays (FPGA). For MNIST, our machine using only 4,264 nodes (p-bits) and about 30,000 parameters achieves the same classification accuracy (90%) as an optimized software-based restricted Boltzmann Machine (RBM) with approximately 3.25 million parameters. Similar results follow for FMNIST and C",
    "link": "http://arxiv.org/abs/2303.10728",
    "context": "Title: Training Deep Boltzmann Networks with Sparse Ising Machines. (arXiv:2303.10728v2 [cs.ET] UPDATED)\nAbstract: The slowing down of Moore's law has driven the development of unconventional computing paradigms, such as specialized Ising machines tailored to solve combinatorial optimization problems. In this paper, we show a new application domain for probabilistic bit (p-bit) based Ising machines by training deep generative AI models with them. Using sparse, asynchronous, and massively parallel Ising machines we train deep Boltzmann networks in a hybrid probabilistic-classical computing setup. We use the full MNIST and Fashion MNIST (FMNIST) dataset without any downsampling and a reduced version of CIFAR-10 dataset in hardware-aware network topologies implemented in moderately sized Field Programmable Gate Arrays (FPGA). For MNIST, our machine using only 4,264 nodes (p-bits) and about 30,000 parameters achieves the same classification accuracy (90%) as an optimized software-based restricted Boltzmann Machine (RBM) with approximately 3.25 million parameters. Similar results follow for FMNIST and C",
    "path": "papers/23/03/2303.10728.json",
    "total_tokens": 948,
    "translated_title": "使用稀疏伊辛机器训练深度玻尔兹曼网络",
    "translated_abstract": "随着摩尔定律放缓，开发非传统计算范式成为趋势，其中包括专门用于解决组合优化问题的伊辛机器。本文展示了一种新的应用领域，即使用伊辛机器训练深度生成式人工智能模型。我们使用稀疏、异步和大规模并行的伊辛机器，在混合概率-经典计算环境中训练深度玻尔兹曼网络。我们在中等规模的可编程门阵列（FPGA）上实现硬件感知的网络拓扑结构，使用完整的MNIST和Fashion MNIST（FMNIST）数据集，没有任何降采样，并对CIFAR-10数据集进行了缩减。对于MNIST而言，我们的机器仅使用4,264个节点（p-bits）和大约30,000个参数，就实现了与优化的基于软件的受限玻尔兹曼机器（RBM）相同的分类准确率（90%），后续的FMNIST和CIFAR-10结果类似。",
    "tldr": "本文展示了使用稀疏伊辛机器训练深度玻尔兹曼网络的新应用领域，通过在硬件感知的网络拓扑中使用伊辛机器，我们实现了与优化的软件模型相同的分类准确率。",
    "en_tdlr": "This paper demonstrates a new application domain for Ising machines, using sparse Ising machines to train deep Boltzmann networks. By implementing hardware-aware network topologies, it achieves the same classification accuracy as optimized software models."
}