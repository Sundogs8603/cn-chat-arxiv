{
    "title": "DualMix: Unleashing the Potential of Data Augmentation for Online Class-Incremental Learning. (arXiv:2303.07864v1 [cs.LG])",
    "abstract": "Online Class-Incremental (OCI) learning has sparked new approaches to expand the previously trained model knowledge from sequentially arriving data streams with new classes. Unfortunately, OCI learning can suffer from catastrophic forgetting (CF) as the decision boundaries for old classes can become inaccurate when perturbated by new ones. Existing literature have applied the data augmentation (DA) to alleviate the model forgetting, while the role of DA in OCI has not been well understood so far. In this paper, we theoretically show that augmented samples with lower correlation to the original data are more effective in preventing forgetting. However, aggressive augmentation may also reduce the consistency between data and corresponding labels, which motivates us to exploit proper DA to boost the OCI performance and prevent the CF problem. We propose the Enhanced Mixup (EnMix) method that mixes the augmented samples and their labels simultaneously, which is shown to enhance the sample ",
    "link": "http://arxiv.org/abs/2303.07864",
    "context": "Title: DualMix: Unleashing the Potential of Data Augmentation for Online Class-Incremental Learning. (arXiv:2303.07864v1 [cs.LG])\nAbstract: Online Class-Incremental (OCI) learning has sparked new approaches to expand the previously trained model knowledge from sequentially arriving data streams with new classes. Unfortunately, OCI learning can suffer from catastrophic forgetting (CF) as the decision boundaries for old classes can become inaccurate when perturbated by new ones. Existing literature have applied the data augmentation (DA) to alleviate the model forgetting, while the role of DA in OCI has not been well understood so far. In this paper, we theoretically show that augmented samples with lower correlation to the original data are more effective in preventing forgetting. However, aggressive augmentation may also reduce the consistency between data and corresponding labels, which motivates us to exploit proper DA to boost the OCI performance and prevent the CF problem. We propose the Enhanced Mixup (EnMix) method that mixes the augmented samples and their labels simultaneously, which is shown to enhance the sample ",
    "path": "papers/23/03/2303.07864.json",
    "total_tokens": 914,
    "translated_title": "DualMix: 释放数据增强对在线类增量学习的潜力",
    "translated_abstract": "在线类增量学习(OCL)已经引发了新的方法，以向先前训练的模型知识中添加新类别的顺序到达的数据流。不幸的是，OCL学习可能会遭受灾难性遗忘(CF)，因为旧类别的决策边界在被新类别扰动时可能变得不准确。现有的文献已经应用数据增强(DA)来减轻模型遗忘，然而DA在OCI中的作用还不太清楚。我们理论上展示了与原始数据相关性较低的增强样本在防止遗忘方面更有效。然而，过度的增强也可能降低数据和相应标签之间的一致性，这促使我们利用适当的DA提高OCI性能并防止CF问题。我们提出了增强的Mixup(EnMix)方法，同时混合增强样本和其标签，显示了增强样本的效果。",
    "tldr": "本文研究了在在线类增量学习(OCL)中如何利用数据增强(DA)来防止灾难性遗忘(CF)的问题。通过提出增强的Mixup(EnMix)方法，同时混合增强样本和其标签，有效提高了OCI性能并防止CF问题。",
    "en_tdlr": "This paper investigates how to use data augmentation (DA) to prevent catastrophic forgetting (CF) in online class-incremental learning (OCI). By proposing the Enhanced Mixup (EnMix) method that mixes the augmented samples and their labels simultaneously, the OCI performance is effectively improved and the CF problem is prevented."
}