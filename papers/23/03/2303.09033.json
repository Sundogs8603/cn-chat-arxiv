{
    "title": "Only Pay for What Is Uncertain: Variance-Adaptive Thompson Sampling. (arXiv:2303.09033v1 [cs.LG])",
    "abstract": "Most bandit algorithms assume that the reward variance or its upper bound is known. While variance overestimation is usually safe and sound, it increases regret. On the other hand, an underestimated variance may lead to linear regret due to committing early to a suboptimal arm. This motivated prior works on variance-aware frequentist algorithms. We lay foundations for the Bayesian setting. In particular, we study multi-armed bandits with known and \\emph{unknown heterogeneous reward variances}, and develop Thompson sampling algorithms for both and bound their Bayes regret. Our regret bounds decrease with lower reward variances, which make learning easier. The bound for unknown reward variances captures the effect of the prior on learning reward variances and is the first of its kind. Our experiments show the superiority of variance-aware Bayesian algorithms and also highlight their robustness.",
    "link": "http://arxiv.org/abs/2303.09033",
    "context": "Title: Only Pay for What Is Uncertain: Variance-Adaptive Thompson Sampling. (arXiv:2303.09033v1 [cs.LG])\nAbstract: Most bandit algorithms assume that the reward variance or its upper bound is known. While variance overestimation is usually safe and sound, it increases regret. On the other hand, an underestimated variance may lead to linear regret due to committing early to a suboptimal arm. This motivated prior works on variance-aware frequentist algorithms. We lay foundations for the Bayesian setting. In particular, we study multi-armed bandits with known and \\emph{unknown heterogeneous reward variances}, and develop Thompson sampling algorithms for both and bound their Bayes regret. Our regret bounds decrease with lower reward variances, which make learning easier. The bound for unknown reward variances captures the effect of the prior on learning reward variances and is the first of its kind. Our experiments show the superiority of variance-aware Bayesian algorithms and also highlight their robustness.",
    "path": "papers/23/03/2303.09033.json",
    "total_tokens": 907,
    "translated_title": "只针对不确定性支付代价：方差自适应汤普森采样",
    "translated_abstract": "大多数赌博算法都假设奖励方差或其上界已知。尽管方差高估通常是安全的，但它会增加遗憾。另一方面，低估的方差可能导致由于过早地选择了次优臂而导致的线性遗憾。这激发了关于方差感知频率算法的先前工作。我们为贝叶斯设置打下基础。特别是，我们研究了具有已知和未知异质奖励方差的多臂赌博机，并为两者开发了汤普森采样算法，并限制了它们的贝叶斯遗憾。我们的遗憾界随着较低奖励方差而减少，这使得学习更加容易。未知奖励方差的边界捕捉了先验对学习奖励方差的影响，是其类型中的首个。我们的实验表明了方差感知的贝叶斯算法的优越性，同时也突出了它们的鲁棒性。",
    "tldr": "本文提出了一种针对多臂赌博机的方差自适应汤普森采样算法，通过考虑奖励方差的信息减少了遗憾，同时提高了鲁棒性",
    "en_tdlr": "This paper proposes variance-adaptive Thompson sampling for multi-armed bandits, which reduces regret and improves robustness by considering the information of reward variances."
}