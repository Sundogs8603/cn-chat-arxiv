{
    "title": "Invariant preservation in machine learned PDE solvers via error correction. (arXiv:2303.16110v1 [math.NA])",
    "abstract": "Machine learned partial differential equation (PDE) solvers trade the reliability of standard numerical methods for potential gains in accuracy and/or speed. The only way for a solver to guarantee that it outputs the exact solution is to use a convergent method in the limit that the grid spacing $\\Delta x$ and timestep $\\Delta t$ approach zero. Machine learned solvers, which learn to update the solution at large $\\Delta x$ and/or $\\Delta t$, can never guarantee perfect accuracy. Some amount of error is inevitable, so the question becomes: how do we constrain machine learned solvers to give us the sorts of errors that we are willing to tolerate? In this paper, we design more reliable machine learned PDE solvers by preserving discrete analogues of the continuous invariants of the underlying PDE. Examples of such invariants include conservation of mass, conservation of energy, the second law of thermodynamics, and/or non-negative density. Our key insight is simple: to preserve invariants,",
    "link": "http://arxiv.org/abs/2303.16110",
    "context": "Title: Invariant preservation in machine learned PDE solvers via error correction. (arXiv:2303.16110v1 [math.NA])\nAbstract: Machine learned partial differential equation (PDE) solvers trade the reliability of standard numerical methods for potential gains in accuracy and/or speed. The only way for a solver to guarantee that it outputs the exact solution is to use a convergent method in the limit that the grid spacing $\\Delta x$ and timestep $\\Delta t$ approach zero. Machine learned solvers, which learn to update the solution at large $\\Delta x$ and/or $\\Delta t$, can never guarantee perfect accuracy. Some amount of error is inevitable, so the question becomes: how do we constrain machine learned solvers to give us the sorts of errors that we are willing to tolerate? In this paper, we design more reliable machine learned PDE solvers by preserving discrete analogues of the continuous invariants of the underlying PDE. Examples of such invariants include conservation of mass, conservation of energy, the second law of thermodynamics, and/or non-negative density. Our key insight is simple: to preserve invariants,",
    "path": "papers/23/03/2303.16110.json",
    "total_tokens": 1040,
    "translated_title": "通过纠错保持不变量在机器学习PDE求解器中的应用",
    "translated_abstract": "机器学习的偏微分方程求解器在可能带来的准确性和/或速度上的潜在收益换取标准数值方法的可靠性。保证求解器输出精确解的唯一方法是在网格间距$\\Delta x$和时间步长$\\Delta t$趋近于零的极限下使用收敛方法。学会在大$\\Delta x$和/或$\\Delta t$下更新解的机器学习求解器永远无法保证完美的准确性。一定程度的误差是不可避免的，因此问题是：我们如何限制机器学习求解器以给出我们愿意容忍的错误？在本文中，我们通过保持基本偏微分方程的离散模拟中的连续不变量来设计更可靠的机器学习PDE求解器。这类不变量的示例包括质量守恒、能量守恒、热力学第二定律和（或）非负密度。我们的关键见解很简单：为了保持不变量，我们强制求解器的误差与满足相关不变量的离散函数空间正交。我们在几个示例PDE上演示了这一思想，包括非线性Burgers方程、Navier-Stokes方程和不可压缩的Euler方程。",
    "tldr": "本文研究通过保持基本偏微分方程的离散模拟中的连续不变量来设计更可靠的机器学习PDE求解器，关键在于通过纠错保持不变量。",
    "en_tdlr": "This paper studies the design of more reliable machine learned PDE solvers by preserving discrete analogues of the continuous invariants of the underlying PDE via error correction."
}