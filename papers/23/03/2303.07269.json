{
    "title": "InPL: Pseudo-labeling the Inliers First for Imbalanced Semi-supervised Learning. (arXiv:2303.07269v1 [cs.CV] CROSS LISTED)",
    "abstract": "Recent state-of-the-art methods in imbalanced semi-supervised learning (SSL) rely on confidence-based pseudo-labeling with consistency regularization. To obtain high-quality pseudo-labels, a high confidence threshold is typically adopted. However, it has been shown that softmax-based confidence scores in deep networks can be arbitrarily high for samples far from the training data, and thus, the pseudo-labels for even high-confidence unlabeled samples may still be unreliable. In this work, we present a new perspective of pseudo-labeling for imbalanced SSL. Without relying on model confidence, we propose to measure whether an unlabeled sample is likely to be ``in-distribution''; i.e., close to the current training data. To decide whether an unlabeled sample is ``in-distribution'' or ``out-of-distribution'', we adopt the energy score from out-of-distribution detection literature. As training progresses and more unlabeled samples become in-distribution and contribute to training, the combi",
    "link": "http://arxiv.org/abs/2303.07269",
    "context": "Title: InPL: Pseudo-labeling the Inliers First for Imbalanced Semi-supervised Learning. (arXiv:2303.07269v1 [cs.CV] CROSS LISTED)\nAbstract: Recent state-of-the-art methods in imbalanced semi-supervised learning (SSL) rely on confidence-based pseudo-labeling with consistency regularization. To obtain high-quality pseudo-labels, a high confidence threshold is typically adopted. However, it has been shown that softmax-based confidence scores in deep networks can be arbitrarily high for samples far from the training data, and thus, the pseudo-labels for even high-confidence unlabeled samples may still be unreliable. In this work, we present a new perspective of pseudo-labeling for imbalanced SSL. Without relying on model confidence, we propose to measure whether an unlabeled sample is likely to be ``in-distribution''; i.e., close to the current training data. To decide whether an unlabeled sample is ``in-distribution'' or ``out-of-distribution'', we adopt the energy score from out-of-distribution detection literature. As training progresses and more unlabeled samples become in-distribution and contribute to training, the combi",
    "path": "papers/23/03/2303.07269.json",
    "total_tokens": 1028,
    "translated_title": "InPL: 伪标签首先标记内点的不平衡半监督学习",
    "translated_abstract": "近来，不平衡半监督学习中的最新先进方法依赖于基于置信度的伪标签和一致性规则。为获得高质量的伪标签，通常采用高置信度阈值。然而，已经证明，在深度网络中，对于远离训练数据的样本，基于softmax的置信度得分可以任意高，因此即使对于高置信度的未标记样本，其伪标签仍可能不可靠。在本文中，我们提出了一种伪标签的新视角，用于不平衡的半监督学习。我们不依赖于模型置信度，而是提出衡量一个未标记 样本很可能属于“内分布”（即接近当前训练数据）的方法。为了确定一个未标记样本是“内分布”还是“外分布”，我们采用来自“外分布检测”文献中的能量分数。随着训练的进行，越来越多的未标记样本成为内部分布并对训练产生贡献，结合伪标签的方法表现出色。",
    "tldr": "该论文提出了一种不依赖于模型置信度的伪标签方法，用于不平衡的半监督学习，该方法用能量分数判断未标记样本是“内分布”还是“外分布”，随着训练的进行，越来越多的未标记样本成为内部分布并对训练产生贡献，结合伪标签的方法表现出色。",
    "en_tdlr": "This paper proposes a pseudo-label method that does not rely on model confidence for imbalanced semi-supervised learning. It uses energy score to determine whether an unlabeled sample is \"in-distribution\" or \"out-of-distribution\" and has shown promising results as more unlabeled samples become \"in-distribution\" during training and contribute to the training process."
}