{
    "title": "Sequential Knockoffs for Variable Selection in Reinforcement Learning. (arXiv:2303.14281v1 [stat.ML])",
    "abstract": "In real-world applications of reinforcement learning, it is often challenging to obtain a state representation that is parsimonious and satisfies the Markov property without prior knowledge. Consequently, it is common practice to construct a state which is larger than necessary, e.g., by concatenating measurements over contiguous time points. However, needlessly increasing the dimension of the state can slow learning and obfuscate the learned policy. We introduce the notion of a minimal sufficient state in a Markov decision process (MDP) as the smallest subvector of the original state under which the process remains an MDP and shares the same optimal policy as the original process. We propose a novel sequential knockoffs (SEEK) algorithm that estimates the minimal sufficient state in a system with high-dimensional complex nonlinear dynamics. In large samples, the proposed method controls the false discovery rate, and selects all sufficient variables with probability approaching one. As",
    "link": "http://arxiv.org/abs/2303.14281",
    "context": "Title: Sequential Knockoffs for Variable Selection in Reinforcement Learning. (arXiv:2303.14281v1 [stat.ML])\nAbstract: In real-world applications of reinforcement learning, it is often challenging to obtain a state representation that is parsimonious and satisfies the Markov property without prior knowledge. Consequently, it is common practice to construct a state which is larger than necessary, e.g., by concatenating measurements over contiguous time points. However, needlessly increasing the dimension of the state can slow learning and obfuscate the learned policy. We introduce the notion of a minimal sufficient state in a Markov decision process (MDP) as the smallest subvector of the original state under which the process remains an MDP and shares the same optimal policy as the original process. We propose a novel sequential knockoffs (SEEK) algorithm that estimates the minimal sufficient state in a system with high-dimensional complex nonlinear dynamics. In large samples, the proposed method controls the false discovery rate, and selects all sufficient variables with probability approaching one. As",
    "path": "papers/23/03/2303.14281.json",
    "total_tokens": 871,
    "translated_title": "基于序列 Knockoffs 的强化学习变量选择",
    "translated_abstract": "在强化学习的实际应用中，通常很难获得一个既简洁又满足马尔可夫属性的状态表示，而不需要使用先验知识。因此，常规做法是构造一个比必要的要大的状态，例如将连续时间点上的测量串联起来。然而，增加状态的维数可能会减缓学习进程并使学习策略模糊不清。我们引入了一个在马尔可夫决策过程(MDP)中的最小充分状态的概念，作为原始状态下最小的子向量，使该过程仍然是MDP，并且与原始过程共享相同的最优策略。我们提出了一种新颖的序列 Knockoffs (SEEK)算法，用于估计高维复杂非线性动力学系统中的最小充分状态。在大样本中，所提出的方法控制了假发现率，并且选择所有充分的变量的概率趋近于1。",
    "tldr": "本论文介绍了一种新颖的序列 Knockoffs (SEEK)算法，用于在强化学习系统中实现变量选择，该算法估计了最小充分状态，确保学习进程良好而不会减缓。",
    "en_tdlr": "This paper introduces a novel sequential knockoffs (SEEK) algorithm for variable selection in reinforcement learning systems. The algorithm estimates the minimal sufficient state to ensure efficient learning without slowing down the process."
}