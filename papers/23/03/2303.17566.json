{
    "title": "Non-Invasive Fairness in Learning through the Lens of Data Drift. (arXiv:2303.17566v1 [cs.LG])",
    "abstract": "Machine Learning (ML) models are widely employed to drive many modern data systems. While they are undeniably powerful tools, ML models often demonstrate imbalanced performance and unfair behaviors. The root of this problem often lies in the fact that different subpopulations commonly display divergent trends: as a learning algorithm tries to identify trends in the data, it naturally favors the trends of the majority groups, leading to a model that performs poorly and unfairly for minority populations. Our goal is to improve the fairness and trustworthiness of ML models by applying only non-invasive interventions, i.e., without altering the data or the learning algorithm. We use a simple but key insight: the divergence of trends between different populations, and, consecutively, between a learned model and minority populations, is analogous to data drift, which indicates the poor conformance between parts of the data and the trained model. We explore two strategies (model-splitting and",
    "link": "http://arxiv.org/abs/2303.17566",
    "context": "Title: Non-Invasive Fairness in Learning through the Lens of Data Drift. (arXiv:2303.17566v1 [cs.LG])\nAbstract: Machine Learning (ML) models are widely employed to drive many modern data systems. While they are undeniably powerful tools, ML models often demonstrate imbalanced performance and unfair behaviors. The root of this problem often lies in the fact that different subpopulations commonly display divergent trends: as a learning algorithm tries to identify trends in the data, it naturally favors the trends of the majority groups, leading to a model that performs poorly and unfairly for minority populations. Our goal is to improve the fairness and trustworthiness of ML models by applying only non-invasive interventions, i.e., without altering the data or the learning algorithm. We use a simple but key insight: the divergence of trends between different populations, and, consecutively, between a learned model and minority populations, is analogous to data drift, which indicates the poor conformance between parts of the data and the trained model. We explore two strategies (model-splitting and",
    "path": "papers/23/03/2303.17566.json",
    "total_tokens": 871,
    "translated_abstract": "机器学习模型被广泛用于驱动许多现代数据系统。虽然它们是无可否认的强大工具，但机器学习模型常常表现出不平衡的性能和不公平的行为。这个问题的根源通常在于不同的子群体显示出不同的趋势：当学习算法试图识别数据中的趋势时，它自然会偏袒大多数群体的趋势，导致模型在少数族裔中表现不佳和不公平。我们的目标是通过仅应用非侵入式干预（即不改变数据或学习算法）来提高机器学习模型的公正性和可信性。我们使用一个简单但重要的视角：不同人群之间的趋势差异，并且，随后，在所学模型和少数族裔之间的差异类似于数据漂移，这表明数据的某些部分与训练好的模型不符。我们探索了两种策略（模型拆分和...）",
    "tldr": "本文提出了通过数据漂移视角的非侵入式学习公正的方法，以提高机器学习模型在少数族裔中的性能和公正性。",
    "en_tdlr": "This paper proposes a method for non-invasive fairness in learning through the lens of data drift, to improve the performance and fairness of machine learning models for minority populations."
}