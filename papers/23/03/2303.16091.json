{
    "title": "Learnability, Sample Complexity, and Hypothesis Class Complexity for Regression Models. (arXiv:2303.16091v1 [stat.ML])",
    "abstract": "The goal of a learning algorithm is to receive a training data set as input and provide a hypothesis that can generalize to all possible data points from a domain set. The hypothesis is chosen from hypothesis classes with potentially different complexities. Linear regression modeling is an important category of learning algorithms. The practical uncertainty of the target samples affects the generalization performance of the learned model. Failing to choose a proper model or hypothesis class can lead to serious issues such as underfitting or overfitting. These issues have been addressed by alternating cost functions or by utilizing cross-validation methods. These approaches can introduce new hyperparameters with their own new challenges and uncertainties or increase the computational complexity of the learning algorithm. On the other hand, the theory of probably approximately correct (PAC) aims at defining learnability based on probabilistic settings. Despite its theoretical value, PAC ",
    "link": "http://arxiv.org/abs/2303.16091",
    "context": "Title: Learnability, Sample Complexity, and Hypothesis Class Complexity for Regression Models. (arXiv:2303.16091v1 [stat.ML])\nAbstract: The goal of a learning algorithm is to receive a training data set as input and provide a hypothesis that can generalize to all possible data points from a domain set. The hypothesis is chosen from hypothesis classes with potentially different complexities. Linear regression modeling is an important category of learning algorithms. The practical uncertainty of the target samples affects the generalization performance of the learned model. Failing to choose a proper model or hypothesis class can lead to serious issues such as underfitting or overfitting. These issues have been addressed by alternating cost functions or by utilizing cross-validation methods. These approaches can introduce new hyperparameters with their own new challenges and uncertainties or increase the computational complexity of the learning algorithm. On the other hand, the theory of probably approximately correct (PAC) aims at defining learnability based on probabilistic settings. Despite its theoretical value, PAC ",
    "path": "papers/23/03/2303.16091.json",
    "total_tokens": 980,
    "tldr": "这篇论文讨论学习算法中假设类和样本复杂度对回归模型的影响，并为构建高可靠性和泛化性能的学习算法提供了基础。"
}