{
    "title": "Large Language Models are Diverse Role-Players for Summarization Evaluation. (arXiv:2303.15078v2 [cs.CL] UPDATED)",
    "abstract": "Text summarization has a wide range of applications in many scenarios. The evaluation of the quality of the generated text is a complex problem. A big challenge to language evaluation is that there is a clear divergence between existing metrics and human evaluation. For example, the quality of a document summary can be measured by human annotators from both objective aspects, such as grammatical and semantic correctness, as well as subjective dimensions, such as comprehensiveness, succinctness, and interestingness. Most of the automatic evaluation methods like BLUE/ROUGE may be not able to capture the above dimensions well. In this paper, we propose a new evaluation framework based on LLMs, which provides a comprehensive evaluation framework by comparing generated text and reference text from both objective and subjective aspects. First, we propose to model objective and subjective dimensions of generated text based on roleplayers prompting mechanism. Furthermore, we introduce a contex",
    "link": "http://arxiv.org/abs/2303.15078",
    "context": "Title: Large Language Models are Diverse Role-Players for Summarization Evaluation. (arXiv:2303.15078v2 [cs.CL] UPDATED)\nAbstract: Text summarization has a wide range of applications in many scenarios. The evaluation of the quality of the generated text is a complex problem. A big challenge to language evaluation is that there is a clear divergence between existing metrics and human evaluation. For example, the quality of a document summary can be measured by human annotators from both objective aspects, such as grammatical and semantic correctness, as well as subjective dimensions, such as comprehensiveness, succinctness, and interestingness. Most of the automatic evaluation methods like BLUE/ROUGE may be not able to capture the above dimensions well. In this paper, we propose a new evaluation framework based on LLMs, which provides a comprehensive evaluation framework by comparing generated text and reference text from both objective and subjective aspects. First, we propose to model objective and subjective dimensions of generated text based on roleplayers prompting mechanism. Furthermore, we introduce a contex",
    "path": "papers/23/03/2303.15078.json",
    "total_tokens": 850,
    "translated_title": "大语言模型是摘要评估的不同角色扮演者",
    "translated_abstract": "文本摘要在许多场景中具有广泛的应用。生成文本的质量评估是一个复杂的问题。语言评估的一个大挑战是现有指标和人工评估之间存在明显的分歧。例如，文档摘要的质量可以通过人工注释者从客观方面（如语法和语义的正确性）以及主观维度（如全面性、简洁性和有趣性）进行评估。大多数自动评估方法（如BLUE/ROUGE）可能无法很好地捕捉以上维度。在本文中，我们提出了一个基于LLMs的新的评估框架，通过比较从客观和主观方面生成的文本和参考文本，提供了全面的评估框架。首先，我们提出了基于角色扮演者提示机制的生成的文本的客观和主观维度的建模。此外，我们还引入了一个上下文。。",
    "tldr": "本文提出了一种新的基于LLMs的评估框架，通过比较生成的文本和参考文本的客观和主观维度，提供了全面的评估框架。",
    "en_tdlr": "This paper proposes a new evaluation framework based on LLMs, which provides a comprehensive evaluation framework by comparing generated text and reference text from both objective and subjective aspects."
}