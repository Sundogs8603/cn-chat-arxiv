{
    "title": "Environment Transformer and Policy Optimization for Model-Based Offline Reinforcement Learning. (arXiv:2303.03811v2 [cs.LG] UPDATED)",
    "abstract": "Interacting with the actual environment to acquire data is often costly and time-consuming in robotic tasks. Model-based offline reinforcement learning (RL) provides a feasible solution. On the one hand, it eliminates the requirements of interaction with the actual environment. On the other hand, it learns the transition dynamics and reward function from the offline datasets and generates simulated rollouts to accelerate training. Previous model-based offline RL methods adopt probabilistic ensemble neural networks (NN) to model aleatoric uncertainty and epistemic uncertainty. However, this results in an exponential increase in training time and computing resource requirements. Furthermore, these methods are easily disturbed by the accumulative errors of the environment dynamics models when simulating long-term rollouts. To solve the above problems, we propose an uncertainty-aware sequence modeling architecture called Environment Transformer. It models the probability distribution of th",
    "link": "http://arxiv.org/abs/2303.03811",
    "context": "Title: Environment Transformer and Policy Optimization for Model-Based Offline Reinforcement Learning. (arXiv:2303.03811v2 [cs.LG] UPDATED)\nAbstract: Interacting with the actual environment to acquire data is often costly and time-consuming in robotic tasks. Model-based offline reinforcement learning (RL) provides a feasible solution. On the one hand, it eliminates the requirements of interaction with the actual environment. On the other hand, it learns the transition dynamics and reward function from the offline datasets and generates simulated rollouts to accelerate training. Previous model-based offline RL methods adopt probabilistic ensemble neural networks (NN) to model aleatoric uncertainty and epistemic uncertainty. However, this results in an exponential increase in training time and computing resource requirements. Furthermore, these methods are easily disturbed by the accumulative errors of the environment dynamics models when simulating long-term rollouts. To solve the above problems, we propose an uncertainty-aware sequence modeling architecture called Environment Transformer. It models the probability distribution of th",
    "path": "papers/23/03/2303.03811.json",
    "total_tokens": 878,
    "translated_title": "基于模型的离线强化学习的环境转换器和策略优化",
    "translated_abstract": "在机器人任务中，与实际环境交互以获取数据通常是昂贵且耗时的。基于模型的离线强化学习提供了一种可行的解决方案。一方面，它消除了与实际环境的交互要求。另一方面，它从离线数据集中学习转换动力学和奖励函数，并生成模拟的回合以加速训练。以前的基于模型的离线强化学习方法采用概率集合神经网络（NN）来建模aleatoric不确定性和epistemic不确定性。然而，这导致了训练时间和计算资源需求的指数增加。此外，这些方法在模拟长期回合时容易受到环境动力学模型的累积误差的干扰。为了解决上述问题，我们提出了一种称为环境转换器的不确定性感知序列建模架构。",
    "tldr": "论文提出了一种称为环境转换器的不确定性感知序列建模架构，用于解决基于模型的离线强化学习中训练时间和计算资源需求增加的问题，并减少环境动力学模型累积误差的干扰。",
    "en_tdlr": "The paper proposes an uncertainty-aware sequence modeling architecture called Environment Transformer to address the increased training time and computing resource requirements in model-based offline reinforcement learning, and mitigate the disturbance from accumulative errors of the environment dynamics models."
}