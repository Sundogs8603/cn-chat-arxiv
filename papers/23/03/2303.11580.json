{
    "title": "Efficient Multi-stage Inference on Tabular Data. (arXiv:2303.11580v1 [cs.LG])",
    "abstract": "Many ML applications and products train on medium amounts of input data but get bottlenecked in real-time inference. When implementing ML systems, conventional wisdom favors segregating ML code into services queried by product code via Remote Procedure Call (RPC) APIs. This approach clarifies the overall software architecture and simplifies product code by abstracting away ML internals. However, the separation adds network latency and entails additional CPU overhead. Hence, we simplify inference algorithms and embed them into the product code to reduce network communication. For public datasets and a high-performance real-time platform that deals with tabular data, we show that over half of the inputs are often amenable to such optimization, while the remainder can be handled by the original model. By applying our optimization with AutoML to both training and inference, we reduce inference latency by 1.3x, CPU resources by 30%, and network communication between application front-end an",
    "link": "http://arxiv.org/abs/2303.11580",
    "context": "Title: Efficient Multi-stage Inference on Tabular Data. (arXiv:2303.11580v1 [cs.LG])\nAbstract: Many ML applications and products train on medium amounts of input data but get bottlenecked in real-time inference. When implementing ML systems, conventional wisdom favors segregating ML code into services queried by product code via Remote Procedure Call (RPC) APIs. This approach clarifies the overall software architecture and simplifies product code by abstracting away ML internals. However, the separation adds network latency and entails additional CPU overhead. Hence, we simplify inference algorithms and embed them into the product code to reduce network communication. For public datasets and a high-performance real-time platform that deals with tabular data, we show that over half of the inputs are often amenable to such optimization, while the remainder can be handled by the original model. By applying our optimization with AutoML to both training and inference, we reduce inference latency by 1.3x, CPU resources by 30%, and network communication between application front-end an",
    "path": "papers/23/03/2303.11580.json",
    "total_tokens": 917,
    "translated_title": "基于表格数据的高效多级推断",
    "translated_abstract": "许多机器学习应用和产品通过中等数量的输入数据进行训练，但在实时推断时被瓶颈所困。传统智慧在实现机器学习系统时，倾向于将ML代码分割成服务，并通过远程过程调用（RPC）API被产品代码查询。这种方法澄清了整体软件架构，并通过抽象ML内部简化了产品代码。然而，这种分离增加了网络延迟，并带来了额外的CPU开销。因此，我们简化推断算法并将其嵌入产品代码中，以减少网络通信。针对公共数据集和处理表格数据的高性能实时平台，我们表明通常有超过一半的输入可以适应这种优化，而其余部分可以由原始模型处理。通过将AutoML应用于训练和推断，我们将推断延迟降低了1.3倍，CPU资源减少了30％，应用程序前端和后端之间的网络通信减少了60％。",
    "tldr": "该论文通过将推断算法简化并嵌入产品代码中，以减少网络通信，在处理表格数据的实时平台上可将推断延迟降低1.3倍，CPU资源减少30％，并将应用程序前端和后端之间的网络通信减少60％。",
    "en_tdlr": "By simplifying inference algorithms and embedding them into the product code to reduce network communication, this paper shows that inference latency can be reduced by 1.3x, CPU resources by 30%, and network communication between the application front-end and back-end by 60% on a high-performance real-time platform that deals with tabular data."
}