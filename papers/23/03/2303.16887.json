{
    "title": "Towards Understanding the Effect of Pretraining Label Granularity. (arXiv:2303.16887v1 [cs.CV])",
    "abstract": "In this paper, we study how pretraining label granularity affects the generalization of deep neural networks in image classification tasks. We focus on the \"fine-to-coarse\" transfer learning setting where the pretraining label is more fine-grained than that of the target problem. We experiment with this method using the label hierarchy of iNaturalist 2021, and observe a 8.76% relative improvement of the error rate over the baseline. We find the following conditions are key for the improvement: 1) the pretraining dataset has a strong and meaningful label hierarchy, 2) its label function strongly aligns with that of the target task, and most importantly, 3) an appropriate level of pretraining label granularity is chosen. The importance of pretraining label granularity is further corroborated by our transfer learning experiments on ImageNet. Most notably, we show that pretraining at the leaf labels of ImageNet21k produces better transfer results on ImageNet1k than pretraining at other coa",
    "link": "http://arxiv.org/abs/2303.16887",
    "context": "Title: Towards Understanding the Effect of Pretraining Label Granularity. (arXiv:2303.16887v1 [cs.CV])\nAbstract: In this paper, we study how pretraining label granularity affects the generalization of deep neural networks in image classification tasks. We focus on the \"fine-to-coarse\" transfer learning setting where the pretraining label is more fine-grained than that of the target problem. We experiment with this method using the label hierarchy of iNaturalist 2021, and observe a 8.76% relative improvement of the error rate over the baseline. We find the following conditions are key for the improvement: 1) the pretraining dataset has a strong and meaningful label hierarchy, 2) its label function strongly aligns with that of the target task, and most importantly, 3) an appropriate level of pretraining label granularity is chosen. The importance of pretraining label granularity is further corroborated by our transfer learning experiments on ImageNet. Most notably, we show that pretraining at the leaf labels of ImageNet21k produces better transfer results on ImageNet1k than pretraining at other coa",
    "path": "papers/23/03/2303.16887.json",
    "total_tokens": 1016,
    "translated_title": "探究预训练标签粒度的影响",
    "translated_abstract": "本文研究了预训练标签粒度如何影响深度神经网络在图像分类任务中的泛化能力。我们关注“细到粗”的迁移学习设置，其中预训练标签比目标问题更细粒度。我们使用iNaturalist 2021的标签层次结构进行了该方法的实验，并观察到相对于基线错误率有8.76％的相对改进。我们发现以下条件对于改进非常关键：1）预训练数据集具有强大且有意义的标签层次结构，2）其标签功能与目标任务的功能强烈对齐，最重要的是，3）选择了适当级别的预训练标签粒度。我们在ImageNet上的迁移学习实验进一步证明了预训练标签粒度的重要性。值得注意的是，我们展示了在ImageNet21k上的叶标签预训练产生了比其他合作标签更好的ImageNet1k迁移结果。",
    "tldr": "本文探究了预训练标签粒度对深度神经网络图像分类任务的泛化能力的影响，并在iNaturalist 2021与ImageNet数据集中进行了实验证明，在预训练数据集具有强有力的标签层次结构，标签功能与目标任务对齐，以及选择适当的预训练标签粒度时，能有效提高模型的性能。",
    "en_tdlr": "This paper investigates the impact of pretraining label granularity on the generalization performance of deep neural networks in image classification tasks. Experimental results on iNaturalist 2021 and ImageNet datasets demonstrate that a pretraining dataset with a strong and meaningful label hierarchy, a label function strongly aligned with the target task, and an appropriate level of pretraining label granularity can significantly improve the model performance. Pretraining at the leaf labels of ImageNet21k has been shown to produce better transfer results on ImageNet1k than pretraining at other coarser labels."
}