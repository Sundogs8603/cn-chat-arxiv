{
    "title": "Dropout Reduces Underfitting. (arXiv:2303.01500v2 [cs.LG] UPDATED)",
    "abstract": "Introduced by Hinton et al. in 2012, dropout has stood the test of time as a regularizer for preventing overfitting in neural networks. In this study, we demonstrate that dropout can also mitigate underfitting when used at the start of training. During the early phase, we find dropout reduces the directional variance of gradients across mini-batches and helps align the mini-batch gradients with the entire dataset's gradient. This helps counteract the stochasticity of SGD and limit the influence of individual batches on model training. Our findings lead us to a solution for improving performance in underfitting models - early dropout: dropout is applied only during the initial phases of training, and turned off afterwards. Models equipped with early dropout achieve lower final training loss compared to their counterparts without dropout. Additionally, we explore a symmetric technique for regularizing overfitting models - late dropout, where dropout is not used in the early iterations an",
    "link": "http://arxiv.org/abs/2303.01500",
    "context": "Title: Dropout Reduces Underfitting. (arXiv:2303.01500v2 [cs.LG] UPDATED)\nAbstract: Introduced by Hinton et al. in 2012, dropout has stood the test of time as a regularizer for preventing overfitting in neural networks. In this study, we demonstrate that dropout can also mitigate underfitting when used at the start of training. During the early phase, we find dropout reduces the directional variance of gradients across mini-batches and helps align the mini-batch gradients with the entire dataset's gradient. This helps counteract the stochasticity of SGD and limit the influence of individual batches on model training. Our findings lead us to a solution for improving performance in underfitting models - early dropout: dropout is applied only during the initial phases of training, and turned off afterwards. Models equipped with early dropout achieve lower final training loss compared to their counterparts without dropout. Additionally, we explore a symmetric technique for regularizing overfitting models - late dropout, where dropout is not used in the early iterations an",
    "path": "papers/23/03/2303.01500.json",
    "total_tokens": 892,
    "translated_title": "《Dropout Reduces Underfitting》",
    "translated_abstract": "《Dropout Reduces Underfitting》是一篇关于神经网络正则化方法Dropout的研究，证明了Dropout可以在训练初期防止欠拟合的效果。在研究中，发现Dropout可以减少小批次梯度的方向差异，并有助于将小批次梯度与整个数据集的梯度对齐，从而缓解SGD中的随机性，减少每个批次对模型训练的影响。因此，我们提出了一个解决欠拟合问题的方案——early dropout: 在训练的初始阶段应用dropout，并在训练后关闭dropout。相比于没有dropout的模型，采用early dropout的模型能够获得更低的最终训练损失。另外，我们还探讨了一种用于正则化过拟合模型的对称技术——late dropout。",
    "tldr": "本研究证明dropout不仅可以防止神经网络过拟合，还可以缓解欠拟合问题。在训练初期采用early dropout方法，可以减少小批次梯度的方向差异，缓解SGD中的随机性，从而提高模型的训练效果。",
    "en_tdlr": "This study demonstrates that dropout can not only prevent overfitting in neural networks, but also mitigate underfitting. Early dropout, applied during the initial phases of training, can reduce directional variance of gradients across mini-batches and help align the mini-batch gradients with the entire dataset's gradient, improving the training effect of the model."
}