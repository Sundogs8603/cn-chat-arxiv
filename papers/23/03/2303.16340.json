{
    "title": "On the Local Cache Update Rules in Streaming Federated Learning. (arXiv:2303.16340v1 [cs.LG])",
    "abstract": "In this study, we address the emerging field of Streaming Federated Learning (SFL) and propose local cache update rules to manage dynamic data distributions and limited cache capacity. Traditional federated learning relies on fixed data sets, whereas in SFL, data is streamed, and its distribution changes over time, leading to discrepancies between the local training dataset and long-term distribution. To mitigate this problem, we propose three local cache update rules - First-In-First-Out (FIFO), Static Ratio Selective Replacement (SRSR), and Dynamic Ratio Selective Replacement (DRSR) - that update the local cache of each client while considering the limited cache capacity. Furthermore, we derive a convergence bound for our proposed SFL algorithm as a function of the distribution discrepancy between the long-term data distribution and the client's local training dataset. We then evaluate our proposed algorithm on two datasets: a network traffic classification dataset and an image class",
    "link": "http://arxiv.org/abs/2303.16340",
    "context": "Title: On the Local Cache Update Rules in Streaming Federated Learning. (arXiv:2303.16340v1 [cs.LG])\nAbstract: In this study, we address the emerging field of Streaming Federated Learning (SFL) and propose local cache update rules to manage dynamic data distributions and limited cache capacity. Traditional federated learning relies on fixed data sets, whereas in SFL, data is streamed, and its distribution changes over time, leading to discrepancies between the local training dataset and long-term distribution. To mitigate this problem, we propose three local cache update rules - First-In-First-Out (FIFO), Static Ratio Selective Replacement (SRSR), and Dynamic Ratio Selective Replacement (DRSR) - that update the local cache of each client while considering the limited cache capacity. Furthermore, we derive a convergence bound for our proposed SFL algorithm as a function of the distribution discrepancy between the long-term data distribution and the client's local training dataset. We then evaluate our proposed algorithm on two datasets: a network traffic classification dataset and an image class",
    "path": "papers/23/03/2303.16340.json",
    "total_tokens": 924,
    "translated_title": "关于流式联邦学习中本地缓存更新规则的研究",
    "translated_abstract": "本研究针对流式联邦学习（SFL）领域，提出了本地缓存更新规则，以管理动态数据分布和有限的缓存容量。传统的联邦学习依赖于固定的数据集，而在SFL中，数据是流式的，并且其分布随时间而变化，导致本地训练数据集与长期分布之间存在差异。为了解决这个问题，我们提出了三个本地缓存更新规则——先进先出（FIFO）、静态比例选择性替换（SRSR）和动态比例选择性替换（DRSR）——在考虑有限的缓存容量的同时更新每个客户端的本地缓存。此外，我们还推导出了基于长期数据分布和客户端本地训练数据集之间分布不一致度的收敛界限。然后，我们在两个数据集上评估了我们提出的算法：网络流量分类数据集和图像分类数据集。",
    "tldr": "本文提出了三种本地缓存更新规则来管理动态数据分布和有限的缓存容量，以应对流式联邦学习中本地训练数据集与长期数据分布之间的差异。我们还推导出了该算法的收敛界限。我们在两个数据集上进行了测试，结果表明我们的算法效果良好。"
}