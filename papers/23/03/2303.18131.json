{
    "title": "AdvCheck: Characterizing Adversarial Examples via Local Gradient Checking. (arXiv:2303.18131v1 [cs.CR])",
    "abstract": "Deep neural networks (DNNs) are vulnerable to adversarial examples, which may lead to catastrophe in security-critical domains. Numerous detection methods are proposed to characterize the feature uniqueness of adversarial examples, or to distinguish DNN's behavior activated by the adversarial examples. Detections based on features cannot handle adversarial examples with large perturbations. Besides, they require a large amount of specific adversarial examples. Another mainstream, model-based detections, which characterize input properties by model behaviors, suffer from heavy computation cost. To address the issues, we introduce the concept of local gradient, and reveal that adversarial examples have a quite larger bound of local gradient than the benign ones. Inspired by the observation, we leverage local gradient for detecting adversarial examples, and propose a general framework AdvCheck. Specifically, by calculating the local gradient from a few benign examples and noise-added misc",
    "link": "http://arxiv.org/abs/2303.18131",
    "context": "Title: AdvCheck: Characterizing Adversarial Examples via Local Gradient Checking. (arXiv:2303.18131v1 [cs.CR])\nAbstract: Deep neural networks (DNNs) are vulnerable to adversarial examples, which may lead to catastrophe in security-critical domains. Numerous detection methods are proposed to characterize the feature uniqueness of adversarial examples, or to distinguish DNN's behavior activated by the adversarial examples. Detections based on features cannot handle adversarial examples with large perturbations. Besides, they require a large amount of specific adversarial examples. Another mainstream, model-based detections, which characterize input properties by model behaviors, suffer from heavy computation cost. To address the issues, we introduce the concept of local gradient, and reveal that adversarial examples have a quite larger bound of local gradient than the benign ones. Inspired by the observation, we leverage local gradient for detecting adversarial examples, and propose a general framework AdvCheck. Specifically, by calculating the local gradient from a few benign examples and noise-added misc",
    "path": "papers/23/03/2303.18131.json",
    "total_tokens": 1017,
    "translated_title": "AdvCheck：通过本地梯度检查表征对抗生成样本",
    "translated_abstract": "深度神经网络（DNN）容易受到对抗性样本攻击，在安全关键领域可能导致灾难。已经提出了各种检测方法来表征对抗生成样本的特征唯一性，或区分由对抗性样本触发的DNN的行为。基于特征的检测方法不能处理受到大扰动的对抗性样本，还需要大量的对抗性样本。另一个主流的基于模型的检测方法，通过模型行为表征输入属性，计算代价很高。为了解决这些问题，我们引入了本地梯度的概念，并揭示出对抗性样本的本地梯度较正常样本有更大的边界。我们受到这一观察的启发，利用本地梯度检测对抗性样本，并提出了一个通用的框架AdvCheck。具体地，通过从一些正常样本和添加噪声的杂项样本计算本地梯度，我们可以有效地区分对抗性样本和正常样本。我们进一步提出了一种名为AdvCheck-LIME的变体，通过引入局部性来处理本地梯度。广泛的实验在基准数据集上证明了我们提出的方法的优越性和效率，相较于其他最先进的检测方法。",
    "tldr": "本文提出了一种新的方法AdvCheck，通过计算本地梯度检测对抗性样本，相较于其他最先进的检测方法具有更高的效率和更好的表现。",
    "en_tdlr": "This paper proposes a novel method, AdvCheck, for detecting adversarial examples by calculating local gradients, which outperforms state-of-the-art detection methods in terms of efficiency and performance."
}