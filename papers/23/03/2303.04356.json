{
    "title": "Soft Actor-Critic Algorithm with Truly-satisfied Inequality Constraint. (arXiv:2303.04356v2 [cs.LG] UPDATED)",
    "abstract": "Soft actor-critic (SAC) in reinforcement learning is expected to be one of the next-generation robot control schemes. Its ability to maximize policy entropy would make a robotic controller robust to noise and perturbation, which is useful for real-world robot applications. However, the priority of maximizing the policy entropy is automatically tuned in the current implementation, the rule of which can be interpreted as one for equality constraint, binding the policy entropy into its specified lower bound. The current SAC is therefore no longer maximize the policy entropy, contrary to our expectation. To resolve this issue in SAC, this paper improves its implementation with a learnable state-dependent slack variable for appropriately handling the inequality constraint to maximize the policy entropy by reformulating it as the corresponding equality constraint. The introduced slack variable is optimized by a switching-type loss function that takes into account the dual objectives of satis",
    "link": "http://arxiv.org/abs/2303.04356",
    "context": "Title: Soft Actor-Critic Algorithm with Truly-satisfied Inequality Constraint. (arXiv:2303.04356v2 [cs.LG] UPDATED)\nAbstract: Soft actor-critic (SAC) in reinforcement learning is expected to be one of the next-generation robot control schemes. Its ability to maximize policy entropy would make a robotic controller robust to noise and perturbation, which is useful for real-world robot applications. However, the priority of maximizing the policy entropy is automatically tuned in the current implementation, the rule of which can be interpreted as one for equality constraint, binding the policy entropy into its specified lower bound. The current SAC is therefore no longer maximize the policy entropy, contrary to our expectation. To resolve this issue in SAC, this paper improves its implementation with a learnable state-dependent slack variable for appropriately handling the inequality constraint to maximize the policy entropy by reformulating it as the corresponding equality constraint. The introduced slack variable is optimized by a switching-type loss function that takes into account the dual objectives of satis",
    "path": "papers/23/03/2303.04356.json",
    "total_tokens": 885,
    "translated_title": "具有真正满足不等式约束的软Actor-Critic算法",
    "translated_abstract": "在强化学习中，软Actor-Critic（SAC）被认为是下一代机器人控制方案之一。其最大化策略熵的能力可以使机器人控制器对噪声和扰动具有鲁棒性，这对于实际的机器人应用非常有用。然而，在当前的实现中，最大化策略熵的优先级是自动调节的，其规则可以解释为等式约束，将策略熵绑定到指定的下界。因此，当前的SAC不再最大化策略熵，与我们的期望相反。为了解决SAC中的这个问题，本文改进了其实现，引入了一个可学习的状态相关的松弛变量，以适当处理不等式约束，通过将其重新制定为相应的等式约束来最大化策略熵。引入的松弛变量通过考虑满足不等式约束的双重目标的切换型损失函数进行优化。",
    "tldr": "本文改进了软Actor-Critic（SAC）算法的实现，通过引入可学习的状态相关的松弛变量来适当处理不等式约束，实现了最大化策略熵。这对于增强机器人控制器的鲁棒性非常有用。",
    "en_tdlr": "This paper improves the implementation of the Soft Actor-Critic (SAC) algorithm by introducing a learnable state-dependent slack variable to handle inequality constraints and maximize policy entropy. This is useful for enhancing the robustness of robotic controllers."
}