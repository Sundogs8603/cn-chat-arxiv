{
    "title": "SVDE: Scalable Value-Decomposition Exploration for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2303.09058v1 [cs.AI])",
    "abstract": "Value-decomposition methods, which reduce the difficulty of a multi-agent system by decomposing the joint state-action space into local observation-action spaces, have become popular in cooperative multi-agent reinforcement learning (MARL). However, value-decomposition methods still have the problems of tremendous sample consumption for training and lack of active exploration. In this paper, we propose a scalable value-decomposition exploration (SVDE) method, which includes a scalable training mechanism, intrinsic reward design, and explorative experience replay. The scalable training mechanism asynchronously decouples strategy learning with environmental interaction, so as to accelerate sample generation in a MapReduce manner. For the problem of lack of exploration, an intrinsic reward design and explorative experience replay are proposed, so as to enhance exploration to produce diverse samples and filter non-novel samples, respectively. Empirically, our method achieves the best perfo",
    "link": "http://arxiv.org/abs/2303.09058",
    "context": "Title: SVDE: Scalable Value-Decomposition Exploration for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2303.09058v1 [cs.AI])\nAbstract: Value-decomposition methods, which reduce the difficulty of a multi-agent system by decomposing the joint state-action space into local observation-action spaces, have become popular in cooperative multi-agent reinforcement learning (MARL). However, value-decomposition methods still have the problems of tremendous sample consumption for training and lack of active exploration. In this paper, we propose a scalable value-decomposition exploration (SVDE) method, which includes a scalable training mechanism, intrinsic reward design, and explorative experience replay. The scalable training mechanism asynchronously decouples strategy learning with environmental interaction, so as to accelerate sample generation in a MapReduce manner. For the problem of lack of exploration, an intrinsic reward design and explorative experience replay are proposed, so as to enhance exploration to produce diverse samples and filter non-novel samples, respectively. Empirically, our method achieves the best perfo",
    "path": "papers/23/03/2303.09058.json",
    "total_tokens": 968,
    "translated_title": "SVDE: 面向协作多智能体强化学习的可伸缩价值分解探索",
    "translated_abstract": "在协作多智能体强化学习中，价值分解方法通过将联合状态-动作空间分解为本地观察-动作空间来降低系统难度，已经变得越来越受欢迎。然而，价值分解方法仍然存在训练时需要大量样本和缺乏主动探索的问题。本文提出了一种可伸缩的价值分解探索（SVDE）方法，其中包括可伸缩训练机制、内在奖励设计和探索性经验回放。可伸缩训练机制异步地将策略学习与环境交互解耦，以MapReduce的方式加速样本生成。针对缺乏探索的问题，提出了内在奖励设计和探索性经验回放，以增强探索以产生多样化的样本和过滤非新颖样本。实验结果表明，我们的方法在多智能体强化学习任务中取得了最佳性能。",
    "tldr": "本文提出了一种面向协作多智能体强化学习的可伸缩价值分解探索方法，通过异步可伸缩训练机制、内在奖励设计和探索性经验回放解决了训练需要大量样本和缺乏主动探索的问题，实验结果显示该方法在多智能体强化学习任务中性能最佳。"
}