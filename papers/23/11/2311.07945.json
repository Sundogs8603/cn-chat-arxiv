{
    "title": "Well begun is half done: Importance of Starting Right in Multi-Step Math Reasoning",
    "abstract": "arXiv:2311.07945v2 Announce Type: replace  Abstract: Smaller language models can solve complex reasoning tasks better by learning to generate rationales for their predictions. However, we observe that these smaller models can sometimes struggle to start correctly, but when corrected, can solve a task that they would otherwise have struggled with. We propose two ways in which a smaller model can benefit from initial guidance: 1) asking an LLM for initial guidance, and 2) self-questioning guidance, where the student model can first initiate a question regarding how to start and then continue that chain. We extend initial question-based guidance to a prompting technique called QuestCoT, where starting with a question before a chain of reasoning proves useful. On two multi-step math reasoning datasets GSM8K and SVAMP, we show that starting correctly can lead to a significant performance gain (up to $+14$ points with LLM guidance and $+6$ points with QuestCoT).",
    "link": "https://arxiv.org/abs/2311.07945",
    "context": "Title: Well begun is half done: Importance of Starting Right in Multi-Step Math Reasoning\nAbstract: arXiv:2311.07945v2 Announce Type: replace  Abstract: Smaller language models can solve complex reasoning tasks better by learning to generate rationales for their predictions. However, we observe that these smaller models can sometimes struggle to start correctly, but when corrected, can solve a task that they would otherwise have struggled with. We propose two ways in which a smaller model can benefit from initial guidance: 1) asking an LLM for initial guidance, and 2) self-questioning guidance, where the student model can first initiate a question regarding how to start and then continue that chain. We extend initial question-based guidance to a prompting technique called QuestCoT, where starting with a question before a chain of reasoning proves useful. On two multi-step math reasoning datasets GSM8K and SVAMP, we show that starting correctly can lead to a significant performance gain (up to $+14$ points with LLM guidance and $+6$ points with QuestCoT).",
    "path": "papers/23/11/2311.07945.json",
    "total_tokens": 851,
    "translated_title": "良好的开端是成功的一半：多步骤数学推理中开始正确的重要性",
    "translated_abstract": "较小的语言模型通过学习为其预测生成原因，可以更好地解决复杂的推理任务。然而，我们观察到这些较小的模型有时会在开始时遇到困难，但在得到纠正后，可以解决原本困难的任务。我们提出了两种较小模型可以从初始指导中受益的方式：1）向LLM寻求初始指导，和2）自问指导，学生模型可以首先发起一个关于如何开始的问题，然后继续这一连锁。我们将初始基于问题的指导扩展到了一种称为QuestCoT的提示技术，该技术在进行推理链之前以一个问题开始是有益的。在两个多步数学推理数据集GSM8K和SVAMP上，我们展示了正确开始可以带来显著的性能提升（通过LLM指导最高高达+14分，通过QuestCoT最高高达+6分）。",
    "tldr": "较小的语言模型在多步骤数学推理中通过正确开始可以获得显着的性能提升，建议通过初始指导和自问指导的方式来引导模型开始正确。",
    "en_tdlr": "Smaller language models can significantly improve performance in multi-step math reasoning by starting correctly, suggesting the guidance through initial guidance and self-questioning guidance."
}