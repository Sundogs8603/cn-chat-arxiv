{
    "title": "More Samples or More Prompts? Exploring Effective In-Context Sampling for LLM Few-Shot Prompt Engineering",
    "abstract": "arXiv:2311.09782v2 Announce Type: replace  Abstract: While most existing works on LLM prompting techniques focus only on how to select a better set of data samples inside one single prompt input (In-Context Learning or ICL), why can not we design and leverage multiple prompts together to further improve the LLM's performance? In this work, we propose In-Context Sampling (ICS), a low-resource LLM prompting technique to produce confident predictions by optimizing the construction of multiple ICL prompt inputs. Extensive experiments with three open-source LLMs (FlanT5-XL, Mistral-7B, and Mixtral-8x7B) on four NLI datasets (e-SNLI, Multi-NLI, ANLI, and Contract-NLI) and one QA dataset (CommonsenseQA) illustrate that ICS can consistently enhance LLMs' performance. An in-depth evaluation with three data similarity-based ICS strategies suggests that these strategies can further elevate LLM's performance, which sheds light on a new yet promising future research direction.",
    "link": "https://arxiv.org/abs/2311.09782",
    "context": "Title: More Samples or More Prompts? Exploring Effective In-Context Sampling for LLM Few-Shot Prompt Engineering\nAbstract: arXiv:2311.09782v2 Announce Type: replace  Abstract: While most existing works on LLM prompting techniques focus only on how to select a better set of data samples inside one single prompt input (In-Context Learning or ICL), why can not we design and leverage multiple prompts together to further improve the LLM's performance? In this work, we propose In-Context Sampling (ICS), a low-resource LLM prompting technique to produce confident predictions by optimizing the construction of multiple ICL prompt inputs. Extensive experiments with three open-source LLMs (FlanT5-XL, Mistral-7B, and Mixtral-8x7B) on four NLI datasets (e-SNLI, Multi-NLI, ANLI, and Contract-NLI) and one QA dataset (CommonsenseQA) illustrate that ICS can consistently enhance LLMs' performance. An in-depth evaluation with three data similarity-based ICS strategies suggests that these strategies can further elevate LLM's performance, which sheds light on a new yet promising future research direction.",
    "path": "papers/23/11/2311.09782.json",
    "total_tokens": 932,
    "translated_title": "更多样本还是更多提示？探索LLM少样本提示工程中有效的上下文采样",
    "translated_abstract": "虽然大多数现有关于LLM提示技术的工作只关注如何在单个提示输入中选择更好的数据样本集（上下文学习或ICL），但为什么我们不能设计并利用多个提示来进一步提高LLM的性能呢？在这项工作中，我们提出了一种低资源的LLM提示技术In-Context Sampling（ICS），通过优化多个ICL提示输入的构建来生成自信的预测。在四个NLI数据集（e-SNLI、Multi-NLI、ANLI和Contract-NLI）和一个QA数据集（CommonsenseQA）上，与三个开源LLM（FlanT5-XL、Mistral-7B和Mixtral-8x7B）的大量实验表明ICS可以持续增强LLM的性能。与三种基于数据相似性的ICS策略的深入评估表明，这些策略可以进一步提升LLM的性能，为新的但有前途的未来研究方向投下光芒。",
    "tldr": "提出了一种低资源的LLM提示技术In-Context Sampling（ICS），通过优化多个ICL提示输入的构建来生成自信的预测，并展示了ICS可以持续增强LLM的性能，同时在数据相似性的基础上提出了新的研究方向。",
    "en_tdlr": "Proposed a low-resource LLM prompting technique In-Context Sampling (ICS) to generate confident predictions by optimizing the construction of multiple ICL prompt inputs, demonstrated consistent enhancement of LLM performance with ICS, and suggested a new research direction based on data similarity."
}