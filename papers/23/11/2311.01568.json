{
    "title": "Anytime-Competitive Reinforcement Learning with Policy Prior. (arXiv:2311.01568v1 [cs.LG])",
    "abstract": "This paper studies the problem of Anytime-Competitive Markov Decision Process (A-CMDP). Existing works on Constrained Markov Decision Processes (CMDPs) aim to optimize the expected reward while constraining the expected cost over random dynamics, but the cost in a specific episode can still be unsatisfactorily high. In contrast, the goal of A-CMDP is to optimize the expected reward while guaranteeing a bounded cost in each round of any episode against a policy prior. We propose a new algorithm, called Anytime-Competitive Reinforcement Learning (ACRL), which provably guarantees the anytime cost constraints. The regret analysis shows the policy asymptotically matches the optimal reward achievable under the anytime competitive constraints. Experiments on the application of carbon-intelligent computing verify the reward performance and cost constraint guarantee of ACRL.",
    "link": "http://arxiv.org/abs/2311.01568",
    "context": "Title: Anytime-Competitive Reinforcement Learning with Policy Prior. (arXiv:2311.01568v1 [cs.LG])\nAbstract: This paper studies the problem of Anytime-Competitive Markov Decision Process (A-CMDP). Existing works on Constrained Markov Decision Processes (CMDPs) aim to optimize the expected reward while constraining the expected cost over random dynamics, but the cost in a specific episode can still be unsatisfactorily high. In contrast, the goal of A-CMDP is to optimize the expected reward while guaranteeing a bounded cost in each round of any episode against a policy prior. We propose a new algorithm, called Anytime-Competitive Reinforcement Learning (ACRL), which provably guarantees the anytime cost constraints. The regret analysis shows the policy asymptotically matches the optimal reward achievable under the anytime competitive constraints. Experiments on the application of carbon-intelligent computing verify the reward performance and cost constraint guarantee of ACRL.",
    "path": "papers/23/11/2311.01568.json",
    "total_tokens": 856,
    "translated_title": "Anytime-Competitive Reinforcement Learning with Policy Prior. (arXiv:2311.01568v1 [cs.LG]) （具有策略先验的任意时刻竞争性强化学习）",
    "translated_abstract": "本文研究了任意时刻竞争性马尔可夫决策过程（A-CMDP）的问题。现有的关于受限制马尔可夫决策过程（CMDP）的研究旨在在随机动态中优化期望回报同时约束期望成本，但是特定情节中的成本仍然可能非常高。相反，A-CMDP的目标是在任意一轮的任何情节中，优化期望回报同时保证有界的成本，并针对策略先验进行了保证。我们提出了一种新的算法，称为Anytime-Competitive Reinforcement Learning（ACRL），它可以证明地保证了任意时刻的成本约束。遗憾分析表明，该策略在任意的竞争性约束下渐近地与最优回报相匹配。在碳智能计算应用中的实验验证了ACRL的回报性能和成本约束保证。",
    "tldr": "本文研究了具有策略先验的任意时刻竞争性强化学习问题（A-CMDP）。我们提出了一种新的算法ACRL，该算法可以保证任意时刻的成本约束，并在实验中验证了其回报性能和成本约束保证。"
}