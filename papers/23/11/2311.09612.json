{
    "title": "Efficient End-to-End Visual Document Understanding with Rationale Distillation",
    "abstract": "arXiv:2311.09612v2 Announce Type: replace-cross  Abstract: Understanding visually situated language requires interpreting complex layouts of textual and visual elements. Pre-processing tools, such as optical character recognition (OCR), can map document image inputs to textual tokens, then large language models (LLMs) can reason over text. However, such methods have high computational and engineering complexity. Can small pretrained image-to-text models accurately understand visual documents through similar recognition and reasoning steps instead? We propose Rationale Distillation (RD), which incorporates the outputs of OCR tools, LLMs, and larger multimodal models as intermediate \"rationales\", and trains a small student model to predict both rationales and answers. On three visual document understanding benchmarks representing infographics, scanned documents, and figures, our Pix2Struct (282M parameters) student model finetuned with RD outperforms the base model by 4-5% absolute accur",
    "link": "https://arxiv.org/abs/2311.09612",
    "context": "Title: Efficient End-to-End Visual Document Understanding with Rationale Distillation\nAbstract: arXiv:2311.09612v2 Announce Type: replace-cross  Abstract: Understanding visually situated language requires interpreting complex layouts of textual and visual elements. Pre-processing tools, such as optical character recognition (OCR), can map document image inputs to textual tokens, then large language models (LLMs) can reason over text. However, such methods have high computational and engineering complexity. Can small pretrained image-to-text models accurately understand visual documents through similar recognition and reasoning steps instead? We propose Rationale Distillation (RD), which incorporates the outputs of OCR tools, LLMs, and larger multimodal models as intermediate \"rationales\", and trains a small student model to predict both rationales and answers. On three visual document understanding benchmarks representing infographics, scanned documents, and figures, our Pix2Struct (282M parameters) student model finetuned with RD outperforms the base model by 4-5% absolute accur",
    "path": "papers/23/11/2311.09612.json",
    "total_tokens": 878,
    "translated_title": "使用Rationale Distillation实现高效的端到端视觉文档理解",
    "translated_abstract": "理解视觉语境需要解释复杂的文本和视觉元素布局。预处理工具，如光学字符识别（OCR），可以将文档图像输入映射到文本标记，然后大型语言模型（LLMs）可以对文本进行推理。然而，这种方法具有高计算和工程复杂性。小型预训练的图像到文本模型能否通过类似的识别和推理步骤准确理解视觉文档？我们提出Rationale Distillation（RD），它将OCR工具、LLMs和更大的多模态模型的输出作为中间“rationales”，并训练一个小型学生模型来预测rationales和答案。在代表信息图表、扫描文档和图表的三个视觉文档理解基准上，我们的Pix2Struct（282M参数）学生模型在经过RD微调后的表现优于基准模型4-5%的绝对准确度。",
    "tldr": "提出了一种称为Rationale Distillation的方法，通过合并OCR工具输出、LLMs和更大的多模态模型的中间“rationales”，并训练一个小型学生模型来预测rationales和答案，以实现对视觉文档的高效端到端理解。",
    "en_tdlr": "Proposed a method called Rationale Distillation, which combines the outputs of OCR tools, LLMs, and larger multimodal models as intermediate \"rationales\", and trains a small student model to predict both rationales and answers, achieving efficient end-to-end understanding of visual documents."
}