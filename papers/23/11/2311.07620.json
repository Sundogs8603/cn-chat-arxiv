{
    "title": "EPIM: Efficient Processing-In-Memory Accelerators based on Epitome",
    "abstract": "arXiv:2311.07620v2 Announce Type: replace-cross  Abstract: The utilization of large-scale neural networks on Processing-In-Memory (PIM) accelerators encounters challenges due to constrained on-chip memory capacity. To tackle this issue, current works explore model compression algorithms to reduce the size of Convolutional Neural Networks (CNNs). Most of these algorithms either aim to represent neural operators with reduced-size parameters (e.g., quantization) or search for the best combinations of neural operators (e.g., neural architecture search). Designing neural operators to align with PIM accelerators' specifications is an area that warrants further study. In this paper, we introduce the Epitome, a lightweight neural operator offering convolution-like functionality, to craft memory-efficient CNN operators for PIM accelerators (EPIM). On the software side, we evaluate epitomes' latency and energy on PIM accelerators and introduce a PIM-aware layer-wise design method to enhance thei",
    "link": "https://arxiv.org/abs/2311.07620",
    "context": "Title: EPIM: Efficient Processing-In-Memory Accelerators based on Epitome\nAbstract: arXiv:2311.07620v2 Announce Type: replace-cross  Abstract: The utilization of large-scale neural networks on Processing-In-Memory (PIM) accelerators encounters challenges due to constrained on-chip memory capacity. To tackle this issue, current works explore model compression algorithms to reduce the size of Convolutional Neural Networks (CNNs). Most of these algorithms either aim to represent neural operators with reduced-size parameters (e.g., quantization) or search for the best combinations of neural operators (e.g., neural architecture search). Designing neural operators to align with PIM accelerators' specifications is an area that warrants further study. In this paper, we introduce the Epitome, a lightweight neural operator offering convolution-like functionality, to craft memory-efficient CNN operators for PIM accelerators (EPIM). On the software side, we evaluate epitomes' latency and energy on PIM accelerators and introduce a PIM-aware layer-wise design method to enhance thei",
    "path": "papers/23/11/2311.07620.json",
    "total_tokens": 845,
    "translated_title": "EPIM: 基于Epitome的高效处理内存加速器",
    "translated_abstract": "处理内存（PIM）加速器上大规模神经网络的利用面临挑战，原因是片上内存容量受限。为了解决这个问题，目前的研究探讨了模型压缩算法，以减小卷积神经网络（CNNs）的大小。大多数这些算法要么旨在用具有减小参数大小的神经算子（例如，量化）表示神经算子，要么寻找神经算子的最佳组合（例如，神经架构搜索）。设计与PIM加速器规格相一致的神经算子是一个需要进一步研究的领域。在本文中，我们介绍Epitome，一种轻量级神经算子，提供类似卷积功能，以为PIM加速器（EPIM）设计内存高效的CNN算子。在软件方面，我们评估了Epitome在PIM加速器上的延迟和能量，并引入了一种PIM感知的逐层设计方法来增强它们。",
    "tldr": "本论文引入了Epitome，一种轻量级神经算子，为PIM加速器设计了内存高效的CNN算子（EPIM）。",
    "en_tdlr": "This paper introduces Epitome, a lightweight neural operator, to design memory-efficient CNN operators for PIM accelerators (EPIM)."
}