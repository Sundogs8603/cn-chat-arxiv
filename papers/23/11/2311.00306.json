{
    "title": "Probing Explicit and Implicit Gender Bias through LLM Conditional Text Generation. (arXiv:2311.00306v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) can generate biased and toxic responses. Yet most prior work on LLM gender bias evaluation requires predefined gender-related phrases or gender stereotypes, which are challenging to be comprehensively collected and are limited to explicit bias evaluation. In addition, we believe that instances devoid of gender-related language or explicit stereotypes in inputs can still induce gender bias in LLMs. Thus, in this work, we propose a conditional text generation mechanism without the need for predefined gender phrases and stereotypes. This approach employs three types of inputs generated through three distinct strategies to probe LLMs, aiming to show evidence of explicit and implicit gender biases in LLMs. We also utilize explicit and implicit evaluation metrics to evaluate gender bias in LLMs under different strategies. Our experiments demonstrate that an increased model size does not consistently lead to enhanced fairness and all tested LLMs exhibit explicit a",
    "link": "http://arxiv.org/abs/2311.00306",
    "context": "Title: Probing Explicit and Implicit Gender Bias through LLM Conditional Text Generation. (arXiv:2311.00306v1 [cs.CL])\nAbstract: Large Language Models (LLMs) can generate biased and toxic responses. Yet most prior work on LLM gender bias evaluation requires predefined gender-related phrases or gender stereotypes, which are challenging to be comprehensively collected and are limited to explicit bias evaluation. In addition, we believe that instances devoid of gender-related language or explicit stereotypes in inputs can still induce gender bias in LLMs. Thus, in this work, we propose a conditional text generation mechanism without the need for predefined gender phrases and stereotypes. This approach employs three types of inputs generated through three distinct strategies to probe LLMs, aiming to show evidence of explicit and implicit gender biases in LLMs. We also utilize explicit and implicit evaluation metrics to evaluate gender bias in LLMs under different strategies. Our experiments demonstrate that an increased model size does not consistently lead to enhanced fairness and all tested LLMs exhibit explicit a",
    "path": "papers/23/11/2311.00306.json",
    "total_tokens": 905,
    "translated_title": "通过LLM条件文本生成探测显性和隐性性别偏见",
    "translated_abstract": "大型语言模型（LLMs）可以生成偏见和有毒的回应。然而，大部分现有关于LLM性别偏见评估的工作都需要预定义的与性别相关的短语或性别刻板印象，这些信息难以全面收集且仅限于显性偏见评估。此外，我们认为在输入中没有与性别相关的语言或显性刻板印象的情况下，仍然可以在LLMs中诱发性别偏见。因此，在这项工作中，我们提出了一种无需预定义性别短语和刻板印象的条件文本生成机制。该方法通过三种不同策略生成的三种类型的输入来探测LLMs，旨在展示LLMs中显性和隐性性别偏见的证据。我们还利用显性和隐性评估指标评估了LLMs在不同策略下的性别偏见。我们的实验表明，模型规模的增加并不能一致地带来公平性的提高，所有测试的LLMs都表现出显性的性别偏见。",
    "tldr": "该论文提出了一种无需预定义性别短语和刻板印象的条件文本生成机制，通过三种不同策略生成的三种类型的输入来探测LLMs，展示了LLMs中显性和隐性性别偏见的证据。",
    "en_tdlr": "This paper presents a conditional text generation mechanism that does not require predefined gender phrases and stereotypes, probing LLMs using three different strategies to reveal evidence of explicit and implicit gender biases in LLMs."
}