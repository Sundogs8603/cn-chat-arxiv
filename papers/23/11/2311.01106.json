{
    "title": "In Defense of Softmax Parametrization for Calibrated and Consistent Learning to Defer. (arXiv:2311.01106v1 [cs.LG])",
    "abstract": "Enabling machine learning classifiers to defer their decision to a downstream expert when the expert is more accurate will ensure improved safety and performance. This objective can be achieved with the learning-to-defer framework which aims to jointly learn how to classify and how to defer to the expert. In recent studies, it has been theoretically shown that popular estimators for learning to defer parameterized with softmax provide unbounded estimates for the likelihood of deferring which makes them uncalibrated. However, it remains unknown whether this is due to the widely used softmax parameterization and if we can find a softmax-based estimator that is both statistically consistent and possesses a valid probability estimator. In this work, we first show that the cause of the miscalibrated and unbounded estimator in prior literature is due to the symmetric nature of the surrogate losses used and not due to softmax. We then propose a novel statistically consistent asymmetric softma",
    "link": "http://arxiv.org/abs/2311.01106",
    "context": "Title: In Defense of Softmax Parametrization for Calibrated and Consistent Learning to Defer. (arXiv:2311.01106v1 [cs.LG])\nAbstract: Enabling machine learning classifiers to defer their decision to a downstream expert when the expert is more accurate will ensure improved safety and performance. This objective can be achieved with the learning-to-defer framework which aims to jointly learn how to classify and how to defer to the expert. In recent studies, it has been theoretically shown that popular estimators for learning to defer parameterized with softmax provide unbounded estimates for the likelihood of deferring which makes them uncalibrated. However, it remains unknown whether this is due to the widely used softmax parameterization and if we can find a softmax-based estimator that is both statistically consistent and possesses a valid probability estimator. In this work, we first show that the cause of the miscalibrated and unbounded estimator in prior literature is due to the symmetric nature of the surrogate losses used and not due to softmax. We then propose a novel statistically consistent asymmetric softma",
    "path": "papers/23/11/2311.01106.json",
    "total_tokens": 897,
    "translated_title": "对于校准和一致学习推迟，采用Softmax参数化的辩护",
    "translated_abstract": "当下游专家更准确时，使得机器学习分类器能推迟其决策将确保提高安全性和性能。这个目标可以通过学习推迟框架来实现，该框架旨在共同学习如何分类和如何推迟给专家。最近的研究已经理论上证明，用Softmax参数化的学习推迟的流行估计器提供了无界估计，这使得它们无法校准。然而，目前尚不清楚这是否是由于广泛使用的Softmax参数化，以及我们是否可以找到一个既具有统计一致性又具有有效概率估计器的基于Softmax的估计器。在这项工作中，我们首先展示了先前文献中误校准和无界估计器的原因是由于所使用的代替损失的对称性质，而不是由于Softmax。然后，我们提出了一种新颖的统计一致性非对称Softmax估计器。",
    "tldr": "本文辩护了采用Softmax参数化的学习推迟方法，解释了先前关于其估计器的误校准和无界估计的原因，并提出了一种新颖的统计一致性非对称Softmax估计器。",
    "en_tdlr": "This paper defends the use of Softmax parametrization for learning to defer, explains the cause of the miscalibration and unbounded estimation in prior literature, and proposes a novel statistically consistent asymmetric Softmax estimator."
}