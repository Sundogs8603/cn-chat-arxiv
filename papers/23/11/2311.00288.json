{
    "title": "Active Instruction Tuning: Improving Cross-Task Generalization by Training on Prompt Sensitive Tasks. (arXiv:2311.00288v1 [cs.CL])",
    "abstract": "Instruction tuning (IT) achieves impressive zero-shot generalization results by training large language models (LLMs) on a massive amount of diverse tasks with instructions. However, how to select new tasks to improve the performance and generalizability of IT models remains an open question. Training on all existing tasks is impractical due to prohibiting computation requirements, and randomly selecting tasks can lead to suboptimal performance. In this work, we propose active instruction tuning based on prompt uncertainty, a novel framework to identify informative tasks, and then actively tune the models on the selected tasks. We represent the informativeness of new tasks with the disagreement of the current model outputs over perturbed prompts. Our experiments on NIV2 and Self-Instruct datasets demonstrate that our method consistently outperforms other baseline strategies for task selection, achieving better out-of-distribution generalization with fewer training tasks. Additionally, ",
    "link": "http://arxiv.org/abs/2311.00288",
    "context": "Title: Active Instruction Tuning: Improving Cross-Task Generalization by Training on Prompt Sensitive Tasks. (arXiv:2311.00288v1 [cs.CL])\nAbstract: Instruction tuning (IT) achieves impressive zero-shot generalization results by training large language models (LLMs) on a massive amount of diverse tasks with instructions. However, how to select new tasks to improve the performance and generalizability of IT models remains an open question. Training on all existing tasks is impractical due to prohibiting computation requirements, and randomly selecting tasks can lead to suboptimal performance. In this work, we propose active instruction tuning based on prompt uncertainty, a novel framework to identify informative tasks, and then actively tune the models on the selected tasks. We represent the informativeness of new tasks with the disagreement of the current model outputs over perturbed prompts. Our experiments on NIV2 and Self-Instruct datasets demonstrate that our method consistently outperforms other baseline strategies for task selection, achieving better out-of-distribution generalization with fewer training tasks. Additionally, ",
    "path": "papers/23/11/2311.00288.json",
    "total_tokens": 843,
    "translated_title": "主动指令调优：通过在敏感指令任务上训练来提高跨任务泛化能力",
    "translated_abstract": "指令调优（IT）通过在大量多样的任务上使用指令对大型语言模型（LLM）进行训练，取得了令人印象深刻的零样本泛化结果。然而，如何选择新任务以提高IT模型的性能和泛化能力仍然是一个未解决的问题。由于计算要求过高，训练所有现有任务是不可行的，而随机选择任务可能会导致亚优性能。在这项工作中，我们提出了基于提示不确定性的主动指令调优，一种识别信息丰富任务并在选定任务上主动调整模型的新框架。我们用当前模型输出在扰动提示上的不一致性表示新任务的信息丰富性。我们在NIV2和Self-Instruct数据集上的实验证明，我们的方法始终优于其他基准策略的任务选择，同时在更少的训练任务下实现了更好的超出分布的泛化能力。",
    "tldr": "本文提出了基于提示不确定性的主动指令调优方法，通过选择信息丰富的任务并主动调整模型，提高了跨任务泛化能力。",
    "en_tdlr": "This paper proposes active instruction tuning based on prompt uncertainty, which improves cross-task generalization by selecting informative tasks and actively adjusting the model."
}