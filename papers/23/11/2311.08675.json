{
    "title": "Refined Coreset Selection: Towards Minimal Coreset Size under Model Performance Constraints",
    "abstract": "arXiv:2311.08675v2 Announce Type: replace  Abstract: Coreset selection is powerful in reducing computational costs and accelerating data processing for deep learning algorithms. It strives to identify a small subset from large-scale data, so that training only on the subset practically performs on par with full data. Practitioners regularly desire to identify the smallest possible coreset in realistic scenes while maintaining comparable model performance, to minimize costs and maximize acceleration. Motivated by this desideratum, for the first time, we pose the problem of refined coreset selection, in which the minimal coreset size under model performance constraints is explored. Moreover, to address this problem, we propose an innovative method, which maintains optimization priority order over the model performance and coreset size, and efficiently optimizes them in the coreset selection procedure. Theoretically, we provide the convergence guarantee of the proposed method. Empirically",
    "link": "https://arxiv.org/abs/2311.08675",
    "context": "Title: Refined Coreset Selection: Towards Minimal Coreset Size under Model Performance Constraints\nAbstract: arXiv:2311.08675v2 Announce Type: replace  Abstract: Coreset selection is powerful in reducing computational costs and accelerating data processing for deep learning algorithms. It strives to identify a small subset from large-scale data, so that training only on the subset practically performs on par with full data. Practitioners regularly desire to identify the smallest possible coreset in realistic scenes while maintaining comparable model performance, to minimize costs and maximize acceleration. Motivated by this desideratum, for the first time, we pose the problem of refined coreset selection, in which the minimal coreset size under model performance constraints is explored. Moreover, to address this problem, we propose an innovative method, which maintains optimization priority order over the model performance and coreset size, and efficiently optimizes them in the coreset selection procedure. Theoretically, we provide the convergence guarantee of the proposed method. Empirically",
    "path": "papers/23/11/2311.08675.json",
    "total_tokens": 823,
    "translated_title": "经过模型性能约束的最小核心集大小精化选择",
    "translated_abstract": "核心集选择在减少计算成本、加速深度学习算法数据处理方面具有强大的作用。它致力于从大规模数据中识别一个小的子集，从而仅在子集上训练就能实际上与完整数据表现相当。实践者经常希望在现实场景中识别可能的最小核心集，同时保持可比较的模型性能，以最小化成本和最大化加速。受此愿景的启发，我们首次提出了经过精化的核心集选择问题，探讨了在模型性能约束下的最小核心集大小。此外，为了解决这个问题，我们提出了一种创新方法，该方法在核心集选择过程中保持优化优先顺序，优先考虑模型性能和核心集大小，并有效地优化它们。在理论上，我们提供了所提出方法的收敛保证。",
    "tldr": "探索了在模型性能约束下的最小核心集大小精化选择问题，并提出了一种创新方法来有效地优化核心集大小和模型性能，同时提供了理论上的收敛保证。"
}