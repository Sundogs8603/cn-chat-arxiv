{
    "title": "Convergence Analysis for Learning Orthonormal Deep Linear Neural Networks",
    "abstract": "arXiv:2311.14658v2 Announce Type: replace  Abstract: Enforcing orthonormal or isometric property for the weight matrices has been shown to enhance the training of deep neural networks by mitigating gradient exploding/vanishing and increasing the robustness of the learned networks. However, despite its practical performance, the theoretical analysis of orthonormality in neural networks is still lacking; for example, how orthonormality affects the convergence of the training process. In this letter, we aim to bridge this gap by providing convergence analysis for training orthonormal deep linear neural networks. Specifically, we show that Riemannian gradient descent with an appropriate initialization converges at a linear rate for training orthonormal deep linear neural networks with a class of loss functions. Unlike existing works that enforce orthonormal weight matrices for all the layers, our approach excludes this requirement for one layer, which is crucial to establish the convergenc",
    "link": "https://arxiv.org/abs/2311.14658",
    "context": "Title: Convergence Analysis for Learning Orthonormal Deep Linear Neural Networks\nAbstract: arXiv:2311.14658v2 Announce Type: replace  Abstract: Enforcing orthonormal or isometric property for the weight matrices has been shown to enhance the training of deep neural networks by mitigating gradient exploding/vanishing and increasing the robustness of the learned networks. However, despite its practical performance, the theoretical analysis of orthonormality in neural networks is still lacking; for example, how orthonormality affects the convergence of the training process. In this letter, we aim to bridge this gap by providing convergence analysis for training orthonormal deep linear neural networks. Specifically, we show that Riemannian gradient descent with an appropriate initialization converges at a linear rate for training orthonormal deep linear neural networks with a class of loss functions. Unlike existing works that enforce orthonormal weight matrices for all the layers, our approach excludes this requirement for one layer, which is crucial to establish the convergenc",
    "path": "papers/23/11/2311.14658.json",
    "total_tokens": 862,
    "translated_title": "学习正交深度线性神经网络的收敛分析",
    "translated_abstract": "强制权重矩阵具有正交或等距性质已被证明可以通过减轻梯度爆炸/消失来增强深度神经网络的训练，并增加学到的网络的鲁棒性。然而，尽管具有实际性能，但神经网络中正交性的理论分析仍然缺乏；例如，正交性如何影响训练过程的收敛。在这封信中，我们旨在填补这一空白，为训练正交深度线性神经网络提供收敛分析。具体地，我们展示了通过适当初始化的Riemannian梯度下降在一类损失函数下以线性速率收敛于训练正交深度线性神经网络。与现有的通过正交化所有层的工作不同，我们的方法排除了对一个层的正交权重矩阵的要求，这对建立收敛性至关重要。",
    "tldr": "提供了学习正交深度线性神经网络的收敛分析，通过Riemannian梯度下降以线性速率收敛于具有一类损失函数的训练正交深度线性神经网络。",
    "en_tdlr": "Convergence analysis is provided for training orthonormal deep linear neural networks, showing that Riemannian gradient descent converges at a linear rate for training orthonormal deep linear neural networks with a class of loss functions."
}