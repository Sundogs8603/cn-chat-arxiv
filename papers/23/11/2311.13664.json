{
    "title": "Sample as You Infer: Predictive Coding With Langevin Dynamics",
    "abstract": "We present a novel algorithm for parameter learning in generic deep generative models that builds upon the predictive coding (PC) framework of computational neuroscience. Our approach modifies the standard PC algorithm to bring performance on-par and exceeding that obtained from standard variational auto-encoder (VAE) training. By injecting Gaussian noise into the PC inference procedure we re-envision it as an overdamped Langevin sampling, which facilitates optimisation with respect to a tight evidence lower bound (ELBO). We improve the resultant encoder-free training method by incorporating an encoder network to provide an amortised warm-start to our Langevin sampling and test three different objectives for doing so. Finally, to increase robustness to the sampling step size and reduce sensitivity to curvature, we validate a lightweight and easily computable form of preconditioning, inspired by Riemann Manifold Langevin and adaptive optimizers from the SGD literature. We compare agains",
    "link": "https://arxiv.org/abs/2311.13664",
    "context": "Title: Sample as You Infer: Predictive Coding With Langevin Dynamics\nAbstract: We present a novel algorithm for parameter learning in generic deep generative models that builds upon the predictive coding (PC) framework of computational neuroscience. Our approach modifies the standard PC algorithm to bring performance on-par and exceeding that obtained from standard variational auto-encoder (VAE) training. By injecting Gaussian noise into the PC inference procedure we re-envision it as an overdamped Langevin sampling, which facilitates optimisation with respect to a tight evidence lower bound (ELBO). We improve the resultant encoder-free training method by incorporating an encoder network to provide an amortised warm-start to our Langevin sampling and test three different objectives for doing so. Finally, to increase robustness to the sampling step size and reduce sensitivity to curvature, we validate a lightweight and easily computable form of preconditioning, inspired by Riemann Manifold Langevin and adaptive optimizers from the SGD literature. We compare agains",
    "path": "papers/23/11/2311.13664.json",
    "total_tokens": 959,
    "translated_title": "以采样为导向: Langevin动力学的预测编码",
    "translated_abstract": "我们提出了一种新颖的算法，用于在通用深度生成模型中学习参数，该算法建立在计算神经科学的预测编码(PC)框架之上。我们的方法修改了标准的PC算法，使其性能与标准变分自动编码器(VAE)训练的性能相当甚至超过。通过将高斯噪声注入PC推理过程中，我们重新将其构想为过阻尼的Langevin采样，从而方便对紧凑证据下界(ELBO)进行优化。我们改进了结果编码器自由训练方法，通过将编码器网络纳入其中，为我们的Langevin采样提供了一种摊销的热启动，并测试了三种不同的目标。最后，为了增加对采样步长的鲁棒性，并减少对曲率的敏感性，我们验证了一种轻量级且易于计算的预处理形式，受到Riemann Manifold Langevin和SGD文献中的自适应优化器的启发。我们与...",
    "tldr": "本文提出了以采样为导向的Langevin动力学的预测编码算法，通过对PC推理过程注入高斯噪声实现过阻尼的Langevin采样，并改进了结果编码器自由训练方法，通过编码器网络提供摊销的热启动。此外，还验证了一种轻量级且易于计算的预处理形式，使得算法具有更好的性能和鲁棒性。",
    "en_tdlr": "This paper presents a sample-as-you-infer algorithm for predictive coding with Langevin dynamics, which injects Gaussian noise into the inference procedure to achieve overdamped Langevin sampling. The algorithm improves encoder-free training by incorporating an encoder network for warm-start and validates a lightweight preconditioning to increase robustness and performance."
}