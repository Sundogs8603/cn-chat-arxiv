{
    "title": "Generate and Pray: Using SALLMS to Evaluate the Security of LLM Generated Code. (arXiv:2311.00889v1 [cs.SE])",
    "abstract": "With the growing popularity of Large Language Models (e.g. GitHub Copilot, ChatGPT, etc.) in software engineers' daily practices, it is important to ensure that the code generated by these tools is not only functionally correct but also free of vulnerabilities. Although LLMs can help developers to be more productive, prior empirical studies have shown that LLMs can generate insecure code. There are two contributing factors to the insecure code generation. First, existing datasets used to evaluate Large Language Models (LLMs) do not adequately represent genuine software engineering tasks sensitive to security. Instead, they are often based on competitive programming challenges or classroom-type coding tasks. In real-world applications, the code produced is integrated into larger codebases, introducing potential security risks. There's a clear absence of benchmarks that focus on evaluating the security of the generated code. Second, existing evaluation metrics primarily focus on the func",
    "link": "http://arxiv.org/abs/2311.00889",
    "context": "Title: Generate and Pray: Using SALLMS to Evaluate the Security of LLM Generated Code. (arXiv:2311.00889v1 [cs.SE])\nAbstract: With the growing popularity of Large Language Models (e.g. GitHub Copilot, ChatGPT, etc.) in software engineers' daily practices, it is important to ensure that the code generated by these tools is not only functionally correct but also free of vulnerabilities. Although LLMs can help developers to be more productive, prior empirical studies have shown that LLMs can generate insecure code. There are two contributing factors to the insecure code generation. First, existing datasets used to evaluate Large Language Models (LLMs) do not adequately represent genuine software engineering tasks sensitive to security. Instead, they are often based on competitive programming challenges or classroom-type coding tasks. In real-world applications, the code produced is integrated into larger codebases, introducing potential security risks. There's a clear absence of benchmarks that focus on evaluating the security of the generated code. Second, existing evaluation metrics primarily focus on the func",
    "path": "papers/23/11/2311.00889.json",
    "total_tokens": 894,
    "translated_title": "生成和验证：使用SALLMS评估LLM生成的代码的安全性",
    "translated_abstract": "随着大型语言模型（例如GitHub Copilot，ChatGPT等）在软件工程师的日常实践中越来越受欢迎，确保这些工具生成的代码不仅功能正确，而且没有漏洞变得非常重要。尽管LLM可以帮助开发人员提高生产力，但之前的实证研究表明LLM可能会生成不安全的代码。存在两个导致不安全代码生成的因素。首先，用于评估大型语言模型（LLM）的现有数据集没有充分地代表与安全相关的真实软件工程任务。相反，它们通常基于竞技编程挑战或以课堂形式为基础的编码任务。在真实世界的应用中，生成的代码将被集成到更大的代码库中，引入潜在的安全风险。目前缺乏专注于评估生成代码安全性的基准。其次，现有的评估指标主要侧重于功能性而忽视安全性。",
    "tldr": "该论文研究了使用SALLMS评估LLM生成代码的安全性，指出现有数据集和评估指标未能充分考虑到与安全相关的真实软件工程任务，从而导致不安全的代码生成。"
}