{
    "title": "Look-Ahead Selective Plasticity for Continual Learning of Visual Tasks. (arXiv:2311.01617v1 [cs.CV])",
    "abstract": "Contrastive representation learning has emerged as a promising technique for continual learning as it can learn representations that are robust to catastrophic forgetting and generalize well to unseen future tasks. Previous work in continual learning has addressed forgetting by using previous task data and trained models. Inspired by event models created and updated in the brain, we propose a new mechanism that takes place during task boundaries, i.e., when one task finishes and another starts. By observing the redundancy-inducing ability of contrastive loss on the output of a neural network, our method leverages the first few samples of the new task to identify and retain parameters contributing most to the transfer ability of the neural network, freeing up the remaining parts of the network to learn new features. We evaluate the proposed methods on benchmark computer vision datasets including CIFAR10 and TinyImagenet and demonstrate state-of-the-art performance in the task-incrementa",
    "link": "http://arxiv.org/abs/2311.01617",
    "context": "Title: Look-Ahead Selective Plasticity for Continual Learning of Visual Tasks. (arXiv:2311.01617v1 [cs.CV])\nAbstract: Contrastive representation learning has emerged as a promising technique for continual learning as it can learn representations that are robust to catastrophic forgetting and generalize well to unseen future tasks. Previous work in continual learning has addressed forgetting by using previous task data and trained models. Inspired by event models created and updated in the brain, we propose a new mechanism that takes place during task boundaries, i.e., when one task finishes and another starts. By observing the redundancy-inducing ability of contrastive loss on the output of a neural network, our method leverages the first few samples of the new task to identify and retain parameters contributing most to the transfer ability of the neural network, freeing up the remaining parts of the network to learn new features. We evaluate the proposed methods on benchmark computer vision datasets including CIFAR10 and TinyImagenet and demonstrate state-of-the-art performance in the task-incrementa",
    "path": "papers/23/11/2311.01617.json",
    "total_tokens": 867,
    "translated_title": "提前选择性可塑性用于视觉任务的持续学习",
    "translated_abstract": "对比表示学习已经成为一种有前途的持续学习技术，因为它可以学习到对灾难性遗忘具有鲁棒性并且对未来的任务有很好泛化能力的表示。以大脑中创建和更新的事件模型为启发，我们提出了一种新的机制，该机制发生在任务边界，即一个任务结束并另一个任务开始时。通过观察对比损失对神经网络输出的冗余诱导能力，我们的方法利用新任务的前几个样本，识别和保留对神经网络的转移能力最有贡献的参数，从而释放网络的其余部分来学习新的特征。我们在诸如CIFAR10和TinyImagenet等基准计算机视觉数据集上评估了所提出的方法，并展示了在任务增量方面的最先进性能。",
    "tldr": "提出了一种新的持续学习机制，利用对比表示学习来减少灾难性遗忘，通过观察冗余诱导能力，识别并保留对神经网络转移能力最有贡献的参数，以实现在任务边界时的选择性可塑性。",
    "en_tdlr": "A new mechanism for continual learning is proposed, utilizing contrastive representation learning to reduce catastrophic forgetting, by observing redundancy-inducing ability, identifying and retaining parameters contributing most to the transfer ability of the neural network, achieving selective plasticity at task boundaries."
}