{
    "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems",
    "abstract": "arXiv:2311.09476v2 Announce Type: replace-cross  Abstract: Evaluating retrieval-augmented generation (RAG) systems traditionally relies on hand annotations for input queries, passages to retrieve, and responses to generate. We introduce ARES, an Automated RAG Evaluation System, for evaluating RAG systems along the dimensions of context relevance, answer faithfulness, and answer relevance. By creating its own synthetic training data, ARES finetunes lightweight LM judges to assess the quality of individual RAG components. To mitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints for prediction-powered inference (PPI). Across eight different knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARES accurately evaluates RAG systems while using only a few hundred human annotations during evaluation. Furthermore, ARES judges remain effective across domain shifts, proving accurate even after changing the type of queries and/or documents used in the eva",
    "link": "https://arxiv.org/abs/2311.09476",
    "context": "Title: ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems\nAbstract: arXiv:2311.09476v2 Announce Type: replace-cross  Abstract: Evaluating retrieval-augmented generation (RAG) systems traditionally relies on hand annotations for input queries, passages to retrieve, and responses to generate. We introduce ARES, an Automated RAG Evaluation System, for evaluating RAG systems along the dimensions of context relevance, answer faithfulness, and answer relevance. By creating its own synthetic training data, ARES finetunes lightweight LM judges to assess the quality of individual RAG components. To mitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints for prediction-powered inference (PPI). Across eight different knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARES accurately evaluates RAG systems while using only a few hundred human annotations during evaluation. Furthermore, ARES judges remain effective across domain shifts, proving accurate even after changing the type of queries and/or documents used in the eva",
    "path": "papers/23/11/2311.09476.json",
    "total_tokens": 847,
    "translated_title": "ARES: 用于检索增强生成系统的自动化评估框架",
    "translated_abstract": "评估检索增强生成（RAG）系统传统上依赖于手动注释输入查询、检索段落和生成响应。我们引入了ARES，一个用于评估RAG系统的自动化评估系统，评估维度包括上下文相关性、答案忠实度和答案相关性。通过创建自己的合成训练数据，ARES微调轻量级LM评估器以评估单个RAG组件的质量。为了减少潜在的预测错误，ARES利用少量人工注释数据集进行预测驱动推理（PPI）。在KILT、SuperGLUE和AIS的八个不同知识密集型任务中，ARES在评估过程中仅使用少量人工注释就准确评估RAG系统。此外，ARES评估器在领域转移中仍然有效，即使在更改用于评估的查询和/或文档类型后仍然准确。",
    "tldr": "ARES是用于评估检索增强生成系统的自动化评估框架，通过创建合成训练数据和微调评估器，有效评估RAG系统在不同任务中的表现。",
    "en_tdlr": "ARES is an automated evaluation framework for retrieval-augmented generation systems, effectively evaluating the performance of RAG systems across different tasks by creating synthetic training data and finetuning evaluators."
}