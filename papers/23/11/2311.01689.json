{
    "title": "Data-Free Distillation of Language Model by Text-to-Text Transfer. (arXiv:2311.01689v1 [cs.CL])",
    "abstract": "Data-Free Knowledge Distillation (DFKD) plays a vital role in compressing the model when original training data is unavailable. Previous works for DFKD in NLP mainly focus on distilling encoder-only structures like BERT on classification tasks, which overlook the notable progress of generative language modeling. In this work, we propose a novel DFKD framework, namely DFKD-T$^{3}$, where the pretrained generative language model can also serve as a controllable data generator for model compression. This novel framework DFKD-T$^{3}$ leads to an end-to-end learnable text-to-text framework to transform the general domain corpus to compression-friendly task data, targeting to improve both the \\textit{specificity} and \\textit{diversity}. Extensive experiments show that our method can boost the distillation performance in various downstream tasks such as sentiment analysis, linguistic acceptability, and information extraction. Furthermore, we show that the generated texts can be directly used ",
    "link": "http://arxiv.org/abs/2311.01689",
    "context": "Title: Data-Free Distillation of Language Model by Text-to-Text Transfer. (arXiv:2311.01689v1 [cs.CL])\nAbstract: Data-Free Knowledge Distillation (DFKD) plays a vital role in compressing the model when original training data is unavailable. Previous works for DFKD in NLP mainly focus on distilling encoder-only structures like BERT on classification tasks, which overlook the notable progress of generative language modeling. In this work, we propose a novel DFKD framework, namely DFKD-T$^{3}$, where the pretrained generative language model can also serve as a controllable data generator for model compression. This novel framework DFKD-T$^{3}$ leads to an end-to-end learnable text-to-text framework to transform the general domain corpus to compression-friendly task data, targeting to improve both the \\textit{specificity} and \\textit{diversity}. Extensive experiments show that our method can boost the distillation performance in various downstream tasks such as sentiment analysis, linguistic acceptability, and information extraction. Furthermore, we show that the generated texts can be directly used ",
    "path": "papers/23/11/2311.01689.json",
    "total_tokens": 973,
    "translated_title": "无数据的语言模型蒸馏：通过文本到文本转换实现",
    "translated_abstract": "当原始训练数据不可用时，无数据知识蒸馏（DFKD）在压缩模型方面起着至关重要的作用。先前在自然语言处理领域对DFKD的研究主要集中在对类别任务进行蒸馏的仅编码器结构，忽视了生成式语言建模的重要进展。在本研究中，我们提出了一种新颖的DFKD框架，名为DFKD-T$^{3}$，预训练的生成式语言模型也可以作为一个可控的数据生成器，用于模型压缩。这个新颖的DFKD-T$^{3}$框架导致了一个端到端可学习的文本到文本框架，将通用领域语料库转化为压缩友好的任务数据，旨在提高模型的特异性和多样性。大量实验证明我们的方法可以提升在情感分析、语言可接受性和信息提取等各种下游任务中的蒸馏性能。此外，我们还展示了生成的文本可以直接应用于...",
    "tldr": "本文提出了一种无数据知识蒸馏（DFKD）框架，即DFKD-T$^{3}$，利用预训练的生成式语言模型作为数据生成器，将通用领域语料库转化为压缩友好的任务数据。实验证明该方法能够提升各种下游任务的蒸馏性能。",
    "en_tdlr": "This paper presents a data-free knowledge distillation framework called DFKD-T$^{3}$, which utilizes a pretrained generative language model as a data generator to transform general domain corpus into compression-friendly task data. Experimental results demonstrate that this method improves distillation performance in various downstream tasks."
}