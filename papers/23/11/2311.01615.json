{
    "title": "FLAP: Fast Language-Audio Pre-training. (arXiv:2311.01615v1 [cs.SD])",
    "abstract": "We propose Fast Language-Audio Pre-training (FLAP), a self-supervised approach that efficiently and effectively learns aligned audio and language representations through masking, contrastive learning and reconstruction. For efficiency, FLAP randomly drops audio spectrogram tokens, focusing solely on the remaining ones for self-supervision. Through inter-modal contrastive learning, FLAP learns to align paired audio and text representations in a shared latent space. Notably, FLAP leverages multiple augmented views via masking for inter-modal contrast and learns to reconstruct the masked portion of audio tokens. Moreover, FLAP leverages large language models (LLMs) to augment the text inputs, contributing to improved performance. These approaches lead to more robust and informative audio-text representations, enabling FLAP to achieve state-of-the-art (SoTA) performance on audio-text retrieval tasks on AudioCaps (achieving 53.0% R@1) and Clotho (achieving 25.5% R@1).",
    "link": "http://arxiv.org/abs/2311.01615",
    "context": "Title: FLAP: Fast Language-Audio Pre-training. (arXiv:2311.01615v1 [cs.SD])\nAbstract: We propose Fast Language-Audio Pre-training (FLAP), a self-supervised approach that efficiently and effectively learns aligned audio and language representations through masking, contrastive learning and reconstruction. For efficiency, FLAP randomly drops audio spectrogram tokens, focusing solely on the remaining ones for self-supervision. Through inter-modal contrastive learning, FLAP learns to align paired audio and text representations in a shared latent space. Notably, FLAP leverages multiple augmented views via masking for inter-modal contrast and learns to reconstruct the masked portion of audio tokens. Moreover, FLAP leverages large language models (LLMs) to augment the text inputs, contributing to improved performance. These approaches lead to more robust and informative audio-text representations, enabling FLAP to achieve state-of-the-art (SoTA) performance on audio-text retrieval tasks on AudioCaps (achieving 53.0% R@1) and Clotho (achieving 25.5% R@1).",
    "path": "papers/23/11/2311.01615.json",
    "total_tokens": 932,
    "translated_title": "FLAP: 快速语言音频预训练",
    "translated_abstract": "我们提出了一种名为FLAP的快速语言音频预训练方法，通过掩盖、对比学习和重构，有效地学习对齐的音频和语言表示。为了提高效率，FLAP随机丢掉音频谱图的标记，仅专注于剩余标记进行自监督学习。通过跨模态对比学习，FLAP学习将配对的音频和文本表示在共享潜在空间中对齐。值得注意的是，FLAP利用多个掩模视图进行跨模态对比，并学习重构音频标记的掩模部分。此外，FLAP利用大型语言模型（LLMs）增强文本输入，从而提高了性能。这些方法导致更强大且信息丰富的音频-文本表示，使得FLAP在AudioCaps（达到53.0％ R@1）和Clotho（达到25.5％ R@1）的音频-文本检索任务中实现了最新技术性能（SoTA）。",
    "tldr": "FLAP是一种快速的语言音频预训练方法，通过掩盖、对比学习和重构，有效地学习对齐的音频和语言表示，以及利用大型语言模型（LLMs）增强文本输入。该方法在音频-文本检索任务上取得了最新技术性能（SoTA）。"
}