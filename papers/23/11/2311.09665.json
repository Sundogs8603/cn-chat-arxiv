{
    "title": "The Wisdom of Partisan Crowds: Comparing Collective Intelligence in Humans and LLM-based Agents",
    "abstract": "arXiv:2311.09665v2 Announce Type: replace  Abstract: Human groups are able to converge on more accurate beliefs through deliberation, even in the presence of polarization and partisan bias -- a phenomenon known as the \"wisdom of partisan crowds.\" Generated agents powered by Large Language Models (LLMs) are increasingly used to simulate human collective behavior, yet few benchmarks exist for evaluating their dynamics against the behavior of human groups. In this paper, we examine the extent to which the wisdom of partisan crowds emerges in groups of LLM-based agents that are prompted to role-play as partisan personas (e.g., Democrat or Republican). We find that they not only display human-like partisan biases, but also converge to more accurate beliefs through deliberation as humans do. We then identify several factors that interfere with convergence, including the use of chain-of-thought prompt and lack of details in personas. Conversely, fine-tuning on human data appears to enhance co",
    "link": "https://arxiv.org/abs/2311.09665",
    "context": "Title: The Wisdom of Partisan Crowds: Comparing Collective Intelligence in Humans and LLM-based Agents\nAbstract: arXiv:2311.09665v2 Announce Type: replace  Abstract: Human groups are able to converge on more accurate beliefs through deliberation, even in the presence of polarization and partisan bias -- a phenomenon known as the \"wisdom of partisan crowds.\" Generated agents powered by Large Language Models (LLMs) are increasingly used to simulate human collective behavior, yet few benchmarks exist for evaluating their dynamics against the behavior of human groups. In this paper, we examine the extent to which the wisdom of partisan crowds emerges in groups of LLM-based agents that are prompted to role-play as partisan personas (e.g., Democrat or Republican). We find that they not only display human-like partisan biases, but also converge to more accurate beliefs through deliberation as humans do. We then identify several factors that interfere with convergence, including the use of chain-of-thought prompt and lack of details in personas. Conversely, fine-tuning on human data appears to enhance co",
    "path": "papers/23/11/2311.09665.json",
    "total_tokens": 903,
    "translated_title": "党派群体的智慧：比较人类和基于LLM的代理人的集体智能",
    "translated_abstract": "人类群体能够通过商讨达成更准确的信念，即使在存在极化和党派偏见的情况下也是如此，这一现象被称为“党派群体的智慧”。由大型语言模型（LLMs）驱动的生成代理人越来越多地被用来模拟人类集体行为，然而很少有基准用于评估它们的动态与人类群体行为的对比。在本文中，我们研究了在提示扮演党派人物（例如，民主党人或共和党人）的LLM代理人群体中，党派群体的智慧出现的程度。我们发现他们不仅显示出类似于人类的党派偏见，而且通过商讨像人类一样收敛到更准确的信念。然后，我们确定了几个干扰收敛的因素，包括链式思维提示的使用和人物缺乏细节。相反，对人类数据进行微调似乎增强",
    "tldr": "本文研究了基于LLM的代理人在扮演党派角色时，展现出类似于人类群体的党派偏见、并通过商讨收敛到更准确信念的能力。",
    "en_tdlr": "This paper examines the ability of LLM-based agents to display human-like partisan biases and converge to more accurate beliefs through deliberation when role-playing as partisan personas."
}