{
    "title": "Heterogeneous federated collaborative filtering using FAIR: Federated Averaging in Random Subspaces. (arXiv:2311.01722v1 [cs.LG])",
    "abstract": "Recommendation systems (RS) for items (e.g., movies, books) and ads are widely used to tailor content to users on various internet platforms. Traditionally, recommendation models are trained on a central server. However, due to rising concerns for data privacy and regulations like the GDPR, federated learning is an increasingly popular paradigm in which data never leaves the client device. Applying federated learning to recommendation models is non-trivial due to large embedding tables, which often exceed the memory constraints of most user devices. To include data from all devices in federated learning, we must enable collective training of embedding tables on devices with heterogeneous memory capacities. Current solutions to heterogeneous federated learning can only accommodate a small range of capacities and thus limit the number of devices that can participate in training. We present Federated Averaging in Random subspaces (FAIR), which allows arbitrary compression of embedding tab",
    "link": "http://arxiv.org/abs/2311.01722",
    "context": "Title: Heterogeneous federated collaborative filtering using FAIR: Federated Averaging in Random Subspaces. (arXiv:2311.01722v1 [cs.LG])\nAbstract: Recommendation systems (RS) for items (e.g., movies, books) and ads are widely used to tailor content to users on various internet platforms. Traditionally, recommendation models are trained on a central server. However, due to rising concerns for data privacy and regulations like the GDPR, federated learning is an increasingly popular paradigm in which data never leaves the client device. Applying federated learning to recommendation models is non-trivial due to large embedding tables, which often exceed the memory constraints of most user devices. To include data from all devices in federated learning, we must enable collective training of embedding tables on devices with heterogeneous memory capacities. Current solutions to heterogeneous federated learning can only accommodate a small range of capacities and thus limit the number of devices that can participate in training. We present Federated Averaging in Random subspaces (FAIR), which allows arbitrary compression of embedding tab",
    "path": "papers/23/11/2311.01722.json",
    "total_tokens": 907,
    "translated_title": "使用FAIR的异构联邦协同过滤：在随机子空间中的联邦平均 (arXiv:2311.01722v1 [cs.LG])",
    "translated_abstract": "推荐系统（RS）广泛用于各种互联网平台上，用于根据用户的喜好个性化推荐内容。传统的推荐模型是在中央服务器上进行训练的。然而，由于对数据隐私的关注和GDPR等法规的出台，联邦学习成为越来越受欢迎的范式，其中数据永远不离开客户端设备。将联邦学习应用于推荐模型是非常困难的，因为嵌入表往往超出了大多数用户设备的内存限制。为了将所有设备的数据纳入联邦学习中，我们必须实现在内存能力不同的设备上对嵌入表进行集体训练。当前的异构联邦学习解决方案只能容纳一小部分能力范围，从而限制了能参与训练的设备数量。我们提出了在随机子空间中的联邦平均（FAIR），它允许对嵌入表进行任意压缩。",
    "tldr": "使用FAIR方法解决了异构联邦学习中的内存限制问题，实现了在空间不同的设备上集体训练嵌入表，用于推荐系统的个性化推荐。",
    "en_tdlr": "FAIR addresses the memory constraints in heterogeneous federated learning by enabling collective training of embedding tables on devices with different memory capacities, facilitating personalized recommendations for recommendation systems."
}