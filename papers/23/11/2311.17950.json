{
    "title": "Generalized Large-Scale Data Condensation via Various Backbone and Statistical Matching",
    "abstract": "arXiv:2311.17950v2 Announce Type: replace-cross  Abstract: The lightweight \"local-match-global\" matching introduced by SRe2L successfully creates a distilled dataset with comprehensive information on the full 224x224 ImageNet-1k. However, this one-sided approach is limited to a particular backbone, layer, and statistics, which limits the improvement of the generalization of a distilled dataset. We suggest that sufficient and various \"local-match-global\" matching are more precise and effective than a single one and has the ability to create a distilled dataset with richer information and better generalization. We call this perspective \"generalized matching\" and propose Generalized Various Backbone and Statistical Matching (G-VBSM) in this work, which aims to create a synthetic dataset with densities, ensuring consistency with the complete dataset across various backbones, layers, and statistics. As experimentally demonstrated, G-VBSM is the first algorithm to obtain strong performance a",
    "link": "https://arxiv.org/abs/2311.17950",
    "context": "Title: Generalized Large-Scale Data Condensation via Various Backbone and Statistical Matching\nAbstract: arXiv:2311.17950v2 Announce Type: replace-cross  Abstract: The lightweight \"local-match-global\" matching introduced by SRe2L successfully creates a distilled dataset with comprehensive information on the full 224x224 ImageNet-1k. However, this one-sided approach is limited to a particular backbone, layer, and statistics, which limits the improvement of the generalization of a distilled dataset. We suggest that sufficient and various \"local-match-global\" matching are more precise and effective than a single one and has the ability to create a distilled dataset with richer information and better generalization. We call this perspective \"generalized matching\" and propose Generalized Various Backbone and Statistical Matching (G-VBSM) in this work, which aims to create a synthetic dataset with densities, ensuring consistency with the complete dataset across various backbones, layers, and statistics. As experimentally demonstrated, G-VBSM is the first algorithm to obtain strong performance a",
    "path": "papers/23/11/2311.17950.json",
    "total_tokens": 863,
    "translated_title": "通过不同的主干和统计匹配进行广义大规模数据压缩",
    "translated_abstract": "SRe2L引入的轻量级“局部匹配-全局匹配”成功地创造了一个经过精心筛选的数据集，其中包含来自完整的224x224 ImageNet-1k的全面信息。然而，这种单边方法只适用于特定的主干、层和统计数据，从而限制了精简数据集概括能力的提升。我们建议充分而各异的“局部匹配-全局匹配”比单一匹配更加精确和有效，并能够创造出更丰富信息和更好概括能力的压缩数据集。我们称之为“广义匹配”观点，并在本文中提出了广义不同主干和统计匹配 (G-VBSM)，旨在创建一个具有密度的合成数据集，确保在不同的主干、层和统计数据上与完整数据集保持一致。实验证明，G-VBSM是第一个获得强大性能的算法。",
    "tldr": "本文提出了广义匹配的概念，并在此基础上提出了Generalized Various Backbone and Statistical Matching (G-VBSM)方法，可以创建一个具有丰富信息和更好概括能力的压缩数据集。"
}