{
    "title": "RiskQ: Risk-sensitive Multi-Agent Reinforcement Learning Value Factorization. (arXiv:2311.01753v1 [cs.MA])",
    "abstract": "Multi-agent systems are characterized by environmental uncertainty, varying policies of agents, and partial observability, which result in significant risks. In the context of Multi-Agent Reinforcement Learning (MARL), learning coordinated and decentralized policies that are sensitive to risk is challenging. To formulate the coordination requirements in risk-sensitive MARL, we introduce the Risk-sensitive Individual-Global-Max (RIGM) principle as a generalization of the Individual-Global-Max (IGM) and Distributional IGM (DIGM) principles. This principle requires that the collection of risk-sensitive action selections of each agent should be equivalent to the risk-sensitive action selection of the central policy. Current MARL value factorization methods do not satisfy the RIGM principle for common risk metrics such as the Value at Risk (VaR) metric or distorted risk measurements. Therefore, we propose RiskQ to address this limitation, which models the joint return distribution by modeli",
    "link": "http://arxiv.org/abs/2311.01753",
    "context": "Title: RiskQ: Risk-sensitive Multi-Agent Reinforcement Learning Value Factorization. (arXiv:2311.01753v1 [cs.MA])\nAbstract: Multi-agent systems are characterized by environmental uncertainty, varying policies of agents, and partial observability, which result in significant risks. In the context of Multi-Agent Reinforcement Learning (MARL), learning coordinated and decentralized policies that are sensitive to risk is challenging. To formulate the coordination requirements in risk-sensitive MARL, we introduce the Risk-sensitive Individual-Global-Max (RIGM) principle as a generalization of the Individual-Global-Max (IGM) and Distributional IGM (DIGM) principles. This principle requires that the collection of risk-sensitive action selections of each agent should be equivalent to the risk-sensitive action selection of the central policy. Current MARL value factorization methods do not satisfy the RIGM principle for common risk metrics such as the Value at Risk (VaR) metric or distorted risk measurements. Therefore, we propose RiskQ to address this limitation, which models the joint return distribution by modeli",
    "path": "papers/23/11/2311.01753.json",
    "total_tokens": 937,
    "translated_title": "RiskQ: 风险敏感的多智能体强化学习价值因子分解",
    "translated_abstract": "多智能体系统特点是环境不确定性、智能体的策略多样性和部分可观测性，这导致了显著的风险。在多智能体强化学习（MARL）的背景下，学习对风险敏感的协调和分散策略是具有挑战性的。为了在风险敏感的MARL中制定协调要求，我们介绍了风险敏感的个体-全局最大（RIGM）原理，作为个体-全局最大（IGM）和分布式IGM（DIGM）原理的一种推广。该原理要求每个智能体的风险敏感动作选择集合应与中央策略的风险敏感动作选择等价。当前的MARL价值因子分解方法对于常见的风险度量（例如风险价值（VaR）度量或扭曲的风险度量）不满足RIGM原则。因此，我们提出了RiskQ来解决这个限制，通过建模联合回报分布来实现价值因子分解。",
    "tldr": "RiskQ是一种解决多智能体强化学习中风险敏感协调要求的方法，通过引入风险敏感的个体-全局最大（RIGM）原则和建模联合回报分布实现价值因子分解。",
    "en_tdlr": "RiskQ is a method that addresses the coordination requirements for risk sensitivity in multi-agent reinforcement learning by introducing the Risk-sensitive Individual-Global-Max (RIGM) principle and modeling the joint return distribution for value factorization."
}