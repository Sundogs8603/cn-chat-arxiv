{
    "title": "Enhanced Generalization through Prioritization and Diversity in Self-Imitation Reinforcement Learning over Procedural Environments with Sparse Rewards. (arXiv:2311.00426v1 [cs.LG])",
    "abstract": "Exploration poses a fundamental challenge in Reinforcement Learning (RL) with sparse rewards, limiting an agent's ability to learn optimal decision-making due to a lack of informative feedback signals. Self-Imitation Learning (self-IL) has emerged as a promising approach for exploration, leveraging a replay buffer to store and reproduce successful behaviors. However, traditional self-IL methods, which rely on high-return transitions and assume singleton environments, face challenges in generalization, especially in procedurally-generated (PCG) environments. Therefore, new self-IL methods have been proposed to rank which experiences to persist, but they replay transitions uniformly regardless of their significance, and do not address the diversity of the stored demonstrations. In this work, we propose tailored self-IL sampling strategies by prioritizing transitions in different ways and extending prioritization techniques to PCG environments. We also address diversity loss through modif",
    "link": "http://arxiv.org/abs/2311.00426",
    "context": "Title: Enhanced Generalization through Prioritization and Diversity in Self-Imitation Reinforcement Learning over Procedural Environments with Sparse Rewards. (arXiv:2311.00426v1 [cs.LG])\nAbstract: Exploration poses a fundamental challenge in Reinforcement Learning (RL) with sparse rewards, limiting an agent's ability to learn optimal decision-making due to a lack of informative feedback signals. Self-Imitation Learning (self-IL) has emerged as a promising approach for exploration, leveraging a replay buffer to store and reproduce successful behaviors. However, traditional self-IL methods, which rely on high-return transitions and assume singleton environments, face challenges in generalization, especially in procedurally-generated (PCG) environments. Therefore, new self-IL methods have been proposed to rank which experiences to persist, but they replay transitions uniformly regardless of their significance, and do not address the diversity of the stored demonstrations. In this work, we propose tailored self-IL sampling strategies by prioritizing transitions in different ways and extending prioritization techniques to PCG environments. We also address diversity loss through modif",
    "path": "papers/23/11/2311.00426.json",
    "total_tokens": 813,
    "translated_title": "基于稀疏奖励的自我模仿强化学习在过程化环境中通过优先级和多样性增强泛化能力",
    "translated_abstract": "在稀疏奖励的强化学习中，探索是一个基本挑战，限制了智能体学习最优决策的能力，因为缺乏信息反馈信号。自我模仿学习已经成为一种有前途的探索方法，利用回放缓冲区来存储和重现成功的行为。然而，传统的自我模仿学习方法在泛化方面面临挑战，特别是在过程化生成的环境中。因此，本研究提出了通过不同方式对转换进行优先级排序的定制自我模仿学习采样策略，并将优先级技术扩展到过程化生成的环境。我们还通过修改回放缓冲区中的演示来解决多样性损失问题。",
    "tldr": "本研究通过优先级排序和多样性来提高自我模仿强化学习在过程化环境中的泛化能力。"
}