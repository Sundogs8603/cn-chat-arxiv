{
    "title": "Program Machine Policy: Addressing Long-Horizon Tasks by Integrating Program Synthesis and State Machines",
    "abstract": "Deep reinforcement learning (deep RL) excels in various domains but lacks generalizability and interpretability. On the other hand, programmatic RL methods (Trivedi et al., 2021; Liu et al., 2023) reformulate RL tasks as synthesizing interpretable programs that can be executed in the environments. Despite encouraging results, these methods are limited to short-horizon tasks. On the other hand, representing RL policies using state machines (Inala et al., 2020) can inductively generalize to long-horizon tasks; however, it struggles to scale up to acquire diverse and complex behaviors. This work proposes the Program Machine Policy (POMP), which bridges the advantages of programmatic RL and state machine policies, allowing for the representation of complex behaviors and the address of long-term tasks. Specifically, we introduce a method that can retrieve a set of effective, diverse, and compatible programs. Then, we use these programs as modes of a state machine and learn a transition func",
    "link": "https://arxiv.org/abs/2311.15960",
    "context": "Title: Program Machine Policy: Addressing Long-Horizon Tasks by Integrating Program Synthesis and State Machines\nAbstract: Deep reinforcement learning (deep RL) excels in various domains but lacks generalizability and interpretability. On the other hand, programmatic RL methods (Trivedi et al., 2021; Liu et al., 2023) reformulate RL tasks as synthesizing interpretable programs that can be executed in the environments. Despite encouraging results, these methods are limited to short-horizon tasks. On the other hand, representing RL policies using state machines (Inala et al., 2020) can inductively generalize to long-horizon tasks; however, it struggles to scale up to acquire diverse and complex behaviors. This work proposes the Program Machine Policy (POMP), which bridges the advantages of programmatic RL and state machine policies, allowing for the representation of complex behaviors and the address of long-term tasks. Specifically, we introduce a method that can retrieve a set of effective, diverse, and compatible programs. Then, we use these programs as modes of a state machine and learn a transition func",
    "path": "papers/23/11/2311.15960.json",
    "total_tokens": 850,
    "translated_title": "程序机器策略：通过集成程序合成和状态机解决长期任务",
    "translated_abstract": "深度强化学习在各个领域表现出色，但缺乏泛化能力和解释性。另一方面，编程式强化学习方法重新定义了强化学习任务，将其视为合成可解释的程序，可以在环境中执行。尽管取得了鼓舞人心的结果，但这些方法局限于短期任务。另一方面，使用状态机表示强化学习策略可以归纳推广到长期任务；然而，它在获取多样和复杂行为方面存在困难。本研究提出了程序机器策略（POMP），以桥接编程式强化学习和状态机策略的优势，允许表示复杂行为并解决长期任务。具体而言，我们介绍了一种方法，可以检索一组有效、多样且兼容的程序。然后，我们将这些程序用作状态机的模式，并学习一个转移函数。",
    "tldr": "这项工作提出了程序机器策略（POMP），在集成程序合成和状态机的基础上，解决了长期任务并表示复杂行为。",
    "en_tdlr": "This work proposes the Program Machine Policy (POMP), which integrates program synthesis and state machines to address long-horizon tasks and represent complex behaviors."
}