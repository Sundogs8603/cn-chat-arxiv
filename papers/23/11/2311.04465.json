{
    "title": "Solving High Frequency and Multi-Scale PDEs with Gaussian Processes",
    "abstract": "arXiv:2311.04465v2 Announce Type: replace  Abstract: Machine learning based solvers have garnered much attention in physical simulation and scientific computing, with a prominent example, physics-informed neural networks (PINNs). However, PINNs often struggle to solve high-frequency and multi-scale PDEs, which can be due to spectral bias during neural network training. To address this problem, we resort to the Gaussian process (GP) framework. To flexibly capture the dominant frequencies, we model the power spectrum of the PDE solution with a student $t$ mixture or Gaussian mixture. We apply the inverse Fourier transform to obtain the covariance function (by Wiener-Khinchin theorem). The covariance derived from the Gaussian mixture spectrum corresponds to the known spectral mixture kernel. Next, we estimate the mixture weights in the log domain, which we show is equivalent to placing a Jeffreys prior. It automatically induces sparsity, prunes excessive frequencies, and adjusts the remai",
    "link": "https://arxiv.org/abs/2311.04465",
    "context": "Title: Solving High Frequency and Multi-Scale PDEs with Gaussian Processes\nAbstract: arXiv:2311.04465v2 Announce Type: replace  Abstract: Machine learning based solvers have garnered much attention in physical simulation and scientific computing, with a prominent example, physics-informed neural networks (PINNs). However, PINNs often struggle to solve high-frequency and multi-scale PDEs, which can be due to spectral bias during neural network training. To address this problem, we resort to the Gaussian process (GP) framework. To flexibly capture the dominant frequencies, we model the power spectrum of the PDE solution with a student $t$ mixture or Gaussian mixture. We apply the inverse Fourier transform to obtain the covariance function (by Wiener-Khinchin theorem). The covariance derived from the Gaussian mixture spectrum corresponds to the known spectral mixture kernel. Next, we estimate the mixture weights in the log domain, which we show is equivalent to placing a Jeffreys prior. It automatically induces sparsity, prunes excessive frequencies, and adjusts the remai",
    "path": "papers/23/11/2311.04465.json",
    "total_tokens": 909,
    "translated_title": "使用高斯过程解决高频和多尺度偏微分方程",
    "translated_abstract": "基于机器学习的求解器在物理模拟和科学计算中引起了广泛关注，其中一个著名示例是基于物理信息的神经网络（PINNs）。然而，PINNs在解决高频和多尺度偏微分方程时经常遇到困难，这可能是由于神经网络训练中的谱偏差引起的。为了解决这一问题，我们使用了高斯过程（GP）框架。为了灵活地捕捉主导频率，我们使用学生$t$混合或高斯混合来建模PDE解的功率谱。我们应用逆Fourier变换来获得协方差函数（通过Wiener-Khinchin定理）。从高斯混合谱导出的协方差对应于已知的谱混合核。接下来，我们在对数域中估计混合权重，我们表明这等效于放置Jeffreys先验。它自动诱导稀疏性，修剪过多的频率，并调整剩下的部分。",
    "tldr": "使用高斯过程框架来解决高频和多尺度偏微分方程，通过灵活捕捉主导频率的方法，估计混合权重并自动诱导稀疏性，有效解决了神经网络训练中的谱偏差问题。"
}