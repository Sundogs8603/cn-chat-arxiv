{
    "title": "AdaSent: Efficient Domain-Adapted Sentence Embeddings for Few-Shot Classification. (arXiv:2311.00408v1 [cs.CL])",
    "abstract": "Recent work has found that few-shot sentence classification based on pre-trained Sentence Encoders (SEs) is efficient, robust, and effective. In this work, we investigate strategies for domain-specialization in the context of few-shot sentence classification with SEs. We first establish that unsupervised Domain-Adaptive Pre-Training (DAPT) of a base Pre-trained Language Model (PLM) (i.e., not an SE) substantially improves the accuracy of few-shot sentence classification by up to 8.4 points. However, applying DAPT on SEs, on the one hand, disrupts the effects of their (general-domain) Sentence Embedding Pre-Training (SEPT). On the other hand, applying general-domain SEPT on top of a domain-adapted base PLM (i.e., after DAPT) is effective but inefficient, since the computationally expensive SEPT needs to be executed on top of a DAPT-ed PLM of each domain. As a solution, we propose AdaSent, which decouples SEPT from DAPT by training a SEPT adapter on the base PLM. The adapter can be inser",
    "link": "http://arxiv.org/abs/2311.00408",
    "context": "Title: AdaSent: Efficient Domain-Adapted Sentence Embeddings for Few-Shot Classification. (arXiv:2311.00408v1 [cs.CL])\nAbstract: Recent work has found that few-shot sentence classification based on pre-trained Sentence Encoders (SEs) is efficient, robust, and effective. In this work, we investigate strategies for domain-specialization in the context of few-shot sentence classification with SEs. We first establish that unsupervised Domain-Adaptive Pre-Training (DAPT) of a base Pre-trained Language Model (PLM) (i.e., not an SE) substantially improves the accuracy of few-shot sentence classification by up to 8.4 points. However, applying DAPT on SEs, on the one hand, disrupts the effects of their (general-domain) Sentence Embedding Pre-Training (SEPT). On the other hand, applying general-domain SEPT on top of a domain-adapted base PLM (i.e., after DAPT) is effective but inefficient, since the computationally expensive SEPT needs to be executed on top of a DAPT-ed PLM of each domain. As a solution, we propose AdaSent, which decouples SEPT from DAPT by training a SEPT adapter on the base PLM. The adapter can be inser",
    "path": "papers/23/11/2311.00408.json",
    "total_tokens": 988,
    "translated_title": "AdaSent：用于少样本分类的高效领域自适应句子嵌入",
    "translated_abstract": "最近的研究发现，基于预训练句子编码器（SEs）的少样本句子分类是高效、鲁棒和有效的。在本研究中，我们探讨了在少样本句子分类中使用SEs进行领域专业化的策略。我们首先证明了对基本预训练语言模型（PLM）进行无监督领域自适应预训练（DAPT）可以显著提高少样本句子分类的准确性，最多提高8.4个百分点。然而，对SEs应用DAPT一方面会破坏它们（通用领域）的句子嵌入预训练（SEPT）的效果。另一方面，在DAPT之后对经过领域自适应的基础PLM进行通用领域SEPT的应用是有效的，但是效率较低，因为计算成本高昂的SEPT需要在每个领域的DAPT PLM之上执行。作为解决方案，我们提出了AdaSent，通过在基础PLM上训练一个SEPT适配器来将SEPT与DAPT分离。该适配器可以被插入进各个领域的基础PLM上。",
    "tldr": "AdaSent是一种将句子嵌入预训练和领域自适应分离的方法，通过训练一个适配器来提高少样本句子分类的准确性和效率。"
}