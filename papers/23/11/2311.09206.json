{
    "title": "TableLlama: Towards Open Large Generalist Models for Tables",
    "abstract": "arXiv:2311.09206v2 Announce Type: replace  Abstract: Semi-structured tables are ubiquitous. There has been a variety of tasks that aim to automatically interpret, augment, and query tables. Current methods often require pretraining on tables or special model architecture design, are restricted to specific table types, or have simplifying assumptions about tables and tasks. This paper makes the first step towards developing open-source large language models (LLMs) as generalists for a diversity of table-based tasks. Towards that end, we construct TableInstruct, a new dataset with a variety of realistic tables and tasks, for instruction tuning and evaluating LLMs. We further develop the first open-source generalist model for tables, TableLlama, by fine-tuning Llama 2 (7B) with LongLoRA to address the long context challenge. We experiment under both in-domain setting and out-of-domain setting. On 7 out of 8 in-domain tasks, TableLlama achieves comparable or better performance than the SOT",
    "link": "https://arxiv.org/abs/2311.09206",
    "context": "Title: TableLlama: Towards Open Large Generalist Models for Tables\nAbstract: arXiv:2311.09206v2 Announce Type: replace  Abstract: Semi-structured tables are ubiquitous. There has been a variety of tasks that aim to automatically interpret, augment, and query tables. Current methods often require pretraining on tables or special model architecture design, are restricted to specific table types, or have simplifying assumptions about tables and tasks. This paper makes the first step towards developing open-source large language models (LLMs) as generalists for a diversity of table-based tasks. Towards that end, we construct TableInstruct, a new dataset with a variety of realistic tables and tasks, for instruction tuning and evaluating LLMs. We further develop the first open-source generalist model for tables, TableLlama, by fine-tuning Llama 2 (7B) with LongLoRA to address the long context challenge. We experiment under both in-domain setting and out-of-domain setting. On 7 out of 8 in-domain tasks, TableLlama achieves comparable or better performance than the SOT",
    "path": "papers/23/11/2311.09206.json",
    "total_tokens": 854,
    "translated_title": "TableLlama：面向表格的开放大型通用模型",
    "translated_abstract": "半结构化表格是无处不在的。目前的方法通常需要对表格进行预训练或特殊的模型架构设计，受限于特定的表格类型，或对表格和任务有简化的假设。本文旨在开发开源的大型语言模型（LLMs），作为各种基于表格任务的通用工具的第一步。为此，我们构建了一个包含各种真实表格和任务的新数据集TableInstruct，以用于指令调整和评估LLMs。我们进一步利用LongLoRA对Llama 2 (7B)进行微调，开发了第一个面向表格的开源通用模型TableLlama，以应对长上下文挑战。我们在同领域和跨领域环境下进行了实验。在8个同领域任务中的7个任务中，TableLlama在性能上实现了与SOT相当或更好的表现。",
    "tldr": "本文旨在开发用于各种基于表格任务的开源大型语言模型，通过构建新数据集TableInstruct和开发第一个面向表格的开源通用模型TableLlama，在表现方面取得了可比或更好的成果。",
    "en_tdlr": "This paper aims to develop open large language models for various table-based tasks, achieving comparable or better performance by constructing a new dataset TableInstruct and developing the first open-source generalist model for tables, TableLlama."
}