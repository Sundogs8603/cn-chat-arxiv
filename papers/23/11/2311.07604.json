{
    "title": "Finetuning Text-to-Image Diffusion Models for Fairness",
    "abstract": "arXiv:2311.07604v2 Announce Type: replace-cross  Abstract: The rapid adoption of text-to-image diffusion models in society underscores an urgent need to address their biases. Without interventions, these biases could propagate a skewed worldview and restrict opportunities for minority groups. In this work, we frame fairness as a distributional alignment problem. Our solution consists of two main technical contributions: (1) a distributional alignment loss that steers specific characteristics of the generated images towards a user-defined target distribution, and (2) adjusted direct finetuning of diffusion model's sampling process (adjusted DFT), which leverages an adjusted gradient to directly optimize losses defined on the generated images. Empirically, our method markedly reduces gender, racial, and their intersectional biases for occupational prompts. Gender bias is significantly reduced even when finetuning just five soft tokens. Crucially, our method supports diverse perspectives ",
    "link": "https://arxiv.org/abs/2311.07604",
    "context": "Title: Finetuning Text-to-Image Diffusion Models for Fairness\nAbstract: arXiv:2311.07604v2 Announce Type: replace-cross  Abstract: The rapid adoption of text-to-image diffusion models in society underscores an urgent need to address their biases. Without interventions, these biases could propagate a skewed worldview and restrict opportunities for minority groups. In this work, we frame fairness as a distributional alignment problem. Our solution consists of two main technical contributions: (1) a distributional alignment loss that steers specific characteristics of the generated images towards a user-defined target distribution, and (2) adjusted direct finetuning of diffusion model's sampling process (adjusted DFT), which leverages an adjusted gradient to directly optimize losses defined on the generated images. Empirically, our method markedly reduces gender, racial, and their intersectional biases for occupational prompts. Gender bias is significantly reduced even when finetuning just five soft tokens. Crucially, our method supports diverse perspectives ",
    "path": "papers/23/11/2311.07604.json",
    "total_tokens": 866,
    "translated_title": "为公平性调整文本到图像扩散模型",
    "translated_abstract": "社会对文本到图像扩散模型的快速采用凸显了解决其偏见的迫切需求。如果不进行干预，这些偏见可能传播出扭曲的世界观，并限制少数群体的机会。在这项工作中，我们将公平性视为一个分布对齐问题。我们的解决方案包括两个主要技术贡献：(1)一个分布对齐损失，将生成的图像的特定特征引向用户定义的目标分布，以及(2)调整了扩散模型采样过程的直接微调（调整DFT），它利用调整后的梯度直接优化在生成的图像上定义的损失。实证上，我们的方法显著减少了职业提示的性别、种族及其交叉偏见。即使只对五个软标记进行微调，性别偏见也大大减少。至关重要的是，我们的方法支持多元视角。",
    "tldr": "将公平性视为分布对齐问题，通过分布对齐损失和调整DFT两项技术贡献，显著减少文本到图像扩散模型中的性别、种族和其交叉偏见。",
    "en_tdlr": "Fairness is framed as a distributional alignment problem in the context of text-to-image diffusion models, with significant reduction in gender, racial, and intersectional biases achieved through a distributional alignment loss and adjusted DFT technique."
}