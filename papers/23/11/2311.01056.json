{
    "title": "Collaboration and Transition: Distilling Item Transitions into Multi-Query Self-Attention for Sequential Recommendation. (arXiv:2311.01056v1 [cs.IR])",
    "abstract": "Modern recommender systems employ various sequential modules such as self-attention to learn dynamic user interests. However, these methods are less effective in capturing collaborative and transitional signals within user interaction sequences. First, the self-attention architecture uses the embedding of a single item as the attention query, which is inherently challenging to capture collaborative signals. Second, these methods typically follow an auto-regressive framework, which is unable to learn global item transition patterns. To overcome these limitations, we propose a new method called Multi-Query Self-Attention with Transition-Aware Embedding Distillation (MQSA-TED). First, we propose an $L$-query self-attention module that employs flexible window sizes for attention queries to capture collaborative signals. In addition, we introduce a multi-query self-attention method that balances the bias-variance trade-off in modeling user preferences by combining long and short-query self-",
    "link": "http://arxiv.org/abs/2311.01056",
    "context": "Title: Collaboration and Transition: Distilling Item Transitions into Multi-Query Self-Attention for Sequential Recommendation. (arXiv:2311.01056v1 [cs.IR])\nAbstract: Modern recommender systems employ various sequential modules such as self-attention to learn dynamic user interests. However, these methods are less effective in capturing collaborative and transitional signals within user interaction sequences. First, the self-attention architecture uses the embedding of a single item as the attention query, which is inherently challenging to capture collaborative signals. Second, these methods typically follow an auto-regressive framework, which is unable to learn global item transition patterns. To overcome these limitations, we propose a new method called Multi-Query Self-Attention with Transition-Aware Embedding Distillation (MQSA-TED). First, we propose an $L$-query self-attention module that employs flexible window sizes for attention queries to capture collaborative signals. In addition, we introduce a multi-query self-attention method that balances the bias-variance trade-off in modeling user preferences by combining long and short-query self-",
    "path": "papers/23/11/2311.01056.json",
    "total_tokens": 779,
    "translated_title": "合作与转换：将物品转换转化为多查询自注意力进行序列推荐",
    "translated_abstract": "现代推荐系统使用各种顺序模块，如自注意力来学习动态用户兴趣。然而，这些方法在捕捉用户交互序列中的合作和过渡信号方面效果较差。为了克服这些限制，我们提出了一种新方法，称为多查询自注意力与过渡感知嵌入蒸馏（MQSA-TED）。首先，我们提出了一个$L$-查询自注意力模块，使用灵活的窗口大小作为注意力查询来捕捉合作信号。此外，我们还引入了一种多查询自注意力方法，通过结合长查询和短查询来平衡建模用户偏好的偏差-方差权衡。",
    "tldr": "这篇论文提出了一种新的推荐系统方法，利用多查询自注意力和过渡感知嵌入蒸馏来捕捉用户交互序列中的合作和过渡信号。"
}