{
    "title": "VQA-GEN: A Visual Question Answering Benchmark for Domain Generalization. (arXiv:2311.00807v1 [cs.CV])",
    "abstract": "Visual question answering (VQA) models are designed to demonstrate visual-textual reasoning capabilities. However, their real-world applicability is hindered by a lack of comprehensive benchmark datasets. Existing domain generalization datasets for VQA exhibit a unilateral focus on textual shifts while VQA being a multi-modal task contains shifts across both visual and textual domains. We propose VQA-GEN, the first ever multi-modal benchmark dataset for distribution shift generated through a shift induced pipeline. Experiments demonstrate VQA-GEN dataset exposes the vulnerability of existing methods to joint multi-modal distribution shifts. validating that comprehensive multi-modal shifts are critical for robust VQA generalization. Models trained on VQA-GEN exhibit improved cross-domain and in-domain performance, confirming the value of VQA-GEN. Further, we analyze the importance of each shift technique of our pipeline contributing to the generalization of the model.",
    "link": "http://arxiv.org/abs/2311.00807",
    "context": "Title: VQA-GEN: A Visual Question Answering Benchmark for Domain Generalization. (arXiv:2311.00807v1 [cs.CV])\nAbstract: Visual question answering (VQA) models are designed to demonstrate visual-textual reasoning capabilities. However, their real-world applicability is hindered by a lack of comprehensive benchmark datasets. Existing domain generalization datasets for VQA exhibit a unilateral focus on textual shifts while VQA being a multi-modal task contains shifts across both visual and textual domains. We propose VQA-GEN, the first ever multi-modal benchmark dataset for distribution shift generated through a shift induced pipeline. Experiments demonstrate VQA-GEN dataset exposes the vulnerability of existing methods to joint multi-modal distribution shifts. validating that comprehensive multi-modal shifts are critical for robust VQA generalization. Models trained on VQA-GEN exhibit improved cross-domain and in-domain performance, confirming the value of VQA-GEN. Further, we analyze the importance of each shift technique of our pipeline contributing to the generalization of the model.",
    "path": "papers/23/11/2311.00807.json",
    "total_tokens": 880,
    "translated_title": "VQA-GEN:一个用于领域通用化的视觉问答基准测试",
    "translated_abstract": "视觉问答（VQA）模型旨在展示视觉-文本推理能力。然而，由于缺乏综合性的基准测试数据集，它们的实际应用受到了限制。现有的VQA通用性数据集单向关注文本变化，而VQA作为一个多模态任务，包含了视觉和文本领域的变化。我们提出了VQA-GEN，这是第一个通过引入变化的流程生成的多模态基准测试数据集。实验证明，VQA-GEN数据集揭示了现有方法对于联合多模态分布变化的脆弱性，验证了全面的多模态变化对于VQA的稳健性通用化至关重要。在VQA-GEN上训练的模型显示出跨领域和领域内性能的提高，证实了VQA-GEN的价值。此外，我们分析了我们的流程中每个变化技术对于模型通用化的重要性。",
    "tldr": "VQA-GEN是首个用于领域通用化的多模态基准测试数据集，揭示了现有VQA方法对于联合多模态分布变化的脆弱性，验证了全面的多模态变化对于VQA的稳健性通用化至关重要。"
}