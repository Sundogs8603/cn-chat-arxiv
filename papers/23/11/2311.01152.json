{
    "title": "Predicting Question-Answering Performance of Large Language Models through Semantic Consistency. (arXiv:2311.01152v1 [cs.CL])",
    "abstract": "Semantic consistency of a language model is broadly defined as the model's ability to produce semantically-equivalent outputs, given semantically-equivalent inputs. We address the task of assessing question-answering (QA) semantic consistency of contemporary large language models (LLMs) by manually creating a benchmark dataset with high-quality paraphrases for factual questions, and release the dataset to the community.  We further combine the semantic consistency metric with additional measurements suggested in prior work as correlating with LLM QA accuracy, for building and evaluating a framework for factual QA reference-less performance prediction -- predicting the likelihood of a language model to accurately answer a question. Evaluating the framework on five contemporary LLMs, we demonstrate encouraging, significantly outperforming baselines, results.",
    "link": "http://arxiv.org/abs/2311.01152",
    "context": "Title: Predicting Question-Answering Performance of Large Language Models through Semantic Consistency. (arXiv:2311.01152v1 [cs.CL])\nAbstract: Semantic consistency of a language model is broadly defined as the model's ability to produce semantically-equivalent outputs, given semantically-equivalent inputs. We address the task of assessing question-answering (QA) semantic consistency of contemporary large language models (LLMs) by manually creating a benchmark dataset with high-quality paraphrases for factual questions, and release the dataset to the community.  We further combine the semantic consistency metric with additional measurements suggested in prior work as correlating with LLM QA accuracy, for building and evaluating a framework for factual QA reference-less performance prediction -- predicting the likelihood of a language model to accurately answer a question. Evaluating the framework on five contemporary LLMs, we demonstrate encouraging, significantly outperforming baselines, results.",
    "path": "papers/23/11/2311.01152.json",
    "total_tokens": 854,
    "translated_title": "通过语义一致性预测大型语言模型的问答性能",
    "translated_abstract": "语言模型的语义一致性广义上定义为模型在给定语义相等的输入时产生语义相等的输出的能力。我们通过手动创建一个具有高质量改写的事实性问题基准数据集，并将该数据集发布给社区，解决了评估当代大型语言模型（LLMs）问答语义一致性的任务。我们还将语义一致性度量与先前工作中建议与LLM问答准确性相关的其他度量结合起来，构建和评估了一个用于事实性问答无参考性能预测的框架，即预测语言模型正确回答问题的可能性。在五个当代LLM上评估框架，我们展示了令人鼓舞的、明显优于基准的结果。",
    "tldr": "本论文提出了一种框架，通过结合语义一致性度量和其他相关度量，预测大型语言模型(LLMs)对问题的准确回答能力。通过手动创建高质量的事实性问题基准数据集，并应用该框架在五个当代LLM上进行评估，实验结果表明框架的效果优于基准算法。",
    "en_tdlr": "This paper proposes a framework that predicts the accurate answering ability of large language models (LLMs) to questions by combining semantic consistency metric with other relevant metrics. By manually creating a high-quality benchmark dataset for factual questions and evaluating the framework on five contemporary LLMs, the experimental results demonstrate that the framework outperforms the baselines."
}