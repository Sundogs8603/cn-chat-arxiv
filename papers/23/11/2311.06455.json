{
    "title": "Aria-NeRF: Multimodal Egocentric View Synthesis",
    "abstract": "arXiv:2311.06455v2 Announce Type: replace-cross  Abstract: We seek to accelerate research in developing rich, multimodal scene models trained from egocentric data, based on differentiable volumetric ray-tracing inspired by Neural Radiance Fields (NeRFs). The construction of a NeRF-like model from an egocentric image sequence plays a pivotal role in understanding human behavior and holds diverse applications within the realms of VR/AR. Such egocentric NeRF-like models may be used as realistic simulations, contributing significantly to the advancement of intelligent agents capable of executing tasks in the real-world. The future of egocentric view synthesis may lead to novel environment representations going beyond today's NeRFs by augmenting visual data with multimodal sensors such as IMU for egomotion tracking, audio sensors to capture surface texture and human language context, and eye-gaze trackers to infer human attention patterns in the scene. To support and facilitate the developm",
    "link": "https://arxiv.org/abs/2311.06455",
    "context": "Title: Aria-NeRF: Multimodal Egocentric View Synthesis\nAbstract: arXiv:2311.06455v2 Announce Type: replace-cross  Abstract: We seek to accelerate research in developing rich, multimodal scene models trained from egocentric data, based on differentiable volumetric ray-tracing inspired by Neural Radiance Fields (NeRFs). The construction of a NeRF-like model from an egocentric image sequence plays a pivotal role in understanding human behavior and holds diverse applications within the realms of VR/AR. Such egocentric NeRF-like models may be used as realistic simulations, contributing significantly to the advancement of intelligent agents capable of executing tasks in the real-world. The future of egocentric view synthesis may lead to novel environment representations going beyond today's NeRFs by augmenting visual data with multimodal sensors such as IMU for egomotion tracking, audio sensors to capture surface texture and human language context, and eye-gaze trackers to infer human attention patterns in the scene. To support and facilitate the developm",
    "path": "papers/23/11/2311.06455.json",
    "total_tokens": 871,
    "translated_title": "Aria-NeRF: 多模态主观视角合成",
    "translated_abstract": "我们旨在加速研究，开发从主观数据训练的丰富多模态场景模型，基于受神经辐射场（NeRFs）启发的可微体积光线追踪。从主观图像序列构建类似于NeRF的模型在理解人类行为方面起着至关重要的作用，并在虚拟现实/增强现实领域具有多样的应用。这种类似于NeRF的主观模型可以用作逼真的模拟，对能够在现实世界中执行任务的智能代理的进步作出重大贡献。主观视角合成的未来可能导致超越今天NeRFs的新颖环境表示，通过将视觉数据与多模态传感器（如用于自我运动跟踪的IMU，用于捕捉表面纹理和人类语境的音频传感器，以及用于推断场景中人类注意模式的眼部凝视追踪器）相结合。",
    "tldr": "通过类似于NeRF的模型和多模态传感器的组合，我们实现了从主观数据训练的丰富多模态场景模型，有望提升智能代理的能力。",
    "en_tdlr": "By combining NeRF-like models with multimodal sensors, we have developed rich multimodal scene models trained from egocentric data, potentially enhancing the capabilities of intelligent agents."
}