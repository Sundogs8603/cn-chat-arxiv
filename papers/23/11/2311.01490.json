{
    "title": "The Behavior of Large Language Models When Prompted to Generate Code Explanations. (arXiv:2311.01490v1 [cs.SE])",
    "abstract": "This paper systematically explores how Large Language Models (LLMs) generate explanations of code examples of the type used in intro-to-programming courses. As we show, the nature of code explanations generated by LLMs varies considerably based on the wording of the prompt, the target code examples being explained, the programming language, the temperature parameter, and the version of the LLM. Nevertheless, they are consistent in two major respects for Java and Python: the readability level, which hovers around 7-8 grade, and lexical density, i.e., the relative size of the meaningful words with respect to the total explanation size. Furthermore, the explanations score very high in correctness but less on three other metrics: completeness, conciseness, and contextualization.",
    "link": "http://arxiv.org/abs/2311.01490",
    "context": "Title: The Behavior of Large Language Models When Prompted to Generate Code Explanations. (arXiv:2311.01490v1 [cs.SE])\nAbstract: This paper systematically explores how Large Language Models (LLMs) generate explanations of code examples of the type used in intro-to-programming courses. As we show, the nature of code explanations generated by LLMs varies considerably based on the wording of the prompt, the target code examples being explained, the programming language, the temperature parameter, and the version of the LLM. Nevertheless, they are consistent in two major respects for Java and Python: the readability level, which hovers around 7-8 grade, and lexical density, i.e., the relative size of the meaningful words with respect to the total explanation size. Furthermore, the explanations score very high in correctness but less on three other metrics: completeness, conciseness, and contextualization.",
    "path": "papers/23/11/2311.01490.json",
    "total_tokens": 781,
    "translated_title": "当被要求生成代码解释时，大型语言模型的行为研究",
    "translated_abstract": "本论文系统地探究了大型语言模型（LLMs）在生成介绍编程课程中使用的代码示例解释时的行为。正如我们所展示的，LLMs生成的代码解释的性质在很大程度上取决于提示的措辞、被解释的目标代码示例、编程语言、温度参数和LLM的版本。然而，对于Java和Python而言，它们在两个主要方面保持一致：可读性水平大约在7-8年级，以及词汇密度，即与总解释大小相对的有意义的单词的相对大小。此外，这些解释在正确性方面得分很高，但在完整性、简洁性和上下文性方面得分较低。",
    "tldr": "本论文研究了大型语言模型在生成代码解释时的行为。Java和Python的解释在可读性和词汇密度方面表现一致，但在完整性、简洁性和上下文性方面得分较低。",
    "en_tdlr": "This paper explores the behavior of large language models in generating code explanations. The explanations for Java and Python are consistent in terms of readability and lexical density, but score lower in terms of completeness, conciseness, and contextualization."
}