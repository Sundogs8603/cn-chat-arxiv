{
    "title": "Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs. (arXiv:2311.00681v1 [cs.CL])",
    "abstract": "In recent years, Large Language Models (LLMs) have gained immense attention due to their notable emergent capabilities, surpassing those seen in earlier language models. A particularly intriguing application of LLMs is their role as evaluators for texts produced by various generative models.  In this study, we delve into the potential of LLMs as reliable assessors of factual consistency in summaries generated by text-generation models. Initially, we introduce an innovative approach for factuality assessment using LLMs. This entails employing a singular LLM for the entirety of the question-answering-based factuality scoring process. Following this, we examine the efficacy of various LLMs in direct factuality scoring, benchmarking them against traditional measures and human annotations.  Contrary to initial expectations, our results indicate a lack of significant correlations between factuality metrics and human evaluations, specifically for GPT-4 and PaLM-2. Notable correlations were on",
    "link": "http://arxiv.org/abs/2311.00681",
    "context": "Title: Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs. (arXiv:2311.00681v1 [cs.CL])\nAbstract: In recent years, Large Language Models (LLMs) have gained immense attention due to their notable emergent capabilities, surpassing those seen in earlier language models. A particularly intriguing application of LLMs is their role as evaluators for texts produced by various generative models.  In this study, we delve into the potential of LLMs as reliable assessors of factual consistency in summaries generated by text-generation models. Initially, we introduce an innovative approach for factuality assessment using LLMs. This entails employing a singular LLM for the entirety of the question-answering-based factuality scoring process. Following this, we examine the efficacy of various LLMs in direct factuality scoring, benchmarking them against traditional measures and human annotations.  Contrary to initial expectations, our results indicate a lack of significant correlations between factuality metrics and human evaluations, specifically for GPT-4 and PaLM-2. Notable correlations were on",
    "path": "papers/23/11/2311.00681.json",
    "total_tokens": 938,
    "translated_title": "大型语言模型是可靠的评判者吗？关于LLMs的事实性评估能力的研究。",
    "translated_abstract": "近年来，由于其显著的新兴能力，大型语言模型（LLMs）引起了极大的关注，超越了早期语言模型的能力。LLMs的一个特别有趣的应用是作为生成模型生成的文本的评估者。在这项研究中，我们深入探讨了LLMs作为可靠的事实一致性评估者的潜力。我们首先介绍了一种使用LLMs进行事实评估的创新方法。这包括在整个基于问答的事实评分过程中使用一个单一的LLM。接下来，我们检验了各种LLMs在直接事实评分方面的效果，并将它们与传统测量方法和人工注释进行了基准测试。与最初的预期相反，我们的结果表明事实评估指标与人工评估之间缺乏显著的相关性，特别是在GPT-4和PaLM-2方面。值得注意的相关性是",
    "tldr": "大型语言模型作为评估生成模型所产生摘要的事实一致性的可靠评估者的潜力被研究。研究发现在GPT-4和PaLM-2方面事实评估指标与人工评估之间缺乏显著的相关性。",
    "en_tdlr": "The potential of large language models (LLMs) as reliable evaluators of factual consistency in generated summaries is investigated. The study finds a lack of significant correlation between factuality metrics and human evaluations, particularly for GPT-4 and PaLM-2."
}