{
    "title": "CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance on coding related tasks, particularly on assisting humans in programming and facilitating programming automation. However, existing benchmarks for evaluating the code understanding and generation capacities of LLMs suffer from severe limitations. First, most benchmarks are deficient as they focus on a narrow range of popular programming languages and specific tasks, whereas the real-world software development scenarios show dire need to implement systems with multilingual programming environments to satisfy diverse requirements. Practical programming practices also strongly expect multi-task settings for testing coding capabilities of LLMs comprehensively and robustly. Second, most benchmarks also fail to consider the actual executability and the consistency of execution results of the generated code. To bridge these gaps between existing benchmarks and expectations from practical applications, we introduce CodeScope",
    "link": "https://arxiv.org/abs/2311.08588",
    "context": "Title: CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation\nAbstract: Large Language Models (LLMs) have demonstrated remarkable performance on coding related tasks, particularly on assisting humans in programming and facilitating programming automation. However, existing benchmarks for evaluating the code understanding and generation capacities of LLMs suffer from severe limitations. First, most benchmarks are deficient as they focus on a narrow range of popular programming languages and specific tasks, whereas the real-world software development scenarios show dire need to implement systems with multilingual programming environments to satisfy diverse requirements. Practical programming practices also strongly expect multi-task settings for testing coding capabilities of LLMs comprehensively and robustly. Second, most benchmarks also fail to consider the actual executability and the consistency of execution results of the generated code. To bridge these gaps between existing benchmarks and expectations from practical applications, we introduce CodeScope",
    "path": "papers/23/11/2311.08588.json",
    "total_tokens": 819,
    "translated_title": "CodeScope:一个基于执行的多语言多任务多维基准用于评估LLMs在代码理解和生成方面的能力",
    "translated_abstract": "大型语言模型（LLMs）在编码相关任务上表现出色，特别是在帮助人类编程和促进编程自动化方面。然而，现有的用于评估LLMs的代码理解和生成能力的基准存在严重的限制。首先，大部分基准存在缺陷，因为它们只关注于狭窄范围内的流行编程语言和特定任务，而实际软件开发场景需要实现多语言编程环境以满足各种需求。实际编程实践还强烈期望多任务设置，以全面和稳健地测试LLMs的编码能力。其次，大部分基准也未考虑生成代码的可执行性和执行结果的一致性。为了弥补现有基准与实际应用期望之间的差距，我们引入了CodeScope。",
    "tldr": "CodeScope是一个用于评估LLMs在代码理解和生成方面能力的多语言多任务多维基准，解决了现有基准在多语言编程环境和多任务设置方面的不足。",
    "en_tdlr": "CodeScope is a multi-language, multi-task, and multi-dimensional benchmark for evaluating the code understanding and generation capabilities of LLMs, addressing the limitations of existing benchmarks in terms of multilingual programming environments and multi-task settings."
}