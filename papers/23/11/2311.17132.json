{
    "title": "TransNeXt: Robust Foveal Visual Perception for Vision Transformers",
    "abstract": "arXiv:2311.17132v2 Announce Type: replace-cross  Abstract: Due to the depth degradation effect in residual connections, many efficient Vision Transformers models that rely on stacking layers for information exchange often fail to form sufficient information mixing, leading to unnatural visual perception. To address this issue, in this paper, we propose Aggregated Attention, a biomimetic design-based token mixer that simulates biological foveal vision and continuous eye movement while enabling each token on the feature map to have a global perception. Furthermore, we incorporate learnable tokens that interact with conventional queries and keys, which further diversifies the generation of affinity matrices beyond merely relying on the similarity between queries and keys. Our approach does not rely on stacking for information exchange, thus effectively avoiding depth degradation and achieving natural visual perception. Additionally, we propose Convolutional GLU, a channel mixer that bridg",
    "link": "https://arxiv.org/abs/2311.17132",
    "context": "Title: TransNeXt: Robust Foveal Visual Perception for Vision Transformers\nAbstract: arXiv:2311.17132v2 Announce Type: replace-cross  Abstract: Due to the depth degradation effect in residual connections, many efficient Vision Transformers models that rely on stacking layers for information exchange often fail to form sufficient information mixing, leading to unnatural visual perception. To address this issue, in this paper, we propose Aggregated Attention, a biomimetic design-based token mixer that simulates biological foveal vision and continuous eye movement while enabling each token on the feature map to have a global perception. Furthermore, we incorporate learnable tokens that interact with conventional queries and keys, which further diversifies the generation of affinity matrices beyond merely relying on the similarity between queries and keys. Our approach does not rely on stacking for information exchange, thus effectively avoiding depth degradation and achieving natural visual perception. Additionally, we propose Convolutional GLU, a channel mixer that bridg",
    "path": "papers/23/11/2311.17132.json",
    "total_tokens": 907,
    "translated_title": "TransNeXt：Vision Transformers的鲁棒伏脉视知觉",
    "translated_abstract": "由于残差连接中的深度退化效应，许多依赖于叠加层进行信息交换的高效Vision Transformers模型通常无法形成足够的信息混合，导致视知觉不自然。为了解决这个问题，在本文中，我们提出了聚合注意力，这是一种基于仿生设计的令牌混合器，模拟生物视觉和连续眼动，同时使特征图上的每个令牌具有全局感知。此外，我们将可学习的令牌纳入常规查询和密钥中，进一步使亲和矩阵的生成多样化，不仅仅依赖于查询和密钥之间的相似性。我们的方法不依赖于叠加进行信息交换，因此有效避免了深度退化，并实现了自然的视知觉。此外，我们提出了Convolutional GLU，这是一种通道混合器，搭起了视知觉中的断层。",
    "tldr": "TransNeXt提出了Aggregated Attention，一种仿生设计的令牌混合器，通过模拟生物视觉和连续眼动，使得每个特征图上的令牌具有全局感知，且不依赖于叠加层进行信息交换，有效避免了深度退化，实现了自然的视知觉。",
    "en_tdlr": "TransNeXt introduces Aggregated Attention, a biomimetic token mixer that enables each token on the feature map to have global perception by simulating biological vision and continuous eye movement, without relying on stacking for information exchange, effectively avoiding depth degradation and achieving natural visual perception."
}