{
    "title": "A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily",
    "abstract": "arXiv:2311.08268v2 Announce Type: replace  Abstract: Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to provide useful and safe responses. However, adversarial prompts known as 'jailbreaks' can circumvent safeguards, leading LLMs to generate potentially harmful content. Exploring jailbreak prompts can help to better reveal the weaknesses of LLMs and further steer us to secure them. Unfortunately, existing jailbreak methods either suffer from intricate manual design or require optimization on other white-box models, compromising generalization or efficiency. In this paper, we generalize jailbreak prompt attacks into two aspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we propose ReNeLLM, an automatic framework that leverages LLMs themselves to generate effective jailbreak prompts. Extensive experiments demonstrate that ReNeLLM significantly improves the attack success rate while greatly reducing the time cost compared to existing baselines. Ou",
    "link": "https://arxiv.org/abs/2311.08268",
    "context": "Title: A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily\nAbstract: arXiv:2311.08268v2 Announce Type: replace  Abstract: Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to provide useful and safe responses. However, adversarial prompts known as 'jailbreaks' can circumvent safeguards, leading LLMs to generate potentially harmful content. Exploring jailbreak prompts can help to better reveal the weaknesses of LLMs and further steer us to secure them. Unfortunately, existing jailbreak methods either suffer from intricate manual design or require optimization on other white-box models, compromising generalization or efficiency. In this paper, we generalize jailbreak prompt attacks into two aspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we propose ReNeLLM, an automatic framework that leverages LLMs themselves to generate effective jailbreak prompts. Extensive experiments demonstrate that ReNeLLM significantly improves the attack success rate while greatly reducing the time cost compared to existing baselines. Ou",
    "path": "papers/23/11/2311.08268.json",
    "total_tokens": 903,
    "translated_title": "伪装成羊的狼：普遍的嵌套越狱提示可以轻松愚弄大型语言模型",
    "translated_abstract": "大型语言模型（LLMs），如ChatGPT和GPT-4，旨在提供有用和安全的响应。然而，被称为“越狱”的对抗性提示可以规避保障措施，导致LLMs生成潜在有害内容。探索越狱提示可以帮助更好地揭示LLMs的弱点，并进一步引导我们安全地保护它们。不幸的是，现有的越狱方法要么遭受复杂的手工设计，要么需要在其他白盒模型上进行优化，从而损害了泛化性或效率。在本文中，我们将越狱提示攻击概括为两个方面：（1）提示重写和（2）场景嵌套。基于此，我们提出了ReNeLLM，一个利用LLMs自身生成有效越狱提示的自动框架。大量实验证明，与现有基线相比，ReNeLLM显著提高了攻击成功率，同时大大减少了时间成本。",
    "tldr": "提出一种利用大型语言模型自动生成有效的越狱提示的自动框架ReNeLLM，显著提高攻击成功率，同时大大减少时间成本。",
    "en_tdlr": "Introduce an automatic framework ReNeLLM that leverages large language models to generate effective jailbreak prompts, significantly improving attack success rate while greatly reducing time cost."
}