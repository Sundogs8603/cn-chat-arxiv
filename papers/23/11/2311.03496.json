{
    "title": "Asynchronous Local Computations in Distributed Bayesian Learning. (arXiv:2311.03496v2 [cs.LG] UPDATED)",
    "abstract": "Due to the expanding scope of machine learning (ML) to the fields of sensor networking, cooperative robotics and many other multi-agent systems, distributed deployment of inference algorithms has received a lot of attention. These algorithms involve collaboratively learning unknown parameters from dispersed data collected by multiple agents. There are two competing aspects in such algorithms, namely, intra-agent computation and inter-agent communication. Traditionally, algorithms are designed to perform both synchronously. However, certain circumstances need frugal use of communication channels as they are either unreliable, time-consuming, or resource-expensive. In this paper, we propose gossip-based asynchronous communication to leverage fast computations and reduce communication overhead simultaneously. We analyze the effects of multiple (local) intra-agent computations by the active agents between successive inter-agent communications. For local computations, Bayesian sampling via ",
    "link": "http://arxiv.org/abs/2311.03496",
    "context": "Title: Asynchronous Local Computations in Distributed Bayesian Learning. (arXiv:2311.03496v2 [cs.LG] UPDATED)\nAbstract: Due to the expanding scope of machine learning (ML) to the fields of sensor networking, cooperative robotics and many other multi-agent systems, distributed deployment of inference algorithms has received a lot of attention. These algorithms involve collaboratively learning unknown parameters from dispersed data collected by multiple agents. There are two competing aspects in such algorithms, namely, intra-agent computation and inter-agent communication. Traditionally, algorithms are designed to perform both synchronously. However, certain circumstances need frugal use of communication channels as they are either unreliable, time-consuming, or resource-expensive. In this paper, we propose gossip-based asynchronous communication to leverage fast computations and reduce communication overhead simultaneously. We analyze the effects of multiple (local) intra-agent computations by the active agents between successive inter-agent communications. For local computations, Bayesian sampling via ",
    "path": "papers/23/11/2311.03496.json",
    "total_tokens": 844,
    "translated_title": "分布式贝叶斯学习中的异步本地计算",
    "translated_abstract": "随着机器学习在传感器网络，协作机器人和其他多智能体系统领域的应用范围不断扩大，分布式推理算法的部署引起了很多关注。这些算法涉及到通过多个智能体收集的分散数据进行协作学习未知参数。在这样的算法中存在两个竞争的方面，即智能体内部计算和智能体之间的通信。传统上，算法被设计成同时执行这两个方面。然而，在某些情况下，由于通信信道不可靠、耗时或资源昂贵，需要节约使用通信通道。在本文中，我们提出了基于八卦的异步通信来同时利用快速计算和减少通信开销。我们分析了连续智能体之间的本地智能体计算对交互智能体通信的影响。对于本地计算部分，我们采用贝叶斯采样实现。",
    "tldr": "这篇论文提出了分布式贝叶斯学习中的异步本地计算方法，通过使用八卦异步通信来减少通信开销，同时利用快速计算实现协作学习未知参数。"
}