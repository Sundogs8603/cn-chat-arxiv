{
    "title": "Machine Translation for Ge'ez Language",
    "abstract": "arXiv:2311.14530v2 Announce Type: replace  Abstract: Machine translation (MT) for low-resource languages such as Ge'ez, an ancient language that is no longer the native language of any community, faces challenges such as out-of-vocabulary words, domain mismatches, and lack of sufficient labeled training data. In this work, we explore various methods to improve Ge'ez MT, including transfer-learning from related languages, optimizing shared vocabulary and token segmentation approaches, finetuning large pre-trained models, and using large language models (LLMs) for few-shot translation with fuzzy matches. We develop a multilingual neural machine translation (MNMT) model based on languages relatedness, which brings an average performance improvement of about 4 BLEU compared to standard bilingual models. We also attempt to finetune the NLLB-200 model, one of the most advanced translation models available today, but find that it performs poorly with only 4k training samples for Ge'ez. Furthe",
    "link": "https://arxiv.org/abs/2311.14530",
    "context": "Title: Machine Translation for Ge'ez Language\nAbstract: arXiv:2311.14530v2 Announce Type: replace  Abstract: Machine translation (MT) for low-resource languages such as Ge'ez, an ancient language that is no longer the native language of any community, faces challenges such as out-of-vocabulary words, domain mismatches, and lack of sufficient labeled training data. In this work, we explore various methods to improve Ge'ez MT, including transfer-learning from related languages, optimizing shared vocabulary and token segmentation approaches, finetuning large pre-trained models, and using large language models (LLMs) for few-shot translation with fuzzy matches. We develop a multilingual neural machine translation (MNMT) model based on languages relatedness, which brings an average performance improvement of about 4 BLEU compared to standard bilingual models. We also attempt to finetune the NLLB-200 model, one of the most advanced translation models available today, but find that it performs poorly with only 4k training samples for Ge'ez. Furthe",
    "path": "papers/23/11/2311.14530.json",
    "total_tokens": 1018,
    "translated_title": "用于盖兹语的机器翻译",
    "translated_abstract": "arXiv:2311.14530v2 公告类型: 替换 摘要: 机器翻译(MT)用于低资源语言，如盖兹语，一种古老的语言，不再是任何社区的母语，面临诸如词汇外生、领域不匹配和缺乏足够标记的训练数据等挑战。在这项工作中，我们探讨了改善盖兹语MT的各种方法，包括从相关语言进行迁移学习、优化共享词汇和标记分割方法、微调大型预训练模型，以及使用大型语言模型(LLMs)进行少样本翻译配合模糊匹配。我们基于语言相关性开发了一种多语言神经机器翻译(MNMT)模型，使标准双语模型的平均性能提高约4 BLEU。我们还尝试微调NLLB-200模型，这是当今可用的最先进的翻译模型之一，但发现只有4k训练样本的盖兹语表现不佳。",
    "tldr": "该研究探讨了改善盖兹语机器翻译的方法，包括从相关语言进行迁移学习、优化共享词汇和标记分割方法、大型预训练模型的微调，以及在少样本情况下使用大型语言模型进行翻译，其中一种基于语言相关性的多语言神经机器翻译模型相比标准双语模型有4 BLEU的平均性能提升。",
    "en_tdlr": "This study explores methods to improve machine translation for Ge'ez, including transfer-learning from related languages, optimizing shared vocabulary and token segmentation approaches, finetuning large pre-trained models, and utilizing large language models for few-shot translation, with a multilingual neural machine translation model based on language relatedness showing an average performance improvement of about 4 BLEU compared to standard bilingual models."
}