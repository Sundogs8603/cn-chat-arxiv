{
    "title": "Syntactic Inductive Bias in Transformer Language Models: Especially Helpful for Low-Resource Languages?. (arXiv:2311.00268v1 [cs.CL])",
    "abstract": "A line of work on Transformer-based language models such as BERT has attempted to use syntactic inductive bias to enhance the pretraining process, on the theory that building syntactic structure into the training process should reduce the amount of data needed for training. But such methods are often tested for high-resource languages such as English. In this work, we investigate whether these methods can compensate for data sparseness in low-resource languages, hypothesizing that they ought to be more effective for low-resource languages. We experiment with five low-resource languages: Uyghur, Wolof, Maltese, Coptic, and Ancient Greek. We find that these syntactic inductive bias methods produce uneven results in low-resource settings, and provide surprisingly little benefit in most cases.",
    "link": "http://arxiv.org/abs/2311.00268",
    "context": "Title: Syntactic Inductive Bias in Transformer Language Models: Especially Helpful for Low-Resource Languages?. (arXiv:2311.00268v1 [cs.CL])\nAbstract: A line of work on Transformer-based language models such as BERT has attempted to use syntactic inductive bias to enhance the pretraining process, on the theory that building syntactic structure into the training process should reduce the amount of data needed for training. But such methods are often tested for high-resource languages such as English. In this work, we investigate whether these methods can compensate for data sparseness in low-resource languages, hypothesizing that they ought to be more effective for low-resource languages. We experiment with five low-resource languages: Uyghur, Wolof, Maltese, Coptic, and Ancient Greek. We find that these syntactic inductive bias methods produce uneven results in low-resource settings, and provide surprisingly little benefit in most cases.",
    "path": "papers/23/11/2311.00268.json",
    "total_tokens": 794,
    "translated_title": "Transformer语言模型中的句法归纳偏好：对低资源语言特别有帮助吗？",
    "translated_abstract": "一系列基于Transformer的语言模型，如BERT，尝试使用句法归纳偏好来增强预训练过程，其理论是在训练过程中构建句法结构应该能减少所需的训练数据量。但是这些方法通常是针对高资源语言，如英语进行测试。在这项工作中，我们研究了这些方法是否能够弥补低资源语言中数据稀缺性的问题，假设它们对于低资源语言应该更有效。我们对五种低资源语言进行了实验：维吾尔语、沃洛夫语、马耳他语、科普特语和古希腊语。我们发现在低资源环境中，这些句法归纳偏好方法产生了不平衡的结果，并且在大多数情况下提供了令人惊讶地少的好处。",
    "tldr": "Transformer语言模型的句法归纳偏好用于增强预训练过程，但在低资源语言中效果不佳。"
}