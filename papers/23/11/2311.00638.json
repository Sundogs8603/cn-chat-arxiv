{
    "title": "FAIRLABEL: Correcting Bias in Labels. (arXiv:2311.00638v1 [cs.LG])",
    "abstract": "There are several algorithms for measuring fairness of ML models. A fundamental assumption in these approaches is that the ground truth is fair or unbiased. In real-world datasets, however, the ground truth often contains data that is a result of historical and societal biases and discrimination. Models trained on these datasets will inherit and propagate the biases to the model outputs. We propose FAIRLABEL, an algorithm which detects and corrects biases in labels. The goal of FAIRLABELis to reduce the Disparate Impact (DI) across groups while maintaining high accuracy in predictions. We propose metrics to measure the quality of bias correction and validate FAIRLABEL on synthetic datasets and show that the label correction is correct 86.7% of the time vs. 71.9% for a baseline model. We also apply FAIRLABEL on benchmark datasets such as UCI Adult, German Credit Risk, and Compas datasets and show that the Disparate Impact Ratio increases by as much as 54.2%.",
    "link": "http://arxiv.org/abs/2311.00638",
    "context": "Title: FAIRLABEL: Correcting Bias in Labels. (arXiv:2311.00638v1 [cs.LG])\nAbstract: There are several algorithms for measuring fairness of ML models. A fundamental assumption in these approaches is that the ground truth is fair or unbiased. In real-world datasets, however, the ground truth often contains data that is a result of historical and societal biases and discrimination. Models trained on these datasets will inherit and propagate the biases to the model outputs. We propose FAIRLABEL, an algorithm which detects and corrects biases in labels. The goal of FAIRLABELis to reduce the Disparate Impact (DI) across groups while maintaining high accuracy in predictions. We propose metrics to measure the quality of bias correction and validate FAIRLABEL on synthetic datasets and show that the label correction is correct 86.7% of the time vs. 71.9% for a baseline model. We also apply FAIRLABEL on benchmark datasets such as UCI Adult, German Credit Risk, and Compas datasets and show that the Disparate Impact Ratio increases by as much as 54.2%.",
    "path": "papers/23/11/2311.00638.json",
    "total_tokens": 952,
    "translated_title": "FAIRLABEL：修正标签中的偏见",
    "translated_abstract": "有多种算法可以衡量机器学习模型的公平性。这些方法的一个基本假设是，真实数据是公平或无偏的。然而，在现实世界的数据集中，真实数据往往包含历史和社会偏见和歧视的数据。在这些数据集上训练的模型将继承并传播偏见到模型输出中。我们提出了一种名为FAIRLABEL的算法，用于检测和纠正标签中的偏见。FAIRLABEL的目标是在保持高准确率的同时减少群体间的不平等影响（DI）。我们提出了度量偏见修正质量的指标，并在合成数据集上验证了FAIRLABEL的正确性，结果表明标签修正的正确率为86.7%，而基准模型为71.9%。我们还将FAIRLABEL应用于UCI Adult、German Credit Risk和Compas数据集等基准数据集，结果显示不平等影响比率最多增加了54.2%。",
    "tldr": "FAIRLABEL是一种检测和纠正标签中偏见的算法，其目标是在保持高准确率的同时减少群体间的不平等影响。通过应用于合成数据集和基准数据集，验证结果显示FAIRLABEL在标签修正方面的正确率较基准模型提高了14.8%, 在不平等影响比率方面达到了54.2%的增长。"
}