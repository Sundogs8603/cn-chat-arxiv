{
    "title": "Semi-Structured Chain-of-Thought: Integrating Multiple Sources of Knowledge for Improved Language Model Reasoning",
    "abstract": "arXiv:2311.08505v2 Announce Type: replace  Abstract: An important open question in the use of large language models for knowledge-intensive tasks is how to effectively integrate knowledge from three sources: the model's parametric memory, external structured knowledge, and external unstructured knowledge. Most existing prompting methods either rely on one or two of these sources, or require repeatedly invoking large language models to generate similar or identical content. In this work, we overcome these limitations by introducing a novel semi-structured prompting approach that seamlessly integrates the model's parametric memory with unstructured knowledge from text documents and structured knowledge from knowledge graphs. Experimental results on open-domain multi-hop question answering datasets demonstrate that our prompting method significantly surpasses existing techniques, even exceeding those that require fine-tuning.",
    "link": "https://arxiv.org/abs/2311.08505",
    "context": "Title: Semi-Structured Chain-of-Thought: Integrating Multiple Sources of Knowledge for Improved Language Model Reasoning\nAbstract: arXiv:2311.08505v2 Announce Type: replace  Abstract: An important open question in the use of large language models for knowledge-intensive tasks is how to effectively integrate knowledge from three sources: the model's parametric memory, external structured knowledge, and external unstructured knowledge. Most existing prompting methods either rely on one or two of these sources, or require repeatedly invoking large language models to generate similar or identical content. In this work, we overcome these limitations by introducing a novel semi-structured prompting approach that seamlessly integrates the model's parametric memory with unstructured knowledge from text documents and structured knowledge from knowledge graphs. Experimental results on open-domain multi-hop question answering datasets demonstrate that our prompting method significantly surpasses existing techniques, even exceeding those that require fine-tuning.",
    "path": "papers/23/11/2311.08505.json",
    "total_tokens": 750,
    "translated_title": "半结构化思维链：整合多源知识以改进语言模型推理",
    "translated_abstract": "大型语言模型在知识密集型任务中的一个重要开放问题是如何有效地整合来自三个来源的知识：模型的参数记忆、外部结构化知识和外部非结构化知识。本文引入一种新颖的半结构化提示方法，通过无缝整合模型的参数记忆、文本文档的非结构化知识和知识图中的结构化知识，克服这些限制。在开放域多跳问题回答数据集上的实验结果表明，我们的提示方法明显超越了现有技术，甚至超过了需要微调的技术。",
    "tldr": "该论文提出了一种半结构化提示方法，通过整合语言模型的参数记忆、文本文档的非结构化知识和知识图的结构化知识，显著改善了开放域多跳问题回答任务的效果。",
    "en_tdlr": "The paper introduces a semi-structured prompting approach that significantly improves the performance of open-domain multi-hop question answering tasks by integrating the model's parametric memory with unstructured knowledge from text documents and structured knowledge from knowledge graphs."
}