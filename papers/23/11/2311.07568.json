{
    "title": "Feature emergence via margin maximization: case studies in algebraic tasks",
    "abstract": "arXiv:2311.07568v2 Announce Type: replace  Abstract: Understanding the internal representations learned by neural networks is a cornerstone challenge in the science of machine learning. While there have been significant recent strides in some cases towards understanding how neural networks implement specific target functions, this paper explores a complementary question -- why do networks arrive at particular computational strategies? Our inquiry focuses on the algebraic learning tasks of modular addition, sparse parities, and finite group operations. Our primary theoretical findings analytically characterize the features learned by stylized neural networks for these algebraic tasks. Notably, our main technique demonstrates how the principle of margin maximization alone can be used to fully specify the features learned by the network. Specifically, we prove that the trained networks utilize Fourier features to perform modular addition and employ features corresponding to irreducible gr",
    "link": "https://arxiv.org/abs/2311.07568",
    "context": "Title: Feature emergence via margin maximization: case studies in algebraic tasks\nAbstract: arXiv:2311.07568v2 Announce Type: replace  Abstract: Understanding the internal representations learned by neural networks is a cornerstone challenge in the science of machine learning. While there have been significant recent strides in some cases towards understanding how neural networks implement specific target functions, this paper explores a complementary question -- why do networks arrive at particular computational strategies? Our inquiry focuses on the algebraic learning tasks of modular addition, sparse parities, and finite group operations. Our primary theoretical findings analytically characterize the features learned by stylized neural networks for these algebraic tasks. Notably, our main technique demonstrates how the principle of margin maximization alone can be used to fully specify the features learned by the network. Specifically, we prove that the trained networks utilize Fourier features to perform modular addition and employ features corresponding to irreducible gr",
    "path": "papers/23/11/2311.07568.json",
    "total_tokens": 790,
    "translated_title": "特征通过边界最大化的出现：代数任务案例研究",
    "translated_abstract": "理解神经网络学习的内部表示是机器学习科学中的一个基石性挑战。虽然在某些情况下近期已经取得了重大进展，以了解神经网络如何实现特定的目标函数，但本文探讨了一个互补的问题——网络为何会采用特定的计算策略？我们的研究重点放在模块化加法、稀疏奇偶性和有限群操作的代数学习任务上。我们的主要理论发现通过分析的方法对这些代数任务的神经网络学到的特征进行了表征。值得注意的是，我们的主要技术展示了边界最大化原则如何单独用于完全指定网络学到的特征。具体来说，我们证明训练后的网络利用傅里叶特征执行模块化加法，并使用与不可约 gr",
    "tldr": "本文研究了神经网络在代数任务中学到的特征，发现边界最大化原则可以完全指定网络学到的特征。",
    "en_tdlr": "This paper investigates the features learned by neural networks in algebraic tasks, showing that the principle of margin maximization can fully specify these learned features."
}