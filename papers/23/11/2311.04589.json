{
    "title": "TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models. (arXiv:2311.04589v3 [cs.CL] UPDATED)",
    "abstract": "Despite Multi-modal Large Language Models (MM-LLMs) have made exciting strides recently, they are still struggling to efficiently model the interactions among multi-modal inputs and the generation in non-textual modalities. In this work, we propose TEAL (Tokenize and Embed ALl)}, an approach to treat the input from any modality as a token sequence and learn a joint embedding space for all modalities. Specifically, for the input from any modality, TEAL first discretizes it into a token sequence with the off-the-shelf tokenizer and embeds the token sequence into a joint embedding space with a learnable embedding matrix. MM-LLMs just need to predict the multi-modal tokens autoregressively as the textual LLMs do. Finally, the corresponding de-tokenizer is applied to generate the output in each modality based on the predicted token sequence. With the joint embedding space, TEAL enables the frozen LLMs to perform both understanding and generation tasks involving non-textual modalities, such ",
    "link": "http://arxiv.org/abs/2311.04589",
    "context": "Title: TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models. (arXiv:2311.04589v3 [cs.CL] UPDATED)\nAbstract: Despite Multi-modal Large Language Models (MM-LLMs) have made exciting strides recently, they are still struggling to efficiently model the interactions among multi-modal inputs and the generation in non-textual modalities. In this work, we propose TEAL (Tokenize and Embed ALl)}, an approach to treat the input from any modality as a token sequence and learn a joint embedding space for all modalities. Specifically, for the input from any modality, TEAL first discretizes it into a token sequence with the off-the-shelf tokenizer and embeds the token sequence into a joint embedding space with a learnable embedding matrix. MM-LLMs just need to predict the multi-modal tokens autoregressively as the textual LLMs do. Finally, the corresponding de-tokenizer is applied to generate the output in each modality based on the predicted token sequence. With the joint embedding space, TEAL enables the frozen LLMs to perform both understanding and generation tasks involving non-textual modalities, such ",
    "path": "papers/23/11/2311.04589.json",
    "total_tokens": 965,
    "translated_title": "TEAL: 对于多模态大语言模型的一种将所有模态进行分词和嵌入的方法",
    "translated_abstract": "尽管多模态大语言模型（MM-LLMs）最近取得了令人兴奋的进展，但它们仍然在有效地建模多模态输入之间的相互作用和非文本模态的生成方面面临困难。在这项工作中，我们提出了一种名为TEAL（Tokenize and Embed ALL）的方法，将任何模态的输入视为令牌序列，并学习所有模态的联合嵌入空间。具体而言，对于任何模态的输入，TEAL首先使用现成的分词器将其离散化为令牌序列，然后使用可学习的嵌入矩阵将令牌序列嵌入到联合嵌入空间中。MM-LLMs只需要像文本LLMs那样自回归地预测多模态令牌。最后，根据预测的令牌序列，应用相应的去分词器生成每个模态的输出。通过联合嵌入空间，TEAL使冻结的LLMs能够执行涉及非文本模态的理解和生成任务，如理解和生成图像或音频。",
    "tldr": "TEAL是一种用于多模态大语言模型的方法，将不同模态的输入视为令牌序列，并学习它们的联合嵌入空间。这使得模型能够有效地建模多模态输入之间的相互作用，并生成非文本模态的输出。",
    "en_tdlr": "TEAL is an approach for multi-modal large language models that treats inputs from different modalities as token sequences and learns a joint embedding space for them. This allows the model to efficiently model interactions among multi-modal inputs and generate outputs in non-textual modalities."
}