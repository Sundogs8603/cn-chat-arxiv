{
    "title": "Understanding Grokking Through A Robustness Viewpoint",
    "abstract": "Recently, an interesting phenomenon called grokking has gained much attention, where generalization occurs long after the models have initially overfitted the training data. We try to understand this seemingly strange phenomenon through the robustness of the neural network. From a robustness perspective, we show that the popular $l_2$ weight norm (metric) of the neural network is actually a sufficient condition for grokking. Based on the previous observations, we propose perturbation-based methods to speed up the generalization process. In addition, we examine the standard training process on the modulo addition dataset and find that it hardly learns other basic group operations before grokking, for example, the commutative law. Interestingly, the speed-up of generalization when using our proposed method can be explained by learning the commutative law, a necessary condition when the model groks on the test dataset. We also empirically find that $l_2$ norm correlates with grokking on t",
    "link": "https://rss.arxiv.org/abs/2311.06597",
    "context": "Title: Understanding Grokking Through A Robustness Viewpoint\nAbstract: Recently, an interesting phenomenon called grokking has gained much attention, where generalization occurs long after the models have initially overfitted the training data. We try to understand this seemingly strange phenomenon through the robustness of the neural network. From a robustness perspective, we show that the popular $l_2$ weight norm (metric) of the neural network is actually a sufficient condition for grokking. Based on the previous observations, we propose perturbation-based methods to speed up the generalization process. In addition, we examine the standard training process on the modulo addition dataset and find that it hardly learns other basic group operations before grokking, for example, the commutative law. Interestingly, the speed-up of generalization when using our proposed method can be explained by learning the commutative law, a necessary condition when the model groks on the test dataset. We also empirically find that $l_2$ norm correlates with grokking on t",
    "path": "papers/23/11/2311.06597.json",
    "total_tokens": 953,
    "translated_title": "通过鲁棒性视角理解Grokking",
    "translated_abstract": "最近，一个有趣的现象被称为Grokking引起了很多关注，它指的是在模型最初过拟合训练数据后很久才出现泛化。我们试图通过神经网络的鲁棒性来理解这个看似奇怪的现象。从鲁棒性的角度来看，我们证明了神经网络中流行的$l_2$权重范数（度量）实际上是Grokking的一个充分条件。基于之前的观察，我们提出了基于扰动的方法来加速泛化过程。此外，我们还在模数相加数据集上考察了标准训练过程，并发现在Grokking之前，它几乎没有学习其他基本的群操作，例如，交换律。有趣的是，当使用我们提出的方法时，泛化加速可以通过学习交换律来解释，这是模型在测试数据集上Grokking时的必要条件。我们还经验性地发现$l_2$范数与Grokking在...",
    "tldr": "通过神经网络的鲁棒性视角，我们发现$l_2$权重范数是Grokking的充分条件，并提出基于扰动的方法加速泛化，在模数相加数据集上发现标准训练过程在Grokking之前几乎没有学习其他基本群操作，而使用我们方法时加速泛化可以通过学习交换律来解释。"
}