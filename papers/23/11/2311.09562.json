{
    "title": "TextEE: Benchmark, Reevaluation, Reflections, and Future Challenges in Event Extraction",
    "abstract": "arXiv:2311.09562v2 Announce Type: replace  Abstract: Event extraction has gained considerable interest due to its wide-ranging applications. However, recent studies draw attention to evaluation issues, suggesting that reported scores may not accurately reflect the true performance. In this work, we identify and address evaluation challenges, including inconsistency due to varying data assumptions or preprocessing steps, the insufficiency of current evaluation frameworks that may introduce dataset or data split bias, and the low reproducibility of some previous approaches. To address these challenges, we present TextEE, a standardized, fair, and reproducible benchmark for event extraction. TextEE comprises standardized data preprocessing scripts and splits for 14 datasets spanning seven diverse domains and includes 14 recent methodologies, conducting a comprehensive benchmark reevaluation. We also evaluate five varied large language models on our TextEE benchmark and demonstrate how the",
    "link": "https://arxiv.org/abs/2311.09562",
    "context": "Title: TextEE: Benchmark, Reevaluation, Reflections, and Future Challenges in Event Extraction\nAbstract: arXiv:2311.09562v2 Announce Type: replace  Abstract: Event extraction has gained considerable interest due to its wide-ranging applications. However, recent studies draw attention to evaluation issues, suggesting that reported scores may not accurately reflect the true performance. In this work, we identify and address evaluation challenges, including inconsistency due to varying data assumptions or preprocessing steps, the insufficiency of current evaluation frameworks that may introduce dataset or data split bias, and the low reproducibility of some previous approaches. To address these challenges, we present TextEE, a standardized, fair, and reproducible benchmark for event extraction. TextEE comprises standardized data preprocessing scripts and splits for 14 datasets spanning seven diverse domains and includes 14 recent methodologies, conducting a comprehensive benchmark reevaluation. We also evaluate five varied large language models on our TextEE benchmark and demonstrate how the",
    "path": "papers/23/11/2311.09562.json",
    "total_tokens": 833,
    "translated_title": "TextEE：事件提取中的基准、重新评估、反思和未来挑战",
    "translated_abstract": "事件提取由于其广泛的应用而引起了广泛关注。然而，最近的研究引起了对评估问题的关注，表明报告的分数可能无法准确反映真实性能。在这项工作中，我们确定并解决了评估挑战，包括由于不同的数据假设或预处理步骤而引起的不一致性，目前评估框架的不足可能引入数据集或数据分割偏见，以及一些先前方法的低可重复性。为了解决这些挑战，我们提出了TextEE，这是一个用于事件提取的标准化、公平和可重复的基准。TextEE包括标准化的数据预处理脚本和用于跨七个不同领域的14个数据集的数据分割，并包括14种最近的方法，进行全面的基准重新评估。我们还对我们的TextEE基准上的五种不同的大型语言模型进行了评估，并演示了如何",
    "tldr": "本研究提出了TextEE，这是一个用于事件提取的标准化、公平和可重复的基准，解决了评估中存在的挑战。",
    "en_tdlr": "This study introduces TextEE, a standardized, fair, and reproducible benchmark for event extraction, addressing the challenges in evaluation."
}