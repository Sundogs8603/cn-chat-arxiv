{
    "title": "Forward $\\chi^2$ Divergence Based Variational Importance Sampling",
    "abstract": "Maximizing the log-likelihood is a crucial aspect of learning latent variable models, and variational inference (VI) stands as the commonly adopted method. However, VI can encounter challenges in achieving a high log-likelihood when dealing with complicated posterior distributions. In response to this limitation, we introduce a novel variational importance sampling (VIS) approach that directly estimates and maximizes the log-likelihood. VIS leverages the optimal proposal distribution, achieved by minimizing the forward $\\chi^2$ divergence, to enhance log-likelihood estimation. We apply VIS to various popular latent variable models, including mixture models, variational auto-encoders, and partially observable generalized linear models. Results demonstrate that our approach consistently outperforms state-of-the-art baselines, both in terms of log-likelihood and model parameter estimation.",
    "link": "https://rss.arxiv.org/abs/2311.02516",
    "context": "Title: Forward $\\chi^2$ Divergence Based Variational Importance Sampling\nAbstract: Maximizing the log-likelihood is a crucial aspect of learning latent variable models, and variational inference (VI) stands as the commonly adopted method. However, VI can encounter challenges in achieving a high log-likelihood when dealing with complicated posterior distributions. In response to this limitation, we introduce a novel variational importance sampling (VIS) approach that directly estimates and maximizes the log-likelihood. VIS leverages the optimal proposal distribution, achieved by minimizing the forward $\\chi^2$ divergence, to enhance log-likelihood estimation. We apply VIS to various popular latent variable models, including mixture models, variational auto-encoders, and partially observable generalized linear models. Results demonstrate that our approach consistently outperforms state-of-the-art baselines, both in terms of log-likelihood and model parameter estimation.",
    "path": "papers/23/11/2311.02516.json",
    "total_tokens": 858,
    "translated_title": "基于前向$\\chi^2$散度的变分重要抽样方法",
    "translated_abstract": "最大化对数似然是学习潜变量模型的关键方面，而变分推断（VI）是目前常用的方法。然而，当处理复杂的后验分布时，VI在实现高对数似然方面可能遇到挑战。针对这个限制，我们引入了一种新颖的变分重要抽样（VIS）方法，直接估计和最大化对数似然。VIS利用通过最小化前向$\\chi^2$散度实现的最佳提议分布来增强对数似然估计。我们将VIS应用于多种流行的潜变量模型，包括混合模型、变分自编码器和部分观测广义线性模型。结果表明，我们的方法在对数似然和模型参数估计方面始终优于最先进的基线方法。",
    "tldr": "引入了一种基于前向$\\chi^2$散度的变分重要抽样方法(VIS)，通过直接估计和最大化对数似然来增强对复杂后验分布的估计性能。实验证明，VIS方法在多种潜变量模型中均优于最先进的基线方法，表现出更高的对数似然和模型参数估计准确性。"
}