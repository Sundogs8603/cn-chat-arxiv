{
    "title": "HARE: Explainable Hate Speech Detection with Step-by-Step Reasoning. (arXiv:2311.00321v1 [cs.CL])",
    "abstract": "With the proliferation of social media, accurate detection of hate speech has become critical to ensure safety online. To combat nuanced forms of hate speech, it is important to identify and thoroughly explain hate speech to help users understand its harmful effects. Recent benchmarks have attempted to tackle this issue by training generative models on free-text annotations of implications in hateful text. However, we find significant reasoning gaps in the existing annotations schemes, which may hinder the supervision of detection models. In this paper, we introduce a hate speech detection framework, HARE, which harnesses the reasoning capabilities of large language models (LLMs) to fill these gaps in explanations of hate speech, thus enabling effective supervision of detection models. Experiments on SBIC and Implicit Hate benchmarks show that our method, using model-generated data, consistently outperforms baselines, using existing free-text human annotations. Analysis demonstrates th",
    "link": "http://arxiv.org/abs/2311.00321",
    "context": "Title: HARE: Explainable Hate Speech Detection with Step-by-Step Reasoning. (arXiv:2311.00321v1 [cs.CL])\nAbstract: With the proliferation of social media, accurate detection of hate speech has become critical to ensure safety online. To combat nuanced forms of hate speech, it is important to identify and thoroughly explain hate speech to help users understand its harmful effects. Recent benchmarks have attempted to tackle this issue by training generative models on free-text annotations of implications in hateful text. However, we find significant reasoning gaps in the existing annotations schemes, which may hinder the supervision of detection models. In this paper, we introduce a hate speech detection framework, HARE, which harnesses the reasoning capabilities of large language models (LLMs) to fill these gaps in explanations of hate speech, thus enabling effective supervision of detection models. Experiments on SBIC and Implicit Hate benchmarks show that our method, using model-generated data, consistently outperforms baselines, using existing free-text human annotations. Analysis demonstrates th",
    "path": "papers/23/11/2311.00321.json",
    "total_tokens": 873,
    "translated_title": "HARE: 支持逐步推理的可解释性仇恨言论检测",
    "translated_abstract": "随着社交媒体的普及，准确检测仇恨言论变得至关重要以确保在线安全。为了应对细微的仇恨言论形式，重要的是要识别并详细解释仇恨言论，以帮助用户理解其有害影响。最近的基准测试试图通过训练生成模型来处理仇恨文本中含义的自由文本注释，但我们发现现有注释方案存在重大推理差距，这可能会阻碍检测模型的监督。在本文中，我们引入了一种名为HARE的仇恨言论检测框架，该框架利用大型语言模型（LLM）的推理能力来填补这些关于仇恨言论解释的差距，从而实现有效的检测模型监督。在SBIC和Implicit Hate基准测试上的实验证明，我们的方法使用模型生成的数据始终优于使用现有自由文本注释的基准。分析表明，",
    "tldr": "HARE是一个支持逐步推理的可解释性仇恨言论检测框架，利用大型语言模型填补现有注释方案的推理差距，从而提高了检测模型的监督效果。"
}