{
    "title": "AbsPyramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph",
    "abstract": "arXiv:2311.09174v3 Announce Type: replace-cross  Abstract: Cognitive research indicates that abstraction ability is essential in human intelligence, which remains under-explored in language models. In this paper, we present AbsPyramid, a unified entailment graph of 221K textual descriptions of abstraction knowledge. While existing resources only touch nouns or verbs within simplified events or specific domains, AbsPyramid collects abstract knowledge for three components of diverse events to comprehensively evaluate the abstraction ability of language models in the open domain. Experimental results demonstrate that current LLMs face challenges comprehending abstraction knowledge in zero-shot and few-shot settings. By training on our rich abstraction knowledge, we find LLMs can acquire basic abstraction abilities and generalize to unseen events. In the meantime, we empirically show that our benchmark is comprehensive to enhance LLMs across two previous abstraction tasks.",
    "link": "https://arxiv.org/abs/2311.09174",
    "context": "Title: AbsPyramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph\nAbstract: arXiv:2311.09174v3 Announce Type: replace-cross  Abstract: Cognitive research indicates that abstraction ability is essential in human intelligence, which remains under-explored in language models. In this paper, we present AbsPyramid, a unified entailment graph of 221K textual descriptions of abstraction knowledge. While existing resources only touch nouns or verbs within simplified events or specific domains, AbsPyramid collects abstract knowledge for three components of diverse events to comprehensively evaluate the abstraction ability of language models in the open domain. Experimental results demonstrate that current LLMs face challenges comprehending abstraction knowledge in zero-shot and few-shot settings. By training on our rich abstraction knowledge, we find LLMs can acquire basic abstraction abilities and generalize to unseen events. In the meantime, we empirically show that our benchmark is comprehensive to enhance LLMs across two previous abstraction tasks.",
    "path": "papers/23/11/2311.09174.json",
    "total_tokens": 839,
    "translated_title": "AbsPyramid：使用统一的蕴涵图评估语言模型的抽象能力",
    "translated_abstract": "认知研究表明，抽象能力对人类智能至关重要，而这在语言模型中仍然没有得到充分探讨。本文介绍了AbsPyramid，这是一个包含221K个文本描述的统一蕴涵图，用于收集广泛事件三个组成部分的抽象知识，以全面评估语言模型在开放领域中的抽象能力。实验结果表明，目前的LLMs在零短和少量数据情况下面临理解抽象知识的挑战。通过在我们丰富的抽象知识上进行训练，我们发现LLMs可以获得基本的抽象能力，并泛化到未见的事件。同时，我们实证表明我们的基准可以全面提高LLMs在两个先前的抽象任务中的性能。",
    "tldr": "该研究提出了AbsPyramid，一个统一的蕴涵图，用于评估语言模型的抽象能力，实验证明LLMs可以通过在丰富抽象知识上进行训练来获得基本的抽象能力，并泛化到未见的事件。"
}