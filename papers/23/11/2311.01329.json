{
    "title": "A Simple Solution for Offline Imitation from Observations and Examples with Possibly Incomplete Trajectories. (arXiv:2311.01329v1 [cs.LG])",
    "abstract": "Offline imitation from observations aims to solve MDPs where only task-specific expert states and task-agnostic non-expert state-action pairs are available. Offline imitation is useful in real-world scenarios where arbitrary interactions are costly and expert actions are unavailable. The state-of-the-art \"DIstribution Correction Estimation\" (DICE) methods minimize divergence of state occupancy between expert and learner policies and retrieve a policy with weighted behavior cloning; however, their results are unstable when learning from incomplete trajectories, due to a non-robust optimization in the dual domain. To address the issue, in this paper, we propose Trajectory-Aware Imitation Learning from Observations (TAILO). TAILO uses a discounted sum along the future trajectory as the weight for weighted behavior cloning. The terms for the sum are scaled by the output of a discriminator, which aims to identify expert states. Despite simplicity, TAILO works well if there exist trajectorie",
    "link": "http://arxiv.org/abs/2311.01329",
    "context": "Title: A Simple Solution for Offline Imitation from Observations and Examples with Possibly Incomplete Trajectories. (arXiv:2311.01329v1 [cs.LG])\nAbstract: Offline imitation from observations aims to solve MDPs where only task-specific expert states and task-agnostic non-expert state-action pairs are available. Offline imitation is useful in real-world scenarios where arbitrary interactions are costly and expert actions are unavailable. The state-of-the-art \"DIstribution Correction Estimation\" (DICE) methods minimize divergence of state occupancy between expert and learner policies and retrieve a policy with weighted behavior cloning; however, their results are unstable when learning from incomplete trajectories, due to a non-robust optimization in the dual domain. To address the issue, in this paper, we propose Trajectory-Aware Imitation Learning from Observations (TAILO). TAILO uses a discounted sum along the future trajectory as the weight for weighted behavior cloning. The terms for the sum are scaled by the output of a discriminator, which aims to identify expert states. Despite simplicity, TAILO works well if there exist trajectorie",
    "path": "papers/23/11/2311.01329.json",
    "total_tokens": 973,
    "translated_title": "离线观测和示例中用于模仿的简单解决方案，可能包含不完整的轨迹",
    "translated_abstract": "从观测中进行离线模仿旨在解决仅具有任务特定专家状态和任务不可知非专家状态-操作对的马尔可夫决策过程（MDP）。离线模仿在现实世界中的场景中非常有用，其中任意的交互都是昂贵的且专家的操作不可用。目前最先进的“分布矫正估计”（DICE）方法通过最小化专家和学习者策略之间的状态占领差异，并使用加权行为克隆检索到一个策略；然而，当从不完整轨迹学习时，由于双域的非鲁棒优化，它们的结果是不稳定的。为了解决这个问题，在本文中，我们提出了基于观测的轨迹感知模仿学习（TAILO）。TAILO使用未来轨迹上的折现和作为加权行为克隆的权重。这些权重是由一个旨在识别专家状态的鉴别器的输出进行缩放的。尽管简单，TAILO在存在轨迹的情况下表现良好。",
    "tldr": "本文提出了一种使用离线观测和示例的简单解决方案，通过使用基于轨迹的加权行为克隆和专家状态鉴别器来稳定地学习，以解决离线模仿中由于不完整轨迹而导致的不稳定问题。",
    "en_tdlr": "This paper proposes a simple solution for offline imitation that uses observations and examples, by employing trajectory-aware weighted behavior cloning and an expert state discriminator to stabilize learning in the presence of incomplete trajectories."
}