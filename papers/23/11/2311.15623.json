{
    "title": "Injecting linguistic knowledge into BERT for Dialogue State Tracking",
    "abstract": "Dialogue State Tracking (DST) models often employ intricate neural network architectures, necessitating substantial training data, and their inference processes lack transparency. This paper proposes a method that extracts linguistic knowledge via an unsupervised framework and subsequently utilizes this knowledge to augment BERT's performance and interpretability in DST tasks. The knowledge extraction procedure is computationally economical and does not necessitate annotations or additional training data. The injection of the extracted knowledge necessitates the addition of only simple neural modules. We employ the Convex Polytopic Model (CPM) as a feature extraction tool for DST tasks and illustrate that the acquired features correlate with the syntactic and semantic patterns in the dialogues. This correlation facilitates a comprehensive understanding of the linguistic features influencing the DST model's decision-making process. We benchmark this framework on various DST tasks and ob",
    "link": "https://arxiv.org/abs/2311.15623",
    "context": "Title: Injecting linguistic knowledge into BERT for Dialogue State Tracking\nAbstract: Dialogue State Tracking (DST) models often employ intricate neural network architectures, necessitating substantial training data, and their inference processes lack transparency. This paper proposes a method that extracts linguistic knowledge via an unsupervised framework and subsequently utilizes this knowledge to augment BERT's performance and interpretability in DST tasks. The knowledge extraction procedure is computationally economical and does not necessitate annotations or additional training data. The injection of the extracted knowledge necessitates the addition of only simple neural modules. We employ the Convex Polytopic Model (CPM) as a feature extraction tool for DST tasks and illustrate that the acquired features correlate with the syntactic and semantic patterns in the dialogues. This correlation facilitates a comprehensive understanding of the linguistic features influencing the DST model's decision-making process. We benchmark this framework on various DST tasks and ob",
    "path": "papers/23/11/2311.15623.json",
    "total_tokens": 902,
    "translated_title": "将语言知识注入到BERT中用于对话状态跟踪",
    "translated_abstract": "对话状态跟踪(DST)模型通常采用复杂的神经网络架构，需要大量的训练数据，其推理过程缺乏透明性。本文提出了一种方法，通过无监督框架提取语言知识，然后利用这些知识来增强BERT在DST任务中的性能和可解释性。知识提取过程计算经济高效，不需要注释或额外的训练数据。注入提取的知识只需要添加简单的神经模块。我们使用凸多面体模型(CPM)作为DST任务的特征提取工具，并表明所获取的特征与对话中的句法和语义模式相关。这种相关性有助于全面理解影响DST模型决策过程的语言特征。我们在不同的DST任务上对这个框架进行基准测试，并展示了其效果。",
    "tldr": "本文提出了一种方法，在对话状态跟踪任务中，通过无监督的知识提取方法将语言知识注入到BERT中，以提高性能和可解释性。这种方法无需额外的训练数据，通过简单的神经模块实现。该方法使用的特征提取工具与对话的句法和语义模式相关，有助于理解DST模型的决策过程。",
    "en_tdlr": "This paper proposes a method to inject linguistic knowledge into BERT for Dialogue State Tracking, improving performance and interpretability. The method extracts knowledge via an unsupervised framework, does not require additional training data, and uses simple neural modules. The feature extraction tool employed correlates with the syntactic and semantic patterns in the dialogues, facilitating a comprehensive understanding of the DST model's decision-making process."
}