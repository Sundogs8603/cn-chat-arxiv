{
    "title": "MultiResFormer: Transformer with Adaptive Multi-Resolution Modeling for General Time Series Forecasting",
    "abstract": "Transformer-based models have greatly pushed the boundaries of time series forecasting recently. Existing methods typically encode time series data into $\\textit{patches}$ using one or a fixed set of patch lengths. This, however, could result in a lack of ability to capture the variety of intricate temporal dependencies present in real-world multi-periodic time series. In this paper, we propose MultiResFormer, which dynamically models temporal variations by adaptively choosing optimal patch lengths. Concretely, at the beginning of each layer, time series data is encoded into several parallel branches, each using a detected periodicity, before going through the transformer encoder block. We conduct extensive evaluations on long- and short-term forecasting datasets comparing MultiResFormer with state-of-the-art baselines. MultiResFormer outperforms patch-based Transformer baselines on long-term forecasting tasks and also consistently outperforms CNN baselines by a large margin, while usi",
    "link": "https://arxiv.org/abs/2311.18780",
    "context": "Title: MultiResFormer: Transformer with Adaptive Multi-Resolution Modeling for General Time Series Forecasting\nAbstract: Transformer-based models have greatly pushed the boundaries of time series forecasting recently. Existing methods typically encode time series data into $\\textit{patches}$ using one or a fixed set of patch lengths. This, however, could result in a lack of ability to capture the variety of intricate temporal dependencies present in real-world multi-periodic time series. In this paper, we propose MultiResFormer, which dynamically models temporal variations by adaptively choosing optimal patch lengths. Concretely, at the beginning of each layer, time series data is encoded into several parallel branches, each using a detected periodicity, before going through the transformer encoder block. We conduct extensive evaluations on long- and short-term forecasting datasets comparing MultiResFormer with state-of-the-art baselines. MultiResFormer outperforms patch-based Transformer baselines on long-term forecasting tasks and also consistently outperforms CNN baselines by a large margin, while usi",
    "path": "papers/23/11/2311.18780.json",
    "total_tokens": 878,
    "translated_title": "多分辨率建模的Transformers：适用于一般时间序列预测的自适应多分辨率建模",
    "translated_abstract": "基于Transformer的模型近期在时间序列预测方面取得了重大突破。现有方法通常使用一个固定的或固定集合的补丁长度来将时间序列数据进行编码。然而，这可能导致无法捕捉到现实世界中多周期时间序列中存在的多样细致的时间依赖性。在本文中，我们提出了MultiResFormer，通过自适应地选择最优的补丁长度来动态建模时序变化。具体来说，在每个层的开始时，时间序列数据被编码为几个并行的分支，每个分支使用检测到的周期性，然后通过Transformer编码块。我们对长期和短期预测数据集进行了广泛评估，将MultiResFormer与最先进的基线进行比较。MultiResFormer在长期预测任务上优于基于补丁的Transformer基准，并且始终显著优于CNN基线，同时使用的资源更少。",
    "tldr": "本文提出了一种名为MultiResFormer的模型，通过自适应选择最优补丁长度，动态建模时序变化。相比于基于补丁的Transformer和CNN基线，MultiResFormer在长期预测任务上表现出更好的性能。",
    "en_tdlr": "This paper introduces MultiResFormer, a model that dynamically models temporal variations in time series forecasting by adaptively selecting optimal patch lengths. Compared to patch-based Transformer and CNN baselines, MultiResFormer demonstrates better performance on long-term forecasting tasks."
}