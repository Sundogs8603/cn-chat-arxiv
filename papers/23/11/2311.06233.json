{
    "title": "Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models",
    "abstract": "We propose the Data Contamination Quiz (DCQ), a simple and effective approach to detect data contamination in large language models (LLMs) and estimate the amount of it. Specifically, we frame data contamination detection as a series of multiple-choice questions and devise a quiz format wherein three perturbed versions of each dataset instance are created. These changes only include word-level perturbations. The generated perturbed versions, along with the original instance, form the options in the DCQ, with an extra option accommodating the possibility that none of the provided choices is correct. Given that the only distinguishing signal among the choices is the exact wording relative to the original instance, an LLM, when tasked with identifying the original instance from the choices, gravitates towards the original one if it has been exposed to it in its pre-training phase--a trait intrinsic to LLMs. Tested over several datasets with GPT-4/3.5, our findings--while fully lacking acc",
    "link": "https://arxiv.org/abs/2311.06233",
    "context": "Title: Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models\nAbstract: We propose the Data Contamination Quiz (DCQ), a simple and effective approach to detect data contamination in large language models (LLMs) and estimate the amount of it. Specifically, we frame data contamination detection as a series of multiple-choice questions and devise a quiz format wherein three perturbed versions of each dataset instance are created. These changes only include word-level perturbations. The generated perturbed versions, along with the original instance, form the options in the DCQ, with an extra option accommodating the possibility that none of the provided choices is correct. Given that the only distinguishing signal among the choices is the exact wording relative to the original instance, an LLM, when tasked with identifying the original instance from the choices, gravitates towards the original one if it has been exposed to it in its pre-training phase--a trait intrinsic to LLMs. Tested over several datasets with GPT-4/3.5, our findings--while fully lacking acc",
    "path": "papers/23/11/2311.06233.json",
    "total_tokens": 978,
    "translated_title": "数据污染问题: 一种检测和估计大型语言模型中污染的工具",
    "translated_abstract": "我们提出了数据污染问题（DCQ），这是一种简单而有效的方法，用于检测大型语言模型（LLM）中的数据污染并估计其数量。具体而言，我们将数据污染检测视为一系列的多项选择问题，并设计了一种测验形式，其中创建了每个数据集实例的三个扰动版本。这些变化仅包括词级扰动。生成的扰动版本与原始实例一起形成DCQ中的选项，额外的选项适应了提供的选择都不正确的可能性。鉴于在选择之间唯一的区别信号是与原始实例的确切措辞相关，如果在预训练阶段已经接触到原始实例，语言模型当被要求从选项中识别原始实例时，倾向于选择原始实例--这是语言模型固有的特性。在使用GPT-4/3.5进行多个数据集的测试中，我们的结果完全缺少准确性。",
    "tldr": "这个工具使用数据污染问题（DCQ）的方法来检测和估计大型语言模型中的数据污染。在DCQ中，我们创建了每个数据集实例的扰动版本，并让语言模型从中选择原始实例，通过词级扰动来区分选项。这种方法利用了语言模型在预训练阶段暴露于原始实例时的固有特性。",
    "en_tdlr": "This tool utilizes the Data Contamination Quiz (DCQ) method to detect and estimate data contamination in large language models. In the DCQ, perturbed versions of dataset instances are created and a language model is tasked with choosing the original instance by distinguishing it through word-level perturbations. This approach leverages the intrinsic trait of language models to gravitate towards the original instance when exposed to it during pre-training."
}