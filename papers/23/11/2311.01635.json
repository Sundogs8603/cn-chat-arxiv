{
    "title": "RTP: Rethinking Tensor Parallelism with Memory Deduplication. (arXiv:2311.01635v1 [cs.DC])",
    "abstract": "In the evolving landscape of neural network models, one prominent challenge stand out: the significant memory overheads associated with training expansive models. Addressing this challenge, this study delves deep into the Rotated Tensor Parallelism (RTP). RTP is an innovative approach that strategically focuses on memory deduplication in distributed training environments. It boasts of unique features like a customized communication primitive and the Flyweight Pattern initialization. Furthermore, RTP ensures a seamless overlap between partition computation and partition weight communication, optimizing the training process. Our empirical evaluations underscore RTP's efficiency, revealing that its memory consumption during distributed system training is remarkably close to the optimal - distributing the memory overhead of a single machine equitably among multiple machines. The experimental results demonstrate that RTP is capable of achieving comparable performance to Distributed Data Par",
    "link": "http://arxiv.org/abs/2311.01635",
    "context": "Title: RTP: Rethinking Tensor Parallelism with Memory Deduplication. (arXiv:2311.01635v1 [cs.DC])\nAbstract: In the evolving landscape of neural network models, one prominent challenge stand out: the significant memory overheads associated with training expansive models. Addressing this challenge, this study delves deep into the Rotated Tensor Parallelism (RTP). RTP is an innovative approach that strategically focuses on memory deduplication in distributed training environments. It boasts of unique features like a customized communication primitive and the Flyweight Pattern initialization. Furthermore, RTP ensures a seamless overlap between partition computation and partition weight communication, optimizing the training process. Our empirical evaluations underscore RTP's efficiency, revealing that its memory consumption during distributed system training is remarkably close to the optimal - distributing the memory overhead of a single machine equitably among multiple machines. The experimental results demonstrate that RTP is capable of achieving comparable performance to Distributed Data Par",
    "path": "papers/23/11/2311.01635.json",
    "total_tokens": 796,
    "translated_title": "RTP: 用内存去重思考张量并行性",
    "translated_abstract": "在神经网络模型的不断发展中，一个突出的挑战是与训练庞大模型相关的显著内存开销。本研究针对这一挑战深入研究了旋转张量并行性（RTP）。RTP是一种创新的方法，重点关注分布式训练环境中的内存去重。它具有定制的通信原语和Flyweight Pattern初始化等独特功能。此外，RTP确保了分区计算和分区权重通信之间的无缝重叠，优化了训练过程。我们的实证评估强调了RTP的效率，揭示了在分布式系统训练过程中，其内存消耗与最佳值非常接近 - 在多台机器之间公平地分配单台机器的内存开销。实验结果表明，RTP能够实现与分布式数据并行性相当的性能。",
    "tldr": "RTP是一种以内存去重为重点的创新方法，能够在分布式训练环境中优化内存消耗并实现与分布式数据并行性相当的性能。",
    "en_tdlr": "RTP is an innovative approach that focuses on memory deduplication, optimizing memory consumption in distributed training environments and achieving comparable performance to distributed data parallelism."
}