{
    "title": "Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking",
    "abstract": "arXiv:2311.18817v2 Announce Type: replace-cross  Abstract: Recent work by Power et al. (2022) highlighted a surprising \"grokking\" phenomenon in learning arithmetic tasks: a neural net first \"memorizes\" the training set, resulting in perfect training accuracy but near-random test accuracy, and after training for sufficiently longer, it suddenly transitions to perfect test accuracy. This paper studies the grokking phenomenon in theoretical setups and shows that it can be induced by a dichotomy of early and late phase implicit biases. Specifically, when training homogeneous neural nets with large initialization and small weight decay on both classification and regression tasks, we prove that the training process gets trapped at a solution corresponding to a kernel predictor for a long time, and then a very sharp transition to min-norm/max-margin predictors occurs, leading to a dramatic change in test accuracy.",
    "link": "https://arxiv.org/abs/2311.18817",
    "context": "Title: Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking\nAbstract: arXiv:2311.18817v2 Announce Type: replace-cross  Abstract: Recent work by Power et al. (2022) highlighted a surprising \"grokking\" phenomenon in learning arithmetic tasks: a neural net first \"memorizes\" the training set, resulting in perfect training accuracy but near-random test accuracy, and after training for sufficiently longer, it suddenly transitions to perfect test accuracy. This paper studies the grokking phenomenon in theoretical setups and shows that it can be induced by a dichotomy of early and late phase implicit biases. Specifically, when training homogeneous neural nets with large initialization and small weight decay on both classification and regression tasks, we prove that the training process gets trapped at a solution corresponding to a kernel predictor for a long time, and then a very sharp transition to min-norm/max-margin predictors occurs, leading to a dramatic change in test accuracy.",
    "path": "papers/23/11/2311.18817.json",
    "total_tokens": 889,
    "translated_title": "早期和晚期隐性偏见的二分法可以明显诱导领悟",
    "translated_abstract": "Power等人（2022）最近的研究突出了学习算术任务中出现的令人惊讶的“领悟”现象：神经网络首先“记忆”训练集，导致训练准确性完美，但测试准确性接近随机，然后在训练足够长时间后，突然过渡至完美测试准确性。本文研究了领悟现象在理论设置中的情况，并展示了它可以通过早期和晚期隐性偏见的二分法来诱导。具体来说，当使用大的初始化和小的权重衰减训练同质神经网络进行分类和回归任务时，我们证明训练过程在长时间内被困在对应于核预测器的解上，然后突然出现对于最小范数/最大间隔预测器的非常明显的过渡，导致测试准确性出现显著变化。",
    "tldr": "通过早期和晚期隐性偏见的二分法，本文证明在训练同质神经网络时会出现从核预测器到最小范数/最大间隔预测器的急剧转变，从而引发测试准确性的显著变化。",
    "en_tdlr": "This paper demonstrates that a dichotomy of early and late phase implicit biases induces a sharp transition from kernel predictors to min-norm/max-margin predictors when training homogeneous neural networks, leading to a dramatic change in test accuracy."
}