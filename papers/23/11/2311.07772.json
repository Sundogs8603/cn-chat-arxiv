{
    "title": "In-context Learning and Gradient Descent Revisited",
    "abstract": "arXiv:2311.07772v4 Announce Type: replace  Abstract: In-context learning (ICL) has shown impressive results in few-shot learning tasks, yet its underlying mechanism is still not fully understood. A recent line of work suggests that ICL performs gradient descent (GD)-based optimization implicitly. While appealing, much of the research focuses on simplified settings, where the parameters of a shallow model are optimized. In this work, we revisit evidence for ICL-GD correspondence on realistic NLP tasks and models. We find gaps in evaluation, both in terms of problematic metrics and insufficient baselines. We show that surprisingly, even untrained models achieve comparable ICL-GD similarity scores despite not exhibiting ICL. Next, we explore a major discrepancy in the flow of information throughout the model between ICL and GD, which we term Layer Causality. We propose a simple GD-based optimization procedure that respects layer causality, and show it improves similarity scores significan",
    "link": "https://arxiv.org/abs/2311.07772",
    "context": "Title: In-context Learning and Gradient Descent Revisited\nAbstract: arXiv:2311.07772v4 Announce Type: replace  Abstract: In-context learning (ICL) has shown impressive results in few-shot learning tasks, yet its underlying mechanism is still not fully understood. A recent line of work suggests that ICL performs gradient descent (GD)-based optimization implicitly. While appealing, much of the research focuses on simplified settings, where the parameters of a shallow model are optimized. In this work, we revisit evidence for ICL-GD correspondence on realistic NLP tasks and models. We find gaps in evaluation, both in terms of problematic metrics and insufficient baselines. We show that surprisingly, even untrained models achieve comparable ICL-GD similarity scores despite not exhibiting ICL. Next, we explore a major discrepancy in the flow of information throughout the model between ICL and GD, which we term Layer Causality. We propose a simple GD-based optimization procedure that respects layer causality, and show it improves similarity scores significan",
    "path": "papers/23/11/2311.07772.json",
    "total_tokens": 938,
    "translated_title": "在上下文学习和梯度下降的再审视",
    "translated_abstract": "在上下文学习（ICL）已在少样本学习任务中取得了令人印象深刻的结果，然而其基本机制仍未被充分理解。最近的一系列研究表明，ICL隐式地执行梯度下降（GD）优化。尽管具有吸引力，但很多研究集中在简化设置，其中优化浅层模型的参数。在这项工作中，我们重新审视了针对现实NLP任务和模型的ICL-GD对应的证据。我们发现在评估方面存在差距，无论是在有问题的指标还是不足的基线方面。我们发现令人惊讶的是，即使是未经训练的模型也能实现可比的ICL-GD相似性分数，尽管未表现出ICL。接下来，我们探讨了模型中信息流动在ICL和GD之间的主要差异，我们将其称为“层因果关系”。我们提出了一个尊重层因果关系的简单GD优化过程，并表明它显著改善了相似性分数。",
    "tldr": "本文重新审视了在现实NLP任务和模型中的上下文学习（ICL）与梯度下降（GD）之间的联系证据，找到了评估上的不足，提出了遵循层次因果关系的简单GD优化程序来改善相似性分数。"
}