{
    "title": "Anti-LM Decoding for Zero-shot In-context Machine Translation",
    "abstract": "arXiv:2311.08324v2 Announce Type: replace-cross  Abstract: Zero-shot In-context learning is the phenomenon where models can perform the task simply given the instructions. However, pre-trained large language models are known to be poorly calibrated for this task. One of the most effective approaches to handling this bias is to adopt a contrastive decoding objective, which accounts for the prior probability of generating the next token by conditioning on some context. This work introduces an Anti-Language Model objective with a decay factor designed to address the weaknesses of In-context Machine Translation. We conduct our experiments across 3 model types and sizes, 3 language directions, and for both greedy decoding and beam search ($B=5$). The proposed method outperforms other state-of-art decoding objectives, with up to $20$ BLEU point improvement from the default objective observed in some settings.",
    "link": "https://arxiv.org/abs/2311.08324",
    "context": "Title: Anti-LM Decoding for Zero-shot In-context Machine Translation\nAbstract: arXiv:2311.08324v2 Announce Type: replace-cross  Abstract: Zero-shot In-context learning is the phenomenon where models can perform the task simply given the instructions. However, pre-trained large language models are known to be poorly calibrated for this task. One of the most effective approaches to handling this bias is to adopt a contrastive decoding objective, which accounts for the prior probability of generating the next token by conditioning on some context. This work introduces an Anti-Language Model objective with a decay factor designed to address the weaknesses of In-context Machine Translation. We conduct our experiments across 3 model types and sizes, 3 language directions, and for both greedy decoding and beam search ($B=5$). The proposed method outperforms other state-of-art decoding objectives, with up to $20$ BLEU point improvement from the default objective observed in some settings.",
    "path": "papers/23/11/2311.08324.json",
    "total_tokens": 842,
    "translated_title": "零翻译上下文机器翻译的反-LM解码",
    "translated_abstract": "零翻译上下文学习是指模型可以根据简单的指令执行任务的现象。然而，已知预训练的大型语言模型在这项任务中校准不佳。处理这种偏见的最有效方法之一是采用对比解码目标，该目标考虑通过在某些上下文上下文的条件下生成下一个令牌的先验概率。本工作引入了一种Anti-Language Model目标，带有一个设计用于解决上下文机器翻译的弱点的衰减因子。我们在3种模型类型和大小，3种语言方向上以及贪婪解码和波束搜索（$B=5$）下进行实验。在某些设置中，所提出的方法优于其他最先进的解码目标，观察到比默认目标高达20个BLEU点的改进。",
    "tldr": "提出了一种反-LM解码目标，通过引入Anti-Language Model目标和一个设计良好的衰减因子，解决了零翻译上下文机器翻译的弱点，与其他解码目标相比，在某些设置中，实现了高达20个BLEU点的性能提升。",
    "en_tdlr": "Introducing an Anti-LM decoding objective with a decay factor, this work addresses the weaknesses of Zero-shot In-context Machine Translation, outperforming other state-of-art decoding objectives with improvements up to 20 BLEU points in some settings."
}