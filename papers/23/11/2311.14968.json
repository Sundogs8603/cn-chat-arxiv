{
    "title": "Hide Your Model: A Parameter Transmission-free Federated Recommender System",
    "abstract": "With the growing concerns regarding user data privacy, Federated Recommender System (FedRec) has garnered significant attention recently due to its privacy-preserving capabilities. Existing FedRecs generally adhere to a learning protocol in which a central server shares a global recommendation model with clients, and participants achieve collaborative learning by frequently communicating the model's public parameters. Nevertheless, this learning framework has two drawbacks that limit its practical usability: (1) It necessitates a global-sharing recommendation model; however, in real-world scenarios, information related to the recommender model, including its algorithm and parameters, constitutes the platforms' intellectual property. Hence, service providers are unlikely to release such information actively. (2) The communication costs of model parameter transmission are expensive since the model parameters are usually high-dimensional matrices. With the model size increasing, the commu",
    "link": "https://arxiv.org/abs/2311.14968",
    "context": "Title: Hide Your Model: A Parameter Transmission-free Federated Recommender System\nAbstract: With the growing concerns regarding user data privacy, Federated Recommender System (FedRec) has garnered significant attention recently due to its privacy-preserving capabilities. Existing FedRecs generally adhere to a learning protocol in which a central server shares a global recommendation model with clients, and participants achieve collaborative learning by frequently communicating the model's public parameters. Nevertheless, this learning framework has two drawbacks that limit its practical usability: (1) It necessitates a global-sharing recommendation model; however, in real-world scenarios, information related to the recommender model, including its algorithm and parameters, constitutes the platforms' intellectual property. Hence, service providers are unlikely to release such information actively. (2) The communication costs of model parameter transmission are expensive since the model parameters are usually high-dimensional matrices. With the model size increasing, the commu",
    "path": "papers/23/11/2311.14968.json",
    "total_tokens": 776,
    "translated_title": "隐藏您的模型：一种无需参数传输的联邦推荐系统",
    "translated_abstract": "随着对用户数据隐私的担忧日益增长，联邦推荐系统（FedRec）由于其保护隐私的能力而近来受到重视。现有的FedRec通常遵循一种学习协议，即中央服务器与客户端共享全局推荐模型，并通过频繁通信模型的公共参数来实现协作学习。然而，这种学习框架有两个缺点，限制了其实际可用性：（1）它需要一个全局共享的推荐模型；然而，在现实场景中，与推荐模型相关的信息，包括其算法和参数，构成了平台的知识产权。因此，服务提供商不太可能主动发布此类信息。（2）模型参数传输的通信成本昂贵，因为模型参数通常是高维矩阵。随着模型大小的增加，通信成本也随之增加。",
    "tldr": "提出了一种无需参数传输的联邦推荐系统，解决了全局共享模型和高维参数传输的问题。",
    "en_tdlr": "A parameter transmission-free federated recommender system is proposed to address the issues of global-sharing model and high-dimensional parameter transmission."
}