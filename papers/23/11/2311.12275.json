{
    "title": "Enabling On-Device Large Language Model Personalization with Self-Supervised Data Selection and Synthesis. (arXiv:2311.12275v3 [cs.CL] UPDATED)",
    "abstract": "After a large language model (LLM) is deployed on edge devices, it is desirable for these devices to learn from user-generated conversation data to generate user-specific and personalized responses in real-time. However, user-generated data usually contains sensitive and private information, and uploading such data to the cloud for annotation is not preferred if not prohibited. While it is possible to obtain annotation locally by directly asking users to provide preferred responses, such annotations have to be sparse to not affect user experience. In addition, the storage of edge devices is usually too limited to enable large-scale fine-tuning with full user-generated data. It remains an open question how to enable on-device LLM personalization, considering sparse annotation and limited on-device storage. In this paper, we propose a novel framework to select and store the most representative data online in a self-supervised way. Such data has a small memory footprint and allows infrequ",
    "link": "http://arxiv.org/abs/2311.12275",
    "context": "Title: Enabling On-Device Large Language Model Personalization with Self-Supervised Data Selection and Synthesis. (arXiv:2311.12275v3 [cs.CL] UPDATED)\nAbstract: After a large language model (LLM) is deployed on edge devices, it is desirable for these devices to learn from user-generated conversation data to generate user-specific and personalized responses in real-time. However, user-generated data usually contains sensitive and private information, and uploading such data to the cloud for annotation is not preferred if not prohibited. While it is possible to obtain annotation locally by directly asking users to provide preferred responses, such annotations have to be sparse to not affect user experience. In addition, the storage of edge devices is usually too limited to enable large-scale fine-tuning with full user-generated data. It remains an open question how to enable on-device LLM personalization, considering sparse annotation and limited on-device storage. In this paper, we propose a novel framework to select and store the most representative data online in a self-supervised way. Such data has a small memory footprint and allows infrequ",
    "path": "papers/23/11/2311.12275.json",
    "total_tokens": 910,
    "translated_title": "在设备上实现自我监督数据选择和合成的大规模语言模型个性化",
    "translated_abstract": "在将大规模语言模型（LLM）部署在边缘设备上后，希望这些设备能从用户生成的对话数据中学习，以实时生成针对用户的个性化回应。然而，用户生成的数据通常包含敏感和私密信息，而将此类数据上传到云端进行注释并不被推荐，甚至是禁止的。虽然可以通过直接询问用户提供首选回应来在本地获取注释，但这种注释必须稀疏以不影响用户体验。此外，边缘设备的存储通常太有限，无法进行全面的大规模微调。如何在考虑稀疏注释和受限的设备存储条件下实现在设备上的LLM个性化仍然是一个待解决的问题。在本文中，我们提出了一种新的框架，以自我监督的方式在线选择和存储最具代表性的数据。这种数据具有较小的内存占用，并允许很少的存储占用。",
    "tldr": "本文提出了一种在设备上实现自我监督数据选择和合成的大规模语言模型个性化的框架，通过选择和存储最具代表性的数据来解决稀疏注释和有限的设备存储限制。"
}