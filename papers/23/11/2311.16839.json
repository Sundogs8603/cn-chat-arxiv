{
    "title": "Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization",
    "abstract": "Multimodal large language models have made significant advancements in recent years, yet they still suffer from a common issue known as the \"hallucination problem\", in which the models generate textual descriptions that inaccurately depict or entirely fabricate content from associated images. This paper introduces a novel solution, Hallucination-Aware Direct Preference Optimization (HA-DPO), which reframes the hallucination problem as a preference selection task. The model is trained to favor the non-hallucinating response when presented with two responses of the same image (one accurate and one hallucinatory). Furthermore, this paper proposes an efficient pipeline for constructing positive~(non-hallucinatory) and negative~(hallucinatory) sample pairs, ensuring a high-quality, style-consistent dataset for robust preference learning. When applied to three mainstream multimodal models, HA-DPO significantly reduced hallucination issues and amplified the models' generalization capabilities",
    "link": "https://arxiv.org/abs/2311.16839",
    "context": "Title: Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization\nAbstract: Multimodal large language models have made significant advancements in recent years, yet they still suffer from a common issue known as the \"hallucination problem\", in which the models generate textual descriptions that inaccurately depict or entirely fabricate content from associated images. This paper introduces a novel solution, Hallucination-Aware Direct Preference Optimization (HA-DPO), which reframes the hallucination problem as a preference selection task. The model is trained to favor the non-hallucinating response when presented with two responses of the same image (one accurate and one hallucinatory). Furthermore, this paper proposes an efficient pipeline for constructing positive~(non-hallucinatory) and negative~(hallucinatory) sample pairs, ensuring a high-quality, style-consistent dataset for robust preference learning. When applied to three mainstream multimodal models, HA-DPO significantly reduced hallucination issues and amplified the models' generalization capabilities",
    "path": "papers/23/11/2311.16839.json",
    "total_tokens": 1000,
    "translated_title": "超越幻觉：通过意识到幻觉的直接偏好优化增强LVLMs",
    "translated_abstract": "多模式大型语言模型在近年来取得了重要进展，但它们仍然面临着一种常见问题，即“幻觉问题”，即模型生成的文本描述不准确地描绘或完全捏造相关图像的内容。本文介绍了一种新颖的解决方案，即幻觉感知的直接偏好优化（HA-DPO），它将幻觉问题重新定义为偏好选择任务。当模型面对两个相同图像的响应（一个准确的和一个幻觉的）时，模型被训练为倾向于选择非幻觉性的响应。此外，本文提出了一种高效的流程，用于构建正样本（非幻觉性）和负样本（幻觉性）对，确保了高质量、风格一致的数据集，以进行健壮的偏好学习。当应用于三种主流的多模式模型时，HA-DPO显著减少了幻觉问题，并增强了模型的泛化能力。",
    "tldr": "本文提出了一种新颖的方法，通过意识到幻觉的直接偏好优化来解决多模式大型语言模型中的幻觉问题。这种方法通过训练模型在面对相同图像的两个响应时偏好选择非幻觉性的响应，并提出了高效的构建样本对的方法，从而显著减少了幻觉问题并增强了模型的泛化能力。"
}