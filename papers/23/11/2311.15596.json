{
    "title": "EgoThink: Evaluating First-Person Perspective Thinking Capability of Vision-Language Models",
    "abstract": "arXiv:2311.15596v2 Announce Type: replace-cross  Abstract: Vision-language models (VLMs) have recently shown promising results in traditional downstream tasks. Evaluation studies have emerged to assess their abilities, with the majority focusing on the third-person perspective, and only a few addressing specific tasks from the first-person perspective. However, the capability of VLMs to \"think\" from a first-person perspective, a crucial attribute for advancing autonomous agents and robotics, remains largely unexplored. To bridge this research gap, we introduce EgoThink, a novel visual question-answering benchmark that encompasses six core capabilities with twelve detailed dimensions. The benchmark is constructed using selected clips from egocentric videos, with manually annotated question-answer pairs containing first-person information. To comprehensively assess VLMs, we evaluate eighteen popular VLMs on EgoThink. Moreover, given the open-ended format of the answers, we use GPT-4 as t",
    "link": "https://arxiv.org/abs/2311.15596",
    "context": "Title: EgoThink: Evaluating First-Person Perspective Thinking Capability of Vision-Language Models\nAbstract: arXiv:2311.15596v2 Announce Type: replace-cross  Abstract: Vision-language models (VLMs) have recently shown promising results in traditional downstream tasks. Evaluation studies have emerged to assess their abilities, with the majority focusing on the third-person perspective, and only a few addressing specific tasks from the first-person perspective. However, the capability of VLMs to \"think\" from a first-person perspective, a crucial attribute for advancing autonomous agents and robotics, remains largely unexplored. To bridge this research gap, we introduce EgoThink, a novel visual question-answering benchmark that encompasses six core capabilities with twelve detailed dimensions. The benchmark is constructed using selected clips from egocentric videos, with manually annotated question-answer pairs containing first-person information. To comprehensively assess VLMs, we evaluate eighteen popular VLMs on EgoThink. Moreover, given the open-ended format of the answers, we use GPT-4 as t",
    "path": "papers/23/11/2311.15596.json",
    "total_tokens": 780,
    "translated_title": "EgoThink: 评估视觉语言模型的第一视角思维能力",
    "translated_abstract": "最近，在传统的下游任务中，视觉语言模型（VLMs）表现出了令人鼓舞的结果。评估研究已经出现来评估它们的能力，但大多数集中在第三人称视角，只有很少涉及第一人称视角的特定任务。为了弥合这一研究空白，我们引入了EgoThink，这是一个包含六个核心能力和十二个详细维度的新颖视觉问答基准。该基准是使用选定的自我中心视频片段构建的，其中包含手动注释的包含第一人称信息的问题-回答对。为了全面评估VLMs，我们在EgoThink上评估了十八种流行的VLMs。",
    "tldr": "EgoThink是一个新颖的视觉问答基准，旨在评估视觉语言模型从第一人称视角“思考”的能力。",
    "en_tdlr": "EgoThink is a novel visual question-answering benchmark designed to evaluate the capability of vision-language models to \"think\" from a first-person perspective."
}