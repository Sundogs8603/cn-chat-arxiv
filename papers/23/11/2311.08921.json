{
    "title": "Self-Improving for Zero-Shot Named Entity Recognition with Large Language",
    "abstract": "arXiv:2311.08921v2 Announce Type: replace  Abstract: Exploring the application of powerful large language models (LLMs) on the named entity recognition (NER) task has drawn much attention recently. This work pushes the performance boundary of zero-shot NER with LLMs by proposing a training-free self-improving framework, which utilizes an unlabeled corpus to stimulate the self-learning ability of LLMs. First, we use the LLM to make predictions on the unlabeled corpus using self-consistency and obtain a self-annotated dataset. Second, we explore various strategies to select reliable annotations to form a reliable self-annotated dataset. Finally, for each test input, we retrieve demonstrations from the reliable self-annotated dataset and perform inference via in-context learning. Experiments on four benchmarks show substantial performance improvements achieved by our framework. Through comprehensive experimental analysis, we find that increasing the size of unlabeled corpus or iterations ",
    "link": "https://arxiv.org/abs/2311.08921",
    "context": "Title: Self-Improving for Zero-Shot Named Entity Recognition with Large Language\nAbstract: arXiv:2311.08921v2 Announce Type: replace  Abstract: Exploring the application of powerful large language models (LLMs) on the named entity recognition (NER) task has drawn much attention recently. This work pushes the performance boundary of zero-shot NER with LLMs by proposing a training-free self-improving framework, which utilizes an unlabeled corpus to stimulate the self-learning ability of LLMs. First, we use the LLM to make predictions on the unlabeled corpus using self-consistency and obtain a self-annotated dataset. Second, we explore various strategies to select reliable annotations to form a reliable self-annotated dataset. Finally, for each test input, we retrieve demonstrations from the reliable self-annotated dataset and perform inference via in-context learning. Experiments on four benchmarks show substantial performance improvements achieved by our framework. Through comprehensive experimental analysis, we find that increasing the size of unlabeled corpus or iterations ",
    "path": "papers/23/11/2311.08921.json",
    "total_tokens": 870,
    "translated_title": "利用大型语言模型进行零-shot命名实体识别的自我改进",
    "translated_abstract": "最近，探索将强大的大型语言模型(LLMs)应用于命名实体识别(NER)任务引起了很大关注。本研究通过提出一个无需训练的自我改进框架，利用未标记语料库来激发LLMs的自我学习能力，从而推动LLMs在零-shot NER上的性能边界。首先，我们使用LLMs对未标注语料库进行自一致性预测，并获得自我注释数据集。其次，我们探索各种策略来选择可靠的注释，形成一个可靠的自我注释数据集。最后，对于每个测试输入，我们从可靠的自我注释数据集中检索演示，并通过上下文学习进行推断。在四个基准测试上的实验表明，我们的框架取得了显著的性能提升。通过全面的实验分析，我们发现增加未标记语料库的规模或迭代次数",
    "tldr": "本研究提出了一个无需训练的自我改进框架，利用未标记语料库激发大型语言模型的自我学习能力，从而推动零-shot命名实体识别性能边界。",
    "en_tdlr": "This work proposes a training-free self-improving framework that utilizes an unlabeled corpus to stimulate the self-learning ability of large language models, pushing the performance boundary of zero-shot named entity recognition."
}