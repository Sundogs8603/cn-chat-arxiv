{
    "title": "Coop: Memory is not a Commodity. (arXiv:2311.00591v1 [cs.LG])",
    "abstract": "Tensor rematerialization allows the training of deep neural networks (DNNs) under limited memory budgets by checkpointing the models and recomputing the evicted tensors as needed. However, the existing tensor rematerialization techniques overlook the memory system in deep learning frameworks and implicitly assume that free memory blocks at different addresses are identical. Under this flawed assumption, discontiguous tensors are evicted, among which some are not used to allocate the new tensor. This leads to severe memory fragmentation and increases the cost of potential rematerializations. To address this issue, we propose to evict tensors within a sliding window to ensure all evictions are contiguous and are immediately used. Furthermore, we proposed cheap tensor partitioning and recomputable in-place to further reduce the rematerialization cost by optimizing the tensor allocation. We named our method Coop as it is a co-optimization of tensor allocation and tensor rematerialization. ",
    "link": "http://arxiv.org/abs/2311.00591",
    "context": "Title: Coop: Memory is not a Commodity. (arXiv:2311.00591v1 [cs.LG])\nAbstract: Tensor rematerialization allows the training of deep neural networks (DNNs) under limited memory budgets by checkpointing the models and recomputing the evicted tensors as needed. However, the existing tensor rematerialization techniques overlook the memory system in deep learning frameworks and implicitly assume that free memory blocks at different addresses are identical. Under this flawed assumption, discontiguous tensors are evicted, among which some are not used to allocate the new tensor. This leads to severe memory fragmentation and increases the cost of potential rematerializations. To address this issue, we propose to evict tensors within a sliding window to ensure all evictions are contiguous and are immediately used. Furthermore, we proposed cheap tensor partitioning and recomputable in-place to further reduce the rematerialization cost by optimizing the tensor allocation. We named our method Coop as it is a co-optimization of tensor allocation and tensor rematerialization. ",
    "path": "papers/23/11/2311.00591.json",
    "total_tokens": 878,
    "translated_title": "Coop: 内存不是商品",
    "translated_abstract": "张量再材料化技术允许在有限的内存预算下训练深度神经网络(DNNs)，通过在需要时检查点模型并根据需要重新计算被驱逐的张量。然而，现有的张量再材料化技术忽视了深度学习框架中的内存系统，并假设不同地址的空闲内存块是相同的。在这个错误的假设下，不连续的张量被驱逐，其中一些不用于分配新的张量。这导致严重的内存碎片化，增加了潜在再材料化的成本。为了解决这个问题，我们提出了在滑动窗口内驱逐张量的方法，以确保所有驱逐都是连续的并且立即使用。此外，我们提出了廉价的张量分区和可重算的就地优化张量分配来进一步降低再材料化成本。我们将我们的方法命名为Coop，因为它是张量分配和张量再材料化的协同优化。",
    "tldr": "本论文提出了一种名为Coop的方法，针对深度学习框架中的内存系统问题，通过驱逐连续张量和优化张量分配，以降低再材料化成本。",
    "en_tdlr": "This paper proposes a method called Coop, which addresses the memory system issue in deep learning frameworks by evicting contiguous tensors and optimizing tensor allocation to reduce the cost of rematerialization."
}