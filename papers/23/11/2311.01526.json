{
    "title": "ATGNN: Audio Tagging Graph Neural Network. (arXiv:2311.01526v1 [cs.SD])",
    "abstract": "Deep learning models such as CNNs and Transformers have achieved impressive performance for end-to-end audio tagging. Recent works have shown that despite stacking multiple layers, the receptive field of CNNs remains severely limited. Transformers on the other hand are able to map global context through self-attention, but treat the spectrogram as a sequence of patches which is not flexible enough to capture irregular audio objects. In this work, we treat the spectrogram in a more flexible way by considering it as graph structure and process it with a novel graph neural architecture called ATGNN. ATGNN not only combines the capability of CNNs with the global information sharing ability of Graph Neural Networks, but also maps semantic relationships between learnable class embeddings and corresponding spectrogram regions. We evaluate ATGNN on two audio tagging tasks, where it achieves 0.585 mAP on the FSD50K dataset and 0.335 mAP on the AudioSet-balanced dataset, achieving comparable res",
    "link": "http://arxiv.org/abs/2311.01526",
    "context": "Title: ATGNN: Audio Tagging Graph Neural Network. (arXiv:2311.01526v1 [cs.SD])\nAbstract: Deep learning models such as CNNs and Transformers have achieved impressive performance for end-to-end audio tagging. Recent works have shown that despite stacking multiple layers, the receptive field of CNNs remains severely limited. Transformers on the other hand are able to map global context through self-attention, but treat the spectrogram as a sequence of patches which is not flexible enough to capture irregular audio objects. In this work, we treat the spectrogram in a more flexible way by considering it as graph structure and process it with a novel graph neural architecture called ATGNN. ATGNN not only combines the capability of CNNs with the global information sharing ability of Graph Neural Networks, but also maps semantic relationships between learnable class embeddings and corresponding spectrogram regions. We evaluate ATGNN on two audio tagging tasks, where it achieves 0.585 mAP on the FSD50K dataset and 0.335 mAP on the AudioSet-balanced dataset, achieving comparable res",
    "path": "papers/23/11/2311.01526.json",
    "total_tokens": 931,
    "translated_title": "ATGNN:音频标记图神经网络",
    "translated_abstract": "深度学习模型，如CNN和Transformers，已经在端到端音频标记方面取得了令人印象深刻的表现。最近的研究表明，尽管堆叠了多个层，CNN的感受野仍然严重受限。而Transformers则能够通过自注意力机制将全局语境映射，但将频谱图视为序列补丁的方式不足以捕捉不规则音频对象。在这项工作中，我们以更加灵活的方式将频谱图视为图结构，并使用一种新颖的图神经网络架构ATGNN进行处理。ATGNN不仅结合了CNN的能力和图神经网络的全局信息共享能力，还映射了可学习类别嵌入和相应的频谱图区域之间的语义关系。我们在两个音频标记任务上评估了ATGNN，在FSD50K数据集上实现了0.585的mAP，在AudioSet-balanced数据集上实现了0.335的mAP，达到了可比较的结果。",
    "tldr": "ATGNN是一种新颖的图神经网络架构，将音频标记任务中的频谱图视为图结构，并结合了CNN的能力和图神经网络的全局信息共享能力，同时还映射了可学习类别嵌入和相应的频谱图区域之间的语义关系。",
    "en_tdlr": "ATGNN is a novel graph neural network architecture that treats spectrograms in audio tagging tasks as a graph structure, combining the capabilities of CNNs with the global information sharing ability of graph neural networks, and mapping semantic relationships between learnable class embeddings and corresponding spectrogram regions."
}