{
    "title": "Cross-Modal Information-Guided Network using Contrastive Learning for Point Cloud Registration. (arXiv:2311.01202v1 [cs.CV])",
    "abstract": "The majority of point cloud registration methods currently rely on extracting features from points. However, these methods are limited by their dependence on information obtained from a single modality of points, which can result in deficiencies such as inadequate perception of global features and a lack of texture information. Actually, humans can employ visual information learned from 2D images to comprehend the 3D world. Based on this fact, we present a novel Cross-Modal Information-Guided Network (CMIGNet), which obtains global shape perception through cross-modal information to achieve precise and robust point cloud registration. Specifically, we first incorporate the projected images from the point clouds and fuse the cross-modal features using the attention mechanism. Furthermore, we employ two contrastive learning strategies, namely overlapping contrastive learning and cross-modal contrastive learning. The former focuses on features in overlapping regions, while the latter emph",
    "link": "http://arxiv.org/abs/2311.01202",
    "context": "Title: Cross-Modal Information-Guided Network using Contrastive Learning for Point Cloud Registration. (arXiv:2311.01202v1 [cs.CV])\nAbstract: The majority of point cloud registration methods currently rely on extracting features from points. However, these methods are limited by their dependence on information obtained from a single modality of points, which can result in deficiencies such as inadequate perception of global features and a lack of texture information. Actually, humans can employ visual information learned from 2D images to comprehend the 3D world. Based on this fact, we present a novel Cross-Modal Information-Guided Network (CMIGNet), which obtains global shape perception through cross-modal information to achieve precise and robust point cloud registration. Specifically, we first incorporate the projected images from the point clouds and fuse the cross-modal features using the attention mechanism. Furthermore, we employ two contrastive learning strategies, namely overlapping contrastive learning and cross-modal contrastive learning. The former focuses on features in overlapping regions, while the latter emph",
    "path": "papers/23/11/2311.01202.json",
    "total_tokens": 847,
    "translated_title": "使用对比学习的跨模态信息引导网络进行点云配准",
    "translated_abstract": "目前大多数点云配准方法依赖于从点中提取特征。然而，这些方法受限于仅从单一模态的点获取的信息，可能导致全局特征感知不足和缺乏纹理信息等问题。实际上，人类可以利用从2D图像中学到的视觉信息来理解3D世界。基于这一事实，我们提出了一种新颖的跨模态信息引导网络（CMIGNet），通过跨模态信息获得全局形状感知，以实现精确且鲁棒的点云配准。具体而言，我们首先将点云的投影图像与注意机制融合，在交叉模态特征上进行融合。此外，我们采用两种对比学习策略，即重叠对比学习和跨模态对比学习。前者侧重于重叠区域的特征，而后者强调跨模态特征的学习。",
    "tldr": "这篇论文提出了一种使用对比学习的跨模态信息引导网络，通过利用从2D图像中学到的视觉信息来实现精确且鲁棒的点云配准。"
}