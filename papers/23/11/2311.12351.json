{
    "title": "Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey",
    "abstract": "arXiv:2311.12351v2 Announce Type: replace  Abstract: Transformer-based Large Language Models (LLMs) have been applied in diverse areas such as knowledge bases, human interfaces, and dynamic agents, and marking a stride towards achieving Artificial General Intelligence (AGI). However, current LLMs are predominantly pretrained on short text snippets, which compromises their effectiveness in processing the long-context prompts that are frequently encountered in practical scenarios. This article offers a comprehensive survey of the recent advancement in Transformer-based LLM architectures aimed at enhancing the long-context capabilities of LLMs throughout the entire model lifecycle, from pre-training through to inference. We first delineate and analyze the problems of handling long-context input and output with the current Transformer-based models. We then provide a taxonomy and the landscape of upgrades on Transformer architecture to solve these problems. Afterwards, we provide an investi",
    "link": "https://arxiv.org/abs/2311.12351",
    "context": "Title: Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey\nAbstract: arXiv:2311.12351v2 Announce Type: replace  Abstract: Transformer-based Large Language Models (LLMs) have been applied in diverse areas such as knowledge bases, human interfaces, and dynamic agents, and marking a stride towards achieving Artificial General Intelligence (AGI). However, current LLMs are predominantly pretrained on short text snippets, which compromises their effectiveness in processing the long-context prompts that are frequently encountered in practical scenarios. This article offers a comprehensive survey of the recent advancement in Transformer-based LLM architectures aimed at enhancing the long-context capabilities of LLMs throughout the entire model lifecycle, from pre-training through to inference. We first delineate and analyze the problems of handling long-context input and output with the current Transformer-based models. We then provide a taxonomy and the landscape of upgrades on Transformer architecture to solve these problems. Afterwards, we provide an investi",
    "path": "papers/23/11/2311.12351.json",
    "total_tokens": 809,
    "translated_title": "在长上下文大语言模型中推进Transformer架构：一项全面调查",
    "translated_abstract": "基于Transformer的大型语言模型（LLMs）已应用于知识库、人机界面和动态代理等多个领域，标志着迈向达到人工通用智能(AGI)的一大步。然而，当前的LLMs主要是在短文本片段上进行预训练，这危及了它们在处理在实际场景中频繁遇到的长上下文提示时的有效性。本文对最近在旨在增强LLMs长上下文能力的基于Transformer的LLM架构的进展进行了全面调查，涵盖了整个模型生命周期，从预训练到推断。首先，我们阐述并分析了当前基于Transformer模型处理长上下文输入和输出的问题。然后，我们提供了一个解决这些问题的Transformer架构升级的分类和景观。随后，我们进行了一项调查",
    "tldr": "本论文对基于Transformer的大型语言模型架构的最新进展进行了全面调查，旨在增强其处理长上下文能力，从预训练到推断过程中进行了分类和分析。"
}