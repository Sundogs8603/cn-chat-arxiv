{
    "title": "BLT: Can Large Language Models Handle Basic Legal Text?",
    "abstract": "arXiv:2311.09693v2 Announce Type: replace-cross  Abstract: We find that the best publicly available LLMs like GPT-4, Claude, and {PaLM 2} currently perform poorly at basic legal text handling. We introduce a benchmark consisting of tasks that lawyers and paralegals would expect LLMs to handle zero-shot, such as looking up the text at a line of a witness deposition or at a subsection of a contract. LLMs' poor performance on this benchmark casts into doubt their reliability as-is for legal practice. However, fine-tuning for these tasks brings even a smaller model to near-perfect performance on our test set and also raises performance on a related legal task. These results suggest that many simple behaviors needed for a domain may not be present in foundational LLMs, without additional engagement from subject matter experts.",
    "link": "https://arxiv.org/abs/2311.09693",
    "context": "Title: BLT: Can Large Language Models Handle Basic Legal Text?\nAbstract: arXiv:2311.09693v2 Announce Type: replace-cross  Abstract: We find that the best publicly available LLMs like GPT-4, Claude, and {PaLM 2} currently perform poorly at basic legal text handling. We introduce a benchmark consisting of tasks that lawyers and paralegals would expect LLMs to handle zero-shot, such as looking up the text at a line of a witness deposition or at a subsection of a contract. LLMs' poor performance on this benchmark casts into doubt their reliability as-is for legal practice. However, fine-tuning for these tasks brings even a smaller model to near-perfect performance on our test set and also raises performance on a related legal task. These results suggest that many simple behaviors needed for a domain may not be present in foundational LLMs, without additional engagement from subject matter experts.",
    "path": "papers/23/11/2311.09693.json",
    "total_tokens": 801,
    "translated_title": "BLT: 大型语言模型能处理基础法律文本吗？",
    "translated_abstract": "我们发现像GPT-4、Claude和{PaLM 2}这样的最好的公开可用的LLM在处理基础法律文本方面表现不佳。我们引入了一个基准，其中包含律师和法律助理期望LLM零-shot处理的任务，比如查找证词文件的某一行或合同的某个子部分的文本。LLM在这个基准上的差劲表现对它们在法律实践中的可靠性提出了质疑。然而，针对这些任务进行微调甚至使一个较小的模型在我们的测试集上表现接近完美，并且还提升了相关法律任务的表现。这些结果表明，许多领域所需的简单行为在基础LLM中可能不存在，除非有领域专家的额外参与。",
    "tldr": "大型语言模型在处理基础法律文本方面表现不佳，但通过针对性微调，甚至较小的模型也能在测试中表现出色，提升了相关法律任务的表现。",
    "en_tdlr": "Large language models perform poorly at handling basic legal text, but fine-tuning for specific tasks can significantly improve their performance, even for smaller models, and enhance performance on related legal tasks."
}