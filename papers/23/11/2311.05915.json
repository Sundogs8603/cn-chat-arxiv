{
    "title": "Fake Alignment: Are LLMs Really Aligned Well?",
    "abstract": "arXiv:2311.05915v3 Announce Type: replace-cross  Abstract: The growing awareness of safety concerns in large language models (LLMs) has sparked considerable interest in the evaluation of safety. This study investigates an under-explored issue about the evaluation of LLMs, namely the substantial discrepancy in performance between multiple-choice questions and open-ended questions. Inspired by research on jailbreak attack patterns, we argue this is caused by mismatched generalization. That is, LLM only remembers the answer style for open-ended safety questions, which makes it unable to solve other forms of safety tests. We refer to this phenomenon as fake alignment and construct a comparative benchmark to empirically verify its existence in LLMs. We introduce a Fake alIgNment Evaluation (FINE) framework and two novel metrics--Consistency Score (CS) and Consistent Safety Score (CSS), which jointly assess two complementary forms of evaluation to quantify fake alignment and obtain corrected",
    "link": "https://arxiv.org/abs/2311.05915",
    "context": "Title: Fake Alignment: Are LLMs Really Aligned Well?\nAbstract: arXiv:2311.05915v3 Announce Type: replace-cross  Abstract: The growing awareness of safety concerns in large language models (LLMs) has sparked considerable interest in the evaluation of safety. This study investigates an under-explored issue about the evaluation of LLMs, namely the substantial discrepancy in performance between multiple-choice questions and open-ended questions. Inspired by research on jailbreak attack patterns, we argue this is caused by mismatched generalization. That is, LLM only remembers the answer style for open-ended safety questions, which makes it unable to solve other forms of safety tests. We refer to this phenomenon as fake alignment and construct a comparative benchmark to empirically verify its existence in LLMs. We introduce a Fake alIgNment Evaluation (FINE) framework and two novel metrics--Consistency Score (CS) and Consistent Safety Score (CSS), which jointly assess two complementary forms of evaluation to quantify fake alignment and obtain corrected",
    "path": "papers/23/11/2311.05915.json",
    "total_tokens": 887,
    "translated_title": "假对齐：LLMs真的对齐得好吗？",
    "translated_abstract": "随着对大型语言模型（LLMs）安全问题的关注日益增强，人们对安全评估产生了相当大的兴趣。本研究探讨了评估LLMs的一个未被充分研究的问题，即多项选择题和开放式问题之间的表现差异。受到越狱攻击模式研究的启发，我们认为这是由于泛化不匹配所引起的。也就是说，LLMs只记住了开放式安全问题的答案风格，这使其无法解决其他形式的安全测试。我们将这种现象称为假对齐，并构建了一个比较基准来在LLMs中经验性地验证其存在。我们引入了一个Fake alIgNment Evaluation (FINE)框架和两个新颖的度量标准--一致性分数（CS）和一致的安全分数（CSS），它们共同评估两种互补形式的评估，以量化假对齐并获得更正",
    "tldr": "研究发现大型语言模型（LLMs）在安全性评估中存在假对齐问题，提出了Fake alIgNment Evaluation (FINE)框架和两个新的度量标准，用于验证和修正这一现象",
    "en_tdlr": "The study identified the issue of fake alignment in the evaluation of large language models (LLMs), introduced the Fake alIgNment Evaluation (FINE) framework and two novel metrics to verify and correct this phenomenon."
}