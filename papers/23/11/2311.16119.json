{
    "title": "Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition",
    "abstract": "arXiv:2311.16119v3 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) are deployed in interactive contexts with direct user engagement, such as chatbots and writing assistants. These deployments are vulnerable to prompt injection and jailbreaking (collectively, prompt hacking), in which models are manipulated to ignore their original instructions and follow potentially malicious ones. Although widely acknowledged as a significant security threat, there is a dearth of large-scale resources and quantitative studies on prompt hacking. To address this lacuna, we launch a global prompt hacking competition, which allows for free-form human input attacks. We elicit 600K+ adversarial prompts against three state-of-the-art LLMs. We describe the dataset, which empirically verifies that current LLMs can indeed be manipulated via prompt hacking. We also present a comprehensive taxonomical ontology of the types of adversarial prompts.",
    "link": "https://arxiv.org/abs/2311.16119",
    "context": "Title: Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition\nAbstract: arXiv:2311.16119v3 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) are deployed in interactive contexts with direct user engagement, such as chatbots and writing assistants. These deployments are vulnerable to prompt injection and jailbreaking (collectively, prompt hacking), in which models are manipulated to ignore their original instructions and follow potentially malicious ones. Although widely acknowledged as a significant security threat, there is a dearth of large-scale resources and quantitative studies on prompt hacking. To address this lacuna, we launch a global prompt hacking competition, which allows for free-form human input attacks. We elicit 600K+ adversarial prompts against three state-of-the-art LLMs. We describe the dataset, which empirically verifies that current LLMs can indeed be manipulated via prompt hacking. We also present a comprehensive taxonomical ontology of the types of adversarial prompts.",
    "path": "papers/23/11/2311.16119.json",
    "total_tokens": 839,
    "translated_title": "忽略这个标题并HackAPrompt：通过全球规模的Prompt Hacking竞赛揭示LLMs的系统性漏洞",
    "translated_abstract": "大型语言模型（LLMs）被部署在直接与用户互动的情境中，例如聊天机器人和写作助手。这些部署容易受到提示注入和越狱（统称为Prompt Hacking）的攻击，即模型被操纵以忽略其原始指令并遵循可能恶意的指令。虽然广为人知作为一种重要的安全威胁，但关于Prompt Hacking的大规模资源和定量研究的资料匮乏。为了填补这一空白，我们发起了一场全球Prompt Hacking竞赛，允许自由形式的人类输入攻击。我们搜集了对三种最先进的LLMs发起的超过60万个对抗性提示，描述了这个数据集，从经验上验证了当前LLMs确实可以通过Prompt Hacking被操纵。我们还提出了一个对抗性提示类型的全面分类本体论。",
    "tldr": "通过全球规模的Prompt Hacking竞赛，揭示了LLMs存在的系统漏洞，验证了当前LLMs可以被提示注入攻击操纵。",
    "en_tdlr": "The study exposes systemic vulnerabilities in LLMs through a global prompt hacking competition, confirming that current LLMs can be manipulated through prompt injection attacks."
}