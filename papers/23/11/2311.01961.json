{
    "title": "Assessing Fidelity in XAI post-hoc techniques: A Comparative Study with Ground Truth Explanations Datasets. (arXiv:2311.01961v1 [cs.CV])",
    "abstract": "The evaluation of the fidelity of eXplainable Artificial Intelligence (XAI) methods to their underlying models is a challenging task, primarily due to the absence of a ground truth for explanations. However, assessing fidelity is a necessary step for ensuring a correct XAI methodology. In this study, we conduct a fair and objective comparison of the current state-of-the-art XAI methods by introducing three novel image datasets with reliable ground truth for explanations. The primary objective of this comparison is to identify methods with low fidelity and eliminate them from further research, thereby promoting the development of more trustworthy and effective XAI techniques. Our results demonstrate that XAI methods based on the backpropagation of output information to input yield higher accuracy and reliability compared to methods relying on sensitivity analysis or Class Activation Maps (CAM). However, the backpropagation method tends to generate more noisy saliency maps. These finding",
    "link": "http://arxiv.org/abs/2311.01961",
    "context": "Title: Assessing Fidelity in XAI post-hoc techniques: A Comparative Study with Ground Truth Explanations Datasets. (arXiv:2311.01961v1 [cs.CV])\nAbstract: The evaluation of the fidelity of eXplainable Artificial Intelligence (XAI) methods to their underlying models is a challenging task, primarily due to the absence of a ground truth for explanations. However, assessing fidelity is a necessary step for ensuring a correct XAI methodology. In this study, we conduct a fair and objective comparison of the current state-of-the-art XAI methods by introducing three novel image datasets with reliable ground truth for explanations. The primary objective of this comparison is to identify methods with low fidelity and eliminate them from further research, thereby promoting the development of more trustworthy and effective XAI techniques. Our results demonstrate that XAI methods based on the backpropagation of output information to input yield higher accuracy and reliability compared to methods relying on sensitivity analysis or Class Activation Maps (CAM). However, the backpropagation method tends to generate more noisy saliency maps. These finding",
    "path": "papers/23/11/2311.01961.json",
    "total_tokens": 921,
    "translated_title": "对XAI后置技术的忠实度评估：与Ground Truth解释数据集的比较研究",
    "translated_abstract": "评估可解释人工智能（XAI）方法与其基础模型的忠实度是一项具有挑战性的任务，主要是因为缺乏解释的真实性。然而，评估忠实度是确保正确的XAI方法的必要步骤。在本研究中，我们通过引入三个具有可靠的解释真相的新颖图像数据集，对当前最先进的XAI方法进行了公平客观的比较。本比较的主要目标是鉴定低忠实度的方法，并将其排除在进一步研究之外，从而促进更可信和有效的XAI技术的发展。我们的结果表明，基于输出信息向输入的反向传播的XAI方法相对于依赖灵敏度分析或类激活图（CAM）的方法具有更高的准确性和可靠性。然而，反向传播方法往往会产生更多的噪声唤醒图。",
    "tldr": "本研究通过引入具有可靠的解释真相的图像数据集，对当前最先进的XAI方法进行了公平客观的比较，结果显示基于反向传播的XAI方法相对于其他方法具有更高的准确性和可靠性，但会生成更多的噪声唤醒图。",
    "en_tdlr": "This study compares the current state-of-the-art XAI methods using novel image datasets with reliable ground truth explanations, and finds that XAI methods based on backpropagation yield higher accuracy and reliability compared to other methods, but tend to generate more noisy saliency maps."
}