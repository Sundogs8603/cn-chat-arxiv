{
    "title": "TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models",
    "abstract": "arXiv:2311.16503v2 Announce Type: replace-cross  Abstract: The Diffusion model, a prevalent framework for image generation, encounters significant challenges in terms of broad applicability due to its extended inference times and substantial memory requirements. Efficient Post-training Quantization (PTQ) is pivotal for addressing these issues in traditional models. Different from traditional models, diffusion models heavily depend on the time-step $t$ to achieve satisfactory multi-round denoising. Usually, $t$ from the finite set $\\{1, \\ldots, T\\}$ is encoded to a temporal feature by a few modules totally irrespective of the sampling data. However, existing PTQ methods do not optimize these modules separately. They adopt inappropriate reconstruction targets and complex calibration methods, resulting in a severe disturbance of the temporal feature and denoising trajectory, as well as a low compression efficiency. To solve these, we propose a Temporal Feature Maintenance Quantization (TF",
    "link": "https://arxiv.org/abs/2311.16503",
    "context": "Title: TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models\nAbstract: arXiv:2311.16503v2 Announce Type: replace-cross  Abstract: The Diffusion model, a prevalent framework for image generation, encounters significant challenges in terms of broad applicability due to its extended inference times and substantial memory requirements. Efficient Post-training Quantization (PTQ) is pivotal for addressing these issues in traditional models. Different from traditional models, diffusion models heavily depend on the time-step $t$ to achieve satisfactory multi-round denoising. Usually, $t$ from the finite set $\\{1, \\ldots, T\\}$ is encoded to a temporal feature by a few modules totally irrespective of the sampling data. However, existing PTQ methods do not optimize these modules separately. They adopt inappropriate reconstruction targets and complex calibration methods, resulting in a severe disturbance of the temporal feature and denoising trajectory, as well as a low compression efficiency. To solve these, we propose a Temporal Feature Maintenance Quantization (TF",
    "path": "papers/23/11/2311.16503.json",
    "total_tokens": 914,
    "translated_title": "TFMQ-DM：面向扩散模型的时间特征维持量化",
    "translated_abstract": "arXiv:2311.16503v2 通告类型：替换-交叉 摘要：扩散模型是一种广泛应用于图像生成的框架，但由于其较长的推理时间和大量的内存需求，在广泛适用性方面遇到了重大挑战。高效的后训练量化（PTQ）对于传统模型解决这些问题至关重要。与传统模型不同，扩散模型严重依赖时间步长 $t$ 来实现令人满意的多轮去噪。通常，从有限集合 $\\{1, \\ldots, T\\}$ 中的 $t$会被几个模块编码为一个时间特征，这完全不考虑采样数据。然而，现有的PTQ方法并不分别优化这些模块。它们采用不恰当的重构目标和复杂的校准方法，导致时间特征和去噪轨迹严重受到干扰，同时压缩效率较低。为了解决这些问题，我们提出了一种称为Temporal Feature Maintenance Quantization (TFMQ)的方法",
    "tldr": "TFMQ-DM提出了一种称为Temporal Feature Maintenance Quantization (TFMQ)的方法，针对扩散模型中的时间特征进行量化，解决了传统模型中存在的优化问题，提高了压缩效率。",
    "en_tdlr": "TFMQ-DM proposes a method called Temporal Feature Maintenance Quantization (TFMQ) to quantize the temporal features in diffusion models, addressing optimization issues present in traditional models and improving compression efficiency."
}