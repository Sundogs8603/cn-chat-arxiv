{
    "title": "Calibrate and Boost Logical Expressiveness of GNN Over Multi-Relational and Temporal Graphs. (arXiv:2311.01647v1 [cs.LG])",
    "abstract": "As a powerful framework for graph representation learning, Graph Neural Networks (GNNs) have garnered significant attention in recent years. However, to the best of our knowledge, there has been no formal analysis of the logical expressiveness of GNNs as Boolean node classifiers over multi-relational graphs, where each edge carries a specific relation type. In this paper, we investigate $\\mathcal{FOC}_2$, a fragment of first-order logic with two variables and counting quantifiers. On the negative side, we demonstrate that the R$^2$-GNN architecture, which extends the local message passing GNN by incorporating global readout, fails to capture $\\mathcal{FOC}_2$ classifiers in the general case. Nevertheless, on the positive side, we establish that R$^2$-GNNs models are equivalent to $\\mathcal{FOC}_2$ classifiers under certain restricted yet reasonable scenarios. To address the limitations of R$^2$-GNNs regarding expressiveness, we propose a simple graph transformation technique, akin to a",
    "link": "http://arxiv.org/abs/2311.01647",
    "context": "Title: Calibrate and Boost Logical Expressiveness of GNN Over Multi-Relational and Temporal Graphs. (arXiv:2311.01647v1 [cs.LG])\nAbstract: As a powerful framework for graph representation learning, Graph Neural Networks (GNNs) have garnered significant attention in recent years. However, to the best of our knowledge, there has been no formal analysis of the logical expressiveness of GNNs as Boolean node classifiers over multi-relational graphs, where each edge carries a specific relation type. In this paper, we investigate $\\mathcal{FOC}_2$, a fragment of first-order logic with two variables and counting quantifiers. On the negative side, we demonstrate that the R$^2$-GNN architecture, which extends the local message passing GNN by incorporating global readout, fails to capture $\\mathcal{FOC}_2$ classifiers in the general case. Nevertheless, on the positive side, we establish that R$^2$-GNNs models are equivalent to $\\mathcal{FOC}_2$ classifiers under certain restricted yet reasonable scenarios. To address the limitations of R$^2$-GNNs regarding expressiveness, we propose a simple graph transformation technique, akin to a",
    "path": "papers/23/11/2311.01647.json",
    "total_tokens": 1009,
    "translated_title": "校准和增强GNN在多关系和时间图上的逻辑表达能力",
    "translated_abstract": "作为图表示学习的强大框架，图神经网络（GNN）近年来引起了极大的关注。然而，据我们所知，关于GNN作为多关系图中布尔节点分类器的逻辑表达能力尚未进行正式分析，其中每条边都具有特定的关系类型。在本文中，我们研究了$\\mathcal{FOC}_2$，这是一种具有两个变量和计数量化器的一阶逻辑片段。在消极方面，我们证明了将全局读取引入局部消息传递GNN的R$^2$-GNN架构在一般情况下无法捕捉$\\mathcal{FOC}_2$分类器。然而，在积极方面，我们建立了R$^2$-GNN模型在某些受限但合理的情况下等价于$\\mathcal{FOC}_2$分类器。为了解决R$^2$-GNN在表达能力方面的局限性，我们提出了一种简单的图转换技术，类似于...",
    "tldr": "本论文研究了GNN在多关系和时间图上的逻辑表达能力，并证明了R$^2$-GNN模型在某些受限但合理的情况下等价于$\\mathcal{FOC}_2$分类器。此外，为了克服R$^2$-GNN在表达能力方面的局限性，提出了一种简单的图转换技术。",
    "en_tdlr": "This paper investigates the logical expressiveness of GNN over multi-relational and temporal graphs, and demonstrates the equivalence between R$^2$-GNN models and $\\mathcal{FOC}_2$ classifiers under certain restricted scenarios. Additionally, a simple graph transformation technique is proposed to overcome the limitations of R$^2$-GNN in expressiveness."
}