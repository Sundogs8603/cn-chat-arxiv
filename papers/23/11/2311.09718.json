{
    "title": "You don't need a personality test to know these models are unreliable: Assessing the Reliability of Large Language Models on Psychometric Instruments",
    "abstract": "arXiv:2311.09718v2 Announce Type: replace-cross  Abstract: The versatility of Large Language Models (LLMs) on natural language understanding tasks has made them popular for research in social sciences. To properly understand the properties and innate personas of LLMs, researchers have performed studies that involve using prompts in the form of questions that ask LLMs about particular opinions. In this study, we take a cautionary step back and examine whether the current format of prompting LLMs elicits responses in a consistent and robust manner. We first construct a dataset that contains 693 questions encompassing 39 different instruments of persona measurement on 115 persona axes. Additionally, we design a set of prompts containing minor variations and examine LLMs' capabilities to generate answers, as well as prompt variations to examine their consistency with respect to content-level variations such as switching the order of response options or negating the statement. Our experimen",
    "link": "https://arxiv.org/abs/2311.09718",
    "context": "Title: You don't need a personality test to know these models are unreliable: Assessing the Reliability of Large Language Models on Psychometric Instruments\nAbstract: arXiv:2311.09718v2 Announce Type: replace-cross  Abstract: The versatility of Large Language Models (LLMs) on natural language understanding tasks has made them popular for research in social sciences. To properly understand the properties and innate personas of LLMs, researchers have performed studies that involve using prompts in the form of questions that ask LLMs about particular opinions. In this study, we take a cautionary step back and examine whether the current format of prompting LLMs elicits responses in a consistent and robust manner. We first construct a dataset that contains 693 questions encompassing 39 different instruments of persona measurement on 115 persona axes. Additionally, we design a set of prompts containing minor variations and examine LLMs' capabilities to generate answers, as well as prompt variations to examine their consistency with respect to content-level variations such as switching the order of response options or negating the statement. Our experimen",
    "path": "papers/23/11/2311.09718.json",
    "total_tokens": 844,
    "translated_title": "你不需要进行人格测试来知道这些模型是不可靠的：评估大型语言模型在心理测量工具上的可靠性",
    "translated_abstract": "大型语言模型（LLMs）在自然语言理解任务中的多功能性使其在社会科学研究中备受青睐。为了正确理解LLMs的属性和固有人格，研究人员进行了涉及使用提示的研究，这些提示采用问题形式询问LLMs关于特定观点。在本研究中，我们谨慎退一步，检查当前提示LLMs的格式是否以一致且稳健的方式引出响应。我们首先构建了一个包含693个问题的数据集，涵盖115个人格轴上的39种不同人格测量工具。此外，我们设计了一组包含微小变化的提示，并检查LLMs生成答案的能力，以及提示变化以检查它们在内容级别变化方面的一致性，例如更改响应选项的顺序或否定该语句。",
    "tldr": "评估大型语言模型在人格测量工具上的可靠性，研究探讨当前提示方式是否导致一致且稳健的响应",
    "en_tdlr": "This study assesses the reliability of large language models on psychometric instruments by examining the consistency and robustness of responses elicited through current prompting formats."
}