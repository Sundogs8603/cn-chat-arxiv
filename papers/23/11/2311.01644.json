{
    "title": "Should Under-parameterized Student Networks Copy or Average Teacher Weights?. (arXiv:2311.01644v1 [cs.LG])",
    "abstract": "Any continuous function $f^*$ can be approximated arbitrarily well by a neural network with sufficiently many neurons $k$. We consider the case when $f^*$ itself is a neural network with one hidden layer and $k$ neurons. Approximating $f^*$ with a neural network with $n< k$ neurons can thus be seen as fitting an under-parameterized \"student\" network with $n$ neurons to a \"teacher\" network with $k$ neurons. As the student has fewer neurons than the teacher, it is unclear, whether each of the $n$ student neurons should copy one of the teacher neurons or rather average a group of teacher neurons. For shallow neural networks with erf activation function and for the standard Gaussian input distribution, we prove that \"copy-average\" configurations are critical points if the teacher's incoming vectors are orthonormal and its outgoing weights are unitary. Moreover, the optimum among such configurations is reached when $n-1$ student neurons each copy one teacher neuron and the $n$-th student ne",
    "link": "http://arxiv.org/abs/2311.01644",
    "context": "Title: Should Under-parameterized Student Networks Copy or Average Teacher Weights?. (arXiv:2311.01644v1 [cs.LG])\nAbstract: Any continuous function $f^*$ can be approximated arbitrarily well by a neural network with sufficiently many neurons $k$. We consider the case when $f^*$ itself is a neural network with one hidden layer and $k$ neurons. Approximating $f^*$ with a neural network with $n< k$ neurons can thus be seen as fitting an under-parameterized \"student\" network with $n$ neurons to a \"teacher\" network with $k$ neurons. As the student has fewer neurons than the teacher, it is unclear, whether each of the $n$ student neurons should copy one of the teacher neurons or rather average a group of teacher neurons. For shallow neural networks with erf activation function and for the standard Gaussian input distribution, we prove that \"copy-average\" configurations are critical points if the teacher's incoming vectors are orthonormal and its outgoing weights are unitary. Moreover, the optimum among such configurations is reached when $n-1$ student neurons each copy one teacher neuron and the $n$-th student ne",
    "path": "papers/23/11/2311.01644.json",
    "total_tokens": 1081,
    "translated_title": "学生网络是否应该复制或平均教师权重？",
    "translated_abstract": "任何连续函数 $f^*$ 都可以用足够多的神经元 $k$来近似。我们考虑 $f^*$ 本身是一个具有一个隐藏层和 $k$ 个神经元的神经网络的情况。用具有 $n<k$ 个神经元的神经网络来逼近 $f^*$ 可以看作是将一个欠参数化的“学生”网络与 $k$ 个神经元的“教师”网络进行拟合。由于学生具有较少的神经元，所以不清楚每个 $n$ 个学生神经元应该复制一个教师神经元还是平均一组教师神经元。对于具有 erf 激活函数和标准高斯输入分布的浅层神经网络，我们证明了当教师的输入向量是正交的并且输出权重是酉的时候，“复制-平均”配置是临界点。此外，在这样的配置中，优化结果是当 $n-1$ 个学生神经元分别复制一个教师神经元，并且第 $n$ 个学生神经元是所有教师神经元的平均。",
    "tldr": "这项研究探讨了在欠参数化情况下，学生网络是否应该复制教师神经元或平均一组教师神经元的权重。研究发现对于特定的网络结构和输入分布，当教师网络的输入向量正交且输出权重为酉时，复制-平均配置将达到优化结果，其中大部分学生神经元复制一个教师神经元，最后一个学生神经元对所有教师神经元取平均值。",
    "en_tdlr": "This study investigates whether under-parameterized student networks should copy teacher neurons or average a group of teacher neurons' weights. The research found that for specific network structures and input distributions, when the teacher network's input vectors are orthogonal and the output weights are unitary, the copy-average configuration achieves the optimal result, where most student neurons copy one teacher neuron and the last student neuron averages all the teacher neurons."
}