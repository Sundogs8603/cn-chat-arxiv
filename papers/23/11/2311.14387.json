{
    "title": "Achieving Margin Maximization Exponentially Fast via Progressive Norm Rescaling. (arXiv:2311.14387v3 [cs.LG] UPDATED)",
    "abstract": "In this work, we investigate the margin-maximization bias exhibited by gradient-based algorithms in classifying linearly separable data. We present an in-depth analysis of the specific properties of the velocity field associated with (normalized) gradients, focusing on their role in margin maximization. Inspired by this analysis, we propose a novel algorithm called Progressive Rescaling Gradient Descent (PRGD) and show that PRGD can maximize the margin at an {\\em exponential rate}. This stands in stark contrast to all existing algorithms, which maximize the margin at a slow {\\em polynomial rate}. Specifically, we identify mild conditions on data distribution under which existing algorithms such as gradient descent (GD) and normalized gradient descent (NGD) {\\em provably fail} in maximizing the margin efficiently. To validate our theoretical findings, we present both synthetic and real-world experiments. Notably, PRGD also shows promise in enhancing the generalization performance when a",
    "link": "http://arxiv.org/abs/2311.14387",
    "context": "Title: Achieving Margin Maximization Exponentially Fast via Progressive Norm Rescaling. (arXiv:2311.14387v3 [cs.LG] UPDATED)\nAbstract: In this work, we investigate the margin-maximization bias exhibited by gradient-based algorithms in classifying linearly separable data. We present an in-depth analysis of the specific properties of the velocity field associated with (normalized) gradients, focusing on their role in margin maximization. Inspired by this analysis, we propose a novel algorithm called Progressive Rescaling Gradient Descent (PRGD) and show that PRGD can maximize the margin at an {\\em exponential rate}. This stands in stark contrast to all existing algorithms, which maximize the margin at a slow {\\em polynomial rate}. Specifically, we identify mild conditions on data distribution under which existing algorithms such as gradient descent (GD) and normalized gradient descent (NGD) {\\em provably fail} in maximizing the margin efficiently. To validate our theoretical findings, we present both synthetic and real-world experiments. Notably, PRGD also shows promise in enhancing the generalization performance when a",
    "path": "papers/23/11/2311.14387.json",
    "total_tokens": 877,
    "translated_title": "通过渐进范数重新缩放实现指数级快速边界最大化",
    "translated_abstract": "在这项工作中，我们研究了基于梯度的算法在分类线性可分数据时表现出的边界最大化偏差。我们对与（归一化的）梯度相关的速度场的特性进行了深入分析，重点关注它们在边界最大化中的作用。受到这个分析的启发，我们提出了一种名为渐进重新缩放梯度下降（PRGD）的新算法，并展示了PRGD可以以指数级快速增大边界。这与目前所有现有算法形成了鲜明对比，后者以缓慢的多项式速率最大化边界。具体而言，我们确定了数据分布的温和条件，在这些条件下，像梯度下降（GD）和归一化梯度下降（NGD）这样的现有算法在高效最大化边界时会出现失败。为了验证我们的理论发现，我们进行了合成和真实世界实验。值得注意的是，PRGD在提高泛化性能方面也表现出了潜力。",
    "tldr": "通过PRGD算法，我们在分类线性可分数据时实现了指数级快速边界最大化，与现有的算法相比，取得了显著的改进。",
    "en_tdlr": "We achieve exponential rate margin maximization in classifying linearly separable data using the PRGD algorithm, which outperforms existing algorithms."
}