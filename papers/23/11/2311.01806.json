{
    "title": "Sketching for Convex and Nonconvex Regularized Least Squares with Sharp Guarantees. (arXiv:2311.01806v1 [math.OC])",
    "abstract": "Randomized algorithms are important for solving large-scale optimization problems. In this paper, we propose a fast sketching algorithm for least square problems regularized by convex or nonconvex regularization functions, Sketching for Regularized Optimization (SRO). Our SRO algorithm first generates a sketch of the original data matrix, then solves the sketched problem. Different from existing randomized algorithms, our algorithm handles general Frechet subdifferentiable regularization functions in an unified framework. We present general theoretical result for the approximation error between the optimization results of the original problem and the sketched problem for regularized least square problems which can be convex or nonconvex. For arbitrary convex regularizer, relative-error bound is proved for the approximation error. Importantly, minimax rates for sparse signal estimation by solving the sketched sparse convex or nonconvex learning problems are also obtained using our gener",
    "link": "http://arxiv.org/abs/2311.01806",
    "context": "Title: Sketching for Convex and Nonconvex Regularized Least Squares with Sharp Guarantees. (arXiv:2311.01806v1 [math.OC])\nAbstract: Randomized algorithms are important for solving large-scale optimization problems. In this paper, we propose a fast sketching algorithm for least square problems regularized by convex or nonconvex regularization functions, Sketching for Regularized Optimization (SRO). Our SRO algorithm first generates a sketch of the original data matrix, then solves the sketched problem. Different from existing randomized algorithms, our algorithm handles general Frechet subdifferentiable regularization functions in an unified framework. We present general theoretical result for the approximation error between the optimization results of the original problem and the sketched problem for regularized least square problems which can be convex or nonconvex. For arbitrary convex regularizer, relative-error bound is proved for the approximation error. Importantly, minimax rates for sparse signal estimation by solving the sketched sparse convex or nonconvex learning problems are also obtained using our gener",
    "path": "papers/23/11/2311.01806.json",
    "total_tokens": 999,
    "translated_title": "利用属性的最小二乘问题的速写算法和锐利保证的凸和非凸正则化。 （arXiv：2311.01806v1 [math.OC]）",
    "translated_abstract": "随机算法对于解决大规模优化问题非常重要。在本文中，我们提出了一种快速速写算法，用于通过凸或非凸正则化函数正则化的最小二乘问题，即速写正则化优化（SRO）。我们的SRO算法首先生成原始数据矩阵的速写，然后解决速写问题。与现有的随机算法不同，我们的算法在一个统一的框架中处理通用的Frechet子微分正则化函数。我们为凸或非凸正则化的最小二乘问题的原始问题的优化结果和速写问题之间的近似误差提供了一般的理论结果。对于任意的凸正则化器，证明了相对误差界限的近似误差。重要的是，使用我们的一般极小极大速写稀疏凸或非凸学习问题的解决方案的稀疏信号估计的极小化速率也得到了。",
    "tldr": "本文提出了一种用于解决较大规模优化问题的快速速写算法，适用于凸或非凸正则化函数的最小二乘问题。相比已有的随机算法，该算法处理通用的Frechet子微分正则化函数并提供了一般的近似误差理论。同时，通过解决速写的稀疏凸或非凸学习问题，我们还得到了稀疏信号估计的极小极大速率。",
    "en_tdlr": "This paper proposes a fast sketching algorithm for solving large-scale optimization problems, specifically for convex or nonconvex regularized least squares. The algorithm handles general Frechet subdifferentiable regularization functions and provides a theoretical analysis of the approximation error. Additionally, it obtains minimax rates for sparse signal estimation in sketched sparse convex or nonconvex learning problems."
}