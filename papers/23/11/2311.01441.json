{
    "title": "Distilling Out-of-Distribution Robustness from Vision-Language Foundation Models. (arXiv:2311.01441v1 [cs.LG])",
    "abstract": "We propose a conceptually simple and lightweight framework for improving the robustness of vision models through the combination of knowledge distillation and data augmentation. We address the conjecture that larger models do not make for better teachers by showing strong gains in out-of-distribution robustness when distilling from pretrained foundation models. Following this finding, we propose Discrete Adversarial Distillation (DAD), which leverages a robust teacher to generate adversarial examples and a VQGAN to discretize them, creating more informative samples than standard data augmentation techniques. We provide a theoretical framework for the use of a robust teacher in the knowledge distillation with data augmentation setting and demonstrate strong gains in out-of-distribution robustness and clean accuracy across different student architectures. Notably, our method adds minor computational overhead compared to similar techniques and can be easily combined with other data augmen",
    "link": "http://arxiv.org/abs/2311.01441",
    "context": "Title: Distilling Out-of-Distribution Robustness from Vision-Language Foundation Models. (arXiv:2311.01441v1 [cs.LG])\nAbstract: We propose a conceptually simple and lightweight framework for improving the robustness of vision models through the combination of knowledge distillation and data augmentation. We address the conjecture that larger models do not make for better teachers by showing strong gains in out-of-distribution robustness when distilling from pretrained foundation models. Following this finding, we propose Discrete Adversarial Distillation (DAD), which leverages a robust teacher to generate adversarial examples and a VQGAN to discretize them, creating more informative samples than standard data augmentation techniques. We provide a theoretical framework for the use of a robust teacher in the knowledge distillation with data augmentation setting and demonstrate strong gains in out-of-distribution robustness and clean accuracy across different student architectures. Notably, our method adds minor computational overhead compared to similar techniques and can be easily combined with other data augmen",
    "path": "papers/23/11/2311.01441.json",
    "total_tokens": 1213,
    "translated_title": "从视觉-语言基础模型中提取对抗性鲁棒性的框架",
    "translated_abstract": "我们提出了一个概念简单且轻量级的框架，通过知识蒸馏和数据增强的结合来提高视觉模型的鲁棒性。我们通过从预训练的基础模型中进行蒸馏，展示了在对抗性样本上获得的鲁棒性增益，以此反驳了更大的模型不一定会成为更好的教师的猜想。我们还提出了离散对抗蒸馏（Discrete Adversarial Distillation，DAD）方法，利用鲁棒的教师模型生成对抗样本，并通过VQGAN将其离散化，从而创造出比标准数据增强技术更有信息量的样本。我们提供了一个理论框架，用于在知识蒸馏和数据增强的设置中使用鲁棒的教师模型，并在不同的学生模型中展示了在对抗性样本上的鲁棒性和干净准确性的显著提升。值得注意的是，与类似技术相比，我们的方法增加了少量的计算负载，并且可以轻松与其他数据增强方法相结合。",
    "tldr": "本文提出了一个概念简单且轻量级的框架，通过结合知识蒸馏和数据增强的方法来提高视觉模型的鲁棒性，并从预训练的基础模型中获得鲁棒教师模型的知识。借助离散对抗蒸馏方法，我们生成更有信息量的对抗样本，取得了在对抗性样本上鲁棒性显著提升的结果。此外，我们提供了理论框架来支持在知识蒸馏和数据增强设置中使用鲁棒教师模型，并展示了在不同学生模型上的显著性能提升。我们的方法在计算负载方面的开销较小，并可以与其他数据增强方法轻松结合。"
}