{
    "title": "Controlled Text Generation via Language Model Arithmetic",
    "abstract": "arXiv:2311.14479v2 Announce Type: replace  Abstract: As Large Language Models (LLMs) are deployed more widely, customization with respect to vocabulary, style, and character becomes more important. In this work, we introduce model arithmetic, a novel inference framework for composing and biasing LLMs without the need for model (re)training or highly specific datasets. In addition, the framework allows for more precise control of generated text than direct prompting and prior controlled text generation (CTG) techniques. Using model arithmetic, we can express prior CTG techniques as simple formulas and naturally extend them to new and more effective formulations. Further, we show that speculative sampling, a technique for efficient LLM sampling, extends to our setting. This enables highly efficient text generation with multiple composed models with only marginal overhead over a single model. Our empirical evaluation demonstrates that model arithmetic allows fine-grained control of genera",
    "link": "https://arxiv.org/abs/2311.14479",
    "context": "Title: Controlled Text Generation via Language Model Arithmetic\nAbstract: arXiv:2311.14479v2 Announce Type: replace  Abstract: As Large Language Models (LLMs) are deployed more widely, customization with respect to vocabulary, style, and character becomes more important. In this work, we introduce model arithmetic, a novel inference framework for composing and biasing LLMs without the need for model (re)training or highly specific datasets. In addition, the framework allows for more precise control of generated text than direct prompting and prior controlled text generation (CTG) techniques. Using model arithmetic, we can express prior CTG techniques as simple formulas and naturally extend them to new and more effective formulations. Further, we show that speculative sampling, a technique for efficient LLM sampling, extends to our setting. This enables highly efficient text generation with multiple composed models with only marginal overhead over a single model. Our empirical evaluation demonstrates that model arithmetic allows fine-grained control of genera",
    "path": "papers/23/11/2311.14479.json",
    "total_tokens": 882,
    "translated_title": "通过语言模型算术实现控制文本生成",
    "translated_abstract": "随着大型语言模型（LLMs）的广泛部署，定制词汇、风格和字符变得越来越重要。在这项工作中，我们介绍了模型算术，这是一种新颖的推断框架，可以在不需要模型（重新）训练或高度特定数据集的情况下构成和偏置LLMs。此外，该框架允许比直接提示和先前的受控文本生成（CTG）技术更精确地控制生成的文本。使用模型算术，我们可以将先前的CTG技术表示为简单的公式，并自然地将其扩展到新的和更有效的公式。此外，我们展示了一种称为推测采样的高效LLM采样技术，可以扩展到我们的设置。这使得使用多个组合模型进行高效文本生成几乎没有超过单个模型的额外开销。我们的实证评估表明，模型算术允许对生成进行精细控制。",
    "tldr": "该论文提出了一种模型算术推断框架，可以在不重新训练模型或使用高度特定数据集的情况下构成和偏置大型语言模型，实现比直接提示和先前受控文本生成技术更精确的文本生成控制。",
    "en_tdlr": "This paper introduces a model arithmetic inference framework for composing and biasing Large Language Models without the need for model retraining or highly specific datasets, allowing for more precise control of text generation than direct prompting and prior controlled text generation techniques."
}