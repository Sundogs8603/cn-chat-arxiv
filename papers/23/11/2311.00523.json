{
    "title": "Learning impartial policies for sequential counterfactual explanations using Deep Reinforcement Learning. (arXiv:2311.00523v1 [cs.LG])",
    "abstract": "In the field of explainable Artificial Intelligence (XAI), sequential counterfactual (SCF) examples are often used to alter the decision of a trained classifier by implementing a sequence of modifications to the input instance. Although certain test-time algorithms aim to optimize for each new instance individually, recently Reinforcement Learning (RL) methods have been proposed that seek to learn policies for discovering SCFs, thereby enhancing scalability. As is typical in RL, the formulation of the RL problem, including the specification of state space, actions, and rewards, can often be ambiguous. In this work, we identify shortcomings in existing methods that can result in policies with undesired properties, such as a bias towards specific actions. We propose to use the output probabilities of the classifier to create a more informative reward, to mitigate this effect.",
    "link": "http://arxiv.org/abs/2311.00523",
    "context": "Title: Learning impartial policies for sequential counterfactual explanations using Deep Reinforcement Learning. (arXiv:2311.00523v1 [cs.LG])\nAbstract: In the field of explainable Artificial Intelligence (XAI), sequential counterfactual (SCF) examples are often used to alter the decision of a trained classifier by implementing a sequence of modifications to the input instance. Although certain test-time algorithms aim to optimize for each new instance individually, recently Reinforcement Learning (RL) methods have been proposed that seek to learn policies for discovering SCFs, thereby enhancing scalability. As is typical in RL, the formulation of the RL problem, including the specification of state space, actions, and rewards, can often be ambiguous. In this work, we identify shortcomings in existing methods that can result in policies with undesired properties, such as a bias towards specific actions. We propose to use the output probabilities of the classifier to create a more informative reward, to mitigate this effect.",
    "path": "papers/23/11/2311.00523.json",
    "total_tokens": 828,
    "translated_title": "使用深度强化学习学习顺序可解释策略的无偏方法",
    "translated_abstract": "在可解释人工智能（XAI）领域中，通常使用顺序可解释（SCF）示例通过对输入实例进行一系列修改来改变训练分类器的决策。虽然某些测试时算法旨在针对每个新实例进行优化，但最近提出了强化学习（RL）方法，旨在学习用于发现SCF的策略，从而提高可伸缩性。在RL中，RL问题的制定，包括状态空间，动作和奖励的规定，通常存在歧义。在这项工作中，我们发现现有方法的缺点可能导致具有不希望的属性（如偏向特定动作）的策略。我们建议使用分类器的输出概率来创建更具信息性的奖励，以减轻这种影响。",
    "tldr": "本文提出了使用深度强化学习学习顺序可解释策略的无偏方法。该方法通过对分类器的输出概率进行奖励来减轻现有方法中可能导致偏向特定动作的策略的不希望属性。"
}