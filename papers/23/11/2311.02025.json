{
    "title": "Vicinal Risk Minimization for Few-Shot Cross-lingual Transfer in Abusive Language Detection. (arXiv:2311.02025v1 [cs.CL])",
    "abstract": "Cross-lingual transfer learning from high-resource to medium and low-resource languages has shown encouraging results. However, the scarcity of resources in target languages remains a challenge. In this work, we resort to data augmentation and continual pre-training for domain adaptation to improve cross-lingual abusive language detection. For data augmentation, we analyze two existing techniques based on vicinal risk minimization and propose MIXAG, a novel data augmentation method which interpolates pairs of instances based on the angle of their representations. Our experiments involve seven languages typologically distinct from English and three different domains. The results reveal that the data augmentation strategies can enhance few-shot cross-lingual abusive language detection. Specifically, we observe that consistently in all target languages, MIXAG improves significantly in multidomain and multilingual environments. Finally, we show through an error analysis how the domain adap",
    "link": "http://arxiv.org/abs/2311.02025",
    "context": "Title: Vicinal Risk Minimization for Few-Shot Cross-lingual Transfer in Abusive Language Detection. (arXiv:2311.02025v1 [cs.CL])\nAbstract: Cross-lingual transfer learning from high-resource to medium and low-resource languages has shown encouraging results. However, the scarcity of resources in target languages remains a challenge. In this work, we resort to data augmentation and continual pre-training for domain adaptation to improve cross-lingual abusive language detection. For data augmentation, we analyze two existing techniques based on vicinal risk minimization and propose MIXAG, a novel data augmentation method which interpolates pairs of instances based on the angle of their representations. Our experiments involve seven languages typologically distinct from English and three different domains. The results reveal that the data augmentation strategies can enhance few-shot cross-lingual abusive language detection. Specifically, we observe that consistently in all target languages, MIXAG improves significantly in multidomain and multilingual environments. Finally, we show through an error analysis how the domain adap",
    "path": "papers/23/11/2311.02025.json",
    "total_tokens": 937,
    "translated_title": "少样本跨语言滥用语言检测中的邻近风险最小化",
    "translated_abstract": "从高资源语言到中低资源语言的跨语言迁移学习已经取得了令人鼓舞的结果。然而，目标语言中资源的稀缺性仍然是一个挑战。在这项工作中，我们采用数据增强和持续预训练进行领域自适应，以改善跨语言滥用语言检测。对于数据增强，我们分析了两种现有的基于邻近风险最小化的技术，并提出了MIXAG，一种根据实例表示的角度插值一对实例的新型数据增强方法。我们的实验涉及七种与英语在语言类型上不同的语言和三个不同的领域。结果显示，数据增强策略可以提升少样本跨语言滥用语言检测的性能。具体而言，我们观察到在所有目标语言中，MIXAG在多领域和多语言环境中显著改善。最后，我们通过错误分析展示了领域自适应如何改进迁移学习模型的性能。",
    "tldr": "本研究提出了一种少样本跨语言滥用语言检测的邻近风险最小化方法，通过数据增强和持续预训练进行领域自适应，取得了令人鼓舞的结果。具体而言，在七种不同语言和三个不同领域的情况下，通过提出的MIXAG数据增强方法，显著提升了少样本跨语言滥用语言检测的性能。"
}