{
    "title": "Long Story Short: a Summarize-then-Search Method for Long Video Question Answering. (arXiv:2311.01233v1 [cs.CV])",
    "abstract": "Large language models such as GPT-3 have demonstrated an impressive capability to adapt to new tasks without requiring task-specific training data. This capability has been particularly effective in settings such as narrative question answering, where the diversity of tasks is immense, but the available supervision data is small. In this work, we investigate if such language models can extend their zero-shot reasoning abilities to long multimodal narratives in multimedia content such as drama, movies, and animation, where the story plays an essential role. We propose Long Story Short, a framework for narrative video QA that first summarizes the narrative of the video to a short plot and then searches parts of the video relevant to the question. We also propose to enhance visual matching with CLIPCheck. Our model outperforms state-of-the-art supervised models by a large margin, highlighting the potential of zero-shot QA for long videos.",
    "link": "http://arxiv.org/abs/2311.01233",
    "context": "Title: Long Story Short: a Summarize-then-Search Method for Long Video Question Answering. (arXiv:2311.01233v1 [cs.CV])\nAbstract: Large language models such as GPT-3 have demonstrated an impressive capability to adapt to new tasks without requiring task-specific training data. This capability has been particularly effective in settings such as narrative question answering, where the diversity of tasks is immense, but the available supervision data is small. In this work, we investigate if such language models can extend their zero-shot reasoning abilities to long multimodal narratives in multimedia content such as drama, movies, and animation, where the story plays an essential role. We propose Long Story Short, a framework for narrative video QA that first summarizes the narrative of the video to a short plot and then searches parts of the video relevant to the question. We also propose to enhance visual matching with CLIPCheck. Our model outperforms state-of-the-art supervised models by a large margin, highlighting the potential of zero-shot QA for long videos.",
    "path": "papers/23/11/2311.01233.json",
    "total_tokens": 1010,
    "translated_title": "长话短说：用于长视频问答的总结后搜索方法",
    "translated_abstract": "大型语言模型（如GPT-3）展示了令人印象深刻的能力，即在不需要特定任务的训练数据的情况下适应新任务。这种能力在叙述式问题回答等场景中特别有效，这些场景中任务的多样性很大，但可用的监督数据很少。在这项工作中，我们探讨了这种语言模型是否可以将其零样本推理能力扩展到包含剧情、电影和动画等多媒体内容的长篇多模态叙述中，其中故事起着重要作用。我们提出了“长话短说”框架，用于叙述性视频问答，该框架首先将视频的叙述总结成一个简短的情节，然后搜索与问题相关的视频部分。我们还提出了CLIPCheck来增强视觉匹配。我们的模型大幅超过了最先进的监督模型，突显了零样本问答在长视频中的潜力。",
    "tldr": "本论文研究了大型语言模型是否能够在长篇多模态叙述的多媒体内容中扩展它们的零样本推理能力。作者提出了“长话短说”框架，该框架通过将视频的叙述总结成简短情节，并搜索与问题相关的视频部分，来进行叙述性视频问答。实验证明，该模型在长视频问答任务中的性能显著优于最先进的监督模型，展示了零样本问答在长视频中的潜力。",
    "en_tdlr": "This paper investigates the extension of zero-shot reasoning abilities of large language models to long multimodal narratives in multimedia content. The proposed \"Long Story Short\" framework first summarizes the video narrative into a short plot and then searches for relevant video segments to answer the questions. The model outperforms state-of-the-art supervised models, highlighting the potential of zero-shot QA for long videos."
}