{
    "title": "QFree: A Universal Value Function Factorization for Multi-Agent Reinforcement Learning. (arXiv:2311.00356v1 [cs.AI])",
    "abstract": "Centralized training is widely utilized in the field of multi-agent reinforcement learning (MARL) to assure the stability of training process. Once a joint policy is obtained, it is critical to design a value function factorization method to extract optimal decentralized policies for the agents, which needs to satisfy the individual-global-max (IGM) principle. While imposing additional limitations on the IGM function class can help to meet the requirement, it comes at the cost of restricting its application to more complex multi-agent environments. In this paper, we propose QFree, a universal value function factorization method for MARL. We start by developing mathematical equivalent conditions of the IGM principle based on the advantage function, which ensures that the principle holds without any compromise, removing the conservatism of conventional methods. We then establish a more expressive mixing network architecture that can fulfill the equivalent factorization. In particular, th",
    "link": "http://arxiv.org/abs/2311.00356",
    "context": "Title: QFree: A Universal Value Function Factorization for Multi-Agent Reinforcement Learning. (arXiv:2311.00356v1 [cs.AI])\nAbstract: Centralized training is widely utilized in the field of multi-agent reinforcement learning (MARL) to assure the stability of training process. Once a joint policy is obtained, it is critical to design a value function factorization method to extract optimal decentralized policies for the agents, which needs to satisfy the individual-global-max (IGM) principle. While imposing additional limitations on the IGM function class can help to meet the requirement, it comes at the cost of restricting its application to more complex multi-agent environments. In this paper, we propose QFree, a universal value function factorization method for MARL. We start by developing mathematical equivalent conditions of the IGM principle based on the advantage function, which ensures that the principle holds without any compromise, removing the conservatism of conventional methods. We then establish a more expressive mixing network architecture that can fulfill the equivalent factorization. In particular, th",
    "path": "papers/23/11/2311.00356.json",
    "total_tokens": 865,
    "translated_title": "QFree: 一种用于多智能体强化学习的通用价值函数分解方法",
    "translated_abstract": "集中式训练广泛用于多智能体强化学习中以确保训练过程的稳定性。一旦获得联合策略，设计一种价值函数分解方法以提取代理的最优去中心化策略变得至关重要，该方法需要满足个体-全局最大(IGM)原则。虽然对IGM函数类施加额外限制可以帮助满足要求，但却限制了它在更复杂的多智能体环境中的应用。本文提出一种名为QFree的用于多智能体强化学习的通用价值函数分解方法。首先，我们根据优势函数，开发了满足IGM原则的数学等价条件，确保原则在没有任何妥协的情况下成立，消除了传统方法的保守性。然后，我们建立了一个更具表达力的混合网络架构，可以实现等价的分解。",
    "tldr": "提出了一种通用的QFree价值函数分解方法，用于提取多智能体强化学习中的最优去中心化策略，并遵循个体-全局最大原则。",
    "en_tdlr": "This paper proposes QFree, a universal value function factorization method for multi-agent reinforcement learning (MARL) to extract optimal decentralized policies, satisfying the individual-global-max (IGM) principle, without compromising its application in complex multi-agent environments."
}