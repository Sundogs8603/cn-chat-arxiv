{
    "title": "Video2Music: Suitable Music Generation from Videos using an Affective Multimodal Transformer model. (arXiv:2311.00968v1 [cs.SD])",
    "abstract": "Numerous studies in the field of music generation have demonstrated impressive performance, yet virtually no models are able to directly generate music to match accompanying videos. In this work, we develop a generative music AI framework, Video2Music, that can match a provided video. We first curated a unique collection of music videos. Then, we analysed the music videos to obtain semantic, scene offset, motion, and emotion features. These distinct features are then employed as guiding input to our music generation model. We transcribe the audio files into MIDI and chords, and extract features such as note density and loudness. This results in a rich multimodal dataset, called MuVi-Sync, on which we train a novel Affective Multimodal Transformer (AMT) model to generate music given a video. This model includes a novel mechanism to enforce affective similarity between video and music. Finally, post-processing is performed based on a biGRU-based regression model to estimate note density ",
    "link": "http://arxiv.org/abs/2311.00968",
    "context": "Title: Video2Music: Suitable Music Generation from Videos using an Affective Multimodal Transformer model. (arXiv:2311.00968v1 [cs.SD])\nAbstract: Numerous studies in the field of music generation have demonstrated impressive performance, yet virtually no models are able to directly generate music to match accompanying videos. In this work, we develop a generative music AI framework, Video2Music, that can match a provided video. We first curated a unique collection of music videos. Then, we analysed the music videos to obtain semantic, scene offset, motion, and emotion features. These distinct features are then employed as guiding input to our music generation model. We transcribe the audio files into MIDI and chords, and extract features such as note density and loudness. This results in a rich multimodal dataset, called MuVi-Sync, on which we train a novel Affective Multimodal Transformer (AMT) model to generate music given a video. This model includes a novel mechanism to enforce affective similarity between video and music. Finally, post-processing is performed based on a biGRU-based regression model to estimate note density ",
    "path": "papers/23/11/2311.00968.json",
    "total_tokens": 901,
    "translated_title": "Video2Music：使用情感多模态Transformer模型从视频中生成合适的音乐",
    "translated_abstract": "在音乐生成领域，许多研究展示了令人印象深刻的性能，但几乎没有模型能够直接根据视频生成相匹配的音乐。在这项工作中，我们开发了一个生成音乐的人工智能框架Video2Music，它可以匹配提供的视频。我们首先精心策划了一个独特的音乐视频集合。然后，我们分析音乐视频以获得语义、场景偏移、动作和情感特征。然后，这些不同的特征被用作我们音乐生成模型的引导输入。我们将音频文件转录为MIDI和和弦，并提取音符密度和音量等特征。这产生了一个丰富的多模态数据集MuVi-Sync，我们用这个数据集训练了一个新颖的情感多模态Transformer模型（AMT）来根据视频生成音乐。该模型包括一种新颖的机制来强制视频和音乐之间的情感相似性。最后，基于一个基于双向GRU的回归模型进行后处理，估计音符密度。",
    "tldr": "Video2Music是一个生成音乐的人工智能框架，可以根据视频生成相匹配的音乐。该框架通过分析视频的语义、场景偏移、动作和情感特征，采用Affective Multimodal Transformer (AMT)模型生成音乐。"
}