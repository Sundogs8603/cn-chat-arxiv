{
    "title": "Local Borsuk-Ulam, Stability, and Replicability. (arXiv:2311.01599v1 [cs.LG])",
    "abstract": "We use and adapt the Borsuk-Ulam Theorem from topology to derive limitations on list-replicable and globally stable learning algorithms. We further demonstrate the applicability of our methods in combinatorics and topology.  We show that, besides trivial cases, both list-replicable and globally stable learning are impossible in the agnostic PAC setting. This is in contrast with the realizable case where it is known that any class with a finite Littlestone dimension can be learned by such algorithms. In the realizable PAC setting, we sharpen previous impossibility results and broaden their scope. Specifically, we establish optimal bounds for list replicability and global stability numbers in finite classes. This provides an exponential improvement over previous works and implies an exponential separation from the Littlestone dimension. We further introduce lower bounds for weak learners, i.e., learners that are only marginally better than random guessing. Lower bounds from previous work",
    "link": "http://arxiv.org/abs/2311.01599",
    "context": "Title: Local Borsuk-Ulam, Stability, and Replicability. (arXiv:2311.01599v1 [cs.LG])\nAbstract: We use and adapt the Borsuk-Ulam Theorem from topology to derive limitations on list-replicable and globally stable learning algorithms. We further demonstrate the applicability of our methods in combinatorics and topology.  We show that, besides trivial cases, both list-replicable and globally stable learning are impossible in the agnostic PAC setting. This is in contrast with the realizable case where it is known that any class with a finite Littlestone dimension can be learned by such algorithms. In the realizable PAC setting, we sharpen previous impossibility results and broaden their scope. Specifically, we establish optimal bounds for list replicability and global stability numbers in finite classes. This provides an exponential improvement over previous works and implies an exponential separation from the Littlestone dimension. We further introduce lower bounds for weak learners, i.e., learners that are only marginally better than random guessing. Lower bounds from previous work",
    "path": "papers/23/11/2311.01599.json",
    "total_tokens": 943,
    "translated_title": "本地Borsuk-Ulam, 稳定性和可复制性",
    "translated_abstract": "我们使用并改编了拓扑学中的Borsuk-Ulam定理，推导出对于列表可复制和全局稳定学习算法的限制。我们进一步展示了我们方法在组合学和拓扑学中的适用性。我们证明在不可知PAC(setting)中，除了平凡的情况外，列表可复制和全局稳定学习都是不可能的。这与可实现的情况形成了对比，已知具有有限Littlestone维度的任何类都可以通过这样的算法学习。在可实现的PAC(setting)中，我们加强了以前的不可能结果并扩大了它们的范围。具体而言，我们为有限类别中的列表可复制和全局稳定数量确定了最佳界限。这相对于以前的工作提供了指数级的改进，并意味着与Littlestone维度的指数级分离。我们进一步引入了弱学习者的下界，即仅比随机猜测稍微好一点的学习者。以前的工作中提供了下界",
    "tldr": "使用拓扑学中的Borsuk-Ulam定理，研究了列表可复制和全局稳定学习算法的限制。在不可知PAC设置中，这些学习算法是不可能的。在可实现的PAC设置中，提供了最佳界限和下界，与Littlestone维度有指数级分离。"
}