{
    "title": "PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion. (arXiv:2311.01767v1 [cs.CL])",
    "abstract": "Recent evaluations of Large Language Models (LLMs) have centered around testing their zero-shot/few-shot capabilities for basic natural language tasks and their ability to translate instructions into tool APIs. However, the evaluation of LLMs utilizing complex tools to finish multi-turn, multi-modal instructions in a complex multi-modal environment has not been investigated. To address this gap, we introduce the PowerPoint Task Completion (PPTC) benchmark to assess LLMs' ability to create and edit PPT files based on user instructions. It contains 279 multi-turn sessions covering diverse topics and hundreds of instructions involving multi-modal operations. We also propose the PPTX-Match Evaluation System that evaluates if LLMs finish the instruction based on the prediction file rather than the label API sequence, thus it supports various LLM-generated API sequences. We measure 3 closed LLMs and 6 open-source LLMs. The results show that GPT-4 outperforms other LLMs with 75.1\\% accuracy i",
    "link": "http://arxiv.org/abs/2311.01767",
    "context": "Title: PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion. (arXiv:2311.01767v1 [cs.CL])\nAbstract: Recent evaluations of Large Language Models (LLMs) have centered around testing their zero-shot/few-shot capabilities for basic natural language tasks and their ability to translate instructions into tool APIs. However, the evaluation of LLMs utilizing complex tools to finish multi-turn, multi-modal instructions in a complex multi-modal environment has not been investigated. To address this gap, we introduce the PowerPoint Task Completion (PPTC) benchmark to assess LLMs' ability to create and edit PPT files based on user instructions. It contains 279 multi-turn sessions covering diverse topics and hundreds of instructions involving multi-modal operations. We also propose the PPTX-Match Evaluation System that evaluates if LLMs finish the instruction based on the prediction file rather than the label API sequence, thus it supports various LLM-generated API sequences. We measure 3 closed LLMs and 6 open-source LLMs. The results show that GPT-4 outperforms other LLMs with 75.1\\% accuracy i",
    "path": "papers/23/11/2311.01767.json",
    "total_tokens": 920,
    "translated_title": "PPTC基准：评估大型语言模型在PowerPoint任务完成中的表现",
    "translated_abstract": "近期对大型语言模型（LLM）的评估主要集中在测试它们对基本自然语言任务的零次/少次尝试能力以及将指令翻译成工具API的能力上。然而，对于利用复杂工具完成复杂多轮、多模态指令的LLM的评估尚未进行研究。为了填补这个空白，我们引入了PowerPoint任务完成（PPTC）基准，评估LLM根据用户指令创建和编辑PPT文件的能力。它包含279个涵盖不同主题的多轮对话，涉及多模态操作的数百个指令。我们还提出了PPTX-Match评估系统，该系统根据预测文件而不是标签API序列来评估LLM是否完成了指令，因此支持各种LLM生成的API序列。我们测试了3个闭合型LLM和6个开源LLM。结果表明，GPT-4在准确率方面优于其他LLM，达到了75.1%。",
    "tldr": "这个论文介绍了PPTC基准，用来评估大型语言模型在根据用户指令创建和编辑PPT文件方面的表现。通过测试，发现GPT-4在准确率方面表现最好，为75.1%。",
    "en_tdlr": "This paper introduces the PPTC benchmark to evaluate the performance of large language models in creating and editing PPT files based on user instructions. The results show that GPT-4 outperforms other models with an accuracy of 75.1%."
}