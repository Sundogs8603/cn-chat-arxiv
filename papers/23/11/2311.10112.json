{
    "title": "zrLLM: Zero-Shot Relational Learning on Temporal Knowledge Graphs with Large Language Models",
    "abstract": "arXiv:2311.10112v2 Announce Type: replace  Abstract: Modeling evolving knowledge over temporal knowledge graphs (TKGs) has become a heated topic. Various methods have been proposed to forecast links on TKGs. Most of them are embedding-based, where hidden representations are learned to represent knowledge graph (KG) entities and relations based on the observed graph contexts. Although these methods show strong performance on traditional TKG forecasting (TKGF) benchmarks, they face a strong challenge in modeling the unseen zero-shot relations that have no prior graph context. In this paper, we try to mitigate this problem as follows. We first input the text descriptions of KG relations into large language models (LLMs) for generating relation representations, and then introduce them into embedding-based TKGF methods. LLM-empowered representations can capture the semantic information in the relation descriptions. This makes the relations, whether seen or unseen, with similar semantic mean",
    "link": "https://arxiv.org/abs/2311.10112",
    "context": "Title: zrLLM: Zero-Shot Relational Learning on Temporal Knowledge Graphs with Large Language Models\nAbstract: arXiv:2311.10112v2 Announce Type: replace  Abstract: Modeling evolving knowledge over temporal knowledge graphs (TKGs) has become a heated topic. Various methods have been proposed to forecast links on TKGs. Most of them are embedding-based, where hidden representations are learned to represent knowledge graph (KG) entities and relations based on the observed graph contexts. Although these methods show strong performance on traditional TKG forecasting (TKGF) benchmarks, they face a strong challenge in modeling the unseen zero-shot relations that have no prior graph context. In this paper, we try to mitigate this problem as follows. We first input the text descriptions of KG relations into large language models (LLMs) for generating relation representations, and then introduce them into embedding-based TKGF methods. LLM-empowered representations can capture the semantic information in the relation descriptions. This makes the relations, whether seen or unseen, with similar semantic mean",
    "path": "papers/23/11/2311.10112.json",
    "total_tokens": 919,
    "translated_title": "zrLLM：在具有大型语言模型的时间知识图上进行零样本关系学习",
    "translated_abstract": "模型化随时间变化的知识在时间知识图(TKGs)上已成为一个炽热话题。已经提出了各种方法来预测TKGs上的链接。其中大多数是基于嵌入的，其中学习隐藏表示以基于观察到的图上下文来表示知识图(KG)实体和关系。尽管这些方法在传统的TKG预测(TKGF)基准上表现出色，但它们在建模没有先前图上下文的未见过的零样本关系上面临强烈挑战。本文尝试解决这个问题的方法如下。我们首先将KG关系的文本描述输入大型语言模型(LLMs)中以生成关系表示，然后将它们引入基于嵌入的TKGF方法中。LLM增强的表示可以捕捉关系描述中的语义信息。这使得关系，无论是已见还是未见的，都能够获得类似的语义含义。",
    "tldr": "本文提出了一种在时间知识图上进行零样本关系学习的方法，该方法利用大型语言模型(LLM)生成关系表示，并将其引入基于嵌入的TKGF方法中，能够捕捉关系描述中的语义信息，从而使得关系在建模时能具有相似的语义含义。",
    "en_tdlr": "This paper presents a method for zero-shot relational learning on temporal knowledge graphs, which leverages large language models (LLMs) to generate relation representations and introduces them into embedding-based TKGF methods to capture semantic information in relation descriptions, enabling relations to have similar semantic meanings during modeling."
}