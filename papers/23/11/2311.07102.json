{
    "title": "Fovea Transformer: Efficient Long-Context Modeling with Structured Fine-to-Coarse Attention. (arXiv:2311.07102v2 [cs.CL] UPDATED)",
    "abstract": "The quadratic complexity of self-attention in Transformers has hindered the processing of long text. To alleviate this problem, previous works have proposed to sparsify the attention matrix, taking advantage of the observation that crucial information about a token can be derived from its neighbors. These methods typically combine one or another form of local attention and global attention. Such combinations introduce abrupt changes in contextual granularity when going from local to global, which may be undesirable. We believe that a smoother transition could potentially enhance model's ability to capture long-context dependencies. In this study, we introduce Fovea Transformer, a long-context focused transformer that addresses the challenges of capturing global dependencies while maintaining computational efficiency. To achieve this, we construct a multi-scale tree from the input sequence, and use representations of context tokens with a progressively coarser granularity in the tree, a",
    "link": "http://arxiv.org/abs/2311.07102",
    "context": "Title: Fovea Transformer: Efficient Long-Context Modeling with Structured Fine-to-Coarse Attention. (arXiv:2311.07102v2 [cs.CL] UPDATED)\nAbstract: The quadratic complexity of self-attention in Transformers has hindered the processing of long text. To alleviate this problem, previous works have proposed to sparsify the attention matrix, taking advantage of the observation that crucial information about a token can be derived from its neighbors. These methods typically combine one or another form of local attention and global attention. Such combinations introduce abrupt changes in contextual granularity when going from local to global, which may be undesirable. We believe that a smoother transition could potentially enhance model's ability to capture long-context dependencies. In this study, we introduce Fovea Transformer, a long-context focused transformer that addresses the challenges of capturing global dependencies while maintaining computational efficiency. To achieve this, we construct a multi-scale tree from the input sequence, and use representations of context tokens with a progressively coarser granularity in the tree, a",
    "path": "papers/23/11/2311.07102.json",
    "total_tokens": 852,
    "translated_title": "Fovea Transformer：具有结构化精细至粗粒度注意力的高效长上下文建模",
    "translated_abstract": "Transformer中自注意力的二次复杂性限制了对长文本的处理。为了缓解这个问题，先前的研究提出了稀疏化注意力矩阵的方法，利用了从邻近单词中获取关键信息的观察结果。这些方法通常结合了局部注意力和全局注意力的形式。然而，这种组合在从局部到全局转变时引入了背景粒度的突变，可能是不可取的。我们认为，更平滑的过渡可能会增强模型捕捉长上下文依赖性的能力。在本研究中，我们引入了Fovea Transformer，这是一个专注于长上下文的Transformer，旨在解决捕捉全局依赖性并保持计算效率的挑战。为了实现这一目标，我们从输入序列构建了一个多尺度树，并在树中使用逐渐粗粒度的上下文单词表示。",
    "tldr": "Fovea Transformer是一个专注于长上下文的Transformer模型，通过构建多尺度树和逐渐粗粒度表示的上下文单词，解决了处理长文本时的复杂性问题。",
    "en_tdlr": "Fovea Transformer is a Transformer model that focuses on handling long-context text by constructing a multi-scale tree and progressively coarser representations of context tokens, addressing the complexity issue when processing long text."
}