{
    "title": "Guaranteeing Control Requirements via Reward Shaping in Reinforcement Learning",
    "abstract": "arXiv:2311.10026v2 Announce Type: replace-cross  Abstract: In addressing control problems such as regulation and tracking through reinforcement learning, it is often required to guarantee that the acquired policy meets essential performance and stability criteria such as a desired settling time and steady-state error prior to deployment. Motivated by this necessity, we present a set of results and a systematic reward shaping procedure that (i) ensures the optimal policy generates trajectories that align with specified control requirements and (ii) allows to assess whether any given policy satisfies them. We validate our approach through comprehensive numerical experiments conducted in two representative environments from OpenAI Gym: the Inverted Pendulum swing-up problem and the Lunar Lander. Utilizing both tabular and deep reinforcement learning methods, our experiments consistently affirm the efficacy of our proposed framework, highlighting its effectiveness in ensuring policy adhere",
    "link": "https://arxiv.org/abs/2311.10026",
    "context": "Title: Guaranteeing Control Requirements via Reward Shaping in Reinforcement Learning\nAbstract: arXiv:2311.10026v2 Announce Type: replace-cross  Abstract: In addressing control problems such as regulation and tracking through reinforcement learning, it is often required to guarantee that the acquired policy meets essential performance and stability criteria such as a desired settling time and steady-state error prior to deployment. Motivated by this necessity, we present a set of results and a systematic reward shaping procedure that (i) ensures the optimal policy generates trajectories that align with specified control requirements and (ii) allows to assess whether any given policy satisfies them. We validate our approach through comprehensive numerical experiments conducted in two representative environments from OpenAI Gym: the Inverted Pendulum swing-up problem and the Lunar Lander. Utilizing both tabular and deep reinforcement learning methods, our experiments consistently affirm the efficacy of our proposed framework, highlighting its effectiveness in ensuring policy adhere",
    "path": "papers/23/11/2311.10026.json",
    "total_tokens": 803,
    "translated_title": "通过奖励塑形在强化学习中保证控制需求",
    "translated_abstract": "在强化学习中解决诸如调节和跟踪等控制问题时，通常需要确保所获得的策略在部署之前满足关键的性能和稳定性标准，如期望的稳定时间和稳态误差。 鉴于这种必要性，我们提出了一组结果和一种系统奖励塑形程序，该程序（i）确保最优策略生成符合指定控制要求的轨迹，（ii）允许评估任何给定策略是否满足这些要求。 我们通过在两个代表性的OpenAI Gym环境中进行的全面数值实验来验证我们的方法：倒立摆摆动问题和月球着陆器。 利用表格式和深度强化学习方法，我们的实验证明了我们提出的框架的有效性，并突出了它在确保策略遵守方面的有效性。",
    "tldr": "提出了一种奖励塑形程序，确保最优策略生成符合指定控制要求的轨迹，并评估策略是否满足这些要求。",
    "en_tdlr": "Introduced a reward shaping procedure to guarantee that the optimal policy generates trajectories aligning with specified control requirements and to assess whether any given policy satisfies them."
}