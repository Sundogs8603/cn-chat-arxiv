{
    "title": "StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving",
    "abstract": "arXiv:2311.08803v2 Announce Type: replace  Abstract: Most existing chain-of-thought (CoT) prompting methods suffer from the issues of generalizability and consistency, as they often rely on instance-specific solutions that may not be applicable to other cases and lack task-level consistency in their reasoning steps. To address these limitations, we propose a comprehensive framework, StrategyLLM, harnessing the capabilities of LLMs to construct generalizable and consistent few-shot prompts for various tasks automatically. To this end, StrategyLLM employs four LLM-based agents: strategy generator, executor, optimizer, and evaluator, working together to generate, evaluate, and select promising strategies for a given task. The experimental results demonstrate that StrategyLLM outperforms the competitive baseline CoT-SC that requires human-annotated solutions on 13 datasets across 4 challenging tasks without human involvement, including math reasoning (34.21% $\\rightarrow$ 38.79%), commonse",
    "link": "https://arxiv.org/abs/2311.08803",
    "context": "Title: StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving\nAbstract: arXiv:2311.08803v2 Announce Type: replace  Abstract: Most existing chain-of-thought (CoT) prompting methods suffer from the issues of generalizability and consistency, as they often rely on instance-specific solutions that may not be applicable to other cases and lack task-level consistency in their reasoning steps. To address these limitations, we propose a comprehensive framework, StrategyLLM, harnessing the capabilities of LLMs to construct generalizable and consistent few-shot prompts for various tasks automatically. To this end, StrategyLLM employs four LLM-based agents: strategy generator, executor, optimizer, and evaluator, working together to generate, evaluate, and select promising strategies for a given task. The experimental results demonstrate that StrategyLLM outperforms the competitive baseline CoT-SC that requires human-annotated solutions on 13 datasets across 4 challenging tasks without human involvement, including math reasoning (34.21% $\\rightarrow$ 38.79%), commonse",
    "path": "papers/23/11/2311.08803.json",
    "total_tokens": 873,
    "translated_title": "StrategyLLM：大型语言模型作为问题解决的策略生成器、执行器、优化器和评估器",
    "translated_abstract": "大多数现有的思维链 (CoT) 提示方法存在泛化和一致性问题，因为它们常常依赖于特定实例的解决方案，这些解决方案可能不适用于其他情况，并缺乏在推理步骤中的任务级一致性。为解决这些限制，我们提出了一个全面的框架，StrategyLLM，利用LLM的能力自动构建可推广和一致的少次提示以用于各种任务。为此，StrategyLLM 使用四个基于LLM的代理：策略生成器、执行器、优化器和评估器，共同工作以为给定任务生成、评估和选择有前途的策略。实验结果表明，在13个数据集上跨4个挑战性任务上，不需要人工参与，StrategyLLM 在数学推理（34.21%->38.79%）、常见推理等任务上优于竞争基线CoT-SC，该基线需要人工注释的解决方案。",
    "tldr": "StrategyLLM提出了一个框架，利用大型语言模型的能力自动构建可推广和一致的少次提示，优于竞争基线，不需要人工参与。",
    "en_tdlr": "StrategyLLM introduces a framework that leverages large language models to automatically generate generalizable and consistent few-shot prompts, outperforming competitive baselines without human involvement."
}