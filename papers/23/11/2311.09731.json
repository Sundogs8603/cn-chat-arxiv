{
    "title": "Examining LLMs' Uncertainty Expression Towards Questions Outside Parametric Knowledge",
    "abstract": "arXiv:2311.09731v2 Announce Type: replace-cross  Abstract: Can large language models (LLMs) express their uncertainty in situations where they lack sufficient parametric knowledge to generate reasonable responses? This work aims to systematically investigate LLMs' behaviors in such situations, emphasizing the trade-off between honesty and helpfulness. To tackle the challenge of precisely determining LLMs' knowledge gaps, we diagnostically create unanswerable questions containing non-existent concepts or false premises, ensuring that they are outside the LLMs' vast training data. By compiling a benchmark, UnknownBench, which consists of both unanswerable and answerable questions, we quantitatively evaluate the LLMs' performance in maintaining honesty while being helpful. Using a model-agnostic unified confidence elicitation approach, we observe that most LLMs fail to consistently refuse or express uncertainty towards questions outside their parametric knowledge, although instruction fin",
    "link": "https://arxiv.org/abs/2311.09731",
    "context": "Title: Examining LLMs' Uncertainty Expression Towards Questions Outside Parametric Knowledge\nAbstract: arXiv:2311.09731v2 Announce Type: replace-cross  Abstract: Can large language models (LLMs) express their uncertainty in situations where they lack sufficient parametric knowledge to generate reasonable responses? This work aims to systematically investigate LLMs' behaviors in such situations, emphasizing the trade-off between honesty and helpfulness. To tackle the challenge of precisely determining LLMs' knowledge gaps, we diagnostically create unanswerable questions containing non-existent concepts or false premises, ensuring that they are outside the LLMs' vast training data. By compiling a benchmark, UnknownBench, which consists of both unanswerable and answerable questions, we quantitatively evaluate the LLMs' performance in maintaining honesty while being helpful. Using a model-agnostic unified confidence elicitation approach, we observe that most LLMs fail to consistently refuse or express uncertainty towards questions outside their parametric knowledge, although instruction fin",
    "path": "papers/23/11/2311.09731.json",
    "total_tokens": 838,
    "translated_title": "探究大型语言模型如何表达对超出参数化知识范围的问题的不确定性",
    "translated_abstract": "这项工作旨在系统地调查大型语言模型在缺乏足够参数化知识以生成合理回应的情况下的行为，强调诚实与帮助性之间的权衡。为了精确确定语言模型的知识空白挑战，我们诊断性地创建了包含不存在概念或错误前提的无法回答的问题，确保它们超出了语言模型庞大的训练数据。通过编制一个包含既有无法回答也有可回答问题的基准，UnknownBench，我们定量评估语言模型在保持诚实的同时提供帮助的表现。使用一个模型无关的统一信心引导方法，我们观察到大多数语言模型在一致拒绝或表达对超出其参数化知识范围的问题的不确定性方面表现不佳。",
    "tldr": "本研究系统地调查了大型语言模型在缺乏足够参数化知识的情况下如何表达对超出其知识范围的问题的不确定性，并强调了诚实与帮助性之间的权衡。"
}