{
    "title": "Weight-Sharing Regularization",
    "abstract": "arXiv:2311.03096v2 Announce Type: replace  Abstract: Weight-sharing is ubiquitous in deep learning. Motivated by this, we propose a \"weight-sharing regularization\" penalty on the weights $w \\in \\mathbb{R}^d$ of a neural network, defined as $\\mathcal{R}(w) = \\frac{1}{d - 1}\\sum_{i > j}^d |w_i - w_j|$. We study the proximal mapping of $\\mathcal{R}$ and provide an intuitive interpretation of it in terms of a physical system of interacting particles. We also parallelize existing algorithms for $\\operatorname{prox}_\\mathcal{R}$ (to run on GPU) and find that one of them is fast in practice but slow ($O(d)$) for worst-case inputs. Using the physical interpretation, we design a novel parallel algorithm which runs in $O(\\log^3 d)$ when sufficient processors are available, thus guaranteeing fast training. Our experiments reveal that weight-sharing regularization enables fully connected networks to learn convolution-like filters even when pixels have been shuffled while convolutional neural netwo",
    "link": "https://arxiv.org/abs/2311.03096",
    "context": "Title: Weight-Sharing Regularization\nAbstract: arXiv:2311.03096v2 Announce Type: replace  Abstract: Weight-sharing is ubiquitous in deep learning. Motivated by this, we propose a \"weight-sharing regularization\" penalty on the weights $w \\in \\mathbb{R}^d$ of a neural network, defined as $\\mathcal{R}(w) = \\frac{1}{d - 1}\\sum_{i > j}^d |w_i - w_j|$. We study the proximal mapping of $\\mathcal{R}$ and provide an intuitive interpretation of it in terms of a physical system of interacting particles. We also parallelize existing algorithms for $\\operatorname{prox}_\\mathcal{R}$ (to run on GPU) and find that one of them is fast in practice but slow ($O(d)$) for worst-case inputs. Using the physical interpretation, we design a novel parallel algorithm which runs in $O(\\log^3 d)$ when sufficient processors are available, thus guaranteeing fast training. Our experiments reveal that weight-sharing regularization enables fully connected networks to learn convolution-like filters even when pixels have been shuffled while convolutional neural netwo",
    "path": "papers/23/11/2311.03096.json",
    "total_tokens": 909,
    "translated_title": "权重共享正则化",
    "translated_abstract": "在深度学习中，权重共享是无处不在的。受此启发，我们提出了对神经网络的权重$w \\in \\mathbb{R}^d$进行“权重共享正则化”惩罚，定义为$\\mathcal{R}(w) = \\frac{1}{d - 1}\\sum_{i > j}^d |w_i - w_j|$。我们研究了$\\mathcal{R}$的近端映射，并通过一个物理粒子相互作用的系统对其进行了直观解释。我们还并行化了现有的$\\operatorname{prox}_\\mathcal{R}$算法（在GPU上运行），并发现其中一种在实践中快速，但对于最坏情况输入较慢（$O(d)$）。利用物理解释，我们设计了一种新颖的并行算法，当有足够的处理器可用时，其运行时间为$O(\\log^3 d)$，从而保证了快速训练。我们的实验显示，权重共享正则化使全连接网络能够学习卷积样式的滤波器，即使像素已被打乱，而卷积神经网",
    "tldr": "提出了一种权重共享正则化方法，通过引入对神经网络权重的惩罚，设计并实现了一个新型并行算法，使网络能够学习卷积样式的滤波器",
    "en_tdlr": "Introduced weight-sharing regularization method, designed and implemented a novel parallel algorithm through penalty on neural network weights, enabling the network to learn convolution-like filters."
}