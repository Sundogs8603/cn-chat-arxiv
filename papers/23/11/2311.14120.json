{
    "title": "Weight fluctuations in (deep) linear neural networks and a derivation of the inverse-variance flatness relation",
    "abstract": "arXiv:2311.14120v2 Announce Type: replace  Abstract: We investigate the stationary (late-time) training regime of single- and two-layer linear underparameterized neural networks within the continuum limit of stochastic gradient descent (SGD) for synthetic Gaussian data. In the case of a single-layer network in the weakly underparameterized regime, the spectrum of the noise covariance matrix deviates notably from the Hessian, which can be attributed to the broken detailed balance of SGD dynamics. The weight fluctuations are in this case generally anisotropic, but are subject to an isotropic loss. For a two-layer network, we obtain the stochastic dynamics of the weights in each layer and analyze the associated stationary covariances. We identify the inter-layer coupling as a new source of anisotropy for the weight fluctuations. In contrast to the single-layer case, the weight fluctuations experience an anisotropic loss, the flatness of which is inversely related to the fluctuation varian",
    "link": "https://arxiv.org/abs/2311.14120",
    "context": "Title: Weight fluctuations in (deep) linear neural networks and a derivation of the inverse-variance flatness relation\nAbstract: arXiv:2311.14120v2 Announce Type: replace  Abstract: We investigate the stationary (late-time) training regime of single- and two-layer linear underparameterized neural networks within the continuum limit of stochastic gradient descent (SGD) for synthetic Gaussian data. In the case of a single-layer network in the weakly underparameterized regime, the spectrum of the noise covariance matrix deviates notably from the Hessian, which can be attributed to the broken detailed balance of SGD dynamics. The weight fluctuations are in this case generally anisotropic, but are subject to an isotropic loss. For a two-layer network, we obtain the stochastic dynamics of the weights in each layer and analyze the associated stationary covariances. We identify the inter-layer coupling as a new source of anisotropy for the weight fluctuations. In contrast to the single-layer case, the weight fluctuations experience an anisotropic loss, the flatness of which is inversely related to the fluctuation varian",
    "path": "papers/23/11/2311.14120.json",
    "total_tokens": 940,
    "translated_title": "(深)线性神经网络中的权重波动和逆方差平直关系的推导",
    "translated_abstract": "我们在合成高斯数据的随机梯度下降（SGD）的连续极限内，研究了单层和双层线性欠参数化神经网络的稳定（末态）训练规则。对于 schwach欠参数化区域中的单层网络，噪声协方差矩阵的谱明显偏离Hessian，可以归因于SGD动态的破坏详细平衡。在这种情况下，权重波动通常是各向异性的，但受各向同性损失限制。对于双层网络，我们获得了每层权重的随机动力学，并分析了相关的稳定协方差。我们确定了层间耦合作为权重波动的各向异性的新来源。与单层情况相反，权重波动经历各向异性损失，其平直度与波动的方差成反比。",
    "tldr": "该研究探讨了单层和双层线性神经网络在随机梯度下降中的稳定训练规则，并发现了权重波动在各种情况下的各向异性特征，其中双层网络中的权重波动受到层间耦合的影响，并呈现出各向异性损失。",
    "en_tdlr": "The study investigates the stationary training regime of single- and two-layer linear neural networks under stochastic gradient descent, revealing the anisotropic characteristics of weight fluctuations in different scenarios, with inter-layer coupling affecting the anisotropy loss of weight fluctuations in two-layer networks."
}