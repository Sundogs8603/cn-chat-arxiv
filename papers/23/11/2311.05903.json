{
    "title": "Establishing Performance Baselines in Fine-Tuning, Retrieval-Augmented Generation and Soft-Prompting for Non-Specialist LLM Users",
    "abstract": "arXiv:2311.05903v2 Announce Type: replace-cross  Abstract: Research into methods for improving the performance of large language models (LLMs) through fine-tuning, retrieval-augmented generation (RAG) and soft-prompting has tended to focus on the use of highly technical or high-cost techniques, making many of the newly discovered approaches comparatively inaccessible to non-technical users. In this paper we tested an unmodified version of GPT 3.5, a fine-tuned version, and the same unmodified model when given access to a vectorised RAG database, both in isolation and in combination with a basic, non-algorithmic soft prompt. In each case we tested the model's ability to answer a set of 100 questions relating primarily to events that occurred after September 2021 (the point at which GPT 3.5's training data set ends). We found that if commercial platforms are used and default settings are applied with no iteration in order to establish a baseline set of outputs, a fine-tuned model outperf",
    "link": "https://arxiv.org/abs/2311.05903",
    "context": "Title: Establishing Performance Baselines in Fine-Tuning, Retrieval-Augmented Generation and Soft-Prompting for Non-Specialist LLM Users\nAbstract: arXiv:2311.05903v2 Announce Type: replace-cross  Abstract: Research into methods for improving the performance of large language models (LLMs) through fine-tuning, retrieval-augmented generation (RAG) and soft-prompting has tended to focus on the use of highly technical or high-cost techniques, making many of the newly discovered approaches comparatively inaccessible to non-technical users. In this paper we tested an unmodified version of GPT 3.5, a fine-tuned version, and the same unmodified model when given access to a vectorised RAG database, both in isolation and in combination with a basic, non-algorithmic soft prompt. In each case we tested the model's ability to answer a set of 100 questions relating primarily to events that occurred after September 2021 (the point at which GPT 3.5's training data set ends). We found that if commercial platforms are used and default settings are applied with no iteration in order to establish a baseline set of outputs, a fine-tuned model outperf",
    "path": "papers/23/11/2311.05903.json",
    "total_tokens": 877,
    "translated_title": "在非专业LLM用户中为微调，检索增强生成和软提示建立性能基线",
    "translated_abstract": "通过微调、检索增强生成（RAG）和软提示来改善大型语言模型（LLMs）性能的方法研究往往聚焦于使用高度技术化或高成本的技术，使许多新发现的方法对非技术用户来说相对不易访问。在本文中，我们测试了未经修改的GPT 3.5版本、经微调的版本，以及在单独访问基于向量的RAG数据库时相同未经修改的模型，这两种情况下都配备了基本的非算法软提示。在每种情况下，我们测试了模型回答一组主要涉及2021年9月之后事件的100个问题的能力（这是GPT 3.5的训练数据集结束的时间点）。我们发现，如果使用商业平台并应用默认设置，不进行迭代以建立一组基线输出，那么经过微调的模型表现更好。",
    "tldr": "通过对GPT 3.5进行微调，并结合基于向量的RAG数据库和非算法软提示，建立了性能基线，发现在特定的测试条件下微调模型表现更好。",
    "en_tdlr": "Performance baselines were established by fine-tuning GPT 3.5, using a vectorised RAG database and non-algorithmic soft prompts, finding that the fine-tuned model performed better under specific testing conditions."
}