{
    "title": "Revisiting the Knowledge Injection Frameworks. (arXiv:2311.01150v1 [cs.CL])",
    "abstract": "In recent years, large language models (LLMs), such as GPTs, have attained great impact worldwide. However, how to adapt these LLMs to better suit the vertical domain-specific tasks by utilizing external knowledge remains not completely solved. Indeed, there have emerged a few works on this line where most of them rely on an alignment heuristic that is built to inject the corresponding knowledge tuple into the associated text sample.  However, despite the promise, we identify a pivotal problem in this work ubiquitously. Simply put, we find that injecting unaligned (i.e., random) knowledge tuple into the LLMs achieves comparable (and sometimes better) results than the aligned knowledge being injected. We therefore take a thorough investigation of this frustrating finding on a variety of related prior work and further provide a chain of potential interpretations for the phenomenon. Based on all that, we offer a simple remediated technique. Briefly, the core of this technique is rooted in",
    "link": "http://arxiv.org/abs/2311.01150",
    "context": "Title: Revisiting the Knowledge Injection Frameworks. (arXiv:2311.01150v1 [cs.CL])\nAbstract: In recent years, large language models (LLMs), such as GPTs, have attained great impact worldwide. However, how to adapt these LLMs to better suit the vertical domain-specific tasks by utilizing external knowledge remains not completely solved. Indeed, there have emerged a few works on this line where most of them rely on an alignment heuristic that is built to inject the corresponding knowledge tuple into the associated text sample.  However, despite the promise, we identify a pivotal problem in this work ubiquitously. Simply put, we find that injecting unaligned (i.e., random) knowledge tuple into the LLMs achieves comparable (and sometimes better) results than the aligned knowledge being injected. We therefore take a thorough investigation of this frustrating finding on a variety of related prior work and further provide a chain of potential interpretations for the phenomenon. Based on all that, we offer a simple remediated technique. Briefly, the core of this technique is rooted in",
    "path": "papers/23/11/2311.01150.json",
    "total_tokens": 916,
    "translated_title": "重访知识注入框架",
    "translated_abstract": "近年来，大型语言模型（LLMs），如GPT，已在全球范围内产生了巨大的影响。然而，如何利用外部知识使这些LLMs更适应垂直领域特定任务的问题尚未完全解决。实际上，在这方面已经出现了一些工作，其中大部分依赖于构建对齐启发式规则，通过将相应的知识元组注入到相关的文本样本中。然而，尽管有希望，但我们普遍发现这项工作中存在一个关键问题。简而言之，我们发现将未对齐（即随机）的知识元组注入到LLMs中，可以取得与注入对齐知识相当甚至更好的结果。因此，我们对这一令人沮丧的发现进行了彻底的调查，并进一步提供了一系列可能的解释。基于这一切，我们提供了一种简单的修正技术。简要地说，这种技术的核心根植于...",
    "tldr": "这项研究重新审视了知识注入框架，发现将未对齐的随机知识注入到大型语言模型中可以取得与对齐知识相当甚至更好的结果。研究还提供了一种简单的修正技术来解决这个问题。",
    "en_tdlr": "This research revisits the knowledge injection frameworks and finds that injecting unaligned random knowledge into large language models can achieve comparable or even better results than aligned knowledge injection. The study also provides a simple remediation technique to address this issue."
}