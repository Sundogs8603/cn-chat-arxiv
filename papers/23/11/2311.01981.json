{
    "title": "ProSG: Using Prompt Synthetic Gradients to Alleviate Prompt Forgetting of RNN-like Language Models. (arXiv:2311.01981v1 [cs.CL])",
    "abstract": "RNN-like language models are getting renewed attention from NLP researchers in recent years and several models have made significant progress, which demonstrates performance comparable to traditional transformers. However, due to the recurrent nature of RNNs, this kind of language model can only store information in a set of fixed-length state vectors. As a consequence, they still suffer from forgetfulness though after a lot of improvements and optimizations, when given complex instructions or prompts. As the prompted generation is the main and most concerned function of LMs, solving the problem of forgetting in the process of generation is no wonder of vital importance. In this paper, focusing on easing the prompt forgetting during generation, we proposed an architecture to teach the model memorizing prompt during generation by synthetic gradient. To force the model to memorize the prompt, we derive the states that encode the prompt, then transform it into model parameter modification",
    "link": "http://arxiv.org/abs/2311.01981",
    "context": "Title: ProSG: Using Prompt Synthetic Gradients to Alleviate Prompt Forgetting of RNN-like Language Models. (arXiv:2311.01981v1 [cs.CL])\nAbstract: RNN-like language models are getting renewed attention from NLP researchers in recent years and several models have made significant progress, which demonstrates performance comparable to traditional transformers. However, due to the recurrent nature of RNNs, this kind of language model can only store information in a set of fixed-length state vectors. As a consequence, they still suffer from forgetfulness though after a lot of improvements and optimizations, when given complex instructions or prompts. As the prompted generation is the main and most concerned function of LMs, solving the problem of forgetting in the process of generation is no wonder of vital importance. In this paper, focusing on easing the prompt forgetting during generation, we proposed an architecture to teach the model memorizing prompt during generation by synthetic gradient. To force the model to memorize the prompt, we derive the states that encode the prompt, then transform it into model parameter modification",
    "path": "papers/23/11/2311.01981.json",
    "total_tokens": 823,
    "translated_title": "使用提示合成梯度缓解RNN-like语言模型的遗忘现象",
    "translated_abstract": "近年来，NLP研究人员越来越关注RNN-like语言模型，并且有几个模型取得了显著进展，展示出与传统Transformer相当的性能。然而，由于RNN的循环性质，这种语言模型只能在一组固定长度的状态向量中存储信息。因此，尽管经过了许多改进和优化，当给出复杂的指令或提示时，它们仍然会遗忘。作为语言模型的主要和最关注的功能，解决在生成过程中的遗忘问题是非常重要的。本文针对在生成过程中缓解提示遗忘的问题，提出了一种通过合成梯度教导模型在生成过程中记住提示的架构。为了强制模型记住提示，我们导出了编码提示的状态，然后将其转化为模型参数的修改。",
    "tldr": "本文解决了RNN-like语言模型在生成过程中遗忘提示的问题，提出了使用合成梯度教导模型记住提示的架构。",
    "en_tdlr": "This paper addresses the issue of prompt forgetting in RNN-like language models during generation and proposes an architecture using synthetic gradients to teach the model to remember the prompts."
}