{
    "title": "The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models",
    "abstract": "arXiv:2311.05928v2 Announce Type: replace-cross  Abstract: In this study, we present an investigation into the anisotropy dynamics and intrinsic dimension of embeddings in transformer architectures, focusing on the dichotomy between encoders and decoders. Our findings reveal that the anisotropy profile in transformer decoders exhibits a distinct bell-shaped curve, with the highest anisotropy concentrations in the middle layers. This pattern diverges from the more uniformly distributed anisotropy observed in encoders. In addition, we found that the intrinsic dimension of embeddings increases in the initial phases of training, indicating an expansion into higher-dimensional space. Which is then followed by a compression phase towards the end of training with dimensionality decrease, suggesting a refinement into more compact representations. Our results provide fresh insights to the understanding of encoders and decoders embedding properties.",
    "link": "https://arxiv.org/abs/2311.05928",
    "context": "Title: The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models\nAbstract: arXiv:2311.05928v2 Announce Type: replace-cross  Abstract: In this study, we present an investigation into the anisotropy dynamics and intrinsic dimension of embeddings in transformer architectures, focusing on the dichotomy between encoders and decoders. Our findings reveal that the anisotropy profile in transformer decoders exhibits a distinct bell-shaped curve, with the highest anisotropy concentrations in the middle layers. This pattern diverges from the more uniformly distributed anisotropy observed in encoders. In addition, we found that the intrinsic dimension of embeddings increases in the initial phases of training, indicating an expansion into higher-dimensional space. Which is then followed by a compression phase towards the end of training with dimensionality decrease, suggesting a refinement into more compact representations. Our results provide fresh insights to the understanding of encoders and decoders embedding properties.",
    "path": "papers/23/11/2311.05928.json",
    "total_tokens": 921,
    "translated_title": "学习的形状：基于Transformer模型的各向异性和内在维度研究",
    "translated_abstract": "在这项研究中，我们针对Transformer架构中嵌入的各向异性动态和内在维度展开调查，重点关注编码器和解码器之间的二分法。我们的研究结果显示，Transformer解码器中的各向异性配置呈现出明显的钟状曲线，具有最高的各向异性浓度在中间层。这种模式与编码器中观察到的更均匀分布的各向异性有所不同。此外，我们发现嵌入的内在维度在训练的初始阶段增加，表明向更高维空间的扩展。然后在训练末尾出现向更低维度的压缩阶段，暗示着对更紧凑表示的改进。我们的结果为理解编码器和解码器嵌入属性提供了新的见解。",
    "tldr": "本研究揭示了Transformer解码器中的各向异性呈钟状曲线，最高各向异性浓度在中间层，与编码器中更均匀分布的各向异性不同，并发现嵌入的内在维度在训练初期增加，随后在训练末期出现压缩，表明更紧凑的表示形式。",
    "en_tdlr": "This study reveals a bell-shaped anisotropy profile in Transformer decoders, with the highest anisotropy concentrations in the middle layers, differing from the more uniformly distributed anisotropy in encoders, and found that the intrinsic dimension of embeddings increases in the initial phases of training, followed by a compression phase towards the end of training, indicating a refinement into more compact representations."
}