{
    "title": "A Baseline Analysis of Reward Models' Ability To Accurately Analyze Foundation Models Under Distribution Shift. (arXiv:2311.14743v7 [cs.CL] UPDATED)",
    "abstract": "Foundation models, specifically Large Language Models (LLMs), have lately gained wide-spread attention and adoption. Reinforcement Learning with Human Feedback (RLHF) involves training a reward model to capture desired behaviors, which is then used to align LLM's. These reward models are additionally used at inference-time to estimate LLM responses' adherence to those desired behaviors. However, there is little work measuring how robust these reward models are to distribution shifts. In this work, we evaluate how reward model performance measured via accuracy and calibration (i.e. alignment between accuracy and confidence) - is affected by distribution shift. We show novel calibration patterns and accuracy drops due to OOD prompts and responses, and that the reward model is more sensitive to shifts in responses than prompts. Additionally, we adapt an OOD detection technique commonly used in classification to the reward model setting to detect these distribution shifts in prompts and ",
    "link": "http://arxiv.org/abs/2311.14743",
    "context": "Title: A Baseline Analysis of Reward Models' Ability To Accurately Analyze Foundation Models Under Distribution Shift. (arXiv:2311.14743v7 [cs.CL] UPDATED)\nAbstract: Foundation models, specifically Large Language Models (LLMs), have lately gained wide-spread attention and adoption. Reinforcement Learning with Human Feedback (RLHF) involves training a reward model to capture desired behaviors, which is then used to align LLM's. These reward models are additionally used at inference-time to estimate LLM responses' adherence to those desired behaviors. However, there is little work measuring how robust these reward models are to distribution shifts. In this work, we evaluate how reward model performance measured via accuracy and calibration (i.e. alignment between accuracy and confidence) - is affected by distribution shift. We show novel calibration patterns and accuracy drops due to OOD prompts and responses, and that the reward model is more sensitive to shifts in responses than prompts. Additionally, we adapt an OOD detection technique commonly used in classification to the reward model setting to detect these distribution shifts in prompts and ",
    "path": "papers/23/11/2311.14743.json",
    "total_tokens": 939,
    "translated_title": "基准分析奖励模型在分布转移下准确分析基础模型的能力的能力",
    "translated_abstract": "最近，基础模型，特别是大型语言模型（LLM），引起了广泛的关注和应用。利用人类反馈进行强化学习（RLHF）包括训练奖励模型来捕捉期望的行为，然后用于对齐LLM。这些奖励模型还在推断时用于估计LLM响应与期望行为的一致性。然而，很少有工作来衡量这些奖励模型在分布转移下的鲁棒性。在这项工作中，我们评估了通过准确性和校准度（即准确性和信心的匹配程度）衡量的奖励模型性能如何受到分布转移的影响。我们展示了由于OOD提示和响应而产生的新型校准模式和准确性下降，并且发现奖励模型对响应的转移比提示更敏感。此外，我们还将常用于分类的OOD检测技术适应到奖励模型设置中，以检测这些提示和响应的分布转移。",
    "tldr": "本论文基于奖励模型的准确性和校准度评估了基础模型在分布转移下的性能。实验结果显示奖励模型对于提示和响应的转移具有不同的敏感性，并呈现出新颖的校准模式和准确性下降。同时，将常用的OOD检测技术引入到奖励模型设置中，用于检测分布转移。"
}