{
    "title": "Deep Hashing via Householder Quantization. (arXiv:2311.04207v3 [cs.CV] UPDATED)",
    "abstract": "Hashing is at the heart of large-scale image similarity search, and recent methods have been substantially improved through deep learning techniques. Such algorithms typically learn continuous embeddings of the data. To avoid a subsequent costly binarization step, a common solution is to employ loss functions that combine a similarity learning term (to ensure similar images are grouped to nearby embeddings) and a quantization penalty term (to ensure that the embedding entries are close to binarized entries, e.g., -1 or 1). Still, the interaction between these two terms can make learning harder and the embeddings worse. We propose an alternative quantization strategy that decomposes the learning problem in two stages: first, perform similarity learning over the embedding space with no quantization; second, find an optimal orthogonal transformation of the embeddings so each coordinate of the embedding is close to its sign, and then quantize the transformed embedding through the sign func",
    "link": "http://arxiv.org/abs/2311.04207",
    "context": "Title: Deep Hashing via Householder Quantization. (arXiv:2311.04207v3 [cs.CV] UPDATED)\nAbstract: Hashing is at the heart of large-scale image similarity search, and recent methods have been substantially improved through deep learning techniques. Such algorithms typically learn continuous embeddings of the data. To avoid a subsequent costly binarization step, a common solution is to employ loss functions that combine a similarity learning term (to ensure similar images are grouped to nearby embeddings) and a quantization penalty term (to ensure that the embedding entries are close to binarized entries, e.g., -1 or 1). Still, the interaction between these two terms can make learning harder and the embeddings worse. We propose an alternative quantization strategy that decomposes the learning problem in two stages: first, perform similarity learning over the embedding space with no quantization; second, find an optimal orthogonal transformation of the embeddings so each coordinate of the embedding is close to its sign, and then quantize the transformed embedding through the sign func",
    "path": "papers/23/11/2311.04207.json",
    "total_tokens": 903,
    "translated_title": "基于 Householder 量化的深度哈希",
    "translated_abstract": "哈希是大规模图像相似性搜索的核心，最近的方法通过深度学习技术实现了显著的改进。这些算法通常学习数据的连续嵌入。为了避免后续昂贵的二值化步骤，常见的解决方案是采用将相似度学习项（确保相似的图像被分组到附近的嵌入中）和量化惩罚项（确保嵌入项接近二值化项，例如-1或1）结合的损失函数。然而，这两个项之间的相互作用可能使学习变得更困难，并且嵌入结果更差。我们提出了一种替代的量化策略，将学习问题分解为两个阶段：首先，不进行量化，在嵌入空间上进行相似度学习；其次，找到嵌入的最佳正交变换，使得嵌入的每个坐标都接近其符号，然后通过符号函数对变换后的嵌入进行量化。",
    "tldr": "基于 Householder 量化的深度哈希提出了一种替代的量化策略，通过将学习问题分解为两个阶段来改善哈希方法中相似度学习和量化之间的交互，从而提高嵌入的质量。",
    "en_tdlr": "Deep Hashing via Householder Quantization proposes an alternative quantization strategy that improves the interaction between similarity learning and quantization in hashing methods by decomposing the learning problem into two stages, leading to higher quality embeddings."
}