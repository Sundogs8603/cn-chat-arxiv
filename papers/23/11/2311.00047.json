{
    "title": "Grounding Visual Illusions in Language: Do Vision-Language Models Perceive Illusions Like Humans?. (arXiv:2311.00047v1 [cs.AI])",
    "abstract": "Vision-Language Models (VLMs) are trained on vast amounts of data captured by humans emulating our understanding of the world. However, known as visual illusions, human's perception of reality isn't always faithful to the physical world. This raises a key question: do VLMs have the similar kind of illusions as humans do, or do they faithfully learn to represent reality? To investigate this question, we build a dataset containing five types of visual illusions and formulate four tasks to examine visual illusions in state-of-the-art VLMs. Our findings have shown that although the overall alignment is low, larger models are closer to human perception and more susceptible to visual illusions. Our dataset and initial findings will promote a better understanding of visual illusions in humans and machines and provide a stepping stone for future computational models that can better align humans and machines in perceiving and communicating about the shared visual world. The code and data are av",
    "link": "http://arxiv.org/abs/2311.00047",
    "context": "Title: Grounding Visual Illusions in Language: Do Vision-Language Models Perceive Illusions Like Humans?. (arXiv:2311.00047v1 [cs.AI])\nAbstract: Vision-Language Models (VLMs) are trained on vast amounts of data captured by humans emulating our understanding of the world. However, known as visual illusions, human's perception of reality isn't always faithful to the physical world. This raises a key question: do VLMs have the similar kind of illusions as humans do, or do they faithfully learn to represent reality? To investigate this question, we build a dataset containing five types of visual illusions and formulate four tasks to examine visual illusions in state-of-the-art VLMs. Our findings have shown that although the overall alignment is low, larger models are closer to human perception and more susceptible to visual illusions. Our dataset and initial findings will promote a better understanding of visual illusions in humans and machines and provide a stepping stone for future computational models that can better align humans and machines in perceiving and communicating about the shared visual world. The code and data are av",
    "path": "papers/23/11/2311.00047.json",
    "total_tokens": 932,
    "translated_title": "用语言来地基视觉幻觉：视觉-语言模型是否像人类一样感知幻觉？",
    "translated_abstract": "视觉-语言模型（VLMs）是在人类理解世界的模拟下，通过大量的数据训练得到的。然而，人类对现实的感知并不总是对物理世界的忠实呈现，被称为视觉幻觉。这引发了一个关键问题：VLMs是否和人类一样有幻觉,或者它们是否忠实地学习了对现实的表达？为了调查这个问题，我们构建了一个包含五种类型的视觉幻觉的数据集，并制定了四个任务来研究最先进的VLMs中的视觉幻觉。我们的研究结果显示，尽管整体对齐性较低，但更大规模的模型更接近人类的感知并更容易受到视觉幻觉的影响。我们的数据集和初步结果将促进对人类和机器在感知和交流共享视觉世界方面的更好理解，并为未来能更好地对齐人类和机器在感知和交流共享视觉世界方面的计算模型提供了一个起点。",
    "tldr": "通过构建一个包含五种视觉幻觉的数据集，研究发现，尽管整体对齐性较低，但更大规模的视觉-语言模型更接近人类的感知并更容易受到视觉幻觉的影响。",
    "en_tdlr": "By building a dataset of five visual illusions, this study found that larger vision-language models have closer alignment with human perception and are more susceptible to visual illusions."
}