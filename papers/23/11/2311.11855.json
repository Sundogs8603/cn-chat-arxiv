{
    "title": "Evil Geniuses: Delving into the Safety of LLM-based Agents",
    "abstract": "Rapid advancements in large language models (LLMs) have revitalized in LLM-based agents, exhibiting impressive human-like behaviors and cooperative capabilities in various scenarios. However, these agents also bring some exclusive risks, stemming from the complexity of interaction environments and the usability of tools. This paper delves into the safety of LLM-based agents from three perspectives: agent quantity, role definition, and attack level. Specifically, we initially propose to employ a template-based attack strategy on LLM-based agents to find the influence of agent quantity. In addition, to address interaction environment and role specificity issues, we introduce Evil Geniuses (EG), an effective attack method that autonomously generates prompts related to the original role to examine the impact across various role definitions and attack levels. EG leverages Red-Blue exercises, significantly improving the generated prompt aggressiveness and similarity to original roles. Our ev",
    "link": "https://rss.arxiv.org/abs/2311.11855",
    "context": "Title: Evil Geniuses: Delving into the Safety of LLM-based Agents\nAbstract: Rapid advancements in large language models (LLMs) have revitalized in LLM-based agents, exhibiting impressive human-like behaviors and cooperative capabilities in various scenarios. However, these agents also bring some exclusive risks, stemming from the complexity of interaction environments and the usability of tools. This paper delves into the safety of LLM-based agents from three perspectives: agent quantity, role definition, and attack level. Specifically, we initially propose to employ a template-based attack strategy on LLM-based agents to find the influence of agent quantity. In addition, to address interaction environment and role specificity issues, we introduce Evil Geniuses (EG), an effective attack method that autonomously generates prompts related to the original role to examine the impact across various role definitions and attack levels. EG leverages Red-Blue exercises, significantly improving the generated prompt aggressiveness and similarity to original roles. Our ev",
    "path": "papers/23/11/2311.11855.json",
    "total_tokens": 862,
    "translated_title": "恶意天才：探索基于LLM的智能体的安全性",
    "translated_abstract": "大型语言模型(LLM)的快速发展使得基于LLM的智能体在各种场景中展示出令人印象深刻的类人行为和合作能力，但这些智能体也带来了一些独特的风险，源于交互环境的复杂性和工具的可用性。本文从智能体数量、角色定义和攻击水平三个角度深入探讨了基于LLM的智能体的安全性。具体来说，我们首先提出了一种基于模板的攻击策略，用于测试智能体数量的影响。此外，为了解决交互环境和角色特异性问题，我们引入了“恶意天才”(EG)，这是一种有效的攻击方法，可以自动化生成与原始角色相关的提示，以检查在各种角色定义和攻击水平下的影响。恶意天才利用红蓝对抗训练，显著提高了生成的提示的攻击性和与原始角色的相似性。",
    "tldr": "本文研究了基于LLM的智能体的安全性，从智能体数量、角色定义和攻击水平三个角度进行了探讨，并提出了一种有效的攻击方法“恶意天才”，通过自动生成与原始角色相关的提示，来测试在不同角色定义和攻击水平下的影响。"
}