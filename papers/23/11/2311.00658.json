{
    "title": "Explicit Morphological Knowledge Improves Pre-training of Language Models for Hebrew. (arXiv:2311.00658v1 [cs.CL])",
    "abstract": "Pre-trained language models (PLMs) have shown remarkable successes in acquiring a wide range of linguistic knowledge, relying solely on self-supervised training on text streams. Nevertheless, the effectiveness of this language-agnostic approach has been frequently questioned for its sub-optimal performance when applied to morphologically-rich languages (MRLs). We investigate the hypothesis that incorporating explicit morphological knowledge in the pre-training phase can improve the performance of PLMs for MRLs. We propose various morphologically driven tokenization methods enabling the model to leverage morphological cues beyond raw text. We pre-train multiple language models utilizing the different methods and evaluate them on Hebrew, a language with complex and highly ambiguous morphology. Our experiments show that morphologically driven tokenization demonstrates improved results compared to a standard language-agnostic tokenization, on a benchmark of both semantic and morphologic ta",
    "link": "http://arxiv.org/abs/2311.00658",
    "context": "Title: Explicit Morphological Knowledge Improves Pre-training of Language Models for Hebrew. (arXiv:2311.00658v1 [cs.CL])\nAbstract: Pre-trained language models (PLMs) have shown remarkable successes in acquiring a wide range of linguistic knowledge, relying solely on self-supervised training on text streams. Nevertheless, the effectiveness of this language-agnostic approach has been frequently questioned for its sub-optimal performance when applied to morphologically-rich languages (MRLs). We investigate the hypothesis that incorporating explicit morphological knowledge in the pre-training phase can improve the performance of PLMs for MRLs. We propose various morphologically driven tokenization methods enabling the model to leverage morphological cues beyond raw text. We pre-train multiple language models utilizing the different methods and evaluate them on Hebrew, a language with complex and highly ambiguous morphology. Our experiments show that morphologically driven tokenization demonstrates improved results compared to a standard language-agnostic tokenization, on a benchmark of both semantic and morphologic ta",
    "path": "papers/23/11/2311.00658.json",
    "total_tokens": 807,
    "translated_title": "预训练语言模型中加入明确的形态学知识提高了希伯来语的性能",
    "translated_abstract": "预训练语言模型（PLM）在自我监督训练的文本流上取得了显著的成功，获得了广泛的语言知识。然而，当应用于形态丰富的语言时，这种语言不可知的方法的有效性经常受到质疑。我们研究了在预训练阶段加入明确的形态学知识能否提高PLM在形态丰富的语言中的性能的假设。我们提出了多种基于形态学的分词方法，使模型能够利用除了原始文本之外的形态学线索。我们使用不同的方法对多个语言模型进行预训练，并在复杂且高度模糊的希伯来语上进行评估。实验结果表明，基于形态学的分词方法在语义和形态学评估标准上相比标准语言不可知的分词方法有改进的结果。",
    "tldr": "加入明确的形态学知识在预训练阶段可以提高希伯来语的语言模型性能。"
}