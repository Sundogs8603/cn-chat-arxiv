{
    "title": "PEMA: An Offsite-Tunable Plug-in External Memory Adaptation for Language Models",
    "abstract": "arXiv:2311.08590v2 Announce Type: replace  Abstract: Pre-trained language models (PLMs) show impressive performance in various downstream NLP tasks. However, pre-training large language models demands substantial memory and training compute. Furthermore, due to the substantial resources required, many PLM weights are confidential. Consequently, users are compelled to share their data with model owners for fine-tuning specific tasks. To overcome the limitations, we introduce Plug-in External Memory Adaptation (PEMA), a Parameter-Efficient Fine-Tuning (PEFT) method, enabling PLM fine-tuning without requiring access to all the weights. PEMA integrates with context representations from test data during inference to perform downstream tasks. It uses external memory to store PLM-generated context representations mapped with target tokens. Our method utilizes weight matrices of LoRA-like bottlenecked adapter in the PLM's final layer to enhance efficiency. Our approach also includes Gradual Un",
    "link": "https://arxiv.org/abs/2311.08590",
    "context": "Title: PEMA: An Offsite-Tunable Plug-in External Memory Adaptation for Language Models\nAbstract: arXiv:2311.08590v2 Announce Type: replace  Abstract: Pre-trained language models (PLMs) show impressive performance in various downstream NLP tasks. However, pre-training large language models demands substantial memory and training compute. Furthermore, due to the substantial resources required, many PLM weights are confidential. Consequently, users are compelled to share their data with model owners for fine-tuning specific tasks. To overcome the limitations, we introduce Plug-in External Memory Adaptation (PEMA), a Parameter-Efficient Fine-Tuning (PEFT) method, enabling PLM fine-tuning without requiring access to all the weights. PEMA integrates with context representations from test data during inference to perform downstream tasks. It uses external memory to store PLM-generated context representations mapped with target tokens. Our method utilizes weight matrices of LoRA-like bottlenecked adapter in the PLM's final layer to enhance efficiency. Our approach also includes Gradual Un",
    "path": "papers/23/11/2311.08590.json",
    "total_tokens": 869,
    "translated_title": "PEMA：一种可在离线调整的外部插件内存自适应语言模型",
    "translated_abstract": "预训练语言模型（PLM）在各种下游自然语言处理任务中表现出色。然而，训练大型语言模型需要大量内存和计算资源。由于资源需求巨大，许多PLM权重是机密的，用户被迫将其数据与模型所有者共享，以便为特定任务进行微调。为了克服这些限制，我们引入了插件外部内存自适应（PEMA），一种参数有效的微调（PEFT）方法，实现了在不需要访问所有权重的情况下对PLM进行微调。PEMA在推理期间集成了来自测试数据的上下文表示以执行下游任务。它使用外部内存存储由PLM生成的上下文表示与目标标记相映射。我们的方法利用了PLM最终层中类似LoRA的瓶颈适配器的权重矩阵来提高效率。",
    "tldr": "PEMA是一种参数有效的微调方法，通过插件外部内存自适应实现了对预训练语言模型的微调，绕过了对所有权重的访问需求，同时利用外部内存和适配器权重矩阵来提高效率。",
    "en_tdlr": "PEMA is a parameter-efficient fine-tuning method that achieves fine-tuning of pre-trained language models through plug-in external memory adaptation, bypassing the need for access to all weights, while using external memory and adapter weight matrices to enhance efficiency."
}