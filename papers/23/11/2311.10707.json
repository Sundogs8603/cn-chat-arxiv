{
    "title": "Multimodal Representation Learning by Alternating Unimodal Adaptation",
    "abstract": "arXiv:2311.10707v2 Announce Type: replace  Abstract: Multimodal learning, which integrates data from diverse sensory modes, plays a pivotal role in artificial intelligence. However, existing multimodal learning methods often struggle with challenges where some modalities appear more dominant than others during multimodal learning, resulting in suboptimal performance. To address this challenge, we propose MLA (Multimodal Learning with Alternating Unimodal Adaptation). MLA reframes the conventional joint multimodal learning process by transforming it into an alternating unimodal learning process, thereby minimizing interference between modalities. Simultaneously, it captures cross-modal interactions through a shared head, which undergoes continuous optimization across different modalities. This optimization process is controlled by a gradient modification mechanism to prevent the shared head from losing previously acquired information. During the inference phase, MLA utilizes a test-time",
    "link": "https://arxiv.org/abs/2311.10707",
    "context": "Title: Multimodal Representation Learning by Alternating Unimodal Adaptation\nAbstract: arXiv:2311.10707v2 Announce Type: replace  Abstract: Multimodal learning, which integrates data from diverse sensory modes, plays a pivotal role in artificial intelligence. However, existing multimodal learning methods often struggle with challenges where some modalities appear more dominant than others during multimodal learning, resulting in suboptimal performance. To address this challenge, we propose MLA (Multimodal Learning with Alternating Unimodal Adaptation). MLA reframes the conventional joint multimodal learning process by transforming it into an alternating unimodal learning process, thereby minimizing interference between modalities. Simultaneously, it captures cross-modal interactions through a shared head, which undergoes continuous optimization across different modalities. This optimization process is controlled by a gradient modification mechanism to prevent the shared head from losing previously acquired information. During the inference phase, MLA utilizes a test-time",
    "path": "papers/23/11/2311.10707.json",
    "total_tokens": 778,
    "translated_title": "通过交替单模态适应进行多模态表示学习",
    "translated_abstract": "多模态学习在人工智能领域中起着关键作用，然而，现有的多模态学习方法往往在某些感知模态在学习过程中表现更显著时面临挑战，导致性能不佳。为了解决这一挑战，我们提出了MLA（具有交替单模态适应的多模态学习）。MLA通过将传统的联合多模态学习过程转化为交替单模态学习过程来重新定义，从而最小化模态之间的干扰。同时，它通过一个共享头部捕获跨模态交互，该头部在不同模态间经历持续优化。这种优化过程由梯度修改机制控制，以防止共享头部丢失先前获取的信息。在推理阶段，MLA利用一个测试时间",
    "tldr": "提出了一种通过交替单模态适应来进行多模态表示学习的方法，旨在最小化干扰并捕获跨模态交互",
    "en_tdlr": "Propose a method for multimodal representation learning through alternating unimodal adaptation to minimize interference and capture cross-modal interactions."
}