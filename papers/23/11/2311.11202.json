{
    "title": "Unmasking and Improving Data Credibility: A Study with Datasets for Training Harmless Language Models",
    "abstract": "arXiv:2311.11202v2 Announce Type: replace-cross  Abstract: Language models have shown promise in various tasks but can be affected by undesired data during training, fine-tuning, or alignment. For example, if some unsafe conversations are wrongly annotated as safe ones, the model fine-tuned on these samples may be harmful. Therefore, the correctness of annotations, i.e., the credibility of the dataset, is important. This study focuses on the credibility of real-world datasets, including the popular benchmarks Jigsaw Civil Comments, Anthropic Harmless & Red Team, PKU BeaverTails & SafeRLHF, that can be used for training a harmless language model. Given the cost and difficulty of cleaning these datasets by humans, we introduce a systematic framework for evaluating the credibility of datasets, identifying label errors, and evaluating the influence of noisy labels in the curated language data, specifically focusing on unsafe comments and conversation classification. With the framework, we ",
    "link": "https://arxiv.org/abs/2311.11202",
    "context": "Title: Unmasking and Improving Data Credibility: A Study with Datasets for Training Harmless Language Models\nAbstract: arXiv:2311.11202v2 Announce Type: replace-cross  Abstract: Language models have shown promise in various tasks but can be affected by undesired data during training, fine-tuning, or alignment. For example, if some unsafe conversations are wrongly annotated as safe ones, the model fine-tuned on these samples may be harmful. Therefore, the correctness of annotations, i.e., the credibility of the dataset, is important. This study focuses on the credibility of real-world datasets, including the popular benchmarks Jigsaw Civil Comments, Anthropic Harmless & Red Team, PKU BeaverTails & SafeRLHF, that can be used for training a harmless language model. Given the cost and difficulty of cleaning these datasets by humans, we introduce a systematic framework for evaluating the credibility of datasets, identifying label errors, and evaluating the influence of noisy labels in the curated language data, specifically focusing on unsafe comments and conversation classification. With the framework, we ",
    "path": "papers/23/11/2311.11202.json",
    "total_tokens": 888,
    "translated_title": "揭示和提高数据可信度：训练无害语言模型的数据集研究",
    "translated_abstract": "arXiv:2311.11202v2宣布类型：替换-跨文档摘要：语言模型在各种任务中显示出潜力，但在训练、微调或对齐过程中可能受到不希望的数据的影响。因此，注解的正确性，即数据集的可信度，变得非常重要。本研究聚焦于真实世界数据集的可信度，其中包括可用于训练无害语言模型的流行基准数据集，如Jigsaw Civil Comments、Anthropic Harmless和Red Team、PKU BeaverTails和SafeRLHF。考虑到人们清洗这些数据集的成本和难度，我们引入了一个系统化框架，用于评估数据集的可信度，识别标签错误，并评估策划语言数据中嘈杂标签的影响，特别关注不安全评论和对话分类。",
    "tldr": "本研究专注于真实世界数据集的可信度，提出了一个系统化框架，用于评估数据集的可信度，识别标签错误，并评估嘈杂标签对不安全评论和对话分类的影响，以提高训练无害语言模型的质量。",
    "en_tdlr": "This study focuses on the credibility of real-world datasets and introduces a systematic framework for evaluating dataset credibility, identifying label errors, and assessing the impact of noisy labels on unsafe comments and conversation classification to enhance the quality of training harmless language models."
}