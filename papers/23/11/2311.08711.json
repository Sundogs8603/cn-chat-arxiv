{
    "title": "PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning",
    "abstract": "Instruction tuning has remarkably advanced large language models (LLMs) in understanding and responding to diverse human instructions. Despite the success in high-resource languages, its application in lower-resource ones faces challenges due to the imbalanced foundational abilities of LLMs across different languages, stemming from the uneven language distribution in their pre-training data. To tackle this issue, we propose pivot language guided generation (PLUG), an approach that utilizes a high-resource language, primarily English, as the pivot to enhance instruction tuning in lower-resource languages. It trains the model to first process instructions in the pivot language, and then produce responses in the target language. To evaluate our approach, we introduce a benchmark, X-AlpacaEval, of instructions in 4 languages (Chinese, Korean, Italian, and Spanish), each annotated by professional translators. Our approach demonstrates a significant improvement in the instruction-following a",
    "link": "https://arxiv.org/abs/2311.08711",
    "context": "Title: PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning\nAbstract: Instruction tuning has remarkably advanced large language models (LLMs) in understanding and responding to diverse human instructions. Despite the success in high-resource languages, its application in lower-resource ones faces challenges due to the imbalanced foundational abilities of LLMs across different languages, stemming from the uneven language distribution in their pre-training data. To tackle this issue, we propose pivot language guided generation (PLUG), an approach that utilizes a high-resource language, primarily English, as the pivot to enhance instruction tuning in lower-resource languages. It trains the model to first process instructions in the pivot language, and then produce responses in the target language. To evaluate our approach, we introduce a benchmark, X-AlpacaEval, of instructions in 4 languages (Chinese, Korean, Italian, and Spanish), each annotated by professional translators. Our approach demonstrates a significant improvement in the instruction-following a",
    "path": "papers/23/11/2311.08711.json",
    "total_tokens": 862,
    "translated_title": "PLUG: 跨语言指令调优中的枢纽语言优势利用",
    "translated_abstract": "指令调优在大型语言模型（LLMs）在理解和回应各种人类指令方面取得了显著进展。尽管在高资源语言中取得了成功，但其在低资源语言中的应用面临挑战，原因是LLMs在不同语言上的基础能力不平衡，这源于在它们的预训练数据中语言分布的不均衡。为了解决这个问题，我们提出了枢纽语言引导生成（PLUG）的方法，该方法利用高资源语言（主要是英语）作为枢纽语言，增强了低资源语言中的指令调优。它训练模型首先处理枢纽语言中的指令，然后在目标语言中生成响应。为了评估我们的方法，我们引入了一个基准数据集X-AlpacaEval，其中包含了4种语言（中文、韩文、意大利文和西班牙文）的指令，并由专业翻译人员进行了注释。我们的方法在指令跟随中显示了显著的改进。",
    "tldr": "提出了一种利用枢纽语言进行指令调优的方法，该方法在低资源语言中取得了显著改进，并通过引入一个基准数据集进行了评估。"
}