{
    "title": "On the Second-Order Convergence of Biased Policy Gradient Algorithms",
    "abstract": "Since the objective functions of reinforcement learning problems are typically highly nonconvex, it is desirable that policy gradient, the most popular algorithm, escapes saddle points and arrives at second-order stationary points. Existing results only consider vanilla policy gradient algorithms with unbiased gradient estimators, but practical implementations under the infinite-horizon discounted reward setting are biased due to finite-horizon sampling. Moreover, actor-critic methods, whose second-order convergence has not yet been established, are also biased due to the critic approximation of the value function. We provide a novel second-order analysis of biased policy gradient methods, including the vanilla gradient estimator computed from Monte-Carlo sampling of trajectories as well as the double-loop actor-critic algorithm, where in the inner loop the critic improves the approximation of the value function via TD(0) learning. Separately, we also establish the convergence of TD(0)",
    "link": "https://arxiv.org/abs/2311.02546",
    "context": "Title: On the Second-Order Convergence of Biased Policy Gradient Algorithms\nAbstract: Since the objective functions of reinforcement learning problems are typically highly nonconvex, it is desirable that policy gradient, the most popular algorithm, escapes saddle points and arrives at second-order stationary points. Existing results only consider vanilla policy gradient algorithms with unbiased gradient estimators, but practical implementations under the infinite-horizon discounted reward setting are biased due to finite-horizon sampling. Moreover, actor-critic methods, whose second-order convergence has not yet been established, are also biased due to the critic approximation of the value function. We provide a novel second-order analysis of biased policy gradient methods, including the vanilla gradient estimator computed from Monte-Carlo sampling of trajectories as well as the double-loop actor-critic algorithm, where in the inner loop the critic improves the approximation of the value function via TD(0) learning. Separately, we also establish the convergence of TD(0)",
    "path": "papers/23/11/2311.02546.json",
    "total_tokens": 929,
    "translated_title": "关于偏置策略梯度算法的二阶收敛性研究",
    "translated_abstract": "由于强化学习问题的目标函数通常是高度非凸的，因此希望策略梯度算法能够脱离鞍点并达到二阶稳定点。现有的结果只考虑了带有无偏梯度估计器的普通策略梯度算法，但在无限时间折扣回报设置下，实际实现是有偏的，因为有限时间采样。此外，由于评论家对价值函数的逼近，评论家-演员方法的二阶收敛性也未被证实。我们提供了对有偏策略梯度方法的新颖的二阶分析，包括通过蒙特卡洛轨迹采样计算得到的普通梯度估计器，以及双循环评论家-演员算法，在内循环中，评论家通过TD(0)学习改进了对价值函数的逼近。另外，我们还证明了TD(0)的收敛性。",
    "tldr": "该论文研究了偏置策略梯度算法的二阶收敛性，包括基于蒙特卡洛轨迹采样的普通梯度估计器和基于双循环评论家-演员算法的演员-评论家方法。实现在实际应用中的偏置主要来自于有限时间采样和对价值函数的逼近。",
    "en_tdlr": "This paper investigates the second-order convergence of biased policy gradient algorithms, including the vanilla gradient estimator computed from Monte-Carlo sampling and the actor-critic method based on double-loop architecture. The bias in practical implementations mainly arises from finite-horizon sampling and the approximation of the value function."
}