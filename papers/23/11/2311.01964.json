{
    "title": "Don't Make Your LLM an Evaluation Benchmark Cheater. (arXiv:2311.01964v1 [cs.CL])",
    "abstract": "Large language models~(LLMs) have greatly advanced the frontiers of artificial intelligence, attaining remarkable improvement in model capacity. To assess the model performance, a typical approach is to construct evaluation benchmarks for measuring the ability level of LLMs in different aspects. Despite that a number of high-quality benchmarks have been released, the concerns about the appropriate use of these benchmarks and the fair comparison of different models are increasingly growing. Considering these concerns, in this paper, we discuss the potential risk and impact of inappropriately using evaluation benchmarks and misleadingly interpreting the evaluation results. Specially, we focus on a special issue that would lead to inappropriate evaluation, \\ie \\emph{benchmark leakage}, referring that the data related to evaluation sets is occasionally used for model training. This phenomenon now becomes more common since pre-training data is often prepared ahead of model test. We conduct ",
    "link": "http://arxiv.org/abs/2311.01964",
    "context": "Title: Don't Make Your LLM an Evaluation Benchmark Cheater. (arXiv:2311.01964v1 [cs.CL])\nAbstract: Large language models~(LLMs) have greatly advanced the frontiers of artificial intelligence, attaining remarkable improvement in model capacity. To assess the model performance, a typical approach is to construct evaluation benchmarks for measuring the ability level of LLMs in different aspects. Despite that a number of high-quality benchmarks have been released, the concerns about the appropriate use of these benchmarks and the fair comparison of different models are increasingly growing. Considering these concerns, in this paper, we discuss the potential risk and impact of inappropriately using evaluation benchmarks and misleadingly interpreting the evaluation results. Specially, we focus on a special issue that would lead to inappropriate evaluation, \\ie \\emph{benchmark leakage}, referring that the data related to evaluation sets is occasionally used for model training. This phenomenon now becomes more common since pre-training data is often prepared ahead of model test. We conduct ",
    "path": "papers/23/11/2311.01964.json",
    "total_tokens": 837,
    "translated_title": "不要让你的LLM成为一个评估基准欺骗者",
    "translated_abstract": "大型语言模型（LLMs）已经极大地推动了人工智能的前沿，实现了模型能力的显著提升。为了评估模型性能，通常的做法是构建评估基准，以测量LLMs在不同方面的能力水平。尽管已经发布了许多高质量的基准，但对于这些基准的合理使用和不同模型的公平比较的关注越来越多。鉴于这些关注，本文讨论了不恰当使用评估基准和误导性解释评估结果的潜在风险和影响。特别地，我们关注了一个特殊问题，即导致不恰当评估的\\emph{基准泄漏}，即评估集相关的数据偶尔被用于模型训练。由于预训练数据通常是在模型测试之前准备的，因此这种现象变得更加普遍。",
    "tldr": "本文讨论了不恰当使用评估基准和误导性解释评估结果的潜在风险和影响，特别关注了基准泄漏现象。"
}