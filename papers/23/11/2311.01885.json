{
    "title": "Domain Randomization via Entropy Maximization. (arXiv:2311.01885v1 [cs.LG])",
    "abstract": "Varying dynamics parameters in simulation is a popular Domain Randomization (DR) approach for overcoming the reality gap in Reinforcement Learning (RL). Nevertheless, DR heavily hinges on the choice of the sampling distribution of the dynamics parameters, since high variability is crucial to regularize the agent's behavior but notoriously leads to overly conservative policies when randomizing excessively. In this paper, we propose a novel approach to address sim-to-real transfer, which automatically shapes dynamics distributions during training in simulation without requiring real-world data. We introduce DOmain RAndomization via Entropy MaximizatiON (DORAEMON), a constrained optimization problem that directly maximizes the entropy of the training distribution while retaining generalization capabilities. In achieving this, DORAEMON gradually increases the diversity of sampled dynamics parameters as long as the probability of success of the current policy is sufficiently high. We empiri",
    "link": "http://arxiv.org/abs/2311.01885",
    "context": "Title: Domain Randomization via Entropy Maximization. (arXiv:2311.01885v1 [cs.LG])\nAbstract: Varying dynamics parameters in simulation is a popular Domain Randomization (DR) approach for overcoming the reality gap in Reinforcement Learning (RL). Nevertheless, DR heavily hinges on the choice of the sampling distribution of the dynamics parameters, since high variability is crucial to regularize the agent's behavior but notoriously leads to overly conservative policies when randomizing excessively. In this paper, we propose a novel approach to address sim-to-real transfer, which automatically shapes dynamics distributions during training in simulation without requiring real-world data. We introduce DOmain RAndomization via Entropy MaximizatiON (DORAEMON), a constrained optimization problem that directly maximizes the entropy of the training distribution while retaining generalization capabilities. In achieving this, DORAEMON gradually increases the diversity of sampled dynamics parameters as long as the probability of success of the current policy is sufficiently high. We empiri",
    "path": "papers/23/11/2311.01885.json",
    "total_tokens": 898,
    "translated_title": "通过熵最大化进行领域随机化",
    "translated_abstract": "在强化学习中，通过改变模拟中的动力学参数是一种流行的领域随机化方法，用于克服现实差距。然而，领域随机化在很大程度上依赖于动力学参数的抽样分布的选择，因为高变异对于规范代理行为至关重要，但过度随机化会导致过于保守的策略。在本文中，我们提出了一种新的方法来解决模拟到真实的转移，即在模拟训练过程中自动调整动力学分布，而无需真实世界数据。我们引入了通过熵最大化实现领域随机化的方法（DORAEMON），这是一个受限优化问题，直接最大化训练分布的熵同时保留泛化能力。为了实现这一目标，DORAEMON随着当前策略成功概率足够高，逐渐增加样本动力学参数的多样性。",
    "tldr": "本文提出了一种新的领域随机化方法，通过熵最大化的方式在模拟训练中调整动力学分布，以实现模拟到真实的转移，无需真实世界数据，能够保持泛化能力和代理的高成功概率。",
    "en_tdlr": "This paper proposes a novel approach called Domain Randomization via Entropy Maximization (DORAEMON) to address sim-to-real transfer in Reinforcement Learning. DORAEMON adjusts the dynamics distribution during simulation training by maximizing the entropy, achieving sim-to-real transfer without real-world data while maintaining generalization capabilities and high success probability of the agent."
}