{
    "title": "Finding Foundation Models for Time Series Classification with a PreText Task",
    "abstract": "arXiv:2311.14534v2 Announce Type: replace  Abstract: Over the past decade, Time Series Classification (TSC) has gained an increasing attention. While various methods were explored, deep learning - particularly through Convolutional Neural Networks (CNNs)-stands out as an effective approach. However, due to the limited availability of training data, defining a foundation model for TSC that overcomes the overfitting problem is still a challenging task. The UCR archive, encompassing a wide spectrum of datasets ranging from motion recognition to ECG-based heart disease detection, serves as a prime example for exploring this issue in diverse TSC scenarios. In this paper, we address the overfitting challenge by introducing pre-trained domain foundation models. A key aspect of our methodology is a novel pretext task that spans multiple datasets. This task is designed to identify the originating dataset of each time series sample, with the goal of creating flexible convolution filters that can",
    "link": "https://arxiv.org/abs/2311.14534",
    "context": "Title: Finding Foundation Models for Time Series Classification with a PreText Task\nAbstract: arXiv:2311.14534v2 Announce Type: replace  Abstract: Over the past decade, Time Series Classification (TSC) has gained an increasing attention. While various methods were explored, deep learning - particularly through Convolutional Neural Networks (CNNs)-stands out as an effective approach. However, due to the limited availability of training data, defining a foundation model for TSC that overcomes the overfitting problem is still a challenging task. The UCR archive, encompassing a wide spectrum of datasets ranging from motion recognition to ECG-based heart disease detection, serves as a prime example for exploring this issue in diverse TSC scenarios. In this paper, we address the overfitting challenge by introducing pre-trained domain foundation models. A key aspect of our methodology is a novel pretext task that spans multiple datasets. This task is designed to identify the originating dataset of each time series sample, with the goal of creating flexible convolution filters that can",
    "path": "papers/23/11/2311.14534.json",
    "total_tokens": 841,
    "translated_title": "寻找时间序列分类的基础模型，以进行预文本任务",
    "translated_abstract": "在过去的十年中，时间序列分类（TSC）越来越受到关注。尽管探索过各种方法，但深度学习-特别是通过卷积神经网络（CNN）-被认为是一种有效的方法。然而，由于训练数据的有限可用性，定义一个克服过拟合问题的TSC基础模型仍然是一项具有挑战性的任务。 UCR档案库涵盖从动作识别到基于心电图的心脏病检测等各种数据集，是探索不同TSC场景下这一问题的一个主要示例。 在本文中，我们通过引入预训练的领域基础模型来解决过拟合挑战。我们方法的一个关键方面是一个跨多个数据集的新领域预文本任务。此任务旨在识别每个时间序列样本的来源数据集，目的是创建灵活的卷积滤波器，可以",
    "tldr": "该论文提出通过引入预训练的领域基础模型来解决时间序列分类中的过拟合挑战，并采用新颖的跨数据集预文本任务的方法。",
    "en_tdlr": "This paper addresses the overfitting challenge in time series classification by introducing pre-trained domain foundation models and utilizing a novel pretext task that spans multiple datasets."
}