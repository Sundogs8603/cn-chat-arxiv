{
    "title": "Forgetting before Learning: Utilizing Parametric Arithmetic for Knowledge Updating in Large Language Models",
    "abstract": "arXiv:2311.08011v2 Announce Type: replace  Abstract: Recent advancements in Large Language Models (LLMs) have showcased their remarkable capabilities in text understanding and generation. However, even stronger LLMs are susceptible to acquiring erroneous or obsolete information from the training corpus. Direct secondary fine-tuning with data containing new knowledge may be ineffective in updating knowledge due to the conflict between old and new knowledge. In this paper, we propose a new paradigm for fine-tuning called F-Learning (Forgetting before Learning), which employs parametric arithmetic to facilitate the forgetting of old knowledge and learning of new knowledge. Experimental results on two publicly available datasets demonstrate that our proposed F-Learning can obviously improve the knowledge updating performance of both full fine-tuning and LoRA fine-tuning, simultaneously outperforming the existing baselines in most cases. Moreover, we have also discovered that forgetting old",
    "link": "https://arxiv.org/abs/2311.08011",
    "context": "Title: Forgetting before Learning: Utilizing Parametric Arithmetic for Knowledge Updating in Large Language Models\nAbstract: arXiv:2311.08011v2 Announce Type: replace  Abstract: Recent advancements in Large Language Models (LLMs) have showcased their remarkable capabilities in text understanding and generation. However, even stronger LLMs are susceptible to acquiring erroneous or obsolete information from the training corpus. Direct secondary fine-tuning with data containing new knowledge may be ineffective in updating knowledge due to the conflict between old and new knowledge. In this paper, we propose a new paradigm for fine-tuning called F-Learning (Forgetting before Learning), which employs parametric arithmetic to facilitate the forgetting of old knowledge and learning of new knowledge. Experimental results on two publicly available datasets demonstrate that our proposed F-Learning can obviously improve the knowledge updating performance of both full fine-tuning and LoRA fine-tuning, simultaneously outperforming the existing baselines in most cases. Moreover, we have also discovered that forgetting old",
    "path": "papers/23/11/2311.08011.json",
    "total_tokens": 842,
    "translated_title": "在学习之前遗忘：利用参数化算术进行大型语言模型中的知识更新",
    "translated_abstract": "大型语言模型（LLMs）的最新进展展示了它们在文本理解和生成方面出色的能力。然而，即使更强大的LLMs也会受到从训练语料库中获取错误或过时信息的影响。直接使用包含新知识的数据进行二次微调可能无法有效更新知识，这是由于旧知识和新知识之间的冲突。本文提出了一种名为F-Learning（学习之前遗忘）的微调新范式，它采用参数化算术来促进旧知识的遗忘和新知识的学习。在两个公开可用数据集上的实验结果表明，我们提出的F-Learning显著改善了完全微调和LoRA微调的知识更新性能，在大多数情况下同时优于现有基线。此外，我们还发现了遗忘旧知识",
    "tldr": "提出了一种名为F-Learning的新微调范式，利用参数化算术促进旧知识的遗忘和新知识的学习，在大型语言模型中显著改善知识更新性能",
    "en_tdlr": "Proposes a new fine-tuning paradigm called F-Learning, which utilizes parametric arithmetic to facilitate forgetting of old knowledge and learning of new knowledge, significantly improving knowledge updating performance in large language models."
}