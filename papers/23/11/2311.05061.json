{
    "title": "Efficient Compression of Overparameterized Deep Models through Low-Dimensional Learning Dynamics",
    "abstract": "arXiv:2311.05061v2 Announce Type: replace  Abstract: Overparameterized models have proven to be powerful tools for solving various machine learning tasks. However, overparameterization often leads to a substantial increase in computational and memory costs, which in turn requires extensive resources to train. In this work, we present a novel approach for compressing overparameterized models, developed through studying their learning dynamics. We observe that for many deep models, updates to the weight matrices occur within a low-dimensional invariant subspace. For deep linear models, we demonstrate that their principal components are fitted incrementally within a small subspace, and use these insights to propose a compression algorithm for deep linear networks that involve decreasing the width of their intermediate layers. We empirically evaluate the effectiveness of our compression technique on matrix recovery problems. Remarkably, by using an initialization that exploits the structur",
    "link": "https://arxiv.org/abs/2311.05061",
    "context": "Title: Efficient Compression of Overparameterized Deep Models through Low-Dimensional Learning Dynamics\nAbstract: arXiv:2311.05061v2 Announce Type: replace  Abstract: Overparameterized models have proven to be powerful tools for solving various machine learning tasks. However, overparameterization often leads to a substantial increase in computational and memory costs, which in turn requires extensive resources to train. In this work, we present a novel approach for compressing overparameterized models, developed through studying their learning dynamics. We observe that for many deep models, updates to the weight matrices occur within a low-dimensional invariant subspace. For deep linear models, we demonstrate that their principal components are fitted incrementally within a small subspace, and use these insights to propose a compression algorithm for deep linear networks that involve decreasing the width of their intermediate layers. We empirically evaluate the effectiveness of our compression technique on matrix recovery problems. Remarkably, by using an initialization that exploits the structur",
    "path": "papers/23/11/2311.05061.json",
    "total_tokens": 861,
    "translated_title": "通过低维学习动态实现超参数化深度模型的高效压缩",
    "translated_abstract": "超参数化模型已被证明是解决各种机器学习任务的强大工具。然而，过度参数化往往导致计算和内存成本大幅增加，进而需要大量资源来训练。在这项工作中，我们提出了一种新颖的方法来压缩超参数化模型，通过研究它们的学习动态来实现。我们观察到对于许多深度模型，权重矩阵的更新发生在低维不变子空间内。对于深度线性模型，我们展示了它们的主要成分在一个小子空间内逐渐适配，并利用这些见解提出了一种针对深度线性网络的压缩算法，其中包括减小其中间层的宽度。我们从实验角度评估了我们的压缩技术在矩阵恢复问题上的有效性。",
    "tldr": "通过研究深度模型的学习动态，提出了一种压缩超参数化模型的新方法，通过在低维不变子空间内更新权重矩阵来压缩深度线性网络，并在矩阵恢复问题上进行了有效性评估",
    "en_tdlr": "A novel approach for compressing overparameterized models is proposed by studying the learning dynamics of deep models, leveraging low-dimensional invariant subspaces for weight matrix updates in deep linear networks, with empirical evaluation on matrix recovery problems."
}