{
    "title": "Energy Efficiency Optimization for Subterranean LoRaWAN Using A Reinforcement Learning Approach: A Direct-to-Satellite Scenario. (arXiv:2311.01743v1 [cs.IT])",
    "abstract": "The integration of subterranean LoRaWAN and non-terrestrial networks (NTN) delivers substantial economic and societal benefits in remote agriculture and disaster rescue operations. The LoRa modulation leverages quasi-orthogonal spreading factors (SFs) to optimize data rates, airtime, coverage and energy consumption. However, it is still challenging to effectively assign SFs to end devices for minimizing co-SF interference in massive subterranean LoRaWAN NTN. To address this, we investigate a reinforcement learning (RL)-based SFs allocation scheme to optimize the system's energy efficiency (EE). To efficiently capture the device-to-environment interactions in dense networks, we proposed an SFs allocation technique using the multi-agent dueling double deep Q-network (MAD3QN) and the multi-agent advantage actor-critic (MAA2C) algorithms based on an analytical reward mechanism. Our proposed RL-based SFs allocation approach evinces better performance compared to four benchmarks in the extre",
    "link": "http://arxiv.org/abs/2311.01743",
    "context": "Title: Energy Efficiency Optimization for Subterranean LoRaWAN Using A Reinforcement Learning Approach: A Direct-to-Satellite Scenario. (arXiv:2311.01743v1 [cs.IT])\nAbstract: The integration of subterranean LoRaWAN and non-terrestrial networks (NTN) delivers substantial economic and societal benefits in remote agriculture and disaster rescue operations. The LoRa modulation leverages quasi-orthogonal spreading factors (SFs) to optimize data rates, airtime, coverage and energy consumption. However, it is still challenging to effectively assign SFs to end devices for minimizing co-SF interference in massive subterranean LoRaWAN NTN. To address this, we investigate a reinforcement learning (RL)-based SFs allocation scheme to optimize the system's energy efficiency (EE). To efficiently capture the device-to-environment interactions in dense networks, we proposed an SFs allocation technique using the multi-agent dueling double deep Q-network (MAD3QN) and the multi-agent advantage actor-critic (MAA2C) algorithms based on an analytical reward mechanism. Our proposed RL-based SFs allocation approach evinces better performance compared to four benchmarks in the extre",
    "path": "papers/23/11/2311.01743.json",
    "total_tokens": 968,
    "translated_title": "地下LoRaWAN网络能效优化的强化学习方法：直接卫星连接场景下的应用",
    "translated_abstract": "在偏远农业和灾害救援行动中，地下LoRaWAN与非地球网络（NTN）的集成为经济和社会带来了重大好处。LoRa调制利用准正交扩频因子（SFs）来优化数据速率、空闲时间、覆盖范围和能量消耗。然而，在大规模地下LoRaWAN NTN中，有效地为终端设备分配SFs以最小化共享扩频因子干扰仍然具有挑战性。为了解决这个问题，我们研究了基于强化学习的SFs分配方案来优化系统的能效。为了有效捕捉密集网络中设备与环境之间的互动，我们提出了一种使用基于分析奖励机制的多智能体dueling double deep Q-network（MAD3QN）和多智能体优势演员-评论家（MAA2C）算法的SFs分配技术。与四个基准相比，我们提出的基于强化学习的SFs分配方法表现更好。",
    "tldr": "本文介绍了一种地下LoRaWAN网络能效优化的强化学习方法，该方法采用多智能体深度Q网络和多智能体优势演员-评论家算法来分配扩频因子，以最小化共享扩频因子干扰并优化系统的能效。",
    "en_tdlr": "This paper presents a reinforcement learning approach for energy efficiency optimization in subterranean LoRaWAN networks. The proposed method utilizes multi-agent deep Q-network and multi-agent advantage actor-critic algorithms to allocate spreading factors, minimizing co-SF interference and optimizing system performance."
}