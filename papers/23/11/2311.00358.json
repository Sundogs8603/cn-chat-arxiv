{
    "title": "Rethinking Samples Selection for Contrastive Learning: Mining of Potential Samples. (arXiv:2311.00358v1 [cs.CV])",
    "abstract": "Contrastive learning predicts whether two images belong to the same category by training a model to make their feature representations as close or as far away as possible. In this paper, we rethink how to mine samples in contrastive learning, unlike other methods, our approach is more comprehensive, taking into account both positive and negative samples, and mining potential samples from two aspects: First, for positive samples, we consider both the augmented sample views obtained by data augmentation and the mined sample views through data mining. Then, we weight and combine them using both soft and hard weighting strategies. Second, considering the existence of uninformative negative samples and false negative samples in the negative samples, we analyze the negative samples from the gradient perspective and finally mine negative samples that are neither too hard nor too easy as potential negative samples, i.e., those negative samples that are close to positive samples. The experiment",
    "link": "http://arxiv.org/abs/2311.00358",
    "context": "Title: Rethinking Samples Selection for Contrastive Learning: Mining of Potential Samples. (arXiv:2311.00358v1 [cs.CV])\nAbstract: Contrastive learning predicts whether two images belong to the same category by training a model to make their feature representations as close or as far away as possible. In this paper, we rethink how to mine samples in contrastive learning, unlike other methods, our approach is more comprehensive, taking into account both positive and negative samples, and mining potential samples from two aspects: First, for positive samples, we consider both the augmented sample views obtained by data augmentation and the mined sample views through data mining. Then, we weight and combine them using both soft and hard weighting strategies. Second, considering the existence of uninformative negative samples and false negative samples in the negative samples, we analyze the negative samples from the gradient perspective and finally mine negative samples that are neither too hard nor too easy as potential negative samples, i.e., those negative samples that are close to positive samples. The experiment",
    "path": "papers/23/11/2311.00358.json",
    "total_tokens": 918,
    "translated_title": "重新思考对比学习中的样本选择：潜在样本的挖掘",
    "translated_abstract": "对比学习通过训练模型使两个图像的特征表示尽可能接近或远离，从而预测它们是否属于同一类别。本文重新思考了对比学习中如何挖掘样本，与其他方法不同，我们的方法更全面，同时考虑了正样本和负样本，并从两个方面挖掘潜在样本：首先，对于正样本，我们考虑了通过数据增强获得的增强样本视图和通过数据挖掘获得的挖掘样本视图。然后，我们使用软权值和硬权值进行加权和组合。其次，考虑到负样本中存在无信息的负样本和误判的负样本，我们从梯度的角度分析负样本，并最终挖掘既不过于困难也不过于容易的负样本作为潜在的负样本，即靠近正样本的负样本。实验结果表明，我们的方法在对比学习中取得了较好的效果。",
    "tldr": "这篇论文重新思考了对比学习中的样本选择方法，从而提出了一种更全面的方法，通过综合考虑正负样本，利用数据增强和数据挖掘挖掘潜在样本，并且分析负样本梯度，最终获得了较好的效果。",
    "en_tdlr": "This paper rethinks the sample selection for contrastive learning, proposing a comprehensive approach that considers both positive and negative samples. It mines potential samples through data augmentation, data mining, and gradient analysis, achieving better results in contrastive learning."
}