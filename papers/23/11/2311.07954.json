{
    "title": "A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning",
    "abstract": "arXiv:2311.07954v2 Announce Type: replace  Abstract: Logical reasoning has been an ongoing pursuit in the field of AI. Despite significant advancements made by large language models (LLMs), they still struggle with complex logical reasoning problems. To enhance reasoning performance, one promising direction is scalable oversight, which requires LLMs to identify their own errors and then improve by themselves. Various self-verification methods have been proposed in pursuit of this goal. Nevertheless, whether existing models understand their own errors well is still under investigation. In this paper, we take a closer look at the self-verification abilities of LLMs in the context of logical reasoning, focusing on their ability to identify logical fallacies accurately. We introduce a dataset, FALLACIES, containing 232 types of reasoning fallacies categorized in a hierarchical taxonomy. By conducting exhaustive experiments on FALLACIES, we obtain comprehensive and detailed analyses of a se",
    "link": "https://arxiv.org/abs/2311.07954",
    "context": "Title: A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning\nAbstract: arXiv:2311.07954v2 Announce Type: replace  Abstract: Logical reasoning has been an ongoing pursuit in the field of AI. Despite significant advancements made by large language models (LLMs), they still struggle with complex logical reasoning problems. To enhance reasoning performance, one promising direction is scalable oversight, which requires LLMs to identify their own errors and then improve by themselves. Various self-verification methods have been proposed in pursuit of this goal. Nevertheless, whether existing models understand their own errors well is still under investigation. In this paper, we take a closer look at the self-verification abilities of LLMs in the context of logical reasoning, focusing on their ability to identify logical fallacies accurately. We introduce a dataset, FALLACIES, containing 232 types of reasoning fallacies categorized in a hierarchical taxonomy. By conducting exhaustive experiments on FALLACIES, we obtain comprehensive and detailed analyses of a se",
    "path": "papers/23/11/2311.07954.json",
    "total_tokens": 813,
    "translated_title": "大型语言模型在逻辑推理中的自我验证能力",
    "translated_abstract": "逻辑推理一直是人工智能领域的追求目标。尽管大型语言模型（LLMs）取得了显著进展，但它们仍然在复杂的逻辑推理问题上面临困难。为了增强推理性能，一个有希望的方向是可扩展的监督，这需要LLMs识别自己的错误，然后自行改进。为了实现这一目标，提出了各种自我验证方法。然而，现有模型是否很好地理解自己的错误仍在调查中。本文着重探讨了LLMs在逻辑推理背景下的自我验证能力，关注它们准确识别逻辑谬误的能力。我们引入了一个包含232种推理谬误的数据集FALLACIES，并进行了大量实验，从而获得了关于LLMs在FALLACIES上的全面和详细分析。",
    "tldr": "本文研究了大型语言模型在逻辑推理中的自我验证能力，特别关注它们准确识别逻辑谬误的能力。"
}