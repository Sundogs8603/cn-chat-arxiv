{
    "title": "Which Modality should I use -- Text, Motif, or Image? : Understanding Graphs with Large Language Models",
    "abstract": "arXiv:2311.09862v2 Announce Type: replace  Abstract: Our research integrates graph data with Large Language Models (LLMs), which, despite their advancements in various fields using large text corpora, face limitations in encoding entire graphs due to context size constraints. This paper introduces a new approach to encoding a graph with diverse modalities, such as text, image, and motif, coupled with prompts to approximate a graph's global connectivity, thereby enhancing LLMs' efficiency in processing complex graph structures. The study also presents GraphTMI, a novel benchmark for evaluating LLMs in graph structure analysis, focusing on homophily, motif presence, and graph difficulty. Key findings indicate that the image modality, especially with vision-language models like GPT-4V, is superior to text in balancing token limits and preserving essential information and outperforms prior graph neural net (GNN) encoders. Furthermore, the research assesses how various factors affect the pe",
    "link": "https://arxiv.org/abs/2311.09862",
    "context": "Title: Which Modality should I use -- Text, Motif, or Image? : Understanding Graphs with Large Language Models\nAbstract: arXiv:2311.09862v2 Announce Type: replace  Abstract: Our research integrates graph data with Large Language Models (LLMs), which, despite their advancements in various fields using large text corpora, face limitations in encoding entire graphs due to context size constraints. This paper introduces a new approach to encoding a graph with diverse modalities, such as text, image, and motif, coupled with prompts to approximate a graph's global connectivity, thereby enhancing LLMs' efficiency in processing complex graph structures. The study also presents GraphTMI, a novel benchmark for evaluating LLMs in graph structure analysis, focusing on homophily, motif presence, and graph difficulty. Key findings indicate that the image modality, especially with vision-language models like GPT-4V, is superior to text in balancing token limits and preserving essential information and outperforms prior graph neural net (GNN) encoders. Furthermore, the research assesses how various factors affect the pe",
    "path": "papers/23/11/2311.09862.json",
    "total_tokens": 909,
    "translated_title": "我应该使用哪种模态--文本、主题或图像？：通过大型语言模型理解图表",
    "translated_abstract": "我们的研究将图数据与大型语言模型（LLMs）结合起来，尽管它们在利用大量文本语料库在各个领域取得了进展，但由于上下文大小限制，面临着在对整个图进行编码时的限制。本文介绍了一种新方法，使用文本、图像和主题等多样的模态对图进行编码，结合提示以近似表示一个图的全局连通性，从而增强LLMs在处理复杂图结构时的效率。该研究还提出了GraphTMI，一个评估LLMs在图结构分析中的新颖基准，重点关注同质性、主题存在和图难度。关键发现表明，尤其是使用视觉语言模型如GPT-4V的图像模态，比文本在平衡标记限制和保留关键信息方面更优，胜过先前的图神经网络（GNN）编码器。此外，该研究评估了各种因素如何影响Pe",
    "tldr": "本研究提出了一种新方法，使用文本、图像和主题等多种模态对图进行编码，结合提示以近似表示图的全局连接性，从而提高了LLMs处理复杂图结构的效率。",
    "en_tdlr": "This study introduces a new approach to encoding graphs with diverse modalities such as text, image, and motif, coupled with prompts to approximate a graph's global connectivity, thereby enhancing the efficiency of LLMs in processing complex graph structures."
}