{
    "title": "DeepInception: Hypnotize Large Language Model to Be Jailbreaker",
    "abstract": "Despite remarkable success in various applications, large language models (LLMs) are vulnerable to adversarial jailbreaks that make the safety guardrails void. However, previous studies for jailbreaks usually resort to brute-force optimization or extrapolations of a high computation cost, which might not be practical or effective. In this paper, inspired by the Milgram experiment w.r.t. the authority power for inciting harmfulness, we disclose a lightweight method, termed DeepInception, which can easily hypnotize LLM to be a jailbreaker. Specifically, DeepInception leverages the personification ability of LLM to construct a novel nested scene to behave, which realizes an adaptive way to escape the usage control in a normal scenario. Empirically, our DeepInception can achieve competitive jailbreak success rates with previous counterparts and realize a continuous jailbreak in subsequent interactions, which reveals the critical weakness of self-losing on both open and closed-source LLMs l",
    "link": "https://arxiv.org/abs/2311.03191",
    "context": "Title: DeepInception: Hypnotize Large Language Model to Be Jailbreaker\nAbstract: Despite remarkable success in various applications, large language models (LLMs) are vulnerable to adversarial jailbreaks that make the safety guardrails void. However, previous studies for jailbreaks usually resort to brute-force optimization or extrapolations of a high computation cost, which might not be practical or effective. In this paper, inspired by the Milgram experiment w.r.t. the authority power for inciting harmfulness, we disclose a lightweight method, termed DeepInception, which can easily hypnotize LLM to be a jailbreaker. Specifically, DeepInception leverages the personification ability of LLM to construct a novel nested scene to behave, which realizes an adaptive way to escape the usage control in a normal scenario. Empirically, our DeepInception can achieve competitive jailbreak success rates with previous counterparts and realize a continuous jailbreak in subsequent interactions, which reveals the critical weakness of self-losing on both open and closed-source LLMs l",
    "path": "papers/23/11/2311.03191.json",
    "total_tokens": 965,
    "translated_title": "DeepInception: 催眠大型语言模型成为破解者",
    "translated_abstract": "尽管大型语言模型（LLMs）在各种应用中取得了显著的成功，但它们容易受到破解攻击，使得安全措施无效。然而，以往的破解研究通常采用暴力优化或高计算成本的外推方法，这可能并不实际或有效。本文受到以米尔格拉姆实验为灵感，关于权威力量对于引发有害行为的影响，我们提出了一种轻量级的方法，称为DeepInception，可以轻松地催眠LLM成为破解者。具体而言，DeepInception利用LLM的角色扮演能力构建了一个新颖的嵌套场景来行为，实现了在正常场景下逃避使用控制的自适应方式。实验结果表明，我们的DeepInception在破解成功率方面与以往的方法竞争力相当，并可以在后续交互中实现持续的破解，揭示了开源和闭源LLM的自失关键弱点。",
    "tldr": "本研究提出了一种名为DeepInception的轻量级方法，利用语言模型的角色扮演能力构建新颖的嵌套场景，成功催眠大型语言模型成为破解者。通过实验证明，DeepInception在破解成功率方面具有竞争力，并揭示了开源和闭源语言模型的关键弱点。",
    "en_tdlr": "This study proposes a lightweight method called DeepInception, which leverages the personification ability of language models to construct novel nested scenes and successfully hypnotize them into becoming jailbreakers. Empirical results show that DeepInception achieves competitive success rates and reveals critical weaknesses in both open-source and closed-source language models."
}