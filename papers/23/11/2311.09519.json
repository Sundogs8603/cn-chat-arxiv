{
    "title": "Leveraging Code to Improve In-context Learning for Semantic Parsing",
    "abstract": "arXiv:2311.09519v2 Announce Type: replace  Abstract: In-context learning (ICL) is an appealing approach for semantic parsing due to its few-shot nature and improved generalization. However, learning to parse to rare domain-specific languages (DSLs) from just a few demonstrations is challenging, limiting the performance of even the most capable LLMs. In this work, we improve the effectiveness of ICL for semantic parsing by (1) using general-purpose programming languages such as Python instead of DSLs, and (2) augmenting prompts with a structured domain description that includes, e.g., the available classes and functions. We show that both these changes significantly improve accuracy across three popular datasets. Combined, they lead to dramatic improvements (e.g. 7.9% to 66.5% on SMCalFlow compositional split), nearly closing the performance gap between easier i.i.d.\\ and harder compositional splits when used with a strong model, and reducing the need for a large number of demonstration",
    "link": "https://arxiv.org/abs/2311.09519",
    "context": "Title: Leveraging Code to Improve In-context Learning for Semantic Parsing\nAbstract: arXiv:2311.09519v2 Announce Type: replace  Abstract: In-context learning (ICL) is an appealing approach for semantic parsing due to its few-shot nature and improved generalization. However, learning to parse to rare domain-specific languages (DSLs) from just a few demonstrations is challenging, limiting the performance of even the most capable LLMs. In this work, we improve the effectiveness of ICL for semantic parsing by (1) using general-purpose programming languages such as Python instead of DSLs, and (2) augmenting prompts with a structured domain description that includes, e.g., the available classes and functions. We show that both these changes significantly improve accuracy across three popular datasets. Combined, they lead to dramatic improvements (e.g. 7.9% to 66.5% on SMCalFlow compositional split), nearly closing the performance gap between easier i.i.d.\\ and harder compositional splits when used with a strong model, and reducing the need for a large number of demonstration",
    "path": "papers/23/11/2311.09519.json",
    "total_tokens": 833,
    "translated_title": "利用代码提升语境学习对语义解析的改进",
    "translated_abstract": "在这项工作中，我们通过以下两项改进，提高了语境学习（ICL）对语义解析的有效性：（1）使用通用编程语言（如Python）代替DSL，并（2）通过增加结构化领域描述的提示，包括可用的类和函数等。我们展示了这两个变化显著提高了三个流行数据集的准确性。它们的结合导致了显著的改进（例如，在SMCalFlow组合划分上从7.9%到66.5%），几乎在使用强模型时消除了易于i.i.d和更困难的组合划分之间的性能差距，并减少了对大量演示的需求。",
    "tldr": "通过使用通用编程语言代替领域特定语言（DSLs）和增加结构化领域描述提示，本研究显著改善了语境学习（ICL）对语义解析的有效性，提高了准确性并减少了对大量示范的需求。",
    "en_tdlr": "By utilizing general-purpose programming languages instead of domain-specific languages (DSLs) and augmenting prompts with a structured domain description, this work significantly improves the effectiveness of in-context learning (ICL) for semantic parsing, enhancing accuracy and reducing the need for a large number of demonstrations."
}