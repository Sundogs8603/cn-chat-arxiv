{
    "title": "Learning Causal Representations from General Environments: Identifiability and Intrinsic Ambiguity",
    "abstract": "We study causal representation learning, the task of recovering high-level latent variables and their causal relationships in the form of a causal graph from low-level observed data (such as text and images), assuming access to observations generated from multiple environments. Prior results on the identifiability of causal representations typically assume access to single-node interventions which is rather unrealistic in practice, since the latent variables are unknown in the first place. In this work, we provide the first identifiability results based on data that stem from general environments. We show that for linear causal models, while the causal graph can be fully recovered, the latent variables are only identified up to the surrounded-node ambiguity (SNA) \\citep{varici2023score}. We provide a counterpart of our guarantee, showing that SNA is basically unavoidable in our setting. We also propose an algorithm, \\texttt{LiNGCReL} which provably recovers the ground-truth model up to",
    "link": "https://arxiv.org/abs/2311.12267",
    "context": "Title: Learning Causal Representations from General Environments: Identifiability and Intrinsic Ambiguity\nAbstract: We study causal representation learning, the task of recovering high-level latent variables and their causal relationships in the form of a causal graph from low-level observed data (such as text and images), assuming access to observations generated from multiple environments. Prior results on the identifiability of causal representations typically assume access to single-node interventions which is rather unrealistic in practice, since the latent variables are unknown in the first place. In this work, we provide the first identifiability results based on data that stem from general environments. We show that for linear causal models, while the causal graph can be fully recovered, the latent variables are only identified up to the surrounded-node ambiguity (SNA) \\citep{varici2023score}. We provide a counterpart of our guarantee, showing that SNA is basically unavoidable in our setting. We also propose an algorithm, \\texttt{LiNGCReL} which provably recovers the ground-truth model up to",
    "path": "papers/23/11/2311.12267.json",
    "total_tokens": 901,
    "translated_title": "从一般环境中学习因果表示：可辨识性和内在歧义",
    "translated_abstract": "我们研究因果表示学习，即从低级观测数据（如文本和图像）中恢复高级潜在变量及其因果关系的任务，假设可以访问从多个环境生成的观察结果。之前关于因果表示可辨识性的结果通常假设可以访问单节点干预，但实际上这是不切实际的，因为潜在变量本身就未知。在本研究中，我们提供了基于来自一般环境的数据的第一个可辨识性结果。我们展示了对于线性因果模型，虽然可以完全恢复因果图，但潜在变量只能被识别到受到围绕节点歧义（SNA）的程度上。我们提供了我们保证的对应对，证明了在我们的设置中SNA基本上是不可避免的。我们还提出了一个算法LiNGCReL，可以被证明可以恢复出地面真实模型",
    "tldr": "该论文研究了从一般环境中学习因果表示的问题，提供了基于这种环境生成的数据的可辨识性结果，并指出了受到围绕节点歧义的限制。同时提出了一个算法可以恢复出地面真实模型"
}