{
    "title": "Improved weight initialization for deep and narrow feedforward neural network",
    "abstract": "arXiv:2311.03733v2 Announce Type: replace  Abstract: Appropriate weight initialization settings, along with the ReLU activation function, have become cornerstones of modern deep learning, enabling the training and deployment of highly effective and efficient neural network models across diverse areas of artificial intelligence. The problem of \\textquotedblleft dying ReLU,\" where ReLU neurons become inactive and yield zero output, presents a significant challenge in the training of deep neural networks with ReLU activation function. Theoretical research and various methods have been introduced to address the problem. However, even with these methods and research, training remains challenging for extremely deep and narrow feedforward networks with ReLU activation function. In this paper, we propose a novel weight initialization method to address this issue. We establish several properties of our initial weight matrix and demonstrate how these properties enable the effective propagation o",
    "link": "https://arxiv.org/abs/2311.03733",
    "context": "Title: Improved weight initialization for deep and narrow feedforward neural network\nAbstract: arXiv:2311.03733v2 Announce Type: replace  Abstract: Appropriate weight initialization settings, along with the ReLU activation function, have become cornerstones of modern deep learning, enabling the training and deployment of highly effective and efficient neural network models across diverse areas of artificial intelligence. The problem of \\textquotedblleft dying ReLU,\" where ReLU neurons become inactive and yield zero output, presents a significant challenge in the training of deep neural networks with ReLU activation function. Theoretical research and various methods have been introduced to address the problem. However, even with these methods and research, training remains challenging for extremely deep and narrow feedforward networks with ReLU activation function. In this paper, we propose a novel weight initialization method to address this issue. We establish several properties of our initial weight matrix and demonstrate how these properties enable the effective propagation o",
    "path": "papers/23/11/2311.03733.json",
    "total_tokens": 804,
    "translated_title": "改进的深窄前馈神经网络权重初始化方法",
    "translated_abstract": "适当的权重初始化设置，以及ReLU激活函数，已成为现代深度学习的基石，使得高效和有效的神经网络模型能够在人工智能的各个领域得以训练和推广。在使用ReLU激活函数训练深度神经网络时，“死亡ReLU”问题，即ReLU神经元变得不活跃并输出为零，是一个重要挑战。理论研究和各种方法已被引入以解决这一问题。然而，即使有了这些方法和研究，对于使用ReLU激活函数的极深窄前馈网络的训练仍然具有挑战性。本文提出了一种新颖的权重初始化方法来解决这个问题。我们确定了我们初始权重矩阵的几个特性，并演示了这些特性如何使有效传播更加可能。",
    "tldr": "本文提出了一种新颖的权重初始化方法，能有效解决在使用ReLU激活函数训练极深窄前馈网络时遇到的“死亡ReLU”问题。",
    "en_tdlr": "This paper proposes a novel weight initialization method that effectively addresses the issue of \"dying ReLU\" encountered when training extremely deep and narrow feedforward networks with ReLU activation function."
}