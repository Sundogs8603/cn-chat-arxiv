{
    "title": "Navigating Scaling Laws: Compute Optimality in Adaptive Model Training",
    "abstract": "arXiv:2311.03233v2 Announce Type: replace  Abstract: In recent years, the state-of-the-art in deep learning has been dominated by very large models that have been pre-trained on vast amounts of data. The paradigm is very simple: investing more computational resources (optimally) leads to better performance, and even predictably so; neural scaling laws have been derived that accurately forecast the performance of a network for a desired level of compute. This leads to the notion of a `compute-optimal' model, i.e. a model that allocates a given level of compute during training optimally to maximize performance. In this work, we extend the concept of optimality by allowing for an `adaptive' model, i.e. a model that can change its shape during training. By doing so, we can design adaptive models that optimally traverse between the underlying scaling laws and outpace their `static' counterparts, leading to a significant reduction in the required compute to reach a given target performance. ",
    "link": "https://arxiv.org/abs/2311.03233",
    "context": "Title: Navigating Scaling Laws: Compute Optimality in Adaptive Model Training\nAbstract: arXiv:2311.03233v2 Announce Type: replace  Abstract: In recent years, the state-of-the-art in deep learning has been dominated by very large models that have been pre-trained on vast amounts of data. The paradigm is very simple: investing more computational resources (optimally) leads to better performance, and even predictably so; neural scaling laws have been derived that accurately forecast the performance of a network for a desired level of compute. This leads to the notion of a `compute-optimal' model, i.e. a model that allocates a given level of compute during training optimally to maximize performance. In this work, we extend the concept of optimality by allowing for an `adaptive' model, i.e. a model that can change its shape during training. By doing so, we can design adaptive models that optimally traverse between the underlying scaling laws and outpace their `static' counterparts, leading to a significant reduction in the required compute to reach a given target performance. ",
    "path": "papers/23/11/2311.03233.json",
    "total_tokens": 893,
    "translated_title": "导航规模定律：自适应模型训练中的计算优化",
    "translated_abstract": "近年来，深度学习的最新技术主要由经过大量数据预训练的非常庞大模型主导。这一范式非常简单：投入更多的计算资源（最优地）会提高性能，而且甚至能够可预测性地做到；已经推导出了神经网络性能的缩放定律，准确预测了网络在所需计算水平下的性能。这引出了“计算优化”模型的概念，即在训练过程中分配给定计算水平以最大化性能的模型。在本研究中，我们通过允许“自适应”模型，即在训练过程中可以改变形状的模型，来扩展优化概念。通过这样做，我们可以设计出能够最优地在基本定律之间穿行并超越它们的“静态”对应物的自适应模型，从而显著减少达到给定目标性能所需的计算量。",
    "tldr": "本研究提出了一种新颖的自适应模型训练方法，通过允许模型在训练过程中调整形状，能够优化地使用计算资源，实现在更少的计算量下达到目标性能。",
    "en_tdlr": "This study introduces a novel approach for adaptive model training, allowing models to adjust their shape during training to optimally utilize computational resources and achieve target performance with less computation."
}