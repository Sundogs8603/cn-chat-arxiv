{
    "title": "HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data",
    "abstract": "arXiv:2311.13614v2 Announce Type: replace-cross  Abstract: Multi-modal Large Language Models (MLLMs) tuned on machine-generated instruction-following data have demonstrated remarkable performance in various multi-modal understanding and generation tasks. However, the hallucinations inherent in machine-generated data, which could lead to hallucinatory outputs in MLLMs, remain under-explored. This work aims to investigate various hallucinations (i.e., object, relation, attribute hallucinations) and mitigate those hallucinatory toxicities in large-scale machine-generated visual instruction datasets. Drawing on the human ability to identify factual errors, we present a novel hallucination detection and elimination framework, HalluciDoctor, based on the cross-checking paradigm. We use our framework to identify and eliminate hallucinations in the training data automatically. Interestingly, HalluciDoctor also indicates that spurious correlations arising from long-tail object co-occurrences co",
    "link": "https://arxiv.org/abs/2311.13614",
    "context": "Title: HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data\nAbstract: arXiv:2311.13614v2 Announce Type: replace-cross  Abstract: Multi-modal Large Language Models (MLLMs) tuned on machine-generated instruction-following data have demonstrated remarkable performance in various multi-modal understanding and generation tasks. However, the hallucinations inherent in machine-generated data, which could lead to hallucinatory outputs in MLLMs, remain under-explored. This work aims to investigate various hallucinations (i.e., object, relation, attribute hallucinations) and mitigate those hallucinatory toxicities in large-scale machine-generated visual instruction datasets. Drawing on the human ability to identify factual errors, we present a novel hallucination detection and elimination framework, HalluciDoctor, based on the cross-checking paradigm. We use our framework to identify and eliminate hallucinations in the training data automatically. Interestingly, HalluciDoctor also indicates that spurious correlations arising from long-tail object co-occurrences co",
    "path": "papers/23/11/2311.13614.json",
    "total_tokens": 849,
    "translated_title": "HalluciDoctor: 减轻视觉指令数据中的幻觉毒性",
    "translated_abstract": "大型多模态语言模型（MLLMs）在机器生成的指令跟随数据上进行调整，已经在各种多模态理解和生成任务中展现出卓越的性能。然而，机器生成数据中固有的幻觉，可能导致MLLMs产生幻觉输出，这一问题仍未得到充分探讨。本研究旨在调查大规模机器生成的视觉指令数据中的各种幻觉（即对象、关系、属性幻觉），并减轻这些幻觉毒性。借鉴人类识别事实错误的能力，我们提出了一个基于交叉检查范式的新颖幻觉检测和消除框架HalluciDoctor。我们使用我们的框架自动地识别和消除训练数据中的幻觉。有趣的是，HalluciDoctor还表明，长尾对象共现产生的虚假相关性也可能导致幻觉。",
    "tldr": "本研究提出了一个新颖的幻觉检测和消除框架HalluciDoctor，旨在减轻大规模机器生成的视觉指令数据中的幻觉毒性。",
    "en_tdlr": "This study introduces a novel hallucination detection and elimination framework, HalluciDoctor, aiming to mitigate hallucinatory toxicity in large-scale machine-generated visual instruction datasets."
}