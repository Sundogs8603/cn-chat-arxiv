{
    "title": "Fixed-Budget Best-Arm Identification in Sparse Linear Bandits. (arXiv:2311.00481v1 [cs.LG])",
    "abstract": "We study the best-arm identification problem in sparse linear bandits under the fixed-budget setting. In sparse linear bandits, the unknown feature vector $\\theta^*$ may be of large dimension $d$, but only a few, say $s \\ll d$ of these features have non-zero values. We design a two-phase algorithm, Lasso and Optimal-Design- (Lasso-OD) based linear best-arm identification. The first phase of Lasso-OD leverages the sparsity of the feature vector by applying the thresholded Lasso introduced by Zhou (2009), which estimates the support of $\\theta^*$ correctly with high probability using rewards from the selected arms and a judicious choice of the design matrix. The second phase of Lasso-OD applies the OD-LinBAI algorithm by Yang and Tan (2022) on that estimated support. We derive a non-asymptotic upper bound on the error probability of Lasso-OD by carefully choosing hyperparameters (such as Lasso's regularization parameter) and balancing the error probabilities of both phases. For fixed spa",
    "link": "http://arxiv.org/abs/2311.00481",
    "context": "Title: Fixed-Budget Best-Arm Identification in Sparse Linear Bandits. (arXiv:2311.00481v1 [cs.LG])\nAbstract: We study the best-arm identification problem in sparse linear bandits under the fixed-budget setting. In sparse linear bandits, the unknown feature vector $\\theta^*$ may be of large dimension $d$, but only a few, say $s \\ll d$ of these features have non-zero values. We design a two-phase algorithm, Lasso and Optimal-Design- (Lasso-OD) based linear best-arm identification. The first phase of Lasso-OD leverages the sparsity of the feature vector by applying the thresholded Lasso introduced by Zhou (2009), which estimates the support of $\\theta^*$ correctly with high probability using rewards from the selected arms and a judicious choice of the design matrix. The second phase of Lasso-OD applies the OD-LinBAI algorithm by Yang and Tan (2022) on that estimated support. We derive a non-asymptotic upper bound on the error probability of Lasso-OD by carefully choosing hyperparameters (such as Lasso's regularization parameter) and balancing the error probabilities of both phases. For fixed spa",
    "path": "papers/23/11/2311.00481.json",
    "total_tokens": 977,
    "translated_title": "在稀疏线性bandit中的固定预算下，最佳臂识别问题的研究",
    "translated_abstract": "本文研究了在稀疏线性bandit中的固定预算条件下的最佳臂识别问题。在稀疏线性bandit中，未知特征向量θ*可能具有很大的维度d，但只有一小部分特征（比如s个）具有非零值。我们设计了一个两阶段的算法，即基于Lasso和最优设计(Lasso-OD)的线性最佳臂识别。Lasso-OD的第一阶段利用了特征向量的稀疏性，通过应用Zhou（2009）引入的阈值化Lasso，利用所选择的臂的回报和合理选择的设计矩阵来高概率地正确估计θ*的支持集。Lasso-OD的第二阶段在估计得到的支持集上应用了Yang和Tan（2022）提出的OD-LinBAI算法。我们通过精心选择超参数（如Lasso的正则化参数）和平衡两个阶段的错误概率，推导了Lasso-OD的非渐近上界。",
    "tldr": "本文研究了在稀疏线性bandit中的固定预算条件下的最佳臂识别问题，设计了基于Lasso和最优设计的两阶段算法，通过适当选择超参数和平衡两个阶段的错误概率，得到了Lasso-OD的非渐近上界。"
}