{
    "title": "Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task. (arXiv:2311.00686v1 [cs.CL])",
    "abstract": "This paper describes and analyzes our participation in the 2023 Eval4NLP shared task, which focuses on assessing the effectiveness of prompt-based techniques to empower Large Language Models to handle the task of quality estimation, particularly in the context of evaluating machine translations and summaries. We conducted systematic experiments with various prompting techniques, including standard prompting, prompts informed by annotator instructions, and innovative chain-of-thought prompting. In addition, we integrated these approaches with zero-shot and one-shot learning methods to maximize the efficacy of our evaluation procedures. Our work reveals that combining these approaches using a \"small\", open source model (orca_mini_v3_7B) yields competitive results.",
    "link": "http://arxiv.org/abs/2311.00686",
    "context": "Title: Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task. (arXiv:2311.00686v1 [cs.CL])\nAbstract: This paper describes and analyzes our participation in the 2023 Eval4NLP shared task, which focuses on assessing the effectiveness of prompt-based techniques to empower Large Language Models to handle the task of quality estimation, particularly in the context of evaluating machine translations and summaries. We conducted systematic experiments with various prompting techniques, including standard prompting, prompts informed by annotator instructions, and innovative chain-of-thought prompting. In addition, we integrated these approaches with zero-shot and one-shot learning methods to maximize the efficacy of our evaluation procedures. Our work reveals that combining these approaches using a \"small\", open source model (orca_mini_v3_7B) yields competitive results.",
    "path": "papers/23/11/2311.00686.json",
    "total_tokens": 829,
    "translated_title": "小巨人：探索小型LLM作为Eval4NLP 2023共享任务中摘要评估度量的潜力",
    "translated_abstract": "本文描述和分析了我们参与2023 Eval4NLP共享任务的情况，该任务专注于评估基于提示的技术对大型语言模型在质量估计中的应用效果，特别是在评估机器翻译和摘要的背景下。我们进行了系统性实验，使用了多种提示技术，包括标准提示、基于注释者指示的提示和创新的思路链提示。此外，我们将这些方法与零样本学习和一次性学习方法结合起来，以最大化我们的评估程序的功效。我们的工作表明，使用“小型”开源模型（orca_mini_v3_7B）结合这些方法可以获得有竞争力的结果。",
    "tldr": "本文通过对Eval4NLP 2023共享任务的研究，探索了小型LLM作为摘要评估度量的潜力，在使用多种提示技术和学习方法的基础上，揭示了结合小型开源模型orca_mini_v3_7B可以取得有竞争力的结果。",
    "en_tdlr": "This paper explores the potential of small LLMs as evaluation metrics in summarization by participating in the Eval4NLP 2023 shared task. Through systematic experiments with various prompting techniques and learning methods, the study reveals that combining these approaches with a small open source model, orca_mini_v3_7B, yields competitive results."
}