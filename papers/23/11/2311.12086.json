{
    "title": "Masked Autoencoders Are Robust Neural Architecture Search Learners",
    "abstract": "arXiv:2311.12086v2 Announce Type: replace  Abstract: Neural Architecture Search (NAS) currently relies heavily on labeled data, which is both expensive and time-consuming to acquire. In this paper, we propose a novel NAS framework based on Masked Autoencoders (MAE) that eliminates the need for labeled data during the search process. By replacing the supervised learning objective with an image reconstruction task, our approach enables the robust discovery of network architectures without compromising performance and generalization ability. Additionally, we address the problem of performance collapse encountered in the widely-used Differentiable Architecture Search (DARTS) method in the unsupervised paradigm by introducing a multi-scale decoder. Through extensive experiments conducted on various search spaces and datasets, we demonstrate the effectiveness and robustness of the proposed method, providing empirical evidence of its superiority over baseline approaches.",
    "link": "https://arxiv.org/abs/2311.12086",
    "context": "Title: Masked Autoencoders Are Robust Neural Architecture Search Learners\nAbstract: arXiv:2311.12086v2 Announce Type: replace  Abstract: Neural Architecture Search (NAS) currently relies heavily on labeled data, which is both expensive and time-consuming to acquire. In this paper, we propose a novel NAS framework based on Masked Autoencoders (MAE) that eliminates the need for labeled data during the search process. By replacing the supervised learning objective with an image reconstruction task, our approach enables the robust discovery of network architectures without compromising performance and generalization ability. Additionally, we address the problem of performance collapse encountered in the widely-used Differentiable Architecture Search (DARTS) method in the unsupervised paradigm by introducing a multi-scale decoder. Through extensive experiments conducted on various search spaces and datasets, we demonstrate the effectiveness and robustness of the proposed method, providing empirical evidence of its superiority over baseline approaches.",
    "path": "papers/23/11/2311.12086.json",
    "total_tokens": 860,
    "translated_title": "掩码自编码器是鲁棒的神经架构搜索学习器",
    "translated_abstract": "神经架构搜索（NAS）目前严重依赖标记数据，而获取标记数据既昂贵又耗时。本文提出了一种基于掩码自编码器（MAE）的新型NAS框架，它在搜索过程中消除了对标记数据的需求。通过将监督学习目标替换为图像重建任务，我们的方法使得能够在不损害性能和泛化能力的情况下鲁棒地发现网络架构。此外，我们通过引入多尺度解码器解决了在无监督范式中广泛使用的可微架构搜索（DARTS）方法遇到的性能崩溃问题。通过在各种搜索空间和数据集上进行大量实验，我们展示了所提方法的有效性和鲁棒性，为其胜过基准方法提供了实证证据。",
    "tldr": "提出了一种基于掩码自编码器的新型神经架构搜索框架，无需标记数据，在搜索过程中使用图像重建任务代替监督学习目标，具有鲁棒性、性能和泛化能力，并通过引入多尺度解码器解决了性能崩溃问题。",
    "en_tdlr": "Proposed a novel NAS framework based on Masked Autoencoders (MAE) that eliminates the need for labeled data during the search process, uses image reconstruction task instead of supervised learning objective, and addresses performance collapse issue by introducing a multi-scale decoder, demonstrating effectiveness and robustness over baseline approaches."
}