{
    "title": "Continual Learning: Applications and the Road Forward",
    "abstract": "arXiv:2311.11908v3 Announce Type: replace-cross  Abstract: Continual learning is a subfield of machine learning, which aims to allow machine learning models to continuously learn on new data, by accumulating knowledge without forgetting what was learned in the past. In this work, we take a step back, and ask: \"Why should one care about continual learning in the first place?\". We set the stage by examining recent continual learning papers published at four major machine learning conferences, and show that memory-constrained settings dominate the field. Then, we discuss five open problems in machine learning, and even though they might seem unrelated to continual learning at first sight, we show that continual learning will inevitably be part of their solution. These problems are model editing, personalization and specialization, on-device learning, faster (re-)training and reinforcement learning. Finally, by comparing the desiderata from these unsolved problems and the current assumptio",
    "link": "https://arxiv.org/abs/2311.11908",
    "context": "Title: Continual Learning: Applications and the Road Forward\nAbstract: arXiv:2311.11908v3 Announce Type: replace-cross  Abstract: Continual learning is a subfield of machine learning, which aims to allow machine learning models to continuously learn on new data, by accumulating knowledge without forgetting what was learned in the past. In this work, we take a step back, and ask: \"Why should one care about continual learning in the first place?\". We set the stage by examining recent continual learning papers published at four major machine learning conferences, and show that memory-constrained settings dominate the field. Then, we discuss five open problems in machine learning, and even though they might seem unrelated to continual learning at first sight, we show that continual learning will inevitably be part of their solution. These problems are model editing, personalization and specialization, on-device learning, faster (re-)training and reinforcement learning. Finally, by comparing the desiderata from these unsolved problems and the current assumptio",
    "path": "papers/23/11/2311.11908.json",
    "total_tokens": 950,
    "translated_title": "连续学习：应用与未来路径",
    "translated_abstract": "连续学习是机器学习的一个子领域，旨在使机器学习模型能够在新数据上不断学习，通过积累知识而不遗忘过去所学。本研究退一步思考，并提出问题：“为什么首先要关注连续学习？”。我们通过审视近期在四个主要机器学习会议上发表的连续学习论文来铺垫，展示了受内存限制的场景主导了该领域。然后，我们讨论了机器学习中的五个未解问题，尽管乍看起来可能与连续学习无关，但我们展示了连续学习将必然成为它们解决方案的一部分。这些问题包括模型编辑、个性化和专业化、设备端学习、更快的（重新）训练和强化学习。最后，通过比较这些未解问题的期望和当前的假设",
    "tldr": "连续学习是机器学习的子领域，致力于让机器学习模型在新数据上不断学习，而不忘记过去学到的知识。研究揭示了内存限制场景的主导地位，并讨论了连续学习在解决模型编辑、个性化、专业化、设备端学习、快速（重新）训练和强化学习等问题中的作用。",
    "en_tdlr": "Continual learning is a subfield of machine learning that enables machine learning models to learn continuously on new data without forgetting what was learned in the past. The research reveals the dominance of memory-constrained settings and discusses the role of continual learning in addressing issues such as model editing, personalization, specialization, on-device learning, faster (re-)training, and reinforcement learning."
}