{
    "title": "Evaluating LLMs on Document-Based QA: Exact Answer Selection and Numerical Extraction using Cogtale dataset. (arXiv:2311.07878v4 [cs.IR] UPDATED)",
    "abstract": "Document-based Question-Answering (QA) tasks are crucial for precise information retrieval. While some existing work focus on evaluating large language models performance on retrieving and answering questions from documents, assessing the LLMs performance on QA types that require exact answer selection from predefined options and numerical extraction is yet to be fully assessed. In this paper, we specifically focus on this underexplored context and conduct empirical analysis of LLMs (GPT-4 and GPT-3.5) on question types, including single-choice, yes-no, multiple-choice, and number extraction questions from documents in zero-shot setting. We use the CogTale dataset for evaluation, which provide human expert-tagged responses, offering a robust benchmark for precision and factual grounding. We found that LLMs, particularly GPT-4, can precisely answer many single-choice and yes-no questions given relevant context, demonstrating their efficacy in information retrieval tasks. However, their ",
    "link": "http://arxiv.org/abs/2311.07878",
    "context": "Title: Evaluating LLMs on Document-Based QA: Exact Answer Selection and Numerical Extraction using Cogtale dataset. (arXiv:2311.07878v4 [cs.IR] UPDATED)\nAbstract: Document-based Question-Answering (QA) tasks are crucial for precise information retrieval. While some existing work focus on evaluating large language models performance on retrieving and answering questions from documents, assessing the LLMs performance on QA types that require exact answer selection from predefined options and numerical extraction is yet to be fully assessed. In this paper, we specifically focus on this underexplored context and conduct empirical analysis of LLMs (GPT-4 and GPT-3.5) on question types, including single-choice, yes-no, multiple-choice, and number extraction questions from documents in zero-shot setting. We use the CogTale dataset for evaluation, which provide human expert-tagged responses, offering a robust benchmark for precision and factual grounding. We found that LLMs, particularly GPT-4, can precisely answer many single-choice and yes-no questions given relevant context, demonstrating their efficacy in information retrieval tasks. However, their ",
    "path": "papers/23/11/2311.07878.json",
    "total_tokens": 854,
    "translated_title": "在基于文档的问答任务中评估LLM：使用Cogtale数据集进行精确答案选择和数字提取",
    "translated_abstract": "基于文档的问答任务对于精确的信息检索至关重要。虽然一些已有的工作关注评估大型语言模型在从文档中检索和回答问题的性能，但对于需要从预定义选项中选择精确答案和进行数字提取的QA类型，LLM的性能尚未得到充分评估。本文特别关注这个未深入研究的背景，并对LLM（GPT-4和GPT-3.5）在单选题、是非题、多选题和数字提取问题等问答类型上进行实证分析，使用CogTale数据集进行评估。该数据集提供了人工专家标注的答案，为精确性和事实依据提供了强有力的基准。我们发现LLM，尤其是GPT-4，在相关背景下可以准确回答许多单选题和是非题，展示了它们在信息检索任务中的有效性。",
    "tldr": "本文在基于文档的问答任务中评估了LLM的性能，包括精确答案选择和数字提取。研究发现LLM在单选题和是非题上的效果较好，展示了其在信息检索任务中的有效性。"
}