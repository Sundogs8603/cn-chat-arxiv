{
    "title": "Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web",
    "abstract": "Language model agents (LMA) recently emerged as a promising paradigm on muti-step decision making tasks, often outperforming humans and other reinforcement learning agents. Despite the promise, their performance on real-world applications that often involve combinations of tasks is still underexplored. In this work, we introduce a new benchmark, called CompWoB -- 50 new compositional web automation tasks reflecting more realistic assumptions. We show that while existing prompted LMAs (gpt-3.5-turbo or gpt-4) achieve 94.0% average success rate on base tasks, their performance degrades to 24.9% success rate on compositional tasks. On the other hand, transferred LMAs (finetuned only on base tasks) show less generalization gap, dropping from 85.4% to 54.8%. By balancing data distribution across tasks, we train a new model, HTML-T5++, that surpasses human-level performance (95.2%) on MiniWoB, and achieves the best zero-shot performance on CompWoB (61.5%). While these highlight the promise o",
    "link": "https://arxiv.org/abs/2311.18751",
    "context": "Title: Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web\nAbstract: Language model agents (LMA) recently emerged as a promising paradigm on muti-step decision making tasks, often outperforming humans and other reinforcement learning agents. Despite the promise, their performance on real-world applications that often involve combinations of tasks is still underexplored. In this work, we introduce a new benchmark, called CompWoB -- 50 new compositional web automation tasks reflecting more realistic assumptions. We show that while existing prompted LMAs (gpt-3.5-turbo or gpt-4) achieve 94.0% average success rate on base tasks, their performance degrades to 24.9% success rate on compositional tasks. On the other hand, transferred LMAs (finetuned only on base tasks) show less generalization gap, dropping from 85.4% to 54.8%. By balancing data distribution across tasks, we train a new model, HTML-T5++, that surpasses human-level performance (95.2%) on MiniWoB, and achieves the best zero-shot performance on CompWoB (61.5%). While these highlight the promise o",
    "path": "papers/23/11/2311.18751.json",
    "total_tokens": 1028,
    "translated_title": "在Web上揭示语言模型代理在顺序任务组合中的局限性",
    "translated_abstract": "最近，语言模型代理(LMA)作为一种在多步决策任务上的有希望的范例出现，通常表现优于人类和其他强化学习代理。尽管有这种希望，但它们在通常涉及任务组合的现实应用中的性能仍未得到充分探索。在这项工作中，我们引入了一个新的基准，叫做CompWoB-反映更现实假设的50个组合性网站自动化任务。我们发现，虽然现有的提示型LMA（gpt-3.5-turbo或gpt-4）在基本任务上实现了94.0％的平均成功率，但在组合任务上降至24.9％的成功率。另一方面，只在基本任务上进行微调的转移性LMA表现出更小的泛化性差距，从85.4％下降到54.8％。通过平衡任务之间的数据分布，我们训练了一个新模型HTML-T5++，在MiniWoB上超过了人类水平的性能（95.2％），并在CompWoB上实现了最佳的零-shot性能（61.5%）。",
    "tldr": "本文介绍了语言模型代理 (LMA) 在多步决策任务上的有希望的范例，在基本任务上具有出色的性能，但在组合任务上表现不佳。通过平衡数据分布，我们训练了一个新模型 HTML-T5++，在现实应用中取得了超越人类的性能，并在新基准测试中实现了最佳零-shot性能。",
    "en_tdlr": "This paper introduces language model agents (LMAs) as a promising paradigm in multi-step decision making tasks, showing impressive performance on individual tasks but limitations in compositional tasks. By balancing data distribution, a new model HTML-T5++ achieves performance surpassing humans and the best zero-shot performance on a new benchmark."
}