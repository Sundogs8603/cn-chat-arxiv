{
    "title": "BoschAI @ PLABA 2023: Leveraging Edit Operations in End-to-End Neural Sentence Simplification. (arXiv:2311.01907v1 [cs.CL])",
    "abstract": "Automatic simplification can help laypeople to comprehend complex scientific text. Language models are frequently applied to this task by translating from complex to simple language. In this paper, we describe our system based on Llama 2, which ranked first in the PLABA shared task addressing the simplification of biomedical text. We find that the large portion of shared tokens between input and output leads to weak training signals and conservatively editing models. To mitigate these issues, we propose sentence-level and token-level loss weights. They give higher weight to modified tokens, indicated by edit distance and edit operations, respectively. We conduct an empirical evaluation on the PLABA dataset and find that both approaches lead to simplifications closer to those created by human annotators (+1.8% / +3.5% SARI), simpler language (-1 / -1.1 FKGL) and more edits (1.6x / 1.8x edit distance) compared to the same model fine-tuned with standard cross entropy. We furthermore show ",
    "link": "http://arxiv.org/abs/2311.01907",
    "context": "Title: BoschAI @ PLABA 2023: Leveraging Edit Operations in End-to-End Neural Sentence Simplification. (arXiv:2311.01907v1 [cs.CL])\nAbstract: Automatic simplification can help laypeople to comprehend complex scientific text. Language models are frequently applied to this task by translating from complex to simple language. In this paper, we describe our system based on Llama 2, which ranked first in the PLABA shared task addressing the simplification of biomedical text. We find that the large portion of shared tokens between input and output leads to weak training signals and conservatively editing models. To mitigate these issues, we propose sentence-level and token-level loss weights. They give higher weight to modified tokens, indicated by edit distance and edit operations, respectively. We conduct an empirical evaluation on the PLABA dataset and find that both approaches lead to simplifications closer to those created by human annotators (+1.8% / +3.5% SARI), simpler language (-1 / -1.1 FKGL) and more edits (1.6x / 1.8x edit distance) compared to the same model fine-tuned with standard cross entropy. We furthermore show ",
    "path": "papers/23/11/2311.01907.json",
    "total_tokens": 889,
    "translated_title": "BoschAI @ PLABA 2023: 利用编辑操作在端到端神经句子简化中的应用",
    "translated_abstract": "自动简化可以帮助普通人理解复杂的科学文本。语言模型经常用于将复杂语言转换为简单语言。本文中，我们描述了基于Llama 2的系统，该系统在处理生物医学文本简化的PLABA共享任务中排名第一。我们发现输入和输出之间共享的标记的数量很多，导致训练信号较弱和保守的编辑模型。为了缓解这些问题，我们提出了句子级和标记级的损失权重。它们给予修改的标记更高的权重，修改通过编辑距离和编辑操作进行指示。我们在PLABA数据集上进行了实证评估，并发现这两种方法都使简化结果更接近人工注释者创建的结果（+1.8% / +3.5% SARI），语言更简单（-1 / -1.1 FKGL），并且编辑更多（1.6x / 1.8x编辑距离），相比于使用标准交叉熵进行微调的相同模型。",
    "tldr": "本文描述了基于Llama 2的系统，在处理生物医学文本简化的PLABA共享任务中排名第一。通过引入句子级和标记级的损失权重，该系统能够产生与人工注释者相似的简化结果，语言更简单，并且进行更多的编辑操作。"
}