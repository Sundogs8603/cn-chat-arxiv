{
    "title": "On robust overfitting: adversarial training induced distribution matters",
    "abstract": "Adversarial training may be regarded as standard training with a modified loss function. But its generalization error appears much larger than standard training under standard loss. This phenomenon, known as robust overfitting, has attracted significant research attention and remains largely as a mystery. In this paper, we first show empirically that robust overfitting correlates with the increasing generalization difficulty of the perturbation-induced distributions along the trajectory of adversarial training (specifically PGD-based adversarial training). We then provide a novel upper bound for generalization error with respect to the perturbation-induced distributions, in which a notion of the perturbation operator, referred to \"local dispersion\", plays an important role. Experimental results are presented to validate the usefulness of the bound and various additional insights are provided.",
    "link": "https://arxiv.org/abs/2311.16526",
    "context": "Title: On robust overfitting: adversarial training induced distribution matters\nAbstract: Adversarial training may be regarded as standard training with a modified loss function. But its generalization error appears much larger than standard training under standard loss. This phenomenon, known as robust overfitting, has attracted significant research attention and remains largely as a mystery. In this paper, we first show empirically that robust overfitting correlates with the increasing generalization difficulty of the perturbation-induced distributions along the trajectory of adversarial training (specifically PGD-based adversarial training). We then provide a novel upper bound for generalization error with respect to the perturbation-induced distributions, in which a notion of the perturbation operator, referred to \"local dispersion\", plays an important role. Experimental results are presented to validate the usefulness of the bound and various additional insights are provided.",
    "path": "papers/23/11/2311.16526.json",
    "total_tokens": 842,
    "translated_title": "关于鲁棒过拟合：对抗训练引起的分布问题很重要",
    "translated_abstract": "对抗训练可以被看作是使用修改后的损失函数的标准训练。但在标准损失下，其泛化误差明显大于标准训练。这一现象被称为鲁棒过拟合，已经引起了相当大的研究关注但仍然是一个谜。本文首先通过实验证实鲁棒过拟合与对抗训练轨迹上的扰动引起的分布的泛化困难性逐渐增加之间的相关性。我们然后给出了一个关于扰动引起的分布的泛化误差的新的上界，其中一个被称为“局部分散”的扰动算子起到重要作用。我们还提供了实验结果来验证该上界的实用性，并提供了各种其他见解。",
    "tldr": "本文首先通过实验证实鲁棒过拟合与对抗训练轨迹上的扰动引起的分布的泛化困难性逐渐增加之间的相关性，然后给出了一个关于扰动引起的分布的泛化误差的新的上界，验证了其实用性，并提供了各种其他见解。"
}