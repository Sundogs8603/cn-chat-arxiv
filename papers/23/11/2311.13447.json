{
    "title": "Differentially Private Non-Convex Optimization under the KL Condition with Optimal Rates",
    "abstract": "arXiv:2311.13447v2 Announce Type: replace  Abstract: We study private empirical risk minimization (ERM) problem for losses satisfying the $(\\gamma,\\kappa)$-Kurdyka-{\\L}ojasiewicz (KL) condition. The Polyak-{\\L}ojasiewicz (PL) condition is a special case of this condition when $\\kappa=2$. Specifically, we study this problem under the constraint of $\\rho$ zero-concentrated differential privacy (zCDP). When $\\kappa\\in[1,2]$ and the loss function is Lipschitz and smooth over a sufficiently large region, we provide a new algorithm based on variance reduced gradient descent that achieves the rate $\\tilde{O}\\big(\\big(\\frac{\\sqrt{d}}{n\\sqrt{\\rho}}\\big)^\\kappa\\big)$ on the excess empirical risk, where $n$ is the dataset size and $d$ is the dimension. We further show that this rate is nearly optimal. When $\\kappa \\geq 2$ and the loss is instead Lipschitz and weakly convex, we show it is possible to achieve the rate $\\tilde{O}\\big(\\big(\\frac{\\sqrt{d}}{n\\sqrt{\\rho}}\\big)^\\kappa\\big)$ with a privat",
    "link": "https://arxiv.org/abs/2311.13447",
    "context": "Title: Differentially Private Non-Convex Optimization under the KL Condition with Optimal Rates\nAbstract: arXiv:2311.13447v2 Announce Type: replace  Abstract: We study private empirical risk minimization (ERM) problem for losses satisfying the $(\\gamma,\\kappa)$-Kurdyka-{\\L}ojasiewicz (KL) condition. The Polyak-{\\L}ojasiewicz (PL) condition is a special case of this condition when $\\kappa=2$. Specifically, we study this problem under the constraint of $\\rho$ zero-concentrated differential privacy (zCDP). When $\\kappa\\in[1,2]$ and the loss function is Lipschitz and smooth over a sufficiently large region, we provide a new algorithm based on variance reduced gradient descent that achieves the rate $\\tilde{O}\\big(\\big(\\frac{\\sqrt{d}}{n\\sqrt{\\rho}}\\big)^\\kappa\\big)$ on the excess empirical risk, where $n$ is the dataset size and $d$ is the dimension. We further show that this rate is nearly optimal. When $\\kappa \\geq 2$ and the loss is instead Lipschitz and weakly convex, we show it is possible to achieve the rate $\\tilde{O}\\big(\\big(\\frac{\\sqrt{d}}{n\\sqrt{\\rho}}\\big)^\\kappa\\big)$ with a privat",
    "path": "papers/23/11/2311.13447.json",
    "total_tokens": 994,
    "translated_title": "在KL条件下具有最优速率的差分私有非凸优化",
    "translated_abstract": "我们研究了满足$(\\gamma,\\kappa)$-Kurdyka-Lojasiewicz (KL)条件的损失函数的私有经验风险最小化（ERM）问题。Polyak-Lojasiewicz (PL)条件是这个条件的特例，当$\\kappa=2$时。具体来说，我们研究了在$\\rho$零集中差分隐私（zCDP）约束下的问题。当$\\kappa\\in[1,2]$且损失函数在足够大的区域内是Lipschitz和光滑的时，我们提出了一种基于方差减少梯度下降的新算法，其在超额经验风险上实现了速率$\\tilde{O}\\big(\\big(\\frac{\\sqrt{d}}{n\\sqrt{\\rho}}\\big)^\\kappa\\big)$，其中$n$是数据集大小，$d$是维度。我们进一步展示了这个速率几乎是最优的。当$\\kappa \\geq 2$且损失函数代替是Lipschitz和弱凸时，我们展示了通过私有方法可以实现速率$\\tilde{O}\\big(\\big(\\frac{\\sqrt{d}}{n\\sqrt{\\rho}}\\big)^\\kappa\\big)$。",
    "tldr": "该论文研究了在KL条件下具有最优速率的差分私有非凸优化问题，并提出了针对不同情况的新算法，实现了接近最优的速率。"
}