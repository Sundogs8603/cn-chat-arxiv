{
    "title": "Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data. (arXiv:2311.00136v1 [q-bio.NC])",
    "abstract": "State-of-the-art systems neuroscience experiments yield large-scale multimodal data, and these data sets require new tools for analysis. Inspired by the success of large pretrained models in vision and language domains, we reframe the analysis of large-scale, cellular-resolution neuronal spiking data into an autoregressive spatiotemporal generation problem. Neuroformer is a multimodal, multitask generative pretrained transformer (GPT) model that is specifically designed to handle the intricacies of data in systems neuroscience. It scales linearly with feature size, can process an arbitrary number of modalities, and is adaptable to downstream tasks, such as predicting behavior. We first trained Neuroformer on simulated datasets, and found that it both accurately predicted simulated neuronal circuit activity, and also intrinsically inferred the underlying neural circuit connectivity, including direction. When pretrained to decode neural responses, the model predicted the behavior of a mo",
    "link": "http://arxiv.org/abs/2311.00136",
    "context": "Title: Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data. (arXiv:2311.00136v1 [q-bio.NC])\nAbstract: State-of-the-art systems neuroscience experiments yield large-scale multimodal data, and these data sets require new tools for analysis. Inspired by the success of large pretrained models in vision and language domains, we reframe the analysis of large-scale, cellular-resolution neuronal spiking data into an autoregressive spatiotemporal generation problem. Neuroformer is a multimodal, multitask generative pretrained transformer (GPT) model that is specifically designed to handle the intricacies of data in systems neuroscience. It scales linearly with feature size, can process an arbitrary number of modalities, and is adaptable to downstream tasks, such as predicting behavior. We first trained Neuroformer on simulated datasets, and found that it both accurately predicted simulated neuronal circuit activity, and also intrinsically inferred the underlying neural circuit connectivity, including direction. When pretrained to decode neural responses, the model predicted the behavior of a mo",
    "path": "papers/23/11/2311.00136.json",
    "total_tokens": 895,
    "translated_title": "Neuroformer：用于脑数据的多模态和多任务生成预训练模型",
    "translated_abstract": "最先进的系统神经科学实验产生了大规模的多模态数据，这些数据集需要新的分析工具。受到视觉和语言领域大规模预训练模型成功的启发，我们将大规模的细胞分辨率神经元尖峰数据的分析重新构建为一个自回归的时空生成问题。Neuroformer是一个多模态、多任务的生成预训练transformer（GPT）模型，专为处理系统神经科学数据的复杂性而设计。它与特征大小呈线性扩展，并且可以处理任意数量的模态，适应下游任务，比如预测行为。我们首先在模拟数据集上训练了Neuroformer，并发现它既能准确预测模拟神经回路活动，也能内在地推断出底层神经回路连接性，包括方向。当预训练用于解码神经响应时，该模型能预测小鼠行为。",
    "tldr": "Neuroformer是一个多模态和多任务的生成预训练模型，旨在处理系统神经科学中大规模的多模态数据。模型经过训练后能准确预测神经回路活动并推断神经回路连接性，同时能用于预测行为。"
}