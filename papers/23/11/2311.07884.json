{
    "title": "Fair Abstractive Summarization of Diverse Perspectives",
    "abstract": "arXiv:2311.07884v2 Announce Type: replace  Abstract: People from different social and demographic groups express diverse perspectives and conflicting opinions on a broad set of topics such as product reviews, healthcare, law, and politics. A fair summary should provide a comprehensive coverage of diverse perspectives without underrepresenting certain groups. However, current work in summarization metrics and Large Language Models (LLMs) evaluation has not explored fair abstractive summarization. In this paper, we systematically investigate fair abstractive summarization for user-generated data. We first formally define fairness in abstractive summarization as not underrepresenting perspectives of any groups of people, and we propose four reference-free automatic metrics by measuring the differences between target and source perspectives. We evaluate nine LLMs, including three GPT models, four LLaMA models, PaLM 2, and Claude, on six datasets collected from social media, online reviews,",
    "link": "https://arxiv.org/abs/2311.07884",
    "context": "Title: Fair Abstractive Summarization of Diverse Perspectives\nAbstract: arXiv:2311.07884v2 Announce Type: replace  Abstract: People from different social and demographic groups express diverse perspectives and conflicting opinions on a broad set of topics such as product reviews, healthcare, law, and politics. A fair summary should provide a comprehensive coverage of diverse perspectives without underrepresenting certain groups. However, current work in summarization metrics and Large Language Models (LLMs) evaluation has not explored fair abstractive summarization. In this paper, we systematically investigate fair abstractive summarization for user-generated data. We first formally define fairness in abstractive summarization as not underrepresenting perspectives of any groups of people, and we propose four reference-free automatic metrics by measuring the differences between target and source perspectives. We evaluate nine LLMs, including three GPT models, four LLaMA models, PaLM 2, and Claude, on six datasets collected from social media, online reviews,",
    "path": "papers/23/11/2311.07884.json",
    "total_tokens": 864,
    "translated_title": "多元视角的公平抽象总结",
    "translated_abstract": "来自不同社会和人口群体的人们对产品评论、医疗保健、法律和政治等一系列主题表达了多元化的观点和相互冲突的意见。 公平的总结应提供对多元化观点的全面覆盖，而不会减少某些群体的代表性。然而，目前关于摘要度量标准和大型语言模型（LLM）评估的工作尚未探讨公平的抽象总结。 本文系统地研究了用户生成数据的公平抽象总结。 我们首先正式定义了抽象总结中的公平性，即不减少任何群体的观点，然后通过衡量目标和源观点之间的差异提出了四个无参考自动度量标准。 我们评估了九个LLM，包括三个GPT模型，四个LLaMA模型，PaLM 2 和 Claude，在来自社交媒体、在线评论等六个数据集上进行了评估。",
    "tldr": "该论文系统地研究了如何在抽象总结中实现公平性，提出了四个无参考自动度量标准，评估了九个大型语言模型，并在多个来源的数据集上进行了实验验证。",
    "en_tdlr": "This paper systematically investigates how to achieve fairness in abstractive summarization, proposes four reference-free automatic metrics, evaluates nine Large Language Models, and conducts experiments on datasets from various sources."
}