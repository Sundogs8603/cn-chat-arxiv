{
    "title": "SCPO: Safe Reinforcement Learning with Safety Critic Policy Optimization. (arXiv:2311.00880v1 [cs.LG])",
    "abstract": "Incorporating safety is an essential prerequisite for broadening the practical applications of reinforcement learning in real-world scenarios. To tackle this challenge, Constrained Markov Decision Processes (CMDPs) are leveraged, which introduce a distinct cost function representing safety violations. In CMDPs' settings, Lagrangian relaxation technique has been employed in previous algorithms to convert constrained optimization problems into unconstrained dual problems. However, these algorithms may inaccurately predict unsafe behavior, resulting in instability while learning the Lagrange multiplier. This study introduces a novel safe reinforcement learning algorithm, Safety Critic Policy Optimization (SCPO). In this study, we define the safety critic, a mechanism that nullifies rewards obtained through violating safety constraints. Furthermore, our theoretical analysis indicates that the proposed algorithm can automatically balance the trade-off between adhering to safety constraints ",
    "link": "http://arxiv.org/abs/2311.00880",
    "context": "Title: SCPO: Safe Reinforcement Learning with Safety Critic Policy Optimization. (arXiv:2311.00880v1 [cs.LG])\nAbstract: Incorporating safety is an essential prerequisite for broadening the practical applications of reinforcement learning in real-world scenarios. To tackle this challenge, Constrained Markov Decision Processes (CMDPs) are leveraged, which introduce a distinct cost function representing safety violations. In CMDPs' settings, Lagrangian relaxation technique has been employed in previous algorithms to convert constrained optimization problems into unconstrained dual problems. However, these algorithms may inaccurately predict unsafe behavior, resulting in instability while learning the Lagrange multiplier. This study introduces a novel safe reinforcement learning algorithm, Safety Critic Policy Optimization (SCPO). In this study, we define the safety critic, a mechanism that nullifies rewards obtained through violating safety constraints. Furthermore, our theoretical analysis indicates that the proposed algorithm can automatically balance the trade-off between adhering to safety constraints ",
    "path": "papers/23/11/2311.00880.json",
    "total_tokens": 831,
    "translated_title": "SCPO: 安全批判策略优化下的安全强化学习",
    "translated_abstract": "将安全性纳入强化学习在现实场景中的实际应用中是必不可少的。为了应对这一挑战，我们利用了约束马尔可夫决策过程（CMDPs），引入了表示安全违规的独立成本函数。在CMDPs的设置中，先前的算法采用了拉格朗日松弛技术将约束优化问题转化为无约束对偶问题。然而，这些算法可能会不准确地预测不安全行为，导致在学习拉格朗日乘子时产生不稳定性。本研究介绍了一种新的安全强化学习算法——安全批判策略优化（SCPO）。在本研究中，我们定义了安全批判器，一种通过违反安全约束而获得的奖励被抵消的机制。此外，我们的理论分析表明，所提出的算法可以自动平衡在遵守安全约束和最大化回报之间的权衡。",
    "tldr": "SCPO是一种安全强化学习算法，通过引入安全批判器来确保遵守安全约束并平衡回报的最大化。",
    "en_tdlr": "SCPO is a safe reinforcement learning algorithm that ensures compliance with safety constraints and balances maximizing rewards."
}