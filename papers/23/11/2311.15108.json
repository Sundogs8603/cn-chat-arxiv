{
    "title": "Leveraging Diffusion Perturbations for Measuring Fairness in Computer Vision",
    "abstract": "Computer vision models have been known to encode harmful biases, leading to the potentially unfair treatment of historically marginalized groups, such as people of color. However, there remains a lack of datasets balanced along demographic traits that can be used to evaluate the downstream fairness of these models. In this work, we demonstrate that diffusion models can be leveraged to create such a dataset. We first use a diffusion model to generate a large set of images depicting various occupations. Subsequently, each image is edited using inpainting to generate multiple variants, where each variant refers to a different perceived race. Using this dataset, we benchmark several vision-language models on a multi-class occupation classification task. We find that images generated with non-Caucasian labels have a significantly higher occupation misclassification rate than images generated with Caucasian labels, and that several misclassifications are suggestive of racial biases. We measu",
    "link": "https://arxiv.org/abs/2311.15108",
    "context": "Title: Leveraging Diffusion Perturbations for Measuring Fairness in Computer Vision\nAbstract: Computer vision models have been known to encode harmful biases, leading to the potentially unfair treatment of historically marginalized groups, such as people of color. However, there remains a lack of datasets balanced along demographic traits that can be used to evaluate the downstream fairness of these models. In this work, we demonstrate that diffusion models can be leveraged to create such a dataset. We first use a diffusion model to generate a large set of images depicting various occupations. Subsequently, each image is edited using inpainting to generate multiple variants, where each variant refers to a different perceived race. Using this dataset, we benchmark several vision-language models on a multi-class occupation classification task. We find that images generated with non-Caucasian labels have a significantly higher occupation misclassification rate than images generated with Caucasian labels, and that several misclassifications are suggestive of racial biases. We measu",
    "path": "papers/23/11/2311.15108.json",
    "total_tokens": 905,
    "translated_title": "利用扩散扰动来衡量计算机视觉中的公平性",
    "translated_abstract": "已知计算机视觉模型可能对历史上被边缘化的族群（如有色人种）产生有害偏见，导致潜在的不公平对待。然而，目前仍缺乏以人口统计特征为平衡基准的数据集，用于评估这些模型的下游公平性。在这项工作中，我们展示了扩散模型可以用来创建这样的数据集。首先，我们使用扩散模型生成一组表示不同职业的大量图像。随后，使用修复技术编辑每个图像生成多个变体，其中每个变体对应不同的感知种族。使用这个数据集，我们在一个多类职业分类任务上对多个视觉语言模型进行了基准测试。我们发现，用非高加索标签生成的图像的职业误分类率明显高于用高加索标签生成的图像，并且几个误分类示意存在种族偏见。",
    "tldr": "本研究利用扩散模型创建了一个以人口统计特征平衡的数据集，用于衡量计算机视觉模型的公平性。通过测试多个模型的职业分类任务，发现使用非高加索标签生成的图像存在明显的职业误分类率高于使用高加索标签生成的图像，且几个误分类表明存在种族偏见。",
    "en_tdlr": "This study leverages diffusion models to create a balanced dataset based on demographic traits to measure the fairness of computer vision models. Through benchmarking multiple models on an occupation classification task, it is found that images generated with non-Caucasian labels have a significantly higher misclassification rate and suggest racial biases."
}