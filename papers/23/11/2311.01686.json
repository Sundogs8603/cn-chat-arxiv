{
    "title": "Disentangled Representation Learning with Transmitted Information Bottleneck. (arXiv:2311.01686v1 [cs.CV])",
    "abstract": "Encoding only the task-related information from the raw data, \\ie, disentangled representation learning, can greatly contribute to the robustness and generalizability of models. Although significant advances have been made by regularizing the information in representations with information theory, two major challenges remain: 1) the representation compression inevitably leads to performance drop; 2) the disentanglement constraints on representations are in complicated optimization. To these issues, we introduce Bayesian networks with transmitted information to formulate the interaction among input and representations during disentanglement. Building upon this framework, we propose \\textbf{DisTIB} (\\textbf{T}ransmitted \\textbf{I}nformation \\textbf{B}ottleneck for \\textbf{Dis}entangled representation learning), a novel objective that navigates the balance between information compression and preservation. We employ variational inference to derive a tractable estimation for DisTIB. This es",
    "link": "http://arxiv.org/abs/2311.01686",
    "context": "Title: Disentangled Representation Learning with Transmitted Information Bottleneck. (arXiv:2311.01686v1 [cs.CV])\nAbstract: Encoding only the task-related information from the raw data, \\ie, disentangled representation learning, can greatly contribute to the robustness and generalizability of models. Although significant advances have been made by regularizing the information in representations with information theory, two major challenges remain: 1) the representation compression inevitably leads to performance drop; 2) the disentanglement constraints on representations are in complicated optimization. To these issues, we introduce Bayesian networks with transmitted information to formulate the interaction among input and representations during disentanglement. Building upon this framework, we propose \\textbf{DisTIB} (\\textbf{T}ransmitted \\textbf{I}nformation \\textbf{B}ottleneck for \\textbf{Dis}entangled representation learning), a novel objective that navigates the balance between information compression and preservation. We employ variational inference to derive a tractable estimation for DisTIB. This es",
    "path": "papers/23/11/2311.01686.json",
    "total_tokens": 832,
    "translated_title": "使用传输的信息瓶颈实现解缠表示学习",
    "translated_abstract": "仅编码与任务相关的原始数据信息，即解缠表示学习，可以极大地提高模型的稳健性和泛化能力。虽然在表示中利用信息理论对信息进行规范化取得了重大进展，但仍存在两个主要挑战：1）表示压缩不可避免地导致性能下降；2）对表示的解缠约束存在复杂的优化问题。针对这些问题，我们引入了传输信息的贝叶斯网络来描述解缠过程中输入和表示之间的相互作用。基于这个框架，我们提出了\"DisTIB\"（用于解缠表示学习的传输信息瓶颈），一种新的目标函数，用于平衡信息压缩和保留之间的关系。我们采用变分推断来导出DisTIB的可计算估计。",
    "tldr": "本研究提出了一种通过引入传输信息瓶颈来实现解缠表示学习的方法。该方法可以在压缩表示信息和保留重要信息之间维持平衡，从而提高模型的稳健性和泛化能力。通过使用贝叶斯网络和变分推断，我们得到了可计算估计的DisTIB。"
}