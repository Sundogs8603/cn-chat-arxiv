{
    "title": "Data-Efficient Task Generalization via Probabilistic Model-based Meta Reinforcement Learning",
    "abstract": "We introduce PACOH-RL, a novel model-based Meta-Reinforcement Learning (Meta-RL) algorithm designed to efficiently adapt control policies to changing dynamics. PACOH-RL meta-learns priors for the dynamics model, allowing swift adaptation to new dynamics with minimal interaction data. Existing Meta-RL methods require abundant meta-learning data, limiting their applicability in settings such as robotics, where data is costly to obtain. To address this, PACOH-RL incorporates regularization and epistemic uncertainty quantification in both the meta-learning and task adaptation stages. When facing new dynamics, we use these uncertainty estimates to effectively guide exploration and data collection. Overall, this enables positive transfer, even when access to data from prior tasks or dynamic settings is severely limited. Our experiment results demonstrate that PACOH-RL outperforms model-based RL and model-based Meta-RL baselines in adapting to new dynamic conditions. Finally, on a real roboti",
    "link": "https://arxiv.org/abs/2311.07558",
    "context": "Title: Data-Efficient Task Generalization via Probabilistic Model-based Meta Reinforcement Learning\nAbstract: We introduce PACOH-RL, a novel model-based Meta-Reinforcement Learning (Meta-RL) algorithm designed to efficiently adapt control policies to changing dynamics. PACOH-RL meta-learns priors for the dynamics model, allowing swift adaptation to new dynamics with minimal interaction data. Existing Meta-RL methods require abundant meta-learning data, limiting their applicability in settings such as robotics, where data is costly to obtain. To address this, PACOH-RL incorporates regularization and epistemic uncertainty quantification in both the meta-learning and task adaptation stages. When facing new dynamics, we use these uncertainty estimates to effectively guide exploration and data collection. Overall, this enables positive transfer, even when access to data from prior tasks or dynamic settings is severely limited. Our experiment results demonstrate that PACOH-RL outperforms model-based RL and model-based Meta-RL baselines in adapting to new dynamic conditions. Finally, on a real roboti",
    "path": "papers/23/11/2311.07558.json",
    "total_tokens": 949,
    "translated_title": "通过概率模型元强化学习实现数据高效任务泛化",
    "translated_abstract": "我们介绍了一种新颖的基于概率模型的元强化学习算法PACOH-RL，旨在有效地将控制策略调整到不断变化的动态环境中。PACOH-RL元学习了动态模型的先验知识，能够以最小的交互数据对新的动态环境进行快速适应。现有的元强化学习方法需要大量的元学习数据，限制了它们在机器人等数据获取成本较高的环境中的适用性。为了解决这个问题，PACOH-RL在元学习和任务适应阶段都引入了正则化和认知不确定性量化。当面对新的动态环境时，我们利用这些不确定性估计来有效地引导探索和数据收集。总体而言，这使得在先前任务或动态环境数据极为有限的情况下也能实现正向迁移效果。我们的实验结果表明，与基于模型的强化学习和基于模型的元强化学习基线相比，PACOH-RL在适应新的动态条件方面表现出色。",
    "tldr": "PACOH-RL是一种基于概率模型的元强化学习算法，通过元学习动态模型的先验知识和引入正则化和认知不确定性量化，实现了在数据有限的情况下对新的动态环境的高效适应。"
}