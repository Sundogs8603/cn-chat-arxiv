{
    "title": "Attention or Convolution: Transformer Encoders in Audio Language Models for Inference Efficiency",
    "abstract": "In this paper, we show that a simple self-supervised pre-trained audio model can achieve comparable inference efficiency to more complicated pre-trained models with speech transformer encoders. These speech transformers rely on mixing convolutional modules with self-attention modules. They achieve state-of-the-art performance on ASR with top efficiency. We first show that employing these speech transformers as an encoder significantly improves the efficiency of pre-trained audio models as well. However, our study shows that we can achieve comparable efficiency with advanced self-attention solely. We demonstrate that this simpler approach is particularly beneficial with a low-bit weight quantization technique of a neural network to improve efficiency. We hypothesize that it prevents propagating the errors between different quantized modules compared to recent speech transformers mixing quantized convolution and the quantized self-attention modules.",
    "link": "https://arxiv.org/abs/2311.02772",
    "context": "Title: Attention or Convolution: Transformer Encoders in Audio Language Models for Inference Efficiency\nAbstract: In this paper, we show that a simple self-supervised pre-trained audio model can achieve comparable inference efficiency to more complicated pre-trained models with speech transformer encoders. These speech transformers rely on mixing convolutional modules with self-attention modules. They achieve state-of-the-art performance on ASR with top efficiency. We first show that employing these speech transformers as an encoder significantly improves the efficiency of pre-trained audio models as well. However, our study shows that we can achieve comparable efficiency with advanced self-attention solely. We demonstrate that this simpler approach is particularly beneficial with a low-bit weight quantization technique of a neural network to improve efficiency. We hypothesize that it prevents propagating the errors between different quantized modules compared to recent speech transformers mixing quantized convolution and the quantized self-attention modules.",
    "path": "papers/23/11/2311.02772.json",
    "total_tokens": 873,
    "translated_title": "注意力还是卷积：用于推断效率的转换器编码器在音频语言模型中的应用",
    "translated_abstract": "在本文中，我们展示了一个简单的自监督预训练音频模型可以达到与更复杂的预训练模型（具有语音转换器编码器）相当的推断效率。这些语音转换器结合了卷积模块和自注意力模块，实现了在ASR方面的最先进性能和最高效率。我们首先展示了将这些语音转换器作为编码器显著提高了预训练音频模型的效率。然而，我们的研究表明，仅使用先进的自注意力也可以实现可比较的效率。我们证明了这种更简单的方法特别有利于低位权重量化技术来提高效率。我们假设这可以防止在最近的语音转换器中混合量化卷积和量化自注意力模块时在不同量化模块之间传播错误。",
    "tldr": "本文展示了将语音转换器作为音频模型的编码器，可以显著提高预训练模型的效率。此外，我们发现只使用自注意力也能实现类似的效果，尤其与低位权重量化技术结合使用时效果更好。这一发现有助于防止错误在量化模块之间传播。",
    "en_tdlr": "This paper demonstrates that using speech transformers as encoders improves the efficiency of pre-trained audio models. Additionally, it shows that using only self-attention achieves similar results, especially when combined with low-bit weight quantization. This finding helps to prevent error propagation between quantized modules."
}