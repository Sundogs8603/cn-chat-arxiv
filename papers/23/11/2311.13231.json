{
    "title": "Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model",
    "abstract": "arXiv:2311.13231v3 Announce Type: replace-cross  Abstract: Using reinforcement learning with human feedback (RLHF) has shown significant promise in fine-tuning diffusion models. Previous methods start by training a reward model that aligns with human preferences, then leverage RL techniques to fine-tune the underlying models. However, crafting an efficient reward model demands extensive datasets, optimal architecture, and manual hyperparameter tuning, making the process both time and cost-intensive. The direct preference optimization (DPO) method, effective in fine-tuning large language models, eliminates the necessity for a reward model. However, the extensive GPU memory requirement of the diffusion model's denoising process hinders the direct application of the DPO method. To address this issue, we introduce the Direct Preference for Denoising Diffusion Policy Optimization (D3PO) method to directly fine-tune diffusion models. The theoretical analysis demonstrates that although D3PO o",
    "link": "https://arxiv.org/abs/2311.13231",
    "context": "Title: Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model\nAbstract: arXiv:2311.13231v3 Announce Type: replace-cross  Abstract: Using reinforcement learning with human feedback (RLHF) has shown significant promise in fine-tuning diffusion models. Previous methods start by training a reward model that aligns with human preferences, then leverage RL techniques to fine-tune the underlying models. However, crafting an efficient reward model demands extensive datasets, optimal architecture, and manual hyperparameter tuning, making the process both time and cost-intensive. The direct preference optimization (DPO) method, effective in fine-tuning large language models, eliminates the necessity for a reward model. However, the extensive GPU memory requirement of the diffusion model's denoising process hinders the direct application of the DPO method. To address this issue, we introduce the Direct Preference for Denoising Diffusion Policy Optimization (D3PO) method to directly fine-tune diffusion models. The theoretical analysis demonstrates that although D3PO o",
    "path": "papers/23/11/2311.13231.json",
    "total_tokens": 917,
    "translated_title": "使用人类反馈来微调扩散模型而无需任何奖励模型",
    "translated_abstract": "使用带有人类反馈的强化学习（RLHF）在微调扩散模型方面显示出了显著的潜力。以往的方法首先是通过训练与人类偏好相一致的奖励模型，然后利用强化学习技术来微调基础模型。然而，设计高效的奖励模型需要大量数据集、最佳架构和手动超参数调整，使得这一过程既耗时又成本高昂。直接偏好优化（DPO）方法，在微调大型语言模型方面表现出色，消除了对奖励模型的需求。然而，扩散模型去噪过程的大量GPU内存需求阻碍了DPO方法的直接应用。为解决这一问题，我们引入了直接偏好去噪扩散策略优化（D3PO）方法来直接微调扩散模型。理论分析表明，尽管D3PO提供了改进，但在具有明显优势的同时仍需要更多研究。",
    "tldr": "该论文提出了一种名为D3PO的方法，通过使用人类反馈直接微调扩散模型，无需奖励模型，从而在消除了奖励模型的前提下改进了现有方法，并解决了DPO方法直接应用的内存需求问题。"
}