{
    "title": "RLIF: Interactive Imitation Learning as Reinforcement Learning",
    "abstract": "arXiv:2311.12996v2 Announce Type: replace  Abstract: Although reinforcement learning methods offer a powerful framework for automatic skill acquisition, for practical learning-based control problems in domains such as robotics, imitation learning often provides a more convenient and accessible alternative. In particular, an interactive imitation learning method such as DAgger, which queries a near-optimal expert to intervene online to collect correction data for addressing the distributional shift challenges that afflict na\\\"ive behavioral cloning, can enjoy good performance both in theory and practice without requiring manually specified reward functions and other components of full reinforcement learning methods. In this paper, we explore how off-policy reinforcement learning can enable improved performance under assumptions that are similar but potentially even more practical than those of interactive imitation learning. Our proposed method uses reinforcement learning with user inte",
    "link": "https://arxiv.org/abs/2311.12996",
    "context": "Title: RLIF: Interactive Imitation Learning as Reinforcement Learning\nAbstract: arXiv:2311.12996v2 Announce Type: replace  Abstract: Although reinforcement learning methods offer a powerful framework for automatic skill acquisition, for practical learning-based control problems in domains such as robotics, imitation learning often provides a more convenient and accessible alternative. In particular, an interactive imitation learning method such as DAgger, which queries a near-optimal expert to intervene online to collect correction data for addressing the distributional shift challenges that afflict na\\\"ive behavioral cloning, can enjoy good performance both in theory and practice without requiring manually specified reward functions and other components of full reinforcement learning methods. In this paper, we explore how off-policy reinforcement learning can enable improved performance under assumptions that are similar but potentially even more practical than those of interactive imitation learning. Our proposed method uses reinforcement learning with user inte",
    "path": "papers/23/11/2311.12996.json",
    "total_tokens": 800,
    "translated_title": "RLIF: 交互式模仿学习作为强化学习",
    "translated_abstract": "虽然强化学习方法为自动技能获取提供了强大的框架，但是对于实际的基于学习的控制问题，如机器人领域，模仿学习往往提供了一种更便利和可访问的替代方案。特别是，像DAgger这样的交互式模仿学习方法，可以从一位接近最优的专家手中在线获取纠正数据，以应对困扰天真行为克隆的分布偏移挑战，无论在理论上还是在实践中都能表现良好，而不需要手动指定奖励函数和强化学习方法的其他组件。在本文中，我们探讨了离策略强化学习如何在类似但潜在上更实用的假设下实现较好的性能，从而可以提升强化学习的表现。我们提出的方法使用用户进行增强学习，",
    "tldr": "本文探讨了离策略强化学习如何在类似但潜在上更实用的假设下提供比交互式模仿学习更好的性能。",
    "en_tdlr": "This paper explores how off-policy reinforcement learning can provide better performance than interactive imitation learning under assumptions that are similar but potentially more practical."
}