{
    "title": "Can Foundation Models Watch, Talk and Guide You Step by Step to Make a Cake?. (arXiv:2311.00738v1 [cs.AI])",
    "abstract": "Despite tremendous advances in AI, it remains a significant challenge to develop interactive task guidance systems that can offer situated, personalized guidance and assist humans in various tasks. These systems need to have a sophisticated understanding of the user as well as the environment, and make timely accurate decisions on when and what to say. To address this issue, we created a new multimodal benchmark dataset, Watch, Talk and Guide (WTaG) based on natural interaction between a human user and a human instructor. We further proposed two tasks: User and Environment Understanding, and Instructor Decision Making. We leveraged several foundation models to study to what extent these models can be quickly adapted to perceptually enabled task guidance. Our quantitative, qualitative, and human evaluation results show that these models can demonstrate fair performances in some cases with no task-specific training, but a fast and reliable adaptation remains a significant challenge. Our ",
    "link": "http://arxiv.org/abs/2311.00738",
    "context": "Title: Can Foundation Models Watch, Talk and Guide You Step by Step to Make a Cake?. (arXiv:2311.00738v1 [cs.AI])\nAbstract: Despite tremendous advances in AI, it remains a significant challenge to develop interactive task guidance systems that can offer situated, personalized guidance and assist humans in various tasks. These systems need to have a sophisticated understanding of the user as well as the environment, and make timely accurate decisions on when and what to say. To address this issue, we created a new multimodal benchmark dataset, Watch, Talk and Guide (WTaG) based on natural interaction between a human user and a human instructor. We further proposed two tasks: User and Environment Understanding, and Instructor Decision Making. We leveraged several foundation models to study to what extent these models can be quickly adapted to perceptually enabled task guidance. Our quantitative, qualitative, and human evaluation results show that these models can demonstrate fair performances in some cases with no task-specific training, but a fast and reliable adaptation remains a significant challenge. Our ",
    "path": "papers/23/11/2311.00738.json",
    "total_tokens": 941,
    "translated_title": "基于基础模型的观察、对话和引导：制作蛋糕",
    "translated_abstract": "尽管人工智能取得了巨大进步，但开发能够提供情境化、个性化指导并协助人类进行各种任务的交互式任务引导系统仍然是一个重大挑战。这些系统需要对用户和环境有深入的理解，并及时准确地决定何时以及要说什么。为了解决这个问题，我们创建了一个新的多模态基准数据集，Watch, Talk and Guide（WTaG），基于人类用户和人类指导者之间的自然交互。我们进一步提出了两个任务：用户和环境理解以及指导者决策。我们利用了几个基础模型来研究这些模型在感知化任务引导方面的快速适应程度。我们的定量、定性和人类评估结果表明，这些模型在某些情况下可以展示出公平的性能，即使没有特定任务的训练，但快速而可靠的适应仍然是一个重大挑战。",
    "tldr": "本文介绍了一个新的多模态基准数据集，观察、对话和引导（WTaG），以及两个任务：用户和环境理解以及指导者决策。研究发现，基础模型在感知化任务引导方面有一定的性能，但快速而可靠的适应仍然是一个挑战。",
    "en_tdlr": "This paper presents a new multimodal benchmark dataset, Watch, Talk and Guide (WTaG), and two tasks: User and Environment Understanding, and Instructor Decision Making. The study finds that foundation models can achieve fair performances in perceptually enabled task guidance, but fast and reliable adaptation remains a challenge."
}