{
    "title": "Charting New Territories: Exploring the Geographic and Geospatial Capabilities of Multimodal LLMs. (arXiv:2311.14656v3 [cs.CV] UPDATED)",
    "abstract": "Multimodal large language models (MLLMs) have shown remarkable capabilities across a broad range of tasks but their knowledge and abilities in the geographic and geospatial domains are yet to be explored, despite potential wide-ranging benefits to navigation, environmental research, urban development, and disaster response. We conduct a series of experiments exploring various vision capabilities of MLLMs within these domains, particularly focusing on the frontier model GPT-4V, and benchmark its performance against open-source counterparts. Our methodology involves challenging these models with a small-scale geographic benchmark consisting of a suite of visual tasks, testing their abilities across a spectrum of complexity. The analysis uncovers not only where such models excel, including instances where they outperform humans, but also where they falter, providing a balanced view of their capabilities in the geographic domain. To enable the comparison and evaluation of future models, ou",
    "link": "http://arxiv.org/abs/2311.14656",
    "context": "Title: Charting New Territories: Exploring the Geographic and Geospatial Capabilities of Multimodal LLMs. (arXiv:2311.14656v3 [cs.CV] UPDATED)\nAbstract: Multimodal large language models (MLLMs) have shown remarkable capabilities across a broad range of tasks but their knowledge and abilities in the geographic and geospatial domains are yet to be explored, despite potential wide-ranging benefits to navigation, environmental research, urban development, and disaster response. We conduct a series of experiments exploring various vision capabilities of MLLMs within these domains, particularly focusing on the frontier model GPT-4V, and benchmark its performance against open-source counterparts. Our methodology involves challenging these models with a small-scale geographic benchmark consisting of a suite of visual tasks, testing their abilities across a spectrum of complexity. The analysis uncovers not only where such models excel, including instances where they outperform humans, but also where they falter, providing a balanced view of their capabilities in the geographic domain. To enable the comparison and evaluation of future models, ou",
    "path": "papers/23/11/2311.14656.json",
    "total_tokens": 906,
    "translated_title": "探索多模态LLMs的地理和地理空间能力：开辟新的领域",
    "translated_abstract": "多模态大语言模型（MLLM）在广泛的任务中展示了卓越的能力，但它们在地理和地理空间领域的知识和能力尚未被探索，尽管对导航、环境研究、城市发展和灾难响应等领域有潜在的广泛好处。我们进行了一系列的实验，探索MLLM在这些领域内的各种视觉能力，特别关注前沿模型GPT-4V，并将其性能与开源对比。我们的方法包括挑战这些模型，使用一个小规模的地理基准测试套件，测试它们在各种复杂性的任务中的能力。分析揭示了这些模型在哪些方面表现出色，包括它们超过人类的情况，但也揭示了它们何处失败，提供了对它们在地理领域能力的平衡观点。为了比较和评估未来模型，我们提供了开源的实验代码和数据集。",
    "tldr": "本论文探索了多模态大语言模型在地理和地理空间领域的能力，并通过实验分析了它们在各种视觉任务中的表现，发现了其卓越之处以及不足之处。",
    "en_tdlr": "This paper explores the capabilities of multimodal large language models (MLLMs) in the geographic and geospatial domains, and analyzes their performance in various visual tasks, highlighting their strengths and weaknesses."
}