{
    "title": "Towards more Practical Threat Models in Artificial Intelligence Security",
    "abstract": "arXiv:2311.09994v2 Announce Type: replace-cross  Abstract: Recent works have identified a gap between research and practice in artificial intelligence security: threats studied in academia do not always reflect the practical use and security risks of AI. For example, while models are often studied in isolation, they form part of larger ML pipelines in practice. Recent works also brought forward that adversarial manipulations introduced by academic attacks are impractical. We take a first step towards describing the full extent of this disparity. To this end, we revisit the threat models of the six most studied attacks in AI security research and match them to AI usage in practice via a survey with 271 industrial practitioners. On the one hand, we find that all existing threat models are indeed applicable. On the other hand, there are significant mismatches: research is often too generous with the attacker, assuming access to information not frequently available in real-world settings. ",
    "link": "https://arxiv.org/abs/2311.09994",
    "context": "Title: Towards more Practical Threat Models in Artificial Intelligence Security\nAbstract: arXiv:2311.09994v2 Announce Type: replace-cross  Abstract: Recent works have identified a gap between research and practice in artificial intelligence security: threats studied in academia do not always reflect the practical use and security risks of AI. For example, while models are often studied in isolation, they form part of larger ML pipelines in practice. Recent works also brought forward that adversarial manipulations introduced by academic attacks are impractical. We take a first step towards describing the full extent of this disparity. To this end, we revisit the threat models of the six most studied attacks in AI security research and match them to AI usage in practice via a survey with 271 industrial practitioners. On the one hand, we find that all existing threat models are indeed applicable. On the other hand, there are significant mismatches: research is often too generous with the attacker, assuming access to information not frequently available in real-world settings. ",
    "path": "papers/23/11/2311.09994.json",
    "total_tokens": 703,
    "translated_title": "在人工智能安全中更实用的威胁模型",
    "translated_abstract": "最近的研究发现，人工智能安全领域的研究和实践之间存在差距：学术界研究的威胁并不总是反映出AI的实际使用和安全风险。我们采取了第一步来描述这种差距的全部程度。为此，我们重新审视了AI安全研究中六种最常研究的攻击的威胁模型，并通过对271名工业从业者的调查将其与实践中的AI使用进行匹配。",
    "tldr": "学术界研究的AI安全威胁模型未能充分反映实际使用情况，存在重要不匹配，需要更加实用的威胁模型。",
    "en_tdlr": "Academic research on AI security threat models does not fully reflect practical usage, revealing significant mismatches and a need for more practical threat models."
}