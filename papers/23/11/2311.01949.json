{
    "title": "Hint-enhanced In-Context Learning wakes Large Language Models up for knowledge-intensive tasks. (arXiv:2311.01949v1 [cs.CL])",
    "abstract": "In-context learning (ICL) ability has emerged with the increasing scale of large language models (LLMs), enabling them to learn input-label mappings from demonstrations and perform well on downstream tasks. However, under the standard ICL setting, LLMs may sometimes neglect query-related information in demonstrations, leading to incorrect predictions. To address this limitation, we propose a new paradigm called Hint-enhanced In-Context Learning (HICL) to explore the power of ICL in open-domain question answering, an important form in knowledge-intensive tasks. HICL leverages LLMs' reasoning ability to extract query-related knowledge from demonstrations, then concatenates the knowledge to prompt LLMs in a more explicit way. Furthermore, we track the source of this knowledge to identify specific examples, and introduce a Hint-related Example Retriever (HER) to select informative examples for enhanced demonstrations. We evaluate HICL with HER on 3 open-domain QA benchmarks, and observe av",
    "link": "http://arxiv.org/abs/2311.01949",
    "context": "Title: Hint-enhanced In-Context Learning wakes Large Language Models up for knowledge-intensive tasks. (arXiv:2311.01949v1 [cs.CL])\nAbstract: In-context learning (ICL) ability has emerged with the increasing scale of large language models (LLMs), enabling them to learn input-label mappings from demonstrations and perform well on downstream tasks. However, under the standard ICL setting, LLMs may sometimes neglect query-related information in demonstrations, leading to incorrect predictions. To address this limitation, we propose a new paradigm called Hint-enhanced In-Context Learning (HICL) to explore the power of ICL in open-domain question answering, an important form in knowledge-intensive tasks. HICL leverages LLMs' reasoning ability to extract query-related knowledge from demonstrations, then concatenates the knowledge to prompt LLMs in a more explicit way. Furthermore, we track the source of this knowledge to identify specific examples, and introduce a Hint-related Example Retriever (HER) to select informative examples for enhanced demonstrations. We evaluate HICL with HER on 3 open-domain QA benchmarks, and observe av",
    "path": "papers/23/11/2311.01949.json",
    "total_tokens": 1080,
    "translated_title": "通过提示增强上下文学习使得大型语言模型在知识密集型任务中表现出色",
    "translated_abstract": "随着大型语言模型（LLM）规模的增加，上下文学习（ICL）能力已经出现，使得它们能够从示范中学习输入-标签映射，并在下游任务中表现良好。然而，在标准ICL设置下，LLM有时会忽略示范中与查询相关的信息，导致错误的预测。为了解决这个限制，我们提出了一种新的范例称为提示增强上下文学习（HICL），来探索ICL在知识密集型任务中的潜力，特别是在开放域问答中。HICL利用LLM的推理能力从示范中提取与查询相关的知识，然后将这些知识与LLM进行更明确的提示。此外，我们跟踪知识的来源，以识别特定的示例，并引入一个与提示相关的示例检索器（HER）来选择有信息量的示例进行增强示范。我们使用HER评估了HICL在3个开放域问答基准测试中，并观察到了显著的改进。",
    "tldr": "提示增强上下文学习（HICL）是一种新的范例，使得大型语言模型（LLM）在知识密集型任务中表现出色。HICL利用LLM的推理能力从示范中提取与查询相关的知识，并通过更明确的提示方式来增强LLM的学习。通过引入一个与提示相关的示例检索器（HER），我们还能选择有信息量的示例来增强示范。对于开放域问答，HICL在三个基准测试上实现了显著的改进。",
    "en_tdlr": "Hint-enhanced In-Context Learning (HICL) is a new paradigm that enables large language models (LLMs) to perform well on knowledge-intensive tasks. HICL leverages LLMs' reasoning ability to extract query-related knowledge from demonstrations and enhances LLMs' learning through more explicit prompts. By introducing a Hint-related Example Retriever (HER), informative examples can be selected to enhance demonstrations. HICL shows significant improvements on three open-domain QA benchmarks."
}