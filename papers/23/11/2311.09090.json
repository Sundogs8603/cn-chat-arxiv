{
    "title": "Social Bias Probing: Fairness Benchmarking for Language Models",
    "abstract": "arXiv:2311.09090v2 Announce Type: replace  Abstract: Large language models have been shown to encode a variety of social biases, which carries the risk of downstream harms. While the impact of these biases has been recognized, prior methods for bias evaluation have been limited to binary association tests on small datasets, offering a constrained view of the nature of societal biases within language models. In this paper, we propose an original framework for probing language models for societal biases. We collect a probing dataset to analyze language models' general associations, as well as along the axes of societal categories, identities, and stereotypes. To this end, we leverage a novel perplexity-based fairness score. We curate a large-scale benchmarking dataset addressing drawbacks and limitations of existing fairness collections, expanding to a variety of different identities and stereotypes. When comparing our methodology with prior work, we demonstrate that biases within langua",
    "link": "https://arxiv.org/abs/2311.09090",
    "context": "Title: Social Bias Probing: Fairness Benchmarking for Language Models\nAbstract: arXiv:2311.09090v2 Announce Type: replace  Abstract: Large language models have been shown to encode a variety of social biases, which carries the risk of downstream harms. While the impact of these biases has been recognized, prior methods for bias evaluation have been limited to binary association tests on small datasets, offering a constrained view of the nature of societal biases within language models. In this paper, we propose an original framework for probing language models for societal biases. We collect a probing dataset to analyze language models' general associations, as well as along the axes of societal categories, identities, and stereotypes. To this end, we leverage a novel perplexity-based fairness score. We curate a large-scale benchmarking dataset addressing drawbacks and limitations of existing fairness collections, expanding to a variety of different identities and stereotypes. When comparing our methodology with prior work, we demonstrate that biases within langua",
    "path": "papers/23/11/2311.09090.json",
    "total_tokens": 642,
    "translated_title": "社会偏见探测：语言模型的公平基准评估",
    "translated_abstract": "大型语言模型已被证明编码了各种社会偏见，这带来了下游风险。本文提出了一个用于探测语言模型中社会偏见的原创框架，包括对语言模型的一般关联以及社会类别、身份和刻板印象的分析。",
    "tldr": "本文提出了一个用于探测语言模型中社会偏见的原创框架，包括对一般关联和社会类别、身份以及刻板印象的分析。",
    "en_tdlr": "This paper proposes an original framework for probing societal biases in language models, analyzing general associations as well as societal categories, identities, and stereotypes."
}