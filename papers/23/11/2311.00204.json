{
    "title": "Continuous Training and Fine-tuning for Domain-Specific Language Models in Medical Question Answering. (arXiv:2311.00204v1 [cs.CL])",
    "abstract": "Large language models exhibit promising general capabilities but often lack specialized knowledge for domain-specific tasks. Developing domain experts from a base model enables a range of applications without prohibitive training costs. This work demonstrates a method using continuous training and instruction fine-tuning to rapidly adapt Llama 2 base models to the Chinese medical domain. We first conduct continuous training on 1B tokens from Chinese medical references to teach relevant vocabulary and knowledge. The models are then fine-tuned on 54K examples sourced from the Chinese National Medical Licensing Examination. Experiments on Chinese medical data confirm the effectiveness of this approach, producing a model comparable to GPT-3.5-turbo while using way less computational resource. The resulting domain-specific model could be useful for various Chinese medical applications. More broadly, this provides a template for domain-specific training of large language models in areas wher",
    "link": "http://arxiv.org/abs/2311.00204",
    "context": "Title: Continuous Training and Fine-tuning for Domain-Specific Language Models in Medical Question Answering. (arXiv:2311.00204v1 [cs.CL])\nAbstract: Large language models exhibit promising general capabilities but often lack specialized knowledge for domain-specific tasks. Developing domain experts from a base model enables a range of applications without prohibitive training costs. This work demonstrates a method using continuous training and instruction fine-tuning to rapidly adapt Llama 2 base models to the Chinese medical domain. We first conduct continuous training on 1B tokens from Chinese medical references to teach relevant vocabulary and knowledge. The models are then fine-tuned on 54K examples sourced from the Chinese National Medical Licensing Examination. Experiments on Chinese medical data confirm the effectiveness of this approach, producing a model comparable to GPT-3.5-turbo while using way less computational resource. The resulting domain-specific model could be useful for various Chinese medical applications. More broadly, this provides a template for domain-specific training of large language models in areas wher",
    "path": "papers/23/11/2311.00204.json",
    "total_tokens": 1043,
    "translated_title": "在医学问题回答中，领域特定语言模型的连续训练和微调",
    "translated_abstract": "大型语言模型具有良好的通用能力，但往往缺乏领域特定任务的专业知识。通过从基础模型中开发领域专家，可以在不会造成过高训练成本的情况下实现多种应用。本研究展示了一种使用连续训练和指导微调的方法，以快速适应中文医学领域的Llama 2基础模型。我们首先对来自中国医学参考资料的10亿个标记进行连续训练，以教授相关的词汇和知识。然后，我们在来自中国国家医疗执业医师资格考试的5.4万个示例上对模型进行微调。在中文医学数据上的实验验证了这种方法的有效性，生成了一个与GPT-3.5-turbo相媲美的模型，同时使用的计算资源要少得多。最终得到的领域特定模型可以在各种中文医学应用中发挥作用。更广泛地说，这为在缺乏专业知识的领域中进行大型语言模型的领域特定训练提供了一个模板。",
    "tldr": "本研究提出了一种连续训练和微调的方法，通过在中文医学数据上进行训练和微调，快速适应Llama 2基础模型到中文医学领域。实验结果表明，这种方法有效，生成的模型与GPT-3.5-turbo相媲美，但使用的计算资源更少。这为在不同领域进行大型语言模型的领域特定训练提供了一个模板。"
}