{
    "title": "The language of prompting: What linguistic properties make a prompt successful?. (arXiv:2311.01967v1 [cs.CL])",
    "abstract": "The latest generation of LLMs can be prompted to achieve impressive zero-shot or few-shot performance in many NLP tasks. However, since performance is highly sensitive to the choice of prompts, considerable effort has been devoted to crowd-sourcing prompts or designing methods for prompt optimisation. Yet, we still lack a systematic understanding of how linguistic properties of prompts correlate with task performance. In this work, we investigate how LLMs of different sizes, pre-trained and instruction-tuned, perform on prompts that are semantically equivalent, but vary in linguistic structure. We investigate both grammatical properties such as mood, tense, aspect and modality, as well as lexico-semantic variation through the use of synonyms. Our findings contradict the common assumption that LLMs achieve optimal performance on lower perplexity prompts that reflect language use in pretraining or instruction-tuning data. Prompts transfer poorly between datasets or models, and performanc",
    "link": "http://arxiv.org/abs/2311.01967",
    "context": "Title: The language of prompting: What linguistic properties make a prompt successful?. (arXiv:2311.01967v1 [cs.CL])\nAbstract: The latest generation of LLMs can be prompted to achieve impressive zero-shot or few-shot performance in many NLP tasks. However, since performance is highly sensitive to the choice of prompts, considerable effort has been devoted to crowd-sourcing prompts or designing methods for prompt optimisation. Yet, we still lack a systematic understanding of how linguistic properties of prompts correlate with task performance. In this work, we investigate how LLMs of different sizes, pre-trained and instruction-tuned, perform on prompts that are semantically equivalent, but vary in linguistic structure. We investigate both grammatical properties such as mood, tense, aspect and modality, as well as lexico-semantic variation through the use of synonyms. Our findings contradict the common assumption that LLMs achieve optimal performance on lower perplexity prompts that reflect language use in pretraining or instruction-tuning data. Prompts transfer poorly between datasets or models, and performanc",
    "path": "papers/23/11/2311.01967.json",
    "total_tokens": 899,
    "translated_title": "提示的语言：什么语言属性使得提示成功？",
    "translated_abstract": "最新一代的语言模型可以通过提示来在许多自然语言处理任务中实现令人印象深刻的零样本或少样本性能。然而，由于性能对提示的选择非常敏感，人们付出了相当大的努力来进行众包提示或设计用于优化提示的方法。然而，我们仍然缺乏对提示的语言属性与任务性能之间的系统理解。在这项工作中，我们研究了不同大小的预训练和指导调优的语言模型在语义上等效但在语言结构上有所不同的提示上的表现。我们通过调查语法属性（如情态、时态、语态和语气）以及通过使用同义词引入词汇-语义变化。我们的发现与常见假设相矛盾，即语言模型在低困惑度的提示上达到最佳性能，这些提示反映了预训练或指导调优数据中的语言使用。提示在数据集或模型之间转移效果不佳，性能有所下降。",
    "tldr": "本文研究了不同大小的预训练和指导调优的语言模型在语言结构上有所不同的提示上的表现，结果显示语言模型在性能上对提示的语言属性有较高的敏感性。",
    "en_tdlr": "This paper investigates how linguistic properties of prompts, such as grammatical properties and lexico-semantic variation, affect the performance of different sizes of pre-trained and instruction-tuned language models. The findings contradict the common assumption and reveal the high sensitivity of language models to prompt language attributes."
}