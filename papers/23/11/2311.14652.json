{
    "title": "One Pass Streaming Algorithm for Super Long Token Attention Approximation in Sublinear Space",
    "abstract": "Attention computation takes both the time complexity of $O(n^2)$ and the space complexity of $O(n^2)$ simultaneously, which makes deploying Large Language Models (LLMs) in streaming applications that involve long contexts requiring substantial computational resources. In recent OpenAI DevDay (Nov 6, 2023), OpenAI released a new model that is able to support a 128K-long document, in our paper, we focus on the memory-efficient issue when context length $n$ is much greater than 128K ($n \\gg 2^d$). Considering a single-layer self-attention with Query, Key, and Value matrices $Q, K, V \\in \\mathbb{R}^{n \\times d}$, the polynomial method approximates the attention output $T \\in \\mathbb{R}^{n \\times d}$. It accomplishes this by constructing $U_1, U_2 \\in \\mathbb{R}^{n \\times t}$ to expedite attention ${\\sf Attn}(Q, K, V)$ computation within $n^{1+o(1)}$ time executions. Despite this, computing the approximated attention matrix $U_1U_2^\\top \\in \\mathbb{R}^{n \\times n}$ still necessitates $O(n^2",
    "link": "https://arxiv.org/abs/2311.14652",
    "context": "Title: One Pass Streaming Algorithm for Super Long Token Attention Approximation in Sublinear Space\nAbstract: Attention computation takes both the time complexity of $O(n^2)$ and the space complexity of $O(n^2)$ simultaneously, which makes deploying Large Language Models (LLMs) in streaming applications that involve long contexts requiring substantial computational resources. In recent OpenAI DevDay (Nov 6, 2023), OpenAI released a new model that is able to support a 128K-long document, in our paper, we focus on the memory-efficient issue when context length $n$ is much greater than 128K ($n \\gg 2^d$). Considering a single-layer self-attention with Query, Key, and Value matrices $Q, K, V \\in \\mathbb{R}^{n \\times d}$, the polynomial method approximates the attention output $T \\in \\mathbb{R}^{n \\times d}$. It accomplishes this by constructing $U_1, U_2 \\in \\mathbb{R}^{n \\times t}$ to expedite attention ${\\sf Attn}(Q, K, V)$ computation within $n^{1+o(1)}$ time executions. Despite this, computing the approximated attention matrix $U_1U_2^\\top \\in \\mathbb{R}^{n \\times n}$ still necessitates $O(n^2",
    "path": "papers/23/11/2311.14652.json",
    "total_tokens": 1022,
    "translated_title": "一种超长Token注意力近似的单次流算法",
    "translated_abstract": "注意力计算同时具有$O(n^2)$的时间复杂度和$O(n^2)$的空间复杂度，这使得在需要大量计算资源的流应用中部署大型语言模型(Large Language Models，LLMs)变得困难。在最近的OpenAI DevDay（2023年11月6日），OpenAI发布了一种能够支持128K长文档的新模型，在我们的论文中，我们关注的是当上下文长度$n$远大于128K ($n \\gg 2^d$)时的内存有效问题。考虑到具有 Query、Key 和 Value 矩阵$Q, K, V \\in \\mathbb{R}^{n \\times d}$的单层自注意力，多项式方法近似了注意力输出$T \\in \\mathbb{R}^{n \\times d}$。它通过构建$U_1, U_2 \\in \\mathbb{R}^{n \\times t}$在$n^{1+o(1)}$次时间执行内加速注意力计算${\\sf Attn}(Q, K, V)$。尽管如此，计算近似的注意力矩阵$U_1U_2^\\top \\in \\mathbb{R}^{n \\times n}$仍需要$O(n^2)$的空间。",
    "tldr": "本文研究了在超长上下文下内存效率的问题，提出一种用于超长Token注意力近似的单次流算法，通过构建矩阵$U_1, U_2$加速注意力计算，解决了部署大型语言模型时的计算资源问题。",
    "en_tdlr": "This paper addresses the memory-efficiency issue in super long contexts and proposes a one-pass streaming algorithm for approximating super long token attention. By constructing matrices U1 and U2, the attention computation is accelerated, solving the computational resource problem in deploying large language models."
}