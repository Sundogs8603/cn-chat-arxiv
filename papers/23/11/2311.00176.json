{
    "title": "ChipNeMo: Domain-Adapted LLMs for Chip Design. (arXiv:2311.00176v1 [cs.CL])",
    "abstract": "ChipNeMo aims to explore the applications of large language models (LLMs) for industrial chip design. Instead of directly deploying off-the-shelf commercial or open-source LLMs, we instead adopt the following domain adaptation techniques: custom tokenizers, domain-adaptive continued pretraining, supervised fine-tuning (SFT) with domain-specific instructions, and domain-adapted retrieval models. We evaluate these methods on three selected LLM applications for chip design: an engineering assistant chatbot, EDA script generation, and bug summarization and analysis. Our results show that these domain adaptation techniques enable significant LLM performance improvements over general-purpose base models across the three evaluated applications, enabling up to 5x model size reduction with similar or better performance on a range of design tasks. Our findings also indicate that there's still room for improvement between our current results and ideal outcomes. We believe that further investigati",
    "link": "http://arxiv.org/abs/2311.00176",
    "context": "Title: ChipNeMo: Domain-Adapted LLMs for Chip Design. (arXiv:2311.00176v1 [cs.CL])\nAbstract: ChipNeMo aims to explore the applications of large language models (LLMs) for industrial chip design. Instead of directly deploying off-the-shelf commercial or open-source LLMs, we instead adopt the following domain adaptation techniques: custom tokenizers, domain-adaptive continued pretraining, supervised fine-tuning (SFT) with domain-specific instructions, and domain-adapted retrieval models. We evaluate these methods on three selected LLM applications for chip design: an engineering assistant chatbot, EDA script generation, and bug summarization and analysis. Our results show that these domain adaptation techniques enable significant LLM performance improvements over general-purpose base models across the three evaluated applications, enabling up to 5x model size reduction with similar or better performance on a range of design tasks. Our findings also indicate that there's still room for improvement between our current results and ideal outcomes. We believe that further investigati",
    "path": "papers/23/11/2311.00176.json",
    "total_tokens": 889,
    "translated_title": "ChipNeMo: 用于芯片设计的领域自适应LLMs",
    "translated_abstract": "ChipNeMo旨在探索大型语言模型（LLMs）在工业芯片设计中的应用。我们不直接使用商业或开源LLMs，而是采用以下领域自适应技术：定制分词器、领域自适应持续预训练、带有领域特定指令的监督微调（SFT）和领域自适应检索模型。我们在芯片设计的三个选定LLM应用上评估了这些方法：工程助手聊天机器人、EDA脚本生成以及缺陷摘要和分析。我们的结果显示，这些领域自适应技术使LLM在这三个应用中性能大幅提升，在各种设计任务上可以实现高达5倍的模型尺寸缩减，同时具有类似或更好的性能。我们的研究结果还表明，当前的结果和理想结果之间还有改进的空间。我们相信进一步的研究将有助于解决这个问题。",
    "tldr": "ChipNeMo通过领域自适应技术，实现了在工业芯片设计中大幅提升LLM性能，同时减小了模型尺寸，在工程助手、脚本生成和缺陷分析等方面具有良好表现。",
    "en_tdlr": "ChipNeMo achieves significant LLM performance improvements in industrial chip design through domain adaptation techniques, enabling model size reduction and demonstrating good performance in engineering assistance, script generation, and bug analysis."
}