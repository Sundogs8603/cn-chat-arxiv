{
    "title": "On the effect of curriculum learning with developmental data for grammar acquisition. (arXiv:2311.00128v1 [cs.CL])",
    "abstract": "This work explores the degree to which grammar acquisition is driven by language `simplicity' and the source modality (speech vs. text) of data. Using BabyBERTa as a probe, we find that grammar acquisition is largely driven by exposure to speech data, and in particular through exposure to two of the BabyLM training corpora: AO-Childes and Open Subtitles. We arrive at this finding by examining various ways of presenting input data to our model. First, we assess the impact of various sequence-level complexity based curricula. We then examine the impact of learning over `blocks' -- covering spans of text that are balanced for the number of tokens in each of the source corpora (rather than number of lines). Finally, we explore curricula that vary the degree to which the model is exposed to different corpora. In all cases, we find that over-exposure to AO-Childes and Open Subtitles significantly drives performance. We verify these findings through a comparable control dataset in which expos",
    "link": "http://arxiv.org/abs/2311.00128",
    "context": "Title: On the effect of curriculum learning with developmental data for grammar acquisition. (arXiv:2311.00128v1 [cs.CL])\nAbstract: This work explores the degree to which grammar acquisition is driven by language `simplicity' and the source modality (speech vs. text) of data. Using BabyBERTa as a probe, we find that grammar acquisition is largely driven by exposure to speech data, and in particular through exposure to two of the BabyLM training corpora: AO-Childes and Open Subtitles. We arrive at this finding by examining various ways of presenting input data to our model. First, we assess the impact of various sequence-level complexity based curricula. We then examine the impact of learning over `blocks' -- covering spans of text that are balanced for the number of tokens in each of the source corpora (rather than number of lines). Finally, we explore curricula that vary the degree to which the model is exposed to different corpora. In all cases, we find that over-exposure to AO-Childes and Open Subtitles significantly drives performance. We verify these findings through a comparable control dataset in which expos",
    "path": "papers/23/11/2311.00128.json",
    "total_tokens": 894,
    "translated_title": "关于使用发展性数据进行语法习得的课程学习效果研究",
    "translated_abstract": "本研究探讨了语法习得在语言“简单性”和数据的来源模态（语音 vs 文本）方面的影响程度。通过使用BabyBERTa作为探针，我们发现语法习得主要受到对语音数据的暴露的驱动，尤其是通过对两个BabyLM训练数据集（AO-Childes和Open Subtitles）的暴露。我们通过检查将输入数据以不同方式呈现给模型的方法得出了这一发现。首先，我们评估了基于序列级复杂性的学习计划的影响。然后，我们研究了学习“块”的影响——这些块覆盖了源数据集中每个语料库中每个标记数量平衡的文本范围。最后，我们探索了不同程度地让模型接触不同语料库的学习计划。在所有情况下，我们发现过度接触AO-Childes和Open Subtitles显著提高了性能。我们通过一个可比较的控制数据集来验证这些发现，该数据集中曝光程度较低。",
    "tldr": "这项研究发现语法习得主要受到对语音数据的暴露驱动，并通过课程学习方法进一步提高其性能。",
    "en_tdlr": "This research finds that grammar acquisition is mainly driven by exposure to speech data and further improved through curriculum learning."
}