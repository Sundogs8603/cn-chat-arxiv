{
    "title": "In Search of a Data Transformation That Accelerates Neural Field Training",
    "abstract": "arXiv:2311.17094v2 Announce Type: replace  Abstract: Neural field is an emerging paradigm in data representation that trains a neural network to approximate the given signal. A key obstacle that prevents its widespread adoption is the encoding speed-generating neural fields requires an overfitting of a neural network, which can take a significant number of SGD steps to reach the desired fidelity level. In this paper, we delve into the impacts of data transformations on the speed of neural field training, specifically focusing on how permuting pixel locations affect the convergence speed of SGD. Counterintuitively, we find that randomly permuting the pixel locations can considerably accelerate the training. To explain this phenomenon, we examine the neural field training through the lens of PSNR curves, loss landscapes, and error patterns. Our analyses suggest that the random pixel permutations remove the easy-to-fit patterns, which facilitate easy optimization in the early stage but hi",
    "link": "https://arxiv.org/abs/2311.17094",
    "context": "Title: In Search of a Data Transformation That Accelerates Neural Field Training\nAbstract: arXiv:2311.17094v2 Announce Type: replace  Abstract: Neural field is an emerging paradigm in data representation that trains a neural network to approximate the given signal. A key obstacle that prevents its widespread adoption is the encoding speed-generating neural fields requires an overfitting of a neural network, which can take a significant number of SGD steps to reach the desired fidelity level. In this paper, we delve into the impacts of data transformations on the speed of neural field training, specifically focusing on how permuting pixel locations affect the convergence speed of SGD. Counterintuitively, we find that randomly permuting the pixel locations can considerably accelerate the training. To explain this phenomenon, we examine the neural field training through the lens of PSNR curves, loss landscapes, and error patterns. Our analyses suggest that the random pixel permutations remove the easy-to-fit patterns, which facilitate easy optimization in the early stage but hi",
    "path": "papers/23/11/2311.17094.json",
    "total_tokens": 796,
    "translated_title": "寻找加速神经场训练的数据转换方法",
    "translated_abstract": "神经场是一种新兴的数据表示范式，通过训练神经网络来逼近给定信号。阻碍其广泛应用的一个关键障碍是生成神经场的编码速度需要神经网络的过度拟合，这可能需要大量的SGD步骤才能达到所需的保真度水平。本文探讨了数据转换对神经场训练速度的影响，特别关注对像素位置进行随机排列如何影响SGD的收敛速度。出乎意料的是，我们发现随机排列像素位置可以显著加速训练。为了解释这一现象，我们通过PSNR曲线、损失地形和错误模式来审视神经场训练。我们的分析表明，随机像素置换消除了易匹配的模式，这有助于在早期阶段进行简单的优化。",
    "tldr": "随机像素置换可以显著加速神经场训练，通过消除易匹配模式来促进早期阶段的优化"
}