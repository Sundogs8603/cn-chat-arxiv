{
    "title": "Bridging the Gap: Addressing Discrepancies in Diffusion Model Training for Classifier-Free Guidance. (arXiv:2311.00938v1 [cs.LG])",
    "abstract": "Diffusion models have emerged as a pivotal advancement in generative models, setting new standards to the quality of the generated instances. In the current paper we aim to underscore a discrepancy between conventional training methods and the desired conditional sampling behavior of these models. While the prevalent classifier-free guidance technique works well, it's not without flaws. At higher values for the guidance scale parameter $w$, we often get out of distribution samples and mode collapse, whereas at lower values for $w$ we may not get the desired specificity. To address these challenges, we introduce an updated loss function that better aligns training objectives with sampling behaviors. Experimental validation with FID scores on CIFAR-10 elucidates our method's ability to produce higher quality samples with fewer sampling timesteps, and be more robust to the choice of guidance scale $w$. We also experiment with fine-tuning Stable Diffusion on the proposed loss, to provide e",
    "link": "http://arxiv.org/abs/2311.00938",
    "context": "Title: Bridging the Gap: Addressing Discrepancies in Diffusion Model Training for Classifier-Free Guidance. (arXiv:2311.00938v1 [cs.LG])\nAbstract: Diffusion models have emerged as a pivotal advancement in generative models, setting new standards to the quality of the generated instances. In the current paper we aim to underscore a discrepancy between conventional training methods and the desired conditional sampling behavior of these models. While the prevalent classifier-free guidance technique works well, it's not without flaws. At higher values for the guidance scale parameter $w$, we often get out of distribution samples and mode collapse, whereas at lower values for $w$ we may not get the desired specificity. To address these challenges, we introduce an updated loss function that better aligns training objectives with sampling behaviors. Experimental validation with FID scores on CIFAR-10 elucidates our method's ability to produce higher quality samples with fewer sampling timesteps, and be more robust to the choice of guidance scale $w$. We also experiment with fine-tuning Stable Diffusion on the proposed loss, to provide e",
    "path": "papers/23/11/2311.00938.json",
    "total_tokens": 951,
    "translated_title": "缩小差距：解决分类模型训练中的差异问题以实现无分类器引导",
    "translated_abstract": "扩散模型已经成为生成模型中的一个重要进展，为生成的实例质量设定了新的标准。本文旨在强调传统训练方法与这些模型所期望的条件采样行为之间存在的差异。虽然流行的无分类器引导技术效果不错，但也存在一些缺陷。在引导规模参数$w$取较高值时，我们经常得到分布之外的样本和模式崩溃，而在$w$取较低值时，可能无法获得所期望的特异性。为了解决这些挑战，我们引入了一种更新的损失函数，更好地将训练目标与采样行为对齐。在CIFAR-10上的FID分数的实验证明了我们的方法能够以较少的采样时间步长生成更高质量的样本，并且对于引导规模$w$的选择更具鲁棒性。我们还尝试了在提出的损失上对稳定性扩散进行微调，以提供e。",
    "tldr": "本文介绍了一种更新的损失函数，以更好地对齐传统训练方法与扩散模型所期望的条件采样行为之间的差异。实验证明该方法能够以更少的采样时间步长生成更高质量的样本，并对于引导规模的选择更具鲁棒性。",
    "en_tdlr": "This paper introduces an updated loss function to better align traditional training methods with the desired conditional sampling behavior of diffusion models. Experimental results demonstrate that this method can generate higher quality samples with fewer sampling timesteps and is more robust in the choice of guidance scale."
}