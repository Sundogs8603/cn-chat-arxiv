{
    "title": "DiffDub: Person-generic Visual Dubbing Using Inpainting Renderer with Diffusion Auto-encoder. (arXiv:2311.01811v1 [cs.CV])",
    "abstract": "Generating high-quality and person-generic visual dubbing remains a challenge. Recent innovation has seen the advent of a two-stage paradigm, decoupling the rendering and lip synchronization process facilitated by intermediate representation as a conduit. Still, previous methodologies rely on rough landmarks or are confined to a single speaker, thus limiting their performance. In this paper, we propose DiffDub: Diffusion-based dubbing. We first craft the Diffusion auto-encoder by an inpainting renderer incorporating a mask to delineate editable zones and unaltered regions. This allows for seamless filling of the lower-face region while preserving the remaining parts. Throughout our experiments, we encountered several challenges. Primarily, the semantic encoder lacks robustness, constricting its ability to capture high-level features. Besides, the modeling ignored facial positioning, causing mouth or nose jitters across frames. To tackle these issues, we employ versatile strategies, inc",
    "link": "http://arxiv.org/abs/2311.01811",
    "context": "Title: DiffDub: Person-generic Visual Dubbing Using Inpainting Renderer with Diffusion Auto-encoder. (arXiv:2311.01811v1 [cs.CV])\nAbstract: Generating high-quality and person-generic visual dubbing remains a challenge. Recent innovation has seen the advent of a two-stage paradigm, decoupling the rendering and lip synchronization process facilitated by intermediate representation as a conduit. Still, previous methodologies rely on rough landmarks or are confined to a single speaker, thus limiting their performance. In this paper, we propose DiffDub: Diffusion-based dubbing. We first craft the Diffusion auto-encoder by an inpainting renderer incorporating a mask to delineate editable zones and unaltered regions. This allows for seamless filling of the lower-face region while preserving the remaining parts. Throughout our experiments, we encountered several challenges. Primarily, the semantic encoder lacks robustness, constricting its ability to capture high-level features. Besides, the modeling ignored facial positioning, causing mouth or nose jitters across frames. To tackle these issues, we employ versatile strategies, inc",
    "path": "papers/23/11/2311.01811.json",
    "total_tokens": 977,
    "translated_title": "DiffDub: 使用扩散自动编码器和修复渲染器的通用演员视觉配音",
    "translated_abstract": "生成高质量且通用的视觉配音仍然是一个挑战。最近的创新看到了一个两阶段的范例的出现，通过中间表示解耦渲染和口型同步过程。然而，先前的方法依赖于粗糙的标记点或局限于单一的说话人，从而限制了它们的性能。在本文中，我们提出了DiffDub：基于扩散的配音方法。首先，我们使用一个修复渲染器和一个蒙版来绘制Diffusion自动编码器，以界定可编辑区域和未更改区域。这样可以在保留其余部分的同时无缝填充下半脸区域。在我们的实验中，我们遇到了一些挑战。首先，语义编码器缺乏鲁棒性，限制了它捕捉高级特征的能力。此外，建模忽略了面部定位，导致帧间口或鼻子抖动。为了解决这些问题，我们采用了多功能的策略，包括...",
    "tldr": "本论文提出了一种名为DiffDub的基于扩散的配音方法，使用扩散自动编码器和修复渲染器，在保留其他部分的同时无缝填充下半脸区域。通过使用多种策略，解决了语义编码和面部定位的挑战，提高了配音的质量和通用性。",
    "en_tdlr": "This paper proposes a diffusion-based dubbing method called DiffDub, which uses a diffusion auto-encoder and an inpainting renderer to seamlessly fill the lower-face region while preserving the remaining parts. By employing versatile strategies, the challenges of semantic encoding and facial positioning are addressed, improving the quality and generality of dubbing."
}