{
    "title": "REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource Constraints",
    "abstract": "arXiv:2311.13349v2 Announce Type: replace  Abstract: Deep models deployed on edge devices frequently encounter resource variability, which arises from fluctuating energy levels, timing constraints, or prioritization of other critical tasks within the system. State-of-the-art machine learning pipelines generate resource-agnostic models, not capable to adapt at runtime. In this work we introduce Resource-Efficient Deep Subnetworks (REDS) to tackle model adaptation to variable resources. In contrast to the state-of-the-art, REDS use structured sparsity constructively by exploiting permutation invariance of neurons, which allows for hardware-specific optimizations. Specifically, REDS achieve computational efficiency by (1) skipping sequential computational blocks identified by a novel iterative knapsack optimizer, and (2) leveraging simple math to re-arrange the order of operations in REDS computational graph to take advantage of the data cache. REDS support conventional deep networks freq",
    "link": "https://arxiv.org/abs/2311.13349",
    "context": "Title: REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource Constraints\nAbstract: arXiv:2311.13349v2 Announce Type: replace  Abstract: Deep models deployed on edge devices frequently encounter resource variability, which arises from fluctuating energy levels, timing constraints, or prioritization of other critical tasks within the system. State-of-the-art machine learning pipelines generate resource-agnostic models, not capable to adapt at runtime. In this work we introduce Resource-Efficient Deep Subnetworks (REDS) to tackle model adaptation to variable resources. In contrast to the state-of-the-art, REDS use structured sparsity constructively by exploiting permutation invariance of neurons, which allows for hardware-specific optimizations. Specifically, REDS achieve computational efficiency by (1) skipping sequential computational blocks identified by a novel iterative knapsack optimizer, and (2) leveraging simple math to re-arrange the order of operations in REDS computational graph to take advantage of the data cache. REDS support conventional deep networks freq",
    "path": "papers/23/11/2311.13349.json",
    "total_tokens": 877,
    "translated_title": "REDS: 资源高效的深度子网络用于动态资源约束",
    "translated_abstract": "部署在边缘设备上的深度模型经常遇到资源变化，这源于能量水平波动、时间约束或系统中其他关键任务的优先级。目前的机器学习流水线生成的是资源不可知的模型，并不能在运行时进行调整。在这项工作中，我们引入了Resource-Efficient Deep Subnetworks (REDS)来应对可变资源下的模型适应性。与最先进技术相比，REDS利用结构化稀疏性，通过利用神经元的排列不变性，从而允许硬件特定的优化。具体来说，REDS通过（1）跳过由新颖的迭代背包优化器识别的顺序计算块，以及（2）利用简单的数学重新安排REDS计算图中操作的顺序，以利用数据缓存而实现计算效率。REDS支持传统的深度网络频率。",
    "tldr": "本论文提出了一种名为REDS的资源高效深度子网络，通过利用神经元的排列不变性和新颖的迭代背包优化器来实现模型在不同资源约束下的自适应性，并通过优化计算块和重新安排操作顺序等方法提高计算效率。",
    "en_tdlr": "This paper proposes REDS, a resource-efficient deep subnetwork that achieves model adaptation to variable resource constraints by utilizing permutation invariance of neurons and a novel iterative knapsack optimizer, and enhances computational efficiency by optimizing computational blocks and rearranging operation orders."
}