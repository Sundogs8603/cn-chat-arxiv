{
    "title": "On the Convergence of Encoder-only Shallow Transformers. (arXiv:2311.01575v1 [cs.LG])",
    "abstract": "In this paper, we aim to build the global convergence theory of encoder-only shallow Transformers under a realistic setting from the perspective of architectures, initialization, and scaling under a finite width regime. The difficulty lies in how to tackle the softmax in self-attention mechanism, the core ingredient of Transformer. In particular, we diagnose the scaling scheme, carefully tackle the input/output of softmax, and prove that quadratic overparameterization is sufficient for global convergence of our shallow Transformers under commonly-used He/LeCun initialization in practice. Besides, neural tangent kernel (NTK) based analysis is also given, which facilitates a comprehensive comparison. Our theory demonstrates the separation on the importance of different scaling schemes and initialization. We believe our results can pave the way for a better understanding of modern Transformers, particularly on training dynamics.",
    "link": "http://arxiv.org/abs/2311.01575",
    "context": "Title: On the Convergence of Encoder-only Shallow Transformers. (arXiv:2311.01575v1 [cs.LG])\nAbstract: In this paper, we aim to build the global convergence theory of encoder-only shallow Transformers under a realistic setting from the perspective of architectures, initialization, and scaling under a finite width regime. The difficulty lies in how to tackle the softmax in self-attention mechanism, the core ingredient of Transformer. In particular, we diagnose the scaling scheme, carefully tackle the input/output of softmax, and prove that quadratic overparameterization is sufficient for global convergence of our shallow Transformers under commonly-used He/LeCun initialization in practice. Besides, neural tangent kernel (NTK) based analysis is also given, which facilitates a comprehensive comparison. Our theory demonstrates the separation on the importance of different scaling schemes and initialization. We believe our results can pave the way for a better understanding of modern Transformers, particularly on training dynamics.",
    "path": "papers/23/11/2311.01575.json",
    "total_tokens": 804,
    "translated_title": "关于仅使用编码器的浅层Transformer收敛性的研究",
    "translated_abstract": "本文旨在从架构、初始化和缩放的角度出发，在有限宽度的条件下，在现实场景下构建仅使用编码器的浅层Transformer的全局收敛理论。难点在于如何处理Transformer的核心组成部分，即自注意力机制中的softmax。特别地，我们诊断了缩放方案，仔细处理了softmax的输入/输出，并证明了在实践中常用的He/LeCun初始化下，我们的浅层Transformer的全局收敛仅需要二次超参数化。此外，本文还提供了基于神经切换核（NTK）的分析，这有助于全面比较。我们的理论展示了不同缩放方案和初始化的重要性分离。我们相信我们的结果可以为更好地理解现代Transformer，特别是训练动力学，铺平道路。",
    "tldr": "本研究旨在构建仅使用编码器的浅层Transformer在有限宽度条件下的全局收敛理论，并通过处理softmax的输入/输出和证明二次超参数化的有效性来解决其收敛困难。"
}