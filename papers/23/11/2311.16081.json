{
    "title": "ViT-Lens: Towards Omni-modal Representations",
    "abstract": "arXiv:2311.16081v2 Announce Type: replace-cross  Abstract: Aiming to advance AI agents, large foundation models significantly improve reasoning and instruction execution, yet the current focus on vision and language neglects the potential of perceiving diverse modalities in open-world environments. However, the success of data-driven vision and language models is costly or even infeasible to be reproduced for rare modalities. In this paper, we present ViT-Lens-2 that facilitates efficient omni-modal representation learning by perceiving novel modalities with a pretrained ViT and aligning them to a pre-defined space. Specifically, the modality-specific lens is tuned to project any-modal signals to an intermediate embedding space, which are then processed by a strong ViT with pre-trained visual knowledge. The encoded representations are optimized toward aligning with the modal-independent space, pre-defined by off-the-shelf foundation models. ViT-Lens-2 provides a unified solution for re",
    "link": "https://arxiv.org/abs/2311.16081",
    "context": "Title: ViT-Lens: Towards Omni-modal Representations\nAbstract: arXiv:2311.16081v2 Announce Type: replace-cross  Abstract: Aiming to advance AI agents, large foundation models significantly improve reasoning and instruction execution, yet the current focus on vision and language neglects the potential of perceiving diverse modalities in open-world environments. However, the success of data-driven vision and language models is costly or even infeasible to be reproduced for rare modalities. In this paper, we present ViT-Lens-2 that facilitates efficient omni-modal representation learning by perceiving novel modalities with a pretrained ViT and aligning them to a pre-defined space. Specifically, the modality-specific lens is tuned to project any-modal signals to an intermediate embedding space, which are then processed by a strong ViT with pre-trained visual knowledge. The encoded representations are optimized toward aligning with the modal-independent space, pre-defined by off-the-shelf foundation models. ViT-Lens-2 provides a unified solution for re",
    "path": "papers/23/11/2311.16081.json",
    "total_tokens": 724,
    "translated_title": "ViT-Lens: 迈向全模态表示",
    "translated_abstract": "旨在推进AI智能体，大型基础模型显著改善推理和指令执行，然而，目前对视觉和语言的关注忽视了在开放世界环境中感知多样的模态的潜力。然而，基于数据驱动的视觉和语言模型的成功成本高昂，甚至难以为稀有模态重现。本文介绍了ViT-Lens-2，通过感知使用预训练的ViT提取新颖模态并将其对准到预定义空间，以促进高效的全模态表示学习。",
    "tldr": "ViT-Lens-2利用预先训练的ViT感知新颖模态，并将其对准到预定义空间，为高效的全模态表示学习提供了统一解决方案。",
    "en_tdlr": "ViT-Lens-2 leverages pretrained ViT to perceive novel modalities and aligns them to predefined space, providing a unified solution for efficient omni-modal representation learning."
}