{
    "title": "On Retrieval Augmentation and the Limitations of Language Model Training",
    "abstract": "arXiv:2311.09615v2 Announce Type: replace  Abstract: Augmenting a language model (LM) with $k$-nearest neighbors ($k$NN) retrieval on its training data alone can decrease its perplexity, though the underlying reasons for this remain elusive. In this work, we rule out one previously posited possibility -- the \"softmax bottleneck.\" We then create a new dataset to evaluate LM generalization ability in the setting where training data contains additional information that is not causally relevant. This task is challenging even for GPT-3.5 Turbo. We show that, for both GPT-2 and Mistral 7B, $k$NN retrieval augmentation consistently improves performance in this setting. Finally, to make $k$NN retrieval more accessible, we propose using a multi-layer perceptron model that maps datastore keys to values as a drop-in replacement for traditional retrieval. This reduces storage costs by over 25x.",
    "link": "https://arxiv.org/abs/2311.09615",
    "context": "Title: On Retrieval Augmentation and the Limitations of Language Model Training\nAbstract: arXiv:2311.09615v2 Announce Type: replace  Abstract: Augmenting a language model (LM) with $k$-nearest neighbors ($k$NN) retrieval on its training data alone can decrease its perplexity, though the underlying reasons for this remain elusive. In this work, we rule out one previously posited possibility -- the \"softmax bottleneck.\" We then create a new dataset to evaluate LM generalization ability in the setting where training data contains additional information that is not causally relevant. This task is challenging even for GPT-3.5 Turbo. We show that, for both GPT-2 and Mistral 7B, $k$NN retrieval augmentation consistently improves performance in this setting. Finally, to make $k$NN retrieval more accessible, we propose using a multi-layer perceptron model that maps datastore keys to values as a drop-in replacement for traditional retrieval. This reduces storage costs by over 25x.",
    "path": "papers/23/11/2311.09615.json",
    "total_tokens": 824,
    "translated_title": "关于检索增强和语言模型训练的局限性",
    "translated_abstract": "将语言模型（LM）与其训练数据上的$k$-nearest neighbors ($k$NN)检索相结合可以降低其困惑度，尽管其背后的原因尚不清楚。本文排除了先前提出的一个可能性-“softmax瓶颈”。我们创造了一个新数据集，用于评估LM在训练数据中包含额外非因果相关信息的情况下的泛化能力。即使对于GPT-3.5 Turbo来说，这项任务也具有挑战性。我们展示了对于GPT-2和Mistral 7B，$k$NN检索增强在这种情况下始终提升了性能。最后，为了使$k$NN检索更易用，提出了使用多层感知机模型将数据存储键映射到值，作为传统检索的替代方案。这样可以将存储成本降低超过25倍。",
    "tldr": "将$k$NN检索与语言模型相结合可以降低困惑度，并提出了使用多层感知机模型的方法，将存储成本降低超过25倍。",
    "en_tdlr": "Combining $k$NN retrieval with language models can decrease perplexity, and a method using multi-layer perceptron model is proposed to reduce storage costs by over 25 times."
}