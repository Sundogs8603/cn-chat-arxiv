{
    "title": "Latent Space Translation via Semantic Alignment. (arXiv:2311.00664v1 [cs.LG])",
    "abstract": "While different neural models often exhibit latent spaces that are alike when exposed to semantically related data, this intrinsic similarity is not always immediately discernible. Towards a better understanding of this phenomenon, our work shows how representations learned from these neural modules can be translated between different pre-trained networks via simpler transformations than previously thought. An advantage of this approach is the ability to estimate these transformations using standard, well-understood algebraic procedures that have closed-form solutions. Our method directly estimates a transformation between two given latent spaces, thereby enabling effective stitching of encoders and decoders without additional training. We extensively validate the adaptability of this translation procedure in different experimental settings: across various trainings, domains, architectures (e.g., ResNet, CNN, ViT), and in multiple downstream tasks (classification, reconstruction). Nota",
    "link": "http://arxiv.org/abs/2311.00664",
    "context": "Title: Latent Space Translation via Semantic Alignment. (arXiv:2311.00664v1 [cs.LG])\nAbstract: While different neural models often exhibit latent spaces that are alike when exposed to semantically related data, this intrinsic similarity is not always immediately discernible. Towards a better understanding of this phenomenon, our work shows how representations learned from these neural modules can be translated between different pre-trained networks via simpler transformations than previously thought. An advantage of this approach is the ability to estimate these transformations using standard, well-understood algebraic procedures that have closed-form solutions. Our method directly estimates a transformation between two given latent spaces, thereby enabling effective stitching of encoders and decoders without additional training. We extensively validate the adaptability of this translation procedure in different experimental settings: across various trainings, domains, architectures (e.g., ResNet, CNN, ViT), and in multiple downstream tasks (classification, reconstruction). Nota",
    "path": "papers/23/11/2311.00664.json",
    "total_tokens": 879,
    "translated_title": "潜在空间翻译通过语义对齐",
    "translated_abstract": "虽然不同的神经模型在接触到语义相关的数据时往往会展现出相似的潜在空间，但这种内在的相似性并不总是立即可辨。为了更好地理解这一现象，我们的工作展示了如何通过比以前认为的更简单的变换将从这些神经模块学到的表示翻译到不同的预训练网络之间。这种方法的优势在于能够使用标准的、通用的代数程序来估计这些变换，并且这些程序具有封闭形式的解。我们的方法直接估计两个给定潜在空间之间的转换，从而实现了有效的编码器和解码器的拼接而无需额外训练。我们在不同的实验设置中广泛验证了这种翻译过程的适应性：包括各种训练数据、领域、架构（如ResNet、CNN、ViT）以及多种下游任务（分类、重构）。",
    "tldr": "本论文研究了潜在空间的翻译问题。通过简单的变换，可以将不同神经模型学到的表示翻译到其他预训练网络中。这种方法能够有效地拼接编码器和解码器，并在各种实验设置中得到验证。"
}