{
    "title": "Large Language Model based Long-tail Query Rewriting in Taobao Search",
    "abstract": "arXiv:2311.03758v3 Announce Type: replace  Abstract: In the realm of e-commerce search, the significance of semantic matching cannot be overstated, as it directly impacts both user experience and company revenue. Along this line, query rewriting, serving as an important technique to bridge the semantic gaps inherent in the semantic matching process, has attached wide attention from the industry and academia. However, existing query rewriting methods often struggle to effectively optimize long-tail queries and alleviate the phenomenon of \"few-recall\" caused by semantic gap. In this paper, we present BEQUE, a comprehensive framework that Bridges the sEmantic gap for long-tail QUEries. In detail, BEQUE comprises three stages: multi-instruction supervised fine tuning (SFT), offline feedback, and objective alignment. We first construct a rewriting dataset based on rejection sampling and auxiliary tasks mixing to fine-tune our large language model (LLM) in a supervised fashion. Subsequently,",
    "link": "https://arxiv.org/abs/2311.03758",
    "context": "Title: Large Language Model based Long-tail Query Rewriting in Taobao Search\nAbstract: arXiv:2311.03758v3 Announce Type: replace  Abstract: In the realm of e-commerce search, the significance of semantic matching cannot be overstated, as it directly impacts both user experience and company revenue. Along this line, query rewriting, serving as an important technique to bridge the semantic gaps inherent in the semantic matching process, has attached wide attention from the industry and academia. However, existing query rewriting methods often struggle to effectively optimize long-tail queries and alleviate the phenomenon of \"few-recall\" caused by semantic gap. In this paper, we present BEQUE, a comprehensive framework that Bridges the sEmantic gap for long-tail QUEries. In detail, BEQUE comprises three stages: multi-instruction supervised fine tuning (SFT), offline feedback, and objective alignment. We first construct a rewriting dataset based on rejection sampling and auxiliary tasks mixing to fine-tune our large language model (LLM) in a supervised fashion. Subsequently,",
    "path": "papers/23/11/2311.03758.json",
    "total_tokens": 839,
    "translated_title": "基于大型语言模型的淘宝搜索长尾查询重写",
    "translated_abstract": "在电子商务搜索领域，语义匹配的重要性不言而喭，因为它直接影响用户体验和公司收入。在这方面，查询重写作为一个重要的技术，用来弥补语义匹配过程中固有的语义差距，受到了行业和学术界的广泛关注。然而，现有的查询重写方法往往难以有效优化长尾查询，缓解由语义差距引起的“少召回”现象。在本文中，我们提出了BEQUE，一个桥接长尾查询语义差距的综合框架。具体而言，BEQUE包括三个阶段：多指导监督微调（SFT）、离线反馈和客观对齐。我们首先基于拒绝抽样和辅助任务混合构建一个重写数据集，以监督方式微调我们的大型语言模型（LLM）。随后，",
    "tldr": "本文提出了基于大型语言模型的BEQUE框架，通过多阶段流程，有效优化长尾查询、弥补语义差距，提高查询重写效果。",
    "en_tdlr": "This paper presents BEQUE, a framework based on large language model, which effectively optimizes long-tail queries, bridges the semantic gap, and improves query rewriting performance."
}