{
    "title": "Align before Adapt: Leveraging Entity-to-Region Alignments for Generalizable Video Action Recognition",
    "abstract": "arXiv:2311.15619v2 Announce Type: replace-cross  Abstract: Large-scale visual-language pre-trained models have achieved significant success in various video tasks. However, most existing methods follow an \"adapt then align\" paradigm, which adapts pre-trained image encoders to model video-level representations and utilizes one-hot or text embedding of the action labels for supervision. This paradigm overlooks the challenge of mapping from static images to complicated activity concepts. In this paper, we propose a novel \"Align before Adapt\" (ALT) paradigm. Prior to adapting to video representation learning, we exploit the entity-to-region alignments for each frame. The alignments are fulfilled by matching the region-aware image embeddings to an offline-constructed text corpus. With the aligned entities, we feed their text embeddings to a transformer-based video adapter as the queries, which can help extract the semantics of the most important entities from a video to a vector. This parad",
    "link": "https://arxiv.org/abs/2311.15619",
    "context": "Title: Align before Adapt: Leveraging Entity-to-Region Alignments for Generalizable Video Action Recognition\nAbstract: arXiv:2311.15619v2 Announce Type: replace-cross  Abstract: Large-scale visual-language pre-trained models have achieved significant success in various video tasks. However, most existing methods follow an \"adapt then align\" paradigm, which adapts pre-trained image encoders to model video-level representations and utilizes one-hot or text embedding of the action labels for supervision. This paradigm overlooks the challenge of mapping from static images to complicated activity concepts. In this paper, we propose a novel \"Align before Adapt\" (ALT) paradigm. Prior to adapting to video representation learning, we exploit the entity-to-region alignments for each frame. The alignments are fulfilled by matching the region-aware image embeddings to an offline-constructed text corpus. With the aligned entities, we feed their text embeddings to a transformer-based video adapter as the queries, which can help extract the semantics of the most important entities from a video to a vector. This parad",
    "path": "papers/23/11/2311.15619.json",
    "total_tokens": 841,
    "translated_title": "在自适应之前对齐：利用实体到区域对齐进行通用视频动作识别",
    "translated_abstract": "大规模的视觉-语言预训练模型在各种视频任务中取得了重大成功。然而，大多数现有方法遵循“自适应然后对齐”的范式，将预训练图像编码器调整为建模视频级表示，并利用动作标签的one-hot或文本嵌入进行监督。本文提出了一种新颖的“对齐前自适应”（ALT）范式。在适应到视频表示学习之前，我们利用为每一帧实体到区域对齐。这些对齐通过将区域感知图像嵌入与离线构建的文本语料库进行匹配来实现。有了对齐的实体，我们将它们的文本嵌入作为查询馈送到基于transformer的视频适配器中，可以帮助从视频到向量提取最重要实体的语义。",
    "tldr": "提出了一种“对齐前自适应”（ALT）范式，通过利用实体到区域对齐，将文本嵌入作为查询，从而帮助提取视频中最重要实体的语义。",
    "en_tdlr": "Introduced an \"Align before Adapt\" (ALT) paradigm that leverages entity-to-region alignments and utilizes text embeddings as queries to extract the semantics of the most important entities from a video."
}