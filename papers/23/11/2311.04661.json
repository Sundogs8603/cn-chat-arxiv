{
    "title": "Massive Editing for Large Language Models via Meta Learning. (arXiv:2311.04661v3 [cs.CL] UPDATED)",
    "abstract": "While large language models (LLMs) have enabled learning knowledge from the pre-training corpora, the acquired knowledge may be fundamentally incorrect or outdated over time, which necessitates rectifying the knowledge of the language model (LM) after the training. A promising approach involves employing a hyper-network to generate parameter shift, whereas existing hyper-networks suffer from inferior scalability in synchronous editing operation amount. To mitigate the problem, we propose the MAssive Language Model Editing Network (MALMEN), which formulates the parameter shift aggregation as the least square problem, subsequently updating the LM parameters using the normal equation. To accommodate editing multiple facts simultaneously with limited memory budgets, we separate the computation on the hyper-network and LM, enabling arbitrary batch size on both neural networks. Our method is evaluated by editing up to thousands of facts on LMs with different architectures, i.e., BERT-base, G",
    "link": "http://arxiv.org/abs/2311.04661",
    "context": "Title: Massive Editing for Large Language Models via Meta Learning. (arXiv:2311.04661v3 [cs.CL] UPDATED)\nAbstract: While large language models (LLMs) have enabled learning knowledge from the pre-training corpora, the acquired knowledge may be fundamentally incorrect or outdated over time, which necessitates rectifying the knowledge of the language model (LM) after the training. A promising approach involves employing a hyper-network to generate parameter shift, whereas existing hyper-networks suffer from inferior scalability in synchronous editing operation amount. To mitigate the problem, we propose the MAssive Language Model Editing Network (MALMEN), which formulates the parameter shift aggregation as the least square problem, subsequently updating the LM parameters using the normal equation. To accommodate editing multiple facts simultaneously with limited memory budgets, we separate the computation on the hyper-network and LM, enabling arbitrary batch size on both neural networks. Our method is evaluated by editing up to thousands of facts on LMs with different architectures, i.e., BERT-base, G",
    "path": "papers/23/11/2311.04661.json",
    "total_tokens": 911,
    "translated_title": "通过元学习实现大规模语言模型的大规模编辑",
    "translated_abstract": "虽然大规模语言模型（LLM）通过对预训练语料库学习知识成为可能，但所获得的知识随着时间的推移可能是基本不正确或过时的，这需要在训练后纠正语言模型（LM）的知识。一种有前景的方法是利用超网络生成参数偏移，然而现有的超网络在同步编辑操作数量方面存在扩展性不足的问题。为解决这个问题，我们提出了大规模语言模型编辑网络（MALMEN），它将参数偏移聚合形式化为最小二乘问题，并使用正规方程更新LM参数。为适应在有限内存预算下同时编辑多个事实，我们将超网络和LM上的计算分离，使得两个神经网络都可以具有任意批量大小。我们的方法通过对具有不同架构的LM（例如BERT-base）进行高达数千个事实的编辑进行了评估。",
    "tldr": "本论文提出了一种通过元学习实现大规模语言模型的大规模编辑的方法。该方法利用超网络来生成参数变化，通过解决最小二乘问题来更新语言模型的参数。通过将计算分离在超网络和语言模型之间，使得可以同时编辑多个事实。该方法在不同架构的语言模型上进行了评估。",
    "en_tdlr": "This paper proposes a method for massive editing of large language models through meta learning. The method uses a hyper-network to generate parameter shifts and updates the language model parameters by solving a least squares problem. By separating the computation between the hyper-network and the language model, multiple facts can be edited simultaneously. The method is evaluated on language models with different architectures."
}