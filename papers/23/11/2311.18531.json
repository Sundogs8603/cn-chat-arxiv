{
    "title": "Dataset Distillation via the Wasserstein Metric",
    "abstract": "arXiv:2311.18531v2 Announce Type: replace-cross  Abstract: Dataset Distillation (DD) emerges as a powerful strategy to encapsulate the expansive information of large datasets into significantly smaller, synthetic equivalents, thereby preserving model performance with reduced computational overhead. Pursuing this objective, we introduce the Wasserstein distance, a metric grounded in optimal transport theory, to enhance distribution matching in DD. Our approach employs the Wasserstein barycenter to provide a geometrically meaningful method for quantifying distribution differences and capturing the centroid of distribution sets efficiently. By embedding synthetic data in the feature spaces of pretrained classification models, we facilitate effective distribution matching that leverages prior knowledge inherent in these models. Our method not only maintains the computational advantages of distribution matching-based techniques but also achieves new state-of-the-art performance across a ran",
    "link": "https://arxiv.org/abs/2311.18531",
    "context": "Title: Dataset Distillation via the Wasserstein Metric\nAbstract: arXiv:2311.18531v2 Announce Type: replace-cross  Abstract: Dataset Distillation (DD) emerges as a powerful strategy to encapsulate the expansive information of large datasets into significantly smaller, synthetic equivalents, thereby preserving model performance with reduced computational overhead. Pursuing this objective, we introduce the Wasserstein distance, a metric grounded in optimal transport theory, to enhance distribution matching in DD. Our approach employs the Wasserstein barycenter to provide a geometrically meaningful method for quantifying distribution differences and capturing the centroid of distribution sets efficiently. By embedding synthetic data in the feature spaces of pretrained classification models, we facilitate effective distribution matching that leverages prior knowledge inherent in these models. Our method not only maintains the computational advantages of distribution matching-based techniques but also achieves new state-of-the-art performance across a ran",
    "path": "papers/23/11/2311.18531.json",
    "total_tokens": 785,
    "translated_title": "通过Wasserstein度量进行数据集精炼",
    "translated_abstract": "数据集精炼（DD）作为一种强大的策略，将大型数据集的丰富信息封装为明显更小的合成等价物，从而在减少计算开销的同时保留模型性能。为实现这一目标，我们引入了Wasserstein距离，这是一种基于最优输运理论的度量，用于增强DD中的分布匹配。我们的方法利用Wasserstein重心提供了一种在量化分布差异和高效捕获分布集合中心的几何意义方法。通过在预训练分类模型的特征空间中嵌入合成数据，我们促进了有效的分布匹配，利用这些模型固有的先验知识。我们的方法不仅保持了基于分布匹配的技术的计算优势，而且在一系列任务中实现了新的最先进性能。",
    "tldr": "通过引入Wasserstein距离及其重心，我们提出一种有效的数据集精炼方法，利用先验知识提高分布匹配效果，实现了新的最先进性能。",
    "en_tdlr": "By introducing the Wasserstein distance and its barycenter, we propose an effective dataset distillation method that leverages prior knowledge to improve distribution matching, achieving new state-of-the-art performance."
}