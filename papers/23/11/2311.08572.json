{
    "title": "Low-Rank Adaptation for Multilingual Summarization: An Empirical Study",
    "abstract": "arXiv:2311.08572v2 Announce Type: replace-cross  Abstract: Although the advancements of pre-trained Large Language Models have significantly accelerated recent progress in NLP, their ever-increasing size poses significant challenges for conventional fine-tuning, especially in memory-intensive tasks. We investigate the potential of Parameter-Efficient Fine-Tuning, focusing on Low-Rank Adaptation (LoRA), in the domain of multilingual summarization, a task that is both challenging (due to typically long inputs), and relatively unexplored. We conduct an extensive study across different data availability scenarios, including high- and low-data settings, and cross-lingual transfer, leveraging models of different sizes. Our findings reveal that LoRA is competitive with full fine-tuning when trained with high quantities of data, and excels in low-data scenarios and cross-lingual transfer. We also study different strategies for few-shot cross-lingual transfer, finding that continued LoRA tuning",
    "link": "https://arxiv.org/abs/2311.08572",
    "context": "Title: Low-Rank Adaptation for Multilingual Summarization: An Empirical Study\nAbstract: arXiv:2311.08572v2 Announce Type: replace-cross  Abstract: Although the advancements of pre-trained Large Language Models have significantly accelerated recent progress in NLP, their ever-increasing size poses significant challenges for conventional fine-tuning, especially in memory-intensive tasks. We investigate the potential of Parameter-Efficient Fine-Tuning, focusing on Low-Rank Adaptation (LoRA), in the domain of multilingual summarization, a task that is both challenging (due to typically long inputs), and relatively unexplored. We conduct an extensive study across different data availability scenarios, including high- and low-data settings, and cross-lingual transfer, leveraging models of different sizes. Our findings reveal that LoRA is competitive with full fine-tuning when trained with high quantities of data, and excels in low-data scenarios and cross-lingual transfer. We also study different strategies for few-shot cross-lingual transfer, finding that continued LoRA tuning",
    "path": "papers/23/11/2311.08572.json",
    "total_tokens": 852,
    "translated_title": "低秩适应用于多语言摘要：一项实证研究",
    "translated_abstract": "尽管预训练的大型语言模型的进展显著加速了近年来自然语言处理领域的进步，但它们不断增长的体积对传统的微调提出了重要挑战，特别是在内存密集型任务中。我们调查了参数高效微调的潜力，重点是低秩适应（LoRA），涉及多语言摘要领域，这是一个具有挑战性的任务（因为输入通常很长），且相对未被充分探索。我们进行了一项广泛的研究，涵盖不同数据可用性场景，包括高数据和低数据设置，以及跨语言转移，利用不同规模的模型。我们的发现表明，当使用大量数据训练时，LoRA与完全微调竞争激烈，并且在低数据情况和跨语言转移方面表现出色。我们还研究了不同的少数据点跨语言转移策略，发现持续的LoRA调优...",
    "tldr": "LoRA 在多语言摘要方面竞争力强，特别在低数据情况和跨语言转移方面表现出色。",
    "en_tdlr": "LoRA is competitive in multilingual summarization, especially excelling in low-data scenarios and cross-lingual transfer."
}