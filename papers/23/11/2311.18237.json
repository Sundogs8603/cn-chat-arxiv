{
    "title": "Knowledge Transfer from Vision Foundation Models for Efficient Training of Small Task-specific Models",
    "abstract": "arXiv:2311.18237v2 Announce Type: replace-cross  Abstract: Vision Foundation Models (VFMs) pretrained on massive datasets exhibit impressive performance on various downstream tasks, especially with limited labeled target data. However, due to their high inference compute cost, these models cannot be deployed for many real-world applications. Motivated by this, we ask the following important question, \"How can we leverage the knowledge from a large VFM to train a small task-specific model for a new target task with limited labeled training data?\", and propose a simple task-oriented knowledge transfer approach as a highly effective solution to this problem. Our experimental results on five target tasks show that the proposed approach outperforms task-agnostic VFM distillation, web-scale CLIP pretraining, supervised ImageNet pretraining, and self-supervised DINO pretraining by up to 11.6%, 22.1%, 13.7%, and 29.8%, respectively. Furthermore, the proposed approach also demonstrates up to 9x",
    "link": "https://arxiv.org/abs/2311.18237",
    "context": "Title: Knowledge Transfer from Vision Foundation Models for Efficient Training of Small Task-specific Models\nAbstract: arXiv:2311.18237v2 Announce Type: replace-cross  Abstract: Vision Foundation Models (VFMs) pretrained on massive datasets exhibit impressive performance on various downstream tasks, especially with limited labeled target data. However, due to their high inference compute cost, these models cannot be deployed for many real-world applications. Motivated by this, we ask the following important question, \"How can we leverage the knowledge from a large VFM to train a small task-specific model for a new target task with limited labeled training data?\", and propose a simple task-oriented knowledge transfer approach as a highly effective solution to this problem. Our experimental results on five target tasks show that the proposed approach outperforms task-agnostic VFM distillation, web-scale CLIP pretraining, supervised ImageNet pretraining, and self-supervised DINO pretraining by up to 11.6%, 22.1%, 13.7%, and 29.8%, respectively. Furthermore, the proposed approach also demonstrates up to 9x",
    "path": "papers/23/11/2311.18237.json",
    "total_tokens": 888,
    "translated_title": "从视觉基础模型中进行知识迁移用于高效训练小型任务特定模型",
    "translated_abstract": "在许多下游任务中，基于大规模数据集预训练的视觉基础模型在有限标记的目标数据上展现出了令人印象深刻的性能。然而，由于推理计算成本高，这些模型无法应用于许多实际应用。为了解决这个问题，我们提出了一个简单的任务导向的知识迁移方法，以高效解决如何利用大规模视觉基础模型的知识来训练小型任务特定模型的问题。我们在五个目标任务上的实验结果表明，该方法在超过Task-Agnostic VFM蒸馏、Web-Scale CLIP预训练、监督式ImageNet预训练和自监督DINO预训练29.8%、22.1%、13.7%和11.6%的方面表现出更好的性能。此外，所提出的方法还展现出了高达9倍的性能提升。",
    "tldr": "本文提出了一个简单的任务导向的知识迁移方法，用于高效训练小型任务特定模型。实验结果表明，该方法在多个目标任务上表现出了更好的性能，并且还展示了高达9倍的性能提升。"
}