{
    "title": "Unified Batch Normalization: Identifying and Alleviating the Feature Condensation in Batch Normalization and a Unified Framework",
    "abstract": "arXiv:2311.15993v2 Announce Type: replace-cross  Abstract: Batch Normalization (BN) has become an essential technique in contemporary neural network design, enhancing training stability. Specifically, BN employs centering and scaling operations to standardize features along the batch dimension and uses an affine transformation to recover features. Although standard BN has shown its capability to improve deep neural network training and convergence, it still exhibits inherent limitations in certain cases. Current enhancements to BN typically address only isolated aspects of its mechanism. In this work, we critically examine BN from a feature perspective, identifying feature condensation during BN as a detrimental factor to test performance. To tackle this problem, we propose a two-stage unified framework called Unified Batch Normalization (UBN). In the first stage, we employ a straightforward feature condensation threshold to mitigate condensation effects, thereby preventing improper up",
    "link": "https://arxiv.org/abs/2311.15993",
    "context": "Title: Unified Batch Normalization: Identifying and Alleviating the Feature Condensation in Batch Normalization and a Unified Framework\nAbstract: arXiv:2311.15993v2 Announce Type: replace-cross  Abstract: Batch Normalization (BN) has become an essential technique in contemporary neural network design, enhancing training stability. Specifically, BN employs centering and scaling operations to standardize features along the batch dimension and uses an affine transformation to recover features. Although standard BN has shown its capability to improve deep neural network training and convergence, it still exhibits inherent limitations in certain cases. Current enhancements to BN typically address only isolated aspects of its mechanism. In this work, we critically examine BN from a feature perspective, identifying feature condensation during BN as a detrimental factor to test performance. To tackle this problem, we propose a two-stage unified framework called Unified Batch Normalization (UBN). In the first stage, we employ a straightforward feature condensation threshold to mitigate condensation effects, thereby preventing improper up",
    "path": "papers/23/11/2311.15993.json",
    "total_tokens": 865,
    "translated_title": "统一批归一化：识别和缓解批归一化中的特征凝聚及统一框架",
    "translated_abstract": "批归一化（BN）已经成为当代神经网络设计中的基本技术，增强了训练稳定性。具体地，BN采用居中和缩放操作来标准化沿批次维度的特征，并使用仿射变换来恢复特征。尽管标准的BN已经显示出改善深度神经网络训练和收敛的能力，但在某些情况下仍存在固有限制。目前对BN的增强通常只解决其机制的某些方面。在这项工作中，我们从特征的角度对BN进行了批判性的检查，将BN中的特征凝聚识别为对测试性能有害的因素。为了解决这一问题，我们提出了一个称为统一批归一化（UBN）的两阶段统一框架。在第一阶段，我们采用了一个直观的特征凝聚阈值来减轻凝聚效应，从而防止不当的升",
    "tldr": "识别了批归一化中的特征凝聚问题，并提出了统一批归一化（UBN）框架来解决，从而改善测试性能。",
    "en_tdlr": "Identified the feature condensation issue in batch normalization and proposed the Unified Batch Normalization (UBN) framework to address it, improving test performance."
}