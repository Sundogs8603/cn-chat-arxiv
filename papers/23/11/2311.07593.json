{
    "title": "Follow-Up Differential Descriptions: Language Models Resolve Ambiguities for Image Classification",
    "abstract": "arXiv:2311.07593v2 Announce Type: replace  Abstract: A promising approach for improving the performance of vision-language models like CLIP for image classification is to extend the class descriptions (i.e., prompts) with related attributes, e.g., using brown sparrow instead of sparrow. However, current zero-shot methods select a subset of attributes regardless of commonalities between the target classes, potentially providing no useful information that would have helped to distinguish between them. For instance, they may use color instead of bill shape to distinguish between sparrows and wrens, which are both brown. We propose Follow-up Differential Descriptions (FuDD), a zero-shot approach that tailors the class descriptions to each dataset and leads to additional attributes that better differentiate the target classes. FuDD first identifies the ambiguous classes for each image, and then uses a Large Language Model (LLM) to generate new class descriptions that differentiate between t",
    "link": "https://arxiv.org/abs/2311.07593",
    "context": "Title: Follow-Up Differential Descriptions: Language Models Resolve Ambiguities for Image Classification\nAbstract: arXiv:2311.07593v2 Announce Type: replace  Abstract: A promising approach for improving the performance of vision-language models like CLIP for image classification is to extend the class descriptions (i.e., prompts) with related attributes, e.g., using brown sparrow instead of sparrow. However, current zero-shot methods select a subset of attributes regardless of commonalities between the target classes, potentially providing no useful information that would have helped to distinguish between them. For instance, they may use color instead of bill shape to distinguish between sparrows and wrens, which are both brown. We propose Follow-up Differential Descriptions (FuDD), a zero-shot approach that tailors the class descriptions to each dataset and leads to additional attributes that better differentiate the target classes. FuDD first identifies the ambiguous classes for each image, and then uses a Large Language Model (LLM) to generate new class descriptions that differentiate between t",
    "path": "papers/23/11/2311.07593.json",
    "total_tokens": 768,
    "translated_title": "跟进差分描述：语言模型解决图像分类中的歧义问题",
    "translated_abstract": "一种改善视觉-语言模型（如CLIP）在图像分类中性能的有希望的方法是通过扩展类描述（即提示）的相关属性，例如使用棕色麻雀代替麻雀。然而，当前的零样本方法无论目标类之间的共同之处如何，都会选择一组属性，可能提供没有帮助区分它们的有用信息。我们提出了Follow-up Differential Descriptions（FuDD），这是一种零样本方法，可以根据每个数据集量身定制类描述，并提供更好区分目标类的附加属性。FuDD首先为每个图像确定模糊类，然后使用大型语言模型（LLM）生成新的类描述，以区分它们。",
    "tldr": "提出了一种零样本方法Follow-up Differential Descriptions（FuDD），通过为每个图像确定模糊类，并使用大型语言模型生成新的类描述，以更好地区分目标类。",
    "en_tdlr": "Introducing Follow-up Differential Descriptions (FuDD), a zero-shot approach that tailors class descriptions to each dataset and leads to additional attributes for better differentiation of target classes."
}