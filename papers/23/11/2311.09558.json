{
    "title": "What if you said that differently?: How Explanation Formats Affect Human Feedback Efficacy and User Perception",
    "abstract": "arXiv:2311.09558v2 Announce Type: replace  Abstract: Eliciting feedback from end users of NLP models can be beneficial for improving models. However, how should we present model responses to users so they are most amenable to be corrected from user feedback? Further, what properties do users value to understand and trust responses? We answer these questions by analyzing the effect of rationales (or explanations) generated by QA models to support their answers. We specifically consider decomposed QA models that first extract an intermediate rationale based on a context and a question and then use solely this rationale to answer the question. A rationale outlines the approach followed by the model to answer the question. Our work considers various formats of these rationales that vary according to well-defined properties of interest. We sample rationales from language models using few-shot prompting for two datasets, and then perform two user studies. First, we present users with incorre",
    "link": "https://arxiv.org/abs/2311.09558",
    "context": "Title: What if you said that differently?: How Explanation Formats Affect Human Feedback Efficacy and User Perception\nAbstract: arXiv:2311.09558v2 Announce Type: replace  Abstract: Eliciting feedback from end users of NLP models can be beneficial for improving models. However, how should we present model responses to users so they are most amenable to be corrected from user feedback? Further, what properties do users value to understand and trust responses? We answer these questions by analyzing the effect of rationales (or explanations) generated by QA models to support their answers. We specifically consider decomposed QA models that first extract an intermediate rationale based on a context and a question and then use solely this rationale to answer the question. A rationale outlines the approach followed by the model to answer the question. Our work considers various formats of these rationales that vary according to well-defined properties of interest. We sample rationales from language models using few-shot prompting for two datasets, and then perform two user studies. First, we present users with incorre",
    "path": "papers/23/11/2311.09558.json",
    "total_tokens": 827,
    "translated_title": "如果你用不同方式表达会怎样：解释格式如何影响人类反馈效果和用户感知",
    "translated_abstract": "从NLP模型的最终用户那里获取反馈对于改进模型是有益的。然而，我们应该如何向用户呈现模型的响应，使其最易于接受来自用户反馈的更正？此外，用户看重理解和信任响应的哪些属性？我们通过分析由QA模型生成的支持其答案的理由（或解释）来回答这些问题。我们特别考虑将QA模型分解为首先根据上下文和问题提取中间原因，然后仅使用该原因来回答问题的模型。原因概述了模型回答问题的方法。我们的工作考虑了这些原因的各种格式，根据感兴趣的明确定义的属性而变化。我们使用少量提示从语言模型中对两个数据集进行抽样，然后进行两项用户研究。首先，我们向用户展示不正确的情况",
    "tldr": "解释格式对人类反馈效果和用户感知有重要影响，提供了研究分析如何呈现模型响应以便更易接受用户纠正的策略。"
}