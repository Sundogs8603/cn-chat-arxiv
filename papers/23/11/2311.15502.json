{
    "title": "The Selected-completely-at-random Complementary Label is a Practical Weak Supervision for Multi-class Classification",
    "abstract": "arXiv:2311.15502v2 Announce Type: replace  Abstract: Complementary-label learning is a weakly supervised learning problem in which each training example is associated with one or multiple complementary labels indicating the classes to which it does not belong. Existing consistent approaches have relied on the uniform distribution assumption to model the generation of complementary labels, or on an ordinary-label training set to estimate the transition matrix in non-uniform cases. However, either condition may not be satisfied in real-world scenarios. In this paper, we propose a novel consistent approach that does not rely on these conditions. Inspired by the positive-unlabeled (PU) learning literature, we propose an unbiased risk estimator based on the Selected Completely At Random assumption for complementary-label learning. We then introduce a risk-correction approach to address overfitting problems. Furthermore, we find that complementary-label learning can be expressed as a set of ",
    "link": "https://arxiv.org/abs/2311.15502",
    "context": "Title: The Selected-completely-at-random Complementary Label is a Practical Weak Supervision for Multi-class Classification\nAbstract: arXiv:2311.15502v2 Announce Type: replace  Abstract: Complementary-label learning is a weakly supervised learning problem in which each training example is associated with one or multiple complementary labels indicating the classes to which it does not belong. Existing consistent approaches have relied on the uniform distribution assumption to model the generation of complementary labels, or on an ordinary-label training set to estimate the transition matrix in non-uniform cases. However, either condition may not be satisfied in real-world scenarios. In this paper, we propose a novel consistent approach that does not rely on these conditions. Inspired by the positive-unlabeled (PU) learning literature, we propose an unbiased risk estimator based on the Selected Completely At Random assumption for complementary-label learning. We then introduce a risk-correction approach to address overfitting problems. Furthermore, we find that complementary-label learning can be expressed as a set of ",
    "path": "papers/23/11/2311.15502.json",
    "total_tokens": 874,
    "translated_title": "选取完全随机的互补标签是多类别分类的实用弱监督方法",
    "translated_abstract": "互补标签学习是一个弱监督学习问题，每个训练样本关联着一个或多个互补标签，指示其不属于的类别。现有的一致方法依赖于均匀分布假设来模拟互补标签的生成，或者依赖于一个普通标签的训练集来估计非均匀情况下的转移矩阵。然而，实际情况下这两个条件可能不会被满足。在本文中，我们提出了一种新颖的一致方法，不依赖于这些条件。受到PU学习文献的启发，我们提出了基于完全随机选择假设的无偏风险估计器，用于互补标签学习。然后，我们引入了一种风险校正方法来解决过拟合问题。此外，我们发现互补标签学习可以被表达为一组...",
    "tldr": "提出了一种不依赖于均匀分布假设的互补标签学习方法，基于完全随机选择假设的无偏风险估计器，以及风险校正方法来解决过拟合问题。",
    "en_tdlr": "Propose a complementary-label learning method that does not rely on the uniform distribution assumption, with an unbiased risk estimator based on the Selected Completely At Random assumption and a risk-correction approach to address overfitting issues."
}