{
    "title": "Selective Visual Representations Improve Convergence and Generalization for Embodied AI",
    "abstract": "arXiv:2311.04193v2 Announce Type: replace-cross  Abstract: Embodied AI models often employ off the shelf vision backbones like CLIP to encode their visual observations. Although such general purpose representations encode rich syntactic and semantic information about the scene, much of this information is often irrelevant to the specific task at hand. This introduces noise within the learning process and distracts the agent's focus from task-relevant visual cues. Inspired by selective attention in humans-the process through which people filter their perception based on their experiences, knowledge, and the task at hand-we introduce a parameter-efficient approach to filter visual stimuli for embodied AI. Our approach induces a task-conditioned bottleneck using a small learnable codebook module. This codebook is trained jointly to optimize task reward and acts as a task-conditioned selective filter over the visual observation. Our experiments showcase state-of-the-art performance for obj",
    "link": "https://arxiv.org/abs/2311.04193",
    "context": "Title: Selective Visual Representations Improve Convergence and Generalization for Embodied AI\nAbstract: arXiv:2311.04193v2 Announce Type: replace-cross  Abstract: Embodied AI models often employ off the shelf vision backbones like CLIP to encode their visual observations. Although such general purpose representations encode rich syntactic and semantic information about the scene, much of this information is often irrelevant to the specific task at hand. This introduces noise within the learning process and distracts the agent's focus from task-relevant visual cues. Inspired by selective attention in humans-the process through which people filter their perception based on their experiences, knowledge, and the task at hand-we introduce a parameter-efficient approach to filter visual stimuli for embodied AI. Our approach induces a task-conditioned bottleneck using a small learnable codebook module. This codebook is trained jointly to optimize task reward and acts as a task-conditioned selective filter over the visual observation. Our experiments showcase state-of-the-art performance for obj",
    "path": "papers/23/11/2311.04193.json",
    "total_tokens": 812,
    "translated_title": "选择性视觉表示提高具身智能的收敛性和泛化性",
    "translated_abstract": "具身智能模型通常使用类似CLIP之类的通用视觉主干来编码它们的视觉观察。尽管这种通用目的的表示编码了关于场景的丰富句法和语义信息，但其中很多信息通常与手头的具体任务无关。这在学习过程中引入了噪声，并使代理人的注意力从任务相关的视觉线索转移。受人类选择性注意的启发-即人们根据他们的经验、知识和手头的任务来过滤他们的感知，我们介绍了一种参数高效的方法来为具身智能过滤视觉刺激。我们的方法使用一个小型可学习的码书模块引入了一个任务条件的瓶颈。这个码书是联合训练来优化任务奖励，并作为一个任务条件的选择性过滤器作用于视觉观察。我们的实验展示了针对obj的表现达到了最先进水平。",
    "tldr": "引入了一种参数高效的方法，在具身智能中使用任务条件的选择性过滤器来改善收敛性和泛化性能"
}