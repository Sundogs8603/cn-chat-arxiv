{
    "title": "ERASER: Machine Unlearning in MLaaS via an Inference Serving-Aware Approach",
    "abstract": "Over the past years, Machine Learning-as-a-Service (MLaaS) has received a surging demand for supporting Machine Learning-driven services to offer revolutionized user experience across diverse application areas. MLaaS provides inference service with low inference latency based on an ML model trained using a dataset collected from numerous individual data owners. Recently, for the sake of data owners' privacy and to comply with the \"right to be forgotten (RTBF)\" as enacted by data protection legislation, many machine unlearning methods have been proposed to remove data owners' data from trained models upon their unlearning requests. However, despite their promising efficiency, almost all existing machine unlearning methods handle unlearning requests independently from inference requests, which unfortunately introduces a new security issue of inference service obsolescence and a privacy vulnerability of undesirable exposure for machine unlearning in MLaaS.   In this paper, we propose the ",
    "link": "https://arxiv.org/abs/2311.16136",
    "context": "Title: ERASER: Machine Unlearning in MLaaS via an Inference Serving-Aware Approach\nAbstract: Over the past years, Machine Learning-as-a-Service (MLaaS) has received a surging demand for supporting Machine Learning-driven services to offer revolutionized user experience across diverse application areas. MLaaS provides inference service with low inference latency based on an ML model trained using a dataset collected from numerous individual data owners. Recently, for the sake of data owners' privacy and to comply with the \"right to be forgotten (RTBF)\" as enacted by data protection legislation, many machine unlearning methods have been proposed to remove data owners' data from trained models upon their unlearning requests. However, despite their promising efficiency, almost all existing machine unlearning methods handle unlearning requests independently from inference requests, which unfortunately introduces a new security issue of inference service obsolescence and a privacy vulnerability of undesirable exposure for machine unlearning in MLaaS.   In this paper, we propose the ",
    "path": "papers/23/11/2311.16136.json",
    "total_tokens": 779,
    "translated_title": "ERASER: 通过推理服务器感知的方法，在MLaaS中实现机器学习的去学习",
    "translated_abstract": "过去几年，机器学习作为一种服务（MLaaS）在支持基于机器学习的服务方面有着大量需求，以在各种应用领域提供革命性的用户体验。MLaaS基于从众多个体数据所有者收集的训练数据集训练的机器学习模型提供低推理延迟的推理服务。最近，为了保护数据所有者的隐私并遵守数据保护法的“被遗忘权（RTBF）”，许多机器去学习方法被提出，以在请求去学习时从训练模型中删除数据所有者的数据。然而，尽管它们具有很高的效率，几乎所有现有的机器去学习方法都独立处理去学习请求和推理请求，这不幸地引入了推理服务过时性的安全问题和机器去学习中极易遭受不必要的曝光的隐私漏洞。",
    "tldr": "本文提出了ERASER，一种通过推理服务器感知的方法，在MLaaS中实现机器学习的去学习。"
}