{
    "title": "Noise-Robust Fine-Tuning of Pretrained Language Models via External Guidance. (arXiv:2311.01108v1 [cs.CL])",
    "abstract": "Adopting a two-stage paradigm of pretraining followed by fine-tuning, Pretrained Language Models (PLMs) have achieved substantial advancements in the field of natural language processing. However, in real-world scenarios, data labels are often noisy due to the complex annotation process, making it essential to develop strategies for fine-tuning PLMs with such noisy labels. To this end, we introduce an innovative approach for fine-tuning PLMs using noisy labels, which incorporates the guidance of Large Language Models (LLMs) like ChatGPT. This guidance assists in accurately distinguishing between clean and noisy samples and provides supplementary information beyond the noisy labels, thereby boosting the learning process during fine-tuning PLMs. Extensive experiments on synthetic and real-world noisy datasets further demonstrate the superior advantages of our framework over the state-of-the-art baselines.",
    "link": "http://arxiv.org/abs/2311.01108",
    "context": "Title: Noise-Robust Fine-Tuning of Pretrained Language Models via External Guidance. (arXiv:2311.01108v1 [cs.CL])\nAbstract: Adopting a two-stage paradigm of pretraining followed by fine-tuning, Pretrained Language Models (PLMs) have achieved substantial advancements in the field of natural language processing. However, in real-world scenarios, data labels are often noisy due to the complex annotation process, making it essential to develop strategies for fine-tuning PLMs with such noisy labels. To this end, we introduce an innovative approach for fine-tuning PLMs using noisy labels, which incorporates the guidance of Large Language Models (LLMs) like ChatGPT. This guidance assists in accurately distinguishing between clean and noisy samples and provides supplementary information beyond the noisy labels, thereby boosting the learning process during fine-tuning PLMs. Extensive experiments on synthetic and real-world noisy datasets further demonstrate the superior advantages of our framework over the state-of-the-art baselines.",
    "path": "papers/23/11/2311.01108.json",
    "total_tokens": 880,
    "translated_title": "通过外部引导实现对预训练语言模型的噪声鲁棒微调",
    "translated_abstract": "在自然语言处理领域，采用预训练后微调的两阶段范式，预训练语言模型（PLMs）已经取得了重大进展。然而，在现实场景中，由于复杂的注释过程，数据标签通常存在噪声，因此有必要开发针对这样噪声标签的微调策略。为此，我们引入了一种创新的方法，使用噪声标签对PLMs进行微调，该方法将像ChatGPT这样的大语言模型（LLMs）的指导纳入其中。这种指导有助于准确区分干净样本和噪声样本，并提供了除噪声标签外的补充信息，从而在微调PLMs的学习过程中提供了额外的帮助。对合成和真实噪声数据集进行的广泛实验进一步证明了我们的框架相对于最先进的基线方法的优势。",
    "tldr": "通过利用大语言模型的指导，我们提出一种创新方法，使用噪声标签对预训练语言模型进行鲁棒微调。实验证明，我们的方法相比最先进的基线方法在合成和真实噪声数据集上具有优越的性能。"
}