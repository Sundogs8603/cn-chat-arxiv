{
    "title": "It's Not Easy Being Wrong: Large Language Models Struggle with Process of Elimination Reasoning",
    "abstract": "arXiv:2311.07532v2 Announce Type: replace  Abstract: Chain-of-thought (COT) prompting can help large language models (LLMs) reason toward correct answers, but its efficacy in reasoning toward incorrect answers is unexplored. This process of elimination (PoE), when used with COT, can enhance self-consistency, interpretability, and tasks such as medical diagnoses of exclusion. Thus, we propose PoE with COT, where LLMs must reason toward incorrect options on multiple-choice questions. We evaluate the ability of GPT-3.5, LLaMA-2, and Falcon to perform PoE with COT on a total of four commonsense and scientific reasoning datasets. We find that the strategy of PoE always underperforms the strategy of choosing the correct answer. The agreement of these strategies is also lower than the self-consistency of each strategy. To study these issues further, we conduct error analyses and give suggestions for future work.",
    "link": "https://arxiv.org/abs/2311.07532",
    "context": "Title: It's Not Easy Being Wrong: Large Language Models Struggle with Process of Elimination Reasoning\nAbstract: arXiv:2311.07532v2 Announce Type: replace  Abstract: Chain-of-thought (COT) prompting can help large language models (LLMs) reason toward correct answers, but its efficacy in reasoning toward incorrect answers is unexplored. This process of elimination (PoE), when used with COT, can enhance self-consistency, interpretability, and tasks such as medical diagnoses of exclusion. Thus, we propose PoE with COT, where LLMs must reason toward incorrect options on multiple-choice questions. We evaluate the ability of GPT-3.5, LLaMA-2, and Falcon to perform PoE with COT on a total of four commonsense and scientific reasoning datasets. We find that the strategy of PoE always underperforms the strategy of choosing the correct answer. The agreement of these strategies is also lower than the self-consistency of each strategy. To study these issues further, we conduct error analyses and give suggestions for future work.",
    "path": "papers/23/11/2311.07532.json",
    "total_tokens": 914,
    "translated_title": "错误并不容易：大型语言模型在排除推理过程中遇到困难",
    "translated_abstract": "链式思维（COT）提示可以帮助大型语言模型（LLMs）朝着正确答案进行推理，但其在朝着错误答案进行推理方面的有效性尚未被探究。当与COT一起使用时，这种排除推理（PoE）可以增强自我一致性、可解释性以及诸如排除性医学诊断等任务。因此，我们提出了在多项选择问题中进行PoE与COT的方法，LLMs必须朝着不正确的选项进行推理。我们评估了GPT-3.5、LLaMA-2和Falcon在总共四个常识和科学推理数据集上执行带有COT的PoE的能力。我们发现PoE策略总是表现不如选择正确答案的策略。这两种策略的一致性也低于每种策略的自我一致性。为了进一步研究这些问题，我们进行了错误分析并提出了未来工作的建议。",
    "tldr": "大型语言模型在排除推理过程中遇到困难，提出了一种新的排除推理方法PoE与COT，发现此方法在多项选择问题上的表现不如选择正确答案，并指出了研究中发现的一致性和错误分析的问题。",
    "en_tdlr": "Large language models struggle with process of elimination reasoning, proposing a new method PoE with COT, but found that it underperforms choosing the correct answer in multiple-choice questions, with issues of consistency and error analysis identified in the study."
}