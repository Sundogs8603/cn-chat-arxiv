{
    "title": "The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback. (arXiv:2311.00168v1 [cs.LG])",
    "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as a powerful technique to make large language models (LLMs) easier to prompt and more capable in complex settings. RLHF at its core is providing a new toolkit to optimize LLMs other than next-token prediction, enabling the integration of qualitative training goals. The attempted match between user preferences and downstream performance, which happens in a learned reward model, results in an optimization landscape where training and evaluation metrics can appear correlated. The apparent correlation can lead to unexpected behaviors and stories of \"too much RLHF.\" In RLHF, challenges emerge because the following sub-modules are not consistent with each other: the reward model training, the policy model training, and the policy model evaluation. This mismatch results in models that sometimes avoid user requests for false safety flags, are difficult to steer to an intended characteristic, or always answer in a specific style. As",
    "link": "http://arxiv.org/abs/2311.00168",
    "context": "Title: The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback. (arXiv:2311.00168v1 [cs.LG])\nAbstract: Reinforcement learning from human feedback (RLHF) has emerged as a powerful technique to make large language models (LLMs) easier to prompt and more capable in complex settings. RLHF at its core is providing a new toolkit to optimize LLMs other than next-token prediction, enabling the integration of qualitative training goals. The attempted match between user preferences and downstream performance, which happens in a learned reward model, results in an optimization landscape where training and evaluation metrics can appear correlated. The apparent correlation can lead to unexpected behaviors and stories of \"too much RLHF.\" In RLHF, challenges emerge because the following sub-modules are not consistent with each other: the reward model training, the policy model training, and the policy model evaluation. This mismatch results in models that sometimes avoid user requests for false safety flags, are difficult to steer to an intended characteristic, or always answer in a specific style. As",
    "path": "papers/23/11/2311.00168.json",
    "total_tokens": 965,
    "translated_title": "强化学习从人类反馈中的目标不匹配问题：对齐上限",
    "translated_abstract": "强化学习从人类反馈中的目标不匹配问题（RLHF）已经成为使大型语言模型（LLM）更易于提示并在复杂环境中更有能力的强大技术。RLHF核心是提供了一种优化LLM的新工具包，而不仅仅是下一个标记的预测，从而实现了定性训练目标的整合。在学习奖励模型中，用户偏好和下游性能之间的匹配尝试导致了一个优化景观，训练和评估指标看起来可能是相关的。这种表面上的相关关系可能导致意想不到的行为和“过度RLHF”的情况。在RLHF中，由于以下子模块不一致，会出现挑战：奖励模型训练、策略模型训练和策略模型评估。这种不匹配导致模型有时会避免用户请求的虚假安全标志，很难引导模型朝着预期的特征发展，或者总是以特定的风格回答。",
    "tldr": "这项研究探讨了强化学习从人类反馈中的目标不匹配问题。研究发现，在强化学习从人类反馈中，奖励模型训练、策略模型训练和策略模型评估之间存在不一致，导致模型行为的意想不到的结果。",
    "en_tdlr": "This study investigates the problem of objective mismatch in reinforcement learning from human feedback. The research finds that in reinforcement learning from human feedback, inconsistencies between reward model training, policy model training, and policy model evaluation can lead to unexpected outcomes in model behavior."
}