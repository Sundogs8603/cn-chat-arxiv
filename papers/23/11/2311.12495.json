{
    "title": "Multi-Objective Reinforcement Learning Based on Decomposition: A Taxonomy and Framework",
    "abstract": "Multi-objective reinforcement learning (MORL) extends traditional RL by seeking policies making different compromises among conflicting objectives. The recent surge of interest in MORL has led to diverse studies and solving methods, often drawing from existing knowledge in multi-objective optimization based on decomposition (MOO/D). Yet, a clear categorization based on both RL and MOO/D is lacking in the existing literature. Consequently, MORL researchers face difficulties when trying to classify contributions within a broader context due to the absence of a standardized taxonomy. To tackle such an issue, this paper introduces multi-objective reinforcement learning based on decomposition (MORL/D), a novel methodology bridging the literature of RL and MOO. A comprehensive taxonomy for MORL/D is presented, providing a structured foundation for categorizing existing and potential MORL works. The introduced taxonomy is then used to scrutinize MORL research, enhancing clarity and concisenes",
    "link": "https://arxiv.org/abs/2311.12495",
    "context": "Title: Multi-Objective Reinforcement Learning Based on Decomposition: A Taxonomy and Framework\nAbstract: Multi-objective reinforcement learning (MORL) extends traditional RL by seeking policies making different compromises among conflicting objectives. The recent surge of interest in MORL has led to diverse studies and solving methods, often drawing from existing knowledge in multi-objective optimization based on decomposition (MOO/D). Yet, a clear categorization based on both RL and MOO/D is lacking in the existing literature. Consequently, MORL researchers face difficulties when trying to classify contributions within a broader context due to the absence of a standardized taxonomy. To tackle such an issue, this paper introduces multi-objective reinforcement learning based on decomposition (MORL/D), a novel methodology bridging the literature of RL and MOO. A comprehensive taxonomy for MORL/D is presented, providing a structured foundation for categorizing existing and potential MORL works. The introduced taxonomy is then used to scrutinize MORL research, enhancing clarity and concisenes",
    "path": "papers/23/11/2311.12495.json",
    "total_tokens": 925,
    "translated_title": "基于分解的多目标强化学习：分类和框架",
    "translated_abstract": "多目标强化学习（MORL）通过寻求权衡冲突目标的策略扩展了传统的强化学习。对MORL的不断兴趣引发了各种各样的研究和解决方法，通常借鉴了基于分解的多目标优化的现有知识。然而，现有文献中缺乏基于RL和MOO/D的清晰分类。因此，MORL研究人员在尝试将贡献归类到更广泛的背景中时遇到困难，因为缺乏标准化的分类法。为了解决这个问题，本文介绍了基于分解的多目标强化学习（MORL/D），这是一种将RL和MOO文献联系起来的新方法。提出了一个全面的MORL/D分类法，为分类现有和潜在的MORL工作提供了结构化的基础。然后使用该分类法对MORL研究进行细致审查，增强了清晰度和简洁性。",
    "tldr": "基于分解的多目标强化学习（MORL/D）是一种新的方法，连接了强化学习和多目标优化领域。该论文提出了一个全面的MORL/D分类法，为现有和潜在的MORL工作提供了结构化的基础。",
    "en_tdlr": "Multi-Objective Reinforcement Learning based on Decomposition (MORL/D) is a novel approach that bridges the fields of reinforcement learning and multi-objective optimization. This paper presents a comprehensive taxonomy for MORL/D, providing a structured foundation for categorizing existing and potential MORL works."
}