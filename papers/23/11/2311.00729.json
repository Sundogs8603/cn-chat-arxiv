{
    "title": "ZEETAD: Adapting Pretrained Vision-Language Model for Zero-Shot End-to-End Temporal Action Detection. (arXiv:2311.00729v1 [cs.CV])",
    "abstract": "Temporal action detection (TAD) involves the localization and classification of action instances within untrimmed videos. While standard TAD follows fully supervised learning with closed-set setting on large training data, recent zero-shot TAD methods showcase the promising of open-set setting by leveraging large-scale contrastive visual-language (ViL) pretrained models. However, existing zero-shot TAD methods have limitations on how to properly construct the strong relationships between two interdependent tasks of localization and classification and adapt ViL model to video understanding. In this work, we present ZEETAD, featuring two modules: dual-localization and zero-shot proposal classification. The former is a Transformer-based module that detects action events while selectively collecting crucial semantic embeddings for later recognition. The latter one, CLIP-based module, generates semantic embeddings from text and frame inputs for each temporal unit. Additionally, we enhance d",
    "link": "http://arxiv.org/abs/2311.00729",
    "context": "Title: ZEETAD: Adapting Pretrained Vision-Language Model for Zero-Shot End-to-End Temporal Action Detection. (arXiv:2311.00729v1 [cs.CV])\nAbstract: Temporal action detection (TAD) involves the localization and classification of action instances within untrimmed videos. While standard TAD follows fully supervised learning with closed-set setting on large training data, recent zero-shot TAD methods showcase the promising of open-set setting by leveraging large-scale contrastive visual-language (ViL) pretrained models. However, existing zero-shot TAD methods have limitations on how to properly construct the strong relationships between two interdependent tasks of localization and classification and adapt ViL model to video understanding. In this work, we present ZEETAD, featuring two modules: dual-localization and zero-shot proposal classification. The former is a Transformer-based module that detects action events while selectively collecting crucial semantic embeddings for later recognition. The latter one, CLIP-based module, generates semantic embeddings from text and frame inputs for each temporal unit. Additionally, we enhance d",
    "path": "papers/23/11/2311.00729.json",
    "total_tokens": 942,
    "translated_title": "ZEETAD: 为零样本端到端时间动作检测改进预训练的视觉-语言模型",
    "translated_abstract": "时间动作检测（TAD）涉及在非剪辑视频中定位和分类动作实例。尽管传统TAD在大量训练数据上采用完全监督学习和封闭集设置，但最近的零样本TAD方法通过利用大规模对比的视觉-语言（ViL）预训练模型展示了开放集设置的潜力。然而，现有的零样本TAD方法在如何适当构建定位和分类这两个相互依赖的任务之间的强关系以及将ViL模型适应于视频理解方面存在局限性。在这项工作中，我们提出了ZEETAD，包含两个模块：双定位和零样本提议分类。前者是基于Transformer的模块，用于检测动作事件，并选择性地收集关键的语义嵌入以供后续识别。后者是基于CLIP的模块，用于为每个时间单位从文本和帧输入生成语义嵌入。",
    "tldr": "ZEETAD是一个零样本端到端时间动作检测模型，其中包括双定位和零样本提议分类两个模块。前者是基于Transformer的模块，用于检测动作事件并收集关键的语义嵌入，后者是基于CLIP的模块，用于生成文本和帧输入的语义嵌入。"
}