{
    "title": "Efficient Neural Ranking using Forward Indexes and Lightweight Encoders. (arXiv:2311.01263v1 [cs.IR])",
    "abstract": "Dual-encoder-based dense retrieval models have become the standard in IR. They employ large Transformer-based language models, which are notoriously inefficient in terms of resources and latency. We propose Fast-Forward indexes -- vector forward indexes which exploit the semantic matching capabilities of dual-encoder models for efficient and effective re-ranking. Our framework enables re-ranking at very high retrieval depths and combines the merits of both lexical and semantic matching via score interpolation. Furthermore, in order to mitigate the limitations of dual-encoders, we tackle two main challenges: Firstly, we improve computational efficiency by either pre-computing representations, avoiding unnecessary computations altogether, or reducing the complexity of encoders. This allows us to considerably improve ranking efficiency and latency. Secondly, we optimize the memory footprint and maintenance cost of indexes; we propose two complementary techniques to reduce the index size a",
    "link": "http://arxiv.org/abs/2311.01263",
    "context": "Title: Efficient Neural Ranking using Forward Indexes and Lightweight Encoders. (arXiv:2311.01263v1 [cs.IR])\nAbstract: Dual-encoder-based dense retrieval models have become the standard in IR. They employ large Transformer-based language models, which are notoriously inefficient in terms of resources and latency. We propose Fast-Forward indexes -- vector forward indexes which exploit the semantic matching capabilities of dual-encoder models for efficient and effective re-ranking. Our framework enables re-ranking at very high retrieval depths and combines the merits of both lexical and semantic matching via score interpolation. Furthermore, in order to mitigate the limitations of dual-encoders, we tackle two main challenges: Firstly, we improve computational efficiency by either pre-computing representations, avoiding unnecessary computations altogether, or reducing the complexity of encoders. This allows us to considerably improve ranking efficiency and latency. Secondly, we optimize the memory footprint and maintenance cost of indexes; we propose two complementary techniques to reduce the index size a",
    "path": "papers/23/11/2311.01263.json",
    "total_tokens": 867,
    "translated_title": "使用前向索引和轻量级编码器的高效神经排名",
    "translated_abstract": "基于双编码器的密集检索模型已经成为信息检索领域的标准。它们采用了大型的基于Transformer的语言模型，但这些模型在资源和延迟方面效率低下。我们提出了快速前向索引——利用双编码器模型的语义匹配能力进行高效和有效的重新排名。我们的框架可以在非常高的检索深度下进行重新排名，并通过分数插值结合了词汇匹配和语义匹配的优点。此外，为了减轻双编码器的局限性，我们解决了两个主要挑战：首先，通过预计算表示、避免不必要的计算或降低编码器的复杂度，提高了计算效率，降低了排名的资源消耗和延迟。其次，我们优化了索引的内存占用和维护成本；我们提出了两种互补的技术来减小索引的尺寸。",
    "tldr": "使用双编码器模型的语义匹配能力和快速前向索引，提高了神经排名的效率和延迟。同时，通过预计算表示和优化索引尺寸，进一步改善了计算效率和资源消耗。"
}