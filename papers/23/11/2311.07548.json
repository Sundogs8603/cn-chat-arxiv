{
    "title": "Interpretable Fine-Tuning for Graph Neural Network Surrogate Models",
    "abstract": "arXiv:2311.07548v2 Announce Type: replace  Abstract: Data-driven surrogate modeling has surged in capability in recent years with the emergence of graph neural networks (GNNs), which can operate directly on mesh-based representations of data. The goal of this work is to introduce an interpretable fine-tuning strategy for GNNs, with application to unstructured mesh-based fluid dynamics modeling. The end result is an enhanced fine-tuned model that isolates regions in physical space, corresponding to sub-graphs, that are intrinsically linked to the forecasting task while retaining the predictive capability of the baseline. These structures, identified by the fine-tuned GNNs, are adaptively produced in the forward pass and serve as explainable links between the baseline model architecture, the optimization goal, and known problem-specific physics. Additionally, through a regularization procedure, the fine-tuned GNNs can also be used to identify, during inference, graph nodes that correspon",
    "link": "https://arxiv.org/abs/2311.07548",
    "context": "Title: Interpretable Fine-Tuning for Graph Neural Network Surrogate Models\nAbstract: arXiv:2311.07548v2 Announce Type: replace  Abstract: Data-driven surrogate modeling has surged in capability in recent years with the emergence of graph neural networks (GNNs), which can operate directly on mesh-based representations of data. The goal of this work is to introduce an interpretable fine-tuning strategy for GNNs, with application to unstructured mesh-based fluid dynamics modeling. The end result is an enhanced fine-tuned model that isolates regions in physical space, corresponding to sub-graphs, that are intrinsically linked to the forecasting task while retaining the predictive capability of the baseline. These structures, identified by the fine-tuned GNNs, are adaptively produced in the forward pass and serve as explainable links between the baseline model architecture, the optimization goal, and known problem-specific physics. Additionally, through a regularization procedure, the fine-tuned GNNs can also be used to identify, during inference, graph nodes that correspon",
    "path": "papers/23/11/2311.07548.json",
    "total_tokens": 881,
    "translated_title": "可解释的图神经网络替代模型微调",
    "translated_abstract": "数据驱动的替代建模随着图神经网络（GNNs）的出现在最近几年内蓬勃发展，GNNs可以直接在基于网格的数据表示上运行。这项工作的目标是为GNN引入一种可解释的微调策略，应用于非结构网格化流体动力学建模。最终结果是一个增强的微调模型，它隔离了与预测任务密切相关的物理空间区域，相应于子图，同时保留了基线的预测能力。这些由微调的GNN识别出的结构在前向传递中是自适应生成的，并作为可解释的链接存在于基线模型架构、优化目标和已知问题特定物理之间。此外，通过正则化程序，微调的GNNs还可以在推断期间用于识别对应的图节点。",
    "tldr": "本论文引入了一种可解释的微调策略，通过应用于非结构网格化流体动力学建模的GNNs，增强了模型的预测能力，并通过识别可解释的物理空间区域及其对应的子图，帮助理解模型架构、优化目标和已知物理之间的关系。",
    "en_tdlr": "This paper introduces an interpretable fine-tuning strategy for GNNs applied to unstructured mesh-based fluid dynamics modeling, enhancing predictive capability by identifying explainable physical space regions and corresponding sub-graphs to aid in understanding the relationships between model architecture, optimization goals, and known physics."
}