{
    "title": "Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game. (arXiv:2311.01011v1 [cs.LG])",
    "abstract": "While Large Language Models (LLMs) are increasingly being used in real-world applications, they remain vulnerable to prompt injection attacks: malicious third party prompts that subvert the intent of the system designer. To help researchers study this problem, we present a dataset of over 126,000 prompt injection attacks and 46,000 prompt-based \"defenses\" against prompt injection, all created by players of an online game called Tensor Trust. To the best of our knowledge, this is currently the largest dataset of human-generated adversarial examples for instruction-following LLMs. The attacks in our dataset have a lot of easily interpretable stucture, and shed light on the weaknesses of LLMs. We also use the dataset to create a benchmark for resistance to two types of prompt injection, which we refer to as prompt extraction and prompt hijacking. Our benchmark results show that many models are vulnerable to the attack strategies in the Tensor Trust dataset. Furthermore, we show that some ",
    "link": "http://arxiv.org/abs/2311.01011",
    "context": "Title: Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game. (arXiv:2311.01011v1 [cs.LG])\nAbstract: While Large Language Models (LLMs) are increasingly being used in real-world applications, they remain vulnerable to prompt injection attacks: malicious third party prompts that subvert the intent of the system designer. To help researchers study this problem, we present a dataset of over 126,000 prompt injection attacks and 46,000 prompt-based \"defenses\" against prompt injection, all created by players of an online game called Tensor Trust. To the best of our knowledge, this is currently the largest dataset of human-generated adversarial examples for instruction-following LLMs. The attacks in our dataset have a lot of easily interpretable stucture, and shed light on the weaknesses of LLMs. We also use the dataset to create a benchmark for resistance to two types of prompt injection, which we refer to as prompt extraction and prompt hijacking. Our benchmark results show that many models are vulnerable to the attack strategies in the Tensor Trust dataset. Furthermore, we show that some ",
    "path": "papers/23/11/2311.01011.json",
    "total_tokens": 881,
    "translated_title": "Tensor Trust：来自在线游戏的可解释提示注入攻击",
    "translated_abstract": "随着大型语言模型（LLMs）在现实应用中的增加，它们仍然容易受到提示注入攻击的威胁：恶意第三方提示会扭曲系统设计者的意图。为了帮助研究人员研究此问题，我们提供了一个包含超过126,000个提示注入攻击和46,000个基于提示的“防御”（由一个名为Tensor Trust的在线游戏的玩家创建）的数据集。据我们所知，这是目前针对LLMs的最大的人工生成的对抗性示例数据集。我们的数据集中的攻击具有很多易于解释的结构，并揭示了LLMs的弱点。我们还利用该数据集创建了两种类型的提示注入抵抗基准，分别称为提示提取和提示劫持。我们的基准结果显示，许多模型容易受到Tensor Trust数据集中的攻击策略的攻击。此外，我们还表明，一些",
    "tldr": "这项研究提供了一个解释性的提示注入攻击的数据集，并利用该数据集创建了两种类型的抵抗基准。研究结果表明，许多模型容易受到这些攻击策略的攻击。"
}