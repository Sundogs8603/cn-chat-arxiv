{
    "title": "Simplifying Transformer Blocks. (arXiv:2311.01906v1 [cs.LG])",
    "abstract": "A simple design recipe for deep Transformers is to compose identical building blocks. But standard transformer blocks are far from simple, interweaving attention and MLP sub-blocks with skip connections & normalisation layers in precise arrangements. This complexity leads to brittle architectures, where seemingly minor changes can significantly reduce training speed, or render models untrainable.  In this work, we ask to what extent the standard transformer block can be simplified? Combining signal propagation theory and empirical observations, we motivate modifications that allow many block components to be removed with no loss of training speed, including skip connections, projection or value parameters, sequential sub-blocks and normalisation layers. In experiments on both autoregressive decoder-only and BERT encoder-only models, our simplified transformers emulate the per-update training speed and performance of standard transformers, while enjoying 15% faster training throughput, ",
    "link": "http://arxiv.org/abs/2311.01906",
    "context": "Title: Simplifying Transformer Blocks. (arXiv:2311.01906v1 [cs.LG])\nAbstract: A simple design recipe for deep Transformers is to compose identical building blocks. But standard transformer blocks are far from simple, interweaving attention and MLP sub-blocks with skip connections & normalisation layers in precise arrangements. This complexity leads to brittle architectures, where seemingly minor changes can significantly reduce training speed, or render models untrainable.  In this work, we ask to what extent the standard transformer block can be simplified? Combining signal propagation theory and empirical observations, we motivate modifications that allow many block components to be removed with no loss of training speed, including skip connections, projection or value parameters, sequential sub-blocks and normalisation layers. In experiments on both autoregressive decoder-only and BERT encoder-only models, our simplified transformers emulate the per-update training speed and performance of standard transformers, while enjoying 15% faster training throughput, ",
    "path": "papers/23/11/2311.01906.json",
    "total_tokens": 877,
    "translated_title": "简化Transformer块",
    "translated_abstract": "对于深度Transformer，一个简单的设计方法是组合相同的构建块。但是标准的Transformer块远非简单，它们将注意力和MLP子块与跳连接和标准化层以精确的方式交织在一起。这种复杂性导致了脆弱的架构，即似乎微小的改变可能会显著降低训练速度，或使模型无法训练。在这项工作中，我们探讨了标准Transformer块可以被简化到什么程度？结合信号传播理论和实证观察，我们提出了可以去除许多块组件而不损失训练速度的修改，包括跳连接、投影或值参数、序列子块和标准化层。在自回归解码器和BERT编码器模型的实验中，我们简化的Transformer块在保持训练速度和性能的基础上，比标准Transformer块的训练吞吐量提高了15%。",
    "tldr": "本论文通过结合信号传播理论和实证观察，提出了一种简化Transformer块的方法。这种简化方法可以去除许多不影响训练速度的组件，并且在保持性能的同时比标准Transformer块的训练吞吐量提高了15%。",
    "en_tdlr": "This paper proposes a method to simplify Transformer blocks by combining signal propagation theory and empirical observations. This approach allows the removal of many components without affecting the training speed and achieves a 15% increase in training throughput compared to standard Transformer blocks, while maintaining performance."
}