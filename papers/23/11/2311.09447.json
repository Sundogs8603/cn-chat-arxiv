{
    "title": "How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities",
    "abstract": "arXiv:2311.09447v2 Announce Type: replace-cross  Abstract: The rapid progress in open-source Large Language Models (LLMs) is significantly driving AI development forward. However, there is still a limited understanding of their trustworthiness. Deploying these models at scale without sufficient trustworthiness can pose significant risks, highlighting the need to uncover these issues promptly. In this work, we conduct an adversarial assessment of open-source LLMs on trustworthiness, scrutinizing them across eight different aspects including toxicity, stereotypes, ethics, hallucination, fairness, sycophancy, privacy, and robustness against adversarial demonstrations. We propose advCoU, an extended Chain of Utterances-based (CoU) prompting strategy by incorporating carefully crafted malicious demonstrations for trustworthiness attack. Our extensive experiments encompass recent and representative series of open-source LLMs, including Vicuna, MPT, Falcon, Mistral, and Llama 2. The empirical",
    "link": "https://arxiv.org/abs/2311.09447",
    "context": "Title: How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities\nAbstract: arXiv:2311.09447v2 Announce Type: replace-cross  Abstract: The rapid progress in open-source Large Language Models (LLMs) is significantly driving AI development forward. However, there is still a limited understanding of their trustworthiness. Deploying these models at scale without sufficient trustworthiness can pose significant risks, highlighting the need to uncover these issues promptly. In this work, we conduct an adversarial assessment of open-source LLMs on trustworthiness, scrutinizing them across eight different aspects including toxicity, stereotypes, ethics, hallucination, fairness, sycophancy, privacy, and robustness against adversarial demonstrations. We propose advCoU, an extended Chain of Utterances-based (CoU) prompting strategy by incorporating carefully crafted malicious demonstrations for trustworthiness attack. Our extensive experiments encompass recent and representative series of open-source LLMs, including Vicuna, MPT, Falcon, Mistral, and Llama 2. The empirical",
    "path": "papers/23/11/2311.09447.json",
    "total_tokens": 904,
    "translated_title": "开源LLMs的可信度有多高？对恶意演示下的评估显示它们的脆弱性",
    "translated_abstract": "arXiv:2311.09447v2 公告类型：替换-交叉摘要：开源大型语言模型（LLMs）的快速发展显著推动着人工智能的发展。然而，对它们的可信度仍然了解有限。在规模部署这些模型，而没有足够的可信度可能会带来重大风险，突出了及时发现这些问题的重要性。本文对开源LLMs在可信度上进行了敌对评估，跨足毒性、陈规俗套、伦理、幻觉、公平性、谄媚、隐私以及对抗性演示的八个不同方面。我们提出了advCoU，一种基于扩展的Chain of Utterances（CoU）提示策略，通过加入精心设计的恶意演示来攻击可信度。我们的广泛实验涵盖了最近和代表性系列的开源LLMs，包括Vicuna、MPT、Falcon、Mistral和Llama 2。",
    "tldr": "本研究通过恶意演示在八个方面对开源LLMs的可信度进行了敌对评估，提出了一种新的攻击策略advCoU，以揭示它们的脆弱性。",
    "en_tdlr": "This study conducted an adversarial assessment of open-source LLMs in eight aspects under malicious demonstrations, proposing a new attack strategy advCoU to reveal their vulnerabilities."
}