{
    "title": "Learning Defect Prediction from Unrealistic Data. (arXiv:2311.00931v1 [cs.LG])",
    "abstract": "Pretrained models of code, such as CodeBERT and CodeT5, have become popular choices for code understanding and generation tasks. Such models tend to be large and require commensurate volumes of training data, which are rarely available for downstream tasks. Instead, it has become popular to train models with far larger but less realistic datasets, such as functions with artificially injected bugs. Models trained on such data, however, tend to only perform well on similar data, while underperforming on real world programs. In this paper, we conjecture that this discrepancy stems from the presence of distracting samples that steer the model away from the real-world task distribution. To investigate this conjecture, we propose an approach for identifying the subsets of these large yet unrealistic datasets that are most similar to examples in real-world datasets based on their learned representations. Our approach extracts high-dimensional embeddings of both real-world and artificial progr",
    "link": "http://arxiv.org/abs/2311.00931",
    "context": "Title: Learning Defect Prediction from Unrealistic Data. (arXiv:2311.00931v1 [cs.LG])\nAbstract: Pretrained models of code, such as CodeBERT and CodeT5, have become popular choices for code understanding and generation tasks. Such models tend to be large and require commensurate volumes of training data, which are rarely available for downstream tasks. Instead, it has become popular to train models with far larger but less realistic datasets, such as functions with artificially injected bugs. Models trained on such data, however, tend to only perform well on similar data, while underperforming on real world programs. In this paper, we conjecture that this discrepancy stems from the presence of distracting samples that steer the model away from the real-world task distribution. To investigate this conjecture, we propose an approach for identifying the subsets of these large yet unrealistic datasets that are most similar to examples in real-world datasets based on their learned representations. Our approach extracts high-dimensional embeddings of both real-world and artificial progr",
    "path": "papers/23/11/2311.00931.json",
    "total_tokens": 832,
    "translated_title": "从不真实数据中学习缺陷预测",
    "translated_abstract": "预训练的代码模型，如CodeBERT和CodeT5，成为代码理解和生成任务的流行选择。这些模型往往庞大且需要相应数量的训练数据，但在下游任务中很少提供。相反，使用远比真实数据集更大但不真实的数据集（如人为注入缺陷的函数）来训练模型已经变得流行。然而，使用此类数据训练的模型往往只在类似数据上表现良好，在真实世界程序上表现不佳。本文假设这种差异是由于存在干扰样本，这些样本使模型偏离了真实世界任务分布。为了验证这个假设，我们提出了一种基于学习表示的方法，用于识别这些大而不真实的数据集中与真实数据集中的示例最相似的子集。我们的方法提取了真实世界和人工程序的高维嵌入表示。",
    "tldr": "该论文研究了从不真实的数据集中学习缺陷预测的问题，并提出了一种基于学习表示的方法来识别与真实数据集最相似的子集。",
    "en_tdlr": "This paper investigates the problem of learning defect prediction from unrealistic datasets and proposes an approach based on learned representations to identify subsets that are most similar to real-world examples."
}