{
    "title": "Sequential Subset Matching for Dataset Distillation. (arXiv:2311.01570v1 [cs.CV])",
    "abstract": "Dataset distillation is a newly emerging task that synthesizes a small-size dataset used in training deep neural networks (DNNs) for reducing data storage and model training costs. The synthetic datasets are expected to capture the essence of the knowledge contained in real-world datasets such that the former yields a similar performance as the latter. Recent advancements in distillation methods have produced notable improvements in generating synthetic datasets. However, current state-of-the-art methods treat the entire synthetic dataset as a unified entity and optimize each synthetic instance equally. This static optimization approach may lead to performance degradation in dataset distillation. Specifically, we argue that static optimization can give rise to a coupling issue within the synthetic data, particularly when a larger amount of synthetic data is being optimized. This coupling issue, in turn, leads to the failure of the distilled dataset to extract the high-level features le",
    "link": "http://arxiv.org/abs/2311.01570",
    "context": "Title: Sequential Subset Matching for Dataset Distillation. (arXiv:2311.01570v1 [cs.CV])\nAbstract: Dataset distillation is a newly emerging task that synthesizes a small-size dataset used in training deep neural networks (DNNs) for reducing data storage and model training costs. The synthetic datasets are expected to capture the essence of the knowledge contained in real-world datasets such that the former yields a similar performance as the latter. Recent advancements in distillation methods have produced notable improvements in generating synthetic datasets. However, current state-of-the-art methods treat the entire synthetic dataset as a unified entity and optimize each synthetic instance equally. This static optimization approach may lead to performance degradation in dataset distillation. Specifically, we argue that static optimization can give rise to a coupling issue within the synthetic data, particularly when a larger amount of synthetic data is being optimized. This coupling issue, in turn, leads to the failure of the distilled dataset to extract the high-level features le",
    "path": "papers/23/11/2311.01570.json",
    "total_tokens": 808,
    "translated_title": "数据集精简的序列子集匹配",
    "translated_abstract": "数据集精简是一项新兴任务，用于合成一个用于训练深度神经网络（DNN）的小型数据集，以降低数据存储和模型训练成本。期望合成数据集能够捕捉到真实数据集中所包含的知识精髓，从而产生与真实数据集相似的性能。最近，数据集精简方法的进展在生成合成数据集方面取得了显著的改进。然而，目前的最先进方法将整个合成数据集视为统一实体，并对每个合成实例进行相同的优化。这种静态优化方法可能导致数据集精简的性能下降。具体来说，我们认为静态优化可能会导致合成数据中的耦合问题，特别是在优化较大量的合成数据时。这种耦合问题进而导致精简数据集无法提取高级特征。",
    "tldr": "这项研究提出了一种序列子集匹配方法来解决数据集精简时的性能下降问题，通过优化每个合成实例的方式提高了生成的合成数据集的质量。"
}