{
    "title": "Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization. (arXiv:2311.03351v3 [cs.LG] UPDATED)",
    "abstract": "Combining offline and online reinforcement learning (RL) is crucial for efficient and safe learning. However, previous approaches treat offline and online learning as separate procedures, resulting in redundant designs and limited performance. We ask: Can we achieve straightforward yet effective offline and online learning without introducing extra conservatism or regularization? In this study, we propose Uni-o4, which utilizes an on-policy objective for both offline and online learning. Owning to the alignment of objectives in two phases, the RL agent can transfer between offline and online learning seamlessly. This property enhances the flexibility of the learning paradigm, allowing for arbitrary combinations of pretraining, fine-tuning, offline, and online learning. In the offline phase, specifically, Uni-o4 leverages diverse ensemble policies to address the mismatch issues between the estimated behavior policy and the offline dataset. Through a simple offline policy evaluation (OPE",
    "link": "http://arxiv.org/abs/2311.03351",
    "context": "Title: Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization. (arXiv:2311.03351v3 [cs.LG] UPDATED)\nAbstract: Combining offline and online reinforcement learning (RL) is crucial for efficient and safe learning. However, previous approaches treat offline and online learning as separate procedures, resulting in redundant designs and limited performance. We ask: Can we achieve straightforward yet effective offline and online learning without introducing extra conservatism or regularization? In this study, we propose Uni-o4, which utilizes an on-policy objective for both offline and online learning. Owning to the alignment of objectives in two phases, the RL agent can transfer between offline and online learning seamlessly. This property enhances the flexibility of the learning paradigm, allowing for arbitrary combinations of pretraining, fine-tuning, offline, and online learning. In the offline phase, specifically, Uni-o4 leverages diverse ensemble policies to address the mismatch issues between the estimated behavior policy and the offline dataset. Through a simple offline policy evaluation (OPE",
    "path": "papers/23/11/2311.03351.json",
    "total_tokens": 900,
    "translated_title": "Uni-O4: 将在线与离线深度强化学习统一起来，采用多步在线策略优化",
    "translated_abstract": "将离线和在线强化学习相结合对于高效和安全的学习至关重要。然而，以往的方法将离线和在线学习视为独立的过程，导致重复的设计和有限的性能。本文中，我们提出了Uni-o4，它在离线和在线学习中都使用了一个在线策略目标。由于两个阶段的目标对齐，RL代理可以在离线和在线学习之间无缝传递。这种性质增强了学习范式的灵活性，允许任意组合预训练、微调、离线和在线学习。在离线阶段，Uni-o4利用多样的集合策略来解决估计行为策略和离线数据集之间的不匹配问题。通过简单的离线策略评估（OPE）",
    "tldr": "Uni-O4提出了统一的离线和在线深度强化学习方法，通过对齐目标实现了无缝传递，增强了学习范式的灵活性。在离线阶段，Uni-O4利用多样的集合策略解决了估计行为策略和离线数据集不匹配的问题。",
    "en_tdlr": "Uni-O4 proposes a unified offline and online deep reinforcement learning method that achieves seamless transfer and enhances the flexibility of the learning paradigm. In the offline phase, Uni-O4 addresses the mismatch between the estimated behavior policy and the offline dataset using diverse ensemble policies."
}