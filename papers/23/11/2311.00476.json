{
    "title": "Group Distributionally Robust Knowledge Distillation. (arXiv:2311.00476v1 [cs.CV])",
    "abstract": "Knowledge distillation enables fast and effective transfer of features learned from a bigger model to a smaller one. However, distillation objectives are susceptible to sub-population shifts, a common scenario in medical imaging analysis which refers to groups/domains of data that are underrepresented in the training set. For instance, training models on health data acquired from multiple scanners or hospitals can yield subpar performance for minority groups. In this paper, inspired by distributionally robust optimization (DRO) techniques, we address this shortcoming by proposing a group-aware distillation loss. During optimization, a set of weights is updated based on the per-group losses at a given iteration. This way, our method can dynamically focus on groups that have low performance during training. We empirically validate our method, GroupDistil on two benchmark datasets (natural images and cardiac MRIs) and show consistent improvement in terms of worst-group accuracy.",
    "link": "http://arxiv.org/abs/2311.00476",
    "context": "Title: Group Distributionally Robust Knowledge Distillation. (arXiv:2311.00476v1 [cs.CV])\nAbstract: Knowledge distillation enables fast and effective transfer of features learned from a bigger model to a smaller one. However, distillation objectives are susceptible to sub-population shifts, a common scenario in medical imaging analysis which refers to groups/domains of data that are underrepresented in the training set. For instance, training models on health data acquired from multiple scanners or hospitals can yield subpar performance for minority groups. In this paper, inspired by distributionally robust optimization (DRO) techniques, we address this shortcoming by proposing a group-aware distillation loss. During optimization, a set of weights is updated based on the per-group losses at a given iteration. This way, our method can dynamically focus on groups that have low performance during training. We empirically validate our method, GroupDistil on two benchmark datasets (natural images and cardiac MRIs) and show consistent improvement in terms of worst-group accuracy.",
    "path": "papers/23/11/2311.00476.json",
    "total_tokens": 886,
    "translated_title": "分组分布鲁棒知识蒸馏",
    "translated_abstract": "知识蒸馏可以快速有效地从大模型向小模型传输学到的特征。然而，蒸馏目标对亚群体的偏移特别敏感，而医学影像分析中常见的是在训练集中存在少数群体/领域的数据。例如，使用来自多个扫描仪或医院获得的健康数据训练模型可能对少数群体表现出较差的性能。受分布鲁棒优化（DRO）技术的启发，本文通过提出一种群组感知的蒸馏损失来解决这个问题。在优化过程中，根据给定迭代中的每个群组损失来更新一组权重。这种方法可以在训练期间动态地关注表现较差的群组。我们在两个基准数据集（自然图像和心脏MRI）上进行了实证验证，结果显示在最差群组准确性方面，我们的方法GroupDistil始终有一致的改进。",
    "tldr": "本文提出了一种群组感知的蒸馏损失来解决知识蒸馏在医学影像分析中的亚群体偏移问题，为在训练期间动态关注表现较差的群组提供了方法。",
    "en_tdlr": "This paper proposes a group-aware distillation loss to address sub-population shifts in knowledge distillation in medical imaging analysis, providing a method to dynamically focus on groups with low performance during training."
}