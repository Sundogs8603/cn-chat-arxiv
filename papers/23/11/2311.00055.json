{
    "title": "Training-Free Generalization on Heterogeneous Tabular Data via Meta-Representation. (arXiv:2311.00055v1 [cs.LG])",
    "abstract": "Tabular data is prevalent across various machine learning domains. Yet, the inherent heterogeneities in attribute and class spaces across different tabular datasets hinder the effective sharing of knowledge, limiting a tabular model to benefit from other datasets. In this paper, we propose Tabular data Pre-Training via Meta-representation (TabPTM), which allows one tabular model pre-training on a set of heterogeneous datasets. Then, this pre-trained model can be directly applied to unseen datasets that have diverse attributes and classes without additional training. Specifically, TabPTM represents an instance through its distance to a fixed number of prototypes, thereby standardizing heterogeneous tabular datasets. A deep neural network is then trained to associate these meta-representations with dataset-specific classification confidences, endowing TabPTM with the ability of training-free generalization. Experiments validate that TabPTM achieves promising performance in new datasets, ",
    "link": "http://arxiv.org/abs/2311.00055",
    "context": "Title: Training-Free Generalization on Heterogeneous Tabular Data via Meta-Representation. (arXiv:2311.00055v1 [cs.LG])\nAbstract: Tabular data is prevalent across various machine learning domains. Yet, the inherent heterogeneities in attribute and class spaces across different tabular datasets hinder the effective sharing of knowledge, limiting a tabular model to benefit from other datasets. In this paper, we propose Tabular data Pre-Training via Meta-representation (TabPTM), which allows one tabular model pre-training on a set of heterogeneous datasets. Then, this pre-trained model can be directly applied to unseen datasets that have diverse attributes and classes without additional training. Specifically, TabPTM represents an instance through its distance to a fixed number of prototypes, thereby standardizing heterogeneous tabular datasets. A deep neural network is then trained to associate these meta-representations with dataset-specific classification confidences, endowing TabPTM with the ability of training-free generalization. Experiments validate that TabPTM achieves promising performance in new datasets, ",
    "path": "papers/23/11/2311.00055.json",
    "total_tokens": 824,
    "translated_title": "通过元表示对异构表格数据进行无训练泛化",
    "translated_abstract": "表格数据在各种机器学习领域中普遍存在。然而，不同表格数据集中属性和类别空间的固有异质性阻碍了知识的有效共享，限制了表格模型从其他数据集中受益。在本文中，我们提出了通过元表示进行表格数据预训练（TabPTM），它允许一个表格模型在一组异构数据集上进行预训练。然后，这个预训练模型可以直接应用于具有不同属性和类别的未见过的数据集，无需额外训练。具体而言，TabPTM通过实例到固定数量的原型的距离来表示一个实例，从而标准化异构表格数据集。然后，一个深度神经网络被训练来将这些元表示与数据集特定的分类置信度关联起来，使TabPTM具有无需训练的泛化能力。实验证实TabPTM在新数据集上具有良好的性能。",
    "tldr": "提出了通过元表示进行表格数据预训练的方法，使得模型可以在异构数据集上进行无训练泛化的应用。"
}