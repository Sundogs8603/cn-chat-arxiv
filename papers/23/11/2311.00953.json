{
    "title": "Blending Reward Functions via Few Expert Demonstrations for Faithful and Accurate Knowledge-Grounded Dialogue Generation. (arXiv:2311.00953v1 [cs.CL])",
    "abstract": "The development of trustworthy conversational information-seeking systems relies on dialogue models that can generate faithful and accurate responses based on relevant knowledge texts. However, two main challenges hinder this task. Firstly, language models may generate hallucinations due to data biases present in their pretraining corpus. Secondly, knowledge texts often contain redundant and irrelevant information that distracts the model's attention from the relevant text span. Previous works use additional data annotations on the knowledge texts to learn a knowledge identification module in order to bypass irrelevant information, but collecting such high-quality span annotations can be costly. In this work, we leverage reinforcement learning algorithms to overcome the above challenges by introducing a novel reward function. Our reward function combines an accuracy metric and a faithfulness metric to provide a balanced quality judgment of generated responses, which can be used as a co",
    "link": "http://arxiv.org/abs/2311.00953",
    "context": "Title: Blending Reward Functions via Few Expert Demonstrations for Faithful and Accurate Knowledge-Grounded Dialogue Generation. (arXiv:2311.00953v1 [cs.CL])\nAbstract: The development of trustworthy conversational information-seeking systems relies on dialogue models that can generate faithful and accurate responses based on relevant knowledge texts. However, two main challenges hinder this task. Firstly, language models may generate hallucinations due to data biases present in their pretraining corpus. Secondly, knowledge texts often contain redundant and irrelevant information that distracts the model's attention from the relevant text span. Previous works use additional data annotations on the knowledge texts to learn a knowledge identification module in order to bypass irrelevant information, but collecting such high-quality span annotations can be costly. In this work, we leverage reinforcement learning algorithms to overcome the above challenges by introducing a novel reward function. Our reward function combines an accuracy metric and a faithfulness metric to provide a balanced quality judgment of generated responses, which can be used as a co",
    "path": "papers/23/11/2311.00953.json",
    "total_tokens": 895,
    "translated_title": "通过少量专家演示融合奖励函数，用于忠实准确的基于知识的对话生成",
    "translated_abstract": "构建可信赖的对话信息寻求系统需要能够基于相关知识文本生成忠实准确回应的对话模型。然而，这个任务面临两个主要挑战。首先，语言模型可能由于预训练语料库中存在的数据偏见而产生幻觉。其次，知识文本通常包含多余和不相关的信息，这会分散模型对相关文本范围的注意力。以前的研究使用额外的数据注释在知识文本上学习知识识别模块，以绕过不相关信息，但是收集这样的高质量范围注释可能是昂贵的。在这项工作中，我们利用强化学习算法通过引入新的奖励函数克服上述挑战。我们的奖励函数将准确度指标和忠实度指标结合起来，提供一个平衡的生成回应质量评判，这可以作为一个协同参考标准。",
    "tldr": "本研究通过融合准确度指标和忠实度指标的新奖励函数，利用强化学习算法解决了语言模型幻象和知识文本多余信息问题，提供了一种平衡的生成对话回应质量评判方法。"
}