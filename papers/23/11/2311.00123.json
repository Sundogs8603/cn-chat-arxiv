{
    "title": "Q-Learning for Stochastic Control under General Information Structures and Non-Markovian Environments. (arXiv:2311.00123v1 [math.OC])",
    "abstract": "As a primary contribution, we present a convergence theorem for stochastic iterations, and in particular, Q-learning iterates, under a general, possibly non-Markovian, stochastic environment. Our conditions for convergence involve an ergodicity and a positivity criterion. We provide a precise characterization on the limit of the iterates and conditions on the environment and initializations for convergence. As our second contribution, we discuss the implications and applications of this theorem to a variety of stochastic control problems with non-Markovian environments involving (i) quantized approximations of fully observed Markov Decision Processes (MDPs) with continuous spaces (where quantization break down the Markovian structure), (ii) quantized approximations of belief-MDP reduced partially observable MDPS (POMDPs) with weak Feller continuity and a mild version of filter stability (which requires the knowledge of the model by the controller), (iii) finite window approximations of",
    "link": "http://arxiv.org/abs/2311.00123",
    "context": "Title: Q-Learning for Stochastic Control under General Information Structures and Non-Markovian Environments. (arXiv:2311.00123v1 [math.OC])\nAbstract: As a primary contribution, we present a convergence theorem for stochastic iterations, and in particular, Q-learning iterates, under a general, possibly non-Markovian, stochastic environment. Our conditions for convergence involve an ergodicity and a positivity criterion. We provide a precise characterization on the limit of the iterates and conditions on the environment and initializations for convergence. As our second contribution, we discuss the implications and applications of this theorem to a variety of stochastic control problems with non-Markovian environments involving (i) quantized approximations of fully observed Markov Decision Processes (MDPs) with continuous spaces (where quantization break down the Markovian structure), (ii) quantized approximations of belief-MDP reduced partially observable MDPS (POMDPs) with weak Feller continuity and a mild version of filter stability (which requires the knowledge of the model by the controller), (iii) finite window approximations of",
    "path": "papers/23/11/2311.00123.json",
    "total_tokens": 948,
    "translated_title": "Q-Learning用于通用信息结构和非马尔可夫环境下的随机控制",
    "translated_abstract": "作为主要贡献，我们提出了一个收敛定理，特别是对于一般的、可能为非马尔可夫的随机环境下的Q-学习迭代。我们的收敛条件涉及到一个遍历性和一个正性准则。我们对迭代的极限和收敛的环境和初始化条件进行了精确的描述。作为我们的第二个贡献，我们讨论了这个定理对于多种具有非马尔可夫环境的随机控制问题的影响和应用，其中包括(i)连续空间的完全观测马尔科夫决策过程（MDPs）的量化近似（量化破坏了马尔可夫结构），(ii)量化近似的置信MDP约化部分可观察MDPS（POMDPs） with 弱Feller连续性和滤波器稳定的轻微版本（控制器需要了解模型），(iii)有限窗口近似。",
    "tldr": "该论文主要贡献是提出了一个对于非马尔可夫环境下的随机迭代（特别是Q-learning迭代）进行收敛的定理，并给出了收敛条件。其次，讨论了该定理在多种具有非马尔可夫环境的随机控制问题中的应用。"
}