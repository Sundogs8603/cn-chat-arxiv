{
    "title": "Safer-Instruct: Aligning Language Models with Automated Preference Data",
    "abstract": "arXiv:2311.08685v2 Announce Type: replace-cross  Abstract: Reinforcement learning from human feedback (RLHF) is a vital strategy for enhancing model capability in language models. However, annotating preference data for RLHF is a resource-intensive and creativity-demanding process, while existing automatic generation methods face limitations in data diversity and quality. In response, we present Safer-Instruct, a novel pipeline for automatically constructing large-scale preference data. Our approach leverages reversed instruction tuning, instruction induction, and expert model evaluation to efficiently generate high-quality preference data without human annotators. To verify the effectiveness of Safer-Instruct, we apply the pipeline to construct a safety preference dataset as a case study. Finetuning an Alpaca model on this synthetic dataset not only demonstrates improved harmlessness but also outperforms models fine-tuned on human-annotated safety preference data, all the while mainta",
    "link": "https://arxiv.org/abs/2311.08685",
    "context": "Title: Safer-Instruct: Aligning Language Models with Automated Preference Data\nAbstract: arXiv:2311.08685v2 Announce Type: replace-cross  Abstract: Reinforcement learning from human feedback (RLHF) is a vital strategy for enhancing model capability in language models. However, annotating preference data for RLHF is a resource-intensive and creativity-demanding process, while existing automatic generation methods face limitations in data diversity and quality. In response, we present Safer-Instruct, a novel pipeline for automatically constructing large-scale preference data. Our approach leverages reversed instruction tuning, instruction induction, and expert model evaluation to efficiently generate high-quality preference data without human annotators. To verify the effectiveness of Safer-Instruct, we apply the pipeline to construct a safety preference dataset as a case study. Finetuning an Alpaca model on this synthetic dataset not only demonstrates improved harmlessness but also outperforms models fine-tuned on human-annotated safety preference data, all the while mainta",
    "path": "papers/23/11/2311.08685.json",
    "total_tokens": 839,
    "translated_title": "Safer-Instruct: 使用自动化偏好数据对齐语言模型",
    "translated_abstract": "人工反馈强化学习（RLHF）是增强语言模型能力的重要策略。然而，为RLHF标注偏好数据是一项资源密集且需要创造力的过程，而现有的自动生成方法在数据多样性和质量方面存在局限性。为了应对这一挑战，我们提出了Safer-Instruct，这是一个用于自动构建大规模偏好数据的全新流水线。我们的方法利用了反向指导调整、指导感应和专家模型评估，以高效生成高质量的偏好数据，无需人工标注者。为了验证Safer-Instruct的有效性，我们将该流水线应用于构建一个安全偏好数据集作为案例研究。在这个合成数据集上微调Alpaca模型不仅展示出更好的无害性，还表现出优于在人工标注的安全偏好数据上微调的模型，同时保持",
    "tldr": "Safer-Instruct通过反向指导调整、指导感应和专家模型评估的方式，实现了自动构建大规模偏好数据的目的，从而在没有人工标注者的情况下高效生成高质量的偏好数据。",
    "en_tdlr": "Safer-Instruct achieves the automatic construction of large-scale preference data through reversed instruction tuning, instruction induction, and expert model evaluation, efficiently generating high-quality preference data without human annotators."
}