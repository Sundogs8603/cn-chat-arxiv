{
    "title": "Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information",
    "abstract": "arXiv:2311.11509v3 Announce Type: replace  Abstract: In recent years, Large Language Models (LLM) have emerged as pivotal tools in various applications. However, these models are susceptible to adversarial prompt attacks, where attackers can carefully curate input strings that mislead LLMs into generating incorrect or undesired outputs. Previous work has revealed that with relatively simple yet effective attacks based on discrete optimization, it is possible to generate adversarial prompts that bypass moderation and alignment of the models. This vulnerability to adversarial prompts underscores a significant concern regarding the robustness and reliability of LLMs. Our work aims to address this concern by introducing a novel approach to detecting adversarial prompts at a token level, leveraging the LLM's capability to predict the next token's probability. We measure the degree of the model's perplexity, where tokens predicted with high probability are considered normal, and those exhibi",
    "link": "https://arxiv.org/abs/2311.11509",
    "context": "Title: Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information\nAbstract: arXiv:2311.11509v3 Announce Type: replace  Abstract: In recent years, Large Language Models (LLM) have emerged as pivotal tools in various applications. However, these models are susceptible to adversarial prompt attacks, where attackers can carefully curate input strings that mislead LLMs into generating incorrect or undesired outputs. Previous work has revealed that with relatively simple yet effective attacks based on discrete optimization, it is possible to generate adversarial prompts that bypass moderation and alignment of the models. This vulnerability to adversarial prompts underscores a significant concern regarding the robustness and reliability of LLMs. Our work aims to address this concern by introducing a novel approach to detecting adversarial prompts at a token level, leveraging the LLM's capability to predict the next token's probability. We measure the degree of the model's perplexity, where tokens predicted with high probability are considered normal, and those exhibi",
    "path": "papers/23/11/2311.11509.json",
    "total_tokens": 820,
    "translated_title": "基于困惑度量和上下文信息的令牌级对抗提示检测",
    "translated_abstract": "近年来，大型语言模型(LLM)已成为各种应用中的关键工具。然而，这些模型容易受到对抗性提示攻击，攻击者可以精心策划输入字符串，误导LLM生成不正确或不希望的输出。先前的研究揭示了利用离散优化的相对简单却有效的攻击方式可以生成绕过模型的调整和对齐的对抗性提示。对对抗性提示的脆弱性凸显了对LLM健壮性和可靠性的重要关注。我们的工作旨在通过引入一种新颖方法，在令牌级别检测对抗性提示，利用LLM预测下一个标记的概率能力。我们测量模型困惑度的程度，其中高概率预测的令牌被视为正常，而那些表现异常的则可能是对抗性提示。",
    "tldr": "该论文引入了一种新的方法，在令牌级别检测对抗性提示，利用语言模型的能力预测下一个标记的概率。",
    "en_tdlr": "The paper introduces a novel approach to detecting adversarial prompts at a token level, leveraging the language model's capability to predict the next token's probability."
}