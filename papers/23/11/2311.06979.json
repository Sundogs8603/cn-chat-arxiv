{
    "title": "Assessing the Interpretability of Programmatic Policies with Large Language Models. (arXiv:2311.06979v2 [cs.AI] UPDATED)",
    "abstract": "Although the synthesis of programs encoding policies often carries the promise of interpretability, systematic evaluations were never performed to assess the interpretability of these policies, likely because of the complexity of such an evaluation. In this paper, we introduce a novel metric that uses large-language models (LLM) to assess the interpretability of programmatic policies. For our metric, an LLM is given both a program and a description of its associated programming language. The LLM then formulates a natural language explanation of the program. This explanation is subsequently fed into a second LLM, which tries to reconstruct the program from the natural-language explanation. Our metric then measures the behavioral similarity between the reconstructed program and the original. We validate our approach with synthesized and human-crafted programmatic policies for playing a real-time strategy game, comparing the interpretability scores of these programmatic policies to obfusc",
    "link": "http://arxiv.org/abs/2311.06979",
    "context": "Title: Assessing the Interpretability of Programmatic Policies with Large Language Models. (arXiv:2311.06979v2 [cs.AI] UPDATED)\nAbstract: Although the synthesis of programs encoding policies often carries the promise of interpretability, systematic evaluations were never performed to assess the interpretability of these policies, likely because of the complexity of such an evaluation. In this paper, we introduce a novel metric that uses large-language models (LLM) to assess the interpretability of programmatic policies. For our metric, an LLM is given both a program and a description of its associated programming language. The LLM then formulates a natural language explanation of the program. This explanation is subsequently fed into a second LLM, which tries to reconstruct the program from the natural-language explanation. Our metric then measures the behavioral similarity between the reconstructed program and the original. We validate our approach with synthesized and human-crafted programmatic policies for playing a real-time strategy game, comparing the interpretability scores of these programmatic policies to obfusc",
    "path": "papers/23/11/2311.06979.json",
    "total_tokens": 876,
    "translated_title": "用大型语言模型评估编程策略的可解释性",
    "translated_abstract": "尽管合成编码策略的程序通常带有可解释性的承诺，但从未进行过系统评估以评估这些策略的可解释性，很可能是因为这样的评估的复杂性。在本文中，我们引入了一种使用大型语言模型（LLM）评估编程策略可解释性的新方法。对于我们的度量，LLM同时提供了一个程序和其相关编程语言的描述。然后，LLM利用自然语言解释形成一个程序的解释。然后，将该解释输入到第二个LLM中，该LLM试图从自然语言解释中重建程序。然后，我们的度量衡量了重建程序与原始程序之间的行为相似性。我们将我们的方法通过合成和人工制作的用于玩实时策略游戏的编程策略进行验证，将这些编程策略的可解释性得分与混淆的策略进行比较。",
    "tldr": "本文介绍了一种使用大型语言模型评估编程策略可解释性的新方法，通过度量重建程序与原始程序的行为相似性来评估。这一方法在合成和人工制作的编程策略中得到了验证。",
    "en_tdlr": "This paper introduces a novel approach using large language models to assess the interpretability of programmatic policies. The approach involves measuring the behavioral similarity between reconstructed programs and original programs to evaluate interpretability. The approach is validated with synthesized and human-crafted programmatic policies."
}