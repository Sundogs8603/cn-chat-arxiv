{
    "title": "Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose Estimation",
    "abstract": "arXiv:2311.12028v2 Announce Type: replace-cross  Abstract: Transformers have been successfully applied in the field of video-based 3D human pose estimation. However, the high computational costs of these video pose transformers (VPTs) make them impractical on resource-constrained devices. In this paper, we present a plug-and-play pruning-and-recovering framework, called Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose estimation from videos. Our HoT begins with pruning pose tokens of redundant frames and ends with recovering full-length tokens, resulting in a few pose tokens in the intermediate transformer blocks and thus improving the model efficiency. To effectively achieve this, we propose a token pruning cluster (TPC) that dynamically selects a few representative tokens with high semantic diversity while eliminating the redundancy of video frames. In addition, we develop a token recovering attention (TRA) to restore the detailed spatio-temporal information b",
    "link": "https://arxiv.org/abs/2311.12028",
    "context": "Title: Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose Estimation\nAbstract: arXiv:2311.12028v2 Announce Type: replace-cross  Abstract: Transformers have been successfully applied in the field of video-based 3D human pose estimation. However, the high computational costs of these video pose transformers (VPTs) make them impractical on resource-constrained devices. In this paper, we present a plug-and-play pruning-and-recovering framework, called Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose estimation from videos. Our HoT begins with pruning pose tokens of redundant frames and ends with recovering full-length tokens, resulting in a few pose tokens in the intermediate transformer blocks and thus improving the model efficiency. To effectively achieve this, we propose a token pruning cluster (TPC) that dynamically selects a few representative tokens with high semantic diversity while eliminating the redundancy of video frames. In addition, we develop a token recovering attention (TRA) to restore the detailed spatio-temporal information b",
    "path": "papers/23/11/2311.12028.json",
    "total_tokens": 897,
    "translated_title": "高效基于Transformer的3D人体姿势估计的Hourglass Tokenizer",
    "translated_abstract": "Transformers已经成功应用于基于视频的3D人体姿势估计领域。然而，这些视频姿势Transformer（VPTs）的高计算成本使得它们在资源受限设备上不切实际。在本文中，我们提出了一种名为Hourglass Tokenizer（HoT）的即插即用的修剪和恢复框架，用于从视频中高效地基于Transformer进行3D人体姿势估计。我们的HoT首先通过修剪冗余帧的姿势标记开始，然后以恢复全长度标记结束，从而在中间的Transformer块中产生少量姿势标记，从而提高模型的效率。为了有效实现这一目标，我们提出了一个标记修剪集群（TPC），动态选择一些具有高语义多样性的代表性标记，同时消除视频帧的冗余。此外，我们开发了一个标记恢复注意力（TRA）来恢复详细的时空信息。",
    "tldr": "提出了一种名为Hourglass Tokenizer（HoT）的修剪和恢复框架，用于从视频中高效地基于Transformer进行3D人体姿势估计，通过动态选择具有高语义多样性的代表性标记并消除视频帧的冗余，最终提高了模型的效率。",
    "en_tdlr": "Introduced a pruning-and-recovering framework called Hourglass Tokenizer (HoT) for efficient transformer-based 3D human pose estimation from videos, which dynamically selects representative tokens with high semantic diversity, eliminates redundancy in video frames, and improves model efficiency."
}