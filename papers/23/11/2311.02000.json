{
    "title": "High Probability Convergence of Adam Under Unbounded Gradients and Affine Variance Noise. (arXiv:2311.02000v1 [math.OC])",
    "abstract": "In this paper, we study the convergence of the Adaptive Moment Estimation (Adam) algorithm under unconstrained non-convex smooth stochastic optimizations. Despite the widespread usage in machine learning areas, its theoretical properties remain limited. Prior researches primarily investigated Adam's convergence from an expectation view, often necessitating strong assumptions like uniformly stochastic bounded gradients or problem-dependent knowledge in prior. As a result, the applicability of these findings in practical real-world scenarios has been constrained. To overcome these limitations, we provide a deep analysis and show that Adam could converge to the stationary point in high probability with a rate of $\\mathcal{O}\\left({\\rm poly}(\\log T)/\\sqrt{T}\\right)$ under coordinate-wise \"affine\" variance noise, not requiring any bounded gradient assumption and any problem-dependent knowledge in prior to tune hyper-parameters. Additionally, it is revealed that Adam confines its gradients' ",
    "link": "http://arxiv.org/abs/2311.02000",
    "context": "Title: High Probability Convergence of Adam Under Unbounded Gradients and Affine Variance Noise. (arXiv:2311.02000v1 [math.OC])\nAbstract: In this paper, we study the convergence of the Adaptive Moment Estimation (Adam) algorithm under unconstrained non-convex smooth stochastic optimizations. Despite the widespread usage in machine learning areas, its theoretical properties remain limited. Prior researches primarily investigated Adam's convergence from an expectation view, often necessitating strong assumptions like uniformly stochastic bounded gradients or problem-dependent knowledge in prior. As a result, the applicability of these findings in practical real-world scenarios has been constrained. To overcome these limitations, we provide a deep analysis and show that Adam could converge to the stationary point in high probability with a rate of $\\mathcal{O}\\left({\\rm poly}(\\log T)/\\sqrt{T}\\right)$ under coordinate-wise \"affine\" variance noise, not requiring any bounded gradient assumption and any problem-dependent knowledge in prior to tune hyper-parameters. Additionally, it is revealed that Adam confines its gradients' ",
    "path": "papers/23/11/2311.02000.json",
    "total_tokens": 919,
    "translated_title": "Adam算法在无界梯度和仿射方差噪声下的高概率收敛性研究",
    "translated_abstract": "本文研究了在非凸平滑随机优化中，自适应矩法（Adam）算法的收敛性。尽管在机器学习领域被广泛使用，但其理论性质仍然有限。之前的研究主要从期望角度考虑了Adam的收敛性，常常需要强假设，比如均匀随机有界梯度或者先验的问题相关知识。因此，这些结果在实际的现实场景中的适用性受到了限制。为了克服这些局限，我们进行了深入分析，并证明了在坐标-wise“仿射”方差噪声下，Adam可以以高概率收敛到稳定点，其收敛速率为$\\mathcal{O}\\left({\\rm poly}(\\log T)/\\sqrt{T}\\right)$，不需要任何有界梯度假设和任何问题相关的知识来调整超参数。此外，我们还发现Adam限制了其梯度的...",
    "tldr": "Adam算法在非凸平滑随机优化中，经过深入分析，证明了在坐标-wise“仿射”方差噪声下，Adam可以以高概率收敛到稳定点，无需任何有界梯度假设和问题相关的知识。"
}