{
    "title": "LRM: Large Reconstruction Model for Single Image to 3D",
    "abstract": "arXiv:2311.04400v2 Announce Type: replace-cross  Abstract: We propose the first Large Reconstruction Model (LRM) that predicts the 3D model of an object from a single input image within just 5 seconds. In contrast to many previous methods that are trained on small-scale datasets such as ShapeNet in a category-specific fashion, LRM adopts a highly scalable transformer-based architecture with 500 million learnable parameters to directly predict a neural radiance field (NeRF) from the input image. We train our model in an end-to-end manner on massive multi-view data containing around 1 million objects, including both synthetic renderings from Objaverse and real captures from MVImgNet. This combination of a high-capacity model and large-scale training data empowers our model to be highly generalizable and produce high-quality 3D reconstructions from various testing inputs, including real-world in-the-wild captures and images created by generative models. Video demos and interactable 3D mes",
    "link": "https://arxiv.org/abs/2311.04400",
    "context": "Title: LRM: Large Reconstruction Model for Single Image to 3D\nAbstract: arXiv:2311.04400v2 Announce Type: replace-cross  Abstract: We propose the first Large Reconstruction Model (LRM) that predicts the 3D model of an object from a single input image within just 5 seconds. In contrast to many previous methods that are trained on small-scale datasets such as ShapeNet in a category-specific fashion, LRM adopts a highly scalable transformer-based architecture with 500 million learnable parameters to directly predict a neural radiance field (NeRF) from the input image. We train our model in an end-to-end manner on massive multi-view data containing around 1 million objects, including both synthetic renderings from Objaverse and real captures from MVImgNet. This combination of a high-capacity model and large-scale training data empowers our model to be highly generalizable and produce high-quality 3D reconstructions from various testing inputs, including real-world in-the-wild captures and images created by generative models. Video demos and interactable 3D mes",
    "path": "papers/23/11/2311.04400.json",
    "total_tokens": 845,
    "translated_title": "LRM：单图像到3D的大型重建模型",
    "translated_abstract": "我们提出了第一个大型重建模型（LRM），可以在短短5秒内从单个输入图像中预测对象的3D模型。与许多先前训练在小规模数据集（如ShapeNet）上的方法相比，LRM采用了一个高度可扩展的基于transformer的架构，具有5亿的可学习参数，可以直接从输入图像中预测神经辐射场（NeRF）。我们以端到端的方式在包含约100万个对象的大规模多视角数据上训练我们的模型，包括来自Objaverse的合成渲染和来自MVImgNet的真实捕获。这种高容量模型和大规模训练数据的结合使得我们的模型具有很强的泛化能力，可以从各种测试输入中产生高质量的3D重建结果，包括真实场景捕获和生成模型创建的图像。视频演示和可交互的3D模型",
    "tldr": "首次提出了LRM，采用大规模训练数据和高容量模型，可在短时间内从单个图像中预测高质量3D重建结果",
    "en_tdlr": "Introducing LRM for the first time, utilizing large-scale training data and high-capacity model to predict high-quality 3D reconstructions from a single image in a short time."
}