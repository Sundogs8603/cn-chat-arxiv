{
    "title": "Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as an Alternative to Attention Layers in Transformers",
    "abstract": "This work presents an analysis of the effectiveness of using standard shallow feed-forward networks to mimic the behavior of the attention mechanism in the original Transformer model, a state-of-the-art architecture for sequence-to-sequence tasks. We substitute key elements of the attention mechanism in the Transformer with simple feed-forward networks, trained using the original components via knowledge distillation. Our experiments, conducted on the IWSLT2017 dataset, reveal the capacity of these \"attentionless Transformers\" to rival the performance of the original architecture. Through rigorous ablation studies, and experimenting with various replacement network types and sizes, we offer insights that support the viability of our approach. This not only sheds light on the adaptability of shallow feed-forward networks in emulating attention mechanisms but also underscores their potential to streamline complex architectures for sequence-to-sequence tasks.",
    "link": "https://arxiv.org/abs/2311.10642",
    "context": "Title: Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as an Alternative to Attention Layers in Transformers\nAbstract: This work presents an analysis of the effectiveness of using standard shallow feed-forward networks to mimic the behavior of the attention mechanism in the original Transformer model, a state-of-the-art architecture for sequence-to-sequence tasks. We substitute key elements of the attention mechanism in the Transformer with simple feed-forward networks, trained using the original components via knowledge distillation. Our experiments, conducted on the IWSLT2017 dataset, reveal the capacity of these \"attentionless Transformers\" to rival the performance of the original architecture. Through rigorous ablation studies, and experimenting with various replacement network types and sizes, we offer insights that support the viability of our approach. This not only sheds light on the adaptability of shallow feed-forward networks in emulating attention mechanisms but also underscores their potential to streamline complex architectures for sequence-to-sequence tasks.",
    "path": "papers/23/11/2311.10642.json",
    "total_tokens": 821,
    "translated_title": "重新思考注意力：探索将浅层前馈神经网络作为Transformers中注意力层的替代方法",
    "translated_abstract": "本研究分析了使用标准的浅层前馈网络来模仿Transformer模型中注意力机制的有效性。我们使用知识蒸馏的方法，将Transformer中的关键元素替换为简单的前馈网络，并使用原始组件进行训练。我们在IWSLT2017数据集上进行实验证明了这种“无注意力的Transformers”可以与原始架构的性能媲美。通过严谨的实验和不同替代网络类型和大小的尝试，我们提供了支持我们方法可行性的见解。这不仅揭示了浅层前馈网络在模仿注意力机制方面的适应性，而且强调了它们在简化序列任务的复杂架构方面的潜力。",
    "tldr": "本研究探索使用浅层前馈神经网络替代注意力机制，通过知识蒸馏方法训练，实验证明了这种\"无注意力的Transformers\"可以与原始架构的性能媲美，并揭示了其简化复杂架构的潜力。",
    "en_tdlr": "This study explores using shallow feed-forward neural networks as an alternative to attention mechanisms in Transformers. Through knowledge distillation training, the \"attentionless Transformers\" are found to be comparable in performance to the original architecture, highlighting their potential to simplify complex structures."
}