{
    "title": "Compositional Capabilities of Autoregressive Transformers: A Study on Synthetic, Interpretable Tasks",
    "abstract": "Transformers trained on huge text corpora exhibit a remarkable set of capabilities, e.g., performing basic arithmetic. Given the inherent compositional nature of language, one can expect the model to learn to compose these capabilities, potentially yielding a combinatorial explosion of what operations it can perform on an input. Motivated by the above, we train autoregressive Transformer models on a synthetic data-generating process that involves compositions of a set of well-defined monolithic capabilities. Through a series of extensive and systematic experiments on this data-generating process, we show that: (1) autoregressive Transformers can learn compositional structures from small amounts of training data and generalize to exponentially or even combinatorially many functions; (2) generating intermediate outputs when composing functions is more effective for generalizing to new, unseen compositions than not generating any intermediate outputs (3) biases in the order of the composi",
    "link": "https://arxiv.org/abs/2311.12997",
    "context": "Title: Compositional Capabilities of Autoregressive Transformers: A Study on Synthetic, Interpretable Tasks\nAbstract: Transformers trained on huge text corpora exhibit a remarkable set of capabilities, e.g., performing basic arithmetic. Given the inherent compositional nature of language, one can expect the model to learn to compose these capabilities, potentially yielding a combinatorial explosion of what operations it can perform on an input. Motivated by the above, we train autoregressive Transformer models on a synthetic data-generating process that involves compositions of a set of well-defined monolithic capabilities. Through a series of extensive and systematic experiments on this data-generating process, we show that: (1) autoregressive Transformers can learn compositional structures from small amounts of training data and generalize to exponentially or even combinatorially many functions; (2) generating intermediate outputs when composing functions is more effective for generalizing to new, unseen compositions than not generating any intermediate outputs (3) biases in the order of the composi",
    "path": "papers/23/11/2311.12997.json",
    "total_tokens": 875,
    "translated_title": "自回归Transformer的组合能力：对合成的可解释任务的研究",
    "translated_abstract": "在大量文本语料库上训练的Transformer展示了一系列出色的能力，例如执行基本算术运算。考虑到语言的内在组合性质，我们可以期望模型学习将这些能力组合起来，从而潜在地产生对输入进行操作的组合爆炸。基于以上动机，我们在一个涉及一组明确定义的整体能力组合的合成数据生成过程上训练自回归Transformer模型。通过一系列广泛而系统的实验，我们发现：(1) 自回归Transformer可以从少量训练数据中学习组合结构，并且泛化到指数甚至组合爆炸数量的函数；(2) 在组合函数时生成中间输出比不生成任何中间输出更有效地泛化到新的、未见过的组合；(3) 在组合顺序中存在偏差会影响模型的性能。",
    "tldr": "本研究通过在合成数据生成过程上训练自回归Transformer模型，展示了其学习组合结构和泛化能力。通过生成中间输出来组合函数在泛化到新的未见组合时比不生成中间输出更有效。同时，组合顺序的偏差会对模型的性能产生影响。",
    "en_tdlr": "This study demonstrates the compositional and generalization capabilities of autoregressive Transformers by training them on a synthetic data-generating process. Generating intermediate outputs when composing functions improves the generalization to new compositions, and biases in the order of composition affect the model's performance."
}