{
    "title": "Imitation Bootstrapped Reinforcement Learning",
    "abstract": "arXiv:2311.02198v4 Announce Type: replace-cross  Abstract: Despite the considerable potential of reinforcement learning (RL), robotic control tasks predominantly rely on imitation learning (IL) due to its better sample efficiency. However, it is costly to collect comprehensive expert demonstrations that enable IL to generalize to all possible scenarios, and any distribution shift would require recollecting data for finetuning. Therefore, RL is appealing if it can build upon IL as an efficient autonomous self-improvement procedure. We propose imitation bootstrapped reinforcement learning (IBRL), a novel framework for sample-efficient RL with demonstrations that first trains an IL policy on the provided demonstrations and then uses it to propose alternative actions for both online exploration and bootstrapping target values. Compared to prior works that oversample the demonstrations or regularize RL with an additional imitation loss, IBRL is able to utilize high quality actions from IL p",
    "link": "https://arxiv.org/abs/2311.02198",
    "context": "Title: Imitation Bootstrapped Reinforcement Learning\nAbstract: arXiv:2311.02198v4 Announce Type: replace-cross  Abstract: Despite the considerable potential of reinforcement learning (RL), robotic control tasks predominantly rely on imitation learning (IL) due to its better sample efficiency. However, it is costly to collect comprehensive expert demonstrations that enable IL to generalize to all possible scenarios, and any distribution shift would require recollecting data for finetuning. Therefore, RL is appealing if it can build upon IL as an efficient autonomous self-improvement procedure. We propose imitation bootstrapped reinforcement learning (IBRL), a novel framework for sample-efficient RL with demonstrations that first trains an IL policy on the provided demonstrations and then uses it to propose alternative actions for both online exploration and bootstrapping target values. Compared to prior works that oversample the demonstrations or regularize RL with an additional imitation loss, IBRL is able to utilize high quality actions from IL p",
    "path": "papers/23/11/2311.02198.json",
    "total_tokens": 844,
    "translated_title": "模仿引导式强化学习",
    "translated_abstract": "尽管强化学习（RL）具有相当大的潜力，但机器人控制任务主要依赖模仿学习（IL）是因为其更好的样本效率。然而，收集能使IL推广到所有可能场景的全面专家演示是昂贵的，任何分布的转变都需要重新收集数据进行微调。因此，如果RL可以建立在IL的基础上作为一种高效的自我改进程序，那么它将具有吸引力。我们提出了一种模仿引导式强化学习（IBRL）的新框架，用于具有示范的高效抽样RL，首先在提供的示范上训练IL策略，然后使用它提出替代动作进行在线探索和引导目标值。与先前过度采样示范或用额外的模仿损失对RL进行正则化的工作相比，IBRL能够利用来自IL的高质量动作。",
    "tldr": "提出了一种模仿引导式强化学习（IBRL）的框架，用于高效的样本-efficient RL，通过先在提供的示范上训练IL策略，然后使用它提出替代动作进行在线探索和引导目标值。",
    "en_tdlr": "Introduced IBRL, a framework for sample-efficient RL that first trains an IL policy on provided demonstrations and then uses it to propose alternative actions for online exploration and bootstrapping target values."
}