{
    "title": "LLMRefine: Pinpointing and Refining Large Language Models via Fine-Grained Actionable Feedback",
    "abstract": "arXiv:2311.09336v2 Announce Type: replace  Abstract: Recent large language models (LLM) are leveraging human feedback to improve their generation quality. However, human feedback is costly to obtain, especially during inference. In this work, we propose LLMRefine, an inference time optimization method to refine LLM's output. The core idea is to use a learned fine-grained feedback model to pinpoint defects and guide LLM to refine them iteratively. Using original LLM as a proposal of edits, LLMRefine searches for defect-less text via simulated annealing, trading off the exploration and exploitation. We conduct experiments on three text generation tasks, including machine translation, long-form question answering (QA), and topical summarization. LLMRefine consistently outperforms all baseline approaches, achieving improvements up to 1.7 MetricX points on translation tasks, 8.1 ROUGE-L on ASQA, 2.2 ROUGE-L on topical summarization.",
    "link": "https://arxiv.org/abs/2311.09336",
    "context": "Title: LLMRefine: Pinpointing and Refining Large Language Models via Fine-Grained Actionable Feedback\nAbstract: arXiv:2311.09336v2 Announce Type: replace  Abstract: Recent large language models (LLM) are leveraging human feedback to improve their generation quality. However, human feedback is costly to obtain, especially during inference. In this work, we propose LLMRefine, an inference time optimization method to refine LLM's output. The core idea is to use a learned fine-grained feedback model to pinpoint defects and guide LLM to refine them iteratively. Using original LLM as a proposal of edits, LLMRefine searches for defect-less text via simulated annealing, trading off the exploration and exploitation. We conduct experiments on three text generation tasks, including machine translation, long-form question answering (QA), and topical summarization. LLMRefine consistently outperforms all baseline approaches, achieving improvements up to 1.7 MetricX points on translation tasks, 8.1 ROUGE-L on ASQA, 2.2 ROUGE-L on topical summarization.",
    "path": "papers/23/11/2311.09336.json",
    "total_tokens": 878,
    "translated_title": "LLMRefine：通过细粒度可操作反馈精确定位和优化大型语言模型",
    "translated_abstract": "最近，大型语言模型（LLM）正在利用人类反馈来提高生成质量。然而，在推断过程中获取人类反馈成本高昂。在这项工作中，我们提出了LLMRefine，一种用于优化推理时间的方法，以改进LLM的输出。其核心思想是利用学习的细粒度反馈模型来准确定位缺陷，并引导LLM进行迭代优化。通过将原始LLM作为编辑建议，LLMRefine通过模拟退火搜索无缺陷文本，权衡探索和开发。我们在三个文本生成任务上进行实验，包括机器翻译，长篇问答（QA）和主题总结。LLMRefine在所有基线方法上一贯表现优异，在翻译任务上取得了高达1.7 MetricX点的改进，在ASQA上为8.1 ROUGE-L，在主题总结上为2.2 ROUGE-L。",
    "tldr": "LLMRefine提出了一种细粒度反馈模型来指导大型语言模型定位缺陷并进行优化，在机器翻译、长篇问答和主题总结等任务中取得显著的改进。",
    "en_tdlr": "LLMRefine proposes a fine-grained feedback model to guide large language models in pinpointing defects and refining them, achieving significant improvements in tasks such as machine translation, long-form question answering, and topical summarization."
}