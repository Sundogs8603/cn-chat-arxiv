{
    "title": "Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation",
    "abstract": "arXiv:2311.04254v3 Announce Type: replace  Abstract: Recent advancements in Large Language Models (LLMs) have revolutionized decision-making by breaking down complex problems into more manageable language sequences referred to as \"thoughts\". An effective thought design should consider three key perspectives: performance, efficiency, and flexibility. However, existing thought can at most exhibit two of these attributes. To address these limitations, we introduce a novel thought prompting approach called \"Everything of Thoughts\" (XoT) to defy the law of \"Penrose triangle of existing thought paradigms. XoT leverages pretrained reinforcement learning and Monte Carlo Tree Search (MCTS) to incorporate external domain knowledge into thoughts, thereby enhancing LLMs' capabilities and enabling them to generalize to unseen problems efficiently. Through the utilization of the MCTS-LLM collaborative thought revision framework, this approach autonomously produces high-quality comprehensive cognitiv",
    "link": "https://arxiv.org/abs/2311.04254",
    "context": "Title: Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation\nAbstract: arXiv:2311.04254v3 Announce Type: replace  Abstract: Recent advancements in Large Language Models (LLMs) have revolutionized decision-making by breaking down complex problems into more manageable language sequences referred to as \"thoughts\". An effective thought design should consider three key perspectives: performance, efficiency, and flexibility. However, existing thought can at most exhibit two of these attributes. To address these limitations, we introduce a novel thought prompting approach called \"Everything of Thoughts\" (XoT) to defy the law of \"Penrose triangle of existing thought paradigms. XoT leverages pretrained reinforcement learning and Monte Carlo Tree Search (MCTS) to incorporate external domain knowledge into thoughts, thereby enhancing LLMs' capabilities and enabling them to generalize to unseen problems efficiently. Through the utilization of the MCTS-LLM collaborative thought revision framework, this approach autonomously produces high-quality comprehensive cognitiv",
    "path": "papers/23/11/2311.04254.json",
    "total_tokens": 923,
    "translated_title": "一切的思考：打破彭罗斯三角定律以生成思想",
    "translated_abstract": "最近大型语言模型（LLMs）的进展通过将复杂问题分解为更易处理的语言序列（即“思想”）彻底改变了决策。一个有效的思想设计应该考虑三个关键视角：性能、效率和灵活性。然而，现有的思想最多只能体现这些属性中的两个。为了解决这些限制，我们引入了一种名为“一切的思考”（XoT）的新型思考促进方法，以打破现有思考范式的“彭罗斯三角定律”。XoT利用预训练的强化学习和蒙特卡洛树搜索（MCTS）将外部领域知识融入思想中，从而增强LLMs的能力，并使其能够高效地推广到未见问题。通过利用MCTS-LLM协作思考修订框架，这种方法自主生产高质量的综合认知。",
    "tldr": "引入一种名为“一切的思考”（XoT）的新型思考促进方法，借助预训练的强化学习和蒙特卡洛树搜索（MCTS）将外部领域知识融入思想，从而提高大型语言模型（LLMs）的能力，使其可以高效地推广到未知问题。",
    "en_tdlr": "Introducing a novel thought prompting approach called \"Everything of Thoughts\" (XoT) that leverages pretrained reinforcement learning and Monte Carlo Tree Search (MCTS) to incorporate external domain knowledge into thoughts, enhancing Large Language Models (LLMs) capabilities and enabling efficient generalization to unseen problems."
}