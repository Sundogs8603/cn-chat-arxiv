{
    "title": "Signal Processing Meets SGD: From Momentum to Filter",
    "abstract": "In deep learning, stochastic gradient descent (SGD) and its momentum-based variants are widely used in optimization algorithms, they usually face the problem of slow convergence. Meanwhile, existing adaptive learning rate optimizers accelerate convergence but often at the expense of generalization ability. We demonstrate that the adaptive learning rate property impairs generalization. To address this contradiction, we propose a novel optimization method that aims to accelerate the convergence rate of SGD without loss of generalization. This approach is based on the idea of reducing the variance of the historical gradient, enhancing the first-order moment estimation of the SGD by applying Wiener filtering theory, and introducing a time-varying adaptive weight. Experimental results show that SGDF achieves a trade-off between convergence and generalization compared to state-of-the-art optimizers.",
    "link": "https://rss.arxiv.org/abs/2311.02818",
    "context": "Title: Signal Processing Meets SGD: From Momentum to Filter\nAbstract: In deep learning, stochastic gradient descent (SGD) and its momentum-based variants are widely used in optimization algorithms, they usually face the problem of slow convergence. Meanwhile, existing adaptive learning rate optimizers accelerate convergence but often at the expense of generalization ability. We demonstrate that the adaptive learning rate property impairs generalization. To address this contradiction, we propose a novel optimization method that aims to accelerate the convergence rate of SGD without loss of generalization. This approach is based on the idea of reducing the variance of the historical gradient, enhancing the first-order moment estimation of the SGD by applying Wiener filtering theory, and introducing a time-varying adaptive weight. Experimental results show that SGDF achieves a trade-off between convergence and generalization compared to state-of-the-art optimizers.",
    "path": "papers/23/11/2311.02818.json",
    "total_tokens": 846,
    "translated_title": "信号处理与SGD相遇：从动量到滤波",
    "translated_abstract": "在深度学习中，随机梯度下降（SGD）及其基于动量的变种广泛应用于优化算法，它们通常面临收敛速度慢的问题。同时，现有的自适应学习率优化器加速收敛，但常常以泛化能力为代价。我们证明了自适应学习率属性会损害泛化能力。为了解决这一矛盾，我们提出了一种新的优化方法，旨在加速SGD的收敛速度，保持泛化能力不变。该方法基于减小历史梯度的方差的思想，通过应用维纳滤波理论增强SGD的一阶矩估计，并引入一个时变自适应权重。实验结果表明，与最先进的优化器相比，SGDF在收敛和泛化之间找到了一个平衡。",
    "tldr": "该论文提出了一种名为SGDF的优化方法，通过应用维纳滤波理论和引入时变自适应权重，加速了SGD的收敛速度，同时保持了泛化能力。实验证明，与其他优化器相比，SGDF在收敛和泛化之间取得了平衡。",
    "en_tdlr": "This paper proposes a novel optimization method called SGDF, which accelerates the convergence rate of SGD while preserving generalization ability by applying Wiener filtering theory and introducing time-varying adaptive weight. Experimental results demonstrate that SGDF achieves a trade-off between convergence and generalization compared to other optimizers."
}