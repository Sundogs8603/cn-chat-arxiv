{
    "title": "Active Neural Topological Mapping for Multi-Agent Exploration. (arXiv:2311.00252v1 [cs.RO])",
    "abstract": "This paper investigates the multi-agent cooperative exploration problem, which requires multiple agents to explore an unseen environment via sensory signals in a limited time. A popular approach to exploration tasks is to combine active mapping with planning. Metric maps capture the details of the spatial representation, but are with high communication traffic and may vary significantly between scenarios, resulting in inferior generalization. Topological maps are a promising alternative as they consist only of nodes and edges with abstract but essential information and are less influenced by the scene structures. However, most existing topology-based exploration tasks utilize classical methods for planning, which are time-consuming and sub-optimal due to their handcrafted design. Deep reinforcement learning (DRL) has shown great potential for learning (near) optimal policies through fast end-to-end inference. In this paper, we propose Multi-Agent Neural Topological Mapping (MANTM) to i",
    "link": "http://arxiv.org/abs/2311.00252",
    "context": "Title: Active Neural Topological Mapping for Multi-Agent Exploration. (arXiv:2311.00252v1 [cs.RO])\nAbstract: This paper investigates the multi-agent cooperative exploration problem, which requires multiple agents to explore an unseen environment via sensory signals in a limited time. A popular approach to exploration tasks is to combine active mapping with planning. Metric maps capture the details of the spatial representation, but are with high communication traffic and may vary significantly between scenarios, resulting in inferior generalization. Topological maps are a promising alternative as they consist only of nodes and edges with abstract but essential information and are less influenced by the scene structures. However, most existing topology-based exploration tasks utilize classical methods for planning, which are time-consuming and sub-optimal due to their handcrafted design. Deep reinforcement learning (DRL) has shown great potential for learning (near) optimal policies through fast end-to-end inference. In this paper, we propose Multi-Agent Neural Topological Mapping (MANTM) to i",
    "path": "papers/23/11/2311.00252.json",
    "total_tokens": 931,
    "translated_title": "多智能体探索的主动神经拓扑映射",
    "translated_abstract": "本文研究了多智能体合作探索问题，即要求多个智能体在有限的时间内通过感知信号来探索未知环境。探索任务的一种常见方法是将主动映射与规划结合起来。度量地图捕捉了空间表示的细节，但通信量大且在不同场景之间可能有很大的变化，导致泛化能力较差。拓扑地图是一个有希望的选择，因为它们只包含节点和边，具有抽象但关键的信息，并且受到场景结构的影响较小。然而，大多数现有的基于拓扑的探索任务使用的是传统的规划方法，这些方法耗时且由于其手工设计而次优。深度强化学习（DRL）已展现出通过快速端到端推导学习（近）最优策略的巨大潜力。在本文中，我们提出了多智能体神经拓扑映射（MANTM）来解决这个问题。",
    "tldr": "本文提出了一种名为多智能体神经拓扑映射（MANTM）的方法，用于解决多智能体探索问题。该方法通过使用拓扑地图作为环境表示并结合深度强化学习，能够在有限时间内快速学习（近）最优策略。"
}