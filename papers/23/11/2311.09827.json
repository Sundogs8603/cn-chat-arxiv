{
    "title": "Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking",
    "abstract": "arXiv:2311.09827v2 Announce Type: replace  Abstract: While large language models (LLMs) have demonstrated increasing power, they have also given rise to a wide range of harmful behaviors. As representatives, jailbreak attacks can provoke harmful or unethical responses from LLMs, even after safety alignment. In this paper, we investigate a novel category of jailbreak attacks specifically designed to target the cognitive structure and processes of LLMs. Specifically, we analyze the safety vulnerability of LLMs in the face of (1) multilingual cognitive overload, (2) veiled expression, and (3) effect-to-cause reasoning. Different from previous jailbreak attacks, our proposed cognitive overload is a black-box attack with no need for knowledge of model architecture or access to model weights. Experiments conducted on AdvBench and MasterKey reveal that various LLMs, including both popular open-source model Llama 2 and the proprietary model ChatGPT, can be compromised through cognitive overloa",
    "link": "https://arxiv.org/abs/2311.09827",
    "context": "Title: Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking\nAbstract: arXiv:2311.09827v2 Announce Type: replace  Abstract: While large language models (LLMs) have demonstrated increasing power, they have also given rise to a wide range of harmful behaviors. As representatives, jailbreak attacks can provoke harmful or unethical responses from LLMs, even after safety alignment. In this paper, we investigate a novel category of jailbreak attacks specifically designed to target the cognitive structure and processes of LLMs. Specifically, we analyze the safety vulnerability of LLMs in the face of (1) multilingual cognitive overload, (2) veiled expression, and (3) effect-to-cause reasoning. Different from previous jailbreak attacks, our proposed cognitive overload is a black-box attack with no need for knowledge of model architecture or access to model weights. Experiments conducted on AdvBench and MasterKey reveal that various LLMs, including both popular open-source model Llama 2 and the proprietary model ChatGPT, can be compromised through cognitive overloa",
    "path": "papers/23/11/2311.09827.json",
    "total_tokens": 908,
    "translated_title": "认知负荷: 通过超载逻辑思维越狱大型语言模型",
    "translated_abstract": "虽然大型语言模型（LLMs）展示了越来越强大的能力，但也引发了各种有害行为。作为代表，越狱攻击可能引发LLMs产生有害或不道德的响应，即使经过了安全对齐。本文研究了一类新颖的越狱攻击，专门针对LLMs的认知结构和过程进行设计。具体来说，我们分析了LLMs在面对（1）多语言认知负荷，（2）隐晦表达和（3）效果推导推理时的安全性脆弱性。与先前的越狱攻击不同，我们提出的认知负载是一种无需了解模型架构或访问模型权重的黑盒攻击。在AdvBench和MasterKey上进行的实验表明，包括流行的开源模型Llama 2和专有模型ChatGPT在内的各种LLMs可以通过认知过载受到影响。",
    "tldr": "本文研究了一种新颖的越狱攻击，针对大型语言模型的认知结构和过程进行设计，通过认知过载攻击，即使在安全对齐之后，也可以激发LLMs产生有害或不道德的响应。",
    "en_tdlr": "This paper investigates a new category of jailbreak attacks specifically designed to target the cognitive structure and processes of large language models, which can provoke harmful or unethical responses from LLMs even after safety alignment."
}