{
    "title": "Effective Large Language Model Adaptation for Improved Grounding and Citation Generation",
    "abstract": "arXiv:2311.09533v2 Announce Type: replace  Abstract: Large language models (LLMs) have achieved remarkable advancements in natural language understanding and generation. However, one major issue towards their widespread deployment in the real world is that they can generate \"hallucinated\" answers that are not factual. Towards this end, this paper focuses on improving LLMs by grounding their responses in retrieved passages and by providing citations. We propose a new framework, AGREE, Adaptation for GRounding EnhancEment, that improves the grounding from a holistic perspective. Our framework tunes LLMs to selfground the claims in their responses and provide accurate citations to retrieved documents. This tuning on top of the pre-trained LLMs requires well-grounded responses (with citations) for paired queries, for which we introduce a method that can automatically construct such data from unlabeled queries. The selfgrounding capability of tuned LLMs further grants them a test-time adapt",
    "link": "https://arxiv.org/abs/2311.09533",
    "context": "Title: Effective Large Language Model Adaptation for Improved Grounding and Citation Generation\nAbstract: arXiv:2311.09533v2 Announce Type: replace  Abstract: Large language models (LLMs) have achieved remarkable advancements in natural language understanding and generation. However, one major issue towards their widespread deployment in the real world is that they can generate \"hallucinated\" answers that are not factual. Towards this end, this paper focuses on improving LLMs by grounding their responses in retrieved passages and by providing citations. We propose a new framework, AGREE, Adaptation for GRounding EnhancEment, that improves the grounding from a holistic perspective. Our framework tunes LLMs to selfground the claims in their responses and provide accurate citations to retrieved documents. This tuning on top of the pre-trained LLMs requires well-grounded responses (with citations) for paired queries, for which we introduce a method that can automatically construct such data from unlabeled queries. The selfgrounding capability of tuned LLMs further grants them a test-time adapt",
    "path": "papers/23/11/2311.09533.json",
    "total_tokens": 865,
    "translated_title": "有效的大型语言模型适应以提升信息关联和引用生成",
    "translated_abstract": "大型语言模型（LLMs）在自然语言理解和生成方面取得了显著进展。然而，一大问题在于它们可能生成“臆想”的答案并非事实。为解决这一问题，本文着眼于通过将LLMs的响应联系到检索到的段落并提供引文来改进它们。我们提出了一个新的框架AGREE，即Adaptation for GRounding EnhancEment，从整体的角度提升了信息关联。我们的框架调整LLMs，使其自我联系其响应中的主张并为检索文档提供准确的引文。在预训练的LLMs基础上进行的这种调整需要对配对查询的响应进行很好的信息关联（带有引文），为此我们介绍了一种能够从未标记查询自动构造这些数据的方法。调整后的LLMs的自我关联能力进一步使其测试时间适应",
    "tldr": "本文提出了一个新的框架 AGREE，通过将大型语言模型的响应联系到检索到的段落并提供引文来提升信息关联，从而解决大型语言模型可能生成“臆想”答案的问题",
    "en_tdlr": "This paper introduces a new framework, AGREE, which improves the grounding of large language models by linking their responses to retrieved passages and providing citations, addressing the issue of potential \"hallucinated\" answers from large language models."
}