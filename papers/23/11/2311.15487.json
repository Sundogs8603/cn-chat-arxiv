{
    "title": "Global $\\mathcal{L}^2$ minimization at uniform exponential rate via geometrically adapted gradient descent in Deep Learning",
    "abstract": "arXiv:2311.15487v3 Announce Type: replace-cross  Abstract: We consider the gradient descent flow widely used for the minimization of the $\\mathcal{L}^2$ cost function in Deep Learning networks, and introduce two modified versions; one adapted for the overparametrized setting, and the other for the underparametrized setting. Both have a clear and natural invariant geometric meaning, taking into account the pullback vector bundle structure in the overparametrized, and the pushforward vector bundle structure in the underparametrized setting. In the overparametrized case, we prove that, provided that a rank condition holds, all orbits of the modified gradient descent drive the $\\mathcal{L}^2$ cost to its global minimum at a uniform exponential convergence rate; one thereby obtains an a priori stopping time for any prescribed proximity to the global minimum. We point out relations of the latter to sub-Riemannian geometry.",
    "link": "https://arxiv.org/abs/2311.15487",
    "context": "Title: Global $\\mathcal{L}^2$ minimization at uniform exponential rate via geometrically adapted gradient descent in Deep Learning\nAbstract: arXiv:2311.15487v3 Announce Type: replace-cross  Abstract: We consider the gradient descent flow widely used for the minimization of the $\\mathcal{L}^2$ cost function in Deep Learning networks, and introduce two modified versions; one adapted for the overparametrized setting, and the other for the underparametrized setting. Both have a clear and natural invariant geometric meaning, taking into account the pullback vector bundle structure in the overparametrized, and the pushforward vector bundle structure in the underparametrized setting. In the overparametrized case, we prove that, provided that a rank condition holds, all orbits of the modified gradient descent drive the $\\mathcal{L}^2$ cost to its global minimum at a uniform exponential convergence rate; one thereby obtains an a priori stopping time for any prescribed proximity to the global minimum. We point out relations of the latter to sub-Riemannian geometry.",
    "path": "papers/23/11/2311.15487.json",
    "total_tokens": 879,
    "translated_title": "深度学习中通过几何调整的梯度下降以均匀指数速率全局$\\mathcal{L}^2$最小化",
    "translated_abstract": "我们考虑在深度学习网络中广泛使用的用于最小化$\\mathcal{L}^2$代价函数的梯度下降流，并引入两个改进版本；一个适用于过参数化设置，另一个适用于欠参数化设置。这两个版本都具有明确自然的不变几何含义，考虑到在过参数化设置中的拉回向量丛结构和在欠参数化设置中的推前向量丛结构。在过参数化情况下，我们证明，只要满足秩条件，改进的梯度下降的所有轨道将以均匀指数收敛速率将$\\mathcal{L}^2$代价驱动到全局最小值；因此，对于任何预先指定的接近全局最小值的近似，我们可以得到先验停止时间。我们指出后者与次Riemann几何的关系。",
    "tldr": "通过几何调整的梯度下降，在深度学习中以均匀指数速率实现全局$\\mathcal{L}^2$最小化，这一方法在过参数化情况下具有明确自然的不变几何含义。",
    "en_tdlr": "Geometrically adapted gradient descent achieves global $\\mathcal{L}^2$ minimization at uniform exponential rate in Deep Learning, with clear and natural invariant geometric meaning in the overparametrized setting."
}