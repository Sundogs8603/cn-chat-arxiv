{
    "title": "Proto-lm: A Prototypical Network-Based Framework for Built-in Interpretability in Large Language Models. (arXiv:2311.01732v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) have significantly advanced the field of Natural Language Processing (NLP), but their lack of interpretability has been a major concern. Current methods for interpreting LLMs are post hoc, applied after inference time, and have limitations such as their focus on low-level features and lack of explainability at higher level text units. In this work, we introduce proto-lm, a prototypical network-based white-box framework that allows LLMs to learn immediately interpretable embeddings during the fine-tuning stage while maintaining competitive performance. Our method's applicability and interpretability are demonstrated through experiments on a wide range of NLP tasks, and our results indicate a new possibility of creating interpretable models without sacrificing performance. This novel approach to interpretability in LLMs can pave the way for more interpretable models without the need to sacrifice performance.",
    "link": "http://arxiv.org/abs/2311.01732",
    "context": "Title: Proto-lm: A Prototypical Network-Based Framework for Built-in Interpretability in Large Language Models. (arXiv:2311.01732v1 [cs.CL])\nAbstract: Large Language Models (LLMs) have significantly advanced the field of Natural Language Processing (NLP), but their lack of interpretability has been a major concern. Current methods for interpreting LLMs are post hoc, applied after inference time, and have limitations such as their focus on low-level features and lack of explainability at higher level text units. In this work, we introduce proto-lm, a prototypical network-based white-box framework that allows LLMs to learn immediately interpretable embeddings during the fine-tuning stage while maintaining competitive performance. Our method's applicability and interpretability are demonstrated through experiments on a wide range of NLP tasks, and our results indicate a new possibility of creating interpretable models without sacrificing performance. This novel approach to interpretability in LLMs can pave the way for more interpretable models without the need to sacrifice performance.",
    "path": "papers/23/11/2311.01732.json",
    "total_tokens": 904,
    "translated_title": "Proto-lm：一种基于原型网络的大型语言模型内置可解释性框架",
    "translated_abstract": "大型语言模型（LLM）在自然语言处理（NLP）领域有显著进展，但其缺乏可解释性是一个主要关注点。目前用于解释LLMs的方法是事后的，在推理时间之后应用，并且存在一些限制，比如它们关注低级特征并且在更高级文本单位上缺乏可解释性。在这项工作中，我们引入了proto-lm，这是一个基于原型网络的白盒子框架，允许LLMs在微调阶段学习即时可解释的嵌入，同时保持具有竞争力的性能。通过对各种NLP任务的实验，我们证明了我们方法的适用性和可解释性，并且我们的结果表明了在不牺牲性能的情况下创建可解释性模型的新可能性。这种在LLMs中的新颖解释性方法可以为无需牺牲性能的更可解释性模型铺平道路。",
    "tldr": "Proto-lm是一种基于原型网络的大型语言模型（LLM）内置可解释性框架，通过在微调阶段学习可解释的嵌入来提供解释性，同时保持竞争性能。该方法为创建可解释性模型提供了新的可能性。",
    "en_tdlr": "Proto-lm is a prototypical network-based framework for built-in interpretability in large language models (LLMs), which provides interpretablility by learning interpretable embeddings during the fine-tuning stage while maintaining competitive performance. This approach offers a new possibility for creating interpretable models without sacrificing performance."
}