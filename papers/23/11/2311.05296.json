{
    "title": "BeLLM: Backward Dependency Enhanced Large Language Model for Sentence Embeddings",
    "abstract": "arXiv:2311.05296v2 Announce Type: replace  Abstract: Sentence embeddings are crucial in measuring semantic similarity. Most recent studies employed large language models (LLMs) to learn sentence embeddings. Existing LLMs mainly adopted autoregressive architecture without explicit backward dependency modeling. Therefore, we examined the effects of backward dependencies in LLMs for semantic similarity measurements. Concretely, we propose a novel model: backward dependency enhanced large language model (BeLLM). It learns sentence embeddings via transforming specific attention layers from uni- to bi-directional. We extensively experiment across various semantic textual similarity (STS) tasks and downstream applications. BeLLM achieves state-of-the-art performance in varying scenarios. It shows that auto-regressive LLMs benefit from backward dependencies for sentence embeddings.",
    "link": "https://arxiv.org/abs/2311.05296",
    "context": "Title: BeLLM: Backward Dependency Enhanced Large Language Model for Sentence Embeddings\nAbstract: arXiv:2311.05296v2 Announce Type: replace  Abstract: Sentence embeddings are crucial in measuring semantic similarity. Most recent studies employed large language models (LLMs) to learn sentence embeddings. Existing LLMs mainly adopted autoregressive architecture without explicit backward dependency modeling. Therefore, we examined the effects of backward dependencies in LLMs for semantic similarity measurements. Concretely, we propose a novel model: backward dependency enhanced large language model (BeLLM). It learns sentence embeddings via transforming specific attention layers from uni- to bi-directional. We extensively experiment across various semantic textual similarity (STS) tasks and downstream applications. BeLLM achieves state-of-the-art performance in varying scenarios. It shows that auto-regressive LLMs benefit from backward dependencies for sentence embeddings.",
    "path": "papers/23/11/2311.05296.json",
    "total_tokens": 801,
    "translated_title": "BeLLM：增强反向依赖的大型语言模型用于句子嵌入",
    "translated_abstract": "句子嵌入在衡量语义相似性中至关重要。最近的研究使用大型语言模型（LLMs）来学习句子嵌入。现有的LLMs主要采用自回归架构，没有明确建模反向依赖性。因此，我们研究了LLMs中反向依赖性对语义相似性测量的影响。具体地，我们提出了一种新型模型：增强反向依赖的大型语言模型（BeLLM）。它通过将特定注意力层从单向转换为双向来学习句子嵌入。我们在各种语义文本相似性（STS）任务和下游应用中进行了广泛实验。BeLLM在不同情境下实现了最新的性能。研究表明，自回归LLMs受益于反向依赖性以用于句子嵌入。",
    "tldr": "BeLLM提出了增强反向依赖的大型语言模型，通过引入显式的反向依赖性，在语义相似性测量中取得了最先进的性能。",
    "en_tdlr": "BeLLM introduces a large language model enhanced with backward dependencies, achieving state-of-the-art performance in semantic similarity measurements by explicitly incorporating backward dependencies."
}