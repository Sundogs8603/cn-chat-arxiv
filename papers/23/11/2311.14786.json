{
    "title": "GPT-4V Takes the Wheel: Promises and Challenges for Pedestrian Behavior Prediction. (arXiv:2311.14786v2 [cs.CV] UPDATED)",
    "abstract": "Predicting pedestrian behavior is the key to ensure safety and reliability of autonomous vehicles. While deep learning methods have been promising by learning from annotated video frame sequences, they often fail to fully grasp the dynamic interactions between pedestrians and traffic, crucial for accurate predictions. These models also lack nuanced common sense reasoning. Moreover, the manual annotation of datasets for these models is expensive and challenging to adapt to new situations. The advent of Vision Language Models (VLMs) introduces promising alternatives to these issues, thanks to their advanced visual and causal reasoning skills. To our knowledge, this research is the first to conduct both quantitative and qualitative evaluations of VLMs in the context of pedestrian behavior prediction for autonomous driving. We evaluate GPT-4V(ision) on publicly available pedestrian datasets: JAAD and WiDEVIEW. Our quantitative analysis focuses on GPT-4V's ability to predict pedestrian beha",
    "link": "http://arxiv.org/abs/2311.14786",
    "context": "Title: GPT-4V Takes the Wheel: Promises and Challenges for Pedestrian Behavior Prediction. (arXiv:2311.14786v2 [cs.CV] UPDATED)\nAbstract: Predicting pedestrian behavior is the key to ensure safety and reliability of autonomous vehicles. While deep learning methods have been promising by learning from annotated video frame sequences, they often fail to fully grasp the dynamic interactions between pedestrians and traffic, crucial for accurate predictions. These models also lack nuanced common sense reasoning. Moreover, the manual annotation of datasets for these models is expensive and challenging to adapt to new situations. The advent of Vision Language Models (VLMs) introduces promising alternatives to these issues, thanks to their advanced visual and causal reasoning skills. To our knowledge, this research is the first to conduct both quantitative and qualitative evaluations of VLMs in the context of pedestrian behavior prediction for autonomous driving. We evaluate GPT-4V(ision) on publicly available pedestrian datasets: JAAD and WiDEVIEW. Our quantitative analysis focuses on GPT-4V's ability to predict pedestrian beha",
    "path": "papers/23/11/2311.14786.json",
    "total_tokens": 957,
    "translated_title": "GPT-4V驾驶：行人行为预测的挑战与前景",
    "translated_abstract": "预测行人行为是确保自动驾驶汽车安全可靠的关键。尽管深度学习方法通过学习注释的视频序列表现出了很大潜力，但它们往往无法完全把握行人和交通之间的动态互动，这对准确预测至关重要。这些模型还缺乏细致入微的常识推理能力。此外，为这些模型手动标注数据集既昂贵又难以适应新情况。视觉语言模型（VLMs）的出现为解决这些问题提供了有希望的替代方案，因为它们具备先进的视觉和因果推理能力。据我们所知，这项研究是首次在自动驾驶的行人行为预测背景下对VLMs进行定量和定性评估。我们在公开可用的行人数据集JAAD和WiDEVIEW上评估了GPT-4V(ision)。我们的定量分析重点关注GPT-4V预测行人行为的能力。",
    "tldr": "本论文研究了GPT-4V在自动驾驶的行人行为预测中的应用，通过视觉语言模型的定量和定性评估，解决了传统深度学习方法难以捕捉行人和交通之间动态互动以及缺乏常识推理能力的问题。",
    "en_tdlr": "This paper explores the application of GPT-4V in pedestrian behavior prediction for autonomous driving, addressing the limitations of traditional deep learning methods in capturing dynamic interactions between pedestrians and traffic, as well as the lack of common sense reasoning. This is achieved through quantitative and qualitative evaluations using vision language models."
}