{
    "title": "Calibrated Language Models Must Hallucinate",
    "abstract": "arXiv:2311.14648v3 Announce Type: replace  Abstract: Recent language models generate false but plausible-sounding text with surprising frequency. Such \"hallucinations\" are an obstacle to the usability of language-based AI systems and can harm people who rely upon their outputs. This work shows that there is an inherent statistical lower-bound on the rate that pretrained language models hallucinate certain types of facts, having nothing to do with the transformer LM architecture or data quality. For \"arbitrary\" facts whose veracity cannot be determined from the training data, we show that hallucinations must occur at a certain rate for language models that satisfy a statistical calibration condition appropriate for generative language models. Specifically, if the maximum probability of any fact is bounded, we show that the probability of generating a hallucination is close to the fraction of facts that occur exactly once in the training data (a \"Good-Turing\" estimate), even assuming ide",
    "link": "https://arxiv.org/abs/2311.14648",
    "context": "Title: Calibrated Language Models Must Hallucinate\nAbstract: arXiv:2311.14648v3 Announce Type: replace  Abstract: Recent language models generate false but plausible-sounding text with surprising frequency. Such \"hallucinations\" are an obstacle to the usability of language-based AI systems and can harm people who rely upon their outputs. This work shows that there is an inherent statistical lower-bound on the rate that pretrained language models hallucinate certain types of facts, having nothing to do with the transformer LM architecture or data quality. For \"arbitrary\" facts whose veracity cannot be determined from the training data, we show that hallucinations must occur at a certain rate for language models that satisfy a statistical calibration condition appropriate for generative language models. Specifically, if the maximum probability of any fact is bounded, we show that the probability of generating a hallucination is close to the fraction of facts that occur exactly once in the training data (a \"Good-Turing\" estimate), even assuming ide",
    "path": "papers/23/11/2311.14648.json",
    "total_tokens": 829,
    "translated_title": "校准语言模型必须出现幻觉",
    "translated_abstract": "最近的语言模型频繁生成虚假但听起来似乎合理的文本。这种“幻觉”是语言为基础的人工智能系统可用性的障碍，并可能伤害依赖其输出的人们。这项工作表明，预先训练的语言模型幻想某些类型的事实具有固有的统计下限，与变压器LM架构或数据质量无关。对于那些无法从训练数据中确定真实性的“任意”事实，我们展示了对于满足生成语言模型适当统计校准条件的语言模型，幻觉必须以某种速率发生。具体而言，如果任何事实的最大概率被限制，我们表明生成幻觉的概率接近于在训练数据中仅发生一次的事实的比例（“Good-Turing”估计），即使考虑到可能的情况",
    "tldr": "该研究揭示了预训练语言模型在生成某些类型的事实时具有固有的统计下限，这与变压器LM架构或数据质量无关。",
    "en_tdlr": "This work shows that there is an inherent statistical lower-bound on the rate that pretrained language models hallucinate certain types of facts, having nothing to do with the transformer LM architecture or data quality."
}