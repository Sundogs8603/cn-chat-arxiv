{
    "title": "Hessian Eigenvectors and Principal Component Analysis of Neural Network Weight Matrices. (arXiv:2311.00452v1 [cs.LG])",
    "abstract": "This study delves into the intricate dynamics of trained deep neural networks and their relationships with network parameters. Trained networks predominantly continue training in a single direction, known as the drift mode. This drift mode can be explained by the quadratic potential model of the loss function, suggesting a slow exponential decay towards the potential minima. We unveil a correlation between Hessian eigenvectors and network weights. This relationship, hinging on the magnitude of eigenvalues, allows us to discern parameter directions within the network. Notably, the significance of these directions relies on two defining attributes: the curvature of their potential wells (indicated by the magnitude of Hessian eigenvalues) and their alignment with the weight vectors. Our exploration extends to the decomposition of weight matrices through singular value decomposition. This approach proves practical in identifying critical directions within the Hessian, considering both thei",
    "link": "http://arxiv.org/abs/2311.00452",
    "context": "Title: Hessian Eigenvectors and Principal Component Analysis of Neural Network Weight Matrices. (arXiv:2311.00452v1 [cs.LG])\nAbstract: This study delves into the intricate dynamics of trained deep neural networks and their relationships with network parameters. Trained networks predominantly continue training in a single direction, known as the drift mode. This drift mode can be explained by the quadratic potential model of the loss function, suggesting a slow exponential decay towards the potential minima. We unveil a correlation between Hessian eigenvectors and network weights. This relationship, hinging on the magnitude of eigenvalues, allows us to discern parameter directions within the network. Notably, the significance of these directions relies on two defining attributes: the curvature of their potential wells (indicated by the magnitude of Hessian eigenvalues) and their alignment with the weight vectors. Our exploration extends to the decomposition of weight matrices through singular value decomposition. This approach proves practical in identifying critical directions within the Hessian, considering both thei",
    "path": "papers/23/11/2311.00452.json",
    "total_tokens": 882,
    "translated_title": "神经网络权重矩阵的Hessian特征向量和主成分分析",
    "translated_abstract": "本研究深入探讨了经过训练的深度神经网络的复杂动力学和与网络参数的关系。经过训练的网络主要在单一方向上继续训练，被称为漂移模式。这种漂移模式可以通过损失函数的二次势能模型来解释，暗示了指数慢慢衰减到势能最小值的过程。我们揭示了Hessian特征向量与网络权重之间的相关性。这种关系依赖于特征值的大小，使我们能够区分网络内的参数方向。值得注意的是，这些方向的重要性取决于它们势阱的曲率（由Hessian特征值的大小指示）以及与权重向量的对齐程度。我们的探索还延伸到了通过奇异值分解对权重矩阵进行分解。这种方法在确定Hessian内的关键方向方面非常实用，考虑到它们的曲率和权重向量的对齐性。",
    "tldr": "本研究揭示了神经网络权重矩阵的Hessian特征向量与网络权重之间的相关性，通过识别关键方向揭示了漂移模式和势能最小值的关系。",
    "en_tdlr": "This study reveals the correlation between Hessian eigenvectors and network weights in neural network weight matrices, highlighting the importance of critical directions in understanding the relationship between drift mode and potential minima."
}