{
    "title": "SoulChat: Improving LLMs' Empathy, Listening, and Comfort Abilities through Fine-tuning with Multi-turn Empathy Conversations. (arXiv:2311.00273v1 [cs.CL])",
    "abstract": "Large language models (LLMs) have been widely applied in various fields due to their excellent capability for memorizing knowledge and chain of thought (CoT). When these language models are applied in the field of psychological counseling, they often rush to provide universal advice. However, when users seek psychological support, they need to gain empathy, trust, understanding and comfort, rather than just reasonable advice. To this end, we constructed a multi-turn empathetic conversation dataset of more than 2 million samples, in which the input is the multi-turn conversation context, and the target is empathetic responses that cover expressions such as questioning, comfort, recognition, listening, trust, emotional support, etc. Experiments have shown that the empathy ability of LLMs can be significantly enhanced when finetuning by using multi-turn dialogue history and responses that are closer to the expression of a psychological consultant.",
    "link": "http://arxiv.org/abs/2311.00273",
    "context": "Title: SoulChat: Improving LLMs' Empathy, Listening, and Comfort Abilities through Fine-tuning with Multi-turn Empathy Conversations. (arXiv:2311.00273v1 [cs.CL])\nAbstract: Large language models (LLMs) have been widely applied in various fields due to their excellent capability for memorizing knowledge and chain of thought (CoT). When these language models are applied in the field of psychological counseling, they often rush to provide universal advice. However, when users seek psychological support, they need to gain empathy, trust, understanding and comfort, rather than just reasonable advice. To this end, we constructed a multi-turn empathetic conversation dataset of more than 2 million samples, in which the input is the multi-turn conversation context, and the target is empathetic responses that cover expressions such as questioning, comfort, recognition, listening, trust, emotional support, etc. Experiments have shown that the empathy ability of LLMs can be significantly enhanced when finetuning by using multi-turn dialogue history and responses that are closer to the expression of a psychological consultant.",
    "path": "papers/23/11/2311.00273.json",
    "total_tokens": 869,
    "translated_title": "SoulChat: 通过多轮共情对话微调来提高LLMs的共情、倾听和安慰能力",
    "translated_abstract": "由于其出色的知识和思维链记忆能力，大型语言模型（LLMs）已广泛应用于各个领域。当这些语言模型应用于心理咨询领域时，它们常常急于提供普遍的建议。然而，当用户寻求心理支持时，他们需要获得共情、信任、理解和安慰，而不仅仅是合理的建议。为此，我们构建了一个包含超过200万个样本的多轮共情对话数据集，其中输入是多轮对话的上下文，目标是包括询问、安慰、认可、倾听、信任、情绪支持等表达的共情回应。实验证明，通过使用更接近心理咨询师表达方式的多轮对话历史和回应进行微调，可以显著增强LLMs的共情能力。",
    "tldr": "本研究通过在心理咨询领域构建多轮共情对话数据集，并利用更接近心理咨询师表达方式的对话历史和回应进行微调，成功提高了LLMs的共情能力。",
    "en_tdlr": "This study successfully improved the empathy ability of LLMs by constructing a multi-turn empathetic conversation dataset in the field of psychological counseling and fine-tuning the dialogue history and responses that are closer to the expression of a psychological consultant."
}