{
    "title": "Universal Multi-modal Multi-domain Pre-trained Recommendation. (arXiv:2311.01831v1 [cs.IR])",
    "abstract": "There is a rapidly-growing research interest in modeling user preferences via pre-training multi-domain interactions for recommender systems. However, Existing pre-trained multi-domain recommendations mostly select the item texts to be bridges across domains, and simply explore the user behaviors in target domains. Hence, they ignore other informative multi-modal item contents (e.g., visual information), and also lack of thorough consideration of user behaviors from all interactive domains. To address these issues, in this paper, we propose to pre-train universal multi-modal item content presentation for multi-domain recommendation, called UniM^2Rec, which could smoothly learn the multi-modal item content presentations and the multi-modal user preferences from all domains. With the pre-trained multi-domain recommendation model, UniM^2Rec could be efficiently and effectively transferred to new target domains in practice. Extensive experiments conducted on five real-world datasets in tar",
    "link": "http://arxiv.org/abs/2311.01831",
    "context": "Title: Universal Multi-modal Multi-domain Pre-trained Recommendation. (arXiv:2311.01831v1 [cs.IR])\nAbstract: There is a rapidly-growing research interest in modeling user preferences via pre-training multi-domain interactions for recommender systems. However, Existing pre-trained multi-domain recommendations mostly select the item texts to be bridges across domains, and simply explore the user behaviors in target domains. Hence, they ignore other informative multi-modal item contents (e.g., visual information), and also lack of thorough consideration of user behaviors from all interactive domains. To address these issues, in this paper, we propose to pre-train universal multi-modal item content presentation for multi-domain recommendation, called UniM^2Rec, which could smoothly learn the multi-modal item content presentations and the multi-modal user preferences from all domains. With the pre-trained multi-domain recommendation model, UniM^2Rec could be efficiently and effectively transferred to new target domains in practice. Extensive experiments conducted on five real-world datasets in tar",
    "path": "papers/23/11/2311.01831.json",
    "total_tokens": 917,
    "translated_title": "通用多模态多领域预训练推荐",
    "translated_abstract": "最近，通过预训练多领域交互模型来建模用户偏好的研究兴趣正在迅速增长。然而，现有的预训练多领域推荐大多选择将物品文本作为跨领域的桥梁，并简单地探索目标领域中的用户行为。因此，它们忽视了其他信息丰富的多模态物品内容（如视觉信息），并且也缺乏对所有交互领域的用户行为的彻底考虑。为了解决这些问题，本文提出了一种用于多领域推荐的通用多模态物品内容展示预训练模型UniM^2Rec，该模型可以平滑地学习所有领域的多模态物品内容展示和多模态用户偏好。通过预训练的多领域推荐模型，UniM^2Rec可以在实践中高效且有效地转移到新的目标领域。在五个真实世界数据集上进行了大量实验，结果表明UniM^2Rec相比现有方法具有更好的性能。",
    "tldr": "本文提出了通用多模态多领域预训练推荐模型UniM^2Rec，该模型可以从所有领域平滑地学习多模态物品内容和用户偏好，实验证明其在多个真实世界数据集上具有更好的性能。",
    "en_tdlr": "This paper proposes a universal multi-modal multi-domain pre-trained recommendation model, UniM^2Rec, which can smoothly learn multi-modal item content and user preferences from all domains, and experiments show that it outperforms existing methods on multiple real-world datasets."
}