{
    "title": "M2T2: Multi-Task Masked Transformer for Object-centric Pick and Place. (arXiv:2311.00926v1 [cs.RO])",
    "abstract": "With the advent of large language models and large-scale robotic datasets, there has been tremendous progress in high-level decision-making for object manipulation. These generic models are able to interpret complex tasks using language commands, but they often have difficulties generalizing to out-of-distribution objects due to the inability of low-level action primitives. In contrast, existing task-specific models excel in low-level manipulation of unknown objects, but only work for a single type of action. To bridge this gap, we present M2T2, a single model that supplies different types of low-level actions that work robustly on arbitrary objects in cluttered scenes. M2T2 is a transformer model which reasons about contact points and predicts valid gripper poses for different action modes given a raw point cloud of the scene. Trained on a large-scale synthetic dataset with 128K scenes, M2T2 achieves zero-shot sim2real transfer on the real robot, outperforming the baseline system with",
    "link": "http://arxiv.org/abs/2311.00926",
    "context": "Title: M2T2: Multi-Task Masked Transformer for Object-centric Pick and Place. (arXiv:2311.00926v1 [cs.RO])\nAbstract: With the advent of large language models and large-scale robotic datasets, there has been tremendous progress in high-level decision-making for object manipulation. These generic models are able to interpret complex tasks using language commands, but they often have difficulties generalizing to out-of-distribution objects due to the inability of low-level action primitives. In contrast, existing task-specific models excel in low-level manipulation of unknown objects, but only work for a single type of action. To bridge this gap, we present M2T2, a single model that supplies different types of low-level actions that work robustly on arbitrary objects in cluttered scenes. M2T2 is a transformer model which reasons about contact points and predicts valid gripper poses for different action modes given a raw point cloud of the scene. Trained on a large-scale synthetic dataset with 128K scenes, M2T2 achieves zero-shot sim2real transfer on the real robot, outperforming the baseline system with",
    "path": "papers/23/11/2311.00926.json",
    "total_tokens": 926,
    "translated_title": "M2T2:多任务遮蔽变换器用于物体中心的拾取和放置",
    "translated_abstract": "随着大语言模型和大规模机器人数据集的出现，高级物体操作的高级决策已经取得了巨大进展。这些通用模型能够使用语言指令解释复杂任务，但由于低级动作原语的能力不足，它们通常难以推广到超出分布范围的对象。相反，现有的任务特定模型在未知对象的低级操作方面表现出色，但仅适用于单一类型的动作。为了弥合这个差距，我们提出了M2T2，它是一个单一模型，可以在杂乱场景中稳定地提供不同类型的低级操作。M2T2是一个变换器模型，通过对场景的原始点云进行推理，推断接触点，并预测不同动作模式的有效夹持器姿势。在一个包含128K个场景的大规模合成数据集上训练，M2T2在真实机器人上实现了零-shot仿真到实际的转移，并超过了基准系统。",
    "tldr": "M2T2是一个用于物体操作的多任务遮蔽变换器模型，通过推理场景的原始点云，可以稳定地提供不同类型的低级操作，并在真实机器人上实现了零-shot仿真到实际的转移。"
}