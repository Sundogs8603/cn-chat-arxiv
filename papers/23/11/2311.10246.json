{
    "title": "Surprisal Driven $k$-NN for Robust and Interpretable Nonparametric Learning",
    "abstract": "Nonparametric learning is a fundamental concept in machine learning that aims to capture complex patterns and relationships in data without making strong assumptions about the underlying data distribution. Owing to simplicity and familiarity, one of the most well-known algorithms under this paradigm is the $k$-nearest neighbors ($k$-NN) algorithm. Driven by the usage of machine learning in safety-critical applications, in this work, we shed new light on the traditional nearest neighbors algorithm from the perspective of information theory and propose a robust and interpretable framework for tasks such as classification, regression, density estimation, and anomaly detection using a single model. We can determine data point weights as well as feature contributions by calculating the conditional entropy for adding a feature without the need for explicit model training. This allows us to compute feature contributions by providing detailed data point influence weights with perfect attributi",
    "link": "https://arxiv.org/abs/2311.10246",
    "context": "Title: Surprisal Driven $k$-NN for Robust and Interpretable Nonparametric Learning\nAbstract: Nonparametric learning is a fundamental concept in machine learning that aims to capture complex patterns and relationships in data without making strong assumptions about the underlying data distribution. Owing to simplicity and familiarity, one of the most well-known algorithms under this paradigm is the $k$-nearest neighbors ($k$-NN) algorithm. Driven by the usage of machine learning in safety-critical applications, in this work, we shed new light on the traditional nearest neighbors algorithm from the perspective of information theory and propose a robust and interpretable framework for tasks such as classification, regression, density estimation, and anomaly detection using a single model. We can determine data point weights as well as feature contributions by calculating the conditional entropy for adding a feature without the need for explicit model training. This allows us to compute feature contributions by providing detailed data point influence weights with perfect attributi",
    "path": "papers/23/11/2311.10246.json",
    "total_tokens": 863,
    "translated_title": "基于惊喜性驱动的稳健可解释的非参数学习中的k-NN算法",
    "translated_abstract": "非参数学习是机器学习中的一个基本概念，旨在捕捉数据中的复杂模式和关系，而不对潜在的数据分布做出强烈的假设。在这一范式下，最为著名的算法之一是k最近邻（k-NN）算法。在这项工作中，我们通过使用机器学习在安全关键应用中的应用，从信息论的角度对传统的最近邻算法进行了新的阐释，并提出了一种稳健可解释的框架，用于分类、回归、密度估计和异常检测等任务。我们可以通过计算增加特征时的条件熵来确定数据点的权重和特征的贡献，而无需进行显式的模型训练。这使我们能够通过提供详细的数据点影响权重来计算特征的贡献。",
    "tldr": "本论文提出了一种基于惊喜性驱动的稳健可解释的k-NN算法，通过使用信息论的角度对传统算法进行新的阐释，实现了在非参数学习中的分类、回归、密度估计和异常检测等任务。",
    "en_tdlr": "This paper presents a surprisal driven robust and interpretable k-NN algorithm, which sheds new light on the traditional algorithm from an information theory perspective and achieves tasks such as classification, regression, density estimation, and anomaly detection in nonparametric learning."
}