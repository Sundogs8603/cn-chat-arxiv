{
    "title": "Self-Influence Guided Data Reweighting for Language Model Pre-training. (arXiv:2311.00913v1 [cs.CL])",
    "abstract": "Language Models (LMs) pre-trained with self-supervision on large text corpora have become the default starting point for developing models for various NLP tasks. Once the pre-training corpus has been assembled, all data samples in the corpus are treated with equal importance during LM pre-training. However, due to varying levels of relevance and quality of data, equal importance to all the data samples may not be the optimal choice. While data reweighting has been explored in the context of task-specific supervised learning and LM fine-tuning, model-driven reweighting for pre-training data has not been explored. We fill this important gap and propose PRESENCE, a method for jointly reweighting samples by leveraging self-influence (SI) scores as an indicator of sample importance and pre-training. PRESENCE promotes novelty and stability for model pre-training. Through extensive analysis spanning multiple model sizes, datasets, and tasks, we present PRESENCE as an important first step in t",
    "link": "http://arxiv.org/abs/2311.00913",
    "context": "Title: Self-Influence Guided Data Reweighting for Language Model Pre-training. (arXiv:2311.00913v1 [cs.CL])\nAbstract: Language Models (LMs) pre-trained with self-supervision on large text corpora have become the default starting point for developing models for various NLP tasks. Once the pre-training corpus has been assembled, all data samples in the corpus are treated with equal importance during LM pre-training. However, due to varying levels of relevance and quality of data, equal importance to all the data samples may not be the optimal choice. While data reweighting has been explored in the context of task-specific supervised learning and LM fine-tuning, model-driven reweighting for pre-training data has not been explored. We fill this important gap and propose PRESENCE, a method for jointly reweighting samples by leveraging self-influence (SI) scores as an indicator of sample importance and pre-training. PRESENCE promotes novelty and stability for model pre-training. Through extensive analysis spanning multiple model sizes, datasets, and tasks, we present PRESENCE as an important first step in t",
    "path": "papers/23/11/2311.00913.json",
    "total_tokens": 860,
    "translated_title": "基于自主影响引导的语言模型预训练数据重加权方法",
    "translated_abstract": "在大型文本语料库上进行自监督预训练的语言模型已成为开发各种自然语言处理任务模型的默认起点。然而，一旦预训练语料库被组装好，在LM预训练期间，所有数据样本都被视为具有相等的重要性。然而，由于数据的相关性和质量存在差异，对所有数据样本给予相等的重要性可能不是最优选择。我们填补了这个重要空白，并提出了一种名为PRESENCE的方法，通过利用自主影响（SI）分数作为样本重要性和预训练的指标，共同对样本进行重加权。PRESENCE促进了模型预训练的新颖性和稳定性。通过涵盖多个模型规模、数据集和任务的广泛分析，我们将PRESENCE提出为一个重要的首要步骤。",
    "tldr": "提出了一种名为PRESENCE的方法，通过利用自主影响分数作为重要性指标，对语言模型预训练的数据样本进行重加权，促进了模型预训练的新颖性和稳定性。",
    "en_tdlr": "Introducing PRESENCE, a method that reweights the data samples in language model pre-training using self-influence scores as indicators of sample importance, promoting novelty and stability in model pre-training."
}