{
    "title": "Disentangling Quantum and Classical Contributions in Hybrid Quantum Machine Learning Architectures. (arXiv:2311.05559v2 [quant-ph] UPDATED)",
    "abstract": "Quantum computing offers the potential for superior computational capabilities, particularly for data-intensive tasks. However, the current state of quantum hardware puts heavy restrictions on input size. To address this, hybrid transfer learning solutions have been developed, merging pre-trained classical models, capable of handling extensive inputs, with variational quantum circuits. Yet, it remains unclear how much each component -- classical and quantum -- contributes to the model's results. We propose a novel hybrid architecture: instead of utilizing a pre-trained network for compression, we employ an autoencoder to derive a compressed version of the input data. This compressed data is then channeled through the encoder part of the autoencoder to the quantum component. We assess our model's classification capabilities against two state-of-the-art hybrid transfer learning architectures, two purely classical architectures and one quantum architecture. Their accuracy is compared acro",
    "link": "http://arxiv.org/abs/2311.05559",
    "context": "Title: Disentangling Quantum and Classical Contributions in Hybrid Quantum Machine Learning Architectures. (arXiv:2311.05559v2 [quant-ph] UPDATED)\nAbstract: Quantum computing offers the potential for superior computational capabilities, particularly for data-intensive tasks. However, the current state of quantum hardware puts heavy restrictions on input size. To address this, hybrid transfer learning solutions have been developed, merging pre-trained classical models, capable of handling extensive inputs, with variational quantum circuits. Yet, it remains unclear how much each component -- classical and quantum -- contributes to the model's results. We propose a novel hybrid architecture: instead of utilizing a pre-trained network for compression, we employ an autoencoder to derive a compressed version of the input data. This compressed data is then channeled through the encoder part of the autoencoder to the quantum component. We assess our model's classification capabilities against two state-of-the-art hybrid transfer learning architectures, two purely classical architectures and one quantum architecture. Their accuracy is compared acro",
    "path": "papers/23/11/2311.05559.json",
    "total_tokens": 717,
    "translated_title": "在混合量子机器学习架构中分离量子和经典贡献",
    "translated_abstract": "量子计算为数据密集型任务提供了更强大的计算能力，但目前的量子硬件对输入规模有很大限制。为了解决这个问题，我们提出了一种新颖的混合架构：使用自编码器得到输入数据的压缩版本，然后将其作为量子部分的输入。我们将该模型的分类能力与两种最先进的混合迁移学习架构、两种纯经典架构和一种量子架构进行了比较。",
    "tldr": "我们提出了一种新颖的混合架构，通过自编码器将输入数据压缩后传递给量子部分，实现了在混合量子机器学习中分离量子和经典贡献。",
    "en_tdlr": "We propose a novel hybrid architecture that separates quantum and classical contributions in hybrid quantum machine learning by using an autoencoder to compress the input data for the quantum component."
}