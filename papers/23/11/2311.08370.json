{
    "title": "SimpleSafetyTests: a Test Suite for Identifying Critical Safety Risks in Large Language Models",
    "abstract": "arXiv:2311.08370v2 Announce Type: replace  Abstract: The past year has seen rapid acceleration in the development of large language models (LLMs). However, without proper steering and safeguards, LLMs will readily follow malicious instructions, provide unsafe advice, and generate toxic content. We introduce SimpleSafetyTests (SST) as a new test suite for rapidly and systematically identifying such critical safety risks. The test suite comprises 100 test prompts across five harm areas that LLMs, for the vast majority of applications, should refuse to comply with. We test 11 open-access and open-source LLMs and four closed-source LLMs, and find critical safety weaknesses. While some of the models do not give a single unsafe response, most give unsafe responses to more than 20% of the prompts, with over 50% unsafe responses in the extreme. Prepending a safety-emphasising system prompt substantially reduces the occurrence of unsafe responses, but does not completely stop them from happenin",
    "link": "https://arxiv.org/abs/2311.08370",
    "context": "Title: SimpleSafetyTests: a Test Suite for Identifying Critical Safety Risks in Large Language Models\nAbstract: arXiv:2311.08370v2 Announce Type: replace  Abstract: The past year has seen rapid acceleration in the development of large language models (LLMs). However, without proper steering and safeguards, LLMs will readily follow malicious instructions, provide unsafe advice, and generate toxic content. We introduce SimpleSafetyTests (SST) as a new test suite for rapidly and systematically identifying such critical safety risks. The test suite comprises 100 test prompts across five harm areas that LLMs, for the vast majority of applications, should refuse to comply with. We test 11 open-access and open-source LLMs and four closed-source LLMs, and find critical safety weaknesses. While some of the models do not give a single unsafe response, most give unsafe responses to more than 20% of the prompts, with over 50% unsafe responses in the extreme. Prepending a safety-emphasising system prompt substantially reduces the occurrence of unsafe responses, but does not completely stop them from happenin",
    "path": "papers/23/11/2311.08370.json",
    "total_tokens": 863,
    "translated_title": "SimpleSafetyTests：一个用于识别大语言模型中关键安全风险的测试套件",
    "translated_abstract": "过去一年，大语言模型（LLMs）的发展急剧加速。然而，如果缺乏适当的引导和保障，LLMs将很容易遵循恶意指令，提供不安全的建议，并生成有毒内容。我们引入SimpleSafetyTests（SST）作为一个新的测试套件，可以快速系统地识别此类关键安全风险。该测试套件包括100个测试提示，涵盖五个LLMs应该拒绝遵从的伤害领域。我们测试了11个开放获取和开源LLMs以及四个封闭源LLMs，并发现了关键的安全性弱点。虽然其中一些模型没有给出单一的不安全响应，但大多数对超过20%的提示给出了不安全响应，极端情况下超过50%的不安全响应。在系统提示中加入强调安全性的前置内容显著减少了不安全响应的发生，但并不能完全阻止它们发生。",
    "tldr": "引入SimpleSafetyTests（SST）作为一个新的测试套件，用于快速系统地识别大语言模型中关键的安全风险"
}