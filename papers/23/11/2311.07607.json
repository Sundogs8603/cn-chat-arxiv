{
    "title": "Modeling Choice via Self-Attention",
    "abstract": "Models of choice are a fundamental input to many now-canonical optimization problems in the field of Operations Management, including assortment, inventory, and price optimization. Naturally, accurate estimation of these models from data is a critical step in the application of these optimization problems in practice. Concurrently, recent advancements in deep learning have sparked interest in integrating these techniques into choice modeling. However, there is a noticeable research gap at the intersection of deep learning and choice modeling, particularly with both theoretical and empirical foundations. Thus motivated, we first propose a choice model that is the first to successfully (both theoretically and practically) leverage a modern neural network architectural concept (self-attention). Theoretically, we show that our attention-based choice model is a low-rank generalization of the Halo Multinomial Logit (Halo-MNL) model. We prove that whereas the Halo-MNL requires $\\Omega(m^2)$ d",
    "link": "https://arxiv.org/abs/2311.07607",
    "context": "Title: Modeling Choice via Self-Attention\nAbstract: Models of choice are a fundamental input to many now-canonical optimization problems in the field of Operations Management, including assortment, inventory, and price optimization. Naturally, accurate estimation of these models from data is a critical step in the application of these optimization problems in practice. Concurrently, recent advancements in deep learning have sparked interest in integrating these techniques into choice modeling. However, there is a noticeable research gap at the intersection of deep learning and choice modeling, particularly with both theoretical and empirical foundations. Thus motivated, we first propose a choice model that is the first to successfully (both theoretically and practically) leverage a modern neural network architectural concept (self-attention). Theoretically, we show that our attention-based choice model is a low-rank generalization of the Halo Multinomial Logit (Halo-MNL) model. We prove that whereas the Halo-MNL requires $\\Omega(m^2)$ d",
    "path": "papers/23/11/2311.07607.json",
    "total_tokens": 819,
    "translated_title": "通过自注意力建模选择",
    "translated_abstract": "选择模型是运营管理领域中许多经典优化问题的基础输入，包括组合、库存和定价优化。准确地从数据中估计这些模型是在实践中应用这些优化问题的关键步骤。与此同时，深度学习的最新进展引起了将这些技术整合到选择建模中的兴趣。然而，在深度学习和选择建模的交叉点上存在明显的研究空白，尤其是在理论和经验基础上。因此，我们首先提出了一种选择模型，这是第一个成功（从理论和实践两个方面）利用现代神经网络架构概念（自注意力）的模型。在理论上，我们证明了我们基于注意力的选择模型是Halo多项式逻辑（Halo-MNL）模型的低秩推广。我们证明了Halo-MNL模型需要$\\Omega(m^2)$的计算量，而我们的模型只需要$\\Omega(m)$的计算量。",
    "tldr": "本论文提出了一种选择模型，利用自注意力成功地进行了建模，这是在深度学习和选择建模领域中的一个重要的研究空白。",
    "en_tdlr": "This paper proposes a choice model that leverages self-attention to successfully model choice, filling a significant research gap in the intersection of deep learning and choice modeling."
}