{
    "title": "Multimodal Foundation Models for Zero-shot Animal Species Recognition in Camera Trap Images. (arXiv:2311.01064v1 [cs.CV])",
    "abstract": "Due to deteriorating environmental conditions and increasing human activity, conservation efforts directed towards wildlife is crucial. Motion-activated camera traps constitute an efficient tool for tracking and monitoring wildlife populations across the globe. Supervised learning techniques have been successfully deployed to analyze such imagery, however training such techniques requires annotations from experts. Reducing the reliance on costly labelled data therefore has immense potential in developing large-scale wildlife tracking solutions with markedly less human labor. In this work we propose WildMatch, a novel zero-shot species classification framework that leverages multimodal foundation models. In particular, we instruction tune vision-language models to generate detailed visual descriptions of camera trap images using similar terminology to experts. Then, we match the generated caption to an external knowledge base of descriptions in order to determine the species in a zero-s",
    "link": "http://arxiv.org/abs/2311.01064",
    "context": "Title: Multimodal Foundation Models for Zero-shot Animal Species Recognition in Camera Trap Images. (arXiv:2311.01064v1 [cs.CV])\nAbstract: Due to deteriorating environmental conditions and increasing human activity, conservation efforts directed towards wildlife is crucial. Motion-activated camera traps constitute an efficient tool for tracking and monitoring wildlife populations across the globe. Supervised learning techniques have been successfully deployed to analyze such imagery, however training such techniques requires annotations from experts. Reducing the reliance on costly labelled data therefore has immense potential in developing large-scale wildlife tracking solutions with markedly less human labor. In this work we propose WildMatch, a novel zero-shot species classification framework that leverages multimodal foundation models. In particular, we instruction tune vision-language models to generate detailed visual descriptions of camera trap images using similar terminology to experts. Then, we match the generated caption to an external knowledge base of descriptions in order to determine the species in a zero-s",
    "path": "papers/23/11/2311.01064.json",
    "total_tokens": 883,
    "translated_title": "零样本条件下基于多模式基础模型的相机陷阱图像动物物种识别",
    "translated_abstract": "由于环境恶化和人类活动增加，野生动物保护工作至关重要。运动触发的相机陷阱是全球追踪和监测野生动物数量的有效工具。监督学习技术已成功应用于分析这类图像，但是训练这些技术需要来自专家的注释。减少对昂贵标记数据的依赖，因此在开发具有明显较少人力劳动的大规模野生动物追踪解决方案方面具有巨大潜力。在这项工作中，我们提出了WildMatch，一个利用多模式基础模型的新型零样本物种分类框架。特别地，我们通过指导调整视觉-语言模型，使用类似专家的术语生成相机陷阱图像的详细视觉描述。然后，我们将生成的描述与外部知识库中的描述进行匹配，以确定零样本条件下的物种。",
    "tldr": "本论文提出了一种新颖的零样本物种分类框架，通过使用多模式基础模型，利用视觉-语言模型生成相机陷阱图像的视觉描述，并通过匹配外部知识库中的描述来确定零样本条件下的物种。",
    "en_tdlr": "This paper proposes a novel zero-shot species classification framework that uses multimodal foundation models. It generates visual descriptions of camera trap images using vision-language models, and matches the descriptions with an external knowledge base to determine the species under zero-shot conditions."
}