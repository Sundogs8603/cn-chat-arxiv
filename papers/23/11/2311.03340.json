{
    "title": "Multitask Kernel-based Learning with First-Order Logic Constraints",
    "abstract": "In this paper we propose a general framework to integrate supervised and unsupervised examples with background knowledge expressed by a collection of first-order logic clauses into kernel machines. In particular, we consider a multi-task learning scheme where multiple predicates defined on a set of objects are to be jointly learned from examples, enforcing a set of FOL constraints on the admissible configurations of their values. The predicates are defined on the feature spaces, in which the input objects are represented, and can be either known a priori or approximated by an appropriate kernel-based learner. A general approach is presented to convert the FOL clauses into a continuous implementation that can deal with the outputs computed by the kernel-based predicates. The learning problem is formulated as a semi-supervised task that requires the optimization in the primal of a loss function that combines a fitting loss measure on the supervised examples, a regularization term, and a ",
    "link": "https://arxiv.org/abs/2311.03340",
    "context": "Title: Multitask Kernel-based Learning with First-Order Logic Constraints\nAbstract: In this paper we propose a general framework to integrate supervised and unsupervised examples with background knowledge expressed by a collection of first-order logic clauses into kernel machines. In particular, we consider a multi-task learning scheme where multiple predicates defined on a set of objects are to be jointly learned from examples, enforcing a set of FOL constraints on the admissible configurations of their values. The predicates are defined on the feature spaces, in which the input objects are represented, and can be either known a priori or approximated by an appropriate kernel-based learner. A general approach is presented to convert the FOL clauses into a continuous implementation that can deal with the outputs computed by the kernel-based predicates. The learning problem is formulated as a semi-supervised task that requires the optimization in the primal of a loss function that combines a fitting loss measure on the supervised examples, a regularization term, and a ",
    "path": "papers/23/11/2311.03340.json",
    "total_tokens": 874,
    "translated_title": "基于一阶逻辑约束的多任务核机器学习",
    "translated_abstract": "本文提出了一个通用的框架，将由一系列一阶逻辑子句表达的背景知识与有监督和无监督示例整合到核机器中。特别地，我们考虑了一个多任务学习方案，在该方案中，定义在一组对象上的多个谓词需要从示例中共同学习，并对其值的合法配置施加一系列的FOL约束。这些谓词是定义在输入对象表示的特征空间上的，并且可以是已知的事先定义好的，也可以由适当的基于核的学习器进行近似。我们提出了一种通用的方法，将FOL子句转化为一个连续实现，能够处理由基于核的谓词计算出的输出。该学习问题被视为半监督任务，需要在损失函数的原属上进行优化，该损失函数包括对有监督示例的拟合损失度量、正则化项和",
    "tldr": "本文提出了一个通用框架，将有监督和无监督示例与一阶逻辑背景知识整合到核机器学习中，实现多任务学习。通过将一阶逻辑约束转化为连续实现的形式，有效处理基于核的谓词的输出。",
    "en_tdlr": "This paper proposes a general framework to integrate supervised and unsupervised examples with first-order logic background knowledge into kernel machines, enabling multitask learning. By converting first-order logic constraints into a continuous implementation, the approach effectively handles the outputs of kernel-based predicates."
}