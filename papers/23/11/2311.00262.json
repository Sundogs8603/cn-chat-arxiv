{
    "title": "Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents. (arXiv:2311.00262v1 [cs.CL])",
    "abstract": "Proactive dialogues serve as a practical yet challenging dialogue problem in the era of large language models (LLMs), where the dialogue policy planning is the key to improving the proactivity of LLMs. Most existing studies enable the dialogue policy planning of LLMs using various prompting schemes or iteratively enhance this capability in handling the given case with verbal AI feedback. However, these approaches are either bounded by the policy planning capability of the frozen LLMs or hard to be transferred to new cases. In this work, we introduce a new dialogue policy planning paradigm to strategize LLMs for proactive dialogue problems with a tunable language model plug-in as a plug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a novel training framework to facilitate supervised fine-tuning over available human-annotated data as well as reinforcement learning from goal-oriented AI feedback with dynamic interaction data collected by the LLM-based self-play s",
    "link": "http://arxiv.org/abs/2311.00262",
    "context": "Title: Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents. (arXiv:2311.00262v1 [cs.CL])\nAbstract: Proactive dialogues serve as a practical yet challenging dialogue problem in the era of large language models (LLMs), where the dialogue policy planning is the key to improving the proactivity of LLMs. Most existing studies enable the dialogue policy planning of LLMs using various prompting schemes or iteratively enhance this capability in handling the given case with verbal AI feedback. However, these approaches are either bounded by the policy planning capability of the frozen LLMs or hard to be transferred to new cases. In this work, we introduce a new dialogue policy planning paradigm to strategize LLMs for proactive dialogue problems with a tunable language model plug-in as a plug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a novel training framework to facilitate supervised fine-tuning over available human-annotated data as well as reinforcement learning from goal-oriented AI feedback with dynamic interaction data collected by the LLM-based self-play s",
    "path": "papers/23/11/2311.00262.json",
    "total_tokens": 970,
    "translated_title": "基于大型语言模型的对话代理的即插即用策略规划器",
    "translated_abstract": "在大型语言模型（LLMs）的时代中，主动对话作为一个实际但具有挑战性的对话问题，对话策略规划是提高LLMs主动性的关键。大多数现有研究使用各种提示方案或通过语言人工智能反馈迭代增强对LLMs的对话策略规划能力。然而，这些方法要么受限于冻结的LLMs的策略规划能力，要么难以转移到新的案例。在这项工作中，我们引入了一种新的对话策略规划范式，以使用可调整的语言模型插件作为即插即用的对话策略规划器来制定LLMs在主动对话问题上的策略，命名为PPDPP。具体而言，我们开发了一个新颖的训练框架，以便利用可用的人工注释数据进行监督微调，并通过基于LLM的自我对弈收集的动态交互数据进行强化学习。",
    "tldr": "基于大型语言模型的对话代理的即插即用策略规划器(PDDPP)引入了一种新的对话策略规划范式，通过可调整的语言模型插件实现主动对话问题的策略制定。利用监督微调和强化学习，该框架在处理新的案例时具有较高的灵活性和性能。"
}