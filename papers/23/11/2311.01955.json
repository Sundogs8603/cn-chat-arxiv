{
    "title": "Too Much Information: Keeping Training Simple for BabyLMs. (arXiv:2311.01955v1 [cs.CL])",
    "abstract": "This paper details the work of the University of Groningen for the BabyLM Challenge. We follow the idea that, like babies, language models should be introduced to simpler concepts first and build off of that knowledge to understand more complex concepts. We examine this strategy of simple-then-complex through a variety of lenses, namely context size, vocabulary, and overall linguistic complexity of the data. We find that only one, context size, is truly beneficial to training a language model. However this simple change to context size gives us improvements of 2 points on average on (Super)GLUE tasks, 1 point on MSGS tasks, and 12\\% on average on BLiMP tasks. Our context-limited model outperforms the baseline that was trained on 10$\\times$ the amount of data.",
    "link": "http://arxiv.org/abs/2311.01955",
    "context": "Title: Too Much Information: Keeping Training Simple for BabyLMs. (arXiv:2311.01955v1 [cs.CL])\nAbstract: This paper details the work of the University of Groningen for the BabyLM Challenge. We follow the idea that, like babies, language models should be introduced to simpler concepts first and build off of that knowledge to understand more complex concepts. We examine this strategy of simple-then-complex through a variety of lenses, namely context size, vocabulary, and overall linguistic complexity of the data. We find that only one, context size, is truly beneficial to training a language model. However this simple change to context size gives us improvements of 2 points on average on (Super)GLUE tasks, 1 point on MSGS tasks, and 12\\% on average on BLiMP tasks. Our context-limited model outperforms the baseline that was trained on 10$\\times$ the amount of data.",
    "path": "papers/23/11/2311.01955.json",
    "total_tokens": 862,
    "translated_title": "太多信息：保持BabyLM训练简单",
    "translated_abstract": "本文详细介绍了格罗宁根大学在BabyLM挑战赛中的工作。我们遵循婴儿一样，语言模型应该先介于较简单的概念，然后建立在此基础上理解更复杂的概念的思想。我们通过多个角度，即上下文大小、词汇量和数据的整体语言复杂性，来研究简单到复杂的策略。我们发现，只有上下文大小这个简单的改变对训练语言模型真正有益。然而，这个简单的上下文大小的变化使我们在(Super)GLUE任务上平均提高了2个点，在MSGS任务上提高了1个点，在BLiMP任务上平均提高了12%。我们的上下文受限模型胜过了训练了10倍数据量的基线模型。",
    "tldr": "本文介绍了格罗宁根大学在BabyLM挑战赛中的工作，探讨了将训练过程简单化的策略。研究发现，对于训练语言模型而言，仅改变上下文大小可以带来显著提升，在各项任务中表现优于基线模型。",
    "en_tdlr": "This paper describes the work of the University of Groningen in the BabyLM Challenge and explores the strategy of simplifying the training process. The study finds that a simple change in context size can significantly improve the performance of language models, surpassing the baseline model in various tasks."
}