{
    "title": "How do Minimum-Norm Shallow Denoisers Look in Function Space?. (arXiv:2311.06748v2 [stat.ML] UPDATED)",
    "abstract": "Neural network (NN) denoisers are an essential building block in many common tasks, ranging from image reconstruction to image generation. However, the success of these models is not well understood from a theoretical perspective. In this paper, we aim to characterize the functions realized by shallow ReLU NN denoisers -- in the common theoretical setting of interpolation (i.e., zero training loss) with a minimal representation cost (i.e., minimal $\\ell^2$ norm weights). First, for univariate data, we derive a closed form for the NN denoiser function, find it is contractive toward the clean data points, and prove it generalizes better than the empirical MMSE estimator at a low noise level. Next, for multivariate data, we find the NN denoiser functions in a closed form under various geometric assumptions on the training data: data contained in a low-dimensional subspace, data contained in a union of one-sided rays, or several types of simplexes. These functions decompose into a sum of s",
    "link": "http://arxiv.org/abs/2311.06748",
    "context": "Title: How do Minimum-Norm Shallow Denoisers Look in Function Space?. (arXiv:2311.06748v2 [stat.ML] UPDATED)\nAbstract: Neural network (NN) denoisers are an essential building block in many common tasks, ranging from image reconstruction to image generation. However, the success of these models is not well understood from a theoretical perspective. In this paper, we aim to characterize the functions realized by shallow ReLU NN denoisers -- in the common theoretical setting of interpolation (i.e., zero training loss) with a minimal representation cost (i.e., minimal $\\ell^2$ norm weights). First, for univariate data, we derive a closed form for the NN denoiser function, find it is contractive toward the clean data points, and prove it generalizes better than the empirical MMSE estimator at a low noise level. Next, for multivariate data, we find the NN denoiser functions in a closed form under various geometric assumptions on the training data: data contained in a low-dimensional subspace, data contained in a union of one-sided rays, or several types of simplexes. These functions decompose into a sum of s",
    "path": "papers/23/11/2311.06748.json",
    "total_tokens": 902,
    "translated_title": "最小范数浅层去噪器在函数空间中的表现如何？",
    "translated_abstract": "神经网络去噪器（NN去噪器）是许多常见任务中的基本构建块，从图像重建到图像生成。然而，从理论角度来看，这些模型的成功尚不明确。本文旨在描述浅层ReLU NN去噪器实现的函数特性--在插值的常见理论设置下（即零训练损失）以及最小表示成本（即最小的l^2范数权重）。首先，对于一元数据，我们导出了NN去噪器函数的闭合形式，发现它对干净数据点具有收缩性，并证明在低噪声水平下它比经验MMSE估计器更好地泛化。接下来，对于多元数据，我们在多种几何假设下找到了闭合形式的NN去噪器函数：数据包含在低维子空间中，数据包含在单向射线的并集中，或者多种类型的简单形状。这些函数分解为一个和的形式。",
    "tldr": "研究了最小范数浅层去噪器在函数空间中的表现，推导出一元数据和多元数据上的闭合形式，并发现其具有收缩性和较好的泛化能力。",
    "en_tdlr": "Explored the performance of minimum-norm shallow denoisers in function space, derived closed forms for univariate and multivariate data, and found that they exhibit contractive behavior and better generalization capability."
}