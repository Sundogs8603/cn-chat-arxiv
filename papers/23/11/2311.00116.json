{
    "title": "BERTwich: Extending BERT's Capabilities to Model Dialectal and Noisy Text. (arXiv:2311.00116v1 [cs.CL])",
    "abstract": "Real-world NLP applications often deal with nonstandard text (e.g., dialectal, informal, or misspelled text). However, language models like BERT deteriorate in the face of dialect variation or noise. How do we push BERT's modeling capabilities to encompass nonstandard text? Fine-tuning helps, but it is designed for specializing a model to a task and does not seem to bring about the deeper, more pervasive changes needed to adapt a model to nonstandard language. In this paper, we introduce the novel idea of sandwiching BERT's encoder stack between additional encoder layers trained to perform masked language modeling on noisy text. We find that our approach, paired with recent work on including character-level noise in fine-tuning data, can promote zero-shot transfer to dialectal text, as well as reduce the distance in the embedding space between words and their noisy counterparts.",
    "link": "http://arxiv.org/abs/2311.00116",
    "context": "Title: BERTwich: Extending BERT's Capabilities to Model Dialectal and Noisy Text. (arXiv:2311.00116v1 [cs.CL])\nAbstract: Real-world NLP applications often deal with nonstandard text (e.g., dialectal, informal, or misspelled text). However, language models like BERT deteriorate in the face of dialect variation or noise. How do we push BERT's modeling capabilities to encompass nonstandard text? Fine-tuning helps, but it is designed for specializing a model to a task and does not seem to bring about the deeper, more pervasive changes needed to adapt a model to nonstandard language. In this paper, we introduce the novel idea of sandwiching BERT's encoder stack between additional encoder layers trained to perform masked language modeling on noisy text. We find that our approach, paired with recent work on including character-level noise in fine-tuning data, can promote zero-shot transfer to dialectal text, as well as reduce the distance in the embedding space between words and their noisy counterparts.",
    "path": "papers/23/11/2311.00116.json",
    "total_tokens": 879,
    "translated_title": "BERTwich: 扩展BERT模型的能力以建模方言和噪声文本",
    "translated_abstract": "现实世界中的自然语言处理应用经常处理非标准文本（例如方言、非正式或拼写错误的文本）。然而，像BERT这样的语言模型在面对方言变化或噪声时退化。我们如何推动BERT的建模能力以涵盖非标准文本？微调有所帮助，但它的设计是为了将模型专门化到一个任务，并不能带来适应非标准语言所需的更深入、更普遍的变化。在本文中，我们引入了一个新颖的想法，即在额外的编码器层中将BERT的编码器堆叠插入到对噪声文本进行遮蔽语言建模训练的层之间。我们发现，我们的方法与最近的工作，以及在微调数据中包含字符级噪声，可以促进零射击传递到方言文本，同时减小词与其噪声对应词在嵌入空间中的距离。",
    "tldr": "BERTwich通过在BERT的编码器堆叠与额外的编码器层之间插入进行噪声文本遮蔽语言建模训练的方式，实现了对方言和噪声文本的建模能力扩展。",
    "en_tdlr": "BERTwich extends BERT's modeling capabilities to encompass dialectal and noisy text by sandwiching BERT's encoder stack between additional encoder layers trained for masked language modeling on noisy text."
}