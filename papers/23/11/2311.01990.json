{
    "title": "Conditions on Preference Relations that Guarantee the Existence of Optimal Policies. (arXiv:2311.01990v1 [cs.LG])",
    "abstract": "Learning from Preferential Feedback (LfPF) plays an essential role in training Large Language Models, as well as certain types of interactive learning agents. However, a substantial gap exists between the theory and application of LfPF algorithms. Current results guaranteeing the existence of optimal policies in LfPF problems assume that both the preferences and transition dynamics are determined by a Markov Decision Process. We introduce the Direct Preference Process, a new framework for analyzing LfPF problems in partially-observable, non-Markovian environments. Within this framework, we establish conditions that guarantee the existence of optimal policies by considering the ordinal structure of the preferences. Using the von Neumann-Morgenstern Expected Utility Theorem, we show that the Direct Preference Process generalizes the standard reinforcement learning problem. Our findings narrow the gap between the empirical success and theoretical understanding of LfPF algorithms and provi",
    "link": "http://arxiv.org/abs/2311.01990",
    "context": "Title: Conditions on Preference Relations that Guarantee the Existence of Optimal Policies. (arXiv:2311.01990v1 [cs.LG])\nAbstract: Learning from Preferential Feedback (LfPF) plays an essential role in training Large Language Models, as well as certain types of interactive learning agents. However, a substantial gap exists between the theory and application of LfPF algorithms. Current results guaranteeing the existence of optimal policies in LfPF problems assume that both the preferences and transition dynamics are determined by a Markov Decision Process. We introduce the Direct Preference Process, a new framework for analyzing LfPF problems in partially-observable, non-Markovian environments. Within this framework, we establish conditions that guarantee the existence of optimal policies by considering the ordinal structure of the preferences. Using the von Neumann-Morgenstern Expected Utility Theorem, we show that the Direct Preference Process generalizes the standard reinforcement learning problem. Our findings narrow the gap between the empirical success and theoretical understanding of LfPF algorithms and provi",
    "path": "papers/23/11/2311.01990.json",
    "total_tokens": 924,
    "translated_title": "条件对偏好关系进行限制以保证最优策略的存在",
    "translated_abstract": "学习偏好反馈 (LfPF) 在训练大型语言模型和某些类型的交互式学习代理中起着至关重要的作用。然而，LfPF算法的理论和应用之间存在着实质性的差距。目前保证在LfPF问题中存在最优策略的结果假设偏好和转移动态都由马尔可夫决策过程确定。我们引入了直接偏好过程，这是一种在部分可观察、非马尔可夫环境中分析LfPF问题的新框架。在这个框架中，我们通过考虑偏好的序结构建立了保证最优策略存在的条件。利用冯·诺依曼-摩根斯特恩期望效用定理，我们表明直接偏好过程推广了标准强化学习问题。我们的研究结果缩小了LfPF算法在经验成功和理论理解之间的差距，并提供了最优策略存在的证明。",
    "tldr": "我们引入了直接偏好过程框架，通过对偏好的序结构进行分析，我们提出了保证最优策略存在的条件。这个研究缩小了学习偏好反馈算法在理论和实践之间的差距，并提供了最优策略存在的证明。"
}