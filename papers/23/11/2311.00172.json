{
    "title": "Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield. (arXiv:2311.00172v1 [cs.CL])",
    "abstract": "Large Language Models' safety remains a critical concern due to their vulnerability to adversarial attacks, which can prompt these systems to produce harmful responses. In the heart of these systems lies a safety classifier, a computational model trained to discern and mitigate potentially harmful, offensive, or unethical outputs. However, contemporary safety classifiers, despite their potential, often fail when exposed to inputs infused with adversarial noise. In response, our study introduces the Adversarial Prompt Shield (APS), a lightweight model that excels in detection accuracy and demonstrates resilience against adversarial prompts. Additionally, we propose novel strategies for autonomously generating adversarial training datasets, named Bot Adversarial Noisy Dialogue (BAND) datasets. These datasets are designed to fortify the safety classifier's robustness, and we investigate the consequences of incorporating adversarial examples into the training process. Through evaluations i",
    "link": "http://arxiv.org/abs/2311.00172",
    "context": "Title: Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield. (arXiv:2311.00172v1 [cs.CL])\nAbstract: Large Language Models' safety remains a critical concern due to their vulnerability to adversarial attacks, which can prompt these systems to produce harmful responses. In the heart of these systems lies a safety classifier, a computational model trained to discern and mitigate potentially harmful, offensive, or unethical outputs. However, contemporary safety classifiers, despite their potential, often fail when exposed to inputs infused with adversarial noise. In response, our study introduces the Adversarial Prompt Shield (APS), a lightweight model that excels in detection accuracy and demonstrates resilience against adversarial prompts. Additionally, we propose novel strategies for autonomously generating adversarial training datasets, named Bot Adversarial Noisy Dialogue (BAND) datasets. These datasets are designed to fortify the safety classifier's robustness, and we investigate the consequences of incorporating adversarial examples into the training process. Through evaluations i",
    "path": "papers/23/11/2311.00172.json",
    "total_tokens": 880,
    "translated_title": "大型语言模型的鲁棒安全分类器：对抗性提示屏蔽",
    "translated_abstract": "大型语言模型的安全性仍然是一个重要关注点，因为它们容易受到对抗性攻击的影响，这可能导致这些系统产生有害的回应。在这些系统的核心是一个安全分类器，这是一个计算模型，被训练来辨别和减轻潜在的有害、冒犯或不道德的输出。然而，尽管潜力巨大，现代的安全分类器通常在暴露于充满对抗性噪声的输入时失败。为此，我们的研究引入了对抗性提示屏蔽（APS），这是一个轻量级模型，在检测准确性方面表现出色，并展示了对抗性提示的韧性。此外，我们提出了自动生成对抗性训练数据集的新策略，称为Bot Adversarial Noisy Dialogue (BAND) 数据集。这些数据集旨在增强安全分类器的鲁棒性，并研究了将对抗性样本纳入训练过程的后果。通过评估实验证明...",
    "tldr": "本研究介绍了对抗性提示屏蔽（APS）模型以及自动生成对抗性训练数据集的新策略。APS模型在检测准确性方面表现出色且具有韧性，并且BAND数据集可以增强安全分类器的鲁棒性。"
}