{
    "title": "Conversation Understanding using Relational Temporal Graph Neural Networks with Auxiliary Cross-Modality Interaction. (arXiv:2311.04507v2 [cs.CL] UPDATED)",
    "abstract": "Emotion recognition is a crucial task for human conversation understanding. It becomes more challenging with the notion of multimodal data, e.g., language, voice, and facial expressions. As a typical solution, the global- and the local context information are exploited to predict the emotional label for every single sentence, i.e., utterance, in the dialogue. Specifically, the global representation could be captured via modeling of cross-modal interactions at the conversation level. The local one is often inferred using the temporal information of speakers or emotional shifts, which neglects vital factors at the utterance level. Additionally, most existing approaches take fused features of multiple modalities in an unified input without leveraging modality-specific representations. Motivating from these problems, we propose the Relational Temporal Graph Neural Network with Auxiliary Cross-Modality Interaction (CORECT), an novel neural network framework that effectively captures convers",
    "link": "http://arxiv.org/abs/2311.04507",
    "context": "Title: Conversation Understanding using Relational Temporal Graph Neural Networks with Auxiliary Cross-Modality Interaction. (arXiv:2311.04507v2 [cs.CL] UPDATED)\nAbstract: Emotion recognition is a crucial task for human conversation understanding. It becomes more challenging with the notion of multimodal data, e.g., language, voice, and facial expressions. As a typical solution, the global- and the local context information are exploited to predict the emotional label for every single sentence, i.e., utterance, in the dialogue. Specifically, the global representation could be captured via modeling of cross-modal interactions at the conversation level. The local one is often inferred using the temporal information of speakers or emotional shifts, which neglects vital factors at the utterance level. Additionally, most existing approaches take fused features of multiple modalities in an unified input without leveraging modality-specific representations. Motivating from these problems, we propose the Relational Temporal Graph Neural Network with Auxiliary Cross-Modality Interaction (CORECT), an novel neural network framework that effectively captures convers",
    "path": "papers/23/11/2311.04507.json",
    "total_tokens": 836,
    "translated_title": "使用具有辅助跨模态交互的关系时态图神经网络进行对话理解",
    "translated_abstract": "情绪识别是人类对话理解的一个关键任务，随着多模态数据的引入，如语言、声音和面部表情，这变得更具挑战性。作为一种典型的解决方案，利用全局和局部上下文信息来预测对话中每个句子（即话语）的情绪标签。具体而言，全局表示可以通过建模对话级别的跨模态交互来捕捉。局部表示通常是通过说话者或情绪变化的时间信息来推断的，忽视了话语级别的重要因素。此外，大多数现有方法采用统一输入的多模态融合特征，而不利用模态特定的表示。为了解决这些问题，我们提出了关系时态图神经网络与辅助跨模态交互（CORECT），这是一个有效捕捉对话中情感信息的神经网络框架。",
    "tldr": "本论文提出了一个名为CORECT的神经网络框架，通过关系时态图神经网络和辅助跨模态交互的方式有效地捕捉对话中的情感信息。"
}