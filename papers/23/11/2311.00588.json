{
    "title": "Boosting Summarization with Normalizing Flows and Aggressive Training. (arXiv:2311.00588v1 [cs.CL])",
    "abstract": "This paper presents FlowSUM, a normalizing flows-based variational encoder-decoder framework for Transformer-based summarization. Our approach tackles two primary challenges in variational summarization: insufficient semantic information in latent representations and posterior collapse during training. To address these challenges, we employ normalizing flows to enable flexible latent posterior modeling, and we propose a controlled alternate aggressive training (CAAT) strategy with an improved gate mechanism. Experimental results show that FlowSUM significantly enhances the quality of generated summaries and unleashes the potential for knowledge distillation with minimal impact on inference time. Furthermore, we investigate the issue of posterior collapse in normalizing flows and analyze how the summary quality is affected by the training strategy, gate initialization, and the type and number of normalizing flows used, offering valuable insights for future research.",
    "link": "http://arxiv.org/abs/2311.00588",
    "context": "Title: Boosting Summarization with Normalizing Flows and Aggressive Training. (arXiv:2311.00588v1 [cs.CL])\nAbstract: This paper presents FlowSUM, a normalizing flows-based variational encoder-decoder framework for Transformer-based summarization. Our approach tackles two primary challenges in variational summarization: insufficient semantic information in latent representations and posterior collapse during training. To address these challenges, we employ normalizing flows to enable flexible latent posterior modeling, and we propose a controlled alternate aggressive training (CAAT) strategy with an improved gate mechanism. Experimental results show that FlowSUM significantly enhances the quality of generated summaries and unleashes the potential for knowledge distillation with minimal impact on inference time. Furthermore, we investigate the issue of posterior collapse in normalizing flows and analyze how the summary quality is affected by the training strategy, gate initialization, and the type and number of normalizing flows used, offering valuable insights for future research.",
    "path": "papers/23/11/2311.00588.json",
    "total_tokens": 973,
    "translated_title": "使用正则化流和激进训练提升摘要生成",
    "translated_abstract": "本文提出了基于正则化流的变分编码器-解码器框架FlowSUM，用于基于Transformer的摘要生成。我们的方法解决了变分摘要生成中的两个主要挑战：潜在表示中的语义信息不足和训练过程中的后向塌陷。为了应对这些挑战，我们使用正则化流实现了灵活的潜在后向建模，并提出了一种改进的门机制下的受控交替激进训练（CAAT）策略。实验结果表明，FlowSUM显著提高了生成摘要的质量，并在对推理时间的影响最小的情况下释放了知识蒸馏的潜力。此外，我们还研究了正则化流中的后向塌陷问题，并分析了训练策略、门初始化以及使用的正则化流类型和数量对摘要质量的影响，为未来研究提供了宝贵的见解。",
    "tldr": "本文提出了FlowSUM，一个基于正则化流的变分编码器-解码器框架，用于改进Transformer-based摘要生成。通过利用正则化流进行灵活的潜在后向建模以及采用受控交替激进训练策略，FlowSUM显著提高了生成摘要的质量，并探讨了正则化流中的后向塌陷问题和相关影响因素，为相关研究提供了宝贵的洞察。",
    "en_tdlr": "This paper presents FlowSUM, a normalizing flows-based variational encoder-decoder framework for improving Transformer-based summarization. By utilizing normalizing flows for flexible latent posterior modeling and employing a controlled alternate aggressive training strategy, FlowSUM significantly enhances the quality of generated summaries, and investigates the issue of posterior collapse in normalizing flows and related influencing factors, providing valuable insights for further research."
}