{
    "title": "Learning to Learn for Few-shot Continual Active Learning",
    "abstract": "arXiv:2311.03732v2 Announce Type: replace-cross  Abstract: Continual learning strives to ensure stability in solving previously seen tasks while demonstrating plasticity in a novel domain. Recent advances in CL are mostly confined to a supervised learning setting, especially in NLP domain. In this work, we consider a few-shot continual active learning (CAL) setting where labeled data are inadequate, and unlabeled data are abundant but with a limited annotation budget. We propose a simple but efficient method, called Meta-Continual Active Learning. Specifically, we employ meta-learning and experience replay to address inter-task confusion and catastrophic forgetting. We further incorporate textual augmentations to ensure generalization. We conduct extensive experiments on benchmark text classification datasets to validate the effectiveness of the proposed method and analyze the effect of different active learning strategies in few-shot CAL setting. Our experimental results demonstrate t",
    "link": "https://arxiv.org/abs/2311.03732",
    "context": "Title: Learning to Learn for Few-shot Continual Active Learning\nAbstract: arXiv:2311.03732v2 Announce Type: replace-cross  Abstract: Continual learning strives to ensure stability in solving previously seen tasks while demonstrating plasticity in a novel domain. Recent advances in CL are mostly confined to a supervised learning setting, especially in NLP domain. In this work, we consider a few-shot continual active learning (CAL) setting where labeled data are inadequate, and unlabeled data are abundant but with a limited annotation budget. We propose a simple but efficient method, called Meta-Continual Active Learning. Specifically, we employ meta-learning and experience replay to address inter-task confusion and catastrophic forgetting. We further incorporate textual augmentations to ensure generalization. We conduct extensive experiments on benchmark text classification datasets to validate the effectiveness of the proposed method and analyze the effect of different active learning strategies in few-shot CAL setting. Our experimental results demonstrate t",
    "path": "papers/23/11/2311.03732.json",
    "total_tokens": 851,
    "translated_title": "学习学习以进行少样本持久主动学习",
    "translated_abstract": "持续学习旨在确保解决先前见过的任务的稳定性，同时展示对新领域的可塑性。最近在持续学习方面的进展主要局限于监督学习设置，尤其是在自然语言处理领域。在这项工作中，我们考虑了一种少样本持续主动学习（CAL）设置，其中标记数据不足，但未标记数据充足，但有限的注释预算。我们提出了一种简单而高效的方法，称为元持续主动学习。具体地，我们采用元学习和经验重播来解决任务之间的混淆和灾难性遗忘。我们进一步结合文本增强来确保泛化性能。我们在基准文本分类数据集上进行了大量实验，以验证所提方法的有效性，并分析了在少样本CAL设置中不同主动学习策略的影响。我们的实验结果表明",
    "tldr": "提出了一种简单而高效的方法，Meta-Continual Active Learning，通过元学习和经验重播解决少样本持续主动学习中的任务混淆和灾难性遗忘，进一步结合文本增强来确保泛化。"
}