{
    "title": "Low-Rank MDPs with Continuous Action Spaces",
    "abstract": "arXiv:2311.03564v2 Announce Type: replace-cross  Abstract: Low-Rank Markov Decision Processes (MDPs) have recently emerged as a promising framework within the domain of reinforcement learning (RL), as they allow for provably approximately correct (PAC) learning guarantees while also incorporating ML algorithms for representation learning. However, current methods for low-rank MDPs are limited in that they only consider finite action spaces, and give vacuous bounds as $|\\mathcal{A}| \\to \\infty$, which greatly limits their applicability. In this work, we study the problem of extending such methods to settings with continuous actions, and explore multiple concrete approaches for performing this extension. As a case study, we consider the seminal FLAMBE algorithm (Agarwal et al., 2020), which is a reward-agnostic method for PAC RL with low-rank MDPs. We show that, without any modifications to the algorithm, we obtain a similar PAC bound when actions are allowed to be continuous. Specifical",
    "link": "https://arxiv.org/abs/2311.03564",
    "context": "Title: Low-Rank MDPs with Continuous Action Spaces\nAbstract: arXiv:2311.03564v2 Announce Type: replace-cross  Abstract: Low-Rank Markov Decision Processes (MDPs) have recently emerged as a promising framework within the domain of reinforcement learning (RL), as they allow for provably approximately correct (PAC) learning guarantees while also incorporating ML algorithms for representation learning. However, current methods for low-rank MDPs are limited in that they only consider finite action spaces, and give vacuous bounds as $|\\mathcal{A}| \\to \\infty$, which greatly limits their applicability. In this work, we study the problem of extending such methods to settings with continuous actions, and explore multiple concrete approaches for performing this extension. As a case study, we consider the seminal FLAMBE algorithm (Agarwal et al., 2020), which is a reward-agnostic method for PAC RL with low-rank MDPs. We show that, without any modifications to the algorithm, we obtain a similar PAC bound when actions are allowed to be continuous. Specifical",
    "path": "papers/23/11/2311.03564.json",
    "total_tokens": 886,
    "translated_title": "具有连续动作空间的低秩马尔可夫决策过程",
    "translated_abstract": "低秩马尔可夫决策过程（MDPs）最近在强化学习（RL）领域中崭露头角，因为它们可以提供约等于正确（PAC）的学习保证，同时还可以融合ML算法进行表示学习。然而，当前对低秩MDPs的方法存在局限性，它们只考虑有限的动作空间，并且在动作数量$|\\mathcal{A}| \\to \\infty$时给出了空洞的界限，这极大地限制了它们的适用性。在这项工作中，我们研究了将这些方法扩展到具有连续动作的设置的问题，并探讨了多种具体的方法来进行这种扩展。作为案例研究，我们考虑了FLAMBE算法（Agarwal等，2020），这是一种用于低秩MDPs的PAC RL的奖励无关方法。我们展示了，在不对算法进行任何修改的情况下，当允许动作为连续时，我们获得了类似的PAC界限。",
    "tldr": "该研究通过探索多种方法，将针对低秩MDPs的现有方法扩展到连续动作空间，同时保持近似正确的学习保证。",
    "en_tdlr": "This study extends existing methods for low-rank MDPs to continuous action spaces by exploring multiple approaches while maintaining approximately correct learning guarantees."
}