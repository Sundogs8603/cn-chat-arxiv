{
    "title": "Leveraging Function Space Aggregation for Federated Learning at Scale",
    "abstract": "arXiv:2311.10291v2 Announce Type: replace  Abstract: The federated learning paradigm has motivated the development of methods for aggregating multiple client updates into a global server model, without sharing client data. Many federated learning algorithms, including the canonical Federated Averaging (FedAvg), take a direct (possibly weighted) average of the client parameter updates, motivated by results in distributed optimization. In this work, we adopt a function space perspective and propose a new algorithm, FedFish, that aggregates local approximations to the functions learned by clients, using an estimate based on their Fisher information. We evaluate FedFish on realistic, large-scale cross-device benchmarks. While the performance of FedAvg can suffer as client models drift further apart, we demonstrate that FedFish is more robust to longer local training. Our evaluation across several settings in image and language benchmarks shows that FedFish outperforms FedAvg as local train",
    "link": "https://arxiv.org/abs/2311.10291",
    "context": "Title: Leveraging Function Space Aggregation for Federated Learning at Scale\nAbstract: arXiv:2311.10291v2 Announce Type: replace  Abstract: The federated learning paradigm has motivated the development of methods for aggregating multiple client updates into a global server model, without sharing client data. Many federated learning algorithms, including the canonical Federated Averaging (FedAvg), take a direct (possibly weighted) average of the client parameter updates, motivated by results in distributed optimization. In this work, we adopt a function space perspective and propose a new algorithm, FedFish, that aggregates local approximations to the functions learned by clients, using an estimate based on their Fisher information. We evaluate FedFish on realistic, large-scale cross-device benchmarks. While the performance of FedAvg can suffer as client models drift further apart, we demonstrate that FedFish is more robust to longer local training. Our evaluation across several settings in image and language benchmarks shows that FedFish outperforms FedAvg as local train",
    "path": "papers/23/11/2311.10291.json",
    "total_tokens": 891,
    "translated_title": "利用函数空间聚合进行大规模联邦学习",
    "translated_abstract": "联邦学习范式激发了将多个客户端更新聚合到全局服务器模型中的方法的发展，而无需共享客户端数据。许多联邦学习算法，包括经典的联邦平均（FedAvg），采用了对客户端参数更新的直接（可能加权）平均值，这是基于分布式优化结果的动机。在这项工作中，我们采用了函数空间角度，并提出了一种新算法FedFish，它聚合了客户端学习到的函数的局部近似，使用基于费舍尔信息的估计。我们在实际的大规模跨设备基准上评估了FedFish。虽然当客户端模型漂离时FedAvg的性能可能会受到影响，但我们证明FedFish对更长的局部训练更具鲁棒性。我们对图像和语言基准测试中的几个设置进行评估，结果显示FedFish在局部训练中优于FedAvg。",
    "tldr": "提出一种新算法FedFish，通过利用客户端学习函数的局部近似，并使用基于费舍尔信息的估计，实现了对联邦学习中的函数空间聚合，并在大规模交叉设备基准测试中表现出更好的鲁棒性和性能。",
    "en_tdlr": "Introducing a new algorithm, FedFish, that leverages local approximations to client-learned functions with estimates based on Fisher information for function space aggregation in federated learning, showing improved robustness and performance in large-scale cross-device benchmarks compared to FedAvg."
}