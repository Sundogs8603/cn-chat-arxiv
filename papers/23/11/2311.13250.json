{
    "title": "FedHCA$^2$: Towards Hetero-Client Federated Multi-Task Learning",
    "abstract": "arXiv:2311.13250v2 Announce Type: replace-cross  Abstract: Federated Learning (FL) enables joint training across distributed clients using their local data privately. Federated Multi-Task Learning (FMTL) builds on FL to handle multiple tasks, assuming model congruity that identical model architecture is deployed in each client. To relax this assumption and thus extend real-world applicability, we introduce a novel problem setting, Hetero-Client Federated Multi-Task Learning (HC-FMTL), to accommodate diverse task setups. The main challenge of HC-FMTL is the model incongruity issue that invalidates conventional aggregation methods. It also escalates the difficulties in accurate model aggregation to deal with data and task heterogeneity inherent in FMTL. To address these challenges, we propose the FedHCA$^2$ framework, which allows for federated training of personalized models by modeling relationships among heterogeneous clients. Drawing on our theoretical insights into the difference be",
    "link": "https://arxiv.org/abs/2311.13250",
    "context": "Title: FedHCA$^2$: Towards Hetero-Client Federated Multi-Task Learning\nAbstract: arXiv:2311.13250v2 Announce Type: replace-cross  Abstract: Federated Learning (FL) enables joint training across distributed clients using their local data privately. Federated Multi-Task Learning (FMTL) builds on FL to handle multiple tasks, assuming model congruity that identical model architecture is deployed in each client. To relax this assumption and thus extend real-world applicability, we introduce a novel problem setting, Hetero-Client Federated Multi-Task Learning (HC-FMTL), to accommodate diverse task setups. The main challenge of HC-FMTL is the model incongruity issue that invalidates conventional aggregation methods. It also escalates the difficulties in accurate model aggregation to deal with data and task heterogeneity inherent in FMTL. To address these challenges, we propose the FedHCA$^2$ framework, which allows for federated training of personalized models by modeling relationships among heterogeneous clients. Drawing on our theoretical insights into the difference be",
    "path": "papers/23/11/2311.13250.json",
    "total_tokens": 913,
    "translated_title": "FedHCA$^2$: 面向异构客户联邦多任务学习",
    "translated_abstract": "联邦学习（FL）通过使用客户端的本地数据进行联合训练，从而实现了分布式客户端之间的联合训练。联邦多任务学习（FMTL）建立在FL基础上，用于处理多个任务，假设模型的一致性，即在每个客户端部署相同的模型架构。为了放宽这一假设，从而扩展现实世界的适用性，我们引入了一个新颖的问题设置，即面向异构客户的联邦多任务学习（HC-FMTL），以适应多样的任务设置。HC-FMTL的主要挑战是模型不一致问题，这使传统聚合方法失效。这也使得准确模型聚合以处理FMTL中固有的数据和任务异质性的困难加剧。为了解决这些挑战，我们提出了FedHCA$^2$框架，它允许通过对异构客户之间的关系进行建模来进行个性化模型的联邦训练。根据我们对客户端不同之处的理论见解，我们发现",
    "tldr": "本文介绍了FedHCA$^2$框架，旨在解决异构客户联邦多任务学习中的模型不一致问题，实现个性化模型联邦训练。",
    "en_tdlr": "This paper presents the FedHCA$^2$ framework to address the model incongruity issue in Hetero-Client Federated Multi-Task Learning, enabling federated training of personalized models."
}