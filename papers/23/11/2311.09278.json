{
    "title": "Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models",
    "abstract": "arXiv:2311.09278v2 Announce Type: replace-cross  Abstract: Although Large Language Models (LLMs) demonstrate remarkable ability in processing and generating human-like text, they do have limitations when it comes to comprehending and expressing world knowledge that extends beyond the boundaries of natural language(e.g., chemical molecular formula). Injecting a collection of symbolic data directly into the training of LLMs can be problematic, as it disregards the synergies among different symbolic families and overlooks the need for a balanced mixture of natural and symbolic data. In this work, we tackle these challenges from both a data and framework perspective and introduce Symbol-LLM series models. First, we curated a data collection consisting of 34 tasks and incorporating approximately 20 distinct symbolic families, intending to capture the interrelations and foster synergies between symbols. Then, a two-stage tuning framework succeeds in injecting symbolic knowledge without loss ",
    "link": "https://arxiv.org/abs/2311.09278",
    "context": "Title: Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models\nAbstract: arXiv:2311.09278v2 Announce Type: replace-cross  Abstract: Although Large Language Models (LLMs) demonstrate remarkable ability in processing and generating human-like text, they do have limitations when it comes to comprehending and expressing world knowledge that extends beyond the boundaries of natural language(e.g., chemical molecular formula). Injecting a collection of symbolic data directly into the training of LLMs can be problematic, as it disregards the synergies among different symbolic families and overlooks the need for a balanced mixture of natural and symbolic data. In this work, we tackle these challenges from both a data and framework perspective and introduce Symbol-LLM series models. First, we curated a data collection consisting of 34 tasks and incorporating approximately 20 distinct symbolic families, intending to capture the interrelations and foster synergies between symbols. Then, a two-stage tuning framework succeeds in injecting symbolic knowledge without loss ",
    "path": "papers/23/11/2311.09278.json",
    "total_tokens": 844,
    "translated_title": "Symbol-LLM: 面向大型语言模型基础符号中心接口",
    "translated_abstract": "虽然大型语言模型(LLMs)展现出在处理和生成类似于人类文本方面的显著能力，但在理解和表达超出自然语言范围的世界知识方面存在局限性(例如化学分子式)。直接将一系列符号数据注入到LLMs的训练中可能存在问题，因为它忽视了不同符号家族之间的协同关系，也忽视了自然数据和符号数据之间平衡混合的必要性。在这项工作中，我们从数据和框架两个方面应对这些挑战，并引入了Symbol-LLM系列模型。首先，我们策划了一个包含34个任务并涵盖约20个不同符号家族的数据集，旨在捕捉符号之间的相互关系并促进符号之间的协同作用。然后，一个两阶段调优框架成功地注入了符号知识而不会损失",
    "tldr": "Symbol-LLM 提出了一种通过数据和框架来解决大型语言模型中符号数据注入的挑战，旨在捕捉符号间的相互关系和促进协同作用。",
    "en_tdlr": "Symbol-LLM introduces a method to address the challenge of injecting symbolic data into large language models through data and framework, aiming to capture interrelations between symbols and foster synergies."
}