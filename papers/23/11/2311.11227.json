{
    "title": "FedRA: A Random Allocation Strategy for Federated Tuning to Unleash the Power of Heterogeneous Clients",
    "abstract": "arXiv:2311.11227v2 Announce Type: replace-cross  Abstract: With the increasing availability of Foundation Models, federated tuning has garnered attention in the field of federated learning, utilizing data and computation resources from multiple clients to collaboratively fine-tune foundation models. However, in real-world federated scenarios, there often exist a multitude of heterogeneous clients with varying computation and communication resources, rendering them incapable of supporting the entire model fine-tuning process. In response to this challenge, we propose a novel federated tuning algorithm, FedRA. The implementation of FedRA is straightforward and can be seamlessly integrated into any transformer-based model without the need for further modification to the original model. Specifically, in each communication round, FedRA randomly generates an allocation matrix. For resource-constrained clients, it reorganizes a small number of layers from the original model based on the alloc",
    "link": "https://arxiv.org/abs/2311.11227",
    "context": "Title: FedRA: A Random Allocation Strategy for Federated Tuning to Unleash the Power of Heterogeneous Clients\nAbstract: arXiv:2311.11227v2 Announce Type: replace-cross  Abstract: With the increasing availability of Foundation Models, federated tuning has garnered attention in the field of federated learning, utilizing data and computation resources from multiple clients to collaboratively fine-tune foundation models. However, in real-world federated scenarios, there often exist a multitude of heterogeneous clients with varying computation and communication resources, rendering them incapable of supporting the entire model fine-tuning process. In response to this challenge, we propose a novel federated tuning algorithm, FedRA. The implementation of FedRA is straightforward and can be seamlessly integrated into any transformer-based model without the need for further modification to the original model. Specifically, in each communication round, FedRA randomly generates an allocation matrix. For resource-constrained clients, it reorganizes a small number of layers from the original model based on the alloc",
    "path": "papers/23/11/2311.11227.json",
    "total_tokens": 824,
    "translated_title": "FedRA:一种用于释放异构客户端强大潜力的随机分配策略",
    "translated_abstract": "随着基础模型的日益可用，联邦调优在联邦学习领域引起了关注，利用多个客户端的数据和计算资源共同对基础模型进行微调。然而，在现实世界的联邦场景中，通常存在大量具有不同计算和通信资源的异构客户端，导致它们无法支持整个模型的微调过程。针对这一挑战，我们提出了一种新颖的联邦调优算法FedRA。FedRA的实施简单，可以无缝集成到任何基于Transformer的模型中，无需对原模型进行进一步修改。具体而言，在每一轮通信中，FedRA会随机生成一个分配矩阵。对于资源受限的客户端，它会根据分配情况重新组织原模型中的少量层。",
    "tldr": "提出了一种名为FedRA的联邦调优算法，可以随机生成分配矩阵来应对拥有不同计算和通信资源的异构客户端，在不需要修改原模型的情况下进行微调。",
    "en_tdlr": "Proposed a federated tuning algorithm called FedRA, which generates allocation matrices randomly to address heterogeneous clients with varying computation and communication resources, enabling fine-tuning without modifying the original model."
}