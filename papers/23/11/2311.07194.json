{
    "title": "Exploring the Factual Consistency in Dialogue Comprehension of Large Language Models",
    "abstract": "arXiv:2311.07194v3 Announce Type: replace  Abstract: LLMs (Large Language Models) usually interact with users in the form of dialogue and generate responses following their instructions, which naturally require dialogue comprehension abilities. However, dialogue comprehension is a general language ability which is hard to be evaluated directly. In this work, we propose to perform the evaluation focusing on the factual consistency issue with the help of the dialogue summarization task. Besides evaluating and analyzing the dialogue summarization performance (DIAC-Sum) of different LLMs, we also derive factual questions from the generated summaries and use them as a more flexible measurement of dialogue comprehension (DIAC-QA). Our evaluation shows that, on average, 26.8% of the summaries generated by LLMs contain factual inconsistency. Even ChatGPT, the strongest model evaluated, has such errors in 16% of its summaries. For answering the factual questions, which is more challenging, the ",
    "link": "https://arxiv.org/abs/2311.07194",
    "context": "Title: Exploring the Factual Consistency in Dialogue Comprehension of Large Language Models\nAbstract: arXiv:2311.07194v3 Announce Type: replace  Abstract: LLMs (Large Language Models) usually interact with users in the form of dialogue and generate responses following their instructions, which naturally require dialogue comprehension abilities. However, dialogue comprehension is a general language ability which is hard to be evaluated directly. In this work, we propose to perform the evaluation focusing on the factual consistency issue with the help of the dialogue summarization task. Besides evaluating and analyzing the dialogue summarization performance (DIAC-Sum) of different LLMs, we also derive factual questions from the generated summaries and use them as a more flexible measurement of dialogue comprehension (DIAC-QA). Our evaluation shows that, on average, 26.8% of the summaries generated by LLMs contain factual inconsistency. Even ChatGPT, the strongest model evaluated, has such errors in 16% of its summaries. For answering the factual questions, which is more challenging, the ",
    "path": "papers/23/11/2311.07194.json",
    "total_tokens": 892,
    "translated_title": "探索大型语言模型对话理解中的事实一致性",
    "translated_abstract": "LLMs（大型语言模型）通常以对话的形式与用户交互，并根据其指令生成回复，这自然需要对话理解能力。然而，对话理解是一种难以直接评估的通用语言能力。本文提出通过对话总结任务的帮助，集中进行对事实一致性问题的评估。除了评估和分析不同LLMs的对话总结性能（DIAC-Sum）外，我们还从生成的摘要中提取事实问题，并将其用作对话理解的更灵活的测量（DIAC-QA）。我们的评估显示，平均而言，由LLMs生成的摘要中有26.8%包含事实不一致。即使是ChatGPT，评估中最强大的模型，在其摘要中也有16%存在此类错误。对于回答事实问题，这更具挑战性。",
    "tldr": "本研究提出了一种评估大型语言模型对话理解能力的方法，通过对话总结任务检测事实一致性问题，并提出用事实问题来作为对话理解能力的灵活衡量标准，结果显示平均约有26.8%的大型语言模型生成的摘要存在事实不一致性。",
    "en_tdlr": "This study proposes a method to evaluate the dialogue comprehension ability of large language models by focusing on factual consistency issue through dialogue summarization task, and introduces factual questions as a flexible metric for dialogue comprehension, revealing that on average around 26.8% of the summaries generated by large language models contain factual inconsistency."
}