{
    "title": "Loss Masking Is Not Needed in Decoder-only Transformer for Discrete-token-based ASR",
    "abstract": "Recently, unified speech-text models, such as SpeechGPT, VioLA, and AudioPaLM, have achieved remarkable performance on various speech tasks. These models discretize speech signals into tokens (speech discretization) and use a shared vocabulary for both text and speech tokens. Then they train a single decoder-only Transformer on a mixture of speech tasks. However, these models rely on the Loss Masking strategy for the ASR task, which ignores the dependency among speech tokens. In this paper, we propose to model speech tokens in an autoregressive way, similar to text. We find that applying the conventional cross-entropy loss on input speech tokens does not consistently improve the ASR performance over the Loss Masking approach. To address this issue, we propose a novel approach denoted Smoothed Label Distillation (SLD), which applies a KL divergence loss with smoothed labels on speech tokens. Our experiments show that SLD effectively models speech tokens and outperforms Loss Masking for ",
    "link": "https://arxiv.org/abs/2311.04534",
    "context": "Title: Loss Masking Is Not Needed in Decoder-only Transformer for Discrete-token-based ASR\nAbstract: Recently, unified speech-text models, such as SpeechGPT, VioLA, and AudioPaLM, have achieved remarkable performance on various speech tasks. These models discretize speech signals into tokens (speech discretization) and use a shared vocabulary for both text and speech tokens. Then they train a single decoder-only Transformer on a mixture of speech tasks. However, these models rely on the Loss Masking strategy for the ASR task, which ignores the dependency among speech tokens. In this paper, we propose to model speech tokens in an autoregressive way, similar to text. We find that applying the conventional cross-entropy loss on input speech tokens does not consistently improve the ASR performance over the Loss Masking approach. To address this issue, we propose a novel approach denoted Smoothed Label Distillation (SLD), which applies a KL divergence loss with smoothed labels on speech tokens. Our experiments show that SLD effectively models speech tokens and outperforms Loss Masking for ",
    "path": "papers/23/11/2311.04534.json",
    "total_tokens": 1025,
    "translated_title": "在仅使用编码器的变压器模型中，对离散化的令牌ASR不需要使用损失遮蔽",
    "translated_abstract": "最近，统一的语音-文本模型，例如SpeechGPT、VioLA和AudioPaLM，在各种语音任务上取得了显著的性能。这些模型将语音信号离散化为令牌（语音离散化），并对文本和语音令牌使用共享词汇表。然后，在混合语音任务上训练单个只有解码器的变压器。然而，这些模型依赖于ASR任务中的损失遮蔽策略，该策略忽略语音令牌之间的依赖关系。在本文中，我们提出以类似于文本的自回归方式对语音令牌进行建模。我们发现，对输入的语音令牌应用传统的交叉熵损失并不能始终改善ASR性能，相比之下，损失遮蔽方法更有效。为了解决这个问题，我们提出了一种新的方法，名为平滑标签蒸馏（SLD），它在语音令牌上应用了带有平滑标签的KL散度损失。我们的实验证明，SLD有效地对语音令牌进行建模，并胜过了损失遮蔽方法。",
    "tldr": "在离散化令牌的ASR中，仅使用解码器的Transformer模型不需要使用损失遮蔽。取而代之的是，我们提出了一种名为平滑标签蒸馏的新方法，它在语音令牌上应用了带有平滑标签的KL散度损失，并且在实验证明效果优于损失遮蔽方法。"
}