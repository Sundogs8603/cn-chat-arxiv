{
    "title": "The Power of the Senses: Generalizable Manipulation from Vision and Touch through Masked Multimodal Learning. (arXiv:2311.00924v1 [cs.RO])",
    "abstract": "Humans rely on the synergy of their senses for most essential tasks. For tasks requiring object manipulation, we seamlessly and effectively exploit the complementarity of our senses of vision and touch. This paper draws inspiration from such capabilities and aims to find a systematic approach to fuse visual and tactile information in a reinforcement learning setting. We propose Masked Multimodal Learning (M3L), which jointly learns a policy and visual-tactile representations based on masked autoencoding. The representations jointly learned from vision and touch improve sample efficiency, and unlock generalization capabilities beyond those achievable through each of the senses separately. Remarkably, representations learned in a multimodal setting also benefit vision-only policies at test time. We evaluate M3L on three simulated environments with both visual and tactile observations: robotic insertion, door opening, and dexterous in-hand manipulation, demonstrating the benefits of learn",
    "link": "http://arxiv.org/abs/2311.00924",
    "context": "Title: The Power of the Senses: Generalizable Manipulation from Vision and Touch through Masked Multimodal Learning. (arXiv:2311.00924v1 [cs.RO])\nAbstract: Humans rely on the synergy of their senses for most essential tasks. For tasks requiring object manipulation, we seamlessly and effectively exploit the complementarity of our senses of vision and touch. This paper draws inspiration from such capabilities and aims to find a systematic approach to fuse visual and tactile information in a reinforcement learning setting. We propose Masked Multimodal Learning (M3L), which jointly learns a policy and visual-tactile representations based on masked autoencoding. The representations jointly learned from vision and touch improve sample efficiency, and unlock generalization capabilities beyond those achievable through each of the senses separately. Remarkably, representations learned in a multimodal setting also benefit vision-only policies at test time. We evaluate M3L on three simulated environments with both visual and tactile observations: robotic insertion, door opening, and dexterous in-hand manipulation, demonstrating the benefits of learn",
    "path": "papers/23/11/2311.00924.json",
    "total_tokens": 922,
    "translated_title": "视觉和触觉的融合学习：基于屏蔽多模态学习的通用化操控能力",
    "translated_abstract": "人类在大部分基本任务中依靠感官的协同作用。对于需要物体操纵的任务，我们无缝且有效地利用视觉和触觉的互补性。本文受到这种能力的启发，旨在找到一种系统的方法来在强化学习环境中融合视觉和触觉信息。我们提出了屏蔽多模态学习（M3L），通过屏蔽自动编码，同时学习策略和视觉-触觉表述。从视觉和触觉中共同学习的表述提高了样本效率，并且展现了超越单一感官单独实现的泛化能力。值得注意的是，多模态环境中学习的表述也对测试时的仅视觉策略有益。我们在三个模拟环境（机器人插入、开门和灵巧的手上操作）中评估了M3L，展示了学习的益处。",
    "tldr": "本文基于屏蔽多模态学习方法提出了一种在强化学习中融合视觉和触觉信息的系统方法，实现了超越单一感官的通用化操控能力，并且这种多模态学习对于仅视觉策略也具有好处。"
}