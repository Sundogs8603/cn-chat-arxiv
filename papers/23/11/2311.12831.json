{
    "title": "ECNR: Efficient Compressive Neural Representation of Time-Varying Volumetric Datasets",
    "abstract": "Due to its conceptual simplicity and generality, compressive neural representation has emerged as a promising alternative to traditional compression methods for managing massive volumetric datasets. The current practice of neural compression utilizes a single large multilayer perceptron (MLP) to encode the global volume, incurring slow training and inference. This paper presents an efficient compressive neural representation (ECNR) solution for time-varying data compression, utilizing the Laplacian pyramid for adaptive signal fitting. Following a multiscale structure, we leverage multiple small MLPs at each scale for fitting local content or residual blocks. By assigning similar blocks to the same MLP via size uniformization, we enable balanced parallelization among MLPs to significantly speed up training and inference. Working in concert with the multiscale structure, we tailor a deep compression strategy to compact the resulting model. We show the effectiveness of ECNR with multiple ",
    "link": "https://arxiv.org/abs/2311.12831",
    "context": "Title: ECNR: Efficient Compressive Neural Representation of Time-Varying Volumetric Datasets\nAbstract: Due to its conceptual simplicity and generality, compressive neural representation has emerged as a promising alternative to traditional compression methods for managing massive volumetric datasets. The current practice of neural compression utilizes a single large multilayer perceptron (MLP) to encode the global volume, incurring slow training and inference. This paper presents an efficient compressive neural representation (ECNR) solution for time-varying data compression, utilizing the Laplacian pyramid for adaptive signal fitting. Following a multiscale structure, we leverage multiple small MLPs at each scale for fitting local content or residual blocks. By assigning similar blocks to the same MLP via size uniformization, we enable balanced parallelization among MLPs to significantly speed up training and inference. Working in concert with the multiscale structure, we tailor a deep compression strategy to compact the resulting model. We show the effectiveness of ECNR with multiple ",
    "path": "papers/23/11/2311.12831.json",
    "total_tokens": 899,
    "translated_title": "ECNR: 高效压缩神经表示的时变体积数据",
    "translated_abstract": "由于其概念简单性和广泛适用性，压缩神经表示已经成为管理大规模体积数据集的传统压缩方法的有希望的替代方案。当前的神经压缩实践利用单个大型多层感知器（MLP）来对全局体积进行编码，导致训练和推理速度慢。本文提出了一种用于时变数据压缩的高效压缩神经表示（ECNR）解决方案，利用拉普拉斯金字塔进行自适应信号拟合。遵循多尺度的结构，我们在每个尺度上利用多个小型MLP来拟合本地内容或残差块。通过将相似的块分配给相同大小的MLP，通过大小统一化，我们实现了MLP之间的平衡并行化，从而显著加速训练和推理。与多尺度结构协同工作的是，我们量身定制了一种深度压缩策略来压缩结果模型。我们展示了ECNR的效果，有多个...",
    "tldr": "ECNR是一种针对时变数据的高效压缩神经表示方法，通过使用多尺度结构和多个小型MLP，以及深度压缩策略，可以显著加速训练和推理过程。"
}