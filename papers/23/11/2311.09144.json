{
    "title": "Grounding Gaps in Language Model Generations",
    "abstract": "arXiv:2311.09144v2 Announce Type: replace  Abstract: Effective conversation requires common ground: a shared understanding between the participants. Common ground, however, does not emerge spontaneously in conversation. Speakers and listeners work together to both identify and construct a shared basis while avoiding misunderstanding. To accomplish grounding, humans rely on a range of dialogue acts, like clarification (What do you mean?) and acknowledgment (I understand.). However, it is unclear whether large language models (LLMs) generate text that reflects human grounding. To this end, we curate a set of grounding acts and propose corresponding metrics that quantify attempted grounding. We study whether LLM generations contain grounding acts, simulating turn-taking from several dialogue datasets and comparing results to humans. We find that -- compared to humans -- LLMs generate language with less conversational grounding, instead generating text that appears to simply presume common",
    "link": "https://arxiv.org/abs/2311.09144",
    "context": "Title: Grounding Gaps in Language Model Generations\nAbstract: arXiv:2311.09144v2 Announce Type: replace  Abstract: Effective conversation requires common ground: a shared understanding between the participants. Common ground, however, does not emerge spontaneously in conversation. Speakers and listeners work together to both identify and construct a shared basis while avoiding misunderstanding. To accomplish grounding, humans rely on a range of dialogue acts, like clarification (What do you mean?) and acknowledgment (I understand.). However, it is unclear whether large language models (LLMs) generate text that reflects human grounding. To this end, we curate a set of grounding acts and propose corresponding metrics that quantify attempted grounding. We study whether LLM generations contain grounding acts, simulating turn-taking from several dialogue datasets and comparing results to humans. We find that -- compared to humans -- LLMs generate language with less conversational grounding, instead generating text that appears to simply presume common",
    "path": "papers/23/11/2311.09144.json",
    "total_tokens": 837,
    "translated_title": "语言模型生成中的对话基础研究",
    "translated_abstract": "有效的对话需要共同的基础：参与者之间的共同理解。然而，共同基础并不会在对话中自发产生。说话者和听众一起努力识别和建构共同基础，同时避免误解。为了完成基础设定，人类依赖于一系列对话行为，如澄清（你是什么意思？）和确认（我明白了）。然而，尚不清楚大型语言模型(LLMs)是否生成反映人类基础设定的文本。为此，我们筛选了一组基础设定行为，并提出相应的度量标准，量化尝试基础设定。我们研究LLM生成是否包含基础设定行为，模拟了几个对话数据集中的轮次，并将结果与人类进行了比较。我们发现，与人类相比，LLMs生成的语言中包含的对话基础较少，而更多地是一味地认为存在共同基础而生成文本。",
    "tldr": "该论文研究了大型语言模型在生成文本时是否具有人类对话基础的表现，通过量化尝试基础设定的一系列指标比较了模型与人类的生成结果。",
    "en_tdlr": "This paper investigates whether large language models reflect human conversational grounding in text generation, comparing the model's performance with humans through quantifying attempted grounding using a set of metrics."
}