{
    "title": "DP-Mix: Mixup-based Data Augmentation for Differentially Private Learning. (arXiv:2311.01295v1 [cs.LG])",
    "abstract": "Data augmentation techniques, such as simple image transformations and combinations, are highly effective at improving the generalization of computer vision models, especially when training data is limited. However, such techniques are fundamentally incompatible with differentially private learning approaches, due to the latter's built-in assumption that each training image's contribution to the learned model is bounded. In this paper, we investigate why naive applications of multi-sample data augmentation techniques, such as mixup, fail to achieve good performance and propose two novel data augmentation techniques specifically designed for the constraints of differentially private learning. Our first technique, DP-Mix_Self, achieves SoTA classification performance across a range of datasets and settings by performing mixup on self-augmented data. Our second technique, DP-Mix_Diff, further improves performance by incorporating synthetic data from a pre-trained diffusion model into the ",
    "link": "http://arxiv.org/abs/2311.01295",
    "context": "Title: DP-Mix: Mixup-based Data Augmentation for Differentially Private Learning. (arXiv:2311.01295v1 [cs.LG])\nAbstract: Data augmentation techniques, such as simple image transformations and combinations, are highly effective at improving the generalization of computer vision models, especially when training data is limited. However, such techniques are fundamentally incompatible with differentially private learning approaches, due to the latter's built-in assumption that each training image's contribution to the learned model is bounded. In this paper, we investigate why naive applications of multi-sample data augmentation techniques, such as mixup, fail to achieve good performance and propose two novel data augmentation techniques specifically designed for the constraints of differentially private learning. Our first technique, DP-Mix_Self, achieves SoTA classification performance across a range of datasets and settings by performing mixup on self-augmented data. Our second technique, DP-Mix_Diff, further improves performance by incorporating synthetic data from a pre-trained diffusion model into the ",
    "path": "papers/23/11/2311.01295.json",
    "total_tokens": 912,
    "translated_title": "DP-Mix：基于Mixup的差分隐私学习数据增强",
    "translated_abstract": "数据增强技术，如简单的图像变换和组合，在改善计算机视觉模型的泛化能力方面非常有效，特别是在训练数据有限的情况下。然而，这些技术与差分隐私学习方法根本不兼容，原因是差分隐私学习方法内置假设每个训练图像对学习模型的贡献是有界的。本文研究了为何对多样本数据增强技术（如mixup）的朴素应用无法取得良好性能，并提出了两种针对差分隐私学习约束的新型数据增强技术。我们的第一种技术，DP-Mix_Self，在自我增强数据上执行mixup，实现了在各种数据集和设置下的SoTA分类性能。我们的第二种技术DP-Mix_Diff进一步通过将从预训练扩散模型中获得的合成数据纳入增强过程，提高了性能。",
    "tldr": "本文研究了差分隐私学习中数据增强技术的问题，并提出了两种针对差分隐私学习约束的新型数据增强技术，通过对自我增强数据进行mixup和合成数据的使用，实现了优于其他方法的分类性能。",
    "en_tdlr": "This paper investigates the issue of data augmentation techniques in differentially private learning and proposes two novel data augmentation techniques specifically designed for the constraints of differentially private learning. By performing mixup on self-augmented data and incorporating synthetic data from a pre-trained diffusion model, the proposed techniques achieve better classification performance compared to other methods."
}