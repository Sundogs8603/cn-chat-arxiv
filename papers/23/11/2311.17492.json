{
    "title": "Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data. (arXiv:2311.17492v2 [cs.CL] UPDATED)",
    "abstract": "The Manchu language, with its roots in the historical Manchurian region of Northeast China, is now facing a critical threat of extinction, as there are very few speakers left. In our efforts to safeguard the Manchu language, we introduce Mergen, the first-ever attempt at a Manchu-Korean Machine Translation (MT) model. To develop this model, we utilize valuable resources such as the Manwen Laodang(a historical book) and a Manchu-Korean dictionary. Due to the scarcity of a Manchu-Korean parallel dataset, we expand our data by employing word replacement guided by GloVe embeddings, trained on both monolingual and parallel texts. Our approach is built around an encoder-decoder neural machine translation model, incorporating a bi-directional Gated Recurrent Unit (GRU) layer. The experiments have yielded promising results, showcasing a significant enhancement in Manchu-Korean translation, with a remarkable 20-30 point increase in the BLEU score.",
    "link": "http://arxiv.org/abs/2311.17492",
    "context": "Title: Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data. (arXiv:2311.17492v2 [cs.CL] UPDATED)\nAbstract: The Manchu language, with its roots in the historical Manchurian region of Northeast China, is now facing a critical threat of extinction, as there are very few speakers left. In our efforts to safeguard the Manchu language, we introduce Mergen, the first-ever attempt at a Manchu-Korean Machine Translation (MT) model. To develop this model, we utilize valuable resources such as the Manwen Laodang(a historical book) and a Manchu-Korean dictionary. Due to the scarcity of a Manchu-Korean parallel dataset, we expand our data by employing word replacement guided by GloVe embeddings, trained on both monolingual and parallel texts. Our approach is built around an encoder-decoder neural machine translation model, incorporating a bi-directional Gated Recurrent Unit (GRU) layer. The experiments have yielded promising results, showcasing a significant enhancement in Manchu-Korean translation, with a remarkable 20-30 point increase in the BLEU score.",
    "path": "papers/23/11/2311.17492.json",
    "total_tokens": 892,
    "translated_title": "Mergen:首个使用增强数据训练的满汉机器翻译模型",
    "translated_abstract": "满语作为源于中国东北满洲地区的历史语言，正面临着灭绝的危机，因为几乎没有剩余的说者。为了保护满语，我们推出了Mergen，这是首个尝试满汉机器翻译模型的项目。为了开发这个模型，我们利用了宝贵的资源，如《满文老档》（一本历史书）和满汉词典。由于满汉平行数据集的稀缺性，我们通过使用在单语和平行文本上训练的GloVe嵌入引导的词替换来扩充我们的数据。我们的方法基于一个编码器-解码器神经机器翻译模型，包括一个双向门控循环单元（GRU）层。实验结果显示出令人满意的成果，在满汉翻译中有显著提升，BLEU分数增加了20-30个点。",
    "tldr": "Mergen是首个满汉机器翻译模型，通过使用增强数据和双向GRU层，取得了在满汉翻译中显著提升的结果。",
    "en_tdlr": "Mergen is the first Manchu-Korean machine translation model trained on augmented data, which achieves significant improvement in Manchu-Korean translation with the utilization of bi-directional GRU layer."
}