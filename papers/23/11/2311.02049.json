{
    "title": "Post Turing: Mapping the landscape of LLM Evaluation. (arXiv:2311.02049v1 [cs.CL])",
    "abstract": "In the rapidly evolving landscape of Large Language Models (LLMs), introduction of well-defined and standardized evaluation methodologies remains a crucial challenge. This paper traces the historical trajectory of LLM evaluations, from the foundational questions posed by Alan Turing to the modern era of AI research. We categorize the evolution of LLMs into distinct periods, each characterized by its unique benchmarks and evaluation criteria. As LLMs increasingly mimic human-like behaviors, traditional evaluation proxies, such as the Turing test, have become less reliable. We emphasize the pressing need for a unified evaluation system, given the broader societal implications of these models. Through an analysis of common evaluation methodologies, we advocate for a qualitative shift in assessment approaches, underscoring the importance of standardization and objective criteria. This work serves as a call for the AI community to collaboratively address the challenges of LLM evaluation, en",
    "link": "http://arxiv.org/abs/2311.02049",
    "context": "Title: Post Turing: Mapping the landscape of LLM Evaluation. (arXiv:2311.02049v1 [cs.CL])\nAbstract: In the rapidly evolving landscape of Large Language Models (LLMs), introduction of well-defined and standardized evaluation methodologies remains a crucial challenge. This paper traces the historical trajectory of LLM evaluations, from the foundational questions posed by Alan Turing to the modern era of AI research. We categorize the evolution of LLMs into distinct periods, each characterized by its unique benchmarks and evaluation criteria. As LLMs increasingly mimic human-like behaviors, traditional evaluation proxies, such as the Turing test, have become less reliable. We emphasize the pressing need for a unified evaluation system, given the broader societal implications of these models. Through an analysis of common evaluation methodologies, we advocate for a qualitative shift in assessment approaches, underscoring the importance of standardization and objective criteria. This work serves as a call for the AI community to collaboratively address the challenges of LLM evaluation, en",
    "path": "papers/23/11/2311.02049.json",
    "total_tokens": 858,
    "translated_title": "后图灵时代: LLM评估的地图绘制",
    "translated_abstract": "在快速发展的大型语言模型（LLMs）领域中，引入明确定义和标准化的评估方法仍然是一个关键挑战。本文追溯了LLM评估的历史轨迹，从Alan Turing提出的基本问题到现代人工智能研究的时代。我们将LLMs的演变分为不同的时期，每个时期都以其独特的基准和评估标准为特点。随着LLMs越来越像人类行为，传统的评估代理，如图灵测试，变得不太可靠。我们强调了建立统一评估系统的紧迫性，考虑到这些模型的更广泛社会影响。通过对常见评估方法的分析，我们主张在评估方法上进行定性转变，强调标准化和客观标准的重要性。本研究呼吁AI社区共同应对LLM评估的挑战，",
    "tldr": "这篇论文追溯了大型语言模型评估的历史轨迹，强调了引入明确定义和标准化的评估方法的重要性，以解决当前LLMs领域中存在的严重挑战。",
    "en_tdlr": "This paper traces the historical trajectory of large language model (LLM) evaluation and emphasizes the importance of introducing well-defined and standardized evaluation methodologies to address the significant challenges in the LLMs field."
}