{
    "title": "Characterizing Large Language Models as Rationalizers of Knowledge-intensive Tasks",
    "abstract": "Large language models (LLMs) are proficient at generating fluent text with minimal task-specific supervision. Yet, their ability to provide well-grounded rationalizations for knowledge-intensive tasks remains under-explored. Such tasks, like commonsense multiple-choice questions, require rationales based on world knowledge to support predictions and refute alternate options. We consider the task of generating knowledge-guided rationalization in natural language by using expert-written examples in a few-shot manner. Surprisingly, crowd-workers preferred knowledge-grounded rationales over crowdsourced rationalizations, citing their factuality, sufficiency, and comprehensive refutations. Although LLMs-generated rationales were preferable, further improvements in conciseness and novelty are required. In another study, we show how rationalization of incorrect model predictions erodes humans' trust in LLM-generated rationales. Motivated by these observations, we create a two-stage pipeline t",
    "link": "https://arxiv.org/abs/2311.05085",
    "context": "Title: Characterizing Large Language Models as Rationalizers of Knowledge-intensive Tasks\nAbstract: Large language models (LLMs) are proficient at generating fluent text with minimal task-specific supervision. Yet, their ability to provide well-grounded rationalizations for knowledge-intensive tasks remains under-explored. Such tasks, like commonsense multiple-choice questions, require rationales based on world knowledge to support predictions and refute alternate options. We consider the task of generating knowledge-guided rationalization in natural language by using expert-written examples in a few-shot manner. Surprisingly, crowd-workers preferred knowledge-grounded rationales over crowdsourced rationalizations, citing their factuality, sufficiency, and comprehensive refutations. Although LLMs-generated rationales were preferable, further improvements in conciseness and novelty are required. In another study, we show how rationalization of incorrect model predictions erodes humans' trust in LLM-generated rationales. Motivated by these observations, we create a two-stage pipeline t",
    "path": "papers/23/11/2311.05085.json",
    "total_tokens": 931,
    "translated_title": "将大型语言模型定性为知识密集型任务的理性化工具",
    "translated_abstract": "大型语言模型(LLMs)在几乎没有任务特定监督的情况下能够生成流畅的文本。然而，它们在提供基于知识密集型任务的充分理性支持方面的能力尚未得到充分探索。这类任务，比如常识性多项选择题，需要基于世界知识的理性来支持预测并推翻备选选项。我们通过使用专家编写的样例以少量样本的方式在自然语言中生成知识引导的理性化任务。令人惊讶的是，工人群体更喜欢基于知识的理性化方式，认为其具有事实性、充分性和全面性的反驳。虽然LLMs生成的理性化方式更受欢迎，但还需要在简洁性和新颖性方面进一步改进。在另一项研究中，我们展示了错误模型预测的理性化如何侵蚀人类对LLMs生成的理性化的信任。在这些观察的基础上，我们创建了一个两阶段的流程。",
    "tldr": "大型语言模型在知识密集型任务中的理性化能力有待探索，通过使用专家编写的示例，可以生成更受欢迎的基于世界知识的理性化方式。这些理性化方式需要进一步改进，在错误预测的理性化方面会损害人类对模型的信任。",
    "en_tdlr": "Large language models have the potential to provide well-grounded rationalizations for knowledge-intensive tasks, such as commonsense multiple-choice questions. However, improvements are needed in terms of generating concise and novel rationales. The study also highlights the impact of rationalization of incorrect predictions on human trust in the models."
}