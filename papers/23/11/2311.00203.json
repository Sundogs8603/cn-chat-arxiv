{
    "title": "Modeling subjectivity (by Mimicking Annotator Annotation) in toxic comment identification across diverse communities. (arXiv:2311.00203v1 [cs.AI])",
    "abstract": "The prevalence and impact of toxic discussions online have made content moderation crucial.Automated systems can play a vital role in identifying toxicity, and reducing the reliance on human moderation.Nevertheless, identifying toxic comments for diverse communities continues to present challenges that are addressed in this paper.The two-part goal of this study is to(1)identify intuitive variances from annotator disagreement using quantitative analysis and (2)model the subjectivity of these viewpoints.To achieve our goal, we published a new dataset\\footnote{\\url{https://github.com/XXX}} with expert annotators' annotations and used two other public datasets to identify the subjectivity of toxicity.Then leveraging the Large Language Model(LLM),we evaluate the model's ability to mimic diverse viewpoints on toxicity by varying size of the training data and utilizing same set of annotators as the test set used during model training and a separate set of annotators as the test set.We conclud",
    "link": "http://arxiv.org/abs/2311.00203",
    "context": "Title: Modeling subjectivity (by Mimicking Annotator Annotation) in toxic comment identification across diverse communities. (arXiv:2311.00203v1 [cs.AI])\nAbstract: The prevalence and impact of toxic discussions online have made content moderation crucial.Automated systems can play a vital role in identifying toxicity, and reducing the reliance on human moderation.Nevertheless, identifying toxic comments for diverse communities continues to present challenges that are addressed in this paper.The two-part goal of this study is to(1)identify intuitive variances from annotator disagreement using quantitative analysis and (2)model the subjectivity of these viewpoints.To achieve our goal, we published a new dataset\\footnote{\\url{https://github.com/XXX}} with expert annotators' annotations and used two other public datasets to identify the subjectivity of toxicity.Then leveraging the Large Language Model(LLM),we evaluate the model's ability to mimic diverse viewpoints on toxicity by varying size of the training data and utilizing same set of annotators as the test set used during model training and a separate set of annotators as the test set.We conclud",
    "path": "papers/23/11/2311.00203.json",
    "total_tokens": 858,
    "translated_title": "跨多元社区模拟注释者注释以建模毒性评论识别中的主观性",
    "translated_abstract": "在线毒性讨论的流行和影响使内容审核成为至关重要的任务。自动化系统在识别毒性和减少对人工审核的依赖方面发挥着重要作用。然而，针对不同社区的毒性评论识别仍然存在挑战，本文对此进行了探讨。本研究的两个目标是：（1）使用定量分析确定注释者分歧的直观差异，（2）模拟这些观点的主观性。为了实现这个目标，我们发布了一个新的数据集\\footnote{\\url{https://github.com/XXX}}，其中包含专家注释者的注释，并使用其他两个公开数据集来识别毒性的主观性。利用大型语言模型(LLM)，我们评估了模型在模仿多元化毒性观点方面的能力，通过变化训练数据的大小和利用与模型训练过程中使用的测试集相同的注释者作为测试集，以及另外一组注释者作为测试集。",
    "tldr": "本文通过模拟注释者的注释，跨多元社区模型主观性，以解决毒性评论识别的挑战。",
    "en_tdlr": "This paper addresses the challenges of toxic comment identification in diverse communities by modeling subjectivity through mimicking annotator annotation."
}