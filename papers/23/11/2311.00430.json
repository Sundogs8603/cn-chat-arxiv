{
    "title": "Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling. (arXiv:2311.00430v1 [cs.CL])",
    "abstract": "As the size of pre-trained speech recognition models increases, running these large models in low-latency or resource-constrained environments becomes challenging. In this work, we leverage pseudo-labelling to assemble a large-scale open-source dataset which we use to distill the Whisper model into a smaller variant, called Distil-Whisper. Using a simple word error rate (WER) heuristic, we select only the highest quality pseudo-labels for training. The distilled model is 5.8 times faster with 51% fewer parameters, while performing to within 1% WER on out-of-distribution test data in a zero-shot transfer setting. Distil-Whisper maintains the robustness of the Whisper model to difficult acoustic conditions, while being less prone to hallucination errors on long-form audio. Distil-Whisper is designed to be paired with Whisper for speculative decoding, yielding a 2 times speed-up while mathematically ensuring the same outputs as the original model. To facilitate further research in this do",
    "link": "http://arxiv.org/abs/2311.00430",
    "context": "Title: Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling. (arXiv:2311.00430v1 [cs.CL])\nAbstract: As the size of pre-trained speech recognition models increases, running these large models in low-latency or resource-constrained environments becomes challenging. In this work, we leverage pseudo-labelling to assemble a large-scale open-source dataset which we use to distill the Whisper model into a smaller variant, called Distil-Whisper. Using a simple word error rate (WER) heuristic, we select only the highest quality pseudo-labels for training. The distilled model is 5.8 times faster with 51% fewer parameters, while performing to within 1% WER on out-of-distribution test data in a zero-shot transfer setting. Distil-Whisper maintains the robustness of the Whisper model to difficult acoustic conditions, while being less prone to hallucination errors on long-form audio. Distil-Whisper is designed to be paired with Whisper for speculative decoding, yielding a 2 times speed-up while mathematically ensuring the same outputs as the original model. To facilitate further research in this do",
    "path": "papers/23/11/2311.00430.json",
    "total_tokens": 970,
    "translated_title": "Distil-Whisper: 通过大规模伪标签实现稳健的知识蒸馏",
    "translated_abstract": "随着预训练语音识别模型的规模增大，将这些大型模型应用于低延迟或资源受限的环境变得具有挑战性。在这项工作中，我们利用伪标记来组装一个大规模开源数据集，用于将Whisper模型蒸馏为一个更小的变种，称为Distil-Whisper。使用简单的词错误率（WER）启发式策略，我们仅选择最高质量的伪标签进行训练。蒸馏后的模型速度提高了5.8倍，参数减少了51％，在零-shot转移设置下，在分布外测试数据上的WER仅有1％的降低。Distil-Whisper保持了Whisper模型对于困难的声学条件的稳健性，同时在长形音频上减少了虚构错误的发生。Distil-Whisper旨在与Whisper配对进行推理解码，从而加快2倍速度，同时数学上确保与原始模型相同的输出。",
    "tldr": "本文提出了Distil-Whisper模型，利用大规模伪标签和知识蒸馏技术将预训练语音识别模型Whisper蒸馏成更小的模型。Distil-Whisper模型在速度和参数数量方面有显著改进，并在零-shot转移设置下表现出良好的鲁棒性。",
    "en_tdlr": "This paper introduces Distil-Whisper, a smaller variant of the pre-trained speech recognition model Whisper achieved by leveraging large-scale pseudo-labelling and knowledge distillation. Distil-Whisper offers significant improvements in speed and parameter reduction, while demonstrating good robustness in a zero-shot transfer setting."
}