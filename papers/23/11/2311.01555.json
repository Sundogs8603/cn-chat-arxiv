{
    "title": "Instruction Distillation Makes Large Language Models Efficient Zero-shot Rankers. (arXiv:2311.01555v1 [cs.IR])",
    "abstract": "Recent studies have demonstrated the great potential of Large Language Models (LLMs) serving as zero-shot relevance rankers. The typical approach involves making comparisons between pairs or lists of documents. Although effective, these listwise and pairwise methods are not efficient and also heavily rely on intricate prompt engineering. To tackle this problem, we introduce a novel instruction distillation method. The key idea is to distill the pairwise ranking ability of open-sourced LLMs to a simpler but more efficient pointwise ranking. Specifically, given the same LLM, we first rank documents using the effective pairwise approach with complex instructions, and then distill the teacher predictions to the pointwise approach with simpler instructions. Evaluation results on the BEIR, TREC, and ReDial datasets demonstrate that instruction distillation can improve efficiency by 10 to 100x and also enhance the ranking performance of LLMs. Furthermore, our approach surpasses the performanc",
    "link": "http://arxiv.org/abs/2311.01555",
    "context": "Title: Instruction Distillation Makes Large Language Models Efficient Zero-shot Rankers. (arXiv:2311.01555v1 [cs.IR])\nAbstract: Recent studies have demonstrated the great potential of Large Language Models (LLMs) serving as zero-shot relevance rankers. The typical approach involves making comparisons between pairs or lists of documents. Although effective, these listwise and pairwise methods are not efficient and also heavily rely on intricate prompt engineering. To tackle this problem, we introduce a novel instruction distillation method. The key idea is to distill the pairwise ranking ability of open-sourced LLMs to a simpler but more efficient pointwise ranking. Specifically, given the same LLM, we first rank documents using the effective pairwise approach with complex instructions, and then distill the teacher predictions to the pointwise approach with simpler instructions. Evaluation results on the BEIR, TREC, and ReDial datasets demonstrate that instruction distillation can improve efficiency by 10 to 100x and also enhance the ranking performance of LLMs. Furthermore, our approach surpasses the performanc",
    "path": "papers/23/11/2311.01555.json",
    "total_tokens": 932,
    "translated_title": "翻译后的论文标题：指令蒸馏使得大型语言模型成为高效的零-shot排序器",
    "translated_abstract": "近期的研究表明，大型语言模型（LLMs）作为零-shot相关性排序器具有巨大潜力。典型的方法涉及对文档进行一对一或一对多的比较。尽管这些一对多和一对一的方法有效，但效率不高，且严重依赖复杂的提示工程。为了解决这个问题，我们引入了一种新颖的指令蒸馏方法。其核心思想是将开源LLMs的一对一排序能力蒸馏为更简单但更高效的单点排序。具体来说，给定相同的LLMs，我们首先使用复杂的指令采用有效的一对一方法对文档进行排序，然后将教师的预测结果转化为采用更简单的指令的单点排序方法。在BEIR、TREC和ReDial数据集上的评估结果表明，指令蒸馏可以将效率提高10到100倍，同时提高LLMs的排序性能。此外，我们的方法超过了性能",
    "tldr": "中文总结出的一句话要点：本研究提出了一种指令蒸馏方法，通过将大型语言模型的一对一排序能力蒸馏为更高效的单点排序，显著提高了大型语言模型作为零-shot排序器的效率和性能。",
    "en_tdlr": "英文总结出的一句话要点：This study introduces an instruction distillation method that improves the efficiency and performance of large language models as zero-shot rankers by distilling their pairwise ranking ability into a more efficient pointwise ranking."
}