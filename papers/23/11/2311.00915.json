{
    "title": "Task-Agnostic Low-Rank Adapters for Unseen English Dialects. (arXiv:2311.00915v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) are trained on corpora disproportionally weighted in favor of Standard American English. As a result, speakers of other dialects experience significantly more failures when interacting with these technologies. In practice, these speakers often accommodate their speech to be better understood. Our work shares the belief that language technologies should be designed to accommodate the diversity in English dialects and not the other way around. However, prior works on dialect struggle with generalizing to evolving and emerging dialects in a scalable manner. To fill this gap, our method, HyperLoRA, leverages expert linguistic knowledge to enable resource-efficient adaptation via hypernetworks. By disentangling dialect-specific and cross-dialectal information, HyperLoRA improves generalization to unseen dialects in a task-agnostic fashion. Not only is HyperLoRA more scalable in the number of parameters, but it also achieves the best or most competitive performan",
    "link": "http://arxiv.org/abs/2311.00915",
    "context": "Title: Task-Agnostic Low-Rank Adapters for Unseen English Dialects. (arXiv:2311.00915v1 [cs.CL])\nAbstract: Large Language Models (LLMs) are trained on corpora disproportionally weighted in favor of Standard American English. As a result, speakers of other dialects experience significantly more failures when interacting with these technologies. In practice, these speakers often accommodate their speech to be better understood. Our work shares the belief that language technologies should be designed to accommodate the diversity in English dialects and not the other way around. However, prior works on dialect struggle with generalizing to evolving and emerging dialects in a scalable manner. To fill this gap, our method, HyperLoRA, leverages expert linguistic knowledge to enable resource-efficient adaptation via hypernetworks. By disentangling dialect-specific and cross-dialectal information, HyperLoRA improves generalization to unseen dialects in a task-agnostic fashion. Not only is HyperLoRA more scalable in the number of parameters, but it also achieves the best or most competitive performan",
    "path": "papers/23/11/2311.00915.json",
    "total_tokens": 860,
    "translated_title": "无任务知识的低秩适配器用于未知的英语方言",
    "translated_abstract": "大语言模型（LLM）是基于偏向于标准美式英语的语料库进行训练的。因此，使用其他方言的人在与这些技术交互时会遇到更多的故障。在实践中，这些讲话者通常会适应自己的言语以便被更好理解。我们的工作认为语言技术应该被设计为适应英语方言的多样性，而不是反其道而行之。然而，之前关于方言的工作在扩展和新出现的方言上存在问题。为了弥补这一空白，我们的方法HyperLoRA利用专家语言知识通过超网络实现资源高效的适应性。通过解开方言特定和跨方言的信息，HyperLoRA以无任务的方式提高对未知方言的泛化能力。HyperLoRA不仅在参数数量上更具可扩展性，而且在性能上是最佳或最具竞争力的。",
    "tldr": "HyperLoRA是一种无任务知识的低秩适配器方法，利用专家语言知识通过超网络实现资源高效的适应性，从而提高了对未知英语方言的泛化能力。",
    "en_tdlr": "HyperLoRA is a task-agnostic low-rank adapter method that leverages expert linguistic knowledge to enable resource-efficient adaptation via hypernetworks, improving generalization to unseen English dialects."
}