{
    "title": "Do Localization Methods Actually Localize Memorized Data in LLMs? A Tale of Two Benchmarks",
    "abstract": "arXiv:2311.09060v2 Announce Type: replace  Abstract: The concept of localization in LLMs is often mentioned in prior work; however, methods for localization have never been systematically and directly evaluated. We propose two complementary benchmarks that evaluate the ability of localization methods to pinpoint LLM components responsible for memorized data. In our INJ benchmark, we actively inject a piece of new information into a small subset of LLM weights, enabling us to directly evaluate whether localization methods can identify these \"ground truth\" weights. In our DEL benchmark, we evaluate localization by measuring how much dropping out identified neurons deletes a memorized pretrained sequence. Despite their different perspectives, our two benchmarks yield consistent rankings of five localization methods. Methods adapted from network pruning perform well on both benchmarks, and all evaluated methods show promising localization ability. On the other hand, even successful methods",
    "link": "https://arxiv.org/abs/2311.09060",
    "context": "Title: Do Localization Methods Actually Localize Memorized Data in LLMs? A Tale of Two Benchmarks\nAbstract: arXiv:2311.09060v2 Announce Type: replace  Abstract: The concept of localization in LLMs is often mentioned in prior work; however, methods for localization have never been systematically and directly evaluated. We propose two complementary benchmarks that evaluate the ability of localization methods to pinpoint LLM components responsible for memorized data. In our INJ benchmark, we actively inject a piece of new information into a small subset of LLM weights, enabling us to directly evaluate whether localization methods can identify these \"ground truth\" weights. In our DEL benchmark, we evaluate localization by measuring how much dropping out identified neurons deletes a memorized pretrained sequence. Despite their different perspectives, our two benchmarks yield consistent rankings of five localization methods. Methods adapted from network pruning perform well on both benchmarks, and all evaluated methods show promising localization ability. On the other hand, even successful methods",
    "path": "papers/23/11/2311.09060.json",
    "total_tokens": 858,
    "translated_title": "LLM中的定位方法真的可以定位记忆数据吗？ 两个基准的故事",
    "translated_abstract": "LLM中的本地化概念经常出现在先前的工作中；然而，本地化方法从未得到系统和直接的评估。我们提出了两个互补的基准，评估本地化方法定位LLM组件负责记忆数据的能力。在我们的INJ基准中，我们主动将新信息注入到LLM权重的一个小子集中，从而能够直接评估本地化方法是否能够识别这些“地面真相”权重。在我们的DEL基准中，通过测量删除已识别神经元会删除已经预训练序列的量，来评估本地化。尽管它们有不同的视角，我们的两个基准产生了五种本地化方法的一致排名。从网络修剪中改编的方法在两个基准上表现良好，所有评估方法均表现出有希望的定位能力。另一方面，即使是成功的方法",
    "tldr": "该论文提出了两个基准来评估LLM中定位方法对记忆数据的定位能力，发现来自网络修剪的方法在两个基准上表现良好，所有评估方法均展现出有希望的定位能力。",
    "en_tdlr": "The paper introduces two benchmarks to evaluate the localization methods in LLMs for memorized data, discovering that methods adapted from network pruning perform well on both benchmarks, and all evaluated methods show promising localization ability."
}