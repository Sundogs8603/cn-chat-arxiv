{
    "title": "Selectively Sharing Experiences Improves Multi-Agent Reinforcement Learning. (arXiv:2311.00865v1 [cs.LG])",
    "abstract": "We present a novel multi-agent RL approach, Selective Multi-Agent Prioritized Experience Relay, in which agents share with other agents a limited number of transitions they observe during training. The intuition behind this is that even a small number of relevant experiences from other agents could help each agent learn. Unlike many other multi-agent RL algorithms, this approach allows for largely decentralized training, requiring only a limited communication channel between agents. We show that our approach outperforms baseline no-sharing decentralized training and state-of-the art multi-agent RL algorithms. Further, sharing only a small number of highly relevant experiences outperforms sharing all experiences between agents, and the performance uplift from selective experience sharing is robust across a range of hyperparameters and DQN variants. A reference implementation of our algorithm is available at https://github.com/mgerstgrasser/super.",
    "link": "http://arxiv.org/abs/2311.00865",
    "context": "Title: Selectively Sharing Experiences Improves Multi-Agent Reinforcement Learning. (arXiv:2311.00865v1 [cs.LG])\nAbstract: We present a novel multi-agent RL approach, Selective Multi-Agent Prioritized Experience Relay, in which agents share with other agents a limited number of transitions they observe during training. The intuition behind this is that even a small number of relevant experiences from other agents could help each agent learn. Unlike many other multi-agent RL algorithms, this approach allows for largely decentralized training, requiring only a limited communication channel between agents. We show that our approach outperforms baseline no-sharing decentralized training and state-of-the art multi-agent RL algorithms. Further, sharing only a small number of highly relevant experiences outperforms sharing all experiences between agents, and the performance uplift from selective experience sharing is robust across a range of hyperparameters and DQN variants. A reference implementation of our algorithm is available at https://github.com/mgerstgrasser/super.",
    "path": "papers/23/11/2311.00865.json",
    "total_tokens": 891,
    "translated_title": "选择性分享体验提升多智能体强化学习",
    "translated_abstract": "我们提出了一种新颖的多智能体强化学习方法，即选择性多智能体优先体验中继，其中代理通过分享训练过程中观察到的有限的转换与其他代理共享。其背后的直觉是，来自其他代理的少量相关经验可以帮助每个代理学习。与许多其他多智能体强化学习算法不同，该方法允许基本去中心化的训练，只需要代理之间的有限通信渠道。我们展示了我们的方法优于基准无共享去中心化训练和最先进的多智能体强化学习算法。此外，仅分享少量高度相关的经验优于代理之间的所有经验共享，而且选择性体验共享的性能提升在各种超参数和DQN变体中均具有鲁棒性。我们的算法的参考实现可在https://github.com/mgerstgrasser/super获得。",
    "tldr": "本文介绍了一种选择性多智能体强化学习方法，即选择性多智能体优先体验中继，代理之间共享有限数量的训练经验。与其他算法相比，该方法实现了去中心化训练，并取得了比基准算法和最先进算法更好的性能。",
    "en_tdlr": "This paper presents a selective multi-agent reinforcement learning approach called Selective Multi-Agent Prioritized Experience Relay, where agents share a limited number of training experiences with each other. This approach achieves decentralized training and outperforms baseline algorithms and state-of-the-art algorithms."
}