{
    "title": "Towards Calibrated Robust Fine-Tuning of Vision-Language Models. (arXiv:2311.01723v1 [cs.CV])",
    "abstract": "While fine-tuning unleashes the potential of a pre-trained model to a specific task, it trades off the model's generalization capability on out-of-distribution (OOD) datasets. To mitigate this, robust fine-tuning aims to ensure performance on OOD datasets as well as an in-distribution (ID) dataset for which the model is being tuned. However, another criterion for reliable machine learning (ML), confidence calibration, has been overlooked despite its increasing demand for real-world high-stakes ML applications (e.g., autonomous driving and medical diagnosis). For the first time, we raise concerns about the calibration of fine-tuned vision-language models (VLMs) under distribution shift by showing that naive fine-tuning and even state-of-the-art robust fine-tuning methods hurt the calibration of pre-trained VLMs, especially on OOD datasets. To address this, we provide a simple approach, called a calibrated robust fine-tuning (CaRot) that incentivizes the calibration and robustness on bot",
    "link": "http://arxiv.org/abs/2311.01723",
    "context": "Title: Towards Calibrated Robust Fine-Tuning of Vision-Language Models. (arXiv:2311.01723v1 [cs.CV])\nAbstract: While fine-tuning unleashes the potential of a pre-trained model to a specific task, it trades off the model's generalization capability on out-of-distribution (OOD) datasets. To mitigate this, robust fine-tuning aims to ensure performance on OOD datasets as well as an in-distribution (ID) dataset for which the model is being tuned. However, another criterion for reliable machine learning (ML), confidence calibration, has been overlooked despite its increasing demand for real-world high-stakes ML applications (e.g., autonomous driving and medical diagnosis). For the first time, we raise concerns about the calibration of fine-tuned vision-language models (VLMs) under distribution shift by showing that naive fine-tuning and even state-of-the-art robust fine-tuning methods hurt the calibration of pre-trained VLMs, especially on OOD datasets. To address this, we provide a simple approach, called a calibrated robust fine-tuning (CaRot) that incentivizes the calibration and robustness on bot",
    "path": "papers/23/11/2311.01723.json",
    "total_tokens": 947,
    "translated_title": "实现对视觉语言模型的校准鲁棒微调",
    "translated_abstract": "微调可以释放预训练模型在特定任务上的潜力，但会影响模型对于非分布数据集的泛化能力。为了缓解这个问题，鲁棒微调旨在确保模型在非分布数据集以及微调的分布数据集上都有良好的性能。然而，在可靠的机器学习中，置信度校准这一标准却经常被忽视，尽管在现实世界中高风险的机器学习应用中（如自动驾驶和医学诊断）需求日益增加。我们首次提出了对细调的视觉语言模型在分布变化下校准的担忧，并通过显示普通微调甚至最先进的鲁棒微调方法对预训练的视觉语言模型的校准造成了损害，尤其是在非分布数据集上。为了解决这个问题，我们提出了一种简单的方法，称为校准鲁棒微调（CaRot），它在校准和鲁棒性上提供了奖励。",
    "tldr": "本文提出了一个名为校准鲁棒微调（CaRot）的方法，针对视觉语言模型在分布变化下的校准问题。通过该方法，作者成功提高了预训练模型的校准性能和鲁棒性能。",
    "en_tdlr": "This paper introduces a method called Calibrated Robust Fine-Tuning (CaRot) to address the issue of calibration in vision-language models under distribution shift. The authors successfully improve the calibration and robustness of pre-trained models using this approach."
}