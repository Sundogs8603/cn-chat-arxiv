{
    "title": "MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training",
    "abstract": "arXiv:2311.17049v2 Announce Type: replace-cross  Abstract: Contrastive pretraining of image-text foundation models, such as CLIP, demonstrated excellent zero-shot performance and improved robustness on a wide range of downstream tasks. However, these models utilize large transformer-based encoders with significant memory and latency overhead which pose challenges for deployment on mobile devices. In this work, we introduce MobileCLIP -- a new family of efficient image-text models optimized for runtime performance along with a novel and efficient training approach, namely multi-modal reinforced training. The proposed training approach leverages knowledge transfer from an image captioning model and an ensemble of strong CLIP encoders to improve the accuracy of efficient models. Our approach avoids train-time compute overhead by storing the additional knowledge in a reinforced dataset. MobileCLIP sets a new state-of-the-art latency-accuracy tradeoff for zero-shot classification and retrie",
    "link": "https://arxiv.org/abs/2311.17049",
    "context": "Title: MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training\nAbstract: arXiv:2311.17049v2 Announce Type: replace-cross  Abstract: Contrastive pretraining of image-text foundation models, such as CLIP, demonstrated excellent zero-shot performance and improved robustness on a wide range of downstream tasks. However, these models utilize large transformer-based encoders with significant memory and latency overhead which pose challenges for deployment on mobile devices. In this work, we introduce MobileCLIP -- a new family of efficient image-text models optimized for runtime performance along with a novel and efficient training approach, namely multi-modal reinforced training. The proposed training approach leverages knowledge transfer from an image captioning model and an ensemble of strong CLIP encoders to improve the accuracy of efficient models. Our approach avoids train-time compute overhead by storing the additional knowledge in a reinforced dataset. MobileCLIP sets a new state-of-the-art latency-accuracy tradeoff for zero-shot classification and retrie",
    "path": "papers/23/11/2311.17049.json",
    "total_tokens": 831,
    "translated_title": "MobileCLIP: 通过多模态强化训练实现快速图像-文本模型",
    "translated_abstract": "对图像-文本基础模型（如CLIP）进行对比预训练表明，在广泛的下游任务中表现出色的零-shot性能和改善的鲁棒性。然而，这些模型利用大型基于transformer的编码器，具有显著的内存和延迟开销，这会给在移动设备上部署带来挑战。在这项工作中，我们介绍了MobileCLIP--一种优化了运行时性能的高效图像-文本模型家族，以及一种新颖而高效的训练方法，即多模态强化训练。所提出的训练方法利用来自图像字幕模型和强CLIP编码器集成的知识转移来提高高效模型的准确性。我们的方法通过将额外的知识存储在强化数据集中避免了训练时的计算开销。MobileCLIP为零-shot分类和检索设置了一个新的延迟-准确性权衡的最新水平。",
    "tldr": "MobileCLIP通过多模态强化训练实现了高效率图像-文本模型，在零-shot任务中取得了新的性能平衡。",
    "en_tdlr": "MobileCLIP achieves efficient image-text models through multi-modal reinforced training, setting a new performance balance in zero-shot tasks."
}