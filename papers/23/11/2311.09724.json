{
    "title": "OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning",
    "abstract": "arXiv:2311.09724v2 Announce Type: replace  Abstract: Large language models (LLMs) often struggle with maintaining accuracy throughout multiple multiple reasoning steps, especially in mathematical reasoning where an error in earlier steps can propagate to subsequent ones and it ultimately leading to an incorrect answer. To reduce error propagation, guided decoding is employed to direct the LM decoding on a step-by-step basis. We argue that in guided decoding, assessing the potential of an incomplete reasoning path can be more advantageous than simply ensuring per-step correctness, as the former approach leads towards a correct final answer. This transforms the task into a $\\textit{value estimation}$ problem in planning.   Inspired by the findings that $\\textit{outcome supervision for guided decoding essentially acts as a value model}$, we propose Outcome-supervised Value Model (OVM) that employs outcome supervision for training a value model, which prioritizes steps that lead to accurat",
    "link": "https://arxiv.org/abs/2311.09724",
    "context": "Title: OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning\nAbstract: arXiv:2311.09724v2 Announce Type: replace  Abstract: Large language models (LLMs) often struggle with maintaining accuracy throughout multiple multiple reasoning steps, especially in mathematical reasoning where an error in earlier steps can propagate to subsequent ones and it ultimately leading to an incorrect answer. To reduce error propagation, guided decoding is employed to direct the LM decoding on a step-by-step basis. We argue that in guided decoding, assessing the potential of an incomplete reasoning path can be more advantageous than simply ensuring per-step correctness, as the former approach leads towards a correct final answer. This transforms the task into a $\\textit{value estimation}$ problem in planning.   Inspired by the findings that $\\textit{outcome supervision for guided decoding essentially acts as a value model}$, we propose Outcome-supervised Value Model (OVM) that employs outcome supervision for training a value model, which prioritizes steps that lead to accurat",
    "path": "papers/23/11/2311.09724.json",
    "total_tokens": 823,
    "translated_title": "OVM，结果监督价值模型用于数学推理规划",
    "translated_abstract": "大型语言模型（LLMs）经常在多个推理步骤中保持准确性方面遇到困难，特别是在数学推理中，早期步骤中的错误可能传播到后续步骤，最终导致错误答案。为了减少错误传播，引入了引导解码以逐步指导LM解码。我们认为，在引导解码中，评估不完整推理路径的潜力可能比仅确保每个步骤的正确性更有优势，因为前一种方法会导向正确的最终答案。这将任务转化为计划中的价值估计问题。受到发现的启发，即$\\textit{引导解码的结果监督本质上充当价值模型}$，我们提出了一种Outcome-supervised Value Model (OVM)，它采用结果监督来训练价值模型，优先考虑导致准确性的步骤。",
    "tldr": "引入Outcome-supervised Value Model (OVM)利用结果监督训练价值模型，以在数学推理中减少错误传播，将任务转化为价值估计问题。",
    "en_tdlr": "Introducing Outcome-supervised Value Model (OVM) that utilizes outcome supervision to train a value model to reduce error propagation in mathematical reasoning, transforming the task into a value estimation problem."
}