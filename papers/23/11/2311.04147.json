{
    "title": "Multi-resolution Time-Series Transformer for Long-term Forecasting",
    "abstract": "arXiv:2311.04147v2 Announce Type: replace  Abstract: The performance of transformers for time-series forecasting has improved significantly. Recent architectures learn complex temporal patterns by segmenting a time-series into patches and using the patches as tokens. The patch size controls the ability of transformers to learn the temporal patterns at different frequencies: shorter patches are effective for learning localized, high-frequency patterns, whereas mining long-term seasonalities and trends requires longer patches. Inspired by this observation, we propose a novel framework, Multi-resolution Time-Series Transformer (MTST), which consists of a multi-branch architecture for simultaneous modeling of diverse temporal patterns at different resolutions. In contrast to many existing time-series transformers, we employ relative positional encoding, which is better suited for extracting periodic components at different scales. Extensive experiments on several real-world datasets demons",
    "link": "https://arxiv.org/abs/2311.04147",
    "context": "Title: Multi-resolution Time-Series Transformer for Long-term Forecasting\nAbstract: arXiv:2311.04147v2 Announce Type: replace  Abstract: The performance of transformers for time-series forecasting has improved significantly. Recent architectures learn complex temporal patterns by segmenting a time-series into patches and using the patches as tokens. The patch size controls the ability of transformers to learn the temporal patterns at different frequencies: shorter patches are effective for learning localized, high-frequency patterns, whereas mining long-term seasonalities and trends requires longer patches. Inspired by this observation, we propose a novel framework, Multi-resolution Time-Series Transformer (MTST), which consists of a multi-branch architecture for simultaneous modeling of diverse temporal patterns at different resolutions. In contrast to many existing time-series transformers, we employ relative positional encoding, which is better suited for extracting periodic components at different scales. Extensive experiments on several real-world datasets demons",
    "path": "papers/23/11/2311.04147.json",
    "total_tokens": 778,
    "translated_title": "多分辨率时间序列Transformer用于长期预测",
    "translated_abstract": "最近transformers在时间序列预测方面的表现显著提高。最近的架构通过将时间序列分割为片段并将这些片段用作标记来学习复杂的时间模式。片段大小控制了transformers学习不同频率的时间模式的能力：较短的片段适用于学习局部的高频模式，而挖掘长期的季节性和趋势则需要较长的片段。受到这一观察的启发，我们提出了一种新颖的框架，即多分辨率时间序列Transformer（MTST），它由多分支架构组成，用于同时建模不同分辨率下的多样化时间模式。与许多现有的时间序列transformers不同，我们采用相对位置编码，更适合提取不同尺度上的周期成分。我们在几个真实世界数据集上进行了大量实验。",
    "tldr": "提出了一种Multi-resolution Time-Series Transformer框架，采用多分支架构和相对位置编码，用于同时建模不同分辨率下的多样化时间模式。"
}