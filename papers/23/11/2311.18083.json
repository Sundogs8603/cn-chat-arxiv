{
    "title": "Meta Co-Training: Two Views are Better than One",
    "abstract": "In many practical computer vision scenarios unlabeled data is plentiful, but labels are scarce and difficult to obtain. As a result, semi-supervised learning which leverages unlabeled data to boost the performance of supervised classifiers have received significant attention in recent literature. One major class of semi-supervised algorithms is co-training. In co-training two different models leverage different independent and sufficient \"views\" of the data to jointly make better predictions. During co-training each model creates pseudo labels on unlabeled points which are used to improve the other model. We show that in the common case when independent views are not available we can construct such views inexpensively using pre-trained models. Co-training on the constructed views yields a performance improvement over any of the individual views we construct and performance comparable with recent approaches in semi-supervised learning, but has some undesirable properties. To alleviate t",
    "link": "https://arxiv.org/abs/2311.18083",
    "context": "Title: Meta Co-Training: Two Views are Better than One\nAbstract: In many practical computer vision scenarios unlabeled data is plentiful, but labels are scarce and difficult to obtain. As a result, semi-supervised learning which leverages unlabeled data to boost the performance of supervised classifiers have received significant attention in recent literature. One major class of semi-supervised algorithms is co-training. In co-training two different models leverage different independent and sufficient \"views\" of the data to jointly make better predictions. During co-training each model creates pseudo labels on unlabeled points which are used to improve the other model. We show that in the common case when independent views are not available we can construct such views inexpensively using pre-trained models. Co-training on the constructed views yields a performance improvement over any of the individual views we construct and performance comparable with recent approaches in semi-supervised learning, but has some undesirable properties. To alleviate t",
    "path": "papers/23/11/2311.18083.json",
    "total_tokens": 814,
    "translated_title": "元共训练：两种视角优于一种",
    "translated_abstract": "在许多实际的计算机视觉场景中，未标记的数据很多，但标签却稀缺且难以获得。因此，半监督学习利用未标记的数据提升监督分类器的性能已经在最近的文献中引起了重要的关注。其中一种主要的半监督算法是共训练。在共训练中，两种不同的模型利用数据的不同独立和足够的“视角”来共同进行更好的预测。在共训练过程中，每个模型在未标记的数据点上创建伪标签，用于改进另一个模型的性能。我们展示了在常见情况下，当独立视角不可用时，我们可以使用预训练模型来廉价地构建这些视角。在构建的视角上进行共训练可以提高性能，优于我们构建的任何单个视角，并且与半监督学习中的最新方法性能相当，但具有一些不可取之处。",
    "tldr": "元共训练通过在数据上构建不同的视角，并利用未标记数据进行共同训练，提高了半监督学习的性能。"
}