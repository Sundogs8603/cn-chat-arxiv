{
    "title": "Chinesewebtext: Large-scale high-quality Chinese web text extracted with effective evaluation model. (arXiv:2311.01149v1 [cs.CL])",
    "abstract": "During the development of large language models (LLMs), the scale and quality of the pre-training data play a crucial role in shaping LLMs' capabilities. To accelerate the research of LLMs, several large-scale datasets, such as C4 [1], Pile [2], RefinedWeb [3] and WanJuan [4], have been released to the public. However, most of the released corpus focus mainly on English, and there is still lack of complete tool-chain for extracting clean texts from web data. Furthermore, fine-grained information of the corpus, e.g. the quality of each text, is missing. To address these challenges, we propose in this paper a new complete tool-chain EvalWeb to extract Chinese clean texts from noisy web data. First, similar to previous work, manually crafted rules are employed to discard explicit noisy texts from the raw crawled web contents. Second, a well-designed evaluation model is leveraged to assess the remaining relatively clean data, and each text is assigned a specific quality score. Finally, we ",
    "link": "http://arxiv.org/abs/2311.01149",
    "context": "Title: Chinesewebtext: Large-scale high-quality Chinese web text extracted with effective evaluation model. (arXiv:2311.01149v1 [cs.CL])\nAbstract: During the development of large language models (LLMs), the scale and quality of the pre-training data play a crucial role in shaping LLMs' capabilities. To accelerate the research of LLMs, several large-scale datasets, such as C4 [1], Pile [2], RefinedWeb [3] and WanJuan [4], have been released to the public. However, most of the released corpus focus mainly on English, and there is still lack of complete tool-chain for extracting clean texts from web data. Furthermore, fine-grained information of the corpus, e.g. the quality of each text, is missing. To address these challenges, we propose in this paper a new complete tool-chain EvalWeb to extract Chinese clean texts from noisy web data. First, similar to previous work, manually crafted rules are employed to discard explicit noisy texts from the raw crawled web contents. Second, a well-designed evaluation model is leveraged to assess the remaining relatively clean data, and each text is assigned a specific quality score. Finally, we ",
    "path": "papers/23/11/2311.01149.json",
    "total_tokens": 980,
    "translated_title": "Chinesewebtext: 用有效的评估模型提取大规模高质量的中文网络文本",
    "translated_abstract": "在大型语言模型（LLM）的发展过程中，预训练数据的规模和质量对于塑造LLM的能力起着至关重要的作用。为了加快LLM的研究进展，已经发布了一些大规模数据集，例如C4 [1]、Pile [2]、RefinedWeb [3]和WanJuan [4]等。然而，大多数已发布的语料库主要关注英文，仍然缺乏完整的工具链来从网络数据中提取出干净的文本。此外，缺乏对语料库的细粒度信息，例如每个文本的质量。为了解决这些挑战，本文提出了一个新的完整的工具链EvalWeb，用于从嘈杂的网络数据中提取中文干净的文本。首先，类似之前的工作，使用手工制定的规则来丢弃原始爬取的网络内容中的明确嘈杂的文本。然后，利用一个精心设计的评估模型来评估剩余相对干净的数据，并为每个文本分配一个特定的质量分数。最后，我们进行了大规模的实验，验证了EvalWeb工具链的有效性。",
    "tldr": "本文提出了一个完整的工具链EvalWeb，用于从网络数据中提取干净的中文文本。通过手工制定的规则筛除噪音数据，并使用评估模型为每个文本分配质量分数。",
    "en_tdlr": "A complete tool-chain, EvalWeb, is proposed in this paper to extract clean Chinese texts from web data. Noisy texts are discarded using manually crafted rules, and each text is assigned a quality score using an evaluation model."
}