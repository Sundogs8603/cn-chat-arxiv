{
    "title": "Performance Optimization of Deep Learning Sparse Matrix Kernels on Intel Max Series GPU. (arXiv:2311.00368v1 [cs.LG])",
    "abstract": "In this paper, we focus on three sparse matrix operations that are relevant for machine learning applications, namely, the sparse-dense matrix multiplication (SPMM), the sampled dense-dense matrix multiplication (SDDMM), and the composition of the SDDMM with SPMM, also termed as FusedMM. We develop optimized implementations for SPMM, SDDMM, and FusedMM operations utilizing Intel oneAPI's Explicit SIMD (ESIMD) SYCL extension API. In contrast to CUDA or SYCL, the ESIMD API enables the writing of explicitly vectorized kernel code. Sparse matrix algorithms implemented with the ESIMD API achieved performance close to the peak of the targeted Intel Data Center GPU. We compare our performance results to Intel's oneMKL library on Intel GPUs and to a recent CUDA implementation for the sparse matrix operations on NVIDIA's V100 GPU and demonstrate that our implementations for sparse matrix operations outperform either.",
    "link": "http://arxiv.org/abs/2311.00368",
    "context": "Title: Performance Optimization of Deep Learning Sparse Matrix Kernels on Intel Max Series GPU. (arXiv:2311.00368v1 [cs.LG])\nAbstract: In this paper, we focus on three sparse matrix operations that are relevant for machine learning applications, namely, the sparse-dense matrix multiplication (SPMM), the sampled dense-dense matrix multiplication (SDDMM), and the composition of the SDDMM with SPMM, also termed as FusedMM. We develop optimized implementations for SPMM, SDDMM, and FusedMM operations utilizing Intel oneAPI's Explicit SIMD (ESIMD) SYCL extension API. In contrast to CUDA or SYCL, the ESIMD API enables the writing of explicitly vectorized kernel code. Sparse matrix algorithms implemented with the ESIMD API achieved performance close to the peak of the targeted Intel Data Center GPU. We compare our performance results to Intel's oneMKL library on Intel GPUs and to a recent CUDA implementation for the sparse matrix operations on NVIDIA's V100 GPU and demonstrate that our implementations for sparse matrix operations outperform either.",
    "path": "papers/23/11/2311.00368.json",
    "total_tokens": 976,
    "translated_title": "英特尔Max系列GPU上深度学习稀疏矩阵核的性能优化",
    "translated_abstract": "本文主要研究与机器学习应用相关的三个稀疏矩阵操作：稀疏-稠密矩阵乘法（SPMM），采样稠密-稠密矩阵乘法（SDDMM），以及SDDMM与SPMM的组合（FusedMM）。我们利用英特尔oneAPI的Explicit SIMD (ESIMD) SYCL扩展API开发了SPMM、SDDMM和FusedMM操作的优化实现。与CUDA或SYCL相比，ESIMD API允许编写显式矢量化的内核代码。使用ESIMD API实现的稀疏矩阵算法的性能接近于目标英特尔数据中心GPU的峰值性能。我们将性能结果与英特尔的oneMKL库在英特尔GPU上以及最近的适用于NVIDIA的V100 GPU的CUDA实现进行比较，并证明我们的稀疏矩阵操作实现优于两者。",
    "tldr": "本文研究了在英特尔Max系列GPU上对深度学习稀疏矩阵核的性能优化。通过使用英特尔oneAPI的ESIMD SYCL扩展API，我们开发了优化实现并实现了高性能的稀疏矩阵操作，性能接近于GPU的峰值性能，并在与英特尔的oneMKL库和NVIDIA的V100 GPU上的CUDA实现进行比较后表现出更好的性能。",
    "en_tdlr": "This paper focuses on optimizing the performance of deep learning sparse matrix kernels on Intel Max Series GPU. By utilizing Intel oneAPI's ESIMD SYCL extension API, optimized implementations were developed to achieve high-performance sparse matrix operations close to the peak performance of the GPU. The implementations outperformed Intel's oneMKL library on Intel GPUs and a recent CUDA implementation on NVIDIA's V100 GPU."
}