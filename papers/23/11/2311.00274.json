{
    "title": "Generalization Bounds for Label Noise Stochastic Gradient Descent. (arXiv:2311.00274v1 [stat.ML])",
    "abstract": "We develop generalization error bounds for stochastic gradient descent (SGD) with label noise in non-convex settings under uniform dissipativity and smoothness conditions. Under a suitable choice of semimetric, we establish a contraction in Wasserstein distance of the label noise stochastic gradient flow that depends polynomially on the parameter dimension $d$. Using the framework of algorithmic stability, we derive time-independent generalisation error bounds for the discretized algorithm with a constant learning rate. The error bound we achieve scales polynomially with $d$ and with the rate of $n^{-2/3}$, where $n$ is the sample size. This rate is better than the best-known rate of $n^{-1/2}$ established for stochastic gradient Langevin dynamics (SGLD) -which employs parameter-independent Gaussian noise -- under similar conditions. Our analysis offers quantitative insights into the effect of label noise.",
    "link": "http://arxiv.org/abs/2311.00274",
    "context": "Title: Generalization Bounds for Label Noise Stochastic Gradient Descent. (arXiv:2311.00274v1 [stat.ML])\nAbstract: We develop generalization error bounds for stochastic gradient descent (SGD) with label noise in non-convex settings under uniform dissipativity and smoothness conditions. Under a suitable choice of semimetric, we establish a contraction in Wasserstein distance of the label noise stochastic gradient flow that depends polynomially on the parameter dimension $d$. Using the framework of algorithmic stability, we derive time-independent generalisation error bounds for the discretized algorithm with a constant learning rate. The error bound we achieve scales polynomially with $d$ and with the rate of $n^{-2/3}$, where $n$ is the sample size. This rate is better than the best-known rate of $n^{-1/2}$ established for stochastic gradient Langevin dynamics (SGLD) -which employs parameter-independent Gaussian noise -- under similar conditions. Our analysis offers quantitative insights into the effect of label noise.",
    "path": "papers/23/11/2311.00274.json",
    "total_tokens": 956,
    "translated_title": "标签噪声随机梯度下降的泛化界",
    "translated_abstract": "我们在非凸设置中，基于均匀耗散和平滑条件，为具有标签噪声的随机梯度下降（SGD）开发了泛化错误界限。在适当选择的半度量下，我们建立了与参数维度$d$多项式相关的标签噪声随机梯度流的Wasserstein距离收缩。利用算法稳定性框架，我们为离散化算法推导了独立于时间的泛化错误界限，并使用固定学习率。我们实现的错误界限与$d$和样本大小$n$的速率以及$n^{-2/3}$有多项式的关系。这个速率在与类似条件下使用参数无关高斯噪声的随机梯度Langevin动力学（SGLD）中，其最佳已知速率$n^{-1/2}$要好。我们的分析提供了关于标签噪声影响的量化见解。",
    "tldr": "本研究通过在非凸设置中使用标签噪声对随机梯度下降进行了泛化错误界限的研究，利用算法稳定性框架得到了时间无关的泛化错误界限，并且在参数维度和样本大小的速率以及特定学习率情况下实现了多项式的错误界限。该分析提供了关于标签噪声影响的量化见解。"
}