{
    "title": "AFPQ: Asymmetric Floating Point Quantization for LLMs. (arXiv:2311.01792v1 [cs.CL])",
    "abstract": "Large language models (LLMs) show great performance in various tasks, but face deployment challenges from limited memory capacity and bandwidth. Low-bit weight quantization can save memory and accelerate inference. Although floating-point (FP) formats show good performance in LLM quantization, they tend to perform poorly with small group sizes or sub-4 bits. We find the reason is that the absence of asymmetry in previous FP quantization makes it unsuitable for handling asymmetric value distribution of LLM weight tensors. In this work, we propose asymmetric FP quantization (AFPQ), which sets separate scales for positive and negative values. Our method leads to large accuracy improvements and can be easily plugged into other quantization methods, including GPTQ and AWQ, for better performance. Besides, no additional storage is needed compared with asymmetric integer (INT) quantization. The code is available at https://github.com/zhangsichengsjtu/AFPQ.",
    "link": "http://arxiv.org/abs/2311.01792",
    "context": "Title: AFPQ: Asymmetric Floating Point Quantization for LLMs. (arXiv:2311.01792v1 [cs.CL])\nAbstract: Large language models (LLMs) show great performance in various tasks, but face deployment challenges from limited memory capacity and bandwidth. Low-bit weight quantization can save memory and accelerate inference. Although floating-point (FP) formats show good performance in LLM quantization, they tend to perform poorly with small group sizes or sub-4 bits. We find the reason is that the absence of asymmetry in previous FP quantization makes it unsuitable for handling asymmetric value distribution of LLM weight tensors. In this work, we propose asymmetric FP quantization (AFPQ), which sets separate scales for positive and negative values. Our method leads to large accuracy improvements and can be easily plugged into other quantization methods, including GPTQ and AWQ, for better performance. Besides, no additional storage is needed compared with asymmetric integer (INT) quantization. The code is available at https://github.com/zhangsichengsjtu/AFPQ.",
    "path": "papers/23/11/2311.01792.json",
    "total_tokens": 870,
    "translated_title": "AFPQ：面向LLMs的非对称浮点量化",
    "translated_abstract": "大型语言模型（LLMs）在各种任务中表现出色，但面临有限的内存容量和带宽的部署挑战。低位权重量化可以节省内存并加速推断。尽管浮点（FP）格式在LLM量化中表现出良好性能，但它们在小组大小或子4位时往往表现不佳。我们发现，之前的FP量化缺乏不对称性，不适合处理LLM权重张量的不对称值分布。在这项工作中，我们提出了非对称FP量化（AFPQ），为正值和负值设置了分别的比例尺。我们的方法显著提高了准确性，并可以轻松地插入其他量化方法，包括GPTQ和AWQ，以获得更好的性能。此外，与非对称整数（INT）量化相比，不需要额外的存储空间。代码可在https://github.com/zhangsichengsjtu/AFPQ获得。",
    "tldr": "AFPQ提出了一种面向LLMs的非对称浮点量化方法，通过为正值和负值设置不同的比例尺，显著提高了准确性，并且可以与其他量化方法相结合，无需额外存储空间。"
}