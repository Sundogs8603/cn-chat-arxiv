{
    "title": "Robust Adversarial Reinforcement Learning via Bounded Rationality Curricula. (arXiv:2311.01642v1 [cs.LG])",
    "abstract": "Robustness against adversarial attacks and distribution shifts is a long-standing goal of Reinforcement Learning (RL). To this end, Robust Adversarial Reinforcement Learning (RARL) trains a protagonist against destabilizing forces exercised by an adversary in a competitive zero-sum Markov game, whose optimal solution, i.e., rational strategy, corresponds to a Nash equilibrium. However, finding Nash equilibria requires facing complex saddle point optimization problems, which can be prohibitive to solve, especially for high-dimensional control. In this paper, we propose a novel approach for adversarial RL based on entropy regularization to ease the complexity of the saddle point optimization problem. We show that the solution of this entropy-regularized problem corresponds to a Quantal Response Equilibrium (QRE), a generalization of Nash equilibria that accounts for bounded rationality, i.e., agents sometimes play random actions instead of optimal ones. Crucially, the connection between ",
    "link": "http://arxiv.org/abs/2311.01642",
    "context": "Title: Robust Adversarial Reinforcement Learning via Bounded Rationality Curricula. (arXiv:2311.01642v1 [cs.LG])\nAbstract: Robustness against adversarial attacks and distribution shifts is a long-standing goal of Reinforcement Learning (RL). To this end, Robust Adversarial Reinforcement Learning (RARL) trains a protagonist against destabilizing forces exercised by an adversary in a competitive zero-sum Markov game, whose optimal solution, i.e., rational strategy, corresponds to a Nash equilibrium. However, finding Nash equilibria requires facing complex saddle point optimization problems, which can be prohibitive to solve, especially for high-dimensional control. In this paper, we propose a novel approach for adversarial RL based on entropy regularization to ease the complexity of the saddle point optimization problem. We show that the solution of this entropy-regularized problem corresponds to a Quantal Response Equilibrium (QRE), a generalization of Nash equilibria that accounts for bounded rationality, i.e., agents sometimes play random actions instead of optimal ones. Crucially, the connection between ",
    "path": "papers/23/11/2311.01642.json",
    "total_tokens": 950,
    "translated_title": "基于有界理性阶梯的强化学习鲁棒对抗性训练",
    "translated_abstract": "鲁棒性对抗攻击和分布偏移一直是强化学习的长期目标。为此，鲁棒对抗性强化学习(RARL)在一个竞争性零和马尔可夫博弈中训练主角来对抗敌对力量，这里的最优解即理性策略对应于纳什均衡。然而，要找到纳什均衡需要面对复杂的鞍点优化问题，对于高维控制尤其难以解决。本文提出了一种新颖的基于熵正则化的对抗性强化学习方法，以减轻鞍点优化问题的复杂性。我们证明，这个熵正则化问题的解对应于量响应均衡(QRE)，它是纳什均衡的推广，并考虑了有界理性，即代理有时会随机执行动作而非最优动作。关键是，连接熵正则化和量响应均衡的关系使得训练鲁棒的对抗性强化学习更加鲁棒。",
    "tldr": "本论文提出了一种基于熵正则化的对抗性强化学习方法，通过解决复杂的鞍点优化问题，实现了鲁棒性对抗攻击和分布偏移，从而在强化学习中取得了重要进展。"
}