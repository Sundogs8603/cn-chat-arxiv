{
    "title": "Identifying Linear Relational Concepts in Large Language Models",
    "abstract": "arXiv:2311.08968v2 Announce Type: replace-cross  Abstract: Transformer language models (LMs) have been shown to represent concepts as directions in the latent space of hidden activations. However, for any human-interpretable concept, how can we find its direction in the latent space? We present a technique called linear relational concepts (LRC) for finding concept directions corresponding to human-interpretable concepts by first modeling the relation between subject and object as a linear relational embedding (LRE). We find that inverting the LRE and using earlier object layers results in a powerful technique for finding concept directions that outperforms standard black-box probing classifiers. We evaluate LRCs on their performance as concept classifiers as well as their ability to causally change model output.",
    "link": "https://arxiv.org/abs/2311.08968",
    "context": "Title: Identifying Linear Relational Concepts in Large Language Models\nAbstract: arXiv:2311.08968v2 Announce Type: replace-cross  Abstract: Transformer language models (LMs) have been shown to represent concepts as directions in the latent space of hidden activations. However, for any human-interpretable concept, how can we find its direction in the latent space? We present a technique called linear relational concepts (LRC) for finding concept directions corresponding to human-interpretable concepts by first modeling the relation between subject and object as a linear relational embedding (LRE). We find that inverting the LRE and using earlier object layers results in a powerful technique for finding concept directions that outperforms standard black-box probing classifiers. We evaluate LRCs on their performance as concept classifiers as well as their ability to causally change model output.",
    "path": "papers/23/11/2311.08968.json",
    "total_tokens": 792,
    "translated_title": "在大型语言模型中识别线性关系概念",
    "translated_abstract": "Transformer语言模型(LMs)已被证明可以将概念表示为隐藏激活的潜在空间中的方向。然而，对于任何可由人类解释的概念，我们如何在潜在空间中找到其方向呢？我们提出了一种称为线性关系概念(LRC)的技术，通过首先建模主体和客体之间的关系为线性关系嵌入(LRE)来找到与人类可解释概念对应的概念方向。我们发现，反转LRE并使用较早的客体层会导致一种强大的技术，用于找到胜过标准黑盒探测分类器的概念方向。我们评估了LRC作为概念分类器的性能，以及它们改变模型输出的因果能力。",
    "tldr": "通过线性关系概念(LRC)技术，可以在大型语言模型的隐藏激活潜在空间中找到与人类可解释概念对应的概念方向，这种技术超越了标准黑盒探测分类器。",
    "en_tdlr": "The technique of Linear Relational Concepts (LRC) allows for identifying concept directions corresponding to human-interpretable concepts in the latent space of hidden activations in large language models, surpassing standard black-box probing classifiers."
}