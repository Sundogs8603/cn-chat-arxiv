{
    "title": "Remember what you did so you know what to do next. (arXiv:2311.01468v1 [cs.CL])",
    "abstract": "We explore using a moderately sized large language model (GPT-J 6B parameters) to create a plan for a simulated robot to achieve 30 classes of goals in ScienceWorld, a text game simulator for elementary science experiments. Previously published empirical work claimed that large language models (LLMs) are a poor fit (Wang et al., 2022) compared to reinforcement learning. Using the Markov assumption (a single previous step), the LLM outperforms the reinforcement learning-based approach by a factor of 1.4. When we fill the LLM's input buffer with as many prior steps as possible, improvement rises to 3.5x. Even when training on only 6.5% of the training data, we observe a 2.2x improvement over the reinforcement-learning-based approach. Our experiments show that performance varies widely across the 30 classes of actions, indicating that averaging over tasks can hide significant performance issues. In work contemporaneous with ours, Lin et al. (2023) demonstrated a two-part approach (SwiftSa",
    "link": "http://arxiv.org/abs/2311.01468",
    "context": "Title: Remember what you did so you know what to do next. (arXiv:2311.01468v1 [cs.CL])\nAbstract: We explore using a moderately sized large language model (GPT-J 6B parameters) to create a plan for a simulated robot to achieve 30 classes of goals in ScienceWorld, a text game simulator for elementary science experiments. Previously published empirical work claimed that large language models (LLMs) are a poor fit (Wang et al., 2022) compared to reinforcement learning. Using the Markov assumption (a single previous step), the LLM outperforms the reinforcement learning-based approach by a factor of 1.4. When we fill the LLM's input buffer with as many prior steps as possible, improvement rises to 3.5x. Even when training on only 6.5% of the training data, we observe a 2.2x improvement over the reinforcement-learning-based approach. Our experiments show that performance varies widely across the 30 classes of actions, indicating that averaging over tasks can hide significant performance issues. In work contemporaneous with ours, Lin et al. (2023) demonstrated a two-part approach (SwiftSa",
    "path": "papers/23/11/2311.01468.json",
    "total_tokens": 1114,
    "translated_title": "记住你所做的，这样你就知道接下来该做什么。",
    "translated_abstract": "我们探讨使用一个中等规模的大型语言模型（GPT-J 6B参数），为模拟机器人在ScienceWorld中实现30类目标（一个用于小学科学实验的文本游戏模拟器）制定计划。先前发表的实证研究声称，与强化学习相比，大型语言模型（LLMs）的适用性较差（Wang等，2022）。使用马尔可夫假设（单个前一个步骤），LLM的性能超过基于强化学习的方法1.4倍。当我们尽可能多地填充LLM的输入缓冲区时，改进效果提高到3.5倍。即使只对6.5%的训练数据进行训练，我们观察到与基于强化学习的方法相比，性能提高了2.2倍。我们的实验显示，对于30类动作，性能差异很大，表明对任务进行平均可能会隐藏显著的性能问题。与我们同时进行的Lin等人（2023）的工作证明了一种两部分方法（SwiftSa）",
    "tldr": "本文通过使用一个大型语言模型（LLM）为模拟机器人制定计划，在ScienceWorld中实现30类目标。实验结果显示，LLM在马尔可夫假设的情况下比强化学习方法的性能提高了1.4倍，当填充尽可能多的先前步骤时提高到3.5倍，即使只训练了6.5%的数据，也比基于强化学习方法的性能提高了2.2倍。不同类别的动作表现差异很大，说明平均任务可能会隐藏性能问题。",
    "en_tdlr": "This paper explores the use of a large language model (LLM) to create a plan for a simulated robot in achieving 30 classes of goals. The experiments show that the LLM outperforms reinforcement learning by 1.4x under the Markov assumption, and the improvement rises to 3.5x when filling the input buffer with more prior steps. Even with only 6.5% of the training data, the LLM achieves a 2.2x improvement over the reinforcement-learning-based approach. Variations in performance across different action classes indicate that averaging over tasks can hide significant performance issues."
}