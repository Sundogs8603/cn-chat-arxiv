{
    "title": "Ask One More Time: Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios",
    "abstract": "arXiv:2311.08154v2 Announce Type: replace-cross  Abstract: Although chain-of-thought (CoT) prompting combined with language models has achieved encouraging results on complex reasoning tasks, the naive greedy decoding used in CoT prompting usually causes the repetitiveness and local optimality. To address this shortcoming, ensemble-optimization tries to obtain multiple reasoning paths to get the final answer assembly. However, current ensemble-optimization methods either simply employ rule-based post-processing such as \\textit{self-consistency}, or train an additional model based on several task-related human annotations to select the best one among multiple reasoning paths, yet fail to generalize to realistic settings where the type of input questions is unknown or the answer format of reasoning paths is unknown. To avoid their limitations, we propose \\textbf{Self-Agreement}, a generalizable ensemble-optimization method applying in almost all scenarios where the type of input question",
    "link": "https://arxiv.org/abs/2311.08154",
    "context": "Title: Ask One More Time: Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios\nAbstract: arXiv:2311.08154v2 Announce Type: replace-cross  Abstract: Although chain-of-thought (CoT) prompting combined with language models has achieved encouraging results on complex reasoning tasks, the naive greedy decoding used in CoT prompting usually causes the repetitiveness and local optimality. To address this shortcoming, ensemble-optimization tries to obtain multiple reasoning paths to get the final answer assembly. However, current ensemble-optimization methods either simply employ rule-based post-processing such as \\textit{self-consistency}, or train an additional model based on several task-related human annotations to select the best one among multiple reasoning paths, yet fail to generalize to realistic settings where the type of input questions is unknown or the answer format of reasoning paths is unknown. To avoid their limitations, we propose \\textbf{Self-Agreement}, a generalizable ensemble-optimization method applying in almost all scenarios where the type of input question",
    "path": "papers/23/11/2311.08154.json",
    "total_tokens": 837,
    "translated_title": "再问一次：自一致性改善语言模型在（几乎）所有场景中的推理",
    "translated_abstract": "尽管思维链（CoT）提示结合语言模型在复杂推理任务上取得了令人鼓舞的结果，但CoT提示中通常使用的贪婪解码会导致重复性和局部最优性。为解决这一缺点，集成优化尝试获得多个推理路径以得到最终答案集成。然而，当前的集成优化方法要么简单地采用基于规则的后处理，比如“自一致性”，要么训练一个基于几个与任务相关的人类注释的附加模型来在多个推理路径中选择最佳路径，但未能推广到现实设置，其中输入问题类型未知或推理路径的答案格式未知。为了避免它们的局限性，我们提出了“自一致性”，这是一种通用的集成优化方法，在几乎所有情景中适用，其中输入问题的类型...",
    "tldr": "自一致性是一种通用的集成优化方法，可以应用于几乎所有情景中，能够解决语言模型推理中的重复性和局部最优性问题。",
    "en_tdlr": "Self-Agreement is a generalizable ensemble-optimization method applicable in almost all scenarios to address the issues of repetitiveness and local optimality in language model reasoning."
}