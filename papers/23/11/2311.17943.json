{
    "title": "LayerCollapse: Adaptive compression of neural networks",
    "abstract": "Handling the ever-increasing scale of contemporary deep learning and transformer-based models poses a significant challenge. Overparameterized Transformer networks outperform prior art in Natural Language processing and Computer Vision. These models contain hundreds of millions of parameters, demanding significant computational resources and making them prone to overfitting. In this work we present LayerCollapse, a form of structured pruning to reduce the depth of fully connected layers. We develop a novel regularizer allowing for post-training compression without finetuning, while having limited impact on performance. LayerCollapse controls model expressiveness with regularization on the activations between fully connected layers, modulating the linearity of activation functions. A linear activation function reduces the rank of the transformation to the rank of the corresponding linear transformation. We demonstrate the effectiveness of LayerCollapse by showing its compression capabil",
    "link": "https://arxiv.org/abs/2311.17943",
    "context": "Title: LayerCollapse: Adaptive compression of neural networks\nAbstract: Handling the ever-increasing scale of contemporary deep learning and transformer-based models poses a significant challenge. Overparameterized Transformer networks outperform prior art in Natural Language processing and Computer Vision. These models contain hundreds of millions of parameters, demanding significant computational resources and making them prone to overfitting. In this work we present LayerCollapse, a form of structured pruning to reduce the depth of fully connected layers. We develop a novel regularizer allowing for post-training compression without finetuning, while having limited impact on performance. LayerCollapse controls model expressiveness with regularization on the activations between fully connected layers, modulating the linearity of activation functions. A linear activation function reduces the rank of the transformation to the rank of the corresponding linear transformation. We demonstrate the effectiveness of LayerCollapse by showing its compression capabil",
    "path": "papers/23/11/2311.17943.json",
    "total_tokens": 829,
    "translated_title": "LayerCollapse: 自适应压缩神经网络",
    "translated_abstract": "处理当代深度学习和基于transformer的模型不断增长的规模是一个重大挑战。超参数化的Transformer网络在自然语言处理和计算机视觉方面的业绩超过了先前的技术。这些模型含有数亿个参数，需要大量的计算资源，并容易过拟合。在这项工作中，我们提出了LayerCollapse，一种结构化剪枝的形式，用于减少全连接层的深度。我们开发了一种新的正则化项，允许在不进行微调的情况下进行训练后压缩，并对性能产生有限的影响。LayerCollapse通过对全连接层之间的激活进行正则化，调节激活函数的线性度来控制模型的表达能力。线性激活函数将线性转换的秩降低到相应线性转换的秩。我们通过展示LayerCollapse的压缩能力来证明其有效性。",
    "tldr": "LayerCollapse是一种自适应压缩神经网络的方法，通过结构化剪枝来减少全连接层的深度，而不需要进行微调，并且对性能影响有限。该方法通过正则化激活函数的线性度来控制模型的表达能力。",
    "en_tdlr": "LayerCollapse is an adaptive compression method for neural networks that reduces the depth of fully connected layers through structured pruning without requiring finetuning, with limited impact on performance. It controls model expressiveness by regulating the linearity of activation functions."
}