{
    "title": "Are You Sure? Challenging LLMs Leads to Performance Drops in The FlipFlop Experiment",
    "abstract": "arXiv:2311.08596v2 Announce Type: replace  Abstract: The interactive nature of Large Language Models (LLMs) theoretically allows models to refine and improve their answers, yet systematic analysis of the multi-turn behavior of LLMs remains limited. In this paper, we propose the FlipFlop experiment: in the first round of the conversation, an LLM completes a classification task. In a second round, the LLM is challenged with a follow-up phrase like \"Are you sure?\", offering an opportunity for the model to reflect on its initial answer, and decide whether to confirm or flip its answer. A systematic study of ten LLMs on seven classification tasks reveals that models flip their answers on average 46% of the time and that all models see a deterioration of accuracy between their first and final prediction, with an average drop of 17% (the FlipFlop effect). We conduct finetuning experiments on an open-source LLM and find that finetuning on synthetically created data can mitigate - reducing perf",
    "link": "https://arxiv.org/abs/2311.08596",
    "context": "Title: Are You Sure? Challenging LLMs Leads to Performance Drops in The FlipFlop Experiment\nAbstract: arXiv:2311.08596v2 Announce Type: replace  Abstract: The interactive nature of Large Language Models (LLMs) theoretically allows models to refine and improve their answers, yet systematic analysis of the multi-turn behavior of LLMs remains limited. In this paper, we propose the FlipFlop experiment: in the first round of the conversation, an LLM completes a classification task. In a second round, the LLM is challenged with a follow-up phrase like \"Are you sure?\", offering an opportunity for the model to reflect on its initial answer, and decide whether to confirm or flip its answer. A systematic study of ten LLMs on seven classification tasks reveals that models flip their answers on average 46% of the time and that all models see a deterioration of accuracy between their first and final prediction, with an average drop of 17% (the FlipFlop effect). We conduct finetuning experiments on an open-source LLM and find that finetuning on synthetically created data can mitigate - reducing perf",
    "path": "papers/23/11/2311.08596.json",
    "total_tokens": 888,
    "translated_title": "您确定吗？挑战LLMs导致FlipFlop实验中的性能下降",
    "translated_abstract": "大型语言模型（LLMs）的交互性理论上允许模型完善和改进其答案，然而对LLMs的多轮行为的系统分析仍受限制。本文提出了FlipFlop实验：在对话的第一轮中，一个LLM完成一个分类任务。在第二轮中，LLM会受到一个追问，比如“您确定吗？”，为模型提供机会反思其初始答案，并决定是确认还是改变答案。对七个分类任务上的十个LLMs的系统研究显示，模型平均有46%的概率改变其答案，并且所有模型在第一次和最终预测之间看到准确性下降，平均下降了17%（FlipFlop效应）。我们对一个开源LLM进行微调实验，发现在合成数据上进行微调可以缓解--降低了性能。",
    "tldr": "本研究通过FlipFlop实验揭示了当挑战LLMs让其反思初始答案时，模型会平均有46%的概率改变答案，所有模型在第一次和最终预测之间表现出准确性下降的现象。",
    "en_tdlr": "This study reveals through the FlipFlop experiment that challenging LLMs to reflect on their initial answers results in an average of 46% of models changing answers, with all models showing a drop in accuracy between the first and final predictions."
}