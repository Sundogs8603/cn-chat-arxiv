{
    "title": "Emergence of Collective Open-Ended Exploration from Decentralized Meta-Reinforcement Learning. (arXiv:2311.00651v1 [cs.MA])",
    "abstract": "Recent works have proven that intricate cooperative behaviors can emerge in agents trained using meta reinforcement learning on open ended task distributions using self-play. While the results are impressive, we argue that self-play and other centralized training techniques do not accurately reflect how general collective exploration strategies emerge in the natural world: through decentralized training and over an open-ended distribution of tasks. In this work we therefore investigate the emergence of collective exploration strategies, where several agents meta-learn independent recurrent policies on an open ended distribution of tasks. To this end we introduce a novel environment with an open ended procedurally generated task space which dynamically combines multiple subtasks sampled from five diverse task types to form a vast distribution of task trees. We show that decentralized agents trained in our environment exhibit strong generalization abilities when confronted with novel obj",
    "link": "http://arxiv.org/abs/2311.00651",
    "context": "Title: Emergence of Collective Open-Ended Exploration from Decentralized Meta-Reinforcement Learning. (arXiv:2311.00651v1 [cs.MA])\nAbstract: Recent works have proven that intricate cooperative behaviors can emerge in agents trained using meta reinforcement learning on open ended task distributions using self-play. While the results are impressive, we argue that self-play and other centralized training techniques do not accurately reflect how general collective exploration strategies emerge in the natural world: through decentralized training and over an open-ended distribution of tasks. In this work we therefore investigate the emergence of collective exploration strategies, where several agents meta-learn independent recurrent policies on an open ended distribution of tasks. To this end we introduce a novel environment with an open ended procedurally generated task space which dynamically combines multiple subtasks sampled from five diverse task types to form a vast distribution of task trees. We show that decentralized agents trained in our environment exhibit strong generalization abilities when confronted with novel obj",
    "path": "papers/23/11/2311.00651.json",
    "total_tokens": 856,
    "translated_title": "分布式元强化学习中的集体自发开放式探索的出现",
    "translated_abstract": "最近的研究证明，在自我对战的开放式任务分布中，通过使用元强化学习来训练的智能体可以产生复杂的合作行为。虽然结果令人印象深刻，但我们认为，自我对战和其他集中化训练技术并不能准确地反映自然界中普遍的集体探索策略是如何出现的：通过分布式训练和对任务的无限分布进行训练。因此，在这项工作中，我们研究了集体探索策略的出现，其中多个智能体在一个无限的任务分布中独立地元学习循环策略。为此，我们引入了一个新的环境，它具有一个无限的过程生成的任务空间，动态组合了从五种不同类型的任务中抽样的多个子任务，形成了一个庞大的任务树分布。我们展示了在我们的环境中训练的分散智能体在面对新的目标时展示出强大的泛化能力。",
    "tldr": "通过分布式元强化学习在开放式任务分布上训练的智能体展现了强大的集体探索能力，从而产生了复杂的合作行为。"
}