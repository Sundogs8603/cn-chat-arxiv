{
    "title": "OFA: A Framework of Initializing Unseen Subword Embeddings for Efficient Large-scale Multilingual Continued Pretraining",
    "abstract": "arXiv:2311.08849v2 Announce Type: replace  Abstract: Instead of pretraining multilingual language models from scratch, a more efficient method is to adapt existing pretrained language models (PLMs) to new languages via vocabulary extension and continued pretraining. However, this method usually randomly initializes the embeddings of new subwords and introduces substantially more embedding parameters to the model, thus weakening the efficiency. To address these issues, we propose a novel framework: $\\textbf{O}$ne $\\textbf{F}$or $\\textbf{A}$ll ($\\textbf{OFA}$), which wisely initializes the embeddings of unseen subwords and thus can adapt a PLM to multiple languages efficiently and effectively. OFA takes advantage of external well-aligned multilingual static word vectors and injects the alignment knowledge into the subword embeddings. In addition, OFA applies matrix factorization and replaces the cumbersome embeddings with two lower-dimensional matrices, which largely reduces the number o",
    "link": "https://arxiv.org/abs/2311.08849",
    "context": "Title: OFA: A Framework of Initializing Unseen Subword Embeddings for Efficient Large-scale Multilingual Continued Pretraining\nAbstract: arXiv:2311.08849v2 Announce Type: replace  Abstract: Instead of pretraining multilingual language models from scratch, a more efficient method is to adapt existing pretrained language models (PLMs) to new languages via vocabulary extension and continued pretraining. However, this method usually randomly initializes the embeddings of new subwords and introduces substantially more embedding parameters to the model, thus weakening the efficiency. To address these issues, we propose a novel framework: $\\textbf{O}$ne $\\textbf{F}$or $\\textbf{A}$ll ($\\textbf{OFA}$), which wisely initializes the embeddings of unseen subwords and thus can adapt a PLM to multiple languages efficiently and effectively. OFA takes advantage of external well-aligned multilingual static word vectors and injects the alignment knowledge into the subword embeddings. In addition, OFA applies matrix factorization and replaces the cumbersome embeddings with two lower-dimensional matrices, which largely reduces the number o",
    "path": "papers/23/11/2311.08849.json",
    "total_tokens": 847,
    "translated_title": "OFA：一种用于初始化未见子词嵌入的框架，以实现高效大规模多语言持续预训练",
    "translated_abstract": "与从头开始预训练多语言语言模型不同，一种更高效的方法是通过词汇扩展和持续预训练来适应现有的预训练语言模型（PLMs）到新的语言。本文提出了一种新颖的框架：OFA（One For All），它聪明地初始化了未见子词的嵌入，从而可以高效有效地将PLM适应到多种语言。OFA利用外部对齐良好的多语言静态词向量，并将对齐知识注入到子词嵌入中。此外，OFA应用矩阵分解，并用两个低维矩阵替换繁琐的嵌入，大大减少了模型的嵌入参数数量。",
    "tldr": "OFA框架通过智能初始化未见子词的嵌入，结合外部多语言静态词向量和矩阵分解，有效实现多语言适应并大幅减少模型嵌入参数数量",
    "en_tdlr": "The OFA framework efficiently adapts pretrained language models to multiple languages by intelligently initializing unseen subword embeddings, leveraging external multilingual static word vectors, and applying matrix factorization to significantly reduce the number of embedding parameters in the model."
}