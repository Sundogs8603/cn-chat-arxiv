{
    "title": "Self-Contradictory Reasoning Evaluation and Detection",
    "abstract": "arXiv:2311.09603v2 Announce Type: replace  Abstract: In a plethora of recent work, large language models (LLMs) demonstrated impressive reasoning ability, but many proposed downstream reasoning tasks focus on performance-wise evaluation. Two fundamental questions persist: 1) how reliable is the quality of reasoning, and 2) can models detect unreliable reasoning? In this paper, we investigate self-contradictory (Self-Contra) reasoning, where the model reasoning does not support predictions. To address 1), we assess the Self-Contra rate across four datasets and delve into finer-grained categories of Self-Contra reasoning. We find that LLMs often contradict themselves when performing reasoning tasks that involve contextual information understanding or commonsense. Importantly, a higher accuracy does not necessarily correspond to a lower Self-Contra rate. The model may appear to generate correct answers but it may take shortcuts in reasoning or skip over contextual evidence, thereby displa",
    "link": "https://arxiv.org/abs/2311.09603",
    "context": "Title: Self-Contradictory Reasoning Evaluation and Detection\nAbstract: arXiv:2311.09603v2 Announce Type: replace  Abstract: In a plethora of recent work, large language models (LLMs) demonstrated impressive reasoning ability, but many proposed downstream reasoning tasks focus on performance-wise evaluation. Two fundamental questions persist: 1) how reliable is the quality of reasoning, and 2) can models detect unreliable reasoning? In this paper, we investigate self-contradictory (Self-Contra) reasoning, where the model reasoning does not support predictions. To address 1), we assess the Self-Contra rate across four datasets and delve into finer-grained categories of Self-Contra reasoning. We find that LLMs often contradict themselves when performing reasoning tasks that involve contextual information understanding or commonsense. Importantly, a higher accuracy does not necessarily correspond to a lower Self-Contra rate. The model may appear to generate correct answers but it may take shortcuts in reasoning or skip over contextual evidence, thereby displa",
    "path": "papers/23/11/2311.09603.json",
    "total_tokens": 892,
    "translated_title": "自相矛盾推理评估与检测",
    "translated_abstract": "在最近的大量工作中，大型语言模型展示了令人印象深刻的推理能力，但许多提出的下游推理任务主要关注性能评估。然而，仍然存在两个基本问题：1）推理质量有多可靠，2）模型能否检测到不可靠的推理？本文研究了自相矛盾（Self-Contra）推理，即模型推理不支持预测的情况。为了解决第一个问题，我们评估了四个数据集中的Self-Contra率，并深入探讨了自相矛盾推理的更细粒度类别。我们发现，大型语言模型在进行涉及上下文信息理解或常识的推理任务时经常自相矛盾。重要的是，更高的准确性并不一定对应更低的自相矛盾率。模型可能会产生正确答案，但在推理过程中可能会采取捷径或忽略上下文证据。",
    "tldr": "研究了大型语言模型在推理任务中自相矛盾的现象，发现在涉及上下文信息理解或常识的任务中经常存在自相矛盾，而高准确性并不总是对应较低的自相矛盾率。",
    "en_tdlr": "Investigated the phenomenon of self-contradictory reasoning in large language models in reasoning tasks, finding that self-contradiction often exists in tasks involving contextual information understanding or common sense, and higher accuracy does not always correspond to lower self-contradiction rates."
}