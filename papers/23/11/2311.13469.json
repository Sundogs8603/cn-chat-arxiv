{
    "title": "Span-Based Optimal Sample Complexity for Average Reward MDPs",
    "abstract": "arXiv:2311.13469v2 Announce Type: replace  Abstract: We study the sample complexity of learning an $\\varepsilon$-optimal policy in an average-reward Markov decision process (MDP) under a generative model. We establish the complexity bound $\\widetilde{O}\\left(SA\\frac{H}{\\varepsilon^2} \\right)$, where $H$ is the span of the bias function of the optimal policy and $SA$ is the cardinality of the state-action space. Our result is the first that is minimax optimal (up to log factors) in all parameters $S,A,H$ and $\\varepsilon$, improving on existing work that either assumes uniformly bounded mixing times for all policies or has suboptimal dependence on the parameters.   Our result is based on reducing the average-reward MDP to a discounted MDP. To establish the optimality of this reduction, we develop improved bounds for $\\gamma$-discounted MDPs, showing that $\\widetilde{O}\\left(SA\\frac{H}{(1-\\gamma)^2\\varepsilon^2} \\right)$ samples suffice to learn a $\\varepsilon$-optimal policy in weakly c",
    "link": "https://arxiv.org/abs/2311.13469",
    "context": "Title: Span-Based Optimal Sample Complexity for Average Reward MDPs\nAbstract: arXiv:2311.13469v2 Announce Type: replace  Abstract: We study the sample complexity of learning an $\\varepsilon$-optimal policy in an average-reward Markov decision process (MDP) under a generative model. We establish the complexity bound $\\widetilde{O}\\left(SA\\frac{H}{\\varepsilon^2} \\right)$, where $H$ is the span of the bias function of the optimal policy and $SA$ is the cardinality of the state-action space. Our result is the first that is minimax optimal (up to log factors) in all parameters $S,A,H$ and $\\varepsilon$, improving on existing work that either assumes uniformly bounded mixing times for all policies or has suboptimal dependence on the parameters.   Our result is based on reducing the average-reward MDP to a discounted MDP. To establish the optimality of this reduction, we develop improved bounds for $\\gamma$-discounted MDPs, showing that $\\widetilde{O}\\left(SA\\frac{H}{(1-\\gamma)^2\\varepsilon^2} \\right)$ samples suffice to learn a $\\varepsilon$-optimal policy in weakly c",
    "path": "papers/23/11/2311.13469.json",
    "total_tokens": 962,
    "translated_title": "基于跨度的平均回报MDP的最优采样复杂性",
    "translated_abstract": "我们研究了在一个生成模型下学习平均回报马尔可夫决策过程（MDP）中的$\\varepsilon$-最优策略的样本复杂性。我们建立了复杂度界限$\\widetilde{O}\\left(SA\\frac{H}{\\varepsilon^2} \\right)$，其中$H$是最优策略的偏差函数的跨度，$SA$是状态-动作空间的基数。我们的结果是第一个在所有参数$S,A,H$和$\\varepsilon$中（最多对数因子）是极小极大最优的，改进了现有工作，现有工作要么假设所有策略的混合时间均匀有界，要么对参数有次最优的依赖。我们的结果基于将平均回报MDP降级为折扣MDP。为了建立这种降级的最优性，我们为$\\gamma$-折扣MDP开发了改进的界限，表明$\\widetilde{O}\\left(SA\\frac{H}{(1-\\gamma)^2\\varepsilon^2} \\right)$个样本足以学习弱ly c",
    "tldr": "该研究提出了基于跨度的平均回报MDP中学习最优策略的最优样本复杂度界限，是首个在所有参数方面都是极小极大最优的结果。",
    "en_tdlr": "This study presents the optimal sample complexity bound for learning an optimal policy in average-reward MDPs based on span, which is the first result that is minimax optimal in all parameters."
}