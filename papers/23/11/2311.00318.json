{
    "title": "Flooding Regularization for Stable Training of Generative Adversarial Networks. (arXiv:2311.00318v1 [cs.LG])",
    "abstract": "Generative Adversarial Networks (GANs) have shown remarkable performance in image generation. However, GAN training suffers from the problem of instability. One of the main approaches to address this problem is to modify the loss function, often using regularization terms in addition to changing the type of adversarial losses. This paper focuses on directly regularizing the adversarial loss function. We propose a method that applies flooding, an overfitting suppression method in supervised learning, to GANs to directly prevent the discriminator's loss from becoming excessively low. Flooding requires tuning the flood level, but when applied to GANs, we propose that the appropriate range of flood level settings is determined by the adversarial loss function, supported by theoretical analysis of GANs using the binary cross entropy loss. We experimentally verify that flooding stabilizes GAN training and can be combined with other stabilization techniques. We also reveal that by restricting",
    "link": "http://arxiv.org/abs/2311.00318",
    "context": "Title: Flooding Regularization for Stable Training of Generative Adversarial Networks. (arXiv:2311.00318v1 [cs.LG])\nAbstract: Generative Adversarial Networks (GANs) have shown remarkable performance in image generation. However, GAN training suffers from the problem of instability. One of the main approaches to address this problem is to modify the loss function, often using regularization terms in addition to changing the type of adversarial losses. This paper focuses on directly regularizing the adversarial loss function. We propose a method that applies flooding, an overfitting suppression method in supervised learning, to GANs to directly prevent the discriminator's loss from becoming excessively low. Flooding requires tuning the flood level, but when applied to GANs, we propose that the appropriate range of flood level settings is determined by the adversarial loss function, supported by theoretical analysis of GANs using the binary cross entropy loss. We experimentally verify that flooding stabilizes GAN training and can be combined with other stabilization techniques. We also reveal that by restricting",
    "path": "papers/23/11/2311.00318.json",
    "total_tokens": 856,
    "translated_title": "用于稳定训练生成对抗网络的泛化正则化",
    "translated_abstract": "生成对抗网络（GANs）在图像生成方面表现出了显著的性能。然而，GAN训练存在不稳定的问题。解决这个问题的主要方法之一是修改损失函数，通常使用正则化项来改变对抗损失的类型。本文着眼于直接对对抗损失函数进行正则化。我们提出了一种方法，将泛化（过拟合抑制）方法应用于GANs中，直接防止判别器的损失过分降低。泛化需要调整泛化水平，但当应用于GANs时，我们提出适当的泛化水平设置范围由对抗损失函数确定，该论据得到了使用二元交叉熵损失对GANs进行理论分析的支持。我们通过实验证实了泛化稳定了GAN训练，并且可以与其他稳定技术相结合。我们还揭示了通过限制",
    "tldr": "本文在生成对抗网络中直接对对抗损失函数进行正则化，通过应用泛化方法防止判别器的损失过分降低，并通过实验证实了其稳定性。",
    "en_tdlr": "This paper proposes a flooding regularization method for directly regularizing the adversarial loss function in Generative Adversarial Networks (GANs), which prevents the discriminator's loss from becoming excessively low. Experimental results demonstrate that this method stabilizes GAN training."
}