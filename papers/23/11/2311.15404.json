{
    "title": "Applying statistical learning theory to deep learning",
    "abstract": "arXiv:2311.15404v2 Announce Type: replace  Abstract: Although statistical learning theory provides a robust framework to understand supervised learning, many theoretical aspects of deep learning remain unclear, in particular how different architectures may lead to inductive bias when trained using gradient based methods. The goal of these lectures is to provide an overview of some of the main questions that arise when attempting to understand deep learning from a learning theory perspective. After a brief reminder on statistical learning theory and stochastic optimization, we discuss implicit bias in the context of benign overfitting. We then move to a general description of the mirror descent algorithm, showing how we may go back and forth between a parameter space and the corresponding function space for a given learning problem, as well as how the geometry of the learning problem may be represented by a metric tensor. Building on this framework, we provide a detailed study of the im",
    "link": "https://arxiv.org/abs/2311.15404",
    "context": "Title: Applying statistical learning theory to deep learning\nAbstract: arXiv:2311.15404v2 Announce Type: replace  Abstract: Although statistical learning theory provides a robust framework to understand supervised learning, many theoretical aspects of deep learning remain unclear, in particular how different architectures may lead to inductive bias when trained using gradient based methods. The goal of these lectures is to provide an overview of some of the main questions that arise when attempting to understand deep learning from a learning theory perspective. After a brief reminder on statistical learning theory and stochastic optimization, we discuss implicit bias in the context of benign overfitting. We then move to a general description of the mirror descent algorithm, showing how we may go back and forth between a parameter space and the corresponding function space for a given learning problem, as well as how the geometry of the learning problem may be represented by a metric tensor. Building on this framework, we provide a detailed study of the im",
    "path": "papers/23/11/2311.15404.json",
    "total_tokens": 889,
    "translated_title": "将统计学习理论应用于深度学习",
    "translated_abstract": "虽然统计学习理论提供了一个理解监督学习的坚实框架，但深度学习的许多理论方面仍然不清楚，特别是在使用基于梯度的方法进行训练时，不同的架构如何导致归纳偏差。这些讲座的目标是从学习理论的角度提供了解深度学习时出现的一些主要问题的概览。在简要回顾统计学习理论和随机优化之后，我们讨论了在良性过拟合的背景下的隐含偏差。然后，我们对镜像下降算法进行一般性描述，展示了我们如何在给定学习问题的参数空间和相应的函数空间之间来回移动，以及学习问题的几何性质如何可以用度量张量表示。在这个框架的基础上，我们对数量化隐含偏差的具体研究进行了详细探讨。",
    "tldr": "深度学习的理论方面仍不清楚，本研究通过将统计学习理论应用于深度学习，探讨了不同架构在基于梯度方法训练时可能导致的归纳偏差，并详细研究了隐含偏差的数量化表示。",
    "en_tdlr": "Theoretical aspects of deep learning remain unclear, this study applies statistical learning theory to delve into the potential inductive biases different architectures may introduce when trained using gradient based methods, with a detailed exploration of quantifying implicit biases."
}