{
    "title": "Universal Sharpness Dynamics in Neural Network Training: Fixed Point Analysis, Edge of Stability, and Route to Chaos. (arXiv:2311.02076v1 [cs.LG])",
    "abstract": "In gradient descent dynamics of neural networks, the top eigenvalue of the Hessian of the loss (sharpness) displays a variety of robust phenomena throughout training. This includes early time regimes where the sharpness may decrease during early periods of training (sharpness reduction), and later time behavior such as progressive sharpening and edge of stability. We demonstrate that a simple $2$-layer linear network (UV model) trained on a single training example exhibits all of the essential sharpness phenomenology observed in real-world scenarios. By analyzing the structure of dynamical fixed points in function space and the vector field of function updates, we uncover the underlying mechanisms behind these sharpness trends. Our analysis reveals (i) the mechanism behind early sharpness reduction and progressive sharpening, (ii) the required conditions for edge of stability, and (iii) a period-doubling route to chaos on the edge of stability manifold as learning rate is increased. Fi",
    "link": "http://arxiv.org/abs/2311.02076",
    "context": "Title: Universal Sharpness Dynamics in Neural Network Training: Fixed Point Analysis, Edge of Stability, and Route to Chaos. (arXiv:2311.02076v1 [cs.LG])\nAbstract: In gradient descent dynamics of neural networks, the top eigenvalue of the Hessian of the loss (sharpness) displays a variety of robust phenomena throughout training. This includes early time regimes where the sharpness may decrease during early periods of training (sharpness reduction), and later time behavior such as progressive sharpening and edge of stability. We demonstrate that a simple $2$-layer linear network (UV model) trained on a single training example exhibits all of the essential sharpness phenomenology observed in real-world scenarios. By analyzing the structure of dynamical fixed points in function space and the vector field of function updates, we uncover the underlying mechanisms behind these sharpness trends. Our analysis reveals (i) the mechanism behind early sharpness reduction and progressive sharpening, (ii) the required conditions for edge of stability, and (iii) a period-doubling route to chaos on the edge of stability manifold as learning rate is increased. Fi",
    "path": "papers/23/11/2311.02076.json",
    "total_tokens": 959,
    "translated_title": "神经网络训练中的普适锐度动力学：固定点分析、稳定边界和混沌路径",
    "translated_abstract": "在神经网络的梯度下降动力学中，损失函数海森矩阵的最大特征值（锐度）在训练过程中展示出各种稳健的现象。这包括早期时间阶段，在训练的早期阶段锐度可能减小（降低锐度），以及后期行为，如逐渐增加的锐化和稳定边界。我们证明了一个简单的2层线性网络（UV模型），在单个训练样本上训练，展示了在真实场景中观察到的所有关键锐度现象。通过分析函数空间中动力学固定点的结构和函数更新的向量场，我们揭示了这些锐度趋势背后的机制。我们的分析揭示了：(i)早期锐度降低和逐渐增加锐化的机制，(ii)稳定边界所需的条件，以及 (iii)当学习率增加时，稳定边界流形上的倍增混沌路径.",
    "tldr": "本研究通过分析神经网络训练中的锐度动力学，揭示出早期锐度降低、逐渐增加锐化和稳定边界的机制，并发现增大学习率时，稳定边界流形上发生倍增混沌路径。",
    "en_tdlr": "This study analyzes the sharpness dynamics in neural network training, revealing the mechanisms behind early sharpness reduction, progressive sharpening, and edge of stability. It also discovers a period-doubling route to chaos on the edge of stability manifold with increasing learning rate."
}