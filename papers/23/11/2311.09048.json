{
    "title": "GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models",
    "abstract": "This paper presents GRASP, a novel benchmark to evaluate the language grounding and physical understanding capabilities of video-based multimodal large language models (LLMs). This evaluation is accomplished via a two-tier approach leveraging Unity simulations. The first level tests for language grounding by assessing a model's ability to relate simple textual descriptions with visual information. The second level evaluates the model's understanding of \"Intuitive Physics\" principles, such as object permanence and continuity. In addition to releasing the benchmark, we use it to evaluate several state-of-the-art multimodal LLMs. Our evaluation reveals significant shortcomings in the language grounding and intuitive physics capabilities of these models. Although they exhibit at least some grounding capabilities, particularly for colors and shapes, these capabilities depend heavily on the prompting strategy. At the same time, all models perform below or at the chance level of 50% in the In",
    "link": "https://arxiv.org/abs/2311.09048",
    "context": "Title: GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models\nAbstract: This paper presents GRASP, a novel benchmark to evaluate the language grounding and physical understanding capabilities of video-based multimodal large language models (LLMs). This evaluation is accomplished via a two-tier approach leveraging Unity simulations. The first level tests for language grounding by assessing a model's ability to relate simple textual descriptions with visual information. The second level evaluates the model's understanding of \"Intuitive Physics\" principles, such as object permanence and continuity. In addition to releasing the benchmark, we use it to evaluate several state-of-the-art multimodal LLMs. Our evaluation reveals significant shortcomings in the language grounding and intuitive physics capabilities of these models. Although they exhibit at least some grounding capabilities, particularly for colors and shapes, these capabilities depend heavily on the prompting strategy. At the same time, all models perform below or at the chance level of 50% in the In",
    "path": "papers/23/11/2311.09048.json",
    "total_tokens": 911,
    "translated_title": "GRASP: 一种评估多模态语言模型中语言基础和情境物理理解的新型基准测试",
    "translated_abstract": "本文介绍了GRASP，一种评估基于视频的多模态大型语言模型(LLMs)的语言基础和物理理解能力的新型基准测试。通过利用Unity模拟的两层方法进行评估。第一层测试语言基础，通过评估模型将简单的文本描述与视觉信息相关联的能力。第二层评估模型对\"直觉物理学\"原理的理解能力，如物体永恒性和连续性。除了发布基准测试，我们还使用它评估了几种最先进的多模态LLMs。我们的评估揭示了这些模型在语言基础和直觉物理学能力方面存在显著的缺陷。尽管它们表现出了一定的基础能力，尤其是对于颜色和形状，但这些能力很大程度上依赖于启发策略。同时，所有模型在In部分的表现都低于或等于50%的随机水平。",
    "tldr": "GRASP是一个新的基准测试，用于评估视频多模态大型语言模型的语言基础和物理理解能力。通过Unity模拟进行两层评估，揭示这些模型在语言基础和直觉物理学能力方面的显著缺陷。"
}