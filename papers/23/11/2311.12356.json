{
    "title": "Random Linear Projections Loss for Hyperplane-Based Optimization in Neural Networks",
    "abstract": "arXiv:2311.12356v2 Announce Type: replace  Abstract: Advancing loss function design is pivotal for optimizing neural network training and performance. This work introduces Random Linear Projections (RLP) loss, a novel approach that enhances training efficiency by leveraging geometric relationships within the data. Distinct from traditional loss functions that target minimizing pointwise errors, RLP loss operates by minimizing the distance between sets of hyperplanes connecting fixed-size subsets of feature-prediction pairs and feature-label pairs. Our empirical evaluations, conducted across benchmark datasets and synthetic examples, demonstrate that neural networks trained with RLP loss outperform those trained with traditional loss functions, achieving improved performance with fewer data samples, and exhibiting greater robustness to additive noise. We provide theoretical analysis supporting our empirical findings.",
    "link": "https://arxiv.org/abs/2311.12356",
    "context": "Title: Random Linear Projections Loss for Hyperplane-Based Optimization in Neural Networks\nAbstract: arXiv:2311.12356v2 Announce Type: replace  Abstract: Advancing loss function design is pivotal for optimizing neural network training and performance. This work introduces Random Linear Projections (RLP) loss, a novel approach that enhances training efficiency by leveraging geometric relationships within the data. Distinct from traditional loss functions that target minimizing pointwise errors, RLP loss operates by minimizing the distance between sets of hyperplanes connecting fixed-size subsets of feature-prediction pairs and feature-label pairs. Our empirical evaluations, conducted across benchmark datasets and synthetic examples, demonstrate that neural networks trained with RLP loss outperform those trained with traditional loss functions, achieving improved performance with fewer data samples, and exhibiting greater robustness to additive noise. We provide theoretical analysis supporting our empirical findings.",
    "path": "papers/23/11/2311.12356.json",
    "total_tokens": 857,
    "translated_title": "基于超平面优化的神经网络中的随机线性投影损失",
    "translated_abstract": "提出了一种名为随机线性投影（RLP）损失的新方法，通过利用数据中的几何关系来提高训练效率。与传统的旨在最小化逐点误差的损失函数不同，RLP损失通过最小化连接固定大小的特征-预测对和特征-标签对的超平面集之间的距离来操作。我们通过在基准数据集和合成示例上进行的实证评估表明，使用RLP损失训练的神经网络优于使用传统损失函数训练的网络，可以在更少的数据样本下实现更好的性能，并且对于添加噪声表现更强鲁棒性。我们还提供了支持我们实证结果的理论分析。",
    "tldr": "本研究引入了一种名为随机线性投影（RLP）损失的新方法，通过利用数据中的几何关系来提高神经网络训练效率。实证评估表明，使用RLP损失训练的神经网络优于传统损失函数训练的网络，在更少的数据样本下实现更好的性能，并且对于添加噪声表现更强鲁棒性。",
    "en_tdlr": "This paper introduces a novel approach called Random Linear Projections (RLP) loss, which enhances training efficiency in neural networks by leveraging geometric relationships within the data. Empirical evaluations demonstrate that neural networks trained with RLP loss outperform those trained with traditional loss functions, achieving improved performance with fewer data samples and exhibiting greater robustness to additive noise."
}