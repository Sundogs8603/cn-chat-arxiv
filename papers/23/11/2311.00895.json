{
    "title": "In-Context Prompt Editing For Conditional Audio Generation. (arXiv:2311.00895v1 [cs.SD])",
    "abstract": "Distributional shift is a central challenge in the deployment of machine learning models as they can be ill-equipped for real-world data. This is particularly evident in text-to-audio generation where the encoded representations are easily undermined by unseen prompts, which leads to the degradation of generated audio -- the limited set of the text-audio pairs remains inadequate for conditional audio generation in the wild as user prompts are under-specified. In particular, we observe a consistent audio quality degradation in generated audio samples with user prompts, as opposed to training set prompts. To this end, we present a retrieval-based in-context prompt editing framework that leverages the training captions as demonstrative exemplars to revisit the user prompts. We show that the framework enhanced the audio quality across the set of collected user prompts, which were edited with reference to the training captions as exemplars.",
    "link": "http://arxiv.org/abs/2311.00895",
    "context": "Title: In-Context Prompt Editing For Conditional Audio Generation. (arXiv:2311.00895v1 [cs.SD])\nAbstract: Distributional shift is a central challenge in the deployment of machine learning models as they can be ill-equipped for real-world data. This is particularly evident in text-to-audio generation where the encoded representations are easily undermined by unseen prompts, which leads to the degradation of generated audio -- the limited set of the text-audio pairs remains inadequate for conditional audio generation in the wild as user prompts are under-specified. In particular, we observe a consistent audio quality degradation in generated audio samples with user prompts, as opposed to training set prompts. To this end, we present a retrieval-based in-context prompt editing framework that leverages the training captions as demonstrative exemplars to revisit the user prompts. We show that the framework enhanced the audio quality across the set of collected user prompts, which were edited with reference to the training captions as exemplars.",
    "path": "papers/23/11/2311.00895.json",
    "total_tokens": 786,
    "translated_title": "条件音频生成的上下文提示编辑",
    "translated_abstract": "分布偏移是部署机器学习模型的一个核心挑战，因为它们可能对真实世界的数据不适应。这在文本到音频生成中尤为明显，其中编码表示很容易被不可见的提示破坏，导致生成的音频质量下降。有限的文本-音频对集合仍然不足以实现条件音频生成，因为用户的提示信息不明确。我们观察到，与训练集中的提示相比，生成的音频样本中具有用户提示的音频质量一直存在一致的降低。为此，我们提出了一个基于检索的上下文提示编辑框架，利用训练字幕作为示例来重新审视用户提示。我们展示了该框架提高了在收集到的用户提示集合上的音频质量，这些提示是参考训练字幕编辑的。",
    "tldr": "本研究提出了一种基于检索的上下文提示编辑框架，通过利用训练字幕作为示例来改进条件音频生成的音频质量。"
}