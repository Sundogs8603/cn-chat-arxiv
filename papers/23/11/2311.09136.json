{
    "title": "Rescue: Ranking LLM Responses with Partial Ordering to Improve Response Generation",
    "abstract": "arXiv:2311.09136v2 Announce Type: replace  Abstract: Customizing LLMs for a specific task involves distinguishing effective responses from erroneous ones. This skill can be developed using supervised fine-tuning with extensive human preference data. However, obtaining expert-annotated preference data is expensive for most tasks. In this paper, we present a novel method to optimize LLMs using ranking metrics. This method trains the model to prioritize the best responses from a pool of candidates created for a particular task. Rather than a traditional full ordering, we advocate for a partial ordering, as achieving consensus on the perfect order of candidate responses can be challenging. Our partial ordering is more robust, less sensitive to noise, and can be achieved with limited human annotations or through heuristic methods. We test our system's improved response generation ability using benchmark datasets, including the latest multi-document question answering task. We conduct ablati",
    "link": "https://arxiv.org/abs/2311.09136",
    "context": "Title: Rescue: Ranking LLM Responses with Partial Ordering to Improve Response Generation\nAbstract: arXiv:2311.09136v2 Announce Type: replace  Abstract: Customizing LLMs for a specific task involves distinguishing effective responses from erroneous ones. This skill can be developed using supervised fine-tuning with extensive human preference data. However, obtaining expert-annotated preference data is expensive for most tasks. In this paper, we present a novel method to optimize LLMs using ranking metrics. This method trains the model to prioritize the best responses from a pool of candidates created for a particular task. Rather than a traditional full ordering, we advocate for a partial ordering, as achieving consensus on the perfect order of candidate responses can be challenging. Our partial ordering is more robust, less sensitive to noise, and can be achieved with limited human annotations or through heuristic methods. We test our system's improved response generation ability using benchmark datasets, including the latest multi-document question answering task. We conduct ablati",
    "path": "papers/23/11/2311.09136.json",
    "total_tokens": 828,
    "translated_title": "用部分排序对LLM响应进行排名以改善响应生成",
    "translated_abstract": "定制LLMs以适应特定任务涉及将有效响应与错误响应区分开。这种技能可以通过使用大量人类偏好数据进行监督微调来发展。然而，对于大多数任务来说，获取专家注释的偏好数据是昂贵的。在本文中，我们提出了一种使用排名度量来优化LLMs的新方法。该方法训练模型优先考虑为特定任务创建的候选响应池中的最佳响应。我们主张采用部分排序而不是传统的完全排序，因为就候选响应的完美顺序达成共识可能具有挑战性。我们的部分排序更加稳健，对噪声的敏感性较低，并且可以通过有限的人类注释或启发式方法来实现。我们使用基准数据集测试了我们系统的改进响应生成能力，包括最新的多文档问答任务。",
    "tldr": "提出一种使用部分排序来优化LLMs的方法，能够通过训练模型优先考虑特定任务候选响应池中的最佳响应，从而改善响应生成能力。",
    "en_tdlr": "Propose a method to optimize LLMs using partial ordering, which prioritizes the best responses from a pool of candidate responses created for a specific task, enhancing response generation capabilities."
}