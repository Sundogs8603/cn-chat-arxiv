{
    "title": "Preserving the knowledge of long clinical texts using aggregated ensembles of large language models. (arXiv:2311.01571v1 [cs.CL])",
    "abstract": "Clinical texts, such as admission notes, discharge summaries, and progress notes, contain rich and valuable information that can be used for various clinical outcome prediction tasks. However, applying large language models, such as BERT-based models, to clinical texts poses two major challenges: the limitation of input length and the diversity of data sources. This paper proposes a novel method to preserve the knowledge of long clinical texts using aggregated ensembles of large language models. Unlike previous studies which use model ensembling or text aggregation methods separately, we combine ensemble learning with text aggregation and train multiple large language models on two clinical outcome tasks: mortality prediction and length of stay prediction. We show that our method can achieve better results than baselines, ensembling, and aggregation individually, and can improve the performance of large language models while handling long inputs and diverse datasets. We conduct extensi",
    "link": "http://arxiv.org/abs/2311.01571",
    "context": "Title: Preserving the knowledge of long clinical texts using aggregated ensembles of large language models. (arXiv:2311.01571v1 [cs.CL])\nAbstract: Clinical texts, such as admission notes, discharge summaries, and progress notes, contain rich and valuable information that can be used for various clinical outcome prediction tasks. However, applying large language models, such as BERT-based models, to clinical texts poses two major challenges: the limitation of input length and the diversity of data sources. This paper proposes a novel method to preserve the knowledge of long clinical texts using aggregated ensembles of large language models. Unlike previous studies which use model ensembling or text aggregation methods separately, we combine ensemble learning with text aggregation and train multiple large language models on two clinical outcome tasks: mortality prediction and length of stay prediction. We show that our method can achieve better results than baselines, ensembling, and aggregation individually, and can improve the performance of large language models while handling long inputs and diverse datasets. We conduct extensi",
    "path": "papers/23/11/2311.01571.json",
    "total_tokens": 928,
    "translated_title": "使用聚合集成模型保留长篇临床文本的知识",
    "translated_abstract": "临床文本，如入院记录、出院小结和进展记录，包含丰富而宝贵的信息，可用于各种临床结果预测任务。然而，将基于BERT的大型语言模型应用于临床文本面临两个主要挑战：输入长度的限制和数据来源的多样性。本文提出了一种新颖的方法，使用聚合集成的大型语言模型来保留长篇临床文本的知识。与以往研究单独使用模型集成或文本聚合方法不同，我们将集成学习与文本聚合相结合，在两个临床结果预测任务（死亡预测和住院天数预测）上训练多个大型语言模型。我们展示了我们的方法可以比基线、独立的集成和聚合效果更好，并且可以在处理长输入和多样性数据集时提高大型语言模型的性能。",
    "tldr": "本文提出了一种使用聚合集成模型的方法来保留长篇临床文本的知识。与以往方法不同，我们将集成学习与文本聚合相结合，并在两个临床预测任务上训练多个大型语言模型。实验证明，我们的方法可以在处理长输入和多样性数据集时提升大型语言模型的性能。",
    "en_tdlr": "This paper proposes a method using aggregated ensembles of large language models to preserve the knowledge of long clinical texts. Unlike previous approaches, we combine ensemble learning with text aggregation and train multiple models on two clinical outcome prediction tasks. Experimental results show that our method improves the performance of large language models when handling long inputs and diverse datasets."
}