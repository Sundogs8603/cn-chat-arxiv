{
    "title": "The Role of Diverse Replay for Generalisation in Reinforcement Learning. (arXiv:2306.05727v1 [cs.LG])",
    "abstract": "In reinforcement learning (RL), key components of many algorithms are the exploration strategy and replay buffer. These strategies regulate what environment data is collected and trained on and have been extensively studied in the RL literature. In this paper, we investigate the impact of these components in the context of generalisation in multi-task RL. We investigate the hypothesis that collecting and training on more diverse data from the training environment will improve zero-shot generalisation to new environments/tasks. We motivate mathematically and show empirically that generalisation to states that are \"reachable\" during training is improved by increasing the diversity of transitions in the replay buffer. Furthermore, we show empirically that this same strategy also shows improvement for generalisation to similar but \"unreachable\" states and could be due to improved generalisation of latent representations.",
    "link": "http://arxiv.org/abs/2306.05727",
    "context": "Title: The Role of Diverse Replay for Generalisation in Reinforcement Learning. (arXiv:2306.05727v1 [cs.LG])\nAbstract: In reinforcement learning (RL), key components of many algorithms are the exploration strategy and replay buffer. These strategies regulate what environment data is collected and trained on and have been extensively studied in the RL literature. In this paper, we investigate the impact of these components in the context of generalisation in multi-task RL. We investigate the hypothesis that collecting and training on more diverse data from the training environment will improve zero-shot generalisation to new environments/tasks. We motivate mathematically and show empirically that generalisation to states that are \"reachable\" during training is improved by increasing the diversity of transitions in the replay buffer. Furthermore, we show empirically that this same strategy also shows improvement for generalisation to similar but \"unreachable\" states and could be due to improved generalisation of latent representations.",
    "path": "papers/23/06/2306.05727.json",
    "total_tokens": 865,
    "translated_title": "强化学习中多样性重放在泛化中的作用",
    "translated_abstract": "在强化学习中，探索策略和重放缓存是其许多算法的关键组成部分。这些策略规定了要收集和训练哪些环境数据，并在强化学习文献中得到了广泛研究。本文旨在研究这些组件在多任务强化学习中泛化时的影响。我们研究了一个假设：从训练环境中收集和训练更多样化的数据将提高到新环境/任务的零-shot泛化性能。我们通过数学推导和实证研究证明，增加重放缓存中过渡的多样性可以提高到训练过程中“可达到”的状态的泛化性能。此外，我们还证明了这种策略对于类似但“不可达”状态的泛化性能也有所提高，并且可能是由于潜在表示的泛化性能得到了改善。",
    "tldr": "本文研究了在多任务强化学习中，增加重放缓存中数据过渡的多样性可以提高零-shot泛化性能，并且可能通过提高潜在表示的泛化性能来实现这种改善。",
    "en_tdlr": "This paper investigates the impact of increasing the diversity of transitions in the replay buffer on zero-shot generalization in multi-task reinforcement learning, and shows that it can improve generalization to \"reachable\" and even \"unreachable\" states, possibly due to improved generalization of latent representations."
}