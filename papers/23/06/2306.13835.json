{
    "title": "Computron: Serving Distributed Deep Learning Models with Model Parallel Swapping. (arXiv:2306.13835v1 [cs.DC])",
    "abstract": "Many of the most performant deep learning models today in fields like language and image understanding are fine-tuned models that contain billions of parameters. In anticipation of workloads that involve serving many of such large models to handle different tasks, we develop Computron, a system that uses memory swapping to serve multiple distributed models on a shared GPU cluster. Computron implements a model parallel swapping design that takes advantage of the aggregate CPU-GPU link bandwidth of a cluster to speed up model parameter transfers. This design makes swapping large models feasible and can improve resource utilization. We demonstrate that Computron successfully parallelizes model swapping on multiple GPUs, and we test it on randomized workloads to show how it can tolerate real world variability factors like burstiness and skewed request rates. Computron's source code is available at https://github.com/dlzou/computron.",
    "link": "http://arxiv.org/abs/2306.13835",
    "context": "Title: Computron: Serving Distributed Deep Learning Models with Model Parallel Swapping. (arXiv:2306.13835v1 [cs.DC])\nAbstract: Many of the most performant deep learning models today in fields like language and image understanding are fine-tuned models that contain billions of parameters. In anticipation of workloads that involve serving many of such large models to handle different tasks, we develop Computron, a system that uses memory swapping to serve multiple distributed models on a shared GPU cluster. Computron implements a model parallel swapping design that takes advantage of the aggregate CPU-GPU link bandwidth of a cluster to speed up model parameter transfers. This design makes swapping large models feasible and can improve resource utilization. We demonstrate that Computron successfully parallelizes model swapping on multiple GPUs, and we test it on randomized workloads to show how it can tolerate real world variability factors like burstiness and skewed request rates. Computron's source code is available at https://github.com/dlzou/computron.",
    "path": "papers/23/06/2306.13835.json",
    "total_tokens": 822,
    "translated_title": "Computron: 利用模型并行交换服务于分布式深度学习模型",
    "translated_abstract": "当今许多以语言和图像为代表的表现最佳的深度学习模型都是包含数十亿参数的调整型模型。为了应对需要将这些大型模型服务于处理不同任务的工作负载，我们开发了Computron，这个系统使用内存交换在一个共享的GPU集群上服务于多个分布式模型。Computron 实现了一个模型并行交换设计，利用集群的总CPU-GPU链接带宽加快模型参数传输速度。该设计使交换大型模型成为可能，并可以提高资源利用率。我们展示了Computron成功地在多个GPU上并行化了模型交换，并对随机负载进行了测试，展示了如何容忍现实世界中的变化因素，例如突发性和偏斜的请求速率。Computron的源代码可在https://github.com/dlzou/computron中获得。",
    "tldr": "该论文介绍了Computron系统，该系统使用内存交换实现模型并行交换设计来服务于多个分布式深度学习模型，在处理多个大型模型任务时可提高资源利用率和交换速度。",
    "en_tdlr": "This paper introduces the Computron system, which uses memory swapping to implement a model parallel swapping design to serve multiple distributed deep learning models, improving resource utilization and swapping speed in handling multiple large model tasks."
}