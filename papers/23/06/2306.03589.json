{
    "title": "How does over-squashing affect the power of GNNs?. (arXiv:2306.03589v1 [cs.LG])",
    "abstract": "Graph Neural Networks (GNNs) are the state-of-the-art model for machine learning on graph-structured data. The most popular class of GNNs operate by exchanging information between adjacent nodes, and are known as Message Passing Neural Networks (MPNNs). Given their widespread use, understanding the expressive power of MPNNs is a key question. However, existing results typically consider settings with uninformative node features. In this paper, we provide a rigorous analysis to determine which function classes of node features can be learned by an MPNN of a given capacity. We do so by measuring the level of pairwise interactions between nodes that MPNNs allow for. This measure provides a novel quantitative characterization of the so-called over-squashing effect, which is observed to occur when a large volume of messages is aggregated into fixed-size vectors. Using our measure, we prove that, to guarantee sufficient communication between pairs of nodes, the capacity of the MPNN must be l",
    "link": "http://arxiv.org/abs/2306.03589",
    "context": "Title: How does over-squashing affect the power of GNNs?. (arXiv:2306.03589v1 [cs.LG])\nAbstract: Graph Neural Networks (GNNs) are the state-of-the-art model for machine learning on graph-structured data. The most popular class of GNNs operate by exchanging information between adjacent nodes, and are known as Message Passing Neural Networks (MPNNs). Given their widespread use, understanding the expressive power of MPNNs is a key question. However, existing results typically consider settings with uninformative node features. In this paper, we provide a rigorous analysis to determine which function classes of node features can be learned by an MPNN of a given capacity. We do so by measuring the level of pairwise interactions between nodes that MPNNs allow for. This measure provides a novel quantitative characterization of the so-called over-squashing effect, which is observed to occur when a large volume of messages is aggregated into fixed-size vectors. Using our measure, we prove that, to guarantee sufficient communication between pairs of nodes, the capacity of the MPNN must be l",
    "path": "papers/23/06/2306.03589.json",
    "total_tokens": 916,
    "translated_title": "过度压缩如何影响GNN的能力？",
    "translated_abstract": "图神经网络（GNN）是处理图结构数据的机器学习的最先进模型。最流行的GNN类别是通过相邻节点间的信息交换来操作的，称为消息传递神经网络（MPNN）。鉴于它们的广泛应用，了解MPNN的表达能力是一个关键问题。然而，现有结果通常考虑具有无信息节点特征的环境。在本文中，我们提供了一种严格的分析方法，以确定具有一定容量的MPNN可以学习哪些节点特征的函数类别。我们通过测量MPNN允许的节点之间的成对交互水平来实现此目的。该测量提供了一种新的量化特性，即所谓的过度压缩效应，该效应被观察到是当大量的信息聚合成固定大小的向量时发生的。使用我们的测量，我们证明，为了保证节点对之间的充分通信，MPNN的容量必须是...",
    "tldr": "本文通过测量节点之间成对交互的水平，提供了严格的分析，以确定具有一定容量的MPNN可以学习哪些节点特征的函数类别。结果表明，为了保证节点对之间的充分通信，MPNN的容量必须是...",
    "en_tdlr": "This paper provides a rigorous analysis to determine which function classes of node features can be learned by an MPNN of a given capacity by measuring the level of pairwise interactions between nodes that MPNNs allow for. The results show that to ensure sufficient communication between pairs of nodes, the capacity of the MPNN must be..."
}