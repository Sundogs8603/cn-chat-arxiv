{
    "title": "On the Relationship Between RNN Hidden State Vectors and Semantic Ground Truth. (arXiv:2306.16854v1 [cs.LG])",
    "abstract": "We examine the assumption that the hidden-state vectors of recurrent neural networks (RNNs) tend to form clusters of semantically similar vectors, which we dub the clustering hypothesis. While this hypothesis has been assumed in the analysis of RNNs in recent years, its validity has not been studied thoroughly on modern neural network architectures. We examine the clustering hypothesis in the context of RNNs that were trained to recognize regular languages. This enables us to draw on perfect ground-truth automata in our evaluation, against which we can compare the RNN's accuracy and the distribution of the hidden-state vectors.  We start with examining the (piecewise linear) separability of an RNN's hidden-state vectors into semantically different classes. We continue the analysis by computing clusters over the hidden-state vector space with multiple state-of-the-art unsupervised clustering approaches. We formally analyze the accuracy of computed clustering functions and the validity o",
    "link": "http://arxiv.org/abs/2306.16854",
    "context": "Title: On the Relationship Between RNN Hidden State Vectors and Semantic Ground Truth. (arXiv:2306.16854v1 [cs.LG])\nAbstract: We examine the assumption that the hidden-state vectors of recurrent neural networks (RNNs) tend to form clusters of semantically similar vectors, which we dub the clustering hypothesis. While this hypothesis has been assumed in the analysis of RNNs in recent years, its validity has not been studied thoroughly on modern neural network architectures. We examine the clustering hypothesis in the context of RNNs that were trained to recognize regular languages. This enables us to draw on perfect ground-truth automata in our evaluation, against which we can compare the RNN's accuracy and the distribution of the hidden-state vectors.  We start with examining the (piecewise linear) separability of an RNN's hidden-state vectors into semantically different classes. We continue the analysis by computing clusters over the hidden-state vector space with multiple state-of-the-art unsupervised clustering approaches. We formally analyze the accuracy of computed clustering functions and the validity o",
    "path": "papers/23/06/2306.16854.json",
    "total_tokens": 880,
    "translated_title": "关于循环神经网络隐藏状态向量和语义基本事实之间的关系",
    "translated_abstract": "我们考察了循环神经网络（RNNs）的隐藏状态向量是否倾向于形成语义相似向量的簇群，这个假设被称为聚类假设。虽然这个假设在近年来对RNNs的分析中已经被假定，但其在现代神经网络架构上的有效性还没有得到充分的研究。我们在训练用于识别常规语言的RNNs的背景下考察了聚类假设。这使我们能够在评估中利用完美的基准自动机，与其比较RNN的准确性和隐藏状态向量的分布。我们首先考察了将RNN的隐藏状态向量在语义上分为不同类别的（分段线性）可分离性。然后，我们通过应用多种最新的无监督聚类方法计算隐藏状态向量空间中的簇群。我们正式分析了计算聚类函数的准确性以及聚类假设的有效性。",
    "tldr": "本研究考察了循环神经网络的隐藏状态向量是否具有语义相似的聚类结构，并通过在训练中识别常规语言的RNNs来验证聚类假设的有效性。",
    "en_tdlr": "This study examines whether the hidden-state vectors of recurrent neural networks tend to form clusters of semantically similar vectors, and validates this clustering hypothesis by training RNNs to recognize regular languages."
}