{
    "title": "Towards Fair and Explainable AI using a Human-Centered AI Approach. (arXiv:2306.07427v1 [cs.CY])",
    "abstract": "The rise of machine learning (ML) is accompanied by several high-profile cases that have stressed the need for fairness, accountability, explainability and trust in ML systems. The existing literature has largely focused on fully automated ML approaches that try to optimize for some performance metric. However, human-centric measures like fairness, trust, explainability, etc. are subjective in nature, context-dependent, and might not correlate with conventional performance metrics. To deal with these challenges, we explore a human-centered AI approach that empowers people by providing more transparency and human control.  In this dissertation, we present 5 research projects that aim to enhance explainability and fairness in classification systems and word embeddings. The first project explores the utility/downsides of introducing local model explanations as interfaces for machine teachers (crowd workers). Our study found that adding explanations supports trust calibration for the resul",
    "link": "http://arxiv.org/abs/2306.07427",
    "context": "Title: Towards Fair and Explainable AI using a Human-Centered AI Approach. (arXiv:2306.07427v1 [cs.CY])\nAbstract: The rise of machine learning (ML) is accompanied by several high-profile cases that have stressed the need for fairness, accountability, explainability and trust in ML systems. The existing literature has largely focused on fully automated ML approaches that try to optimize for some performance metric. However, human-centric measures like fairness, trust, explainability, etc. are subjective in nature, context-dependent, and might not correlate with conventional performance metrics. To deal with these challenges, we explore a human-centered AI approach that empowers people by providing more transparency and human control.  In this dissertation, we present 5 research projects that aim to enhance explainability and fairness in classification systems and word embeddings. The first project explores the utility/downsides of introducing local model explanations as interfaces for machine teachers (crowd workers). Our study found that adding explanations supports trust calibration for the resul",
    "path": "papers/23/06/2306.07427.json",
    "total_tokens": 881,
    "translated_title": "采用以人为中心的AI方法实现公正和可解释性的人工智能之路",
    "translated_abstract": "机器学习的崛起伴随着几个高调案例的出现，强调了在ML系统中需要公正性、问责制、可解释性和信任度。现有文献主要关注于完全自动化的ML方法，试图优化某些性能指标。然而，像公正性、信任度、可解释性等以人为中心的度量标准在本质上是主观的，依赖于上下文，并且可能与传统的性能指标不相关。为了应对这些挑战，我们探讨了一种以人为中心的AI方法，通过提供更多的透明度和人类控制，赋予人们更多的权力。在这篇论文中，我们介绍了5个研究项目，旨在增强分类系统和词嵌入中的可解释性和公正性。第一个项目探讨了引入本地模型解释作为机器教师（众包工作者）接口的效用/缺陷。我们的研究发现，添加解释可以支持结果信任校准。",
    "tldr": "研究探索了一种以人为中心的AI方法，通过提供更多的透明度和人类控制，赋予人们更多的权力，以实现公正和可解释性的人工智能。",
    "en_tdlr": "The research explores a human-centered AI approach that empowers people by providing more transparency and human control to achieve fairness and explainability in artificial intelligence."
}