{
    "title": "Unveiling the Hessian's Connection to the Decision Boundary. (arXiv:2306.07104v1 [cs.LG])",
    "abstract": "Understanding the properties of well-generalizing minima is at the heart of deep learning research. On the one hand, the generalization of neural networks has been connected to the decision boundary complexity, which is hard to study in the high-dimensional input space. Conversely, the flatness of a minimum has become a controversial proxy for generalization. In this work, we provide the missing link between the two approaches and show that the Hessian top eigenvectors characterize the decision boundary learned by the neural network. Notably, the number of outliers in the Hessian spectrum is proportional to the complexity of the decision boundary. Based on this finding, we provide a new and straightforward approach to studying the complexity of a high-dimensional decision boundary; show that this connection naturally inspires a new generalization measure; and finally, we develop a novel margin estimation technique which, in combination with the generalization measure, precisely identif",
    "link": "http://arxiv.org/abs/2306.07104",
    "context": "Title: Unveiling the Hessian's Connection to the Decision Boundary. (arXiv:2306.07104v1 [cs.LG])\nAbstract: Understanding the properties of well-generalizing minima is at the heart of deep learning research. On the one hand, the generalization of neural networks has been connected to the decision boundary complexity, which is hard to study in the high-dimensional input space. Conversely, the flatness of a minimum has become a controversial proxy for generalization. In this work, we provide the missing link between the two approaches and show that the Hessian top eigenvectors characterize the decision boundary learned by the neural network. Notably, the number of outliers in the Hessian spectrum is proportional to the complexity of the decision boundary. Based on this finding, we provide a new and straightforward approach to studying the complexity of a high-dimensional decision boundary; show that this connection naturally inspires a new generalization measure; and finally, we develop a novel margin estimation technique which, in combination with the generalization measure, precisely identif",
    "path": "papers/23/06/2306.07104.json",
    "total_tokens": 891,
    "translated_title": "揭示Hessian与决策边界的联系",
    "translated_abstract": "理解良好泛化最小值的属性是深度学习研究的核心。一方面，神经网络的泛化与决策边界复杂性有关，而在高维输入空间中难以研究。相反，最小值的平坦性已成为泛化的有争议的代理。在这项工作中，我们提供了两种方法之间的缺失链接，并展示了Hessian的前几个特征向量描述了神经网络学到的决策边界。值得注意的是，Hessian谱中的异常值数量与决策边界的复杂性成正比。基于此发现，我们提供了一种研究高维决策边界复杂性的新而简单的方法; 表明该连接自然地启发了一种新的泛化度量; 最后，我们开发了一种新的边界估计技术，该技术与泛化度量结合使用，精确地确定各种模型的边界。",
    "tldr": "Hessian的前几个特征向量描述了神经网络学到的决策边界，异常值数量与决策边界的复杂性成正比，这启发了一种新的泛化度量和边界估计技术。",
    "en_tdlr": "The top eigenvectors of the Hessian characterize the decision boundary learned by the neural network, and the number of outliers in the Hessian spectrum is proportional to the complexity of the decision boundary. This inspires a new generalization measure and a novel margin estimation technique that precisely identifies the boundaries of various models."
}