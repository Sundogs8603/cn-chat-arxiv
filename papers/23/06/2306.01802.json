{
    "title": "Linear Time GPs for Inferring Latent Trajectories from Neural Spike Trains. (arXiv:2306.01802v1 [q-bio.NC])",
    "abstract": "Latent Gaussian process (GP) models are widely used in neuroscience to uncover hidden state evolutions from sequential observations, mainly in neural activity recordings. While latent GP models provide a principled and powerful solution in theory, the intractable posterior in non-conjugate settings necessitates approximate inference schemes, which may lack scalability. In this work, we propose cvHM, a general inference framework for latent GP models leveraging Hida-Mat\\'ern kernels and conjugate computation variational inference (CVI). With cvHM, we are able to perform variational inference of latent neural trajectories with linear time complexity for arbitrary likelihoods. The reparameterization of stationary kernels using Hida-Mat\\'ern GPs helps us connect the latent variable models that encode prior assumptions through dynamical systems to those that encode trajectory assumptions through GPs. In contrast to previous work, we use bidirectional information filtering, leading to a more",
    "link": "http://arxiv.org/abs/2306.01802",
    "context": "Title: Linear Time GPs for Inferring Latent Trajectories from Neural Spike Trains. (arXiv:2306.01802v1 [q-bio.NC])\nAbstract: Latent Gaussian process (GP) models are widely used in neuroscience to uncover hidden state evolutions from sequential observations, mainly in neural activity recordings. While latent GP models provide a principled and powerful solution in theory, the intractable posterior in non-conjugate settings necessitates approximate inference schemes, which may lack scalability. In this work, we propose cvHM, a general inference framework for latent GP models leveraging Hida-Mat\\'ern kernels and conjugate computation variational inference (CVI). With cvHM, we are able to perform variational inference of latent neural trajectories with linear time complexity for arbitrary likelihoods. The reparameterization of stationary kernels using Hida-Mat\\'ern GPs helps us connect the latent variable models that encode prior assumptions through dynamical systems to those that encode trajectory assumptions through GPs. In contrast to previous work, we use bidirectional information filtering, leading to a more",
    "path": "papers/23/06/2306.01802.json",
    "total_tokens": 984,
    "translated_title": "线性时间高斯过程推断神经脉冲序列中的潜在轨迹",
    "translated_abstract": "潜在高斯过程模型被广泛应用于神经科学中，以从顺序观测中揭示隐藏状态的演化，主要用于神经活动记录。虽然潜在GP模型在理论上提供了一个有原则和有力的解决方案，但在非共轭设置中的不可行后验需要近似推断方案，这些方案可能缺乏可扩展性。在这项工作中，我们提出了cvHM，一种使用Hida-Mat'ern核和共轭计算变分推理（CVI）的潜在GP模型的通用推理框架。使用cvHM，我们能够以线性时间复杂度执行潜在神经轨迹的变分推断，以适应任意的似然函数。使用Hida-Mat'ern GPs对平稳核进行重新参数化，帮助我们将编码先前假设的潜在变量模型与编码轨迹假设的GP连接起来，通过动力系统。与以前的工作不同，我们使用双向信息过滤，导致模型推断更清晰，估计更准确。",
    "tldr": "本论文提出了cvHM，一种使用Hida-Mat'ern核和共轭计算变分推理（CVI）的潜在高斯过程模型的通用推理框架，能够以线性时间复杂度执行潜在神经轨迹的变分推断，以适应任意的似然函数。",
    "en_tdlr": "This paper proposes a general inference framework called cvHM for latent Gaussian process models, which leverages Hida-Mat'ern kernels and conjugate computation variational inference (CVI) to perform linear time complexity variational inference of latent neural trajectories with arbitrary likelihoods."
}