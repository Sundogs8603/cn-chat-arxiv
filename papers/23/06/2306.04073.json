{
    "title": "Patch-level Routing in Mixture-of-Experts is Provably Sample-efficient for Convolutional Neural Networks. (arXiv:2306.04073v1 [cs.LG])",
    "abstract": "In deep learning, mixture-of-experts (MoE) activates one or few experts (sub-networks) on a per-sample or per-token basis, resulting in significant computation reduction. The recently proposed \\underline{p}atch-level routing in \\underline{MoE} (pMoE) divides each input into $n$ patches (or tokens) and sends $l$ patches ($l\\ll n$) to each expert through prioritized routing. pMoE has demonstrated great empirical success in reducing training and inference costs while maintaining test accuracy. However, the theoretical explanation of pMoE and the general MoE remains elusive. Focusing on a supervised classification task using a mixture of two-layer convolutional neural networks (CNNs), we show for the first time that pMoE provably reduces the required number of training samples to achieve desirable generalization (referred to as the sample complexity) by a factor in the polynomial order of $n/l$, and outperforms its single-expert counterpart of the same or even larger capacity. The advantag",
    "link": "http://arxiv.org/abs/2306.04073",
    "context": "Title: Patch-level Routing in Mixture-of-Experts is Provably Sample-efficient for Convolutional Neural Networks. (arXiv:2306.04073v1 [cs.LG])\nAbstract: In deep learning, mixture-of-experts (MoE) activates one or few experts (sub-networks) on a per-sample or per-token basis, resulting in significant computation reduction. The recently proposed \\underline{p}atch-level routing in \\underline{MoE} (pMoE) divides each input into $n$ patches (or tokens) and sends $l$ patches ($l\\ll n$) to each expert through prioritized routing. pMoE has demonstrated great empirical success in reducing training and inference costs while maintaining test accuracy. However, the theoretical explanation of pMoE and the general MoE remains elusive. Focusing on a supervised classification task using a mixture of two-layer convolutional neural networks (CNNs), we show for the first time that pMoE provably reduces the required number of training samples to achieve desirable generalization (referred to as the sample complexity) by a factor in the polynomial order of $n/l$, and outperforms its single-expert counterpart of the same or even larger capacity. The advantag",
    "path": "papers/23/06/2306.04073.json",
    "total_tokens": 1179,
    "translated_title": "Mixture-of-Experts 中的 Patch-level 路由对于 CNN 可以证明具有采样效率",
    "translated_abstract": "在深度学习中，Mixture-of-Experts（MoE）根据每个样本或每个标记激活一个或几个专家（子网络），从而大大减少计算量。最近提出的 Patch-level routing in MoE (pMoE) 将每个输入分为 $n$ 个 patch（或 token） 并通过优先路由将 $l$ 个 patch ($l \\ll n$) 发送到每个专家。pMoE 在维持测试准确性的同时，展现出在减少训练和推断成本方面的巨大实际价值。然而，pMoE 和一般的 MoE 的理论解释仍然含糊不清。在使用两层卷积神经网络（CNN）混合的监督分类任务中，我们首次展示了 pMoE 可以通过多项式阶数中 $n/l$ 的因子证明减少所需的训练样本数以实现理想的泛化（即样本复杂度），并且胜过其具有相同或甚至更大容量的单专家对应项。pMoE 的优势主要归因于两个机制：1）每个专家的有效参数数量大大降低，2）通过 Patch-level routing 选择相关特征的能力使得深度学习网络中特征空间的利用效率提高。我们的分析表明，pMoE 是一个原则性和有效的深度神经网络训练策略，对 pMoE 的理解可以潜在地推进 MoE 的理论研究。",
    "tldr": "Patch-level Routing in Mixture-of-Experts 可以证明在 CNN 中具有采样效率，其用于 patch-level 路由的两个机制分别是专家的有效参数数量显著降低以及选择相关特征，从而提高深度学习神经网络的特征利用效率。",
    "en_tdlr": "Patch-level Routing in Mixture-of-Experts is proven to be sample-efficient for CNNs, and is attributed to two mechanisms: significantly reduced number of effective parameters, and efficient utilization of feature space by selecting relevant features via patch-level routing."
}