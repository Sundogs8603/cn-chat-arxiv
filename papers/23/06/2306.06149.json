{
    "title": "Read, look and detect: Bounding box annotation from image-caption pairs. (arXiv:2306.06149v1 [cs.CV])",
    "abstract": "Various methods have been proposed to detect objects while reducing the cost of data annotation. For instance, weakly supervised object detection (WSOD) methods rely only on image-level annotations during training. Unfortunately, data annotation remains expensive since annotators must provide the categories describing the content of each image and labeling is restricted to a fixed set of categories. In this paper, we propose a method to locate and label objects in an image by using a form of weaker supervision: image-caption pairs. By leveraging recent advances in vision-language (VL) models and self-supervised vision transformers (ViTs), our method is able to perform phrase grounding and object detection in a weakly supervised manner. Our experiments demonstrate the effectiveness of our approach by achieving a 47.51% recall@1 score in phrase grounding on Flickr30k Entities and establishing a new state-of-the-art in object detection by achieving 21.1 mAP 50 and 10.5 mAP 50:95 on MS COC",
    "link": "http://arxiv.org/abs/2306.06149",
    "context": "Title: Read, look and detect: Bounding box annotation from image-caption pairs. (arXiv:2306.06149v1 [cs.CV])\nAbstract: Various methods have been proposed to detect objects while reducing the cost of data annotation. For instance, weakly supervised object detection (WSOD) methods rely only on image-level annotations during training. Unfortunately, data annotation remains expensive since annotators must provide the categories describing the content of each image and labeling is restricted to a fixed set of categories. In this paper, we propose a method to locate and label objects in an image by using a form of weaker supervision: image-caption pairs. By leveraging recent advances in vision-language (VL) models and self-supervised vision transformers (ViTs), our method is able to perform phrase grounding and object detection in a weakly supervised manner. Our experiments demonstrate the effectiveness of our approach by achieving a 47.51% recall@1 score in phrase grounding on Flickr30k Entities and establishing a new state-of-the-art in object detection by achieving 21.1 mAP 50 and 10.5 mAP 50:95 on MS COC",
    "path": "papers/23/06/2306.06149.json",
    "total_tokens": 739,
    "translated_title": "从图像-标题对标注中获取边界框",
    "translated_abstract": "提出了一种使用图像-标题对进行对象定位和标注的方法，在训练过程中利用了较弱的监督，并通过结合最近的视觉-语言模型和自监督视觉转换器来实现短语接地和目标检测。实验结果表明该方法的有效性。",
    "tldr": "本文提出了一种通过图像-标题对进行弱监督学习的方法来实现定位和标注图像中的对象，并在实验中实现了47.51%的短语接地以及21.1 和 10.5的新的最高mAP得分。",
    "en_tdlr": "This paper proposes a weakly supervised approach for object localization and labeling using image-caption pairs. It leverages recent advances in vision-language models and self-supervised vision transformers to achieve phrase grounding and object detection, achieving a 47.51% recall@1 score in phrase grounding on Flickr30k Entities and establishing a new state-of-the-art in object detection with 21.1 and 10.5 mAP scores on MS COC dataset."
}