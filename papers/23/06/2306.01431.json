{
    "title": "On Knowledge Editing in Federated Learning: Perspectives, Challenges, and Future Directions. (arXiv:2306.01431v1 [cs.LG])",
    "abstract": "As Federated Learning (FL) has gained increasing attention, it has become widely acknowledged that straightforwardly applying stochastic gradient descent (SGD) on the overall framework when learning over a sequence of tasks results in the phenomenon known as ``catastrophic forgetting''. Consequently, much FL research has centered on devising federated increasing learning methods to alleviate forgetting while augmenting knowledge. On the other hand, forgetting is not always detrimental. The selective amnesia, also known as federated unlearning, which entails the elimination of specific knowledge, can address privacy concerns and create additional ``space'' for acquiring new knowledge. However, there is a scarcity of extensive surveys that encompass recent advancements and provide a thorough examination of this issue. In this manuscript, we present an extensive survey on the topic of knowledge editing (augmentation/removal) in Federated Learning, with the goal of summarizing the state-of",
    "link": "http://arxiv.org/abs/2306.01431",
    "context": "Title: On Knowledge Editing in Federated Learning: Perspectives, Challenges, and Future Directions. (arXiv:2306.01431v1 [cs.LG])\nAbstract: As Federated Learning (FL) has gained increasing attention, it has become widely acknowledged that straightforwardly applying stochastic gradient descent (SGD) on the overall framework when learning over a sequence of tasks results in the phenomenon known as ``catastrophic forgetting''. Consequently, much FL research has centered on devising federated increasing learning methods to alleviate forgetting while augmenting knowledge. On the other hand, forgetting is not always detrimental. The selective amnesia, also known as federated unlearning, which entails the elimination of specific knowledge, can address privacy concerns and create additional ``space'' for acquiring new knowledge. However, there is a scarcity of extensive surveys that encompass recent advancements and provide a thorough examination of this issue. In this manuscript, we present an extensive survey on the topic of knowledge editing (augmentation/removal) in Federated Learning, with the goal of summarizing the state-of",
    "path": "papers/23/06/2306.01431.json",
    "total_tokens": 906,
    "translated_title": "关于联邦学习中的知识编辑：展望，挑战和未来方向的透视",
    "translated_abstract": "随着联邦学习（FL）引起越来越多的关注，人们普遍认为在学习一系列任务时，在整个框架上简单地应用随机梯度下降（SGD）会导致所谓的“灾难性遗忘”。因此，许多FL研究集中于设计联邦增量学习方法，以减轻遗忘并增加知识。另一方面，遗忘并不总是有害的。选择性遗忘，也称为联邦遗忘，包括消除特定知识，可以解决隐私问题并为获取新知识创造额外的“空间”。然而，缺乏广泛调查涵盖最新进展并对此问题进行全面检查。在本文中，我们介绍了一份关于联邦学习中知识编辑（增强/删除）的广泛调查，旨在总结最新技术，确定挑战和机遇，并概述未来的研究方向。",
    "tldr": "本文介绍了一份关于联邦学习中知识编辑的广泛调查，总结了最新技术，确定了挑战和机遇，并概述了未来的研究方向。",
    "en_tdlr": "This paper presents an extensive survey on the topic of knowledge editing (augmentation/removal) in Federated Learning, summarizing the state-of-the-art, identifying challenges and opportunities, and outlining future research directions."
}