{
    "title": "Scaling Up Semi-supervised Learning with Unconstrained Unlabelled Data. (arXiv:2306.01222v1 [cs.LG])",
    "abstract": "We propose UnMixMatch, a semi-supervised learning framework which can learn effective representations from unconstrained unlabelled data in order to scale up performance. Most existing semi-supervised methods rely on the assumption that labelled and unlabelled samples are drawn from the same distribution, which limits the potential for improvement through the use of free-living unlabeled data. Consequently, the generalizability and scalability of semi-supervised learning are often hindered by this assumption. Our method aims to overcome these constraints and effectively utilize unconstrained unlabelled data in semi-supervised learning. UnMixMatch consists of three main components: a supervised learner with hard augmentations that provides strong regularization, a contrastive consistency regularizer to learn underlying representations from the unlabelled data, and a self-supervised loss to enhance the representations that are learnt from the unlabelled data. We perform extensive experim",
    "link": "http://arxiv.org/abs/2306.01222",
    "context": "Title: Scaling Up Semi-supervised Learning with Unconstrained Unlabelled Data. (arXiv:2306.01222v1 [cs.LG])\nAbstract: We propose UnMixMatch, a semi-supervised learning framework which can learn effective representations from unconstrained unlabelled data in order to scale up performance. Most existing semi-supervised methods rely on the assumption that labelled and unlabelled samples are drawn from the same distribution, which limits the potential for improvement through the use of free-living unlabeled data. Consequently, the generalizability and scalability of semi-supervised learning are often hindered by this assumption. Our method aims to overcome these constraints and effectively utilize unconstrained unlabelled data in semi-supervised learning. UnMixMatch consists of three main components: a supervised learner with hard augmentations that provides strong regularization, a contrastive consistency regularizer to learn underlying representations from the unlabelled data, and a self-supervised loss to enhance the representations that are learnt from the unlabelled data. We perform extensive experim",
    "path": "papers/23/06/2306.01222.json",
    "total_tokens": 1004,
    "translated_title": "用非约束的未标记数据扩展半监督学习的规模",
    "translated_abstract": "本文提出了一种半监督学习框架UnMixMatch，可以从非约束的未标记数据中学习有效的表征以提高性能。大多数现有的半监督方法基于有标记和未标记样本来自同一分布的假设，这限制了通过使用自由生活的未标记数据进行改进的潜力。因此，该假设经常限制半监督学习的可推广性和可扩展性，本文旨在克服这些限制并有效利用非约束的未标记数据进行半监督学习。UnMixMatch包括三个主要组成部分：具有强正则化作用的硬增强监督学习器，用于从未标记数据中学习基础表征的对比一致性正则化器以及通过自监督损失来增强从未标记数据学习的表征。我们进行了大量实验来证明UnMixMatch的有效性。",
    "tldr": "本论文提出UnMixMatch框架，可以从非约束的未标记数据中学习有效的表征以提高性能，以解决大多数半监督方法基于有标记和未标记样本来自同一分布的假设。该框架具有强正则化作用的硬增强监督学习器、用于从未标记数据中学习基础表征的对比一致性正则化器以及通过自监督损失来增强从未标记数据学习的表征。"
}