{
    "title": "Feature Interactions Reveal Linguistic Structure in Language Models. (arXiv:2306.12181v1 [cs.CL])",
    "abstract": "We study feature interactions in the context of feature attribution methods for post-hoc interpretability. In interpretability research, getting to grips with feature interactions is increasingly recognised as an important challenge, because interacting features are key to the success of neural networks. Feature interactions allow a model to build up hierarchical representations for its input, and might provide an ideal starting point for the investigation into linguistic structure in language models. However, uncovering the exact role that these interactions play is also difficult, and a diverse range of interaction attribution methods has been proposed. In this paper, we focus on the question which of these methods most faithfully reflects the inner workings of the target models. We work out a grey box methodology, in which we train models to perfection on a formal language classification task, using PCFGs. We show that under specific configurations, some methods are indeed able to u",
    "link": "http://arxiv.org/abs/2306.12181",
    "context": "Title: Feature Interactions Reveal Linguistic Structure in Language Models. (arXiv:2306.12181v1 [cs.CL])\nAbstract: We study feature interactions in the context of feature attribution methods for post-hoc interpretability. In interpretability research, getting to grips with feature interactions is increasingly recognised as an important challenge, because interacting features are key to the success of neural networks. Feature interactions allow a model to build up hierarchical representations for its input, and might provide an ideal starting point for the investigation into linguistic structure in language models. However, uncovering the exact role that these interactions play is also difficult, and a diverse range of interaction attribution methods has been proposed. In this paper, we focus on the question which of these methods most faithfully reflects the inner workings of the target models. We work out a grey box methodology, in which we train models to perfection on a formal language classification task, using PCFGs. We show that under specific configurations, some methods are indeed able to u",
    "path": "papers/23/06/2306.12181.json",
    "total_tokens": 835,
    "translated_title": "特征交互揭示语言模型中的语言结构",
    "translated_abstract": "我们研究特征指派方法的特征交互。在可解释性研究中，理解特征交互越来越被认为是一个重要的挑战，因为交互特征对神经网络的成功至关重要。特征交互让一个模型为其输入建立层次结构表示，并可能成为研究语言模型中语言结构的理想起点。然而，揭示这些交互精确的作用也是困难的，而且已经提出了各种各样的交互指派方法。在本文中，我们关注的问题是哪种方法最忠实地反映目标模型的内部运作。我们制定了一个灰箱方法，通过使用PCFGs在正式语言分类任务上完美训练模型。我们展示了在特定的配置下，一些方法确实能够揭示交互作用。",
    "tldr": "研究特征交互在神经网络中的重要性，提出了一种灰箱方法来分析交互特征的作用，揭示了某些方法能够反映目标模型的内部运作。",
    "en_tdlr": "This paper studies the importance of feature interactions in neural networks, and proposes a grey box methodology to analyze the role of interacting features. It reveals that some methods are able to reflect the internal workings of the target models."
}