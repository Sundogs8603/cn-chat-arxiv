{
    "title": "LexGPT 0.1: pre-trained GPT-J models with Pile of Law. (arXiv:2306.05431v1 [cs.CL])",
    "abstract": "This research aims to build generative language models specialized for the legal domain. The manuscript presents the development of LexGPT models based on GPT-J models and pre-trained with Pile of Law. The foundation model built in this manuscript is the initial step for the development of future applications in the legal domain, such as further training with reinforcement learning from human feedback. Another objective of this manuscript is to assist legal professionals in utilizing language models through the ``No Code'' approach. By fine-tuning models with specialized data and without modifying any source code, legal professionals can create custom language models for downstream tasks with minimum effort and technical knowledge. The downstream task in this manuscript is to turn a LexGPT model into a classifier, although the performance is notably lower than the state-of-the-art result. How to enhance downstream task performance without modifying the model or its source code is a res",
    "link": "http://arxiv.org/abs/2306.05431",
    "context": "Title: LexGPT 0.1: pre-trained GPT-J models with Pile of Law. (arXiv:2306.05431v1 [cs.CL])\nAbstract: This research aims to build generative language models specialized for the legal domain. The manuscript presents the development of LexGPT models based on GPT-J models and pre-trained with Pile of Law. The foundation model built in this manuscript is the initial step for the development of future applications in the legal domain, such as further training with reinforcement learning from human feedback. Another objective of this manuscript is to assist legal professionals in utilizing language models through the ``No Code'' approach. By fine-tuning models with specialized data and without modifying any source code, legal professionals can create custom language models for downstream tasks with minimum effort and technical knowledge. The downstream task in this manuscript is to turn a LexGPT model into a classifier, although the performance is notably lower than the state-of-the-art result. How to enhance downstream task performance without modifying the model or its source code is a res",
    "path": "papers/23/06/2306.05431.json",
    "total_tokens": 1030,
    "translated_title": "LexGPT 0.1：基于Pile of Law的预训练GPT-J模型（arXiv:2306.05431v1 [cs.CL]）",
    "translated_abstract": "本研究旨在构建专门用于法律领域的生成语言模型。本文介绍了基于GPT-J模型并使用Pile of Law进行预训练的LexGPT模型的开发过程。本文所构建的基础模型是未来在法律领域开发应用的初始步骤，例如通过人类反馈进行强化学习的进一步培训。本文的另一个目标是通过“无代码”方法帮助法律专业人员利用语言模型。通过使用专门数据进行精调并且不修改任何源代码，法律专业人员可以轻松地为下游任务创建定制化的语言模型，减少技术知识和投入的最小化。本文中的下游任务是将LexGPT模型转换为分类器，尽管性能明显低于最先进的结果。如何在不修改模型或其源代码的情况下提高下游任务性能是一个研究重点。",
    "tldr": "本研究旨在构建专门用于法律领域的生成语言模型。此发展基前提模型是为了在法律领域开发未来的应用，如通过人类反馈进行强化学习进一步培训。此外，通过“无代码”方法，法律专业人员可以轻松地为下游任务创建定制化的语言模型。虽然性能低于最先进的结果，但本文研究如何在不修改模型或其源代码的情况下改进LexGPT模型的下游任务性能。",
    "en_tdlr": "This research aims to build generative language models specialized for the legal domain. By fine-tuning models with specialized data and without modifying any source code, legal professionals can create custom language models for downstream tasks with minimum effort and technical knowledge. The study focuses on developing a foundation model for future applications in the legal domain, and discusses how to improve the performance of downstream tasks without modifying the LexGPT model or its source code."
}