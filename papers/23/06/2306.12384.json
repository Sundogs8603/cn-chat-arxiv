{
    "title": "Probing the limit of hydrologic predictability with the Transformer network. (arXiv:2306.12384v1 [cs.LG])",
    "abstract": "For a number of years since its introduction to hydrology, recurrent neural networks like long short-term memory (LSTM) have proven remarkably difficult to surpass in terms of daily hydrograph metrics on known, comparable benchmarks. Outside of hydrology, Transformers have now become the model of choice for sequential prediction tasks, making it a curious architecture to investigate. Here, we first show that a vanilla Transformer architecture is not competitive against LSTM on the widely benchmarked CAMELS dataset, and lagged especially for the high-flow metrics due to short-term processes. However, a recurrence-free variant of Transformer can obtain mixed comparisons with LSTM, producing the same Kling-Gupta efficiency coefficient (KGE), along with other metrics. The lack of advantages for the Transformer is linked to the Markovian nature of the hydrologic prediction problem. Similar to LSTM, the Transformer can also merge multiple forcing dataset to improve model performance. While t",
    "link": "http://arxiv.org/abs/2306.12384",
    "context": "Title: Probing the limit of hydrologic predictability with the Transformer network. (arXiv:2306.12384v1 [cs.LG])\nAbstract: For a number of years since its introduction to hydrology, recurrent neural networks like long short-term memory (LSTM) have proven remarkably difficult to surpass in terms of daily hydrograph metrics on known, comparable benchmarks. Outside of hydrology, Transformers have now become the model of choice for sequential prediction tasks, making it a curious architecture to investigate. Here, we first show that a vanilla Transformer architecture is not competitive against LSTM on the widely benchmarked CAMELS dataset, and lagged especially for the high-flow metrics due to short-term processes. However, a recurrence-free variant of Transformer can obtain mixed comparisons with LSTM, producing the same Kling-Gupta efficiency coefficient (KGE), along with other metrics. The lack of advantages for the Transformer is linked to the Markovian nature of the hydrologic prediction problem. Similar to LSTM, the Transformer can also merge multiple forcing dataset to improve model performance. While t",
    "path": "papers/23/06/2306.12384.json",
    "total_tokens": 953,
    "translated_title": "用Transformer网络探究水文预测的极限（arXiv:2306.12384v1 [cs.LG]）",
    "translated_abstract": "自其引入水文学领域以来，循环神经网络（如LSTM）在已知的可比基准上的日水文图表指标方面一直证明难以超越。除水文学外，Transformer现在已成为连续预测任务的首选模型，这使得它成为一个有趣的架构进行研究。在本文中，我们首先展示了香草Transformer架构在广泛使用的CAMELS数据集上与LSTM相比毫无竞争力，由于短期过程而特别滞后于高流量指标。然而，不需要递归的变体Transformer可以获得与LSTM相混合的比较，产生相同的Kling-Gupta效率系数（KGE）以及其他指标。Transformer没有优势与水文预测问题的马尔可夫特性有关。与LSTM类似，Transformer也可以合并多个强制数据集来改善模型性能。然而，虽然将水文预测问题的关键特征转换为更描述性的空间，Transformer模型似乎无法超越LSTM模型的预测性能。",
    "tldr": "本文探究了采用Transformer网络对水文预测问题的建模效果，发现相比LSTM模型缺乏优势，可能是由于水文预测问题的马尔可夫特性所导致。",
    "en_tdlr": "This paper investigates the effectiveness of using the Transformer network for hydrologic prediction and finds that it lacks advantages compared to the LSTM model, possibly due to the Markovian nature of the hydrologic prediction problem."
}