{
    "title": "Understanding How Consistency Works in Federated Learning via Stage-wise Relaxed Initialization. (arXiv:2306.05706v1 [cs.LG])",
    "abstract": "Federated learning (FL) is a distributed paradigm that coordinates massive local clients to collaboratively train a global model via stage-wise local training processes on the heterogeneous dataset. Previous works have implicitly studied that FL suffers from the ``client-drift'' problem, which is caused by the inconsistent optimum across local clients. However, till now it still lacks solid theoretical analysis to explain the impact of this local inconsistency. To alleviate the negative impact of the ``client drift'' and explore its substance in FL, in this paper, we first design an efficient FL algorithm \\textit{FedInit}, which allows employing the personalized relaxed initialization state at the beginning of each local training stage. Specifically, \\textit{FedInit} initializes the local state by moving away from the current global state towards the reverse direction of the latest local state. This relaxed initialization helps to revise the local divergence and enhance the local consi",
    "link": "http://arxiv.org/abs/2306.05706",
    "context": "Title: Understanding How Consistency Works in Federated Learning via Stage-wise Relaxed Initialization. (arXiv:2306.05706v1 [cs.LG])\nAbstract: Federated learning (FL) is a distributed paradigm that coordinates massive local clients to collaboratively train a global model via stage-wise local training processes on the heterogeneous dataset. Previous works have implicitly studied that FL suffers from the ``client-drift'' problem, which is caused by the inconsistent optimum across local clients. However, till now it still lacks solid theoretical analysis to explain the impact of this local inconsistency. To alleviate the negative impact of the ``client drift'' and explore its substance in FL, in this paper, we first design an efficient FL algorithm \\textit{FedInit}, which allows employing the personalized relaxed initialization state at the beginning of each local training stage. Specifically, \\textit{FedInit} initializes the local state by moving away from the current global state towards the reverse direction of the latest local state. This relaxed initialization helps to revise the local divergence and enhance the local consi",
    "path": "papers/23/06/2306.05706.json",
    "total_tokens": 1107,
    "translated_title": "通过阶段性放松初始化理解联邦学习的一致性问题",
    "translated_abstract": "联邦学习是一种分布式范式，通过阶段性的本地训练过程协调大量本地客户端，对异构数据集进行全局模型协同训练。先前的研究已经暗示了联邦学习面临“客户机漂移”问题的困扰，这是由于本地客户端的不一致性最优解所导致的。然而，到目前为止，仍缺乏实质性的理论分析来解释这种本地不一致性的影响。为了缓解“客户机漂移”的负面影响并探索其在联邦学习中的实质，本文首先设计了一种高效的联邦学习算法 FedInit，该算法允许在每个本地训练阶段开始时使用个性化的放松初始化状态。具体而言，FedInit通过朝着最新本地状态的反向移动，将本地状态从当前全局状态中移开以进行放松的初始化。这种放松初始化有助于修正本地分歧并增强本地一致性。此外，我们提出了一个理论框架，以理解联邦学习中本地不一致性的行为。我们展示了阶段性放松初始化可以缓解本地不一致性的负面影响，并强调初始化过程在优化联邦学习模型中的重要性。",
    "tldr": "本文提出了一种高效的联邦学习算法 FedInit，通过阶段性放松初始化来缓解“客户机漂移”问题，从而增强本地一致性，同时提出了一个理论框架解释本地不一致性的行为，并强调了初始化过程在优化联邦学习模型中的重要性。",
    "en_tdlr": "This paper proposes an efficient federated learning algorithm FedInit to alleviate the negative impact of \"client drift\" problem via stage-wise relaxed initialization, enhance the local consistency and highlights the importance of initialization process in optimizing federated learning models. It also proposes a theoretical framework to understand the behavior of local inconsistency in federated learning."
}