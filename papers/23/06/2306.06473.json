{
    "title": "Interpretable Differencing of Machine Learning Models. (arXiv:2306.06473v2 [cs.LG] UPDATED)",
    "abstract": "Understanding the differences between machine learning (ML) models is of interest in scenarios ranging from choosing amongst a set of competing models, to updating a deployed model with new training data. In these cases, we wish to go beyond differences in overall metrics such as accuracy to identify where in the feature space do the differences occur. We formalize this problem of model differencing as one of predicting a dissimilarity function of two ML models' outputs, subject to the representation of the differences being human-interpretable. Our solution is to learn a Joint Surrogate Tree (JST), which is composed of two conjoined decision tree surrogates for the two models. A JST provides an intuitive representation of differences and places the changes in the context of the models' decision logic. Context is important as it helps users to map differences to an underlying mental model of an AI system. We also propose a refinement procedure to increase the precision of a JST. We dem",
    "link": "http://arxiv.org/abs/2306.06473",
    "context": "Title: Interpretable Differencing of Machine Learning Models. (arXiv:2306.06473v2 [cs.LG] UPDATED)\nAbstract: Understanding the differences between machine learning (ML) models is of interest in scenarios ranging from choosing amongst a set of competing models, to updating a deployed model with new training data. In these cases, we wish to go beyond differences in overall metrics such as accuracy to identify where in the feature space do the differences occur. We formalize this problem of model differencing as one of predicting a dissimilarity function of two ML models' outputs, subject to the representation of the differences being human-interpretable. Our solution is to learn a Joint Surrogate Tree (JST), which is composed of two conjoined decision tree surrogates for the two models. A JST provides an intuitive representation of differences and places the changes in the context of the models' decision logic. Context is important as it helps users to map differences to an underlying mental model of an AI system. We also propose a refinement procedure to increase the precision of a JST. We dem",
    "path": "papers/23/06/2306.06473.json",
    "total_tokens": 968,
    "translated_title": "可解释的机器学习模型差异比较",
    "translated_abstract": "在选择竞争模型或更新已部署的模型时，理解机器学习（ML）模型之间的差异非常重要。在这些情况下，我们希望在超出准确率等整体指标的基础上，确定差异在特征空间的哪些位置发生。我们将这个模型差异的问题形式化为一种预测两个ML模型输出相似度函数的问题，同时要求表示差异的方式可以被人类解释。我们的解决方案是学习一个联合代理树（JST），它由两个决策树代理构成，分别对应两个模型。JST提供了直观的差异表示，并将变化放置在模型决策逻辑的背景下。上下文很重要，因为它帮助用户将差异映射到AI系统的基础性思维模型。我们还提出了一种精细化过程，以增加JST的精度。我们在不同的数据集和模型上展示了我们方法的有效性。",
    "tldr": "本论文提出了一种预测两个机器学习模型输出相似度函数的方法，同时要求表示差异的方式可被人类解释，并通过学习联合代理树提供直观的差异表示和上下文信息，以帮助用户对AI系统进行基础性思维模型的映射。",
    "en_tdlr": "This paper proposes a method for predicting a dissimilarity function of two machine learning models' outputs, with interpretable representations of differences, by learning a Joint Surrogate Tree. The JST provides an intuitive representation of differences and places the changes in the context of the models' decision logic, helping users to map differences to an underlying mental model of an AI system."
}