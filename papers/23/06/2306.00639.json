{
    "title": "Being Right for Whose Right Reasons?. (arXiv:2306.00639v2 [cs.CL] UPDATED)",
    "abstract": "Explainability methods are used to benchmark the extent to which model predictions align with human rationales i.e., are 'right for the right reasons'. Previous work has failed to acknowledge, however, that what counts as a rationale is sometimes subjective. This paper presents what we think is a first of its kind, a collection of human rationale annotations augmented with the annotators demographic information. We cover three datasets spanning sentiment analysis and common-sense reasoning, and six demographic groups (balanced across age and ethnicity). Such data enables us to ask both what demographics our predictions align with and whose reasoning patterns our models' rationales align with. We find systematic inter-group annotator disagreement and show how 16 Transformer-based models align better with rationales provided by certain demographic groups: We find that models are biased towards aligning best with older and/or white annotators. We zoom in on the effects of model size and m",
    "link": "http://arxiv.org/abs/2306.00639",
    "context": "Title: Being Right for Whose Right Reasons?. (arXiv:2306.00639v2 [cs.CL] UPDATED)\nAbstract: Explainability methods are used to benchmark the extent to which model predictions align with human rationales i.e., are 'right for the right reasons'. Previous work has failed to acknowledge, however, that what counts as a rationale is sometimes subjective. This paper presents what we think is a first of its kind, a collection of human rationale annotations augmented with the annotators demographic information. We cover three datasets spanning sentiment analysis and common-sense reasoning, and six demographic groups (balanced across age and ethnicity). Such data enables us to ask both what demographics our predictions align with and whose reasoning patterns our models' rationales align with. We find systematic inter-group annotator disagreement and show how 16 Transformer-based models align better with rationales provided by certain demographic groups: We find that models are biased towards aligning best with older and/or white annotators. We zoom in on the effects of model size and m",
    "path": "papers/23/06/2306.00639.json",
    "total_tokens": 900,
    "translated_title": "谁的正确理由是正确的？（arXiv:2306.00639v2 [cs.CL] 已更新）",
    "translated_abstract": "解释性方法用于评估模型预测与人类推理的一致程度，即是否\"出于正确的原因\"。然而，先前的研究未能认识到，什么被视为原因有时是主观的。本文介绍了我们认为是首次的人类原因注释集合，其中包含注释者的人口统计信息。我们涵盖了涵盖情感分析和常识推理的三个数据集以及六个人口统计组（在年龄和族裔上均衡）。这样的数据使我们能够同时询问我们的预测与哪些人口统计相符，以及我们模型的推理模式与哪些人的原因相符。我们发现不同人群的注释者之间存在系统性的相互分歧，并展示了16个基于Transformer的模型与某些人口统计组提供的原因更加一致：我们发现模型更倾向于与年长和/或白人注释者提供的原因最为一致。我们重点研究了模型大小和...",
    "tldr": "本论文针对解释性方法的应用进行研究，发现模型预测与人类推理的一致性与人口统计信息有关，模型更倾向于与年长和/或白人注释者提供的原因最为一致。",
    "en_tdlr": "This paper investigates the application of explainability methods and finds that the alignment between model predictions and human rationales is influenced by demographic information, with models tending to align better with rationales provided by older and/or white annotators."
}