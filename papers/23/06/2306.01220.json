{
    "title": "Is Model Attention Aligned with Human Attention? An Empirical Study on Large Language Models for Code Generation. (arXiv:2306.01220v1 [cs.SE])",
    "abstract": "Large Language Models (LLMs) have been demonstrated effective for code generation. Due to the complexity and opacity of LLMs, little is known about how these models generate code. To deepen our understanding, we investigate whether LLMs attend to the same parts of a natural language description as human programmers during code generation. An analysis of five LLMs on a popular benchmark, HumanEval, revealed a consistent misalignment between LLMs' and programmers' attention. Furthermore, we found that there is no correlation between the code generation accuracy of LLMs and their alignment with human programmers. Through a quantitative experiment and a user study, we confirmed that, among twelve different attention computation methods, attention computed by the perturbation-based method is most aligned with human attention and is constantly favored by human programmers. Our findings highlight the need for human-aligned LLMs for better interpretability and programmer trust.",
    "link": "http://arxiv.org/abs/2306.01220",
    "context": "Title: Is Model Attention Aligned with Human Attention? An Empirical Study on Large Language Models for Code Generation. (arXiv:2306.01220v1 [cs.SE])\nAbstract: Large Language Models (LLMs) have been demonstrated effective for code generation. Due to the complexity and opacity of LLMs, little is known about how these models generate code. To deepen our understanding, we investigate whether LLMs attend to the same parts of a natural language description as human programmers during code generation. An analysis of five LLMs on a popular benchmark, HumanEval, revealed a consistent misalignment between LLMs' and programmers' attention. Furthermore, we found that there is no correlation between the code generation accuracy of LLMs and their alignment with human programmers. Through a quantitative experiment and a user study, we confirmed that, among twelve different attention computation methods, attention computed by the perturbation-based method is most aligned with human attention and is constantly favored by human programmers. Our findings highlight the need for human-aligned LLMs for better interpretability and programmer trust.",
    "path": "papers/23/06/2306.01220.json",
    "total_tokens": 965,
    "translated_title": "模型的注意力机制是否与人类的注意力机制一致？关于大型语言模型用于代码生成的实证研究。",
    "translated_abstract": "大型语言模型（LLMs）已被证明对于代码生成非常有效。由于LLMs的复杂性和不透明性，我们对这些模型如何生成代码知之甚少。为了深入了解，我们研究了LLMs在代码生成过程中是否与人类程序员相同地关注自然语言描述中的某些部分。通过对流行基准测试HumanEval上的五个LLMs进行分析，发现LLMs的注意力与程序员的注意力存在一致性偏差。此外，我们发现LLMs的代码生成准确性与它们与人类程序员的注意力对齐程度之间没有相关性。通过量化实验和用户研究，我们确认，在12种不同的注意力计算方法中，基于扰动的方法计算的注意力最接近人类注意力，并且始终受到人类程序员的青睐。我们的研究结果强调了需要人类对齐的LLMs以获得更好的可解释性和程序员信任。",
    "tldr": "本文研究了LLMs在代码生成过程中是否与人类程序员的注意力有所不同，结果发现他们之间存在一致性偏差。作者通过量化实验和用户研究，确认了扰动方法计算的注意力最接近人类程序员的注意力，并且这种LLMs模型具有更好的可解释能力和程序员信任度。",
    "en_tdlr": "This study investigates whether LLMs attend to the same parts of a natural language description as human programmers during code generation, and finds a consistent misalignment between LLMs' and programmers' attention. The perturbation-based method is confirmed to be most aligned with human attention, and the need for human-aligned LLMs for better interpretability and programmer trust is highlighted."
}