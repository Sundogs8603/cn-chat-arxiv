{
    "title": "Low-rank Adaptation Method for Wav2vec2-based Fake Audio Detection. (arXiv:2306.05617v1 [cs.SD])",
    "abstract": "Self-supervised speech models are a rapidly developing research topic in fake audio detection. Many pre-trained models can serve as feature extractors, learning richer and higher-level speech features. However,when fine-tuning pre-trained models, there is often a challenge of excessively long training times and high memory consumption, and complete fine-tuning is also very expensive. To alleviate this problem, we apply low-rank adaptation(LoRA) to the wav2vec2 model, freezing the pre-trained model weights and injecting a trainable rank-decomposition matrix into each layer of the transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared with fine-tuning with Adam on the wav2vec2 model containing 317M training parameters, LoRA achieved similar performance by reducing the number of trainable parameters by 198 times.",
    "link": "http://arxiv.org/abs/2306.05617",
    "context": "Title: Low-rank Adaptation Method for Wav2vec2-based Fake Audio Detection. (arXiv:2306.05617v1 [cs.SD])\nAbstract: Self-supervised speech models are a rapidly developing research topic in fake audio detection. Many pre-trained models can serve as feature extractors, learning richer and higher-level speech features. However,when fine-tuning pre-trained models, there is often a challenge of excessively long training times and high memory consumption, and complete fine-tuning is also very expensive. To alleviate this problem, we apply low-rank adaptation(LoRA) to the wav2vec2 model, freezing the pre-trained model weights and injecting a trainable rank-decomposition matrix into each layer of the transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared with fine-tuning with Adam on the wav2vec2 model containing 317M training parameters, LoRA achieved similar performance by reducing the number of trainable parameters by 198 times.",
    "path": "papers/23/06/2306.05617.json",
    "total_tokens": 907,
    "translated_title": "基于低秩适应性的Wav2vec2声音伪造检测方法",
    "translated_abstract": "自监督语音模型是声音伪造检测中快速发展的研究主题。许多预训练模型可以作为特征提取器，学习更丰富和更高级别的语音特征。然而，在微调预先训练的模型时，通常存在过长的训练时间和高内存消耗的挑战，而完全微调也非常昂贵。为了缓解这个问题，我们应用低秩适应（LoRA）到wav2vec2模型中，冻结预训练模型的权重，并将可训练秩分解矩阵注入到变压器结构的每一层中，从而大大减少了下游任务的可训练参数数量。与使用包含317M训练参数的wav2vec2模型上的Adam微调相比，LoRA通过将可训练参数数量减少198倍，实现了类似的性能。",
    "tldr": "本论文提出了一种基于低秩适应性的Wav2vec2声音伪造检测方法，通过注入可训练秩分解矩阵到变压器结构的每一层中来减少可训练参数数量，可以有效缓解微调预训练模型时过长训练时间和高内存消耗的问题。",
    "en_tdlr": "This paper proposes a low-rank adaptation method for Wav2vec2-based fake audio detection, which reduces the number of trainable parameters by injecting a trainable rank-decomposition matrix into each layer of the transformer architecture, effectively alleviating the challenges of long training times and high memory consumption when fine-tuning pre-trained models."
}