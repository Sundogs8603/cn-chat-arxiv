{
    "title": "PLASTIC: Improving Input and Label Plasticity for Sample Efficient Reinforcement Learning. (arXiv:2306.10711v2 [cs.LG] UPDATED)",
    "abstract": "In Reinforcement Learning (RL), enhancing sample efficiency is crucial, particularly in scenarios when data acquisition is costly and risky. In principle, off-policy RL algorithms can improve sample efficiency by allowing multiple updates per environment interaction. However, these multiple updates often lead the model to overfit to earlier interactions, which is referred to as the loss of plasticity. Our study investigates the underlying causes of this phenomenon by dividing plasticity into two aspects. Input plasticity, which denotes the model's adaptability to changing input data, and label plasticity, which denotes the model's adaptability to evolving input-output relationships. Synthetic experiments on the CIFAR-10 dataset reveal that finding smoother minima of loss landscape enhances input plasticity, whereas refined gradient propagation improves label plasticity. Leveraging these findings, we introduce the PLASTIC algorithm, which harmoniously combines techniques to address both",
    "link": "http://arxiv.org/abs/2306.10711",
    "context": "Title: PLASTIC: Improving Input and Label Plasticity for Sample Efficient Reinforcement Learning. (arXiv:2306.10711v2 [cs.LG] UPDATED)\nAbstract: In Reinforcement Learning (RL), enhancing sample efficiency is crucial, particularly in scenarios when data acquisition is costly and risky. In principle, off-policy RL algorithms can improve sample efficiency by allowing multiple updates per environment interaction. However, these multiple updates often lead the model to overfit to earlier interactions, which is referred to as the loss of plasticity. Our study investigates the underlying causes of this phenomenon by dividing plasticity into two aspects. Input plasticity, which denotes the model's adaptability to changing input data, and label plasticity, which denotes the model's adaptability to evolving input-output relationships. Synthetic experiments on the CIFAR-10 dataset reveal that finding smoother minima of loss landscape enhances input plasticity, whereas refined gradient propagation improves label plasticity. Leveraging these findings, we introduce the PLASTIC algorithm, which harmoniously combines techniques to address both",
    "path": "papers/23/06/2306.10711.json",
    "total_tokens": 843,
    "translated_title": "PLASTIC: 改善样本高效强化学习的输入和标签可塑性",
    "translated_abstract": "在强化学习（RL）中，提高样本效率是至关重要的，特别是在数据获取成本高昂且风险高的情况下。原则上，离策略RL算法可以通过允许每个环境交互进行多次更新来提高样本效率。然而，这些多次更新往往导致模型过度拟合之前的交互，这被称为可塑性的丧失。我们的研究将可塑性分为两个方面进行调查。输入可塑性，指的是模型对变化的输入数据的适应能力，标签可塑性，指的是模型对不断演化的输入输出关系的适应能力。对CIFAR-10数据集进行的合成实验表明，在损失概览中寻找更平滑的最小值可以增强输入可塑性，而细化的梯度传播可以提高标签可塑性。基于这些发现，我们提出了PLASTIC算法，它融合了这两方面的技术来解决这个问题。",
    "tldr": "PLASTIC算法通过改善模型的输入和标签可塑性，提高样本高效强化学习的效果。",
    "en_tdlr": "The PLASTIC algorithm improves sample efficient reinforcement learning by enhancing the input and label plasticity of the model."
}