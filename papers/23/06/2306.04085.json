{
    "title": "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations. (arXiv:2306.04085v1 [cs.CL])",
    "abstract": "Cross-Lingual Semantic Parsing (CLSP) aims to translate queries in multiple natural languages (NLs) into meaning representations (MRs) such as SQL, lambda calculus, and logic forms. However, existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications, impeding a comprehensive and unified evaluation of CLSP on a diverse range of NLs and MRs. To this end, we present XSemPLR, a unified benchmark for cross-lingual semantic parsing featured with 22 natural languages and 8 meaning representations by examining and selecting 9 existing datasets to cover 5 tasks and 164 domains. We use XSemPLR to conduct a comprehensive benchmark study on a wide range of multilingual language models including encoder-based models (mBERT, XLM-R), encoder-decoder models (mBART, mT5), and decoder-based models (Codex, BLOOM). We design 6 experiment settings covering various lingual combinations (monolingual, multilingual, cross-lingual) and numbers of learning samples (f",
    "link": "http://arxiv.org/abs/2306.04085",
    "context": "Title: XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations. (arXiv:2306.04085v1 [cs.CL])\nAbstract: Cross-Lingual Semantic Parsing (CLSP) aims to translate queries in multiple natural languages (NLs) into meaning representations (MRs) such as SQL, lambda calculus, and logic forms. However, existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications, impeding a comprehensive and unified evaluation of CLSP on a diverse range of NLs and MRs. To this end, we present XSemPLR, a unified benchmark for cross-lingual semantic parsing featured with 22 natural languages and 8 meaning representations by examining and selecting 9 existing datasets to cover 5 tasks and 164 domains. We use XSemPLR to conduct a comprehensive benchmark study on a wide range of multilingual language models including encoder-based models (mBERT, XLM-R), encoder-decoder models (mBART, mT5), and decoder-based models (Codex, BLOOM). We design 6 experiment settings covering various lingual combinations (monolingual, multilingual, cross-lingual) and numbers of learning samples (f",
    "path": "papers/23/06/2306.04085.json",
    "total_tokens": 918,
    "translated_title": "XSemPLR：多种自然语言和意义表示下的跨语义解析",
    "translated_abstract": "跨语义解析旨在将多种自然语言的查询转换为诸如SQL、lambda演算和逻辑形式等意义表示。然而，现有的模型都是分别提出并在有限的任务和应用数据集上进行评估，这阻碍了在多种自然语言和意义表示下全面和统一的评价跨语义解析。为此，我们提出了XSemPLR，这是一个统一的跨语义解析基准，包括22种自然语言和8种意义表示，覆盖了5个任务和164个域，选择了9个现有的数据集进行研究和筛选。我们使用XSemPLR对多种多语言模型进行全面的基准研究，包括基于编码器的模型（mBERT、XLM-R）、编码器-解码器模型（mBART、mT5）和解码器模型（Codex、BLOOM）。我们设计了6个实验设置，涵盖各种语言组合（单语言、多语言、跨语言）和学习样本数量（f）",
    "tldr": "XSemPLR是一个统一的跨语义解析基准，覆盖了22种自然语言和8种意义表示，能够全面地评价多种多语言模型的性能和效果。",
    "en_tdlr": "XSemPLR is a unified benchmark for cross-lingual semantic parsing, covering 22 natural languages and 8 meaning representations, enabling comprehensive evaluation of the performance and effectiveness of various multilingual language models."
}