{
    "title": "White-Box Transformers via Sparse Rate Reduction. (arXiv:2306.01129v1 [cs.LG])",
    "abstract": "In this paper, we contend that the objective of representation learning is to compress and transform the distribution of the data, say sets of tokens, towards a mixture of low-dimensional Gaussian distributions supported on incoherent subspaces. The quality of the final representation can be measured by a unified objective function called sparse rate reduction. From this perspective, popular deep networks such as transformers can be naturally viewed as realizing iterative schemes to optimize this objective incrementally. Particularly, we show that the standard transformer block can be derived from alternating optimization on complementary parts of this objective: the multi-head self-attention operator can be viewed as a gradient descent step to compress the token sets by minimizing their lossy coding rate, and the subsequent multi-layer perceptron can be viewed as attempting to sparsify the representation of the tokens. This leads to a family of white-box transformer-like deep network ",
    "link": "http://arxiv.org/abs/2306.01129",
    "context": "Title: White-Box Transformers via Sparse Rate Reduction. (arXiv:2306.01129v1 [cs.LG])\nAbstract: In this paper, we contend that the objective of representation learning is to compress and transform the distribution of the data, say sets of tokens, towards a mixture of low-dimensional Gaussian distributions supported on incoherent subspaces. The quality of the final representation can be measured by a unified objective function called sparse rate reduction. From this perspective, popular deep networks such as transformers can be naturally viewed as realizing iterative schemes to optimize this objective incrementally. Particularly, we show that the standard transformer block can be derived from alternating optimization on complementary parts of this objective: the multi-head self-attention operator can be viewed as a gradient descent step to compress the token sets by minimizing their lossy coding rate, and the subsequent multi-layer perceptron can be viewed as attempting to sparsify the representation of the tokens. This leads to a family of white-box transformer-like deep network ",
    "path": "papers/23/06/2306.01129.json",
    "total_tokens": 984,
    "tldr": "本文提出使用稀疏率降低的统一目标函数来衡量模型的质量，通过优化多头自我关注算子和多层感知机实现压缩和转换标记，从而创建出一系列白盒 Transformer 类型的深度网络。"
}