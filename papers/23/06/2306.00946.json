{
    "title": "Exposing Attention Glitches with Flip-Flop Language Modeling. (arXiv:2306.00946v2 [cs.LG] UPDATED)",
    "abstract": "Why do large language models sometimes output factual inaccuracies and exhibit erroneous reasoning? The brittleness of these models, particularly when executing long chains of reasoning, currently seems to be an inevitable price to pay for their advanced capabilities of coherently synthesizing knowledge, pragmatics, and abstract thought. Towards making sense of this fundamentally unsolved problem, this work identifies and analyzes the phenomenon of attention glitches, in which the Transformer architecture's inductive biases intermittently fail to capture robust reasoning. To isolate the issue, we introduce flip-flop language modeling (FFLM), a parametric family of synthetic benchmarks designed to probe the extrapolative behavior of neural language models. This simple generative task requires a model to copy binary symbols over long-range dependencies, ignoring the tokens in between. We find that Transformer FFLMs suffer from a long tail of sporadic reasoning errors, some of which we ca",
    "link": "http://arxiv.org/abs/2306.00946",
    "context": "Title: Exposing Attention Glitches with Flip-Flop Language Modeling. (arXiv:2306.00946v2 [cs.LG] UPDATED)\nAbstract: Why do large language models sometimes output factual inaccuracies and exhibit erroneous reasoning? The brittleness of these models, particularly when executing long chains of reasoning, currently seems to be an inevitable price to pay for their advanced capabilities of coherently synthesizing knowledge, pragmatics, and abstract thought. Towards making sense of this fundamentally unsolved problem, this work identifies and analyzes the phenomenon of attention glitches, in which the Transformer architecture's inductive biases intermittently fail to capture robust reasoning. To isolate the issue, we introduce flip-flop language modeling (FFLM), a parametric family of synthetic benchmarks designed to probe the extrapolative behavior of neural language models. This simple generative task requires a model to copy binary symbols over long-range dependencies, ignoring the tokens in between. We find that Transformer FFLMs suffer from a long tail of sporadic reasoning errors, some of which we ca",
    "path": "papers/23/06/2306.00946.json",
    "total_tokens": 862,
    "translated_title": "揭示Attention故障的翻转-翻转语言建模",
    "translated_abstract": "为什么大型语言模型有时会输出事实错误并表现出错误的推理？这些模型的脆弱性，特别是在执行长链推理时，目前似乎是为了它们能够精确地综合知识、语用和抽象思维而必须付出的代价。为了理解这个根本未解决的问题，本研究确定并分析了注意力故障现象，其中Transformer架构的归纳性偏见间歇性地未能捕捉到稳健的推理。为了隔离这个问题，我们引入了翻转-翻转语言建模（FFLM），这是一组参数化合成基准，旨在探索神经语言模型的外推行为。这个简单的生成任务要求模型在长程依赖关系中复制二进制符号，忽略中间的标记。我们发现Transformer FFLMs在推理错误方面存在着长尾现象，其中一些我们可以",
    "tldr": "本论文揭示了语言模型中注意力故障的现象，并通过引入翻转-翻转语言建模来分析这个问题。研究发现，Transformer FFLMs经常出现推理错误。",
    "en_tdlr": "This paper exposes the phenomenon of attention glitches in language models, and analyzes the issue by introducing flip-flop language modeling. The study finds that Transformer FFLMs often suffer from reasoning errors."
}