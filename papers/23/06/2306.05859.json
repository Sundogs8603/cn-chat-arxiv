{
    "title": "Robust Reinforcement Learning via Adversarial Kernel Approximation. (arXiv:2306.05859v1 [cs.LG])",
    "abstract": "Robust Markov Decision Processes (RMDPs) provide a framework for sequential decision-making that is robust to perturbations on the transition kernel. However, robust reinforcement learning (RL) approaches in RMDPs do not scale well to realistic online settings with high-dimensional domains. By characterizing the adversarial kernel in RMDPs, we propose a novel approach for online robust RL that approximates the adversarial kernel and uses a standard (non-robust) RL algorithm to learn a robust policy. Notably, our approach can be applied on top of any underlying RL algorithm, enabling easy scaling to high-dimensional domains. Experiments in classic control tasks, MinAtar and DeepMind Control Suite demonstrate the effectiveness and the applicability of our method.",
    "link": "http://arxiv.org/abs/2306.05859",
    "context": "Title: Robust Reinforcement Learning via Adversarial Kernel Approximation. (arXiv:2306.05859v1 [cs.LG])\nAbstract: Robust Markov Decision Processes (RMDPs) provide a framework for sequential decision-making that is robust to perturbations on the transition kernel. However, robust reinforcement learning (RL) approaches in RMDPs do not scale well to realistic online settings with high-dimensional domains. By characterizing the adversarial kernel in RMDPs, we propose a novel approach for online robust RL that approximates the adversarial kernel and uses a standard (non-robust) RL algorithm to learn a robust policy. Notably, our approach can be applied on top of any underlying RL algorithm, enabling easy scaling to high-dimensional domains. Experiments in classic control tasks, MinAtar and DeepMind Control Suite demonstrate the effectiveness and the applicability of our method.",
    "path": "papers/23/06/2306.05859.json",
    "total_tokens": 899,
    "translated_title": "通过对抗核近似实现鲁棒强化学习",
    "translated_abstract": "鲁棒性马尔可夫决策过程提供了一个框架，可以在转移核发生扰动的情况下进行序列决策。然而，在具有高维度域的现实在线环境中，鲁棒强化学习方法并不容易扩展。通过表征鲁棒性马尔可夫决策过程中的对抗核，我们提出一种新的在线鲁棒强化学习方法，它近似对抗核并使用标准（非鲁棒）的强化学习算法来学习一个鲁棒性方针。值得注意的是，我们的方法可以应用于任何基础的强化学习算法之上，可以实现对高维度域的轻松扩展。在经典控制任务、MinAtar和DeepMind控制套件中的实验中，我们的方法表现出了有效性和适用性。",
    "tldr": "该论文提出了一种新奇的在线鲁棒强化学习方法，它通过近似对抗核并使用标准非鲁棒强化学习算法学习鲁棒策略，可应用于任何基础的强化学习算法之上，可以轻松扩展到高维度域。",
    "en_tdlr": "This paper proposes a novel online robust reinforcement learning method, which approximates the adversarial kernel and uses a standard non-robust reinforcement learning algorithm to learn a robust policy, can be applied on top of any underlying reinforcement learning algorithm, and can be easily scaled to high-dimensional domains, and demonstrates effectiveness and applicability in classic control tasks, MinAtar, and DeepMind Control Suite."
}