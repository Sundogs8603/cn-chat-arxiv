{
    "title": "shs-nlp at RadSum23: Domain-Adaptive Pre-training of Instruction-tuned LLMs for Radiology Report Impression Generation. (arXiv:2306.03264v1 [cs.CL])",
    "abstract": "Instruction-tuned generative Large language models (LLMs) like ChatGPT and Bloomz possess excellent generalization abilities, but they face limitations in understanding radiology reports, particularly in the task of generating the IMPRESSIONS section from the FINDINGS section. They tend to generate either verbose or incomplete IMPRESSIONS, mainly due to insufficient exposure to medical text data during training. We present a system which leverages large-scale medical text data for domain-adaptive pre-training of instruction-tuned LLMs to enhance its medical knowledge and performance on specific medical tasks. We show that this system performs better in a zero-shot setting than a number of pretrain-and-finetune adaptation methods on the IMPRESSIONS generation task, and ranks 1st among participating systems in Task 1B: Radiology Report Summarization at the BioNLP 2023 workshop.",
    "link": "http://arxiv.org/abs/2306.03264",
    "context": "Title: shs-nlp at RadSum23: Domain-Adaptive Pre-training of Instruction-tuned LLMs for Radiology Report Impression Generation. (arXiv:2306.03264v1 [cs.CL])\nAbstract: Instruction-tuned generative Large language models (LLMs) like ChatGPT and Bloomz possess excellent generalization abilities, but they face limitations in understanding radiology reports, particularly in the task of generating the IMPRESSIONS section from the FINDINGS section. They tend to generate either verbose or incomplete IMPRESSIONS, mainly due to insufficient exposure to medical text data during training. We present a system which leverages large-scale medical text data for domain-adaptive pre-training of instruction-tuned LLMs to enhance its medical knowledge and performance on specific medical tasks. We show that this system performs better in a zero-shot setting than a number of pretrain-and-finetune adaptation methods on the IMPRESSIONS generation task, and ranks 1st among participating systems in Task 1B: Radiology Report Summarization at the BioNLP 2023 workshop.",
    "path": "papers/23/06/2306.03264.json",
    "total_tokens": 1003,
    "translated_title": "RadSum23 上的 shs-nlp: 面向放射学报告的指导调整语言模型基于领域自适应预训练下的印象生成",
    "translated_abstract": "ChatGPT 和 Bloomz 这样的指导调整生成语言模型在普适性方面表现出色，但在理解放射学报告方面存在局限性，特别是在从所发现的内容生成 IMPRESSIONS 部分的任务中。这些模型会生成冗长或不完整的 IMPRESSIONS，主要是由于训练期间对医学文本数据的曝光不足。我们提出了一个系统，利用大规模医学文本数据进行基于领域自适应预训练的指导调整语言模型的训练，以增强其医学知识和在特定医学任务上的性能。我们展示了在零-shot 设置下，这个系统在 IMPRESSIONS 生成任务上比许多预训练和微调自适应方法表现更好，并且在 BioNLP 2023 研讨会的任务1B：放射学报告摘要中排名第1。",
    "tldr": "本文提出了一个基于领域自适应预训练的指导调整语言模型的系统，用于增强其医学知识和在特定医学任务上的性能，特别是在 IMPRESSIONS 生成任务中表现出色，比许多预训练和微调自适应方法表现更好，并且在 BioNLP 2023 研讨会的任务1B：放射学报告摘要中排名第1。",
    "en_tdlr": "This paper introduces a system that leverages domain-adaptive pre-training of instruction-tuned LLMs using large-scale medical text data to enhance medical knowledge and performance on specific tasks, particularly in generating IMPRESSIONS section from FINDINGS section in radiology reports. It outperforms many pretrain-and-finetune adaptation methods in a zero-shot setting, and ranks 1st in Task 1B: Radiology Report Summarization at the BioNLP 2023 workshop."
}