{
    "title": "Finite Gaussian Neurons: Defending against adversarial attacks by making neural networks say \"I don't know\". (arXiv:2306.07796v1 [cs.LG])",
    "abstract": "Since 2014, artificial neural networks have been known to be vulnerable to adversarial attacks, which can fool the network into producing wrong or nonsensical outputs by making humanly imperceptible alterations to inputs. While defenses against adversarial attacks have been proposed, they usually involve retraining a new neural network from scratch, a costly task. In this work, I introduce the Finite Gaussian Neuron (FGN), a novel neuron architecture for artificial neural networks. My works aims to: - easily convert existing models to Finite Gaussian Neuron architecture, - while preserving the existing model's behavior on real data, - and offering resistance against adversarial attacks. I show that converted and retrained Finite Gaussian Neural Networks (FGNN) always have lower confidence (i.e., are not overconfident) in their predictions over randomized and Fast Gradient Sign Method adversarial images when compared to classical neural networks, while maintaining high accuracy and conf",
    "link": "http://arxiv.org/abs/2306.07796",
    "context": "Title: Finite Gaussian Neurons: Defending against adversarial attacks by making neural networks say \"I don't know\". (arXiv:2306.07796v1 [cs.LG])\nAbstract: Since 2014, artificial neural networks have been known to be vulnerable to adversarial attacks, which can fool the network into producing wrong or nonsensical outputs by making humanly imperceptible alterations to inputs. While defenses against adversarial attacks have been proposed, they usually involve retraining a new neural network from scratch, a costly task. In this work, I introduce the Finite Gaussian Neuron (FGN), a novel neuron architecture for artificial neural networks. My works aims to: - easily convert existing models to Finite Gaussian Neuron architecture, - while preserving the existing model's behavior on real data, - and offering resistance against adversarial attacks. I show that converted and retrained Finite Gaussian Neural Networks (FGNN) always have lower confidence (i.e., are not overconfident) in their predictions over randomized and Fast Gradient Sign Method adversarial images when compared to classical neural networks, while maintaining high accuracy and conf",
    "path": "papers/23/06/2306.07796.json",
    "total_tokens": 955,
    "translated_title": "有限高斯神经元：通过让神经网络说“我不知道”来防御对抗性攻击",
    "translated_abstract": "自2014年以来，人工神经网络已知容易受到对抗性攻击的影响，这些攻击可以通过对输入进行微小改动，使网络产生错误或无意义的输出。虽然已提出防御对抗性攻击的方法，但这些方法通常涉及从头重新训练一个新的神经网络，是一个昂贵的任务。在这项工作中，我引入了有限高斯神经元（FGN），这是一种人工神经网络的新型神经元结构。我的工作旨在：-将现有模型轻松转换为有限高斯神经元结构，-同时保留现有模型在真实数据上的行为，-并抵抗对抗性攻击。我展示了转换和重新训练的有限高斯神经网络（FGNN）在与经典神经网络相比较时，在随机和快速梯度符号方法对抗图像上预测始终具有较低的置信度（即不会过度自信），同时保持高准确性和可信度。",
    "tldr": "这篇论文介绍了有限高斯神经元（FGN）这种新的神经元架构，可以通过简单地将现有模型转换为FGN结构，从而抵御对抗性攻击并同时保持高准确性和可信度。",
    "en_tdlr": "The paper introduces a novel Finite Gaussian Neuron (FGN) architecture for artificial neural networks, which can resist adversarial attacks while maintaining high accuracy and confidence. The FGN can be easily converted from existing models and always has lower confidence in their predictions over adversarial images compared to classical neural networks."
}