{
    "title": "A Comprehensive Survey on Applications of Transformers for Deep Learning Tasks. (arXiv:2306.07303v1 [cs.LG])",
    "abstract": "Transformer is a deep neural network that employs a self-attention mechanism to comprehend the contextual relationships within sequential data. Unlike conventional neural networks or updated versions of Recurrent Neural Networks (RNNs) such as Long Short-Term Memory (LSTM), transformer models excel in handling long dependencies between input sequence elements and enable parallel processing. As a result, transformer-based models have attracted substantial interest among researchers in the field of artificial intelligence. This can be attributed to their immense potential and remarkable achievements, not only in Natural Language Processing (NLP) tasks but also in a wide range of domains, including computer vision, audio and speech processing, healthcare, and the Internet of Things (IoT). Although several survey papers have been published highlighting the transformer's contributions in specific fields, architectural differences, or performance evaluations, there is still a significant abs",
    "link": "http://arxiv.org/abs/2306.07303",
    "context": "Title: A Comprehensive Survey on Applications of Transformers for Deep Learning Tasks. (arXiv:2306.07303v1 [cs.LG])\nAbstract: Transformer is a deep neural network that employs a self-attention mechanism to comprehend the contextual relationships within sequential data. Unlike conventional neural networks or updated versions of Recurrent Neural Networks (RNNs) such as Long Short-Term Memory (LSTM), transformer models excel in handling long dependencies between input sequence elements and enable parallel processing. As a result, transformer-based models have attracted substantial interest among researchers in the field of artificial intelligence. This can be attributed to their immense potential and remarkable achievements, not only in Natural Language Processing (NLP) tasks but also in a wide range of domains, including computer vision, audio and speech processing, healthcare, and the Internet of Things (IoT). Although several survey papers have been published highlighting the transformer's contributions in specific fields, architectural differences, or performance evaluations, there is still a significant abs",
    "path": "papers/23/06/2306.07303.json",
    "total_tokens": 867,
    "translated_title": "基于Transformer的深度学习任务应用综述",
    "translated_abstract": "Transformer是一种深度神经网络，采用自注意机制来理解序列数据中的上下文关系。与传统神经网络或更新版本的循环神经网络（RNN）（如长短期记忆（LSTM））不同，Transformer模型在处理输入序列元素之间的长依赖关系和实现并行处理方面表现出色。因此，基于Transformer的模型在人工智能领域引起了广泛兴趣。这得益于它们在自然语言处理（NLP）任务以及计算机视觉、音频和语音处理、医疗保健和物联网（IoT）等各个领域中的巨大潜力和显著成就。虽然已经出版了几篇综述文章，重点介绍了Transformer在特定领域的贡献、架构差异或性能评估，但仍存在较大的空白。",
    "tldr": "本文综述了基于Transformer的深度学习任务应用，Transformer能够理解序列数据中的上下文关系且实现并行处理，在NLP、计算机视觉、语音处理、医疗保健和物联网等领域表现出色。",
    "en_tdlr": "This survey paper provides an overview of the applications of Transformer-based deep learning tasks, highlighting its ability to comprehend contextual relationships within sequential data using a self-attention mechanism and enabling parallel processing. Transformer models have shown remarkable achievements in various domains, including NLP, computer vision, speech processing, healthcare, and IoT."
}