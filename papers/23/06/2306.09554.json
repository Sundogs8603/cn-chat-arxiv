{
    "title": "Low-Switching Policy Gradient with Exploration via Online Sensitivity Sampling. (arXiv:2306.09554v1 [cs.LG])",
    "abstract": "Policy optimization methods are powerful algorithms in Reinforcement Learning (RL) for their flexibility to deal with policy parameterization and ability to handle model misspecification. However, these methods usually suffer from slow convergence rates and poor sample complexity. Hence it is important to design provably sample efficient algorithms for policy optimization. Yet, recent advances for this problems have only been successful in tabular and linear setting, whose benign structures cannot be generalized to non-linearly parameterized policies. In this paper, we address this problem by leveraging recent advances in value-based algorithms, including bounded eluder-dimension and online sensitivity sampling, to design a low-switching sample-efficient policy optimization algorithm, LPO, with general non-linear function approximation. We show that, our algorithm obtains an $\\varepsilon$-optimal policy with only $\\widetilde{O}(\\frac{\\text{poly}(d)}{\\varepsilon^3})$ samples, where $\\va",
    "link": "http://arxiv.org/abs/2306.09554",
    "context": "Title: Low-Switching Policy Gradient with Exploration via Online Sensitivity Sampling. (arXiv:2306.09554v1 [cs.LG])\nAbstract: Policy optimization methods are powerful algorithms in Reinforcement Learning (RL) for their flexibility to deal with policy parameterization and ability to handle model misspecification. However, these methods usually suffer from slow convergence rates and poor sample complexity. Hence it is important to design provably sample efficient algorithms for policy optimization. Yet, recent advances for this problems have only been successful in tabular and linear setting, whose benign structures cannot be generalized to non-linearly parameterized policies. In this paper, we address this problem by leveraging recent advances in value-based algorithms, including bounded eluder-dimension and online sensitivity sampling, to design a low-switching sample-efficient policy optimization algorithm, LPO, with general non-linear function approximation. We show that, our algorithm obtains an $\\varepsilon$-optimal policy with only $\\widetilde{O}(\\frac{\\text{poly}(d)}{\\varepsilon^3})$ samples, where $\\va",
    "path": "papers/23/06/2306.09554.json",
    "total_tokens": 959,
    "translated_title": "通过在线灵敏度抽样实现具探索性的低切换策略梯度",
    "translated_abstract": "策略优化方法是增强学习中强大的算法之一，其具备对策略参数化的灵活性和处理模型错误的能力。然而，这些方法通常存在收敛速度缓慢，样本复杂度低的缺点。因此，设计可证明的策略优化算法对于提高样本效率至关重要。而针对这个问题的最新进展只适用于表格和线性设置，且这些成果的良好结构不能推广到非线性参数化的策略。本文利用最近价值算法包括有界eluder-维数和在线灵敏度抽样等的进展，针对一般非线性函数逼近设计低切换样本有效的策略优化算法LPO。我们证明了我们的算法只需$\\widetilde{O}(\\frac{\\text{poly}(d)}{\\varepsilon^3})$个样本即可获得$\\varepsilon$-最优策略。",
    "tldr": "本文提出了一个具有探索和样本复杂度低的策略优化算法LPO，使用有界eluder-维数和在线灵敏度抽样来适用于非线性参数化的策略，并且在较少的样本量下获得了近似最优策略。",
    "en_tdlr": "This paper proposes a low-switching policy gradient algorithm, LPO, with exploration and low sample complexity for policy optimization. It leverages recent advances in value-based algorithms, such as bounded Eluder-dimension and online sensitivity sampling, to apply to general nonlinear function approximation and obtain $\\varepsilon$-optimal policy with $\\widetilde{O}(\\frac{\\text{poly}(d)}{\\varepsilon^3})$ samples."
}