{
    "title": "Effective Structured Prompting by Meta-Learning and Representative Verbalizer",
    "abstract": "arXiv:2306.00618v2 Announce Type: replace-cross  Abstract: Prompt tuning for pre-trained masked language models (MLM) has shown promising performance in natural language processing tasks with few labeled examples. It tunes a prompt for the downstream task, and a verbalizer is used to bridge the predicted token and label prediction. Due to the limited training data, prompt initialization is crucial for prompt tuning. Recently, MetaPrompting (Hou et al., 2022) uses meta-learning to learn a shared initialization for all task-specific prompts. However, a single initialization is insufficient to obtain good prompts for all tasks and samples when the tasks are complex. Moreover, MetaPrompting requires tuning the whole MLM, causing a heavy burden on computation and memory as the MLM is usually large. To address these issues, we use a prompt pool to extract more task knowledge and construct instance-dependent prompts via attention. We further propose a novel soft verbalizer (RepVerb) which con",
    "link": "https://arxiv.org/abs/2306.00618",
    "context": "Title: Effective Structured Prompting by Meta-Learning and Representative Verbalizer\nAbstract: arXiv:2306.00618v2 Announce Type: replace-cross  Abstract: Prompt tuning for pre-trained masked language models (MLM) has shown promising performance in natural language processing tasks with few labeled examples. It tunes a prompt for the downstream task, and a verbalizer is used to bridge the predicted token and label prediction. Due to the limited training data, prompt initialization is crucial for prompt tuning. Recently, MetaPrompting (Hou et al., 2022) uses meta-learning to learn a shared initialization for all task-specific prompts. However, a single initialization is insufficient to obtain good prompts for all tasks and samples when the tasks are complex. Moreover, MetaPrompting requires tuning the whole MLM, causing a heavy burden on computation and memory as the MLM is usually large. To address these issues, we use a prompt pool to extract more task knowledge and construct instance-dependent prompts via attention. We further propose a novel soft verbalizer (RepVerb) which con",
    "path": "papers/23/06/2306.00618.json",
    "total_tokens": 828,
    "translated_title": "通过元学习和代表性语言化器实现有效的结构化提示",
    "translated_abstract": "预训练的遮蔽语言模型（MLM）的提示调整在自然语言处理任务中显示出有限标记示例的良好性能。它为下游任务调整提示，并使用语言化器来连接预测的标记和标签预测。由于训练数据有限，提示初始化对于提示调整至关重要。最近，MetaPrompting（Hou等，2022）使用元学习来学习所有特定任务提示的共享初始化。然而，当任务复杂时，单一初始化无法获得所有任务和样本的良好提示。此外，由于MLM通常很大，MetaPrompting需要调整整个MLM，导致计算和内存负担沉重。为了解决这些问题，我们使用提示池提取更多任务知识，并通过注意力构建基于实例的提示。我们进一步提出了一种新颖的软语言化器（RepVerb）...（剩余内容未提供）",
    "tldr": "通过使用提示池和构建基于实例的提示以及引入新颖的软语言化器，提出了一种通过元学习和代表性语言化器实现有效的结构化提示的方法"
}