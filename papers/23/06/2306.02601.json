{
    "title": "Aiming towards the minimizers: fast convergence of SGD for overparametrized problems. (arXiv:2306.02601v1 [cs.LG])",
    "abstract": "Modern machine learning paradigms, such as deep learning, occur in or close to the interpolation regime, wherein the number of model parameters is much larger than the number of data samples. In this work, we propose a regularity condition within the interpolation regime which endows the stochastic gradient method with the same worst-case iteration complexity as the deterministic gradient method, while using only a single sampled gradient (or a minibatch) in each iteration. In contrast, all existing guarantees require the stochastic gradient method to take small steps, thereby resulting in a much slower linear rate of convergence. Finally, we demonstrate that our condition holds when training sufficiently wide feedforward neural networks with a linear output layer.",
    "link": "http://arxiv.org/abs/2306.02601",
    "context": "Title: Aiming towards the minimizers: fast convergence of SGD for overparametrized problems. (arXiv:2306.02601v1 [cs.LG])\nAbstract: Modern machine learning paradigms, such as deep learning, occur in or close to the interpolation regime, wherein the number of model parameters is much larger than the number of data samples. In this work, we propose a regularity condition within the interpolation regime which endows the stochastic gradient method with the same worst-case iteration complexity as the deterministic gradient method, while using only a single sampled gradient (or a minibatch) in each iteration. In contrast, all existing guarantees require the stochastic gradient method to take small steps, thereby resulting in a much slower linear rate of convergence. Finally, we demonstrate that our condition holds when training sufficiently wide feedforward neural networks with a linear output layer.",
    "path": "papers/23/06/2306.02601.json",
    "total_tokens": 810,
    "translated_title": "朝向最小化器：过参数化问题中随机梯度下降的快速收敛",
    "translated_abstract": "现代机器学习范式，如深度学习，出现在插值区域内或接近插值区域，其中模型参数的数量远大于数据样本的数量。在这项工作中，我们提出了插值区域内的一种正则条件，赋予随机梯度方法与确定性梯度方法相同的最坏迭代复杂度，而在每次迭代中仅使用单个抽样的梯度（或小批量）。相比之下，所有现有的保证都要求随机梯度方法采取小步长，从而导致收敛速率明显减慢。最后，我们证明了当训练具有线性输出层的足够宽的前馈神经网络时，我们的条件成立。",
    "tldr": "本文提出了在插值区域内的正则条件，对于过参数化问题采用随机梯度下降方法能够达到和确定性梯度下降相同的最坏情况复杂度，并具有更快的收敛速率。",
    "en_tdlr": "This paper proposes a regularity condition within the interpolation regime which enables the stochastic gradient method to achieve the same worst-case complexity as the deterministic gradient method in overparametrized problems, while using only a single sampled gradient (or a minibatch) in each iteration, resulting in faster convergence rates."
}