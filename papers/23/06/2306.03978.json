{
    "title": "B\\\"{u}y\\\"{u}k dil modellerinin T\\\"{u}rk\\c{c}e verisetleri ile e\\u{g}itilmesi ve ince ayarlanmas\\i. (arXiv:2306.03978v1 [cs.CL])",
    "abstract": "Large language models have advanced enormously, gained vast attraction and are having a phase of intensed research. Some of the developed models and training datasets have been made open-accessible. Hence these may be further fine-tuned with some techniques to obtain specialized models for specific tasks. When it comes to Turkish language, open-access models do not provide satisfactory coverage. This is also observed over published datasets. In this work, we propose some ideas to mitigate this issue: creating large Turkish datasets, training LLMs with these and fine-tuning pre-trained models with Turkish inputs. We report our findings on Turkish-based trainings with the problems encountered along the way. We conclude with outcomes of these experiments and propose ideas for further works.  - B\\\"uy\\\"uk dil modelleri inan{\\i}lmaz \\\"ol\\c{c}\\\"ude geli\\c{s}mekte, b\\\"uy\\\"uk ilgi toplayarak ve \\\"uzerlerinde yo\\u{g}un ara\\c{s}tirmalarin yapildi\\u{g}i bir d\\\"onemdedirler. Geli\\c{s}tirilen mode",
    "link": "http://arxiv.org/abs/2306.03978",
    "context": "Title: B\\\"{u}y\\\"{u}k dil modellerinin T\\\"{u}rk\\c{c}e verisetleri ile e\\u{g}itilmesi ve ince ayarlanmas\\i. (arXiv:2306.03978v1 [cs.CL])\nAbstract: Large language models have advanced enormously, gained vast attraction and are having a phase of intensed research. Some of the developed models and training datasets have been made open-accessible. Hence these may be further fine-tuned with some techniques to obtain specialized models for specific tasks. When it comes to Turkish language, open-access models do not provide satisfactory coverage. This is also observed over published datasets. In this work, we propose some ideas to mitigate this issue: creating large Turkish datasets, training LLMs with these and fine-tuning pre-trained models with Turkish inputs. We report our findings on Turkish-based trainings with the problems encountered along the way. We conclude with outcomes of these experiments and propose ideas for further works.  - B\\\"uy\\\"uk dil modelleri inan{\\i}lmaz \\\"ol\\c{c}\\\"ude geli\\c{s}mekte, b\\\"uy\\\"uk ilgi toplayarak ve \\\"uzerlerinde yo\\u{g}un ara\\c{s}tirmalarin yapildi\\u{g}i bir d\\\"onemdedirler. Geli\\c{s}tirilen mode",
    "path": "papers/23/06/2306.03978.json",
    "total_tokens": 1332,
    "translated_title": "用土耳其语数据训练和微调大型语言模型",
    "translated_abstract": "大型语言模型的发展得到了极大的提升和吸引力，并处于密集研究阶段。其中开放访问的模型和训练数据集已公开。因此，这些模型可以进一步使用一些技术进行微调，以获得特定任务的专业模型。就土耳其语而言，开放访问的模型不提供满意的覆盖范围。本文提出了一些想法来缓解这个问题：创建大型土耳其语数据集，用这些训练语言模型，并用土耳其语输入微调预训练的模型。我们报告了在土耳其语训练中遇到的问题，并在结论中提出了这些实验的结果和进一步工作的想法。",
    "tldr": "本文提出了一些解决土耳其语开放访问模型不足的方法：创建大型土耳其语数据集，用这些训练语言模型，并微调预训练的模型以获得特定任务的专业模型。",
    "en_tdlr": "This paper proposes some methods to solve the inadequacy of open-access models for Turkish, including creating large Turkish datasets, training language models with these datasets, and fine-tuning pre-trained models with Turkish inputs to obtain specialized models for specific tasks."
}