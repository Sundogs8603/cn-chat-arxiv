{
    "title": "MUBen: Benchmarking the Uncertainty of Pre-Trained Models for Molecular Property Prediction. (arXiv:2306.10060v1 [physics.chem-ph])",
    "abstract": "Large Transformer models pre-trained on massive unlabeled molecular data have shown great success in predicting molecular properties. However, these models can be prone to overfitting during fine-tuning, resulting in over-confident predictions on test data that fall outside of the training distribution. To address this issue, uncertainty quantification (UQ) methods can be used to improve the models' calibration of predictions. Although many UQ approaches exist, not all of them lead to improved performance. While some studies have used UQ to improve molecular pre-trained models, the process of selecting suitable backbone and UQ methods for reliable molecular uncertainty estimation remains underexplored. To address this gap, we present MUBen, which evaluates different combinations of backbone and UQ models to quantify their performance for both property prediction and uncertainty estimation. By fine-tuning various backbone molecular representation models using different molecular descrip",
    "link": "http://arxiv.org/abs/2306.10060",
    "context": "Title: MUBen: Benchmarking the Uncertainty of Pre-Trained Models for Molecular Property Prediction. (arXiv:2306.10060v1 [physics.chem-ph])\nAbstract: Large Transformer models pre-trained on massive unlabeled molecular data have shown great success in predicting molecular properties. However, these models can be prone to overfitting during fine-tuning, resulting in over-confident predictions on test data that fall outside of the training distribution. To address this issue, uncertainty quantification (UQ) methods can be used to improve the models' calibration of predictions. Although many UQ approaches exist, not all of them lead to improved performance. While some studies have used UQ to improve molecular pre-trained models, the process of selecting suitable backbone and UQ methods for reliable molecular uncertainty estimation remains underexplored. To address this gap, we present MUBen, which evaluates different combinations of backbone and UQ models to quantify their performance for both property prediction and uncertainty estimation. By fine-tuning various backbone molecular representation models using different molecular descrip",
    "path": "papers/23/06/2306.10060.json",
    "total_tokens": 886,
    "translated_title": "MUBen：评估分子性质预测预训练模型的不确定性基准",
    "translated_abstract": "预训练于大规模无标签分子数据的大型Transformer模型在预测分子性质方面取得了巨大成功。然而，在微调期间，这些模型可能容易出现过拟合，导致对测试数据的过度自信预测落在了训练分布之外。为了解决这个问题，可以使用不确定性量化（UQ）方法来改善模型的预测校准。虽然存在许多UQ方法，但并不是所有方法都会导致性能改善。虽然一些研究使用UQ来改善分子预训练模型，但选择适合的骨干和UQ方法以可靠地估计分子不确定性的过程仍然是未经探索的。为了解决这个差距，我们提出了MUBen，评估不同的骨干和UQ模型组合，以量化它们在属性预测和不确定性估计方面的性能。通过微调使用不同分子描述符的各种骨干分子表示模型。",
    "tldr": "MUBen评估不同骨干和UQ模型组合对分子不确定性估计和属性预测的性能，以解决预训练模型微调中的过拟合校准问题。",
    "en_tdlr": "MUBen evaluates various combinations of backbone and UQ models for reliable molecular uncertainty estimation and property prediction, addressing the overfitting calibration problem during pre-trained model fine-tuning."
}