{
    "title": "Explore and Exploit the Diverse Knowledge in Model Zoo for Domain Generalization. (arXiv:2306.02595v1 [cs.LG])",
    "abstract": "The proliferation of pretrained models, as a result of advancements in pretraining techniques, has led to the emergence of a vast zoo of publicly available models. Effectively utilizing these resources to obtain models with robust out-of-distribution generalization capabilities for downstream tasks has become a crucial area of research. Previous research has primarily focused on identifying the most powerful models within the model zoo, neglecting to fully leverage the diverse inductive biases contained within. This paper argues that the knowledge contained in weaker models is valuable and presents a method for leveraging the diversity within the model zoo to improve out-of-distribution generalization capabilities. Specifically, we investigate the behaviors of various pretrained models across different domains of downstream tasks by characterizing the variations in their encoded representations in terms of two dimensions: diversity shift and correlation shift. This characterization ena",
    "link": "http://arxiv.org/abs/2306.02595",
    "context": "Title: Explore and Exploit the Diverse Knowledge in Model Zoo for Domain Generalization. (arXiv:2306.02595v1 [cs.LG])\nAbstract: The proliferation of pretrained models, as a result of advancements in pretraining techniques, has led to the emergence of a vast zoo of publicly available models. Effectively utilizing these resources to obtain models with robust out-of-distribution generalization capabilities for downstream tasks has become a crucial area of research. Previous research has primarily focused on identifying the most powerful models within the model zoo, neglecting to fully leverage the diverse inductive biases contained within. This paper argues that the knowledge contained in weaker models is valuable and presents a method for leveraging the diversity within the model zoo to improve out-of-distribution generalization capabilities. Specifically, we investigate the behaviors of various pretrained models across different domains of downstream tasks by characterizing the variations in their encoded representations in terms of two dimensions: diversity shift and correlation shift. This characterization ena",
    "path": "papers/23/06/2306.02595.json",
    "total_tokens": 890,
    "translated_title": "探索和利用模型库中多样化的知识以实现领域泛化",
    "translated_abstract": "预训练模型的广泛应用已经导致了大量公共可用模型的出现。有效地利用这些资源，以获得在下游任务中具有强韧性的模型，已成为一个至关重要的研究领域。之前的研究主要集中在识别模型库中最强大的模型上，而忽视了其中包含的多样的归纳偏差。本文认为弱模型中所包含的知识具有价值，并提出了一种利用模型库中多样性以提高领域泛化能力的方法。具体来说，我们通过将编码表示的变化刻画为多元的“多样性偏移”和“相关性偏移”，来研究不同领域下的多种预训练模型的行为。",
    "tldr": "本文探讨了通过利用模型库中多样化的知识来提高领域泛化能力的方法，强调认为弱模型中所包含的知识具有价值。通过比较各种已预训练好的模型在不同领域下的表现，刻画它们在编码表示上的多样性偏移和相关性偏移等特征以提高领域泛化能力。",
    "en_tdlr": "This paper proposes a method for improving domain generalization capabilities by exploring and leveraging diverse knowledge within a model zoo, including the valuable knowledge contained in weaker models. By characterizing the variations in encoded representations in terms of diversity shift and correlation shift, various pretrained models are compared across different domains of downstream tasks."
}