{
    "title": "End-to-end Differentiable Clustering with Associative Memories. (arXiv:2306.03209v1 [cs.LG])",
    "abstract": "Clustering is a widely used unsupervised learning technique involving an intensive discrete optimization problem. Associative Memory models or AMs are differentiable neural networks defining a recursive dynamical system, which have been integrated with various deep learning architectures. We uncover a novel connection between the AM dynamics and the inherent discrete assignment necessary in clustering to propose a novel unconstrained continuous relaxation of the discrete clustering problem, enabling end-to-end differentiable clustering with AM, dubbed ClAM. Leveraging the pattern completion ability of AMs, we further develop a novel self-supervised clustering loss. Our evaluations on varied datasets demonstrate that ClAM benefits from the self-supervision, and significantly improves upon both the traditional Lloyd's k-means algorithm, and more recent continuous clustering relaxations (by upto 60% in terms of the Silhouette Coefficient).",
    "link": "http://arxiv.org/abs/2306.03209",
    "context": "Title: End-to-end Differentiable Clustering with Associative Memories. (arXiv:2306.03209v1 [cs.LG])\nAbstract: Clustering is a widely used unsupervised learning technique involving an intensive discrete optimization problem. Associative Memory models or AMs are differentiable neural networks defining a recursive dynamical system, which have been integrated with various deep learning architectures. We uncover a novel connection between the AM dynamics and the inherent discrete assignment necessary in clustering to propose a novel unconstrained continuous relaxation of the discrete clustering problem, enabling end-to-end differentiable clustering with AM, dubbed ClAM. Leveraging the pattern completion ability of AMs, we further develop a novel self-supervised clustering loss. Our evaluations on varied datasets demonstrate that ClAM benefits from the self-supervision, and significantly improves upon both the traditional Lloyd's k-means algorithm, and more recent continuous clustering relaxations (by upto 60% in terms of the Silhouette Coefficient).",
    "path": "papers/23/06/2306.03209.json",
    "total_tokens": 875,
    "translated_title": "带有关联记忆的端到端可微聚类",
    "translated_abstract": "聚类是一种广泛使用的无监督学习技术，涉及强烈的离散优化问题。关联记忆模型或AM是可微的神经网络，定义了递归动态系统，并已与各种深度学习体系结构集成。我们揭示了AM动态和聚类中固有的离散分配之间的新联系，以提出一种新的无约束连续传播离散聚类问题，使得AM可以进行端到端可微的聚类，称为ClAM。利用AM的模式完成能力，我们进一步开发了一种新的自监督聚类损失。我们对各种数据集的评估表明，ClAM从自监督中受益，并且在传统的Lloyd's k-means算法和最近的连续聚类松弛方案上均显着改善了（在轮廓系数方面提高了60％）。",
    "tldr": "本文提出了基于关联记忆的端到端可微聚类算法ClAM，结合模式完成技术开发自监督聚类损失函数，相对传统算法和最新的连续聚类松弛方案在轮廓系数方面提高了60%。",
    "en_tdlr": "This paper proposes an end-to-end differentiable clustering algorithm called ClAM based on associative memory models, which leverages pattern completion to develop a novel self-supervised clustering loss, and achieves significant improvements (up to 60% in terms of the Silhouette Coefficient) compared with traditional algorithms and recent continuous clustering relaxation methods."
}