{
    "title": "MKOR: Momentum-Enabled Kronecker-Factor-Based Optimizer Using Rank-1 Updates. (arXiv:2306.01685v1 [cs.LG])",
    "abstract": "This work proposes a Momentum-Enabled Kronecker-Factor-Based Optimizer Using Rank-1 updates, called MKOR, that improves the training time and convergence properties of deep neural networks (DNNs). Second-order techniques, while enjoying higher convergence rates vs first-order counterparts, have cubic complexity with respect to either the model size and/or the training batch size. Hence they exhibit poor scalability and performance in transformer models, e.g. large language models (LLMs), because the batch sizes in these models scale by the attention mechanism sequence length, leading to large model size and batch sizes. MKOR's complexity is quadratic with respect to the model size, alleviating the computation bottlenecks in second-order methods. Because of their high computation complexity, state-of-the-art implementations of second-order methods can only afford to update the second order information infrequently, and thus do not fully exploit the promise of better convergence from the",
    "link": "http://arxiv.org/abs/2306.01685",
    "context": "Title: MKOR: Momentum-Enabled Kronecker-Factor-Based Optimizer Using Rank-1 Updates. (arXiv:2306.01685v1 [cs.LG])\nAbstract: This work proposes a Momentum-Enabled Kronecker-Factor-Based Optimizer Using Rank-1 updates, called MKOR, that improves the training time and convergence properties of deep neural networks (DNNs). Second-order techniques, while enjoying higher convergence rates vs first-order counterparts, have cubic complexity with respect to either the model size and/or the training batch size. Hence they exhibit poor scalability and performance in transformer models, e.g. large language models (LLMs), because the batch sizes in these models scale by the attention mechanism sequence length, leading to large model size and batch sizes. MKOR's complexity is quadratic with respect to the model size, alleviating the computation bottlenecks in second-order methods. Because of their high computation complexity, state-of-the-art implementations of second-order methods can only afford to update the second order information infrequently, and thus do not fully exploit the promise of better convergence from the",
    "path": "papers/23/06/2306.01685.json",
    "total_tokens": 906,
    "tldr": "MKOR是一个使用一阶随机更新的动量克朗克因子优化器，它提高了深度神经网络的训练时间和收敛性能，缓解了二阶方法中的计算瓶颈。"
}