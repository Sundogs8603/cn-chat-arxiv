{
    "title": "Tune As You Scale: Hyperparameter Optimization For Compute Efficient Training. (arXiv:2306.08055v1 [cs.LG])",
    "abstract": "Hyperparameter tuning of deep learning models can lead to order-of-magnitude performance gains for the same amount of compute. Despite this, systematic tuning is uncommon, particularly for large models, which are expensive to evaluate and tend to have many hyperparameters, necessitating difficult judgment calls about tradeoffs, budgets, and search bounds. To address these issues and propose a practical method for robustly tuning large models, we present Cost-Aware Pareto Region Bayesian Search (CARBS), a Bayesian optimization algorithm that performs local search around the performance-cost Pareto frontier. CARBS does well even in unbounded search spaces with many hyperparameters, learns scaling relationships so that it can tune models even as they are scaled up, and automates much of the \"black magic\" of tuning. Among our results, we effectively solve the entire ProcGen benchmark just by tuning a simple baseline (PPO, as provided in the original ProcGen paper). We also reproduce the mo",
    "link": "http://arxiv.org/abs/2306.08055",
    "context": "Title: Tune As You Scale: Hyperparameter Optimization For Compute Efficient Training. (arXiv:2306.08055v1 [cs.LG])\nAbstract: Hyperparameter tuning of deep learning models can lead to order-of-magnitude performance gains for the same amount of compute. Despite this, systematic tuning is uncommon, particularly for large models, which are expensive to evaluate and tend to have many hyperparameters, necessitating difficult judgment calls about tradeoffs, budgets, and search bounds. To address these issues and propose a practical method for robustly tuning large models, we present Cost-Aware Pareto Region Bayesian Search (CARBS), a Bayesian optimization algorithm that performs local search around the performance-cost Pareto frontier. CARBS does well even in unbounded search spaces with many hyperparameters, learns scaling relationships so that it can tune models even as they are scaled up, and automates much of the \"black magic\" of tuning. Among our results, we effectively solve the entire ProcGen benchmark just by tuning a simple baseline (PPO, as provided in the original ProcGen paper). We also reproduce the mo",
    "path": "papers/23/06/2306.08055.json",
    "total_tokens": 1135,
    "translated_title": "在伸缩性训练中优化超参数：计算效率训练的超参数优化",
    "translated_abstract": "深度学习模型的超参数调整可以使相同的计算量获得数量级的性能提升。尽管如此，系统调整还不普遍，尤其是对于大型模型更是如此，这些模型评估昂贵，超参数较多，需要进行难以把握的折中、预算和搜索边界决策。为了解决这些问题并提出一种实用的方法来稳健地调整大型模型，我们提出了成本感知 Pareto 区域贝叶斯搜索（CARBS），这是一种贝叶斯优化算法，它在性能-计算 Pareto 前沿附近执行局部搜索。CARBS 在具有许多超参数的无界搜索空间中表现良好，学习缩放关系，因此即使在模型缩放的同时也可以调整模型，并自动化了许多“黑魔法”调整。在我们的结果中，我们通过调整简单的基线（ProcGen 论文中提供的 PPO 方法）有效地解决了整个 ProcGen 基准测试。我们还在使用更少的评估时复制了 Bertinetto 等人的模型选择结果。我们的方法通常适用，但特别适合计算受限的情况，其中设计师可以轻松评估或限制培训成本。",
    "tldr": "该论文提出了一种名为“CARBS”的算法，它利用贝叶斯优化算法在性能-计算 Pareto 前沿附近执行局部搜索来解决大型深度学习模型超参数调整的问题。该方法适用于具有许多超参数的无界搜索空间，学习缩放关系，并自动化了许多调整中的“黑魔法”。此外，该方法对计算受限制的情况尤其有效。",
    "en_tdlr": "This paper proposes a Bayesian optimization algorithm called \"CARBS\", which performs local search around the performance-cost Pareto frontier to tackle the issue of hyperparameter tuning for large deep learning models. It is applicable to unbounded search spaces with many hyperparameters, learns scaling relationships, and automates many aspects of the tuning process. Additionally, this method is particularly effective for compute-limited settings."
}