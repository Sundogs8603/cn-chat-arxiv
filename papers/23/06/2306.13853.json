{
    "title": "A Unified Approach to Controlling Implicit Regularization via Mirror Descent. (arXiv:2306.13853v1 [cs.LG])",
    "abstract": "Inspired by the remarkable success of deep neural networks, there has been significant interest in understanding the generalization performance of overparameterized models. Substantial efforts have been invested in characterizing how optimization algorithms impact generalization through their \"preferred\" solutions, a phenomenon commonly referred to as implicit regularization. In particular, it has been argued that gradient descent (GD) induces an implicit $\\ell_2$-norm regularization in regression and classification problems. However, the implicit regularization of different algorithms are confined to either a specific geometry or a particular class of learning problems, indicating a gap in a general approach for controlling the implicit regularization. To address this, we present a unified approach using mirror descent (MD), a notable generalization of GD, to control implicit regularization in both regression and classification settings. More specifically, we show that MD with the gen",
    "link": "http://arxiv.org/abs/2306.13853",
    "context": "Title: A Unified Approach to Controlling Implicit Regularization via Mirror Descent. (arXiv:2306.13853v1 [cs.LG])\nAbstract: Inspired by the remarkable success of deep neural networks, there has been significant interest in understanding the generalization performance of overparameterized models. Substantial efforts have been invested in characterizing how optimization algorithms impact generalization through their \"preferred\" solutions, a phenomenon commonly referred to as implicit regularization. In particular, it has been argued that gradient descent (GD) induces an implicit $\\ell_2$-norm regularization in regression and classification problems. However, the implicit regularization of different algorithms are confined to either a specific geometry or a particular class of learning problems, indicating a gap in a general approach for controlling the implicit regularization. To address this, we present a unified approach using mirror descent (MD), a notable generalization of GD, to control implicit regularization in both regression and classification settings. More specifically, we show that MD with the gen",
    "path": "papers/23/06/2306.13853.json",
    "total_tokens": 953,
    "translated_title": "一种通过镜面下降控制隐式正则化的统一方法",
    "translated_abstract": "受深度神经网络的显著成功启发，人们对超参数模型的泛化性能产生了极大的兴趣。人们花费了大量精力来确定优化算法通过其“首选”解如何影响泛化，这种现象通常被称为隐式正则化。特别地，已经有人论证梯度下降（GD）在回归和分类问题中会引起隐式的$\\ell_2$ -范数正则化。然而，不同算法的隐式正则化受限于特定的几何或特定类的学习问题，表明需要一个通用的方法来控制隐式正则化。为此，我们提出了一个统一的方法，使用镜面下降（MD）来控制回归和分类设置中的隐式正则化。具体而言，我们表明，MD与通用的下降方向一起使用时，在所有标准几何下都可以实现$\\ell_p$（$p\\in[1,\\infty]$）范式的隐式正则化，并且在学习理论中使用的许多特殊类中也可以实现控制的泛化保证。",
    "tldr": "本文提出了一种使用镜面下降方法来统一控制回归和分类问题中的隐式正则化的方法，在所有标准几何下都可以实现$\\ell_p$（$p\\in[1,\\infty]$）范式的隐式正则化，并且可以实现学习理论中许多特殊类的控制的泛化保证。",
    "en_tdlr": "This paper proposes a unified approach using mirror descent to control implicit regularization in both regression and classification settings, achieving implicit $\\ell_p$ ($p\\in[1,\\infty]$) norm regularization in all standard geometries and controlled generalization guarantees for many special classes in learning theory."
}