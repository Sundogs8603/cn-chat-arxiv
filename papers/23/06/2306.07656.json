{
    "title": "Is Anisotropy Inherent to Transformers?. (arXiv:2306.07656v1 [cs.CL])",
    "abstract": "The representation degeneration problem is a phenomenon that is widely observed among self-supervised learning methods based on Transformers. In NLP, it takes the form of anisotropy, a singular property of hidden representations which makes them unexpectedly close to each other in terms of angular distance (cosine-similarity). Some recent works tend to show that anisotropy is a consequence of optimizing the cross-entropy loss on long-tailed distributions of tokens. We show in this paper that anisotropy can also be observed empirically in language models with specific objectives that should not suffer directly from the same consequences. We also show that the anisotropy problem extends to Transformers trained on other modalities. Our observations tend to demonstrate that anisotropy might actually be inherent to Transformers-based models.",
    "link": "http://arxiv.org/abs/2306.07656",
    "context": "Title: Is Anisotropy Inherent to Transformers?. (arXiv:2306.07656v1 [cs.CL])\nAbstract: The representation degeneration problem is a phenomenon that is widely observed among self-supervised learning methods based on Transformers. In NLP, it takes the form of anisotropy, a singular property of hidden representations which makes them unexpectedly close to each other in terms of angular distance (cosine-similarity). Some recent works tend to show that anisotropy is a consequence of optimizing the cross-entropy loss on long-tailed distributions of tokens. We show in this paper that anisotropy can also be observed empirically in language models with specific objectives that should not suffer directly from the same consequences. We also show that the anisotropy problem extends to Transformers trained on other modalities. Our observations tend to demonstrate that anisotropy might actually be inherent to Transformers-based models.",
    "path": "papers/23/06/2306.07656.json",
    "total_tokens": 767,
    "translated_title": "Transformers的各向异性是否固有？",
    "translated_abstract": "表征退化问题是基于Transformers的自监督学习方法中普遍观察到的现象。在自然语言处理中，它采用各向异性的形式，这是一种隐藏表示的奇异属性，使它们在角度距离（余弦相似性）方面意外地靠近彼此。一些最近的工作表明，各向异性是优化长尾分布令牌的交叉熵损失的结果。本文证明，即使对于不应直接受到相同后果的具有特定目标的语言模型，各向异性也可以在其上观察到。我们还证明了各向异性问题也会延伸到训练其他模态的Transformers 。我们的观察结果表明，各向异性实际上可能是Transformers-based模型固有的。",
    "tldr": "本文证明各向异性不仅是优化长尾分布令牌的交叉熵损失结果，还可能是Transformers-based模型固有的。",
    "en_tdlr": "This paper proves that anisotropy may not only be a consequence of optimizing the cross-entropy loss on long-tailed distributions of tokens, but also inherent to Transformers-based models."
}