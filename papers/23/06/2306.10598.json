{
    "title": "DropCompute: simple and more robust distributed synchronous training via compute variance reduction. (arXiv:2306.10598v2 [cs.LG] UPDATED)",
    "abstract": "Background: Distributed training is essential for large scale training of deep neural networks (DNNs). The dominant methods for large scale DNN training are synchronous (e.g. All-Reduce), but these require waiting for all workers in each step. Thus, these methods are limited by the delays caused by straggling workers. Results: We study a typical scenario in which workers are straggling due to variability in compute time. We find an analytical relation between compute time properties and scalability limitations, caused by such straggling workers. With these findings, we propose a simple yet effective decentralized method to reduce the variation among workers and thus improve the robustness of synchronous training. This method can be integrated with the widely used All-Reduce. Our findings are validated on large-scale training tasks using 200 Gaudi Accelerators.",
    "link": "http://arxiv.org/abs/2306.10598",
    "context": "Title: DropCompute: simple and more robust distributed synchronous training via compute variance reduction. (arXiv:2306.10598v2 [cs.LG] UPDATED)\nAbstract: Background: Distributed training is essential for large scale training of deep neural networks (DNNs). The dominant methods for large scale DNN training are synchronous (e.g. All-Reduce), but these require waiting for all workers in each step. Thus, these methods are limited by the delays caused by straggling workers. Results: We study a typical scenario in which workers are straggling due to variability in compute time. We find an analytical relation between compute time properties and scalability limitations, caused by such straggling workers. With these findings, we propose a simple yet effective decentralized method to reduce the variation among workers and thus improve the robustness of synchronous training. This method can be integrated with the widely used All-Reduce. Our findings are validated on large-scale training tasks using 200 Gaudi Accelerators.",
    "path": "papers/23/06/2306.10598.json",
    "total_tokens": 808,
    "translated_title": "DropCompute: 通过计算方差减少分布式同步训练的简易且更稳定的方法",
    "translated_abstract": "背景：分布式训练对于大规模深度神经网络（DNN）的训练至关重要。目前大规模DNN训练的主流方法是同步训练（例如All-Reduce），但这些方法需要等待每一步中所有的工作节点完成。因此，这些方法受制于由延迟导致的工作节点拖延的问题。结果：本文研究了由于计算时间的变异性导致工作节点拖延的典型场景，并找到了计算时间属性和可扩展性限制之间的分析关系。基于这些发现，我们提出了一种简单而有效的分布式方法，以减少工作节点间的变异性，从而提高同步训练的稳定性。该方法可以与广泛使用的All-Reduce相结合。我们通过使用200个Gaudi加速器验证了我们的发现。",
    "tldr": "该论文提出了一种称为DropCompute的简单且稳定的分布式同步训练方法，通过减少工作节点间的计算变异性来提高训练的稳定性。",
    "en_tdlr": "This paper introduces a simple and robust method for distributed synchronous training called DropCompute, which improves training stability by reducing compute variance among workers."
}