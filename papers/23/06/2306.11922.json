{
    "title": "No Wrong Turns: The Simple Geometry Of Neural Networks Optimization Paths. (arXiv:2306.11922v1 [cs.LG])",
    "abstract": "Understanding the optimization dynamics of neural networks is necessary for closing the gap between theory and practice. Stochastic first-order optimization algorithms are known to efficiently locate favorable minima in deep neural networks. This efficiency, however, contrasts with the non-convex and seemingly complex structure of neural loss landscapes. In this study, we delve into the fundamental geometric properties of sampled gradients along optimization paths. We focus on two key quantities, which appear in the restricted secant inequality and error bound. Both hold high significance for first-order optimization. Our analysis reveals that these quantities exhibit predictable, consistent behavior throughout training, despite the stochasticity induced by sampling minibatches. Our findings suggest that not only do optimization trajectories never encounter significant obstacles, but they also maintain stable dynamics during the majority of training. These observed properties are suffi",
    "link": "http://arxiv.org/abs/2306.11922",
    "context": "Title: No Wrong Turns: The Simple Geometry Of Neural Networks Optimization Paths. (arXiv:2306.11922v1 [cs.LG])\nAbstract: Understanding the optimization dynamics of neural networks is necessary for closing the gap between theory and practice. Stochastic first-order optimization algorithms are known to efficiently locate favorable minima in deep neural networks. This efficiency, however, contrasts with the non-convex and seemingly complex structure of neural loss landscapes. In this study, we delve into the fundamental geometric properties of sampled gradients along optimization paths. We focus on two key quantities, which appear in the restricted secant inequality and error bound. Both hold high significance for first-order optimization. Our analysis reveals that these quantities exhibit predictable, consistent behavior throughout training, despite the stochasticity induced by sampling minibatches. Our findings suggest that not only do optimization trajectories never encounter significant obstacles, but they also maintain stable dynamics during the majority of training. These observed properties are suffi",
    "path": "papers/23/06/2306.11922.json",
    "total_tokens": 939,
    "translated_title": "没有错误的转弯：神经网络优化路径的简单几何学",
    "translated_abstract": "了解神经网络的优化动态对于弥合理论与实践之间的差距是必要的。随机一阶优化算法已被证明在深度神经网络中能够有效地定位有利的极小值。然而，这种效率与神经网络损失景观的非凸和看似复杂的结构形成了对比。在本研究中，我们深入研究了样本梯度沿优化路径的基本几何性质。我们专注于两个关键量，这些量出现在受限割线不等式和误差界限中。这两个量对于一阶优化具有高度重要性。我们的分析揭示了这些量在训练期间表现出可预测、一致的行为，尽管采样小批量引起的随机性。我们的发现表明，优化轨迹不仅从未遇到显著的障碍，而且在大多数训练期间能够保持稳定的动力学。这些观察到的特性已经足够",
    "tldr": "本文研究了样本梯度沿优化路径的基本几何性质，发现这些量在训练期间表现出可预测、一致的行为。我们的发现表明，优化轨迹不仅从未遇到显著的障碍，而且在大多数训练期间能够保持稳定的动力学。",
    "en_tdlr": "This paper studies the fundamental geometric properties of sampled gradients along optimization paths in neural networks and reveals their predictable and consistent behavior throughout training. The findings suggest that optimization trajectories never encounter significant obstacles and maintain stable dynamics during the majority of training."
}