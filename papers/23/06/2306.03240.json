{
    "title": "Improving Accelerated Federated Learning with Compression and Importance Sampling. (arXiv:2306.03240v1 [cs.LG])",
    "abstract": "Federated Learning is a collaborative training framework that leverages heterogeneous data distributed across a vast number of clients. Since it is practically infeasible to request and process all clients during the aggregation step, partial participation must be supported. In this setting, the communication between the server and clients poses a major bottleneck. To reduce communication loads, there are two main approaches: compression and local steps. Recent work by Mishchenko et al. [2022] introduced the new ProxSkip method, which achieves an accelerated rate using the local steps technique. Follow-up works successfully combined local steps acceleration with partial participation [Grudzie\\'n et al., 2023, Condat et al. 2023] and gradient compression [Condat et al. [2022]. In this paper, we finally present a complete method for Federated Learning that incorporates all necessary ingredients: Local Training, Compression, and Partial Participation. We obtain state-of-the-art convergenc",
    "link": "http://arxiv.org/abs/2306.03240",
    "context": "Title: Improving Accelerated Federated Learning with Compression and Importance Sampling. (arXiv:2306.03240v1 [cs.LG])\nAbstract: Federated Learning is a collaborative training framework that leverages heterogeneous data distributed across a vast number of clients. Since it is practically infeasible to request and process all clients during the aggregation step, partial participation must be supported. In this setting, the communication between the server and clients poses a major bottleneck. To reduce communication loads, there are two main approaches: compression and local steps. Recent work by Mishchenko et al. [2022] introduced the new ProxSkip method, which achieves an accelerated rate using the local steps technique. Follow-up works successfully combined local steps acceleration with partial participation [Grudzie\\'n et al., 2023, Condat et al. 2023] and gradient compression [Condat et al. [2022]. In this paper, we finally present a complete method for Federated Learning that incorporates all necessary ingredients: Local Training, Compression, and Partial Participation. We obtain state-of-the-art convergenc",
    "path": "papers/23/06/2306.03240.json",
    "total_tokens": 856,
    "translated_title": "利用压缩和重要性抽样提高加速的联邦学习",
    "translated_abstract": "联邦学习是一种利用分布在大量客户端上的异构数据的协作训练框架。在聚合步骤中，要求并处理所有客户端是不切实际的，必须支持部分参与。在这种情况下，服务器与客户端之间的通信是一个主要的瓶颈。为了减少通信负担，有两种主要方法：压缩和本地步骤。最近的研究通过使用本地步骤技术引入了新的ProxSkip方法，实现了加速率。后续的研究成功地将本地步骤加速与部分参与和梯度压缩相结合。本文最终提出了一个完整的联邦学习方法，包括所有必要的成分：本地训练、压缩和部分参与。我们获得了最先进的收敛结果。",
    "tldr": "本文提出了利用压缩和重要性抽样提高加速的联邦学习方法，将本地训练、压缩和部分参与结合应用，以获得最先进的收敛结果。",
    "en_tdlr": "This paper proposes a method for improving accelerated federated learning with compression and importance sampling, which incorporates local training, compression, and partial participation to achieve state-of-the-art convergence results."
}