{
    "title": "Language models are not naysayers: An analysis of language models on negation benchmarks. (arXiv:2306.08189v1 [cs.CL])",
    "abstract": "Negation has been shown to be a major bottleneck for masked language models, such as BERT. However, whether this finding still holds for larger-sized auto-regressive language models (``LLMs'') has not been studied comprehensively. With the ever-increasing volume of research and applications of LLMs, we take a step back to evaluate the ability of current-generation LLMs to handle negation, a fundamental linguistic phenomenon that is central to language understanding. We evaluate different LLMs -- including the open-source GPT-neo, GPT-3, and InstructGPT -- against a wide range of negation benchmarks. Through systematic experimentation with varying model sizes and prompts, we show that LLMs have several limitations including insensitivity to the presence of negation, an inability to capture the lexical semantics of negation, and a failure to reason under negation.",
    "link": "http://arxiv.org/abs/2306.08189",
    "context": "Title: Language models are not naysayers: An analysis of language models on negation benchmarks. (arXiv:2306.08189v1 [cs.CL])\nAbstract: Negation has been shown to be a major bottleneck for masked language models, such as BERT. However, whether this finding still holds for larger-sized auto-regressive language models (``LLMs'') has not been studied comprehensively. With the ever-increasing volume of research and applications of LLMs, we take a step back to evaluate the ability of current-generation LLMs to handle negation, a fundamental linguistic phenomenon that is central to language understanding. We evaluate different LLMs -- including the open-source GPT-neo, GPT-3, and InstructGPT -- against a wide range of negation benchmarks. Through systematic experimentation with varying model sizes and prompts, we show that LLMs have several limitations including insensitivity to the presence of negation, an inability to capture the lexical semantics of negation, and a failure to reason under negation.",
    "path": "papers/23/06/2306.08189.json",
    "total_tokens": 841,
    "translated_title": "语言模型不是否定者：对否定基准测试中语言模型的分析。",
    "translated_abstract": "否定已被证明是被掩蔽语言模型（如BERT）的主要瓶颈。然而，目前并没有全面研究大型自回归语言模型（“LLMs”）是否仍然存在这一发现。随着研究和应用LLMs体积的日益增长，我们退后一步，评估当前一代LLMs处理否定的能力，而这是对于语言理解至关重要的基本语言现象。我们评估不同的LLMs-包括开源的GPT-NEO、GPT-3和InstructGPT-对一系列否定基准测试进行评估。通过对模型大小和提示进行系统实验，我们发现LLMs存在几个限制，包括对否定存在的不敏感性、无法捕捉否定的词汇语义以及无法在否定情况下进行推理。",
    "tldr": "本研究系统评估了当前一代自回归语言模型（LLMs）处理否定的能力，发现LLMs存在无法捕捉否定词汇语义、不能推理的问题等限制。",
    "en_tdlr": "This study comprehensively evaluates the ability of current-generation auto-regressive language models (LLMs) to handle negation, and shows that LLMs have several limitations including inability to capture the lexical semantics of negation and inability to reason under negation."
}