{
    "title": "CFL: Causally Fair Language Models Through Token-level Attribute Controlled Generation. (arXiv:2306.00374v1 [cs.CL])",
    "abstract": "We propose a method to control the attributes of Language Models (LMs) for the text generation task using Causal Average Treatment Effect (ATE) scores and counterfactual augmentation. We explore this method, in the context of LM detoxification, and propose the Causally Fair Language (CFL) architecture for detoxifying pre-trained LMs in a plug-and-play manner. Our architecture is based on a Structural Causal Model (SCM) that is mathematically transparent and computationally efficient as compared with many existing detoxification techniques. We also propose several new metrics that aim to better understand the behaviour of LMs in the context of toxic text generation. Further, we achieve state of the art performance for toxic degeneration, which are computed using \\RTP (RTP) benchmark. Our experiments show that CFL achieves such a detoxification without much impact on the model perplexity. We also show that CFL mitigates the unintended bias problem through experiments on the BOLD dataset.",
    "link": "http://arxiv.org/abs/2306.00374",
    "context": "Title: CFL: Causally Fair Language Models Through Token-level Attribute Controlled Generation. (arXiv:2306.00374v1 [cs.CL])\nAbstract: We propose a method to control the attributes of Language Models (LMs) for the text generation task using Causal Average Treatment Effect (ATE) scores and counterfactual augmentation. We explore this method, in the context of LM detoxification, and propose the Causally Fair Language (CFL) architecture for detoxifying pre-trained LMs in a plug-and-play manner. Our architecture is based on a Structural Causal Model (SCM) that is mathematically transparent and computationally efficient as compared with many existing detoxification techniques. We also propose several new metrics that aim to better understand the behaviour of LMs in the context of toxic text generation. Further, we achieve state of the art performance for toxic degeneration, which are computed using \\RTP (RTP) benchmark. Our experiments show that CFL achieves such a detoxification without much impact on the model perplexity. We also show that CFL mitigates the unintended bias problem through experiments on the BOLD dataset.",
    "path": "papers/23/06/2306.00374.json",
    "total_tokens": 969,
    "translated_title": "CFL：通过基于Token级属性控制的生成实现因果公平的语言模型",
    "translated_abstract": "我们提出了一种方法来控制语言模型（LMs）的属性，用于文本生成任务，该方法使用因果平均处理效应（ATE）分数和反事实增强。我们探索了这种方法，在LM解毒的背景下提出了CFL（causally fair language）体系结构，以插入式方式对预训练的LM进行解毒。我们的体系结构基于结构因果模型（SCM），数学透明且与许多现有解毒技术相比计算效率高。我们还提出了几个新的度量标准，旨在更好地理解LM在有害文本生成环境中的行为。此外，我们在\\RTP(RTP)基准测试中实现了最先进的毒性退化性能。我们的实验表明，CFL实现了这种解毒，对模型困惑没有太大影响。我们还通过对BOLD数据集的实验展示了CFL的减轻意外偏置问题的能力。",
    "tldr": "本文提出了一种使用因果平均处理效应（ATE）分数和反事实增强方法的CFL体系架构，通过插入式方式对预训练的LM进行解毒，实现了因果公平的语言模型。此方法不会对模型困惑产生太大影响，还减轻了意外的偏置问题。",
    "en_tdlr": "The paper proposes a causally fair language (CFL) architecture for detoxifying pre-trained language models (LMs) in a plug-and-play manner, using causal average treatment effect (ATE) scores and counterfactual augmentation. It achieves state-of-the-art performance for toxic degeneration without much impact on the model perplexity. CFL mitigates the unintended bias problem and provides a better understanding of LM behavior in the context of toxic text generation."
}