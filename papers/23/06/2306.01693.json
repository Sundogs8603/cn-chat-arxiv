{
    "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training. (arXiv:2306.01693v1 [cs.CL])",
    "abstract": "Language models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs. Reinforcement learning from human feedback (RLHF) - where human preference judgments on LM outputs are transformed into a learning signal - has recently shown promise in addressing these issues. However, such holistic feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced user preference; e.g., which parts contain what type(s) of errors. In this paper, we use fine-grained human feedback (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal. We introduce Fine-Grained RLHF, a framework that enables training and learning from reward functions that are fine-grained in two respects: (1) density, providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating multiple reward models associated with different feedback types (e.",
    "link": "http://arxiv.org/abs/2306.01693",
    "context": "Title: Fine-Grained Human Feedback Gives Better Rewards for Language Model Training. (arXiv:2306.01693v1 [cs.CL])\nAbstract: Language models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs. Reinforcement learning from human feedback (RLHF) - where human preference judgments on LM outputs are transformed into a learning signal - has recently shown promise in addressing these issues. However, such holistic feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced user preference; e.g., which parts contain what type(s) of errors. In this paper, we use fine-grained human feedback (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal. We introduce Fine-Grained RLHF, a framework that enables training and learning from reward functions that are fine-grained in two respects: (1) density, providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating multiple reward models associated with different feedback types (e.",
    "path": "papers/23/06/2306.01693.json",
    "total_tokens": 946,
    "translated_title": "精细化的人类反馈可以提供更好的语言模型训练奖励",
    "translated_abstract": "语言模型经常表现出不良的文本生成行为，包括生成虚假、有害或无关的输出。最近，从人类反馈中进行强化学习（RLHF）-其中人类对LM输出的偏好评价被转化为学习信号-已经显示出解决这些问题的潜力。然而，这种整体反馈对长文本输出传达的信息有限；它不表明输出的哪些方面影响了用户的偏好；例如，哪些部分包含什么类型的错误。本文中，我们使用精细化的人类反馈（例如，哪个句子是错误的，哪个子句是无关的）作为明确的训练信号。我们介绍了Fine-Grained RLHF，这是一个能够训练和学习与不同反馈类型相关的多个奖励模型的精细化奖励功能的框架，具有以下两个特征：（1）密度，以在生成每个段落（例如一个句子）后提供奖励； （2）并入不同反馈类型的多个奖励模型。",
    "tldr": "本文提出了Fine-Grained RLHF框架，使用精细化的人类反馈作为明确的训练信号来训练和学习语言模型。该框架提供了多个细致的奖励模型来获得更好的效果。",
    "en_tdlr": "The paper proposes the Fine-Grained RLHF framework that uses fine-grained human feedback as explicit training signals to train and learn language models. This framework provides multiple detailed reward models to obtain better results."
}