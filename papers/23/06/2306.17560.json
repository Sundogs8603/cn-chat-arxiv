{
    "title": "Class-Incremental Learning using Diffusion Model for Distillation and Replay. (arXiv:2306.17560v1 [cs.LG])",
    "abstract": "Class-incremental learning aims to learn new classes in an incremental fashion without forgetting the previously learned ones. Several research works have shown how additional data can be used by incremental models to help mitigate catastrophic forgetting. In this work, following the recent breakthrough in text-to-image generative models and their wide distribution, we propose the use of a pretrained Stable Diffusion model as a source of additional data for class-incremental learning. Compared to competitive methods that rely on external, often unlabeled, datasets of real images, our approach can generate synthetic samples belonging to the same classes as the previously encountered images. This allows us to use those additional data samples not only in the distillation loss but also for replay in the classification loss. Experiments on the competitive benchmarks CIFAR100, ImageNet-Subset, and ImageNet demonstrate how this new approach can be used to further improve the performance of s",
    "link": "http://arxiv.org/abs/2306.17560",
    "context": "Title: Class-Incremental Learning using Diffusion Model for Distillation and Replay. (arXiv:2306.17560v1 [cs.LG])\nAbstract: Class-incremental learning aims to learn new classes in an incremental fashion without forgetting the previously learned ones. Several research works have shown how additional data can be used by incremental models to help mitigate catastrophic forgetting. In this work, following the recent breakthrough in text-to-image generative models and their wide distribution, we propose the use of a pretrained Stable Diffusion model as a source of additional data for class-incremental learning. Compared to competitive methods that rely on external, often unlabeled, datasets of real images, our approach can generate synthetic samples belonging to the same classes as the previously encountered images. This allows us to use those additional data samples not only in the distillation loss but also for replay in the classification loss. Experiments on the competitive benchmarks CIFAR100, ImageNet-Subset, and ImageNet demonstrate how this new approach can be used to further improve the performance of s",
    "path": "papers/23/06/2306.17560.json",
    "total_tokens": 870,
    "translated_title": "使用扩散模型进行蒸馏和重播的增量学习",
    "translated_abstract": "增量学习旨在以增量的方式学习新类别，而不会忘记先前学习的类别。多个研究表明，增量模型可以利用附加数据来帮助减轻灾难性遗忘。本文提出了使用预训练的稳定扩散模型作为增量学习的附加数据源。与依赖于外部、通常是无标签的真实图像数据集的竞争方法相比，我们的方法可以生成属于先前遇到的图像所属类别的合成样本。这使我们不仅可以在蒸馏损失中使用这些附加数据样本，还可以在分类损失中进行重播。在CIFAR100、ImageNet-Subset和ImageNet等竞争基准上的实验表明，这种新方法可以进一步提高模型的性能。",
    "tldr": "本文提出了一种使用预训练的扩散模型作为增量学习的附加数据源的方法，通过生成属于先前遇到的图像所属类别的合成样本，并在蒸馏损失和分类损失中使用这些样本，进一步提高了模型的性能。"
}