{
    "title": "Privacy Preserving Bayesian Federated Learning in Heterogeneous Settings. (arXiv:2306.07959v1 [cs.LG])",
    "abstract": "In several practical applications of federated learning (FL), the clients are highly heterogeneous in terms of both their data and compute resources, and therefore enforcing the same model architecture for each client is very limiting. Moreover, the need for uncertainty quantification and data privacy constraints are often particularly amplified for clients that have limited local data. This paper presents a unified FL framework to simultaneously address all these constraints and concerns, based on training customized local Bayesian models that learn well even in the absence of large local datasets. A Bayesian framework provides a natural way of incorporating supervision in the form of prior distributions. We use priors in the functional (output) space of the networks to facilitate collaboration across heterogeneous clients. Moreover, formal differential privacy guarantees are provided for this framework. Experiments on standard FL datasets demonstrate that our approach outperforms str",
    "link": "http://arxiv.org/abs/2306.07959",
    "context": "Title: Privacy Preserving Bayesian Federated Learning in Heterogeneous Settings. (arXiv:2306.07959v1 [cs.LG])\nAbstract: In several practical applications of federated learning (FL), the clients are highly heterogeneous in terms of both their data and compute resources, and therefore enforcing the same model architecture for each client is very limiting. Moreover, the need for uncertainty quantification and data privacy constraints are often particularly amplified for clients that have limited local data. This paper presents a unified FL framework to simultaneously address all these constraints and concerns, based on training customized local Bayesian models that learn well even in the absence of large local datasets. A Bayesian framework provides a natural way of incorporating supervision in the form of prior distributions. We use priors in the functional (output) space of the networks to facilitate collaboration across heterogeneous clients. Moreover, formal differential privacy guarantees are provided for this framework. Experiments on standard FL datasets demonstrate that our approach outperforms str",
    "path": "papers/23/06/2306.07959.json",
    "total_tokens": 878,
    "translated_title": "异构设置下的隐私保护贝叶斯联邦学习",
    "translated_abstract": "在许多联邦学习（FL）的实际应用中，客户端的数据和计算资源高度异构，因此强制为每个客户端施加相同的模型架构非常受限。此外，对于拥有受限本地数据的客户端，需要不确定性量化和数据隐私约束的需求通常特别增强。本文提出了一个统一的FL框架，以同时解决所有这些约束和关注点，基于训练定制本地贝叶斯模型，即使在没有大量本地数据的情况下也能很好地学习。贝叶斯框架提供了一种自然的方式来以先验分布的形式融入监督。我们在网络的功能（输出）空间中使用先验来促进异构客户端之间的合作。此外，本框架还提供了形式化的差分隐私保证。在标准FL数据集上的实验表明，我们的方法优于策略。",
    "tldr": "本文提出了一种基于贝叶斯框架的联邦学习方法，可以训练定制本地模型以同时满足异构设置、不确定性量化和数据隐私约束，并通过先验分布来促进异构客户端之间的合作。"
}