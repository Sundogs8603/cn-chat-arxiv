{
    "title": "Resources and Evaluations for Multi-Distribution Dense Information Retrieval. (arXiv:2306.12601v1 [cs.IR])",
    "abstract": "We introduce and define the novel problem of multi-distribution information retrieval (IR) where given a query, systems need to retrieve passages from within multiple collections, each drawn from a different distribution. Some of these collections and distributions might not be available at training time. To evaluate methods for multi-distribution retrieval, we design three benchmarks for this task from existing single-distribution datasets, namely, a dataset based on question answering and two based on entity matching. We propose simple methods for this task which allocate the fixed retrieval budget (top-k passages) strategically across domains to prevent the known domains from consuming most of the budget. We show that our methods lead to an average of 3.8+ and up to 8.0 points improvements in Recall@100 across the datasets and that improvements are consistent when fine-tuning different base retrieval models. Our benchmarks are made publicly available.",
    "link": "http://arxiv.org/abs/2306.12601",
    "context": "Title: Resources and Evaluations for Multi-Distribution Dense Information Retrieval. (arXiv:2306.12601v1 [cs.IR])\nAbstract: We introduce and define the novel problem of multi-distribution information retrieval (IR) where given a query, systems need to retrieve passages from within multiple collections, each drawn from a different distribution. Some of these collections and distributions might not be available at training time. To evaluate methods for multi-distribution retrieval, we design three benchmarks for this task from existing single-distribution datasets, namely, a dataset based on question answering and two based on entity matching. We propose simple methods for this task which allocate the fixed retrieval budget (top-k passages) strategically across domains to prevent the known domains from consuming most of the budget. We show that our methods lead to an average of 3.8+ and up to 8.0 points improvements in Recall@100 across the datasets and that improvements are consistent when fine-tuning different base retrieval models. Our benchmarks are made publicly available.",
    "path": "papers/23/06/2306.12601.json",
    "total_tokens": 869,
    "translated_title": "多分布稠密信息检索的资源和评估",
    "translated_abstract": "我们引入并定义了多分布信息检索（IR）的新问题，即在给定查询的情况下，系统需要从多个集合中检索出段落，每个集合都来自不同的分布。其中一些集合和分布可能在训练时不可用。为了评估多分布检索的方法，我们从现有的单分布数据集设计了三个基准，分别是基于问题回答的数据集和两个基于实体匹配的数据集。我们提出了针对此任务的简单方法，该方法在域之间战略性地分配固定的检索预算（前k个段落），以防已知领域消耗大部分预算。我们展示我们的方法在数据集上导致了平均3.8+和高达8.0个Recall @ 100点的提高，并且在微调不同的基础检索模型时改进是一致的。我们的基准公开可用。",
    "tldr": "本文提出了一个新问题——多分布信息检索，并通过三个基准数据集展示了简单方法的有效性，可防止已知领域消耗大部分检索预算，平均提高Recall @ 100点3.8+，最高可达8.0个点。",
    "en_tdlr": "This article introduces the novel problem of multi-distribution information retrieval and demonstrates the effectiveness of simple methods through three benchmark datasets, which can prevent known domains from consuming most of the retrieval budget, resulting in an average improvement of 3.8+ and up to 8.0 points in Recall@100."
}