{
    "title": "Boosting Language Models Reasoning with Chain-of-Knowledge Prompting. (arXiv:2306.06427v2 [cs.CL] UPDATED)",
    "abstract": "Recently, Chain-of-Thought (CoT) prompting has delivered success on complex reasoning tasks, which aims at designing a simple prompt like ``Let's think step by step'' or multiple in-context exemplars with well-designed rationales to elicit Large Language Models (LLMs) to generate intermediate reasoning steps. However, the generated rationales often come with mistakes, making unfactual and unfaithful reasoning chains. To mitigate this brittleness, we propose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple. This is inspired by our human behaviors, i.e., we can draw a mind map or knowledge map as the reasoning evidence in the brain before answering a complex question. Benefiting from CoK, we additionally introduce a F^2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness. For the unreliable response, the wrong evidence can",
    "link": "http://arxiv.org/abs/2306.06427",
    "context": "Title: Boosting Language Models Reasoning with Chain-of-Knowledge Prompting. (arXiv:2306.06427v2 [cs.CL] UPDATED)\nAbstract: Recently, Chain-of-Thought (CoT) prompting has delivered success on complex reasoning tasks, which aims at designing a simple prompt like ``Let's think step by step'' or multiple in-context exemplars with well-designed rationales to elicit Large Language Models (LLMs) to generate intermediate reasoning steps. However, the generated rationales often come with mistakes, making unfactual and unfaithful reasoning chains. To mitigate this brittleness, we propose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple. This is inspired by our human behaviors, i.e., we can draw a mind map or knowledge map as the reasoning evidence in the brain before answering a complex question. Benefiting from CoK, we additionally introduce a F^2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness. For the unreliable response, the wrong evidence can",
    "path": "papers/23/06/2306.06427.json",
    "total_tokens": 913,
    "translated_title": "使用知识链推动提升语言模型的推理能力",
    "translated_abstract": "最近，链式思维（CoT）提示在复杂的推理任务上取得了成功，其目标是设计一个简单的提示，如“我们一步一步地思考”或多个上下文示例，以及设计良好的理由，以引导大型语言模型（LLM）生成中间推理步骤。然而，生成的理由往往带有错误，导致不准确和不可信的推理链。为了减少这种脆弱性，我们提出了一种新颖的知识链提示（CoK），旨在引导LLM生成显性的知识证据，以结构化三元组的形式呈现。这一灵感来自于人类行为，即在回答复杂问题之前，我们可以在脑海中绘制思维导图或知识图作为推理证据。通过使用CoK，我们额外引入了一种F^2-Verification方法来估计推理链的可靠性，包括准确性和可信度。对于不可靠的回答，错误的证据可以",
    "tldr": "该论文提出了一种新的知识链提示（CoK）方法，旨在引导语言模型生成明确的知识证据，以提升推理能力，并通过F^2-Verification方法评估推理的准确性和可信度。",
    "en_tdlr": "This paper proposes a novel approach of Chain-of-Knowledge (CoK) prompting to guide language models in generating explicit knowledge evidence, aiming to enhance reasoning abilities. It also introduces an F^2-Verification method to evaluate the accuracy and fidelity of the generated reasoning chains."
}