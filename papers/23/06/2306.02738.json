{
    "title": "A Large-Scale Study of Probabilistic Calibration in Neural Network Regression. (arXiv:2306.02738v2 [cs.LG] UPDATED)",
    "abstract": "Accurate probabilistic predictions are essential for optimal decision making. While neural network miscalibration has been studied primarily in classification, we investigate this in the less-explored domain of regression. We conduct the largest empirical study to date to assess the probabilistic calibration of neural networks. We also analyze the performance of recalibration, conformal, and regularization methods to enhance probabilistic calibration. Additionally, we introduce novel differentiable recalibration and regularization methods, uncovering new insights into their effectiveness. Our findings reveal that regularization methods offer a favorable tradeoff between calibration and sharpness. Post-hoc methods exhibit superior probabilistic calibration, which we attribute to the finite-sample coverage guarantee of conformal prediction. Furthermore, we demonstrate that quantile recalibration can be considered as a specific case of conformal prediction. Our study is fully reproducible",
    "link": "http://arxiv.org/abs/2306.02738",
    "context": "Title: A Large-Scale Study of Probabilistic Calibration in Neural Network Regression. (arXiv:2306.02738v2 [cs.LG] UPDATED)\nAbstract: Accurate probabilistic predictions are essential for optimal decision making. While neural network miscalibration has been studied primarily in classification, we investigate this in the less-explored domain of regression. We conduct the largest empirical study to date to assess the probabilistic calibration of neural networks. We also analyze the performance of recalibration, conformal, and regularization methods to enhance probabilistic calibration. Additionally, we introduce novel differentiable recalibration and regularization methods, uncovering new insights into their effectiveness. Our findings reveal that regularization methods offer a favorable tradeoff between calibration and sharpness. Post-hoc methods exhibit superior probabilistic calibration, which we attribute to the finite-sample coverage guarantee of conformal prediction. Furthermore, we demonstrate that quantile recalibration can be considered as a specific case of conformal prediction. Our study is fully reproducible",
    "path": "papers/23/06/2306.02738.json",
    "total_tokens": 946,
    "translated_title": "神经网络回归中概率校准的大规模研究",
    "translated_abstract": "准确的概率预测对于最优决策非常重要。虽然神经网络的误校准主要研究了分类问题，但我们对于回归问题中较少探讨的误校准进行了研究。我们进行了迄今为止最大的实证研究，评估神经网络的概率校准。我们还分析了重新校准、合规和正则化方法来增强概率校准的性能。此外，我们引入了新颖的可微分重新校准和正则化方法，揭示了它们的有效性的新见解。我们的研究发现，正则化方法在校准和锐度之间提供了有利的平衡。后续方法表现出更优越的概率校准，我们将其归因于合规性预测的有限样本覆盖保证。此外，我们还证明，分位数重新校准可以被认为是合规性预测的一个特定情况。我们的研究是完全可再生的。",
    "tldr": "本研究对于神经网络的概率校准在回归问题中进行了最大的实证研究，发现正则化方法在校准和锐度之间提供了有利的平衡，而后续方法表现更优越，此外还证明了分位数重新校准可以被认为是合规性预测的一个特定情况。",
    "en_tdlr": "This study conducted the largest empirical research on probabilistic calibration of neural networks in regression, highlighting the effective balance between calibration and sharpness achieved by regularization methods while post-hoc methods showed superior probabilistic calibration. The study also identified quantile recalibration as a particular case of conformal prediction."
}