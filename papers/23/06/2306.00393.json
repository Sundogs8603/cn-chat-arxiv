{
    "title": "Teacher Agent: A Non-Knowledge Distillation Method for Rehearsal-based Video Incremental Learning. (arXiv:2306.00393v1 [cs.CV])",
    "abstract": "With the rise in popularity of video-based social media, new categories of videos are constantly being generated, creating an urgent need for robust incremental learning techniques for video understanding. One of the biggest challenges in this task is catastrophic forgetting, where the network tends to forget previously learned data while learning new categories. To overcome this issue, knowledge distillation is a widely used technique for rehearsal-based video incremental learning that involves transferring important information on similarities among different categories to enhance the student model. Therefore, it is preferable to have a strong teacher model to guide the students. However, the limited performance of the network itself and the occurrence of catastrophic forgetting can result in the teacher network making inaccurate predictions for some memory exemplars, ultimately limiting the student network's performance. Based on these observations, we propose a teacher agent capabl",
    "link": "http://arxiv.org/abs/2306.00393",
    "context": "Title: Teacher Agent: A Non-Knowledge Distillation Method for Rehearsal-based Video Incremental Learning. (arXiv:2306.00393v1 [cs.CV])\nAbstract: With the rise in popularity of video-based social media, new categories of videos are constantly being generated, creating an urgent need for robust incremental learning techniques for video understanding. One of the biggest challenges in this task is catastrophic forgetting, where the network tends to forget previously learned data while learning new categories. To overcome this issue, knowledge distillation is a widely used technique for rehearsal-based video incremental learning that involves transferring important information on similarities among different categories to enhance the student model. Therefore, it is preferable to have a strong teacher model to guide the students. However, the limited performance of the network itself and the occurrence of catastrophic forgetting can result in the teacher network making inaccurate predictions for some memory exemplars, ultimately limiting the student network's performance. Based on these observations, we propose a teacher agent capabl",
    "path": "papers/23/06/2306.00393.json",
    "total_tokens": 1001,
    "translated_title": "Teacher Agent：一种基于重复训练的视频增量学习非知识蒸馏方法",
    "translated_abstract": "随着基于视频的社交媒体的普及，不断有新的视频类别被生成，迫切需要稳健的增量学习技术来理解这些视频。其中最大的挑战之一是灾难性遗忘，在学习新类别的同时，网络往往会忘记先前学习过的数据。为了解决此问题，知识蒸馏是一种广泛使用的技术，它通过将不同类别之间的相似性的重要信息传输到学生模型中来增强其性能。因此，最好有一个强大的教师模型来指导学生。然而，网络本身的有限表现和灾难性遗忘的发生可能导致教师网络对某些记忆样本做出不准确的预测，从而限制了学生网络的性能。基于这些观察，我们提出了一种教师代理，能够从先前学习的知识中生成高质量样本的数据集，从而使学生网络可以从样本的多种观点中进行学习。我们的方法在视频分类任务的标准基准上优于基于知识蒸馏的方法。",
    "tldr": "提出了一种教师代理方法，能够从先前学习的知识中生成高质量样本的数据集，从而使学生网络可以从样本的多种观点中进行学习，在标准基准上优于基于知识蒸馏的方法。",
    "en_tdlr": "A teacher agent method is proposed, which can generate a high-quality dataset of prototypes from previously learned knowledge to enable the student network to learn from the diverse perspectives within the prototypes. The proposed method outperforms knowledge distillation-based approaches on standard benchmarks for video classification tasks."
}