{
    "title": "Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation. (arXiv:2306.05783v1 [cs.CL])",
    "abstract": "New Natural Langauge Process~(NLP) benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present Xiezhi, the most comprehensive evaluation suite designed to assess holistic domain knowledge. Xiezhi comprises multiple-choice questions across 516 diverse disciplines ranging from 13 different subjects with 220,000 questions and accompanied by Xiezhi-Specialty and Xiezhi-Interdiscipline, both with 15k questions. We conduct evaluation of the 47 cutting-edge LLMs on Xiezhi. Results indicate that LLMs exceed average performance of humans in science, engineering, agronomy, medicine, and art, but fall short in economics, jurisprudence, pedagogy, literature, history, and management. We anticipate Xiezhi will help analyze important strengths and shortcomings of LLMs, and the benchmark is released in https://github.com/MikeGu721/XiezhiBenchmark .",
    "link": "http://arxiv.org/abs/2306.05783",
    "context": "Title: Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation. (arXiv:2306.05783v1 [cs.CL])\nAbstract: New Natural Langauge Process~(NLP) benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present Xiezhi, the most comprehensive evaluation suite designed to assess holistic domain knowledge. Xiezhi comprises multiple-choice questions across 516 diverse disciplines ranging from 13 different subjects with 220,000 questions and accompanied by Xiezhi-Specialty and Xiezhi-Interdiscipline, both with 15k questions. We conduct evaluation of the 47 cutting-edge LLMs on Xiezhi. Results indicate that LLMs exceed average performance of humans in science, engineering, agronomy, medicine, and art, but fall short in economics, jurisprudence, pedagogy, literature, history, and management. We anticipate Xiezhi will help analyze important strengths and shortcomings of LLMs, and the benchmark is released in https://github.com/MikeGu721/XiezhiBenchmark .",
    "path": "papers/23/06/2306.05783.json",
    "total_tokens": 926,
    "translated_title": "Xiezhi：一种全面更新的综合领域知识评估基准",
    "translated_abstract": "随着大型语言模型（LLM）的快速发展，急需新的自然语言处理（NLP）基准来实现对齐。我们提出了Xiezhi，这是一个最全面的评估套件，旨在评估综合领域知识。Xiezhi包括跨越13个不同学科的516个多项选择问题，包括22万个问题，并且附带Xiezhi-Specialty和Xiezhi-Interdiscipline，均有15,000个问题。我们对47个先进的LLM在Xiezhi上进行了评估。结果表明，LLM在科学、工程、农学、医学和艺术方面超过了人类的平均表现，但在经济学、法学、教育学、文学、历史和管理方面则表现不佳。我们期望Xiezhi将有助于分析LLM的重要优点和不足之处，该基准已在https://github.com/MikeGu721/XiezhiBenchmark发布。",
    "tldr": "Xiezhi是一种全面综合的评估套件，设有516个多项选择问题，覆盖了从13个不同学科跨越的15个专业领域，并对47个先进的LLMs进行评估，结果表明LLMs在大多数领域超越人类，但在一些领域表现不佳。",
    "en_tdlr": "Xiezhi is a comprehensive evaluation suite with 516 multiple-choice questions covering 15 subject fields across 13 different disciplines and evaluated 47 cutting-edge LLMs. Results show that LLMs outperform humans in most fields but fall short in some areas."
}