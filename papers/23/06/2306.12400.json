{
    "title": "Timely Asynchronous Hierarchical Federated Learning: Age of Convergence. (arXiv:2306.12400v1 [cs.IT])",
    "abstract": "We consider an asynchronous hierarchical federated learning (AHFL) setting with a client-edge-cloud framework. The clients exchange the trained parameters with their corresponding edge servers, which update the locally aggregated model. This model is then transmitted to all the clients in the local cluster. The edge servers communicate to the central cloud server for global model aggregation. The goal of each client is to converge to the global model, while maintaining timeliness of the clients, i.e., having optimum training iteration time. We investigate the convergence criteria for such a system with dense clusters. Our analysis shows that for a system of $n$ clients with fixed average timeliness, the convergence in finite time is probabilistically guaranteed, if the nodes are divided into $O(1)$ number of clusters, that is, if the system is built as a sparse set of edge servers with dense client bases each.",
    "link": "http://arxiv.org/abs/2306.12400",
    "context": "Title: Timely Asynchronous Hierarchical Federated Learning: Age of Convergence. (arXiv:2306.12400v1 [cs.IT])\nAbstract: We consider an asynchronous hierarchical federated learning (AHFL) setting with a client-edge-cloud framework. The clients exchange the trained parameters with their corresponding edge servers, which update the locally aggregated model. This model is then transmitted to all the clients in the local cluster. The edge servers communicate to the central cloud server for global model aggregation. The goal of each client is to converge to the global model, while maintaining timeliness of the clients, i.e., having optimum training iteration time. We investigate the convergence criteria for such a system with dense clusters. Our analysis shows that for a system of $n$ clients with fixed average timeliness, the convergence in finite time is probabilistically guaranteed, if the nodes are divided into $O(1)$ number of clusters, that is, if the system is built as a sparse set of edge servers with dense client bases each.",
    "path": "papers/23/06/2306.12400.json",
    "total_tokens": 890,
    "translated_title": "及时异步分层联邦学习：收敛时代。",
    "translated_abstract": "我们考虑一个客户端-边缘-云架构下的异步分层联邦学习（AHFL）设置。客户端与相应的边缘服务器交换训练参数，然后更新本地聚合的模型。此模型然后传输给本地集群中的所有客户端。边缘服务器与中央云服务器通信以进行全局模型汇集。每个客户端的目标是收敛于全局模型，同时保持客户端的时效性，即具有最佳的训练迭代时间。我们研究了具有密集集群的这种系统的收敛标准。我们的分析表明，如果将节点划分为$O(1)$数量的集群，即将系统构建为每个密集客户端群体的稀疏边缘服务器集，那么对于具有固定平均时效性的$n$个客户的系统，有限时间内的收敛被概率保证。",
    "tldr": "这篇论文讨论了异步分层联邦学习设定下如何在保证客户端时效性的情况下实现全局模型的收敛，研究表明对于系统中节点较少的情况，只需要将节点划分为少量的集群便可有限时间内实现收敛。",
    "en_tdlr": "This paper discusses how to achieve convergence of global model while ensuring timely clients in asynchronous hierarchical federated learning settings. The research shows that for systems with fewer nodes, convergence in finite time is probabilistically guaranteed by dividing the nodes into a small number of clusters."
}