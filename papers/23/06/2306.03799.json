{
    "title": "Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models",
    "abstract": "arXiv:2306.03799v2 Announce Type: replace  Abstract: Prompt engineering is an essential technique for enhancing the abilities of large language models (LLMs) by providing explicit and specific instructions. It enables LLMs to excel in various tasks, such as arithmetic reasoning, question answering, summarization, relation extraction, machine translation, and sentiment analysis. Researchers have been actively exploring different prompt engineering strategies, such as Chain of Thought (CoT), Zero-CoT, and In-context learning. However, an unresolved problem arises from the fact that current approaches lack a solid mathematical solution for determining optimal prompts. To address this issue in prompt engineering, we propose a new and effective approach called Prompt Space. Our methodology utilizes text embeddings to obtain basis vectors by matrix decomposition, and then constructs a space for representing all prompts. Prompt Space significantly outperforms state-of-the-art prompt paradigms",
    "link": "https://arxiv.org/abs/2306.03799",
    "context": "Title: Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models\nAbstract: arXiv:2306.03799v2 Announce Type: replace  Abstract: Prompt engineering is an essential technique for enhancing the abilities of large language models (LLMs) by providing explicit and specific instructions. It enables LLMs to excel in various tasks, such as arithmetic reasoning, question answering, summarization, relation extraction, machine translation, and sentiment analysis. Researchers have been actively exploring different prompt engineering strategies, such as Chain of Thought (CoT), Zero-CoT, and In-context learning. However, an unresolved problem arises from the fact that current approaches lack a solid mathematical solution for determining optimal prompts. To address this issue in prompt engineering, we propose a new and effective approach called Prompt Space. Our methodology utilizes text embeddings to obtain basis vectors by matrix decomposition, and then constructs a space for representing all prompts. Prompt Space significantly outperforms state-of-the-art prompt paradigms",
    "path": "papers/23/06/2306.03799.json",
    "total_tokens": 828,
    "translated_title": "优化大语言模型的少样本推理成功与提示空间",
    "translated_abstract": "提示工程是增强大语言模型（LLMs）能力的必要技术，通过提供明确和具体的指示。它使LLMs在各种任务中表现出色，如算术推理、问答、总结、关系提取、机器翻译和情感分析。研究人员一直在积极探索不同的提示工程策略，如Chain of Thought（CoT）、Zero-CoT和In-context学习。然而，一个未解决的问题源于当前方法缺乏确定最佳提示的坚实数学解决方案。为了解决提示工程中的这一问题，我们提出了一种新的有效方法称为Prompt Space。我们的方法利用文本嵌入通过矩阵分解获得基向量，然后构建一个表示所有提示的空间。Prompt Space在性能上显著优于最先进的提示范例。",
    "tldr": "提示工程是增强大语言模型能力的必要技术，我们提出了Prompt Space方法，通过文本嵌入和矩阵分解构建提示空间来解决当前方法缺乏确定最佳提示的问题，并取得显著优越表现。",
    "en_tdlr": "Prompt engineering is crucial for enhancing the capabilities of large language models, and our proposed Prompt Space approach effectively addresses the lack of a solid mathematical solution for determining optimal prompts, outperforming state-of-the-art paradigms."
}