{
    "title": "Rethinking Closed-loop Training for Autonomous Driving. (arXiv:2306.15713v1 [cs.RO])",
    "abstract": "Recent advances in high-fidelity simulators have enabled closed-loop training of autonomous driving agents, potentially solving the distribution shift in training v.s. deployment and allowing training to be scaled both safely and cheaply. However, there is a lack of understanding of how to build effective training benchmarks for closed-loop training. In this work, we present the first empirical study which analyzes the effects of different training benchmark designs on the success of learning agents, such as how to design traffic scenarios and scale training environments. Furthermore, we show that many popular RL algorithms cannot achieve satisfactory performance in the context of autonomous driving, as they lack long-term planning and take an extremely long time to train. To address these issues, we propose trajectory value learning (TRAVL), an RL-based driving agent that performs planning with multistep look-ahead and exploits cheaply generated imagined data for efficient learning. O",
    "link": "http://arxiv.org/abs/2306.15713",
    "context": "Title: Rethinking Closed-loop Training for Autonomous Driving. (arXiv:2306.15713v1 [cs.RO])\nAbstract: Recent advances in high-fidelity simulators have enabled closed-loop training of autonomous driving agents, potentially solving the distribution shift in training v.s. deployment and allowing training to be scaled both safely and cheaply. However, there is a lack of understanding of how to build effective training benchmarks for closed-loop training. In this work, we present the first empirical study which analyzes the effects of different training benchmark designs on the success of learning agents, such as how to design traffic scenarios and scale training environments. Furthermore, we show that many popular RL algorithms cannot achieve satisfactory performance in the context of autonomous driving, as they lack long-term planning and take an extremely long time to train. To address these issues, we propose trajectory value learning (TRAVL), an RL-based driving agent that performs planning with multistep look-ahead and exploits cheaply generated imagined data for efficient learning. O",
    "path": "papers/23/06/2306.15713.json",
    "total_tokens": 951,
    "translated_title": "重新思考闭环训练对于自动驾驶的作用",
    "translated_abstract": "最近高保真度模拟器的进展使得自动驾驶代理的闭环训练成为可能，从而潜在地解决了训练与部署之间的分布偏移问题，并允许安全且廉价地进行大规模训练。然而，对于如何构建闭环训练的有效训练基准缺乏了解。在这项工作中，我们提出了第一项经验研究，分析了不同训练基准设计对于学习代理成功的影响，例如如何设计交通场景和规模化训练环境。此外，我们发现许多流行的强化学习算法在自动驾驶的背景下无法达到令人满意的性能，因为它们缺乏长期规划，并且训练时间非常长。为了解决这些问题，我们提出了轨迹价值学习（TRAVL），一种基于强化学习的驾驶代理，通过多步预测进行规划，并利用廉价生成的虚拟数据进行高效学习。",
    "tldr": "这项研究提出了闭环训练对于自动驾驶的重要性，并分析了不同训练基准设计和流行RL算法的不足。为了解决这些问题，提出了一种新的强化学习驾驶代理TRAVL，通过多步预测和利用虚拟数据进行高效学习。",
    "en_tdlr": "This research highlights the importance of closed-loop training for autonomous driving and analyzes the shortcomings of different training benchmark designs and popular RL algorithms. To address these issues, a new RL-based driving agent called TRAVL is proposed, which utilizes multi-step lookahead and cheaply generated imagined data for efficient learning."
}