{
    "title": "Online Prototype Alignment for Few-shot Policy Transfer. (arXiv:2306.07307v1 [cs.LG])",
    "abstract": "Domain adaptation in reinforcement learning (RL) mainly deals with the changes of observation when transferring the policy to a new environment. Many traditional approaches of domain adaptation in RL manage to learn a mapping function between the source and target domain in explicit or implicit ways. However, they typically require access to abundant data from the target domain. Besides, they often rely on visual clues to learn the mapping function and may fail when the source domain looks quite different from the target domain. To address these problems, we propose a novel framework Online Prototype Alignment (OPA) to learn the mapping function based on the functional similarity of elements and is able to achieve the few-shot policy transfer within only several episodes. The key insight of OPA is to introduce an exploration mechanism that can interact with the unseen elements of the target domain in an efficient and purposeful manner, and then connect them with the seen elements in th",
    "link": "http://arxiv.org/abs/2306.07307",
    "context": "Title: Online Prototype Alignment for Few-shot Policy Transfer. (arXiv:2306.07307v1 [cs.LG])\nAbstract: Domain adaptation in reinforcement learning (RL) mainly deals with the changes of observation when transferring the policy to a new environment. Many traditional approaches of domain adaptation in RL manage to learn a mapping function between the source and target domain in explicit or implicit ways. However, they typically require access to abundant data from the target domain. Besides, they often rely on visual clues to learn the mapping function and may fail when the source domain looks quite different from the target domain. To address these problems, we propose a novel framework Online Prototype Alignment (OPA) to learn the mapping function based on the functional similarity of elements and is able to achieve the few-shot policy transfer within only several episodes. The key insight of OPA is to introduce an exploration mechanism that can interact with the unseen elements of the target domain in an efficient and purposeful manner, and then connect them with the seen elements in th",
    "path": "papers/23/06/2306.07307.json",
    "total_tokens": 905,
    "translated_title": "少样本策略迁移的在线原型对齐",
    "translated_abstract": "强化学习中的领域适应主要涉及在将策略转移到新环境时观察的变化。领域适应的许多传统方法以显式或隐式地学习源域和目标域之间的映射函数为主。然而，它们通常需要访问目标域的大量数据。此外，它们常常依赖于视觉线索来学习映射函数，当源域与目标域看起来非常不同时，可能会失败。为解决这些问题，我们提出了一个新的框架在线原型对齐（OPA），该框架基于元素的功能相似性来学习映射函数，能够在仅几个回合内实现少样本策略转移。OPA的关键见解是引入一个探索机制，可以高效、有目的地与目标域的未知元素交互，并将它们与已知的元素连接起来。",
    "tldr": "本文提出了一种基于元素功能相似性的在线原型对齐（OPA）框架，能够在少样本的情况下实现策略转移，解决了传统方法映射函数学习需要大量数据以及依赖视觉特征等问题。",
    "en_tdlr": "This paper proposes an Online Prototype Alignment (OPA) framework based on the functional similarity of elements for mapping function learning in few-shot policy transfer. OPA introduces a new exploration mechanism that can efficiently and purposefully interact with unseen elements of the target domain. It addresses the issues of traditional mapping function learning methods that require abundant data and rely on visual clues."
}