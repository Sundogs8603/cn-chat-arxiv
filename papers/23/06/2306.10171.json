{
    "title": "Bootstrapped Representations in Reinforcement Learning. (arXiv:2306.10171v1 [cs.LG])",
    "abstract": "In reinforcement learning (RL), state representations are key to dealing with large or continuous state spaces. While one of the promises of deep learning algorithms is to automatically construct features well-tuned for the task they try to solve, such a representation might not emerge from end-to-end training of deep RL agents. To mitigate this issue, auxiliary objectives are often incorporated into the learning process and help shape the learnt state representation. Bootstrapping methods are today's method of choice to make these additional predictions. Yet, it is unclear which features these algorithms capture and how they relate to those from other auxiliary-task-based approaches. In this paper, we address this gap and provide a theoretical characterization of the state representation learnt by temporal difference learning (Sutton, 1988). Surprisingly, we find that this representation differs from the features learned by Monte Carlo and residual gradient algorithms for most transit",
    "link": "http://arxiv.org/abs/2306.10171",
    "context": "Title: Bootstrapped Representations in Reinforcement Learning. (arXiv:2306.10171v1 [cs.LG])\nAbstract: In reinforcement learning (RL), state representations are key to dealing with large or continuous state spaces. While one of the promises of deep learning algorithms is to automatically construct features well-tuned for the task they try to solve, such a representation might not emerge from end-to-end training of deep RL agents. To mitigate this issue, auxiliary objectives are often incorporated into the learning process and help shape the learnt state representation. Bootstrapping methods are today's method of choice to make these additional predictions. Yet, it is unclear which features these algorithms capture and how they relate to those from other auxiliary-task-based approaches. In this paper, we address this gap and provide a theoretical characterization of the state representation learnt by temporal difference learning (Sutton, 1988). Surprisingly, we find that this representation differs from the features learned by Monte Carlo and residual gradient algorithms for most transit",
    "path": "papers/23/06/2306.10171.json",
    "total_tokens": 833,
    "translated_title": "强化学习中的引导式表示",
    "translated_abstract": "在强化学习中，状态表示是处理大型或连续状态空间的关键。尽管深度学习算法的承诺是自动构建适合解决任务的特征，但这样的表示可能不会从深度RL代理的端到端训练中出现。为了缓解这个问题，常常将辅助目标纳入学习过程中，并帮助形塑学习的状态表示。引导方法是如今此类附加预测的选择方法。然而，这些算法捕获的特征不清楚，也不知道它们与其他基于辅助任务的方法中的特征相关性如何。本文填补了这一空白，提供了时间差分学习(Sutton, 1988)学习的状态表示的理论刻画。令人惊讶的是，我们发现这种表示与通过蒙特卡罗和残差梯度算法学习的特征在大多数转移中是不同的。",
    "tldr": "本研究对强化学习中引导式表示进行了理论分析，发现与蒙特卡罗和残差梯度算法学习的特征大多不同",
    "en_tdlr": "This study provides a theoretical analysis of bootstrapped representations in reinforcement learning, finding that they differ from those learned by Monte Carlo and residual gradient algorithms in most transitions."
}