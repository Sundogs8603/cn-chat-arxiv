{
    "title": "Near-Optimal Nonconvex-Strongly-Convex Bilevel Optimization with Fully First-Order Oracles. (arXiv:2306.14853v2 [math.OC] UPDATED)",
    "abstract": "Bilevel optimization has wide applications such as hyperparameter tuning, neural architecture search, and meta-learning. Designing efficient algorithms for bilevel optimization is challenging because the lower-level problem defines a feasibility set implicitly via another optimization problem. In this work, we consider one tractable case when the lower-level problem is strongly convex. Recent works show that with a Hessian-vector product oracle, one can provably find an $\\epsilon$-first-order stationary point within $\\tilde{\\mathcal{O}}(\\epsilon^{-2})$ oracle calls. However, Hessian-vector product may be inaccessible or expensive in practice. Kwon et al. (ICML 2023) addressed this issue by proposing a first-order method that can achieve the same goal at a slower rate of $\\tilde{\\mathcal{O}}(\\epsilon^{-3})$. In this work, we provide a tighter analysis demonstrating that this method can converge at the near-optimal $\\tilde {\\mathcal{O}}(\\epsilon^{-2})$ rate as second-order methods. Our a",
    "link": "http://arxiv.org/abs/2306.14853",
    "context": "Title: Near-Optimal Nonconvex-Strongly-Convex Bilevel Optimization with Fully First-Order Oracles. (arXiv:2306.14853v2 [math.OC] UPDATED)\nAbstract: Bilevel optimization has wide applications such as hyperparameter tuning, neural architecture search, and meta-learning. Designing efficient algorithms for bilevel optimization is challenging because the lower-level problem defines a feasibility set implicitly via another optimization problem. In this work, we consider one tractable case when the lower-level problem is strongly convex. Recent works show that with a Hessian-vector product oracle, one can provably find an $\\epsilon$-first-order stationary point within $\\tilde{\\mathcal{O}}(\\epsilon^{-2})$ oracle calls. However, Hessian-vector product may be inaccessible or expensive in practice. Kwon et al. (ICML 2023) addressed this issue by proposing a first-order method that can achieve the same goal at a slower rate of $\\tilde{\\mathcal{O}}(\\epsilon^{-3})$. In this work, we provide a tighter analysis demonstrating that this method can converge at the near-optimal $\\tilde {\\mathcal{O}}(\\epsilon^{-2})$ rate as second-order methods. Our a",
    "path": "papers/23/06/2306.14853.json",
    "total_tokens": 1002,
    "translated_title": "近似最优非凸-强凸双层优化与全一阶预言机",
    "translated_abstract": "双层优化在超参数调整、神经架构搜索和元学习等领域有着广泛应用。设计高效的双层优化算法是具有挑战性的，因为底层问题通过另一个优化问题隐式定义了一个可行性集。在这项工作中，我们考虑一种易于处理的情况，即底层问题是强凸的。最近的研究表明，通过Hessian-向量乘积预言机，可以在$\\tilde{\\mathcal{O}}(\\epsilon^{-2})$个预言调用内可靠地找到一个$\\epsilon$-一阶稳定点。然而，在实践中，Hessian-向量乘积可能无法获得或代价昂贵。Kwon等人（ICML 2023）通过提出一个一阶方法来解决这个问题，该方法可以以较慢的$\\tilde{\\mathcal{O}}(\\epsilon^{-3})$的速率实现相同的目标。在这项工作中，我们提供了更严格的分析，证明这种方法可以以接近最优的$\\tilde {\\mathcal{O}}(\\epsilon^{-2})$的速率像二阶方法一样收敛。",
    "tldr": "本文针对双层优化问题中底层问题为强凸的情况，提出了一种更加高效的算法，可以以接近最优的速率收敛。这种算法避免了在实践中可能无法获得或代价昂贵的Hessian-向量乘积预言机的使用。"
}