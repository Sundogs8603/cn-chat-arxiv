{
    "title": "The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter. (arXiv:2306.03805v2 [cs.LG] UPDATED)",
    "abstract": "Large pre-trained transformers are show-stealer in modern-day deep learning, and it becomes crucial to comprehend the parsimonious patterns that exist within them as they grow in scale. With exploding parameter counts, Lottery Ticket Hypothesis (LTH) and its variants, have lost their pragmatism in sparsifying them due to high computation and memory bottleneck of repetitive train-prune-retrain routine of iterative magnitude pruning (IMP) which worsens with increasing model size. This paper comprehensively studies induced sparse patterns across multiple large pre-trained vision and language transformers. We propose the existence of -- essential sparsity defined with a sharp dropping point beyond which the performance declines much faster w.r.t the rise of sparsity level, when we directly remove weights with the smallest magnitudes in one-shot without re-training. We also find essential sparsity to hold valid for N:M sparsity patterns as well as on modern-scale large language models (Vicu",
    "link": "http://arxiv.org/abs/2306.03805",
    "context": "Title: The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter. (arXiv:2306.03805v2 [cs.LG] UPDATED)\nAbstract: Large pre-trained transformers are show-stealer in modern-day deep learning, and it becomes crucial to comprehend the parsimonious patterns that exist within them as they grow in scale. With exploding parameter counts, Lottery Ticket Hypothesis (LTH) and its variants, have lost their pragmatism in sparsifying them due to high computation and memory bottleneck of repetitive train-prune-retrain routine of iterative magnitude pruning (IMP) which worsens with increasing model size. This paper comprehensively studies induced sparse patterns across multiple large pre-trained vision and language transformers. We propose the existence of -- essential sparsity defined with a sharp dropping point beyond which the performance declines much faster w.r.t the rise of sparsity level, when we directly remove weights with the smallest magnitudes in one-shot without re-training. We also find essential sparsity to hold valid for N:M sparsity patterns as well as on modern-scale large language models (Vicu",
    "path": "papers/23/06/2306.03805.json",
    "total_tokens": 966,
    "translated_title": "大型预训练模型中的关键稀疏性的出现：重要的权重。",
    "translated_abstract": "大型预训练变压器在现代深度学习中表现出色，因此理解它们内部存在的简化模式变得至关重要。随着参数数量的增加，通过迭代量化剪枝（IMP）的“抽奖票据假设”（LTH）及其变体已经失去了稀疏化它们的实用性，因为重复的训练-剪枝-重新训练过程导致了计算和内存瓶颈问题，而这种问题在模型规模增大时变得更加严重。本文全面研究了在多个大型预训练视觉和语言变压器中引发的稀疏模式。我们提出了一种关键稀疏性的存在，即当我们一次性直接删除具有最小幅度的权重时，性能下降速度随着稀疏程度的增加而加快，存在一个拐点。我们还发现关键稀疏性在N:M稀疏模式以及现代大规模语言模型中仍然有效。",
    "tldr": "本文在多个大型预训练模型中研究了关键稀疏性，发现在去除具有最小幅度权重的情况下，性能下降速度与稀疏程度的增加呈现出拐点，这对于大型语言模型仍然有效。",
    "en_tdlr": "This paper comprehensively studies the emergence of essential sparsity in large pre-trained models, finding that there is a sharp dropping point where the performance declines much faster in relation to the rise of sparsity level when removing weights with the smallest magnitudes. This holds true for N:M sparsity patterns and modern-scale large language models as well."
}