{
    "title": "Automated Annotation with Generative AI Requires Validation. (arXiv:2306.00176v1 [cs.CL])",
    "abstract": "Generative large language models (LLMs) can be a powerful tool for augmenting text annotation procedures, but their performance varies across annotation tasks due to prompt quality, text data idiosyncrasies, and conceptual difficulty. Because these challenges will persist even as LLM technology improves, we argue that any automated annotation process using an LLM must validate the LLM's performance against labels generated by humans. To this end, we outline a workflow to harness the annotation potential of LLMs in a principled, efficient way. Using GPT-4, we validate this approach by replicating 27 annotation tasks across 11 datasets from recent social science articles in high-impact journals. We find that LLM performance for text annotation is promising but highly contingent on both the dataset and the type of annotation task, which reinforces the necessity to validate on a task-by-task basis. We make available easy-to-use software designed to implement our workflow and streamline the",
    "link": "http://arxiv.org/abs/2306.00176",
    "context": "Title: Automated Annotation with Generative AI Requires Validation. (arXiv:2306.00176v1 [cs.CL])\nAbstract: Generative large language models (LLMs) can be a powerful tool for augmenting text annotation procedures, but their performance varies across annotation tasks due to prompt quality, text data idiosyncrasies, and conceptual difficulty. Because these challenges will persist even as LLM technology improves, we argue that any automated annotation process using an LLM must validate the LLM's performance against labels generated by humans. To this end, we outline a workflow to harness the annotation potential of LLMs in a principled, efficient way. Using GPT-4, we validate this approach by replicating 27 annotation tasks across 11 datasets from recent social science articles in high-impact journals. We find that LLM performance for text annotation is promising but highly contingent on both the dataset and the type of annotation task, which reinforces the necessity to validate on a task-by-task basis. We make available easy-to-use software designed to implement our workflow and streamline the",
    "path": "papers/23/06/2306.00176.json",
    "total_tokens": 903,
    "translated_title": "基于生成式人工智能的自动标注需要验证",
    "translated_abstract": "生成式大型语言模型（LLM）可以成为文本注释过程的强大工具，但由于提示质量，文本数据特定性和概念难度等原因，它们的性能在注释任务方面存在差异。因为即使LLM技术得到改进，这些挑战仍将存在，所以我们认为使用LLM的任何自动标注过程都必须针对人类生成的标签验证LLM的性能。为此，我们概述了一个工作流程，以一种原则性和有效率的方式利用LLM的注释潜力。使用GPT-4，我们通过在高影响期刊的最新社会科学文章中复制11个数据集的27个标注任务来验证这种方法。我们发现，对于文本注释，LLM的性能很有前途，但高度取决于数据集和注释任务类型，这强调了按任务验证的必要性。我们提供易于使用的软件，旨在实现我们的工作流程并简化操作。",
    "tldr": "本文探讨了利用生成式大型语言模型进行文本注释的自动化流程，强调必须针对人类生成的标签验证它们的性能，验证结果表明LLM的性能很有前途，但高度依赖于数据集和注释任务类型。",
    "en_tdlr": "This article discusses the automated workflow of using generative large language models for text annotation, emphasizing the necessity to validate their performance against labels generated by humans. The results of the validation show promising performance of LLMs for text annotation but highly contingent on both the dataset and the type of annotation task."
}