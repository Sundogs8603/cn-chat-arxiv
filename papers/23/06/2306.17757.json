{
    "title": "Should you marginalize over possible tokenizations?. (arXiv:2306.17757v1 [cs.CL])",
    "abstract": "Autoregressive language models (LMs) map token sequences to probabilities. The usual practice for computing the probability of any character string (e.g. English sentences) is to first transform it into a sequence of tokens that is scored by the model. However, there are exponentially many token sequences that represent any given string. To truly compute the probability of a string one should marginalize over all tokenizations, which is typically intractable. Here, we analyze whether the practice of ignoring the marginalization is justified. To this end, we devise an importance-sampling-based algorithm that allows us to compute estimates of the marginal probabilities and compare them to the default procedure in a range of state-of-the-art models and datasets. Our results show that the gap in log-likelihood is no larger than 0.5% in most cases, but that it becomes more pronounced for data with long complex words.",
    "link": "http://arxiv.org/abs/2306.17757",
    "context": "Title: Should you marginalize over possible tokenizations?. (arXiv:2306.17757v1 [cs.CL])\nAbstract: Autoregressive language models (LMs) map token sequences to probabilities. The usual practice for computing the probability of any character string (e.g. English sentences) is to first transform it into a sequence of tokens that is scored by the model. However, there are exponentially many token sequences that represent any given string. To truly compute the probability of a string one should marginalize over all tokenizations, which is typically intractable. Here, we analyze whether the practice of ignoring the marginalization is justified. To this end, we devise an importance-sampling-based algorithm that allows us to compute estimates of the marginal probabilities and compare them to the default procedure in a range of state-of-the-art models and datasets. Our results show that the gap in log-likelihood is no larger than 0.5% in most cases, but that it becomes more pronounced for data with long complex words.",
    "path": "papers/23/06/2306.17757.json",
    "total_tokens": 896,
    "translated_title": "是否应该对可能的标记化进行边缘化计算？",
    "translated_abstract": "自回归语言模型(LMs)将令牌序列映射到概率。计算任何字符串(例如英文句子)的概率的常见做法是先将其转换为由模型评分的令牌序列。然而，有指数级的令牌序列可以表示任何给定的字符串。为了真正计算字符串的概率，应该对所有标记化进行边缘化计算，但这通常是难以处理的。在这里，我们分析忽略边缘化计算的做法是否合理。为此，我们设计了一种基于重要性采样的算法，使我们能够计算边缘概率的估计，并将其与一系列最先进的模型和数据集中的默认过程进行比较。我们的结果表明，在大多数情况下，对数似然差距不超过0.5％，但对于包含长复杂单词的数据，这种差距变得更加明显。",
    "tldr": "该论文分析了语言模型计算字符串概率时是否应该边缘化所有可能的标记化。研究结果表明，在大多数情况下，忽略边缘化计算的差距不超过0.5%，但对于含有长复杂单词的数据来说，这种差距更加明显。"
}