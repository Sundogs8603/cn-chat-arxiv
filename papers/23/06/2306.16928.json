{
    "title": "One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization. (arXiv:2306.16928v1 [cs.CV])",
    "abstract": "Single image 3D reconstruction is an important but challenging task that requires extensive knowledge of our natural world. Many existing methods solve this problem by optimizing a neural radiance field under the guidance of 2D diffusion models but suffer from lengthy optimization time, 3D inconsistency results, and poor geometry. In this work, we propose a novel method that takes a single image of any object as input and generates a full 360-degree 3D textured mesh in a single feed-forward pass. Given a single image, we first use a view-conditioned 2D diffusion model, Zero123, to generate multi-view images for the input view, and then aim to lift them up to 3D space. Since traditional reconstruction methods struggle with inconsistent multi-view predictions, we build our 3D reconstruction module upon an SDF-based generalizable neural surface reconstruction method and propose several critical training strategies to enable the reconstruction of 360-degree meshes. Without costly optimizat",
    "link": "http://arxiv.org/abs/2306.16928",
    "context": "Title: One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization. (arXiv:2306.16928v1 [cs.CV])\nAbstract: Single image 3D reconstruction is an important but challenging task that requires extensive knowledge of our natural world. Many existing methods solve this problem by optimizing a neural radiance field under the guidance of 2D diffusion models but suffer from lengthy optimization time, 3D inconsistency results, and poor geometry. In this work, we propose a novel method that takes a single image of any object as input and generates a full 360-degree 3D textured mesh in a single feed-forward pass. Given a single image, we first use a view-conditioned 2D diffusion model, Zero123, to generate multi-view images for the input view, and then aim to lift them up to 3D space. Since traditional reconstruction methods struggle with inconsistent multi-view predictions, we build our 3D reconstruction module upon an SDF-based generalizable neural surface reconstruction method and propose several critical training strategies to enable the reconstruction of 360-degree meshes. Without costly optimizat",
    "path": "papers/23/06/2306.16928.json",
    "total_tokens": 980,
    "translated_title": "One-2-3-45: 在45秒内将任意单张图像转换为3D网格，无需进行形状优化",
    "translated_abstract": "单图像3D重建是一项重要但具有挑战性的任务，需要对自然世界有广泛的知识。许多现有方法通过在2D扩散模型的指导下优化神经辐射场来解决这个问题，但存在优化时间长、3D结果不一致和几何质量差的问题。本文提出了一种新的方法，它以任意物体的单张图像作为输入，并在单次前馈传递中生成一个完整的360度3D纹理网格。给定一张图像，我们首先使用一个视图条件的2D扩散模型Zero123为输入视图生成多视图图像，然后将它们提升到3D空间。由于传统重建方法难以处理不一致的多视图预测，我们基于基于SDF的通用神经表面重建方法构建了我们的3D重建模块，并提出了几种关键的训练策略，以实现360度网格的重建。无需耗时的优化过程。",
    "tldr": "本论文提出了一种无需进行形状优化的新方法，可以在45秒内将任意单张图像转换为360度的3D纹理网格。该方法采用了视图条件的2D扩散模型和基于SDF的神经表面重建方法，通过关键的训练策略实现了准确且一致的多视图预测。",
    "en_tdlr": "This paper proposes a novel method that can convert any single image to a 360-degree 3D textured mesh in 45 seconds without per-shape optimization. It utilizes a view-conditioned 2D diffusion model and an SDF-based neural surface reconstruction method with critical training strategies to achieve accurate and consistent multi-view predictions."
}