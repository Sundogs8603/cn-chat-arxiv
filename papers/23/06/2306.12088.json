{
    "title": "An Efficient Virtual Data Generation Method for Reducing Communication in Federated Learning. (arXiv:2306.12088v1 [cs.LG])",
    "abstract": "Communication overhead is one of the major challenges in Federated Learning(FL). A few classical schemes assume the server can extract the auxiliary information about training data of the participants from the local models to construct a central dummy dataset. The server uses the dummy dataset to finetune aggregated global model to achieve the target test accuracy in fewer communication rounds. In this paper, we summarize the above solutions into a data-based communication-efficient FL framework. The key of the proposed framework is to design an efficient extraction module(EM) which ensures the dummy dataset has a positive effect on finetuning aggregated global model. Different from the existing methods that use generator to design EM, our proposed method, FedINIBoost borrows the idea of gradient match to construct EM. Specifically, FedINIBoost builds a proxy dataset of the real dataset in two steps for each participant at each communication round. Then the server aggregates all the pr",
    "link": "http://arxiv.org/abs/2306.12088",
    "context": "Title: An Efficient Virtual Data Generation Method for Reducing Communication in Federated Learning. (arXiv:2306.12088v1 [cs.LG])\nAbstract: Communication overhead is one of the major challenges in Federated Learning(FL). A few classical schemes assume the server can extract the auxiliary information about training data of the participants from the local models to construct a central dummy dataset. The server uses the dummy dataset to finetune aggregated global model to achieve the target test accuracy in fewer communication rounds. In this paper, we summarize the above solutions into a data-based communication-efficient FL framework. The key of the proposed framework is to design an efficient extraction module(EM) which ensures the dummy dataset has a positive effect on finetuning aggregated global model. Different from the existing methods that use generator to design EM, our proposed method, FedINIBoost borrows the idea of gradient match to construct EM. Specifically, FedINIBoost builds a proxy dataset of the real dataset in two steps for each participant at each communication round. Then the server aggregates all the pr",
    "path": "papers/23/06/2306.12088.json",
    "total_tokens": 870,
    "translated_title": "一种减少联邦学习通信的高效虚拟数据生成方法",
    "translated_abstract": "通信开销是联邦学习中的主要挑战之一。一些经典的方案假设服务器可以从本地模型中提取参与者训练数据的辅助信息来构建中央虚拟数据集。服务器使用虚拟数据集来微调聚合的全局模型，以在较少的通信轮次内达到目标测试精度。本文将上述解决方案概括为基于数据的通信高效联邦学习框架。提出框架的关键是设计一个有效的提取模块（EM），它确保虚拟数据集对微调聚合的全局模型产生积极影响。与现有方法使用生成器来设计EM不同，我们提出的方法FedINIBoost借鉴了梯度匹配的思想来构建EM。具体而言，FedINIBoost在每个通信轮次的每个参与者中使用两个步骤构建真实数据集的代理数据集。然后服务器聚合所有的代理数据集来构建中央虚拟数据集。",
    "tldr": "本论文提出FedINIBoost，一种新的联邦学习方法，通过梯度匹配构建有效的提取模块，在减少通信开销的同时提高了模型准确率。",
    "en_tdlr": "This paper proposes a new federated learning method called FedINIBoost, which constructs an efficient extraction module through gradient matching to reduce communication overhead while improving model accuracy."
}