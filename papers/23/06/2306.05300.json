{
    "title": "Correlated Noise in Epoch-Based Stochastic Gradient Descent: Implications for Weight Variances. (arXiv:2306.05300v1 [cs.LG])",
    "abstract": "Stochastic gradient descent (SGD) has become a cornerstone of neural network optimization, yet the noise introduced by SGD is often assumed to be uncorrelated over time, despite the ubiquity of epoch-based training. In this work, we challenge this assumption and investigate the effects of epoch-based noise correlations on the stationary distribution of discrete-time SGD with momentum, limited to a quadratic loss. Our main contributions are twofold: first, we calculate the exact autocorrelation of the noise for training in epochs under the assumption that the noise is independent of small fluctuations in the weight vector; second, we explore the influence of correlations introduced by the epoch-based learning scheme on SGD dynamics. We find that for directions with a curvature greater than a hyperparameter-dependent crossover value, the results for uncorrelated noise are recovered. However, for relatively flat directions, the weight variance is significantly reduced. We provide an intui",
    "link": "http://arxiv.org/abs/2306.05300",
    "context": "Title: Correlated Noise in Epoch-Based Stochastic Gradient Descent: Implications for Weight Variances. (arXiv:2306.05300v1 [cs.LG])\nAbstract: Stochastic gradient descent (SGD) has become a cornerstone of neural network optimization, yet the noise introduced by SGD is often assumed to be uncorrelated over time, despite the ubiquity of epoch-based training. In this work, we challenge this assumption and investigate the effects of epoch-based noise correlations on the stationary distribution of discrete-time SGD with momentum, limited to a quadratic loss. Our main contributions are twofold: first, we calculate the exact autocorrelation of the noise for training in epochs under the assumption that the noise is independent of small fluctuations in the weight vector; second, we explore the influence of correlations introduced by the epoch-based learning scheme on SGD dynamics. We find that for directions with a curvature greater than a hyperparameter-dependent crossover value, the results for uncorrelated noise are recovered. However, for relatively flat directions, the weight variance is significantly reduced. We provide an intui",
    "path": "papers/23/06/2306.05300.json",
    "total_tokens": 976,
    "translated_title": "基于Epoch的随机梯度下降中相关噪声：权重方差的影响",
    "translated_abstract": "随机梯度下降（SGD）已成为神经网络优化的基石，但认为SGD引入的噪声在时间上是不相关的，尽管epoch-based训练是无处不在的。在这项工作中，我们对此进行了挑战，并调查了epoch-based噪声相关性对离散时间带动量的SGD的稳态分布的影响，限于二次损失。我们的主要贡献有两个：首先，我们计算训练epoch时噪声的精确自相关性，假设该噪声独立于权重向量中的小波动;其次，我们探索epoch-based学习方案引入的相关性对SGD动态的影响。我们发现，在曲率大于一个超参数相关值的方向上，还原了不相关噪声的结果。然而，在相对平坦的方向上，权重方差显着减小。我们使用简单的二维图例对这些结果进行了直观解释。总的来说，我们的工作提供了关于epoch-based SGD中相关噪声影响的见解，可以指导设计更有效的优化算法。",
    "tldr": "研究挑战了在时间上是不相关的假设，并强调了epoch-based噪声相关性对离散时间带动量的SGD的权重方差的影响。",
    "en_tdlr": "The paper challenges the assumption of uncorrelated noise over time in epoch-based stochastic gradient descent and investigates the impact of correlated noise on the weight variance. The study finds that correlated noise significantly reduces the weight variance in relatively flat directions and provides insights for the design of more effective optimization algorithms."
}