{
    "title": "Precision-aware Latency and Energy Balancing on Multi-Accelerator Platforms for DNN Inference. (arXiv:2306.05060v1 [cs.LG])",
    "abstract": "The need to execute Deep Neural Networks (DNNs) at low latency and low power at the edge has spurred the development of new heterogeneous Systems-on-Chips (SoCs) encapsulating a diverse set of hardware accelerators. How to optimally map a DNN onto such multi-accelerator systems is an open problem. We propose ODiMO, a hardware-aware tool that performs a fine-grain mapping across different accelerators on-chip, splitting individual layers and executing them in parallel, to reduce inference energy consumption or latency, while taking into account each accelerator's quantization precision to maintain accuracy. Pareto-optimal networks in the accuracy vs. energy or latency space are pursued for three popular dataset/DNN pairs, and deployed on the DIANA heterogeneous ultra-low power edge AI SoC. We show that ODiMO reduces energy/latency by up to 33%/31% with limited accuracy drop (-0.53%/-0.32%) compared to manual heuristic mappings.",
    "link": "http://arxiv.org/abs/2306.05060",
    "context": "Title: Precision-aware Latency and Energy Balancing on Multi-Accelerator Platforms for DNN Inference. (arXiv:2306.05060v1 [cs.LG])\nAbstract: The need to execute Deep Neural Networks (DNNs) at low latency and low power at the edge has spurred the development of new heterogeneous Systems-on-Chips (SoCs) encapsulating a diverse set of hardware accelerators. How to optimally map a DNN onto such multi-accelerator systems is an open problem. We propose ODiMO, a hardware-aware tool that performs a fine-grain mapping across different accelerators on-chip, splitting individual layers and executing them in parallel, to reduce inference energy consumption or latency, while taking into account each accelerator's quantization precision to maintain accuracy. Pareto-optimal networks in the accuracy vs. energy or latency space are pursued for three popular dataset/DNN pairs, and deployed on the DIANA heterogeneous ultra-low power edge AI SoC. We show that ODiMO reduces energy/latency by up to 33%/31% with limited accuracy drop (-0.53%/-0.32%) compared to manual heuristic mappings.",
    "path": "papers/23/06/2306.05060.json",
    "total_tokens": 988,
    "translated_title": "面向DNN推断的多加速器平台上精度感知的延迟和能量平衡",
    "translated_abstract": "在边缘计算中需要低延迟和低功耗地执行深度神经网络(DNN)，这促使了新的异构片上系统(System-on-Chips, SoCs)的开发，其封装了各种硬件加速器。如何最优地将DNN映射到这样的多加速器系统中仍然是一个开放的问题。我们提出ODiMO，一种硬件感知的工具，它在片上的不同加速器之间执行细粒度映射，分割单个层并并行执行它们，以降低推断的能量消耗或延迟，同时考虑每个加速器的量化精度以维护准确性。在三个流行的数据集/DNN对上追求准确性与能量或延迟空间中的帕累托最优网络，并部署在DIANA异构超低功耗边缘AI SoC上。我们展示了ODiMO相对于手动启发式映射可以降低能量/延迟高达33%/31%，而准确度下降有限(-0.53%/-0.32%)。",
    "tldr": "介绍了ODiMO，一种硬件感知的工具，通过精细映射将DNN分割并在不同加速器上并行执行，以在维持准确性的前提下降低推断的能耗和延迟。",
    "en_tdlr": "This paper proposes ODiMO, a hardware-aware tool that fine-grain maps DNNs onto different accelerators on-chip to reduce inference energy consumption or latency, while maintaining accuracy by taking into account each accelerator's quantization precision. ODiMO achieves up to 33% energy/latency reduction with limited accuracy drop (-0.53%/-0.32%) compared to manual mappings."
}