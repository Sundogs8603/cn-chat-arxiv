{
    "title": "Generalization Across Observation Shifts in Reinforcement Learning. (arXiv:2306.04595v1 [cs.LG])",
    "abstract": "Learning policies which are robust to changes in the environment are critical for real world deployment of Reinforcement Learning agents. They are also necessary for achieving good generalization across environment shifts. We focus on bisimulation metrics, which provide a powerful means for abstracting task relevant components of the observation and learning a succinct representation space for training the agent using reinforcement learning. In this work, we extend the bisimulation framework to also account for context dependent observation shifts. Specifically, we focus on the simulator based learning setting and use alternate observations to learn a representation space which is invariant to observation shifts using a novel bisimulation based objective. This allows us to deploy the agent to varying observation settings during test time and generalize to unseen scenarios. We further provide novel theoretical bounds for simulator fidelity and performance transfer guarantees for using a",
    "link": "http://arxiv.org/abs/2306.04595",
    "context": "Title: Generalization Across Observation Shifts in Reinforcement Learning. (arXiv:2306.04595v1 [cs.LG])\nAbstract: Learning policies which are robust to changes in the environment are critical for real world deployment of Reinforcement Learning agents. They are also necessary for achieving good generalization across environment shifts. We focus on bisimulation metrics, which provide a powerful means for abstracting task relevant components of the observation and learning a succinct representation space for training the agent using reinforcement learning. In this work, we extend the bisimulation framework to also account for context dependent observation shifts. Specifically, we focus on the simulator based learning setting and use alternate observations to learn a representation space which is invariant to observation shifts using a novel bisimulation based objective. This allows us to deploy the agent to varying observation settings during test time and generalize to unseen scenarios. We further provide novel theoretical bounds for simulator fidelity and performance transfer guarantees for using a",
    "path": "papers/23/06/2306.04595.json",
    "total_tokens": 720,
    "translated_title": "强化学习中的观测转移泛化问题研究",
    "translated_abstract": "学习到的策略对于环境变化的鲁棒性对于强化学习智能体在实际世界的应用和跨环境转移学习至关重要。本文关注于基于仿真器学习的情况下的观测变化问题，提出了一种基于等价度量的新型目标函数，通过学习到可以适应不同观测转移的表征空间，实现了对于未知环境的泛化能力。",
    "tldr": "本文研究了强化学习中观测转移泛化问题，在基于仿真器学习的情况下，通过学习到可以适应不同观测转移的表征空间，实现了对于未知环境的泛化能力。",
    "en_tdlr": "This paper focuses on the observation shift generalization problem in reinforcement learning, proposing a novel bisimulation based objective to learn an invariant representation space for varying observation settings in simulator based learning, achieving the generalization ability to unknown environments."
}