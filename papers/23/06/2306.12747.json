{
    "title": "Don't be so Monotone: Relaxing Stochastic Line Search in Over-Parameterized Models. (arXiv:2306.12747v1 [math.OC])",
    "abstract": "Recent works have shown that line search methods can speed up Stochastic Gradient Descent (SGD) and Adam in modern over-parameterized settings. However, existing line searches may take steps that are smaller than necessary since they require a monotone decrease of the (mini-)batch objective function. We explore nonmonotone line search methods to relax this condition and possibly accept larger step sizes. Despite the lack of a monotonic decrease, we prove the same fast rates of convergence as in the monotone case. Our experiments show that nonmonotone methods improve the speed of convergence and generalization properties of SGD/Adam even beyond the previous monotone line searches. We propose a POlyak NOnmonotone Stochastic (PoNoS) method, obtained by combining a nonmonotone line search with a Polyak initial step size. Furthermore, we develop a new resetting technique that in the majority of the iterations reduces the amount of backtracks to zero while still maintaining a large initial s",
    "link": "http://arxiv.org/abs/2306.12747",
    "context": "Title: Don't be so Monotone: Relaxing Stochastic Line Search in Over-Parameterized Models. (arXiv:2306.12747v1 [math.OC])\nAbstract: Recent works have shown that line search methods can speed up Stochastic Gradient Descent (SGD) and Adam in modern over-parameterized settings. However, existing line searches may take steps that are smaller than necessary since they require a monotone decrease of the (mini-)batch objective function. We explore nonmonotone line search methods to relax this condition and possibly accept larger step sizes. Despite the lack of a monotonic decrease, we prove the same fast rates of convergence as in the monotone case. Our experiments show that nonmonotone methods improve the speed of convergence and generalization properties of SGD/Adam even beyond the previous monotone line searches. We propose a POlyak NOnmonotone Stochastic (PoNoS) method, obtained by combining a nonmonotone line search with a Polyak initial step size. Furthermore, we develop a new resetting technique that in the majority of the iterations reduces the amount of backtracks to zero while still maintaining a large initial s",
    "path": "papers/23/06/2306.12747.json",
    "total_tokens": 917,
    "translated_title": "不要太单调：放宽超参数模型中的随机线搜索",
    "translated_abstract": "近期的工作表明，线搜索方法可以提高现代超参数设置下的随机梯度下降（SGD）和Adam的速度。但是，由于需要（小批量）目标函数的单调减少，现有的线搜索可能会采取比必要更小的步骤。我们探索了非单调线搜索方法来放宽这个条件，并可能接受更大的步长。尽管缺乏单调递减，但我们证明与单调情况相同的快速收敛速度。我们的实验表明，非单调方法在SGD / Adam的收敛速度和泛化性能方面甚至超越了先前的单调线搜索。我们提出了一种POlyak NOnmonotone随机（PoNoS）方法，通过将非单调线搜索与Polyak初始步长结合而得到。此外，我们开发了一种新的重置技术，在大多数迭代中将回溯的数量减少到零，同时仍保持较大的初始s。",
    "tldr": "本文研究了在超参数模型中解决随机梯度下降（SGD）和Adam的速度问题，提出了一种非单调线搜索方法，取得更快的收敛速度和泛化性能，并结合同步的Polyak初始化步伐实现。"
}