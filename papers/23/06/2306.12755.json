{
    "title": "Beyond OOD State Actions: Supported Cross-Domain Offline Reinforcement Learning. (arXiv:2306.12755v1 [cs.LG])",
    "abstract": "Offline reinforcement learning (RL) aims to learn a policy using only pre-collected and fixed data. Although avoiding the time-consuming online interactions in RL, it poses challenges for out-of-distribution (OOD) state actions and often suffers from data inefficiency for training. Despite many efforts being devoted to addressing OOD state actions, the latter (data inefficiency) receives little attention in offline RL. To address this, this paper proposes the cross-domain offline RL, which assumes offline data incorporate additional source-domain data from varying transition dynamics (environments), and expects it to contribute to the offline data efficiency. To do so, we identify a new challenge of OOD transition dynamics, beyond the common OOD state actions issue, when utilizing cross-domain offline data. Then, we propose our method BOSA, which employs two support-constrained objectives to address the above OOD issues. Through extensive experiments in the cross-domain offline RL sett",
    "link": "http://arxiv.org/abs/2306.12755",
    "context": "Title: Beyond OOD State Actions: Supported Cross-Domain Offline Reinforcement Learning. (arXiv:2306.12755v1 [cs.LG])\nAbstract: Offline reinforcement learning (RL) aims to learn a policy using only pre-collected and fixed data. Although avoiding the time-consuming online interactions in RL, it poses challenges for out-of-distribution (OOD) state actions and often suffers from data inefficiency for training. Despite many efforts being devoted to addressing OOD state actions, the latter (data inefficiency) receives little attention in offline RL. To address this, this paper proposes the cross-domain offline RL, which assumes offline data incorporate additional source-domain data from varying transition dynamics (environments), and expects it to contribute to the offline data efficiency. To do so, we identify a new challenge of OOD transition dynamics, beyond the common OOD state actions issue, when utilizing cross-domain offline data. Then, we propose our method BOSA, which employs two support-constrained objectives to address the above OOD issues. Through extensive experiments in the cross-domain offline RL sett",
    "path": "papers/23/06/2306.12755.json",
    "total_tokens": 718,
    "translated_title": "超越OOD状态行动：支持交叉领域离线强化学习",
    "translated_abstract": "离线强化学习旨在仅使用预先收集且固定的数据来学习策略。尽管避免了RL中耗时的在线交互，但它对于OOD状态操作提出了挑战，并且通常在训练中存在数据效率问题。该论文提出了交叉领域离线RL模型，以解决跨领域离线数据的OOD转换动态问题，并期望其提高线下数据的效率。",
    "tldr": "该论文提出了一种交叉领域离线RL模型，以应对线下数据效率问题，并通过支持约束目标解决了OOD状态操作和OOD转换动态的挑战。",
    "en_tdlr": "This paper proposes a cross-domain offline RL model to address the data inefficiency issue, and tackles the challenges of OOD state actions and OOD transition dynamics through support-constrained objectives."
}