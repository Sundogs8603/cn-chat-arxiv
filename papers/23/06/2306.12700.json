{
    "title": "Accelerated Training via Incrementally Growing Neural Networks using Variance Transfer and Learning Rate Adaptation. (arXiv:2306.12700v1 [cs.LG])",
    "abstract": "We develop an approach to efficiently grow neural networks, within which parameterization and optimization strategies are designed by considering their effects on the training dynamics. Unlike existing growing methods, which follow simple replication heuristics or utilize auxiliary gradient-based local optimization, we craft a parameterization scheme which dynamically stabilizes weight, activation, and gradient scaling as the architecture evolves, and maintains the inference functionality of the network. To address the optimization difficulty resulting from imbalanced training effort distributed to subnetworks fading in at different growth phases, we propose a learning rate adaption mechanism that rebalances the gradient contribution of these separate subcomponents. Experimental results show that our method achieves comparable or better accuracy than training large fixed-size models, while saving a substantial portion of the original computation budget for training. We demonstrate that",
    "link": "http://arxiv.org/abs/2306.12700",
    "context": "Title: Accelerated Training via Incrementally Growing Neural Networks using Variance Transfer and Learning Rate Adaptation. (arXiv:2306.12700v1 [cs.LG])\nAbstract: We develop an approach to efficiently grow neural networks, within which parameterization and optimization strategies are designed by considering their effects on the training dynamics. Unlike existing growing methods, which follow simple replication heuristics or utilize auxiliary gradient-based local optimization, we craft a parameterization scheme which dynamically stabilizes weight, activation, and gradient scaling as the architecture evolves, and maintains the inference functionality of the network. To address the optimization difficulty resulting from imbalanced training effort distributed to subnetworks fading in at different growth phases, we propose a learning rate adaption mechanism that rebalances the gradient contribution of these separate subcomponents. Experimental results show that our method achieves comparable or better accuracy than training large fixed-size models, while saving a substantial portion of the original computation budget for training. We demonstrate that",
    "path": "papers/23/06/2306.12700.json",
    "total_tokens": 823,
    "translated_title": "利用方差传递和学习率调节通过递增成长神经网络的加速训练",
    "translated_abstract": "我们提出了一种高效增长神经网络的方法，其中考虑了参数化和优化策略对训练动态的影响。与现有的增长方法不同，该方法采用动态稳定化权重、激活和梯度缩放的参数化方案，并保持网络推理功能。为了解决由于不同生长阶段的子网络分布不均衡而导致的优化困难，我们提出了一种学习率适应机制，以重新平衡这些独立子组件的梯度贡献。实验结果表明，我们的方法在节约原始计算预算的同时，实现了与训练大型固定模型相当或更好的准确性。",
    "tldr": "本文提出一个新的神经网络增长方法，采用动态稳定化的参数化方案和学习率适应机制，节约计算预算的同时，在准确性上也取得了可比较或更好的表现。",
    "en_tdlr": "This paper proposes a new method for efficiently growing neural networks, with a parameterization scheme that dynamically stabilizes weight, activation, and gradient scaling, and a learning rate adaption mechanism that rebalances the gradient contribution of separate subcomponents. By doing so, it achieves comparable or better accuracy than training large fixed-size models while saving a substantial portion of the original computation budget."
}