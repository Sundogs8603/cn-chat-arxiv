{
    "title": "Towards Adaptable and Interactive Image Captioning with Data Augmentation and Episodic Memory. (arXiv:2306.03500v1 [cs.CL])",
    "abstract": "Interactive machine learning (IML) is a beneficial learning paradigm in cases of limited data availability, as human feedback is incrementally integrated into the training process. In this paper, we present an IML pipeline for image captioning which allows us to incrementally adapt a pre-trained image captioning model to a new data distribution based on user input. In order to incorporate user input into the model, we explore the use of a combination of simple data augmentation methods to obtain larger data batches for each newly annotated data instance and implement continual learning methods to prevent catastrophic forgetting from repeated updates. For our experiments, we split a domain-specific image captioning dataset, namely VizWiz, into non-overlapping parts to simulate an incremental input flow for continually adapting the model to new data. We find that, while data augmentation worsens results, even when relatively small amounts of data are available, episodic memory is an effe",
    "link": "http://arxiv.org/abs/2306.03500",
    "context": "Title: Towards Adaptable and Interactive Image Captioning with Data Augmentation and Episodic Memory. (arXiv:2306.03500v1 [cs.CL])\nAbstract: Interactive machine learning (IML) is a beneficial learning paradigm in cases of limited data availability, as human feedback is incrementally integrated into the training process. In this paper, we present an IML pipeline for image captioning which allows us to incrementally adapt a pre-trained image captioning model to a new data distribution based on user input. In order to incorporate user input into the model, we explore the use of a combination of simple data augmentation methods to obtain larger data batches for each newly annotated data instance and implement continual learning methods to prevent catastrophic forgetting from repeated updates. For our experiments, we split a domain-specific image captioning dataset, namely VizWiz, into non-overlapping parts to simulate an incremental input flow for continually adapting the model to new data. We find that, while data augmentation worsens results, even when relatively small amounts of data are available, episodic memory is an effe",
    "path": "papers/23/06/2306.03500.json",
    "total_tokens": 886,
    "translated_title": "基于数据增强与情节记忆的可适应交互式图像描述生成",
    "translated_abstract": "交互式机器学习可以在数据有限的情况下起到很好的学习效果，因为人类反馈可以逐步地融入到训练过程中。本文提出了一种基于交互式学习的图像描述生成模型，能够利用用户输入的信息来逐步微调预训练的模型以适应新的数据分布。我们探讨了如何利用简单的数据增强方法在每个新的输入样例上获取更多的数据并实现了持续学习方法来防止重复更新导致的灾难性遗忘。我们将一个特定领域的图像描述数据集 VizWiz 分成不重叠的部分，以模拟逐步输入流，不断适应新数据的过程。结果表明，虽然数据增强会导致结果变差，但即使只有相对较少的数据可用，情节记忆也是一种有效的方法。",
    "tldr": "本文提出一种基于交互式学习的图像描述生成方法，实现了通过数据增强与情节记忆来微调模型以适应新数据的目的，结果表明情节记忆是一种有效的方法。",
    "en_tdlr": "This paper presents an interactive machine learning pipeline for image captioning that incrementally adapts a pre-trained model to new data using user input. Data augmentation and episodic memory are used to achieve this goal, and experiments on the VizWiz dataset demonstrate that episodic memory is an effective method even with limited data."
}