{
    "title": "A Massive Scale Semantic Similarity Dataset of Historical English. (arXiv:2306.17810v1 [cs.CL])",
    "abstract": "A diversity of tasks use language models trained on semantic similarity data. While there are a variety of datasets that capture semantic similarity, they are either constructed from modern web data or are relatively small datasets created in the past decade by human annotators. This study utilizes a novel source, newly digitized articles from off-copyright, local U.S. newspapers, to assemble a massive-scale semantic similarity dataset spanning 70 years from 1920 to 1989 and containing nearly 400M positive semantic similarity pairs. Historically, around half of articles in U.S. local newspapers came from newswires like the Associated Press. While local papers reproduced articles from the newswire, they wrote their own headlines, which form abstractive summaries of the associated articles. We associate articles and their headlines by exploiting document layouts and language understanding. We then use deep neural methods to detect which articles are from the same underlying source, in th",
    "link": "http://arxiv.org/abs/2306.17810",
    "context": "Title: A Massive Scale Semantic Similarity Dataset of Historical English. (arXiv:2306.17810v1 [cs.CL])\nAbstract: A diversity of tasks use language models trained on semantic similarity data. While there are a variety of datasets that capture semantic similarity, they are either constructed from modern web data or are relatively small datasets created in the past decade by human annotators. This study utilizes a novel source, newly digitized articles from off-copyright, local U.S. newspapers, to assemble a massive-scale semantic similarity dataset spanning 70 years from 1920 to 1989 and containing nearly 400M positive semantic similarity pairs. Historically, around half of articles in U.S. local newspapers came from newswires like the Associated Press. While local papers reproduced articles from the newswire, they wrote their own headlines, which form abstractive summaries of the associated articles. We associate articles and their headlines by exploiting document layouts and language understanding. We then use deep neural methods to detect which articles are from the same underlying source, in th",
    "path": "papers/23/06/2306.17810.json",
    "total_tokens": 860,
    "translated_title": "一个历史英语的大规模语义相似性数据集",
    "translated_abstract": "各种任务使用在语义相似性数据上训练的语言模型。虽然有多种数据集可捕捉语义相似性，但它们要么是从现代网络数据构建的，要么是由人工标注员在过去十年中创建的相对较小的数据集。本研究利用一种新颖的来源，即重新数字化的无版权美国本地报纸文章，构建了一个大规模的语义相似性数据集，跨越了1920年到1989年的70年，并包含近4亿个正向语义相似性对。在美国本地报纸中，大约一半的文章来自新闻机构的新闻稿，而本地报纸复制了新闻稿的文章，并撰写了自己的标题，这些标题形成了与文章相关的提取性摘要。我们通过利用文档布局和语言理解将文章和标题关联起来。然后，我们使用深度神经方法来检测哪些文章来自相同的基础来源。",
    "tldr": "本研究利用重新数字化的无版权美国本地报纸文章，构建了一个大规模的跨越了70年的语义相似性数据集，并包含近4亿个正向语义相似性对。",
    "en_tdlr": "This study utilizes newly digitized off-copyright local U.S. newspaper articles to create a massive-scale semantic similarity dataset spanning 70 years, containing nearly 400 million positive semantic similarity pairs."
}