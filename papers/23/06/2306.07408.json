{
    "title": "Robust Reinforcement Learning through Efficient Adversarial Herding. (arXiv:2306.07408v1 [cs.LG])",
    "abstract": "Although reinforcement learning (RL) is considered the gold standard for policy design, it may not always provide a robust solution in various scenarios. This can result in severe performance degradation when the environment is exposed to potential disturbances. Adversarial training using a two-player max-min game has been proven effective in enhancing the robustness of RL agents. In this work, we extend the two-player game by introducing an adversarial herd, which involves a group of adversaries, in order to address ($\\textit{i}$) the difficulty of the inner optimization problem, and ($\\textit{ii}$) the potential over pessimism caused by the selection of a candidate adversary set that may include unlikely scenarios. We first prove that adversarial herds can efficiently approximate the inner optimization problem. Then we address the second issue by replacing the worst-case performance in the inner optimization with the average performance over the worst-$k$ adversaries. We evaluate the",
    "link": "http://arxiv.org/abs/2306.07408",
    "context": "Title: Robust Reinforcement Learning through Efficient Adversarial Herding. (arXiv:2306.07408v1 [cs.LG])\nAbstract: Although reinforcement learning (RL) is considered the gold standard for policy design, it may not always provide a robust solution in various scenarios. This can result in severe performance degradation when the environment is exposed to potential disturbances. Adversarial training using a two-player max-min game has been proven effective in enhancing the robustness of RL agents. In this work, we extend the two-player game by introducing an adversarial herd, which involves a group of adversaries, in order to address ($\\textit{i}$) the difficulty of the inner optimization problem, and ($\\textit{ii}$) the potential over pessimism caused by the selection of a candidate adversary set that may include unlikely scenarios. We first prove that adversarial herds can efficiently approximate the inner optimization problem. Then we address the second issue by replacing the worst-case performance in the inner optimization with the average performance over the worst-$k$ adversaries. We evaluate the",
    "path": "papers/23/06/2306.07408.json",
    "total_tokens": 1012,
    "translated_title": "通过高效的对抗性聚集实现强化学习的鲁棒性",
    "translated_abstract": "尽管强化学习在策略设计方面被认为是业界黄金标准，但在各种情况下，它并不总能提供健壮的解决方案。这可能导致环境暴露于潜在干扰时严重的性能下降。经过证明，使用双人博弈的对抗性训练可以有效提高RL代理的鲁棒性。在这项工作中，我们通过引入一个对抗性聚集来扩展双人博弈，并涉及一个对手群，以解决($\\textit{i}$)内部优化问题的困难和($\\textit{ii}$)由于候选对手集可能包含不太可能的情况而可能产生的过度悲观主义。我们首先证明了对抗性聚集可以有效地近似内部优化问题。然后，我们通过用最差的表现者中最差的k个表现的平均表现替换内部优化中的最坏情况表现来解决第二个问题。我们评估了我们的方法在多种环境下的性能，证明了对抗性聚集在与其他更先进的对抗性方法相比具有竞争力。",
    "tldr": "本研究提出了对抗性聚集方法，通过引入对手群来解决内部优化问题的困难，从而提高强化学习代理的鲁棒性，并通过对最差表现者中最坏的k个表现的平均表现来解决过度悲观主义问题。",
    "en_tdlr": "This work proposes an adversarial herding method that introduces a group of adversaries to address the difficulty of the inner optimization problem and improve the robustness of reinforcement learning agents. The method replaces the worst-case performance with the average performance over the worst-k adversaries to address the potential over-pessimism caused by the selection of a candidate adversary set that may include unlikely scenarios."
}