{
    "title": "Federated Linear Contextual Bandits with User-level Differential Privacy. (arXiv:2306.05275v1 [cs.LG])",
    "abstract": "This paper studies federated linear contextual bandits under the notion of user-level differential privacy (DP). We first introduce a unified federated bandits framework that can accommodate various definitions of DP in the sequential decision-making setting. We then formally introduce user-level central DP (CDP) and local DP (LDP) in the federated bandits framework, and investigate the fundamental trade-offs between the learning regrets and the corresponding DP guarantees in a federated linear contextual bandits model. For CDP, we propose a federated algorithm termed as \\robin and show that it is near-optimal in terms of the number of clients $M$ and the privacy budget $\\varepsilon$ by deriving nearly-matching upper and lower regret bounds when user-level DP is satisfied. For LDP, we obtain several lower bounds, indicating that learning under user-level $(\\varepsilon,\\delta)$-LDP must suffer a regret blow-up factor at least {$\\min\\{1/\\varepsilon,M\\}$ or $\\min\\{1/\\sqrt{\\varepsilon},\\sq",
    "link": "http://arxiv.org/abs/2306.05275",
    "context": "Title: Federated Linear Contextual Bandits with User-level Differential Privacy. (arXiv:2306.05275v1 [cs.LG])\nAbstract: This paper studies federated linear contextual bandits under the notion of user-level differential privacy (DP). We first introduce a unified federated bandits framework that can accommodate various definitions of DP in the sequential decision-making setting. We then formally introduce user-level central DP (CDP) and local DP (LDP) in the federated bandits framework, and investigate the fundamental trade-offs between the learning regrets and the corresponding DP guarantees in a federated linear contextual bandits model. For CDP, we propose a federated algorithm termed as \\robin and show that it is near-optimal in terms of the number of clients $M$ and the privacy budget $\\varepsilon$ by deriving nearly-matching upper and lower regret bounds when user-level DP is satisfied. For LDP, we obtain several lower bounds, indicating that learning under user-level $(\\varepsilon,\\delta)$-LDP must suffer a regret blow-up factor at least {$\\min\\{1/\\varepsilon,M\\}$ or $\\min\\{1/\\sqrt{\\varepsilon},\\sq",
    "path": "papers/23/06/2306.05275.json",
    "total_tokens": 1002,
    "translated_title": "带有用户级差分隐私的联邦线性上下文强化学习",
    "translated_abstract": "本文研究了在用户级差分隐私（DP）概念下的联邦线性上下文强化学习。我们首先介绍了一个统一的联邦强化学习框架，可以适应顺序决策设置中DP的各种定义。然后在联邦强化学习框架中正式引入了用户级中心DP和本地DP，并研究了联邦线性上下文强化学习模型中学习遗憾和相应DP保证之间的基本权衡。对于CDP，我们提出了一种称为\\robin的联邦算法，并通过推导在满足用户级DP时的几乎匹配的上界和下界遗憾界，证明其在客户端数量$M$和隐私预算$\\varepsilon$方面是近乎最优的。对于LDP，我们获得了几个下界，表明在用户级$(\\varepsilon,\\delta)$-LDP下学习必须至少承受一个遗憾膨胀因子至少为{$\\min\\{1/\\varepsilon,M\\}$或$\\min\\{1/\\sqrt{\\varepsilon},\\sq",
    "tldr": "本文研究了带有用户级差分隐私的联邦线性上下文强化学习模型，为CDP提出了近乎最优的联邦算法\\robin，在LDP下证明了学习必须承受至少一个遗憾膨胀因子。",
    "en_tdlr": "This paper studies federated linear contextual bandits under user-level differential privacy (DP), proposing a near-optimal federated algorithm named \\robin for central DP (CDP) and showing lower bounds for learning under user-level local DP (LDP), indicating a regret blow-up factor."
}