{
    "title": "Symmetric Exploration in Combinatorial Optimization is Free!. (arXiv:2306.01276v1 [cs.LG])",
    "abstract": "Recently, deep reinforcement learning (DRL) has shown promise in solving combinatorial optimization (CO) problems. However, they often require a large number of evaluations on the objective function, which can be time-consuming in real-world scenarios. To address this issue, we propose a \"free\" technique to enhance the performance of any deep reinforcement learning (DRL) solver by exploiting symmetry without requiring additional objective function evaluations. Our key idea is to augment the training of DRL-based combinatorial optimization solvers by reward-preserving transformations. The proposed algorithm is likely to be impactful since it is simple, easy to integrate with existing solvers, and applicable to a wide range of combinatorial optimization tasks. Extensive empirical evaluations on NP-hard routing optimization, scheduling optimization, and de novo molecular optimization confirm that our method effortlessly improves the sample efficiency of state-of-the-art DRL algorithms. Ou",
    "link": "http://arxiv.org/abs/2306.01276",
    "context": "Title: Symmetric Exploration in Combinatorial Optimization is Free!. (arXiv:2306.01276v1 [cs.LG])\nAbstract: Recently, deep reinforcement learning (DRL) has shown promise in solving combinatorial optimization (CO) problems. However, they often require a large number of evaluations on the objective function, which can be time-consuming in real-world scenarios. To address this issue, we propose a \"free\" technique to enhance the performance of any deep reinforcement learning (DRL) solver by exploiting symmetry without requiring additional objective function evaluations. Our key idea is to augment the training of DRL-based combinatorial optimization solvers by reward-preserving transformations. The proposed algorithm is likely to be impactful since it is simple, easy to integrate with existing solvers, and applicable to a wide range of combinatorial optimization tasks. Extensive empirical evaluations on NP-hard routing optimization, scheduling optimization, and de novo molecular optimization confirm that our method effortlessly improves the sample efficiency of state-of-the-art DRL algorithms. Ou",
    "path": "papers/23/06/2306.01276.json",
    "total_tokens": 898,
    "translated_title": "组合优化中对称探索是免费的！",
    "translated_abstract": "最近，深度强化学习（DRL）在解决组合优化（CO）问题方面已经显示出潜力。然而，他们经常需要大量的目标函数评估，这在现实场景中可能耗时。为了解决这个问题，我们提出了一种“免费”的技术，通过利用对称性来增强任何深度强化学习（DRL）求解器的性能，而无需额外的目标函数评估。我们的关键思想是通过保留奖励的变换来增强基于DRL的组合优化求解器的训练。该算法可能具有影响力，因为它简单，易于与现有求解器集成，并适用于广泛的组合优化任务。在NP难的路线优化，调度优化和新型分子优化的广泛实证评估结果表明，我们的方法轻松提高了最先进的DRL算法的样本效率。",
    "tldr": "该论文提出了一种免费的技术，通过利用对称性提高了基于DRL的组合优化求解器的性能，无需额外的目标函数评估，适用于广泛的组合优化任务，并在多种任务上进行实证评估证实了其有效性。",
    "en_tdlr": "This paper proposes a \"free\" technique to enhance the performance of any deep reinforcement learning (DRL) solver by exploiting symmetry without requiring additional objective function evaluations, which is impactful due to its simplicity and applicability to a wide range of combinatorial optimization tasks, as confirmed by extensive empirical evaluations on NP-hard routing optimization, scheduling optimization, and de novo molecular optimization."
}