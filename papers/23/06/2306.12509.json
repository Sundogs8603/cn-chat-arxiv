{
    "title": "Deep Language Networks: Joint Prompt Training of Stacked LLMs using Variational Inference. (arXiv:2306.12509v1 [cs.CL])",
    "abstract": "We view large language models (LLMs) as stochastic \\emph{language layers} in a network, where the learnable parameters are the natural language \\emph{prompts} at each layer. We stack two such layers, feeding the output of one layer to the next. We call the stacked architecture a \\emph{Deep Language Network} (DLN). We first show how to effectively perform prompt optimization for a 1-Layer language network (DLN-1). We then show how to train 2-layer DLNs (DLN-2), where two prompts must be learnt. We consider the output of the first layer as a latent variable to marginalize, and devise a variational inference algorithm for joint prompt training. A DLN-2 reaches higher performance than a single layer, sometimes comparable to few-shot GPT-4 even when each LLM in the network is smaller and less powerful. The DLN code is open source: https://github.com/microsoft/deep-language-networks .",
    "link": "http://arxiv.org/abs/2306.12509",
    "context": "Title: Deep Language Networks: Joint Prompt Training of Stacked LLMs using Variational Inference. (arXiv:2306.12509v1 [cs.CL])\nAbstract: We view large language models (LLMs) as stochastic \\emph{language layers} in a network, where the learnable parameters are the natural language \\emph{prompts} at each layer. We stack two such layers, feeding the output of one layer to the next. We call the stacked architecture a \\emph{Deep Language Network} (DLN). We first show how to effectively perform prompt optimization for a 1-Layer language network (DLN-1). We then show how to train 2-layer DLNs (DLN-2), where two prompts must be learnt. We consider the output of the first layer as a latent variable to marginalize, and devise a variational inference algorithm for joint prompt training. A DLN-2 reaches higher performance than a single layer, sometimes comparable to few-shot GPT-4 even when each LLM in the network is smaller and less powerful. The DLN code is open source: https://github.com/microsoft/deep-language-networks .",
    "path": "papers/23/06/2306.12509.json",
    "total_tokens": 948,
    "translated_title": "深度语言网络：使用变分推断联合训练叠加LLM的提示层",
    "translated_abstract": "我们将大型语言模型（LLMs）视为网络中的随机“语言层”，其中可学习的参数是每个层的自然语言“提示”。我们将两个这样的层叠加在一起，将一个层的输出馈送到下一个层。我们将这种堆叠的结构称为“深度语言网络”（DLN）。首先，我们展示如何有效地针对单层语言网络（DLN-1）执行提示优化。然后，我们展示如何训练2层DLNs（DLN-2），其中必须学习两个提示。我们认为第一层的输出是一个潜在变量，需要进行边缘化，并设计了一种联合提示训练的变分推断算法。DLN-2比单层达到更高的性能，有时即使网络中的每个LLM更小且更弱，也可以与少量训练数据的GPT-4相媲美。DLN代码是开源的：https://github.com/microsoft/deep-language-networks。",
    "tldr": "本文提出了一种称为深度语言网络（DLN）的架构，通过联合训练叠加的语言模型层（LLMs），使用变分推断算法进行提示训练，使得DLN-2的性能甚至可以与少量训练数据的GPT-4相媲美。",
    "en_tdlr": "The paper proposes an architecture called Deep Language Network (DLN) that stacks language model layers (LLMs) and trains them jointly using variational inference algorithm for prompt training. DLN-2 shows performance comparable to few-shot GPT-4 even when each LLM in the network is smaller and less powerful."
}