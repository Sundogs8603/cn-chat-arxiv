{
    "title": "The Unintended Consequences of Discount Regularization: Improving Regularization in Certainty Equivalence Reinforcement Learning. (arXiv:2306.11208v1 [cs.LG])",
    "abstract": "Discount regularization, using a shorter planning horizon when calculating the optimal policy, is a popular choice to restrict planning to a less complex set of policies when estimating an MDP from sparse or noisy data (Jiang et al., 2015). It is commonly understood that discount regularization functions by de-emphasizing or ignoring delayed effects. In this paper, we reveal an alternate view of discount regularization that exposes unintended consequences. We demonstrate that planning under a lower discount factor produces an identical optimal policy to planning using any prior on the transition matrix that has the same distribution for all states and actions. In fact, it functions like a prior with stronger regularization on state-action pairs with more transition data. This leads to poor performance when the transition matrix is estimated from data sets with uneven amounts of data across state-action pairs. Our equivalence theorem leads to an explicit formula to set regularization pa",
    "link": "http://arxiv.org/abs/2306.11208",
    "context": "Title: The Unintended Consequences of Discount Regularization: Improving Regularization in Certainty Equivalence Reinforcement Learning. (arXiv:2306.11208v1 [cs.LG])\nAbstract: Discount regularization, using a shorter planning horizon when calculating the optimal policy, is a popular choice to restrict planning to a less complex set of policies when estimating an MDP from sparse or noisy data (Jiang et al., 2015). It is commonly understood that discount regularization functions by de-emphasizing or ignoring delayed effects. In this paper, we reveal an alternate view of discount regularization that exposes unintended consequences. We demonstrate that planning under a lower discount factor produces an identical optimal policy to planning using any prior on the transition matrix that has the same distribution for all states and actions. In fact, it functions like a prior with stronger regularization on state-action pairs with more transition data. This leads to poor performance when the transition matrix is estimated from data sets with uneven amounts of data across state-action pairs. Our equivalence theorem leads to an explicit formula to set regularization pa",
    "path": "papers/23/06/2306.11208.json",
    "total_tokens": 869,
    "translated_title": "折扣正则化的非预期结果：改进确定等价强化学习中的正则化",
    "translated_abstract": "折扣正则化是一种常用的方法，用于从稀疏或嘈杂的数据中估计MDP时将规划限制为一个较简单的策略集。然而，在本文中我们揭示了折扣正则化的另一种视角，暴露出了非预期的结果。我们证明了，使用较低的折扣因子进行规划可以产生与使用具有相同分布的转移矩阵先验的任何规划得到的最优策略相同的结果。实际上，它的作用类似于对具有更多转移数据的状态-动作对进行更强的正则化。然而，当转移矩阵从数据集中估计时，对于状态-动作对之间数据量不均的情况，这将导致性能不佳。我们的等价定理导致了一个明确的公式来设置正则化。",
    "tldr": "折扣正则化不仅可以通过忽略延迟效应来缩小规划空间，还可以看作是具有先验分布的正则化，而低折扣因子的使用会导致状态-动作对之间数据量不均的性能下降。",
    "en_tdlr": "Discount regularization not only reduces the planning space by ignoring delayed effects, but also functions as regularization with a prior distribution. However, using a low discount factor leads to poor performance when there is uneven data across state-action pairs."
}