{
    "title": "On the Robustness of Removal-Based Feature Attributions. (arXiv:2306.07462v1 [cs.LG])",
    "abstract": "To explain complex models based on their inputs, many feature attribution methods have been developed that assign importance scores to input features. However, some recent work challenges the robustness of feature attributions by showing that these methods are sensitive to input and model perturbations, while other work addresses this robustness issue by proposing robust attribution methods and model modifications. Nevertheless, previous work on attribution robustness has focused primarily on gradient-based feature attributions. In contrast, the robustness properties of removal-based attribution methods are not comprehensively well understood. To bridge this gap, we theoretically characterize the robustness of removal-based feature attributions. Specifically, we provide a unified analysis of such methods and prove upper bounds for the difference between intact and perturbed attributions, under settings of both input and model perturbations. Our empirical experiments on synthetic and re",
    "link": "http://arxiv.org/abs/2306.07462",
    "context": "Title: On the Robustness of Removal-Based Feature Attributions. (arXiv:2306.07462v1 [cs.LG])\nAbstract: To explain complex models based on their inputs, many feature attribution methods have been developed that assign importance scores to input features. However, some recent work challenges the robustness of feature attributions by showing that these methods are sensitive to input and model perturbations, while other work addresses this robustness issue by proposing robust attribution methods and model modifications. Nevertheless, previous work on attribution robustness has focused primarily on gradient-based feature attributions. In contrast, the robustness properties of removal-based attribution methods are not comprehensively well understood. To bridge this gap, we theoretically characterize the robustness of removal-based feature attributions. Specifically, we provide a unified analysis of such methods and prove upper bounds for the difference between intact and perturbed attributions, under settings of both input and model perturbations. Our empirical experiments on synthetic and re",
    "path": "papers/23/06/2306.07462.json",
    "total_tokens": 884,
    "translated_title": "关于Removal-Based特征归因的鲁棒性研究",
    "translated_abstract": "为了解释基于输入的复杂模型，开发了许多特征归因方法来分配输入特征的重要性分数。然而，最近的一些研究挑战了特征归因的鲁棒性，指出这些方法对输入和模型扰动敏感，而其他研究通过提出鲁棒归因方法和模型修改来解决这个问题。然而，以往的归因鲁棒性研究主要侧重于基于梯度的特征归因。相比之下，Removal-Based归因方法的鲁棒性质尚未全面地得到理解。为了弥补这一差距，我们从理论上对Removal-Based特征归因的鲁棒性进行了全面的阐述。具体而言，我们对这种方法进行了统一的分析，并在输入和模型扰动的情况下证明了完好和受扰动的归因之间的差异的上限。我们在合成和真实数据集上的实验验证了我们的理论结果，并证明了所提出方法的实际有效性。",
    "tldr": "本文研究了Removal-Based特征归因的鲁棒性，提供了全面的理论和实验分析，并证明了所提方法的实际有效性。"
}