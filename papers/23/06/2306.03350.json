{
    "title": "Click: Controllable Text Generation with Sequence Likelihood Contrastive Learning. (arXiv:2306.03350v1 [cs.CL])",
    "abstract": "It has always been an important yet challenging problem to control language models to avoid generating texts with undesirable attributes, such as toxic language and unnatural repetition. We introduce Click for controllable text generation, which needs no modification to the model architecture and facilitates out-of-the-box use of trained models. It employs a contrastive loss on sequence likelihood, which fundamentally decreases the generation probability of negative samples (i.e., generations with undesirable attributes). It also adopts a novel likelihood ranking-based strategy to construct contrastive samples from model generations. On the tasks of language detoxification, sentiment steering, and repetition reduction, we show that Click outperforms strong baselines of controllable text generation and demonstrate the superiority of Click's sample construction strategy.",
    "link": "http://arxiv.org/abs/2306.03350",
    "context": "Title: Click: Controllable Text Generation with Sequence Likelihood Contrastive Learning. (arXiv:2306.03350v1 [cs.CL])\nAbstract: It has always been an important yet challenging problem to control language models to avoid generating texts with undesirable attributes, such as toxic language and unnatural repetition. We introduce Click for controllable text generation, which needs no modification to the model architecture and facilitates out-of-the-box use of trained models. It employs a contrastive loss on sequence likelihood, which fundamentally decreases the generation probability of negative samples (i.e., generations with undesirable attributes). It also adopts a novel likelihood ranking-based strategy to construct contrastive samples from model generations. On the tasks of language detoxification, sentiment steering, and repetition reduction, we show that Click outperforms strong baselines of controllable text generation and demonstrate the superiority of Click's sample construction strategy.",
    "path": "papers/23/06/2306.03350.json",
    "total_tokens": 859,
    "translated_title": "Click: 序列似然对比学习控制文本生成",
    "translated_abstract": "控制语言模型避免生成带有不良属性的文本一直是个重要而具有挑战性的问题，比如有害的语言和不自然的重复。我们引入了 Click 来进行可控文本生成，无需修改模型架构，并且便于使用已经训练好的模型。它通过采用序列似然对比损失来根本减少负样本的生成概率（即具有不良属性的生成结果）。同时采用了一种新颖的似然排名策略来构建对比样本。在语言去毒、情感调整和减少重复的任务中，我们证明了 Click 優於强基线的可控文字生成，并展示了 Click 的样本构造策略的优越性。",
    "tldr": "Click是一种无需修改模型架构的可控文本生成方法，它采用序列似然对比损失来根本减少不良属性的生成概率，同时采用一种新颖的样本构造策略来构建对比样本。在相关任务中，Click表现出了优异的性能，且样本构造策略相较于其他方法更加优秀。",
    "en_tdlr": "Click is an approach for controllable text generation that doesn't require modification of the model architecture and decreases the likelihood of generating texts with undesirable attributes through contrastive loss on sequence likelihood. Click also uses a novel likelihood ranking-based strategy to construct contrastive samples. It outperforms strong baselines for controllable text generation and demonstrates the superiority of its sample construction strategy in tasks such as language detoxification, sentiment steering, and repetition reduction."
}