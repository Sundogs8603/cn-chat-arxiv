{
    "title": "Instruct-NeuralTalker: Editing Audio-Driven Talking Radiance Fields with Instructions. (arXiv:2306.10813v2 [cs.CV] UPDATED)",
    "abstract": "Recent neural talking radiance field methods have shown great success in photorealistic audio-driven talking face synthesis. In this paper, we propose a novel interactive framework that utilizes human instructions to edit such implicit neural representations to achieve real-time personalized talking face generation. Given a short speech video, we first build an efficient talking radiance field, and then apply the latest conditional diffusion model for image editing based on the given instructions and guiding implicit representation optimization towards the editing target. To ensure audio-lip synchronization during the editing process, we propose an iterative dataset updating strategy and utilize a lip-edge loss to constrain changes in the lip region. We also introduce a lightweight refinement network for complementing image details and achieving controllable detail generation in the final rendered image. Our method also enables real-time rendering at up to 30FPS on consumer hardware. M",
    "link": "http://arxiv.org/abs/2306.10813",
    "context": "Title: Instruct-NeuralTalker: Editing Audio-Driven Talking Radiance Fields with Instructions. (arXiv:2306.10813v2 [cs.CV] UPDATED)\nAbstract: Recent neural talking radiance field methods have shown great success in photorealistic audio-driven talking face synthesis. In this paper, we propose a novel interactive framework that utilizes human instructions to edit such implicit neural representations to achieve real-time personalized talking face generation. Given a short speech video, we first build an efficient talking radiance field, and then apply the latest conditional diffusion model for image editing based on the given instructions and guiding implicit representation optimization towards the editing target. To ensure audio-lip synchronization during the editing process, we propose an iterative dataset updating strategy and utilize a lip-edge loss to constrain changes in the lip region. We also introduce a lightweight refinement network for complementing image details and achieving controllable detail generation in the final rendered image. Our method also enables real-time rendering at up to 30FPS on consumer hardware. M",
    "path": "papers/23/06/2306.10813.json",
    "total_tokens": 970,
    "translated_title": "Instruct-NeuralTalker: 利用指令编辑音频驱动的对话辐射场",
    "translated_abstract": "最近的神经对话辐射场方法在逼真的音频驱动的对话面部合成方面取得了巨大成功。在本文中，我们提出了一种新颖的交互式框架，利用人类指令来编辑这种隐式神经表示，实现实时个性化的对话面部生成。给定一个短视频，我们首先构建一个高效的对话辐射场，然后根据给定的指令和引导的隐式表示优化，应用最新的条件扩散模型进行图像编辑，以实现编辑目标。为了确保编辑过程中的音频唇同步，我们提出了一种迭代数据集更新策略，并利用唇沿损失约束唇部区域的变化。我们还引入了一个轻量级的细化网络，用于补充图像细节，并在最终渲染图像中实现可控的细节生成。我们的方法还能在消费级硬件上实现最高30FPS的实时渲染。",
    "tldr": "本文提出了Instruct-NeuralTalker，一种利用指令编辑音频驱动的对话辐射场的交互式框架。该方法可以实现实时个性化的对话面部生成，并在编辑过程中保持音频唇同步。此外，还引入了轻量级的细化网络来实现可控的细节生成，并且在消费级硬件上可以达到最高30FPS的实时渲染。"
}