{
    "title": "On the Validation of Gibbs Algorithms: Training Datasets, Test Datasets and their Aggregation. (arXiv:2306.12380v1 [cs.LG])",
    "abstract": "The dependence on training data of the Gibbs algorithm (GA) is analytically characterized. By adopting the expected empirical risk as the performance metric, the sensitivity of the GA is obtained in closed form. In this case, sensitivity is the performance difference with respect to an arbitrary alternative algorithm. This description enables the development of explicit expressions involving the training errors and test errors of GAs trained with different datasets. Using these tools, dataset aggregation is studied and different figures of merit to evaluate the generalization capabilities of GAs are introduced. For particular sizes of such datasets and parameters of the GAs, a connection between Jeffrey's divergence, training and test errors is established.",
    "link": "http://arxiv.org/abs/2306.12380",
    "context": "Title: On the Validation of Gibbs Algorithms: Training Datasets, Test Datasets and their Aggregation. (arXiv:2306.12380v1 [cs.LG])\nAbstract: The dependence on training data of the Gibbs algorithm (GA) is analytically characterized. By adopting the expected empirical risk as the performance metric, the sensitivity of the GA is obtained in closed form. In this case, sensitivity is the performance difference with respect to an arbitrary alternative algorithm. This description enables the development of explicit expressions involving the training errors and test errors of GAs trained with different datasets. Using these tools, dataset aggregation is studied and different figures of merit to evaluate the generalization capabilities of GAs are introduced. For particular sizes of such datasets and parameters of the GAs, a connection between Jeffrey's divergence, training and test errors is established.",
    "path": "papers/23/06/2306.12380.json",
    "total_tokens": 722,
    "translated_title": "关于 Gibbs 算法的验证：训练数据集、测试数据集及其聚合",
    "translated_abstract": "本论文分析了 Gibbs 算法（GA）对训练数据的相关性，采用期望经验风险作为性能指标，并获得了 GA 在封闭形式下的灵敏度。通过运用这个描述，可以生成与不同训练集训练的 GA 的测试误差和训练误差相关的显式表达式。利用这些工具，探讨了数据集的聚合，并介绍了用于评估GA泛化能力的不同性能指标。对于这些数据集的特定大小和GA的参数，建立了 Jeffrey's divergence、训练误差和测试误差之间的关联。",
    "tldr": "本论文分析了 Gibbs 算法对训练数据的相关性，并介绍了用于评估 GA 泛化能力的不同性能指标，以及训练误差和测试误差之间的关联。",
    "en_tdlr": "This paper analyzes the correlation between the Gibbs algorithm and training data, introduces different performance metrics to evaluate the GA's generalization capabilities, and establishes a connection between training and test errors and Jeffrey’s divergence."
}