{
    "title": "Combining Primal and Dual Representations in Deep Restricted Kernel Machines Classifiers. (arXiv:2306.07015v2 [cs.LG] UPDATED)",
    "abstract": "In the context of deep learning with kernel machines, the deep Restricted Kernel Machine (DRKM) framework allows multiple levels of kernel PCA (KPCA) and Least-Squares Support Vector Machines (LSSVM) to be combined into a deep architecture using visible and hidden units. We propose a new method for DRKM classification coupling the objectives of KPCA and classification levels, with the hidden feature matrix lying on the Stiefel manifold. The classification level can be formulated as an LSSVM or as an MLP feature map, combining depth in terms of levels and layers. The classification level is expressed in its primal formulation, as the deep KPCA levels, in their dual formulation, can embed the most informative components of the data in a much lower dimensional space. The dual setting is independent of the dimension of the inputs and the primal setting is parametric, which makes the proposed method computationally efficient for both high-dimensional inputs and large datasets. In the experi",
    "link": "http://arxiv.org/abs/2306.07015",
    "context": "Title: Combining Primal and Dual Representations in Deep Restricted Kernel Machines Classifiers. (arXiv:2306.07015v2 [cs.LG] UPDATED)\nAbstract: In the context of deep learning with kernel machines, the deep Restricted Kernel Machine (DRKM) framework allows multiple levels of kernel PCA (KPCA) and Least-Squares Support Vector Machines (LSSVM) to be combined into a deep architecture using visible and hidden units. We propose a new method for DRKM classification coupling the objectives of KPCA and classification levels, with the hidden feature matrix lying on the Stiefel manifold. The classification level can be formulated as an LSSVM or as an MLP feature map, combining depth in terms of levels and layers. The classification level is expressed in its primal formulation, as the deep KPCA levels, in their dual formulation, can embed the most informative components of the data in a much lower dimensional space. The dual setting is independent of the dimension of the inputs and the primal setting is parametric, which makes the proposed method computationally efficient for both high-dimensional inputs and large datasets. In the experi",
    "path": "papers/23/06/2306.07015.json",
    "total_tokens": 907,
    "translated_title": "在深度受限核机器分类器中结合原始和对偶表示",
    "translated_abstract": "在深度学习与核机器的背景下，深度受限核机器（DRKM）框架允许将多个级别的核主成分分析（KPCA）和最小二乘支持向量机（LSSVM）结合为一个使用可见单元和隐藏单元的深度架构。我们提出了一种新的DRKM分类方法，将KPCA和分类级别的目标相结合，隐藏特征矩阵位于Stiefel流形上。分类级别可以表示为LSSVM或MLP特征图，结合级别和层数的深度。分类级别在其原始形式中表达，而KPCA的深度级别在其对偶形式中可以将数据的最信息化组分嵌入到一个更低维的空间中。对偶设置独立于输入的维度，原始设置是参数化的，这使得所提出的方法在高维输入和大数据集上都具有计算效率。",
    "tldr": "该论文提出了一种在深度学习和核机器分类器中结合原始和对偶表示的新方法，通过将可见单元和隐藏单元的核主成分分析和最小二乘支持向量机相结合，实现了多个级别的深度架构。该方法在计算效率上具有优势，适用于高维输入和大数据集。",
    "en_tdlr": "This paper proposes a new method that combines primal and dual representations in deep learning with kernel machines, allowing for multiple levels of deep architecture using visible and hidden units. The method is computationally efficient and suitable for high-dimensional inputs and large datasets."
}