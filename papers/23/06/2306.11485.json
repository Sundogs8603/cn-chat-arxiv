{
    "title": "Explicit Syntactic Guidance for Neural Text Generation. (arXiv:2306.11485v2 [cs.CL] UPDATED)",
    "abstract": "Most existing text generation models follow the sequence-to-sequence paradigm. Generative Grammar suggests that humans generate natural language texts by learning language grammar. We propose a syntax-guided generation schema, which generates the sequence guided by a constituency parse tree in a top-down direction. The decoding process can be decomposed into two parts: (1) predicting the infilling texts for each constituent in the lexicalized syntax context given the source sentence; (2) mapping and expanding each constituent to construct the next-level syntax context. Accordingly, we propose a structural beam search method to find possible syntax structures hierarchically. Experiments on paraphrase generation and machine translation show that the proposed method outperforms autoregressive baselines, while also demonstrating effectiveness in terms of interpretability, controllability, and diversity.",
    "link": "http://arxiv.org/abs/2306.11485",
    "context": "Title: Explicit Syntactic Guidance for Neural Text Generation. (arXiv:2306.11485v2 [cs.CL] UPDATED)\nAbstract: Most existing text generation models follow the sequence-to-sequence paradigm. Generative Grammar suggests that humans generate natural language texts by learning language grammar. We propose a syntax-guided generation schema, which generates the sequence guided by a constituency parse tree in a top-down direction. The decoding process can be decomposed into two parts: (1) predicting the infilling texts for each constituent in the lexicalized syntax context given the source sentence; (2) mapping and expanding each constituent to construct the next-level syntax context. Accordingly, we propose a structural beam search method to find possible syntax structures hierarchically. Experiments on paraphrase generation and machine translation show that the proposed method outperforms autoregressive baselines, while also demonstrating effectiveness in terms of interpretability, controllability, and diversity.",
    "path": "papers/23/06/2306.11485.json",
    "total_tokens": 840,
    "translated_title": "显式句法引导神经文本生成",
    "translated_abstract": "大多数现有文本生成模型都遵循序列对序列范例。生成语法表明人类通过学习语言语法来生成自然语言文本。我们提出了一种句法引导的生成模式，它通过自上而下的形式结构树生成序列。解码过程可以分解为两个部分：（1）在源句子的词汇化句法上下文中为每个成分预测填充文本；（2）映射和扩展每个成分以构建下一级句法上下文。因此，我们提出了一种结构束搜索方法，用于分层地查找可能的句法结构。对于释义生成和机器翻译的实验结果表明，所提出的方法优于自回归基线，同时还表现出可解释性、可控制性和多样性的有效性。",
    "tldr": "本研究提出了一种基于句法引导的文本生成模式，通过自上而下的形式结构树生成序列，在自回归基线中表现优异，具有可解释性、可控制性和多样性的优点。",
    "en_tdlr": "This paper proposes a syntax-guided generation schema for neural text generation, which outperforms autoregressive baselines in terms of interpretability, controllability, and diversity. It generates sequences by a top-down constituency parse tree and consists of predicting infilling texts for each constituent and mapping and expanding each constituent to construct the next-level syntax context. Experiments on paraphrase generation and machine translation demonstrate its effectiveness."
}