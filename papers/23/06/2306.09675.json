{
    "title": "Multi-View Class Incremental Learning. (arXiv:2306.09675v1 [cs.LG])",
    "abstract": "Multi-view learning (MVL) has gained great success in integrating information from multiple perspectives of a dataset to improve downstream task performance. To make MVL methods more practical in an open-ended environment, this paper investigates a novel paradigm called multi-view class incremental learning (MVCIL), where a single model incrementally classifies new classes from a continual stream of views, requiring no access to earlier views of data. However, MVCIL is challenged by the catastrophic forgetting of old information and the interference with learning new concepts. To address this, we first develop a randomization-based representation learning technique serving for feature extraction to guarantee their separate view-optimal working states, during which multiple views belonging to a class are presented sequentially; Then, we integrate them one by one in the orthogonality fusion subspace spanned by the extracted features; Finally, we introduce selective weight consolidation f",
    "link": "http://arxiv.org/abs/2306.09675",
    "context": "Title: Multi-View Class Incremental Learning. (arXiv:2306.09675v1 [cs.LG])\nAbstract: Multi-view learning (MVL) has gained great success in integrating information from multiple perspectives of a dataset to improve downstream task performance. To make MVL methods more practical in an open-ended environment, this paper investigates a novel paradigm called multi-view class incremental learning (MVCIL), where a single model incrementally classifies new classes from a continual stream of views, requiring no access to earlier views of data. However, MVCIL is challenged by the catastrophic forgetting of old information and the interference with learning new concepts. To address this, we first develop a randomization-based representation learning technique serving for feature extraction to guarantee their separate view-optimal working states, during which multiple views belonging to a class are presented sequentially; Then, we integrate them one by one in the orthogonality fusion subspace spanned by the extracted features; Finally, we introduce selective weight consolidation f",
    "path": "papers/23/06/2306.09675.json",
    "total_tokens": 997,
    "translated_title": "多视角分类增量学习",
    "translated_abstract": "多视角学习（MVL）在整合数据集的多个视角以提高下游任务性能方面取得了巨大成功。为了使MVL方法在开放式环境中更实用，本文研究了一种新的范例，称为多视角分类增量学习（MVCIL），其中单个模型从连续的视图流中逐步分类新类，不需要访问早期数据的视图。但是，MVCIL面临着老信息的灾难性遗忘和学习新概念的干扰。为了解决这个问题，我们首先开发了一种基于随机化的表示学习技术，用于特征提取，以保证它们在工作状态下的分离视图最优，其中属于类的多个视图按顺序呈现；然后，我们将它们逐个集成到由提取的特征跨越的正交融合子空间中；最后，我们介绍选择性权重合并，以保留旧类的知识。基准数据集上的实验结果证明了我们提出的方法相对于最新方法的有效性。",
    "tldr": "本文提出了一种名为多视角分类增量学习（MVCIL）的新模型，该模型使用随机化的表示学习技术进行特征提取，并提出正交融合子空间和选择性权重合并来解决增量学习中遗忘旧信息和学习新概念的挑战。实验结果表明该方法相比最新方法有效性更高。",
    "en_tdlr": "This paper proposes a new model called Multi-View Class Incremental Learning (MVCIL), which uses randomization-based representation learning for feature extraction and introduces an orthogonality fusion subspace and selective weight consolidation to address the challenge of forgetting old information and learning new concepts in incremental learning. Experimental results show that the proposed method is more effective than state-of-the-art approaches."
}