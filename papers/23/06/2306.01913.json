{
    "title": "PDT: Pretrained Dual Transformers for Time-aware Bipartite Graphs. (arXiv:2306.01913v2 [cs.AI] UPDATED)",
    "abstract": "Pre-training on large models is prevalent and emerging with the ever-growing user-generated content in many machine learning application categories. It has been recognized that learning contextual knowledge from the datasets depicting user-content interaction plays a vital role in downstream tasks. Despite several studies attempting to learn contextual knowledge via pre-training methods, finding an optimal training objective and strategy for this type of task remains a challenging problem. In this work, we contend that there are two distinct aspects of contextual knowledge, namely the user-side and the content-side, for datasets where user-content interaction can be represented as a bipartite graph. To learn contextual knowledge, we propose a pre-training method that learns a bi-directional mapping between the spaces of the user-side and the content-side. We formulate the training goal as a contrastive learning task and propose a dual-Transformer architecture to encode the contextual k",
    "link": "http://arxiv.org/abs/2306.01913",
    "context": "Title: PDT: Pretrained Dual Transformers for Time-aware Bipartite Graphs. (arXiv:2306.01913v2 [cs.AI] UPDATED)\nAbstract: Pre-training on large models is prevalent and emerging with the ever-growing user-generated content in many machine learning application categories. It has been recognized that learning contextual knowledge from the datasets depicting user-content interaction plays a vital role in downstream tasks. Despite several studies attempting to learn contextual knowledge via pre-training methods, finding an optimal training objective and strategy for this type of task remains a challenging problem. In this work, we contend that there are two distinct aspects of contextual knowledge, namely the user-side and the content-side, for datasets where user-content interaction can be represented as a bipartite graph. To learn contextual knowledge, we propose a pre-training method that learns a bi-directional mapping between the spaces of the user-side and the content-side. We formulate the training goal as a contrastive learning task and propose a dual-Transformer architecture to encode the contextual k",
    "path": "papers/23/06/2306.01913.json",
    "total_tokens": 838,
    "translated_title": "PDT: 面向时间感知的双向预训练变压器模型用于二分图",
    "translated_abstract": "在许多机器学习应用中，预先训练大型模型正在普及并涌现，随着不断增长的用户生成内容。已经认识到，从描绘用户内容交互的数据集中学习上下文知识对下游任务至关重要。尽管有几项研究尝试通过预训练方法学习上下文知识，但为这种任务找到最佳的训练目标和策略仍然是一个具有挑战性的问题。在这项工作中，我们认为在表示用户-内容交互的数据集中，有两个不同的上下文知识方面，即用户方面和内容方面。为了学习上下文知识，我们提出了一种预训练方法，该方法学习用户方面和内容方面之间的双向映射，我们将训练目标制定为对比学习任务，并提出了双重Transformer架构来编码上下文知识。",
    "tldr": "该论文提出了一种预训练模型，用于学习二分图中用户和内容之间的上下文知识，并采用对比学习任务以提高性能。"
}