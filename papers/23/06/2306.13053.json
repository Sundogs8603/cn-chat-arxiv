{
    "title": "Context-lumpable stochastic bandits. (arXiv:2306.13053v1 [cs.LG])",
    "abstract": "We consider a contextual bandit problem with $S $ contexts and $A $ actions. In each round $t=1,2,\\dots$ the learner observes a random context and chooses an action based on its past experience. The learner then observes a random reward whose mean is a function of the context and the action for the round. Under the assumption that the contexts can be lumped into $r\\le \\min\\{S ,A \\}$ groups such that the mean reward for the various actions is the same for any two contexts that are in the same group, we give an algorithm that outputs an $\\epsilon$-optimal policy after using at most $\\widetilde O(r (S +A )/\\epsilon^2)$ samples with high probability and provide a matching $\\widetilde\\Omega(r (S +A )/\\epsilon^2)$ lower bound. In the regret minimization setting, we give an algorithm whose cumulative regret up to time $T$ is bounded by $\\widetilde O(\\sqrt{r^3(S +A )T})$. To the best of our knowledge, we are the first to show the near-optimal sample complexity in the PAC setting and $\\widetild",
    "link": "http://arxiv.org/abs/2306.13053",
    "context": "Title: Context-lumpable stochastic bandits. (arXiv:2306.13053v1 [cs.LG])\nAbstract: We consider a contextual bandit problem with $S $ contexts and $A $ actions. In each round $t=1,2,\\dots$ the learner observes a random context and chooses an action based on its past experience. The learner then observes a random reward whose mean is a function of the context and the action for the round. Under the assumption that the contexts can be lumped into $r\\le \\min\\{S ,A \\}$ groups such that the mean reward for the various actions is the same for any two contexts that are in the same group, we give an algorithm that outputs an $\\epsilon$-optimal policy after using at most $\\widetilde O(r (S +A )/\\epsilon^2)$ samples with high probability and provide a matching $\\widetilde\\Omega(r (S +A )/\\epsilon^2)$ lower bound. In the regret minimization setting, we give an algorithm whose cumulative regret up to time $T$ is bounded by $\\widetilde O(\\sqrt{r^3(S +A )T})$. To the best of our knowledge, we are the first to show the near-optimal sample complexity in the PAC setting and $\\widetild",
    "path": "papers/23/06/2306.13053.json",
    "total_tokens": 1074,
    "translated_title": "可分组的随机赌徒问题",
    "translated_abstract": "本文研究了具有 $S$ 个上下文和 $A$ 种行动的情境赌徒问题。在每一轮 $t=1,2,\\dots$ 中，学习者观察一个随机上下文，并根据其以往的经验选择一个行动。然后，学习者观察一个随机奖励，其平均值是该轮上下文和行动的一个函数。在假设上下文可以分为 $r\\leq \\min\\{S,A\\}$ 组，使得任意两个在同一组内的上下文的各种行动的平均奖励相同的情况下，我们设计了一个算法，它在使用 $\\widetilde O(r(S + A)/\\epsilon^2)$ 个样本后可以生成一个 $\\epsilon$-最优策略，并且具有较高的置信度提供了匹配的 $\\widetilde\\Omega(r (S + A )/\\epsilon^2)$ 下限。在遗憾最小化设置下，我们提供了一个算法，其累积遗憾在时间 $T$ 内有界，即 $\\widetilde O(\\sqrt{r^3(S +A)T})$。据我们所知，我们是第一个展示在可近似正确设置中接近最优样本复杂度和在遗憾最小化设置下的 $\\widetilde O(\\sqrt{r^3(S +A)T})$ 累积遗憾。",
    "tldr": "本文研究了具有 $S$ 个上下文和 $A$ 种行动的情境赌徒问题，并提出了一种可分组的随机赌徒问题算法。",
    "en_tdlr": "This paper studies contextual bandit problem with $S$ contexts and $A$ actions, proposes a context-lumpable stochastic bandits algorithm and achieves near-optimal sample complexity in the PAC setting and cumulative regret of $\\widetilde O(\\sqrt{r^3(S +A)T})$ in the regret minimization setting."
}