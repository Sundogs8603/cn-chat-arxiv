{
    "title": "Exploring Anisotropy and Outliers in Multilingual Language Models for Cross-Lingual Semantic Sentence Similarity. (arXiv:2306.00458v1 [cs.CL])",
    "abstract": "Previous work has shown that the representations output by contextual language models are more anisotropic than static type embeddings, and typically display outlier dimensions. This seems to be true for both monolingual and multilingual models, although much less work has been done on the multilingual context. Why these outliers occur and how they affect the representations is still an active area of research. We investigate outlier dimensions and their relationship to anisotropy in multiple pre-trained multilingual language models. We focus on cross-lingual semantic similarity tasks, as these are natural tasks for evaluating multilingual representations. Specifically, we examine sentence representations. Sentence transformers which are fine-tuned on parallel resources (that are not always available) perform better on this task, and we show that their representations are more isotropic. However, we aim to improve multilingual representations in general. We investigate how much of the ",
    "link": "http://arxiv.org/abs/2306.00458",
    "context": "Title: Exploring Anisotropy and Outliers in Multilingual Language Models for Cross-Lingual Semantic Sentence Similarity. (arXiv:2306.00458v1 [cs.CL])\nAbstract: Previous work has shown that the representations output by contextual language models are more anisotropic than static type embeddings, and typically display outlier dimensions. This seems to be true for both monolingual and multilingual models, although much less work has been done on the multilingual context. Why these outliers occur and how they affect the representations is still an active area of research. We investigate outlier dimensions and their relationship to anisotropy in multiple pre-trained multilingual language models. We focus on cross-lingual semantic similarity tasks, as these are natural tasks for evaluating multilingual representations. Specifically, we examine sentence representations. Sentence transformers which are fine-tuned on parallel resources (that are not always available) perform better on this task, and we show that their representations are more isotropic. However, we aim to improve multilingual representations in general. We investigate how much of the ",
    "path": "papers/23/06/2306.00458.json",
    "total_tokens": 1129,
    "translated_title": "探究多语言语言模型中的各向异性和异常值，针对跨语言语义句子相似度进行研究",
    "translated_abstract": "先前的研究表明，上下文语言模型输出的表示比静态类型嵌入更具各向异性，并且通常显示异常值维度。虽然对于单语和多语模型都是如此，但在多语言情境下的研究还远不够。为什么会出现这些异常值并且它们如何影响表示仍是研究的活跃领域。我们调查多个预训练的多语言语言模型中异常值维度及其与各向异性之间的关系。我们专注于跨语言语义相似性任务，因为这些是评估多语言表示自然的任务。具体来说，我们研究句子表示。在平行资源上进行微调的句子转换器在此任务上表现更好，我们展示它们的表示更各向同性。然而，我们的目标是总体改善多语言表示。我们调查通过在跨语言目标上训练可减少多少各向异性和异常值，以及这样做如何影响跨语言性能。我们的结果表明，在跨语言相似性目标上进行训练可以改善表示的各向同性，但不会统一减少异常值。我们还发现不同语言之间存在差异，其中一些语言显示出比其他语言更大的异常值维度。",
    "tldr": "该论文研究了多语言语言模型中各向异性和异常值维度，针对跨语言语义相似性任务进行了实验研究。发现跨语言相似性目标上的训练可以改善表示的各向同性，但不会统一减少异常值。",
    "en_tdlr": "This paper investigates anisotropy and outlier dimensions in multilingual language models and their relationship to cross-lingual semantic sentence similarity. Results show that training on cross-lingual similarity objectives can improve isotropy of representations, but not uniformly reduce outliers. Differences were also found across languages."
}