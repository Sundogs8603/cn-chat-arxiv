{
    "title": "Integrating Geometric Control into Text-to-Image Diffusion Models for High-Quality Detection Data Generation via Text Prompt. (arXiv:2306.04607v1 [cs.CV])",
    "abstract": "Diffusion models have attracted significant attention due to their remarkable ability to create content and generate data for tasks such as image classification. However, the usage of diffusion models to generate high-quality object detection data remains an underexplored area, where not only the image-level perceptual quality but also geometric conditions such as bounding boxes and camera views are essential. Previous studies have utilized either copy-paste synthesis or layout-to-image (L2I) generation with specifically designed modules to encode semantic layouts. In this paper, we propose GeoDiffusion, a simple framework that can flexibly translate various geometric conditions into text prompts and empower the pre-trained text-to-image (T2I) diffusion models for high-quality detection data generation. Unlike previous L2I methods, our GeoDiffusion is able to encode not only bounding boxes but also extra geometric conditions such as camera views in self-driving scenes. Extensive experi",
    "link": "http://arxiv.org/abs/2306.04607",
    "context": "Title: Integrating Geometric Control into Text-to-Image Diffusion Models for High-Quality Detection Data Generation via Text Prompt. (arXiv:2306.04607v1 [cs.CV])\nAbstract: Diffusion models have attracted significant attention due to their remarkable ability to create content and generate data for tasks such as image classification. However, the usage of diffusion models to generate high-quality object detection data remains an underexplored area, where not only the image-level perceptual quality but also geometric conditions such as bounding boxes and camera views are essential. Previous studies have utilized either copy-paste synthesis or layout-to-image (L2I) generation with specifically designed modules to encode semantic layouts. In this paper, we propose GeoDiffusion, a simple framework that can flexibly translate various geometric conditions into text prompts and empower the pre-trained text-to-image (T2I) diffusion models for high-quality detection data generation. Unlike previous L2I methods, our GeoDiffusion is able to encode not only bounding boxes but also extra geometric conditions such as camera views in self-driving scenes. Extensive experi",
    "path": "papers/23/06/2306.04607.json",
    "total_tokens": 939,
    "translated_title": "将几何控制集成到文本到图像扩散模型中以通过文本提示生成高质量的检测数据",
    "translated_abstract": "扩散模型因其在创建内容和生成数据方面的显着能力而受到重视，例如图像分类。然而，使用扩散模型生成高质量的物体检测数据仍然是一个不被充分探索的领域，其中不仅图像水平的感知质量，而且边界框和相机视图等几何条件也是至关重要的。前期研究使用模块编码语义布局来实现复制粘贴合成或布局到图像(L2I)生成。本文提出了GeoDiffusion，一种简单的框架，可以灵活地将各种几何条件转化为文本提示，并使用预训练的文本到图像(T2I)扩散模型生成高质量的检测数据。与以往的L2I方法不同，我们的GeoDiffusion不仅能够编码边界框，还能够编码自驾场景中的额外几何条件，如摄像头视图。广泛的实验结果表明，GeoDiffusion在物体检测准确性方面优于最先进的方法，并针对各种几何条件生成具有更高感知质量的图像。",
    "tldr": "GeoDiffusion使用文本提示将各种几何条件转化为图像，生成高质量的检测数据，性能优于现有方法。",
    "en_tdlr": "GeoDiffusion integrates geometric control into text-to-image diffusion models and generates high-quality detection data via text prompts, outperforming existing methods."
}