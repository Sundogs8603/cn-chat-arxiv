{
    "title": "Augmenting Hessians with Inter-Layer Dependencies for Mixed-Precision Post-Training Quantization. (arXiv:2306.04879v1 [cs.LG])",
    "abstract": "Efficiently serving neural network models with low latency is becoming more challenging due to increasing model complexity and parameter count. Model quantization offers a solution which simultaneously reduces memory footprint and compute requirements. However, aggressive quantization may lead to an unacceptable loss in model accuracy owing to differences in sensitivity to numerical imperfection across different layers in the model. To address this challenge, we propose a mixed-precision post training quantization (PTQ) approach that assigns different numerical precisions to tensors in a network based on their specific needs, for a reduced memory footprint and improved latency while preserving model accuracy. Previous works rely on layer-wise Hessian information to determine numerical precision, but as we demonstrate, Hessian estimation is typically insufficient in determining an effective ordering of layer sensitivities. We address this by augmenting the estimated Hessian with additio",
    "link": "http://arxiv.org/abs/2306.04879",
    "context": "Title: Augmenting Hessians with Inter-Layer Dependencies for Mixed-Precision Post-Training Quantization. (arXiv:2306.04879v1 [cs.LG])\nAbstract: Efficiently serving neural network models with low latency is becoming more challenging due to increasing model complexity and parameter count. Model quantization offers a solution which simultaneously reduces memory footprint and compute requirements. However, aggressive quantization may lead to an unacceptable loss in model accuracy owing to differences in sensitivity to numerical imperfection across different layers in the model. To address this challenge, we propose a mixed-precision post training quantization (PTQ) approach that assigns different numerical precisions to tensors in a network based on their specific needs, for a reduced memory footprint and improved latency while preserving model accuracy. Previous works rely on layer-wise Hessian information to determine numerical precision, but as we demonstrate, Hessian estimation is typically insufficient in determining an effective ordering of layer sensitivities. We address this by augmenting the estimated Hessian with additio",
    "path": "papers/23/06/2306.04879.json",
    "total_tokens": 1053,
    "translated_title": "增强具有层间依赖性的Hessians用于混合精度后训练量化",
    "translated_abstract": "随着模型复杂性和参数数量的增加，有效地为低延迟服务神经网络模型变得更加具有挑战性。模型量化提供了一种解决方案，可以同时减少内存占用和计算需求。然而，过度的量化可能会导致由于模型不同层之间对数字缺陷的敏感性差异而导致模型准确度的不可接受损失。为了解决这个问题，我们提出了一种混合精度后训练量化（PTQ）方法，根据网络中张量的特定需求为它们分配不同的数值精度，从而减少内存占用和改善延迟，同时保持模型准确度。以前的工作依赖于层次Hessian信息来确定数值精度，但正如我们所证明的那样，Hessian估计通常不足以确定层敏感性的有效排序。我们通过增强估计的Hessian与其他层之间的依赖关系来解决这个问题。我们的实验显示，我们的方法在一系列神经网络架构的混合精度PTQ方面优于先前的最先进方法。",
    "tldr": "本文提出一种混合精度后训练量化方法，分配不同的数值精度以减少内存占用和改善延迟，同时通过增强Hessian与其他层之间的依赖关系来解决传统方法不足以确定层敏感性排序的问题。实验证明该方法优于现有的混合精度后训练量化方法。",
    "en_tdlr": "This paper proposes a mixed-precision post training quantization method which assigns different numerical precisions to tensors in a network for reduced memory footprint and improved latency while addressing the issue of insufficient ordering of layer sensitivities in traditional methods by augmenting Hessian with inter-layer dependencies. Experiments show that this method outperforms current state-of-the-art techniques for mixed-precision PTQ."
}