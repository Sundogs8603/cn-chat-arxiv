{
    "title": "Solving Large-scale Spatial Problems with Convolutional Neural Networks. (arXiv:2306.08191v1 [cs.LG])",
    "abstract": "Over the past decade, deep learning research has been accelerated by increasingly powerful hardware, which facilitated rapid growth in the model complexity and the amount of data ingested. This is becoming unsustainable and therefore refocusing on efficiency is necessary. In this paper, we employ transfer learning to improve training efficiency for large-scale spatial problems. We propose that a convolutional neural network (CNN) can be trained on small windows of signals, but evaluated on arbitrarily large signals with little to no performance degradation, and provide a theoretical bound on the resulting generalization error. Our proof leverages shift-equivariance of CNNs, a property that is underexploited in transfer learning. The theoretical results are experimentally supported in the context of mobile infrastructure on demand (MID). The proposed approach is able to tackle MID at large scales with hundreds of agents, which was computationally intractable prior to this work.",
    "link": "http://arxiv.org/abs/2306.08191",
    "context": "Title: Solving Large-scale Spatial Problems with Convolutional Neural Networks. (arXiv:2306.08191v1 [cs.LG])\nAbstract: Over the past decade, deep learning research has been accelerated by increasingly powerful hardware, which facilitated rapid growth in the model complexity and the amount of data ingested. This is becoming unsustainable and therefore refocusing on efficiency is necessary. In this paper, we employ transfer learning to improve training efficiency for large-scale spatial problems. We propose that a convolutional neural network (CNN) can be trained on small windows of signals, but evaluated on arbitrarily large signals with little to no performance degradation, and provide a theoretical bound on the resulting generalization error. Our proof leverages shift-equivariance of CNNs, a property that is underexploited in transfer learning. The theoretical results are experimentally supported in the context of mobile infrastructure on demand (MID). The proposed approach is able to tackle MID at large scales with hundreds of agents, which was computationally intractable prior to this work.",
    "path": "papers/23/06/2306.08191.json",
    "total_tokens": 893,
    "translated_title": "用卷积神经网络解决大规模空间问题",
    "translated_abstract": "在过去的十年中，深度学习研究得到了不断增强的硬件支持，这促进了模型复杂性和摄入数据量的迅速增长。然而，这种趋势正在变得不可持续，因此，转向提高效率是必要的。在本文中，我们采用迁移学习提高大规模空间问题的训练效率。我们提出，卷积神经网络（CNN）可以在小台阶信号上进行训练，但在评估任意大小信号时几乎不会发生性能下降，并提供了所得到的泛化误差的理论界限。我们的证明利用了CNN的平移不变性，这是迁移学习中未被充分利用的一个属性。在基于移动基础架构的需求（MID）的情况下，理论结果在实验中得到了支持。本文所提出的方法能够处理大规模MID，其中包括数百个代理，这在此前的工作中是计算上难以实现的。",
    "tldr": "本文提出了一种采用迁移学习和小台阶信号训练卷积神经网络的方法，能够有效解决大规模空间问题，尤其在移动基础设施需求方面具有实际应用价值。",
    "en_tdlr": "This paper proposes a method of using transfer learning and small-window signals to train convolutional neural networks, which can effectively solve large-scale spatial problems, particularly in the practical application of mobile infrastructure demands."
}