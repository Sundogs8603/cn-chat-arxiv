{
    "title": "Kosmos-2: Grounding Multimodal Large Language Models to the World. (arXiv:2306.14824v2 [cs.CL] UPDATED)",
    "abstract": "We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generatio",
    "link": "http://arxiv.org/abs/2306.14824",
    "context": "Title: Kosmos-2: Grounding Multimodal Large Language Models to the World. (arXiv:2306.14824v2 [cs.CL] UPDATED)\nAbstract: We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generatio",
    "path": "papers/23/06/2306.14824.json",
    "total_tokens": 894,
    "translated_title": "Kosmos-2: 将多模态大规模语言模型与世界连接",
    "translated_abstract": "我们介绍了Kosmos-2，一个多模态大规模语言模型（MLLM），使其能够感知物体描述（例如，边界框）并将文本与视觉世界联系起来。具体而言，我们将引用表达式表示为Markdown中的链接，即``[text span](bounding boxes)''，其中物体描述是位置标记序列。通过与多模态语料库结合，我们构建了大规模的图像文本对（称为GrIT）的数据来训练该模型。除了MLLM的现有功能（例如，感知各种模态，遵循指令和进行上下文学习）外，Kosmos-2还将接地能力集成到下游应用中。我们在广泛的任务上评估了Kosmos-2，包括多模态接地（例如，引用表达理解和短语接地），多模态引用（例如，引用表达生成），感知语言任务以及语言理解和生成。",
    "tldr": "Kosmos-2是一个多模态大规模语言模型，可以感知物体描述并将文本与视觉世界联系起来。它在多个任务上展示了出色表现，包括多模态接地、多模态引用、感知语言任务以及语言理解和生成。"
}