{
    "title": "Causal interventions expose implicit situation models for commonsense language understanding. (arXiv:2306.03882v2 [cs.CL] UPDATED)",
    "abstract": "Accounts of human language processing have long appealed to implicit ``situation models'' that enrich comprehension with relevant but unstated world knowledge. Here, we apply causal intervention techniques to recent transformer models to analyze performance on the Winograd Schema Challenge (WSC), where a single context cue shifts interpretation of an ambiguous pronoun. We identify a relatively small circuit of attention heads that are responsible for propagating information from the context word that guides which of the candidate noun phrases the pronoun ultimately attends to. We then compare how this circuit behaves in a closely matched ``syntactic'' control where the situation model is not strictly necessary. These analyses suggest distinct pathways through which implicit situation models are constructed to guide pronoun resolution.",
    "link": "http://arxiv.org/abs/2306.03882",
    "context": "Title: Causal interventions expose implicit situation models for commonsense language understanding. (arXiv:2306.03882v2 [cs.CL] UPDATED)\nAbstract: Accounts of human language processing have long appealed to implicit ``situation models'' that enrich comprehension with relevant but unstated world knowledge. Here, we apply causal intervention techniques to recent transformer models to analyze performance on the Winograd Schema Challenge (WSC), where a single context cue shifts interpretation of an ambiguous pronoun. We identify a relatively small circuit of attention heads that are responsible for propagating information from the context word that guides which of the candidate noun phrases the pronoun ultimately attends to. We then compare how this circuit behaves in a closely matched ``syntactic'' control where the situation model is not strictly necessary. These analyses suggest distinct pathways through which implicit situation models are constructed to guide pronoun resolution.",
    "path": "papers/23/06/2306.03882.json",
    "total_tokens": 834,
    "translated_title": "因果干预揭示了通识语言理解中的隐含情景模型",
    "translated_abstract": "人类语言加工的理论一直在强调隐含的“情景模型”，以丰富理解的同时补充相关但未明确表述的世界知识。本文应用因果干预技术对最近的Transformer模型进行分析，研究其在Winograd Schema Challenge（WSC）上的表现。WSC提供了一个单一的上下文线索，用于转换有歧义的代词的解释。我们确定了一个相对较小的注意力头电路，负责从上下文词传播信息，以指导代词最终所涉及的候选名词短语的选择。然后，我们比较了在严格不需要情景模型的“语法”控制下电路的行为方式。这些分析表明，构建指导代词解决问题的隐含情景模型的途径是不同的。",
    "tldr": "本文通过因果干预技术分析了Transformer模型在Winograd Schema Challenge上的表现，确定了负责指导代词解释的相对较小的注意力头电路，并比较了其在“语法”控制下的行为方式，这揭示了构建隐含情景模型的不同途径。",
    "en_tdlr": "This paper applies causal intervention techniques to analyze transformer models' performance on the Winograd Schema Challenge and identifies a small circuit of attention heads responsible for guiding pronoun resolution. The paper compares this circuit's behavior in a \"syntactic\" control and suggests distinct pathways for constructing implicit situation models."
}