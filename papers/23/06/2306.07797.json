{
    "title": "Monolingual and Cross-Lingual Knowledge Transfer for Topic Classification. (arXiv:2306.07797v1 [cs.CL])",
    "abstract": "This article investigates the knowledge transfer from the RuQTopics dataset. This Russian topical dataset combines a large sample number (361,560 single-label, 170,930 multi-label) with extensive class coverage (76 classes). We have prepared this dataset from the \"Yandex Que\" raw data. By evaluating the RuQTopics - trained models on the six matching classes of the Russian MASSIVE subset, we have proved that the RuQTopics dataset is suitable for real-world conversational tasks, as the Russian-only models trained on this dataset consistently yield an accuracy around 85\\% on this subset. We also have figured out that for the multilingual BERT, trained on the RuQTopics and evaluated on the same six classes of MASSIVE (for all MASSIVE languages), the language-wise accuracy closely correlates (Spearman correlation 0.773 with p-value 2.997e-11) with the approximate size of the pretraining BERT's data for the corresponding language. At the same time, the correlation of the language-wise accura",
    "link": "http://arxiv.org/abs/2306.07797",
    "context": "Title: Monolingual and Cross-Lingual Knowledge Transfer for Topic Classification. (arXiv:2306.07797v1 [cs.CL])\nAbstract: This article investigates the knowledge transfer from the RuQTopics dataset. This Russian topical dataset combines a large sample number (361,560 single-label, 170,930 multi-label) with extensive class coverage (76 classes). We have prepared this dataset from the \"Yandex Que\" raw data. By evaluating the RuQTopics - trained models on the six matching classes of the Russian MASSIVE subset, we have proved that the RuQTopics dataset is suitable for real-world conversational tasks, as the Russian-only models trained on this dataset consistently yield an accuracy around 85\\% on this subset. We also have figured out that for the multilingual BERT, trained on the RuQTopics and evaluated on the same six classes of MASSIVE (for all MASSIVE languages), the language-wise accuracy closely correlates (Spearman correlation 0.773 with p-value 2.997e-11) with the approximate size of the pretraining BERT's data for the corresponding language. At the same time, the correlation of the language-wise accura",
    "path": "papers/23/06/2306.07797.json",
    "total_tokens": 692,
    "tldr": "本研究探讨了来自RuQTopics数据集的知识转移，该数据集包含361,560个单标签和170,930个多标签实例，并覆盖了76个类别。实证结果表明，RuQTopics数据集非常适合进行真实世界会话任务的训练，且具有良好的跨语言泛化性能，语言间的分类准确率与预训练BERT数据的大小密切相关。"
}