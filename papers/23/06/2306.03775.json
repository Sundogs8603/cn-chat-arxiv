{
    "title": "Matched Pair Calibration for Ranking Fairness. (arXiv:2306.03775v2 [cs.LG] UPDATED)",
    "abstract": "We propose a test of fairness in score-based ranking systems called matched pair calibration. Our approach constructs a set of matched item pairs with minimal confounding differences between subgroups before computing an appropriate measure of ranking error over the set. The matching step ensures that we compare subgroup outcomes between identically scored items so that measured performance differences directly imply unfairness in subgroup-level exposures. We show how our approach generalizes the fairness intuitions of calibration from a binary classification setting to ranking and connect our approach to other proposals for ranking fairness measures. Moreover, our strategy shows how the logic of marginal outcome tests extends to cases where the analyst has access to model scores. Lastly, we provide an example of applying matched pair calibration to a real-word ranking data set to demonstrate its efficacy in detecting ranking bias.",
    "link": "http://arxiv.org/abs/2306.03775",
    "context": "Title: Matched Pair Calibration for Ranking Fairness. (arXiv:2306.03775v2 [cs.LG] UPDATED)\nAbstract: We propose a test of fairness in score-based ranking systems called matched pair calibration. Our approach constructs a set of matched item pairs with minimal confounding differences between subgroups before computing an appropriate measure of ranking error over the set. The matching step ensures that we compare subgroup outcomes between identically scored items so that measured performance differences directly imply unfairness in subgroup-level exposures. We show how our approach generalizes the fairness intuitions of calibration from a binary classification setting to ranking and connect our approach to other proposals for ranking fairness measures. Moreover, our strategy shows how the logic of marginal outcome tests extends to cases where the analyst has access to model scores. Lastly, we provide an example of applying matched pair calibration to a real-word ranking data set to demonstrate its efficacy in detecting ranking bias.",
    "path": "papers/23/06/2306.03775.json",
    "total_tokens": 913,
    "translated_title": "匹配对校准用于排名公平性的测试",
    "translated_abstract": "我们提出了一种针对基于分数的排名系统中公平性的测试方法——匹配对校准。我们的方法构建了一组匹配的物品对，这些物品对子组之间的混淆差异最小，然后在这组物品上计算适当的排名误差测量结果。匹配步骤确保了我们在相同分数的物品之间比较子组结果，从而直接说明子组水平曝光的不公平性。我们展示了我们的方法如何将校准的公平性直觉从二元分类设置推广到排名，并将我们的方法与其他提议的排名公平性措施联系起来。此外，我们的策略展示了边际结果测试逻辑如何扩展到分析人员可以访问模型得分的情况。最后，我们提供了一个将匹配对校准应用于真实排名数据集以证明其检测排名偏差的效能的示例。",
    "tldr": "本文提出了一个针对排名系统公平性的测试方法——匹配对校准，通过构建混淆差异最小的匹配物品对来计算适当的排名误差测量结果，可以直接说明子组水平曝光的不公平性。实验证明该方法在检测排名偏差方面具有很好的效果。",
    "en_tdlr": "The paper proposes a test of fairness in score-based ranking systems called \"Matched Pair Calibration\", which constructs a set of matched item pairs with minimal confounding differences between subgroups before computing an appropriate measure of ranking error over the set, ensuring that subgroup outcomes are compared between identically scored items to imply unfairness in subgroup-level exposures. Experimental results indicate that this method has good performance in detecting ranking bias."
}