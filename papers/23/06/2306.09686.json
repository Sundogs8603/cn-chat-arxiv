{
    "title": "Collapsed Inference for Bayesian Deep Learning. (arXiv:2306.09686v1 [cs.LG])",
    "abstract": "Bayesian neural networks (BNNs) provide a formalism to quantify and calibrate uncertainty in deep learning. Current inference approaches for BNNs often resort to few-sample estimation for scalability, which can harm predictive performance, while its alternatives tend to be computationally prohibitively expensive. We tackle this challenge by revealing a previously unseen connection between inference on BNNs and volume computation problems. With this observation, we introduce a novel collapsed inference scheme that performs Bayesian model averaging using collapsed samples. It improves over a Monte-Carlo sample by limiting sampling to a subset of the network weights while pairing it with some closed-form conditional distribution over the rest. A collapsed sample represents uncountably many models drawn from the approximate posterior and thus yields higher sample efficiency. Further, we show that the marginalization of a collapsed sample can be solved analytically and efficiently despite t",
    "link": "http://arxiv.org/abs/2306.09686",
    "context": "Title: Collapsed Inference for Bayesian Deep Learning. (arXiv:2306.09686v1 [cs.LG])\nAbstract: Bayesian neural networks (BNNs) provide a formalism to quantify and calibrate uncertainty in deep learning. Current inference approaches for BNNs often resort to few-sample estimation for scalability, which can harm predictive performance, while its alternatives tend to be computationally prohibitively expensive. We tackle this challenge by revealing a previously unseen connection between inference on BNNs and volume computation problems. With this observation, we introduce a novel collapsed inference scheme that performs Bayesian model averaging using collapsed samples. It improves over a Monte-Carlo sample by limiting sampling to a subset of the network weights while pairing it with some closed-form conditional distribution over the rest. A collapsed sample represents uncountably many models drawn from the approximate posterior and thus yields higher sample efficiency. Further, we show that the marginalization of a collapsed sample can be solved analytically and efficiently despite t",
    "path": "papers/23/06/2306.09686.json",
    "total_tokens": 883,
    "translated_abstract": "贝叶斯神经网络为量化和校准深度学习中的不确定性提供了一种形式化方法。目前对于BNN的推断方法，使用少样本估计以实现可扩展性，但可能会损害预测性能。而其替代方法则可能有着计算代价过高的问题。我们通过揭示BNN推断和计算体积问题之间以往未曾发现的连接来应对这一挑战。基于这一发现，我们引入了一种新颖的折叠推断方案，它使用折叠样本执行贝叶斯模型平均。它通过将采样局限于网络权重的子集，并将其与其余部分的某些闭式条件分布相配对，优于传统的蒙特卡洛采样。折叠样本表示从近似后验中绘制的无数个模型，因此具有更高效的采样效率。此外，我们展示了折叠样本的边缘化可以在解析函数的帮助下高效地解决，尽管它是在更小的样本集上操作。",
    "tldr": "本文提出了一种折叠推断方法，通过限制采样数量和采样集来实现对BNNs更有效的贝叶斯模型平均，进而提高推断效率。",
    "en_tdlr": "This paper proposes a collapsed inference scheme that performs Bayesian model averaging using a subset of network weights paired with a closed-form conditional distribution to improve the efficiency and accuracy of Bayesian neural networks (BNNs)."
}