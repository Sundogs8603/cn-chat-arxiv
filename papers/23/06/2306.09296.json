{
    "title": "KoLA: Carefully Benchmarking World Knowledge of Large Language Models. (arXiv:2306.09296v2 [cs.CL] UPDATED)",
    "abstract": "The unprecedented performance of large language models (LLMs) necessitates improvements in evaluations. Rather than merely exploring the breadth of LLM abilities, we believe meticulous and thoughtful designs are essential to thorough, unbiased, and applicable evaluations. Given the importance of world knowledge to LLMs, we construct a Knowledge-oriented LLM Assessment benchmark (KoLA), in which we carefully design three crucial factors: (1) For ability modeling, we mimic human cognition to form a four-level taxonomy of knowledge-related abilities, covering $19$ tasks. (2) For data, to ensure fair comparisons, we use both Wikipedia, a corpus prevalently pre-trained by LLMs, along with continuously collected emerging corpora, aiming to evaluate the capacity to handle unseen data and evolving knowledge. (3) For evaluation criteria, we adopt a contrastive system, including overall standard scores for better numerical comparability across tasks and models and a unique self-contrast metric f",
    "link": "http://arxiv.org/abs/2306.09296",
    "context": "Title: KoLA: Carefully Benchmarking World Knowledge of Large Language Models. (arXiv:2306.09296v2 [cs.CL] UPDATED)\nAbstract: The unprecedented performance of large language models (LLMs) necessitates improvements in evaluations. Rather than merely exploring the breadth of LLM abilities, we believe meticulous and thoughtful designs are essential to thorough, unbiased, and applicable evaluations. Given the importance of world knowledge to LLMs, we construct a Knowledge-oriented LLM Assessment benchmark (KoLA), in which we carefully design three crucial factors: (1) For ability modeling, we mimic human cognition to form a four-level taxonomy of knowledge-related abilities, covering $19$ tasks. (2) For data, to ensure fair comparisons, we use both Wikipedia, a corpus prevalently pre-trained by LLMs, along with continuously collected emerging corpora, aiming to evaluate the capacity to handle unseen data and evolving knowledge. (3) For evaluation criteria, we adopt a contrastive system, including overall standard scores for better numerical comparability across tasks and models and a unique self-contrast metric f",
    "path": "papers/23/06/2306.09296.json",
    "total_tokens": 991,
    "translated_title": "KoLA: 认真基准大型语言模型的世界知识",
    "translated_abstract": "大型语言模型 (LLM) 的前所未有的性能需要改进评估。我们认为，除了探索LLM能力的广度之外，细致和深思熟虑的设计对于全面、公正和实用的评估是必要的。鉴于全球知识对LLM的重要性，我们构建了一个以知识为导向的LLM评估基准(KoLA)，其中我们精心设计了三个关键因素：(1) 对于能力建模，我们模仿人类认知构建了一个四级知识相关能力的分类体系，涵盖了19个任务。(2) 对于数据，为了确保公正比较，我们使用了维基百科作为LLM普遍预训练的语料库，同时还使用了持续收集的新兴语料库，旨在评估处理未见数据和不断发展的知识的能力。(3) 对于评估标准，我们采用了对比系统，包括整体标准分数，以实现在任务和模型之间更好的数值比较性，以及独特的自对照指标。",
    "tldr": "本研究提出了一个针对大型语言模型的知识导向评估基准 (KoLA)，通过模仿人类认知构建了四级知识相关能力的分类体系，并使用维基百科和新兴语料库进行评估。这个基准旨在全面、公正和实用地评估LLM的能力，以处理未见数据和不断发展的知识。",
    "en_tdlr": "This study presents a knowledge-oriented benchmark (KoLA) for large language models (LLMs), which utilizes a four-level taxonomy of knowledge-related abilities and incorporates both Wikipedia and emerging corpora for evaluation. The benchmark aims to provide thorough, unbiased, and applicable evaluations of LLMs' ability to handle unseen data and evolving knowledge."
}