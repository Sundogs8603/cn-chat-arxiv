{
    "title": "Towards Better Orthogonality Regularization with Disentangled Norm in Training Deep CNNs. (arXiv:2306.09939v1 [cs.CV])",
    "abstract": "Orthogonality regularization has been developed to prevent deep CNNs from training instability and feature redundancy. Among existing proposals, kernel orthogonality regularization enforces orthogonality by minimizing the residual between the Gram matrix formed by convolutional filters and the orthogonality matrix.  We propose a novel measure for achieving better orthogonality among filters, which disentangles diagonal and correlation information from the residual. The model equipped with the measure under the principle of imposing strict orthogonality between filters surpasses previous regularization methods in near-orthogonality. Moreover, we observe the benefits of improved strict filter orthogonality in relatively shallow models, but as model depth increases, the performance gains in models employing strict kernel orthogonality decrease sharply.  Furthermore, based on the observation of the potential conflict between strict kernel orthogonality and growing model capacity, we propos",
    "link": "http://arxiv.org/abs/2306.09939",
    "context": "Title: Towards Better Orthogonality Regularization with Disentangled Norm in Training Deep CNNs. (arXiv:2306.09939v1 [cs.CV])\nAbstract: Orthogonality regularization has been developed to prevent deep CNNs from training instability and feature redundancy. Among existing proposals, kernel orthogonality regularization enforces orthogonality by minimizing the residual between the Gram matrix formed by convolutional filters and the orthogonality matrix.  We propose a novel measure for achieving better orthogonality among filters, which disentangles diagonal and correlation information from the residual. The model equipped with the measure under the principle of imposing strict orthogonality between filters surpasses previous regularization methods in near-orthogonality. Moreover, we observe the benefits of improved strict filter orthogonality in relatively shallow models, but as model depth increases, the performance gains in models employing strict kernel orthogonality decrease sharply.  Furthermore, based on the observation of the potential conflict between strict kernel orthogonality and growing model capacity, we propos",
    "path": "papers/23/06/2306.09939.json",
    "total_tokens": 954,
    "translated_title": "通过解离规范优化实现更好的卷积神经网络训练",
    "translated_abstract": "为了防止深度卷积神经网络训练的不稳定和特征冗余，人们开发了正交规范化。在现有的提议中，核正交规范化通过最小化由卷积过滤器形成的格拉姆矩阵与正交矩阵之间的残差来强制实现正交性。本文提出了一种新的度量方法来实现更好的滤波器正交性，该方法从残差中解离出对角线和相关信息。在实现滤波器之间的严格正交性原则下，所采用的模型比以前的规范化方法在近正交性方面表现更好。此外，我们观察到改进后的严格滤波器正交性在相对较浅的模型中的优点，但随着模型深度的增加，采用严格核正交性的模型的性能提升急剧下降。基于观察到的严格核正交性和不断增加的模型容Capacity中的潜在冲突，我们提出了一个新的正交规范化方法。",
    "tldr": "本文提出一种新的通过解离规范优化实现更好的滤波器正交性的方法，在实现滤波器之间的严格正交性原则下，所采用的模型比以前的规范化方法在近正交性方面表现更好。",
    "en_tdlr": "This paper proposes a novel method for achieving better orthogonality among filters by disentangling diagonal and correlation information from the residual. The model equipped with this measure under the principle of imposing strict orthogonality between filters surpasses previous regularization methods in near-orthogonality."
}