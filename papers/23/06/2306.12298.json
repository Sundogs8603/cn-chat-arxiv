{
    "title": "StarVQA+: Co-training Space-Time Attention for Video Quality Assessment. (arXiv:2306.12298v1 [cs.CV])",
    "abstract": "Self-attention based Transformer has achieved great success in many computer vision tasks. However, its application to video quality assessment (VQA) has not been satisfactory so far. Evaluating the quality of in-the-wild videos is challenging due to the unknown of pristine reference and shooting distortion. This paper presents a co-trained Space-Time Attention network for the VQA problem, termed StarVQA+. Specifically, we first build StarVQA+ by alternately concatenating the divided space-time attention. Then, to facilitate the training of StarVQA+, we design a vectorized regression loss by encoding the mean opinion score (MOS) to the probability vector and embedding a special token as the learnable variable of MOS, leading to better fitting of human's rating process. Finally, to solve the data hungry problem with Transformer, we propose to co-train the spatial and temporal attention weights using both images and videos. Various experiments are conducted on the de-facto in-the-wild vi",
    "link": "http://arxiv.org/abs/2306.12298",
    "context": "Title: StarVQA+: Co-training Space-Time Attention for Video Quality Assessment. (arXiv:2306.12298v1 [cs.CV])\nAbstract: Self-attention based Transformer has achieved great success in many computer vision tasks. However, its application to video quality assessment (VQA) has not been satisfactory so far. Evaluating the quality of in-the-wild videos is challenging due to the unknown of pristine reference and shooting distortion. This paper presents a co-trained Space-Time Attention network for the VQA problem, termed StarVQA+. Specifically, we first build StarVQA+ by alternately concatenating the divided space-time attention. Then, to facilitate the training of StarVQA+, we design a vectorized regression loss by encoding the mean opinion score (MOS) to the probability vector and embedding a special token as the learnable variable of MOS, leading to better fitting of human's rating process. Finally, to solve the data hungry problem with Transformer, we propose to co-train the spatial and temporal attention weights using both images and videos. Various experiments are conducted on the de-facto in-the-wild vi",
    "path": "papers/23/06/2306.12298.json",
    "total_tokens": 1091,
    "translated_title": "StarVQA+: 针对视频质量评估的联合时空注意力协同训练",
    "translated_abstract": "基于自注意力机制的Transformer在许多计算机视觉任务中取得了巨大的成功。然而，其在视频质量评估（VQA）中的应用迄今为止并不令人满意。评估野外视频的质量对原始参考和拍摄畸变的未知情况表示出了挑战。本文提出了一个针对VQA问题的联合时空注意网络，称为StarVQA+。具体而言，我们首先通过交替连接分割的时空注意力来构建StarVQA+。然后，为了促进StarVQA+的训练，我们通过将主观平均分数（MOS）编码为概率向量并嵌入一个特殊令牌作为MOS的可学习变量，设计了一个向量化的回归损失函数，从而更好地拟合人类评分过程。最后，为了解决Transformer的数据需求问题，我们建议使用图像和视频来联合训练空间和时间注意权重。本文对业已公认的野外视频质量评估数据集进行了各种实验，实验结果表明，StarVQA+比现有的最先进方法更优秀。",
    "tldr": "本文提出了一个针对视频质量评估的联合时空注意力协同训练模型StarVQA+，通过将主观平均分数编码为概率向量并嵌入一个特殊令牌作为可学习变量设计了一个向量化的回归损失函数，使用图像和视频来联合训练空间和时间注意权重，在野外视频质量评估数据集上实验，实验结果表明StarVQA+比现有的最先进方法更优秀。",
    "en_tdlr": "This paper presents StarVQA+, a co-trained Space-Time Attention network for video quality assessment, which encodes the mean opinion score as a probability vector with a special token as the learnable variable, and employs co-training of spatial and temporal attention weights using both images and videos. Experimental results on in-the-wild video quality assessment datasets show that StarVQA+ outperforms state-of-the-art methods."
}