{
    "title": "Sy-CON: Symmetric Contrastive Loss for Continual Self-Supervised Representation Learning. (arXiv:2306.05101v1 [cs.LG])",
    "abstract": "We introduce a novel and general loss function, called Symmetric Contrastive (Sy-CON) loss, for effective continual self-supervised learning (CSSL). We first argue that the conventional loss form of continual learning which consists of single task-specific loss (for plasticity) and a regularizer (for stability) may not be ideal for contrastive loss based CSSL that focus on representation learning. Our reasoning is that, in contrastive learning based methods, the task-specific loss would suffer from decreasing diversity of negative samples and the regularizer may hinder learning new distinctive representations. To that end, we propose Sy-CON that consists of two losses (one for plasticity and the other for stability) with symmetric dependence on current and past models' negative sample embeddings. We argue our model can naturally find good trade-off between the plasticity and stability without any explicit hyperparameter tuning. We validate the effectiveness of our approach through exte",
    "link": "http://arxiv.org/abs/2306.05101",
    "context": "Title: Sy-CON: Symmetric Contrastive Loss for Continual Self-Supervised Representation Learning. (arXiv:2306.05101v1 [cs.LG])\nAbstract: We introduce a novel and general loss function, called Symmetric Contrastive (Sy-CON) loss, for effective continual self-supervised learning (CSSL). We first argue that the conventional loss form of continual learning which consists of single task-specific loss (for plasticity) and a regularizer (for stability) may not be ideal for contrastive loss based CSSL that focus on representation learning. Our reasoning is that, in contrastive learning based methods, the task-specific loss would suffer from decreasing diversity of negative samples and the regularizer may hinder learning new distinctive representations. To that end, we propose Sy-CON that consists of two losses (one for plasticity and the other for stability) with symmetric dependence on current and past models' negative sample embeddings. We argue our model can naturally find good trade-off between the plasticity and stability without any explicit hyperparameter tuning. We validate the effectiveness of our approach through exte",
    "path": "papers/23/06/2306.05101.json",
    "total_tokens": 1021,
    "translated_title": "Sy-CON：用于持续自监督表示学习的对称对比损失",
    "translated_abstract": "我们引入了一种新颖的通用损失函数，称为Symmetric Contrastive（Sy-CON）损失，用于有效的持续自监督学习（CSSL）。我们首先认为，传统的持续学习损失形式由单个任务特定损失（用于可塑性）和一个正则化器（用于稳定性）组成，对于基于对比损失的CSSL来说可能不理想，因为在对比学习方法中，任务特定损失会遭受负样本多样性降低的困扰，而正则化器可能会阻碍学习新的有区别的表示。为此，我们提出Sy-CON，它由两个损失（一个用于可塑性，另一个用于稳定性）组成，对当前和过去模型的负样本嵌入具有对称依赖关系。我们认为，我们的模型可以自然地找到良好的可塑性和稳定性之间的权衡，而无需任何显式的超参数调整。我们通过对几种持续学习基准的外部和内部实验验证了我们方法的有效性，并表明Sy-CON在稳定性和表征质量方面始终优于现有的持续学习方法。",
    "tldr": "Sy-CON是一种新的损失函数，用于持续自监督学习，它由两个损失组成，可以自然地找到良好的可塑性和稳定性之间的权衡，超过了现有持续学习方法。",
    "en_tdlr": "Sy-CON is a novel loss function for continual self-supervised learning, consisting of two losses with symmetric dependence on negative sample embeddings of current and past models. It can naturally find a good trade-off between plasticity and stability without any explicit hyperparameter tuning, and outperforms existing continuous learning methods."
}