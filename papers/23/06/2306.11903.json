{
    "title": "Deep Fusion: Efficient Network Training via Pre-trained Initializations. (arXiv:2306.11903v1 [cs.LG])",
    "abstract": "In recent years, deep learning has made remarkable progress in a wide range of domains, with a particularly notable impact on natural language processing tasks. One of the challenges associated with training deep neural networks is the need for large amounts of computational resources and time. In this paper, we present Deep Fusion, an efficient approach to network training that leverages pre-trained initializations of smaller networks. % We show that Deep Fusion accelerates the training process, reduces computational requirements, and leads to improved generalization performance on a variety of NLP tasks and T5 model sizes. % Our experiments demonstrate that Deep Fusion is a practical and effective approach to reduce the training time and resource consumption while maintaining, or even surpassing, the performance of traditional training methods.",
    "link": "http://arxiv.org/abs/2306.11903",
    "context": "Title: Deep Fusion: Efficient Network Training via Pre-trained Initializations. (arXiv:2306.11903v1 [cs.LG])\nAbstract: In recent years, deep learning has made remarkable progress in a wide range of domains, with a particularly notable impact on natural language processing tasks. One of the challenges associated with training deep neural networks is the need for large amounts of computational resources and time. In this paper, we present Deep Fusion, an efficient approach to network training that leverages pre-trained initializations of smaller networks. % We show that Deep Fusion accelerates the training process, reduces computational requirements, and leads to improved generalization performance on a variety of NLP tasks and T5 model sizes. % Our experiments demonstrate that Deep Fusion is a practical and effective approach to reduce the training time and resource consumption while maintaining, or even surpassing, the performance of traditional training methods.",
    "path": "papers/23/06/2306.11903.json",
    "total_tokens": 823,
    "translated_title": "Deep Fusion：基于预训练初始化的高效网络训练",
    "translated_abstract": "近年来，深度学习在众多领域中取得了显著进展，尤其是对自然语言处理任务产生了显著影响。训练深度神经网络的挑战之一是需要大量的计算资源和时间。本文提出了Deep Fusion，一种利用较小网络的预训练初始化的高效网络训练方法。我们展示了Deep Fusion在各种NLP任务和T5模型大小上加速训练过程，降低计算要求，并导致改进的泛化性能。我们的实验表明，Deep Fusion是一种实用和有效的方法，可以在维持传统训练方法的性能甚至超越其性能的同时，减少训练时间和资源消耗。",
    "tldr": "本文提出了Deep Fusion，一种基于预训练初始化的高效网络训练方法。通过加速训练过程、降低计算要求，并导致改进的泛化性能，使得该方法在维持传统训练方法的性能甚至超越其性能的同时，减少了训练时间和资源消耗。",
    "en_tdlr": "This paper proposes Deep Fusion, an efficient approach to network training that leverages pre-trained initializations of smaller networks. It accelerates the training process, reduces computational requirements, and leads to improved generalization performance on a variety of NLP tasks and T5 model sizes, while reducing training time and resource consumption."
}