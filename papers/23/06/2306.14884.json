{
    "title": "Learning to Modulate pre-trained Models in RL. (arXiv:2306.14884v2 [cs.LG] UPDATED)",
    "abstract": "Reinforcement Learning (RL) has been successful in various domains like robotics, game playing, and simulation. While RL agents have shown impressive capabilities in their specific tasks, they insufficiently adapt to new tasks. In supervised learning, this adaptation problem is addressed by large-scale pre-training followed by fine-tuning to new down-stream tasks. Recently, pre-training on multiple tasks has been gaining traction in RL. However, fine-tuning a pre-trained model often suffers from catastrophic forgetting. That is, the performance on the pre-training tasks deteriorates when fine-tuning on new tasks. To investigate the catastrophic forgetting phenomenon, we first jointly pre-train a model on datasets from two benchmark suites, namely Meta-World and DMControl. Then, we evaluate and compare a variety of fine-tuning methods prevalent in natural language processing, both in terms of performance on new tasks, and how well performance on pre-training tasks is retained. Our study",
    "link": "http://arxiv.org/abs/2306.14884",
    "context": "Title: Learning to Modulate pre-trained Models in RL. (arXiv:2306.14884v2 [cs.LG] UPDATED)\nAbstract: Reinforcement Learning (RL) has been successful in various domains like robotics, game playing, and simulation. While RL agents have shown impressive capabilities in their specific tasks, they insufficiently adapt to new tasks. In supervised learning, this adaptation problem is addressed by large-scale pre-training followed by fine-tuning to new down-stream tasks. Recently, pre-training on multiple tasks has been gaining traction in RL. However, fine-tuning a pre-trained model often suffers from catastrophic forgetting. That is, the performance on the pre-training tasks deteriorates when fine-tuning on new tasks. To investigate the catastrophic forgetting phenomenon, we first jointly pre-train a model on datasets from two benchmark suites, namely Meta-World and DMControl. Then, we evaluate and compare a variety of fine-tuning methods prevalent in natural language processing, both in terms of performance on new tasks, and how well performance on pre-training tasks is retained. Our study",
    "path": "papers/23/06/2306.14884.json",
    "total_tokens": 823,
    "translated_title": "学习在强化学习中调整预训练模型",
    "translated_abstract": "强化学习在机器人、游戏和仿真等领域取得了成功。然而，尽管强化学习代理在特定任务中展示了令人印象深刻的能力，但它们对新任务的适应性不足。最近，强化学习中的多任务预训练逐渐受到关注。然而，对预训练模型进行微调时往往会出现灾难性遗忘。也就是说，当在新任务上进行微调时，对预训练任务的性能会下降。为了研究灾难性遗忘现象，我们首先在Meta-World和DMControl两个基准套件的数据集上联合预训练模型。然后，我们评估和比较了自然语言处理中常见的多种微调方法在新任务性能和对预训练任务性能保留方面的表现。",
    "tldr": "本研究通过联合预训练模型，并评估比较了不同的微调方法，旨在解决强化学习中的灾难性遗忘问题。",
    "en_tdlr": "This study investigates catastrophic forgetting in reinforcement learning by joint pre-training on benchmark datasets and evaluates various fine-tuning methods to address this issue."
}