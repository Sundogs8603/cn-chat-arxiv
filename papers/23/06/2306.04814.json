{
    "title": "Revisiting Inferential Benchmarks for Knowledge Graph Completion. (arXiv:2306.04814v1 [cs.AI])",
    "abstract": "Knowledge Graph (KG) completion is the problem of extending an incomplete KG with missing facts. A key feature of Machine Learning approaches for KG completion is their ability to learn inference patterns, so that the predicted facts are the results of applying these patterns to the KG. Standard completion benchmarks, however, are not well-suited for evaluating models' abilities to learn patterns, because the training and test sets of these benchmarks are a random split of a given KG and hence do not capture the causality of inference patterns. We propose a novel approach for designing KG completion benchmarks based on the following principles: there is a set of logical rules so that the missing facts are the results of the rules' application; the training set includes both premises matching rule antecedents and the corresponding conclusions; the test set consists of the results of applying the rules to the training set; the negative examples are designed to discourage the models from ",
    "link": "http://arxiv.org/abs/2306.04814",
    "context": "Title: Revisiting Inferential Benchmarks for Knowledge Graph Completion. (arXiv:2306.04814v1 [cs.AI])\nAbstract: Knowledge Graph (KG) completion is the problem of extending an incomplete KG with missing facts. A key feature of Machine Learning approaches for KG completion is their ability to learn inference patterns, so that the predicted facts are the results of applying these patterns to the KG. Standard completion benchmarks, however, are not well-suited for evaluating models' abilities to learn patterns, because the training and test sets of these benchmarks are a random split of a given KG and hence do not capture the causality of inference patterns. We propose a novel approach for designing KG completion benchmarks based on the following principles: there is a set of logical rules so that the missing facts are the results of the rules' application; the training set includes both premises matching rule antecedents and the corresponding conclusions; the test set consists of the results of applying the rules to the training set; the negative examples are designed to discourage the models from ",
    "path": "papers/23/06/2306.04814.json",
    "total_tokens": 1026,
    "translated_title": "重新审视知识图谱完成问题的推理基准",
    "translated_abstract": "知识图谱（KG）完成是将不完整的KG与缺失的事实扩展的问题。KG完成的机器学习方法的一个关键特征是能够学习推理模式，以便预测的事实是将这些模式应用于KG的结果。然而，标准的完成基准不适合评估模型学习模式的能力，因为这些基准的训练和测试集是给定KG的随机拆分，因此不捕捉推理模式的因果关系。我们提出了一种新的KG完成基准设计方法，基于以下原则：有一组逻辑规则，使得缺失的事实是规则应用的结果；训练集包括匹配规则前提和相应结论的前提；测试集由将规则应用于训练集得出的结果组成；负面示例旨在防止模型学习虚假的模式。我们使用一项基准实例化了我们的方法，即FB15k-237，并表明它比以前的基准更具有区分度，可以评估模型推广到新模式的能力。",
    "tldr": "该论文提出了一种新的KG完成基准设计方法，用于评估模型学习推理模式的能力。他们的方法基于一组逻辑规则，使得缺失的事实是规则应用的结果。使用他们的基准FB15k-237，表明它比以前的基准更具有区分度，可以评估模型推广到新模式的能力。",
    "en_tdlr": "This paper proposes a novel approach to design benchmarks for evaluating models' abilities to learn inference patterns in Knowledge Graph (KG) completion. Their approach is based on a set of logical rules so that the missing facts are the results of the rules' application. Using their benchmark, FB15k-237, they show that it is more discriminative than previous benchmarks in evaluating the ability of models to generalize to new patterns."
}