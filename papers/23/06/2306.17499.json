{
    "title": "The Implicit Bias of Minima Stability in Multivariate Shallow ReLU Networks. (arXiv:2306.17499v1 [cs.LG])",
    "abstract": "We study the type of solutions to which stochastic gradient descent converges when used to train a single hidden-layer multivariate ReLU network with the quadratic loss. Our results are based on a dynamical stability analysis. In the univariate case, it was shown that linearly stable minima correspond to network functions (predictors), whose second derivative has a bounded weighted $L^1$ norm. Notably, the bound gets smaller as the step size increases, implying that training with a large step size leads to `smoother' predictors. Here we generalize this result to the multivariate case, showing that a similar result applies to the Laplacian of the predictor. We demonstrate the tightness of our bound on the MNIST dataset, and show that it accurately captures the behavior of the solutions as a function of the step size. Additionally, we prove a depth separation result on the approximation power of ReLU networks corresponding to stable minima of the loss. Specifically, although shallow ReLU",
    "link": "http://arxiv.org/abs/2306.17499",
    "context": "Title: The Implicit Bias of Minima Stability in Multivariate Shallow ReLU Networks. (arXiv:2306.17499v1 [cs.LG])\nAbstract: We study the type of solutions to which stochastic gradient descent converges when used to train a single hidden-layer multivariate ReLU network with the quadratic loss. Our results are based on a dynamical stability analysis. In the univariate case, it was shown that linearly stable minima correspond to network functions (predictors), whose second derivative has a bounded weighted $L^1$ norm. Notably, the bound gets smaller as the step size increases, implying that training with a large step size leads to `smoother' predictors. Here we generalize this result to the multivariate case, showing that a similar result applies to the Laplacian of the predictor. We demonstrate the tightness of our bound on the MNIST dataset, and show that it accurately captures the behavior of the solutions as a function of the step size. Additionally, we prove a depth separation result on the approximation power of ReLU networks corresponding to stable minima of the loss. Specifically, although shallow ReLU",
    "path": "papers/23/06/2306.17499.json",
    "total_tokens": 1037,
    "translated_title": "多变量浅层ReLU网络中最小值稳定性的隐含偏差",
    "translated_abstract": "我们研究了随机梯度下降在训练单隐藏层的多变量ReLU网络时收敛到哪种解的问题，其中使用的是二次损失函数。我们的结果基于动态稳定性分析。在一元情况中，已经证明了线性稳定的最小值对应于网络函数（预测器），其二阶导数具有有界加权$L^1$范数。值得注意的是，边界随着步长增加而变小，这意味着使用较大的步长进行训练会导致“更平滑”的预测器。在这里，我们将这个结果推广到多变量情况，证明了类似的结果适用于预测器的拉普拉斯算子。我们在MNIST数据集上证明了我们边界的紧密性，并展示了它如何准确地捕捉到解在步长函数中的行为。此外，我们证明了与损失函数的稳定最小值对应的ReLU网络的逼近能力的深度分离结果。具体来说，浅层ReLU网络的逼近能力远低于稳定最小值。",
    "tldr": "本文研究了在训练单隐藏层的多变量ReLU网络时，随机梯度下降收敛到何种解。我们发现线性稳定的最小值对应于具有有界$L^1$范数的预测器的二阶导数。我们将这个结果推广到多变量情况，并证明了与损失函数的稳定最小值对应的ReLU网络的逼近能力很低。",
    "en_tdlr": "This paper investigates the type of solutions that stochastic gradient descent converges to when training a single hidden-layer multivariate ReLU network with quadratic loss. The study shows that linearly stable minima correspond to predictors with bounded weighted L^1 norms of their second derivatives. The research extends this result to the multivariate case and demonstrates the limited approximation power of ReLU networks corresponding to stable minima of the loss."
}