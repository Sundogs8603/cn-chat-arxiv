{
    "title": "Understanding Certified Training with Interval Bound Propagation",
    "abstract": "arXiv:2306.10426v2 Announce Type: replace-cross  Abstract: As robustness verification methods are becoming more precise, training certifiably robust neural networks is becoming ever more relevant. To this end, certified training methods compute and then optimize an upper bound on the worst-case loss over a robustness specification. Curiously, training methods based on the imprecise interval bound propagation (IBP) consistently outperform those leveraging more precise bounding methods. Still, we lack an understanding of the mechanisms making IBP so successful.   In this work, we thoroughly investigate these mechanisms by leveraging a novel metric measuring the tightness of IBP bounds. We first show theoretically that, for deep linear models, tightness decreases with width and depth at initialization, but improves with IBP training, given sufficient network width. We, then, derive sufficient and necessary conditions on weight matrices for IBP bounds to become exact and demonstrate that t",
    "link": "https://arxiv.org/abs/2306.10426",
    "context": "Title: Understanding Certified Training with Interval Bound Propagation\nAbstract: arXiv:2306.10426v2 Announce Type: replace-cross  Abstract: As robustness verification methods are becoming more precise, training certifiably robust neural networks is becoming ever more relevant. To this end, certified training methods compute and then optimize an upper bound on the worst-case loss over a robustness specification. Curiously, training methods based on the imprecise interval bound propagation (IBP) consistently outperform those leveraging more precise bounding methods. Still, we lack an understanding of the mechanisms making IBP so successful.   In this work, we thoroughly investigate these mechanisms by leveraging a novel metric measuring the tightness of IBP bounds. We first show theoretically that, for deep linear models, tightness decreases with width and depth at initialization, but improves with IBP training, given sufficient network width. We, then, derive sufficient and necessary conditions on weight matrices for IBP bounds to become exact and demonstrate that t",
    "path": "papers/23/06/2306.10426.json",
    "total_tokens": 913,
    "translated_title": "了解使用区间传播边界进行认证训练",
    "translated_abstract": "随着鲁棒性验证方法变得更加精确，对训练具有认证鲁棒性的神经网络变得越来越重要。认证训练方法计算并优化了对鲁棒性规范下最坏情况损失的上界。有趣的是，基于不精确的区间传播边界（IBP）的训练方法一直表现出色，优于利用更精确边界方法的方法。然而，我们仍然缺乏对使IBP如此成功的机制的理解。在这项工作中，我们通过利用一种衡量IBP边界紧密度的新颖指标，彻底研究了这些机制。我们首先在理论上表明，对于深度线性模型，在初始化时，紧密度随着宽度和深度减小，但在给定足够网络宽度的情况下，通过IBP训练会得到改进。然后，我们推导了使IBP边界变得精确的权重矩阵的充分和必要条件，并证明了t",
    "tldr": "本研究通过引入一种新颖指标，深入探讨了区间传播边界（IBP）训练成功的机制。理论上表明，对于深度线性模型，IBP训练能够在足够宽度的条件下改善边界紧密度。",
    "en_tdlr": "This study thoroughly investigates the mechanisms behind the success of training with Interval Bound Propagation (IBP) by introducing a novel metric, showing theoretically that IBP training can improve boundary tightness for deep linear models under sufficient width."
}