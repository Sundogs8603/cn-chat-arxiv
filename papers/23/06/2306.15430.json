{
    "title": "KnowPrefix-Tuning: A Two-Stage Prefix-Tuning Framework for Knowledge-Grounded Dialogue Generation. (arXiv:2306.15430v1 [cs.CL])",
    "abstract": "Existing knowledge-grounded conversation systems generate responses typically in a retrieve-then-generate manner. They require a large knowledge base and a strong knowledge retrieval component, which is time- and resource-consuming. In this paper, we address the challenge by leveraging the inherent knowledge encoded in the pre-trained language models (PLMs). We propose Knowledgeable Prefix Tuning (KnowPrefix-Tuning), a two-stage tuning framework, bypassing the retrieval process in a knowledge-grounded conversation system by injecting prior knowledge into the lightweight knowledge prefix. The knowledge prefix is a sequence of continuous knowledge-specific vectors that can be learned during training. In addition, we propose a novel interactive re-parameterization mechanism that allows the prefix to interact fully with the PLM during the optimization of response generation. Experimental results demonstrate that KnowPrefix-Tuning outperforms fine-tuning and other lightweight tuning approac",
    "link": "http://arxiv.org/abs/2306.15430",
    "context": "Title: KnowPrefix-Tuning: A Two-Stage Prefix-Tuning Framework for Knowledge-Grounded Dialogue Generation. (arXiv:2306.15430v1 [cs.CL])\nAbstract: Existing knowledge-grounded conversation systems generate responses typically in a retrieve-then-generate manner. They require a large knowledge base and a strong knowledge retrieval component, which is time- and resource-consuming. In this paper, we address the challenge by leveraging the inherent knowledge encoded in the pre-trained language models (PLMs). We propose Knowledgeable Prefix Tuning (KnowPrefix-Tuning), a two-stage tuning framework, bypassing the retrieval process in a knowledge-grounded conversation system by injecting prior knowledge into the lightweight knowledge prefix. The knowledge prefix is a sequence of continuous knowledge-specific vectors that can be learned during training. In addition, we propose a novel interactive re-parameterization mechanism that allows the prefix to interact fully with the PLM during the optimization of response generation. Experimental results demonstrate that KnowPrefix-Tuning outperforms fine-tuning and other lightweight tuning approac",
    "path": "papers/23/06/2306.15430.json",
    "total_tokens": 887,
    "translated_title": "《KnowPrefix-Tuning: 一种用于知识驱动对话生成的两阶段前缀调优框架》",
    "translated_abstract": "现有的知识驱动对话系统通常以检索再生成的方式生成回复。它们需要一个庞大的知识库和一个强大的知识检索组件，这需要时间和资源。在本文中，我们通过利用预训练语言模型（PLMs）中内在编码的知识来应对这一挑战。我们提出了一种名为Knowledgeable Prefix Tuning（KnowPrefix-Tuning）的两阶段调优框架，在知识驱动对话系统中通过将先验知识注入到轻量级知识前缀中来避免检索过程。知识前缀是一系列连续的与知识相关的向量，在训练过程中可以学习到。此外，我们提出了一种新颖的交互重参数化机制，允许前缀在响应生成优化过程中与PLM进行全面的交互。实验结果表明，KnowPrefix-Tuning优于fine-tuning和其他轻量级调优方法。",
    "tldr": "KnowPrefix-Tuning是一种知识驱动对话生成的两阶段前缀调优框架，通过将先验知识注入到轻量级知识前缀中来避免检索过程，与PLM进行全面交互优化响应生成的过程。",
    "en_tdlr": "KnowPrefix-Tuning is a two-stage prefix-tuning framework for knowledge-grounded dialogue generation. It bypasses the retrieval process by injecting prior knowledge into the lightweight knowledge prefix and allows full interaction with PLM to optimize response generation."
}