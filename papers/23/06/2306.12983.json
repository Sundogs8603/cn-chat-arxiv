{
    "title": "Towards More Realistic Membership Inference Attacks on Large Diffusion Models. (arXiv:2306.12983v1 [cs.LG])",
    "abstract": "Generative diffusion models, including Stable Diffusion and Midjourney, can generate visually appealing, diverse, and high-resolution images for various applications. These models are trained on billions of internet-sourced images, raising significant concerns about the potential unauthorized use of copyright-protected images. In this paper, we examine whether it is possible to determine if a specific image was used in the training set, a problem known in the cybersecurity community and referred to as a membership inference attack. Our focus is on Stable Diffusion, and we address the challenge of designing a fair evaluation framework to answer this membership question. We propose a methodology to establish a fair evaluation setup and apply it to Stable Diffusion, enabling potential extensions to other generative models. Utilizing this evaluation setup, we execute membership attacks (both known and newly introduced). Our research reveals that previously proposed evaluation setups do not",
    "link": "http://arxiv.org/abs/2306.12983",
    "context": "Title: Towards More Realistic Membership Inference Attacks on Large Diffusion Models. (arXiv:2306.12983v1 [cs.LG])\nAbstract: Generative diffusion models, including Stable Diffusion and Midjourney, can generate visually appealing, diverse, and high-resolution images for various applications. These models are trained on billions of internet-sourced images, raising significant concerns about the potential unauthorized use of copyright-protected images. In this paper, we examine whether it is possible to determine if a specific image was used in the training set, a problem known in the cybersecurity community and referred to as a membership inference attack. Our focus is on Stable Diffusion, and we address the challenge of designing a fair evaluation framework to answer this membership question. We propose a methodology to establish a fair evaluation setup and apply it to Stable Diffusion, enabling potential extensions to other generative models. Utilizing this evaluation setup, we execute membership attacks (both known and newly introduced). Our research reveals that previously proposed evaluation setups do not",
    "path": "papers/23/06/2306.12983.json",
    "total_tokens": 954,
    "translated_title": "面向大型扩散模型的更真实成员推断攻击",
    "translated_abstract": "生成扩散模型，包括稳定扩散和Midjourney，可以为各种应用程序生成具有视觉吸引力、多样性和高分辨率的图像。这些模型是在数十亿个互联网来源的图像上进行训练的，引发了关于潜在未经授权使用受版权保护的图像的重要担忧。本文研究了如何确定特定图像是否在训练集中使用了，这在网络安全社区中被称为成员推断攻击问题。我们的研究重点是稳定扩散，并解决了设计一个公平的评估框架来回答这个成员问题的挑战。我们提出了一种方法来建立一个公平的评估设置，并将其应用于稳定扩散，使潜在的扩展到其他生成模型成为可能。利用这个评估设置，我们执行成员攻击（包括已知和新引入的攻击）。我们的研究揭示了先前提出的评估设置不能很好地模拟真实世界中的成员攻击。",
    "tldr": "本文研究了成员推断攻击问题，以确定图像是否在训练集中使用。研究集中于稳定扩散模型，提出了一种公平的评估框架，并进行了成员攻击，揭示了先前提出的评估设置不能很好地模拟真实世界中的成员攻击。",
    "en_tdlr": "This paper investigates the problem of membership inference attacks to determine if a specific image was used in the training set, with a focus on the Stable Diffusion model. The authors propose a fair evaluation framework and conduct various membership attacks, revealing the limitations of previous evaluation setups."
}