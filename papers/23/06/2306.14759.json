{
    "title": "PMaF: Deep Declarative Layers for Principal Matrix Features. (arXiv:2306.14759v2 [cs.LG] UPDATED)",
    "abstract": "We explore two differentiable deep declarative layers, namely least squares on sphere (LESS) and implicit eigen decomposition (IED), for learning the principal matrix features (PMaF). This can be used to represent data features with a low-dimension vector containing dominant information from a high-dimension matrix. We first solve the problems with iterative optimization in the forward pass and then backpropagate the solution for implicit gradients under a bi-level optimization framework. Particularly, adaptive descent steps with the backtracking line search method and descent decay in the tangent space are studied to improve the forward pass efficiency of LESS. Meanwhile, exploited data structures are used to greatly reduce the computational complexity in the backward pass of LESS and IED. Empirically, we demonstrate the superiority of our layers over the off-the-shelf baselines by comparing the solution optimality and computational requirements.",
    "link": "http://arxiv.org/abs/2306.14759",
    "context": "Title: PMaF: Deep Declarative Layers for Principal Matrix Features. (arXiv:2306.14759v2 [cs.LG] UPDATED)\nAbstract: We explore two differentiable deep declarative layers, namely least squares on sphere (LESS) and implicit eigen decomposition (IED), for learning the principal matrix features (PMaF). This can be used to represent data features with a low-dimension vector containing dominant information from a high-dimension matrix. We first solve the problems with iterative optimization in the forward pass and then backpropagate the solution for implicit gradients under a bi-level optimization framework. Particularly, adaptive descent steps with the backtracking line search method and descent decay in the tangent space are studied to improve the forward pass efficiency of LESS. Meanwhile, exploited data structures are used to greatly reduce the computational complexity in the backward pass of LESS and IED. Empirically, we demonstrate the superiority of our layers over the off-the-shelf baselines by comparing the solution optimality and computational requirements.",
    "path": "papers/23/06/2306.14759.json",
    "total_tokens": 902,
    "translated_title": "PMaF:用于主要矩阵特征的深度声明性层",
    "translated_abstract": "本文探讨了两种不同iable的深度声明性层，即球上的最小二乘法(LESS)和隐式特征值分解(IED)，用于学习主要矩阵特征(PMaF)。这可以用一个低维向量表示包含来自高维矩阵的主要信息的数据特征。我们首先通过前向传递的迭代优化来解决问题，然后在一个双层优化框架下反向传播解决方案进行隐式梯度。具体来说，我们研究了自适应下降步骤和反向线搜索方法以及切线空间中的下降衰减，以提高LESS的前向通过程的效率。与此同时，在LESS和IED的反向传递中使用了优化的数据结构，大大降低了计算复杂度。在经验上，通过比较解决方案的最优性和计算要求，我们证明了我们的层优于现成的基线模型。",
    "tldr": "本文介绍了PMaF框架，使用声明性的深度层来学习主要矩阵特征，通过迭代优化解决问题并应用双层优化框架进行反向传播，从而提高效率。实验证明了该框架优于现有的基线模型。",
    "en_tdlr": "This paper presents the PMaF framework, which uses declarative deep layers to learn principal matrix features, solves the problem through iterative optimization, and applies bi-level optimization framework for backward propagation to improve efficiency. Empirical results demonstrate the superiority of this framework over existing baseline models."
}