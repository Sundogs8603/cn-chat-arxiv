{
    "title": "HomE: Homography-Equivariant Video Representation Learning. (arXiv:2306.01623v1 [cs.CV])",
    "abstract": "Recent advances in self-supervised representation learning have enabled more efficient and robust model performance without relying on extensive labeled data. However, most works are still focused on images, with few working on videos and even fewer on multi-view videos, where more powerful inductive biases can be leveraged for self-supervision. In this work, we propose a novel method for representation learning of multi-view videos, where we explicitly model the representation space to maintain Homography Equivariance (HomE). Our method learns an implicit mapping between different views, culminating in a representation space that maintains the homography relationship between neighboring views. We evaluate our HomE representation via action recognition and pedestrian intent prediction as downstream tasks. On action classification, our method obtains 96.4% 3-fold accuracy on the UCF101 dataset, better than most state-of-the-art self-supervised learning methods. Similarly, on the STIP da",
    "link": "http://arxiv.org/abs/2306.01623",
    "context": "Title: HomE: Homography-Equivariant Video Representation Learning. (arXiv:2306.01623v1 [cs.CV])\nAbstract: Recent advances in self-supervised representation learning have enabled more efficient and robust model performance without relying on extensive labeled data. However, most works are still focused on images, with few working on videos and even fewer on multi-view videos, where more powerful inductive biases can be leveraged for self-supervision. In this work, we propose a novel method for representation learning of multi-view videos, where we explicitly model the representation space to maintain Homography Equivariance (HomE). Our method learns an implicit mapping between different views, culminating in a representation space that maintains the homography relationship between neighboring views. We evaluate our HomE representation via action recognition and pedestrian intent prediction as downstream tasks. On action classification, our method obtains 96.4% 3-fold accuracy on the UCF101 dataset, better than most state-of-the-art self-supervised learning methods. Similarly, on the STIP da",
    "path": "papers/23/06/2306.01623.json",
    "total_tokens": 913,
    "translated_title": "HomE: 同变性单应性视频表示学习",
    "translated_abstract": "近年来，自监督表示学习的快速发展使得模型在不需要大量标注数据的情况下得到了更高效和更强健的性能。然而，大部分工作仍然集中在图像上，很少有工作关注视频，甚至更少的工作关注多视角视频，其中可以利用更强大的归纳偏差进行自监督。在本文中，我们提出了一种新的多视角视频表示学习方法，在其中我们显式地模拟表示空间以保持同变性。我们的方法学习不同视角之间的隐式映射，最终形成一个表示空间，其中保持相邻视角之间的单应关系。我们通过行动识别和行人意图预测作为下游任务来评估我们的HomE表示。在UCF101数据集上，我们的方法在3次交叉验证的准确度达到了96.4％，优于大多数最先进的自监督学习方法。同样，在行人意图预测的STIP数据集上，我们的方法也达到了最先进的结果。",
    "tldr": "本文提出了一种新的多视角视频表示学习方法，通过学习隐式映射，形成了一个具有同变性的表示空间，能够对行动识别和行人意图预测任务获得最先进的结果。",
    "en_tdlr": "This paper proposes a novel method for representation learning of multi-view videos, which learns an implicit mapping to form a homography-equivariant representation space, achieving state-of-the-art performance on action recognition and pedestrian intent prediction tasks."
}