{
    "title": "Adaptive DNN Surgery for Selfish Inference Acceleration with On-demand Edge Resource. (arXiv:2306.12185v1 [cs.GT])",
    "abstract": "Deep Neural Networks (DNNs) have significantly improved the accuracy of intelligent applications on mobile devices. DNN surgery, which partitions DNN processing between mobile devices and multi-access edge computing (MEC) servers, can enable real-time inference despite the computational limitations of mobile devices. However, DNN surgery faces a critical challenge: determining the optimal computing resource demand from the server and the corresponding partition strategy, while considering both inference latency and MEC server usage costs. This problem is compounded by two factors: (1) the finite computing capacity of the MEC server, which is shared among multiple devices, leading to inter-dependent demands, and (2) the shift in modern DNN architecture from chains to directed acyclic graphs (DAGs), which complicates potential solutions.  In this paper, we introduce a novel Decentralized DNN Surgery (DDS) framework. We formulate the partition strategy as a min-cut and propose a resource ",
    "link": "http://arxiv.org/abs/2306.12185",
    "context": "Title: Adaptive DNN Surgery for Selfish Inference Acceleration with On-demand Edge Resource. (arXiv:2306.12185v1 [cs.GT])\nAbstract: Deep Neural Networks (DNNs) have significantly improved the accuracy of intelligent applications on mobile devices. DNN surgery, which partitions DNN processing between mobile devices and multi-access edge computing (MEC) servers, can enable real-time inference despite the computational limitations of mobile devices. However, DNN surgery faces a critical challenge: determining the optimal computing resource demand from the server and the corresponding partition strategy, while considering both inference latency and MEC server usage costs. This problem is compounded by two factors: (1) the finite computing capacity of the MEC server, which is shared among multiple devices, leading to inter-dependent demands, and (2) the shift in modern DNN architecture from chains to directed acyclic graphs (DAGs), which complicates potential solutions.  In this paper, we introduce a novel Decentralized DNN Surgery (DDS) framework. We formulate the partition strategy as a min-cut and propose a resource ",
    "path": "papers/23/06/2306.12185.json",
    "total_tokens": 845,
    "translated_title": "自私推理加速下的自适应深度神经网络手术与按需边缘资源",
    "translated_abstract": "深度神经网络（DNN）显著提高了移动设备的智能应用准确性。DNN手术将DNN处理分割在移动设备和多接入边缘计算（MEC）服务器之间，可以在移动设备计算能力有限的情况下实现实时推理。然而，DNN手术面临一个关键挑战：确定服务器的最佳计算资源需求和相应的分区策略，同时考虑推理延迟和MEC服务器使用成本。本文引入了一个新颖的分散式DNN手术（DDS）框架。我们将分区策略表述为最小割，并提出一种资源自适应动态协商 (R-ADB) 方法，以便自适应地适应变化的资源情况。",
    "tldr": "本研究提出了一个新颖的分散式DNN手术框架（DDS），并提出了一种资源自适应动态协商（R-ADB）方法，以便自适应地适应变化的资源情况。",
    "en_tdlr": "The paper introduces a novel Decentralized DNN Surgery (DDS) framework and proposes a Resource Adaptive Dynamic Bargaining (R-ADB) method to enable adaptive allocation of computing resources for DNN surgery in the face of changing resource availability."
}