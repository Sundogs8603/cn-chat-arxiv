{
    "title": "Designing explainable artificial intelligence with active inference: A framework for transparent introspection and decision-making. (arXiv:2306.04025v1 [cs.AI])",
    "abstract": "This paper investigates the prospect of developing human-interpretable, explainable artificial intelligence (AI) systems based on active inference and the free energy principle. We first provide a brief overview of active inference, and in particular, of how it applies to the modeling of decision-making, introspection, as well as the generation of overt and covert actions. We then discuss how active inference can be leveraged to design explainable AI systems, namely, by allowing us to model core features of ``introspective'' processes and by generating useful, human-interpretable models of the processes involved in decision-making. We propose an architecture for explainable AI systems using active inference. This architecture foregrounds the role of an explicit hierarchical generative model, the operation of which enables the AI system to track and explain the factors that contribute to its own decisions, and whose structure is designed to be interpretable and auditable by human users.",
    "link": "http://arxiv.org/abs/2306.04025",
    "context": "Title: Designing explainable artificial intelligence with active inference: A framework for transparent introspection and decision-making. (arXiv:2306.04025v1 [cs.AI])\nAbstract: This paper investigates the prospect of developing human-interpretable, explainable artificial intelligence (AI) systems based on active inference and the free energy principle. We first provide a brief overview of active inference, and in particular, of how it applies to the modeling of decision-making, introspection, as well as the generation of overt and covert actions. We then discuss how active inference can be leveraged to design explainable AI systems, namely, by allowing us to model core features of ``introspective'' processes and by generating useful, human-interpretable models of the processes involved in decision-making. We propose an architecture for explainable AI systems using active inference. This architecture foregrounds the role of an explicit hierarchical generative model, the operation of which enables the AI system to track and explain the factors that contribute to its own decisions, and whose structure is designed to be interpretable and auditable by human users.",
    "path": "papers/23/06/2306.04025.json",
    "total_tokens": 884,
    "translated_title": "使用主动推理设计可解释人工智能：透明内省与决策制定的框架",
    "translated_abstract": "本文研究了基于主动推理和自由能原理发展人可解读的、可解释人工智能系统的前景。我们首先提供了有关主动推理的简要概述，特别是如何应用于决策制定、内省以及产生公开和隐蔽操作的建模。然后我们讨论了如何利用主动推理来设计可解释人工智能系统，即通过允许我们对“内省”过程的核心功能进行建模并生成有用的、可解读的决策制定过程的人可理解模型。我们提出了一个使用主动推理的可解释人工智能系统架构。该架构强调了显式分层生成模型的作用，其操作可以使人工智能系统跟踪和解释有助于其决策的因素，并且其结构设计成可以被人类用户解读和审查。",
    "tldr": "本论文提出了一种使用主动推理的可解释人工智能系统的架构，可以追踪决策的因素并生成易于理解的模型，从而实现透明化内省和决策制定，提供了一种新的可解释人工智能的思路。",
    "en_tdlr": "This paper proposes an architecture for explainable AI systems using active inference, which can track the factors contributing to its own decisions and generate understandable models, enabling transparent introspection and decision-making and providing a new approach to explainable AI."
}