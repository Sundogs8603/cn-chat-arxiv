{
    "title": "Pre-Trained Language-Meaning Models for Multilingual Parsing and Generation. (arXiv:2306.00124v1 [cs.CL])",
    "abstract": "Pre-trained language models (PLMs) have achieved great success in NLP and have recently been used for tasks in computational semantics. However, these tasks do not fully benefit from PLMs since meaning representations are not explicitly included in the pre-training stage. We introduce multilingual pre-trained language-meaning models based on Discourse Representation Structures (DRSs), including meaning representations besides natural language texts in the same model, and design a new strategy to reduce the gap between the pre-training and fine-tuning objectives. Since DRSs are language neutral, cross-lingual transfer learning is adopted to further improve the performance of non-English tasks. Automatic evaluation results show that our approach achieves the best performance on both the multilingual DRS parsing and DRS-to-text generation tasks. Correlation analysis between automatic metrics and human judgements on the generation task further validates the effectiveness of our model. Huma",
    "link": "http://arxiv.org/abs/2306.00124",
    "context": "Title: Pre-Trained Language-Meaning Models for Multilingual Parsing and Generation. (arXiv:2306.00124v1 [cs.CL])\nAbstract: Pre-trained language models (PLMs) have achieved great success in NLP and have recently been used for tasks in computational semantics. However, these tasks do not fully benefit from PLMs since meaning representations are not explicitly included in the pre-training stage. We introduce multilingual pre-trained language-meaning models based on Discourse Representation Structures (DRSs), including meaning representations besides natural language texts in the same model, and design a new strategy to reduce the gap between the pre-training and fine-tuning objectives. Since DRSs are language neutral, cross-lingual transfer learning is adopted to further improve the performance of non-English tasks. Automatic evaluation results show that our approach achieves the best performance on both the multilingual DRS parsing and DRS-to-text generation tasks. Correlation analysis between automatic metrics and human judgements on the generation task further validates the effectiveness of our model. Huma",
    "path": "papers/23/06/2306.00124.json",
    "total_tokens": 1014,
    "translated_title": "面向多语言解析和生成的预训练语言-语义模型",
    "translated_abstract": "预训练语言模型（PLMs）在自然语言处理（NLP）中取得了巨大成功，并近期被用于计算语义任务。然而，这些任务并没有充分受益于PLMs，因为意义表示并未在预训练阶段显式包括。本文引入了基于语篇表示结构（DRSs）的多语言预训练语言-语义模型，将自然语言文本和意义表示同时包含在一个模型中，并设计了一种新的策略来减少预训练和微调目标之间的差距。由于DRS是语言中立的，该模型采用跨语言迁移学习进一步提高非英语任务的性能。自动评估结果表明，我们的方法在多语言DRS解析和DRS到文本生成任务上都取得了最佳性能。关于生成任务的自动度量和人类判断之间的相关性分析进一步验证了我们模型的有效性。人类评估结果还表明，我们的模型可以大大降低多语言语义注释的成本。",
    "tldr": "本文提出了一种基于DRS的多语言预训练语言-语义模型，可以同时处理自然语言文本和意义表示，并通过跨语言迁移学习来提高非英语任务的性能。实验表明，该模型在多语言DRS解析和DRS到文本生成任务上表现最佳，且可以大大降低多语言语义注释的成本。",
    "en_tdlr": "This paper proposes a multilingual pre-trained language-meaning model based on Discourse Representation Structures (DRSs), which includes both natural language texts and meaning representations in the same model, and adopts cross-lingual transfer learning to improve non-English task performance. Experimental results show that the model achieves the best performance on both multilingual DRS parsing and DRS-to-text generation tasks, and greatly reduces the annotation cost in multilingual semantic annotation."
}