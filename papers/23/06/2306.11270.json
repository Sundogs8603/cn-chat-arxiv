{
    "title": "Evaluating the Zero-shot Robustness of Instruction-tuned Language Models. (arXiv:2306.11270v2 [cs.CL] UPDATED)",
    "abstract": "Instruction fine-tuning has recently emerged as a promising approach for improving the zero-shot capabilities of Large Language Models (LLMs) on new tasks. This technique has shown particular strength in improving the performance of modestly sized LLMs, sometimes inducing performance competitive with much larger model variants. In this paper we ask two questions: (1) How sensitive are instruction-tuned models to the particular phrasings of instructions, and, (2) How can we make them more robust to such natural language variation? To answer the former, we collect a set of 319 instructions manually written by NLP practitioners for over 80 unique tasks included in widely used benchmarks, and we evaluate the variance and average performance of these instructions as compared to instruction phrasings observed during instruction fine-tuning. We find that using novel (unobserved) but appropriate instruction phrasings consistently degrades model performance, sometimes substantially so. Further,",
    "link": "http://arxiv.org/abs/2306.11270",
    "context": "Title: Evaluating the Zero-shot Robustness of Instruction-tuned Language Models. (arXiv:2306.11270v2 [cs.CL] UPDATED)\nAbstract: Instruction fine-tuning has recently emerged as a promising approach for improving the zero-shot capabilities of Large Language Models (LLMs) on new tasks. This technique has shown particular strength in improving the performance of modestly sized LLMs, sometimes inducing performance competitive with much larger model variants. In this paper we ask two questions: (1) How sensitive are instruction-tuned models to the particular phrasings of instructions, and, (2) How can we make them more robust to such natural language variation? To answer the former, we collect a set of 319 instructions manually written by NLP practitioners for over 80 unique tasks included in widely used benchmarks, and we evaluate the variance and average performance of these instructions as compared to instruction phrasings observed during instruction fine-tuning. We find that using novel (unobserved) but appropriate instruction phrasings consistently degrades model performance, sometimes substantially so. Further,",
    "path": "papers/23/06/2306.11270.json",
    "total_tokens": 870,
    "translated_title": "评估指导微调语言模型的零样本鲁棒性",
    "translated_abstract": "指导微调最近被提出作为提高大型语言模型在新任务上的零样本能力的一种有希望的方法。这种技术在改善中等大小的语言模型（LLMs）的性能方面表现出了特别的优势，有时甚至与更大的模型变种相竞争。在本文中，我们提出两个问题：（1）指导微调的模型对指导的特定措辞有多敏感，（2）如何使它们更能抵抗自然语言的变化。为了回答前一个问题，我们收集了由NLP从业者手工编写的319个指导，涵盖了广泛使用的基准测试中的80多个独特任务，并评估了这些指导与指导微调期间观察到的指导措辞之间的方差和平均性能。我们发现，使用新颖（未观察到的）但合适的指导措辞会一致地降低模型的性能，有时甚至会大幅降低。",
    "tldr": "本文评估了指导微调语言模型的零样本鲁棒性，并发现使用新颖但合适的指导措辞会降低模型性能。",
    "en_tdlr": "This paper evaluates the zero-shot robustness of instruction-tuned language models and finds that using novel but appropriate instruction phrasings degrades model performance."
}