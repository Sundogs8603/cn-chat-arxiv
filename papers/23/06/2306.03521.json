{
    "title": "Machine learning in and out of equilibrium. (arXiv:2306.03521v1 [cs.LG])",
    "abstract": "The algorithms used to train neural networks, like stochastic gradient descent (SGD), have close parallels to natural processes that navigate a high-dimensional parameter space -- for example protein folding or evolution. Our study uses a Fokker-Planck approach, adapted from statistical physics, to explore these parallels in a single, unified framework. We focus in particular on the stationary state of the system in the long-time limit, which in conventional SGD is out of equilibrium, exhibiting persistent currents in the space of network parameters. As in its physical analogues, the current is associated with an entropy production rate for any given training trajectory. The stationary distribution of these rates obeys the integral and detailed fluctuation theorems -- nonequilibrium generalizations of the second law of thermodynamics. We validate these relations in two numerical examples, a nonlinear regression network and MNIST digit classification. While the fluctuation theorems are ",
    "link": "http://arxiv.org/abs/2306.03521",
    "context": "Title: Machine learning in and out of equilibrium. (arXiv:2306.03521v1 [cs.LG])\nAbstract: The algorithms used to train neural networks, like stochastic gradient descent (SGD), have close parallels to natural processes that navigate a high-dimensional parameter space -- for example protein folding or evolution. Our study uses a Fokker-Planck approach, adapted from statistical physics, to explore these parallels in a single, unified framework. We focus in particular on the stationary state of the system in the long-time limit, which in conventional SGD is out of equilibrium, exhibiting persistent currents in the space of network parameters. As in its physical analogues, the current is associated with an entropy production rate for any given training trajectory. The stationary distribution of these rates obeys the integral and detailed fluctuation theorems -- nonequilibrium generalizations of the second law of thermodynamics. We validate these relations in two numerical examples, a nonlinear regression network and MNIST digit classification. While the fluctuation theorems are ",
    "path": "papers/23/06/2306.03521.json",
    "total_tokens": 881,
    "translated_title": "非平衡条件下的机器学习",
    "translated_abstract": "用于训练神经网络的算法，如随机梯度下降（SGD），与自然过程存在密切的相似之处，例如蛋白质折叠或进化。我们采用从统计物理学中改编的Fokker-Planck方法，在一个统一的框架中探讨这些相似之处。我们特别关注系统在长时间极限下的平衡状态，这在传统的SGD中是不平衡的，在网络参数空间中表现出持续的电流。与其物理类比一样，该电流与任何给定训练轨迹的熵产生率相关。这些率的平衡分布遵循积分和详细波动定理，即热力学第二定律的非平衡推广。我们在两个数值例子中验证了这些关系，一个是非线性回归网络，另一个是MNIST数字分类。虽然波动定理是",
    "tldr": "该研究使用Fokker-Planck方法从统计物理学的角度探讨了训练神经网络算法与自然过程之间的相似之处，并验证了其在非平衡系统中的应用，对于研究神经网络的状态及其训练有着重要的意义。",
    "en_tdlr": "This study explores the similarities between the algorithms used for training neural networks and natural processes using a Fokker-Planck approach adapted from statistical physics. The research validates their application in non-equilibrium systems and has important implications for understanding the state and training of neural networks."
}