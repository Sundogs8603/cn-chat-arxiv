{
    "title": "Smoothed $f$-Divergence Distributionally Robust Optimization: Exponential Rate Efficiency and Complexity-Free Calibration. (arXiv:2306.14041v1 [math.OC])",
    "abstract": "In data-driven optimization, sample average approximation is known to suffer from the so-called optimizer's curse that causes optimistic bias in evaluating the solution performance. This can be tackled by adding a \"margin\" to the estimated objective value, or via distributionally robust optimization (DRO), a fast-growing approach based on worst-case analysis, which gives a protective bound on the attained objective value. However, in all these existing approaches, a statistically guaranteed bound on the true solution performance either requires restrictive conditions and knowledge on the objective function complexity, or otherwise exhibits an over-conservative rate that depends on the distribution dimension. We argue that a special type of DRO offers strong theoretical advantages in regard to these challenges: It attains a statistical bound on the true solution performance that is the tightest possible in terms of exponential decay rate, for a wide class of objective functions that not",
    "link": "http://arxiv.org/abs/2306.14041",
    "context": "Title: Smoothed $f$-Divergence Distributionally Robust Optimization: Exponential Rate Efficiency and Complexity-Free Calibration. (arXiv:2306.14041v1 [math.OC])\nAbstract: In data-driven optimization, sample average approximation is known to suffer from the so-called optimizer's curse that causes optimistic bias in evaluating the solution performance. This can be tackled by adding a \"margin\" to the estimated objective value, or via distributionally robust optimization (DRO), a fast-growing approach based on worst-case analysis, which gives a protective bound on the attained objective value. However, in all these existing approaches, a statistically guaranteed bound on the true solution performance either requires restrictive conditions and knowledge on the objective function complexity, or otherwise exhibits an over-conservative rate that depends on the distribution dimension. We argue that a special type of DRO offers strong theoretical advantages in regard to these challenges: It attains a statistical bound on the true solution performance that is the tightest possible in terms of exponential decay rate, for a wide class of objective functions that not",
    "path": "papers/23/06/2306.14041.json",
    "total_tokens": 923,
    "translated_title": "平滑的$f$-散度分布鲁棒优化：指数率效率和不带复杂性的校准。",
    "translated_abstract": "在数据驱动的优化中，样本平均逼近已知存在一个所谓的优化者诅咒，会导致在评估解决方案性能时产生乐观偏差。可以通过在估计的目标值中增加“保证空间”或通过分布鲁棒优化（DRO）来解决这个问题，后者是一种快速增长的方法，基于最坏情况分析，为获得的目标价值提供了保护界限。然而，在所有这些现有方法中，对真实解决方案性能的统计保证界限要么需要对目标函数复杂性有限制性条件和知识，要么会表现出取决于分布维度的过于保守的速率。我们认为，在这些挑战方面，一种特殊类型的DRO在理论上提供了强大的优势：对于一大类目标函数，它获得了对真实解的解决方案性能的统计界限，这在指数衰减率方面是可能的，就其紧缩程度而言，要紧密得多。",
    "tldr": "分布鲁棒优化在实现统计保证界限时存在限制和保守性问题，但平滑的$f$-散度分布鲁棒优化可在指数衰减率方面实现最紧密的统计保证。"
}