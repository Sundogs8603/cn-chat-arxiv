{
    "title": "Nearly Optimal Algorithms with Sublinear Computational Complexity for Online Kernel Regression. (arXiv:2306.08320v1 [cs.LG])",
    "abstract": "The trade-off between regret and computational cost is a fundamental problem for online kernel regression, and previous algorithms worked on the trade-off can not keep optimal regret bounds at a sublinear computational complexity. In this paper, we propose two new algorithms, AOGD-ALD and NONS-ALD, which can keep nearly optimal regret bounds at a sublinear computational complexity, and give sufficient conditions under which our algorithms work. Both algorithms dynamically maintain a group of nearly orthogonal basis used to approximate the kernel mapping, and keep nearly optimal regret bounds by controlling the approximate error. The number of basis depends on the approximate error and the decay rate of eigenvalues of the kernel matrix. If the eigenvalues decay exponentially, then AOGD-ALD and NONS-ALD separately achieves a regret of $O(\\sqrt{L(f)})$ and $O(\\mathrm{d}_{\\mathrm{eff}}(\\mu)\\ln{T})$ at a computational complexity in $O(\\ln^2{T})$. If the eigenvalues decay polynomially with d",
    "link": "http://arxiv.org/abs/2306.08320",
    "context": "Title: Nearly Optimal Algorithms with Sublinear Computational Complexity for Online Kernel Regression. (arXiv:2306.08320v1 [cs.LG])\nAbstract: The trade-off between regret and computational cost is a fundamental problem for online kernel regression, and previous algorithms worked on the trade-off can not keep optimal regret bounds at a sublinear computational complexity. In this paper, we propose two new algorithms, AOGD-ALD and NONS-ALD, which can keep nearly optimal regret bounds at a sublinear computational complexity, and give sufficient conditions under which our algorithms work. Both algorithms dynamically maintain a group of nearly orthogonal basis used to approximate the kernel mapping, and keep nearly optimal regret bounds by controlling the approximate error. The number of basis depends on the approximate error and the decay rate of eigenvalues of the kernel matrix. If the eigenvalues decay exponentially, then AOGD-ALD and NONS-ALD separately achieves a regret of $O(\\sqrt{L(f)})$ and $O(\\mathrm{d}_{\\mathrm{eff}}(\\mu)\\ln{T})$ at a computational complexity in $O(\\ln^2{T})$. If the eigenvalues decay polynomially with d",
    "path": "papers/23/06/2306.08320.json",
    "total_tokens": 1225,
    "translated_title": "在线核回归的亚线性计算复杂度近似最优算法",
    "translated_abstract": "在线核回归中遗憾和计算代价之间的权衡是一个基本问题，之前的算法在权衡上努力时无法保持在亚线性计算复杂度下的最优遗憾界。本文提出两种新算法AOGD-ALD和NONS-ALD，可以在亚线性计算复杂度下保持几乎最优的遗憾度，并给出我们的算法适用的充分条件。这两个算法动态地维护一组用于近似核映射的几乎正交基，并通过控制近似误差来保持近似最优的遗憾度。基数取决于近似误差和核矩阵特征值的衰减率。如果特征值呈指数衰减，则AOGD-ALD和NONS-ALD分别在$O(\\ln^2{T})$的计算复杂度下达到$O(\\sqrt{L(f)})$和$O(\\mathrm{d}_{\\mathrm{eff}}(\\mu)\\ln{T})$的遗憾度。如果特征值呈多项式衰减，则两个算法在$O(T^{\\frac{-2}{3}}(\\ln{T})^{\\frac{4}{3}})$的计算复杂度下分别达到$O(\\sqrt{L(f)})$遗憾度和$O(\\mathrm{d}_{\\mathrm{eff}}(\\mu)(\\ln{T})^{\\frac{2}{3}})$遗憾度。",
    "tldr": "本文提出了两种新的算法AOGD-ALD和NONS-ALD，可以在亚线性计算复杂度下达到几乎最优的遗憾度，通过控制近似误差维护一组用于近似核映射的几乎正交基。",
    "en_tdlr": "This paper proposes two new algorithms, AOGD-ALD and NONS-ALD, which can achieve nearly optimal regret bounds with sublinear computational complexity in online kernel regression by controlling approximate error and maintaining a group of nearly orthogonal basis. The number of bases depends on the approximate error and the decay rate of eigenvalues of the kernel matrix. If the eigenvalues decay exponentially, the regret bounds of AOGD-ALD and NONS-ALD are respectively $O(\\sqrt{L(f)})$ and $O(\\mathrm{d}_{\\mathrm{eff}}(\\mu)\\ln{T})$ with computational complexity in $O(\\ln^2{T})$. If the eigenvalues decay polynomially with d, the regret bounds are respectively $O(\\sqrt{L(f)})$ and $O(\\mathrm{d}_{\\mathrm{eff}}(\\mu)(\\ln{T})^{\\frac{2}{3}})$ with computational complexity in $O(T^{\\frac{-2}{3}}(\\ln{T})^{\\frac{4}{3}})$."
}