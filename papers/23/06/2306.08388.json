{
    "title": "Skill-Critic: Refining Learned Skills for Reinforcement Learning. (arXiv:2306.08388v2 [cs.LG] UPDATED)",
    "abstract": "Hierarchical reinforcement learning (RL) can accelerate long-horizon decision-making by temporally abstracting a policy into multiple levels. Promising results in sparse reward environments have been seen with skills, i.e. sequences of primitive actions. Typically, a skill latent space and policy are discovered from offline data, but the resulting low-level policy can be unreliable due to low-coverage demonstrations or distribution shifts. As a solution, we propose fine-tuning the low-level policy in conjunction with high-level skill selection. Our Skill-Critic algorithm optimizes both the low and high-level policies; these policies are also initialized and regularized by the latent space learned from offline demonstrations to guide the joint policy optimization. We validate our approach in multiple sparse RL environments, including a new sparse reward autonomous racing task in Gran Turismo Sport. The experiments show that Skill-Critic's low-level policy fine-tuning and demonstration-g",
    "link": "http://arxiv.org/abs/2306.08388",
    "context": "Title: Skill-Critic: Refining Learned Skills for Reinforcement Learning. (arXiv:2306.08388v2 [cs.LG] UPDATED)\nAbstract: Hierarchical reinforcement learning (RL) can accelerate long-horizon decision-making by temporally abstracting a policy into multiple levels. Promising results in sparse reward environments have been seen with skills, i.e. sequences of primitive actions. Typically, a skill latent space and policy are discovered from offline data, but the resulting low-level policy can be unreliable due to low-coverage demonstrations or distribution shifts. As a solution, we propose fine-tuning the low-level policy in conjunction with high-level skill selection. Our Skill-Critic algorithm optimizes both the low and high-level policies; these policies are also initialized and regularized by the latent space learned from offline demonstrations to guide the joint policy optimization. We validate our approach in multiple sparse RL environments, including a new sparse reward autonomous racing task in Gran Turismo Sport. The experiments show that Skill-Critic's low-level policy fine-tuning and demonstration-g",
    "path": "papers/23/06/2306.08388.json",
    "total_tokens": 927,
    "translated_title": "Skill-Critic: 用于强化学习中学习技能的筛选与优化",
    "translated_abstract": "分层强化学习可以通过时间抽象将一个策略分为多个层次，加快长期决策的速度。在稀疏奖励的环境中，技能即原始动作的序列，已经取得了有望的结果。通常情况下，技能的潜在空间和策略是从离线数据中发现的，但由于演示覆盖范围低或分布转移，所得到的低层策略可能不可靠。因此，我们提出了一种Fine-tuning低层策略与高层技能选择相结合的解决方案。我们的Skill-Critic算法优化了低层和高层策略，并通过从离线演示数据中学习的潜在空间进行初始化和规范化，以引导联合策略优化。我们在多个稀疏强化学习环境中验证了我们的方法，包括Gran Turismo Sport中新的稀疏奖励自主赛车任务。实验表明，Skill-Critic的低层策略Fine-tuning和演示引导策略初始化显著提高了性能。",
    "tldr": "基于技能筛选与优化的Skill-Critic算法能够提高稀疏奖励环境下强化学习中低层策略的可靠性，并显著提高了性能。",
    "en_tdlr": "Skill-Critic algorithm based on skill selection and optimization can improve the reliability of low-level policies in reinforcement learning in sparse reward environments, and significantly improve performance."
}