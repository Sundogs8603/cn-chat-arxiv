{
    "title": "Prompting Large Language Models for Zero-Shot Domain Adaptation in Speech Recognition. (arXiv:2306.16007v1 [cs.CL])",
    "abstract": "The integration of Language Models (LMs) has proven to be an effective way to address domain shifts in speech recognition. However, these approaches usually require a significant amount of target domain text data for the training of LMs. Different from these methods, in this work, with only a domain-specific text prompt, we propose two zero-shot ASR domain adaptation methods using LLaMA, a 7-billion-parameter large language model (LLM). LLM is used in two ways: 1) second-pass rescoring: reranking N-best hypotheses of a given ASR system with LLaMA; 2) deep LLM-fusion: incorporating LLM into the decoder of an encoder-decoder based ASR system. Experiments show that, with only one domain prompt, both methods can effectively reduce word error rates (WER) on out-of-domain TedLium-2 and SPGISpeech datasets. Especially, the deep LLM-fusion has the advantage of better recall of entity and out-of-vocabulary words.",
    "link": "http://arxiv.org/abs/2306.16007",
    "context": "Title: Prompting Large Language Models for Zero-Shot Domain Adaptation in Speech Recognition. (arXiv:2306.16007v1 [cs.CL])\nAbstract: The integration of Language Models (LMs) has proven to be an effective way to address domain shifts in speech recognition. However, these approaches usually require a significant amount of target domain text data for the training of LMs. Different from these methods, in this work, with only a domain-specific text prompt, we propose two zero-shot ASR domain adaptation methods using LLaMA, a 7-billion-parameter large language model (LLM). LLM is used in two ways: 1) second-pass rescoring: reranking N-best hypotheses of a given ASR system with LLaMA; 2) deep LLM-fusion: incorporating LLM into the decoder of an encoder-decoder based ASR system. Experiments show that, with only one domain prompt, both methods can effectively reduce word error rates (WER) on out-of-domain TedLium-2 and SPGISpeech datasets. Especially, the deep LLM-fusion has the advantage of better recall of entity and out-of-vocabulary words.",
    "path": "papers/23/06/2306.16007.json",
    "total_tokens": 952,
    "translated_title": "提示大型语言模型在语音识别中进行零样本领域适应",
    "translated_abstract": "语言模型（LMs）的整合已被证明是解决语音识别中的领域转移的有效方法。然而，这些方法通常需要大量的目标领域文本数据来训练LMs。与这些方法不同的是，在本研究中，我们只使用一个特定领域的文本提示，提出了两种使用LLaMA（一个具有70亿参数的大型语言模型）进行零样本ASR领域适应的方法。LLaMA有两种用法：1）二次修正：使用LLaMA重新排列给定ASR系统的N个最佳假设；2）深度LLM融合：将LLaMA整合到基于编码器-解码器的ASR系统的解码器中。实验证明，只使用一个领域提示，两种方法都可以有效降低超出领域的TedLium-2和SPGISpeech数据集的词错误率（WER）。尤其是，深度LLM融合具有更好的实体和超出词汇的单词召回优势。",
    "tldr": "这篇论文提出了两种新方法在语音识别中进行零样本领域适应，通过使用大型语言模型进行二次修正和深度融合，只使用一个领域提示即可有效降低词错误率，并且有更好的实体和超出词汇的召回效果。"
}