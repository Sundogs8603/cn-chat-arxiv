{
    "title": "The Importance of Robust Features in Mitigating Catastrophic Forgetting. (arXiv:2306.17091v1 [cs.CV])",
    "abstract": "Continual learning (CL) is an approach to address catastrophic forgetting, which refers to forgetting previously learned knowledge by neural networks when trained on new tasks or data distributions. The adversarial robustness has decomposed features into robust and non-robust types and demonstrated that models trained on robust features significantly enhance adversarial robustness. However, no study has been conducted on the efficacy of robust features from the lens of the CL model in mitigating catastrophic forgetting in CL. In this paper, we introduce the CL robust dataset and train four baseline models on both the standard and CL robust datasets. Our results demonstrate that the CL models trained on the CL robust dataset experienced less catastrophic forgetting of the previously learned tasks than when trained on the standard dataset. Our observations highlight the significance of the features provided to the underlying CL models, showing that CL robust features can alleviate catast",
    "link": "http://arxiv.org/abs/2306.17091",
    "context": "Title: The Importance of Robust Features in Mitigating Catastrophic Forgetting. (arXiv:2306.17091v1 [cs.CV])\nAbstract: Continual learning (CL) is an approach to address catastrophic forgetting, which refers to forgetting previously learned knowledge by neural networks when trained on new tasks or data distributions. The adversarial robustness has decomposed features into robust and non-robust types and demonstrated that models trained on robust features significantly enhance adversarial robustness. However, no study has been conducted on the efficacy of robust features from the lens of the CL model in mitigating catastrophic forgetting in CL. In this paper, we introduce the CL robust dataset and train four baseline models on both the standard and CL robust datasets. Our results demonstrate that the CL models trained on the CL robust dataset experienced less catastrophic forgetting of the previously learned tasks than when trained on the standard dataset. Our observations highlight the significance of the features provided to the underlying CL models, showing that CL robust features can alleviate catast",
    "path": "papers/23/06/2306.17091.json",
    "total_tokens": 965,
    "translated_title": "增强鲁棒特征在减轻灾难性遗忘中的重要性",
    "translated_abstract": "连续学习（CL）是一种应对神经网络在新任务或数据分布上训练时遗忘先前学到知识的方法。鲁棒性对特征进行了分解，将其分为鲁棒和非鲁棒类型，并表明在鲁棒特征上训练的模型显著增强了对抗鲁棒性。然而，至今没有研究关于从CL模型角度评估鲁棒特征在减轻连续学习中的灾难性遗忘方面的有效性。本文介绍了CL鲁棒数据集，并在标准数据集和CL鲁棒数据集上训练了四个基准模型。我们的结果表明，在CL鲁棒数据集上训练的CL模型对先前学习任务的灾难性遗忘较少，而在标准数据集上训练的模型会遗忘较多。我们的观察强调了提供给底层CL模型的特征的重要性，显示CL鲁棒特征可以减轻灾难性遗忘。",
    "tldr": "本研究着眼于连续学习模型，通过引入鲁棒特征数据集发现，在其上训练的模型比在标准数据集上训练的模型具有更小的灾难性遗忘，从而凸显出增强鲁棒特征在减轻灾难性遗忘中的重要性。",
    "en_tdlr": "This study focuses on continual learning models and finds that models trained on robust features from a robust dataset have less catastrophic forgetting compared to models trained on a standard dataset, highlighting the importance of robust features in mitigating catastrophic forgetting."
}