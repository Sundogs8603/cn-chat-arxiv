{
    "title": "Comparison of Pre-trained Language Models for Turkish Address Parsing. (arXiv:2306.13947v1 [cs.CL])",
    "abstract": "Transformer based pre-trained models such as BERT and its variants, which are trained on large corpora, have demonstrated tremendous success for natural language processing (NLP) tasks. Most of academic works are based on the English language; however, the number of multilingual and language specific studies increase steadily. Furthermore, several studies claimed that language specific models outperform multilingual models in various tasks. Therefore, the community tends to train or fine-tune the models for the language of their case study, specifically. In this paper, we focus on Turkish maps data and thoroughly evaluate both multilingual and Turkish based BERT, DistilBERT, ELECTRA and RoBERTa. Besides, we also propose a MultiLayer Perceptron (MLP) for fine-tuning BERT in addition to the standard approach of one-layer fine-tuning. For the dataset, a mid-sized Address Parsing corpus taken with a relatively high quality is constructed. Conducted experiments on this dataset indicate that",
    "link": "http://arxiv.org/abs/2306.13947",
    "context": "Title: Comparison of Pre-trained Language Models for Turkish Address Parsing. (arXiv:2306.13947v1 [cs.CL])\nAbstract: Transformer based pre-trained models such as BERT and its variants, which are trained on large corpora, have demonstrated tremendous success for natural language processing (NLP) tasks. Most of academic works are based on the English language; however, the number of multilingual and language specific studies increase steadily. Furthermore, several studies claimed that language specific models outperform multilingual models in various tasks. Therefore, the community tends to train or fine-tune the models for the language of their case study, specifically. In this paper, we focus on Turkish maps data and thoroughly evaluate both multilingual and Turkish based BERT, DistilBERT, ELECTRA and RoBERTa. Besides, we also propose a MultiLayer Perceptron (MLP) for fine-tuning BERT in addition to the standard approach of one-layer fine-tuning. For the dataset, a mid-sized Address Parsing corpus taken with a relatively high quality is constructed. Conducted experiments on this dataset indicate that",
    "path": "papers/23/06/2306.13947.json",
    "total_tokens": 920,
    "translated_title": "面向土耳其地址解析的预训练语言模型比较研究",
    "translated_abstract": "基于Transformer的预训练模型，如BERT及其变种，通过在大型语料库上的训练，在自然语言处理（NLP）任务中取得了巨大的成功。大多数学术研究都是基于英语进行的;然而，多语言和特定语言的研究数量正在稳步增加。此外，一些研究声称，针对特定语言的模型在各种任务中优于多语言模型。因此，社区倾向于针对其案例研究的语言来训练或微调模型。本文针对土耳其地图数据，全面评估了多语言和土耳其BERT、DistilBERT、ELECTRA和RoBERTa。此外，我们还提出了一个多层感知器（MLP）用于微调BERT，以及标准的一层微调方法。对于数据集，我们构建了一个质量相对较高的中等规模地址解析语料库。在这个数据集上进行的实验表明，",
    "tldr": "本研究比较、评估了多语言和针对土耳其的BERT、DistilBERT、ELECTRA和RoBERTa模型在土耳其地址解析上的性能，结果发现针对土耳其的模型表现更佳。",
    "en_tdlr": "This paper compares and evaluates the performance of multilingual and Turkish-based BERT, DistilBERT, ELECTRA, and RoBERTa models for Turkish address parsing, and finds that the Turkish-based models outperform the multilingual models. Additionally, a MultiLayer Perceptron (MLP) for fine-tuning BERT is proposed."
}