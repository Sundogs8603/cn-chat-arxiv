{
    "title": "Delivering Inflated Explanations. (arXiv:2306.15272v1 [cs.AI])",
    "abstract": "In the quest for Explainable Artificial Intelligence (XAI) one of the questions that frequently arises given a decision made by an AI system is, ``why was the decision made in this way?'' Formal approaches to explainability build a formal model of the AI system and use this to reason about the properties of the system. Given a set of feature values for an instance to be explained, and a resulting decision, a formal abductive explanation is a set of features, such that if they take the given value will always lead to the same decision. This explanation is useful, it shows that only some features were used in making the final decision. But it is narrow, it only shows that if the selected features take their given values the decision is unchanged. It's possible that some features may change values and still lead to the same decision. In this paper we formally define inflated explanations which is a set of features, and for each feature of set of values (always including the value of the i",
    "link": "http://arxiv.org/abs/2306.15272",
    "context": "Title: Delivering Inflated Explanations. (arXiv:2306.15272v1 [cs.AI])\nAbstract: In the quest for Explainable Artificial Intelligence (XAI) one of the questions that frequently arises given a decision made by an AI system is, ``why was the decision made in this way?'' Formal approaches to explainability build a formal model of the AI system and use this to reason about the properties of the system. Given a set of feature values for an instance to be explained, and a resulting decision, a formal abductive explanation is a set of features, such that if they take the given value will always lead to the same decision. This explanation is useful, it shows that only some features were used in making the final decision. But it is narrow, it only shows that if the selected features take their given values the decision is unchanged. It's possible that some features may change values and still lead to the same decision. In this paper we formally define inflated explanations which is a set of features, and for each feature of set of values (always including the value of the i",
    "path": "papers/23/06/2306.15272.json",
    "total_tokens": 887,
    "translated_title": "提供夸大的解释",
    "translated_abstract": "在追求可解释的人工智能（XAI）的过程中，经常出现一个问题，即 AI 系统做出决策的原因是什么。解释性的正式方法建立了 AI 系统的形式模型，并利用该模型推理系统的特性。给定一个要解释的实例的特征值集和相应的决策，一个形式推理解释是一组特征，如果它们采取给定值，将始终导致同样的决策。这种解释是有用的，它显示只有一些特征被用于做出最终决策。但它是狭义的，它只显示如果选择的特征采取它们给定的值，决策就不会改变。可能有些特征的值会改变，但仍然导致相同的决策。在本文中，我们正式定义了夸大的解释，它是一组特征，对于每个特征有一组值（始终包括该特征的值）。",
    "tldr": "本文提出了夸大的解释，它是为了解释人工智能系统的决策原因所定义的一种方法。传统的解释方法只显示选择的特征对决策的影响，而夸大的解释考虑了更多特征的可能性。",
    "en_tdlr": "This paper introduces inflated explanations, a method defined to explain the reasons behind AI system decisions. Traditional approaches only show the impact of selected features on the decision, while inflated explanations consider the possibilities of more features."
}