{
    "title": "TADIL: Task-Agnostic Domain-Incremental Learning through Task-ID Inference using Transformer Nearest-Centroid Embeddings. (arXiv:2306.11955v1 [cs.LG])",
    "abstract": "Machine Learning (ML) models struggle with data that changes over time or across domains due to factors such as noise, occlusion, illumination, or frequency, unlike humans who can learn from such non independent and identically distributed data. Consequently, a Continual Learning (CL) approach is indispensable, particularly, Domain-Incremental Learning. In this paper, we propose a novel pipeline for identifying tasks in domain-incremental learning scenarios without supervision. The pipeline comprises four steps. First, we obtain base embeddings from the raw data using an existing transformer-based model. Second, we group the embedding densities based on their similarity to obtain the nearest points to each cluster centroid. Third, we train an incremental task classifier using only these few points. Finally, we leverage the lightweight computational requirements of the pipeline to devise an algorithm that decides in an online fashion when to learn a new task using the task classifier an",
    "link": "http://arxiv.org/abs/2306.11955",
    "context": "Title: TADIL: Task-Agnostic Domain-Incremental Learning through Task-ID Inference using Transformer Nearest-Centroid Embeddings. (arXiv:2306.11955v1 [cs.LG])\nAbstract: Machine Learning (ML) models struggle with data that changes over time or across domains due to factors such as noise, occlusion, illumination, or frequency, unlike humans who can learn from such non independent and identically distributed data. Consequently, a Continual Learning (CL) approach is indispensable, particularly, Domain-Incremental Learning. In this paper, we propose a novel pipeline for identifying tasks in domain-incremental learning scenarios without supervision. The pipeline comprises four steps. First, we obtain base embeddings from the raw data using an existing transformer-based model. Second, we group the embedding densities based on their similarity to obtain the nearest points to each cluster centroid. Third, we train an incremental task classifier using only these few points. Finally, we leverage the lightweight computational requirements of the pipeline to devise an algorithm that decides in an online fashion when to learn a new task using the task classifier an",
    "path": "papers/23/06/2306.11955.json",
    "total_tokens": 877,
    "translated_title": "TADIL：使用Transformer最近中心嵌入进行任务无关增量域学习中的任务识别",
    "translated_abstract": "机器学习模型在面对随时间或领域变化的数据时会出现困难，而人类可以从这些非独立同分布的数据中学习。因此，连续学习是不可或缺的，特别是域增量学习。本文提出了一个新颖的流程，在不需要监督的情况下识别增量域学习场景中的任务。该流程包含四个步骤：首先，使用现有的基于Transformer的模型从原始数据中获取基础嵌入。其次，基于它们与集群中心的相似性，分组嵌入密度以获取每个集群中心最近的点。第三，只使用这些点训练增量任务分类器。最后，利用流程的轻量级计算要求，设计了一种算法，在在线情况下使用任务分类器来决定何时学习新任务并选择该任务的数据。",
    "tldr": "本文提出了TADIL方法，使用Transformer最近中心嵌入技术，能够在任务无关的条件下，对域增量学习中的任务进行识别，并训练出一个轻量级的增量任务分类器。",
    "en_tdlr": "This paper proposes a TADIL method that uses Transformer nearest-centroid embeddings to identify tasks in domain-incremental learning scenarios without supervision, and trains a lightweight incremental task classifier."
}