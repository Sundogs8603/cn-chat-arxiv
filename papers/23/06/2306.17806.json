{
    "title": "Stay on topic with Classifier-Free Guidance. (arXiv:2306.17806v1 [cs.CL])",
    "abstract": "Classifier-Free Guidance (CFG) has recently emerged in text-to-image generation as a lightweight technique to encourage prompt-adherence in generations. In this work, we demonstrate that CFG can be used broadly as an inference-time technique in pure language modeling. We show that CFG (1) improves the performance of Pythia, GPT-2 and LLaMA-family models across an array of tasks: Q\\&A, reasoning, code generation, and machine translation, achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements equivalent to a model with twice the parameter-count; (3) can stack alongside other inference-time methods like Chain-of-Thought and Self-Consistency, yielding further improvements in difficult tasks; (4) can be used to increase the faithfulness and coherence of assistants in challenging form-driven and content-driven prompts: in a human evaluation we show a 75\\% preference for GPT4All using CFG over baseline.",
    "link": "http://arxiv.org/abs/2306.17806",
    "context": "Title: Stay on topic with Classifier-Free Guidance. (arXiv:2306.17806v1 [cs.CL])\nAbstract: Classifier-Free Guidance (CFG) has recently emerged in text-to-image generation as a lightweight technique to encourage prompt-adherence in generations. In this work, we demonstrate that CFG can be used broadly as an inference-time technique in pure language modeling. We show that CFG (1) improves the performance of Pythia, GPT-2 and LLaMA-family models across an array of tasks: Q\\&A, reasoning, code generation, and machine translation, achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements equivalent to a model with twice the parameter-count; (3) can stack alongside other inference-time methods like Chain-of-Thought and Self-Consistency, yielding further improvements in difficult tasks; (4) can be used to increase the faithfulness and coherence of assistants in challenging form-driven and content-driven prompts: in a human evaluation we show a 75\\% preference for GPT4All using CFG over baseline.",
    "path": "papers/23/06/2306.17806.json",
    "total_tokens": 889,
    "translated_title": "不使用分类器的指导下保持话题的一致性",
    "translated_abstract": "分类器无关的指导（CFG）最近在文本到图像生成中出现，作为一种轻量级技术促进生成的立即遵循。在这项工作中，我们证明CFG可以广泛用作纯语言建模的推断时间技术。我们展示了CFG在一系列任务上提高了Pythia、GPT-2和LLaMA-family模型的性能：问答，推理，代码生成和机器翻译，在LAMBADA上使用LLaMA-7B超过PaLM-540B的SOTA；（2）带来了相当于双倍参数数的模型的改进；（3）可以与其他推断时间方法如Chain-of-Thought和Self-Consistency一起使用，在困难任务中取得进一步改进；（4）可以用于增加助手在具有挑战性的形式驱动和内容驱动提示中的忠实度和连贯性：在人类评估中，我们展示了75％的用户更喜欢使用CFG的GPT4All而不是基准方法。",
    "tldr": "本论文展示了分类器无关的指导（CFG）可以作为一种推断时间技术，显著提高了纯语言建模中各种任务的性能，并能够增强助手在具有挑战性的提示中的准确性和一致性。",
    "en_tdlr": "This paper demonstrates that Classifier-Free Guidance (CFG) can be used as an inference-time technique in language modeling, significantly improving performance across various tasks and enhancing assistant's accuracy and coherence in challenging prompts."
}