{
    "title": "PoET: A generative model of protein families as sequences-of-sequences. (arXiv:2306.06156v1 [q-bio.QM])",
    "abstract": "Generative protein language models are a natural way to design new proteins with desired functions. However, current models are either difficult to direct to produce a protein from a specific family of interest, or must be trained on a large multiple sequence alignment (MSA) from the specific family of interest, making them unable to benefit from transfer learning across families. To address this, we propose $\\textbf{P}$r$\\textbf{o}$tein $\\textbf{E}$volutionary $\\textbf{T}$ransformer (PoET), an autoregressive generative model of whole protein families that learns to generate sets of related proteins as sequences-of-sequences across tens of millions of natural protein sequence clusters. PoET can be used as a retrieval-augmented language model to generate and score arbitrary modifications conditioned on any protein family of interest, and can extrapolate from short context lengths to generalize well even for small families. This is enabled by a unique Transformer layer; we model tokens s",
    "link": "http://arxiv.org/abs/2306.06156",
    "context": "Title: PoET: A generative model of protein families as sequences-of-sequences. (arXiv:2306.06156v1 [q-bio.QM])\nAbstract: Generative protein language models are a natural way to design new proteins with desired functions. However, current models are either difficult to direct to produce a protein from a specific family of interest, or must be trained on a large multiple sequence alignment (MSA) from the specific family of interest, making them unable to benefit from transfer learning across families. To address this, we propose $\\textbf{P}$r$\\textbf{o}$tein $\\textbf{E}$volutionary $\\textbf{T}$ransformer (PoET), an autoregressive generative model of whole protein families that learns to generate sets of related proteins as sequences-of-sequences across tens of millions of natural protein sequence clusters. PoET can be used as a retrieval-augmented language model to generate and score arbitrary modifications conditioned on any protein family of interest, and can extrapolate from short context lengths to generalize well even for small families. This is enabled by a unique Transformer layer; we model tokens s",
    "path": "papers/23/06/2306.06156.json",
    "total_tokens": 898,
    "translated_title": "PoET: 一种将蛋白质家族看作序列的生成模型",
    "translated_abstract": "生成式蛋白质语言模型是设计具有所需功能的新蛋白质的自然方法。然而，当前的模型要么难以指导其生成特定类型的蛋白质，要么必须在特定类型的蛋白质家族的大型多重序列比对上进行训练，这使得它们无法从家族之间的迁移学习中受益。为了解决这个问题，我们提出了蛋白质进化变换器（PoET），这是一种全蛋白质家族自回归生成模型，学习在数千万个天然蛋白质序列簇之间生成一系列相关的蛋白质序列。PoET可以作为一个检索增强语言模型，在任何感兴趣的蛋白质家族条件下生成和评分任意修改，而且可以从短序列长度进行外推，在小家族中也能很好地泛化。这是通过独特的Transformer层实现的；我们模拟了令牌s",
    "tldr": "PoET是一个模型，能够生成任何蛋白质家族的一系列相关蛋白质序列，可以用作检索增强语言模型生成和评分任何修改",
    "en_tdlr": "PoET is a model that can generate sets of related protein sequences for any protein family, and can be used as a retrieval-augmented language model to generate and score arbitrary modifications."
}