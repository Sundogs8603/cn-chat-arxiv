{
    "title": "The Law of Parsimony in Gradient Descent for Learning Deep Linear Networks. (arXiv:2306.01154v1 [cs.LG])",
    "abstract": "Over the past few years, an extensively studied phenomenon in training deep networks is the implicit bias of gradient descent towards parsimonious solutions. In this work, we investigate this phenomenon by narrowing our focus to deep linear networks. Through our analysis, we reveal a surprising \"law of parsimony\" in the learning dynamics when the data possesses low-dimensional structures. Specifically, we show that the evolution of gradient descent starting from orthogonal initialization only affects a minimal portion of singular vector spaces across all weight matrices. In other words, the learning process happens only within a small invariant subspace of each weight matrix, despite the fact that all weight parameters are updated throughout training. This simplicity in learning dynamics could have significant implications for both efficient training and a better understanding of deep networks. First, the analysis enables us to considerably improve training efficiency by taking advanta",
    "link": "http://arxiv.org/abs/2306.01154",
    "context": "Title: The Law of Parsimony in Gradient Descent for Learning Deep Linear Networks. (arXiv:2306.01154v1 [cs.LG])\nAbstract: Over the past few years, an extensively studied phenomenon in training deep networks is the implicit bias of gradient descent towards parsimonious solutions. In this work, we investigate this phenomenon by narrowing our focus to deep linear networks. Through our analysis, we reveal a surprising \"law of parsimony\" in the learning dynamics when the data possesses low-dimensional structures. Specifically, we show that the evolution of gradient descent starting from orthogonal initialization only affects a minimal portion of singular vector spaces across all weight matrices. In other words, the learning process happens only within a small invariant subspace of each weight matrix, despite the fact that all weight parameters are updated throughout training. This simplicity in learning dynamics could have significant implications for both efficient training and a better understanding of deep networks. First, the analysis enables us to considerably improve training efficiency by taking advanta",
    "path": "papers/23/06/2306.01154.json",
    "total_tokens": 926,
    "translated_title": "梯度下降中的简约定律与深度线性网络学习",
    "translated_abstract": "过去几年来，对于训练深度网络的一个广泛研究现象是梯度下降对简约解的隐式偏差。在本文中，我们通过将焦点缩小到深度线性网络来研究这种现象。通过我们的分析，我们揭示了在数据具有低维结构时学习动态中的一个惊人的“简约定律”。具体来说，我们发现从正交初始化开始的梯度下降的演变仅影响所有权重矩阵的极小部分奇异向量空间。换句话说，尽管所有权重参数在训练过程中都会更新，但学习过程仅发生在每个权重矩阵的一个小的不变子空间内。这种学习动态的简单性可能对有效训练和更好地理解深度网络都有重要影响。",
    "tldr": "研究发现梯度下降对于简约解有偏好，针对深度线性网络数据具有低维结构时，从正交初始化开始，演变只会影响权重矩阵的少数奇异空间向量，说明学习过程仅发生在每个权重矩阵的一个小不变空间内，深度网络的学习动态存在简化。",
    "en_tdlr": "This paper reveals a \"law of parsimony\" in the learning dynamics of deep linear networks, showing that evolution of gradient descent starting from orthogonal initialization only affects a minimal portion of singular vector spaces across all weight matrices, and that the learning process happens only within a small invariant subspace of each weight matrix in the presence of low-dimensional structures in the data, indicating a simplification in the learning dynamics of deep networks."
}