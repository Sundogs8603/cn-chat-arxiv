{
    "title": "BatchGNN: Efficient CPU-Based Distributed GNN Training on Very Large Graphs. (arXiv:2306.13814v1 [cs.LG])",
    "abstract": "We present BatchGNN, a distributed CPU system that showcases techniques that can be used to efficiently train GNNs on terabyte-sized graphs. It reduces communication overhead with macrobatching in which multiple minibatches' subgraph sampling and feature fetching are batched into one communication relay to reduce redundant feature fetches when input features are static. BatchGNN provides integrated graph partitioning and native GNN layer implementations to improve runtime, and it can cache aggregated input features to further reduce sampling overhead. BatchGNN achieves an average $3\\times$ speedup over DistDGL on three GNN models trained on OGBN graphs, outperforms the runtimes reported by distributed GPU systems $P^3$ and DistDGLv2, and scales to a terabyte-sized graph.",
    "link": "http://arxiv.org/abs/2306.13814",
    "context": "Title: BatchGNN: Efficient CPU-Based Distributed GNN Training on Very Large Graphs. (arXiv:2306.13814v1 [cs.LG])\nAbstract: We present BatchGNN, a distributed CPU system that showcases techniques that can be used to efficiently train GNNs on terabyte-sized graphs. It reduces communication overhead with macrobatching in which multiple minibatches' subgraph sampling and feature fetching are batched into one communication relay to reduce redundant feature fetches when input features are static. BatchGNN provides integrated graph partitioning and native GNN layer implementations to improve runtime, and it can cache aggregated input features to further reduce sampling overhead. BatchGNN achieves an average $3\\times$ speedup over DistDGL on three GNN models trained on OGBN graphs, outperforms the runtimes reported by distributed GPU systems $P^3$ and DistDGLv2, and scales to a terabyte-sized graph.",
    "path": "papers/23/06/2306.13814.json",
    "total_tokens": 862,
    "translated_title": "BatchGNN：高效地在超大规模图上进行基于CPU的分布式GNN训练",
    "translated_abstract": "本文介绍了BatchGNN，一个分布式CPU系统，展示了一些技术，可以用于在大规模图上高效地训练GNN。通过宏批处理（macrobatching），将多个小批量的子图采样和特征提取批处理成一个通信中继，从而减少了输入特征静态时的冗余特征提取和通信开销。BatchGNN提供集成的图分区和原生的GNN层实现，以提高运行效率，并可以缓存聚合的输入特征以进一步减少采样开销。BatchGNN在OGBN图上训练的三个GNN模型中，平均速度比DistDGL快了$3\\times$，超过了分布式GPU系统$P^3$和DistDGLv2报告的运行时，并可扩展到超大规模图。",
    "tldr": "BatchGNN是一个高效的分布式CPU系统，用于在超大规模图上训练GNN模型，通过宏批处理、集成的图分区和原生的GNN层实现以及缓存聚合的输入特征等技术，实现了高效的训练，并在多个指标上超越了其他分布式GPU系统。"
}