{
    "title": "Convergence of mean-field Langevin dynamics: Time and space discretization, stochastic gradient, and variance reduction. (arXiv:2306.07221v1 [cs.LG])",
    "abstract": "The mean-field Langevin dynamics (MFLD) is a nonlinear generalization of the Langevin dynamics that incorporates a distribution-dependent drift, and it naturally arises from the optimization of two-layer neural networks via (noisy) gradient descent. Recent works have shown that MFLD globally minimizes an entropy-regularized convex functional in the space of measures. However, all prior analyses assumed the infinite-particle or continuous-time limit, and cannot handle stochastic gradient updates. We provide an general framework to prove a uniform-in-time propagation of chaos for MFLD that takes into account the errors due to finite-particle approximation, time-discretization, and stochastic gradient approximation. To demonstrate the wide applicability of this framework, we establish quantitative convergence rate guarantees to the regularized global optimal solution under (i) a wide range of learning problems such as neural network in the mean-field regime and MMD minimization, and (ii) ",
    "link": "http://arxiv.org/abs/2306.07221",
    "context": "Title: Convergence of mean-field Langevin dynamics: Time and space discretization, stochastic gradient, and variance reduction. (arXiv:2306.07221v1 [cs.LG])\nAbstract: The mean-field Langevin dynamics (MFLD) is a nonlinear generalization of the Langevin dynamics that incorporates a distribution-dependent drift, and it naturally arises from the optimization of two-layer neural networks via (noisy) gradient descent. Recent works have shown that MFLD globally minimizes an entropy-regularized convex functional in the space of measures. However, all prior analyses assumed the infinite-particle or continuous-time limit, and cannot handle stochastic gradient updates. We provide an general framework to prove a uniform-in-time propagation of chaos for MFLD that takes into account the errors due to finite-particle approximation, time-discretization, and stochastic gradient approximation. To demonstrate the wide applicability of this framework, we establish quantitative convergence rate guarantees to the regularized global optimal solution under (i) a wide range of learning problems such as neural network in the mean-field regime and MMD minimization, and (ii) ",
    "path": "papers/23/06/2306.07221.json",
    "total_tokens": 982,
    "translated_title": "均场 Langevin 动力学的收敛性: 时间和空间离散化，随机梯度和方差减少",
    "translated_abstract": "均场 Langevin 动力学（MFLD）是 Langevin 动力学的非线性推广，它包含一个分布相关的漂移，并通过（带噪）梯度下降从两层神经网络的优化中自然产生。最近的研究表明，MFLD 在测度空间中全局最小化熵正则化的凸泛函。然而，以前的所有分析都假设了无限粒子或连续时间极限，并且无法处理随机梯度更新。我们提供了一个通用框架，证明了考虑到有限粒子逼近误差、时间离散化和随机梯度逼近的 MFLD 在时间上一致收敛。为了展示该框架的广泛适用性，我们建立了量化收敛速率保证，以获得（i）像在均场区域的神经网络和 MMD 最小化这样的广泛学习问题的正则全局最优解，以及（ii）",
    "tldr": "该论文研究了均场 Langevin 动力学的收敛性，并通过一个通用框架证明了考虑到有限粒子逼近误差、时间离散化和随机梯度逼近的 MFLD 在时间上一致收敛，为神经网络和 MMD 最小化等广泛学习问题提供了量化收敛速率保证。",
    "en_tdlr": "This paper investigates the convergence of mean-field Langevin dynamics and proves a uniform-in-time convergence by providing a general framework that takes into account the errors due to finite-particle approximation, time-discretization, and stochastic gradient approximation, offering quantitative convergence rate guarantees for a range of learning problems such as neural networks and MMD minimization."
}