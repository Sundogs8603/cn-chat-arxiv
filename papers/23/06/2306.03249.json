{
    "title": "Probabilistic Unrolling: Scalable, Inverse-Free Maximum Likelihood Estimation for Latent Gaussian Models. (arXiv:2306.03249v1 [cs.LG])",
    "abstract": "Latent Gaussian models have a rich history in statistics and machine learning, with applications ranging from factor analysis to compressed sensing to time series analysis. The classical method for maximizing the likelihood of these models is the expectation-maximization (EM) algorithm. For problems with high-dimensional latent variables and large datasets, EM scales poorly because it needs to invert as many large covariance matrices as the number of data points. We introduce probabilistic unrolling, a method that combines Monte Carlo sampling with iterative linear solvers to circumvent matrix inversion. Our theoretical analyses reveal that unrolling and backpropagation through the iterations of the solver can accelerate gradient estimation for maximum likelihood estimation. In experiments on simulated and real data, we demonstrate that probabilistic unrolling learns latent Gaussian models up to an order of magnitude faster than gradient EM, with minimal losses in model performance.",
    "link": "http://arxiv.org/abs/2306.03249",
    "context": "Title: Probabilistic Unrolling: Scalable, Inverse-Free Maximum Likelihood Estimation for Latent Gaussian Models. (arXiv:2306.03249v1 [cs.LG])\nAbstract: Latent Gaussian models have a rich history in statistics and machine learning, with applications ranging from factor analysis to compressed sensing to time series analysis. The classical method for maximizing the likelihood of these models is the expectation-maximization (EM) algorithm. For problems with high-dimensional latent variables and large datasets, EM scales poorly because it needs to invert as many large covariance matrices as the number of data points. We introduce probabilistic unrolling, a method that combines Monte Carlo sampling with iterative linear solvers to circumvent matrix inversion. Our theoretical analyses reveal that unrolling and backpropagation through the iterations of the solver can accelerate gradient estimation for maximum likelihood estimation. In experiments on simulated and real data, we demonstrate that probabilistic unrolling learns latent Gaussian models up to an order of magnitude faster than gradient EM, with minimal losses in model performance.",
    "path": "papers/23/06/2306.03249.json",
    "total_tokens": 914,
    "translated_title": "概率展开：用于潜在高斯模型的可扩展、无反演最大似然估计",
    "translated_abstract": "潜在高斯模型在统计学和机器学习中有着悠久的历史，应用范围从因子分析到压缩感知再到时间序列分析。最大化这些模型的似然函数的传统方法是期望最大化(EM)算法。对于具有高维潜在变量和大型数据集的问题，由于需要求解与数据点数量一样多的大型协方差矩阵的逆矩阵，因此EM的可扩展性很差。我们引入了概率展开，这是一种将蒙特卡罗抽样与迭代线性求解器相结合的方法，可以绕过矩阵求逆。我们的理论分析揭示了展开和反向传播通过求解器的迭代可以加速最大似然估计的梯度估计。在模拟和真实数据的实验中，我们展示了概率展开学习潜在高斯模型的速度比梯度EM快一个数量级，而模型性能的损失很小。",
    "tldr": "概率展开是用于可扩展、无反演最大似然估计的方法，可以在学习潜在高斯模型时比传统的期望最大化算法快一个数量级，而模型性能损失很小。",
    "en_tdlr": "Probabilistic unrolling is a method for scalable, inverse-free maximum likelihood estimation for latent Gaussian models, which learns these models up to an order of magnitude faster than the traditional expectation-maximization algorithm, with minimal losses in model performance."
}