{
    "title": "Timing Process Interventions with Causal Inference and Reinforcement Learning. (arXiv:2306.04299v1 [cs.LG])",
    "abstract": "The shift from the understanding and prediction of processes to their optimization offers great benefits to businesses and other organizations. Precisely timed process interventions are the cornerstones of effective optimization. Prescriptive process monitoring (PresPM) is the sub-field of process mining that concentrates on process optimization. The emerging PresPM literature identifies state-of-the-art methods, causal inference (CI) and reinforcement learning (RL), without presenting a quantitative comparison. Most experiments are carried out using historical data, causing problems with the accuracy of the methods' evaluations and preempting online RL. Our contribution consists of experiments on timed process interventions with synthetic data that renders genuine online RL and the comparison to CI possible, and allows for an accurate evaluation of the results. Our experiments reveal that RL's policies outperform those from CI and are more robust at the same time. Indeed, the RL polic",
    "link": "http://arxiv.org/abs/2306.04299",
    "context": "Title: Timing Process Interventions with Causal Inference and Reinforcement Learning. (arXiv:2306.04299v1 [cs.LG])\nAbstract: The shift from the understanding and prediction of processes to their optimization offers great benefits to businesses and other organizations. Precisely timed process interventions are the cornerstones of effective optimization. Prescriptive process monitoring (PresPM) is the sub-field of process mining that concentrates on process optimization. The emerging PresPM literature identifies state-of-the-art methods, causal inference (CI) and reinforcement learning (RL), without presenting a quantitative comparison. Most experiments are carried out using historical data, causing problems with the accuracy of the methods' evaluations and preempting online RL. Our contribution consists of experiments on timed process interventions with synthetic data that renders genuine online RL and the comparison to CI possible, and allows for an accurate evaluation of the results. Our experiments reveal that RL's policies outperform those from CI and are more robust at the same time. Indeed, the RL polic",
    "path": "papers/23/06/2306.04299.json",
    "total_tokens": 851,
    "translated_title": "用因果推断和强化学习进行时间过程干预。",
    "translated_abstract": "从理解和预测过程到其优化，这场转变为企业和其他组织带来了巨大的利益。精确计时的过程干预是有效优化的基石。处方式过程监控(PresPM)是过程挖掘的子领域，专注于过程优化。新兴的PresPM文献确定了最先进的方法，因果推断(CI)和强化学习(RL)，但没有进行定量比较。大多数实验是使用历史数据进行的，导致评估方法的准确性问题，并阻止了在线RL。我们的贡献是通过使用合成数据对定时过程干预进行实验证明，使得真正的在线RL和与CI的比较成为可能，并允许对结果进行准确评估。我们的实验揭示了RL的策略胜过CI的策略，并且同时更加稳健。事实上，RL策略的结果比CI更可靠。",
    "tldr": "该论文使用合成数据进行实验，比较了因果推断和强化学习方法在定时过程干预方面的优劣。实验结果表明，强化学习在效果和稳定性方面均优于因果推断方法。",
    "en_tdlr": "This paper conducts experiments using synthetic data to compare the effectiveness of causal inference and reinforcement learning in timed process interventions, and finds that reinforcement learning outperforms causal inference in both effectiveness and stability."
}