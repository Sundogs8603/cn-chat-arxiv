{
    "title": "Masked Autoencoders with Multi-Window Local-Global Attention Are Better Audio Learners. (arXiv:2306.00561v2 [cs.SD] UPDATED)",
    "abstract": "In this work, we propose a Multi-Window Masked Autoencoder (MW-MAE) fitted with a novel Multi-Window Multi-Head Attention (MW-MHA) module that facilitates the modelling of local-global interactions in every decoder transformer block through attention heads of several distinct local and global windows. Empirical results on ten downstream audio tasks show that MW-MAEs consistently outperform standard MAEs in overall performance and learn better general-purpose audio representations, along with demonstrating considerably better scaling characteristics. Investigating attention distances and entropies reveals that MW-MAE encoders learn heads with broader local and global attention. Analyzing attention head feature representations through Projection Weighted Canonical Correlation Analysis (PWCCA) shows that attention heads with the same window sizes across the decoder layers of the MW-MAE learn correlated feature representations which enables each block to independently capture local and glo",
    "link": "http://arxiv.org/abs/2306.00561",
    "context": "Title: Masked Autoencoders with Multi-Window Local-Global Attention Are Better Audio Learners. (arXiv:2306.00561v2 [cs.SD] UPDATED)\nAbstract: In this work, we propose a Multi-Window Masked Autoencoder (MW-MAE) fitted with a novel Multi-Window Multi-Head Attention (MW-MHA) module that facilitates the modelling of local-global interactions in every decoder transformer block through attention heads of several distinct local and global windows. Empirical results on ten downstream audio tasks show that MW-MAEs consistently outperform standard MAEs in overall performance and learn better general-purpose audio representations, along with demonstrating considerably better scaling characteristics. Investigating attention distances and entropies reveals that MW-MAE encoders learn heads with broader local and global attention. Analyzing attention head feature representations through Projection Weighted Canonical Correlation Analysis (PWCCA) shows that attention heads with the same window sizes across the decoder layers of the MW-MAE learn correlated feature representations which enables each block to independently capture local and glo",
    "path": "papers/23/06/2306.00561.json",
    "total_tokens": 864,
    "translated_title": "多窗口本地-全局注意力的掩码自编码器是更好的音频学习器",
    "translated_abstract": "在这项工作中，我们提出了一种配备了新型多窗口多头注意力模块的多窗口掩码自编码器（MW-MAE），通过几个不同的本地和全局窗口的注意力头，有助于在每个解码器变压器块中对局部-全局交互进行建模。在十个下游音频任务的实证结果表明，MW-MAEs在整体性能上始终优于标准MAEs，并学习到更好的通用音频表示，同时显示出更好的可扩展性。通过研究注意距离和熵，发现MW-MAE编码器学习到具有更宽广的本地和全局注意力的头部。通过投影加权典型相关分析（PWCCA）分析注意头特征表示，显示MW-MAE的解码器层中具有相同窗口大小的注意力头学习到相关的特征表示，这使得每个块能够独立捕捉到本地和全局信息。",
    "tldr": "多窗口本地-全局注意力的掩码自编码器（MW-MAE）在音频学习任务中表现出更好的性能和通用表示能力，并具有更好的可扩展性。",
    "en_tdlr": "Masked Autoencoders with Multi-Window Local-Global Attention (MW-MAE) demonstrate better performance and general-purpose audio representations, as well as improved scalability, in audio learning tasks."
}