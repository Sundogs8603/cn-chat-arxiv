{
    "title": "Generalized Implicit Follow-The-Regularized-Leader. (arXiv:2306.00201v1 [cs.LG])",
    "abstract": "We propose a new class of online learning algorithms, generalized implicit Follow-The-Regularized-Leader (FTRL), that expands the scope of FTRL framework. Generalized implicit FTRL can recover known algorithms, as FTRL with linearized losses and implicit FTRL, and it allows the design of new update rules, as extensions of aProx and Mirror-Prox to FTRL. Our theory is constructive in the sense that it provides a simple unifying framework to design updates that directly improve the worst-case upper bound on the regret. The key idea is substituting the linearization of the losses with a Fenchel-Young inequality. We show the flexibility of the framework by proving that some known algorithms, like the Mirror-Prox updates, are instantiations of the generalized implicit FTRL. Finally, the new framework allows us to recover the temporal variation bound of implicit OMD, with the same computational complexity.",
    "link": "http://arxiv.org/abs/2306.00201",
    "context": "Title: Generalized Implicit Follow-The-Regularized-Leader. (arXiv:2306.00201v1 [cs.LG])\nAbstract: We propose a new class of online learning algorithms, generalized implicit Follow-The-Regularized-Leader (FTRL), that expands the scope of FTRL framework. Generalized implicit FTRL can recover known algorithms, as FTRL with linearized losses and implicit FTRL, and it allows the design of new update rules, as extensions of aProx and Mirror-Prox to FTRL. Our theory is constructive in the sense that it provides a simple unifying framework to design updates that directly improve the worst-case upper bound on the regret. The key idea is substituting the linearization of the losses with a Fenchel-Young inequality. We show the flexibility of the framework by proving that some known algorithms, like the Mirror-Prox updates, are instantiations of the generalized implicit FTRL. Finally, the new framework allows us to recover the temporal variation bound of implicit OMD, with the same computational complexity.",
    "path": "papers/23/06/2306.00201.json",
    "total_tokens": 927,
    "translated_title": "广义隐式Follow-The-Regularized-Leader算法",
    "translated_abstract": "我们提出了一类新的在线学习算法，称为广义隐式Follow-The-Regularized-Leader (FTRL)，它扩展了FTRL框架的范围。广义隐式FTRL可以恢复已知的算法，如线性损失的FTRL和隐式FTRL，并允许设计新的更新规则，例如aProx和Mirror-Prox的扩展到FTRL。我们的理论是建设性的，因为它提供了一个简单的统一框架，以设计直接改进最坏情况后悔的上限的更新。关键思想是用Fenchel-Young不等式代替损失的线性化。我们通过证明一些已知的算法，如Mirror-Prox更新，是广义隐式FTRL的实例，展示了框架的灵活性。最后，新框架使我们能够恢复隐式OMD的时间变化界，同时具有相同的计算复杂度。",
    "tldr": "提出了一种新的在线学习算法——广义隐式Follow-The-Regularized-Leader (FTRL)，它可以恢复已知的算法，如线性损失的FTRL和隐式FTRL，并允许设计新的更新规则。该算法的关键思想是用Fenchel-Young不等式代替损失的线性化，可以直接改进最坏情况下的后悔上限。",
    "en_tdlr": "A new class of online learning algorithm, generalized implicit Follow-The-Regularized-Leader (FTRL), is proposed, which expands the scope of FTRL framework, recovers known algorithms, and allows the design of new update rules. The key idea is substituting the linearization of the losses with a Fenchel-Young inequality. It can directly improve the worst-case upper bound on the regret."
}