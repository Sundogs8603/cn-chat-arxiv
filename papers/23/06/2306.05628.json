{
    "title": "Quantifying the Knowledge in GNNs for Reliable Distillation into MLPs. (arXiv:2306.05628v1 [cs.LG])",
    "abstract": "To bridge the gaps between topology-aware Graph Neural Networks (GNNs) and inference-efficient Multi-Layer Perceptron (MLPs), GLNN proposes to distill knowledge from a well-trained teacher GNN into a student MLP. Despite their great progress, comparatively little work has been done to explore the reliability of different knowledge points (nodes) in GNNs, especially their roles played during distillation. In this paper, we first quantify the knowledge reliability in GNN by measuring the invariance of their information entropy to noise perturbations, from which we observe that different knowledge points (1) show different distillation speeds (temporally); (2) are differentially distributed in the graph (spatially). To achieve reliable distillation, we propose an effective approach, namely Knowledge-inspired Reliable Distillation (KRD), that models the probability of each node being an informative and reliable knowledge point, based on which we sample a set of additional reliable knowledg",
    "link": "http://arxiv.org/abs/2306.05628",
    "context": "Title: Quantifying the Knowledge in GNNs for Reliable Distillation into MLPs. (arXiv:2306.05628v1 [cs.LG])\nAbstract: To bridge the gaps between topology-aware Graph Neural Networks (GNNs) and inference-efficient Multi-Layer Perceptron (MLPs), GLNN proposes to distill knowledge from a well-trained teacher GNN into a student MLP. Despite their great progress, comparatively little work has been done to explore the reliability of different knowledge points (nodes) in GNNs, especially their roles played during distillation. In this paper, we first quantify the knowledge reliability in GNN by measuring the invariance of their information entropy to noise perturbations, from which we observe that different knowledge points (1) show different distillation speeds (temporally); (2) are differentially distributed in the graph (spatially). To achieve reliable distillation, we propose an effective approach, namely Knowledge-inspired Reliable Distillation (KRD), that models the probability of each node being an informative and reliable knowledge point, based on which we sample a set of additional reliable knowledg",
    "path": "papers/23/06/2306.05628.json",
    "total_tokens": 1015,
    "translated_title": "量化GNN中的知识以实现可靠的蒸馏到MLP",
    "translated_abstract": "为了弥合有拓扑感知的图神经网络（GNN）和推理高效的多层感知器（MLP）之间的差距，GLNN提议从经过充分训练的Teacher GNN中蒸馏出知识到Student MLP中。本文不同于其他工作探索了GNN中不同知识点（节点）的可靠性，特别是在蒸馏过程中扮演的角色方面。我们首先通过计算信息熵对噪声扰动的不变性来量化GNN中的知识可靠性，从中我们观察到不同的知识点：（1）显示出不同的蒸馏速度（时间性）；（2）在图中分布不均匀（空间方面）。为了实现可靠蒸馏，我们提出了一种有效的方法，即基于知识启发的可靠蒸馏（KRD），它基于对节点成为信息丰富、可靠的知识点的概率建模，从中采样一组额外的可靠知识点以实现更好的蒸馏。各种基准数据集上的实验结果显示了KRD相对于现有技术水平的优越性。",
    "tldr": "该论文提出了一种基于知识可靠性量化的方法，并通过采样一组额外的可靠知识点，实现了GNN向MLP的可靠蒸馏，从而提高了蒸馏效率和效果。",
    "en_tdlr": "This paper proposes a method for quantifying the reliability of knowledge in GNNs and achieves reliable distillation from GNN to MLP by sampling a set of additional reliable knowledge points based on the probability of each node being informative and reliable. The results on benchmark datasets demonstrate the superiority of the proposed approach over state-of-the-art baselines."
}