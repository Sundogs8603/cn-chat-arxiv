{
    "title": "Decentralized Multi-Agent Reinforcement Learning with Global State Prediction. (arXiv:2306.12926v1 [cs.RO])",
    "abstract": "Deep reinforcement learning (DRL) has seen remarkable success in the control of single robots. However, applying DRL to robot swarms presents significant challenges. A critical challenge is non-stationarity, which occurs when two or more robots update individual or shared policies concurrently, thereby engaging in an interdependent training process with no guarantees of convergence. Circumventing non-stationarity typically involves training the robots with global information about other agents' states and/or actions. In contrast, in this paper we explore how to remove the need for global information. We pose our problem as a Partially Observable Markov Decision Process, due to the absence of global knowledge on other agents. Using collective transport as a testbed scenario, we study two approaches to multi-agent training. In the first, the robots exchange no messages, and are trained to rely on implicit communication through push-and-pull on the object to transport. In the second appro",
    "link": "http://arxiv.org/abs/2306.12926",
    "context": "Title: Decentralized Multi-Agent Reinforcement Learning with Global State Prediction. (arXiv:2306.12926v1 [cs.RO])\nAbstract: Deep reinforcement learning (DRL) has seen remarkable success in the control of single robots. However, applying DRL to robot swarms presents significant challenges. A critical challenge is non-stationarity, which occurs when two or more robots update individual or shared policies concurrently, thereby engaging in an interdependent training process with no guarantees of convergence. Circumventing non-stationarity typically involves training the robots with global information about other agents' states and/or actions. In contrast, in this paper we explore how to remove the need for global information. We pose our problem as a Partially Observable Markov Decision Process, due to the absence of global knowledge on other agents. Using collective transport as a testbed scenario, we study two approaches to multi-agent training. In the first, the robots exchange no messages, and are trained to rely on implicit communication through push-and-pull on the object to transport. In the second appro",
    "path": "papers/23/06/2306.12926.json",
    "total_tokens": 1095,
    "translated_title": "具有全局状态预测的分散式多智能体强化学习",
    "translated_abstract": "深度强化学习（DRL）在控制单个机器人方面取得了显着的成功。然而，将DRL应用于机器人群体存在重大挑战。其中一个关键挑战是非静态性，即当两个或更多机器人同时更新个体或共享政策时，会进入一个相互依存的培训过程，并且不保证收敛。克服非静态性通常涉及使用其他智能体的全局信息来训练机器人，例如其他智能体的状态和/或行动。相比之下，本文探讨了如何消除全局信息的需求。由于缺乏其他信息体的全局知识，我们将问题描述为部分可观察的马尔可夫决策过程。在以集体运输为测试场景的情况下，我们研究了两种多智能体培训方法。在第一种方法中，机器人不交换信息，并且被训练依靠通过推（push）和拉（pull）物体进行隐式通信。在第二种方法中，机器人彼此共享状态预测，使他们能够在没有显式通信的情况下协调行动。我们的实验表明，共享预测可以使智能体更有效地学习，同时在需要更少与环境交互的情况下实现更好的任务执行性能。",
    "tldr": "本文研究了分散式多智能体强化学习中的一个关键挑战：如何在没有全局信息的情况下有效训练机器人。我们提出了一种基于状态预测的方法，在不需要显式通信的情况下使机器人能够协调行动，实现更快更好的学习效果和任务执行性能。",
    "en_tdlr": "This paper explores the challenge of training robots effectively in decentralized multi-agent reinforcement learning without global information. The proposed approach relies on state prediction and enables robots to coordinate actions without explicit communication, achieving better task performance while requiring fewer interactions with the environment."
}