{
    "title": "Magnitude Attention-based Dynamic Pruning. (arXiv:2306.05056v1 [cs.CV])",
    "abstract": "Existing pruning methods utilize the importance of each weight based on specified criteria only when searching for a sparse structure but do not utilize it during training. In this work, we propose a novel approach \\textbf{M}agnitude \\textbf{A}ttention-based Dynamic \\textbf{P}runing (MAP) method, which applies the importance of weights throughout both the forward and backward paths to explore sparse model structures dynamically. Magnitude attention is defined based on the magnitude of weights as continuous real-valued numbers enabling a seamless transition from a redundant to an effective sparse network by promoting efficient exploration. Additionally, the attention mechanism ensures more effective updates for important layers within the sparse network. In later stages of training, our approach shifts from exploration to exploitation, exclusively updating the sparse model composed of crucial weights based on the explored structure, resulting in pruned models that not only achieve per",
    "link": "http://arxiv.org/abs/2306.05056",
    "context": "Title: Magnitude Attention-based Dynamic Pruning. (arXiv:2306.05056v1 [cs.CV])\nAbstract: Existing pruning methods utilize the importance of each weight based on specified criteria only when searching for a sparse structure but do not utilize it during training. In this work, we propose a novel approach \\textbf{M}agnitude \\textbf{A}ttention-based Dynamic \\textbf{P}runing (MAP) method, which applies the importance of weights throughout both the forward and backward paths to explore sparse model structures dynamically. Magnitude attention is defined based on the magnitude of weights as continuous real-valued numbers enabling a seamless transition from a redundant to an effective sparse network by promoting efficient exploration. Additionally, the attention mechanism ensures more effective updates for important layers within the sparse network. In later stages of training, our approach shifts from exploration to exploitation, exclusively updating the sparse model composed of crucial weights based on the explored structure, resulting in pruned models that not only achieve per",
    "path": "papers/23/06/2306.05056.json",
    "total_tokens": 922,
    "translated_title": "基于幅值注意力的动态剪枝方法",
    "translated_abstract": "现有的剪枝方法只会根据特定标准利用每个权重的重要性来搜索稀疏结构，但在训练过程中不会利用。本文提出一种新颖的方法——基于幅值注意力的动态剪枝方法(MAP)。该方法在前向和反向路径中应用权重的重要性动态地探索稀疏模型结构。通过基于权重的幅值定义幅值注意力，以连续实值数的形式使得从冗余到有效的稀疏网络的无缝转换成为可能，从而促进了高效的探索。此外，注意力机制确保了在稀疏网络中对重要层的更有效的更新。在训练的后期阶段，我们的方法从探索转向了开发，仅基于所探索出的结构更新由重要权重组成的稀疏模型，从而得到了既具有高性能又经过精简的模型。",
    "tldr": "本文提出一种基于幅值注意力的动态剪枝方法(MAP)，这种方法通过在前向和反向路径中应用权重的重要性动态地探索稀疏模型结构，从而实现了从冗余到有效的稀疏网络的无缝转换，得到了既具有高性能又经过精简的模型。"
}