{
    "title": "H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models. (arXiv:2306.14048v1 [cs.LG])",
    "abstract": "Large Language Models (LLMs), despite their recent impressive accomplishments, are notably cost-prohibitive to deploy, particularly for applications involving long-content generation, such as dialogue systems and story writing. Often, a large amount of transient state information, referred to as the KV cache, is stored in GPU memory in addition to model parameters, scaling linearly with the sequence length and batch size. In this paper, we introduce a novel approach for implementing the KV cache which significantly reduces its memory footprint. Our approach is based on the noteworthy observation that a small portion of tokens contributes most of the value when computing attention scores. We call these tokens Heavy Hitters (H$_2$). Through a comprehensive investigation, we find that (i) the emergence of H$_2$ is natural and strongly correlates with the frequent co-occurrence of tokens in the text, and (ii) removing them results in significant performance degradation. Based on these insi",
    "link": "http://arxiv.org/abs/2306.14048",
    "context": "Title: H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models. (arXiv:2306.14048v1 [cs.LG])\nAbstract: Large Language Models (LLMs), despite their recent impressive accomplishments, are notably cost-prohibitive to deploy, particularly for applications involving long-content generation, such as dialogue systems and story writing. Often, a large amount of transient state information, referred to as the KV cache, is stored in GPU memory in addition to model parameters, scaling linearly with the sequence length and batch size. In this paper, we introduce a novel approach for implementing the KV cache which significantly reduces its memory footprint. Our approach is based on the noteworthy observation that a small portion of tokens contributes most of the value when computing attention scores. We call these tokens Heavy Hitters (H$_2$). Through a comprehensive investigation, we find that (i) the emergence of H$_2$ is natural and strongly correlates with the frequent co-occurrence of tokens in the text, and (ii) removing them results in significant performance degradation. Based on these insi",
    "path": "papers/23/06/2306.14048.json",
    "total_tokens": 1077,
    "translated_title": "H$_2$O: 高效生成大型语言模型的热门元素预测器",
    "translated_abstract": "大型语言模型(LLM)在最近取得了令人瞩目的成就, 但是由于成本过高，它们特别难以用于对话系统和故事创作等需要生成长内容的应用。除了模型参数外，通常还需要在GPU内存中存储大量临时状态信息，称为KV cache，它与序列长度和批量大小呈线性关系。在本文中, 我们提出了一种新颖的实现KV cache的方法，它显著地减少了其内存占用量。我们的方法基于一个引人注目的发现，即在计算注意力分数时，小部分标记贡献最大价值。我们称这些标记为热门元素(H$_2$)。通过全面的研究，我们发现(i) H$_2$的出现是自然而然的，并且与文本中标记的频繁共现强相关；(ii)去除它们会导致明显的性能下降。基于这些见解，我们提出了H$_2$O，一种用于高效生成LLM的热门元素预测器。H$_2$O可以准确地预测给定序列中的热门元素，并因此保持一个更小的KV cache,将GPU内存消耗减少了50%。我们在几个基准数据集上的实验表明,H$_2$O可以在显著减少内存消耗的同时，实现与完整KV cache相当的性能。",
    "tldr": "本文提出了一种通过预测文本中的热门元素来减少GPU内存消耗的方法，在实验中表现良好。",
    "en_tdlr": "The paper proposes a method to reduce GPU memory consumption by predicting heavy hitters in the text, which achieves comparable performance to full KV cache in experiments."
}