{
    "title": "Transferable Curricula through Difficulty Conditioned Generators. (arXiv:2306.13028v1 [cs.AI])",
    "abstract": "Advancements in reinforcement learning (RL) have demonstrated superhuman performance in complex tasks such as Starcraft, Go, Chess etc. However, knowledge transfer from Artificial \"Experts\" to humans remain a significant challenge. A promising avenue for such transfer would be the use of curricula. Recent methods in curricula generation focuses on training RL agents efficiently, yet such methods rely on surrogate measures to track student progress, and are not suited for training robots in the real world (or more ambitiously humans). In this paper, we introduce a method named Parameterized Environment Response Model (PERM) that shows promising results in training RL agents in parameterized environments. Inspired by Item Response Theory, PERM seeks to model difficulty of environments and ability of RL agents directly. Given that RL agents and humans are trained more efficiently under the \"zone of proximal development\", our method generates a curriculum by matching the difficulty of an e",
    "link": "http://arxiv.org/abs/2306.13028",
    "context": "Title: Transferable Curricula through Difficulty Conditioned Generators. (arXiv:2306.13028v1 [cs.AI])\nAbstract: Advancements in reinforcement learning (RL) have demonstrated superhuman performance in complex tasks such as Starcraft, Go, Chess etc. However, knowledge transfer from Artificial \"Experts\" to humans remain a significant challenge. A promising avenue for such transfer would be the use of curricula. Recent methods in curricula generation focuses on training RL agents efficiently, yet such methods rely on surrogate measures to track student progress, and are not suited for training robots in the real world (or more ambitiously humans). In this paper, we introduce a method named Parameterized Environment Response Model (PERM) that shows promising results in training RL agents in parameterized environments. Inspired by Item Response Theory, PERM seeks to model difficulty of environments and ability of RL agents directly. Given that RL agents and humans are trained more efficiently under the \"zone of proximal development\", our method generates a curriculum by matching the difficulty of an e",
    "path": "papers/23/06/2306.13028.json",
    "total_tokens": 903,
    "translated_title": "基于难度条件生成器的可转移课程",
    "translated_abstract": "强化学习在复杂任务中展示了超人类水平的表现，如星际争霸、围棋、国际象棋等等。然而，将人工“专家”的知识传递给人类仍然是一个重大挑战。在课程中使用课程是一种有前途的转移途径。然而，最近的课程生成方法侧重于高效训练RL代理，然而这种方法依赖于代理人进展的代理测量标准，不能用于在现实世界中培训机器人（或更有雄心的是人类）。在本文中，我们介绍了一种名为参数化环境响应模型（PERM）的方法，该方法在参数化环境中训练RL代理表现出了有前途的结果。受项目反应理论的启发，PERM试图直接对环境难度和RL代理的能力建模。鉴于RL代理和人类在“发展的邻域”中更有效地接受训练，我们的方法通过匹配环境的难度以产生课程。",
    "tldr": "本文提出了一种名为PERM的方法，可以直接对环境难度和RL代理的能力进行建模，通过匹配环境的难度生成课程，实现了在参数化环境中训练RL代理的有前途的结果。",
    "en_tdlr": "This paper proposes a method named PERM which models the difficulty of environments and the ability of RL agents directly, generates curriculum by matching environment difficulty and has promising results in training RL agents in parameterized environments."
}