{
    "title": "Centered Self-Attention Layers. (arXiv:2306.01610v1 [cs.LG])",
    "abstract": "The self-attention mechanism in transformers and the message-passing mechanism in graph neural networks are repeatedly applied within deep learning architectures. We show that this application inevitably leads to oversmoothing, i.e., to similar representations at the deeper layers for different tokens in transformers and different nodes in graph neural networks. Based on our analysis, we present a correction term to the aggregating operator of these mechanisms. Empirically, this simple term eliminates much of the oversmoothing problem in visual transformers, obtaining performance in weakly supervised segmentation that surpasses elaborate baseline methods that introduce multiple auxiliary networks and training phrases. In graph neural networks, the correction term enables the training of very deep architectures more effectively than many recent solutions to the same problem.",
    "link": "http://arxiv.org/abs/2306.01610",
    "context": "Title: Centered Self-Attention Layers. (arXiv:2306.01610v1 [cs.LG])\nAbstract: The self-attention mechanism in transformers and the message-passing mechanism in graph neural networks are repeatedly applied within deep learning architectures. We show that this application inevitably leads to oversmoothing, i.e., to similar representations at the deeper layers for different tokens in transformers and different nodes in graph neural networks. Based on our analysis, we present a correction term to the aggregating operator of these mechanisms. Empirically, this simple term eliminates much of the oversmoothing problem in visual transformers, obtaining performance in weakly supervised segmentation that surpasses elaborate baseline methods that introduce multiple auxiliary networks and training phrases. In graph neural networks, the correction term enables the training of very deep architectures more effectively than many recent solutions to the same problem.",
    "path": "papers/23/06/2306.01610.json",
    "total_tokens": 769,
    "translated_title": "中心化自注意力层",
    "translated_abstract": "在深度学习结构中，transformer中的自注意力机制和图神经网络中的消息传递机制被反复应用。我们发现，这种应用不可避免地导致过度平滑，即在transformer中不同令牌和在图神经网络中不同节点的深层表示非常相似。基于我们的分析，我们呈现了这些机制聚合运算符的纠正项。在实证方面，这个简单的术语消除了图像transformers中凸显的问题，在弱监督分割方面获得了超过引入多个辅助网络和训练阶段的精心基准方法的性能。在图神经网络中，纠正项使得训练非常深的架构比许多最近的解决方案更有效。",
    "tldr": "本文提出了一种基于中心化自注意力层的纠错机制，它可以消除transformers中过度平滑的问题，并使图神经网络训练非常深的架构比许多最近的解决方案更有效。",
    "en_tdlr": "This paper proposes a correction mechanism based on centered self-attention layers that eliminates the oversmoothing problem in transformers and allows for more effective training of very deep architectures in graph neural networks compared to recent solutions."
}