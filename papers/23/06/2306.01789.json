{
    "title": "Edit Distance based RL for RNNT decoding. (arXiv:2306.01789v1 [cs.SD])",
    "abstract": "RNN-T is currently considered the industry standard in ASR due to its exceptional WERs in various benchmark tests and its ability to support seamless streaming and longform transcription. However, its biggest drawback lies in the significant discrepancy between its training and inference objectives. During training, RNN-T maximizes all alignment probabilities by teacher forcing, while during inference, it uses beam search which may not necessarily find the maximum probable alignment. Additionally, RNN-T's inability to experience mistakes during teacher forcing training makes it more problematic when a mistake occurs in inference. To address this issue, this paper proposes a Reinforcement Learning method that minimizes the gap between training and inference time. Our Edit Distance based RL (EDRL) approach computes rewards based on the edit distance, and trains the network at every action level. The proposed approach yielded SoTA WERs on LibriSpeech for the 600M Conformer RNN-T model.",
    "link": "http://arxiv.org/abs/2306.01789",
    "context": "Title: Edit Distance based RL for RNNT decoding. (arXiv:2306.01789v1 [cs.SD])\nAbstract: RNN-T is currently considered the industry standard in ASR due to its exceptional WERs in various benchmark tests and its ability to support seamless streaming and longform transcription. However, its biggest drawback lies in the significant discrepancy between its training and inference objectives. During training, RNN-T maximizes all alignment probabilities by teacher forcing, while during inference, it uses beam search which may not necessarily find the maximum probable alignment. Additionally, RNN-T's inability to experience mistakes during teacher forcing training makes it more problematic when a mistake occurs in inference. To address this issue, this paper proposes a Reinforcement Learning method that minimizes the gap between training and inference time. Our Edit Distance based RL (EDRL) approach computes rewards based on the edit distance, and trains the network at every action level. The proposed approach yielded SoTA WERs on LibriSpeech for the 600M Conformer RNN-T model.",
    "path": "papers/23/06/2306.01789.json",
    "total_tokens": 805,
    "translated_title": "基于编辑距离的强化学习用于RNN-T解码",
    "translated_abstract": "基于其在各种基准测试中出色的WER和支持无缝流式传输和长篇转录的能力，RNN-T目前被认为是ASR的工业标准。 然而，它最大的缺点在于训练和推理目标之间存在显着差异。为了解决这个问题，本文提出了一种最小化训练和推理时间之间差距的强化学习方法。我们的编辑距离RL (EDRL)方法基于编辑距离计算奖励，并在每个操作级别训练网络。该方法在LibriSpeech的600M Conformer RNN-T模型上获得了SoTA WERs。",
    "tldr": "本文提出了一种基于编辑距离的强化学习方法，用于最小化RNN-T训练和推理过程之间的差距，并在LibriSpeech的600M Conformer RNN-T模型上取得了最佳结果。",
    "en_tdlr": "This paper proposes an RL approach based on edit distance to minimize the gap between training and inference time for RNN-T decoding. The approach computes rewards based on edit distance and trains the network at every action level. It achieves SoTA WERs on the LibriSpeech 600M Conformer RNN-T model."
}