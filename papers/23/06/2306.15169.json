{
    "title": "Exploiting Inferential Structure in Neural Processes. (arXiv:2306.15169v1 [cs.LG])",
    "abstract": "Neural Processes (NPs) are appealing due to their ability to perform fast adaptation based on a context set. This set is encoded by a latent variable, which is often assumed to follow a simple distribution. However, in real-word settings, the context set may be drawn from richer distributions having multiple modes, heavy tails, etc. In this work, we provide a framework that allows NPs' latent variable to be given a rich prior defined by a graphical model. These distributional assumptions directly translate into an appropriate aggregation strategy for the context set. Moreover, we describe a message-passing procedure that still allows for end-to-end optimization with stochastic gradients. We demonstrate the generality of our framework by using mixture and Student-t assumptions that yield improvements in function modelling and test-time robustness.",
    "link": "http://arxiv.org/abs/2306.15169",
    "context": "Title: Exploiting Inferential Structure in Neural Processes. (arXiv:2306.15169v1 [cs.LG])\nAbstract: Neural Processes (NPs) are appealing due to their ability to perform fast adaptation based on a context set. This set is encoded by a latent variable, which is often assumed to follow a simple distribution. However, in real-word settings, the context set may be drawn from richer distributions having multiple modes, heavy tails, etc. In this work, we provide a framework that allows NPs' latent variable to be given a rich prior defined by a graphical model. These distributional assumptions directly translate into an appropriate aggregation strategy for the context set. Moreover, we describe a message-passing procedure that still allows for end-to-end optimization with stochastic gradients. We demonstrate the generality of our framework by using mixture and Student-t assumptions that yield improvements in function modelling and test-time robustness.",
    "path": "papers/23/06/2306.15169.json",
    "total_tokens": 901,
    "translated_title": "利用推理结构在神经过程中进行扩展",
    "translated_abstract": "神经过程(NPs)由于其能够基于上下文集执行快速适应而具有吸引力。这个集合由一个潜变量编码，通常假设该变量遵循一个简单的分布。然而，在现实世界的情况下，上下文集可能来自具有多个模式、重尾等丰富分布的抽样。在这项工作中，我们提供了一个框架，允许给予NPs潜变量一个由图模型定义的丰富先验。这些分布假设直接转化为适合上下文集的适当聚合策略。此外，我们描述了一种消息传递过程，仍然可以通过随机梯度进行端到端优化。我们通过使用混合和学生-t假设来证明我们框架的普适性，从而改善了函数建模和测试时间的鲁棒性。",
    "tldr": "这项工作提供了一个框架，利用推理结构在神经过程中进行扩展。我们为NPs的潜变量提供了丰富的先验，并提出了适当的上下文集聚合策略。此外，我们还描述了一种消息传递过程，可以进行端到端优化，并通过使用混合和学生-t假设改善了函数建模和测试时间的鲁棒性。"
}