{
    "title": "Building Blocks for a Complex-Valued Transformer Architecture. (arXiv:2306.09827v1 [cs.LG])",
    "abstract": "Most deep learning pipelines are built on real-valued operations to deal with real-valued inputs such as images, speech or music signals. However, a lot of applications naturally make use of complex-valued signals or images, such as MRI or remote sensing. Additionally the Fourier transform of signals is complex-valued and has numerous applications. We aim to make deep learning directly applicable to these complex-valued signals without using projections into $\\mathbb{R}^2$. Thus we add to the recent developments of complex-valued neural networks by presenting building blocks to transfer the transformer architecture to the complex domain. We present multiple versions of a complex-valued Scaled Dot-Product Attention mechanism as well as a complex-valued layer normalization. We test on a classification and a sequence generation task on the MusicNet dataset and show improved robustness to overfitting while maintaining on-par performance when compared to the real-valued transformer architec",
    "link": "http://arxiv.org/abs/2306.09827",
    "context": "Title: Building Blocks for a Complex-Valued Transformer Architecture. (arXiv:2306.09827v1 [cs.LG])\nAbstract: Most deep learning pipelines are built on real-valued operations to deal with real-valued inputs such as images, speech or music signals. However, a lot of applications naturally make use of complex-valued signals or images, such as MRI or remote sensing. Additionally the Fourier transform of signals is complex-valued and has numerous applications. We aim to make deep learning directly applicable to these complex-valued signals without using projections into $\\mathbb{R}^2$. Thus we add to the recent developments of complex-valued neural networks by presenting building blocks to transfer the transformer architecture to the complex domain. We present multiple versions of a complex-valued Scaled Dot-Product Attention mechanism as well as a complex-valued layer normalization. We test on a classification and a sequence generation task on the MusicNet dataset and show improved robustness to overfitting while maintaining on-par performance when compared to the real-valued transformer architec",
    "path": "papers/23/06/2306.09827.json",
    "total_tokens": 975,
    "translated_title": "复数变压器体系结构的构建块",
    "translated_abstract": "大多数深度学习流程都是建立在处理实值输入（如图像、语音或音乐信号）的实值操作之上。然而，许多应用程序自然使用复值信号或图像，例如MRI或遥感。此外，信号的傅里叶变换是复值的，并且具有许多应用。我们旨在使深度学习直接适用于这些复值信号，而不使用对 $ \\mathbb {R} ^2 $ 的投影。因此，我们通过提供建立块，将变压器架构转换到复数域中，来增加复值神经网络的最近发展。我们提出了多个版本的复值缩放点积注意机制，以及复值层归一化。我们在MusicNet数据集上进行了分类和序列生成任务的测试，并表现出了对过拟合的改进稳健性，同时与实值变压器体系结构相比保持了同等的性能。",
    "tldr": "该论文提出了复数变压器架构的构建块，以便将深度学习应用到处理复数信号或图像上，在不使用投影到 $ \\mathbb {R} ^2 $ 的情况下实现。该方法通过提供多个版本的复数缩放点积注意机制和复数层归一化，测试结果显示其在MusicNet数据集上的分类和序列生成任务上表现优秀，具有更好的鲁棒性和相当的性能。",
    "en_tdlr": "This paper introduces building blocks for a complex-valued transformer architecture to directly apply deep learning to complex-valued signals or images without projections into $ \\mathbb{R} ^2 $. The proposed approach presents multiple versions of a complex-valued Scaled Dot-Product Attention mechanism and a complex-valued layer normalization, and tests show improved robustness to overfitting while maintaining comparable performance to real-valued transformer architecture on MusicNet dataset for classification and sequence generation tasks."
}