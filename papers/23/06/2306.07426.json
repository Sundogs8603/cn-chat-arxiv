{
    "title": "Izindaba-Tindzaba: Machine learning news categorisation for Long and Short Text for isiZulu and Siswati. (arXiv:2306.07426v1 [cs.CL])",
    "abstract": "Local/Native South African languages are classified as low-resource languages. As such, it is essential to build the resources for these languages so that they can benefit from advances in the field of natural language processing. In this work, the focus was to create annotated news datasets for the isiZulu and Siswati native languages based on news topic classification tasks and present the findings from these baseline classification models. Due to the shortage of data for these native South African languages, the datasets that were created were augmented and oversampled to increase data size and overcome class classification imbalance. In total, four different classification models were used namely Logistic regression, Naive bayes, XGBoost and LSTM. These models were trained on three different word embeddings namely Bag-Of-Words, TFIDF and Word2vec. The results of this study showed that XGBoost, Logistic Regression and LSTM, trained from Word2vec performed better than the other combi",
    "link": "http://arxiv.org/abs/2306.07426",
    "context": "Title: Izindaba-Tindzaba: Machine learning news categorisation for Long and Short Text for isiZulu and Siswati. (arXiv:2306.07426v1 [cs.CL])\nAbstract: Local/Native South African languages are classified as low-resource languages. As such, it is essential to build the resources for these languages so that they can benefit from advances in the field of natural language processing. In this work, the focus was to create annotated news datasets for the isiZulu and Siswati native languages based on news topic classification tasks and present the findings from these baseline classification models. Due to the shortage of data for these native South African languages, the datasets that were created were augmented and oversampled to increase data size and overcome class classification imbalance. In total, four different classification models were used namely Logistic regression, Naive bayes, XGBoost and LSTM. These models were trained on three different word embeddings namely Bag-Of-Words, TFIDF and Word2vec. The results of this study showed that XGBoost, Logistic Regression and LSTM, trained from Word2vec performed better than the other combi",
    "path": "papers/23/06/2306.07426.json",
    "total_tokens": 893,
    "translated_title": "Izindaba-Tindzaba：面向isiZulu和Siswati的长短文本机器学习新闻分类",
    "translated_abstract": "本文针对南非少数民族本土语言进行了研究与实验，通过新闻主题分类任务构建并标注了isiZulu和Siswati本地语言的新闻数据集，并提出了基于这些分类模型的研究结果。鉴于这些本土南非语言数据的缺乏，我们对数据集进行了扩增和过采样以增加数据量，并解决类别分类不平衡问题。本文共使用了四种不同的分类模型，包括逻辑回归，朴素贝叶斯，XGBoost和LSTM。这些模型是基于三种不同的词嵌入方式进行训练的，包括词袋模型，TFIDF和Word2vec。研究结果显示，基于Word2vec模型的XGBoost，逻辑回归和LSTM性能优于其他模型的组合。",
    "tldr": "本研究为南非本土isiZulu和Siswati语言构建了新闻主题分类数据集，并采用扩增、过采样等方法增加数据量。基于Word2vec模型的XGBoost、逻辑回归和LSTM分类器表现优于其他模型。",
    "en_tdlr": "This research created annotated news datasets for indigenous isiZulu and Siswati languages, and used augmentation and oversampling to overcome data scarcity and class imbalance. The XGBoost, logistic regression, and LSTM classifiers trained with Word2vec outperformed other models."
}