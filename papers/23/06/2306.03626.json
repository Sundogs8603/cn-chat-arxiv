{
    "title": "Understanding Progressive Training Through the Framework of Randomized Coordinate Descent. (arXiv:2306.03626v1 [cs.LG])",
    "abstract": "We propose a Randomized Progressive Training algorithm (RPT) -- a stochastic proxy for the well-known Progressive Training method (PT) (Karras et al., 2017). Originally designed to train GANs (Goodfellow et al., 2014), PT was proposed as a heuristic, with no convergence analysis even for the simplest objective functions. On the contrary, to the best of our knowledge, RPT is the first PT-type algorithm with rigorous and sound theoretical guarantees for general smooth objective functions. We cast our method into the established framework of Randomized Coordinate Descent (RCD) (Nesterov, 2012; Richt\\'arik & Tak\\'a\\v{c}, 2014), for which (as a by-product of our investigations) we also propose a novel, simple and general convergence analysis encapsulating strongly-convex, convex and nonconvex objectives. We then use this framework to establish a convergence theory for RPT. Finally, we validate the effectiveness of our method through extensive computational experiments.",
    "link": "http://arxiv.org/abs/2306.03626",
    "context": "Title: Understanding Progressive Training Through the Framework of Randomized Coordinate Descent. (arXiv:2306.03626v1 [cs.LG])\nAbstract: We propose a Randomized Progressive Training algorithm (RPT) -- a stochastic proxy for the well-known Progressive Training method (PT) (Karras et al., 2017). Originally designed to train GANs (Goodfellow et al., 2014), PT was proposed as a heuristic, with no convergence analysis even for the simplest objective functions. On the contrary, to the best of our knowledge, RPT is the first PT-type algorithm with rigorous and sound theoretical guarantees for general smooth objective functions. We cast our method into the established framework of Randomized Coordinate Descent (RCD) (Nesterov, 2012; Richt\\'arik & Tak\\'a\\v{c}, 2014), for which (as a by-product of our investigations) we also propose a novel, simple and general convergence analysis encapsulating strongly-convex, convex and nonconvex objectives. We then use this framework to establish a convergence theory for RPT. Finally, we validate the effectiveness of our method through extensive computational experiments.",
    "path": "papers/23/06/2306.03626.json",
    "total_tokens": 921,
    "translated_title": "通过随机坐标下降框架理解渐进式训练",
    "translated_abstract": "我们提出了随机渐进式训练算法（RPT）——渐进式训练方法（PT）（Karras等，2017）的随机代理。最初设计用于训练GAN（Goodfellow等，2014），PT被提出作为一种启发式方法，即使对于最简单的目标函数也没有收敛分析。相反，据我们所知，RPT是第一个对于一般光滑目标函数具有严格和可靠理论保证的PT类型算法。我们将我们的方法转化为随机坐标下降（RCD）（Nesterov，2012；Richtárk＆Takáč，2014）的成熟框架，对此提出了一种新颖、简单和通用的收敛性分析，包括强凸、凸和非凸目标。然后我们使用这个框架来建立RPT的收敛理论。最后，我们通过广泛的计算实验证明了我们方法的有效性。",
    "tldr": "该论文提出了一种名为RPT的算法，通过随机坐标下降框架理解了渐进式训练，并对其进行了收敛性分析，为一般光滑目标函数提供了可靠理论保证。",
    "en_tdlr": "This paper proposes a stochastic algorithm called RPT, which provides a rigorous and sound theoretical guarantee for the general smooth objective functions of Progressive Training method. The algorithm is based on the framework of Randomized Coordinate Descent and its convergence is analyzed with a novel and general approach. Computational experiments show its effectiveness."
}