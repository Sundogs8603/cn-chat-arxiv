{
    "title": "Leveraging Cross-Utterance Context For ASR Decoding. (arXiv:2306.16903v1 [cs.CL])",
    "abstract": "While external language models (LMs) are often incorporated into the decoding stage of automated speech recognition systems, these models usually operate with limited context. Cross utterance information has been shown to be beneficial during second pass re-scoring, however this limits the hypothesis space based on the local information available to the first pass LM. In this work, we investigate the incorporation of long-context transformer LMs for cross-utterance decoding of acoustic models via beam search, and compare against results from n-best rescoring. Results demonstrate that beam search allows for an improved use of cross-utterance context. When evaluating on the long-format dataset AMI, results show a 0.7\\% and 0.3\\% absolute reduction on dev and test sets compared to the single-utterance setting, with improvements when including up to 500 tokens of prior context. Evaluations are also provided for Tedlium-1 with less significant improvements of around 0.1\\% absolute.",
    "link": "http://arxiv.org/abs/2306.16903",
    "context": "Title: Leveraging Cross-Utterance Context For ASR Decoding. (arXiv:2306.16903v1 [cs.CL])\nAbstract: While external language models (LMs) are often incorporated into the decoding stage of automated speech recognition systems, these models usually operate with limited context. Cross utterance information has been shown to be beneficial during second pass re-scoring, however this limits the hypothesis space based on the local information available to the first pass LM. In this work, we investigate the incorporation of long-context transformer LMs for cross-utterance decoding of acoustic models via beam search, and compare against results from n-best rescoring. Results demonstrate that beam search allows for an improved use of cross-utterance context. When evaluating on the long-format dataset AMI, results show a 0.7\\% and 0.3\\% absolute reduction on dev and test sets compared to the single-utterance setting, with improvements when including up to 500 tokens of prior context. Evaluations are also provided for Tedlium-1 with less significant improvements of around 0.1\\% absolute.",
    "path": "papers/23/06/2306.16903.json",
    "total_tokens": 890,
    "translated_title": "利用跨话语上下文进行语音识别解码",
    "translated_abstract": "尽管外部语言模型（LMs）通常被用于自动语音识别系统的解码阶段，但这些模型通常只使用有限的上下文信息。研究表明，在第二次重新评分时，跨话语信息对提升性能有益，然而这仅基于第一次语言模型可用的局部信息来限制假设空间。本文研究了通过波束搜索将长上下文转换器LMs应用于声学模型的跨话语解码，并与n-best重新评分的结果进行比较。结果表明，波束搜索可以更好地利用跨话语上下文。在长格式数据集AMI上进行评估时，与单话语设置相比，dev和test集的绝对减少分别为0.7％和0.3％，包括多达500个标记的先前上下文时还有改进。Tedlium-1也进行了评估，改进幅度约为0.1％。",
    "tldr": "本文研究了如何利用跨话语上下文来提升语音识别系统的解码性能，实验结果表明，通过波束搜索和长上下文转换器LMs可以更好地利用跨话语上下文，实现了较低的识别误差。",
    "en_tdlr": "This paper investigates how to leverage cross-utterance context to improve the decoding performance of speech recognition systems. Experimental results show that utilizing beam search and long-context transformer LMs can better utilize cross-utterance context, leading to lower recognition errors."
}