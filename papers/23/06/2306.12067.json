{
    "title": "Optimal Algorithms for Stochastic Bilevel Optimization under Relaxed Smoothness Conditions. (arXiv:2306.12067v1 [math.OC])",
    "abstract": "Stochastic Bilevel optimization usually involves minimizing an upper-level (UL) function that is dependent on the arg-min of a strongly-convex lower-level (LL) function. Several algorithms utilize Neumann series to approximate certain matrix inverses involved in estimating the implicit gradient of the UL function (hypergradient). The state-of-the-art StOchastic Bilevel Algorithm (SOBA) [16] instead uses stochastic gradient descent steps to solve the linear system associated with the explicit matrix inversion. This modification enables SOBA to match the lower bound of sample complexity for the single-level counterpart in non-convex settings. Unfortunately, the current analysis of SOBA relies on the assumption of higher-order smoothness for the UL and LL functions to achieve optimality. In this paper, we introduce a novel fully single-loop and Hessian-inversion-free algorithmic framework for stochastic bilevel optimization and present a tighter analysis under standard smoothness assumpti",
    "link": "http://arxiv.org/abs/2306.12067",
    "context": "Title: Optimal Algorithms for Stochastic Bilevel Optimization under Relaxed Smoothness Conditions. (arXiv:2306.12067v1 [math.OC])\nAbstract: Stochastic Bilevel optimization usually involves minimizing an upper-level (UL) function that is dependent on the arg-min of a strongly-convex lower-level (LL) function. Several algorithms utilize Neumann series to approximate certain matrix inverses involved in estimating the implicit gradient of the UL function (hypergradient). The state-of-the-art StOchastic Bilevel Algorithm (SOBA) [16] instead uses stochastic gradient descent steps to solve the linear system associated with the explicit matrix inversion. This modification enables SOBA to match the lower bound of sample complexity for the single-level counterpart in non-convex settings. Unfortunately, the current analysis of SOBA relies on the assumption of higher-order smoothness for the UL and LL functions to achieve optimality. In this paper, we introduce a novel fully single-loop and Hessian-inversion-free algorithmic framework for stochastic bilevel optimization and present a tighter analysis under standard smoothness assumpti",
    "path": "papers/23/06/2306.12067.json",
    "total_tokens": 864,
    "translated_title": "松弛光滑条件下随机双层优化的最优算法",
    "translated_abstract": "随机双层优化通常涉及最小化依赖于强凸下层函数的下层函数（LL）最小值的上层（UL）函数。几种算法利用Neumann级数来近似估计隐式梯度（超梯度）的矩阵逆。最先进的随机双层算法（SOBA）[16]改用随机梯度下降步骤来解决与显式矩阵反演相关的线性系统。该修改使SOBA能够在非凸设置中匹配单层对应项的样本复杂度下限。不幸的是，当前SOBA的分析依赖于UL和LL函数的高阶光滑性假设以实现最优性。本文介绍了随机双层优化的一种新型完全单循环且无Hessian反演算法框架，并在标准光滑性假设下提供了更紧密的分析。",
    "tldr": "本文提出了一种新型随机双层优化的最优算法，避免了使用高阶光滑性假设，能够更好地适应非凸设置。"
}