{
    "title": "Bayesian Learning of Optimal Policies in Markov Decision Processes with Countably Infinite State-Space",
    "abstract": "Models of many real-life applications, such as queuing models of communication networks or computing systems, have a countably infinite state-space. Algorithmic and learning procedures that have been developed to produce optimal policies mainly focus on finite state settings, and do not directly apply to these models. To overcome this lacuna, in this work we study the problem of optimal control of a family of discrete-time countable state-space Markov Decision Processes (MDPs) governed by an unknown parameter $\\theta\\in\\Theta$, and defined on a countably-infinite state space $\\mathcal X=\\mathbb{Z}_+^d$, with finite action space $\\mathcal A$, and an unbounded cost function. We take a Bayesian perspective with the random unknown parameter $\\boldsymbol{\\theta}^*$ generated via a given fixed prior distribution on $\\Theta$. To optimally control the unknown MDP, we propose an algorithm based on Thompson sampling with dynamically-sized episodes: at the beginning of each episode, the posterior",
    "link": "https://arxiv.org/abs/2306.02574",
    "context": "Title: Bayesian Learning of Optimal Policies in Markov Decision Processes with Countably Infinite State-Space\nAbstract: Models of many real-life applications, such as queuing models of communication networks or computing systems, have a countably infinite state-space. Algorithmic and learning procedures that have been developed to produce optimal policies mainly focus on finite state settings, and do not directly apply to these models. To overcome this lacuna, in this work we study the problem of optimal control of a family of discrete-time countable state-space Markov Decision Processes (MDPs) governed by an unknown parameter $\\theta\\in\\Theta$, and defined on a countably-infinite state space $\\mathcal X=\\mathbb{Z}_+^d$, with finite action space $\\mathcal A$, and an unbounded cost function. We take a Bayesian perspective with the random unknown parameter $\\boldsymbol{\\theta}^*$ generated via a given fixed prior distribution on $\\Theta$. To optimally control the unknown MDP, we propose an algorithm based on Thompson sampling with dynamically-sized episodes: at the beginning of each episode, the posterior",
    "path": "papers/23/06/2306.02574.json",
    "total_tokens": 917,
    "translated_title": "在具有可数无限状态空间的马尔可夫决策过程中的贝叶斯学习最优策略",
    "translated_abstract": "很多现实应用的模型，如通信网络或计算系统的排队模型，都具有可数无限状态空间。目前已经开发的算法和学习过程主要针对有限状态设置，并不能直接应用于这些模型。为了解决这个问题，本文研究了在一个未知参数θ∈Θ控制下的家族离散时间可数状态空间马尔可夫决策过程(MDP)的最优控制问题，该过程定义在可数无限状态空间Xυ={Z+}d上，具有有限动作空间Aυ以及无界成本函数。我们采用贝叶斯观点，将随机未知参数θ*作为给定先验分布在Θ上生成。为了最优地控制未知的MDP，我们提出了一种基于汤普森采样和动态大小片段的算法：在每个片段的开始，根据后验概率分布选择动作，并在每个片段结束时更新后验概率分布。",
    "tldr": "本文研究了马尔可夫决策过程中具有可数无限状态空间的最优控制问题，提出了基于汤普森采样和动态大小片段的算法进行贝叶斯学习，解决了在这些模型上的挑战。"
}