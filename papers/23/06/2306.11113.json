{
    "title": "Learn to Accumulate Evidence from All Training Samples: Theory and Practice. (arXiv:2306.11113v2 [cs.LG] UPDATED)",
    "abstract": "Evidential deep learning, built upon belief theory and subjective logic, offers a principled and computationally efficient way to turn a deterministic neural network uncertainty-aware. The resultant evidential models can quantify fine-grained uncertainty using the learned evidence. To ensure theoretically sound evidential models, the evidence needs to be non-negative, which requires special activation functions for model training and inference. This constraint often leads to inferior predictive performance compared to standard softmax models, making it challenging to extend them to many large-scale datasets. To unveil the real cause of this undesired behavior, we theoretically investigate evidential models and identify a fundamental limitation that explains the inferior performance: existing evidential activation functions create zero evidence regions, which prevent the model to learn from training samples falling into such regions. A deeper analysis of evidential activation functions ",
    "link": "http://arxiv.org/abs/2306.11113",
    "context": "Title: Learn to Accumulate Evidence from All Training Samples: Theory and Practice. (arXiv:2306.11113v2 [cs.LG] UPDATED)\nAbstract: Evidential deep learning, built upon belief theory and subjective logic, offers a principled and computationally efficient way to turn a deterministic neural network uncertainty-aware. The resultant evidential models can quantify fine-grained uncertainty using the learned evidence. To ensure theoretically sound evidential models, the evidence needs to be non-negative, which requires special activation functions for model training and inference. This constraint often leads to inferior predictive performance compared to standard softmax models, making it challenging to extend them to many large-scale datasets. To unveil the real cause of this undesired behavior, we theoretically investigate evidential models and identify a fundamental limitation that explains the inferior performance: existing evidential activation functions create zero evidence regions, which prevent the model to learn from training samples falling into such regions. A deeper analysis of evidential activation functions ",
    "path": "papers/23/06/2306.11113.json",
    "total_tokens": 1282,
    "translated_title": "学习从所有训练样本中累积证据：理论与实践",
    "translated_abstract": "基于置信度理论和主观逻辑的证据深度学习提供了一种原则性和计算效率高的方法，用于将确定性神经网络变为不确定性感知的模型。结果产生的证据模型可以使用学习到的证据量化细粒度不确定性。为了确保理论上合理的证据模型，证据需要是非负的，这需要使用特殊的激活函数进行模型训练和推断。这个限制通常导致预测性能不如标准的softmax模型，使得将它们扩展到许多大规模数据集具有挑战性。为了揭示这种不良行为的真正原因，我们在理论上研究证据模型，并确定了一个根本限制，它解释了不良性能：现有的证明激活函数创建了零证据区域，这阻止了模型从落入这些区域的训练样本中学习。对证据激活函数的深入分析揭示了它们可以被视为将原始输入转换为证据量，然后使用信念更新规则进行聚合的转换器。受这一观察的启发，我们提出了一种新的激活函数，称为全正激活（All-Positive，AP），它通过构造避免零证据区域，优于现有的证据激活函数在许多基准数据集上。此外，我们表明AP激活减少了ReLU激活，从而恢复了ReLU的良好正定性质，并且它的泛化允许负证据量，因此比现有的证据激活更具表现力。实验证明，我们提出的方法在几个具有挑战性的基准数据集上实现了有竞争力的性能，并优于现有方法。",
    "tldr": "本文提出了一种新的激活函数，All-Positive (AP)激活，避免了现有证据激活函数中的零证据区域，同时能够更好地表达负证据量。实验证明，该方法在多个基准数据集上优于现有方法。",
    "en_tdlr": "The paper proposes a new activation function, All-Positive (AP), that avoids zero evidence regions and allows for negative evidential masses, thereby outperforming existing evidential activations on multiple benchmark datasets. Empirical results demonstrate its competitive performance compared to existing methods."
}