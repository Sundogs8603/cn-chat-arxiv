{
    "title": "Consistent Explanations in the Face of Model Indeterminacy via Ensembling. (arXiv:2306.06193v1 [cs.LG])",
    "abstract": "This work addresses the challenge of providing consistent explanations for predictive models in the presence of model indeterminacy, which arises due to the existence of multiple (nearly) equally well-performing models for a given dataset and task. Despite their similar performance, such models often exhibit inconsistent or even contradictory explanations for their predictions, posing challenges to end users who rely on these models to make critical decisions. Recognizing this issue, we introduce ensemble methods as an approach to enhance the consistency of the explanations provided in these scenarios. Leveraging insights from recent work on neural network loss landscapes and mode connectivity, we devise ensemble strategies to efficiently explore the $\\textit{underspecification set}$ -- the set of models with performance variations resulting solely from changes in the random seed during training. Experiments on five benchmark financial datasets reveal that ensembling can yield signific",
    "link": "http://arxiv.org/abs/2306.06193",
    "context": "Title: Consistent Explanations in the Face of Model Indeterminacy via Ensembling. (arXiv:2306.06193v1 [cs.LG])\nAbstract: This work addresses the challenge of providing consistent explanations for predictive models in the presence of model indeterminacy, which arises due to the existence of multiple (nearly) equally well-performing models for a given dataset and task. Despite their similar performance, such models often exhibit inconsistent or even contradictory explanations for their predictions, posing challenges to end users who rely on these models to make critical decisions. Recognizing this issue, we introduce ensemble methods as an approach to enhance the consistency of the explanations provided in these scenarios. Leveraging insights from recent work on neural network loss landscapes and mode connectivity, we devise ensemble strategies to efficiently explore the $\\textit{underspecification set}$ -- the set of models with performance variations resulting solely from changes in the random seed during training. Experiments on five benchmark financial datasets reveal that ensembling can yield signific",
    "path": "papers/23/06/2306.06193.json",
    "total_tokens": 888,
    "translated_abstract": "本文解决了模型不确定性下提供一致解释的挑战，该不确定性源于给定数据集和任务存在多个（接近）同样表现出色的模型。尽管这些模型表现出类似的性能，但它们的解释往往是不一致甚至矛盾的，这给依赖这些模型做出关键决策的最终用户带来了挑战。为了解决这个问题，我们引入了集成方法作为一种提高这种情况下所提供的解释一致性的方法。借鉴最近神经网络损失景观和模式连接工作的见解，我们设计了集成策略来有效地探索“欠规定集”——表现出性能差异的模型集，这些差异仅仅是训练过程中随机种子的改变造成的。在五个基准金融数据集上进行的实验表明，集成可以产生显著的性能提升，并且所提供的解释更加一致。",
    "tldr": "本文研究了模型不确定性下提供一致解释的问题，并引入集成方法来提高解释一致性，实验表明集成可以显著提升性能和解释一致性。"
}