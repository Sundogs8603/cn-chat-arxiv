{
    "title": "StudentEval: A Benchmark of Student-Written Prompts for Large Language Models of Code. (arXiv:2306.04556v1 [cs.LG])",
    "abstract": "Code LLMs are being rapidly deployed and there is evidence that they can make professional programmers more productive. Current benchmarks for code generation measure whether models generate correct programs given an expert prompt. In this paper, we present a new benchmark containing multiple prompts per problem, written by a specific population of non-expert prompters: beginning programmers. StudentEval contains 1,749 prompts for 48 problems, written by 80 students who have only completed one semester of Python programming. Our students wrote these prompts while working interactively with a Code LLM, and we observed very mixed success rates. We use StudentEval to evaluate 5 Code LLMs and find that StudentEval is a better discriminator of model performance than existing benchmarks. We analyze the prompts and find significant variation in students' prompting techniques. We also find that nondeterministic LLM sampling could mislead students into thinking that their prompts are more (or l",
    "link": "http://arxiv.org/abs/2306.04556",
    "context": "Title: StudentEval: A Benchmark of Student-Written Prompts for Large Language Models of Code. (arXiv:2306.04556v1 [cs.LG])\nAbstract: Code LLMs are being rapidly deployed and there is evidence that they can make professional programmers more productive. Current benchmarks for code generation measure whether models generate correct programs given an expert prompt. In this paper, we present a new benchmark containing multiple prompts per problem, written by a specific population of non-expert prompters: beginning programmers. StudentEval contains 1,749 prompts for 48 problems, written by 80 students who have only completed one semester of Python programming. Our students wrote these prompts while working interactively with a Code LLM, and we observed very mixed success rates. We use StudentEval to evaluate 5 Code LLMs and find that StudentEval is a better discriminator of model performance than existing benchmarks. We analyze the prompts and find significant variation in students' prompting techniques. We also find that nondeterministic LLM sampling could mislead students into thinking that their prompts are more (or l",
    "path": "papers/23/06/2306.04556.json",
    "total_tokens": 949,
    "translated_title": "学生编写的问题对大规模语言模型的基准测试: StudentEval",
    "translated_abstract": "代码语言模型（LLMs）正在被快速部署，并且有证据表明它们可以提高专业程序员的生产力。目前的代码生成基准测试是通过评估模型是否可以根据专家提示生成正确的程序来进行的。本文提出了一个新的基准测试，包含每个问题的多个提示，由特定的非专业提示者（即初学者程序员）编写。StudentEval包含了由80名仅完成了一学期Python编程的学生编写的48个问题的1749个提示。我们的学生在与Code LLM互动工作时编写了这些提示，观察到成功率非常不稳定。我们使用StudentEval评估了5种Code LLM，并发现StudentEval比现有的基准测试更好地区分模型性能。我们分析了提示，并发现学生提示技巧存在显着差异。我们还发现，非确定性LLM抽样可能会让学生误以为他们的提示比实际效果更好（或更差）。",
    "tldr": "本文提出了一个新的基准测试StudentEval，由初学者编写多个提示来测试Code LLM的性能，并发现这比现有基准测试更好地区分模型性能；分析提示发现学生提示技巧差异显著，非确定性LLM抽样可能会误导学生。",
    "en_tdlr": "This paper presents a new benchmark, StudentEval, containing multiple prompts per problem, written by a specific population of non-expert prompters, to evaluate the performance of Code LLMs. StudentEval is a better discriminator of model performance than existing benchmarks. The study finds significant variation in students' prompting techniques and identifies that nondeterministic LLM sampling could mislead students into thinking their prompts are more (or less) effective than they actually are."
}