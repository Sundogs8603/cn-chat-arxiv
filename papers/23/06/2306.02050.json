{
    "title": "Provable Dynamic Fusion for Low-Quality Multimodal Data. (arXiv:2306.02050v2 [cs.LG] UPDATED)",
    "abstract": "The inherent challenge of multimodal fusion is to precisely capture the cross-modal correlation and flexibly conduct cross-modal interaction. To fully release the value of each modality and mitigate the influence of low-quality multimodal data, dynamic multimodal fusion emerges as a promising learning paradigm. Despite its widespread use, theoretical justifications in this field are still notably lacking. Can we design a provably robust multimodal fusion method? This paper provides theoretical understandings to answer this question under a most popular multimodal fusion framework from the generalization perspective. We proceed to reveal that several uncertainty estimation solutions are naturally available to achieve robust multimodal fusion. Then a novel multimodal fusion framework termed Quality-aware Multimodal Fusion (QMF) is proposed, which can improve the performance in terms of classification accuracy and model robustness. Extensive experimental results on multiple benchmarks can",
    "link": "http://arxiv.org/abs/2306.02050",
    "context": "Title: Provable Dynamic Fusion for Low-Quality Multimodal Data. (arXiv:2306.02050v2 [cs.LG] UPDATED)\nAbstract: The inherent challenge of multimodal fusion is to precisely capture the cross-modal correlation and flexibly conduct cross-modal interaction. To fully release the value of each modality and mitigate the influence of low-quality multimodal data, dynamic multimodal fusion emerges as a promising learning paradigm. Despite its widespread use, theoretical justifications in this field are still notably lacking. Can we design a provably robust multimodal fusion method? This paper provides theoretical understandings to answer this question under a most popular multimodal fusion framework from the generalization perspective. We proceed to reveal that several uncertainty estimation solutions are naturally available to achieve robust multimodal fusion. Then a novel multimodal fusion framework termed Quality-aware Multimodal Fusion (QMF) is proposed, which can improve the performance in terms of classification accuracy and model robustness. Extensive experimental results on multiple benchmarks can",
    "path": "papers/23/06/2306.02050.json",
    "total_tokens": 860,
    "translated_title": "对低质量多模态数据的可证明动态融合",
    "translated_abstract": "多模态融合的困难在于精确捕捉跨模态相关性和灵活进行跨模态交互。为了充分释放每个模态的价值并减轻低质量多模态数据的影响，动态多模态融合成为有前途的学习范式。尽管被广泛使用，但该领域仍然缺乏理论证明。本文从泛化角度的最流行的多模态融合框架出发，提供了理论上的解释。我们接着揭示，多种不确定性估计解决方法可以自然地实现健壮的多模态融合。然后，提出一种名为质量感知多模态融合（QMF）的新型多模态融合框架，可以提高分类准确性和模型的鲁棒性。在多个基准测试上进行广泛的实验结果。",
    "tldr": "本文从泛化的角度出发，提供了理论上对可证明动态多模态融合方法的解释。提出了一种质量感知多模态融合（QMF）框架，可以提高分类准确性和模型的鲁棒性。",
    "en_tdlr": "This paper provides theoretical insights into the provable robustness of dynamic multimodal fusion method. The proposed Quality-aware Multimodal Fusion (QMF) framework can improve classification accuracy and model robustness by naturally incorporating uncertainty estimation solutions."
}