{
    "title": "Graph Ladling: Shockingly Simple Parallel GNN Training without Intermediate Communication. (arXiv:2306.10466v2 [cs.LG] UPDATED)",
    "abstract": "Graphs are omnipresent and GNNs are a powerful family of neural networks for learning over graphs. Despite their popularity, scaling GNNs either by deepening or widening suffers from prevalent issues of unhealthy gradients, over-smoothening, information squashing, which often lead to sub-standard performance. In this work, we are interested in exploring a principled way to scale GNNs capacity without deepening or widening, which can improve its performance across multiple small and large graphs. Motivated by the recent intriguing phenomenon of model soups, which suggest that fine-tuned weights of multiple large-language pre-trained models can be merged to a better minima, we argue to exploit the fundamentals of model soups to mitigate the aforementioned issues of memory bottleneck and trainability during GNNs scaling. More specifically, we propose not to deepen or widen current GNNs, but instead present a data-centric perspective of model soups tailored for GNNs, i.e., to build powerfu",
    "link": "http://arxiv.org/abs/2306.10466",
    "context": "Title: Graph Ladling: Shockingly Simple Parallel GNN Training without Intermediate Communication. (arXiv:2306.10466v2 [cs.LG] UPDATED)\nAbstract: Graphs are omnipresent and GNNs are a powerful family of neural networks for learning over graphs. Despite their popularity, scaling GNNs either by deepening or widening suffers from prevalent issues of unhealthy gradients, over-smoothening, information squashing, which often lead to sub-standard performance. In this work, we are interested in exploring a principled way to scale GNNs capacity without deepening or widening, which can improve its performance across multiple small and large graphs. Motivated by the recent intriguing phenomenon of model soups, which suggest that fine-tuned weights of multiple large-language pre-trained models can be merged to a better minima, we argue to exploit the fundamentals of model soups to mitigate the aforementioned issues of memory bottleneck and trainability during GNNs scaling. More specifically, we propose not to deepen or widen current GNNs, but instead present a data-centric perspective of model soups tailored for GNNs, i.e., to build powerfu",
    "path": "papers/23/06/2306.10466.json",
    "total_tokens": 919,
    "translated_title": "图加载：令人震惊的简化并行GNN训练，无需中间通信",
    "translated_abstract": "图在各处都是存在的，而GNN是一类用于图学习的强大神经网络。尽管它们很受欢迎，但通过加深或加宽来扩展GNN还存在着梯度不健康、过度平滑、信息压缩等普遍问题，常常导致次标准的性能。在这项工作中，我们有兴趣探索一种有原则的方法，以不加深或加宽的方式扩展GNN的容量，从而提高其在多个小型和大型图上的性能。受最近引人注目的模型汤现象的启发，该现象表明多个大型预训练模型的微调权重可以合并成更好的最小值，我们提出利用模型汤的基本原理来减轻GNN扩展过程中的内存瓶颈和可训练性问题。更具体地说，我们建议不加深或加宽当前的GNN，而是提出了一种专为GNN定制的以数据为中心的模型汤视角，即构建功能强大的",
    "tldr": "这项工作提出了一种无需中间通信的简化并行GNN训练方法，通过利用模型汤的原理减轻了GNN扩展过程中的内存瓶颈和可训练性问题。",
    "en_tdlr": "This work proposes a simplified parallel GNN training method without intermediate communication, alleviating memory bottleneck and trainability issues in GNN scaling by leveraging the principles of model soups."
}