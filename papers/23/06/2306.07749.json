{
    "title": "Provably Learning Nash Policies in Constrained Markov Potential Games. (arXiv:2306.07749v1 [cs.LG])",
    "abstract": "Multi-agent reinforcement learning (MARL) addresses sequential decision-making problems with multiple agents, where each agent optimizes its own objective. In many real-world instances, the agents may not only want to optimize their objectives, but also ensure safe behavior. For example, in traffic routing, each car (agent) aims to reach its destination quickly (objective) while avoiding collisions (safety). Constrained Markov Games (CMGs) are a natural formalism for safe MARL problems, though generally intractable. In this work, we introduce and study Constrained Markov Potential Games (CMPGs), an important class of CMGs. We first show that a Nash policy for CMPGs can be found via constrained optimization. One tempting approach is to solve it by Lagrangian-based primal-dual methods. As we show, in contrast to the single-agent setting, however, CMPGs do not satisfy strong duality, rendering such approaches inapplicable and potentially unsafe. To solve the CMPG problem, we propose our a",
    "link": "http://arxiv.org/abs/2306.07749",
    "context": "Title: Provably Learning Nash Policies in Constrained Markov Potential Games. (arXiv:2306.07749v1 [cs.LG])\nAbstract: Multi-agent reinforcement learning (MARL) addresses sequential decision-making problems with multiple agents, where each agent optimizes its own objective. In many real-world instances, the agents may not only want to optimize their objectives, but also ensure safe behavior. For example, in traffic routing, each car (agent) aims to reach its destination quickly (objective) while avoiding collisions (safety). Constrained Markov Games (CMGs) are a natural formalism for safe MARL problems, though generally intractable. In this work, we introduce and study Constrained Markov Potential Games (CMPGs), an important class of CMGs. We first show that a Nash policy for CMPGs can be found via constrained optimization. One tempting approach is to solve it by Lagrangian-based primal-dual methods. As we show, in contrast to the single-agent setting, however, CMPGs do not satisfy strong duality, rendering such approaches inapplicable and potentially unsafe. To solve the CMPG problem, we propose our a",
    "path": "papers/23/06/2306.07749.json",
    "total_tokens": 981,
    "translated_title": "在约束马尔可夫潜在博弈中证明学习纳什策略",
    "translated_abstract": "多智能体强化学习（MARL）解决了多个代理的顺序决策问题，每个代理都最优化其自身的目标。在许多现实世界的实例中，不仅仅是要优化它们各自的目标，还要确保安全行为。在交通路由中，每辆车（代理）旨在快速到达目的地（目标），同时避免碰撞（安全）。约束马尔可夫博弈（CMGs）是安全MARL问题的一种自然形式，但通常难以处理。在本文中，我们介绍并研究了一种重要的CMGs类别——约束马尔可夫潜在博弈（CMPGs）。我们首先表明，可以通过约束优化找到CMPGs的纳什策略。一个试图解决问题的方法是拉格朗日基于原始 - 对偶方法。正如我们所显示出来的，与单智能体场景相反，CMPG不满足强对偶性，这使得这种方法应用不适当且可能不安全。为了解决CMPG问题，我们提出了一种新的概率算法。",
    "tldr": "本文研究了约束马尔可夫潜在博弈（CMPGs），并提出了一种寻找CMPGs的纳什策略的约束优化算法，该算法解决了单智能体模型中CMPG问题不满足强对偶性的问题",
    "en_tdlr": "This paper studies Constrained Markov Potential Games (CMPGs), and proposes a constrained optimization algorithm to find the Nash policies of CMPGs, which solves the problem of CMPGs not satisfying strong duality in the single-agent setting."
}