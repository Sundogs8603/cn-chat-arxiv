{
    "title": "Adversarial Attacks Neutralization via Data Set Randomization. (arXiv:2306.12161v1 [cs.LG])",
    "abstract": "Adversarial attacks on deep-learning models pose a serious threat to their reliability and security. Existing defense mechanisms are narrow addressing a specific type of attack or being vulnerable to sophisticated attacks. We propose a new defense mechanism that, while being focused on image-based classifiers, is general with respect to the cited category. It is rooted on hyperspace projection. In particular, our solution provides a pseudo-random projection of the original dataset into a new dataset. The proposed defense mechanism creates a set of diverse projected datasets, where each projected dataset is used to train a specific classifier, resulting in different trained classifiers with different decision boundaries. During testing, it randomly selects a classifier to test the input. Our approach does not sacrifice accuracy over legitimate input. Other than detailing and providing a thorough characterization of our defense mechanism, we also provide a proof of concept of using four ",
    "link": "http://arxiv.org/abs/2306.12161",
    "context": "Title: Adversarial Attacks Neutralization via Data Set Randomization. (arXiv:2306.12161v1 [cs.LG])\nAbstract: Adversarial attacks on deep-learning models pose a serious threat to their reliability and security. Existing defense mechanisms are narrow addressing a specific type of attack or being vulnerable to sophisticated attacks. We propose a new defense mechanism that, while being focused on image-based classifiers, is general with respect to the cited category. It is rooted on hyperspace projection. In particular, our solution provides a pseudo-random projection of the original dataset into a new dataset. The proposed defense mechanism creates a set of diverse projected datasets, where each projected dataset is used to train a specific classifier, resulting in different trained classifiers with different decision boundaries. During testing, it randomly selects a classifier to test the input. Our approach does not sacrifice accuracy over legitimate input. Other than detailing and providing a thorough characterization of our defense mechanism, we also provide a proof of concept of using four ",
    "path": "papers/23/06/2306.12161.json",
    "total_tokens": 917,
    "translated_title": "数据集随机化的对抗攻击中和方法",
    "translated_abstract": "对深度学习模型的对抗攻击对它们的可靠性和安全性构成了严重的威胁。现有的防御机制要么只针对某一特定类型的攻击，要么容易受到复杂攻击的影响。我们提出了一种新的防御机制，虽然着眼于基于图像的分类器，但与引用类别相关的特征是一般的。它植根于超空间投影，提供了原始数据集的伪随机投影到一个新的数据集。在训练过程中，我们使用创建了各种投影数据集的一套生成器，每个生成器训练一个特定的分类器，从而得到具有不同决策边界的不同训练分类器。在测试过程中，选择一个分类器来测试输入。我们的方法不会损害对于合法输入的准确性。除了详细阐述和提供我们的防御机制的全面特征化外，我们还提供了四个实验的概念证明。",
    "tldr": "本文提出一种新的防御机制，将原始数据集伪随机投影到一个新的数据集中，在多个有不同决策边界的训练分类器中随机选择一个来测试输入，能够中和对抗攻击，提高了分类器的安全性和可靠性。",
    "en_tdlr": "This article proposes a new defense mechanism that pseudo-randomly projects the original dataset into a new dataset to create a set of diverse projected datasets for training multiple classifiers with different decision boundaries, and randomly selects one to test the input, which can neutralize adversarial attacks and improve the security and reliability of classifiers."
}