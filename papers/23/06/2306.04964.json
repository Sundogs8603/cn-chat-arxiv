{
    "title": "Leveraging Language Identification to Enhance Code-Mixed Text Classification. (arXiv:2306.04964v1 [cs.CL])",
    "abstract": "The usage of more than one language in the same text is referred to as Code Mixed. It is evident that there is a growing degree of adaption of the use of code-mixed data, especially English with a regional language, on social media platforms. Existing deep-learning models do not take advantage of the implicit language information in the code-mixed text. Our study aims to improve BERT-based models performance on low-resource Code-Mixed Hindi-English Datasets by experimenting with language augmentation approaches. We propose a pipeline to improve code-mixed systems that comprise data preprocessing, word-level language identification, language augmentation, and model training on downstream tasks like sentiment analysis. For language augmentation in BERT models, we explore word-level interleaving and post-sentence placement of language information. We have examined the performance of vanilla BERT-based models and their code-mixed HingBERT counterparts on respective benchmark datasets, comp",
    "link": "http://arxiv.org/abs/2306.04964",
    "context": "Title: Leveraging Language Identification to Enhance Code-Mixed Text Classification. (arXiv:2306.04964v1 [cs.CL])\nAbstract: The usage of more than one language in the same text is referred to as Code Mixed. It is evident that there is a growing degree of adaption of the use of code-mixed data, especially English with a regional language, on social media platforms. Existing deep-learning models do not take advantage of the implicit language information in the code-mixed text. Our study aims to improve BERT-based models performance on low-resource Code-Mixed Hindi-English Datasets by experimenting with language augmentation approaches. We propose a pipeline to improve code-mixed systems that comprise data preprocessing, word-level language identification, language augmentation, and model training on downstream tasks like sentiment analysis. For language augmentation in BERT models, we explore word-level interleaving and post-sentence placement of language information. We have examined the performance of vanilla BERT-based models and their code-mixed HingBERT counterparts on respective benchmark datasets, comp",
    "path": "papers/23/06/2306.04964.json",
    "total_tokens": 938,
    "translated_title": "利用语言识别技术提升代码混合文本分类",
    "translated_abstract": "在同一段文本中使用多种语言叫做代码混合。当前社交媒体平台上，特别是英语和地方语言混合使用的数据越来越多。现有的深度学习模型没有充分利用代码混合文本中的隐性语言信息。本研究旨在通过尝试不同的语言增强方法，提高基于BERT的模型在低资源代码混合印地语-英语数据集上的性能。我们提出了一种改进代码混合系统的流程，包括数据预处理、词级语言识别、语言增强和模型训练等步骤，用于下游任务，如情感分析。在BERT模型中进行语言增强时，我们探索了词级交错和句子后置的语言信息插入方法。我们测试了原始BERT模型和经过代码混合改进的HingBERT在各自基准数据集上的性能。",
    "tldr": "本研究提出了一种改进代码混合文本分类的流程，包括数据预处理、词级语言识别、语言增强和模型训练等步骤，用于下游任务，如情感分析。我们探索了词级交错和句子后置的语言信息插入方法，以提高基于BERT的模型在低资源代码混合印地语-英语数据集上的性能。",
    "en_tdlr": "This study proposes a pipeline to improve code-mixed text classification, including data preprocessing, word-level language identification, language augmentation, and model training for downstream tasks such as sentiment analysis. We explore word-level interleaving and post-sentence placement of language information to enhance the performance of BERT-based models on low-resource code-mixed Hindi-English datasets."
}