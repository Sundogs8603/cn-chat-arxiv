{
    "title": "My Boli: Code-mixed Marathi-English Corpora, Pretrained Language Models and Evaluation Benchmarks. (arXiv:2306.14030v1 [cs.CL])",
    "abstract": "The research on code-mixed data is limited due to the unavailability of dedicated code-mixed datasets and pre-trained language models. In this work, we focus on the low-resource Indian language Marathi which lacks any prior work in code-mixing. We present L3Cube-MeCorpus, a large code-mixed Marathi-English (Mr-En) corpus with 5 million tweets for pretraining. We also release L3Cube-MeBERT and MeRoBERTa, code-mixed BERT-based transformer models pre-trained on MeCorpus. Furthermore, for benchmarking, we present three supervised datasets MeHate, MeSent, and MeLID for downstream tasks like code-mixed Mr-En hate speech detection, sentiment analysis, and language identification respectively. These evaluation datasets individually consist of manually annotated \\url{~}12,000 Marathi-English code-mixed tweets. Ablations show that the models trained on this novel corpus significantly outperform the existing state-of-the-art BERT models. This is the first work that presents artifacts for code-mix",
    "link": "http://arxiv.org/abs/2306.14030",
    "context": "Title: My Boli: Code-mixed Marathi-English Corpora, Pretrained Language Models and Evaluation Benchmarks. (arXiv:2306.14030v1 [cs.CL])\nAbstract: The research on code-mixed data is limited due to the unavailability of dedicated code-mixed datasets and pre-trained language models. In this work, we focus on the low-resource Indian language Marathi which lacks any prior work in code-mixing. We present L3Cube-MeCorpus, a large code-mixed Marathi-English (Mr-En) corpus with 5 million tweets for pretraining. We also release L3Cube-MeBERT and MeRoBERTa, code-mixed BERT-based transformer models pre-trained on MeCorpus. Furthermore, for benchmarking, we present three supervised datasets MeHate, MeSent, and MeLID for downstream tasks like code-mixed Mr-En hate speech detection, sentiment analysis, and language identification respectively. These evaluation datasets individually consist of manually annotated \\url{~}12,000 Marathi-English code-mixed tweets. Ablations show that the models trained on this novel corpus significantly outperform the existing state-of-the-art BERT models. This is the first work that presents artifacts for code-mix",
    "path": "papers/23/06/2306.14030.json",
    "total_tokens": 1044,
    "translated_title": "My Boli：混合马拉地语-英语的语料库、预训练语言模型和评估基准",
    "translated_abstract": "由于缺乏专门的混合语料库和预训练语言模型，混合语言数据的研究受到了限制。在这项工作中，我们关注资源匮乏的印度语言马拉地语，这个语言之前没有任何混合语言的研究。我们提出了L3Cube-MeCorpus，一个包含500万条推特的大型混合马拉地语-英语(Mr-En)语料库，用于预训练。我们还发布了L3Cube-MeBERT和MeRoBERTa，基于BERT的混合模型，在MeCorpus上预训练。此外，为了基准测试，我们提供了三个有监督的数据集MeHate、MeSent和MeLID，用于混合Mr-En仇恨言论检测、情感分析和语言识别等下游任务。这些评估数据集分别包含手动注释的\\url{~}12,000条马拉地语-英语混合推特。削减实验表明，这个新语料库训练的模型显著优于现有的BERT模型。这是第一个提供混合马拉地语的代码的工作。",
    "tldr": "本研究针对资源匮乏的印度语言马拉地语，提出了一个大型混合马拉地语-英语语料库，以及预训练的混合BERT模型和针对混合语言下游任务的评估数据集，该语料库训练的模型显著优于现有的BERT模型。",
    "en_tdlr": "We present a large code-mixed Marathi-English corpus, pretrained BERT-based transformer models, and evaluation benchmarks for downstream tasks in code-mixed Indian languages, which significantly outperform existing BERT models in this novel corpus. This is the first work to present artifacts for code-mixing study in Marathi language."
}