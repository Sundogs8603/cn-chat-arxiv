{
    "title": "Learning Latent Dynamics via Invariant Decomposition and (Spatio-)Temporal Transformers. (arXiv:2306.12077v1 [cs.LG])",
    "abstract": "We propose a method for learning dynamical systems from high-dimensional empirical data that combines variational autoencoders and (spatio-)temporal attention within a framework designed to enforce certain scientifically-motivated invariances. We focus on the setting in which data are available from multiple different instances of a system whose underlying dynamical model is entirely unknown at the outset. The approach rests on a separation into an instance-specific encoding (capturing initial conditions, constants etc.) and a latent dynamics model that is itself universal across all instances/realizations of the system. The separation is achieved in an automated, data-driven manner and only empirical data are required as inputs to the model. The approach allows effective inference of system behaviour at any continuous time but does not require an explicit neural ODE formulation, which makes it efficient and highly scalable. We study behaviour through simple theoretical analyses and ex",
    "link": "http://arxiv.org/abs/2306.12077",
    "context": "Title: Learning Latent Dynamics via Invariant Decomposition and (Spatio-)Temporal Transformers. (arXiv:2306.12077v1 [cs.LG])\nAbstract: We propose a method for learning dynamical systems from high-dimensional empirical data that combines variational autoencoders and (spatio-)temporal attention within a framework designed to enforce certain scientifically-motivated invariances. We focus on the setting in which data are available from multiple different instances of a system whose underlying dynamical model is entirely unknown at the outset. The approach rests on a separation into an instance-specific encoding (capturing initial conditions, constants etc.) and a latent dynamics model that is itself universal across all instances/realizations of the system. The separation is achieved in an automated, data-driven manner and only empirical data are required as inputs to the model. The approach allows effective inference of system behaviour at any continuous time but does not require an explicit neural ODE formulation, which makes it efficient and highly scalable. We study behaviour through simple theoretical analyses and ex",
    "path": "papers/23/06/2306.12077.json",
    "total_tokens": 955,
    "translated_title": "通过不变分解和（空间-）时间变换器学习潜在动态",
    "translated_abstract": "我们提出了一种利用变分自编码器和（空间-）时间注意力的方法，通过一个设计旨在强制实施某些科学动机的不变性的框架，从高维经验数据中学习动力学系统。我们专注于这样一种情况，即来自一个系统的多个不同实例的数据可用，其潜在动力学模型在开始时完全不知道。该方法基于一个分离，即一种实例特定的编码（捕获初始条件、常数等）和一种潜在动态模型，该模型本身在系统的所有实例/实现中都是通用的。这种分离是以自动化、数据驱动的方式实现的，只需要经验数据作为模型的输入。该方法允许在任何连续时间有效地推断系统行为，但不需要显式的神经ODE公式，这使得它高效且高度可扩展。我们通过简单的理论分析和实验结果研究了这种行为。",
    "tldr": "该文提出了一种从高维经验数据中学习动力系统的方法，基于不变分解和（空间-）时间变换器，能够自动分离出一种实例特定的编码和一种潜在动态模型，使用经验数据作为模型的输入，通过在任意连续时间推断系统行为，与显式的神经ODE公式不同，高效可扩展。"
}