{
    "title": "On Achieving Optimal Adversarial Test Error. (arXiv:2306.07544v1 [cs.LG])",
    "abstract": "We first elucidate various fundamental properties of optimal adversarial predictors: the structure of optimal adversarial convex predictors in terms of optimal adversarial zero-one predictors, bounds relating the adversarial convex loss to the adversarial zero-one loss, and the fact that continuous predictors can get arbitrarily close to the optimal adversarial error for both convex and zero-one losses. Applying these results along with new Rademacher complexity bounds for adversarial training near initialization, we prove that for general data distributions and perturbation sets, adversarial training on shallow networks with early stopping and an idealized optimal adversary is able to achieve optimal adversarial test error. By contrast, prior theoretical work either considered specialized data distributions or only provided training error guarantees.",
    "link": "http://arxiv.org/abs/2306.07544",
    "context": "Title: On Achieving Optimal Adversarial Test Error. (arXiv:2306.07544v1 [cs.LG])\nAbstract: We first elucidate various fundamental properties of optimal adversarial predictors: the structure of optimal adversarial convex predictors in terms of optimal adversarial zero-one predictors, bounds relating the adversarial convex loss to the adversarial zero-one loss, and the fact that continuous predictors can get arbitrarily close to the optimal adversarial error for both convex and zero-one losses. Applying these results along with new Rademacher complexity bounds for adversarial training near initialization, we prove that for general data distributions and perturbation sets, adversarial training on shallow networks with early stopping and an idealized optimal adversary is able to achieve optimal adversarial test error. By contrast, prior theoretical work either considered specialized data distributions or only provided training error guarantees.",
    "path": "papers/23/06/2306.07544.json",
    "total_tokens": 820,
    "translated_title": "关于实现最优对抗测试误差的研究",
    "translated_abstract": "本文首先阐述了最优对抗预测器的各种基本特性：最优对抗凸预测器的结构、将对抗凸损失与对抗0-1损失相关联的界限以及连续预测器可以在凸和0-1损失下无限接近最优对抗误差。本文还将这些结果与对抗训练在初始化附近的新Rademacher复杂度界限相结合，证明了对于一般的数据分布和扰动集，在浅层网络上进行对抗训练，采用早停和理想的最优对手，能够实现最优对抗测试误差。相比之下，先前的理论工作只考虑了特定的数据分布或仅提供了训练误差的保证。",
    "tldr": "本文提出了最优对抗预测器的各种基本特性，并结合新的Rademacher复杂度界限证明了，在浅层网络上进行对抗训练，采用早停和理想的最优对手，能够实现最优对抗测试误差。"
}