{
    "title": "Inferring the Goals of Communicating Agents from Actions and Instructions. (arXiv:2306.16207v1 [cs.AI])",
    "abstract": "When humans cooperate, they frequently coordinate their activity through both verbal communication and non-verbal actions, using this information to infer a shared goal and plan. How can we model this inferential ability? In this paper, we introduce a model of a cooperative team where one agent, the principal, may communicate natural language instructions about their shared plan to another agent, the assistant, using GPT-3 as a likelihood function for instruction utterances. We then show how a third person observer can infer the team's goal via multi-modal Bayesian inverse planning from actions and instructions, computing the posterior distribution over goals under the assumption that agents will act and communicate rationally to achieve them. We evaluate this approach by comparing it with human goal inferences in a multi-agent gridworld, finding that our model's inferences closely correlate with human judgments (R = 0.96). When compared to inference from actions alone, we also find th",
    "link": "http://arxiv.org/abs/2306.16207",
    "context": "Title: Inferring the Goals of Communicating Agents from Actions and Instructions. (arXiv:2306.16207v1 [cs.AI])\nAbstract: When humans cooperate, they frequently coordinate their activity through both verbal communication and non-verbal actions, using this information to infer a shared goal and plan. How can we model this inferential ability? In this paper, we introduce a model of a cooperative team where one agent, the principal, may communicate natural language instructions about their shared plan to another agent, the assistant, using GPT-3 as a likelihood function for instruction utterances. We then show how a third person observer can infer the team's goal via multi-modal Bayesian inverse planning from actions and instructions, computing the posterior distribution over goals under the assumption that agents will act and communicate rationally to achieve them. We evaluate this approach by comparing it with human goal inferences in a multi-agent gridworld, finding that our model's inferences closely correlate with human judgments (R = 0.96). When compared to inference from actions alone, we also find th",
    "path": "papers/23/06/2306.16207.json",
    "total_tokens": 836,
    "translated_title": "从行动和指令中推断沟通代理的目标",
    "translated_abstract": "当人类合作时，他们经常通过口头沟通和非口头行动来协调活动，利用这些信息来推断共同的目标和计划。我们如何建模这种推理能力？本文介绍了一个合作团队的模型，其中一个代理（主要的）可以通过自然语言指令向另一个代理（助理）传达关于共同计划的信息，使用GPT-3作为指令语句的似然函数。然后，我们展示了一个第三方观察者如何通过多模态贝叶斯逆向规划从行动和指令中推断团队的目标，计算在代理人会采取和交流以实现目标的假设下的目标的后验分布。我们通过将其与多代理系统的网格世界中的人类目标推断进行比较来评估这种方法，发现我们模型的推断与人类判断密切相关（R = 0.96）。与仅从行动推断相比，我们还发现...",
    "tldr": "本文介绍了一个模型，可以通过观察行动和指令推断合作团队的目标，并评估了其与人类判断的相关性。",
    "en_tdlr": "This paper introduces a model that can infer the goals of a cooperative team by observing actions and instructions, and evaluates its correlation with human judgments."
}