{
    "title": "Effect-Invariant Mechanisms for Policy Generalization. (arXiv:2306.10983v1 [stat.ML])",
    "abstract": "Policy learning is an important component of many real-world learning systems. A major challenge in policy learning is how to adapt efficiently to unseen environments or tasks. Recently, it has been suggested to exploit invariant conditional distributions to learn models that generalize better to unseen environments. However, assuming invariance of entire conditional distributions (which we call full invariance) may be too strong of an assumption in practice. In this paper, we introduce a relaxation of full invariance called effect-invariance (e-invariance for short) and prove that it is sufficient, under suitable assumptions, for zero-shot policy generalization. We also discuss an extension that exploits e-invariance when we have a small sample from the test environment, enabling few-shot policy generalization. Our work does not assume an underlying causal graph or that the data are generated by a structural causal model; instead, we develop testing procedures to test e-invariance dir",
    "link": "http://arxiv.org/abs/2306.10983",
    "context": "Title: Effect-Invariant Mechanisms for Policy Generalization. (arXiv:2306.10983v1 [stat.ML])\nAbstract: Policy learning is an important component of many real-world learning systems. A major challenge in policy learning is how to adapt efficiently to unseen environments or tasks. Recently, it has been suggested to exploit invariant conditional distributions to learn models that generalize better to unseen environments. However, assuming invariance of entire conditional distributions (which we call full invariance) may be too strong of an assumption in practice. In this paper, we introduce a relaxation of full invariance called effect-invariance (e-invariance for short) and prove that it is sufficient, under suitable assumptions, for zero-shot policy generalization. We also discuss an extension that exploits e-invariance when we have a small sample from the test environment, enabling few-shot policy generalization. Our work does not assume an underlying causal graph or that the data are generated by a structural causal model; instead, we develop testing procedures to test e-invariance dir",
    "path": "papers/23/06/2306.10983.json",
    "total_tokens": 920,
    "translated_title": "策略概括中的效果不变机制",
    "translated_abstract": "策略学习是许多实际学习系统的重要组成部分。策略学习的一个主要挑战是如何有效地适应未见过的环境或任务。最近，有人建议利用不变的条件分布来学习更好地概括未见过环境的模型。然而，假设整个条件分布是不变的（我们称之为完全不变性），在实践中可能是一个太强的假设。在本文中，我们引入了一种松弛完全不变性的方法，称为效果不变性（简称e-不变性），并证明它是足够的（在适当的假设下），用于零样本策略概括。我们还讨论了一种扩展，它在测试环境中只有少量样本时利用e-不变性，从而实现了少样本策略推广。我们的工作不假设存在一个基础因果图，也不假设数据是由结构因果模型生成的。相反，我们开发了测试过程来测试e-不变性。",
    "tldr": "本文提出了一种松弛了完全不变性的方法，称为效果不变性，证明它足以进行零样本策略概括，并讨论了基于少量样本的扩展。"
}