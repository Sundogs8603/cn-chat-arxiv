{
    "title": "FedET: A Communication-Efficient Federated Class-Incremental Learning Framework Based on Enhanced Transformer. (arXiv:2306.15347v1 [cs.LG])",
    "abstract": "Federated Learning (FL) has been widely concerned for it enables decentralized learning while ensuring data privacy. However, most existing methods unrealistically assume that the classes encountered by local clients are fixed over time. After learning new classes, this assumption will make the model's catastrophic forgetting of old classes significantly severe. Moreover, due to the limitation of communication cost, it is challenging to use large-scale models in FL, which will affect the prediction accuracy. To address these challenges, we propose a novel framework, Federated Enhanced Transformer (FedET), which simultaneously achieves high accuracy and low communication cost. Specifically, FedET uses Enhancer, a tiny module, to absorb and communicate new knowledge, and applies pre-trained Transformers combined with different Enhancers to ensure high precision on various tasks. To address local forgetting caused by new classes of new tasks and global forgetting brought by non-i.i.d (non",
    "link": "http://arxiv.org/abs/2306.15347",
    "context": "Title: FedET: A Communication-Efficient Federated Class-Incremental Learning Framework Based on Enhanced Transformer. (arXiv:2306.15347v1 [cs.LG])\nAbstract: Federated Learning (FL) has been widely concerned for it enables decentralized learning while ensuring data privacy. However, most existing methods unrealistically assume that the classes encountered by local clients are fixed over time. After learning new classes, this assumption will make the model's catastrophic forgetting of old classes significantly severe. Moreover, due to the limitation of communication cost, it is challenging to use large-scale models in FL, which will affect the prediction accuracy. To address these challenges, we propose a novel framework, Federated Enhanced Transformer (FedET), which simultaneously achieves high accuracy and low communication cost. Specifically, FedET uses Enhancer, a tiny module, to absorb and communicate new knowledge, and applies pre-trained Transformers combined with different Enhancers to ensure high precision on various tasks. To address local forgetting caused by new classes of new tasks and global forgetting brought by non-i.i.d (non",
    "path": "papers/23/06/2306.15347.json",
    "total_tokens": 949,
    "translated_title": "FedET: 一种基于增强Transformer的通信高效的联邦类增量学习框架",
    "translated_abstract": "联邦学习(Federated Learning, FL)由于能够在保证数据隐私的前提下进行分散学习而受到广泛关注。然而，大多数现有方法都不切实际地假设本地客户端遇到的类别随时间固定。在学习新类别后，这个假设会导致模型对旧类别的灾难性遗忘变得更加严重。此外，受通信成本的限制，在FL中使用大规模模型是具有挑战性的，这将影响预测精度。为了解决这些挑战，我们提出了一种新颖的框架FedET，它同时实现了高精度和低通信成本。具体而言，FedET使用增强器(Enhancer)这个小型模块来吸收和传递新的知识，并将预训练的Transformer与不同的增强器结合使用，以在各种任务上保证高精度。为了解决新任务的新类别引起的本地遗忘问题和非i.i.d（非独立同分布）所带来的全局遗忘问题，FedET使用了动态扩展方法和重放机制。",
    "tldr": "FedET是一种通信高效的联邦类增量学习框架，利用增强Transformer和增强器来解决联邦学习中的灾难性遗忘和通信成本问题，保证了高精度和数据隐私。"
}