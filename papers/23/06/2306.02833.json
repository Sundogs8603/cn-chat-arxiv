{
    "title": "The $L^\\infty$ Learnability of Reproducing Kernel Hilbert Spaces. (arXiv:2306.02833v1 [stat.ML])",
    "abstract": "In this work, we analyze the learnability of reproducing kernel Hilbert spaces (RKHS) under the $L^\\infty$ norm, which is critical for understanding the performance of kernel methods and random feature models in safety- and security-critical applications. Specifically, we relate the $L^\\infty$ learnability of a RKHS to the spectrum decay of the associate kernel and both lower bounds and upper bounds of the sample complexity are established. In particular, for dot-product kernels on the sphere, we identify conditions when the $L^\\infty$ learning can be achieved with polynomial samples. Let $d$ denote the input dimension and assume the kernel spectrum roughly decays as $\\lambda_k\\sim k^{-1-\\beta}$ with $\\beta>0$. We prove that if $\\beta$ is independent of the input dimension $d$, then functions in the RKHS can be learned efficiently under the $L^\\infty$ norm, i.e., the sample complexity depends polynomially on $d$. In contrast, if $\\beta=1/\\mathrm{poly}(d)$, then the $L^\\infty$ learning ",
    "link": "http://arxiv.org/abs/2306.02833",
    "context": "Title: The $L^\\infty$ Learnability of Reproducing Kernel Hilbert Spaces. (arXiv:2306.02833v1 [stat.ML])\nAbstract: In this work, we analyze the learnability of reproducing kernel Hilbert spaces (RKHS) under the $L^\\infty$ norm, which is critical for understanding the performance of kernel methods and random feature models in safety- and security-critical applications. Specifically, we relate the $L^\\infty$ learnability of a RKHS to the spectrum decay of the associate kernel and both lower bounds and upper bounds of the sample complexity are established. In particular, for dot-product kernels on the sphere, we identify conditions when the $L^\\infty$ learning can be achieved with polynomial samples. Let $d$ denote the input dimension and assume the kernel spectrum roughly decays as $\\lambda_k\\sim k^{-1-\\beta}$ with $\\beta>0$. We prove that if $\\beta$ is independent of the input dimension $d$, then functions in the RKHS can be learned efficiently under the $L^\\infty$ norm, i.e., the sample complexity depends polynomially on $d$. In contrast, if $\\beta=1/\\mathrm{poly}(d)$, then the $L^\\infty$ learning ",
    "path": "papers/23/06/2306.02833.json",
    "total_tokens": 975,
    "translated_title": "带有再生核希尔伯特空间的$L^\\infty$可学习性分析",
    "translated_abstract": "在这项工作中，我们分析了$L^\\infty$范数下再生核希尔伯特空间（RKHS）的可学习性，这对于理解核方法和随机特征模型在安全和安全关键应用中的性能至关重要。具体地，我们将RKHS的$L^\\infty$可学习性与关联核的频谱衰减相关联，并建立了样本复杂性的下界和上界。特别地，对于球上的点积核，我们确定了在多项式样本下可以实现$L^\\infty$学习的条件。假设输入维数为$d$，且核频谱大致衰减为$\\lambda_k\\sim k^{-1-\\beta}$，其中$\\beta>0$。我们证明，如果$\\beta$独立于输入维数$d$，那么RKHS中的函数可以在$L^\\infty$范数下有效地学习，即样本复杂度多项式地依赖于$d$。相反，如果$\\beta=1/\\mathrm{poly}(d)$，则$L^\\infty$学习是不可能的。",
    "tldr": "本文分析了$L^\\infty$范数下再生核希尔伯特空间的可学习性，建立了样本复杂性的下界和上界，并确定了在满足条件时可以多项式样本下实现$L^\\infty$学习的频谱衰减条件。",
    "en_tdlr": "This paper analyzes the $L^\\infty$ learnability of reproducing kernel Hilbert spaces (RKHS), establishes both lower bounds and upper bounds for sample complexity, and identifies conditions for achieving polynomial sample complexity in $L^\\infty$ learning under certain decay conditions of the associated kernel spectrum."
}