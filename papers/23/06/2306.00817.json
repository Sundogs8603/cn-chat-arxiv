{
    "title": "Dilated Convolution with Learnable Spacings: beyond bilinear interpolation. (arXiv:2306.00817v2 [cs.CV] UPDATED)",
    "abstract": "Dilated Convolution with Learnable Spacings (DCLS) is a recently proposed variation of the dilated convolution in which the spacings between the non-zero elements in the kernel, or equivalently their positions, are learnable. Non-integer positions are handled via interpolation. Thanks to this trick, positions have well-defined gradients. The original DCLS used bilinear interpolation, and thus only considered the four nearest pixels. Yet here we show that longer range interpolations, and in particular a Gaussian interpolation, allow improving performance on ImageNet1k classification on two state-of-the-art convolutional architectures (ConvNeXt and Conv\\-Former), without increasing the number of parameters. The method code is based on PyTorch and is available at https://github.com/K-H-Ismail/Dilated-Convolution-with-Learnable-Spacings-PyTorch",
    "link": "http://arxiv.org/abs/2306.00817",
    "context": "Title: Dilated Convolution with Learnable Spacings: beyond bilinear interpolation. (arXiv:2306.00817v2 [cs.CV] UPDATED)\nAbstract: Dilated Convolution with Learnable Spacings (DCLS) is a recently proposed variation of the dilated convolution in which the spacings between the non-zero elements in the kernel, or equivalently their positions, are learnable. Non-integer positions are handled via interpolation. Thanks to this trick, positions have well-defined gradients. The original DCLS used bilinear interpolation, and thus only considered the four nearest pixels. Yet here we show that longer range interpolations, and in particular a Gaussian interpolation, allow improving performance on ImageNet1k classification on two state-of-the-art convolutional architectures (ConvNeXt and Conv\\-Former), without increasing the number of parameters. The method code is based on PyTorch and is available at https://github.com/K-H-Ismail/Dilated-Convolution-with-Learnable-Spacings-PyTorch",
    "path": "papers/23/06/2306.00817.json",
    "total_tokens": 857,
    "translated_title": "可学习空间可伸展卷积：超越双线性插值",
    "translated_abstract": "可学习空间可伸展卷积（Dilated Convolution with Learnable Spacings，DCLS）是一种最近提出的改进型膨胀卷积，其中卷积核中非零元素的间距或者等效地说，它们的位置是可学习的。非整数位置通过插值处理，这种技巧使得位置具有明确的梯度。原始的DCLS使用双线性插值，只考虑了最近的四个像素。然而，我们在这里展示了更长程的插值，尤其是高斯插值，可以在不增加参数的情况下提高ConvNeXt和Conv-Former两个最先进的卷积结构在ImageNet1k分类上的性能。该方法的代码基于PyTorch，可以在https://github.com/K-H-Ismail/Dilated-Convolution-with-Learnable-Spacings-PyTorch上获得。",
    "tldr": "可学习空间可伸展卷积（DCLS）通过使用可学习的间距和插值技巧，超越了双线性插值，在不增加参数的情况下提高了最先进卷积结构在ImageNet1k分类上的性能。",
    "en_tdlr": "Dilated Convolution with Learnable Spacings (DCLS) improves the performance of state-of-the-art convolutional architectures on ImageNet1k classification without increasing the number of parameters, by using learnable spacings and interpolation techniques beyond bilinear interpolation."
}