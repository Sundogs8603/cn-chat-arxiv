{
    "title": "Blocked Cross-Validation: A Precise and Efficient Method for Hyperparameter Tuning. (arXiv:2306.06591v2 [cs.LG] UPDATED)",
    "abstract": "Hyperparameter tuning plays a crucial role in optimizing the performance of predictive learners. Cross--validation (CV) is a widely adopted technique for estimating the error of different hyperparameter settings. Repeated cross-validation (RCV) has been commonly employed to reduce the variability of CV errors. In this paper, we introduce a novel approach called blocked cross-validation (BCV), where the repetitions are blocked with respect to both CV partition and the random behavior of the learner. Theoretical analysis and empirical experiments demonstrate that BCV provides more precise error estimates compared to RCV, even with a significantly reduced number of runs. We present extensive examples using real--world data sets to showcase the effectiveness and efficiency of BCV in hyperparameter tuning. Our results indicate that BCV outperforms RCV in hyperparameter tuning, achieving greater precision with fewer computations.",
    "link": "http://arxiv.org/abs/2306.06591",
    "context": "Title: Blocked Cross-Validation: A Precise and Efficient Method for Hyperparameter Tuning. (arXiv:2306.06591v2 [cs.LG] UPDATED)\nAbstract: Hyperparameter tuning plays a crucial role in optimizing the performance of predictive learners. Cross--validation (CV) is a widely adopted technique for estimating the error of different hyperparameter settings. Repeated cross-validation (RCV) has been commonly employed to reduce the variability of CV errors. In this paper, we introduce a novel approach called blocked cross-validation (BCV), where the repetitions are blocked with respect to both CV partition and the random behavior of the learner. Theoretical analysis and empirical experiments demonstrate that BCV provides more precise error estimates compared to RCV, even with a significantly reduced number of runs. We present extensive examples using real--world data sets to showcase the effectiveness and efficiency of BCV in hyperparameter tuning. Our results indicate that BCV outperforms RCV in hyperparameter tuning, achieving greater precision with fewer computations.",
    "path": "papers/23/06/2306.06591.json",
    "total_tokens": 908,
    "translated_title": "屏蔽交叉验证：一种准确高效的超参数调整方法",
    "translated_abstract": "超参数调整在优化预测学习器的性能中起着关键作用。交叉验证（CV）是一种广泛采用的技术，用于估计不同超参数设置的误差。重复交叉验证（RCV）常用于减少CV误差的变化性。在本文中，我们引入了一种名为屏蔽交叉验证（BCV）的新方法，其中重复次数相对于CV分区和学习器的随机行为进行了阻塞。理论分析和实证实验证明，与RCV相比，BCV可以提供更准确的误差估计结果，即使运行次数显著减少。我们使用真实数据集提供了大量示例，展示了BCV在超参数调优中的有效性和效率。我们的结果表明，BCV在超参数调优中优于RCV，可以以更少的计算实现更高的精度。",
    "tldr": "屏蔽交叉验证（BCV）是一种准确高效的超参数调整方法，相比于传统的重复交叉验证（RCV），BCV可以提供更准确的误差估计结果，且运行次数更少。使用真实数据集的实验结果表明，BCV在超参数调优任务中优于RCV，能够以更少的计算实现更高的精度。",
    "en_tdlr": "Blocked cross-validation (BCV) is a precise and efficient method for hyperparameter tuning. BCV provides more accurate error estimates compared to the traditional repeated cross-validation (RCV) method even with fewer runs. Experimental results using real-world datasets demonstrate that BCV outperforms RCV in hyperparameter tuning, achieving higher precision with fewer computations."
}