{
    "title": "Kernel $\\epsilon$-Greedy for Contextual Bandits. (arXiv:2306.17329v1 [stat.ML])",
    "abstract": "We consider a kernelized version of the $\\epsilon$-greedy strategy for contextual bandits. More precisely, in a setting with finitely many arms, we consider that the mean reward functions lie in a reproducing kernel Hilbert space (RKHS). We propose an online weighted kernel ridge regression estimator for the reward functions. Under some conditions on the exploration probability sequence, $\\{\\epsilon_t\\}_t$, and choice of the regularization parameter, $\\{\\lambda_t\\}_t$, we show that the proposed estimator is consistent. We also show that for any choice of kernel and the corresponding RKHS, we achieve a sub-linear regret rate depending on the intrinsic dimensionality of the RKHS. Furthermore, we achieve the optimal regret rate of $\\sqrt{T}$ under a margin condition for finite-dimensional RKHS.",
    "link": "http://arxiv.org/abs/2306.17329",
    "context": "Title: Kernel $\\epsilon$-Greedy for Contextual Bandits. (arXiv:2306.17329v1 [stat.ML])\nAbstract: We consider a kernelized version of the $\\epsilon$-greedy strategy for contextual bandits. More precisely, in a setting with finitely many arms, we consider that the mean reward functions lie in a reproducing kernel Hilbert space (RKHS). We propose an online weighted kernel ridge regression estimator for the reward functions. Under some conditions on the exploration probability sequence, $\\{\\epsilon_t\\}_t$, and choice of the regularization parameter, $\\{\\lambda_t\\}_t$, we show that the proposed estimator is consistent. We also show that for any choice of kernel and the corresponding RKHS, we achieve a sub-linear regret rate depending on the intrinsic dimensionality of the RKHS. Furthermore, we achieve the optimal regret rate of $\\sqrt{T}$ under a margin condition for finite-dimensional RKHS.",
    "path": "papers/23/06/2306.17329.json",
    "total_tokens": 911,
    "translated_title": "基于核的$\\epsilon$-贪心策略在情境脉冲中的应用",
    "translated_abstract": "我们考虑了情境脉冲中的基于核的$\\epsilon$-贪心策略。更具体地说，在有限数量的臂的情况下，我们认为平均奖励函数位于再生核希尔伯特空间（RKHS）中。我们提出了一种用于奖励函数的在线加权核岭回归估计器。在对探索概率序列$\\{\\epsilon_t\\}_t$和正则化参数$\\{\\lambda_t\\}_t$的一些条件下，我们证明了所提出的估计器的一致性。我们还证明，对于任何核和相应的RKHS的选择，我们可以实现依赖于RKHS内在维度的次线性后悔率。此外，在有限维RKHS的边际条件下，我们实现了$\\sqrt{T}$的最优后悔率。",
    "tldr": "本文提出了基于核的$\\epsilon$-贪心策略应用于情境脉冲中的方法，通过在线加权核岭回归估计器实现对奖励函数的估计，并证明了其一致性和依赖于RKHS维度的次线性后悔率，在有限维RKHS的边际条件下实现了最优后悔率。",
    "en_tdlr": "This paper proposes a kernelized version of the $\\epsilon$-greedy strategy for contextual bandits, utilizing an online weighted kernel ridge regression estimator to estimate the reward functions. The consistency of the estimator is shown under certain conditions, and a sub-linear regret rate dependent on the intrinsic dimensionality of the RKHS is achieved for any choice of kernel and RKHS. Furthermore, the optimal regret rate of $\\sqrt{T}$ is achieved under a margin condition for finite-dimensional RKHS."
}