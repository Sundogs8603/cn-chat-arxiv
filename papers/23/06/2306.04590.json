{
    "title": "Proximity-Informed Calibration for Deep Neural Networks. (arXiv:2306.04590v1 [cs.LG])",
    "abstract": "Confidence calibration is central to providing accurate and interpretable uncertainty estimates, especially under safety-critical scenarios. However, we find that existing calibration algorithms often overlook the issue of proximity bias, a phenomenon where models tend to be more overconfident in low proximity data (i.e., lying in the sparse region of the data distribution) compared to high proximity samples, and thus suffer from inconsistent miscalibration across different proximity samples. We examine the problem over pretrained ImageNet models and observe that: 1) Proximity bias exists across a wide variety of model architectures and sizes; 2) Transformer-based models are more susceptible to proximity bias than CNN-based models; 3) Proximity bias persists even after performing popular calibration algorithms like temperature scaling; 4) Models tend to overfit more heavily on low proximity samples than on high proximity samples. Motivated by the empirical findings, we propose ProCal, ",
    "link": "http://arxiv.org/abs/2306.04590",
    "context": "Title: Proximity-Informed Calibration for Deep Neural Networks. (arXiv:2306.04590v1 [cs.LG])\nAbstract: Confidence calibration is central to providing accurate and interpretable uncertainty estimates, especially under safety-critical scenarios. However, we find that existing calibration algorithms often overlook the issue of proximity bias, a phenomenon where models tend to be more overconfident in low proximity data (i.e., lying in the sparse region of the data distribution) compared to high proximity samples, and thus suffer from inconsistent miscalibration across different proximity samples. We examine the problem over pretrained ImageNet models and observe that: 1) Proximity bias exists across a wide variety of model architectures and sizes; 2) Transformer-based models are more susceptible to proximity bias than CNN-based models; 3) Proximity bias persists even after performing popular calibration algorithms like temperature scaling; 4) Models tend to overfit more heavily on low proximity samples than on high proximity samples. Motivated by the empirical findings, we propose ProCal, ",
    "path": "papers/23/06/2306.04590.json",
    "total_tokens": 887,
    "translated_title": "深度神经网络的相似性校准",
    "translated_abstract": "推理结果的可信度校准对于提供准确和可解释的不确定性估计至关重要，特别是在安全关键场景下。已有的校准方法通常忽略了接近度偏差的问题，即模型在低接近性数据（即分布的稀疏区域）中倾向于更自信，而在高接近性样本中表现出不一致的误校准，我们发现这一问题。我们在ImageNet预训练模型上研究了这一问题，并观察到：1）接近度偏差存在于各种模型架构和大小之间；2）基于Transformer的模型比基于CNN的模型更容易受到接近度偏差的影响；3）即使采用流行的校准算法如温度缩放，接近度偏差也会持续存在；4）模型在低接近性样本上的过拟合程度比高接近性样本更严重。在这些实证发现的基础上，我们提出了ProCal。",
    "tldr": "该论文提出了一个校准算法，解决了深度神经网络推理过程中低接近度数据和高接近度数据之间不一致的误校准问题。",
    "en_tdlr": "This paper proposes a calibration algorithm that solves the problem of inconsistent mis-calibration between low proximity data and high proximity data in the inference process of deep neural networks."
}