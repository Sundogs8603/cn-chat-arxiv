{
    "title": "SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs. (arXiv:2306.17842v1 [cs.CV])",
    "abstract": "In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.",
    "link": "http://arxiv.org/abs/2306.17842",
    "context": "Title: SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs. (arXiv:2306.17842v1 [cs.CV])\nAbstract: In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.",
    "path": "papers/23/06/2306.17842.json",
    "total_tokens": 828,
    "translated_title": "SPAE: 基于语义金字塔自编码器的冻结LLM的多模态生成",
    "translated_abstract": "本研究引入了Semantic Pyramid AutoEncoder (SPAE)，使冻结的LLM能够执行涉及非语言模态（如图像或视频）的理解和生成任务。SPAE在原始像素和从LLM词汇表中提取的可解释的词汇标记（或单词）之间进行转换。生成的标记捕捉了视觉重建所需的语义含义和细粒度细节，将视觉内容转化为LLM能理解的语言，并使其能够执行各种多模态任务。我们的方法通过在多样化的图像理解和生成任务上，与冻结的PaLM 2和GPT 3.5进行上下文学习实验证实。在相同的设置下，我们的方法是第一个成功使冻结LLM生成图像内容，并在图像理解任务中的性能超过现有技术25%以上的尝试。",
    "tldr": "本研究引入了SPAE，使用语义金字塔自编码器实现了冻结LLM执行涉及非语言模态的理解和生成任务。通过将图像转化为LLM可理解的词汇标记，我们的方法成功地提升了冻结LLM在图像理解任务中的性能，超过了现有技术25%以上。"
}