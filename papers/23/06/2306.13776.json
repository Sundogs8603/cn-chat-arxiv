{
    "title": "Swin-Free: Achieving Better Cross-Window Attention and Efficiency with Size-varying Window. (arXiv:2306.13776v1 [cs.CV])",
    "abstract": "Transformer models have shown great potential in computer vision, following their success in language tasks. Swin Transformer is one of them that outperforms convolution-based architectures in terms of accuracy, while improving efficiency when compared to Vision Transformer (ViT) and its variants, which have quadratic complexity with respect to the input size. Swin Transformer features shifting windows that allows cross-window connection while limiting self-attention computation to non-overlapping local windows. However, shifting windows introduces memory copy operations, which account for a significant portion of its runtime. To mitigate this issue, we propose Swin-Free in which we apply size-varying windows across stages, instead of shifting windows, to achieve cross-connection among local windows. With this simple design change, Swin-Free runs faster than the Swin Transformer at inference with better accuracy. Furthermore, we also propose a few of Swin-Free variants that are faster ",
    "link": "http://arxiv.org/abs/2306.13776",
    "context": "Title: Swin-Free: Achieving Better Cross-Window Attention and Efficiency with Size-varying Window. (arXiv:2306.13776v1 [cs.CV])\nAbstract: Transformer models have shown great potential in computer vision, following their success in language tasks. Swin Transformer is one of them that outperforms convolution-based architectures in terms of accuracy, while improving efficiency when compared to Vision Transformer (ViT) and its variants, which have quadratic complexity with respect to the input size. Swin Transformer features shifting windows that allows cross-window connection while limiting self-attention computation to non-overlapping local windows. However, shifting windows introduces memory copy operations, which account for a significant portion of its runtime. To mitigate this issue, we propose Swin-Free in which we apply size-varying windows across stages, instead of shifting windows, to achieve cross-connection among local windows. With this simple design change, Swin-Free runs faster than the Swin Transformer at inference with better accuracy. Furthermore, we also propose a few of Swin-Free variants that are faster ",
    "path": "papers/23/06/2306.13776.json",
    "total_tokens": 877,
    "translated_title": "Swin-Free: 通过变化大小的窗口实现更好的跨窗口注意力和效率",
    "translated_abstract": "Transformer模型在语言任务中的成功后，已经展现出在计算机视觉领域的巨大潜力。其中Swin Transformer在准确度上超越了基于卷积的架构，在与Vision Transformer及其变体相比的效率方面也有所改进，因为后者相对于输入大小具有二次复杂度。Swin Transformer采用了移动窗口的功能，允许跨窗口连接，同时将自我注意计算限制在不重叠的局部窗口中。然而，移动窗口引入了内存复制操作，这占据了其运行时间的重要部分。为了解决这个问题，我们提出了Swin-Free，其中我们在各个阶段应用变化大小的窗口，而不是移动窗口，以实现局部窗口之间的交叉连接。通过这个简单的设计改变，Swin-Free在推理时比Swin Transformer运行更快，准确度更高。此外，我们还提出了几个比Swin-Free更快的变体。",
    "tldr": "Swin-Free是一个Transformer模型的变体，通过变化大小的窗口实现了更好的跨窗口注意力和效率。与原始模型相比，在推理时速度更快、准确率更高。",
    "en_tdlr": "Swin-Free is a variant of the Transformer model that achieves better cross-window attention and efficiency with size-varying windows. Compared to the original model, it runs faster and has higher accuracy during inference."
}