{
    "title": "Datasets and Benchmarks for Offline Safe Reinforcement Learning. (arXiv:2306.09303v2 [cs.LG] UPDATED)",
    "abstract": "This paper presents a comprehensive benchmarking suite tailored to offline safe reinforcement learning (RL) challenges, aiming to foster progress in the development and evaluation of safe learning algorithms in both the training and deployment phases. Our benchmark suite contains three packages: 1) expertly crafted safe policies, 2) D4RL-styled datasets along with environment wrappers, and 3) high-quality offline safe RL baseline implementations. We feature a methodical data collection pipeline powered by advanced safe RL algorithms, which facilitates the generation of diverse datasets across 38 popular safe RL tasks, from robot control to autonomous driving. We further introduce an array of data post-processing filters, capable of modifying each dataset's diversity, thereby simulating various data collection conditions. Additionally, we provide elegant and extensible implementations of prevalent offline safe RL algorithms to accelerate research in this area. Through extensive experime",
    "link": "http://arxiv.org/abs/2306.09303",
    "context": "Title: Datasets and Benchmarks for Offline Safe Reinforcement Learning. (arXiv:2306.09303v2 [cs.LG] UPDATED)\nAbstract: This paper presents a comprehensive benchmarking suite tailored to offline safe reinforcement learning (RL) challenges, aiming to foster progress in the development and evaluation of safe learning algorithms in both the training and deployment phases. Our benchmark suite contains three packages: 1) expertly crafted safe policies, 2) D4RL-styled datasets along with environment wrappers, and 3) high-quality offline safe RL baseline implementations. We feature a methodical data collection pipeline powered by advanced safe RL algorithms, which facilitates the generation of diverse datasets across 38 popular safe RL tasks, from robot control to autonomous driving. We further introduce an array of data post-processing filters, capable of modifying each dataset's diversity, thereby simulating various data collection conditions. Additionally, we provide elegant and extensible implementations of prevalent offline safe RL algorithms to accelerate research in this area. Through extensive experime",
    "path": "papers/23/06/2306.09303.json",
    "total_tokens": 946,
    "translated_title": "离线安全强化学习的数据集和基准",
    "translated_abstract": "本文提出了一个专门针对离线安全强化学习（RL）挑战的综合基准套件，旨在促进开发和评估训练和部署阶段中的安全学习算法的进展。我们的基准套件包含三个组件：1）专家制作的安全策略，2）D4RL样式的数据集以及环境包装器，以及3）高质量的离线安全RL基准实现。我们提供了一种有条理的数据收集流程，由先进的安全RL算法支持，可以促进在38个受欢迎的安全RL任务中生成各种数据集，从机器人控制到自动驾驶。我们还引入了一系列数据后处理过滤器，能够修改每个数据集的多样性，从而模拟各种数据收集条件。此外，我们提供了流行的离线安全RL算法的精美且可扩展的实现，以加速该领域的研究。 通过广泛的实验",
    "tldr": "该论文提出了一个专门针对离线安全强化学习的基准套件，包含了安全策略、数据集和高质量RL算法实现。作者还提供了一种数据收集流程，利用先进算法生成多样性数据集，用于38个受欢迎的安全RL任务。该套件可加速该领域的研究进展。",
    "en_tdlr": "This paper proposes a comprehensive benchmarking suite tailored to offline safe reinforcement learning, containing safe policies, datasets, and high-quality RL algorithm implementations. The authors provide a data collection pipeline using advanced algorithms, generating diverse datasets for 38 popular safe RL tasks. This suite can accelerate progress in this area."
}