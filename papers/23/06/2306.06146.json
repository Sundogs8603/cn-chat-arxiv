{
    "title": "Hidden Classification Layers: a study on Data Hidden Representations with a Higher Degree of Linear Separability between the Classes. (arXiv:2306.06146v1 [cs.LG])",
    "abstract": "In the context of classification problems, Deep Learning (DL) approaches represent state of art. Many DL approaches are based on variations of standard multi-layer feed-forward neural networks. These are also referred to as deep networks. The basic idea is that each hidden neural layer accomplishes a data transformation which is expected to make the data representation \"somewhat more linearly separable\" than the previous one to obtain a final data representation which is as linearly separable as possible. However, determining the appropriate neural network parameters that can perform these transformations is a critical problem. In this paper, we investigate the impact on deep network classifier performances of a training approach favouring solutions where data representations at the hidden layers have a higher degree of linear separability between the classes with respect to standard methods. To this aim, we propose a neural network architecture which induces an error function involvin",
    "link": "http://arxiv.org/abs/2306.06146",
    "context": "Title: Hidden Classification Layers: a study on Data Hidden Representations with a Higher Degree of Linear Separability between the Classes. (arXiv:2306.06146v1 [cs.LG])\nAbstract: In the context of classification problems, Deep Learning (DL) approaches represent state of art. Many DL approaches are based on variations of standard multi-layer feed-forward neural networks. These are also referred to as deep networks. The basic idea is that each hidden neural layer accomplishes a data transformation which is expected to make the data representation \"somewhat more linearly separable\" than the previous one to obtain a final data representation which is as linearly separable as possible. However, determining the appropriate neural network parameters that can perform these transformations is a critical problem. In this paper, we investigate the impact on deep network classifier performances of a training approach favouring solutions where data representations at the hidden layers have a higher degree of linear separability between the classes with respect to standard methods. To this aim, we propose a neural network architecture which induces an error function involvin",
    "path": "papers/23/06/2306.06146.json",
    "total_tokens": 837,
    "translated_title": "隐藏分类层：关于数据隐藏表示中更高线性可分性的研究",
    "translated_abstract": "在分类问题的背景下，深度学习（DL）方法代表了最先进的技术。许多深度学习方法都基于标准的多层前馈神经网络的变种。这些也被称为深度网络。基本思想是每个隐藏神经层完成一种数据转换，预期使数据表示“比之前更线性可分”，以获得尽可能线性可分的最终数据表示。然而，确定可以执行这些转换的适当神经网络参数是一个关键问题。在本文中，我们研究了一种培训方法对深层网络分类器性能的影响，这种方法倾向于使用标准方法相比，隐藏层的数据表示具有更高的类之间线性可分性。为此，我们提出了一个神经网络架构，该架构引入了一个涉及误差函数的新颖培训方法。",
    "tldr": "本文中，研究了一种新颖的培训方法影响深层网络分类器性能，并提出了一个新的神经网络架构，在数据隐藏表示中达到更高的线性可分性。",
    "en_tdlr": "This paper investigates the impact of a novel training approach on the performance of deep network classifiers and proposes a new neural network architecture to achieve higher linear separability in the data hidden representation between classes."
}