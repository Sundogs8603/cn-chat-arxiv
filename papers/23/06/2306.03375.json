{
    "title": "Identifying Shared Decodable Concepts in the Human Brain Using Image-Language Foundation Models. (arXiv:2306.03375v1 [cs.AI])",
    "abstract": "We introduce a method that takes advantage of high-quality pretrained multimodal representations to explore fine-grained semantic networks in the human brain. Previous studies have documented evidence of functional localization in the brain, with different anatomical regions preferentially activating for different types of sensory input. Many such localized structures are known, including the fusiform face area and parahippocampal place area. This raises the question of whether additional brain regions (or conjunctions of brain regions) are also specialized for other important semantic concepts. To identify such brain regions, we developed a data-driven approach to uncover visual concepts that are decodable from a massive functional magnetic resonance imaging (fMRI) dataset. Our analysis is broadly split into three sections. First, a fully connected neural network is trained to map brain responses to the outputs of an image-language foundation model, CLIP (Radford et al., 2021). Subseq",
    "link": "http://arxiv.org/abs/2306.03375",
    "context": "Title: Identifying Shared Decodable Concepts in the Human Brain Using Image-Language Foundation Models. (arXiv:2306.03375v1 [cs.AI])\nAbstract: We introduce a method that takes advantage of high-quality pretrained multimodal representations to explore fine-grained semantic networks in the human brain. Previous studies have documented evidence of functional localization in the brain, with different anatomical regions preferentially activating for different types of sensory input. Many such localized structures are known, including the fusiform face area and parahippocampal place area. This raises the question of whether additional brain regions (or conjunctions of brain regions) are also specialized for other important semantic concepts. To identify such brain regions, we developed a data-driven approach to uncover visual concepts that are decodable from a massive functional magnetic resonance imaging (fMRI) dataset. Our analysis is broadly split into three sections. First, a fully connected neural network is trained to map brain responses to the outputs of an image-language foundation model, CLIP (Radford et al., 2021). Subseq",
    "path": "papers/23/06/2306.03375.json",
    "total_tokens": 732,
    "translated_title": "使用图像-语言基础模型在人类大脑中识别共享的可解码概念",
    "translated_abstract": "我们介绍了一种利用高质量的预训练多模态表示探索人类大脑中细粒度语义网络的方法。我们开发了一种数据驱动的方法，以揭示可解码的视觉概念，从而识别专门用于其他重要语义概念的大脑区域。",
    "tldr": "该论文介绍了一种数据驱动的方法，利用预训练的多模态表示方法探索人脑中关于视觉概念的高度细分的语义网络，从而识别共享的可解码概念，以推断是否存在专门用于重要语义概念的脑区域。"
}