{
    "title": "Taming Diffusion Models for Music-driven Conducting Motion Generation. (arXiv:2306.10065v1 [eess.AS])",
    "abstract": "Generating the motion of orchestral conductors from a given piece of symphony music is a challenging task since it requires a model to learn semantic music features and capture the underlying distribution of real conducting motion. Prior works have applied Generative Adversarial Networks (GAN) to this task, but the promising diffusion model, which recently showed its advantages in terms of both training stability and output quality, has not been exploited in this context. This paper presents Diffusion-Conductor, a novel DDIM-based approach for music-driven conducting motion generation, which integrates the diffusion model to a two-stage learning framework. We further propose a random masking strategy to improve the feature robustness, and use a pair of geometric loss functions to impose additional regularizations and increase motion diversity. We also design several novel metrics, including Frechet Gesture Distance (FGD) and Beat Consistency Score (BC) for a more comprehensive evaluati",
    "link": "http://arxiv.org/abs/2306.10065",
    "context": "Title: Taming Diffusion Models for Music-driven Conducting Motion Generation. (arXiv:2306.10065v1 [eess.AS])\nAbstract: Generating the motion of orchestral conductors from a given piece of symphony music is a challenging task since it requires a model to learn semantic music features and capture the underlying distribution of real conducting motion. Prior works have applied Generative Adversarial Networks (GAN) to this task, but the promising diffusion model, which recently showed its advantages in terms of both training stability and output quality, has not been exploited in this context. This paper presents Diffusion-Conductor, a novel DDIM-based approach for music-driven conducting motion generation, which integrates the diffusion model to a two-stage learning framework. We further propose a random masking strategy to improve the feature robustness, and use a pair of geometric loss functions to impose additional regularizations and increase motion diversity. We also design several novel metrics, including Frechet Gesture Distance (FGD) and Beat Consistency Score (BC) for a more comprehensive evaluati",
    "path": "papers/23/06/2306.10065.json",
    "total_tokens": 932,
    "translated_title": "驯服扩散模型生成音乐驱动的指挥动作",
    "translated_abstract": "从交响乐中生成指挥家的动作是一项具有挑战性的任务，需要学习语义音乐特征并捕捉真实指挥动作的潜在分布。先前的工作已经将生成对抗网络（GAN）应用于此任务，但前景光追迹的模型在训练稳定性和输出质量方面显示出优势，但在此上下文中还未被利用。本文提出了Diffusion-Conductor，一种基于DDIM的新颖方法，用于音乐驱动的指挥运动生成，将扩散模型整合到一个两阶段学习框架中。我们进一步提出了随机屏蔽策略以提高特征的鲁棒性，并使用一对几何损失函数施加附加规则化和增加运动多样性。我们还设计了几个新的度量标准，包括Frechet Gesture Distance（FGD）和Beat Consistency Score（BC），以进行更全面的评估。",
    "tldr": "本文提出了Diffusion-Conductor，一种基于DDIM的方法，用于音乐驱动的指挥运动生成，利用扩散模型整合到一个两阶段学习框架中，并应用随机屏蔽策略和一对几何损失函数来提高运动多样性和鲁棒性。"
}