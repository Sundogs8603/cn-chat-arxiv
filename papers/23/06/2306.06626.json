{
    "title": "On Kinetic Optimal Probability Paths for Generative Models. (arXiv:2306.06626v1 [cs.LG])",
    "abstract": "Recent successful generative models are trained by fitting a neural network to an a-priori defined tractable probability density path taking noise to training examples. In this paper we investigate the space of Gaussian probability paths, which includes diffusion paths as an instance, and look for an optimal member in some useful sense. In particular, minimizing the Kinetic Energy (KE) of a path is known to make particles' trajectories simple, hence easier to sample, and empirically improve performance in terms of likelihood of unseen data and sample generation quality. We investigate Kinetic Optimal (KO) Gaussian paths and offer the following observations: (i) We show the KE takes a simplified form on the space of Gaussian paths, where the data is incorporated only through a single, one dimensional scalar function, called the \\emph{data separation function}. (ii) We characterize the KO solutions with a one dimensional ODE. (iii) We approximate data-dependent KO paths by approximating ",
    "link": "http://arxiv.org/abs/2306.06626",
    "context": "Title: On Kinetic Optimal Probability Paths for Generative Models. (arXiv:2306.06626v1 [cs.LG])\nAbstract: Recent successful generative models are trained by fitting a neural network to an a-priori defined tractable probability density path taking noise to training examples. In this paper we investigate the space of Gaussian probability paths, which includes diffusion paths as an instance, and look for an optimal member in some useful sense. In particular, minimizing the Kinetic Energy (KE) of a path is known to make particles' trajectories simple, hence easier to sample, and empirically improve performance in terms of likelihood of unseen data and sample generation quality. We investigate Kinetic Optimal (KO) Gaussian paths and offer the following observations: (i) We show the KE takes a simplified form on the space of Gaussian paths, where the data is incorporated only through a single, one dimensional scalar function, called the \\emph{data separation function}. (ii) We characterize the KO solutions with a one dimensional ODE. (iii) We approximate data-dependent KO paths by approximating ",
    "path": "papers/23/06/2306.06626.json",
    "total_tokens": 872,
    "translated_title": "关于生成模型的动力学最佳概率路径",
    "translated_abstract": "最近成功的生成模型通过将神经网络拟合到事先定义的可处理的概率密度路径上来训练。本文研究高斯概率路径空间，包含扩散路径作为一种例子，并在某种意义上寻找一个最优成员。特别是，最小化路径的动能已知可以使粒子的轨迹简单，因此更易于采样，并在未看到数据和样本生成质量方面在经验上得到提高。我们研究动力学最佳高斯路径，并提供以下观察：（i）我们展示了动能在高斯路径空间上采取简化形式，其中仅通过称为“数据分离函数”的单一一维标量函数来纳入数据。（ii）我们用一维ODE描述了KO解。（iii）我们通过逼近表示数据相关的KO路径。",
    "tldr": "本文研究高斯概率路径的动力学最佳成员，发现动能在这样的路径空间上可以通过单一的一维标量函数来整合数据，从而得到了提高研究所需的粒子轨迹简单性的方法。",
    "en_tdlr": "This paper investigates the kinetic optimal members in the space of Gaussian probability paths for generative models, and shows that minimizing the kinetic energy in such paths can be achieved by incorporating data through a single one-dimensional scalar function called the \"data separation function\", which leads to simpler particle trajectories and improves performance in likelihood and sample generation quality."
}