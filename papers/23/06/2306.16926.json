{
    "title": "OSP: Boosting Distributed Model Training with 2-stage Synchronization. (arXiv:2306.16926v1 [cs.DC])",
    "abstract": "Distributed deep learning (DDL) is a promising research area, which aims to increase the efficiency of training deep learning tasks with large size of datasets and models. As the computation capability of DDL nodes continues to increase, the network connection between nodes is becoming a major bottleneck. Various methods of gradient compression and improved model synchronization have been proposed to address this bottleneck in Parameter-Server-based DDL. However, these two types of methods can result in accuracy loss due to discarded gradients and have limited enhancement on the throughput of model synchronization, respectively. To address these challenges, we propose a new model synchronization method named Overlapped Synchronization Parallel (OSP), which achieves efficient communication with a 2-stage synchronization approach and uses Local-Gradient-based Parameter correction (LGP) to avoid accuracy loss caused by stale parameters. The prototype of OSP has been implemented using PyTo",
    "link": "http://arxiv.org/abs/2306.16926",
    "context": "Title: OSP: Boosting Distributed Model Training with 2-stage Synchronization. (arXiv:2306.16926v1 [cs.DC])\nAbstract: Distributed deep learning (DDL) is a promising research area, which aims to increase the efficiency of training deep learning tasks with large size of datasets and models. As the computation capability of DDL nodes continues to increase, the network connection between nodes is becoming a major bottleneck. Various methods of gradient compression and improved model synchronization have been proposed to address this bottleneck in Parameter-Server-based DDL. However, these two types of methods can result in accuracy loss due to discarded gradients and have limited enhancement on the throughput of model synchronization, respectively. To address these challenges, we propose a new model synchronization method named Overlapped Synchronization Parallel (OSP), which achieves efficient communication with a 2-stage synchronization approach and uses Local-Gradient-based Parameter correction (LGP) to avoid accuracy loss caused by stale parameters. The prototype of OSP has been implemented using PyTo",
    "path": "papers/23/06/2306.16926.json",
    "total_tokens": 824,
    "translated_title": "OSP: 使用两阶段同步提升分布式模型训练",
    "translated_abstract": "分布式深度学习（DDL）是一个有前景的研究领域，旨在提高训练大规模数据集和模型的深度学习任务的效率。随着DDL节点的计算能力不断增强，节点之间的网络连接成为一个主要瓶颈。为了解决参数服务器式DDL中这个瓶颈问题，人们提出了各种梯度压缩和改进的模型同步方法。然而，这两种方法可能会导致丢失梯度而出现精度损失，并且对模型同步的吞吐量的提升有限。为了解决这些挑战，我们提出了一种新的模型同步方法，名为Overlapped Synchronization Parallel（OSP），它采用两阶段同步方法实现高效通信，并使用基于本地梯度的参数修正（LGP）来避免由过期参数引起的精度损失。OSP的原型使用PyTo实现。",
    "tldr": "OSP是一种新的分布式模型训练方法，通过使用两阶段同步和本地梯度修正来提高通信效率，避免了精度损失。",
    "en_tdlr": "OSP is a new method for distributed model training that improves communication efficiency by using 2-stage synchronization and local gradient correction to avoid accuracy loss."
}