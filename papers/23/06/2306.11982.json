{
    "title": "Balanced Mixture of SuperNets for Learning the CNN Pooling Architecture. (arXiv:2306.11982v1 [cs.CV])",
    "abstract": "Downsampling layers, including pooling and strided convolutions, are crucial components of the convolutional neural network architecture that determine both the granularity/scale of image feature analysis as well as the receptive field size of a given layer. To fully understand this problem, we analyse the performance of models independently trained with each pooling configurations on CIFAR10, using a ResNet20 network, and show that the position of the downsampling layers can highly influence the performance of a network and predefined downsampling configurations are not optimal. Network Architecture Search (NAS) might be used to optimize downsampling configurations as an hyperparameter. However, we find that common one-shot NAS based on a single SuperNet does not work for this problem. We argue that this is because a SuperNet trained for finding the optimal pooling configuration fully shares its parameters among all pooling configurations. This makes its training hard, because learnin",
    "link": "http://arxiv.org/abs/2306.11982",
    "context": "Title: Balanced Mixture of SuperNets for Learning the CNN Pooling Architecture. (arXiv:2306.11982v1 [cs.CV])\nAbstract: Downsampling layers, including pooling and strided convolutions, are crucial components of the convolutional neural network architecture that determine both the granularity/scale of image feature analysis as well as the receptive field size of a given layer. To fully understand this problem, we analyse the performance of models independently trained with each pooling configurations on CIFAR10, using a ResNet20 network, and show that the position of the downsampling layers can highly influence the performance of a network and predefined downsampling configurations are not optimal. Network Architecture Search (NAS) might be used to optimize downsampling configurations as an hyperparameter. However, we find that common one-shot NAS based on a single SuperNet does not work for this problem. We argue that this is because a SuperNet trained for finding the optimal pooling configuration fully shares its parameters among all pooling configurations. This makes its training hard, because learnin",
    "path": "papers/23/06/2306.11982.json",
    "total_tokens": 902,
    "translated_title": "用均衡混合的超网络学习CNN池化结构",
    "translated_abstract": "下采样层是卷积神经网络结构的关键组成部分，决定图像特征分析的粒度/尺度以及给定层的感受野大小。为了深入了解这个问题，我们使用ResNet20网络分析了在CIFAR10上各个池化配置单独训练的模型的性能，并且表明下采样层的位置可以极大地影响网络的性能，预定义的下采样配置并不是最优的。网络架构搜索（NAS）可以用作优化下采样配置的超参数。但是，我们发现基于单个超网络的常见一次性 NAS 在这个问题上不起作用。我们认为这是因为为了找到最优池化配置而训练的超网络将其参数完全共享于所有池化配置。这使它的训练非常困难，因为学习率的调整不当会导致不稳定的反向传播。我们提出一个新的均衡混合超网络（BMSN）来解决这个问题。",
    "tldr": "本研究分析了在CIFAR10数据集上不同池化配置下的模型性能表现，并发现预定义的下采样配置不一定是最优的。为了解决这个问题，提出了一个新的均衡混合超网络（BMSN）来优化下采样配置。",
    "en_tdlr": "This paper analyzes the performance of models with different pooling configurations on CIFAR10 dataset and shows that predefined downsampling configurations are not necessarily optimal. A Balanced Mixture of SuperNets (BMSN) is proposed to optimize downsampling configurations."
}