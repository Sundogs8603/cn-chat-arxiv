{
    "title": "S$^{3}$: Increasing GPU Utilization during Generative Inference for Higher Throughput. (arXiv:2306.06000v1 [cs.AR])",
    "abstract": "Generating texts with a large language model (LLM) consumes massive amounts of memory. Apart from the already-large model parameters, the key/value (KV) cache that holds information about previous tokens in a sequence can grow to be even larger than the model itself. This problem is exacerbated in one of the current LLM serving frameworks which reserves the maximum sequence length of memory for the KV cache to guarantee generating a complete sequence as they do not know the output sequence length. This restricts us to use a smaller batch size leading to lower GPU utilization and above all, lower throughput. We argue that designing a system with a priori knowledge of the output sequence can mitigate this problem. To this end, we propose S$^{3}$, which predicts the output sequence length, schedules generation queries based on the prediction to increase device resource utilization and throughput, and handle mispredictions. Our proposed method achieves 6.49$\\times$ throughput over those sy",
    "link": "http://arxiv.org/abs/2306.06000",
    "context": "Title: S$^{3}$: Increasing GPU Utilization during Generative Inference for Higher Throughput. (arXiv:2306.06000v1 [cs.AR])\nAbstract: Generating texts with a large language model (LLM) consumes massive amounts of memory. Apart from the already-large model parameters, the key/value (KV) cache that holds information about previous tokens in a sequence can grow to be even larger than the model itself. This problem is exacerbated in one of the current LLM serving frameworks which reserves the maximum sequence length of memory for the KV cache to guarantee generating a complete sequence as they do not know the output sequence length. This restricts us to use a smaller batch size leading to lower GPU utilization and above all, lower throughput. We argue that designing a system with a priori knowledge of the output sequence can mitigate this problem. To this end, we propose S$^{3}$, which predicts the output sequence length, schedules generation queries based on the prediction to increase device resource utilization and throughput, and handle mispredictions. Our proposed method achieves 6.49$\\times$ throughput over those sy",
    "path": "papers/23/06/2306.06000.json",
    "total_tokens": 865,
    "translated_title": "S$^{3}$: 提高生成推断 GPU 利用率的方法以达到更高的吞吐量",
    "translated_abstract": "利用大型语言模型生成文本需要消耗大量的内存。除了大型模型参数外，保存序列中前面标记信息的键/值（KV）缓存也可能比模型本身更大。在当前的LLM服务框架中，这个问题被加剧，因为其将最大序列长度的内存保留给KV缓存，以确保生成完整的序列，但输出序列长度是未知的。这限制了我们使用较小的批量大小，导致GPU利用率降低，吞吐量也降低。我们认为，设计一个具有先验输出序列长度的系统可以缓解这个问题。因此，我们提出了 S$^{3}$，它预测输出序列长度，基于预测调度生成查询以提高设备资源利用率和吞吐量，并处理预测错误。我们提出的方法使吞吐量比其他方法提高了6.49倍。",
    "tldr": "该论文提出了一种名为S$^{3}$的方法，通过预测输出序列长度并调度生成查询，以提高设备资源利用率和吞吐量，该方法比其他方法提高了6.49倍的吞吐量。"
}