{
    "title": "Investigating Cross-Domain Behaviors of BERT in Review Understanding. (arXiv:2306.15123v1 [cs.CL])",
    "abstract": "Review score prediction requires review text understanding, a critical real-world application of natural language processing. Due to dissimilar text domains in product reviews, a common practice is fine-tuning BERT models upon reviews of differing domains. However, there has not yet been an empirical study of cross-domain behaviors of BERT models in the various tasks of product review understanding. In this project, we investigate text classification BERT models fine-tuned on single-domain and multi-domain Amazon review data. In our findings, though single-domain models achieved marginally improved performance on their corresponding domain compared to multi-domain models, multi-domain models outperformed single-domain models when evaluated on multi-domain data, single-domain data the single-domain model was not fine-tuned on, and on average when considering all tests. Though slight increases in accuracy can be achieved through single-domain model fine-tuning, computational resources an",
    "link": "http://arxiv.org/abs/2306.15123",
    "context": "Title: Investigating Cross-Domain Behaviors of BERT in Review Understanding. (arXiv:2306.15123v1 [cs.CL])\nAbstract: Review score prediction requires review text understanding, a critical real-world application of natural language processing. Due to dissimilar text domains in product reviews, a common practice is fine-tuning BERT models upon reviews of differing domains. However, there has not yet been an empirical study of cross-domain behaviors of BERT models in the various tasks of product review understanding. In this project, we investigate text classification BERT models fine-tuned on single-domain and multi-domain Amazon review data. In our findings, though single-domain models achieved marginally improved performance on their corresponding domain compared to multi-domain models, multi-domain models outperformed single-domain models when evaluated on multi-domain data, single-domain data the single-domain model was not fine-tuned on, and on average when considering all tests. Though slight increases in accuracy can be achieved through single-domain model fine-tuning, computational resources an",
    "path": "papers/23/06/2306.15123.json",
    "total_tokens": 914,
    "translated_title": "研究BERT在评论理解中的跨域行为",
    "translated_abstract": "评论分数预测需要理解评论文本，这是自然语言处理的一个关键实际应用。由于产品评论中的文本领域不同，常见做法是在不同领域的评论上对BERT模型进行微调。然而，目前还没有对BERT模型在产品评论理解的各种任务中的跨域行为进行实证研究。在本项目中，我们研究了在单域和多域亚马逊评论数据上微调的文本分类BERT模型。通过我们的发现，尽管单域模型在对应域上的性能略有提高，但在多域数据上评估时，多域模型优于单域模型，特别是在单域模型未进行微调的单域数据上，以及在考虑所有测试时的平均性能。虽然通过单域模型微调可以略微提高准确性，但计算资源也会增加。",
    "tldr": "该研究调查了在产品评论理解的各种任务中，BERT模型在不同域上的跨域行为。尽管单域模型在对应域上略有提高，多域模型在评估多域数据时表现更好，并且在平均测试中也更优。尽管单域模型微调可以提高准确性，但会增加计算资源消耗。",
    "en_tdlr": "This study investigates the cross-domain behaviors of BERT models in various tasks of product review understanding. While single-domain models achieve marginal improvements on their corresponding domains, multi-domain models outperform single-domain models when evaluating multi-domain data, and overall in the average tests. Though fine-tuning single-domain models can slightly increase accuracy, it comes with increased computational resources consumption."
}