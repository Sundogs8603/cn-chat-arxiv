{
    "title": "Large-scale Language Model Rescoring on Long-form Data. (arXiv:2306.08133v1 [eess.AS])",
    "abstract": "In this work, we study the impact of Large-scale Language Models (LLM) on Automated Speech Recognition (ASR) of YouTube videos, which we use as a source for long-form ASR. We demonstrate up to 8\\% relative reduction in Word Error Eate (WER) on US English (en-us) and code-switched Indian English (en-in) long-form ASR test sets and a reduction of up to 30\\% relative on Salient Term Error Rate (STER) over a strong first-pass baseline that uses a maximum-entropy based language model. Improved lattice processing that results in a lattice with a proper (non-tree) digraph topology and carrying context from the 1-best hypothesis of the previous segment(s) results in significant wins in rescoring with LLMs. We also find that the gains in performance from the combination of LLMs trained on vast quantities of available data (such as C4) and conventional neural LMs is additive and significantly outperforms a strong first-pass baseline with a maximum entropy LM.",
    "link": "http://arxiv.org/abs/2306.08133",
    "context": "Title: Large-scale Language Model Rescoring on Long-form Data. (arXiv:2306.08133v1 [eess.AS])\nAbstract: In this work, we study the impact of Large-scale Language Models (LLM) on Automated Speech Recognition (ASR) of YouTube videos, which we use as a source for long-form ASR. We demonstrate up to 8\\% relative reduction in Word Error Eate (WER) on US English (en-us) and code-switched Indian English (en-in) long-form ASR test sets and a reduction of up to 30\\% relative on Salient Term Error Rate (STER) over a strong first-pass baseline that uses a maximum-entropy based language model. Improved lattice processing that results in a lattice with a proper (non-tree) digraph topology and carrying context from the 1-best hypothesis of the previous segment(s) results in significant wins in rescoring with LLMs. We also find that the gains in performance from the combination of LLMs trained on vast quantities of available data (such as C4) and conventional neural LMs is additive and significantly outperforms a strong first-pass baseline with a maximum entropy LM.",
    "path": "papers/23/06/2306.08133.json",
    "total_tokens": 955,
    "translated_title": "基于大规模语言模型的长形式数据重新评分研究",
    "translated_abstract": "本文研究了大规模语言模型（LLM）对YouTube视频的自动语音识别（ASR）的影响，这些视频被用作长形式ASR的源。我们证明在美国英语（en-us）和印度英语（en-in）长形式ASR测试集上，相对于基于最大熵的语言模型强一次通过基线，我们实现了高达8％的相对Word Error Rate（WER）降低和高达30％的相对Salient Term Error Rate（STER）降低。经过改进的格处理导致带有正确（非树形）有向图拓扑和携带前一段最佳假设的上下文的格的显着获胜。我们还发现，基于大量可用数据（如C4）的LLMs和传统神经LMs的组合的性能提升是累加的，并且显着优于具有最大熵LM的强一次通过基线。",
    "tldr": "本文研究了大规模语言模型对长视频ASR的影响，证明与最大熵基线相比，使用LLM能够最多减少8％的Word Error Rate和30％的Salient Term Error Rate。经过改进的格处理和携带上下文的组合可以获得更好的效果。",
    "en_tdlr": "This work explores the impact of Large-scale Language Models (LLM) on long-form Automated Speech Recognition (ASR) and demonstrates that using LLMs can reduce Word Error Rate by up to 8% and Salient Term Error Rate by up to 30% compared to a maximum-entropy based language model. Improved lattice processing and incorporating context from previous segments can further improve ASR performance."
}