{
    "title": "Inverse Scaling: When Bigger Isn't Better. (arXiv:2306.09479v1 [cs.CL])",
    "abstract": "Work on scaling laws has found that large language models (LMs) show predictable improvements to overall loss with increased scale (model size, training data, and compute). Here, we present evidence for the claim that LMs may show inverse scaling, or worse task performance with increased scale, e.g., due to flaws in the training objective and data. We present empirical evidence of inverse scaling on 11 datasets collected by running a public contest, the Inverse Scaling Prize, with a substantial prize pool. Through analysis of the datasets, along with other examples found in the literature, we identify four potential causes of inverse scaling: (i) preference to repeat memorized sequences over following in-context instructions, (ii) imitation of undesirable patterns in the training data, (iii) tasks containing an easy distractor task which LMs could focus on, rather than the harder real task, and (iv) correct but misleading few-shot demonstrations of the task. We release the winning data",
    "link": "http://arxiv.org/abs/2306.09479",
    "context": "Title: Inverse Scaling: When Bigger Isn't Better. (arXiv:2306.09479v1 [cs.CL])\nAbstract: Work on scaling laws has found that large language models (LMs) show predictable improvements to overall loss with increased scale (model size, training data, and compute). Here, we present evidence for the claim that LMs may show inverse scaling, or worse task performance with increased scale, e.g., due to flaws in the training objective and data. We present empirical evidence of inverse scaling on 11 datasets collected by running a public contest, the Inverse Scaling Prize, with a substantial prize pool. Through analysis of the datasets, along with other examples found in the literature, we identify four potential causes of inverse scaling: (i) preference to repeat memorized sequences over following in-context instructions, (ii) imitation of undesirable patterns in the training data, (iii) tasks containing an easy distractor task which LMs could focus on, rather than the harder real task, and (iv) correct but misleading few-shot demonstrations of the task. We release the winning data",
    "path": "papers/23/06/2306.09479.json",
    "total_tokens": 963,
    "translated_title": "逆向缩放：变得更大并不意味着更好",
    "translated_abstract": "近期研究表明，随着模型规模、训练数据、计算量的增加，大型语言模型（LMs）的损失比例有可预测的改进。然而，本研究提供了证据表明，LMs也可能显示逆向缩放，即随着规模的增加任务性能越来越差，这可能是由于训练目标和数据的缺陷所致。本文通过公开比赛，Inverse Scaling Prize，在11个数据集上进行实证研究，证明了逆向缩放现象。通过分析数据集及其他实例，我们认为逆向缩放的原因可能有四种：（i）倾向于重复记忆的序列而非跟随上下文指示，（ii）在训练数据中模仿不良模式，（iii）任务中有一个易于干扰LMs的任务，将其注意力转移到较简单的任务，而非较难的任务，（iv）任务的正确示范误导LMs。作者还公布了比赛的获胜数据。",
    "tldr": "本文研究发现，相对于规模的增加，大型语言模型的任务性能可能出现逆向缩放现象。这一逆向缩放的原因可能有四种：记忆重现、学习样本错误、任务易于干扰、和任务示范的误导。",
    "en_tdlr": "This paper presents evidence that large language models (LMs) may show inverse scaling, i.e., worse task performance with increased scale, and identifies four potential causes of inverse scaling: repeat memorized sequences, imitation of undesirable patterns, focus on easier distractor tasks, and misleading few-shot demonstrations. The study implies that increasing the scale of LMs may not always lead to better performance."
}