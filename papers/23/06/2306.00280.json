{
    "title": "Towards Bias Correction of FedAvg over Nonuniform and Time-Varying Communications. (arXiv:2306.00280v1 [cs.LG])",
    "abstract": "Federated learning (FL) is a decentralized learning framework wherein a parameter server (PS) and a collection of clients collaboratively train a model via minimizing a global objective. Communication bandwidth is a scarce resource; in each round, the PS aggregates the updates from a subset of clients only. In this paper, we focus on non-convex minimization that is vulnerable to non-uniform and time-varying communication failures between the PS and the clients. Specifically, in each round $t$, the link between the PS and client $i$ is active with probability $p_i^t$, which is $\\textit{unknown}$ to both the PS and the clients. This arises when the channel conditions are heterogeneous across clients and are changing over time.  We show that when the $p_i^t$'s are not uniform, $\\textit{Federated Average}$ (FedAvg) -- the most widely adopted FL algorithm -- fails to minimize the global objective. Observing this, we propose $\\textit{Federated Postponed Broadcast}$ (FedPBC) which is a simple",
    "link": "http://arxiv.org/abs/2306.00280",
    "context": "Title: Towards Bias Correction of FedAvg over Nonuniform and Time-Varying Communications. (arXiv:2306.00280v1 [cs.LG])\nAbstract: Federated learning (FL) is a decentralized learning framework wherein a parameter server (PS) and a collection of clients collaboratively train a model via minimizing a global objective. Communication bandwidth is a scarce resource; in each round, the PS aggregates the updates from a subset of clients only. In this paper, we focus on non-convex minimization that is vulnerable to non-uniform and time-varying communication failures between the PS and the clients. Specifically, in each round $t$, the link between the PS and client $i$ is active with probability $p_i^t$, which is $\\textit{unknown}$ to both the PS and the clients. This arises when the channel conditions are heterogeneous across clients and are changing over time.  We show that when the $p_i^t$'s are not uniform, $\\textit{Federated Average}$ (FedAvg) -- the most widely adopted FL algorithm -- fails to minimize the global objective. Observing this, we propose $\\textit{Federated Postponed Broadcast}$ (FedPBC) which is a simple",
    "path": "papers/23/06/2306.00280.json",
    "total_tokens": 1249,
    "translated_title": "面向非均匀和时变通信的FedAvg偏差修正研究",
    "translated_abstract": "联邦学习是一种分散的学习框架，在这个框架下，参数服务器（PS）和多个客户端合作通过最小化全局目标来训练模型。通信带宽是一种稀缺资源；在每轮迭代中，PS仅从客户端的子集聚合更新。本文关注的是非凸优化问题，这种问题容易受到PS和客户端之间不均匀、时变的通信故障的影响。具体而言，在每轮$t$中，PS和客户端$i$之间的连接只有概率$p_i^t$是活跃的，而这个概率是不知道的。当信道条件在客户端之间异构并且随时间变化时，这种情况会发生。我们发现当$p_i^t$非均匀时，被广泛采用的FL算法FedAvg无法最小化全局目标。鉴于此，我们提出了一种名为FedPBC的简单有效的方法来减轻非均匀通信故障的影响。FedPBC在客户端之间构建二叉树，在建立拓扑结构之前推迟更新的传输，并捕捉本地更新的重要性。我们在温和假设下理论证明了FedPBC收敛于非凸问题的全局最优解。在非凸任务（如图像分类和语言建模）上的实验结果表明，FedPBC在非均匀和时变的通信设置下优于FedAvg。",
    "tldr": "本文面向非均匀和时变通信故障问题，提出了一种名为FedPBC的联邦学习算法来修正FedAvg的偏差，该算法构建二叉树来减轻非均匀通信故障的影响，并在非凸任务上实验结果表现优于FedAvg。",
    "en_tdlr": "This paper proposes a federated learning algorithm called FedPBC to correct the bias of FedAvg in the face of non-uniform and time-varying communication failures, which constructs a binary tree to mitigate the impact of non-uniform communication failures, postpones the transmission of updates, captures the importance of local updates, and outperforms FedAvg on non-convex tasks according to experimental results."
}