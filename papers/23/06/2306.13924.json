{
    "title": "Structuring Representation Geometry with Rotationally Equivariant Contrastive Learning. (arXiv:2306.13924v1 [cs.LG])",
    "abstract": "Self-supervised learning converts raw perceptual data such as images to a compact space where simple Euclidean distances measure meaningful variations in data. In this paper, we extend this formulation by adding additional geometric structure to the embedding space by enforcing transformations of input space to correspond to simple (i.e., linear) transformations of embedding space. Specifically, in the contrastive learning setting, we introduce an equivariance objective and theoretically prove that its minima forces augmentations on input space to correspond to rotations on the spherical embedding space. We show that merely combining our equivariant loss with a non-collapse term results in non-trivial representations, without requiring invariance to data augmentations. Optimal performance is achieved by also encouraging approximate invariance, where input augmentations correspond to small rotations. Our method, CARE: Contrastive Augmentation-induced Rotational Equivariance, leads to im",
    "link": "http://arxiv.org/abs/2306.13924",
    "context": "Title: Structuring Representation Geometry with Rotationally Equivariant Contrastive Learning. (arXiv:2306.13924v1 [cs.LG])\nAbstract: Self-supervised learning converts raw perceptual data such as images to a compact space where simple Euclidean distances measure meaningful variations in data. In this paper, we extend this formulation by adding additional geometric structure to the embedding space by enforcing transformations of input space to correspond to simple (i.e., linear) transformations of embedding space. Specifically, in the contrastive learning setting, we introduce an equivariance objective and theoretically prove that its minima forces augmentations on input space to correspond to rotations on the spherical embedding space. We show that merely combining our equivariant loss with a non-collapse term results in non-trivial representations, without requiring invariance to data augmentations. Optimal performance is achieved by also encouraging approximate invariance, where input augmentations correspond to small rotations. Our method, CARE: Contrastive Augmentation-induced Rotational Equivariance, leads to im",
    "path": "papers/23/06/2306.13924.json",
    "total_tokens": 890,
    "translated_title": "使用旋转等变对比学习构建表示几何",
    "translated_abstract": "自我监督学习将原始感知数据（如图像）转换为一个紧凑的空间，在这个空间中，简单的欧几里得距离可以衡量数据的有意义变化。本文通过将输入空间的变换对应于嵌入空间的简单（即线性）变换，来增加嵌入空间的额外几何结构。特别地，在对比学习场景下，我们引入等变性目标，并在理论上证明了最小值强制输入空间的增强变换对应于球形嵌入空间上的旋转变换。我们展示了仅将等变损失与非折叠项相结合可以导致非平凡的表示，而无需对数据增强具有不变性。通过鼓励近似不变性，即输入增强对应于小的旋转，可以实现最优性能。我们的方法CARE：通过对比增强诱导旋转等变性，提供更好的嵌入表示能力。",
    "tldr": "本文在对比学习中引入了等变性目标并理论证明了最优解会将输入空间的增强变换对应于球形嵌入空间上的旋转变换，而我们的方法CARE获得更好的嵌入表示能力。",
    "en_tdlr": "This paper introduces an equivariance objective in contrastive learning and proves that its minima forces input augmentations to correspond to rotations on the spherical embedding space. Their method, CARE: Contrastive Augmentation-induced Rotational Equivariance, achieves better embedding representation by encouraging approximate invariance of input augmentations corresponding to small rotations."
}