{
    "title": "Pb-Hash: Partitioned b-bit Hashing. (arXiv:2306.15944v1 [cs.LG])",
    "abstract": "Many hashing algorithms including minwise hashing (MinHash), one permutation hashing (OPH), and consistent weighted sampling (CWS) generate integers of $B$ bits. With $k$ hashes for each data vector, the storage would be $B\\times k$ bits; and when used for large-scale learning, the model size would be $2^B\\times k$, which can be expensive. A standard strategy is to use only the lowest $b$ bits out of the $B$ bits and somewhat increase $k$, the number of hashes. In this study, we propose to re-use the hashes by partitioning the $B$ bits into $m$ chunks, e.g., $b\\times m =B$. Correspondingly, the model size becomes $m\\times 2^b \\times k$, which can be substantially smaller than the original $2^B\\times k$.  Our theoretical analysis reveals that by partitioning the hash values into $m$ chunks, the accuracy would drop. In other words, using $m$ chunks of $B/m$ bits would not be as accurate as directly using $B$ bits. This is due to the correlation from re-using the same hash. On the other h",
    "link": "http://arxiv.org/abs/2306.15944",
    "context": "Title: Pb-Hash: Partitioned b-bit Hashing. (arXiv:2306.15944v1 [cs.LG])\nAbstract: Many hashing algorithms including minwise hashing (MinHash), one permutation hashing (OPH), and consistent weighted sampling (CWS) generate integers of $B$ bits. With $k$ hashes for each data vector, the storage would be $B\\times k$ bits; and when used for large-scale learning, the model size would be $2^B\\times k$, which can be expensive. A standard strategy is to use only the lowest $b$ bits out of the $B$ bits and somewhat increase $k$, the number of hashes. In this study, we propose to re-use the hashes by partitioning the $B$ bits into $m$ chunks, e.g., $b\\times m =B$. Correspondingly, the model size becomes $m\\times 2^b \\times k$, which can be substantially smaller than the original $2^B\\times k$.  Our theoretical analysis reveals that by partitioning the hash values into $m$ chunks, the accuracy would drop. In other words, using $m$ chunks of $B/m$ bits would not be as accurate as directly using $B$ bits. This is due to the correlation from re-using the same hash. On the other h",
    "path": "papers/23/06/2306.15944.json",
    "total_tokens": 913,
    "translated_title": "Pb-Hash: 分区b位哈希",
    "translated_abstract": "许多哈希算法，包括minwise哈希（MinHash），一次置换哈希（OPH）和一致加权采样（CWS），生成B位整数。对于每个数据向量的k个哈希，存储空间将是B×k位；当用于大规模学习时，模型大小将是2^B×k，这可能很昂贵。一种标准策略是仅使用B位中的最低b位，并略微增加哈希的数量k。在这项研究中，我们提出通过将B位分成m个块，例如b×m=B，来重复使用哈希。对应地，模型大小变为m×2^b×k，这可能比原来的2^B×k要小得多。我们的理论分析显示，通过将哈希值分成m个块，准确性会下降。换句话说，使用B/m位的m个块将不如直接使用B位精确。这是由于通过重新使用相同的哈希值引起的相关性。另一方面，",
    "tldr": "Pb-Hash提出了一种分区b位哈希的方法，通过将B位哈希分成m个块来重复使用已有的哈希，能够显著减小模型的大小。",
    "en_tdlr": "Pb-Hash proposes a method of partitioning b-bit hashing, which reduces the model size substantially by reusing existing hashes through partitioning the B-bit hashes into m chunks."
}