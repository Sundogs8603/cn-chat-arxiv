{
    "title": "Enhancing Representation Learning on High-Dimensional, Small-Size Tabular Data: A Divide and Conquer Method with Ensembled VAEs. (arXiv:2306.15661v1 [cs.LG])",
    "abstract": "Variational Autoencoders and their many variants have displayed impressive ability to perform dimensionality reduction, often achieving state-of-the-art performance. Many current methods however, struggle to learn good representations in High Dimensional, Low Sample Size (HDLSS) tasks, which is an inherently challenging setting. We address this challenge by using an ensemble of lightweight VAEs to learn posteriors over subsets of the feature-space, which get aggregated into a joint posterior in a novel divide-and-conquer approach. Specifically, we present an alternative factorisation of the joint posterior that induces a form of implicit data augmentation that yields greater sample efficiency. Through a series of experiments on eight real-world datasets, we show that our method learns better latent representations in HDLSS settings, which leads to higher accuracy in a downstream classification task. Furthermore, we verify that our approach has a positive effect on disentanglement and a",
    "link": "http://arxiv.org/abs/2306.15661",
    "context": "Title: Enhancing Representation Learning on High-Dimensional, Small-Size Tabular Data: A Divide and Conquer Method with Ensembled VAEs. (arXiv:2306.15661v1 [cs.LG])\nAbstract: Variational Autoencoders and their many variants have displayed impressive ability to perform dimensionality reduction, often achieving state-of-the-art performance. Many current methods however, struggle to learn good representations in High Dimensional, Low Sample Size (HDLSS) tasks, which is an inherently challenging setting. We address this challenge by using an ensemble of lightweight VAEs to learn posteriors over subsets of the feature-space, which get aggregated into a joint posterior in a novel divide-and-conquer approach. Specifically, we present an alternative factorisation of the joint posterior that induces a form of implicit data augmentation that yields greater sample efficiency. Through a series of experiments on eight real-world datasets, we show that our method learns better latent representations in HDLSS settings, which leads to higher accuracy in a downstream classification task. Furthermore, we verify that our approach has a positive effect on disentanglement and a",
    "path": "papers/23/06/2306.15661.json",
    "total_tokens": 909,
    "translated_title": "在高维度、小样本表格数据上增强表示学习：一种基于集成VAE的分而治之方法",
    "translated_abstract": "变分自动编码器及其各种变体展现了在降维任务中的出色性能，通常能达到最新水平。然而，在高维度、低样本数量(HDLSS)任务中，许多当前方法在学习良好的表示方面存在困难，这是一个固有的具有挑战性的设置。我们通过使用轻量级VAE的集成来学习特征空间子集上的后验概率，从而在新颖的分而治之方法中聚合到一个联合后验中，从而应对这一挑战。具体而言，我们提出了联合后验的一种替代分解方式，引入了一种隐式数据增强形式，提高了样本效率。通过对八个真实数据集进行一系列实验，我们展示了我们的方法在HDLSS设置中学习到更好的潜在表示，这导致了下游分类任务中更高的准确性。此外，我们验证我们的方法对解缠缚效果有积极影响。",
    "tldr": "这项研究使用了基于集成VAE的分而治之方法，在高维度、低样本数量的任务中学习到更好的潜在表示，提高了样本效率，并在下游分类任务中取得了更高的准确性。"
}