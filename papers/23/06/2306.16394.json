{
    "title": "Sharper Model-free Reinforcement Learning for Average-reward Markov Decision Processes. (arXiv:2306.16394v1 [cs.LG])",
    "abstract": "We develop several provably efficient model-free reinforcement learning (RL) algorithms for infinite-horizon average-reward Markov Decision Processes (MDPs). We consider both online setting and the setting with access to a simulator. In the online setting, we propose model-free RL algorithms based on reference-advantage decomposition. Our algorithm achieves $\\widetilde{O}(S^5A^2\\mathrm{sp}(h^*)\\sqrt{T})$ regret after $T$ steps, where $S\\times A$ is the size of state-action space, and  $\\mathrm{sp}(h^*)$ the span of the optimal bias function. Our results are the first to achieve optimal dependence in $T$ for weakly communicating MDPs.  In the simulator setting, we propose a model-free RL algorithm that finds an $\\epsilon$-optimal policy using $\\widetilde{O} \\left(\\frac{SA\\mathrm{sp}^2(h^*)}{\\epsilon^2}+\\frac{S^2A\\mathrm{sp}(h^*)}{\\epsilon} \\right)$ samples, whereas the minimax lower bound is $\\Omega\\left(\\frac{SA\\mathrm{sp}(h^*)}{\\epsilon^2}\\right)$.  Our results are based on two new te",
    "link": "http://arxiv.org/abs/2306.16394",
    "context": "Title: Sharper Model-free Reinforcement Learning for Average-reward Markov Decision Processes. (arXiv:2306.16394v1 [cs.LG])\nAbstract: We develop several provably efficient model-free reinforcement learning (RL) algorithms for infinite-horizon average-reward Markov Decision Processes (MDPs). We consider both online setting and the setting with access to a simulator. In the online setting, we propose model-free RL algorithms based on reference-advantage decomposition. Our algorithm achieves $\\widetilde{O}(S^5A^2\\mathrm{sp}(h^*)\\sqrt{T})$ regret after $T$ steps, where $S\\times A$ is the size of state-action space, and  $\\mathrm{sp}(h^*)$ the span of the optimal bias function. Our results are the first to achieve optimal dependence in $T$ for weakly communicating MDPs.  In the simulator setting, we propose a model-free RL algorithm that finds an $\\epsilon$-optimal policy using $\\widetilde{O} \\left(\\frac{SA\\mathrm{sp}^2(h^*)}{\\epsilon^2}+\\frac{S^2A\\mathrm{sp}(h^*)}{\\epsilon} \\right)$ samples, whereas the minimax lower bound is $\\Omega\\left(\\frac{SA\\mathrm{sp}(h^*)}{\\epsilon^2}\\right)$.  Our results are based on two new te",
    "path": "papers/23/06/2306.16394.json",
    "total_tokens": 1062,
    "translated_title": "平均奖励马尔可夫决策过程的更精确的无模型强化学习",
    "translated_abstract": "我们提出了几种经过验证高效的无模型强化学习算法，用于无限周期平均奖励马尔可夫决策过程（MDPs）。我们考虑在线设置和拥有仿真器的设置。在在线设置中，我们提出了基于参考优势分解的无模型强化学习算法。我们的算法在T步之后达到 $\\widetilde{O}(S^5A^2\\mathrm{sp}(h^*)\\sqrt{T})$ 的遗憾，其中 $S\\times A$ 是状态-动作空间的大小， $\\mathrm{sp}(h^*)$ 是最优偏差函数的跨度。我们的结果是第一个在弱通信MDPs中达到T的最佳依赖性的。在仿真器设置中，我们提出了一个无模型强化学习算法，使用 $\\widetilde{O} \\left(\\frac{SA\\mathrm{sp}^2(h^*)}{\\epsilon^2}+\\frac{S^2A\\mathrm{sp}(h^*)}{\\epsilon} \\right)$ 个样本找到一个 $\\epsilon$-最优策略，而极小化下界是 $\\Omega\\left(\\frac{SA\\mathrm{sp}(h^*)}{\\epsilon^2}\\right)$。我们的结果基于两个新的技术",
    "tldr": "本论文提出了针对无限周期平均奖励马尔可夫决策过程（MDPs）的多个高效无模型强化学习（RL）算法，包括在线设置和仿真器设置，实现了最佳的遗憾和样本使用依赖性。",
    "en_tdlr": "This paper presents several provably efficient model-free reinforcement learning (RL) algorithms for infinite-horizon average-reward Markov Decision Processes (MDPs), achieving optimal regret and sample usage dependence in both online and simulator settings."
}