{
    "title": "Personalization Disentanglement for Federated Learning. (arXiv:2306.03570v1 [cs.LG])",
    "abstract": "Personalized federated learning (PFL) jointly trains a variety of local models through balancing between knowledge sharing across clients and model personalization per client. This paper addresses PFL via explicit disentangling latent representations into two parts to capture the shared knowledge and client-specific personalization, which leads to more reliable and effective PFL. The disentanglement is achieved by a novel Federated Dual Variational Autoencoder (FedDVA), which employs two encoders to infer the two types of representations. FedDVA can produce a better understanding of the trade-off between global knowledge sharing and local personalization in PFL. Moreover, it can be integrated with existing FL methods and turn them into personalized models for heterogeneous downstream tasks. Extensive experiments validate the advantages caused by disentanglement and show that models trained with disentangled representations substantially outperform those vanilla methods.",
    "link": "http://arxiv.org/abs/2306.03570",
    "context": "Title: Personalization Disentanglement for Federated Learning. (arXiv:2306.03570v1 [cs.LG])\nAbstract: Personalized federated learning (PFL) jointly trains a variety of local models through balancing between knowledge sharing across clients and model personalization per client. This paper addresses PFL via explicit disentangling latent representations into two parts to capture the shared knowledge and client-specific personalization, which leads to more reliable and effective PFL. The disentanglement is achieved by a novel Federated Dual Variational Autoencoder (FedDVA), which employs two encoders to infer the two types of representations. FedDVA can produce a better understanding of the trade-off between global knowledge sharing and local personalization in PFL. Moreover, it can be integrated with existing FL methods and turn them into personalized models for heterogeneous downstream tasks. Extensive experiments validate the advantages caused by disentanglement and show that models trained with disentangled representations substantially outperform those vanilla methods.",
    "path": "papers/23/06/2306.03570.json",
    "total_tokens": 874,
    "translated_title": "联邦学习的个性化分离",
    "translated_abstract": "个性化联邦学习（PFL）通过在客户端之间平衡的知识共享和模型个性化之间进行训练，从而联合训练各种局部模型。本文通过将潜在表示明确分解为两个部分来解决PFL问题，以捕捉共享知识和客户特定个性化，从而导致更可靠和有效的PFL。该分离是通过一种新颖的联邦双变分自编码器（FedDVA）实现的，它使用两个编码器来推断两种类型的表示。FedDVA可以更好地理解PFL中全局知识共享和本地个性化之间的权衡。此外，它可以与现有的FL方法集成，并将它们转变为用于异构下游任务的个性化模型。广泛的实验证实了分离所带来的优势，并表明经过分离训练的模型明显优于那些普通方法。",
    "tldr": "本文通过使用联邦双变分自编码器（FedDVA）明确分解潜在表示，捕捉共享知识和客户特定个性化，从而导致更可靠和有效的个性化联邦学习，并在广泛实验中验证了该方法的优越性。",
    "en_tdlr": "This paper proposes a novel Federated Dual Variational Autoencoder (FedDVA) to explicitly disentangle latent representations into shared knowledge and client-specific personalization for personalized federated learning (PFL), and extensively validates its advantage over vanilla methods."
}