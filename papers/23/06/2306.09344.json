{
    "title": "DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data. (arXiv:2306.09344v2 [cs.CV] UPDATED)",
    "abstract": "Current perceptual similarity metrics operate at the level of pixels and patches. These metrics compare images in terms of their low-level colors and textures, but fail to capture mid-level similarities and differences in image layout, object pose, and semantic content. In this paper, we develop a perceptual metric that assesses images holistically. Our first step is to collect a new dataset of human similarity judgments over image pairs that are alike in diverse ways. Critical to this dataset is that judgments are nearly automatic and shared by all observers. To achieve this we use recent text-to-image models to create synthetic pairs that are perturbed along various dimensions. We observe that popular perceptual metrics fall short of explaining our new data, and we introduce a new metric, DreamSim, tuned to better align with human perception. We analyze how our metric is affected by different visual attributes, and find that it focuses heavily on foreground objects and semantic conte",
    "link": "http://arxiv.org/abs/2306.09344",
    "context": "Title: DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data. (arXiv:2306.09344v2 [cs.CV] UPDATED)\nAbstract: Current perceptual similarity metrics operate at the level of pixels and patches. These metrics compare images in terms of their low-level colors and textures, but fail to capture mid-level similarities and differences in image layout, object pose, and semantic content. In this paper, we develop a perceptual metric that assesses images holistically. Our first step is to collect a new dataset of human similarity judgments over image pairs that are alike in diverse ways. Critical to this dataset is that judgments are nearly automatic and shared by all observers. To achieve this we use recent text-to-image models to create synthetic pairs that are perturbed along various dimensions. We observe that popular perceptual metrics fall short of explaining our new data, and we introduce a new metric, DreamSim, tuned to better align with human perception. We analyze how our metric is affected by different visual attributes, and find that it focuses heavily on foreground objects and semantic conte",
    "path": "papers/23/06/2306.09344.json",
    "total_tokens": 984,
    "translated_title": "DreamSim：使用合成数据学习人类视觉相似性的新维度",
    "translated_abstract": "当前的感知相似性度量是在像素和图像块的层面操作的。这些度量使用低层次的颜色和纹理来比较图像，但未能捕捉图像布局、对象姿态和语义内容的中层次相似性和差异。本文提出了一种可以全面评估图像的感知度量。我们的第一步是收集一个包含多个相似维度图像对的人类相似性判断的新数据集。这个数据集的关键是评判是几乎自动的，并且由所有观察者共享。为了实现这一点，我们使用最近的文本到图像模型创建了一些沿不同维度扰动的合成图像对。我们观察到，现有的流行的感知度量无法解释我们的新数据，因此引入了一种新的度量DreamSim，以更好地与人类感知相匹配。我们分析了不同视觉属性如何影响度量结果，并发现它严重关注前景物体和语义内容。DreamSim表现出优越性能，在广泛的任务中比现有的度量更优，包括预测行为实验结果、预测对抗鲁棒性和与人类相似性判断相关。",
    "tldr": "本文提出了一种全面评估图像的感知度量DreamSim，该度量使用合成数据学习人类视觉相似性的新维度，表现出优越性能。",
    "en_tdlr": "The paper proposes a novel perceptual metric, DreamSim, which assesses holistic image similarity based on diverse dimensions learned from synthetic data, outperforming existing metrics in a range of tasks, including predicting human similarity judgments and adversarial robustness."
}