{
    "title": "Fast, Distribution-free Predictive Inference for Neural Networks with Coverage Guarantees. (arXiv:2306.06582v1 [stat.ML])",
    "abstract": "This paper introduces a novel, computationally-efficient algorithm for predictive inference (PI) that requires no distributional assumptions on the data and can be computed faster than existing bootstrap-type methods for neural networks. Specifically, if there are $n$ training samples, bootstrap methods require training a model on each of the $n$ subsamples of size $n-1$; for large models like neural networks, this process can be computationally prohibitive. In contrast, our proposed method trains one neural network on the full dataset with $(\\epsilon, \\delta)$-differential privacy (DP) and then approximates each leave-one-out model efficiently using a linear approximation around the differentially-private neural network estimate. With exchangeable data, we prove that our approach has a rigorous coverage guarantee that depends on the preset privacy parameters and the stability of the neural network, regardless of the data distribution. Simulations and experiments on real data demonstra",
    "link": "http://arxiv.org/abs/2306.06582",
    "context": "Title: Fast, Distribution-free Predictive Inference for Neural Networks with Coverage Guarantees. (arXiv:2306.06582v1 [stat.ML])\nAbstract: This paper introduces a novel, computationally-efficient algorithm for predictive inference (PI) that requires no distributional assumptions on the data and can be computed faster than existing bootstrap-type methods for neural networks. Specifically, if there are $n$ training samples, bootstrap methods require training a model on each of the $n$ subsamples of size $n-1$; for large models like neural networks, this process can be computationally prohibitive. In contrast, our proposed method trains one neural network on the full dataset with $(\\epsilon, \\delta)$-differential privacy (DP) and then approximates each leave-one-out model efficiently using a linear approximation around the differentially-private neural network estimate. With exchangeable data, we prove that our approach has a rigorous coverage guarantee that depends on the preset privacy parameters and the stability of the neural network, regardless of the data distribution. Simulations and experiments on real data demonstra",
    "path": "papers/23/06/2306.06582.json",
    "total_tokens": 1008,
    "translated_title": "具有覆盖保证的神经网络无分布预测推断的快速计算方法",
    "translated_abstract": "该论文提出了一种新颖的、计算效率高的预测推断(PI)算法，它不需要对数据做出分布假设，且比现有的神经网络引导式方法更快地计算。具体来说，如果有$n$个训练样本，引导式方法需要在大小为$n-1$的$n$个子样本上训练一个模型；对于像神经网络这样的大模型，这个过程可能计算上不切实际。相比之下，我们提出的方法在完整数据集上使用$(\\epsilon, \\delta)$-差分隐私(DP)训练一个神经网络，然后使用线性逼近来高效地近似每个留一出模型，该逼近是围绕差别私有的神经网络估计来进行的。对于交换数据，我们证明了我们的方法具有严格的覆盖保证，这取决于预设的隐私参数和神经网络的稳定性，而不取决于数据分布。模拟和实验数据证明了我们的方法比基于引导式的方法具有更好的计算效率。",
    "tldr": "该论文提出了一种新颖、计算效率高的预测推断算法，不需要数据做出分布假设。对于交换数据，通过使用$(\\epsilon, \\delta)$-差分隐私训练一个神经网络并围绕差别私有的神经网络估计进行线性逼近，该方法已被证明具有严格的覆盖保证。",
    "en_tdlr": "This paper proposes a novel and computationally-efficient predictive inference algorithm that does not require distributional assumptions on the data. With exchangeable data, rigorous coverage guarantee can be achieved through using $(\\epsilon, \\delta)$-differential privacy to train a neural network and approximating leave-one-out models efficiently using linear approximation around the differentially-private neural network estimate."
}