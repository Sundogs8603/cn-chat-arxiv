{
    "title": "Can Continual Learning Improve Long-Tailed Recognition? Toward a Unified Framework. (arXiv:2306.13275v1 [cs.LG])",
    "abstract": "The Long-Tailed Recognition (LTR) problem emerges in the context of learning from highly imbalanced datasets, in which the number of samples among different classes is heavily skewed. LTR methods aim to accurately learn a dataset comprising both a larger Head set and a smaller Tail set. We propose a theorem where under the assumption of strong convexity of the loss function, the weights of a learner trained on the full dataset are within an upper bound of the weights of the same learner trained strictly on the Head. Next, we assert that by treating the learning of the Head and Tail as two separate and sequential steps, Continual Learning (CL) methods can effectively update the weights of the learner to learn the Tail without forgetting the Head. First, we validate our theoretical findings with various experiments on the toy MNIST-LT dataset. We then evaluate the efficacy of several CL strategies on multiple imbalanced variations of two standard LTR benchmarks (CIFAR100-LT and CIFAR10-L",
    "link": "http://arxiv.org/abs/2306.13275",
    "context": "Title: Can Continual Learning Improve Long-Tailed Recognition? Toward a Unified Framework. (arXiv:2306.13275v1 [cs.LG])\nAbstract: The Long-Tailed Recognition (LTR) problem emerges in the context of learning from highly imbalanced datasets, in which the number of samples among different classes is heavily skewed. LTR methods aim to accurately learn a dataset comprising both a larger Head set and a smaller Tail set. We propose a theorem where under the assumption of strong convexity of the loss function, the weights of a learner trained on the full dataset are within an upper bound of the weights of the same learner trained strictly on the Head. Next, we assert that by treating the learning of the Head and Tail as two separate and sequential steps, Continual Learning (CL) methods can effectively update the weights of the learner to learn the Tail without forgetting the Head. First, we validate our theoretical findings with various experiments on the toy MNIST-LT dataset. We then evaluate the efficacy of several CL strategies on multiple imbalanced variations of two standard LTR benchmarks (CIFAR100-LT and CIFAR10-L",
    "path": "papers/23/06/2306.13275.json",
    "total_tokens": 942,
    "translated_title": "持续学习能改进长尾识别吗？走向统一框架",
    "translated_abstract": "在高度不平衡的数据集中，不同类别之间的样本数量极度失衡会出现长尾识别（LTR）问题。LTR方法旨在准确地学习包含一个较大“头”集和一个较小“尾”集的数据集。我们提出了一个定理，假设损失函数是强凸的，那么完整数据集上训练的学习者的权重在同一个学习者严格训练头集时的权重上限之内。接下来，我们声称将头集和尾集的学习视为两个独立的连续步骤，持续学习（CL）方法可以有效地更新学习者的权重以学习尾部，而不会忘记头部。首先，我们使用玩具MNIST-LT数据集验证了我们的理论发现。接着，我们在两个标准LTR基准（CIFAR100-LT和CIFAR10-L）的多个不平衡变体上评估了几种CL策略的有效性。",
    "tldr": "本文针对长尾识别问题，提出一种持续学习方法，通过将头部集和尾部集的学习视为两个独立连续的步骤，并利用定理证明持续学习可以有效地更新学习者的权重以学习尾部，同时不会忘记头部。"
}