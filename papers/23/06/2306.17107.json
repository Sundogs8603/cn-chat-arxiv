{
    "title": "LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding. (arXiv:2306.17107v1 [cs.CV])",
    "abstract": "Instruction tuning unlocks the superior capability of Large Language Models (LLM) to interact with humans. Furthermore, recent instruction-following datasets include images as visual inputs, collecting responses for image-based instructions. However, visual instruction-tuned models cannot comprehend textual details within images well. This work enhances the current visual instruction tuning pipeline with text-rich images (e.g., movie posters, book covers, etc.). Specifically, we first use publicly available OCR tools to collect results on 422K text-rich images from the LAION dataset. Moreover, we prompt text-only GPT-4 with recognized texts and image captions to generate 16K conversations, each containing question-answer pairs for text-rich images. By combining our collected data with previous multi-modal instruction-following data, our model, LLaVAR, substantially improves the LLaVA model's capability on text-based VQA datasets (up to 20% accuracy improvement) while achieving an accur",
    "link": "http://arxiv.org/abs/2306.17107",
    "context": "Title: LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding. (arXiv:2306.17107v1 [cs.CV])\nAbstract: Instruction tuning unlocks the superior capability of Large Language Models (LLM) to interact with humans. Furthermore, recent instruction-following datasets include images as visual inputs, collecting responses for image-based instructions. However, visual instruction-tuned models cannot comprehend textual details within images well. This work enhances the current visual instruction tuning pipeline with text-rich images (e.g., movie posters, book covers, etc.). Specifically, we first use publicly available OCR tools to collect results on 422K text-rich images from the LAION dataset. Moreover, we prompt text-only GPT-4 with recognized texts and image captions to generate 16K conversations, each containing question-answer pairs for text-rich images. By combining our collected data with previous multi-modal instruction-following data, our model, LLaVAR, substantially improves the LLaVA model's capability on text-based VQA datasets (up to 20% accuracy improvement) while achieving an accur",
    "path": "papers/23/06/2306.17107.json",
    "total_tokens": 899,
    "translated_title": "LLaVAR:增强的视觉指令调整用于文本丰富的图像理解",
    "translated_abstract": "指令调整可以发挥大型语言模型（LLM）与人类互动的出色能力。此外，最近的指令遵循数据集包括图像作为视觉输入，收集图像指令的响应。然而，视觉指令调整的模型不能很好地理解图像中的文本细节。本研究增强了当前的视觉指令调整流程，使用文本丰富的图像（如电影海报、图书封面等）。具体地，我们首先使用公开可用的OCR工具从LAION数据集的422K个文本丰富的图像上提取结果。此外，我们使用识别到的文本和图像标题来启动仅文本的GPT-4生成16K个对话，每个对话包含文本丰富的图像的问答对。通过将我们收集的数据与先前的多模态指令遵循数据组合，我们的模型LLaVAR在文本为基础的VQA数据集上显著提高了LLaVA模型的能力（准确率提高了20%）同时 achieving an accur",
    "tldr": "LLaVAR是一个增强的视觉指令调整模型，通过使用文本丰富的图像数据，它能够显著提升在文本为基础的视觉问答数据集上的准确率。",
    "en_tdlr": "LLaVAR is an enhanced visual instruction tuning model that significantly improves the accuracy on text-based visual question answering datasets by utilizing text-rich image data."
}