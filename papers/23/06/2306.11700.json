{
    "title": "Last-Iterate Convergent Policy Gradient Primal-Dual Methods for Constrained MDPs. (arXiv:2306.11700v2 [math.OC] UPDATED)",
    "abstract": "We study the problem of computing an optimal policy of an infinite-horizon discounted constrained Markov decision process (constrained MDP). Despite the popularity of Lagrangian-based policy search methods used in practice, the oscillation of policy iterates in these methods has not been fully understood, bringing out issues such as violation of constraints and sensitivity to hyper-parameters. To fill this gap, we employ the Lagrangian method to cast a constrained MDP into a constrained saddle-point problem in which max/min players correspond to primal/dual variables, respectively, and develop two single-time-scale policy-based primal-dual algorithms with non-asymptotic convergence of their policy iterates to an optimal constrained policy. Specifically, we first propose a regularized policy gradient primal-dual (RPG-PD) method that updates the policy using an entropy-regularized policy gradient, and the dual variable via a quadratic-regularized gradient ascent, simultaneously. We prove",
    "link": "http://arxiv.org/abs/2306.11700",
    "context": "Title: Last-Iterate Convergent Policy Gradient Primal-Dual Methods for Constrained MDPs. (arXiv:2306.11700v2 [math.OC] UPDATED)\nAbstract: We study the problem of computing an optimal policy of an infinite-horizon discounted constrained Markov decision process (constrained MDP). Despite the popularity of Lagrangian-based policy search methods used in practice, the oscillation of policy iterates in these methods has not been fully understood, bringing out issues such as violation of constraints and sensitivity to hyper-parameters. To fill this gap, we employ the Lagrangian method to cast a constrained MDP into a constrained saddle-point problem in which max/min players correspond to primal/dual variables, respectively, and develop two single-time-scale policy-based primal-dual algorithms with non-asymptotic convergence of their policy iterates to an optimal constrained policy. Specifically, we first propose a regularized policy gradient primal-dual (RPG-PD) method that updates the policy using an entropy-regularized policy gradient, and the dual variable via a quadratic-regularized gradient ascent, simultaneously. We prove",
    "path": "papers/23/06/2306.11700.json",
    "total_tokens": 928,
    "translated_title": "为约束MDP问题设计的最后迭代收敛的策略梯度原始-对偶方法",
    "translated_abstract": "本文研究无限时间折扣约束马尔可夫决策过程（约束MDP）的最优策略计算问题。尽管在实践中使用基于Lagrangian的政策搜索方法很受欢迎，但这些方法中策略迭代的振荡尚未完全解释清楚，从而引出了诸如约束违规和对超参数的敏感性等问题。为了填补这一空白，我们采用Lagrangian方法将约束MDP转化为一个约束鞍点问题，其中最大/最小玩家分别对应原始/对偶变量，并开发了两种单时间尺度的基于策略的原始-对偶算法，其策略迭代非渐进收敛到最优约束策略。具体而言，我们首先提出了一种正则化策略梯度原始-对偶（RPG-PD）方法，该方法使用熵正则化的策略梯度更新策略，同时使用二次正则化的梯度上升更新对偶变量。我们证明了 ...",
    "tldr": "本文研究了约束MDP问题，提出了两种基于策略的原始-对偶算法，实现了策略迭代的非渐进收敛到最优约束策略。",
    "en_tdlr": "This paper investigates constrained Markov decision process (MDP) problems and proposes two policy-based primal-dual algorithms that achieve non-asymptotic convergence of policy iterates to an optimal constrained policy."
}