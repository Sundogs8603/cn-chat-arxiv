{
    "title": "Continual Learning with Pretrained Backbones by Tuning in the Input Space. (arXiv:2306.02947v2 [cs.LG] UPDATED)",
    "abstract": "The intrinsic difficulty in adapting deep learning models to non-stationary environments limits the applicability of neural networks to real-world tasks. This issue is critical in practical supervised learning settings, such as the ones in which a pre-trained model computes projections toward a latent space where different task predictors are sequentially learned over time. As a matter of fact, incrementally fine-tuning the whole model to better adapt to new tasks usually results in catastrophic forgetting, with decreasing performance over the past experiences and losing valuable knowledge from the pre-training stage. In this paper, we propose a novel strategy to make the fine-tuning procedure more effective, by avoiding to update the pre-trained part of the network and learning not only the usual classification head, but also a set of newly-introduced learnable parameters that are responsible for transforming the input data. This process allows the network to effectively leverage the ",
    "link": "http://arxiv.org/abs/2306.02947",
    "context": "Title: Continual Learning with Pretrained Backbones by Tuning in the Input Space. (arXiv:2306.02947v2 [cs.LG] UPDATED)\nAbstract: The intrinsic difficulty in adapting deep learning models to non-stationary environments limits the applicability of neural networks to real-world tasks. This issue is critical in practical supervised learning settings, such as the ones in which a pre-trained model computes projections toward a latent space where different task predictors are sequentially learned over time. As a matter of fact, incrementally fine-tuning the whole model to better adapt to new tasks usually results in catastrophic forgetting, with decreasing performance over the past experiences and losing valuable knowledge from the pre-training stage. In this paper, we propose a novel strategy to make the fine-tuning procedure more effective, by avoiding to update the pre-trained part of the network and learning not only the usual classification head, but also a set of newly-introduced learnable parameters that are responsible for transforming the input data. This process allows the network to effectively leverage the ",
    "path": "papers/23/06/2306.02947.json",
    "total_tokens": 741,
    "translated_title": "通过调整输入空间进行预训练骨干网络的持续学习",
    "translated_abstract": "将深度学习模型适应非稳态环境的内在困难限制了神经网络在实际任务中的应用。本文提出了一种新的策略，通过避免更新网络的预训练部分并学习不仅是通常的分类头，而且还有一组新引入的可以学习参数，这些参数负责转换输入数据。这个过程允许网络有效地利用已学习的知识，并使微调过程更为有效。",
    "tldr": "本论文提出了一种新颖的策略，使得通过微调不仅可以避免更新预训练部分从而损失宝贵的知识，并且还可以学习一组新的可学习参数将输入数据转换为有效信息。"
}