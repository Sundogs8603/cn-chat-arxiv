{
    "title": "Better Context Makes Better Code Language Models: A Case Study on Function Call Argument Completion. (arXiv:2306.00381v1 [cs.SE])",
    "abstract": "Pretrained code language models have enabled great progress towards program synthesis. However, common approaches only consider in-file local context and thus miss information and constraints imposed by other parts of the codebase and its external dependencies. Existing code completion benchmarks also lack such context. To resolve these restrictions we curate a new dataset of permissively licensed Python packages that includes full projects and their dependencies and provide tools to extract non-local information with the help of program analyzers. We then focus on the task of function call argument completion which requires predicting the arguments to function calls. We show that existing code completion models do not yield good results on our completion task. To better solve this task, we query a program analyzer for information relevant to a given function call, and consider ways to provide the analyzer results to different code completion models during inference and training. Our e",
    "link": "http://arxiv.org/abs/2306.00381",
    "context": "Title: Better Context Makes Better Code Language Models: A Case Study on Function Call Argument Completion. (arXiv:2306.00381v1 [cs.SE])\nAbstract: Pretrained code language models have enabled great progress towards program synthesis. However, common approaches only consider in-file local context and thus miss information and constraints imposed by other parts of the codebase and its external dependencies. Existing code completion benchmarks also lack such context. To resolve these restrictions we curate a new dataset of permissively licensed Python packages that includes full projects and their dependencies and provide tools to extract non-local information with the help of program analyzers. We then focus on the task of function call argument completion which requires predicting the arguments to function calls. We show that existing code completion models do not yield good results on our completion task. To better solve this task, we query a program analyzer for information relevant to a given function call, and consider ways to provide the analyzer results to different code completion models during inference and training. Our e",
    "path": "papers/23/06/2306.00381.json",
    "total_tokens": 933,
    "translated_title": "更好的上下文使得代码语言模型更出色：以函数调用实参自动补全为例的案例研究",
    "translated_abstract": "预训练的代码语言模型在程序合成方面取得了很大进展。然而，常见方法只考虑文件内的局部上下文，因此忽视了代码库其他部分及其外部依赖所施加的信息和限制。现有的代码补全基准测试也缺乏这种上下文。为解决这些限制，我们策划了一个包含完整项目及其依赖的、许可宽松的Python包的新数据集，并提供工具来辅助程序分析器提取非本地信息。我们接着关注函数调用实参自动补全任务，该任务需要预测函数调用的实参。我们展示了现有的代码补全模型不能很好地解决我们的自动补全任务。为更好地解决此任务，我们向程序分析器查询与给定函数调用相关的信息，并考虑在推理和训练期间将分析器结果提供给不同的代码补全模型的方法。我们的研究表明非局部上下文可以帮助提高代码补全模型的性能。",
    "tldr": "本文针对代码补全任务展开研究，提出了考虑局部和非局部上下文信息的方法，设计了一个新的Python包数据集，通过程序分析器提取了非局部信息，并证明了这种方法可以提高代码补全模型的性能。",
    "en_tdlr": "This paper studies the task of code completion and proposes a method that takes into account both local and non-local context information. A new dataset of permissively licensed Python packages is designed and non-local information is extracted with the help of program analyzers. The study shows that considering non-local context can improve code completion model performance."
}