{
    "title": "Near Optimal Heteroscedastic Regression with Symbiotic Learning. (arXiv:2306.14288v1 [stat.ML])",
    "abstract": "We consider the classical problem of heteroscedastic linear regression, where we are given $n$ samples $(\\mathbf{x}_i, y_i) \\in \\mathbb{R}^d \\times \\mathbb{R}$ obtained from $y_i = \\langle \\mathbf{w}^{*}, \\mathbf{x}_i \\rangle + \\epsilon_i \\cdot \\langle \\mathbf{f}^{*}, \\mathbf{x}_i \\rangle$, where $\\mathbf{x}_i \\sim N(0,\\mathbf{I})$, $\\epsilon_i \\sim N(0,1)$, and our task is to estimate $\\mathbf{w}^{*}$. In addition to the classical applications of heteroscedastic models in fields such as statistics, econometrics, time series analysis etc., it is also particularly relevant in machine learning when data is collected from multiple sources of varying but apriori unknown quality, e.g., large model training. Our work shows that we can estimate $\\mathbf{w}^{*}$ in squared norm up to an error of $\\tilde{O}\\left(\\|\\mathbf{f}^{*}\\|^2 \\cdot \\left(\\frac{1}{n} + \\left(\\frac{d}{n}\\right)^2\\right)\\right)$ and prove a matching lower bound (up to logarithmic factors). Our result substantially improves ",
    "link": "http://arxiv.org/abs/2306.14288",
    "context": "Title: Near Optimal Heteroscedastic Regression with Symbiotic Learning. (arXiv:2306.14288v1 [stat.ML])\nAbstract: We consider the classical problem of heteroscedastic linear regression, where we are given $n$ samples $(\\mathbf{x}_i, y_i) \\in \\mathbb{R}^d \\times \\mathbb{R}$ obtained from $y_i = \\langle \\mathbf{w}^{*}, \\mathbf{x}_i \\rangle + \\epsilon_i \\cdot \\langle \\mathbf{f}^{*}, \\mathbf{x}_i \\rangle$, where $\\mathbf{x}_i \\sim N(0,\\mathbf{I})$, $\\epsilon_i \\sim N(0,1)$, and our task is to estimate $\\mathbf{w}^{*}$. In addition to the classical applications of heteroscedastic models in fields such as statistics, econometrics, time series analysis etc., it is also particularly relevant in machine learning when data is collected from multiple sources of varying but apriori unknown quality, e.g., large model training. Our work shows that we can estimate $\\mathbf{w}^{*}$ in squared norm up to an error of $\\tilde{O}\\left(\\|\\mathbf{f}^{*}\\|^2 \\cdot \\left(\\frac{1}{n} + \\left(\\frac{d}{n}\\right)^2\\right)\\right)$ and prove a matching lower bound (up to logarithmic factors). Our result substantially improves ",
    "path": "papers/23/06/2306.14288.json",
    "total_tokens": 1108,
    "translated_title": "基于共生学习的异方差回归的近似最优算法研究",
    "translated_abstract": "本研究针对经典的异方差线性回归问题展开讨论。假设我们有n个样本 $(\\mathbf{x}_i, y_i) \\in \\mathbb{R}^d \\times \\mathbb{R}$，其中 $y_i = \\langle \\mathbf{w}^{*}, \\mathbf{x}_i \\rangle + \\epsilon_i \\cdot \\langle \\mathbf{f}^{*}, \\mathbf{x}_i \\rangle$， $\\mathbf{x}_i \\sim N(0,\\mathbf{I})$，$\\epsilon_i \\sim N(0,1)$，我们的目标是估计 $\\mathbf{w}^{*}$。在统计学、计量经济学、时间序列分析等领域，异方差模型具有广泛的应用，同时，在机器学习中如果数据来源不同，而不同来源的数据质量也不一，则异方差模型也显得特别相关。本研究表明，我们可以估计出$\\mathbf{w}^{*}$的平方范数，误差为$\\tilde{O}\\left(\\|\\mathbf{f}^{*}\\|^2 \\cdot \\left(\\frac{1}{n} + \\left(\\frac{d}{n}\\right)^2\\right)\\right)$，并证明了一个匹配的下限（上界存在对数因子）。本研究的结果显著改进了异方差回归问题的近似最优算法。",
    "tldr": "本研究提出了一种基于共生学习的异方差回归的近似最优算法，可以在统计学、计量经济学、时间序列分析等领域，以及在不同来源数据质量不一的机器学习中应用。",
    "en_tdlr": "This study proposes a near optimal algorithm of heteroscedastic linear regression based on symbiotic learning, which can be applied in fields such as statistics, econometrics, time series analysis, and machine learning where data comes from multiple sources of varying quality."
}