{
    "title": "Near-optimal Conservative Exploration in Reinforcement Learning under Episode-wise Constraints. (arXiv:2306.06265v1 [cs.LG])",
    "abstract": "This paper investigates conservative exploration in reinforcement learning where the performance of the learning agent is guaranteed to be above a certain threshold throughout the learning process. It focuses on the tabular episodic Markov Decision Process (MDP) setting that has finite states and actions. With the knowledge of an existing safe baseline policy, an algorithm termed as StepMix is proposed to balance the exploitation and exploration while ensuring that the conservative constraint is never violated in each episode with high probability. StepMix features a unique design of a mixture policy that adaptively and smoothly interpolates between the baseline policy and the optimistic policy. Theoretical analysis shows that StepMix achieves near-optimal regret order as in the constraint-free setting, indicating that obeying the stringent episode-wise conservative constraint does not compromise the learning performance. Besides, a randomization-based EpsMix algorithm is also proposed",
    "link": "http://arxiv.org/abs/2306.06265",
    "context": "Title: Near-optimal Conservative Exploration in Reinforcement Learning under Episode-wise Constraints. (arXiv:2306.06265v1 [cs.LG])\nAbstract: This paper investigates conservative exploration in reinforcement learning where the performance of the learning agent is guaranteed to be above a certain threshold throughout the learning process. It focuses on the tabular episodic Markov Decision Process (MDP) setting that has finite states and actions. With the knowledge of an existing safe baseline policy, an algorithm termed as StepMix is proposed to balance the exploitation and exploration while ensuring that the conservative constraint is never violated in each episode with high probability. StepMix features a unique design of a mixture policy that adaptively and smoothly interpolates between the baseline policy and the optimistic policy. Theoretical analysis shows that StepMix achieves near-optimal regret order as in the constraint-free setting, indicating that obeying the stringent episode-wise conservative constraint does not compromise the learning performance. Besides, a randomization-based EpsMix algorithm is also proposed",
    "path": "papers/23/06/2306.06265.json",
    "total_tokens": 915,
    "translated_title": "强化学习中基于回合限制的近优保守探索",
    "translated_abstract": "本文研究了在强化学习中实现保守探索的问题，其中学习代理的性能在整个学习过程中保证高于某个特定阈值。研究针对有限状态和动作的标签式回合式马尔可夫决策过程（MDP）环境。本文提出了一种名为StepMix的算法，利用现有的安全基线策略在保证每个回合不违反保守限制的前提下，平衡开发和探索。StepMix具有独特的混合策略设计，自适应地、平滑地插值基线策略和乐观策略之间。理论分析表明，StepMix在不受限制的情况下具有接近最优的后悔量级，说明遵守严格的回合限制不会损害学习性能。此外，还提出了基于随机化的EpsMix算法。",
    "tldr": "本文研究了强化学习中实现保守探索的问题，提出了名为StepMix的算法，利用现有的安全基线策略平衡开发和探索，同时保证每个回合不违反保守限制，并且能够在不受限制的情况下达到接近最优的后悔量级。",
    "en_tdlr": "This paper proposes an algorithm named StepMix for conservative exploration in reinforcement learning, aiming to balance exploitation and exploration while ensuring that the conservative constraint is never violated in each episode. It achieves near-optimal regret order as in the constraint-free setting, indicating that obeying the stringent episode-wise conservative constraint does not compromise the learning performance."
}