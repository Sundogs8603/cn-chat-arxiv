{
    "title": "MetaVL: Transferring In-Context Learning Ability From Language Models to Vision-Language Models. (arXiv:2306.01311v1 [cs.CL])",
    "abstract": "Large-scale language models have shown the ability to adapt to a new task via conditioning on a few demonstrations (i.e., in-context learning). However, in the vision-language domain, most large-scale pre-trained vision-language (VL) models do not possess the ability to conduct in-context learning. How can we enable in-context learning for VL models? In this paper, we study an interesting hypothesis: can we transfer the in-context learning ability from the language domain to VL domain? Specifically, we first meta-trains a language model to perform in-context learning on NLP tasks (as in MetaICL); then we transfer this model to perform VL tasks by attaching a visual encoder. Our experiments suggest that indeed in-context learning ability can be transferred cross modalities: our model considerably improves the in-context learning capability on VL tasks and can even compensate for the size of the model significantly. On VQA, OK-VQA, and GQA, our method could outperform the baseline model ",
    "link": "http://arxiv.org/abs/2306.01311",
    "context": "Title: MetaVL: Transferring In-Context Learning Ability From Language Models to Vision-Language Models. (arXiv:2306.01311v1 [cs.CL])\nAbstract: Large-scale language models have shown the ability to adapt to a new task via conditioning on a few demonstrations (i.e., in-context learning). However, in the vision-language domain, most large-scale pre-trained vision-language (VL) models do not possess the ability to conduct in-context learning. How can we enable in-context learning for VL models? In this paper, we study an interesting hypothesis: can we transfer the in-context learning ability from the language domain to VL domain? Specifically, we first meta-trains a language model to perform in-context learning on NLP tasks (as in MetaICL); then we transfer this model to perform VL tasks by attaching a visual encoder. Our experiments suggest that indeed in-context learning ability can be transferred cross modalities: our model considerably improves the in-context learning capability on VL tasks and can even compensate for the size of the model significantly. On VQA, OK-VQA, and GQA, our method could outperform the baseline model ",
    "path": "papers/23/06/2306.01311.json",
    "total_tokens": 929,
    "translated_title": "MetaVL：从语言模型向视觉-语言模型中迁移上下文学习能力",
    "translated_abstract": "大规模语言模型表现出通过在少量示例上进行条件训练（即上下文学习）来适应新任务的能力。然而，在视觉-语言领域中，大多数大规模预先训练的视觉-语言（VL）模型缺乏进行上下文学习的能力。本文研究了一种有趣的假设：我们能否将语言领域的上下文学习能力转移到VL领域？具体来说，我们首先在NLP任务上元训练语言模型执行上下文学习（如MetaICL）；然后通过附加视觉编码器将此模型转移到VL任务上。我们的实验表明，的确可以跨模态地转移上下文学习能力：我们的模型显着提高了VL任务的上下文学习能力，甚至可以显著弥补模型大小的不足。在VQA、OK-VQA和GQA上，我们的方法可以超越基线模型。",
    "tldr": "本文研究了如何在视觉-语言领域实现上下文学习能力，并通过在NLP任务上元训练语言模型，成功将上下文学习能力转移到VL任务上，实验结果表明，该方法具有显著优势。",
    "en_tdlr": "This paper investigates how to enable in-context learning ability in vision-language models by transferring the ability from language models through meta-training. The experiments show that the method significantly improves the performance on VL tasks and outperforms the baseline models on VQA, OK-VQA and GQA."
}