{
    "title": "A Primal-Dual-Critic Algorithm for Offline Constrained Reinforcement Learning. (arXiv:2306.07818v1 [cs.LG])",
    "abstract": "Offline constrained reinforcement learning (RL) aims to learn a policy that maximizes the expected cumulative reward subject to constraints on expected value of cost functions using an existing dataset. In this paper, we propose Primal-Dual-Critic Algorithm (PDCA), a novel algorithm for offline constrained RL with general function approximation. PDCA runs a primal-dual algorithm on the Lagrangian function estimated by critics. The primal player employs a no-regret policy optimization oracle to maximize the Lagrangian estimate given any choices of the critics and the dual player. The dual player employs a no-regret online linear optimization oracle to minimize the Lagrangian estimate given any choices of the critics and the primal player. We show that PDCA can successfully find a near saddle point of the Lagrangian, which is nearly optimal for the constrained RL problem. Unlike previous work that requires concentrability and strong Bellman completeness assumptions, PDCA only requires co",
    "link": "http://arxiv.org/abs/2306.07818",
    "context": "Title: A Primal-Dual-Critic Algorithm for Offline Constrained Reinforcement Learning. (arXiv:2306.07818v1 [cs.LG])\nAbstract: Offline constrained reinforcement learning (RL) aims to learn a policy that maximizes the expected cumulative reward subject to constraints on expected value of cost functions using an existing dataset. In this paper, we propose Primal-Dual-Critic Algorithm (PDCA), a novel algorithm for offline constrained RL with general function approximation. PDCA runs a primal-dual algorithm on the Lagrangian function estimated by critics. The primal player employs a no-regret policy optimization oracle to maximize the Lagrangian estimate given any choices of the critics and the dual player. The dual player employs a no-regret online linear optimization oracle to minimize the Lagrangian estimate given any choices of the critics and the primal player. We show that PDCA can successfully find a near saddle point of the Lagrangian, which is nearly optimal for the constrained RL problem. Unlike previous work that requires concentrability and strong Bellman completeness assumptions, PDCA only requires co",
    "path": "papers/23/06/2306.07818.json",
    "total_tokens": 949,
    "translated_title": "一种基于原始-对偶-评论家算法的离线约束强化学习",
    "translated_abstract": "离线约束强化学习旨在学习一种策略，以在现有数据集上满足对成本函数期望值的限制条件下最大化预期的累积奖励。本文提出了一种称为原始-对偶-评论家算法（PDCA）的新算法，用于具有一般函数逼近的离线约束强化学习。PDCA在评论家估计的Lagrangian函数上运行原始-对偶算法。原始玩家采用无悔策略优化神谕，在给定任何评论家和对偶玩家的选择的情况下最大化拉格朗日函数的估计。对偶玩家通过采用无悔在线线性优化神谕，在给定评论家和原始玩家的任何选择的情况下最小化拉格朗日函数的估计。我们展示了PDCA可以成功地找到拉格朗日函数的近似鞍点，这对于约束强化学习问题几乎是最优的。与以前需要集中性和强Bellman完备性假设的作品不同，PDCA只需要一致性和自闭性这两个假设。",
    "tldr": "PDCA是一种用于离线约束强化学习的算法，它可以通过在Lagrangian函数上运行原始-对偶算法来找到近似鞍点，而无需集中性和强Bellman完备性假设。",
    "en_tdlr": "PDCA is an algorithm for offline constrained reinforcement learning, which can find an approximate saddle point by running primal-dual algorithm on the Lagrangian function, without requiring concentrability and strong Bellman completeness assumptions."
}