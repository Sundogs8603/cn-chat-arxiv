{
    "title": "AQuA: A Benchmarking Tool for Label Quality Assessment. (arXiv:2306.09467v1 [cs.LG])",
    "abstract": "Machine learning (ML) models are only as good as the data they are trained on. But recent studies have found datasets widely used to train and evaluate ML models, e.g. ImageNet, to have pervasive labeling errors. Erroneous labels on the train set hurt ML models' ability to generalize, and they impact evaluation and model selection using the test set. Consequently, learning in the presence of labeling errors is an active area of research, yet this field lacks a comprehensive benchmark to evaluate these methods. Most of these methods are evaluated on a few computer vision datasets with significant variance in the experimental protocols. With such a large pool of methods and inconsistent evaluation, it is also unclear how ML practitioners can choose the right models to assess label quality in their data. To this end, we propose a benchmarking environment AQuA to rigorously evaluate methods that enable machine learning in the presence of label noise. We also introduce a design space to del",
    "link": "http://arxiv.org/abs/2306.09467",
    "context": "Title: AQuA: A Benchmarking Tool for Label Quality Assessment. (arXiv:2306.09467v1 [cs.LG])\nAbstract: Machine learning (ML) models are only as good as the data they are trained on. But recent studies have found datasets widely used to train and evaluate ML models, e.g. ImageNet, to have pervasive labeling errors. Erroneous labels on the train set hurt ML models' ability to generalize, and they impact evaluation and model selection using the test set. Consequently, learning in the presence of labeling errors is an active area of research, yet this field lacks a comprehensive benchmark to evaluate these methods. Most of these methods are evaluated on a few computer vision datasets with significant variance in the experimental protocols. With such a large pool of methods and inconsistent evaluation, it is also unclear how ML practitioners can choose the right models to assess label quality in their data. To this end, we propose a benchmarking environment AQuA to rigorously evaluate methods that enable machine learning in the presence of label noise. We also introduce a design space to del",
    "path": "papers/23/06/2306.09467.json",
    "total_tokens": 907,
    "translated_title": "AQuA：一种用于标签质量评估的基准测试工具",
    "translated_abstract": "机器学习模型的好坏取决于用于训练它们的数据。然而，最近的研究发现，被广泛用于训练和评估机器学习模型的数据集，例如ImageNet，存在普遍的标注错误。训练集中的错误标签会削弱机器学习模型的泛化性能，并影响使用测试集进行模型选择和评估。因此，在存在标签误差的情况下学习是研究的一个活跃领域，但该领域缺乏一个全面的基准来评估这些方法。为此，我们提出了AQuA这个基准测试环境，以严格评估在标签噪声存在的情况下实现机器学习的方法。",
    "tldr": "AQuA是一款用于标签质量评估的基准测试工具，目的是评估标签噪声存在下的机器学习方法。该基准测试环境包括数据模拟、质量不同的真实数据集和几种最先进的标签噪声消除方法。",
    "en_tdlr": "AQuA is a benchmarking tool designed to evaluate machine learning methods in the presence of label noise. The benchmark includes data simulations, real-world datasets with varying levels of label quality, and several state-of-the-art label noise mitigation methods."
}