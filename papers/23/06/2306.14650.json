{
    "title": "PhD Thesis: Exploring the role of (self-)attention in cognitive and computer vision architecture. (arXiv:2306.14650v2 [cs.AI] UPDATED)",
    "abstract": "We investigate the role of attention and memory in complex reasoning tasks. We analyze Transformer-based self-attention as a model and extend it with memory. By studying a synthetic visual reasoning test, we refine the taxonomy of reasoning tasks. Incorporating self-attention with ResNet50, we enhance feature maps using feature-based and spatial attention, achieving efficient solving of challenging visual reasoning tasks. Our findings contribute to understanding the attentional needs of SVRT tasks. Additionally, we propose GAMR, a cognitive architecture combining attention and memory, inspired by active vision theory. GAMR outperforms other architectures in sample efficiency, robustness, and compositionality, and shows zero-shot generalization on new reasoning tasks.",
    "link": "http://arxiv.org/abs/2306.14650",
    "context": "Title: PhD Thesis: Exploring the role of (self-)attention in cognitive and computer vision architecture. (arXiv:2306.14650v2 [cs.AI] UPDATED)\nAbstract: We investigate the role of attention and memory in complex reasoning tasks. We analyze Transformer-based self-attention as a model and extend it with memory. By studying a synthetic visual reasoning test, we refine the taxonomy of reasoning tasks. Incorporating self-attention with ResNet50, we enhance feature maps using feature-based and spatial attention, achieving efficient solving of challenging visual reasoning tasks. Our findings contribute to understanding the attentional needs of SVRT tasks. Additionally, we propose GAMR, a cognitive architecture combining attention and memory, inspired by active vision theory. GAMR outperforms other architectures in sample efficiency, robustness, and compositionality, and shows zero-shot generalization on new reasoning tasks.",
    "path": "papers/23/06/2306.14650.json",
    "total_tokens": 972,
    "translated_title": "博士论文：探索认知和计算机视觉架构中的(自我)注意力的作用",
    "translated_abstract": "我们研究了注意力和记忆在复杂推理任务中的作用。我们通过分析基于Transformer的自我注意力模型并将其与记忆相结合来扩展它。通过研究合成视觉推理测试，我们完善了推理任务的分类法。通过将自我注意力与ResNet50结合，我们使用基于特征和空间注意力增强特征图，从而实现了对具有挑战性的视觉推理任务的高效解决。我们的研究结果有助于理解SVRT任务对注意力的需求。此外，我们提出了GAMR，一种结合了注意力和记忆的认知架构，灵感来自主动视觉理论。GAMR在样本效率、鲁棒性和组合性方面优于其他架构，并在新的推理任务上表现出零样本泛化能力。",
    "tldr": "该论文研究了注意力和记忆在复杂推理任务中的作用，通过以Transformer为基础模型并结合记忆，扩展了自我注意力模型。研究结果表明，在视觉推理任务中，使用基于特征和空间注意力的自我注意力与ResNet50相结合可以高效解决具有挑战性的任务。此外，该论文提出了基于注意力和记忆的认知架构GAMR，它在样本效率、鲁棒性和组合性方面优于其他架构，并具有对新的推理任务的零样本泛化能力。"
}