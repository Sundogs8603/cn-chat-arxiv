{
    "title": "Masking Augmentation for Supervised Learning",
    "abstract": "arXiv:2306.11339v2 Announce Type: replace-cross  Abstract: Pre-training using random masking has emerged as a novel trend in training techniques. However, supervised learning faces a challenge in adopting masking augmentations, primarily due to unstable training. In this paper, we propose a novel way to involve masking augmentations dubbed Masked Sub-model (MaskSub). MaskSub consists of the main-model and sub-model; while the former enjoys conventional training recipes, the latter leverages the benefit of strong masking augmentations in training. MaskSub addresses the challenge by mitigating adverse effects through a relaxed loss function similar to a self-distillation loss. Our analysis shows that MaskSub improves performance, with the training loss converging even faster than regular training, which suggests our method facilitates training. We further validate MaskSub across diverse training recipes and models, including DeiT-III, MAE fine-tuning, CLIP fine-tuning, ResNet, and Swin T",
    "link": "https://arxiv.org/abs/2306.11339",
    "context": "Title: Masking Augmentation for Supervised Learning\nAbstract: arXiv:2306.11339v2 Announce Type: replace-cross  Abstract: Pre-training using random masking has emerged as a novel trend in training techniques. However, supervised learning faces a challenge in adopting masking augmentations, primarily due to unstable training. In this paper, we propose a novel way to involve masking augmentations dubbed Masked Sub-model (MaskSub). MaskSub consists of the main-model and sub-model; while the former enjoys conventional training recipes, the latter leverages the benefit of strong masking augmentations in training. MaskSub addresses the challenge by mitigating adverse effects through a relaxed loss function similar to a self-distillation loss. Our analysis shows that MaskSub improves performance, with the training loss converging even faster than regular training, which suggests our method facilitates training. We further validate MaskSub across diverse training recipes and models, including DeiT-III, MAE fine-tuning, CLIP fine-tuning, ResNet, and Swin T",
    "path": "papers/23/06/2306.11339.json",
    "total_tokens": 857,
    "translated_title": "遮罩数据增强用于监督学习",
    "translated_abstract": "使用随机遮罩进行预训练已经成为训练技术中的新趋势。然而，监督学习在采用遮罩增强方面面临挑战，主要是由于不稳定的训练。本文提出了一种涉及遮罩增强的新方法，称为Masked Sub-model (MaskSub)。MaskSub由主模型和子模型组成；前者享受传统训练方法，而后者利用强大的遮罩增强来训练。MaskSub通过缓解类似于自蒸馏损失的放松损失函数来解决挑战。我们的分析表明，MaskSub提高了性能，训练损失的收敛速度甚至比常规训练更快，这表明我们的方法有助于训练。我们进一步验证了MaskSub在各种训练方法和模型上的有效性，包括DeiT-III，MAE微调，CLIP微调，ResNet和Swin T。",
    "tldr": "提出了一种名为MaskSub的新方法，通过使用遮罩子模型和放松的损失函数来强化监督学习中的遮罩增强，提高了性能并加速训练过程。",
    "en_tdlr": "A novel method named MaskSub is proposed to enhance masking augmentations in supervised learning by using a masked sub-model and a relaxed loss function, improving performance and accelerating training."
}