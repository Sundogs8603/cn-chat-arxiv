{
    "title": "Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with Architecture-Routed Mixture-of-Experts. (arXiv:2306.04845v1 [cs.CL])",
    "abstract": "Weight-sharing supernet has become a vital component for performance estimation in the state-of-the-art (SOTA) neural architecture search (NAS) frameworks. Although supernet can directly generate different subnetworks without retraining, there is no guarantee for the quality of these subnetworks because of weight sharing. In NLP tasks such as machine translation and pre-trained language modeling, we observe that given the same model architecture, there is a large performance gap between supernet and training from scratch. Hence, supernet cannot be directly used and retraining is necessary after finding the optimal architectures.  In this work, we propose mixture-of-supernets, a generalized supernet formulation where mixture-of-experts (MoE) is adopted to enhance the expressive power of the supernet model, with negligible training overhead. In this way, different subnetworks do not share the model weights directly, but through an architecture-based routing mechanism. As a result, model ",
    "link": "http://arxiv.org/abs/2306.04845",
    "context": "Title: Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with Architecture-Routed Mixture-of-Experts. (arXiv:2306.04845v1 [cs.CL])\nAbstract: Weight-sharing supernet has become a vital component for performance estimation in the state-of-the-art (SOTA) neural architecture search (NAS) frameworks. Although supernet can directly generate different subnetworks without retraining, there is no guarantee for the quality of these subnetworks because of weight sharing. In NLP tasks such as machine translation and pre-trained language modeling, we observe that given the same model architecture, there is a large performance gap between supernet and training from scratch. Hence, supernet cannot be directly used and retraining is necessary after finding the optimal architectures.  In this work, we propose mixture-of-supernets, a generalized supernet formulation where mixture-of-experts (MoE) is adopted to enhance the expressive power of the supernet model, with negligible training overhead. In this way, different subnetworks do not share the model weights directly, but through an architecture-based routing mechanism. As a result, model ",
    "path": "papers/23/06/2306.04845.json",
    "total_tokens": 914,
    "translated_title": "混合超网络：通过基于结构路由的专家混合改进共享权重超网络训练",
    "translated_abstract": "共享权重的超级网络已经成为当前最先进的神经体系结构搜索（NAS）框架中性能评估的关键组成部分。然而，由于权重共享，超级网络直接生成的不同子网络的质量无法保证。在机器翻译和预训练语言建模等NLP任务中，我们观察到，在相同的模型架构下，超级网络与从头开始训练之间存在较大的性能差距。因此，在找到最佳架构后，不能直接使用超级网络，必须进行重新训练。我们提出了混合超网络，这是一种广义的超级网络公式，其中采用了专家混合（MoE）来增强超级网络模型的表达能力，训练开销可以忽略不计。通过这种方式，不同的子网络不是直接共享模型权重，而是通过基于结构的路由机制共享。因此，模型性能得到了改善。",
    "tldr": "该论文提出了一种混合超网络方法，通过基于结构路由的专家混合来增强超级网络模型的表达能力，改善了子网络的质量问题和性能差异。"
}