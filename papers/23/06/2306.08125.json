{
    "title": "Implicit Compressibility of Overparametrized Neural Networks Trained with Heavy-Tailed SGD. (arXiv:2306.08125v1 [stat.ML])",
    "abstract": "Neural network compression has been an increasingly important subject, due to its practical implications in terms of reducing the computational requirements and its theoretical implications, as there is an explicit connection between compressibility and the generalization error. Recent studies have shown that the choice of the hyperparameters of stochastic gradient descent (SGD) can have an effect on the compressibility of the learned parameter vector. Even though these results have shed some light on the role of the training dynamics over compressibility, they relied on unverifiable assumptions and the resulting theory does not provide a practical guideline due to its implicitness. In this study, we propose a simple modification for SGD, such that the outputs of the algorithm will be provably compressible without making any nontrivial assumptions. We consider a one-hidden-layer neural network trained with SGD and we inject additive heavy-tailed noise to the iterates at each iteration.",
    "link": "http://arxiv.org/abs/2306.08125",
    "context": "Title: Implicit Compressibility of Overparametrized Neural Networks Trained with Heavy-Tailed SGD. (arXiv:2306.08125v1 [stat.ML])\nAbstract: Neural network compression has been an increasingly important subject, due to its practical implications in terms of reducing the computational requirements and its theoretical implications, as there is an explicit connection between compressibility and the generalization error. Recent studies have shown that the choice of the hyperparameters of stochastic gradient descent (SGD) can have an effect on the compressibility of the learned parameter vector. Even though these results have shed some light on the role of the training dynamics over compressibility, they relied on unverifiable assumptions and the resulting theory does not provide a practical guideline due to its implicitness. In this study, we propose a simple modification for SGD, such that the outputs of the algorithm will be provably compressible without making any nontrivial assumptions. We consider a one-hidden-layer neural network trained with SGD and we inject additive heavy-tailed noise to the iterates at each iteration.",
    "path": "papers/23/06/2306.08125.json",
    "total_tokens": 849,
    "translated_title": "带有沉重尾部SGD训练的过参数化神经网络的隐式可压缩性",
    "translated_abstract": "由于减少计算需求和压缩与泛化误差之间的显式关系，神经网络压缩成为越来越重要的研究对象。最近的研究表明，随机梯度下降(SGD)的超参数选择可以影响学习参数向量的压缩性。虽然这些结果揭示了训练动态对压缩性的影响，但是它们依赖于不可验证的假设，由于隐含性质，得出的理论并没有提供实用的指导方针。在本研究中，我们提出了一种简单的SGD修改方法，使得算法的输出能够被证明是可压缩的，而不需要任何非平凡假设。我们考虑了一个使用SGD训练的单隐藏层神经网络，并在每次迭代中注入附加的沉重尾部噪声。",
    "tldr": "本研究提出了一种简单的SGD修改方法，使训练出的神经网络输出可被证明为可压缩，而不需要任何非平凡假设。",
    "en_tdlr": "This study proposes a simple modification for SGD to ensure provable compressibility of the learned parameter vector in overparametrized neural networks, without any nontrivial assumptions."
}