{
    "title": "Understanding Masked Autoencoders via Hierarchical Latent Variable Models. (arXiv:2306.04898v1 [cs.LG])",
    "abstract": "Masked autoencoder (MAE), a simple and effective self-supervised learning framework based on the reconstruction of masked image regions, has recently achieved prominent success in a variety of vision tasks. Despite the emergence of intriguing empirical observations on MAE, a theoretically principled understanding is still lacking. In this work, we formally characterize and justify existing empirical insights and provide theoretical guarantees of MAE. We formulate the underlying data-generating process as a hierarchical latent variable model and show that under reasonable assumptions, MAE provably identifies a set of latent variables in the hierarchical model, explaining why MAE can extract high-level information from pixels. Further, we show how key hyperparameters in MAE (the masking ratio and the patch size) determine which true latent variables to be recovered, therefore influencing the level of semantic information in the representation. Specifically, extremely large or small maski",
    "link": "http://arxiv.org/abs/2306.04898",
    "context": "Title: Understanding Masked Autoencoders via Hierarchical Latent Variable Models. (arXiv:2306.04898v1 [cs.LG])\nAbstract: Masked autoencoder (MAE), a simple and effective self-supervised learning framework based on the reconstruction of masked image regions, has recently achieved prominent success in a variety of vision tasks. Despite the emergence of intriguing empirical observations on MAE, a theoretically principled understanding is still lacking. In this work, we formally characterize and justify existing empirical insights and provide theoretical guarantees of MAE. We formulate the underlying data-generating process as a hierarchical latent variable model and show that under reasonable assumptions, MAE provably identifies a set of latent variables in the hierarchical model, explaining why MAE can extract high-level information from pixels. Further, we show how key hyperparameters in MAE (the masking ratio and the patch size) determine which true latent variables to be recovered, therefore influencing the level of semantic information in the representation. Specifically, extremely large or small maski",
    "path": "papers/23/06/2306.04898.json",
    "total_tokens": 1023,
    "translated_title": "通过分层潜变量模型理解遮蔽自编码器",
    "translated_abstract": "遮蔽自编码器（MAE）是一种基于遮蔽图像区域重构的简单有效的自监督学习框架，近来在各种视觉任务中取得了显著的成功。尽管MAE存在有趣的经验性观察结果，但仍然缺乏理论上的原则性理解。在本文中，我们正式刻画和证明了现有的经验性见解，并对MAE提供了理论保证。我们将底层数据生成过程建模为一个分层潜变量模型，并表明在合理的假设下，MAE能够证明地识别出一组潜在变量，解释了MAE如何从像素中提取高层信息。此外，我们展示了MAE中的关键超参数（遮蔽比率和补丁大小）如何决定要恢复哪些真实潜变量，从而影响表示中的语义信息水平。具体而言，极大或极小的遮蔽比率会导致表示质量差，而不同的补丁大小可以捕获不同级别的对象信息。我们的理论结果为MAE的训练和超参数调整提供了指导，以实现更好的表示学习。",
    "tldr": "本文正式刻画和证明了现有的经验性见解，并为MAE的训练和超参数调整提供了指导，以实现更好的表示学习。"
}