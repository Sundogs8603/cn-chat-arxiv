{
    "title": "Unsupervised Learning of Style-Aware Facial Animation from Real Acting Performances. (arXiv:2306.10006v1 [cs.CV])",
    "abstract": "This paper presents a novel approach for text/speech-driven animation of a photo-realistic head model based on blend-shape geometry, dynamic textures, and neural rendering. Training a VAE for geometry and texture yields a parametric model for accurate capturing and realistic synthesis of facial expressions from a latent feature vector. Our animation method is based on a conditional CNN that transforms text or speech into a sequence of animation parameters. In contrast to previous approaches, our animation model learns disentangling/synthesizing different acting-styles in an unsupervised manner, requiring only phonetic labels that describe the content of training sequences. For realistic real-time rendering, we train a U-Net that refines rasterization-based renderings by computing improved pixel colors and a foreground matte. We compare our framework qualitatively/quantitatively against recent methods for head modeling as well as facial animation and evaluate the perceived rendering/ani",
    "link": "http://arxiv.org/abs/2306.10006",
    "context": "Title: Unsupervised Learning of Style-Aware Facial Animation from Real Acting Performances. (arXiv:2306.10006v1 [cs.CV])\nAbstract: This paper presents a novel approach for text/speech-driven animation of a photo-realistic head model based on blend-shape geometry, dynamic textures, and neural rendering. Training a VAE for geometry and texture yields a parametric model for accurate capturing and realistic synthesis of facial expressions from a latent feature vector. Our animation method is based on a conditional CNN that transforms text or speech into a sequence of animation parameters. In contrast to previous approaches, our animation model learns disentangling/synthesizing different acting-styles in an unsupervised manner, requiring only phonetic labels that describe the content of training sequences. For realistic real-time rendering, we train a U-Net that refines rasterization-based renderings by computing improved pixel colors and a foreground matte. We compare our framework qualitatively/quantitatively against recent methods for head modeling as well as facial animation and evaluate the perceived rendering/ani",
    "path": "papers/23/06/2306.10006.json",
    "total_tokens": 923,
    "translated_title": "基于现实表演的面部动画风格感知非监督学习",
    "translated_abstract": "本文提出了一种新的方法，基于混合形状几何、动态纹理和神经渲染，用于从真实动作表演中驱动面部模型的文本/语音动画。通过训练包括形状和纹理的VAE，我们得到了一个参数化模型，以精确捕捉和逼真合成潜在特征向量中的面部表情。我们的动画方法基于条件卷积神经网络，将文本或语音转换为一系列动画参数。与以往的方法不同，我们的动画模型以非监督的方式学习区分和合成不同的表演风格，只需要用于描述训练序列内容的语音标签。为了实现逼真的实时渲染，我们训练了一个U-Net，通过计算改进的像素颜色和前景遮罩来改善栅格化渲染。我们定性/定量地将我们的框架与最近的头部建模方法以及面部动画方法进行比较，并评估感知渲染/动画效果。",
    "tldr": "本文提出了一种非监督学习的动画方法，可以通过文本或语音输入，实现基于真实动作表演的面部动画，并且可以不同程度地学习并合成不同的表演风格。",
    "en_tdlr": "This paper proposes an unsupervised animation method that can generate facial animation based on real acting performances and different styles can be learned and synthesized to some extent. The method utilizes a conditional CNN to transform text or speech into animation parameters and trains a VAE for accurate capturing and realistic synthesis of facial expressions."
}