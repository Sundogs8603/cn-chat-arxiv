{
    "title": "Multi-Site Clinical Federated Learning using Recursive and Attentive Models and NVFlare. (arXiv:2306.16367v1 [cs.LG])",
    "abstract": "The prodigious growth of digital health data has precipitated a mounting interest in harnessing machine learning methodologies, such as natural language processing (NLP), to scrutinize medical records, clinical notes, and other text-based health information. Although NLP techniques have exhibited substantial potential in augmenting patient care and informing clinical decision-making, data privacy and adherence to regulations persist as critical concerns. Federated learning (FL) emerges as a viable solution, empowering multiple organizations to train machine learning models collaboratively without disseminating raw data. This paper proffers a pragmatic approach to medical NLP by amalgamating FL, NLP models, and the NVFlare framework, developed by NVIDIA. We introduce two exemplary NLP models, the Long-Short Term Memory (LSTM)-based model and Bidirectional Encoder Representations from Transformers (BERT), which have demonstrated exceptional performance in comprehending context and semant",
    "link": "http://arxiv.org/abs/2306.16367",
    "context": "Title: Multi-Site Clinical Federated Learning using Recursive and Attentive Models and NVFlare. (arXiv:2306.16367v1 [cs.LG])\nAbstract: The prodigious growth of digital health data has precipitated a mounting interest in harnessing machine learning methodologies, such as natural language processing (NLP), to scrutinize medical records, clinical notes, and other text-based health information. Although NLP techniques have exhibited substantial potential in augmenting patient care and informing clinical decision-making, data privacy and adherence to regulations persist as critical concerns. Federated learning (FL) emerges as a viable solution, empowering multiple organizations to train machine learning models collaboratively without disseminating raw data. This paper proffers a pragmatic approach to medical NLP by amalgamating FL, NLP models, and the NVFlare framework, developed by NVIDIA. We introduce two exemplary NLP models, the Long-Short Term Memory (LSTM)-based model and Bidirectional Encoder Representations from Transformers (BERT), which have demonstrated exceptional performance in comprehending context and semant",
    "path": "papers/23/06/2306.16367.json",
    "total_tokens": 945,
    "translated_title": "多站点临床联邦学习使用递归和注意力模型以及NVFlare",
    "translated_abstract": "数字健康数据的快速增长引发了对利用机器学习方法（如自然语言处理）来审查医疗记录、临床笔记和其他基于文本的健康信息的兴趣。虽然自然语言处理技术在增强患者护理和决策制定方面展示出了巨大的潜力，但数据隐私和遵守法规仍然是关键问题。联邦学习成为一种可行的解决方案，使多个组织能够在不传播原始数据的情况下共同训练机器学习模型。本文通过将联邦学习、自然语言处理模型和由NVIDIA开发的NVFlare框架相结合，提出了一种实用的医疗自然语言处理方法。我们介绍了两个示例性的自然语言处理模型：基于长短期记忆（LSTM）的模型和双向编码器表示转换（BERT），它们在理解上下文和语义方面表现出 exceptional 性能。",
    "tldr": "本论文提出了一种多站点临床联邦学习的方法，使用递归和注意力模型以及NVFlare框架。研究引入了两种示例性的自然语言处理模型，LSTM模型和BERT模型，这些模型在理解上下文和语义方面表现出了优异的性能。",
    "en_tdlr": "This paper proposes a method for multi-site clinical federated learning using recursive and attentive models and the NVFlare framework. The study introduces two exemplary natural language processing models, the LSTM-based model and BERT model, which have shown exceptional performance in understanding context and semantics."
}