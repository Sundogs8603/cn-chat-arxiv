{
    "title": "Binary domain generalization for sparsifying binary neural networks. (arXiv:2306.13515v1 [cs.LG])",
    "abstract": "Binary neural networks (BNNs) are an attractive solution for developing and deploying deep neural network (DNN)-based applications in resource constrained devices. Despite their success, BNNs still suffer from a fixed and limited compression factor that may be explained by the fact that existing pruning methods for full-precision DNNs cannot be directly applied to BNNs. In fact, weight pruning of BNNs leads to performance degradation, which suggests that the standard binarization domain of BNNs is not well adapted for the task. This work proposes a novel more general binary domain that extends the standard binary one that is more robust to pruning techniques, thus guaranteeing improved compression and avoiding severe performance losses. We demonstrate a closed-form solution for quantizing the weights of a full-precision network into the proposed binary domain. Finally, we show the flexibility of our method, which can be combined with other pruning strategies. Experiments over CIFAR-10 ",
    "link": "http://arxiv.org/abs/2306.13515",
    "context": "Title: Binary domain generalization for sparsifying binary neural networks. (arXiv:2306.13515v1 [cs.LG])\nAbstract: Binary neural networks (BNNs) are an attractive solution for developing and deploying deep neural network (DNN)-based applications in resource constrained devices. Despite their success, BNNs still suffer from a fixed and limited compression factor that may be explained by the fact that existing pruning methods for full-precision DNNs cannot be directly applied to BNNs. In fact, weight pruning of BNNs leads to performance degradation, which suggests that the standard binarization domain of BNNs is not well adapted for the task. This work proposes a novel more general binary domain that extends the standard binary one that is more robust to pruning techniques, thus guaranteeing improved compression and avoiding severe performance losses. We demonstrate a closed-form solution for quantizing the weights of a full-precision network into the proposed binary domain. Finally, we show the flexibility of our method, which can be combined with other pruning strategies. Experiments over CIFAR-10 ",
    "path": "papers/23/06/2306.13515.json",
    "total_tokens": 905,
    "translated_title": "二进制域广义化：用于 BNN 稀疏化的方法",
    "translated_abstract": "二进制神经网络 (BNN) 是在资源受限设备上开发和部署基于深度神经网络的应用的有效解决方案。尽管其成功，但 BNN 仍然受到固定和有限的压缩因子的影响，这可能是由于现有的全精度 DNN 稀疏化方法不能直接应用于 BNN。事实上，对 BNN 进行权重稀疏化会导致性能下降，这表明 BNN 的标准二进制化域并不适用于该任务。本研究提出了一种新的更一般的二进制域，扩展了标准二进制域，更能抵御稀疏化技术，从而保证了改进的压缩和避免了严重的性能损失。我们演示了一个将全精度网络的权重量化到所提出的二进制域的闭式解。最后，我们展示了我们的方法的灵活性，可以与其他稀疏化策略相结合。在 CIFAR-10 数据集上进行了实验。",
    "tldr": "该研究提出了一种新的更一般的二进制域，更能抵御稀疏化技术，从而保证了改进的压缩和避免了严重的性能损失。",
    "en_tdlr": "This paper proposes a novel general binary domain that is more robust to pruning techniques, thus guaranteeing improved compression and avoiding severe performance losses in binary neural networks (BNNs)."
}