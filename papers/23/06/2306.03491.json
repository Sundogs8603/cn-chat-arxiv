{
    "title": "SciCap+: A Knowledge Augmented Dataset to Study the Challenges of Scientific Figure Captioning. (arXiv:2306.03491v1 [cs.CV])",
    "abstract": "In scholarly documents, figures provide a straightforward way of communicating scientific findings to readers. Automating figure caption generation helps move model understandings of scientific documents beyond text and will help authors write informative captions that facilitate communicating scientific findings. Unlike previous studies, we reframe scientific figure captioning as a knowledge-augmented image captioning task that models need to utilize knowledge embedded across modalities for caption generation. To this end, we extended the large-scale SciCap dataset~\\cite{hsu-etal-2021-scicap-generating} to SciCap+ which includes mention-paragraphs (paragraphs mentioning figures) and OCR tokens. Then, we conduct experiments with the M4C-Captioner (a multimodal transformer-based model with a pointer network) as a baseline for our study. Our results indicate that mention-paragraphs serves as additional context knowledge, which significantly boosts the automatic standard image caption eva",
    "link": "http://arxiv.org/abs/2306.03491",
    "context": "Title: SciCap+: A Knowledge Augmented Dataset to Study the Challenges of Scientific Figure Captioning. (arXiv:2306.03491v1 [cs.CV])\nAbstract: In scholarly documents, figures provide a straightforward way of communicating scientific findings to readers. Automating figure caption generation helps move model understandings of scientific documents beyond text and will help authors write informative captions that facilitate communicating scientific findings. Unlike previous studies, we reframe scientific figure captioning as a knowledge-augmented image captioning task that models need to utilize knowledge embedded across modalities for caption generation. To this end, we extended the large-scale SciCap dataset~\\cite{hsu-etal-2021-scicap-generating} to SciCap+ which includes mention-paragraphs (paragraphs mentioning figures) and OCR tokens. Then, we conduct experiments with the M4C-Captioner (a multimodal transformer-based model with a pointer network) as a baseline for our study. Our results indicate that mention-paragraphs serves as additional context knowledge, which significantly boosts the automatic standard image caption eva",
    "path": "papers/23/06/2306.03491.json",
    "total_tokens": 904,
    "translated_title": "SciCap+: 一份知识增强的数据集，用于研究科学图例标题的挑战。",
    "translated_abstract": "在学术文献中，图例为向读者传达科学研究结果提供了一种直接的方式。自动生成图例标题有助于将科学文档模型理解超越文本，帮助作者编写有助于传达科学研究结果的信息性标题。与之前的研究不同，我们将科学图例标题重新构建为一种知识增强的图像标题生成任务，模型需要使用跨模态嵌入的知识进行标题生成。为此，我们扩展了大规模的SciCap数据集，增加了提到图片的段落和OCR标记。然后，我们使用基于指针网络的多模态变换模型M4C-Captioner作为我们研究的基线，进行实验。我们的结果表明，提到图片的段落作为附加上下文知识，极大地增强了自动标准图片的字幕生成的效果。",
    "tldr": "SciCap+ 是一份知识增强的数据集，用于研究科学图例标题自动生成的任务，从提到图片的段落和OCR标记中提取跨模态嵌入的知识，经实验发现这些知识可以显著地提高自动标准图片的字幕生成效果。",
    "en_tdlr": "SciCap+ is a knowledge-augmented dataset aimed at studying the task of automated scientific figure captioning. By utilizing knowledge embedded across modalities extracted from mention-paragraphs and OCR tokens, our experiments with the M4C-Captioner model indicate significant improvement in automatic standard image caption generation."
}