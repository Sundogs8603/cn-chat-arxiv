{
    "title": "Latent SDEs on Homogeneous Spaces. (arXiv:2306.16248v1 [cs.LG])",
    "abstract": "We consider the problem of variational Bayesian inference in a latent variable model where a (possibly complex) observed stochastic process is governed by the solution of a latent stochastic differential equation (SDE). Motivated by the challenges that arise when trying to learn an (almost arbitrary) latent neural SDE from large-scale data, such as efficient gradient computation, we take a step back and study a specific subclass instead. In our case, the SDE evolves on a homogeneous latent space and is induced by stochastic dynamics of the corresponding (matrix) Lie group. In learning problems, SDEs on the unit $n$-sphere are arguably the most relevant incarnation of this setup. Notably, for variational inference, the sphere not only facilitates using a truly uninformative prior SDE, but we also obtain a particularly simple and intuitive expression for the Kullback-Leibler divergence between the approximate posterior and prior process in the evidence lower bound. Experiments demonstrat",
    "link": "http://arxiv.org/abs/2306.16248",
    "context": "Title: Latent SDEs on Homogeneous Spaces. (arXiv:2306.16248v1 [cs.LG])\nAbstract: We consider the problem of variational Bayesian inference in a latent variable model where a (possibly complex) observed stochastic process is governed by the solution of a latent stochastic differential equation (SDE). Motivated by the challenges that arise when trying to learn an (almost arbitrary) latent neural SDE from large-scale data, such as efficient gradient computation, we take a step back and study a specific subclass instead. In our case, the SDE evolves on a homogeneous latent space and is induced by stochastic dynamics of the corresponding (matrix) Lie group. In learning problems, SDEs on the unit $n$-sphere are arguably the most relevant incarnation of this setup. Notably, for variational inference, the sphere not only facilitates using a truly uninformative prior SDE, but we also obtain a particularly simple and intuitive expression for the Kullback-Leibler divergence between the approximate posterior and prior process in the evidence lower bound. Experiments demonstrat",
    "path": "papers/23/06/2306.16248.json",
    "total_tokens": 916,
    "translated_title": "齐次空间上的潜在SDE",
    "translated_abstract": "我们考虑在潜在变量模型中的变分贝叶斯推断问题，其中一个（可能是复杂的）观测随机过程由潜在随机微分方程（SDE）的解决方案所驱动。受到学习大规模数据中（几乎任意）潜在神经SDE时所面临的挑战的启发，例如效率梯度计算，我们退一步并研究了一个特定的子类。在我们的情况下，SDE在一个齐次潜在空间上演变，并由相应（矩阵）Lie群的随机动力学所诱导。在学习问题中，单位$n$-球上的SDE可以说是这一设置中最相关的。值得注意的是，在变分推断中，单位球不仅有助于使用真正无信息的先验SDE，而且我们还获得了关于近似后验和先验过程之间的Kullback-Leibler散度的特别简单和直观的表达式，这在证据下界中至关重要。实验证明了我们的方法的性能优势。",
    "tldr": "这篇论文研究了在齐次空间上的潜在SDE，通过使用单位球上的SDE进行变分推断，提出了一种简单且直观的表达式来计算近似后验和先验过程之间的KL散度。"
}