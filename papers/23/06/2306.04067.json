{
    "title": "An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models. (arXiv:2306.04067v1 [cs.CL])",
    "abstract": "The increasingly large size of modern pretrained language models not only makes them inherit more human-like biases from the training corpora, but also makes it computationally expensive to mitigate such biases. In this paper, we investigate recent parameter-efficient methods in combination with counterfactual data augmentation (CDA) for bias mitigation. We conduct extensive experiments with prefix tuning, prompt tuning, and adapter tuning on different language models and bias types to evaluate their debiasing performance and abilities to preserve the internal knowledge of a pre-trained model. We find that the parameter-efficient methods (i) are effective in mitigating gender bias, where adapter tuning is consistently the most effective one and prompt tuning is more suitable for GPT-2 than BERT, (ii) are less effective when it comes to racial and religious bias, which may be attributed to the limitations of CDA, and (iii) can perform similarly to or sometimes better than full fine-tuni",
    "link": "http://arxiv.org/abs/2306.04067",
    "context": "Title: An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models. (arXiv:2306.04067v1 [cs.CL])\nAbstract: The increasingly large size of modern pretrained language models not only makes them inherit more human-like biases from the training corpora, but also makes it computationally expensive to mitigate such biases. In this paper, we investigate recent parameter-efficient methods in combination with counterfactual data augmentation (CDA) for bias mitigation. We conduct extensive experiments with prefix tuning, prompt tuning, and adapter tuning on different language models and bias types to evaluate their debiasing performance and abilities to preserve the internal knowledge of a pre-trained model. We find that the parameter-efficient methods (i) are effective in mitigating gender bias, where adapter tuning is consistently the most effective one and prompt tuning is more suitable for GPT-2 than BERT, (ii) are less effective when it comes to racial and religious bias, which may be attributed to the limitations of CDA, and (iii) can perform similarly to or sometimes better than full fine-tuni",
    "path": "papers/23/06/2306.04067.json",
    "total_tokens": 1009,
    "translated_title": "预训练语言模型去偏见的参数高效方法的实证分析",
    "translated_abstract": "现代预训练语言模型越来越大的尺寸不仅会让它们从训练数据中继承更多类似于人类的偏见，而且也会让缓解这些偏见变得计算上昂贵。本文研究使用反事实数据增强(CDA)结合最近的参数高效方法来进行偏见缓解。我们在不同的语言模型和偏见类型上进行前缀调整、提示调整和适配器调整方面的广泛实验，以评估它们的去偏见性能和保持预训练模型的内部知识能力。我们发现，参数高效方法(i)在减轻性别偏见方面是有效的，其中适配器调整始终是最有效的，提示调整更适合于GPT-2而不是BERT；(ii)在减轻种族和宗教偏见方面不太有效，这可能归因于CDA的局限性；(iii)在有时可以与完全微调的模型表现类似或更好时效果也不错。",
    "tldr": "本文分析了使用反事实数据增强(CDA)结合最近的参数高效方法来进行偏见缓解的实验结果。研究发现参数高效方法对减轻性别偏见有效，其中适配器调整效果最好，提示调整适合于GPT-2。但在减轻种族和宗教偏见方面效果不太明显，可能因为CDA的局限性。",
    "en_tdlr": "This paper explores parameter-efficient methods combined with counterfactual data augmentation (CDA) for debiasing pre-trained language models. The study finds that these methods are effective in mitigating gender bias, with adapter tuning being the most effective, and prompt tuning being more suitable for GPT-2 than BERT. However, they are less effective in mitigating racial and religious bias, potentially due to the limitations of CDA."
}