{
    "title": "Safeguarding Data in Multimodal AI: A Differentially Private Approach to CLIP Training. (arXiv:2306.08173v1 [cs.LG])",
    "abstract": "The surge in multimodal AI's success has sparked concerns over data privacy in vision-and-language tasks. While CLIP has revolutionized multimodal learning through joint training on images and text, its potential to unintentionally disclose sensitive information necessitates the integration of privacy-preserving mechanisms. We introduce a differentially private adaptation of the Contrastive Language-Image Pretraining (CLIP) model that effectively addresses privacy concerns while retaining accuracy. Our proposed method, Dp-CLIP, is rigorously evaluated on benchmark datasets encompassing diverse vision-and-language tasks such as image classification and visual question answering. We demonstrate that our approach retains performance on par with the standard non-private CLIP model. Furthermore, we analyze our proposed algorithm under linear representation settings. We derive the convergence rate of our algorithm and show a trade-off between utility and privacy when gradients are clipped pe",
    "link": "http://arxiv.org/abs/2306.08173",
    "context": "Title: Safeguarding Data in Multimodal AI: A Differentially Private Approach to CLIP Training. (arXiv:2306.08173v1 [cs.LG])\nAbstract: The surge in multimodal AI's success has sparked concerns over data privacy in vision-and-language tasks. While CLIP has revolutionized multimodal learning through joint training on images and text, its potential to unintentionally disclose sensitive information necessitates the integration of privacy-preserving mechanisms. We introduce a differentially private adaptation of the Contrastive Language-Image Pretraining (CLIP) model that effectively addresses privacy concerns while retaining accuracy. Our proposed method, Dp-CLIP, is rigorously evaluated on benchmark datasets encompassing diverse vision-and-language tasks such as image classification and visual question answering. We demonstrate that our approach retains performance on par with the standard non-private CLIP model. Furthermore, we analyze our proposed algorithm under linear representation settings. We derive the convergence rate of our algorithm and show a trade-off between utility and privacy when gradients are clipped pe",
    "path": "papers/23/06/2306.08173.json",
    "total_tokens": 906,
    "translated_title": "在多模态人工智能中保护数据：一种差分隐私方法用于CLIP训练",
    "translated_abstract": "多模态人工智能的成功引发了视觉和语言任务中数据隐私的关注。虽然CLIP通过对图像和文本的联合训练彻底改变了多模态学习，但其可能无意中披露敏感信息的潜力需要集成保护隐私的机制。我们引入了对比语言-图像预训练（CLIP）模型的差分隐私改进，有效地解决了隐私问题，同时保持准确性。我们提出的方法Dp-CLIP在包括图像分类和视觉问答等多样的视觉和语言任务的基准数据集上进行了严格评估。我们证明了我们的方法保持了与标准的非私有CLIP模型同等的性能。此外，我们在线性表示设置下分析了我们提出的算法。我们推导了算法的收敛速度，并展示了在梯度被剪辑时实用性和隐私之间的权衡。",
    "tldr": "本文提出了一种差分隐私的CLIP模型（Dp-CLIP），旨在保护多模态AI任务中的数据隐私，同时保持模型准确性。该方法在基准数据集上得到了验证，并表明其与标准非私有CLIP模型相比具有同等的性能。",
    "en_tdlr": "This paper proposes a differentially private CLIP model (Dp-CLIP) to protect data privacy in multimodal AI tasks while maintaining model accuracy. The proposed method is rigorously evaluated on benchmark datasets and demonstrates comparable performance with the standard non-private CLIP model."
}