{
    "title": "Lower Complexity Adaptation for Empirical Entropic Optimal Transport. (arXiv:2306.13580v1 [math.ST])",
    "abstract": "Entropic optimal transport (EOT) presents an effective and computationally viable alternative to unregularized optimal transport (OT), offering diverse applications for large-scale data analysis. In this work, we derive novel statistical bounds for empirical plug-in estimators of the EOT cost and show that their statistical performance in the entropy regularization parameter $\\epsilon$ and the sample size $n$ only depends on the simpler of the two probability measures. For instance, under sufficiently smooth costs this yields the parametric rate $n^{-1/2}$ with factor $\\epsilon^{-d/2}$, where $d$ is the minimum dimension of the two population measures. This confirms that empirical EOT also adheres to the lower complexity adaptation principle, a hallmark feature only recently identified for unregularized OT. As a consequence of our theory, we show that the empirical entropic Gromov-Wasserstein distance and its unregularized version for measures on Euclidean spaces also obey this princip",
    "link": "http://arxiv.org/abs/2306.13580",
    "context": "Title: Lower Complexity Adaptation for Empirical Entropic Optimal Transport. (arXiv:2306.13580v1 [math.ST])\nAbstract: Entropic optimal transport (EOT) presents an effective and computationally viable alternative to unregularized optimal transport (OT), offering diverse applications for large-scale data analysis. In this work, we derive novel statistical bounds for empirical plug-in estimators of the EOT cost and show that their statistical performance in the entropy regularization parameter $\\epsilon$ and the sample size $n$ only depends on the simpler of the two probability measures. For instance, under sufficiently smooth costs this yields the parametric rate $n^{-1/2}$ with factor $\\epsilon^{-d/2}$, where $d$ is the minimum dimension of the two population measures. This confirms that empirical EOT also adheres to the lower complexity adaptation principle, a hallmark feature only recently identified for unregularized OT. As a consequence of our theory, we show that the empirical entropic Gromov-Wasserstein distance and its unregularized version for measures on Euclidean spaces also obey this princip",
    "path": "papers/23/06/2306.13580.json",
    "total_tokens": 892,
    "translated_title": "经验熵正则化最优输运的低复杂度适应性",
    "translated_abstract": "经验熵正则化最优输运 (EOT) 是优化输运 (OT) 的一种有效且计算可行的替代方案，对大规模数据分析有着广泛的应用。本文推导出了 EOT 成本的新的统计界限，并显示它们在熵正则化参数 $\\epsilon$ 和样本大小 $n$ 的统计性能仅取决于两个概率测度之中较简单的那个。例如，在充分平滑的成本下，这会产生具有$\\epsilon^{-d/2}$因子的参数化速率$n^{-1/2}$，其中$d$是两个总体测度的最小维度。这确认了经验EOT也遵循了最近才为未规则化OT确认的低复杂度适应原则的标志性特征。根据我们的理论，我们展示了欧几里得空间上的测度的经验熵Gromov-Wasserstein距离及其未规则化版本也遵循此原则。",
    "tldr": "本文研究了经验熵正则化最优输运的统计表现，并证明了它遵循低复杂度适应原则，推导出了其统计界限及参数化速率。",
    "en_tdlr": "This paper studies the statistical performance of empirical entropic optimal transport (EOT) and proves that it adheres to the principle of lower complexity adaptation with statistical bounds and rates derived, also showing that the empirical entropic Gromov-Wasserstein distance and its unregularized version on Euclidean spaces follow this principle."
}