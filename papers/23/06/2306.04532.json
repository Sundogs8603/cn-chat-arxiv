{
    "title": "Long Sequence Hopfield Memory. (arXiv:2306.04532v2 [cs.NE] CROSS LISTED)",
    "abstract": "Sequence memory is an essential attribute of natural and artificial intelligence that enables agents to encode, store, and retrieve complex sequences of stimuli and actions. Computational models of sequence memory have been proposed where recurrent Hopfield-like neural networks are trained with temporally asymmetric Hebbian rules. However, these networks suffer from limited sequence capacity (maximal length of the stored sequence) due to interference between the memories. Inspired by recent work on Dense Associative Memories, we expand the sequence capacity of these models by introducing a nonlinear interaction term, enhancing separation between the patterns. We derive novel scaling laws for sequence capacity with respect to network size, significantly outperforming existing scaling laws for models based on traditional Hopfield networks, and verify these theoretical results with numerical simulation. Moreover, we introduce a generalized pseudoinverse rule to recall sequences of highly ",
    "link": "http://arxiv.org/abs/2306.04532",
    "context": "Title: Long Sequence Hopfield Memory. (arXiv:2306.04532v2 [cs.NE] CROSS LISTED)\nAbstract: Sequence memory is an essential attribute of natural and artificial intelligence that enables agents to encode, store, and retrieve complex sequences of stimuli and actions. Computational models of sequence memory have been proposed where recurrent Hopfield-like neural networks are trained with temporally asymmetric Hebbian rules. However, these networks suffer from limited sequence capacity (maximal length of the stored sequence) due to interference between the memories. Inspired by recent work on Dense Associative Memories, we expand the sequence capacity of these models by introducing a nonlinear interaction term, enhancing separation between the patterns. We derive novel scaling laws for sequence capacity with respect to network size, significantly outperforming existing scaling laws for models based on traditional Hopfield networks, and verify these theoretical results with numerical simulation. Moreover, we introduce a generalized pseudoinverse rule to recall sequences of highly ",
    "path": "papers/23/06/2306.04532.json",
    "total_tokens": 839,
    "translated_title": "长序列 Hopfield内存",
    "translated_abstract": "序列记忆是自然和人工智能的重要属性，它使代理能够编码、存储和检索复杂的刺激和行为序列。已经提出了计算模型，其中用时间非对称的Hebbian规则训练递归的Hopfield样神经网络。然而，这些网络由于记忆之间的干扰而具有有限的序列容量（存储序列的最大长度）。受密集关联记忆的最新工作的启发，我们通过引入非线性相互作用项来扩展这些模型的序列容量，增强模式之间的分离性。我们推导出序列容量与网络大小的新的标度定律，显著优于基于传统Hopfield网络的现有标度定律，并通过数值模拟验证了这些理论结果。此外，我们引入了一个广义伪逆规则来回忆高度连续的序列。",
    "tldr": "这篇论文提出了一种增强Hopfield-like神经网络序列记忆模型的序列容量的方法，通过引入非线性相互作用项，显著优于传统Hopfield网络，同时也引入了一个新的回忆规则以回忆连续的序列。",
    "en_tdlr": "This paper proposes a method to enhance the sequence capacity of Hopfield-like neural network sequence memory models by introducing nonlinear interaction terms, significantly outperforming traditional Hopfield networks, and also introduces a new recall rule for recalling continuous sequences."
}