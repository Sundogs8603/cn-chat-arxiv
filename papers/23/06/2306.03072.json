{
    "title": "Explore to Generalize in Zero-Shot RL. (arXiv:2306.03072v3 [cs.LG] UPDATED)",
    "abstract": "We study zero-shot generalization in reinforcement learning-optimizing a policy on a set of training tasks to perform well on a similar but unseen test task. To mitigate overfitting, previous work explored different notions of invariance to the task. However, on problems such as the ProcGen Maze, an adequate solution that is invariant to the task visualization does not exist, and therefore invariance-based approaches fail. Our insight is that learning a policy that effectively $\\textit{explores}$ the domain is harder to memorize than a policy that maximizes reward for a specific task, and therefore we expect such learned behavior to generalize well; we indeed demonstrate this empirically on several domains that are difficult for invariance-based approaches. Our $\\textit{Explore to Generalize}$ algorithm (ExpGen) builds on this insight: we train an additional ensemble of agents that optimize reward. At test time, either the ensemble agrees on an action, and we generalize well, or we tak",
    "link": "http://arxiv.org/abs/2306.03072",
    "context": "Title: Explore to Generalize in Zero-Shot RL. (arXiv:2306.03072v3 [cs.LG] UPDATED)\nAbstract: We study zero-shot generalization in reinforcement learning-optimizing a policy on a set of training tasks to perform well on a similar but unseen test task. To mitigate overfitting, previous work explored different notions of invariance to the task. However, on problems such as the ProcGen Maze, an adequate solution that is invariant to the task visualization does not exist, and therefore invariance-based approaches fail. Our insight is that learning a policy that effectively $\\textit{explores}$ the domain is harder to memorize than a policy that maximizes reward for a specific task, and therefore we expect such learned behavior to generalize well; we indeed demonstrate this empirically on several domains that are difficult for invariance-based approaches. Our $\\textit{Explore to Generalize}$ algorithm (ExpGen) builds on this insight: we train an additional ensemble of agents that optimize reward. At test time, either the ensemble agrees on an action, and we generalize well, or we tak",
    "path": "papers/23/06/2306.03072.json",
    "total_tokens": 951,
    "translated_title": "在零样本强化学习中探索泛化",
    "translated_abstract": "本研究探讨了在强化学习中的零样本泛化问题，即在一组训练任务上优化策略以在类似但未见过的测试任务上表现良好。为了缓解过拟合问题，先前的工作探索了不同的任务不变性概念。然而，在像ProcGen Maze这样的问题上，不存在一种与任务可视化无关的适当解决方案，因此基于不变性的方法失败了。我们的洞察力是，学习一种有效地探索领域的策略比学习一种在特定任务中最大化奖励的策略更难被记忆，因此我们期望这种学习到的行为能够良好泛化；我们在几个对于基于不变性方法来说困难的领域上通过实证验证了这一点。我们的“探索泛化”算法（ExpGen）基于这一观点：我们训练了一个额外的代理集合来优化奖励。在测试时，如果代理集合对一个动作达成一致，我们将能够良好泛化，否则我们将采取其他策略。",
    "tldr": "本文针对零样本强化学习中的泛化问题进行了研究，通过学习一种有效探索领域的策略，成功解决了在像ProcGen Maze这样的问题上基于不变性的方法失败的困境。",
    "en_tdlr": "This paper investigates zero-shot generalization in reinforcement learning and proposes a solution by learning a policy that effectively explores the domain, which addresses the failure of invariance-based approaches on problems such as ProcGen Maze."
}