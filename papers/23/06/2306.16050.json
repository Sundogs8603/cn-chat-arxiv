{
    "title": "Evaluating Similitude and Robustness of Deep Image Denoising Models via Adversarial Attack. (arXiv:2306.16050v1 [cs.CV])",
    "abstract": "Deep neural networks (DNNs) have a wide range of applications in the field of image denoising, and they are superior to traditional image denoising. However, DNNs inevitably show vulnerability, which is the weak robustness in the face of adversarial attacks. In this paper, we find some similitudes between existing deep image denoising methods, as they are consistently fooled by adversarial attacks. First, denoising-PGD is proposed which is a denoising model full adversarial method. The current mainstream non-blind denoising models (DnCNN, FFDNet, ECNDNet, BRDNet), blind denoising models (DnCNN-B, Noise2Noise, RDDCNN-B, FAN), and plug-and-play (DPIR, CurvPnP) and unfolding denoising models (DeamNet) applied to grayscale and color images can be attacked by the same set of methods. Second, since the transferability of denoising-PGD is prominent in the image denoising task, we design experiments to explore the characteristic of the latent under the transferability. We correlate transferabi",
    "link": "http://arxiv.org/abs/2306.16050",
    "context": "Title: Evaluating Similitude and Robustness of Deep Image Denoising Models via Adversarial Attack. (arXiv:2306.16050v1 [cs.CV])\nAbstract: Deep neural networks (DNNs) have a wide range of applications in the field of image denoising, and they are superior to traditional image denoising. However, DNNs inevitably show vulnerability, which is the weak robustness in the face of adversarial attacks. In this paper, we find some similitudes between existing deep image denoising methods, as they are consistently fooled by adversarial attacks. First, denoising-PGD is proposed which is a denoising model full adversarial method. The current mainstream non-blind denoising models (DnCNN, FFDNet, ECNDNet, BRDNet), blind denoising models (DnCNN-B, Noise2Noise, RDDCNN-B, FAN), and plug-and-play (DPIR, CurvPnP) and unfolding denoising models (DeamNet) applied to grayscale and color images can be attacked by the same set of methods. Second, since the transferability of denoising-PGD is prominent in the image denoising task, we design experiments to explore the characteristic of the latent under the transferability. We correlate transferabi",
    "path": "papers/23/06/2306.16050.json",
    "total_tokens": 1004,
    "translated_title": "通过对抗攻击评估深度图像去噪模型的相似性和鲁棒性",
    "translated_abstract": "深度神经网络在图像去噪领域有着广泛的应用，并且比传统的图像去噪方法更优越。然而，深度神经网络不可避免地显示出弱鲁棒性，在面对对抗攻击时易受损。本文发现了现有深度图像去噪方法之间的相似之处，它们都容易被对抗攻击欺骗。首先，我们提出了一种去噪-PGD方法，它是一种全对抗的去噪模型。当前主流的非盲去噪模型（DnCNN，FFDNet，ECNDNet，BRDNet），盲去噪模型（DnCNN-B，Noise2Noise，RDDCNN-B，FAN），即插即用（DPIR，CurvPnP）和展开去噪模型（DeamNet）应用于灰度和彩色图像都可以被同一组方法攻击。其次，由于去噪-PGD的迁移性在图像去噪任务中很突出，我们设计了实验来探索迁移性下的潜在特性。",
    "tldr": "通过对抗攻击评估了深度图像去噪模型的相似性和鲁棒性，发现现有模型容易被攻击。还研究了去噪模型的迁移性在图像去噪任务中的特性。"
}