{
    "title": "Toward Grounded Commonsense Reasoning",
    "abstract": "arXiv:2306.08651v2 Announce Type: replace-cross  Abstract: Consider a robot tasked with tidying a desk with a meticulously constructed Lego sports car. A human may recognize that it is not appropriate to disassemble the sports car and put it away as part of the \"tidying.\" How can a robot reach that conclusion? Although large language models (LLMs) have recently been used to enable commonsense reasoning, grounding this reasoning in the real world has been challenging. To reason in the real world, robots must go beyond passively querying LLMs and actively gather information from the environment that is required to make the right decision. For instance, after detecting that there is an occluded car, the robot may need to actively perceive the car to know whether it is an advanced model car made out of Legos or a toy car built by a toddler. We propose an approach that leverages an LLM and vision language model (VLM) to help a robot actively perceive its environment to perform grounded comm",
    "link": "https://arxiv.org/abs/2306.08651",
    "context": "Title: Toward Grounded Commonsense Reasoning\nAbstract: arXiv:2306.08651v2 Announce Type: replace-cross  Abstract: Consider a robot tasked with tidying a desk with a meticulously constructed Lego sports car. A human may recognize that it is not appropriate to disassemble the sports car and put it away as part of the \"tidying.\" How can a robot reach that conclusion? Although large language models (LLMs) have recently been used to enable commonsense reasoning, grounding this reasoning in the real world has been challenging. To reason in the real world, robots must go beyond passively querying LLMs and actively gather information from the environment that is required to make the right decision. For instance, after detecting that there is an occluded car, the robot may need to actively perceive the car to know whether it is an advanced model car made out of Legos or a toy car built by a toddler. We propose an approach that leverages an LLM and vision language model (VLM) to help a robot actively perceive its environment to perform grounded comm",
    "path": "papers/23/06/2306.08651.json",
    "total_tokens": 848,
    "translated_title": "迈向基于常识的推理",
    "translated_abstract": "考虑一个被交付整理桌子上精心构建的乐高跑车的机器人。人类可能认识到，拆开跑车并将其放回作为“整理”的一部分是不合适的。一个机器人如何得出这个结论呢？尽管大型语言模型(LLMs)最近被用于实现常识推理，但将这种推理落实到现实世界中一直是具有挑战性的。为了在现实世界中进行推理，机器人必须超越被动查询LLMs，积极地从环境中收集必要的信息来做出正确的决策。例如，在检测到有一个被遮挡的汽车后，机器人可能需要主动感知汽车，以了解它是由乐高制作的高级型号汽车，还是由幼儿制作的玩具汽车。我们提出了一种方法，利用一个LLM和视觉语言模型(VLM)来帮助机器人主动感知其环境以进行基于事实的推理。",
    "tldr": "提出了一种利用大型语言模型和视觉语言模型帮助机器人积极感知环境，进行基于常识的推理的方法。"
}