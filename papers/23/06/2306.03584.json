{
    "title": "RDFC-GAN: RGB-Depth Fusion CycleGAN for Indoor Depth Completion. (arXiv:2306.03584v1 [cs.CV])",
    "abstract": "The raw depth image captured by indoor depth sensors usually has an extensive range of missing depth values due to inherent limitations such as the inability to perceive transparent objects and the limited distance range. The incomplete depth map with missing values burdens many downstream vision tasks, and a rising number of depth completion methods have been proposed to alleviate this issue. While most existing methods can generate accurate dense depth maps from sparse and uniformly sampled depth maps, they are not suitable for complementing large contiguous regions of missing depth values, which is common and critical in images captured in indoor environments. To overcome these challenges, we design a novel two-branch end-to-end fusion network named RDFC-GAN, which takes a pair of RGB and incomplete depth images as input to predict a dense and completed depth map. The first branch employs an encoder-decoder structure, by adhering to the Manhattan world assumption and utilizing norma",
    "link": "http://arxiv.org/abs/2306.03584",
    "context": "Title: RDFC-GAN: RGB-Depth Fusion CycleGAN for Indoor Depth Completion. (arXiv:2306.03584v1 [cs.CV])\nAbstract: The raw depth image captured by indoor depth sensors usually has an extensive range of missing depth values due to inherent limitations such as the inability to perceive transparent objects and the limited distance range. The incomplete depth map with missing values burdens many downstream vision tasks, and a rising number of depth completion methods have been proposed to alleviate this issue. While most existing methods can generate accurate dense depth maps from sparse and uniformly sampled depth maps, they are not suitable for complementing large contiguous regions of missing depth values, which is common and critical in images captured in indoor environments. To overcome these challenges, we design a novel two-branch end-to-end fusion network named RDFC-GAN, which takes a pair of RGB and incomplete depth images as input to predict a dense and completed depth map. The first branch employs an encoder-decoder structure, by adhering to the Manhattan world assumption and utilizing norma",
    "path": "papers/23/06/2306.03584.json",
    "total_tokens": 891,
    "translated_title": "RDFC-GAN:室内深度完形补全的RGB-Depth融合CycleGAN(arXiv:2306.03584v1 [cs.CV])",
    "translated_abstract": "室内深度传感器捕捉的原始深度图像通常具有大量缺失深度值的范围，导致了很多带下游视觉任务的不完整的深度图，因此已经提出了越来越多的深度完成方法。为了解决这些问题，设计了一种名为RDFC-GAN的新颖的双支端到端融合网络，它需要一对RGB和不完整深度图像作为输入来预测一个密集的和完成的深度图。",
    "tldr": "RDFC-GAN使用两个分支结构生成精确的深度图像，它通过解决室内环境中普遍缺失的大面积深度值问题，克服了现有方法的不足，可用于各种室内深度完成任务。",
    "en_tdlr": "RDFC-GAN is a novel two-branch end-to-end fusion network designed to overcome the challenge of completing large contiguous regions of missing depth values in indoor environments. It employs an encoder-decoder structure and utilizes norma guidance loss to generate depth maps that respect geometric constraints. The CycleGAN approach is used to fuse RGB and incomplete depth features, generating visually plausible depth maps. Experimental results demonstrate that RDFC-GAN outperforms state-of-the-art methods in indoor depth completion benchmarks."
}