{
    "title": "Zero-Concentrated Private Distributed Learning for Nonsmooth Objective Functions. (arXiv:2306.14012v1 [math.OC])",
    "abstract": "This paper develops a fully distributed differentially-private learning algorithm to solve nonsmooth optimization problems. We distribute the Alternating Direction Method of Multipliers (ADMM) to comply with the distributed setting and employ an approximation of the augmented Lagrangian to handle nonsmooth objective functions. Furthermore, we ensure zero-concentrated differential privacy (zCDP) by perturbing the outcome of the computation at each agent with a variance-decreasing Gaussian noise. This privacy-preserving method allows for better accuracy than the conventional $(\\epsilon, \\delta)$-DP and stronger guarantees than the more recent R\\'enyi-DP. The developed fully distributed algorithm has a competitive privacy accuracy trade-off and handles nonsmooth and non-necessarily strongly convex problems. We provide complete theoretical proof for the privacy guarantees and the convergence of the algorithm to the exact solution. We also prove under additional assumptions that the algorit",
    "link": "http://arxiv.org/abs/2306.14012",
    "context": "Title: Zero-Concentrated Private Distributed Learning for Nonsmooth Objective Functions. (arXiv:2306.14012v1 [math.OC])\nAbstract: This paper develops a fully distributed differentially-private learning algorithm to solve nonsmooth optimization problems. We distribute the Alternating Direction Method of Multipliers (ADMM) to comply with the distributed setting and employ an approximation of the augmented Lagrangian to handle nonsmooth objective functions. Furthermore, we ensure zero-concentrated differential privacy (zCDP) by perturbing the outcome of the computation at each agent with a variance-decreasing Gaussian noise. This privacy-preserving method allows for better accuracy than the conventional $(\\epsilon, \\delta)$-DP and stronger guarantees than the more recent R\\'enyi-DP. The developed fully distributed algorithm has a competitive privacy accuracy trade-off and handles nonsmooth and non-necessarily strongly convex problems. We provide complete theoretical proof for the privacy guarantees and the convergence of the algorithm to the exact solution. We also prove under additional assumptions that the algorit",
    "path": "papers/23/06/2306.14012.json",
    "total_tokens": 931,
    "translated_title": "用于非平滑目标函数的零集中度私有分布式学习",
    "translated_abstract": "本文开发了一种完全分布式的差分隐私学习算法来解决非平滑优化问题。我们将交替方向乘子法（ADMM）分布到分布式设置中，并采用增广拉格朗日近似来处理非平滑目标函数。此外，我们通过在每个代理处用方差递减的高斯噪声扰动计算结果来确保零集中差分隐私（zCDP）。这种隐私保护方法允许比传统的$(\\epsilon，\\delta)$-DP更好的准确性，比最近的Rényi-DP提供更强的保证。开发的完全分布式算法具有竞争性的隐私准确性平衡，并处理非平滑和非必须强凸问题。我们提供了隐私保证和算法收敛到精确解的完整理论证明。我们还证明，在其他假设下，该算法的收敛速度比集中式非私有算法更快。",
    "tldr": "本文提出了一种用于解决非平滑优化问题的完全分布式的差分隐私学习算法，保证零集中度差分隐私，具有更好的准确性和更强的保证，并且处理非平滑和非必须强凸问题。",
    "en_tdlr": "This paper proposes a fully distributed differentially-private learning algorithm to tackle nonsmooth optimization problems. By employing zero-concentrated differential privacy, the algorithm ensures better accuracy and stronger guarantees, while also handling non-convex problems. This approach provides complete theoretical proof for the privacy guarantees and the convergence of the algorithm to the exact solution."
}