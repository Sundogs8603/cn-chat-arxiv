{
    "title": "Transforming Graphs for Enhanced Attribute-Based Clustering: An Innovative Graph Transformer Method. (arXiv:2306.11307v2 [cs.LG] UPDATED)",
    "abstract": "Graph Representation Learning (GRL) is an influential methodology, enabling a more profound understanding of graph-structured data and aiding graph clustering, a critical task across various domains. The recent incursion of attention mechanisms, originally an artifact of Natural Language Processing (NLP), into the realm of graph learning has spearheaded a notable shift in research trends. Consequently, Graph Attention Networks (GATs) and Graph Attention Auto-Encoders have emerged as preferred tools for graph clustering tasks. Yet, these methods primarily employ a local attention mechanism, thereby curbing their capacity to apprehend the intricate global dependencies between nodes within graphs. Addressing these impediments, this study introduces an innovative method known as the Graph Transformer Auto-Encoder for Graph Clustering (GTAGC). By melding the Graph Auto-Encoder with the Graph Transformer, GTAGC is adept at capturing global dependencies between nodes. This integration amplifi",
    "link": "http://arxiv.org/abs/2306.11307",
    "context": "Title: Transforming Graphs for Enhanced Attribute-Based Clustering: An Innovative Graph Transformer Method. (arXiv:2306.11307v2 [cs.LG] UPDATED)\nAbstract: Graph Representation Learning (GRL) is an influential methodology, enabling a more profound understanding of graph-structured data and aiding graph clustering, a critical task across various domains. The recent incursion of attention mechanisms, originally an artifact of Natural Language Processing (NLP), into the realm of graph learning has spearheaded a notable shift in research trends. Consequently, Graph Attention Networks (GATs) and Graph Attention Auto-Encoders have emerged as preferred tools for graph clustering tasks. Yet, these methods primarily employ a local attention mechanism, thereby curbing their capacity to apprehend the intricate global dependencies between nodes within graphs. Addressing these impediments, this study introduces an innovative method known as the Graph Transformer Auto-Encoder for Graph Clustering (GTAGC). By melding the Graph Auto-Encoder with the Graph Transformer, GTAGC is adept at capturing global dependencies between nodes. This integration amplifi",
    "path": "papers/23/06/2306.11307.json",
    "total_tokens": 918,
    "translated_title": "增强属性聚类的图形变换方法：一种创新的图形转换器方法",
    "translated_abstract": "图表示学习是一种有影响力的方法，它使得我们更深入地理解图结构化数据，并有助于图聚类，这是各个领域的一个关键任务。注意力机制最近已经进入了图学习的领域，这从根本上改变了研究趋势。因此，图注意力网络和图注意力自编码器已成为图聚类任务优选的工具。然而，这些方法主要采用局部注意机制，从而限制了它们理解图中节点之间复杂全局依赖性的能力。为了解决这些障碍，本研究介绍了一种称为图自编码器的图形变换自编码器的图形聚类（GTAGC）的创新方法。通过将图自编码器与图形变换器融合，GTAGC能够捕获节点之间的全局依赖关系。这种融合提高了性能，使得图形聚类任务有了显着的提升。",
    "tldr": "本文提出了一种称为GTAGC的图形自编码器图形变换自编码器方法，通过融合图自编码器和图形变换器，GTAGC能够捕获全局依赖关系，从而有助于提高图聚类的性能。",
    "en_tdlr": "This paper proposes an innovative method called GTAGC for graph clustering tasks by combining the Graph Auto-Encoder with the Graph Transformer, enabling the capture of global dependencies between nodes, resulting in significant performance improvements."
}