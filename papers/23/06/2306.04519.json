{
    "title": "Sample-Level Weighting for Multi-Task Learning with Auxiliary Tasks. (arXiv:2306.04519v1 [cs.LG])",
    "abstract": "Multi-task learning (MTL) can improve the generalization performance of neural networks by sharing representations with related tasks. Nonetheless, MTL can also degrade performance through harmful interference between tasks. Recent work has pursued task-specific loss weighting as a solution for this interference. However, existing algorithms treat tasks as atomic, lacking the ability to explicitly separate harmful and helpful signals beyond the task level. To this end, we propose SLGrad, a sample-level weighting algorithm for multi-task learning with auxiliary tasks. Through sample-specific task weights, SLGrad reshapes the task distributions during training to eliminate harmful auxiliary signals and augment useful task signals. Substantial generalization performance gains are observed on (semi-) synthetic datasets and common supervised multi-task problems.",
    "link": "http://arxiv.org/abs/2306.04519",
    "context": "Title: Sample-Level Weighting for Multi-Task Learning with Auxiliary Tasks. (arXiv:2306.04519v1 [cs.LG])\nAbstract: Multi-task learning (MTL) can improve the generalization performance of neural networks by sharing representations with related tasks. Nonetheless, MTL can also degrade performance through harmful interference between tasks. Recent work has pursued task-specific loss weighting as a solution for this interference. However, existing algorithms treat tasks as atomic, lacking the ability to explicitly separate harmful and helpful signals beyond the task level. To this end, we propose SLGrad, a sample-level weighting algorithm for multi-task learning with auxiliary tasks. Through sample-specific task weights, SLGrad reshapes the task distributions during training to eliminate harmful auxiliary signals and augment useful task signals. Substantial generalization performance gains are observed on (semi-) synthetic datasets and common supervised multi-task problems.",
    "path": "papers/23/06/2306.04519.json",
    "total_tokens": 816,
    "translated_title": "辅助任务下多任务学习的样本级加权",
    "translated_abstract": "多任务学习(MTL)可以通过与相关任务共享表示来提高神经网络的泛化性能。然而，MTL也可能通过任务之间的有害干扰而降低性能。最近的工作采用任务特定的损失加权作为解决干扰的方法。然而，现有算法将任务视为原子性，缺乏将有害和有用信号明确分离的能力。为此，我们提出了SLGrad，一种用于辅助任务下多任务学习的样本级加权算法。通过样本特定的任务权重，SLGrad在训练过程中重新塑造任务分布，消除有害的辅助信号并增强有用的任务信号。在(半)合成数据集和常见的监督多任务问题上观察到了显著的泛化性能提升。",
    "tldr": "提出了一种用于辅助任务下多任务学习的样本级加权算法SLGrad，通过样本特定的任务权重，消除有害的辅助信号并增强有用的任务信号，实现了泛化性能的提升。",
    "en_tdlr": "The paper proposes a sample-level weighting algorithm named SLGrad for multi-task learning with auxiliary tasks, which reshapes the task distributions during training through sample-specific task weights to eliminate harmful auxiliary signals and augment useful task signals, leading to significant improvement in generalization performance on both (semi-) synthetic datasets and common supervised multi-task problems."
}