{
    "title": "Using Visual Cropping to Enhance Fine-Detail Question Answering of BLIP-Family Models. (arXiv:2306.00228v1 [cs.CV])",
    "abstract": "Visual Question Answering is a challenging task, as it requires seamless interaction between perceptual, linguistic, and background knowledge systems. While the recent progress of visual and natural language models like BLIP has led to improved performance on this task, we lack understanding of the ability of such models to perform on different kinds of questions and reasoning types. As our initial analysis of BLIP-family models revealed difficulty with answering fine-detail questions, we investigate the following question: Can visual cropping be employed to improve the performance of state-of-the-art visual question answering models on fine-detail questions? Given the recent success of the BLIP-family models, we study a zero-shot and a fine-tuned BLIP model. We define three controlled subsets of the popular VQA-v2 benchmark to measure whether cropping can help model performance. Besides human cropping, we devise two automatic cropping strategies based on multi-modal embedding by CLIP ",
    "link": "http://arxiv.org/abs/2306.00228",
    "context": "Title: Using Visual Cropping to Enhance Fine-Detail Question Answering of BLIP-Family Models. (arXiv:2306.00228v1 [cs.CV])\nAbstract: Visual Question Answering is a challenging task, as it requires seamless interaction between perceptual, linguistic, and background knowledge systems. While the recent progress of visual and natural language models like BLIP has led to improved performance on this task, we lack understanding of the ability of such models to perform on different kinds of questions and reasoning types. As our initial analysis of BLIP-family models revealed difficulty with answering fine-detail questions, we investigate the following question: Can visual cropping be employed to improve the performance of state-of-the-art visual question answering models on fine-detail questions? Given the recent success of the BLIP-family models, we study a zero-shot and a fine-tuned BLIP model. We define three controlled subsets of the popular VQA-v2 benchmark to measure whether cropping can help model performance. Besides human cropping, we devise two automatic cropping strategies based on multi-modal embedding by CLIP ",
    "path": "papers/23/06/2306.00228.json",
    "total_tokens": 839,
    "translated_title": "使用视觉裁剪来增强BLIP模型细节问题回答的能力",
    "translated_abstract": "视觉问答是一项具有挑战性的任务，需要感知、语言和背景知识系统之间的无缝交互。虽然最近视觉和自然语言模型的进展如BLIP已经提高了这项任务的性能，但我们缺乏理解这些模型在不同类型的问题和推理类型上表现的能力。鉴于我们对BLIP家族模型进行的初步分析显示出回答细节问题方面的困难，我们研究以下问题：能否使用视觉裁剪来提高最先进的视觉问答模型在细节问题上的性能？考虑到BLIP家族模型的最近成功，我们研究了一个零样本和一个微调的BLIP模型。我们定义了三个受控的子集，以衡量裁剪是否有助于模型性能。除了人工裁剪外，我们还设计了两种基于多模态嵌入的自动裁剪策略，这些策略以CLIP为基础。",
    "tldr": "本文研究如何通过使用视觉裁剪来提高BLIP模型在细节问题上的表现能力。"
}