{
    "title": "Empirical Risk Minimization with Shuffled SGD: A Primal-Dual Perspective and Improved Bounds. (arXiv:2306.12498v1 [math.OC])",
    "abstract": "Stochastic gradient descent (SGD) is perhaps the most prevalent optimization method in modern machine learning. Contrary to the empirical practice of sampling from the datasets without replacement and with (possible) reshuffling at each epoch, the theoretical counterpart of SGD usually relies on the assumption of sampling with replacement. It is only very recently that SGD with sampling without replacement -- shuffled SGD -- has been analyzed. For convex finite sum problems with $n$ components and under the $L$-smoothness assumption for each component function, there are matching upper and lower bounds, under sufficiently small -- $\\mathcal{O}(\\frac{1}{nL})$ -- step sizes. Yet those bounds appear too pessimistic -- in fact, the predicted performance is generally no better than for full gradient descent -- and do not agree with the empirical observations. In this work, to narrow the gap between the theory and practice of shuffled SGD, we sharpen the focus from general finite sum problem",
    "link": "http://arxiv.org/abs/2306.12498",
    "context": "Title: Empirical Risk Minimization with Shuffled SGD: A Primal-Dual Perspective and Improved Bounds. (arXiv:2306.12498v1 [math.OC])\nAbstract: Stochastic gradient descent (SGD) is perhaps the most prevalent optimization method in modern machine learning. Contrary to the empirical practice of sampling from the datasets without replacement and with (possible) reshuffling at each epoch, the theoretical counterpart of SGD usually relies on the assumption of sampling with replacement. It is only very recently that SGD with sampling without replacement -- shuffled SGD -- has been analyzed. For convex finite sum problems with $n$ components and under the $L$-smoothness assumption for each component function, there are matching upper and lower bounds, under sufficiently small -- $\\mathcal{O}(\\frac{1}{nL})$ -- step sizes. Yet those bounds appear too pessimistic -- in fact, the predicted performance is generally no better than for full gradient descent -- and do not agree with the empirical observations. In this work, to narrow the gap between the theory and practice of shuffled SGD, we sharpen the focus from general finite sum problem",
    "path": "papers/23/06/2306.12498.json",
    "total_tokens": 874,
    "translated_title": "带有Shuffled SGD的经验风险最小化：原始-对偶视角和改进界限",
    "translated_abstract": "随机梯度下降（SGD）是现代机器学习中最普遍的优化方法。与每个时期从数据集中无替换随机抽样和与（可能的）重排练的经验惯例相反，SGD的理论对应通常依赖于带替换的抽样假设。仅最近才分析了采用无替换抽样的Shuffled SGD。对于具有$n$个组件和对于每个组件函数$L$-平滑性假设的凸有限和问题，在足够小的步长（$\\mathcal{O}(\\frac{1}{nL})$）下，存在匹配的上下界。然而，这些界限似乎过于悲观 - 实际上，预测的性能通常不比全梯度下降更好 - 并且与经验观察不符。为了缩小理论和实践之间的差距，本文将焦点从一般有限和问题集中到了...",
    "tldr": "本文提出了带有Shuffled SGD的经验风险最小化的原始-对偶视角和改进界限，旨在解决理论和实践之间的差距。",
    "en_tdlr": "This paper presents a primal-dual perspective and improved bounds for empirical risk minimization with Shuffled SGD, aiming to narrow the gap between theory and practice."
}