{
    "title": "T3L: Translate-and-Test Transfer Learning for Cross-Lingual Text Classification. (arXiv:2306.04996v1 [cs.CL])",
    "abstract": "Cross-lingual text classification leverages text classifiers trained in a high-resource language to perform text classification in other languages with no or minimal fine-tuning (zero/few-shots cross-lingual transfer). Nowadays, cross-lingual text classifiers are typically built on large-scale, multilingual language models (LMs) pretrained on a variety of languages of interest. However, the performance of these models vary significantly across languages and classification tasks, suggesting that the superposition of the language modelling and classification tasks is not always effective. For this reason, in this paper we propose revisiting the classic \"translate-and-test\" pipeline to neatly separate the translation and classification stages. The proposed approach couples 1) a neural machine translator translating from the targeted language to a high-resource language, with 2) a text classifier trained in the high-resource language, but the neural machine translator generates \"soft\" tran",
    "link": "http://arxiv.org/abs/2306.04996",
    "context": "Title: T3L: Translate-and-Test Transfer Learning for Cross-Lingual Text Classification. (arXiv:2306.04996v1 [cs.CL])\nAbstract: Cross-lingual text classification leverages text classifiers trained in a high-resource language to perform text classification in other languages with no or minimal fine-tuning (zero/few-shots cross-lingual transfer). Nowadays, cross-lingual text classifiers are typically built on large-scale, multilingual language models (LMs) pretrained on a variety of languages of interest. However, the performance of these models vary significantly across languages and classification tasks, suggesting that the superposition of the language modelling and classification tasks is not always effective. For this reason, in this paper we propose revisiting the classic \"translate-and-test\" pipeline to neatly separate the translation and classification stages. The proposed approach couples 1) a neural machine translator translating from the targeted language to a high-resource language, with 2) a text classifier trained in the high-resource language, but the neural machine translator generates \"soft\" tran",
    "path": "papers/23/06/2306.04996.json",
    "total_tokens": 1095,
    "translated_title": "T3L: 翻译-测试迁移学习用于跨语言文本分类",
    "translated_abstract": "跨语言文本分类利用在高资源语言上训练的文本分类器来执行在其他语言上的文本分类，而无需或只需要微调（零/少量关键词跨语言转移）。如今，跨语言文本分类器通常基于预先在多种感兴趣的语言上进行预训练的大规模多语言语言模型（LM）来构建。然而，这些模型在不同的语言和分类任务上的表现有很大的差异，这表明语言建模和分类任务的叠加并不总是有效的。因此，在本文中，我们提议重新审视经典的“翻译-测试”流程，以清晰地分离翻译和分类阶段。所提出的方法将1）神经机器翻译器将目标语言翻译成高资源语言，和2）在高资源语言上训练的文本分类器相结合，但神经机器翻译器生成“软”翻译以保留原始目标语言的特征。我们在十种语言的多样化跨语言文本分类基准测试中评估了我们的方法。结果显示，所提出的方法在零关键词设置下的大部分数据集上胜过领先的基线框架，并且在大多数数据集上，在监督和少量关键词设置下也实现了最先进的性能。",
    "tldr": "本文提出了一种翻译-测试的迁移学习方法以用于跨语言文本分类，在神经机器翻译器的帮助下将目标语言翻译成高资源语言来训练文本分类器，并取得了比基线更好的结果。",
    "en_tdlr": "This paper proposes a Translate-and-Test transfer learning approach for cross-lingual text classification, which uses a neural machine translator to translate the target language to a high-resource language, and achieves better performance than the baseline by separating the translation and classification stages."
}