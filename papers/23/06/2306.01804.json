{
    "title": "Extracting Reward Functions from Diffusion Models. (arXiv:2306.01804v1 [cs.LG])",
    "abstract": "Diffusion models have achieved remarkable results in image generation, and have similarly been used to learn high-performing policies in sequential decision-making tasks. Decision-making diffusion models can be trained on lower-quality data, and then be steered with a reward function to generate near-optimal trajectories. We consider the problem of extracting a reward function by comparing a decision-making diffusion model that models low-reward behavior and one that models high-reward behavior; a setting related to inverse reinforcement learning. We first define the notion of a relative reward function of two diffusion models and show conditions under which it exists and is unique. We then devise a practical learning algorithm for extracting it by aligning the gradients of a reward function -- parametrized by a neural network -- to the difference in outputs of both diffusion models. Our method finds correct reward functions in navigation environments, and we demonstrate that steering ",
    "link": "http://arxiv.org/abs/2306.01804",
    "context": "Title: Extracting Reward Functions from Diffusion Models. (arXiv:2306.01804v1 [cs.LG])\nAbstract: Diffusion models have achieved remarkable results in image generation, and have similarly been used to learn high-performing policies in sequential decision-making tasks. Decision-making diffusion models can be trained on lower-quality data, and then be steered with a reward function to generate near-optimal trajectories. We consider the problem of extracting a reward function by comparing a decision-making diffusion model that models low-reward behavior and one that models high-reward behavior; a setting related to inverse reinforcement learning. We first define the notion of a relative reward function of two diffusion models and show conditions under which it exists and is unique. We then devise a practical learning algorithm for extracting it by aligning the gradients of a reward function -- parametrized by a neural network -- to the difference in outputs of both diffusion models. Our method finds correct reward functions in navigation environments, and we demonstrate that steering ",
    "path": "papers/23/06/2306.01804.json",
    "total_tokens": 1077,
    "translated_title": "从Diffusion模型中提取奖励函数",
    "translated_abstract": "Diffusion模型在图像生成方面取得了显著成果，也被用于学习序列决策任务中的高性能策略。我们考虑通过比较建模低奖励行为和建模高奖励行为的决策传播模型来提取奖励函数的问题；这与逆强化学习相关。我们设计了一种实用的学习算法，通过将神经网络参数化的奖励函数的梯度与两个Diffusion模型的输出差异对齐来提取奖励函数。我们的方法可以在导航环境中找到正确的奖励函数，并且表明可以通过控制Diffusion模型来学习足够复杂的任务。",
    "tldr": "本论文提出了一种从两个Diffusion模型中提取奖励函数的实用学习算法，可以在导航环境中找到正确的奖励函数，并以此控制Diffusion模型学习足够复杂的任务。",
    "en_tdlr": "This paper proposes a practical learning algorithm for extracting reward functions from two decision-making diffusion models, which can find correct reward functions in navigation environments and use them to control diffusion models to learn sufficiently complex tasks."
}