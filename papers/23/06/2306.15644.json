{
    "title": "Style-transfer based Speech and Audio-visual Scene Understanding for Robot Action Sequence Acquisition from Videos. (arXiv:2306.15644v1 [cs.CL])",
    "abstract": "To realize human-robot collaboration, robots need to execute actions for new tasks according to human instructions given finite prior knowledge. Human experts can share their knowledge of how to perform a task with a robot through multi-modal instructions in their demonstrations, showing a sequence of short-horizon steps to achieve a long-horizon goal. This paper introduces a method for robot action sequence generation from instruction videos using (1) an audio-visual Transformer that converts audio-visual features and instruction speech to a sequence of robot actions called dynamic movement primitives (DMPs) and (2) style-transfer-based training that employs multi-task learning with video captioning and weakly-supervised learning with a semantic classifier to exploit unpaired video-action data. We built a system that accomplishes various cooking actions, where an arm robot executes a DMP sequence acquired from a cooking video using the audio-visual Transformer. Experiments with Epic-K",
    "link": "http://arxiv.org/abs/2306.15644",
    "context": "Title: Style-transfer based Speech and Audio-visual Scene Understanding for Robot Action Sequence Acquisition from Videos. (arXiv:2306.15644v1 [cs.CL])\nAbstract: To realize human-robot collaboration, robots need to execute actions for new tasks according to human instructions given finite prior knowledge. Human experts can share their knowledge of how to perform a task with a robot through multi-modal instructions in their demonstrations, showing a sequence of short-horizon steps to achieve a long-horizon goal. This paper introduces a method for robot action sequence generation from instruction videos using (1) an audio-visual Transformer that converts audio-visual features and instruction speech to a sequence of robot actions called dynamic movement primitives (DMPs) and (2) style-transfer-based training that employs multi-task learning with video captioning and weakly-supervised learning with a semantic classifier to exploit unpaired video-action data. We built a system that accomplishes various cooking actions, where an arm robot executes a DMP sequence acquired from a cooking video using the audio-visual Transformer. Experiments with Epic-K",
    "path": "papers/23/06/2306.15644.json",
    "total_tokens": 874,
    "translated_title": "基于风格转移的语音与视听场景理解方法用于从视频中获取机器人动作序列",
    "translated_abstract": "为了实现人机协作，机器人需要根据有限的先验知识执行新任务的动作，根据人类的指令执行。人类专家可以通过多模态指令的演示向机器人分享执行任务的知识，展示一系列短暂步骤来实现一个长期目标。本文介绍了一种从指令视频中生成机器人动作序列的方法，使用(1)音视Transformer将音视特征和指令语音转换为称为动态运动原理(DMPs)的机器人动作序列，以及(2)基于风格转移的训练，利用视频字幕和语义分类器进行多任务学习和弱监督学习，以利用不配对的视频动作数据。我们构建了一个系统，实现了各种烹饪动作，其中一个臂式机器人使用音视Transformer执行从烹饪视频中获取的DMP序列。在Epic-K实验中进行了实验。",
    "tldr": "本文介绍了一种从指令视频中生成机器人动作序列的方法，使用音视Transformer将音视特征和指令语音转换为机器人动作序列，并利用基于风格转移的训练来提高模型的性能。"
}