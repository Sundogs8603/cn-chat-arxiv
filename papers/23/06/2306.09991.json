{
    "title": "Evolutionary Algorithms in the Light of SGD: Limit Equivalence, Minima Flatness, and Transfer Learning. (arXiv:2306.09991v1 [cs.NE])",
    "abstract": "Whenever applicable, the Stochastic Gradient Descent (SGD) has shown itself to be unreasonably effective. Instead of underperforming and getting trapped in local minima due to the batch noise, SGD leverages it to learn to generalize better and find minima that are good enough for the entire dataset. This led to numerous theoretical and experimental investigations, especially in the context of Artificial Neural Networks (ANNs), leading to better machine learning algorithms. However, SGD is not applicable in a non-differentiable setting, leaving all that prior research off the table.  In this paper, we show that a class of evolutionary algorithms (EAs) inspired by the Gillespie-Orr Mutational Landscapes model for natural evolution is formally equivalent to SGD in certain settings and, in practice, is well adapted to large ANNs. We refer to such EAs as Gillespie-Orr EA class (GO-EAs) and empirically show how an insight transfer from SGD can work for them. We then show that for ANNs traine",
    "link": "http://arxiv.org/abs/2306.09991",
    "context": "Title: Evolutionary Algorithms in the Light of SGD: Limit Equivalence, Minima Flatness, and Transfer Learning. (arXiv:2306.09991v1 [cs.NE])\nAbstract: Whenever applicable, the Stochastic Gradient Descent (SGD) has shown itself to be unreasonably effective. Instead of underperforming and getting trapped in local minima due to the batch noise, SGD leverages it to learn to generalize better and find minima that are good enough for the entire dataset. This led to numerous theoretical and experimental investigations, especially in the context of Artificial Neural Networks (ANNs), leading to better machine learning algorithms. However, SGD is not applicable in a non-differentiable setting, leaving all that prior research off the table.  In this paper, we show that a class of evolutionary algorithms (EAs) inspired by the Gillespie-Orr Mutational Landscapes model for natural evolution is formally equivalent to SGD in certain settings and, in practice, is well adapted to large ANNs. We refer to such EAs as Gillespie-Orr EA class (GO-EAs) and empirically show how an insight transfer from SGD can work for them. We then show that for ANNs traine",
    "path": "papers/23/06/2306.09991.json",
    "total_tokens": 1073,
    "translated_abstract": "每当适用时，随机梯度下降（SGD）总是表现得异常出色。SGD不同于批量噪声降低性能并陷入局部最小值，而是利用噪声学习，以获得更好的泛化效果和找到适用于整个数据集的极小值。这导致了大量理论和实验研究，特别是在人工神经网络（ANN）的背景下，进而促进了更好的机器学习算法。然而，SGD在不可微分的情况下无法应用，使先前所有研究都无效。在本文中，我们展示了一类受自然进化的Gillespie-Orr突变景观模型启发的进化算法（EA），在某些设置下与SGD正式等价，并且实际上适用于大型ANN。我们将这类EA称为Gillespie-Orr EA类（GO-EA），并通过实验证明了SGD思想迁移如何适用于它们。然后，我们展示了在使用GO-EA训练ANN后，我们可以证明ANN的最小值更加平坦，这也是SGD算法中的一个关键优势。",
    "tldr": "本文介绍了一类启发自自然进化的Gillespie-Orr突变景观模型的进化算法（EA），在某些设置下与SGD正式等价，并且在大型ANN上实际适用。这些EA被称为Gillespie-Orr EA类（GO-EA），SGD思想适用于GO-EA，可以证明ANN的最小值更加平坦。",
    "en_tdlr": "This paper introduces a class of evolutionary algorithms (EA) inspired by the Gillespie-Orr Mutational Landscapes model that is formally equivalent to SGD in certain settings and effectively applicable on large ANNs. The EA is referred to as the Gillespie-Orr EA class (GO-EA), SGD ideas are transferrable to them, and it is shown that the minimum of ANN is flatter when trained using GO-EA, which is a key advantage of the SGD algorithm."
}