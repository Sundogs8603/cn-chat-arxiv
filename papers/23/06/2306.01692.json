{
    "title": "Uniform Convergence of Deep Neural Networks with Lipschitz Continuous Activation Functions and Variable Widths. (arXiv:2306.01692v1 [cs.LG])",
    "abstract": "We consider deep neural networks with a Lipschitz continuous activation function and with weight matrices of variable widths. We establish a uniform convergence analysis framework in which sufficient conditions on weight matrices and bias vectors together with the Lipschitz constant are provided to ensure uniform convergence of the deep neural networks to a meaningful function as the number of their layers tends to infinity. In the framework, special results on uniform convergence of deep neural networks with a fixed width, bounded widths and unbounded widths are presented. In particular, as convolutional neural networks are special deep neural networks with weight matrices of increasing widths, we put forward conditions on the mask sequence which lead to uniform convergence of resulting convolutional neural networks. The Lipschitz continuity assumption on the activation functions allows us to include in our theory most of commonly used activation functions in applications.",
    "link": "http://arxiv.org/abs/2306.01692",
    "context": "Title: Uniform Convergence of Deep Neural Networks with Lipschitz Continuous Activation Functions and Variable Widths. (arXiv:2306.01692v1 [cs.LG])\nAbstract: We consider deep neural networks with a Lipschitz continuous activation function and with weight matrices of variable widths. We establish a uniform convergence analysis framework in which sufficient conditions on weight matrices and bias vectors together with the Lipschitz constant are provided to ensure uniform convergence of the deep neural networks to a meaningful function as the number of their layers tends to infinity. In the framework, special results on uniform convergence of deep neural networks with a fixed width, bounded widths and unbounded widths are presented. In particular, as convolutional neural networks are special deep neural networks with weight matrices of increasing widths, we put forward conditions on the mask sequence which lead to uniform convergence of resulting convolutional neural networks. The Lipschitz continuity assumption on the activation functions allows us to include in our theory most of commonly used activation functions in applications.",
    "path": "papers/23/06/2306.01692.json",
    "total_tokens": 938,
    "translated_title": "具有Lipschitz连续激活函数和可变宽度的深度神经网络的统一收敛性分析",
    "translated_abstract": "本文考虑了具有Lipschitz连续激活函数和可变宽度的深度神经网络。我们建立了一个统一的收敛分析框架，其中提供了关于权重矩阵和偏置向量的充分条件以及Lipschitz常数，以确保深度神经网络在层数趋近于无穷大时对一个有意义的函数的统一收敛。在框架中，我们特别提出了针对具有固定宽度、有界宽度和无界宽度的深度神经网络的统一收敛性结果。特别是，由于卷积神经网络是具有不断增加的权重矩阵的特殊深度神经网络，我们提出了关于掩码序列的条件，从而导致所得卷积神经网络的统一收敛。激活函数的Lipschitz连续性假设允许我们在我们的理论中包括大多数应用中常用的激活函数。",
    "tldr": "本文提出了针对具有Lipschitz连续激活函数和可变宽度的深度神经网络的统一收敛性分析，给出了充分条件以及Lipschitz常数，建立了相应的统一收敛性结果。"
}