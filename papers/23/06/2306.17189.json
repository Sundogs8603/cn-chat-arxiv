{
    "title": "Steganographic Capacity of Deep Learning Models. (arXiv:2306.17189v1 [cs.CR])",
    "abstract": "As machine learning and deep learning models become ubiquitous, it is inevitable that there will be attempts to exploit such models in various attack scenarios. For example, in a steganographic-based attack, information could be hidden in a learning model, which might then be used to distribute malware, or for other malicious purposes. In this research, we consider the steganographic capacity of several learning models. Specifically, we train a Multilayer Perceptron (MLP), Convolutional Neural Network (CNN), and Transformer model on a challenging malware classification problem. For each of the resulting models, we determine the number of low-order bits of the trained parameters that can be altered without significantly affecting the performance of the model. We find that the steganographic capacity of the learning models tested is surprisingly high, and that in each case, there is a clear threshold after which model performance rapidly degrades.",
    "link": "http://arxiv.org/abs/2306.17189",
    "context": "Title: Steganographic Capacity of Deep Learning Models. (arXiv:2306.17189v1 [cs.CR])\nAbstract: As machine learning and deep learning models become ubiquitous, it is inevitable that there will be attempts to exploit such models in various attack scenarios. For example, in a steganographic-based attack, information could be hidden in a learning model, which might then be used to distribute malware, or for other malicious purposes. In this research, we consider the steganographic capacity of several learning models. Specifically, we train a Multilayer Perceptron (MLP), Convolutional Neural Network (CNN), and Transformer model on a challenging malware classification problem. For each of the resulting models, we determine the number of low-order bits of the trained parameters that can be altered without significantly affecting the performance of the model. We find that the steganographic capacity of the learning models tested is surprisingly high, and that in each case, there is a clear threshold after which model performance rapidly degrades.",
    "path": "papers/23/06/2306.17189.json",
    "total_tokens": 855,
    "translated_title": "深度学习模型的隐写能力研究",
    "translated_abstract": "随着机器学习和深度学习模型的普遍应用，必然会有人试图在各种攻击场景中利用这些模型。例如，在一种基于隐写的攻击中，信息可以被隐藏在一个学习模型中，然后用于分发恶意软件或其他恶意目的。本研究考虑了几个学习模型的隐写能力。具体来说，我们对一个具有挑战性的恶意软件分类问题训练了一个多层感知器（MLP），卷积神经网络（CNN）和Transformer模型。对于每个训练得到的模型，我们确定了在不显著影响模型性能的情况下可以改变的训练参数的低位比特数。我们发现，被测试的学习模型的隐写能力出乎意料地高，并且在每种情况下，存在一个明显的门限，门限之后模型性能迅速下降。",
    "tldr": "本研究考虑了几个学习模型的隐写能力，并发现这些模型的隐写能力非常高，且存在一个明显的门限，门限之后模型性能迅速下降。",
    "en_tdlr": "This research investigates the steganographic capacity of several learning models and finds that these models have surprisingly high steganographic capacity, with a clear threshold after which the model performance rapidly degrades."
}