{
    "title": "A Reference-less Quality Metric for Automatic Speech Recognition via Contrastive-Learning of a Multi-Language Model with Self-Supervision. (arXiv:2306.13114v1 [cs.CL])",
    "abstract": "The common standard for quality evaluation of automatic speech recognition (ASR) systems is reference-based metrics such as the Word Error Rate (WER), computed using manual ground-truth transcriptions that are time-consuming and expensive to obtain. This work proposes a multi-language referenceless quality metric, which allows comparing the performance of different ASR models on a speech dataset without ground truth transcriptions. To estimate the quality of ASR hypotheses, a pre-trained language model (LM) is fine-tuned with contrastive learning in a self-supervised learning manner. In experiments conducted on several unseen test datasets consisting of outputs from top commercial ASR engines in various languages, the proposed referenceless metric obtains a much higher correlation with WER scores and their ranks than the perplexity metric from the state-of-art multi-lingual LM in all experiments, and also reduces WER by more than $7\\%$ when used for ensembling hypotheses. The fine-tune",
    "link": "http://arxiv.org/abs/2306.13114",
    "context": "Title: A Reference-less Quality Metric for Automatic Speech Recognition via Contrastive-Learning of a Multi-Language Model with Self-Supervision. (arXiv:2306.13114v1 [cs.CL])\nAbstract: The common standard for quality evaluation of automatic speech recognition (ASR) systems is reference-based metrics such as the Word Error Rate (WER), computed using manual ground-truth transcriptions that are time-consuming and expensive to obtain. This work proposes a multi-language referenceless quality metric, which allows comparing the performance of different ASR models on a speech dataset without ground truth transcriptions. To estimate the quality of ASR hypotheses, a pre-trained language model (LM) is fine-tuned with contrastive learning in a self-supervised learning manner. In experiments conducted on several unseen test datasets consisting of outputs from top commercial ASR engines in various languages, the proposed referenceless metric obtains a much higher correlation with WER scores and their ranks than the perplexity metric from the state-of-art multi-lingual LM in all experiments, and also reduces WER by more than $7\\%$ when used for ensembling hypotheses. The fine-tune",
    "path": "papers/23/06/2306.13114.json",
    "total_tokens": 917,
    "translated_title": "一种基于反差学习和自监督的多语言模型的无参考自动语音识别质量评价指标",
    "translated_abstract": "自动语音识别（ASR）系统的质量评价通常采用基于参考的指标，如使用耗时且昂贵的手工真实转录来计算的词错误率（WER）。本文提出了一种多语言无参考质量评价指标，可在没有真实转录的情况下比较不同ASR模型在语音数据集上的性能。为了估计ASR假设的质量，使用预训练的语言模型（LM）以自监督学习方式进行反差学习的微调。在对多个未见过的测试数据集进行的实验中，所提出的无参考指标在所有实验中都比最先进的多语言LM的困惑度指标获得了更高的与WER得分及其排名的相关性，并且在用于合并假设时可将WER降低超过7％。",
    "tldr": "本文提出了一种基于反差学习和自监督的多语言模型的无参考质量评价指标，可以在没有真实转录的情况下比较不同ASR模型在语音数据集上的性能，并可将WER降低超过7％。",
    "en_tdlr": "This paper proposes a reference-less quality metric for automatic speech recognition using a pre-trained language model fine-tuned with contrastive learning in a self-supervised manner. The proposed metric outperforms the state-of-the-art multi-lingual LM's perplexity metric in correlation with WER scores and reduces WER by more than 7% when used for ensembling hypotheses."
}