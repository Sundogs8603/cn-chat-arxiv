{
    "title": "Learning Any-View 6DoF Robotic Grasping in Cluttered Scenes via Neural Surface Rendering. (arXiv:2306.07392v1 [cs.RO])",
    "abstract": "Robotic manipulation is critical for admitting robotic agents to various application domains, like intelligent assistance. A major challenge therein is the effective 6DoF grasping of objects in cluttered environments from any viewpoint without requiring additional scene exploration. We introduce $\\textit{NeuGraspNet}$, a novel method for 6DoF grasp detection that leverages recent advances in neural volumetric representations and surface rendering. Our approach learns both global (scene-level) and local (grasp-level) neural surface representations, enabling effective and fully implicit 6DoF grasp quality prediction, even in unseen parts of the scene. Further, we reinterpret grasping as a local neural surface rendering problem, allowing the model to encode the interaction between the robot's end-effector and the object's surface geometry. NeuGraspNet operates on single viewpoints and can sample grasp candidates in occluded scenes, outperforming existing implicit and semi-implicit baselin",
    "link": "http://arxiv.org/abs/2306.07392",
    "context": "Title: Learning Any-View 6DoF Robotic Grasping in Cluttered Scenes via Neural Surface Rendering. (arXiv:2306.07392v1 [cs.RO])\nAbstract: Robotic manipulation is critical for admitting robotic agents to various application domains, like intelligent assistance. A major challenge therein is the effective 6DoF grasping of objects in cluttered environments from any viewpoint without requiring additional scene exploration. We introduce $\\textit{NeuGraspNet}$, a novel method for 6DoF grasp detection that leverages recent advances in neural volumetric representations and surface rendering. Our approach learns both global (scene-level) and local (grasp-level) neural surface representations, enabling effective and fully implicit 6DoF grasp quality prediction, even in unseen parts of the scene. Further, we reinterpret grasping as a local neural surface rendering problem, allowing the model to encode the interaction between the robot's end-effector and the object's surface geometry. NeuGraspNet operates on single viewpoints and can sample grasp candidates in occluded scenes, outperforming existing implicit and semi-implicit baselin",
    "path": "papers/23/06/2306.07392.json",
    "total_tokens": 908,
    "translated_title": "通过神经表面渲染，在混乱场景中学习任意视角的6DoF机器人抓取",
    "translated_abstract": "机器人操作在智能辅助等各种应用领域中至关重要。其中一个主要挑战是在杂乱的环境中从任何视角有效地抓取对象，而不需要额外的场景探索。我们引入了NeuGraspNet，一种新颖的6DoF抓取检测方法，利用了神经体积表示和表面渲染的最新进展。我们的方法学习了全局（场景级别）和局部（抓取级别）神经表面表示，使得即使在场景的未见部分，也能有效地预测6DoF抓取质量。此外，我们将抓取重新解释为一个局部的神经表面渲染问题，使得模型能够编码机器人末端执行器和对象表面几何之间的交互。NeuGraspNet在单个视角上运行，并且可以在遮挡的场景中采样抓取候选项，表现出优于现有隐式和半隐式基线模型的性能。",
    "tldr": "通过神经表面渲染，NeuGraspNet能够在混乱场景中有效地从任意视角预测6DoF抓取质量，并能够在遮挡的场景中采样抓取候选项。",
    "en_tdlr": "NeuGraspNet utilizes neural surface rendering to predict 6DoF grasp quality from any viewpoint in cluttered scenes, and can sample grasp candidates in occluded scenes."
}