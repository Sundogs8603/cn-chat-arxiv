{
    "title": "Benchmarking Large Language Model Capabilities for Conditional Generation. (arXiv:2306.16793v1 [cs.CL])",
    "abstract": "Pre-trained large language models (PLMs) underlie most new developments in natural language processing. They have shifted the field from application-specific model pipelines to a single model that is adapted to a wide range of tasks. Autoregressive PLMs like GPT-3 or PaLM, alongside techniques like few-shot learning, have additionally shifted the output modality to generation instead of classification or regression. Despite their ubiquitous use, the generation quality of language models is rarely evaluated when these models are introduced. Additionally, it is unclear how existing generation tasks--while they can be used to compare systems at a high level--relate to the real world use cases for which people have been adopting them. In this work, we discuss how to adapt existing application-specific generation benchmarks to PLMs and provide an in-depth, empirical study of the limitations and capabilities of PLMs in natural language generation tasks along dimensions such as scale, archite",
    "link": "http://arxiv.org/abs/2306.16793",
    "context": "Title: Benchmarking Large Language Model Capabilities for Conditional Generation. (arXiv:2306.16793v1 [cs.CL])\nAbstract: Pre-trained large language models (PLMs) underlie most new developments in natural language processing. They have shifted the field from application-specific model pipelines to a single model that is adapted to a wide range of tasks. Autoregressive PLMs like GPT-3 or PaLM, alongside techniques like few-shot learning, have additionally shifted the output modality to generation instead of classification or regression. Despite their ubiquitous use, the generation quality of language models is rarely evaluated when these models are introduced. Additionally, it is unclear how existing generation tasks--while they can be used to compare systems at a high level--relate to the real world use cases for which people have been adopting them. In this work, we discuss how to adapt existing application-specific generation benchmarks to PLMs and provide an in-depth, empirical study of the limitations and capabilities of PLMs in natural language generation tasks along dimensions such as scale, archite",
    "path": "papers/23/06/2306.16793.json",
    "total_tokens": 824,
    "translated_title": "测评大型语言模型在条件生成中的能力",
    "translated_abstract": "预训练的大型语言模型 (PLMs) 是自然语言处理中大多数新发展的基础。它们将该领域从应用特定的模型流程转变为一个适应各种任务的单一模型。与分类或回归不同，自回归 PLMs（例如GPT-3或PaLM）以及少样本学习等技术将输出方式进一步转变为生成。尽管它们被广泛使用，但在引入这些模型时很少对语言模型的生成质量进行评估。此外，目前还不清楚现有的生成任务——尽管可以用于比较系统——如何与人们采用它们的真实世界应用场景相关联。在这项工作中，我们讨论如何将现有的应用特定生成基准适应PLMs，并对PLMs在自然语言生成任务中的限制和能力进行了深入的经验研究，包括规模、架构等维度。",
    "tldr": "本文测评了大型语言模型在条件生成中的能力，并讨论了其生成质量的评估和与真实世界应用场景的关联性。",
    "en_tdlr": "This paper benchmarks the capabilities of large language models in conditional generation and discusses the evaluation of generation quality and its relevance to real-world application scenarios."
}