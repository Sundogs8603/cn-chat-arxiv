{
    "title": "Incorporating Graph Information in Transformer-based AMR Parsing. (arXiv:2306.13467v1 [cs.CL])",
    "abstract": "Abstract Meaning Representation (AMR) is a Semantic Parsing formalism that aims at providing a semantic graph abstraction representing a given text. Current approaches are based on autoregressive language models such as BART or T5, fine-tuned through Teacher Forcing to obtain a linearized version of the AMR graph from a sentence. In this paper, we present LeakDistill, a model and method that explores a modification to the Transformer architecture, using structural adapters to explicitly incorporate graph information into the learned representations and improve AMR parsing performance. Our experiments show how, by employing word-to-node alignment to embed graph structural information into the encoder at training time, we can obtain state-of-the-art AMR parsing through self-knowledge distillation, even without the use of additional data. We release the code at \\url{this http URL}.",
    "link": "http://arxiv.org/abs/2306.13467",
    "context": "Title: Incorporating Graph Information in Transformer-based AMR Parsing. (arXiv:2306.13467v1 [cs.CL])\nAbstract: Abstract Meaning Representation (AMR) is a Semantic Parsing formalism that aims at providing a semantic graph abstraction representing a given text. Current approaches are based on autoregressive language models such as BART or T5, fine-tuned through Teacher Forcing to obtain a linearized version of the AMR graph from a sentence. In this paper, we present LeakDistill, a model and method that explores a modification to the Transformer architecture, using structural adapters to explicitly incorporate graph information into the learned representations and improve AMR parsing performance. Our experiments show how, by employing word-to-node alignment to embed graph structural information into the encoder at training time, we can obtain state-of-the-art AMR parsing through self-knowledge distillation, even without the use of additional data. We release the code at \\url{this http URL}.",
    "path": "papers/23/06/2306.13467.json",
    "total_tokens": 906,
    "translated_title": "将图表信息融入基于Transformer的AMR分析",
    "translated_abstract": "抽象意义表达（AMR）是一种语义解析形式主义，旨在提供表示给定文本的语义图表抽象。当前的方法基于自回归语言模型，例如BART或T5，通过Teacher Forcing进行微调，以从句子得到线性化版本的AMR图表。在本文中，我们提出了LeakDistill，一种探索转换器架构修改的模型和方法，使用结构适配器明确将图形信息并入到学习的表示中，以提高AMR解析性能。我们的实验表明，通过在训练时使用单词到节点对齐将图形结构信息嵌入编码器中，即使不使用其他数据，也可以通过自我知识蒸馏获得最先进的AMR解析性能。我们在\\url{this http URL}上发布了代码。",
    "tldr": "本论文介绍了一种新的模型和方法LeakDistill，它使用结构适配器将图形信息明确并入到学习的表示中，从而提高了AMR解析性能。实验表明，我们可以通过在训练时使用单词到节点对齐将图形结构信息嵌入编码器中，即使不使用其他数据，也可以通过自我知识蒸馏获得最先进的AMR解析性能。",
    "en_tdlr": "This paper introduces a novel model and method, LeakDistill, that uses structural adapters to explicitly incorporate graph information into the learned representations and improve AMR parsing performance. Experiments show that state-of-the-art AMR parsing can be achieved through self-knowledge distillation by embedding graph structural information into the encoder at training time using word-to-node alignment, even without the use of additional data. Code is released at \\url{this http URL}."
}