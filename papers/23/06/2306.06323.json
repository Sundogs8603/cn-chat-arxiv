{
    "title": "Learning Joint Latent Space EBM Prior Model for Multi-layer Generator. (arXiv:2306.06323v1 [cs.CV])",
    "abstract": "This paper studies the fundamental problem of learning multi-layer generator models. The multi-layer generator model builds multiple layers of latent variables as a prior model on top of the generator, which benefits learning complex data distribution and hierarchical representations. However, such a prior model usually focuses on modeling inter-layer relations between latent variables by assuming non-informative (conditional) Gaussian distributions, which can be limited in model expressivity. To tackle this issue and learn more expressive prior models, we propose an energy-based model (EBM) on the joint latent space over all layers of latent variables with the multi-layer generator as its backbone. Such joint latent space EBM prior model captures the intra-layer contextual relations at each layer through layer-wise energy terms, and latent variables across different layers are jointly corrected. We develop a joint training scheme via maximum likelihood estimation (MLE), which involves",
    "link": "http://arxiv.org/abs/2306.06323",
    "context": "Title: Learning Joint Latent Space EBM Prior Model for Multi-layer Generator. (arXiv:2306.06323v1 [cs.CV])\nAbstract: This paper studies the fundamental problem of learning multi-layer generator models. The multi-layer generator model builds multiple layers of latent variables as a prior model on top of the generator, which benefits learning complex data distribution and hierarchical representations. However, such a prior model usually focuses on modeling inter-layer relations between latent variables by assuming non-informative (conditional) Gaussian distributions, which can be limited in model expressivity. To tackle this issue and learn more expressive prior models, we propose an energy-based model (EBM) on the joint latent space over all layers of latent variables with the multi-layer generator as its backbone. Such joint latent space EBM prior model captures the intra-layer contextual relations at each layer through layer-wise energy terms, and latent variables across different layers are jointly corrected. We develop a joint training scheme via maximum likelihood estimation (MLE), which involves",
    "path": "papers/23/06/2306.06323.json",
    "total_tokens": 1021,
    "translated_title": "学习多层生成器的联合潜在空间EBM先验模型",
    "translated_abstract": "本文研究了学习多层生成器模型的基本问题。多层生成器模型在生成器之上构建多层潜在变量作为先验模型，有利于学习复杂的数据分布和分层表示。然而，这样的先验模型通常通过假设非信息（条件）高斯分布来专注于建模潜在变量之间的层间关系，并且在模型表达能力方面存在局限性。为了解决这个问题并学习更具表现力的先验模型，我们在所有潜在变量的联合潜在空间上提出了一种以多层生成器为骨干的能量基模型（EBM）的先验模型。这种联合潜在空间EBM 先验模型通过层间能量项捕获每层内的内部关系，并对不同层的潜在变量进行联合修正。我们通过最大似然估计开发了一种联合训练方案，其中同时学习生成器和EBM先验模型。在生成图像建模和修复任务上的实验表明，与几个基线相比，我们提出的方法可以学习更具表现力和可解释性的先验模型，从而实现更好的生成质量和修复结果。",
    "tldr": "本文研究了学习多层生成器模型的基本问题，并提出了一种以多层生成器为骨干的联合潜在空间EBM先验模型，该模型可以更好地学习复杂的数据分布和分层表示，实现更好的生成质量和修复结果。",
    "en_tdlr": "This paper proposes a joint latent space EBM prior model for multi-layer generator, which captures intra-layer contextual relations and jointly corrects latent variables across different layers, leading to better generation quality and inpainting results."
}