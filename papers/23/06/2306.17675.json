{
    "title": "Multimodal Prompt Retrieval for Generative Visual Question Answering. (arXiv:2306.17675v1 [cs.CV])",
    "abstract": "Recent years have witnessed impressive results of pre-trained vision-language models on knowledge-intensive tasks such as visual question answering (VQA). Despite the recent advances in VQA, existing methods mainly adopt a discriminative formulation that predicts answers within a pre-defined label set, leading to easy overfitting on low-resource domains with limited labeled data (e.g., medicine) and poor generalization under domain shift to another dataset. To tackle this limitation, we propose a novel generative model enhanced by multimodal prompt retrieval (MPR) that integrates retrieved prompts and multimodal features to generate answers in free text. Our generative model enables rapid zero-shot dataset adaptation to unseen data distributions and open-set answer labels across datasets. Our experiments on medical VQA tasks show that MPR outperforms its non-retrieval counterpart by up to 30% accuracy points in a few-shot domain adaptation setting.",
    "link": "http://arxiv.org/abs/2306.17675",
    "context": "Title: Multimodal Prompt Retrieval for Generative Visual Question Answering. (arXiv:2306.17675v1 [cs.CV])\nAbstract: Recent years have witnessed impressive results of pre-trained vision-language models on knowledge-intensive tasks such as visual question answering (VQA). Despite the recent advances in VQA, existing methods mainly adopt a discriminative formulation that predicts answers within a pre-defined label set, leading to easy overfitting on low-resource domains with limited labeled data (e.g., medicine) and poor generalization under domain shift to another dataset. To tackle this limitation, we propose a novel generative model enhanced by multimodal prompt retrieval (MPR) that integrates retrieved prompts and multimodal features to generate answers in free text. Our generative model enables rapid zero-shot dataset adaptation to unseen data distributions and open-set answer labels across datasets. Our experiments on medical VQA tasks show that MPR outperforms its non-retrieval counterpart by up to 30% accuracy points in a few-shot domain adaptation setting.",
    "path": "papers/23/06/2306.17675.json",
    "total_tokens": 959,
    "translated_title": "多模态提示检索用于生成式视觉问答",
    "translated_abstract": "近年来，预训练的视觉语言模型在知识密集型任务（如视觉问答）上取得了令人瞩目的成果。尽管在视觉问答方面已取得了一些进展，但现有方法主要采用判别式公式，在预定义的标签集内预测答案，导致在有限标注数据（如医学领域）的低资源领域容易过拟合，且在领域转移到另一个数据集时泛化能力较差。为了解决这个问题，我们提出了一种新颖的生成模型，通过多模态提示检索（MPR）来增强，它将检索到的提示和多模态特征结合起来以生成自由文本答案。我们的生成模型能够快速进行零样本数据集自适应，适应未见过的数据分布和跨数据集的开放式答案标签。我们在医学视觉问答任务上的实验证明，在少样本领域自适应设置中，MPR在准确率上优于非检索模型最多30个百分点。",
    "tldr": "本研究提出了一种基于多模态提示检索的生成式视觉问答模型，通过整合检索到的提示和多模态特征，实现了在自由文本中生成答案。该模型具有快速零样本自适应能力，并在医学视觉问答任务中比非检索模型高出30%的准确率。"
}