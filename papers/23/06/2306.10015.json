{
    "title": "Just One Byte (per gradient): A Note on Low-Bandwidth Decentralized Language Model Finetuning Using Shared Randomness. (arXiv:2306.10015v1 [cs.LG])",
    "abstract": "Language model training in distributed settings is limited by the communication cost of gradient exchanges. In this short note, we extend recent work from Malladi et al. (2023), using shared randomness to perform distributed fine-tuning with low bandwidth. The method is a natural decentralized extension of memory-efficient Simultaneous Perturbation Stochastic Approximation (SPSA). Each iteration, each machine seeds a Random Number Generator (RNG) to perform local reproducible perturbations on model weights and calculate and exchange scalar projected gradients, which are then used to update each model. By using a (machine, sample) identifier as the random seed, each model can regenerate one another's perturbations. As machines only exchange single-byte projected gradients, this is highly communication efficient. There are also potential privacy benefits, as projected gradients may be calculated on different training data, and models never access the other's data. Our approach not only d",
    "link": "http://arxiv.org/abs/2306.10015",
    "context": "Title: Just One Byte (per gradient): A Note on Low-Bandwidth Decentralized Language Model Finetuning Using Shared Randomness. (arXiv:2306.10015v1 [cs.LG])\nAbstract: Language model training in distributed settings is limited by the communication cost of gradient exchanges. In this short note, we extend recent work from Malladi et al. (2023), using shared randomness to perform distributed fine-tuning with low bandwidth. The method is a natural decentralized extension of memory-efficient Simultaneous Perturbation Stochastic Approximation (SPSA). Each iteration, each machine seeds a Random Number Generator (RNG) to perform local reproducible perturbations on model weights and calculate and exchange scalar projected gradients, which are then used to update each model. By using a (machine, sample) identifier as the random seed, each model can regenerate one another's perturbations. As machines only exchange single-byte projected gradients, this is highly communication efficient. There are also potential privacy benefits, as projected gradients may be calculated on different training data, and models never access the other's data. Our approach not only d",
    "path": "papers/23/06/2306.10015.json",
    "total_tokens": 974,
    "translated_title": "仅使用一字节（每梯度）：关于使用共享随机性进行低带宽分散式语言模型微调的简要说明",
    "translated_abstract": "分布式模型训练受到梯度交换的通信成本的限制。本文通过使用共享随机性来进行低带宽的分散式微调，扩展了Malladi等人2023年的最新工作。该方法是对具有记忆效率的同时扰动随机逼近（SPSA）的自然分散式扩展。在每次迭代中，每台机器都使用随机数生成器（RNG）来对模型权重进行局部可重现的扰动，并计算和交换标量投影梯度，然后用于更新每个模型。通过将（机器，样本）标识符用作随机种子，每个模型可以重新生成彼此的扰动。由于机器只交换单字节的投影梯度，因此这是高度通信效率的。此外，还存在潜在的隐私优势，因为投影梯度可以在不同的训练数据上计算，而模型从不访问其他数据。我们的方法不仅在具有低通信成本的同时，也在模型的准确性和训练速度方面展现出优势。",
    "tldr": "本文介绍了一种通过使用共享随机性来进行低带宽分散式微调的方法，该方法可以通过每台机器生成不同的随机扰动来更新每个模型，从而具有高度通信效率，并且具有隐私保护的优势。",
    "en_tdlr": "This paper introduces a method for low-bandwidth decentralized fine-tuning of language models using shared randomness, which relies on generating unique random perturbations on each machine to update the models with highly communication efficiency. The approach also has potential privacy benefits."
}