{
    "title": "Oracle-Efficient Pessimism: Offline Policy Optimization in Contextual Bandits. (arXiv:2306.07923v1 [cs.LG])",
    "abstract": "We consider policy optimization in contextual bandits, where one is given a fixed dataset of logged interactions. While pessimistic regularizers are typically used to mitigate distribution shift, prior implementations thereof are not computationally efficient. We present the first oracle-efficient algorithm for pessimistic policy optimization: it reduces to supervised learning, leading to broad applicability. We also obtain best-effort statistical guarantees analogous to those for pessimistic approaches in prior work. We instantiate our approach for both discrete and continuous actions. We perform extensive experiments in both settings, showing advantage over unregularized policy optimization across a wide range of configurations.",
    "link": "http://arxiv.org/abs/2306.07923",
    "context": "Title: Oracle-Efficient Pessimism: Offline Policy Optimization in Contextual Bandits. (arXiv:2306.07923v1 [cs.LG])\nAbstract: We consider policy optimization in contextual bandits, where one is given a fixed dataset of logged interactions. While pessimistic regularizers are typically used to mitigate distribution shift, prior implementations thereof are not computationally efficient. We present the first oracle-efficient algorithm for pessimistic policy optimization: it reduces to supervised learning, leading to broad applicability. We also obtain best-effort statistical guarantees analogous to those for pessimistic approaches in prior work. We instantiate our approach for both discrete and continuous actions. We perform extensive experiments in both settings, showing advantage over unregularized policy optimization across a wide range of configurations.",
    "path": "papers/23/06/2306.07923.json",
    "total_tokens": 763,
    "translated_title": "面向Oracle的悲观策略优化：离线上下文强化学习中的计算有效性",
    "translated_abstract": "本文考虑在上下文强化学习中的策略优化问题，其中给定一个固定数据集的日志交互。虽然通常使用悲观惩罚来缓解分布偏移，但先前的实现并不计算有效。本文提出了第一个面向Oracle有效的悲观策略优化算法：它简化为监督学习，具有广泛的适用性。我们也得出了类似于先前工作中悲观方法的最佳统计保证。我们为离散和连续动作都实例化了我们的方法。我们在两种情况下进行了广泛的实验，显示出在各种配置中都比未正则化的策略优化更具优势。",
    "tldr": "本文提出了第一个面向Oracle有效的悲观策略优化算法，它简化为监督学习，具有广泛的适用性，能够在上下文强化学习中优化策略。",
    "en_tdlr": "The paper proposes the first oracle-efficient algorithm for pessimistic policy optimization that reduces to supervised learning with wide applicability in contextual bandits. Extensive experiments show its advantage over unregularized policy optimization across a wide range of configurations."
}