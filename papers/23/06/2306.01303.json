{
    "title": "DistilXLSR: A Light Weight Cross-Lingual Speech Representation Model. (arXiv:2306.01303v1 [cs.CL])",
    "abstract": "Multilingual self-supervised speech representation models have greatly enhanced the speech recognition performance for low-resource languages, and the compression of these huge models has also become a crucial prerequisite for their industrial application. In this paper, we propose DistilXLSR, a distilled cross-lingual speech representation model. By randomly shuffling the phonemes of existing speech, we reduce the linguistic information and distill cross-lingual models using only English data. We also design a layer-jumping initialization method to fully leverage the teacher's pre-trained weights. Experiments on 2 kinds of teacher models and 15 low-resource languages show that our method can reduce the parameters by 50% while maintaining cross-lingual representation ability. Our method is proven to be generalizable to various languages/teacher models and has the potential to improve the cross-lingual performance of the English pre-trained models.",
    "link": "http://arxiv.org/abs/2306.01303",
    "context": "Title: DistilXLSR: A Light Weight Cross-Lingual Speech Representation Model. (arXiv:2306.01303v1 [cs.CL])\nAbstract: Multilingual self-supervised speech representation models have greatly enhanced the speech recognition performance for low-resource languages, and the compression of these huge models has also become a crucial prerequisite for their industrial application. In this paper, we propose DistilXLSR, a distilled cross-lingual speech representation model. By randomly shuffling the phonemes of existing speech, we reduce the linguistic information and distill cross-lingual models using only English data. We also design a layer-jumping initialization method to fully leverage the teacher's pre-trained weights. Experiments on 2 kinds of teacher models and 15 low-resource languages show that our method can reduce the parameters by 50% while maintaining cross-lingual representation ability. Our method is proven to be generalizable to various languages/teacher models and has the potential to improve the cross-lingual performance of the English pre-trained models.",
    "path": "papers/23/06/2306.01303.json",
    "total_tokens": 935,
    "translated_title": "DistilXLSR：一种轻量级跨语言语音表示模型",
    "translated_abstract": "多语言自监督语音表示模型大大提高了低资源语言的语音识别能力，而这些巨大模型的压缩也已成为其工业应用的重要前提。本文提出DistilXLSR，一种经过精简的跨语言语音表示模型。通过随机打乱已有语音中的音素，减少语言信息并仅使用英语数据来蒸馏跨语言模型。同时，我们还设计了一种跳层初始化方法，以充分利用教师预训练模型的权重。在两种教师模型和15种低资源语言上的实验表明，我们的方法可以将参数降低50%，同时保持跨语言表示能力。我们的方法已被证明适用于各种语言/教师模型，并有潜力提高英语预训练模型的跨语言性能。",
    "tldr": "提出一种跨语言语音表示模型DistilXLSR，通过随机打乱语音中的音素，只使用英语数据来训练模型，并设计一种初始化方法，成功将参数减少50%，同时保持跨语言表达能力，适用于各种语言/教师模型并有潜力提高英语预训练模型的跨语言性能。",
    "en_tdlr": "The paper proposes a distilled cross-lingual speech representation model DistilXLSR, which reduces the linguistic information by randomly shuffling phonemes and only uses English data to train the model. With a layer-jumping initialization method, it can reduce the parameters by 50% while maintaining cross-lingual representation ability, and has the potential to improve the cross-lingual performance of English pre-trained models, proving its generalizability to various languages/teacher models."
}