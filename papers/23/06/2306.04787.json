{
    "title": "Absformer: Transformer-based Model for Unsupervised Multi-Document Abstractive Summarization. (arXiv:2306.04787v1 [cs.CL])",
    "abstract": "Multi-document summarization (MDS) refers to the task of summarizing the text in multiple documents into a concise summary. The generated summary can save the time of reading many documents by providing the important content in the form of a few sentences. Abstractive MDS aims to generate a coherent and fluent summary for multiple documents using natural language generation techniques. In this paper, we consider the unsupervised abstractive MDS setting where there are only documents with no groundtruh summaries provided, and we propose Absformer, a new Transformer-based method for unsupervised abstractive summary generation. Our method consists of a first step where we pretrain a Transformer-based encoder using the masked language modeling (MLM) objective as the pretraining task in order to cluster the documents into semantically similar groups; and a second step where we train a Transformer-based decoder to generate abstractive summaries for the clusters of documents. To our knowledge",
    "link": "http://arxiv.org/abs/2306.04787",
    "context": "Title: Absformer: Transformer-based Model for Unsupervised Multi-Document Abstractive Summarization. (arXiv:2306.04787v1 [cs.CL])\nAbstract: Multi-document summarization (MDS) refers to the task of summarizing the text in multiple documents into a concise summary. The generated summary can save the time of reading many documents by providing the important content in the form of a few sentences. Abstractive MDS aims to generate a coherent and fluent summary for multiple documents using natural language generation techniques. In this paper, we consider the unsupervised abstractive MDS setting where there are only documents with no groundtruh summaries provided, and we propose Absformer, a new Transformer-based method for unsupervised abstractive summary generation. Our method consists of a first step where we pretrain a Transformer-based encoder using the masked language modeling (MLM) objective as the pretraining task in order to cluster the documents into semantically similar groups; and a second step where we train a Transformer-based decoder to generate abstractive summaries for the clusters of documents. To our knowledge",
    "path": "papers/23/06/2306.04787.json",
    "total_tokens": 1055,
    "translated_title": "基于Transformer的无监督多文档抽象摘要模型",
    "translated_abstract": "多文档摘要（MDS）是将多个文档中的文本总结成简洁概括的任务。所生成的摘要通过用少数几句话提供重要内容，可以省去阅读多个文档的时间。抽象MDS旨在使用自然语言生成技术为多个文档生成连贯、流畅的摘要。本文考虑仅有文档而没有摘要的无监督抽象MDS环境，并提出Absformer，这是一种用于无监督抽象摘要生成的新型基于Transformer的方法。我们的方法包括第一步，使用掩码语言建模（MLM）目标作为预训练任务，以训练Transformer编码器，将文档聚类为语义相似的组；第二步，训练一个Transformer解码器，为文档集群生成抽象摘要。据我们所知，这是第一份提出将聚类用于文档组合的无监督抽象MDS方法的工作。我们在基准数据集上的实验结果显示，我们的方法优于几种最先进的无监督抽象MDS方法。",
    "tldr": "本文提出了一种新的用于无监督多文档抽象摘要生成的Transformer-based方法Absformer，它使用掩码语言建模（MLM）目标进行预训练聚类文档并生成抽象摘要，实验结果显示，它在基准数据集上优于几种最先进的无监督抽象MDS方法。",
    "en_tdlr": "This paper proposes a new Transformer-based method Absformer for unsupervised multi-document abstractive summarization, which pretrains a Transformer-based encoder using masked language modeling (MLM) objective to cluster documents and generate abstractive summaries for the clusters using a Transformer-based decoder. Experimental results show that Absformer outperforms several state-of-the-art unsupervised abstractive MDS methods on benchmark datasets."
}