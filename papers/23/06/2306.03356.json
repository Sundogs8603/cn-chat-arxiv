{
    "title": "Query Complexity of Active Learning for Function Family With Nearly Orthogonal Basis. (arXiv:2306.03356v1 [cs.LG])",
    "abstract": "Many machine learning algorithms require large numbers of labeled data to deliver state-of-the-art results. In applications such as medical diagnosis and fraud detection, though there is an abundance of unlabeled data, it is costly to label the data by experts, experiments, or simulations. Active learning algorithms aim to reduce the number of required labeled data points while preserving performance. For many convex optimization problems such as linear regression and $p$-norm regression, there are theoretical bounds on the number of required labels to achieve a certain accuracy. We call this the query complexity of active learning. However, today's active learning algorithms require the underlying learned function to have an orthogonal basis. For example, when applying active learning to linear regression, the requirement is the target function is a linear composition of a set of orthogonal linear functions, and active learning can find the coefficients of these linear functions. We p",
    "link": "http://arxiv.org/abs/2306.03356",
    "context": "Title: Query Complexity of Active Learning for Function Family With Nearly Orthogonal Basis. (arXiv:2306.03356v1 [cs.LG])\nAbstract: Many machine learning algorithms require large numbers of labeled data to deliver state-of-the-art results. In applications such as medical diagnosis and fraud detection, though there is an abundance of unlabeled data, it is costly to label the data by experts, experiments, or simulations. Active learning algorithms aim to reduce the number of required labeled data points while preserving performance. For many convex optimization problems such as linear regression and $p$-norm regression, there are theoretical bounds on the number of required labels to achieve a certain accuracy. We call this the query complexity of active learning. However, today's active learning algorithms require the underlying learned function to have an orthogonal basis. For example, when applying active learning to linear regression, the requirement is the target function is a linear composition of a set of orthogonal linear functions, and active learning can find the coefficients of these linear functions. We p",
    "path": "papers/23/06/2306.03356.json",
    "total_tokens": 784,
    "translated_title": "近正交基函数族的主动学习查询复杂度分析",
    "translated_abstract": "许多机器学习算法需要大量标记数据才能提供最先进的结果。在医学诊断和欺诈检测等领域，虽然存在大量未标记数据，但将这些数据标记成本高昂。主动学习算法旨在减少所需的标记数据点数目，同时保持性能。对于许多凸优化问题，例如线性回归和$p$- 范数回归，都已经存在了所需的标记数目实现某种准确性的理论界限。我们称之为主动学习的查询复杂度。然而，今天的主动学习算法需要所学习函数的正交基与目标函数相匹配。例如，当将主动学习应用于线性回归时，要求目标函数是一组正交线性函数的线性组合，并且可以使用主动学习找到这些线性函数的系数。",
    "tldr": "该论文分析了近正交基函数族的主动学习查询复杂度，提出了相应的算法优化方案。"
}