{
    "title": "BetaZero: Belief-State Planning for Long-Horizon POMDPs using Learned Approximations. (arXiv:2306.00249v1 [cs.AI])",
    "abstract": "Real-world planning problems$\\unicode{x2014}$including autonomous driving and sustainable energy applications like carbon storage and resource exploration$\\unicode{x2014}$have recently been modeled as partially observable Markov decision processes (POMDPs) and solved using approximate methods. To solve high-dimensional POMDPs in practice, state-of-the-art methods use online planning with problem-specific heuristics to reduce planning horizons and make the problems tractable. Algorithms that learn approximations to replace heuristics have recently found success in large-scale problems in the fully observable domain. The key insight is the combination of online Monte Carlo tree search with offline neural network approximations of the optimal policy and value function. In this work, we bring this insight to partially observed domains and propose BetaZero, a belief-state planning algorithm for POMDPs. BetaZero learns offline approximations based on accurate belief models to enable online d",
    "link": "http://arxiv.org/abs/2306.00249",
    "context": "Title: BetaZero: Belief-State Planning for Long-Horizon POMDPs using Learned Approximations. (arXiv:2306.00249v1 [cs.AI])\nAbstract: Real-world planning problems$\\unicode{x2014}$including autonomous driving and sustainable energy applications like carbon storage and resource exploration$\\unicode{x2014}$have recently been modeled as partially observable Markov decision processes (POMDPs) and solved using approximate methods. To solve high-dimensional POMDPs in practice, state-of-the-art methods use online planning with problem-specific heuristics to reduce planning horizons and make the problems tractable. Algorithms that learn approximations to replace heuristics have recently found success in large-scale problems in the fully observable domain. The key insight is the combination of online Monte Carlo tree search with offline neural network approximations of the optimal policy and value function. In this work, we bring this insight to partially observed domains and propose BetaZero, a belief-state planning algorithm for POMDPs. BetaZero learns offline approximations based on accurate belief models to enable online d",
    "path": "papers/23/06/2306.00249.json",
    "total_tokens": 847,
    "translated_title": "BetaZero：基于学习的近似算法的置信状态规划用于长时间跨度的POMDPs",
    "translated_abstract": "实际的规划问题，包括自动驾驶、碳储存和资源勘探等可持续能源应用，最近被建模为部分观测马尔可夫决策过程（POMDPs）并使用近似方法解决。为了在实践中解决高维度POMDPs，最先进的方法使用了问题特定的启发式算法进行在线规划，以减少规划时间跨度并使问题易于解决。最近成功地在完全可观察的领域中找到了用于替换启发式算法的学习近似算法。关键洞见是将在线蒙特卡罗树搜索与离线神经网络近似相结合，以优化策略和值函数。本文将这一洞见应用到了部分观察域，并提出了BetaZero，一种适用于POMDP的置信状态规划算法。",
    "tldr": "本文提出了一种叫做BetaZero的方法，它是一种使用学习近似算法的置信状态规划算法，可以用于解决长时间跨度的POMDP问题。",
    "en_tdlr": "This paper proposes a belief-state planning algorithm called BetaZero, which utilizes learned approximations and can solve long-horizon POMDP problems."
}