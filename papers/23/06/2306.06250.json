{
    "title": "Strategic Apple Tasting. (arXiv:2306.06250v1 [cs.GT])",
    "abstract": "Algorithmic decision-making in high-stakes domains often involves assigning decisions to agents with incentives to strategically modify their input to the algorithm. In addition to dealing with incentives, in many domains of interest (e.g. lending and hiring) the decision-maker only observes feedback regarding their policy for rounds in which they assign a positive decision to the agent; this type of feedback is often referred to as apple tasting (or one-sided) feedback. We formalize this setting as an online learning problem with apple-tasting feedback where a principal makes decisions about a sequence of $T$ agents, each of which is represented by a context that may be strategically modified. Our goal is to achieve sublinear strategic regret, which compares the performance of the principal to that of the best fixed policy in hindsight, if the agents were truthful when revealing their contexts. Our main result is a learning algorithm which incurs $\\tilde{\\mathcal{O}}(\\sqrt{T})$ strate",
    "link": "http://arxiv.org/abs/2306.06250",
    "context": "Title: Strategic Apple Tasting. (arXiv:2306.06250v1 [cs.GT])\nAbstract: Algorithmic decision-making in high-stakes domains often involves assigning decisions to agents with incentives to strategically modify their input to the algorithm. In addition to dealing with incentives, in many domains of interest (e.g. lending and hiring) the decision-maker only observes feedback regarding their policy for rounds in which they assign a positive decision to the agent; this type of feedback is often referred to as apple tasting (or one-sided) feedback. We formalize this setting as an online learning problem with apple-tasting feedback where a principal makes decisions about a sequence of $T$ agents, each of which is represented by a context that may be strategically modified. Our goal is to achieve sublinear strategic regret, which compares the performance of the principal to that of the best fixed policy in hindsight, if the agents were truthful when revealing their contexts. Our main result is a learning algorithm which incurs $\\tilde{\\mathcal{O}}(\\sqrt{T})$ strate",
    "path": "papers/23/06/2306.06250.json",
    "total_tokens": 994,
    "translated_title": "战略性苹果品尝：带有一面性反馈的在线学习问题",
    "translated_abstract": "在高风险领域中，算法决策往往涉及将决策分配给具有策略性修改其算法输入动机的代理。除了应对激励因素外，在许多感兴趣的领域（例如贷款和招聘）中，决策者只观察到在分配积极决策给代理时的回馈。我们将这种反馈称为苹果品尝（或单向反馈）。我们将这一情境形式化为带有苹果品尝反馈的在线学习问题，其中一个负责人决策一系列 $T$ 个代理，每个代理由可被策略性修改的上下文表示。我们的目标是在代理揭示其上下文时获得亚线性的战略遗憾，即如果代理在揭示其上下文时是真实的，则将负责人的表现与后见之明的最佳固定策略进行比较。我们的主要结果是一种学习算法，在这种情况下产生 $\\tilde{\\mathcal{O}}(\\sqrt{T})$ 的战略遗憾，与更一般类型的反馈所知的最佳速率相匹配。",
    "tldr": "本篇论文介绍了一种带有苹果品尝反馈的在线学习问题，该问题需要使用一种学习算法以实现战略遗憾。研究结果表明，我们提出的算法的战略遗憾近似于最佳速率。",
    "en_tdlr": "This paper introduces an online learning problem with apple-tasting feedback, which requires a learning algorithm to achieve strategic regret. The research shows that the algorithm we propose achieves a strategic regret rate approximately equaling the best known rate."
}