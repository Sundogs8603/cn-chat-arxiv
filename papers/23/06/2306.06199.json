{
    "title": "Reliability Check: An Analysis of GPT-3's Response to Sensitive Topics and Prompt Wording. (arXiv:2306.06199v1 [cs.CL])",
    "abstract": "Large language models (LLMs) have become mainstream technology with their versatile use cases and impressive performance. Despite the countless out-of-the-box applications, LLMs are still not reliable. A lot of work is being done to improve the factual accuracy, consistency, and ethical standards of these models through fine-tuning, prompting, and Reinforcement Learning with Human Feedback (RLHF), but no systematic analysis of the responses of these models to different categories of statements, or on their potential vulnerabilities to simple prompting changes is available. In this work, we analyze what confuses GPT-3: how the model responds to certain sensitive topics and what effects the prompt wording has on the model response. We find that GPT-3 correctly disagrees with obvious Conspiracies and Stereotypes but makes mistakes with common Misconceptions and Controversies. The model responses are inconsistent across prompts and settings, highlighting GPT-3's unreliability. Dataset and ",
    "link": "http://arxiv.org/abs/2306.06199",
    "context": "Title: Reliability Check: An Analysis of GPT-3's Response to Sensitive Topics and Prompt Wording. (arXiv:2306.06199v1 [cs.CL])\nAbstract: Large language models (LLMs) have become mainstream technology with their versatile use cases and impressive performance. Despite the countless out-of-the-box applications, LLMs are still not reliable. A lot of work is being done to improve the factual accuracy, consistency, and ethical standards of these models through fine-tuning, prompting, and Reinforcement Learning with Human Feedback (RLHF), but no systematic analysis of the responses of these models to different categories of statements, or on their potential vulnerabilities to simple prompting changes is available. In this work, we analyze what confuses GPT-3: how the model responds to certain sensitive topics and what effects the prompt wording has on the model response. We find that GPT-3 correctly disagrees with obvious Conspiracies and Stereotypes but makes mistakes with common Misconceptions and Controversies. The model responses are inconsistent across prompts and settings, highlighting GPT-3's unreliability. Dataset and ",
    "path": "papers/23/06/2306.06199.json",
    "total_tokens": 917,
    "translated_title": "可靠性检查：对GPT-3在敏感话题和提示措辞方面的反应分析",
    "translated_abstract": "大型语言模型已成为主流技术，具有多种用途和出色的性能。尽管有无数的应用，但LLMs仍然不是可靠的。通过微调、提示和人类反馈的强化学习等方法，正在进行大量工作来提高这些模型的事实准确性、一致性和道德标准，但缺乏对这些模型对不同语句类别的反应或在简单提示变化下可能存在的漏洞的系统分析。在本研究中，我们分析了什么会让GPT-3困惑：模型如何响应某些敏感话题以及提示措辞对模型响应的影响。我们发现，GPT-3正确地反对明显的阴谋论和刻板印象，但在普遍的误解和争议中犯了错误。模型响应在提示和设置上不一致，突显出GPT-3的不可靠性。",
    "tldr": "本文分析了大型语言模型GPT-3对敏感话题和提示措辞的反应，发现其在阴谋论和刻板印象方面有正确的反应，但在误解和争议方面存在错误，并具有不可靠性。",
    "en_tdlr": "This paper analyzes the response of the large language model GPT-3 to sensitive topics and prompt wording, finding that it correctly responds to conspiracies and stereotypes but makes mistakes in misconceptions and controversies, and highlights its unreliability due to inconsistent responses across prompts and settings."
}