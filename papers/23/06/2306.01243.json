{
    "title": "Efficient RL with Impaired Observability: Learning to Act with Delayed and Missing State Observations. (arXiv:2306.01243v1 [cs.LG])",
    "abstract": "In real-world reinforcement learning (RL) systems, various forms of impaired observability can complicate matters. These situations arise when an agent is unable to observe the most recent state of the system due to latency or lossy channels, yet the agent must still make real-time decisions. This paper introduces a theoretical investigation into efficient RL in control systems where agents must act with delayed and missing state observations. We establish near-optimal regret bounds, of the form $\\tilde{\\mathcal{O}}(\\sqrt{{\\rm poly}(H) SAK})$, for RL in both the delayed and missing observation settings. Despite impaired observability posing significant challenges to the policy class and planning, our results demonstrate that learning remains efficient, with the regret bound optimally depending on the state-action size of the original system. Additionally, we provide a characterization of the performance of the optimal policy under impaired observability, comparing it to the optimal val",
    "link": "http://arxiv.org/abs/2306.01243",
    "context": "Title: Efficient RL with Impaired Observability: Learning to Act with Delayed and Missing State Observations. (arXiv:2306.01243v1 [cs.LG])\nAbstract: In real-world reinforcement learning (RL) systems, various forms of impaired observability can complicate matters. These situations arise when an agent is unable to observe the most recent state of the system due to latency or lossy channels, yet the agent must still make real-time decisions. This paper introduces a theoretical investigation into efficient RL in control systems where agents must act with delayed and missing state observations. We establish near-optimal regret bounds, of the form $\\tilde{\\mathcal{O}}(\\sqrt{{\\rm poly}(H) SAK})$, for RL in both the delayed and missing observation settings. Despite impaired observability posing significant challenges to the policy class and planning, our results demonstrate that learning remains efficient, with the regret bound optimally depending on the state-action size of the original system. Additionally, we provide a characterization of the performance of the optimal policy under impaired observability, comparing it to the optimal val",
    "path": "papers/23/06/2306.01243.json",
    "total_tokens": 1003,
    "translated_title": "有损可观察性下的高效强化学习：学习在延迟和缺失环境中做出决策",
    "translated_abstract": "在现实世界的强化学习系统中，各种形式的有损可观察性可能会使问题变得复杂。当代理由于延迟或丢失的通道而无法观察到系统的最新状态时，这些情况会出现，但代理仍必须做出实时决策。本文介绍了对在控制系统中进行高效强化学习进行理论研究，其中代理必须在延迟和缺失状态观察环境中操作。我们建立了接近最优的遗憾边界，形式为$ \\tilde{\\mathcal{O}}(\\sqrt{{\\rm poly}(H) SAK}) $，以在延迟和缺失观察设置下实现强化学习。尽管有损可观察性对策略和规划的类别造成了重大挑战，但我们的结果表明学习仍然是高效的，遗憾边界最优地依赖于原始系统的状态-动作大小。此外，我们比较了受影响的观察下最佳策略的性能与最优值之间的差异。",
    "tldr": "本文提出了在延迟和缺失状态观察环境中进行高效强化学习的理论研究，并建立了接近最优的遗憾边界，尽管有损可观察性对策略和规划的类别造成了重大挑战，但学习仍然是高效的。",
    "en_tdlr": "This paper presents a theoretical investigation into efficient reinforcement learning in control systems where agents must act with delayed and missing state observations, establishing near-optimal regret bounds and proving that, despite posing significant challenges to the policy class and planning, learning remains efficient in impaired observability environments."
}