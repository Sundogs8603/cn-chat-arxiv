{
    "title": "Beyond Implicit Bias: The Insignificance of SGD Noise in Online Learning. (arXiv:2306.08590v1 [cs.LG])",
    "abstract": "The success of SGD in deep learning has been ascribed by prior works to the implicit bias induced by high learning rate or small batch size (\"SGD noise\"). While prior works that focused on offline learning (i.e., multiple-epoch training), we study the impact of SGD noise on online (i.e., single epoch) learning. Through an extensive empirical analysis of image and language data, we demonstrate that large learning rate and small batch size do not confer any implicit bias advantages in online learning. In contrast to offline learning, the benefits of SGD noise in online learning are strictly computational, facilitating larger or more cost-effective gradient steps. Our work suggests that SGD in the online regime can be construed as taking noisy steps along the \"golden path\" of the noiseless gradient flow algorithm. We provide evidence to support this hypothesis by conducting experiments that reduce SGD noise during training and by measuring the pointwise functional distance between models ",
    "link": "http://arxiv.org/abs/2306.08590",
    "context": "Title: Beyond Implicit Bias: The Insignificance of SGD Noise in Online Learning. (arXiv:2306.08590v1 [cs.LG])\nAbstract: The success of SGD in deep learning has been ascribed by prior works to the implicit bias induced by high learning rate or small batch size (\"SGD noise\"). While prior works that focused on offline learning (i.e., multiple-epoch training), we study the impact of SGD noise on online (i.e., single epoch) learning. Through an extensive empirical analysis of image and language data, we demonstrate that large learning rate and small batch size do not confer any implicit bias advantages in online learning. In contrast to offline learning, the benefits of SGD noise in online learning are strictly computational, facilitating larger or more cost-effective gradient steps. Our work suggests that SGD in the online regime can be construed as taking noisy steps along the \"golden path\" of the noiseless gradient flow algorithm. We provide evidence to support this hypothesis by conducting experiments that reduce SGD noise during training and by measuring the pointwise functional distance between models ",
    "path": "papers/23/06/2306.08590.json",
    "total_tokens": 981,
    "translated_title": "超越隐性偏差：SGD噪声在在线学习中的不重要性",
    "translated_abstract": "先前的研究认为，SGD在深度学习中的成功归因于高学习率或小批量大小所引起的隐性偏差（“SGD噪声”）。而我们研究了SGD噪声在在线（即单个epoch）学习中的影响，通过对图像和语言数据的全面实证分析，我们证明了在在线学习中，大学习率和小批量大小并不会带来任何隐性偏差的优势。与离线学习相反，在线学习中SGD噪声的好处严格来说只是计算上的便利，可以促进更大或更具成本效益的梯度步骤。我们的研究表明，SGD在在线模式下可以被视为是在“无噪声梯度流算法”的“黄金路径”上踩踏嘈杂步伐。通过减少训练期间的SGD噪声和测量模型之间的逐点功能距离，我们提供了支持此假设的证据。",
    "tldr": "本文通过实际数据的全面实证分析证明了在在线学习中，大学习率和小批量大小并不会带来任何隐性偏差的优势。在线学习中SGD噪音的好处只是计算上的便利，可以促进更大或更具成本效益的梯度步骤。",
    "en_tdlr": "This paper presents an empirical analysis showing that in online learning, large learning rate and small batch size do not confer any implicit bias advantages. The benefits of SGD noise in online learning are strictly computational, facilitating larger or more cost-effective gradient steps. This suggests that SGD in the online regime can be viewed as taking noisy steps along the \"golden path\" of the noiseless gradient flow algorithm."
}