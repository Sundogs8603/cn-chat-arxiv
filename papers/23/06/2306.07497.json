{
    "title": "GQFedWAvg: Optimization-Based Quantized Federated Learning in General Edge Computing Systems. (arXiv:2306.07497v1 [cs.LG])",
    "abstract": "The optimal implementation of federated learning (FL) in practical edge computing systems has been an outstanding problem. In this paper, we propose an optimization-based quantized FL algorithm, which can appropriately fit a general edge computing system with uniform or nonuniform computing and communication resources at the workers. Specifically, we first present a new random quantization scheme and analyze its properties. Then, we propose a general quantized FL algorithm, namely GQFedWAvg. Specifically, GQFedWAvg applies the proposed quantization scheme to quantize wisely chosen model update-related vectors and adopts a generalized mini-batch stochastic gradient descent (SGD) method with the weighted average local model updates in global model aggregation. Besides, GQFedWAvg has several adjustable algorithm parameters to flexibly adapt to the computing and communication resources at the server and workers. We also analyze the convergence of GQFedWAvg. Next, we optimize the algorithm ",
    "link": "http://arxiv.org/abs/2306.07497",
    "context": "Title: GQFedWAvg: Optimization-Based Quantized Federated Learning in General Edge Computing Systems. (arXiv:2306.07497v1 [cs.LG])\nAbstract: The optimal implementation of federated learning (FL) in practical edge computing systems has been an outstanding problem. In this paper, we propose an optimization-based quantized FL algorithm, which can appropriately fit a general edge computing system with uniform or nonuniform computing and communication resources at the workers. Specifically, we first present a new random quantization scheme and analyze its properties. Then, we propose a general quantized FL algorithm, namely GQFedWAvg. Specifically, GQFedWAvg applies the proposed quantization scheme to quantize wisely chosen model update-related vectors and adopts a generalized mini-batch stochastic gradient descent (SGD) method with the weighted average local model updates in global model aggregation. Besides, GQFedWAvg has several adjustable algorithm parameters to flexibly adapt to the computing and communication resources at the server and workers. We also analyze the convergence of GQFedWAvg. Next, we optimize the algorithm ",
    "path": "papers/23/06/2306.07497.json",
    "total_tokens": 973,
    "translated_title": "GQFedWAvg：基于优化的量化联邦学习在一般的边缘计算系统中的应用",
    "translated_abstract": "在实际边缘计算系统中，联邦学习（FL）的最佳实现一直是一个突出的问题。本文提出了一种基于优化的量化FL算法，可以在工作节点具有均匀或非均匀的计算和通信资源的一般边缘计算系统中适当地适应。具体而言，我们首先提出了一种新的随机量化方案并分析了其性质。然后，我们提出了一种通用的量化FL算法，即GQFedWAvg。具体而言，GQFedWAvg将所提出的量化方案应用于智能选择的模型更新相关向量，并采用加权平均本地模型更新的广义小批量随机梯度下降（SGD）方法在全局模型聚合中。此外，GQFedWAvg有一些可调整的算法参数，可以灵活地适应服务器和工作节点的计算和通信资源。我们还分析了GQFedWAvg的收敛性。接下来，我们优化该算法。",
    "tldr": "本论文提出了一种适用于在一般的边缘计算系统中的基于优化的量化FL算法，采用新的随机量化方案和加权平均本地模型更新的广义小批量随机梯度下降方法在全局模型聚合中，以适应在工作节点具有均匀或非均匀的计算和通信资源的情况。",
    "en_tdlr": "This paper proposes an optimization-based quantized FL algorithm for general edge computing systems, which uses a new random quantization scheme and a generalized mini-batch SGD method with weighted average local model updates in global model aggregation to adapt to computing and communication resources at workers with uniform or nonuniform resources."
}