{
    "title": "Creating Valid Adversarial Examples of Malware. (arXiv:2306.13587v1 [cs.CR])",
    "abstract": "Machine learning is becoming increasingly popular as a go-to approach for many tasks due to its world-class results. As a result, antivirus developers are incorporating machine learning models into their products. While these models improve malware detection capabilities, they also carry the disadvantage of being susceptible to adversarial attacks. Although this vulnerability has been demonstrated for many models in white-box settings, a black-box attack is more applicable in practice for the domain of malware detection. We present a generator of adversarial malware examples using reinforcement learning algorithms. The reinforcement learning agents utilize a set of functionality-preserving modifications, thus creating valid adversarial examples. Using the proximal policy optimization (PPO) algorithm, we achieved an evasion rate of 53.84% against the gradient-boosted decision tree (GBDT) model. The PPO agent previously trained against the GBDT classifier scored an evasion rate of 11.41%",
    "link": "http://arxiv.org/abs/2306.13587",
    "context": "Title: Creating Valid Adversarial Examples of Malware. (arXiv:2306.13587v1 [cs.CR])\nAbstract: Machine learning is becoming increasingly popular as a go-to approach for many tasks due to its world-class results. As a result, antivirus developers are incorporating machine learning models into their products. While these models improve malware detection capabilities, they also carry the disadvantage of being susceptible to adversarial attacks. Although this vulnerability has been demonstrated for many models in white-box settings, a black-box attack is more applicable in practice for the domain of malware detection. We present a generator of adversarial malware examples using reinforcement learning algorithms. The reinforcement learning agents utilize a set of functionality-preserving modifications, thus creating valid adversarial examples. Using the proximal policy optimization (PPO) algorithm, we achieved an evasion rate of 53.84% against the gradient-boosted decision tree (GBDT) model. The PPO agent previously trained against the GBDT classifier scored an evasion rate of 11.41%",
    "path": "papers/23/06/2306.13587.json",
    "total_tokens": 833,
    "translated_title": "制造对恶意软件有效的对抗样本",
    "translated_abstract": "由于机器学习在许多任务中表现优异，因此成为越来越受欢迎的方法。因此，杀毒软件开发者将机器学习模型纳入其产品中。尽管这些模型改进了恶意软件检测能力，但它们也带有易受对抗攻击的劣势。我们通过强化学习算法提出了一个对抗性恶意代码示例生成器，其强化学习代理使用一组功能保护修改来创建有效的对抗示例。使用近端策略优化（PPO）算法，我们针对梯度提升决策树（GBDT）模型实现了53.84％的逃避率。之前针对GBDT分类器进行训练的PPO代理得分为11.41％的逃避率。",
    "tldr": "该论文提出了一个可以制造对恶意软件有效的对抗样本的生成器，使用强化学习和功能保护修改，可以成功地攻击梯度提升决策树（GBDT）模型，并获得53.84%的成功逃避率。",
    "en_tdlr": "This paper presents a generator of adversarial malware examples that uses reinforcement learning and functionality-preserving modifications to successfully attack the gradient-boosted decision tree (GBDT) model, achieving 53.84% evasion rate."
}