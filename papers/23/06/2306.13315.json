{
    "title": "Abstractive Text Summarization for Resumes With Cutting Edge NLP Transformers and LSTM. (arXiv:2306.13315v1 [cs.CL])",
    "abstract": "Text summarization is a fundamental task in natural language processing that aims to condense large amounts of textual information into concise and coherent summaries. With the exponential growth of content and the need to extract key information efficiently, text summarization has gained significant attention in recent years. In this study, LSTM and pre-trained T5, Pegasus, BART and BART-Large model performances were evaluated on the open source dataset (Xsum, CNN/Daily Mail, Amazon Fine Food Review and News Summary) and the prepared resume dataset. This resume dataset consists of many information such as language, education, experience, personal information, skills, and this data includes 75 resumes. The primary objective of this research was to classify resume text. Various techniques such as LSTM, pre-trained models, and fine-tuned models were assessed using a dataset of resumes. The BART-Large model fine-tuned with the resume dataset gave the best performance.",
    "link": "http://arxiv.org/abs/2306.13315",
    "context": "Title: Abstractive Text Summarization for Resumes With Cutting Edge NLP Transformers and LSTM. (arXiv:2306.13315v1 [cs.CL])\nAbstract: Text summarization is a fundamental task in natural language processing that aims to condense large amounts of textual information into concise and coherent summaries. With the exponential growth of content and the need to extract key information efficiently, text summarization has gained significant attention in recent years. In this study, LSTM and pre-trained T5, Pegasus, BART and BART-Large model performances were evaluated on the open source dataset (Xsum, CNN/Daily Mail, Amazon Fine Food Review and News Summary) and the prepared resume dataset. This resume dataset consists of many information such as language, education, experience, personal information, skills, and this data includes 75 resumes. The primary objective of this research was to classify resume text. Various techniques such as LSTM, pre-trained models, and fine-tuned models were assessed using a dataset of resumes. The BART-Large model fine-tuned with the resume dataset gave the best performance.",
    "path": "papers/23/06/2306.13315.json",
    "total_tokens": 911,
    "translated_title": "利用先进的NLP变形器和LSTM进行简历的提取性文本摘要",
    "translated_abstract": "文本摘要是自然语言处理中的一项基本任务，旨在将大量的文本信息压缩成简洁连贯的摘要。随着内容的指数增长和高效提取关键信息的需求，文本摘要在近年来受到了极大的关注。在本研究中，评估了LSTM和预训练的T5、Pegasus、BART和BART-Large模型在开源数据集（Xsum、CNN/Daily Mail、亚马逊精美食品评论和新闻摘要）和准备的简历数据集上的表现。该简历数据集包括许多信息，如语言、教育、经验、个人信息、技能等，数据集中包括了75份简历。本研究的主要目标是对简历文本进行分类。使用简历数据集评估了各种技术，包括LSTM、预训练模型和微调模型。使用简历数据集微调的BART-Large模型表现最佳。",
    "tldr": "本研究评估了多种技术（包括LSTM、T5、Pegasus、BART和BART-Large模型）在不同数据集上对简历文本进行分类任务的表现，结果显示微调后的BART-Large模型效果最佳。",
    "en_tdlr": "This study evaluates the performance of various techniques (including LSTM, T5, Pegasus, BART and BART-Large models) on the task of classifying resume text on different datasets, and the results show that the BART-Large model fine-tuned with the resume dataset performs the best."
}