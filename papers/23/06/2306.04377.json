{
    "title": "Get More for Less in Decentralized Learning Systems. (arXiv:2306.04377v1 [cs.DC])",
    "abstract": "Decentralized learning (DL) systems have been gaining popularity because they avoid raw data sharing by communicating only model parameters, hence preserving data confidentiality. However, the large size of deep neural networks poses a significant challenge for decentralized training, since each node needs to exchange gigabytes of data, overloading the network. In this paper, we address this challenge with JWINS, a communication-efficient and fully decentralized learning system that shares only a subset of parameters through sparsification. JWINS uses wavelet transform to limit the information loss due to sparsification and a randomized communication cut-off that reduces communication usage without damaging the performance of trained models. We demonstrate empirically with 96 DL nodes on non-IID datasets that JWINS can achieve similar accuracies to full-sharing DL while sending up to 64% fewer bytes. Additionally, on low communication budgets, JWINS outperforms the state-of-the-art com",
    "link": "http://arxiv.org/abs/2306.04377",
    "context": "Title: Get More for Less in Decentralized Learning Systems. (arXiv:2306.04377v1 [cs.DC])\nAbstract: Decentralized learning (DL) systems have been gaining popularity because they avoid raw data sharing by communicating only model parameters, hence preserving data confidentiality. However, the large size of deep neural networks poses a significant challenge for decentralized training, since each node needs to exchange gigabytes of data, overloading the network. In this paper, we address this challenge with JWINS, a communication-efficient and fully decentralized learning system that shares only a subset of parameters through sparsification. JWINS uses wavelet transform to limit the information loss due to sparsification and a randomized communication cut-off that reduces communication usage without damaging the performance of trained models. We demonstrate empirically with 96 DL nodes on non-IID datasets that JWINS can achieve similar accuracies to full-sharing DL while sending up to 64% fewer bytes. Additionally, on low communication budgets, JWINS outperforms the state-of-the-art com",
    "path": "papers/23/06/2306.04377.json",
    "total_tokens": 1011,
    "translated_title": "分布式学习系统中用更少的代价获得更多。（arXiv:2306.04377v1 [cs.DC]）",
    "translated_abstract": "分布式学习系统因为能够避免原始数据共享而仅通过交流模型参数保护了数据的机密性，在机器学习领域越来越受欢迎。但是，深度神经网络的庞大规模对分布式训练提出了重要挑战——每个节点需要交换数千万个数据，容易导致网络超负荷。本文提出JWINS：一种通信效率高且完全分布式的机器学习系统，仅通过稀疏化来共享部分参数。JWINS使用小波变换来限制因稀疏化导致的信息损失，并采用随机通信切断来降低通信用量，而不会影响训练模型的性能。通过96个非独立同分布数据集的分布式学习节点的实现效果，证明JWINS可以在发送64％更少的字节时实现与完全共享分布式学习的相似准确性。此外，在低通信预算下，JWINS胜过了最先进的计算方法。",
    "tldr": "本文提出了一种名为JWINS的分布式学习系统，它仅通过稀疏化的方式共享部分模型参数，使用小波变换来补偿由稀疏化引起的信息损失，并通过随机通信截断来减少通信用量。实验证明，JWINS可以在发送更少的字节的情况下实现与完全共享分布式学习相似的准确性。"
}