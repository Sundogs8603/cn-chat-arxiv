{
    "title": "Accelerated Convergence of Nesterov's Momentum for Deep Neural Networks under Partial Strong Convexity. (arXiv:2306.08109v1 [cs.LG])",
    "abstract": "Current state-of-the-art analyses on the convergence of gradient descent for training neural networks focus on characterizing properties of the loss landscape, such as the Polyak-Lojaciewicz (PL) condition and the restricted strong convexity. While gradient descent converges linearly under such conditions, it remains an open question whether Nesterov's momentum enjoys accelerated convergence under similar settings and assumptions. In this work, we consider a new class of objective functions, where only a subset of the parameters satisfies strong convexity, and show Nesterov's momentum achieves acceleration in theory for this objective class. We provide two realizations of the problem class, one of which is deep ReLU networks, which --to the best of our knowledge--constitutes this work the first that proves accelerated convergence rate for non-trivial neural network architectures.",
    "link": "http://arxiv.org/abs/2306.08109",
    "context": "Title: Accelerated Convergence of Nesterov's Momentum for Deep Neural Networks under Partial Strong Convexity. (arXiv:2306.08109v1 [cs.LG])\nAbstract: Current state-of-the-art analyses on the convergence of gradient descent for training neural networks focus on characterizing properties of the loss landscape, such as the Polyak-Lojaciewicz (PL) condition and the restricted strong convexity. While gradient descent converges linearly under such conditions, it remains an open question whether Nesterov's momentum enjoys accelerated convergence under similar settings and assumptions. In this work, we consider a new class of objective functions, where only a subset of the parameters satisfies strong convexity, and show Nesterov's momentum achieves acceleration in theory for this objective class. We provide two realizations of the problem class, one of which is deep ReLU networks, which --to the best of our knowledge--constitutes this work the first that proves accelerated convergence rate for non-trivial neural network architectures.",
    "path": "papers/23/06/2306.08109.json",
    "total_tokens": 810,
    "translated_title": "针对部分强凸性，在Nesterov动量法下加速收敛深度神经网络",
    "translated_abstract": "当前最先进的神经网络梯度下降收敛分析聚焦于表征损失函数的特性，例如Polyak-Lojaciewicz（PL）条件和受限强凸性。虽然在这些条件下梯度下降具有线性收敛性，但是Nesterov动量法是否在类似的条件和假设下具有加速收敛仍然是一个未解决的问题。在这项研究中，我们考虑了一类新的目标函数，只有部分参数满足强凸性，并证明了Nesterov动量法在这种目标函数下实现了加速收敛。我们提供了两种问题类别的实现，其中一种是深度ReLU网络，这是我们所知道的第一个证明非平凡神经网络体系结构具有加速收敛率的论文。",
    "tldr": "本研究证明了针对一类新的目标函数，只有部分参数满足强凸性，Nesterov动量法在深度神经网络中实现了加速收敛。",
    "en_tdlr": "This study proves that for a new class of objective functions where only a subset of the parameters satisfies strong convexity, Nesterov's momentum achieves accelerated convergence in deep neural networks."
}