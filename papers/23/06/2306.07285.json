{
    "title": "TransCoder: Towards Unified Transferable Code Representation Learning Inspired by Human Skills. (arXiv:2306.07285v1 [cs.SE])",
    "abstract": "Code pre-trained models (CodePTMs) have recently demonstrated a solid capacity to process various software intelligence tasks, e.g., code clone detection, code translation, and code summarization. The current mainstream method that deploys these models to downstream tasks is to fine-tune them on individual tasks, which is generally costly and needs sufficient data for large models. To tackle the issue, in this paper, we present TransCoder, a unified Transferable fine-tuning strategy for Code representation learning. Inspired by human inherent skills of knowledge generalization, TransCoder drives the model to learn better code-related meta-knowledge like human programmers. Specifically, we employ a tunable prefix encoder as the meta-learner to capture cross-task and cross-language transferable knowledge, respectively. Besides, tasks with minor training sample sizes and languages with small corpus can be remarkably benefited from our approach. Extensive experiments conducted on benchmark",
    "link": "http://arxiv.org/abs/2306.07285",
    "context": "Title: TransCoder: Towards Unified Transferable Code Representation Learning Inspired by Human Skills. (arXiv:2306.07285v1 [cs.SE])\nAbstract: Code pre-trained models (CodePTMs) have recently demonstrated a solid capacity to process various software intelligence tasks, e.g., code clone detection, code translation, and code summarization. The current mainstream method that deploys these models to downstream tasks is to fine-tune them on individual tasks, which is generally costly and needs sufficient data for large models. To tackle the issue, in this paper, we present TransCoder, a unified Transferable fine-tuning strategy for Code representation learning. Inspired by human inherent skills of knowledge generalization, TransCoder drives the model to learn better code-related meta-knowledge like human programmers. Specifically, we employ a tunable prefix encoder as the meta-learner to capture cross-task and cross-language transferable knowledge, respectively. Besides, tasks with minor training sample sizes and languages with small corpus can be remarkably benefited from our approach. Extensive experiments conducted on benchmark",
    "path": "papers/23/06/2306.07285.json",
    "total_tokens": 1115,
    "translated_title": "TransCoder：受人类技能启发的统一可转移代码表示学习",
    "translated_abstract": "最近，代码预训练模型（CodePTMs）已经表现出在处理各种软件智能任务方面的扎实能力，例如，代码克隆检测、代码翻译和代码摘要。目前将这些模型部署到下游任务的主流方法是在单个任务上对它们进行微调，这通常是昂贵的，并且需要大型模型的充足数据。为了解决这个问题，我们提出了TransCoder，这是一种统一的可转移的代码表示学习任务的微调策略。受人类内在知识泛化技能的启发，TransCoder驱动模型像人类程序员一样学习更好的代码相关元知识。具体地，我们采用可调整的前缀编码器作为元学习器，分别捕捉跨任务和跨语言的可转移知识。此外，我们的方法可以remarkably地提高训练样本量较小和语料库较小的任务和语言的效果。",
    "tldr": "TransCoder是一种可转移的代码表示学习任务的微调策略，通过可调整的前缀编码器作为元学习器来捕捉跨任务和跨语言的可转移知识，使模型学习更好的代码相关元知识。此外，我们的方法可以remarkably地提高训练样本量较小和语料库较小的任务和语言的效果。"
}