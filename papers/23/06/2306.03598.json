{
    "title": "CUE: An Uncertainty Interpretation Framework for Text Classifiers Built on Pre-Trained Language Models. (arXiv:2306.03598v1 [cs.CL])",
    "abstract": "Text classifiers built on Pre-trained Language Models (PLMs) have achieved remarkable progress in various tasks including sentiment analysis, natural language inference, and question-answering. However, the occurrence of uncertain predictions by these classifiers poses a challenge to their reliability when deployed in practical applications. Much effort has been devoted to designing various probes in order to understand what PLMs capture. But few studies have delved into factors influencing PLM-based classifiers' predictive uncertainty. In this paper, we propose a novel framework, called CUE, which aims to interpret uncertainties inherent in the predictions of PLM-based models. In particular, we first map PLM-encoded representations to a latent space via a variational auto-encoder. We then generate text representations by perturbing the latent space which causes fluctuation in predictive uncertainty. By comparing the difference in predictive uncertainty between the perturbed and the or",
    "link": "http://arxiv.org/abs/2306.03598",
    "context": "Title: CUE: An Uncertainty Interpretation Framework for Text Classifiers Built on Pre-Trained Language Models. (arXiv:2306.03598v1 [cs.CL])\nAbstract: Text classifiers built on Pre-trained Language Models (PLMs) have achieved remarkable progress in various tasks including sentiment analysis, natural language inference, and question-answering. However, the occurrence of uncertain predictions by these classifiers poses a challenge to their reliability when deployed in practical applications. Much effort has been devoted to designing various probes in order to understand what PLMs capture. But few studies have delved into factors influencing PLM-based classifiers' predictive uncertainty. In this paper, we propose a novel framework, called CUE, which aims to interpret uncertainties inherent in the predictions of PLM-based models. In particular, we first map PLM-encoded representations to a latent space via a variational auto-encoder. We then generate text representations by perturbing the latent space which causes fluctuation in predictive uncertainty. By comparing the difference in predictive uncertainty between the perturbed and the or",
    "path": "papers/23/06/2306.03598.json",
    "total_tokens": 1218,
    "translated_title": "CUE: 基于预训练语言模型的文本分类器不确定性解释框架",
    "translated_abstract": "基于预训练语言模型（PLMs）构建的文本分类器在情感分析、自然语言推理和问答等各种任务中取得了显著的进展。然而，这些分类器产生不确定预测时，对于它们在实际应用中的可靠性构成了挑战。为了理解PLMs的学习特点，已经进行了大量的工作，但是很少有研究探究影响PLM模型预测不确定性的因素。本文提出了一种新的框架——CUE，旨在解释PLM模型预测中的不确定性。具体来说，我们首先通过变分自编码器将PLM编码表示映射到一个潜在空间中。然后通过扰动潜在空间生成文本表示，从而导致预测不确定性的波动。通过比较扰动和原始表示之间的预测不确定性差异，可以确定每个输入标记对总体预测不确定性的影响。此外，我们提出了一种新的不确定性感知微调算法，在训练过程中利用已识别的标记级不确定性从而进一步改善文本分类性能。我们的实验结果表明，CUE可以有效地揭示不确定性，并为理解基于PLM的文本分类器的行为提供有价值的见解。此外，我们的不确定性感知微调算法在几个基准数据集上显著优于传统的微调方法。",
    "tldr": "本文提出了一个框架CUE，它使用变分自编码器将预训练语言模型（PLMs）编码表示映射到潜在空间中，并通过扰动潜在空间生成不确定性预测，从而确定每个输入标记对总体预测不确定性的影响。同时，我们提出了一种不确定性感知微调算法，旨在利用标记级别的不确定性来提高文本分类器性能。",
    "en_tdlr": "This paper proposes a framework called CUE that aims to interpret uncertainties in the predictions of Pre-trained Language Model-based text classifiers, by mapping PLM-encoded representations to a latent space and generating text representations through perturbation, which helps to identify the contribution of each input token to overall predictive uncertainty. Additionally, an uncertainty-aware fine-tuning algorithm is proposed."
}