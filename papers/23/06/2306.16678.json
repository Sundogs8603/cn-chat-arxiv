{
    "title": "BinaryViT: Pushing Binary Vision Transformers Towards Convolutional Models. (arXiv:2306.16678v1 [cs.CV])",
    "abstract": "With the increasing popularity and the increasing size of vision transformers (ViTs), there has been an increasing interest in making them more efficient and less computationally costly for deployment on edge devices with limited computing resources. Binarization can be used to help reduce the size of ViT models and their computational cost significantly, using popcount operations when the weights and the activations are in binary. However, ViTs suffer a larger performance drop when directly applying convolutional neural network (CNN) binarization methods or existing binarization methods to binarize ViTs compared to CNNs on datasets with a large number of classes such as ImageNet-1k. With extensive analysis, we find that binary vanilla ViTs such as DeiT miss out on a lot of key architectural properties that CNNs have that allow binary CNNs to have much higher representational capability than binary vanilla ViT. Therefore, we propose BinaryViT, in which inspired by the CNN architecture,",
    "link": "http://arxiv.org/abs/2306.16678",
    "context": "Title: BinaryViT: Pushing Binary Vision Transformers Towards Convolutional Models. (arXiv:2306.16678v1 [cs.CV])\nAbstract: With the increasing popularity and the increasing size of vision transformers (ViTs), there has been an increasing interest in making them more efficient and less computationally costly for deployment on edge devices with limited computing resources. Binarization can be used to help reduce the size of ViT models and their computational cost significantly, using popcount operations when the weights and the activations are in binary. However, ViTs suffer a larger performance drop when directly applying convolutional neural network (CNN) binarization methods or existing binarization methods to binarize ViTs compared to CNNs on datasets with a large number of classes such as ImageNet-1k. With extensive analysis, we find that binary vanilla ViTs such as DeiT miss out on a lot of key architectural properties that CNNs have that allow binary CNNs to have much higher representational capability than binary vanilla ViT. Therefore, we propose BinaryViT, in which inspired by the CNN architecture,",
    "path": "papers/23/06/2306.16678.json",
    "total_tokens": 827,
    "translated_title": "BinaryViT：将二进制视觉Transformer推向卷积模型",
    "translated_abstract": "随着视觉Transformer（ViT）的日益流行和规模的增加，人们越来越关注如何使它们在计算资源有限的边缘设备上更高效、计算成本更低。通过使用二值化，在权重和激活值为二进制时可以显著减小ViT模型的大小和计算成本，使用popcount操作。然而，与CNN在具有大量类别的数据集（如ImageNet-1k）上直接应用卷积神经网络（CNN）二值化方法或现有的二值化方法相比，ViT的性能下降更大。经过广泛分析，我们发现，二值化的基础ViT（如DeiT）缺少许多CNN所具有的关键架构特性，这些特性使二值化的CNN具有比基础ViT更高的表示能力。因此，我们提出了BinaryViT，受CNN架构启发，",
    "tldr": "BinaryViT是一种针对二值化视觉Transformer的改进模型，通过借鉴CNN的架构特性，提高了二值化ViT的表示能力和性能。"
}