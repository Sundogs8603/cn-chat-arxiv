{
    "title": "NAR-Former V2: Rethinking Transformer for Universal Neural Network Representation Learning. (arXiv:2306.10792v2 [cs.LG] UPDATED)",
    "abstract": "As more deep learning models are being applied in real-world applications, there is a growing need for modeling and learning the representations of neural networks themselves. An efficient representation can be used to predict target attributes of networks without the need for actual training and deployment procedures, facilitating efficient network deployment and design. Recently, inspired by the success of Transformer, some Transformer-based representation learning frameworks have been proposed and achieved promising performance in handling cell-structured models. However, graph neural network (GNN) based approaches still dominate the field of learning representation for the entire network. In this paper, we revisit Transformer and compare it with GNN to analyse their different architecture characteristics. We then propose a modified Transformer-based universal neural network representation learning model NAR-Former V2. It can learn efficient representations from both cell-structured",
    "link": "http://arxiv.org/abs/2306.10792",
    "context": "Title: NAR-Former V2: Rethinking Transformer for Universal Neural Network Representation Learning. (arXiv:2306.10792v2 [cs.LG] UPDATED)\nAbstract: As more deep learning models are being applied in real-world applications, there is a growing need for modeling and learning the representations of neural networks themselves. An efficient representation can be used to predict target attributes of networks without the need for actual training and deployment procedures, facilitating efficient network deployment and design. Recently, inspired by the success of Transformer, some Transformer-based representation learning frameworks have been proposed and achieved promising performance in handling cell-structured models. However, graph neural network (GNN) based approaches still dominate the field of learning representation for the entire network. In this paper, we revisit Transformer and compare it with GNN to analyse their different architecture characteristics. We then propose a modified Transformer-based universal neural network representation learning model NAR-Former V2. It can learn efficient representations from both cell-structured",
    "path": "papers/23/06/2306.10792.json",
    "total_tokens": 814,
    "translated_title": "NAR-Former V2：重新思考Transformer用于通用神经网络表示学习",
    "translated_abstract": "随着越来越多的深度学习模型被应用于现实世界的应用，对于建模和学习神经网络本身的表示的需求也越来越大。高效的表示可以用于预测网络的目标属性，而无需实际训练和部署过程，从而促进高效的网络部署和设计。最近，受到Transformer的成功启发，一些基于Transformer的表示学习框架已被提出，并在处理基于细胞结构的模型方面取得了有希望的性能。然而，图神经网络（GNN）方法仍然主导了整个网络的学习表示领域。在本文中，我们重新审视Transformer，并与GNN进行比较，分析它们不同的架构特性。然后，我们提出了一个修改后的基于Transformer的通用神经网络表示学习模型NAR-Former V2。",
    "tldr": "本文重新思考了Transformer和图神经网络在网络表示学习中的不同特性，并提出了一个修改后的基于Transformer的通用神经网络表示学习模型NAR-Former V2。",
    "en_tdlr": "This paper rethinks the differences between Transformer and graph neural networks in network representation learning, and proposes a modified Transformer-based universal neural network representation learning model, NAR-Former V2."
}