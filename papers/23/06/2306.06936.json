{
    "title": "CARL-G: Clustering-Accelerated Representation Learning on Graphs. (arXiv:2306.06936v2 [cs.LG] UPDATED)",
    "abstract": "Self-supervised learning on graphs has made large strides in achieving great performance in various downstream tasks. However, many state-of-the-art methods suffer from a number of impediments, which prevent them from realizing their full potential. For instance, contrastive methods typically require negative sampling, which is often computationally costly. While non-contrastive methods avoid this expensive step, most existing methods either rely on overly complex architectures or dataset-specific augmentations. In this paper, we ask: Can we borrow from classical unsupervised machine learning literature in order to overcome those obstacles? Guided by our key insight that the goal of distance-based clustering closely resembles that of contrastive learning: both attempt to pull representations of similar items together and dissimilar items apart. As a result, we propose CARL-G - a novel clustering-based framework for graph representation learning that uses a loss inspired by Cluster Vali",
    "link": "http://arxiv.org/abs/2306.06936",
    "context": "Title: CARL-G: Clustering-Accelerated Representation Learning on Graphs. (arXiv:2306.06936v2 [cs.LG] UPDATED)\nAbstract: Self-supervised learning on graphs has made large strides in achieving great performance in various downstream tasks. However, many state-of-the-art methods suffer from a number of impediments, which prevent them from realizing their full potential. For instance, contrastive methods typically require negative sampling, which is often computationally costly. While non-contrastive methods avoid this expensive step, most existing methods either rely on overly complex architectures or dataset-specific augmentations. In this paper, we ask: Can we borrow from classical unsupervised machine learning literature in order to overcome those obstacles? Guided by our key insight that the goal of distance-based clustering closely resembles that of contrastive learning: both attempt to pull representations of similar items together and dissimilar items apart. As a result, we propose CARL-G - a novel clustering-based framework for graph representation learning that uses a loss inspired by Cluster Vali",
    "path": "papers/23/06/2306.06936.json",
    "total_tokens": 892,
    "translated_title": "CARL-G: 基于聚类加速的图表征学习",
    "translated_abstract": "自我监督学习在图上取得了巨大的进展，并在各种下游任务中实现了出色的性能。然而，许多最先进的方法都存在一些障碍，阻碍了它们充分发挥潜力。例如，对比方法通常需要负采样，这在计算上往往是昂贵的。虽然非对比方法避免了这一昂贵的步骤，但大多数现有方法要么依赖过于复杂的架构，要么依赖于特定数据集的增强。在本文中，我们提出了一个问题：我们是否可以借鉴经典的无监督机器学习文献，以克服这些障碍？在我们的关键洞察的指导下，距离聚类的目标与对比学习的目标非常相似：都试图将相似项的表示拉在一起，并将不相似项分开。因此，我们提出了CARL-G - 一个新颖的基于聚类的图表征学习框架，它使用了一个受集群验证启发的损失函数。",
    "tldr": "CARL-G是一个基于聚类的图表征学习框架，通过使用受集群验证启发的损失函数，克服了负采样和复杂架构的问题。",
    "en_tdlr": "CARL-G is a clustering-based framework for graph representation learning that overcomes the issues of negative sampling and complex architectures by using a loss function inspired by cluster validation."
}