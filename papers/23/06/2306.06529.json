{
    "title": "Neural Injective Functions for Multisets, Measures and Graphs via a Finite Witness Theorem. (arXiv:2306.06529v2 [cs.LG] UPDATED)",
    "abstract": "Injective multiset functions have a key role in the theoretical study of machine learning on multisets and graphs. Yet, there remains a gap between the provably injective multiset functions considered in theory, which typically rely on polynomial moments, and the multiset functions used in practice, which rely on $\\textit{neural moments}$ $\\unicode{x2014}$ whose injectivity on multisets has not been studied to date.  In this paper, we bridge this gap by showing that moments of neural networks do define injective multiset functions, provided that an analytic non-polynomial activation is used. The number of moments required by our theory is optimal essentially up to a multiplicative factor of two. To prove this result, we state and prove a $\\textit{finite witness theorem}$, which is of independent interest.  As a corollary to our main theorem, we derive new approximation results for functions on multisets and measures, and new separation results for graph neural networks. We also provide",
    "link": "http://arxiv.org/abs/2306.06529",
    "context": "Title: Neural Injective Functions for Multisets, Measures and Graphs via a Finite Witness Theorem. (arXiv:2306.06529v2 [cs.LG] UPDATED)\nAbstract: Injective multiset functions have a key role in the theoretical study of machine learning on multisets and graphs. Yet, there remains a gap between the provably injective multiset functions considered in theory, which typically rely on polynomial moments, and the multiset functions used in practice, which rely on $\\textit{neural moments}$ $\\unicode{x2014}$ whose injectivity on multisets has not been studied to date.  In this paper, we bridge this gap by showing that moments of neural networks do define injective multiset functions, provided that an analytic non-polynomial activation is used. The number of moments required by our theory is optimal essentially up to a multiplicative factor of two. To prove this result, we state and prove a $\\textit{finite witness theorem}$, which is of independent interest.  As a corollary to our main theorem, we derive new approximation results for functions on multisets and measures, and new separation results for graph neural networks. We also provide",
    "path": "papers/23/06/2306.06529.json",
    "total_tokens": 936,
    "translated_title": "通过有限证据定理，用于多重集合、度量和图的神经可逆函数",
    "translated_abstract": "可逆多重集合函数在多重集合和图的机器学习理论研究中起着关键作用。然而，目前理论上考虑的可证明可逆多重集合函数通常依赖于多项式矩，而实际中使用的多重集合函数依赖于尚未研究过在多重集合上的可逆神经矩。在本文中，我们通过展示神经网络的矩确实定义了可逆多重集合函数，前提是使用了一个解析的非多项式激活函数，从而弥合了这一差距。我们的理论所需的矩数量基本上是最优的，最多相差一个乘法因子为二。为了证明这一结果，我们提出并证明了一个独立引人注目的“有限证据定理”。作为我们主要定理的推论，我们推导了关于多重集合和度量函数的新近似结果，并得到了图神经网络的新的分离结果。",
    "tldr": "本文介绍了一种通过使用解析的非多项式激活函数，基于神经网络的矩来定义可逆多重集合函数的方法，并通过有限证据定理证明了其有效性。该方法在多重集合和图的机器学习中具有重要的应用价值。",
    "en_tdlr": "This paper presents a method for defining injective multiset functions using moments of neural networks with an analytic non-polynomial activation function. The effectiveness of this method is demonstrated through a finite witness theorem. This approach has important applications in machine learning on multisets and graphs."
}