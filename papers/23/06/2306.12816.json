{
    "title": "XAI-TRIS: Non-linear benchmarks to quantify ML explanation performance. (arXiv:2306.12816v1 [cs.LG])",
    "abstract": "The field of 'explainable' artificial intelligence (XAI) has produced highly cited methods that seek to make the decisions of complex machine learning (ML) methods 'understandable' to humans, for example by attributing 'importance' scores to input features. Yet, a lack of formal underpinning leaves it unclear as to what conclusions can safely be drawn from the results of a given XAI method and has also so far hindered the theoretical verification and empirical validation of XAI methods. This means that challenging non-linear problems, typically solved by deep neural networks, presently lack appropriate remedies. Here, we craft benchmark datasets for three different non-linear classification scenarios, in which the important class-conditional features are known by design, serving as ground truth explanations. Using novel quantitative metrics, we benchmark the explanation performance of a wide set of XAI methods across three deep learning model architectures. We show that popular XAI met",
    "link": "http://arxiv.org/abs/2306.12816",
    "context": "Title: XAI-TRIS: Non-linear benchmarks to quantify ML explanation performance. (arXiv:2306.12816v1 [cs.LG])\nAbstract: The field of 'explainable' artificial intelligence (XAI) has produced highly cited methods that seek to make the decisions of complex machine learning (ML) methods 'understandable' to humans, for example by attributing 'importance' scores to input features. Yet, a lack of formal underpinning leaves it unclear as to what conclusions can safely be drawn from the results of a given XAI method and has also so far hindered the theoretical verification and empirical validation of XAI methods. This means that challenging non-linear problems, typically solved by deep neural networks, presently lack appropriate remedies. Here, we craft benchmark datasets for three different non-linear classification scenarios, in which the important class-conditional features are known by design, serving as ground truth explanations. Using novel quantitative metrics, we benchmark the explanation performance of a wide set of XAI methods across three deep learning model architectures. We show that popular XAI met",
    "path": "papers/23/06/2306.12816.json",
    "total_tokens": 863,
    "translated_title": "XAI-TRIS：用于量化机器学习解释性能的非线性基准测试",
    "translated_abstract": "“可解释的”人工智能（XAI）领域已经产生了高度引用的方法，旨在使复杂的机器学习（ML）方法的决策“可理解”给人类，例如通过对输入特征进行“重要性”评分来实现。然而，缺乏正式的基础，使得无法从给定XAI方法的结果中安全地得出结论，迄今为止，也阻碍了XAI方法的理论验证和实证验证。这意味着，目前缺乏适当的解决方法来解决通常由深度神经网络解决的具有挑战性的非线性问题。本文针对三种不同的非线性分类情景制作基准数据集，其中通过设计已知重要的类条件特征，作为地面实况解释。利用新的定量指标，我们在三个深度学习模型架构上测试了广泛的XAI方法的解释性能。我们展示了流行的XAI方法的很多局限性。",
    "tldr": "XAI-TRIS提供了用于测试机器学习解释性能的具有挑战性的非线性基准数据集，并揭示了现有XAI方法的局限性。"
}