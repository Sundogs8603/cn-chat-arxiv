{
    "title": "Robustness Testing for Multi-Agent Reinforcement Learning: State Perturbations on Critical Agents. (arXiv:2306.06136v1 [cs.LG])",
    "abstract": "Multi-Agent Reinforcement Learning (MARL) has been widely applied in many fields such as smart traffic and unmanned aerial vehicles. However, most MARL algorithms are vulnerable to adversarial perturbations on agent states. Robustness testing for a trained model is an essential step for confirming the trustworthiness of the model against unexpected perturbations. This work proposes a novel Robustness Testing framework for MARL that attacks states of Critical Agents (RTCA). The RTCA has two innovations: 1) a Differential Evolution (DE) based method to select critical agents as victims and to advise the worst-case joint actions on them; and 2) a team cooperation policy evaluation method employed as the objective function for the optimization of DE. Then, adversarial state perturbations of the critical agents are generated based on the worst-case joint actions. This is the first robustness testing framework with varying victim agents. RTCA demonstrates outstanding performance in terms of ",
    "link": "http://arxiv.org/abs/2306.06136",
    "context": "Title: Robustness Testing for Multi-Agent Reinforcement Learning: State Perturbations on Critical Agents. (arXiv:2306.06136v1 [cs.LG])\nAbstract: Multi-Agent Reinforcement Learning (MARL) has been widely applied in many fields such as smart traffic and unmanned aerial vehicles. However, most MARL algorithms are vulnerable to adversarial perturbations on agent states. Robustness testing for a trained model is an essential step for confirming the trustworthiness of the model against unexpected perturbations. This work proposes a novel Robustness Testing framework for MARL that attacks states of Critical Agents (RTCA). The RTCA has two innovations: 1) a Differential Evolution (DE) based method to select critical agents as victims and to advise the worst-case joint actions on them; and 2) a team cooperation policy evaluation method employed as the objective function for the optimization of DE. Then, adversarial state perturbations of the critical agents are generated based on the worst-case joint actions. This is the first robustness testing framework with varying victim agents. RTCA demonstrates outstanding performance in terms of ",
    "path": "papers/23/06/2306.06136.json",
    "total_tokens": 994,
    "translated_title": "多智能体强化学习的鲁棒性测试：对关键智能体进行状态扰动",
    "translated_abstract": "多智能体强化学习（MARL）已经广泛应用于智能交通和无人机等许多领域。然而，大多数MARL算法容易受到智能体状态的对抗性扰动。对训练模型进行鲁棒性测试是确认模型在面对意外扰动时可信赖的基本步骤。本文提出一种针对MARL的新型鲁棒性测试框架，以攻击关键智能体的状态（RTCA）。RTCA有两个创新点：1）基于差分进化（DE）的方法选定关键的智能体作为受害者，并为它们提供最坏情况的联合动作建议；2）采用团队合作政策评估方法作为DE优化的目标函数。然后，基于最坏情况的联合动作生成关键智能体的对抗性状态扰动。这是第一个具有不同受害者智能体的鲁棒性测试框架。RTCA在鲁棒性测试中表现出了出色的性能。",
    "tldr": "提出了一个新的多智能体强化学习鲁棒性测试框架RTCA，采用基于差分进化的关键智能体攻击方法，关键智能体通过团队合作政策评估方法被选取为受害者。该框架在鲁棒性测试中表现出优异的性能。",
    "en_tdlr": "A new Robustness Testing framework (RTCA) for Multi-Agent Reinforcement Learning (MARL) is proposed, which attacks the states of critical agents using a Differential Evolution (DE) based method, and critical agents are selected as victims based on a team cooperation policy evaluation method. This framework demonstrates outstanding performance in robustness testing."
}