{
    "title": "Value-aware Importance Weighting for Off-policy Reinforcement Learning. (arXiv:2306.15625v1 [cs.LG])",
    "abstract": "Importance sampling is a central idea underlying off-policy prediction in reinforcement learning. It provides a strategy for re-weighting samples from a distribution to obtain unbiased estimates under another distribution. However, importance sampling weights tend to exhibit extreme variance, often leading to stability issues in practice. In this work, we consider a broader class of importance weights to correct samples in off-policy learning. We propose the use of $\\textit{value-aware importance weights}$ which take into account the sample space to provide lower variance, but still unbiased, estimates under a target distribution. We derive how such weights can be computed, and detail key properties of the resulting importance weights. We then extend several reinforcement learning prediction algorithms to the off-policy setting with these weights, and evaluate them empirically.",
    "link": "http://arxiv.org/abs/2306.15625",
    "context": "Title: Value-aware Importance Weighting for Off-policy Reinforcement Learning. (arXiv:2306.15625v1 [cs.LG])\nAbstract: Importance sampling is a central idea underlying off-policy prediction in reinforcement learning. It provides a strategy for re-weighting samples from a distribution to obtain unbiased estimates under another distribution. However, importance sampling weights tend to exhibit extreme variance, often leading to stability issues in practice. In this work, we consider a broader class of importance weights to correct samples in off-policy learning. We propose the use of $\\textit{value-aware importance weights}$ which take into account the sample space to provide lower variance, but still unbiased, estimates under a target distribution. We derive how such weights can be computed, and detail key properties of the resulting importance weights. We then extend several reinforcement learning prediction algorithms to the off-policy setting with these weights, and evaluate them empirically.",
    "path": "papers/23/06/2306.15625.json",
    "total_tokens": 864,
    "translated_title": "针对离策略强化学习的价值感知重要性加权",
    "translated_abstract": "重要性采样是强化学习中离策略预测的一个核心思想，它提供了一种策略，可以通过对一个分布中的样本进行重新加权，从而在另一个分布下获得无偏的估计值。然而，重要性采样权重往往会表现出极端的方差，常常导致实践中的稳定性问题。在这项工作中，我们考虑了一类更广泛的重要性权重来校正离策略学习中的样本。我们提出了使用“价值感知重要性权重”，它考虑了样本空间，从而在目标分布下提供更低的方差，但仍然是无偏的估计。我们推导了如何计算这样的权重，并详细说明了结果重要性权重的关键属性。然后，我们将几个强化学习预测算法扩展到使用这些权重的离策略设置，并进行了实证评估。",
    "tldr": "本文提出了一种称为“价值感知重要性权重”的方法，用于校正离策略学习中的样本。这种方法可以降低重要性采样权重的方差并保持无偏性，从而提高实践中的稳定性和效果。",
    "en_tdlr": "This paper proposes a method called \"value-aware importance weights\" to correct samples in off-policy learning, which reduces the variance of importance sampling weights and maintains unbiasedness, thus improving stability and effectiveness in practice."
}