{
    "title": "Accelerated Quasi-Newton Proximal Extragradient: Faster Rate for Smooth Convex Optimization. (arXiv:2306.02212v1 [math.OC])",
    "abstract": "In this paper, we propose an accelerated quasi-Newton proximal extragradient (A-QPNE) method for solving unconstrained smooth convex optimization problems. With access only to the gradients of the objective, we prove that our method can achieve a convergence rate of ${O}\\bigl(\\min\\{\\frac{1}{k^2}, \\frac{\\sqrt{d\\log k}}{k^{2.5}}\\}\\bigr)$, where $d$ is the problem dimension and $k$ is the number of iterations. In particular, in the regime where $k = {O}(d)$, our method matches the optimal rate of ${O}(\\frac{1}{k^2})$ by Nesterov's accelerated gradient (NAG). Moreover, in the the regime where $k = \\Omega(d \\log d)$, it outperforms NAG and converges at a faster rate of ${O}\\bigl(\\frac{\\sqrt{d\\log k}}{k^{2.5}}\\bigr)$. To the best of our knowledge, this result is the first to demonstrate a provable gain of a quasi-Newton-type method over NAG in the convex setting. To achieve such results, we build our method on a recent variant of the Monteiro-Svaiter acceleration framework and adopt an onlin",
    "link": "http://arxiv.org/abs/2306.02212",
    "context": "Title: Accelerated Quasi-Newton Proximal Extragradient: Faster Rate for Smooth Convex Optimization. (arXiv:2306.02212v1 [math.OC])\nAbstract: In this paper, we propose an accelerated quasi-Newton proximal extragradient (A-QPNE) method for solving unconstrained smooth convex optimization problems. With access only to the gradients of the objective, we prove that our method can achieve a convergence rate of ${O}\\bigl(\\min\\{\\frac{1}{k^2}, \\frac{\\sqrt{d\\log k}}{k^{2.5}}\\}\\bigr)$, where $d$ is the problem dimension and $k$ is the number of iterations. In particular, in the regime where $k = {O}(d)$, our method matches the optimal rate of ${O}(\\frac{1}{k^2})$ by Nesterov's accelerated gradient (NAG). Moreover, in the the regime where $k = \\Omega(d \\log d)$, it outperforms NAG and converges at a faster rate of ${O}\\bigl(\\frac{\\sqrt{d\\log k}}{k^{2.5}}\\bigr)$. To the best of our knowledge, this result is the first to demonstrate a provable gain of a quasi-Newton-type method over NAG in the convex setting. To achieve such results, we build our method on a recent variant of the Monteiro-Svaiter acceleration framework and adopt an onlin",
    "path": "papers/23/06/2306.02212.json",
    "total_tokens": 1159,
    "translated_title": "加速拟牛顿近端外推法：平滑凸优化更快的收敛率",
    "translated_abstract": "本文提出了一种加速拟牛顿近端外推（A-QPNE）方法，用于解决无约束平滑凸优化问题。仅利用目标函数的梯度，我们证明了我们的方法可以达到收敛速率${O}\\bigl(\\min\\{\\frac{1}{k^2}, \\frac{\\sqrt{d\\log k}}{k^{2.5}}\\}\\bigr)$，其中$d$是问题维度，$k$是迭代次数。特别地，在$k = {O}(d)$的情况下，我们的方法与Nesterov加速梯度（NAG）达到了$O(\\frac{1}{k^2})$的最优速率。此外，在$k = \\Omega(d \\log d)$的区域，我们的方法优于NAG，并以更快的速率${O}\\bigl(\\frac{\\sqrt{d\\log k}}{k^{2.5}}\\bigr)$收敛。据我们所知，这是首个在凸设置中证明拟牛顿类型方法比NAG有可证明优势的结果。为了实现这样的结果，我们基于Monteiro-Svaiter加速框架的最新变体构建了我们的方法，并采用了在线梯度估计技术。",
    "tldr": "本论文提出了一种加速拟牛顿近端外推的方法，用于解决无约束平滑凸优化问题，在$k = {O}(d)$时达到最优速率，并在$k = \\Omega(d \\log d)$时以更快的速率收敛。这是在凸设置中，首个证明拟牛顿类型方法比NAG有可证明优势的方法。"
}