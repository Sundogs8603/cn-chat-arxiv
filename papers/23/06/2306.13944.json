{
    "title": "Safe Reinforcement Learning with Dead-Ends Avoidance and Recovery. (arXiv:2306.13944v1 [cs.LG])",
    "abstract": "Safety is one of the main challenges in applying reinforcement learning to realistic environmental tasks. To ensure safety during and after training process, existing methods tend to adopt overly conservative policy to avoid unsafe situations. However, overly conservative policy severely hinders the exploration, and makes the algorithms substantially less rewarding. In this paper, we propose a method to construct a boundary that discriminates safe and unsafe states. The boundary we construct is equivalent to distinguishing dead-end states, indicating the maximum extent to which safe exploration is guaranteed, and thus has minimum limitation on exploration. Similar to Recovery Reinforcement Learning, we utilize a decoupled RL framework to learn two policies, (1) a task policy that only considers improving the task performance, and (2) a recovery policy that maximizes safety. The recovery policy and a corresponding safety critic are pretrained on an offline dataset, in which the safety c",
    "link": "http://arxiv.org/abs/2306.13944",
    "context": "Title: Safe Reinforcement Learning with Dead-Ends Avoidance and Recovery. (arXiv:2306.13944v1 [cs.LG])\nAbstract: Safety is one of the main challenges in applying reinforcement learning to realistic environmental tasks. To ensure safety during and after training process, existing methods tend to adopt overly conservative policy to avoid unsafe situations. However, overly conservative policy severely hinders the exploration, and makes the algorithms substantially less rewarding. In this paper, we propose a method to construct a boundary that discriminates safe and unsafe states. The boundary we construct is equivalent to distinguishing dead-end states, indicating the maximum extent to which safe exploration is guaranteed, and thus has minimum limitation on exploration. Similar to Recovery Reinforcement Learning, we utilize a decoupled RL framework to learn two policies, (1) a task policy that only considers improving the task performance, and (2) a recovery policy that maximizes safety. The recovery policy and a corresponding safety critic are pretrained on an offline dataset, in which the safety c",
    "path": "papers/23/06/2306.13944.json",
    "total_tokens": 998,
    "translated_title": "具有避开死局和恢复能力的安全强化学习",
    "translated_abstract": "安全性是将强化学习应用于现实环境任务时面临的主要挑战之一。为了确保在训练过程中和之后的安全性，现有的方法往往采用过于保守的策略以避免不安全的情况。但是，过于保守的策略严重阻碍了探索，使算法的回报大大降低。在本文中，我们提出了一种方法来构建一个边界，区分安全和不安全的状态。我们构建的边界等价于区分死局状态，表明安全探索的最大程度，因此在探索方面的限制最小。类似于恢复强化学习，我们利用一个分离的强化学习框架来学习两个策略，(1) 只考虑改善任务表现的任务策略，以及 (2) 最大化安全性的恢复策略。恢复策略和相应的安全性批判家在离线数据集上进行预训练，其中安全批判家会区分安全和不安全的状态，而恢复策略会采取措施以从不安全状态中恢复。",
    "tldr": "本文提出了一种利用死局的边界来辨别安全和不安全状态，以确保安全性同时减少对探索的限制的方法。采用了分离的强化学习框架，训练了两个策略：一个任务策略，专注于任务表现，以及一个恢复策略，最大化安全性。",
    "en_tdlr": "This paper proposes a method of using a dead-end boundary to distinguish safe and unsafe states, ensuring safety while reducing limitations on exploration. With a decoupled reinforcement learning framework, two policies are trained: a task policy that focuses on task performance and a recovery policy for maximized safety."
}