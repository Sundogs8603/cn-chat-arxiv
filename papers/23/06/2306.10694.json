{
    "title": "On the Model-Misspecification in Reinforcement Learning. (arXiv:2306.10694v2 [cs.LG] UPDATED)",
    "abstract": "The success of reinforcement learning (RL) crucially depends on effective function approximation when dealing with complex ground-truth models. Existing sample-efficient RL algorithms primarily employ three approaches to function approximation: policy-based, value-based, and model-based methods. However, in the face of model misspecification (a disparity between the ground-truth and optimal function approximators), it is shown that policy-based approaches can be robust even when the policy function approximation is under a large locally-bounded misspecification error, with which the function class may exhibit a $\\Omega(1)$ approximation error in specific states and actions, but remains small on average within a policy-induced state distribution. Yet it remains an open question whether similar robustness can be achieved with value-based and model-based approaches, especially with general function approximation.  To bridge this gap, in this paper we present a unified theoretical framewor",
    "link": "http://arxiv.org/abs/2306.10694",
    "context": "Title: On the Model-Misspecification in Reinforcement Learning. (arXiv:2306.10694v2 [cs.LG] UPDATED)\nAbstract: The success of reinforcement learning (RL) crucially depends on effective function approximation when dealing with complex ground-truth models. Existing sample-efficient RL algorithms primarily employ three approaches to function approximation: policy-based, value-based, and model-based methods. However, in the face of model misspecification (a disparity between the ground-truth and optimal function approximators), it is shown that policy-based approaches can be robust even when the policy function approximation is under a large locally-bounded misspecification error, with which the function class may exhibit a $\\Omega(1)$ approximation error in specific states and actions, but remains small on average within a policy-induced state distribution. Yet it remains an open question whether similar robustness can be achieved with value-based and model-based approaches, especially with general function approximation.  To bridge this gap, in this paper we present a unified theoretical framewor",
    "path": "papers/23/06/2306.10694.json",
    "total_tokens": 894,
    "translated_title": "关于强化学习中模型错误指定的研究",
    "translated_abstract": "强化学习的成功关键在于处理复杂的真实模型时的有效函数逼近。现有的样本有效的强化学习算法主要采用三种函数逼近方法：基于策略的方法、基于值的方法和基于模型的方法。然而，在面对模型错误指定的情况下（真实模型与最优函数逼近器之间的不一致），研究表明基于策略的方法即使在策略函数逼近存在较大局部有界错误的情况下也能保持稳健性。其中函数类可能在特定状态和动作下呈现$Ω(1)$的逼近误差，但在由策略引起的状态分布下保持较小的平均误差。然而，对于基于值和基于模型的方法在一般函数逼近中是否能实现类似的稳健性仍然是一个开放的问题。",
    "tldr": "这项研究研究了强化学习中模型错误指定问题，发现基于策略的方法在策略函数逼近存在较大误差的情况下仍然具有稳健性，但对于基于值和基于模型的方法在一般函数逼近中是否能实现类似的稳健性仍然存在疑问。",
    "en_tdlr": "This research investigates the problem of model misspecification in reinforcement learning and finds that policy-based approaches remain robust even in the presence of large errors in policy function approximation, while the robustness of value-based and model-based approaches in general function approximation is still uncertain."
}