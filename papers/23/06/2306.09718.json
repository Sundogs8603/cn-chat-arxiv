{
    "title": "Label-noise-tolerant medical image classification via self-attention and self-supervised learning. (arXiv:2306.09718v1 [cs.CV])",
    "abstract": "Deep neural networks (DNNs) have been widely applied in medical image classification and achieve remarkable classification performance. These achievements heavily depend on large-scale accurately annotated training data. However, label noise is inevitably introduced in the medical image annotation, as the labeling process heavily relies on the expertise and experience of annotators. Meanwhile, DNNs suffer from overfitting noisy labels, degrading the performance of models. Therefore, in this work, we innovatively devise noise-robust training approach to mitigate the adverse effects of noisy labels in medical image classification. Specifically, we incorporate contrastive learning and intra-group attention mixup strategies into the vanilla supervised learning. The contrastive learning for feature extractor helps to enhance visual representation of DNNs. The intra-group attention mixup module constructs groups and assigns self-attention weights for group-wise samples, and subsequently inte",
    "link": "http://arxiv.org/abs/2306.09718",
    "context": "Title: Label-noise-tolerant medical image classification via self-attention and self-supervised learning. (arXiv:2306.09718v1 [cs.CV])\nAbstract: Deep neural networks (DNNs) have been widely applied in medical image classification and achieve remarkable classification performance. These achievements heavily depend on large-scale accurately annotated training data. However, label noise is inevitably introduced in the medical image annotation, as the labeling process heavily relies on the expertise and experience of annotators. Meanwhile, DNNs suffer from overfitting noisy labels, degrading the performance of models. Therefore, in this work, we innovatively devise noise-robust training approach to mitigate the adverse effects of noisy labels in medical image classification. Specifically, we incorporate contrastive learning and intra-group attention mixup strategies into the vanilla supervised learning. The contrastive learning for feature extractor helps to enhance visual representation of DNNs. The intra-group attention mixup module constructs groups and assigns self-attention weights for group-wise samples, and subsequently inte",
    "path": "papers/23/06/2306.09718.json",
    "total_tokens": 1044,
    "translated_title": "自我注意力与自监督学习在医学图像分类领域中的标签噪声容忍方法研究",
    "translated_abstract": "深度神经网络（DNN）已广泛应用于医学图像分类，并取得了显著的分类性能。然而，这些成就严重依赖于大规模准确注释的训练数据。然而，标签噪声不可避免地会在医学图像注释过程中被引入，因为标注过程严重依赖于注释人员的专业知识和经验。同时，DNN容易过拟合噪声标签，从而降低模型性能。因此，本文创新性地提出了一种抗噪声训练方法，以缓解医学图像分类中噪声标签的不利影响。具体而言，我们将对比学习和组内注意力混合策略纳入基础监督学习。特征提取器的对比学习有助于增强DNN的视觉表示能力。组内注意力混合模块构建组并为组内样本分配自注意力权重，随后整合加权特征以减少标签噪声的影响。在多个医学图像数据集上的实验表明，该方法显著提高了DNN对标签噪声的鲁棒性和准确性。",
    "tldr": "本文提出结合对比学习和自我注意力混合策略的抗噪声训练方法，以缓解医学图像分类中噪声标签的影响，实验证明该方法显著提高了DNN对标签噪声的鲁棒性和准确性。",
    "en_tdlr": "This paper proposes a noise-robust training approach in medical image classification which incorporates contrastive learning and intra-group attention mixup strategies to mitigate the adverse effects of label noise. Experimental results demonstrate significant improvement in the robustness and accuracy of DNNs against label noise."
}