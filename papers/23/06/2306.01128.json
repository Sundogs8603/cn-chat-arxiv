{
    "title": "Learning Transformer Programs. (arXiv:2306.01128v1 [cs.LG])",
    "abstract": "Recent research in mechanistic interpretability has attempted to reverse-engineer Transformer models by carefully inspecting network weights and activations. However, these approaches require considerable manual effort and still fall short of providing complete, faithful descriptions of the underlying algorithms. In this work, we introduce a procedure for training Transformers that are mechanistically interpretable by design. We build on RASP [Weiss et al., 2021], a programming language that can be compiled into Transformer weights. Instead of compiling human-written programs into Transformers, we design a modified Transformer that can be trained using gradient-based optimization and then be automatically converted into a discrete, human-readable program. We refer to these models as Transformer Programs. To validate our approach, we learn Transformer Programs for a variety of problems, including an in-context learning task, a suite of algorithmic problems (e.g. sorting, recognizing Dyc",
    "link": "http://arxiv.org/abs/2306.01128",
    "context": "Title: Learning Transformer Programs. (arXiv:2306.01128v1 [cs.LG])\nAbstract: Recent research in mechanistic interpretability has attempted to reverse-engineer Transformer models by carefully inspecting network weights and activations. However, these approaches require considerable manual effort and still fall short of providing complete, faithful descriptions of the underlying algorithms. In this work, we introduce a procedure for training Transformers that are mechanistically interpretable by design. We build on RASP [Weiss et al., 2021], a programming language that can be compiled into Transformer weights. Instead of compiling human-written programs into Transformers, we design a modified Transformer that can be trained using gradient-based optimization and then be automatically converted into a discrete, human-readable program. We refer to these models as Transformer Programs. To validate our approach, we learn Transformer Programs for a variety of problems, including an in-context learning task, a suite of algorithmic problems (e.g. sorting, recognizing Dyc",
    "path": "papers/23/06/2306.01128.json",
    "total_tokens": 816,
    "tldr": "本文介绍了一种经过设计具有机械式解释功能的Transformer程序的训练方法，相比于其他方法，它们可以自动转换成离散和人类可读的程序。",
    "en_tdlr": "This paper introduces a training method for Transformer Programs that are designed to have mechanistic interpretability. In contrast to other methods, they can be automatically converted into discrete, human-readable programs."
}