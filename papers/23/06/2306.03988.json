{
    "title": "Learn the Force We Can: Multi-Object Video Generation from Pixel-Level Interactions. (arXiv:2306.03988v1 [cs.CV])",
    "abstract": "We propose a novel unsupervised method to autoregressively generate videos from a single frame and a sparse motion input. Our trained model can generate realistic object-to-object interactions and separate the dynamics and the extents of multiple objects despite only observing them under correlated motion activities. Key components in our method are the randomized conditioning scheme, the encoding of the input motion control, and the randomized and sparse sampling to break correlations. Our model, which we call YODA, has the ability to move objects without physically touching them. We show both qualitatively and quantitatively that YODA accurately follows the user control, while yielding a video quality that is on par with or better than state of the art video generation prior work on several datasets. For videos, visit our project website https://araachie.github.io/yoda.",
    "link": "http://arxiv.org/abs/2306.03988",
    "context": "Title: Learn the Force We Can: Multi-Object Video Generation from Pixel-Level Interactions. (arXiv:2306.03988v1 [cs.CV])\nAbstract: We propose a novel unsupervised method to autoregressively generate videos from a single frame and a sparse motion input. Our trained model can generate realistic object-to-object interactions and separate the dynamics and the extents of multiple objects despite only observing them under correlated motion activities. Key components in our method are the randomized conditioning scheme, the encoding of the input motion control, and the randomized and sparse sampling to break correlations. Our model, which we call YODA, has the ability to move objects without physically touching them. We show both qualitatively and quantitatively that YODA accurately follows the user control, while yielding a video quality that is on par with or better than state of the art video generation prior work on several datasets. For videos, visit our project website https://araachie.github.io/yoda.",
    "path": "papers/23/06/2306.03988.json",
    "total_tokens": 865,
    "translated_title": "学习我们能够掌握的力量：基于像素级交互的多目标视频生成",
    "translated_abstract": "我们提出了一种新颖的无监督方法，通过单帧图像与稀疏运动输入来自我回归生成视频。我们训练的模型能够生成逼真的物体间相互作用，并在仅观测到它们在相关运动活动下时分离多个物体的动态和范围。我们方法的关键组件是随机化条件方案、输入运动控制的编码以及随机化和稀疏采样来打破相关性。我们称之为YODA的模型具有能够移动物体而无需实际触摸的能力。我们定量和定性地展示，YODA能够准确跟随用户的控制，并在多个数据集上呈现出与现有最先进的视频生成先前工作相媲美甚至更好的视频质量。详情请参阅我们的项目网站 https://araachie.github.io/yoda。",
    "tldr": "本论文提出了一种基于像素级交互的多目标视频生成方法，能够生成逼真的物体间相互作用的视频，且能准确跟随用户的控制，达到了最先进的视频生成先前工作相媲美甚至更好的效果。",
    "en_tdlr": "This paper proposes a novel unsupervised method for autoregressively generating videos with realistic object-to-object interactions and the ability to accurately follow user control. The method achieves state-of-the-art or better video quality on several datasets."
}