{
    "title": "Reinforcement Learning with General Utilities: Simpler Variance Reduction and Large State-Action Space. (arXiv:2306.01854v1 [cs.LG])",
    "abstract": "We consider the reinforcement learning (RL) problem with general utilities which consists in maximizing a function of the state-action occupancy measure. Beyond the standard cumulative reward RL setting, this problem includes as particular cases constrained RL, pure exploration and learning from demonstrations among others. For this problem, we propose a simpler single-loop parameter-free normalized policy gradient algorithm. Implementing a recursive momentum variance reduction mechanism, our algorithm achieves $\\tilde{\\mathcal{O}}(\\epsilon^{-3})$ and $\\tilde{\\mathcal{O}}(\\epsilon^{-2})$ sample complexities for $\\epsilon$-first-order stationarity and $\\epsilon$-global optimality respectively, under adequate assumptions. We further address the setting of large finite state action spaces via linear function approximation of the occupancy measure and show a $\\tilde{\\mathcal{O}}(\\epsilon^{-4})$ sample complexity for a simple policy gradient method with a linear regression subroutine.",
    "link": "http://arxiv.org/abs/2306.01854",
    "context": "Title: Reinforcement Learning with General Utilities: Simpler Variance Reduction and Large State-Action Space. (arXiv:2306.01854v1 [cs.LG])\nAbstract: We consider the reinforcement learning (RL) problem with general utilities which consists in maximizing a function of the state-action occupancy measure. Beyond the standard cumulative reward RL setting, this problem includes as particular cases constrained RL, pure exploration and learning from demonstrations among others. For this problem, we propose a simpler single-loop parameter-free normalized policy gradient algorithm. Implementing a recursive momentum variance reduction mechanism, our algorithm achieves $\\tilde{\\mathcal{O}}(\\epsilon^{-3})$ and $\\tilde{\\mathcal{O}}(\\epsilon^{-2})$ sample complexities for $\\epsilon$-first-order stationarity and $\\epsilon$-global optimality respectively, under adequate assumptions. We further address the setting of large finite state action spaces via linear function approximation of the occupancy measure and show a $\\tilde{\\mathcal{O}}(\\epsilon^{-4})$ sample complexity for a simple policy gradient method with a linear regression subroutine.",
    "path": "papers/23/06/2306.01854.json",
    "total_tokens": 945,
    "translated_title": "带有通用效用的强化学习：简单的方差缩减和大状态行动空间",
    "translated_abstract": "我们考虑具有通用效用的强化学习问题，其中包括最大化状态-动作占用度量函数。除了标准的累积奖励RL设置外，这个问题包括约束RL，纯探索和从演示中学习等特定情况。为这个问题，我们提出了一种更简单、单循环、无参数的归一化策略梯度算法。我们的算法实现了递归动量方差缩减机制，在适当的假设下，实现了$\\tilde{\\mathcal{O}}(\\epsilon^{-3})$和$\\tilde{\\mathcal{O}}(\\epsilon^{-2})$的样本复杂度，用于$\\epsilon$-一阶稳定性和$\\epsilon$-全局最优性。我们还通过占用度量的线性函数逼近解决了大有限状态行动空间的设置，并展示了一个使用线性回归子程序的简单策略梯度方法的$\\tilde{\\mathcal{O}}(\\epsilon^{-4})$的样本复杂度。",
    "tldr": "本文提出了一种适用于通用效用的强化学习算法，具有更简单、无参数的归一化策略梯度算法。算法包括递归动量方差缩减机制，针对大有限状态行动空间提出了线性函数逼近方法，具有较低的样本复杂度。"
}