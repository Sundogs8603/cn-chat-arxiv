{
    "title": "Logic Diffusion for Knowledge Graph Reasoning. (arXiv:2306.03515v1 [cs.LG])",
    "abstract": "Most recent works focus on answering first order logical queries to explore the knowledge graph reasoning via multi-hop logic predictions. However, existing reasoning models are limited by the circumscribed logical paradigms of training samples, which leads to a weak generalization of unseen logic. To address these issues, we propose a plug-in module called Logic Diffusion (LoD) to discover unseen queries from surroundings and achieves dynamical equilibrium between different kinds of patterns. The basic idea of LoD is relation diffusion and sampling sub-logic by random walking as well as a special training mechanism called gradient adaption. Besides, LoD is accompanied by a novel loss function to further achieve the robust logical diffusion when facing noisy data in training or testing sets. Extensive experiments on four public datasets demonstrate the superiority of mainstream knowledge graph reasoning models with LoD over state-of-the-art. Moreover, our ablation study proves the gene",
    "link": "http://arxiv.org/abs/2306.03515",
    "context": "Title: Logic Diffusion for Knowledge Graph Reasoning. (arXiv:2306.03515v1 [cs.LG])\nAbstract: Most recent works focus on answering first order logical queries to explore the knowledge graph reasoning via multi-hop logic predictions. However, existing reasoning models are limited by the circumscribed logical paradigms of training samples, which leads to a weak generalization of unseen logic. To address these issues, we propose a plug-in module called Logic Diffusion (LoD) to discover unseen queries from surroundings and achieves dynamical equilibrium between different kinds of patterns. The basic idea of LoD is relation diffusion and sampling sub-logic by random walking as well as a special training mechanism called gradient adaption. Besides, LoD is accompanied by a novel loss function to further achieve the robust logical diffusion when facing noisy data in training or testing sets. Extensive experiments on four public datasets demonstrate the superiority of mainstream knowledge graph reasoning models with LoD over state-of-the-art. Moreover, our ablation study proves the gene",
    "path": "papers/23/06/2306.03515.json",
    "total_tokens": 1105,
    "translated_title": "知识图谱推理的逻辑扩散",
    "translated_abstract": "最近的研究集中于回答一阶逻辑查询，通过多跳逻辑预测来探索知识图谱推理。然而，现有的推理模型受到训练样本所围绕的逻辑范式的限制，导致在未见逻辑推理上表现还不够强。为了解决这些问题，我们提出了一个名为逻辑扩散（LoD）的插件模块，能够从周围环境中发现未见查询，并实现不同模式之间的动态平衡。LoD的基本思想是关系扩散和随机游走子逻辑采样以及一种特殊的训练机制——梯度自适应。此外，LoD还配备了一种新颖的损失函数，以进一步在训练或测试集中应对嘈杂数据时实现稳健的逻辑扩散。在四个公共数据集上的大量实验证明，带有LoD的主流知识图谱推理模型优于最先进的模型。此外，我们的消融研究证明了逻辑扩散在克服现有推理模型的局限性和实现更好的未见逻辑推理方面的潜力。",
    "tldr": "该篇论文提出了一种名为逻辑扩散（LoD）的插件模块，解决了现有推理模型受训练样本限制、表现不够强的问题。LoD通过关系扩散、随机游走子逻辑采样和梯度自适应等方式实现了对未见查询的发现和不同模式之间的动态平衡，并配备了特殊的损失函数以实现稳健的逻辑扩散。",
    "en_tdlr": "This paper proposes a plug-in module called Logic Diffusion (LoD) to overcome the limitations of existing knowledge graph reasoning models. By relation diffusion, random walking sub-logic sampling, and gradient adaption training mechanism, LoD discovers unseen queries and achieves dynamic equilibrium between different patterns. It is also equipped with a novel loss function to deal with noisy data in training or testing sets. Results from extensive experiments on four public datasets demonstrate the superiority of LoD over state-of-the-art."
}