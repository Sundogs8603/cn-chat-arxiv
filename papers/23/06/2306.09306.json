{
    "title": "Propagating Knowledge Updates to LMs Through Distillation. (arXiv:2306.09306v2 [cs.CL] UPDATED)",
    "abstract": "Modern language models have the capacity to store and use immense amounts of knowledge about real-world entities, but it remains unclear how to update such knowledge stored in model parameters. While prior methods for updating knowledge in LMs successfully inject atomic facts, updated LMs fail to make inferences based on injected facts. In this work, we demonstrate that a context distillation-based approach can both impart knowledge about entities and propagate that knowledge to enable broader inferences. Our approach consists of two stages: transfer set generation and distillation on the transfer set. We first generate a transfer set by prompting a language model to generate continuations from the entity definition. Then, we update the model parameters so that the distribution of the LM (the student) matches the distribution of the LM conditioned on the definition (the teacher) on the transfer set. Our experiments demonstrate that this approach is more effective at propagating knowled",
    "link": "http://arxiv.org/abs/2306.09306",
    "context": "Title: Propagating Knowledge Updates to LMs Through Distillation. (arXiv:2306.09306v2 [cs.CL] UPDATED)\nAbstract: Modern language models have the capacity to store and use immense amounts of knowledge about real-world entities, but it remains unclear how to update such knowledge stored in model parameters. While prior methods for updating knowledge in LMs successfully inject atomic facts, updated LMs fail to make inferences based on injected facts. In this work, we demonstrate that a context distillation-based approach can both impart knowledge about entities and propagate that knowledge to enable broader inferences. Our approach consists of two stages: transfer set generation and distillation on the transfer set. We first generate a transfer set by prompting a language model to generate continuations from the entity definition. Then, we update the model parameters so that the distribution of the LM (the student) matches the distribution of the LM conditioned on the definition (the teacher) on the transfer set. Our experiments demonstrate that this approach is more effective at propagating knowled",
    "path": "papers/23/06/2306.09306.json",
    "total_tokens": 810,
    "translated_title": "通过蒸馏将知识更新传播到语言模型中",
    "translated_abstract": "现代语言模型可以存储和使用大量关于现实世界实体的知识，但如何更新存储在模型参数中的知识尚不清楚。尽管先前的语言模型知识更新方法成功地注入了原子事实，但更新后的语言模型无法根据注入的事实进行推理。在这项工作中，我们证明了基于上下文蒸馏的方法可以在提供关于实体的知识的同时传播该知识以实现更广泛的推理。我们的方法由两个阶段组成：传输集生成和传输集上的蒸馏。我们首先通过提示语言模型从实体定义中生成延续来生成一个传输集。然后，我们更新模型参数，使得语言模型的分布（学生）与在传输集上给定定义条件下的语言模型的分布（教师）相匹配。我们的实验表明，这种方法在传播知识方面更有效。",
    "tldr": "本研究通过上下文蒸馏的方法成功将知识更新传播到语言模型中，实现了更广泛的推理能力。",
    "en_tdlr": "This research successfully propagates knowledge updates to language models through a context distillation-based approach, enabling broader inference capabilities."
}