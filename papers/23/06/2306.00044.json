{
    "title": "How to Construct Perfect and Worse-than-Coin-Flip Spoofing Countermeasures: A Word of Warning on Shortcut Learning. (arXiv:2306.00044v1 [cs.LG])",
    "abstract": "Shortcut learning, or `Clever Hans effect` refers to situations where a learning agent (e.g., deep neural networks) learns spurious correlations present in data, resulting in biased models. We focus on finding shortcuts in deep learning based spoofing countermeasures (CMs) that predict whether a given utterance is spoofed or not. While prior work has addressed specific data artifacts, such as silence, no general normative framework has been explored for analyzing shortcut learning in CMs. In this study, we propose a generic approach to identifying shortcuts by introducing systematic interventions on the training and test sides, including the boundary cases of `near-perfect` and `worse than coin flip` (label flip). By using three different models, ranging from classic to state-of-the-art, we demonstrate the presence of shortcut learning in five simulated conditions. We analyze the results using a regression model to understand how biases affect the class-conditional score statistics.",
    "link": "http://arxiv.org/abs/2306.00044",
    "context": "Title: How to Construct Perfect and Worse-than-Coin-Flip Spoofing Countermeasures: A Word of Warning on Shortcut Learning. (arXiv:2306.00044v1 [cs.LG])\nAbstract: Shortcut learning, or `Clever Hans effect` refers to situations where a learning agent (e.g., deep neural networks) learns spurious correlations present in data, resulting in biased models. We focus on finding shortcuts in deep learning based spoofing countermeasures (CMs) that predict whether a given utterance is spoofed or not. While prior work has addressed specific data artifacts, such as silence, no general normative framework has been explored for analyzing shortcut learning in CMs. In this study, we propose a generic approach to identifying shortcuts by introducing systematic interventions on the training and test sides, including the boundary cases of `near-perfect` and `worse than coin flip` (label flip). By using three different models, ranging from classic to state-of-the-art, we demonstrate the presence of shortcut learning in five simulated conditions. We analyze the results using a regression model to understand how biases affect the class-conditional score statistics.",
    "path": "papers/23/06/2306.00044.json",
    "total_tokens": 993,
    "translated_title": "如何构建完美的反欺骗对策及比瞎猜还差的对策：关于快捷学习的警告（arXiv:2306.00044v1 [cs.LG]）",
    "translated_abstract": "快捷学习，或称为“聪明汉斯效应”，指的是学习代理（例如深度神经网络）学习数据中存在的伪相关性，导致有偏差的模型。我们在寻找深度学习为基础的反欺骗对策（CMs）中发现了快捷方式，这些对策用于预测给定话语是否被欺骗。虽然先前的工作已经涉及特定的数据伪装技巧，例如静默，但尚未探索分析CMs的快捷学习的通用规范框架。在本研究中，我们提出了一种通用方法来识别快捷方式，包括对训练和测试方面的系统干预，包括“近乎完美”和“比瞎猜还差”（标签翻转）的边界情况。通过使用从经典到最先进的三种不同模型，我们展示了在五个模拟条件下存在快捷学习的情况。我们使用回归模型分析结果，以了解偏差如何影响类条件得分统计信息。",
    "tldr": "本研究提出了一种通用方法来识别深度学习为基础的反欺骗对策中的快捷方式，并在实验中证明了快捷方式的存在，分析了如何影响类条件得分统计信息。",
    "en_tdlr": "This study proposes a generic approach to identifying shortcuts in deep learning based spoofing countermeasures and demonstrates their presence in five simulated conditions, with analysis of the impact of biases on class-conditional score statistics."
}