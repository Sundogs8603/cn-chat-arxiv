{
    "title": "MobileNMT: Enabling Translation in 15MB and 30ms. (arXiv:2306.04235v1 [cs.AI])",
    "abstract": "Deploying NMT models on mobile devices is essential for privacy, low latency, and offline scenarios. For high model capacity, NMT models are rather large. Running these models on devices is challenging with limited storage, memory, computation, and power consumption. Existing work either only focuses on a single metric such as FLOPs or general engine which is not good at auto-regressive decoding. In this paper, we present MobileNMT, a system that can translate in 15MB and 30ms on devices. We propose a series of principles for model compression when combined with quantization. Further, we implement an engine that is friendly to INT8 and decoding. With the co-design of model and engine, compared with the existing system, we speed up 47.0x and save 99.5% of memory with only 11.6% loss of BLEU. The code is publicly available at https://github.com/zjersey/Lightseq-ARM.",
    "link": "http://arxiv.org/abs/2306.04235",
    "context": "Title: MobileNMT: Enabling Translation in 15MB and 30ms. (arXiv:2306.04235v1 [cs.AI])\nAbstract: Deploying NMT models on mobile devices is essential for privacy, low latency, and offline scenarios. For high model capacity, NMT models are rather large. Running these models on devices is challenging with limited storage, memory, computation, and power consumption. Existing work either only focuses on a single metric such as FLOPs or general engine which is not good at auto-regressive decoding. In this paper, we present MobileNMT, a system that can translate in 15MB and 30ms on devices. We propose a series of principles for model compression when combined with quantization. Further, we implement an engine that is friendly to INT8 and decoding. With the co-design of model and engine, compared with the existing system, we speed up 47.0x and save 99.5% of memory with only 11.6% loss of BLEU. The code is publicly available at https://github.com/zjersey/Lightseq-ARM.",
    "path": "papers/23/06/2306.04235.json",
    "total_tokens": 977,
    "translated_title": "MobileNMT：在15MB和30ms内实现翻译",
    "translated_abstract": "在隐私、低延迟和离线场景下，将NMT模型部署在移动设备上是必不可少的。然而，由于NMT模型的容量较大，在设备上运行这些模型面临着存储、内存、计算和功耗等方面的挑战。本文提出了一个名为MobileNMT的系统，它可以在设备上实现15MB和30ms的翻译。我们提出了一系列模型压缩的原则，并与量化相结合。此外，我们设计了一个朋友INT8和解码的引擎。通过模型和引擎的协同设计，与现有系统相比，我们的系统加速了47.0倍，节省了99.5%的内存，仅损失了11.6%的BLEU。代码可在 https://github.com/zjersey/Lightseq-ARM 上公开获取。",
    "tldr": "MobileNMT是一个可以在移动设备上实现15MB和30ms翻译的系统，通过一系列模型压缩原则和朋友INT8和解码的引擎的协同设计，它成功解决了NMT模型在移动设备上存储、内存、计算和功耗等方面的挑战，且其速度提升了47.0倍，节省了99.5%的内存，但仅损失了11.6%的BLEU。",
    "en_tdlr": "MobileNMT is a system that enables translation on mobile devices in 15MB and 30ms, which addresses the challenges of large model capacity, limited storage, memory, computation, and power consumption by proposing principles for model compression combined with quantization and implementing an engine friendly to INT8 and decoding. With a co-design of model and engine, MobileNMT achieves a speedup of 47.0x and saves 99.5% of memory with only 11.6% loss of BLEU, compared with existing systems."
}