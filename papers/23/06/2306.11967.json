{
    "title": "Complementary Learning Subnetworks for Parameter-Efficient Class-Incremental Learning. (arXiv:2306.11967v1 [cs.LG])",
    "abstract": "In the scenario of class-incremental learning (CIL), deep neural networks have to adapt their model parameters to non-stationary data distributions, e.g., the emergence of new classes over time. However, CIL models are challenged by the well-known catastrophic forgetting phenomenon. Typical methods such as rehearsal-based ones rely on storing exemplars of old classes to mitigate catastrophic forgetting, which limits real-world applications considering memory resources and privacy issues. In this paper, we propose a novel rehearsal-free CIL approach that learns continually via the synergy between two Complementary Learning Subnetworks. Our approach involves jointly optimizing a plastic CNN feature extractor and an analytical feed-forward classifier. The inaccessibility of historical data is tackled by holistically controlling the parameters of a well-trained model, ensuring that the decision boundary learned fits new classes while retaining recognition of previously learned classes. Spe",
    "link": "http://arxiv.org/abs/2306.11967",
    "context": "Title: Complementary Learning Subnetworks for Parameter-Efficient Class-Incremental Learning. (arXiv:2306.11967v1 [cs.LG])\nAbstract: In the scenario of class-incremental learning (CIL), deep neural networks have to adapt their model parameters to non-stationary data distributions, e.g., the emergence of new classes over time. However, CIL models are challenged by the well-known catastrophic forgetting phenomenon. Typical methods such as rehearsal-based ones rely on storing exemplars of old classes to mitigate catastrophic forgetting, which limits real-world applications considering memory resources and privacy issues. In this paper, we propose a novel rehearsal-free CIL approach that learns continually via the synergy between two Complementary Learning Subnetworks. Our approach involves jointly optimizing a plastic CNN feature extractor and an analytical feed-forward classifier. The inaccessibility of historical data is tackled by holistically controlling the parameters of a well-trained model, ensuring that the decision boundary learned fits new classes while retaining recognition of previously learned classes. Spe",
    "path": "papers/23/06/2306.11967.json",
    "total_tokens": 900,
    "translated_title": "基于互补学习子网络的参数高效类增量学习",
    "translated_abstract": "在类增量学习(CIL)的场景下，深度神经网络必须适应非静态数据分布，例如随时间出现的新类。然而，CIL模型面临着臭名昭著的灾难性遗忘现象。传统的基于重演的方法依赖于存储旧类的样本来缓解灾难性遗忘，这限制了考虑内存资源和隐私问题的实际应用。本文提出了一种新颖的无重演CIL方法，通过两个互补学习子网络之间的协同学习不断地进行学习。我们的方法涉及联合优化可塑性卷积神经网络特征提取器和分析前馈分类器。历史数据的不可访问性通过完整地控制训练良好的模型的参数来处理，确保所学习的决策边界适合新类，同时保留先前学习的类的识别能力。",
    "tldr": "本文提出了一种基于互补学习子网络的无重演类增量学习方法，通过联合优化可塑性CNN特征提取器和分析前馈分类器来达到在不访问历史数据的情况下缓解灾难性遗忘的目的。",
    "en_tdlr": "This paper proposes a novel rehearsal-free approach for class-incremental learning based on the synergy between two Complementary Learning Subnetworks, which tackles the catastrophic forgetting phenomenon by holistically controlling the parameters of a well-trained model and achieves efficient learning without accessing historical data."
}