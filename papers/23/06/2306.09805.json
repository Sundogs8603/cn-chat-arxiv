{
    "title": "Sample-Efficient On-Policy Imitation Learning from Observations. (arXiv:2306.09805v1 [cs.LG])",
    "abstract": "Imitation learning from demonstrations (ILD) aims to alleviate numerous shortcomings of reinforcement learning through the use of demonstrations. However, in most real-world applications, expert action guidance is absent, making the use of ILD impossible. Instead, we consider imitation learning from observations (ILO), where no expert actions are provided, making it a significantly more challenging problem to address. Existing methods often employ on-policy learning, which is known to be sample-costly. This paper presents SEILO, a novel sample-efficient on-policy algorithm for ILO, that combines standard adversarial imitation learning with inverse dynamics modeling. This approach enables the agent to receive feedback from both the adversarial procedure and a behavior cloning loss. We empirically demonstrate that our proposed algorithm requires fewer interactions with the environment to achieve expert performance compared to other state-of-the-art on-policy ILO and ILD methods.",
    "link": "http://arxiv.org/abs/2306.09805",
    "context": "Title: Sample-Efficient On-Policy Imitation Learning from Observations. (arXiv:2306.09805v1 [cs.LG])\nAbstract: Imitation learning from demonstrations (ILD) aims to alleviate numerous shortcomings of reinforcement learning through the use of demonstrations. However, in most real-world applications, expert action guidance is absent, making the use of ILD impossible. Instead, we consider imitation learning from observations (ILO), where no expert actions are provided, making it a significantly more challenging problem to address. Existing methods often employ on-policy learning, which is known to be sample-costly. This paper presents SEILO, a novel sample-efficient on-policy algorithm for ILO, that combines standard adversarial imitation learning with inverse dynamics modeling. This approach enables the agent to receive feedback from both the adversarial procedure and a behavior cloning loss. We empirically demonstrate that our proposed algorithm requires fewer interactions with the environment to achieve expert performance compared to other state-of-the-art on-policy ILO and ILD methods.",
    "path": "papers/23/06/2306.09805.json",
    "total_tokens": 913,
    "translated_title": "观测中的样本高效策略模仿学习",
    "translated_abstract": "通过使用专家演示，模仿学习 (ILD) 旨在通过消除强化学习的许多缺点来帮助学习输出更好的策略。然而，在大多数真实世界的应用中，缺乏专家行动指导，因此无法使用ILD。相反，我们考虑观测中的模仿学习 (ILO)，其中没有提供专家动作，使其成为更具挑战性的问题。现有方法通常使用策略学习，这是众所周知的成本昂贵的。本文提出了 SEILO，一种新颖的样本高效策略算法，用于 ILO，将标准的对抗模仿学习与逆动力学建模相结合。这种方法使代理能够从对抗程序和行为克隆损失中获得反馈。我们实验证明，与其他最先进的策略 ILO 和 ILD 方法相比，我们提出的算法需要较少的与环境的交互来实现专家性能。",
    "tldr": "提出了一种称为SEILO的算法，该算法结合了标准的对抗模仿学习和逆动力学建模，实现了从无专家数据的观测中的样本高效策略模仿学习，成功地减少了与环境的交互并实现了专家水平的表现。",
    "en_tdlr": "SEILO, a novel sample-efficient on-policy algorithm combining adversarial imitation learning with inverse dynamics modeling, is proposed for imitation learning from observations (ILO). It successfully reduces interactions with environment and achieves expert-level performance with no expert action guidance."
}