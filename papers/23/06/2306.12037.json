{
    "title": "Distributed Random Reshuffling Methods with Improved Convergence. (arXiv:2306.12037v1 [math.OC])",
    "abstract": "This paper proposes two distributed random reshuffling methods, namely Gradient Tracking with Random Reshuffling (GT-RR) and Exact Diffusion with Random Reshuffling (ED-RR), to solve the distributed optimization problem over a connected network, where a set of agents aim to minimize the average of their local cost functions. Both algorithms invoke random reshuffling (RR) update for each agent, inherit favorable characteristics of RR for minimizing smooth nonconvex objective functions, and improve the performance of previous distributed random reshuffling methods both theoretically and empirically. Specifically, both GT-RR and ED-RR achieve the convergence rate of $O(1/[(1-\\lambda)^{1/3}m^{1/3}T^{2/3}])$ in driving the (minimum) expected squared norm of the gradient to zero, where $T$ denotes the number of epochs, $m$ is the sample size for each agent, and $1-\\lambda$ represents the spectral gap of the mixing matrix. When the objective functions further satisfy the Polyak-{\\L}ojasiewicz",
    "link": "http://arxiv.org/abs/2306.12037",
    "context": "Title: Distributed Random Reshuffling Methods with Improved Convergence. (arXiv:2306.12037v1 [math.OC])\nAbstract: This paper proposes two distributed random reshuffling methods, namely Gradient Tracking with Random Reshuffling (GT-RR) and Exact Diffusion with Random Reshuffling (ED-RR), to solve the distributed optimization problem over a connected network, where a set of agents aim to minimize the average of their local cost functions. Both algorithms invoke random reshuffling (RR) update for each agent, inherit favorable characteristics of RR for minimizing smooth nonconvex objective functions, and improve the performance of previous distributed random reshuffling methods both theoretically and empirically. Specifically, both GT-RR and ED-RR achieve the convergence rate of $O(1/[(1-\\lambda)^{1/3}m^{1/3}T^{2/3}])$ in driving the (minimum) expected squared norm of the gradient to zero, where $T$ denotes the number of epochs, $m$ is the sample size for each agent, and $1-\\lambda$ represents the spectral gap of the mixing matrix. When the objective functions further satisfy the Polyak-{\\L}ojasiewicz",
    "path": "papers/23/06/2306.12037.json",
    "total_tokens": 1156,
    "tldr": "本文提出了两种分布式随机重排序算法，GT-RR和ED-RR，用于解决分布式优化问题，收敛速度比之前的方法更快，具有最小化平滑非凸函数的有利特性。在驱动梯度平方范数为零的过程中，两种算法都以$O(1/[(1-\\lambda)^{1/3}m^{1/3}T^{2/3}])$的速度收敛。",
    "en_tdlr": "This paper proposes two distributed random reshuffling methods, GT-RR and ED-RR, for solving distributed optimization problems over a connected network. These algorithms achieve better convergence and inherit favorable characteristics for minimizing smooth nonconvex functions. They converge at a rate of $O(1/[(1-\\lambda)^{1/3}m^{1/3}T^{2/3}])$ in driving the expected squared gradient norm to zero."
}