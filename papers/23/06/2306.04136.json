{
    "title": "Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering. (arXiv:2306.04136v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) are capable of performing zero-shot closed-book question answering tasks, based on their internal knowledge stored in parameters during pre-training. However, such internalized knowledge might be insufficient and incorrect, which could lead LLMs to generate factually wrong answers. Furthermore, fine-tuning LLMs to update their knowledge is expensive. To this end, we propose to augment the knowledge directly in the input of LLMs. Specifically, we first retrieve the relevant facts to the input question from the knowledge graph based on semantic similarities between the question and its associated facts. After that, we prepend the retrieved facts to the input question in the form of the prompt, which is then forwarded to LLMs to generate the answer. Our framework, Knowledge-Augmented language model PromptING (KAPING), requires no model training, thus completely zero-shot. We validate the performance of our KAPING framework on the knowledge graph question answe",
    "link": "http://arxiv.org/abs/2306.04136",
    "context": "Title: Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering. (arXiv:2306.04136v1 [cs.CL])\nAbstract: Large Language Models (LLMs) are capable of performing zero-shot closed-book question answering tasks, based on their internal knowledge stored in parameters during pre-training. However, such internalized knowledge might be insufficient and incorrect, which could lead LLMs to generate factually wrong answers. Furthermore, fine-tuning LLMs to update their knowledge is expensive. To this end, we propose to augment the knowledge directly in the input of LLMs. Specifically, we first retrieve the relevant facts to the input question from the knowledge graph based on semantic similarities between the question and its associated facts. After that, we prepend the retrieved facts to the input question in the form of the prompt, which is then forwarded to LLMs to generate the answer. Our framework, Knowledge-Augmented language model PromptING (KAPING), requires no model training, thus completely zero-shot. We validate the performance of our KAPING framework on the knowledge graph question answe",
    "path": "papers/23/06/2306.04136.json",
    "total_tokens": 848,
    "translated_title": "知识增强的语言模型提示用于零样本知识图谱问答",
    "translated_abstract": "大语言模型可基于其在预训练期间存储的内部知识执行零样本封闭书问题回答任务。然而，这种内部化的知识可能不足或不正确，这可能导致大语言模型生成事实上错误的答案。为此，我们提出在语言模型的输入中直接增强知识。我们首先基于问题与相关事实之间的语义相似性从知识图谱中检索与输入问题相关的事实。接下来，我们以提示的形式将检索到的事实附加到输入问题之前，然后将其发送到语言模型以生成答案。我们的框架KAPING（Knowledge-Augmented language model PromptING）无需模型训练，完全零样本。我们验证了KAPING框架在知识图谱问答任务中的性能。",
    "tldr": "该论文提出了一种知识增强的语言模型提示方法，以用于零样本知识图谱问答。它使用知识图谱中与问题相关的事实作为提示，并将它们添加到问题之前，以生成更准确的答案。",
    "en_tdlr": "This paper proposes a knowledge-augmented language model prompting method for zero-shot knowledge graph question answering. It uses relevant facts from the knowledge graph as prompts to generate more accurate answers."
}