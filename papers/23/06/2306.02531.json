{
    "title": "PLANNER: Generating Diversified Paragraph via Latent Language Diffusion Model. (arXiv:2306.02531v2 [cs.CL] UPDATED)",
    "abstract": "Autoregressive models for text sometimes generate repetitive and low-quality output because errors accumulate during the steps of generation. This issue is often attributed to exposure bias - the difference between how a model is trained, and how it is used during inference. Denoising diffusion models provide an alternative approach in which a model can revisit and revise its output. However, they can be computationally expensive and prior efforts on text have led to models that produce less fluent output compared to autoregressive models, especially for longer text and paragraphs. In this paper, we propose PLANNER, a model that combines latent semantic diffusion with autoregressive generation, to generate fluent text while exercising global control over paragraphs. The model achieves this by combining an autoregressive \"decoding\" module with a \"planning\" module that uses latent diffusion to generate semantic paragraph embeddings in a coarse-to-fine manner. The proposed method is evalu",
    "link": "http://arxiv.org/abs/2306.02531",
    "context": "Title: PLANNER: Generating Diversified Paragraph via Latent Language Diffusion Model. (arXiv:2306.02531v2 [cs.CL] UPDATED)\nAbstract: Autoregressive models for text sometimes generate repetitive and low-quality output because errors accumulate during the steps of generation. This issue is often attributed to exposure bias - the difference between how a model is trained, and how it is used during inference. Denoising diffusion models provide an alternative approach in which a model can revisit and revise its output. However, they can be computationally expensive and prior efforts on text have led to models that produce less fluent output compared to autoregressive models, especially for longer text and paragraphs. In this paper, we propose PLANNER, a model that combines latent semantic diffusion with autoregressive generation, to generate fluent text while exercising global control over paragraphs. The model achieves this by combining an autoregressive \"decoding\" module with a \"planning\" module that uses latent diffusion to generate semantic paragraph embeddings in a coarse-to-fine manner. The proposed method is evalu",
    "path": "papers/23/06/2306.02531.json",
    "total_tokens": 890,
    "translated_title": "PLANNER:通过潜在语言扩散模型生成多样化段落",
    "translated_abstract": "文本的自回归模型有时会生成重复且质量低下的输出，因为在生成的步骤中错误会累积。这个问题常常被归因于曝光偏差-模型在训练和推理过程中的使用方式之间的差异。去噪扩散模型提供了一种替代方法，模型可以回顾和修正其输出。然而，它们在计算上可能很昂贵，并且在文本和段落较长的情况下，与自回归模型相比，先前关于文本的研究努力已导致产生的模型产生不太流畅的输出。在本文中，我们提出了PLANNER，一个将潜在语义扩散与自回归生成结合的模型，以在段落上进行全局控制来生成流畅的文本。该模型通过将自回归的“解码”模块与使用潜在扩散以粗粒度方式生成语义段落嵌入的“规划”模块相结合来实现这一目标。该方法进行了评估",
    "tldr": "本文提出了PLANNER模型，它通过将自回归生成和潜在语义扩散相结合，以在生成文本时在段落级别实现全局控制，生成流畅的文本。",
    "en_tdlr": "This paper presents the PLANNER model, which combines autoregressive generation with latent semantic diffusion to exercise global control over paragraphs and generate fluent text."
}