{
    "title": "ExpPoint-MAE: Better interpretability and performance for self-supervised point cloud transformers. (arXiv:2306.10798v2 [cs.CV] UPDATED)",
    "abstract": "In this paper we delve into the properties of transformers, attained through self-supervision, in the point cloud domain. Specifically, we evaluate the effectiveness of Masked Autoencoding as a pretraining scheme, and explore Momentum Contrast as an alternative. In our study we investigate the impact of data quantity on the learned features, and uncover similarities in the transformer's behavior across domains. Through comprehensive visualiations, we observe that the transformer learns to attend to semantically meaningful regions, indicating that pretraining leads to a better understanding of the underlying geometry. Moreover, we examine the finetuning process and its effect on the learned representations. Based on that, we devise an unfreezing strategy which consistently outperforms our baseline without introducing any other modifications to the model or the training pipeline, and achieve state-of-the-art results in the classification task among transformer models.",
    "link": "http://arxiv.org/abs/2306.10798",
    "context": "Title: ExpPoint-MAE: Better interpretability and performance for self-supervised point cloud transformers. (arXiv:2306.10798v2 [cs.CV] UPDATED)\nAbstract: In this paper we delve into the properties of transformers, attained through self-supervision, in the point cloud domain. Specifically, we evaluate the effectiveness of Masked Autoencoding as a pretraining scheme, and explore Momentum Contrast as an alternative. In our study we investigate the impact of data quantity on the learned features, and uncover similarities in the transformer's behavior across domains. Through comprehensive visualiations, we observe that the transformer learns to attend to semantically meaningful regions, indicating that pretraining leads to a better understanding of the underlying geometry. Moreover, we examine the finetuning process and its effect on the learned representations. Based on that, we devise an unfreezing strategy which consistently outperforms our baseline without introducing any other modifications to the model or the training pipeline, and achieve state-of-the-art results in the classification task among transformer models.",
    "path": "papers/23/06/2306.10798.json",
    "total_tokens": 844,
    "translated_title": "ExpPoint-MAE：自监督点云Transformer的更好可解释性和性能",
    "translated_abstract": "本论文深入探讨了自监督Transformer在点云领域中的特性。具体而言，我们评估了Masked Autoencoding作为预训练方案的有效性，并探索了Momentum Contrast作为替代方案。通过全面的可视化，我们观察到Transformer学习关注语义上有意义的区域，表明预训练有助于更好地理解基础几何。此外，我们还研究了微调过程及其对所学表示的影响。基于此，我们设计了一种解冻策略，它在不引入任何其他修改模型或训练流程的情况下始终优于我们的基线，并在Transformer模型中在分类任务中取得了最佳结果。",
    "tldr": "本文探讨了自监督Transformer在点云领域中的特性，并发现预训练方案能够帮助更好地理解基础几何，提出一种解冻策略，在不引入其他修改的情况下始终优于基线，并在Transformer模型中取得了最佳结果。"
}