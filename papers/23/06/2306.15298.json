{
    "title": "Gender Bias in BERT -- Measuring and Analysing Biases through Sentiment Rating in a Realistic Downstream Classification Task. (arXiv:2306.15298v1 [cs.CL])",
    "abstract": "Pretrained language models are publicly available and constantly finetuned for various real-life applications. As they become capable of grasping complex contextual information, harmful biases are likely increasingly intertwined with those models. This paper analyses gender bias in BERT models with two main contributions: First, a novel bias measure is introduced, defining biases as the difference in sentiment valuation of female and male sample versions. Second, we comprehensively analyse BERT's biases on the example of a realistic IMDB movie classifier. By systematically varying elements of the training pipeline, we can conclude regarding their impact on the final model bias. Seven different public BERT models in nine training conditions, i.e. 63 models in total, are compared. Almost all conditions yield significant gender biases. Results indicate that reflected biases stem from public BERT models rather than task-specific data, emphasising the weight of responsible usage.",
    "link": "http://arxiv.org/abs/2306.15298",
    "context": "Title: Gender Bias in BERT -- Measuring and Analysing Biases through Sentiment Rating in a Realistic Downstream Classification Task. (arXiv:2306.15298v1 [cs.CL])\nAbstract: Pretrained language models are publicly available and constantly finetuned for various real-life applications. As they become capable of grasping complex contextual information, harmful biases are likely increasingly intertwined with those models. This paper analyses gender bias in BERT models with two main contributions: First, a novel bias measure is introduced, defining biases as the difference in sentiment valuation of female and male sample versions. Second, we comprehensively analyse BERT's biases on the example of a realistic IMDB movie classifier. By systematically varying elements of the training pipeline, we can conclude regarding their impact on the final model bias. Seven different public BERT models in nine training conditions, i.e. 63 models in total, are compared. Almost all conditions yield significant gender biases. Results indicate that reflected biases stem from public BERT models rather than task-specific data, emphasising the weight of responsible usage.",
    "path": "papers/23/06/2306.15298.json",
    "total_tokens": 942,
    "translated_title": "BERT中的性别偏见——通过情感评分在现实的下游分类任务中测量和分析偏见",
    "translated_abstract": "预训练语言模型在各种现实应用中公开可用，并不断进行微调。随着它们具备抓取复杂上下文信息的能力，有害偏见很可能越来越多地与这些模型交织在一起。本文通过两个主要贡献来分析BERT模型中的性别偏见：首先，引入了一种新的偏见度量方法，将偏见定义为女性和男性样本版本在情感评估上的差异。其次，我们对一个现实的IMDB电影分类器的例子中BERT的偏见进行了全面分析。通过系统地变化训练流程的各个元素，我们可以对最终模型偏见的影响做出结论。我们比较了七个不同的公开BERT模型的九种训练条件，即总共63个模型。几乎所有的条件都产生了显著的性别偏见。结果表明，反映的偏见源于公开的BERT模型而不是任务特定数据，强调了负责任使用的重要性。",
    "tldr": "该论文通过引入新的偏见度量方法，并在一个现实的IMDB电影分类器的例子中对BERT的性别偏见进行了全面分析。研究结果表明，公开的BERT模型中存在着显著的性别偏见，并强调了负责任使用的重要性。"
}