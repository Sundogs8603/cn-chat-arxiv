{
    "title": "Improving Energy Conserving Descent for Machine Learning: Theory and Practice. (arXiv:2306.00352v1 [cs.LG])",
    "abstract": "We develop the theory of Energy Conserving Descent (ECD) and introduce ECDSep, a gradient-based optimization algorithm able to tackle convex and non-convex optimization problems. The method is based on the novel ECD framework of optimization as physical evolution of a suitable chaotic energy-conserving dynamical system, enabling analytic control of the distribution of results - dominated at low loss - even for generic high-dimensional problems with no symmetries. Compared to previous realizations of this idea, we exploit the theoretical control to improve both the dynamics and chaos-inducing elements, enhancing performance while simplifying the hyper-parameter tuning of the optimization algorithm targeted to different classes of problems. We empirically compare with popular optimization methods such as SGD, Adam and AdamW on a wide range of machine learning problems, finding competitive or improved performance compared to the best among them on each task. We identify limitations in our",
    "link": "http://arxiv.org/abs/2306.00352",
    "context": "Title: Improving Energy Conserving Descent for Machine Learning: Theory and Practice. (arXiv:2306.00352v1 [cs.LG])\nAbstract: We develop the theory of Energy Conserving Descent (ECD) and introduce ECDSep, a gradient-based optimization algorithm able to tackle convex and non-convex optimization problems. The method is based on the novel ECD framework of optimization as physical evolution of a suitable chaotic energy-conserving dynamical system, enabling analytic control of the distribution of results - dominated at low loss - even for generic high-dimensional problems with no symmetries. Compared to previous realizations of this idea, we exploit the theoretical control to improve both the dynamics and chaos-inducing elements, enhancing performance while simplifying the hyper-parameter tuning of the optimization algorithm targeted to different classes of problems. We empirically compare with popular optimization methods such as SGD, Adam and AdamW on a wide range of machine learning problems, finding competitive or improved performance compared to the best among them on each task. We identify limitations in our",
    "path": "papers/23/06/2306.00352.json",
    "total_tokens": 980,
    "translated_title": "改进机器学习的能量守恒下降法：理论与实践",
    "translated_abstract": "我们发展了能量守恒下降（ECD）的理论，并介绍了ECDSep，这是一种基于梯度的优化算法，能够处理凸优化问题和非凸优化问题。该方法基于新颖的ECD优化框架，通过适当混沌的能量守恒动力系统的物理演化来实现，使得即使对于无对称性的通用高维问题的结果分布也能进行分析控制，从而主导低损失。与以往的实现相比，我们利用理论控制来改进动态和诱导混沌的元素，提高性能，同时简化面向不同类别问题的优化算法的超参数调整。我们在各种机器学习问题上与流行的优化方法（如SGD，Adam和AdamW等）进行了实证比较，发现在每个任务中与它们中的最佳方法相比具有竞争力或改进的性能。我们确定了我们方法的局限性，并提出了未来研究的方向。",
    "tldr": "这篇论文介绍了能量守恒下降（ECD）的理论，提出了基于梯度的优化算法ECDSep，能够处理凸优化问题和非凸优化问题，通过改进动态和混沌诱导元素，提高性能；在各种机器学习问题上与流行的优化方法进行了实证比较，发现在每个任务中具有竞争力或改进的性能。"
}