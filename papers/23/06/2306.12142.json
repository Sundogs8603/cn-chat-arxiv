{
    "title": "Synaptic metaplasticity with multi-level memristive devices. (arXiv:2306.12142v1 [cs.NE])",
    "abstract": "Deep learning has made remarkable progress in various tasks, surpassing human performance in some cases. However, one drawback of neural networks is catastrophic forgetting, where a network trained on one task forgets the solution when learning a new one. To address this issue, recent works have proposed solutions based on Binarized Neural Networks (BNNs) incorporating metaplasticity. In this work, we extend this solution to quantized neural networks (QNNs) and present a memristor-based hardware solution for implementing metaplasticity during both inference and training. We propose a hardware architecture that integrates quantized weights in memristor devices programmed in an analog multi-level fashion with a digital processing unit for high-precision metaplastic storage. We validated our approach using a combined software framework and memristor based crossbar array for in-memory computing fabricated in 130 nm CMOS technology. Our experimental results show that a two-layer perceptron ",
    "link": "http://arxiv.org/abs/2306.12142",
    "context": "Title: Synaptic metaplasticity with multi-level memristive devices. (arXiv:2306.12142v1 [cs.NE])\nAbstract: Deep learning has made remarkable progress in various tasks, surpassing human performance in some cases. However, one drawback of neural networks is catastrophic forgetting, where a network trained on one task forgets the solution when learning a new one. To address this issue, recent works have proposed solutions based on Binarized Neural Networks (BNNs) incorporating metaplasticity. In this work, we extend this solution to quantized neural networks (QNNs) and present a memristor-based hardware solution for implementing metaplasticity during both inference and training. We propose a hardware architecture that integrates quantized weights in memristor devices programmed in an analog multi-level fashion with a digital processing unit for high-precision metaplastic storage. We validated our approach using a combined software framework and memristor based crossbar array for in-memory computing fabricated in 130 nm CMOS technology. Our experimental results show that a two-layer perceptron ",
    "path": "papers/23/06/2306.12142.json",
    "total_tokens": 984,
    "translated_title": "多级忆阻器的突触元可塑性",
    "translated_abstract": "深度学习在各种任务上取得了显著进展，有时在某些情况下超越了人类表现。然而，神经网络的一个缺点是灾难性遗忘，即在学习新任务时，曾经训练过的网络会忘记原有的解决方案。为了解决这个问题，最近的一些研究提出了基于元可塑性的二值神经网络（BNN）的解决方案。在本研究中，我们将这种解决方案扩展到量化神经网络（QNN），并提出了一种基于忆阻器的硬件解决方案，用于在推理和训练期间实现元可塑性。我们提出了一种硬件架构，将经过量化的权重与以模拟多级方式编程的忆阻器器件集成在一起，并与数字处理单元结合使用，实现高精度的元可塑存储。我们使用一个组合软件框架和基于忆阻器交叉阵列的内存计算，在130纳米CMOS技术下进行了验证。实验结果表明，一个两层感知器可以使用我们的方法正确地学习两个不同的任务。",
    "tldr": "本研究提出了一种基于忆阻器的硬件解决方案，用于在推理和训练期间实现元可塑性，从而解决神经网络的灾难性遗忘问题。"
}