{
    "title": "Balanced Training of Energy-Based Models with Adaptive Flow Sampling. (arXiv:2306.00684v1 [cs.LG])",
    "abstract": "Energy-based models (EBMs) are versatile density estimation models that directly parameterize an unnormalized log density. Although very flexible, EBMs lack a specified normalization constant of the model, making the likelihood of the model computationally intractable. Several approximate samplers and variational inference techniques have been proposed to estimate the likelihood gradients for training. These techniques have shown promising results in generating samples, but little attention has been paid to the statistical accuracy of the estimated density, such as determining the relative importance of different classes in a dataset. In this work, we propose a new maximum likelihood training algorithm for EBMs that uses a different type of generative model, normalizing flows (NF), which have recently been proposed to facilitate sampling. Our method fits an NF to an EBM during training so that an NF-assisted sampling scheme provides an accurate gradient for the EBMs at all times, ultim",
    "link": "http://arxiv.org/abs/2306.00684",
    "context": "Title: Balanced Training of Energy-Based Models with Adaptive Flow Sampling. (arXiv:2306.00684v1 [cs.LG])\nAbstract: Energy-based models (EBMs) are versatile density estimation models that directly parameterize an unnormalized log density. Although very flexible, EBMs lack a specified normalization constant of the model, making the likelihood of the model computationally intractable. Several approximate samplers and variational inference techniques have been proposed to estimate the likelihood gradients for training. These techniques have shown promising results in generating samples, but little attention has been paid to the statistical accuracy of the estimated density, such as determining the relative importance of different classes in a dataset. In this work, we propose a new maximum likelihood training algorithm for EBMs that uses a different type of generative model, normalizing flows (NF), which have recently been proposed to facilitate sampling. Our method fits an NF to an EBM during training so that an NF-assisted sampling scheme provides an accurate gradient for the EBMs at all times, ultim",
    "path": "papers/23/06/2306.00684.json",
    "total_tokens": 956,
    "translated_title": "使用自适应流采样平衡训练能量基模型",
    "translated_abstract": "能量基模型 (EBM) 是一种直接参数化未标准化对数密度的多功能密度估计模型。EBM 非常灵活，但缺乏模型的规范化常量，使模型的似然函数计算不可行。近年来，已经提出了许多近似采样器和变分推理技术来估计似然函数梯度进行训练。这些技术在生成样本方面表现出色，但对于估计密度的统计精度，例如确定数据集中不同类的相对重要性，却付出了很少的关注。在本文中，我们提出了一种新的最大似然训练算法，使用一种不同类型的生成模型，归一化流 (NF)，这种模型最近被提出以便于采样。我们的方法在训练过程中将 NF 拟合到 EBM 上，以便 NF 辅助下的采样方案能够始终为 EBM 提供准确的梯度，最终提高模型的统计精度。实验结果表明，与传统 EBM 训练技术相比，我们的方法产生了更高质量的样本和更好的生成性能。",
    "tldr": "本文研究了能量基模型的训练算法，使用归一化流进行采样，提高了模型的统计精度和生成性能。"
}