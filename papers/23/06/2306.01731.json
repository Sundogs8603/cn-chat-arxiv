{
    "title": "PAGAR: Taming Reward Misalignment in Inverse Reinforcement Learning-Based Imitation Learning with Protagonist Antagonist Guided Adversarial Reward. (arXiv:2306.01731v2 [cs.LG] UPDATED)",
    "abstract": "Many imitation learning (IL) algorithms employ inverse reinforcement learning (IRL) to infer the underlying reward function that an expert is implicitly optimizing for, based on their demonstrated behaviors. However, a misalignment between the inferred reward and the true task objective can result in task failures. In this paper, we introduce Protagonist Antagonist Guided Adversarial Reward (PAGAR), a semi-supervised reward design paradigm to tackle this reward misalignment problem in IRL-based IL. We identify the conditions on the candidate reward functions under which PAGAR can guarantee to induce a policy that succeeds in the underlying task. Furthermore, we present a practical on-and-off policy approach to implement PAGAR in IRL-based IL. Experimental results show that our algorithm outperforms competitive baselines on complex IL tasks and zero-shot IL tasks in transfer environments with limited demonstrations.",
    "link": "http://arxiv.org/abs/2306.01731",
    "context": "Title: PAGAR: Taming Reward Misalignment in Inverse Reinforcement Learning-Based Imitation Learning with Protagonist Antagonist Guided Adversarial Reward. (arXiv:2306.01731v2 [cs.LG] UPDATED)\nAbstract: Many imitation learning (IL) algorithms employ inverse reinforcement learning (IRL) to infer the underlying reward function that an expert is implicitly optimizing for, based on their demonstrated behaviors. However, a misalignment between the inferred reward and the true task objective can result in task failures. In this paper, we introduce Protagonist Antagonist Guided Adversarial Reward (PAGAR), a semi-supervised reward design paradigm to tackle this reward misalignment problem in IRL-based IL. We identify the conditions on the candidate reward functions under which PAGAR can guarantee to induce a policy that succeeds in the underlying task. Furthermore, we present a practical on-and-off policy approach to implement PAGAR in IRL-based IL. Experimental results show that our algorithm outperforms competitive baselines on complex IL tasks and zero-shot IL tasks in transfer environments with limited demonstrations.",
    "path": "papers/23/06/2306.01731.json",
    "total_tokens": 915,
    "translated_title": "PAGAR: 用主角-反派引导的对抗性奖励驯服逆强化学习在基于模仿学习中的奖励不对齐问题",
    "translated_abstract": "许多模仿学习(imitation learning, IL)算法使用逆强化学习(inverse reinforcement learning, IRL)来推断专家以隐式方式优化的潜在奖励函数，基于其展示的行为。然而，推断的奖励与真实任务目标之间的不对齐可能导致任务失败。在本文中，我们引入了主角-反派引导的对抗性奖励(PAGAR)，这是一种半监督奖励设计范式，用于解决IRL-based IL中的奖励不对齐问题。我们确定了候选奖励函数满足的条件，PAGAR能够保证产生一个在底层任务中成功的策略。此外，我们提出了一种实用的在策略和离策略方法来在IRL-based IL中实施PAGAR。实验结果表明，我们的算法在复杂的IL任务和有限演示的迁移环境的零-shot IL任务上优于竞争的基线模型。",
    "tldr": "PAGAR是一种用于解决IRL-based IL中奖励不对齐问题的半监督奖励设计方法，在复杂IL任务和零-shot IL任务中表现优越。",
    "en_tdlr": "PAGAR is a semi-supervised reward design method that solves the reward misalignment problem in IRL-based IL, and it outperforms competitive baselines in complex IL tasks and zero-shot IL tasks."
}