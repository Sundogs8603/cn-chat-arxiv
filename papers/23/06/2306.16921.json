{
    "title": "Provable Advantage of Curriculum Learning on Parity Targets with Mixed Inputs. (arXiv:2306.16921v1 [cs.LG])",
    "abstract": "Experimental results have shown that curriculum learning, i.e., presenting simpler examples before more complex ones, can improve the efficiency of learning. Some recent theoretical results also showed that changing the sampling distribution can help neural networks learn parities, with formal results only for large learning rates and one-step arguments. Here we show a separation result in the number of training steps with standard (bounded) learning rates on a common sample distribution: if the data distribution is a mixture of sparse and dense inputs, there exists a regime in which a 2-layer ReLU neural network trained by a curriculum noisy-GD (or SGD) algorithm that uses sparse examples first, can learn parities of sufficiently large degree, while any fully connected neural network of possibly larger width or depth trained by noisy-GD on the unordered samples cannot learn without additional steps. We also provide experimental results supporting the qualitative separation beyond the ",
    "link": "http://arxiv.org/abs/2306.16921",
    "context": "Title: Provable Advantage of Curriculum Learning on Parity Targets with Mixed Inputs. (arXiv:2306.16921v1 [cs.LG])\nAbstract: Experimental results have shown that curriculum learning, i.e., presenting simpler examples before more complex ones, can improve the efficiency of learning. Some recent theoretical results also showed that changing the sampling distribution can help neural networks learn parities, with formal results only for large learning rates and one-step arguments. Here we show a separation result in the number of training steps with standard (bounded) learning rates on a common sample distribution: if the data distribution is a mixture of sparse and dense inputs, there exists a regime in which a 2-layer ReLU neural network trained by a curriculum noisy-GD (or SGD) algorithm that uses sparse examples first, can learn parities of sufficiently large degree, while any fully connected neural network of possibly larger width or depth trained by noisy-GD on the unordered samples cannot learn without additional steps. We also provide experimental results supporting the qualitative separation beyond the ",
    "path": "papers/23/06/2306.16921.json",
    "total_tokens": 877,
    "translated_title": "对混合输入的奇偶目标，课程学习的可证明优势",
    "translated_abstract": "实验结果表明，课程学习，即先呈现简单示例，然后再呈现更复杂的示例，可以提高学习效率。近期的一些理论结果也表明，改变采样分布可以帮助神经网络学习奇偶性，但只有大学习率和单步参数的形式结果。在这里，我们展示了在标准（有界）学习率和常见样本分布的训练步骤数量上的分离结果：如果数据分布是稀疏和密集输入的混合物，则存在一种情况，在这种情况下，通过课程嘈杂梯度下降（或SGD）算法训练的2层ReLU神经网络，先使用稀疏示例，可以学习到足够大阶数的奇偶性，而任何由嘈杂梯度下降算法训练的完全连接的神经网络（宽度或深度可能更大）在乱序样本上都不能在没有额外步骤的情况下学习。我们还提供了实验结果，支持超出的定性分离。",
    "tldr": "通过课程学习，采用稀疏示例先学习的2层ReLU神经网络可以在混合输入的奇偶目标上学习到足够大阶数的奇偶性，而其他神经网络无法在相同的条件下学习。"
}