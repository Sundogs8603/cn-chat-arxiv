{
    "title": "Joint Path planning and Power Allocation of a Cellular-Connected UAV using Apprenticeship Learning via Deep Inverse Reinforcement Learning. (arXiv:2306.10071v1 [cs.LG])",
    "abstract": "This paper investigates an interference-aware joint path planning and power allocation mechanism for a cellular-connected unmanned aerial vehicle (UAV) in a sparse suburban environment. The UAV's goal is to fly from an initial point and reach a destination point by moving along the cells to guarantee the required quality of service (QoS). In particular, the UAV aims to maximize its uplink throughput and minimize the level of interference to the ground user equipment (UEs) connected to the neighbor cellular BSs, considering the shortest path and flight resource limitation. Expert knowledge is used to experience the scenario and define the desired behavior for the sake of the agent (i.e., UAV) training. To solve the problem, an apprenticeship learning method is utilized via inverse reinforcement learning (IRL) based on both Q-learning and deep reinforcement learning (DRL). The performance of this method is compared to learning from a demonstration technique called behavioral cloning (BC)",
    "link": "http://arxiv.org/abs/2306.10071",
    "context": "Title: Joint Path planning and Power Allocation of a Cellular-Connected UAV using Apprenticeship Learning via Deep Inverse Reinforcement Learning. (arXiv:2306.10071v1 [cs.LG])\nAbstract: This paper investigates an interference-aware joint path planning and power allocation mechanism for a cellular-connected unmanned aerial vehicle (UAV) in a sparse suburban environment. The UAV's goal is to fly from an initial point and reach a destination point by moving along the cells to guarantee the required quality of service (QoS). In particular, the UAV aims to maximize its uplink throughput and minimize the level of interference to the ground user equipment (UEs) connected to the neighbor cellular BSs, considering the shortest path and flight resource limitation. Expert knowledge is used to experience the scenario and define the desired behavior for the sake of the agent (i.e., UAV) training. To solve the problem, an apprenticeship learning method is utilized via inverse reinforcement learning (IRL) based on both Q-learning and deep reinforcement learning (DRL). The performance of this method is compared to learning from a demonstration technique called behavioral cloning (BC)",
    "path": "papers/23/06/2306.10071.json",
    "total_tokens": 878,
    "translated_title": "基于学徒学习的无人机联网路径规划与功率分配机制",
    "translated_abstract": "本文研究了在稀疏郊区环境中，一个联网的无人机通过沿着信号覆盖单元移动，从出发点飞向目的地以保证服务质量（QoS）的干扰感知联网路径规划与功率分配机制。专家知识被用来体验情景并为智能体（即，无人机）培训定义所需的行为，为此，一种基于逆强化学习（IRL）的学徒学习方法被采用，并通过Q学习和深度强化学习（DRL）进行比较性能，并将其与行为克隆（BC）调节学习技术的学习进行比较。",
    "tldr": "本论文通过学徒学习方法，采用基于Q学习和深度强化学习（DRL）的逆强化学习 (IRL)，解决了一个稀疏郊区环境中，无人机的联网路径规划与功率分配的干扰感知问题，优于传统的行为克隆（BC）调节学习技术。",
    "en_tdlr": "This paper proposes an apprenticeship learning method via inverse reinforcement learning (IRL) based on Q-learning and deep reinforcement learning (DRL) to solve the interference-aware joint path planning and power allocation problem for a cellular-connected UAV in a sparse suburban environment, and outperforms traditional behavioral cloning (BC) technique."
}