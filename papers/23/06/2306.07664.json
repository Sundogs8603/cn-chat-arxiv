{
    "title": "Rethink the Effectiveness of Text Data Augmentation: An Empirical Analysis. (arXiv:2306.07664v1 [cs.CL])",
    "abstract": "In recent years, language models (LMs) have made remarkable progress in advancing the field of natural language processing (NLP). However, the impact of data augmentation (DA) techniques on the fine-tuning (FT) performance of these LMs has been a topic of ongoing debate. In this study, we evaluate the effectiveness of three different FT methods in conjugation with back-translation across an array of 7 diverse NLP tasks, including classification and regression types, covering single-sentence and sentence-pair tasks. Contrary to prior assumptions that DA does not contribute to the enhancement of LMs' FT performance, our findings reveal that continued pre-training on augmented data can effectively improve the FT performance of the downstream tasks. In the most favourable case, continued pre-training improves the performance of FT by more than 10% in the few-shot learning setting. Our finding highlights the potential of DA as a powerful tool for bolstering LMs' performance.",
    "link": "http://arxiv.org/abs/2306.07664",
    "context": "Title: Rethink the Effectiveness of Text Data Augmentation: An Empirical Analysis. (arXiv:2306.07664v1 [cs.CL])\nAbstract: In recent years, language models (LMs) have made remarkable progress in advancing the field of natural language processing (NLP). However, the impact of data augmentation (DA) techniques on the fine-tuning (FT) performance of these LMs has been a topic of ongoing debate. In this study, we evaluate the effectiveness of three different FT methods in conjugation with back-translation across an array of 7 diverse NLP tasks, including classification and regression types, covering single-sentence and sentence-pair tasks. Contrary to prior assumptions that DA does not contribute to the enhancement of LMs' FT performance, our findings reveal that continued pre-training on augmented data can effectively improve the FT performance of the downstream tasks. In the most favourable case, continued pre-training improves the performance of FT by more than 10% in the few-shot learning setting. Our finding highlights the potential of DA as a powerful tool for bolstering LMs' performance.",
    "path": "papers/23/06/2306.07664.json",
    "total_tokens": 896,
    "translated_title": "重新思考文本数据增强的有效性：一个经验分析",
    "translated_abstract": "最近几年，语言模型在推进自然语言处理领域方面取得了显著进展。然而，数据增强技术对这些语言模型微调表现的影响一直是一个争论的话题。在本研究中，我们评估了三种不同的微调方法结合回译在7个不同的自然语言处理任务中的有效性，包括分类和回归类型，涵盖单句和句子对任务。与优先的假设相反，即数据增强对提高语言模型的微调表现没有贡献，我们的发现表明，持续在增强数据上进行预训练可以有效地改善下游任务的微调表现。在最有利的情况下，持续的预训练可以使微调性能在小样本学习设置下提高10%以上。我们的发现突显出数据增强作为增强语言模型性能的强大工具的潜力。",
    "tldr": "本研究发现数据增强技术在语言模型的预训练及微调中仍具有显著的提升作用，尤其在少样本学习的情况下，持续的预训练可以提高微调性能10%以上。",
    "en_tdlr": "This study found that data augmentation techniques can still significantly improve the pre-training and fine-tuning performance of language models, especially in the case of few-shot learning, where continued pre-training can improve the fine-tuning performance by more than 10%."
}