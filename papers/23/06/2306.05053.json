{
    "title": "Active Inference in Hebbian Learning Networks. (arXiv:2306.05053v1 [cs.NE])",
    "abstract": "This work studies how brain-inspired neural ensembles equipped with local Hebbian plasticity can perform active inference (AIF) in order to control dynamical agents. A generative model capturing the environment dynamics is learned by a network composed of two distinct Hebbian ensembles: a posterior network, which infers latent states given the observations, and a state transition network, which predicts the next expected latent state given current state-action pairs. Experimental studies are conducted using the Mountain Car environment from the OpenAI gym suite, to study the effect of the various Hebbian network parameters on the task performance. It is shown that the proposed Hebbian AIF approach outperforms the use of Q-learning, while not requiring any replay buffer, as in typical reinforcement learning systems. These results motivate further investigations of Hebbian learning for the design of AIF networks that can learn environment dynamics without the need for revisiting past buf",
    "link": "http://arxiv.org/abs/2306.05053",
    "context": "Title: Active Inference in Hebbian Learning Networks. (arXiv:2306.05053v1 [cs.NE])\nAbstract: This work studies how brain-inspired neural ensembles equipped with local Hebbian plasticity can perform active inference (AIF) in order to control dynamical agents. A generative model capturing the environment dynamics is learned by a network composed of two distinct Hebbian ensembles: a posterior network, which infers latent states given the observations, and a state transition network, which predicts the next expected latent state given current state-action pairs. Experimental studies are conducted using the Mountain Car environment from the OpenAI gym suite, to study the effect of the various Hebbian network parameters on the task performance. It is shown that the proposed Hebbian AIF approach outperforms the use of Q-learning, while not requiring any replay buffer, as in typical reinforcement learning systems. These results motivate further investigations of Hebbian learning for the design of AIF networks that can learn environment dynamics without the need for revisiting past buf",
    "path": "papers/23/06/2306.05053.json",
    "total_tokens": 939,
    "translated_title": "Hebbian学习网络中的主动推理",
    "translated_abstract": "本研究探讨了具有局部Hebbian可塑性的仿脑神经群体如何执行主动推理（AIF），以控制动态代理。通过由两个不同的Hebbian神经元组成的网络进行学习，生成了一个捕捉环境动态的生成模型：一个后验网络，用于在给定观测的情况下推断潜在状态，以及一个状态转移网络，用于在给定当前状态-动作对的情况下预测下一个期望的潜在状态。使用OpenAI gym套件中的Mountain Car环境进行实验研究，以研究各种Hebbian网络参数对任务性能的影响。结果表明，所提出的Hebbian AIF方法优于使用Q-learning，同时不需要回放缓冲区，如典型的强化学习系统。这些结果促使我们进一步探讨Hebbian学习，以设计能够学习环境动态而不需要重新访问过去缓冲区的AIF网络。",
    "tldr": "本文研究了具有局部Hebbian可塑性的仿脑神经群体如何执行主动推理，通过两个不同的Hebbian神经元组成的网络来生成捕捉环境动态的生成模型，使用Mountain Car环境进行实验研究，结果表明所提出的Hebbian AIF方法优于使用Q-learning，同时不需要回放缓冲区。",
    "en_tdlr": "This paper investigates how brain-inspired neural ensembles with local Hebbian plasticity can perform active inference, and proposes a Hebbian AIF approach for learning environment dynamics without the need for revisiting past buffer. Experimental results demonstrate the effectiveness of the proposed method compared with Q-learning."
}