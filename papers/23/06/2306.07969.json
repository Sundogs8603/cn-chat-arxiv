{
    "title": "GeneCIS: A Benchmark for General Conditional Image Similarity. (arXiv:2306.07969v1 [cs.CV])",
    "abstract": "We argue that there are many notions of 'similarity' and that models, like humans, should be able to adapt to these dynamically. This contrasts with most representation learning methods, supervised or self-supervised, which learn a fixed embedding function and hence implicitly assume a single notion of similarity. For instance, models trained on ImageNet are biased towards object categories, while a user might prefer the model to focus on colors, textures or specific elements in the scene. In this paper, we propose the GeneCIS ('genesis') benchmark, which measures models' ability to adapt to a range of similarity conditions. Extending prior work, our benchmark is designed for zero-shot evaluation only, and hence considers an open-set of similarity conditions. We find that baselines from powerful CLIP models struggle on GeneCIS and that performance on the benchmark is only weakly correlated with ImageNet accuracy, suggesting that simply scaling existing methods is not fruitful. We furth",
    "link": "http://arxiv.org/abs/2306.07969",
    "context": "Title: GeneCIS: A Benchmark for General Conditional Image Similarity. (arXiv:2306.07969v1 [cs.CV])\nAbstract: We argue that there are many notions of 'similarity' and that models, like humans, should be able to adapt to these dynamically. This contrasts with most representation learning methods, supervised or self-supervised, which learn a fixed embedding function and hence implicitly assume a single notion of similarity. For instance, models trained on ImageNet are biased towards object categories, while a user might prefer the model to focus on colors, textures or specific elements in the scene. In this paper, we propose the GeneCIS ('genesis') benchmark, which measures models' ability to adapt to a range of similarity conditions. Extending prior work, our benchmark is designed for zero-shot evaluation only, and hence considers an open-set of similarity conditions. We find that baselines from powerful CLIP models struggle on GeneCIS and that performance on the benchmark is only weakly correlated with ImageNet accuracy, suggesting that simply scaling existing methods is not fruitful. We furth",
    "path": "papers/23/06/2306.07969.json",
    "total_tokens": 855,
    "translated_title": "GeneCIS：一种通用条件图像相似度的基准测试",
    "translated_abstract": "我们认为存在许多“相似性”的概念，而模型（如人类）应该能够动态地适应这些概念。这与大多数表示学习方法（受监督或自监督）不同，它们学习一个固定的嵌入函数，因此隐含地假定了单一的相似性概念。 在本文中，我们提出了GeneCIS（“创世纪”）基准测试，该测试衡量了模型适应各种相似性条件的能力。扩展之前的工作，我们的基准测试只设计用于零样本评估，因此考虑了开放集的相似性条件。我们发现，强大的CLIP模型的基线在GeneCIS上较为困难，并且基准测试的表现仅与ImageNet的准确性弱相关，这表明简单地扩展现有方法并不是有成果的。我们进一步",
    "tldr": "GeneCIS基准测试衡量模型适应各种相似性条件的能力，并且基准测试的表现与ImageNet的准确性弱相关，仅简单扩展现有方法并不行。",
    "en_tdlr": "GeneCIS benchmark measures models' ability to adapt to a range of similarity conditions. The benchmark's performance is weakly correlated with ImageNet accuracy, indicating that simply scaling existing methods is not fruitful."
}