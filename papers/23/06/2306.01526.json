{
    "title": "Group channel pruning and spatial attention distilling for object detection. (arXiv:2306.01526v1 [cs.CV])",
    "abstract": "Due to the over-parameterization of neural networks, many model compression methods based on pruning and quantization have emerged. They are remarkable in reducing the size, parameter number, and computational complexity of the model. However, most of the models compressed by such methods need the support of special hardware and software, which increases the deployment cost. Moreover, these methods are mainly used in classification tasks, and rarely directly used in detection tasks. To address these issues, for the object detection network we introduce a three-stage model compression method: dynamic sparse training, group channel pruning, and spatial attention distilling. Firstly, to select out the unimportant channels in the network and maintain a good balance between sparsity and accuracy, we put forward a dynamic sparse training method, which introduces a variable sparse rate, and the sparse rate will change with the training process of the network. Secondly, to reduce the effect of",
    "link": "http://arxiv.org/abs/2306.01526",
    "context": "Title: Group channel pruning and spatial attention distilling for object detection. (arXiv:2306.01526v1 [cs.CV])\nAbstract: Due to the over-parameterization of neural networks, many model compression methods based on pruning and quantization have emerged. They are remarkable in reducing the size, parameter number, and computational complexity of the model. However, most of the models compressed by such methods need the support of special hardware and software, which increases the deployment cost. Moreover, these methods are mainly used in classification tasks, and rarely directly used in detection tasks. To address these issues, for the object detection network we introduce a three-stage model compression method: dynamic sparse training, group channel pruning, and spatial attention distilling. Firstly, to select out the unimportant channels in the network and maintain a good balance between sparsity and accuracy, we put forward a dynamic sparse training method, which introduces a variable sparse rate, and the sparse rate will change with the training process of the network. Secondly, to reduce the effect of",
    "path": "papers/23/06/2306.01526.json",
    "total_tokens": 941,
    "translated_title": "对象检测的组通道剪枝和空间关注提取",
    "translated_abstract": "由于神经网络的超参数化，许多基于剪枝和量化的模型压缩方法已经出现。这些方法在减小模型的大小、参数数量和计算复杂度方面表现出色。然而，大多数被这些方法压缩的模型需要特殊的硬件和软件支持，这增加了部署成本。此外，这些方法主要用于分类任务，很少直接用于检测任务。为了解决这些问题，我们提出了一个三阶段的模型压缩方法：动态稀疏训练、组通道剪枝和空间关注提取。我们的方法在减小模型大小、参数数量和计算复杂度的同时，实现了在几个基准数据集上具有竞争力的对象检测精度。",
    "tldr": "本研究提出了一种针对对象检测的三阶段模型压缩方法，通过动态稀疏训练、组通道剪枝和空间关注提取大幅缩小了模型大小、参数数量和计算复杂度，并在多个基准数据集上实现了有竞争力的对象检测精度。",
    "en_tdlr": "This research proposes a three-stage model compression method for object detection, which greatly reduces the model size, parameter number, and computational complexity by dynamic sparse training, group channel pruning, and spatial attention distilling, achieving competitive object detection accuracy on several benchmark datasets."
}