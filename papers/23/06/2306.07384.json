{
    "title": "Probing Quantifier Comprehension in Large Language Models. (arXiv:2306.07384v1 [cs.CL])",
    "abstract": "With their increasing size, Large language models (LLMs) are becoming increasingly good at language understanding tasks. But even with high performance on specific downstream task, LLMs fail at simple linguistic tests for negation or quantifier understanding. Previous work on testing capability of LLMs on understanding quantifiers suggest that as the size of the models increase, they get better at understanding most-type quantifiers but get increasingly worse at understanding few-type quantifiers, thus presenting a case of an inverse-scaling law. In this paper, we question the claims of inverse scaling of few-type quantifier understanding in LLMs and show that it is a result of inappropriate testing methodology. We also present alternate methods to measure quantifier comprehension in LLMs and show that as the size of the models increase, these behaviours are different from what is shown in previous research. LLMs are consistently able to understand the difference between the meaning of",
    "link": "http://arxiv.org/abs/2306.07384",
    "context": "Title: Probing Quantifier Comprehension in Large Language Models. (arXiv:2306.07384v1 [cs.CL])\nAbstract: With their increasing size, Large language models (LLMs) are becoming increasingly good at language understanding tasks. But even with high performance on specific downstream task, LLMs fail at simple linguistic tests for negation or quantifier understanding. Previous work on testing capability of LLMs on understanding quantifiers suggest that as the size of the models increase, they get better at understanding most-type quantifiers but get increasingly worse at understanding few-type quantifiers, thus presenting a case of an inverse-scaling law. In this paper, we question the claims of inverse scaling of few-type quantifier understanding in LLMs and show that it is a result of inappropriate testing methodology. We also present alternate methods to measure quantifier comprehension in LLMs and show that as the size of the models increase, these behaviours are different from what is shown in previous research. LLMs are consistently able to understand the difference between the meaning of",
    "path": "papers/23/06/2306.07384.json",
    "total_tokens": 926,
    "translated_title": "大型语言模型对量化理解的探究",
    "translated_abstract": "随着它们的规模增大，大型语言模型（LLMs）在语言理解任务上的表现越来越好。但即使在具体下游任务上表现出高性能，LLMs 在否定或量化理解等简单语言测试中仍然失败。以前测试 LLMs 对于理解量词的能力的研究表明，随着模型的不断增大，它们在理解大多数类型的量词时变得更好，但在理解极少数类型的量词时变得越来越差，从而呈现出反比例缩放法则的情况。本文质疑了在 LLMs 中反比例缩放极少数类型量词理解能力的说法，并表明这是不合适的测试方法的结果。我们还提出了替代方法来测量 LLMs 的量化理解能力，并展示了随着模型的规模增大，这些行为与以前的研究所展示的不同。LLMs 能够不断理解含义的差异。",
    "tldr": "本文提出了对于大型语言模型（LLMs）对量化理解的探究，并质疑之前研究中关于LLMs理解极少数类型的量词能力呈现反比例缩放的说法，并提出新的测试方法，展示其与以前研究所展示的行为不同。",
    "en_tdlr": "This paper presents an exploration of quantifier comprehension in large language models (LLMs), questions the claims of inverse scaling of few-type quantifier understanding in LLMs, and proposes alternate testing methodologies to demonstrate that LLMs are capable of consistently understanding the differences in meaning between quantifiers."
}