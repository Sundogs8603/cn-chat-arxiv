{
    "title": "Towards Sustainable Learning: Coresets for Data-efficient Deep Learning. (arXiv:2306.01244v1 [cs.LG])",
    "abstract": "To improve the efficiency and sustainability of learning deep models, we propose CREST, the first scalable framework with rigorous theoretical guarantees to identify the most valuable examples for training non-convex models, particularly deep networks. To guarantee convergence to a stationary point of a non-convex function, CREST models the non-convex loss as a series of quadratic functions and extracts a coreset for each quadratic sub-region. In addition, to ensure faster convergence of stochastic gradient methods such as (mini-batch) SGD, CREST iteratively extracts multiple mini-batch coresets from larger random subsets of training data, to ensure nearly-unbiased gradients with small variances. Finally, to further improve scalability and efficiency, CREST identifies and excludes the examples that are learned from the coreset selection pipeline. Our extensive experiments on several deep networks trained on vision and NLP datasets, including CIFAR-10, CIFAR-100, TinyImageNet, and SNLI,",
    "link": "http://arxiv.org/abs/2306.01244",
    "context": "Title: Towards Sustainable Learning: Coresets for Data-efficient Deep Learning. (arXiv:2306.01244v1 [cs.LG])\nAbstract: To improve the efficiency and sustainability of learning deep models, we propose CREST, the first scalable framework with rigorous theoretical guarantees to identify the most valuable examples for training non-convex models, particularly deep networks. To guarantee convergence to a stationary point of a non-convex function, CREST models the non-convex loss as a series of quadratic functions and extracts a coreset for each quadratic sub-region. In addition, to ensure faster convergence of stochastic gradient methods such as (mini-batch) SGD, CREST iteratively extracts multiple mini-batch coresets from larger random subsets of training data, to ensure nearly-unbiased gradients with small variances. Finally, to further improve scalability and efficiency, CREST identifies and excludes the examples that are learned from the coreset selection pipeline. Our extensive experiments on several deep networks trained on vision and NLP datasets, including CIFAR-10, CIFAR-100, TinyImageNet, and SNLI,",
    "path": "papers/23/06/2306.01244.json",
    "total_tokens": 918,
    "translated_title": "迈向可持续学习：用于数据高效深度学习的核心集合",
    "translated_abstract": "为了提高深度模型的效率和可持续性，我们提出了CREST，这是第一个具有严格理论保证的可扩展框架，用于识别训练非凸模型（特别是深度网络）最有价值的样本。为了保证收敛到非凸函数的稳定点，CREST将非凸损失模拟为一系列二次函数，并为每个二次子区域提取一个核心集。此外，为了确保随机梯度方法（如小批量随机梯度下降）的更快收敛，CREST从更大的随机训练数据子集中迭代地提取多个小批量核心集，以确保近似无偏的梯度和小方差。最后，为了进一步提高可扩展性和效率，CREST确定并排除从核心集选择流程中学习到的样本。我们在几个深度网络上进行了大量实验，包括CIFAR-10、CIFAR-100、TinyImageNet和SNLI等视觉和NLP数据集。",
    "tldr": "CREST是一个可扩展的框架，用于数据高效深度学习，通过提取有价值的样本和多个小批量核心集，保证非凸模型收敛到稳定点，提高可扩展性和效率。",
    "en_tdlr": "CREST is a scalable framework for data-efficient deep learning, that guarantees convergence to a stationary point of a non-convex function by extracting valuable examples and multiple mini-batch coresets, improving scalability and efficiency."
}