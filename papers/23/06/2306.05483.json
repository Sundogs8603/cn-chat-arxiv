{
    "title": "On the Importance of Exploration for Generalization in Reinforcement Learning. (arXiv:2306.05483v1 [cs.LG])",
    "abstract": "Existing approaches for improving generalization in deep reinforcement learning (RL) have mostly focused on representation learning, neglecting RL-specific aspects such as exploration. We hypothesize that the agent's exploration strategy plays a key role in its ability to generalize to new environments. Through a series of experiments in a tabular contextual MDP, we show that exploration is helpful not only for efficiently finding the optimal policy for the training environments but also for acquiring knowledge that helps decision making in unseen environments. Based on these observations, we propose EDE: Exploration via Distributional Ensemble, a method that encourages exploration of states with high epistemic uncertainty through an ensemble of Q-value distributions. Our algorithm is the first value-based approach to achieve state-of-the-art on both Procgen and Crafter, two benchmarks for generalization in RL with high-dimensional observations. The open-sourced implementation can be f",
    "link": "http://arxiv.org/abs/2306.05483",
    "context": "Title: On the Importance of Exploration for Generalization in Reinforcement Learning. (arXiv:2306.05483v1 [cs.LG])\nAbstract: Existing approaches for improving generalization in deep reinforcement learning (RL) have mostly focused on representation learning, neglecting RL-specific aspects such as exploration. We hypothesize that the agent's exploration strategy plays a key role in its ability to generalize to new environments. Through a series of experiments in a tabular contextual MDP, we show that exploration is helpful not only for efficiently finding the optimal policy for the training environments but also for acquiring knowledge that helps decision making in unseen environments. Based on these observations, we propose EDE: Exploration via Distributional Ensemble, a method that encourages exploration of states with high epistemic uncertainty through an ensemble of Q-value distributions. Our algorithm is the first value-based approach to achieve state-of-the-art on both Procgen and Crafter, two benchmarks for generalization in RL with high-dimensional observations. The open-sourced implementation can be f",
    "path": "papers/23/06/2306.05483.json",
    "total_tokens": 867,
    "translated_title": "关于勘探在强化学习泛化中的重要性",
    "translated_abstract": "目前提高强化学习泛化能力的方法主要集中于表示学习，而忽略了勘探等强化学习特有的方面。我们假设代理的勘探策略在其泛化到新环境的能力中发挥了关键作用。通过一系列实验，在一个基于表格的情境MDP中展示了勘探不仅有助于在训练环境中有效地找到最优策略，而且有助于获取知识以便在未知环境中进行决策。基于这些观察结果，我们提出了EDE：通过分布式集合实现勘探，这是一种方法，通过Q值分布的集合鼓励开发具有高先验不确定性的状态的勘探。我们的算法是第一个在高维观察中获得Procgen和Crafter中泛化性最好的值方法。提供了开源实现。",
    "tldr": "本论文研究了勘探在强化学习中的重要性，提出了一种名为EDE的基于值的算法，它是第一个在Procgen和Crafter等环境中取得最先进结果的方法，具有实用价值。",
    "en_tdlr": "This paper studies the importance of exploration in reinforcement learning and proposes a value-based algorithm called EDE, which is the first approach to achieve state-of-the-art results in benchmarks with high-dimensional observations such as Procgen and Crafter, and has practical value."
}