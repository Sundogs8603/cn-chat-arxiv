{
    "title": "Symbolic Chain-of-Thought Distillation: Small Models Can Also \"Think\" Step-by-Step. (arXiv:2306.14050v1 [cs.CL])",
    "abstract": "Chain-of-thought prompting (e.g., \"Let's think step-by-step\") primes large language models to verbalize rationalization for their predictions. While chain-of-thought can lead to dramatic performance gains, benefits appear to emerge only for sufficiently large models (beyond 50B parameters). We show that orders-of-magnitude smaller models (125M -- 1.3B parameters) can still benefit from chain-of-thought prompting. To achieve this, we introduce Symbolic Chain-of-Thought Distillation (SCoTD), a method to train a smaller student model on rationalizations sampled from a significantly larger teacher model. Experiments across several commonsense benchmarks show that: 1) SCoTD enhances the performance of the student model in both supervised and few-shot settings, and especially for challenge sets; 2) sampling many reasoning chains per instance from the teacher is paramount; and 3) after distillation, student chain-of-thoughts are judged by humans as comparable to the teacher, despite orders of",
    "link": "http://arxiv.org/abs/2306.14050",
    "context": "Title: Symbolic Chain-of-Thought Distillation: Small Models Can Also \"Think\" Step-by-Step. (arXiv:2306.14050v1 [cs.CL])\nAbstract: Chain-of-thought prompting (e.g., \"Let's think step-by-step\") primes large language models to verbalize rationalization for their predictions. While chain-of-thought can lead to dramatic performance gains, benefits appear to emerge only for sufficiently large models (beyond 50B parameters). We show that orders-of-magnitude smaller models (125M -- 1.3B parameters) can still benefit from chain-of-thought prompting. To achieve this, we introduce Symbolic Chain-of-Thought Distillation (SCoTD), a method to train a smaller student model on rationalizations sampled from a significantly larger teacher model. Experiments across several commonsense benchmarks show that: 1) SCoTD enhances the performance of the student model in both supervised and few-shot settings, and especially for challenge sets; 2) sampling many reasoning chains per instance from the teacher is paramount; and 3) after distillation, student chain-of-thoughts are judged by humans as comparable to the teacher, despite orders of",
    "path": "papers/23/06/2306.14050.json",
    "total_tokens": 973,
    "translated_title": "符号化思维链提炼：小模型也能逐步“思考”",
    "translated_abstract": "思维链提示（例如“我们来逐步思考”）会促使大型语言模型对其预测进行合理化解释。虽然思维链可以带来显著的性能提升，但益处似乎仅适用于足够大的模型（超过50亿参数）。我们展示了数量级较小（125M-1.3B参数）的模型仍然可以从思维链提示中受益。为了实现这一点，我们引入了符号化思维链提炼（SCoTD），一种将较大的教师模型中抽样出的合理化解释用于训练较小的学生模型的方法。在几个常识基准测试上的实验表明：1）SCoTD提升了学生模型的性能，无论是在监督学习还是少样本学习的情况下，特别是在挑战集方面。2）从教师模型中抽样多个推理链是至关重要的。3）在提炼后，虽然参数少得多，但学生思维链与教师相当。",
    "tldr": "本研究提出了符号化思维链提炼（SCoTD）方法，该方法使用从大型教师模型中抽样出的合理化解释来训练数量级更小的学生模型，从而实现小模型也能受益于思维链提示，提升模型性能。",
    "en_tdlr": "This paper proposes Symbolic Chain-of-Thought Distillation (SCoTD) method to train smaller student models with rationalizations sampled from larger teacher models, which achieves performance gains from chain-of-thought prompts even for significantly smaller models."
}