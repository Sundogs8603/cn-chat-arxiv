{
    "title": "Understanding Deep Generative Models with Generalized Empirical Likelihoods. (arXiv:2306.09780v1 [cs.LG])",
    "abstract": "Understanding how well a deep generative model captures a distribution of high-dimensional data remains an important open challenge. It is especially difficult for certain model classes, such as Generative Adversarial Networks and Diffusion Models, whose models do not admit exact likelihoods. In this work, we demonstrate that generalized empirical likelihood (GEL) methods offer a family of diagnostic tools that can identify many deficiencies of deep generative models (DGMs). We show, with appropriate specification of moment conditions, that the proposed method can identify which modes have been dropped, the degree to which DGMs are mode imbalanced, and whether DGMs sufficiently capture intra-class diversity. We show how to combine techniques from Maximum Mean Discrepancy and Generalized Empirical Likelihood to create not only distribution tests that retain per-sample interpretability, but also metrics that include label information. We find that such tests predict the degree of mode dr",
    "link": "http://arxiv.org/abs/2306.09780",
    "context": "Title: Understanding Deep Generative Models with Generalized Empirical Likelihoods. (arXiv:2306.09780v1 [cs.LG])\nAbstract: Understanding how well a deep generative model captures a distribution of high-dimensional data remains an important open challenge. It is especially difficult for certain model classes, such as Generative Adversarial Networks and Diffusion Models, whose models do not admit exact likelihoods. In this work, we demonstrate that generalized empirical likelihood (GEL) methods offer a family of diagnostic tools that can identify many deficiencies of deep generative models (DGMs). We show, with appropriate specification of moment conditions, that the proposed method can identify which modes have been dropped, the degree to which DGMs are mode imbalanced, and whether DGMs sufficiently capture intra-class diversity. We show how to combine techniques from Maximum Mean Discrepancy and Generalized Empirical Likelihood to create not only distribution tests that retain per-sample interpretability, but also metrics that include label information. We find that such tests predict the degree of mode dr",
    "path": "papers/23/06/2306.09780.json",
    "total_tokens": 937,
    "translated_title": "利用广义经验似然方法理解深度生成模型",
    "translated_abstract": "理解深度生成模型如何捕获高维数据分布仍然是一个重要的挑战。对于某些模型类别，比如生成对抗网络和扩散模型等不允许精确似然度量的模型，尤其困难。本文展示了广义经验似然（GEL）方法提供了一系列诊断工具来识别深度生成模型的许多缺陷。通过适当的矩条件规定，我们展示了本文提出的方法可以识别哪些模式被删除、DGM存在的模式不平衡程度以及DGM是否足够捕获类内多样性。我们展示了如何结合最大均值差异和广义经验似然的技术，不仅创造了保留每个样本可解释性的分布测试，还包括标签信息的指标。我们发现这些测试可以预测模式降低的程度。",
    "tldr": "本文展示了广义经验似然（GEL）方法提供了一系列诊断工具来识别深度生成模型的许多缺陷，并结合最大均值差异和广义经验似然的技术，创造了保留每个样本可解释性的分布测试，还包括标签信息的指标。这些测试可以预测模式降低的程度。"
}