{
    "title": "Informed POMDP: Leveraging Additional Information in Model-Based RL. (arXiv:2306.11488v2 [cs.LG] UPDATED)",
    "abstract": "In this work, we generalize the problem of learning through interaction in a POMDP by accounting for eventual additional information available at training time. First, we introduce the informed POMDP, a new learning paradigm offering a clear distinction between the training information and the execution observation. Next, we propose an objective for learning a sufficient statistic from the history for the optimal control that leverages this information. We then show that this informed objective consists of learning an environment model from which we can sample latent trajectories. Finally, we show for the Dreamer algorithm that the convergence speed of the policies is sometimes greatly improved on several environments by using this informed environment model. Those results and the simplicity of the proposed adaptation advocate for a systematic consideration of eventual additional information when learning in a POMDP using model-based RL.",
    "link": "http://arxiv.org/abs/2306.11488",
    "context": "Title: Informed POMDP: Leveraging Additional Information in Model-Based RL. (arXiv:2306.11488v2 [cs.LG] UPDATED)\nAbstract: In this work, we generalize the problem of learning through interaction in a POMDP by accounting for eventual additional information available at training time. First, we introduce the informed POMDP, a new learning paradigm offering a clear distinction between the training information and the execution observation. Next, we propose an objective for learning a sufficient statistic from the history for the optimal control that leverages this information. We then show that this informed objective consists of learning an environment model from which we can sample latent trajectories. Finally, we show for the Dreamer algorithm that the convergence speed of the policies is sometimes greatly improved on several environments by using this informed environment model. Those results and the simplicity of the proposed adaptation advocate for a systematic consideration of eventual additional information when learning in a POMDP using model-based RL.",
    "path": "papers/23/06/2306.11488.json",
    "total_tokens": 841,
    "translated_title": "透视额外信息的Informed POMDP: 模型驱动强化学习中的利用",
    "translated_abstract": "本文通过考虑训练时可用的额外信息，推广了POMDP中的交互学习问题。首先，我们引入了Informed POMDP，这是一种新的学习范式，清晰区分了训练信息和执行观察。接下来，我们提出了一个目标，用于从历史记录中学习出充分统计信息，以实现最优控制并利用这些信息。然后，我们展示了这个Informed目标由学习一个环境模型组成，从其中我们可以采样隐式轨迹。最后，我们证明在几个环境中，使用这个Informed环境模型可以大大提高Dreamer算法策略的收敛速度。这些结果以及提议的简单适应性，倡导在使用模型驱动强化学习学习POMDP时，要系统地考虑可用的额外信息。",
    "tldr": "本文提出了Informed POMDP，这是一种新的学习范式，通过学习环境模型来利用训练时可用的额外信息，该模型可以提高Dreamer算法策略的收敛速度。",
    "en_tdlr": "This paper proposes the Informed POMDP, a new learning paradigm that leverages eventual additional information available at training time by learning an environment model. This model can greatly improve the convergence speed of the policies in the Dreamer algorithm on several environments."
}