{
    "title": "FDINet: Protecting against DNN Model Extraction via Feature Distortion Index. (arXiv:2306.11338v2 [cs.CR] UPDATED)",
    "abstract": "Machine Learning as a Service (MLaaS) platforms have gained popularity due to their accessibility, cost-efficiency, scalability, and rapid development capabilities. However, recent research has highlighted the vulnerability of cloud-based models in MLaaS to model extraction attacks. In this paper, we introduce FDINET, a novel defense mechanism that leverages the feature distribution of deep neural network (DNN) models. Concretely, by analyzing the feature distribution from the adversary's queries, we reveal that the feature distribution of these queries deviates from that of the model's training set. Based on this key observation, we propose Feature Distortion Index (FDI), a metric designed to quantitatively measure the feature distribution deviation of received queries. The proposed FDINET utilizes FDI to train a binary detector and exploits FDI similarity to identify colluding adversaries from distributed extraction attacks. We conduct extensive experiments to evaluate FDINET against",
    "link": "http://arxiv.org/abs/2306.11338",
    "context": "Title: FDINet: Protecting against DNN Model Extraction via Feature Distortion Index. (arXiv:2306.11338v2 [cs.CR] UPDATED)\nAbstract: Machine Learning as a Service (MLaaS) platforms have gained popularity due to their accessibility, cost-efficiency, scalability, and rapid development capabilities. However, recent research has highlighted the vulnerability of cloud-based models in MLaaS to model extraction attacks. In this paper, we introduce FDINET, a novel defense mechanism that leverages the feature distribution of deep neural network (DNN) models. Concretely, by analyzing the feature distribution from the adversary's queries, we reveal that the feature distribution of these queries deviates from that of the model's training set. Based on this key observation, we propose Feature Distortion Index (FDI), a metric designed to quantitatively measure the feature distribution deviation of received queries. The proposed FDINET utilizes FDI to train a binary detector and exploits FDI similarity to identify colluding adversaries from distributed extraction attacks. We conduct extensive experiments to evaluate FDINET against",
    "path": "papers/23/06/2306.11338.json",
    "total_tokens": 919,
    "translated_title": "FDINet：利用特征失真指数保护 DNN 模型免受模型提取攻击",
    "translated_abstract": "机器学习即服务（MLaaS）平台由于其易用性、成本效益、可扩展性和快速开发能力而变得越来越受欢迎。然而，最近的研究强调了 MLaaS 中基于云的模型对模型提取攻击的脆弱性。本文介绍了 FDINET，一种利用深度神经网络（DNN）模型特征分布的新颖防御机制。具体地，通过分析对手的查询的特征分布，我们揭示了这些查询的特征分布与模型的训练集不同。基于这个关键观察，我们提出了特征失真指数（FDI），这是一种度量设计，用于定量测量接收到的查询的特征分布偏差。所提出的 FDINET 利用 FDI 训练一个二进制检测器，并利用 FDI 相似性识别分布式提取攻击中的勾结敌人。我们进行了广泛的实验来评估 FDINET 对抗模型提取攻击的效果。",
    "tldr": "FDINet是一种新颖的防御机制，该机制利用特征失真指数来保护DNN模型免受模型提取攻击，并利用FDI相似性来识别分布式提取攻击中的勾结敌人。"
}