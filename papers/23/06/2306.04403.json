{
    "title": "Policy-Based Self-Competition for Planning Problems. (arXiv:2306.04403v1 [cs.LG])",
    "abstract": "AlphaZero-type algorithms may stop improving on single-player tasks in case the value network guiding the tree search is unable to approximate the outcome of an episode sufficiently well. One technique to address this problem is transforming the single-player task through self-competition. The main idea is to compute a scalar baseline from the agent's historical performances and to reshape an episode's reward into a binary output, indicating whether the baseline has been exceeded or not. However, this baseline only carries limited information for the agent about strategies how to improve. We leverage the idea of self-competition and directly incorporate a historical policy into the planning process instead of its scalar performance. Based on the recently introduced Gumbel AlphaZero (GAZ), we propose our algorithm GAZ 'Play-to-Plan' (GAZ PTP), in which the agent learns to find strong trajectories by planning against possible strategies of its past self. We show the effectiveness of our ",
    "link": "http://arxiv.org/abs/2306.04403",
    "context": "Title: Policy-Based Self-Competition for Planning Problems. (arXiv:2306.04403v1 [cs.LG])\nAbstract: AlphaZero-type algorithms may stop improving on single-player tasks in case the value network guiding the tree search is unable to approximate the outcome of an episode sufficiently well. One technique to address this problem is transforming the single-player task through self-competition. The main idea is to compute a scalar baseline from the agent's historical performances and to reshape an episode's reward into a binary output, indicating whether the baseline has been exceeded or not. However, this baseline only carries limited information for the agent about strategies how to improve. We leverage the idea of self-competition and directly incorporate a historical policy into the planning process instead of its scalar performance. Based on the recently introduced Gumbel AlphaZero (GAZ), we propose our algorithm GAZ 'Play-to-Plan' (GAZ PTP), in which the agent learns to find strong trajectories by planning against possible strategies of its past self. We show the effectiveness of our ",
    "path": "papers/23/06/2306.04403.json",
    "total_tokens": 834,
    "translated_title": "基于策略的自对抗算法解决规划问题",
    "translated_abstract": "AlphaZero类型的算法在单人任务上如果由于引导树搜索的价值网络无法足够准确地近似一次运算的结果，就有可能停止提高。处理这一问题的技术之一是通过自对抗来转化单人任务。主要思想是从代理的历史表现计算标量基线，并将一个序列的奖励重构为二进制输出，指示是否超过了基线。但是，此基线对代理效率提升的战略信息有限。我们利用自对抗的思想，直接将历史策略纳入规划过程中，而不是将其标量表现纳入。基于最近引入的Gumbel AlphaZero（GAZ），我们提出了GAZ'Play-to-Plan'（GAZ PTP）算法，代理通过规划反对自己的可能策略来学习找到强大的轨迹。我们展示了我们算法的有效性。",
    "tldr": "本文介绍了一种基于策略的自对抗算法，该算法将历史策略纳入规划过程中，从而实现代理的效率提升和强大的轨迹查找。",
    "en_tdlr": "This paper presents a policy-based self-competition algorithm, which incorporates historical policies into the planning process to improve agent efficiency and find strong trajectories."
}