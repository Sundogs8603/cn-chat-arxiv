{
    "title": "Limits for Learning with Language Models. (arXiv:2306.12213v1 [cs.CL])",
    "abstract": "With the advent of large language models (LLMs), the trend in NLP has been to train LLMs on vast amounts of data to solve diverse language understanding and generation tasks. The list of LLM successes is long and varied. Nevertheless, several recent papers provide empirical evidence that LLMs fail to capture important aspects of linguistic meaning. Focusing on universal quantification, we provide a theoretical foundation for these empirical findings by proving that LLMs cannot learn certain fundamental semantic properties including semantic entailment and consistency as they are defined in formal semantics. More generally, we show that LLMs are unable to learn concepts beyond the first level of the Borel Hierarchy, which imposes severe limits on the ability of LMs, both large and small, to capture many aspects of linguistic meaning. This means that LLMs will continue to operate without formal guarantees on tasks that require entailments and deep linguistic understanding.",
    "link": "http://arxiv.org/abs/2306.12213",
    "context": "Title: Limits for Learning with Language Models. (arXiv:2306.12213v1 [cs.CL])\nAbstract: With the advent of large language models (LLMs), the trend in NLP has been to train LLMs on vast amounts of data to solve diverse language understanding and generation tasks. The list of LLM successes is long and varied. Nevertheless, several recent papers provide empirical evidence that LLMs fail to capture important aspects of linguistic meaning. Focusing on universal quantification, we provide a theoretical foundation for these empirical findings by proving that LLMs cannot learn certain fundamental semantic properties including semantic entailment and consistency as they are defined in formal semantics. More generally, we show that LLMs are unable to learn concepts beyond the first level of the Borel Hierarchy, which imposes severe limits on the ability of LMs, both large and small, to capture many aspects of linguistic meaning. This means that LLMs will continue to operate without formal guarantees on tasks that require entailments and deep linguistic understanding.",
    "path": "papers/23/06/2306.12213.json",
    "total_tokens": 934,
    "translated_title": "语言模型的学习限制",
    "translated_abstract": "随着大型语言模型（LLMs）的出现，NLP的趋势已经转向在大量数据上训练LLMs，以解决各种语言理解和生成任务。LLM的成功列表很长，也很多样化。尽管如此，最近的几篇论文提供了经验性的证据表明，LLMs未能捕捉语言意义的重要方面。本文侧重于普遍量化，通过证明LLMs无法学习某些基本的语义属性（如正式语义中定义的语义蕴含和一致性），为这些经验性发现提供了理论基础。 更普遍地说，我们展示了LLMs无法学习超出可数集层次结构（Borel Hierarchy）第一层的概念，这对于LLMs的能力施加了严格限制，无论其大小，都无法捕捉许多语言意义方面的内容。这意味着，LLMs将继续在需要蕴含和深刻语言理解的任务中没有正式保证地运行。",
    "tldr": "本论文研究了语言模型的学习限制，证明了大型语言模型无法学习某些基本的语义属性，包括语义蕴含和一致性，并且无法学习超出可数集层次结构第一层的概念，这限制了它们在语言意义方面的能力。",
    "en_tdlr": "This paper studies the learning limitations of language models and proves that large language models cannot learn certain fundamental semantic properties, including semantic entailment and consistency, and can't learn concepts beyond the first level of the Borel Hierarchy, which severely limits their ability to capture many aspects of linguistic meaning."
}