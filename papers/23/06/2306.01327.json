{
    "title": "Speech Translation with Foundation Models and Optimal Transport: UPC at IWSLT23. (arXiv:2306.01327v1 [cs.CL])",
    "abstract": "This paper describes the submission of the UPC Machine Translation group to the IWSLT 2023 Offline Speech Translation task. Our Speech Translation systems utilize foundation models for speech (wav2vec 2.0) and text (mBART50). We incorporate a Siamese pretraining step of the speech and text encoders with CTC and Optimal Transport, to adapt the speech representations to the space of the text model, thus maximizing transfer learning from MT. After this pretraining, we fine-tune our system end-to-end on ST, with Cross Entropy and Knowledge Distillation. Apart from the available ST corpora, we create synthetic data with SegAugment to better adapt our models to the custom segmentations of the IWSLT test sets. Our best single model obtains 31.2 BLEU points on MuST-C tst-COMMON, 29.8 points on IWLST.tst2020 and 33.4 points on the newly released IWSLT.ACLdev2023.",
    "link": "http://arxiv.org/abs/2306.01327",
    "context": "Title: Speech Translation with Foundation Models and Optimal Transport: UPC at IWSLT23. (arXiv:2306.01327v1 [cs.CL])\nAbstract: This paper describes the submission of the UPC Machine Translation group to the IWSLT 2023 Offline Speech Translation task. Our Speech Translation systems utilize foundation models for speech (wav2vec 2.0) and text (mBART50). We incorporate a Siamese pretraining step of the speech and text encoders with CTC and Optimal Transport, to adapt the speech representations to the space of the text model, thus maximizing transfer learning from MT. After this pretraining, we fine-tune our system end-to-end on ST, with Cross Entropy and Knowledge Distillation. Apart from the available ST corpora, we create synthetic data with SegAugment to better adapt our models to the custom segmentations of the IWSLT test sets. Our best single model obtains 31.2 BLEU points on MuST-C tst-COMMON, 29.8 points on IWLST.tst2020 and 33.4 points on the newly released IWSLT.ACLdev2023.",
    "path": "papers/23/06/2306.01327.json",
    "total_tokens": 924,
    "translated_title": "使用基础模型和最优传输的语音翻译：UPC在IWSLT23的参赛",
    "translated_abstract": "本篇论文描述了UPC机器翻译组参加IWSLT 2023离线语音翻译任务的系统。我们使用基础模型分别处理语音（wav2vec 2.0）和文本（mBART50），通过CTC和最优传输的“联体”预训练步骤，将语音表示适应到文本模型的空间，以实现最大的迁移学习。在此预训练后，我们使用交叉熵和知识蒸馏在ST上进行端到端微调系统。除了已有的ST语料库外，我们使用SegAugment创建合成数据，以更好地适应IWSLT测试集的自定义分割。我们的最佳单模型在MuST-C tst-COMMON上获得31.2 BLEU分，IWLST.tst2020上获得29.8分，在新发布的IWSLT.ACLdev2023上获得33.4分。",
    "tldr": "本论文阐述了UPC机器翻译组使用基础模型和最优传输技术以及合成数据在IWSLT23离线语音翻译任务中的表现，最佳单模型在IWSLT.ACLdev2023上获得33.4分。",
    "en_tdlr": "This paper presents the UPC Machine Translation group's submission for the IWSLT 2023 Offline Speech Translation task, using foundation models for speech and text, Siamese pretraining with CTC and Optimal Transport, and synthetic data with SegAugment. The best single model achieved 33.4 BLEU points on the IWSLT.ACLdev2023 dataset."
}