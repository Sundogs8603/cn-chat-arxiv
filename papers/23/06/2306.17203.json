{
    "title": "Diff-Foley: Synchronized Video-to-Audio Synthesis with Latent Diffusion Models. (arXiv:2306.17203v1 [cs.SD])",
    "abstract": "The Video-to-Audio (V2A) model has recently gained attention for its practical application in generating audio directly from silent videos, particularly in video/film production. However, previous methods in V2A have limited generation quality in terms of temporal synchronization and audio-visual relevance. We present Diff-Foley, a synchronized Video-to-Audio synthesis method with a latent diffusion model (LDM) that generates high-quality audio with improved synchronization and audio-visual relevance. We adopt contrastive audio-visual pretraining (CAVP) to learn more temporally and semantically aligned features, then train an LDM with CAVP-aligned visual features on spectrogram latent space. The CAVP-aligned features enable LDM to capture the subtler audio-visual correlation via a cross-attention module. We further significantly improve sample quality with `double guidance'. Diff-Foley achieves state-of-the-art V2A performance on current large scale V2A dataset. Furthermore, we demonst",
    "link": "http://arxiv.org/abs/2306.17203",
    "context": "Title: Diff-Foley: Synchronized Video-to-Audio Synthesis with Latent Diffusion Models. (arXiv:2306.17203v1 [cs.SD])\nAbstract: The Video-to-Audio (V2A) model has recently gained attention for its practical application in generating audio directly from silent videos, particularly in video/film production. However, previous methods in V2A have limited generation quality in terms of temporal synchronization and audio-visual relevance. We present Diff-Foley, a synchronized Video-to-Audio synthesis method with a latent diffusion model (LDM) that generates high-quality audio with improved synchronization and audio-visual relevance. We adopt contrastive audio-visual pretraining (CAVP) to learn more temporally and semantically aligned features, then train an LDM with CAVP-aligned visual features on spectrogram latent space. The CAVP-aligned features enable LDM to capture the subtler audio-visual correlation via a cross-attention module. We further significantly improve sample quality with `double guidance'. Diff-Foley achieves state-of-the-art V2A performance on current large scale V2A dataset. Furthermore, we demonst",
    "path": "papers/23/06/2306.17203.json",
    "total_tokens": 959,
    "translated_title": "Diff-Foley: 使用潜在扩散模型实现同步的视频到音频合成",
    "translated_abstract": "视频到音频（V2A）模型近年来引起了广泛关注，它在直接从无声视频生成音频方面具有实际应用，特别是在视频/电影制作领域。然而，先前V2A方法在时间同步和音视频关联方面存在生成质量限制。我们提出了Diff-Foley，一种具有潜在扩散模型（LDM）的同步视频到音频合成方法，可以生成具有改进的同步性和音视频关联性的高质量音频。我们采用对比式音视频预训练（CAVP）来学习更具时间和语义对齐的特征，然后在声谱图潜在空间上使用经CAVP对齐的视觉特征训练LDM。CAVP对齐的特征通过交叉注意力模块使LDM能够捕捉到更微妙的音视频相关性。我们通过“双重引导”进一步显著提高样本质量。Diff-Foley在当前大规模V2A数据集上实现了最先进的V2A性能。此外，我们展示了...",
    "tldr": "Diff-Foley是一种使用潜在扩散模型实现同步的视频到音频合成方法，通过对比式音视频预训练来学习特征，并在声谱图潜在空间上训练潜在扩散模型，以提高生成音频的同步性和音视频关联性。",
    "en_tdlr": "Diff-Foley is a synchronized Video-to-Audio synthesis method using latent diffusion models, which improves the synchronization and audio-visual relevance by learning features through contrastive audio-visual pretraining and training the diffusion model on the spectrogram latent space."
}