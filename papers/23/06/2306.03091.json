{
    "title": "RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems. (arXiv:2306.03091v2 [cs.CL] UPDATED)",
    "abstract": "Large Language Models (LLMs) have greatly advanced code auto-completion systems, with a potential for substantial productivity enhancements for developers. However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios. To fill this gap, we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems. RepoBench supports both Python and Java and consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). Each task respectively measures the system's ability to retrieve the most relevant code snippets from other files as cross-file context, predict the next line of code with cross-file and in-file context, and handle complex tasks that require a combination of both retrieval and next-line prediction. RepoBench aims to facilitate a more complete comparison of performance",
    "link": "http://arxiv.org/abs/2306.03091",
    "context": "Title: RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems. (arXiv:2306.03091v2 [cs.CL] UPDATED)\nAbstract: Large Language Models (LLMs) have greatly advanced code auto-completion systems, with a potential for substantial productivity enhancements for developers. However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios. To fill this gap, we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems. RepoBench supports both Python and Java and consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). Each task respectively measures the system's ability to retrieve the most relevant code snippets from other files as cross-file context, predict the next line of code with cross-file and in-file context, and handle complex tasks that require a combination of both retrieval and next-line prediction. RepoBench aims to facilitate a more complete comparison of performance",
    "path": "papers/23/06/2306.03091.json",
    "total_tokens": 917,
    "translated_title": "RepoBench：评估代码自动补全系统的代码库级别性能基准",
    "translated_abstract": "大型语言模型（LLMs）在代码自动补全系统方面取得了巨大进步，有潜力为开发人员提供显著的生产力增强。然而，当前的基准主要集中在单个文件任务上，对于更复杂的真实世界的多文件编程场景的评估存在差距。为填补这一差距，我们介绍了RepoBench，一个专门设计用于评估代码库级别的代码自动补全系统的新基准。RepoBench支持Python和Java，并包含三个相互关联的评估任务：RepoBench-R（检索）、RepoBench-C（代码补全）和RepoBench-P（流水线）。每个任务分别测量系统检索其他文件中最相关代码片段的能力、使用跨文件和文件内上下文预测下一行代码的能力以及处理需要检索和下一行预测组合的复杂任务的能力。RepoBench旨在促进对性能的更全面比较。",
    "tldr": "RepoBench是一个评估代码自动补全系统在代码库级别上性能的基准，支持Python和Java，包含三个相互关联的评估任务。它旨在填补当前基准在多文件编程场景方面的评估差距，并为不同系统的性能提供更全面的比较。",
    "en_tdlr": "RepoBench is a benchmark for evaluating the performance of code auto-completion systems at the repository level. It supports Python and Java and consists of three interconnected evaluation tasks. It aims to fill the evaluation gap in multi-file programming scenarios and provide a more comprehensive comparison of performance for different systems."
}