{
    "title": "Large Language Models Sometimes Generate Purely Negatively-Reinforced Text. (arXiv:2306.07567v1 [cs.LG])",
    "abstract": "When using adversarial training, it is common practice to train against the most egregious failures. However, this might imply using examples with sensitive information (such as leaked passwords or security vulnerabilities) as training data. One might assume that language models trained with gradient descent never generate text snippets which were only present in examples associated with the lowest possible reward. In this paper, we show that this assumption is wrong: in some situations, large language models do learn from such negatively-reinforced examples. We present a specific training setup that enables Pythia-160M to generate passwords with a probability slightly greater than chance, despite only showing it these passwords on examples where the model is incentivized to not output these passwords. Our code is available at https://github.com/FabienRoger/Learning-From-Negative-Examples",
    "link": "http://arxiv.org/abs/2306.07567",
    "context": "Title: Large Language Models Sometimes Generate Purely Negatively-Reinforced Text. (arXiv:2306.07567v1 [cs.LG])\nAbstract: When using adversarial training, it is common practice to train against the most egregious failures. However, this might imply using examples with sensitive information (such as leaked passwords or security vulnerabilities) as training data. One might assume that language models trained with gradient descent never generate text snippets which were only present in examples associated with the lowest possible reward. In this paper, we show that this assumption is wrong: in some situations, large language models do learn from such negatively-reinforced examples. We present a specific training setup that enables Pythia-160M to generate passwords with a probability slightly greater than chance, despite only showing it these passwords on examples where the model is incentivized to not output these passwords. Our code is available at https://github.com/FabienRoger/Learning-From-Negative-Examples",
    "path": "papers/23/06/2306.07567.json",
    "total_tokens": 805,
    "translated_title": "大型语言模型有时生成纯负反馈文本",
    "translated_abstract": "在使用对抗性训练时，通常会训练反对最严重失败的案例。然而，这可能会意味着使用包含敏感信息（例如泄露的密码或安全漏洞）的案例作为训练数据。我们可能会认为使用梯度下降算法训练的语言模型永远不会生成仅在与最低奖励相关联的示例中出现的文本片段。本文表明这种假设是错误的：在某些情况下，大型语言模型确实从这种纯负反馈的示例中学习到了东西。我们提出了一种特定的训练设置，使得Pythia-160M能够生成密码的概率略高于随机，尽管仅在对模型不输出这些密码的示例中展示了这些密码。我们的代码可在https://github.com/FabienRoger/Learning-From-Negative-Examples上找到。",
    "tldr": "大型语言模型有时会从仅包含负奖励的例子中学习，导致生成类似泄漏密码或安全漏洞等敏感信息的文本",
    "en_tdlr": "Large language models may sometimes learn from examples with purely negative feedback, leading to the generation of sensitive information such as leaked passwords or security vulnerabilities."
}