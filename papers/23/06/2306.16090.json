{
    "title": "Empirical Loss Landscape Analysis of Neural Network Activation Functions. (arXiv:2306.16090v1 [cs.LG])",
    "abstract": "Activation functions play a significant role in neural network design by enabling non-linearity. The choice of activation function was previously shown to influence the properties of the resulting loss landscape. Understanding the relationship between activation functions and loss landscape properties is important for neural architecture and training algorithm design. This study empirically investigates neural network loss landscapes associated with hyperbolic tangent, rectified linear unit, and exponential linear unit activation functions. Rectified linear unit is shown to yield the most convex loss landscape, and exponential linear unit is shown to yield the least flat loss landscape, and to exhibit superior generalisation performance. The presence of wide and narrow valleys in the loss landscape is established for all activation functions, and the narrow valleys are shown to correlate with saturated neurons and implicitly regularised network configurations.",
    "link": "http://arxiv.org/abs/2306.16090",
    "context": "Title: Empirical Loss Landscape Analysis of Neural Network Activation Functions. (arXiv:2306.16090v1 [cs.LG])\nAbstract: Activation functions play a significant role in neural network design by enabling non-linearity. The choice of activation function was previously shown to influence the properties of the resulting loss landscape. Understanding the relationship between activation functions and loss landscape properties is important for neural architecture and training algorithm design. This study empirically investigates neural network loss landscapes associated with hyperbolic tangent, rectified linear unit, and exponential linear unit activation functions. Rectified linear unit is shown to yield the most convex loss landscape, and exponential linear unit is shown to yield the least flat loss landscape, and to exhibit superior generalisation performance. The presence of wide and narrow valleys in the loss landscape is established for all activation functions, and the narrow valleys are shown to correlate with saturated neurons and implicitly regularised network configurations.",
    "path": "papers/23/06/2306.16090.json",
    "total_tokens": 967,
    "translated_title": "神经网络激活函数的经验损失曲面分析",
    "translated_abstract": "激活函数通过引入非线性在神经网络设计中起着重要作用。先前的研究表明激活函数的选择会影响损失曲面的性质。了解激活函数与损失曲面性质的关系对神经网络架构和训练算法设计是重要的。本研究从经验上分析了与双曲正切、修正线性单元和指数线性单元激活函数相关的神经网络损失曲面。实验证明修正线性单元产生最凸型的损失曲面，指数线性单元产生最平坦的损失曲面，并且展现出更优的泛化性能。对于所有激活函数，损失曲面中存在宽阔和狭窄的山谷，并且狭窄的山谷与饱和神经元和隐含的正则化网络结构相关。",
    "tldr": "本研究通过经验分析了双曲正切、修正线性单元和指数线性单元激活函数相关的神经网络损失曲面，发现修正线性单元呈现最凸型曲面，指数线性单元呈现最平坦曲面并具有更优的泛化性能。所有激活函数的损失曲面中存在宽阔和狭窄的山谷，而狭窄的山谷与饱和神经元和隐含的正则化网络结构相关。"
}