{
    "title": "GPT-Based Models Meet Simulation: How to Efficiently Use Large-Scale Pre-Trained Language Models Across Simulation Tasks. (arXiv:2306.13679v1 [cs.HC])",
    "abstract": "The disruptive technology provided by large-scale pre-trained language models (LLMs) such as ChatGPT or GPT-4 has received significant attention in several application domains, often with an emphasis on high-level opportunities and concerns. This paper is the first examination regarding the use of LLMs for scientific simulations. We focus on four modeling and simulation tasks, each time assessing the expected benefits and limitations of LLMs while providing practical guidance for modelers regarding the steps involved. The first task is devoted to explaining the structure of a conceptual model to promote the engagement of participants in the modeling process. The second task focuses on summarizing simulation outputs, so that model users can identify a preferred scenario. The third task seeks to broaden accessibility to simulation platforms by conveying the insights of simulation visualizations via text. Finally, the last task evokes the possibility of explaining simulation errors and pr",
    "link": "http://arxiv.org/abs/2306.13679",
    "context": "Title: GPT-Based Models Meet Simulation: How to Efficiently Use Large-Scale Pre-Trained Language Models Across Simulation Tasks. (arXiv:2306.13679v1 [cs.HC])\nAbstract: The disruptive technology provided by large-scale pre-trained language models (LLMs) such as ChatGPT or GPT-4 has received significant attention in several application domains, often with an emphasis on high-level opportunities and concerns. This paper is the first examination regarding the use of LLMs for scientific simulations. We focus on four modeling and simulation tasks, each time assessing the expected benefits and limitations of LLMs while providing practical guidance for modelers regarding the steps involved. The first task is devoted to explaining the structure of a conceptual model to promote the engagement of participants in the modeling process. The second task focuses on summarizing simulation outputs, so that model users can identify a preferred scenario. The third task seeks to broaden accessibility to simulation platforms by conveying the insights of simulation visualizations via text. Finally, the last task evokes the possibility of explaining simulation errors and pr",
    "path": "papers/23/06/2306.13679.json",
    "total_tokens": 930,
    "translated_title": "基于GPT的模型遇见仿真：如何有效地应用大规模预训练语言模型于仿真任务",
    "translated_abstract": "大规模预训练语言模型（LLMs），如ChatGPT或GPT-4所提供的颠覆性技术，在多个应用领域引起了广泛关注，通常强调高水平的机会和担忧。本文是关于LLMs在科学仿真中应用的第一篇研究。我们关注四个建模和仿真任务，每次评估LLMs的预期益处和限制，同时为模型构建者提供实际指导。第一个任务旨在解释概念模型的结构，以促进参与者在建模过程中的参与。第二个任务专注于汇总仿真输出，以便模型用户能够识别出优选场景。第三个任务旨在通过文本传达对仿真可视化的见解，以扩大仿真平台的可访问性。最后，最后一个任务引出了使用LLMs解释仿真错误和问题的可能性，以便模型开发者更有效地解决这些问题。",
    "tldr": "本文是关于大规模预训练语言模型在科学仿真中的研究。研究关注四个建模和仿真任务的LLMs的预期益处和限制，并提供实际指导。"
}