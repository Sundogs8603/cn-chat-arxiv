{
    "title": "Event Stream GPT: A Data Pre-processing and Modeling Library for Generative, Pre-trained Transformers over Continuous-time Sequences of Complex Events. (arXiv:2306.11547v2 [cs.LG] UPDATED)",
    "abstract": "Generative, pre-trained transformers (GPTs, a.k.a. \"Foundation Models\") have reshaped natural language processing (NLP) through their versatility in diverse downstream tasks. However, their potential extends far beyond NLP. This paper provides a software utility to help realize this potential, extending the applicability of GPTs to continuous-time sequences of complex events with internal dependencies, such as medical record datasets. Despite their potential, the adoption of foundation models in these domains has been hampered by the lack of suitable tools for model construction and evaluation. To bridge this gap, we introduce Event Stream GPT (ESGPT), an open-source library designed to streamline the end-to-end process for building GPTs for continuous-time event sequences. ESGPT allows users to (1) build flexible, foundation-model scale input datasets by specifying only a minimal configuration file, (2) leverage a Hugging Face compatible modeling API for GPTs over this modality that i",
    "link": "http://arxiv.org/abs/2306.11547",
    "context": "Title: Event Stream GPT: A Data Pre-processing and Modeling Library for Generative, Pre-trained Transformers over Continuous-time Sequences of Complex Events. (arXiv:2306.11547v2 [cs.LG] UPDATED)\nAbstract: Generative, pre-trained transformers (GPTs, a.k.a. \"Foundation Models\") have reshaped natural language processing (NLP) through their versatility in diverse downstream tasks. However, their potential extends far beyond NLP. This paper provides a software utility to help realize this potential, extending the applicability of GPTs to continuous-time sequences of complex events with internal dependencies, such as medical record datasets. Despite their potential, the adoption of foundation models in these domains has been hampered by the lack of suitable tools for model construction and evaluation. To bridge this gap, we introduce Event Stream GPT (ESGPT), an open-source library designed to streamline the end-to-end process for building GPTs for continuous-time event sequences. ESGPT allows users to (1) build flexible, foundation-model scale input datasets by specifying only a minimal configuration file, (2) leverage a Hugging Face compatible modeling API for GPTs over this modality that i",
    "path": "papers/23/06/2306.11547.json",
    "total_tokens": 850,
    "translated_title": "事件流GPT：用于连续时间序列的复杂事件的数据预处理和建模库",
    "translated_abstract": "生成式、预训练变压器（GPT）通过在多个下游任务中的多样性，改变了自然语言处理（NLP），但其潜力远不止于此。本文提供了一种软件工具，扩展了GPT的适用性，使其适用于内部依赖关系的连续时间序列的复杂事件，例如医疗记录数据集。",
    "tldr": "这篇论文介绍了Event Stream GPT (ESGPT)——一种用于构建连续时间事件序列的GPT模型的开源库。该库支持在医疗记录等具有内部依赖性的复杂事件上进行预测，具有高效易用且能达到最佳性能的特点。",
    "en_tdlr": "This paper introduces Event Stream GPT (ESGPT), an open-source library for building GPT models on continuous-time sequences of complex events with internal dependencies, such as medical records. ESGPT supports prediction on these complex events with high efficiency and streamlined ease of use, achieving state-of-the-art performance."
}