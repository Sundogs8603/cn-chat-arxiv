{
    "title": "Enhancing Virtual Assistant Intelligence: Precise Area Targeting for Instance-level User Intents beyond Metadata. (arXiv:2306.04163v1 [cs.HC])",
    "abstract": "Virtual assistants have been widely used by mobile phone users in recent years. Although their capabilities of processing user intents have been developed rapidly, virtual assistants in most platforms are only capable of handling pre-defined high-level tasks supported by extra manual efforts of developers. However, instance-level user intents containing more detailed objectives with complex practical situations, are yet rarely studied so far. In this paper, we explore virtual assistants capable of processing instance-level user intents based on pixels of application screens, without the requirements of extra extensions on the application side. We propose a novel cross-modal deep learning pipeline, which understands the input vocal or textual instance-level user intents, predicts the targeting operational area, and detects the absolute button area on screens without any metadata of applications. We conducted a user study with 10 participants to collect a testing dataset with instance-le",
    "link": "http://arxiv.org/abs/2306.04163",
    "context": "Title: Enhancing Virtual Assistant Intelligence: Precise Area Targeting for Instance-level User Intents beyond Metadata. (arXiv:2306.04163v1 [cs.HC])\nAbstract: Virtual assistants have been widely used by mobile phone users in recent years. Although their capabilities of processing user intents have been developed rapidly, virtual assistants in most platforms are only capable of handling pre-defined high-level tasks supported by extra manual efforts of developers. However, instance-level user intents containing more detailed objectives with complex practical situations, are yet rarely studied so far. In this paper, we explore virtual assistants capable of processing instance-level user intents based on pixels of application screens, without the requirements of extra extensions on the application side. We propose a novel cross-modal deep learning pipeline, which understands the input vocal or textual instance-level user intents, predicts the targeting operational area, and detects the absolute button area on screens without any metadata of applications. We conducted a user study with 10 participants to collect a testing dataset with instance-le",
    "path": "papers/23/06/2306.04163.json",
    "total_tokens": 896,
    "translated_title": "提升虚拟助手智能：针对元数据以外的实例级用户意图精准区域定位",
    "translated_abstract": "近年来，虚拟助手已广泛应用于移动电话用户中。虚拟助手在处理用户意图方面的能力已经迅速发展，但是，在大多数平台上，虚拟助手只能处理由开发人员额外手动努力支持的预定义高级任务。然而，包含更详细目标和复杂实际情况的实例级用户意图目前却鲜有研究。本文旨在探索能够基于应用程序屏幕像素处理实例级用户意图的虚拟助手，而不需要在应用程序端进行额外扩展。我们提出了一种新的跨模态深度学习方法，该方法可以理解语音或文本实例级用户意图，预测目标操作区域，并在没有应用程序元数据的情况下检测屏幕上的绝对按钮区域。我们进行了一项用户研究，收集了一个测试数据集，该数据集包含了10个参与者的实例级用户意图。",
    "tldr": "本文提出了一种能够处理应用程序屏幕像素级别虚拟助手，可以理解实例级用户意图并预测其目标操作区域，不需要应用程序元数据扩展。"
}