{
    "title": "Unified Language Representation for Question Answering over Text, Tables, and Images. (arXiv:2306.16762v1 [cs.CL])",
    "abstract": "When trying to answer complex questions, people often rely on multiple sources of information, such as visual, textual, and tabular data. Previous approaches to this problem have focused on designing input features or model structure in the multi-modal space, which is inflexible for cross-modal reasoning or data-efficient training. In this paper, we call for an alternative paradigm, which transforms the images and tables into unified language representations, so that we can simplify the task into a simpler textual QA problem that can be solved using three steps: retrieval, ranking, and generation, all within a language space. This idea takes advantage of the power of pre-trained language models and is implemented in a framework called Solar. Our experimental results show that Solar outperforms all existing methods by 10.6-32.3 pts on two datasets, MultimodalQA and MMCoQA, across ten different metrics. Additionally, Solar achieves the best performance on the WebQA leaderboard",
    "link": "http://arxiv.org/abs/2306.16762",
    "context": "Title: Unified Language Representation for Question Answering over Text, Tables, and Images. (arXiv:2306.16762v1 [cs.CL])\nAbstract: When trying to answer complex questions, people often rely on multiple sources of information, such as visual, textual, and tabular data. Previous approaches to this problem have focused on designing input features or model structure in the multi-modal space, which is inflexible for cross-modal reasoning or data-efficient training. In this paper, we call for an alternative paradigm, which transforms the images and tables into unified language representations, so that we can simplify the task into a simpler textual QA problem that can be solved using three steps: retrieval, ranking, and generation, all within a language space. This idea takes advantage of the power of pre-trained language models and is implemented in a framework called Solar. Our experimental results show that Solar outperforms all existing methods by 10.6-32.3 pts on two datasets, MultimodalQA and MMCoQA, across ten different metrics. Additionally, Solar achieves the best performance on the WebQA leaderboard",
    "path": "papers/23/06/2306.16762.json",
    "total_tokens": 902,
    "translated_title": "文本、表格和图像的统一语言表示在问答中的应用",
    "translated_abstract": "在试图回答复杂问题时，人们经常依赖于多种信息源，如视觉、文本和表格数据。之前的方法主要集中在设计多模态空间的输入特征或模型结构，这对于跨模态推理或数据高效训练来说是不灵活的。本文提出了一种新的范式，将图像和表格转化为统一的语言表示，从而将任务简化为一个更简单的文本问答问题，可以使用三个步骤解决：检索、排序和生成，所有这些都在语言空间内进行。这个想法利用了预训练语言模型的能力，并在一个名为Solar的框架中实现。我们的实验结果显示，Solar在两个数据集MultimodalQA和MMCoQA上相对于所有现有方法的指标提高了10.6-32.3个百分点。此外，Solar在WebQA榜单上取得了最佳表现。",
    "tldr": "本文提出了一种新的方法来回答复杂问题，即将图像和表格转化为统一的语言表示，通过检索、排序和生成三个步骤解决文本问答问题。实验证明，这种方法在两个数据集上表现优于所有现有方法，并在WebQA榜单上取得了最佳表现。"
}