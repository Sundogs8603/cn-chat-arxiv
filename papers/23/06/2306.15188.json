{
    "title": "One-class systems seamlessly fit in the forward-forward algorithm. (arXiv:2306.15188v1 [cs.LG])",
    "abstract": "The forward-forward algorithm presents a new method of training neural networks by updating weights during an inference, performing parameter updates for each layer individually. This immediately reduces memory requirements during training and may lead to many more benefits, like seamless online training. This method relies on a loss (\"goodness\") function that can be evaluated on the activations of each layer, of which can have a varied parameter size, depending on the hyperparamaterization of the network. In the seminal paper, a goodness function was proposed to fill this need; however, if placed in a one-class problem context, one need not pioneer a new loss because these functions can innately handle dynamic network sizes. In this paper, we investigate the performance of deep one-class objective functions when trained in a forward-forward fashion. The code is available at \\url{https://github.com/MichaelHopwood/ForwardForwardOneclass}.",
    "link": "http://arxiv.org/abs/2306.15188",
    "context": "Title: One-class systems seamlessly fit in the forward-forward algorithm. (arXiv:2306.15188v1 [cs.LG])\nAbstract: The forward-forward algorithm presents a new method of training neural networks by updating weights during an inference, performing parameter updates for each layer individually. This immediately reduces memory requirements during training and may lead to many more benefits, like seamless online training. This method relies on a loss (\"goodness\") function that can be evaluated on the activations of each layer, of which can have a varied parameter size, depending on the hyperparamaterization of the network. In the seminal paper, a goodness function was proposed to fill this need; however, if placed in a one-class problem context, one need not pioneer a new loss because these functions can innately handle dynamic network sizes. In this paper, we investigate the performance of deep one-class objective functions when trained in a forward-forward fashion. The code is available at \\url{https://github.com/MichaelHopwood/ForwardForwardOneclass}.",
    "path": "papers/23/06/2306.15188.json",
    "total_tokens": 830,
    "translated_title": "一类系统完美适用于前向算法",
    "translated_abstract": "前向算法提出了一种新的神经网络训练方法，通过在推理过程中更新权重，逐层进行参数更新。这一方法在训练过程中立即降低了内存需求，可能带来更多好处，比如无缝在线训练。这种方法依赖于一个损失（“好度”）函数，该函数可以在每个层的激活上进行评估，这些层的参数大小可以根据网络的超参数化而变化。在开创性论文中，提出了一个好度函数来满足这一需求；然而，如果将其置于一个单类问题的背景下，就无需开创新的损失函数，因为这些函数本身可以处理动态网络大小。在本文中，我们研究了在前向训练方式下训练深度单类目标函数的性能。代码可以在 \\url{https://github.com/MichaelHopwood/ForwardForwardOneclass} 找到。",
    "tldr": "本文研究了在前向训练方式下训练深度单类目标函数的性能，发现这些函数可以处理动态网络大小，为无缝在线训练带来了许多好处。",
    "en_tdlr": "This paper investigates the performance of training deep one-class objective functions in a forward-forward fashion and finds that these functions can handle dynamic network sizes, bringing many benefits to seamless online training."
}