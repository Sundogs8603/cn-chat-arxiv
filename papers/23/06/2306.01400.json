{
    "title": "Adaptive Attractors: A Defense Strategy against ML Adversarial Collusion Attacks. (arXiv:2306.01400v1 [cs.LG])",
    "abstract": "In the seller-buyer setting on machine learning models, the seller generates different copies based on the original model and distributes them to different buyers, such that adversarial samples generated on one buyer's copy would likely not work on other copies. A known approach achieves this using attractor-based rewriter which injects different attractors to different copies. This induces different adversarial regions in different copies, making adversarial samples generated on one copy not replicable on others. In this paper, we focus on a scenario where multiple malicious buyers collude to attack. We first give two formulations and conduct empirical studies to analyze effectiveness of collusion attack under different assumptions on the attacker's capabilities and properties of the attractors. We observe that existing attractor-based methods do not effectively mislead the colluders in the sense that adversarial samples found are influenced more by the original model instead of the a",
    "link": "http://arxiv.org/abs/2306.01400",
    "context": "Title: Adaptive Attractors: A Defense Strategy against ML Adversarial Collusion Attacks. (arXiv:2306.01400v1 [cs.LG])\nAbstract: In the seller-buyer setting on machine learning models, the seller generates different copies based on the original model and distributes them to different buyers, such that adversarial samples generated on one buyer's copy would likely not work on other copies. A known approach achieves this using attractor-based rewriter which injects different attractors to different copies. This induces different adversarial regions in different copies, making adversarial samples generated on one copy not replicable on others. In this paper, we focus on a scenario where multiple malicious buyers collude to attack. We first give two formulations and conduct empirical studies to analyze effectiveness of collusion attack under different assumptions on the attacker's capabilities and properties of the attractors. We observe that existing attractor-based methods do not effectively mislead the colluders in the sense that adversarial samples found are influenced more by the original model instead of the a",
    "path": "papers/23/06/2306.01400.json",
    "total_tokens": 898,
    "tldr": "本文提出了一种自适应吸引子防御策略，可用于防御机器学习模型受到多个恶意买家勾结攻击的情况。已有的基于吸引子的方法无法有效地误导勾结者。"
}