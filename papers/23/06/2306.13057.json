{
    "title": "SQ Lower Bounds for Learning Bounded Covariance GMMs. (arXiv:2306.13057v1 [cs.LG])",
    "abstract": "We study the complexity of learning mixtures of separated Gaussians with common unknown bounded covariance matrix. Specifically, we focus on learning Gaussian mixture models (GMMs) on $\\mathbb{R}^d$ of the form $P= \\sum_{i=1}^k w_i \\mathcal{N}(\\boldsymbol \\mu_i,\\mathbf \\Sigma_i)$, where $\\mathbf \\Sigma_i = \\mathbf \\Sigma \\preceq \\mathbf I$ and $\\min_{i \\neq j} \\| \\boldsymbol \\mu_i \\boldsymbol \\mu_j\\|_2 \\geq k^\\epsilon$ for some $\\epsilon>0$. Known learning algorithms for this family of GMMs have complexity $(dk)^{O(1/\\epsilon)}$. In this work, we prove that any Statistical Query (SQ) algorithm for this problem requires complexity at least $d^{\\Omega(1/\\epsilon)}$. In the special case where the separation is on the order of $k^{1/2}$, we additionally obtain fine-grained SQ lower bounds with the correct exponent. Our SQ lower bounds imply similar lower bounds for low-degree polynomial tests. Conceptually, our results provide evidence that known algorithms for this problem are nearly be",
    "link": "http://arxiv.org/abs/2306.13057",
    "context": "Title: SQ Lower Bounds for Learning Bounded Covariance GMMs. (arXiv:2306.13057v1 [cs.LG])\nAbstract: We study the complexity of learning mixtures of separated Gaussians with common unknown bounded covariance matrix. Specifically, we focus on learning Gaussian mixture models (GMMs) on $\\mathbb{R}^d$ of the form $P= \\sum_{i=1}^k w_i \\mathcal{N}(\\boldsymbol \\mu_i,\\mathbf \\Sigma_i)$, where $\\mathbf \\Sigma_i = \\mathbf \\Sigma \\preceq \\mathbf I$ and $\\min_{i \\neq j} \\| \\boldsymbol \\mu_i \\boldsymbol \\mu_j\\|_2 \\geq k^\\epsilon$ for some $\\epsilon>0$. Known learning algorithms for this family of GMMs have complexity $(dk)^{O(1/\\epsilon)}$. In this work, we prove that any Statistical Query (SQ) algorithm for this problem requires complexity at least $d^{\\Omega(1/\\epsilon)}$. In the special case where the separation is on the order of $k^{1/2}$, we additionally obtain fine-grained SQ lower bounds with the correct exponent. Our SQ lower bounds imply similar lower bounds for low-degree polynomial tests. Conceptually, our results provide evidence that known algorithms for this problem are nearly be",
    "path": "papers/23/06/2306.13057.json",
    "total_tokens": 1005,
    "translated_title": "用于学习有界协方差高斯混合模型的SQ下限",
    "translated_abstract": "本文研究了具有相同未知有界协方差矩阵的分离高斯混合模型的复杂性。 具体来说，我们关注形式为$P = \\sum_{i=1}^k w_i \\mathcal{N}(\\boldsymbol \\mu_i,\\mathbf \\Sigma_i)$的$\\mathbb{R}^d$上的高斯混合模型（GMMs），其中$\\mathbf \\Sigma_i = \\mathbf \\Sigma \\preceq \\mathbf I$且$\\min_{i \\neq j} \\| \\boldsymbol \\mu_i \\boldsymbol \\mu_j\\|_2 \\geq k^\\epsilon$对于某些$\\epsilon> 0$。已知的学习算法的复杂度为$(dk)^{O(1/\\epsilon)}$。在本文中，我们证明了任何用于此问题的统计查询（SQ）算法的复杂度至少需要$d^{\\Omega(1/\\epsilon)}$。当分离在$k^{1/2}$数量级上时，我们另外获得了具有正确指数的细粒度SQ下限。我们的SQ下限意味着低次多项式测试的类似下限。从概念上讲，我们的结果表明已知算法几乎是最优的。",
    "tldr": "本文研究了在有界协方差情况下学习分离高斯混合模型问题的复杂性，并证明了使用SQ算法的复杂度下限为$d^{\\Omega(1/\\epsilon)}$。",
    "en_tdlr": "This paper studies the complexity of learning separated Gaussian mixture models under bounded covariance and proves that any Statistical Query algorithm for this problem has a lower bound complexity of at least $d^{\\Omega(1/\\epsilon)}$."
}