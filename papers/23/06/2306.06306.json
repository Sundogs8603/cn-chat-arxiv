{
    "title": "DocumentCLIP: Linking Figures and Main Body Text in Reflowed Documents. (arXiv:2306.06306v1 [cs.CV])",
    "abstract": "Vision-language pretraining models have achieved great success in supporting multimedia applications by understanding the alignments between images and text. While existing vision-language pretraining models primarily focus on understanding single image associated with a single piece of text, they often ignore the alignment at the intra-document level, consisting of multiple sentences with multiple images. In this work, we propose DocumentCLIP, a salience-aware contrastive learning framework to enforce vision-language pretraining models to comprehend the interaction between images and longer text within documents. Our model is beneficial for the real-world multimodal document understanding like news article, magazines, product descriptions, which contain linguistically and visually richer content. To the best of our knowledge, we are the first to explore multimodal intra-document links by contrastive learning. In addition, we collect a large Wikipedia dataset for pretraining, which pro",
    "link": "http://arxiv.org/abs/2306.06306",
    "context": "Title: DocumentCLIP: Linking Figures and Main Body Text in Reflowed Documents. (arXiv:2306.06306v1 [cs.CV])\nAbstract: Vision-language pretraining models have achieved great success in supporting multimedia applications by understanding the alignments between images and text. While existing vision-language pretraining models primarily focus on understanding single image associated with a single piece of text, they often ignore the alignment at the intra-document level, consisting of multiple sentences with multiple images. In this work, we propose DocumentCLIP, a salience-aware contrastive learning framework to enforce vision-language pretraining models to comprehend the interaction between images and longer text within documents. Our model is beneficial for the real-world multimodal document understanding like news article, magazines, product descriptions, which contain linguistically and visually richer content. To the best of our knowledge, we are the first to explore multimodal intra-document links by contrastive learning. In addition, we collect a large Wikipedia dataset for pretraining, which pro",
    "path": "papers/23/06/2306.06306.json",
    "total_tokens": 880,
    "translated_title": "DocumentCLIP：链接换行文件中的图像和正文文本",
    "translated_abstract": "视觉语言预训练模型通过理解图像和文本之间的对齐而在支持多媒体应用方面取得了巨大成功。然而，现有的视觉语言预训练模型主要集中在理解单个图像与一段文本相关联，而往往忽略了多句话与多个图像组成的文档内部的对齐。在本文中，我们提出了一种称为DocumentCLIP的显著性感知对比学习框架，以强制视觉语言预训练模型理解文档内长文本和图像之间的交互作用。我们的模型有助于对新闻文章、杂志、产品描述等具有更丰富语言和视觉内容的真实世界多模态文档理解。据我们所知，我们是第一个通过对比学习探索多模态文档内部链接的人。此外，我们收集了一个大型的Wikipedia数据集进行预培训，为训练提供了多样化的来源。",
    "tldr": "DocumentCLIP是一种显著性感知对比学习框架，用于理解文档内长文本和图像之间的交互作用。我们是第一个在多模态文档内部链接方面进行对比学习的人。",
    "en_tdlr": "DocumentCLIP is a salience-aware contrastive learning framework that understands the interaction between long text and images within documents. We are the first to explore intra-document links for multimodal documents using contrastive learning."
}