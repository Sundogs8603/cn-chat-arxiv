{
    "title": "A new approach to generalisation error of machine learning algorithms: Estimates and convergence. (arXiv:2306.13784v1 [stat.ML])",
    "abstract": "In this work we consider a model problem of deep neural learning, namely the learning of a given function when it is assumed that we have access to its point values on a finite set of points. The deep neural network interpolant is the the resulting approximation of f, which is obtained by a typical machine learning algorithm involving a given DNN architecture and an optimisation step, which is assumed to be solved exactly. These are among the simplest regression algorithms based on neural networks. In this work we introduce a new approach to the estimation of the (generalisation) error and to convergence. Our results include (i) estimates of the error without any structural assumption on the neural networks and under mild regularity assumptions on the learning function f (ii) convergence of the approximations to the target function f by only requiring that the neural network spaces have appropriate approximation capability.",
    "link": "http://arxiv.org/abs/2306.13784",
    "context": "Title: A new approach to generalisation error of machine learning algorithms: Estimates and convergence. (arXiv:2306.13784v1 [stat.ML])\nAbstract: In this work we consider a model problem of deep neural learning, namely the learning of a given function when it is assumed that we have access to its point values on a finite set of points. The deep neural network interpolant is the the resulting approximation of f, which is obtained by a typical machine learning algorithm involving a given DNN architecture and an optimisation step, which is assumed to be solved exactly. These are among the simplest regression algorithms based on neural networks. In this work we introduce a new approach to the estimation of the (generalisation) error and to convergence. Our results include (i) estimates of the error without any structural assumption on the neural networks and under mild regularity assumptions on the learning function f (ii) convergence of the approximations to the target function f by only requiring that the neural network spaces have appropriate approximation capability.",
    "path": "papers/23/06/2306.13784.json",
    "total_tokens": 842,
    "translated_title": "机器学习算法泛化误差的新方法：估计和收敛性",
    "translated_abstract": "本文考虑了深度神经学习的一个模型问题，即在有限的点集上已知一个函数的点值，学习该函数。通过典型的机器学习算法，包括给定的DNN体系结构和一个被认为可以完全解决的优化步骤，得到了深度神经网络插值器，该插值器是f的近似。在本文中，我们引入了一种估计（泛化）误差和收敛性的新方法。我们的结果包括：（i）在神经网络没有任何结构性假设的情况下，并在学习函数f的温和正则性假设下估计误差，（ii）只要神经网络空间具有适当的逼近能力，就可以将近似转化为目标函数f。",
    "tldr": "本文提出了一种新的机器学习算法泛化误差的估计方法和收敛性分析，可以在不需要神经网络的任何假设下对误差进行估计，并只要求神经网络具有适当的逼近能力就可以将近似转化为目标函数f。",
    "en_tdlr": "This paper proposes a new method for estimating generalization error and analyzing convergence of machine learning algorithms, which can estimate errors without any structural assumptions on neural networks and only require appropriate approximation capability of neural networks for converting approximation to the target function f."
}