{
    "title": "Knowledge-Infused Self Attention Transformers. (arXiv:2306.13501v1 [cs.CL])",
    "abstract": "Transformer-based language models have achieved impressive success in various natural language processing tasks due to their ability to capture complex dependencies and contextual information using self-attention mechanisms. However, they are not without limitations. These limitations include hallucinations, where they produce incorrect outputs with high confidence, and alignment issues, where they generate unhelpful and unsafe outputs for human users. These limitations stem from the absence of implicit and missing context in the data alone. To address this, researchers have explored augmenting these models with external knowledge from knowledge graphs to provide the necessary additional context. However, the ad-hoc nature of existing methods makes it difficult to properly analyze the effects of knowledge infusion on the many moving parts or components of a transformer. This paper introduces a systematic method for infusing knowledge into different components of a transformer-based mod",
    "link": "http://arxiv.org/abs/2306.13501",
    "context": "Title: Knowledge-Infused Self Attention Transformers. (arXiv:2306.13501v1 [cs.CL])\nAbstract: Transformer-based language models have achieved impressive success in various natural language processing tasks due to their ability to capture complex dependencies and contextual information using self-attention mechanisms. However, they are not without limitations. These limitations include hallucinations, where they produce incorrect outputs with high confidence, and alignment issues, where they generate unhelpful and unsafe outputs for human users. These limitations stem from the absence of implicit and missing context in the data alone. To address this, researchers have explored augmenting these models with external knowledge from knowledge graphs to provide the necessary additional context. However, the ad-hoc nature of existing methods makes it difficult to properly analyze the effects of knowledge infusion on the many moving parts or components of a transformer. This paper introduces a systematic method for infusing knowledge into different components of a transformer-based mod",
    "path": "papers/23/06/2306.13501.json",
    "total_tokens": 1018,
    "translated_title": "知识注入自注意力变压器",
    "translated_abstract": "基于变换器的语言模型以其使用自我关注机制捕捉复杂依赖关系和上下文信息的能力，在各种自然语言处理任务中取得了令人瞩目的成功。然而，它们并非没有局限性。这些限制包括幻觉，即它们产生高置信度的错误输出，以及对人类用户生成无用和不安全输出的对齐问题。这些限制源于数据中隐含和缺失上下文的缺乏。为了解决这个问题，研究人员探索了利用来自知识图谱的外部知识来提供必要的附加上下文来增强这些模型的方法。然而，现有方法的临时性质使得在分析知识注入对变压器的许多部分或组件产生的影响以及组件之间的相互影响方面困难重重。本文介绍了一种将知识注入到变压器模型的不同组件中的系统方法。该提出的方法允许更彻底地分析知识注入对每个组件以及组件之间相互作用的影响。实验结果表明，所提出的方法显着提高了变压器模型在各种自然语言处理任务中的性能。",
    "tldr": "本文介绍了一种将知识注入到变压器模型不同组件的系统方法，可以更彻底地分析知识注入对每个组件和组件之间相互作用的影响，并使变压器模型在各种自然语言处理任务中性能得到了显著提高。",
    "en_tdlr": "This paper introduces a systematic method for infusing knowledge into different components of a transformer-based model, which allows for a more thorough analysis of the effects of knowledge infusion on each component and between components, resulting in significant performance improvement on various natural language processing tasks."
}