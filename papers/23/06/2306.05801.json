{
    "title": "Strategies to exploit XAI to improve classification systems. (arXiv:2306.05801v1 [cs.AI])",
    "abstract": "Explainable Artificial Intelligence (XAI) aims to provide insights into the decision-making process of AI models, allowing users to understand their results beyond their decisions. A significant goal of XAI is to improve the performance of AI models by providing explanations for their decision-making processes. However, most XAI literature focuses on how to explain an AI system, while less attention has been given to how XAI methods can be exploited to improve an AI system. In this work, a set of well-known XAI methods typically used with Machine Learning (ML) classification tasks are investigated to verify if they can be exploited, not just to provide explanations but also to improve the performance of the model itself. To this aim, two strategies to use the explanation to improve a classification system are reported and empirically evaluated on three datasets: Fashion-MNIST, CIFAR10, and STL10. Results suggest that explanations built by Integrated Gradients highlight input features t",
    "link": "http://arxiv.org/abs/2306.05801",
    "context": "Title: Strategies to exploit XAI to improve classification systems. (arXiv:2306.05801v1 [cs.AI])\nAbstract: Explainable Artificial Intelligence (XAI) aims to provide insights into the decision-making process of AI models, allowing users to understand their results beyond their decisions. A significant goal of XAI is to improve the performance of AI models by providing explanations for their decision-making processes. However, most XAI literature focuses on how to explain an AI system, while less attention has been given to how XAI methods can be exploited to improve an AI system. In this work, a set of well-known XAI methods typically used with Machine Learning (ML) classification tasks are investigated to verify if they can be exploited, not just to provide explanations but also to improve the performance of the model itself. To this aim, two strategies to use the explanation to improve a classification system are reported and empirically evaluated on three datasets: Fashion-MNIST, CIFAR10, and STL10. Results suggest that explanations built by Integrated Gradients highlight input features t",
    "path": "papers/23/06/2306.05801.json",
    "total_tokens": 867,
    "translated_title": "利用XAI改进分类系统的策略",
    "translated_abstract": "可解释的人工智能（XAI）旨在向用户呈现AI模型决策过程的详细信息，使用户可以了解AI模型的结果并超越其决策。 XAI的一个重要目标是通过提供决策过程的解释来改进AI模型的性能。然而，大多数XAI文献都集中在如何解释AI系统，而较少关注如何利用XAI方法改进AI系统。本研究调查了一组通常用于机器学习（ML）分类任务的知名XAI方法，以验证它们是否可以被利用，不仅为模型提供解释，而且还可以改进其性能。为此，在三个数据集Fashion-MNIST、CIFAR10和STL10上报告了两种使用解释来改进分类系统的策略，并进行了实证评估。结果表明，通过综合渐变创建的解释突出了输入特征。",
    "tldr": "本文研究了利用XAI方法改进分类系统的策略，探究了两种使用解释来提高模型性能的方法，并在三个数据集上进行实证评估，结果表明，Integrated Gradients方法创建的解释可以突出输入特征从而提高模型性能。",
    "en_tdlr": "This paper investigates strategies for exploiting XAI methods to improve classification systems by exploring two methods of using explanations to enhance model performance and empirically evaluating them on three datasets. Results suggest that explanations built by Integrated Gradients can improve model performance by highlighting important input features."
}