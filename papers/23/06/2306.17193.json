{
    "title": "Limits of Machine Learning for Automatic Vulnerability Detection. (arXiv:2306.17193v1 [cs.CR])",
    "abstract": "Recent results of machine learning for automatic vulnerability detection have been very promising indeed: Given only the source code of a function $f$, models trained by machine learning techniques can decide if $f$ contains a security flaw with up to 70% accuracy.  But how do we know that these results are general and not specific to the datasets? To study this question, researchers proposed to amplify the testing set by injecting semantic preserving changes and found that the model's accuracy significantly drops. In other words, the model uses some unrelated features during classification. In order to increase the robustness of the model, researchers proposed to train on amplified training data, and indeed model accuracy increased to previous levels.  In this paper, we replicate and continue this investigation, and provide an actionable model benchmarking methodology to help researchers better evaluate advances in machine learning for vulnerability detection. Specifically, we propose",
    "link": "http://arxiv.org/abs/2306.17193",
    "context": "Title: Limits of Machine Learning for Automatic Vulnerability Detection. (arXiv:2306.17193v1 [cs.CR])\nAbstract: Recent results of machine learning for automatic vulnerability detection have been very promising indeed: Given only the source code of a function $f$, models trained by machine learning techniques can decide if $f$ contains a security flaw with up to 70% accuracy.  But how do we know that these results are general and not specific to the datasets? To study this question, researchers proposed to amplify the testing set by injecting semantic preserving changes and found that the model's accuracy significantly drops. In other words, the model uses some unrelated features during classification. In order to increase the robustness of the model, researchers proposed to train on amplified training data, and indeed model accuracy increased to previous levels.  In this paper, we replicate and continue this investigation, and provide an actionable model benchmarking methodology to help researchers better evaluate advances in machine learning for vulnerability detection. Specifically, we propose",
    "path": "papers/23/06/2306.17193.json",
    "total_tokens": 1062,
    "translated_title": "机器学习在自动漏洞检测中的局限性",
    "translated_abstract": "机器学习在自动漏洞检测方面的最新结果非常有希望：仅给定函数$f$的源代码，经过机器学习训练的模型可以以高达70%的准确率判断$f$是否存在安全漏洞。但我们如何知道这些结果是否普适，而不仅限于特定数据集？为了研究这个问题，研究者们提出了通过注入语义保持的更改来扩大测试集，并发现模型的准确率显著下降。换句话说，该模型在分类时使用了一些无关的特征。为了增加模型的鲁棒性，研究者们提出在扩展的训练数据上进行训练，结果模型的准确率恢复到之前的水平。本文复制并继续了这项研究，并提供了可行的模型评估方法，以帮助研究者更好地评估机器学习在漏洞检测方面的进展。具体而言，我们提出了一种可行的模型基准测试方法。",
    "tldr": "机器学习在自动漏洞检测方面取得了很大的进展，但其结果是否普适仍存在疑问。本研究通过注入语义保持的更改来扩大测试集，并发现模型准确率显著下降，这表明模型在分类时使用了一些无关的特征。通过在扩展的训练数据上进行训练，模型的准确率恢复到之前的水平。本文提出了一种可行的模型基准测试方法，以帮助研究者更好地评估机器学习在漏洞检测方面的进展。"
}