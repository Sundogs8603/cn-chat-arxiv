{
    "title": "Chain-of-Thought Prompt Distillation for Multimodal Named Entity and Multimodal Relation Extraction. (arXiv:2306.14122v2 [cs.CL] UPDATED)",
    "abstract": "Multimodal Named Entity Recognition (MNER) and Multimodal Relation Extraction (MRE) necessitate the fundamental reasoning capacity for intricate linguistic and multimodal comprehension. In this study, we explore distilling the reasoning ability of large language models (LLMs) into a more compact student model by generating a \\textit{chain of thought} (CoT) -- a sequence of intermediate reasoning steps. Specifically, we commence by exemplifying the elicitation of such reasoning ability from LLMs through CoT prompts covering multi-grain (noun, sentence, multimodality) and data-augmentation (style, entity, image) dimensions. Subsequently, we present a novel conditional prompt distillation method to assimilate the commonsense reasoning ability from LLMs, thereby enhancing the utility of the student model in addressing text-only inputs without the requisite addition of image and CoT knowledge. Extensive experiments reveal that our approach attains state-of-the-art accuracy and manifests a p",
    "link": "http://arxiv.org/abs/2306.14122",
    "context": "Title: Chain-of-Thought Prompt Distillation for Multimodal Named Entity and Multimodal Relation Extraction. (arXiv:2306.14122v2 [cs.CL] UPDATED)\nAbstract: Multimodal Named Entity Recognition (MNER) and Multimodal Relation Extraction (MRE) necessitate the fundamental reasoning capacity for intricate linguistic and multimodal comprehension. In this study, we explore distilling the reasoning ability of large language models (LLMs) into a more compact student model by generating a \\textit{chain of thought} (CoT) -- a sequence of intermediate reasoning steps. Specifically, we commence by exemplifying the elicitation of such reasoning ability from LLMs through CoT prompts covering multi-grain (noun, sentence, multimodality) and data-augmentation (style, entity, image) dimensions. Subsequently, we present a novel conditional prompt distillation method to assimilate the commonsense reasoning ability from LLMs, thereby enhancing the utility of the student model in addressing text-only inputs without the requisite addition of image and CoT knowledge. Extensive experiments reveal that our approach attains state-of-the-art accuracy and manifests a p",
    "path": "papers/23/06/2306.14122.json",
    "total_tokens": 909,
    "translated_title": "链式思维提示提取多模态命名实体和多模态关系抽取技术",
    "translated_abstract": "多模态命名实体识别（MNER）和多模态关系抽取（MRE）需要处理复杂语言和多模态理解的基本推理能力。本研究探索了将大型语言模型（LLMs）的推理能力提炼为更紧凑的学生模型的方法，通过生成一系列中间推理步骤来实现。具体而言，我们首先通过涵盖多粒度（名词、句子、多模态）和数据增强（样式、实体、图像）维度的链式思维提示，展示了从LLMs中引导此类推理能力的示例。随后，我们提出了一种新的条件提示提取方法，以吸收LLMs中的常识推理能力，从而增强学生模型在处理仅文本输入时的实用性，而无需添加图像和链式思维知识。大量实验证明，我们的方法达到了最先进的准确性，并表现出更好的性能。",
    "tldr": "本研究提出了一种链式思维提示提取方法，将大型语言模型的推理能力转化为更紧凑的学生模型，从而提高了多模态命名实体识别和多模态关系抽取的效果。",
    "en_tdlr": "This study proposes a chain-of-thought prompt distillation method to distill the reasoning ability of large language models (LLMs) into a more compact student model, thereby improving the performance of multimodal named entity recognition and multimodal relation extraction."
}