{
    "title": "Reward-Free Curricula for Training Robust World Models. (arXiv:2306.09205v2 [cs.LG] UPDATED)",
    "abstract": "There has been a recent surge of interest in developing generally-capable agents that can adapt to new tasks without additional training in the environment. Learning world models from reward-free exploration is a promising approach, and enables policies to be trained using imagined experience for new tasks. However, achieving a general agent requires robustness across different environments. In this work, we address the novel problem of generating curricula in the reward-free setting to train robust world models. We consider robustness in terms of minimax regret over all environment instantiations and show that the minimax regret can be connected to minimising the maximum error in the world model across environment instances. This result informs our algorithm, WAKER: Weighted Acquisition of Knowledge across Environments for Robustness. WAKER selects environments for data collection based on the estimated error of the world model for each environment. Our experiments demonstrate that WA",
    "link": "http://arxiv.org/abs/2306.09205",
    "context": "Title: Reward-Free Curricula for Training Robust World Models. (arXiv:2306.09205v2 [cs.LG] UPDATED)\nAbstract: There has been a recent surge of interest in developing generally-capable agents that can adapt to new tasks without additional training in the environment. Learning world models from reward-free exploration is a promising approach, and enables policies to be trained using imagined experience for new tasks. However, achieving a general agent requires robustness across different environments. In this work, we address the novel problem of generating curricula in the reward-free setting to train robust world models. We consider robustness in terms of minimax regret over all environment instantiations and show that the minimax regret can be connected to minimising the maximum error in the world model across environment instances. This result informs our algorithm, WAKER: Weighted Acquisition of Knowledge across Environments for Robustness. WAKER selects environments for data collection based on the estimated error of the world model for each environment. Our experiments demonstrate that WA",
    "path": "papers/23/06/2306.09205.json",
    "total_tokens": 935,
    "translated_title": "无奖励训练鲁棒世界模型的无奖励课程",
    "translated_abstract": "最近人们对开发能够适应新任务而无需在环境中进行额外训练的通用能力代理产生了浓厚的兴趣。从无奖励探索中学习世界模型是一种有前景的方法，它使得可以使用想象的经验来训练新任务的策略。然而，实现一个通用的代理需要在不同环境下具有鲁棒性。在这项工作中，我们解决了在无奖励设置中生成课程以训练鲁棒性世界模型的新问题。我们从极小极大后悔的角度考虑鲁棒性，并展示了极小极大后悔与在不同环境实例中世界模型的最大误差最小化之间的联系。这个结果为我们的算法WAKER（基于鲁棒性的环境中的知识加权获取）提供了指导。WAKER根据每个环境中世界模型的估计误差选择数据收集的环境。我们的实验表明WAKER能够提升鲁棒性模型的质量。",
    "tldr": "通过无奖励课程进行训练可以实现鲁棒世界模型。我们提出了WAKER算法，通过根据世界模型在不同环境中的估计误差选择数据收集环境，从而提升模型的鲁棒性。",
    "en_tdlr": "Training robust world models without rewards is possible through reward-free curricula. We introduce the WAKER algorithm, which selects data collection environments based on the estimated error of the world model in different environments, thereby improving the model's robustness."
}