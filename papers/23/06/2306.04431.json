{
    "title": "Faithful Knowledge Distillation. (arXiv:2306.04431v1 [cs.LG])",
    "abstract": "Knowledge distillation (KD) has received much attention due to its success in compressing networks to allow for their deployment in resource-constrained systems. While the problem of adversarial robustness has been studied before in the KD setting, previous works overlook what we term the relative calibration of the student network with respect to its teacher in terms of soft confidences. In particular, we focus on two crucial questions with regard to a teacher-student pair: (i) do the teacher and student disagree at points close to correctly classified dataset examples, and (ii) is the distilled student as confident as the teacher around dataset examples? These are critical questions when considering the deployment of a smaller student network trained from a robust teacher within a safety-critical setting. To address these questions, we introduce a faithful imitation framework to discuss the relative calibration of confidences, as well as provide empirical and certified methods to eva",
    "link": "http://arxiv.org/abs/2306.04431",
    "context": "Title: Faithful Knowledge Distillation. (arXiv:2306.04431v1 [cs.LG])\nAbstract: Knowledge distillation (KD) has received much attention due to its success in compressing networks to allow for their deployment in resource-constrained systems. While the problem of adversarial robustness has been studied before in the KD setting, previous works overlook what we term the relative calibration of the student network with respect to its teacher in terms of soft confidences. In particular, we focus on two crucial questions with regard to a teacher-student pair: (i) do the teacher and student disagree at points close to correctly classified dataset examples, and (ii) is the distilled student as confident as the teacher around dataset examples? These are critical questions when considering the deployment of a smaller student network trained from a robust teacher within a safety-critical setting. To address these questions, we introduce a faithful imitation framework to discuss the relative calibration of confidences, as well as provide empirical and certified methods to eva",
    "path": "papers/23/06/2306.04431.json",
    "total_tokens": 858,
    "translated_title": "忠实知识蒸馏",
    "translated_abstract": "知识蒸馏是一种压缩神经网络使其能够在资源受限的系统中部署的成功方法，但过去的研究忽略了教师与学生之间在软置信度方面的相对校准问题。本文聚焦于一个教师-学生对中两个关键问题：（i）教师和学生是否在接近正确分类的数据样本时存在分歧，（ii）在数据样本周围，经过蒸馏的学生是否像教师一样自信。这些都是在安全关键环境中考虑从鲁棒教师中训练较小学生网络的部署时非常关键的问题。为了解决这些问题，我们引入了一个忠实的模仿框架来讨论置信度的相对校准，并提供实证和认证方法来评估学生的训练。",
    "tldr": "本文研究了知识蒸馏中教师和学生之间的相对校准问题，提出了一个忠实的模仿框架来解决学生置信度和软标签的问题，并提供了一种实证和认证的方法来评估学生模型的鲁棒性。"
}