{
    "title": "PriSampler: Mitigating Property Inference of Diffusion Models. (arXiv:2306.05208v1 [cs.CR])",
    "abstract": "Diffusion models have been remarkably successful in data synthesis. Such successes have also driven diffusion models to apply to sensitive data, such as human face data, but this might bring about severe privacy concerns. In this work, we systematically present the first privacy study about property inference attacks against diffusion models, in which adversaries aim to extract sensitive global properties of the training set from a diffusion model, such as the proportion of the training data for certain sensitive properties. Specifically, we consider the most practical attack scenario: adversaries are only allowed to obtain synthetic data. Under this realistic scenario, we evaluate the property inference attacks on different types of samplers and diffusion models. A broad range of evaluations shows that various diffusion models and their samplers are all vulnerable to property inference attacks. Furthermore, one case study on off-the-shelf pre-trained diffusion models also demonstrates",
    "link": "http://arxiv.org/abs/2306.05208",
    "context": "Title: PriSampler: Mitigating Property Inference of Diffusion Models. (arXiv:2306.05208v1 [cs.CR])\nAbstract: Diffusion models have been remarkably successful in data synthesis. Such successes have also driven diffusion models to apply to sensitive data, such as human face data, but this might bring about severe privacy concerns. In this work, we systematically present the first privacy study about property inference attacks against diffusion models, in which adversaries aim to extract sensitive global properties of the training set from a diffusion model, such as the proportion of the training data for certain sensitive properties. Specifically, we consider the most practical attack scenario: adversaries are only allowed to obtain synthetic data. Under this realistic scenario, we evaluate the property inference attacks on different types of samplers and diffusion models. A broad range of evaluations shows that various diffusion models and their samplers are all vulnerable to property inference attacks. Furthermore, one case study on off-the-shelf pre-trained diffusion models also demonstrates",
    "path": "papers/23/06/2306.05208.json",
    "total_tokens": 885,
    "translated_title": "PriSampler: 缓解扩散模型的属性推断问题",
    "translated_abstract": "扩散模型在数据合成方面取得了巨大成功。这些成功也促使扩散模型应用于敏感数据，例如人脸数据，但这可能带来严重的隐私问题。本文系统地介绍了针对扩散模型的属性推断攻击的第一项隐私研究，其中攻击者旨在从扩散模型中提取训练集的敏感全局属性，例如某些敏感属性的训练数据比例。具体而言，我们考虑了最实用的攻击场景：攻击者只能获得合成数据。在现实场景下，我们对不同类型的取样器和扩散模型进行了属性推断攻击的评估。广泛的评估范围表明，各种扩散模型及其取样器都容易受到属性推断攻击的影响。此外，对现成的预训练扩散模型进行一项案例研究也展示了攻击的实际效果。",
    "tldr": "本文是第一项针对扩散模型属性推断攻击的隐私研究，攻击者将从模型中提取训练集的敏感全局属性，结果表明各种扩散模型及其取样器容易受到攻击的影响。",
    "en_tdlr": "This is the first privacy study on property inference attacks against diffusion models. Attackers aim to extract sensitive global properties of the training set from a diffusion model, and results show that various diffusion models and their samplers are vulnerable to attacks."
}