{
    "title": "A Static Evaluation of Code Completion by Large Language Models. (arXiv:2306.03203v1 [cs.CL])",
    "abstract": "Large language models trained on code have shown great potential to increase productivity of software developers. Several execution-based benchmarks have been proposed to evaluate functional correctness of model-generated code on simple programming problems. Nevertheless, it is expensive to perform the same evaluation on complex real-world projects considering the execution cost. On the contrary, static analysis tools such as linters, which can detect errors without running the program, haven't been well explored for evaluating code generation models. In this work, we propose a static evaluation framework to quantify static errors in Python code completions, by leveraging Abstract Syntax Trees. Compared with execution-based evaluation, our method is not only more efficient, but also applicable to code in the wild. For experiments, we collect code context from open source repos to generate one million function bodies using public models. Our static analysis reveals that Undefined Name a",
    "link": "http://arxiv.org/abs/2306.03203",
    "context": "Title: A Static Evaluation of Code Completion by Large Language Models. (arXiv:2306.03203v1 [cs.CL])\nAbstract: Large language models trained on code have shown great potential to increase productivity of software developers. Several execution-based benchmarks have been proposed to evaluate functional correctness of model-generated code on simple programming problems. Nevertheless, it is expensive to perform the same evaluation on complex real-world projects considering the execution cost. On the contrary, static analysis tools such as linters, which can detect errors without running the program, haven't been well explored for evaluating code generation models. In this work, we propose a static evaluation framework to quantify static errors in Python code completions, by leveraging Abstract Syntax Trees. Compared with execution-based evaluation, our method is not only more efficient, but also applicable to code in the wild. For experiments, we collect code context from open source repos to generate one million function bodies using public models. Our static analysis reveals that Undefined Name a",
    "path": "papers/23/06/2306.03203.json",
    "total_tokens": 895,
    "translated_title": "大型语言模型的代码补全的静态评价。",
    "translated_abstract": "针对利用代码训练的大型语言模型提高软件开发人员生产力的研究，已经提出了数个基于执行的评估标准，用来评估模型生成的代码在简单编程问题上的功能正确性。然而，考虑到执行成本的问题，在复杂的实际项目上执行同样的评估是非常昂贵的。与此相反的是，可以静态地检测错误而无需运行程序的语法检查工具（如本文中使用的“linter”），尚未被广泛用于评估代码生成模型。本文提出了一种基于抽象语法树的静态评价框架，以量化Python代码补全中的静态错误。与基于执行的评估相比，我们的方法不仅更高效，而且适用于实际代码。在实验中，我们从开源代码库中收集代码上下文，使用公共模型生成了一百万个函数体。我们的静态分析揭示了未定义的名称（Undefined Name）这个常见错误。",
    "tldr": "本文提出了一种基于抽象语法树的静态评价框架来评估大型语言模型的Python代码补全质量，相比于基于执行的评估方法更加高效，适用于实际代码。研究揭示了未定义名称是一个常见错误。"
}