{
    "title": "POP: Prompt Of Prompts for Continual Learning. (arXiv:2306.08200v1 [cs.CV])",
    "abstract": "Continual learning (CL) has attracted increasing attention in the recent past. It aims to mimic the human ability to learn new concepts without catastrophic forgetting. While existing CL methods accomplish this to some extent, they are still prone to semantic drift of the learned feature space. Foundation models, which are endowed with a robust feature representation, learned from very large datasets, provide an interesting substrate for the solution of the CL problem. Recent work has also shown that they can be adapted to specific tasks by prompt tuning techniques that leave the generality of the representation mostly unscathed. An open question is, however, how to learn both prompts that are task specific and prompts that are global, i.e. capture cross-task information. In this work, we propose the Prompt Of Prompts (POP) model, which addresses this goal by progressively learning a group of task-specified prompts and a group of global prompts, denoted as POP, to integrate information",
    "link": "http://arxiv.org/abs/2306.08200",
    "context": "Title: POP: Prompt Of Prompts for Continual Learning. (arXiv:2306.08200v1 [cs.CV])\nAbstract: Continual learning (CL) has attracted increasing attention in the recent past. It aims to mimic the human ability to learn new concepts without catastrophic forgetting. While existing CL methods accomplish this to some extent, they are still prone to semantic drift of the learned feature space. Foundation models, which are endowed with a robust feature representation, learned from very large datasets, provide an interesting substrate for the solution of the CL problem. Recent work has also shown that they can be adapted to specific tasks by prompt tuning techniques that leave the generality of the representation mostly unscathed. An open question is, however, how to learn both prompts that are task specific and prompts that are global, i.e. capture cross-task information. In this work, we propose the Prompt Of Prompts (POP) model, which addresses this goal by progressively learning a group of task-specified prompts and a group of global prompts, denoted as POP, to integrate information",
    "path": "papers/23/06/2306.08200.json",
    "total_tokens": 870,
    "translated_title": "POP: Prompt Of Prompts for Continual Learning",
    "translated_abstract": "连续学习（CL）近年来越来越受到关注。它旨在模仿人类学习新概念的能力而不会发生灾难性遗忘。虽然现有的CL方法在某种程度上已经实现了这一目标，但它们仍然容易受到学习到的特征空间的语义漂移的影响。拥有健壮特征表示的基础模型，通过从非常大的数据集中获得的学习提供了解决CL问题的有趣基础。最近的工作还表明，它们可以通过提示调整技术适应特定任务，而大部分的表示的一般性基本不受影响。然而，一个待解决的问题是如何学习既为任务特定的提示又为全局提示的模型，即捕捉跨任务的信息。在这项工作中，我们提出了Prompt Of Prompts（POP）模型，通过逐步学习一组任务指定的提示和一组全局提示（即POP）来实现该目标，以集成信息。",
    "tldr": "本研究提出了一种称为POP的模型，通过逐步学习一组任务指定的提示和一组全局提示，以解决连续学习中的信息集成问题。",
    "en_tdlr": "In this study, we propose a model called POP to address the challenge of integrating information in continual learning by progressively learning a group of task-specified prompts and a group of global prompts."
}