{
    "title": "The Inductive Bias of Flatness Regularization for Deep Matrix Factorization. (arXiv:2306.13239v1 [cs.LG])",
    "abstract": "Recent works on over-parameterized neural networks have shown that the stochasticity in optimizers has the implicit regularization effect of minimizing the sharpness of the loss function (in particular, the trace of its Hessian) over the family zero-loss solutions. More explicit forms of flatness regularization also empirically improve the generalization performance. However, it remains unclear why and when flatness regularization leads to better generalization. This work takes the first step toward understanding the inductive bias of the minimum trace of the Hessian solutions in an important setting: learning deep linear networks from linear measurements, also known as \\emph{deep matrix factorization}. We show that for all depth greater than one, with the standard Restricted Isometry Property (RIP) on the measurements, minimizing the trace of Hessian is approximately equivalent to minimizing the Schatten 1-norm of the corresponding end-to-end matrix parameters (i.e., the product of al",
    "link": "http://arxiv.org/abs/2306.13239",
    "context": "Title: The Inductive Bias of Flatness Regularization for Deep Matrix Factorization. (arXiv:2306.13239v1 [cs.LG])\nAbstract: Recent works on over-parameterized neural networks have shown that the stochasticity in optimizers has the implicit regularization effect of minimizing the sharpness of the loss function (in particular, the trace of its Hessian) over the family zero-loss solutions. More explicit forms of flatness regularization also empirically improve the generalization performance. However, it remains unclear why and when flatness regularization leads to better generalization. This work takes the first step toward understanding the inductive bias of the minimum trace of the Hessian solutions in an important setting: learning deep linear networks from linear measurements, also known as \\emph{deep matrix factorization}. We show that for all depth greater than one, with the standard Restricted Isometry Property (RIP) on the measurements, minimizing the trace of Hessian is approximately equivalent to minimizing the Schatten 1-norm of the corresponding end-to-end matrix parameters (i.e., the product of al",
    "path": "papers/23/06/2306.13239.json",
    "total_tokens": 968,
    "translated_title": "深度矩阵分解中平坦性正则化的归纳偏差",
    "translated_abstract": "最近关于过参数神经网络的研究表明，优化器中的随机性具有隐式的正则化效应，可以通过减小损失函数的尖锐度（特别是其海森矩阵的迹）来最小化零损失解族。更明确的平坦性正则化形式也在经验上提高了泛化性能。然而，平坦性正则化何时和为什么会导致更好的泛化性能仍不清楚。本文以重要的学习深度线性网络的设置为例，即从线性测量中学习深度矩阵分解，为理解最小化海森矩阵迹解的归纳偏差迈出第一步。我们发现，在满足测量标准受限等距性质（RIP）的所有深度大于一的情况下，最小化海森矩阵迹近似等同于最小化相应端对端矩阵参数的Schatten 1-范数（即所有奇异值之和）的乘积。",
    "tldr": "本研究通过学习深度矩阵分解，阐明了大深度神经网络优化器中的随机性隐式正则化效应，并证明了最小化损失函数中海森矩阵迹近似等同于最小化其对应端对端矩阵参数的Schatten 1-范数乘积。",
    "en_tdlr": "This work explains the implicit regularization effect of stochasticity in the optimizer of over-parameterized neural networks through deep matrix factorization and proves that minimizing the trace of Hessian in the loss function is approximately equivalent to minimizing the Schatten 1-norm of the corresponding end-to-end matrix parameters."
}