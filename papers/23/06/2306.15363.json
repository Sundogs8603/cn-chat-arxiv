{
    "title": "Your Attack Is Too DUMB: Formalizing Attacker Scenarios for Adversarial Transferability. (arXiv:2306.15363v1 [cs.CR])",
    "abstract": "Evasion attacks are a threat to machine learning models, where adversaries attempt to affect classifiers by injecting malicious samples. An alarming side-effect of evasion attacks is their ability to transfer among different models: this property is called transferability. Therefore, an attacker can produce adversarial samples on a custom model (surrogate) to conduct the attack on a victim's organization later. Although literature widely discusses how adversaries can transfer their attacks, their experimental settings are limited and far from reality. For instance, many experiments consider both attacker and defender sharing the same dataset, balance level (i.e., how the ground truth is distributed), and model architecture.  In this work, we propose the DUMB attacker model. This framework allows analyzing if evasion attacks fail to transfer when the training conditions of surrogate and victim models differ. DUMB considers the following conditions: Dataset soUrces, Model architecture, a",
    "link": "http://arxiv.org/abs/2306.15363",
    "context": "Title: Your Attack Is Too DUMB: Formalizing Attacker Scenarios for Adversarial Transferability. (arXiv:2306.15363v1 [cs.CR])\nAbstract: Evasion attacks are a threat to machine learning models, where adversaries attempt to affect classifiers by injecting malicious samples. An alarming side-effect of evasion attacks is their ability to transfer among different models: this property is called transferability. Therefore, an attacker can produce adversarial samples on a custom model (surrogate) to conduct the attack on a victim's organization later. Although literature widely discusses how adversaries can transfer their attacks, their experimental settings are limited and far from reality. For instance, many experiments consider both attacker and defender sharing the same dataset, balance level (i.e., how the ground truth is distributed), and model architecture.  In this work, we propose the DUMB attacker model. This framework allows analyzing if evasion attacks fail to transfer when the training conditions of surrogate and victim models differ. DUMB considers the following conditions: Dataset soUrces, Model architecture, a",
    "path": "papers/23/06/2306.15363.json",
    "total_tokens": 883,
    "translated_title": "你的攻击太愚蠢了: 对敌对迁移的攻击者场景进行形式化。(arXiv:2306.15363v1 [cs.CR])",
    "translated_abstract": "攻击规避对机器学习模型构成威胁，攻击者试图通过注入恶意样本来影响分类器。攻击规避的一个令人担忧的副作用是其能够在不同模型之间进行迁移，这被称为迁移性。因此，攻击者可以在自定义模型（替代品）上生成对抗样本，然后在受害组织上进行攻击。尽管文献广泛讨论了敌对方如何转移他们的攻击，但他们的实验设置有限且与现实相距甚远。例如，许多实验考虑攻击者和防御者共享相同的数据集、平衡级别（即，基本事实分布）和模型架构。在这项工作中，我们提出了愚蠢的攻击者模型（DUMB）。该框架允许分析当替代品和受害模型的训练条件不同时，攻击规避是否失败转移。愚蠢攻击者模型考虑以下条件：数据集来源，模型架构，敌对方选择性。",
    "tldr": "本论文提出了DUMB攻击者模型，通过分析在替代品和受害模型的训练条件不同的情况下，攻击规避是否失败转移。"
}