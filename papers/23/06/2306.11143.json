{
    "title": "Nonlinear Feature Aggregation: Two Algorithms driven by Theory. (arXiv:2306.11143v1 [cs.LG])",
    "abstract": "Many real-world machine learning applications are characterized by a huge number of features, leading to computational and memory issues, as well as the risk of overfitting. Ideally, only relevant and non-redundant features should be considered to preserve the complete information of the original data and limit the dimensionality. Dimensionality reduction and feature selection are common preprocessing techniques addressing the challenge of efficiently dealing with high-dimensional data. Dimensionality reduction methods control the number of features in the dataset while preserving its structure and minimizing information loss. Feature selection aims to identify the most relevant features for a task, discarding the less informative ones. Previous works have proposed approaches that aggregate features depending on their correlation without discarding any of them and preserving their interpretability through aggregation with the mean. A limitation of methods based on correlation is the as",
    "link": "http://arxiv.org/abs/2306.11143",
    "context": "Title: Nonlinear Feature Aggregation: Two Algorithms driven by Theory. (arXiv:2306.11143v1 [cs.LG])\nAbstract: Many real-world machine learning applications are characterized by a huge number of features, leading to computational and memory issues, as well as the risk of overfitting. Ideally, only relevant and non-redundant features should be considered to preserve the complete information of the original data and limit the dimensionality. Dimensionality reduction and feature selection are common preprocessing techniques addressing the challenge of efficiently dealing with high-dimensional data. Dimensionality reduction methods control the number of features in the dataset while preserving its structure and minimizing information loss. Feature selection aims to identify the most relevant features for a task, discarding the less informative ones. Previous works have proposed approaches that aggregate features depending on their correlation without discarding any of them and preserving their interpretability through aggregation with the mean. A limitation of methods based on correlation is the as",
    "path": "papers/23/06/2306.11143.json",
    "total_tokens": 1188,
    "translated_title": "非线性特征聚合：由理论推导而来的两个算法",
    "translated_abstract": "许多现实世界的机器学习应用程序都具有大量的特征，这导致了计算和内存问题，以及过拟合的风险。理想情况下，应该仅考虑相关的非冗余特征，以保留原始数据的完整信息并限制维数。降维和特征选择是常用的预处理技术，用于解决高维数据的有效处理挑战。维度缩减方法控制数据集中特征的数量，同时保留其结构并最小化信息损失。特征选择旨在识别任务的最相关特征，舍弃信息较少的特征。先前的工作提出了基于相关性聚合特征的方法，而不丢弃任何特征，并通过平均聚合保持其可解释性。基于相关性的方法的局限性在于假设特征之间的关系是线性的，这在处理非线性数据时可能导致子优表示。本文提出了两种聚合非线性特征的新算法：通过信息瓶颈进行非线性特征聚合（NFA-IB）和通过核嵌入进行非线性特征聚合（NFA-KE）。NFA-IB基于信息瓶颈原理，而NFA-KE使用核函数将原始特征转换为更高维的空间，可以在其中捕捉特征之间的非线性关系。我们展示了我们的方法在各种场景中与最先进的技术相比的有效性。",
    "tldr": "本文提出了两种聚合非线性特征的新算法：通过信息瓶颈进行非线性特征聚合（NFA-IB）和通过核嵌入进行非线性特征聚合（NFA-KE）。这些方法可以有效地解决处理非线性数据时基于相关性的方法可能导致子优表示的问题。",
    "en_tdlr": "This paper proposes two new algorithms for aggregating nonlinear features: Nonlinear Feature Aggregation via Information Bottleneck (NFA-IB) and Nonlinear Feature Aggregation via Kernel Embedding (NFA-KE). These methods can effectively address the issue of suboptimal representations that may arise when using correlation-based methods to deal with nonlinear data."
}