{
    "title": "From Hypergraph Energy Functions to Hypergraph Neural Networks. (arXiv:2306.09623v1 [cs.LG])",
    "abstract": "Hypergraphs are a powerful abstraction for representing higher-order interactions between entities of interest. To exploit these relationships in making downstream predictions, a variety of hypergraph neural network architectures have recently been proposed, in large part building upon precursors from the more traditional graph neural network (GNN) literature. Somewhat differently, in this paper we begin by presenting an expressive family of parameterized, hypergraph-regularized energy functions. We then demonstrate how minimizers of these energies effectively serve as node embeddings that, when paired with a parameterized classifier, can be trained end-to-end via a supervised bilevel optimization process. Later, we draw parallels between the implicit architecture of the predictive models emerging from the proposed bilevel hypergraph optimization, and existing GNN architectures in common use. Empirically, we demonstrate state-of-the-art results on various hypergraph node classification",
    "link": "http://arxiv.org/abs/2306.09623",
    "context": "Title: From Hypergraph Energy Functions to Hypergraph Neural Networks. (arXiv:2306.09623v1 [cs.LG])\nAbstract: Hypergraphs are a powerful abstraction for representing higher-order interactions between entities of interest. To exploit these relationships in making downstream predictions, a variety of hypergraph neural network architectures have recently been proposed, in large part building upon precursors from the more traditional graph neural network (GNN) literature. Somewhat differently, in this paper we begin by presenting an expressive family of parameterized, hypergraph-regularized energy functions. We then demonstrate how minimizers of these energies effectively serve as node embeddings that, when paired with a parameterized classifier, can be trained end-to-end via a supervised bilevel optimization process. Later, we draw parallels between the implicit architecture of the predictive models emerging from the proposed bilevel hypergraph optimization, and existing GNN architectures in common use. Empirically, we demonstrate state-of-the-art results on various hypergraph node classification",
    "path": "papers/23/06/2306.09623.json",
    "total_tokens": 815,
    "translated_title": "从超图能量函数到超图神经网络",
    "translated_abstract": "超图是表示实体之间高阶交互的强大抽象模型。为了在实现下游预测的过程中利用这些关系，最近提出了多种超图神经网络架构，这些架构在很大程度上是建立在传统图神经网络（GNN）文献的先驱上的。在这篇论文中，我们首先介绍了一类具有表达能力的参数化超图正则化能量函数。然后，我们演示了如何将这些能量的极小值有效地作为节点嵌入器，再配合一个参数化分类器进行端到端训练，通过一个监督的双层优化过程实现。之后，我们发现了建议的双层超图优化中出现的预测模型的隐式架构和常用GNN架构之间的相似之处。在实证方面，我们在各种超图节点分类任务中展示了最先进的结果。",
    "tldr": "本文提出了一种新的基于超图能量函数的节点嵌入方法，可以通过双层优化实现节点分类任务，相比传统GNN模型有更好的表现。",
    "en_tdlr": "This paper presents a novel approach for node embedding based on hypergraph energy functions, which can achieve node classification tasks through bilevel optimization and outperforms traditional GNN models."
}