{
    "title": "Quasi-Newton Updating for Large-Scale Distributed Learning. (arXiv:2306.04111v1 [cs.LG])",
    "abstract": "Distributed computing is critically important for modern statistical analysis. Herein, we develop a distributed quasi-Newton (DQN) framework with excellent statistical, computation, and communication efficiency. In the DQN method, no Hessian matrix inversion or communication is needed. This considerably reduces the computation and communication complexity of the proposed method. Notably, related existing methods only analyze numerical convergence and require a diverging number of iterations to converge. However, we investigate the statistical properties of the DQN method and theoretically demonstrate that the resulting estimator is statistically efficient over a small number of iterations under mild conditions. Extensive numerical analyses demonstrate the finite sample performance.",
    "link": "http://arxiv.org/abs/2306.04111",
    "context": "Title: Quasi-Newton Updating for Large-Scale Distributed Learning. (arXiv:2306.04111v1 [cs.LG])\nAbstract: Distributed computing is critically important for modern statistical analysis. Herein, we develop a distributed quasi-Newton (DQN) framework with excellent statistical, computation, and communication efficiency. In the DQN method, no Hessian matrix inversion or communication is needed. This considerably reduces the computation and communication complexity of the proposed method. Notably, related existing methods only analyze numerical convergence and require a diverging number of iterations to converge. However, we investigate the statistical properties of the DQN method and theoretically demonstrate that the resulting estimator is statistically efficient over a small number of iterations under mild conditions. Extensive numerical analyses demonstrate the finite sample performance.",
    "path": "papers/23/06/2306.04111.json",
    "total_tokens": 775,
    "translated_title": "大规模分布式学习的拟牛顿更新",
    "translated_abstract": "分布式计算对于现代统计分析至关重要。本文提出了一种具有出色的统计、计算和通信效率的分布式拟牛顿(DQN)框架。在DQN方法中，不需要牛顿矩阵求逆或通信，这大大减少了所提出方法的计算和通信复杂性。值得注意的是，现有的相关方法只分析数值收敛，并需要发散的迭代次数才能收敛。然而，我们研究了DQN方法的统计特性，并在温和条件下理论上证明了结果估计器在少量迭代下的统计效率。广泛的数值分析证明了有限的样本性能。",
    "tldr": "本文提出了一种具有出色统计、计算和通信效率的分布式拟牛顿(DQN)框架，与现有方法相比，它不需要牛顿矩阵求逆或通信，并且通过理论证明和数值分析证明其统计特性和有限的样本性能。",
    "en_tdlr": "The paper proposes a distributed quasi-Newton (DQN) framework with excellent statistical, computation, and communication efficiency for large-scale distributed learning. Compared with existing methods, the DQN method does not require Hessian matrix inversion or communication, and its statistical characteristics and finite sample performance are theoretically demonstrated and numerically analyzed."
}