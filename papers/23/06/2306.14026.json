{
    "title": "Information criteria for structured parameter selection in high dimensional tree and graph models. (arXiv:2306.14026v1 [cs.LG])",
    "abstract": "Parameter selection in high-dimensional models is typically finetuned in a way that keeps the (relative) number of false positives under control. This is because otherwise the few true positives may be dominated by the many possible false positives. This happens, for instance, when the selection follows from a naive optimisation of an information criterion, such as AIC or Mallows's Cp. It can be argued that the overestimation of the selection comes from the optimisation process itself changing the statistics of the selected variables, in a way that the information criterion no longer reflects the true divergence between the selection and the data generating process. In lasso, the overestimation can also be linked to the shrinkage estimator, which makes the selection too tolerant of false positive selections. For these reasons, this paper works on refined information criteria, carefully balancing false positives and false negatives, for use with estimators without shrinkage. In particul",
    "link": "http://arxiv.org/abs/2306.14026",
    "context": "Title: Information criteria for structured parameter selection in high dimensional tree and graph models. (arXiv:2306.14026v1 [cs.LG])\nAbstract: Parameter selection in high-dimensional models is typically finetuned in a way that keeps the (relative) number of false positives under control. This is because otherwise the few true positives may be dominated by the many possible false positives. This happens, for instance, when the selection follows from a naive optimisation of an information criterion, such as AIC or Mallows's Cp. It can be argued that the overestimation of the selection comes from the optimisation process itself changing the statistics of the selected variables, in a way that the information criterion no longer reflects the true divergence between the selection and the data generating process. In lasso, the overestimation can also be linked to the shrinkage estimator, which makes the selection too tolerant of false positive selections. For these reasons, this paper works on refined information criteria, carefully balancing false positives and false negatives, for use with estimators without shrinkage. In particul",
    "path": "papers/23/06/2306.14026.json",
    "total_tokens": 953,
    "translated_title": "高维树和图模型中的结构参数选择信息准则",
    "translated_abstract": "在高维模型中进行参数选择通常会通过控制虚假阳性（相对）数量的方式进行微调。这是因为否则，少数的真实阳性可能会被众多的虚假阳性所主导。例如，当选择遵循于信息准则的朴素优化（例如AIC或Mallows的Cp）时，就会发生这种情况。可以认为，选择的过度估计来自于优化过程自身改变所选变量的统计特性，这样信息准则就不再反映选择与数据生成过程之间的真实差异。在lasso中，过度估计也可以与收缩估计器联系起来，这使得选择对错误的阳性选择过于宽容。因此，本文研究了精细的信息准则，仔细平衡虚假阳性和虚假阴性，用于没有收缩的估计器。具体而言，本文提出了一种树结构模型和一种图结构模型，并针对这两种模型调整了两个信息准则（阿卡贝克信息准则和贝叶斯信息准则），考虑了结构的因素。",
    "tldr": "本文提出了适用于两种结构模型的信息准则，用于高维数据的参数选择，并平衡了虚假阳性和虚假阴性。",
    "en_tdlr": "This paper proposes refined information criteria for parameter selection in high-dimensional tree and graph models, which balance false positives and false negatives, and adapts them to both tree-structured and graph-structured models."
}