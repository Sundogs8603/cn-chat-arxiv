{
    "title": "Achieving Sample and Computational Efficient Reinforcement Learning by Action Space Reduction via Grouping. (arXiv:2306.12981v1 [cs.LG])",
    "abstract": "Reinforcement learning often needs to deal with the exponential growth of states and actions when exploring optimal control in high-dimensional spaces (often known as the curse of dimensionality). In this work, we address this issue by learning the inherent structure of action-wise similar MDP to appropriately balance the performance degradation versus sample/computational complexity. In particular, we partition the action spaces into multiple groups based on the similarity in transition distribution and reward function, and build a linear decomposition model to capture the difference between the intra-group transition kernel and the intra-group rewards. Both our theoretical analysis and experiments reveal a \\emph{surprising and counter-intuitive result}: while a more refined grouping strategy can reduce the approximation error caused by treating actions in the same group as identical, it also leads to increased estimation error when the size of samples or the computation resources is ",
    "link": "http://arxiv.org/abs/2306.12981",
    "context": "Title: Achieving Sample and Computational Efficient Reinforcement Learning by Action Space Reduction via Grouping. (arXiv:2306.12981v1 [cs.LG])\nAbstract: Reinforcement learning often needs to deal with the exponential growth of states and actions when exploring optimal control in high-dimensional spaces (often known as the curse of dimensionality). In this work, we address this issue by learning the inherent structure of action-wise similar MDP to appropriately balance the performance degradation versus sample/computational complexity. In particular, we partition the action spaces into multiple groups based on the similarity in transition distribution and reward function, and build a linear decomposition model to capture the difference between the intra-group transition kernel and the intra-group rewards. Both our theoretical analysis and experiments reveal a \\emph{surprising and counter-intuitive result}: while a more refined grouping strategy can reduce the approximation error caused by treating actions in the same group as identical, it also leads to increased estimation error when the size of samples or the computation resources is ",
    "path": "papers/23/06/2306.12981.json",
    "total_tokens": 1014,
    "tldr": "本文提出一种分组学习动作间相似的马尔科夫决策过程内在结构的方法，成功解决高维强化学习中的样本和计算复杂度问题，提高了效率。"
}