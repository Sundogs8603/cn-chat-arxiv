{
    "title": "Structure-Aware DropEdge Towards Deep Graph Convolutional Networks. (arXiv:2306.12091v1 [cs.LG])",
    "abstract": "It has been discovered that Graph Convolutional Networks (GCNs) encounter a remarkable drop in performance when multiple layers are piled up. The main factor that accounts for why deep GCNs fail lies in over-smoothing, which isolates the network output from the input with the increase of network depth, weakening expressivity and trainability. In this paper, we start by investigating refined measures upon DropEdge -- an existing simple yet effective technique to relieve over-smoothing. We term our method as DropEdge++ for its two structure-aware samplers in contrast to DropEdge: layer-dependent sampler and feature-dependent sampler. Regarding the layer-dependent sampler, we interestingly find that increasingly sampling edges from the bottom layer yields superior performance than the decreasing counterpart as well as DropEdge. We theoretically reveal this phenomenon with Mean-Edge-Number (MEN), a metric closely related to over-smoothing. For the feature-dependent sampler, we associate th",
    "link": "http://arxiv.org/abs/2306.12091",
    "context": "Title: Structure-Aware DropEdge Towards Deep Graph Convolutional Networks. (arXiv:2306.12091v1 [cs.LG])\nAbstract: It has been discovered that Graph Convolutional Networks (GCNs) encounter a remarkable drop in performance when multiple layers are piled up. The main factor that accounts for why deep GCNs fail lies in over-smoothing, which isolates the network output from the input with the increase of network depth, weakening expressivity and trainability. In this paper, we start by investigating refined measures upon DropEdge -- an existing simple yet effective technique to relieve over-smoothing. We term our method as DropEdge++ for its two structure-aware samplers in contrast to DropEdge: layer-dependent sampler and feature-dependent sampler. Regarding the layer-dependent sampler, we interestingly find that increasingly sampling edges from the bottom layer yields superior performance than the decreasing counterpart as well as DropEdge. We theoretically reveal this phenomenon with Mean-Edge-Number (MEN), a metric closely related to over-smoothing. For the feature-dependent sampler, we associate th",
    "path": "papers/23/06/2306.12091.json",
    "total_tokens": 962,
    "translated_title": "针对深度图卷积网络的结构感知DropEdge方法",
    "translated_abstract": "研究发现，当堆积多个层时，图卷积网络（GCNs）的性能会显著下降。导致深层GCNs失败的主要因素在于过度平滑，随着网络深度的增加，使输出的网络与输入隔离，削弱了表达能力和可训练性。本文从DropEdge这一现有的简单而有效的技术入手，提出了一种新方法——DropEdge++。与DropEdge相比，DropEdge++具有两个结构感知采样器：基于层的采样器和基于特征的采样器。关于基于层的采样器，有趣的是，我们发现越来越多地从底层采样边比逐渐减少的采样边以及DropEdge更能提高性能。我们通过一个与过度平滑密切相关的指标——平均边数(MEN)来理论上揭示了这一现象。对于基于特征的采样器，我们将其与DropEdge++的其余部分相结合...",
    "tldr": "本文提出了一种结构感知的DropEdge++方法，其中包含基于层的采样器和基于特征的采样器。研究还发现，从底层采样边比逐渐减少的采样边以及DropEdge更能提高性能，证明了这一现象与过度平滑密切相关。",
    "en_tdlr": "This paper proposes a structure-aware DropEdge++ method with a layer-dependent sampler and feature-dependent sampler to relieve the over-smoothing problem in deep Graph Convolutional Networks (GCNs). It is found that sampling more edges from the bottom layer outperforms the decreasing counterpart and DropEdge, which is theoretically explained with a metric called Mean-Edge-Number (MEN)."
}