{
    "title": "MimiC: Combating Client Dropouts in Federated Learning by Mimicking Central Updates. (arXiv:2306.12212v1 [cs.LG])",
    "abstract": "Federated learning (FL) is a promising framework for privacy-preserving collaborative learning. In FL, the model training tasks are distributed to clients and only the model updates need to be collected at a central server. However, when being deployed at the mobile edge network, clients (e.g., smartphones and wearables) may have unpredictable availability and randomly drop out of any training iteration, which hinders FL from achieving the convergence. This paper tackles such a critical challenge of FL. In particular, we first investigate the convergence of the classical FedAvg algorithm with arbitrary client dropouts. We find that with the common choice of a decaying learning rate, FedAvg can only oscillate within the neighborhood of a stationary point of the global loss function, which is caused by the divergence between the aggregated update and the desired central update. Motivated by this new observation, we then design a novel training algorithm named MimiC, where the server modi",
    "link": "http://arxiv.org/abs/2306.12212",
    "context": "Title: MimiC: Combating Client Dropouts in Federated Learning by Mimicking Central Updates. (arXiv:2306.12212v1 [cs.LG])\nAbstract: Federated learning (FL) is a promising framework for privacy-preserving collaborative learning. In FL, the model training tasks are distributed to clients and only the model updates need to be collected at a central server. However, when being deployed at the mobile edge network, clients (e.g., smartphones and wearables) may have unpredictable availability and randomly drop out of any training iteration, which hinders FL from achieving the convergence. This paper tackles such a critical challenge of FL. In particular, we first investigate the convergence of the classical FedAvg algorithm with arbitrary client dropouts. We find that with the common choice of a decaying learning rate, FedAvg can only oscillate within the neighborhood of a stationary point of the global loss function, which is caused by the divergence between the aggregated update and the desired central update. Motivated by this new observation, we then design a novel training algorithm named MimiC, where the server modi",
    "path": "papers/23/06/2306.12212.json",
    "total_tokens": 955,
    "translated_title": "MimiC：模仿中心更新解决联邦学习中的客户端退出问题",
    "translated_abstract": "联邦学习是一种有前途的隐私保护协作学习框架。在联邦学习中，模型训练任务分发给客户端，只需要在中央服务器收集模型更新。然而，在移动边缘网络中部署时，客户端（如智能手机和可穿戴设备）可能会无预警地退出任何一次训练迭代，这会阻碍联邦学习达到收敛。本文解决了联邦学习中这一关键挑战，设计出一种名为 MimiC 的新型训练算法，该算法在中心服务器修改其更新以模仿缺失客户端更新，通过实验结果显示，MimiC 相对现有方法在多个基准数据集上均取得了更高的测试准确率和更低的通信成本。",
    "tldr": "本文提出的 MimiC 算法解决了联邦学习中客户端退出问题，通过模仿缺失的客户端更新解决了聚合更新和期望中心更新之间的分歧，实现了更高的测试准确率和更低的通信成本。",
    "en_tdlr": "The MimiC algorithm proposed in this paper addresses the problem of client dropouts in federated learning by mimicking the missing client updates, resolving the divergence between aggregated updates and desired central updates, and achieving higher test accuracy and lower communication costs."
}