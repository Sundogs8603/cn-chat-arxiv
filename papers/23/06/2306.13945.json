{
    "title": "Large Sequence Models for Sequential Decision-Making: A Survey. (arXiv:2306.13945v1 [cs.LG])",
    "abstract": "Transformer architectures have facilitated the development of large-scale and general-purpose sequence models for prediction tasks in natural language processing and computer vision, e.g., GPT-3 and Swin Transformer. Although originally designed for prediction problems, it is natural to inquire about their suitability for sequential decision-making and reinforcement learning problems, which are typically beset by long-standing issues involving sample efficiency, credit assignment, and partial observability. In recent years, sequence models, especially the Transformer, have attracted increasing interest in the RL communities, spawning numerous approaches with notable effectiveness and generalizability. This survey presents a comprehensive overview of recent works aimed at solving sequential decision-making tasks with sequence models such as the Transformer, by discussing the connection between sequential decision-making and sequence modeling, and categorizing them based on the way they ",
    "link": "http://arxiv.org/abs/2306.13945",
    "context": "Title: Large Sequence Models for Sequential Decision-Making: A Survey. (arXiv:2306.13945v1 [cs.LG])\nAbstract: Transformer architectures have facilitated the development of large-scale and general-purpose sequence models for prediction tasks in natural language processing and computer vision, e.g., GPT-3 and Swin Transformer. Although originally designed for prediction problems, it is natural to inquire about their suitability for sequential decision-making and reinforcement learning problems, which are typically beset by long-standing issues involving sample efficiency, credit assignment, and partial observability. In recent years, sequence models, especially the Transformer, have attracted increasing interest in the RL communities, spawning numerous approaches with notable effectiveness and generalizability. This survey presents a comprehensive overview of recent works aimed at solving sequential decision-making tasks with sequence models such as the Transformer, by discussing the connection between sequential decision-making and sequence modeling, and categorizing them based on the way they ",
    "path": "papers/23/06/2306.13945.json",
    "total_tokens": 825,
    "translated_title": "大型序列模型用于顺序决策：综述",
    "translated_abstract": "Transformer结构促进了自然语言处理和计算机视觉中预测任务的大规模通用序列模型的发展，例如GPT-3和Swin Transformer。虽然最初设计用于预测问题，但自然而然地会询问它们是否适用于通常存在样本效率、信用分配和部分可观察性问题的顺序决策和强化学习问题。近年来，序列模型，特别是Transformer，吸引了RL社区越来越多的关注，产生了许多具有显着有效性和通用性的方法。该综述全面概述了最近的工作，旨在通过讨论顺序决策和序列建模之间的联系，并基于它们处理前述问题的方式对它们进行分类，解决使用序列模型（例如Transformer）解决顺序决策任务的问题。",
    "tldr": "本综述全面概述了使用Transformer等序列模型解决顺序决策问题的最近研究进展，并按照处理样本效率、信用分配和部分可观察性的方式对其进行分类。",
    "en_tdlr": "This survey provides a comprehensive overview of recent works that utilize large-scale, general-purpose sequence models, such as the Transformer, for solving sequential decision-making tasks in reinforcement learning. Works are categorized based on how they handle issues such as sample efficiency, credit assignment, and partial observability."
}