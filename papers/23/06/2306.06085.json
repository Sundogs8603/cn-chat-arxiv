{
    "title": "Trapping LLM Hallucinations Using Tagged Context Prompts. (arXiv:2306.06085v1 [cs.CL])",
    "abstract": "Recent advances in large language models (LLMs), such as ChatGPT, have led to highly sophisticated conversation agents. However, these models suffer from \"hallucinations,\" where the model generates false or fabricated information. Addressing this challenge is crucial, particularly with AI-driven platforms being adopted across various sectors. In this paper, we propose a novel method to recognize and flag instances when LLMs perform outside their domain knowledge, and ensuring users receive accurate information.  We find that the use of context combined with embedded tags can successfully combat hallucinations within generative language models. To do this, we baseline hallucination frequency in no-context prompt-response pairs using generated URLs as easily-tested indicators of fabricated data. We observed a significant reduction in overall hallucination when context was supplied along with question prompts for tested generative engines. Lastly, we evaluated how placing tags within cont",
    "link": "http://arxiv.org/abs/2306.06085",
    "context": "Title: Trapping LLM Hallucinations Using Tagged Context Prompts. (arXiv:2306.06085v1 [cs.CL])\nAbstract: Recent advances in large language models (LLMs), such as ChatGPT, have led to highly sophisticated conversation agents. However, these models suffer from \"hallucinations,\" where the model generates false or fabricated information. Addressing this challenge is crucial, particularly with AI-driven platforms being adopted across various sectors. In this paper, we propose a novel method to recognize and flag instances when LLMs perform outside their domain knowledge, and ensuring users receive accurate information.  We find that the use of context combined with embedded tags can successfully combat hallucinations within generative language models. To do this, we baseline hallucination frequency in no-context prompt-response pairs using generated URLs as easily-tested indicators of fabricated data. We observed a significant reduction in overall hallucination when context was supplied along with question prompts for tested generative engines. Lastly, we evaluated how placing tags within cont",
    "path": "papers/23/06/2306.06085.json",
    "total_tokens": 957,
    "translated_title": "使用标记的上下文提示捕捉LLM妄想症",
    "translated_abstract": "大型语言模型（LLM）的最新进展，例如ChatGPT，已经导致高度复杂的对话代理。但是，这些模型受到“妄想症”的困扰，即模型生成虚假或捏造的信息。解决这一挑战非常关键，特别是在各个领域采用AI驱动的平台时。本文提出了一种新方法，用于识别和标记LLM在其领域知识范围之外的问题，确保用户获得准确的信息。我们发现，结合嵌入的标记和上下文来对抗生成语言模型中的妄想症可以成功地应对。为此，我们使用生成的URL作为易于测试的捏造数据指标，在没有上下文提示-响应对的情况下基线依赖性妄想频率。我们观察到，在为测试的生成引擎提供问题提示时提供上下文可以显著减少总体妄想症。最后，我们评估了在上下文提示中放置标记如何影响检测妄想症。我们的实验表明，在LLM中标记的上下文提示提高了检测妄想症的准确性和有效性。",
    "tldr": "本文提出一种使用标记的上下文提示的新方法，可以显著减少生成语言模型中的妄想症，提高对话代理的准确性和有效性。",
    "en_tdlr": "This paper proposes a novel method using tagged context prompts to significantly reduce hallucinations in generative language models, improving the accuracy and effectiveness of conversation agents."
}