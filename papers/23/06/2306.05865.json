{
    "title": "Faster Discrete Convex Function Minimization with Predictions: The M-Convex Case. (arXiv:2306.05865v1 [cs.LG])",
    "abstract": "Recent years have seen a growing interest in accelerating optimization algorithms with machine-learned predictions. Sakaue and Oki (NeurIPS 2022) have developed a general framework that warm-starts the L-convex function minimization method with predictions, revealing the idea's usefulness for various discrete optimization problems. In this paper, we present a framework for using predictions to accelerate M-convex function minimization, thus complementing previous research and extending the range of discrete optimization algorithms that can benefit from predictions. Our framework is particularly effective for an important subclass called laminar convex minimization, which appears in many operations research applications. Our methods can improve time complexity bounds upon the best worst-case results by using predictions and even have potential to go beyond a lower-bound result.",
    "link": "http://arxiv.org/abs/2306.05865",
    "context": "Title: Faster Discrete Convex Function Minimization with Predictions: The M-Convex Case. (arXiv:2306.05865v1 [cs.LG])\nAbstract: Recent years have seen a growing interest in accelerating optimization algorithms with machine-learned predictions. Sakaue and Oki (NeurIPS 2022) have developed a general framework that warm-starts the L-convex function minimization method with predictions, revealing the idea's usefulness for various discrete optimization problems. In this paper, we present a framework for using predictions to accelerate M-convex function minimization, thus complementing previous research and extending the range of discrete optimization algorithms that can benefit from predictions. Our framework is particularly effective for an important subclass called laminar convex minimization, which appears in many operations research applications. Our methods can improve time complexity bounds upon the best worst-case results by using predictions and even have potential to go beyond a lower-bound result.",
    "path": "papers/23/06/2306.05865.json",
    "total_tokens": 830,
    "translated_title": "利用预测加速分离凸函数极小化：M-凸情况",
    "translated_abstract": "近年来，越来越多的研究者将机器学习的预测方法应用于优化算法的加速。Sakaue和Oki（NeurIPS 2022）提出了一个通用框架，利用预测方法启动L-凸函数极小化方法，展示了这种方法在各种离散优化问题中的有效性。本文提出了一个利用预测方法加速M-凸函数极小化的框架，旨在补充以前的研究，扩展能从预测中受益的离散优化算法范围。我们的方法在重要子类——分层凸极小化中特别有效，该子类出现在许多运筹学应用中。我们的方法可以使用预测改善时间复杂度的最差情况下的结果，甚至有可能超过最低下界结果。",
    "tldr": "本文提出了利用预测方法加速M-凸函数极小化的框架，特别适用于分层凸极小化等离散优化问题，并且能够较好地改善时间复杂度。",
    "en_tdlr": "This paper proposes a framework for using predictions to accelerate M-convex function minimization, which is particularly effective for an important subclass called laminar convex minimization and can improve time complexity bounds upon the best worst-case results by using predictions."
}