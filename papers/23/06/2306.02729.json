{
    "title": "Gibbs Sampling the Posterior of Neural Networks. (arXiv:2306.02729v1 [cs.LG])",
    "abstract": "In this paper, we study sampling from a posterior derived from a neural network. We propose a new probabilistic model consisting of adding noise at every pre- and post-activation in the network, arguing that the resulting posterior can be sampled using an efficient Gibbs sampler. The Gibbs sampler attains similar performances as the state-of-the-art Monte Carlo Markov chain methods, such as the Hamiltonian Monte Carlo or the Metropolis adjusted Langevin algorithm, both on real and synthetic data. By framing our analysis in the teacher-student setting, we introduce a thermalization criterion that allows us to detect when an algorithm, when run on data with synthetic labels, fails to sample from the posterior. The criterion is based on the fact that in the teacher-student setting we can initialize an algorithm directly at equilibrium.",
    "link": "http://arxiv.org/abs/2306.02729",
    "context": "Title: Gibbs Sampling the Posterior of Neural Networks. (arXiv:2306.02729v1 [cs.LG])\nAbstract: In this paper, we study sampling from a posterior derived from a neural network. We propose a new probabilistic model consisting of adding noise at every pre- and post-activation in the network, arguing that the resulting posterior can be sampled using an efficient Gibbs sampler. The Gibbs sampler attains similar performances as the state-of-the-art Monte Carlo Markov chain methods, such as the Hamiltonian Monte Carlo or the Metropolis adjusted Langevin algorithm, both on real and synthetic data. By framing our analysis in the teacher-student setting, we introduce a thermalization criterion that allows us to detect when an algorithm, when run on data with synthetic labels, fails to sample from the posterior. The criterion is based on the fact that in the teacher-student setting we can initialize an algorithm directly at equilibrium.",
    "path": "papers/23/06/2306.02729.json",
    "total_tokens": 845,
    "translated_title": "Gibbs采样神经网络的后验分布",
    "translated_abstract": "本文研究了从神经网络的后验分布中进行采样。我们提出了一种新的概率模型，该模型在网络的每个预激活和后激活中添加噪声，并认为使用有效的Gibbs采样器可以采样得到所得到的后验分布。在真实数据和合成数据上，Gibbs采样器能够达到类似于状态-of-the-art的马尔科夫链蒙特卡洛方法（如哈密顿蒙特卡洛或Metropolis调整Langevin算法）的性能。通过在师生设置中进行分析，我们引入了一个热化准则，该准则允许我们检测算法在使用合成标签的数据上运行时是否无法从后验分布中采样。该准则基于师生设置中的事实，我们可以直接在平衡点处初始化算法。",
    "tldr": "这篇论文提出了一种添加噪声的神经网络模型，并使用Gibbs采样器从后验分布中进行采样，该方法在真实数据和合成数据中能够达到类似于马尔科夫链蒙特卡洛方法的性能。",
    "en_tdlr": "This paper proposes a neural network model with added noise and uses a Gibbs sampler to sample from the posterior distribution. The method achieves similar performance to state-of-the-art Monte Carlo Markov Chain methods on both real and synthetic data, and introduces a thermalization criterion for detecting when an algorithm fails to sample from the posterior in the teacher-student setting."
}