{
    "title": "Word Embeddings for Banking Industry. (arXiv:2306.01807v1 [cs.CL])",
    "abstract": "Applications of Natural Language Processing (NLP) are plentiful, from sentiment analysis to text classification. Practitioners rely on static word embeddings (e.g. Word2Vec or GloVe) or static word representation from contextual models (e.g. BERT or ELMo) to perform many of these NLP tasks. These widely available word embeddings are built from large amount of text, so they are likely to have captured most of the vocabulary in different context. However, how well would they capture domain-specific semantics and word relatedness? This paper explores this idea by creating a bank-specific word embeddings and evaluates them against other sources of word embeddings such as GloVe and BERT. Not surprising that embeddings built from bank-specific corpora does a better job of capturing the bank-specific semantics and word relatedness. This finding suggests that bank-specific word embeddings could be a good stand-alone source or a complement to other widely available embeddings when performing NL",
    "link": "http://arxiv.org/abs/2306.01807",
    "context": "Title: Word Embeddings for Banking Industry. (arXiv:2306.01807v1 [cs.CL])\nAbstract: Applications of Natural Language Processing (NLP) are plentiful, from sentiment analysis to text classification. Practitioners rely on static word embeddings (e.g. Word2Vec or GloVe) or static word representation from contextual models (e.g. BERT or ELMo) to perform many of these NLP tasks. These widely available word embeddings are built from large amount of text, so they are likely to have captured most of the vocabulary in different context. However, how well would they capture domain-specific semantics and word relatedness? This paper explores this idea by creating a bank-specific word embeddings and evaluates them against other sources of word embeddings such as GloVe and BERT. Not surprising that embeddings built from bank-specific corpora does a better job of capturing the bank-specific semantics and word relatedness. This finding suggests that bank-specific word embeddings could be a good stand-alone source or a complement to other widely available embeddings when performing NL",
    "path": "papers/23/06/2306.01807.json",
    "total_tokens": 926,
    "translated_title": "面向银行业的词嵌入",
    "translated_abstract": "自然语言处理（NLP）应用广泛，从情感分析到文本分类。从静态词嵌入（如Word2Vec或GloVe）到上下文模型提取的静态词表征（如BERT或ELMo），从这些NLP任务中，从业人员往往依赖于这些广泛可用的词嵌入。这些词嵌入是从大量文本构建而成，因此可能已经捕捉了不同上下文中大部分词汇。但是，它们能多好地捕捉特定领域的语义和词相关性？本文通过创建一种银行特定的词嵌入并将其与其他来源的词嵌入（如GloVe和BERT）进行评估，探讨了这个想法。不出所料，从银行特定语料库构建的嵌入更能捕捉银行特定语义和词相关性。该发现表明，面向银行业的词嵌入在执行NL任务时可以作为一个良好的独立来源或者作为其他可广泛获得的嵌入的补充。",
    "tldr": "本文研究了面向银行业的词嵌入，发现其比广泛可用的词嵌入更能捕捉银行特定语义和词相关性，因此可作为NL任务的一个良好独立来源或者补充。",
    "en_tdlr": "This paper explores the idea of creating bank-specific word embeddings and finds that they capture bank-specific semantics and word relatedness better than widely available embeddings, suggesting they could be a good standalone source or a complement to other embeddings in performing NLP tasks."
}