{
    "title": "CMLM-CSE: Based on Conditional MLM Contrastive Learning for Sentence Embeddings. (arXiv:2306.09594v1 [cs.CL])",
    "abstract": "Traditional comparative learning sentence embedding directly uses the encoder to extract sentence features, and then passes in the comparative loss function for learning. However, this method pays too much attention to the sentence body and ignores the influence of some words in the sentence on the sentence semantics. To this end, we propose CMLM-CSE, an unsupervised contrastive learning framework based on conditional MLM. On the basis of traditional contrastive learning, an additional auxiliary network is added to integrate sentence embedding to perform MLM tasks, forcing sentence embedding to learn more masked word information. Finally, when Bertbase was used as the pretraining language model, we exceeded SimCSE by 0.55 percentage points on average in textual similarity tasks, and when Robertabase was used as the pretraining language model, we exceeded SimCSE by 0.3 percentage points on average in textual similarity tasks.",
    "link": "http://arxiv.org/abs/2306.09594",
    "context": "Title: CMLM-CSE: Based on Conditional MLM Contrastive Learning for Sentence Embeddings. (arXiv:2306.09594v1 [cs.CL])\nAbstract: Traditional comparative learning sentence embedding directly uses the encoder to extract sentence features, and then passes in the comparative loss function for learning. However, this method pays too much attention to the sentence body and ignores the influence of some words in the sentence on the sentence semantics. To this end, we propose CMLM-CSE, an unsupervised contrastive learning framework based on conditional MLM. On the basis of traditional contrastive learning, an additional auxiliary network is added to integrate sentence embedding to perform MLM tasks, forcing sentence embedding to learn more masked word information. Finally, when Bertbase was used as the pretraining language model, we exceeded SimCSE by 0.55 percentage points on average in textual similarity tasks, and when Robertabase was used as the pretraining language model, we exceeded SimCSE by 0.3 percentage points on average in textual similarity tasks.",
    "path": "papers/23/06/2306.09594.json",
    "total_tokens": 868,
    "translated_title": "基于条件MLM对比学习的句子嵌入技术CMLM-CSE",
    "translated_abstract": "传统的比较学习句子嵌入技术直接使用编码器提取句子特征，然后通过比较损失函数进行学习。然而，这种方法过于关注句子主体，而忽略了句子中一些词对句子语义的影响。为此，我们提出了CMLM-CSE，这是一种基于条件MLM的无监督对比学习框架。在传统对比学习的基础上，增加一个附加的网络来集成句子嵌入以执行MLM任务，强制句子嵌入学习更多的掩码词信息。最后，当使用Bertbase作为预训练语言模型时，我们在文本相似度任务中比SimCSE高0.55个百分点，在使用Robertabase作为预训练语言模型时，在文本相似度任务中平均超过SimCSE 0.3个百分点。",
    "tldr": "本论文提出了一种基于条件MLM的无监督对比学习框架CMLM-CSE，强制句子嵌入学习更多的掩码词信息，可以在文本相似度任务中超越SimCSE。",
    "en_tdlr": "This paper proposes an unsupervised contrastive learning framework CMLM-CSE based on conditional MLM, which forces the sentence embedding to learn more masked word information. It can exceed SimCSE in textual similarity tasks."
}