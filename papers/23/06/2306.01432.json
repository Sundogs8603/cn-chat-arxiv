{
    "title": "Audio-Visual Speech Enhancement with Score-Based Generative Models. (arXiv:2306.01432v1 [eess.AS])",
    "abstract": "This paper introduces an audio-visual speech enhancement system that leverages score-based generative models, also known as diffusion models, conditioned on visual information. In particular, we exploit audio-visual embeddings obtained from a self-super\\-vised learning model that has been fine-tuned on lipreading. The layer-wise features of its transformer-based encoder are aggregated, time-aligned, and incorporated into the noise conditional score network. Experimental evaluations show that the proposed audio-visual speech enhancement system yields improved speech quality and reduces generative artifacts such as phonetic confusions with respect to the audio-only equivalent. The latter is supported by the word error rate of a downstream automatic speech recognition model, which decreases noticeably, especially at low input signal-to-noise ratios.",
    "link": "http://arxiv.org/abs/2306.01432",
    "context": "Title: Audio-Visual Speech Enhancement with Score-Based Generative Models. (arXiv:2306.01432v1 [eess.AS])\nAbstract: This paper introduces an audio-visual speech enhancement system that leverages score-based generative models, also known as diffusion models, conditioned on visual information. In particular, we exploit audio-visual embeddings obtained from a self-super\\-vised learning model that has been fine-tuned on lipreading. The layer-wise features of its transformer-based encoder are aggregated, time-aligned, and incorporated into the noise conditional score network. Experimental evaluations show that the proposed audio-visual speech enhancement system yields improved speech quality and reduces generative artifacts such as phonetic confusions with respect to the audio-only equivalent. The latter is supported by the word error rate of a downstream automatic speech recognition model, which decreases noticeably, especially at low input signal-to-noise ratios.",
    "path": "papers/23/06/2306.01432.json",
    "total_tokens": 786,
    "translated_title": "基于得分的生成模型的音视频语音增强",
    "translated_abstract": "本文介绍了一种利用基于得分的生成模型进行音视频语音增强的系统，该模型以视觉信息为条件。我们利用了一个自我监督学习模型从中获得的音视频嵌入，该模型已经在口型识别上进行了微调。它的基于变压器的编码器的逐层特征被聚合、时间对齐并融合到噪声条件分值网络中。实验评估表明，所提出的音视频语音增强系统在语音质量方面具有改进作用，并减少了与仅音频等效的生成画面中的语音混淆。这得到了下游自动语音识别模型的单词错误率的支持，特别是在低信噪比输入时，单词错误率明显降低。",
    "tldr": "本文介绍了一种利用基于得分的生成模型，以视觉信息为条件的音视频语音增强系统。实验证明该系统提高了语音质量，并减少了语音混淆。",
    "en_tdlr": "This paper presents an audio-visual speech enhancement system that utilizes score-based generative models conditioned on visual information, and demonstrates improved speech quality and reduced confusion with a downstream automatic speech recognition model, especially at low input signal-to-noise ratios."
}