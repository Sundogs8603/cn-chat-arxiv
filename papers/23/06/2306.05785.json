{
    "title": "End-to-End Neural Network Compression via $\\frac{\\ell_1}{\\ell_2}$ Regularized Latency Surrogates. (arXiv:2306.05785v1 [cs.LG])",
    "abstract": "Neural network (NN) compression via techniques such as pruning, quantization requires setting compression hyperparameters (e.g., number of channels to be pruned, bitwidths for quantization) for each layer either manually or via neural architecture search (NAS) which can be computationally expensive. We address this problem by providing an end-to-end technique that optimizes for model's Floating Point Operations (FLOPs) or for on-device latency via a novel $\\frac{\\ell_1}{\\ell_2}$ latency surrogate. Our algorithm is versatile and can be used with many popular compression methods including pruning, low-rank factorization, and quantization. Crucially, it is fast and runs in almost the same amount of time as single model training; which is a significant training speed-up over standard NAS methods. For BERT compression on GLUE fine-tuning tasks, we achieve $50\\%$ reduction in FLOPs with only $1\\%$ drop in performance. For compressing MobileNetV3 on ImageNet-1K, we achieve $15\\%$ reduction in",
    "link": "http://arxiv.org/abs/2306.05785",
    "context": "Title: End-to-End Neural Network Compression via $\\frac{\\ell_1}{\\ell_2}$ Regularized Latency Surrogates. (arXiv:2306.05785v1 [cs.LG])\nAbstract: Neural network (NN) compression via techniques such as pruning, quantization requires setting compression hyperparameters (e.g., number of channels to be pruned, bitwidths for quantization) for each layer either manually or via neural architecture search (NAS) which can be computationally expensive. We address this problem by providing an end-to-end technique that optimizes for model's Floating Point Operations (FLOPs) or for on-device latency via a novel $\\frac{\\ell_1}{\\ell_2}$ latency surrogate. Our algorithm is versatile and can be used with many popular compression methods including pruning, low-rank factorization, and quantization. Crucially, it is fast and runs in almost the same amount of time as single model training; which is a significant training speed-up over standard NAS methods. For BERT compression on GLUE fine-tuning tasks, we achieve $50\\%$ reduction in FLOPs with only $1\\%$ drop in performance. For compressing MobileNetV3 on ImageNet-1K, we achieve $15\\%$ reduction in",
    "path": "papers/23/06/2306.05785.json",
    "total_tokens": 1136,
    "translated_title": "通过$\\frac{\\ell_1}{\\ell_2}$正则化延迟代理进行端到端神经网络压缩",
    "translated_abstract": "神经网络（NN）的压缩通常需要手动或通过神经网络结构搜索（NAS）设置每个层的压缩超参数（例如，要剪枝的通道数，量化的位宽），这可能会耗费大量计算资源。我们提供了一种端到端技术，通过新颖的$\\frac{\\ell_1}{\\ell_2}$延迟代理优化模型的浮点运算量（FLOPs）或设备延迟来解决此问题。我们的算法非常灵活，可以与许多常用的压缩方法（包括剪枝、低秩分解和量化）一起使用。关键是，它非常快速，几乎与单模型训练相同的时间，这比标准NAS方法可节省大量的训练时间。在GLUE微调任务的BERT压缩中，我们的算法可以使FLOPs降低50％，仅损失1％的性能。而在ImageNet-1K上对MobileNetV3进行压缩，则可以使FLOPs降低15％，同时性能基本不损失。",
    "tldr": "提出了一种端到端的神经网络压缩技术，通过优化模型的浮点运算量或设备延迟来自动设置压缩超参数。算法速度快，并且可以与多种常用压缩方法一起使用，如剪枝、低秩分解和量化。在GLUE微调任务的BERT压缩中，FLOPs可降低50％，且性能仅下降1％；而在ImageNet-1K上对MobileNetV3进行压缩，则可以使FLOPs降低15％，同时性能基本不损失。",
    "en_tdlr": "Proposed an end-to-end neural network compression technique that automatically sets compression hyperparameters by optimizing the model's FLOPs or device latency with a novel $\\frac{\\ell_1}{\\ell_2}$ latency surrogate. The algorithm is versatile, fast, and can be used with popular compression methods such as pruning, low-rank factorization, and quantization. Achieved significant FLOPs reduction while maintaining performance, such as 50% reduction with only 1% performance drop in BERT compression on GLUE fine-tuning tasks."
}