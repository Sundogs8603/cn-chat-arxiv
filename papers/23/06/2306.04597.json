{
    "title": "Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions. (arXiv:2306.04597v1 [cs.CL])",
    "abstract": "Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 de-biased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, our few-shot debiasing approach is highly feasible and practical. Through extensive experimentation,",
    "link": "http://arxiv.org/abs/2306.04597",
    "context": "Title: Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions. (arXiv:2306.04597v1 [cs.CL])\nAbstract: Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 de-biased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, our few-shot debiasing approach is highly feasible and practical. Through extensive experimentation,",
    "path": "papers/23/06/2306.04597.json",
    "total_tokens": 1129,
    "translated_title": "语言模型性别去偏置：少样本数据干预方法降低性别偏见",
    "translated_abstract": "目前主流的去偏置技术大多集中在改变训练过程，而我们提出了一种数据干预策略，该策略是一种强大而简单的技术，可以减少预训练模型中的性别偏见。我们经验证明，只要导入10个去偏置的训练数据，即可显著减少任何性别的偏好倾向。由于我们的方法只需要少量的训练样本，因此是高度可行和实用的。通过大量实验，我们证明了该方法的有效性。",
    "tldr": "该论文提出了一种数据干预策略，可以通过极少的训练数据减少预训练模型中的性别偏差，有效降低对任何性别的偏好倾向。",
    "en_tdlr": "This paper proposes a data intervention strategy that can reduce gender bias in pre-trained models with only a few biased data interventions, effectively reducing preferences for any gender. Through extensive experimentation, the method has been proven to be highly feasible and practical."
}