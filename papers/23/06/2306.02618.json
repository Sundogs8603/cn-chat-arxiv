{
    "title": "Enhance Diffusion to Improve Robust Generalization. (arXiv:2306.02618v2 [cs.LG] UPDATED)",
    "abstract": "Deep neural networks are susceptible to human imperceptible adversarial perturbations. One of the strongest defense mechanisms is \\emph{Adversarial Training} (AT). In this paper, we aim to address two predominant problems in AT. First, there is still little consensus on how to set hyperparameters with a performance guarantee for AT research, and customized settings impede a fair comparison between different model designs in AT research. Second, the robustly trained neural networks struggle to generalize well and suffer from tremendous overfitting. This paper focuses on the primary AT framework - Projected Gradient Descent Adversarial Training (PGD-AT). We approximate the dynamic of PGD-AT by a continuous-time Stochastic Differential Equation (SDE), and show that the diffusion term of this SDE determines the robust generalization. An immediate implication of this theoretical finding is that robust generalization is positively correlated with the ratio between learning rate and batch siz",
    "link": "http://arxiv.org/abs/2306.02618",
    "context": "Title: Enhance Diffusion to Improve Robust Generalization. (arXiv:2306.02618v2 [cs.LG] UPDATED)\nAbstract: Deep neural networks are susceptible to human imperceptible adversarial perturbations. One of the strongest defense mechanisms is \\emph{Adversarial Training} (AT). In this paper, we aim to address two predominant problems in AT. First, there is still little consensus on how to set hyperparameters with a performance guarantee for AT research, and customized settings impede a fair comparison between different model designs in AT research. Second, the robustly trained neural networks struggle to generalize well and suffer from tremendous overfitting. This paper focuses on the primary AT framework - Projected Gradient Descent Adversarial Training (PGD-AT). We approximate the dynamic of PGD-AT by a continuous-time Stochastic Differential Equation (SDE), and show that the diffusion term of this SDE determines the robust generalization. An immediate implication of this theoretical finding is that robust generalization is positively correlated with the ratio between learning rate and batch siz",
    "path": "papers/23/06/2306.02618.json",
    "total_tokens": 897,
    "translated_title": "提高扩散以改善鲁棒性泛化",
    "translated_abstract": "深度神经网络容易受到人类难以察觉的对抗性扰动的影响。其中一种最强的防御机制是对抗性训练（AT）。本文旨在解决AT中的两个主要问题。首先，在AT研究中如何设置具有性能保证的超参数仍然存在争议，定制化设置妨碍不同模型设计在AT研究中的公平比较。其次，经过鲁棒训练的神经网络在泛化时面临困难，并且受到严重的过拟合问题的困扰。本文聚焦于主要的AT框架 - 投影梯度下降对抗性训练（PGD-AT）。我们通过连续时间随机微分方程（SDE）近似PGD-AT的动态，并展示了该SDE的扩散项决定了鲁棒泛化。该理论发现的一个直接推论是，鲁棒泛化与学习率和批次大小之比呈正相关关系。",
    "tldr": "本文通过连续时间随机微分方程的研究，发现对抗性训练中的扩散项决定了神经网络的鲁棒泛化能力，进而提出了一种改进的AT框架。",
    "en_tdlr": "This paper investigates the diffusion term in adversarial training, and proposes an improved framework based on the findings from analyzing a continuous-time stochastic differential equation."
}