{
    "title": "The Devil is in the Details: On the Pitfalls of Event Extraction Evaluation. (arXiv:2306.06918v2 [cs.CL] UPDATED)",
    "abstract": "Event extraction (EE) is a crucial task aiming at extracting events from texts, which includes two subtasks: event detection (ED) and event argument extraction (EAE). In this paper, we check the reliability of EE evaluations and identify three major pitfalls: (1) The data preprocessing discrepancy makes the evaluation results on the same dataset not directly comparable, but the data preprocessing details are not widely noted and specified in papers. (2) The output space discrepancy of different model paradigms makes different-paradigm EE models lack grounds for comparison and also leads to unclear mapping issues between predictions and annotations. (3) The absence of pipeline evaluation of many EAE-only works makes them hard to be directly compared with EE works and may not well reflect the model performance in real-world pipeline scenarios. We demonstrate the significant influence of these pitfalls through comprehensive meta-analyses of recent papers and empirical experiments. To avoi",
    "link": "http://arxiv.org/abs/2306.06918",
    "context": "Title: The Devil is in the Details: On the Pitfalls of Event Extraction Evaluation. (arXiv:2306.06918v2 [cs.CL] UPDATED)\nAbstract: Event extraction (EE) is a crucial task aiming at extracting events from texts, which includes two subtasks: event detection (ED) and event argument extraction (EAE). In this paper, we check the reliability of EE evaluations and identify three major pitfalls: (1) The data preprocessing discrepancy makes the evaluation results on the same dataset not directly comparable, but the data preprocessing details are not widely noted and specified in papers. (2) The output space discrepancy of different model paradigms makes different-paradigm EE models lack grounds for comparison and also leads to unclear mapping issues between predictions and annotations. (3) The absence of pipeline evaluation of many EAE-only works makes them hard to be directly compared with EE works and may not well reflect the model performance in real-world pipeline scenarios. We demonstrate the significant influence of these pitfalls through comprehensive meta-analyses of recent papers and empirical experiments. To avoi",
    "path": "papers/23/06/2306.06918.json",
    "total_tokens": 894,
    "translated_abstract": "事件提取（EE）是提取文本中的事件的关键任务，包括两个子任务：事件检测（ED）和事件论元提取（EAE）。本文检查EE评估的可靠性并确定了3个主要陷阱：（1）数据预处理的差异使得同一数据集上的评估结果不直接可比，但数据预处理细节在论文中并未得到广泛的注明和详细说明。（2）不同模型范例的输出空间差异使得不同的EE模型缺乏比较的依据，也导致预测结果与注释之间的映射问题不明确。（3）许多仅限于EAE的工作缺乏管道评估，使它们难以直接与EE工作进行比较，并且可能无法很好地反映模型在实际管道场景中的性能。我们通过最近论文的综合元分析和实证实验展示了这些陷阱的重要影响。",
    "tldr": "本文探讨事件提取（EE）评估的可靠性以及通过综合元分析和实验确定的三个主要陷阱：数据预处理的差异、不同模型范例的输出空间差异和缺乏管道评估。",
    "en_tdlr": "This paper explores the reliability of event extraction (EE) evaluation and identifies three major pitfalls through comprehensive meta-analyses of recent papers and empirical experiments: data preprocessing discrepancies, output space discrepancies of different model paradigms, and a lack of pipeline evaluation."
}