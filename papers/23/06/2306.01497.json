{
    "title": "Data-Efficient French Language Modeling with CamemBERTa. (arXiv:2306.01497v1 [cs.CL])",
    "abstract": "Recent advances in NLP have significantly improved the performance of language models on a variety of tasks. While these advances are largely driven by the availability of large amounts of data and computational power, they also benefit from the development of better training methods and architectures. In this paper, we introduce CamemBERTa, a French DeBERTa model that builds upon the DeBERTaV3 architecture and training objective. We evaluate our model's performance on a variety of French downstream tasks and datasets, including question answering, part-of-speech tagging, dependency parsing, named entity recognition, and the FLUE benchmark, and compare against CamemBERT, the state-of-the-art monolingual model for French. Our results show that, given the same amount of training tokens, our model outperforms BERT-based models trained with MLM on most tasks. Furthermore, our new model reaches similar or superior performance on downstream tasks compared to CamemBERT, despite being trained ",
    "link": "http://arxiv.org/abs/2306.01497",
    "context": "Title: Data-Efficient French Language Modeling with CamemBERTa. (arXiv:2306.01497v1 [cs.CL])\nAbstract: Recent advances in NLP have significantly improved the performance of language models on a variety of tasks. While these advances are largely driven by the availability of large amounts of data and computational power, they also benefit from the development of better training methods and architectures. In this paper, we introduce CamemBERTa, a French DeBERTa model that builds upon the DeBERTaV3 architecture and training objective. We evaluate our model's performance on a variety of French downstream tasks and datasets, including question answering, part-of-speech tagging, dependency parsing, named entity recognition, and the FLUE benchmark, and compare against CamemBERT, the state-of-the-art monolingual model for French. Our results show that, given the same amount of training tokens, our model outperforms BERT-based models trained with MLM on most tasks. Furthermore, our new model reaches similar or superior performance on downstream tasks compared to CamemBERT, despite being trained ",
    "path": "papers/23/06/2306.01497.json",
    "total_tokens": 1196,
    "translated_title": "使用CamemBERTa实现高效的法语语言建模",
    "translated_abstract": "最新的自然语言处理领域的研究大大提高了语言模型在多种任务上的表现。虽然这些进展很大程度上是由于大量的数据和计算能力的可用性所推动，但它们也受益于更好的训练方法和架构的开发。本文介绍了一种CamemBERTa模型，它是一种法语DeBERTa模型，基于DeBERTaV3的架构和训练目标。作者在多个法语下游任务和数据集上评估了我们模型的性能，包括问答、词性标注、依存解析、命名实体识别和FLUE基准，并与法语最先进的单语模型CamemBERT进行了比较。结果显示，给定相同数量的训练令牌，本模型在大多数任务中优于以MLM训练的BERT-based模型。此外，本模型在下游任务中达到了与CamemBERT相似或更优的性能，尽管它是新模型。",
    "tldr": "本文介绍了一种名为CamemBERTa的法语DeBERTa模型，相对于使用MLM训练的BERT-based模型，在相同数量的训练令牌下性能更加优秀，在多项法语下游任务中已经达到甚至超过了CamemBERT。",
    "en_tdlr": "This paper introduces a French DeBERTa model called CamemBERTa, which outperforms BERT-based models trained with MLM on most tasks given the same amount of training tokens. Evaluations on various French downstream tasks and datasets show that CamemBERTa reaches similar or superior performance compared to the state-of-the-art monolingual model for French, CamemBERT."
}