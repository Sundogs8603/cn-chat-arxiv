{
    "title": "Transfer Learning from Pre-trained Language Models Improves End-to-End Speech Summarization. (arXiv:2306.04233v1 [cs.CL])",
    "abstract": "End-to-end speech summarization (E2E SSum) directly summarizes input speech into easy-to-read short sentences with a single model. This approach is promising because it, in contrast to the conventional cascade approach, can utilize full acoustical information and mitigate to the propagation of transcription errors. However, due to the high cost of collecting speech-summary pairs, an E2E SSum model tends to suffer from training data scarcity and output unnatural sentences. To overcome this drawback, we propose for the first time to integrate a pre-trained language model (LM), which is highly capable of generating natural sentences, into the E2E SSum decoder via transfer learning. In addition, to reduce the gap between the independently pre-trained encoder and decoder, we also propose to transfer the baseline E2E SSum encoder instead of the commonly used automatic speech recognition encoder. Experimental results show that the proposed model outperforms baseline and data augmented models.",
    "link": "http://arxiv.org/abs/2306.04233",
    "context": "Title: Transfer Learning from Pre-trained Language Models Improves End-to-End Speech Summarization. (arXiv:2306.04233v1 [cs.CL])\nAbstract: End-to-end speech summarization (E2E SSum) directly summarizes input speech into easy-to-read short sentences with a single model. This approach is promising because it, in contrast to the conventional cascade approach, can utilize full acoustical information and mitigate to the propagation of transcription errors. However, due to the high cost of collecting speech-summary pairs, an E2E SSum model tends to suffer from training data scarcity and output unnatural sentences. To overcome this drawback, we propose for the first time to integrate a pre-trained language model (LM), which is highly capable of generating natural sentences, into the E2E SSum decoder via transfer learning. In addition, to reduce the gap between the independently pre-trained encoder and decoder, we also propose to transfer the baseline E2E SSum encoder instead of the commonly used automatic speech recognition encoder. Experimental results show that the proposed model outperforms baseline and data augmented models.",
    "path": "papers/23/06/2306.04233.json",
    "total_tokens": 932,
    "translated_title": "预训练语言模型的迁移学习提升了端到端语音摘要。",
    "translated_abstract": "端到端语音摘要(E2E SSum)通过单个模型直接将输入语音总结为易于阅读的短句。这种方法很有前途，因为相较于传统的级联方法，它可以利用全部声学信息并减轻转录错误的影响。然而，由于收集语音-摘要对的成本较高，E2E SSum模型往往会因训练数据稀缺且输出句子不自然而受到影响。为了克服这种缺点，我们首次提出将预训练语言模型(LM)通过迁移学习整合到E2E SSum解码器中，LM能够生成自然句子的能力很强。此外，为了减少独立预训练的编码器和解码器之间的差距，我们还提出了迁移基线E2E SSum编码器而不是通常使用的自动语音识别编码器。实验结果表明，所提出的模型优于基线和数据增强模型。",
    "tldr": "本文为了克服端到端语音摘要模型训练数据稀缺和不自然输出句子的问题，首次提出通过迁移学习将预训练语言模型整合到解码器中，其实验结果优于基线和数据增强模型。"
}