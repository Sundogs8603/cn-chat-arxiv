{
    "title": "Hyperparameters in Reinforcement Learning and How To Tune Them. (arXiv:2306.01324v1 [cs.LG])",
    "abstract": "In order to improve reproducibility, deep reinforcement learning (RL) has been adopting better scientific practices such as standardized evaluation metrics and reporting. However, the process of hyperparameter optimization still varies widely across papers, which makes it challenging to compare RL algorithms fairly. In this paper, we show that hyperparameter choices in RL can significantly affect the agent's final performance and sample efficiency, and that the hyperparameter landscape can strongly depend on the tuning seed which may lead to overfitting. We therefore propose adopting established best practices from AutoML, such as the separation of tuning and testing seeds, as well as principled hyperparameter optimization (HPO) across a broad search space. We support this by comparing multiple state-of-the-art HPO tools on a range of RL algorithms and environments to their hand-tuned counterparts, demonstrating that HPO approaches often have higher performance and lower compute overhe",
    "link": "http://arxiv.org/abs/2306.01324",
    "context": "Title: Hyperparameters in Reinforcement Learning and How To Tune Them. (arXiv:2306.01324v1 [cs.LG])\nAbstract: In order to improve reproducibility, deep reinforcement learning (RL) has been adopting better scientific practices such as standardized evaluation metrics and reporting. However, the process of hyperparameter optimization still varies widely across papers, which makes it challenging to compare RL algorithms fairly. In this paper, we show that hyperparameter choices in RL can significantly affect the agent's final performance and sample efficiency, and that the hyperparameter landscape can strongly depend on the tuning seed which may lead to overfitting. We therefore propose adopting established best practices from AutoML, such as the separation of tuning and testing seeds, as well as principled hyperparameter optimization (HPO) across a broad search space. We support this by comparing multiple state-of-the-art HPO tools on a range of RL algorithms and environments to their hand-tuned counterparts, demonstrating that HPO approaches often have higher performance and lower compute overhe",
    "path": "papers/23/06/2306.01324.json",
    "total_tokens": 844,
    "translated_title": "强化学习中的超参数及其调整方法",
    "translated_abstract": "为了提高可重复性，深度强化学习 (RL) 已经采用了更好的科学实践，如标准化评估指标和报告。然而，RL 中的超参数优化过程仍然存在巨大差异，这使得公平比较 RL 算法变得具有挑战性。本文展示了 RL 中的超参数选择可以显着影响代理的最终性能和采样效率，并且超参数的选择会强烈依赖于调整种子，可能导致过度拟合。因此，我们提出采用 AutoML 中已有的最佳实践，例如分离调整和测试种子，以及在广泛搜索空间内进行合理的超参数优化 (HPO)。通过比较多个最新的 HPO 工具在一系列 RL 算法和环境上与手动调整对比，展示了 HPO 方法往往具有更高的性能和更低的计算开销。",
    "tldr": "本文研究了强化学习中的超参数选择问题，提出了采用AutoML中的最佳实践，并展示了采用HPO方法往往具有更高的性能和更低的计算开销。",
    "en_tdlr": "This paper studies the hyperparameter selection problem in reinforcement learning, proposes adopting best practices from AutoML, and demonstrates that using HPO approaches often leads to higher performance and lower computation cost."
}