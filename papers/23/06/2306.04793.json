{
    "title": "On the Joint Interaction of Models, Data, and Features. (arXiv:2306.04793v1 [cs.LG])",
    "abstract": "Learning features from data is one of the defining characteristics of deep learning, but our theoretical understanding of the role features play in deep learning is still rudimentary. To address this gap, we introduce a new tool, the interaction tensor, for empirically analyzing the interaction between data and model through features. With the interaction tensor, we make several key observations about how features are distributed in data and how models with different random seeds learn different features. Based on these observations, we propose a conceptual framework for feature learning. Under this framework, the expected accuracy for a single hypothesis and agreement for a pair of hypotheses can both be derived in closed-form. We demonstrate that the proposed framework can explain empirically observed phenomena, including the recently discovered Generalization Disagreement Equality (GDE) that allows for estimating the generalization error with only unlabeled data. Further, our theory",
    "link": "http://arxiv.org/abs/2306.04793",
    "context": "Title: On the Joint Interaction of Models, Data, and Features. (arXiv:2306.04793v1 [cs.LG])\nAbstract: Learning features from data is one of the defining characteristics of deep learning, but our theoretical understanding of the role features play in deep learning is still rudimentary. To address this gap, we introduce a new tool, the interaction tensor, for empirically analyzing the interaction between data and model through features. With the interaction tensor, we make several key observations about how features are distributed in data and how models with different random seeds learn different features. Based on these observations, we propose a conceptual framework for feature learning. Under this framework, the expected accuracy for a single hypothesis and agreement for a pair of hypotheses can both be derived in closed-form. We demonstrate that the proposed framework can explain empirically observed phenomena, including the recently discovered Generalization Disagreement Equality (GDE) that allows for estimating the generalization error with only unlabeled data. Further, our theory",
    "path": "papers/23/06/2306.04793.json",
    "total_tokens": 985,
    "translated_title": "论模型、数据和特征的联合交互",
    "translated_abstract": "从数据中学习特征是深度学习的一个定义性特征，但是我们对于特征在深度学习中所起的作用的理解仍然很有限。为了弥补这一空白，我们引入了一种新工具，交互张量，用于通过特征对模型和数据之间的交互进行经验分析。通过交互张量，我们对特征在数据中的分布以及不同随机种子的模型学习不同特征等方面做出了几个关键观察。基于这些观察结果，我们提出了一个特征学习的概念框架。在这个框架下，可以通过闭式形式推导出单个假设的期望准确率和一对假设的一致性。我们证明了所提出的框架可以解释一些经验观察现象，包括最近发现的广义化不同等式（GDE），它可以使用仅无标签数据来估计泛化误差。此外，我们的理论表明，深度学习可能受益于探索传统IID（独立同分布）假设之外的数据。",
    "tldr": "本文提出了一种新工具--交互张量用于通过特征对模型和数据之间的交互进行经验分析，并提出了一个特征学习的概念框架，可以解释一些经验观察现象，表明深度学习可能受益于探索传统IID（独立同分布）假设之外的数据。",
    "en_tdlr": "This paper proposes a new tool, the interaction tensor, to empirically analyze the interaction between data and model through features, and proposes a conceptual framework for feature learning which explains some empirical phenomenon and suggests that exploring non-IID data may benefit deep learning."
}