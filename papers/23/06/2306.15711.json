{
    "title": "Semi-supervised Multimodal Representation Learning through a Global Workspace. (arXiv:2306.15711v1 [cs.AI])",
    "abstract": "Recent deep learning models can efficiently combine inputs from different modalities (e.g., images and text) and learn to align their latent representations, or to translate signals from one domain to another (as in image captioning, or text-to-image generation). However, current approaches mainly rely on brute-force supervised training over large multimodal datasets. In contrast, humans (and other animals) can learn useful multimodal representations from only sparse experience with matched cross-modal data. Here we evaluate the capabilities of a neural network architecture inspired by the cognitive notion of a \"Global Workspace\": a shared representation for two (or more) input modalities. Each modality is processed by a specialized system (pretrained on unimodal data, and subsequently frozen). The corresponding latent representations are then encoded to and decoded from a single shared workspace. Importantly, this architecture is amenable to self-supervised training via cycle-consiste",
    "link": "http://arxiv.org/abs/2306.15711",
    "context": "Title: Semi-supervised Multimodal Representation Learning through a Global Workspace. (arXiv:2306.15711v1 [cs.AI])\nAbstract: Recent deep learning models can efficiently combine inputs from different modalities (e.g., images and text) and learn to align their latent representations, or to translate signals from one domain to another (as in image captioning, or text-to-image generation). However, current approaches mainly rely on brute-force supervised training over large multimodal datasets. In contrast, humans (and other animals) can learn useful multimodal representations from only sparse experience with matched cross-modal data. Here we evaluate the capabilities of a neural network architecture inspired by the cognitive notion of a \"Global Workspace\": a shared representation for two (or more) input modalities. Each modality is processed by a specialized system (pretrained on unimodal data, and subsequently frozen). The corresponding latent representations are then encoded to and decoded from a single shared workspace. Importantly, this architecture is amenable to self-supervised training via cycle-consiste",
    "path": "papers/23/06/2306.15711.json",
    "total_tokens": 962,
    "translated_title": "通过全局工作区的半监督多模态表示学习",
    "translated_abstract": "最近的深度学习模型可以有效地组合不同的输入模态（例如图像和文本）并学习对其潜在表示进行对齐，或者将一个领域的信号转化为另一个领域的信号（例如图像字幕生成或者文本到图像生成）。然而，当前的方法主要依赖于对大型多模态数据集进行暴力监督训练。相比之下，人类（和其他动物）可以通过匹配的跨模态数据的稀疏经验来学习有用的多模态表示。在这里，我们评估了一个受认知概念“全局工作区”启发的神经网络架构的能力：一个共享的两个（或多个）输入模态的表示。每个模态都经过一个专门的系统处理（在单模态数据上预训练，并随后冻结）。相应的潜在表示然后被编码到一个共享的工作区并从中解码。重要的是，这种架构适用于通过循环一致性自我监督训练。",
    "tldr": "本研究通过创建一个共享的两个或多个输入模态的表示的神经网络架构，实现了半监督的多模态表示学习。这种架构可以通过循环一致性自我监督训练。这种方法可以减少对大型多模态数据集的依赖，并模仿人类从有限的经验中学习有用的多模态表示的能力。"
}