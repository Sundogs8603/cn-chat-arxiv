{
    "title": "Sharper Bounds for $\\ell_p$ Sensitivity Sampling. (arXiv:2306.00732v1 [cs.DS])",
    "abstract": "In large scale machine learning, random sampling is a popular way to approximate datasets by a small representative subset of examples. In particular, sensitivity sampling is an intensely studied technique which provides provable guarantees on the quality of approximation, while reducing the number of examples to the product of the VC dimension $d$ and the total sensitivity $\\mathfrak S$ in remarkably general settings. However, guarantees going beyond this general bound of $\\mathfrak S d$ are known in perhaps only one setting, for $\\ell_2$ subspace embeddings, despite intense study of sensitivity sampling in prior work. In this work, we show the first bounds for sensitivity sampling for $\\ell_p$ subspace embeddings for $p\\neq 2$ that improve over the general $\\mathfrak S d$ bound, achieving a bound of roughly $\\mathfrak S^{2/p}$ for $1\\leq p<2$ and $\\mathfrak S^{2-2/p}$ for $2<p<\\infty$. For $1\\leq p<2$, we show that this bound is tight, in the sense that there exist matrices for which",
    "link": "http://arxiv.org/abs/2306.00732",
    "context": "Title: Sharper Bounds for $\\ell_p$ Sensitivity Sampling. (arXiv:2306.00732v1 [cs.DS])\nAbstract: In large scale machine learning, random sampling is a popular way to approximate datasets by a small representative subset of examples. In particular, sensitivity sampling is an intensely studied technique which provides provable guarantees on the quality of approximation, while reducing the number of examples to the product of the VC dimension $d$ and the total sensitivity $\\mathfrak S$ in remarkably general settings. However, guarantees going beyond this general bound of $\\mathfrak S d$ are known in perhaps only one setting, for $\\ell_2$ subspace embeddings, despite intense study of sensitivity sampling in prior work. In this work, we show the first bounds for sensitivity sampling for $\\ell_p$ subspace embeddings for $p\\neq 2$ that improve over the general $\\mathfrak S d$ bound, achieving a bound of roughly $\\mathfrak S^{2/p}$ for $1\\leq p<2$ and $\\mathfrak S^{2-2/p}$ for $2<p<\\infty$. For $1\\leq p<2$, we show that this bound is tight, in the sense that there exist matrices for which",
    "path": "papers/23/06/2306.00732.json",
    "total_tokens": 1136,
    "translated_title": "$\\ell_p$灵敏度采样的更严格界限",
    "translated_abstract": "在大规模机器学习中，随机采样是一种近似数据集的流行方式，这种方式可以通过一小部分具有代表性的示例来进行。特别地，灵敏度采样是一种强烈研究的技术，它在极其普遍的情况下提供可证明的近似质量保证，同时将示例的数量减少到VC维$d$和总灵敏度$\\mathfrak{S}$的乘积。然而，除了$\\ell_2$子空间嵌入以外，很少有保证超过这个$\\mathfrak{S}d$通用界限的知识，尽管以前的工作非常强调灵敏度采样。在这项工作中，我们首次展示了对于$ p\\neq2$的$\\ell_p$子空间嵌入的灵敏度采样界限，这些界限超过了一般的$\\mathfrak{S}d$界限，对于$1\\leq p<2$，我们取得了大约$\\mathfrak{S}^{2/p}$的界限，并且对于$2<p<\\infty$，取得了$\\mathfrak{S}^{2-2/p}$的界限。在$1\\leq p<2$的情况下，我们表明这个边界是密切相关的。",
    "tldr": "该论文研究了$\\ell_p$子空间嵌入的灵敏度采样界限，取得了比通用界限更好的结果，对于$1\\leq p<2$的情况下，界限达到了$\\mathfrak{S}^{2/p}$，对于$2<p<\\infty$的情况下，界限达到了$\\mathfrak{S}^{2-2/p}$。"
}