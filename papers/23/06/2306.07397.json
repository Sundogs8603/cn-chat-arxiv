{
    "title": "Adversarial Attacks on the Interpretation of Neuron Activation Maximization. (arXiv:2306.07397v1 [cs.LG])",
    "abstract": "The internal functional behavior of trained Deep Neural Networks is notoriously difficult to interpret. Activation-maximization approaches are one set of techniques used to interpret and analyze trained deep-learning models. These consist in finding inputs that maximally activate a given neuron or feature map. These inputs can be selected from a data set or obtained by optimization. However, interpretability methods may be subject to being deceived. In this work, we consider the concept of an adversary manipulating a model for the purpose of deceiving the interpretation. We propose an optimization framework for performing this manipulation and demonstrate a number of ways that popular activation-maximization interpretation techniques associated with CNNs can be manipulated to change the interpretations, shedding light on the reliability of these methods.",
    "link": "http://arxiv.org/abs/2306.07397",
    "context": "Title: Adversarial Attacks on the Interpretation of Neuron Activation Maximization. (arXiv:2306.07397v1 [cs.LG])\nAbstract: The internal functional behavior of trained Deep Neural Networks is notoriously difficult to interpret. Activation-maximization approaches are one set of techniques used to interpret and analyze trained deep-learning models. These consist in finding inputs that maximally activate a given neuron or feature map. These inputs can be selected from a data set or obtained by optimization. However, interpretability methods may be subject to being deceived. In this work, we consider the concept of an adversary manipulating a model for the purpose of deceiving the interpretation. We propose an optimization framework for performing this manipulation and demonstrate a number of ways that popular activation-maximization interpretation techniques associated with CNNs can be manipulated to change the interpretations, shedding light on the reliability of these methods.",
    "path": "papers/23/06/2306.07397.json",
    "total_tokens": 828,
    "translated_title": "对神经元激活最大化解释的对抗攻击",
    "translated_abstract": "训练好的深度神经网络的内部功能行为非常难以解释。激活最大化方法是用于解释和分析训练深度学习模型的一组技术。这些方法包括找到最大化给定的神经元或特征映射激活的输入。这些输入可以选择自数据集或通过优化得到。然而，可解释性方法可能会受到欺骗。在这项工作中，我们考虑了对手为欺骗解释而操纵模型的概念。我们提出了一个用于执行这种操作的优化框架，并演示了几种流行的与卷积神经网络相关的激活最大化解释技术可以被操纵以改变解释，从而揭示了这些方法的可靠性。",
    "tldr": "本文研究了对神经网络内部功能行为解释方法的对抗攻击，提出了一个操纵模型以干扰解释的优化框架，并演示了流行的激活最大化解释技术被操纵以改变其解释的方法。",
    "en_tdlr": "This paper investigates adversarial attacks on interpretation methods for the internal functional behavior of deep neural networks. The authors propose an optimization framework for manipulating the model to interfere with interpretation, and demonstrate ways in which popular activation-maximization interpretation techniques associated with CNNs can be manipulated to change their interpretations, shedding light on the reliability of these methods."
}