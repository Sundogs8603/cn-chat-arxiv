{
    "title": "Finding Counterfactually Optimal Action Sequences in Continuous State Spaces. (arXiv:2306.03929v1 [cs.LG])",
    "abstract": "Humans performing tasks that involve taking a series of multiple dependent actions over time often learn from experience by reflecting on specific cases and points in time, where different actions could have led to significantly better outcomes. While recent machine learning methods to retrospectively analyze sequential decision making processes promise to aid decision makers in identifying such cases, they have focused on environments with finitely many discrete states. However, in many practical applications, the state of the environment is inherently continuous in nature. In this paper, we aim to fill this gap. We start by formally characterizing a sequence of discrete actions and continuous states using finite horizon Markov decision processes and a broad class of bijective structural causal models. Building upon this characterization, we formalize the problem of finding counterfactually optimal action sequences and show that, in general, we cannot expect to solve it in polynomial ",
    "link": "http://arxiv.org/abs/2306.03929",
    "context": "Title: Finding Counterfactually Optimal Action Sequences in Continuous State Spaces. (arXiv:2306.03929v1 [cs.LG])\nAbstract: Humans performing tasks that involve taking a series of multiple dependent actions over time often learn from experience by reflecting on specific cases and points in time, where different actions could have led to significantly better outcomes. While recent machine learning methods to retrospectively analyze sequential decision making processes promise to aid decision makers in identifying such cases, they have focused on environments with finitely many discrete states. However, in many practical applications, the state of the environment is inherently continuous in nature. In this paper, we aim to fill this gap. We start by formally characterizing a sequence of discrete actions and continuous states using finite horizon Markov decision processes and a broad class of bijective structural causal models. Building upon this characterization, we formalize the problem of finding counterfactually optimal action sequences and show that, in general, we cannot expect to solve it in polynomial ",
    "path": "papers/23/06/2306.03929.json",
    "total_tokens": 829,
    "tldr": "本文研究如何在具有连续状态空间的环境中寻找反事实最优行动序列。",
    "en_tdlr": "This paper explores ways to find counterfactually optimal action sequences in environments with continuous state spaces, which is a challenge as existing machine learning methods have focused on discrete states."
}