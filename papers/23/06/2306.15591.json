{
    "title": "Learning to Sail Dynamic Networks: The MARLIN Reinforcement Learning Framework for Congestion Control in Tactical Environments. (arXiv:2306.15591v1 [cs.LG])",
    "abstract": "Conventional Congestion Control (CC) algorithms,such as TCP Cubic, struggle in tactical environments as they misinterpret packet loss and fluctuating network performance as congestion symptoms. Recent efforts, including our own MARLIN, have explored the use of Reinforcement Learning (RL) for CC, but they often fall short of generalization, particularly in competitive, unstable, and unforeseen scenarios. To address these challenges, this paper proposes an RL framework that leverages an accurate and parallelizable emulation environment to reenact the conditions of a tactical network. We also introduce refined RL formulation and performance evaluation methods tailored for agents operating in such intricate scenarios. We evaluate our RL learning framework by training a MARLIN agent in conditions replicating a bottleneck link transition between a Satellite Communication (SATCOM) and an UHF Wide Band (UHF) radio link. Finally, we compared its performance in file transfer tasks against Transm",
    "link": "http://arxiv.org/abs/2306.15591",
    "context": "Title: Learning to Sail Dynamic Networks: The MARLIN Reinforcement Learning Framework for Congestion Control in Tactical Environments. (arXiv:2306.15591v1 [cs.LG])\nAbstract: Conventional Congestion Control (CC) algorithms,such as TCP Cubic, struggle in tactical environments as they misinterpret packet loss and fluctuating network performance as congestion symptoms. Recent efforts, including our own MARLIN, have explored the use of Reinforcement Learning (RL) for CC, but they often fall short of generalization, particularly in competitive, unstable, and unforeseen scenarios. To address these challenges, this paper proposes an RL framework that leverages an accurate and parallelizable emulation environment to reenact the conditions of a tactical network. We also introduce refined RL formulation and performance evaluation methods tailored for agents operating in such intricate scenarios. We evaluate our RL learning framework by training a MARLIN agent in conditions replicating a bottleneck link transition between a Satellite Communication (SATCOM) and an UHF Wide Band (UHF) radio link. Finally, we compared its performance in file transfer tasks against Transm",
    "path": "papers/23/06/2306.15591.json",
    "total_tokens": 1059,
    "translated_title": "学习在动态网络中航行：MARLIN强化学习框架在战术环境中的拥塞控制",
    "translated_abstract": "传统的拥塞控制算法，如TCP Cubic，在战术环境中遇到困难，因为它们错误地将丢包和网络性能波动视为拥塞症状。最近的努力，包括我们自己的MARLIN，已经探索了使用强化学习（RL）进行拥塞控制，但在竞争激烈、不稳定和意外情况下往往不能很好地泛化。为了解决这些挑战，本文提出了一个RL框架，利用准确且可并行化的仿真环境重现战术网络的条件。我们还引入了适用于在这种复杂场景中运行的代理的精细化RL公式和性能评估方法。我们通过训练一个在卫星通信（SATCOM）和UHF宽带（UHF）无线电链路之间模拟瓶颈链路转换条件的MARLIN智能体来评估我们的RL学习框架。最后，我们将其在文件传输任务中与Transm的性能进行了比较。",
    "tldr": "本论文提出了一个强化学习框架MARLIN，用于在战术环境中进行拥塞控制。该框架利用准确且可并行化的仿真环境来模拟战术网络条件，并引入了适用于复杂场景中的代理的精细化强化学习公式和评估方法。通过训练MARLIN智能体在模拟条件下进行瓶颈链路转换的性能评估，我们证明了该框架的有效性。"
}