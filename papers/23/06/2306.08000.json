{
    "title": "Improving Zero-Shot Detection of Low Prevalence Chest Pathologies using Domain Pre-trained Language Models. (arXiv:2306.08000v1 [physics.med-ph])",
    "abstract": "Recent advances in zero-shot learning have enabled the use of paired image-text data to replace structured labels, replacing the need for expert annotated datasets. Models such as CLIP-based CheXzero utilize these advancements in the domain of chest X-ray interpretation. We hypothesize that domain pre-trained models such as CXR-BERT, BlueBERT, and ClinicalBERT offer the potential to improve the performance of CLIP-like models with specific domain knowledge by replacing BERT weights at the cost of breaking the original model's alignment. We evaluate the performance of zero-shot classification models with domain-specific pre-training for detecting low-prevalence pathologies. Even though replacing the weights of the original CLIP-BERT degrades model performance on commonly found pathologies, we show that pre-trained text towers perform exceptionally better on low-prevalence diseases. This motivates future ensemble models with a combination of differently trained language models for maxima",
    "link": "http://arxiv.org/abs/2306.08000",
    "context": "Title: Improving Zero-Shot Detection of Low Prevalence Chest Pathologies using Domain Pre-trained Language Models. (arXiv:2306.08000v1 [physics.med-ph])\nAbstract: Recent advances in zero-shot learning have enabled the use of paired image-text data to replace structured labels, replacing the need for expert annotated datasets. Models such as CLIP-based CheXzero utilize these advancements in the domain of chest X-ray interpretation. We hypothesize that domain pre-trained models such as CXR-BERT, BlueBERT, and ClinicalBERT offer the potential to improve the performance of CLIP-like models with specific domain knowledge by replacing BERT weights at the cost of breaking the original model's alignment. We evaluate the performance of zero-shot classification models with domain-specific pre-training for detecting low-prevalence pathologies. Even though replacing the weights of the original CLIP-BERT degrades model performance on commonly found pathologies, we show that pre-trained text towers perform exceptionally better on low-prevalence diseases. This motivates future ensemble models with a combination of differently trained language models for maxima",
    "path": "papers/23/06/2306.08000.json",
    "total_tokens": 1052,
    "translated_title": "利用领域预训练语言模型改进低患病率胸部病症的零样本检测",
    "translated_abstract": "零样本学习的最新进展使得可以利用成对的图像识别标签数据替代结构化标签，消除了对专家注释数据集的需求。像CLIP-based CheXzero这样的模型利用了这些在胸部X射线解释领域的进步。我们假设，使用CX-BERT、BlueBERT和ClinicalBERT等领域预训练模型，通过替换BERT权重来增加特定领域知识，有可能提高类似于CLIP的模型的性能，但代价是打破原始模型的对齐性。我们评估了具有特定领域预训练的零样本分类模型在检测低患病率病理方面的性能。尽管替换原始CLIP-BERT权重会降低模型在常见病理方面的性能，但我们发现预训练文本塔在低患病率疾病的检测中表现出色。这激发了未来使用不同训练语言模型组合的集成模型的可能性。",
    "tldr": "该论文研究了如何利用领域预训练语言模型CX-BERT、BlueBERT和ClinicalBERT提高CLIP-like模型对低患病率胸部病症的零样本检测。实验结果表明，预训练的文本塔对于低患病率疾病的检测有显著的性能提升。这提示了未来可使用不同训练语言模型的集成模型进行进一步研究。"
}