{
    "title": "FLamE: Few-shot Learning from Natural Language Explanations. (arXiv:2306.08042v1 [cs.CL])",
    "abstract": "Natural language explanations have the potential to provide rich information that in principle guides model reasoning. Yet, recent work by Lampinen et al. (2022) has shown limited utility of natural language explanations in improving classification. To effectively learn from explanations, we present FLamE, a two-stage few-shot learning framework that first generates explanations using GPT-3, and then finetunes a smaller model (e.g., RoBERTa) with generated explanations. Our experiments on natural language inference demonstrate effectiveness over strong baselines, increasing accuracy by 17.6% over GPT-3 Babbage and 5.7% over GPT-3 Davinci in e-SNLI. Despite improving classification performance, human evaluation surprisingly reveals that the majority of generated explanations does not adequately justify classification decisions. Additional analyses point to the important role of label-specific cues (e.g., \"not know\" for the neutral label) in generated explanations.",
    "link": "http://arxiv.org/abs/2306.08042",
    "context": "Title: FLamE: Few-shot Learning from Natural Language Explanations. (arXiv:2306.08042v1 [cs.CL])\nAbstract: Natural language explanations have the potential to provide rich information that in principle guides model reasoning. Yet, recent work by Lampinen et al. (2022) has shown limited utility of natural language explanations in improving classification. To effectively learn from explanations, we present FLamE, a two-stage few-shot learning framework that first generates explanations using GPT-3, and then finetunes a smaller model (e.g., RoBERTa) with generated explanations. Our experiments on natural language inference demonstrate effectiveness over strong baselines, increasing accuracy by 17.6% over GPT-3 Babbage and 5.7% over GPT-3 Davinci in e-SNLI. Despite improving classification performance, human evaluation surprisingly reveals that the majority of generated explanations does not adequately justify classification decisions. Additional analyses point to the important role of label-specific cues (e.g., \"not know\" for the neutral label) in generated explanations.",
    "path": "papers/23/06/2306.08042.json",
    "total_tokens": 923,
    "translated_title": "FLamE: 自然语言解释下的少样本学习",
    "translated_abstract": "自然语言解释在理论上具有为模型推理提供丰富信息的潜力。然而，Lampinen等人的最新研究表明，自然语言解释在提高分类方面的效用有限。为了有效地从解释中学习，我们提出了FLamE，这是一个两阶段的少样本学习框架，首先使用GPT-3生成解释，然后使用生成的解释对较小的模型（例如RoBERTa）进行微调。自然语言推理实验表明，与强基线相比，FLamE具有很好的效果，在e-SNLI数据集上的准确性比GPT-3 Babbage提高了17.6％，比GPT-3 Davinci提高了5.7％。尽管提高了分类性能，但是人类评估出人生成的大部分解释都不能充分地证明分类决策。额外的分析表明，标签特定线索（如中立标签的“不知道”）在生成解释中发挥了重要作用。",
    "tldr": "FLamE是一个利用GPT-3生成自然语言解释用于RoBERTa微调的少样本学习框架，在自然语言推理任务上展现出较好的性能，但是人生成的解释大多不能充分地证明分类决策，标签特定线索在解释中起着重要作用。",
    "en_tdlr": "FLamE is a few-shot learning framework that generates natural language explanations using GPT-3 for RoBERTa fine-tuning, demonstrating good performance on natural language inference tasks. However, most of the generated explanations cannot adequately justify classification decisions, with label-specific cues playing an important role in explanations."
}