{
    "title": "Beyond Surface Statistics: Scene Representations in a Latent Diffusion Model. (arXiv:2306.05720v1 [cs.CV])",
    "abstract": "Latent diffusion models (LDMs) exhibit an impressive ability to produce realistic images, yet the inner workings of these models remain mysterious. Even when trained purely on images without explicit depth information, they typically output coherent pictures of 3D scenes. In this work, we investigate a basic interpretability question: does an LDM create and use an internal representation of simple scene geometry? Using linear probes, we find evidence that the internal activations of the LDM encode linear representations of both 3D depth data and a salient-object / background distinction. These representations appear surprisingly early in the denoising process$-$well before a human can easily make sense of the noisy images. Intervention experiments further indicate these representations play a causal role in image synthesis, and may be used for simple high-level editing of an LDM's output.",
    "link": "http://arxiv.org/abs/2306.05720",
    "context": "Title: Beyond Surface Statistics: Scene Representations in a Latent Diffusion Model. (arXiv:2306.05720v1 [cs.CV])\nAbstract: Latent diffusion models (LDMs) exhibit an impressive ability to produce realistic images, yet the inner workings of these models remain mysterious. Even when trained purely on images without explicit depth information, they typically output coherent pictures of 3D scenes. In this work, we investigate a basic interpretability question: does an LDM create and use an internal representation of simple scene geometry? Using linear probes, we find evidence that the internal activations of the LDM encode linear representations of both 3D depth data and a salient-object / background distinction. These representations appear surprisingly early in the denoising process$-$well before a human can easily make sense of the noisy images. Intervention experiments further indicate these representations play a causal role in image synthesis, and may be used for simple high-level editing of an LDM's output.",
    "path": "papers/23/06/2306.05720.json",
    "total_tokens": 913,
    "translated_title": "超越表面统计学：潜在扩散模型中的场景表示",
    "translated_abstract": "潜在扩散模型（LDM）展现了产生逼真图像的惊人能力，但这些模型的内在机制仍然神秘。即使在没有显式深度信息的图像上进行训练，它们通常也会输出一致的3D场景图片。在这项工作中，我们探讨了一个基本的可解释性问题：LDM是否创建并使用一个简单的场景几何内部表示？使用线性探针，我们发现LDM的内部激活编码了线性表示，既包括3D深度数据，又包括突出对象/背景区别。这些表示似乎在去噪过程的早期就出现了——在人类能轻易理解嘈杂的图像之前。干预性实验进一步表明，这些表示在图像合成中扮演因果作用，并且可能用于LDM输出的简单高级编辑。",
    "tldr": "本文探究了潜在扩散模型内部是否创建和使用简单场景几何内部表示，使用线性探针发现LDM内部激活提供了关于3D深度数据和突出对象/背景分离的线性表示，在图像合成中具有因果作用, 可用于LDM输出的简单高级编辑。",
    "en_tdlr": "The article explores whether LDM creates and uses a simple internal representation of scene geometry and finds that the internal activations of LDM encode linear representations of both 3D depth data and a salient-object/background distinction. The representations appear early in the denoising process and play a causal role in image synthesis and can be used for simple high-level editing of LDM's output."
}