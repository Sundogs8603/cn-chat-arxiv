{
    "title": "Efficient Quantization-aware Training with Adaptive Coreset Selection. (arXiv:2306.07215v2 [cs.LG] UPDATED)",
    "abstract": "The expanding model size and computation of deep neural networks (DNNs) have increased the demand for efficient model deployment methods. Quantization-aware training (QAT) is a representative model compression method to leverage redundancy in weights and activations. However, most existing QAT methods require end-to-end training on the entire dataset, which suffers from long training time and high energy costs. Coreset selection, aiming to improve data efficiency utilizing the redundancy of training data, has also been widely used for efficient training. In this work, we propose a new angle through the coreset selection to improve the training efficiency of quantization-aware training. Based on the characteristics of QAT, we propose two metrics: error vector score and disagreement score, to quantify the importance of each sample during training. Guided by these two metrics of importance, we proposed a quantization-aware adaptive coreset selection (ACS) method to select the data for the",
    "link": "http://arxiv.org/abs/2306.07215",
    "context": "Title: Efficient Quantization-aware Training with Adaptive Coreset Selection. (arXiv:2306.07215v2 [cs.LG] UPDATED)\nAbstract: The expanding model size and computation of deep neural networks (DNNs) have increased the demand for efficient model deployment methods. Quantization-aware training (QAT) is a representative model compression method to leverage redundancy in weights and activations. However, most existing QAT methods require end-to-end training on the entire dataset, which suffers from long training time and high energy costs. Coreset selection, aiming to improve data efficiency utilizing the redundancy of training data, has also been widely used for efficient training. In this work, we propose a new angle through the coreset selection to improve the training efficiency of quantization-aware training. Based on the characteristics of QAT, we propose two metrics: error vector score and disagreement score, to quantify the importance of each sample during training. Guided by these two metrics of importance, we proposed a quantization-aware adaptive coreset selection (ACS) method to select the data for the",
    "path": "papers/23/06/2306.07215.json",
    "total_tokens": 868,
    "translated_title": "高效的量化感知训练与自适应核心集选择",
    "translated_abstract": "深度神经网络（DNN）的模型大小和计算量的增加，增加了对有效模型部署方法的需求。量化感知训练（QAT）是一种代表性的模型压缩方法，可以利用权重和激活中的冗余信息。然而，大多数现有的QAT方法需要在整个数据集上进行端到端训练，这会导致长时间的训练和高能耗。核心集选择是利用训练数据的冗余性提高数据效率的方法，在高效训练中被广泛应用。在这项工作中，我们提出了一种新的角度，通过核心集选择来提高量化感知训练的训练效率。基于QAT的特性，我们提出了两个指标：误差向量分数和不一致分数，用于量化训练过程中每个样本的重要性。基于这两个重要性指标，我们提出了一种量化感知的自适应核心集选择（ACS）方法，用于选择训练数据的子集。",
    "tldr": "本研究提出了一种用于改善量化感知训练的训练效率的方法，通过核心集选择和两个重要性指标来选择训练数据的子集。",
    "en_tdlr": "This work proposes a method to improve the training efficiency of quantization-aware training by using coreset selection and two importance metrics to select a subset of training data."
}