{
    "title": "Designing Stable Neural Networks using Convex Analysis and ODEs. (arXiv:2306.17332v1 [cs.LG])",
    "abstract": "Motivated by classical work on the numerical integration of ordinary differential equations we present a ResNet-styled neural network architecture that encodes non-expansive (1-Lipschitz) operators, as long as the spectral norms of the weights are appropriately constrained. This is to be contrasted with the ordinary ResNet architecture which, even if the spectral norms of the weights are constrained, has a Lipschitz constant that, in the worst case, grows exponentially with the depth of the network. Further analysis of the proposed architecture shows that the spectral norms of the weights can be further constrained to ensure that the network is an averaged operator, making it a natural candidate for a learned denoiser in Plug-and-Play algorithms. Using a novel adaptive way of enforcing the spectral norm constraints, we show that, even with these constraints, it is possible to train performant networks. The proposed architecture is applied to the problem of adversarially robust image cl",
    "link": "http://arxiv.org/abs/2306.17332",
    "context": "Title: Designing Stable Neural Networks using Convex Analysis and ODEs. (arXiv:2306.17332v1 [cs.LG])\nAbstract: Motivated by classical work on the numerical integration of ordinary differential equations we present a ResNet-styled neural network architecture that encodes non-expansive (1-Lipschitz) operators, as long as the spectral norms of the weights are appropriately constrained. This is to be contrasted with the ordinary ResNet architecture which, even if the spectral norms of the weights are constrained, has a Lipschitz constant that, in the worst case, grows exponentially with the depth of the network. Further analysis of the proposed architecture shows that the spectral norms of the weights can be further constrained to ensure that the network is an averaged operator, making it a natural candidate for a learned denoiser in Plug-and-Play algorithms. Using a novel adaptive way of enforcing the spectral norm constraints, we show that, even with these constraints, it is possible to train performant networks. The proposed architecture is applied to the problem of adversarially robust image cl",
    "path": "papers/23/06/2306.17332.json",
    "total_tokens": 891,
    "translated_title": "使用凸分析和ODE设计稳定的神经网络",
    "translated_abstract": "创造了一种基于ResNet风格的神经网络架构，该架构编码非扩张（1-Lipschitz）算子，只要权重的谱范数受到适当的约束。与传统的ResNet架构相比，即使权重的谱范数受到约束，其Lipschitz常数在最坏情况下也会随网络的深度呈指数级增长。进一步分析表明，可以进一步约束权重的谱范数，以确保网络是一个平均算子，使其成为Plug-and-Play算法中的学习去噪器的自然候选。使用一种新颖的自适应方式来强制谱范数约束，我们表明即使在这些约束条件下，也可以训练出性能优越的网络。所提出的架构应用于对抗性稳健图像分类问题。",
    "tldr": "通过使用凸分析和ODE，设计了一种稳定的神经网络架构，该架构编码非扩张算子，并能够通过约束权重的谱范数来限制Lipschitz常数的增长。此架构还可以被应用于学习去噪器，并通过一种自适应的方式来保证性能优越。"
}