{
    "title": "MeLM, a generative pretrained language modeling framework that solves forward and inverse mechanics problems. (arXiv:2306.17525v1 [cond-mat.mtrl-sci] CROSS LISTED)",
    "abstract": "We report a flexible multi-modal mechanics language model, MeLM, applied to solve various nonlinear forward and inverse problems, that can deal with a set of instructions, numbers and microstructure data. The framework is applied to various examples including bio-inspired hierarchical honeycomb design, carbon nanotube mechanics, and protein unfolding. In spite of the flexible nature of the model-which allows us to easily incorporate diverse materials, scales, and mechanical features-it performs well across disparate forward and inverse tasks. Based on an autoregressive attention-model, MeLM effectively represents a large multi-particle system consisting of hundreds of millions of neurons, where the interaction potentials are discovered through graph-forming self-attention mechanisms that are then used to identify relationships from emergent structures, while taking advantage of synergies discovered in the training data. We show that the model can solve complex degenerate mechanics desi",
    "link": "http://arxiv.org/abs/2306.17525",
    "context": "Title: MeLM, a generative pretrained language modeling framework that solves forward and inverse mechanics problems. (arXiv:2306.17525v1 [cond-mat.mtrl-sci] CROSS LISTED)\nAbstract: We report a flexible multi-modal mechanics language model, MeLM, applied to solve various nonlinear forward and inverse problems, that can deal with a set of instructions, numbers and microstructure data. The framework is applied to various examples including bio-inspired hierarchical honeycomb design, carbon nanotube mechanics, and protein unfolding. In spite of the flexible nature of the model-which allows us to easily incorporate diverse materials, scales, and mechanical features-it performs well across disparate forward and inverse tasks. Based on an autoregressive attention-model, MeLM effectively represents a large multi-particle system consisting of hundreds of millions of neurons, where the interaction potentials are discovered through graph-forming self-attention mechanisms that are then used to identify relationships from emergent structures, while taking advantage of synergies discovered in the training data. We show that the model can solve complex degenerate mechanics desi",
    "path": "papers/23/06/2306.17525.json",
    "total_tokens": 803,
    "translated_title": "MeLM：一个解决正向和逆向力学问题的生成式预训练语言建模框架",
    "translated_abstract": "我们报道了一个灵活的多模式力学语言模型MeLM，应用于解决各种非线性的正向和逆向问题，可以处理一组指令、数字和微结构数据。该框架应用于各个示例，包括仿生分层蜂窝设计、碳纳米管力学和蛋白质展开。尽管模型具有灵活性，能够轻松地融入不同材料、尺度和力学特征，但它在不同的正向和逆向任务中表现出色。基于自回归注意力模型，MeLM有效地表示了一个由数亿个神经元组成的庞大多粒子系统，通过形成图形的自注意机制发现相互作用势能，然后利用在训练数据中发现的协同效应来识别出现结构之间的关系。我们展示了该模型可以解决复杂的退化力学设计问题。",
    "tldr": "MeLM是一个灵活的多模式力学语言模型，利用自回归注意力机制解决正向和逆向力学问题，表现出色。"
}