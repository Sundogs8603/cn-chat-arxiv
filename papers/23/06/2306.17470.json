{
    "title": "An Oblivious Stochastic Composite Optimization Algorithm for Eigenvalue Optimization Problems. (arXiv:2306.17470v1 [math.OC])",
    "abstract": "In this work, we revisit the problem of solving large-scale semidefinite programs using randomized first-order methods and stochastic smoothing. We introduce two oblivious stochastic mirror descent algorithms based on a complementary composite setting. One algorithm is designed for non-smooth objectives, while an accelerated version is tailored for smooth objectives. Remarkably, both algorithms work without prior knowledge of the Lipschitz constant or smoothness of the objective function. For the non-smooth case with $\\mathcal{M}-$bounded oracles, we prove a convergence rate of $ O( {\\mathcal{M}}/{\\sqrt{T}} ) $. For the $L$-smooth case with a feasible set bounded by $D$, we derive a convergence rate of $ O( {L^2 D^2}/{(T^{2}\\sqrt{T})} + {(D_0^2+\\sigma^2)}/{\\sqrt{T}} )$, where $D_0$ is the starting distance to an optimal solution, and $ \\sigma^2$ is the stochastic oracle variance. These rates had only been obtained so far by either assuming prior knowledge of the Lipschitz constant or t",
    "link": "http://arxiv.org/abs/2306.17470",
    "context": "Title: An Oblivious Stochastic Composite Optimization Algorithm for Eigenvalue Optimization Problems. (arXiv:2306.17470v1 [math.OC])\nAbstract: In this work, we revisit the problem of solving large-scale semidefinite programs using randomized first-order methods and stochastic smoothing. We introduce two oblivious stochastic mirror descent algorithms based on a complementary composite setting. One algorithm is designed for non-smooth objectives, while an accelerated version is tailored for smooth objectives. Remarkably, both algorithms work without prior knowledge of the Lipschitz constant or smoothness of the objective function. For the non-smooth case with $\\mathcal{M}-$bounded oracles, we prove a convergence rate of $ O( {\\mathcal{M}}/{\\sqrt{T}} ) $. For the $L$-smooth case with a feasible set bounded by $D$, we derive a convergence rate of $ O( {L^2 D^2}/{(T^{2}\\sqrt{T})} + {(D_0^2+\\sigma^2)}/{\\sqrt{T}} )$, where $D_0$ is the starting distance to an optimal solution, and $ \\sigma^2$ is the stochastic oracle variance. These rates had only been obtained so far by either assuming prior knowledge of the Lipschitz constant or t",
    "path": "papers/23/06/2306.17470.json",
    "total_tokens": 972,
    "translated_title": "一种用于特征值优化问题的无视觉随机复合优化算法",
    "translated_abstract": "在这项工作中，我们重新审视了使用随机化一阶方法和随机平滑解决大规模半定规划问题的问题。我们引入了两种基于互补复合设置的无视觉随机镜像下降算法。一种算法设计用于非光滑目标，而加速版本则适用于光滑目标。值得注意的是，这两种算法都不需要对目标函数的Lipschitz常数或光滑度有先验知识。对于具有$\\mathcal{M}-$有界预言的非光滑情况，我们证明了一个收敛速度为$ O( {\\mathcal{M}}/{\\sqrt{T}} ) $的收敛速度。对于具有由$D$限制的可行集的$L$-光滑情况，我们得到了一个收敛速度为$ O( {L^2 D^2}/{(T^{2}\\sqrt{T})} + {(D_0^2+\\sigma^2)}/{\\sqrt{T}} )$的收敛速度，其中$D_0$是到最优解的起始距离，$ \\sigma^2$是随机预言方差。目前只有在假设先验知识的Lipschitz常数或t情况下才能得到这些速度。",
    "tldr": "本论文提出了两种针对非光滑和光滑目标的无视觉随机镜像下降算法，不需要先验知识，并给出了相应收敛速度。",
    "en_tdlr": "This paper proposes two oblivious stochastic mirror descent algorithms for non-smooth and smooth objectives, without the need for prior knowledge, and provides corresponding convergence rates."
}