{
    "title": "Cross-Language Speech Emotion Recognition Using Multimodal Dual Attention Transformers. (arXiv:2306.13804v1 [cs.CL])",
    "abstract": "Despite the recent progress in speech emotion recognition (SER), state-of-the-art systems are unable to achieve improved performance in cross-language settings. In this paper, we propose a Multimodal Dual Attention Transformer (MDAT) model to improve cross-language SER. Our model utilises pre-trained models for multimodal feature extraction and is equipped with a dual attention mechanism including graph attention and co-attention to capture complex dependencies across different modalities and achieve improved cross-language SER results using minimal target language data. In addition, our model also exploits a transformer encoder layer for high-level feature representation to improve emotion classification accuracy. In this way, MDAT performs refinement of feature representation at various stages and provides emotional salient features to the classification layer. This novel approach also ensures the preservation of modality-specific emotional information while enhancing cross-modality ",
    "link": "http://arxiv.org/abs/2306.13804",
    "context": "Title: Cross-Language Speech Emotion Recognition Using Multimodal Dual Attention Transformers. (arXiv:2306.13804v1 [cs.CL])\nAbstract: Despite the recent progress in speech emotion recognition (SER), state-of-the-art systems are unable to achieve improved performance in cross-language settings. In this paper, we propose a Multimodal Dual Attention Transformer (MDAT) model to improve cross-language SER. Our model utilises pre-trained models for multimodal feature extraction and is equipped with a dual attention mechanism including graph attention and co-attention to capture complex dependencies across different modalities and achieve improved cross-language SER results using minimal target language data. In addition, our model also exploits a transformer encoder layer for high-level feature representation to improve emotion classification accuracy. In this way, MDAT performs refinement of feature representation at various stages and provides emotional salient features to the classification layer. This novel approach also ensures the preservation of modality-specific emotional information while enhancing cross-modality ",
    "path": "papers/23/06/2306.13804.json",
    "total_tokens": 878,
    "translated_title": "多模态双重注意力变换器实现跨语音情感识别",
    "translated_abstract": "尽管语音情感识别（SER）取得了近期的进展，但最先进的系统无法在跨语言环境中实现改进的性能。本文提出了一种多模态双重注意力变换器（MDAT）模型，以改进跨语言SER。我们的模型利用预训练模型进行多模态特征提取，并配备双重注意机制，包括图形注意和共同关注，以捕获不同模态之间的复杂依赖关系，并使用最少的目标语言数据实现改进的跨语言SER结果。此外，我们的模型还利用变换器编码器层进行高层特征表示，以提高情感分类准确性。MDAT在各个阶段执行特征表示的细化，并为分类层提供情感显着特征。这种新颖方法还确保了模态特定的情感信息的保存，同时增强了交叉模态。",
    "tldr": "提出一种多模态双重注意力变换器（MDAT）模型，利用预训练模型进行多模态特征提取，通过引入图形注意和共同关注机制来捕捉不同情感的跨模态依赖，并使用最少的目标语言数据实现改进的跨语言情感识别结果。",
    "en_tdlr": "A Multimodal Dual Attention Transformer (MDAT) model is proposed to improve cross-language speech emotion recognition, which utilizes pre-trained models for multimodal feature extraction and introduces graph attention and co-attention to capture cross-modal dependencies, achieving improved results with minimal target language data."
}