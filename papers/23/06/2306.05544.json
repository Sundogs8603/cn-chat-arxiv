{
    "title": "BOOT: Data-free Distillation of Denoising Diffusion Models with Bootstrapping. (arXiv:2306.05544v1 [cs.CV])",
    "abstract": "Diffusion models have demonstrated excellent potential for generating diverse images. However, their performance often suffers from slow generation due to iterative denoising. Knowledge distillation has been recently proposed as a remedy that can reduce the number of inference steps to one or a few without significant quality degradation. However, existing distillation methods either require significant amounts of offline computation for generating synthetic training data from the teacher model or need to perform expensive online learning with the help of real data. In this work, we present a novel technique called BOOT, that overcomes these limitations with an efficient data-free distillation algorithm. The core idea is to learn a time-conditioned model that predicts the output of a pre-trained diffusion model teacher given any time step. Such a model can be efficiently trained based on bootstrapping from two consecutive sampled steps. Furthermore, our method can be easily adapted to ",
    "link": "http://arxiv.org/abs/2306.05544",
    "context": "Title: BOOT: Data-free Distillation of Denoising Diffusion Models with Bootstrapping. (arXiv:2306.05544v1 [cs.CV])\nAbstract: Diffusion models have demonstrated excellent potential for generating diverse images. However, their performance often suffers from slow generation due to iterative denoising. Knowledge distillation has been recently proposed as a remedy that can reduce the number of inference steps to one or a few without significant quality degradation. However, existing distillation methods either require significant amounts of offline computation for generating synthetic training data from the teacher model or need to perform expensive online learning with the help of real data. In this work, we present a novel technique called BOOT, that overcomes these limitations with an efficient data-free distillation algorithm. The core idea is to learn a time-conditioned model that predicts the output of a pre-trained diffusion model teacher given any time step. Such a model can be efficiently trained based on bootstrapping from two consecutive sampled steps. Furthermore, our method can be easily adapted to ",
    "path": "papers/23/06/2306.05544.json",
    "total_tokens": 1071,
    "translated_title": "BOOT: 无需数据的差分扩散模型蒸馏方法",
    "translated_abstract": "差分扩散模型在生成多样的图像方面具有潜在的优势。然而，由于迭代去噪的缘故，它们的性能常常受到缓慢的生成速度的影响。知识蒸馏最近被提出作为一种可行的方法，它可以在不显著降低质量的情况下将推理步骤减少到一个或几个步骤。然而，现有的蒸馏方法要么需要在老师模型中生成大量的合成训练数据，要么需要使用真实数据进行昂贵的在线学习。本文提出了一种新的技术BOOT，通过有效的无数据蒸馏算法克服了这些限制。核心思想是学习一个时间条件模型，它可以根据任何时间步长预测一个预训练的差分扩散模型老师的输出。这种模型可以通过两个连续采样步骤的自助法进行有效的训练。此外，我们的方法可以轻松适用于不同的扩散模型，并且不需要任何额外的训练数据或计算资源。在样本质量和多样性方面，我们在几个扩散模型上展示了我们方法的有效性，同时实现了显著的推理时间加速。",
    "tldr": "本文提出了一种名为BOOT的技术，它使用有效的无数据蒸馏算法来训练一个时间条件模型，以预测预训练的差分扩散模型老师的输出。我们的方法可以适用于不同的扩散模型，同时实现了显著的推理时间加速。",
    "en_tdlr": "The paper proposes a novel technique called BOOT for data-free distillation of diffusion models. The method trains a time-conditioned model to predict the output of a pre-trained diffusion model teacher, without any need for additional training data or computational resources. BOOT achieves significant speedup in inference time while maintaining sample quality and diversity."
}