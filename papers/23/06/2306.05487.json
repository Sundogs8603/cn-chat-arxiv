{
    "title": "Boosting with Tempered Exponential Measures. (arXiv:2306.05487v1 [cs.LG])",
    "abstract": "One of the most popular ML algorithms, AdaBoost, can be derived from the dual of a relative entropy minimization problem subject to the fact that the positive weights on the examples sum to one. Essentially, harder examples receive higher probabilities. We generalize this setup to the recently introduced {\\it tempered exponential measure}s (TEMs) where normalization is enforced on a specific power of the measure and not the measure itself. TEMs are indexed by a parameter $t$ and generalize exponential families ($t=1$). Our algorithm, $t$-AdaBoost, recovers AdaBoost~as a special case ($t=1$). We show that $t$-AdaBoost retains AdaBoost's celebrated exponential convergence rate when $t\\in [0,1)$ while allowing a slight improvement of the rate's hidden constant compared to $t=1$. $t$-AdaBoost partially computes on a generalization of classical arithmetic over the reals and brings notable properties like guaranteed bounded leveraging coefficients for $t\\in [0,1)$. From the loss that $t$-Ada",
    "link": "http://arxiv.org/abs/2306.05487",
    "context": "Title: Boosting with Tempered Exponential Measures. (arXiv:2306.05487v1 [cs.LG])\nAbstract: One of the most popular ML algorithms, AdaBoost, can be derived from the dual of a relative entropy minimization problem subject to the fact that the positive weights on the examples sum to one. Essentially, harder examples receive higher probabilities. We generalize this setup to the recently introduced {\\it tempered exponential measure}s (TEMs) where normalization is enforced on a specific power of the measure and not the measure itself. TEMs are indexed by a parameter $t$ and generalize exponential families ($t=1$). Our algorithm, $t$-AdaBoost, recovers AdaBoost~as a special case ($t=1$). We show that $t$-AdaBoost retains AdaBoost's celebrated exponential convergence rate when $t\\in [0,1)$ while allowing a slight improvement of the rate's hidden constant compared to $t=1$. $t$-AdaBoost partially computes on a generalization of classical arithmetic over the reals and brings notable properties like guaranteed bounded leveraging coefficients for $t\\in [0,1)$. From the loss that $t$-Ada",
    "path": "papers/23/06/2306.05487.json",
    "total_tokens": 871,
    "translated_title": "带温度指数测度的增强学习算法",
    "translated_abstract": "本文提出了一种新的机器学习算法——$t$-AdaBoost，它是AdaBoost算法的推广。我们使用以温度参数$t$为索引的温度指数测度（TEM）对损失函数进行建模并证明当$t\\in [0,1)$时，该算法保持了AdaBoost的指数收敛速率。此外，我们还进行了一些实验，将本算法与AdaBoost算法进行比较，并展示了$t$-AdaBoost算法在某些情况下的性能优势。",
    "tldr": "本文提出了一种以温度指数测度为基础的机器学习算法——$t$-AdaBoost，并证明其在$t\\in [0,1)$时可以保持AdaBoost的指数收敛速率，同时具有更好的性能表现。",
    "en_tdlr": "This paper introduces a machine learning algorithm based on tempered exponential measures called $t$-AdaBoost, which is a generalization of AdaBoost. The authors model the loss function using TEMs indexed by a temperature parameter $t$, and prove that $t$-AdaBoost retains the exponential convergence rate of AdaBoost when $t\\in [0,1)$. They also provide experiments comparing the performance of $t$-AdaBoost to AdaBoost, demonstrating its advantages in some cases."
}