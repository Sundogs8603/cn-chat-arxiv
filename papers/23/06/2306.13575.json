{
    "title": "Scaling MLPs: A Tale of Inductive Bias. (arXiv:2306.13575v1 [cs.LG])",
    "abstract": "In this work we revisit the most fundamental building block in deep learning, the multi-layer perceptron (MLP), and study the limits of its performance on vision tasks. Empirical insights into MLPs are important for multiple reasons. (1) Given the recent narrative \"less inductive bias is better\", popularized due to transformers eclipsing convolutional models, it is natural to explore the limits of this hypothesis. To that end, MLPs offer an ideal test bed, being completely free of any inductive bias. (2) MLPs have almost exclusively been the main protagonist in the deep learning theory literature due to their mathematical simplicity, serving as a proxy to explain empirical phenomena observed for more complex architectures. Surprisingly, experimental datapoints for MLPs are very difficult to find in the literature, especially when coupled with large pre-training protocols. This discrepancy between practice and theory is worrying: Do MLPs reflect the empirical advances exhibited by pract",
    "link": "http://arxiv.org/abs/2306.13575",
    "context": "Title: Scaling MLPs: A Tale of Inductive Bias. (arXiv:2306.13575v1 [cs.LG])\nAbstract: In this work we revisit the most fundamental building block in deep learning, the multi-layer perceptron (MLP), and study the limits of its performance on vision tasks. Empirical insights into MLPs are important for multiple reasons. (1) Given the recent narrative \"less inductive bias is better\", popularized due to transformers eclipsing convolutional models, it is natural to explore the limits of this hypothesis. To that end, MLPs offer an ideal test bed, being completely free of any inductive bias. (2) MLPs have almost exclusively been the main protagonist in the deep learning theory literature due to their mathematical simplicity, serving as a proxy to explain empirical phenomena observed for more complex architectures. Surprisingly, experimental datapoints for MLPs are very difficult to find in the literature, especially when coupled with large pre-training protocols. This discrepancy between practice and theory is worrying: Do MLPs reflect the empirical advances exhibited by pract",
    "path": "papers/23/06/2306.13575.json",
    "total_tokens": 678,
    "translated_title": "MLP的规模化：归纳偏差的故事",
    "translated_abstract": "在此工作中，我们重新审视了深度学习中最基本的构建块——多层感知器（MLP），并研究了它在视觉任务中的性能极限。MLP的实验性洞见在多个方面都非常重要。",
    "tldr": "本文研究了多层感知器（MLP）在视觉任务中的性能极限，并探讨了MLP相较于其他深度学习模型的归纳偏差，旨在推进深度学习理论和实践的结合。",
    "en_tdlr": "This paper explores the limits of performance of multi-layer perceptrons (MLPs) on vision tasks and investigates the inductive bias of MLPs compared to other deep learning models, aiming to bridge the gap between deep learning theory and practice."
}