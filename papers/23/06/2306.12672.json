{
    "title": "From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought. (arXiv:2306.12672v1 [cs.CL])",
    "abstract": "How does language inform our downstream thinking? In particular, how do humans make meaning from language -- and how can we leverage a theory of linguistic meaning to build machines that think in more human-like ways? In this paper, we propose \\textit{rational meaning construction}, a computational framework for language-informed thinking that combines neural models of language with probabilistic models for rational inference. We frame linguistic meaning as a context-sensitive mapping from natural language into a \\textit{probabilistic language of thought} (PLoT) -- a general-purpose symbolic substrate for probabilistic, generative world modeling. Our architecture integrates two powerful computational tools that have not previously come together: we model thinking with \\textit{probabilistic programs}, an expressive representation for flexible commonsense reasoning; and we model meaning construction with \\textit{large language models} (LLMs), which support broad-coverage translation from",
    "link": "http://arxiv.org/abs/2306.12672",
    "context": "Title: From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought. (arXiv:2306.12672v1 [cs.CL])\nAbstract: How does language inform our downstream thinking? In particular, how do humans make meaning from language -- and how can we leverage a theory of linguistic meaning to build machines that think in more human-like ways? In this paper, we propose \\textit{rational meaning construction}, a computational framework for language-informed thinking that combines neural models of language with probabilistic models for rational inference. We frame linguistic meaning as a context-sensitive mapping from natural language into a \\textit{probabilistic language of thought} (PLoT) -- a general-purpose symbolic substrate for probabilistic, generative world modeling. Our architecture integrates two powerful computational tools that have not previously come together: we model thinking with \\textit{probabilistic programs}, an expressive representation for flexible commonsense reasoning; and we model meaning construction with \\textit{large language models} (LLMs), which support broad-coverage translation from",
    "path": "papers/23/06/2306.12672.json",
    "total_tokens": 879,
    "tldr": "本文提出了一种名为“合理意义构建”的计算框架，将神经语言模型与概率推理相结合，建立一种普适的符号衬底，用于概率、生成世界建模。"
}