{
    "title": "Systematic Evaluation of GPT-3 for Zero-Shot Personality Estimation. (arXiv:2306.01183v1 [cs.CL])",
    "abstract": "Very large language models (LLMs) perform extremely well on a spectrum of NLP tasks in a zero-shot setting. However, little is known about their performance on human-level NLP problems which rely on understanding psychological concepts, such as assessing personality traits. In this work, we investigate the zero-shot ability of GPT-3 to estimate the Big 5 personality traits from users' social media posts. Through a set of systematic experiments, we find that zero-shot GPT-3 performance is somewhat close to an existing pre-trained SotA for broad classification upon injecting knowledge about the trait in the prompts. However, when prompted to provide fine-grained classification, its performance drops to close to a simple most frequent class (MFC) baseline. We further analyze where GPT-3 performs better, as well as worse, than a pretrained lexical model, illustrating systematic errors that suggest ways to improve LLMs on human-level NLP tasks.",
    "link": "http://arxiv.org/abs/2306.01183",
    "context": "Title: Systematic Evaluation of GPT-3 for Zero-Shot Personality Estimation. (arXiv:2306.01183v1 [cs.CL])\nAbstract: Very large language models (LLMs) perform extremely well on a spectrum of NLP tasks in a zero-shot setting. However, little is known about their performance on human-level NLP problems which rely on understanding psychological concepts, such as assessing personality traits. In this work, we investigate the zero-shot ability of GPT-3 to estimate the Big 5 personality traits from users' social media posts. Through a set of systematic experiments, we find that zero-shot GPT-3 performance is somewhat close to an existing pre-trained SotA for broad classification upon injecting knowledge about the trait in the prompts. However, when prompted to provide fine-grained classification, its performance drops to close to a simple most frequent class (MFC) baseline. We further analyze where GPT-3 performs better, as well as worse, than a pretrained lexical model, illustrating systematic errors that suggest ways to improve LLMs on human-level NLP tasks.",
    "path": "papers/23/06/2306.01183.json",
    "total_tokens": 939,
    "translated_title": "GPT-3对零样本个性估计的系统评估",
    "translated_abstract": "非常大的语言模型在零样本环境下表现出色，但在人类水平的 NLP 问题上，如评估个性特征，它们的表现却鲜有人知。本文通过一系列系统实验，探究了使用 GPT-3 从用户社交媒体帖子中零样本估计 Big 5 个性特质的能力。我们发现，当插入关于特质的知识后，零样本 GPT-3 性能与现有的预训练 SotA 稍有接近。但在被提示提供细粒度分类时，其性能降至接近一个简单的最常见类（MFC）基线。我们进一步分析了 GPT-3 表现比预训练词汇模型更好、更差的地方，揭示了系统误差，提出了改进 NLP 任务中的 LLM 的方法。",
    "tldr": "本文系统研究了使用 GPT-3 从用户社交媒体帖子中零样本估计个性特质的能力。在插入关于特质的知识后，GPT-3 性能接近于现有的预训练 SotA，但在被提示提供细粒度分类时，其性能降至基线水平。",
    "en_tdlr": "This paper systematically investigates the ability of GPT-3 to estimate personality traits from users' social media posts in a zero-shot setting. The performance of zero-shot GPT-3 is close to an existing pre-trained SotA for broad classification with the injection of knowledge about the trait, but drops to a baseline level when prompted to provide fine-grained classification. Systematic errors are also analyzed to suggest ways to improve LLMs on human-level NLP tasks."
}