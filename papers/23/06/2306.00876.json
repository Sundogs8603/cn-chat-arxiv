{
    "title": "Quantifying Deep Learning Model Uncertainty in Conformal Prediction. (arXiv:2306.00876v2 [cs.LG] UPDATED)",
    "abstract": "Precise estimation of predictive uncertainty in deep neural networks is a critical requirement for reliable decision-making in machine learning and statistical modeling, particularly in the context of medical AI. Conformal Prediction (CP) has emerged as a promising framework for representing the model uncertainty by providing well-calibrated confidence levels for individual predictions. However, the quantification of model uncertainty in conformal prediction remains an active research area, yet to be fully addressed. In this paper, we explore state-of-the-art CP methodologies and their theoretical foundations. We propose a probabilistic approach in quantifying the model uncertainty derived from the produced prediction sets in conformal prediction and provide certified boundaries for the computed uncertainty. By doing so, we allow model uncertainty measured by CP to be compared by other uncertainty quantification methods such as Bayesian (e.g., MC-Dropout and DeepEnsemble) and Evidentia",
    "link": "http://arxiv.org/abs/2306.00876",
    "context": "Title: Quantifying Deep Learning Model Uncertainty in Conformal Prediction. (arXiv:2306.00876v2 [cs.LG] UPDATED)\nAbstract: Precise estimation of predictive uncertainty in deep neural networks is a critical requirement for reliable decision-making in machine learning and statistical modeling, particularly in the context of medical AI. Conformal Prediction (CP) has emerged as a promising framework for representing the model uncertainty by providing well-calibrated confidence levels for individual predictions. However, the quantification of model uncertainty in conformal prediction remains an active research area, yet to be fully addressed. In this paper, we explore state-of-the-art CP methodologies and their theoretical foundations. We propose a probabilistic approach in quantifying the model uncertainty derived from the produced prediction sets in conformal prediction and provide certified boundaries for the computed uncertainty. By doing so, we allow model uncertainty measured by CP to be compared by other uncertainty quantification methods such as Bayesian (e.g., MC-Dropout and DeepEnsemble) and Evidentia",
    "path": "papers/23/06/2306.00876.json",
    "total_tokens": 860,
    "translated_title": "在符合性预测中量化深度学习模型的不确定性",
    "translated_abstract": "在机器学习和统计建模中，精确估计深度神经网络的预测不确定性对于可靠的决策至关重要，尤其在医疗人工智能的背景下。符合性预测（CP）已经成为一个有希望的框架，通过提供良好校准的置信水平来表示模型的不确定性以进行单个预测。然而，对于在符合性预测中量化模型不确定性的研究仍然是一个活跃的领域，还没有完全解决。在本文中，我们探讨了最先进的符合性预测方法及其理论基础。我们提出了一个概率方法来量化符合性预测中产生的预测集的模型不确定性，并为计算得到的不确定性提供了认证边界。通过这样做，我们可以将通过CP测量的模型不确定性与其他不确定性量化方法（如贝叶斯方法、MC-Dropout和DeepEnsemble）进行比较.",
    "tldr": "本文针对深度学习模型的不确定性进行了量化，采用了符合性预测框架来计算模型的置信水平，并与其他不确定性量化方法进行了比较。"
}