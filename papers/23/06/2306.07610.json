{
    "title": "Soft Language Clustering for Multilingual Model Pre-training. (arXiv:2306.07610v1 [cs.CL])",
    "abstract": "Multilingual pre-trained language models have demonstrated impressive (zero-shot) cross-lingual transfer abilities, however, their performance is hindered when the target language has distant typology from source languages or when pre-training data is limited in size. In this paper, we propose XLM-P, which contextually retrieves prompts as flexible guidance for encoding instances conditionally. Our XLM-P enables (1) lightweight modeling of language-invariant and language-specific knowledge across languages, and (2) easy integration with other multilingual pre-training methods. On the tasks of XTREME including text classification, sequence labeling, question answering, and sentence retrieval, both base- and large-size language models pre-trained with our proposed method exhibit consistent performance improvement. Furthermore, it provides substantial advantages for low-resource languages in unsupervised sentence retrieval and for target languages that differ greatly from the source langu",
    "link": "http://arxiv.org/abs/2306.07610",
    "context": "Title: Soft Language Clustering for Multilingual Model Pre-training. (arXiv:2306.07610v1 [cs.CL])\nAbstract: Multilingual pre-trained language models have demonstrated impressive (zero-shot) cross-lingual transfer abilities, however, their performance is hindered when the target language has distant typology from source languages or when pre-training data is limited in size. In this paper, we propose XLM-P, which contextually retrieves prompts as flexible guidance for encoding instances conditionally. Our XLM-P enables (1) lightweight modeling of language-invariant and language-specific knowledge across languages, and (2) easy integration with other multilingual pre-training methods. On the tasks of XTREME including text classification, sequence labeling, question answering, and sentence retrieval, both base- and large-size language models pre-trained with our proposed method exhibit consistent performance improvement. Furthermore, it provides substantial advantages for low-resource languages in unsupervised sentence retrieval and for target languages that differ greatly from the source langu",
    "path": "papers/23/06/2306.07610.json",
    "total_tokens": 941,
    "translated_title": "多语言模型预训练的软语言聚类",
    "translated_abstract": "多语言预训练语言模型已经展示了惊人的跨语言迁移能力，然而，当目标语言与源语言远离语言类型或预训练数据有限时，它们的表现会受到限制。在本文中，我们提出了XLM-P，它以上下文方式检索提示作为条件编码实例的灵活指导。我们的XLM-P实现了(1)在多语言之间轻量化建模，包括语言不变和语言特定知识，以及(2)容易与其他多语言预训练方法整合。在多个任务中包括文本分类，序列标记，问答和句子检索的XTREME中，我们提出的基于XLM-P预训练的大模型和基准模型表现都得到了一致的提升。此外，我们还证明在无监督句子检索中，它针对低资源语言提供了重大优势，并针对与源语言差异很大的目标语言在序列标记和问答方面也具有显著优势。",
    "tldr": "本文提出了XLM-P方法，通过软语言聚类实现了多语言预训练模型的轻量化和语言不变与语言特定知识的建模，并在多个任务中表现出了一致的性能提升，针对低资源语言和语言类型差异大的目标语言具有显著优势。",
    "en_tdlr": "This paper proposes XLM-P method which enables lightweight modeling of language-invariant and language-specific knowledge across languages by soft language clustering for multilingual pre-trained language models, and exhibits consistent performance improvement on multiple tasks, particularly advantageous for low-resource and distant-typology languages."
}