{
    "title": "Generative Text-Guided 3D Vision-Language Pretraining for Unified Medical Image Segmentation. (arXiv:2306.04811v1 [cs.CV])",
    "abstract": "Vision-Language Pretraining (VLP) has demonstrated remarkable capabilities in learning visual representations from textual descriptions of images without annotations. Yet, effective VLP demands large-scale image-text pairs, a resource that suffers scarcity in the medical domain. Moreover, conventional VLP is limited to 2D images while medical images encompass diverse modalities, often in 3D, making the learning process more challenging. To address these challenges, we present Generative Text-Guided 3D Vision-Language Pretraining for Unified Medical Image Segmentation (GTGM), a framework that extends of VLP to 3D medical images without relying on paired textual descriptions. Specifically, GTGM utilizes large language models (LLM) to generate medical-style text from 3D medical images. This synthetic text is then used to supervise 3D visual representation learning. Furthermore, a negative-free contrastive learning objective strategy is introduced to cultivate consistent visual representat",
    "link": "http://arxiv.org/abs/2306.04811",
    "context": "Title: Generative Text-Guided 3D Vision-Language Pretraining for Unified Medical Image Segmentation. (arXiv:2306.04811v1 [cs.CV])\nAbstract: Vision-Language Pretraining (VLP) has demonstrated remarkable capabilities in learning visual representations from textual descriptions of images without annotations. Yet, effective VLP demands large-scale image-text pairs, a resource that suffers scarcity in the medical domain. Moreover, conventional VLP is limited to 2D images while medical images encompass diverse modalities, often in 3D, making the learning process more challenging. To address these challenges, we present Generative Text-Guided 3D Vision-Language Pretraining for Unified Medical Image Segmentation (GTGM), a framework that extends of VLP to 3D medical images without relying on paired textual descriptions. Specifically, GTGM utilizes large language models (LLM) to generate medical-style text from 3D medical images. This synthetic text is then used to supervise 3D visual representation learning. Furthermore, a negative-free contrastive learning objective strategy is introduced to cultivate consistent visual representat",
    "path": "papers/23/06/2306.04811.json",
    "total_tokens": 1139,
    "translated_title": "生成文本引导的三维视觉语言预训练用于统一医学图像分割",
    "translated_abstract": "视觉语言预训练（VLP）已经表现出在没有注释的情况下从图像的文本描述中学习视觉表示方面的显着能力。然而，有效的VLP需要大规模的图像-文本对，而医学领域中缺乏这种资源。此外，传统的VLP仅限于2D图像，而医学图像包括不同的模态，通常是3D图像，使得学习过程更具挑战性。为了解决这些挑战，我们提出了一种生成文本引导的三维视觉语言预训练用于统一医学图像分割（GTGM）的框架，该框架扩展了VLP以适用于不依赖配对文本描述的三维医学图像。具体而言，GTGM利用大型语言模型（LLM）从三维医学图像生成医学风格的文本。然后，这个合成文本被用来监督三维视觉表示的学习。此外，引入了一种无负对比学习目标策略，来培养一致的视觉表示。在两个公共的医学图像分割基准测试上的实验结果表明，GTGM优于几种最先进的基准算法，并且在降低医学注释需求的同时实现了与完全监督方法相当的性能。",
    "tldr": "本文提出了一种生成文本引导的三维视觉语言预训练用于统一医学图像分割的框架，不需要大规模的图像-文本对，可用于3D医学图像并且性能优越。",
    "en_tdlr": "This paper proposes a framework of Generative Text-Guided 3D Vision-Language Pretraining for Unified Medical Image Segmentation (GTGM), which extends standard VLP to 3D medical images without relying on paired textual descriptions. The synthetic text generated from large language models is used to supervise 3D visual representation learning, and a negative-free contrastive learning objective strategy is introduced to cultivate consistent visual representations. GTGM outperforms several state-of-the-art baselines and achieves competitive performance with fully supervised methods while reducing the requirement of expensive medical annotations."
}