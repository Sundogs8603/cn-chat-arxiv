{
    "title": "Unsupervised Dense Retrieval with Relevance-Aware Contrastive Pre-Training. (arXiv:2306.03166v1 [cs.IR])",
    "abstract": "Dense retrievers have achieved impressive performance, but their demand for abundant training data limits their application scenarios. Contrastive pre-training, which constructs pseudo-positive examples from unlabeled data, has shown great potential to solve this problem. However, the pseudo-positive examples crafted by data augmentations can be irrelevant. To this end, we propose relevance-aware contrastive learning. It takes the intermediate-trained model itself as an imperfect oracle to estimate the relevance of positive pairs and adaptively weighs the contrastive loss of different pairs according to the estimated relevance. Our method consistently improves the SOTA unsupervised Contriever model on the BEIR and open-domain QA retrieval benchmarks. Further exploration shows that our method can not only beat BM25 after further pre-training on the target corpus but also serves as a good few-shot learner. Our code is publicly available at https://github.com/Yibin-Lei/ReContriever.",
    "link": "http://arxiv.org/abs/2306.03166",
    "context": "Title: Unsupervised Dense Retrieval with Relevance-Aware Contrastive Pre-Training. (arXiv:2306.03166v1 [cs.IR])\nAbstract: Dense retrievers have achieved impressive performance, but their demand for abundant training data limits their application scenarios. Contrastive pre-training, which constructs pseudo-positive examples from unlabeled data, has shown great potential to solve this problem. However, the pseudo-positive examples crafted by data augmentations can be irrelevant. To this end, we propose relevance-aware contrastive learning. It takes the intermediate-trained model itself as an imperfect oracle to estimate the relevance of positive pairs and adaptively weighs the contrastive loss of different pairs according to the estimated relevance. Our method consistently improves the SOTA unsupervised Contriever model on the BEIR and open-domain QA retrieval benchmarks. Further exploration shows that our method can not only beat BM25 after further pre-training on the target corpus but also serves as a good few-shot learner. Our code is publicly available at https://github.com/Yibin-Lei/ReContriever.",
    "path": "papers/23/06/2306.03166.json",
    "total_tokens": 925,
    "translated_title": "无监督稠密检索及相关性感知对比学习",
    "translated_abstract": "稠密检索已经取得了令人瞩目的性能，但其对丰富的训练数据的要求限制了其应用场景。对比学习可以从无标签数据中构建伪正向示例，已经显示出解决这个问题的巨大潜力。然而，数据增广制作的伪正向示例可能不相关。因此，我们提出了相关性感知对比学习。它将中间训练的模型本身作为不完美的预测器来估计正对偶的相关性，并根据估计的相关性自适应地加权不同对偶的对比损失。我们的方法在BEIR和开放领域的QA检索基准上始终优于SOTA无监督检索模型Contriever。进一步的探索表明，我们的方法不仅可以在目标语料库上进一步预训练后击败BM25，而且还可以作为很好的少量样本学习器。我们的代码公开在https://github.com/Yibin-Lei/ReContriever。",
    "tldr": "本文提出了相关性感知对比学习方法，该方法可以自适应地加权不同对偶的对比损失，以改善无监督检索模型性能，进一步的探索表明它可以击败BM25，作为很好的少量样本学习器。",
    "en_tdlr": "This paper proposes a relevance-aware contrastive learning method that adaptively weighs the contrastive loss of different pairs to improve the performance of unsupervised retrieval models. It consistently outperforms the state-of-the-art Contriever model on retrieval benchmarks and can beat BM25, serving as a good few-shot learner."
}