{
    "title": "Revisit Few-shot Intent Classification with PLMs: Direct Fine-tuning vs. Continual Pre-training. (arXiv:2306.05278v1 [cs.CL])",
    "abstract": "We consider the task of few-shot intent detection, which involves training a deep learning model to classify utterances based on their underlying intents using only a small amount of labeled data. The current approach to address this problem is through continual pre-training, i.e., fine-tuning pre-trained language models (PLMs) on external resources (e.g., conversational corpora, public intent detection datasets, or natural language understanding datasets) before using them as utterance encoders for training an intent classifier. In this paper, we show that continual pre-training may not be essential, since the overfitting problem of PLMs on this task may not be as serious as expected. Specifically, we find that directly fine-tuning PLMs on only a handful of labeled examples already yields decent results compared to methods that employ continual pre-training, and the performance gap diminishes rapidly as the number of labeled data increases. To maximize the utilization of the limited a",
    "link": "http://arxiv.org/abs/2306.05278",
    "context": "Title: Revisit Few-shot Intent Classification with PLMs: Direct Fine-tuning vs. Continual Pre-training. (arXiv:2306.05278v1 [cs.CL])\nAbstract: We consider the task of few-shot intent detection, which involves training a deep learning model to classify utterances based on their underlying intents using only a small amount of labeled data. The current approach to address this problem is through continual pre-training, i.e., fine-tuning pre-trained language models (PLMs) on external resources (e.g., conversational corpora, public intent detection datasets, or natural language understanding datasets) before using them as utterance encoders for training an intent classifier. In this paper, we show that continual pre-training may not be essential, since the overfitting problem of PLMs on this task may not be as serious as expected. Specifically, we find that directly fine-tuning PLMs on only a handful of labeled examples already yields decent results compared to methods that employ continual pre-training, and the performance gap diminishes rapidly as the number of labeled data increases. To maximize the utilization of the limited a",
    "path": "papers/23/06/2306.05278.json",
    "total_tokens": 978,
    "translated_title": "重新审视使用PLMs的Few-shot Intent Classification: 直接微调 vs 连续预训练",
    "translated_abstract": "本文考虑Few-shot Intent Classification任务，该任务涉及仅使用少量标记数据训练深度学习模型以基于其基础意图分类话语。解决此问题的当前方法是通过连续预训练，即在外部资源（例如会话语料库、公共意图检测数据集或自然语言理解数据集）上微调预训练语言模型（PLMs），然后使用它们作为话语编码器来训练意图分类器。在本文中，我们展示了连续预训练可能并非必要，因为PLMs在此任务上的过拟合问题可能并不像预期的那样严重。具体来说，我们发现，直接对仅有少量标记示例的PLMs进行微调已经可以产生相当不错的结果，而绩效差距随着标记数据量的增加迅速缩小。为了最大限度地利用有限的标记数据，我们提出了一种新的微调策略，即注意力流控（Attention Flow Control），其允许在不同的预训练层之间动态分配微调的重心。",
    "tldr": "本文探讨了Few-shot Intent Classification任务的解决方法。相较于传统的在外部资源上连续预训练，本文提出了直接微调预训练语言模型的方法，并通过实验证明其在少量标记数据情况下已经可以取得不错的结果，表明连续预训练并非必要。",
    "en_tdlr": "This paper explores the solution to the Few-shot Intent Classification task. Instead of traditional continual pre-training on external resources, this paper proposes direct fine-tuning of pre-trained language models and proves through experiments that it can already achieve decent results with only a small amount of labeled data, indicating that continual pre-training is not necessary."
}