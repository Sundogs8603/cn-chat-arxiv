{
    "title": "Sampling weights of deep neural networks. (arXiv:2306.16830v1 [cs.LG])",
    "abstract": "We introduce a probability distribution, combined with an efficient sampling algorithm, for weights and biases of fully-connected neural networks. In a supervised learning context, no iterative optimization or gradient computations of internal network parameters are needed to obtain a trained network. The sampling is based on the idea of random feature models. However, instead of a data-agnostic distribution, e.g., a normal distribution, we use both the input and the output training data of the supervised learning problem to sample both shallow and deep networks. We prove that the sampled networks we construct are universal approximators. We also show that our sampling scheme is invariant to rigid body transformations and scaling of the input data. This implies many popular pre-processing techniques are no longer required. For Barron functions, we show that the $L^2$-approximation error of sampled shallow networks decreases with the square root of the number of neurons. In numerical ex",
    "link": "http://arxiv.org/abs/2306.16830",
    "context": "Title: Sampling weights of deep neural networks. (arXiv:2306.16830v1 [cs.LG])\nAbstract: We introduce a probability distribution, combined with an efficient sampling algorithm, for weights and biases of fully-connected neural networks. In a supervised learning context, no iterative optimization or gradient computations of internal network parameters are needed to obtain a trained network. The sampling is based on the idea of random feature models. However, instead of a data-agnostic distribution, e.g., a normal distribution, we use both the input and the output training data of the supervised learning problem to sample both shallow and deep networks. We prove that the sampled networks we construct are universal approximators. We also show that our sampling scheme is invariant to rigid body transformations and scaling of the input data. This implies many popular pre-processing techniques are no longer required. For Barron functions, we show that the $L^2$-approximation error of sampled shallow networks decreases with the square root of the number of neurons. In numerical ex",
    "path": "papers/23/06/2306.16830.json",
    "total_tokens": 902,
    "translated_title": "深度神经网络的采样权重",
    "translated_abstract": "我们引入了一个概率分布和有效的采样算法，用于完全连接神经网络的权重和偏差。在监督学习环境中，不需要通过迭代优化或计算内部网络参数的梯度来训练网络。采样基于随机特征模型的思想。然而，我们使用输入和输出训练数据对浅层和深度网络进行采样，而不是使用数据无关的分布，如正态分布。我们证明了我们构造的采样网络是通用逼近器。我们还证明了我们的采样方案对刚体变换和输入数据的缩放是不变的。这意味着许多常用的预处理技术不再需要。对于巴龙函数，我们证明了采样浅层网络的L^2近似误差随着神经元数量的平方根减小。在数值实验中，我们展示了我们的方法在多个数据集上的优越性能。",
    "tldr": "我们提出了一种基于随机特征模型的采样方法，用于深度神经网络的权重和偏差。我们的方法不需要迭代优化或计算梯度，能够生成通用逼近器，并且对数据的变换和缩放是不变的。在数值实验中，我们展示了我们的方法的优越性能。",
    "en_tdlr": "We propose a sampling method based on random feature models for the weights and biases of deep neural networks. Our method does not require iterative optimization or gradient computation, can generate universal approximators, and is invariant to data transformations and scaling. Numerical experiments demonstrate the superior performance of our method."
}