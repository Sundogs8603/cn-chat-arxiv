{
    "title": "Provably Robust Temporal Difference Learning for Heavy-Tailed Rewards. (arXiv:2306.11455v1 [cs.LG])",
    "abstract": "In a broad class of reinforcement learning applications, stochastic rewards have heavy-tailed distributions, which lead to infinite second-order moments for stochastic (semi)gradients in policy evaluation and direct policy optimization. In such instances, the existing RL methods may fail miserably due to frequent statistical outliers. In this work, we establish that temporal difference (TD) learning with a dynamic gradient clipping mechanism, and correspondingly operated natural actor-critic (NAC), can be provably robustified against heavy-tailed reward distributions. It is shown in the framework of linear function approximation that a favorable tradeoff between bias and variability of the stochastic gradients can be achieved with this dynamic gradient clipping mechanism. In particular, we prove that robust versions of TD learning achieve sample complexities of order $\\mathcal{O}(\\varepsilon^{-\\frac{1}{p}})$ and $\\mathcal{O}(\\varepsilon^{-1-\\frac{1}{p}})$ with and without the full-rank",
    "link": "http://arxiv.org/abs/2306.11455",
    "context": "Title: Provably Robust Temporal Difference Learning for Heavy-Tailed Rewards. (arXiv:2306.11455v1 [cs.LG])\nAbstract: In a broad class of reinforcement learning applications, stochastic rewards have heavy-tailed distributions, which lead to infinite second-order moments for stochastic (semi)gradients in policy evaluation and direct policy optimization. In such instances, the existing RL methods may fail miserably due to frequent statistical outliers. In this work, we establish that temporal difference (TD) learning with a dynamic gradient clipping mechanism, and correspondingly operated natural actor-critic (NAC), can be provably robustified against heavy-tailed reward distributions. It is shown in the framework of linear function approximation that a favorable tradeoff between bias and variability of the stochastic gradients can be achieved with this dynamic gradient clipping mechanism. In particular, we prove that robust versions of TD learning achieve sample complexities of order $\\mathcal{O}(\\varepsilon^{-\\frac{1}{p}})$ and $\\mathcal{O}(\\varepsilon^{-1-\\frac{1}{p}})$ with and without the full-rank",
    "path": "papers/23/06/2306.11455.json",
    "total_tokens": 909,
    "translated_title": "重尾奖励的可证明鲁棒时间差分学习",
    "translated_abstract": "在广泛的强化学习应用中，随机奖励具有重尾分布，这导致策略评估和直接策略优化中的随机（半）梯度具有无限的二阶矩。在这种情况下，由于经常出现统计上的离群值，现有的强化学习方法可能会失败。本研究证明了一种带有动态梯度剪裁机制的时间差分（TD）学习，以及相应操作的自然演员-评论家（NAC），可以在重尾奖励分布下被证明具有鲁棒性。在线性函数逼近的框架下，证明了这种动态梯度剪裁机制可以在偏差和随机梯度变差之间取得有利的折衷。特别地，证明了TD学习的鲁棒版本可以达到$\\mathcal{O}(\\varepsilon^{-\\frac{1}{p}})$和$\\mathcal{O}(\\varepsilon^{-1-\\frac{1}{p}})$的样本复杂度。",
    "tldr": "证明了一种带有动态梯度剪裁机制的时间差分（TD）学习可以在重尾奖励分布下被证明具有鲁棒性。",
    "en_tdlr": "This paper establishes that temporal difference (TD) learning with a dynamic gradient clipping mechanism, and correspondingly operated natural actor-critic (NAC), can be provably robustified against heavy-tailed reward distributions."
}