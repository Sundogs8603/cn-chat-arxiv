{
    "title": "Faster Training of Diffusion Models and Improved Density Estimation via Parallel Score Matching. (arXiv:2306.02658v1 [cs.LG])",
    "abstract": "In Diffusion Probabilistic Models (DPMs), the task of modeling the score evolution via a single time-dependent neural network necessitates extended training periods and may potentially impede modeling flexibility and capacity. To counteract these challenges, we propose leveraging the independence of learning tasks at different time points inherent to DPMs. More specifically, we partition the learning task by utilizing independent networks, each dedicated to learning the evolution of scores within a specific time sub-interval. Further, inspired by residual flows, we extend this strategy to its logical conclusion by employing separate networks to independently model the score at each individual time point. As empirically demonstrated on synthetic and image datasets, our approach not only significantly accelerates the training process by introducing an additional layer of parallelization atop data parallelization, but it also enhances density estimation performance when compared to the co",
    "link": "http://arxiv.org/abs/2306.02658",
    "context": "Title: Faster Training of Diffusion Models and Improved Density Estimation via Parallel Score Matching. (arXiv:2306.02658v1 [cs.LG])\nAbstract: In Diffusion Probabilistic Models (DPMs), the task of modeling the score evolution via a single time-dependent neural network necessitates extended training periods and may potentially impede modeling flexibility and capacity. To counteract these challenges, we propose leveraging the independence of learning tasks at different time points inherent to DPMs. More specifically, we partition the learning task by utilizing independent networks, each dedicated to learning the evolution of scores within a specific time sub-interval. Further, inspired by residual flows, we extend this strategy to its logical conclusion by employing separate networks to independently model the score at each individual time point. As empirically demonstrated on synthetic and image datasets, our approach not only significantly accelerates the training process by introducing an additional layer of parallelization atop data parallelization, but it also enhances density estimation performance when compared to the co",
    "path": "papers/23/06/2306.02658.json",
    "total_tokens": 882,
    "translated_title": "基于并行评分匹配的扩散模型更快的训练和改进密度估计",
    "translated_abstract": "在扩散概率模型(DPMs)中，单个时间依赖神经网络模型评分演变的任务需要长时间的训练，可能会阻碍建模的灵活性和能力。为了克服这些挑战，我们提出利用 DPMs 固有的不同时间点学习任务的独立性。具体而言，我们通过使用独立的网络将学习任务进行划分，每个网络专门学习特定时间子间隔内的分数演变。受残差流的启发，我们将此策略扩展到其逻辑结论，采用单独的网络独立建模每个时间点的分数。通过对合成和图像数据集的实证证明，我们的方法不仅通过在数据并行化之上引入额外的并行化层显著加速了训练过程，而且在与现有方法比较时，提高了密度估计性能。",
    "tldr": "本文提出一种基于并行评分匹配的扩散模型训练方法，通过利用不同时间点的任务独立性，采用独立网络建模分数演变，提高了密度估计性能，加速了训练过程。"
}