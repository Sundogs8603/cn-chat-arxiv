{
    "title": "Can Pretrained Language Models Derive Correct Semantics from Corrupt Subwords under Noise?. (arXiv:2306.15268v1 [cs.CL])",
    "abstract": "For Pretrained Language Models (PLMs), their susceptibility to noise has recently been linked to subword segmentation. However, it is unclear which aspects of segmentation affect their understanding. This study assesses the robustness of PLMs against various disrupted segmentation caused by noise. An evaluation framework for subword segmentation, named Contrastive Lexical Semantic (CoLeS) probe, is proposed. It provides a systematic categorization of segmentation corruption under noise and evaluation protocols by generating contrastive datasets with canonical-noisy word pairs. Experimental results indicate that PLMs are unable to accurately compute word meanings if the noise introduces completely different subwords, small subword fragments, or a large number of additional subwords, particularly when they are inserted within other subwords.",
    "link": "http://arxiv.org/abs/2306.15268",
    "context": "Title: Can Pretrained Language Models Derive Correct Semantics from Corrupt Subwords under Noise?. (arXiv:2306.15268v1 [cs.CL])\nAbstract: For Pretrained Language Models (PLMs), their susceptibility to noise has recently been linked to subword segmentation. However, it is unclear which aspects of segmentation affect their understanding. This study assesses the robustness of PLMs against various disrupted segmentation caused by noise. An evaluation framework for subword segmentation, named Contrastive Lexical Semantic (CoLeS) probe, is proposed. It provides a systematic categorization of segmentation corruption under noise and evaluation protocols by generating contrastive datasets with canonical-noisy word pairs. Experimental results indicate that PLMs are unable to accurately compute word meanings if the noise introduces completely different subwords, small subword fragments, or a large number of additional subwords, particularly when they are inserted within other subwords.",
    "path": "papers/23/06/2306.15268.json",
    "total_tokens": 901,
    "translated_title": "预训练语言模型是否能够在噪音下从损坏的子词中得到正确的语义？",
    "translated_abstract": "对于预训练语言模型（PLMs），它们对噪音的敏感性最近被认为与子词分割有关。然而，目前还不清楚分割的哪些方面会影响它们的理解能力。本研究评估了PLMs对由噪音引起的各种受损分割的鲁棒性。提出了一个用于子词分割的评估框架，名为对比词汇语义（CoLeS）探针。它通过生成带有规范-噪音词对的对比数据集，对分割错误进行了系统分类，并提供了评估协议。实验结果表明，如果噪音引入完全不同的子词、小的子词片段或大量的额外子词，特别是当它们插入在其他子词中时，PLMs将无法准确计算单词的含义。",
    "tldr": "本研究评估了预训练语言模型对噪音引起的不同分割错误的鲁棒性。实验结果表明，如果噪音引入不同的子词、小的子词片段或大量的额外子词，特别是插入在其他子词中，PLMs将无法准确计算单词的含义。"
}