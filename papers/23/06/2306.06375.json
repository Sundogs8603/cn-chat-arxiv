{
    "title": "Optimized Gradient Tracking for Decentralized Online Learning",
    "abstract": "This work considers the problem of decentralized online learning, where the goal is to track the optimum of the sum of time-varying functions, distributed across several nodes in a network. The local availability of the functions and their gradients necessitates coordination and consensus among the nodes. We put forth the Generalized Gradient Tracking (GGT) framework that unifies a number of existing approaches, including the state-of-the-art ones. The performance of the proposed GGT algorithm is theoretically analyzed using a novel semidefinite programming-based analysis that yields the desired regret bounds under very general conditions and without requiring the gradient boundedness assumption. The results are applicable to the special cases of GGT, which include various state-of-the-art algorithms as well as new dynamic versions of various classical decentralized algorithms. To further minimize the regret, we consider a condensed version of GGT with only four free parameters. A proc",
    "link": "https://arxiv.org/abs/2306.06375",
    "context": "Title: Optimized Gradient Tracking for Decentralized Online Learning\nAbstract: This work considers the problem of decentralized online learning, where the goal is to track the optimum of the sum of time-varying functions, distributed across several nodes in a network. The local availability of the functions and their gradients necessitates coordination and consensus among the nodes. We put forth the Generalized Gradient Tracking (GGT) framework that unifies a number of existing approaches, including the state-of-the-art ones. The performance of the proposed GGT algorithm is theoretically analyzed using a novel semidefinite programming-based analysis that yields the desired regret bounds under very general conditions and without requiring the gradient boundedness assumption. The results are applicable to the special cases of GGT, which include various state-of-the-art algorithms as well as new dynamic versions of various classical decentralized algorithms. To further minimize the regret, we consider a condensed version of GGT with only four free parameters. A proc",
    "path": "papers/23/06/2306.06375.json",
    "total_tokens": 903,
    "translated_title": "优化的梯度跟踪用于分布式在线学习",
    "translated_abstract": "本文考虑了分布式在线学习的问题，其中目标是跟踪分布在网络中的多个节点上的时间变化函数的最优值之和。函数和梯度的局部可用性需要节点之间的协调和共识。我们提出了通用梯度跟踪（GGT）框架，统一了一些现有的方法，包括最先进的方法。通过使用基于半定规划的新颖分析方法对所提出的GGT算法进行理论分析，得到了在非常一般的条件下且不需要梯度有界性假设的理想遗憾界。这些结果适用于GGT的特殊情况，包括各种最先进的算法以及各种古典分布式算法的新动态版本。为了进一步减小遗憾，我们考虑了只有四个自由参数的GGT的简化版本。",
    "tldr": "这项工作提出了一种新的通用梯度跟踪（GGT）框架，用于解决分布式在线学习中的优化问题。该方法在不需要梯度有界性假设的情况下，通过一种新颖的半定规划分析方法来获得理想的遗憾界，并可以适用于包括最先进算法和新动态版本的各种情况。"
}