{
    "title": "Non-autoregressive Conditional Diffusion Models for Time Series Prediction. (arXiv:2306.05043v1 [cs.LG])",
    "abstract": "Recently, denoising diffusion models have led to significant breakthroughs in the generation of images, audio and text. However, it is still an open question on how to adapt their strong modeling ability to model time series. In this paper, we propose TimeDiff, a non-autoregressive diffusion model that achieves high-quality time series prediction with the introduction of two novel conditioning mechanisms: future mixup and autoregressive initialization. Similar to teacher forcing, future mixup allows parts of the ground-truth future predictions for conditioning, while autoregressive initialization helps better initialize the model with basic time series patterns such as short-term trends. Extensive experiments are performed on nine real-world datasets. Results show that TimeDiff consistently outperforms existing time series diffusion models, and also achieves the best overall performance across a variety of the existing strong baselines (including transformers and FiLM).",
    "link": "http://arxiv.org/abs/2306.05043",
    "context": "Title: Non-autoregressive Conditional Diffusion Models for Time Series Prediction. (arXiv:2306.05043v1 [cs.LG])\nAbstract: Recently, denoising diffusion models have led to significant breakthroughs in the generation of images, audio and text. However, it is still an open question on how to adapt their strong modeling ability to model time series. In this paper, we propose TimeDiff, a non-autoregressive diffusion model that achieves high-quality time series prediction with the introduction of two novel conditioning mechanisms: future mixup and autoregressive initialization. Similar to teacher forcing, future mixup allows parts of the ground-truth future predictions for conditioning, while autoregressive initialization helps better initialize the model with basic time series patterns such as short-term trends. Extensive experiments are performed on nine real-world datasets. Results show that TimeDiff consistently outperforms existing time series diffusion models, and also achieves the best overall performance across a variety of the existing strong baselines (including transformers and FiLM).",
    "path": "papers/23/06/2306.05043.json",
    "total_tokens": 917,
    "translated_title": "基于非自回归条件扩散模型的时间序列预测研究",
    "translated_abstract": "最近，去噪扩散模型在图像、音频和文本生成方面取得了重大突破。然而，如何适应其强大的建模能力来建模时间序列仍然是一个未解之谜。在本文中，我们提出了TimeDiff，一种非自回归扩散模型，通过引入两种新颖的条件机制 - 未来混合和自回归初始化，实现高质量时间序列预测。类似于teacher forcing，未来混合允许使用部分真实未来预测结果进行条件，而自回归初始化有助于更好地初始化模型并获得基本的时间序列模式，如短期趋势。在九个真实世界数据集上进行了大量实验。结果表明，TimeDiff始终优于现有的时间序列扩散模型，并在各种现有的强基线模型（包括transformers和FiLM）中取得了最佳的整体性能。",
    "tldr": "本文提出了一种名为TimeDiff的非自回归扩散模型来实现高质量时间序列预测，通过引入两种新颖的条件机制 - 未来混合和自回归初始化。实验结果表明，TimeDiff在各种强基线模型中取得了最佳整体性能。",
    "en_tdlr": "This paper proposes a non-autoregressive diffusion model called TimeDiff for high-quality time series prediction, which introduces two novel conditioning mechanisms, future mixup and autoregressive initialization. Experimental results demonstrate that TimeDiff consistently outperforms existing time series diffusion models, and achieves the best overall performance across a variety of existing strong baselines (including transformers and FiLM)."
}