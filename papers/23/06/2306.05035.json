{
    "title": "Does Long-Term Series Forecasting Need Complex Attention and Extra Long Inputs?. (arXiv:2306.05035v1 [cs.LG])",
    "abstract": "As Transformer-based models have achieved impressive performance on various time series tasks, Long-Term Series Forecasting (LTSF) tasks have also received extensive attention in recent years. However, due to the inherent computational complexity and long sequences demanding of Transformer-based methods, its application on LTSF tasks still has two major issues that need to be further investigated: 1) Whether the sparse attention mechanism designed by these methods actually reduce the running time on real devices; 2) Whether these models need extra long input sequences to guarantee their performance? The answers given in this paper are negative. Therefore, to better copy with these two issues, we design a lightweight Period-Attention mechanism (Periodformer), which renovates the aggregation of long-term subseries via explicit periodicity and short-term subseries via built-in proximity. Meanwhile, a gating mechanism is embedded into Periodformer to regulate the influence of the attention",
    "link": "http://arxiv.org/abs/2306.05035",
    "context": "Title: Does Long-Term Series Forecasting Need Complex Attention and Extra Long Inputs?. (arXiv:2306.05035v1 [cs.LG])\nAbstract: As Transformer-based models have achieved impressive performance on various time series tasks, Long-Term Series Forecasting (LTSF) tasks have also received extensive attention in recent years. However, due to the inherent computational complexity and long sequences demanding of Transformer-based methods, its application on LTSF tasks still has two major issues that need to be further investigated: 1) Whether the sparse attention mechanism designed by these methods actually reduce the running time on real devices; 2) Whether these models need extra long input sequences to guarantee their performance? The answers given in this paper are negative. Therefore, to better copy with these two issues, we design a lightweight Period-Attention mechanism (Periodformer), which renovates the aggregation of long-term subseries via explicit periodicity and short-term subseries via built-in proximity. Meanwhile, a gating mechanism is embedded into Periodformer to regulate the influence of the attention",
    "path": "papers/23/06/2306.05035.json",
    "total_tokens": 885,
    "translated_title": "长期序列预测是否需要复杂的注意力机制和额外的长输入数据？",
    "translated_abstract": "随着基于Transformer的模型在各种时间序列任务上取得了令人印象深刻的性能，长期序列预测（LTSF）任务在近年来也受到了广泛关注。然而，由于基于Transformer的方法所固有的计算复杂性和需要长序列，它在LTSF任务上的应用仍然存在两个主要问题需要进一步研究：1）这些方法设计的稀疏注意机制是否实际上缩短了真实设备上的运行时间；2）这些模型是否需要额外长的输入序列来保证它们的性能？本论文的答案是否定的。因此，为更好地解决这两个问题，我们设计了一种轻量级的周期-注意机制（Periodformer），通过显式周期性和内置的接近性来重新设计长期子序列和短期子序列的聚合。同时，我们还嵌入了一个门控机制到Periodformer中以调整注意力的影响。",
    "tldr": "本论文介绍了一种新的轻量级周期-注意机制，名为Periodformer，解决了长期序列预测中的两个主要问题，并证明了Transformer-based方法不需要额外长的输入序列来保证性能。",
    "en_tdlr": "This paper introduces a new lightweight Period-Attention mechanism called Periodformer, which solves two major issues in Long-Term Series Forecasting and shows that Transformer-based methods do not need extra-long input sequences to guarantee their performance."
}