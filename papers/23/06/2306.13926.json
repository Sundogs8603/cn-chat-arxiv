{
    "title": "Graph Neural Networks Provably Benefit from Structural Information: A Feature Learning Perspective. (arXiv:2306.13926v1 [cs.LG])",
    "abstract": "Graph neural networks (GNNs) have pioneered advancements in graph representation learning, exhibiting superior feature learning and performance over multilayer perceptrons (MLPs) when handling graph inputs. However, understanding the feature learning aspect of GNNs is still in its initial stage. This study aims to bridge this gap by investigating the role of graph convolution within the context of feature learning theory in neural networks using gradient descent training. We provide a distinct characterization of signal learning and noise memorization in two-layer graph convolutional networks (GCNs), contrasting them with two-layer convolutional neural networks (CNNs). Our findings reveal that graph convolution significantly augments the benign overfitting regime over the counterpart CNNs, where signal learning surpasses noise memorization, by approximately factor $\\sqrt{D}^{q-2}$, with $D$ denoting a node's expected degree and $q$ being the power of the ReLU activation function where ",
    "link": "http://arxiv.org/abs/2306.13926",
    "context": "Title: Graph Neural Networks Provably Benefit from Structural Information: A Feature Learning Perspective. (arXiv:2306.13926v1 [cs.LG])\nAbstract: Graph neural networks (GNNs) have pioneered advancements in graph representation learning, exhibiting superior feature learning and performance over multilayer perceptrons (MLPs) when handling graph inputs. However, understanding the feature learning aspect of GNNs is still in its initial stage. This study aims to bridge this gap by investigating the role of graph convolution within the context of feature learning theory in neural networks using gradient descent training. We provide a distinct characterization of signal learning and noise memorization in two-layer graph convolutional networks (GCNs), contrasting them with two-layer convolutional neural networks (CNNs). Our findings reveal that graph convolution significantly augments the benign overfitting regime over the counterpart CNNs, where signal learning surpasses noise memorization, by approximately factor $\\sqrt{D}^{q-2}$, with $D$ denoting a node's expected degree and $q$ being the power of the ReLU activation function where ",
    "path": "papers/23/06/2306.13926.json",
    "total_tokens": 934,
    "translated_title": "图神经网络从结构信息中获益的证明：一个特征学习的视角",
    "translated_abstract": "图神经网络(GNNs)在图表示学习方面取得了先驱性进展，在处理图输入时表现出比多层感知器(MLPs)更优越的特征学习和性能。然而，理解GNN的特征学习方面仍处于初始阶段。本研究旨在通过使用梯度下降训练研究图卷积在神经网络特征学习理论中的作用来弥补这一差距。我们提供了对两层图卷积网络(GCNs)中信号学习和噪声记忆的不同刻画，并将它们与两层卷积神经网络(CNNs)进行对比。我们的研究结果表明，与对应的CNNs相比，图卷积网络显著增强了良性过拟合区域，在这个区域内信号学习超越了噪声记忆，并且近似于因子$\\sqrt{D}^{q-2}$，其中$D$表示节点的期望度数，$q$表示ReLU激活功能的幂次。",
    "tldr": "本论文研究了GNN在神经网络特征学习理论中的作用。 发现图卷积网络显著增强了良性过拟合区域，在这个区域内信号学习超越了噪声记忆。",
    "en_tdlr": "This paper investigates the role of GNN in the theory of neural network feature learning and finds that GNN significantly enhances the benign overfitting regime and signal learning surpasses noise memorization in this regime."
}