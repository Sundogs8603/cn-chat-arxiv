{
    "title": "Privacy Distillation: Reducing Re-identification Risk of Multimodal Diffusion Models. (arXiv:2306.01322v1 [cs.LG])",
    "abstract": "Knowledge distillation in neural networks refers to compressing a large model or dataset into a smaller version of itself. We introduce Privacy Distillation, a framework that allows a text-to-image generative model to teach another model without exposing it to identifiable data. Here, we are interested in the privacy issue faced by a data provider who wishes to share their data via a multimodal generative model. A question that immediately arises is ``How can a data provider ensure that the generative model is not leaking identifiable information about a patient?''. Our solution consists of (1) training a first diffusion model on real data (2) generating a synthetic dataset using this model and filtering it to exclude images with a re-identifiability risk (3) training a second diffusion model on the filtered synthetic data only. We showcase that datasets sampled from models trained with privacy distillation can effectively reduce re-identification risk whilst maintaining downstream per",
    "link": "http://arxiv.org/abs/2306.01322",
    "context": "Title: Privacy Distillation: Reducing Re-identification Risk of Multimodal Diffusion Models. (arXiv:2306.01322v1 [cs.LG])\nAbstract: Knowledge distillation in neural networks refers to compressing a large model or dataset into a smaller version of itself. We introduce Privacy Distillation, a framework that allows a text-to-image generative model to teach another model without exposing it to identifiable data. Here, we are interested in the privacy issue faced by a data provider who wishes to share their data via a multimodal generative model. A question that immediately arises is ``How can a data provider ensure that the generative model is not leaking identifiable information about a patient?''. Our solution consists of (1) training a first diffusion model on real data (2) generating a synthetic dataset using this model and filtering it to exclude images with a re-identifiability risk (3) training a second diffusion model on the filtered synthetic data only. We showcase that datasets sampled from models trained with privacy distillation can effectively reduce re-identification risk whilst maintaining downstream per",
    "path": "papers/23/06/2306.01322.json",
    "total_tokens": 900,
    "translated_title": "隐私蒸馏：降低多模态扩散模型的重新识别风险。",
    "translated_abstract": "神经网络中的知识蒸馏是指将大型模型或数据集压缩为其自身的较小版本。我们引入了隐私蒸馏，这是一个框架，允许一个文本到图像生成模型教授另一个模型，而不会暴露可识别的数据。本文关注的是通过多模态生成模型共享数据时所面临的隐私问题。一个立即出现的问题是，“数据提供者如何确保生成模型不会泄露与患者有关的可识别信息？”我们的解决方案包括：（1）在真实数据上训练第一扩散模型；（2）使用该模型生成合成数据，并排除具有重新识别风险的图像；（3）仅在过滤后的合成数据上进行第二次扩散模型的训练。我们展示了使用隐私蒸馏训练的数据集可以有效降低重新识别风险，同时保持下游性能。",
    "tldr": "本文提出了隐私蒸馏框架，通过过滤具有重新识别风险的图像，训练生成模型，从而减少数据提供者共享多模态生成模型时的隐私泄露风险。",
    "en_tdlr": "The paper proposes the Privacy Distillation framework to reduce privacy leakage risks when sharing data through multimodal generative models, achieving this by filtering images with re-identification risk and training models only on filtered synthetic data."
}