{
    "title": "Strategies for improving low resource speech to text translation relying on pre-trained ASR models. (arXiv:2306.00208v1 [cs.CL])",
    "abstract": "This paper presents techniques and findings for improving the performance of low-resource speech to text translation (ST). We conducted experiments on both simulated and real-low resource setups, on language pairs English - Portuguese, and Tamasheq - French respectively. Using the encoder-decoder framework for ST, our results show that a multilingual automatic speech recognition system acts as a good initialization under low-resource scenarios. Furthermore, using the CTC as an additional objective for translation during training and decoding helps to reorder the internal representations and improves the final translation. Through our experiments, we try to identify various factors (initializations, objectives, and hyper-parameters) that contribute the most for improvements in low-resource setups. With only 300 hours of pre-training data, our model achieved 7.3 BLEU score on Tamasheq - French data, outperforming prior published works from IWSLT 2022 by 1.6 points.",
    "link": "http://arxiv.org/abs/2306.00208",
    "context": "Title: Strategies for improving low resource speech to text translation relying on pre-trained ASR models. (arXiv:2306.00208v1 [cs.CL])\nAbstract: This paper presents techniques and findings for improving the performance of low-resource speech to text translation (ST). We conducted experiments on both simulated and real-low resource setups, on language pairs English - Portuguese, and Tamasheq - French respectively. Using the encoder-decoder framework for ST, our results show that a multilingual automatic speech recognition system acts as a good initialization under low-resource scenarios. Furthermore, using the CTC as an additional objective for translation during training and decoding helps to reorder the internal representations and improves the final translation. Through our experiments, we try to identify various factors (initializations, objectives, and hyper-parameters) that contribute the most for improvements in low-resource setups. With only 300 hours of pre-training data, our model achieved 7.3 BLEU score on Tamasheq - French data, outperforming prior published works from IWSLT 2022 by 1.6 points.",
    "path": "papers/23/06/2306.00208.json",
    "total_tokens": 988,
    "translated_title": "基于预训练的ASR模型的低资源语音转文本翻译的改进策略",
    "translated_abstract": "本文提出了改进低资源语音转文本翻译的技术和发现。我们在英语-葡萄牙语和Tamasheq-法语两种语言对的模拟和真实低资源环境下进行了实验。使用编码器-解码器框架来进行翻译，我们的结果表明，多语种自动语音识别系统在低资源情况下作为良好的初始化。此外，在训练和解码时使用CTC作为额外的翻译目标有助于重新排序内部表示并改进最终的翻译。通过实验，我们试图确定对于低资源环境中的改进最有贡献的各种因素（初始化，目标和超参数）。只使用300个小时的预训练数据，我们的模型在Tamasheq-法语数据上达到了7.3的BLEU得分，优于IWSLT 2022的之前发表的作品1.6个点。",
    "tldr": "本论文介绍了针对低资源语音转文本翻译的改进策略，利用编码器-解码器框架以及多语种自动语音识别系统初步提升性能，在训练和解码时使用CTC作为额外的目标进一步优化结果，在Tamasheq-法语数据上达到了7.3的BLEU得分。",
    "en_tdlr": "This paper proposes strategies for improving low-resource speech to text translation using the encoder-decoder framework and pre-trained multilingual automatic speech recognition system. By introducing CTC as an additional objective for training and decoding, the internal representations can be re-ordered and the final translation can be improved. With only 300 hours of pre-training data, the proposed model achieved a BLEU score of 7.3 on Tamasheq - French data, outperforming prior published works from IWSLT 2022 by 1.6 points."
}