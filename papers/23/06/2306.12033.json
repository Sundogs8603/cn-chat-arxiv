{
    "title": "End-to-End Augmentation Hyperparameter Tuning for Self-Supervised Anomaly Detection. (arXiv:2306.12033v1 [cs.LG])",
    "abstract": "Self-supervised learning (SSL) has emerged as a promising paradigm that presents self-generated supervisory signals to real-world problems, bypassing the extensive manual labeling burden. SSL is especially attractive for unsupervised tasks such as anomaly detection, where labeled anomalies are often nonexistent and costly to obtain. While self-supervised anomaly detection (SSAD) has seen a recent surge of interest, the literature has failed to treat data augmentation as a hyperparameter. Meanwhile, recent works have reported that the choice of augmentation has significant impact on detection performance. In this paper, we introduce ST-SSAD (Self-Tuning Self-Supervised Anomaly Detection), the first systematic approach to SSAD in regards to rigorously tuning augmentation. To this end, our work presents two key contributions. The first is a new unsupervised validation loss that quantifies the alignment between the augmented training data and the (unlabeled) test data. In principle we adop",
    "link": "http://arxiv.org/abs/2306.12033",
    "context": "Title: End-to-End Augmentation Hyperparameter Tuning for Self-Supervised Anomaly Detection. (arXiv:2306.12033v1 [cs.LG])\nAbstract: Self-supervised learning (SSL) has emerged as a promising paradigm that presents self-generated supervisory signals to real-world problems, bypassing the extensive manual labeling burden. SSL is especially attractive for unsupervised tasks such as anomaly detection, where labeled anomalies are often nonexistent and costly to obtain. While self-supervised anomaly detection (SSAD) has seen a recent surge of interest, the literature has failed to treat data augmentation as a hyperparameter. Meanwhile, recent works have reported that the choice of augmentation has significant impact on detection performance. In this paper, we introduce ST-SSAD (Self-Tuning Self-Supervised Anomaly Detection), the first systematic approach to SSAD in regards to rigorously tuning augmentation. To this end, our work presents two key contributions. The first is a new unsupervised validation loss that quantifies the alignment between the augmented training data and the (unlabeled) test data. In principle we adop",
    "path": "papers/23/06/2306.12033.json",
    "total_tokens": 1147,
    "translated_title": "自我监督异常检测的端到端增强超参数调整",
    "translated_abstract": "自我监督学习（SSL）已经成为一个有前途的范例，它为现实问题提供自产生的监督信号，避免了繁琐的手动标注工作。SSL对于无监督任务，如异常检测尤其具有吸引力，因为标记的异常通常不存在或难以获得。虽然自我监督异常检测（SSAD）近年来受到了广泛关注，但文献却未将数据增强视为超参数。同时，最近的研究表明，增强选择对检测性能有重要影响。在本文中，我们介绍了ST-SSAD（自我调整自我监督异常检测），这是一种关于严格调整增强的SSAD的第一个系统方法。为此，我们的工作提出了两个关键贡献。第一是一种新的无监督验证损失函数，量化增强训练数据与（无标签）测试数据之间的对齐程度。在原则上，我们采用了最近高效的有监督学习方法借鉴的无监督验证方案和增强数据搜索策略，并将其适应于SSAD。我们进一步提出了一种新的增强搜索方法，通过贝叶斯优化的形式，将轻量级数据增强搜索器的简单集成。在各种异常检测基准数据集上的实验表明，我们的增强调整方法相对于以前的最新结果可以获得一致的性能提升，并且相对于最近的有监督方法具有竞争性的结果。",
    "tldr": "这项研究提出了一种名为ST-SSAD的新方法，可以系统地调整数据增强的超参数，从而有助于提高自我监督异常检测（SSAD）的性能。",
    "en_tdlr": "This study proposes a new approach called ST-SSAD to systematically tune the hyperparameters of data augmentation for self-supervised anomaly detection (SSAD), with experiments showing consistent performance improvements over previous state-of-the-art results."
}