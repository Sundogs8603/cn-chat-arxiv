{
    "title": "SLACK: Stable Learning of Augmentations with Cold-start and KL regularization. (arXiv:2306.09998v1 [cs.CV])",
    "abstract": "Data augmentation is known to improve the generalization capabilities of neural networks, provided that the set of transformations is chosen with care, a selection often performed manually. Automatic data augmentation aims at automating this process. However, most recent approaches still rely on some prior information; they start from a small pool of manually-selected default transformations that are either used to pretrain the network or forced to be part of the policy learned by the automatic data augmentation algorithm. In this paper, we propose to directly learn the augmentation policy without leveraging such prior knowledge. The resulting bilevel optimization problem becomes more challenging due to the larger search space and the inherent instability of bilevel optimization algorithms. To mitigate these issues (i) we follow a successive cold-start strategy with a Kullback-Leibler regularization, and (ii) we parameterize magnitudes as continuous distributions. Our approach leads to",
    "link": "http://arxiv.org/abs/2306.09998",
    "context": "Title: SLACK: Stable Learning of Augmentations with Cold-start and KL regularization. (arXiv:2306.09998v1 [cs.CV])\nAbstract: Data augmentation is known to improve the generalization capabilities of neural networks, provided that the set of transformations is chosen with care, a selection often performed manually. Automatic data augmentation aims at automating this process. However, most recent approaches still rely on some prior information; they start from a small pool of manually-selected default transformations that are either used to pretrain the network or forced to be part of the policy learned by the automatic data augmentation algorithm. In this paper, we propose to directly learn the augmentation policy without leveraging such prior knowledge. The resulting bilevel optimization problem becomes more challenging due to the larger search space and the inherent instability of bilevel optimization algorithms. To mitigate these issues (i) we follow a successive cold-start strategy with a Kullback-Leibler regularization, and (ii) we parameterize magnitudes as continuous distributions. Our approach leads to",
    "path": "papers/23/06/2306.09998.json",
    "total_tokens": 651,
    "translated_title": "SLACK: 冷启动和KL正则化的稳定数据增强学习",
    "translated_abstract": "数据增强已被证明可以提高神经网络的泛化能力，前提是要精心选择一组变换。自动数据增强旨在自动化这个过程。然而，大多数最近的方法仍然依赖于一些先前的信息。本文提出了一种在不依赖先前知识的情况下直接学习数据增强策略的方法。",
    "tldr": "该论文提出了一种在不依赖先前知识的情况下直接学习数据增强策略的方法。"
}