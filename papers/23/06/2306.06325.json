{
    "title": "Explaining a machine learning decision to physicians via counterfactuals. (arXiv:2306.06325v1 [cs.LG])",
    "abstract": "Machine learning models perform well on several healthcare tasks and can help reduce the burden on the healthcare system. However, the lack of explainability is a major roadblock to their adoption in hospitals. \\textit{How can the decision of an ML model be explained to a physician?} The explanations considered in this paper are counterfactuals (CFs), hypothetical scenarios that would have resulted in the opposite outcome. Specifically, time-series CFs are investigated, inspired by the way physicians converse and reason out decisions `I would have given the patient a vasopressor if their blood pressure was lower and falling'. Key properties of CFs that are particularly meaningful in clinical settings are outlined: physiological plausibility, relevance to the task and sparse perturbations. Past work on CF generation does not satisfy these properties, specifically plausibility in that realistic time-series CFs are not generated. A variational autoencoder (VAE)-based approach is proposed ",
    "link": "http://arxiv.org/abs/2306.06325",
    "context": "Title: Explaining a machine learning decision to physicians via counterfactuals. (arXiv:2306.06325v1 [cs.LG])\nAbstract: Machine learning models perform well on several healthcare tasks and can help reduce the burden on the healthcare system. However, the lack of explainability is a major roadblock to their adoption in hospitals. \\textit{How can the decision of an ML model be explained to a physician?} The explanations considered in this paper are counterfactuals (CFs), hypothetical scenarios that would have resulted in the opposite outcome. Specifically, time-series CFs are investigated, inspired by the way physicians converse and reason out decisions `I would have given the patient a vasopressor if their blood pressure was lower and falling'. Key properties of CFs that are particularly meaningful in clinical settings are outlined: physiological plausibility, relevance to the task and sparse perturbations. Past work on CF generation does not satisfy these properties, specifically plausibility in that realistic time-series CFs are not generated. A variational autoencoder (VAE)-based approach is proposed ",
    "path": "papers/23/06/2306.06325.json",
    "total_tokens": 903,
    "translated_title": "通过反事实论证向医生解释机器学习决策",
    "translated_abstract": "机器学习模型在多个医疗保健任务上表现良好，可以帮助减轻医疗保健系统的负担。然而，缺乏可解释性是它们在医院中被采用的主要障碍。本文考虑用反事实论证(CFs)来解释机器学习模型的决策，即假设将结果翻转的情况。特别地，考虑时间序列的CFs，受到医生讨论和推理决策的启发: “如果患者的血压更低且下降，我会给他一个血管紧张素。” 讨论了在临床环境中特别有意义的CFs的关键属性: 生理合理性、任务相关性和稀疏扰动。过去CF生成方面的工作不能满足这些属性，尤其是在现实时间序列CFs方面。本文提出了一种基于变分自编码器(VAE)的方法来生成现实的时间序列CFs。",
    "tldr": "本文提出了一种基于变分自编码器的方法来生成在临床环境中有用的现实时间序列反事实论证(CFs)，以帮助医生解释机器学习模型的决策。",
    "en_tdlr": "This paper proposes a variational autoencoder-based approach to generate realistic time-series counterfactuals (CFs) that are physiologically plausible, task-relevant, and sparsely perturbed, to help physicians explain machine learning model decisions."
}