{
    "title": "An Invariant Learning Characterization of Controlled Text Generation. (arXiv:2306.00198v1 [cs.CL])",
    "abstract": "Controlled generation refers to the problem of creating text that contains stylistic or semantic attributes of interest. Many approaches reduce this problem to training a predictor of the desired attribute. For example, researchers hoping to deploy a large language model to produce non-toxic content may use a toxicity classifier to filter generated text. In practice, the generated text to classify, which is determined by user prompts, may come from a wide range of distributions. In this paper, we show that the performance of controlled generation may be poor if the distributions of text in response to user prompts differ from the distribution the predictor was trained on. To address this problem, we cast controlled generation under distribution shift as an invariant learning problem: the most effective predictor should be invariant across multiple text environments. We then discuss a natural solution that arises from this characterization and propose heuristics for selecting natural en",
    "link": "http://arxiv.org/abs/2306.00198",
    "context": "Title: An Invariant Learning Characterization of Controlled Text Generation. (arXiv:2306.00198v1 [cs.CL])\nAbstract: Controlled generation refers to the problem of creating text that contains stylistic or semantic attributes of interest. Many approaches reduce this problem to training a predictor of the desired attribute. For example, researchers hoping to deploy a large language model to produce non-toxic content may use a toxicity classifier to filter generated text. In practice, the generated text to classify, which is determined by user prompts, may come from a wide range of distributions. In this paper, we show that the performance of controlled generation may be poor if the distributions of text in response to user prompts differ from the distribution the predictor was trained on. To address this problem, we cast controlled generation under distribution shift as an invariant learning problem: the most effective predictor should be invariant across multiple text environments. We then discuss a natural solution that arises from this characterization and propose heuristics for selecting natural en",
    "path": "papers/23/06/2306.00198.json",
    "total_tokens": 600,
    "translated_title": "控制文本生成的不变学习特征描述",
    "translated_abstract": "控制生成是指创建包含所需样式或语义属性的文本的问题。许多方法将这个问题归结为训练所需属性的预测器。在实际应用中，生成的文本可能来自各种分布。本文通过不变学习的方式解决了控制生成中分布转移的问题。",
    "tldr": "本文提出了一种控制文本生成的不变学习方法，解决了生成的文本分布转移的问题。",
    "en_tdlr": "This paper proposes an invariant learning method for controlled text generation, which solves the problem of distribution shift in generated text."
}