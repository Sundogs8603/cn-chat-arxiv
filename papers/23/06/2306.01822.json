{
    "title": "ErfReLU: Adaptive Activation Function for Deep Neural Network. (arXiv:2306.01822v1 [cs.NE])",
    "abstract": "Recent research has found that the activation function (AF) selected for adding non-linearity into the output can have a big impact on how effectively deep learning networks perform. Developing activation functions that can adapt simultaneously with learning is a need of time. Researchers recently started developing activation functions that can be trained throughout the learning process, known as trainable, or adaptive activation functions (AAF). Research on AAF that enhance the outcomes is still in its early stages. In this paper, a novel activation function 'ErfReLU' has been developed based on the erf function and ReLU. This function exploits the ReLU and the error function (erf) to its advantage. State of art activation functions like Sigmoid, ReLU, Tanh, and their properties have been briefly explained. Adaptive activation functions like Tanhsoft1, Tanhsoft2, Tanhsoft3, TanhLU, SAAF, ErfAct, Pserf, Smish, and Serf have also been described. Lastly, performance analysis of 9 traina",
    "link": "http://arxiv.org/abs/2306.01822",
    "context": "Title: ErfReLU: Adaptive Activation Function for Deep Neural Network. (arXiv:2306.01822v1 [cs.NE])\nAbstract: Recent research has found that the activation function (AF) selected for adding non-linearity into the output can have a big impact on how effectively deep learning networks perform. Developing activation functions that can adapt simultaneously with learning is a need of time. Researchers recently started developing activation functions that can be trained throughout the learning process, known as trainable, or adaptive activation functions (AAF). Research on AAF that enhance the outcomes is still in its early stages. In this paper, a novel activation function 'ErfReLU' has been developed based on the erf function and ReLU. This function exploits the ReLU and the error function (erf) to its advantage. State of art activation functions like Sigmoid, ReLU, Tanh, and their properties have been briefly explained. Adaptive activation functions like Tanhsoft1, Tanhsoft2, Tanhsoft3, TanhLU, SAAF, ErfAct, Pserf, Smish, and Serf have also been described. Lastly, performance analysis of 9 traina",
    "path": "papers/23/06/2306.01822.json",
    "total_tokens": 867,
    "translated_title": "ErfReLU: 深度神经网络的自适应激活函数",
    "translated_abstract": "最近的研究发现，选择用于增加非线性的激活函数（AF）对于深度学习网络的有效性有很大影响。开发同时能够随着学习而自适应的激活函数是当前的需要。研究人员最近开始开发能够在学习过程中训练的激活函数，称为可训练或自适应激活函数（AAF）。增强结果的AAF研究仍处于早期阶段。本文提出了一种基于误差函数和ReLU的新型激活函数'ErfReLU'。简要介绍了Sigmoid、ReLU、Tanh等最新激活函数及其相关特性。还介绍了Tanhsoft1、Tanhsoft2、Tanhsoft3、TanhLU、SAAF、ErfAct、Pserf、Smish和Serf等自适应激活函数。最后，分析了9种可训练AAF在各项任务上的表现。",
    "tldr": "本论文提出了一种基于误差函数和ReLU的新型激活函数'ErfReLU'，可以自适应地随着学习而训练，适用于各项任务。",
    "en_tdlr": "This paper proposes a novel activation function 'ErfReLU', based on the error function and ReLU, which can be trained adaptively with learning and is suitable for various tasks."
}