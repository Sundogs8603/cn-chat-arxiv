{
    "title": "I'm Afraid I Can't Do That: Predicting Prompt Refusal in Black-Box Generative Language Models. (arXiv:2306.03423v1 [cs.AI])",
    "abstract": "Since the release of OpenAI's ChatGPT, generative language models have attracted extensive public attention. The increased usage has highlighted generative models' broad utility, but also revealed several forms of embedded bias. Some is induced by the pre-training corpus; but additional bias specific to generative models arises from the use of subjective fine-tuning to avoid generating harmful content. Fine-tuning bias may come from individual engineers and company policies, and affects which prompts the model chooses to refuse. In this experiment, we characterize ChatGPT's refusal behavior using a black-box attack. We first query ChatGPT with a variety of offensive and benign prompts (n=1,730), then manually label each response as compliance or refusal. Manual examination of responses reveals that refusal is not cleanly binary, and lies on a continuum; as such, we map several different kinds of responses to a binary of compliance or refusal. The small manually-labeled dataset is used ",
    "link": "http://arxiv.org/abs/2306.03423",
    "context": "Title: I'm Afraid I Can't Do That: Predicting Prompt Refusal in Black-Box Generative Language Models. (arXiv:2306.03423v1 [cs.AI])\nAbstract: Since the release of OpenAI's ChatGPT, generative language models have attracted extensive public attention. The increased usage has highlighted generative models' broad utility, but also revealed several forms of embedded bias. Some is induced by the pre-training corpus; but additional bias specific to generative models arises from the use of subjective fine-tuning to avoid generating harmful content. Fine-tuning bias may come from individual engineers and company policies, and affects which prompts the model chooses to refuse. In this experiment, we characterize ChatGPT's refusal behavior using a black-box attack. We first query ChatGPT with a variety of offensive and benign prompts (n=1,730), then manually label each response as compliance or refusal. Manual examination of responses reveals that refusal is not cleanly binary, and lies on a continuum; as such, we map several different kinds of responses to a binary of compliance or refusal. The small manually-labeled dataset is used ",
    "path": "papers/23/06/2306.03423.json",
    "total_tokens": 954,
    "translated_title": "我害怕我做不到：预测黑匣子生成语言模型中的提示拒绝行为",
    "translated_abstract": "自从OpenAI的ChatGPT发布以来，生成语言模型引起了广泛关注。增加的使用量凸显了生成模型的广泛实用性，但同时也揭示了一些嵌入式偏见。其中一些是由预训练语料库引起的；但是，针对生成模型的额外偏见来自于主观微调以避免生成有害内容。微调偏见可能来自个别工程师和公司政策，并影响模型选择拒绝哪些提示。本实验使用黑盒攻击，对ChatGPT的拒绝行为进行了表征。我们首先使用各种攻击性和良性提示（n = 1,730）查询ChatGPT，然后手动标记每个响应是否履行或拒绝。响应的手动检查表明，拒绝不是完全二元的，并且在连续的范围内；因此，我们将几种不同类型的响应映射到履行或拒绝的二元值中。使用了小型手动标记的数据集。",
    "tldr": "本文研究了生成语言模型的拒绝行为，并发现这种行为不是完全二元的。作者对ChatGPT进行的实验表明，在微调过程中的偏见来自于个别工程师和公司政策，并影响模型选择拒绝哪些提示。",
    "en_tdlr": "This paper studies the refusal behavior of generative language models and finds that it is not purely binary. The experiment conducted on ChatGPT shows that fine-tuning bias in the process of training comes from individual engineers and company policies, which affects the model's choice of which prompts to refuse."
}