{
    "title": "Resources for Brewing BEIR: Reproducible Reference Models and an Official Leaderboard. (arXiv:2306.07471v1 [cs.IR])",
    "abstract": "BEIR is a benchmark dataset for zero-shot evaluation of information retrieval models across 18 different domain/task combinations. In recent years, we have witnessed the growing popularity of a representation learning approach to building retrieval models, typically using pretrained transformers in a supervised setting. This naturally begs the question: How effective are these models when presented with queries and documents that differ from the training data? Examples include searching in different domains (e.g., medical or legal text) and with different types of queries (e.g., keywords vs. well-formed questions). While BEIR was designed to answer these questions, our work addresses two shortcomings that prevent the benchmark from achieving its full potential: First, the sophistication of modern neural methods and the complexity of current software infrastructure create barriers to entry for newcomers. To this end, we provide reproducible reference implementations that cover the two m",
    "link": "http://arxiv.org/abs/2306.07471",
    "context": "Title: Resources for Brewing BEIR: Reproducible Reference Models and an Official Leaderboard. (arXiv:2306.07471v1 [cs.IR])\nAbstract: BEIR is a benchmark dataset for zero-shot evaluation of information retrieval models across 18 different domain/task combinations. In recent years, we have witnessed the growing popularity of a representation learning approach to building retrieval models, typically using pretrained transformers in a supervised setting. This naturally begs the question: How effective are these models when presented with queries and documents that differ from the training data? Examples include searching in different domains (e.g., medical or legal text) and with different types of queries (e.g., keywords vs. well-formed questions). While BEIR was designed to answer these questions, our work addresses two shortcomings that prevent the benchmark from achieving its full potential: First, the sophistication of modern neural methods and the complexity of current software infrastructure create barriers to entry for newcomers. To this end, we provide reproducible reference implementations that cover the two m",
    "path": "papers/23/06/2306.07471.json",
    "total_tokens": 899,
    "translated_title": "《用于编制BEIR的资源：可重复参考模型和官方排行榜》",
    "translated_abstract": "BEIR是一个跨越18个不同领域/任务组合进行零样本评估的基准数据集，用于信息检索模型。我们目睹了利用预先训练的transformers在监督学习框架下建立检索模型的表示学习方法的日益普及。但这自然会引出一个问题：这些模型在遇到与训练数据不同的查询和文档时有多有效？我们的工作解决了BEIR在实现其全部潜力方面存在的两个缺陷。第一，现代神经方法的复杂性和当前的软件基础设施创建了对新手的进入门槛。为此，我们提供了覆盖两个主要检索模型类的可重复参考实现。第二，虽然BEIR提供了多样化的测试套件，但没有官方排名榜可跟踪模型性能和进展。为解决这个问题，我们为参与者提供了一个官方BEIR排行榜，可提交结果并与最先进的模型进行比较。",
    "tldr": "提供了BEIR基准数据集的可重复参考实现和官方排行榜以跟踪模型性能和进展。",
    "en_tdlr": "Provides reproducible reference implementations and an official leaderboard for the BEIR benchmark dataset for zero-shot evaluation of information retrieval models."
}