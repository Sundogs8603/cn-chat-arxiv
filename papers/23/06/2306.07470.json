{
    "title": "Reviving Shift Equivariance in Vision Transformers. (arXiv:2306.07470v1 [cs.CV])",
    "abstract": "Shift equivariance is a fundamental principle that governs how we perceive the world - our recognition of an object remains invariant with respect to shifts. Transformers have gained immense popularity due to their effectiveness in both language and vision tasks. While the self-attention operator in vision transformers (ViT) is permutation-equivariant and thus shift-equivariant, patch embedding, positional encoding, and subsampled attention in ViT variants can disrupt this property, resulting in inconsistent predictions even under small shift perturbations. Although there is a growing trend in incorporating the inductive bias of convolutional neural networks (CNNs) into vision transformers, it does not fully address the issue. We propose an adaptive polyphase anchoring algorithm that can be seamlessly integrated into vision transformer models to ensure shift-equivariance in patch embedding and subsampled attention modules, such as window attention and global subsampled attention. Furth",
    "link": "http://arxiv.org/abs/2306.07470",
    "context": "Title: Reviving Shift Equivariance in Vision Transformers. (arXiv:2306.07470v1 [cs.CV])\nAbstract: Shift equivariance is a fundamental principle that governs how we perceive the world - our recognition of an object remains invariant with respect to shifts. Transformers have gained immense popularity due to their effectiveness in both language and vision tasks. While the self-attention operator in vision transformers (ViT) is permutation-equivariant and thus shift-equivariant, patch embedding, positional encoding, and subsampled attention in ViT variants can disrupt this property, resulting in inconsistent predictions even under small shift perturbations. Although there is a growing trend in incorporating the inductive bias of convolutional neural networks (CNNs) into vision transformers, it does not fully address the issue. We propose an adaptive polyphase anchoring algorithm that can be seamlessly integrated into vision transformer models to ensure shift-equivariance in patch embedding and subsampled attention modules, such as window attention and global subsampled attention. Furth",
    "path": "papers/23/06/2306.07470.json",
    "total_tokens": 1065,
    "translated_title": "恢复视觉Transformer的平移等变性",
    "translated_abstract": "平移等变性是一种基本原理，它规定了我们如何感知世界-我们对对象的认知在平移方面保持不变。由于它们在语言和视觉任务中的有效性，Transformers变得非常流行。虽然ViT中的自注意力操作是置换等变且因此是平移等变的，但ViT变体中的补丁嵌入、位置编码和子采样注意力可能会破坏这种性质，从而导致即使在小的平移扰动下也会出现不一致的预测。虽然目前将卷积神经网络（CNNs）的归纳偏置融入到视觉Transformer中的趋势正在增长，但这并不能完全解决问题。我们提出了一个自适应的多相位固定算法，可以无缝地集成到视觉Transformer模型中，以确保在补丁嵌入和子采样注意力模块中保持变换等变性，例如窗口注意力和全局子采样注意力。此外，我们介绍了一种基于补丁的适配器，以恢复位置编码的平移等变性。我们的方法简单高效，并显著提高了ViT模型在平移不变数据集（如ImageNet-C和CLEVR-C）上的性能。",
    "tldr": "本文提出了一种自适应的多相位固定算法，可以无缝地集成到视觉Transformer模型中，以确保平移等变性，并介绍了一种基于补丁的适配器，以恢复位置编码的平移等变性。该方法在ImageNet-C和CLEVR-C等平移不变数据集上显著提高了ViT模型的性能。",
    "en_tdlr": "This paper proposes an adaptive polyphase anchoring algorithm to ensure shift-equivariance in patch embedding and subsampled attention modules, and introduces a patch-wise adaptor to restore shift-equivariance to positional encoding in vision transformer models. The method significantly improves the performance of ViT models on shift-invariant datasets such as ImageNet-C and CLEVR-C."
}