{
    "title": "Neural Network Compression using Binarization and Few Full-Precision Weights. (arXiv:2306.08960v2 [cs.CV] UPDATED)",
    "abstract": "Quantization and pruning are two effective Deep Neural Networks model compression methods. In this paper, we propose Automatic Prune Binarization (APB), a novel compression technique combining quantization with pruning. APB enhances the representational capability of binary networks using a few full-precision weights. Our technique jointly maximizes the accuracy of the network while minimizing its memory impact by deciding whether each weight should be binarized or kept in full precision. We show how to efficiently perform a forward pass through layers compressed using APB by decomposing it into a binary and a sparse-dense matrix multiplication. Moreover, we design two novel efficient algorithms for extremely quantized matrix multiplication on CPU, leveraging highly efficient bitwise operations. The proposed algorithms are 6.9x and 1.5x faster than available state-of-the-art solutions. We extensively evaluate APB on two widely adopted model compression datasets, namely CIFAR10 and Imag",
    "link": "http://arxiv.org/abs/2306.08960",
    "context": "Title: Neural Network Compression using Binarization and Few Full-Precision Weights. (arXiv:2306.08960v2 [cs.CV] UPDATED)\nAbstract: Quantization and pruning are two effective Deep Neural Networks model compression methods. In this paper, we propose Automatic Prune Binarization (APB), a novel compression technique combining quantization with pruning. APB enhances the representational capability of binary networks using a few full-precision weights. Our technique jointly maximizes the accuracy of the network while minimizing its memory impact by deciding whether each weight should be binarized or kept in full precision. We show how to efficiently perform a forward pass through layers compressed using APB by decomposing it into a binary and a sparse-dense matrix multiplication. Moreover, we design two novel efficient algorithms for extremely quantized matrix multiplication on CPU, leveraging highly efficient bitwise operations. The proposed algorithms are 6.9x and 1.5x faster than available state-of-the-art solutions. We extensively evaluate APB on two widely adopted model compression datasets, namely CIFAR10 and Imag",
    "path": "papers/23/06/2306.08960.json",
    "total_tokens": 931,
    "translated_title": "使用二进制化和少量全精度权重的神经网络压缩",
    "translated_abstract": "量化和修剪是两种有效的深度神经网络模型压缩方法。本文提出了自动修剪二进制化（APB），这是一种结合了量化和修剪的新型压缩技术。APB利用少量全精度权重增强了二进制网络的表示能力。我们的技术在最大化网络准确性的同时最小化了其内存占用，通过决定每个权重是应该进行二进制化还是保持全精度。我们展示了如何通过将其分解为二进制和稀疏-稠密矩阵乘法来高效地执行使用APB压缩的前向传递。此外，我们在CPU上设计了两种新颖的高效量化矩阵乘法算法，利用了高效的位操作。所提出的算法比现有的最先进解决方案快6.9倍和1.5倍。我们对APB在两个广泛采用的模型压缩数据集CIFAR10和Imag上进行了广泛评估。",
    "tldr": "本文提出了一种自动修剪二进制化（APB）的新型神经网络压缩技术，通过结合量化和修剪，利用少量全精度权重来增强二进制网络的表示能力，并通过高效的算法在CPU上实现了高速的量化矩阵乘法运算。",
    "en_tdlr": "This paper proposes Automatic Prune Binarization (APB), a novel neural network compression technique that combines quantization and pruning. APB enhances the representational capability of binary networks using a few full-precision weights, and achieves high-speed quantized matrix multiplication on CPU."
}