{
    "title": "Multi-market Energy Optimization with Renewables via Reinforcement Learning. (arXiv:2306.08147v1 [cs.LG])",
    "abstract": "This paper introduces a deep reinforcement learning (RL) framework for optimizing the operations of power plants pairing renewable energy with storage. The objective is to maximize revenue from energy markets while minimizing storage degradation costs and renewable curtailment. The framework handles complexities such as time coupling by storage devices, uncertainty in renewable generation and energy prices, and non-linear storage models. The study treats the problem as a hierarchical Markov Decision Process (MDP) and uses component-level simulators for storage. It utilizes RL to incorporate complex storage models, overcoming restrictions of optimization-based methods that require convex and differentiable component models. A significant aspect of this approach is ensuring policy actions respect system constraints, achieved via a novel method of projecting potentially infeasible actions onto a safe state-action set. The paper demonstrates the efficacy of this approach through extensive ",
    "link": "http://arxiv.org/abs/2306.08147",
    "context": "Title: Multi-market Energy Optimization with Renewables via Reinforcement Learning. (arXiv:2306.08147v1 [cs.LG])\nAbstract: This paper introduces a deep reinforcement learning (RL) framework for optimizing the operations of power plants pairing renewable energy with storage. The objective is to maximize revenue from energy markets while minimizing storage degradation costs and renewable curtailment. The framework handles complexities such as time coupling by storage devices, uncertainty in renewable generation and energy prices, and non-linear storage models. The study treats the problem as a hierarchical Markov Decision Process (MDP) and uses component-level simulators for storage. It utilizes RL to incorporate complex storage models, overcoming restrictions of optimization-based methods that require convex and differentiable component models. A significant aspect of this approach is ensuring policy actions respect system constraints, achieved via a novel method of projecting potentially infeasible actions onto a safe state-action set. The paper demonstrates the efficacy of this approach through extensive ",
    "path": "papers/23/06/2306.08147.json",
    "total_tokens": 989,
    "translated_title": "多市场能源优化与强化学习中的可再生能源",
    "translated_abstract": "本文介绍了一种基于深度强化学习框架的功率厂优化方法，通过储能与可再生能源的组合，旨在最大化能源市场的收入、最小化储能损耗成本以及可再生能源数量的削减。该框架处理了储能设备的时间耦合复杂性、可再生能源发电量和能源价格的不确定性以及非线性储能模型等问题。本研究将问题视为分层马尔可夫决策过程，并使用部件级模拟器来处理储能。它利用强化学习的方法来整合复杂的储能模型，克服了需要凸性和可微分的部件模型的优化方法的限制。该方法的重要特点是确保策略动作尊重系统约束，通过将潜在非法动作投影到安全状态动作集上来实现。本文通过广泛的实践证明了该方法的有效性。",
    "tldr": "本文提出了一种基于深度强化学习的功率厂优化方法，通过储能与可再生能源的组合最大化能源市场的收入，最小化储能损耗成本以及可再生能源的削减。通过部件级模拟器处理了储能过程中的时间耦合、不确定性以及非线性存储模型等问题，并通过RL方法处理复杂储能模型，实现了策略动作与系统约束的平衡。"
}