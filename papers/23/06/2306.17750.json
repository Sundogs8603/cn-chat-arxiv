{
    "title": "TD Convergence: An Optimization Perspective. (arXiv:2306.17750v1 [cs.LG])",
    "abstract": "We study the convergence behavior of the celebrated temporal-difference (TD) learning algorithm. By looking at the algorithm through the lens of optimization, we first argue that TD can be viewed as an iterative optimization algorithm where the function to be minimized changes per iteration. By carefully investigating the divergence displayed by TD on a classical counter example, we identify two forces that determine the convergent or divergent behavior of the algorithm. We next formalize our discovery in the linear TD setting with quadratic loss and prove that convergence of TD hinges on the interplay between these two forces. We extend this optimization perspective to prove convergence of TD in a much broader setting than just linear approximation and squared loss. Our results provide a theoretical explanation for the successful application of TD in reinforcement learning.",
    "link": "http://arxiv.org/abs/2306.17750",
    "context": "Title: TD Convergence: An Optimization Perspective. (arXiv:2306.17750v1 [cs.LG])\nAbstract: We study the convergence behavior of the celebrated temporal-difference (TD) learning algorithm. By looking at the algorithm through the lens of optimization, we first argue that TD can be viewed as an iterative optimization algorithm where the function to be minimized changes per iteration. By carefully investigating the divergence displayed by TD on a classical counter example, we identify two forces that determine the convergent or divergent behavior of the algorithm. We next formalize our discovery in the linear TD setting with quadratic loss and prove that convergence of TD hinges on the interplay between these two forces. We extend this optimization perspective to prove convergence of TD in a much broader setting than just linear approximation and squared loss. Our results provide a theoretical explanation for the successful application of TD in reinforcement learning.",
    "path": "papers/23/06/2306.17750.json",
    "total_tokens": 833,
    "translated_title": "TD收敛性：一个优化的视角",
    "translated_abstract": "我们研究了著名的时差(TD)学习算法的收敛特性。通过优化的视角来看待算法，我们首先论证了TD可以被视为一种迭代优化算法，其中每次迭代时要最小化的函数都会发生变化。通过仔细研究TD在经典反例中的发散行为，我们确定了决定算法收敛或发散行为的两个力量。我们还将这一优化视角推广到了比线性逼近和平方损失更广泛的设置中，证明了TD的收敛性取决于这两个力量之间的相互作用。我们的结果为TD在强化学习中的成功应用提供了理论上的解释。",
    "tldr": "本研究从优化的视角研究了时差(TD)学习算法的收敛行为，在经典反例中确定了影响算法收敛或发散的两个力量，并在线性逼近和平方损失以外的情况下证明了TD的收敛性。这一研究为TD在强化学习领域的成功应用提供了理论解释。",
    "en_tdlr": "This study investigates the convergence behavior of the TD learning algorithm from an optimization perspective. By analyzing a classical counter example, the study identifies two forces that determine the convergent or divergent behavior of the algorithm, and proves the convergence of TD in a broader setting. These findings provide a theoretical explanation for the successful application of TD in reinforcement learning."
}