{
    "title": "Morphosyntactic probing of multilingual BERT models. (arXiv:2306.06205v1 [cs.CL])",
    "abstract": "We introduce an extensive dataset for multilingual probing of morphological information in language models (247 tasks across 42 languages from 10 families), each consisting of a sentence with a target word and a morphological tag as the desired label, derived from the Universal Dependencies treebanks. We find that pre-trained Transformer models (mBERT and XLM-RoBERTa) learn features that attain strong performance across these tasks. We then apply two methods to locate, for each probing task, where the disambiguating information resides in the input. The first is a new perturbation method that masks various parts of context; the second is the classical method of Shapley values. The most intriguing finding that emerges is a strong tendency for the preceding context to hold more information relevant to the prediction than the following context.",
    "link": "http://arxiv.org/abs/2306.06205",
    "context": "Title: Morphosyntactic probing of multilingual BERT models. (arXiv:2306.06205v1 [cs.CL])\nAbstract: We introduce an extensive dataset for multilingual probing of morphological information in language models (247 tasks across 42 languages from 10 families), each consisting of a sentence with a target word and a morphological tag as the desired label, derived from the Universal Dependencies treebanks. We find that pre-trained Transformer models (mBERT and XLM-RoBERTa) learn features that attain strong performance across these tasks. We then apply two methods to locate, for each probing task, where the disambiguating information resides in the input. The first is a new perturbation method that masks various parts of context; the second is the classical method of Shapley values. The most intriguing finding that emerges is a strong tendency for the preceding context to hold more information relevant to the prediction than the following context.",
    "path": "papers/23/06/2306.06205.json",
    "total_tokens": 857,
    "translated_title": "多语言BERT模型的形态句法探测",
    "translated_abstract": "我们介绍了一个广泛的数据集，用于对语言模型中形态信息进行多语言探测（来自10个族群的42种语言中的247项任务），每个任务包括一个带有目标单词和形态标签的句子作为期望的标签，来自Universal Dependencies树库。我们发现预训练的Transformer模型（mBERT和XLM-RoBERTa）学习的特征在这些任务中具有强大的表现。然后，我们应用了两种方法来定位每个探测任务中的决策信息所在的位置。第一种是一种新的扰动方法，可以遮蔽上下文的各个部分；第二种是Shapley值的经典方法。最引人注目的发现是，前面的上下文比后面的上下文包含更多与预测相关的信息。",
    "tldr": "该论文介绍了一个广泛的数据集，用于在42种语言中探测语言模型的形态句法信息。研究发现，预训练的Transformer模型（mBERT和XLM-RoBERTa）在这些任务中表现出色，而前面的上下文比后面的上下文包含更多与预测相关的信息。",
    "en_tdlr": "This paper introduces an extensive dataset for multilingual probing of morphological information in language models and finds that pre-trained Transformer models (mBERT and XLM-RoBERTa) attain strong performance across the tasks, with a strong tendency for the preceding context to hold more information relevant to the prediction than the following context."
}