{
    "title": "Adversarial Robustness of Prompt-based Few-Shot Learning for Natural Language Understanding. (arXiv:2306.11066v2 [cs.CL] UPDATED)",
    "abstract": "State-of-the-art few-shot learning (FSL) methods leverage prompt-based fine-tuning to obtain remarkable results for natural language understanding (NLU) tasks. While much of the prior FSL methods focus on improving downstream task performance, there is a limited understanding of the adversarial robustness of such methods. In this work, we conduct an extensive study of several state-of-the-art FSL methods to assess their robustness to adversarial perturbations. To better understand the impact of various factors towards robustness (or the lack of it), we evaluate prompt-based FSL methods against fully fine-tuned models for aspects such as the use of unlabeled data, multiple prompts, number of few-shot examples, model size and type. Our results on six GLUE tasks indicate that compared to fully fine-tuned models, vanilla FSL methods lead to a notable relative drop in task performance (i.e., are less robust) in the face of adversarial perturbations. However, using (i) unlabeled data for pro",
    "link": "http://arxiv.org/abs/2306.11066",
    "context": "Title: Adversarial Robustness of Prompt-based Few-Shot Learning for Natural Language Understanding. (arXiv:2306.11066v2 [cs.CL] UPDATED)\nAbstract: State-of-the-art few-shot learning (FSL) methods leverage prompt-based fine-tuning to obtain remarkable results for natural language understanding (NLU) tasks. While much of the prior FSL methods focus on improving downstream task performance, there is a limited understanding of the adversarial robustness of such methods. In this work, we conduct an extensive study of several state-of-the-art FSL methods to assess their robustness to adversarial perturbations. To better understand the impact of various factors towards robustness (or the lack of it), we evaluate prompt-based FSL methods against fully fine-tuned models for aspects such as the use of unlabeled data, multiple prompts, number of few-shot examples, model size and type. Our results on six GLUE tasks indicate that compared to fully fine-tuned models, vanilla FSL methods lead to a notable relative drop in task performance (i.e., are less robust) in the face of adversarial perturbations. However, using (i) unlabeled data for pro",
    "path": "papers/23/06/2306.11066.json",
    "total_tokens": 1032,
    "translated_title": "基于提示的少样本学习在自然语言理解中的对抗鲁棒性研究",
    "translated_abstract": "现有的少样本学习（Few-shot learning，FSL）方法通过基于提示的微调在自然语言理解（NLU）任务上取得了显著的结果。本文对几种最先进的FSL方法进行了广泛的研究，以评估它们对于对抗扰动的鲁棒性。研究表明，相对于全微调模型，纯FSL方法面对对抗扰动会出现明显的任务性能下降，但使用无标签数据进行提示生成或在训练中使用多个提示，可以在某些情况下显著提高FSL方法的鲁棒性，甚至超过全微调模型的性能表现。此外，我们发现模型大小和类型选择在对抗鲁棒性上也有显著的影响。",
    "tldr": "本文针对几种FSL方法的鲁棒性指出，与全微调模型相比，纯FSL模型面对对抗扰动会带来不可忽视的任务性能下降。但使用无标签数据生成提示或使用多个提示可以显著提高鲁棒性，在某些情况下甚至胜过全微调模型。",
    "en_tdlr": "This paper extensively studies the adversarial robustness of several state-of-the-art prompt-based FSL methods for NLU tasks, and finds out that compared to fully fine-tuned models, vanilla FSL methods suffer a notable drop in task performance under adversarial perturbations. However, using unlabeled data for prompt generation or multiple prompts during training can significantly improve the robustness of FSL methods, even outperforming fully fine-tuned models in some cases. The choice of model size and type also plays a significant role in adversarial robustness."
}