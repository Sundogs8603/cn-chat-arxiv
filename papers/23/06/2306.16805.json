{
    "title": "CLIPAG: Towards Generator-Free Text-to-Image Generation. (arXiv:2306.16805v1 [cs.CV])",
    "abstract": "Perceptually Aligned Gradients (PAG) refer to an intriguing property observed in robust image classification models, wherein their input gradients align with human perception and pose semantic meanings. While this phenomenon has gained significant research attention, it was solely studied in the context of unimodal vision-only architectures. In this work, we extend the study of PAG to Vision-Language architectures, which form the foundations for diverse image-text tasks and applications. Through an adversarial robustification finetuning of CLIP, we demonstrate that robust Vision-Language models exhibit PAG in contrast to their vanilla counterparts. This work reveals the merits of CLIP with PAG (CLIPAG) in several vision-language generative tasks. Notably, we show that seamlessly integrating CLIPAG in a \"plug-n-play\" manner leads to substantial improvements in vision-language generative applications. Furthermore, leveraging its PAG property, CLIPAG enables text-to-image generation witho",
    "link": "http://arxiv.org/abs/2306.16805",
    "context": "Title: CLIPAG: Towards Generator-Free Text-to-Image Generation. (arXiv:2306.16805v1 [cs.CV])\nAbstract: Perceptually Aligned Gradients (PAG) refer to an intriguing property observed in robust image classification models, wherein their input gradients align with human perception and pose semantic meanings. While this phenomenon has gained significant research attention, it was solely studied in the context of unimodal vision-only architectures. In this work, we extend the study of PAG to Vision-Language architectures, which form the foundations for diverse image-text tasks and applications. Through an adversarial robustification finetuning of CLIP, we demonstrate that robust Vision-Language models exhibit PAG in contrast to their vanilla counterparts. This work reveals the merits of CLIP with PAG (CLIPAG) in several vision-language generative tasks. Notably, we show that seamlessly integrating CLIPAG in a \"plug-n-play\" manner leads to substantial improvements in vision-language generative applications. Furthermore, leveraging its PAG property, CLIPAG enables text-to-image generation witho",
    "path": "papers/23/06/2306.16805.json",
    "total_tokens": 962,
    "translated_title": "CLIPAG: 走向无需生成器的文本到图像生成",
    "translated_abstract": "感知对齐梯度 (Perceptually Aligned Gradients, PAG) 是在健壮的图像分类模型中观察到的一种有趣属性，其中它们的输入渐变与人类感知对齐并具有语义意义。虽然这一现象引起了显着的研究关注，但仅仅在单模态纯视觉架构的背景下进行了研究。在本研究中，我们将 PAG 的研究扩展到视觉-语言架构，这是多样化的图像-文本任务和应用的基础。通过对 CLIP 进行对抗性鲁棒微调，我们证明了鲁棒的视觉-语言模型相对于其基准模型表现出了 PAG。这项工作展示了 CLIPAG 在几种视觉-语言生成任务中的优势。值得注意的是，我们展示了无缝集成 CLIPAG 的 \"即插即用\" 方式显著改进了视觉-语言生成应用。此外，利用其 PAG 属性，CLIPAG 实现了无生成器的文本到图像生成。",
    "tldr": "本文将感知对齐梯度（PAG）的研究扩展到视觉-语言架构，并通过对 CLIP 进行鲁棒性调整，展示了在视觉-语言生成任务中集成 CLIPAG 可以实现显著改进，并实现了无生成器的文本到图像生成。",
    "en_tdlr": "This paper extends the study of perceptually aligned gradients (PAG) to vision-language architectures and demonstrates significant improvements in vision-language generative tasks by integrating CLIPAG, achieved through adversarial robustification finetuning of CLIP, while enabling generator-free text-to-image generation."
}