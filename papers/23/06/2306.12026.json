{
    "title": "Continual Learners are Incremental Model Generalizers. (arXiv:2306.12026v1 [cs.LG])",
    "abstract": "Motivated by the efficiency and rapid convergence of pre-trained models for solving downstream tasks, this paper extensively studies the impact of Continual Learning (CL) models as pre-trainers. In both supervised and unsupervised CL, we find that the transfer quality of the representation often increases gradually without noticeable degradation in fine-tuning performance. This is because CL models can learn improved task-general features when easily forgetting task-specific knowledge. Based on this observation, we suggest a new unsupervised CL framework with masked modeling, which aims to capture fluent task-generic representation during training. Furthermore, we propose a new fine-tuning scheme, GLobal Attention Discretization (GLAD), that preserves rich task-generic representation during solving downstream tasks. The model fine-tuned with GLAD achieves competitive performance and can also be used as a good pre-trained model itself. We believe this paper breaks the barriers between p",
    "link": "http://arxiv.org/abs/2306.12026",
    "context": "Title: Continual Learners are Incremental Model Generalizers. (arXiv:2306.12026v1 [cs.LG])\nAbstract: Motivated by the efficiency and rapid convergence of pre-trained models for solving downstream tasks, this paper extensively studies the impact of Continual Learning (CL) models as pre-trainers. In both supervised and unsupervised CL, we find that the transfer quality of the representation often increases gradually without noticeable degradation in fine-tuning performance. This is because CL models can learn improved task-general features when easily forgetting task-specific knowledge. Based on this observation, we suggest a new unsupervised CL framework with masked modeling, which aims to capture fluent task-generic representation during training. Furthermore, we propose a new fine-tuning scheme, GLobal Attention Discretization (GLAD), that preserves rich task-generic representation during solving downstream tasks. The model fine-tuned with GLAD achieves competitive performance and can also be used as a good pre-trained model itself. We believe this paper breaks the barriers between p",
    "path": "papers/23/06/2306.12026.json",
    "total_tokens": 960,
    "translated_title": "Continual学习是渐进的模型泛化器",
    "translated_abstract": "本论文通过大量研究Continual Learning（CL）模型作为预训练器对下游任务的影响，探讨了其高效性和快速收敛的动机。在有监督和无监督的CL中，我们发现表示的转移质量通常会逐渐提高而不会影响微调性能的下降。这是因为当容易忘记特定任务的知识时，CL模型能够学习到更好的任务一般特征。基于这一观察，本文提出一种新的无监督CL框架，使用掩蔽建模旨在在训练期间捕获流畅的任务通用表示。此外，我们提出了一种新的微调方案GLobal Attention Discretization（GLAD），在解决下游任务时保留丰富的任务通用表示。使用GLAD微调的模型具有竞争性能，并且本身也可以用作较好的预训练模型。我们相信本文打破了p之间的障碍。",
    "tldr": "本文研究了continual learners作为预训练器对下游任务的影响，提出了一种新的无监督CL框架和微调方案GLAD，可用作较好的预训练模型本身。通过学习改善通用性特征，在容易忘记特定任务知识时，CL模型可逐步提高表示的转移质量而不影响微调性能。",
    "en_tdlr": "This paper studies the impact of continual learners as pre-trainers on downstream tasks and proposes a new unsupervised CL framework and fine-tuning scheme called GLAD, which can be used as a good pre-trained model itself. CL models can gradually improve the transfer quality of representations without affecting fine-tuning performance by learning improved task-general features when easily forgetting task-specific knowledge."
}