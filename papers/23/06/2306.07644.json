{
    "title": "SRATTA : Sample Re-ATTribution Attack of Secure Aggregation in Federated Learning. (arXiv:2306.07644v1 [cs.LG])",
    "abstract": "We consider a cross-silo federated learning (FL) setting where a machine learning model with a fully connected first layer is trained between different clients and a central server using FedAvg, and where the aggregation step can be performed with secure aggregation (SA). We present SRATTA an attack relying only on aggregated models which, under realistic assumptions, (i) recovers data samples from the different clients, and (ii) groups data samples coming from the same client together. While sample recovery has already been explored in an FL setting, the ability to group samples per client, despite the use of SA, is novel. This poses a significant unforeseen security threat to FL and effectively breaks SA. We show that SRATTA is both theoretically grounded and can be used in practice on realistic models and datasets. We also propose counter-measures, and claim that clients should play an active role to guarantee their privacy during training.",
    "link": "http://arxiv.org/abs/2306.07644",
    "context": "Title: SRATTA : Sample Re-ATTribution Attack of Secure Aggregation in Federated Learning. (arXiv:2306.07644v1 [cs.LG])\nAbstract: We consider a cross-silo federated learning (FL) setting where a machine learning model with a fully connected first layer is trained between different clients and a central server using FedAvg, and where the aggregation step can be performed with secure aggregation (SA). We present SRATTA an attack relying only on aggregated models which, under realistic assumptions, (i) recovers data samples from the different clients, and (ii) groups data samples coming from the same client together. While sample recovery has already been explored in an FL setting, the ability to group samples per client, despite the use of SA, is novel. This poses a significant unforeseen security threat to FL and effectively breaks SA. We show that SRATTA is both theoretically grounded and can be used in practice on realistic models and datasets. We also propose counter-measures, and claim that clients should play an active role to guarantee their privacy during training.",
    "path": "papers/23/06/2306.07644.json",
    "total_tokens": 952,
    "translated_title": "SRATTA: 在联邦学习中针对安全聚合的样本重新归属攻击",
    "translated_abstract": "我们考虑了一个跨边缘联邦学习（FL）设置，在这个设置中，使用FedAvg训练了一个具有完全连接的第一层的机器学习模型，该模型在不同的客户端和中央服务器之间进行训练，并且可以使用安全聚合（SA）进行聚合步骤。我们提出了SRATTA攻击，仅依赖于聚合模型，根据现实假设，（i）从不同的客户端恢复数据样本，并且（ii）将来自同一客户端的数据样本组合在一起。虽然在FL设置中已经探索了样本恢复，但尽管使用了SA，将样本按客户分组的能力仍然是新颖的。这对于FL构成了重要的未预见安全威胁，并有效地破坏了SA。我们展示了SRATTA的理论基础，并且可以在实际模型和数据集上使用。我们还提出了反制措施，并声称客户端应该在训练期间发挥积极作用以保证其隐私。",
    "tldr": "SRATTA是一个新型的联邦学习攻击，可以在安全聚合下恢复客户端样本数据并将其按客户端分组，对联邦学习构成重要的安全威胁，需要客户端积极保护隐私并采取反制措施。",
    "en_tdlr": "SRATTA is a novel attack in federated learning, which can recover client samples and group them per client even under secure aggregation, posing a significant security threat to FL. Counter-measures should be taken and clients should actively protect their privacy during training."
}