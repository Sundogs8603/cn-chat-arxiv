{
    "title": "Opening the Black Box: Analyzing Attention Weights and Hidden States in Pre-trained Language Models for Non-language Tasks. (arXiv:2306.12198v1 [cs.CL])",
    "abstract": "Investigating deep learning language models has always been a significant research area due to the ``black box\" nature of most advanced models. With the recent advancements in pre-trained language models based on transformers and their increasing integration into daily life, addressing this issue has become more pressing. In order to achieve an explainable AI model, it is essential to comprehend the procedural steps involved and compare them with human thought processes. Thus, in this paper, we use simple, well-understood non-language tasks to explore these models' inner workings. Specifically, we apply a pre-trained language model to constrained arithmetic problems with hierarchical structure, to analyze their attention weight scores and hidden states. The investigation reveals promising results, with the model addressing hierarchical problems in a moderately structured manner, similar to human problem-solving strategies. Additionally, by inspecting the attention weights layer by laye",
    "link": "http://arxiv.org/abs/2306.12198",
    "context": "Title: Opening the Black Box: Analyzing Attention Weights and Hidden States in Pre-trained Language Models for Non-language Tasks. (arXiv:2306.12198v1 [cs.CL])\nAbstract: Investigating deep learning language models has always been a significant research area due to the ``black box\" nature of most advanced models. With the recent advancements in pre-trained language models based on transformers and their increasing integration into daily life, addressing this issue has become more pressing. In order to achieve an explainable AI model, it is essential to comprehend the procedural steps involved and compare them with human thought processes. Thus, in this paper, we use simple, well-understood non-language tasks to explore these models' inner workings. Specifically, we apply a pre-trained language model to constrained arithmetic problems with hierarchical structure, to analyze their attention weight scores and hidden states. The investigation reveals promising results, with the model addressing hierarchical problems in a moderately structured manner, similar to human problem-solving strategies. Additionally, by inspecting the attention weights layer by laye",
    "path": "papers/23/06/2306.12198.json",
    "total_tokens": 909,
    "translated_title": "揭开黑匣子：分析预训练语言模型在非语言任务中的注意力权重和隐藏状态",
    "translated_abstract": "由于大多数先进模型的“黑匣子”特性，研究深度学习语言模型一直是一个重要的研究领域。随着基于transformers的预训练语言模型的最近进展及其在日常生活中的不断集成，解决这个问题变得更加紧迫。为了实现可解释的AI模型，必须理解涉及的过程步骤，并将其与人类思维过程进行比较。因此，在本文中，我们使用简单易懂的非语言任务来探索这些模型的内部运作。具体来说，我们将预训练语言模型应用于具有分层结构的约束算术问题，以分析其注意力权重分数和隐藏状态。调查结果显示出令人鼓舞的结果，模型以略微结构化的方式解决分层问题，类似于人类解决问题的策略。",
    "tldr": "本论文揭示了预训练语言模型在非语言任务中的内部运作，具体使用约束算术问题探索模型的注意力权重分数和隐藏状态，并发现了有前途的结果，该模型以略微结构化的方式解决分层问题，类似于人类解决问题的策略。",
    "en_tdlr": "This paper reveals the inner workings of pre-trained language models in non-language tasks by using constrained arithmetic problems to explore attention weight scores and hidden states, and uncovers promising results showing that the model addresses hierarchical problems in a moderately structured manner, similar to human problem-solving strategies."
}