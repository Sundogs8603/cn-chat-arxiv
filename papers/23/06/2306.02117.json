{
    "title": "Oversmoothing: A Nightmare for Graph Contrastive Learning?",
    "abstract": "arXiv:2306.02117v2 Announce Type: replace-cross  Abstract: Oversmoothing is a common phenomenon observed in graph neural networks (GNNs), in which an increase in the network depth leads to a deterioration in their performance. Graph contrastive learning (GCL) is emerging as a promising way of leveraging vast unlabeled graph data. As a marriage between GNNs and contrastive learning, it remains unclear whether GCL inherits the same oversmoothing defect from GNNs. This work undertakes a fundamental analysis of GCL from the perspective of oversmoothing on the first hand. We demonstrate empirically that increasing network depth in GCL also leads to oversmoothing in their deep representations, and surprisingly, the shallow ones. We refer to this phenomenon in GCL as `long-range starvation', wherein lower layers in deep networks suffer from degradation due to the lack of sufficient guidance from supervision. Based on our findings, we present BlockGCL, a remarkably simple yet effective blockwi",
    "link": "https://arxiv.org/abs/2306.02117",
    "context": "Title: Oversmoothing: A Nightmare for Graph Contrastive Learning?\nAbstract: arXiv:2306.02117v2 Announce Type: replace-cross  Abstract: Oversmoothing is a common phenomenon observed in graph neural networks (GNNs), in which an increase in the network depth leads to a deterioration in their performance. Graph contrastive learning (GCL) is emerging as a promising way of leveraging vast unlabeled graph data. As a marriage between GNNs and contrastive learning, it remains unclear whether GCL inherits the same oversmoothing defect from GNNs. This work undertakes a fundamental analysis of GCL from the perspective of oversmoothing on the first hand. We demonstrate empirically that increasing network depth in GCL also leads to oversmoothing in their deep representations, and surprisingly, the shallow ones. We refer to this phenomenon in GCL as `long-range starvation', wherein lower layers in deep networks suffer from degradation due to the lack of sufficient guidance from supervision. Based on our findings, we present BlockGCL, a remarkably simple yet effective blockwi",
    "path": "papers/23/06/2306.02117.json",
    "total_tokens": 874,
    "translated_title": "过度平滑：图对比学习的噩梦？",
    "translated_abstract": "过度平滑是图神经网络（GNNs）中常见的现象，即网络深度的增加导致性能下降。图对比学习（GCL）正日益成为利用大量未标记图数据的一种有前途的方式。作为GNNs和对比学习的融合，尚不清楚GCL是否会继承GNNs的过度平滑缺陷。本文从过度平滑的角度对GCL进行了基础分析。我们通过实验证明，在GCL中增加网络深度也会导致它们的深度表示过度平滑，而且令人惊讶的是浅层也会出现这种现象。我们将这种现象在GCL中称为“长距离饥饿”，即深度网络中的较低层由于缺乏来自监督的充分指导而遭受退化。根据我们的研究结果，我们提出了BlockGCL，这是一个非常简单但有效的块wi",
    "tldr": "对于图对比学习（GCL），研究表明增加网络深度会导致过度平滑，包括深度表示和浅层，提出了BlockGCL解决这一问题",
    "en_tdlr": "This work uncovers the oversmoothing issue in Graph Contrastive Learning (GCL), showing that increasing network depth leads to oversmoothing in both deep and shallow representations, and introduces BlockGCL as a solution."
}