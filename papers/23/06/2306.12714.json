{
    "title": "Toward Leveraging Pre-Trained Self-Supervised Frontends for Automatic Singing Voice Understanding Tasks: Three Case Studies. (arXiv:2306.12714v1 [cs.SD])",
    "abstract": "Automatic singing voice understanding tasks, such as singer identification, singing voice transcription, and singing technique classification, benefit from data-driven approaches that utilize deep learning techniques. These approaches work well even under the rich diversity of vocal and noisy samples owing to their representation ability. However, the limited availability of labeled data remains a significant obstacle to achieving satisfactory performance. In recent years, self-supervised learning models (SSL models) have been trained using large amounts of unlabeled data in the field of speech processing and music classification. By fine-tuning these models for the target tasks, comparable performance to conventional supervised learning can be achieved with limited training data. Therefore, in this paper, we investigate the effectiveness of SSL models for various singing voice recognition tasks. We report the results of experiments comparing SSL models for three different tasks (i.e.,",
    "link": "http://arxiv.org/abs/2306.12714",
    "context": "Title: Toward Leveraging Pre-Trained Self-Supervised Frontends for Automatic Singing Voice Understanding Tasks: Three Case Studies. (arXiv:2306.12714v1 [cs.SD])\nAbstract: Automatic singing voice understanding tasks, such as singer identification, singing voice transcription, and singing technique classification, benefit from data-driven approaches that utilize deep learning techniques. These approaches work well even under the rich diversity of vocal and noisy samples owing to their representation ability. However, the limited availability of labeled data remains a significant obstacle to achieving satisfactory performance. In recent years, self-supervised learning models (SSL models) have been trained using large amounts of unlabeled data in the field of speech processing and music classification. By fine-tuning these models for the target tasks, comparable performance to conventional supervised learning can be achieved with limited training data. Therefore, in this paper, we investigate the effectiveness of SSL models for various singing voice recognition tasks. We report the results of experiments comparing SSL models for three different tasks (i.e.,",
    "path": "papers/23/06/2306.12714.json",
    "total_tokens": 1142,
    "translated_title": "利用预训练的自监督前端实现歌唱声音自动理解任务：三个案例研究",
    "translated_abstract": "采用深度学习技术的数据驱动方法对自动歌唱声音理解任务，如歌手识别、歌声转录和歌唱技巧分类等方面具有表征能力，即使在具有丰富人声和噪声样本的情况下也能发挥良好的作用。然而，有限的标注数据仍然是实现令人满意的性能的重要障碍。近年来，自监督学习模型（SSL模型）在语音处理和音乐分类领域经过大量未标注数据的训练。通过微调这些模型以用于目标任务，即使在有限的训练数据下也能实现与常规监督学习相当的性能。因此，本文研究了SSL模型在各种歌唱声音识别任务中的有效性。我们报告了比较三个不同任务（即歌手识别、歌声转录和歌唱技巧分类）的SSL模型和常规监督学习模型的实验结果。我们的实验表明，SSL模型在所有任务中均优于有监督学习模型。此外，我们表明，将在大量未标注数据上训练的自监督前端转移到目标任务可以比从头开始训练取得更好的性能。这些结果说明了自监督学习在提高歌唱声音理解任务的数据效率和性能方面的潜力。",
    "tldr": "本文研究了利用自监督学习模型（SSL模型）进行歌唱声音理解任务的有效性，并展示了将自监督前端转移到目标任务可以取得更好性能的潜力。此外，SSL模型在所有任务中均优于常规监督学习模型。",
    "en_tdlr": "This paper investigates the effectiveness of self-supervised learning models (SSL models) for singing voice understanding tasks and demonstrates the potential of transferring self-supervised frontends to target tasks for better performance. In addition, SSL models outperform conventional supervised learning models in all tasks."
}