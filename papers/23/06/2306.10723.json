{
    "title": "Fine-tuning Large Enterprise Language Models via Ontological Reasoning. (arXiv:2306.10723v2 [cs.CL] UPDATED)",
    "abstract": "Large Language Models (LLMs) exploit fine-tuning as a technique to adapt to diverse goals, thanks to task-specific training data. Task specificity should go hand in hand with domain orientation, that is, the specialization of an LLM to accurately address the tasks of a given realm of interest. However, models are usually fine-tuned over publicly available data or, at most, over ground data from databases, ignoring business-level definitions and domain experience. On the other hand, Enterprise Knowledge Graphs (EKGs) are able to capture and augment such domain knowledge via ontological reasoning. With the goal of combining LLM flexibility with the domain orientation of EKGs, we propose a novel neurosymbolic architecture that leverages the power of ontological reasoning to build task- and domain-specific corpora for LLM fine-tuning.",
    "link": "http://arxiv.org/abs/2306.10723",
    "context": "Title: Fine-tuning Large Enterprise Language Models via Ontological Reasoning. (arXiv:2306.10723v2 [cs.CL] UPDATED)\nAbstract: Large Language Models (LLMs) exploit fine-tuning as a technique to adapt to diverse goals, thanks to task-specific training data. Task specificity should go hand in hand with domain orientation, that is, the specialization of an LLM to accurately address the tasks of a given realm of interest. However, models are usually fine-tuned over publicly available data or, at most, over ground data from databases, ignoring business-level definitions and domain experience. On the other hand, Enterprise Knowledge Graphs (EKGs) are able to capture and augment such domain knowledge via ontological reasoning. With the goal of combining LLM flexibility with the domain orientation of EKGs, we propose a novel neurosymbolic architecture that leverages the power of ontological reasoning to build task- and domain-specific corpora for LLM fine-tuning.",
    "path": "papers/23/06/2306.10723.json",
    "total_tokens": 764,
    "translated_title": "通过本体推理对大型企业语言模型进行微调",
    "translated_abstract": "大型语言模型利用微调作为一种技术，通过任务特定的训练数据来适应多样化的目标。任务特定性应该与领域定向相结合，即将语言模型专门化，以准确地处理给定领域的任务。然而，模型通常通过公开可用的数据进行微调，或者最多只通过数据库中的基础数据进行微调，而忽略了基于业务的定义和领域经验。另一方面，企业知识图谱能够通过本体推理捕获和增加这种领域知识。为了将语言模型的灵活性与企业知识图谱的领域定向相结合，我们提出了一种新颖的神经符号架构，利用本体推理的能力为语言模型的微调构建任务和领域特定的语料库。",
    "tldr": "通过本体推理构建任务和领域特定的语料库，对大型企业语言模型进行微调。",
    "en_tdlr": "Fine-tune large enterprise language models by leveraging ontological reasoning to build task- and domain-specific corpora."
}