{
    "title": "Answer Mining from a Pool of Images: Towards Retrieval-Based Visual Question Answering. (arXiv:2306.16713v1 [cs.CV])",
    "abstract": "We study visual question answering in a setting where the answer has to be mined from a pool of relevant and irrelevant images given as a context. For such a setting, a model must first retrieve relevant images from the pool and answer the question from these retrieved images. We refer to this problem as retrieval-based visual question answering (or RETVQA in short). The RETVQA is distinctively different and more challenging than the traditionally-studied Visual Question Answering (VQA), where a given question has to be answered with a single relevant image in context. Towards solving the RETVQA task, we propose a unified Multi Image BART (MI-BART) that takes a question and retrieved images using our relevance encoder for free-form fluent answer generation. Further, we introduce the largest dataset in this space, namely RETVQA, which has the following salient features: multi-image and retrieval requirement for VQA, metadata-independent questions over a pool of heterogeneous images, exp",
    "link": "http://arxiv.org/abs/2306.16713",
    "context": "Title: Answer Mining from a Pool of Images: Towards Retrieval-Based Visual Question Answering. (arXiv:2306.16713v1 [cs.CV])\nAbstract: We study visual question answering in a setting where the answer has to be mined from a pool of relevant and irrelevant images given as a context. For such a setting, a model must first retrieve relevant images from the pool and answer the question from these retrieved images. We refer to this problem as retrieval-based visual question answering (or RETVQA in short). The RETVQA is distinctively different and more challenging than the traditionally-studied Visual Question Answering (VQA), where a given question has to be answered with a single relevant image in context. Towards solving the RETVQA task, we propose a unified Multi Image BART (MI-BART) that takes a question and retrieved images using our relevance encoder for free-form fluent answer generation. Further, we introduce the largest dataset in this space, namely RETVQA, which has the following salient features: multi-image and retrieval requirement for VQA, metadata-independent questions over a pool of heterogeneous images, exp",
    "path": "papers/23/06/2306.16713.json",
    "total_tokens": 1004,
    "translated_title": "来自图像池的答案挖掘：面向基于检索的视觉问答",
    "translated_abstract": "我们研究在一个给定上下文的相关和无关图像池中挖掘答案的视觉问答问题。在这样的设置中，模型必须首先从图像池中检索相关图像，并从这些检索到的图像中回答问题。我们将这个问题称为基于检索的视觉问答（或简称为RETVQA）。RETVQA与传统研究的视觉问答（VQA）有着明显不同和更大的挑战，传统的VQA要求根据上下文中的单个相关图像回答给定的问题。为了解决RETVQA任务，我们提出了一个统一的Multi Image BART（MI-BART）模型，该模型使用我们的相关性编码器来生成自由流畅的答案。此外，我们还引入了这个领域最大的数据集，即RETVQA，具有以下显著特点：VQA的多图像和检索要求，对一组异构图像进行元数据无关的问题提问。",
    "tldr": "本研究探究了在给定上下文中从相关和无关图像池中挖掘答案的视觉问答问题。我们提出了一个统一的模型，Multi Image BART (MI-BART)，通过检索相关图像并使用相关性编码器进行自由流畅的答案生成。同时，我们还引入了最大的RETVQA数据集，该数据集具有多图像和检索要求，并且可以对一组异构图像进行元数据无关的问题提问。",
    "en_tdlr": "This study explores visual question answering in the scenario where the answer needs to be retrieved from a pool of relevant and irrelevant images provided as context. The proposed Multi Image BART (MI-BART) model utilizes a relevance encoder to retrieve relevant images and generate fluent answers. Additionally, the introduction of the largest RETVQA dataset with multi-image and retrieval requirements enables metadata-independent questioning over a set of heterogeneous images."
}