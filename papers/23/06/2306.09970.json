{
    "title": "HePCo: Data-Free Heterogeneous Prompt Consolidation for Continual Federated Learning. (arXiv:2306.09970v1 [cs.CV])",
    "abstract": "In this paper, we focus on the important yet understudied problem of Continual Federated Learning (CFL), where a server communicates with a set of clients to incrementally learn new concepts over time without sharing or storing any data. The complexity of this problem is compounded by challenges from both the Continual and Federated Learning perspectives. Specifically, models trained in a CFL setup suffer from catastrophic forgetting which is exacerbated by data heterogeneity across clients. Existing attempts at this problem tend to impose large overheads on clients and communication channels or require access to stored data which renders them unsuitable for real-world use due to privacy. In this paper, we attempt to tackle forgetting and heterogeneity while minimizing overhead costs and without requiring access to any stored data. We achieve this by leveraging a prompting based approach (such that only prompts and classifier heads have to be communicated) and proposing a novel and lig",
    "link": "http://arxiv.org/abs/2306.09970",
    "context": "Title: HePCo: Data-Free Heterogeneous Prompt Consolidation for Continual Federated Learning. (arXiv:2306.09970v1 [cs.CV])\nAbstract: In this paper, we focus on the important yet understudied problem of Continual Federated Learning (CFL), where a server communicates with a set of clients to incrementally learn new concepts over time without sharing or storing any data. The complexity of this problem is compounded by challenges from both the Continual and Federated Learning perspectives. Specifically, models trained in a CFL setup suffer from catastrophic forgetting which is exacerbated by data heterogeneity across clients. Existing attempts at this problem tend to impose large overheads on clients and communication channels or require access to stored data which renders them unsuitable for real-world use due to privacy. In this paper, we attempt to tackle forgetting and heterogeneity while minimizing overhead costs and without requiring access to any stored data. We achieve this by leveraging a prompting based approach (such that only prompts and classifier heads have to be communicated) and proposing a novel and lig",
    "path": "papers/23/06/2306.09970.json",
    "total_tokens": 915,
    "translated_title": "HePCo：用于连续联邦学习的无数据异构提示合并方法",
    "translated_abstract": "本文研究了连续联邦学习的重要但鲜为人知的问题。在这种情况下，服务器与一组客户端通信，以逐步学习新的概念，同时不共享或存储任何数据。由于来自连续和联邦学习角度的挑战，此问题的复杂性受到了加剧。本文尝试在不需要访问任何存储数据的情况下解决遗忘和异构问题，同时最小化开销。我们通过采用一种基于提示的方法并提出一种名为HePCo的新颖轻量级提示合并算法，实现了此目标。我们的方法在真实数据集和合成数据集上均能取得最先进的结果并保持低通信开销，同时不影响数据隐私。",
    "tldr": "本文提出了一种名为HePCo的轻量级提示合并算法，解决了在连续联邦学习中的数据异构和遗忘问题，并在不共享或存储任何数据的情况下最小化了通信开销。在真实数据集和合成数据集上实现了最先进的结果，并且保持了数据隐私。"
}