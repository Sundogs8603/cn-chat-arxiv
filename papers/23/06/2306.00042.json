{
    "title": "Graph-based methods coupled with specific distributional distances for adversarial attack detection. (arXiv:2306.00042v1 [cs.LG])",
    "abstract": "Artificial neural networks are prone to being fooled by carefully perturbed inputs which cause an egregious misclassification. These \\textit{adversarial} attacks have been the focus of extensive research. Likewise, there has been an abundance of research in ways to detect and defend against them. We introduce a novel approach of detection and interpretation of adversarial attacks from a graph perspective. For an image, benign or adversarial, we study how a neural network's architecture can induce an associated graph. We study this graph and introduce specific measures used to predict and interpret adversarial attacks. We show that graphs-based approaches help to investigate the inner workings of adversarial attacks.",
    "link": "http://arxiv.org/abs/2306.00042",
    "context": "Title: Graph-based methods coupled with specific distributional distances for adversarial attack detection. (arXiv:2306.00042v1 [cs.LG])\nAbstract: Artificial neural networks are prone to being fooled by carefully perturbed inputs which cause an egregious misclassification. These \\textit{adversarial} attacks have been the focus of extensive research. Likewise, there has been an abundance of research in ways to detect and defend against them. We introduce a novel approach of detection and interpretation of adversarial attacks from a graph perspective. For an image, benign or adversarial, we study how a neural network's architecture can induce an associated graph. We study this graph and introduce specific measures used to predict and interpret adversarial attacks. We show that graphs-based approaches help to investigate the inner workings of adversarial attacks.",
    "path": "papers/23/06/2306.00042.json",
    "total_tokens": 822,
    "translated_title": "基于图形的方法与特定分布距离相结合的对抗攻击检测",
    "translated_abstract": "人工神经网络容易被精心扰动的输入所欺骗，导致严重的误分类。这些“对抗性”攻击已成为广泛研究的焦点。同样，对抗攻击的检测和防御也有大量研究。我们从图的角度介绍了一种新的方法来检测和解释对抗性攻击。对于图像，无论是良性的还是对抗性的，我们研究了神经网络的架构如何引入一个相关的图形。我们研究了这个图形，并引入了用于预测和解释对抗性攻击的特定测量方法。我们展示了基于图形的方法有助于研究对抗性攻击的内在机理。",
    "tldr": "本论文提出了利用图形的方法结合特定分布距离来检测对抗性攻击，通过研究神经网络的图结构，介绍了用于预测和解释对抗性攻击的特定测量方法，这有助于研究对抗性攻击的内在机理。",
    "en_tdlr": "This paper proposes a method of using graphs with specific distributional distances to detect adversarial attacks. The neural network's architecture is studied to induce an associated graph, which is then analyzed using specific measures to predict and interpret adversarial attacks. The approach helps investigate the inner workings of adversarial attacks."
}