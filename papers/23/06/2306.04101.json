{
    "title": "Gotta: Generative Few-shot Question Answering by Prompt-based Cloze Data Augmentation. (arXiv:2306.04101v1 [cs.CL])",
    "abstract": "Few-shot question answering (QA) aims at precisely discovering answers to a set of questions from context passages while only a few training samples are available. Although existing studies have made some progress and can usually achieve proper results, they suffer from understanding deep semantics for reasoning out the questions. In this paper, we develop Gotta, a Generative prOmpT-based daTa Augmentation framework to mitigate the challenge above. Inspired by the human reasoning process, we propose to integrate the cloze task to enhance few-shot QA learning. Following the recent success of prompt-tuning, we present the cloze task in the same format as the main QA task, allowing the model to learn both tasks seamlessly together to fully take advantage of the power of prompt-tuning. Extensive experiments on widely used benchmarks demonstrate that Gotta consistently outperforms competitive baselines, validating the effectiveness of our proposed prompt-tuning-based cloze task, which not o",
    "link": "http://arxiv.org/abs/2306.04101",
    "context": "Title: Gotta: Generative Few-shot Question Answering by Prompt-based Cloze Data Augmentation. (arXiv:2306.04101v1 [cs.CL])\nAbstract: Few-shot question answering (QA) aims at precisely discovering answers to a set of questions from context passages while only a few training samples are available. Although existing studies have made some progress and can usually achieve proper results, they suffer from understanding deep semantics for reasoning out the questions. In this paper, we develop Gotta, a Generative prOmpT-based daTa Augmentation framework to mitigate the challenge above. Inspired by the human reasoning process, we propose to integrate the cloze task to enhance few-shot QA learning. Following the recent success of prompt-tuning, we present the cloze task in the same format as the main QA task, allowing the model to learn both tasks seamlessly together to fully take advantage of the power of prompt-tuning. Extensive experiments on widely used benchmarks demonstrate that Gotta consistently outperforms competitive baselines, validating the effectiveness of our proposed prompt-tuning-based cloze task, which not o",
    "path": "papers/23/06/2306.04101.json",
    "total_tokens": 907,
    "translated_title": "Gotta: 基于提示式填空数据增强的生成式少样本问题回答",
    "translated_abstract": "少样本问题回答旨在仅使用少量训练样本的情况下，从上下文段落中精确地找到一组问题的答案。虽然现有的研究已经取得了一定的进展并且通常能够取得良好的结果，但是它们在理解深层语义的能力方面仍然存在困难。本文提出了Gotta，一个基于生成式提示式数据增强框架，以缓解上述挑战。受到人类推理过程的启发，我们提出了将提示式填空任务与问题回答任务相结合来增强少样本问题回答的学习能力。在最近提示调整的成功之后，我们以与主要的问题回答任务相同的格式提出了填空任务，使模型能够无缝地学习两个任务，充分利用提示调整的能力。在广泛使用的基准测试中进行的大量实验表明，Gotta始终优于竞争基线，验证了我们提出的基于提示调整的填空任务的有效性，它不仅提高了性能还增强了学习能力。",
    "tldr": "Gotta是一个基于填空数据增强框架，通过将提示式填空任务与问题回答任务相结合，提高了少样本问题回答的学习能力。",
    "en_tdlr": "Gotta is a framework based on cloze data augmentation, which enhances few-shot question answering by combining prompt-based cloze tasks with the main question answering task, improving the learning ability and performance."
}