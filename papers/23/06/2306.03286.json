{
    "title": "Survival Instinct in Offline Reinforcement Learning. (arXiv:2306.03286v1 [cs.LG])",
    "abstract": "We present a novel observation about the behavior of offline reinforcement learning (RL) algorithms: on many benchmark datasets, offline RL can produce well-performing and safe policies even when trained with \"wrong\" reward labels, such as those that are zero everywhere or are negatives of the true rewards. This phenomenon cannot be easily explained by offline RL's return maximization objective. Moreover, it gives offline RL a degree of robustness that is uncharacteristic of its online RL counterparts, which are known to be sensitive to reward design. We demonstrate that this surprising robustness property is attributable to an interplay between the notion of pessimism in offline RL algorithms and a certain bias implicit in common data collection practices. As we prove in this work, pessimism endows the agent with a \"survival instinct\", i.e., an incentive to stay within the data support in the long term, while the limited and biased data coverage further constrains the set of survival ",
    "link": "http://arxiv.org/abs/2306.03286",
    "context": "Title: Survival Instinct in Offline Reinforcement Learning. (arXiv:2306.03286v1 [cs.LG])\nAbstract: We present a novel observation about the behavior of offline reinforcement learning (RL) algorithms: on many benchmark datasets, offline RL can produce well-performing and safe policies even when trained with \"wrong\" reward labels, such as those that are zero everywhere or are negatives of the true rewards. This phenomenon cannot be easily explained by offline RL's return maximization objective. Moreover, it gives offline RL a degree of robustness that is uncharacteristic of its online RL counterparts, which are known to be sensitive to reward design. We demonstrate that this surprising robustness property is attributable to an interplay between the notion of pessimism in offline RL algorithms and a certain bias implicit in common data collection practices. As we prove in this work, pessimism endows the agent with a \"survival instinct\", i.e., an incentive to stay within the data support in the long term, while the limited and biased data coverage further constrains the set of survival ",
    "path": "papers/23/06/2306.03286.json",
    "total_tokens": 961,
    "translated_title": "离线强化学习中的生存本能",
    "translated_abstract": "我们提出了一个关于离线强化学习算法行为的新观察：在许多基准数据集上，离线强化学习即使使用“错误”的奖励标签（例如在所有地方都为零或是真实奖励的负数），也能产生良好的表现和安全的策略。这种现象不能仅通过离线强化学习的回报最大化目标来解释。此外，它赋予了离线强化学习一定的鲁棒性，这在其在线强化学习对应物中是不典型的，因为后者对奖励设计敏感。我们证明了此惊人的鲁棒性属性是离线强化学习算法中悲观主义概念和常见数据收集实践中某种偏见之间相互作用的结果。悲观主义赋予了代理生存本能，即长期内留在数据支持中的激励，而有限且有偏见的数据覆盖进一步限制了生存行为集合。",
    "tldr": "离线强化学习算法即使使用错误的奖励标签，也能产生良好的表现和安全的策略，这种鲁棒性属性是由离线强化学习算法的悲观主义和常见数据收集实践中的偏见之间相互作用的结果，赋予了代理生存本能。",
    "en_tdlr": "Offline reinforcement learning algorithms can produce good and safe policies even with \"wrong\" reward labels, thanks to the interplay between pessimism in the algorithms and a certain bias implicit in common data collection practices, which give the agents a survival instinct to stay within the data support in the long term."
}