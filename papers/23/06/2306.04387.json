{
    "title": "M$^3$IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning. (arXiv:2306.04387v1 [cs.CV])",
    "abstract": "Instruction tuning has significantly advanced large language models (LLMs) such as ChatGPT, enabling them to align with human instructions across diverse tasks. However, progress in open vision-language models (VLMs) has been limited due to the scarcity of high-quality instruction datasets. To tackle this challenge and promote research in the vision-language field, we introduce the Multi-Modal, Multilingual Instruction Tuning (M$^3$IT) dataset, designed to optimize VLM alignment with human instructions. Our M$^3$IT dataset comprises 40 carefully curated datasets, including 2.4 million instances and 400 manually written task instructions, reformatted into a vision-to-text structure. Key tasks are translated into 80 languages with an advanced translation system, ensuring broader accessibility. M$^3$IT surpasses previous datasets regarding task coverage, instruction number and instance scale. Moreover, we develop Ying-VLM, a VLM model trained on our M$^3$IT dataset, showcasing its potenti",
    "link": "http://arxiv.org/abs/2306.04387",
    "context": "Title: M$^3$IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning. (arXiv:2306.04387v1 [cs.CV])\nAbstract: Instruction tuning has significantly advanced large language models (LLMs) such as ChatGPT, enabling them to align with human instructions across diverse tasks. However, progress in open vision-language models (VLMs) has been limited due to the scarcity of high-quality instruction datasets. To tackle this challenge and promote research in the vision-language field, we introduce the Multi-Modal, Multilingual Instruction Tuning (M$^3$IT) dataset, designed to optimize VLM alignment with human instructions. Our M$^3$IT dataset comprises 40 carefully curated datasets, including 2.4 million instances and 400 manually written task instructions, reformatted into a vision-to-text structure. Key tasks are translated into 80 languages with an advanced translation system, ensuring broader accessibility. M$^3$IT surpasses previous datasets regarding task coverage, instruction number and instance scale. Moreover, we develop Ying-VLM, a VLM model trained on our M$^3$IT dataset, showcasing its potenti",
    "path": "papers/23/06/2306.04387.json",
    "total_tokens": 963,
    "translated_title": "M$^3$IT: 多模态多语种指令调整的大规模数据集",
    "translated_abstract": "指令调整已经显著推进了像ChatGPT这样的大型语言模型（LLMs），使它们能够在各种任务中与人类指令保持一致。然而，由于高质量指令数据集的稀缺性，开放式视觉语言模型（VLM）的进展一直受到限制。为了解决这个挑战，并促进视觉语言领域的研究，我们介绍了多模态多语言指令调整（M$^3$IT）数据集，旨在优化VLM与人类指令的对齐。我们的M$^3$IT数据集包括40个精心策划的数据集，包括240万个实例和400个手动编写的任务指令，格式化为视觉到文本结构。重要任务被翻译成80种语言，使用先进的翻译系统，确保更广泛的可访问性。M$^3$IT在任务覆盖范围、指令数量和实例规模方面超过了以前的数据集。此外，我们开发了Ying-VLM，它是在我们的M$^3$IT数据集上训练的VLM模型，展示了其潜在的有效性。",
    "tldr": "M$^3$IT数据集旨在优化开放式视觉语言模型（VLM）与人类指令的对齐，是一个大规模、多模态和多语种的数据集。",
    "en_tdlr": "The M$^3$IT dataset aims to optimize the alignment of open vision-language models with human instructions, it is a large-scale, multi-modal and multilingual dataset."
}