{
    "title": "Generalized Low-Rank Update: Model Parameter Bounds for Low-Rank Training Data Modifications. (arXiv:2306.12670v1 [stat.ML])",
    "abstract": "In this study, we have developed an incremental machine learning (ML) method that efficiently obtains the optimal model when a small number of instances or features are added or removed. This problem holds practical importance in model selection, such as cross-validation (CV) and feature selection. Among the class of ML methods known as linear estimators, there exists an efficient model update framework called the low-rank update that can effectively handle changes in a small number of rows and columns within the data matrix. However, for ML methods beyond linear estimators, there is currently no comprehensive framework available to obtain knowledge about the updated solution within a specific computational complexity. In light of this, our study introduces a method called the Generalized Low-Rank Update (GLRU) which extends the low-rank update framework of linear estimators to ML methods formulated as a certain class of regularized empirical risk minimization, including commonly used ",
    "link": "http://arxiv.org/abs/2306.12670",
    "context": "Title: Generalized Low-Rank Update: Model Parameter Bounds for Low-Rank Training Data Modifications. (arXiv:2306.12670v1 [stat.ML])\nAbstract: In this study, we have developed an incremental machine learning (ML) method that efficiently obtains the optimal model when a small number of instances or features are added or removed. This problem holds practical importance in model selection, such as cross-validation (CV) and feature selection. Among the class of ML methods known as linear estimators, there exists an efficient model update framework called the low-rank update that can effectively handle changes in a small number of rows and columns within the data matrix. However, for ML methods beyond linear estimators, there is currently no comprehensive framework available to obtain knowledge about the updated solution within a specific computational complexity. In light of this, our study introduces a method called the Generalized Low-Rank Update (GLRU) which extends the low-rank update framework of linear estimators to ML methods formulated as a certain class of regularized empirical risk minimization, including commonly used ",
    "path": "papers/23/06/2306.12670.json",
    "total_tokens": 983,
    "tldr": "本研究引入了一种称为广义低秩更新（GLRU）的方法，将线性估计器的低秩更新框架扩展到以某些类别的规则经验风险最小化为形式的ML方法，包括常用的方法。这可以在特定的计算复杂度内有效地处理少量行和列的更改。"
}