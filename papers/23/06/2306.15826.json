{
    "title": "MAT: Mixed-Strategy Game of Adversarial Training in Fine-tuning. (arXiv:2306.15826v1 [cs.CL])",
    "abstract": "Fine-tuning large-scale pre-trained language models has been demonstrated effective for various natural language processing (NLP) tasks. Previous studies have established that incorporating adversarial training during the fine-tuning stage can significantly enhance model generalization and robustness. However, from the perspective of game theory, such utilizations of adversarial training correspond to pure-strategy games, which are inherently limited in terms of the scope of their strategies, thereby still having room for improvement. In order to push the performance boundaries, we propose a novel Mixed-strategy Adversarial Training algorithm (MAT). Methodologically, we derive the Nash equilibrium of a mixed-strategy game for adversarial training using Entropy Mirror Descent to establish MAT by sampling method. To verify the effectiveness of MAT, we conducted extensive benchmark experiments on large-scale pre-trained models, such as BERT and RoBERTa. MAT significantly outperforms the s",
    "link": "http://arxiv.org/abs/2306.15826",
    "context": "Title: MAT: Mixed-Strategy Game of Adversarial Training in Fine-tuning. (arXiv:2306.15826v1 [cs.CL])\nAbstract: Fine-tuning large-scale pre-trained language models has been demonstrated effective for various natural language processing (NLP) tasks. Previous studies have established that incorporating adversarial training during the fine-tuning stage can significantly enhance model generalization and robustness. However, from the perspective of game theory, such utilizations of adversarial training correspond to pure-strategy games, which are inherently limited in terms of the scope of their strategies, thereby still having room for improvement. In order to push the performance boundaries, we propose a novel Mixed-strategy Adversarial Training algorithm (MAT). Methodologically, we derive the Nash equilibrium of a mixed-strategy game for adversarial training using Entropy Mirror Descent to establish MAT by sampling method. To verify the effectiveness of MAT, we conducted extensive benchmark experiments on large-scale pre-trained models, such as BERT and RoBERTa. MAT significantly outperforms the s",
    "path": "papers/23/06/2306.15826.json",
    "total_tokens": 938,
    "translated_title": "MAT: 对微调中对抗训练的混合策略游戏",
    "translated_abstract": "细调大规模预训练语言模型已被证明在各种自然语言处理任务中有效。之前的研究表明，在细调阶段加入对抗训练可以显著提高模型的泛化能力和鲁棒性。然而，从博弈论的角度来看，这种对抗训练的应用对应于纯策略游戏，其在策略范围方面存在固有的限制，因此仍有改进空间。为了推动性能边界，我们提出了一种新颖的混合策略对抗训练算法（MAT）。在方法上，我们使用熵镜像下降推导了对抗训练的混合策略游戏的纳什均衡，通过采样方法建立了MAT。为了验证MAT的有效性，我们在BERT和RoBERTa等大规模预训练模型上进行了大量基准实验。MAT在性能上显著优于其他方法。",
    "tldr": "本论文提出了一种新颖的混合策略对抗训练算法（MAT），通过在细调阶段加入对抗训练，显著提高了模型的泛化能力和鲁棒性，通过采样方法建立了MAT。实验证明，MAT在大规模预训练模型上的性能明显优于其他方法。",
    "en_tdlr": "This paper proposes a novel mixed-strategy adversarial training algorithm (MAT) which significantly improves the model's generalization and robustness by incorporating adversarial training during the fine-tuning stage. MAT is established through the sampling method and outperforms other methods on large-scale pre-trained models."
}