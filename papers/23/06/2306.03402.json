{
    "title": "Binary Classification with Instance and Label Dependent Label Noise. (arXiv:2306.03402v1 [stat.ML])",
    "abstract": "Learning with label dependent label noise has been extensively explored in both theory and practice; however, dealing with instance (i.e., feature) and label dependent label noise continues to be a challenging task. The difficulty arises from the fact that the noise rate varies for each instance, making it challenging to estimate accurately. The question of whether it is possible to learn a reliable model using only noisy samples remains unresolved. We answer this question with a theoretical analysis that provides matching upper and lower bounds. Surprisingly, our results show that, without any additional assumptions, empirical risk minimization achieves the optimal excess risk bound. Specifically, we derive a novel excess risk bound proportional to the noise level, which holds in very general settings, by comparing the empirical risk minimizers obtained from clean samples and noisy samples. Second, we show that the minimax lower bound for the 0-1 loss is a constant proportional to the",
    "link": "http://arxiv.org/abs/2306.03402",
    "context": "Title: Binary Classification with Instance and Label Dependent Label Noise. (arXiv:2306.03402v1 [stat.ML])\nAbstract: Learning with label dependent label noise has been extensively explored in both theory and practice; however, dealing with instance (i.e., feature) and label dependent label noise continues to be a challenging task. The difficulty arises from the fact that the noise rate varies for each instance, making it challenging to estimate accurately. The question of whether it is possible to learn a reliable model using only noisy samples remains unresolved. We answer this question with a theoretical analysis that provides matching upper and lower bounds. Surprisingly, our results show that, without any additional assumptions, empirical risk minimization achieves the optimal excess risk bound. Specifically, we derive a novel excess risk bound proportional to the noise level, which holds in very general settings, by comparing the empirical risk minimizers obtained from clean samples and noisy samples. Second, we show that the minimax lower bound for the 0-1 loss is a constant proportional to the",
    "path": "papers/23/06/2306.03402.json",
    "total_tokens": 924,
    "translated_title": "带有实例和标签相关的标签噪声的二分类问题",
    "translated_abstract": "学习带有标签相关的标签噪声在理论和实践中得到了广泛探讨，然而处理带有实例和标签相关的标签噪声仍然是一项具有挑战性的任务。这种困难在于噪声率因每个实例而异，使得准确估计噪声率成为一项具有挑战性的任务。目前还没有解决能否仅使用含有噪声样本来学习可靠模型的问题。我们通过理论分析回答了这个问题，提供了匹配的上界和下界。令人惊讶的是，我们的结果表明，不需要任何额外的假设，经验风险最小化可以实现最优的超额风险界限。具体而言，我们通过比较从干净样本和噪声样本中得到的经验风险最小化器来导出一种与噪声水平成比例的新的超额风险界限，在非常一般的情况下都成立。其次，我们表明了0-1损失的极小极大下界是一个与标签数成比例的常数。",
    "tldr": "本文研究解决带有实例和标签相关的标签噪声对于二分类问题的困难，通过理论分析得到经验风险最小化可以实现最优的超额风险界限。",
    "en_tdlr": "This paper investigates the challenge of binary classification with instance and label dependent label noise and shows that empirical risk minimization achieves the optimal excess risk bound, despite varied noise rate for each instance, through a novel excess risk bound proportional to the noise level."
}