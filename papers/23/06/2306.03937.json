{
    "title": "Guiding The Last Layer in Federated Learning with Pre-Trained Models. (arXiv:2306.03937v1 [cs.AI])",
    "abstract": "Federated Learning (FL) is an emerging paradigm that allows a model to be trained across a number of participants without sharing data. Recent works have begun to consider the effects of using pre-trained models as an initialization point for existing FL algorithms; however, these approaches ignore the vast body of efficient transfer learning literature from the centralized learning setting. Here we revisit the problem of FL from a pre-trained model considered in prior work and expand it to a set of computer vision transfer learning problems. We first observe that simply fitting a linear classification head can be efficient and effective in many cases. We then show that in the FL setting, fitting a classifier using the Nearest Class Means (NCM) can be done exactly and orders of magnitude more efficiently than existing proposals, while obtaining strong performance. Finally, we demonstrate that using a two-phase approach of obtaining the classifier and then fine-tuning the model can yiel",
    "link": "http://arxiv.org/abs/2306.03937",
    "context": "Title: Guiding The Last Layer in Federated Learning with Pre-Trained Models. (arXiv:2306.03937v1 [cs.AI])\nAbstract: Federated Learning (FL) is an emerging paradigm that allows a model to be trained across a number of participants without sharing data. Recent works have begun to consider the effects of using pre-trained models as an initialization point for existing FL algorithms; however, these approaches ignore the vast body of efficient transfer learning literature from the centralized learning setting. Here we revisit the problem of FL from a pre-trained model considered in prior work and expand it to a set of computer vision transfer learning problems. We first observe that simply fitting a linear classification head can be efficient and effective in many cases. We then show that in the FL setting, fitting a classifier using the Nearest Class Means (NCM) can be done exactly and orders of magnitude more efficiently than existing proposals, while obtaining strong performance. Finally, we demonstrate that using a two-phase approach of obtaining the classifier and then fine-tuning the model can yiel",
    "path": "papers/23/06/2306.03937.json",
    "total_tokens": 891,
    "translated_title": "在联邦学习中使用预训练模型引导最后一层",
    "translated_abstract": "联邦学习(Fl)是一种新兴的范式，允许在不共享数据的情况下跨多个参与者训练模型。最近的一些工作开始考虑使用预训练模型作为现有FL算法的初始化点的影响; 但是，这些方法忽略了集中式学习设置中大量有效的迁移学习文献。在这里，我们重新审视了先前工作中考虑预训练模型的FL问题，并将其扩展到一组计算机视觉迁移学习问题。我们首先观察到，在许多情况下，仅拟合线性分类头是高效且有效的。然后，我们展示了在FL设置中，使用最近类均值(NCM)拟合分类器可以精确地且比现有提议高效地完成，同时获得了强大的性能。最后，我们证明了使用两阶段方法获得分类器，然后微调模型可以产生更好的结果。",
    "tldr": "本文研究了在联邦学习中使用预训练模型引导最后一层的问题，提出了使用最近类均值(NCM)精确且高效地拟合分类器的方法，并取得了很好的效果。",
    "en_tdlr": "This paper revisits the problem of using pre-trained models in Federated Learning and proposes fitting a classifier using Nearest Class Means (NCM) in an efficient and effective way, achieving strong performance and better results with a two-phase approach of obtaining the classifier and fine-tuning the model."
}