{
    "title": "SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking. (arXiv:2306.05426v2 [cs.LG] UPDATED)",
    "abstract": "In many domains, autoregressive models can attain high likelihood on the task of predicting the next observation. However, this maximum-likelihood (MLE) objective does not necessarily match a downstream use-case of autoregressively generating high-quality sequences. The MLE objective weights sequences proportionally to their frequency under the data distribution, with no guidance for the model's behaviour out of distribution (OOD): leading to compounding error during autoregressive generation. In order to address this compounding error problem, we formulate sequence generation as an imitation learning (IL) problem. This allows us to minimize a variety of divergences between the distribution of sequences generated by an autoregressive model and sequences from a dataset, including divergences with weight on OOD generated sequences. The IL framework also allows us to incorporate backtracking by introducing a backspace action into the generation process. This further mitigates the compound",
    "link": "http://arxiv.org/abs/2306.05426",
    "context": "Title: SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking. (arXiv:2306.05426v2 [cs.LG] UPDATED)\nAbstract: In many domains, autoregressive models can attain high likelihood on the task of predicting the next observation. However, this maximum-likelihood (MLE) objective does not necessarily match a downstream use-case of autoregressively generating high-quality sequences. The MLE objective weights sequences proportionally to their frequency under the data distribution, with no guidance for the model's behaviour out of distribution (OOD): leading to compounding error during autoregressive generation. In order to address this compounding error problem, we formulate sequence generation as an imitation learning (IL) problem. This allows us to minimize a variety of divergences between the distribution of sequences generated by an autoregressive model and sequences from a dataset, including divergences with weight on OOD generated sequences. The IL framework also allows us to incorporate backtracking by introducing a backspace action into the generation process. This further mitigates the compound",
    "path": "papers/23/06/2306.05426.json",
    "total_tokens": 922,
    "translated_title": "SequenceMatch：带回溯的自回归序列模型的模仿学习",
    "translated_abstract": "在许多领域，自回归模型可以在预测下一个观测值的任务上获得高似然度。然而，这种最大似然（MLE）目标不一定与自回归生成高质量序列的下游用例相匹配。MLE目标按照数据分布下序列的频率加权，不提供模型在分布之外行为的指导，这会导致在自回归生成过程中复合误差。为了解决这个复合误差问题，我们将序列生成定为模仿学习（IL）问题。这使我们可以最小化自回归模型生成的序列分布和数据集序列之间的各种分歧，包括考虑出分布序列的分歧。IL框架还允许我们通过在生成过程中引入回格动作来引入回溯。这进一步减轻了复合效应。",
    "tldr": "本文提出了一种称为SequenceMatch的带有回溯的自回归模型的模仿学习框架，该框架通过最小化自回归模型生成序列和数据集序列之间的各种分歧来减少在自回归生成过程中的复合误差，并允许引入回溯动作。",
    "en_tdlr": "This paper proposes an imitation learning framework called SequenceMatch for autoregressive models with backtracking, which minimizes divergences between generated sequences and those from a dataset, including out-of-distribution sequences, and allows for the introduction of backtracking actions to mitigate compounding errors in the generation process."
}