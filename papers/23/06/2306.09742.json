{
    "title": "Meta Generative Flow Networks with Personalization for Task-Specific Adaptation. (arXiv:2306.09742v1 [cs.LG])",
    "abstract": "Multi-task reinforcement learning and meta-reinforcement learning have been developed to quickly adapt to new tasks, but they tend to focus on tasks with higher rewards and more frequent occurrences, leading to poor performance on tasks with sparse rewards. To address this issue, GFlowNets can be integrated into meta-learning algorithms (GFlowMeta) by leveraging the advantages of GFlowNets on tasks with sparse rewards. However, GFlowMeta suffers from performance degradation when encountering heterogeneous transitions from distinct tasks. To overcome this challenge, this paper proposes a personalized approach named pGFlowMeta, which combines task-specific personalized policies with a meta policy. Each personalized policy balances the loss on its personalized task and the difference from the meta policy, while the meta policy aims to minimize the average loss of all tasks. The theoretical analysis shows that the algorithm converges at a sublinear rate. Extensive experiments demonstrate t",
    "link": "http://arxiv.org/abs/2306.09742",
    "context": "Title: Meta Generative Flow Networks with Personalization for Task-Specific Adaptation. (arXiv:2306.09742v1 [cs.LG])\nAbstract: Multi-task reinforcement learning and meta-reinforcement learning have been developed to quickly adapt to new tasks, but they tend to focus on tasks with higher rewards and more frequent occurrences, leading to poor performance on tasks with sparse rewards. To address this issue, GFlowNets can be integrated into meta-learning algorithms (GFlowMeta) by leveraging the advantages of GFlowNets on tasks with sparse rewards. However, GFlowMeta suffers from performance degradation when encountering heterogeneous transitions from distinct tasks. To overcome this challenge, this paper proposes a personalized approach named pGFlowMeta, which combines task-specific personalized policies with a meta policy. Each personalized policy balances the loss on its personalized task and the difference from the meta policy, while the meta policy aims to minimize the average loss of all tasks. The theoretical analysis shows that the algorithm converges at a sublinear rate. Extensive experiments demonstrate t",
    "path": "papers/23/06/2306.09742.json",
    "total_tokens": 915,
    "translated_abstract": "多任务强化学习和元强化学习已经发展起来，可以快速适应新的任务，但它们往往关注具有更高奖励和更频繁出现的任务，导致在具有稀疏奖励的任务上表现不佳。为了解决这个问题，可以将GFlowNets整合到元学习算法(GFlowMeta)中，利用GFlowNets在具有稀疏奖励的任务上的优势。然而，当遇到不同任务的异构转换时，GFlowMeta的表现会下降。为了克服这一挑战，本文提出了一种名为pGFlowMeta的个性化方法，它将任务特定的个性化策略与元策略相结合。每个个性化策略平衡其个性化任务上的损失和与元策略的差异，而元策略旨在最小化所有任务的平均损失。理论分析表明，该算法以次线性速率收敛。大量的实验表明了算法的有效性和性能。",
    "tldr": "本文提出了一个名为pGFlowMeta的个性化方法，结合了任务特定的个性化策略和元策略，以解决 GFlowMeta 在表现任务特定性方面的不足以及遇到异构任务转换的情况下表现下降的问题。",
    "en_tdlr": "This paper proposes a personalized approach named pGFlowMeta, which combines task-specific personalized policies with a meta policy, to address the issues of GFlowMeta's performance in task-specificity and degradation when encountering heterogeneous transitions from distinct tasks."
}