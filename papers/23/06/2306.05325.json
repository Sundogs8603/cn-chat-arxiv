{
    "title": "Federated Learning under Covariate Shifts with Generalization Guarantees. (arXiv:2306.05325v1 [cs.LG])",
    "abstract": "This paper addresses intra-client and inter-client covariate shifts in federated learning (FL) with a focus on the overall generalization performance. To handle covariate shifts, we formulate a new global model training paradigm and propose Federated Importance-Weighted Empirical Risk Minimization (FTW-ERM) along with improving density ratio matching methods without requiring perfect knowledge of the supremum over true ratios. We also propose the communication-efficient variant FITW-ERM with the same level of privacy guarantees as those of classical ERM in FL. We theoretically show that FTW-ERM achieves smaller generalization error than classical ERM under certain settings. Experimental results demonstrate the superiority of FTW-ERM over existing FL baselines in challenging imbalanced federated settings in terms of data distribution shifts across clients.",
    "link": "http://arxiv.org/abs/2306.05325",
    "context": "Title: Federated Learning under Covariate Shifts with Generalization Guarantees. (arXiv:2306.05325v1 [cs.LG])\nAbstract: This paper addresses intra-client and inter-client covariate shifts in federated learning (FL) with a focus on the overall generalization performance. To handle covariate shifts, we formulate a new global model training paradigm and propose Federated Importance-Weighted Empirical Risk Minimization (FTW-ERM) along with improving density ratio matching methods without requiring perfect knowledge of the supremum over true ratios. We also propose the communication-efficient variant FITW-ERM with the same level of privacy guarantees as those of classical ERM in FL. We theoretically show that FTW-ERM achieves smaller generalization error than classical ERM under certain settings. Experimental results demonstrate the superiority of FTW-ERM over existing FL baselines in challenging imbalanced federated settings in terms of data distribution shifts across clients.",
    "path": "papers/23/06/2306.05325.json",
    "total_tokens": 784,
    "translated_title": "在协变量转移下具有泛化保证的联邦学习",
    "translated_abstract": "本文解决了联邦学习中客户端内和客户端间协变量转移的问题，重点关注总体泛化性能。为了处理协变量转移，我们制定了一种新的全局模型训练范式，并提出了Federated Importance-Weighted Empirical Risk Minimization（FTW-ERM），改进了密度比匹配方法，而不需要完美知识来处理真实比率的上确界。我们还提出了通信高效的变体FITW-ERM，其隐私保证与经典ERM在FL中的隐私保证相同。我们在理论上证明了FTW-ERM在某些设置下比经典ERM达到更小的泛化误差。实验结果表明，在具有数据分布在客户端之间发生偏移的挑战性失衡联邦设置方面，FTW-ERM优于现有的FL基线。",
    "tldr": "本文提出了具有泛化保证的联邦学习方法FTW-ERM，在处理协变量转移方面表现出优势。",
    "en_tdlr": "This paper presents a Federated Learning method, FTW-ERM, with generalization guarantees to handle covariate shifts and outperform existing baselines in challenging imbalanced federated settings with data distribution shifts across clients."
}