{
    "title": "WSPAlign: Word Alignment Pre-training via Large-Scale Weakly Supervised Span Prediction. (arXiv:2306.05644v1 [cs.CL])",
    "abstract": "Most existing word alignment methods rely on manual alignment datasets or parallel corpora, which limits their usefulness. Here, to mitigate the dependence on manual data, we broaden the source of supervision by relaxing the requirement for correct, fully-aligned, and parallel sentences. Specifically, we make noisy, partially aligned, and non-parallel paragraphs. We then use such a large-scale weakly-supervised dataset for word alignment pre-training via span prediction. Extensive experiments with various settings empirically demonstrate that our approach, which is named WSPAlign, is an effective and scalable way to pre-train word aligners without manual data. When fine-tuned on standard benchmarks, WSPAlign has set a new state-of-the-art by improving upon the best-supervised baseline by 3.3~6.1 points in F1 and 1.5~6.1 points in AER. Furthermore, WSPAlign also achieves competitive performance compared with the corresponding baselines in few-shot, zero-shot and cross-lingual tests, whi",
    "link": "http://arxiv.org/abs/2306.05644",
    "context": "Title: WSPAlign: Word Alignment Pre-training via Large-Scale Weakly Supervised Span Prediction. (arXiv:2306.05644v1 [cs.CL])\nAbstract: Most existing word alignment methods rely on manual alignment datasets or parallel corpora, which limits their usefulness. Here, to mitigate the dependence on manual data, we broaden the source of supervision by relaxing the requirement for correct, fully-aligned, and parallel sentences. Specifically, we make noisy, partially aligned, and non-parallel paragraphs. We then use such a large-scale weakly-supervised dataset for word alignment pre-training via span prediction. Extensive experiments with various settings empirically demonstrate that our approach, which is named WSPAlign, is an effective and scalable way to pre-train word aligners without manual data. When fine-tuned on standard benchmarks, WSPAlign has set a new state-of-the-art by improving upon the best-supervised baseline by 3.3~6.1 points in F1 and 1.5~6.1 points in AER. Furthermore, WSPAlign also achieves competitive performance compared with the corresponding baselines in few-shot, zero-shot and cross-lingual tests, whi",
    "path": "papers/23/06/2306.05644.json",
    "total_tokens": 945,
    "translated_title": "WSPAlign: 大规模弱监督跨度预测下的词对齐预训练",
    "translated_abstract": "大多数现有的词对齐方法依赖于手动对齐数据集或平行语料库，这限制了它们的实用性。为了缓解对手动数据的依赖，我们通过放宽对正确、完全对齐和平行句子的要求，扩大了监督数据的来源。具体而言，我们生成了带有噪声、部分对齐和非平行段落作为大规模弱监督数据集，通过跨度预测对词对齐进行预训练。广泛的实验表明，我们的方法名为WSPAlign，是一种有效且可扩展的无需手动数据的预训练词对齐方法。在标准基准测试中fine-tuning时，WSPAlign在F1和AER两个指标上的最佳监督基线分别提高了3.3~6.1和1.5~6.1个点，成为了新的最优结果。此外，WSPAlign在少样本、零样本和跨语言测试中也获得了与相应基线相同的竞争性能。",
    "tldr": "本文提出了一种名为WSPAlign的无需手动数据的预训练词对齐方法，通过用大规模弱监督数据中的跨度预测进行预训练，取得了比当前最佳方法更好的效果。",
    "en_tdlr": "This paper proposes a pre-training word alignment method named WSPAlign, which does not require manual data and achieves better performance than the current state-of-the-art by training on a large-scale weakly supervised dataset with span prediction."
}