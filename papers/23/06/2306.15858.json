{
    "title": "Hierarchical Graph Neural Networks for Proprioceptive 6D Pose Estimation of In-hand Objects. (arXiv:2306.15858v1 [cs.RO])",
    "abstract": "Robotic manipulation, in particular in-hand object manipulation, often requires an accurate estimate of the object's 6D pose. To improve the accuracy of the estimated pose, state-of-the-art approaches in 6D object pose estimation use observational data from one or more modalities, e.g., RGB images, depth, and tactile readings. However, existing approaches make limited use of the underlying geometric structure of the object captured by these modalities, thereby, increasing their reliance on visual features. This results in poor performance when presented with objects that lack such visual features or when visual features are simply occluded. Furthermore, current approaches do not take advantage of the proprioceptive information embedded in the position of the fingers. To address these limitations, in this paper: (1) we introduce a hierarchical graph neural network architecture for combining multimodal (vision and touch) data that allows for a geometrically informed 6D object pose estima",
    "link": "http://arxiv.org/abs/2306.15858",
    "context": "Title: Hierarchical Graph Neural Networks for Proprioceptive 6D Pose Estimation of In-hand Objects. (arXiv:2306.15858v1 [cs.RO])\nAbstract: Robotic manipulation, in particular in-hand object manipulation, often requires an accurate estimate of the object's 6D pose. To improve the accuracy of the estimated pose, state-of-the-art approaches in 6D object pose estimation use observational data from one or more modalities, e.g., RGB images, depth, and tactile readings. However, existing approaches make limited use of the underlying geometric structure of the object captured by these modalities, thereby, increasing their reliance on visual features. This results in poor performance when presented with objects that lack such visual features or when visual features are simply occluded. Furthermore, current approaches do not take advantage of the proprioceptive information embedded in the position of the fingers. To address these limitations, in this paper: (1) we introduce a hierarchical graph neural network architecture for combining multimodal (vision and touch) data that allows for a geometrically informed 6D object pose estima",
    "path": "papers/23/06/2306.15858.json",
    "total_tokens": 851,
    "translated_title": "用于手中物体自感知6D姿态估计的分层图神经网络",
    "translated_abstract": "机器人操作，特别是手中物体的操作，通常需要准确估计物体的6D姿态。为了提高估计姿态的准确性，目前6D物体姿态估计的最先进方法使用来自一个或多个模态的观测数据，例如RGB图像、深度和触觉读数。然而，现有方法对这些模态捕获的物体的基本几何结构的利用有限，从而增加了对视觉特征的依赖性。这导致当面对缺乏这种视觉特征的物体或者视觉特征被遮挡时，性能较差。此外，现有方法也没有充分利用手指位置中嵌入的感觉信息。为了解决这些限制，本文介绍了一种用于结合多模态（视觉和触觉）数据的分层图神经网络架构，实现几何信息有根据的6D物体姿态估计。",
    "tldr": "本文提出了一种分层图神经网络架构，用于结合多模态数据，实现几何信息有根据的6D物体姿态估计。"
}