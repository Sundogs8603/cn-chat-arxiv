{
    "title": "Wasserstein distributional robustness of neural networks. (arXiv:2306.09844v1 [cs.LG])",
    "abstract": "Deep neural networks are known to be vulnerable to adversarial attacks (AA). For an image recognition task, this means that a small perturbation of the original can result in the image being misclassified. Design of such attacks as well as methods of adversarial training against them are subject of intense research. We re-cast the problem using techniques of Wasserstein distributionally robust optimization (DRO) and obtain novel contributions leveraging recent insights from DRO sensitivity analysis. We consider a set of distributional threat models. Unlike the traditional pointwise attacks, which assume a uniform bound on perturbation of each input data point, distributional threat models allow attackers to perturb inputs in a non-uniform way. We link these more general attacks with questions of out-of-sample performance and Knightian uncertainty. To evaluate the distributional robustness of neural networks, we propose a first-order AA algorithm and its multi-step version. Our attack a",
    "link": "http://arxiv.org/abs/2306.09844",
    "context": "Title: Wasserstein distributional robustness of neural networks. (arXiv:2306.09844v1 [cs.LG])\nAbstract: Deep neural networks are known to be vulnerable to adversarial attacks (AA). For an image recognition task, this means that a small perturbation of the original can result in the image being misclassified. Design of such attacks as well as methods of adversarial training against them are subject of intense research. We re-cast the problem using techniques of Wasserstein distributionally robust optimization (DRO) and obtain novel contributions leveraging recent insights from DRO sensitivity analysis. We consider a set of distributional threat models. Unlike the traditional pointwise attacks, which assume a uniform bound on perturbation of each input data point, distributional threat models allow attackers to perturb inputs in a non-uniform way. We link these more general attacks with questions of out-of-sample performance and Knightian uncertainty. To evaluate the distributional robustness of neural networks, we propose a first-order AA algorithm and its multi-step version. Our attack a",
    "path": "papers/23/06/2306.09844.json",
    "total_tokens": 871,
    "translated_title": "神经网络的Wasserstein分布鲁棒性",
    "translated_abstract": "深度神经网络已知易受到对抗攻击(Adversarial Attacks, AA)的威胁。对于图像识别任务而言，这意味着对原始图像进行微小扰动就有可能导致其被错误分类。因此，设计对抗攻击以及对抗训练的方法是研究的焦点。本研究使用Wasserstein分布鲁棒性优化技术重新构思该问题，并利用最近的DRO敏感性分析的新见解进行了新的贡献。我们考虑一组分布威胁模型。与传统的点对点攻击不同的是，分布威胁模型允许攻击者以非均匀的方式扰动输入。我们将这些更一般的攻击与样本外性能和Knightian不确定性问题联系起来。为了评估神经网络的分布鲁棒性，我们提出了一种一阶AA算法及其多步版本。",
    "tldr": "本研究探讨了基于Wasserstein分布鲁棒性的神经网络对抗攻击，提出了一组分布威胁模型，并使用一阶对抗攻击算法及其多步版本对其进行了评估。",
    "en_tdlr": "This paper explores the Wasserstein distributional robustness of neural networks against adversarial attacks, proposing a set of distributional threat models and evaluating them using a first-order AA algorithm and its multi-step version."
}