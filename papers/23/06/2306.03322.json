{
    "title": "Stochastic Multi-Level Compositional Optimization Algorithms over Networks with Level-Independent Convergence Rate. (arXiv:2306.03322v1 [cs.LG])",
    "abstract": "Stochastic multi-level compositional optimization problems cover many new machine learning paradigms, e.g., multi-step model-agnostic meta-learning, which require efficient optimization algorithms for large-scale applications. This paper studies the decentralized stochastic multi-level optimization algorithm, which is challenging because the multi-level structure and decentralized communication scheme may make the number of levels affect the order of the convergence rate. To this end, we develop two novel decentralized optimization algorithms to deal with the multi-level function and its gradient. Our theoretical results show that both algorithms can achieve the level-independent convergence rate for nonconvex problems under much milder conditions compared with existing single-machine algorithms. To the best of our knowledge, this is the first work that achieves the level-independent convergence rate under the decentralized setting. Moreover, extensive experiments confirm the efficacy ",
    "link": "http://arxiv.org/abs/2306.03322",
    "context": "Title: Stochastic Multi-Level Compositional Optimization Algorithms over Networks with Level-Independent Convergence Rate. (arXiv:2306.03322v1 [cs.LG])\nAbstract: Stochastic multi-level compositional optimization problems cover many new machine learning paradigms, e.g., multi-step model-agnostic meta-learning, which require efficient optimization algorithms for large-scale applications. This paper studies the decentralized stochastic multi-level optimization algorithm, which is challenging because the multi-level structure and decentralized communication scheme may make the number of levels affect the order of the convergence rate. To this end, we develop two novel decentralized optimization algorithms to deal with the multi-level function and its gradient. Our theoretical results show that both algorithms can achieve the level-independent convergence rate for nonconvex problems under much milder conditions compared with existing single-machine algorithms. To the best of our knowledge, this is the first work that achieves the level-independent convergence rate under the decentralized setting. Moreover, extensive experiments confirm the efficacy ",
    "path": "papers/23/06/2306.03322.json",
    "total_tokens": 863,
    "translated_title": "面向网络的随机多层组合优化算法，具有独立于层数的收敛速度",
    "translated_abstract": "随机多层组合优化问题涵盖了很多新的机器学习范例，如多步骤模型无关元学习，需要大规模应用的高效优化算法。本文研究了分散随机多层优化算法，这是一项具有挑战性的任务，因为多层结构和分散式通讯方案可能增加层数对收敛速度的影响。为此，我们开发了两种新的分散式优化算法，来处理多层函数和其梯度。我们的理论结果表明，与现有的单机算法相比，这两种算法均能在非凸问题中实现独立于层数的收敛速度，条件更加宽松。据我们所知，这是首次在分散式设置下实现独立于层数的收敛速度的工作。此外，广泛的实验证实了其有效性。",
    "tldr": "本文提出了面向网络的随机多层组合优化算法，其中包括两种新的分散式优化算法，可实现独立于层数的收敛速度，理论结果和实验证明它们更加有效。",
    "en_tdlr": "This paper proposes stochastic multi-level compositional optimization algorithms over networks with level-independent convergence rate. Two novel decentralized optimization algorithms are developed to deal with the multi-level function and its gradient. Theoretical results and extensive experiments show that both algorithms effectively achieve level-independent convergence rate for nonconvex problems."
}