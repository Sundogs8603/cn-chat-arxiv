{
    "title": "Paste, Inpaint and Harmonize via Denoising: Subject-Driven Image Editing with Pre-Trained Diffusion Model. (arXiv:2306.07596v1 [cs.CV])",
    "abstract": "Text-to-image generative models have attracted rising attention for flexible image editing via user-specified descriptions. However, text descriptions alone are not enough to elaborate the details of subjects, often compromising the subjects' identity or requiring additional per-subject fine-tuning. We introduce a new framework called \\textit{Paste, Inpaint and Harmonize via Denoising} (PhD), which leverages an exemplar image in addition to text descriptions to specify user intentions. In the pasting step, an off-the-shelf segmentation model is employed to identify a user-specified subject within an exemplar image which is subsequently inserted into a background image to serve as an initialization capturing both scene context and subject identity in one. To guarantee the visual coherence of the generated or edited image, we introduce an inpainting and harmonizing module to guide the pre-trained diffusion model to seamlessly blend the inserted subject into the scene naturally. As we kee",
    "link": "http://arxiv.org/abs/2306.07596",
    "context": "Title: Paste, Inpaint and Harmonize via Denoising: Subject-Driven Image Editing with Pre-Trained Diffusion Model. (arXiv:2306.07596v1 [cs.CV])\nAbstract: Text-to-image generative models have attracted rising attention for flexible image editing via user-specified descriptions. However, text descriptions alone are not enough to elaborate the details of subjects, often compromising the subjects' identity or requiring additional per-subject fine-tuning. We introduce a new framework called \\textit{Paste, Inpaint and Harmonize via Denoising} (PhD), which leverages an exemplar image in addition to text descriptions to specify user intentions. In the pasting step, an off-the-shelf segmentation model is employed to identify a user-specified subject within an exemplar image which is subsequently inserted into a background image to serve as an initialization capturing both scene context and subject identity in one. To guarantee the visual coherence of the generated or edited image, we introduce an inpainting and harmonizing module to guide the pre-trained diffusion model to seamlessly blend the inserted subject into the scene naturally. As we kee",
    "path": "papers/23/06/2306.07596.json",
    "total_tokens": 929,
    "translated_title": "通过去噪来实现粘贴、修补和协调：基于预训练扩散模型的主体驱动图像编辑",
    "translated_abstract": "文本到图像生成模型因其可以通过用户指定的描述进行灵活的图像编辑而受到越来越多的关注。然而，仅有文本描述并不足以说明主题的细节，这经常会损害主题的身份或需要额外的针对主题的微调。我们引入了一个名为\"通过去噪实现粘贴、修补和协调\"（PhD）的新框架，该框架利用了除文本描述之外的示例图像来指定用户意图。在粘贴步骤中，使用现成的分割模型来识别示例图像中用户指定的主题，然后将其插入到背景图像中，作为同时捕捉场景上下文和主题身份的初始化。为了保证生成或编辑图像的视觉连贯性，我们引入修补和协调模块，指导预训练扩散模型将插入的主题自然地无缝地融入场景中。",
    "tldr": "本文提出了一个名为PhD的图像编辑框架，使用一个示例图像和文本描述来指定用户意图。该框架包括粘贴、修补和协调步骤，可以将插入的主题无缝融入场景中。",
    "en_tdlr": "The paper proposes a new image editing framework called PhD, which leverages both an exemplar image and text description to specify user intentions. The framework includes steps for pasting, inpainting and harmonizing and can seamlessly blend the inserted subject into the scene."
}