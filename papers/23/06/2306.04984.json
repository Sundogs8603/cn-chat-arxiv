{
    "title": "G$^2$uardFL: Safeguarding Federated Learning Against Backdoor Attacks through Attributed Client Graph Clustering. (arXiv:2306.04984v1 [cs.CR])",
    "abstract": "As a collaborative paradigm, Federated Learning (FL) empowers clients to engage in collective model training without exchanging their respective local data. Nevertheless, FL remains vulnerable to backdoor attacks in which an attacker compromises malicious clients, and injects poisoned model weights into the aggregation process to yield attacker-chosen predictions for particular samples. Existing countermeasures, mainly based on anomaly detection, may erroneously reject legitimate weights while accepting malicious ones, which is due to inadequacies in quantifying client model similarities. Other defense mechanisms prove effective exclusively when confronted with a restricted number of malicious clients, e.g., less than 10%. To address these vulnerabilities, we present G$^2$uardFL, a protective framework that reframes the detection of malicious clients as an attributed graph clustering problem, thereby safeguarding FL systems. This framework employs a client graph clustering technique to",
    "link": "http://arxiv.org/abs/2306.04984",
    "context": "Title: G$^2$uardFL: Safeguarding Federated Learning Against Backdoor Attacks through Attributed Client Graph Clustering. (arXiv:2306.04984v1 [cs.CR])\nAbstract: As a collaborative paradigm, Federated Learning (FL) empowers clients to engage in collective model training without exchanging their respective local data. Nevertheless, FL remains vulnerable to backdoor attacks in which an attacker compromises malicious clients, and injects poisoned model weights into the aggregation process to yield attacker-chosen predictions for particular samples. Existing countermeasures, mainly based on anomaly detection, may erroneously reject legitimate weights while accepting malicious ones, which is due to inadequacies in quantifying client model similarities. Other defense mechanisms prove effective exclusively when confronted with a restricted number of malicious clients, e.g., less than 10%. To address these vulnerabilities, we present G$^2$uardFL, a protective framework that reframes the detection of malicious clients as an attributed graph clustering problem, thereby safeguarding FL systems. This framework employs a client graph clustering technique to",
    "path": "papers/23/06/2306.04984.json",
    "total_tokens": 1130,
    "translated_title": "G$^2$uardFL: 通过属性化客户端图聚类来防御后门攻击的联邦学习保护框架",
    "translated_abstract": "作为协同范式，联邦学习（FL）使客户端能够进行集体模型训练而不交换各自的本地数据。然而，FL仍然容易受到后门攻击的影响，攻击者会通过篡改模型权重注入有毒数据，从而得到针对特定样本的攻击者选择的预测结果。现有的对策主要基于异常检测，但由于量化客户模型相似性的不足，这些对策可能会错误地拒绝合法权重，同时接受恶意权重。其他防御机制仅在面对少量恶意客户端，例如少于10％的恶意客户端时才有效。为了解决这些漏洞，我们提出了G$^2$uardFL，这是一个保护框架，它将检测恶意客户端视为一个属性图聚类问题，从而保护FL系统。该框架采用客户端图聚类技术，根据模型权重的相似性将客户端分类为正常或恶意。通过采用对客户端固有属性进行编码的属性标签，G$^2$uardFL在识别受损客户端方面优于现有的防御机制，而不排除合法客户端。实验结果表明，即使有50％的客户端是恶意的，G$^2$uardFL也能显著降低后门攻击成功率。",
    "tldr": "本论文提出了G$^2$uardFL，这是一个基于属性化客户端图聚类的联邦学习保护框架，能够有效识别恶意客户端，即使恶意客户端数量高达50％。",
    "en_tdlr": "The paper proposes G$^2$uardFL, a protective framework for federated learning based on attributed client graph clustering, which is effective in identifying malicious clients even when up to 50% of clients are malicious."
}