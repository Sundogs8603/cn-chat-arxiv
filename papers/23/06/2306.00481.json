{
    "title": "Automatic Data Augmentation for Domain Adapted Fine-Tuning of Self-Supervised Speech Representations. (arXiv:2306.00481v1 [eess.AS])",
    "abstract": "Self-Supervised Learning (SSL) has allowed leveraging large amounts of unlabeled speech data to improve the performance of speech recognition models even with small annotated datasets. Despite this, speech SSL representations may fail while facing an acoustic mismatch between the pretraining and target datasets. To address this issue, we propose a novel supervised domain adaptation method, designed for cases exhibiting such a mismatch in acoustic domains. It consists in applying properly calibrated data augmentations on a large clean dataset, bringing it closer to the target domain, and using it as part of an initial fine-tuning stage. Augmentations are automatically selected through the minimization of a conditional-dependence estimator, based on the target dataset. The approach is validated during an oracle experiment with controlled distortions and on two amateur-collected low-resource domains, reaching better performances compared to the baselines in both cases.",
    "link": "http://arxiv.org/abs/2306.00481",
    "context": "Title: Automatic Data Augmentation for Domain Adapted Fine-Tuning of Self-Supervised Speech Representations. (arXiv:2306.00481v1 [eess.AS])\nAbstract: Self-Supervised Learning (SSL) has allowed leveraging large amounts of unlabeled speech data to improve the performance of speech recognition models even with small annotated datasets. Despite this, speech SSL representations may fail while facing an acoustic mismatch between the pretraining and target datasets. To address this issue, we propose a novel supervised domain adaptation method, designed for cases exhibiting such a mismatch in acoustic domains. It consists in applying properly calibrated data augmentations on a large clean dataset, bringing it closer to the target domain, and using it as part of an initial fine-tuning stage. Augmentations are automatically selected through the minimization of a conditional-dependence estimator, based on the target dataset. The approach is validated during an oracle experiment with controlled distortions and on two amateur-collected low-resource domains, reaching better performances compared to the baselines in both cases.",
    "path": "papers/23/06/2306.00481.json",
    "total_tokens": 861,
    "translated_title": "自监督语音表示域适应微调的自动数据增强",
    "translated_abstract": "自监督学习（SSL）使得利用大量未标记的语音数据来提高语音识别模型的性能成为可能，即使是只有少量有注释的数据集也能做到。然而，语音SSL表示可能会在预训练和目标数据集之间遇到声学不匹配。为了解决这个问题，我们提出了一种新的监督域适应方法，专门针对声学领域存在不匹配的情况。该方法包括对大型干净数据集应用经过适当校准的数据增强，使其更接近目标域，并将其用作初始微调阶段的一部分。增强通过针对目标数据集基于条件依赖性估计器的最小化自动选择。该方法通过控制失真进行了神谕实验并在两个业余收集的低资源领域进行了验证，在两种情况下均比基线展现出更好的性能。",
    "tldr": "该论文介绍了一种专门针对声学领域不匹配情况下的自动数据增强方法来解决语音自监督学习的表现问题，并在低资源领域中比基线展现出更好的性能。",
    "en_tdlr": "This paper presents an automatic data augmentation method for solving the problem of self-supervised speech representation performance caused by acoustic domain mismatch, and achieves better performance than baselines in low-resource domains."
}