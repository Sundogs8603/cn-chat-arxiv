{
    "title": "Vacant Holes for Unsupervised Detection of the Outliers in Compact Latent Representation. (arXiv:2306.09646v1 [stat.ML])",
    "abstract": "Detection of the outliers is pivotal for any machine learning model deployed and operated in real-world. It is essential for the Deep Neural Networks that were shown to be overconfident with such inputs. Moreover, even deep generative models that allow estimation of the probability density of the input fail in achieving this task. In this work, we concentrate on the specific type of these models: Variational Autoencoders (VAEs). First, we unveil a significant theoretical flaw in the assumption of the classical VAE model. Second, we enforce an accommodating topological property to the image of the deep neural mapping to the latent space: compactness to alleviate the flaw and obtain the means to provably bound the image within the determined limits by squeezing both inliers and outliers together. We enforce compactness using two approaches: (i) Alexandroff extension and (ii) fixed Lipschitz continuity constant on the mapping of the encoder of the VAEs. Finally and most importantly, we di",
    "link": "http://arxiv.org/abs/2306.09646",
    "context": "Title: Vacant Holes for Unsupervised Detection of the Outliers in Compact Latent Representation. (arXiv:2306.09646v1 [stat.ML])\nAbstract: Detection of the outliers is pivotal for any machine learning model deployed and operated in real-world. It is essential for the Deep Neural Networks that were shown to be overconfident with such inputs. Moreover, even deep generative models that allow estimation of the probability density of the input fail in achieving this task. In this work, we concentrate on the specific type of these models: Variational Autoencoders (VAEs). First, we unveil a significant theoretical flaw in the assumption of the classical VAE model. Second, we enforce an accommodating topological property to the image of the deep neural mapping to the latent space: compactness to alleviate the flaw and obtain the means to provably bound the image within the determined limits by squeezing both inliers and outliers together. We enforce compactness using two approaches: (i) Alexandroff extension and (ii) fixed Lipschitz continuity constant on the mapping of the encoder of the VAEs. Finally and most importantly, we di",
    "path": "papers/23/06/2306.09646.json",
    "total_tokens": 1039,
    "translated_title": "用于压缩潜在表示的无监督异常检测方法",
    "translated_abstract": "在真实世界中，对于任何机器学习模型的部署和操作，检测异常值至关重要。对于深度神经网络而言，这一点尤为重要，因为这些网络对于此类输入显示出过度自信。此外，即使是允许估计输入概率密度的深度生成模型也难以完成此任务。本文主要集中于这类模型中的一种：变分自编码器（VAE）。首先，我们揭示了经典VAE模型假设中的一个重大理论缺陷。其次，我们通过引入紧性作为从深度神经映射到潜在空间的图像的拓扑特征来纠正这一缺陷，并获得将图像压缩在确定限制内的可证界限来同时压缩内点和离群点的手段。我们采用两种方法实现紧性：（i）亚历山大夫扩展和（ii）对VAE编码器的映射进行固定的Lipschitz连续性常数。最后但也最重要的是，我们提出了一种基于利用已有建模技术和结构知识的无监督异常检测算法。",
    "tldr": "本文研究了基于变分自编码器（VAE）的无监督异常检测方法，通过引入图像的紧性特征来纠正VAE模型中的理论缺陷并缩小内点与离群点之间的距离，同时结合建模技术和结构知识提出了一种无监督异常检测算法。",
    "en_tdlr": "This paper focuses on unsupervised outlier detection method based on variational autoencoders (VAEs). The paper corrects a significant theoretical flaw in the classical VAE model by introducing compactness as a topological feature of the image from deep neural mapping to the latent space, and proposes two methods for enforcing compactness. The paper also presents an unsupervised outlier detection algorithm that combines modeling techniques and structural knowledge."
}