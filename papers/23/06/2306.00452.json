{
    "title": "Speech Self-Supervised Representation Benchmarking: Are We Doing it Right?. (arXiv:2306.00452v1 [eess.AS])",
    "abstract": "Self-supervised learning (SSL) has recently allowed leveraging large datasets of unlabeled speech signals to reach impressive performance on speech tasks using only small amounts of annotated data. The high number of proposed approaches fostered the need and rise of extended benchmarks that evaluate their performance on a set of downstream tasks exploring various aspects of the speech signal. However, and while the number of considered tasks has been growing, most rely upon a single decoding architecture that maps the frozen SSL representations to the downstream labels. This work investigates the robustness of such benchmarking results to changes in the decoder architecture. Interestingly, it appears that varying the architecture of the downstream decoder leads to significant variations in the leaderboards of most tasks. Concerningly, our study reveals that benchmarking using limited decoders may cause a counterproductive increase in the sizes of the developed SSL models.",
    "link": "http://arxiv.org/abs/2306.00452",
    "context": "Title: Speech Self-Supervised Representation Benchmarking: Are We Doing it Right?. (arXiv:2306.00452v1 [eess.AS])\nAbstract: Self-supervised learning (SSL) has recently allowed leveraging large datasets of unlabeled speech signals to reach impressive performance on speech tasks using only small amounts of annotated data. The high number of proposed approaches fostered the need and rise of extended benchmarks that evaluate their performance on a set of downstream tasks exploring various aspects of the speech signal. However, and while the number of considered tasks has been growing, most rely upon a single decoding architecture that maps the frozen SSL representations to the downstream labels. This work investigates the robustness of such benchmarking results to changes in the decoder architecture. Interestingly, it appears that varying the architecture of the downstream decoder leads to significant variations in the leaderboards of most tasks. Concerningly, our study reveals that benchmarking using limited decoders may cause a counterproductive increase in the sizes of the developed SSL models.",
    "path": "papers/23/06/2306.00452.json",
    "total_tokens": 848,
    "translated_title": "语音自监督表示基准测试：我们做得对吗？",
    "translated_abstract": "最近，自监督学习（SSL）使得利用大量未标记的语音信号数据集，在只有少量注释数据的情况下，在语音任务上达到了令人印象深刻的性能。大量提出的方法促进了基准测试的需求和崛起，该测试评估它们在一组探索语音信号的各个方面的下游任务上的性能。然而，虽然涉及的任务数量正在增长，但大多数任务都依赖于单个解码器，将冻结的SSL表示映射到下游标签。本文研究了这种基准测试结果对解码器架构变化的鲁棒性。有趣的是，改变下游解码器的结构会导致大多数任务排行榜的显着变化。令人担忧的是，我们的研究表明，使用有限的解码器进行基准测试可能会导致开发的SSL模型的大小不必要地增加。",
    "tldr": "研究发现语音自监督表示的基准测试结果对解码器架构变化很敏感，而使用有限解码器进行基准测试可能会导致开发的模型过大。",
    "en_tdlr": "The study shows that the benchmarking results of self-supervised speech representation are sensitive to the changes in the decoder architecture, and using limited decoders for benchmarking may cause a counterproductive increase in the developed models."
}