{
    "title": "A Simple Unified Uncertainty-Guided Framework for Offline-to-Online Reinforcement Learning. (arXiv:2306.07541v1 [cs.LG])",
    "abstract": "Offline reinforcement learning (RL) provides a promising solution to learning an agent fully relying on a data-driven paradigm. However, constrained by the limited quality of the offline dataset, its performance is often sub-optimal. Therefore, it is desired to further finetune the agent via extra online interactions before deployment. Unfortunately, offline-to-online RL can be challenging due to two main challenges: constrained exploratory behavior and state-action distribution shift. To this end, we propose a Simple Unified uNcertainty-Guided (SUNG) framework, which naturally unifies the solution to both challenges with the tool of uncertainty. Specifically, SUNG quantifies uncertainty via a VAE-based state-action visitation density estimator. To facilitate efficient exploration, SUNG presents a practical optimistic exploration strategy to select informative actions with both high value and high uncertainty. Moreover, SUNG develops an adaptive exploitation method by applying conserva",
    "link": "http://arxiv.org/abs/2306.07541",
    "context": "Title: A Simple Unified Uncertainty-Guided Framework for Offline-to-Online Reinforcement Learning. (arXiv:2306.07541v1 [cs.LG])\nAbstract: Offline reinforcement learning (RL) provides a promising solution to learning an agent fully relying on a data-driven paradigm. However, constrained by the limited quality of the offline dataset, its performance is often sub-optimal. Therefore, it is desired to further finetune the agent via extra online interactions before deployment. Unfortunately, offline-to-online RL can be challenging due to two main challenges: constrained exploratory behavior and state-action distribution shift. To this end, we propose a Simple Unified uNcertainty-Guided (SUNG) framework, which naturally unifies the solution to both challenges with the tool of uncertainty. Specifically, SUNG quantifies uncertainty via a VAE-based state-action visitation density estimator. To facilitate efficient exploration, SUNG presents a practical optimistic exploration strategy to select informative actions with both high value and high uncertainty. Moreover, SUNG develops an adaptive exploitation method by applying conserva",
    "path": "papers/23/06/2306.07541.json",
    "total_tokens": 1046,
    "translated_title": "一种简单统一的基于不确定性引导的离线到在线强化学习框架",
    "translated_abstract": "离线强化学习为依靠数据驱动范例学习智能体提供了一种有前途的解决方案。 然而，受限于离线数据集的有限质量，其性能常常不够优秀。因此，在部署之前通过额外的在线交互进一步微调智能体是有必要的。不幸的是，由于受到两个主要挑战的制约，即受限的探索行为和状态-动作分布偏移，离线到在线强化学习可能具有挑战性。为此，我们提出了一个简单统一的基于不确定性引导的（SUNG）框架，其通过不确定性工具自然地统一了这两个挑战的解决方案。具体而言，SUNG通过基于VAE的状态-动作访问密度估计器量化不确定性。为了促进高效探索，SUNG提出了一种实用的乐观探索策略，以选择具有高价值和高不确定性的信息动作。此外，SUNG通过在不确定性指导下应用保守Q值估计来开发一种自适应利用方法。我们在Atari和MuJoCo基准测试上进行了全面的实验，结果表明SUNG始终优于最先进的离线到在线强化学习方法，并在许多任务中实现了接近在线学习的性能。",
    "tldr": "SUNG是一种基于不确定性引导的离线到在线强化学习框架，在通过量化不确定性进行探索和应用保守Q值估计的指导下，实现了高效的老化强化学习。",
    "en_tdlr": "SUNG is an uncertainty-guided offline-to-online reinforcement learning framework that achieves efficient lifelong learning through exploration quantification and conservative Q-value estimation under the guidance of uncertainty."
}