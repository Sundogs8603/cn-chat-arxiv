{
    "title": "Federated Few-shot Learning. (arXiv:2306.10234v2 [cs.LG] UPDATED)",
    "abstract": "Federated Learning (FL) enables multiple clients to collaboratively learn a machine learning model without exchanging their own local data. In this way, the server can exploit the computational power of all clients and train the model on a larger set of data samples among all clients. Although such a mechanism is proven to be effective in various fields, existing works generally assume that each client preserves sufficient data for training. In practice, however, certain clients may only contain a limited number of samples (i.e., few-shot samples). For example, the available photo data taken by a specific user with a new mobile device is relatively rare. In this scenario, existing FL efforts typically encounter a significant performance drop on these clients. Therefore, it is urgent to develop a few-shot model that can generalize to clients with limited data under the FL scenario. In this paper, we refer to this novel problem as federated few-shot learning. Nevertheless, the problem re",
    "link": "http://arxiv.org/abs/2306.10234",
    "context": "Title: Federated Few-shot Learning. (arXiv:2306.10234v2 [cs.LG] UPDATED)\nAbstract: Federated Learning (FL) enables multiple clients to collaboratively learn a machine learning model without exchanging their own local data. In this way, the server can exploit the computational power of all clients and train the model on a larger set of data samples among all clients. Although such a mechanism is proven to be effective in various fields, existing works generally assume that each client preserves sufficient data for training. In practice, however, certain clients may only contain a limited number of samples (i.e., few-shot samples). For example, the available photo data taken by a specific user with a new mobile device is relatively rare. In this scenario, existing FL efforts typically encounter a significant performance drop on these clients. Therefore, it is urgent to develop a few-shot model that can generalize to clients with limited data under the FL scenario. In this paper, we refer to this novel problem as federated few-shot learning. Nevertheless, the problem re",
    "path": "papers/23/06/2306.10234.json",
    "total_tokens": 1035,
    "translated_title": "联邦少样本学习",
    "translated_abstract": "联邦学习使得多个客户端可以在不交换本地数据的情况下协作学习一个机器学习模型。然而，现有的方法通常假设每个客户端都有足够的数据用于训练，而事实上某些客户端可能只有有限数量的样本，即少样本数据。这种情况下，现有的联邦学习方法可能在这些客户端上遇到显著的性能下降。本文提出了一个处理这一问题的框架，命名为联邦少样本学习。我们设计了特征提取模块和任务适应模块来提高模型对于少样本客户端的泛化能力，并利用注意力机制来动态地调整每个客户端在训练过程中的重要性。实验结果表明，我们的方法在各种数据集上均取得了最先进的联邦少样本学习性能。",
    "tldr": "本研究提出了一种名为“联邦少样本学习”的新问题，旨在解决联邦学习在少样本数据上的性能问题。我们提出了一个简单而有效的框架，使用特征提取和任务适应模块以及注意力机制来提高模型对于少样本客户端的泛化能力，在各种数据集上取得了最先进的联邦少样本学习性能。",
    "en_tdlr": "This research introduces a new problem called \"Federated Few-shot learning\" to address the performance issue of federated learning on clients with limited data. They proposed a framework using feature extraction and task adaptation modules, as well as an attention mechanism, to improve the model's generalization ability. Experimental results show that their method achieves state-of-the-art performance for federated few-shot learning on various datasets."
}