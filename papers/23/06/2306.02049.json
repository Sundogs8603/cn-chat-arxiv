{
    "title": "LambdaBeam: Neural Program Search with Higher-Order Functions and Lambdas. (arXiv:2306.02049v2 [cs.LG] UPDATED)",
    "abstract": "Search is an important technique in program synthesis that allows for adaptive strategies such as focusing on particular search directions based on execution results. Several prior works have demonstrated that neural models are effective at guiding program synthesis searches. However, a common drawback of those approaches is the inability to handle iterative loops, higher-order functions, or lambda functions, thus limiting prior neural searches from synthesizing longer and more general programs. We address this gap by designing a search algorithm called LambdaBeam that can construct arbitrary lambda functions that compose operations within a given DSL. We create semantic vector representations of the execution behavior of the lambda functions and train a neural policy network to choose which lambdas to construct during search, and pass them as arguments to higher-order functions to perform looping computations. Our experiments show that LambdaBeam outperforms neural, symbolic, and LLM-",
    "link": "http://arxiv.org/abs/2306.02049",
    "context": "Title: LambdaBeam: Neural Program Search with Higher-Order Functions and Lambdas. (arXiv:2306.02049v2 [cs.LG] UPDATED)\nAbstract: Search is an important technique in program synthesis that allows for adaptive strategies such as focusing on particular search directions based on execution results. Several prior works have demonstrated that neural models are effective at guiding program synthesis searches. However, a common drawback of those approaches is the inability to handle iterative loops, higher-order functions, or lambda functions, thus limiting prior neural searches from synthesizing longer and more general programs. We address this gap by designing a search algorithm called LambdaBeam that can construct arbitrary lambda functions that compose operations within a given DSL. We create semantic vector representations of the execution behavior of the lambda functions and train a neural policy network to choose which lambdas to construct during search, and pass them as arguments to higher-order functions to perform looping computations. Our experiments show that LambdaBeam outperforms neural, symbolic, and LLM-",
    "path": "papers/23/06/2306.02049.json",
    "total_tokens": 766,
    "translated_title": "LambdaBeam：具有高阶函数和Lambda的神经程序搜索",
    "translated_abstract": "搜索是程序合成中的一种重要技术，它允许根据执行结果采用自适应策略，例如专注于特定搜索方向。几项先前的研究已经证明神经模型在引导程序合成搜索方面是有效的。然而，这些方法的一个常见缺点是无法处理迭代循环、高阶函数或lambda函数，从而限制了先前的神经搜索合成更长且更通用的程序。为解决这一问题，我们设计了一种名为LambdaBeam的搜索算法，可以构建在给定DSL内组合操作的任意lambda函数。我们创建了lambda函数执行行为的语义向量表示，并训练了一个神经策略网络，在搜索过程中选择要构建的lambda函数，并将其作为参数传递给高阶函数执行循环计算。我们的实验表明，LambdaBeam在神经、符号和LLM方法上表现出色。",
    "tldr": "LambdaBeam是一种神经程序搜索算法，通过构建任意lambda函数，并将其作为参数传递给高阶函数来解决先前神经搜索合成更长且更通用程序的限制。"
}