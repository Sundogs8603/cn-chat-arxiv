{
    "title": "Bayesian Non-linear Latent Variable Modeling via Random Fourier Features. (arXiv:2306.08352v1 [stat.ML])",
    "abstract": "The Gaussian process latent variable model (GPLVM) is a popular probabilistic method used for nonlinear dimension reduction, matrix factorization, and state-space modeling. Inference for GPLVMs is computationally tractable only when the data likelihood is Gaussian. Moreover, inference for GPLVMs has typically been restricted to obtaining maximum a posteriori point estimates, which can lead to overfitting, or variational approximations, which mischaracterize the posterior uncertainty. Here, we present a method to perform Markov chain Monte Carlo (MCMC) inference for generalized Bayesian nonlinear latent variable modeling. The crucial insight necessary to generalize GPLVMs to arbitrary observation models is that we approximate the kernel function in the Gaussian process mappings with random Fourier features; this allows us to compute the gradient of the posterior in closed form with respect to the latent variables. We show that we can generalize GPLVMs to non-Gaussian observations, such ",
    "link": "http://arxiv.org/abs/2306.08352",
    "context": "Title: Bayesian Non-linear Latent Variable Modeling via Random Fourier Features. (arXiv:2306.08352v1 [stat.ML])\nAbstract: The Gaussian process latent variable model (GPLVM) is a popular probabilistic method used for nonlinear dimension reduction, matrix factorization, and state-space modeling. Inference for GPLVMs is computationally tractable only when the data likelihood is Gaussian. Moreover, inference for GPLVMs has typically been restricted to obtaining maximum a posteriori point estimates, which can lead to overfitting, or variational approximations, which mischaracterize the posterior uncertainty. Here, we present a method to perform Markov chain Monte Carlo (MCMC) inference for generalized Bayesian nonlinear latent variable modeling. The crucial insight necessary to generalize GPLVMs to arbitrary observation models is that we approximate the kernel function in the Gaussian process mappings with random Fourier features; this allows us to compute the gradient of the posterior in closed form with respect to the latent variables. We show that we can generalize GPLVMs to non-Gaussian observations, such ",
    "path": "papers/23/06/2306.08352.json",
    "total_tokens": 861,
    "translated_title": "基于随机傅里叶特征的贝叶斯非线性潜变量建模",
    "translated_abstract": "高斯过程潜变量模型(GPLVM)是一种广泛用于非线性降维、矩阵分解和状态空间建模的概率方法。当数据似然为高斯分布时，GPLVM的推断才是可计算的。此外，GPLVM的推断通常被限制在获得最大后验概率点估计 or 变分近似，这可能会导致过度拟合或误判后验不确定性。本文提出了一种通过MCMC进行广义贝叶斯非线性潜变量建模推断的方法。将高斯过程映射中的核函数用随机傅里叶特征近似的关键洞察力是我们可以计算出相对于潜变量的后验梯度，从而将GPLVM推广到非高斯观测。",
    "tldr": "本文介绍了一种基于随机傅里叶特征的方法，可以将GPLVM推广到非高斯观测情况下的广义贝叶斯非线性潜变量建模，并提出了一种通过MCMC进行推断的方法。",
    "en_tdlr": "This paper proposes a method based on random Fourier features to generalize Gaussian process latent variable models (GPLVMs) to non-Gaussian observations, and presents a Markov chain Monte Carlo (MCMC) inference method for generalized Bayesian nonlinear latent variable modeling."
}