{
    "title": "Probing the Transition to Dataset-Level Privacy in ML Models Using an Output-Specific and Data-Resolved Privacy Profile. (arXiv:2306.15790v1 [cs.LG])",
    "abstract": "Differential privacy (DP) is the prevailing technique for protecting user data in machine learning models. However, deficits to this framework include a lack of clarity for selecting the privacy budget $\\epsilon$ and a lack of quantification for the privacy leakage for a particular data row by a particular trained model. We make progress toward these limitations and a new perspective by which to visualize DP results by studying a privacy metric that quantifies the extent to which a model trained on a dataset using a DP mechanism is ``covered\" by each of the distributions resulting from training on neighboring datasets. We connect this coverage metric to what has been established in the literature and use it to rank the privacy of individual samples from the training set in what we call a privacy profile. We additionally show that the privacy profile can be used to probe an observed transition to indistinguishability that takes place in the neighboring distributions as $\\epsilon$ decrea",
    "link": "http://arxiv.org/abs/2306.15790",
    "context": "Title: Probing the Transition to Dataset-Level Privacy in ML Models Using an Output-Specific and Data-Resolved Privacy Profile. (arXiv:2306.15790v1 [cs.LG])\nAbstract: Differential privacy (DP) is the prevailing technique for protecting user data in machine learning models. However, deficits to this framework include a lack of clarity for selecting the privacy budget $\\epsilon$ and a lack of quantification for the privacy leakage for a particular data row by a particular trained model. We make progress toward these limitations and a new perspective by which to visualize DP results by studying a privacy metric that quantifies the extent to which a model trained on a dataset using a DP mechanism is ``covered\" by each of the distributions resulting from training on neighboring datasets. We connect this coverage metric to what has been established in the literature and use it to rank the privacy of individual samples from the training set in what we call a privacy profile. We additionally show that the privacy profile can be used to probe an observed transition to indistinguishability that takes place in the neighboring distributions as $\\epsilon$ decrea",
    "path": "papers/23/06/2306.15790.json",
    "total_tokens": 979,
    "translated_title": "利用输出特定和数据解析的隐私配置文件研究机器学习模型中的数据集级隐私转换",
    "translated_abstract": "差分隐私（DP）是保护机器学习模型中用户数据的主要技术。然而，这种框架存在选择隐私预算ε的不清晰以及特定训练模型对特定数据行的隐私泄露量的缺乏量化等问题。我们通过研究一个将使用DP机制在数据集上训练的模型与训练相邻数据集产生的分布的“覆盖程度”进行量化的隐私指标，解决了这些限制，并提出了一种新的视角来可视化DP结果。我们将这种覆盖程度指标与文献中的已有研究相连接，并用它来对训练集中个别样本的隐私进行排名，形成了隐私配置文件。我们还展示了隐私配置文件可以用来探测观察到的邻近分布中发生的不可区分性转换随着ε的递减",
    "tldr": "本论文针对差分隐私（DP）框架的不足，研究了使用DP机制训练的模型在相邻数据集上的“覆盖程度”。通过连接覆盖程度指标和已有研究，我们排名了训练集中个别样本的隐私，并形成了一个隐私配置文件。此外，我们展示了隐私配置文件可以用来探测观察到的隐私转换。",
    "en_tdlr": "This paper addresses the deficiencies in the differential privacy (DP) framework by studying the \"coverage\" of models trained using the DP mechanism on neighboring datasets. By connecting the coverage metric to existing research, we rank the privacy of individual samples in the training set and create a privacy profile. Additionally, we demonstrate that the privacy profile can be used to probe observed privacy transitions."
}