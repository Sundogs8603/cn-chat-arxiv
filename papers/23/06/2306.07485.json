{
    "title": "Learning Unnormalized Statistical Models via Compositional Optimization. (arXiv:2306.07485v1 [cs.LG])",
    "abstract": "Learning unnormalized statistical models (e.g., energy-based models) is computationally challenging due to the complexity of handling the partition function. To eschew this complexity, noise-contrastive estimation~(NCE) has been proposed by formulating the objective as the logistic loss of the real data and the artificial noise. However, as found in previous works, NCE may perform poorly in many tasks due to its flat loss landscape and slow convergence. In this paper, we study it a direct approach for optimizing the negative log-likelihood of unnormalized models from the perspective of compositional optimization. To tackle the partition function, a noise distribution is introduced such that the log partition function can be written as a compositional function whose inner function can be estimated with stochastic samples. Hence, the objective can be optimized by stochastic compositional optimization algorithms. Despite being a simple method, we demonstrate that it is more favorable than",
    "link": "http://arxiv.org/abs/2306.07485",
    "context": "Title: Learning Unnormalized Statistical Models via Compositional Optimization. (arXiv:2306.07485v1 [cs.LG])\nAbstract: Learning unnormalized statistical models (e.g., energy-based models) is computationally challenging due to the complexity of handling the partition function. To eschew this complexity, noise-contrastive estimation~(NCE) has been proposed by formulating the objective as the logistic loss of the real data and the artificial noise. However, as found in previous works, NCE may perform poorly in many tasks due to its flat loss landscape and slow convergence. In this paper, we study it a direct approach for optimizing the negative log-likelihood of unnormalized models from the perspective of compositional optimization. To tackle the partition function, a noise distribution is introduced such that the log partition function can be written as a compositional function whose inner function can be estimated with stochastic samples. Hence, the objective can be optimized by stochastic compositional optimization algorithms. Despite being a simple method, we demonstrate that it is more favorable than",
    "path": "papers/23/06/2306.07485.json",
    "total_tokens": 906,
    "translated_title": "通过组合优化学习非归一化统计模型",
    "translated_abstract": "学习非归一化统计模型（如能量模型）由于处理分区函数的复杂性而具有计算上的挑战。为了避开这种复杂性，噪声对比估计（NCE）已被提出来将目标公式化为实际数据和人为噪声的逻辑损失。然而，正如以前的研究所发现的那样，由于其平坦的损失函数图景和缓慢的收敛速度，NCE在许多任务中表现较差。本文从组合优化的角度研究了一种直接优化非归一化模型的负对数似然的方法。为了处理分区函数，引入了噪声分布，使得对数分区函数可以被写成一个组合函数，内部函数可以用随机样本估计。因此，目标可以通过随机组合优化算法进行优化。尽管它是一种简单的方法，但我们证明它在学习非归一化统计模型的优化效率和泛化性能方面比NCE更有利。",
    "tldr": "本文介绍了一种通过组合优化的方式学习非归一化统计模型的方法，采用噪声分布处理分区函数，具有优化效率和泛化性能更好的特点。",
    "en_tdlr": "This paper introduces a method for learning unnormalized statistical models via compositional optimization, which uses a noise distribution to handle the partition function and has better optimization efficiency and generalization performance than NCE."
}