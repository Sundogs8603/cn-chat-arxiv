{
    "title": "Permutation Decision Trees. (arXiv:2306.02617v2 [cs.LG] UPDATED)",
    "abstract": "Decision Tree is a well understood Machine Learning model that is based on minimizing impurities in the internal nodes. The most common impurity measures are Shannon entropy and Gini impurity. These impurity measures are insensitive to the order of training data and hence the final tree obtained is invariant to any permutation of the data. This leads to a serious limitation in modeling data instances that have order dependencies. In this work, we propose the use of Effort-To-Compress (ETC) - a complexity measure, for the first time, as an impurity measure. Unlike Shannon entropy and Gini impurity, structural impurity based on ETC is able to capture order dependencies in the data, thus obtaining potentially different decision trees for different permutations of the same data instances (Permutation Decision Trees). We then introduce the notion of Permutation Bagging achieved using permutation decision trees without the need for random feature selection and sub-sampling. We compare the pe",
    "link": "http://arxiv.org/abs/2306.02617",
    "context": "Title: Permutation Decision Trees. (arXiv:2306.02617v2 [cs.LG] UPDATED)\nAbstract: Decision Tree is a well understood Machine Learning model that is based on minimizing impurities in the internal nodes. The most common impurity measures are Shannon entropy and Gini impurity. These impurity measures are insensitive to the order of training data and hence the final tree obtained is invariant to any permutation of the data. This leads to a serious limitation in modeling data instances that have order dependencies. In this work, we propose the use of Effort-To-Compress (ETC) - a complexity measure, for the first time, as an impurity measure. Unlike Shannon entropy and Gini impurity, structural impurity based on ETC is able to capture order dependencies in the data, thus obtaining potentially different decision trees for different permutations of the same data instances (Permutation Decision Trees). We then introduce the notion of Permutation Bagging achieved using permutation decision trees without the need for random feature selection and sub-sampling. We compare the pe",
    "path": "papers/23/06/2306.02617.json",
    "total_tokens": 945,
    "translated_title": "排列决策树",
    "translated_abstract": "决策树是一种基于最小化内部节点中的不纯度的机器学习模型。最常见的不纯度度量是香农熵和基尼不纯度。这些不纯度度量对训练数据的顺序不敏感，因此得到的最终树对数据的任何排列都是不变的。这导致了在建模存在顺序依赖性的数据实例时的严重限制。在这项工作中，我们首次提出使用“压缩努力”(ETC) - 一种复杂度度量，作为不纯度度量。与香农熵和基尼不纯度不同，基于ETC的结构性不纯度能够捕捉到数据的顺序依赖性，从而为相同数据实例的不同排列获得潜在不同的决策树（排列决策树）。然后，我们引入了使用排列决策树实现排列Bagging的概念，而无需随机特征选择和子采样。我们比较了翻译包…(信息不全)",
    "tldr": "该论文提出了一种称为排列决策树的模型，通过引入一种新的复杂度度量方法，能够捕捉到数据实例的顺序依赖性，在不同排列的数据实例上得到不同的决策树模型，从而克服了传统决策树模型在处理顺序相关数据时的限制。"
}