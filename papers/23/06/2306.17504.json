{
    "title": "Systematic Investigation of Sparse Perturbed Sharpness-Aware Minimization Optimizer. (arXiv:2306.17504v1 [cs.AI])",
    "abstract": "Deep neural networks often suffer from poor generalization due to complex and non-convex loss landscapes. Sharpness-Aware Minimization (SAM) is a popular solution that smooths the loss landscape by minimizing the maximized change of training loss when adding a perturbation to the weight. However, indiscriminate perturbation of SAM on all parameters is suboptimal and results in excessive computation, double the overhead of common optimizers like Stochastic Gradient Descent (SGD). In this paper, we propose Sparse SAM (SSAM), an efficient and effective training scheme that achieves sparse perturbation by a binary mask. To obtain the sparse mask, we provide two solutions based on Fisher information and dynamic sparse training, respectively. We investigate the impact of different masks, including unstructured, structured, and $N$:$M$ structured patterns, as well as explicit and implicit forms of implementing sparse perturbation. We theoretically prove that SSAM can converge at the same rate",
    "link": "http://arxiv.org/abs/2306.17504",
    "context": "Title: Systematic Investigation of Sparse Perturbed Sharpness-Aware Minimization Optimizer. (arXiv:2306.17504v1 [cs.AI])\nAbstract: Deep neural networks often suffer from poor generalization due to complex and non-convex loss landscapes. Sharpness-Aware Minimization (SAM) is a popular solution that smooths the loss landscape by minimizing the maximized change of training loss when adding a perturbation to the weight. However, indiscriminate perturbation of SAM on all parameters is suboptimal and results in excessive computation, double the overhead of common optimizers like Stochastic Gradient Descent (SGD). In this paper, we propose Sparse SAM (SSAM), an efficient and effective training scheme that achieves sparse perturbation by a binary mask. To obtain the sparse mask, we provide two solutions based on Fisher information and dynamic sparse training, respectively. We investigate the impact of different masks, including unstructured, structured, and $N$:$M$ structured patterns, as well as explicit and implicit forms of implementing sparse perturbation. We theoretically prove that SSAM can converge at the same rate",
    "path": "papers/23/06/2306.17504.json",
    "total_tokens": 928,
    "translated_title": "稀疏扰动的锐化感知最小化优化器的系统研究",
    "translated_abstract": "深度神经网络由于复杂且非凸的损失景观而经常在泛化上表现不佳。锐化感知最小化（SAM）是一种流行的解决方案，通过最小化权重添加扰动时训练损失的最大化变化来平滑损失景观。然而，SAM对所有参数的不加区分的扰动是次优的，并且导致计算过多，两倍于常见优化器如随机梯度下降（SGD）的开销。在本文中，我们提出了一种高效和有效的训练方案Sparse SAM（SSAM），通过二进制掩码实现稀疏扰动。为了获得稀疏掩码，我们提供了基于Fisher信息和动态稀疏训练的两种解决方案。我们研究了不同掩码的影响，包括非结构化、结构化和N:M结构化模式，以及实现稀疏扰动的显式和隐式形式。我们在理论上证明了SSAM可以以相同的速度收敛。",
    "tldr": "本论文系统研究了一种稀疏扰动的锐化感知最小化优化器（Sparse SAM），该优化器通过二进制掩码实现稀疏扰动，有效地平滑了深度神经网络的损失景观。",
    "en_tdlr": "This paper systematically investigates a sparse perturbed sharpness-aware minimization optimizer (Sparse SAM), which achieves sparse perturbation through binary masks, effectively smoothing the loss landscape of deep neural networks."
}