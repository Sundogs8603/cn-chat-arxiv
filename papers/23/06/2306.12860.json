{
    "title": "Learning from Visual Observation via Offline Pretrained State-to-Go Transformer. (arXiv:2306.12860v1 [cs.LG])",
    "abstract": "Learning from visual observation (LfVO), aiming at recovering policies from only visual observation data, is promising yet a challenging problem. Existing LfVO approaches either only adopt inefficient online learning schemes or require additional task-specific information like goal states, making them not suited for open-ended tasks. To address these issues, we propose a two-stage framework for learning from visual observation. In the first stage, we introduce and pretrain State-to-Go (STG) Transformer offline to predict and differentiate latent transitions of demonstrations. Subsequently, in the second stage, the STG Transformer provides intrinsic rewards for downstream reinforcement learning tasks where an agent learns merely from intrinsic rewards. Empirical results on Atari and Minecraft show that our proposed method outperforms baselines and in some tasks even achieves performance comparable to the policy learned from environmental rewards. These results shed light on the potentia",
    "link": "http://arxiv.org/abs/2306.12860",
    "context": "Title: Learning from Visual Observation via Offline Pretrained State-to-Go Transformer. (arXiv:2306.12860v1 [cs.LG])\nAbstract: Learning from visual observation (LfVO), aiming at recovering policies from only visual observation data, is promising yet a challenging problem. Existing LfVO approaches either only adopt inefficient online learning schemes or require additional task-specific information like goal states, making them not suited for open-ended tasks. To address these issues, we propose a two-stage framework for learning from visual observation. In the first stage, we introduce and pretrain State-to-Go (STG) Transformer offline to predict and differentiate latent transitions of demonstrations. Subsequently, in the second stage, the STG Transformer provides intrinsic rewards for downstream reinforcement learning tasks where an agent learns merely from intrinsic rewards. Empirical results on Atari and Minecraft show that our proposed method outperforms baselines and in some tasks even achieves performance comparable to the policy learned from environmental rewards. These results shed light on the potentia",
    "path": "papers/23/06/2306.12860.json",
    "total_tokens": 789,
    "translated_title": "通过离线预训练状态到目标Transformer视觉观察学习",
    "translated_abstract": "视觉观察学习旨在仅从视觉观察数据中恢复策略，是一个有前途但具有挑战性的问题。为了解决这些问题，我们提出了一个学习框架，其中第一阶段引入并离线预训练状态到目标(STG) Transformer以预测和区分演示的潜在转换。随后，在第二阶段中，STG Transformer为下游强化学习任务提供内在奖励，其中代理仅从内在奖励中学习。实验结果表明，我们提出的方法在Atari和Minecraft上优于基线，甚至在某些任务中可以达到与从环境奖励学习的策略相当的性能。",
    "tldr": "该论文提出了一个学习框架，其中使用离线预训练和内在奖励来解决视觉观察学习的挑战性问题，在Atari和Minecraft上取得了优异的实验结果。"
}