{
    "title": "Two Independent Teachers are Better Role Model. (arXiv:2306.05745v1 [eess.IV])",
    "abstract": "Recent deep learning models have attracted substantial attention in infant brain analysis. These models have performed state-of-the-art performance, such as semi-supervised techniques (e.g., Temporal Ensembling, mean teacher). However, these models depend on an encoder-decoder structure with stacked local operators to gather long-range information, and the local operators limit the efficiency and effectiveness. Besides, the $MRI$ data contain different tissue properties ($TPs$) such as $T1$ and $T2$. One major limitation of these models is that they use both data as inputs to the segment process, i.e., the models are trained on the dataset once, and it requires much computational and memory requirements during inference. In this work, we address the above limitations by designing a new deep-learning model, called 3D-DenseUNet, which works as adaptable global aggregation blocks in down-sampling to solve the issue of spatial information loss. The self-attention module connects the down-s",
    "link": "http://arxiv.org/abs/2306.05745",
    "context": "Title: Two Independent Teachers are Better Role Model. (arXiv:2306.05745v1 [eess.IV])\nAbstract: Recent deep learning models have attracted substantial attention in infant brain analysis. These models have performed state-of-the-art performance, such as semi-supervised techniques (e.g., Temporal Ensembling, mean teacher). However, these models depend on an encoder-decoder structure with stacked local operators to gather long-range information, and the local operators limit the efficiency and effectiveness. Besides, the $MRI$ data contain different tissue properties ($TPs$) such as $T1$ and $T2$. One major limitation of these models is that they use both data as inputs to the segment process, i.e., the models are trained on the dataset once, and it requires much computational and memory requirements during inference. In this work, we address the above limitations by designing a new deep-learning model, called 3D-DenseUNet, which works as adaptable global aggregation blocks in down-sampling to solve the issue of spatial information loss. The self-attention module connects the down-s",
    "path": "papers/23/06/2306.05745.json",
    "total_tokens": 1061,
    "translated_title": "两个独立训练器是更好的模范",
    "translated_abstract": "最近深度学习模型在婴儿脑部分析中引起了巨大的关注。这些模型表现出最先进的性能，例如半监督技术（例如，时间集成，平均教师）。然而，这些模型依赖于编码器-解码器结构，用堆叠的局部运算符来收集远程信息，而局部运算符限制了效率和效果。此外，MRI数据包含不同的组织特性（TPs），例如T1和T2。这些模型的一个主要限制是，它们将两种数据都作为输入用于分割过程，即模型一次训练于数据集，推断过程中需要大量的计算和存储空间。在本文中，我们通过设计一个名为3D-DenseUNet的新深度学习模型来解决上述限制，其作为自适应全局聚合块在下采样中工作以解决空间信息丢失的问题。自注意模块将下采样层与上采样层连接在一起，以收集远程信息。此外，我们提出的模型通过训练两个独立的训练器处理不同的组织特性，并取得比当前最先进模型更好的结果。",
    "tldr": "提出了一种名为3D-DenseUNet的新深度学习模型，该模型采用自适应全局聚合块和自注意模块，以解决半监督技术中的效率和效果问题，同时通过训练两个独立的训练器处理不同的组织特性，在婴儿脑部分析中取得更好的结果。",
    "en_tdlr": "A new deep learning model called 3D-DenseUNet is proposed to address the efficiency and effectiveness issues in semi-supervised techniques by using adaptable global aggregation blocks and self-attention module. The model achieves better results in infant brain analysis by training two independent teachers to handle different tissue properties."
}