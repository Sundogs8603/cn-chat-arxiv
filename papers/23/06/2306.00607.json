{
    "title": "FACT: Federated Adversarial Cross Training. (arXiv:2306.00607v2 [cs.LG] UPDATED)",
    "abstract": "Federated Learning (FL) facilitates distributed model development to aggregate multiple confidential data sources. The information transfer among clients can be compromised by distributional differences, i.e., by non-i.i.d. data. A particularly challenging scenario is the federated model adaptation to a target client without access to annotated data. We propose Federated Adversarial Cross Training (FACT), which uses the implicit domain differences between source clients to identify domain shifts in the target domain. In each round of FL, FACT cross initializes a pair of source clients to generate domain specialized representations which are then used as a direct adversary to learn a domain invariant data representation. We empirically show that FACT outperforms state-of-the-art federated, non-federated and source-free domain adaptation models on three popular multi-source-single-target benchmarks, and state-of-the-art Unsupervised Domain Adaptation (UDA) models on single-source-single-",
    "link": "http://arxiv.org/abs/2306.00607",
    "context": "Title: FACT: Federated Adversarial Cross Training. (arXiv:2306.00607v2 [cs.LG] UPDATED)\nAbstract: Federated Learning (FL) facilitates distributed model development to aggregate multiple confidential data sources. The information transfer among clients can be compromised by distributional differences, i.e., by non-i.i.d. data. A particularly challenging scenario is the federated model adaptation to a target client without access to annotated data. We propose Federated Adversarial Cross Training (FACT), which uses the implicit domain differences between source clients to identify domain shifts in the target domain. In each round of FL, FACT cross initializes a pair of source clients to generate domain specialized representations which are then used as a direct adversary to learn a domain invariant data representation. We empirically show that FACT outperforms state-of-the-art federated, non-federated and source-free domain adaptation models on three popular multi-source-single-target benchmarks, and state-of-the-art Unsupervised Domain Adaptation (UDA) models on single-source-single-",
    "path": "papers/23/06/2306.00607.json",
    "total_tokens": 909,
    "translated_title": "FACT: 联邦敌对交叉训练",
    "translated_abstract": "联邦学习（FL）可以促进分布式模型开发，以聚合多个机密数据源。客户之间的信息传输可能受到分布差异的影响，即非独立同分布的数据。一个特别具有挑战性的情景是将联邦模型适应到目标客户端，而该客户端无法访问带注释的数据。我们提出了联邦敌对交叉训练（FACT），利用源客户端之间的隐式领域差异来识别目标领域的领域转移。在FL的每一轮中，FACT交叉初始化一对源客户端，生成专用于领域的表示，然后将其用作直接对手来学习领域不变的数据表示。我们通过实验证明，FACT在三个流行的多源单目标基准上优于最先进的联邦、非联邦和无源领域适应模型，并且优于最先进的无监督领域适应模型。",
    "tldr": "FACT是一种联邦敌对交叉训练算法，利用源客户端之间的隐式领域差异来识别目标领域的领域转移。实验证明，FACT在多源单目标基准上优于其他模型，并且优于无监督领域适应模型。"
}