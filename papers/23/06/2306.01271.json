{
    "title": "Why Clean Generalization and Robust Overfitting Both Happen in Adversarial Training. (arXiv:2306.01271v1 [cs.LG])",
    "abstract": "Adversarial training is a standard method to train deep neural networks to be robust to adversarial perturbation. Similar to surprising $\\textit{clean generalization}$ ability in the standard deep learning setting, neural networks trained by adversarial training also generalize well for $\\textit{unseen clean data}$. However, in constrast with clean generalization, while adversarial training method is able to achieve low $\\textit{robust training error}$, there still exists a significant $\\textit{robust generalization gap}$, which promotes us exploring what mechanism leads to both $\\textit{clean generalization and robust overfitting (CGRO)}$ during learning process. In this paper, we provide a theoretical understanding of this CGRO phenomenon in adversarial training. First, we propose a theoretical framework of adversarial training, where we analyze $\\textit{feature learning process}$ to explain how adversarial training leads network learner to CGRO regime. Specifically, we prove that, u",
    "link": "http://arxiv.org/abs/2306.01271",
    "context": "Title: Why Clean Generalization and Robust Overfitting Both Happen in Adversarial Training. (arXiv:2306.01271v1 [cs.LG])\nAbstract: Adversarial training is a standard method to train deep neural networks to be robust to adversarial perturbation. Similar to surprising $\\textit{clean generalization}$ ability in the standard deep learning setting, neural networks trained by adversarial training also generalize well for $\\textit{unseen clean data}$. However, in constrast with clean generalization, while adversarial training method is able to achieve low $\\textit{robust training error}$, there still exists a significant $\\textit{robust generalization gap}$, which promotes us exploring what mechanism leads to both $\\textit{clean generalization and robust overfitting (CGRO)}$ during learning process. In this paper, we provide a theoretical understanding of this CGRO phenomenon in adversarial training. First, we propose a theoretical framework of adversarial training, where we analyze $\\textit{feature learning process}$ to explain how adversarial training leads network learner to CGRO regime. Specifically, we prove that, u",
    "path": "papers/23/06/2306.01271.json",
    "total_tokens": 989,
    "translated_title": "为什么在对抗训练中会同时出现干净泛化和强健过拟合现象？",
    "translated_abstract": "对抗训练是训练深度神经网络抗击对抗扰动的标准方法。与在标准深度学习环境中出现惊人的干净泛化能力类似，通过对抗训练训练的神经网络也能很好地泛化到未见过的干净数据。然而，与干净泛化不同的是，尽管对抗训练能够实现低鲁棒训练误差，仍存在显著的鲁棒泛化距离，这促使我们探索在学习过程中导致干净泛化和强健过拟合现象同时发生的机制。本文提供了对抗训练中这种现象的理论理解。首先，我们提出了对抗训练的理论框架，分析了特征学习过程，解释了对抗训练如何导致网络学习者进入到干净泛化和强健过拟合状态。具体来说，我们证明了，通过迫使学习器成为强预测网络，对抗训练将导致干净泛化和鲁棒过拟合现象同时发生。",
    "tldr": "对抗训练是训练深度神经网络抗击对抗扰动的标准方法, 其学习机制导致干净泛化和强健过拟合现象同时发生。",
    "en_tdlr": "Adversarial training is a standard method to train deep neural networks to be robust to adversarial perturbation, and its learning mechanism leads to the simultaneous occurrence of clean generalization and robust overfitting."
}