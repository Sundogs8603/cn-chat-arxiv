{
    "title": "Subgraph Stationary Hardware-Software Inference Co-Design. (arXiv:2306.17266v1 [cs.DC])",
    "abstract": "A growing number of applications depend on Machine Learning (ML) functionality and benefits from both higher quality ML predictions and better timeliness (latency) at the same time. A growing body of research in computer architecture, ML, and systems software literature focuses on reaching better latency-accuracy tradeoffs for ML models. Efforts include compression, quantization, pruning, early-exit models, mixed DNN precision, as well as ML inference accelerator designs that minimize latency and energy, while preserving delivered accuracy. All of them, however, yield improvements for a single static point in the latency-accuracy tradeoff space. We make a case for applications that operate in dynamically changing deployment scenarios, where no single static point is optimal. We draw on a recently proposed weight-shared SuperNet mechanism to enable serving a stream of queries that uses (activates) different SubNets within this weight-shared construct. This creates an opportunity to expl",
    "link": "http://arxiv.org/abs/2306.17266",
    "context": "Title: Subgraph Stationary Hardware-Software Inference Co-Design. (arXiv:2306.17266v1 [cs.DC])\nAbstract: A growing number of applications depend on Machine Learning (ML) functionality and benefits from both higher quality ML predictions and better timeliness (latency) at the same time. A growing body of research in computer architecture, ML, and systems software literature focuses on reaching better latency-accuracy tradeoffs for ML models. Efforts include compression, quantization, pruning, early-exit models, mixed DNN precision, as well as ML inference accelerator designs that minimize latency and energy, while preserving delivered accuracy. All of them, however, yield improvements for a single static point in the latency-accuracy tradeoff space. We make a case for applications that operate in dynamically changing deployment scenarios, where no single static point is optimal. We draw on a recently proposed weight-shared SuperNet mechanism to enable serving a stream of queries that uses (activates) different SubNets within this weight-shared construct. This creates an opportunity to expl",
    "path": "papers/23/06/2306.17266.json",
    "total_tokens": 932,
    "translated_title": "子图静态硬件软件推理联合设计",
    "translated_abstract": "越来越多的应用程序依赖于机器学习（ML）功能，同时也从更高质量的ML预测和更好的及时性（延迟）受益。计算机体系结构、ML和系统软件领域的研究日益增多，重点是在ML模型的延迟-准确性权衡方面取得更好的结果。努力包括压缩、量化、修剪、提前退出模型、混合DNN精度以及ML推理加速器设计，以最小化延迟和能量，同时保持传递的准确性。然而，所有这些方法都只对延迟-准确性权衡空间中的一个静态点产生改进。我们提出了一个案例，针对在动态变化的部署场景中运行的应用程序，这里没有一个单一的静态点是最优的。我们利用最近提出的权重共享的SuperNet机制，使得能够服务于使用该权重共享结构中的不同子网的查询流。这为我们提供了探索基于子图的静态硬件软件推理联合设计的机会。",
    "tldr": "本文提出了一个子图静态硬件软件推理联合设计的案例，针对在动态变化的部署场景中运行的应用程序，通过利用权重共享的SuperNet机制，能够在延迟-准确性权衡中展现出更好的表现。",
    "en_tdlr": "This paper presents a case for subgraph stationary hardware-software inference co-design for applications that operate in dynamically changing deployment scenarios. By utilizing the weight-shared SuperNet mechanism, it enables better performance in the latency-accuracy tradeoff."
}