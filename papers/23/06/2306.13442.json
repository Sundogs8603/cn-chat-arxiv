{
    "title": "Minibatch training of neural network ensembles via trajectory sampling. (arXiv:2306.13442v1 [cond-mat.stat-mech])",
    "abstract": "Most iterative neural network training methods use estimates of the loss function over small random subsets (or minibatches) of the data to update the parameters, which aid in decoupling the training time from the (often very large) size of the training datasets. Here, we show that a minibatch approach can also be used to train neural network ensembles (NNEs) via trajectory methods in a highly efficent manner. We illustrate this approach by training NNEs to classify images in the MNIST datasets. This method gives an improvement to the training times, allowing it to scale as the ratio of the size of the dataset to that of the average minibatch size which, in the case of MNIST, gives a computational improvement typically of two orders of magnitude. We highlight the advantage of using longer trajectories to represent NNEs, both for improved accuracy in inference and reduced update cost in terms of the samples needed in minibatch updates.",
    "link": "http://arxiv.org/abs/2306.13442",
    "context": "Title: Minibatch training of neural network ensembles via trajectory sampling. (arXiv:2306.13442v1 [cond-mat.stat-mech])\nAbstract: Most iterative neural network training methods use estimates of the loss function over small random subsets (or minibatches) of the data to update the parameters, which aid in decoupling the training time from the (often very large) size of the training datasets. Here, we show that a minibatch approach can also be used to train neural network ensembles (NNEs) via trajectory methods in a highly efficent manner. We illustrate this approach by training NNEs to classify images in the MNIST datasets. This method gives an improvement to the training times, allowing it to scale as the ratio of the size of the dataset to that of the average minibatch size which, in the case of MNIST, gives a computational improvement typically of two orders of magnitude. We highlight the advantage of using longer trajectories to represent NNEs, both for improved accuracy in inference and reduced update cost in terms of the samples needed in minibatch updates.",
    "path": "papers/23/06/2306.13442.json",
    "total_tokens": 880,
    "translated_title": "轨迹采样下的神经网络集合小批量训练",
    "translated_abstract": "大多数迭代神经网络训练方法使用数据的小随机子集（或小批量）的损失函数估计来更新参数，在训练时间与庞大的训练数据集大小之间解耦，提高了效率。我们展示了一种小批量方法可以以高效的方式通过轨迹方法训练神经网络集合(NNEs)。我们通过训练NNE来分类MNIST数据集中的图像来说明这种方法。这种方法可以提高训练时间，使其能够缩放为数据集大小与平均小批量大小之比，对于MNIST来说，计算效率通常提高两个数量级。我们强调使用较长的轨迹来表示NNE的优点，既可以提高推断的准确性，又可以在小批量更新所需的样本方面降低更新成本。",
    "tldr": "本文介绍了一种轨迹采样下的神经网络集合小批量训练方法，通过对MNIST数据集实验，发现相较于传统方法，该方法提高了两个数量级的计算效率，同时还提高了推断准确性。",
    "en_tdlr": "This paper proposes a minibatch approach for training neural network ensembles (NNEs) via trajectory methods, which can improve computational efficiency by two orders of magnitude compared to traditional methods and achieve higher accuracy in inference by using longer trajectories to represent NNEs. The approach is demonstrated by training NNEs to classify images in the MNIST datasets."
}