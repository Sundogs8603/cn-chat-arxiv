{
    "title": "Langevin Thompson Sampling with Logarithmic Communication: Bandits and Reinforcement Learning. (arXiv:2306.08803v1 [cs.LG])",
    "abstract": "Thompson sampling (TS) is widely used in sequential decision making due to its ease of use and appealing empirical performance. However, many existing analytical and empirical results for TS rely on restrictive assumptions on reward distributions, such as belonging to conjugate families, which limits their applicability in realistic scenarios. Moreover, sequential decision making problems are often carried out in a batched manner, either due to the inherent nature of the problem or to serve the purpose of reducing communication and computation costs. In this work, we jointly study these problems in two popular settings, namely, stochastic multi-armed bandits (MABs) and infinite-horizon reinforcement learning (RL), where TS is used to learn the unknown reward distributions and transition dynamics, respectively. We propose batched $\\textit{Langevin Thompson Sampling}$ algorithms that leverage MCMC methods to sample from approximate posteriors with only logarithmic communication costs in ",
    "link": "http://arxiv.org/abs/2306.08803",
    "context": "Title: Langevin Thompson Sampling with Logarithmic Communication: Bandits and Reinforcement Learning. (arXiv:2306.08803v1 [cs.LG])\nAbstract: Thompson sampling (TS) is widely used in sequential decision making due to its ease of use and appealing empirical performance. However, many existing analytical and empirical results for TS rely on restrictive assumptions on reward distributions, such as belonging to conjugate families, which limits their applicability in realistic scenarios. Moreover, sequential decision making problems are often carried out in a batched manner, either due to the inherent nature of the problem or to serve the purpose of reducing communication and computation costs. In this work, we jointly study these problems in two popular settings, namely, stochastic multi-armed bandits (MABs) and infinite-horizon reinforcement learning (RL), where TS is used to learn the unknown reward distributions and transition dynamics, respectively. We propose batched $\\textit{Langevin Thompson Sampling}$ algorithms that leverage MCMC methods to sample from approximate posteriors with only logarithmic communication costs in ",
    "path": "papers/23/06/2306.08803.json",
    "total_tokens": 869,
    "translated_title": "带有对数通信的 Langevin Thompson Sampling：在赌博机和强化学习中的应用",
    "translated_abstract": "Thompson Sampling (TS)由于易于使用和良好的实验表现而被广泛应用于序贯决策。但许多TS的现有分析和实证结果都依赖于奖励分布的限制性假设，如属于共轭家族，这限制了它们在现实场景中的适用性。此外，序列决策问题往往以批处理方式进行，这既是由于问题本身的固有性质，也是为了减少通信和计算成本。在本文中，我们联合研究了两种常见的设置中的这些问题，即基于随机赌博机（MABs）和基于无限时域强化学习（RL），其中TS用于学习未知的奖励分布和转移动态。我们提出了批量化的 $\\textit{Langevin Thompson Sampling}$算法，利用MCMC方法从近似后验中采样，其通信成本仅为对数。",
    "tldr": "本文提出了一种带有对数通信的 Langevin Thompson Sampling 算法，可用于解决在赌博机和强化学习中学习未知的奖励分布和转移动态等问题。",
    "en_tdlr": "The paper proposes a batched Langevin Thompson Sampling algorithm with logarithmic communication costs for learning unknown reward distributions and transition dynamics in stochastic multi-armed bandits and infinite-horizon reinforcement learning."
}