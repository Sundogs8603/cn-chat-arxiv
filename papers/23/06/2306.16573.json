{
    "title": "Finite-Sample Symmetric Mean Estimation with Fisher Information Rate. (arXiv:2306.16573v1 [math.ST])",
    "abstract": "The mean of an unknown variance-$\\sigma^2$ distribution $f$ can be estimated from $n$ samples with variance $\\frac{\\sigma^2}{n}$ and nearly corresponding subgaussian rate. When $f$ is known up to translation, this can be improved asymptotically to $\\frac{1}{n\\mathcal I}$, where $\\mathcal I$ is the Fisher information of the distribution. Such an improvement is not possible for general unknown $f$, but [Stone, 1975] showed that this asymptotic convergence $\\textit{is}$ possible if $f$ is $\\textit{symmetric}$ about its mean. Stone's bound is asymptotic, however: the $n$ required for convergence depends in an unspecified way on the distribution $f$ and failure probability $\\delta$. In this paper we give finite-sample guarantees for symmetric mean estimation in terms of Fisher information. For every $f, n, \\delta$ with $n > \\log \\frac{1}{\\delta}$, we get convergence close to a subgaussian with variance $\\frac{1}{n \\mathcal I_r}$, where $\\mathcal I_r$ is the $r$-$\\textit{smoothed}$ Fisher in",
    "link": "http://arxiv.org/abs/2306.16573",
    "context": "Title: Finite-Sample Symmetric Mean Estimation with Fisher Information Rate. (arXiv:2306.16573v1 [math.ST])\nAbstract: The mean of an unknown variance-$\\sigma^2$ distribution $f$ can be estimated from $n$ samples with variance $\\frac{\\sigma^2}{n}$ and nearly corresponding subgaussian rate. When $f$ is known up to translation, this can be improved asymptotically to $\\frac{1}{n\\mathcal I}$, where $\\mathcal I$ is the Fisher information of the distribution. Such an improvement is not possible for general unknown $f$, but [Stone, 1975] showed that this asymptotic convergence $\\textit{is}$ possible if $f$ is $\\textit{symmetric}$ about its mean. Stone's bound is asymptotic, however: the $n$ required for convergence depends in an unspecified way on the distribution $f$ and failure probability $\\delta$. In this paper we give finite-sample guarantees for symmetric mean estimation in terms of Fisher information. For every $f, n, \\delta$ with $n > \\log \\frac{1}{\\delta}$, we get convergence close to a subgaussian with variance $\\frac{1}{n \\mathcal I_r}$, where $\\mathcal I_r$ is the $r$-$\\textit{smoothed}$ Fisher in",
    "path": "papers/23/06/2306.16573.json",
    "total_tokens": 953,
    "translated_title": "有限样本下具有费舍尔信息速率的对称均值估计",
    "translated_abstract": "对于一个未知方差为$\\sigma^2$的分布$f$，可以通过$n$个样本以方差$\\frac{\\sigma^2}{n}$和几乎相对应的次高斯速率来估计均值。当$f$已知且对称时，可以在渐近条件下将其改进为$\\frac{1}{n\\mathcal I}$，其中$\\mathcal I$为该分布的费舍尔信息。然而，对于一般的未知分布$f$，这样的改进是不可能的。但是，Stone(1975)证明了当$f$关于其均值对称时，这种渐近收敛是可能的。然而，Stone的界限是渐近的，即收敛所需的$n$以未指定的方式取决于分布$f$和失败概率$\\delta$。在本文中，我们就对称均值估计的费舍尔信息给出有限样本的保证。对于每个$f,n,\\delta$满足$n > \\log \\frac{1}{\\delta}$，我们可以得到收敛到方差为$\\frac{1}{n \\mathcal I_r}$的次高斯附近的收敛，其中$\\mathcal I_r$是$r$-$\\textit{平滑化}$费舍尔信息。",
    "tldr": "本文研究了有限样本下对称均值估计的问题，并给出了基于费舍尔信息的保证。对于对称分布，可以获得收敛到次高斯的收敛速度，而不需要渐近条件。"
}