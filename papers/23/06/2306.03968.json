{
    "title": "Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels. (arXiv:2306.03968v1 [stat.ML])",
    "abstract": "Selecting hyperparameters in deep learning greatly impacts its effectiveness but requires manual effort and expertise. Recent works show that Bayesian model selection with Laplace approximations can allow to optimize such hyperparameters just like standard neural network parameters using gradients and on the training data. However, estimating a single hyperparameter gradient requires a pass through the entire dataset, limiting the scalability of such algorithms. In this work, we overcome this issue by introducing lower bounds to the linearized Laplace approximation of the marginal likelihood. In contrast to previous estimators, these bounds are amenable to stochastic-gradient-based optimization and allow to trade off estimation accuracy against computational complexity. We derive them using the function-space form of the linearized Laplace, which can be estimated using the neural tangent kernel. Experimentally, we show that the estimators can significantly accelerate gradient-based hyp",
    "link": "http://arxiv.org/abs/2306.03968",
    "context": "Title: Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels. (arXiv:2306.03968v1 [stat.ML])\nAbstract: Selecting hyperparameters in deep learning greatly impacts its effectiveness but requires manual effort and expertise. Recent works show that Bayesian model selection with Laplace approximations can allow to optimize such hyperparameters just like standard neural network parameters using gradients and on the training data. However, estimating a single hyperparameter gradient requires a pass through the entire dataset, limiting the scalability of such algorithms. In this work, we overcome this issue by introducing lower bounds to the linearized Laplace approximation of the marginal likelihood. In contrast to previous estimators, these bounds are amenable to stochastic-gradient-based optimization and allow to trade off estimation accuracy against computational complexity. We derive them using the function-space form of the linearized Laplace, which can be estimated using the neural tangent kernel. Experimentally, we show that the estimators can significantly accelerate gradient-based hyp",
    "path": "papers/23/06/2306.03968.json",
    "total_tokens": 841,
    "translated_title": "使用神经切向核的随机边际似然梯度。",
    "translated_abstract": "选择深度学习中的超参数对其有效性有重大影响，但需要人工努力和专业知识。最近的研究表明，使用拉普拉斯近似的贝叶斯模型选择能够像使用梯度优化标准神经网络参数一样优化这些超参数，并使用训练数据进行训练。然而，估计单个超参数梯度需要通过整个数据集，限制了这些算法的可伸缩性。在这项研究中，我们通过引入线性化拉普拉斯逼近的下限来克服这个问题。与以前的估计器不同，这些下限适用于基于随机梯度的优化，并允许在估计精度和计算复杂性之间进行权衡。我们使用线性化拉普拉斯的函数空间形式导出了它们，这可以使用神经切向核进行估计。在实验中，我们展示了这些估计器可以显著加速基于梯度的超参数优化过程。",
    "tldr": "本文提出了使用神经切向核的随机边际似然梯度，可以加速基于梯度的超参数优化过程。",
    "en_tdlr": "This paper proposes the use of stochastic marginal likelihood gradients with neural tangent kernels, which significantly accelerates gradient-based hyperparameter optimization."
}