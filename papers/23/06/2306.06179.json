{
    "title": "Hidden symmetries of ReLU networks. (arXiv:2306.06179v1 [cs.LG])",
    "abstract": "The parameter space for any fixed architecture of feedforward ReLU neural networks serves as a proxy during training for the associated class of functions - but how faithful is this representation? It is known that many different parameter settings can determine the same function. Moreover, the degree of this redundancy is inhomogeneous: for some networks, the only symmetries are permutation of neurons in a layer and positive scaling of parameters at a neuron, while other networks admit additional hidden symmetries. In this work, we prove that, for any network architecture where no layer is narrower than the input, there exist parameter settings with no hidden symmetries. We also describe a number of mechanisms through which hidden symmetries can arise, and empirically approximate the functional dimension of different network architectures at initialization. These experiments indicate that the probability that a network has no hidden symmetries decreases towards 0 as depth increases, w",
    "link": "http://arxiv.org/abs/2306.06179",
    "context": "Title: Hidden symmetries of ReLU networks. (arXiv:2306.06179v1 [cs.LG])\nAbstract: The parameter space for any fixed architecture of feedforward ReLU neural networks serves as a proxy during training for the associated class of functions - but how faithful is this representation? It is known that many different parameter settings can determine the same function. Moreover, the degree of this redundancy is inhomogeneous: for some networks, the only symmetries are permutation of neurons in a layer and positive scaling of parameters at a neuron, while other networks admit additional hidden symmetries. In this work, we prove that, for any network architecture where no layer is narrower than the input, there exist parameter settings with no hidden symmetries. We also describe a number of mechanisms through which hidden symmetries can arise, and empirically approximate the functional dimension of different network architectures at initialization. These experiments indicate that the probability that a network has no hidden symmetries decreases towards 0 as depth increases, w",
    "path": "papers/23/06/2306.06179.json",
    "total_tokens": 862,
    "translated_title": "ReLU网络的隐藏对称性",
    "translated_abstract": "对于前馈ReLU神经网络的任何固定架构，参数空间在训练期间用作相关函数类的代理 - 但是这种表示有多真实呢？已知许多不同的参数设置可以确定相同的函数。此外，这种冗余的程度是不均匀的：对于某些网络，唯一的对称性是层中神经元的排列和神经元参数的正比例缩放，而其他网络则具有其他隐藏对称性。 在这项工作中，我们证明了对于没有比输入层更窄的层的网络架构，存在没有隐藏对称性的参数设置。 我们还描述了一些机制，通过这些机制隐藏的对称性会出现，并在初始化时经验地近似不同网络架构的功能维度。这些实验表明，随着深度的增加，网络没有隐藏对称性的概率逐渐降低到0。",
    "tldr": "研究发现，对于前馈ReLU神经网络的任何固定架构，在没有比输入层更窄的层的情况下，存在没有隐藏对称性的参数设置。此外，随着深度的增加，网络没有隐藏对称性的概率逐渐降低到0。",
    "en_tdlr": "This paper shows that for any fixed architecture of feedforward ReLU neural networks where no layer is narrower than the input, there exist parameter settings with no hidden symmetries. The study also suggests that the probability of a network having no hidden symmetries decreases towards 0 as depth increases."
}