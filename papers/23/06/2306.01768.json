{
    "title": "A Quantitative Review on Language Model Efficiency Research. (arXiv:2306.01768v1 [cs.LG])",
    "abstract": "Language models (LMs) are being scaled and becoming powerful. Improving their efficiency is one of the core research topics in neural information processing systems. Tay et al. (2022) provided a comprehensive overview of efficient Transformers that have become an indispensable staple in the field of NLP. However, in the section of \"On Evaluation\", they left an open question \"which fundamental efficient Transformer one should consider,\" answered by \"still a mystery\" because \"many research papers select their own benchmarks.\" Unfortunately, there was not quantitative analysis about the performances of Transformers on any benchmarks. Moreover, state space models (SSMs) have demonstrated their abilities of modeling long-range sequences with non-attention mechanisms, which were not discussed in the prior review. This article makes a meta analysis on the results from a set of papers on efficient Transformers as well as those on SSMs. It provides a quantitative review on LM efficiency researc",
    "link": "http://arxiv.org/abs/2306.01768",
    "context": "Title: A Quantitative Review on Language Model Efficiency Research. (arXiv:2306.01768v1 [cs.LG])\nAbstract: Language models (LMs) are being scaled and becoming powerful. Improving their efficiency is one of the core research topics in neural information processing systems. Tay et al. (2022) provided a comprehensive overview of efficient Transformers that have become an indispensable staple in the field of NLP. However, in the section of \"On Evaluation\", they left an open question \"which fundamental efficient Transformer one should consider,\" answered by \"still a mystery\" because \"many research papers select their own benchmarks.\" Unfortunately, there was not quantitative analysis about the performances of Transformers on any benchmarks. Moreover, state space models (SSMs) have demonstrated their abilities of modeling long-range sequences with non-attention mechanisms, which were not discussed in the prior review. This article makes a meta analysis on the results from a set of papers on efficient Transformers as well as those on SSMs. It provides a quantitative review on LM efficiency researc",
    "path": "papers/23/06/2306.01768.json",
    "total_tokens": 644,
    "translated_title": "语言模型效率研究的定量评估",
    "translated_abstract": "语言模型是被扩展用于处理一些复杂、自然语言处理任务的有效工具之一。本篇论文涵盖了对于提高语言模型效率这一核心问题的研究概述，并尝试对最近的基于Transformers和State Space Models的研究进行定量分析。",
    "tldr": "这篇论文从定量方面对基于Transformers和State Space Models的一系列研究进行了综述，并总结了如何提高语言模型效率。"
}