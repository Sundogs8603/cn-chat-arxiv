{
    "title": "Shedding light on underrepresentation and Sampling Bias in machine learning. (arXiv:2306.05068v1 [cs.LG])",
    "abstract": "Accurately measuring discrimination is crucial to faithfully assessing fairness of trained machine learning (ML) models. Any bias in measuring discrimination leads to either amplification or underestimation of the existing disparity. Several sources of bias exist and it is assumed that bias resulting from machine learning is born equally by different groups (e.g. females vs males, whites vs blacks, etc.). If, however, bias is born differently by different groups, it may exacerbate discrimination against specific sub-populations. Sampling bias, is inconsistently used in the literature to describe bias due to the sampling procedure. In this paper, we attempt to disambiguate this term by introducing clearly defined variants of sampling bias, namely, sample size bias (SSB) and underrepresentation bias (URB). We show also how discrimination can be decomposed into variance, bias, and noise. Finally, we challenge the commonly accepted mitigation approach that discrimination can be addressed b",
    "link": "http://arxiv.org/abs/2306.05068",
    "context": "Title: Shedding light on underrepresentation and Sampling Bias in machine learning. (arXiv:2306.05068v1 [cs.LG])\nAbstract: Accurately measuring discrimination is crucial to faithfully assessing fairness of trained machine learning (ML) models. Any bias in measuring discrimination leads to either amplification or underestimation of the existing disparity. Several sources of bias exist and it is assumed that bias resulting from machine learning is born equally by different groups (e.g. females vs males, whites vs blacks, etc.). If, however, bias is born differently by different groups, it may exacerbate discrimination against specific sub-populations. Sampling bias, is inconsistently used in the literature to describe bias due to the sampling procedure. In this paper, we attempt to disambiguate this term by introducing clearly defined variants of sampling bias, namely, sample size bias (SSB) and underrepresentation bias (URB). We show also how discrimination can be decomposed into variance, bias, and noise. Finally, we challenge the commonly accepted mitigation approach that discrimination can be addressed b",
    "path": "papers/23/06/2306.05068.json",
    "total_tokens": 989,
    "translated_title": "揭示机器学习中的代表性不足和抽样偏差问题",
    "translated_abstract": "准确地衡量歧视对于忠实地评估训练好的机器学习（ML）模型的公正性至关重要。存在任何测量歧视的偏见都会导致现有差距的放大或低估。存在几种偏见来源，假设机器学习导致的偏见在不同的群体（例如女性与男性、白人与黑人等）之间平等分配。然而，如果偏见在不同的群体中分别存在，可能会加剧对特定亚群体的歧视。抽样偏差是文献中描述由抽样程序引起的偏差的术语。在本文中，我们试图通过引入明确定义的抽样偏差变体，即样本量偏差和代表性不足偏差，来消除这个术语的歧义。我们还展示了如何将歧视分解为方差、偏差和噪声。最后，我们挑战了通常被接受的缓解方法，即通过“简单地”从训练数据中删除敏感属性（例如种族或性别）来解决歧视问题。",
    "tldr": "文章讨论了机器学习中的偏见对公正性的影响，提出抽样偏差的变体：样本量偏差和代表性不足偏差，揭示歧视可以分解为方差、偏差和噪声，并挑战了通常被接受的缓解方法。",
    "en_tdlr": "The paper discusses the impact of bias on fairness in machine learning, proposes variants of sampling bias: sample size bias and underrepresentation bias, reveals that discrimination can be decomposed into variance, bias, and noise, and challenges the commonly accepted mitigation approach."
}