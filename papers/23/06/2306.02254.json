{
    "title": "A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean Language Models. (arXiv:2306.02254v2 [cs.CL] UPDATED)",
    "abstract": "Polyglot is a pioneering project aimed at enhancing the non-English language performance of multilingual language models. Despite the availability of various multilingual models such as mBERT (Devlin et al., 2019), XGLM (Lin et al., 2022), and BLOOM (Scao et al., 2022), researchers and developers often resort to building monolingual models in their respective languages due to the dissatisfaction with the current multilingual models non-English language capabilities. Addressing this gap, we seek to develop advanced multilingual language models that offer improved performance in non-English languages. In this paper, we introduce the Polyglot Korean models, which represent a specific focus rather than being multilingual in nature. In collaboration with TUNiB, our team collected 1.2TB of Korean data meticulously curated for our research journey. We made a deliberate decision to prioritize the development of Korean models before venturing into multilingual models. This choice was motivated ",
    "link": "http://arxiv.org/abs/2306.02254",
    "context": "Title: A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean Language Models. (arXiv:2306.02254v2 [cs.CL] UPDATED)\nAbstract: Polyglot is a pioneering project aimed at enhancing the non-English language performance of multilingual language models. Despite the availability of various multilingual models such as mBERT (Devlin et al., 2019), XGLM (Lin et al., 2022), and BLOOM (Scao et al., 2022), researchers and developers often resort to building monolingual models in their respective languages due to the dissatisfaction with the current multilingual models non-English language capabilities. Addressing this gap, we seek to develop advanced multilingual language models that offer improved performance in non-English languages. In this paper, we introduce the Polyglot Korean models, which represent a specific focus rather than being multilingual in nature. In collaboration with TUNiB, our team collected 1.2TB of Korean data meticulously curated for our research journey. We made a deliberate decision to prioritize the development of Korean models before venturing into multilingual models. This choice was motivated ",
    "path": "papers/23/06/2306.02254.json",
    "total_tokens": 862,
    "translated_title": "Polyglot-Ko: 开源大规模韩语语言模型的技术报告",
    "translated_abstract": "Polyglot是一个旨在增强多语言语言模型的非英语语言性能的开创性项目。本文介绍了Polyglot Korean模型，它是专注于韩语而不是多语言性质的。我们与TUNiB合作，收集了1.2TB的精心筛选的韩语数据，重点开发了韩语模型。 Polyglot-Ko模型包括多种预训练模型，具有不同的大小和特性以适应各种下游任务。这些模型在多个韩语NLP任务上表现比mBERT和XGLM更好，进一步支持具有专门针对某一语言的模型的需求和潜力。",
    "tldr": "Polyglot-Ko是一种韩语语言模型，提供了比mBERT和XGLM更好的性能，有助于改善非英语语言的表现。",
    "en_tdlr": "Polyglot-Ko is a Korean language model that offers better performance than mBERT and XGLM, helping to improve non-English language capabilities."
}