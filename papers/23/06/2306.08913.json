{
    "title": "Advancing Volumetric Medical Image Segmentation via Global-Local Masked Autoencoder. (arXiv:2306.08913v2 [cs.CV] UPDATED)",
    "abstract": "Masked autoencoder (MAE) is a promising self-supervised pre-training technique that can improve the representation learning of a neural network without human intervention. However, applying MAE directly to volumetric medical images poses two challenges: (i) a lack of global information that is crucial for understanding the clinical context of the holistic data, (ii) no guarantee of stabilizing the representations learned from randomly masked inputs. To address these limitations, we propose the \\textbf{G}lobal-\\textbf{L}ocal \\textbf{M}asked \\textbf{A}uto\\textbf{E}ncoder (GL-MAE), a simple yet effective self-supervised pre-training strategy. In addition to reconstructing masked local views, as in previous methods, GL-MAE incorporates global context learning by reconstructing masked global views. Furthermore, a complete global view is integrated as an anchor to guide the reconstruction and stabilize the learning process through global-to-global consistency learning and global-to-local con",
    "link": "http://arxiv.org/abs/2306.08913",
    "context": "Title: Advancing Volumetric Medical Image Segmentation via Global-Local Masked Autoencoder. (arXiv:2306.08913v2 [cs.CV] UPDATED)\nAbstract: Masked autoencoder (MAE) is a promising self-supervised pre-training technique that can improve the representation learning of a neural network without human intervention. However, applying MAE directly to volumetric medical images poses two challenges: (i) a lack of global information that is crucial for understanding the clinical context of the holistic data, (ii) no guarantee of stabilizing the representations learned from randomly masked inputs. To address these limitations, we propose the \\textbf{G}lobal-\\textbf{L}ocal \\textbf{M}asked \\textbf{A}uto\\textbf{E}ncoder (GL-MAE), a simple yet effective self-supervised pre-training strategy. In addition to reconstructing masked local views, as in previous methods, GL-MAE incorporates global context learning by reconstructing masked global views. Furthermore, a complete global view is integrated as an anchor to guide the reconstruction and stabilize the learning process through global-to-global consistency learning and global-to-local con",
    "path": "papers/23/06/2306.08913.json",
    "total_tokens": 907,
    "translated_title": "通过全局局部掩码自编码器推进体积医学图像分割",
    "translated_abstract": "局部掩码自编码器（MAE）是一种有前景的自监督预训练技术，可以在没有人工干预的情况下改善神经网络的表示学习。然而，将MAE直接应用于体积医学图像面临两个挑战：（一）缺乏对整体数据临床背景理解至关重要的全局信息，（二）无法保证从随机掩码输入学习到的表示稳定。为解决这些限制，我们提出了全局-局部掩码自编码器（GL-MAE），这是一种简单而有效的自监督预训练策略。除了重构局部掩码视图，GL-MAE还通过重构全局掩码视图来融入全局上下文学习。此外，完整的全局视图被作为锚点来指导重构并通过全局一致性学习和全局-局部一致性学习稳定学习过程。",
    "tldr": "本研究提出了一种全局-局部掩码自编码器（GL-MAE）的自监督预训练策略，通过重构全局和局部掩码视图来改善体积医学图像分割任务。",
    "en_tdlr": "This research proposes a self-supervised pre-training strategy called Global-Local Masked Autoencoder (GL-MAE), which improves volumetric medical image segmentation by reconstructing both global and local masked views."
}