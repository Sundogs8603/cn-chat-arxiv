{
    "title": "Language to Rewards for Robotic Skill Synthesis. (arXiv:2306.08647v2 [cs.RO] UPDATED)",
    "abstract": "Large language models (LLMs) have demonstrated exciting progress in acquiring diverse new capabilities through in-context learning, ranging from logical reasoning to code-writing. Robotics researchers have also explored using LLMs to advance the capabilities of robotic control. However, since low-level robot actions are hardware-dependent and underrepresented in LLM training corpora, existing efforts in applying LLMs to robotics have largely treated LLMs as semantic planners or relied on human-engineered control primitives to interface with the robot. On the other hand, reward functions are shown to be flexible representations that can be optimized for control policies to achieve diverse tasks, while their semantic richness makes them suitable to be specified by LLMs. In this work, we introduce a new paradigm that harnesses this realization by utilizing LLMs to define reward parameters that can be optimized and accomplish variety of robotic tasks. Using reward as the intermediate inter",
    "link": "http://arxiv.org/abs/2306.08647",
    "context": "Title: Language to Rewards for Robotic Skill Synthesis. (arXiv:2306.08647v2 [cs.RO] UPDATED)\nAbstract: Large language models (LLMs) have demonstrated exciting progress in acquiring diverse new capabilities through in-context learning, ranging from logical reasoning to code-writing. Robotics researchers have also explored using LLMs to advance the capabilities of robotic control. However, since low-level robot actions are hardware-dependent and underrepresented in LLM training corpora, existing efforts in applying LLMs to robotics have largely treated LLMs as semantic planners or relied on human-engineered control primitives to interface with the robot. On the other hand, reward functions are shown to be flexible representations that can be optimized for control policies to achieve diverse tasks, while their semantic richness makes them suitable to be specified by LLMs. In this work, we introduce a new paradigm that harnesses this realization by utilizing LLMs to define reward parameters that can be optimized and accomplish variety of robotic tasks. Using reward as the intermediate inter",
    "path": "papers/23/06/2306.08647.json",
    "total_tokens": 1039,
    "translated_title": "语言转奖励：用于机器人技能综合的方法",
    "translated_abstract": "大型语言模型已经取得了许多令人兴奋的进展，从逻辑推理到代码编写等，展现了在情境学习中获得多种新能力的潜力。机器人学研究人员也探索使用大型语言模型来提高机器人控制的能力。然而，由于低级机器人动作取决于硬件并且在大型语言模型的训练语料库中所占的比重较小，因此在机器人学中应用大型语言模型的现有努力主要将其视为语义规划器，或依赖于人工控制原语与机器人进行交互。另一方面，奖励函数被证明是可以灵活表示并且可以被优化以实现多种任务的控制策略，其语义丰富性使其适合由大型语言模型来指定。在本文中，我们引入了一种新的方法，通过利用大型语言模型来定义可以被优化的奖励参数并完成各种机器人任务。使用奖励作为中间介质，我们提出的方法使机器人能够执行各种由自然语言指令指定的任务，而无需人类在设计行为原语方面付出努力。我们在仿真环境中展示了我们的方法在拾取物品和搭建塔两个任务上的有效性。",
    "tldr": "该研究介绍了一种将大型语言模型用于定义奖励参数，并通过奖励函数进行优化，从而使机器人可以执行各种自然语言指令指定的任务的新方法。",
    "en_tdlr": "This study proposes a new approach that utilizes large language models to define reward parameters that can be optimized for control policies to achieve various robotic tasks specified by natural language instructions, which can enable robots to perform tasks without requiring human efforts in designing action primitives."
}