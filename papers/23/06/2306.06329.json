{
    "title": "HIPODE: Enhancing Offline Reinforcement Learning with High-Quality Synthetic Data from a Policy-Decoupled Approach. (arXiv:2306.06329v1 [cs.LG])",
    "abstract": "Offline reinforcement learning (ORL) has gained attention as a means of training reinforcement learning models using pre-collected static data. To address the issue of limited data and improve downstream ORL performance, recent work has attempted to expand the dataset's coverage through data augmentation. However, most of these methods are tied to a specific policy (policy-dependent), where the generated data can only guarantee to support the current downstream ORL policy, limiting its usage scope on other downstream policies. Moreover, the quality of synthetic data is often not well-controlled, which limits the potential for further improving the downstream policy. To tackle these issues, we propose \\textbf{HI}gh-quality \\textbf{PO}licy-\\textbf{DE}coupled~(HIPODE), a novel data augmentation method for ORL. On the one hand, HIPODE generates high-quality synthetic data by selecting states near the dataset distribution with potentially high value among candidate states using the negative",
    "link": "http://arxiv.org/abs/2306.06329",
    "context": "Title: HIPODE: Enhancing Offline Reinforcement Learning with High-Quality Synthetic Data from a Policy-Decoupled Approach. (arXiv:2306.06329v1 [cs.LG])\nAbstract: Offline reinforcement learning (ORL) has gained attention as a means of training reinforcement learning models using pre-collected static data. To address the issue of limited data and improve downstream ORL performance, recent work has attempted to expand the dataset's coverage through data augmentation. However, most of these methods are tied to a specific policy (policy-dependent), where the generated data can only guarantee to support the current downstream ORL policy, limiting its usage scope on other downstream policies. Moreover, the quality of synthetic data is often not well-controlled, which limits the potential for further improving the downstream policy. To tackle these issues, we propose \\textbf{HI}gh-quality \\textbf{PO}licy-\\textbf{DE}coupled~(HIPODE), a novel data augmentation method for ORL. On the one hand, HIPODE generates high-quality synthetic data by selecting states near the dataset distribution with potentially high value among candidate states using the negative",
    "path": "papers/23/06/2306.06329.json",
    "total_tokens": 924,
    "translated_title": "HIPODE: 从策略解耦角度增强离线强化学习中高质量合成数据的应用",
    "translated_abstract": "离线强化学习（ORL）作为利用预先收集的静态数据训练强化学习模型的方法，已引起人们的关注。为了解决有限数据的问题并改善下游ORL的性能，最近的研究试图通过数据增强来扩展数据集的覆盖范围。然而，大多数方法都与特定策略（策略相关）相关，生成的数据只能保证支持当前下游ORL策略，限制了它在其他下游策略上的使用范围。此外，合成数据的质量通常不能很好地控制，这限制了进一步改进下游策略的潜力。为了解决这些问题，我们提出了一种新的ORL数据增强方法，称为高质量策略解耦（HIPODE）。一方面，HIPODE通过选择潜在高价值状态的候选状态中接近数据集分布的状态来产生高质量的合成数据，使用负",
    "tldr": "HIPODE是一种新的离线强化学习数据增强方法，通过选择接近数据集分布中潜在高价值状态的候选状态来生成高质量的合成数据，增强了下游ORL性能和适用性。",
    "en_tdlr": "HIPODE is a novel data augmentation method for offline reinforcement learning (ORL) that enhances downstream ORL performance and applicability by generating high-quality synthetic data through selecting candidate states near the dataset distribution with potentially high value."
}