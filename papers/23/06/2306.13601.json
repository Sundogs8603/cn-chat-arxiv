{
    "title": "Active Coverage for PAC Reinforcement Learning. (arXiv:2306.13601v1 [cs.LG])",
    "abstract": "Collecting and leveraging data with good coverage properties plays a crucial role in different aspects of reinforcement learning (RL), including reward-free exploration and offline learning. However, the notion of \"good coverage\" really depends on the application at hand, as data suitable for one context may not be so for another. In this paper, we formalize the problem of active coverage in episodic Markov decision processes (MDPs), where the goal is to interact with the environment so as to fulfill given sampling requirements. This framework is sufficiently flexible to specify any desired coverage property, making it applicable to any problem that involves online exploration. Our main contribution is an instance-dependent lower bound on the sample complexity of active coverage and a simple game-theoretic algorithm, CovGame, that nearly matches it. We then show that CovGame can be used as a building block to solve different PAC RL tasks. In particular, we obtain a simple algorithm for",
    "link": "http://arxiv.org/abs/2306.13601",
    "context": "Title: Active Coverage for PAC Reinforcement Learning. (arXiv:2306.13601v1 [cs.LG])\nAbstract: Collecting and leveraging data with good coverage properties plays a crucial role in different aspects of reinforcement learning (RL), including reward-free exploration and offline learning. However, the notion of \"good coverage\" really depends on the application at hand, as data suitable for one context may not be so for another. In this paper, we formalize the problem of active coverage in episodic Markov decision processes (MDPs), where the goal is to interact with the environment so as to fulfill given sampling requirements. This framework is sufficiently flexible to specify any desired coverage property, making it applicable to any problem that involves online exploration. Our main contribution is an instance-dependent lower bound on the sample complexity of active coverage and a simple game-theoretic algorithm, CovGame, that nearly matches it. We then show that CovGame can be used as a building block to solve different PAC RL tasks. In particular, we obtain a simple algorithm for",
    "path": "papers/23/06/2306.13601.json",
    "total_tokens": 1138,
    "translated_title": "PAC强化学习的主动覆盖",
    "translated_abstract": "收集和利用具有良好覆盖特性的数据在强化学习（RL）的不同方面中起着关键作用，包括无奖励探索和离线学习。我们形式化了周期性马尔可夫决策过程（MDP）中的主动覆盖问题，其中的目标是与环境交互，以满足给定的采样要求。我们的主要贡献是对主动覆盖样本复杂度的一个与示例有关的下界和一个简单的博弈理论算法CovGame，该算法几乎与下界相匹配。然后，我们展示了CovGame可用作解决不同PAC RL任务的构建模块。",
    "tldr": "本文提出了PAC强化学习的主动覆盖问题，其中博弈理论算法CovGame能解决不同PAC RL任务。",
    "en_tdlr": "This paper proposes the problem of active coverage for PAC reinforcement learning and presents CovGame, a game-theoretic algorithm that can be used as a building block to solve various PAC RL tasks."
}