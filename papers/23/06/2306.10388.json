{
    "title": "Breaking On-device Training Memory Wall: A Systematic Survey. (arXiv:2306.10388v2 [cs.DC] UPDATED)",
    "abstract": "On-device training has become an increasingly popular approach to machine learning, enabling models to be trained directly on mobile and edge devices. However, a major challenge in this area is the limited memory available on these devices, which can severely restrict the size and complexity of the models that can be trained. In this systematic survey, we aim to explore the current state-of-the-art techniques for breaking on-device training memory walls, focusing on methods that can enable larger and more complex models to be trained on resource-constrained devices.  Specifically, we first analyze the key factors that contribute to the phenomenon of memory walls encountered during on-device training. Then, we present a comprehensive literature review of on-device training, which addresses the issue of memory limitations. Finally, we summarize on-device training and highlight the open problems for future research.  By providing a comprehensive overview of these techniques and their effe",
    "link": "http://arxiv.org/abs/2306.10388",
    "context": "Title: Breaking On-device Training Memory Wall: A Systematic Survey. (arXiv:2306.10388v2 [cs.DC] UPDATED)\nAbstract: On-device training has become an increasingly popular approach to machine learning, enabling models to be trained directly on mobile and edge devices. However, a major challenge in this area is the limited memory available on these devices, which can severely restrict the size and complexity of the models that can be trained. In this systematic survey, we aim to explore the current state-of-the-art techniques for breaking on-device training memory walls, focusing on methods that can enable larger and more complex models to be trained on resource-constrained devices.  Specifically, we first analyze the key factors that contribute to the phenomenon of memory walls encountered during on-device training. Then, we present a comprehensive literature review of on-device training, which addresses the issue of memory limitations. Finally, we summarize on-device training and highlight the open problems for future research.  By providing a comprehensive overview of these techniques and their effe",
    "path": "papers/23/06/2306.10388.json",
    "total_tokens": 947,
    "translated_title": "打破设备上训练内存壁垒：一项系统性调查",
    "translated_abstract": "设备上的训练已经成为机器学习中越来越流行的方法，可以直接在移动设备和边缘设备上进行模型训练。然而，在这个领域面临的一个主要挑战是这些设备上有限的内存，这会严重限制可以进行训练的模型的大小和复杂性。在这个系统性调查中，我们旨在探索打破设备上训练内存壁垒的当前最先进技术，重点关注可以在资源受限设备上训练更大更复杂的模型的方法。具体而言，我们首先分析了设备上训练中遇到的内存壁垒的关键因素。然后，我们提出了关于设备上训练的综合文献综述，地址了内存限制的问题。最后，我们总结了设备上训练，并突出了未来研究的开放问题。通过提供这些技术及其效果的全面概述，我们的调查对于解决设备上训练中的内存限制问题具有重要意义。",
    "tldr": "本研究调查了打破设备上训练内存壁垒的最先进技术，并提出了解决资源受限设备上训练更大更复杂模型的方法。调查分析了内存壁垒的关键因素，并总结了设备上训练的开放问题。",
    "en_tdlr": "This systematic survey explores the state-of-the-art techniques for breaking on-device training memory walls and proposes methods for training larger and more complex models on resource-constrained devices. The survey analyzes the key factors contributing to memory walls and summarizes the open questions in on-device training."
}