{
    "title": "Comparative study on Judgment Text Classification for Transformer Based Models. (arXiv:2306.01739v1 [cs.CL])",
    "abstract": "This work involves the usage of various NLP models to predict the winner of a particular judgment by the means of text extraction and summarization from a judgment document. These documents are useful when it comes to legal proceedings. One such advantage is that these can be used for citations and precedence reference in Lawsuits and cases which makes a strong argument for their case by the ones using it. When it comes to precedence, it is necessary to refer to an ample number of documents in order to collect legal points with respect to the case. However, reviewing these documents takes a long time to analyze due to the complex word structure and the size of the document. This work involves the comparative study of 6 different self-attention-based transformer models and how they perform when they are being tweaked in 4 different activation functions. These models which are trained with 200 judgement contexts and their results are being judged based on different benchmark parameters. ",
    "link": "http://arxiv.org/abs/2306.01739",
    "context": "Title: Comparative study on Judgment Text Classification for Transformer Based Models. (arXiv:2306.01739v1 [cs.CL])\nAbstract: This work involves the usage of various NLP models to predict the winner of a particular judgment by the means of text extraction and summarization from a judgment document. These documents are useful when it comes to legal proceedings. One such advantage is that these can be used for citations and precedence reference in Lawsuits and cases which makes a strong argument for their case by the ones using it. When it comes to precedence, it is necessary to refer to an ample number of documents in order to collect legal points with respect to the case. However, reviewing these documents takes a long time to analyze due to the complex word structure and the size of the document. This work involves the comparative study of 6 different self-attention-based transformer models and how they perform when they are being tweaked in 4 different activation functions. These models which are trained with 200 judgement contexts and their results are being judged based on different benchmark parameters. ",
    "path": "papers/23/06/2306.01739.json",
    "total_tokens": 870,
    "translated_title": "基于 Transformer 模型的裁判文本分类比较研究",
    "translated_abstract": "本研究利用各种自然语言处理模型，通过从判决文书中提取和摘要文本来预测特定判决的获胜者。这些文档在法律诉讼中非常有用。其中一个优点是，这些文档可用于引用和先例参考，这使得使用者可以更有力地为其案件辩护。当涉及到案例先例时，必须参考大量文档，以收集与该案件有关的法律要点。然而，由于文档的复杂词汇结构和文档大小，审查这些文档需要花费很长时间进行分析。本文涉及在4种不同激活函数下调整 6 种自我注意力的基于 Transformer 的模型，并研究它们在训练了 200 个判决上的表现，以及根据不同基准参数评估其结果。",
    "tldr": "本研究对6个不同的基于自我注意力的Transformer模型进行了比较研究，并针对4种激活函数进行了调整，以用于判决文本分类。该方法可以帮助在法律诉讼中进行引用和先例参考。",
    "en_tdlr": "This study compares 6 different self-attention-based transformer models and their performance in judgment text classification, with adjustments made for 4 different activation functions. The method can assist with citation and precedence reference in legal proceedings."
}