{
    "title": "ROSARL: Reward-Only Safe Reinforcement Learning. (arXiv:2306.00035v1 [cs.LG])",
    "abstract": "An important problem in reinforcement learning is designing agents that learn to solve tasks safely in an environment. A common solution is for a human expert to define either a penalty in the reward function or a cost to be minimised when reaching unsafe states. However, this is non-trivial, since too small a penalty may lead to agents that reach unsafe states, while too large a penalty increases the time to convergence. Additionally, the difficulty in designing reward or cost functions can increase with the complexity of the problem. Hence, for a given environment with a given set of unsafe states, we are interested in finding the upper bound of rewards at unsafe states whose optimal policies minimise the probability of reaching those unsafe states, irrespective of task rewards. We refer to this exact upper bound as the \"Minmax penalty\", and show that it can be obtained by taking into account both the controllability and diameter of an environment. We provide a simple practical model",
    "link": "http://arxiv.org/abs/2306.00035",
    "context": "Title: ROSARL: Reward-Only Safe Reinforcement Learning. (arXiv:2306.00035v1 [cs.LG])\nAbstract: An important problem in reinforcement learning is designing agents that learn to solve tasks safely in an environment. A common solution is for a human expert to define either a penalty in the reward function or a cost to be minimised when reaching unsafe states. However, this is non-trivial, since too small a penalty may lead to agents that reach unsafe states, while too large a penalty increases the time to convergence. Additionally, the difficulty in designing reward or cost functions can increase with the complexity of the problem. Hence, for a given environment with a given set of unsafe states, we are interested in finding the upper bound of rewards at unsafe states whose optimal policies minimise the probability of reaching those unsafe states, irrespective of task rewards. We refer to this exact upper bound as the \"Minmax penalty\", and show that it can be obtained by taking into account both the controllability and diameter of an environment. We provide a simple practical model",
    "path": "papers/23/06/2306.00035.json",
    "total_tokens": 906,
    "translated_title": "ROSARL: 基于奖励的安全强化学习",
    "translated_abstract": "强化学习中一个重要的问题是设计能够在环境中安全学习完成任务的智能体。常见的解决方法是由人类专家定义一个奖励函数中的惩罚或要最小化的成本，以达到不走入危险状态的目的。然而，太小的惩罚可能会导致智能体进入不安全的状态，而太大的惩罚则会增加收敛时间。此外，设计奖励或成本函数的难度随着问题的复杂度而增加。因此，对于给定的环境和一组不安全状态，我们希望找到在不考虑任务奖励的情况下，最小化到达不安全状态的概率的最高奖励上限。我们称这个确切的上限为“Minmax惩罚”，并展示了可以通过考虑环境的可控性和直径来获得它。我们提供了一个简单的实践模型。",
    "tldr": "ROSARL提出了一种基于奖励的安全强化学习方法，通过定义“Minmax惩罚”确定智能体在达到不安全状态时所允许的奖励上限，并考虑环境的可控性和直径来获得这个上限。",
    "en_tdlr": "ROSARL proposes a reward-based safe reinforcement learning approach, which defines the upper bound of rewards at unsafe states, referred to as \"Minmax penalty,\" to minimize the probability of reaching those states without considering task rewards. The approach takes into account both the controllability and diameter of the environment to obtain the upper bound."
}