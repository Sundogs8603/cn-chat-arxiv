{
    "title": "Visual-Aware Text-to-Speech. (arXiv:2306.12020v1 [eess.AS])",
    "abstract": "Dynamically synthesizing talking speech that actively responds to a listening head is critical during the face-to-face interaction. For example, the speaker could take advantage of the listener's facial expression to adjust the tones, stressed syllables, or pauses. In this work, we present a new visual-aware text-to-speech (VA-TTS) task to synthesize speech conditioned on both textual inputs and sequential visual feedback (e.g., nod, smile) of the listener in face-to-face communication. Different from traditional text-to-speech, VA-TTS highlights the impact of visual modality. On this newly-minted task, we devise a baseline model to fuse phoneme linguistic information and listener visual signals for speech synthesis. Extensive experiments on multimodal conversation dataset ViCo-X verify our proposal for generating more natural audio with scenario-appropriate rhythm and prosody.",
    "link": "http://arxiv.org/abs/2306.12020",
    "context": "Title: Visual-Aware Text-to-Speech. (arXiv:2306.12020v1 [eess.AS])\nAbstract: Dynamically synthesizing talking speech that actively responds to a listening head is critical during the face-to-face interaction. For example, the speaker could take advantage of the listener's facial expression to adjust the tones, stressed syllables, or pauses. In this work, we present a new visual-aware text-to-speech (VA-TTS) task to synthesize speech conditioned on both textual inputs and sequential visual feedback (e.g., nod, smile) of the listener in face-to-face communication. Different from traditional text-to-speech, VA-TTS highlights the impact of visual modality. On this newly-minted task, we devise a baseline model to fuse phoneme linguistic information and listener visual signals for speech synthesis. Extensive experiments on multimodal conversation dataset ViCo-X verify our proposal for generating more natural audio with scenario-appropriate rhythm and prosody.",
    "path": "papers/23/06/2306.12020.json",
    "total_tokens": 895,
    "translated_title": "视觉感知的语音合成",
    "translated_abstract": "动态地合成能够主动响应听众的语音在面对面的交互中至关重要。例如，演讲者可以利用听众的面部表情来调整语调、重读音节或停顿。在本文中，我们提出了一个新的视觉感知的文本到语音（VA-TTS）任务，用于在面对面的交流中，基于文本输入和接收者的序列视觉反馈（如点头、微笑）来合成语音。不同于传统的文本到语音，VA-TTS强调了视觉模态的影响。在这个新任务上，我们设计了一个基准模型，用于融合音素语言信息和听众的视觉信号来进行语音合成。对多模态对话数据集ViCo-X的广泛实验验证了我们的提议，生成了更自然的音频，并具有场景适当的韵律和语调。",
    "tldr": "本文提出了一个新的视觉感知的文本到语音（VA-TTS）任务，用于在面对面的交流中，基于文本输入和接收者的序列视觉反馈来合成语音，并设计了一个基准模型进行语音合成，实验证明在多模态对话数据集上生成更自然的音频，具有场景适当的韵律和语调。",
    "en_tdlr": "This paper proposes a new visual-aware text-to-speech (VA-TTS) task to synthesize speech conditioned on both textual inputs and sequential visual feedback of the listener, and designs a baseline model to fuse linguistic information and visual signals for speech synthesis, verifying the generation of more natural audio with scenario-appropriate rhythm and prosody on the multimodal conversation dataset ViCo-X."
}