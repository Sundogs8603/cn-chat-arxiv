{
    "title": "Decision S4: Efficient Sequence-Based RL via State Spaces Layers. (arXiv:2306.05167v1 [cs.LG])",
    "abstract": "Recently, sequence learning methods have been applied to the problem of off-policy Reinforcement Learning, including the seminal work on Decision Transformers, which employs transformers for this task. Since transformers are parameter-heavy, cannot benefit from history longer than a fixed window size, and are not computed using recurrence, we set out to investigate the suitability of the S4 family of models, which are based on state-space layers and have been shown to outperform transformers, especially in modeling long-range dependencies. In this work we present two main algorithms: (i) an off-policy training procedure that works with trajectories, while still maintaining the training efficiency of the S4 model. (ii) An on-policy training procedure that is trained in a recurrent manner, benefits from long-range dependencies, and is based on a novel stable actor-critic mechanism. Our results indicate that our method outperforms multiple variants of decision transformers, as well as the",
    "link": "http://arxiv.org/abs/2306.05167",
    "context": "Title: Decision S4: Efficient Sequence-Based RL via State Spaces Layers. (arXiv:2306.05167v1 [cs.LG])\nAbstract: Recently, sequence learning methods have been applied to the problem of off-policy Reinforcement Learning, including the seminal work on Decision Transformers, which employs transformers for this task. Since transformers are parameter-heavy, cannot benefit from history longer than a fixed window size, and are not computed using recurrence, we set out to investigate the suitability of the S4 family of models, which are based on state-space layers and have been shown to outperform transformers, especially in modeling long-range dependencies. In this work we present two main algorithms: (i) an off-policy training procedure that works with trajectories, while still maintaining the training efficiency of the S4 model. (ii) An on-policy training procedure that is trained in a recurrent manner, benefits from long-range dependencies, and is based on a novel stable actor-critic mechanism. Our results indicate that our method outperforms multiple variants of decision transformers, as well as the",
    "path": "papers/23/06/2306.05167.json",
    "total_tokens": 892,
    "translated_title": "Decision S4：基于状态空间层的高效序列强化学习方法",
    "translated_abstract": "最近，序列学习方法被应用于离线强化学习问题，其中包括使用Transformer的开创性工作——Decision Transformer。由于Transformer参数重，无法利用超过固定窗口大小的历史信息，且不能使用复现计算，因此我们开始研究S4模型系列的适用性。该模型基于状态空间层构建，已被证明在建模长程依赖方面优于Transformer。在本研究中，我们提出了两个主要算法：（i）一种离线训练过程，可以使用轨迹数据，同时仍然保持S4模型的训练效率；（ii）一种基于新颖稳定的演员 - 评论家机制，以循环方式训练的在线训练过程，可以从长程依赖中受益。我们的结果表明，在各种基准环境下，我们的方法优于多种Decision Transformer的变体以及传统的LSTM和GRU模型。",
    "tldr": "本论文提出了基于状态空间层的高效序列强化学习方法，包含一种离线和一种在线训练过程，可以从长程依赖中受益，取得了较好的效果。",
    "en_tdlr": "This paper proposes an efficient sequence-based reinforcement learning method based on state-space layers, which includes both offline and online training procedures, benefiting from long-range dependencies and achieving better performance compared to traditional LSTM, GRU and multiple variants of Decision Transformers on benchmark environments."
}