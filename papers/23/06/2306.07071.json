{
    "title": "Budgeted Multi-Armed Bandits with Asymmetric Confidence Intervals. (arXiv:2306.07071v1 [cs.LG])",
    "abstract": "We study the stochastic Budgeted Multi-Armed Bandit (MAB) problem, where a player chooses from $K$ arms with unknown expected rewards and costs. The goal is to maximize the total reward under a budget constraint. A player thus seeks to choose the arm with the highest reward-cost ratio as often as possible. Current state-of-the-art policies for this problem have several issues, which we illustrate. To overcome them, we propose a new upper confidence bound (UCB) sampling policy, $\\omega$-UCB, that uses asymmetric confidence intervals. These intervals scale with the distance between the sample mean and the bounds of a random variable, yielding a more accurate and tight estimation of the reward-cost ratio compared to our competitors. We show that our approach has logarithmic regret and consistently outperforms existing policies in synthetic and real settings.",
    "link": "http://arxiv.org/abs/2306.07071",
    "context": "Title: Budgeted Multi-Armed Bandits with Asymmetric Confidence Intervals. (arXiv:2306.07071v1 [cs.LG])\nAbstract: We study the stochastic Budgeted Multi-Armed Bandit (MAB) problem, where a player chooses from $K$ arms with unknown expected rewards and costs. The goal is to maximize the total reward under a budget constraint. A player thus seeks to choose the arm with the highest reward-cost ratio as often as possible. Current state-of-the-art policies for this problem have several issues, which we illustrate. To overcome them, we propose a new upper confidence bound (UCB) sampling policy, $\\omega$-UCB, that uses asymmetric confidence intervals. These intervals scale with the distance between the sample mean and the bounds of a random variable, yielding a more accurate and tight estimation of the reward-cost ratio compared to our competitors. We show that our approach has logarithmic regret and consistently outperforms existing policies in synthetic and real settings.",
    "path": "papers/23/06/2306.07071.json",
    "total_tokens": 911,
    "translated_title": "具有不对称置信区间的有限预算多臂老虎机问题",
    "translated_abstract": "我们研究了随机预算多臂老虎机（MAB）问题，其中玩家选择具有未知期望奖励和成本的K个臂。目标是在预算约束下最大化总奖励。因此，玩家试图尽可能经常地选择具有最高奖励成本比的臂。当前针对此问题的最先进策略存在一些问题，我们予以说明。为了克服这些问题，我们提出了一种新的上置信区间（UCB）抽样策略，称为ω-UCB，并使用不对称置信区间。这些区间尺度随着样本均值和随机变量边界之间的距离而变化，相对于我们的竞争对手，可以更准确、更紧密地估计奖励成本比。我们证明了我们的方法具有对数后悔，并在合成和真实环境中始终优于现有策略。",
    "tldr": "这篇论文提出了一种名为ω-UCB的新上置信区间抽样策略，使用不对称置信区间以更准确、更紧密地估计奖励成本比，解决了现有预算多臂老虎机问题策略存在的问题，并在合成和真实环境中表现出色。",
    "en_tdlr": "This paper proposes a new upper confidence bound sampling policy named ω-UCB, which uses asymmetric confidence intervals to more accurately and tightly estimate the reward-cost ratio and overcomes current issues in budgeted multi-armed bandit strategies. The approach shows logarithmic regret and consistently outperforms existing policies in both synthetic and real settings."
}