{
    "title": "Deep Gaussian Mixture Ensembles. (arXiv:2306.07235v1 [stat.ML])",
    "abstract": "This work introduces a novel probabilistic deep learning technique called deep Gaussian mixture ensembles (DGMEs), which enables accurate quantification of both epistemic and aleatoric uncertainty. By assuming the data generating process follows that of a Gaussian mixture, DGMEs are capable of approximating complex probability distributions, such as heavy-tailed or multimodal distributions. Our contributions include the derivation of an expectation-maximization (EM) algorithm used for learning the model parameters, which results in an upper-bound on the log-likelihood of training data over that of standard deep ensembles. Additionally, the proposed EM training procedure allows for learning of mixture weights, which is not commonly done in ensembles. Our experimental results demonstrate that DGMEs outperform state-of-the-art uncertainty quantifying deep learning models in handling complex predictive densities.",
    "link": "http://arxiv.org/abs/2306.07235",
    "context": "Title: Deep Gaussian Mixture Ensembles. (arXiv:2306.07235v1 [stat.ML])\nAbstract: This work introduces a novel probabilistic deep learning technique called deep Gaussian mixture ensembles (DGMEs), which enables accurate quantification of both epistemic and aleatoric uncertainty. By assuming the data generating process follows that of a Gaussian mixture, DGMEs are capable of approximating complex probability distributions, such as heavy-tailed or multimodal distributions. Our contributions include the derivation of an expectation-maximization (EM) algorithm used for learning the model parameters, which results in an upper-bound on the log-likelihood of training data over that of standard deep ensembles. Additionally, the proposed EM training procedure allows for learning of mixture weights, which is not commonly done in ensembles. Our experimental results demonstrate that DGMEs outperform state-of-the-art uncertainty quantifying deep learning models in handling complex predictive densities.",
    "path": "papers/23/06/2306.07235.json",
    "total_tokens": 832,
    "translated_title": "深度高斯混合集合模型",
    "translated_abstract": "本文介绍了一种新颖的概率深度学习技术，称为深度高斯混合集合 (DGMEs)，可实现对先验误差和后验误差的准确量化。通过假设数据生成过程符合高斯混合分布，DGMEs 能够逼近复杂的概率分布，例如重尾或多峰分布。我们的贡献包括推导出用于学习模型参数的期望最大化 (EM) 算法，这将得到对标准深度集合的训练数据对数似然的上界。此外，所提出的 EM 训练过程允许学习混合权重，这在集合中通常不会做。我们的实验结果表明，DGMEs 在处理复杂预测密度方面优于最先进的不确定性量化深度学习模型。",
    "tldr": "本文介绍了一种深度高斯混合集合模型，可逼近复杂概率分布，并能够准确量化先验误差和后验误差，实验结果表明其优于最先进的不确定性量化深度学习模型。"
}