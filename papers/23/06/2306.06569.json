{
    "title": "Policy Regularization with Dataset Constraint for Offline Reinforcement Learning. (arXiv:2306.06569v2 [cs.LG] UPDATED)",
    "abstract": "We consider the problem of learning the best possible policy from a fixed dataset, known as offline Reinforcement Learning (RL). A common taxonomy of existing offline RL works is policy regularization, which typically constrains the learned policy by distribution or support of the behavior policy. However, distribution and support constraints are overly conservative since they both force the policy to choose similar actions as the behavior policy when considering particular states. It will limit the learned policy's performance, especially when the behavior policy is sub-optimal. In this paper, we find that regularizing the policy towards the nearest state-action pair can be more effective and thus propose Policy Regularization with Dataset Constraint (PRDC). When updating the policy in a given state, PRDC searches the entire dataset for the nearest state-action sample and then restricts the policy with the action of this sample. Unlike previous works, PRDC can guide the policy with pr",
    "link": "http://arxiv.org/abs/2306.06569",
    "context": "Title: Policy Regularization with Dataset Constraint for Offline Reinforcement Learning. (arXiv:2306.06569v2 [cs.LG] UPDATED)\nAbstract: We consider the problem of learning the best possible policy from a fixed dataset, known as offline Reinforcement Learning (RL). A common taxonomy of existing offline RL works is policy regularization, which typically constrains the learned policy by distribution or support of the behavior policy. However, distribution and support constraints are overly conservative since they both force the policy to choose similar actions as the behavior policy when considering particular states. It will limit the learned policy's performance, especially when the behavior policy is sub-optimal. In this paper, we find that regularizing the policy towards the nearest state-action pair can be more effective and thus propose Policy Regularization with Dataset Constraint (PRDC). When updating the policy in a given state, PRDC searches the entire dataset for the nearest state-action sample and then restricts the policy with the action of this sample. Unlike previous works, PRDC can guide the policy with pr",
    "path": "papers/23/06/2306.06569.json",
    "total_tokens": 891,
    "translated_title": "离线强化学习中具有数据集约束的策略正则化",
    "translated_abstract": "本文考虑从固定数据集中学习最佳策略的问题，即离线强化学习。现有的离线强化学习方法通常采用策略正则化来约束学习策略的分布或行为策略的支持。然而，分布和支持约束过于保守，因为它们都要求学习策略在特定状态下选择与行为策略相似的动作。这将限制学习策略的性能，特别是当行为策略是次优的情况下。本文发现，将策略正则化指向最近的状态-动作对可能更加有效，因此提出了具有数据集约束的策略正则化（PRDC）。在给定状态下更新策略时，PRDC会在整个数据集中搜索最接近的状态-动作样本，然后用该样本的动作来约束策略。与现有方法不同，PRDC可以指导策略根据最相关的状态-动作来进行更新，从而提高策略的性能。",
    "tldr": "本文提出了一种离线强化学习方法，称为PRDC，它使用数据集约束来正则化学习策略。相比于现有方法，PRDC可以更有效地指导策略的更新，并提高其性能。"
}