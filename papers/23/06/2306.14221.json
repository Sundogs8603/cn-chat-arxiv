{
    "title": "Feature Adversarial Distillation for Point Cloud Classification. (arXiv:2306.14221v2 [cs.CV] UPDATED)",
    "abstract": "Due to the point cloud's irregular and unordered geometry structure, conventional knowledge distillation technology lost a lot of information when directly used on point cloud tasks. In this paper, we propose Feature Adversarial Distillation (FAD) method, a generic adversarial loss function in point cloud distillation, to reduce loss during knowledge transfer. In the feature extraction stage, the features extracted by the teacher are used as the discriminator, and the students continuously generate new features in the training stage. The feature of the student is obtained by attacking the feedback from the teacher and getting a score to judge whether the student has learned the knowledge well or not. In experiments on standard point cloud classification on ModelNet40 and ScanObjectNN datasets, our method reduced the information loss of knowledge transfer in distillation in 40x model compression while maintaining competitive performance.",
    "link": "http://arxiv.org/abs/2306.14221",
    "context": "Title: Feature Adversarial Distillation for Point Cloud Classification. (arXiv:2306.14221v2 [cs.CV] UPDATED)\nAbstract: Due to the point cloud's irregular and unordered geometry structure, conventional knowledge distillation technology lost a lot of information when directly used on point cloud tasks. In this paper, we propose Feature Adversarial Distillation (FAD) method, a generic adversarial loss function in point cloud distillation, to reduce loss during knowledge transfer. In the feature extraction stage, the features extracted by the teacher are used as the discriminator, and the students continuously generate new features in the training stage. The feature of the student is obtained by attacking the feedback from the teacher and getting a score to judge whether the student has learned the knowledge well or not. In experiments on standard point cloud classification on ModelNet40 and ScanObjectNN datasets, our method reduced the information loss of knowledge transfer in distillation in 40x model compression while maintaining competitive performance.",
    "path": "papers/23/06/2306.14221.json",
    "total_tokens": 849,
    "translated_title": "特征对抗蒸馏用于点云分类",
    "translated_abstract": "由于点云的不规则和无序的几何结构，传统的知识蒸馏技术在直接应用于点云任务时丢失了很多信息。本文提出了一种特征对抗蒸馏（Feature Adversarial Distillation，简称FAD）方法，这是一种在点云蒸馏中使用的通用对抗损失函数，可减少知识传递过程中的信息损失。在特征提取阶段，利用教师提取的特征作为鉴别器，在训练阶段学生不断生成新特征。学生的特征通过攻击教师的反馈得到得分，以判断学生是否学习了知识。在ModelNet40和ScanObjectNN数据集上进行的标准点云分类实验中，我们的方法在40倍模型压缩的同时降低了知识传递中的信息损失，并保持了竞争性能。",
    "tldr": "本文提出了一种特征对抗蒸馏方法（FAD），用于解决点云分类中知识传递的信息损失问题。在实验证明，该方法在模型压缩的同时保持了竞争性能。",
    "en_tdlr": "This paper proposes a Feature Adversarial Distillation (FAD) method to address the problem of information loss during knowledge transfer in point cloud classification. The experiments demonstrate that this method achieves competitive performance while reducing information loss in model compression."
}