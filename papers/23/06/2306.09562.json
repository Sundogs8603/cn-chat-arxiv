{
    "title": "Reproducibility in NLP: What Have We Learned from the Checklist?. (arXiv:2306.09562v1 [cs.CL])",
    "abstract": "Scientific progress in NLP rests on the reproducibility of researchers' claims. The *CL conferences created the NLP Reproducibility Checklist in 2020 to be completed by authors at submission to remind them of key information to include. We provide the first analysis of the Checklist by examining 10,405 anonymous responses to it. First, we find evidence of an increase in reporting of information on efficiency, validation performance, summary statistics, and hyperparameters after the Checklist's introduction. Further, we show acceptance rate grows for submissions with more Yes responses. We find that the 44% of submissions that gather new data are 5% less likely to be accepted than those that did not; the average reviewer-rated reproducibility of these submissions is also 2% lower relative to the rest. We find that only 46% of submissions claim to open-source their code, though submissions that do have 8% higher reproducibility score relative to those that do not, the most for any item. ",
    "link": "http://arxiv.org/abs/2306.09562",
    "context": "Title: Reproducibility in NLP: What Have We Learned from the Checklist?. (arXiv:2306.09562v1 [cs.CL])\nAbstract: Scientific progress in NLP rests on the reproducibility of researchers' claims. The *CL conferences created the NLP Reproducibility Checklist in 2020 to be completed by authors at submission to remind them of key information to include. We provide the first analysis of the Checklist by examining 10,405 anonymous responses to it. First, we find evidence of an increase in reporting of information on efficiency, validation performance, summary statistics, and hyperparameters after the Checklist's introduction. Further, we show acceptance rate grows for submissions with more Yes responses. We find that the 44% of submissions that gather new data are 5% less likely to be accepted than those that did not; the average reviewer-rated reproducibility of these submissions is also 2% lower relative to the rest. We find that only 46% of submissions claim to open-source their code, though submissions that do have 8% higher reproducibility score relative to those that do not, the most for any item. ",
    "path": "papers/23/06/2306.09562.json",
    "total_tokens": 953,
    "translated_title": "自然语言处理中的可重复性：从清单中学到了什么？",
    "translated_abstract": "NLP的科学进展建立在研究人员声明的可重复性基础之上。*CL会议在2020年创建了NLP可重复性清单，要求作者在提交时完成，以提醒他们包括关键信息。我们通过检查10,405个匿名回应，提供了该清单的第一次分析。首先，我们发现在清单引入后，有关效率、验证性能、摘要统计和超参数的信息报告有所增加。此外，我们展示了接受率随着越来越多的“是”回答的提交增长。我们发现，收集新数据的44%的提交比没有收集新数据的提交少5%的被接受率；这些提交的平均评审可重复性与其他提交相比也低了2%。我们发现只有46%的提交声明开源他们的代码，尽管采用此项的提交与未采用此项的提交相比，可重复性得分高8％，是任何条目中最高的。",
    "tldr": "本论文讨论了NLP科学进展的可重复性，介绍了NLP可重复性清单的情况和通过对此清单10,405个匿名回答的分析发现某些项回答“是”的提交数量增加、随之接受率增加以及开源代码与可重复性得分提高之间的关系等。"
}