{
    "title": "Landmark Guided Active Exploration with Stable Low-level Policy Learning. (arXiv:2306.17484v1 [cs.LG])",
    "abstract": "Goal-conditioned hierarchical reinforcement learning (GCHRL) decomposes long-horizon tasks into sub-tasks through a hierarchical framework and it has demonstrated promising results across a variety of domains. However, the high-level policy's action space is often excessively large, presenting a significant challenge to effective exploration and resulting in potentially inefficient training. Moreover, the dynamic variability of the low-level policy introduces non-stationarity to the high-level state transition function, significantly impeding the learning of the high-level policy. In this paper, we design a measure of prospect for subgoals by planning in the goal space based on the goal-conditioned value function. Building upon the measure of prospect, we propose a landmark-guided exploration strategy by integrating the measures of prospect and novelty which aims to guide the agent to explore efficiently and improve sample efficiency. To address the non-stationarity arising from the dy",
    "link": "http://arxiv.org/abs/2306.17484",
    "context": "Title: Landmark Guided Active Exploration with Stable Low-level Policy Learning. (arXiv:2306.17484v1 [cs.LG])\nAbstract: Goal-conditioned hierarchical reinforcement learning (GCHRL) decomposes long-horizon tasks into sub-tasks through a hierarchical framework and it has demonstrated promising results across a variety of domains. However, the high-level policy's action space is often excessively large, presenting a significant challenge to effective exploration and resulting in potentially inefficient training. Moreover, the dynamic variability of the low-level policy introduces non-stationarity to the high-level state transition function, significantly impeding the learning of the high-level policy. In this paper, we design a measure of prospect for subgoals by planning in the goal space based on the goal-conditioned value function. Building upon the measure of prospect, we propose a landmark-guided exploration strategy by integrating the measures of prospect and novelty which aims to guide the agent to explore efficiently and improve sample efficiency. To address the non-stationarity arising from the dy",
    "path": "papers/23/06/2306.17484.json",
    "total_tokens": 876,
    "translated_title": "通过稳定的低层策略学习进行地标引导的主动探索",
    "translated_abstract": "目标导向的分级强化学习（GCHRL）通过分层框架将长期任务分解为子任务，并在各种领域中展示出了有希望的结果。然而，高层策略的行动空间通常过大，给有效探索带来了重要挑战，并可能导致训练效率低下。此外，低层策略的动态变异性将非稳态引入到高层状态转换函数中，严重阻碍了高层策略的学习。在本文中，我们设计了一种在目标空间中基于目标导向价值函数进行规划的子目标前景度量。在子目标前景度量的基础上，我们提出了一种地标引导的探索策略，通过整合前景度量和新颖性度量来引导智能体进行高效探索并提高样本效率。",
    "tldr": "本文设计了一种稳定的低层策略学习方法，通过目标导向的分级强化学习和地标引导的探索策略，在提高训练效率的同时解决了高层策略行动空间过大和低层策略的非稳态问题。",
    "en_tdlr": "This paper proposes a stable low-level policy learning method that addresses the challenges of large action space and non-stationarity in high-level policy, using goal-conditioned hierarchical reinforcement learning and landmark-guided exploration strategy, while improving training efficiency."
}