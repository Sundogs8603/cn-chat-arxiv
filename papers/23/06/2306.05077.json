{
    "title": "Improving Language Model Integration for Neural Machine Translation. (arXiv:2306.05077v1 [cs.CL])",
    "abstract": "The integration of language models for neural machine translation has been extensively studied in the past. It has been shown that an external language model, trained on additional target-side monolingual data, can help improve translation quality. However, there has always been the assumption that the translation model also learns an implicit target-side language model during training, which interferes with the external language model at decoding time. Recently, some works on automatic speech recognition have demonstrated that, if the implicit language model is neutralized in decoding, further improvements can be gained when integrating an external language model. In this work, we transfer this concept to the task of machine translation and compare with the most prominent way of including additional monolingual data - namely back-translation. We find that accounting for the implicit language model significantly boosts the performance of language model fusion, although this approach is",
    "link": "http://arxiv.org/abs/2306.05077",
    "context": "Title: Improving Language Model Integration for Neural Machine Translation. (arXiv:2306.05077v1 [cs.CL])\nAbstract: The integration of language models for neural machine translation has been extensively studied in the past. It has been shown that an external language model, trained on additional target-side monolingual data, can help improve translation quality. However, there has always been the assumption that the translation model also learns an implicit target-side language model during training, which interferes with the external language model at decoding time. Recently, some works on automatic speech recognition have demonstrated that, if the implicit language model is neutralized in decoding, further improvements can be gained when integrating an external language model. In this work, we transfer this concept to the task of machine translation and compare with the most prominent way of including additional monolingual data - namely back-translation. We find that accounting for the implicit language model significantly boosts the performance of language model fusion, although this approach is",
    "path": "papers/23/06/2306.05077.json",
    "total_tokens": 893,
    "translated_abstract": "过去对于神经机器翻译中语言模型的集成进行了广泛的研究。研究表明，一个在额外目标语言单语数据上训练的外部语言模型可以帮助提高翻译质量。然而，在训练时常常假设翻译模型同时在学习一个隐式的目标语言模型，而此时在解码时它会与外部语言模型产生干扰。最近，自动语音识别领域的一些研究表明，如果在解码中去除隐式语言模型，集成外部语言模型时会带来进一步的提高。本文将这一概念转移至机器翻译任务中，并与目前比较突出的添加额外单语数据的方法——反向翻译进行了比较研究。我们发现，考虑到隐式语言模型会显著提升语言模型融合的性能，尽管这种方法在具体示例中并不一定表现得最佳。",
    "tldr": "本文在神经机器翻译中特别针对语言模型的集成进行了研究，发现如果能够在解码时去除隐式语言模型，则可以在集成外部语言模型的时候进一步提高翻译质量。",
    "en_tdlr": "This paper focuses on the integration of language models for neural machine translation, and finds that if the implicit language model can be eliminated during decoding, further improvements can be gained when integrating an external language model."
}