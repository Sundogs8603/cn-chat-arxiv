{
    "title": "GroundNLQ @ Ego4D Natural Language Queries Challenge 2023. (arXiv:2306.15255v1 [cs.CV])",
    "abstract": "In this report, we present our champion solution for Ego4D Natural Language Queries (NLQ) Challenge in CVPR 2023. Essentially, to accurately ground in a video, an effective egocentric feature extractor and a powerful grounding model are required. Motivated by this, we leverage a two-stage pre-training strategy to train egocentric feature extractors and the grounding model on video narrations, and further fine-tune the model on annotated data. In addition, we introduce a novel grounding model GroundNLQ, which employs a multi-modal multi-scale grounding module for effective video and text fusion and various temporal intervals, especially for long videos. On the blind test set, GroundNLQ achieves 25.67 and 18.18 for R1@IoU=0.3 and R1@IoU=0.5, respectively, and surpasses all other teams by a noticeable margin. Our code will be released at\\url{https://github.com/houzhijian/GroundNLQ}.",
    "link": "http://arxiv.org/abs/2306.15255",
    "context": "Title: GroundNLQ @ Ego4D Natural Language Queries Challenge 2023. (arXiv:2306.15255v1 [cs.CV])\nAbstract: In this report, we present our champion solution for Ego4D Natural Language Queries (NLQ) Challenge in CVPR 2023. Essentially, to accurately ground in a video, an effective egocentric feature extractor and a powerful grounding model are required. Motivated by this, we leverage a two-stage pre-training strategy to train egocentric feature extractors and the grounding model on video narrations, and further fine-tune the model on annotated data. In addition, we introduce a novel grounding model GroundNLQ, which employs a multi-modal multi-scale grounding module for effective video and text fusion and various temporal intervals, especially for long videos. On the blind test set, GroundNLQ achieves 25.67 and 18.18 for R1@IoU=0.3 and R1@IoU=0.5, respectively, and surpasses all other teams by a noticeable margin. Our code will be released at\\url{https://github.com/houzhijian/GroundNLQ}.",
    "path": "papers/23/06/2306.15255.json",
    "total_tokens": 920,
    "translated_title": "GroundNLQ @ Ego4D自然语言查询挑战2023年",
    "translated_abstract": "在本报告中，我们呈现了我们在CVPR 2023年的Ego4D自然语言查询挑战中的冠军解决方案。为了在视频中准确进行标注，需要一个有效的自我中心特征提取器和一个强大的标注模型。为此，我们采用两阶段的预训练策略，在视频叙述上训练自我中心特征提取器和标注模型，并在标注数据上进行进一步的微调。此外，我们引入了一种新颖的标注模型GroundNLQ，它采用了多模态多尺度的标注模块，用于有效融合视频和文本，并对各种时间间隔（尤其是长视频）进行处理。在盲测集上，GroundNLQ在R1@IoU=0.3和R1@IoU=0.5分别达到了25.67和18.18，并且在各项指标上都明显超过了其他所有团队。我们的代码将在\\url{https://github.com/houzhijian/GroundNLQ}上发布。",
    "tldr": "GroundNLQ是一个用于自然语言查询挑战的创新标注模型，通过采用两阶段的预训练策略和多模态多尺度的标注模块，实现了在视频中的准确标注，获得了较好的性能表现。",
    "en_tdlr": "GroundNLQ is an innovative grounding model for natural language queries, which achieves accurate grounding in videos through a two-stage pre-training strategy and a multi-modal multi-scale grounding module, demonstrating excellent performance."
}