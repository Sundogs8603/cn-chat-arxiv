{
    "title": "Beyond dynamic programming. (arXiv:2306.15029v1 [cs.LG])",
    "abstract": "In this paper, we present Score-life programming, a novel theoretical approach for solving reinforcement learning problems. In contrast with classical dynamic programming-based methods, our method can search over non-stationary policy functions, and can directly compute optimal infinite horizon action sequences from a given state. The central idea in our method is the construction of a mapping between infinite horizon action sequences and real numbers in a bounded interval. This construction enables us to formulate an optimization problem for directly computing optimal infinite horizon action sequences, without requiring a policy function. We demonstrate the effectiveness of our approach by applying it to nonlinear optimal control problems. Overall, our contributions provide a novel theoretical framework for formulating and solving reinforcement learning problems.",
    "link": "http://arxiv.org/abs/2306.15029",
    "context": "Title: Beyond dynamic programming. (arXiv:2306.15029v1 [cs.LG])\nAbstract: In this paper, we present Score-life programming, a novel theoretical approach for solving reinforcement learning problems. In contrast with classical dynamic programming-based methods, our method can search over non-stationary policy functions, and can directly compute optimal infinite horizon action sequences from a given state. The central idea in our method is the construction of a mapping between infinite horizon action sequences and real numbers in a bounded interval. This construction enables us to formulate an optimization problem for directly computing optimal infinite horizon action sequences, without requiring a policy function. We demonstrate the effectiveness of our approach by applying it to nonlinear optimal control problems. Overall, our contributions provide a novel theoretical framework for formulating and solving reinforcement learning problems.",
    "path": "papers/23/06/2306.15029.json",
    "total_tokens": 836,
    "translated_title": "超越动态规划",
    "translated_abstract": "本文提出了一种新颖的理论方法——得分寿命规划，用于解决强化学习问题。与传统的基于动态规划的方法相比，我们的方法可以搜索非稳态策略函数，并且可以直接计算给定状态下的最优无限时间区间动作序列。我们方法的核心思想是建立无限时间区间动作序列和有界区间内实数之间的映射。这种构造使我们能够制定一个优化问题，直接计算最优无限时间区间动作序列，无需策略函数。我们通过将该方法应用于非线性最优控制问题，验证了其有效性。总的来说，我们的贡献为制定和解决强化学习问题提供了一种新颖的理论框架。",
    "tldr": "提出了一种新的理论方法——得分寿命规划，用于解决强化学习问题。方法可以搜索非稳态策略函数，并直接计算最优无限时间区间动作序列。这种方法的核心思想是建立动作序列和实数之间的映射，并通过优化问题计算最优动作序列。方法在非线性最优控制问题中证明了有效性。",
    "en_tdlr": "A novel theoretical approach called Score-life programming is proposed for solving reinforcement learning problems. This method can search over non-stationary policy functions and directly compute optimal infinite horizon action sequences. The central idea is the construction of a mapping between action sequences and real numbers, enabling the formulation of an optimization problem for computing optimal action sequences. The effectiveness of the approach is demonstrated through applications in nonlinear optimal control problems."
}