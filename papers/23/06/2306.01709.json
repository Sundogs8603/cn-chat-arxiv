{
    "title": "Distilling Efficient Language-Specific Models for Cross-Lingual Transfer. (arXiv:2306.01709v1 [cs.CL])",
    "abstract": "Massively multilingual Transformers (MMTs), such as mBERT and XLM-R, are widely used for cross-lingual transfer learning. While these are pretrained to represent hundreds of languages, end users of NLP systems are often interested only in individual languages. For such purposes, the MMTs' language coverage makes them unnecessarily expensive to deploy in terms of model size, inference time, energy, and hardware cost. We thus propose to extract compressed, language-specific models from MMTs which retain the capacity of the original MMTs for cross-lingual transfer. This is achieved by distilling the MMT bilingually, i.e., using data from only the source and target language of interest. Specifically, we use a two-phase distillation approach, termed BiStil: (i) the first phase distils a general bilingual model from the MMT, while (ii) the second, task-specific phase sparsely fine-tunes the bilingual \"student\" model using a task-tuned variant of the original MMT as its \"teacher\". We evaluate",
    "link": "http://arxiv.org/abs/2306.01709",
    "context": "Title: Distilling Efficient Language-Specific Models for Cross-Lingual Transfer. (arXiv:2306.01709v1 [cs.CL])\nAbstract: Massively multilingual Transformers (MMTs), such as mBERT and XLM-R, are widely used for cross-lingual transfer learning. While these are pretrained to represent hundreds of languages, end users of NLP systems are often interested only in individual languages. For such purposes, the MMTs' language coverage makes them unnecessarily expensive to deploy in terms of model size, inference time, energy, and hardware cost. We thus propose to extract compressed, language-specific models from MMTs which retain the capacity of the original MMTs for cross-lingual transfer. This is achieved by distilling the MMT bilingually, i.e., using data from only the source and target language of interest. Specifically, we use a two-phase distillation approach, termed BiStil: (i) the first phase distils a general bilingual model from the MMT, while (ii) the second, task-specific phase sparsely fine-tunes the bilingual \"student\" model using a task-tuned variant of the original MMT as its \"teacher\". We evaluate",
    "path": "papers/23/06/2306.01709.json",
    "total_tokens": 957,
    "translated_title": "跨语言迁移的高效语言特定模型蒸馏",
    "translated_abstract": "大规模多语言Transformer（MMTs），如mBERT和XLM-R，被广泛用于跨语言迁移学习。虽然它们进行了预训练来表示数百种语言，但自然语言处理系统的最终用户通常只对个别语言感兴趣。为此，MMT的语言覆盖范围使它们在模型大小、推理时间、能源和硬件成本方面不必要地昂贵。因此，我们提出从MMTs中提取压缩的语言特定模型，保留原始MMTs在跨语言迁移方面的能力。这是通过进行双语蒸馏实现的，即仅使用源语言和目标语言的数据。具体来说，我们使用一个名为BiStil的两阶段蒸馏方法：（i）第一阶段从MMT中蒸馏出一般的双语模型，而（ii）第二阶段使用经过任务调整的原始MMT变体作为“教师”，通过稀疏精调双语“学生”模型来进行任务特定的蒸馏。我们评估了...",
    "tldr": "本文提出了一种从大规模多语言模型中提取语言特定模型的方法，它通过双语蒸馏实现，可以保留原始模型的跨语言迁移能力，同时可以避免不必要的模型部署成本。"
}