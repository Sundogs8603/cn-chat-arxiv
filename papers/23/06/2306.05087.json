{
    "title": "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization. (arXiv:2306.05087v1 [cs.CL])",
    "abstract": "Instruction tuning large language models (LLMs) remains a challenging task, owing to the complexity of hyperparameter selection and the difficulty involved in evaluating the tuned models. To determine the optimal hyperparameters, an automatic, robust, and reliable evaluation benchmark is essential. However, establishing such a benchmark is not a trivial task due to the challenges associated with evaluation accuracy and privacy protection. In response to these challenges, we introduce a judge large language model, named PandaLM, which is trained to distinguish the superior model given several LLMs. PandaLM's focus extends beyond just the objective correctness of responses, which is the main focus of traditional evaluation datasets. It addresses vital subjective factors such as relative conciseness, clarity, adherence to instructions, comprehensiveness, and formality. To ensure the reliability of PandaLM, we collect a diverse human-annotated test dataset, where all contexts are generated",
    "link": "http://arxiv.org/abs/2306.05087",
    "context": "Title: PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization. (arXiv:2306.05087v1 [cs.CL])\nAbstract: Instruction tuning large language models (LLMs) remains a challenging task, owing to the complexity of hyperparameter selection and the difficulty involved in evaluating the tuned models. To determine the optimal hyperparameters, an automatic, robust, and reliable evaluation benchmark is essential. However, establishing such a benchmark is not a trivial task due to the challenges associated with evaluation accuracy and privacy protection. In response to these challenges, we introduce a judge large language model, named PandaLM, which is trained to distinguish the superior model given several LLMs. PandaLM's focus extends beyond just the objective correctness of responses, which is the main focus of traditional evaluation datasets. It addresses vital subjective factors such as relative conciseness, clarity, adherence to instructions, comprehensiveness, and formality. To ensure the reliability of PandaLM, we collect a diverse human-annotated test dataset, where all contexts are generated",
    "path": "papers/23/06/2306.05087.json",
    "total_tokens": 833,
    "translated_title": "PandaLM：LLM指令调优优化的自动评估基准",
    "translated_abstract": "由于超参数选择的复杂性和评估调整模型的困难性，LLM（大型语言模型）的指令调优仍然是一项具有挑战性的任务。为确定最佳超参数，需要一个自动的、强大且可靠的评估基准。然而，由于评估准确性和隐私保护的挑战，建立这样一个基准并不是一项简单的任务。为应对这些挑战，我们引入了一款名为PandaLM的评测大型语言模型，该模型经过训练，能够区分出多个LLM中最佳的模型。PandaLM的关注点不仅限于传统评估数据集的客观正确性，还涵盖了诸如相对简洁性、清晰度、遵循说明、全面性和形式性等重要主观因素。为确保PandaLM的可靠性，我们收集了一个多样化的人工注释测试数据集，其中所有上下文都是生成的。",
    "tldr": "PandaLM是一个评估LLM指令调优的自动基准，它能够区分最优模型，并关注于主观因素。",
    "en_tdlr": "PandaLM is an automatic benchmark for evaluating LLM instruction tuning optimization, which can distinguish the best model and focus on subjective factors."
}