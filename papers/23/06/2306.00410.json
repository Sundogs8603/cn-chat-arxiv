{
    "title": "Towards hate speech detection in low-resource languages: Comparing ASR to acoustic word embeddings on Wolof and Swahili. (arXiv:2306.00410v1 [cs.CL])",
    "abstract": "We consider hate speech detection through keyword spotting on radio broadcasts. One approach is to build an automatic speech recognition (ASR) system for the target low-resource language. We compare this to using acoustic word embedding (AWE) models that map speech segments to a space where matching words have similar vectors. We specifically use a multilingual AWE model trained on labelled data from well-resourced languages to spot keywords in data in the unseen target language. In contrast to ASR, the AWE approach only requires a few keyword exemplars. In controlled experiments on Wolof and Swahili where training and test data are from the same domain, an ASR model trained on just five minutes of data outperforms the AWE approach. But in an in-the-wild test on Swahili radio broadcasts with actual hate speech keywords, the AWE model (using one minute of template data) is more robust, giving similar performance to an ASR system trained on 30 hours of labelled data.",
    "link": "http://arxiv.org/abs/2306.00410",
    "context": "Title: Towards hate speech detection in low-resource languages: Comparing ASR to acoustic word embeddings on Wolof and Swahili. (arXiv:2306.00410v1 [cs.CL])\nAbstract: We consider hate speech detection through keyword spotting on radio broadcasts. One approach is to build an automatic speech recognition (ASR) system for the target low-resource language. We compare this to using acoustic word embedding (AWE) models that map speech segments to a space where matching words have similar vectors. We specifically use a multilingual AWE model trained on labelled data from well-resourced languages to spot keywords in data in the unseen target language. In contrast to ASR, the AWE approach only requires a few keyword exemplars. In controlled experiments on Wolof and Swahili where training and test data are from the same domain, an ASR model trained on just five minutes of data outperforms the AWE approach. But in an in-the-wild test on Swahili radio broadcasts with actual hate speech keywords, the AWE model (using one minute of template data) is more robust, giving similar performance to an ASR system trained on 30 hours of labelled data.",
    "path": "papers/23/06/2306.00410.json",
    "total_tokens": 912,
    "translated_title": "对低资源语言进行仇恨言论检测：比较ASR和Wolof以及Swahili中基于声学词嵌入的方法",
    "translated_abstract": "我们考虑通过关键词识别在广播中检测仇恨言论。一种方法是为目标低资源语言建立一个自动语音识别（ASR）系统。我们将此与使用基于声学词嵌入（AWE）模型进行比较，该模型将语音段映射到一个空间，其中匹配的词有相似的向量。具体来说，我们使用一个在资源丰富的语言中训练的多语言AWE模型，在未见过的目标语言数据中检测关键词。与ASR相比，AWE方法仅需要少量的关键词样本。在Wolof和Swahili的控制实验中，由仅五分钟的数据训练的ASR模型优于AWE方法。但是在实际野外测试的Swahili广播中，使用一分钟的模板数据的AWE模型更加鲁棒，提供类似于30小时标记数据训练的ASR系统的性能。",
    "tldr": "本文研究了在低资源语言中进行仇恨言论检测的方法。在控制实验中，ASR模型表现更好，但在实际应用中，AWE模型表现更为鲁棒。",
    "en_tdlr": "This paper explores hate speech detection in low-resource languages and compares the effectiveness of ASR and AWE approaches. While ASR performs better in controlled experiments, AWE proves more robust in real-world tests."
}