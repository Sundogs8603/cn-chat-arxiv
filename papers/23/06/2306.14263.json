{
    "title": "Revolutionizing Cyber Threat Detection with Large Language Models: A privacy-preserving BERT-based Lightweight Model for IoT/IIoT Devices",
    "abstract": "The field of Natural Language Processing (NLP) is currently undergoing a revolutionary transformation driven by the power of pre-trained Large Language Models (LLMs) based on groundbreaking Transformer architectures. As the frequency and diversity of cybersecurity attacks continue to rise, the importance of incident detection has significantly increased. IoT devices are expanding rapidly, resulting in a growing need for efficient techniques to autonomously identify network-based attacks in IoT networks with both high precision and minimal computational requirements. This paper presents SecurityBERT, a novel architecture that leverages the Bidirectional Encoder Representations from Transformers (BERT) model for cyber threat detection in IoT networks. During the training of SecurityBERT, we incorporated a novel privacy-preserving encoding technique called Privacy-Preserving Fixed-Length Encoding (PPFLE). We effectively represented network traffic data in a structured format by combining ",
    "link": "https://arxiv.org/abs/2306.14263",
    "context": "Title: Revolutionizing Cyber Threat Detection with Large Language Models: A privacy-preserving BERT-based Lightweight Model for IoT/IIoT Devices\nAbstract: The field of Natural Language Processing (NLP) is currently undergoing a revolutionary transformation driven by the power of pre-trained Large Language Models (LLMs) based on groundbreaking Transformer architectures. As the frequency and diversity of cybersecurity attacks continue to rise, the importance of incident detection has significantly increased. IoT devices are expanding rapidly, resulting in a growing need for efficient techniques to autonomously identify network-based attacks in IoT networks with both high precision and minimal computational requirements. This paper presents SecurityBERT, a novel architecture that leverages the Bidirectional Encoder Representations from Transformers (BERT) model for cyber threat detection in IoT networks. During the training of SecurityBERT, we incorporated a novel privacy-preserving encoding technique called Privacy-Preserving Fixed-Length Encoding (PPFLE). We effectively represented network traffic data in a structured format by combining ",
    "path": "papers/23/06/2306.14263.json",
    "total_tokens": 896,
    "translated_title": "通过大规模语言模型革新网络威胁检测：一种面向物联网/工业物联网设备的隐私保护BERT轻量级模型",
    "translated_abstract": "自然语言处理领域（NLP）目前正在经历一场由基于革命性Transformer架构的预训练大型语言模型（LLM）驱动的革命性转变。随着网络安全攻击的频率和多样性持续增加，事件检测的重要性显著提高。物联网设备正在迅速扩展，因此需要高精度和最小计算要求的自主识别物联网中的基于网络的攻击的有效技术。本文提出了SecurityBERT，一种新颖的架构，利用双向编码器表示来自Transformer（BERT）模型，用于物联网网络中的网络威胁检测。在SecurityBERT的训练过程中，我们采用了一种名为隐私保护固定长度编码（PPFLE）的新型隐私保护编码技术。通过将网络流量数据以结构化格式进行有效表示，我们成功地将其与基于Transformer的语言模型相结合。",
    "tldr": "本文介绍了一种名为SecurityBERT的新型模型，采用预训练的BERT模型和隐私保护编码技术，用于在物联网网络中检测网络威胁。该模型能够高精度识别网络攻击，并具有最小的计算要求。",
    "en_tdlr": "This paper presents SecurityBERT, a novel model that utilizes pre-trained BERT model and privacy-preserving encoding technique for detecting network threats in IoT networks. The model achieves high accuracy in identifying network attacks with minimal computational requirements."
}