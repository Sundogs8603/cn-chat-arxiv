{
    "title": "Omega: Optimistic EMA Gradients. (arXiv:2306.07905v1 [cs.LG])",
    "abstract": "Stochastic min-max optimization has gained interest in the machine learning community with the advancements in GANs and adversarial training. Although game optimization is fairly well understood in the deterministic setting, some issues persist in the stochastic regime. Recent work has shown that stochastic gradient descent-ascent methods such as the optimistic gradient are highly sensitive to noise or can fail to converge. Although alternative strategies exist, they can be prohibitively expensive. We introduce Omega, a method with optimistic-like updates that mitigates the impact of noise by incorporating an EMA of historic gradients in its update rule. We also explore a variation of this algorithm that incorporates momentum. Although we do not provide convergence guarantees, our experiments on stochastic games show that Omega outperforms the optimistic gradient method when applied to linear players.",
    "link": "http://arxiv.org/abs/2306.07905",
    "context": "Title: Omega: Optimistic EMA Gradients. (arXiv:2306.07905v1 [cs.LG])\nAbstract: Stochastic min-max optimization has gained interest in the machine learning community with the advancements in GANs and adversarial training. Although game optimization is fairly well understood in the deterministic setting, some issues persist in the stochastic regime. Recent work has shown that stochastic gradient descent-ascent methods such as the optimistic gradient are highly sensitive to noise or can fail to converge. Although alternative strategies exist, they can be prohibitively expensive. We introduce Omega, a method with optimistic-like updates that mitigates the impact of noise by incorporating an EMA of historic gradients in its update rule. We also explore a variation of this algorithm that incorporates momentum. Although we do not provide convergence guarantees, our experiments on stochastic games show that Omega outperforms the optimistic gradient method when applied to linear players.",
    "path": "papers/23/06/2306.07905.json",
    "total_tokens": 798,
    "translated_title": "Omega: 乐观EMA Gradients",
    "translated_abstract": "随着GAN和对抗性训练的进步，随机min-max优化受到了机器学习界的关注。尽管确定性状态下的博弈优化已经相当好地理解了，但在随机状态下仍存在一些问题。最近的研究表明，像乐观梯度这样的随机梯度下降-上升方法对噪声非常敏感或者会导致失败。虽然存在替代策略，但这些策略可能成本过高。我们引入了Omega，一种具有类似于乐观更新的方法，通过在其更新规则中合并历史梯度的EMA来减轻噪声的影响。我们还探讨了一种包含动量的该算法的变体。虽然我们没有提供收敛性保证，但我们在随机游戏上的实验表明，当应用于线性玩家时，Omega优于乐观梯度方法。",
    "tldr": "Omega是一种优化算法，通过加入历史梯度EMA来减轻噪声的影响并在随机游戏上表现更佳。",
    "en_tdlr": "Omega is an optimization algorithm that mitigates the impact of noise by incorporating an EMA of historic gradients and outperforms the optimistic gradient method on stochastic games when applied to linear players."
}