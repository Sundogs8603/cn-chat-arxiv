{
    "title": "Continual Learning in Linear Classification on Separable Data. (arXiv:2306.03534v1 [cs.LG])",
    "abstract": "We analyze continual learning on a sequence of separable linear classification tasks with binary labels. We show theoretically that learning with weak regularization reduces to solving a sequential max-margin problem, corresponding to a special case of the Projection Onto Convex Sets (POCS) framework. We then develop upper bounds on the forgetting and other quantities of interest under various settings with recurring tasks, including cyclic and random orderings of tasks. We discuss several practical implications to popular training practices like regularization scheduling and weighting. We point out several theoretical differences between our continual classification setting and a recently studied continual regression setting.",
    "link": "http://arxiv.org/abs/2306.03534",
    "context": "Title: Continual Learning in Linear Classification on Separable Data. (arXiv:2306.03534v1 [cs.LG])\nAbstract: We analyze continual learning on a sequence of separable linear classification tasks with binary labels. We show theoretically that learning with weak regularization reduces to solving a sequential max-margin problem, corresponding to a special case of the Projection Onto Convex Sets (POCS) framework. We then develop upper bounds on the forgetting and other quantities of interest under various settings with recurring tasks, including cyclic and random orderings of tasks. We discuss several practical implications to popular training practices like regularization scheduling and weighting. We point out several theoretical differences between our continual classification setting and a recently studied continual regression setting.",
    "path": "papers/23/06/2306.03534.json",
    "total_tokens": 766,
    "translated_title": "分类模型在可分数据上的持续学习",
    "translated_abstract": "本文分析了在具有二元标签的可分线性分类任务序列上的持续学习。我们从理论上证明，弱正则化学习退化为解决连续最大间隔问题，对应于凸集投影框架的特殊情况。然后我们在不同设置下开发了遗忘和其他感兴趣量的上限，包括循环和随机排序的任务。我们讨论了对流行的训练方法（如正则化调度和加权）的几个实际影响。我们指出持续分类设置和最近研究的持续回归设置之间的几个理论差异。",
    "tldr": "本文从理论上探究了在可分数据上的线性分类持续学习，使得弱正则化学习退化为解决连续最大间隔问题；提出在循环和随机排序任务下的遗忘和其他量感兴趣的上限，并探讨了对训练方法的实际影响。",
    "en_tdlr": "This paper theoretically explores continual learning on separable linear classification tasks with binary labels, showing weak regularization reduces to solving a sequential max-margin problem, and proposing upper bounds on forgetting and other quantities under various settings with recurring tasks. The practical implications to popular training practices such as regularization scheduling and weighting are discussed. Several theoretical differences between continual classification and regression settings are pointed out."
}