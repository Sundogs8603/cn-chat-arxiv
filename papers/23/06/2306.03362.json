{
    "title": "Boosting Offline Reinforcement Learning with Action Preference Query. (arXiv:2306.03362v1 [cs.LG])",
    "abstract": "Training practical agents usually involve offline and online reinforcement learning (RL) to balance the policy's performance and interaction costs. In particular, online fine-tuning has become a commonly used method to correct the erroneous estimates of out-of-distribution data learned in the offline training phase. However, even limited online interactions can be inaccessible or catastrophic for high-stake scenarios like healthcare and autonomous driving. In this work, we introduce an interaction-free training scheme dubbed Offline-with-Action-Preferences (OAP). The main insight is that, compared to online fine-tuning, querying the preferences between pre-collected and learned actions can be equally or even more helpful to the erroneous estimate problem. By adaptively encouraging or suppressing policy constraint according to action preferences, OAP could distinguish overestimation from beneficial policy improvement and thus attains a more accurate evaluation of unseen data. Theoretica",
    "link": "http://arxiv.org/abs/2306.03362",
    "context": "Title: Boosting Offline Reinforcement Learning with Action Preference Query. (arXiv:2306.03362v1 [cs.LG])\nAbstract: Training practical agents usually involve offline and online reinforcement learning (RL) to balance the policy's performance and interaction costs. In particular, online fine-tuning has become a commonly used method to correct the erroneous estimates of out-of-distribution data learned in the offline training phase. However, even limited online interactions can be inaccessible or catastrophic for high-stake scenarios like healthcare and autonomous driving. In this work, we introduce an interaction-free training scheme dubbed Offline-with-Action-Preferences (OAP). The main insight is that, compared to online fine-tuning, querying the preferences between pre-collected and learned actions can be equally or even more helpful to the erroneous estimate problem. By adaptively encouraging or suppressing policy constraint according to action preferences, OAP could distinguish overestimation from beneficial policy improvement and thus attains a more accurate evaluation of unseen data. Theoretica",
    "path": "papers/23/06/2306.03362.json",
    "total_tokens": 789,
    "translated_title": "使用行为偏好查询提升离线强化学习",
    "translated_abstract": "训练实用代理通常涉及离线和在线强化学习以平衡政策的性能和交互成本。本文介绍了一种无需交互的训练方案 Offline-with-Action-Preferences（OAP）。 OAP的主要见解是，与在线微调相比，查询事先收集的和学习到的行为之间的偏好可以同样或甚至更有助于解决错误估计问题。通过根据行为偏好自适应地鼓励或抑制策略约束，OAP可以区分过度估计和有益的策略改进，从而获得对未见数据更精确的评估。",
    "tldr": "本文提出了一种名为Offline-with-Action-Preferences（OAP）的无交互训练方案，通过查询先前收集的和学习到的行动之间的偏好，来帮助解决错误估计问题，从而获得对未见数据更精确的评估。",
    "en_tdlr": "This paper proposes an interaction-free training scheme called Offline-with-Action-Preferences (OAP), which can help solve the problem of erroneous estimates by querying the preferences between pre-collected and learned actions, and thus obtains a more accurate evaluation of unseen data."
}