{
    "title": "Noise Stability Optimization for Flat Minima with Optimal Convergence Rates. (arXiv:2306.08553v1 [cs.LG])",
    "abstract": "We consider finding flat, local minimizers by adding average weight perturbations. Given a nonconvex function $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ and a $d$-dimensional distribution $\\mathcal{P}$ which is symmetric at zero, we perturb the weight of $f$ and define $F(W) = \\mathbb{E}[f({W + U})]$, where $U$ is a random sample from $\\mathcal{P}$. This injection induces regularization through the Hessian trace of $f$ for small, isotropic Gaussian perturbations. Thus, the weight-perturbed function biases to minimizers with low Hessian trace. Several prior works have studied settings related to this weight-perturbed function by designing algorithms to improve generalization. Still, convergence rates are not known for finding minima under the average perturbations of the function $F$. This paper considers an SGD-like algorithm that injects random noise before computing gradients while leveraging the symmetry of $\\mathcal{P}$ to reduce variance. We then provide a rigorous analysis, showing",
    "link": "http://arxiv.org/abs/2306.08553",
    "context": "Title: Noise Stability Optimization for Flat Minima with Optimal Convergence Rates. (arXiv:2306.08553v1 [cs.LG])\nAbstract: We consider finding flat, local minimizers by adding average weight perturbations. Given a nonconvex function $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ and a $d$-dimensional distribution $\\mathcal{P}$ which is symmetric at zero, we perturb the weight of $f$ and define $F(W) = \\mathbb{E}[f({W + U})]$, where $U$ is a random sample from $\\mathcal{P}$. This injection induces regularization through the Hessian trace of $f$ for small, isotropic Gaussian perturbations. Thus, the weight-perturbed function biases to minimizers with low Hessian trace. Several prior works have studied settings related to this weight-perturbed function by designing algorithms to improve generalization. Still, convergence rates are not known for finding minima under the average perturbations of the function $F$. This paper considers an SGD-like algorithm that injects random noise before computing gradients while leveraging the symmetry of $\\mathcal{P}$ to reduce variance. We then provide a rigorous analysis, showing",
    "path": "papers/23/06/2306.08553.json",
    "total_tokens": 926,
    "translated_title": "噪声稳定优化对于具有最优收敛率的平坦极小值的影响",
    "translated_abstract": "本文研究通过加入加权扰动来找到平坦的极小值。给定一个非凸函数$f:\\mathbb{R}^d\\rightarrow \\mathbb{R}$和一个$d$维分布$\\mathcal{P}$，我们扰动$f$的权重，并定义$F(W)=\\mathbb{E}[f({W+U})]$，其中$U$是一个从$\\mathcal{P}$中随机抽取的样本。这个过程通过$f$的海森矩阵的迹来诱导正则化，以适应于小的、各向同性的高斯扰动。因此，加权扰动的函数偏向于带有低海森矩阵迹的极小值。本文提出了一种类似于SGD的算法，在计算梯度之前注入随机噪声，同时利用$\\mathcal{P}$的对称性来减少方差。我们还提供了严格的分析，证明了...",
    "tldr": "本文提出了一个SGD-like算法，注入随机噪声并利用分布对称性来减少方差，以寻找具有低海森矩阵迹的平坦极小值，同时提供了收敛速率分析。"
}