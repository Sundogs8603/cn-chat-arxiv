{
    "title": "Boosting Adversarial Transferability by Achieving Flat Local Maxima. (arXiv:2306.05225v1 [cs.CV])",
    "abstract": "Transfer-based attack adopts the adversarial examples generated on the surrogate model to attack various models, making it applicable in the physical world and attracting increasing interest. Recently, various adversarial attacks have emerged to boost adversarial transferability from different perspectives. In this work, inspired by the fact that flat local minima are correlated with good generalization, we assume and empirically validate that adversarial examples at a flat local region tend to have good transferability by introducing a penalized gradient norm to the original loss function. Since directly optimizing the gradient regularization norm is computationally expensive and intractable for generating adversarial examples, we propose an approximation optimization method to simplify the gradient update of the objective function. Specifically, we randomly sample an example and adopt the first-order gradient to approximate the second-order Hessian matrix, which makes computing more ",
    "link": "http://arxiv.org/abs/2306.05225",
    "context": "Title: Boosting Adversarial Transferability by Achieving Flat Local Maxima. (arXiv:2306.05225v1 [cs.CV])\nAbstract: Transfer-based attack adopts the adversarial examples generated on the surrogate model to attack various models, making it applicable in the physical world and attracting increasing interest. Recently, various adversarial attacks have emerged to boost adversarial transferability from different perspectives. In this work, inspired by the fact that flat local minima are correlated with good generalization, we assume and empirically validate that adversarial examples at a flat local region tend to have good transferability by introducing a penalized gradient norm to the original loss function. Since directly optimizing the gradient regularization norm is computationally expensive and intractable for generating adversarial examples, we propose an approximation optimization method to simplify the gradient update of the objective function. Specifically, we randomly sample an example and adopt the first-order gradient to approximate the second-order Hessian matrix, which makes computing more ",
    "path": "papers/23/06/2306.05225.json",
    "total_tokens": 820,
    "translated_title": "实现平坦局部极小值以提高对抗性转移的效果",
    "translated_abstract": "基于转移的攻击采用在替代模型上生成的对抗性示例来攻击各种模型，在物理世界中适用并吸引了越来越多的关注。在这项工作中，受到平坦局部极小值与良好的泛化之间的相关性启发，我们假设并经验性地验证了，在平坦局部区域的对抗性示例倾向于具有良好的转移能力，通过在原始损失函数中引入惩罚梯度范数。由于直接优化梯度正则化范数在生成对抗性示例时计算代价昂贵且难以处理，因此我们提出了一种近似优化方法，以简化目标函数的梯度更新。我们随机采样一个示例，并采用一阶梯度来逼近二阶黑塞矩阵，从而使计算更加容易。",
    "tldr": "本文提出了一种近似优化方法来实现平坦局部极小值，该方法可以提高对抗性转移能力，并在实验中证实了该方法的有效性。",
    "en_tdlr": "This paper proposes an approximation optimization method to achieve flat local minima, which can enhance adversarial transferability and the effectiveness of attack. The experimental results validate the proposed method."
}