{
    "title": "Factually Consistent Summarization via Reinforcement Learning with Textual Entailment Feedback. (arXiv:2306.00186v1 [cs.CL])",
    "abstract": "Despite the seeming success of contemporary grounded text generation systems, they often tend to generate factually inconsistent text with respect to their input. This phenomenon is emphasized in tasks like summarization, in which the generated summaries should be corroborated by their source article. In this work, we leverage recent progress on textual entailment models to directly address this problem for abstractive summarization systems. We use reinforcement learning with reference-free, textual entailment rewards to optimize for factual consistency and explore the ensuing trade-offs, as improved consistency may come at the cost of less informative or more extractive summaries. Our results, according to both automatic metrics and human evaluation, show that our method considerably improves the faithfulness, salience, and conciseness of the generated summaries.",
    "link": "http://arxiv.org/abs/2306.00186",
    "context": "Title: Factually Consistent Summarization via Reinforcement Learning with Textual Entailment Feedback. (arXiv:2306.00186v1 [cs.CL])\nAbstract: Despite the seeming success of contemporary grounded text generation systems, they often tend to generate factually inconsistent text with respect to their input. This phenomenon is emphasized in tasks like summarization, in which the generated summaries should be corroborated by their source article. In this work, we leverage recent progress on textual entailment models to directly address this problem for abstractive summarization systems. We use reinforcement learning with reference-free, textual entailment rewards to optimize for factual consistency and explore the ensuing trade-offs, as improved consistency may come at the cost of less informative or more extractive summaries. Our results, according to both automatic metrics and human evaluation, show that our method considerably improves the faithfulness, salience, and conciseness of the generated summaries.",
    "path": "papers/23/06/2306.00186.json",
    "total_tokens": 806,
    "translated_title": "通过使用文本蕴含反馈的强化学习实现事实一致性摘要",
    "translated_abstract": "尽管当代的文本生成系统表现出表面上的成功，但它们往往会生成与其输入不相符的错误文本。这种现象在摘要等任务中更加突出，因为生成的摘要应该得到其来源文章的证实。本研究利用最近在文本蕴含模型上的进展来直接解决抽象摘要系统中的这个问题。我们使用强化学习与无需参考的文本蕴含奖励来优化事实的一致性，并探索随之而来的权衡，因为改进的一致性可能会以摘要的信息量少或更提取为代价。根据自动指标和人类评估，我们的结果显示，我们的方法显着提高了生成的摘要的忠实度、明显性和简洁性。",
    "tldr": "本研究使用带有文本蕴含奖励的强化学习来实现抽象摘要系统的事实一致性，从而提高了生成的摘要的忠实度、明显性和简洁性。",
    "en_tdlr": "This study uses reinforcement learning with textual entailment rewards to achieve factually consistent abstractive summarization and as a result, improves the faithfulness, salience, and conciseness of the generated summaries."
}