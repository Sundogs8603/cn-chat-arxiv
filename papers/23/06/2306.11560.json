{
    "title": "MILD: Modeling the Instance Learning Dynamics for Learning with Noisy Labels. (arXiv:2306.11560v2 [cs.LG] UPDATED)",
    "abstract": "Despite deep learning has achieved great success, it often relies on a large amount of training data with accurate labels, which are expensive and time-consuming to collect. A prominent direction to reduce the cost is to learn with noisy labels, which are ubiquitous in the real-world applications. A critical challenge for such a learning task is to reduce the effect of network memorization on the falsely-labeled data. In this work, we propose an iterative selection approach based on the Weibull mixture model, which identifies clean data by considering the overall learning dynamics of each data instance. In contrast to the previous small-loss heuristics, we leverage the observation that deep network is easy to memorize and hard to forget clean data. In particular, we measure the difficulty of memorization and forgetting for each instance via the transition times between being misclassified and being memorized in training, and integrate them into a novel metric for selection. Based on th",
    "link": "http://arxiv.org/abs/2306.11560",
    "context": "Title: MILD: Modeling the Instance Learning Dynamics for Learning with Noisy Labels. (arXiv:2306.11560v2 [cs.LG] UPDATED)\nAbstract: Despite deep learning has achieved great success, it often relies on a large amount of training data with accurate labels, which are expensive and time-consuming to collect. A prominent direction to reduce the cost is to learn with noisy labels, which are ubiquitous in the real-world applications. A critical challenge for such a learning task is to reduce the effect of network memorization on the falsely-labeled data. In this work, we propose an iterative selection approach based on the Weibull mixture model, which identifies clean data by considering the overall learning dynamics of each data instance. In contrast to the previous small-loss heuristics, we leverage the observation that deep network is easy to memorize and hard to forget clean data. In particular, we measure the difficulty of memorization and forgetting for each instance via the transition times between being misclassified and being memorized in training, and integrate them into a novel metric for selection. Based on th",
    "path": "papers/23/06/2306.11560.json",
    "total_tokens": 891,
    "translated_title": "MILD: 模型化学习动态，用于学习带有噪声标签的数据",
    "translated_abstract": "尽管深度学习取得了巨大的成功，但它通常依赖于大量带有准确标签的训练数据，而这些数据的收集成本高且耗时。降低成本的一个重要方向是学习带噪声标签的数据，这在现实世界的应用中普遍存在。对于这样的学习任务，一个关键挑战是减少网络对错误标签数据的记忆效应。在这项工作中，我们提出了一种基于Weibull混合模型的迭代选择方法，通过考虑每个数据实例的整体学习动态来识别干净的数据。与先前的小损失启发式方法不同，我们利用了深度网络容易记忆和难以忘记干净数据的观察结果。特别地，我们通过训练期间被错误分类和被记忆之间的转换次数来测量每个实例的记忆和遗忘难度，并将它们整合到一个新的选择指标中。",
    "tldr": "MILD模型化了学习动态，通过基于Weibull混合模型的迭代选择方法，识别干净的数据实例，以减少网络对带有噪声标签的数据的记忆影响。",
    "en_tdlr": "MILD models the learning dynamics by proposing an iterative selection approach based on the Weibull mixture model to identify clean instances, reducing network memorization of data with noisy labels."
}