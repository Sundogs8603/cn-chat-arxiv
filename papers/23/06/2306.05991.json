{
    "title": "Approximate information state based convergence analysis of recurrent Q-learning. (arXiv:2306.05991v1 [cs.LG])",
    "abstract": "In spite of the large literature on reinforcement learning (RL) algorithms for partially observable Markov decision processes (POMDPs), a complete theoretical understanding is still lacking. In a partially observable setting, the history of data available to the agent increases over time so most practical algorithms either truncate the history to a finite window or compress it using a recurrent neural network leading to an agent state that is non-Markovian. In this paper, it is shown that in spite of the lack of the Markov property, recurrent Q-learning (RQL) converges in the tabular setting. Moreover, it is shown that the quality of the converged limit depends on the quality of the representation which is quantified in terms of what is known as an approximate information state (AIS). Based on this characterization of the approximation error, a variant of RQL with AIS losses is presented. This variant performs better than a strong baseline for RQL that does not use AIS losses. It is de",
    "link": "http://arxiv.org/abs/2306.05991",
    "context": "Title: Approximate information state based convergence analysis of recurrent Q-learning. (arXiv:2306.05991v1 [cs.LG])\nAbstract: In spite of the large literature on reinforcement learning (RL) algorithms for partially observable Markov decision processes (POMDPs), a complete theoretical understanding is still lacking. In a partially observable setting, the history of data available to the agent increases over time so most practical algorithms either truncate the history to a finite window or compress it using a recurrent neural network leading to an agent state that is non-Markovian. In this paper, it is shown that in spite of the lack of the Markov property, recurrent Q-learning (RQL) converges in the tabular setting. Moreover, it is shown that the quality of the converged limit depends on the quality of the representation which is quantified in terms of what is known as an approximate information state (AIS). Based on this characterization of the approximation error, a variant of RQL with AIS losses is presented. This variant performs better than a strong baseline for RQL that does not use AIS losses. It is de",
    "path": "papers/23/06/2306.05991.json",
    "total_tokens": 881,
    "translated_title": "基于近似信息状态的循环Q-learning收敛性分析",
    "translated_abstract": "尽管有很多针对部分可观测的马尔可夫决策过程的强化学习算法的文献，但仍缺乏完整的理论理解。在部分可观测的情况下，代理可用的数据历史会随着时间的推移而增加，因此大多数实用算法要么将历史截断到有限的窗口，要么使用递归神经网络进行压缩，导致代理状态不是马尔可夫的。本文证明了尽管缺乏马尔可夫性质，循环Q-learning（RQL）在表格设置中也会收敛。此外，本文还证明了收敛极限的质量取决于表示形式的质量，这是使用近似信息状态（AIS）来量化的。基于这种近似误差的表征，提出了一种带有AIS损失的RQL变体。与不使用AIS损失的RQL强基线相比，这种变体表现更好。",
    "tldr": "本文研究了循环Q-learning在部分可观测环境下的收敛性，并提出了一种使用近似信息状态的变体算法，该算法在表现上优于强基线。",
    "en_tdlr": "This paper studies the convergence of recurrent Q-learning in partially observable environments and proposes a variant algorithm using approximate information state, which outperforms the strong baseline."
}