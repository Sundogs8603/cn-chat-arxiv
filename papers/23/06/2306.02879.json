{
    "title": "Neuron Activation Coverage: Rethinking Out-of-distribution Detection and Generalization. (arXiv:2306.02879v2 [cs.LG] UPDATED)",
    "abstract": "The out-of-distribution (OOD) problem generally arises when neural networks encounter data that significantly deviates from the training data distribution, i.e., in-distribution (InD). In this paper, we study the OOD problem from a neuron activation view. We first formulate neuron activation states by considering both the neuron output and its influence on model decisions. Then, to characterize the relationship between neurons and OOD issues, we introduce the \\textit{neuron activation coverage} (NAC) -- a simple measure for neuron behaviors under InD data. Leveraging our NAC, we show that 1) InD and OOD inputs can be largely separated based on the neuron behavior, which significantly eases the OOD detection problem and beats the 21 previous methods over three benchmarks (CIFAR-10, CIFAR-100, and ImageNet-1K). 2) a positive correlation between NAC and model generalization ability consistently holds across architectures and datasets, which enables a NAC-based criterion for evaluating mod",
    "link": "http://arxiv.org/abs/2306.02879",
    "context": "Title: Neuron Activation Coverage: Rethinking Out-of-distribution Detection and Generalization. (arXiv:2306.02879v2 [cs.LG] UPDATED)\nAbstract: The out-of-distribution (OOD) problem generally arises when neural networks encounter data that significantly deviates from the training data distribution, i.e., in-distribution (InD). In this paper, we study the OOD problem from a neuron activation view. We first formulate neuron activation states by considering both the neuron output and its influence on model decisions. Then, to characterize the relationship between neurons and OOD issues, we introduce the \\textit{neuron activation coverage} (NAC) -- a simple measure for neuron behaviors under InD data. Leveraging our NAC, we show that 1) InD and OOD inputs can be largely separated based on the neuron behavior, which significantly eases the OOD detection problem and beats the 21 previous methods over three benchmarks (CIFAR-10, CIFAR-100, and ImageNet-1K). 2) a positive correlation between NAC and model generalization ability consistently holds across architectures and datasets, which enables a NAC-based criterion for evaluating mod",
    "path": "papers/23/06/2306.02879.json",
    "total_tokens": 1007,
    "translated_title": "神经元激活覆盖度：重新思考离域检测和泛化问题",
    "translated_abstract": "离域问题通常在神经网络遇到明显偏离训练数据分布的数据时出现，即在域内数据（InD）之外。本文从神经元激活的角度研究了离域问题。我们首先通过考虑神经元的输出和其对模型决策的影响来定义了神经元激活状态。然后，为了描述神经元与离域问题的关系，我们引入了神经元激活覆盖度（NAC）——一种衡量神经元在域内数据下行为的简单度量。利用我们的NAC，我们展示了：1）基于神经元行为可以在很大程度上区分域内和离域输入，大大简化了离域检测问题，并在三个基准测试集（CIFAR-10、CIFAR-100和ImageNet-1K）上超过了21个先前的方法；2）NAC与模型的泛化能力之间存在正相关关系，这种关系在不同架构和数据集上一致成立，使得基于NAC的准则可以用于评估模型的泛化能力。",
    "tldr": "本文通过研究神经元激活状态，提出了神经元激活覆盖度（NAC）作为衡量神经元行为的指标。利用NAC可以有效区分域内和离域输入，简化离域检测问题，并且NAC与模型的泛化能力间存在正相关关系。"
}