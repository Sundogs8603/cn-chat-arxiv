{
    "title": "Training generative models from privatized data. (arXiv:2306.09547v1 [cs.LG])",
    "abstract": "Local differential privacy (LDP) is a powerful method for privacy-preserving data collection. In this paper, we develop a framework for training Generative Adversarial Networks (GAN) on differentially privatized data. We show that entropic regularization of the Wasserstein distance -- a popular regularization method in the literature that has been often leveraged for its computational benefits -- can be used to denoise the data distribution when data is privatized by common additive noise mechanisms, such as Laplace and Gaussian. This combination uniquely enables the mitigation of both the regularization bias and the effects of privatization noise, thereby enhancing the overall efficacy of the model. We analyse the proposed method, provide sample complexity results and experimental evidence to support its efficacy.",
    "link": "http://arxiv.org/abs/2306.09547",
    "context": "Title: Training generative models from privatized data. (arXiv:2306.09547v1 [cs.LG])\nAbstract: Local differential privacy (LDP) is a powerful method for privacy-preserving data collection. In this paper, we develop a framework for training Generative Adversarial Networks (GAN) on differentially privatized data. We show that entropic regularization of the Wasserstein distance -- a popular regularization method in the literature that has been often leveraged for its computational benefits -- can be used to denoise the data distribution when data is privatized by common additive noise mechanisms, such as Laplace and Gaussian. This combination uniquely enables the mitigation of both the regularization bias and the effects of privatization noise, thereby enhancing the overall efficacy of the model. We analyse the proposed method, provide sample complexity results and experimental evidence to support its efficacy.",
    "path": "papers/23/06/2306.09547.json",
    "total_tokens": 742,
    "translated_title": "从隐私化数据中训练生成模型",
    "translated_abstract": "本文介绍了一种在差分隐私化数据上训练生成式对抗网络（GAN）的框架，其中采用了常见的加性噪声机制（如拉普拉斯噪声和高斯噪声）对数据进行差分隐私保护。我们展示了当使用熵正则化Wasserstein距离来去噪数据分布时，这种方法可以唯一地缓解正则化偏差和隐私化噪声的影响，从而提高模型的有效性。同时，我们还分析了所提出的方法并提供了样本复杂度结果和实验证据以支持其可靠性。",
    "tldr": "介绍了一种在隐私化数据上训练GAN的框架，使用熵正则化Wasserstein距离去噪可以缓解正则化偏差和隐私化噪声的影响，提高模型有效性。",
    "en_tdlr": "This paper proposes a framework for training GAN on differentially privatized data. The method involves the use of entropic regularization of Wasserstein distance to denoise data distribution affected by additive noise mechanisms such as Laplace and Gaussian, resulting in enhanced model efficacy."
}