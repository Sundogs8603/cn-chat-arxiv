{
    "title": "Achieving Fairness in Multi-Agent Markov Decision Processes Using Reinforcement Learning. (arXiv:2306.00324v1 [cs.LG])",
    "abstract": "Fairness plays a crucial role in various multi-agent systems (e.g., communication networks, financial markets, etc.). Many multi-agent dynamical interactions can be cast as Markov Decision Processes (MDPs). While existing research has focused on studying fairness in known environments, the exploration of fairness in such systems for unknown environments remains open. In this paper, we propose a Reinforcement Learning (RL) approach to achieve fairness in multi-agent finite-horizon episodic MDPs. Instead of maximizing the sum of individual agents' value functions, we introduce a fairness function that ensures equitable rewards across agents. Since the classical Bellman's equation does not hold when the sum of individual value functions is not maximized, we cannot use traditional approaches. Instead, in order to explore, we maintain a confidence bound of the unknown environment and then propose an online convex optimization based approach to obtain a policy constrained to this confidence ",
    "link": "http://arxiv.org/abs/2306.00324",
    "context": "Title: Achieving Fairness in Multi-Agent Markov Decision Processes Using Reinforcement Learning. (arXiv:2306.00324v1 [cs.LG])\nAbstract: Fairness plays a crucial role in various multi-agent systems (e.g., communication networks, financial markets, etc.). Many multi-agent dynamical interactions can be cast as Markov Decision Processes (MDPs). While existing research has focused on studying fairness in known environments, the exploration of fairness in such systems for unknown environments remains open. In this paper, we propose a Reinforcement Learning (RL) approach to achieve fairness in multi-agent finite-horizon episodic MDPs. Instead of maximizing the sum of individual agents' value functions, we introduce a fairness function that ensures equitable rewards across agents. Since the classical Bellman's equation does not hold when the sum of individual value functions is not maximized, we cannot use traditional approaches. Instead, in order to explore, we maintain a confidence bound of the unknown environment and then propose an online convex optimization based approach to obtain a policy constrained to this confidence ",
    "path": "papers/23/06/2306.00324.json",
    "total_tokens": 938,
    "translated_title": "利用强化学习在多智能体马尔可夫决策过程中实现公平性",
    "translated_abstract": "公平性在各种多智能体系统（例如通信网络、金融市场等）中都扮演着至关重要的角色。许多多智能体动态交互可以被视为马尔可夫决策过程（MDPs）。尽管现有研究专注于研究已知环境中的公平性，但是在未知环境中探索这种系统中的公平性仍然是开放的问题。在本文中，我们提出了一种强化学习（RL）方法，以实现多智能体有限时间段情节MDPs中的公平性。我们引入了一个公平函数，它确保代理之间的奖励公平，而不是最大化各个代理的值函数之和。由于当不最大化单个值函数之和时，经典的Bellman方程不再成立，因此我们不能使用传统的方法。相反，为了进行探索，我们维护未知环境的置信度界限，然后提出了一种在线凸优化方法来获得受限于此置信范围的策略。",
    "tldr": "本文利用强化学习方法在多智能体马尔可夫决策过程中实现公平性，引入公平函数来确保代理之间的奖励公平，并提出一种基于在线凸优化的方法获得策略。",
    "en_tdlr": "This paper proposes a reinforcement learning approach to achieve fairness in multi-agent Markov Decision Processes by introducing a fairness function that ensures equitable rewards across agents, and presents an online convex optimization-based method to obtain a policy constrained to a confidence bound of unknown environment."
}