{
    "title": "TACO: Temporal Latent Action-Driven Contrastive Loss for Visual Reinforcement Learning. (arXiv:2306.13229v1 [cs.LG])",
    "abstract": "Despite recent progress in reinforcement learning (RL) from raw pixel data, sample inefficiency continues to present a substantial obstacle. Prior works have attempted to address this challenge by creating self-supervised auxiliary tasks, aiming to enrich the agent's learned representations with control-relevant information for future state prediction. However, these objectives are often insufficient to learn representations that can represent the optimal policy or value function, and they often consider tasks with small, abstract discrete action spaces and thus overlook the importance of action representation learning in continuous control. In this paper, we introduce TACO: Temporal Action-driven Contrastive Learning, a simple yet powerful temporal contrastive learning approach that facilitates the concurrent acquisition of latent state and action representations for agents. TACO simultaneously learns a state and an action representation by optimizing the mutual information between re",
    "link": "http://arxiv.org/abs/2306.13229",
    "context": "Title: TACO: Temporal Latent Action-Driven Contrastive Loss for Visual Reinforcement Learning. (arXiv:2306.13229v1 [cs.LG])\nAbstract: Despite recent progress in reinforcement learning (RL) from raw pixel data, sample inefficiency continues to present a substantial obstacle. Prior works have attempted to address this challenge by creating self-supervised auxiliary tasks, aiming to enrich the agent's learned representations with control-relevant information for future state prediction. However, these objectives are often insufficient to learn representations that can represent the optimal policy or value function, and they often consider tasks with small, abstract discrete action spaces and thus overlook the importance of action representation learning in continuous control. In this paper, we introduce TACO: Temporal Action-driven Contrastive Learning, a simple yet powerful temporal contrastive learning approach that facilitates the concurrent acquisition of latent state and action representations for agents. TACO simultaneously learns a state and an action representation by optimizing the mutual information between re",
    "path": "papers/23/06/2306.13229.json",
    "total_tokens": 876,
    "translated_title": "TACO：基于时间潜在动作驱动对比损失的视觉强化学习",
    "translated_abstract": "尽管在强化学习（RL）从原始像素数据中取得了最近的进展，但样本效率仍然是一个重要的障碍。先前的工作试图通过创建自监督辅助任务来解决这个挑战，旨在为未来状态预测丰富代理学习的表示与控制相关信息。然而，这些目标通常不足以学习能够表示最优策略或值函数的表示，并且它们通常考虑具有小的抽象离散动作空间的任务，因此忽视了在连续控制中动作表示学习的重要性。在本文中，我们引入了TACO：一种简单而强大的时间对比学习方法，利用它，代理可以同时获得潜在状态和动作表示。TACO通过优化重新获得观察与最近的多个先前观察的相似性，同时学习状态与动作表示。",
    "tldr": "本文提出了TACO方法，一种基于时间潜在动作驱动对比损失的视觉强化学习方法，能够同时学习状态表示和动作表示，提高代理学习的效率。"
}