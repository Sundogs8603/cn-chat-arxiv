{
    "title": "Retrieving Supporting Evidence for LLMs Generated Answers. (arXiv:2306.13781v1 [cs.IR])",
    "abstract": "Current large language models (LLMs) can exhibit near-human levels of performance on many natural language tasks, including open-domain question answering. Unfortunately, they also convincingly hallucinate incorrect answers, so that responses to questions must be verified against external sources before they can be accepted at face value. In this paper, we report a simple experiment to automatically verify generated answers against a corpus. After presenting a question to an LLM and receiving a generated answer, we query the corpus with the combination of the question + generated answer. We then present the LLM with the combination of the question + generated answer + retrieved answer, prompting it to indicate if the generated answer can be supported by the retrieved answer. We base our experiment on questions and passages from the MS MARCO (V1) test collection, exploring three retrieval approaches ranging from standard BM25 to a full question answering stack, including a reader based ",
    "link": "http://arxiv.org/abs/2306.13781",
    "context": "Title: Retrieving Supporting Evidence for LLMs Generated Answers. (arXiv:2306.13781v1 [cs.IR])\nAbstract: Current large language models (LLMs) can exhibit near-human levels of performance on many natural language tasks, including open-domain question answering. Unfortunately, they also convincingly hallucinate incorrect answers, so that responses to questions must be verified against external sources before they can be accepted at face value. In this paper, we report a simple experiment to automatically verify generated answers against a corpus. After presenting a question to an LLM and receiving a generated answer, we query the corpus with the combination of the question + generated answer. We then present the LLM with the combination of the question + generated answer + retrieved answer, prompting it to indicate if the generated answer can be supported by the retrieved answer. We base our experiment on questions and passages from the MS MARCO (V1) test collection, exploring three retrieval approaches ranging from standard BM25 to a full question answering stack, including a reader based ",
    "path": "papers/23/06/2306.13781.json",
    "total_tokens": 826,
    "translated_title": "获取LLMs生成答案的支持证据",
    "translated_abstract": "目前的大型语言模型（LLM）在许多自然语言任务，包括开放域问答方面都表现出接近人类水平的性能。不幸的是，它们也会诱导出不正确的答案，因此在接受回答之前必须对其进行验证。在本文中，我们报告了一个简单的实验，以自动验证生成的答案是否与语料库匹配。我们将问题展示给 LLM 并获得生成的答案后，我们使用问题 + 生成的答案这一组合在语料库中查询。然后我们向 LLM 提供问题 + 生成的答案 + 检索到的答案这一组合，促使其指示生成的答案是否可以得到支持。我们的实验基于 MS MARCO (V1) 测试集中的问题和段落，探索了三种检索方法，从标准 BM25到完整的问答堆栈，包括基于阅读的方法。",
    "tldr": "本文提出了通过检索外部语料库获取LLMs生成答案的支持证据的实验方法，以解决LLMs容易产生错误答案的问题。",
    "en_tdlr": "This paper proposes an experimental method of retrieving supporting evidence for LLMs generated answers from external corpus to solve the problem of the LLMs' tendency to generate incorrect answers."
}