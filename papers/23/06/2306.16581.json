{
    "title": "Does Saliency-Based Training bring Robustness for Deep Neural Networks in Image Classification?. (arXiv:2306.16581v1 [cs.CV])",
    "abstract": "Deep Neural Networks are powerful tools to understand complex patterns and making decisions. However, their black-box nature impedes a complete understanding of their inner workings. While online saliency-guided training methods try to highlight the prominent features in the model's output to alleviate this problem, it is still ambiguous if the visually explainable features align with robustness of the model against adversarial examples. In this paper, we investigate the saliency trained model's vulnerability to adversarial examples methods. Models are trained using an online saliency-guided training method and evaluated against popular algorithms of adversarial examples. We quantify the robustness and conclude that despite the well-explained visualizations in the model's output, the salient models suffer from the lower performance against adversarial examples attacks.",
    "link": "http://arxiv.org/abs/2306.16581",
    "context": "Title: Does Saliency-Based Training bring Robustness for Deep Neural Networks in Image Classification?. (arXiv:2306.16581v1 [cs.CV])\nAbstract: Deep Neural Networks are powerful tools to understand complex patterns and making decisions. However, their black-box nature impedes a complete understanding of their inner workings. While online saliency-guided training methods try to highlight the prominent features in the model's output to alleviate this problem, it is still ambiguous if the visually explainable features align with robustness of the model against adversarial examples. In this paper, we investigate the saliency trained model's vulnerability to adversarial examples methods. Models are trained using an online saliency-guided training method and evaluated against popular algorithms of adversarial examples. We quantify the robustness and conclude that despite the well-explained visualizations in the model's output, the salient models suffer from the lower performance against adversarial examples attacks.",
    "path": "papers/23/06/2306.16581.json",
    "total_tokens": 815,
    "translated_title": "通过显著性训练为图像分类中的深度神经网络带来鲁棒性吗？",
    "translated_abstract": "深度神经网络是理解复杂模式和做出决策的强大工具。然而，它们的黑箱特性阻碍了对其内部工作的完全理解。虽然在线显著性引导训练方法试图通过突出显示模型输出中的明显特征来缓解这个问题，但显然非常规的特征与模型对抗性示例的鲁棒性是否对齐仍不清楚。本文研究了经过显著性训练的模型对抗性示例方法的脆弱性。采用在线显著性引导训练方法训练模型，并针对常见的对抗性示例算法进行评估。我们量化了鲁棒性，并得出结论：尽管模型输出中有良好解释的可视化效果，但显著性训练的模型在对抗性示例攻击中的性能较低。",
    "tldr": "通过对深度神经网络进行显著性训练无法提高其在对抗性示例攻击下的鲁棒性。"
}