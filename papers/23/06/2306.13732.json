{
    "title": "Reinforcement Learning with Temporal-Logic-Based Causal Diagrams. (arXiv:2306.13732v1 [cs.AI])",
    "abstract": "We study a class of reinforcement learning (RL) tasks where the objective of the agent is to accomplish temporally extended goals. In this setting, a common approach is to represent the tasks as deterministic finite automata (DFA) and integrate them into the state-space for RL algorithms. However, while these machines model the reward function, they often overlook the causal knowledge about the environment. To address this limitation, we propose the Temporal-Logic-based Causal Diagram (TL-CD) in RL, which captures the temporal causal relationships between different properties of the environment. We exploit the TL-CD to devise an RL algorithm in which an agent requires significantly less exploration of the environment. To this end, based on a TL-CD and a task DFA, we identify configurations where the agent can determine the expected rewards early during an exploration. Through a series of case studies, we demonstrate the benefits of using TL-CDs, particularly the faster convergence of t",
    "link": "http://arxiv.org/abs/2306.13732",
    "context": "Title: Reinforcement Learning with Temporal-Logic-Based Causal Diagrams. (arXiv:2306.13732v1 [cs.AI])\nAbstract: We study a class of reinforcement learning (RL) tasks where the objective of the agent is to accomplish temporally extended goals. In this setting, a common approach is to represent the tasks as deterministic finite automata (DFA) and integrate them into the state-space for RL algorithms. However, while these machines model the reward function, they often overlook the causal knowledge about the environment. To address this limitation, we propose the Temporal-Logic-based Causal Diagram (TL-CD) in RL, which captures the temporal causal relationships between different properties of the environment. We exploit the TL-CD to devise an RL algorithm in which an agent requires significantly less exploration of the environment. To this end, based on a TL-CD and a task DFA, we identify configurations where the agent can determine the expected rewards early during an exploration. Through a series of case studies, we demonstrate the benefits of using TL-CDs, particularly the faster convergence of t",
    "path": "papers/23/06/2306.13732.json",
    "total_tokens": 888,
    "translated_abstract": "我们研究了一类强化学习任务，在这些任务中，智能体的目标是完成时间上延续的目标。在这种情况下，一种常见的方法是将任务表示为确定性有限自动机(DFA)，并将其集成到强化学习算法的状态空间中。然而，尽管这些机器模型成本函数，它们经常忽略环境的因果知识。为了解决这个限制，我们在强化学习中提出了基于时序逻辑的因果图(TL-CD)，它捕捉了环境中不同属性之间的时间因果关系。我们利用TL-CD设计了一种强化学习算法，使得代理人对环境的探索显著减少。为此，基于TL-CD和任务DFA，我们确定了代理人可以在探索早期确定预期回报的配置。通过一系列案例研究，我们证明了使用TL-CD的好处，特别是更快速的收敛。",
    "tldr": "本文章提出了一种基于时序逻辑因果图的强化学习算法，该算法能够帮助智能体在探索环境时，更快地确定预期回报，从而使其训练更加迅速和高效。",
    "en_tdlr": "This paper proposes a reinforcement learning algorithm based on temporal-logic-based causal diagrams, which helps agents determine expected rewards faster during exploration, leading to faster and more efficient training."
}