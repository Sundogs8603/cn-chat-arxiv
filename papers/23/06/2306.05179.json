{
    "title": "M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models. (arXiv:2306.05179v1 [cs.CL])",
    "abstract": "Despite the existence of various benchmarks for evaluating natural language processing models, we argue that human exams are a more suitable means of evaluating general intelligence for large language models (LLMs), as they inherently demand a much wider range of abilities such as language understanding, domain knowledge, and problem-solving skills. To this end, we introduce M3Exam, a novel benchmark sourced from real and official human exam questions for evaluating LLMs in a multilingual, multimodal, and multilevel context. M3Exam exhibits three unique characteristics: (1) multilingualism, encompassing questions from multiple countries that require strong multilingual proficiency and cultural knowledge; (2) multimodality, accounting for the multimodal nature of many exam questions to test the model's multimodal understanding capability; and (3) multilevel structure, featuring exams from three critical educational periods to comprehensively assess a model's proficiency at different lev",
    "link": "http://arxiv.org/abs/2306.05179",
    "context": "Title: M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models. (arXiv:2306.05179v1 [cs.CL])\nAbstract: Despite the existence of various benchmarks for evaluating natural language processing models, we argue that human exams are a more suitable means of evaluating general intelligence for large language models (LLMs), as they inherently demand a much wider range of abilities such as language understanding, domain knowledge, and problem-solving skills. To this end, we introduce M3Exam, a novel benchmark sourced from real and official human exam questions for evaluating LLMs in a multilingual, multimodal, and multilevel context. M3Exam exhibits three unique characteristics: (1) multilingualism, encompassing questions from multiple countries that require strong multilingual proficiency and cultural knowledge; (2) multimodality, accounting for the multimodal nature of many exam questions to test the model's multimodal understanding capability; and (3) multilevel structure, featuring exams from three critical educational periods to comprehensively assess a model's proficiency at different lev",
    "path": "papers/23/06/2306.05179.json",
    "total_tokens": 910,
    "translated_title": "M3Exam: 一种多语言、多模态、多层次的基准测试，用于评估大型语言模型",
    "translated_abstract": "尽管存在着各种针对自然语言处理模型进行评估的基准测试，但我们认为考试更适合评估大型语言模型的普适智能，因为它们囊括了更广泛的能力需求，例如语言理解、领域知识和解决问题的能力。为此，我们引入了 M3Exam，这是一个基于真实和官方人类考试题目的新型基准测试，用于在多语言、多模态和多层次的情境中评估 LLM。M3Exam 具有三个独特特点:（1）多语言性，涵盖多个国家的问题，需要强大的多语言能力和文化知识；（2）多模态，考虑到许多考试问题的多模态性质，以测试模型的多模态理解能力；（3）多层次结构，特别涵盖了三个关键教育阶段的考试，全面评估模型在不同教育水平上的熟练程度。",
    "tldr": "M3Exam是一个来源于真实人类考试题目的新型基准测试，用于评估大型语言模型在多语言、多模态和多层次的情境中的普适智能。",
    "en_tdlr": "M3Exam is a novel benchmark sourced from real and official human exam questions for evaluating LLMs in a multilingual, multimodal, and multilevel context, suitable for examining the general intelligence of large language models."
}