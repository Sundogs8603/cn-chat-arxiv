{
    "title": "FlowCam: Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow. (arXiv:2306.00180v1 [cs.CV])",
    "abstract": "Reconstruction of 3D neural fields from posed images has emerged as a promising method for self-supervised representation learning. The key challenge preventing the deployment of these 3D scene learners on large-scale video data is their dependence on precise camera poses from structure-from-motion, which is prohibitively expensive to run at scale. We propose a method that jointly reconstructs camera poses and 3D neural scene representations online and in a single forward pass. We estimate poses by first lifting frame-to-frame optical flow to 3D scene flow via differentiable rendering, preserving locality and shift-equivariance of the image processing backbone. SE(3) camera pose estimation is then performed via a weighted least-squares fit to the scene flow field. This formulation enables us to jointly supervise pose estimation and a generalizable neural scene representation via re-rendering the input video, and thus, train end-to-end and fully self-supervised on real-world video datas",
    "link": "http://arxiv.org/abs/2306.00180",
    "context": "Title: FlowCam: Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow. (arXiv:2306.00180v1 [cs.CV])\nAbstract: Reconstruction of 3D neural fields from posed images has emerged as a promising method for self-supervised representation learning. The key challenge preventing the deployment of these 3D scene learners on large-scale video data is their dependence on precise camera poses from structure-from-motion, which is prohibitively expensive to run at scale. We propose a method that jointly reconstructs camera poses and 3D neural scene representations online and in a single forward pass. We estimate poses by first lifting frame-to-frame optical flow to 3D scene flow via differentiable rendering, preserving locality and shift-equivariance of the image processing backbone. SE(3) camera pose estimation is then performed via a weighted least-squares fit to the scene flow field. This formulation enables us to jointly supervise pose estimation and a generalizable neural scene representation via re-rendering the input video, and thus, train end-to-end and fully self-supervised on real-world video datas",
    "path": "papers/23/06/2306.00180.json",
    "total_tokens": 879,
    "translated_title": "FlowCam: 通过像素对齐场景流，无需相机位姿训练具有通用性的三维辐射场",
    "translated_abstract": "从定位图像中重建三维神经场已成为自监督表示学习的一种有前途的方法。但依赖于大规模视频数据上结构光学精确相机位姿，这也是其无法推广的关键挑战。我们提出了一种联合在线重建相机位姿和三维神经场表示的方法，并通过单次前向传递来完成。该方法通过可微分渲染将帧间光流映射到三维场景流，以估计位姿。然后再通过对场景流场进行加权最小二乘拟合来执行SE（3）相机位姿估计。该方法能够通过重新渲染输入视频来联合监督位姿估计和通用的神经场表示，从而在真实世界的视频数据上进行全自监督训练。",
    "tldr": "本文提出了一种利用可微分渲染将帧间光流映射到三维场景流以同步重建相机位姿和三维神经场表示的方法。这种方法不需要精确相机位姿，可在真实世界的视频数据上进行全自监督训练。",
    "en_tdlr": "This paper proposes a method using differentiable rendering to estimate camera poses and reconstruct 3D neural scene representations without precise camera poses, which enables fully self-supervised training on real-world video data."
}