{
    "title": "HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution. (arXiv:2306.15794v1 [cs.LG])",
    "abstract": "Genomic (DNA) sequences encode an enormous amount of information for gene regulation and protein synthesis. Similar to natural language models, researchers have proposed foundation models in genomics to learn generalizable features from unlabeled genome data that can then be fine-tuned for downstream tasks such as identifying regulatory elements. Due to the quadratic scaling of attention, previous Transformer-based genomic models have used 512 to 4k tokens as context (<0.001% of the human genome), significantly limiting the modeling of long-range interactions in DNA. In addition, these methods rely on tokenizers to aggregate meaningful DNA units, losing single nucleotide resolution where subtle genetic variations can completely alter protein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a large language model based on implicit convolutions was shown to match attention in quality while allowing longer context lengths and lower time complexity. Leveraging Hyenas n",
    "link": "http://arxiv.org/abs/2306.15794",
    "context": "Title: HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution. (arXiv:2306.15794v1 [cs.LG])\nAbstract: Genomic (DNA) sequences encode an enormous amount of information for gene regulation and protein synthesis. Similar to natural language models, researchers have proposed foundation models in genomics to learn generalizable features from unlabeled genome data that can then be fine-tuned for downstream tasks such as identifying regulatory elements. Due to the quadratic scaling of attention, previous Transformer-based genomic models have used 512 to 4k tokens as context (<0.001% of the human genome), significantly limiting the modeling of long-range interactions in DNA. In addition, these methods rely on tokenizers to aggregate meaningful DNA units, losing single nucleotide resolution where subtle genetic variations can completely alter protein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a large language model based on implicit convolutions was shown to match attention in quality while allowing longer context lengths and lower time complexity. Leveraging Hyenas n",
    "path": "papers/23/06/2306.15794.json",
    "total_tokens": 881,
    "translated_title": "HyenaDNA：单核苷酸分辨率下的长范围基因组序列建模",
    "translated_abstract": "基因组（DNA）序列编码了大量关于基因调控和蛋白质合成的信息。类似自然语言模型，研究人员提出了基因组的基础模型，从非标记的基因组数据中学习可推广的特征，然后进行下游任务的微调，如识别调控元件。由于注意力的二次扩展，先前基于Transformer的基因组模型仅使用512到4k个标记作为上下文（<0.001%的人类基因组），严重限制了DNA的长范围相互作用建模。此外，这些方法依赖于标记器来聚合有意义的DNA单元，丢失了单核苷酸分辨率，其中微小的遗传变异可以通过单核苷酸多态性（SNP）完全改变蛋白质功能。最近，基于隐式卷积的大型语言模型Hyena显示出能够与注意力质量相匹配，同时允许更长的上下文长度和更低的时间复杂性。利用Hyenas n",
    "tldr": "HyenaDNA是一种基于隐式卷积的基因组序列建模方法，可以在单核苷酸分辨率下对长范围相互作用进行建模。",
    "en_tdlr": "HyenaDNA is a genomic sequence modeling method based on implicit convolutions, allowing for modeling of long-range interactions at single nucleotide resolution."
}