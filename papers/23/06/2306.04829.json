{
    "title": "Object-Centric Learning for Real-World Videos by Predicting Temporal Feature Similarities. (arXiv:2306.04829v1 [cs.CV])",
    "abstract": "Unsupervised video-based object-centric learning is a promising avenue to learn structured representations from large, unlabeled video collections, but previous approaches have only managed to scale to real-world datasets in restricted domains. Recently, it was shown that the reconstruction of pre-trained self-supervised features leads to object-centric representations on unconstrained real-world image datasets. Building on this approach, we propose a novel way to use such pre-trained features in the form of a temporal feature similarity loss. This loss encodes temporal correlations between image patches and is a natural way to introduce a motion bias for object discovery. We demonstrate that this loss leads to state-of-the-art performance on the challenging synthetic MOVi datasets. When used in combination with the feature reconstruction loss, our model is the first object-centric video model that scales to unconstrained video datasets such as YouTube-VIS.",
    "link": "http://arxiv.org/abs/2306.04829",
    "context": "Title: Object-Centric Learning for Real-World Videos by Predicting Temporal Feature Similarities. (arXiv:2306.04829v1 [cs.CV])\nAbstract: Unsupervised video-based object-centric learning is a promising avenue to learn structured representations from large, unlabeled video collections, but previous approaches have only managed to scale to real-world datasets in restricted domains. Recently, it was shown that the reconstruction of pre-trained self-supervised features leads to object-centric representations on unconstrained real-world image datasets. Building on this approach, we propose a novel way to use such pre-trained features in the form of a temporal feature similarity loss. This loss encodes temporal correlations between image patches and is a natural way to introduce a motion bias for object discovery. We demonstrate that this loss leads to state-of-the-art performance on the challenging synthetic MOVi datasets. When used in combination with the feature reconstruction loss, our model is the first object-centric video model that scales to unconstrained video datasets such as YouTube-VIS.",
    "path": "papers/23/06/2306.04829.json",
    "total_tokens": 940,
    "translated_title": "利用预测时间特征相似性的物体中心学习实现对真实世界视频的分析",
    "translated_abstract": "无监督的基于视频的物体中心学习是从大规模无标签视频集合中学习结构化表示的有前途的途径。然而，以前的方法只能在受限领域内缩放到真实世界的数据集。最近的研究表明，预训练的自监督特征的重建会导致在不受约束的真实世界图像数据集上的物体中心表示。基于这种方法，我们提出了一种利用这些预训练特征的新方法，形式为时间特征相似性损失。该损失编码图像块之间的时间相关性，并自然地引入运动偏差来发现物体。我们证明，这种损失导致了在具有挑战性的合成MOVi数据集上的最先进性能。当与特征重建损失结合使用时，我们的模型是首个能够扩展到无约束视频数据集（如YouTube-VIS）的物体中心视频模型。",
    "tldr": "本研究提出了一种新方法，利用预训练的自监督特征和时间特征相似性损失，实现了对真实世界视频的物体中心学习，在合成MOVi数据集上取得了最先进的性能。同时，本模型是首个能够扩展到无约束视频数据集的物体中心视频模型。",
    "en_tdlr": "The study proposes a new approach that utilizes pre-trained self-supervised features and a temporal feature similarity loss to achieve object-centric learning on real-world videos. The model achieves state-of-the-art performance on the challenging synthetic MOVi dataset and is the first object-centric video model that scales to unconstrained video datasets like YouTube-VIS."
}