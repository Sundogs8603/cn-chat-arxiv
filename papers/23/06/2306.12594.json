{
    "title": "State-wise Constrained Policy Optimization. (arXiv:2306.12594v1 [cs.LG])",
    "abstract": "Reinforcement Learning (RL) algorithms have shown tremendous success in simulation environments, but their application to real-world problems faces significant challenges, with safety being a major concern. In particular, enforcing state-wise constraints is essential for many challenging tasks such as autonomous driving and robot manipulation. However, existing safe RL algorithms under the framework of Constrained Markov Decision Process (CMDP) do not consider state-wise constraints. To address this gap, we propose State-wise Constrained Policy Optimization (SCPO), the first general-purpose policy search algorithm for state-wise constrained reinforcement learning. SCPO provides guarantees for state-wise constraint satisfaction in expectation. In particular, we introduce the framework of Maximum Markov Decision Process, and prove that the worst-case safety violation is bounded under SCPO. We demonstrate the effectiveness of our approach on training neural network policies for extensive ",
    "link": "http://arxiv.org/abs/2306.12594",
    "context": "Title: State-wise Constrained Policy Optimization. (arXiv:2306.12594v1 [cs.LG])\nAbstract: Reinforcement Learning (RL) algorithms have shown tremendous success in simulation environments, but their application to real-world problems faces significant challenges, with safety being a major concern. In particular, enforcing state-wise constraints is essential for many challenging tasks such as autonomous driving and robot manipulation. However, existing safe RL algorithms under the framework of Constrained Markov Decision Process (CMDP) do not consider state-wise constraints. To address this gap, we propose State-wise Constrained Policy Optimization (SCPO), the first general-purpose policy search algorithm for state-wise constrained reinforcement learning. SCPO provides guarantees for state-wise constraint satisfaction in expectation. In particular, we introduce the framework of Maximum Markov Decision Process, and prove that the worst-case safety violation is bounded under SCPO. We demonstrate the effectiveness of our approach on training neural network policies for extensive ",
    "path": "papers/23/06/2306.12594.json",
    "total_tokens": 860,
    "translated_title": "约束下的策略优化：State-wise Constrained Policy Optimization",
    "translated_abstract": "强化学习算法在模拟环境中已经取得了巨大的成功，但是在实际问题中应用仍然面临着重大挑战，其中安全性是一个主要问题。特别地，对于许多具有挑战性的任务，例如自动驾驶和机器人操作，强制执行状态限制是十分必要的。然而，现有的约束马尔可夫决策过程（CMDP）框架下的安全强化学习算法并没有考虑状态约束。为填补这一空白，我们提出了State-wise Constrained Policy Optimization（SCPO），这是第一个旨在处理状态限制的通用策略搜索算法。SCPO能够在期望上保证状态约束的满足。特别地，我们引入了最大马尔可夫决策过程框架，并证明了在SCPO下最坏的安全违反是有界的。我们在大量训练神经网络策略时展示了该方法的有效性。",
    "tldr": "本文提出了一种新的通用策略搜索算法，State-wise Constrained Policy Optimization (SCPO)，可用于处理状态限制约束下的强化学习，具有良好的期望状态约束保证和最坏安全违反的有界性。",
    "en_tdlr": "This paper proposes a new general-purpose policy search algorithm, State-wise Constrained Policy Optimization (SCPO), for handling state constraints in reinforcement learning, with good expected state constraint guarantees and bounded worst-case safety violation."
}