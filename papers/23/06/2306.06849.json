{
    "title": "Mitigating Transformer Overconfidence via Lipschitz Regularization. (arXiv:2306.06849v2 [cs.LG] UPDATED)",
    "abstract": "Though Transformers have achieved promising results in many computer vision tasks, they tend to be over-confident in predictions, as the standard Dot Product Self-Attention (DPSA) can barely preserve distance for the unbounded input domain. In this work, we fill this gap by proposing a novel Lipschitz Regularized Transformer (LRFormer). Specifically, we present a new similarity function with the distance within Banach Space to ensure the Lipschitzness and also regularize the term by a contractive Lipschitz Bound. The proposed method is analyzed with a theoretical guarantee, providing a rigorous basis for its effectiveness and reliability. Extensive experiments conducted on standard vision benchmarks demonstrate that our method outperforms the state-of-the-art single forward pass approaches in prediction, calibration, and uncertainty estimation.",
    "link": "http://arxiv.org/abs/2306.06849",
    "context": "Title: Mitigating Transformer Overconfidence via Lipschitz Regularization. (arXiv:2306.06849v2 [cs.LG] UPDATED)\nAbstract: Though Transformers have achieved promising results in many computer vision tasks, they tend to be over-confident in predictions, as the standard Dot Product Self-Attention (DPSA) can barely preserve distance for the unbounded input domain. In this work, we fill this gap by proposing a novel Lipschitz Regularized Transformer (LRFormer). Specifically, we present a new similarity function with the distance within Banach Space to ensure the Lipschitzness and also regularize the term by a contractive Lipschitz Bound. The proposed method is analyzed with a theoretical guarantee, providing a rigorous basis for its effectiveness and reliability. Extensive experiments conducted on standard vision benchmarks demonstrate that our method outperforms the state-of-the-art single forward pass approaches in prediction, calibration, and uncertainty estimation.",
    "path": "papers/23/06/2306.06849.json",
    "total_tokens": 815,
    "translated_title": "通过Lipschitz正则化来减轻Transformer的过度自信问题",
    "translated_abstract": "虽然Transformer在许多计算机视觉任务中取得了有希望的结果，但它们往往在预测中过于自信，因为标准的点积自注意力(DPSA)在无界输入域中几乎无法保持距离。在这项工作中，我们通过提出一种新的Lipschitz正则化Transformer(LRFormer)来填补这个空白。具体来说，我们提出了一种新的相似性函数，它在Banach空间内保证了Lipschitz性质，并通过一个收缩的Lipschitz边界来正则化该项。所提出的方法经过理论保证进行了分析，为其有效性和可靠性提供了严谨的基础。在标准的视觉基准上进行的大量实验证明，我们的方法在预测、校准和不确定性估计方面优于最先进的单次前向传递方法。",
    "tldr": "这项工作通过提出一种新的Lipschitz正则化Transformer (LRFormer)来减轻Transformer在预测中的过度自信问题，并在视觉基准实验证明了其优于现有方法的性能。",
    "en_tdlr": "This work mitigates the issue of overconfidence in Transformer predictions by introducing a novel Lipschitz Regularized Transformer (LRFormer) and demonstrates its superior performance over existing methods on standard vision benchmarks."
}