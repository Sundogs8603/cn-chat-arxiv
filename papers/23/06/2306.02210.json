{
    "title": "GPT-FL: Generative Pre-trained Model-Assisted Federated Learning. (arXiv:2306.02210v2 [cs.LG] UPDATED)",
    "abstract": "In this work, we propose GPT-FL, a generative pre-trained model-assisted federated learning (FL) framework. At its core, GPT-FL leverages generative pre-trained models to generate diversified synthetic data. These generated data are used to train a downstream model on the server, which is then fine-tuned with private client data under the standard FL framework. We show that GPT-FL consistently outperforms state-of-the-art FL methods in terms of model test accuracy, communication efficiency, and client sampling efficiency. Through comprehensive ablation analysis, we discover that the downstream model generated by synthetic data plays a crucial role in controlling the direction of gradient diversity during FL training, which enhances convergence speed and contributes to the notable accuracy boost observed with GPT-FL. Also, regardless of whether the target data falls within or outside the domain of the pre-trained generative model, GPT-FL consistently achieves significant performance gai",
    "link": "http://arxiv.org/abs/2306.02210",
    "context": "Title: GPT-FL: Generative Pre-trained Model-Assisted Federated Learning. (arXiv:2306.02210v2 [cs.LG] UPDATED)\nAbstract: In this work, we propose GPT-FL, a generative pre-trained model-assisted federated learning (FL) framework. At its core, GPT-FL leverages generative pre-trained models to generate diversified synthetic data. These generated data are used to train a downstream model on the server, which is then fine-tuned with private client data under the standard FL framework. We show that GPT-FL consistently outperforms state-of-the-art FL methods in terms of model test accuracy, communication efficiency, and client sampling efficiency. Through comprehensive ablation analysis, we discover that the downstream model generated by synthetic data plays a crucial role in controlling the direction of gradient diversity during FL training, which enhances convergence speed and contributes to the notable accuracy boost observed with GPT-FL. Also, regardless of whether the target data falls within or outside the domain of the pre-trained generative model, GPT-FL consistently achieves significant performance gai",
    "path": "papers/23/06/2306.02210.json",
    "total_tokens": 1015,
    "translated_title": "GPT-FL: 生成预训练模型辅助的联邦学习",
    "translated_abstract": "在这项工作中，我们提出了GPT-FL，一种生成预训练模型辅助的联邦学习（FL）框架。GPT-FL利用生成预训练模型生成多样化的合成数据。这些生成的数据用于在服务器上训练下游模型，然后在标准FL框架下使用私有客户端数据进行微调。我们展示了GPT-FL在模型测试准确性、通信效率和客户端采样效率方面始终优于最先进的FL方法。通过全面的消融分析，我们发现在FL训练过程中，由合成数据生成的下游模型对于控制梯度多样性的方向起着关键作用，这提高了收敛速度，并对观察到的GPT-FL的显著准确性提升做出了贡献。此外，无论目标数据是否在预训练生成模型的领域内或外，GPT-FL始终实现了显著的性能提升。",
    "tldr": "GPT-FL是一种生成预训练模型辅助的联邦学习框架，通过生成多样化的合成数据并结合私有客户端数据进行训练，它在模型准确性、通信效率和客户端采样效率等方面优于最先进的方法。在FL训练中，由合成数据生成的下游模型对于控制梯度多样性的方向起着关键作用，提高了收敛速度，并显著提升了准确性。",
    "en_tdlr": "GPT-FL is a generative pre-trained model-assisted federated learning framework that leverages diversified synthetic data and private client data to outperform state-of-the-art methods in terms of model accuracy, communication efficiency, and client sampling efficiency. The downstream model generated by synthetic data plays a crucial role in controlling gradient diversity direction, enhancing convergence speed, and significantly improving accuracy."
}