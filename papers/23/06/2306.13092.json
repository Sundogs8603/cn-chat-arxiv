{
    "title": "Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective. (arXiv:2306.13092v1 [cs.CV])",
    "abstract": "We present a new dataset condensation framework termed Squeeze, Recover and Relabel (SRe$^2$L) that decouples the bilevel optimization of model and synthetic data during training, to handle varying scales of datasets, model architectures and image resolutions for effective dataset condensation. The proposed method demonstrates flexibility across diverse dataset scales and exhibits multiple advantages in terms of arbitrary resolutions of synthesized images, low training cost and memory consumption with high-resolution training, and the ability to scale up to arbitrary evaluation network architectures. Extensive experiments are conducted on Tiny-ImageNet and full ImageNet-1K datasets. Under 50 IPC, our approach achieves the highest 42.5% and 60.8% validation accuracy on Tiny-ImageNet and ImageNet-1K, outperforming all previous state-of-the-art methods by margins of 14.5% and 32.9%, respectively. Our approach also outperforms MTT by approximately 52$\\times$ (ConvNet-4) and 16$\\times$ (Res",
    "link": "http://arxiv.org/abs/2306.13092",
    "context": "Title: Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective. (arXiv:2306.13092v1 [cs.CV])\nAbstract: We present a new dataset condensation framework termed Squeeze, Recover and Relabel (SRe$^2$L) that decouples the bilevel optimization of model and synthetic data during training, to handle varying scales of datasets, model architectures and image resolutions for effective dataset condensation. The proposed method demonstrates flexibility across diverse dataset scales and exhibits multiple advantages in terms of arbitrary resolutions of synthesized images, low training cost and memory consumption with high-resolution training, and the ability to scale up to arbitrary evaluation network architectures. Extensive experiments are conducted on Tiny-ImageNet and full ImageNet-1K datasets. Under 50 IPC, our approach achieves the highest 42.5% and 60.8% validation accuracy on Tiny-ImageNet and ImageNet-1K, outperforming all previous state-of-the-art methods by margins of 14.5% and 32.9%, respectively. Our approach also outperforms MTT by approximately 52$\\times$ (ConvNet-4) and 16$\\times$ (Res",
    "path": "papers/23/06/2306.13092.json",
    "total_tokens": 992,
    "translated_title": "从新的角度压缩ImageNet规模数据集：SRe$^2$L",
    "translated_abstract": "我们提出了一个新的数据集压缩框架，称为Squeeze、Recover和Relabel（SRe$^2$L），它在训练期间分离了模型和合成数据的双层优化，以处理不同规模的数据集、模型体系结构和图像分辨率，从而实现有效的数据集压缩。所提出的方法展示了在不同数据集规模上的灵活性，并在合成图像任意分辨率、高分辨率训练的情况下具有低训练成本和内存消耗以及扩展到任意评估网络体系结构的能力。我们在Tiny-ImageNet和完整的ImageNet-1K数据集上进行了广泛的实验。在50IPC下，我们的方法在Tiny-ImageNet和ImageNet-1K上分别实现了42.5％和60.8％的最高验证精度，较之前所有最先进方法提高了14.5％和32.9％。我们的方法在Res上也比MTT快约52倍(ConvNet-4)和16倍。",
    "tldr": "SRe$^2$L是一种数据集压缩方法，可以处理不同规模的数据集、模型体系结构和图像分辨率，具有低训练成本和内存消耗以及扩展到任意评估网络体系结构的能力，在Tiny-ImageNet和ImageNet-1K数据集上实现了最佳性能，并超越了现有的最先进方法。",
    "en_tdlr": "SRe$^2$L is a dataset condensation method that can handle datasets of varying scales, model architectures and image resolutions, with low training cost and memory consumption and the ability to scale up to arbitrary evaluation network architectures. It achieves the best performance on Tiny-ImageNet and ImageNet-1K datasets, surpassing existing state-of-the-art methods."
}