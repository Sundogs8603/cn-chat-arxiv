{
    "title": "When to Read Documents or QA History: On Unified and Selective Open-domain QA. (arXiv:2306.04176v1 [cs.CL])",
    "abstract": "This paper studies the problem of open-domain question answering, with the aim of answering a diverse range of questions leveraging knowledge resources. Two types of sources, QA-pair and document corpora, have been actively leveraged with the following complementary strength. The former is highly precise when the paraphrase of given question $q$ was seen and answered during training, often posed as a retrieval problem, while the latter generalizes better for unseen questions. A natural follow-up is thus leveraging both models, while a naive pipelining or integration approaches have failed to bring additional gains over either model alone. Our distinction is interpreting the problem as calibration, which estimates the confidence of predicted answers as an indicator to decide when to use a document or QA-pair corpus. The effectiveness of our method was validated on widely adopted benchmarks such as Natural Questions and TriviaQA.",
    "link": "http://arxiv.org/abs/2306.04176",
    "context": "Title: When to Read Documents or QA History: On Unified and Selective Open-domain QA. (arXiv:2306.04176v1 [cs.CL])\nAbstract: This paper studies the problem of open-domain question answering, with the aim of answering a diverse range of questions leveraging knowledge resources. Two types of sources, QA-pair and document corpora, have been actively leveraged with the following complementary strength. The former is highly precise when the paraphrase of given question $q$ was seen and answered during training, often posed as a retrieval problem, while the latter generalizes better for unseen questions. A natural follow-up is thus leveraging both models, while a naive pipelining or integration approaches have failed to bring additional gains over either model alone. Our distinction is interpreting the problem as calibration, which estimates the confidence of predicted answers as an indicator to decide when to use a document or QA-pair corpus. The effectiveness of our method was validated on widely adopted benchmarks such as Natural Questions and TriviaQA.",
    "path": "papers/23/06/2306.04176.json",
    "total_tokens": 706,
    "translated_title": "何时查阅文档或QA历史记录：统一和有选择的开放领域QA问题研究",
    "translated_abstract": "本文研究了开放领域问答的问题，致力于利用知识资源回答各种各样的问题。文档语料库和QA-pair是两种常用的信息源，前者在处理已知问题时非常准确，而后者在处理未知问题时更具广泛性。我们提出了一种对预测答案置信度进行分析的方案，以决定何时使用文档或QA-pair。该方法在Natural Questions和TriviaQA等广泛采用的基准测试中被证明是有效的。",
    "tldr": "提出了一种方法来决定在何时使用文档或QA-pair回答问题，并在多个基准测试中验证了其有效性。",
    "en_tdlr": "This paper proposes a method to determine when to use a document corpus or QA-pair to answer questions in open-domain QA, and its effectiveness is validated on widely adopted benchmarks."
}