{
    "title": "ContraBAR: Contrastive Bayes-Adaptive Deep RL. (arXiv:2306.02418v1 [cs.LG])",
    "abstract": "In meta reinforcement learning (meta RL), an agent seeks a Bayes-optimal policy -- the optimal policy when facing an unknown task that is sampled from some known task distribution. Previous approaches tackled this problem by inferring a belief over task parameters, using variational inference methods. Motivated by recent successes of contrastive learning approaches in RL, such as contrastive predictive coding (CPC), we investigate whether contrastive methods can be used for learning Bayes-optimal behavior. We begin by proving that representations learned by CPC are indeed sufficient for Bayes optimality. Based on this observation, we propose a simple meta RL algorithm that uses CPC in lieu of variational belief inference. Our method, ContraBAR, achieves comparable performance to state-of-the-art in domains with state-based observation and circumvents the computational toll of future observation reconstruction, enabling learning in domains with image-based observations. It can also be c",
    "link": "http://arxiv.org/abs/2306.02418",
    "context": "Title: ContraBAR: Contrastive Bayes-Adaptive Deep RL. (arXiv:2306.02418v1 [cs.LG])\nAbstract: In meta reinforcement learning (meta RL), an agent seeks a Bayes-optimal policy -- the optimal policy when facing an unknown task that is sampled from some known task distribution. Previous approaches tackled this problem by inferring a belief over task parameters, using variational inference methods. Motivated by recent successes of contrastive learning approaches in RL, such as contrastive predictive coding (CPC), we investigate whether contrastive methods can be used for learning Bayes-optimal behavior. We begin by proving that representations learned by CPC are indeed sufficient for Bayes optimality. Based on this observation, we propose a simple meta RL algorithm that uses CPC in lieu of variational belief inference. Our method, ContraBAR, achieves comparable performance to state-of-the-art in domains with state-based observation and circumvents the computational toll of future observation reconstruction, enabling learning in domains with image-based observations. It can also be c",
    "path": "papers/23/06/2306.02418.json",
    "total_tokens": 922,
    "translated_title": "ContraBAR: 对比贝叶斯自适应深度强化学习",
    "translated_abstract": "在元强化学习（meta RL）中，智能体寻求贝叶斯最优策略——面对从某些已知任务分布中采样的未知任务时的最优策略。以前的方法通过推断任务参数上的信念，使用变分推理方法解决这个问题。受强化学习中对比学习方法的最近成功启发，例如对比预测编码（CPC），我们调查对比方法是否可用于学习贝叶斯最优行为。我们首先证明了CPC学习到的表示足以进行贝叶斯最优化。基于这个观察结果，我们提出了一种简单的元RL算法——ContraBAR，它使用CPC代替变分信念推断。我们的方法在基于状态观察的域中实现了与最先进方法可比较的性能，并规避了未来观察重建的计算代价，从而在基于图像的观察的领域中进行学习。",
    "tldr": "ContraBAR是一种使用对比贝叶斯自适应深度强化学习的元RL算法，可以在基于状态观察的领域中实现与最先进方法可比较的性能，并规避了未来观察重建的计算代价，从而在基于图像的观察的领域中进行学习。",
    "en_tdlr": "ContraBAR is a meta RL algorithm that employs contrastive Bayes-adaptive deep RL, achieving comparable performance to state-of-the-art in domains with state-based observation and circumventing the computational toll of future observation reconstruction, enabling learning in domains with image-based observations."
}