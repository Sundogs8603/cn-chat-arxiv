{
    "title": "Adaptive Sharpness-Aware Pruning for Robust Sparse Networks",
    "abstract": "arXiv:2306.14306v2 Announce Type: replace  Abstract: Robustness and compactness are two essential attributes of deep learning models that are deployed in the real world. The goals of robustness and compactness may seem to be at odds, since robustness requires generalization across domains, while the process of compression exploits specificity in one domain. We introduce Adaptive Sharpness-Aware Pruning (AdaSAP), which unifies these goals through the lens of network sharpness. The AdaSAP method produces sparse networks that are robust to input variations which are unseen at training time. We achieve this by strategically incorporating weight perturbations in order to optimize the loss landscape. This allows the model to be both primed for pruning and regularized for improved robustness. AdaSAP improves the robust accuracy of pruned models on image classification by up to +6% on ImageNet C and +4% on ImageNet V2, and on object detection by +4% on a corrupted Pascal VOC dataset, over a wi",
    "link": "https://arxiv.org/abs/2306.14306",
    "context": "Title: Adaptive Sharpness-Aware Pruning for Robust Sparse Networks\nAbstract: arXiv:2306.14306v2 Announce Type: replace  Abstract: Robustness and compactness are two essential attributes of deep learning models that are deployed in the real world. The goals of robustness and compactness may seem to be at odds, since robustness requires generalization across domains, while the process of compression exploits specificity in one domain. We introduce Adaptive Sharpness-Aware Pruning (AdaSAP), which unifies these goals through the lens of network sharpness. The AdaSAP method produces sparse networks that are robust to input variations which are unseen at training time. We achieve this by strategically incorporating weight perturbations in order to optimize the loss landscape. This allows the model to be both primed for pruning and regularized for improved robustness. AdaSAP improves the robust accuracy of pruned models on image classification by up to +6% on ImageNet C and +4% on ImageNet V2, and on object detection by +4% on a corrupted Pascal VOC dataset, over a wi",
    "path": "papers/23/06/2306.14306.json",
    "total_tokens": 962,
    "translated_title": "自适应锐度感知修剪用于稳健稀疏网络",
    "translated_abstract": "arXiv:2306.14306v2 公告类型: 替换 摘要: 在现实世界中部署的深度学习模型必须具有稳健性和紧凑性这两个基本属性。稳健性和紧凑性的目标似乎是相互矛盾的，因为稳健性需要在域间进行泛化，而压缩过程则利用一个域中的特定性。我们引入了自适应锐度感知修剪（AdaSAP），通过网络锐度的视角统一了这些目标。AdaSAP方法通过策略性地引入权重扰动来优化损失景观，从而产生对训练时未见输入变化稳健的稀疏网络。这使得模型既适合修剪又适用于改进稳健性的正则化。在图像分类上，AdaSAP能够使修剪模型在ImageNet C上的稳健精度提高最高达到+6％，在ImageNet V2上提高+4％，在目标检测上在一组受损Pascal VOC数据集上提高+4％。",
    "tldr": "AdaSAP方法通过网络锐度的视角统一了稳健性和紧凑性的目标，并通过策略性地引入权重扰动来使稀疏网络对训练时未见的输入变化稳健，在图像分类和目标检测任务上取得显著改进。",
    "en_tdlr": "The AdaSAP method unifies the goals of robustness and compactness through the lens of network sharpness, strategically introducing weight perturbations to make sparse networks robust to unseen input variations at training time, achieving significant improvements in image classification and object detection tasks."
}