{
    "title": "Learning a Neuron by a Shallow ReLU Network: Dynamics and Implicit Bias for Correlated Inputs. (arXiv:2306.06479v2 [cs.LG] UPDATED)",
    "abstract": "We prove that, for the fundamental regression task of learning a single neuron, training a one-hidden layer ReLU network of any width by gradient flow from a small initialisation converges to zero loss and is implicitly biased to minimise the rank of network parameters. By assuming that the training points are correlated with the teacher neuron, we complement previous work that considered orthogonal datasets. Our results are based on a detailed non-asymptotic analysis of the dynamics of each hidden neuron throughout the training. We also show and characterise a surprising distinction in this setting between interpolator networks of minimal rank and those of minimal Euclidean norm. Finally we perform a range of numerical experiments, which corroborate our theoretical findings.",
    "link": "http://arxiv.org/abs/2306.06479",
    "context": "Title: Learning a Neuron by a Shallow ReLU Network: Dynamics and Implicit Bias for Correlated Inputs. (arXiv:2306.06479v2 [cs.LG] UPDATED)\nAbstract: We prove that, for the fundamental regression task of learning a single neuron, training a one-hidden layer ReLU network of any width by gradient flow from a small initialisation converges to zero loss and is implicitly biased to minimise the rank of network parameters. By assuming that the training points are correlated with the teacher neuron, we complement previous work that considered orthogonal datasets. Our results are based on a detailed non-asymptotic analysis of the dynamics of each hidden neuron throughout the training. We also show and characterise a surprising distinction in this setting between interpolator networks of minimal rank and those of minimal Euclidean norm. Finally we perform a range of numerical experiments, which corroborate our theoretical findings.",
    "path": "papers/23/06/2306.06479.json",
    "total_tokens": 848,
    "translated_title": "通过浅层ReLU网络学习神经元：对相关输入的动态和隐式偏差的探索",
    "translated_abstract": "我们证明了对于学习单个神经元的基本回归任务，通过从小的初始值梯度流训练任意宽度的一层隐藏层ReLU网络会收敛到零误差，并且在隐式上偏向于最小化网络参数的秩。假设训练点与教师神经元相关，我们补充了先前考虑正交数据集的研究结果。我们的结果基于对训练过程中每个隐藏神经元动态的详细非渐进性分析。我们还展示并表征了这种情况下最小秩内插网络与最小欧几里德范数的令人惊讶的区别。最后，我们进行了一系列数值实验，以验证我们的理论发现。",
    "tldr": "这项研究证明，通过梯度流训练一个宽度任意的一层ReLU网络，可以学习一个单个神经元并收敛到零误差，同时隐式偏向于最小化网络参数的秩。这对于相关的训练数据点是有效的，与之前研究正交数据集的结果补充了彼此。",
    "en_tdlr": "This study demonstrates that training a one-hidden layer ReLU network of any width using gradient flow can learn a single neuron and converge to zero loss, while implicitly biasing towards minimizing the rank of network parameters. This is effective for correlated training data points and complements previous research on orthogonal datasets."
}