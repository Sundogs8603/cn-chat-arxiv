{
    "title": "Attention Weighted Mixture of Experts with Contrastive Learning for Personalized Ranking in E-commerce. (arXiv:2306.05011v1 [cs.IR])",
    "abstract": "Ranking model plays an essential role in e-commerce search and recommendation. An effective ranking model should give a personalized ranking list for each user according to the user preference. Existing algorithms usually extract a user representation vector from the user behavior sequence, then feed the vector into a feed-forward network (FFN) together with other features for feature interactions, and finally produce a personalized ranking score. Despite tremendous progress in the past, there is still room for improvement. Firstly, the personalized patterns of feature interactions for different users are not explicitly modeled. Secondly, most of existing algorithms have poor personalized ranking results for long-tail users with few historical behaviors due to the data sparsity. To overcome the two challenges, we propose Attention Weighted Mixture of Experts (AW-MoE) with contrastive learning for personalized ranking. Firstly, AW-MoE leverages the MoE framework to capture personalized ",
    "link": "http://arxiv.org/abs/2306.05011",
    "context": "Title: Attention Weighted Mixture of Experts with Contrastive Learning for Personalized Ranking in E-commerce. (arXiv:2306.05011v1 [cs.IR])\nAbstract: Ranking model plays an essential role in e-commerce search and recommendation. An effective ranking model should give a personalized ranking list for each user according to the user preference. Existing algorithms usually extract a user representation vector from the user behavior sequence, then feed the vector into a feed-forward network (FFN) together with other features for feature interactions, and finally produce a personalized ranking score. Despite tremendous progress in the past, there is still room for improvement. Firstly, the personalized patterns of feature interactions for different users are not explicitly modeled. Secondly, most of existing algorithms have poor personalized ranking results for long-tail users with few historical behaviors due to the data sparsity. To overcome the two challenges, we propose Attention Weighted Mixture of Experts (AW-MoE) with contrastive learning for personalized ranking. Firstly, AW-MoE leverages the MoE framework to capture personalized ",
    "path": "papers/23/06/2306.05011.json",
    "total_tokens": 894,
    "translated_title": "基于对比学习加权注意力的电子商务个性化排名专家混合模型",
    "translated_abstract": "排名模型在电子商务搜索和推荐中起着至关重要的作用。有效的排名模型应该根据用户喜好为每个用户提供个性化的排名列表。现有算法通常从用户行为序列中提取用户表示向量，然后将该向量与其他特征一起馈入前馈神经网络（FFN）进行特征交互，并最终生成个性化排名得分。尽管过去取得了巨大的进展，但仍有改进的空间。首先，不同用户的个性化特征交互模式没有明确建模。其次，由于数据稀疏，大多数现有算法在具有少量历史行为的长尾用户上的个性化排名结果较差。为了克服这两个挑战，我们提出了基于对比学习加权注意力的个性化排名专家混合模型（AW-MoE）。首先，AW-MoE利用MoE框架捕获个性化特征交互模式，",
    "tldr": "电子商务个性化排名专家混合模型，利用注意力机制和MoE框架进行特征交互建模，采用对比学习提升历史行为较少的长尾用户个性化排名结果。",
    "en_tdlr": "Attention Weighted Mixture of Experts (AW-MoE) model uses attention mechanism and MoE framework for feature interaction modeling, and contrastive learning for improving personalized ranking results for long-tail users with few historical behaviors in e-commerce."
}