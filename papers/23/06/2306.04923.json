{
    "title": "Unconstrained Online Learning with Unbounded Losses. (arXiv:2306.04923v1 [cs.LG])",
    "abstract": "Algorithms for online learning typically require one or more boundedness assumptions: that the domain is bounded, that the losses are Lipschitz, or both. In this paper, we develop a new setting for online learning with unbounded domains and non-Lipschitz losses. For this setting we provide an algorithm which guarantees $R_{T}(u)\\le \\tilde O(G\\|u\\|\\sqrt{T}+L\\|u\\|^{2}\\sqrt{T})$ regret on any problem where the subgradients satisfy $\\|g_{t}\\|\\le G+L\\|w_{t}\\|$, and show that this bound is unimprovable without further assumptions. We leverage this algorithm to develop new saddle-point optimization algorithms that converge in duality gap in unbounded domains, even in the absence of meaningful curvature. Finally, we provide the first algorithm achieving non-trivial dynamic regret in an unbounded domain for non-Lipschitz losses, as well as a matching lower bound. The regret of our dynamic regret algorithm automatically improves to a novel $L^{*}$ bound when the losses are smooth.",
    "link": "http://arxiv.org/abs/2306.04923",
    "context": "Title: Unconstrained Online Learning with Unbounded Losses. (arXiv:2306.04923v1 [cs.LG])\nAbstract: Algorithms for online learning typically require one or more boundedness assumptions: that the domain is bounded, that the losses are Lipschitz, or both. In this paper, we develop a new setting for online learning with unbounded domains and non-Lipschitz losses. For this setting we provide an algorithm which guarantees $R_{T}(u)\\le \\tilde O(G\\|u\\|\\sqrt{T}+L\\|u\\|^{2}\\sqrt{T})$ regret on any problem where the subgradients satisfy $\\|g_{t}\\|\\le G+L\\|w_{t}\\|$, and show that this bound is unimprovable without further assumptions. We leverage this algorithm to develop new saddle-point optimization algorithms that converge in duality gap in unbounded domains, even in the absence of meaningful curvature. Finally, we provide the first algorithm achieving non-trivial dynamic regret in an unbounded domain for non-Lipschitz losses, as well as a matching lower bound. The regret of our dynamic regret algorithm automatically improves to a novel $L^{*}$ bound when the losses are smooth.",
    "path": "papers/23/06/2306.04923.json",
    "total_tokens": 1010,
    "translated_title": "无约束在线学习和无界损失的算法",
    "translated_abstract": "在线学习算法通常需要一个或多个有界性假设：即域是有界的，损失是Lipschitz的或两者都有。在本文中，我们为具有无界域和非Lipschitz损失的在线学习开发了一个新的设置。针对该场景，我们提供了一种算法，可以保证在任何满足子梯度满足$\\|g_{t}\\|\\le G+L\\|w_{t}\\|$的问题中，其遗憾的度量值$R_{T}(u)\\le \\tilde O(G\\|u\\|\\sqrt{T}+L\\|u\\|^{2}\\sqrt{T})$，并且表明除非有进一步 假设，否则该界限是不能进一步改进的。",
    "tldr": "本论文提出了一种算法，可用于解决无界域和非Lipschitz损失的在线学习问题，并提供了一个遗憾的度量，以衡量该算法的性能。此外，我们还利用该算法开发了一种新的鞍点优化算法，即使在没有有意义的曲率的情况下，也能够在无界领域中收敛于对偶间隙。最后，我们提供了一种算法，在无界域和非Lipschitz损失的情况下实现了非平凡的动态遗憾，以及相匹配的下界。",
    "en_tdlr": "This paper proposes an algorithm that can be used to solve online learning problems with unbounded domains and non-Lipschitz losses, and provides a regret measure to evaluate the performance of the algorithm. In addition, we have developed a new saddle-point optimization algorithm using this algorithm, which can converge in duality gap in unbounded domains even without significant curvature. Finally, we provide the first algorithm that achieves non-trivial dynamic regret in an unbounded domain for non-Lipschitz losses, as well as a matching lower bound."
}