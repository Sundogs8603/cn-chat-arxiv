{
    "title": "DynaQuant: Compressing Deep Learning Training Checkpoints via Dynamic Quantization. (arXiv:2306.11800v1 [cs.LG])",
    "abstract": "With the increase in the scale of Deep Learning (DL) training workloads in terms of compute resources and time consumption, the likelihood of encountering in-training failures rises substantially, leading to lost work and resource wastage. Such failures are typically offset by a checkpointing mechanism, which comes at the cost of storage and network bandwidth overhead. State-of-the-art approaches involve lossy model compression mechanisms, which induce a tradeoff between the resulting model quality (accuracy) and compression ratio. Delta compression is then also used to further reduce the overhead by only storing the difference between consecutive checkpoints. We make a key enabling observation that the sensitivity of model weights to compression varies during training, and different weights benefit from different quantization levels (ranging from retaining full precision to pruning). We propose (1) a non-uniform quantization scheme that leverages this variation, (2) an efficient searc",
    "link": "http://arxiv.org/abs/2306.11800",
    "context": "Title: DynaQuant: Compressing Deep Learning Training Checkpoints via Dynamic Quantization. (arXiv:2306.11800v1 [cs.LG])\nAbstract: With the increase in the scale of Deep Learning (DL) training workloads in terms of compute resources and time consumption, the likelihood of encountering in-training failures rises substantially, leading to lost work and resource wastage. Such failures are typically offset by a checkpointing mechanism, which comes at the cost of storage and network bandwidth overhead. State-of-the-art approaches involve lossy model compression mechanisms, which induce a tradeoff between the resulting model quality (accuracy) and compression ratio. Delta compression is then also used to further reduce the overhead by only storing the difference between consecutive checkpoints. We make a key enabling observation that the sensitivity of model weights to compression varies during training, and different weights benefit from different quantization levels (ranging from retaining full precision to pruning). We propose (1) a non-uniform quantization scheme that leverages this variation, (2) an efficient searc",
    "path": "papers/23/06/2306.11800.json",
    "total_tokens": 835,
    "translated_title": "DynaQuant: 通过动态量化压缩深度学习训练检查点",
    "translated_abstract": "随着深度学习训练工作量在计算资源和时间消耗方面的增加，遇到训练失败的可能性显著增加，导致工作丢失和资源浪费。最新的方法涉及有损模型压缩机制，这会在模型质量（准确性）和压缩比之间产生权衡。我们提出了一个新颖的动态量化框架，称为DynaQuant，它可以根据训练期间模型权重的灵敏度变化来更新量化级别，从而实现对各种最先进模型的显着压缩，并且几乎不影响模型准确性。",
    "tldr": "DynaQuant通过动态量化实现对各种最先进模型的显着压缩，几乎不影响模型准确性。",
    "en_tdlr": "DynaQuant achieves significant compression on various state-of-the-art models with negligible losses in model accuracy through dynamic quantization."
}