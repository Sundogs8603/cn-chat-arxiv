{
    "title": "Loss-Optimal Classification Trees: A Generalized Framework and the Logistic Case. (arXiv:2306.00857v1 [stat.ML])",
    "abstract": "The Classification Tree (CT) is one of the most common models in interpretable machine learning. Although such models are usually built with greedy strategies, in recent years, thanks to remarkable advances in Mixer-Integer Programming (MIP) solvers, several exact formulations of the learning problem have been developed. In this paper, we argue that some of the most relevant ones among these training models can be encapsulated within a general framework, whose instances are shaped by the specification of loss functions and regularizers. Next, we introduce a novel realization of this framework: specifically, we consider the logistic loss, handled in the MIP setting by a linear piece-wise approximation, and couple it with $\\ell_1$-regularization terms. The resulting Optimal Logistic Tree model numerically proves to be able to induce trees with enhanced interpretability features and competitive generalization capabilities, compared to the state-of-the-art MIP-based approaches.",
    "link": "http://arxiv.org/abs/2306.00857",
    "context": "Title: Loss-Optimal Classification Trees: A Generalized Framework and the Logistic Case. (arXiv:2306.00857v1 [stat.ML])\nAbstract: The Classification Tree (CT) is one of the most common models in interpretable machine learning. Although such models are usually built with greedy strategies, in recent years, thanks to remarkable advances in Mixer-Integer Programming (MIP) solvers, several exact formulations of the learning problem have been developed. In this paper, we argue that some of the most relevant ones among these training models can be encapsulated within a general framework, whose instances are shaped by the specification of loss functions and regularizers. Next, we introduce a novel realization of this framework: specifically, we consider the logistic loss, handled in the MIP setting by a linear piece-wise approximation, and couple it with $\\ell_1$-regularization terms. The resulting Optimal Logistic Tree model numerically proves to be able to induce trees with enhanced interpretability features and competitive generalization capabilities, compared to the state-of-the-art MIP-based approaches.",
    "path": "papers/23/06/2306.00857.json",
    "total_tokens": 886,
    "translated_title": "损失最小化分类树：一个通用的框架和逻辑斯蒂回归情况研究",
    "translated_abstract": "分类树是可解释机器学习中最常见的模型之一，尽管这样的模型通常采用贪婪策略构建，但是近年来，由于混合整数规划（MIP）求解器的显著进展，已经开发了几种精确的学习问题的表述。本文认为其中一些最相关的训练模型可以封装在一个通用框架内，其实例由损失函数和正则化器的规定塑造。接下来，我们介绍了这个框架的新颖实现：具体来说，我们考虑逻辑损失，它在MIP设置中通过线性分段逼近处理，并与$\\ell_1$-规则化项相结合。最终的最优逻辑树模型在数值上被证明能够诱导具有增强可解释性特征和竞争性泛化能力的树，与最先进的基于MIP的方法相比。",
    "tldr": "本论文为损失最小化分类树的解决提供了一个通用框架，基于逻辑斯蒂回归的解决方式具有具有解释性特征和竞争泛化能力。",
    "en_tdlr": "This paper presents a generalized framework for loss-optimal classification trees and proposes a novel method based on logistic regression, which demonstrates enhanced interpretability and competitive generalizability compared to state-of-the-art approaches."
}