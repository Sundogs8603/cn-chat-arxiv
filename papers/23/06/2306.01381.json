{
    "title": "Adaptive Message Quantization and Parallelization for Distributed Full-graph GNN Training. (arXiv:2306.01381v1 [cs.LG])",
    "abstract": "Distributed full-graph training of Graph Neural Networks (GNNs) over large graphs is bandwidth-demanding and time-consuming. Frequent exchanges of node features, embeddings and embedding gradients (all referred to as messages) across devices bring significant communication overhead for nodes with remote neighbors on other devices (marginal nodes) and unnecessary waiting time for nodes without remote neighbors (central nodes) in the training graph. This paper proposes an efficient GNN training system, AdaQP, to expedite distributed full-graph GNN training. We stochastically quantize messages transferred across devices to lower-precision integers for communication traffic reduction and advocate communication-computation parallelization between marginal nodes and central nodes. We provide theoretical analysis to prove fast training convergence (at the rate of O(T^{-1}) with T being the total number of training epochs) and design an adaptive quantization bit-width assignment scheme for eac",
    "link": "http://arxiv.org/abs/2306.01381",
    "context": "Title: Adaptive Message Quantization and Parallelization for Distributed Full-graph GNN Training. (arXiv:2306.01381v1 [cs.LG])\nAbstract: Distributed full-graph training of Graph Neural Networks (GNNs) over large graphs is bandwidth-demanding and time-consuming. Frequent exchanges of node features, embeddings and embedding gradients (all referred to as messages) across devices bring significant communication overhead for nodes with remote neighbors on other devices (marginal nodes) and unnecessary waiting time for nodes without remote neighbors (central nodes) in the training graph. This paper proposes an efficient GNN training system, AdaQP, to expedite distributed full-graph GNN training. We stochastically quantize messages transferred across devices to lower-precision integers for communication traffic reduction and advocate communication-computation parallelization between marginal nodes and central nodes. We provide theoretical analysis to prove fast training convergence (at the rate of O(T^{-1}) with T being the total number of training epochs) and design an adaptive quantization bit-width assignment scheme for eac",
    "path": "papers/23/06/2306.01381.json",
    "total_tokens": 920,
    "translated_title": "自适应消息量化和并行化在分布式全图GNN训练中的应用",
    "translated_abstract": "大规模图网络神经网络（GNNs）的分布式全图训练具有带宽需求高和耗时长的特点。跨设备频繁交换节点特征、嵌入和嵌入梯度（均称为消息）带来了显著的通信开销，对于在其他设备上具有远程邻居的节点（边缘节点）而言，而对于没有远程邻居的节点（中心节点）而言则带来了不必要的等待时间。本文提出了一种高效的GNN训练系统AdaQP，通过随机量化跨设备传输的消息以降低通信流量，并提倡边缘节点和中心节点之间的通信-计算并行化，以加快分布式全图GNN训练速度。我们提供了理论分析，证明了快速训练收敛（以O（T ^ {-1}）的速率，其中T为训练周期的总数），并设计了一种自适应量化位宽分配方案。",
    "tldr": "本文提出了一种高效的GNN训练系统AdaQP，通过随机量化跨设备传输的消息以降低通信流量，并提倡边缘节点和中心节点之间的通信-计算并行化，以加快分布式全图GNN训练速度。",
    "en_tdlr": "This paper proposes an efficient GNN training system, AdaQP, to expedite distributed full-graph GNN training by stochastically quantizing messages transferred across devices to lower-precision integers for communication traffic reduction and advocating communication-computation parallelization between marginal nodes and central nodes."
}