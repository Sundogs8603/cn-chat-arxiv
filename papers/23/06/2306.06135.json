{
    "title": "Safety and Fairness for Content Moderation in Generative Models. (arXiv:2306.06135v1 [cs.LG])",
    "abstract": "With significant advances in generative AI, new technologies are rapidly being deployed with generative components. Generative models are typically trained on large datasets, resulting in model behaviors that can mimic the worst of the content in the training data. Responsible deployment of generative technologies requires content moderation strategies, such as safety input and output filters. Here, we provide a theoretical framework for conceptualizing responsible content moderation of text-to-image generative technologies, including a demonstration of how to empirically measure the constructs we enumerate. We define and distinguish the concepts of safety, fairness, and metric equity, and enumerate example harms that can come in each domain. We then provide a demonstration of how the defined harms can be quantified. We conclude with a summary of how the style of harms quantification we demonstrate enables data-driven content moderation decisions.",
    "link": "http://arxiv.org/abs/2306.06135",
    "context": "Title: Safety and Fairness for Content Moderation in Generative Models. (arXiv:2306.06135v1 [cs.LG])\nAbstract: With significant advances in generative AI, new technologies are rapidly being deployed with generative components. Generative models are typically trained on large datasets, resulting in model behaviors that can mimic the worst of the content in the training data. Responsible deployment of generative technologies requires content moderation strategies, such as safety input and output filters. Here, we provide a theoretical framework for conceptualizing responsible content moderation of text-to-image generative technologies, including a demonstration of how to empirically measure the constructs we enumerate. We define and distinguish the concepts of safety, fairness, and metric equity, and enumerate example harms that can come in each domain. We then provide a demonstration of how the defined harms can be quantified. We conclude with a summary of how the style of harms quantification we demonstrate enables data-driven content moderation decisions.",
    "path": "papers/23/06/2306.06135.json",
    "total_tokens": 879,
    "translated_title": "生成模型中的内容审核安全与公平性",
    "translated_abstract": "随着生成人工智能的显著进步，新技术正迅速部署到生成组件中。生成模型通常是在大型数据集上进行训练，导致其行为可能模仿训练数据中最糟糕的内容。负责任地部署生成技术需要内容审核策略，例如安全输入和输出过滤器。在此，我们提供一个理论框架，用于概念化文本到图像生成技术的负责任内容审核，包括如何实证地衡量我们列举的构造。我们定义和区分了安全、公平和指标公平的概念，并列举了每个领域可能出现的例子损害。然后，我们提供了如何量化定义的损害的演示。最后，我们总结了我们演示的损害量化风格如何实现数据驱动的内容审核决策。",
    "tldr": "生成模型在训练数据中模仿最糟糕的内容，通过安全输入和输出过滤器实现负责任部署；通过安全、公平和指标公平的定义，列举了每个领域可能遇到的例子损害，并提供了损害量化的演示。",
    "en_tdlr": "Generative models trained on large datasets can mimic unwanted behaviors. This paper proposes a framework for responsible content moderation of text-to-image generative technologies, including safety input and output filters. It distinguishes and defines concepts of safety, fairness, and metric equity with examples of potential harms, and demonstrates how these harms can be quantified to enable data-driven content moderation decisions."
}