{
    "title": "Improving Offline RL by Blending Heuristics. (arXiv:2306.00321v1 [cs.LG])",
    "abstract": "We propose Heuristic Blending (HUBL), a simple performance-improving technique for a broad class of offline RL algorithms based on value bootstrapping. HUBL modifies Bellman operators used in these algorithms, partially replacing the bootstrapped values with Monte-Carlo returns as heuristics. For trajectories with higher returns, HUBL relies more on heuristics and less on bootstrapping; otherwise, it leans more heavily on bootstrapping. We show that this idea can be easily implemented by relabeling the offline datasets with adjusted rewards and discount factors, making HUBL readily usable by many existing offline RL implementations. We theoretically prove that HUBL reduces offline RL's complexity and thus improves its finite-sample performance. Furthermore, we empirically demonstrate that HUBL consistently improves the policy quality of four state-of-the-art bootstrapping-based offline RL algorithms (ATAC, CQL, TD3+BC, and IQL), by 9% on average over 27 datasets of the D4RL and Meta-Wo",
    "link": "http://arxiv.org/abs/2306.00321",
    "context": "Title: Improving Offline RL by Blending Heuristics. (arXiv:2306.00321v1 [cs.LG])\nAbstract: We propose Heuristic Blending (HUBL), a simple performance-improving technique for a broad class of offline RL algorithms based on value bootstrapping. HUBL modifies Bellman operators used in these algorithms, partially replacing the bootstrapped values with Monte-Carlo returns as heuristics. For trajectories with higher returns, HUBL relies more on heuristics and less on bootstrapping; otherwise, it leans more heavily on bootstrapping. We show that this idea can be easily implemented by relabeling the offline datasets with adjusted rewards and discount factors, making HUBL readily usable by many existing offline RL implementations. We theoretically prove that HUBL reduces offline RL's complexity and thus improves its finite-sample performance. Furthermore, we empirically demonstrate that HUBL consistently improves the policy quality of four state-of-the-art bootstrapping-based offline RL algorithms (ATAC, CQL, TD3+BC, and IQL), by 9% on average over 27 datasets of the D4RL and Meta-Wo",
    "path": "papers/23/06/2306.00321.json",
    "total_tokens": 988,
    "translated_title": "通过启发式策略混合改进离线强化学习",
    "translated_abstract": "我们提出了启发式策略混合（HUBL），一种用于基于值函数回溯的广泛类离线强化学习算法的简单性能提升技术。HUBL修改了这些算法中使用的Bellman操作符，部分用启发式的蒙特卡罗回报替换了值函数回溯。对于回报更高的轨迹，HUBL更多地依赖于启发式，较少依赖于值函数回溯。否则，它会更加倚重于值函数回溯。我们证明了这个想法可以通过调整奖励和折扣因子来简单实现，使HUBL可适用于现有的许多离线强化学习算法。我们从理论上证明HUBL降低了离线强化学习的复杂度，从而提高了其有限样本的性能。此外，我们还从实证方面表明了HUBL对四种最先进的基于回溯的离线强化学习算法的策略质量进行了改进，平均提高了9%的效果，在D4RL和Meta-Wo的27个数据集上表现稳定。",
    "tldr": "HUBL是一种用于基于值函数回溯的广泛类离线强化学习算法的简单性能提升技术。我们证明了HUBL可通过调整奖励和折扣因子来简单实现，并且实验结果表明HUBL能够在提高性能的同时降低复杂度。",
    "en_tdlr": "HUBL is a simple performance-improving technique for a broad class of offline RL algorithms based on value bootstrapping. We demonstrate that HUBL can be easily implemented by adjusting rewards and discount factors, and the experimental results show that HUBL can improve performance while reducing complexity."
}