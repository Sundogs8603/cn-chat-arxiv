{
    "title": "Adversarial Resilience in Sequential Prediction via Abstention. (arXiv:2306.13119v1 [cs.LG])",
    "abstract": "We study the problem of sequential prediction in the stochastic setting with an adversary that is allowed to inject clean-label adversarial (or out-of-distribution) examples. Algorithms designed to handle purely stochastic data tend to fail in the presence of such adversarial examples, often leading to erroneous predictions. This is undesirable in many high-stakes applications such as medical recommendations, where abstaining from predictions on adversarial examples is preferable to misclassification. On the other hand, assuming fully adversarial data leads to very pessimistic bounds that are often vacuous in practice.  To capture this motivation, we propose a new model of sequential prediction that sits between the purely stochastic and fully adversarial settings by allowing the learner to abstain from making a prediction at no cost on adversarial examples. Assuming access to the marginal distribution on the non-adversarial examples, we design a learner whose error scales with the VC ",
    "link": "http://arxiv.org/abs/2306.13119",
    "context": "Title: Adversarial Resilience in Sequential Prediction via Abstention. (arXiv:2306.13119v1 [cs.LG])\nAbstract: We study the problem of sequential prediction in the stochastic setting with an adversary that is allowed to inject clean-label adversarial (or out-of-distribution) examples. Algorithms designed to handle purely stochastic data tend to fail in the presence of such adversarial examples, often leading to erroneous predictions. This is undesirable in many high-stakes applications such as medical recommendations, where abstaining from predictions on adversarial examples is preferable to misclassification. On the other hand, assuming fully adversarial data leads to very pessimistic bounds that are often vacuous in practice.  To capture this motivation, we propose a new model of sequential prediction that sits between the purely stochastic and fully adversarial settings by allowing the learner to abstain from making a prediction at no cost on adversarial examples. Assuming access to the marginal distribution on the non-adversarial examples, we design a learner whose error scales with the VC ",
    "path": "papers/23/06/2306.13119.json",
    "total_tokens": 878,
    "translated_title": "通过弃权实现顺序预测中的对抗鲁棒性",
    "translated_abstract": "本文研究了在带有允许注入干净标签对抗性（或超出分布）示例的对抗者的情况下，在随机设置下的顺序预测问题。针对纯随机数据的算法在存在此类对抗性示例的情况下往往失败，从而导致错误的预测。这在许多高风险应用中是不可取的，例如医学建议，这里弃权不进行对抗性示例的预测优于误分类。另一方面，假设完全对抗性数据导致非常悲观的界限，在实践中往往是空洞的。为了实现这一目标，我们提出了一种新的顺序预测模型，它位于纯随机和完全对抗性设置之间，通过允许学习器在对抗样例上无代价地放弃进行预测来实现。假设访问非对抗样例的边际分布，我们设计了一个学习器，其误差随着VC维的变化而变化。",
    "tldr": "本文提出了一种处理顺序预测的模型，允许在不对对抗性样例进行预测的情况下提高算法抗对抗攻击的能力。",
    "en_tdlr": "This paper proposes a sequential prediction model that improves the algorithm's ability to resist adversarial attacks by allowing abstention from making predictions on adversarial examples."
}