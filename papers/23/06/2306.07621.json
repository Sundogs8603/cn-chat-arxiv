{
    "title": "Rank-Aware Negative Training for Semi-Supervised Text Classification. (arXiv:2306.07621v1 [cs.CL])",
    "abstract": "Semi-supervised text classification-based paradigms (SSTC) typically employ the spirit of self-training. The key idea is to train a deep classifier on limited labeled texts and then iteratively predict the unlabeled texts as their pseudo-labels for further training. However, the performance is largely affected by the accuracy of pseudo-labels, which may not be significant in real-world scenarios. This paper presents a Rank-aware Negative Training (RNT) framework to address SSTC in learning with noisy label manner. To alleviate the noisy information, we adapt a reasoning with uncertainty-based approach to rank the unlabeled texts based on the evidential support received from the labeled texts. Moreover, we propose the use of negative training to train RNT based on the concept that ``the input instance does not belong to the complementary label''. A complementary label is randomly selected from all labels except the label on-target. Intuitively, the probability of a true label serving as",
    "link": "http://arxiv.org/abs/2306.07621",
    "context": "Title: Rank-Aware Negative Training for Semi-Supervised Text Classification. (arXiv:2306.07621v1 [cs.CL])\nAbstract: Semi-supervised text classification-based paradigms (SSTC) typically employ the spirit of self-training. The key idea is to train a deep classifier on limited labeled texts and then iteratively predict the unlabeled texts as their pseudo-labels for further training. However, the performance is largely affected by the accuracy of pseudo-labels, which may not be significant in real-world scenarios. This paper presents a Rank-aware Negative Training (RNT) framework to address SSTC in learning with noisy label manner. To alleviate the noisy information, we adapt a reasoning with uncertainty-based approach to rank the unlabeled texts based on the evidential support received from the labeled texts. Moreover, we propose the use of negative training to train RNT based on the concept that ``the input instance does not belong to the complementary label''. A complementary label is randomly selected from all labels except the label on-target. Intuitively, the probability of a true label serving as",
    "path": "papers/23/06/2306.07621.json",
    "total_tokens": 865,
    "translated_title": "面向半监督文本分类的排名感知负样本训练",
    "translated_abstract": "半监督文本分类常常使用自我训练的方法，对有限标注数据进行训练并迭代地对未标注文本进行伪标签预测以供进一步训练。本文提出了一种排名感知负样本训练框架（RNT）以解决学习中存在噪声标签的半监督文本分类问题。为了减轻噪声信息，我们采用基于不确定性的推理方法来排名未标注文本，以其从标注文本中获得的证据支持为依据。此外，我们提出了负样本训练的方法来训练RNT，其中“输入实例不属于对应标签”是RNT的基本概念。直观地，一个真实标签作为补充标签的概率越小，对应的负样本的训练难度就越大。",
    "tldr": "本文提出了一种Rank-aware Negative Training（RNT）框架，通过基于不确定性的推理方法排名未标注文本并利用负样本训练解决了在半监督文本分类中存在噪声标签的问题。",
    "en_tdlr": "This paper proposes a Rank-aware Negative Training (RNT) framework, which uses a reasoning with uncertainty-based approach to rank unlabeled texts and trains on negative samples. RNT effectively solves the problem of noisy labels in semi-supervised text classification."
}