{
    "title": "Stochastic Methods in Variational Inequalities: Ergodicity, Bias and Refinements. (arXiv:2306.16502v1 [stat.ML])",
    "abstract": "For min-max optimization and variational inequalities problems (VIP) encountered in diverse machine learning tasks, Stochastic Extragradient (SEG) and Stochastic Gradient Descent Ascent (SGDA) have emerged as preeminent algorithms. Constant step-size variants of SEG/SGDA have gained popularity, with appealing benefits such as easy tuning and rapid forgiveness of initial conditions, but their convergence behaviors are more complicated even in rudimentary bilinear models. Our work endeavors to elucidate and quantify the probabilistic structures intrinsic to these algorithms. By recasting the constant step-size SEG/SGDA as time-homogeneous Markov Chains, we establish a first-of-its-kind Law of Large Numbers and a Central Limit Theorem, demonstrating that the average iterate is asymptotically normal with a unique invariant distribution for an extensive range of monotone and non-monotone VIPs. Specializing to convex-concave min-max optimization, we characterize the relationship between the ",
    "link": "http://arxiv.org/abs/2306.16502",
    "context": "Title: Stochastic Methods in Variational Inequalities: Ergodicity, Bias and Refinements. (arXiv:2306.16502v1 [stat.ML])\nAbstract: For min-max optimization and variational inequalities problems (VIP) encountered in diverse machine learning tasks, Stochastic Extragradient (SEG) and Stochastic Gradient Descent Ascent (SGDA) have emerged as preeminent algorithms. Constant step-size variants of SEG/SGDA have gained popularity, with appealing benefits such as easy tuning and rapid forgiveness of initial conditions, but their convergence behaviors are more complicated even in rudimentary bilinear models. Our work endeavors to elucidate and quantify the probabilistic structures intrinsic to these algorithms. By recasting the constant step-size SEG/SGDA as time-homogeneous Markov Chains, we establish a first-of-its-kind Law of Large Numbers and a Central Limit Theorem, demonstrating that the average iterate is asymptotically normal with a unique invariant distribution for an extensive range of monotone and non-monotone VIPs. Specializing to convex-concave min-max optimization, we characterize the relationship between the ",
    "path": "papers/23/06/2306.16502.json",
    "total_tokens": 912,
    "translated_title": "变分不等式中的随机方法：遍历性、偏差与改进",
    "translated_abstract": "对于在各种机器学习任务中遇到的min-max优化和变分不等式问题(VIP)，随机外推梯度(SEG)和随机梯度上升下降(SGDA)算法已成为杰出的算法。SEG/SGDA的恒定步长变种广受欢迎，具有易于调节和原始条件迅速适应的优点，但即使在基本的双线性模型中，它们的收敛行为也更加复杂。我们的工作旨在阐明和量化这些算法内在的概率结构。通过将恒定步长SEG/SGDA重新构造为时间齐次马尔可夫链，我们建立了首个大数定律和中心极限定理，证明了在广泛的单调和非单调VIP情况下，平均迭代收敛到一个唯一的不变分布。特别是对于凸凹min-max优化，我们刻画了连接VIP和优化偏差的关系。",
    "tldr": "本论文研究了变分不等式中的随机方法，并通过建立大数定律和中心极限定理揭示了这些算法的收敛性质，对于广泛的VIP问题，平均迭代收敛到一个唯一的不变分布。",
    "en_tdlr": "This paper investigates stochastic methods in variational inequalities and establishes the Law of Large Numbers and Central Limit Theorem to reveal the convergence properties of these algorithms. It shows that for a wide range of VIP problems, the average iterate converges to a unique invariant distribution."
}