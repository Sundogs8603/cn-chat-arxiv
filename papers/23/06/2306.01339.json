{
    "title": "Resource-Efficient Federated Hyperdimensional Computing. (arXiv:2306.01339v1 [cs.LG])",
    "abstract": "In conventional federated hyperdimensional computing (HDC), training larger models usually results in higher predictive performance but also requires more computational, communication, and energy resources. If the system resources are limited, one may have to sacrifice the predictive performance by reducing the size of the HDC model. The proposed resource-efficient federated hyperdimensional computing (RE-FHDC) framework alleviates such constraints by training multiple smaller independent HDC sub-models and refining the concatenated HDC model using the proposed dropout-inspired procedure. Our numerical comparison demonstrates that the proposed framework achieves a comparable or higher predictive performance while consuming less computational and wireless resources than the baseline federated HDC implementation.",
    "link": "http://arxiv.org/abs/2306.01339",
    "context": "Title: Resource-Efficient Federated Hyperdimensional Computing. (arXiv:2306.01339v1 [cs.LG])\nAbstract: In conventional federated hyperdimensional computing (HDC), training larger models usually results in higher predictive performance but also requires more computational, communication, and energy resources. If the system resources are limited, one may have to sacrifice the predictive performance by reducing the size of the HDC model. The proposed resource-efficient federated hyperdimensional computing (RE-FHDC) framework alleviates such constraints by training multiple smaller independent HDC sub-models and refining the concatenated HDC model using the proposed dropout-inspired procedure. Our numerical comparison demonstrates that the proposed framework achieves a comparable or higher predictive performance while consuming less computational and wireless resources than the baseline federated HDC implementation.",
    "path": "papers/23/06/2306.01339.json",
    "total_tokens": 767,
    "translated_title": "资源高效的联合超维计算",
    "translated_abstract": "在传统的联合超维计算（HDC）中，训练更大的模型通常会带来更高的预测性能，但也需要更多的计算、通信和能源资源。如果系统资源有限，人们可能不得不通过减小HDC模型大小来牺牲预测性能。提出的资源高效的联合超维计算（RE-FHDC）框架通过训练多个更小的独立的HDC子模型，并使用所提出的类似于dropout的过程精炼合并的HDC模型来缓解这种约束。我们的数值比较表明，所提出的框架在消耗比基线联合HDC实现更少的计算和无线资源的同时，实现了可比或更高的预测性能。",
    "tldr": "提出了一种资源高效的联合超维计算框架，通过训练多个小的HDC子模型并使用改进后的dropout流程，实现可与大模型相当的预测性能，消耗更少的计算和无线资源。",
    "en_tdlr": "A resource-efficient federated hyperdimensional computing (RE-FHDC) framework is proposed to achieve comparable or higher predictive performance while consuming less computational and wireless resources than the baseline federated HDC implementation by training multiple smaller independent HDC sub-models and refining the concatenated HDC model using the proposed dropout-inspired procedure."
}