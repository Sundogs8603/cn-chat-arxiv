{
    "title": "SGLD-Based Information Criteria and the Over-Parameterized Regime. (arXiv:2306.05583v1 [cs.LG])",
    "abstract": "Double-descent refers to the unexpected drop in test loss of a learning algorithm beyond an interpolating threshold with over-parameterization, which is not predicted by information criteria in their classical forms due to the limitations in the standard asymptotic approach. We update these analyses using the information risk minimization framework and provide Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) for models learned by stochastic gradient Langevin dynamics (SGLD). Notably, the AIC and BIC penalty terms for SGLD correspond to specific information measures, i.e., symmetrized KL information and KL divergence. We extend this information-theoretic analysis to over-parameterized models by characterizing the SGLD-based BIC for the random feature model in the regime where the number of parameters $p$ and the number of samples $n$ tend to infinity, with $p/n$ fixed. Our experiments demonstrate that the refined SGLD-based BIC can track the double-descent cur",
    "link": "http://arxiv.org/abs/2306.05583",
    "context": "Title: SGLD-Based Information Criteria and the Over-Parameterized Regime. (arXiv:2306.05583v1 [cs.LG])\nAbstract: Double-descent refers to the unexpected drop in test loss of a learning algorithm beyond an interpolating threshold with over-parameterization, which is not predicted by information criteria in their classical forms due to the limitations in the standard asymptotic approach. We update these analyses using the information risk minimization framework and provide Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) for models learned by stochastic gradient Langevin dynamics (SGLD). Notably, the AIC and BIC penalty terms for SGLD correspond to specific information measures, i.e., symmetrized KL information and KL divergence. We extend this information-theoretic analysis to over-parameterized models by characterizing the SGLD-based BIC for the random feature model in the regime where the number of parameters $p$ and the number of samples $n$ tend to infinity, with $p/n$ fixed. Our experiments demonstrate that the refined SGLD-based BIC can track the double-descent cur",
    "path": "papers/23/06/2306.05583.json",
    "total_tokens": 851,
    "translated_title": "基于SGLD的信息准则与超参数化模型研究",
    "translated_abstract": "“双丘陵”是指过度参数化学习算法在插值阈值之外的意外测试损失下降，这不是由于标准渐进方法的局限性，导致经典形式的信息准则无法预测。我们使用信息风险最小化框架更新这些分析，并为通过随机梯度Langevin动力学（SGLD）学习的模型提供了Akaike信息准则（AIC）和贝叶斯信息准则（BIC）。值得注意的是，SGLD的AIC和BIC罚项对应特定的信息度量，即对称的KL信息和KL散度。我们通过表征大量参数模型的SGLD-BIC扩展了此信息理论分析，其中参数数$p$和样本数$n$趋于无穷大，$p/n$固定。我们的实验表明，改进的SGLD-BIC可以跟踪双下降曲线。",
    "tldr": "本研究提出针对超参数化学习算法的SGLD信息准则，通过KL信息和KL散度罚项来追踪双下降现象。",
    "en_tdlr": "This study proposes SGLD-based information criteria for over-parameterized learning algorithms, which track the double-descent phenomenon through penalty terms based on KL information and divergence."
}