{
    "title": "Binary and Ternary Natural Language Generation. (arXiv:2306.01841v1 [cs.CL])",
    "abstract": "Ternary and binary neural networks enable multiplication-free computation and promise multiple orders of magnitude efficiency gains over full-precision networks if implemented on specialized hardware. However, since both the parameter and the output space are highly discretized, such networks have proven very difficult to optimize. The difficulties are compounded for the class of transformer text generation models due to the sensitivity of the attention operation to quantization and the noise-compounding effects of autoregressive decoding in the high-cardinality output space. We approach the problem with a mix of statistics-based quantization for the weights and elastic quantization of the activations and demonstrate the first ternary and binary transformer models on the downstream tasks of summarization and machine translation. Our ternary BART base achieves an R1 score of 41 on the CNN/DailyMail benchmark, which is merely 3.9 points behind the full model while being 16x more efficien",
    "link": "http://arxiv.org/abs/2306.01841",
    "context": "Title: Binary and Ternary Natural Language Generation. (arXiv:2306.01841v1 [cs.CL])\nAbstract: Ternary and binary neural networks enable multiplication-free computation and promise multiple orders of magnitude efficiency gains over full-precision networks if implemented on specialized hardware. However, since both the parameter and the output space are highly discretized, such networks have proven very difficult to optimize. The difficulties are compounded for the class of transformer text generation models due to the sensitivity of the attention operation to quantization and the noise-compounding effects of autoregressive decoding in the high-cardinality output space. We approach the problem with a mix of statistics-based quantization for the weights and elastic quantization of the activations and demonstrate the first ternary and binary transformer models on the downstream tasks of summarization and machine translation. Our ternary BART base achieves an R1 score of 41 on the CNN/DailyMail benchmark, which is merely 3.9 points behind the full model while being 16x more efficien",
    "path": "papers/23/06/2306.01841.json",
    "total_tokens": 829,
    "translated_title": "二进制和三进制自然语言生成",
    "translated_abstract": "二进制和三进制神经网络具有无需乘法的计算能力，如果在专用硬件上实现，则可以比全精度网络带来数个数量级的效率提升。然而，由于参数和输出空间都高度离散化，这种网络很难进行优化。对于变压器文本生成模型这一类，由于注意力运算对量化的敏感性以及自回归解码在高基数输出空间的噪声叠加效应，这些困难变得更加复杂。我们采用基于统计的权重量化和激活弹性量化的混合方法来解决这个问题，并在摘要和机器翻译的下游任务上展示了第一个三进制和二进制变压器模型。我们的三进制BART基本模型在CNN/DailyMail基准测试中取得了41的R1得分，仅比全模型低3.9分，同时效率提升了16倍。",
    "tldr": "本文提出了基于混合方法的二进制和三进制变压器模型，在摘要和机器翻译任务上取得了不错的效果，并在效率上获得了大幅度提升。"
}