{
    "title": "A blind spot for large language models: Supradiegetic linguistic information",
    "abstract": "arXiv:2306.06794v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) like ChatGPT reflect profound changes in the field of Artificial Intelligence, achieving a linguistic fluency that is impressively, even shockingly, human-like. The extent of their current and potential capabilities is an active area of investigation by no means limited to scientific researchers. It is common for people to frame the training data for LLMs as \"text\" or even \"language\". We examine the details of this framing using ideas from several areas, including linguistics, embodied cognition, cognitive science, mathematics, and history. We propose that considering what it is like to be an LLM like ChatGPT, as Nagel might have put it, can help us gain insight into its capabilities in general, and in particular, that its exposure to linguistic training data can be productively reframed as exposure to the diegetic information encoded in language, and its deficits can be reframed as ignorance of ext",
    "link": "https://arxiv.org/abs/2306.06794",
    "context": "Title: A blind spot for large language models: Supradiegetic linguistic information\nAbstract: arXiv:2306.06794v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) like ChatGPT reflect profound changes in the field of Artificial Intelligence, achieving a linguistic fluency that is impressively, even shockingly, human-like. The extent of their current and potential capabilities is an active area of investigation by no means limited to scientific researchers. It is common for people to frame the training data for LLMs as \"text\" or even \"language\". We examine the details of this framing using ideas from several areas, including linguistics, embodied cognition, cognitive science, mathematics, and history. We propose that considering what it is like to be an LLM like ChatGPT, as Nagel might have put it, can help us gain insight into its capabilities in general, and in particular, that its exposure to linguistic training data can be productively reframed as exposure to the diegetic information encoded in language, and its deficits can be reframed as ignorance of ext",
    "path": "papers/23/06/2306.06794.json",
    "total_tokens": 883,
    "translated_title": "大型语言模型的盲点：超叙事语言信息",
    "translated_abstract": "像ChatGPT这样的大型语言模型(LLMs)反映了人工智能领域的深刻变革，实现了令人印象深刻甚至令人震惊的类人语言流利度。它们目前和潜在的能力范围是一个积极探讨的领域，绝非仅限于科研人员。人们通常将LLMs的训练数据框定为“文本”甚至“语言”。我们使用来自语言学、体现认知、认知科学、数学和历史等领域的思想，仔细审视这一框架的细节。我们提出，考虑像ChatGPT这样的LLM是什么感觉，正如纳格尔可能会说的那样，可以帮助我们深入了解其整体能力，特别是，其接受的语言训练数据可以被有益地重新构思为对语言中编码的叙事信息的接触，其缺陷可以被重新构思为对这些信息的无知。",
    "tldr": "大型语言模型的盲点在于其对超叙事语言信息的忽视，研究提出考虑模型如何感知语言信息有助于深入了解其能力。"
}