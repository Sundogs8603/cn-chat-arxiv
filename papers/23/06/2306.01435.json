{
    "title": "Improving Adversarial Robustness of DEQs with Explicit Regulations Along the Neural Dynamics. (arXiv:2306.01435v1 [cs.LG])",
    "abstract": "Deep equilibrium (DEQ) models replace the multiple-layer stacking of conventional deep networks with a fixed-point iteration of a single-layer transformation. Having been demonstrated to be competitive in a variety of real-world scenarios, the adversarial robustness of general DEQs becomes increasingly crucial for their reliable deployment. Existing works improve the robustness of general DEQ models with the widely-used adversarial training (AT) framework, but they fail to exploit the structural uniquenesses of DEQ models. To this end, we interpret DEQs through the lens of neural dynamics and find that AT under-regulates intermediate states. Besides, the intermediate states typically provide predictions with a high prediction entropy. Informed by the correlation between the entropy of dynamical systems and their stability properties, we propose reducing prediction entropy by progressively updating inputs along the neural dynamics. During AT, we also utilize random intermediate states t",
    "link": "http://arxiv.org/abs/2306.01435",
    "context": "Title: Improving Adversarial Robustness of DEQs with Explicit Regulations Along the Neural Dynamics. (arXiv:2306.01435v1 [cs.LG])\nAbstract: Deep equilibrium (DEQ) models replace the multiple-layer stacking of conventional deep networks with a fixed-point iteration of a single-layer transformation. Having been demonstrated to be competitive in a variety of real-world scenarios, the adversarial robustness of general DEQs becomes increasingly crucial for their reliable deployment. Existing works improve the robustness of general DEQ models with the widely-used adversarial training (AT) framework, but they fail to exploit the structural uniquenesses of DEQ models. To this end, we interpret DEQs through the lens of neural dynamics and find that AT under-regulates intermediate states. Besides, the intermediate states typically provide predictions with a high prediction entropy. Informed by the correlation between the entropy of dynamical systems and their stability properties, we propose reducing prediction entropy by progressively updating inputs along the neural dynamics. During AT, we also utilize random intermediate states t",
    "path": "papers/23/06/2306.01435.json",
    "total_tokens": 899,
    "translated_title": "通过神经动力学的显式规定来提高DEQ模型的对抗鲁棒性",
    "translated_abstract": "深度平衡（ DEQ ）模型将传统深层网络的多层堆叠替换为单层变换的不动点迭代。已经证明在各种实际应用场景中 DEQ 模型具有竞争优势，因此一般 DEQ 模型的对抗鲁棒性变得越来越重要。现有的工作通过广泛使用的对抗训练（ AT）框架来提高一般 DEQ 模型的鲁棒性，但它们未能利用 DEQ 模型的结构独特性。为此，我们通过神经动力学的视角解释 DEQs，并发现 AT 对中间状态进行了不充分的规定。此外，中间状态通常提供具有高预测熵的预测。受动态系统熵与其稳定性质之间关联的启发，我们提出通过沿着神经动力学逐步更新输入来降低预测熵。在 AT 过程中，我们还利用随机中间状态t",
    "tldr": "本论文通过神经动力学的解释，提出了一种新的对抗训练框架，通过逐步更新输入来降低预测熵，从而提高DEQ模型的对抗性。",
    "en_tdlr": "This paper proposes a new adversarial training framework for improving the adversarial robustness of DEQ models by reducing prediction entropy through progressive input updates along the neural dynamics, which is based on the interpretation of DEQs through the lens of neural dynamics."
}