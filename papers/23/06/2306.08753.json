{
    "title": "Towards training Bilingual and Code-Switched Speech Recognition models from Monolingual data sources. (arXiv:2306.08753v2 [eess.AS] UPDATED)",
    "abstract": "Multilingual Automatic Speech Recognition (ASR) models are capable of transcribing audios across multiple languages, eliminating the need for separate models. In addition, they can perform Language Identification (LID) and handle code-switched speech. However, training these models requires special code-switch and multilingual speech corpora which are sparsely available. In this paper, we evaluate different approaches towards training of bilingual as well as code-switched ASR models using purely monolingual data sources. We introduce the concept of aggregate tokenizers that differs from the current prevalent technique of generating LIDs at the boundaries of monolingual samples and produces LID for each emitted token instead. We compare bilingual and monolingual model performance, showcase the efficacy of aggregate tokenizers, present a synthetic code-switched ASR data generation technique and demonstrate the effectiveness of the proposed code-switched ASR models for the tasks of speech",
    "link": "http://arxiv.org/abs/2306.08753",
    "context": "Title: Towards training Bilingual and Code-Switched Speech Recognition models from Monolingual data sources. (arXiv:2306.08753v2 [eess.AS] UPDATED)\nAbstract: Multilingual Automatic Speech Recognition (ASR) models are capable of transcribing audios across multiple languages, eliminating the need for separate models. In addition, they can perform Language Identification (LID) and handle code-switched speech. However, training these models requires special code-switch and multilingual speech corpora which are sparsely available. In this paper, we evaluate different approaches towards training of bilingual as well as code-switched ASR models using purely monolingual data sources. We introduce the concept of aggregate tokenizers that differs from the current prevalent technique of generating LIDs at the boundaries of monolingual samples and produces LID for each emitted token instead. We compare bilingual and monolingual model performance, showcase the efficacy of aggregate tokenizers, present a synthetic code-switched ASR data generation technique and demonstrate the effectiveness of the proposed code-switched ASR models for the tasks of speech",
    "path": "papers/23/06/2306.08753.json",
    "total_tokens": 960,
    "translated_title": "从单语数据源中训练双语和代码切换语音识别模型的方法",
    "translated_abstract": "多语言自动语音识别（ASR）模型能够转录多种语言的音频，消除了使用不同模型的需要。此外，它们还能进行语言识别（LID）和处理代码切换语音。然而，训练这些模型需要稀缺的代码切换和多语音数据语料库。本文评估了使用纯粹的单语数据源训练双语和代码切换ASR模型的不同方法。我们引入了集合标记器的概念，它与目前主流技术在单语样本边界生成LID的方法不同，而是为每个发射的标记生成LID。我们比较了双语和单语模型的性能，展示了集合标记器的有效性，提出了一种合成的代码切换ASR数据生成技术，并证明了所提出的代码切换ASR模型在语音任务中的有效性。",
    "tldr": "本文介绍了一种使用纯粹的单语数据源训练双语和代码切换ASR模型的方法。通过引入集合标记器，将LID应用到每个标记，而不是在单语样本边界生成LID，我们展示了集合标记器的有效性，并提出了合成代码切换ASR数据生成技术，证明了所提出的代码切换ASR模型在语音任务中的有效性。",
    "en_tdlr": "This paper presents an approach for training bilingual and code-switched ASR models using solely monolingual data sources. By introducing aggregate tokenizers that generate language identification for each emitted token instead of at the boundaries of monolingual samples, the effectiveness of the aggregate tokenizers is demonstrated. Additionally, a synthetic code-switched ASR data generation technique is proposed, showcasing the effectiveness of the proposed code-switched ASR models for speech tasks."
}