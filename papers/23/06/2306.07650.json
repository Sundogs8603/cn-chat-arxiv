{
    "title": "Modality Adaption or Regularization? A Case Study on End-to-End Speech Translation. (arXiv:2306.07650v1 [cs.CL])",
    "abstract": "Pre-training and fine-tuning is a paradigm for alleviating the data scarcity problem in end-to-end speech translation (E2E ST). The commonplace \"modality gap\" between speech and text data often leads to inconsistent inputs between pre-training and fine-tuning. However, we observe that this gap occurs in the early stages of fine-tuning, but does not have a major impact on the final performance. On the other hand, we find that there has another gap, which we call the \"capacity gap\": high resource tasks (such as ASR and MT) always require a large model to fit, when the model is reused for a low resource task (E2E ST), it will get a sub-optimal performance due to the over-fitting. In a case study, we find that the regularization plays a more important role than the well-designed modality adaption method, which achieves 29.0 for en-de and 40.3 for en-fr on the MuST-C dataset. Code and models are available at https://github.com/hannlp/TAB.",
    "link": "http://arxiv.org/abs/2306.07650",
    "context": "Title: Modality Adaption or Regularization? A Case Study on End-to-End Speech Translation. (arXiv:2306.07650v1 [cs.CL])\nAbstract: Pre-training and fine-tuning is a paradigm for alleviating the data scarcity problem in end-to-end speech translation (E2E ST). The commonplace \"modality gap\" between speech and text data often leads to inconsistent inputs between pre-training and fine-tuning. However, we observe that this gap occurs in the early stages of fine-tuning, but does not have a major impact on the final performance. On the other hand, we find that there has another gap, which we call the \"capacity gap\": high resource tasks (such as ASR and MT) always require a large model to fit, when the model is reused for a low resource task (E2E ST), it will get a sub-optimal performance due to the over-fitting. In a case study, we find that the regularization plays a more important role than the well-designed modality adaption method, which achieves 29.0 for en-de and 40.3 for en-fr on the MuST-C dataset. Code and models are available at https://github.com/hannlp/TAB.",
    "path": "papers/23/06/2306.07650.json",
    "total_tokens": 947,
    "translated_title": "模态适应还是规范化？一项端到端语音翻译案例研究",
    "translated_abstract": "预训练和微调是缓解端到端语音翻译(E2E ST)中数据匮乏问题的一种范例。但是，常见的语音和文本数据之间的“模态差距”通常导致预训练和微调之间的不一致输入。然而，我们观察到这种差距发生在微调的早期阶段，但对最终性能没有重大影响。另一方面，我们发现存在另一个差距，我们称之为“能力差距”：高资源任务（如ASR和MT）总是需要一个大型模型来拟合，当模型被重用于低资源任务（E2E ST）时，由于过度拟合会导致次优性能。在一项案例研究中，我们发现规范化方法比精心设计的模态适应方法发挥更重要的作用，该方法在MuST-C数据集上实现了29.0 en-de和40.3 en-fr的效果。代码和模型可在https://github.com/hannlp/TAB上找到。",
    "tldr": "该文讨论了解决端到端语音翻译中的数据匮乏问题的预训练和微调方法，并发现规范化方法比精心设计的模态适应方法更重要，能够优化E2E ST的性能。",
    "en_tdlr": "This paper discusses pre-training and fine-tuning methods to alleviate the data scarcity problem in end-to-end speech translation, and finds that regularization is more important than modality adaption in optimizing the performance of E2E ST."
}