{
    "title": "Evaluating GPT's Programming Capability through CodeWars' Katas. (arXiv:2306.01784v1 [cs.AI])",
    "abstract": "In the burgeoning field of artificial intelligence (AI), understanding the capabilities and limitations of programming-oriented models is crucial. This paper presents a novel evaluation of the programming proficiency of Generative Pretrained Transformer (GPT) models, specifically GPT-3.5 and GPT-4, against coding problems of varying difficulty levels drawn from Codewars. The experiments reveal a distinct boundary at the 3kyu level, beyond which these GPT models struggle to provide solutions. These findings led to the proposal of a measure for coding problem complexity that incorporates both problem difficulty and the time required for solution. The research emphasizes the need for validation and creative thinking capabilities in AI models to better emulate human problem-solving techniques. Future work aims to refine this proposed complexity measure, enhance AI models with these suggested capabilities, and develop an objective measure for programming problem difficulty. The results of t",
    "link": "http://arxiv.org/abs/2306.01784",
    "context": "Title: Evaluating GPT's Programming Capability through CodeWars' Katas. (arXiv:2306.01784v1 [cs.AI])\nAbstract: In the burgeoning field of artificial intelligence (AI), understanding the capabilities and limitations of programming-oriented models is crucial. This paper presents a novel evaluation of the programming proficiency of Generative Pretrained Transformer (GPT) models, specifically GPT-3.5 and GPT-4, against coding problems of varying difficulty levels drawn from Codewars. The experiments reveal a distinct boundary at the 3kyu level, beyond which these GPT models struggle to provide solutions. These findings led to the proposal of a measure for coding problem complexity that incorporates both problem difficulty and the time required for solution. The research emphasizes the need for validation and creative thinking capabilities in AI models to better emulate human problem-solving techniques. Future work aims to refine this proposed complexity measure, enhance AI models with these suggested capabilities, and develop an objective measure for programming problem difficulty. The results of t",
    "path": "papers/23/06/2306.01784.json",
    "total_tokens": 943,
    "translated_title": "通过Codewars编程问题评估GPT模型的编程能力",
    "translated_abstract": "在人工智能领域，理解面向编程的模型的能力和局限性至关重要。本文提出了一种新颖的方法，通过对来自Codewars的不同难度级别的编程问题进行评估，评估生成预训练转换器（GPT）模型的编程能力，特别是GPT-3.5和GPT-4。实验揭示了一个显著的边界，即超过3kyu级别，这些GPT模型难以提供解决方案。这些发现引发了一个考虑问题难度和解决方案所需时间的编程问题复杂度度量提议。研究强调了在人工智能模型中需要验证和创造性思维能力，以更好地模拟人类解决问题的技巧。未来的工作旨在完善这个所提议的复杂度度量、增强AI模型的这些建议能力，并开发一个衡量编程问题难度的客观标准。",
    "tldr": "本文在Codewars上对GPT-3.5和GPT-4模型的编程能力进行了评估，发现它们在3 kyu级别以上的问题上遇到困难。作者提出了一个综合考虑问题难度和所需时间的编程问题复杂度度量，并强调AI模型需要验证和创造性思维能力。",
    "en_tdlr": "This paper evaluates the programming proficiency of GPT models, namely GPT-3.5 and GPT-4, on Codewars coding problems of varying difficulty levels. It proposes a measure for coding problem complexity and highlights the need for validation and creative thinking capabilities in AI models to better emulate human problem-solving techniques. The experiments reveal that these GPT models struggle to provide solutions beyond the 3kyu level."
}