{
    "title": "On Masked Pre-training and the Marginal Likelihood. (arXiv:2306.00520v1 [stat.ML])",
    "abstract": "Masked pre-training removes random input dimensions and learns a model that can predict the missing values. Empirical results indicate that this intuitive form of self-supervised learning yields models that generalize very well to new domains. A theoretical understanding is, however, lacking. This paper shows that masked pre-training with a suitable cumulative scoring function corresponds to maximizing the model's marginal likelihood, which is de facto the Bayesian model selection measure of generalization. Beyond shedding light on the success of masked pre-training, this insight also suggests that Bayesian models can be trained with appropriately designed self-supervision. Empirically, we confirm the developed theory and explore the main learning principles of masked pre-training in large language models.",
    "link": "http://arxiv.org/abs/2306.00520",
    "context": "Title: On Masked Pre-training and the Marginal Likelihood. (arXiv:2306.00520v1 [stat.ML])\nAbstract: Masked pre-training removes random input dimensions and learns a model that can predict the missing values. Empirical results indicate that this intuitive form of self-supervised learning yields models that generalize very well to new domains. A theoretical understanding is, however, lacking. This paper shows that masked pre-training with a suitable cumulative scoring function corresponds to maximizing the model's marginal likelihood, which is de facto the Bayesian model selection measure of generalization. Beyond shedding light on the success of masked pre-training, this insight also suggests that Bayesian models can be trained with appropriately designed self-supervision. Empirically, we confirm the developed theory and explore the main learning principles of masked pre-training in large language models.",
    "path": "papers/23/06/2306.00520.json",
    "total_tokens": 861,
    "translated_title": "关于遮蔽预训练和边缘似然的研究",
    "translated_abstract": "遮蔽预训练是一种移除随机输入维度并学习可以预测缺失值的模型的自我监督学习方法。实证结果表明，这种直观的自我监督学习方法可以生成具有很好泛化性能的模型。然而，还缺乏对其理论的深入理解。本文证明了通过适当的积分打分函数进行的遮蔽预训练可以对模型的边缘似然极大化。边缘似然实际上是广义化模型选择的贝叶斯度量。除了揭示遮蔽预训练成功的原因，本文的理论还建议可以设计适当的自监督方法来训练贝叶斯模型。我们在大型语言模型中实验验证了所开发的理论并探索了遮蔽预训练的主要学习原理。",
    "tldr": "遮蔽预训练利用累积打分函数实现了对模型的边缘似然极大化，提出了设计适当的自监督方法来训练贝叶斯模型的理论。实验证实了这一理论，并探索了遮蔽预训练的主要学习原理。",
    "en_tdlr": "Masked pre-training achieves maximizing the model's marginal likelihood by using a cumulative scoring function, which sheds light on the success of masked pre-training and suggests designing appropriately supervised methods for Bayesian models. Experimental results confirm the theory and explore the main learning principles of masked pre-training in large language models."
}