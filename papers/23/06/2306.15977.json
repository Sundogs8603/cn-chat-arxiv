{
    "title": "A Dimensional Structure based Knowledge Distillation Method for Cross-Modal Learning. (arXiv:2306.15977v1 [cs.CV])",
    "abstract": "Due to limitations in data quality, some essential visual tasks are difficult to perform independently. Introducing previously unavailable information to transfer informative dark knowledge has been a common way to solve such hard tasks. However, research on why transferred knowledge works has not been extensively explored. To address this issue, in this paper, we discover the correlation between feature discriminability and dimensional structure (DS) by analyzing and observing features extracted from simple and hard tasks. On this basis, we express DS using deep channel-wise correlation and intermediate spatial distribution, and propose a novel cross-modal knowledge distillation (CMKD) method for better supervised cross-modal learning (CML) performance. The proposed method enforces output features to be channel-wise independent and intermediate ones to be uniformly distributed, thereby learning semantically irrelevant features from the hard task to boost its accuracy. This is especial",
    "link": "http://arxiv.org/abs/2306.15977",
    "context": "Title: A Dimensional Structure based Knowledge Distillation Method for Cross-Modal Learning. (arXiv:2306.15977v1 [cs.CV])\nAbstract: Due to limitations in data quality, some essential visual tasks are difficult to perform independently. Introducing previously unavailable information to transfer informative dark knowledge has been a common way to solve such hard tasks. However, research on why transferred knowledge works has not been extensively explored. To address this issue, in this paper, we discover the correlation between feature discriminability and dimensional structure (DS) by analyzing and observing features extracted from simple and hard tasks. On this basis, we express DS using deep channel-wise correlation and intermediate spatial distribution, and propose a novel cross-modal knowledge distillation (CMKD) method for better supervised cross-modal learning (CML) performance. The proposed method enforces output features to be channel-wise independent and intermediate ones to be uniformly distributed, thereby learning semantically irrelevant features from the hard task to boost its accuracy. This is especial",
    "path": "papers/23/06/2306.15977.json",
    "total_tokens": 912,
    "translated_title": "基于维度结构的跨模态学习的知识蒸馏方法",
    "translated_abstract": "由于数据质量的限制，一些重要的视觉任务很难独立完成。引入先前不可用的信息以转移有信息的黑暗知识已成为解决这些困难任务的常用方法。然而，关于为什么转移知识有效的研究还没有广泛探索。为了解决这个问题，在本文中，我们通过分析和观察从简单任务和困难任务中提取的特征，发现了特征可辨别性和维度结构（DS）之间的相关性。在此基础上，我们使用深度通道相关性和中间空间分布来表示DS，并提出了一种新颖的跨模态知识蒸馏（CMKD）方法，以提高监督式跨模态学习（CML）性能。所提出的方法强制输出特征是通道独立的，且中间特征是均匀分布的，从而从困难任务中学习语义不相关的特征，以提高其准确性。",
    "tldr": "本文通过分析和观察从简单和困难任务中提取的特征，发现了特征可辨别性和维度结构之间的相关性，并提出了一种基于维度结构的跨模态学习的知识蒸馏方法，以提高跨模态学习性能。"
}