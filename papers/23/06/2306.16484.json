{
    "title": "Towards a Better Theoretical Understanding of Independent Subnetwork Training. (arXiv:2306.16484v1 [cs.LG])",
    "abstract": "Modern advancements in large-scale machine learning would be impossible without the paradigm of data-parallel distributed computing. Since distributed computing with large-scale models imparts excessive pressure on communication channels, significant recent research has been directed toward co-designing communication compression strategies and training algorithms with the goal of reducing communication costs. While pure data parallelism allows better data scaling, it suffers from poor model scaling properties. Indeed, compute nodes are severely limited by memory constraints, preventing further increases in model size. For this reason, the latest achievements in training giant neural network models also rely on some form of model parallelism. In this work, we take a closer theoretical look at Independent Subnetwork Training (IST), which is a recently proposed and highly effective technique for solving the aforementioned problems. We identify fundamental differences between IST and alter",
    "link": "http://arxiv.org/abs/2306.16484",
    "context": "Title: Towards a Better Theoretical Understanding of Independent Subnetwork Training. (arXiv:2306.16484v1 [cs.LG])\nAbstract: Modern advancements in large-scale machine learning would be impossible without the paradigm of data-parallel distributed computing. Since distributed computing with large-scale models imparts excessive pressure on communication channels, significant recent research has been directed toward co-designing communication compression strategies and training algorithms with the goal of reducing communication costs. While pure data parallelism allows better data scaling, it suffers from poor model scaling properties. Indeed, compute nodes are severely limited by memory constraints, preventing further increases in model size. For this reason, the latest achievements in training giant neural network models also rely on some form of model parallelism. In this work, we take a closer theoretical look at Independent Subnetwork Training (IST), which is a recently proposed and highly effective technique for solving the aforementioned problems. We identify fundamental differences between IST and alter",
    "path": "papers/23/06/2306.16484.json",
    "total_tokens": 835,
    "translated_title": "朝着对独立子网络训练的更好理论理解迈进 (arXiv:2306.16484v1 [cs.LG])",
    "translated_abstract": "现代大规模机器学习的进展离不开数据并行分布式计算的范式。由于大规模模型的分布式计算对通信通道施加了巨大压力，最近的研究主要集中在共同设计通信压缩策略和训练算法，以降低通信成本。尽管纯数据并行性允许更好的数据扩展性，但其在模型扩展性方面存在问题。事实上，计算节点受内存限制严重限制，阻止了模型尺寸的进一步增加。因此，训练巨型神经网络模型的最新成果也依赖于某种形式的模型并行性。在这项工作中，我们对独立子网络训练（IST）进行了更详细的理论研究，这是一种最近提出的高效解决上述问题的技术。我们发现IST和其他模型并行方法之间存在根本性的差异。",
    "tldr": "本研究对独立子网络训练（IST）进行了理论分析，发现了IST与其他模型并行方法之间的根本差异。",
    "en_tdlr": "This paper provides a theoretical analysis of Independent Subnetwork Training (IST) and identifies fundamental differences between IST and other model parallel methods."
}