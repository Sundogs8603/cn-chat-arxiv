{
    "title": "CMMLU: Measuring massive multitask language understanding in Chinese. (arXiv:2306.09212v2 [cs.CL] UPDATED)",
    "abstract": "As the capabilities of large language models (LLMs) continue to advance, evaluating their performance becomes increasingly crucial and challenging. This paper aims to bridge this gap by introducing CMMLU, a comprehensive Chinese benchmark that covers various subjects, including natural science, social sciences, engineering, and humanities. We conduct a thorough evaluation of 18 advanced multilingual- and Chinese-oriented LLMs, assessing their performance across different subjects and settings. The results reveal that most existing LLMs struggle to achieve an average accuracy of 50%, even when provided with in-context examples and chain-of-thought prompts, whereas the random baseline stands at 25%. This highlights significant room for improvement in LLMs. Additionally, we conduct extensive experiments to identify factors impacting the models' performance and propose directions for enhancing LLMs. CMMLU fills the gap in evaluating the knowledge and reasoning capabilities of large languag",
    "link": "http://arxiv.org/abs/2306.09212",
    "context": "Title: CMMLU: Measuring massive multitask language understanding in Chinese. (arXiv:2306.09212v2 [cs.CL] UPDATED)\nAbstract: As the capabilities of large language models (LLMs) continue to advance, evaluating their performance becomes increasingly crucial and challenging. This paper aims to bridge this gap by introducing CMMLU, a comprehensive Chinese benchmark that covers various subjects, including natural science, social sciences, engineering, and humanities. We conduct a thorough evaluation of 18 advanced multilingual- and Chinese-oriented LLMs, assessing their performance across different subjects and settings. The results reveal that most existing LLMs struggle to achieve an average accuracy of 50%, even when provided with in-context examples and chain-of-thought prompts, whereas the random baseline stands at 25%. This highlights significant room for improvement in LLMs. Additionally, we conduct extensive experiments to identify factors impacting the models' performance and propose directions for enhancing LLMs. CMMLU fills the gap in evaluating the knowledge and reasoning capabilities of large languag",
    "path": "papers/23/06/2306.09212.json",
    "total_tokens": 937,
    "translated_title": "CMMLU：衡量中文大规模多任务语言理解",
    "translated_abstract": "随着大型语言模型（LLM）的能力不断提高，评估它们的性能变得越来越关键和具有挑战性。本论文通过引入CMMLU，一个包括自然科学、社会科学、工程和人文等多个领域的综合性中文基准测试，来弥合这一差距。我们对18种先进的多语言和面向中文的LLM进行了全面评价，评估它们在不同主题和设置下的性能。结果显示，大多数现有的LLM在提供上下文示例和思维链提示时，难以达到平均50%的准确率，而随机基准线则为25%。这突显了LLM的改进空间。此外，我们进行了大量实验，以确定影响模型性能的因素，并提出了增强LLM的方向。CMMLU填补了评估大型语言模型的知识和推理能力的空白。",
    "tldr": "本论文介绍了CMMLU，这是一个衡量中文大规模多任务语言理解的综合性基准测试。研究发现，大多数现有的语言模型在不同主题和设置下的性能都不够理想，存在改进空间。CMMLU填补了评估大型语言模型知识和推理能力的空白，并提出了增强LLM的方向。",
    "en_tdlr": "This paper presents CMMLU, a comprehensive Chinese benchmark for measuring massive multitask language understanding. It finds that most existing language models have room for improvement in their performance across different subjects and settings. CMMLU fills the gap in evaluating the knowledge and reasoning capabilities of large language models and proposes directions for enhancing LLMs."
}