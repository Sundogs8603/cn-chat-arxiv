{
    "title": "Self-supervised Audio Teacher-Student Transformer for Both Clip-level and Frame-level Tasks. (arXiv:2306.04186v1 [eess.AS])",
    "abstract": "In recent years, self-supervised learning (SSL) has emerged as a popular approach for learning audio representations. The ultimate goal of audio self-supervised pre-training is to transfer knowledge to downstream audio tasks, generally including clip-level and frame-level tasks. Clip-level tasks classify the scene or sound of an entire audio clip, e.g. audio tagging, instrument recognition, etc. While frame-level tasks detect event-level timestamps from an audio clip, e.g. sound event detection, speaker diarization, etc. Prior studies primarily evaluate on clip-level downstream tasks. Frame-level tasks are important for fine-grained acoustic scene/event understanding, and are generally more challenging than clip-level tasks. In order to tackle both clip-level and frame-level tasks, this paper proposes two self-supervised audio representation learning methods: ATST-Clip and ATST-Frame, responsible for learning clip-level and frame-level representations, respectively. ATST stands for Aud",
    "link": "http://arxiv.org/abs/2306.04186",
    "context": "Title: Self-supervised Audio Teacher-Student Transformer for Both Clip-level and Frame-level Tasks. (arXiv:2306.04186v1 [eess.AS])\nAbstract: In recent years, self-supervised learning (SSL) has emerged as a popular approach for learning audio representations. The ultimate goal of audio self-supervised pre-training is to transfer knowledge to downstream audio tasks, generally including clip-level and frame-level tasks. Clip-level tasks classify the scene or sound of an entire audio clip, e.g. audio tagging, instrument recognition, etc. While frame-level tasks detect event-level timestamps from an audio clip, e.g. sound event detection, speaker diarization, etc. Prior studies primarily evaluate on clip-level downstream tasks. Frame-level tasks are important for fine-grained acoustic scene/event understanding, and are generally more challenging than clip-level tasks. In order to tackle both clip-level and frame-level tasks, this paper proposes two self-supervised audio representation learning methods: ATST-Clip and ATST-Frame, responsible for learning clip-level and frame-level representations, respectively. ATST stands for Aud",
    "path": "papers/23/06/2306.04186.json",
    "total_tokens": 935,
    "translated_title": "自监督音频师生Transformer用于片段级和帧级任务",
    "translated_abstract": "近年来，自监督学习已成为学习音频表示的流行方法。音频自监督预训练的最终目标是将知识传递到下游音频任务中，通常包括片段级和帧级任务。为了应对这两种任务，本文提出了两种自监督音频表示学习方法：ATST-Clip和ATST-Frame，分别用于学习片段级和帧级表示。这两种方法都使用基于Transformer的师生模型，并通过音频实例辨别和音频序列重构两个自监督任务进行训练。评估结果显示，我们提出的方法在片段级和帧级下游任务上均优于几个最先进的预训练方法。",
    "tldr": "本文提出了两种自监督音频表示学习方法：ATST-Clip和ATST-Frame；这两种方法使用基于Transformer的师生模型，通过音频实例辨别和音频序列重构两个自监督任务进行训练；评估结果表明，这两种方法在片段级和帧级下游任务上都优于其他最先进的预训练方法。",
    "en_tdlr": "This paper proposes two self-supervised audio representation learning methods: ATST-Clip and ATST-Frame; both use Transformer-based teacher and student models, and are trained on two self-supervised tasks: audio instance discrimination and audio sequence reconstruction. Evaluations show that these methods outperform several state-of-the-art pre-training methods on both clip-level and frame-level downstream tasks."
}