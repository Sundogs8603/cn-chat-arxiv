{
    "title": "Statler: State-Maintaining Language Models for Embodied Reasoning. (arXiv:2306.17840v1 [cs.RO])",
    "abstract": "Large language models (LLMs) provide a promising tool that enable robots to perform complex robot reasoning tasks. However, the limited context window of contemporary LLMs makes reasoning over long time horizons difficult. Embodied tasks such as those that one might expect a household robot to perform typically require that the planner consider information acquired a long time ago (e.g., properties of the many objects that the robot previously encountered in the environment). Attempts to capture the world state using an LLM's implicit internal representation is complicated by the paucity of task- and environment-relevant information available in a robot's action history, while methods that rely on the ability to convey information via the prompt to the LLM are subject to its limited context window. In this paper, we propose Statler, a framework that endows LLMs with an explicit representation of the world state as a form of ``memory'' that is maintained over time. Integral to Statler i",
    "link": "http://arxiv.org/abs/2306.17840",
    "context": "Title: Statler: State-Maintaining Language Models for Embodied Reasoning. (arXiv:2306.17840v1 [cs.RO])\nAbstract: Large language models (LLMs) provide a promising tool that enable robots to perform complex robot reasoning tasks. However, the limited context window of contemporary LLMs makes reasoning over long time horizons difficult. Embodied tasks such as those that one might expect a household robot to perform typically require that the planner consider information acquired a long time ago (e.g., properties of the many objects that the robot previously encountered in the environment). Attempts to capture the world state using an LLM's implicit internal representation is complicated by the paucity of task- and environment-relevant information available in a robot's action history, while methods that rely on the ability to convey information via the prompt to the LLM are subject to its limited context window. In this paper, we propose Statler, a framework that endows LLMs with an explicit representation of the world state as a form of ``memory'' that is maintained over time. Integral to Statler i",
    "path": "papers/23/06/2306.17840.json",
    "total_tokens": 839,
    "translated_title": "Statler：用于具身推理的保持状态的语言模型",
    "translated_abstract": "大型语言模型（LLMs）为机器人执行复杂的机器人推理任务提供了一种有希望的工具。然而，当代LLMs的有限上下文窗口使得在长时间范围内进行推理变得困难。具身任务（例如我们期望一个家庭机器人执行的任务）通常需要规划者考虑很久之前获得的信息（例如，机器人在环境中遇到的许多对象的属性）。通过LLM的隐含内部表示来捕获世界状态的尝试会因为机器人操作历史中可用的与任务和环境相关的信息有限而变得复杂，而依赖通过提示向LLM传递信息的方法则受其有限的上下文窗口的限制。在本文中，我们提出了Statler，一个为LLMs赋予了明确的、作为“记忆”的世界状态表示的框架，这种记忆随时间保持。",
    "tldr": "Statler是一个为LLMs赋予了明确的、维持状态的语言模型，可以解决当代LLMs在长时间范围内推理的困难。",
    "en_tdlr": "Statler is a framework that endows LLMs with an explicit, state-maintaining language model, addressing the challenge of long-term reasoning in contemporary LLMs."
}