{
    "title": "Learning When to Speak: Latency and Quality Trade-offs for Simultaneous Speech-to-Speech Translation with Offline Models. (arXiv:2306.01201v1 [cs.CL])",
    "abstract": "Recent work in speech-to-speech translation (S2ST) has focused primarily on offline settings, where the full input utterance is available before any output is given. This, however, is not reasonable in many real-world scenarios. In latency-sensitive applications, rather than waiting for the full utterance, translations should be spoken as soon as the information in the input is present. In this work, we introduce a system for simultaneous S2ST targeting real-world use cases. Our system supports translation from 57 languages to English with tunable parameters for dynamically adjusting the latency of the output -- including four policies for determining when to speak an output sequence. We show that these policies achieve offline-level accuracy with minimal increases in latency over a Greedy (wait-$k$) baseline. We open-source our evaluation code and interactive test script to aid future SimulS2ST research and application development.",
    "link": "http://arxiv.org/abs/2306.01201",
    "context": "Title: Learning When to Speak: Latency and Quality Trade-offs for Simultaneous Speech-to-Speech Translation with Offline Models. (arXiv:2306.01201v1 [cs.CL])\nAbstract: Recent work in speech-to-speech translation (S2ST) has focused primarily on offline settings, where the full input utterance is available before any output is given. This, however, is not reasonable in many real-world scenarios. In latency-sensitive applications, rather than waiting for the full utterance, translations should be spoken as soon as the information in the input is present. In this work, we introduce a system for simultaneous S2ST targeting real-world use cases. Our system supports translation from 57 languages to English with tunable parameters for dynamically adjusting the latency of the output -- including four policies for determining when to speak an output sequence. We show that these policies achieve offline-level accuracy with minimal increases in latency over a Greedy (wait-$k$) baseline. We open-source our evaluation code and interactive test script to aid future SimulS2ST research and application development.",
    "path": "papers/23/06/2306.01201.json",
    "total_tokens": 889,
    "translated_title": "学习何时说话：离线模型进行同时语音翻译的延迟与质量权衡",
    "translated_abstract": "最近的语音翻译研究主要集中在离线场景中，在此场景中，完整的输入话语在任何输出之前都是可用的。然而，在许多实际应用场景中，这并不合理。在对延迟敏感的应用程序中，翻译应该在输入信息出现时立即发音。我们引入了一个系统，用于同时语音翻译的实际用例。我们的系统支持从57种语言翻译成英语，并具有调整输出延迟的可调参数。我们展示了这些策略达到了离线水平的准确性，同时在Greedy（wait-$k$）基线上最小化了延迟增加。我们开源了我们的评估代码和互动测试脚本，以帮助未来的SimulS2ST研究和应用程序发展。",
    "tldr": "本文提出了一个适用于实际场景的同时语音翻译系统，支持从57种语言翻译成英语，并具有调整输出延迟的可调参数。我们展示了可以在不增加显著延迟的情况下达到离线水平的准确性。",
    "en_tdlr": "The paper proposes a simultaneous speech-to-speech translation system for real-world scenarios, supporting translation from 57 languages to English with tunable parameters for dynamically adjusting the latency of the output. The paper shows that the proposed policies achieve offline-level accuracy with minimal increases in latency."
}