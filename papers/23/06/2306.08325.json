{
    "title": "GCformer: An Efficient Framework for Accurate and Scalable Long-Term Multivariate Time Series Forecasting. (arXiv:2306.08325v2 [cs.LG] UPDATED)",
    "abstract": "Transformer-based models have emerged as promising tools for time series forecasting.  However, these model cannot make accurate prediction for long input time series. On the one hand, they failed to capture global dependencies within time series data. On the other hand, the long input sequence usually leads to large model size and high time complexity.  To address these limitations, we present GCformer, which combines a structured global convolutional branch for processing long input sequences with a local Transformer-based branch for capturing short, recent signals. A cohesive framework for a global convolution kernel has been introduced, utilizing three distinct parameterization methods. The selected structured convolutional kernel in the global branch has been specifically crafted with sublinear complexity, thereby allowing for the efficient and effective processing of lengthy and noisy input signals. Empirical studies on six benchmark datasets demonstrate that GCformer outperforms",
    "link": "http://arxiv.org/abs/2306.08325",
    "context": "Title: GCformer: An Efficient Framework for Accurate and Scalable Long-Term Multivariate Time Series Forecasting. (arXiv:2306.08325v2 [cs.LG] UPDATED)\nAbstract: Transformer-based models have emerged as promising tools for time series forecasting.  However, these model cannot make accurate prediction for long input time series. On the one hand, they failed to capture global dependencies within time series data. On the other hand, the long input sequence usually leads to large model size and high time complexity.  To address these limitations, we present GCformer, which combines a structured global convolutional branch for processing long input sequences with a local Transformer-based branch for capturing short, recent signals. A cohesive framework for a global convolution kernel has been introduced, utilizing three distinct parameterization methods. The selected structured convolutional kernel in the global branch has been specifically crafted with sublinear complexity, thereby allowing for the efficient and effective processing of lengthy and noisy input signals. Empirical studies on six benchmark datasets demonstrate that GCformer outperforms",
    "path": "papers/23/06/2306.08325.json",
    "total_tokens": 861,
    "translated_title": "GCformer:一种用于准确和可扩展的长期多元时间序列预测的高效框架",
    "translated_abstract": "基于Transformer的模型已经成为时间序列预测的有希望的工具。然而，这些模型对于长输入时间序列的预测不够准确。一方面，它们未能捕捉时间序列数据中的全局依赖关系。另一方面，长输入序列通常导致模型尺寸大和时间复杂度高。为了解决这些限制，我们提出了GCformer，它将用于处理长输入序列的结构化全局卷积分支与用于捕捉短期近期信号的局部Transformer分支相结合。引入了一个全局卷积核的连贯框架，利用了三种不同的参数化方法。全局分支中选择的结构化卷积核经过特殊设计，具有亚线性复杂度，从而可以高效和有效地处理长而嘈杂的输入信号。对六个基准数据集进行的实证研究表明，GCformer的性能优于其他方法。",
    "tldr": "GCformer是一个结合了全局卷积和局部Transformer分支的框架，旨在解决长期多元时间序列预测的精确性和可扩展性问题。通过引入具有亚线性复杂度的结构化卷积核，GCformer在各种基准数据集上实现了优于其他方法的性能。"
}