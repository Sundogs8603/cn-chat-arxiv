{
    "title": "Evaluation of Popular XAI Applied to Clinical Prediction Models: Can They be Trusted?. (arXiv:2306.11985v1 [cs.LG])",
    "abstract": "The absence of transparency and explainability hinders the clinical adoption of Machine learning (ML) algorithms. Although various methods of explainable artificial intelligence (XAI) have been suggested, there is a lack of literature that delves into their practicality and assesses them based on criteria that could foster trust in clinical environments. To address this gap this study evaluates two popular XAI methods used for explaining predictive models in the healthcare context in terms of whether they (i) generate domain-appropriate representation, i.e. coherent with respect to the application task, (ii) impact clinical workflow and (iii) are consistent. To that end, explanations generated at the cohort and patient levels were analysed. The paper reports the first benchmarking of the XAI methods applied to risk prediction models obtained by evaluating the concordance between generated explanations and the trigger of a future clinical deterioration episode recorded by the data colle",
    "link": "http://arxiv.org/abs/2306.11985",
    "context": "Title: Evaluation of Popular XAI Applied to Clinical Prediction Models: Can They be Trusted?. (arXiv:2306.11985v1 [cs.LG])\nAbstract: The absence of transparency and explainability hinders the clinical adoption of Machine learning (ML) algorithms. Although various methods of explainable artificial intelligence (XAI) have been suggested, there is a lack of literature that delves into their practicality and assesses them based on criteria that could foster trust in clinical environments. To address this gap this study evaluates two popular XAI methods used for explaining predictive models in the healthcare context in terms of whether they (i) generate domain-appropriate representation, i.e. coherent with respect to the application task, (ii) impact clinical workflow and (iii) are consistent. To that end, explanations generated at the cohort and patient levels were analysed. The paper reports the first benchmarking of the XAI methods applied to risk prediction models obtained by evaluating the concordance between generated explanations and the trigger of a future clinical deterioration episode recorded by the data colle",
    "path": "papers/23/06/2306.11985.json",
    "total_tokens": 782,
    "translated_title": "应用于临床预测模型的热门XAI的评估：它们可信吗？",
    "translated_abstract": "缺乏透明度和可解释性阻碍了机器学习（ML）算法在临床上的采用。本研究评估了用于解释医疗领域预测模型的两种流行的可解释人工智能（XAI）方法，以确定它们是否产生特定应用任务相关的领域适当的表示，是否会影响临床工作流程并保持一致性。因此，分析了在群体和患者水平上生成的解释。本文报道了第一次将XAI方法应用于风险预测模型的基准测试，评估了生成的解释与未来临床恶化事件的触发器之间的一致性。",
    "tldr": "该论文评估了两种流行的解释人工智能方法，分析它们是否为临床决策提供可信的解释和适当的领域表示。",
    "en_tdlr": "This paper evaluates two popular XAI methods used for explaining predictive models in healthcare and analyses their ability to provide trusted explanations and appropriate domain representation for clinical decision-making."
}