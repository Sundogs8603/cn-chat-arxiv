{
    "title": "Correcting discount-factor mismatch in on-policy policy gradient methods. (arXiv:2306.13284v1 [cs.LG])",
    "abstract": "The policy gradient theorem gives a convenient form of the policy gradient in terms of three factors: an action value, a gradient of the action likelihood, and a state distribution involving discounting called the \\emph{discounted stationary distribution}. But commonly used on-policy methods based on the policy gradient theorem ignores the discount factor in the state distribution, which is technically incorrect and may even cause degenerate learning behavior in some environments. An existing solution corrects this discrepancy by using $\\gamma^t$ as a factor in the gradient estimate. However, this solution is not widely adopted and does not work well in tasks where the later states are similar to earlier states. We introduce a novel distribution correction to account for the discounted stationary distribution that can be plugged into many existing gradient estimators. Our correction circumvents the performance degradation associated with the $\\gamma^t$ correction with a lower variance.",
    "link": "http://arxiv.org/abs/2306.13284",
    "context": "Title: Correcting discount-factor mismatch in on-policy policy gradient methods. (arXiv:2306.13284v1 [cs.LG])\nAbstract: The policy gradient theorem gives a convenient form of the policy gradient in terms of three factors: an action value, a gradient of the action likelihood, and a state distribution involving discounting called the \\emph{discounted stationary distribution}. But commonly used on-policy methods based on the policy gradient theorem ignores the discount factor in the state distribution, which is technically incorrect and may even cause degenerate learning behavior in some environments. An existing solution corrects this discrepancy by using $\\gamma^t$ as a factor in the gradient estimate. However, this solution is not widely adopted and does not work well in tasks where the later states are similar to earlier states. We introduce a novel distribution correction to account for the discounted stationary distribution that can be plugged into many existing gradient estimators. Our correction circumvents the performance degradation associated with the $\\gamma^t$ correction with a lower variance.",
    "path": "papers/23/06/2306.13284.json",
    "total_tokens": 890,
    "translated_title": "改正策略梯度算法中折扣因子不匹配的问题",
    "translated_abstract": "策略梯度定理提供了一种方便的策略梯度形式，包括三个因素: 动作值、动作似然梯度和折扣利润。但是，基于策略梯度定理的常用的进策略方法忽略了状态分布中的折扣因子，这是技术上的错误，在某些环境下甚至可能引发退化的学习行为。既有的解决方案通过在梯度估计中使用 $\\gamma^t$ 作为因子来纠正此 discrepency。然而，这种解决方案并不被广泛采用，并且在后续状态类似于前面状态的任务中表现不佳。我们引入了一种新颖的分布校正方法来解决折扣稳态分布问题，可以插入到许多现有的梯度估计器中。我们的校正方法在方差更低的情况下避免了与 $\\gamma^t$ 校正相关的性能下降。",
    "tldr": "该论文提出了一种改进的算法来解决策略梯度算法中折扣因子不匹配的问题。该算法适用于许多现有的梯度估计器，避免了性能下降的问题。",
    "en_tdlr": "This paper proposes an improved algorithm to correct the discount-factor mismatch in on-policy policy gradient methods. The algorithm can be used with many existing gradient estimators and avoids the problem of performance degradation."
}