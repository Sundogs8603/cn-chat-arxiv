{
    "title": "Area is all you need: repeatable elements make stronger adversarial attacks. (arXiv:2306.07768v1 [cs.CV])",
    "abstract": "Over the last decade, deep neural networks have achieved state of the art in computer vision tasks. These models, however, are susceptible to unusual inputs, known as adversarial examples, that cause them to misclassify or otherwise fail to detect objects. Here, we provide evidence that the increasing success of adversarial attacks is primarily due to increasing their size. We then demonstrate a method for generating the largest possible adversarial patch by building a adversarial pattern out of repeatable elements. This approach achieves a new state of the art in evading detection by YOLOv2 and YOLOv3. Finally, we present an experiment that fails to replicate the prior success of several attacks published in this field, and end with some comments on testing and reproducibility.",
    "link": "http://arxiv.org/abs/2306.07768",
    "context": "Title: Area is all you need: repeatable elements make stronger adversarial attacks. (arXiv:2306.07768v1 [cs.CV])\nAbstract: Over the last decade, deep neural networks have achieved state of the art in computer vision tasks. These models, however, are susceptible to unusual inputs, known as adversarial examples, that cause them to misclassify or otherwise fail to detect objects. Here, we provide evidence that the increasing success of adversarial attacks is primarily due to increasing their size. We then demonstrate a method for generating the largest possible adversarial patch by building a adversarial pattern out of repeatable elements. This approach achieves a new state of the art in evading detection by YOLOv2 and YOLOv3. Finally, we present an experiment that fails to replicate the prior success of several attacks published in this field, and end with some comments on testing and reproducibility.",
    "path": "papers/23/06/2306.07768.json",
    "total_tokens": 864,
    "translated_title": "区域是你所需要的一切：重复元素使对抗性攻击更强大",
    "translated_abstract": "近十年来，深度神经网络在计算机视觉任务中取得了最先进的成果。然而，这些模型容易受到非正常输入（称为对抗性示例）的影响，导致它们错误分类或无法检测到物体。在本文中，我们提出了证据表明，对抗性攻击变得越来越成功主要是由于攻击尺寸的增加。然后，我们展示了一种通过构建可重复元素的对抗性图案来生成最大可能的对抗性补丁的方法。这种方法在躲避YOLOv2和YOLOv3检测方面实现了新的最先进。最后，我们提出了一个未能复制该领域中发布的几个攻击先前成功的实验，并以一些关于测试和可重复性的评论结束。",
    "tldr": "本文证明对抗性攻击的成功主要是由于攻击尺寸的增加，通过构建可重复元素的对抗性图案可以生成最大可能的对抗性补丁，并实现了躲避检测的新最先进。",
    "en_tdlr": "This paper demonstrates that the success of adversarial attacks is primarily due to increasing their size, and presents a method for generating the largest possible adversarial patch by building a adversarial pattern out of repeatable elements. The approach achieves a new state of the art in evading detection by YOLOv2 and YOLOv3. Additionally, the paper comments on testing and reproducibility, and presents an experiment that fails to replicate prior successful attacks."
}