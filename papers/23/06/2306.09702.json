{
    "title": "A Hierarchical Bayesian Model for Deep Few-Shot Meta Learning. (arXiv:2306.09702v1 [cs.LG])",
    "abstract": "We propose a novel hierarchical Bayesian model for learning with a large (possibly infinite) number of tasks/episodes, which suits well the few-shot meta learning problem. We consider episode-wise random variables to model episode-specific target generative processes, where these local random variables are governed by a higher-level global random variate. The global variable helps memorize the important information from historic episodes while controlling how much the model needs to be adapted to new episodes in a principled Bayesian manner. Within our model framework, the prediction on a novel episode/task can be seen as a Bayesian inference problem. However, a main obstacle in learning with a large/infinite number of local random variables in online nature, is that one is not allowed to store the posterior distribution of the current local random variable for frequent future updates, typical in conventional variational inference. We need to be able to treat each local variable as a o",
    "link": "http://arxiv.org/abs/2306.09702",
    "context": "Title: A Hierarchical Bayesian Model for Deep Few-Shot Meta Learning. (arXiv:2306.09702v1 [cs.LG])\nAbstract: We propose a novel hierarchical Bayesian model for learning with a large (possibly infinite) number of tasks/episodes, which suits well the few-shot meta learning problem. We consider episode-wise random variables to model episode-specific target generative processes, where these local random variables are governed by a higher-level global random variate. The global variable helps memorize the important information from historic episodes while controlling how much the model needs to be adapted to new episodes in a principled Bayesian manner. Within our model framework, the prediction on a novel episode/task can be seen as a Bayesian inference problem. However, a main obstacle in learning with a large/infinite number of local random variables in online nature, is that one is not allowed to store the posterior distribution of the current local random variable for frequent future updates, typical in conventional variational inference. We need to be able to treat each local variable as a o",
    "path": "papers/23/06/2306.09702.json",
    "total_tokens": 1104,
    "translated_title": "一种适用于深度少样本元学习的层级贝叶斯模型",
    "translated_abstract": "我们提出了一种新颖的层级贝叶斯模型，适用于具有大规模（可能是无限的）任务/情节的学习，很好地适应了少样本元学习问题。我们考虑以情节为基础的随机变量来模拟情节特定的目标生成过程，其中这些本地随机变量由更高级别的全局随机变量管理。全局变量有助于记住历史情节中的重要信息，同时以基于贝叶斯的原则控制模型需要适应新情节的程度。在我们的模型框架内，对于新的情节/任务的预测可以看作是一个贝叶斯推断问题。然而，使用大规模/无限数量的局部随机变量进行在线学习的一个主要障碍是，不能存储当前局部随机变量的后验分布以进行频繁的未来更新，而这在传统变分推断中很常见。我们需要能够将每个本地变量视为一次性学习问题，同时保持全局变量的影响，这使得使用在线推断成为必要。我们提出了一种高效的在线变分贝叶斯方法，具有计算时间和易于实现性的优点。通过实验，我们证明了我们的模型在少样本分类任务上优于几种现有方法。",
    "tldr": "我们提出了适用于少样本元学习问题的层级贝叶斯模型，并通过引入全局变量，以帮助记忆历史情节的重要信息，同时控制模型对新情节适应的程度。我们提出了一种在线变分贝叶斯方法，通过实验证明该模型在少样本分类任务上优于现有方法。",
    "en_tdlr": "We propose a hierarchical Bayesian model for the few-shot meta learning problem that incorporates a global variable to memorize important information from previous episodes, and an efficient online variational Bayes method for inference. This model outperforms existing methods in few-shot classification tasks according to experimental results."
}