{
    "title": "Edge Devices Inference Performance Comparison. (arXiv:2306.12093v1 [cs.LG])",
    "abstract": "In this work, we investigate the inference time of the MobileNet family, EfficientNet V1 and V2 family, VGG models, Resnet family, and InceptionV3 on four edge platforms. Specifically NVIDIA Jetson Nano, Intel Neural Stick, Google Coral USB Dongle, and Google Coral PCIe. Our main contribution is a thorough analysis of the aforementioned models in multiple settings, especially as a function of input size, the presence of the classification head, its size, and the scale of the model. Since throughout the industry, those architectures are mainly utilized as feature extractors we put our main focus on analyzing them as such. We show that Google platforms offer the fastest average inference time, especially for newer models like MobileNet or EfficientNet family, while Intel Neural Stick is the most universal accelerator allowing to run most architectures. These results should provide guidance for engineers in the early stages of AI edge systems development. All of them are accessible at htt",
    "link": "http://arxiv.org/abs/2306.12093",
    "context": "Title: Edge Devices Inference Performance Comparison. (arXiv:2306.12093v1 [cs.LG])\nAbstract: In this work, we investigate the inference time of the MobileNet family, EfficientNet V1 and V2 family, VGG models, Resnet family, and InceptionV3 on four edge platforms. Specifically NVIDIA Jetson Nano, Intel Neural Stick, Google Coral USB Dongle, and Google Coral PCIe. Our main contribution is a thorough analysis of the aforementioned models in multiple settings, especially as a function of input size, the presence of the classification head, its size, and the scale of the model. Since throughout the industry, those architectures are mainly utilized as feature extractors we put our main focus on analyzing them as such. We show that Google platforms offer the fastest average inference time, especially for newer models like MobileNet or EfficientNet family, while Intel Neural Stick is the most universal accelerator allowing to run most architectures. These results should provide guidance for engineers in the early stages of AI edge systems development. All of them are accessible at htt",
    "path": "papers/23/06/2306.12093.json",
    "total_tokens": 846,
    "translated_title": "边缘设备推理性能比较",
    "translated_abstract": "本研究探讨了MobileNet家族、EfficientNet V1和V2家族、VGG模型、Resnet家族和InceptionV3在四个边缘平台上的推理时间。具体而言，这些平台包括NVIDIA Jetson Nano、Intel Neural Stick、Google Coral USB Dongle和Google Coral PCIe。我们的主要贡献是对多种设置下所述模型的深入分析，特别是在输入大小、分类头的存在、其大小以及模型规模的影响下进行分析。由于在整个行业中，这些架构主要用作特征提取器，因此我们的重点是分析它们的特征提取性能。我们发现，Google平台提供了最快的平均推理时间，尤其是对于像MobileNet或EfficientNet家族这样的较新模型，而Intel Neural Stick是最通用的加速器，可以运行大多数架构。这些结果应为AI边缘系统开发的工程师提供指导。",
    "tldr": "本研究比较了多种边缘设备上常用的深度学习模型的推理性能，发现Google平台对于新型模型表现最佳，而Intel Neural Stick是最通用的加速器。"
}