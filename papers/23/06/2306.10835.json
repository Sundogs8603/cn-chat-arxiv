{
    "title": "Online Dynamic Submodular Optimization. (arXiv:2306.10835v2 [math.OC] UPDATED)",
    "abstract": "We propose new algorithms with provable performance for online binary optimization subject to general constraints and in dynamic settings. We consider the subset of problems in which the objective function is submodular. We propose the online submodular greedy algorithm (OSGA) which solves to optimality an approximation of the previous round loss function to avoid the NP-hardness of the original problem. We extend OSGA to a generic approximation function. We show that OSGA has a dynamic regret bound similar to the tightest bounds in online convex optimization with respect to the time horizon and the cumulative round optimum variation. For instances where no approximation exists or a computationally simpler implementation is desired, we design the online submodular projected gradient descent (OSPGD) by leveraging the Lova\\'sz extension. We obtain a regret bound that is akin to the conventional online gradient descent (OGD). Finally, we numerically test our algorithms in two power system",
    "link": "http://arxiv.org/abs/2306.10835",
    "context": "Title: Online Dynamic Submodular Optimization. (arXiv:2306.10835v2 [math.OC] UPDATED)\nAbstract: We propose new algorithms with provable performance for online binary optimization subject to general constraints and in dynamic settings. We consider the subset of problems in which the objective function is submodular. We propose the online submodular greedy algorithm (OSGA) which solves to optimality an approximation of the previous round loss function to avoid the NP-hardness of the original problem. We extend OSGA to a generic approximation function. We show that OSGA has a dynamic regret bound similar to the tightest bounds in online convex optimization with respect to the time horizon and the cumulative round optimum variation. For instances where no approximation exists or a computationally simpler implementation is desired, we design the online submodular projected gradient descent (OSPGD) by leveraging the Lova\\'sz extension. We obtain a regret bound that is akin to the conventional online gradient descent (OGD). Finally, we numerically test our algorithms in two power system",
    "path": "papers/23/06/2306.10835.json",
    "total_tokens": 901,
    "translated_title": "在线动态子模规划优化",
    "translated_abstract": "我们提出了一种新的具有可证明性能的算法，用于处理满足一般约束条件和动态环境下的在线二元优化问题。我们考虑了目标函数为子模规划的问题子集。我们提出了在线子模贪婪算法（OSGA），该算法通过求解先前轮损失函数的近似值来避免原问题的NP-困难性。我们将OSGA扩展为通用的近似函数。我们证明了OSGA在时间长度和累积轮次最优变化方面具有与在线凸优化中最严格边界相似的动态遗憾界。对于没有近似解或需要更简单的实现的情况，我们设计了在线子模映射梯度下降（OSPGD）算法，利用Lova\\'sz扩展。我们得到了类似于传统在线梯度下降（OGD）的遗憾界。最后，我们在两个电力系统中对算法进行了数值测试。",
    "tldr": "该论文介绍了在线动态子模规划优化问题，并提出了在线子模贪婪算法（OSGA）和在线子模映射梯度下降（OSPGD）算法以解决此类问题。实验结果表明，这些算法在不同的电力系统中表现良好。",
    "en_tdlr": "This paper presents online dynamic submodular optimization problems and proposes the Online Submodular Greedy Algorithm (OSGA) and Online Submodular Projected Gradient Descent (OSPGD) algorithms to tackle such problems. Experimental results demonstrate the effectiveness of these algorithms in various power system scenarios."
}