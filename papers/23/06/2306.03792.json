{
    "title": "FAMO: Fast Adaptive Multitask Optimization. (arXiv:2306.03792v2 [cs.LG] UPDATED)",
    "abstract": "One of the grand enduring goals of AI is to create generalist agents that can learn multiple different tasks from diverse data via multitask learning (MTL). However, in practice, applying gradient descent (GD) on the average loss across all tasks may yield poor multitask performance due to severe under-optimization of certain tasks. Previous approaches that manipulate task gradients for a more balanced loss decrease require storing and computing all task gradients ($\\mathcal{O}(k)$ space and time where $k$ is the number of tasks), limiting their use in large-scale scenarios. In this work, we introduce Fast Adaptive Multitask Optimization FAMO, a dynamic weighting method that decreases task losses in a balanced way using $\\mathcal{O}(1)$ space and time. We conduct an extensive set of experiments covering multi-task supervised and reinforcement learning problems. Our results indicate that FAMO achieves comparable or superior performance to state-of-the-art gradient manipulation technique",
    "link": "http://arxiv.org/abs/2306.03792",
    "context": "Title: FAMO: Fast Adaptive Multitask Optimization. (arXiv:2306.03792v2 [cs.LG] UPDATED)\nAbstract: One of the grand enduring goals of AI is to create generalist agents that can learn multiple different tasks from diverse data via multitask learning (MTL). However, in practice, applying gradient descent (GD) on the average loss across all tasks may yield poor multitask performance due to severe under-optimization of certain tasks. Previous approaches that manipulate task gradients for a more balanced loss decrease require storing and computing all task gradients ($\\mathcal{O}(k)$ space and time where $k$ is the number of tasks), limiting their use in large-scale scenarios. In this work, we introduce Fast Adaptive Multitask Optimization FAMO, a dynamic weighting method that decreases task losses in a balanced way using $\\mathcal{O}(1)$ space and time. We conduct an extensive set of experiments covering multi-task supervised and reinforcement learning problems. Our results indicate that FAMO achieves comparable or superior performance to state-of-the-art gradient manipulation technique",
    "path": "papers/23/06/2306.03792.json",
    "total_tokens": 893,
    "translated_title": "FAMO：快速自适应多任务优化",
    "translated_abstract": "人工智能的一个长久的目标是创建能够通过多任务学习从多样化数据中学习多个不同任务的通用代理。然而，在实践中，对所有任务的平均损失应用梯度下降可能会由于某些任务的严重欠优化而导致较差的多任务性能。以往的方法通过操纵任务梯度以获得更平衡的损失减少，但需要存储和计算所有任务的梯度（时间和空间复杂度为O(k)，其中k是任务数量），限制了它们在大规模场景中的使用。在这项工作中，我们引入了快速自适应多任务优化（FAMO），一种动态加权方法，以O(1)的时间和空间复杂度以平衡的方式减少任务损失。我们进行了一系列广泛的实验，涵盖了多任务监督学习和强化学习问题。我们的结果表明，FAMO在性能上达到了与最先进的梯度操作技术相媲美或更优的水平。",
    "tldr": "FAMO是一种快速自适应多任务优化方法，通过动态加权方式实现平衡的任务损失减少，相比最先进的梯度操作技术具有相似或更优的性能。",
    "en_tdlr": "FAMO is a fast adaptive multitask optimization method that achieves balanced task loss reduction through dynamic weighting, and it has comparable or superior performance to state-of-the-art gradient manipulation techniques."
}