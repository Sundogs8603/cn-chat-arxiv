{
    "title": "GPT Self-Supervision for a Better Data Annotator. (arXiv:2306.04349v1 [cs.CL])",
    "abstract": "The task of annotating data into concise summaries poses a significant challenge across various domains, frequently requiring the allocation of significant time and specialized knowledge by human experts. Despite existing efforts to use large language models for annotation tasks, significant problems such as limited applicability to unlabeled data, the absence of self-supervised methods, and the lack of focus on complex structured data still persist. In this work, we propose a GPT self-supervision annotation method. This method embodies a generating-recovering paradigm that leverages the capabilities of one-shot learning capabilities in Generative Pretrained Transformer (GPT). The proposed approach comprises a one-shot tuning phase followed by a generation phase. In the one-shot tuning phase, we sample a data from the support set as part of the prompt for GPT to generate a textual summary, which is then used to recover the original data. The alignment score between the recovered and or",
    "link": "http://arxiv.org/abs/2306.04349",
    "context": "Title: GPT Self-Supervision for a Better Data Annotator. (arXiv:2306.04349v1 [cs.CL])\nAbstract: The task of annotating data into concise summaries poses a significant challenge across various domains, frequently requiring the allocation of significant time and specialized knowledge by human experts. Despite existing efforts to use large language models for annotation tasks, significant problems such as limited applicability to unlabeled data, the absence of self-supervised methods, and the lack of focus on complex structured data still persist. In this work, we propose a GPT self-supervision annotation method. This method embodies a generating-recovering paradigm that leverages the capabilities of one-shot learning capabilities in Generative Pretrained Transformer (GPT). The proposed approach comprises a one-shot tuning phase followed by a generation phase. In the one-shot tuning phase, we sample a data from the support set as part of the prompt for GPT to generate a textual summary, which is then used to recover the original data. The alignment score between the recovered and or",
    "path": "papers/23/06/2306.04349.json",
    "total_tokens": 1016,
    "translated_title": "GPT自监督学习在数据标注中的应用",
    "translated_abstract": "在各个领域中，将数据注释为简洁的摘要是一个巨大的挑战，常常需要人类专家投入大量时间和专业知识。尽管现有的大型语言模型在注释任务中的使用已经有所尝试，但仍存在诸如不适用于无标签数据、缺乏自监督方法、缺乏对复杂结构化数据的关注等重要问题。本文提出了一种基于GPT自监督学习的数据标注方法。该方法采用生成-恢复范式，利用生成预训练转换器（GPT）中的一次性学习能力。该方法包括一次性调整阶段和生成阶段。在一次性调整阶段，我们从支持集中抽取数据作为GPT生成文本摘要的一部分，然后用该摘要恢复原始数据。恢复数据与原始数据的对齐分数用于微调GPT权重。在生成阶段，我们使用相同基于GPT的方法为无标签数据生成新的文本摘要。在两个不同的数据集上进行实验表明，我们的方法在覆盖率、准确性和易读性方面具有显著的优势，比现有的标注方法更加有效。",
    "tldr": "本文提出了一种基于GPT自监督学习的数据标注方法，将生成-恢复范式与GPT的一次性学习能力相结合，可显著提高数据标注的覆盖率、准确性和易读性。",
    "en_tdlr": "This paper proposes a GPT self-supervision annotation method that combines generating-recovering paradigm with the one-shot learning capabilities of GPT, which significantly improves the coverage, accuracy, and readability of data annotation."
}