{
    "title": "Simplified Temporal Consistency Reinforcement Learning. (arXiv:2306.09466v1 [cs.LG])",
    "abstract": "Reinforcement learning is able to solve complex sequential decision-making tasks but is currently limited by sample efficiency and required computation. To improve sample efficiency, recent work focuses on model-based RL which interleaves model learning with planning. Recent methods further utilize policy learning, value estimation, and, self-supervised learning as auxiliary objectives. In this paper we show that, surprisingly, a simple representation learning approach relying only on a latent dynamics model trained by latent temporal consistency is sufficient for high-performance RL. This applies when using pure planning with a dynamics model conditioned on the representation, but, also when utilizing the representation as policy and value function features in model-free RL. In experiments, our approach learns an accurate dynamics model to solve challenging high-dimensional locomotion tasks with online planners while being 4.1 times faster to train compared to ensemble-based methods. ",
    "link": "http://arxiv.org/abs/2306.09466",
    "context": "Title: Simplified Temporal Consistency Reinforcement Learning. (arXiv:2306.09466v1 [cs.LG])\nAbstract: Reinforcement learning is able to solve complex sequential decision-making tasks but is currently limited by sample efficiency and required computation. To improve sample efficiency, recent work focuses on model-based RL which interleaves model learning with planning. Recent methods further utilize policy learning, value estimation, and, self-supervised learning as auxiliary objectives. In this paper we show that, surprisingly, a simple representation learning approach relying only on a latent dynamics model trained by latent temporal consistency is sufficient for high-performance RL. This applies when using pure planning with a dynamics model conditioned on the representation, but, also when utilizing the representation as policy and value function features in model-free RL. In experiments, our approach learns an accurate dynamics model to solve challenging high-dimensional locomotion tasks with online planners while being 4.1 times faster to train compared to ensemble-based methods. ",
    "path": "papers/23/06/2306.09466.json",
    "total_tokens": 992,
    "translated_title": "简化后的时态一致性强化学习",
    "translated_abstract": "强化学习能够解决复杂的序贯决策任务，但目前仍存在样本效率和所需计算量等限制。为提高样本效率，最近的工作集中于基于模型的强化学习，其将模型学习与规划交错进行。最近的一些方法进一步利用策略学习、价值估计和自监督学习作为辅助目标。本文表明，令人惊讶的是，一种只依靠通过潜在时间一致性训练的潜在动力学模型的简单表示学习方法就足以实现高性能的强化学习。这适用于纯规划过程中利用被表示条件的动力学模型，同时也适用于在模型无关的强化学习中利用该表示作为策略和价值函数特征。在实验中，我们的方法通过在线规划器学习了准确的动力学模型，解决了挑战性的高维奔跑任务，而其训练速度比基于集合的方法要快4.1倍。",
    "tldr": "本文表明，一种仅依靠潜在动力学模型的潜在时间一致性训练的简单表示学习方法，可以实现高性能的强化学习，同时在纯规划过程中和模型无关的强化学习中都适用。在实验中，该方法解决了挑战性的高维奔跑任务，并且训练速度比基于集合的方法要快4.1倍。"
}