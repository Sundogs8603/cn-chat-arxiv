{
    "title": "Evading Forensic Classifiers with Attribute-Conditioned Adversarial Faces. (arXiv:2306.13091v1 [cs.CV])",
    "abstract": "The ability of generative models to produce highly realistic synthetic face images has raised security and ethical concerns. As a first line of defense against such fake faces, deep learning based forensic classifiers have been developed. While these forensic models can detect whether a face image is synthetic or real with high accuracy, they are also vulnerable to adversarial attacks. Although such attacks can be highly successful in evading detection by forensic classifiers, they introduce visible noise patterns that are detectable through careful human scrutiny. Additionally, these attacks assume access to the target model(s) which may not always be true. Attempts have been made to directly perturb the latent space of GANs to produce adversarial fake faces that can circumvent forensic classifiers. In this work, we go one step further and show that it is possible to successfully generate adversarial fake faces with a specified set of attributes (e.g., hair color, eye size, race, gend",
    "link": "http://arxiv.org/abs/2306.13091",
    "context": "Title: Evading Forensic Classifiers with Attribute-Conditioned Adversarial Faces. (arXiv:2306.13091v1 [cs.CV])\nAbstract: The ability of generative models to produce highly realistic synthetic face images has raised security and ethical concerns. As a first line of defense against such fake faces, deep learning based forensic classifiers have been developed. While these forensic models can detect whether a face image is synthetic or real with high accuracy, they are also vulnerable to adversarial attacks. Although such attacks can be highly successful in evading detection by forensic classifiers, they introduce visible noise patterns that are detectable through careful human scrutiny. Additionally, these attacks assume access to the target model(s) which may not always be true. Attempts have been made to directly perturb the latent space of GANs to produce adversarial fake faces that can circumvent forensic classifiers. In this work, we go one step further and show that it is possible to successfully generate adversarial fake faces with a specified set of attributes (e.g., hair color, eye size, race, gend",
    "path": "papers/23/06/2306.13091.json",
    "total_tokens": 1033,
    "translated_title": "带属性条件的对抗人脸生成器能够逃避取证分类器",
    "translated_abstract": "生成模型生成高度逼真的合成人脸图像的能力引起了安全和伦理问题。为了防止这种假面孔的第一道防线，基于深度学习的取证分类器已经被开发出来。虽然这些取证模型可以高精度地检测出面部图像是否为合成的或真实的，但它们也容易受到对抗攻击。虽然这样的攻击在逃避鉴定分类器方面非常成功，但它们会引入可通过仔细的人类检查进行检测的可见噪声模式。此外，这些攻击假设有访问目标模型的权限，这并不总是正确的。已经尝试直接扰动GAN的潜在空间以产生对抗性的伪造面孔，可以规避取证分类器。在这项研究中，我们更进一步，展示了可以成功生成带有指定属性（例如，头发颜色、眼睛大小、种族、性别）的对抗人脸，它们可以规避取证分类器，同时保持所需的属性。我们提出了一种基于属性条件的GAN方法，其中GAN生成器是以所需伪造面孔的属性作为条件，生成逃避取证分类器的对抗性面孔图像，并紧密匹配所需的属性。",
    "tldr": "该论文提出了一种基于属性条件的GAN方法，可以生成具有指定属性的对抗性伪造人脸图像，这些图像可以规避取证分类器的检测，同时保留所需的属性。"
}