{
    "title": "DNA-TEQ: An Adaptive Exponential Quantization of Tensors for DNN Inference. (arXiv:2306.16430v1 [cs.LG])",
    "abstract": "Quantization is commonly used in Deep Neural Networks (DNNs) to reduce the storage and computational complexity by decreasing the arithmetical precision of activations and weights, a.k.a. tensors. Efficient hardware architectures employ linear quantization to enable the deployment of recent DNNs onto embedded systems and mobile devices. However, linear uniform quantization cannot usually reduce the numerical precision to less than 8 bits without sacrificing high performance in terms of model accuracy. The performance loss is due to the fact that tensors do not follow uniform distributions. In this paper, we show that a significant amount of tensors fit into an exponential distribution. Then, we propose DNA-TEQ to exponentially quantize DNN tensors with an adaptive scheme that achieves the best trade-off between numerical precision and accuracy loss. The experimental results show that DNA-TEQ provides a much lower quantization bit-width compared to previous proposals, resulting in an av",
    "link": "http://arxiv.org/abs/2306.16430",
    "context": "Title: DNA-TEQ: An Adaptive Exponential Quantization of Tensors for DNN Inference. (arXiv:2306.16430v1 [cs.LG])\nAbstract: Quantization is commonly used in Deep Neural Networks (DNNs) to reduce the storage and computational complexity by decreasing the arithmetical precision of activations and weights, a.k.a. tensors. Efficient hardware architectures employ linear quantization to enable the deployment of recent DNNs onto embedded systems and mobile devices. However, linear uniform quantization cannot usually reduce the numerical precision to less than 8 bits without sacrificing high performance in terms of model accuracy. The performance loss is due to the fact that tensors do not follow uniform distributions. In this paper, we show that a significant amount of tensors fit into an exponential distribution. Then, we propose DNA-TEQ to exponentially quantize DNN tensors with an adaptive scheme that achieves the best trade-off between numerical precision and accuracy loss. The experimental results show that DNA-TEQ provides a much lower quantization bit-width compared to previous proposals, resulting in an av",
    "path": "papers/23/06/2306.16430.json",
    "total_tokens": 891,
    "translated_title": "DNA-TEQ：一种用于DNN推理的自适应指数量化张量的方法",
    "translated_abstract": "量化是深度神经网络（DNN）中常用的技术，通过降低激活和权重（即张量）的算术精度来减少存储和计算复杂度。高效的硬件架构采用线性量化，以便将最新的DNN部署到嵌入式系统和移动设备上。然而，线性均匀量化通常无法将数值精度降低到小于8位而不牺牲模型准确性。这是因为张量并不服从均匀分布。本文中，我们展示了大量张量符合指数分布。然后，我们提出了DNA-TEQ，通过自适应方案指数量化DNN张量，以实现数值精度和准确性损失之间的最佳平衡。实验结果表明，与先前的方案相比，DNA-TEQ提供了更低的量化位宽，从而提高了性能。",
    "tldr": "本文介绍了一种在DNN推理中使用的自适应指数量化张量方法DNA-TEQ，该方法通过发现大量张量符合指数分布来实现最佳的数值精度和准确性损失平衡。",
    "en_tdlr": "This paper presents a method called DNA-TEQ for adaptive exponential quantization of tensors in DNN inference. By identifying that a significant amount of tensors follow an exponential distribution, DNA-TEQ achieves the best trade-off between numerical precision and accuracy loss, providing a lower quantization bit-width compared to previous proposals."
}