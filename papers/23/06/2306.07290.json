{
    "title": "Value function estimation using conditional diffusion models for control. (arXiv:2306.07290v1 [cs.LG])",
    "abstract": "A fairly reliable trend in deep reinforcement learning is that the performance scales with the number of parameters, provided a complimentary scaling in amount of training data. As the appetite for large models increases, it is imperative to address, sooner than later, the potential problem of running out of high-quality demonstrations. In this case, instead of collecting only new data via costly human demonstrations or risking a simulation-to-real transfer with uncertain effects, it would be beneficial to leverage vast amounts of readily-available low-quality data. Since classical control algorithms such as behavior cloning or temporal difference learning cannot be used on reward-free or action-free data out-of-the-box, this solution warrants novel training paradigms for continuous control. We propose a simple algorithm called Diffused Value Function (DVF), which learns a joint multi-step model of the environment-robot interaction dynamics using a diffusion model. This model can be ef",
    "link": "http://arxiv.org/abs/2306.07290",
    "context": "Title: Value function estimation using conditional diffusion models for control. (arXiv:2306.07290v1 [cs.LG])\nAbstract: A fairly reliable trend in deep reinforcement learning is that the performance scales with the number of parameters, provided a complimentary scaling in amount of training data. As the appetite for large models increases, it is imperative to address, sooner than later, the potential problem of running out of high-quality demonstrations. In this case, instead of collecting only new data via costly human demonstrations or risking a simulation-to-real transfer with uncertain effects, it would be beneficial to leverage vast amounts of readily-available low-quality data. Since classical control algorithms such as behavior cloning or temporal difference learning cannot be used on reward-free or action-free data out-of-the-box, this solution warrants novel training paradigms for continuous control. We propose a simple algorithm called Diffused Value Function (DVF), which learns a joint multi-step model of the environment-robot interaction dynamics using a diffusion model. This model can be ef",
    "path": "papers/23/06/2306.07290.json",
    "total_tokens": 827,
    "translated_title": "基于条件扩散模型的值函数估计控制方法",
    "translated_abstract": "深度强化学习的一个可靠趋势是性能随参数数量的增加而提高，前提是有充足的训练数据。然而，随着大型模型的需求增加，很可能会出现高质量示范数据不足的问题。我们提出了一种名为扩散值函数的简单算法(DVF)，它使用扩散模型来学习环境和机器人交互动态的联合多步模型，并估计所需任务的值函数。在模拟连续控制任务上，我们证明了我们的方法的有效性，并显示它只需要使用一小部分示范数据就可以胜过基准算法。",
    "tldr": "论文提出了一种基于条件扩散模型的值函数估计控制方法（DVF），该方法可以在大量条件化数据上有效地进行训练，并使用只有一小部分示范数据就能超过基准算法。",
    "en_tdlr": "This paper proposes a value function estimation control method based on conditional diffusion models (DVF). It can be efficiently trained on large amounts of conditioning data and outperform baseline algorithms using only a fraction of demonstration data in simulated continuous control tasks."
}