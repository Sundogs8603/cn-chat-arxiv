{
    "title": "MetaGait: Learning to Learn an Omni Sample Adaptive Representation for Gait Recognition. (arXiv:2306.03445v1 [cs.CV])",
    "abstract": "Gait recognition, which aims at identifying individuals by their walking patterns, has recently drawn increasing research attention. However, gait recognition still suffers from the conflicts between the limited binary visual clues of the silhouette and numerous covariates with diverse scales, which brings challenges to the model's adaptiveness. In this paper, we address this conflict by developing a novel MetaGait that learns to learn an omni sample adaptive representation. Towards this goal, MetaGait injects meta-knowledge, which could guide the model to perceive sample-specific properties, into the calibration network of the attention mechanism to improve the adaptiveness from the omni-scale, omni-dimension, and omni-process perspectives. Specifically, we leverage the meta-knowledge across the entire process, where Meta Triple Attention and Meta Temporal Pooling are presented respectively to adaptively capture omni-scale dependency from spatial/channel/temporal dimensions simultaneo",
    "link": "http://arxiv.org/abs/2306.03445",
    "context": "Title: MetaGait: Learning to Learn an Omni Sample Adaptive Representation for Gait Recognition. (arXiv:2306.03445v1 [cs.CV])\nAbstract: Gait recognition, which aims at identifying individuals by their walking patterns, has recently drawn increasing research attention. However, gait recognition still suffers from the conflicts between the limited binary visual clues of the silhouette and numerous covariates with diverse scales, which brings challenges to the model's adaptiveness. In this paper, we address this conflict by developing a novel MetaGait that learns to learn an omni sample adaptive representation. Towards this goal, MetaGait injects meta-knowledge, which could guide the model to perceive sample-specific properties, into the calibration network of the attention mechanism to improve the adaptiveness from the omni-scale, omni-dimension, and omni-process perspectives. Specifically, we leverage the meta-knowledge across the entire process, where Meta Triple Attention and Meta Temporal Pooling are presented respectively to adaptively capture omni-scale dependency from spatial/channel/temporal dimensions simultaneo",
    "path": "papers/23/06/2306.03445.json",
    "total_tokens": 900,
    "translated_title": "MetaGait：学习学习步态识别的全样本自适应表示",
    "translated_abstract": "步态识别旨在通过个体的行走模式识别其身份，近年来受到越来越多的研究关注。然而，步态识别仍面临着轮廓的有限二进制视觉线索和众多尺度不同的协变量之间的冲突，这给模型的适应性带来了挑战。本文通过开发一种新的MetaGait技术来解决这一冲突，该技术学习学习全样本自适应表示。为实现这一目标，MetaGait将元知识注入到注意机制的校准网络中，从全样本自适应性的全尺度、全维度和全过程的角度改善了模型的适应性。具体而言，我们跨越整个过程利用了元知识，分别提出了Meta Triple Attention和Meta Temporal Pooling来自适应地捕捉空间/通道/时间维度的全尺度依赖关系。",
    "tldr": "本文开发了MetaGait技术来学习全样本自适应表示，通过注入元知识到校准网络中，从全尺度、全维度和全过程的角度改善了模型的适应性。",
    "en_tdlr": "This paper proposes the MetaGait method that learns to learn an omni sample adaptive representation for gait recognition, which improves the model's adaptiveness from the omni-scale, omni-dimension, and omni-process perspectives by injecting meta-knowledge into the calibration network of the attention mechanism."
}