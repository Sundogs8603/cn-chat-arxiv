{
    "title": "Visually grounded few-shot word learning in low-resource settings. (arXiv:2306.11371v2 [eess.AS] UPDATED)",
    "abstract": "We propose a visually grounded speech model that learns new words and their visual depictions from just a few word-image example pairs. Given a set of test images and a spoken query, we ask the model which image depicts the query word. Previous work has simplified this few-shot learning problem by either using an artificial setting with digit word-image pairs or by using a large number of examples per class. Moreover, all previous studies were performed using English speech-image data. We propose an approach that can work on natural word-image pairs but with less examples, i.e. fewer shots, and then illustrate how this approach can be applied for multimodal few-shot learning in a real low-resource language, Yoruba. Our approach involves using the given word-image example pairs to mine new unsupervised word-image training pairs from large collections of unlabelledspeech and images. Additionally, we use a word-to-image attention mechanism to determine word-image similarity. With this new",
    "link": "http://arxiv.org/abs/2306.11371",
    "context": "Title: Visually grounded few-shot word learning in low-resource settings. (arXiv:2306.11371v2 [eess.AS] UPDATED)\nAbstract: We propose a visually grounded speech model that learns new words and their visual depictions from just a few word-image example pairs. Given a set of test images and a spoken query, we ask the model which image depicts the query word. Previous work has simplified this few-shot learning problem by either using an artificial setting with digit word-image pairs or by using a large number of examples per class. Moreover, all previous studies were performed using English speech-image data. We propose an approach that can work on natural word-image pairs but with less examples, i.e. fewer shots, and then illustrate how this approach can be applied for multimodal few-shot learning in a real low-resource language, Yoruba. Our approach involves using the given word-image example pairs to mine new unsupervised word-image training pairs from large collections of unlabelledspeech and images. Additionally, we use a word-to-image attention mechanism to determine word-image similarity. With this new",
    "path": "papers/23/06/2306.11371.json",
    "total_tokens": 1009,
    "translated_title": "低资源环境中的视觉基础少量示例词汇学习",
    "translated_abstract": "我们提出了一个视觉语音模型，在只有少量词-图像实例对的情况下学习新单词及其视觉表示。我们的方法可以在自然词-图像对上工作，但使用更少的示例，并且可以应用于实际低资源语言Yoruba的多模态少量示例学习中。",
    "tldr": "本论文提出了一个可以在低资源环境中只用少量样本学习新词汇及其视觉表示的视觉语音模型，可应用于Yoruba等低资源语言的多模态少量示例学习。"
}