{
    "title": "Surveying (Dis)Parities and Concerns of Compute Hungry NLP Research. (arXiv:2306.16900v1 [cs.CL])",
    "abstract": "Many recent improvements in NLP stem from the development and use of large pre-trained language models (PLMs) with billions of parameters. Large model sizes makes computational cost one of the main limiting factors for training and evaluating such models; and has raised severe concerns about the sustainability, reproducibility, and inclusiveness for researching PLMs. These concerns are often based on personal experiences and observations. However, there had not been any large-scale surveys that investigate them. In this work, we provide a first attempt to quantify these concerns regarding three topics, namely, environmental impact, equity, and impact on peer reviewing. By conducting a survey with 312 participants from the NLP community, we capture existing (dis)parities between different and within groups with respect to seniority, academia, and industry; and their impact on the peer reviewing process. For each topic, we provide an analysis and devise recommendations to mitigate found ",
    "link": "http://arxiv.org/abs/2306.16900",
    "context": "Title: Surveying (Dis)Parities and Concerns of Compute Hungry NLP Research. (arXiv:2306.16900v1 [cs.CL])\nAbstract: Many recent improvements in NLP stem from the development and use of large pre-trained language models (PLMs) with billions of parameters. Large model sizes makes computational cost one of the main limiting factors for training and evaluating such models; and has raised severe concerns about the sustainability, reproducibility, and inclusiveness for researching PLMs. These concerns are often based on personal experiences and observations. However, there had not been any large-scale surveys that investigate them. In this work, we provide a first attempt to quantify these concerns regarding three topics, namely, environmental impact, equity, and impact on peer reviewing. By conducting a survey with 312 participants from the NLP community, we capture existing (dis)parities between different and within groups with respect to seniority, academia, and industry; and their impact on the peer reviewing process. For each topic, we provide an analysis and devise recommendations to mitigate found ",
    "path": "papers/23/06/2306.16900.json",
    "total_tokens": 962,
    "translated_title": "调查计算需求量大的自然语言处理研究中的不平等和担忧",
    "translated_abstract": "自然语言处理领域的许多最新进展源于开发和使用具有数十亿参数的大规模预训练语言模型（PLM）。大模型的规模使得计算成本成为训练和评估这些模型的主要限制因素之一；并且对于研究PLMs的可持续性、可重复性和包容性引发了严重的担忧。这些担忧往往基于个人经验和观察。然而，迄今为止还没有进行大规模调查来调查这些担忧。在这项工作中，我们首次尝试量化与环境影响、公平性和同行评审影响相关的这些担忧。通过对NLP社区的312位参与者进行调查，我们捕捉到不同群体内部和之间的现有（不）平等现象，包括资历、学术界和工业界，以及它们对同行评审过程的影响。对于每个主题，我们提供了分析结果，并提出了相应的缓解建议。",
    "tldr": "这项研究调查了自然语言处理领域计算需求量大的研究中存在的不平等和担忧，通过对NLP社区的312位参与者进行调查，发现了在资历、学术界和工业界等方面存在的（不）平等现象，并提出了相应的缓解建议。",
    "en_tdlr": "This study investigates the disparities and concerns in computationally intensive research in the field of natural language processing (NLP). By surveying 312 participants from the NLP community, the study identifies existing (dis)parities within and between different groups, including seniority, academia, and industry, and provides recommendations to mitigate these disparities."
}