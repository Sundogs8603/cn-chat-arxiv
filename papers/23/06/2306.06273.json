{
    "title": "Using Auxiliary Data to Boost Precision in the Analysis of A/B Tests on an Online Educational Platform: New Data and New Results. (arXiv:2306.06273v1 [stat.ME])",
    "abstract": "Randomized A/B tests within online learning platforms represent an exciting direction in learning sciences. With minimal assumptions, they allow causal effect estimation without confounding bias and exact statistical inference even in small samples. However, often experimental samples and/or treatment effects are small, A/B tests are underpowered, and effect estimates are overly imprecise. Recent methodological advances have shown that power and statistical precision can be substantially boosted by coupling design-based causal estimation to machine-learning models of rich log data from historical users who were not in the experiment. Estimates using these techniques remain unbiased and inference remains exact without any additional assumptions. This paper reviews those methods and applies them to a new dataset including over 250 randomized A/B comparisons conducted within ASSISTments, an online learning platform. We compare results across experiments using four novel deep-learning mode",
    "link": "http://arxiv.org/abs/2306.06273",
    "context": "Title: Using Auxiliary Data to Boost Precision in the Analysis of A/B Tests on an Online Educational Platform: New Data and New Results. (arXiv:2306.06273v1 [stat.ME])\nAbstract: Randomized A/B tests within online learning platforms represent an exciting direction in learning sciences. With minimal assumptions, they allow causal effect estimation without confounding bias and exact statistical inference even in small samples. However, often experimental samples and/or treatment effects are small, A/B tests are underpowered, and effect estimates are overly imprecise. Recent methodological advances have shown that power and statistical precision can be substantially boosted by coupling design-based causal estimation to machine-learning models of rich log data from historical users who were not in the experiment. Estimates using these techniques remain unbiased and inference remains exact without any additional assumptions. This paper reviews those methods and applies them to a new dataset including over 250 randomized A/B comparisons conducted within ASSISTments, an online learning platform. We compare results across experiments using four novel deep-learning mode",
    "path": "papers/23/06/2306.06273.json",
    "total_tokens": 928,
    "translated_title": "利用辅助数据提高在线教育平台A/B测试分析的精度：新数据和新结果",
    "translated_abstract": "在线学习平台上的随机A/B测试代表着学习科学中一个激动人心的方向。他们几乎没有任何假设，可以在没有混杂偏差的情况下进行因果效应估计和精确的统计推断即使在小样本下。然而，经常由于实验样本和/或处理效果过小，A/B测试缺乏动力，效应估计过于不精密。最近的方法论进展表明，通过将基于设计的因果估计与历史用户的丰富日志数据的机器学习模型相结合，可以实现大幅提高能力和统计精度。即使没有任何其他假设，使用这些技术得出的估计仍然保持不偏，推断仍然精确。本文回顾了这些方法，并将其应用于包括在在线学习平台ASSISTments内进行的250多个随机A/B比较的新数据集。我们使用四个新的深度学习模型比较不同实验的结果。",
    "tldr": "A/B测试的小样本和处理效果过小导致效应估计过于不精密，本研究利用历史用户的丰富日志数据的机器学习模型来提高技术，解决了这个问题，有效提高了A/B测试的能力和统计精度。",
    "en_tdlr": "The effect estimates of A/B tests are often imprecise due to small sample sizes and small treatment effects. This paper proposes a solution by using machine-learning models of rich log data from historical users to improve the power and statistical precision of A/B tests. This method shows unbiased estimates and exact inference without additional assumptions."
}