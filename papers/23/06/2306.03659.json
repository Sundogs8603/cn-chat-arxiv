{
    "title": "Schema First! Learn Versatile Knowledge Graph Embeddings by Capturing Semantics with MASCHInE. (arXiv:2306.03659v2 [cs.AI] UPDATED)",
    "abstract": "Knowledge graph embedding models (KGEMs) have gained considerable traction in recent years. These models learn a vector representation of knowledge graph entities and relations, a.k.a. knowledge graph embeddings (KGEs). Learning versatile KGEs is desirable as it makes them useful for a broad range of tasks. However, KGEMs are usually trained for a specific task, which makes their embeddings task-dependent. In parallel, the widespread assumption that KGEMs actually create a semantic representation of the underlying entities and relations (e.g., project similar entities closer than dissimilar ones) has been challenged. In this work, we design heuristics for generating protographs -small, modified versions of a KG that leverage RDF/S information. The learnt protograph-based embeddings are meant to encapsulate the semantics of a KG, and can be leveraged in learning KGEs that, in turn, also better capture semantics. Extensive experiments on various evaluation benchmarks demonstrate the so",
    "link": "http://arxiv.org/abs/2306.03659",
    "context": "Title: Schema First! Learn Versatile Knowledge Graph Embeddings by Capturing Semantics with MASCHInE. (arXiv:2306.03659v2 [cs.AI] UPDATED)\nAbstract: Knowledge graph embedding models (KGEMs) have gained considerable traction in recent years. These models learn a vector representation of knowledge graph entities and relations, a.k.a. knowledge graph embeddings (KGEs). Learning versatile KGEs is desirable as it makes them useful for a broad range of tasks. However, KGEMs are usually trained for a specific task, which makes their embeddings task-dependent. In parallel, the widespread assumption that KGEMs actually create a semantic representation of the underlying entities and relations (e.g., project similar entities closer than dissimilar ones) has been challenged. In this work, we design heuristics for generating protographs -small, modified versions of a KG that leverage RDF/S information. The learnt protograph-based embeddings are meant to encapsulate the semantics of a KG, and can be leveraged in learning KGEs that, in turn, also better capture semantics. Extensive experiments on various evaluation benchmarks demonstrate the so",
    "path": "papers/23/06/2306.03659.json",
    "total_tokens": 933,
    "translated_title": "Schema First！通过MASCHInE捕捉语义学习通用知识图嵌入",
    "translated_abstract": "近年来，知识图嵌入模型（KGEMs）受到了广泛关注，这些模型学习了知识图中实体和关系的向量表示，即知识图嵌入（KGEs）。学习多功能的KGEs非常有意义，因为这使得它们在广泛的任务上有用。然而，KGEMs通常是针对特定任务进行训练的，这使得它们的嵌入是任务相关的。与此同时，关于KGEMs实际上是否创建了底层实体和关系的语义表示（例如，将相似的实体放在一起，将不相似的实体放在一起）的普遍假设受到了质疑。在这项工作中，我们设计了启发式方法来生成原型图-一个小型、修改过的KG版本，利用了RDF/S信息。所学习的基于原型图的嵌入旨在封装KG的语义，并可以在学习KGEs时加以利用，从而更好地捕捉语义。对各种评估基准进行了大量实验证明了这一点。",
    "tldr": "本论文提出了一种通过 MASCHInE 捕捉语义学习知识图嵌入的方法，通过设计生成原型图并利用其语义，进而训练出更好地捕捉语义的 KGEs。",
    "en_tdlr": "This paper proposes a method for learning knowledge graph embeddings by capturing semantics with MASCHInE. It introduces the concept of protographs and leverages their semantics to train KGEs that better capture the underlying semantic representations of a knowledge graph."
}