{
    "title": "How Good is the Model in Model-in-the-loop Event Coreference Resolution Annotation?. (arXiv:2306.05434v1 [cs.CL])",
    "abstract": "Annotating cross-document event coreference links is a time-consuming and cognitively demanding task that can compromise annotation quality and efficiency. To address this, we propose a model-in-the-loop annotation approach for event coreference resolution, where a machine learning model suggests likely corefering event pairs only. We evaluate the effectiveness of this approach by first simulating the annotation process and then, using a novel annotator-centric Recall-Annotation effort trade-off metric, we compare the results of various underlying models and datasets. We finally present a method for obtaining 97\\% recall while substantially reducing the workload required by a fully manual annotation process. Code and data can be found at https://github.com/ahmeshaf/model_in_coref",
    "link": "http://arxiv.org/abs/2306.05434",
    "context": "Title: How Good is the Model in Model-in-the-loop Event Coreference Resolution Annotation?. (arXiv:2306.05434v1 [cs.CL])\nAbstract: Annotating cross-document event coreference links is a time-consuming and cognitively demanding task that can compromise annotation quality and efficiency. To address this, we propose a model-in-the-loop annotation approach for event coreference resolution, where a machine learning model suggests likely corefering event pairs only. We evaluate the effectiveness of this approach by first simulating the annotation process and then, using a novel annotator-centric Recall-Annotation effort trade-off metric, we compare the results of various underlying models and datasets. We finally present a method for obtaining 97\\% recall while substantially reducing the workload required by a fully manual annotation process. Code and data can be found at https://github.com/ahmeshaf/model_in_coref",
    "path": "papers/23/06/2306.05434.json",
    "total_tokens": 786,
    "translated_title": "模型在环节内事件共指关系注释中表现如何？",
    "translated_abstract": "注释跨文档事件共指链接是一项费时且认知要求高的任务，可能会影响注释质量和效率。为了解决这个问题，我们提出了一种模型在环节中事件共指关系注释方法，其中机器学习模型只建议可能的共指事件对。我们通过首先模拟注释过程，然后使用一种新颖的注释者为中心的召回-注释工作量平衡度量来比较不同基础模型和数据集的结果，评估了这种方法的有效性。最后，我们提出了一种方法，可以在大幅减少完全手动注释过程所需的工作量的同时，实现97％的召回率。 代码和数据可在 https://github.com/ahmeshaf/model_in_coref 找到。",
    "tldr": "本论文提出了一种模型在环节中事件共指关系注释方法，通过机器学习模型只建议可能的共指事件对，将完全手动注释过程所需的工作量大幅度减少，同时实现97％的召回率。",
    "en_tdlr": "This paper proposes a model-in-the-loop annotation approach for event coreference resolution, in which a machine learning model suggests likely corefering event pairs only, greatly reducing the workload required by a fully manual annotation process, while achieving 97% recall rate."
}