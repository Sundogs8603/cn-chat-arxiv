{
    "title": "IDOL: Indicator-oriented Logic Pre-training for Logical Reasoning. (arXiv:2306.15273v1 [cs.CL])",
    "abstract": "In the field of machine reading comprehension (MRC), existing systems have surpassed the average performance of human beings in many tasks like SQuAD. However, there is still a long way to go when it comes to logical reasoning. Although some methods for it have been put forward, they either are designed in a quite complicated way or rely too much on external structures. In this paper, we proposed IDOL (InDicator-Oriented Logic Pre-training), an easy-to-understand but highly effective further pre-training task which logically strengthens the pre-trained models with the help of 6 types of logical indicators and a logically rich dataset LGP (LoGic Pre-training). IDOL achieves state-of-the-art performance on ReClor and LogiQA, the two most representative benchmarks in logical reasoning MRC, and is proven to be capable of generalizing to different pre-trained models and other types of MRC benchmarks like RACE and SQuAD 2.0 while keeping competitive general language understanding ability thr",
    "link": "http://arxiv.org/abs/2306.15273",
    "context": "Title: IDOL: Indicator-oriented Logic Pre-training for Logical Reasoning. (arXiv:2306.15273v1 [cs.CL])\nAbstract: In the field of machine reading comprehension (MRC), existing systems have surpassed the average performance of human beings in many tasks like SQuAD. However, there is still a long way to go when it comes to logical reasoning. Although some methods for it have been put forward, they either are designed in a quite complicated way or rely too much on external structures. In this paper, we proposed IDOL (InDicator-Oriented Logic Pre-training), an easy-to-understand but highly effective further pre-training task which logically strengthens the pre-trained models with the help of 6 types of logical indicators and a logically rich dataset LGP (LoGic Pre-training). IDOL achieves state-of-the-art performance on ReClor and LogiQA, the two most representative benchmarks in logical reasoning MRC, and is proven to be capable of generalizing to different pre-trained models and other types of MRC benchmarks like RACE and SQuAD 2.0 while keeping competitive general language understanding ability thr",
    "path": "papers/23/06/2306.15273.json",
    "total_tokens": 909,
    "translated_title": "IDOL: 面向逻辑推理的指标导向预训练方法",
    "translated_abstract": "在机器阅读理解领域，现有系统在许多任务（如SQuAD）中的表现已经超过了人类平均水平。然而，当涉及到逻辑推理时，仍有很大的进步空间。在本文中，我们提出了一种名为IDOL（InDicator-Oriented Logic Pre-training）的易于理解且高效的进一步预训练任务，该任务利用6种逻辑指标和逻辑丰富的数据集LGP（LoGic Pre-training）在逻辑上强化了预训练模型。IDOL在ReClor和LogiQA这两个具有代表性的逻辑推理MRC基准测试上实现了最先进的性能，并且被证明能够推广到不同的预训练模型和其他类型的MRC基准测试，如RACE和SQuAD 2.0，同时保持有竞争力的综合语言理解能力。",
    "tldr": "IDOL是一种面向逻辑推理的指标导向预训练方法，通过使用逻辑指标和逻辑丰富的数据集在逻辑上增强了预训练模型。IDOL在逻辑推理MRC基准测试中取得了最先进的性能，并且具有竞争力的综合语言理解能力。",
    "en_tdlr": "IDOL is an indicator-oriented logic pre-training method that enhances pre-trained models by using logical indicators and a logically rich dataset. IDOL achieves state-of-the-art performance in logical reasoning MRC benchmarks and maintains competitive general language understanding ability."
}