{
    "title": "Prompter: Zero-shot Adaptive Prefixes for Dialogue State Tracking Domain Adaptation. (arXiv:2306.04724v1 [cs.CL])",
    "abstract": "A challenge in the Dialogue State Tracking (DST) field is adapting models to new domains without using any supervised data, zero-shot domain adaptation. Parameter-Efficient Transfer Learning (PETL) has the potential to address this problem due to its robustness. However, it has yet to be applied to the zero-shot scenarios, as it is not clear how to apply it unsupervisedly.  Our method, Prompter, uses descriptions of target domain slots to generate dynamic prefixes that are concatenated to the key and values at each layer's self-attention mechanism. This allows for the use of prefix-tuning in zero-shot. Prompter outperforms previous methods on both the MultiWOZ and SGD benchmarks. In generating prefixes, our analyses find that Prompter not only utilizes the semantics of slot descriptions but also how often the slots appear together in conversation. Moreover, Prompter's gains are due to its improved ability to distinguish \"none\"-valued dialogue slots, compared against baselines.",
    "link": "http://arxiv.org/abs/2306.04724",
    "context": "Title: Prompter: Zero-shot Adaptive Prefixes for Dialogue State Tracking Domain Adaptation. (arXiv:2306.04724v1 [cs.CL])\nAbstract: A challenge in the Dialogue State Tracking (DST) field is adapting models to new domains without using any supervised data, zero-shot domain adaptation. Parameter-Efficient Transfer Learning (PETL) has the potential to address this problem due to its robustness. However, it has yet to be applied to the zero-shot scenarios, as it is not clear how to apply it unsupervisedly.  Our method, Prompter, uses descriptions of target domain slots to generate dynamic prefixes that are concatenated to the key and values at each layer's self-attention mechanism. This allows for the use of prefix-tuning in zero-shot. Prompter outperforms previous methods on both the MultiWOZ and SGD benchmarks. In generating prefixes, our analyses find that Prompter not only utilizes the semantics of slot descriptions but also how often the slots appear together in conversation. Moreover, Prompter's gains are due to its improved ability to distinguish \"none\"-valued dialogue slots, compared against baselines.",
    "path": "papers/23/06/2306.04724.json",
    "total_tokens": 986,
    "translated_title": "Prompter:用于对话状态跟踪领域自适应的零样本自适应前缀",
    "translated_abstract": "对话状态跟踪领域的挑战是在不使用任何监督数据的情况下使模型适应新域，即零样本领域自适应。参数高效的转移学习（PETL）具有解决此问题的潜力，但尚未应用于零样本场景，因为如何无监督地应用尚不清楚。我们的方法Prompter使用目标领域槽的描述生成动态前缀，并将其连接到每个层的self-attention机制中的键和值上。这允许在零样本中使用前缀调整。Prompter在MultiWOZ和SGD基准测试中表现优异。在生成前缀时，我们的分析发现Prompter不仅利用了槽描述的语义，而且还考虑到了对话中槽一起出现的频率。此外，与基准相比，Prompter的收益在于其能够更好地区分“无”值对话槽。",
    "tldr": "本论文提出了Prompter方法，利用目标领域槽的描述生成动态前缀来进行自适应前缀调整，实现对话状态跟踪领域自适应。通过分析发现，Prompter除了利用槽描述的语义，还能考虑到槽的频率。实验表明，在区分“无”值对话槽方面，Prompter相比基准方法表现更好。",
    "en_tdlr": "This paper proposes the Prompter method for zero-shot adaptive prefixes, which uses descriptions of target domain slots to generate dynamic prefixes for dialogue state tracking domain adaptation. The method outperforms previous ones and is able to distinguish \"none\"-valued dialogue slots. The analysis shows that Prompter not only utilizes the semantics of slot descriptions but also considers the frequency of slot appearance in conversation."
}