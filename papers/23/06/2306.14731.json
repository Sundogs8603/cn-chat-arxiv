{
    "title": "Leveraging Locality and Robustness to Achieve Massively Scalable Gaussian Process Regression. (arXiv:2306.14731v1 [stat.ML])",
    "abstract": "The accurate predictions and principled uncertainty measures provided by GP regression incur O(n^3) cost which is prohibitive for modern-day large-scale applications. This has motivated extensive work on computationally efficient approximations. We introduce a new perspective by exploring robustness properties and limiting behaviour of GP nearest-neighbour (GPnn) prediction. We demonstrate through theory and simulation that as the data-size n increases, accuracy of estimated parameters and GP model assumptions become increasingly irrelevant to GPnn predictive accuracy. Consequently, it is sufficient to spend small amounts of work on parameter estimation in order to achieve high MSE accuracy, even in the presence of gross misspecification. In contrast, as n tends to infinity, uncertainty calibration and NLL are shown to remain sensitive to just one parameter, the additive noise-variance; but we show that this source of inaccuracy can be corrected for, thereby achieving both well-calibra",
    "link": "http://arxiv.org/abs/2306.14731",
    "context": "Title: Leveraging Locality and Robustness to Achieve Massively Scalable Gaussian Process Regression. (arXiv:2306.14731v1 [stat.ML])\nAbstract: The accurate predictions and principled uncertainty measures provided by GP regression incur O(n^3) cost which is prohibitive for modern-day large-scale applications. This has motivated extensive work on computationally efficient approximations. We introduce a new perspective by exploring robustness properties and limiting behaviour of GP nearest-neighbour (GPnn) prediction. We demonstrate through theory and simulation that as the data-size n increases, accuracy of estimated parameters and GP model assumptions become increasingly irrelevant to GPnn predictive accuracy. Consequently, it is sufficient to spend small amounts of work on parameter estimation in order to achieve high MSE accuracy, even in the presence of gross misspecification. In contrast, as n tends to infinity, uncertainty calibration and NLL are shown to remain sensitive to just one parameter, the additive noise-variance; but we show that this source of inaccuracy can be corrected for, thereby achieving both well-calibra",
    "path": "papers/23/06/2306.14731.json",
    "total_tokens": 1027,
    "translated_title": "利用本地性和鲁棒性实现大规模高斯过程回归",
    "translated_abstract": "高斯过程回归所提供的精确预测和原则性不确定性测量会产生 O(n^3) 的成本，这对于现代大规模应用来说是难以承受的。因此，出现了大量关于计算效率的研究。我们通过探索 GP 最近邻预测(GPnn) 的鲁棒性和极限行为引入了一种新的视角。我们通过理论和模拟证明，随着数据量 n 的增加，估计参数和 GP 模型假设的准确性对 GPnn 预测准确性的影响逐渐减小。因此，为了实现高 MSE 准确性，即使在出现重大错误的情况下, 只需要花费少量的工作进行参数估计即可。相比之下，随着 n 趋近于无穷大，我们发现不确定度校准和 NLL 仍对一个参数敏感，即加性噪声方差；但我们证明可以纠正这种不准确性，并实现良好的不确定度校准和 NLL。",
    "tldr": "该研究提出了一种新的思路，通过探索 GPnn 的鲁棒性和极限行为实现大规模高斯过程回归，即使在出现重大小错误的情况下只需要花费少量的工作进行参数估计即可实现高 MSE 准确性。同时，该研究成功解决了加性噪声方差带来的不确定度校准和 NLL 准确性问题。"
}