{
    "title": "Agents Explore the Environment Beyond Good Actions to Improve Their Model for Better Decisions. (arXiv:2306.03408v1 [cs.AI])",
    "abstract": "Improving the decision-making capabilities of agents is a key challenge on the road to artificial intelligence. To improve the planning skills needed to make good decisions, MuZero's agent combines prediction by a network model and planning by a tree search using the predictions. MuZero's learning process can fail when predictions are poor but planning requires them. We use this as an impetus to get the agent to explore parts of the decision tree in the environment that it otherwise would not explore. The agent achieves this, first by normal planning to come up with an improved policy. Second, it randomly deviates from this policy at the beginning of each training episode. And third, it switches back to the improved policy at a random time step to experience the rewards from the environment associated with the improved policy, which is the basis for learning the correct value expectation. The simple board game Tic-Tac-Toe is used to illustrate how this approach can improve the agent's ",
    "link": "http://arxiv.org/abs/2306.03408",
    "context": "Title: Agents Explore the Environment Beyond Good Actions to Improve Their Model for Better Decisions. (arXiv:2306.03408v1 [cs.AI])\nAbstract: Improving the decision-making capabilities of agents is a key challenge on the road to artificial intelligence. To improve the planning skills needed to make good decisions, MuZero's agent combines prediction by a network model and planning by a tree search using the predictions. MuZero's learning process can fail when predictions are poor but planning requires them. We use this as an impetus to get the agent to explore parts of the decision tree in the environment that it otherwise would not explore. The agent achieves this, first by normal planning to come up with an improved policy. Second, it randomly deviates from this policy at the beginning of each training episode. And third, it switches back to the improved policy at a random time step to experience the rewards from the environment associated with the improved policy, which is the basis for learning the correct value expectation. The simple board game Tic-Tac-Toe is used to illustrate how this approach can improve the agent's ",
    "path": "papers/23/06/2306.03408.json",
    "total_tokens": 859,
    "translated_title": "智能体通过探索决策树中的状态来提高其决策模型的性能",
    "translated_abstract": "提高智能体决策能力是人工智能发展道路上的一个关键挑战。MuZero智能体通过网络模型的预测和基于预测结果的树搜索规划相结合来提高规划技能，但当模型预测结果不准确时，学习进程可能会遇到瓶颈。我们通过让智能体探索环境中决策树中一些不会被访问到的状态来改进模型的性能。具体而言，智能体首先通过规划得到改进策略，然后在每个训练阶段的开始时随机偏离这个策略。在一个随机的时间阶段，智能体将又恢复到改进策略以得到环境奖励并对期望价值进行学习。我们在井字棋游戏中展示了该方法对智能体性能的提升。",
    "tldr": "通过探索未被访问的决策树状态和引入随机性，MuZero智能体改进了树搜索规划和模型预测之间的不一致性，提高了决策能力。",
    "en_tdlr": "MuZero agent improves the inconsistency between tree search planning and model prediction by exploring unvisited decision tree states and introducing randomness, thus enhancing decision-making capabilities."
}