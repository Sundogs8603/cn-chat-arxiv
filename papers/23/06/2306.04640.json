{
    "title": "ModuleFormer: Learning Modular Large Language Models From Uncurated Data. (arXiv:2306.04640v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) have achieved remarkable results. But existing models are expensive to train and deploy, and it is also difficult to expand their knowledge beyond pre-training data without forgetting previous knowledge. This paper proposes a new neural network architecture, ModuleFormer, that leverages modularity to improve the efficiency and flexibility of large language models. ModuleFormer is based on the Sparse Mixture of Experts (SMoE). Unlike the previous SMoE-based modular language model [Gururangan et al., 2021], which requires domain-labeled data to learn domain-specific experts, ModuleFormer can induce modularity from uncurated data with its new load balancing and load concentration losses. ModuleFormer is a modular architecture that includes two different types of modules, new stick-breaking attention heads, and feedforward experts. Different modules are sparsely activated conditions on the input token during training and inference. In our experiment, we found t",
    "link": "http://arxiv.org/abs/2306.04640",
    "context": "Title: ModuleFormer: Learning Modular Large Language Models From Uncurated Data. (arXiv:2306.04640v1 [cs.CL])\nAbstract: Large Language Models (LLMs) have achieved remarkable results. But existing models are expensive to train and deploy, and it is also difficult to expand their knowledge beyond pre-training data without forgetting previous knowledge. This paper proposes a new neural network architecture, ModuleFormer, that leverages modularity to improve the efficiency and flexibility of large language models. ModuleFormer is based on the Sparse Mixture of Experts (SMoE). Unlike the previous SMoE-based modular language model [Gururangan et al., 2021], which requires domain-labeled data to learn domain-specific experts, ModuleFormer can induce modularity from uncurated data with its new load balancing and load concentration losses. ModuleFormer is a modular architecture that includes two different types of modules, new stick-breaking attention heads, and feedforward experts. Different modules are sparsely activated conditions on the input token during training and inference. In our experiment, we found t",
    "path": "papers/23/06/2306.04640.json",
    "total_tokens": 920,
    "translated_abstract": "大型语言模型（LLM）已经取得了显著的效果。但是现有模型在训练和部署方面都很昂贵，而且很难扩展它们的知识，以超出预训练数据，同时不会忘记以前的知识。本文提出了一种新的神经网络架构ModuleFormer，利用模块化来提高大型语言模型的效率和灵活性。ModuleFormer基于Sparse Mixture of Experts（SMoE）。与先前基于SMoE的模块化语言模型[Gururangan等人，2021]不同，该模型需要领域标记数据来学习领域特定的专家，ModuleFormer可以通过新的负载平衡和负载集中损失，从未经整理的数据中引出模块化。ModuleFormer是一种模块化架构，包括两种不同类型的模块，新的断棒式注意力头和前馈专家。不同的模块会在训练和推理过程中根据输入令牌的条件而稀疏地激活。",
    "tldr": "本文提出了一种名为ModuleFormer的模块化语言模型架构，通过新的负载平衡和负载集中损失，可以从未经整理的数据中引出模块化，提高了大型语言模型的效率和灵活性。",
    "en_tdlr": "This paper proposes a modular language model architecture called ModuleFormer, which can induce modularity from uncurated data with its new load balancing and load concentration losses, improving the efficiency and flexibility of large language models."
}