{
    "title": "Hierarchical Attention Encoder Decoder. (arXiv:2306.01070v1 [cs.LG])",
    "abstract": "Recent advances in large language models have shown that autoregressive modeling can generate complex and novel sequences that have many real-world applications. However, these models must generate outputs autoregressively, which becomes time-consuming when dealing with long sequences. Hierarchical autoregressive approaches that compress data have been proposed as a solution, but these methods still generate outputs at the original data frequency, resulting in slow and memory-intensive models. In this paper, we propose a model based on the Hierarchical Recurrent Encoder Decoder (HRED) architecture. This model independently encodes input sub-sequences without global context, processes these sequences using a lower-frequency model, and decodes outputs at the original data frequency. By interpreting the encoder as an implicitly defined embedding matrix and using sampled softmax estimation, we develop a training algorithm that can train the entire model without a high-frequency decoder, wh",
    "link": "http://arxiv.org/abs/2306.01070",
    "context": "Title: Hierarchical Attention Encoder Decoder. (arXiv:2306.01070v1 [cs.LG])\nAbstract: Recent advances in large language models have shown that autoregressive modeling can generate complex and novel sequences that have many real-world applications. However, these models must generate outputs autoregressively, which becomes time-consuming when dealing with long sequences. Hierarchical autoregressive approaches that compress data have been proposed as a solution, but these methods still generate outputs at the original data frequency, resulting in slow and memory-intensive models. In this paper, we propose a model based on the Hierarchical Recurrent Encoder Decoder (HRED) architecture. This model independently encodes input sub-sequences without global context, processes these sequences using a lower-frequency model, and decodes outputs at the original data frequency. By interpreting the encoder as an implicitly defined embedding matrix and using sampled softmax estimation, we develop a training algorithm that can train the entire model without a high-frequency decoder, wh",
    "path": "papers/23/06/2306.01070.json",
    "total_tokens": 971,
    "translated_title": "层次化注意力编码器解码器",
    "translated_abstract": "最近，大型语言模型的进展表明，自回归建模可以生成具有许多现实应用的复杂和新颖的序列。然而，这些模型必须自回归地生成输出，当处理长序列时，这变得耗时。已经提出了压缩数据的分层自回归方法作为解决方案，但是这些方法仍然在原始数据频率下生成输出，导致模型速度慢和内存占用高。在本文中，我们提出一种基于分层循环编码器解码器（HRED）架构的模型。该模型独立地对输入子序列进行编码，在较低频率模型中处理这些序列，并在原始数据频率下解码输出。通过将编码器解释为隐式定义的嵌入矩阵并使用采样softmax估计，我们开发了一种训练算法，可以训练整个模型而不需要高频解码器，从而减少计算时间和内存使用。在文本生成任务上的实验表明，与现有方法相比，我们提出的模型可以生成高质量的长序列，并显著减少训练时间和内存要求。",
    "tldr": "本论文提出了一种基于分层循环编码器解码器（HRED）架构的模型，可以独立地对输入子序列进行编码，在较低频率模型中处理这些序列，并在原始数据频率下解码输出，同时减少计算时间和内存使用。",
    "en_tdlr": "This paper proposes a model based on the Hierarchical Recurrent Encoder Decoder (HRED) architecture, which independently encodes input sub-sequences, processes these sequences using a lower-frequency model, and decodes outputs at the original data frequency, while reducing computation time and memory usage."
}