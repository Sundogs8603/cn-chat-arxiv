{
    "title": "DSGD-CECA: Decentralized SGD with Communication-Optimal Exact Consensus Algorithm. (arXiv:2306.00256v1 [cs.LG])",
    "abstract": "Decentralized Stochastic Gradient Descent (SGD) is an emerging neural network training approach that enables multiple agents to train a model collaboratively and simultaneously. Rather than using a central parameter server to collect gradients from all the agents, each agent keeps a copy of the model parameters and communicates with a small number of other agents to exchange model updates. Their communication, governed by the communication topology and gossip weight matrices, facilitates the exchange of model updates. The state-of-the-art approach uses the dynamic one-peer exponential-2 topology, achieving faster training times and improved scalability than the ring, grid, torus, and hypercube topologies. However, this approach requires a power-of-2 number of agents, which is impractical at scale. In this paper, we remove this restriction and propose \\underline{D}ecentralized \\underline{SGD} with \\underline{C}ommunication-optimal \\underline{E}xact \\underline{C}onsensus \\underline{A}lgo",
    "link": "http://arxiv.org/abs/2306.00256",
    "context": "Title: DSGD-CECA: Decentralized SGD with Communication-Optimal Exact Consensus Algorithm. (arXiv:2306.00256v1 [cs.LG])\nAbstract: Decentralized Stochastic Gradient Descent (SGD) is an emerging neural network training approach that enables multiple agents to train a model collaboratively and simultaneously. Rather than using a central parameter server to collect gradients from all the agents, each agent keeps a copy of the model parameters and communicates with a small number of other agents to exchange model updates. Their communication, governed by the communication topology and gossip weight matrices, facilitates the exchange of model updates. The state-of-the-art approach uses the dynamic one-peer exponential-2 topology, achieving faster training times and improved scalability than the ring, grid, torus, and hypercube topologies. However, this approach requires a power-of-2 number of agents, which is impractical at scale. In this paper, we remove this restriction and propose \\underline{D}ecentralized \\underline{SGD} with \\underline{C}ommunication-optimal \\underline{E}xact \\underline{C}onsensus \\underline{A}lgo",
    "path": "papers/23/06/2306.00256.json",
    "total_tokens": 1017,
    "translated_title": "DSGD-CECA: 具有通信优化精确共识算法的分散式 SGD",
    "translated_abstract": "分散式 SGD 是一种新兴的神经网络训练方法，它使多个代理能够协作地并行地训练模型。与使用中央参数服务器从所有代理收集梯度不同，每个代理保留模型参数的副本并与少量其他代理通信以交换模型更新。他们的通信由通信拓扑和八卦权重矩阵控制，促进模型更新的交换。最先进的方法使用动态的指数-2拓扑结构，实现比环形、网格、环面和超立方拓扑结构更快的训练时间和更好的可扩展性。然而，这种方法需要一个2的幂次方数量的代理，这在大规模时不切实际。在本文中，我们消除了这个限制，并提出了具有通信优化精确共识算法的分散式 SGD-CECA，利用这种算法在分散式环境下确保收敛性而不需要2的幂次方限制。我们证明，我们的方法在实现更实际的网络规模的同时，实现了与先进技术相当的训练时间和模型质量。",
    "tldr": "DSGD-CECA 提出了一种通信优化的精确共识算法，使分散式 SGD 能够在没有2的幂次方限制的情况下实现收敛，在大规模时更为实用。",
    "en_tdlr": "DSGD-CECA proposes a communication-optimal exact consensus algorithm for decentralized SGD, which enables convergence without the power-of-2 restriction, making it more practical for large-scale applications."
}