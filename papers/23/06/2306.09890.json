{
    "title": "Studying Generalization on Memory-Based Methods in Continual Learning. (arXiv:2306.09890v1 [cs.LG])",
    "abstract": "One of the objectives of Continual Learning is to learn new concepts continually over a stream of experiences and at the same time avoid catastrophic forgetting. To mitigate complete knowledge overwriting, memory-based methods store a percentage of previous data distributions to be used during training. Although these methods produce good results, few studies have tested their out-of-distribution generalization properties, as well as whether these methods overfit the replay memory. In this work, we show that although these methods can help in traditional in-distribution generalization, they can strongly impair out-of-distribution generalization by learning spurious features and correlations. Using a controlled environment, the Synbol benchmark generator (Lacoste et al., 2020), we demonstrate that this lack of out-of-distribution generalization mainly occurs in the linear classifier.",
    "link": "http://arxiv.org/abs/2306.09890",
    "context": "Title: Studying Generalization on Memory-Based Methods in Continual Learning. (arXiv:2306.09890v1 [cs.LG])\nAbstract: One of the objectives of Continual Learning is to learn new concepts continually over a stream of experiences and at the same time avoid catastrophic forgetting. To mitigate complete knowledge overwriting, memory-based methods store a percentage of previous data distributions to be used during training. Although these methods produce good results, few studies have tested their out-of-distribution generalization properties, as well as whether these methods overfit the replay memory. In this work, we show that although these methods can help in traditional in-distribution generalization, they can strongly impair out-of-distribution generalization by learning spurious features and correlations. Using a controlled environment, the Synbol benchmark generator (Lacoste et al., 2020), we demonstrate that this lack of out-of-distribution generalization mainly occurs in the linear classifier.",
    "path": "papers/23/06/2306.09890.json",
    "total_tokens": 873,
    "translated_title": "连续学习中基于记忆的方法的泛化性研究",
    "translated_abstract": "连续学习的目标之一是在一系列经验中不断学习新的概念，同时避免灾难性遗忘。为了减轻完全知识覆盖的问题，基于记忆的方法会存储一定比例的先前数据分布，在训练中使用。虽然这些方法产生了良好的结果，但很少有研究测试它们的超出分布泛化性能以及这些方法是否过度拟合重放记忆。在这项工作中，我们展示了尽管这些方法可以帮助传统的内分布泛化，但它们可以通过学习虚假的特征和相关性来大大损害超出分布泛化。使用控制环境，我们使用Synbol基准生成器（Lacoste等人，2020）展示了这种缺乏超出分布泛化主要出现在线性分类器中。",
    "tldr": "本文研究了连续学习中基于记忆的方法的泛化性能，发现虽然这些方法可以帮助传统的内分布泛化，但它们可以通过学习虚假的特征和相关性来大大损害超出分布泛化，尤其是在线性分类器中。",
    "en_tdlr": "This paper studies the generalization performance of memory-based methods in continual learning, finding that although these methods can help in traditional in-distribution generalization, they can strongly impair out-of-distribution generalization by learning spurious features and correlations, especially in linear classifiers."
}