{
    "title": "Lottery Tickets in Evolutionary Optimization: On Sparse Backpropagation-Free Trainability. (arXiv:2306.00045v1 [cs.NE])",
    "abstract": "Is the lottery ticket phenomenon an idiosyncrasy of gradient-based training or does it generalize to evolutionary optimization? In this paper we establish the existence of highly sparse trainable initializations for evolution strategies (ES) and characterize qualitative differences compared to gradient descent (GD)-based sparse training. We introduce a novel signal-to-noise iterative pruning procedure, which incorporates loss curvature information into the network pruning step. This can enable the discovery of even sparser trainable network initializations when using black-box evolution as compared to GD-based optimization. Furthermore, we find that these initializations encode an inductive bias, which transfers across different ES, related tasks and even to GD-based training. Finally, we compare the local optima resulting from the different optimization paradigms and sparsity levels. In contrast to GD, ES explore diverse and flat local optima and do not preserve linear mode connectivi",
    "link": "http://arxiv.org/abs/2306.00045",
    "context": "Title: Lottery Tickets in Evolutionary Optimization: On Sparse Backpropagation-Free Trainability. (arXiv:2306.00045v1 [cs.NE])\nAbstract: Is the lottery ticket phenomenon an idiosyncrasy of gradient-based training or does it generalize to evolutionary optimization? In this paper we establish the existence of highly sparse trainable initializations for evolution strategies (ES) and characterize qualitative differences compared to gradient descent (GD)-based sparse training. We introduce a novel signal-to-noise iterative pruning procedure, which incorporates loss curvature information into the network pruning step. This can enable the discovery of even sparser trainable network initializations when using black-box evolution as compared to GD-based optimization. Furthermore, we find that these initializations encode an inductive bias, which transfers across different ES, related tasks and even to GD-based training. Finally, we compare the local optima resulting from the different optimization paradigms and sparsity levels. In contrast to GD, ES explore diverse and flat local optima and do not preserve linear mode connectivi",
    "path": "papers/23/06/2306.00045.json",
    "total_tokens": 949,
    "translated_title": "进化优化中的彩票票现象：稀疏且无需反向传播训练",
    "translated_abstract": "彩票票现象是否只存在于梯度下降算法中，还是可以推广到进化优化算法中？本文证明了进化策略算法具有高度稀疏可训练的初始参数，并比较了与梯度下降算法稀疏训练的差异。我们提出了一种新的信噪比迭代剪枝过程，将损失曲率信息融入到网络剪枝步骤中，可以发现与梯度下降相比，黑盒进化算法方法有可能发现更稀疏且可训练的网络初始参数。此外，我们发现这些初始参数包含了归纳偏见，可以在不同的进化策略任务及梯度下降训练中进行传递。最后，我们比较了不同优化算法和稀疏水平产生的局部最优解，与梯度下降算法不同，进化策略算法可以探索各种不同的、平坦的局部最优解，并且不会保留线性模式的连接性。",
    "tldr": "本文证明了进化策略算法具有高度稀疏可训练的初始参数，并比较了与梯度下降算法稀疏训练的差异。进化策略算法可以探索各种不同的、平坦的局部最优解，并且不会保留线性模式的连接性。",
    "en_tdlr": "This paper establishes the existence of highly sparse trainable initializations for evolution strategies and compares the differences with gradient descent-based sparse training. Evolution strategies can explore diverse and flat local optima and do not preserve linear mode connectivity."
}