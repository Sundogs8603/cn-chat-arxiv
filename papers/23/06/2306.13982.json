{
    "title": "Mobile-Cloud Inference for Collaborative Intelligence. (arXiv:2306.13982v1 [cs.LG])",
    "abstract": "As AI applications for mobile devices become more prevalent, there is an increasing need for faster execution and lower energy consumption for deep learning model inference. Historically, the models run on mobile devices have been smaller and simpler in comparison to large state-of-the-art research models, which can only run on the cloud. However, cloud-only inference has drawbacks such as increased network bandwidth consumption and higher latency. In addition, cloud-only inference requires the input data (images, audio) to be fully transferred to the cloud, creating concerns about potential privacy breaches.  There is an alternative approach: shared mobile-cloud inference. Partial inference is performed on the mobile in order to reduce the dimensionality of the input data and arrive at a compact feature tensor, which is a latent space representation of the input signal. The feature tensor is then transmitted to the server for further inference. This strategy can reduce inference laten",
    "link": "http://arxiv.org/abs/2306.13982",
    "context": "Title: Mobile-Cloud Inference for Collaborative Intelligence. (arXiv:2306.13982v1 [cs.LG])\nAbstract: As AI applications for mobile devices become more prevalent, there is an increasing need for faster execution and lower energy consumption for deep learning model inference. Historically, the models run on mobile devices have been smaller and simpler in comparison to large state-of-the-art research models, which can only run on the cloud. However, cloud-only inference has drawbacks such as increased network bandwidth consumption and higher latency. In addition, cloud-only inference requires the input data (images, audio) to be fully transferred to the cloud, creating concerns about potential privacy breaches.  There is an alternative approach: shared mobile-cloud inference. Partial inference is performed on the mobile in order to reduce the dimensionality of the input data and arrive at a compact feature tensor, which is a latent space representation of the input signal. The feature tensor is then transmitted to the server for further inference. This strategy can reduce inference laten",
    "path": "papers/23/06/2306.13982.json",
    "total_tokens": 823,
    "translated_title": "移动云协同智能推断",
    "translated_abstract": "随着移动设备的人工智能应用越来越普及，对于深度学习模型推断的更快执行和更低的能量消耗需求也日益增加。历史上，移动设备上运行的模型较大型最新研究模型较小且简单，只能在云端运行。然而，只在云上进行推断存在带宽消耗增加和更高延迟的缺陷。此外，只在云上进行推断需要将输入数据（图像、音频）全部传输到云上，存在隐私泄露的风险。另有一种选择：共享移动云推断。在移动设备上执行部分推断以缩减输入数据的维度并生成一个紧凑的特征张量，这是输入信号的潜在空间表示。然后将特征张量传输到服务器进行进一步推断。这种策略可以降低推断时延。",
    "tldr": "移动设备上部分推断，生成紧凑的特征张量，并在服务器上进行进一步的推断，以降低推断时延和降低隐私风险。",
    "en_tdlr": "Partial inference on mobile devices generates a compact feature tensor, which is then transmitted to the server for further inference to reduce inference latency and privacy risks."
}