{
    "title": "Revisiting Neural Retrieval on Accelerators. (arXiv:2306.04039v1 [cs.LG])",
    "abstract": "Retrieval finds a small number of relevant candidates from a large corpus for information retrieval and recommendation applications. A key component of retrieval is to model (user, item) similarity, which is commonly represented as the dot product of two learned embeddings. This formulation permits efficient inference, commonly known as Maximum Inner Product Search (MIPS). Despite its popularity, dot products cannot capture complex user-item interactions, which are multifaceted and likely high rank. We hence examine non-dot-product retrieval settings on accelerators, and propose \\textit{mixture of logits} (MoL), which models (user, item) similarity as an adaptive composition of elementary similarity functions. This new formulation is expressive, capable of modeling high rank (user, item) interactions, and further generalizes to the long tail. When combined with a hierarchical retrieval strategy, \\textit{h-indexer}, we are able to scale up MoL to 100M corpus on a single GPU with latency",
    "link": "http://arxiv.org/abs/2306.04039",
    "context": "Title: Revisiting Neural Retrieval on Accelerators. (arXiv:2306.04039v1 [cs.LG])\nAbstract: Retrieval finds a small number of relevant candidates from a large corpus for information retrieval and recommendation applications. A key component of retrieval is to model (user, item) similarity, which is commonly represented as the dot product of two learned embeddings. This formulation permits efficient inference, commonly known as Maximum Inner Product Search (MIPS). Despite its popularity, dot products cannot capture complex user-item interactions, which are multifaceted and likely high rank. We hence examine non-dot-product retrieval settings on accelerators, and propose \\textit{mixture of logits} (MoL), which models (user, item) similarity as an adaptive composition of elementary similarity functions. This new formulation is expressive, capable of modeling high rank (user, item) interactions, and further generalizes to the long tail. When combined with a hierarchical retrieval strategy, \\textit{h-indexer}, we are able to scale up MoL to 100M corpus on a single GPU with latency",
    "path": "papers/23/06/2306.04039.json",
    "total_tokens": 954,
    "translated_title": "重新思考加速器上的神经检索",
    "translated_abstract": "信息检索需要从大量文本中找出与用户需求相关的文本。其中，建模用户与文本（item）的相似度是信息检索的关键。常见的做法是将用户与文本的相似度表示为两个学习嵌入的点积，又被称为最大内积检索（MIPS）。虽然这种方法具有效率，但是它无法捕捉复杂的用户-文本交互作用。本文提出了一种基于加速器的非点积检索方法：混合对数模型（Mixture of Logits，MoL）。该方法通过自适应地组合基本相似度函数来建模用户与文本之间的相似度，以更好地捕捉高阶用户-文本交互作用并进一步推广到长尾数据中。本文还结合一种分层检索策略（h-indexer）将MoL扩展到单个GPU上的100M个文本语料库。",
    "tldr": "本文提出了一种名为混合对数模型（MoL）的非点积检索方法。该方法通过自适应地组合基本相似度函数来建模用户与文本之间的相似度，以更好地捕捉高阶用户-文本交互作用并进一步推广到长尾数据中。结合分层检索策略（h-indexer），本文成功将MoL扩展到单个GPU上的100M个文本语料库。",
    "en_tdlr": "This paper proposes a non-dot-product retrieval method called Mixture of Logits (MoL) for modeling user-item similarity, which combines elementary similarity functions adaptively to capture high-rank user-item interactions and generalize to long-tail data. Combined with a hierarchical retrieval strategy (h-indexer), MoL is scalable up to 100M corpus on a single GPU."
}