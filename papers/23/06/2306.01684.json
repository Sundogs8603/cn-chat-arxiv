{
    "title": "Harnessing large-language models to generate private synthetic text. (arXiv:2306.01684v1 [cs.LG])",
    "abstract": "Differentially private (DP) training methods like DP-SGD can protect sensitive training data by ensuring that ML models will not reveal private information. An alternative approach, which this paper studies, is to use a sensitive dataset to generate a new synthetic dataset which is differentially private with respect to the original data. Doing so has several advantages: synthetic data can be reused for other tasks (including for hyper parameter tuning), retained indefinitely, or shared with third parties without sacrificing privacy.  However, obtaining DP data is much harder than introducing DP during training. To make it feasible for text, recent work has utilized public data by starting with a pre-trained generative language model and privately finetuning it on sensitive data. This model can be used to sample a DP synthetic dataset. While this strategy seems straightforward, executing it has proven problematic. Previous approaches either show significant performance loss, or have, a",
    "link": "http://arxiv.org/abs/2306.01684",
    "context": "Title: Harnessing large-language models to generate private synthetic text. (arXiv:2306.01684v1 [cs.LG])\nAbstract: Differentially private (DP) training methods like DP-SGD can protect sensitive training data by ensuring that ML models will not reveal private information. An alternative approach, which this paper studies, is to use a sensitive dataset to generate a new synthetic dataset which is differentially private with respect to the original data. Doing so has several advantages: synthetic data can be reused for other tasks (including for hyper parameter tuning), retained indefinitely, or shared with third parties without sacrificing privacy.  However, obtaining DP data is much harder than introducing DP during training. To make it feasible for text, recent work has utilized public data by starting with a pre-trained generative language model and privately finetuning it on sensitive data. This model can be used to sample a DP synthetic dataset. While this strategy seems straightforward, executing it has proven problematic. Previous approaches either show significant performance loss, or have, a",
    "path": "papers/23/06/2306.01684.json",
    "total_tokens": 919,
    "translated_title": "利用大型语言模型生成私有的合成文本",
    "translated_abstract": "差分隐私训练方法，如DP-SGD，可以通过确保机器学习模型不会透露私有信息来保护敏感的训练数据。本文研究了一种替代方法，利用敏感数据集生成新的合成数据集，并确保相对于原始数据具有差分隐私。这样做有几个优点：合成数据可以重复使用于其他任务（包括超参数调整），可以无限期保留，或与第三方共享而不损失隐私。但是，获取差分隐私数据比在训练期间引入差分隐私更加困难。为了使其在文本中可行，最近的研究利用公共数据，从预训练的生成语言模型开始，并在敏感数据上进行私人调整。该模型可以用于抽样差分隐私合成数据集。虽然这个策略似乎很简单，但执行它已被证明是有问题的。以前的方法要么表现出显著的性能损失，要么...",
    "tldr": "本文研究了一种通过利用公开数据集和预训练的生成语言模型，结合私有的微调方法，实现生成具有差分隐私性质的合成文本数据集的方法。生成的文本数据集可以重复使用于其他任务，可无限期保留，或与第三方共享而不损失隐私。",
    "en_tdlr": "This paper studies a method to generate differentially private synthetic text dataset by utilizing pre-trained generative language model and private finetuning on sensitive data, which can be reused for other tasks and shared with third parties without sacrificing privacy."
}