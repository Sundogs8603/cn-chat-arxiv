{
    "title": "Light Coreference Resolution for Russian with Hierarchical Discourse Features. (arXiv:2306.01465v1 [cs.CL])",
    "abstract": "Coreference resolution is the task of identifying and grouping mentions referring to the same real-world entity. Previous neural models have mainly focused on learning span representations and pairwise scores for coreference decisions. However, current methods do not explicitly capture the referential choice in the hierarchical discourse, an important factor in coreference resolution. In this study, we propose a new approach that incorporates rhetorical information into neural coreference resolution models. We collect rhetorical features from automated discourse parses and examine their impact. As a base model, we implement an end-to-end span-based coreference resolver using a partially fine-tuned multilingual entity-aware language model LUKE. We evaluate our method on the RuCoCo-23 Shared Task for coreference resolution in Russian. Our best model employing rhetorical distance between mentions has ranked 1st on the development set (74.6% F1) and 2nd on the test set (73.3% F1) of the Sh",
    "link": "http://arxiv.org/abs/2306.01465",
    "context": "Title: Light Coreference Resolution for Russian with Hierarchical Discourse Features. (arXiv:2306.01465v1 [cs.CL])\nAbstract: Coreference resolution is the task of identifying and grouping mentions referring to the same real-world entity. Previous neural models have mainly focused on learning span representations and pairwise scores for coreference decisions. However, current methods do not explicitly capture the referential choice in the hierarchical discourse, an important factor in coreference resolution. In this study, we propose a new approach that incorporates rhetorical information into neural coreference resolution models. We collect rhetorical features from automated discourse parses and examine their impact. As a base model, we implement an end-to-end span-based coreference resolver using a partially fine-tuned multilingual entity-aware language model LUKE. We evaluate our method on the RuCoCo-23 Shared Task for coreference resolution in Russian. Our best model employing rhetorical distance between mentions has ranked 1st on the development set (74.6% F1) and 2nd on the test set (73.3% F1) of the Sh",
    "path": "papers/23/06/2306.01465.json",
    "total_tokens": 870,
    "translated_title": "带有分层话语特征的俄语轻量级指代消解",
    "translated_abstract": "指代消解是识别和分组指称同一现实世界实体的任务。以往神经模型主要集中于学习跨度表示和配对得分以进行指代消解。然而，目前的方法没有明确捕捉到层级话语中的指称选择，这是指代消解中的重要因素。本研究提出了一种新的方法，将修辞信息纳入神经指代消解模型中，收集自动话语分析的修辞特征并检查其影响。作为基线模型，我们使用部分微调的多语言实体感知语言模型LUKE实现了端到端的基于跨度的指代消解解析器。我们在俄语的RuCoCo-23共享任务中评估了我们的方法。我们最佳模型采用了提及之间的修辞距离，并在开发集（74.6％ F1）上排名第1，在测试集（73.3％ F1）上排名第2。",
    "tldr": "本篇论文提出了一种通过收集自动话语分析的修辞特征来将修辞信息纳入神经指代消解模型的新方法，并取得了俄语RuCoCo-23共享任务中的最好成绩。",
    "en_tdlr": "This paper proposes a new method that incorporates rhetorical features into neural coreference resolution models by collecting automated discourse parses and achieves the best performance in the Russian RuCoCo-23 Shared Task."
}