{
    "title": "Tensorformer: Normalized Matrix Attention Transformer for High-quality Point Cloud Reconstruction. (arXiv:2306.15989v1 [cs.GR])",
    "abstract": "Surface reconstruction from raw point clouds has been studied for decades in the computer graphics community, which is highly demanded by modeling and rendering applications nowadays. Classic solutions, such as Poisson surface reconstruction, require point normals as extra input to perform reasonable results. Modern transformer-based methods can work without normals, while the results are less fine-grained due to limited encoding performance in local fusion from discrete points. We introduce a novel normalized matrix attention transformer (Tensorformer) to perform high-quality reconstruction. The proposed matrix attention allows for simultaneous point-wise and channel-wise message passing, while the previous vector attention loses neighbor point information across different channels. It brings more degree of freedom in feature learning and thus facilitates better modeling of local geometries. Our method achieves state-of-the-art on two commonly used datasets, ShapeNetCore and ABC, and ",
    "link": "http://arxiv.org/abs/2306.15989",
    "context": "Title: Tensorformer: Normalized Matrix Attention Transformer for High-quality Point Cloud Reconstruction. (arXiv:2306.15989v1 [cs.GR])\nAbstract: Surface reconstruction from raw point clouds has been studied for decades in the computer graphics community, which is highly demanded by modeling and rendering applications nowadays. Classic solutions, such as Poisson surface reconstruction, require point normals as extra input to perform reasonable results. Modern transformer-based methods can work without normals, while the results are less fine-grained due to limited encoding performance in local fusion from discrete points. We introduce a novel normalized matrix attention transformer (Tensorformer) to perform high-quality reconstruction. The proposed matrix attention allows for simultaneous point-wise and channel-wise message passing, while the previous vector attention loses neighbor point information across different channels. It brings more degree of freedom in feature learning and thus facilitates better modeling of local geometries. Our method achieves state-of-the-art on two commonly used datasets, ShapeNetCore and ABC, and ",
    "path": "papers/23/06/2306.15989.json",
    "total_tokens": 895,
    "translated_title": "Tensorformer: 高质量点云重建的归一化矩阵注意力变换器",
    "translated_abstract": "在计算机图形学界，从原始点云进行表面重建的研究已经进行了几十年，这在现今的建模和渲染应用中需求非常高。传统的解决方案，如Poisson表面重建，需要额外的点法线输入以产生合理的结果。现代基于变换器的方法可以在没有法线的情况下工作，但由于离散点的局部融合编码性能有限，结果较为粗糙。我们引入了一种新颖的归一化矩阵注意力变换器（Tensorformer）来进行高质量的重建。所提出的矩阵注意力允许同时进行逐点和逐通道的消息传递，而之前的向量注意力在不同通道之间丢失了相邻点的信息。它在特征学习中带来更多自由度，从而更好地建模局部几何结构。我们的方法在两个常用数据集ShapeNetCore和ABC上达到了最先进的水平，并且",
    "tldr": "Tensorformer是一种归一化矩阵注意力变换器，用于高质量的点云重建。它通过矩阵注意力实现了逐点和逐通道的消息传递，提供了更好的局部几何建模能力，并在两个数据集上取得了最先进的结果。"
}