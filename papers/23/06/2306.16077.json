{
    "title": "Secure and Fast Asynchronous Vertical Federated Learning via Cascaded Hybrid Optimization. (arXiv:2306.16077v1 [cs.LG])",
    "abstract": "Vertical Federated Learning (VFL) attracts increasing attention because it empowers multiple parties to jointly train a privacy-preserving model over vertically partitioned data. Recent research has shown that applying zeroth-order optimization (ZOO) has many advantages in building a practical VFL algorithm. However, a vital problem with the ZOO-based VFL is its slow convergence rate, which limits its application in handling modern large models. To address this problem, we propose a cascaded hybrid optimization method in VFL. In this method, the downstream models (clients) are trained with ZOO to protect privacy and ensure that no internal information is shared. Meanwhile, the upstream model (server) is updated with first-order optimization (FOO) locally, which significantly improves the convergence rate, making it feasible to train the large models without compromising privacy and security. We theoretically prove that our VFL framework converges faster than the ZOO-based VFL, as the c",
    "link": "http://arxiv.org/abs/2306.16077",
    "context": "Title: Secure and Fast Asynchronous Vertical Federated Learning via Cascaded Hybrid Optimization. (arXiv:2306.16077v1 [cs.LG])\nAbstract: Vertical Federated Learning (VFL) attracts increasing attention because it empowers multiple parties to jointly train a privacy-preserving model over vertically partitioned data. Recent research has shown that applying zeroth-order optimization (ZOO) has many advantages in building a practical VFL algorithm. However, a vital problem with the ZOO-based VFL is its slow convergence rate, which limits its application in handling modern large models. To address this problem, we propose a cascaded hybrid optimization method in VFL. In this method, the downstream models (clients) are trained with ZOO to protect privacy and ensure that no internal information is shared. Meanwhile, the upstream model (server) is updated with first-order optimization (FOO) locally, which significantly improves the convergence rate, making it feasible to train the large models without compromising privacy and security. We theoretically prove that our VFL framework converges faster than the ZOO-based VFL, as the c",
    "path": "papers/23/06/2306.16077.json",
    "total_tokens": 940,
    "translated_title": "安全高效的异步垂直联邦学习:基于级联混合优化方法",
    "translated_abstract": "垂直联邦学习(VFL)因能够在垂直分割的数据上联合训练隐私保护模型而引起越来越多的关注。最近的研究表明，应用零阶优化(ZOO)在构建实用的VFL算法方面具有许多优势。然而，基于ZOO的VFL存在一个关键问题，即其收敛速度较慢，限制了其在处理现代大型模型时的应用。为了解决这个问题，我们提出了一种在VFL中使用级联混合优化方法。该方法中，下游模型（客户端）使用ZOO进行训练以保护隐私并确保不共享内部信息。同时，上游模型（服务器）在本地使用一阶优化(FOO)进行更新，这显著提高了收敛速度，使得能够在不损害隐私和安全性的前提下训练大型模型。我们在理论上证明了我们的VFL框架比基于ZOO的VFL更快地收敛。",
    "tldr": "本论文提出了一种在垂直联邦学习中使用级联混合优化的方法，通过在下游使用零阶优化保护隐私并在上游使用一阶优化提高收敛速度，从而解决了ZOO-based VFL收敛速度较慢的问题。",
    "en_tdlr": "This paper proposes a cascaded hybrid optimization method for vertical federated learning, which addresses the slow convergence rate issue in ZOO-based VFL by using zeroth-order optimization for privacy protection in downstream and first-order optimization for improved convergence rate in the upstream."
}