{
    "title": "Weight Freezing: A Regularization Approach for Fully Connected Layers with an Application in EEG Classification. (arXiv:2306.05775v1 [cs.LG])",
    "abstract": "In the realm of EEG decoding, enhancing the performance of artificial neural networks (ANNs) carries significant potential. This study introduces a novel approach, termed \"weight freezing\", that is anchored on the principles of ANN regularization and neuroscience prior knowledge. The concept of weight freezing revolves around the idea of reducing certain neurons' influence on the decision-making process for a specific EEG task by freezing specific weights in the fully connected layer during the backpropagation process. This is actualized through the use of a mask matrix and a threshold to determine the proportion of weights to be frozen during backpropagation. Moreover, by setting the masked weights to zero, weight freezing can not only realize sparse connections in networks with a fully connected layer as the classifier but also function as an efficacious regularization method for fully connected layers. Through experiments involving three distinct ANN architectures and three widely r",
    "link": "http://arxiv.org/abs/2306.05775",
    "context": "Title: Weight Freezing: A Regularization Approach for Fully Connected Layers with an Application in EEG Classification. (arXiv:2306.05775v1 [cs.LG])\nAbstract: In the realm of EEG decoding, enhancing the performance of artificial neural networks (ANNs) carries significant potential. This study introduces a novel approach, termed \"weight freezing\", that is anchored on the principles of ANN regularization and neuroscience prior knowledge. The concept of weight freezing revolves around the idea of reducing certain neurons' influence on the decision-making process for a specific EEG task by freezing specific weights in the fully connected layer during the backpropagation process. This is actualized through the use of a mask matrix and a threshold to determine the proportion of weights to be frozen during backpropagation. Moreover, by setting the masked weights to zero, weight freezing can not only realize sparse connections in networks with a fully connected layer as the classifier but also function as an efficacious regularization method for fully connected layers. Through experiments involving three distinct ANN architectures and three widely r",
    "path": "papers/23/06/2306.05775.json",
    "total_tokens": 853,
    "translated_title": "权重冻结：一种正则化方法在脑电分类中的应用",
    "translated_abstract": "在脑电解码领域，提高人工神经网络（ANNs）的性能具有重要的潜力。本研究介绍了一种新颖的方法，称为“权重冻结”，该方法基于ANN正则化和神经科学先验知识的原则。权重冻结的概念围绕着通过冻结全连接层中的特定权重来减少某些神经元对特定EEG任务的决策过程的影响。这是通过使用掩码矩阵和阈值来确定在反向传播过程中需要冻结的权重比例来实现的。此外，通过将被掩蔽的权重设置为零，权重冻结不仅可以在具有全连接层分类器的网络中实现稀疏连接，还可以作为全连接层的有效正则化方法。",
    "tldr": "本文提出了一种新颖的方法，权重冻结，它可以有效地减少全连接层中的特定神经元对特定EEG任务的决策过程的影响，从而提高人工神经网络（ANNs）的性能，是一种有效的正则化方法。"
}