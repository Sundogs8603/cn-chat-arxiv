{
    "title": "Training-Free Neural Active Learning with Initialization-Robustness Guarantees. (arXiv:2306.04454v1 [cs.LG])",
    "abstract": "Existing neural active learning algorithms have aimed to optimize the predictive performance of neural networks (NNs) by selecting data for labelling. However, other than a good predictive performance, being robust against random parameter initializations is also a crucial requirement in safety-critical applications. To this end, we introduce our expected variance with Gaussian processes (EV-GP) criterion for neural active learning, which is theoretically guaranteed to select data points which lead to trained NNs with both (a) good predictive performances and (b) initialization robustness. Importantly, our EV-GP criterion is training-free, i.e., it does not require any training of the NN during data selection, which makes it computationally efficient. We empirically demonstrate that our EV-GP criterion is highly correlated with both initialization robustness and generalization performance, and show that it consistently outperforms baseline methods in terms of both desiderata, especiall",
    "link": "http://arxiv.org/abs/2306.04454",
    "context": "Title: Training-Free Neural Active Learning with Initialization-Robustness Guarantees. (arXiv:2306.04454v1 [cs.LG])\nAbstract: Existing neural active learning algorithms have aimed to optimize the predictive performance of neural networks (NNs) by selecting data for labelling. However, other than a good predictive performance, being robust against random parameter initializations is also a crucial requirement in safety-critical applications. To this end, we introduce our expected variance with Gaussian processes (EV-GP) criterion for neural active learning, which is theoretically guaranteed to select data points which lead to trained NNs with both (a) good predictive performances and (b) initialization robustness. Importantly, our EV-GP criterion is training-free, i.e., it does not require any training of the NN during data selection, which makes it computationally efficient. We empirically demonstrate that our EV-GP criterion is highly correlated with both initialization robustness and generalization performance, and show that it consistently outperforms baseline methods in terms of both desiderata, especiall",
    "path": "papers/23/06/2306.04454.json",
    "total_tokens": 905,
    "translated_title": "无需训练的神经主动学习与初始化鲁棒性保证",
    "translated_abstract": "现有的神经主动学习算法旨在通过选择数据进行标记来优化神经网络（NN）的预测性能。然而，除了良好的预测性能外，对于随机参数初始化的鲁棒性也是安全关键应用的重要要求。为此，我们引入了期望方差与高斯过程（EV-GP）标准来进行神经主动学习，该标准在理论上保证选择数据点可以导致训练的 NN 具有良好的预测性能和初始化鲁棒性。重要的是，我们的 EV-GP 标准是无需训练的，即在数据选择过程中不需要对 NN 进行任何训练，这使其在计算上更加高效。我们通过实验证明，我们的 EV-GP 标准与初始化鲁棒性和概括性能高度相关，并且表明它在两个期望方面的表现均优于基线方法，尤其是初始化鲁棒性方面。",
    "tldr": "本研究提出了一种期望方差与高斯过程（EV-GP）标准用于神经主动学习，该方法不需要对神经网络进行训练，并且保证在数据选择时NN具有良好的预测性能和初始化鲁棒性。",
    "en_tdlr": "This paper proposes an expected variance with Gaussian processes (EV-GP) criterion for neural active learning, which is training-free and theoretically guaranteed to select data points leading to trained neural networks with both good predictive performances and initialization robustness."
}