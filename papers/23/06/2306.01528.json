{
    "title": "Does it pay to optimize AUC?. (arXiv:2306.01528v1 [cs.CG])",
    "abstract": "The Area Under the ROC Curve (AUC) is an important model metric for evaluating binary classifiers, and many algorithms have been proposed to optimize AUC approximately. It raises the question of whether the generally insignificant gains observed by previous studies are due to inherent limitations of the metric or the inadequate quality of optimization.  To better understand the value of optimizing for AUC, we present an efficient algorithm, namely AUC-opt, to find the provably optimal AUC linear classifier in $\\mathbb{R}^2$, which runs in $\\mathcal{O}(n_+ n_- \\log (n_+ n_-))$ where $n_+$ and $n_-$ are the number of positive and negative samples respectively. Furthermore, it can be naturally extended to $\\mathbb{R}^d$ in $\\mathcal{O}((n_+n_-)^{d-1}\\log (n_+n_-))$ by calling AUC-opt in lower-dimensional spaces recursively. We prove the problem is NP-complete when $d$ is not fixed, reducing from the \\textit{open hemisphere problem}.  Experiments show that compared with other methods, AUC-",
    "link": "http://arxiv.org/abs/2306.01528",
    "context": "Title: Does it pay to optimize AUC?. (arXiv:2306.01528v1 [cs.CG])\nAbstract: The Area Under the ROC Curve (AUC) is an important model metric for evaluating binary classifiers, and many algorithms have been proposed to optimize AUC approximately. It raises the question of whether the generally insignificant gains observed by previous studies are due to inherent limitations of the metric or the inadequate quality of optimization.  To better understand the value of optimizing for AUC, we present an efficient algorithm, namely AUC-opt, to find the provably optimal AUC linear classifier in $\\mathbb{R}^2$, which runs in $\\mathcal{O}(n_+ n_- \\log (n_+ n_-))$ where $n_+$ and $n_-$ are the number of positive and negative samples respectively. Furthermore, it can be naturally extended to $\\mathbb{R}^d$ in $\\mathcal{O}((n_+n_-)^{d-1}\\log (n_+n_-))$ by calling AUC-opt in lower-dimensional spaces recursively. We prove the problem is NP-complete when $d$ is not fixed, reducing from the \\textit{open hemisphere problem}.  Experiments show that compared with other methods, AUC-",
    "path": "papers/23/06/2306.01528.json",
    "total_tokens": 1162,
    "translated_title": "AUC优化是否值得？",
    "translated_abstract": "ROC曲线下面积（AUC）是二元分类器评估的一个重要模型指标，已经提出了许多算法来近似优化AUC。这引发了一个问题，即先前研究观察到的普遍微不足道的收益是由指标固有的限制还是由优化的不足质量引起的？为了更好地理解优化AUC的价值，我们提出了一种高效算法AUC-opt，在$\\mathbb{R}^2$中找到可证明的最优AUC线性分类器，其运行时间为$\\mathcal{O}(n_+ n_- \\log (n_+ n_-))$，其中$n_+$和$n_-$分别是正样本和负样本的数量。此外，通过递归调用低维空间中的AUC-opt，它可以自然地扩展到$\\mathbb{R}^d$中，时间复杂度为$\\mathcal{O}((n_+n_-)^{d-1}\\log (n_+n_-))$。我们证明，当$d$不固定时，该问题是NP完全的，从“开放半球问题”中减少。实验表明，与其他方法相比，AUC-opt在合成和真实数据集上都可以显着提高AUC值。我们的结果表明，优化AUC确实有价值，并且先前研究的限制主要是由于缺乏高效的优化算法。",
    "tldr": "本文提出了一种高效算法AUC-opt，用于在$\\mathbb{R}^2$中找到可证明的最优AUC线性分类器，并可扩展到$\\mathbb{R}^d$。实验表明，在合成和真实数据集上，与其他方法相比，AUC-opt可以显着提高AUC值。优化AUC确实有价值，并且先前研究的限制主要是由于缺乏高效的优化算法。",
    "en_tdlr": "This paper proposes an efficient algorithm, AUC-opt, to find the provably optimal AUC linear classifier in $\\mathbb{R}^2$ and extend it to $\\mathbb{R}^d$. Experiment shows superior performance compared to other methods, indicating the value of optimizing AUC with the limitation of previous studies mainly due to the lack of efficient optimization algorithms."
}