{
    "title": "Design from Policies: Conservative Test-Time Adaptation for Offline Policy Optimization. (arXiv:2306.14479v2 [cs.LG] UPDATED)",
    "abstract": "In this work, we decouple the iterative bi-level offline RL (value estimation and policy extraction) from the offline training phase, forming a non-iterative bi-level paradigm and avoiding the iterative error propagation over two levels. Specifically, this non-iterative paradigm allows us to conduct inner-level optimization (value estimation) in training, while performing outer-level optimization (policy extraction) in testing. Naturally, such a paradigm raises three core questions that are not fully answered by prior non-iterative offline RL counterparts like reward-conditioned policy: (q1) What information should we transfer from the inner-level to the outer-level? (q2) What should we pay attention to when exploiting the transferred information for safe/confident outer-level optimization? (q3) What are the benefits of concurrently conducting outer-level optimization during testing? Motivated by model-based optimization (MBO), we propose DROP (design from policies), which fully answer",
    "link": "http://arxiv.org/abs/2306.14479",
    "context": "Title: Design from Policies: Conservative Test-Time Adaptation for Offline Policy Optimization. (arXiv:2306.14479v2 [cs.LG] UPDATED)\nAbstract: In this work, we decouple the iterative bi-level offline RL (value estimation and policy extraction) from the offline training phase, forming a non-iterative bi-level paradigm and avoiding the iterative error propagation over two levels. Specifically, this non-iterative paradigm allows us to conduct inner-level optimization (value estimation) in training, while performing outer-level optimization (policy extraction) in testing. Naturally, such a paradigm raises three core questions that are not fully answered by prior non-iterative offline RL counterparts like reward-conditioned policy: (q1) What information should we transfer from the inner-level to the outer-level? (q2) What should we pay attention to when exploiting the transferred information for safe/confident outer-level optimization? (q3) What are the benefits of concurrently conducting outer-level optimization during testing? Motivated by model-based optimization (MBO), we propose DROP (design from policies), which fully answer",
    "path": "papers/23/06/2306.14479.json",
    "total_tokens": 916,
    "translated_title": "从策略中设计：线下策略优化的保守测试时适应",
    "translated_abstract": "在这项工作中，我们将迭代式双层线下强化学习（值估计和策略抽取）与线下训练阶段解耦，形成非迭代的双层范式，并避免了两个层次的迭代误差传播。具体而言，这种非迭代的范式允许我们在训练中进行内层优化（值估计），同时在测试中进行外层优化（策略抽取）。自然地，这种范式提出了三个核心问题，这些问题在之前的非迭代线下强化学习对应物（如奖励条件策略）中没有得到完全回答：(q1)我们应该从内层向外层传递什么信息？(q2)当利用传递的信息进行安全/自信的外层优化时，我们应该注意什么？(q3)在测试期间同时进行外层优化有什么好处？受到基于模型的优化（MBO）的启发，我们提出了DROP（从策略中设计），它完全回答了这三个问题。",
    "tldr": "本文提出了一种非迭代的双层范式，将迭代式双层线下强化学习解耦，并回答了传递信息、安全优化和同时进行外层优化的核心问题。",
    "en_tdlr": "This paper proposes a non-iterative bi-level paradigm for offline reinforcement learning, which decouples the iterative process and addresses key questions regarding information transfer, safe optimization, and concurrent outer-level optimization."
}