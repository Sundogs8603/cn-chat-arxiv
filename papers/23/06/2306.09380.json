{
    "title": "Understanding Parameter Sharing in Transformers. (arXiv:2306.09380v1 [cs.LG])",
    "abstract": "Parameter sharing has proven to be a parameter-efficient approach. Previous work on Transformers has focused on sharing parameters in different layers, which can improve the performance of models with limited parameters by increasing model depth. In this paper, we study why this approach works from two perspectives. First, increasing model depth makes the model more complex, and we hypothesize that the reason is related to model complexity (referring to FLOPs). Secondly, since each shared parameter will participate in the network computation several times in forward propagation, its corresponding gradient will have a different range of values from the original model, which will affect the model convergence. Based on this, we hypothesize that training convergence may also be one of the reasons. Through further analysis, we show that the success of this approach can be largely attributed to better convergence, with only a small part due to the increased model complexity. Inspired by this",
    "link": "http://arxiv.org/abs/2306.09380",
    "context": "Title: Understanding Parameter Sharing in Transformers. (arXiv:2306.09380v1 [cs.LG])\nAbstract: Parameter sharing has proven to be a parameter-efficient approach. Previous work on Transformers has focused on sharing parameters in different layers, which can improve the performance of models with limited parameters by increasing model depth. In this paper, we study why this approach works from two perspectives. First, increasing model depth makes the model more complex, and we hypothesize that the reason is related to model complexity (referring to FLOPs). Secondly, since each shared parameter will participate in the network computation several times in forward propagation, its corresponding gradient will have a different range of values from the original model, which will affect the model convergence. Based on this, we hypothesize that training convergence may also be one of the reasons. Through further analysis, we show that the success of this approach can be largely attributed to better convergence, with only a small part due to the increased model complexity. Inspired by this",
    "path": "papers/23/06/2306.09380.json",
    "total_tokens": 851,
    "translated_title": "理解Transformer中的参数共享",
    "translated_abstract": "参数共享已经被证明是一种高效的方法。在Transformer上的先前工作集中在在不同层次上共享参数，这可以通过增加模型深度来提高有限参数模型的性能。在本文中，我们从两个角度研究为什么此方法有效。首先，增加模型深度会使模型更加复杂，我们假设原因与模型复杂度（指FLOPs）有关。其次，由于每个共享参数在向前传播中会参与网络计算多次，其对应的梯度值与原模型的梯度值范围不同，这将影响模型的收敛性。基于此，我们假设训练收敛性也是其中之一的原因。通过进一步的分析，我们显示此方法的成功很大程度上归功于更好的收敛性，只有一小部分归因于增加的模型复杂度。这启发了我们进行更深层次的研究。",
    "tldr": "本文从模型复杂度和梯度范围两个角度研究了Transformer中参数共享的有效性，发现提高训练收敛性是其中主要原因，模型复杂度只有一小部分贡献。",
    "en_tdlr": "This paper studies the effectiveness of parameter sharing in Transformers from the perspectives of model complexity and gradient range, and concludes that the improved training convergence is the main reason for the success, with only a small contribution from increased model complexity."
}