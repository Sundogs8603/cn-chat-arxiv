{
    "title": "Towards Foundation Models for Scientific Machine Learning: Characterizing Scaling and Transfer Behavior. (arXiv:2306.00258v1 [cs.LG])",
    "abstract": "Pre-trained machine learning (ML) models have shown great performance for a wide range of applications, in particular in natural language processing (NLP) and computer vision (CV). Here, we study how pre-training could be used for scientific machine learning (SciML) applications, specifically in the context of transfer learning. We study the transfer behavior of these models as (i) the pre-trained model size is scaled, (ii) the downstream training dataset size is scaled, (iii) the physics parameters are systematically pushed out of distribution, and (iv) how a single model pre-trained on a mixture of different physics problems can be adapted to various downstream applications. We find that-when fine-tuned appropriately-transfer learning can help reach desired accuracy levels with orders of magnitude fewer downstream examples (across different tasks that can even be out-of-distribution) than training from scratch, with consistent behavior across a wide range of downstream examples. We a",
    "link": "http://arxiv.org/abs/2306.00258",
    "context": "Title: Towards Foundation Models for Scientific Machine Learning: Characterizing Scaling and Transfer Behavior. (arXiv:2306.00258v1 [cs.LG])\nAbstract: Pre-trained machine learning (ML) models have shown great performance for a wide range of applications, in particular in natural language processing (NLP) and computer vision (CV). Here, we study how pre-training could be used for scientific machine learning (SciML) applications, specifically in the context of transfer learning. We study the transfer behavior of these models as (i) the pre-trained model size is scaled, (ii) the downstream training dataset size is scaled, (iii) the physics parameters are systematically pushed out of distribution, and (iv) how a single model pre-trained on a mixture of different physics problems can be adapted to various downstream applications. We find that-when fine-tuned appropriately-transfer learning can help reach desired accuracy levels with orders of magnitude fewer downstream examples (across different tasks that can even be out-of-distribution) than training from scratch, with consistent behavior across a wide range of downstream examples. We a",
    "path": "papers/23/06/2306.00258.json",
    "total_tokens": 927,
    "translated_title": "面向科学机器学习的基础模型：特征比较与迁移性能",
    "translated_abstract": "预训练的机器学习（ML）模型在自然语言处理（NLP）和计算机视觉（CV）等各种应用中表现出巨大的性能。在这里，我们研究了预训练模型如何在科学机器学习（SciML）应用中应用，特别是在迁移学习的上下文中。我们研究了这些模型的迁移特性，包括（i）预训练模型大小的缩放，（ii）下游训练数据集大小的缩放，（iii）系统地将物理参数推出分布，以及（iv）如何将单个在不同物理问题的混合物上预训练的模型适应于各种下游应用。我们发现，适当地微调迁移学习可以帮助在比从头开始训练少几个数量级的下游示例（甚至可以是分布之外的不同任务）达到所需的准确性水平，并且在各种下游示例中具有一致的行为。",
    "tldr": "本研究研究了预训练模型在科学机器学习中的迁移性能，发现适当微调迁移学习可以代替从头开始训练，达到更高的准确性水平，而且减少了许多数据集的使用。",
    "en_tdlr": "This study investigates the transfer performance of pre-trained models in scientific machine learning and finds that fine-tuning transfer learning can replace training from scratch, achieve higher accuracy levels, and reduce the amount of required datasets across a wide range of downstream examples."
}