{
    "title": "Harnessing Mixed Offline Reinforcement Learning Datasets via Trajectory Weighting. (arXiv:2306.13085v1 [cs.LG])",
    "abstract": "Most offline reinforcement learning (RL) algorithms return a target policy maximizing a trade-off between (1) the expected performance gain over the behavior policy that collected the dataset, and (2) the risk stemming from the out-of-distribution-ness of the induced state-action occupancy. It follows that the performance of the target policy is strongly related to the performance of the behavior policy and, thus, the trajectory return distribution of the dataset. We show that in mixed datasets consisting of mostly low-return trajectories and minor high-return trajectories, state-of-the-art offline RL algorithms are overly restrained by low-return trajectories and fail to exploit high-performing trajectories to the fullest. To overcome this issue, we show that, in deterministic MDPs with stochastic initial states, the dataset sampling can be re-weighted to induce an artificial dataset whose behavior policy has a higher return. This re-weighted sampling strategy may be combined with any",
    "link": "http://arxiv.org/abs/2306.13085",
    "context": "Title: Harnessing Mixed Offline Reinforcement Learning Datasets via Trajectory Weighting. (arXiv:2306.13085v1 [cs.LG])\nAbstract: Most offline reinforcement learning (RL) algorithms return a target policy maximizing a trade-off between (1) the expected performance gain over the behavior policy that collected the dataset, and (2) the risk stemming from the out-of-distribution-ness of the induced state-action occupancy. It follows that the performance of the target policy is strongly related to the performance of the behavior policy and, thus, the trajectory return distribution of the dataset. We show that in mixed datasets consisting of mostly low-return trajectories and minor high-return trajectories, state-of-the-art offline RL algorithms are overly restrained by low-return trajectories and fail to exploit high-performing trajectories to the fullest. To overcome this issue, we show that, in deterministic MDPs with stochastic initial states, the dataset sampling can be re-weighted to induce an artificial dataset whose behavior policy has a higher return. This re-weighted sampling strategy may be combined with any",
    "path": "papers/23/06/2306.13085.json",
    "total_tokens": 837,
    "translated_title": "通过轨迹加权利用混合的离线强化学习数据集",
    "translated_abstract": "多数离线强化学习算法返回一个最大化预期表现与诱导状态-动作占用的分布差异所带来的风险之间的权衡的目标策略，因此，目标策略的表现与数据集收集的行为策略的表现密切相关。我们发现，在由大多数低回报轨迹和少数高回报轨迹组成的混合数据集中，现有的离线强化学习算法受到低回报轨迹的过度制约，未能充分利用高表现轨迹。为了解决这个问题，我们表明，在带有随机初始状态的确定性MDPs中，可以通过重新加权采样数据集来诱导具有更高回报的行为策略的人工数据集。这种重新加权的采样策略可以与任何算法结合使用。",
    "tldr": "该论文提出了一种混合离线强化学习数据集的改进方法，可以通过重新加权采样数据集来充分利用高表现的轨迹，以提高目标策略的表现。"
}