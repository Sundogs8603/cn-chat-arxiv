{
    "title": "ConKI: Contrastive Knowledge Injection for Multimodal Sentiment Analysis. (arXiv:2306.15796v1 [cs.AI])",
    "abstract": "Multimodal Sentiment Analysis leverages multimodal signals to detect the sentiment of a speaker. Previous approaches concentrate on performing multimodal fusion and representation learning based on general knowledge obtained from pretrained models, which neglects the effect of domain-specific knowledge. In this paper, we propose Contrastive Knowledge Injection (ConKI) for multimodal sentiment analysis, where specific-knowledge representations for each modality can be learned together with general knowledge representations via knowledge injection based on an adapter architecture. In addition, ConKI uses a hierarchical contrastive learning procedure performed between knowledge types within every single modality, across modalities within each sample, and across samples to facilitate the effective learning of the proposed representations, hence improving multimodal sentiment predictions. The experiments on three popular multimodal sentiment analysis benchmarks show that ConKI outperforms a",
    "link": "http://arxiv.org/abs/2306.15796",
    "context": "Title: ConKI: Contrastive Knowledge Injection for Multimodal Sentiment Analysis. (arXiv:2306.15796v1 [cs.AI])\nAbstract: Multimodal Sentiment Analysis leverages multimodal signals to detect the sentiment of a speaker. Previous approaches concentrate on performing multimodal fusion and representation learning based on general knowledge obtained from pretrained models, which neglects the effect of domain-specific knowledge. In this paper, we propose Contrastive Knowledge Injection (ConKI) for multimodal sentiment analysis, where specific-knowledge representations for each modality can be learned together with general knowledge representations via knowledge injection based on an adapter architecture. In addition, ConKI uses a hierarchical contrastive learning procedure performed between knowledge types within every single modality, across modalities within each sample, and across samples to facilitate the effective learning of the proposed representations, hence improving multimodal sentiment predictions. The experiments on three popular multimodal sentiment analysis benchmarks show that ConKI outperforms a",
    "path": "papers/23/06/2306.15796.json",
    "total_tokens": 839,
    "translated_title": "ConKI: 对多模态情感分析的对比性知识注入",
    "translated_abstract": "多模态情感分析利用多模态信号来检测说话者的情感。之前的方法集中于基于预训练模型获得的通用知识上进行多模态融合和表示学习，忽略了领域特定知识的影响。本文提出了一种对比性知识注入（ConKI）的方案，用于多模态情感分析，通过适配器结构在通用知识表示的基础上学习每种模态的特定知识表示。此外，ConKI还使用层次化对比学习过程，在每个模态内部的知识类型之间、每个样本内部的模态之间、以及样本之间进行对比学习，以促进所提出的表示的有效学习，从而提高多模态情感预测的效果。对三个流行的多模态情感分析基准测试进行的实验表明，ConKI的性能优于先进的多模态情感分析方法。",
    "tldr": "ConKI提出了一种对比性知识注入方案，用于多模态情感分析，通过在通用知识表示的基础上学习每种模态的特定知识表示，以提高多模态情感预测的效果。",
    "en_tdlr": "ConKI proposes a contrastive knowledge injection approach for multimodal sentiment analysis, improving the effectiveness of multimodal sentiment prediction by learning specific knowledge representations for each modality on top of general knowledge representations."
}