{
    "title": "Zero-shot Preference Learning for Offline RL via Optimal Transport. (arXiv:2306.03615v1 [cs.LG])",
    "abstract": "Preference-based Reinforcement Learning (PbRL) has demonstrated remarkable efficacy in aligning rewards with human intentions. However, a significant challenge lies in the need of substantial human labels, which is costly and time-consuming. Additionally, the expensive preference data obtained from prior tasks is not typically reusable for subsequent task learning, leading to extensive labeling for each new task. In this paper, we propose a novel zero-shot preference-based RL algorithm that leverages labeled preference data from source tasks to infer labels for target tasks, eliminating the requirement for human queries. Our approach utilizes Gromov-Wasserstein distance to align trajectory distributions between source and target tasks. The solved optimal transport matrix serves as a correspondence between trajectories of two tasks, making it possible to identify corresponding trajectory pairs between tasks and transfer the preference labels. However, learning directly from inferred lab",
    "link": "http://arxiv.org/abs/2306.03615",
    "context": "Title: Zero-shot Preference Learning for Offline RL via Optimal Transport. (arXiv:2306.03615v1 [cs.LG])\nAbstract: Preference-based Reinforcement Learning (PbRL) has demonstrated remarkable efficacy in aligning rewards with human intentions. However, a significant challenge lies in the need of substantial human labels, which is costly and time-consuming. Additionally, the expensive preference data obtained from prior tasks is not typically reusable for subsequent task learning, leading to extensive labeling for each new task. In this paper, we propose a novel zero-shot preference-based RL algorithm that leverages labeled preference data from source tasks to infer labels for target tasks, eliminating the requirement for human queries. Our approach utilizes Gromov-Wasserstein distance to align trajectory distributions between source and target tasks. The solved optimal transport matrix serves as a correspondence between trajectories of two tasks, making it possible to identify corresponding trajectory pairs between tasks and transfer the preference labels. However, learning directly from inferred lab",
    "path": "papers/23/06/2306.03615.json",
    "total_tokens": 1148,
    "translated_title": "通过最优传输实现离线RL的零样本偏好学习",
    "translated_abstract": "基于偏好的强化学习(PbRL)已经在将奖励与人类意图对齐方面表现出了显著的效果。然而，需要大量的人工标记仍然是一个重要的挑战，而且从先前任务中获取的昂贵的偏好数据通常不能重复使用于后续任务学习中，导致需要对每个新任务进行大量的标注。本文提出了一种新颖的零样本基于偏好的RL算法，以利用源任务中的标记偏好数据来推断目标任务的标签，从而消除了对人类查询的要求。我们的方法利用了Gromov-Wasserstein距离来对齐源任务和目标任务之间的轨迹分布。求解的最优传输矩阵作为两个任务之间轨迹对应关系的一个对应关系，使得识别任务之间对应的轨迹对和传递偏好标签成为可能。然而，直接从推断出的标签中学习可能会导致错误的累积。为了缓解这个问题，我们提出了一个交替式的学习框架，交替进行偏好推断和策略优化，同时改进推断出的标签并训练策略。我们的实验结果表明，我们提出的算法在零样本基于偏好的RL方面优于其他几种最先进的算法。",
    "tldr": "本文提出了一种利用源任务中的标签偏好数据来推断目标任务标签的零样本偏好学习算法，使用Gromov-Wasserstein距离对齐任务之间的轨迹分布。通过交替进行偏好推断和策略优化，同时改进推断出的标签并训练策略的方式，避免了需要大量人工标记的挑战。",
    "en_tdlr": "This paper proposes a zero-shot preference-based RL algorithm that leverages labeled preference data from source tasks to infer labels for target tasks, using Gromov-Wasserstein distance to align trajectory distributions between tasks. An iterative learning framework that alternates between preference inference and policy optimization is proposed to refine the inferred labels and train the policy simultaneously, avoiding the challenge of substantial human labels. Experimental results show that the proposed algorithm outperforms state-of-the-art algorithms."
}