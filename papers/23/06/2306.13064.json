{
    "title": "Auditing Predictive Models for Intersectional Biases. (arXiv:2306.13064v1 [cs.LG])",
    "abstract": "Predictive models that satisfy group fairness criteria in aggregate for members of a protected class, but do not guarantee subgroup fairness, could produce biased predictions for individuals at the intersection of two or more protected classes. To address this risk, we propose Conditional Bias Scan (CBS), a flexible auditing framework for detecting intersectional biases in classification models. CBS identifies the subgroup for which there is the most significant bias against the protected class, as compared to the equivalent subgroup in the non-protected class, and can incorporate multiple commonly used fairness definitions for both probabilistic and binarized predictions. We show that this methodology can detect previously unidentified intersectional and contextual biases in the COMPAS pre-trial risk assessment tool and has higher bias detection power compared to similar methods that audit for subgroup fairness.",
    "link": "http://arxiv.org/abs/2306.13064",
    "context": "Title: Auditing Predictive Models for Intersectional Biases. (arXiv:2306.13064v1 [cs.LG])\nAbstract: Predictive models that satisfy group fairness criteria in aggregate for members of a protected class, but do not guarantee subgroup fairness, could produce biased predictions for individuals at the intersection of two or more protected classes. To address this risk, we propose Conditional Bias Scan (CBS), a flexible auditing framework for detecting intersectional biases in classification models. CBS identifies the subgroup for which there is the most significant bias against the protected class, as compared to the equivalent subgroup in the non-protected class, and can incorporate multiple commonly used fairness definitions for both probabilistic and binarized predictions. We show that this methodology can detect previously unidentified intersectional and contextual biases in the COMPAS pre-trial risk assessment tool and has higher bias detection power compared to similar methods that audit for subgroup fairness.",
    "path": "papers/23/06/2306.13064.json",
    "total_tokens": 843,
    "translated_title": "审计预测模型中的交叉偏见",
    "translated_abstract": "满足受保护类别成员群体公平准则的预测模型，但不保证子组公平，可能会为两个或多个保护类别的个体产生偏差的预测。为了解决这个风险，我们提出了一种灵活的审计框架——条件偏差扫描（CBS），用于检测分类模型中的交叉偏见。CBS识别了对受保护类别最有偏差的子组，相比于非受保护类别中的等价子组，以及可以包括多种通常用于概率和二元预测的公平定义。我们展示了这种方法可以检测到COMPAS预审风险评估工具中先前未识别的交叉和情境偏见，并且与审计子组公平性的类似方法相比，具有更高的偏见检测能力。",
    "tldr": "本研究提出了一种灵活的审计框架——条件偏差扫描（CBS），用于检测分类模型中的交叉偏见。与审计子组公平性的类似方法相比，CBS能够检测到更多未曾发现的交叉和情境偏见。",
    "en_tdlr": "This study proposes a flexible auditing framework called Conditional Bias Scan (CBS) to detect intersectional biases in classification models. Compared to similar methods that audit for subgroup fairness, CBS has higher bias detection power and can identify previously unidentified intersectional and contextual biases."
}