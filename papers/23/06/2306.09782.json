{
    "title": "Full Parameter Fine-tuning for Large Language Models with Limited Resources. (arXiv:2306.09782v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) but demand massive GPU resources for training. Lowering the threshold for LLMs training would encourage greater participation from researchers, benefiting both academia and society. While existing approaches have focused on parameter-efficient fine-tuning, which tunes or adds a small number of parameters, few have addressed the challenge of tuning the full parameters of LLMs with limited resources. In this work, we propose a new optimizer, LOw-Memory Optimization (LOMO), which fuses the gradient computation and the parameter update in one step to reduce memory usage. By integrating LOMO with existing memory saving techniques, we reduce memory usage to 10.8% compared to the standard approach (DeepSpeed solution). Consequently, our approach enables the full parameter fine-tuning of a 65B model on a single machine with 8 RTX 3090, each with 24GB memory.",
    "link": "http://arxiv.org/abs/2306.09782",
    "context": "Title: Full Parameter Fine-tuning for Large Language Models with Limited Resources. (arXiv:2306.09782v1 [cs.CL])\nAbstract: Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) but demand massive GPU resources for training. Lowering the threshold for LLMs training would encourage greater participation from researchers, benefiting both academia and society. While existing approaches have focused on parameter-efficient fine-tuning, which tunes or adds a small number of parameters, few have addressed the challenge of tuning the full parameters of LLMs with limited resources. In this work, we propose a new optimizer, LOw-Memory Optimization (LOMO), which fuses the gradient computation and the parameter update in one step to reduce memory usage. By integrating LOMO with existing memory saving techniques, we reduce memory usage to 10.8% compared to the standard approach (DeepSpeed solution). Consequently, our approach enables the full parameter fine-tuning of a 65B model on a single machine with 8 RTX 3090, each with 24GB memory.",
    "path": "papers/23/06/2306.09782.json",
    "total_tokens": 824,
    "translated_title": "低资源情况下大型语言模型全参数微调方法研究",
    "translated_abstract": "大型语言模型（LLMs）在自然语言处理（NLP）领域具有革命性的影响，但需要大量GPU资源进行训练，从而造成研究门槛高。现有方法主要关注参数有效微调，即微调或添加少量参数，但很少有关注在有限资源情况下全参数微调的挑战。本文提出了一种新的优化器LOw-Memory Optimization（LOMO）, 通过将梯度计算和参数更新一步融合以减少内存使用。通过将LOMO与现有的内存节省技术相结合，我们将内存使用量降低到DeepSpeed方案的10.8％。因此，我们的方法使65B模型的全参数微调在只需单台机器上执行，该机器搭载8个RTX 3090，每个显存为24GB。",
    "tldr": "本文提出了一种低内存使用的优化器LOMO，可以实现在有限资源下对大型语言模型进行全参数微调，从而降低研究门槛。",
    "en_tdlr": "This paper proposes a new optimizer, LOMO, which reduces memory usage and enables full parameter fine-tuning of large language models with limited resources, lowering the threshold for research in natural language processing."
}