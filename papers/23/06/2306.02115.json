{
    "title": "Table and Image Generation for Investigating Knowledge of Entities in Pre-trained Vision and Language Models. (arXiv:2306.02115v2 [cs.CL] UPDATED)",
    "abstract": "In this paper, we propose a table and image generation task to verify how the knowledge about entities acquired from natural language is retained in Vision & Language (V&L) models. This task consists of two parts: the first is to generate a table containing knowledge about an entity and its related image, and the second is to generate an image from an entity with a caption and a table containing related knowledge of the entity. In both tasks, the model must know the entities used to perform the generation properly. We created the Wikipedia Table and Image Generation (WikiTIG) dataset from about 200,000 infoboxes in English Wikipedia articles to perform the proposed tasks. We evaluated the performance on the tasks with respect to the above research question using the V&L model OFA, which has achieved state-of-the-art results in multiple tasks. Experimental results show that OFA forgets part of its entity knowledge by pre-training as a complement to improve the performance of image relat",
    "link": "http://arxiv.org/abs/2306.02115",
    "context": "Title: Table and Image Generation for Investigating Knowledge of Entities in Pre-trained Vision and Language Models. (arXiv:2306.02115v2 [cs.CL] UPDATED)\nAbstract: In this paper, we propose a table and image generation task to verify how the knowledge about entities acquired from natural language is retained in Vision & Language (V&L) models. This task consists of two parts: the first is to generate a table containing knowledge about an entity and its related image, and the second is to generate an image from an entity with a caption and a table containing related knowledge of the entity. In both tasks, the model must know the entities used to perform the generation properly. We created the Wikipedia Table and Image Generation (WikiTIG) dataset from about 200,000 infoboxes in English Wikipedia articles to perform the proposed tasks. We evaluated the performance on the tasks with respect to the above research question using the V&L model OFA, which has achieved state-of-the-art results in multiple tasks. Experimental results show that OFA forgets part of its entity knowledge by pre-training as a complement to improve the performance of image relat",
    "path": "papers/23/06/2306.02115.json",
    "total_tokens": 905,
    "translated_title": "预训练的视觉与语言模型中的实体知识探究的表格和图像生成",
    "translated_abstract": "本文提出了一个表格和图像生成任务，以验证自然语言中获取的关于实体的知识如何被保留在视觉与语言（V&L）模型中。该任务由两个部分组成：第一个部分是生成一个包含关于实体及其相关图像的知识的表格，第二个部分是根据实体、标题和包含相关实体知识的表格生成图像。在两个任务中，模型必须正确地了解用于执行生成的实体。我们从英文维基百科文章中的约200,000个信息框创建了维基百科表格和图像生成（WikiTIG）数据集来执行提出的任务。我们使用在多个任务中取得了最先进结果的V&L模型OFA对任务的表现进行了评估。实验结果显示，OFA在预训练过程中忘记了部分实体知识，这对于提高图像相关性能是一种补充。",
    "tldr": "本文提出了一个表格和图像生成的任务，研究了预训练的视觉与语言模型中关于实体的知识如何被保留。通过实验结果发现，预训练模型OFA在生成图像的过程中忘记了部分实体知识。",
    "en_tdlr": "This paper proposes a task of table and image generation to investigate how the knowledge about entities in pre-trained vision and language models is retained. Experimental results show that the pre-trained model OFA forgets part of its entity knowledge during image generation."
}