{
    "title": "Beyond the Hype: Assessing the Performance, Trustworthiness, and Clinical Suitability of GPT3.5. (arXiv:2306.15887v1 [cs.AI])",
    "abstract": "The use of large language models (LLMs) in healthcare is gaining popularity, but their practicality and safety in clinical settings have not been thoroughly assessed. In high-stakes environments like medical settings, trust and safety are critical issues for LLMs. To address these concerns, we present an approach to evaluate the performance and trustworthiness of a GPT3.5 model for medical image protocol assignment. We compare it with a fine-tuned BERT model and a radiologist. In addition, we have a radiologist review the GPT3.5 output to evaluate its decision-making process. Our evaluation dataset consists of 4,700 physician entries across 11 imaging protocol classes spanning the entire head. Our findings suggest that the GPT3.5 performance falls behind BERT and a radiologist. However, GPT3.5 outperforms BERT in its ability to explain its decision, detect relevant word indicators, and model calibration. Furthermore, by analyzing the explanations of GPT3.5 for misclassifications, we re",
    "link": "http://arxiv.org/abs/2306.15887",
    "context": "Title: Beyond the Hype: Assessing the Performance, Trustworthiness, and Clinical Suitability of GPT3.5. (arXiv:2306.15887v1 [cs.AI])\nAbstract: The use of large language models (LLMs) in healthcare is gaining popularity, but their practicality and safety in clinical settings have not been thoroughly assessed. In high-stakes environments like medical settings, trust and safety are critical issues for LLMs. To address these concerns, we present an approach to evaluate the performance and trustworthiness of a GPT3.5 model for medical image protocol assignment. We compare it with a fine-tuned BERT model and a radiologist. In addition, we have a radiologist review the GPT3.5 output to evaluate its decision-making process. Our evaluation dataset consists of 4,700 physician entries across 11 imaging protocol classes spanning the entire head. Our findings suggest that the GPT3.5 performance falls behind BERT and a radiologist. However, GPT3.5 outperforms BERT in its ability to explain its decision, detect relevant word indicators, and model calibration. Furthermore, by analyzing the explanations of GPT3.5 for misclassifications, we re",
    "path": "papers/23/06/2306.15887.json",
    "total_tokens": 1040,
    "translated_title": "超越炒作：评估GPT3.5在性能、可信度和临床适用性方面的表现",
    "translated_abstract": "在医疗保健领域中，大型语言模型（LLM）的使用越来越受欢迎，但它们在临床环境中的实用性和安全性尚未得到全面评估。在高风险的环境中，如医疗环境下，对LLM的信任和安全性是关键问题。为了解决这些问题，我们提出了一种评估GPT3.5模型在医学图像协议分配方面性能和可信度的方法。我们将其与经过微调的BERT模型和放射科医师进行比较。此外，我们还请一位放射科医师审查GPT3.5的输出，评估其决策过程。我们的评估数据集包括横跨整个头部的11个成像协议类别中的4,700个医师输入。我们的研究结果表明，GPT3.5的性能在BERT和放射科医师之后。然而，GPT3.5在解释其决策能力、检测相关词标识和模型校准方面优于BERT。此外，通过分析GPT3.5错误分类的解释，我们重新评估了其在性能方面的表现。",
    "tldr": "该研究评估了GPT3.5模型在医学图像协议分配方面的性能和可信度，发现其在性能上不如BERT和放射科医师，但在解释决策能力、检测相关词标识和模型校准方面优于BERT。"
}