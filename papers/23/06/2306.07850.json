{
    "title": "Exact Mean Square Linear Stability Analysis for SGD. (arXiv:2306.07850v1 [cs.LG])",
    "abstract": "The dynamical stability of optimization methods at the vicinity of minima of the loss has recently attracted significant attention. For gradient descent (GD), stable convergence is possible only to minima that are sufficiently flat w.r.t. the step size, and those have been linked with favorable properties of the trained model. However, while the stability threshold of GD is well-known, to date, no explicit expression has been derived for the exact threshold of stochastic GD (SGD). In this paper, we derive such a closed-form expression. Specifically, we provide an explicit condition on the step size $\\eta$ that is both necessary and sufficient for the stability of SGD in the mean square sense. Our analysis sheds light on the precise role of the batch size $B$. Particularly, we show that the stability threshold is a monotonically non-decreasing function of the batch size, which means that reducing the batch size can only hurt stability. Furthermore, we show that SGD's stability threshold",
    "link": "http://arxiv.org/abs/2306.07850",
    "context": "Title: Exact Mean Square Linear Stability Analysis for SGD. (arXiv:2306.07850v1 [cs.LG])\nAbstract: The dynamical stability of optimization methods at the vicinity of minima of the loss has recently attracted significant attention. For gradient descent (GD), stable convergence is possible only to minima that are sufficiently flat w.r.t. the step size, and those have been linked with favorable properties of the trained model. However, while the stability threshold of GD is well-known, to date, no explicit expression has been derived for the exact threshold of stochastic GD (SGD). In this paper, we derive such a closed-form expression. Specifically, we provide an explicit condition on the step size $\\eta$ that is both necessary and sufficient for the stability of SGD in the mean square sense. Our analysis sheds light on the precise role of the batch size $B$. Particularly, we show that the stability threshold is a monotonically non-decreasing function of the batch size, which means that reducing the batch size can only hurt stability. Furthermore, we show that SGD's stability threshold",
    "path": "papers/23/06/2306.07850.json",
    "total_tokens": 909,
    "translated_title": "SGD的精确平均二次线性稳定性分析",
    "translated_abstract": "近来，优化方法在损失函数极小值点附近的动态稳定性引起了极大关注。对于梯度下降法（GD），稳定的收敛仅可能发生在足够平坦的极小值处，并且已经与训练模型的良好性能联系在一起。但是，尽管GD的稳定性阈值已经众所周知，但迄今为止，尚未推导出随机梯度下降（SGD）的精确阈值的显式表达式。本文提供了这样一种封闭形式的表达式。具体而言，我们提供了一个关于步长$\\eta$的显式条件，既是SGD在均方意义下稳定的必要条件，也是充分条件。我们的分析揭示了批量大小$B$的精确作用，特别的，我们展示出稳定性阈值是批量大小的单调非降函数，这意味着减小批量大小只会降低稳定性。此外，我们还展示了SGD的稳定性阈值。",
    "tldr": "本文提供了SGD稳定性的精确阈值表达式，发现其与批量大小之间呈单调非降关系，进一步展示了减小批量大小可能会影响SGD的稳定性。",
    "en_tdlr": "This paper provides an exact threshold expression for the stability of SGD, which is a monotonically non-decreasing function of the batch size. The result shows that reducing the batch size may affect the stability of SGD."
}