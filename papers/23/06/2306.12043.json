{
    "title": "Sample Attackability in Natural Language Adversarial Attacks. (arXiv:2306.12043v1 [cs.CL])",
    "abstract": "Adversarial attack research in natural language processing (NLP) has made significant progress in designing powerful attack methods and defence approaches. However, few efforts have sought to identify which source samples are the most attackable or robust, i.e. can we determine for an unseen target model, which samples are the most vulnerable to an adversarial attack. This work formally extends the definition of sample attackability/robustness for NLP attacks. Experiments on two popular NLP datasets, four state of the art models and four different NLP adversarial attack methods, demonstrate that sample uncertainty is insufficient for describing characteristics of attackable/robust samples and hence a deep learning based detector can perform much better at identifying the most attackable and robust samples for an unseen target model. Nevertheless, further analysis finds that there is little agreement in which samples are considered the most attackable/robust across different NLP attack ",
    "link": "http://arxiv.org/abs/2306.12043",
    "context": "Title: Sample Attackability in Natural Language Adversarial Attacks. (arXiv:2306.12043v1 [cs.CL])\nAbstract: Adversarial attack research in natural language processing (NLP) has made significant progress in designing powerful attack methods and defence approaches. However, few efforts have sought to identify which source samples are the most attackable or robust, i.e. can we determine for an unseen target model, which samples are the most vulnerable to an adversarial attack. This work formally extends the definition of sample attackability/robustness for NLP attacks. Experiments on two popular NLP datasets, four state of the art models and four different NLP adversarial attack methods, demonstrate that sample uncertainty is insufficient for describing characteristics of attackable/robust samples and hence a deep learning based detector can perform much better at identifying the most attackable and robust samples for an unseen target model. Nevertheless, further analysis finds that there is little agreement in which samples are considered the most attackable/robust across different NLP attack ",
    "path": "papers/23/06/2306.12043.json",
    "total_tokens": 1035,
    "translated_title": "自然语言对抗攻击中的样本攻击能力研究",
    "translated_abstract": "自然语言处理领域的对抗攻击研究已经在设计强大的攻击方法和防御方法方面取得了重大进展。然而，很少有研究努力去确定哪些源样本是最易受攻击或最具鲁棒性，即在一个看不见的目标模型上，我们是否可以确定哪些样本最容易受到对抗攻击。本文正式扩展了自然语言处理攻击中样本攻击能力/鲁棒性的定义。在两个流行的自然语言处理数据集、四种最先进的模型和四种不同的自然语言对抗攻击方法上的实验表明，样本不确定性不能充分描述攻击性/鲁棒性样本的特征，因此基于深度学习的检测器可以更好地识别出一个看不见的目标模型中最易受攻击和最具鲁棒性的样本。然而，进一步的分析发现，在不同的自然语言处理攻击方法中，对于哪些样本被认为是最易受攻击/最具鲁棒性，不存在明显的一致性。",
    "tldr": "本文研究了自然语言处理领域中，哪些样本最易受到对抗攻击或最具鲁棒性的问题，并扩展了该领域中的“样本攻击能力/鲁棒性”的定义。实验发现，基于深度学习的检测器可以更好地识别出一个看不见的目标模型中最易受攻击和最具鲁棒性的样本。",
    "en_tdlr": "This work extends the definition of \"sample attackability/robustness\" in adversarial attacks in natural language processing (NLP) and proposes a deep learning-based detector to identify the most attackable and robust samples for an unseen target model. The experiments show that there is little consensus on which samples are considered the most attackable/robust across different NLP attack methods."
}