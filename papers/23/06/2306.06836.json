{
    "title": "Tackling Heavy-Tailed Rewards in Reinforcement Learning with Function Approximation: Minimax Optimal and Instance-Dependent Regret Bounds. (arXiv:2306.06836v1 [cs.LG])",
    "abstract": "While numerous works have focused on devising efficient algorithms for reinforcement learning (RL) with uniformly bounded rewards, it remains an open question whether sample or time-efficient algorithms for RL with large state-action space exist when the rewards are \\emph{heavy-tailed}, i.e., with only finite $(1+\\epsilon)$-th moments for some $\\epsilon\\in(0,1]$. In this work, we address the challenge of such rewards in RL with linear function approximation. We first design an algorithm, \\textsc{Heavy-OFUL}, for heavy-tailed linear bandits, achieving an \\emph{instance-dependent} $T$-round regret of $\\tilde{O}\\big(d T^{\\frac{1-\\epsilon}{2(1+\\epsilon)}} \\sqrt{\\sum_{t=1}^T \\nu_t^2} + d T^{\\frac{1-\\epsilon}{2(1+\\epsilon)}}\\big)$, the \\emph{first} of this kind. Here, $d$ is the feature dimension, and $\\nu_t^{1+\\epsilon}$ is the $(1+\\epsilon)$-th central moment of the reward at the $t$-th round. We further show the above bound is minimax optimal when applied to the worst-case instances in st",
    "link": "http://arxiv.org/abs/2306.06836",
    "context": "Title: Tackling Heavy-Tailed Rewards in Reinforcement Learning with Function Approximation: Minimax Optimal and Instance-Dependent Regret Bounds. (arXiv:2306.06836v1 [cs.LG])\nAbstract: While numerous works have focused on devising efficient algorithms for reinforcement learning (RL) with uniformly bounded rewards, it remains an open question whether sample or time-efficient algorithms for RL with large state-action space exist when the rewards are \\emph{heavy-tailed}, i.e., with only finite $(1+\\epsilon)$-th moments for some $\\epsilon\\in(0,1]$. In this work, we address the challenge of such rewards in RL with linear function approximation. We first design an algorithm, \\textsc{Heavy-OFUL}, for heavy-tailed linear bandits, achieving an \\emph{instance-dependent} $T$-round regret of $\\tilde{O}\\big(d T^{\\frac{1-\\epsilon}{2(1+\\epsilon)}} \\sqrt{\\sum_{t=1}^T \\nu_t^2} + d T^{\\frac{1-\\epsilon}{2(1+\\epsilon)}}\\big)$, the \\emph{first} of this kind. Here, $d$ is the feature dimension, and $\\nu_t^{1+\\epsilon}$ is the $(1+\\epsilon)$-th central moment of the reward at the $t$-th round. We further show the above bound is minimax optimal when applied to the worst-case instances in st",
    "path": "papers/23/06/2306.06836.json",
    "total_tokens": 1065,
    "translated_title": "用函数逼近解决强化学习中重尾奖励问题的极小最大化算法和实例相关遗憾度量",
    "translated_abstract": "虽然有许多工作都专注于为有界奖励的强化学习设计有效算法，但当奖励呈现“重尾”分布时——即存在某个 $\\epsilon\\in(0,1]$ 使得仅有有限的$(1+\\epsilon)$-阶矩——是否存在对大状态-动作空间进行采样或时效性算法仍然是一个未解决的问题。 在本文中，我们解决了具有线性函数逼近的 RL 中的这种奖励机制的挑战。我们首先为重尾线性赌臂设计了一种算法——\\textsc{Heavy-OFUL}，其实现了一种实例相关的 $T$-round 遗憾度量，为 $\\tilde{O}\\big(d T^{\\frac{1-\\epsilon}{2(1+\\epsilon)}} \\sqrt{\\sum_{t=1}^T \\nu_t^2} + d T^{\\frac{1-\\epsilon}{2(1+\\epsilon)}}\\big)$，这是这种类型的\\emph{第一篇}文章。$\\nu_t^{1+\\epsilon}$是第 $t$ 轮奖励的 $(1+\\epsilon)$-阶中心矩。我们进一步证明了在应用于 st 的最坏情况时，上述界是极小值的最优解。",
    "tldr": "本文解决了强化学习中当奖励呈“重尾”分布时的问题，提出了第一种处理这种情况的实例相关算法，并得到了极小最大化的遗憾界。",
    "en_tdlr": "This paper addresses the challenge in reinforcement learning when rewards exhibit heavy-tailed distribution, proposing a novel instance-dependent algorithm for heavy-tailed linear bandits and achieving the first instance-dependent regret bounds with a minimax optimal guarantee applied to worst-case instances."
}