{
    "title": "Limits, approximation and size transferability for GNNs on sparse graphs via graphops. (arXiv:2306.04495v1 [cs.LG])",
    "abstract": "Can graph neural networks generalize to graphs that are different from the graphs they were trained on, e.g., in size? In this work, we study this question from a theoretical perspective. While recent work established such transferability and approximation results via graph limits, e.g., via graphons, these only apply non-trivially to dense graphs. To include frequently encountered sparse graphs such as bounded-degree or power law graphs, we take a perspective of taking limits of operators derived from graphs, such as the aggregation operation that makes up GNNs. This leads to the recently introduced limit notion of graphops (Backhausz and Szegedy, 2022). We demonstrate how the operator perspective allows us to develop quantitative bounds on the distance between a finite GNN and its limit on an infinite graph, as well as the distance between the GNN on graphs of different sizes that share structural properties, under a regularity assumption verified for various graph sequences. Our res",
    "link": "http://arxiv.org/abs/2306.04495",
    "context": "Title: Limits, approximation and size transferability for GNNs on sparse graphs via graphops. (arXiv:2306.04495v1 [cs.LG])\nAbstract: Can graph neural networks generalize to graphs that are different from the graphs they were trained on, e.g., in size? In this work, we study this question from a theoretical perspective. While recent work established such transferability and approximation results via graph limits, e.g., via graphons, these only apply non-trivially to dense graphs. To include frequently encountered sparse graphs such as bounded-degree or power law graphs, we take a perspective of taking limits of operators derived from graphs, such as the aggregation operation that makes up GNNs. This leads to the recently introduced limit notion of graphops (Backhausz and Szegedy, 2022). We demonstrate how the operator perspective allows us to develop quantitative bounds on the distance between a finite GNN and its limit on an infinite graph, as well as the distance between the GNN on graphs of different sizes that share structural properties, under a regularity assumption verified for various graph sequences. Our res",
    "path": "papers/23/06/2306.04495.json",
    "total_tokens": 827,
    "translated_title": "通过GraphOps探究GNN在稀疏图上的限制、逼近和大小可迁移性",
    "translated_abstract": "这项工作从理论角度研究图神经网络能否推广到与其训练图不同的图形中。为了包括经常遇到的稀疏图，如有界度或幂律图，我们采取从图形导出运算符的视角，如组成GNN的聚合操作。这导致了最近介绍的GraphOps极限概念。我们展示了运算符视角如何允许我们开发关于有限GNN与其在无限图上的极限之间距离的定量界限，以及具有共享结构属性的不同大小图形上的GNN之间的距离，在验证各种图序列的规则性假设下。",
    "tldr": "本文从理论角度研究了图神经网络在稀疏图中推广的问题。通过GraphOps极限概念，我们开发了关于不同大小图形上的GNN之间距离的定量界限和结构属性的共享。",
    "en_tdlr": "This paper explores the generalization of graph neural networks on sparse graphs from a theoretical perspective. By using the limit notion of GraphOps, the authors establish quantitative bounds on the distance between GNNs of graphs with different sizes that share structural properties."
}