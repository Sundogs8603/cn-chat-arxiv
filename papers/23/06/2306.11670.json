{
    "title": "GIO: Gradient Information Optimization for Training Dataset Selection. (arXiv:2306.11670v2 [cs.LG] UPDATED)",
    "abstract": "It is often advantageous to train models on a subset of the available train examples, because the examples are of variable quality or because one would like to train with fewer examples, without sacrificing performance. We present Gradient Information Optimization (GIO), a scalable, task-agnostic approach to this data selection problem that requires only a small set of (unlabeled) examples representing a target distribution. GIO begins from a natural, information-theoretic objective that is intractable in practice. Our contribution is in showing that it can be made highly scalable through a simple relaxation of the objective and a highly efficient implementation. In experiments with machine translation, spelling correction, and image recognition, we show that GIO delivers outstanding results with very small train sets. These findings are robust to different representation models and hyperparameters for GIO itself. GIO is task- and domain-agnostic and can be applied out-of-the-box to ne",
    "link": "http://arxiv.org/abs/2306.11670",
    "context": "Title: GIO: Gradient Information Optimization for Training Dataset Selection. (arXiv:2306.11670v2 [cs.LG] UPDATED)\nAbstract: It is often advantageous to train models on a subset of the available train examples, because the examples are of variable quality or because one would like to train with fewer examples, without sacrificing performance. We present Gradient Information Optimization (GIO), a scalable, task-agnostic approach to this data selection problem that requires only a small set of (unlabeled) examples representing a target distribution. GIO begins from a natural, information-theoretic objective that is intractable in practice. Our contribution is in showing that it can be made highly scalable through a simple relaxation of the objective and a highly efficient implementation. In experiments with machine translation, spelling correction, and image recognition, we show that GIO delivers outstanding results with very small train sets. These findings are robust to different representation models and hyperparameters for GIO itself. GIO is task- and domain-agnostic and can be applied out-of-the-box to ne",
    "path": "papers/23/06/2306.11670.json",
    "total_tokens": 859,
    "translated_title": "GIO：用于训练数据集选择的梯度信息优化",
    "translated_abstract": "在训练模型时，通常有利于在可用训练样本的子集上进行训练，因为这些样本具有不同的质量，或者希望在不影响性能的情况下使用更少的样本进行训练。我们提出了梯度信息优化（GIO），这是一种可伸缩且任务不可知的方法，用于解决这个数据选择问题，它只需要一小组（未标记的）样本来代表目标分布。GIO从一个实践中难以处理的自然的信息理论目标开始。我们的贡献在于展示出通过对目标进行简单的放松和高效的实现，它可以被高度扩展。在机器翻译、拼写纠正和图像识别等实验中，我们证明了GIO在非常小的训练集上产生了出色的结果。这些发现对于GIO本身的不同表示模型和超参数是稳健的。GIO是任务和领域无关的，可以直接应用于新领域。",
    "tldr": "GIO是一种可伸缩且任务不可知的方法，通过对目标的简单放松和高效实现，可以在非常小的训练集上产生出色的结果。",
    "en_tdlr": "GIO is a scalable and task-agnostic approach that achieves outstanding results on very small train sets by relaxing the objective and implementing it efficiently."
}