{
    "title": "Online Bootstrap Inference with Nonconvex Stochastic Gradient Descent Estimator. (arXiv:2306.02205v1 [stat.ML])",
    "abstract": "In this paper, we investigate the theoretical properties of stochastic gradient descent (SGD) for statistical inference in the context of nonconvex optimization problems, which have been relatively unexplored compared to convex settings. Our study is the first to establish provable inferential procedures using the SGD estimator for general nonconvex objective functions, which may contain multiple local minima.  We propose two novel online inferential procedures that combine SGD and the multiplier bootstrap technique. The first procedure employs a consistent covariance matrix estimator, and we establish its error convergence rate. The second procedure approximates the limit distribution using bootstrap SGD estimators, yielding asymptotically valid bootstrap confidence intervals. We validate the effectiveness of both approaches through numerical experiments.  Furthermore, our analysis yields an intermediate result: the in-expectation error convergence rate for the original SGD estimator ",
    "link": "http://arxiv.org/abs/2306.02205",
    "context": "Title: Online Bootstrap Inference with Nonconvex Stochastic Gradient Descent Estimator. (arXiv:2306.02205v1 [stat.ML])\nAbstract: In this paper, we investigate the theoretical properties of stochastic gradient descent (SGD) for statistical inference in the context of nonconvex optimization problems, which have been relatively unexplored compared to convex settings. Our study is the first to establish provable inferential procedures using the SGD estimator for general nonconvex objective functions, which may contain multiple local minima.  We propose two novel online inferential procedures that combine SGD and the multiplier bootstrap technique. The first procedure employs a consistent covariance matrix estimator, and we establish its error convergence rate. The second procedure approximates the limit distribution using bootstrap SGD estimators, yielding asymptotically valid bootstrap confidence intervals. We validate the effectiveness of both approaches through numerical experiments.  Furthermore, our analysis yields an intermediate result: the in-expectation error convergence rate for the original SGD estimator ",
    "path": "papers/23/06/2306.02205.json",
    "total_tokens": 859,
    "translated_title": "非凸随机梯度下降估计的在线自举推断",
    "translated_abstract": "在本文中，我们研究了随机梯度下降法（SGD）在非凸优化问题中的统计推断理论特性。相对于凸优化问题，这部分内容还比较未被探索。我们的研究是第一篇针对包含多个局部最小值可能的通用非凸目标函数，使用SGD估计器建立可证明推断程序的论文。我们提出了两种新型在线推断程序，将SGD和乘数自举技术相结合。第一种程序使用一致协方差矩阵估计器，并且我们建立了其误差收敛速率。第二种程序使用自举SGD估计器来逼近极限分布，产生渐进有效的自举置信区间。我们通过数值实验验证了两种方法的有效性。此外，我们的分析还产生了一个中间结果：原始SGD估计器的期望误差收敛速率。",
    "tldr": "本文提出了两种新型在线推断程序，将随机梯度下降法和乘数自举技术相结合，用于非凸目标函数的推断。同时，我们建立了这些程序的错误收敛速率，并验证了效果。"
}