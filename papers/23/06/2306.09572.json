{
    "title": "How do different tokenizers perform on downstream tasks in scriptio continua languages?: A case study in Japanese. (arXiv:2306.09572v1 [cs.CL])",
    "abstract": "This paper investigates the effect of tokenizers on the downstream performance of pretrained language models (PLMs) in scriptio continua languages where no explicit spaces exist between words, using Japanese as a case study. The tokenizer for such languages often consists of a morphological analyzer and a subword tokenizer, requiring us to conduct a comprehensive study of all possible pairs. However, previous studies lack this comprehensiveness. We therefore train extensive sets of tokenizers, build a PLM using each, and measure the downstream performance on a wide range of tasks. Our results demonstrate that each downstream task has a different optimal morphological analyzer, and that it is better to use Byte-Pair-Encoding or Unigram rather than WordPiece as a subword tokenizer, regardless of the type of task.",
    "link": "http://arxiv.org/abs/2306.09572",
    "context": "Title: How do different tokenizers perform on downstream tasks in scriptio continua languages?: A case study in Japanese. (arXiv:2306.09572v1 [cs.CL])\nAbstract: This paper investigates the effect of tokenizers on the downstream performance of pretrained language models (PLMs) in scriptio continua languages where no explicit spaces exist between words, using Japanese as a case study. The tokenizer for such languages often consists of a morphological analyzer and a subword tokenizer, requiring us to conduct a comprehensive study of all possible pairs. However, previous studies lack this comprehensiveness. We therefore train extensive sets of tokenizers, build a PLM using each, and measure the downstream performance on a wide range of tasks. Our results demonstrate that each downstream task has a different optimal morphological analyzer, and that it is better to use Byte-Pair-Encoding or Unigram rather than WordPiece as a subword tokenizer, regardless of the type of task.",
    "path": "papers/23/06/2306.09572.json",
    "total_tokens": 875,
    "translated_title": "不同的分词器在连续书写文字中的下游任务中的表现如何？以日语为例研究。",
    "translated_abstract": "本文研究了在连续书写文字语言中，分词器对预训练语言模型（PLMs）在下游任务中的影响，以日语为案例研究。这种语言的分词器通常由形态学分析器和子词分词器组成，需要我们对所有可能的组合进行综合研究。然而，以前的研究缺乏这种全面性。因此，我们训练了大量的分词器集，并使用每个集合构建了一个PLM，并在广泛的任务范围内测量了下游性能。我们的结果表明，每个下游任务都有一个不同的最佳形态学分析器，并且无论任务类型如何，最好使用字节对编码或Unigram而不是WordPiece作为子词分词器。",
    "tldr": "本文研究了在日语这种连续书写文字语言中，不同的分词器对预训练语言模型在下游任务中的影响，发现每个下游任务都有一个最佳的形态学分析器，并且无论任务类型如何，最好使用字节对编码或Unigram作为子词分词器。",
    "en_tdlr": "This paper investigates the impact of different tokenizers on pretrained language models (PLMs) in scriptio continua languages using Japanese as a case study. The study finds that each downstream task has a different optimal morphological analyzer and that Byte-Pair-Encoding or Unigram should be used as a subword tokenizer instead of WordPiece, regardless of the type of task."
}