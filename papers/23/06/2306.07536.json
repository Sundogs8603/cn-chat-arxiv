{
    "title": "TART: A plug-and-play Transformer module for task-agnostic reasoning. (arXiv:2306.07536v1 [cs.LG])",
    "abstract": "Large language models (LLMs) exhibit in-context learning abilities which enable the same model to perform several tasks without any task-specific training. In contrast, traditional adaptation approaches, such as fine-tuning, modify the underlying models for each specific task. In-context learning, however, consistently underperforms task-specific tuning approaches even when presented with the same examples. While most existing approaches (e.g., prompt engineering) focus on the LLM's learned representations to patch this performance gap, our analysis actually reveal that LLM representations contain sufficient information to make good predictions. As such, we focus on the LLM's reasoning abilities and demonstrate that this performance gap exists due to their inability to perform simple probabilistic reasoning tasks. This raises an intriguing question: Are LLMs actually capable of learning how to reason in a task-agnostic manner? We answer this in the affirmative and propose TART which ge",
    "link": "http://arxiv.org/abs/2306.07536",
    "context": "Title: TART: A plug-and-play Transformer module for task-agnostic reasoning. (arXiv:2306.07536v1 [cs.LG])\nAbstract: Large language models (LLMs) exhibit in-context learning abilities which enable the same model to perform several tasks without any task-specific training. In contrast, traditional adaptation approaches, such as fine-tuning, modify the underlying models for each specific task. In-context learning, however, consistently underperforms task-specific tuning approaches even when presented with the same examples. While most existing approaches (e.g., prompt engineering) focus on the LLM's learned representations to patch this performance gap, our analysis actually reveal that LLM representations contain sufficient information to make good predictions. As such, we focus on the LLM's reasoning abilities and demonstrate that this performance gap exists due to their inability to perform simple probabilistic reasoning tasks. This raises an intriguing question: Are LLMs actually capable of learning how to reason in a task-agnostic manner? We answer this in the affirmative and propose TART which ge",
    "path": "papers/23/06/2306.07536.json",
    "total_tokens": 896,
    "translated_title": "TART: 一种面向任务无关推理的即插即用Transformer模块",
    "translated_abstract": "大型语言模型(LLMs)表现出上下文学习能力,能让同一模型执行多个任务,而无需进行任何特定任务的训练。相比之下,传统的自适应方法(如微调)会针对每个特定任务修改基础模型。然而,即使在使用相同示例的情况下,上下文学习一直表现不佳,而大多数现有方法(如提示工程)侧重于LLM的学习表示，以弥补性能差距,而我们的分析实际上揭示了LLM表示包含足够的信息来做出好的预测。因此,我们关注LLM的推理能力,并展示该性能差距存在是由于它们无法执行简单的概率推理任务。这引发了一个有趣的问题: LLM实际上能否以任务无关的方式学习如何推理？我们肯定地回答了这个问题,并提出了TART，它以即插即用的方式在不进行任务特定训练或微调的情况下横跨不同推理目标进行泛化。",
    "tldr": "TART提出了一种即插即用的Transformer模块，它能够在没有任务特定训练或微调的情况下，在不同推理目标之间进行泛化。",
    "en_tdlr": "TART proposes a plug-and-play Transformer module that can generalize across different reasoning targets without task-specific training or fine-tuning."
}