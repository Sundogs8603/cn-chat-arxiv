{
    "title": "Curricular Subgoals for Inverse Reinforcement Learning. (arXiv:2306.08232v1 [cs.LG])",
    "abstract": "Inverse Reinforcement Learning (IRL) aims to reconstruct the reward function from expert demonstrations to facilitate policy learning, and has demonstrated its remarkable success in imitation learning. To promote expert-like behavior, existing IRL methods mainly focus on learning global reward functions to minimize the trajectory difference between the imitator and the expert. However, these global designs are still limited by the redundant noise and error propagation problems, leading to the unsuitable reward assignment and thus downgrading the agent capability in complex multi-stage tasks. In this paper, we propose a novel Curricular Subgoal-based Inverse Reinforcement Learning (CSIRL) framework, that explicitly disentangles one task with several local subgoals to guide agent imitation. Specifically, CSIRL firstly introduces decision uncertainty of the trained agent over expert trajectories to dynamically select subgoals, which directly determines the exploration boundary of differen",
    "link": "http://arxiv.org/abs/2306.08232",
    "context": "Title: Curricular Subgoals for Inverse Reinforcement Learning. (arXiv:2306.08232v1 [cs.LG])\nAbstract: Inverse Reinforcement Learning (IRL) aims to reconstruct the reward function from expert demonstrations to facilitate policy learning, and has demonstrated its remarkable success in imitation learning. To promote expert-like behavior, existing IRL methods mainly focus on learning global reward functions to minimize the trajectory difference between the imitator and the expert. However, these global designs are still limited by the redundant noise and error propagation problems, leading to the unsuitable reward assignment and thus downgrading the agent capability in complex multi-stage tasks. In this paper, we propose a novel Curricular Subgoal-based Inverse Reinforcement Learning (CSIRL) framework, that explicitly disentangles one task with several local subgoals to guide agent imitation. Specifically, CSIRL firstly introduces decision uncertainty of the trained agent over expert trajectories to dynamically select subgoals, which directly determines the exploration boundary of differen",
    "path": "papers/23/06/2306.08232.json",
    "total_tokens": 778,
    "translated_title": "逆强化学习中的课程子目标",
    "translated_abstract": "逆强化学习（IRL）旨在从专家演示中重构奖励函数以促进策略学习，并已在模仿学习中展示出卓越的成功。现有的IRL方法主要集中于学习全局奖励函数以最小化模仿者和专家之间的轨迹差异，但这些全局设计仍然存在冗余噪声和误差传播问题，导致不适当的奖励分配，从而降低代理在复杂的多阶段任务中的能力。",
    "tldr": "本论文提出一种新的逆强化学习框架，即基于课程子目标的逆强化学习 (CSIRL)。与现有方法不同的是，该框架将一个任务分解为多个局部子目标，以引导智能体进行模仿，这有助于解决全局设计中存在的问题。",
    "en_tdlr": "This paper proposes a novel framework for inverse reinforcement learning (IRL), Curricular Subgoal-based IRL (CSIRL), which disentangles tasks into multiple local subgoals to guide agent imitation. This helps to address current issues with global designs in IRL methods."
}