{
    "title": "Efficient Vision Transformer for Human Pose Estimation via Patch Selection. (arXiv:2306.04225v1 [cs.CV])",
    "abstract": "While Convolutional Neural Networks (CNNs) have been widely successful in 2D human pose estimation, Vision Transformers (ViTs) have emerged as a promising alternative to CNNs, boosting state-of-the-art performance. However, the quadratic computational complexity of ViTs has limited their applicability for processing high-resolution images and long videos. To address this challenge, we propose a simple method for reducing ViT's computational complexity based on selecting and processing a small number of most informative patches while disregarding others. We leverage a lightweight pose estimation network to guide the patch selection process, ensuring that the selected patches contain the most important information. Our experimental results on three widely used 2D pose estimation benchmarks, namely COCO, MPII and OCHuman, demonstrate the effectiveness of our proposed methods in significantly improving speed and reducing computational complexity with a slight drop in performance.",
    "link": "http://arxiv.org/abs/2306.04225",
    "context": "Title: Efficient Vision Transformer for Human Pose Estimation via Patch Selection. (arXiv:2306.04225v1 [cs.CV])\nAbstract: While Convolutional Neural Networks (CNNs) have been widely successful in 2D human pose estimation, Vision Transformers (ViTs) have emerged as a promising alternative to CNNs, boosting state-of-the-art performance. However, the quadratic computational complexity of ViTs has limited their applicability for processing high-resolution images and long videos. To address this challenge, we propose a simple method for reducing ViT's computational complexity based on selecting and processing a small number of most informative patches while disregarding others. We leverage a lightweight pose estimation network to guide the patch selection process, ensuring that the selected patches contain the most important information. Our experimental results on three widely used 2D pose estimation benchmarks, namely COCO, MPII and OCHuman, demonstrate the effectiveness of our proposed methods in significantly improving speed and reducing computational complexity with a slight drop in performance.",
    "path": "papers/23/06/2306.04225.json",
    "total_tokens": 882,
    "translated_title": "基于补丁选择的高效视觉Transformer在人体姿态估计中的应用",
    "translated_abstract": "虽然卷积神经网络在2D人体姿态估计方面已经取得了广泛成功，但是视觉Transformer作为卷积神经网络的有力替代者，通过提高最先进的性能而崭露头角。 然而，视觉Transformer的二次计算复杂度限制了其在处理高分辨率图像和长视频方面的适用性。为了解决这一挑战，我们提出了一种简单的方法来减少视觉Transformer的计算复杂度，基于选择和处理少量最具信息的补丁，而忽略其他地方的补丁。我们利用轻量级姿态估计网络来指导补丁选择过程，确保所选补丁包含最重要的信息。我们在三个广泛使用的2D姿态估计基准（即COCO、MPII和OCHuman）上的实验结果表明，我们提出的方法在显著提高速度和降低计算复杂度方面具有很好的效果，虽然性能略有下降。",
    "tldr": "该论文提出了一种基于补丁选择的高效视觉Transformer方法，大幅度提高了处理速度和降低计算复杂度，用于2D人体姿态估计方面。",
    "en_tdlr": "The paper proposes an efficient method based on patch selection for Vision Transformer, which significantly improves processing speed and reduces computational complexity for 2D human pose estimation with a slight drop in performance."
}