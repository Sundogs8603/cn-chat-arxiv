{
    "title": "Align, Distill, and Augment Everything All at Once for Imbalanced Semi-Supervised Learning. (arXiv:2306.04621v1 [cs.LG])",
    "abstract": "Addressing the class imbalance in long-tailed semi-supervised learning (SSL) poses a few significant challenges stemming from differences between the marginal distributions of unlabeled data and the labeled data, as the former is often unknown and potentially distinct from the latter. The first challenge is to avoid biasing the pseudo-labels towards an incorrect distribution, such as that of the labeled data or a balanced distribution, during training. However, we still wish to ensure a balanced unlabeled distribution during inference, which is the second challenge. To address both of these challenges, we propose a three-faceted solution: a flexible distribution alignment that progressively aligns the classifier from a dynamically estimated unlabeled prior towards a balanced distribution, a soft consistency regularization that exploits underconfident pseudo-labels discarded by threshold-based methods, and a schema for expanding the unlabeled set with input data from the labeled partiti",
    "link": "http://arxiv.org/abs/2306.04621",
    "context": "Title: Align, Distill, and Augment Everything All at Once for Imbalanced Semi-Supervised Learning. (arXiv:2306.04621v1 [cs.LG])\nAbstract: Addressing the class imbalance in long-tailed semi-supervised learning (SSL) poses a few significant challenges stemming from differences between the marginal distributions of unlabeled data and the labeled data, as the former is often unknown and potentially distinct from the latter. The first challenge is to avoid biasing the pseudo-labels towards an incorrect distribution, such as that of the labeled data or a balanced distribution, during training. However, we still wish to ensure a balanced unlabeled distribution during inference, which is the second challenge. To address both of these challenges, we propose a three-faceted solution: a flexible distribution alignment that progressively aligns the classifier from a dynamically estimated unlabeled prior towards a balanced distribution, a soft consistency regularization that exploits underconfident pseudo-labels discarded by threshold-based methods, and a schema for expanding the unlabeled set with input data from the labeled partiti",
    "path": "papers/23/06/2306.04621.json",
    "total_tokens": 891,
    "translated_title": "一次性对齐、提炼和扩充所有不平衡的半监督学习",
    "translated_abstract": "在解决长尾半监督学习中的类别不平衡问题时，需面对未标记数据和已标记数据之间边缘分布的区别，前者通常是未知的且可能与后者不同，这导致了一些重大挑战。第一个挑战是在训练过程中避免使伪标签对目标分布的偏倚，如已标记数据或平衡分布。第二个挑战是确保推理时的平衡未标记分布。为应对这些挑战，我们提出了一个多方面的解决方案：通过灵活的分布对齐，逐渐将分类器从动态估计的未标记先验分布对齐到平衡分布；利用被基于阈值的方法舍弃的低置信度伪标签的软一致性正则化；以及一种将标记部分的输入数据扩展到未标记集的方案。",
    "tldr": "本文针对长尾半监督学习中类别不平衡的问题，提出了三个解决方案：一种灵活的分布对齐方法，一种软一致性正则化方法和一种扩充未标记集的方案。",
    "en_tdlr": "This paper proposes a three-faceted solution to address the challenges of class imbalance in long-tailed semi-supervised learning, including a flexible distribution alignment, a soft consistency regularization, and a schema for expanding the unlabeled set with input data from the labeled partition."
}