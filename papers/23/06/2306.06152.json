{
    "title": "EfficientBioAI: Making Bioimaging AI Models Efficient in Energy, Latency and Representation. (arXiv:2306.06152v1 [cs.LG])",
    "abstract": "Artificial intelligence (AI) has been widely used in bioimage image analysis nowadays, but the efficiency of AI models, like the energy consumption and latency is not ignorable due to the growing model size and complexity, as well as the fast-growing analysis needs in modern biomedical studies. Like we can compress large images for efficient storage and sharing, we can also compress the AI models for efficient applications and deployment. In this work, we present EfficientBioAI, a plug-and-play toolbox that can compress given bioimaging AI models for them to run with significantly reduced energy cost and inference time on both CPU and GPU, without compromise on accuracy. In some cases, the prediction accuracy could even increase after compression, since the compression procedure could remove redundant information in the model representation and therefore reduce over-fitting. From four different bioimage analysis applications, we observed around 2-5 times speed-up during inference and 3",
    "link": "http://arxiv.org/abs/2306.06152",
    "context": "Title: EfficientBioAI: Making Bioimaging AI Models Efficient in Energy, Latency and Representation. (arXiv:2306.06152v1 [cs.LG])\nAbstract: Artificial intelligence (AI) has been widely used in bioimage image analysis nowadays, but the efficiency of AI models, like the energy consumption and latency is not ignorable due to the growing model size and complexity, as well as the fast-growing analysis needs in modern biomedical studies. Like we can compress large images for efficient storage and sharing, we can also compress the AI models for efficient applications and deployment. In this work, we present EfficientBioAI, a plug-and-play toolbox that can compress given bioimaging AI models for them to run with significantly reduced energy cost and inference time on both CPU and GPU, without compromise on accuracy. In some cases, the prediction accuracy could even increase after compression, since the compression procedure could remove redundant information in the model representation and therefore reduce over-fitting. From four different bioimage analysis applications, we observed around 2-5 times speed-up during inference and 3",
    "path": "papers/23/06/2306.06152.json",
    "total_tokens": 941,
    "translated_title": "EfficientBioAI：使生物成像AI模型在能量、延迟和表示方面更高效",
    "translated_abstract": "如今，人工智能（AI）在生物成像分析中被广泛使用，但由于模型规模和复杂性的增长以及现代生物医学研究中需要快速增长的分析需求，AI模型的效率，如能量消耗和延迟，是不可忽视的。就像我们可以压缩大型图像以实现高效存储和共享一样，我们也可以压缩AI模型以实现高效应用和部署。在这项工作中，我们提出了EfficientBioAI，这是一个即插即用的工具箱，可以压缩给定的生物成像AI模型，使它们在CPU和GPU上运行时能够显著减少能源成本和推理时间，而不会影响准确性。在某些情况下，压缩后的预测准确率甚至可能会增加，因为压缩过程可以去除模型表示中的冗余信息，从而减少过度拟合。从四种不同的生物图像分析应用中，我们观察到推理过程中的速度提高了约2-5倍，在GPU上提高了约3-10倍，并且准确性始终得到保证。",
    "tldr": "EfficientBioAI是一个即插即用的工具箱，可以压缩生物成像AI模型，使它们在CPU和GPU上运行时能够显著减少能源成本和推理时间，而不会影响准确性。",
    "en_tdlr": "EfficientBioAI is a plug-and-play toolbox that can compress bioimaging AI models to significantly reduce energy cost and inference time on both CPU and GPU without compromising accuracy."
}