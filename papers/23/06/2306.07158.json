{
    "title": "Riemannian Laplace approximations for Bayesian neural networks. (arXiv:2306.07158v1 [stat.ML])",
    "abstract": "Bayesian neural networks often approximate the weight-posterior with a Gaussian distribution. However, practical posteriors are often, even locally, highly non-Gaussian, and empirical performance deteriorates. We propose a simple parametric approximate posterior that adapts to the shape of the true posterior through a Riemannian metric that is determined by the log-posterior gradient. We develop a Riemannian Laplace approximation where samples naturally fall into weight-regions with low negative log-posterior. We show that these samples can be drawn by solving a system of ordinary differential equations, which can be done efficiently by leveraging the structure of the Riemannian metric and automatic differentiation. Empirically, we demonstrate that our approach consistently improves over the conventional Laplace approximation across tasks. We further show that, unlike the conventional Laplace approximation, our method is not overly sensitive to the choice of prior, which alleviates a p",
    "link": "http://arxiv.org/abs/2306.07158",
    "context": "Title: Riemannian Laplace approximations for Bayesian neural networks. (arXiv:2306.07158v1 [stat.ML])\nAbstract: Bayesian neural networks often approximate the weight-posterior with a Gaussian distribution. However, practical posteriors are often, even locally, highly non-Gaussian, and empirical performance deteriorates. We propose a simple parametric approximate posterior that adapts to the shape of the true posterior through a Riemannian metric that is determined by the log-posterior gradient. We develop a Riemannian Laplace approximation where samples naturally fall into weight-regions with low negative log-posterior. We show that these samples can be drawn by solving a system of ordinary differential equations, which can be done efficiently by leveraging the structure of the Riemannian metric and automatic differentiation. Empirically, we demonstrate that our approach consistently improves over the conventional Laplace approximation across tasks. We further show that, unlike the conventional Laplace approximation, our method is not overly sensitive to the choice of prior, which alleviates a p",
    "path": "papers/23/06/2306.07158.json",
    "total_tokens": 905,
    "translated_title": "基于黎曼流形拉普拉斯逼近的贝叶斯神经网络",
    "translated_abstract": "贝叶斯神经网络通常用高斯分布来近似权值后验分布。然而，实际后验分布往往是高度非高斯的，即使是在局部情况下，模型表现也会恶化。在本文章中，我们提出了一种简单的参数逼近后验分布方法，通过用黎曼度量来确定对数后验梯度。我们开发了黎曼拉普拉斯逼近，在这种逼近下，样本会自然地落入负对数后验小的权值区域。我们表明，这些样本可以通过解一组常微分方程来抽取，并且可以通过利用黎曼度量和自动微分的结构来高效地完成。在实证上，我们证明了我们的方法在各种任务上均比传统的拉普拉斯逼近表现更好。我们进一步展示，与传统的拉普拉斯逼近不同的是，我们的方法对先验选择不过度敏感，这缓解了先验选择问题。",
    "tldr": "本论文提出了一种基于黎曼度量的简单参数逼近后验分布方法，实验表明该方法对先验选择不过度敏感，可以较好地改善传统拉普拉斯逼近的表现。",
    "en_tdlr": "This paper proposes a simple parametric approximate posterior method based on Riemannian metric, demonstrating its ability to adapt to the shape of the true posterior. The developed Riemannian Laplace approximation improves over the conventional Laplace approximation and is not overly sensitive to the choice of prior."
}