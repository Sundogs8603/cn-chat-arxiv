{
    "title": "Enhancing training of physics-informed neural networks using domain-decomposition based preconditioning strategies. (arXiv:2306.17648v1 [math.NA])",
    "abstract": "We propose to enhance the training of physics-informed neural networks (PINNs). To this aim, we introduce nonlinear additive and multiplicative preconditioning strategies for the widely used L-BFGS optimizer. The nonlinear preconditioners are constructed by utilizing the Schwarz domain-decomposition framework, where the parameters of the network are decomposed in a layer-wise manner. Through a series of numerical experiments, we demonstrate that both, additive and multiplicative preconditioners significantly improve the convergence of the standard L-BFGS optimizer, while providing more accurate solutions of the underlying partial differential equations. Moreover, the additive preconditioner is inherently parallel, thus giving rise to a novel approach to model parallelism.",
    "link": "http://arxiv.org/abs/2306.17648",
    "context": "Title: Enhancing training of physics-informed neural networks using domain-decomposition based preconditioning strategies. (arXiv:2306.17648v1 [math.NA])\nAbstract: We propose to enhance the training of physics-informed neural networks (PINNs). To this aim, we introduce nonlinear additive and multiplicative preconditioning strategies for the widely used L-BFGS optimizer. The nonlinear preconditioners are constructed by utilizing the Schwarz domain-decomposition framework, where the parameters of the network are decomposed in a layer-wise manner. Through a series of numerical experiments, we demonstrate that both, additive and multiplicative preconditioners significantly improve the convergence of the standard L-BFGS optimizer, while providing more accurate solutions of the underlying partial differential equations. Moreover, the additive preconditioner is inherently parallel, thus giving rise to a novel approach to model parallelism.",
    "path": "papers/23/06/2306.17648.json",
    "total_tokens": 807,
    "translated_title": "使用基于域分解的预条件策略增强物理信息神经网络的训练",
    "translated_abstract": "我们提出了一种增强物理信息神经网络（PINNs）训练的方法。为此，我们引入了非线性加性和乘性预条件策略，用于广泛使用的L-BFGS优化器。非线性预条件器是通过利用Schwarz域分解框架构建的，其中网络的参数以逐层方式进行分解。通过一系列数值实验，我们证明了加性和乘性预条件器都能显著改善标准L-BFGS优化器的收敛性，同时提供了更准确的偏微分方程解。此外，加性预条件器本质上是并行的，因此为一种新的模型并行方法提供了可能。",
    "tldr": "这项研究提出了一种基于域分解的预条件策略，用于增强物理信息神经网络的训练。该方法通过非线性预条件器改进了L-BFGS优化器的收敛性，并提供了更准确的偏微分方程解。加性预条件器还具有并行性，为模型并行提供了新的方法。",
    "en_tdlr": "This study proposes a domain-decomposition based preconditioning strategy to enhance the training of physics-informed neural networks. The approach improves the convergence of the L-BFGS optimizer and provides more accurate solutions for partial differential equations. The additive preconditioner also introduces parallelism, offering a novel method for model parallelism."
}