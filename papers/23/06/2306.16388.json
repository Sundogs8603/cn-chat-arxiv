{
    "title": "Towards Measuring the Representation of Subjective Global Opinions in Language Models. (arXiv:2306.16388v1 [cs.CL])",
    "abstract": "Large language models (LLMs) may not equitably represent diverse global perspectives on societal issues. In this paper, we develop a quantitative framework to evaluate whose opinions model-generated responses are more similar to. We first build a dataset, GlobalOpinionQA, comprised of questions and answers from cross-national surveys designed to capture diverse opinions on global issues across different countries. Next, we define a metric that quantifies the similarity between LLM-generated survey responses and human responses, conditioned on country. With our framework, we run three experiments on an LLM trained to be helpful, honest, and harmless with Constitutional AI. By default, LLM responses tend to be more similar to the opinions of certain populations, such as those from the USA, and some European and South American countries, highlighting the potential for biases. When we prompt the model to consider a particular country's perspective, responses shift to be more similar to the",
    "link": "http://arxiv.org/abs/2306.16388",
    "context": "Title: Towards Measuring the Representation of Subjective Global Opinions in Language Models. (arXiv:2306.16388v1 [cs.CL])\nAbstract: Large language models (LLMs) may not equitably represent diverse global perspectives on societal issues. In this paper, we develop a quantitative framework to evaluate whose opinions model-generated responses are more similar to. We first build a dataset, GlobalOpinionQA, comprised of questions and answers from cross-national surveys designed to capture diverse opinions on global issues across different countries. Next, we define a metric that quantifies the similarity between LLM-generated survey responses and human responses, conditioned on country. With our framework, we run three experiments on an LLM trained to be helpful, honest, and harmless with Constitutional AI. By default, LLM responses tend to be more similar to the opinions of certain populations, such as those from the USA, and some European and South American countries, highlighting the potential for biases. When we prompt the model to consider a particular country's perspective, responses shift to be more similar to the",
    "path": "papers/23/06/2306.16388.json",
    "total_tokens": 989,
    "translated_title": "测量语言模型中主观全球观点的方法研究",
    "translated_abstract": "大型语言模型（LLMs）可能无法公平地代表社会问题中多样化的全球观点。本文开发了一个定量框架，用于评估模型生成的回答与哪些人的观点更为相似。我们首先构建了一个数据集GlobalOpinionQA，包含了来自跨国调查的问题和答案，旨在捕捉不同国家关于全球问题的多样观点。然后，我们定义了一个度量标准，以国家为条件，量化了LLM生成的调查回答与人类回答之间的相似性。在我们的框架下，我们对一个经过宪法AI培训的LLM进行了三个实验，分别考虑其帮助性、诚实性和无害性。默认情况下，LLM的回应更倾向于与某些人群的观点更类似，例如来自美国、欧洲和南美洲的人群，凸显了潜在的偏见。当我们提示模型考虑某个特定国家的观点时，回应会更加类似于该国家的观点。",
    "tldr": "本文提出了一个方法来评估大型语言模型对全球观点的代表性。通过构建一个包含跨国调查问题和答案的数据集，并定义一个相似度度量标准，研究发现默认情况下语言模型的回应更倾向于某些人群的观点，但当模型考虑特定国家的观点时，回应会更加贴近该国家的观点。"
}