{
    "title": "Weight Compander: A Simple Weight Reparameterization for Regularization. (arXiv:2306.16993v1 [cs.LG])",
    "abstract": "Regularization is a set of techniques that are used to improve the generalization ability of deep neural networks. In this paper, we introduce weight compander (WC), a novel effective method to improve generalization by reparameterizing each weight in deep neural networks using a nonlinear function. It is a general, intuitive, cheap and easy to implement method, which can be combined with various other regularization techniques. Large weights in deep neural networks are a sign of a more complex network that is overfitted to the training data. Moreover, regularized networks tend to have a greater range of weights around zero with fewer weights centered at zero. We introduce a weight reparameterization function which is applied to each weight and implicitly reduces overfitting by restricting the magnitude of the weights while forcing them away from zero at the same time. This leads to a more democratic decision-making in the network. Firstly, individual weights cannot have too much influ",
    "link": "http://arxiv.org/abs/2306.16993",
    "context": "Title: Weight Compander: A Simple Weight Reparameterization for Regularization. (arXiv:2306.16993v1 [cs.LG])\nAbstract: Regularization is a set of techniques that are used to improve the generalization ability of deep neural networks. In this paper, we introduce weight compander (WC), a novel effective method to improve generalization by reparameterizing each weight in deep neural networks using a nonlinear function. It is a general, intuitive, cheap and easy to implement method, which can be combined with various other regularization techniques. Large weights in deep neural networks are a sign of a more complex network that is overfitted to the training data. Moreover, regularized networks tend to have a greater range of weights around zero with fewer weights centered at zero. We introduce a weight reparameterization function which is applied to each weight and implicitly reduces overfitting by restricting the magnitude of the weights while forcing them away from zero at the same time. This leads to a more democratic decision-making in the network. Firstly, individual weights cannot have too much influ",
    "path": "papers/23/06/2306.16993.json",
    "total_tokens": 863,
    "translated_title": "Weight Compander: 一种用于正则化的简单权重重新参数化方法",
    "translated_abstract": "正则化是一种用于提高深度神经网络泛化能力的技术集合。本文引入了一种名为权重压缩器（WC）的新方法，通过使用非线性函数对深度神经网络中的每个权重进行重新参数化来提高泛化能力。它是一种通用、直观、廉价且易于实现的方法，可以与其他各种正则化技术结合使用。深度神经网络中的大权重是过度拟合训练数据的复杂网络的标志。此外，正则化网络往往具有更广范围的接近零的权重，而中心接近零的权重较少。我们引入了一种权重重新参数化函数，将其应用于每个权重，通过限制权重的幅值同时使其远离零来隐式减少过拟合。这导致网络中更加民主的决策过程。首先，个体权重的影响力不太大。",
    "tldr": "本文提出了一种名为权重压缩器（WC）的新方法，通过使用非线性函数对深度神经网络中的每个权重进行重新参数化，从而提高了泛化能力并减少过拟合。",
    "en_tdlr": "This paper introduces a novel method called Weight Compander (WC) to improve generalization and reduce overfitting in deep neural networks by reparameterizing each weight using a nonlinear function."
}