{
    "title": "Phonetically-Grounded Language Generation: The Case of Tongue Twisters. (arXiv:2306.03457v1 [cs.CL])",
    "abstract": "Previous work in phonetically-grounded language generation has mainly focused on domains such as lyrics and poetry. In this paper, we present work on the generation of tongue twisters - a form of language that is required to be phonetically conditioned to maximise sound overlap, whilst maintaining semantic consistency with an input topic, and still being grammatically correct. We present \\textbf{TwistList}, a large annotated dataset of tongue twisters, consisting of 2.1K+ human-authored examples. We additionally present several benchmark systems (referred to as TwisterMisters) for the proposed task of tongue twister generation, including models that both do and do not require training on in-domain data. We present the results of automatic and human evaluation to demonstrate the performance of existing mainstream pre-trained models in this task with limited (or no) task specific training and data, and no explicit phonetic knowledge. We find that the task of tongue twister generation is ",
    "link": "http://arxiv.org/abs/2306.03457",
    "context": "Title: Phonetically-Grounded Language Generation: The Case of Tongue Twisters. (arXiv:2306.03457v1 [cs.CL])\nAbstract: Previous work in phonetically-grounded language generation has mainly focused on domains such as lyrics and poetry. In this paper, we present work on the generation of tongue twisters - a form of language that is required to be phonetically conditioned to maximise sound overlap, whilst maintaining semantic consistency with an input topic, and still being grammatically correct. We present \\textbf{TwistList}, a large annotated dataset of tongue twisters, consisting of 2.1K+ human-authored examples. We additionally present several benchmark systems (referred to as TwisterMisters) for the proposed task of tongue twister generation, including models that both do and do not require training on in-domain data. We present the results of automatic and human evaluation to demonstrate the performance of existing mainstream pre-trained models in this task with limited (or no) task specific training and data, and no explicit phonetic knowledge. We find that the task of tongue twister generation is ",
    "path": "papers/23/06/2306.03457.json",
    "total_tokens": 871,
    "translated_title": "基于音韵学的语言生成：以绕口令为例",
    "translated_abstract": "先前的音韵学语言生成主要集中在词歌和诗歌等领域。本文介绍了围绕绕口令生成展开的工作，绕口令需要在保持语义正确性的同时，最大化音频重叠并保持语法正确。我们提供了TwistList，一个包含超过2.1K人工编写的绕口令的大型注释数据集。此外，我们针对绕口令生成提出了一些基准系统(TwisterMisters)，包括需要和不需要在域内数据上进行训练的模型。我们使用自动和人工评估的结果来证明现有主流预训练模型在此任务中性能优良，即使在没有任务特定训练数据和显式音韵知识的情况下。我们发现，绕口令生成的任务是有挑战性的。",
    "tldr": "本文介绍了针对绕口令生成的基于音韵学的语言生成任务，提供了TwistList数据集和TwisterMisters基准系统，并验证了预训练模型在没有任务特定数据和显式音韵知识的情况下的良好性能。"
}