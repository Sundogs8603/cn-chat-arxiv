{
    "title": "Principles for Initialization and Architecture Selection in Graph Neural Networks with ReLU Activations. (arXiv:2306.11668v1 [stat.ML])",
    "abstract": "This article derives and validates three principles for initialization and architecture selection in finite width graph neural networks (GNNs) with ReLU activations. First, we theoretically derive what is essentially the unique generalization to ReLU GNNs of the well-known He-initialization. Our initialization scheme guarantees that the average scale of network outputs and gradients remains order one at initialization. Second, we prove in finite width vanilla ReLU GNNs that oversmoothing is unavoidable at large depth when using fixed aggregation operator, regardless of initialization. We then prove that using residual aggregation operators, obtained by interpolating a fixed aggregation operator with the identity, provably alleviates oversmoothing at initialization. Finally, we show that the common practice of using residual connections with a fixup-type initialization provably avoids correlation collapse in final layer features at initialization. Through ablation studies we find that u",
    "link": "http://arxiv.org/abs/2306.11668",
    "context": "Title: Principles for Initialization and Architecture Selection in Graph Neural Networks with ReLU Activations. (arXiv:2306.11668v1 [stat.ML])\nAbstract: This article derives and validates three principles for initialization and architecture selection in finite width graph neural networks (GNNs) with ReLU activations. First, we theoretically derive what is essentially the unique generalization to ReLU GNNs of the well-known He-initialization. Our initialization scheme guarantees that the average scale of network outputs and gradients remains order one at initialization. Second, we prove in finite width vanilla ReLU GNNs that oversmoothing is unavoidable at large depth when using fixed aggregation operator, regardless of initialization. We then prove that using residual aggregation operators, obtained by interpolating a fixed aggregation operator with the identity, provably alleviates oversmoothing at initialization. Finally, we show that the common practice of using residual connections with a fixup-type initialization provably avoids correlation collapse in final layer features at initialization. Through ablation studies we find that u",
    "path": "papers/23/06/2306.11668.json",
    "total_tokens": 895,
    "translated_title": "ReLU激活下图神经网络初始化和架构选择的原则",
    "translated_abstract": "本文在有限宽度的ReLU激活图神经网络(GNNs)中推导并验证了三个初始化和架构选择的原则。首先，我们理论上推导了ReLU GNNs He初始化的本质唯一泛化。我们的初始化方案保证了初始化时网络输出和梯度的平均规模保持为一阶。其次，在有限宽度的vanilla ReLU GNNs中，我们证明了使用固定聚合算子时，在大深度上过度平滑是不可避免的，无论初始化如何。然后我们证明，使用残差聚合算子，通过插值固定聚合算子和恒等算子来获得，可以在初始化时有效减轻过度平滑。最后，我们展示了使用修复型初始化的残差连接常规做法，可以在初始化时合理避免最终层特征的相关崩溃。通过消融实验，我们发现在GNNs中，修复型初始化+残差连接是关键。",
    "tldr": "本文提出了三个原则来指导ReLU激活下图神经网络初始化和架构选择，其中关键在于使用残差聚合算子可以减轻过度平滑，使用修复型初始化的残差连接可以避免最终层特征的相关崩溃。",
    "en_tdlr": "This paper proposes three principles to guide the initialization and architecture selection of graph neural networks with ReLU activations. The key is to use residual aggregation operators to alleviate oversmoothing and residual connections with fixup-type initialization to avoid correlation collapse in final layer features."
}