{
    "title": "Using Foundation Models to Detect Policy Violations with Minimal Supervision. (arXiv:2306.06234v1 [cs.CL])",
    "abstract": "Foundation models, i.e. large neural networks pre-trained on large text corpora, have revolutionized NLP. They can be instructed directly (e.g. (arXiv:2005.14165)) - this is called hard prompting - and they can be tuned using very little data (e.g. (arXiv:2104.08691)) - this technique is called soft prompting. We seek to leverage their capabilities to detect policy violations. Our contributions are: We identify a hard prompt that adapts chain-of-thought prompting to policy violation tasks. This prompt produces policy violation classifications, along with extractive explanations that justify the classification. We compose the hard-prompts with soft prompt tuning to produce a classifier that attains high accuracy with very little supervision; the same classifier also produces explanations. Though the supervision only acts on the classifications, we find that the modified explanations remain consistent with the (tuned) model's response. Along the way, we identify several unintuitive aspec",
    "link": "http://arxiv.org/abs/2306.06234",
    "context": "Title: Using Foundation Models to Detect Policy Violations with Minimal Supervision. (arXiv:2306.06234v1 [cs.CL])\nAbstract: Foundation models, i.e. large neural networks pre-trained on large text corpora, have revolutionized NLP. They can be instructed directly (e.g. (arXiv:2005.14165)) - this is called hard prompting - and they can be tuned using very little data (e.g. (arXiv:2104.08691)) - this technique is called soft prompting. We seek to leverage their capabilities to detect policy violations. Our contributions are: We identify a hard prompt that adapts chain-of-thought prompting to policy violation tasks. This prompt produces policy violation classifications, along with extractive explanations that justify the classification. We compose the hard-prompts with soft prompt tuning to produce a classifier that attains high accuracy with very little supervision; the same classifier also produces explanations. Though the supervision only acts on the classifications, we find that the modified explanations remain consistent with the (tuned) model's response. Along the way, we identify several unintuitive aspec",
    "path": "papers/23/06/2306.06234.json",
    "total_tokens": 877,
    "translated_title": "使用基础模型在极少监督下检测政策违规",
    "translated_abstract": "基础模型，即预训练于大型文本语料库的大型神经网络，已经彻底改变了自然语言处理。它们可以直接指导，例如硬提示(例如(arXiv:2005.14165))，也可以使用极少的数据进行调整，这种技术被称为软提示，我们试图利用它们的能力来检测政策违规。我们的创新点是：我们确定了一种硬提示，将思维链提示适应于政策违规任务。这个提示生成政策违规分类以及提取式解释来证明分类的合理性。我们将硬提示与软提示调整相结合，使用非常少的监督来生成一个分类器，同时该分类器还可以产生解释。虽然监督只作用于分类，但我们发现修改后的解释与(调整后的)模型响应保持一致。在此过程中，我们确定了一些令人费解的方面。",
    "tldr": "本文利用基础模型在极少监督下检测政策违规，创新性地将思维链提示引入政策违规任务，同时将硬提示与软提示相结合，可以高准确度地生成合理解释。",
    "en_tdlr": "This paper leverages foundation models to detect policy violations with minimal supervision, introduces chain-of-thought prompting to policy violation tasks, and combines hard and soft prompting to achieve high accuracy and generate explanations."
}