{
    "title": "To Spike or Not to Spike? A Quantitative Comparison of SNN and CNN FPGA Implementations. (arXiv:2306.12742v1 [cs.AR])",
    "abstract": "Convolutional Neural Networks (CNNs) are widely employed to solve various problems, e.g., image classification. Due to their compute- and data-intensive nature, CNN accelerators have been developed as ASICs or on FPGAs. Increasing complexity of applications has caused resource costs and energy requirements of these accelerators to grow. Spiking Neural Networks (SNNs) are an emerging alternative to CNN implementations, promising higher resource and energy efficiency. The main research question addressed in this paper is whether SNN accelerators truly meet these expectations of reduced energy requirements compared to their CNN equivalents. For this purpose, we analyze multiple SNN hardware accelerators for FPGAs regarding performance and energy efficiency. We present a novel encoding scheme of spike event queues and a novel memory organization technique to improve SNN energy efficiency further. Both techniques have been integrated into a state-of-the-art SNN architecture and evaluated fo",
    "link": "http://arxiv.org/abs/2306.12742",
    "context": "Title: To Spike or Not to Spike? A Quantitative Comparison of SNN and CNN FPGA Implementations. (arXiv:2306.12742v1 [cs.AR])\nAbstract: Convolutional Neural Networks (CNNs) are widely employed to solve various problems, e.g., image classification. Due to their compute- and data-intensive nature, CNN accelerators have been developed as ASICs or on FPGAs. Increasing complexity of applications has caused resource costs and energy requirements of these accelerators to grow. Spiking Neural Networks (SNNs) are an emerging alternative to CNN implementations, promising higher resource and energy efficiency. The main research question addressed in this paper is whether SNN accelerators truly meet these expectations of reduced energy requirements compared to their CNN equivalents. For this purpose, we analyze multiple SNN hardware accelerators for FPGAs regarding performance and energy efficiency. We present a novel encoding scheme of spike event queues and a novel memory organization technique to improve SNN energy efficiency further. Both techniques have been integrated into a state-of-the-art SNN architecture and evaluated fo",
    "path": "papers/23/06/2306.12742.json",
    "total_tokens": 927,
    "translated_title": "加Spiking还是不加Spiking？SNN和CNN FPGA实现的量化比较",
    "translated_abstract": "卷积神经网络（CNNs）被广泛用于解决各种问题，例如图像分类。由于它们的计算和数据密集型特性，CNN加速器已经作为ASIC或FPGA开发。由于应用的复杂性增加，这些加速器的资源成本和能量要求不断增长。脉冲神经网络（SNNs）是一种新兴的替代CNN实现的方法，承诺更高的资源和能量效率。本文主要研究问题是SNN加速器是否真正满足降低能量要求的期望，相比于它们的CNN等效项。为此，我们分析多个SNN硬件加速器，用于FPGA的性能和能量效率。我们提出了一种新的脉冲事件队列编码方案和一种新的内存组织技术，以进一步提高SNN的能量效率。这两种技术已经集成到最先进的SNN架构中，并进行了评估。",
    "tldr": "本文分析了多个SNN硬件加速器，用于FPGA的性能和能量效率。提出了一种SNN编码方案和一种内存组织技术以进一步提高能量效率。比较结果显示，SNN加速器没有比CNN加速器显著降低能量消耗。",
    "en_tdlr": "This paper analyzes the performance and energy efficiency of multiple SNN hardware accelerators for FPGAs. A novel encoding scheme of spike event queues and a novel memory organization technique are proposed to improve SNN energy efficiency further. However, the comparison shows that SNN accelerators do not significantly reduce energy consumption compared to CNN accelerators."
}