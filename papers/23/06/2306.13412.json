{
    "title": "CLUE: Calibrated Latent Guidance for Offline Reinforcement Learning. (arXiv:2306.13412v1 [cs.LG])",
    "abstract": "Offline reinforcement learning (RL) aims to learn an optimal policy from pre-collected and labeled datasets, which eliminates the time-consuming data collection in online RL. However, offline RL still bears a large burden of specifying/handcrafting extrinsic rewards for each transition in the offline data. As a remedy for the labor-intensive labeling, we propose to endow offline RL tasks with a few expert data and utilize the limited expert data to drive intrinsic rewards, thus eliminating the need for extrinsic rewards. To achieve that, we introduce \\textbf{C}alibrated \\textbf{L}atent g\\textbf{U}idanc\\textbf{E} (CLUE), which utilizes a conditional variational auto-encoder to learn a latent space such that intrinsic rewards can be directly qualified over the latent space. CLUE's key idea is to align the intrinsic rewards consistent with the expert intention via enforcing the embeddings of expert data to a calibrated contextual representation. We instantiate the expert-driven intrinsic ",
    "link": "http://arxiv.org/abs/2306.13412",
    "context": "Title: CLUE: Calibrated Latent Guidance for Offline Reinforcement Learning. (arXiv:2306.13412v1 [cs.LG])\nAbstract: Offline reinforcement learning (RL) aims to learn an optimal policy from pre-collected and labeled datasets, which eliminates the time-consuming data collection in online RL. However, offline RL still bears a large burden of specifying/handcrafting extrinsic rewards for each transition in the offline data. As a remedy for the labor-intensive labeling, we propose to endow offline RL tasks with a few expert data and utilize the limited expert data to drive intrinsic rewards, thus eliminating the need for extrinsic rewards. To achieve that, we introduce \\textbf{C}alibrated \\textbf{L}atent g\\textbf{U}idanc\\textbf{E} (CLUE), which utilizes a conditional variational auto-encoder to learn a latent space such that intrinsic rewards can be directly qualified over the latent space. CLUE's key idea is to align the intrinsic rewards consistent with the expert intention via enforcing the embeddings of expert data to a calibrated contextual representation. We instantiate the expert-driven intrinsic ",
    "path": "papers/23/06/2306.13412.json",
    "total_tokens": 870,
    "translated_title": "CLUE: 离线强化学习的校准潜在导向",
    "translated_abstract": "离线强化学习旨在从预先收集和标记的数据集中学习最优策略，消除了在线强化学习中耗时的数据收集过程。但是，离线强化学习仍然需要确定和制定每个数据转换的外在奖励，这仍然是一个繁重的工作。为了解决这个问题，我们提出了CLUE：通过使用一些专家数据为离线强化学习任务提供内在奖励来消除外在奖励的需求。为了实现这一点，我们引入了一种条件可变自编码器来学习一个潜在空间，使得内在奖励可以直接在潜在空间中进行评估。CLUE的关键思想是通过将专家数据的嵌入强制转换为校准的上下文表示，使内在奖励与专家意图保持一致。我们验证了专家驱动的内在奖励在多个环境中的有效性。",
    "tldr": "CLUE使用条件可变自编码器实现专家数据的内在奖励，消除了离线强化学习中其它繁重的外在奖励工作。",
    "en_tdlr": "CLUE utilizes a conditional variational auto-encoder to achieve expert-driven intrinsic rewards for offline RL, eliminating the need for handcrafting extrinsic rewards."
}