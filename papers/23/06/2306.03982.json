{
    "title": "Globally injective and bijective neural operators. (arXiv:2306.03982v1 [cs.LG])",
    "abstract": "Recently there has been great interest in operator learning, where networks learn operators between function spaces from an essentially infinite-dimensional perspective. In this work we present results for when the operators learned by these networks are injective and surjective. As a warmup, we combine prior work in both the finite-dimensional ReLU and operator learning setting by giving sharp conditions under which ReLU layers with linear neural operators are injective. We then consider the case the case when the activation function is pointwise bijective and obtain sufficient conditions for the layer to be injective. We remark that this question, while trivial in the finite-rank case, is subtler in the infinite-rank case and is proved using tools from Fredholm theory. Next, we prove that our supplied injective neural operators are universal approximators and that their implementation, with finite-rank neural networks, are still injective. This ensures that injectivity is not `lost' ",
    "link": "http://arxiv.org/abs/2306.03982",
    "context": "Title: Globally injective and bijective neural operators. (arXiv:2306.03982v1 [cs.LG])\nAbstract: Recently there has been great interest in operator learning, where networks learn operators between function spaces from an essentially infinite-dimensional perspective. In this work we present results for when the operators learned by these networks are injective and surjective. As a warmup, we combine prior work in both the finite-dimensional ReLU and operator learning setting by giving sharp conditions under which ReLU layers with linear neural operators are injective. We then consider the case the case when the activation function is pointwise bijective and obtain sufficient conditions for the layer to be injective. We remark that this question, while trivial in the finite-rank case, is subtler in the infinite-rank case and is proved using tools from Fredholm theory. Next, we prove that our supplied injective neural operators are universal approximators and that their implementation, with finite-rank neural networks, are still injective. This ensures that injectivity is not `lost' ",
    "path": "papers/23/06/2306.03982.json",
    "total_tokens": 968,
    "translated_title": "全球可测和可逆神经运算符",
    "translated_abstract": "最近，在运算学习领域，网络从基本上无限维度的视角学习函数空间之间的运算符，我们针对网络学习的运算符是单射和满射的情况进行了研究。",
    "tldr": "这篇论文研究了网络学习的运算符是否是单射和满射的情况，并给出了精确条件。它们提供的单射神经运算符是通用逼近器，并且使用有限秩神经网络实现它们，使得网络仍然单射。"
}