{
    "title": "Tokenization and the Noiseless Channel. (arXiv:2306.16842v1 [cs.CL])",
    "abstract": "Subword tokenization is a key part of many NLP pipelines. However, little is known about why some tokenizer and hyperparameter combinations lead to better downstream model performance than others. We propose that good tokenizers lead to \\emph{efficient} channel usage, where the channel is the means by which some input is conveyed to the model and efficiency can be quantified in information-theoretic terms as the ratio of the Shannon entropy to the maximum possible entropy of the token distribution. Yet, an optimal encoding according to Shannon entropy assigns extremely long codes to low-frequency tokens and very short codes to high-frequency tokens. Defining efficiency in terms of R\\'enyi entropy, on the other hand, penalizes distributions with either very high or very low-frequency tokens. In machine translation, we find that across multiple tokenizers, the R\\'enyi entropy with $\\alpha = 2.5$ has a very strong correlation with \\textsc{Bleu}: $0.78$ in comparison to just $-0.32$ for co",
    "link": "http://arxiv.org/abs/2306.16842",
    "context": "Title: Tokenization and the Noiseless Channel. (arXiv:2306.16842v1 [cs.CL])\nAbstract: Subword tokenization is a key part of many NLP pipelines. However, little is known about why some tokenizer and hyperparameter combinations lead to better downstream model performance than others. We propose that good tokenizers lead to \\emph{efficient} channel usage, where the channel is the means by which some input is conveyed to the model and efficiency can be quantified in information-theoretic terms as the ratio of the Shannon entropy to the maximum possible entropy of the token distribution. Yet, an optimal encoding according to Shannon entropy assigns extremely long codes to low-frequency tokens and very short codes to high-frequency tokens. Defining efficiency in terms of R\\'enyi entropy, on the other hand, penalizes distributions with either very high or very low-frequency tokens. In machine translation, we find that across multiple tokenizers, the R\\'enyi entropy with $\\alpha = 2.5$ has a very strong correlation with \\textsc{Bleu}: $0.78$ in comparison to just $-0.32$ for co",
    "path": "papers/23/06/2306.16842.json",
    "total_tokens": 918,
    "translated_title": "Tokenization和无噪声通道",
    "translated_abstract": "子词分词是许多自然语言处理流程的关键组成部分。然而，我们对于为什么某些分词器和超参数组合会比其他组合在下游模型性能上表现更好还知之甚少。我们提出优秀的分词器会导致\\emph{效率}较高的通道使用，其中通道是指将某些输入传递给模型的方式，而效率可以用信息论术语中的Shannon熵与令牌分布的最大熵之比来量化。然而，根据Shannon熵进行的最优编码将把低频令牌赋予极长的编码，把高频令牌赋予极短的编码。另一方面，用R\\'enyi熵来定义效率则会惩罚具有极高或极低频令牌的分布。在机器翻译中，我们发现在多个分词器中，当$\\alpha = 2.5$时，R\\'enyi熵与\\textsc{Bleu}有很强的相关性（$0.78$），而相比之下，Shannon熵与\\textsc{Bleu}的相关性仅为$-0.32$。",
    "tldr": "优秀的分词器能够实现较高的通道使用效率，并且R\\'enyi熵在机器翻译中与\\textsc{Bleu}有很强的相关性。"
}