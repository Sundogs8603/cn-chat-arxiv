{
    "title": "Evading Black-box Classifiers Without Breaking Eggs. (arXiv:2306.02895v1 [cs.CR])",
    "abstract": "Decision-based evasion attacks repeatedly query a black-box classifier to generate adversarial examples. Prior work measures the cost of such attacks by the total number of queries made to the classifier. We argue this metric is flawed. Most security-critical machine learning systems aim to weed out \"bad\" data (e.g., malware, harmful content, etc). Queries to such systems carry a fundamentally asymmetric cost: queries detected as \"bad\" come at a higher cost because they trigger additional security filters, e.g., usage throttling or account suspension. Yet, we find that existing decision-based attacks issue a large number of \"bad\" queries, which likely renders them ineffective against security-critical systems. We then design new attacks that reduce the number of bad queries by $1.5$-$7.3\\times$, but often at a significant increase in total (non-bad) queries. We thus pose it as an open problem to build black-box attacks that are more effective under realistic cost metrics.",
    "link": "http://arxiv.org/abs/2306.02895",
    "context": "Title: Evading Black-box Classifiers Without Breaking Eggs. (arXiv:2306.02895v1 [cs.CR])\nAbstract: Decision-based evasion attacks repeatedly query a black-box classifier to generate adversarial examples. Prior work measures the cost of such attacks by the total number of queries made to the classifier. We argue this metric is flawed. Most security-critical machine learning systems aim to weed out \"bad\" data (e.g., malware, harmful content, etc). Queries to such systems carry a fundamentally asymmetric cost: queries detected as \"bad\" come at a higher cost because they trigger additional security filters, e.g., usage throttling or account suspension. Yet, we find that existing decision-based attacks issue a large number of \"bad\" queries, which likely renders them ineffective against security-critical systems. We then design new attacks that reduce the number of bad queries by $1.5$-$7.3\\times$, but often at a significant increase in total (non-bad) queries. We thus pose it as an open problem to build black-box attacks that are more effective under realistic cost metrics.",
    "path": "papers/23/06/2306.02895.json",
    "total_tokens": 908,
    "translated_title": "不破坏黑盒分类器的情况下规避它的分类——基于实际代价的黑盒攻击",
    "translated_abstract": "基于决策的规避攻击是通过不断查询黑盒分类器来生成对抗性样本。本文认为现有的攻击方式在处理对安全性敏感的机器学习系统时有缺陷。因为这些系统主要目的是过滤出有害数据（例如恶意软件、有害内容等），所以查询的代价是不对等的，一旦查询被检测出是有害的，就会触发额外的安全过滤，例如使用限制或账户暂停。然而，现有的基于决策的攻击产生了大量的“有害”查询，导致它们很可能对安全关键系统无效。因此，本文提出新的攻击方式，通过减少“有害”查询的数量（最多可以减少 $1.5$ 倍到 $7.3$ 倍），以实现更加有效的黑盒攻击。但这些攻击的正常查询数量大大增加，因此提出了在实际代价度量下构建更有效的黑盒攻击的开放性问题。",
    "tldr": "本文提出了一种基于实际代价的黑盒攻击，通过设计新的攻击方式，成功减少了“有害”查询的数量，提高了黑盒攻击效率。",
    "en_tdlr": "This paper proposes a new decision-based attack method with realistic cost metrics that reduces \"bad\" queries and increases the efficiency of black-box attacks, posing an open problem for building more effective attacks under realistic cost metrics."
}