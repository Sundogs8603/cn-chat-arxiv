{
    "title": "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory. (arXiv:2306.03901v2 [cs.AI] UPDATED)",
    "abstract": "Large language models (LLMs) with memory are computationally universal. However, mainstream LLMs are not taking full advantage of memory, and the designs are heavily influenced by biological brains. Due to their approximate nature and proneness to the accumulation of errors, conventional neural memory mechanisms cannot support LLMs to simulate complex reasoning. In this paper, we seek inspiration from modern computer architectures to augment LLMs with symbolic memory for complex multi-hop reasoning. Such a symbolic memory framework is instantiated as an LLM and a set of SQL databases, where the LLM generates SQL instructions to manipulate the SQL databases. We validate the effectiveness of the proposed memory framework on a synthetic dataset requiring complex reasoning. The project website is available at https://chatdatabase.github.io/ .",
    "link": "http://arxiv.org/abs/2306.03901",
    "context": "Title: ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory. (arXiv:2306.03901v2 [cs.AI] UPDATED)\nAbstract: Large language models (LLMs) with memory are computationally universal. However, mainstream LLMs are not taking full advantage of memory, and the designs are heavily influenced by biological brains. Due to their approximate nature and proneness to the accumulation of errors, conventional neural memory mechanisms cannot support LLMs to simulate complex reasoning. In this paper, we seek inspiration from modern computer architectures to augment LLMs with symbolic memory for complex multi-hop reasoning. Such a symbolic memory framework is instantiated as an LLM and a set of SQL databases, where the LLM generates SQL instructions to manipulate the SQL databases. We validate the effectiveness of the proposed memory framework on a synthetic dataset requiring complex reasoning. The project website is available at https://chatdatabase.github.io/ .",
    "path": "papers/23/06/2306.03901.json",
    "total_tokens": 721,
    "translated_title": "ChatDB: 将数据库作为符号内存增强LLMs",
    "translated_abstract": "大语言模型（LLMs）与内存是计算通用的。然而，主流LLMs没有充分利用内存，并且设计受到生物大脑的严重影响。由于其近似性质和错误累积倾向，传统神经内存机制不能支持LLMs模拟复杂推理。在本文中，我们从现代计算机架构中寻求灵感，为复杂的多跳推理增强LLMs符号内存。这样的符号内存框架被实例化为一个LLM和一组SQL数据库，其中LLM生成SQL指令以操作SQL数据库。我们在一个需要复杂推理的合成数据集上验证了所提出的内存框架的有效性。 项目网站位于https://chatdatabase.github.io/。",
    "tldr": "ChatDB项目将SQL数据库作为符号内存，增强LLMs的复杂多跳推理能力。",
    "en_tdlr": "ChatDB project enhances the complex multi-hop reasoning capability of LLMs by augmenting them with a symbolic memory of SQL databases."
}