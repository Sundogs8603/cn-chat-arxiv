{
    "title": "MOFI: Learning Image Representations from Noisy Entity Annotated Images. (arXiv:2306.07952v1 [cs.CV])",
    "abstract": "We present MOFI, a new vision foundation model designed to learn image representations from noisy entity annotated images. MOFI differs from previous work in two key aspects: ($i$) pre-training data, and ($ii$) training recipe. Regarding data, we introduce a new approach to automatically assign entity labels to images from noisy image-text pairs. Our approach involves employing a named entity recognition model to extract entities from the alt-text, and then using a CLIP model to select the correct entities as labels of the paired image. The approach is simple, does not require costly human annotation, and can be readily scaled up to billions of image-text pairs mined from the web. Through this method, we have created Image-to-Entities (I2E), a new large-scale dataset with 1 billion images and 2 million distinct entities, covering rich visual concepts in the wild. Building upon the I2E dataset, we study different training recipes, including supervised pre-training, contrastive pre-train",
    "link": "http://arxiv.org/abs/2306.07952",
    "context": "Title: MOFI: Learning Image Representations from Noisy Entity Annotated Images. (arXiv:2306.07952v1 [cs.CV])\nAbstract: We present MOFI, a new vision foundation model designed to learn image representations from noisy entity annotated images. MOFI differs from previous work in two key aspects: ($i$) pre-training data, and ($ii$) training recipe. Regarding data, we introduce a new approach to automatically assign entity labels to images from noisy image-text pairs. Our approach involves employing a named entity recognition model to extract entities from the alt-text, and then using a CLIP model to select the correct entities as labels of the paired image. The approach is simple, does not require costly human annotation, and can be readily scaled up to billions of image-text pairs mined from the web. Through this method, we have created Image-to-Entities (I2E), a new large-scale dataset with 1 billion images and 2 million distinct entities, covering rich visual concepts in the wild. Building upon the I2E dataset, we study different training recipes, including supervised pre-training, contrastive pre-train",
    "path": "papers/23/06/2306.07952.json",
    "total_tokens": 940,
    "translated_title": "MOFI: 从含噪实体标注的图像中学习图像表示",
    "translated_abstract": "本文提出了一种新的视觉基础模型 MOFI，旨在从含噪实体标注的图像中学习图像表示。MOFI 与以往的工作有两点不同：（i）预训练数据，（ii）训练配方。在数据方面，我们引入了一种新方法，自动从含噪图像文本对中为图像指定实体标签。我们使用命名实体识别模型从 alt-text 中提取实体，然后使用 CLIP 模型选择正确的实体作为图像的标签。这种方法简单易行，不需要昂贵的人工注释，并且可以轻松扩展到从 web 上挖掘的数十亿个图像文本对。通过这种方法，我们创建了 Image-to-Entities（I2E）这一新的大规模数据集，其中包含 10 亿张图像和 200 万个不同的实体，涵盖了野外丰富的视觉概念。基于 I2E 数据集，我们研究了不同的训练配方，包括有监督的预训练、对比度预训练。",
    "tldr": "MOFI 提出了一种新的方法，自动从含噪图像文本对中为图像指定实体标签，创建了一个新的大规模数据集 I2E，通过研究不同的训练配方，学习到了能够有效学习图像表示的模型。",
    "en_tdlr": "MOFI proposes a new method to automatically assign entity labels to images from noisy image-text pairs and creates a new large-scale I2E dataset. Through studying different training recipes, an effective model for learning image representations is learned."
}