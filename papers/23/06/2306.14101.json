{
    "title": "Language models are weak learners. (arXiv:2306.14101v1 [cs.LG])",
    "abstract": "A central notion in practical and theoretical machine learning is that of a $\\textit{weak learner}$, classifiers that achieve better-than-random performance (on any given distribution over data), even by a small margin. Such weak learners form the practical basis for canonical machine learning methods such as boosting. In this work, we illustrate that prompt-based large language models can operate effectively as said weak learners. Specifically, we illustrate the use of a large language model (LLM) as a weak learner in a boosting algorithm applied to tabular data. We show that by providing (properly sampled according to the distribution of interest) text descriptions of tabular data samples, LLMs can produce a summary of the samples that serves as a template for classification and achieves the aim of acting as a weak learner on this task. We incorporate these models into a boosting approach, which in some settings can leverage the knowledge within the LLM to outperform traditional tree",
    "link": "http://arxiv.org/abs/2306.14101",
    "context": "Title: Language models are weak learners. (arXiv:2306.14101v1 [cs.LG])\nAbstract: A central notion in practical and theoretical machine learning is that of a $\\textit{weak learner}$, classifiers that achieve better-than-random performance (on any given distribution over data), even by a small margin. Such weak learners form the practical basis for canonical machine learning methods such as boosting. In this work, we illustrate that prompt-based large language models can operate effectively as said weak learners. Specifically, we illustrate the use of a large language model (LLM) as a weak learner in a boosting algorithm applied to tabular data. We show that by providing (properly sampled according to the distribution of interest) text descriptions of tabular data samples, LLMs can produce a summary of the samples that serves as a template for classification and achieves the aim of acting as a weak learner on this task. We incorporate these models into a boosting approach, which in some settings can leverage the knowledge within the LLM to outperform traditional tree",
    "path": "papers/23/06/2306.14101.json",
    "total_tokens": 846,
    "translated_title": "语言模型是弱学习器",
    "translated_abstract": "实践和理论机器学习中的一个中心概念是弱学习器，即在任何给定的数据分布上都能取得比随机更好的性能的分类器，即使只是略微好一点。这样的弱学习器构成了经典机器学习方法（如boosting）的实用基础。在这项工作中，我们展示了基于提示的大型语言模型可以有效地作为上述弱学习器进行操作。具体而言，我们演示了在表格数据中使用大型语言模型（LLM）作为boosting算法中的弱学习器。我们展示了通过提供（根据感兴趣的分布进行适当采样的）表格数据样本的文本描述，LLM可以产生样本的汇总，作为分类的模板，并在这个任务中实现作为弱学习器的目的。我们将这些模型纳入boosting方法中，在某些环境中，可以利用LLM中的知识，优于传统的树形模型。",
    "tldr": "本文证明了，基于提示的大型语言模型可以作为弱学习器应用于表格数据的boosting算法中，并且在某些情况下可以优于传统的树形模型。",
    "en_tdlr": "Language models based on prompts can be used as weak learners in boosting algorithms for tabular data, and in some cases can outperform traditional tree models."
}