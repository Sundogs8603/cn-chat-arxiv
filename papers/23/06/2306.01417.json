{
    "title": "The Flawed Foundations of Fair Machine Learning. (arXiv:2306.01417v1 [cs.CY])",
    "abstract": "The definition and implementation of fairness in automated decisions has been extensively studied by the research community. Yet, there hides fallacious reasoning, misleading assertions, and questionable practices at the foundations of the current fair machine learning paradigm. Those flaws are the result of a failure to understand that the trade-off between statistically accurate outcomes and group similar outcomes exists as independent, external constraint rather than as a subjective manifestation as has been commonly argued. First, we explain that there is only one conception of fairness present in the fair machine learning literature: group similarity of outcomes based on a sensitive attribute where the similarity benefits an underprivileged group. Second, we show that there is, in fact, a trade-off between statistically accurate outcomes and group similar outcomes in any data setting where group disparities exist, and that the trade-off presents an existential threat to the equita",
    "link": "http://arxiv.org/abs/2306.01417",
    "context": "Title: The Flawed Foundations of Fair Machine Learning. (arXiv:2306.01417v1 [cs.CY])\nAbstract: The definition and implementation of fairness in automated decisions has been extensively studied by the research community. Yet, there hides fallacious reasoning, misleading assertions, and questionable practices at the foundations of the current fair machine learning paradigm. Those flaws are the result of a failure to understand that the trade-off between statistically accurate outcomes and group similar outcomes exists as independent, external constraint rather than as a subjective manifestation as has been commonly argued. First, we explain that there is only one conception of fairness present in the fair machine learning literature: group similarity of outcomes based on a sensitive attribute where the similarity benefits an underprivileged group. Second, we show that there is, in fact, a trade-off between statistically accurate outcomes and group similar outcomes in any data setting where group disparities exist, and that the trade-off presents an existential threat to the equita",
    "path": "papers/23/06/2306.01417.json",
    "total_tokens": 848,
    "translated_title": "公平机器学习的基础存在缺陷",
    "translated_abstract": "研究人员广泛研究了自动化决策中公平性的定义和实施。然而，当前公平机器学习范例的基础存在着错误的推理、误导性的断言和可疑的实践。这些缺陷是由于没有理解在存在统计上准确的结果和组相似的结果之间的权衡是一种独立的外部限制而非主观彰显的结果所导致的。首先，我们解释了在公平机器学习文献中只存在一种公平的概念：基于敏感属性的结果组相似性，其中相似性有益于一个弱势组。其次，我们展示在任何存在组差异的数据情况下，统计上准确的结果和组相似的结果之间确实存在权衡，而这种权衡对公平存在着重大威胁。",
    "tldr": "该论文对公平机器学习领域的现在存在的缺陷进行探讨，指出了研究者未能正确理解在实现公平性时，准确率和组相似性之间的权衡关系，该权衡关系可能对公平性构成实质性威胁。",
    "en_tdlr": "This paper discusses the flaws in the current fair machine learning paradigm, pointing out that researchers have failed to properly understand the trade-off between accuracy and group similarity when implementing fairness, which may pose an existential threat to fairness."
}