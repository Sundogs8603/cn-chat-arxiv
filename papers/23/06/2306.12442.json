{
    "title": "Knowledge Distillation via Token-level Relationship Graph. (arXiv:2306.12442v1 [cs.LG])",
    "abstract": "Knowledge distillation is a powerful technique for transferring knowledge from a pre-trained teacher model to a student model. However, the true potential of knowledge transfer has not been fully explored. Existing approaches primarily focus on distilling individual information or instance-level relationships, overlooking the valuable information embedded in token-level relationships, which may be particularly affected by the long-tail effects. To address the above limitations, we propose a novel method called Knowledge Distillation with Token-level Relationship Graph (TRG) that leverages the token-wise relational knowledge to enhance the performance of knowledge distillation. By employing TRG, the student model can effectively emulate higher-level semantic information from the teacher model, resulting in improved distillation results. To further enhance the learning process, we introduce a token-wise contextual loss called contextual loss, which encourages the student model to capture",
    "link": "http://arxiv.org/abs/2306.12442",
    "context": "Title: Knowledge Distillation via Token-level Relationship Graph. (arXiv:2306.12442v1 [cs.LG])\nAbstract: Knowledge distillation is a powerful technique for transferring knowledge from a pre-trained teacher model to a student model. However, the true potential of knowledge transfer has not been fully explored. Existing approaches primarily focus on distilling individual information or instance-level relationships, overlooking the valuable information embedded in token-level relationships, which may be particularly affected by the long-tail effects. To address the above limitations, we propose a novel method called Knowledge Distillation with Token-level Relationship Graph (TRG) that leverages the token-wise relational knowledge to enhance the performance of knowledge distillation. By employing TRG, the student model can effectively emulate higher-level semantic information from the teacher model, resulting in improved distillation results. To further enhance the learning process, we introduce a token-wise contextual loss called contextual loss, which encourages the student model to capture",
    "path": "papers/23/06/2306.12442.json",
    "total_tokens": 893,
    "translated_title": "基于词级别关系图的知识蒸馏",
    "translated_abstract": "知识蒸馏是将预先训练好的教师模型中的知识传递给学生模型的一种有效技术。然而，知识传递的真正潜力还没有被充分挖掘。现有的方法主要集中在蒸馏单个信息或实例级别的关系，忽略了嵌入在词级别关系中的有价值的信息，这可能会受到长尾效应的影响。为了解决上述限制，我们提出了一种称为基于词级别关系图(TRG)的知识蒸馏的新方法，利用词级别的关系知识来提高知识蒸馏的性能。通过使用TRG，学生模型可以有效地模拟教师模型中更高级别的语义信息，从而提高了蒸馏结果。为了进一步增强学习过程，我们引入了一种称为上下文损失的词级别上下文损失，鼓励学生模型捕捉",
    "tldr": "本文提出了一种新方法，基于词级别关系图(TRG)，用于提高知识蒸馏的性能。通过利用TRG，学生模型可以模拟教师模型中更高级别的语义信息。同时，还引入了一种上下文损失以进一步增强学习过程。",
    "en_tdlr": "This paper proposes a novel method named Knowledge Distillation with Token-level Relationship Graph (TRG) for improving knowledge distillation performance by leveraging token-wise relational knowledge. It introduces a token-wise contextual loss to further enhance the learning process."
}