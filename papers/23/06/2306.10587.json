{
    "title": "Optimism and Adaptivity in Policy Optimization. (arXiv:2306.10587v1 [cs.LG])",
    "abstract": "We work towards a unifying paradigm for accelerating policy optimization methods in reinforcement learning (RL) through \\emph{optimism} \\& \\emph{adaptivity}. Leveraging the deep connection between policy iteration and policy gradient methods, we recast seemingly unrelated policy optimization algorithms as the repeated application of two interleaving steps (i) an \\emph{optimistic policy improvement operator} maps a prior policy $\\pi_t$ to a hypothesis $\\pi_{t+1}$ using a \\emph{gradient ascent prediction}, followed by (ii) a \\emph{hindsight adaptation} of the optimistic prediction based on a partial evaluation of the performance of $\\pi_{t+1}$. We use this shared lens to jointly express other well-known algorithms, including soft and optimistic policy iteration, natural actor-critic methods, model-based policy improvement based on forward search, and meta-learning algorithms. By doing so, we shed light on collective theoretical properties related to acceleration via optimism \\& adaptivit",
    "link": "http://arxiv.org/abs/2306.10587",
    "context": "Title: Optimism and Adaptivity in Policy Optimization. (arXiv:2306.10587v1 [cs.LG])\nAbstract: We work towards a unifying paradigm for accelerating policy optimization methods in reinforcement learning (RL) through \\emph{optimism} \\& \\emph{adaptivity}. Leveraging the deep connection between policy iteration and policy gradient methods, we recast seemingly unrelated policy optimization algorithms as the repeated application of two interleaving steps (i) an \\emph{optimistic policy improvement operator} maps a prior policy $\\pi_t$ to a hypothesis $\\pi_{t+1}$ using a \\emph{gradient ascent prediction}, followed by (ii) a \\emph{hindsight adaptation} of the optimistic prediction based on a partial evaluation of the performance of $\\pi_{t+1}$. We use this shared lens to jointly express other well-known algorithms, including soft and optimistic policy iteration, natural actor-critic methods, model-based policy improvement based on forward search, and meta-learning algorithms. By doing so, we shed light on collective theoretical properties related to acceleration via optimism \\& adaptivit",
    "path": "papers/23/06/2306.10587.json",
    "total_tokens": 939,
    "translated_title": "策略优化中的乐观性和适应性",
    "translated_abstract": "本文致力于通过“乐观性”和“适应性”在强化学习中加速策略优化方法的统一范式。通过利用策略迭代和策略梯度方法之间的深刻联系，我们将一些看似无关的策略优化算法重新构造为两个交错步骤（i）乐观策略改进操作器使用“梯度上升预测”将先前的策略$\\pi_t$映射到一个假设$\\pi_{t+1}$，然后（ii）对$\\pi_{t+1}$的性能进行部分评估，并基于此进行“后见适应”。我们使用这个共享的视角来共同表达其他众所周知的算法，包括软件和乐观策略迭代、自然演员-评论家方法、基于前向搜索的基于模型的策略改进和元学习算法。通过这样做，我们揭示了关于通过乐观性和适应性加速的共同理论属性。",
    "tldr": "本文通过将看似无关的策略优化算法重新构造为共同的两个交错步骤，即乐观策略改进和后见适应，统一了强化学习中的策略优化方法，揭示了加速方法中的乐观性和适应性的共同理论属性。",
    "en_tdlr": "This paper unifies policy optimization methods in reinforcement learning through optimism and adaptivity, by recasting seemingly unrelated algorithms as two interleaving steps of optimistic policy improvement and hindsight adaptation. The shared lens sheds light on collective theoretical properties related to acceleration via optimism and adaptivity."
}