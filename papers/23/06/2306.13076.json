{
    "title": "A Comparison of Time-based Models for Multimodal Emotion Recognition. (arXiv:2306.13076v1 [cs.LG])",
    "abstract": "Emotion recognition has become an important research topic in the field of human-computer interaction. Studies on sound and videos to understand emotions focused mainly on analyzing facial expressions and classified 6 basic emotions. In this study, the performance of different sequence models in multi-modal emotion recognition was compared. The sound and images were first processed by multi-layered CNN models, and the outputs of these models were fed into various sequence models. The sequence model is GRU, Transformer, LSTM and Max Pooling. Accuracy, precision, and F1 Score values of all models were calculated. The multi-modal CREMA-D dataset was used in the experiments. As a result of the comparison of the CREMA-D dataset, GRU-based architecture with 0.640 showed the best result in F1 score, LSTM-based architecture with 0.699 in precision metric, while sensitivity showed the best results over time with Max Pooling-based architecture with 0.620. As a result, it has been observed that t",
    "link": "http://arxiv.org/abs/2306.13076",
    "context": "Title: A Comparison of Time-based Models for Multimodal Emotion Recognition. (arXiv:2306.13076v1 [cs.LG])\nAbstract: Emotion recognition has become an important research topic in the field of human-computer interaction. Studies on sound and videos to understand emotions focused mainly on analyzing facial expressions and classified 6 basic emotions. In this study, the performance of different sequence models in multi-modal emotion recognition was compared. The sound and images were first processed by multi-layered CNN models, and the outputs of these models were fed into various sequence models. The sequence model is GRU, Transformer, LSTM and Max Pooling. Accuracy, precision, and F1 Score values of all models were calculated. The multi-modal CREMA-D dataset was used in the experiments. As a result of the comparison of the CREMA-D dataset, GRU-based architecture with 0.640 showed the best result in F1 score, LSTM-based architecture with 0.699 in precision metric, while sensitivity showed the best results over time with Max Pooling-based architecture with 0.620. As a result, it has been observed that t",
    "path": "papers/23/06/2306.13076.json",
    "total_tokens": 946,
    "translated_title": "多模态情感识别中基于时间的模型比较",
    "translated_abstract": "情感识别已成为人机交互领域中的重要研究课题。关于声音和视频以了解情绪的研究主要集中在分析面部表情，并将其分类为6种基本情绪。本文比较了不同序列模型在多模态情感识别中的表现。首先，使用多层CNN模型对声音和图像进行处理，然后将这些模型的输出馈送到各种序列模型中。序列模型包括GRU、Transformer、LSTM和Max Pooling。计算了所有模型的准确度、精确度和F1 Score值。实验中使用了多模态CREMA-D数据集。比较CREMA-D数据集的结果表明，基于GRU的架构在F1分数方面表现最佳，得分为0.640，基于LSTM的架构在精度指标方面表现最佳，得分为0.699，而基于Max Pooling的架构随着时间的推移显示出最佳的敏感度，得分为0.620。因此，观察到将声音数据转录成文本并与图像数据一起分析可以提高情感识别的准确性。",
    "tldr": "本研究比较了不同序列模型在多模态情感识别中的表现，发现将声音数据转录成文本并与图像数据一起分析可以提高情感识别的准确性。",
    "en_tdlr": "This study compares the performance of different sequence models in multi-modal emotion recognition and finds that transcribing sound data into text and analyzing them with image data increases the accuracy of emotion recognition."
}