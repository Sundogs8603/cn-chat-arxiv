{
    "title": "Verifying Global Neural Network Specifications using Hyperproperties. (arXiv:2306.12495v1 [cs.LG])",
    "abstract": "Current approaches to neural network verification focus on specifications that target small regions around known input data points, such as local robustness. Thus, using these approaches, we can not obtain guarantees for inputs that are not close to known inputs. Yet, it is highly likely that a neural network will encounter such truly unseen inputs during its application. We study global specifications that - when satisfied - provide guarantees for all potential inputs. We introduce a hyperproperty formalism that allows for expressing global specifications such as monotonicity, Lipschitz continuity, global robustness, and dependency fairness. Our formalism enables verifying global specifications using existing neural network verification approaches by leveraging capabilities for verifying general computational graphs. Thereby, we extend the scope of guarantees that can be provided using existing methods. Recent success in verifying specific global specifications shows that attaining st",
    "link": "http://arxiv.org/abs/2306.12495",
    "context": "Title: Verifying Global Neural Network Specifications using Hyperproperties. (arXiv:2306.12495v1 [cs.LG])\nAbstract: Current approaches to neural network verification focus on specifications that target small regions around known input data points, such as local robustness. Thus, using these approaches, we can not obtain guarantees for inputs that are not close to known inputs. Yet, it is highly likely that a neural network will encounter such truly unseen inputs during its application. We study global specifications that - when satisfied - provide guarantees for all potential inputs. We introduce a hyperproperty formalism that allows for expressing global specifications such as monotonicity, Lipschitz continuity, global robustness, and dependency fairness. Our formalism enables verifying global specifications using existing neural network verification approaches by leveraging capabilities for verifying general computational graphs. Thereby, we extend the scope of guarantees that can be provided using existing methods. Recent success in verifying specific global specifications shows that attaining st",
    "path": "papers/23/06/2306.12495.json",
    "total_tokens": 787,
    "translated_abstract": "目前神经网络验证的方法主要关注小范围区域内的规范，例如局部稳健性。因此，使用这些方法，我们无法为未接触过的输入数据提供保障。我们研究了全局规范，当这些规范得到满足时，可以为所有潜在的输入提供保障。我们引入了一种超性质形式化方法，可以表达全局规范，例如单调性，Lipschitz连续性，全局稳健性和依赖公平性。我们的形式化方法通过利用验证通用计算图的能力，可以使用现有的神经网络验证方法来验证全局规范，并扩展了现有方法可以提供的保障范围。最近在验证特定的全局规范方面取得的成功表明，达成全局规范的验证是可行的。",
    "tldr": "该论文提出了一种新的形式化方法，可以验证全局神经网络规范。该方法可以为所有潜在的输入提供保障，并扩展了现有方法可以提供的保障范围。",
    "en_tdlr": "This paper proposes a new formalism for verifying global neural network specifications, which provides guarantees for all potential inputs and extends the scope of existing verification methods."
}