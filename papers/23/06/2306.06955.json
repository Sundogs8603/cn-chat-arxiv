{
    "title": "A Brief Review of Hypernetworks in Deep Learning. (arXiv:2306.06955v2 [cs.LG] UPDATED)",
    "abstract": "Hypernetworks, or hypernets in short, are neural networks that generate weights for another neural network, known as the target network. They have emerged as a powerful deep learning technique that allows for greater flexibility, adaptability, dynamism, faster training, information sharing, and model compression etc. Hypernets have shown promising results in a variety of deep learning problems, including continual learning, causal inference, transfer learning, weight pruning, uncertainty quantification, zero-shot learning, natural language processing, and reinforcement learning etc. Despite their success across different problem settings, currently, there is no review available to inform the researchers about the developments and to help in utilizing hypernets. To fill this gap, we review the progress in hypernets. We present an illustrative example to train deep neural networks using hypernets and propose categorizing hypernets based on five design criteria as inputs, outputs, variabi",
    "link": "http://arxiv.org/abs/2306.06955",
    "context": "Title: A Brief Review of Hypernetworks in Deep Learning. (arXiv:2306.06955v2 [cs.LG] UPDATED)\nAbstract: Hypernetworks, or hypernets in short, are neural networks that generate weights for another neural network, known as the target network. They have emerged as a powerful deep learning technique that allows for greater flexibility, adaptability, dynamism, faster training, information sharing, and model compression etc. Hypernets have shown promising results in a variety of deep learning problems, including continual learning, causal inference, transfer learning, weight pruning, uncertainty quantification, zero-shot learning, natural language processing, and reinforcement learning etc. Despite their success across different problem settings, currently, there is no review available to inform the researchers about the developments and to help in utilizing hypernets. To fill this gap, we review the progress in hypernets. We present an illustrative example to train deep neural networks using hypernets and propose categorizing hypernets based on five design criteria as inputs, outputs, variabi",
    "path": "papers/23/06/2306.06955.json",
    "total_tokens": 981,
    "translated_title": "深度学习中的超网络简要回顾",
    "translated_abstract": "超网络（Hypernetworks）是生成另一个神经网络（目标网络）权重的神经网络。它们作为一种强大的深度学习技术出现，能够提供更大的灵活性、适应性、动态性、更快的训练速度、信息共享和模型压缩等。超网络在各种深度学习问题中显示出了良好的效果，包括持续学习、因果推断、迁移学习、权重剪枝、不确定性量化、零样本学习、自然语言处理和强化学习等。尽管超网络在不同的问题设置中取得了成功，但目前还没有可用的综述来告知研究人员有关其发展情况并帮助利用超网络。为了填补这一空白，我们回顾了超网络的进展。我们通过一个例子来说明如何使用超网络来训练深度神经网络，并提出了基于五个设计准则对超网络进行分类。",
    "tldr": "超网络是一种生成另一个神经网络权重的深度学习技术，具有灵活性、适应性、动态性、更快的训练速度、信息共享和模型压缩等优点。它在各种深度学习问题中显示了良好的效果，并在持续学习、迁移学习、权重剪枝、不确定性量化、零样本学习、自然语言处理和强化学习等领域取得了成功。",
    "en_tdlr": "Hypernetworks are a powerful deep learning technique that generate weights for another neural network. They offer flexibility, adaptability, dynamism, faster training, information sharing, and model compression. Hypernets have shown promising results in various deep learning problems, including continual learning, transfer learning, weight pruning, uncertainty quantification, zero-shot learning, natural language processing, and reinforcement learning."
}