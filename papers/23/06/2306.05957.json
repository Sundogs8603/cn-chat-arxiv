{
    "title": "DDLP: Unsupervised Object-Centric Video Prediction with Deep Dynamic Latent Particles. (arXiv:2306.05957v1 [cs.CV])",
    "abstract": "We propose a new object-centric video prediction algorithm based on the deep latent particle (DLP) representation. In comparison to existing slot- or patch-based representations, DLPs model the scene using a set of keypoints with learned parameters for properties such as position and size, and are both efficient and interpretable. Our method, deep dynamic latent particles (DDLP), yields state-of-the-art object-centric video prediction results on several challenging datasets. The interpretable nature of DDLP allows us to perform ``what-if'' generation -- predict the consequence of changing properties of objects in the initial frames, and DLP's compact structure enables efficient diffusion-based unconditional video generation. Videos, code and pre-trained models are available: https://taldatech.github.io/ddlp-web",
    "link": "http://arxiv.org/abs/2306.05957",
    "context": "Title: DDLP: Unsupervised Object-Centric Video Prediction with Deep Dynamic Latent Particles. (arXiv:2306.05957v1 [cs.CV])\nAbstract: We propose a new object-centric video prediction algorithm based on the deep latent particle (DLP) representation. In comparison to existing slot- or patch-based representations, DLPs model the scene using a set of keypoints with learned parameters for properties such as position and size, and are both efficient and interpretable. Our method, deep dynamic latent particles (DDLP), yields state-of-the-art object-centric video prediction results on several challenging datasets. The interpretable nature of DDLP allows us to perform ``what-if'' generation -- predict the consequence of changing properties of objects in the initial frames, and DLP's compact structure enables efficient diffusion-based unconditional video generation. Videos, code and pre-trained models are available: https://taldatech.github.io/ddlp-web",
    "path": "papers/23/06/2306.05957.json",
    "total_tokens": 845,
    "translated_title": "DDLP：基于深度动态潜在粒子的无监督物体中心视频预测",
    "translated_abstract": "本文提出了一种基于深度潜在粒子（DLP）表示的新型物体中心视频预测算法。与现有的基于槽或补丁的表示相比，DLP使用一组关键点模拟场景，学习参数用于属性例如位置和大小，并且既高效又可解释。我们的方法——深度动态潜在粒子(DDLP)，在多个具有挑战性的数据集上实现了最先进的物体中心视频预测结果。DDLP的可解释性使我们能够执行“假设”生成——预测更改初始帧中对象属性的结果，而DLP的紧凑结构使得效率高并可以进行基于扩散的无条件视频生成。视频、代码和预训练模型可在此链接找到：https://taldatech.github.io/ddlp-web",
    "tldr": "DDLP算法使用深度潜在粒子(DLP)表示法实现无监督物体中心视频预测，并取得了最先进的预测结果。算法的可解释性使得可以进行“假设”生成，而DLP的紧凑结构使得效率高并可以进行基于扩散的无条件视频生成。",
    "en_tdlr": "DDLP algorithm uses deep latent particles (DLP) representation to achieve unsupervised object-centric video prediction and achieves state-of-the-art results. The interpretability of the algorithm allows for \"what-if\" generation, while the compact structure of DLP enables efficient diffusion-based unconditional video generation."
}