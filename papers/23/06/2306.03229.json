{
    "title": "Adversarial alignment: Breaking the trade-off between the strength of an attack and its relevance to human perception. (arXiv:2306.03229v1 [cs.CV])",
    "abstract": "Deep neural networks (DNNs) are known to have a fundamental sensitivity to adversarial attacks, perturbations of the input that are imperceptible to humans yet powerful enough to change the visual decision of a model. Adversarial attacks have long been considered the \"Achilles' heel\" of deep learning, which may eventually force a shift in modeling paradigms. Nevertheless, the formidable capabilities of modern large-scale DNNs have somewhat eclipsed these early concerns. Do adversarial attacks continue to pose a threat to DNNs?  Here, we investigate how the robustness of DNNs to adversarial attacks has evolved as their accuracy on ImageNet has continued to improve. We measure adversarial robustness in two different ways: First, we measure the smallest adversarial attack needed to cause a model to change its object categorization decision. Second, we measure how aligned successful attacks are with the features that humans find diagnostic for object recognition. We find that adversarial a",
    "link": "http://arxiv.org/abs/2306.03229",
    "context": "Title: Adversarial alignment: Breaking the trade-off between the strength of an attack and its relevance to human perception. (arXiv:2306.03229v1 [cs.CV])\nAbstract: Deep neural networks (DNNs) are known to have a fundamental sensitivity to adversarial attacks, perturbations of the input that are imperceptible to humans yet powerful enough to change the visual decision of a model. Adversarial attacks have long been considered the \"Achilles' heel\" of deep learning, which may eventually force a shift in modeling paradigms. Nevertheless, the formidable capabilities of modern large-scale DNNs have somewhat eclipsed these early concerns. Do adversarial attacks continue to pose a threat to DNNs?  Here, we investigate how the robustness of DNNs to adversarial attacks has evolved as their accuracy on ImageNet has continued to improve. We measure adversarial robustness in two different ways: First, we measure the smallest adversarial attack needed to cause a model to change its object categorization decision. Second, we measure how aligned successful attacks are with the features that humans find diagnostic for object recognition. We find that adversarial a",
    "path": "papers/23/06/2306.03229.json",
    "total_tokens": 822,
    "translated_title": "对抗对齐: 打破攻击强度与其对人类感知影响之间的权衡",
    "translated_abstract": "深度神经网络(DNNs), 受对抗性攻击的影响敏感。这些攻击会对输入进行微小的扰动，但足以改变模型的视觉决策。本文研究了DNNs对对抗攻击的鲁棒性随着其在ImageNet上的准确性改善而发展的方式。研究发现，对抗对齐是DNNs的新的基本挑战，并且在评估其鲁棒性时应加以考虑。",
    "tldr": "对抗攻击已成为深度神经网络的弱点, 而对抗对齐是一种新的挑战，需要考虑更多。",
    "en_tdlr": "Adversarial attacks are a weakness of deep neural networks, and adversarial alignment presents a new challenge that must be taken into account."
}