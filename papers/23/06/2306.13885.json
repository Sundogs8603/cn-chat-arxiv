{
    "title": "Manipulation Risks in Explainable AI: The Implications of the Disagreement Problem. (arXiv:2306.13885v1 [cs.AI])",
    "abstract": "Artificial Intelligence (AI) systems are increasingly used in high-stakes domains of our life, increasing the need to explain these decisions and to make sure that they are aligned with how we want the decision to be made. The field of Explainable AI (XAI) has emerged in response. However, it faces a significant challenge known as the disagreement problem, where multiple explanations are possible for the same AI decision or prediction. While the existence of the disagreement problem is acknowledged, the potential implications associated with this problem have not yet been widely studied. First, we provide an overview of the different strategies explanation providers could deploy to adapt the returned explanation to their benefit. We make a distinction between strategies that attack the machine learning model or underlying data to influence the explanations, and strategies that leverage the explanation phase directly. Next, we analyse several objectives and concrete scenarios the provid",
    "link": "http://arxiv.org/abs/2306.13885",
    "context": "Title: Manipulation Risks in Explainable AI: The Implications of the Disagreement Problem. (arXiv:2306.13885v1 [cs.AI])\nAbstract: Artificial Intelligence (AI) systems are increasingly used in high-stakes domains of our life, increasing the need to explain these decisions and to make sure that they are aligned with how we want the decision to be made. The field of Explainable AI (XAI) has emerged in response. However, it faces a significant challenge known as the disagreement problem, where multiple explanations are possible for the same AI decision or prediction. While the existence of the disagreement problem is acknowledged, the potential implications associated with this problem have not yet been widely studied. First, we provide an overview of the different strategies explanation providers could deploy to adapt the returned explanation to their benefit. We make a distinction between strategies that attack the machine learning model or underlying data to influence the explanations, and strategies that leverage the explanation phase directly. Next, we analyse several objectives and concrete scenarios the provid",
    "path": "papers/23/06/2306.13885.json",
    "total_tokens": 1001,
    "translated_title": "可解释人工智能中的操纵风险: 不一致问题的影响",
    "translated_abstract": "人工智能系统在我们生活的高风险领域中越来越广泛地应用，这增加了解释这些决策并确保它们与我们想要的决策一致的需求。因此，可解释人工智能（XAI）领域显现出来。然而，它面临一项重大挑战，即不一致问题，即同一人工智能的决策或预测可能有多种解释。虽然已经认识到了不一致问题的存在，但与此问题相关的潜在影响尚未被广泛研究。在本文中，我们首先概述了解释提供者可以采用的不同策略，以使返回的解释符合他们的利益。我们区分了攻击机器学习模型或底层数据以影响解释的策略和直接利用解释阶段的策略。接下来，我们分析了解释提供者可追求的几个目标和具体场景。",
    "tldr": "本文讨论可解释人工智能中的操纵风险，即同一决策或预测可能有多种解释带来的挑战。本文分析了攻击机器学习模型或底层数据以影响解释与直接利用解释阶段的策略，并探讨了解释提供者可追求的几个目标和具体场景。",
    "en_tdlr": "This paper discusses the risks of manipulation in explainable AI, which arise from the challenge of the disagreement problem where multiple explanations are possible for the same AI decision or prediction. The authors provide an overview of different strategies used by explanation providers to adapt explanations to their benefit, distinguishing between those that attack the machine learning model or underlying data and those that leverage the explanation phase directly. The paper also analyzes several objectives and concrete scenarios that explanation providers may pursue."
}