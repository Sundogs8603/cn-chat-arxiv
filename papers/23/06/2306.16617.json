{
    "title": "Curvature-Independent Last-Iterate Convergence for Games on Riemannian Manifolds. (arXiv:2306.16617v1 [math.OC])",
    "abstract": "Numerous applications in machine learning and data analytics can be formulated as equilibrium computation over Riemannian manifolds. Despite the extensive investigation of their Euclidean counterparts, the performance of Riemannian gradient-based algorithms remain opaque and poorly understood. We revisit the original scheme of Riemannian gradient descent (RGD) and analyze it under a geodesic monotonicity assumption, which includes the well-studied geodesically convex-concave min-max optimization problem as a special case. Our main contribution is to show that, despite the phenomenon of distance distortion, the RGD scheme, with a step size that is agnostic to the manifold's curvature, achieves a curvature-independent and linear last-iterate convergence rate in the geodesically strongly monotone setting. To the best of our knowledge, the possibility of curvature-independent rates and/or last-iterate convergence in the Riemannian setting has not been considered before.",
    "link": "http://arxiv.org/abs/2306.16617",
    "context": "Title: Curvature-Independent Last-Iterate Convergence for Games on Riemannian Manifolds. (arXiv:2306.16617v1 [math.OC])\nAbstract: Numerous applications in machine learning and data analytics can be formulated as equilibrium computation over Riemannian manifolds. Despite the extensive investigation of their Euclidean counterparts, the performance of Riemannian gradient-based algorithms remain opaque and poorly understood. We revisit the original scheme of Riemannian gradient descent (RGD) and analyze it under a geodesic monotonicity assumption, which includes the well-studied geodesically convex-concave min-max optimization problem as a special case. Our main contribution is to show that, despite the phenomenon of distance distortion, the RGD scheme, with a step size that is agnostic to the manifold's curvature, achieves a curvature-independent and linear last-iterate convergence rate in the geodesically strongly monotone setting. To the best of our knowledge, the possibility of curvature-independent rates and/or last-iterate convergence in the Riemannian setting has not been considered before.",
    "path": "papers/23/06/2306.16617.json",
    "total_tokens": 917,
    "translated_title": "在黎曼流形上的游戏中无关曲率的最后收敛性",
    "translated_abstract": "机器学习和数据分析中的许多应用可以以黎曼流形上的均衡计算形式化。尽管对它们的欧几里德对应物进行了大量研究，但黎曼梯度下降算法的性能仍然不透明且难以理解。我们重新审视了黎曼梯度下降（RGD）的原始方案，并在对测地线单调性假设进行分析，其中包括了研究充分的测地线凸凹极值优化问题作为一个特殊情况。我们的主要贡献是表明，尽管存在距离失真现象，但具有对曲率不敏感的固定步长的RGD方案在测地线强单调设置下可以实现曲率无关和线性的最后收敛速度。据我们所知，以前从未考虑过在黎曼设置中存在曲率无关速率和/或最后收敛性的可能性。",
    "tldr": "该论文通过对黎曼梯度下降算法进行分析，证明了在测地线强单调设置下，具有对曲率不敏感的固定步长的RGD方案可以实现曲率无关和线性的最后收敛速度。",
    "en_tdlr": "This paper analyzes the original scheme of Riemannian gradient descent (RGD) and shows that, under a geodesically strongly monotone setting, RGD with a step size that is agnostic to the manifold's curvature achieves a curvature-independent and linear last-iterate convergence rate."
}