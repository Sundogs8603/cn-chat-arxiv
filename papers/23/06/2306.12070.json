{
    "title": "Task-Robust Pre-Training for Worst-Case Downstream Adaptation. (arXiv:2306.12070v1 [cs.CV])",
    "abstract": "Pre-training has achieved remarkable success when transferred to downstream tasks. In machine learning, we care about not only the good performance of a model but also its behavior under reasonable shifts of condition. The same philosophy holds when pre-training a foundation model. However, the foundation model may not uniformly behave well for a series of related downstream tasks. This happens, for example, when conducting mask recovery regression where the recovery ability or the training instances diverge like pattern features are extracted dominantly on pre-training, but semantic features are also required on a downstream task. This paper considers pre-training a model that guarantees a uniformly good performance over the downstream tasks. We call this goal as $\\textit{downstream-task robustness}$. Our method first separates the upstream task into several representative ones and applies a simple minimax loss for pre-training. We then design an efficient algorithm to solve the minim",
    "link": "http://arxiv.org/abs/2306.12070",
    "context": "Title: Task-Robust Pre-Training for Worst-Case Downstream Adaptation. (arXiv:2306.12070v1 [cs.CV])\nAbstract: Pre-training has achieved remarkable success when transferred to downstream tasks. In machine learning, we care about not only the good performance of a model but also its behavior under reasonable shifts of condition. The same philosophy holds when pre-training a foundation model. However, the foundation model may not uniformly behave well for a series of related downstream tasks. This happens, for example, when conducting mask recovery regression where the recovery ability or the training instances diverge like pattern features are extracted dominantly on pre-training, but semantic features are also required on a downstream task. This paper considers pre-training a model that guarantees a uniformly good performance over the downstream tasks. We call this goal as $\\textit{downstream-task robustness}$. Our method first separates the upstream task into several representative ones and applies a simple minimax loss for pre-training. We then design an efficient algorithm to solve the minim",
    "path": "papers/23/06/2306.12070.json",
    "total_tokens": 845,
    "translated_title": "对最坏情况下游任务适应性的任务鲁棒预训练",
    "translated_abstract": "预训练在转移到下游任务时取得了显着的成功。在机器学习中，我们关心模型不仅具有良好的性能，而且在合理的条件变化下的行为。当预训练基础模型时，同样的哲学也适用。然而，基础模型可能并不会在一系列相关下游任务中均匀地表现良好。本文考虑预训练一个模型，保证其在下游任务中具有均匀良好的性能，我们称此目标为下游任务鲁棒性。我们的方法首先将上游任务分成几个代表性任务，并应用简单的minimax loss 进行预训练，然后设计了一个高效的算法来解决极小极大问题，并表明我们的方法优于先前的基线。",
    "tldr": "本文提出了一种任务鲁棒的预训练方法，将上游任务分成几个代表性任务并应用极小极大损失进行预训练，以保证模型能够在下游任务中具有均匀良好的性能。",
    "en_tdlr": "This paper proposes a task-robust pre-training method by separating the upstream task into several representative ones and applying minimax loss for pre-training, to guarantee the model's uniformly good performance in downstream tasks."
}