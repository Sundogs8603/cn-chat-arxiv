{
    "title": "CBBQ: A Chinese Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models. (arXiv:2306.16244v1 [cs.CL])",
    "abstract": "Holistically measuring societal biases of large language models is crucial for detecting and reducing ethical risks in highly capable AI models. In this work, we present a Chinese Bias Benchmark dataset that consists of over 100K questions jointly constructed by human experts and generative language models, covering stereotypes and societal biases in 14 social dimensions related to Chinese culture and values. The curation process contains 4 essential steps: bias identification via extensive literature review, ambiguous context generation, AI-assisted disambiguous context generation, snd manual review \\& recomposition. The testing instances in the dataset are automatically derived from 3K+ high-quality templates manually authored with stringent quality control. The dataset exhibits wide coverage and high diversity. Extensive experiments demonstrate the effectiveness of the dataset in detecting model bias, with all 10 publicly available Chinese large language models exhibiting strong bia",
    "link": "http://arxiv.org/abs/2306.16244",
    "context": "Title: CBBQ: A Chinese Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models. (arXiv:2306.16244v1 [cs.CL])\nAbstract: Holistically measuring societal biases of large language models is crucial for detecting and reducing ethical risks in highly capable AI models. In this work, we present a Chinese Bias Benchmark dataset that consists of over 100K questions jointly constructed by human experts and generative language models, covering stereotypes and societal biases in 14 social dimensions related to Chinese culture and values. The curation process contains 4 essential steps: bias identification via extensive literature review, ambiguous context generation, AI-assisted disambiguous context generation, snd manual review \\& recomposition. The testing instances in the dataset are automatically derived from 3K+ high-quality templates manually authored with stringent quality control. The dataset exhibits wide coverage and high diversity. Extensive experiments demonstrate the effectiveness of the dataset in detecting model bias, with all 10 publicly available Chinese large language models exhibiting strong bia",
    "path": "papers/23/06/2306.16244.json",
    "total_tokens": 950,
    "translated_title": "CBBQ: 通过人工智能与人类协同合作为大型语言模型构建的中文偏差基准数据集",
    "translated_abstract": "全面衡量大型语言模型的社会偏见对于检测和降低高能力人工智能模型的道德风险至关重要。在这项工作中，我们介绍了一个由人类专家和生成式语言模型共同构建的中文偏差基准数据集，包括与中国文化和价值观相关的14个社会维度中的刻板印象和社会偏见。在数据集的整理过程中，包括4个关键步骤：通过广泛的文献评论识别偏见，生成模糊的上下文，通过人工智能辅助消除模糊的上下文，以及手动审查和重组。数据集中的测试实例是从3000多个经过严格质量控制的高质量模板手动提取的。数据集具有广泛的覆盖范围和高度的多样性。广泛的实验证明了该数据集在检测模型偏见方面的有效性，10个公开可用的中文大型语言模型均表现出明显的偏见。",
    "tldr": "通过人工智能与人类协同合作构建的CBBQ中文偏差基准数据集，全面衡量了与中国文化和价值观相关的14个社会维度中的刻板印象和社会偏见，对于检测模型偏见具有广泛的覆盖范围和高度的多样性。"
}