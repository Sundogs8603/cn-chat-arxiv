{
    "title": "Understanding In-Context Learning via Supportive Pretraining Data. (arXiv:2306.15091v1 [cs.CL])",
    "abstract": "In-context learning (ICL) improves language models' performance on a variety of NLP tasks by simply demonstrating a handful of examples at inference time. It is not well understood why ICL ability emerges, as the model has never been specifically trained on such demonstrations. Unlike prior work that explores implicit mechanisms behind ICL, we study ICL via investigating the pretraining data. Specifically, we first adapt an iterative, gradient-based approach to find a small subset of pretraining data that supports ICL. We observe that a continued pretraining on this small subset significantly improves the model's ICL ability, by up to 18%. We then compare the supportive subset constrastively with random subsets of pretraining data and discover: (1) The supportive pretraining data to ICL do not have a higher domain relevance to downstream tasks. (2) The supportive pretraining data have a higher mass of rarely occurring, long-tail tokens. (3) The supportive pretraining data are challengi",
    "link": "http://arxiv.org/abs/2306.15091",
    "context": "Title: Understanding In-Context Learning via Supportive Pretraining Data. (arXiv:2306.15091v1 [cs.CL])\nAbstract: In-context learning (ICL) improves language models' performance on a variety of NLP tasks by simply demonstrating a handful of examples at inference time. It is not well understood why ICL ability emerges, as the model has never been specifically trained on such demonstrations. Unlike prior work that explores implicit mechanisms behind ICL, we study ICL via investigating the pretraining data. Specifically, we first adapt an iterative, gradient-based approach to find a small subset of pretraining data that supports ICL. We observe that a continued pretraining on this small subset significantly improves the model's ICL ability, by up to 18%. We then compare the supportive subset constrastively with random subsets of pretraining data and discover: (1) The supportive pretraining data to ICL do not have a higher domain relevance to downstream tasks. (2) The supportive pretraining data have a higher mass of rarely occurring, long-tail tokens. (3) The supportive pretraining data are challengi",
    "path": "papers/23/06/2306.15091.json",
    "total_tokens": 907,
    "translated_title": "通过支持性预训练数据理解上下文学习",
    "translated_abstract": "上下文学习（ICL）通过在推理时简单展示少量示例，提高语言模型在各种NLP任务上的性能。尽管模型从未被专门训练过这样的示例，ICL的能力如何出现仍不为人所知。与探索ICL背后的隐式机制的先前工作不同，我们通过研究预训练数据来研究ICL。具体而言，我们首先采用迭代的基于梯度的方法，找到一小部分支持ICL的预训练数据。我们观察到，在这个小的子集上持续预训练可以显著提高模型的ICL能力，最多可以提高18%。然后，我们将支持性子集与随机子集进行对比，发现：（1）支持性预训练数据与下游任务的域相关性并不高。（2）支持性预训练数据中有更多罕见且长尾的词。（3）支持性预训练数据具有挑战性。",
    "tldr": "通过研究预训练数据，我们发现支持性预训练数据中的罕见且长尾的词对于上下文学习非常重要。这些发现有助于提高语言模型在各种NLP任务中的性能。"
}