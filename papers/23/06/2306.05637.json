{
    "title": "On the Importance of Feature Decorrelation for Unsupervised Representation Learning in Reinforcement Learning. (arXiv:2306.05637v1 [cs.LG])",
    "abstract": "Recently, unsupervised representation learning (URL) has improved the sample efficiency of Reinforcement Learning (RL) by pretraining a model from a large unlabeled dataset. The underlying principle of these methods is to learn temporally predictive representations by predicting future states in the latent space. However, an important challenge of this approach is the representational collapse, where the subspace of the latent representations collapses into a low-dimensional manifold. To address this issue, we propose a novel URL framework that causally predicts future states while increasing the dimension of the latent manifold by decorrelating the features in the latent space. Through extensive empirical studies, we demonstrate that our framework effectively learns predictive representations without collapse, which significantly improves the sample efficiency of state-of-the-art URL methods on the Atari 100k benchmark. The code is available at https://github.com/dojeon-ai/SimTPR.",
    "link": "http://arxiv.org/abs/2306.05637",
    "context": "Title: On the Importance of Feature Decorrelation for Unsupervised Representation Learning in Reinforcement Learning. (arXiv:2306.05637v1 [cs.LG])\nAbstract: Recently, unsupervised representation learning (URL) has improved the sample efficiency of Reinforcement Learning (RL) by pretraining a model from a large unlabeled dataset. The underlying principle of these methods is to learn temporally predictive representations by predicting future states in the latent space. However, an important challenge of this approach is the representational collapse, where the subspace of the latent representations collapses into a low-dimensional manifold. To address this issue, we propose a novel URL framework that causally predicts future states while increasing the dimension of the latent manifold by decorrelating the features in the latent space. Through extensive empirical studies, we demonstrate that our framework effectively learns predictive representations without collapse, which significantly improves the sample efficiency of state-of-the-art URL methods on the Atari 100k benchmark. The code is available at https://github.com/dojeon-ai/SimTPR.",
    "path": "papers/23/06/2306.05637.json",
    "total_tokens": 924,
    "translated_title": "关于无监督表示学习在强化学习中特征去相关性的重要性",
    "translated_abstract": "最近，无监督表示学习（URL）通过从大型无标签数据集中预训练模型来提高强化学习（RL）的样本效率。这些方法的基本原则是通过在潜空间中预测未来状态来学习时间上的预测表示。然而，这种方法的一个重要挑战是表示崩溃，其中潜表征的子空间崩溃为低维流形。为了解决这个问题，我们提出了一种新的URL框架，通过去相关潜空间中的特征来因果性地预测未来状态并增加潜空间的维度。通过广泛的经验研究，我们证明了我们的框架有效地学习预测表示而不会崩溃，从而显着提高了基于Atari 100k基准的最新URL方法的样本效率。代码可在 https://github.com/dojeon-ai/SimTPR 上获得。",
    "tldr": "本研究提出了一种新的URL框架，通过去相关潜空间中的特征来因果性地预测未来状态并增加潜空间的维度，从而有效地学习预测表示而不会崩溃，显着提高了基于Atari 100k基准的最新URL方法的样本效率。",
    "en_tdlr": "This paper proposes a novel URL framework that causally predicts future states while increasing the dimension of the latent manifold by decorrelating the features in the latent space, effectively learning predictive representations without collapse, which significantly improves the sample efficiency of state-of-the-art URL methods on the Atari 100k benchmark."
}