{
    "title": "CLIPSonic: Text-to-Audio Synthesis with Unlabeled Videos and Pretrained Language-Vision Models. (arXiv:2306.09635v1 [cs.SD])",
    "abstract": "Recent work has studied text-to-audio synthesis using large amounts of paired text-audio data. However, audio recordings with high-quality text annotations can be difficult to acquire. In this work, we approach text-to-audio synthesis using unlabeled videos and pretrained language-vision models. We propose to learn the desired text-audio correspondence by leveraging the visual modality as a bridge. We train a conditional diffusion model to generate the audio track of a video, given a video frame encoded by a pretrained contrastive language-image pretraining (CLIP) model. At test time, we first explore performing a zero-shot modality transfer and condition the diffusion model with a CLIP-encoded text query. However, we observe a noticeable performance drop with respect to image queries. To close this gap, we further adopt a pretrained diffusion prior model to generate a CLIP image embedding given a CLIP text embedding. Our results show the effectiveness of the proposed method, and that ",
    "link": "http://arxiv.org/abs/2306.09635",
    "context": "Title: CLIPSonic: Text-to-Audio Synthesis with Unlabeled Videos and Pretrained Language-Vision Models. (arXiv:2306.09635v1 [cs.SD])\nAbstract: Recent work has studied text-to-audio synthesis using large amounts of paired text-audio data. However, audio recordings with high-quality text annotations can be difficult to acquire. In this work, we approach text-to-audio synthesis using unlabeled videos and pretrained language-vision models. We propose to learn the desired text-audio correspondence by leveraging the visual modality as a bridge. We train a conditional diffusion model to generate the audio track of a video, given a video frame encoded by a pretrained contrastive language-image pretraining (CLIP) model. At test time, we first explore performing a zero-shot modality transfer and condition the diffusion model with a CLIP-encoded text query. However, we observe a noticeable performance drop with respect to image queries. To close this gap, we further adopt a pretrained diffusion prior model to generate a CLIP image embedding given a CLIP text embedding. Our results show the effectiveness of the proposed method, and that ",
    "path": "papers/23/06/2306.09635.json",
    "total_tokens": 1073,
    "translated_title": "CLIPSonic：使用未标注视频和预训练语言视觉模型的文本合成音频",
    "translated_abstract": "最近的研究探讨了使用大量成对的文本和音频数据进行文本合成音频。 然而，带有高质量文本注释的音频录音可能难以获取。 在本文中，我们使用未标记的视频和预训练语言视觉模型来进行文本合成音频的研究。 我们建议利用视觉模态作为桥梁来学习所需的文本-音频对应关系。 我们训练一个有条件的扩散模型，以生成视频的音频轨道，给定由预训练的对比语言图像预训练（CLIP）模型编码的视频帧。 在测试时间，我们首先探索执行零样本模态转换，并使用CLIP编码的文本查询条件扩散模型。 但是，我们观察到与图像查询相比存在明显的性能下降。 为了弥合这一差距，我们进一步采用预训练的扩散先验模型，生成给定CLIP文本嵌入的CLIP图像嵌入。 我们的结果显示了所提出方法的有效性，并且",
    "tldr": "本文提出了一种使用未标注视频和预训练语言视觉模型进行文本合成音频的方法。通过利用视觉模态作为桥梁来学习文本-音频对应关系，提出了有条件的扩散模型，生成视频的音轨。使用CLIP图像查询条件进行零样本模态转换。并采用预训练的扩散先验模型，生成对应于CLIP文本嵌入的CLIP图像嵌入。实验证明了该方法的有效性。",
    "en_tdlr": "This paper proposes a method for text-to-audio synthesis using unlabeled videos and pretrained language-vision models, which leverages the visual modality as a bridge and trains a conditional diffusion model to generate the audio track of a video. The method explores performing zero-shot modality transfer with CLIP-encoded text queries and further adopts a pretrained diffusion prior model to generate a CLIP image embedding given a CLIP text embedding, showing its effectiveness in experiments."
}