{
    "title": "The Double Helix inside the NLP Transformer. (arXiv:2306.13817v1 [cs.AI])",
    "abstract": "We introduce a framework for analyzing various types of information in an NLP Transformer. In this approach, we distinguish four layers of information: positional, syntactic, semantic, and contextual. We also argue that the common practice of adding positional information to semantic embedding is sub-optimal and propose instead a Linear-and-Add approach. Our analysis reveals an autogenetic separation of positional information through the deep layers. We show that the distilled positional components of the embedding vectors follow the path of a helix, both on the encoder side and on the decoder side. We additionally show that on the encoder side, the conceptual dimensions generate Part-of-Speech (PoS) clusters. On the decoder side, we show that a di-gram approach helps to reveal the PoS clusters of the next token. Our approach paves a way to elucidate the processing of information through the deep layers of an NLP Transformer.",
    "link": "http://arxiv.org/abs/2306.13817",
    "context": "Title: The Double Helix inside the NLP Transformer. (arXiv:2306.13817v1 [cs.AI])\nAbstract: We introduce a framework for analyzing various types of information in an NLP Transformer. In this approach, we distinguish four layers of information: positional, syntactic, semantic, and contextual. We also argue that the common practice of adding positional information to semantic embedding is sub-optimal and propose instead a Linear-and-Add approach. Our analysis reveals an autogenetic separation of positional information through the deep layers. We show that the distilled positional components of the embedding vectors follow the path of a helix, both on the encoder side and on the decoder side. We additionally show that on the encoder side, the conceptual dimensions generate Part-of-Speech (PoS) clusters. On the decoder side, we show that a di-gram approach helps to reveal the PoS clusters of the next token. Our approach paves a way to elucidate the processing of information through the deep layers of an NLP Transformer.",
    "path": "papers/23/06/2306.13817.json",
    "total_tokens": 886,
    "translated_title": "NLP Transformer中的双螺旋结构",
    "translated_abstract": "我们介绍了一个NLP Transformer中分析不同信息类型的框架。在这个方法中，我们区分了四层信息：位置、句法、语义和上下文。我们还提出，常见的在语义嵌入中添加位置信息的做法是次优的，提议使用线性加法(Linar-and-Add)的方法。我们的分析揭示了位置信息在深层中的自我分离。我们展示了嵌入向量的位置组成部分遵循螺旋的路径，在编码器侧和解码器侧都是如此。我们另外还展示，编码器侧的概念维度生成了词性(PoS)的聚类。在解码器侧，我们展示使用二元语法的方法有助于揭示下一个标记的词性聚类。我们的方法为阐明通过NLP Transformer的深层信息处理铺平了道路。",
    "tldr": "本文介绍了一个NLP Transformer中分析位置、句法、语义和上下文信息的框架，揭示了位置信息通过螺旋路径在深层中自我分离，并在编码器和解码器侧生成词性聚类。提出了替代在语义嵌入中添加位置信息的方法。",
    "en_tdlr": "This paper introduces a framework for analyzing positional, syntactic, semantic, and contextual information in an NLP Transformer, revealing the autogenetic separation of positional information through a helix pathway in deep layers, and generating Part-of-Speech (PoS) clusters on the encoder side. A Linear-and-Add approach is proposed as a substitute for adding positional information to semantic embedding."
}