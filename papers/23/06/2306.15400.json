{
    "title": "Length Generalization in Arithmetic Transformers. (arXiv:2306.15400v1 [cs.LG])",
    "abstract": "We examine how transformers cope with two challenges: learning basic integer arithmetic, and generalizing to longer sequences than seen during training. We find that relative position embeddings enable length generalization for simple tasks, such as addition: models trained on $5$-digit numbers can perform $15$-digit sums. However, this method fails for multiplication, and we propose train set priming: adding a few ($10$ to $50$) long sequences to the training set. We show that priming allows models trained on $5$-digit $\\times$ $3$-digit multiplications to generalize to $35\\times 3$ examples. We also show that models can be primed for different generalization lengths, and that the priming sample size scales as the logarithm of the training set size. Finally, we discuss potential applications of priming beyond arithmetic.",
    "link": "http://arxiv.org/abs/2306.15400",
    "context": "Title: Length Generalization in Arithmetic Transformers. (arXiv:2306.15400v1 [cs.LG])\nAbstract: We examine how transformers cope with two challenges: learning basic integer arithmetic, and generalizing to longer sequences than seen during training. We find that relative position embeddings enable length generalization for simple tasks, such as addition: models trained on $5$-digit numbers can perform $15$-digit sums. However, this method fails for multiplication, and we propose train set priming: adding a few ($10$ to $50$) long sequences to the training set. We show that priming allows models trained on $5$-digit $\\times$ $3$-digit multiplications to generalize to $35\\times 3$ examples. We also show that models can be primed for different generalization lengths, and that the priming sample size scales as the logarithm of the training set size. Finally, we discuss potential applications of priming beyond arithmetic.",
    "path": "papers/23/06/2306.15400.json",
    "total_tokens": 854,
    "translated_title": "算术Transformer中的长度推广",
    "translated_abstract": "我们研究了Transformer在两个挑战中的表现：学习基本整数运算和推广到训练中未见过的更长序列。我们发现，相对位置嵌入可以使简单任务（如加法）的长度推广：在训练中使用5位数的模型可以执行15位数的求和。然而，这种方法在乘法上失效，我们提出了训练集引导：将几个（10到50个）长序列添加到训练集中。我们展示了引导可以使训练在5位数乘以3位数的模型推广到35乘以3的例子。我们还展示了模型可以针对不同的推广长度进行引导，而引导样本量的缩放与训练集大小的对数成比例。最后，我们讨论了引导在算术之外的潜在应用。",
    "tldr": "本研究通过引入相对位置嵌入和训练集引导方法，提高了Transformer模型在学习整数算术和推广到更长序列上的性能，在简单任务中表现良好；同时展示了在乘法任务上引导方法的有效性，并探讨了引导方法在其他领域的潜在应用。",
    "en_tdlr": "This study improves the performance of Transformer models in learning integer arithmetic and generalizing to longer sequences by introducing relative position embeddings and training set priming. The method shows good results in simple tasks, and the effectiveness of priming is demonstrated in multiplication tasks, with potential applications beyond arithmetic."
}