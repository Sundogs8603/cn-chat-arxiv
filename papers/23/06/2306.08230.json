{
    "title": "Unbiased Learning of Deep Generative Models with Structured Discrete Representations. (arXiv:2306.08230v1 [cs.LG])",
    "abstract": "By composing graphical models with deep learning architectures, we learn generative models with the strengths of both frameworks. The structured variational autoencoder (SVAE) inherits structure and interpretability from graphical models, and flexible likelihoods for high-dimensional data from deep learning, but poses substantial optimization challenges. We propose novel algorithms for learning SVAEs, and are the first to demonstrate the SVAE's ability to handle multimodal uncertainty when data is missing by incorporating discrete latent variables. Our memory-efficient implicit differentiation scheme makes the SVAE tractable to learn via gradient descent, while demonstrating robustness to incomplete optimization. To more rapidly learn accurate graphical model parameters, we derive a method for computing natural gradients without manual derivations, which avoids biases found in prior work. These optimization innovations enable the first comparisons of the SVAE to state-of-the-art time s",
    "link": "http://arxiv.org/abs/2306.08230",
    "context": "Title: Unbiased Learning of Deep Generative Models with Structured Discrete Representations. (arXiv:2306.08230v1 [cs.LG])\nAbstract: By composing graphical models with deep learning architectures, we learn generative models with the strengths of both frameworks. The structured variational autoencoder (SVAE) inherits structure and interpretability from graphical models, and flexible likelihoods for high-dimensional data from deep learning, but poses substantial optimization challenges. We propose novel algorithms for learning SVAEs, and are the first to demonstrate the SVAE's ability to handle multimodal uncertainty when data is missing by incorporating discrete latent variables. Our memory-efficient implicit differentiation scheme makes the SVAE tractable to learn via gradient descent, while demonstrating robustness to incomplete optimization. To more rapidly learn accurate graphical model parameters, we derive a method for computing natural gradients without manual derivations, which avoids biases found in prior work. These optimization innovations enable the first comparisons of the SVAE to state-of-the-art time s",
    "path": "papers/23/06/2306.08230.json",
    "total_tokens": 1014,
    "translated_title": "结构化离散表示的深度生成模型的无偏学习",
    "translated_abstract": "通过将图形模型与深度学习架构组合，我们学习具有两种框架优势的生成模型。 结构化变分自编码器（SVAE）从图形模型继承结构和可解释性，从深度学习中继承了适用于高维数据的灵活似然，但是会带来相当大的优化挑战。 我们提出了学习SVAE的新算法，并且首次证明了SVAE在含有缺失数据且包含离散潜变量时处理多模态不确定性的能力。我们的内存高效隐式微分方案使得SVAE可以通过梯度下降来学习，并且证明了鲁棒性。为了更快地学习准确的图形模型参数，我们推导了一种计算自然梯度的方法，而不需要手动进行导出，从而避免了先前工作中发现的偏差。这些优化创新使得首次能够将SVAE与最先进的时间序列模型进行比较。",
    "tldr": "该论文提出了一种名为结构化变分自编码器的深度生成模型，它通过图像模型的结构和可解释性以及深度学习的适用于高维数据的灵活似然，结合两种框架的优势。同时，该论文还提出了一种学习SVAE的新算法，与此同时，推导出了一种计算自然梯度的方法，这些优化创新使得SVAE首次能与最先进的时间序列模型进行比较。",
    "en_tdlr": "This paper proposes a deep generative model called Structured Variational Autoencoder (SVAE), which combines the strengths of a graphical model's structure and interpretability with deep learning's flexible likelihood for high-dimensional data. The paper also presents new algorithms for learning SVAEs and a method for calculating natural gradients, allowing for the first comparison of SVAEs to state-of-the-art time series models."
}