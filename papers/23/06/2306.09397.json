{
    "title": "Non-Asymptotic Performance of Social Machine Learning Under Limited Data. (arXiv:2306.09397v1 [cs.LG])",
    "abstract": "This paper studies the probability of error associated with the social machine learning framework, which involves an independent training phase followed by a cooperative decision-making phase over a graph. This framework addresses the problem of classifying a stream of unlabeled data in a distributed manner. We consider two kinds of classification tasks with limited observations in the prediction phase, namely, the statistical classification task and the single-sample classification task. For each task, we describe the distributed learning rule and analyze the probability of error accordingly. To do so, we first introduce a stronger consistent training condition that involves the margin distributions generated by the trained classifiers. Based on this condition, we derive an upper bound on the probability of error for both tasks, which depends on the statistical properties of the data and the combination policy used to combine the distributed classifiers. For the statistical classifica",
    "link": "http://arxiv.org/abs/2306.09397",
    "context": "Title: Non-Asymptotic Performance of Social Machine Learning Under Limited Data. (arXiv:2306.09397v1 [cs.LG])\nAbstract: This paper studies the probability of error associated with the social machine learning framework, which involves an independent training phase followed by a cooperative decision-making phase over a graph. This framework addresses the problem of classifying a stream of unlabeled data in a distributed manner. We consider two kinds of classification tasks with limited observations in the prediction phase, namely, the statistical classification task and the single-sample classification task. For each task, we describe the distributed learning rule and analyze the probability of error accordingly. To do so, we first introduce a stronger consistent training condition that involves the margin distributions generated by the trained classifiers. Based on this condition, we derive an upper bound on the probability of error for both tasks, which depends on the statistical properties of the data and the combination policy used to combine the distributed classifiers. For the statistical classifica",
    "path": "papers/23/06/2306.09397.json",
    "total_tokens": 818,
    "translated_title": "限制数据下社交机器学习的非渐进性能研究",
    "translated_abstract": "本文研究了社交机器学习框架中与错误概率相关的问题，该框架涉及独立的训练阶段，随后在图上进行协作决策阶段。该框架解决了分布式分类未标记数据的问题。我们考虑了两种分类任务，分别为统计分类和单样本分类，同时考虑了预测阶段数据观测受限的情况。对于每个任务，我们描述了分布式学习规则，并相应分析了错误概率。为此，我们首先引入了一个更强的一致性训练条件，该条件涉及训练分类器生成的边缘分布。基于此条件，我们为两种任务推导出了概率误差的上界，该上界取决于数据的统计特性和用于组合分布式分类器的组合策略。",
    "tldr": "本文研究了限制数据下社交机器学习的概率误差问题，并提出了一种更强的一致性训练条件，推导出了两种任务的概率误差上界。",
    "en_tdlr": "This paper studies the probability of error for social machine learning under limited data, and proposes a stronger consistent training condition to derive an upper bound on the probability of error for two classification tasks."
}