{
    "title": "Investigating Pre-trained Language Models on Cross-Domain Datasets, a Step Closer to General AI. (arXiv:2306.12205v1 [cs.CL])",
    "abstract": "Pre-trained language models have recently emerged as a powerful tool for fine-tuning a variety of language tasks. Ideally, when models are pre-trained on large amount of data, they are expected to gain implicit knowledge. In this paper, we investigate the ability of pre-trained language models to generalize to different non-language tasks. In particular, we test them on tasks from different domains such as computer vision, reasoning on hierarchical data, and protein fold prediction. The four pre-trained models that we used, T5, BART, BERT, and GPT-2 achieve outstanding results. They all have similar performance and they outperform transformers that are trained from scratch by a large margin. For instance, pre-trained language models perform better on the Listops dataset, with an average accuracy of 58.7\\%, compared to transformers trained from scratch, which have an average accuracy of 29.0\\%. The significant improvement demonstrated across three types of datasets suggests that pre-tra",
    "link": "http://arxiv.org/abs/2306.12205",
    "context": "Title: Investigating Pre-trained Language Models on Cross-Domain Datasets, a Step Closer to General AI. (arXiv:2306.12205v1 [cs.CL])\nAbstract: Pre-trained language models have recently emerged as a powerful tool for fine-tuning a variety of language tasks. Ideally, when models are pre-trained on large amount of data, they are expected to gain implicit knowledge. In this paper, we investigate the ability of pre-trained language models to generalize to different non-language tasks. In particular, we test them on tasks from different domains such as computer vision, reasoning on hierarchical data, and protein fold prediction. The four pre-trained models that we used, T5, BART, BERT, and GPT-2 achieve outstanding results. They all have similar performance and they outperform transformers that are trained from scratch by a large margin. For instance, pre-trained language models perform better on the Listops dataset, with an average accuracy of 58.7\\%, compared to transformers trained from scratch, which have an average accuracy of 29.0\\%. The significant improvement demonstrated across three types of datasets suggests that pre-tra",
    "path": "papers/23/06/2306.12205.json",
    "total_tokens": 947,
    "translated_title": "探究预训练语言模型在跨领域数据集上的表现，迈向通用人工智能的一步。",
    "translated_abstract": "最近，预训练语言模型已成为微调各种语言任务的强有力工具。理想情况下，当模型在大量数据上进行预训练时，希望其能获得隐式知识。本文研究了预训练语言模型在泛化到不同非语言任务的能力。特别是，我们测试了来自不同领域的任务，如计算机视觉、分层数据推理和蛋白质折叠预测。我们使用的四个预训练模型，T5、BART、BERT 和 GPT-2 都取得了优异的成绩。它们的表现很相似，且它们的表现比从头开始训练的 transformers 要好得多。例如，在 Listops 数据集上，预训练语言模型的平均准确率为 58.7％，而从头开始训练的 transformers 的平均准确率为 29.0％。跨三种类型的数据集所展示的显著改进表明，预训练语言模型可能是实现通用人工智能的有前途的一步。",
    "tldr": "本文研究了预训练语言模型在跨领域任务中的表现，结果表明预训练模型可以是实现通用人工智能的重要一步。",
    "en_tdlr": "This paper investigates the performance of pre-trained language models on cross-domain tasks, and the results suggest that pre-trained models can be a significant step towards achieving general artificial intelligence."
}