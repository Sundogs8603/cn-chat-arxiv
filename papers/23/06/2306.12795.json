{
    "title": "Learning Unseen Modality Interaction. (arXiv:2306.12795v2 [cs.CV] CROSS LISTED)",
    "abstract": "Multimodal learning assumes all modality combinations of interest are available during training to learn cross-modal correspondences.In this paper, we challenge this modality-complete assumption for multimodal learning and instead strive for generalization to unseen modality combinations during inference. We pose the problem of unseen modality interaction and introduce a first solution. It exploits a module that projects the multidimensional features of different modalities into a common space with rich information preserved. This allows the information to be accumulated with a simple summation operation across available modalities. To reduce overfitting to less discriminative modality combinations during training, we further improve the model learning with pseudo-supervision indicating the reliability of a modality's prediction. We demonstrate that our approach is effective for diverse tasks and modalities by evaluating it for multimodal video classification, robot state regression, a",
    "link": "http://arxiv.org/abs/2306.12795",
    "context": "Title: Learning Unseen Modality Interaction. (arXiv:2306.12795v2 [cs.CV] CROSS LISTED)\nAbstract: Multimodal learning assumes all modality combinations of interest are available during training to learn cross-modal correspondences.In this paper, we challenge this modality-complete assumption for multimodal learning and instead strive for generalization to unseen modality combinations during inference. We pose the problem of unseen modality interaction and introduce a first solution. It exploits a module that projects the multidimensional features of different modalities into a common space with rich information preserved. This allows the information to be accumulated with a simple summation operation across available modalities. To reduce overfitting to less discriminative modality combinations during training, we further improve the model learning with pseudo-supervision indicating the reliability of a modality's prediction. We demonstrate that our approach is effective for diverse tasks and modalities by evaluating it for multimodal video classification, robot state regression, a",
    "path": "papers/23/06/2306.12795.json",
    "total_tokens": 877,
    "translated_title": "学习未见过的模态交互",
    "translated_abstract": "多模态学习假定在训练期间可以使用所有感兴趣的模态组合来学习跨模态对应关系。本文对多模态学习中的模态完整性做出了挑战，而是在推理过程中努力实现对未见过的模态组合的泛化。我们提出了未见过的模态交互问题，并介绍了第一个解决方案。它利用一个模块将不同模态的多维特征投影到一个具有多样信息的公共空间中，从而可以通过简单的求和操作对可用的模态进行信息累积。为了减少在训练过程中对较少辨别力的模态组合的过拟合，我们进一步改进了模型学习，通过伪监督指示模态预测的可靠性。我们通过对多模态视频分类、机器人状态回归等进行评估，证明了我们的方法对于不同任务和模态是有效的。",
    "tldr": "本文提出了一个解决多模态学习中未见过的模态组合的问题的方法。该方法利用一个模块将不同模态的特征投影到一个共享的空间中，并通过伪监督来减少过拟合。实验证明该方法在多个任务和模态上都是有效的。"
}