{
    "title": "Conformalizing Machine Translation Evaluation. (arXiv:2306.06221v1 [cs.CL])",
    "abstract": "Several uncertainty estimation methods have been recently proposed for machine translation evaluation. While these methods can provide a useful indication of when not to trust model predictions, we show in this paper that the majority of them tend to underestimate model uncertainty, and as a result they often produce misleading confidence intervals that do not cover the ground truth. We propose as an alternative the use of conformal prediction, a distribution-free method to obtain confidence intervals with a theoretically established guarantee on coverage. First, we demonstrate that split conformal prediction can ``correct'' the confidence intervals of previous methods to yield a desired coverage level. Then, we highlight biases in estimated confidence intervals, both in terms of the translation language pairs and the quality of translations. We apply conditional conformal prediction techniques to obtain calibration subsets for each data subgroup, leading to equalized coverage.",
    "link": "http://arxiv.org/abs/2306.06221",
    "context": "Title: Conformalizing Machine Translation Evaluation. (arXiv:2306.06221v1 [cs.CL])\nAbstract: Several uncertainty estimation methods have been recently proposed for machine translation evaluation. While these methods can provide a useful indication of when not to trust model predictions, we show in this paper that the majority of them tend to underestimate model uncertainty, and as a result they often produce misleading confidence intervals that do not cover the ground truth. We propose as an alternative the use of conformal prediction, a distribution-free method to obtain confidence intervals with a theoretically established guarantee on coverage. First, we demonstrate that split conformal prediction can ``correct'' the confidence intervals of previous methods to yield a desired coverage level. Then, we highlight biases in estimated confidence intervals, both in terms of the translation language pairs and the quality of translations. We apply conditional conformal prediction techniques to obtain calibration subsets for each data subgroup, leading to equalized coverage.",
    "path": "papers/23/06/2306.06221.json",
    "total_tokens": 895,
    "translated_title": "规范化机器翻译评估",
    "translated_abstract": "最近提出了多种不确定性估计方法用于机器翻译评估。虽然这些方法可以提供一个有用的指示，来判断何时不能相信模型预测，但本文表明，这些方法大部分倾向于低估模型的不确定性，结果往往产生具有误导性的置信区间，而这些置信区间未能覆盖真实值。我们提出用规范化预测作为替代方法，这是一种无分布方法，用于获得具有理论保证覆盖率的置信区间。首先，我们证明分裂规范化预测可以“修正”之前方法的置信区间，以产生所需的覆盖率。然后，我们突出显示了估计置信区间的偏差，无论是在翻译语言对方面还是在翻译质量方面。我们应用条件规范化预测技术，为每个数据子组获得校准子集，从而实现平等覆盖。",
    "tldr": "本文提出了一种无分布方法——规范化预测，用于机器翻译评估置信区间的计算，并证明其具有理论保证覆盖率，可以纠正其他方法的误差。应用条件规范化预测技术，获得平等覆盖的校准子集。",
    "en_tdlr": "This paper proposes a distribution-free method, conformal prediction, for calculating confidence intervals in machine translation evaluation, which has a theoretically established guarantee on coverage and can correct previous methods' errors. Conditional conformal prediction techniques are applied to obtain calibration subsets for each data subgroup, allowing for equalized coverage."
}