{
    "title": "Waypoint Transformer: Reinforcement Learning via Supervised Learning with Intermediate Targets. (arXiv:2306.14069v1 [cs.LG])",
    "abstract": "Despite the recent advancements in offline reinforcement learning via supervised learning (RvS) and the success of the decision transformer (DT) architecture in various domains, DTs have fallen short in several challenging benchmarks. The root cause of this underperformance lies in their inability to seamlessly connect segments of suboptimal trajectories. To overcome this limitation, we present a novel approach to enhance RvS methods by integrating intermediate targets. We introduce the Waypoint Transformer (WT), using an architecture that builds upon the DT framework and conditioned on automatically-generated waypoints. The results show a significant increase in the final return compared to existing RvS methods, with performance on par or greater than existing state-of-the-art temporal difference learning-based methods. Additionally, the performance and stability improvements are largest in the most challenging environments and data configurations, including AntMaze Large Play/Diverse",
    "link": "http://arxiv.org/abs/2306.14069",
    "context": "Title: Waypoint Transformer: Reinforcement Learning via Supervised Learning with Intermediate Targets. (arXiv:2306.14069v1 [cs.LG])\nAbstract: Despite the recent advancements in offline reinforcement learning via supervised learning (RvS) and the success of the decision transformer (DT) architecture in various domains, DTs have fallen short in several challenging benchmarks. The root cause of this underperformance lies in their inability to seamlessly connect segments of suboptimal trajectories. To overcome this limitation, we present a novel approach to enhance RvS methods by integrating intermediate targets. We introduce the Waypoint Transformer (WT), using an architecture that builds upon the DT framework and conditioned on automatically-generated waypoints. The results show a significant increase in the final return compared to existing RvS methods, with performance on par or greater than existing state-of-the-art temporal difference learning-based methods. Additionally, the performance and stability improvements are largest in the most challenging environments and data configurations, including AntMaze Large Play/Diverse",
    "path": "papers/23/06/2306.14069.json",
    "total_tokens": 877,
    "translated_title": "Waypoint Transformer: 通过中间目标的监督学习进行强化学习",
    "translated_abstract": "尽管最近通过监督学习进行的离线强化学习取得了很多进展，以决策转换器（DT）架构在各个领域的成功而言，但在几个具有挑战性的基准测试中，DT还是表现不佳。这一低性能的根本原因在于它们无法无缝连接亚优化轨迹的片段。为了克服这个限制，我们提出了一种增强RvS方法的新方法，即通过整合中间目标来实现。我们引入Waypoint Transformer（WT），使用一种基于DT框架的架构，并且是通过自动生成的路径点进行条件化的。结果表明，与现有的RvS方法相比，最终回报显著增加，并且在性能上与现有的基于时间差分学习的最新方法相当或更优。此外，性能和稳定性的改进最大的是在最具挑战性的环境和数据配置中，包括AntMaze Large Play/Diverse。",
    "tldr": "Waypoint Transformer提出了一种改进RL的新方法，通过整合中间目标来实现，极大地提高了性能和稳定性，尤其在最具挑战性的环境和数据配置中表现得更加优秀。",
    "en_tdlr": "Waypoint Transformer proposed a novel approach to improve RL by integrating intermediate targets, which significantly improves performance and stability, especially in the most challenging environments and data configurations, and outperforms existing state-of-the-art temporal difference learning-based methods."
}