{
    "title": "Training Transformers with 4-bit Integers. (arXiv:2306.11987v1 [cs.LG])",
    "abstract": "Quantizing the activation, weight, and gradient to 4-bit is promising to accelerate neural network training. However, existing 4-bit training methods require custom numerical formats which are not supported by contemporary hardware. In this work, we propose a training method for transformers with all matrix multiplications implemented with the INT4 arithmetic. Training with an ultra-low INT4 precision is challenging. To achieve this, we carefully analyze the specific structures of activation and gradients in transformers to propose dedicated quantizers for them. For forward propagation, we identify the challenge of outliers and propose a Hadamard quantizer to suppress the outliers. For backpropagation, we leverage the structural sparsity of gradients by proposing bit splitting and leverage score sampling techniques to quantize gradients accurately. Our algorithm achieves competitive accuracy on a wide range of tasks including natural language understanding, machine translation, and ima",
    "link": "http://arxiv.org/abs/2306.11987",
    "context": "Title: Training Transformers with 4-bit Integers. (arXiv:2306.11987v1 [cs.LG])\nAbstract: Quantizing the activation, weight, and gradient to 4-bit is promising to accelerate neural network training. However, existing 4-bit training methods require custom numerical formats which are not supported by contemporary hardware. In this work, we propose a training method for transformers with all matrix multiplications implemented with the INT4 arithmetic. Training with an ultra-low INT4 precision is challenging. To achieve this, we carefully analyze the specific structures of activation and gradients in transformers to propose dedicated quantizers for them. For forward propagation, we identify the challenge of outliers and propose a Hadamard quantizer to suppress the outliers. For backpropagation, we leverage the structural sparsity of gradients by proposing bit splitting and leverage score sampling techniques to quantize gradients accurately. Our algorithm achieves competitive accuracy on a wide range of tasks including natural language understanding, machine translation, and ima",
    "path": "papers/23/06/2306.11987.json",
    "total_tokens": 891,
    "translated_title": "使用4位整数训练Transformer",
    "translated_abstract": "将激活、权重和梯度量化为4位有望加速神经网络训练。然而，现有的4位训练方法需要自定义数值格式，而这些格式不受当代硬件支持。本文提出一种使用INT4算术实现所有矩阵乘法的Transformer训练方法。以极低的INT4精度训练是一项具有挑战性的任务。为了实现这一点，我们仔细分析了Transformer中激活和梯度的特定结构，为它们提出了专用量化器。对于前向传播，我们识别了离群值的挑战，并提出了一种哈达玛量化器来抑制离群值。对于反向传播，我们利用梯度的结构稀疏性，提出位分裂和杠杆得分采样技术来精确量化梯度。我们的算法在包括自然语言理解、机器翻译和图像分类在内的一系列任务上取得了竞争性的精度。",
    "tldr": "本文提出了一种使用INT4算术训练Transformer的方法，并细致地分析了转换器中激活和梯度的特定结构，为它们提出了专用的量化器。算法在多个任务中达到了竞争性的准确性。",
    "en_tdlr": "This paper proposes a method of training Transformers using INT4 arithmetic, and carefully analyze the specific structures of activation and gradients in transformers to propose dedicated quantizers for them. The algorithm achieves competitive accuracy on a range of tasks."
}