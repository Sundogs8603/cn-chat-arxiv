{
    "title": "Stop Pre-Training: Adapt Visual-Language Models to Unseen Languages. (arXiv:2306.16774v1 [cs.CL])",
    "abstract": "Vision-Language Pre-training (VLP) has advanced the performance of many vision-language tasks, such as image-text retrieval, visual entailment, and visual reasoning. The pre-training mostly utilizes lexical databases and image queries in English. Previous work has demonstrated that the pre-training in English does not transfer well to other languages in a zero-shot setting. However, multilingual pre-trained language models (MPLM) have excelled at a variety of single-modal language tasks. In this paper, we propose a simple yet efficient approach to adapt VLP to unseen languages using MPLM. We utilize a cross-lingual contextualized token embeddings alignment approach to train text encoders for non-English languages. Our approach does not require image input and primarily uses machine translation, eliminating the need for target language data. Our evaluation across three distinct tasks (image-text retrieval, visual entailment, and natural language visual reasoning) demonstrates that this ",
    "link": "http://arxiv.org/abs/2306.16774",
    "context": "Title: Stop Pre-Training: Adapt Visual-Language Models to Unseen Languages. (arXiv:2306.16774v1 [cs.CL])\nAbstract: Vision-Language Pre-training (VLP) has advanced the performance of many vision-language tasks, such as image-text retrieval, visual entailment, and visual reasoning. The pre-training mostly utilizes lexical databases and image queries in English. Previous work has demonstrated that the pre-training in English does not transfer well to other languages in a zero-shot setting. However, multilingual pre-trained language models (MPLM) have excelled at a variety of single-modal language tasks. In this paper, we propose a simple yet efficient approach to adapt VLP to unseen languages using MPLM. We utilize a cross-lingual contextualized token embeddings alignment approach to train text encoders for non-English languages. Our approach does not require image input and primarily uses machine translation, eliminating the need for target language data. Our evaluation across three distinct tasks (image-text retrieval, visual entailment, and natural language visual reasoning) demonstrates that this ",
    "path": "papers/23/06/2306.16774.json",
    "total_tokens": 960,
    "translated_title": "停止预训练：将视觉语言模型适应于未见过的语言",
    "translated_abstract": "视觉语言预训练(VLP)已经提高了许多视觉语言任务的性能，比如图像文本检索、视觉蕴涵和视觉推理。预训练主要利用英语的词汇数据库和图像查询。先前的研究表明，在零射未见语言中，英语的预训练效果不佳。然而，多语言预训练语言模型(MPLM)在各种单模态语言任务中表现出色。本文提出了一种简单而有效的方法，利用MPLM将VLP适应于未见语言。我们利用跨语言上下文化的词元嵌入对齐方法来训练非英语语言的文本编码器。我们的方法不需要图像输入，主要使用机器翻译，消除了对目标语言数据的需求。我们在三个不同的任务(图像-文本检索、视觉蕴涵和自然语言视觉推理)上进行了评估，结果表明此方法",
    "tldr": "这项研究提出了一种利用多语言预训练语言模型将视觉语言模型适应于未见语言的方法，通过跨语言的上下文化词元嵌入对齐技术，无需图像输入和目标语言数据，取得了在图像-文本检索、视觉蕴涵和自然语言视觉推理等任务上的良好结果。",
    "en_tdlr": "This research proposes a method to adapt visual-language models to unseen languages using multilingual pre-trained language models, achieving good results in tasks such as image-text retrieval, visual entailment, and natural language visual reasoning through cross-lingual contextualized token embeddings alignment, without requiring image input or target language data."
}