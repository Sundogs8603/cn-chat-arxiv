{
    "title": "Ordering for Non-Replacement SGD. (arXiv:2306.15848v1 [cs.LG])",
    "abstract": "One approach for reducing run time and improving efficiency of machine learning is to reduce the convergence rate of the optimization algorithm used. Shuffling is an algorithm technique that is widely used in machine learning, but it only started to gain attention theoretically in recent years. With different convergence rates developed for random shuffling and incremental gradient descent, we seek to find an ordering that can improve the convergence rates for the non-replacement form of the algorithm. Based on existing bounds of the distance between the optimal and current iterate, we derive an upper bound that is dependent on the gradients at the beginning of the epoch. Through analysis of the bound, we are able to develop optimal orderings for constant and decreasing step sizes for strongly convex and convex functions. We further test and verify our results through experiments on synthesis and real data sets. In addition, we are able to combine the ordering with mini-batch and furth",
    "link": "http://arxiv.org/abs/2306.15848",
    "context": "Title: Ordering for Non-Replacement SGD. (arXiv:2306.15848v1 [cs.LG])\nAbstract: One approach for reducing run time and improving efficiency of machine learning is to reduce the convergence rate of the optimization algorithm used. Shuffling is an algorithm technique that is widely used in machine learning, but it only started to gain attention theoretically in recent years. With different convergence rates developed for random shuffling and incremental gradient descent, we seek to find an ordering that can improve the convergence rates for the non-replacement form of the algorithm. Based on existing bounds of the distance between the optimal and current iterate, we derive an upper bound that is dependent on the gradients at the beginning of the epoch. Through analysis of the bound, we are able to develop optimal orderings for constant and decreasing step sizes for strongly convex and convex functions. We further test and verify our results through experiments on synthesis and real data sets. In addition, we are able to combine the ordering with mini-batch and furth",
    "path": "papers/23/06/2306.15848.json",
    "total_tokens": 802,
    "translated_title": "非替代性SGD的排序",
    "translated_abstract": "降低机器学习的运行时间和提高效率的一种方法是减少所使用的优化算法的收敛速度。洗牌是一种广泛应用于机器学习的算法技术，但在理论上却只在最近几年才开始受到关注。针对随机洗牌和增量梯度下降的不同收敛速度，我们寻求找到一种排序方式，以改善非替代形式算法的收敛速度。基于已有的最优与当前迭代之间的距离界限，我们得到了一个上界，该上界取决于迭代开始时的梯度。通过对该界限进行分析，我们能够为强凸和凸函数开发出常数和递减步长的最优排序。我们还通过对合成数据集和真实数据集进行实验证实了我们的结果。另外，我们还能将这种排序方式与小批量和增大批次梯度结合起来。",
    "tldr": "这项研究提出了一种优化非替代性SGD算法收敛速度的排序策略，并通过实验证实了其有效性。"
}