{
    "title": "SIFTER: A Task-specific Alignment Strategy for Enhancing Sentence Embeddings. (arXiv:2306.12280v1 [cs.CL])",
    "abstract": "The paradigm of pre-training followed by fine-tuning on downstream tasks has become the mainstream method in natural language processing tasks. Although pre-trained models have the advantage of generalization, their performance may still vary significantly across different domain tasks. This is because the data distribution in different domains varies. For example, the different parts of the sentence 'He married Smt. Dipali Ghosh in 1947 and led a very happy married life' may have different impact for downstream tasks. For similarity calculations, words such as 'led' and 'life' are more important. On the other hand, for sentiment analysis, the word 'happy' is crucial. This indicates that different downstream tasks have different levels of sensitivity to sentence components. Our starting point is to scale information of the model and data according to the specifics of downstream tasks, enhancing domain information of relevant parts for these tasks and reducing irrelevant elements for di",
    "link": "http://arxiv.org/abs/2306.12280",
    "context": "Title: SIFTER: A Task-specific Alignment Strategy for Enhancing Sentence Embeddings. (arXiv:2306.12280v1 [cs.CL])\nAbstract: The paradigm of pre-training followed by fine-tuning on downstream tasks has become the mainstream method in natural language processing tasks. Although pre-trained models have the advantage of generalization, their performance may still vary significantly across different domain tasks. This is because the data distribution in different domains varies. For example, the different parts of the sentence 'He married Smt. Dipali Ghosh in 1947 and led a very happy married life' may have different impact for downstream tasks. For similarity calculations, words such as 'led' and 'life' are more important. On the other hand, for sentiment analysis, the word 'happy' is crucial. This indicates that different downstream tasks have different levels of sensitivity to sentence components. Our starting point is to scale information of the model and data according to the specifics of downstream tasks, enhancing domain information of relevant parts for these tasks and reducing irrelevant elements for di",
    "path": "papers/23/06/2306.12280.json",
    "total_tokens": 703,
    "translated_title": "SIFTER: 一种用于增强句子嵌入的任务特定对齐策略",
    "translated_abstract": "预训练模型后进行微调已成为自然语言处理任务中的主流方法。虽然预训练模型具有泛化的优势，但它们的表现在不同领域任务中仍可能存在显著差异。这是因为不同领域的数据分布不同，并且不同的下游任务对句子的敏感程度也不同。该论文提出了一种用于增强句子嵌入的任务特定对齐策略 SIFTER，以适应不同任务需求。",
    "tldr": "SIFTER 是一种适应不同任务需求的任务特定对齐策略，用于增强句子嵌入。"
}