{
    "title": "OWQ: Lessons learned from activation outliers for weight quantization in large language models. (arXiv:2306.02272v2 [cs.CL] UPDATED)",
    "abstract": "Large language models (LLMs) with hundreds of billions of parameters show impressive results across various language tasks using simple prompt tuning and few-shot examples, without the need for task-specific fine-tuning. However, their enormous size requires multiple server-grade GPUs even for inference, creating a significant cost barrier. To address this limitation, we introduce a novel post-training quantization method for weights with minimal quality degradation. While activation outliers are known to be problematic in activation quantization, our theoretical analysis suggests that we can identify factors contributing to weight quantization errors by considering activation outliers. We propose an innovative PTQ scheme called outlier-aware weight quantization (OWQ), which identifies vulnerable weights and allocates high-precision to them. Our extensive experiments demonstrate that the 3.01-bit models produced by OWQ exhibit comparable quality to the 4-bit models generated by OPTQ.",
    "link": "http://arxiv.org/abs/2306.02272",
    "context": "Title: OWQ: Lessons learned from activation outliers for weight quantization in large language models. (arXiv:2306.02272v2 [cs.CL] UPDATED)\nAbstract: Large language models (LLMs) with hundreds of billions of parameters show impressive results across various language tasks using simple prompt tuning and few-shot examples, without the need for task-specific fine-tuning. However, their enormous size requires multiple server-grade GPUs even for inference, creating a significant cost barrier. To address this limitation, we introduce a novel post-training quantization method for weights with minimal quality degradation. While activation outliers are known to be problematic in activation quantization, our theoretical analysis suggests that we can identify factors contributing to weight quantization errors by considering activation outliers. We propose an innovative PTQ scheme called outlier-aware weight quantization (OWQ), which identifies vulnerable weights and allocates high-precision to them. Our extensive experiments demonstrate that the 3.01-bit models produced by OWQ exhibit comparable quality to the 4-bit models generated by OPTQ.",
    "path": "papers/23/06/2306.02272.json",
    "total_tokens": 981,
    "translated_title": "OWQ：大语言模型权重量化中激活离群值的启示",
    "translated_abstract": "拥有数十亿个参数的大型语言模型(LLMs)通过简单的提示调整和少量的示例，在各种语言任务中展现出令人惊叹的结果，而无需进行任务特定的微调。然而，它们巨大的尺寸要求甚至在推理时使用多个服务器级的GPU，从而产生了显著的成本障碍。为了解决这一限制，我们提出了一种新型的后训练量化方法来量化权重，减少质量损失。虽然已知激活离群值在激活量化中存在问题，但我们的理论分析表明，通过考虑激活离群值，我们可以确定导致权重量化误差的因素。我们提出了一种创新的后训练量化方案，名为Outlier-Aware Weight Quantization (OWQ)，它可以识别易受攻击的权重并为它们分配高精度。我们的大量实验表明，OWQ生成的3.01位模型具有与OPTQ生成的4位模型相当的质量。",
    "tldr": "在大语言模型的推理中，要使用多个服务器贵重的GPU导致显著的成本障碍，OWQ提出的一种后训练量化方法可以在最小质量损失的情况下减少这种限制。它可以通过考虑激活离群值来确定权值量化误差的因素，并为易受攻击的权重分配高精度，具有与OPTQ相当的质量。",
    "en_tdlr": "OWQ proposes a post-training quantization method to reduce the cost barrier caused by the use of multiple, expensive GPUs for inference in large language models. By considering activation outliers, OWQ identifies factors contributing to weight quantization errors and allocates high-precision to vulnerable weights, with comparable quality to OPTQ."
}