{
    "title": "Revealing the Illusion of Joint Multimodal Understanding in VideoQA Models. (arXiv:2306.08889v2 [cs.CV] UPDATED)",
    "abstract": "While VideoQA Transformer models demonstrate competitive performance on standard benchmarks, the reasons behind their success are not fully understood. Do these models jointly capture and leverage the rich multimodal structures and dynamics from video and text? Or are they merely exploiting shortcuts to achieve high scores? Hence, we design $\\textit{QUAG}$ (QUadrant AveraGe), a lightweight and non-parametric probe, to critically analyze multimodal representations. QUAG facilitates combined dataset-model study by systematic ablation of model's coupled multimodal understanding during inference. Surprisingly, it demonstrates that the models manage to maintain high performance even under multimodal impairment. We extend QUAG to design \"QUAG-attention\", a simplistic and less-expressive replacement of self-attention. We find that the models with QUAG-attention achieve similar performance with significantly less mulops without any finetuning. These findings indicate that the current VideoQA b",
    "link": "http://arxiv.org/abs/2306.08889",
    "context": "Title: Revealing the Illusion of Joint Multimodal Understanding in VideoQA Models. (arXiv:2306.08889v2 [cs.CV] UPDATED)\nAbstract: While VideoQA Transformer models demonstrate competitive performance on standard benchmarks, the reasons behind their success are not fully understood. Do these models jointly capture and leverage the rich multimodal structures and dynamics from video and text? Or are they merely exploiting shortcuts to achieve high scores? Hence, we design $\\textit{QUAG}$ (QUadrant AveraGe), a lightweight and non-parametric probe, to critically analyze multimodal representations. QUAG facilitates combined dataset-model study by systematic ablation of model's coupled multimodal understanding during inference. Surprisingly, it demonstrates that the models manage to maintain high performance even under multimodal impairment. We extend QUAG to design \"QUAG-attention\", a simplistic and less-expressive replacement of self-attention. We find that the models with QUAG-attention achieve similar performance with significantly less mulops without any finetuning. These findings indicate that the current VideoQA b",
    "path": "papers/23/06/2306.08889.json",
    "total_tokens": 952,
    "translated_title": "揭示视频问答模型中联合多模态理解的幻象",
    "translated_abstract": "尽管视频问答（VideoQA）Transformer模型在标准基准测试中表现出竞争力，但其成功原因尚未完全理解。这些模型是否能够共同捕捉和利用视频和文本中丰富的多模态结构和动态性？或者它们仅仅是利用了捷径来获得高分？因此，我们设计了一个轻量级且非参数化的探针“QUAG”（QUadrant AveraGe），以对多模态表示进行批判性分析。QUAG通过在推理过程中系统地消除模型的耦合多模态理解来促进联合数据集-模型研究。令人惊讶的是，它表明即使在多模态损伤下，模型仍能保持高性能。我们将QUAG扩展为“QUAG-attention”，这是一个简化且表达能力较弱的自注意力替代方法。我们发现，带有QUAG-attention的模型在没有任何微调的情况下能够达到类似的性能，而且计算量显著减少。这些发现表明当前的VideoQA模型在理解多模态信息时存在一定的幻象。",
    "tldr": "通过设计轻量级探针QUAG和替代方法QUAG-attention，发现视频问答模型在多模态理解方面存在幻象，即使在多模态损伤下仍能保持高性能，且用更少的计算量实现相似的性能。"
}