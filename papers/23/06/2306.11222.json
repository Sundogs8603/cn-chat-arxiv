{
    "title": "LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation. (arXiv:2306.11222v2 [cs.LG] UPDATED)",
    "abstract": "Transformer models have achieved remarkable results in various natural language tasks, but they are often prohibitively large, requiring massive memories and computational resources. To reduce the size and complexity of these models, we propose LoSparse (Low-Rank and Sparse approximation), a novel model compression technique that approximates a weight matrix by the sum of a low-rank matrix and a sparse matrix. Our method combines the advantages of both low-rank approximations and pruning, while avoiding their limitations. Low-rank approximation compresses the coherent and expressive parts in neurons, while pruning removes the incoherent and non-expressive parts in neurons. Pruning enhances the diversity of low-rank approximations, and low-rank approximation prevents pruning from losing too many expressive neurons. We evaluate our method on natural language understanding, question answering, and natural language generation tasks. We show that it significantly outperforms existing compre",
    "link": "http://arxiv.org/abs/2306.11222",
    "context": "Title: LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation. (arXiv:2306.11222v2 [cs.LG] UPDATED)\nAbstract: Transformer models have achieved remarkable results in various natural language tasks, but they are often prohibitively large, requiring massive memories and computational resources. To reduce the size and complexity of these models, we propose LoSparse (Low-Rank and Sparse approximation), a novel model compression technique that approximates a weight matrix by the sum of a low-rank matrix and a sparse matrix. Our method combines the advantages of both low-rank approximations and pruning, while avoiding their limitations. Low-rank approximation compresses the coherent and expressive parts in neurons, while pruning removes the incoherent and non-expressive parts in neurons. Pruning enhances the diversity of low-rank approximations, and low-rank approximation prevents pruning from losing too many expressive neurons. We evaluate our method on natural language understanding, question answering, and natural language generation tasks. We show that it significantly outperforms existing compre",
    "path": "papers/23/06/2306.11222.json",
    "total_tokens": 986,
    "translated_title": "基于低秩和稀疏逼近的大型语言模型结构化压缩 LoSparse",
    "translated_abstract": "Transformer 模型在多种自然语言任务中取得了显著的成果，但它们通常过于庞大，需要大量的内存和计算资源。为了降低这些模型的大小和复杂性，我们提出了 LoSparse（低秩和稀疏逼近）一种新的模型压缩技术，通过低秩矩阵和稀疏矩阵之和逼近权重矩阵。我们的方法结合了低秩逼近和裁剪的优点，同时避免了它们的局限性。低秩逼近压缩了神经元中的一致和表达力强的部分，而裁剪则消除了神经元中的不一致和表达力不强的部分。裁剪增强了低秩逼近的多样性，低秩逼近防止了裁剪丢失表达力强的神经元过多。我们在自然语言理解、问答和自然语言生成任务上评估了我们的方法。实验结果表明，我们的方法明显优于现有的压缩方法。",
    "tldr": "LoSparse 是一种新的语言模型压缩技术，通过低秩矩阵和稀疏矩阵逼近权重矩阵，结合了低秩逼近和裁剪的优点，可以显著降低语言模型大小和复杂度，并在多个自然语言任务中表现优异。",
    "en_tdlr": "LoSparse is a novel model compression technique for language models, which approximates weight matrix by the sum of a low-rank matrix and a sparse matrix. It combines the advantages of low-rank approximation and pruning, and significantly reduces the size and complexity of language models while achieving excellent performance in multiple natural language tasks."
}