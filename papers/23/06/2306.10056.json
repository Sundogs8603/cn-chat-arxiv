{
    "title": "Generate to Understand for Representation. (arXiv:2306.10056v1 [cs.CL])",
    "abstract": "In recent years, a significant number of high-quality pretrained models have emerged, greatly impacting Natural Language Understanding (NLU), Natural Language Generation (NLG), and Text Representation tasks. Traditionally, these models are pretrained on custom domain corpora and finetuned for specific tasks, resulting in high costs related to GPU usage and labor. Unfortunately, recent trends in language modeling have shifted towards enhancing performance through scaling, further exacerbating the associated costs.  Introducing GUR: a pretraining framework that combines language modeling and contrastive learning objectives in a single training step. We select similar text pairs based on their Longest Common Substring (LCS) from raw unlabeled documents and train the model using masked language modeling and unsupervised contrastive learning. The resulting model, GUR, achieves impressive results without any labeled training data, outperforming all other pretrained baselines as a retriever a",
    "link": "http://arxiv.org/abs/2306.10056",
    "context": "Title: Generate to Understand for Representation. (arXiv:2306.10056v1 [cs.CL])\nAbstract: In recent years, a significant number of high-quality pretrained models have emerged, greatly impacting Natural Language Understanding (NLU), Natural Language Generation (NLG), and Text Representation tasks. Traditionally, these models are pretrained on custom domain corpora and finetuned for specific tasks, resulting in high costs related to GPU usage and labor. Unfortunately, recent trends in language modeling have shifted towards enhancing performance through scaling, further exacerbating the associated costs.  Introducing GUR: a pretraining framework that combines language modeling and contrastive learning objectives in a single training step. We select similar text pairs based on their Longest Common Substring (LCS) from raw unlabeled documents and train the model using masked language modeling and unsupervised contrastive learning. The resulting model, GUR, achieves impressive results without any labeled training data, outperforming all other pretrained baselines as a retriever a",
    "path": "papers/23/06/2306.10056.json",
    "total_tokens": 924,
    "translated_title": "为了表示而生成——一种结合对比学习的语言预训练框架",
    "translated_abstract": "近年来涌现了大量高质量的预训练模型，极大地影响了自然语言理解、自然语言生成和文本表示等任务。然而，传统上这些模型是在特定领域的语料库上进行预训练，并进行特定任务的微调，这导致了高昂的GPU使用和劳动力成本。文章提出了GUR：一种将语言建模和对比学习目标结合在单个训练步骤中的预训练框架。我们从原始的无标签文档中基于最长公共子字符串（LCS）选择相似的文本对，并使用掩码语言建模和无监督对比学习来训练模型。结果表明，GUR模型在没有任何标记训练数据的情况下取得了令人印象深刻的结果，作为检索器超过了所有其他预训练基线模型。",
    "tldr": "GUR是一种预训练框架，将语言建模和对比学习目标结合在单个训练步骤中，通过从原始无标签文档中选择相似的文本对来训练模型，无需任何标记训练数据即可作为检索器超过其他预训练基线模型。",
    "en_tdlr": "GUR is a pretraining framework that combines language modeling and contrastive learning objectives in a single training step. It trains the model by selecting similar text pairs based on their Longest Common Substring (LCS) from raw unlabeled documents using masked language modeling and unsupervised contrastive learning. GUR achieves impressive results as a retriever without any labeled training data, outperforming all other pretrained baselines."
}