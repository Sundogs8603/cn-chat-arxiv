{
    "title": "Exploring Qualitative Research Using LLMs. (arXiv:2306.13298v1 [cs.SE])",
    "abstract": "The advent of AI driven large language models (LLMs) have stirred discussions about their role in qualitative research. Some view these as tools to enrich human understanding, while others perceive them as threats to the core values of the discipline. This study aimed to compare and contrast the comprehension capabilities of humans and LLMs. We conducted an experiment with small sample of Alexa app reviews, initially classified by a human analyst. LLMs were then asked to classify these reviews and provide the reasoning behind each classification. We compared the results with human classification and reasoning. The research indicated a significant alignment between human and ChatGPT 3.5 classifications in one third of cases, and a slightly lower alignment with GPT4 in over a quarter of cases. The two AI models showed a higher alignment, observed in more than half of the instances. However, a consensus across all three methods was seen only in about one fifth of the classifications. In t",
    "link": "http://arxiv.org/abs/2306.13298",
    "context": "Title: Exploring Qualitative Research Using LLMs. (arXiv:2306.13298v1 [cs.SE])\nAbstract: The advent of AI driven large language models (LLMs) have stirred discussions about their role in qualitative research. Some view these as tools to enrich human understanding, while others perceive them as threats to the core values of the discipline. This study aimed to compare and contrast the comprehension capabilities of humans and LLMs. We conducted an experiment with small sample of Alexa app reviews, initially classified by a human analyst. LLMs were then asked to classify these reviews and provide the reasoning behind each classification. We compared the results with human classification and reasoning. The research indicated a significant alignment between human and ChatGPT 3.5 classifications in one third of cases, and a slightly lower alignment with GPT4 in over a quarter of cases. The two AI models showed a higher alignment, observed in more than half of the instances. However, a consensus across all three methods was seen only in about one fifth of the classifications. In t",
    "path": "papers/23/06/2306.13298.json",
    "total_tokens": 866,
    "translated_title": "利用LLMs探索质性研究",
    "translated_abstract": "AI驱动的大语言模型（LLMs）的出现引发了有关它们在质性研究中作用的讨论。一些人认为这些是丰富人类理解的工具，而另一些人则认为它们威胁到该学科的核心价值观。本研究旨在比较人类和LLMs的理解能力。我们对小型Alexa应用程序评论进行了实验，最初由人类分析师进行分类。然后要求LLMs对这些评论进行分类，并提供每个分类背后的推理。我们将结果与人类分类和推理进行了比较。研究表明，在三分之一的情况下，人类分类和ChatGPT 3.5分类之间存在显着对齐，超过四分之一的情况下，与 GPT4的对齐略低。两个AI模型在超过一半的实例中表现出更高的对齐性。然而，在所有三种方法中达成一致的仅约占分类的五分之一。",
    "tldr": "本研究比较了人类和LLMs在评论分类方面的理解能力，结果表明两种方法在分类上不太一致，对于分类达成一致仅约占五分之一。",
    "en_tdlr": "This study compares the comprehension capabilities of humans and LLMs in comment classification and finds that the two methods are not highly consistent, with only about one fifth of the classifications reaching a consensus."
}