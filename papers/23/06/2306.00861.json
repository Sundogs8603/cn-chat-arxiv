{
    "title": "Non-stationary Reinforcement Learning under General Function Approximation. (arXiv:2306.00861v1 [cs.LG])",
    "abstract": "General function approximation is a powerful tool to handle large state and action spaces in a broad range of reinforcement learning (RL) scenarios. However, theoretical understanding of non-stationary MDPs with general function approximation is still limited. In this paper, we make the first such an attempt. We first propose a new complexity metric called dynamic Bellman Eluder (DBE) dimension for non-stationary MDPs, which subsumes majority of existing tractable RL problems in static MDPs as well as non-stationary MDPs. Based on the proposed complexity metric, we propose a novel confidence-set based model-free algorithm called SW-OPEA, which features a sliding window mechanism and a new confidence set design for non-stationary MDPs. We then establish an upper bound on the dynamic regret for the proposed algorithm, and show that SW-OPEA is provably efficient as long as the variation budget is not significantly large. We further demonstrate via examples of non-stationary linear and tab",
    "link": "http://arxiv.org/abs/2306.00861",
    "context": "Title: Non-stationary Reinforcement Learning under General Function Approximation. (arXiv:2306.00861v1 [cs.LG])\nAbstract: General function approximation is a powerful tool to handle large state and action spaces in a broad range of reinforcement learning (RL) scenarios. However, theoretical understanding of non-stationary MDPs with general function approximation is still limited. In this paper, we make the first such an attempt. We first propose a new complexity metric called dynamic Bellman Eluder (DBE) dimension for non-stationary MDPs, which subsumes majority of existing tractable RL problems in static MDPs as well as non-stationary MDPs. Based on the proposed complexity metric, we propose a novel confidence-set based model-free algorithm called SW-OPEA, which features a sliding window mechanism and a new confidence set design for non-stationary MDPs. We then establish an upper bound on the dynamic regret for the proposed algorithm, and show that SW-OPEA is provably efficient as long as the variation budget is not significantly large. We further demonstrate via examples of non-stationary linear and tab",
    "path": "papers/23/06/2306.00861.json",
    "total_tokens": 975,
    "translated_title": "一般函数逼近下的非平稳强化学习",
    "translated_abstract": "一般函数逼近是处理广泛强化学习场景中的大状态和动作空间的一种强有力的工具。然而，对于具有一般函数逼近的非平稳MDP的理论理解仍然有限。本文首次尝试解决此问题。我们首先为非平稳MDP提出了一个称为动态贝尔曼难度维数（DBE）的新复杂度度量，该度量包含静态MDP中大多数现有易处理的RL问题以及非平稳MDP。基于所提出的复杂度度量，我们提出了一种新的置信集模型自由算法SW-OPEA, 其具有滑动窗口机制和新的置信集设计来解决非平稳MDP问题。然后，我们建立了所提出算法的动态后悔上界，并表明，只要变化预算不是显著大，SW-OPEA可以证明是有效的。我们还通过非平稳线性和表格化环境的示例证明，SW-OPEA可以比现有方法更有效地学习接近最优策略的策略。",
    "tldr": "本文提出了一种新的置信集模型自由算法 SW-OPEA，其具有滑动窗口机制和新的置信集设计来解决非平稳 MDP 问题",
    "en_tdlr": "This paper proposes a novel confidence-set based model-free algorithm, SW-OPEA, under general function approximation for non-stationary Markov decision processes (MDPs). The algorithm features a sliding window mechanism and a new confidence set design. It is shown to be efficient in learning near-optimal policies in non-stationary linear and tabulated environments, with an established upper bound on the dynamic regret."
}