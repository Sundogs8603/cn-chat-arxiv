{
    "title": "Avoid Adversarial Adaption in Federated Learning by Multi-Metric Investigations. (arXiv:2306.03600v1 [cs.LG])",
    "abstract": "Federated Learning (FL) trains machine learning models on data distributed across multiple devices, avoiding data transfer to a central location. This improves privacy, reduces communication costs, and enhances model performance. However, FL is prone to poisoning attacks, which can be untargeted aiming to reduce the model performance, or targeted, so-called backdoors, which add adversarial behavior that can be triggered with appropriately crafted inputs. Striving for stealthiness, backdoor attacks are harder to deal with.  Mitigation techniques against poisoning attacks rely on monitoring certain metrics and filtering malicious model updates. However, previous works didn't consider real-world adversaries and data distributions. To support our statement, we define a new notion of strong adaptive adversaries that can simultaneously adapt to multiple objectives and demonstrate through extensive tests, that existing defense methods can be circumvented in this adversary model. We also demon",
    "link": "http://arxiv.org/abs/2306.03600",
    "context": "Title: Avoid Adversarial Adaption in Federated Learning by Multi-Metric Investigations. (arXiv:2306.03600v1 [cs.LG])\nAbstract: Federated Learning (FL) trains machine learning models on data distributed across multiple devices, avoiding data transfer to a central location. This improves privacy, reduces communication costs, and enhances model performance. However, FL is prone to poisoning attacks, which can be untargeted aiming to reduce the model performance, or targeted, so-called backdoors, which add adversarial behavior that can be triggered with appropriately crafted inputs. Striving for stealthiness, backdoor attacks are harder to deal with.  Mitigation techniques against poisoning attacks rely on monitoring certain metrics and filtering malicious model updates. However, previous works didn't consider real-world adversaries and data distributions. To support our statement, we define a new notion of strong adaptive adversaries that can simultaneously adapt to multiple objectives and demonstrate through extensive tests, that existing defense methods can be circumvented in this adversary model. We also demon",
    "path": "papers/23/06/2306.03600.json",
    "total_tokens": 997,
    "translated_title": "通过多度量调查避免联邦学习中的对抗适应",
    "translated_abstract": "联邦学习 (FL) 可以在分布在多个设备上的数据上训练机器学习模型，避免将数据传输到中央位置。这提高了隐私保护，减少了通信成本，并增强了模型性能。然而，FL 容易受到污染攻击，可以是非定向的，旨在降低模型性能，也可以是有目的的，即所谓的后门攻击，这种攻击会添加对抗性行为，可以通过适当制作的输入触发。为了追求隐蔽性，后门攻击更难应对。对抗性攻击缓解技术依赖于监视某些度量标准和过滤恶意模型更新。然而，以前的工作没有考虑到现实世界的对手和数据分布。为了支持我们的论点，我们定义了一个新概念——强适应对手，它可以同时适应多个目标，并通过广泛的测试证明，现有的防御方法在这种对手模型中可以被绕过。我们还证明可以使用多度量标准调查来显著提高 FL 对强适应对手的韧性，而无需额外的数据或模型假设。",
    "tldr": "本文提出了一个新概念——强适应对手，并通过实验表明，现有的防御方法不足以解决现实世界中的对手和数据分布问题。作者使用多度量调查来增强 FL 对这些对手的抵抗力。",
    "en_tdlr": "This paper introduces a new concept - strong adaptive adversaries and demonstrates through experiments that existing defense methods are insufficient to deal with adversaries and data distribution in the real world. The authors use multi-metric investigations to enhance the resistance of FL to these adversaries."
}