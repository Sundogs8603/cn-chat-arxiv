{
    "title": "Multimodal Audio-textual Architecture for Robust Spoken Language Understanding. (arXiv:2306.06819v2 [cs.CL] UPDATED)",
    "abstract": "Recent voice assistants are usually based on the cascade spoken language understanding (SLU) solution, which consists of an automatic speech recognition (ASR) engine and a natural language understanding (NLU) system. Because such approach relies on the ASR output, it often suffers from the so-called ASR error propagation. In this work, we investigate impacts of this ASR error propagation on state-of-the-art NLU systems based on pre-trained language models (PLM), such as BERT and RoBERTa. Moreover, a multimodal language understanding (MLU) module is proposed to mitigate SLU performance degradation caused by errors present in the ASR transcript. The MLU benefits from self-supervised features learned from both audio and text modalities, specifically Wav2Vec for speech and Bert/RoBERTa for language. Our MLU combines an encoder network to embed the audio signal and a text encoder to process text transcripts followed by a late fusion layer to fuse audio and text logits. We found that the pro",
    "link": "http://arxiv.org/abs/2306.06819",
    "context": "Title: Multimodal Audio-textual Architecture for Robust Spoken Language Understanding. (arXiv:2306.06819v2 [cs.CL] UPDATED)\nAbstract: Recent voice assistants are usually based on the cascade spoken language understanding (SLU) solution, which consists of an automatic speech recognition (ASR) engine and a natural language understanding (NLU) system. Because such approach relies on the ASR output, it often suffers from the so-called ASR error propagation. In this work, we investigate impacts of this ASR error propagation on state-of-the-art NLU systems based on pre-trained language models (PLM), such as BERT and RoBERTa. Moreover, a multimodal language understanding (MLU) module is proposed to mitigate SLU performance degradation caused by errors present in the ASR transcript. The MLU benefits from self-supervised features learned from both audio and text modalities, specifically Wav2Vec for speech and Bert/RoBERTa for language. Our MLU combines an encoder network to embed the audio signal and a text encoder to process text transcripts followed by a late fusion layer to fuse audio and text logits. We found that the pro",
    "path": "papers/23/06/2306.06819.json",
    "total_tokens": 1143,
    "translated_title": "多模式音文档架构的鲁棒性口语理解",
    "translated_abstract": "目前的语音助手通常基于级联口语理解（SLU）解决方案，包括自动语音识别（ASR）引擎和自然语言理解（NLU）系统。由于这种方法依靠ASR输出，因此经常遭受所谓的ASR错误传播的影响。在本文中，我们研究了此类ASR错误传播对基于预训练语言模型（PLM）（如BERT和RoBERTa）的最先进NLU系统的影响。此外，提出了一种多模态语言理解（MLU）模块，以减轻由ASR转录中存在的错误引起的SLU性能下降。MLU受益于从音频和文本模态学习的自我监督特征，特别是Wav2Vec用于语音和Bert / RoBERTa用于语言。我们的MLU结合一个编码器网络来嵌入音频信号和一个文本编码器来处理文本转录，然后是一个后期融合层来融合音频和文本逻辑。我们发现，使用这种多模式音文档架构可以有效提高口语理解的鲁棒性。",
    "tldr": "本文提出了一种使用多模态语言理解（MLU）模块的口语理解解决方案，结合了音频和文本模态的自我监督特征，并充分利用了预训练语言模型（BERT和RoBERTa），以减轻由ASR错误传播带来的性能下降。",
    "en_tdlr": "This paper proposes a multimodal approach for spoken language understanding by introducing a Multimodal Language Understanding (MLU) module that combines self-supervised features from audio and text modalities and incorporates pre-trained language models (PLM) such as BERT and RoBERTa. It effectively mitigates the performance degradation caused by ASR error propagation and ultimately improves the robustness of SLU systems."
}