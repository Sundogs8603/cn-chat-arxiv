{
    "title": "INT2.1: Towards Fine-Tunable Quantized Large Language Models with Error Correction through Low-Rank Adaptation. (arXiv:2306.08162v1 [cs.CL])",
    "abstract": "We introduce a method that dramatically reduces fine-tuning VRAM requirements and rectifies quantization errors in quantized Large Language Models. First, we develop an extremely memory-efficient fine-tuning (EMEF) method for quantized models using Low-Rank Adaptation (LoRA), and drawing upon it, we construct an error-correcting algorithm designed to minimize errors induced by the quantization process. Our method reduces the memory requirements by up to 5.6 times, which enables fine-tuning a 7 billion parameter Large Language Model (LLM) on consumer laptops. At the same time, we propose a Low-Rank Error Correction (LREC) method that exploits the added LoRA layers to ameliorate the gap between the quantized model and its float point counterpart. Our error correction framework leads to a fully functional INT2 quantized LLM with the capacity to generate coherent English text. To the best of our knowledge, this is the first INT2 Large Language Model that has been able to reach such a perfo",
    "link": "http://arxiv.org/abs/2306.08162",
    "context": "Title: INT2.1: Towards Fine-Tunable Quantized Large Language Models with Error Correction through Low-Rank Adaptation. (arXiv:2306.08162v1 [cs.CL])\nAbstract: We introduce a method that dramatically reduces fine-tuning VRAM requirements and rectifies quantization errors in quantized Large Language Models. First, we develop an extremely memory-efficient fine-tuning (EMEF) method for quantized models using Low-Rank Adaptation (LoRA), and drawing upon it, we construct an error-correcting algorithm designed to minimize errors induced by the quantization process. Our method reduces the memory requirements by up to 5.6 times, which enables fine-tuning a 7 billion parameter Large Language Model (LLM) on consumer laptops. At the same time, we propose a Low-Rank Error Correction (LREC) method that exploits the added LoRA layers to ameliorate the gap between the quantized model and its float point counterpart. Our error correction framework leads to a fully functional INT2 quantized LLM with the capacity to generate coherent English text. To the best of our knowledge, this is the first INT2 Large Language Model that has been able to reach such a perfo",
    "path": "papers/23/06/2306.08162.json",
    "total_tokens": 976,
    "translated_title": "INT2.1：通过低秩自适应纠错实现精细可调的量化大语言模型",
    "translated_abstract": "我们提出了一种方法，可以显著地减少精细调整VRAM需求，并纠正量化大语言模型中的量化误差。首先，我们使用低秩自适应（LoRA）开发了一种极其内存高效的量化模型精细调整方法（EMEF），并根据它构建了一个错误修正算法，旨在最小化量化过程中引起的误差。我们的方法可以将内存要求降低多达5.6倍，从而使消费者笔记本电脑可以对70亿个参数的大语言模型进行精细调整。同时，我们提出了一种低秩纠错（LREC）方法，利用增加的LoRA层来改善量化模型与其浮点数对应物之间的差距。我们的纠错框架可以生成连贯的英文文本，实现了完全功能的INT2量化大语言模型。据我们所知，这是第一个能够达到这种性能的INT2大语言模型。",
    "tldr": "该论文提出了一种通过低秩自适应纠错的方法，从而可以显著地减少精细调整VRAM需求，并纠正量化大语言模型中的量化误差，使消费者笔记本电脑可以对70亿个参数的大语言模型进行精细调整，生成连贯的英文文本。"
}