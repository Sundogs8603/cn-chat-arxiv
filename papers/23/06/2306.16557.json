{
    "title": "Non-Convex Optimizations for Machine Learning with Theoretical Guarantee: Robust Matrix Completion and Neural Network Learning. (arXiv:2306.16557v1 [cs.LG])",
    "abstract": "Despite the recent development in machine learning, most learning systems are still under the concept of \"black box\", where the performance cannot be understood and derived. With the rise of safety and privacy concerns in public, designing an explainable learning system has become a new trend in machine learning. In general, many machine learning problems are formulated as minimizing (or maximizing) some loss function. Since real data are most likely generated from non-linear models, the loss function is non-convex in general. Unlike the convex optimization problem, gradient descent algorithms will be trapped in spurious local minima in solving non-convex optimization. Therefore, it is challenging to provide explainable algorithms when studying non-convex optimization problems. In this thesis, two popular non-convex problems are studied: (1) low-rank matrix completion and (2) neural network learning.",
    "link": "http://arxiv.org/abs/2306.16557",
    "context": "Title: Non-Convex Optimizations for Machine Learning with Theoretical Guarantee: Robust Matrix Completion and Neural Network Learning. (arXiv:2306.16557v1 [cs.LG])\nAbstract: Despite the recent development in machine learning, most learning systems are still under the concept of \"black box\", where the performance cannot be understood and derived. With the rise of safety and privacy concerns in public, designing an explainable learning system has become a new trend in machine learning. In general, many machine learning problems are formulated as minimizing (or maximizing) some loss function. Since real data are most likely generated from non-linear models, the loss function is non-convex in general. Unlike the convex optimization problem, gradient descent algorithms will be trapped in spurious local minima in solving non-convex optimization. Therefore, it is challenging to provide explainable algorithms when studying non-convex optimization problems. In this thesis, two popular non-convex problems are studied: (1) low-rank matrix completion and (2) neural network learning.",
    "path": "papers/23/06/2306.16557.json",
    "total_tokens": 870,
    "translated_title": "用于具有理论保证的机器学习的非凸优化：稳健矩阵补全和神经网络学习",
    "translated_abstract": "尽管机器学习最近取得了进展，大多数学习系统仍然是\"黑盒\"的概念，其性能无法理解和推导。随着公共安全和隐私问题的日益引起关注，设计一个可解释的学习系统成为机器学习的新趋势。一般来说，许多机器学习问题被表述为最小化（或最大化）某个损失函数。由于真实数据很可能来自非线性模型，损失函数一般是非凸的。与凸优化问题不同，梯度下降算法在解决非凸优化时会陷入假局部最小值。因此，在研究非凸优化问题时，提供可解释的算法是具有挑战性的。本文研究了两个流行的非凸问题：（1）低秩矩阵补全和（2）神经网络学习。",
    "tldr": "本论文研究了稳健矩阵补全和神经网络学习两个流行的非凸优化问题，以提供可解释的算法解决机器学习中的黑盒问题。",
    "en_tdlr": "This paper studies two popular non-convex optimization problems, robust matrix completion and neural network learning, to provide interpretable algorithms for addressing the \"black box\" issue in machine learning."
}