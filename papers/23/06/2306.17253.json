{
    "title": "Towards Zero-Shot Scale-Aware Monocular Depth Estimation. (arXiv:2306.17253v1 [cs.CV])",
    "abstract": "Monocular depth estimation is scale-ambiguous, and thus requires scale supervision to produce metric predictions. Even so, the resulting models will be geometry-specific, with learned scales that cannot be directly transferred across domains. Because of that, recent works focus instead on relative depth, eschewing scale in favor of improved up-to-scale zero-shot transfer. In this work we introduce ZeroDepth, a novel monocular depth estimation framework capable of predicting metric scale for arbitrary test images from different domains and camera parameters. This is achieved by (i) the use of input-level geometric embeddings that enable the network to learn a scale prior over objects; and (ii) decoupling the encoder and decoder stages, via a variational latent representation that is conditioned on single frame information. We evaluated ZeroDepth targeting both outdoor (KITTI, DDAD, nuScenes) and indoor (NYUv2) benchmarks, and achieved a new state-of-the-art in both settings using the sa",
    "link": "http://arxiv.org/abs/2306.17253",
    "context": "Title: Towards Zero-Shot Scale-Aware Monocular Depth Estimation. (arXiv:2306.17253v1 [cs.CV])\nAbstract: Monocular depth estimation is scale-ambiguous, and thus requires scale supervision to produce metric predictions. Even so, the resulting models will be geometry-specific, with learned scales that cannot be directly transferred across domains. Because of that, recent works focus instead on relative depth, eschewing scale in favor of improved up-to-scale zero-shot transfer. In this work we introduce ZeroDepth, a novel monocular depth estimation framework capable of predicting metric scale for arbitrary test images from different domains and camera parameters. This is achieved by (i) the use of input-level geometric embeddings that enable the network to learn a scale prior over objects; and (ii) decoupling the encoder and decoder stages, via a variational latent representation that is conditioned on single frame information. We evaluated ZeroDepth targeting both outdoor (KITTI, DDAD, nuScenes) and indoor (NYUv2) benchmarks, and achieved a new state-of-the-art in both settings using the sa",
    "path": "papers/23/06/2306.17253.json",
    "total_tokens": 987,
    "translated_title": "零射击尺度感知单目深度估计",
    "translated_abstract": "单目深度估计存在尺度不确定性，因此需要尺度监督来产生度量预测。即便如此，由此产生的模型将是几何特定的，学习到的尺度无法直接跨领域传递。因此，最近的研究集中在相对深度上，放弃尺度以提高零射击转移能力。在这项工作中，我们引入了ZeroDepth，一种新的单目深度估计框架，能够从不同领域和相机参数的任意测试图像中预测度量尺度。这是通过两个方面实现的：（i）使用输入级几何嵌入，使网络能够学习对象上的尺度先验；（ii）通过变分潜在表示来解耦编码器和解码器阶段，该表示以单帧信息为条件。我们在室外（KITTI、DDAD、nuScenes）和室内（NYUv2）基准测试中评估了ZeroDepth，并在两种设置中均取得了最新的最优结果。",
    "tldr": "针对单目深度估计的尺度不确定性问题，提出了一种能够在不同领域和相机参数的任意测试图像中预测度量尺度的ZeroDepth框架，通过输入级几何嵌入和变分潜在表示实现了尺度先验的学习和编码器解码器阶段的解耦，在室内和室外基准测试中取得了最新最优结果。",
    "en_tdlr": "Towards addressing the scale ambiguity problem in monocular depth estimation, ZeroDepth framework is proposed for predicting metric scale in arbitrary test images from different domains and camera parameters. This is achieved by utilizing input-level geometric embeddings and decoupling the encoder and decoder stages through variational latent representation, resulting in state-of-the-art performance on both indoor and outdoor benchmarks."
}