{
    "title": "DP-HyPO: An Adaptive Private Hyperparameter Optimization Framework. (arXiv:2306.05734v1 [cs.LG])",
    "abstract": "Hyperparameter optimization, also known as hyperparameter tuning, is a widely recognized technique for improving model performance. Regrettably, when training private ML models, many practitioners often overlook the privacy risks associated with hyperparameter optimization, which could potentially expose sensitive information about the underlying dataset. Currently, the sole existing approach to allow privacy-preserving hyperparameter optimization is to uniformly and randomly select hyperparameters for a number of runs, subsequently reporting the best-performing hyperparameter. In contrast, in non-private settings, practitioners commonly utilize \"adaptive\" hyperparameter optimization methods such as Gaussian process-based optimization, which select the next candidate based on information gathered from previous outputs. This substantial contrast between private and non-private hyperparameter optimization underscores a critical concern. In our paper, we introduce DP-HyPO, a pioneering fr",
    "link": "http://arxiv.org/abs/2306.05734",
    "context": "Title: DP-HyPO: An Adaptive Private Hyperparameter Optimization Framework. (arXiv:2306.05734v1 [cs.LG])\nAbstract: Hyperparameter optimization, also known as hyperparameter tuning, is a widely recognized technique for improving model performance. Regrettably, when training private ML models, many practitioners often overlook the privacy risks associated with hyperparameter optimization, which could potentially expose sensitive information about the underlying dataset. Currently, the sole existing approach to allow privacy-preserving hyperparameter optimization is to uniformly and randomly select hyperparameters for a number of runs, subsequently reporting the best-performing hyperparameter. In contrast, in non-private settings, practitioners commonly utilize \"adaptive\" hyperparameter optimization methods such as Gaussian process-based optimization, which select the next candidate based on information gathered from previous outputs. This substantial contrast between private and non-private hyperparameter optimization underscores a critical concern. In our paper, we introduce DP-HyPO, a pioneering fr",
    "path": "papers/23/06/2306.05734.json",
    "total_tokens": 1028,
    "translated_title": "DP-HyPO: 一种自适应的私有超参数优化框架",
    "translated_abstract": "超参数优化是提高模型性能的广泛认可的技术。然而，在训练私有的机器学习模型时，许多从业者经常忽视与超参数优化相关的隐私风险，这可能会暴露有关底层数据集的敏感信息。目前，唯一现有的允许隐私保护超参数优化的方法是随机选择一些超参数进行多次运行，并最终报告最佳超参数。相比之下，在非私有设置中，从业者通常使用“自适应”超参数优化方法，如基于高斯过程的优化，该优化方法基于先前输出收集的信息选择下一个候选项。这种私有和非私有超参数优化之间的巨大差异凸显出一个关键问题。在本文中，我们介绍了DP-HyPO，一种提供自适应和隐私保护超参数优化的开创性框架。DP-HyPO采用最先进的差分隐私高斯过程方法，并提出了量身定制的优化启发式方法，保证隐私和效率，并且在保持强大隐私保证的同时实现了与非私有自适应优化方法相当的性能。",
    "tldr": "DP-HyPO 是一种自适应的、隐私保护的超参数优化框架，采用最先进的差分隐私高斯过程方法，提出了量身定制的优化启发式方法，保证隐私和效率，在保持强大隐私保证的同时实现了与非私有自适应优化方法相当的性能。",
    "en_tdlr": "DP-HyPO is an adaptive and privacy-preserving hyperparameter optimization framework, utilizing state-of-the-art differentially private Gaussian process approach and proposing tailored optimization heuristics that guarantee privacy and efficiency, achieving comparable performance to non-private adaptive optimization methods while maintaining strong privacy guarantees."
}