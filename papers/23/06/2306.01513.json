{
    "title": "Network Degeneracy as an Indicator of Training Performance: Comparing Finite and Infinite Width Angle Predictions. (arXiv:2306.01513v1 [cs.LG])",
    "abstract": "Neural networks are powerful functions with widespread use, but the theoretical behaviour of these functions is not fully understood. Creating deep neural networks by stacking many layers has achieved exceptional performance in many applications and contributed to the recent explosion of these methods. Previous works have shown that depth can exponentially increase the expressibility of the network. However, as networks get deeper and deeper, they are more susceptible to becoming degenerate. We observe this degeneracy in the sense that on initialization, inputs tend to become more and more correlated as they travel through the layers of the network. If a network has too many layers, it tends to approximate a (random) constant function, making it effectively incapable of distinguishing between inputs. This seems to affect the training of the network and cause it to perform poorly, as we empirically investigate in this paper. We use a simple algorithm that can accurately predict the leve",
    "link": "http://arxiv.org/abs/2306.01513",
    "context": "Title: Network Degeneracy as an Indicator of Training Performance: Comparing Finite and Infinite Width Angle Predictions. (arXiv:2306.01513v1 [cs.LG])\nAbstract: Neural networks are powerful functions with widespread use, but the theoretical behaviour of these functions is not fully understood. Creating deep neural networks by stacking many layers has achieved exceptional performance in many applications and contributed to the recent explosion of these methods. Previous works have shown that depth can exponentially increase the expressibility of the network. However, as networks get deeper and deeper, they are more susceptible to becoming degenerate. We observe this degeneracy in the sense that on initialization, inputs tend to become more and more correlated as they travel through the layers of the network. If a network has too many layers, it tends to approximate a (random) constant function, making it effectively incapable of distinguishing between inputs. This seems to affect the training of the network and cause it to perform poorly, as we empirically investigate in this paper. We use a simple algorithm that can accurately predict the leve",
    "path": "papers/23/06/2306.01513.json",
    "total_tokens": 876,
    "translated_title": "网络劣化作为训练性能评估的指标：有限和无限宽度角度预测的比较。",
    "translated_abstract": "神经网络是功能强大且广泛使用的方法，但其理论行为并没有完全被理解。通过堆叠许多层，可以创建深层神经网络，在许多应用中取得了出色的性能，并促成了最近这些方法的爆炸。先前的研究表明，深度可以指数级增加网络的表达能力。然而，随着网络越来越深，它们越来越容易变得劣化。我们观察到这种退化现象，因为在初始化时，输入倾向于在通过网络的层时变得越来越相关。如果一个网络有太多层，它倾向于逼近一个（随机的）常数函数，有效地无法区分输入。我们在本文中进行了实证研究，发现这似乎影响了网络的训练，并导致它表现不佳。我们使用一种简单的算法，可以准确地预测网络达到的劣化水平。",
    "tldr": "本文发现网络劣化现象，这会影响网络训练并导致其表现不佳。我们还使用了一种简单算法来预测网络劣化的水平。",
    "en_tdlr": "This paper discovers the phenomenon of network degeneracy, which affects the training and leads to poor performance. The paper also presents a simple algorithm to predict the level of network degeneracy."
}