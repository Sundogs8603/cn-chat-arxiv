{
    "title": "Effective Minkowski Dimension of Deep Nonparametric Regression: Function Approximation and Statistical Theories. (arXiv:2306.14859v1 [cs.LG])",
    "abstract": "Existing theories on deep nonparametric regression have shown that when the input data lie on a low-dimensional manifold, deep neural networks can adapt to the intrinsic data structures. In real world applications, such an assumption of data lying exactly on a low dimensional manifold is stringent. This paper introduces a relaxed assumption that the input data are concentrated around a subset of $\\mathbb{R}^d$ denoted by $\\mathcal{S}$, and the intrinsic dimension of $\\mathcal{S}$ can be characterized by a new complexity notation -- effective Minkowski dimension. We prove that, the sample complexity of deep nonparametric regression only depends on the effective Minkowski dimension of $\\mathcal{S}$ denoted by $p$. We further illustrate our theoretical findings by considering nonparametric regression with an anisotropic Gaussian random design $N(0,\\Sigma)$, where $\\Sigma$ is full rank. When the eigenvalues of $\\Sigma$ have an exponential or polynomial decay, the effective Minkowski dimens",
    "link": "http://arxiv.org/abs/2306.14859",
    "context": "Title: Effective Minkowski Dimension of Deep Nonparametric Regression: Function Approximation and Statistical Theories. (arXiv:2306.14859v1 [cs.LG])\nAbstract: Existing theories on deep nonparametric regression have shown that when the input data lie on a low-dimensional manifold, deep neural networks can adapt to the intrinsic data structures. In real world applications, such an assumption of data lying exactly on a low dimensional manifold is stringent. This paper introduces a relaxed assumption that the input data are concentrated around a subset of $\\mathbb{R}^d$ denoted by $\\mathcal{S}$, and the intrinsic dimension of $\\mathcal{S}$ can be characterized by a new complexity notation -- effective Minkowski dimension. We prove that, the sample complexity of deep nonparametric regression only depends on the effective Minkowski dimension of $\\mathcal{S}$ denoted by $p$. We further illustrate our theoretical findings by considering nonparametric regression with an anisotropic Gaussian random design $N(0,\\Sigma)$, where $\\Sigma$ is full rank. When the eigenvalues of $\\Sigma$ have an exponential or polynomial decay, the effective Minkowski dimens",
    "path": "papers/23/06/2306.14859.json",
    "total_tokens": 1012,
    "translated_title": "深度非参数回归的有效闵可夫斯基维数：函数逼近和统计理论",
    "translated_abstract": "现有的关于深度非参数回归的理论表明，当输入数据位于低维流形上时，深度神经网络可以适应固有的数据结构。在实际应用中，数据恰好位于低维流形上的假设是苛刻的。本文引入了一个更宽松的假设，即输入数据集中在$\\mathbb{R}^d$的一个子集$\\mathcal{S}$周围，并且$\\mathcal{S}$的内在维数可以用新的复杂度表示——有效闵可夫斯基维数进行刻画。我们证明，深度非参数回归的样本复杂度仅取决于$\\mathcal{S}$的有效闵可夫斯基维数$p$。此外，我们通过考虑具有各向异性高斯随机设计$ N(0，\\Sigma)$的非参数回归，进一步说明了我们的理论发现，其中$ \\Sigma $具有全秩。当$ \\Sigma $的特征值有指数或多项式衰减时，子集$ \\mathcal {S }$的有效闵可夫斯基维数被证明等于各向异性高斯流形的内在维数。",
    "tldr": "本文提出了深度非参数回归的有效闵可夫斯基维数表示方法，证明了深度非参数回归的样本复杂度仅取决于数据集周围的子集的有效闵可夫斯基维数。",
    "en_tdlr": "This paper introduces the effective Minkowski dimension notation for deep nonparametric regression, which shows that the sample complexity only depends on the effective Minkowski dimension of the subset of input data, and illustrates this with anisotropic Gaussian random designs."
}