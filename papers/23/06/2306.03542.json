{
    "title": "Masked Autoencoders are Efficient Continual Federated Learners. (arXiv:2306.03542v1 [cs.LG])",
    "abstract": "Machine learning is typically framed from a perspective of i.i.d., and more importantly, isolated data. In parts, federated learning lifts this assumption, as it sets out to solve the real-world challenge of collaboratively learning a shared model from data distributed across clients. However, motivated primarily by privacy and computational constraints, the fact that data may change, distributions drift, or even tasks advance individually on clients, is seldom taken into account. The field of continual learning addresses this separate challenge and first steps have recently been taken to leverage synergies in distributed supervised settings, in which several clients learn to solve changing classification tasks over time without forgetting previously seen ones. Motivated by these prior works, we posit that such federated continual learning should be grounded in unsupervised learning of representations that are shared across clients; in the loose spirit of how humans can indirectly leve",
    "link": "http://arxiv.org/abs/2306.03542",
    "context": "Title: Masked Autoencoders are Efficient Continual Federated Learners. (arXiv:2306.03542v1 [cs.LG])\nAbstract: Machine learning is typically framed from a perspective of i.i.d., and more importantly, isolated data. In parts, federated learning lifts this assumption, as it sets out to solve the real-world challenge of collaboratively learning a shared model from data distributed across clients. However, motivated primarily by privacy and computational constraints, the fact that data may change, distributions drift, or even tasks advance individually on clients, is seldom taken into account. The field of continual learning addresses this separate challenge and first steps have recently been taken to leverage synergies in distributed supervised settings, in which several clients learn to solve changing classification tasks over time without forgetting previously seen ones. Motivated by these prior works, we posit that such federated continual learning should be grounded in unsupervised learning of representations that are shared across clients; in the loose spirit of how humans can indirectly leve",
    "path": "papers/23/06/2306.03542.json",
    "total_tokens": 874,
    "tldr": "本论文提出使用遮盖自编码器来进行持续联邦学习，解决数据分布漂移、任务推进等问题，避免遗忘之前的任务。"
}