{
    "title": "Extensive Evaluation of Transformer-based Architectures for Adverse Drug Events Extraction. (arXiv:2306.05276v1 [cs.CL])",
    "abstract": "Adverse Event (ADE) extraction is one of the core tasks in digital pharmacovigilance, especially when applied to informal texts. This task has been addressed by the Natural Language Processing community using large pre-trained language models, such as BERT. Despite the great number of Transformer-based architectures used in the literature, it is unclear which of them has better performances and why. Therefore, in this paper we perform an extensive evaluation and analysis of 19 Transformer-based models for ADE extraction on informal texts. We compare the performance of all the considered models on two datasets with increasing levels of informality (forums posts and tweets). We also combine the purely Transformer-based models with two commonly-used additional processing layers (CRF and LSTM), and analyze their effect on the models performance. Furthermore, we use a well-established feature importance technique (SHAP) to correlate the performance of the models with a set of features that ",
    "link": "http://arxiv.org/abs/2306.05276",
    "context": "Title: Extensive Evaluation of Transformer-based Architectures for Adverse Drug Events Extraction. (arXiv:2306.05276v1 [cs.CL])\nAbstract: Adverse Event (ADE) extraction is one of the core tasks in digital pharmacovigilance, especially when applied to informal texts. This task has been addressed by the Natural Language Processing community using large pre-trained language models, such as BERT. Despite the great number of Transformer-based architectures used in the literature, it is unclear which of them has better performances and why. Therefore, in this paper we perform an extensive evaluation and analysis of 19 Transformer-based models for ADE extraction on informal texts. We compare the performance of all the considered models on two datasets with increasing levels of informality (forums posts and tweets). We also combine the purely Transformer-based models with two commonly-used additional processing layers (CRF and LSTM), and analyze their effect on the models performance. Furthermore, we use a well-established feature importance technique (SHAP) to correlate the performance of the models with a set of features that ",
    "path": "papers/23/06/2306.05276.json",
    "total_tokens": 894,
    "translated_title": "基于Transformer架构的不良药物事件提取方法的广泛评估",
    "translated_abstract": "不良事件（ADE）提取是数字药物监管中的核心任务之一，特别是当应用于非正式文本时。自然语言处理社区使用像BERT这样的大型预训练语言模型来解决这个任务。尽管在文献中使用了大量的基于Transformer的架构，但尚不清楚它们哪一个表现更好以及为什么。因此，在本文中，我们对19种基于Transformer的ADE提取模型进行了广泛评估和分析，并在逐渐增加非正式水平的两个数据集上比较了所有考虑的模型的性能（论坛帖子和推文）。我们还将纯Transformer模型与两个常用的额外处理层（CRF和LSTM）相结合，并分析它们对模型性能的影响。此外，我们使用了一种成熟的特征重要性技术（SHAP）将模型性能与一组特征相关联，以进一步分析模型的性能。",
    "tldr": "本文对19种基于Transformer的ADE提取模型进行广泛评估，并在具有不同非正式程度的数据集上比较它们的性能。此外，我们使用了成熟的特征重要性技术（SHAP）进一步分析了模型的性能。",
    "en_tdlr": "This paper extensively evaluates 19 Transformer-based models for Adverse Event (ADE) extraction on informal texts, comparing their performance on two datasets with increasing levels of informality, and using a well-established feature importance technique to further analyze model performance."
}