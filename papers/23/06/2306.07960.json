{
    "title": "Supervised-Contrastive Loss Learns Orthogonal Frames and Batching Matters. (arXiv:2306.07960v1 [cs.LG])",
    "abstract": "Supervised contrastive loss (SCL) is a competitive and often superior alternative to the cross-entropy (CE) loss for classification. In this paper we ask: what differences in the learning process occur when the two different loss functions are being optimized? To answer this question, our main finding is that the geometry of embeddings learned by SCL forms an orthogonal frame (OF) regardless of the number of training examples per class. This is in contrast to the CE loss, for which previous work has shown that it learns embeddings geometries that are highly dependent on the class sizes. We arrive at our finding theoretically, by proving that the global minimizers of an unconstrained features model with SCL loss and entry-wise non-negativity constraints form an OF. We then validate the model's prediction by conducting experiments with standard deep-learning models on benchmark vision datasets. Finally, our analysis and experiments reveal that the batching scheme chosen during SCL traini",
    "link": "http://arxiv.org/abs/2306.07960",
    "context": "Title: Supervised-Contrastive Loss Learns Orthogonal Frames and Batching Matters. (arXiv:2306.07960v1 [cs.LG])\nAbstract: Supervised contrastive loss (SCL) is a competitive and often superior alternative to the cross-entropy (CE) loss for classification. In this paper we ask: what differences in the learning process occur when the two different loss functions are being optimized? To answer this question, our main finding is that the geometry of embeddings learned by SCL forms an orthogonal frame (OF) regardless of the number of training examples per class. This is in contrast to the CE loss, for which previous work has shown that it learns embeddings geometries that are highly dependent on the class sizes. We arrive at our finding theoretically, by proving that the global minimizers of an unconstrained features model with SCL loss and entry-wise non-negativity constraints form an OF. We then validate the model's prediction by conducting experiments with standard deep-learning models on benchmark vision datasets. Finally, our analysis and experiments reveal that the batching scheme chosen during SCL traini",
    "path": "papers/23/06/2306.07960.json",
    "total_tokens": 966,
    "translated_abstract": "监督对比损失函数（SCL）是分类问题中优于交叉熵损失函数（CE）的一种方法。本文研究了在优化这两种不同的损失函数时，学习过程中出现的差异。我们的研究发现，使用SCL学习到的嵌入向量的几何关系形成了正交框架（OF），而这种规律不受每个类别的训练样本数量的影响。而对于CE损失函数，先前的研究表明它学习到的嵌入向量几何关系高度依赖于类别大小。我们通过数学证明，得出了使用SCL损失函数和逐元素非负性约束的无约束特征模型的全局最小值形成了OF的结论。然后，我们使用基准视觉数据集对标准深度学习模型进行了实验，从而验证了这个模型的预测结果。最后，我们的分析和实验揭示了在SCL训练期间选择的批处理方案对结果的影响。",
    "tldr": "本文探究了监督对比损失函数和交叉熵损失函数使用时的差异，发现在使用SCL学习时，嵌入向量的几何关系形成了正交框架，而且这种规律不受训练数据类别数量的影响。"
}