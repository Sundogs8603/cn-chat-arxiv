{
    "title": "Flexible Channel Dimensions for Differentiable Architecture Search. (arXiv:2306.08021v1 [cs.LG])",
    "abstract": "Finding optimal channel dimensions (i.e., the number of filters in DNN layers) is essential to design DNNs that perform well under computational resource constraints. Recent work in neural architecture search aims at automating the optimization of the DNN model implementation. However, existing neural architecture search methods for channel dimensions rely on fixed search spaces, which prevents achieving an efficient and fully automated solution. In this work, we propose a novel differentiable neural architecture search method with an efficient dynamic channel allocation algorithm to enable a flexible search space for channel dimensions. We show that the proposed framework is able to find DNN architectures that are equivalent to previous methods in task accuracy and inference latency for the CIFAR-10 dataset with an improvement of $1.3-1.7\\times$ in GPU-hours and $1.5-1.7\\times$ in the memory requirements during the architecture search stage. Moreover, the proposed frameworks do not re",
    "link": "http://arxiv.org/abs/2306.08021",
    "context": "Title: Flexible Channel Dimensions for Differentiable Architecture Search. (arXiv:2306.08021v1 [cs.LG])\nAbstract: Finding optimal channel dimensions (i.e., the number of filters in DNN layers) is essential to design DNNs that perform well under computational resource constraints. Recent work in neural architecture search aims at automating the optimization of the DNN model implementation. However, existing neural architecture search methods for channel dimensions rely on fixed search spaces, which prevents achieving an efficient and fully automated solution. In this work, we propose a novel differentiable neural architecture search method with an efficient dynamic channel allocation algorithm to enable a flexible search space for channel dimensions. We show that the proposed framework is able to find DNN architectures that are equivalent to previous methods in task accuracy and inference latency for the CIFAR-10 dataset with an improvement of $1.3-1.7\\times$ in GPU-hours and $1.5-1.7\\times$ in the memory requirements during the architecture search stage. Moreover, the proposed frameworks do not re",
    "path": "papers/23/06/2306.08021.json",
    "total_tokens": 1026,
    "translated_title": "可变通道维度的可微架构搜索方法",
    "translated_abstract": "在计算资源有限的条件下设计表现良好的深度神经网络，找到最优的通道维度（即DNN层中的过滤器数量）至关重要。最近的神经架构搜索工作旨在自动化DNN模型实现的优化。然而，现有的通道维度神经架构搜索方法依赖于固定的搜索空间，这阻碍了实现高效且完全自动化的解决方案。在这项工作中，我们提出了一种新颖的可微神经架构搜索方法，配备有效的动态通道分配算法，以实现通道维度的灵活搜索空间。我们展示了所提出的框架能够找到等同于以前方法在CIFAR-10数据集上任务准确性和推理延迟方面的DNN架构，architecture search阶段GPU-hours提高了1.3-1.7倍，内存要求提高了1.5-1.7倍。此外，所提出的框架不需要任何手动设计或架构工程专业知识，并且可以轻松扩展到各种DNN架构和不同的数据集。",
    "tldr": "本文提出了一种新颖的可微神经架构搜索方法，使用动态通道分配算法实现通道维度的灵活搜索空间，能够有效地找到等同于以前方法在CIFAR-10数据集上任务准确性和推理延迟方面的DNN架构，并且不需要手动设计或架构工程专业知识。"
}