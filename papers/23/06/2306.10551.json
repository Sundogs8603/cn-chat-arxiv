{
    "title": "Can predictive models be used for causal inference?. (arXiv:2306.10551v1 [stat.ML])",
    "abstract": "Supervised machine learning (ML) and deep learning (DL) algorithms excel at predictive tasks, but it is commonly assumed that they often do so by exploiting non-causal correlations, which may limit both interpretability and generalizability. Here, we show that this trade-off between explanation and prediction is not as deep and fundamental as expected. Whereas ML and DL algorithms will indeed tend to use non-causal features for prediction when fed indiscriminately with all data, it is possible to constrain the learning process of any ML and DL algorithm by selecting features according to Pearl's backdoor adjustment criterion. In such a situation, some algorithms, in particular deep neural networks, can provide near unbiased effect estimates under feature collinearity. Remaining biases are explained by the specific algorithmic structures as well as hyperparameter choice. Consequently, optimal hyperparameter settings are different when tuned for prediction or inference, confirming the ge",
    "link": "http://arxiv.org/abs/2306.10551",
    "context": "Title: Can predictive models be used for causal inference?. (arXiv:2306.10551v1 [stat.ML])\nAbstract: Supervised machine learning (ML) and deep learning (DL) algorithms excel at predictive tasks, but it is commonly assumed that they often do so by exploiting non-causal correlations, which may limit both interpretability and generalizability. Here, we show that this trade-off between explanation and prediction is not as deep and fundamental as expected. Whereas ML and DL algorithms will indeed tend to use non-causal features for prediction when fed indiscriminately with all data, it is possible to constrain the learning process of any ML and DL algorithm by selecting features according to Pearl's backdoor adjustment criterion. In such a situation, some algorithms, in particular deep neural networks, can provide near unbiased effect estimates under feature collinearity. Remaining biases are explained by the specific algorithmic structures as well as hyperparameter choice. Consequently, optimal hyperparameter settings are different when tuned for prediction or inference, confirming the ge",
    "path": "papers/23/06/2306.10551.json",
    "total_tokens": 949,
    "translated_title": "预测模型可以用于因果推断吗？",
    "translated_abstract": "监督机器学习和深度学习算法在预测任务中表现出色，但通常假设它们常常利用非因果相关性，这可能会限制解释性和泛化能力。本文展示了解释和预测之间的这种权衡并不像预期的那么深刻和基本。虽然当提供所有数据进行预测时，机器学习和深度学习算法确实倾向于使用非因果特征进行预测，但是可以通过按照Pearl的反向门控准则选择特征来约束任何机器学习和深度学习算法的学习过程。在这种情况下，某些算法（尤其是深度神经网络）可以在特征共线性下提供近似无偏的效应估计。剩余的偏差可由特定的算法结构和超参数选择进行解释。因此，当用于预测或推断时，最佳超参数设置是不同的，这证实了两者之间存在权衡的一般直觉。然而，通过使用适当的特征选择和超参数调整，预测模型仍然可以有效地用于因果推断。",
    "tldr": "尽管传统的机器学习和深度学习算法常常利用非因果相关性，但是通过适当的特征选择和超参数调整，预测模型仍然可以有效地用于因果推断。",
    "en_tdlr": "Although traditional machine learning and deep learning algorithms often exploit non-causal correlations, predictive models can still be effectively used for causal inference by appropriate feature selection and hyperparameter tuning according to Pearl's backdoor adjustment criterion."
}