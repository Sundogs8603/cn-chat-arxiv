{
    "title": "Stepsize Learning for Policy Gradient Methods in Contextual Markov Decision Processes. (arXiv:2306.07741v1 [cs.LG])",
    "abstract": "Policy-based algorithms are among the most widely adopted techniques in model-free RL, thanks to their strong theoretical groundings and good properties in continuous action spaces. Unfortunately, these methods require precise and problem-specific hyperparameter tuning to achieve good performance, and tend to struggle when asked to accomplish a series of heterogeneous tasks. In particular, the selection of the step size has a crucial impact on their ability to learn a highly performing policy, affecting the speed and the stability of the training process, and often being the main culprit for poor results. In this paper, we tackle these issues with a Meta Reinforcement Learning approach, by introducing a new formulation, known as meta-MDP, that can be used to solve any hyperparameter selection problem in RL with contextual processes. After providing a theoretical Lipschitz bound to the difference of performance in different tasks, we adopt the proposed framework to train a batch RL algo",
    "link": "http://arxiv.org/abs/2306.07741",
    "context": "Title: Stepsize Learning for Policy Gradient Methods in Contextual Markov Decision Processes. (arXiv:2306.07741v1 [cs.LG])\nAbstract: Policy-based algorithms are among the most widely adopted techniques in model-free RL, thanks to their strong theoretical groundings and good properties in continuous action spaces. Unfortunately, these methods require precise and problem-specific hyperparameter tuning to achieve good performance, and tend to struggle when asked to accomplish a series of heterogeneous tasks. In particular, the selection of the step size has a crucial impact on their ability to learn a highly performing policy, affecting the speed and the stability of the training process, and often being the main culprit for poor results. In this paper, we tackle these issues with a Meta Reinforcement Learning approach, by introducing a new formulation, known as meta-MDP, that can be used to solve any hyperparameter selection problem in RL with contextual processes. After providing a theoretical Lipschitz bound to the difference of performance in different tasks, we adopt the proposed framework to train a batch RL algo",
    "path": "papers/23/06/2306.07741.json",
    "total_tokens": 1029,
    "translated_title": "在情境马尔可夫决策过程中学习策略梯度方法的步长选择",
    "translated_abstract": "基于策略的算法是模型无关强化学习中最广泛采用的技术之一，由于其在连续动作空间中的强理论基础和良好性质而得到广泛应用。不幸的是，这些方法需要精确和问题特定的超参数调整才能实现良好的性能，并且往往在被要求完成一系列异质任务时难以胜任。特别是，步长的选择对于它们学习高性能策略的能力具有至关重要的影响，影响训练过程的速度和稳定性，经常是不良结果的主要原因。在本文中，我们采用元强化学习方法解决这些问题，引入了一种新的公式，称为元MDP，可以用于解决任何具有情境过程的强化学习中的超参数选择问题。在提供了不同任务性能差异的理论利普希茨界后，我们采用所提出的框架在一组基准测试中训练批量RL算法。通过利用元MDP公式，我们表明我们的方法在标准基准测试上优于现有的步长自适应方法，在各种任务范围内实现良好性能。",
    "tldr": "本文提出了一种元强化学习方法，在情境马尔可夫决策过程中用于超参数选择问题。该方法在标准基准测试上优于现有步长自适应方法，能够实现各种任务范围内的良好性能。"
}