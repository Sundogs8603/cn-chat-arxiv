{
    "title": "Generalised $f$-Mean Aggregation for Graph Neural Networks. (arXiv:2306.13826v1 [cs.LG])",
    "abstract": "Graph Neural Network (GNN) architectures are defined by their implementations of update and aggregation modules. While many works focus on new ways to parametrise the update modules, the aggregation modules receive comparatively little attention. Because it is difficult to parametrise aggregation functions, currently most methods select a \"standard aggregator\" such as $\\mathrm{mean}$, $\\mathrm{sum}$, or $\\mathrm{max}$. While this selection is often made without any reasoning, it has been shown that the choice in aggregator has a significant impact on performance, and the best choice in aggregator is problem-dependent. Since aggregation is a lossy operation, it is crucial to select the most appropriate aggregator in order to minimise information loss. In this paper, we present GenAgg, a generalised aggregation operator, which parametrises a function space that includes all standard aggregators. In our experiments, we show that GenAgg is able to represent the standard aggregators with mu",
    "link": "http://arxiv.org/abs/2306.13826",
    "context": "Title: Generalised $f$-Mean Aggregation for Graph Neural Networks. (arXiv:2306.13826v1 [cs.LG])\nAbstract: Graph Neural Network (GNN) architectures are defined by their implementations of update and aggregation modules. While many works focus on new ways to parametrise the update modules, the aggregation modules receive comparatively little attention. Because it is difficult to parametrise aggregation functions, currently most methods select a \"standard aggregator\" such as $\\mathrm{mean}$, $\\mathrm{sum}$, or $\\mathrm{max}$. While this selection is often made without any reasoning, it has been shown that the choice in aggregator has a significant impact on performance, and the best choice in aggregator is problem-dependent. Since aggregation is a lossy operation, it is crucial to select the most appropriate aggregator in order to minimise information loss. In this paper, we present GenAgg, a generalised aggregation operator, which parametrises a function space that includes all standard aggregators. In our experiments, we show that GenAgg is able to represent the standard aggregators with mu",
    "path": "papers/23/06/2306.13826.json",
    "total_tokens": 843,
    "translated_title": "图神经网络的广义$f$-均值聚合",
    "translated_abstract": "图神经网络(GNN)架构由其更新和聚合模块的实现方式定义。许多工作都集中在新型参数化更新模块的方法上，而聚合模块相对较少受到关注。由于聚合函数很难参数化，目前大多数方法选择“标准聚合器”，如$\\mathrm{mean}$、$\\mathrm{sum}$或$\\mathrm{max}$。尽管这种选择通常没有任何理由，但已经表明聚合器的选择对性能有重大影响，最佳聚合器的选择取决于问题。由于聚合是一种有损操作，选择最适合的聚合器以最小化信息丢失至关重要。本文提出了GenAgg，一种广义聚合运算符，它参数化了一个包括所有标准聚合器的函数空间。在我们的实验中，我们展示了GenAgg能够表示标准聚合器。",
    "tldr": "本文提出了一个广义聚合算子，GenAgg，它包括所有标准聚合器的函数空间。实验结果表明，GenAgg能够表示标准聚合器。",
    "en_tdlr": "This paper presents GenAgg, a generalized aggregation operator, that includes the function space of all standard aggregators, for graph neural networks. The experiments show that it is able to represent the standard aggregators."
}