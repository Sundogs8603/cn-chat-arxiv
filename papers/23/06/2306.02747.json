{
    "title": "Tackling Non-Stationarity in Reinforcement Learning via Causal-Origin Representation. (arXiv:2306.02747v2 [cs.LG] UPDATED)",
    "abstract": "In real-world scenarios, the application of reinforcement learning is significantly challenged by complex non-stationarity. Most existing methods attempt to model changes in the environment explicitly, often requiring impractical prior knowledge. In this paper, we propose a new perspective, positing that non-stationarity can propagate and accumulate through complex causal relationships during state transitions, thereby compounding its sophistication and affecting policy learning. We believe that this challenge can be more effectively addressed by tracing the causal origin of non-stationarity. To this end, we introduce the Causal-Origin REPresentation (COREP) algorithm. COREP primarily employs a guided updating mechanism to learn a stable graph representation for states termed as causal-origin representation. By leveraging this representation, the learned policy exhibits impressive resilience to non-stationarity. We supplement our approach with a theoretical analysis grounded in the cau",
    "link": "http://arxiv.org/abs/2306.02747",
    "context": "Title: Tackling Non-Stationarity in Reinforcement Learning via Causal-Origin Representation. (arXiv:2306.02747v2 [cs.LG] UPDATED)\nAbstract: In real-world scenarios, the application of reinforcement learning is significantly challenged by complex non-stationarity. Most existing methods attempt to model changes in the environment explicitly, often requiring impractical prior knowledge. In this paper, we propose a new perspective, positing that non-stationarity can propagate and accumulate through complex causal relationships during state transitions, thereby compounding its sophistication and affecting policy learning. We believe that this challenge can be more effectively addressed by tracing the causal origin of non-stationarity. To this end, we introduce the Causal-Origin REPresentation (COREP) algorithm. COREP primarily employs a guided updating mechanism to learn a stable graph representation for states termed as causal-origin representation. By leveraging this representation, the learned policy exhibits impressive resilience to non-stationarity. We supplement our approach with a theoretical analysis grounded in the cau",
    "path": "papers/23/06/2306.02747.json",
    "total_tokens": 880,
    "translated_title": "通过因果源表示解决强化学习中的非平稳性问题",
    "translated_abstract": "在真实世界的场景中，强化学习的应用受到复杂的非平稳性的挑战。大多数现有方法试图明确建模环境中的变化，但往往需要不切实际的先验知识。在本文中，我们提出了一种新的观点，认为非平稳性可以通过状态转换中复杂的因果关系传播和累积，从而增加了其复杂性并影响策略学习。我们相信通过追踪非平稳性的因果起源可以更有效地解决这个挑战。为此，我们引入了因果源表示（COREP）算法。COREP主要采用引导更新机制来学习一种稳定的图形表示，称为因果起源表示，通过利用这种表示，学到的策略对非平稳性表现出令人印象深刻的韧性。我们补充了一个基于因果关系的理论分析。",
    "tldr": "本论文介绍了一种通过追踪非平稳性的因果起源来解决强化学习中的挑战的方法，通过引入因果源表示（COREP）算法，学到的策略对非平稳性表现出韧性。"
}