{
    "title": "Where Does My Model Underperform? A Human Evaluation of Slice Discovery Algorithms. (arXiv:2306.08167v1 [cs.HC])",
    "abstract": "Machine learning (ML) models that achieve high average accuracy can still underperform on semantically coherent subsets (i.e. \"slices\") of data. This behavior can have significant societal consequences for the safety or bias of the model in deployment, but identifying these underperforming slices can be difficult in practice, especially in domains where practitioners lack access to group annotations to define coherent subsets of their data. Motivated by these challenges, ML researchers have developed new slice discovery algorithms that aim to group together coherent and high-error subsets of data. However, there has been little evaluation focused on whether these tools help humans form correct hypotheses about where (for which groups) their model underperforms. We conduct a controlled user study (N = 15) where we show 40 slices output by two state-of-the-art slice discovery algorithms to users, and ask them to form hypotheses about where an object detection model underperforms. Our res",
    "link": "http://arxiv.org/abs/2306.08167",
    "context": "Title: Where Does My Model Underperform? A Human Evaluation of Slice Discovery Algorithms. (arXiv:2306.08167v1 [cs.HC])\nAbstract: Machine learning (ML) models that achieve high average accuracy can still underperform on semantically coherent subsets (i.e. \"slices\") of data. This behavior can have significant societal consequences for the safety or bias of the model in deployment, but identifying these underperforming slices can be difficult in practice, especially in domains where practitioners lack access to group annotations to define coherent subsets of their data. Motivated by these challenges, ML researchers have developed new slice discovery algorithms that aim to group together coherent and high-error subsets of data. However, there has been little evaluation focused on whether these tools help humans form correct hypotheses about where (for which groups) their model underperforms. We conduct a controlled user study (N = 15) where we show 40 slices output by two state-of-the-art slice discovery algorithms to users, and ask them to form hypotheses about where an object detection model underperforms. Our res",
    "path": "papers/23/06/2306.08167.json",
    "total_tokens": 1261,
    "translated_title": "我的模型性能为什么会下降？对片段发现算法的人工评估",
    "translated_abstract": "机器学习（ML）模型可以在语义连贯的数据子集（即“片段”）上表现不佳，而高平均准确率的模型仍然会出现这种问题。这种行为可能对模型的安全性或偏见在部署中产生重大影响，但在实践中确定这些性能下降的片段可能很困难，特别是在从业者缺乏访问群组注释以定义其数据的连贯子集的领域。受到这些挑战的驱动，ML研究人员开发了新的片段发现算法，旨在将数据的连贯和高误差子集分组在一起。然而，评估这些工具是否帮助人类正确形成他们的模型性能下降的假设还很少。我们进行了一项受控用户研究（N = 15），向用户展示两种最先进的片段发现算法输出的40个片段，并要求他们形成有关对象检测模型性能下降的假设。我们的响应变量是参与者正确按错误率对片段进行排名的能力。我们的主要发现是：（1）两种片段发现算法都不会让参与者在假设上表现出系统性的优势；（2）即使在同一种片段发现算法中，参与者在正确对片段进行排序的能力上也存在显着变异；（3）对象类别的错误率比对象大小或位置等隐含语义更好地预测了片段难度。总体而言，我们的结果表明自动生成的片段并非确定人工从业者问题性片段的银弹，而在实践中使用这些算法必须小心。",
    "tldr": "机器学习模型在语义连贯的数据子集上表现不佳仍然会出现问题，但是确定这些问题片段可能很困难，自动生成的片段并不是确定人工从业者问题性片段的银弹。",
    "en_tdlr": "Machine learning models may underperform on semantically coherent subsets of data, but identifying these underperforming slices can be difficult in practice, ML researchers have developed new slice discovery algorithms that aim to group together coherent and high-error subsets of data, but there has been little evaluation on whether these tools help humans form correct hypotheses about where their model underperforms, and overall, automatically generated slices are not a silver bullet for identifying problematic slices for human practitioners."
}