{
    "title": "Pre-Pruning and Gradient-Dropping Improve Differentially Private Image Classification. (arXiv:2306.11754v1 [cs.CV])",
    "abstract": "Scalability is a significant challenge when it comes to applying differential privacy to training deep neural networks. The commonly used DP-SGD algorithm struggles to maintain a high level of privacy protection while achieving high accuracy on even moderately sized models. To tackle this challenge, we take advantage of the fact that neural networks are overparameterized, which allows us to improve neural network training with differential privacy. Specifically, we introduce a new training paradigm that uses \\textit{pre-pruning} and \\textit{gradient-dropping} to reduce the parameter space and improve scalability. The process starts with pre-pruning the parameters of the original network to obtain a smaller model that is then trained with DP-SGD. During training, less important gradients are dropped, and only selected gradients are updated. Our training paradigm introduces a tension between the rates of pre-pruning and gradient-dropping, privacy loss, and classification accuracy. Too mu",
    "link": "http://arxiv.org/abs/2306.11754",
    "context": "Title: Pre-Pruning and Gradient-Dropping Improve Differentially Private Image Classification. (arXiv:2306.11754v1 [cs.CV])\nAbstract: Scalability is a significant challenge when it comes to applying differential privacy to training deep neural networks. The commonly used DP-SGD algorithm struggles to maintain a high level of privacy protection while achieving high accuracy on even moderately sized models. To tackle this challenge, we take advantage of the fact that neural networks are overparameterized, which allows us to improve neural network training with differential privacy. Specifically, we introduce a new training paradigm that uses \\textit{pre-pruning} and \\textit{gradient-dropping} to reduce the parameter space and improve scalability. The process starts with pre-pruning the parameters of the original network to obtain a smaller model that is then trained with DP-SGD. During training, less important gradients are dropped, and only selected gradients are updated. Our training paradigm introduces a tension between the rates of pre-pruning and gradient-dropping, privacy loss, and classification accuracy. Too mu",
    "path": "papers/23/06/2306.11754.json",
    "total_tokens": 1073,
    "translated_title": "预修剪和梯度下降改进差分隐私图像分类",
    "translated_abstract": "应用差分隐私到深度神经网络的训练时，可扩展性是一个重大的挑战。常用的DP-SGD算法在实现高准确性的同时保持高隐私保护水平方面存在困难，即使在中等大小的模型上也是如此。为了解决这个挑战，我们利用神经网络有过度参数化的特点，从而提高了差分隐私中神经网络的训练效果。具体而言，我们引入了一种新的训练范式，使用预修剪和梯度下降来减少参数空间并提高可扩展性。该过程始于对原始网络的参数进行修剪，以获取更小的模型，然后使用DP-SGD进行训练。在训练过程中，不重要的梯度被删除，只有选择的梯度被更新。我们的训练范式在预修剪和梯度下降速率、隐私损失和分类准确性之间引入了一种张力。过多或过少的任何一种都可能导致准确性或隐私保护的下降。通过对CIFAR-10和CIFAR-100进行大量实验证明，我们提出的训练范式可以显著改善隐私-准确性的折衷，并实现最先进的差分隐私图像分类结果。",
    "tldr": "该研究提出了一种新的训练范式，使用预修剪和梯度下降技巧来减少参数空间和提高可扩展性，从而显著改善差分隐私图像分类的隐私-准确性折衷问题。"
}