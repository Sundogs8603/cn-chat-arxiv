{
    "title": "Rethinking Weak Supervision in Helping Contrastive Learning. (arXiv:2306.04160v1 [cs.LG])",
    "abstract": "Contrastive learning has shown outstanding performances in both supervised and unsupervised learning, and has recently been introduced to solve weakly supervised learning problems such as semi-supervised learning and noisy label learning. Despite the empirical evidence showing that semi-supervised labels improve the representations of contrastive learning, it remains unknown if noisy supervised information can be directly used in training instead of after manual denoising. Therefore, to explore the mechanical differences between semi-supervised and noisy-labeled information in helping contrastive learning, we establish a unified theoretical framework of contrastive learning under weak supervision. Specifically, we investigate the most intuitive paradigm of jointly training supervised and unsupervised contrastive losses. By translating the weakly supervised information into a similarity graph under the framework of spectral clustering based on the posterior probability of weak labels, w",
    "link": "http://arxiv.org/abs/2306.04160",
    "context": "Title: Rethinking Weak Supervision in Helping Contrastive Learning. (arXiv:2306.04160v1 [cs.LG])\nAbstract: Contrastive learning has shown outstanding performances in both supervised and unsupervised learning, and has recently been introduced to solve weakly supervised learning problems such as semi-supervised learning and noisy label learning. Despite the empirical evidence showing that semi-supervised labels improve the representations of contrastive learning, it remains unknown if noisy supervised information can be directly used in training instead of after manual denoising. Therefore, to explore the mechanical differences between semi-supervised and noisy-labeled information in helping contrastive learning, we establish a unified theoretical framework of contrastive learning under weak supervision. Specifically, we investigate the most intuitive paradigm of jointly training supervised and unsupervised contrastive losses. By translating the weakly supervised information into a similarity graph under the framework of spectral clustering based on the posterior probability of weak labels, w",
    "path": "papers/23/06/2306.04160.json",
    "total_tokens": 825,
    "translated_title": "在对比学习中重新思考弱监督的作用",
    "translated_abstract": "对比学习既可以在有监督学习中表现出色，也可用于无监督学习，在半监督学习和噪声标签学习等领域中也被引入。尽管实证证据表明半监督标签可以改善对比学习的表示，但仍不清楚噪声标签学习是否可以直接用于训练，而不是进行手动去噪后再使用。因此，我们建立了一个统一的理论框架，在弱监督下探讨了用于对比学习的半监督和噪声标签之间的机械差异。具体而言，我们研究了最直观的联合训练有监督和无监督对比损失的范式。通过将弱监督信息转换为基于后验概率的谱聚类框架下的相似性图，我们提出了一种新的训练方法。",
    "tldr": "本论文探讨了使用半监督和噪声标签为弱监督的对比学习，并建立了一个谱聚类框架用于转换弱监督信息。",
    "en_tdlr": "This paper investigates the use of semi-supervised and noisy labels as weak supervision for contrastive learning, and proposes a new training method by establishing a spectral clustering framework for translating weakly supervised information."
}