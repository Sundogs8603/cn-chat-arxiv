{
    "title": "How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources. (arXiv:2306.04751v1 [cs.CL])",
    "abstract": "In this work we explore recent advances in instruction-tuning language models on a range of open instruction-following datasets. Despite recent claims that open models can be on par with state-of-the-art proprietary models, these claims are often accompanied by limited evaluation, making it difficult to compare models across the board and determine the utility of various resources. We provide a large set of instruction-tuned models from 6.7B to 65B parameters in size, trained on 12 instruction datasets ranging from manually curated (e.g., OpenAssistant) to synthetic and distilled (e.g., Alpaca) and systematically evaluate them on their factual knowledge, reasoning, multilinguality, coding, and open-ended instruction following abilities through a collection of automatic, model-based, and human-based metrics. We further introduce T\\\"ulu, our best performing instruction-tuned model suite finetuned on a combination of high-quality open resources.  Our experiments show that different instru",
    "link": "http://arxiv.org/abs/2306.04751",
    "context": "Title: How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources. (arXiv:2306.04751v1 [cs.CL])\nAbstract: In this work we explore recent advances in instruction-tuning language models on a range of open instruction-following datasets. Despite recent claims that open models can be on par with state-of-the-art proprietary models, these claims are often accompanied by limited evaluation, making it difficult to compare models across the board and determine the utility of various resources. We provide a large set of instruction-tuned models from 6.7B to 65B parameters in size, trained on 12 instruction datasets ranging from manually curated (e.g., OpenAssistant) to synthetic and distilled (e.g., Alpaca) and systematically evaluate them on their factual knowledge, reasoning, multilinguality, coding, and open-ended instruction following abilities through a collection of automatic, model-based, and human-based metrics. We further introduce T\\\"ulu, our best performing instruction-tuned model suite finetuned on a combination of high-quality open resources.  Our experiments show that different instru",
    "path": "papers/23/06/2306.04751.json",
    "total_tokens": 1003,
    "translated_title": "骆驼能走多远？探索开放资源中指令调优的现状。",
    "translated_abstract": "本研究探索了指令调优语言模型在一系列开放指令跟随数据集上的最新进展。尽管最近声称开放模型可以与最先进的专有模型相媲美，但这些声称常常伴随着有限的评估，使得难以全面比较模型并确定各种资源的效用。我们提供了一组大型指令调优模型，大小为6.7B到65B个参数，在12个指令数据集上进行训练，包括手动策划的（例如OpenAssistant）和综合的指令数据集（例如Alpaca），并通过一系列自动、基于模型和基于人的指标对其在事实知识、推理、多语言、编码和开放式指令跟随能力方面进行系统评估。我们进一步介绍了T\\\"ulu，我们在高质量开放资源组合上微调的表现最佳的指令调优模型组合。我们的实验表明，不同的指令数据集和模型架构对指令调优模型的性能影响很大，需要进行精细的调整和设计。",
    "tldr": "本文探究了指令调优语言模型在一系列开放指令跟随数据集上的最新进展，提供了一组大型指令调优模型，并进行了系统评估。实验表明，不同的指令数据集和模型架构对指令调优模型的性能影响很大，需要进行精细的调整和设计。",
    "en_tdlr": "This paper explores the recent progress in instruction-tuning language models on a range of open instruction-following datasets, provides a large set of instruction-tuned models, and conducts systematic evaluations. The experiments demonstrate that different instruction datasets and model architectures have a significant impact on the performance of instruction-tuning models and require fine-tuning and careful design."
}