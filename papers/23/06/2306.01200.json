{
    "title": "Multi-Dimensional Evaluation of Text Summarization with In-Context Learning. (arXiv:2306.01200v1 [cs.CL])",
    "abstract": "Evaluation of natural language generation (NLG) is complex and multi-dimensional. Generated text can be evaluated for fluency, coherence, factuality, or any other dimensions of interest. Most frameworks that perform such multi-dimensional evaluation require training on large manually or synthetically generated datasets. In this paper, we study the efficacy of large language models as multi-dimensional evaluators using in-context learning, obviating the need for large training datasets. Our experiments show that in-context learning-based evaluators are competitive with learned evaluation frameworks for the task of text summarization, establishing state-of-the-art on dimensions such as relevance and factual consistency. We then analyze the effects of factors such as the selection and number of in-context examples on performance. Finally, we study the efficacy of in-context learning based evaluators in evaluating zero-shot summaries written by large language models such as GPT-3.",
    "link": "http://arxiv.org/abs/2306.01200",
    "context": "Title: Multi-Dimensional Evaluation of Text Summarization with In-Context Learning. (arXiv:2306.01200v1 [cs.CL])\nAbstract: Evaluation of natural language generation (NLG) is complex and multi-dimensional. Generated text can be evaluated for fluency, coherence, factuality, or any other dimensions of interest. Most frameworks that perform such multi-dimensional evaluation require training on large manually or synthetically generated datasets. In this paper, we study the efficacy of large language models as multi-dimensional evaluators using in-context learning, obviating the need for large training datasets. Our experiments show that in-context learning-based evaluators are competitive with learned evaluation frameworks for the task of text summarization, establishing state-of-the-art on dimensions such as relevance and factual consistency. We then analyze the effects of factors such as the selection and number of in-context examples on performance. Finally, we study the efficacy of in-context learning based evaluators in evaluating zero-shot summaries written by large language models such as GPT-3.",
    "path": "papers/23/06/2306.01200.json",
    "total_tokens": 977,
    "translated_title": "基于上下文学习的文本摘要的多维评价研究",
    "translated_abstract": "自然语言生成（NLG）的评估是复杂而多维的，可以评估生成文本的流畅性、连贯性、事实性或任何其他感兴趣的维度。大多数执行这种多维评估的框架需要在手动或合成生成的大型数据集上进行训练。本文研究采用基于上下文学习的大型语言模型作为多维度评估器的有效性，在无需大型训练数据集的情况下，针对文本摘要这一任务，我们的实验表明基于上下文学习的评估器在诸如相关性和事实一致性等方面处于与已有的学习评估框架相竞争的地位。接着，我们分析了因素如选择和数量的上下文样例对性能的影响。最后，我们研究了采用基于上下文学习的评估器来评估诸如GPT-3这样的大型语言模型生成的零-shot摘要的有效性。",
    "tldr": "本文研究采用基于上下文学习的大型语言模型作为文本多维度评估器的有效性，针对文本摘要任务，实验表明该方法在与学习评估框架相竞争的地位。同时，本文探究了上下文样例选择、数量以及评估零-shot摘要等方面的影响。",
    "en_tdlr": "This paper studies the efficacy of large language models as multi-dimensional evaluators using in-context learning for text summarization. The experiments show competitive performance in dimensions such as relevance and factual consistency, and the effects of factors such as the selection and number of in-context examples on performance are analyzed. The paper also investigates the effectiveness of in-context learning-based evaluators in evaluating zero-shot summaries generated by large language models such as GPT-3."
}