{
    "title": "Comparing the Efficacy of Fine-Tuning and Meta-Learning for Few-Shot Policy Imitation. (arXiv:2306.13554v1 [cs.LG])",
    "abstract": "In this paper we explore few-shot imitation learning for control problems, which involves learning to imitate a target policy by accessing a limited set of offline rollouts. This setting has been relatively under-explored despite its relevance to robotics and control applications. State-of-the-art methods developed to tackle few-shot imitation rely on meta-learning, which is expensive to train as it requires access to a distribution over tasks (rollouts from many target policies and variations of the base environment). Given this limitation we investigate an alternative approach, fine-tuning, a family of methods that pretrain on a single dataset and then fine-tune on unseen domain-specific data. Recent work has shown that fine-tuners outperform meta-learners in few-shot image classification tasks, especially when the data is out-of-domain. Here we evaluate to what extent this is true for control problems, proposing a simple yet effective baseline which relies on two stages: (i) trainin",
    "link": "http://arxiv.org/abs/2306.13554",
    "context": "Title: Comparing the Efficacy of Fine-Tuning and Meta-Learning for Few-Shot Policy Imitation. (arXiv:2306.13554v1 [cs.LG])\nAbstract: In this paper we explore few-shot imitation learning for control problems, which involves learning to imitate a target policy by accessing a limited set of offline rollouts. This setting has been relatively under-explored despite its relevance to robotics and control applications. State-of-the-art methods developed to tackle few-shot imitation rely on meta-learning, which is expensive to train as it requires access to a distribution over tasks (rollouts from many target policies and variations of the base environment). Given this limitation we investigate an alternative approach, fine-tuning, a family of methods that pretrain on a single dataset and then fine-tune on unseen domain-specific data. Recent work has shown that fine-tuners outperform meta-learners in few-shot image classification tasks, especially when the data is out-of-domain. Here we evaluate to what extent this is true for control problems, proposing a simple yet effective baseline which relies on two stages: (i) trainin",
    "path": "papers/23/06/2306.13554.json",
    "total_tokens": 967,
    "translated_abstract": "本文探讨了在控制问题中应用少样本模仿学习的方法，该方法通过访问有限的线下数据获取以学习模仿目标策略。这个领域尚未被广泛探索，尽管它与机器人和控制应用有关。目前，用于应对少样本模仿学习的最先进的方法依赖于元学习，但这种方法的训练成本很高，因为它需要访问任务的分布（来自多个目标策略和基础环境的 rollout）。鉴于这种局限性，我们调查了另一种方法，即微调，这是一族方法，它们在单个数据集上进行预训练，然后在未见过的领域特定数据上进行微调。最近的研究表明，在少量样本的图像分类任务中，微调方法在数据不在领域时性能优于元学习方法。本文评估了这在控制问题中的效果，提出了一种简单而有效的基线方法，该方法依赖于两个阶段：（i）对原任务进行训练",
    "tldr": "本文研究了应用少样本模仿学习方法在控制问题中的效果，比较了传统方法：元学习和微调。结果显示微调在跨领域图像分类等任务中有着优异的表现水平，在控制问题中也具有同样的优势。",
    "en_tdlr": "This paper investigates the effectiveness of few-shot imitation learning for control problems, comparing traditional methods: meta-learning and fine-tuning. Results show that fine-tuning has demonstrated superior performance in cross-domain image classification tasks and carries the same advantages in control problems."
}