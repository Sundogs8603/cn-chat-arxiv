{
    "title": "Accelerating Molecular Graph Neural Networks via Knowledge Distillation. (arXiv:2306.14818v2 [cs.LG] UPDATED)",
    "abstract": "Recent advances in graph neural networks (GNNs) have enabled more comprehensive modeling of molecules and molecular systems, thereby enhancing the precision of molecular property prediction and molecular simulations. Nonetheless, as the field has been progressing to bigger and more complex architectures, state-of-the-art GNNs have become largely prohibitive for many large-scale applications. In this paper, we explore the utility of knowledge distillation (KD) for accelerating molecular GNNs. To this end, we devise KD strategies that facilitate the distillation of hidden representations in directional and equivariant GNNs, and evaluate their performance on the regression task of energy and force prediction. We validate our protocols across different teacher-student configurations and datasets, and demonstrate that they can consistently boost the predictive accuracy of student models without any modifications to their architecture. Moreover, we conduct comprehensive optimization of vario",
    "link": "http://arxiv.org/abs/2306.14818",
    "context": "Title: Accelerating Molecular Graph Neural Networks via Knowledge Distillation. (arXiv:2306.14818v2 [cs.LG] UPDATED)\nAbstract: Recent advances in graph neural networks (GNNs) have enabled more comprehensive modeling of molecules and molecular systems, thereby enhancing the precision of molecular property prediction and molecular simulations. Nonetheless, as the field has been progressing to bigger and more complex architectures, state-of-the-art GNNs have become largely prohibitive for many large-scale applications. In this paper, we explore the utility of knowledge distillation (KD) for accelerating molecular GNNs. To this end, we devise KD strategies that facilitate the distillation of hidden representations in directional and equivariant GNNs, and evaluate their performance on the regression task of energy and force prediction. We validate our protocols across different teacher-student configurations and datasets, and demonstrate that they can consistently boost the predictive accuracy of student models without any modifications to their architecture. Moreover, we conduct comprehensive optimization of vario",
    "path": "papers/23/06/2306.14818.json",
    "total_tokens": 860,
    "translated_title": "通过知识蒸馏加速分子图神经网络",
    "translated_abstract": "近年来，图神经网络（GNNs）的进展使得对分子和分子系统的建模更加全面，从而提高了分子性质预测和分子模拟的精度。然而，随着该领域逐渐发展到更大更复杂的结构，最先进的GNNs对许多大规模应用来说变得很难应用。在本文中，我们探索了知识蒸馏（KD）在加速分子GNNs中的实用性。为此，我们设计了KD策略，以促进定向和等变GNNs中隐藏表示的蒸馏，并在能量和力预测的回归任务上评估其性能。我们在不同的师生配置和数据集上验证了我们的协议，并证明它们可以在不对其架构进行任何修改的情况下持续提高学生模型的预测准确性。此外，我们对不同优化技术进行了全面的优化。",
    "tldr": "本文以知识蒸馏为基础，旨在加速分子图神经网络。通过开发知识蒸馏策略，我们成功地加速了分子GNNs，并在能量和力预测任务中取得了更高的预测准确性。",
    "en_tdlr": "This paper aims to accelerate molecular graph neural networks using knowledge distillation. By developing knowledge distillation strategies, we successfully speed up molecular GNNs and achieve higher predictive accuracy in energy and force prediction tasks."
}