{
    "title": "Large-scale unsupervised audio pre-training for video-to-speech synthesis. (arXiv:2306.15464v1 [cs.SD])",
    "abstract": "Video-to-speech synthesis is the task of reconstructing the speech signal from a silent video of a speaker. Most established approaches to date involve a two-step process, whereby an intermediate representation from the video, such as a spectrogram, is extracted first and then passed to a vocoder to produce the raw audio. Some recent work has focused on end-to-end synthesis, whereby the generation of raw audio and any intermediate representations is performed jointly. All such approaches involve training on data from almost exclusively audio-visual datasets, i.e. every audio sample has a corresponding video sample. This precludes the use of abundant audio-only datasets which may not have a corresponding visual modality (e.g. audiobooks, radio podcasts, speech recognition datasets etc.), as well as audio-only architectures that have been developed by the audio machine learning community over the years. In this paper we propose to train encoder-decoder models on more than 3,500 hours of ",
    "link": "http://arxiv.org/abs/2306.15464",
    "context": "Title: Large-scale unsupervised audio pre-training for video-to-speech synthesis. (arXiv:2306.15464v1 [cs.SD])\nAbstract: Video-to-speech synthesis is the task of reconstructing the speech signal from a silent video of a speaker. Most established approaches to date involve a two-step process, whereby an intermediate representation from the video, such as a spectrogram, is extracted first and then passed to a vocoder to produce the raw audio. Some recent work has focused on end-to-end synthesis, whereby the generation of raw audio and any intermediate representations is performed jointly. All such approaches involve training on data from almost exclusively audio-visual datasets, i.e. every audio sample has a corresponding video sample. This precludes the use of abundant audio-only datasets which may not have a corresponding visual modality (e.g. audiobooks, radio podcasts, speech recognition datasets etc.), as well as audio-only architectures that have been developed by the audio machine learning community over the years. In this paper we propose to train encoder-decoder models on more than 3,500 hours of ",
    "path": "papers/23/06/2306.15464.json",
    "total_tokens": 864,
    "translated_title": "大规模无监督音频预训练用于视频到语音合成",
    "translated_abstract": "视频到语音合成是从无声视频中重建语音信号的任务。目前大多数已建立的方法都采用了一个两步法，首先从视频中提取中间表示，如谱图，然后传递给声码器生成原始音频。最近的一些工作专注于端到端合成，即同时生成原始音频和任何中间表示。所有这些方法都需要在几乎完全是音频-视觉数据集上进行训练，即每个音频样本都有对应的视频样本。这排除了使用丰富的仅音频数据集的可能性，这些数据集可能没有对应的视觉模态（例如有声读物、广播播客、语音识别数据集等），以及多年来由音频机器学习社区开发的仅音频架构。在本文中，我们建议在超过3500小时的数据上训练编码器-解码器模型。",
    "tldr": "本文提出了一种利用大规模无监督音频预训练的方法，用于视频到语音合成。通过训练编码器-解码器模型，我们可以在不需要视频对应的情况下，使用丰富的仅音频数据集进行合成。",
    "en_tdlr": "This paper proposes a method of large-scale unsupervised audio pre-training for video-to-speech synthesis. By training encoder-decoder models, we can perform synthesis using abundant audio-only datasets without the need for corresponding videos."
}