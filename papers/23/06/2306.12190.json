{
    "title": "Quantifying lottery tickets under label noise: accuracy, calibration, and complexity. (arXiv:2306.12190v1 [cs.LG])",
    "abstract": "Pruning deep neural networks is a widely used strategy to alleviate the computational burden in machine learning. Overwhelming empirical evidence suggests that pruned models retain very high accuracy even with a tiny fraction of parameters. However, relatively little work has gone into characterising the small pruned networks obtained, beyond a measure of their accuracy. In this paper, we use the sparse double descent approach to identify univocally and characterise pruned models associated with classification tasks. We observe empirically that, for a given task, iterative magnitude pruning (IMP) tends to converge to networks of comparable sizes even when starting from full networks with sizes ranging over orders of magnitude. We analyse the best pruned models in a controlled experimental setup and show that their number of parameters reflects task difficulty and that they are much better than full networks at capturing the true conditional probability distribution of the labels. On re",
    "link": "http://arxiv.org/abs/2306.12190",
    "context": "Title: Quantifying lottery tickets under label noise: accuracy, calibration, and complexity. (arXiv:2306.12190v1 [cs.LG])\nAbstract: Pruning deep neural networks is a widely used strategy to alleviate the computational burden in machine learning. Overwhelming empirical evidence suggests that pruned models retain very high accuracy even with a tiny fraction of parameters. However, relatively little work has gone into characterising the small pruned networks obtained, beyond a measure of their accuracy. In this paper, we use the sparse double descent approach to identify univocally and characterise pruned models associated with classification tasks. We observe empirically that, for a given task, iterative magnitude pruning (IMP) tends to converge to networks of comparable sizes even when starting from full networks with sizes ranging over orders of magnitude. We analyse the best pruned models in a controlled experimental setup and show that their number of parameters reflects task difficulty and that they are much better than full networks at capturing the true conditional probability distribution of the labels. On re",
    "path": "papers/23/06/2306.12190.json",
    "total_tokens": 893,
    "translated_title": "在标签噪声下量化彩票中奖号码：准确性、校准度和复杂度",
    "translated_abstract": "对深度神经网络进行修剪是减轻机器学习计算负担的一种广泛使用的策略。压倒性的经验证据表明，修剪模型即使只有极少量的参数，仍然能保持非常高的准确性。然而，相对少量的工作用于表征所获得的小型修剪网络，仅仅依靠精度的度量。本文使用稀疏双下降方法识别、表征与分类任务相关的修剪模型。我们经验证实，对于给定的任务，迭代幅值剪枝(IMP)倾向于从大小跨度几个数量级的全网络开始收敛到可比大小的网络。我们在一个可控实验环境中分析最佳修剪模型，并展示他们的参数数量反映了任务难度，而且他们比全网络更好的捕捉到真实条件概率分布。",
    "tldr": "本文使用双稀疏下降方法识别和表征与分类任务相关的修剪模型，展示了这些模型的大小与任务难度呈现振荡态势；同时，相较于全网络，这些修剪模型更好地捕捉到真实条件概率分布。"
}