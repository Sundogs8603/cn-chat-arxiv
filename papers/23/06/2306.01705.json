{
    "title": "The Information Pathways Hypothesis: Transformers are Dynamic Self-Ensembles. (arXiv:2306.01705v1 [cs.LG])",
    "abstract": "Transformers use the dense self-attention mechanism which gives a lot of flexibility for long-range connectivity. Over multiple layers of a deep transformer, the number of possible connectivity patterns increases exponentially. However, very few of these contribute to the performance of the network, and even fewer are essential. We hypothesize that there are sparsely connected sub-networks within a transformer, called information pathways which can be trained independently. However, the dynamic (i.e., input-dependent) nature of these pathways makes it difficult to prune dense self-attention during training. But the overall distribution of these pathways is often predictable. We take advantage of this fact to propose Stochastically Subsampled self-Attention (SSA) - a general-purpose training strategy for transformers that can reduce both the memory and computational cost of self-attention by 4 to 8 times during training while also serving as a regularization method - improving generaliz",
    "link": "http://arxiv.org/abs/2306.01705",
    "context": "Title: The Information Pathways Hypothesis: Transformers are Dynamic Self-Ensembles. (arXiv:2306.01705v1 [cs.LG])\nAbstract: Transformers use the dense self-attention mechanism which gives a lot of flexibility for long-range connectivity. Over multiple layers of a deep transformer, the number of possible connectivity patterns increases exponentially. However, very few of these contribute to the performance of the network, and even fewer are essential. We hypothesize that there are sparsely connected sub-networks within a transformer, called information pathways which can be trained independently. However, the dynamic (i.e., input-dependent) nature of these pathways makes it difficult to prune dense self-attention during training. But the overall distribution of these pathways is often predictable. We take advantage of this fact to propose Stochastically Subsampled self-Attention (SSA) - a general-purpose training strategy for transformers that can reduce both the memory and computational cost of self-attention by 4 to 8 times during training while also serving as a regularization method - improving generaliz",
    "path": "papers/23/06/2306.01705.json",
    "total_tokens": 916,
    "translated_title": "信息通路假说：Transformer是动态自组织",
    "translated_abstract": "Transformer模型使用了稠密的自注意力机制，赋予了它在远距离连接方面更高的灵活性。在深度Transformer的多个层中，可能的连接模式数量呈指数级增长。然而，其中很少有连接模式对模型性能有所贡献，其中有用的更少。我们提出假设，在一个Transformer中存在着名为信息通路的稀疏子网络，可以独立地进行训练。然而，这些通路的动态（即依赖于输入）特性使得在训练过程中很难剪枝稠密的自注意力。但是，这些通路的整体分布往往是可以预测的。我们利用这一事实提出了随机子采样自注意力（SSA）——一种用于Transformer的通用训练策略，可以在训练期间将自注意力的内存和计算成本减少4到8倍，同时还可以作为正则化方法，提高模型的泛化能力。",
    "tldr": "Transformer使用稠密的自注意力机制，使得在深度网络中可能的连接模式数量呈指数级增长。我们提出了信息通路假说，利用这种特性将连接模式分解为相互独立的信息通路，减少训练过程中的内存和计算成本以及提高模型的泛化能力。",
    "en_tdlr": "Transformers use dense self-attention mechanism which increases possible connectivity patterns exponentially. We propose the Information Pathways Hypothesis which decomposes connectivity patterns into independent information pathways to reduce memory and computational cost of self-attention during training and improve the model's generalization ability."
}