{
    "title": "Reducing the gap between streaming and non-streaming Transducer-based ASR by adaptive two-stage knowledge distillation. (arXiv:2306.15171v1 [cs.CL])",
    "abstract": "Transducer is one of the mainstream frameworks for streaming speech recognition. There is a performance gap between the streaming and non-streaming transducer models due to limited context. To reduce this gap, an effective way is to ensure that their hidden and output distributions are consistent, which can be achieved by hierarchical knowledge distillation. However, it is difficult to ensure the distribution consistency simultaneously because the learning of the output distribution depends on the hidden one. In this paper, we propose an adaptive two-stage knowledge distillation method consisting of hidden layer learning and output layer learning. In the former stage, we learn hidden representation with full context by applying mean square error loss function. In the latter stage, we design a power transformation based adaptive smoothness method to learn stable output distribution. It achieved 19\\% relative reduction in word error rate, and a faster response for the first token compare",
    "link": "http://arxiv.org/abs/2306.15171",
    "context": "Title: Reducing the gap between streaming and non-streaming Transducer-based ASR by adaptive two-stage knowledge distillation. (arXiv:2306.15171v1 [cs.CL])\nAbstract: Transducer is one of the mainstream frameworks for streaming speech recognition. There is a performance gap between the streaming and non-streaming transducer models due to limited context. To reduce this gap, an effective way is to ensure that their hidden and output distributions are consistent, which can be achieved by hierarchical knowledge distillation. However, it is difficult to ensure the distribution consistency simultaneously because the learning of the output distribution depends on the hidden one. In this paper, we propose an adaptive two-stage knowledge distillation method consisting of hidden layer learning and output layer learning. In the former stage, we learn hidden representation with full context by applying mean square error loss function. In the latter stage, we design a power transformation based adaptive smoothness method to learn stable output distribution. It achieved 19\\% relative reduction in word error rate, and a faster response for the first token compare",
    "path": "papers/23/06/2306.15171.json",
    "total_tokens": 962,
    "translated_title": "通过自适应两阶段知识蒸馏来缩小流式与非流式转导式自动语音识别之间的差距",
    "translated_abstract": "转导式自动语音识别是流式语音识别的一个主流框架。由于上下文有限，流式与非流式转导式模型之间存在性能差距。为了缩小这个差距，一个有效的方式是确保它们的隐藏层和输出分布一致，可以通过层级知识蒸馏来实现。然而，同时确保分布一致性是困难的，因为输出分布的学习依赖于隐藏层。在本文中，我们提出了一个自适应两阶段知识蒸馏的方法，包括隐藏层学习和输出层学习。在前一阶段，我们通过应用均方误差损失函数学习具有完整上下文的隐藏表示。在后一阶段，我们设计了基于幂变换的自适应平滑方法来学习稳定的输出分布。实验结果表明，该方法在词错误率上实现了19%相对降低，并且对于第一个令牌的响应更快。",
    "tldr": "提出了一个自适应两阶段知识蒸馏的方法，通过学习隐藏层和输出层的分布一致性来缩小流式与非流式转导式自动语音识别之间的差距。该方法在词错误率上实现了19%的相对降低，并且对第一个令牌的响应更快。"
}