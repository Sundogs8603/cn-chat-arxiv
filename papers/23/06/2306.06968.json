{
    "title": "Can Forward Gradient Match Backpropagation?. (arXiv:2306.06968v1 [cs.LG])",
    "abstract": "Forward Gradients - the idea of using directional derivatives in forward differentiation mode - have recently been shown to be utilizable for neural network training while avoiding problems generally associated with backpropagation gradient computation, such as locking and memorization requirements. The cost is the requirement to guess the step direction, which is hard in high dimensions. While current solutions rely on weighted averages over isotropic guess vector distributions, we propose to strongly bias our gradient guesses in directions that are much more promising, such as feedback obtained from small, local auxiliary networks. For a standard computer vision neural network, we conduct a rigorous study systematically covering a variety of combinations of gradient targets and gradient guesses, including those previously presented in the literature. We find that using gradients obtained from a local loss as a candidate direction drastically improves on random noise in Forward Gradie",
    "link": "http://arxiv.org/abs/2306.06968",
    "context": "Title: Can Forward Gradient Match Backpropagation?. (arXiv:2306.06968v1 [cs.LG])\nAbstract: Forward Gradients - the idea of using directional derivatives in forward differentiation mode - have recently been shown to be utilizable for neural network training while avoiding problems generally associated with backpropagation gradient computation, such as locking and memorization requirements. The cost is the requirement to guess the step direction, which is hard in high dimensions. While current solutions rely on weighted averages over isotropic guess vector distributions, we propose to strongly bias our gradient guesses in directions that are much more promising, such as feedback obtained from small, local auxiliary networks. For a standard computer vision neural network, we conduct a rigorous study systematically covering a variety of combinations of gradient targets and gradient guesses, including those previously presented in the literature. We find that using gradients obtained from a local loss as a candidate direction drastically improves on random noise in Forward Gradie",
    "path": "papers/23/06/2306.06968.json",
    "total_tokens": 1129,
    "translated_title": "前向梯度算法是否能够取代反向传播算法？",
    "translated_abstract": "最近，前向梯度算法——利用正向微分模式中的方向导数的想法——已被证明可用于神经网络训练，同时避免了通常与反向传播梯度计算相关的问题，例如锁定和记忆要求。代价是需要猜测步骤方向，在高维空间中很难。我们提出在更有前途的方向上强烈偏向我们的梯度猜测方向，例如来自小型局部辅助网络的反馈。针对标准的计算机视觉神经网络，我们进行了一项严格的研究，系统地涵盖了各种梯度目标和梯度猜测的组合，包括以前在文献中提出的组合。我们发现，使用从本地损失获得的梯度作为候选方向，比在前向梯度算法中使用随机噪声，具有明显的改进效果。",
    "tldr": "本文研究通过从小型局部辅助网络反馈中获得的梯度偏向更有前途的方向，提高了前向梯度算法的性能。",
    "en_tdlr": "This article studies an improved forward gradient algorithm by strongly biasing gradient guesses towards more promising directions using feedback from small, local auxiliary networks."
}