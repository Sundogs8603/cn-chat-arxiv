{
    "title": "Decom--CAM: Tell Me What You See, In Details! Feature-Level Interpretation via Decomposition Class Activation Map. (arXiv:2306.04644v1 [cs.CV])",
    "abstract": "Interpretation of deep learning remains a very challenging problem. Although the Class Activation Map (CAM) is widely used to interpret deep model predictions by highlighting object location, it fails to provide insight into the salient features used by the model to make decisions. Furthermore, existing evaluation protocols often overlook the correlation between interpretability performance and the model's decision quality, which presents a more fundamental issue. This paper proposes a new two-stage interpretability method called the Decomposition Class Activation Map (Decom-CAM), which offers a feature-level interpretation of the model's prediction. Decom-CAM decomposes intermediate activation maps into orthogonal features using singular value decomposition and generates saliency maps by integrating them. The orthogonality of features enables CAM to capture local features and can be used to pinpoint semantic components such as eyes, noses, and faces in the input image, making it more ",
    "link": "http://arxiv.org/abs/2306.04644",
    "context": "Title: Decom--CAM: Tell Me What You See, In Details! Feature-Level Interpretation via Decomposition Class Activation Map. (arXiv:2306.04644v1 [cs.CV])\nAbstract: Interpretation of deep learning remains a very challenging problem. Although the Class Activation Map (CAM) is widely used to interpret deep model predictions by highlighting object location, it fails to provide insight into the salient features used by the model to make decisions. Furthermore, existing evaluation protocols often overlook the correlation between interpretability performance and the model's decision quality, which presents a more fundamental issue. This paper proposes a new two-stage interpretability method called the Decomposition Class Activation Map (Decom-CAM), which offers a feature-level interpretation of the model's prediction. Decom-CAM decomposes intermediate activation maps into orthogonal features using singular value decomposition and generates saliency maps by integrating them. The orthogonality of features enables CAM to capture local features and can be used to pinpoint semantic components such as eyes, noses, and faces in the input image, making it more ",
    "path": "papers/23/06/2306.04644.json",
    "total_tokens": 1024,
    "translated_title": "基于分解类激活图的特征级解释方法：Decom-CAM",
    "translated_abstract": "解释深度学习模型一直是一个非常具有挑战性的问题。虽然类激活图（CAM）被广泛用于通过突出对象位置来解释深度模型的预测结果，但它未能提供有关模型用于做出决策的显著特征的见解。此外，现有的评估协议常常忽略了可解释性表现与模型决策质量之间的关联，这是一个更基本的问题。本文提出了一种新的分解类激活图（Decom-CAM）两阶段可解释性方法，用于对模型的预测结果进行特征级解释。Decom-CAM使用奇异值分解将中间激活图分解为正交特征，并通过集成这些特征生成显著性图。特征的正交性使CAM能够捕捉本地特征，并可以用于定位输入图像中的语义组件，如眼睛、鼻子和面部，使其更易于理解和解释。实验结果表明，我们的方法可以有效地定位重要区域和显著特征，在解释准确性方面优于以前的方法，并提高了模型的决策质量。",
    "tldr": "本文提出了一种名为Decom-CAM的两阶段可解释性方法，可用于对深度学习模型的预测结果进行特征级解释。实验结果表明该方法可有效定位重要区域和显著特征，并改善了模型的决策质量。",
    "en_tdlr": "This paper proposes a two-stage interpretability method called Decom-CAM, which offers a feature-level interpretation of deep learning model predictions. Experimental results demonstrate that it can effectively locate important regions and salient features, and improve the decision-making quality of the model."
}