{
    "title": "Language Models Can Learn Exceptions to Syntactic Rules. (arXiv:2306.05969v1 [cs.CL])",
    "abstract": "Artificial neural networks can generalize productively to novel contexts. Can they also learn exceptions to those productive rules? We explore this question using the case of restrictions on English passivization (e.g., the fact that \"The vacation lasted five days\" is grammatical, but \"*Five days was lasted by the vacation\" is not). We collect human acceptability judgments for passive sentences with a range of verbs, and show that the probability distribution defined by GPT-2, a language model, matches the human judgments with high correlation. We also show that the relative acceptability of a verb in the active vs. passive voice is positively correlated with the relative frequency of its occurrence in those voices. These results provide preliminary support for the entrenchment hypothesis, according to which learners track and uses the distributional properties of their input to learn negative exceptions to rules. At the same time, this hypothesis fails to explain the magnitude of unpa",
    "link": "http://arxiv.org/abs/2306.05969",
    "context": "Title: Language Models Can Learn Exceptions to Syntactic Rules. (arXiv:2306.05969v1 [cs.CL])\nAbstract: Artificial neural networks can generalize productively to novel contexts. Can they also learn exceptions to those productive rules? We explore this question using the case of restrictions on English passivization (e.g., the fact that \"The vacation lasted five days\" is grammatical, but \"*Five days was lasted by the vacation\" is not). We collect human acceptability judgments for passive sentences with a range of verbs, and show that the probability distribution defined by GPT-2, a language model, matches the human judgments with high correlation. We also show that the relative acceptability of a verb in the active vs. passive voice is positively correlated with the relative frequency of its occurrence in those voices. These results provide preliminary support for the entrenchment hypothesis, according to which learners track and uses the distributional properties of their input to learn negative exceptions to rules. At the same time, this hypothesis fails to explain the magnitude of unpa",
    "path": "papers/23/06/2306.05969.json",
    "total_tokens": 853,
    "translated_title": "语言模型能够学习语法规则的例外情况",
    "translated_abstract": "人工神经网络可以生产性地概括新的上下文。它们能否也学习这些生产性规则的例外情况？我们以英语被动式句法限制为例（例如，“The vacation lasted five days”是语法正确的，但“*Five days was lasted by the vacation”不是）。我们收集了多种动词格的人类接受度判断数据，并展示了语言模型GPT-2定义的概率分布与人的接受度判断高度相关。我们还展示了一个动词在主动语态和被动语态中的相对可接受性与其在这些语态中的相对出现频率呈正相关关系。这些结果初步支持扎根假设，即学习者能够跟踪和使用他们输入的分布特性，以学习规则的例外情况。同时，这个假设无法解释负面例外情况的数量级。",
    "tldr": "语言模型能够通过分布特性学习规则的例外情况，通过研究动词在主动语态和被动语态中的相对可接受性和相对出现频率的正相关性进行支持。",
    "en_tdlr": "Language models are able to learn exceptions to rules through distributional properties. This study supports the entrenchment hypothesis by showing a positive correlation between the relative acceptability of a verb in active vs. passive voice and its relative frequency in those voices."
}