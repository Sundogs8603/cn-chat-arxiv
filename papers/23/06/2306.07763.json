{
    "title": "NAVER LABS Europe's Multilingual Speech Translation Systems for the IWSLT 2023 Low-Resource Track. (arXiv:2306.07763v1 [cs.CL])",
    "abstract": "This paper presents NAVER LABS Europe's systems for Tamasheq-French and Quechua-Spanish speech translation in the IWSLT 2023 Low-Resource track. Our work attempts to maximize translation quality in low-resource settings using multilingual parameter-efficient solutions that leverage strong pre-trained models. Our primary submission for Tamasheq outperforms the previous state of the art by 7.5 BLEU points on the IWSLT 2022 test set, and achieves 23.6 BLEU on this year's test set, outperforming the second best participant by 7.7 points. For Quechua, we also rank first and achieve 17.7 BLEU, despite having only two hours of translation data. Finally, we show that our proposed multilingual architecture is also competitive for high-resource languages, outperforming the best unconstrained submission to the IWSLT 2021 Multilingual track, despite using much less training data and compute.",
    "link": "http://arxiv.org/abs/2306.07763",
    "context": "Title: NAVER LABS Europe's Multilingual Speech Translation Systems for the IWSLT 2023 Low-Resource Track. (arXiv:2306.07763v1 [cs.CL])\nAbstract: This paper presents NAVER LABS Europe's systems for Tamasheq-French and Quechua-Spanish speech translation in the IWSLT 2023 Low-Resource track. Our work attempts to maximize translation quality in low-resource settings using multilingual parameter-efficient solutions that leverage strong pre-trained models. Our primary submission for Tamasheq outperforms the previous state of the art by 7.5 BLEU points on the IWSLT 2022 test set, and achieves 23.6 BLEU on this year's test set, outperforming the second best participant by 7.7 points. For Quechua, we also rank first and achieve 17.7 BLEU, despite having only two hours of translation data. Finally, we show that our proposed multilingual architecture is also competitive for high-resource languages, outperforming the best unconstrained submission to the IWSLT 2021 Multilingual track, despite using much less training data and compute.",
    "path": "papers/23/06/2306.07763.json",
    "total_tokens": 952,
    "translated_title": "NAVER LABS Europe的多语言语音翻译系统在IWSLT 2023低资源赛道中的应用",
    "translated_abstract": "本文介绍了NAVER LABS Europe在IWSLT 2023低资源赛道中Tamasheq-French和Quechua-Spanish语音翻译的系统。我们的工作旨在利用强大的预训练模型和多语言参数效率解决方案，在低资源环境中实现最大化的翻译质量。我们的Tamasheq主要提交版本在IWSLT 2022测试集上优于先前的最优结果7.5 BLEU分数，并在今年的测试集上达到23.6 BLEU，超过第二名参赛者7.7分。对于Quechua，尽管只有两小时的翻译数据，我们还是排名第一，并达到17.7 BLEU。最后，我们展示了我们提出的多语言架构在高资源语言方面也很有竞争力，在使用更少的训练数据和计算资源的情况下，优于IWSLT 2021多语言赛道最佳未加限制的提交结果。",
    "tldr": "本文介绍了NAVER LABS Europe使用预训练模型和多语言参数效率解决方案在低资源环境下最大化翻译质量的方法，并在IWSLT 2023低资源赛道中Tamasheq-French和Quechua-Spanish语音翻译上取得了出色的结果。",
    "en_tdlr": "This paper describes NAVER LABS Europe's approach of maximizing translation quality in low-resource settings using pre-trained models and multilingual parameter-efficient solutions, and achieving outstanding results on Tamasheq-French and Quechua-Spanish speech translation in the IWSLT 2023 Low-Resource track."
}