{
    "title": "Evaluating Language Models for Mathematics through Interactions. (arXiv:2306.01694v1 [cs.LG])",
    "abstract": "The standard methodology of evaluating large language models (LLMs) based on static pairs of inputs and outputs is insufficient for developing assistants: this kind of assessments fails to take into account the essential interactive element in their deployment, and therefore limits how we understand language model capabilities. We introduce CheckMate, an adaptable prototype platform for humans to interact with and evaluate LLMs. We conduct a study with CheckMate to evaluate three language models~(InstructGPT, ChatGPT, and GPT-4) as assistants in proving undergraduate-level mathematics, with a mixed cohort of participants from undergraduate students to professors of mathematics. We release the resulting interaction and rating dataset, MathConverse. By analysing MathConverse, we derive a preliminary taxonomy of human behaviours and uncover that despite a generally positive correlation, there are notable instances of divergence between correctness and perceived helpfulness in LLM generati",
    "link": "http://arxiv.org/abs/2306.01694",
    "context": "Title: Evaluating Language Models for Mathematics through Interactions. (arXiv:2306.01694v1 [cs.LG])\nAbstract: The standard methodology of evaluating large language models (LLMs) based on static pairs of inputs and outputs is insufficient for developing assistants: this kind of assessments fails to take into account the essential interactive element in their deployment, and therefore limits how we understand language model capabilities. We introduce CheckMate, an adaptable prototype platform for humans to interact with and evaluate LLMs. We conduct a study with CheckMate to evaluate three language models~(InstructGPT, ChatGPT, and GPT-4) as assistants in proving undergraduate-level mathematics, with a mixed cohort of participants from undergraduate students to professors of mathematics. We release the resulting interaction and rating dataset, MathConverse. By analysing MathConverse, we derive a preliminary taxonomy of human behaviours and uncover that despite a generally positive correlation, there are notable instances of divergence between correctness and perceived helpfulness in LLM generati",
    "path": "papers/23/06/2306.01694.json",
    "total_tokens": 907,
    "translated_title": "通过交互评估数学语言模型",
    "translated_abstract": "传统的基于静态输入和输出对大型语言模型（LLMs）进行评估的方法不足以开发助手：这种评估未能考虑部署中的基本交互性质，因此限制了我们对语言模型能力的理解。我们引入了CheckMate，这是一个适应性的人机交互原型平台，用于评估LLMs。我们利用CheckMate对三个语言模型（InstructGPT、ChatGPT和GPT-4）进行了一项研究，评估它们作为大学级数学证明助手的能力，参与者混合了从本科生到数学教授的各层次。我们发布了结果交互和评分数据集MathConverse。通过对MathConverse的分析，我们得出了人类行为的初步分类，并发现尽管普遍存在积极相关性，但在LLM生成的正确性和感知帮助性之间存在显著的分歧。",
    "tldr": "本文介绍了一个可进行人机交互评估LLMs的原型平台CheckMate，并利用该平台评估了三个语言模型在大学级数学证明助手方面的能力，发布了结果数据集MathConverse，并得出了人类行为的初步分类和LMM生成正确性与感知帮助性分歧的发现。",
    "en_tdlr": "This paper introduces a prototype platform, CheckMate, for human interaction with and evaluation of LLMs, and evaluates three language models' ability as assistants in proving undergraduate-level mathematics. They release the MathConverse dataset and derive a preliminary taxonomy of human behaviors, as well as uncovering notable instances of divergence between correctness and perceived helpfulness in LLM generation."
}