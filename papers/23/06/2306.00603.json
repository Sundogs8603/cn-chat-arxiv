{
    "title": "Safe Offline Reinforcement Learning with Real-Time Budget Constraints",
    "abstract": "arXiv:2306.00603v2 Announce Type: replace-cross  Abstract: Aiming at promoting the safe real-world deployment of Reinforcement Learning (RL), research on safe RL has made significant progress in recent years. However, most existing works in the literature still focus on the online setting where risky violations of the safety budget are likely to be incurred during training. Besides, in many real-world applications, the learned policy is required to respond to dynamically determined safety budgets (i.e., constraint threshold) in real time. In this paper, we target at the above real-time budget constraint problem under the offline setting, and propose Trajectory-based REal-time Budget Inference (TREBI) as a novel solution that models this problem from the perspective of trajectory distribution and solves it through diffusion model planning. Theoretically, we prove an error bound of the estimation on the episodic reward and cost under the offline setting and thus provide a performance gua",
    "link": "https://arxiv.org/abs/2306.00603",
    "context": "Title: Safe Offline Reinforcement Learning with Real-Time Budget Constraints\nAbstract: arXiv:2306.00603v2 Announce Type: replace-cross  Abstract: Aiming at promoting the safe real-world deployment of Reinforcement Learning (RL), research on safe RL has made significant progress in recent years. However, most existing works in the literature still focus on the online setting where risky violations of the safety budget are likely to be incurred during training. Besides, in many real-world applications, the learned policy is required to respond to dynamically determined safety budgets (i.e., constraint threshold) in real time. In this paper, we target at the above real-time budget constraint problem under the offline setting, and propose Trajectory-based REal-time Budget Inference (TREBI) as a novel solution that models this problem from the perspective of trajectory distribution and solves it through diffusion model planning. Theoretically, we prove an error bound of the estimation on the episodic reward and cost under the offline setting and thus provide a performance gua",
    "path": "papers/23/06/2306.00603.json",
    "total_tokens": 831,
    "translated_title": "具有实时预算约束的安全离线强化学习",
    "translated_abstract": "为促进强化学习（RL）在现实世界中的安全部署，近年来对安全RL的研究取得了显著进展。然而，文献中大多数现有工作仍专注于在线设置，训练过程中可能会发生对安全预算的风险违规。此外，在许多实际应用中，学得策略需要实时响应动态确定的安全预算（即约束阈值）。本文针对离线设置下的实时预算约束问题，并提出了基于轨迹的实时预算推断（TREBI）作为一个新颖解决方案，该方法从轨迹分布的角度对问题进行建模，并通过扩散模型规划来解决。从理论上讲，我们证明在离线设置下对情节奖励和成本的估计存在误差界限，从而提供了性能保证。",
    "tldr": "提出了一种名为TREBI的新方法，在离线设置下解决强化学习中实时预算约束的问题，通过轨迹分布建模和扩散模型规划来提供性能保证。",
    "en_tdlr": "Introducing a new method called TREBI to address the real-time budget constraint in reinforcement learning under offline setting, providing performance guarantee through trajectory distribution modeling and diffusion model planning."
}