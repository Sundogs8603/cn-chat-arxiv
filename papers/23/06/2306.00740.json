{
    "title": "A Uniform Confidence Phenomenon in Deep Learning and its Implications for Calibration. (arXiv:2306.00740v1 [cs.LG])",
    "abstract": "Despite the impressive generalization capabilities of deep neural networks, they have been repeatedly shown to poorly estimate their predictive uncertainty - in other words, they are frequently overconfident when they are wrong. Fixing this issue is known as model calibration, and has consequently received much attention in the form of modified training schemes and post-training calibration procedures. In this work, we present a significant hurdle to the calibration of modern models: deep neural networks have large neighborhoods of almost certain confidence around their training points. We demonstrate in our experiments that this phenomenon consistently arises (in the context of image classification) across many model and dataset pairs. Furthermore, we prove that when this phenomenon holds, for a large class of data distributions with overlaps between classes, it is not possible to obtain a model that is asymptotically better than random (with respect to calibration) even after applyin",
    "link": "http://arxiv.org/abs/2306.00740",
    "context": "Title: A Uniform Confidence Phenomenon in Deep Learning and its Implications for Calibration. (arXiv:2306.00740v1 [cs.LG])\nAbstract: Despite the impressive generalization capabilities of deep neural networks, they have been repeatedly shown to poorly estimate their predictive uncertainty - in other words, they are frequently overconfident when they are wrong. Fixing this issue is known as model calibration, and has consequently received much attention in the form of modified training schemes and post-training calibration procedures. In this work, we present a significant hurdle to the calibration of modern models: deep neural networks have large neighborhoods of almost certain confidence around their training points. We demonstrate in our experiments that this phenomenon consistently arises (in the context of image classification) across many model and dataset pairs. Furthermore, we prove that when this phenomenon holds, for a large class of data distributions with overlaps between classes, it is not possible to obtain a model that is asymptotically better than random (with respect to calibration) even after applyin",
    "path": "papers/23/06/2306.00740.json",
    "total_tokens": 838,
    "translated_title": "深度学习中的一致置信现象及其对校准的影响",
    "translated_abstract": "尽管深度神经网络具有惊人的泛化能力，但它们屡次表现出在预测不确定性方面估计不佳的情况——换句话说，它们在错误时经常过度自信。解决这个问题被称为模型校准，并以修改训练方案和训练后校准程序的形式受到了广泛关注。在本文中，我们提出了一个现代模型校准的重要障碍：深度神经网络在它们的训练点周围有大的几乎确定的置信邻域。我们在实验中证明了这种现象在很多模型和数据集对中都会出现（在图像分类的背景下）。此外，我们证明了当这种现象出现时，在类别之间存在重叠的大类数据分布中，即使在应用校准后也不能获得比随机更好的渐近校准模型（在渐近意义下）。",
    "tldr": "深度神经网络在训练点周围有大的几乎确定的置信邻域，这导致现代模型校准面临重要障碍。",
    "en_tdlr": "Deep neural networks have large neighborhoods of almost certain confidence around their training points, which presents a significant hurdle to modern model calibration."
}