{
    "title": "What Constitutes Good Contrastive Learning in Time-Series Forecasting?. (arXiv:2306.12086v1 [cs.LG])",
    "abstract": "In recent years, the introduction of self-supervised contrastive learning (SSCL) has demonstrated remarkable improvements in representation learning across various domains, including natural language processing and computer vision. By leveraging the inherent benefits of self-supervision, SSCL enables the pre-training of representation models using vast amounts of unlabeled data. Despite these advances, there remains a significant gap in understanding the impact of different SSCL strategies on time series forecasting performance, as well as the specific benefits that SSCL can bring. This paper aims to address these gaps by conducting a comprehensive analysis of the effectiveness of various training variables, including different SSCL algorithms, learning strategies, model architectures, and their interplay. Additionally, to gain deeper insights into the improvements brought about by SSCL in the context of time-series forecasting, a qualitative analysis of the empirical receptive field i",
    "link": "http://arxiv.org/abs/2306.12086",
    "context": "Title: What Constitutes Good Contrastive Learning in Time-Series Forecasting?. (arXiv:2306.12086v1 [cs.LG])\nAbstract: In recent years, the introduction of self-supervised contrastive learning (SSCL) has demonstrated remarkable improvements in representation learning across various domains, including natural language processing and computer vision. By leveraging the inherent benefits of self-supervision, SSCL enables the pre-training of representation models using vast amounts of unlabeled data. Despite these advances, there remains a significant gap in understanding the impact of different SSCL strategies on time series forecasting performance, as well as the specific benefits that SSCL can bring. This paper aims to address these gaps by conducting a comprehensive analysis of the effectiveness of various training variables, including different SSCL algorithms, learning strategies, model architectures, and their interplay. Additionally, to gain deeper insights into the improvements brought about by SSCL in the context of time-series forecasting, a qualitative analysis of the empirical receptive field i",
    "path": "papers/23/06/2306.12086.json",
    "total_tokens": 908,
    "translated_title": "时间序列预测中的对比学习的重要性是什么？",
    "translated_abstract": "近年来，自监督对比学习(Self-Supervised Contrastive Learning, SSCL)在各个领域中(包括自然语言处理和计算机视觉等)的引入已经展示了在表示学习方面的显著提升。通过利用自监督的潜在优势，SSCL使用大量无标签数据进行了表示模型的预训练。尽管这些进展，但仍然存在一个显著的差距——即我们对于不同的SSCL策略对时间序列预测性能的影响以及SSCL所带来的具体好处理解不足。本文旨在通过对各种训练变量的有效性进行全面分析来解决这些差距，其中包括不同的SSCL算法、学习策略、模型体系结构以及它们之间的相互作用。此外，为了深入了解SSCL在时间序列预测背景下带来的改进，我们还进行了经验感受野的定性分析。",
    "tldr": "本文通过对比分析各种训练变量(包括不同的SSCL算法、学习策略，模型体系结构以及它们之间的相互作用)的有效性，研究了SSCL在时间序列预测中的影响及具体好处。",
    "en_tdlr": "This paper conducts a comprehensive analysis of the effectiveness of various training variables (including different SSCL algorithms, learning strategies, model architectures, and their interplay) to address the gap in understanding the impact of different SSCL strategies on time series forecasting performance, as well as the specific benefits that SSCL can bring."
}