{
    "title": "Improving Hyperparameter Learning under Approximate Inference in Gaussian Process Models. (arXiv:2306.04201v1 [cs.LG])",
    "abstract": "Approximate inference in Gaussian process (GP) models with non-conjugate likelihoods gets entangled with the learning of the model hyperparameters. We improve hyperparameter learning in GP models and focus on the interplay between variational inference (VI) and the learning target. While VI's lower bound to the marginal likelihood is a suitable objective for inferring the approximate posterior, we show that a direct approximation of the marginal likelihood as in Expectation Propagation (EP) is a better learning objective for hyperparameter optimization. We design a hybrid training procedure to bring the best of both worlds: it leverages conjugate-computation VI for inference and uses an EP-like marginal likelihood approximation for hyperparameter learning. We compare VI, EP, Laplace approximation, and our proposed training procedure and empirically demonstrate the effectiveness of our proposal across a wide range of data sets.",
    "link": "http://arxiv.org/abs/2306.04201",
    "context": "Title: Improving Hyperparameter Learning under Approximate Inference in Gaussian Process Models. (arXiv:2306.04201v1 [cs.LG])\nAbstract: Approximate inference in Gaussian process (GP) models with non-conjugate likelihoods gets entangled with the learning of the model hyperparameters. We improve hyperparameter learning in GP models and focus on the interplay between variational inference (VI) and the learning target. While VI's lower bound to the marginal likelihood is a suitable objective for inferring the approximate posterior, we show that a direct approximation of the marginal likelihood as in Expectation Propagation (EP) is a better learning objective for hyperparameter optimization. We design a hybrid training procedure to bring the best of both worlds: it leverages conjugate-computation VI for inference and uses an EP-like marginal likelihood approximation for hyperparameter learning. We compare VI, EP, Laplace approximation, and our proposed training procedure and empirically demonstrate the effectiveness of our proposal across a wide range of data sets.",
    "path": "papers/23/06/2306.04201.json",
    "total_tokens": 863,
    "translated_title": "在高斯过程模型的近似推断中改善超参数学习",
    "translated_abstract": "在具有非共轭似然函数的高斯过程（GP）模型中，近似推断与模型超参数的学习纠缠在一起。我们改进了 GP 模型中的超参数学习，并关注变分推断（VI）与学习目标之间的相互作用。虽然 VI 对边缘似然函数的下界是推断近似后验的合适目标，但我们发现像期望传播（EP）中直接逼近边缘似然函数是更适合超参数优化的学习目标。我们设计了一个混合训练过程，将最佳效果结合到一起：利用共轭计算 VI 进行推断，并使用类似于 EP 的边缘似然函数逼近进行超参数学习。我们比较了 VI、EP、Laplace 近似和我们提出的训练过程，并在广泛的数据集上经验证明了我们的提议的有效性。",
    "tldr": "本文改进了高斯过程模型中的超参数学习，提出了一种混合训练方法来兼顾变分推断和期望传播方法，以优化超参数的学习目标，该方法实验结果表明有效性。",
    "en_tdlr": "This paper improves hyperparameter learning in Gaussian process models by proposing a hybrid training method that combines conjugate-computation VI for inference and uses an EP-like marginal likelihood approximation for hyperparameter learning, which has been empirically demonstrated to be effective across a wide range of data sets."
}