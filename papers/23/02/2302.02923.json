{
    "title": "In Search of Insights, Not Magic Bullets: Towards Demystification of the Model Selection Dilemma in Heterogeneous Treatment Effect Estimation. (arXiv:2302.02923v2 [stat.ML] UPDATED)",
    "abstract": "Personalized treatment effect estimates are often of interest in high-stakes applications -- thus, before deploying a model estimating such effects in practice, one needs to be sure that the best candidate from the ever-growing machine learning toolbox for this task was chosen. Unfortunately, due to the absence of counterfactual information in practice, it is usually not possible to rely on standard validation metrics for doing so, leading to a well-known model selection dilemma in the treatment effect estimation literature. While some solutions have recently been investigated, systematic understanding of the strengths and weaknesses of different model selection criteria is still lacking. In this paper, instead of attempting to declare a global `winner', we therefore empirically investigate success- and failure modes of different selection criteria. We highlight that there is a complex interplay between selection strategies, candidate estimators and the data used for comparing them, an",
    "link": "http://arxiv.org/abs/2302.02923",
    "context": "Title: In Search of Insights, Not Magic Bullets: Towards Demystification of the Model Selection Dilemma in Heterogeneous Treatment Effect Estimation. (arXiv:2302.02923v2 [stat.ML] UPDATED)\nAbstract: Personalized treatment effect estimates are often of interest in high-stakes applications -- thus, before deploying a model estimating such effects in practice, one needs to be sure that the best candidate from the ever-growing machine learning toolbox for this task was chosen. Unfortunately, due to the absence of counterfactual information in practice, it is usually not possible to rely on standard validation metrics for doing so, leading to a well-known model selection dilemma in the treatment effect estimation literature. While some solutions have recently been investigated, systematic understanding of the strengths and weaknesses of different model selection criteria is still lacking. In this paper, instead of attempting to declare a global `winner', we therefore empirically investigate success- and failure modes of different selection criteria. We highlight that there is a complex interplay between selection strategies, candidate estimators and the data used for comparing them, an",
    "path": "papers/23/02/2302.02923.json",
    "total_tokens": 889,
    "translated_title": "不是神奇药丸，而是洞察力之搜寻：消除异质性处理效应估计中的模型选择困境",
    "translated_abstract": "个性化处理效应估计在高风险应用中经常备受关注，因此，在实践中部署估计这种效应的模型之前，需要确信已经选择了最好的机器学习工具箱中的候选模型。不幸的是，由于实践中缺乏反事实信息，通常无法依靠标准验证指标完成此任务，导致了处理效应估计文献中已知的模型选择困境。虽然最近已经研究了一些解决方案，但对不同模型选择标准的优缺点的系统理解仍然缺乏。因此，在本文中，我们并没有试图宣布全局“胜者”，而是对不同选择标准的成功和失败模式进行了实证研究。我们强调选择策略，候选估计量和用于比较它们的数据之间存在复杂的相互作用，并提出了未来研究的方向。",
    "tldr": "本文研究在具有高风险应用的个性化处理效应估计中，不同模型选择标准的优点和缺点，并提出未来研究方向。"
}