{
    "title": "$(\\alpha_D,\\alpha_G)$-GANs: Addressing GAN Training Instabilities via Dual Objectives. (arXiv:2302.14320v2 [cs.LG] UPDATED)",
    "abstract": "In an effort to address the training instabilities of GANs, we introduce a class of dual-objective GANs with different value functions (objectives) for the generator (G) and discriminator (D). In particular, we model each objective using $\\alpha$-loss, a tunable classification loss, to obtain $(\\alpha_D,\\alpha_G)$-GANs, parameterized by $(\\alpha_D,\\alpha_G)\\in (0,\\infty]^2$. For sufficiently large number of samples and capacities for G and D, we show that the resulting non-zero sum game simplifies to minimizing an $f$-divergence under appropriate conditions on $(\\alpha_D,\\alpha_G)$. In the finite sample and capacity setting, we define estimation error to quantify the gap in the generator's performance relative to the optimal setting with infinite samples and obtain upper bounds on this error, showing it to be order optimal under certain conditions. Finally, we highlight the value of tuning $(\\alpha_D,\\alpha_G)$ in alleviating training instabilities for the synthetic 2D Gaussian mixture",
    "link": "http://arxiv.org/abs/2302.14320",
    "context": "Title: $(\\alpha_D,\\alpha_G)$-GANs: Addressing GAN Training Instabilities via Dual Objectives. (arXiv:2302.14320v2 [cs.LG] UPDATED)\nAbstract: In an effort to address the training instabilities of GANs, we introduce a class of dual-objective GANs with different value functions (objectives) for the generator (G) and discriminator (D). In particular, we model each objective using $\\alpha$-loss, a tunable classification loss, to obtain $(\\alpha_D,\\alpha_G)$-GANs, parameterized by $(\\alpha_D,\\alpha_G)\\in (0,\\infty]^2$. For sufficiently large number of samples and capacities for G and D, we show that the resulting non-zero sum game simplifies to minimizing an $f$-divergence under appropriate conditions on $(\\alpha_D,\\alpha_G)$. In the finite sample and capacity setting, we define estimation error to quantify the gap in the generator's performance relative to the optimal setting with infinite samples and obtain upper bounds on this error, showing it to be order optimal under certain conditions. Finally, we highlight the value of tuning $(\\alpha_D,\\alpha_G)$ in alleviating training instabilities for the synthetic 2D Gaussian mixture",
    "path": "papers/23/02/2302.14320.json",
    "total_tokens": 1004,
    "translated_title": "$(\\alpha_D,\\alpha_G)$-GANs：通过双重目标来解决GAN训练不稳定性",
    "translated_abstract": "为了解决GAN的训练不稳定性，我们引入了一类具有不同价值函数（目标）的双重目标GAN，特别地，我们使用可调的分类损失——$\\alpha$-loss来建模每个目标，以得到由$(\\alpha_D,\\alpha_G)\\in(0,\\infty]^2$参数化的$(\\alpha_D,\\alpha_G)$-GAN。对于足够大的样本数和G、D的容量，我们展示了在$(\\alpha_D,\\alpha_G)$的适当条件下，导致的非零和游戏简化为最小化$f$-散度。在有限的样本数和容量的情况下，我们定义估计误差，以量化相对于无限样本下的最优设定而言生成器性能的差距，并得到了这个误差的上界，证明了在某些条件下它的阶有效。最后，我们强调了调整$(\\alpha_D,\\alpha_G)$在缓解合成2D高斯混合问题的训练不稳定性方面的价值。",
    "tldr": "本文提出了一个双重目标GAN，通过使用可调的$\\alpha$-loss来建模每个目标。在足够大的样本数和容量下，这类GAN的非零和游戏简化为最小化$f$-散度。最后，调整$(\\alpha_D,\\alpha_G)$可以缓解训练不稳定性。"
}