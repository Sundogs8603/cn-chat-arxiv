{
    "title": "Few-Shot Table-to-Text Generation with Prompt Planning and Knowledge Memorization. (arXiv:2302.04415v3 [cs.CL] UPDATED)",
    "abstract": "Pre-trained language models (PLM) have achieved remarkable advancement in table-to-text generation tasks. However, the lack of labeled domain-specific knowledge and the topology gap between tabular data and text make it difficult for PLMs to yield faithful text. Low-resource generation likewise faces unique challenges in this domain. Inspired by how humans descript tabular data with prior knowledge, we suggest a new framework: PromptMize, which targets table-to-text generation under few-shot settings. The design of our framework consists of two aspects: a prompt planner and a knowledge adapter. The prompt planner aims to generate a prompt signal that provides instance guidance for PLMs to bridge the topology gap between tabular data and text. Moreover, the knowledge adapter memorizes domain-specific knowledge from the unlabelled corpus to supply essential information during generation. Extensive experiments and analyses are investigated on three open domain few-shot NLG datasets: human",
    "link": "http://arxiv.org/abs/2302.04415",
    "context": "Title: Few-Shot Table-to-Text Generation with Prompt Planning and Knowledge Memorization. (arXiv:2302.04415v3 [cs.CL] UPDATED)\nAbstract: Pre-trained language models (PLM) have achieved remarkable advancement in table-to-text generation tasks. However, the lack of labeled domain-specific knowledge and the topology gap between tabular data and text make it difficult for PLMs to yield faithful text. Low-resource generation likewise faces unique challenges in this domain. Inspired by how humans descript tabular data with prior knowledge, we suggest a new framework: PromptMize, which targets table-to-text generation under few-shot settings. The design of our framework consists of two aspects: a prompt planner and a knowledge adapter. The prompt planner aims to generate a prompt signal that provides instance guidance for PLMs to bridge the topology gap between tabular data and text. Moreover, the knowledge adapter memorizes domain-specific knowledge from the unlabelled corpus to supply essential information during generation. Extensive experiments and analyses are investigated on three open domain few-shot NLG datasets: human",
    "path": "papers/23/02/2302.04415.json",
    "total_tokens": 946,
    "translated_title": "带有提示策划和知识记忆的少样本表格到文本生成",
    "translated_abstract": "预训练语言模型在表格到文本生成任务中取得了显著进展。然而，缺乏标记的领域特定知识和表格数据与文本之间的拓扑差距使得预训练语言模型难以生成准确的文本。在低资源生成中，这个领域面临着独特的挑战。受到人类如何使用先前的知识描述表格数据的启发，我们提出了一种新的框架：PromptMize，该框架针对少样本情况下的表格到文本生成。我们的框架的设计包含两个方面：提示策划和知识适配器。提示策划的目标是生成一个提示信号，为预训练语言模型提供实例指导，以弥合表格数据和文本之间的拓扑差距。此外，知识适配器从未标记的语料库中记忆领域特定的知识，在生成过程中提供必要的信息。我们对三个开放领域的少样本自然语言生成数据集进行了大量实验和分析。",
    "tldr": "本论文提出了PromptMize框架，用于解决少样本情况下的表格到文本生成问题。该框架包含提示策划和知识适配器两个方面，通过生成提示信号和利用领域特定知识来改善文本生成结果。",
    "en_tdlr": "This paper proposes the PromptMize framework for addressing the problem of table-to-text generation under few-shot settings. The framework includes a prompt planner and a knowledge adapter, which improve text generation results by generating prompt signals and utilizing domain-specific knowledge."
}