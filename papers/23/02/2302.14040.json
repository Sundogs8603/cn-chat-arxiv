{
    "title": "Permutation Equivariant Neural Functionals. (arXiv:2302.14040v2 [cs.LG] UPDATED)",
    "abstract": "This work studies the design of neural networks that can process the weights or gradients of other neural networks, which we refer to as neural functional networks (NFNs). Despite a wide range of potential applications, including learned optimization, processing implicit neural representations, network editing, and policy evaluation, there are few unifying principles for designing effective architectures that process the weights of other networks. We approach the design of neural functionals through the lens of symmetry, in particular by focusing on the permutation symmetries that arise in the weights of deep feedforward networks because hidden layer neurons have no inherent order. We introduce a framework for building permutation equivariant neural functionals, whose architectures encode these symmetries as an inductive bias. The key building blocks of this framework are NF-Layers (neural functional layers) that we constrain to be permutation equivariant through an appropriate paramet",
    "link": "http://arxiv.org/abs/2302.14040",
    "context": "Title: Permutation Equivariant Neural Functionals. (arXiv:2302.14040v2 [cs.LG] UPDATED)\nAbstract: This work studies the design of neural networks that can process the weights or gradients of other neural networks, which we refer to as neural functional networks (NFNs). Despite a wide range of potential applications, including learned optimization, processing implicit neural representations, network editing, and policy evaluation, there are few unifying principles for designing effective architectures that process the weights of other networks. We approach the design of neural functionals through the lens of symmetry, in particular by focusing on the permutation symmetries that arise in the weights of deep feedforward networks because hidden layer neurons have no inherent order. We introduce a framework for building permutation equivariant neural functionals, whose architectures encode these symmetries as an inductive bias. The key building blocks of this framework are NF-Layers (neural functional layers) that we constrain to be permutation equivariant through an appropriate paramet",
    "path": "papers/23/02/2302.14040.json",
    "total_tokens": 818,
    "translated_title": "置换等变神经功能网络",
    "translated_abstract": "本文研究了能够处理其他神经网络的权重或梯度的神经网络的设计，我们将其称为神经功能网络（NFN）。尽管具有广泛的潜在应用，包括学习优化、处理隐式神经表示、网络编辑和策略评估，但设计处理其他网络权重的有效架构的统一原则很少。我们通过对称性的视角来设计神经功能，特别是通过关注深度前馈网络权重中出现的置换对称性，因为隐藏层神经元没有固有顺序。我们介绍了一种构建置换等变神经功能的框架，该框架将这些对称性编码为归纳偏差。该框架的关键组成部分是我们通过适当的参数来约束为置换等变的NF-Layers（神经功能层）。",
    "tldr": "本文介绍了置换等变神经功能网络的设计，通过对权重进行置换对称性编码，实现对其他网络权重或梯度进行处理，为学习优化、处理隐式神经表示等应用提供了架构原则。",
    "en_tdlr": "This paper presents the design of permutation equivariant neural functionals, which encode permutation symmetries in the weights of other neural networks, providing architectural principles for applications such as learned optimization and processing implicit neural representations."
}