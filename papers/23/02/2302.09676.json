{
    "title": "Leveraging Prior Knowledge in Reinforcement Learning via Double-Sided Bounds on the Value Function. (arXiv:2302.09676v2 [cs.LG] UPDATED)",
    "abstract": "An agent's ability to leverage past experience is critical for efficiently solving new tasks. Approximate solutions for new tasks can be obtained from previously derived value functions, as demonstrated by research on transfer learning, curriculum learning, and compositionality. However, prior work has primarily focused on using value functions to obtain zero-shot approximations for solutions to a new task. In this work, we show how an arbitrary approximation for the value function can be used to derive double-sided bounds on the optimal value function of interest. We further extend the framework with error analysis for continuous state and action spaces. The derived results lead to new approaches for clipping during training which we validate numerically in simple domains.",
    "link": "http://arxiv.org/abs/2302.09676",
    "context": "Title: Leveraging Prior Knowledge in Reinforcement Learning via Double-Sided Bounds on the Value Function. (arXiv:2302.09676v2 [cs.LG] UPDATED)\nAbstract: An agent's ability to leverage past experience is critical for efficiently solving new tasks. Approximate solutions for new tasks can be obtained from previously derived value functions, as demonstrated by research on transfer learning, curriculum learning, and compositionality. However, prior work has primarily focused on using value functions to obtain zero-shot approximations for solutions to a new task. In this work, we show how an arbitrary approximation for the value function can be used to derive double-sided bounds on the optimal value function of interest. We further extend the framework with error analysis for continuous state and action spaces. The derived results lead to new approaches for clipping during training which we validate numerically in simple domains.",
    "path": "papers/23/02/2302.09676.json",
    "total_tokens": 822,
    "translated_title": "通过对值函数的双边界限来利用先前知识在强化学习中",
    "translated_abstract": "一个代理机器人能够利用以往的经验对于高效解决新任务至关重要。先前的研究已经证明，可以从之前推导的值函数中获得新任务的近似解决方案，例如迁移学习，课程学习和组合性等。然而，先前的工作主要集中在使用值函数来获得新任务解决方案的零样本近似。在本文中，我们展示了任意近似值函数如何用来推导对感兴趣的最优值函数的双边界限。我们进一步为连续状态和动作空间的误差分析扩展了这个框架。推导出的结果为训练过程中的裁剪提供了新方法，我们在简单领域进行了数值验证。",
    "tldr": "本研究利用先前的值函数近似解决方案，提出了一种使用任意近似值函数来推导感兴趣的最优值函数的双边界限的方法，并为连续状态和动作空间的误差分析提供了新的框架，通过在简单领域进行数值验证证实了这些结果。",
    "en_tdlr": "This research proposes a method to derive double-sided bounds on the optimal value function of interest using an arbitrary approximation for the value function, and extends the framework with error analysis for continuous state and action spaces. The results are validated numerically in simple domains."
}