{
    "title": "To Be Forgotten or To Be Fair: Unveiling Fairness Implications of Machine Unlearning Methods. (arXiv:2302.03350v2 [cs.SE] UPDATED)",
    "abstract": "The right to be forgotten (RTBF) is motivated by the desire of people not to be perpetually disadvantaged by their past deeds. For this, data deletion needs to be deep and permanent, and should be removed from machine learning models. Researchers have proposed machine unlearning algorithms which aim to erase specific data from trained models more efficiently. However, these methods modify how data is fed into the model and how training is done, which may subsequently compromise AI ethics from the fairness perspective. To help software engineers make responsible decisions when adopting these unlearning methods, we present the first study on machine unlearning methods to reveal their fairness implications. We designed and conducted experiments on two typical machine unlearning methods (SISA and AmnesiacML) along with a retraining method (ORTR) as baseline using three fairness datasets under three different deletion strategies. Experimental results show that under non-uniform data deletio",
    "link": "http://arxiv.org/abs/2302.03350",
    "context": "Title: To Be Forgotten or To Be Fair: Unveiling Fairness Implications of Machine Unlearning Methods. (arXiv:2302.03350v2 [cs.SE] UPDATED)\nAbstract: The right to be forgotten (RTBF) is motivated by the desire of people not to be perpetually disadvantaged by their past deeds. For this, data deletion needs to be deep and permanent, and should be removed from machine learning models. Researchers have proposed machine unlearning algorithms which aim to erase specific data from trained models more efficiently. However, these methods modify how data is fed into the model and how training is done, which may subsequently compromise AI ethics from the fairness perspective. To help software engineers make responsible decisions when adopting these unlearning methods, we present the first study on machine unlearning methods to reveal their fairness implications. We designed and conducted experiments on two typical machine unlearning methods (SISA and AmnesiacML) along with a retraining method (ORTR) as baseline using three fairness datasets under three different deletion strategies. Experimental results show that under non-uniform data deletio",
    "path": "papers/23/02/2302.03350.json",
    "total_tokens": 898,
    "translated_title": "被遗忘还是被公平对待：揭示机器遗忘方法的公平性影响",
    "translated_abstract": "个人信息被遗忘的权利是由人们不希望自己的过去行为永久地给予不利影响的愿望所推动的。为了实现这一目标，数据删除需要彻底和永久，还必须从机器学习模型中删除。研究人员提出了机器遗忘算法，旨在更高效地从训练模型中删除特定数据。然而，这些方法改变了数据输入模型和训练方式，可能从公平性的角度妥协了人工智能伦理。为了帮助软件工程师在采用这些遗忘方法时做出负责任的决策，我们进行了首个机器遗忘方法的公平性影响研究。我们设计并进行了实验，使用三个公平性数据集和三种不同的删除策略，对两种典型的机器遗忘方法（SISA和AmnesiacML）以及重新训练方法（ORTR）进行了实验。实验结果表明，在非均匀数据删除的情况下，机器遗忘方法可能对公平性产生影响。",
    "tldr": "这项研究揭示了机器遗忘方法在公平性方面的影响，帮助软件工程师做出负责任的决策。",
    "en_tdlr": "This study reveals the fairness implications of machine unlearning methods, providing guidance for software engineers to make responsible decisions."
}