{
    "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models. (arXiv:2302.10866v3 [cs.LG] UPDATED)",
    "abstract": "Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers, indicating a gap in capability. In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. In recall and reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-spaces and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard dat",
    "link": "http://arxiv.org/abs/2302.10866",
    "context": "Title: Hyena Hierarchy: Towards Larger Convolutional Language Models. (arXiv:2302.10866v3 [cs.LG] UPDATED)\nAbstract: Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers, indicating a gap in capability. In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. In recall and reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-spaces and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard dat",
    "path": "papers/23/02/2302.10866.json",
    "total_tokens": 1025,
    "translated_title": "几千到几十万个标记的序列推理和记忆任务中的卷积语言模型 - 九斑狼等级: 迈向更大的卷积语言模型",
    "translated_abstract": "深度学习的最新进展在很大程度上依赖于大型Transformer的使用，因为它们可以在任意规模上进行学习。然而，Transformers的核心构件——注意力操作符——在长度方面呈现出二次的成本，限制了可以访问的上下文量。现有的基于低秩和稀疏逼近的亚二次方法需要与密集的注意力层结合使用来匹配Transformers，表明存在能力差距。在这项工作中，我们提出了九斑狼，一种亚二次的注意力替代品，通过交错隐式参数化的长卷积和数据控制门构造。在记忆任务和推理任务中，序列有几千到几十万个标记，九斑狼的准确度比依赖于状态空间和其他隐式和显式方法的算子提高了50以上，达到了基于注意力的模型的水平。在标准数据集上，九斑狼并不需要密集注意力的结构，就已经取得了密集注意力模型的最新结构。",
    "tldr": "本文提出一种名为九斑狼的卷积语言模型，通过交错隐式参数化的长卷积和数据控制门构造。在记忆任务和推理任务中，序列有几千到几十万个标记，九斑狼取得了比其他算子更为精确的表现，达到了基于注意力的模型的水平，并取得了密集注意力模型的最新结构。",
    "en_tdlr": "This paper proposes a convolutional language model called Hyena, which interleaves implicitly parametrized long convolutions and data-controlled gating. Compared to other operators, Hyena achieves higher accuracy for recall and reasoning tasks with sequences of thousands to hundreds of thousands of tokens, matching the level of attention-based models and outperforming dense attention models."
}