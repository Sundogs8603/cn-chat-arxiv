{
    "title": "CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code. (arXiv:2302.05527v2 [cs.SE] UPDATED)",
    "abstract": "Since the rise of neural natural-language-to-code models (NL->Code) that can generate long expressions and statements rather than a single next-token, one of the major problems has been reliably evaluating their generated output. In this paper, we propose CodeBERTScore: an evaluation metric for code generation, which builds on BERTScore (Zhang et al., 2020). Instead of encoding only the generated tokens as in BERTScore, CodeBERTScore also encodes the natural language input preceding the generated code, thus modeling the consistency between the generated code and its given natural language context as well. We perform an extensive evaluation of CodeBERTScore across four programming languages. We find that CodeBERTScore achieves a higher correlation with human preference and with functional correctness than all existing metrics. That is, generated code that receives a higher score by CodeBERTScore is more likely to be preferred by humans, as well as to function correctly when executed. We",
    "link": "http://arxiv.org/abs/2302.05527",
    "context": "Title: CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code. (arXiv:2302.05527v2 [cs.SE] UPDATED)\nAbstract: Since the rise of neural natural-language-to-code models (NL->Code) that can generate long expressions and statements rather than a single next-token, one of the major problems has been reliably evaluating their generated output. In this paper, we propose CodeBERTScore: an evaluation metric for code generation, which builds on BERTScore (Zhang et al., 2020). Instead of encoding only the generated tokens as in BERTScore, CodeBERTScore also encodes the natural language input preceding the generated code, thus modeling the consistency between the generated code and its given natural language context as well. We perform an extensive evaluation of CodeBERTScore across four programming languages. We find that CodeBERTScore achieves a higher correlation with human preference and with functional correctness than all existing metrics. That is, generated code that receives a higher score by CodeBERTScore is more likely to be preferred by humans, as well as to function correctly when executed. We",
    "path": "papers/23/02/2302.05527.json",
    "total_tokens": 846,
    "translated_title": "CodeBERTScore: 使用预训练的代码模型评估代码生成",
    "translated_abstract": "自然语言到代码模型（NL->Code）的崛起使得能够生成长表达式和语句而不仅仅是一个单独的下一个标记的模型成为可能，但是其中一个主要问题是可靠地评估它们所生成的输出。在本文中，我们提出了CodeBERTScore：一种用于代码生成的评估指标，它建立在BERTScore (Zhang et al., 2020) 的基础上。与BERTScore只对生成的标记进行编码不同，CodeBERTScore还对生成代码之前的自然语言输入进行编码，从而建模生成的代码与其所给定的自然语言上下文之间的一致性。我们对CodeBERTScore在四种编程语言上进行了广泛的评估。我们发现，CodeBERTScore与人类偏好和功能正确性的相关性比所有现有指标都更高。也就是说，CodeBERTScore评分更高的生成代码更有可能被人类青睐，并且在执行时更可能正确运行。",
    "tldr": "CodeBERTScore是一种用于代码生成的评估指标，通过对生成的代码以及其所给定的自然语言上下文进行编码，能够更可靠地评估代码生成模型的输出。",
    "en_tdlr": "CodeBERTScore is an evaluation metric for code generation that encodes both the generated code and its given natural language context, providing a more reliable evaluation of code generation models."
}