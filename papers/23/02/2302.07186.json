{
    "title": "Adversarial Rewards in Universal Learning for Contextual Bandits. (arXiv:2302.07186v2 [stat.ML] UPDATED)",
    "abstract": "We study the fundamental limits of learning in contextual bandits, where a learner's rewards depend on their actions and a known context, which extends the canonical multi-armed bandit to the case where side-information is available. We are interested in universally consistent algorithms, which achieve sublinear regret compared to any measurable fixed policy, without any function class restriction. For stationary contextual bandits, when the underlying reward mechanism is time-invariant, Blanchard et. al (2022) characterized learnable context processes for which universal consistency is achievable; and further gave algorithms ensuring universal consistency whenever this is achievable, a property known as optimistic universal consistency. It is well understood, however, that reward mechanisms can evolve over time, possibly adversarially, and depending on the learner's actions. We show that optimistic universal learning for contextual bandits with adversarial rewards is impossible in gen",
    "link": "http://arxiv.org/abs/2302.07186",
    "context": "Title: Adversarial Rewards in Universal Learning for Contextual Bandits. (arXiv:2302.07186v2 [stat.ML] UPDATED)\nAbstract: We study the fundamental limits of learning in contextual bandits, where a learner's rewards depend on their actions and a known context, which extends the canonical multi-armed bandit to the case where side-information is available. We are interested in universally consistent algorithms, which achieve sublinear regret compared to any measurable fixed policy, without any function class restriction. For stationary contextual bandits, when the underlying reward mechanism is time-invariant, Blanchard et. al (2022) characterized learnable context processes for which universal consistency is achievable; and further gave algorithms ensuring universal consistency whenever this is achievable, a property known as optimistic universal consistency. It is well understood, however, that reward mechanisms can evolve over time, possibly adversarially, and depending on the learner's actions. We show that optimistic universal learning for contextual bandits with adversarial rewards is impossible in gen",
    "path": "papers/23/02/2302.07186.json",
    "total_tokens": 792,
    "translated_title": "通用学习中对抗奖励在上下文Bandits中的应用",
    "translated_abstract": "本文研究了上下文Bandits中学习的基本极限，其中学习者的奖励取决于其行为和已知上下文，这扩展了经典的多臂赌博机，在有附加信息的情况下。我们对能够实现亚线性遗憾的通用一致性算法感兴趣，相对于任何可测定的固定策略，无需任何功能类限制。然而，奖励机制可以随着时间的推移而发生变化，我们展示了在对抗奖励下，上下文Bandits的乐观通用一致性学习是不可能的。",
    "tldr": "本文研究了在上下文Bandits中学习的基本极限，给出了关于可学习的上下文过程和通用一致性算法的特性，并讨论了对抗奖励下的乐观通用一致性学习的不可能性。",
    "en_tdlr": "The paper studies the fundamental limits of learning in contextual bandits, gives characterizations of learnable context processes and algorithms ensuring optimistic universal consistency, and discusses the impossibility of optimistic universal learning for contextual bandits with adversarial rewards."
}