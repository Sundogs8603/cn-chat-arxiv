{
    "title": "KS-DETR: Knowledge Sharing in Attention Learning for Detection Transformer. (arXiv:2302.11208v2 [cs.CV] UPDATED)",
    "abstract": "Scaled dot-product attention applies a softmax function on the scaled dot-product of queries and keys to calculate weights and then multiplies the weights and values. In this work, we study how to improve the learning of scaled dot-product attention to improve the accuracy of DETR. Our method is based on the following observations: using ground truth foreground-background mask (GT Fg-Bg Mask) as additional cues in the weights/values learning enables learning much better weights/values; with better weights/values, better values/weights can be learned. We propose a triple-attention module in which the first attention is a plain scaled dot-product attention, the second/third attention generates high-quality weights/values (with the assistance of GT Fg-Bg Mask) and shares the values/weights with the first attention to improve the quality of values/weights. The second and third attentions are removed during inference. We call our method knowledge-sharing DETR (KS-DETR), which is an extensio",
    "link": "http://arxiv.org/abs/2302.11208",
    "context": "Title: KS-DETR: Knowledge Sharing in Attention Learning for Detection Transformer. (arXiv:2302.11208v2 [cs.CV] UPDATED)\nAbstract: Scaled dot-product attention applies a softmax function on the scaled dot-product of queries and keys to calculate weights and then multiplies the weights and values. In this work, we study how to improve the learning of scaled dot-product attention to improve the accuracy of DETR. Our method is based on the following observations: using ground truth foreground-background mask (GT Fg-Bg Mask) as additional cues in the weights/values learning enables learning much better weights/values; with better weights/values, better values/weights can be learned. We propose a triple-attention module in which the first attention is a plain scaled dot-product attention, the second/third attention generates high-quality weights/values (with the assistance of GT Fg-Bg Mask) and shares the values/weights with the first attention to improve the quality of values/weights. The second and third attentions are removed during inference. We call our method knowledge-sharing DETR (KS-DETR), which is an extensio",
    "path": "papers/23/02/2302.11208.json",
    "total_tokens": 867,
    "translated_title": "KS-DETR: 知识共享的注意力学习用于检测Transformer。",
    "translated_abstract": "缩放点积注意力（scaled dot-product attention）使用标准化后的点积对查询和键进行softmax计算来计算权重，然后乘以权重和值。本文研究如何改进缩放点积注意力的学习以提高DETR的准确性。我们的方法基于以下观察：使用真实的前景-背景蒙版（GT Fg-Bg Mask）作为额外的提示能够更好地学习权重/值；通过更好的权重/值，可以学习到更好的值/权重。我们提出了一个三重注意模块，其中第一个注意力是纯粹的缩放点积注意力，第二/三个注意力通过GT Fg-Bg Mask生成高质量的权重/值，并与第一个注意力共享值/权重以提高值/权重的质量。推断时移除第二和第三个注意力。我们称该方法为知识共享DETR（KS-DETR），它是对DETR的扩展。",
    "tldr": "本文提出了一个基于缩放点积注意力的知识共享DETR方法，通过使用真实的前景-背景蒙版进行权重/值学习，进而提高了DETR的准确性。",
    "en_tdlr": "This paper proposes a knowledge-sharing DETR method based on scaled dot-product attention, which uses ground truth foreground-background mask for better weight/value learning and leads to improved accuracy of DETR."
}