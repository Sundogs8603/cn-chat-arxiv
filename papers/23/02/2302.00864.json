{
    "title": "CLIPood: Generalizing CLIP to Out-of-Distributions. (arXiv:2302.00864v2 [cs.LG] UPDATED)",
    "abstract": "Out-of-distribution (OOD) generalization, where the model needs to handle distribution shifts from training, is a major challenge of machine learning. Contrastive language-image pre-training (CLIP) models have shown impressive zero-shot ability, but the further adaptation of CLIP on downstream tasks undesirably degrades OOD performances. This paper aims at generalizing CLIP to out-of-distribution test data on downstream tasks. We propose CLIPood, a fine-tuning method that can adapt CLIP models to OOD situations where both domain shifts and open classes may occur on the unseen test data. To exploit the semantic relations between classes from the text modality, CLIPood introduces a new training objective, margin metric softmax (MMS), with class adaptive margins for fine-tuning. To incorporate both pre-trained zero-shot model and fine-tuned task-adaptive model, CLIPood leverages a new optimization strategy, Beta moving average (BMA), to maintain a temporal ensemble weighted by Beta distri",
    "link": "http://arxiv.org/abs/2302.00864",
    "context": "Title: CLIPood: Generalizing CLIP to Out-of-Distributions. (arXiv:2302.00864v2 [cs.LG] UPDATED)\nAbstract: Out-of-distribution (OOD) generalization, where the model needs to handle distribution shifts from training, is a major challenge of machine learning. Contrastive language-image pre-training (CLIP) models have shown impressive zero-shot ability, but the further adaptation of CLIP on downstream tasks undesirably degrades OOD performances. This paper aims at generalizing CLIP to out-of-distribution test data on downstream tasks. We propose CLIPood, a fine-tuning method that can adapt CLIP models to OOD situations where both domain shifts and open classes may occur on the unseen test data. To exploit the semantic relations between classes from the text modality, CLIPood introduces a new training objective, margin metric softmax (MMS), with class adaptive margins for fine-tuning. To incorporate both pre-trained zero-shot model and fine-tuned task-adaptive model, CLIPood leverages a new optimization strategy, Beta moving average (BMA), to maintain a temporal ensemble weighted by Beta distri",
    "path": "papers/23/02/2302.00864.json",
    "total_tokens": 946,
    "translated_title": "CLIPood：将CLIP泛化到超出分布",
    "translated_abstract": "机器学习中的一个重要挑战是处理训练过程中分布发生变化的情况，即超出分布（OOD）泛化问题。对比语言-图像预训练（CLIP）模型展示了令人印象深刻的零样本能力，但在后续任务中进一步调整CLIP模型会不可避免地降低OOD性能。本文旨在将CLIP泛化到超出分布测试数据的后续任务中。我们提出了CLIPood，一种可以适应OOD情况的微调方法，这些情况可能包括领域转移和未知类别在未见测试数据中出现。为了利用文本模态下类别之间的语义关系，CLIPood引入了一个新的训练目标，即边距度量软最大化（MMS），其具有类别自适应边距用于微调。为了将预训练的零样本模型和微调的任务自适应模型结合起来，CLIPood采用了一种新的优化策略，即Beta移动平均（BMA），以通过Beta分布维持一个时间上的集成加权模型。",
    "tldr": "本论文提出了CLIPood，一种将CLIP泛化到超出分布测试数据的方法。CLIPood引入了新的训练目标MMS和优化策略BMA，以适应OOD情况并提升性能。"
}