{
    "title": "Optimal Convergence Rate for Exact Policy Mirror Descent in Discounted Markov Decision Processes. (arXiv:2302.11381v2 [math.OC] UPDATED)",
    "abstract": "Policy Mirror Descent (PMD) is a general family of algorithms that covers a wide range of novel and fundamental methods in reinforcement learning. Motivated by the instability of policy iteration (PI) with inexact policy evaluation, unregularised PMD algorithmically regularises the policy improvement step of PI without regularising the objective function. With exact policy evaluation, PI is known to converge linearly with a rate given by the discount factor $\\gamma$ of a Markov Decision Process. In this work, we bridge the gap between PI and PMD with exact policy evaluation and show that the dimension-free $\\gamma$-rate of PI can be achieved by the general family of unregularised PMD algorithms under an adaptive step-size. We show that both the rate and step-size are unimprovable for PMD: we provide matching lower bounds that demonstrate that the $\\gamma$-rate is optimal for PMD methods as well as PI and that the adaptive step-size is necessary to achieve it. Our work is the first to r",
    "link": "http://arxiv.org/abs/2302.11381",
    "context": "Title: Optimal Convergence Rate for Exact Policy Mirror Descent in Discounted Markov Decision Processes. (arXiv:2302.11381v2 [math.OC] UPDATED)\nAbstract: Policy Mirror Descent (PMD) is a general family of algorithms that covers a wide range of novel and fundamental methods in reinforcement learning. Motivated by the instability of policy iteration (PI) with inexact policy evaluation, unregularised PMD algorithmically regularises the policy improvement step of PI without regularising the objective function. With exact policy evaluation, PI is known to converge linearly with a rate given by the discount factor $\\gamma$ of a Markov Decision Process. In this work, we bridge the gap between PI and PMD with exact policy evaluation and show that the dimension-free $\\gamma$-rate of PI can be achieved by the general family of unregularised PMD algorithms under an adaptive step-size. We show that both the rate and step-size are unimprovable for PMD: we provide matching lower bounds that demonstrate that the $\\gamma$-rate is optimal for PMD methods as well as PI and that the adaptive step-size is necessary to achieve it. Our work is the first to r",
    "path": "papers/23/02/2302.11381.json",
    "total_tokens": 933,
    "translated_title": "折扣马尔可夫决策过程中精确策略镜像下降算法的最优收敛速率",
    "translated_abstract": "策略镜像下降(Policy Mirror Descent，PMD)是一种广泛的算法族，包括强化学习中的一系列新颖和基础方法。本文旨在通过精确的策略评价来建立PI和PMD之间的联系，并展示一个不需要对目标函数正则化就可以对PI中的策略改进步骤进行算法正则化的未正则化PMD算法族来实现PI的维度自由的线性$\\gamma$-收敛速率。我们展示了PMD的这种速率和步长都是无法改进的：我们提供的匹配下界表明$\\gamma$-率对于PMD方法和PI方法都是最优的，并且自适应步长是实现它所必需的。我们的工作是首次在精确策略评价下研究PMD方法的最优误差速率。",
    "tldr": "本论文展示了一个不需要对目标函数正则化的未正则化PMD算法族，可以实现PI的维度自由的线性$\\gamma$收敛速率，这个速率是最优的，且自适应步长是实现这个速率所必需的。"
}