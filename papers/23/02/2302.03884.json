{
    "title": "DIFF2: Differential Private Optimization via Gradient Differences for Nonconvex Distributed Learning. (arXiv:2302.03884v2 [cs.LG] UPDATED)",
    "abstract": "Differential private optimization for nonconvex smooth objective is considered. In the previous work, the best known utility bound is $\\widetilde O(\\sqrt{d}/(n\\varepsilon_\\mathrm{DP}))$ in terms of the squared full gradient norm, which is achieved by Differential Private Gradient Descent (DP-GD) as an instance, where $n$ is the sample size, $d$ is the problem dimensionality and $\\varepsilon_\\mathrm{DP}$ is the differential privacy parameter. To improve the best known utility bound, we propose a new differential private optimization framework called \\emph{DIFF2 (DIFFerential private optimization via gradient DIFFerences)} that constructs a differential private global gradient estimator with possibly quite small variance based on communicated \\emph{gradient differences} rather than gradients themselves. It is shown that DIFF2 with a gradient descent subroutine achieves the utility of $\\widetilde O(d^{2/3}/(n\\varepsilon_\\mathrm{DP})^{4/3})$, which can be significantly better than the prev",
    "link": "http://arxiv.org/abs/2302.03884",
    "context": "Title: DIFF2: Differential Private Optimization via Gradient Differences for Nonconvex Distributed Learning. (arXiv:2302.03884v2 [cs.LG] UPDATED)\nAbstract: Differential private optimization for nonconvex smooth objective is considered. In the previous work, the best known utility bound is $\\widetilde O(\\sqrt{d}/(n\\varepsilon_\\mathrm{DP}))$ in terms of the squared full gradient norm, which is achieved by Differential Private Gradient Descent (DP-GD) as an instance, where $n$ is the sample size, $d$ is the problem dimensionality and $\\varepsilon_\\mathrm{DP}$ is the differential privacy parameter. To improve the best known utility bound, we propose a new differential private optimization framework called \\emph{DIFF2 (DIFFerential private optimization via gradient DIFFerences)} that constructs a differential private global gradient estimator with possibly quite small variance based on communicated \\emph{gradient differences} rather than gradients themselves. It is shown that DIFF2 with a gradient descent subroutine achieves the utility of $\\widetilde O(d^{2/3}/(n\\varepsilon_\\mathrm{DP})^{4/3})$, which can be significantly better than the prev",
    "path": "papers/23/02/2302.03884.json",
    "total_tokens": 1017,
    "translated_title": "DIFF2: 基于梯度差异的差分隐私优化方法用于非凸分布式学习",
    "translated_abstract": "本文考虑了非凸平滑目标的差分隐私优化问题。在以往的工作中，基于全梯度范数的最佳效用边界已知为 $\\widetilde O(\\sqrt{d}/(n\\varepsilon_\\mathrm{DP}))$，其中 $n$ 为样本大小，$d$ 为问题维度，$\\varepsilon_\\mathrm{DP}$ 为差分隐私参数，DP-GD 是实现该边界的一种算法。为了提高已知的效用边界，本文提出了一种新的差分隐私优化框架 DIFF2（DIFFerential private optimization via gradient DIFFerences），它构造了一个差分隐私全局梯度估计器，该估计器具有可能非常小的方差，基于通信的“梯度差异”，而不是梯度本身。实验证明，使用梯度下降子例程的 DIFF2 实现的效用为 $\\widetilde O(d^{2/3}/(n\\varepsilon_\\mathrm{DP})^{4/3})$，比先前的方法显著更好。",
    "tldr": "本文提出了一种名为 DIFF2 的新型差分隐私优化框架，其以梯度差异为基础构造了一个具有小方差的全局梯度估计器，相比于之前的算法，该方法在非凸平滑目标优化问题中能够更好地减小误差下界，提高效用。"
}