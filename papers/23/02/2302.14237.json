{
    "title": "Towards Surgical Context Inference and Translation to Gestures. (arXiv:2302.14237v2 [cs.CV] UPDATED)",
    "abstract": "Manual labeling of gestures in robot-assisted surgery is labor intensive, prone to errors, and requires expertise or training. We propose a method for automated and explainable generation of gesture transcripts that leverages the abundance of data for image segmentation. Surgical context is detected using segmentation masks by examining the distances and intersections between the tools and objects. Next, context labels are translated into gesture transcripts using knowledge-based Finite State Machine (FSM) and data-driven Long Short Term Memory (LSTM) models. We evaluate the performance of each stage of our method by comparing the results with the ground truth segmentation masks, the consensus context labels, and the gesture labels in the JIGSAWS dataset. Our results show that our segmentation models achieve state-of-the-art performance in recognizing needle and thread in Suturing and we can automatically detect important surgical states with high agreement with crowd-sourced labels (e",
    "link": "http://arxiv.org/abs/2302.14237",
    "context": "Title: Towards Surgical Context Inference and Translation to Gestures. (arXiv:2302.14237v2 [cs.CV] UPDATED)\nAbstract: Manual labeling of gestures in robot-assisted surgery is labor intensive, prone to errors, and requires expertise or training. We propose a method for automated and explainable generation of gesture transcripts that leverages the abundance of data for image segmentation. Surgical context is detected using segmentation masks by examining the distances and intersections between the tools and objects. Next, context labels are translated into gesture transcripts using knowledge-based Finite State Machine (FSM) and data-driven Long Short Term Memory (LSTM) models. We evaluate the performance of each stage of our method by comparing the results with the ground truth segmentation masks, the consensus context labels, and the gesture labels in the JIGSAWS dataset. Our results show that our segmentation models achieve state-of-the-art performance in recognizing needle and thread in Suturing and we can automatically detect important surgical states with high agreement with crowd-sourced labels (e",
    "path": "papers/23/02/2302.14237.json",
    "total_tokens": 924,
    "translated_title": "实现手术背景推理与手势转换的方法",
    "translated_abstract": "机器人辅助手术中手势的手动标注工作繁琐且容易出错，需要专业知识和培训。本文提出了一种自动推理手势并可解释生成手势转录的方法，利用图像分割的数据丰富性。通过检查工具和物体之间的距离和交点，使用分割蒙版检测手术背景。然后，使用基于知识的有限状态机和数据驱动的长期短期记忆模型将背景标签转换为手势转录。通过将结果与JIGSAWS数据集中的地面真实分割蒙版、共识背景标签和手势标签进行比较，评估了我们方法的每个阶段的表现。我们的结果表明，我们的分割模型在缝合时识别针和线的性能达到了最新水平，并且我们可以高度一致地自动检测重要的手术状态。",
    "tldr": "本研究提出了一种自动推理手势并可解释生成手势转录的方法，利用图像分割的数据丰富性。通过检查工具和物体之间的距离和交点，使用分割蒙版检测手术背景。本方法可以高度一致地自动检测重要的手术状态，对机器人辅助手术具有实际应用价值。",
    "en_tdlr": "This paper proposes a method for automated and explainable generation of gesture transcripts using image segmentation, which detects surgical context by examining the distances and intersections between tools and objects. The method can automatically detect important surgical states and has practical value for robot-assisted surgery."
}