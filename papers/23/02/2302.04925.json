{
    "title": "Information Theoretic Lower Bounds for Information Theoretic Upper Bounds. (arXiv:2302.04925v2 [cs.LG] UPDATED)",
    "abstract": "We examine the relationship between the mutual information between the output model and the empirical sample and the generalization of the algorithm in the context of stochastic convex optimization. Despite increasing interest in information-theoretic generalization bounds, it is uncertain if these bounds can provide insight into the exceptional performance of various learning algorithms. Our study of stochastic convex optimization reveals that, for true risk minimization, dimension-dependent mutual information is necessary. This indicates that existing information-theoretic generalization bounds fall short in capturing the generalization capabilities of algorithms like SGD and regularized ERM, which have dimension-independent sample complexity.",
    "link": "http://arxiv.org/abs/2302.04925",
    "context": "Title: Information Theoretic Lower Bounds for Information Theoretic Upper Bounds. (arXiv:2302.04925v2 [cs.LG] UPDATED)\nAbstract: We examine the relationship between the mutual information between the output model and the empirical sample and the generalization of the algorithm in the context of stochastic convex optimization. Despite increasing interest in information-theoretic generalization bounds, it is uncertain if these bounds can provide insight into the exceptional performance of various learning algorithms. Our study of stochastic convex optimization reveals that, for true risk minimization, dimension-dependent mutual information is necessary. This indicates that existing information-theoretic generalization bounds fall short in capturing the generalization capabilities of algorithms like SGD and regularized ERM, which have dimension-independent sample complexity.",
    "path": "papers/23/02/2302.04925.json",
    "total_tokens": 780,
    "translated_title": "信息理论上界对信息理论下界的贡献",
    "translated_abstract": "本文在随机凸优化的背景下研究了输出模型和经验样本之间的互信息与算法的泛化之间的关系。尽管对信息理论泛化界限的兴趣日益增加，但这些界限能否揭示各种学习算法的卓越性能还不确定。我们对随机凸优化的研究表明，对于真正的风险最小化，依赖于维度的互信息是必要的。这表明现有的信息理论泛化界限不能完全捕捉到像SGD和正则化ERM这样具有维度无关样本复杂度的算法的泛化能力。",
    "tldr": "本文研究了在随机凸优化中，输出模型和经验样本之间的互信息与算法泛化之间的关系。研究结果表明，现有的信息理论泛化界限不足以捕捉到像SGD和正则化ERM这样具有维度无关样本复杂度的算法的泛化能力。",
    "en_tdlr": "This paper examines the relationship between the mutual information between the output model and the empirical sample and the generalization of the algorithm in the context of stochastic convex optimization. The study reveals that existing information-theoretic generalization bounds fall short in capturing the generalization capabilities of algorithms with dimension-independent sample complexity like SGD and regularized ERM."
}