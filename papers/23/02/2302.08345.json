{
    "title": "Linear Bandits with Memory: from Rotting to Rising. (arXiv:2302.08345v2 [cs.LG] UPDATED)",
    "abstract": "Nonstationary phenomena, such as satiation effects in recommendations, have mostly been modeled using bandits with finitely many arms. However, the richer action space provided by linear bandits is often preferred in practice. In this work, we introduce a novel nonstationary linear bandit model, where current rewards are influenced by the learner's past actions in a fixed-size window. Our model, which recovers stationary linear bandits as a special case, leverages two parameters: the window size $m \\ge 0$, and an exponent $\\gamma$ that captures the rotting ($\\gamma < 0)$ or rising ($\\gamma > 0$) nature of the phenomenon. When both $m$ and $\\gamma$ are known, we propose and analyze a variant of OFUL which minimizes regret against cycling policies. By choosing the cycle length so as to trade-off approximation and estimation errors, we then prove a bound of order $\\sqrt{d}\\,(m+1)^{\\frac{1}{2}+\\max\\{\\gamma,0\\}}\\,T^{3/4}$ (ignoring log factors) on the regret against the optimal sequence of ",
    "link": "http://arxiv.org/abs/2302.08345",
    "context": "Title: Linear Bandits with Memory: from Rotting to Rising. (arXiv:2302.08345v2 [cs.LG] UPDATED)\nAbstract: Nonstationary phenomena, such as satiation effects in recommendations, have mostly been modeled using bandits with finitely many arms. However, the richer action space provided by linear bandits is often preferred in practice. In this work, we introduce a novel nonstationary linear bandit model, where current rewards are influenced by the learner's past actions in a fixed-size window. Our model, which recovers stationary linear bandits as a special case, leverages two parameters: the window size $m \\ge 0$, and an exponent $\\gamma$ that captures the rotting ($\\gamma < 0)$ or rising ($\\gamma > 0$) nature of the phenomenon. When both $m$ and $\\gamma$ are known, we propose and analyze a variant of OFUL which minimizes regret against cycling policies. By choosing the cycle length so as to trade-off approximation and estimation errors, we then prove a bound of order $\\sqrt{d}\\,(m+1)^{\\frac{1}{2}+\\max\\{\\gamma,0\\}}\\,T^{3/4}$ (ignoring log factors) on the regret against the optimal sequence of ",
    "path": "papers/23/02/2302.08345.json",
    "total_tokens": 1107,
    "translated_title": "带内存的线性赌臂：从衰退到崛起",
    "translated_abstract": "非平稳现象（如推荐中的饱和效应）大多使用有限臂赌臂来建模。然而，在实践中，通常更喜欢使用具有更丰富行动空间的线性赌臂。在本文中，我们引入了一种新的非平稳线性赌臂模型，其中当前奖励受学习者以固定大小的窗口内的先前操作的影响。我们的模型，作为特殊情况恢复平稳的线性赌臂，利用两个参数：窗口大小$ m \\geq 0 $和指数$ \\gamma $，它捕捉现象的衰退（$ \\gamma <0 $）或崛起（$ \\gamma> 0 $）的性质。当同时知道$ m $和$ \\gamma $时，我们提出并分析了一种针对循环策略最小化遗憾的OFUL变体。通过选择循环长度以在逼近和估计误差之间进行权衡，我们证明了对最佳序列的遗憾大小为$\\sqrt{d}\\,(m+1)^{\\frac{1}{2}+\\max\\{\\gamma,0\\}}\\,T^{3/4}$ （忽略对数因子）。",
    "tldr": "本文引入了一种新型非平稳线性赌臂模型，结合操作历史信息且具有两个参数。它以恢复平稳的线性赌臂作为特殊情况，且在已知窗口大小和指数的情况下，提出了一个针对循环策略最小化遗憾的OFUL变体。",
    "en_tdlr": "This paper introduces a novel non-stationary linear bandit model with two parameters that leverages historical information and recovers stationary linear bandits as a special case. It proposes and analyzes a variant of OFUL that minimizes regret against cycling policies, and proves a regret bound of order $\\sqrt{d}\\,(m+1)^{\\frac{1}{2}+\\max\\{\\gamma,0\\}}\\,T^{3/4}$ (ignoring log factors) against the optimal sequence of actions when both $m$ and $\\gamma$ are known."
}