{
    "title": "Mitigating Adversarial Attacks in Deepfake Detection: An Exploration of Perturbation and AI Techniques. (arXiv:2302.11704v2 [cs.LG] UPDATED)",
    "abstract": "Deep learning constitutes a pivotal component within the realm of machine learning, offering remarkable capabilities in tasks ranging from image recognition to natural language processing. However, this very strength also renders deep learning models susceptible to adversarial examples, a phenomenon pervasive across a diverse array of applications. These adversarial examples are characterized by subtle perturbations artfully injected into clean images or videos, thereby causing deep learning algorithms to misclassify or produce erroneous outputs. This susceptibility extends beyond the confines of digital domains, as adversarial examples can also be strategically designed to target human cognition, leading to the creation of deceptive media, such as deepfakes. Deepfakes, in particular, have emerged as a potent tool to manipulate public opinion and tarnish the reputations of public figures, underscoring the urgent need to address the security and ethical implications associated with adve",
    "link": "http://arxiv.org/abs/2302.11704",
    "context": "Title: Mitigating Adversarial Attacks in Deepfake Detection: An Exploration of Perturbation and AI Techniques. (arXiv:2302.11704v2 [cs.LG] UPDATED)\nAbstract: Deep learning constitutes a pivotal component within the realm of machine learning, offering remarkable capabilities in tasks ranging from image recognition to natural language processing. However, this very strength also renders deep learning models susceptible to adversarial examples, a phenomenon pervasive across a diverse array of applications. These adversarial examples are characterized by subtle perturbations artfully injected into clean images or videos, thereby causing deep learning algorithms to misclassify or produce erroneous outputs. This susceptibility extends beyond the confines of digital domains, as adversarial examples can also be strategically designed to target human cognition, leading to the creation of deceptive media, such as deepfakes. Deepfakes, in particular, have emerged as a potent tool to manipulate public opinion and tarnish the reputations of public figures, underscoring the urgent need to address the security and ethical implications associated with adve",
    "path": "papers/23/02/2302.11704.json",
    "total_tokens": 877,
    "translated_title": "缓解深假检测中的对抗攻击：对扰动和人工智能技术的探索",
    "translated_abstract": "深度学习在机器学习领域起着重要作用，具有识别图像、自然语言处理等任务的显著能力。然而，这种强大的特性也使得深度学习模型容易受到对抗性示例的攻击，这种现象在各种应用中普遍存在。这些对抗性示例通过巧妙地注入微小的扰动到清晰的图像或视频中，从而导致深度学习算法误分类或产生错误输出。这种易受攻击的情况不仅局限于数字领域，因为对抗性示例也可以被精心设计来针对人类认知，从而产生欺骗性媒体，如深度伪造。特别是，深度伪造已经成为一种操控舆论和破坏公众人物声誉的有力工具，这凸显了需要解决与对抗攻击相关的安全和道德问题的紧迫性。",
    "tldr": "本论文探索了扰动和人工智能技术在深假检测中缓解对抗攻击的方法，这是一项迫切需要解决的安全和道德问题。"
}