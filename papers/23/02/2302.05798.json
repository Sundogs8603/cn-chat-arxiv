{
    "title": "Optimizing Orthogonalized Tensor Deflation via Random Tensor Theory. (arXiv:2302.05798v2 [stat.ML] UPDATED)",
    "abstract": "This paper tackles the problem of recovering a low-rank signal tensor with possibly correlated components from a random noisy tensor, or so-called spiked tensor model. When the underlying components are orthogonal, they can be recovered efficiently using tensor deflation which consists of successive rank-one approximations, while non-orthogonal components may alter the tensor deflation mechanism, thereby preventing efficient recovery. Relying on recently developed random tensor tools, this paper deals precisely with the non-orthogonal case by deriving an asymptotic analysis of a parameterized deflation procedure performed on an order-three and rank-two spiked tensor. Based on this analysis, an efficient tensor deflation algorithm is proposed by optimizing the parameter introduced in the deflation mechanism, which in turn is proven to be optimal by construction for the studied tensor model. The same ideas could be extended to more general low-rank tensor models, e.g., higher ranks and o",
    "link": "http://arxiv.org/abs/2302.05798",
    "context": "Title: Optimizing Orthogonalized Tensor Deflation via Random Tensor Theory. (arXiv:2302.05798v2 [stat.ML] UPDATED)\nAbstract: This paper tackles the problem of recovering a low-rank signal tensor with possibly correlated components from a random noisy tensor, or so-called spiked tensor model. When the underlying components are orthogonal, they can be recovered efficiently using tensor deflation which consists of successive rank-one approximations, while non-orthogonal components may alter the tensor deflation mechanism, thereby preventing efficient recovery. Relying on recently developed random tensor tools, this paper deals precisely with the non-orthogonal case by deriving an asymptotic analysis of a parameterized deflation procedure performed on an order-three and rank-two spiked tensor. Based on this analysis, an efficient tensor deflation algorithm is proposed by optimizing the parameter introduced in the deflation mechanism, which in turn is proven to be optimal by construction for the studied tensor model. The same ideas could be extended to more general low-rank tensor models, e.g., higher ranks and o",
    "path": "papers/23/02/2302.05798.json",
    "total_tokens": 842,
    "translated_title": "随机张量理论优化正交张量缩减",
    "translated_abstract": "本文研究了如何从随机噪声张量中恢复可能存在相关部分的低秩信号张量问题。当底层部分是正交的时，可以通过张量缩减（由一系列秩为一的逼近组成）高效地进行恢复，而非正交的部分可能会影响张量缩减机制，从而导致恢复效率低下。基于最近发展的随机张量工具，本文通过对一个三阶、秩为二的尖峰张量的参数化缩减过程进行渐近分析，精确地处理了非正交情况。基于该分析，提出了一种通过优化缩减机制中引入的参数来进行高效张量缩减的算法，并且根据构造证明在所研究的张量模型下最优。相同的思想也可以扩展到更一般的低秩张量模型，例如更高阶和其他类型的约束条件。",
    "tldr": "本文提出了一种通过优化参数引入的正交张量缩减机制，以高效地从带噪声的张量中恢复相关低秩信号。",
    "en_tdlr": "This paper proposes an efficient tensor deflation algorithm by optimizing the parameter introduced in the deflation mechanism, which can recover low-rank signal tensors with possibly correlated components from noisy tensors."
}