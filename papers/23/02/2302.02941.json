{
    "title": "On Over-Squashing in Message Passing Neural Networks: The Impact of Width, Depth, and Topology. (arXiv:2302.02941v2 [cs.LG] UPDATED)",
    "abstract": "Message Passing Neural Networks (MPNNs) are instances of Graph Neural Networks that leverage the graph to send messages over the edges. This inductive bias leads to a phenomenon known as over-squashing, where a node feature is insensitive to information contained at distant nodes. Despite recent methods introduced to mitigate this issue, an understanding of the causes for over-squashing and of possible solutions are lacking. In this theoretical work, we prove that: (i) Neural network width can mitigate over-squashing, but at the cost of making the whole network more sensitive; (ii) Conversely, depth cannot help mitigate over-squashing: increasing the number of layers leads to over-squashing being dominated by vanishing gradients; (iii) The graph topology plays the greatest role, since over-squashing occurs between nodes at high commute (access) time. Our analysis provides a unified framework to study different recent methods introduced to cope with over-squashing and serves as a justif",
    "link": "http://arxiv.org/abs/2302.02941",
    "context": "Title: On Over-Squashing in Message Passing Neural Networks: The Impact of Width, Depth, and Topology. (arXiv:2302.02941v2 [cs.LG] UPDATED)\nAbstract: Message Passing Neural Networks (MPNNs) are instances of Graph Neural Networks that leverage the graph to send messages over the edges. This inductive bias leads to a phenomenon known as over-squashing, where a node feature is insensitive to information contained at distant nodes. Despite recent methods introduced to mitigate this issue, an understanding of the causes for over-squashing and of possible solutions are lacking. In this theoretical work, we prove that: (i) Neural network width can mitigate over-squashing, but at the cost of making the whole network more sensitive; (ii) Conversely, depth cannot help mitigate over-squashing: increasing the number of layers leads to over-squashing being dominated by vanishing gradients; (iii) The graph topology plays the greatest role, since over-squashing occurs between nodes at high commute (access) time. Our analysis provides a unified framework to study different recent methods introduced to cope with over-squashing and serves as a justif",
    "path": "papers/23/02/2302.02941.json",
    "total_tokens": 956,
    "tldr": "本文针对消息传递神经网络中的过度压缩问题进行了理论探讨，证明了网络宽度可以缓解这一问题，而深度则不能，同时图的拓扑结构在该问题中起着最大的作用。"
}