{
    "title": "CADIS: Handling Cluster-skewed Non-IID Data in Federated Learning with Clustered Aggregation and Knowledge DIStilled Regularization. (arXiv:2302.10413v2 [cs.LG] UPDATED)",
    "abstract": "Federated learning enables edge devices to train a global model collaboratively without exposing their data. Despite achieving outstanding advantages in computing efficiency and privacy protection, federated learning faces a significant challenge when dealing with non-IID data, i.e., data generated by clients that are typically not independent and identically distributed. In this paper, we tackle a new type of Non-IID data, called cluster-skewed non-IID, discovered in actual data sets. The cluster-skewed non-IID is a phenomenon in which clients can be grouped into clusters with similar data distributions. By performing an in-depth analysis of the behavior of a classification model's penultimate layer, we introduce a metric that quantifies the similarity between two clients' data distributions without violating their privacy. We then propose an aggregation scheme that guarantees equality between clusters. In addition, we offer a novel local training regularization based on the knowledge",
    "link": "http://arxiv.org/abs/2302.10413",
    "context": "Title: CADIS: Handling Cluster-skewed Non-IID Data in Federated Learning with Clustered Aggregation and Knowledge DIStilled Regularization. (arXiv:2302.10413v2 [cs.LG] UPDATED)\nAbstract: Federated learning enables edge devices to train a global model collaboratively without exposing their data. Despite achieving outstanding advantages in computing efficiency and privacy protection, federated learning faces a significant challenge when dealing with non-IID data, i.e., data generated by clients that are typically not independent and identically distributed. In this paper, we tackle a new type of Non-IID data, called cluster-skewed non-IID, discovered in actual data sets. The cluster-skewed non-IID is a phenomenon in which clients can be grouped into clusters with similar data distributions. By performing an in-depth analysis of the behavior of a classification model's penultimate layer, we introduce a metric that quantifies the similarity between two clients' data distributions without violating their privacy. We then propose an aggregation scheme that guarantees equality between clusters. In addition, we offer a novel local training regularization based on the knowledge",
    "path": "papers/23/02/2302.10413.json",
    "total_tokens": 895,
    "translated_title": "CADIS：采用聚类聚合和知识蒸馏正则化处理联邦学习中的聚类偏斜非独立同分布数据",
    "translated_abstract": "联邦学习使得边缘设备能够协作地训练全局模型，而不暴露它们的数据。然而，在处理非独立同分布数据（IID）时，即由通常不独立且同分布的客户端生成的数据时，联邦学习面临重大挑战。本文针对实际数据集中发现的一种新型非IID数据，称为聚类偏斜非IID进行研究。这种数据现象是指客户端可以被分成具有相似数据分布的群组。通过对分类模型的次级层行为进行深入分析，我们引入了一种度量方法来量化两个客户端数据分布的相似度，同时不违反其隐私权。然后，我们提出了一种聚合方案，确保不同群组之间的平等。此外，我们还提出了一种基于知识蒸馏的新型局部训练正则化方法，",
    "tldr": "本文提出了一种针对聚类偏斜非独立同分布数据的联邦学习聚合方案和基于知识蒸馏的局部训练正则化方法"
}