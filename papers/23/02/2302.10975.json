{
    "title": "Improved uncertainty quantification for neural networks with Bayesian last layer. (arXiv:2302.10975v2 [cs.LG] UPDATED)",
    "abstract": "Uncertainty quantification is an essential task in machine learning - a task in which neural networks (NNs) have traditionally not excelled. This can be a limitation for safety-critical applications, where uncertainty-aware methods like Gaussian processes or Bayesian linear regression are often preferred. Bayesian neural networks are an approach to address this limitation. They assume probability distributions for all parameters and yield distributed predictions. However, training and inference are typically intractable and approximations must be employed. A promising approximation is NNs with Bayesian last layer (BLL). They assume distributed weights only in the last linear layer and yield a normally distributed prediction. NNs with BLL can be seen as a Bayesian linear regression model with learned nonlinear features. To approximate the intractable Bayesian neural network, point estimates of the distributed weights in all but the last layer should be obtained by maximizing the margina",
    "link": "http://arxiv.org/abs/2302.10975",
    "context": "Title: Improved uncertainty quantification for neural networks with Bayesian last layer. (arXiv:2302.10975v2 [cs.LG] UPDATED)\nAbstract: Uncertainty quantification is an essential task in machine learning - a task in which neural networks (NNs) have traditionally not excelled. This can be a limitation for safety-critical applications, where uncertainty-aware methods like Gaussian processes or Bayesian linear regression are often preferred. Bayesian neural networks are an approach to address this limitation. They assume probability distributions for all parameters and yield distributed predictions. However, training and inference are typically intractable and approximations must be employed. A promising approximation is NNs with Bayesian last layer (BLL). They assume distributed weights only in the last linear layer and yield a normally distributed prediction. NNs with BLL can be seen as a Bayesian linear regression model with learned nonlinear features. To approximate the intractable Bayesian neural network, point estimates of the distributed weights in all but the last layer should be obtained by maximizing the margina",
    "path": "papers/23/02/2302.10975.json",
    "total_tokens": 894,
    "translated_title": "使用贝叶斯最后一层改进神经网络的不确定性量化",
    "translated_abstract": "不确定性量化是机器学习中的重要任务，神经网络在这方面通常表现不佳。这对于安全关键应用来说是一个限制，因此常常倾向于使用能够感知不确定性的方法，如高斯过程或贝叶斯线性回归。贝叶斯神经网络是一种解决这个问题的方法，它假设所有参数都服从概率分布，并产生分布预测。然而，训练和推断通常是不可处理的，需要使用近似方法。一种有前景的近似方法是具有贝叶斯最后一层的神经网络。他们仅在最后一个线性层中假设分布权重，并产生一个正态分布的预测。具有贝叶斯最后一层的神经网络可以看作是具有学习非线性特征的贝叶斯线性回归模型。为了近似不可处理的贝叶斯神经网络，应通过最大化边际来获得除最后一层以外的所有分布权重的点估计。",
    "tldr": "本文提出了一种改进神经网络不确定性量化的方法，使用贝叶斯最后一层近似不可处理的贝叶斯神经网络，通过最大化边际来获得权重的点估计。",
    "en_tdlr": "This paper presents an improved method for uncertainty quantification in neural networks by using Bayesian last layer to approximate the intractable Bayesian neural network, obtaining point estimates of the weights by maximizing the marginal."
}