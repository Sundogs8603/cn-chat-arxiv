{
    "title": "AUTOLYCUS: Exploiting Explainable AI (XAI) for Model Extraction Attacks against White-Box Models. (arXiv:2302.02162v2 [cs.LG] UPDATED)",
    "abstract": "Explainable Artificial Intelligence (XAI) encompasses a range of techniques and procedures aimed at elucidating the decision-making processes of AI models. While XAI is valuable in understanding the reasoning behind AI models, the data used for such revelations poses potential security and privacy vulnerabilities. Existing literature has identified privacy risks targeting machine learning models, including membership inference, model inversion, and model extraction attacks. Depending on the settings and parties involved, such attacks may target either the model itself or the training data used to create the model.  We have identified that tools providing XAI can particularly increase the vulnerability of model extraction attacks, which can be a significant issue when the owner of an AI model prefers to provide only black-box access rather than sharing the model parameters and architecture with other parties. To explore this privacy risk, we propose AUTOLYCUS, a model extraction attack ",
    "link": "http://arxiv.org/abs/2302.02162",
    "context": "Title: AUTOLYCUS: Exploiting Explainable AI (XAI) for Model Extraction Attacks against White-Box Models. (arXiv:2302.02162v2 [cs.LG] UPDATED)\nAbstract: Explainable Artificial Intelligence (XAI) encompasses a range of techniques and procedures aimed at elucidating the decision-making processes of AI models. While XAI is valuable in understanding the reasoning behind AI models, the data used for such revelations poses potential security and privacy vulnerabilities. Existing literature has identified privacy risks targeting machine learning models, including membership inference, model inversion, and model extraction attacks. Depending on the settings and parties involved, such attacks may target either the model itself or the training data used to create the model.  We have identified that tools providing XAI can particularly increase the vulnerability of model extraction attacks, which can be a significant issue when the owner of an AI model prefers to provide only black-box access rather than sharing the model parameters and architecture with other parties. To explore this privacy risk, we propose AUTOLYCUS, a model extraction attack ",
    "path": "papers/23/02/2302.02162.json",
    "total_tokens": 887,
    "translated_title": "AUTOLYCUS: 利用可解释人工智能（XAI）对白盒模型进行模型提取攻击",
    "translated_abstract": "可解释的人工智能（XAI）涵盖了一系列旨在阐明AI模型决策过程的技术和程序。虽然XAI对于理解AI模型的推理过程很有价值，但用于这种揭示的数据会带来潜在的安全和隐私漏洞。现有文献已经确定了针对机器学习模型的隐私风险，包括成员推论、模型反演和模型提取攻击。根据涉及的设置和各方，这些攻击可能针对模型本身或用于创建模型的训练数据。我们认为提供XAI的工具特别会增加模型提取攻击的风险，这可能是一个重要问题，当AI模型的所有者仅愿提供黑盒访问而不与其他方共享模型参数和结构时。为了探究这种隐私风险，我们提出了AUTOLYCUS，一种模型提取攻击方法。",
    "tldr": "本文探究了可解释人工智能（XAI）工具对机器学习模型提取攻击的风险，并提出了一种模型提取攻击方法AUTOLYCUS。"
}