{
    "title": "Cyclic and Randomized Stepsizes Invoke Heavier Tails in SGD than Constant Stepsize. (arXiv:2302.05516v2 [stat.ML] UPDATED)",
    "abstract": "Cyclic and randomized stepsizes are widely used in the deep learning practice and can often outperform standard stepsize choices such as constant stepsize in SGD. Despite their empirical success, not much is currently known about when and why they can theoretically improve the generalization performance. We consider a general class of Markovian stepsizes for learning, which contain i.i.d. random stepsize, cyclic stepsize as well as the constant stepsize as special cases, and motivated by the literature which shows that heaviness of the tails (measured by the so-called \"tail-index\") in the SGD iterates is correlated with generalization, we study tail-index and provide a number of theoretical results that demonstrate how the tail-index varies on the stepsize scheduling. Our results bring a new understanding of the benefits of cyclic and randomized stepsizes compared to constant stepsize in terms of the tail behavior. We illustrate our theory on linear regression experiments and show thro",
    "link": "http://arxiv.org/abs/2302.05516",
    "context": "Title: Cyclic and Randomized Stepsizes Invoke Heavier Tails in SGD than Constant Stepsize. (arXiv:2302.05516v2 [stat.ML] UPDATED)\nAbstract: Cyclic and randomized stepsizes are widely used in the deep learning practice and can often outperform standard stepsize choices such as constant stepsize in SGD. Despite their empirical success, not much is currently known about when and why they can theoretically improve the generalization performance. We consider a general class of Markovian stepsizes for learning, which contain i.i.d. random stepsize, cyclic stepsize as well as the constant stepsize as special cases, and motivated by the literature which shows that heaviness of the tails (measured by the so-called \"tail-index\") in the SGD iterates is correlated with generalization, we study tail-index and provide a number of theoretical results that demonstrate how the tail-index varies on the stepsize scheduling. Our results bring a new understanding of the benefits of cyclic and randomized stepsizes compared to constant stepsize in terms of the tail behavior. We illustrate our theory on linear regression experiments and show thro",
    "path": "papers/23/02/2302.05516.json",
    "total_tokens": 952,
    "translated_title": "随机步长和循环步长引发了比常数步长更重的SGD尾部",
    "translated_abstract": "随机步长和循环步长在深度学习实践中被广泛使用，并且通常可以胜过常数步长选择，如SGD中的常数步长。尽管它们在经验上获得成功，但目前对于它们何时以及为什么能在理论上提高泛化性能的了解还不够。我们考虑了一类关于学习的马尔可夫步长，其中包含独立同分布的随机步长、循环步长以及常数步长等特殊情况。受到文献中显示的SGD迭代的尾部（通过所谓的“尾部指数”来衡量）的重尾现象与泛化的相关性，我们研究了尾部指数，并提供了一些理论结果，展示了尾部指数在步长调度上的变化。我们的结果为从尾部行为的角度上讨论循环和随机步长相对于常数步长的好处提供了新的理解。我们通过线性回归实验证明了我们的理论，并展示了结果。",
    "tldr": "本文研究了深度学习中的随机步长和循环步长相对于常数步长的优势，通过考虑尾部指数如何随步长调度的变化而变化来推动理论研究，进一步揭示了它们对于泛化性能的改善机制。"
}