{
    "title": "Cross-Modal Fine-Tuning: Align then Refine. (arXiv:2302.05738v2 [cs.LG] UPDATED)",
    "abstract": "Fine-tuning large-scale pretrained models has led to tremendous progress in well-studied modalities such as vision and NLP. However, similar gains have not been observed in many other modalities due to a lack of relevant pretrained models. In this work, we propose ORCA, a general cross-modal fine-tuning framework that extends the applicability of a single large-scale pretrained model to diverse modalities. ORCA adapts to a target task via an align-then-refine workflow: given the target input, ORCA first learns an embedding network that aligns the embedded feature distribution with the pretraining modality. The pretrained model is then fine-tuned on the embedded data to exploit the knowledge shared across modalities. Through extensive experiments, we show that ORCA obtains state-of-the-art results on 3 benchmarks containing over 60 datasets from 12 modalities, outperforming a wide range of hand-designed, AutoML, general-purpose, and task-specific methods. We highlight the importance of ",
    "link": "http://arxiv.org/abs/2302.05738",
    "context": "Title: Cross-Modal Fine-Tuning: Align then Refine. (arXiv:2302.05738v2 [cs.LG] UPDATED)\nAbstract: Fine-tuning large-scale pretrained models has led to tremendous progress in well-studied modalities such as vision and NLP. However, similar gains have not been observed in many other modalities due to a lack of relevant pretrained models. In this work, we propose ORCA, a general cross-modal fine-tuning framework that extends the applicability of a single large-scale pretrained model to diverse modalities. ORCA adapts to a target task via an align-then-refine workflow: given the target input, ORCA first learns an embedding network that aligns the embedded feature distribution with the pretraining modality. The pretrained model is then fine-tuned on the embedded data to exploit the knowledge shared across modalities. Through extensive experiments, we show that ORCA obtains state-of-the-art results on 3 benchmarks containing over 60 datasets from 12 modalities, outperforming a wide range of hand-designed, AutoML, general-purpose, and task-specific methods. We highlight the importance of ",
    "path": "papers/23/02/2302.05738.json",
    "total_tokens": 1007,
    "translated_title": "跨模态微调：先对齐再精细调整",
    "translated_abstract": "大规模预训练模型的微调已经在诸如视觉和NLP等经过研究的模态中取得了巨大进展。但是，在许多其他模态中由于缺乏相关的预训练模型，类似的进展并没有得到观察。在这项工作中，我们提出了ORCA，这是一个通用的跨模态微调框架，它将单个大规模预训练模型的适用性扩展到多种模态。通过对目标输入进行一种先对齐再精细调整的工作流来适应目标任务：给定目标输入，ORCA首先学习一个嵌入网络，该网络将嵌入特征分布与预训练模态进行对齐，然后在嵌入数据上微调预训练模型，以利用模态之间分享的知识。通过广泛的实验，我们展示了ORCA在包含来自12种模态、超过60个数据集的3个基准测试中获得了最先进的结果，优于广泛范围的手工设计、AutoML、通用型和任务特定型的方法。我们强调了跨模态微调的重要性。",
    "tldr": "ORCA是一个通用的跨模态微调框架，它将单个大规模预训练模型的适用性扩展到多种模态，通过先对齐再微调的方法使跨模态微调成为可能，实验结果优于广泛范围的方法。",
    "en_tdlr": "ORCA is a general cross-modal fine-tuning framework that extends the applicability of a single large-scale pretrained model to diverse modalities. By aligning and refining the feature distributions, ORCA adapts to a target task and achieves state-of-the-art results on benchmarks containing over 60 datasets from 12 modalities, outperforming a wide range of hand-designed, AutoML, general-purpose, and task-specific methods."
}