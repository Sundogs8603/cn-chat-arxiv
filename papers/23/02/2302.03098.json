{
    "title": "One-shot Empirical Privacy Estimation for Federated Learning. (arXiv:2302.03098v3 [cs.LG] UPDATED)",
    "abstract": "Privacy estimation techniques for differentially private (DP) algorithms are useful for comparing against analytical bounds, or to empirically measure privacy loss in settings where known analytical bounds are not tight. However, existing privacy auditing techniques usually make strong assumptions on the adversary (e.g., knowledge of intermediate model iterates or the training data distribution), are tailored to specific tasks and model architectures, and require retraining the model many times (typically on the order of thousands). These shortcomings make deploying such techniques at scale difficult in practice, especially in federated settings where model training can take days or weeks. In this work, we present a novel \"one-shot\" approach that can systematically address these challenges, allowing efficient auditing or estimation of the privacy loss of a model during the same, single training run used to fit model parameters, and without requiring any a priori knowledge about the mod",
    "link": "http://arxiv.org/abs/2302.03098",
    "context": "Title: One-shot Empirical Privacy Estimation for Federated Learning. (arXiv:2302.03098v3 [cs.LG] UPDATED)\nAbstract: Privacy estimation techniques for differentially private (DP) algorithms are useful for comparing against analytical bounds, or to empirically measure privacy loss in settings where known analytical bounds are not tight. However, existing privacy auditing techniques usually make strong assumptions on the adversary (e.g., knowledge of intermediate model iterates or the training data distribution), are tailored to specific tasks and model architectures, and require retraining the model many times (typically on the order of thousands). These shortcomings make deploying such techniques at scale difficult in practice, especially in federated settings where model training can take days or weeks. In this work, we present a novel \"one-shot\" approach that can systematically address these challenges, allowing efficient auditing or estimation of the privacy loss of a model during the same, single training run used to fit model parameters, and without requiring any a priori knowledge about the mod",
    "path": "papers/23/02/2302.03098.json",
    "total_tokens": 994,
    "translated_title": "一种用于联邦学习的单次经验隐私估计方法",
    "translated_abstract": "不同ially private（DP）算法的隐私估计技术可用于与分析上界进行比较，或在已知分析上界不紧的情况下实验测量隐私损失。但是，现有的隐私审计技术通常对对手做出强烈假设（例如，了解中间模型迭代或训练数据分布），针对特定任务和模型架构进行调整，并需要重新训练模型多次（通常数量级为数千）。这些缺点使得在实践中难以大规模部署此类技术，尤其是在联邦设置中，模型训练可能需要数天或数周。在本研究中，我们提出了一种新的“单次”方法，可以系统地解决这些挑战，在单个训练运行期间高效地审计或估计模型的隐私损失，而不需要事先了解模型体系结构或训练数据分布。我们的方法适用于联邦学习等设置中使用的一般DP算法，并由实验在实际数据集上验证其提供的准确隐私损失估计。",
    "tldr": "本论文提出了一种用于联邦学习的单次经验隐私估计方法，可有效进行隐私损失审计，且无需事先了解模型体系结构或训练数据分布，适用于在实践中大规模部署。",
    "en_tdlr": "This paper proposes a novel \"one-shot\" approach for efficient auditing or estimation of the privacy loss of a model during the same, single training run used to fit model parameters, without requiring any a priori knowledge about the model architecture or training data distribution, which is applicable to general DP algorithms used in settings such as federated learning and validated by experiments on real-world datasets."
}