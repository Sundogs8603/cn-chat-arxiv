{
    "title": "Averaged Method of Multipliers for Bi-Level Optimization without Lower-Level Strong Convexity. (arXiv:2302.03407v2 [math.OC] UPDATED)",
    "abstract": "Gradient methods have become mainstream techniques for Bi-Level Optimization (BLO) in learning fields. The validity of existing works heavily rely on either a restrictive Lower-Level Strong Convexity (LLSC) condition or on solving a series of approximation subproblems with high accuracy or both. In this work, by averaging the upper and lower level objectives, we propose a single loop Bi-level Averaged Method of Multipliers (sl-BAMM) for BLO that is simple yet efficient for large-scale BLO and gets rid of the limited LLSC restriction. We further provide non-asymptotic convergence analysis of sl-BAMM towards KKT stationary points, and the comparative advantage of our analysis lies in the absence of strong gradient boundedness assumption, which is always required by others. Thus our theory safely captures a wider variety of applications in deep learning, especially where the upper-level objective is quadratic w.r.t. the lower-level variable. Experimental results demonstrate the superiorit",
    "link": "http://arxiv.org/abs/2302.03407",
    "context": "Title: Averaged Method of Multipliers for Bi-Level Optimization without Lower-Level Strong Convexity. (arXiv:2302.03407v2 [math.OC] UPDATED)\nAbstract: Gradient methods have become mainstream techniques for Bi-Level Optimization (BLO) in learning fields. The validity of existing works heavily rely on either a restrictive Lower-Level Strong Convexity (LLSC) condition or on solving a series of approximation subproblems with high accuracy or both. In this work, by averaging the upper and lower level objectives, we propose a single loop Bi-level Averaged Method of Multipliers (sl-BAMM) for BLO that is simple yet efficient for large-scale BLO and gets rid of the limited LLSC restriction. We further provide non-asymptotic convergence analysis of sl-BAMM towards KKT stationary points, and the comparative advantage of our analysis lies in the absence of strong gradient boundedness assumption, which is always required by others. Thus our theory safely captures a wider variety of applications in deep learning, especially where the upper-level objective is quadratic w.r.t. the lower-level variable. Experimental results demonstrate the superiorit",
    "path": "papers/23/02/2302.03407.json",
    "total_tokens": 1006,
    "translated_title": "无需较强下水平凸性的双层优化的均值乘法方法",
    "translated_abstract": "梯度方法已经成为学习领域双层优化(BLO)的主流技术。现有工作的有效性严重依赖于要么有限的下水平凸性(LLSC)条件，要么解决一系列高精度的近似子问题，或者两者兼有。本研究通过对上下层目标求平均值，提出了一种简单而高效的大规模BLO的单循环均值乘法双层(sl-BAMM)方法，并摆脱了有限的LLSC约束。我们进一步对sl-BAMM的非渐近收敛性分析KKT平稳点进行了分析，并且我们的分析的比较优势在于不需要强梯度有界性假设，这是其他方法总是需要的。因此，我们的理论安全地捕捉到了更广泛的深度学习应用，特别是在上层目标相对于下层变量是二次的情况下。实验结果表明了我们方法的优越性。",
    "tldr": "本论文提出了一种无需下水平强凸性的双层优化均值乘法方法(sl-BAMM)，通过对上下层目标求平均值，实现了在大规模BLO中高效而简单的求解。与其他方法相比，本论文的分析不需要强梯度有界性假设，适用范围更广泛。实验结果表明了该方法的优越性。",
    "en_tdlr": "This paper proposes a bi-level optimization averaged method of multipliers (sl-BAMM) that does not require lower-level strong convexity. By averaging the upper and lower level objectives, it provides a simple and efficient solution for large-scale BLO. The analysis in this paper is advantageous as it does not rely on strong gradient boundedness assumption, making it applicable to a wider variety of applications. Experimental results demonstrate the superiority of this method."
}