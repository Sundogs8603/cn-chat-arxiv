{
    "title": "Scalable Real-Time Recurrent Learning Using Sparse Connections and Selective Learning. (arXiv:2302.05326v2 [cs.LG] UPDATED)",
    "abstract": "State construction from sensory observations is an important component of a reinforcement learning agent. One solution for state construction is to use recurrent neural networks. Back-propagation through time (BPTT), and real-time recurrent learning (RTRL) are two popular gradient-based methods for recurrent learning. BPTT requires the complete sequence of observations before computing gradients and is unsuitable for online real-time updates. RTRL can do online updates but scales poorly to large networks. In this paper, we propose two constraints that make RTRL scalable. We show that by either decomposing the network into independent modules, or learning the network incrementally, we can make RTRL scale linearly with the number of parameters. Unlike prior scalable gradient estimation algorithms, such as UORO and Truncated-BPTT, our algorithms do not add noise or bias to the gradient estimate. Instead, they trade-off the functional capacity of the network to achieve scalable learning. W",
    "link": "http://arxiv.org/abs/2302.05326",
    "context": "Title: Scalable Real-Time Recurrent Learning Using Sparse Connections and Selective Learning. (arXiv:2302.05326v2 [cs.LG] UPDATED)\nAbstract: State construction from sensory observations is an important component of a reinforcement learning agent. One solution for state construction is to use recurrent neural networks. Back-propagation through time (BPTT), and real-time recurrent learning (RTRL) are two popular gradient-based methods for recurrent learning. BPTT requires the complete sequence of observations before computing gradients and is unsuitable for online real-time updates. RTRL can do online updates but scales poorly to large networks. In this paper, we propose two constraints that make RTRL scalable. We show that by either decomposing the network into independent modules, or learning the network incrementally, we can make RTRL scale linearly with the number of parameters. Unlike prior scalable gradient estimation algorithms, such as UORO and Truncated-BPTT, our algorithms do not add noise or bias to the gradient estimate. Instead, they trade-off the functional capacity of the network to achieve scalable learning. W",
    "path": "papers/23/02/2302.05326.json",
    "total_tokens": 945,
    "translated_title": "使用稀疏连接和选择性学习的可扩展实时循环学习",
    "translated_abstract": "从感知观察中构建状态是强化学习代理的重要组成部分。一种用于状态构建的解决方案是使用循环神经网络。 BPTT和实时循环学习（RTRL）是两种流行的基于梯度的循环学习方法。 BPTT在计算梯度之前需要完整的观察序列，不适合在线实时更新。 RTRL可以进行在线更新，但不适用于大型网络。 在本文中，我们提出了两个限制，使RTRL具有可扩展性。我们表明，通过将网络分解为独立模块或逐步学习网络，我们可以使RTRL与参数数量呈线性比例关系。与先前的可扩展梯度估计算法（例如UORO和Truncated-BPTT）不同，我们的算法不会向梯度估计添加噪声或偏差。相反，它们权衡了网络的功能能力以实现可扩展学习。",
    "tldr": "本文提出了两个限制使得实时循环学习算法具有可扩展性，分别是将网络分解为独立模块或逐步学习网络。与其他可扩展算法不同的是，这些算法不会向梯度估计添加噪声或偏差，而是通过权衡网络的功能能力以实现可扩展学习。",
    "en_tdlr": "This paper proposes two constraints to make real-time recurrent learning scalable, namely decomposing the network into independent modules or learning the network incrementally. Unlike other scalable algorithms, these algorithms do not add noise or bias to the gradient estimate, but achieve scalable learning by trading off the functional capacity of the network."
}