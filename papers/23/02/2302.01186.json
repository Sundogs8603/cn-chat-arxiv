{
    "title": "The Power of Preconditioning in Overparameterized Low-Rank Matrix Sensing. (arXiv:2302.01186v2 [cs.LG] UPDATED)",
    "abstract": "We propose $\\textsf{ScaledGD($\\lambda$)}$, a preconditioned gradient descent method to tackle the low-rank matrix sensing problem when the true rank is unknown, and when the matrix is possibly ill-conditioned. Using overparametrized factor representations, $\\textsf{ScaledGD($\\lambda$)}$ starts from a small random initialization, and proceeds by gradient descent with a specific form of damped preconditioning to combat bad curvatures induced by overparameterization and ill-conditioning. At the expense of light computational overhead incurred by preconditioners, $\\textsf{ScaledGD($\\lambda$)}$ is remarkably robust to ill-conditioning compared to vanilla gradient descent ($\\textsf{GD}$) even with overprameterization. Specifically, we show that, under the Gaussian design, $\\textsf{ScaledGD($\\lambda$)}$ converges to the true low-rank matrix at a constant linear rate after a small number of iterations that scales only logarithmically with respect to the condition number and the problem dimensi",
    "link": "http://arxiv.org/abs/2302.01186",
    "context": "Title: The Power of Preconditioning in Overparameterized Low-Rank Matrix Sensing. (arXiv:2302.01186v2 [cs.LG] UPDATED)\nAbstract: We propose $\\textsf{ScaledGD($\\lambda$)}$, a preconditioned gradient descent method to tackle the low-rank matrix sensing problem when the true rank is unknown, and when the matrix is possibly ill-conditioned. Using overparametrized factor representations, $\\textsf{ScaledGD($\\lambda$)}$ starts from a small random initialization, and proceeds by gradient descent with a specific form of damped preconditioning to combat bad curvatures induced by overparameterization and ill-conditioning. At the expense of light computational overhead incurred by preconditioners, $\\textsf{ScaledGD($\\lambda$)}$ is remarkably robust to ill-conditioning compared to vanilla gradient descent ($\\textsf{GD}$) even with overprameterization. Specifically, we show that, under the Gaussian design, $\\textsf{ScaledGD($\\lambda$)}$ converges to the true low-rank matrix at a constant linear rate after a small number of iterations that scales only logarithmically with respect to the condition number and the problem dimensi",
    "path": "papers/23/02/2302.01186.json",
    "total_tokens": 839,
    "translated_title": "预条件对超参化低秩矩阵感知的影响",
    "translated_abstract": "本文提出了ScaledGD(𝜆)方法来解决低秩矩阵感知中矩阵可能病态以及真实秩未知的问题。该方法使用超参式表示，从一个小的随机初始化开始，通过使用特定形式的阻尼预条件梯度下降来对抗超参化和病态曲率的影响。与基准梯度下降（GD）相比，尽管预处理需要轻微的计算开销，但ScaledGD（𝜆）在面对病态问题时表现出了出色的鲁棒性。在高斯设计下，ScaledGD($\\lambda$) 会在仅迭代数对数级别的情况下，以线性速率收敛到真实的低秩矩阵。",
    "tldr": "该研究提出了ScaledGD(𝜆)方法，相较于传统梯度下降法更加鲁棒，并且在处理低秩矩阵感知问题时具有很好的表现。",
    "en_tdlr": "This study proposes the ScaledGD(𝜆) method which is more robust than the traditional gradient descent method and performs well in solving the low-rank matrix sensing problem."
}