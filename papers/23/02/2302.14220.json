{
    "title": "Are Character-level Translations Worth the Wait? Comparing Character- and Subword-level Models for Machine Translation. (arXiv:2302.14220v2 [cs.CL] UPDATED)",
    "abstract": "Pretrained character-level language models were recently shown to be competitive with popular subword models across a range of NLP tasks. However, there has been little research on their effectiveness for neural machine translation (NMT). This work performs an extensive comparison across multiple languages and experimental conditions of state-of-the-art character- and subword-level pre-trained models (ByT5 and mT5, respectively) on NMT, showing the effectiveness of character-level modeling in translation, particularly in cases where training data is limited. In our analysis, we show how character models' performance gains are reflected in better translations of orthographically similar words and rare words. While evaluating the importance of source texts in driving model predictions, we highlight ByT5 word-level patterns suggesting an ability to modulate word and character-level information during the translation, providing insights into a potential weakness of character-level modeling",
    "link": "http://arxiv.org/abs/2302.14220",
    "context": "Title: Are Character-level Translations Worth the Wait? Comparing Character- and Subword-level Models for Machine Translation. (arXiv:2302.14220v2 [cs.CL] UPDATED)\nAbstract: Pretrained character-level language models were recently shown to be competitive with popular subword models across a range of NLP tasks. However, there has been little research on their effectiveness for neural machine translation (NMT). This work performs an extensive comparison across multiple languages and experimental conditions of state-of-the-art character- and subword-level pre-trained models (ByT5 and mT5, respectively) on NMT, showing the effectiveness of character-level modeling in translation, particularly in cases where training data is limited. In our analysis, we show how character models' performance gains are reflected in better translations of orthographically similar words and rare words. While evaluating the importance of source texts in driving model predictions, we highlight ByT5 word-level patterns suggesting an ability to modulate word and character-level information during the translation, providing insights into a potential weakness of character-level modeling",
    "path": "papers/23/02/2302.14220.json",
    "total_tokens": 924,
    "translated_title": "字符级别的翻译是否值得等待？将字符级别和子词级别模型用于机器翻译的比较",
    "translated_abstract": "最近的研究表明，在多种自然语言处理任务中，预先训练的字符级别语言模型与流行的子词模型具有相当的竞争力。然而，对于神经机器翻译方面，它们的有效性鲜有研究。本研究在多种语言和实验条件下，比较了最先进的字符级别和子词级别预训练模型（分别为ByT5和mT5）在机器翻译中的效果，结果表明字符级别建模在翻译方面是有效的，特别是在训练数据有限的情况下。我们的分析表明，字符模型的性能提升在于更好地翻译了形似单词和稀有单词。在评估源文本在驱动模型预测中的重要性时，我们突出ByT5单词级别模式表明字符级别建模的潜在弱点。",
    "tldr": "本文比较了字符级别和子词级别的预训练模型在机器翻译方面的效果，结果表明字符级别建模在形似单词和稀有单词的翻译方面具有更好的效果，在训练数据有限的情况下尤为明显。",
    "en_tdlr": "This paper compares the performance of character-level and subword-level pre-trained models in machine translation, showing that character-level modeling is more effective in translating orthographically similar and rare words, especially when training data is limited. The analysis also suggests potential weaknesses of character-level modeling."
}