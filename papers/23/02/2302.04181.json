{
    "title": "Attending to Graph Transformers",
    "abstract": "arXiv:2302.04181v3 Announce Type: replace-cross  Abstract: Recently, transformer architectures for graphs emerged as an alternative to established techniques for machine learning with graphs, such as (message-passing) graph neural networks. So far, they have shown promising empirical results, e.g., on molecular prediction datasets, often attributed to their ability to circumvent graph neural networks' shortcomings, such as over-smoothing and over-squashing. Here, we derive a taxonomy of graph transformer architectures, bringing some order to this emerging field. We overview their theoretical properties, survey structural and positional encodings, and discuss extensions for important graph classes, e.g., 3D molecular graphs. Empirically, we probe how well graph transformers can recover various graph properties, how well they can deal with heterophilic graphs, and to what extent they prevent over-squashing. Further, we outline open challenges and research direction to stimulate future wo",
    "link": "https://arxiv.org/abs/2302.04181",
    "context": "Title: Attending to Graph Transformers\nAbstract: arXiv:2302.04181v3 Announce Type: replace-cross  Abstract: Recently, transformer architectures for graphs emerged as an alternative to established techniques for machine learning with graphs, such as (message-passing) graph neural networks. So far, they have shown promising empirical results, e.g., on molecular prediction datasets, often attributed to their ability to circumvent graph neural networks' shortcomings, such as over-smoothing and over-squashing. Here, we derive a taxonomy of graph transformer architectures, bringing some order to this emerging field. We overview their theoretical properties, survey structural and positional encodings, and discuss extensions for important graph classes, e.g., 3D molecular graphs. Empirically, we probe how well graph transformers can recover various graph properties, how well they can deal with heterophilic graphs, and to what extent they prevent over-squashing. Further, we outline open challenges and research direction to stimulate future wo",
    "path": "papers/23/02/2302.04181.json",
    "total_tokens": 822,
    "translated_title": "关注图转换器",
    "translated_abstract": "最近，用于图形的转换器架构作为机器学习的替代技术出现，例如（消息传递）图神经网络。到目前为止，它们已经展示出有希望的实证结果，例如在分子预测数据集上，通常归因于其绕过图神经网络的缺点，如过度平滑和过度压缩。在这里，我们提出了一种图转换器架构的分类法，为这个新兴领域带来了一些秩序。我们概述它们的理论性质，调查结构和位置编码，并讨论了对重要图类的扩展，例如3D分子图。在实证方面，我们研究图转换器如何恢复各种图属性，如何处理异性图，以及它们在多大程度上可以防止过度压缩。此外，我们概述了未解决的挑战和研究方向，以促进未来工作。",
    "tldr": "提出了一种图转换器架构的分类法，概述了其理论性质，调查了结构和位置编码，并探讨了对重要图类的扩展，如3D分子图。",
    "en_tdlr": "Proposed a taxonomy of graph transformer architectures, overviewed their theoretical properties, surveyed structural and positional encodings, and discussed extensions for important graph classes, such as 3D molecular graphs."
}