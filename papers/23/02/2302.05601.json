{
    "title": "Pruning Deep Neural Networks from a Sparsity Perspective. (arXiv:2302.05601v2 [cs.LG] UPDATED)",
    "abstract": "In recent years, deep network pruning has attracted significant attention in order to enable the rapid deployment of AI into small devices with computation and memory constraints. Pruning is often achieved by dropping redundant weights, neurons, or layers of a deep network while attempting to retain a comparable test performance. Many deep pruning algorithms have been proposed with impressive empirical success. However, existing approaches lack a quantifiable measure to estimate the compressibility of a sub-network during each pruning iteration and thus may under-prune or over-prune the model. In this work, we propose PQ Index (PQI) to measure the potential compressibility of deep neural networks and use this to develop a Sparsity-informed Adaptive Pruning (SAP) algorithm. Our extensive experiments corroborate the hypothesis that for a generic pruning procedure, PQI decreases first when a large model is being effectively regularized and then increases when its compressibility reaches a",
    "link": "http://arxiv.org/abs/2302.05601",
    "context": "Title: Pruning Deep Neural Networks from a Sparsity Perspective. (arXiv:2302.05601v2 [cs.LG] UPDATED)\nAbstract: In recent years, deep network pruning has attracted significant attention in order to enable the rapid deployment of AI into small devices with computation and memory constraints. Pruning is often achieved by dropping redundant weights, neurons, or layers of a deep network while attempting to retain a comparable test performance. Many deep pruning algorithms have been proposed with impressive empirical success. However, existing approaches lack a quantifiable measure to estimate the compressibility of a sub-network during each pruning iteration and thus may under-prune or over-prune the model. In this work, we propose PQ Index (PQI) to measure the potential compressibility of deep neural networks and use this to develop a Sparsity-informed Adaptive Pruning (SAP) algorithm. Our extensive experiments corroborate the hypothesis that for a generic pruning procedure, PQI decreases first when a large model is being effectively regularized and then increases when its compressibility reaches a",
    "path": "papers/23/02/2302.05601.json",
    "total_tokens": 964,
    "translated_title": "基于稀疏性的深度神经网络剪枝技术",
    "translated_abstract": "近年来，深度网络剪枝技术受到了重视，旨在将人工智能快速部署到计算和内存受限的小型设备上。这种剪枝通常通过丢弃深度网络中的冗余权重、神经元或层来实现，同时努力保持可比的测试性能。已经提出了许多深层剪枝算法，取得了令人瞩目的实证成果。然而，现有方法缺乏可量化的措施来估算每个剪枝迭代中子网络的可压缩性，因此可能会对模型进行过剪枝或欠剪枝。在本文中，我们提出了PQ指数（PQI）来衡量深度神经网络的潜在压缩性，并利用此指数开发出了一种稀疏感知自适应剪枝（SAP）算法。我们的广泛实验支持假设，对于通用剪枝程序，当大型模型被有效地正则化时，PQI首先减小，然后在其可压缩性达到最小值时增加。",
    "tldr": "本文提出了一种稀疏感知自适应剪枝算法，通过利用PQ指数来衡量深度神经网络的潜在压缩性，可以有效地确定模型的剪枝程度，确保不会过度或欠剪枝。"
}