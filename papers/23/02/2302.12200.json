{
    "title": "A Neural Span-Based Continual Named Entity Recognition Model. (arXiv:2302.12200v2 [cs.CL] UPDATED)",
    "abstract": "Named Entity Recognition (NER) models capable of Continual Learning (CL) are realistically valuable in areas where entity types continuously increase (e.g., personal assistants). Meanwhile the learning paradigm of NER advances to new patterns such as the span-based methods. However, its potential to CL has not been fully explored. In this paper, we propose SpanKL, a simple yet effective Span-based model with Knowledge distillation (KD) to preserve memories and multi-Label prediction to prevent conflicts in CL-NER. Unlike prior sequence labeling approaches, the inherently independent modeling in span and entity level with the designed coherent optimization on SpanKL promotes its learning at each incremental step and mitigates the forgetting. Experiments on synthetic CL datasets derived from OntoNotes and Few-NERD show that SpanKL significantly outperforms previous SoTA in many aspects, and obtains the smallest gap from CL to the upper bound revealing its high practiced value. The code i",
    "link": "http://arxiv.org/abs/2302.12200",
    "context": "Title: A Neural Span-Based Continual Named Entity Recognition Model. (arXiv:2302.12200v2 [cs.CL] UPDATED)\nAbstract: Named Entity Recognition (NER) models capable of Continual Learning (CL) are realistically valuable in areas where entity types continuously increase (e.g., personal assistants). Meanwhile the learning paradigm of NER advances to new patterns such as the span-based methods. However, its potential to CL has not been fully explored. In this paper, we propose SpanKL, a simple yet effective Span-based model with Knowledge distillation (KD) to preserve memories and multi-Label prediction to prevent conflicts in CL-NER. Unlike prior sequence labeling approaches, the inherently independent modeling in span and entity level with the designed coherent optimization on SpanKL promotes its learning at each incremental step and mitigates the forgetting. Experiments on synthetic CL datasets derived from OntoNotes and Few-NERD show that SpanKL significantly outperforms previous SoTA in many aspects, and obtains the smallest gap from CL to the upper bound revealing its high practiced value. The code i",
    "path": "papers/23/02/2302.12200.json",
    "total_tokens": 946,
    "translated_title": "一种基于神经网络的跨时期命名实体识别模型",
    "translated_abstract": "在实体类型不断增加的领域（例如个人助手）中，具备持续学习能力的命名实体识别模型（NER）具有现实价值。与此同时，NER的学习范式逐渐发展出了新的模式，如基于跨度的方法。然而，跨时期学习在这方面的潜力尚未被充分探索。本文提出了SpanKL，一种简单而有效的基于跨度的模型，利用知识蒸馏（KD）来保留记忆，并采用多标签预测来防止跨时期NER中的冲突。与之前的序列标记方法不同，在SpanKL中，跨度和实体级别的独立建模以及设计的一致优化促进了每个增量步骤的学习，并减轻了遗忘。在从OntoNotes和Few-NERD衍生的合成CL数据集上的实验表明，SpanKL在许多方面显著优于先前的最优结果，并且从CL到上限的差距最小，显示了其高实际价值。",
    "tldr": "本文提出了一种基于神经网络的跨时期命名实体识别模型SpanKL，通过知识蒸馏和多标签预测来实现记忆保留和冲突防止，该模型在跨时期NER任务中表现出色，显示出高实际价值。",
    "en_tdlr": "This paper proposes a neural span-based model called SpanKL for continual named entity recognition (NER), which utilizes knowledge distillation and multi-label prediction to preserve memories and prevent conflicts in the continual learning context. SpanKL achieves superior performance in synthetic CL datasets and demonstrates its high practical value."
}