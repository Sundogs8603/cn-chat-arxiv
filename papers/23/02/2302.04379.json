{
    "title": "The Certification Paradox: Certifications Admit Better Attacks. (arXiv:2302.04379v2 [cs.LG] UPDATED)",
    "abstract": "In guaranteeing that no adversarial examples exist within a bounded region, certification mechanisms play an important role in demonstrating the robustness of neural networks. In this work we ask: Could certifications have any unintended consequences, through exposing additional information about certified models? We answer this question in the affirmative, demonstrating that certifications not only measure model robustness but also present a new attack surface. We propose \\emph{Certification Aware Attacks}, that produce smaller adversarial perturbations more than twice as frequently as any prior approach, when launched against certified models. Our attacks achieve an up to $34\\%$ reduction in the median perturbation norm (comparing target and attack instances), while requiring $90 \\%$ less computational time than approaches like PGD. That our attacks achieve such significant reductions in perturbation size and computational cost highlights an apparent paradox in deploying certificatio",
    "link": "http://arxiv.org/abs/2302.04379",
    "context": "Title: The Certification Paradox: Certifications Admit Better Attacks. (arXiv:2302.04379v2 [cs.LG] UPDATED)\nAbstract: In guaranteeing that no adversarial examples exist within a bounded region, certification mechanisms play an important role in demonstrating the robustness of neural networks. In this work we ask: Could certifications have any unintended consequences, through exposing additional information about certified models? We answer this question in the affirmative, demonstrating that certifications not only measure model robustness but also present a new attack surface. We propose \\emph{Certification Aware Attacks}, that produce smaller adversarial perturbations more than twice as frequently as any prior approach, when launched against certified models. Our attacks achieve an up to $34\\%$ reduction in the median perturbation norm (comparing target and attack instances), while requiring $90 \\%$ less computational time than approaches like PGD. That our attacks achieve such significant reductions in perturbation size and computational cost highlights an apparent paradox in deploying certificatio",
    "path": "papers/23/02/2302.04379.json",
    "total_tokens": 968,
    "translated_title": "认证悖论: 认证会揭示更好的攻击",
    "translated_abstract": "在保证有一个有界区域内不存在对抗样本的情况下，认证机制在展示神经网络的稳健性方面扮演着重要角色。本文提出了一个问题: 认证是否会有任何意想不到的后果，通过揭示有关认证模型的额外信息？我们以肯定的答案回答了这个问题，证明了认证不仅测量模型的稳健性，而且展现了新的攻击面。我们提出了\"认证感知攻击\"，在针对经过认证的模型进行攻击时，这种攻击会比以前的任何方法更频繁地产生更小的对抗性扰动。我们的攻击实现了最多34%的扰动规范中位数的减小(比较目标和攻击实例)，同时需要的计算时间比PGD等方法少了90%。我们的攻击实现了如此显着的扰动大小和计算成本的降低，突显了以认证作为对抗攻击防御的一种悖论。具体来说，认证不仅揭示了稳健模型的属性，而且还可以用来发起更有效的攻击。",
    "tldr": "本文指出了一个\"认证悖论\"，认证虽然可以展示模型的稳健性，但额外揭示了有关认证模型的信息也成为新的攻击面，导致更好的攻击效果。",
    "en_tdlr": "This paper presents a \"Certification Paradox\" - while certifications demonstrate the robustness of neural networks, they also expose additional information about certified models which can be used to launch more effective attacks, thereby creating a new attack surface."
}