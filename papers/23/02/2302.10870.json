{
    "title": "On Provable Copyright Protection for Generative Models. (arXiv:2302.10870v2 [cs.LG] UPDATED)",
    "abstract": "There is a growing concern that learned conditional generative models may output samples that are substantially similar to some copyrighted data $C$ that was in their training set. We give a formal definition of $\\textit{near access-freeness (NAF)}$ and prove bounds on the probability that a model satisfying this definition outputs a sample similar to $C$, even if $C$ is included in its training set. Roughly speaking, a generative model $p$ is $\\textit{$k$-NAF}$ if for every potentially copyrighted data $C$, the output of $p$ diverges by at most $k$-bits from the output of a model $q$ that $\\textit{did not access $C$ at all}$. We also give generative model learning algorithms, which efficiently modify the original generative model learning algorithm in a black box manner, that output generative models with strong bounds on the probability of sampling protected content. Furthermore, we provide promising experiments for both language (transformers) and image (diffusion) generative models",
    "link": "http://arxiv.org/abs/2302.10870",
    "context": "Title: On Provable Copyright Protection for Generative Models. (arXiv:2302.10870v2 [cs.LG] UPDATED)\nAbstract: There is a growing concern that learned conditional generative models may output samples that are substantially similar to some copyrighted data $C$ that was in their training set. We give a formal definition of $\\textit{near access-freeness (NAF)}$ and prove bounds on the probability that a model satisfying this definition outputs a sample similar to $C$, even if $C$ is included in its training set. Roughly speaking, a generative model $p$ is $\\textit{$k$-NAF}$ if for every potentially copyrighted data $C$, the output of $p$ diverges by at most $k$-bits from the output of a model $q$ that $\\textit{did not access $C$ at all}$. We also give generative model learning algorithms, which efficiently modify the original generative model learning algorithm in a black box manner, that output generative models with strong bounds on the probability of sampling protected content. Furthermore, we provide promising experiments for both language (transformers) and image (diffusion) generative models",
    "path": "papers/23/02/2302.10870.json",
    "total_tokens": 995,
    "translated_title": "关于可证明版权保护生成模型的研究",
    "translated_abstract": "担心学习的条件生成模型可能输出与其训练集中的某些受版权保护的数据$C$极为相似的样本。我们给出了“近无阻碍性（NAF）”的正式定义，并证明了满足这个定义的模型输出与$C$相似样本的概率上限，即使$C$包含在其训练集中。粗略地说，生成模型$p$是“$k$-NAF”的，如果对于每一个潜在的受版权保护的数据$C$，$p$的输出与一个完全未访问$C$的模型$q$的输出之间的差别最大为$k$比特。我们还提供了生成模型学习算法，以黑盒方式高效修改原始生成模型学习算法，输出具有强大的保护内容采样概率上限的生成模型。此外，我们还针对语言（Transformer）和图像（Diffusion）生成模型进行了有前景的实验。",
    "tldr": "该论文研究了可证明版权保护生成模型的问题，提出了近无阻碍性（NAF）的定义，并给出了满足该定义的模型输出与受版权保护数据相似样本的概率上限。同时，还提供了生成模型学习算法，能够高效输出具有强大版权保护能力的生成模型。最后，进行了有前景的语言和图像生成模型实验。",
    "en_tdlr": "This paper investigates the provable copyright protection for generative models. It provides a formal definition of near access-freeness (NAF) and proves bounds on the probability of the model generating samples similar to copyrighted data. The paper also proposes generative model learning algorithms that output models with strong protection against sampling protected content. Promising experiments on language and image generative models are conducted."
}