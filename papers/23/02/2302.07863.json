{
    "title": "Big Little Transformer Decoder. (arXiv:2302.07863v2 [cs.CL] UPDATED)",
    "abstract": "The recent emergence of Large Language Models based on the Transformer architecture has enabled dramatic advancements in the field of Natural Language Processing. However, these models have long inference latency, which limits their deployment, and which makes them prohibitively expensive for various real-time applications. The inference latency is further exacerbated by autoregressive generative tasks, as models need to run iteratively to generate tokens sequentially without leveraging token-level parallelization. To address this, we propose Big Little Decoder (BiLD), a framework that can improve inference efficiency and latency for a wide range of text generation applications. The BiLD framework contains two models with different sizes that collaboratively generate text. The small model runs autoregressively to generate text with a low inference cost, and the large model is only invoked occasionally to refine the small model's inaccurate predictions in a non-autoregressive manner. To",
    "link": "http://arxiv.org/abs/2302.07863",
    "context": "Title: Big Little Transformer Decoder. (arXiv:2302.07863v2 [cs.CL] UPDATED)\nAbstract: The recent emergence of Large Language Models based on the Transformer architecture has enabled dramatic advancements in the field of Natural Language Processing. However, these models have long inference latency, which limits their deployment, and which makes them prohibitively expensive for various real-time applications. The inference latency is further exacerbated by autoregressive generative tasks, as models need to run iteratively to generate tokens sequentially without leveraging token-level parallelization. To address this, we propose Big Little Decoder (BiLD), a framework that can improve inference efficiency and latency for a wide range of text generation applications. The BiLD framework contains two models with different sizes that collaboratively generate text. The small model runs autoregressively to generate text with a low inference cost, and the large model is only invoked occasionally to refine the small model's inaccurate predictions in a non-autoregressive manner. To",
    "path": "papers/23/02/2302.07863.json",
    "total_tokens": 1028,
    "translated_title": "大小不同的Transformer解码器",
    "translated_abstract": "基于Transformer架构的大型语言模型的出现，使得自然语言处理领域取得了巨大的进展。然而，这些模型存在长时间的推理延迟，限制了它们的使用并且使得它们在各种实时应用中过于昂贵。在自回归生成任务中，由于模型需要迭代地运行才能逐个生成标记，因此推理延迟更加严重。为了解决这个问题，我们提出了Big Little Decoder（BiLD）框架，它可以提高各种文本生成应用的推理效率和延迟。BiLD框架包含两个不同大小的模型，它们协作地生成文本。小型模型自回归地运行以低延迟生成文本，大型模型只在需要时以非自回归的方式调整小型模型不准确的预测。为了提高训练的稳定性和改善模型性能，我们引入了一种渐进蒸馏机制，使小型模型逐渐地从大型模型中学习。实验结果证明，所提出的BiLD框架显著降低了推理延迟，同时在保持与大型自回归模型相当甚至更好的生成质量的情况下。",
    "tldr": "提出了一种名为BiLD的框架，它由大小不同的两个模型协作生成文本。其中小型模型自回归地生成文本，而大型模型则在必要时以非自回归的方式对小型模型的预测进行微调，从而显著减少了推理延迟。",
    "en_tdlr": "The BiLD framework proposes two different-sized models collaborating to generate text, where the small model runs autoregressively and the large model occasionally refines the small model's predictions in a non-autoregressive manner to significantly reduce inference latency."
}