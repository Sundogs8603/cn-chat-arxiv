{
    "title": "In-context Example Selection with Influences. (arXiv:2302.11042v2 [cs.CL] UPDATED)",
    "abstract": "In-context learning (ICL) is a powerful paradigm emerged from large language models (LLMs). Despite its promises, ICL performance is known to be highly sensitive to input examples. In this work, we use $\\textit{in-context influences}$ to analyze few-shot ICL performance directly from the in-context examples. Our proposed influence-based example selection method can identify both positive and negative examples, outperforming several baselines when evaluated on 9 SuperGLUE tasks. Our analysis uncovers up to a $16.3\\%$ performance gap between using the most negative in-context examples compared to the most positive. In a case study, we apply our influence-based framework to quantify the phenomena of recency bias in example ordering for few-shot ICL.",
    "link": "http://arxiv.org/abs/2302.11042",
    "context": "Title: In-context Example Selection with Influences. (arXiv:2302.11042v2 [cs.CL] UPDATED)\nAbstract: In-context learning (ICL) is a powerful paradigm emerged from large language models (LLMs). Despite its promises, ICL performance is known to be highly sensitive to input examples. In this work, we use $\\textit{in-context influences}$ to analyze few-shot ICL performance directly from the in-context examples. Our proposed influence-based example selection method can identify both positive and negative examples, outperforming several baselines when evaluated on 9 SuperGLUE tasks. Our analysis uncovers up to a $16.3\\%$ performance gap between using the most negative in-context examples compared to the most positive. In a case study, we apply our influence-based framework to quantify the phenomena of recency bias in example ordering for few-shot ICL.",
    "path": "papers/23/02/2302.11042.json",
    "total_tokens": 831,
    "translated_title": "带有影响力的上下文示例选择",
    "translated_abstract": "上下文学习(ICL)是从大语言模型(LLM)中出现的一种强大的范例。尽管有着许多有利方面，ICL的性能仍然对输入示例非常敏感。本文提出使用$\\textit{上下文影响}$来直接分析少样本ICL的性能。我们提出了基于影响力的示例选择方法，可以识别出正面和负面示例，在9个SuperGLUE任务的评估中优于几种基准线。我们的分析揭示，在使用最正面示例和最负面示例之间，性能差异可高达$16.3\\%$。在一个案例研究中，我们将基于影响力的框架应用于量化少样本ICL示例排序中最近性偏差的现象。",
    "tldr": "本文提出使用带有影响力的示例选择方法来提高上下文学习(ICL)的性能，分析表明正面和负面示例对ICL取得的性能有高达16.3%的影响，案例研究中也发现了示例排序中的“最近性偏差”现象。",
    "en_tdlr": "This paper proposes an influence-based example selection method to improve the performance of in-context learning (ICL). The analysis shows that positive and negative examples can have a performance impact of up to 16.3% on ICL. In a case study, the framework is applied to quantify the phenomena of recency bias in example ordering for few-shot ICL."
}