{
    "title": "Distributional Offline Policy Evaluation with Predictive Error Guarantees. (arXiv:2302.09456v2 [cs.LG] UPDATED)",
    "abstract": "We study the problem of estimating the distribution of the return of a policy using an offline dataset that is not generated from the policy, i.e., distributional offline policy evaluation (OPE). We propose an algorithm called Fitted Likelihood Estimation (FLE), which conducts a sequence of Maximum Likelihood Estimation (MLE) and has the flexibility of integrating any state-of-the-art probabilistic generative models as long as it can be trained via MLE. FLE can be used for both finite-horizon and infinite-horizon discounted settings where rewards can be multi-dimensional vectors. Our theoretical results show that for both finite-horizon and infinite-horizon discounted settings, FLE can learn distributions that are close to the ground truth under total variation distance and Wasserstein distance, respectively. Our theoretical results hold under the conditions that the offline data covers the test policy's traces and that the supervised learning MLE procedures succeed. Experimentally, we",
    "link": "http://arxiv.org/abs/2302.09456",
    "context": "Title: Distributional Offline Policy Evaluation with Predictive Error Guarantees. (arXiv:2302.09456v2 [cs.LG] UPDATED)\nAbstract: We study the problem of estimating the distribution of the return of a policy using an offline dataset that is not generated from the policy, i.e., distributional offline policy evaluation (OPE). We propose an algorithm called Fitted Likelihood Estimation (FLE), which conducts a sequence of Maximum Likelihood Estimation (MLE) and has the flexibility of integrating any state-of-the-art probabilistic generative models as long as it can be trained via MLE. FLE can be used for both finite-horizon and infinite-horizon discounted settings where rewards can be multi-dimensional vectors. Our theoretical results show that for both finite-horizon and infinite-horizon discounted settings, FLE can learn distributions that are close to the ground truth under total variation distance and Wasserstein distance, respectively. Our theoretical results hold under the conditions that the offline data covers the test policy's traces and that the supervised learning MLE procedures succeed. Experimentally, we",
    "path": "papers/23/02/2302.09456.json",
    "total_tokens": 922,
    "translated_title": "具有预测误差保证的分布式离线策略评估算法",
    "translated_abstract": "本研究探讨使用非策略生成的离线数据集来估算策略回报分布的问题，即分布式离线策略评估（OPE）。提出了一种名为Fitted Likelihood Estimation（FLE）的算法，它执行了一系列的最大似然估计，具有将任何最先进的概率生成模型集成的灵活性，只要它可以通过最大似然估计进行训练。FLE能够用于有限或无限时间折扣设置，其中奖励可以是多维向量。我们的理论结果表明，无论是在有限时间折扣设置还是无限时间折扣设置下，FLE都可以学习到密切接近真实分布的分布，分别在总变差距离和Wasserstein距离下。在训练MLE过程成功时，我们的理论结果适用于离线数据覆盖测试策略痕迹的条件。在实验上，我们证明了FLE在各种环境中都能取得良好的效果。",
    "tldr": "本论文提出了一种名为Fitted Likelihood Estimation (FLE)的算法来解决分布式离线策略评估的问题，该算法能够学习到密切接近真实分布的策略回报分布。",
    "en_tdlr": "This paper proposes an algorithm named Fitted Likelihood Estimation (FLE) to solve the problem of distributional offline policy evaluation, which can learn the policy return distribution that is close to the ground truth under total variation and Wasserstein distance."
}