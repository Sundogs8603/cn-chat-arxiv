{
    "title": "Multi-Modal Self-Supervised Learning for Recommendation. (arXiv:2302.10632v4 [cs.IR] UPDATED)",
    "abstract": "The online emergence of multi-modal sharing platforms (eg, TikTok, Youtube) is powering personalized recommender systems to incorporate various modalities (eg, visual, textual and acoustic) into the latent user representations. While existing works on multi-modal recommendation exploit multimedia content features in enhancing item embeddings, their model representation capability is limited by heavy label reliance and weak robustness on sparse user behavior data. Inspired by the recent progress of self-supervised learning in alleviating label scarcity issue, we explore deriving self-supervision signals with effectively learning of modality-aware user preference and cross-modal dependencies. To this end, we propose a new Multi-Modal Self-Supervised Learning (MMSSL) method which tackles two key challenges. Specifically, to characterize the inter-dependency between the user-item collaborative view and item multi-modal semantic view, we design a modality-aware interactive structure learnin",
    "link": "http://arxiv.org/abs/2302.10632",
    "context": "Title: Multi-Modal Self-Supervised Learning for Recommendation. (arXiv:2302.10632v4 [cs.IR] UPDATED)\nAbstract: The online emergence of multi-modal sharing platforms (eg, TikTok, Youtube) is powering personalized recommender systems to incorporate various modalities (eg, visual, textual and acoustic) into the latent user representations. While existing works on multi-modal recommendation exploit multimedia content features in enhancing item embeddings, their model representation capability is limited by heavy label reliance and weak robustness on sparse user behavior data. Inspired by the recent progress of self-supervised learning in alleviating label scarcity issue, we explore deriving self-supervision signals with effectively learning of modality-aware user preference and cross-modal dependencies. To this end, we propose a new Multi-Modal Self-Supervised Learning (MMSSL) method which tackles two key challenges. Specifically, to characterize the inter-dependency between the user-item collaborative view and item multi-modal semantic view, we design a modality-aware interactive structure learnin",
    "path": "papers/23/02/2302.10632.json",
    "total_tokens": 1114,
    "translated_title": "多模态自监督学习用于推荐系统",
    "translated_abstract": "多模态分享平台（例如 TikTok、YouTube）的崛起使得个性化推荐系统能够将各种模式（例如视觉、文本和声音）纳入潜在用户表示法。虽然现有的多模态推荐工作利用多媒体内容特征增强物品嵌入，但它们的模型表示能力受到重标签依赖性和稀疏用户行为数据的影响。受自监控学习在减轻标签稀缺性问题方面的最新进展的启发，我们探索有效地学习模态感知用户偏好和跨模态依赖关系的自我监控信号。为此，我们提出了一种新的多模态自监控学习（MMSSL）方法，解决了两个关键挑战。具体而言，为了表征用户-物品协同视图和物品多模态语义视图之间的相互依赖关系，我们设计了一种模态感知的交互结构学习组件；为了在模态相关的推荐任务中进一步利用自我监控信号，我们开发了一种学习用户内部模态分布的模态相关的预文本任务。对各种真实世界推荐数据集的实验结果表明，MMSSL优于最先进的推荐基线和多模态对应物，特别是在用户-物品交互稀缺的情况下。",
    "tldr": "本论文提出了一种名为“多模态自监督学习”的方法，通过有效地学习模态感知用户偏好和跨模态依赖关系的自我监控信号，以提高推荐系统的性能，实验结果表明其效果优于现有方法和多模态推荐。",
    "en_tdlr": "This paper proposes a novel method called \"Multi-Modal Self-Supervised Learning\" to improve the performance of recommendation systems by effectively learning self-supervision signals of modality-aware user preference and cross-modal dependencies. Experimental results show that the proposed method outperforms existing approaches and multi-modal counterparts, particularly in the scenarios where user-item interactions are scarce."
}