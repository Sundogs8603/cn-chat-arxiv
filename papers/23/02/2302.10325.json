{
    "title": "Adaptive Sparse Gaussian Process. (arXiv:2302.10325v2 [cs.LG] UPDATED)",
    "abstract": "Adaptive learning is necessary for non-stationary environments where the learning machine needs to forget past data distribution. Efficient algorithms require a compact model update to not grow in computational burden with the incoming data and with the lowest possible computational cost for online parameter updating. Existing solutions only partially cover these needs. Here, we propose the first adaptive sparse Gaussian Process (GP) able to address all these issues. We first reformulate a variational sparse GP algorithm to make it adaptive through a forgetting factor. Next, to make the model inference as simple as possible, we propose updating a single inducing point of the sparse GP model together with the remaining model parameters every time a new sample arrives. As a result, the algorithm presents a fast convergence of the inference process, which allows an efficient model update (with a single inference iteration) even in highly non-stationary environments. Experimental results d",
    "link": "http://arxiv.org/abs/2302.10325",
    "context": "Title: Adaptive Sparse Gaussian Process. (arXiv:2302.10325v2 [cs.LG] UPDATED)\nAbstract: Adaptive learning is necessary for non-stationary environments where the learning machine needs to forget past data distribution. Efficient algorithms require a compact model update to not grow in computational burden with the incoming data and with the lowest possible computational cost for online parameter updating. Existing solutions only partially cover these needs. Here, we propose the first adaptive sparse Gaussian Process (GP) able to address all these issues. We first reformulate a variational sparse GP algorithm to make it adaptive through a forgetting factor. Next, to make the model inference as simple as possible, we propose updating a single inducing point of the sparse GP model together with the remaining model parameters every time a new sample arrives. As a result, the algorithm presents a fast convergence of the inference process, which allows an efficient model update (with a single inference iteration) even in highly non-stationary environments. Experimental results d",
    "path": "papers/23/02/2302.10325.json",
    "total_tokens": 852,
    "translated_title": "自适应稀疏高斯过程",
    "translated_abstract": "自适应学习对于非平稳环境中的学习机器是必要的，因为它需要忘记过去的数据分布。高效的算法需要紧凑的模型更新，以便不随着新数据的到来而增加计算负担，并以最低的计算成本进行在线参数更新。现有的解决方案只是部分满足这些需求。在这里，我们提出了第一个能够解决所有这些问题的自适应稀疏高斯过程（GP）。我们首先通过遗忘因子重新定义了变分稀疏GP算法，使其具有自适应性。接下来，为了使模型推理尽可能简单，我们建议每当出现新样本时同时更新稀疏GP模型的一个单个引导点和其他模型参数。结果，该算法呈现出推理过程的快速收敛性，即使在高度非平稳的环境中也能进行高效的模型更新（只需一次推理迭代）。试验结果表明，该算法在多种数据集上表现出了良好的性能。",
    "tldr": "这篇论文提出了第一个自适应稀疏高斯过程，能够在非平稳环境中进行高效的模型更新，并具有快速的推理收敛性。",
    "en_tdlr": "This paper proposes the first adaptive sparse Gaussian Process that allows efficient model updates in non-stationary environments and exhibits fast convergence of inference process."
}