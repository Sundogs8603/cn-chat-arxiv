{
    "title": "Energy Transformer. (arXiv:2302.07253v2 [cs.LG] UPDATED)",
    "abstract": "Our work combines aspects of three promising paradigms in machine learning, namely, attention mechanism, energy-based models, and associative memory. Attention is the power-house driving modern deep learning successes, but it lacks clear theoretical foundations. Energy-based models allow a principled approach to discriminative and generative tasks, but the design of the energy functional is not straightforward. At the same time, Dense Associative Memory models or Modern Hopfield Networks have a well-established theoretical foundation, and allow an intuitive design of the energy function. We propose a novel architecture, called the Energy Transformer (or ET for short), that uses a sequence of attention layers that are purposely designed to minimize a specifically engineered energy function, which is responsible for representing the relationships between the tokens. In this work, we introduce the theoretical foundations of ET, explore its empirical capabilities using the image completion",
    "link": "http://arxiv.org/abs/2302.07253",
    "context": "Title: Energy Transformer. (arXiv:2302.07253v2 [cs.LG] UPDATED)\nAbstract: Our work combines aspects of three promising paradigms in machine learning, namely, attention mechanism, energy-based models, and associative memory. Attention is the power-house driving modern deep learning successes, but it lacks clear theoretical foundations. Energy-based models allow a principled approach to discriminative and generative tasks, but the design of the energy functional is not straightforward. At the same time, Dense Associative Memory models or Modern Hopfield Networks have a well-established theoretical foundation, and allow an intuitive design of the energy function. We propose a novel architecture, called the Energy Transformer (or ET for short), that uses a sequence of attention layers that are purposely designed to minimize a specifically engineered energy function, which is responsible for representing the relationships between the tokens. In this work, we introduce the theoretical foundations of ET, explore its empirical capabilities using the image completion",
    "path": "papers/23/02/2302.07253.json",
    "total_tokens": 833,
    "translated_title": "能量变换器",
    "translated_abstract": "我们的工作将注意力机制、能量模型和联想记忆三种潜力巨大的机器学习范式结合起来。注意力是推动现代深度学习成功的动力源，但它缺乏明确的理论基础。能量模型允许在判别和生成任务上采用有原则的方法，但能量函数的设计并不直观。与此同时，稠密联想记忆模型或现代霍普菲尔德网络具有良好的理论基础，并且允许能量函数的直观设计。我们提出了一种新颖的架构，称为能量变换器（简称ET），它使用一系列经过特意设计以最小化特殊设计的能量函数的注意力层，该函数负责表示标记之间的关系。在这项工作中，我们介绍了ET的理论基础，通过使用图像补全技术探索了它的经验能力。",
    "tldr": "本研究将注意力机制、能量模型和联想记忆结合，提出了一种新颖的架构——能量变换器（ET），它通过特意设计的注意力层以最小化能量函数，用于表示标记之间的关系。",
    "en_tdlr": "This work combines attention mechanism, energy-based models, and associative memory to propose a novel architecture called Energy Transformer (ET), which employs specially designed attention layers to minimize an engineered energy function for representing token relationships."
}