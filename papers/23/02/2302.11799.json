{
    "title": "FiTs: Fine-grained Two-stage Training for Knowledge-aware Question Answering. (arXiv:2302.11799v2 [cs.CL] UPDATED)",
    "abstract": "Knowledge-aware question answering (KAQA) requires the model to answer questions over a knowledge base, which is essential for both open-domain QA and domain-specific QA, especially when language models alone cannot provide all the knowledge needed. Despite the promising result of recent KAQA systems which tend to integrate linguistic knowledge from pre-trained language models (PLM) and factual knowledge from knowledge graphs (KG) to answer complex questions, a bottleneck exists in effectively fusing the representations from PLMs and KGs because of (i) the semantic and distributional gaps between them, and (ii) the difficulties in joint reasoning over the provided knowledge from both modalities. To address the above two problems, we propose a Fine-grained Two-stage training framework (FiTs) to boost the KAQA system performance: The first stage aims at aligning representations from the PLM and the KG, thus bridging the modality gaps between them, named knowledge adaptive post-training. ",
    "link": "http://arxiv.org/abs/2302.11799",
    "context": "Title: FiTs: Fine-grained Two-stage Training for Knowledge-aware Question Answering. (arXiv:2302.11799v2 [cs.CL] UPDATED)\nAbstract: Knowledge-aware question answering (KAQA) requires the model to answer questions over a knowledge base, which is essential for both open-domain QA and domain-specific QA, especially when language models alone cannot provide all the knowledge needed. Despite the promising result of recent KAQA systems which tend to integrate linguistic knowledge from pre-trained language models (PLM) and factual knowledge from knowledge graphs (KG) to answer complex questions, a bottleneck exists in effectively fusing the representations from PLMs and KGs because of (i) the semantic and distributional gaps between them, and (ii) the difficulties in joint reasoning over the provided knowledge from both modalities. To address the above two problems, we propose a Fine-grained Two-stage training framework (FiTs) to boost the KAQA system performance: The first stage aims at aligning representations from the PLM and the KG, thus bridging the modality gaps between them, named knowledge adaptive post-training. ",
    "path": "papers/23/02/2302.11799.json",
    "total_tokens": 936,
    "translated_title": "FiTs:细粒度两阶段训练用于知识感知问答",
    "translated_abstract": "知识感知问答（KAQA）需要模型在知识库中回答问题，这对于开放域QA和特定领域QA都是必要的，尤其是当语言模型无法提供所需的所有知识时。最近KAQA系统融合了从预训练语言模型（PLM）和知识图谱（KG）中获得的语言知识和事实知识以回答复杂问题，取得了令人鼓舞的结果，但是存在困难，即有效地融合来自PLMs和KGs的表示，因为（i）它们之间存在语义和分布差异，以及（ii）难以联合推理提供的两类知识。针对上述两个问题，我们提出了一个Fine-grained Two-stage训练框架（FiTs），旨在提高KAQA系统的性能。第一阶段旨在通过知识适应后训练来对齐来自PLM和KG的表示，从而弥合它们之间的模态差距。",
    "tldr": "本文提出了一个Fine-grained Two-stage训练框架（FiTs），用于解决知识感知问答（KAQA）中，从语言模型和知识图谱中获得的两种不同类型的知识在表示上的差异和联合推理的困难问题。",
    "en_tdlr": "This paper proposes a Fine-grained Two-stage training framework (FiTs) to tackle the semantic and distributional gaps between linguistic and factual knowledge in KAQA, by aligning representations from pre-trained language models and knowledge graphs, and joint reasoning over the provided knowledge from both modalities."
}