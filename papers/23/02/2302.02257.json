{
    "title": "Multi-Source Diffusion Models for Simultaneous Music Generation and Separation. (arXiv:2302.02257v3 [cs.SD] UPDATED)",
    "abstract": "In this work, we define a diffusion-based generative model capable of both music synthesis and source separation by learning the score of the joint probability density of sources sharing a context. Alongside the classic total inference tasks (i.e., generating a mixture, separating the sources), we also introduce and experiment on the partial generation task of source imputation, where we generate a subset of the sources given the others (e.g., play a piano track that goes well with the drums). Additionally, we introduce a novel inference method for the separation task based on Dirac likelihood functions. We train our model on Slakh2100, a standard dataset for musical source separation, provide qualitative results in the generation settings, and showcase competitive quantitative results in the source separation setting. Our method is the first example of a single model that can handle both generation and separation tasks, thus representing a step toward general audio models.",
    "link": "http://arxiv.org/abs/2302.02257",
    "context": "Title: Multi-Source Diffusion Models for Simultaneous Music Generation and Separation. (arXiv:2302.02257v3 [cs.SD] UPDATED)\nAbstract: In this work, we define a diffusion-based generative model capable of both music synthesis and source separation by learning the score of the joint probability density of sources sharing a context. Alongside the classic total inference tasks (i.e., generating a mixture, separating the sources), we also introduce and experiment on the partial generation task of source imputation, where we generate a subset of the sources given the others (e.g., play a piano track that goes well with the drums). Additionally, we introduce a novel inference method for the separation task based on Dirac likelihood functions. We train our model on Slakh2100, a standard dataset for musical source separation, provide qualitative results in the generation settings, and showcase competitive quantitative results in the source separation setting. Our method is the first example of a single model that can handle both generation and separation tasks, thus representing a step toward general audio models.",
    "path": "papers/23/02/2302.02257.json",
    "total_tokens": 871,
    "translated_title": "多源扩散模型用于同时进行音乐生成和分离。",
    "translated_abstract": "在本研究中，我们定义了一个扩散基于的生成模型，可以通过学习共享上下文源的联合概率密度的得分来进行音乐合成和源分离。除了经典的总推理任务（即生成混合物体，分离源），我们还引入并在源填充的部分生成任务上进行实验，在这个任务中，我们生成一些源给其他人（例如，演奏一条与鼓相配的钢琴曲）。此外，我们还提出了一种基于Dirac似然函数的分离任务的新推理方法。我们在Slakh2100这个标准的音乐源分离数据集上训练我们的模型，在生成设置中提供定性结果，并展示在源分离设置中的有竞争力的定量结果。我们的方法是第一个能够处理生成和分离任务的单一模型的例子，因此代表了通用音频模型的一步。",
    "tldr": "本研究提出了一种基于扩散模型的生成模型，能够同时进行音乐合成和源分离，并于部分生成和分离任务上实现有竞争力的定量结果。",
    "en_tdlr": "This work proposes a diffusion-based generative model capable of both music synthesis and source separation by learning the joint probability density of sources sharing a context. The model also introduces a novel inference method for separation task and showcases competitive quantitative results. It is the first single model that can handle both generation and separation tasks, representing a step toward general audio models."
}