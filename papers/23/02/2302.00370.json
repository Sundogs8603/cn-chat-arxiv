{
    "title": "How to select predictive models for causal inference?. (arXiv:2302.00370v2 [stat.ML] UPDATED)",
    "abstract": "As predictive models -- e.g., from machine learning -- give likely outcomes, they may be used to reason on the effect of an intervention, a causal-inference task. The increasing complexity of health data has opened the door to a plethora of models, but also the Pandora box of model selection: which of these models yield the most valid causal estimates? Here we highlight that classic machine-learning model selection does not select the best outcome models for causal inference. Indeed, causal model selection should control both outcome errors for each individual, treated or not treated, whereas only one outcome is observed. Theoretically, simple risks used in machine learning do not control causal effects when treated and non-treated population differ too much. More elaborate risks build proxies of the causal error using ``nuisance'' re-weighting to compute it on the observed data. But does computing these nuisance adds noise to model selection? Drawing from an extensive empirical study,",
    "link": "http://arxiv.org/abs/2302.00370",
    "context": "Title: How to select predictive models for causal inference?. (arXiv:2302.00370v2 [stat.ML] UPDATED)\nAbstract: As predictive models -- e.g., from machine learning -- give likely outcomes, they may be used to reason on the effect of an intervention, a causal-inference task. The increasing complexity of health data has opened the door to a plethora of models, but also the Pandora box of model selection: which of these models yield the most valid causal estimates? Here we highlight that classic machine-learning model selection does not select the best outcome models for causal inference. Indeed, causal model selection should control both outcome errors for each individual, treated or not treated, whereas only one outcome is observed. Theoretically, simple risks used in machine learning do not control causal effects when treated and non-treated population differ too much. More elaborate risks build proxies of the causal error using ``nuisance'' re-weighting to compute it on the observed data. But does computing these nuisance adds noise to model selection? Drawing from an extensive empirical study,",
    "path": "papers/23/02/2302.00370.json",
    "total_tokens": 1023,
    "tldr": "健康数据的增加复杂性打开了模型的大门，也打开了模型选择的潘多拉盒子: 选择哪些模型产生最有效的因果估计是个问题。因果模型选择应该控制每个个体，治疗或未治疗，的结果错误，而只能观察到一个结果，更具代表性的风险建立因果误差的代理来计算观察数据中的因果误差。",
    "en_tdlr": "The increasing complexity of health data has opened the door to a plethora of models, but also the Pandora box of model selection: which of these models yield the most valid causal estimates? Causal model selection should control both outcome errors for each individual, treated or not treated, whereas only one outcome is observed, and more elaborate risks build proxies of the causal error using “nuisance” re-weighting to compute it on the observed data."
}