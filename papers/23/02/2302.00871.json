{
    "title": "Using In-Context Learning to Improve Dialogue Safety. (arXiv:2302.00871v2 [cs.CL] UPDATED)",
    "abstract": "While large neural-based conversational models have become increasingly proficient dialogue agents, recent work has highlighted safety issues with these systems. For example, these systems can be goaded into generating toxic content, which often perpetuates social biases or stereotypes. We investigate a retrieval-based method for reducing bias and toxicity in responses from chatbots. It uses in-context learning to steer a model towards safer generations. Concretely, to generate a response to an unsafe dialogue context, we retrieve demonstrations of safe responses to similar dialogue contexts. We find our method performs competitively with strong baselines without requiring training. For instance, using automatic evaluation, we find our best fine-tuned baseline only generates safe responses to unsafe dialogue contexts from DiaSafety 4.04% more than our approach. Finally, we also propose a re-ranking procedure which can further improve response safeness.",
    "link": "http://arxiv.org/abs/2302.00871",
    "context": "Title: Using In-Context Learning to Improve Dialogue Safety. (arXiv:2302.00871v2 [cs.CL] UPDATED)\nAbstract: While large neural-based conversational models have become increasingly proficient dialogue agents, recent work has highlighted safety issues with these systems. For example, these systems can be goaded into generating toxic content, which often perpetuates social biases or stereotypes. We investigate a retrieval-based method for reducing bias and toxicity in responses from chatbots. It uses in-context learning to steer a model towards safer generations. Concretely, to generate a response to an unsafe dialogue context, we retrieve demonstrations of safe responses to similar dialogue contexts. We find our method performs competitively with strong baselines without requiring training. For instance, using automatic evaluation, we find our best fine-tuned baseline only generates safe responses to unsafe dialogue contexts from DiaSafety 4.04% more than our approach. Finally, we also propose a re-ranking procedure which can further improve response safeness.",
    "path": "papers/23/02/2302.00871.json",
    "total_tokens": 896,
    "translated_title": "使用上下文学习来提高对话安全性",
    "translated_abstract": "随着神经网络对话模型变得越来越成熟，人们开始关注这些系统的安全问题。这些系统很容易生成有毒内容，这些内容通常会延续社会偏见或刻板印象。本文提出了一种使用检索式方法来减少Chatbots回应中偏见和有毒内容的方法。我们采用上下文学习的方式来引导模型生成更加安全的内容。具体而言，当针对不安全的对话语境生成回答时，我们会从类似的对话语境中检索出安全回答的示例。我们发现，我们的方法不需要进行训练，就可以与强基线方法相媲美。例如，采用自动评估，我们发现我们最好的微调基线方法相较于我们的方法只能比“DiaSafety”中对不安全的对话语境生成安全回应多4.04%。最后，我们还提出了一种可进一步提高响应安全性的重新排名程序。",
    "tldr": "本文研究了一种检索式方法来减少Chatbots回应中偏见和有毒内容的方法，通过使用上下文学习来引导模型生成更加安全的内容，该方法性能与强基线方法相媲美，还提出了一个可进一步提高响应安全性的重新排名程序。"
}