{
    "title": "Fact or Artifact? Revise Layer-wise Relevance Propagation on various ANN Architectures. (arXiv:2302.12317v2 [cs.LG] UPDATED)",
    "abstract": "Layer-wise relevance propagation (LRP) is a widely used and powerful technique to reveal insights into various artificial neural network (ANN) architectures. LRP is often used in the context of image classification. The aim is to understand, which parts of the input sample have highest relevance and hence most influence on the model prediction. Relevance can be traced back through the network to attribute a certain score to each input pixel. Relevance scores are then combined and displayed as heat maps and give humans an intuitive visual understanding of classification models. Opening the black box to understand the classification engine in great detail is essential for domain experts to gain trust in ANN models. However, there are pitfalls in terms of model-inherent artifacts included in the obtained relevance maps, that can easily be missed. But for a valid interpretation, these artifacts must not be ignored. Here, we apply and revise LRP on various ANN architectures trained as class",
    "link": "http://arxiv.org/abs/2302.12317",
    "context": "Title: Fact or Artifact? Revise Layer-wise Relevance Propagation on various ANN Architectures. (arXiv:2302.12317v2 [cs.LG] UPDATED)\nAbstract: Layer-wise relevance propagation (LRP) is a widely used and powerful technique to reveal insights into various artificial neural network (ANN) architectures. LRP is often used in the context of image classification. The aim is to understand, which parts of the input sample have highest relevance and hence most influence on the model prediction. Relevance can be traced back through the network to attribute a certain score to each input pixel. Relevance scores are then combined and displayed as heat maps and give humans an intuitive visual understanding of classification models. Opening the black box to understand the classification engine in great detail is essential for domain experts to gain trust in ANN models. However, there are pitfalls in terms of model-inherent artifacts included in the obtained relevance maps, that can easily be missed. But for a valid interpretation, these artifacts must not be ignored. Here, we apply and revise LRP on various ANN architectures trained as class",
    "path": "papers/23/02/2302.12317.json",
    "total_tokens": 909,
    "translated_title": "事实还是人工制品？在不同的ANN架构上修订逐层相关传播",
    "translated_abstract": "逐层相关传播（LRP）是一种广泛使用且强大的技术，用于揭示各种人工神经网络（ANN）架构的见解。LRP经常在图像分类的背景下使用。其目的是理解输入样本的哪些部分具有最高相关性，从而对模型预测产生最大影响。可以通过网络追溯相关性，并为每个输入像素分配特定的得分。然后将相关性得分组合并显示为热图，使人类对分类模型有直观的视觉理解。打开黑盒以详细了解分类引擎对于领域专家来说至关重要，以获得对ANN模型的信任。然而，在获得的相关性图中存在模型本质上的人工制品陷阱，很容易被忽视。但是，为了进行有效的解释，不能忽视这些制品。在这里，我们在各种经过训练的ANN架构上应用和修订LRP。",
    "tldr": "本文介绍了逐层相关传播（LRP）在不同的人工神经网络（ANN）架构上的修订和应用。LRP通过可视化相关性热图揭示模型预测的影响原因，但需要注意其中可能存在的人工制品。"
}