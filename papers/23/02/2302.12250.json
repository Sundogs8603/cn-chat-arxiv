{
    "title": "Phase diagram of early training dynamics in deep neural networks: effect of the learning rate, depth, and width. (arXiv:2302.12250v2 [cs.LG] UPDATED)",
    "abstract": "We systematically analyze optimization dynamics in deep neural networks (DNNs) trained with stochastic gradient descent (SGD) and study the effect of learning rate $\\eta$, depth $d$, and width $w$ of the neural network. By analyzing the maximum eigenvalue $\\lambda^H_t$ of the Hessian of the loss, which is a measure of sharpness of the loss landscape, we find that the dynamics can show four distinct regimes: (i) an early time transient regime, (ii) an intermediate saturation regime, (iii) a progressive sharpening regime, and (iv) a late time ``edge of stability\" regime. The early and intermediate regimes (i) and (ii) exhibit a rich phase diagram depending on $\\eta \\equiv c / \\lambda_0^H $, $d$, and $w$. We identify several critical values of $c$, which separate qualitatively distinct phenomena in the early time dynamics of training loss and sharpness. Notably, we discover the opening up of a ``sharpness reduction\" phase, where sharpness decreases at early times, as $d$ and $1/w$ are inc",
    "link": "http://arxiv.org/abs/2302.12250",
    "context": "Title: Phase diagram of early training dynamics in deep neural networks: effect of the learning rate, depth, and width. (arXiv:2302.12250v2 [cs.LG] UPDATED)\nAbstract: We systematically analyze optimization dynamics in deep neural networks (DNNs) trained with stochastic gradient descent (SGD) and study the effect of learning rate $\\eta$, depth $d$, and width $w$ of the neural network. By analyzing the maximum eigenvalue $\\lambda^H_t$ of the Hessian of the loss, which is a measure of sharpness of the loss landscape, we find that the dynamics can show four distinct regimes: (i) an early time transient regime, (ii) an intermediate saturation regime, (iii) a progressive sharpening regime, and (iv) a late time ``edge of stability\" regime. The early and intermediate regimes (i) and (ii) exhibit a rich phase diagram depending on $\\eta \\equiv c / \\lambda_0^H $, $d$, and $w$. We identify several critical values of $c$, which separate qualitatively distinct phenomena in the early time dynamics of training loss and sharpness. Notably, we discover the opening up of a ``sharpness reduction\" phase, where sharpness decreases at early times, as $d$ and $1/w$ are inc",
    "path": "papers/23/02/2302.12250.json",
    "total_tokens": 998,
    "translated_title": "深度神经网络早期训练动力学的相图：学习率、深度和宽度的影响",
    "translated_abstract": "我们系统地分析用随机梯度下降（SGD）训练的深度神经网络（DNN）中的优化动力学，并研究学习率 $\\eta$、深度 $d$ 和宽度 $w$ 的神经网络的影响。通过分析损失函数的Hessian矩阵的最大特征值 $\\lambda^H_t$，即损失函数景观的陡峭程度的衡量，我们发现动力学可以展示出四种不同的状态：（i）早期临时状态，（ii）中间饱和状态，（iii）逐渐锐化状态和（iv）后期“稳定边缘”状态。早期和中间状态（i）和（ii）呈现出丰富的相图，取决于 $\\eta \\equiv c / \\lambda_0^H $、$d$ 和 $w$。我们确定了几个临界值 $c$，它们在训练损失和陡峭度的早期动力学中分隔出不同的现象。值得注意的是，我们发现了一个“陡峭度减小”相位的出现，其中陡峭度在早期时间下降，当 $d$ 和 $1/w$ 增加时。",
    "tldr": "我们研究了深度神经网络早期训练动力学的相图，发现四种不同的状态，并发现了一个“陡峭度减小”相位的出现。",
    "en_tdlr": "We analyze the phase diagram of early training dynamics in deep neural networks, identify four distinct regimes, and discover the opening up of a \"sharpness reduction\" phase."
}