{
    "title": "QLABGrad: a Hyperparameter-Free and Convergence-Guaranteed Scheme for Deep Learning",
    "abstract": "arXiv:2302.00252v2 Announce Type: replace  Abstract: The learning rate is a critical hyperparameter for deep learning tasks since it determines the extent to which the model parameters are updated during the learning course. However, the choice of learning rates typically depends on empirical judgment, which may not result in satisfactory outcomes without intensive try-and-error experiments. In this study, we propose a novel learning rate adaptation scheme called QLABGrad. Without any user-specified hyperparameter, QLABGrad automatically determines the learning rate by optimizing the Quadratic Loss Approximation-Based (QLAB) function for a given gradient descent direction, where only one extra forward propagation is required. We theoretically prove the convergence of QLABGrad with a smooth Lipschitz condition on the loss function. Experiment results on multiple architectures, including MLP, CNN, and ResNet, on MNIST, CIFAR10, and ImageNet datasets, demonstrate that QLABGrad outperforms",
    "link": "https://arxiv.org/abs/2302.00252",
    "context": "Title: QLABGrad: a Hyperparameter-Free and Convergence-Guaranteed Scheme for Deep Learning\nAbstract: arXiv:2302.00252v2 Announce Type: replace  Abstract: The learning rate is a critical hyperparameter for deep learning tasks since it determines the extent to which the model parameters are updated during the learning course. However, the choice of learning rates typically depends on empirical judgment, which may not result in satisfactory outcomes without intensive try-and-error experiments. In this study, we propose a novel learning rate adaptation scheme called QLABGrad. Without any user-specified hyperparameter, QLABGrad automatically determines the learning rate by optimizing the Quadratic Loss Approximation-Based (QLAB) function for a given gradient descent direction, where only one extra forward propagation is required. We theoretically prove the convergence of QLABGrad with a smooth Lipschitz condition on the loss function. Experiment results on multiple architectures, including MLP, CNN, and ResNet, on MNIST, CIFAR10, and ImageNet datasets, demonstrate that QLABGrad outperforms",
    "path": "papers/23/02/2302.00252.json",
    "total_tokens": 861,
    "translated_title": "QLABGrad: 一种无需超参数且收敛保证的深度学习方案",
    "translated_abstract": "学习率是深度学习任务中关键的超参数，因为它决定了模型参数在学习过程中的更新程度。然而，学习率的选择通常依赖于经验判断，在没有进行大量尝试和错误实验的情况下可能无法得到令人满意的结果。在本研究中，我们提出了一种名为QLABGrad的新型学习率自适应方案。QLABGrad无需任何用户指定的超参数，通过优化基于二次损失近似(QLAB)函数来自动确定学习率，仅需要进行一次额外的前向传播。我们在损失函数上理论上证明了QLABGrad的收敛性具有平滑的Lipschitz条件。在多种架构（包括MLP、CNN和ResNet）以及MNIST、CIFAR10和ImageNet数据集上进行的实验结果表明，QLABGrad表现优越。",
    "tldr": "QLABGrad是一种无需超参数的学习率自适应方案，通过优化QLAB函数自动确定学习率，并在平滑的Lipschitz条件下证明了其收敛性，实验证明其在多种架构和数据集上表现优越。",
    "en_tdlr": "QLABGrad is a hyperparameter-free learning rate adaptation scheme that automatically determines the learning rate by optimizing the QLAB function, proving its convergence under a smooth Lipschitz condition, with experiments showing its superiority across various architectures and datasets."
}