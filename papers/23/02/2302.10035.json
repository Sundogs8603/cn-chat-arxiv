{
    "title": "Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey. (arXiv:2302.10035v2 [cs.CV] UPDATED)",
    "abstract": "With the urgent demand for generalized deep models, many pre-trained big models are proposed, such as BERT, ViT, GPT, etc. Inspired by the success of these models in single domains (like computer vision and natural language processing), the multi-modal pre-trained big models have also drawn more and more attention in recent years. In this work, we give a comprehensive survey of these models and hope this paper could provide new insights and helps fresh researchers to track the most cutting-edge works. Specifically, we firstly introduce the background of multi-modal pre-training by reviewing the conventional deep learning, pre-training works in natural language process, computer vision, and speech. Then, we introduce the task definition, key challenges, and advantages of multi-modal pre-training models (MM-PTMs), and discuss the MM-PTMs with a focus on data, objectives, network architectures, and knowledge enhanced pre-training. After that, we introduce the downstream tasks used for the",
    "link": "http://arxiv.org/abs/2302.10035",
    "context": "Title: Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey. (arXiv:2302.10035v2 [cs.CV] UPDATED)\nAbstract: With the urgent demand for generalized deep models, many pre-trained big models are proposed, such as BERT, ViT, GPT, etc. Inspired by the success of these models in single domains (like computer vision and natural language processing), the multi-modal pre-trained big models have also drawn more and more attention in recent years. In this work, we give a comprehensive survey of these models and hope this paper could provide new insights and helps fresh researchers to track the most cutting-edge works. Specifically, we firstly introduce the background of multi-modal pre-training by reviewing the conventional deep learning, pre-training works in natural language process, computer vision, and speech. Then, we introduce the task definition, key challenges, and advantages of multi-modal pre-training models (MM-PTMs), and discuss the MM-PTMs with a focus on data, objectives, network architectures, and knowledge enhanced pre-training. After that, we introduce the downstream tasks used for the",
    "path": "papers/23/02/2302.10035.json",
    "total_tokens": 958,
    "translated_title": "大规模多模态预训练模型：综合调查",
    "translated_abstract": "随着对通用深度模型的迫切需求，许多预训练模型被提出，例如BERT，ViT，GPT等。受到这些模型在单一领域（如计算机视觉和自然语言处理）中的成功启发，多模态预训练大模型近年来也越来越受到关注。在这项工作中，我们对这些模型进行了全面调查，并希望本论文能提供新的见解，并帮助新研究人员追踪最前沿的工作。具体而言，我们首先通过回顾传统的深度学习、自然语言处理、计算机视觉和语音的预训练研究工作，介绍了多模态预训练的背景。然后，我们介绍了多模态预训练模型（MM-PTMs）的任务定义、关键挑战和优势，并重点讨论了数据、目标、网络架构和知识增强预训练方面的MM-PTMs。之后，我们介绍了用于后续任务的数据集以及评估指标。最后，我们提供了一个综合的比较和总结，并讨论了未来发展方向。",
    "tldr": "本文综合调查了大规模多模态预训练模型，介绍了背景、任务定义、关键挑战和优势，并讨论了数据、目标、网络架构和知识增强预训练等方面的相关内容。",
    "en_tdlr": "This comprehensive survey explores large-scale multi-modal pre-trained models, covering their background, task definitions, key challenges, and advantages, with a focus on data, objectives, network architectures, and knowledge enhanced pre-training."
}