{
    "title": "SGD learning on neural networks: leap complexity and saddle-to-saddle dynamics. (arXiv:2302.11055v2 [cs.LG] UPDATED)",
    "abstract": "We investigate the time complexity of SGD learning on fully-connected neural networks with isotropic data. We put forward a complexity measure -- the leap -- which measures how \"hierarchical\" target functions are. For $d$-dimensional uniform Boolean or isotropic Gaussian data, our main conjecture states that the time complexity to learn a function $f$ with low-dimensional support is $\\tilde\\Theta (d^{\\max(\\mathrm{Leap}(f),2)})$. We prove a version of this conjecture for a class of functions on Gaussian isotropic data and 2-layer neural networks, under additional technical assumptions on how SGD is run. We show that the training sequentially learns the function support with a saddle-to-saddle dynamic. Our result departs from [Abbe et al. 2022] by going beyond leap 1 (merged-staircase functions), and by going beyond the mean-field and gradient flow approximations that prohibit the full complexity control obtained here. Finally, we note that this gives an SGD complexity for the full train",
    "link": "http://arxiv.org/abs/2302.11055",
    "context": "Title: SGD learning on neural networks: leap complexity and saddle-to-saddle dynamics. (arXiv:2302.11055v2 [cs.LG] UPDATED)\nAbstract: We investigate the time complexity of SGD learning on fully-connected neural networks with isotropic data. We put forward a complexity measure -- the leap -- which measures how \"hierarchical\" target functions are. For $d$-dimensional uniform Boolean or isotropic Gaussian data, our main conjecture states that the time complexity to learn a function $f$ with low-dimensional support is $\\tilde\\Theta (d^{\\max(\\mathrm{Leap}(f),2)})$. We prove a version of this conjecture for a class of functions on Gaussian isotropic data and 2-layer neural networks, under additional technical assumptions on how SGD is run. We show that the training sequentially learns the function support with a saddle-to-saddle dynamic. Our result departs from [Abbe et al. 2022] by going beyond leap 1 (merged-staircase functions), and by going beyond the mean-field and gradient flow approximations that prohibit the full complexity control obtained here. Finally, we note that this gives an SGD complexity for the full train",
    "path": "papers/23/02/2302.11055.json",
    "total_tokens": 1006,
    "translated_title": "SGD学习神经网络: 跃迁复杂度与鞍到鞍动力学",
    "translated_abstract": "我们研究了随机梯度下降(SGD)学习具有各向同性数据的全连接神经网络的时间复杂度。我们提出了一种复杂度度量——跃迁，用来度量目标函数的\"层级\"程度。对于$d$维均匀布尔或各向同性高斯数据，我们的主要猜想是学习一个低维支持函数$f$的时间复杂度为$\\tilde\\Theta (d^{\\max(\\mathrm{Leap}(f),2)})$。我们在额外对SGD的技术假设下，证明了这个猜想在高斯各向同性数据和二层神经网络上的一个版本。我们展示出训练过程以鞍点到鞍点的动态方式逐渐学习了函数的支持。与[Abbe et al. 2022]不同，我们的结果超越了跃迁1(合并阶梯函数)的限制，并超越了均场和梯度流近似，在这里可以获得完全的复杂度控制。最后，我们指出这给出了完整训练的SGD复杂度。",
    "tldr": "该论文研究了随机梯度下降(SGD)算法在神经网络上的学习时间复杂度，提出了一种复杂度度量称为跃迁，证明了在高斯各向同性数据和二层神经网络上的研究结果，并展示了训练过程中函数支持的动态学习方法。",
    "en_tdlr": "This paper investigates the time complexity of SGD learning on neural networks, introduces a complexity measure called \"leap\", proves results on Gaussian isotropic data and 2-layer neural networks, and demonstrates a dynamic learning approach for the function support in the training process."
}