{
    "title": "Counterfactual Reasoning for Bias Evaluation and Detection in a Fairness under Unawareness setting. (arXiv:2302.08204v2 [cs.LG] UPDATED)",
    "abstract": "Current AI regulations require discarding sensitive features (e.g., gender, race, religion) in the algorithm's decision-making process to prevent unfair outcomes. However, even without sensitive features in the training set, algorithms can persist in discrimination. Indeed, when sensitive features are omitted (fairness under unawareness), they could be inferred through non-linear relations with the so called proxy features. In this work, we propose a way to reveal the potential hidden bias of a machine learning model that can persist even when sensitive features are discarded. This study shows that it is possible to unveil whether the black-box predictor is still biased by exploiting counterfactual reasoning. In detail, when the predictor provides a negative classification outcome, our approach first builds counterfactual examples for a discriminated user category to obtain a positive outcome. Then, the same counterfactual samples feed an external classifier (that targets a sensitive f",
    "link": "http://arxiv.org/abs/2302.08204",
    "context": "Title: Counterfactual Reasoning for Bias Evaluation and Detection in a Fairness under Unawareness setting. (arXiv:2302.08204v2 [cs.LG] UPDATED)\nAbstract: Current AI regulations require discarding sensitive features (e.g., gender, race, religion) in the algorithm's decision-making process to prevent unfair outcomes. However, even without sensitive features in the training set, algorithms can persist in discrimination. Indeed, when sensitive features are omitted (fairness under unawareness), they could be inferred through non-linear relations with the so called proxy features. In this work, we propose a way to reveal the potential hidden bias of a machine learning model that can persist even when sensitive features are discarded. This study shows that it is possible to unveil whether the black-box predictor is still biased by exploiting counterfactual reasoning. In detail, when the predictor provides a negative classification outcome, our approach first builds counterfactual examples for a discriminated user category to obtain a positive outcome. Then, the same counterfactual samples feed an external classifier (that targets a sensitive f",
    "path": "papers/23/02/2302.08204.json",
    "total_tokens": 937,
    "translated_title": "在无意识公平性设置中，对偏见评估和检测进行反事实推理",
    "translated_abstract": "当前的AI法规要求在算法的决策过程中丢弃敏感特征（如性别、种族、宗教），以防止不公平的结果。然而，即使在训练集中没有敏感特征，算法仍可能持续进行歧视。事实上，在敏感特征被忽略的情况下（无意识公平性），通过非线性关系，可以通过所谓的代理特征推测出这些敏感特征。在这项工作中，我们提出了一种揭示即使在丢弃敏感特征的情况下机器学习模型可能仍存在的潜在偏见的方法。本研究表明，通过利用反事实推理，可以揭示出黑盒预测器是否仍存在偏见。具体而言，当预测器提供负面分类结果时，我们的方法首先为被歧视的用户类别构建反事实示例，以获得正面结果。然后，相同的反事实样本被用于外部分类器（针对敏感特征）进行训练和测试，以评估黑盒预测器的偏见。",
    "tldr": "本研究提出了一种方法来揭示即使在丢弃敏感特征的情况下机器学习模型可能仍存在的潜在偏见，并通过利用反事实推理来检测黑盒预测器的偏见。",
    "en_tdlr": "This study proposes a method to reveal potential biases that may persist in a machine learning model even when sensitive features are not considered, and detects these biases by utilizing counterfactual reasoning."
}