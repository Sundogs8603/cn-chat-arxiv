{
    "title": "CrossSpeech: Speaker-independent Acoustic Representation for Cross-lingual Speech Synthesis. (arXiv:2302.14370v2 [cs.SD] UPDATED)",
    "abstract": "While recent text-to-speech (TTS) systems have made remarkable strides toward human-level quality, the performance of cross-lingual TTS lags behind that of intra-lingual TTS. This gap is mainly rooted from the speaker-language entanglement problem in cross-lingual TTS. In this paper, we propose CrossSpeech which improves the quality of cross-lingual speech by effectively disentangling speaker and language information in the level of acoustic feature space. Specifically, CrossSpeech decomposes the speech generation pipeline into the speaker-independent generator (SIG) and speaker-dependent generator (SDG). The SIG produces the speaker-independent acoustic representation which is not biased to specific speaker distributions. On the other hand, the SDG models speaker-dependent speech variation that characterizes speaker attributes. By handling each information separately, CrossSpeech can obtain disentangled speaker and language representations. From the experiments, we verify that CrossSp",
    "link": "http://arxiv.org/abs/2302.14370",
    "context": "Title: CrossSpeech: Speaker-independent Acoustic Representation for Cross-lingual Speech Synthesis. (arXiv:2302.14370v2 [cs.SD] UPDATED)\nAbstract: While recent text-to-speech (TTS) systems have made remarkable strides toward human-level quality, the performance of cross-lingual TTS lags behind that of intra-lingual TTS. This gap is mainly rooted from the speaker-language entanglement problem in cross-lingual TTS. In this paper, we propose CrossSpeech which improves the quality of cross-lingual speech by effectively disentangling speaker and language information in the level of acoustic feature space. Specifically, CrossSpeech decomposes the speech generation pipeline into the speaker-independent generator (SIG) and speaker-dependent generator (SDG). The SIG produces the speaker-independent acoustic representation which is not biased to specific speaker distributions. On the other hand, the SDG models speaker-dependent speech variation that characterizes speaker attributes. By handling each information separately, CrossSpeech can obtain disentangled speaker and language representations. From the experiments, we verify that CrossSp",
    "path": "papers/23/02/2302.14370.json",
    "total_tokens": 946,
    "translated_title": "跨语音: 面向跨语音合成的语音无关特征表示方法",
    "translated_abstract": "尽管最近的文本到语音合成系统已经在实现人类水平质量方面取得了显著进展，但跨语音合成的性能仍然落后于同种语言的语音合成。本文提出了一种名为CrossSpeech的方法，通过在声学特征空间的级别上有效地分离说话人和语言信息，改善跨语音语音的质量。具体而言，CrossSpeech将语音生成流程分解为语音无关生成器（SIG）和说话人相关的生成器（SDG）。 SIG生成与特定说话人分布无关的语音无关声学表示。另一方面，SDG模型说话人相关的语音变化，表征说话人属性。通过将每个信息分开处理，CrossSpeech可以获得分离的说话人和语言表示。从实验中，我们验证了CrossSpeech在自然度和说话人相似性方面优于现有的跨语音TTS模型，并在Blizzard Challenge 2019任务中取得了最先进的性能。",
    "tldr": "CrossSpeech提出了一种改善跨语音TTS质量的方法，通过将语音表示分解为语音无关生成器和说话人相关的生成器并分别处理，实现了分离的说话人和语言表示，同时在Blizzard Challenge 2019任务中取得了最先进的性能。",
    "en_tdlr": "CrossSpeech proposes a method to improve the quality of cross-lingual TTS by effectively disentangling speaker and language information in the acoustic feature space, achieving separated speaker and language representations, and achieving state-of-the-art performance on the Blizzard Challenge 2019 task."
}