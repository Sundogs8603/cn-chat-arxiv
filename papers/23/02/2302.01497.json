{
    "title": "Gradient Estimation for Unseen Domain Risk Minimization with Pre-Trained Models. (arXiv:2302.01497v3 [cs.LG] UPDATED)",
    "abstract": "Domain generalization aims to build generalized models that perform well on unseen domains when only source domains are available for model optimization. Recent studies have shown that large-scale pre-trained models can enhance domain generalization by leveraging their generalization power. However, these pre-trained models lack target task-specific knowledge yet due to discrepancies between the pre-training objectives and the target task. Although the task-specific knowledge could be learned from source domains by fine-tuning, this hurts the generalization power of pre-trained models due to gradient bias toward the source domains. To alleviate this problem, we propose a new domain generalization method that estimates unobservable gradients that reduce potential risks in unseen domains using a large-scale pre-trained model. These estimated unobservable gradients allow the pre-trained model to learn task-specific knowledge further while preserving its generalization ability by relieving",
    "link": "http://arxiv.org/abs/2302.01497",
    "context": "Title: Gradient Estimation for Unseen Domain Risk Minimization with Pre-Trained Models. (arXiv:2302.01497v3 [cs.LG] UPDATED)\nAbstract: Domain generalization aims to build generalized models that perform well on unseen domains when only source domains are available for model optimization. Recent studies have shown that large-scale pre-trained models can enhance domain generalization by leveraging their generalization power. However, these pre-trained models lack target task-specific knowledge yet due to discrepancies between the pre-training objectives and the target task. Although the task-specific knowledge could be learned from source domains by fine-tuning, this hurts the generalization power of pre-trained models due to gradient bias toward the source domains. To alleviate this problem, we propose a new domain generalization method that estimates unobservable gradients that reduce potential risks in unseen domains using a large-scale pre-trained model. These estimated unobservable gradients allow the pre-trained model to learn task-specific knowledge further while preserving its generalization ability by relieving",
    "path": "papers/23/02/2302.01497.json",
    "total_tokens": 912,
    "translated_title": "使用预训练模型进行未知领域风险最小化的梯度估计",
    "translated_abstract": "领域广义化旨在构建在只有源领域用于模型优化时在未知领域上表现良好的广义模型。最近的研究表明，大规模的预训练模型可以通过利用它们的广义能力来增强领域广义化。然而，由于预训练目标与目标任务之间存在差异，这些预训练模型缺乏目标任务特定的知识。虽然可以通过微调从源领域中学习任务特定的知识，但由于梯度偏差导致对源领域的广义能力受损。为了缓解这个问题，我们提出了一种新的领域广义化方法，它通过使用大规模的预训练模型估计不可观测的梯度，在未知领域中减少潜在风险。这些估计的不可观测梯度使预训练模型能够进一步学习任务特定的知识，同时保持其广义能力，从而减轻了潜在的风险。",
    "tldr": "该论文提出了一种新的领域广义化方法，使用大规模预训练模型估计不可观测梯度来减少未知领域中的潜在风险，从而增强模型的广义能力。",
    "en_tdlr": "This paper proposes a new domain generalization method that estimates unobservable gradients using large-scale pre-trained models to reduce potential risks in unseen domains, thereby enhancing the model's generalization power."
}