{
    "title": "Towards Memory- and Time-Efficient Backpropagation for Training Spiking Neural Networks. (arXiv:2302.14311v3 [cs.NE] UPDATED)",
    "abstract": "Spiking Neural Networks (SNNs) are promising energy-efficient models for neuromorphic computing. For training the non-differentiable SNN models, the backpropagation through time (BPTT) with surrogate gradients (SG) method has achieved high performance. However, this method suffers from considerable memory cost and training time during training. In this paper, we propose the Spatial Learning Through Time (SLTT) method that can achieve high performance while greatly improving training efficiency compared with BPTT. First, we show that the backpropagation of SNNs through the temporal domain contributes just a little to the final calculated gradients. Thus, we propose to ignore the unimportant routes in the computational graph during backpropagation. The proposed method reduces the number of scalar multiplications and achieves a small memory occupation that is independent of the total time steps. Furthermore, we propose a variant of SLTT, called SLTT-K, that allows backpropagation only at ",
    "link": "http://arxiv.org/abs/2302.14311",
    "context": "Title: Towards Memory- and Time-Efficient Backpropagation for Training Spiking Neural Networks. (arXiv:2302.14311v3 [cs.NE] UPDATED)\nAbstract: Spiking Neural Networks (SNNs) are promising energy-efficient models for neuromorphic computing. For training the non-differentiable SNN models, the backpropagation through time (BPTT) with surrogate gradients (SG) method has achieved high performance. However, this method suffers from considerable memory cost and training time during training. In this paper, we propose the Spatial Learning Through Time (SLTT) method that can achieve high performance while greatly improving training efficiency compared with BPTT. First, we show that the backpropagation of SNNs through the temporal domain contributes just a little to the final calculated gradients. Thus, we propose to ignore the unimportant routes in the computational graph during backpropagation. The proposed method reduces the number of scalar multiplications and achieves a small memory occupation that is independent of the total time steps. Furthermore, we propose a variant of SLTT, called SLTT-K, that allows backpropagation only at ",
    "path": "papers/23/02/2302.14311.json",
    "total_tokens": 902,
    "translated_title": "面向训练脉冲神经网络的内存和时间高效反向传播",
    "translated_abstract": "脉冲神经网络（SNN）是能源高效的神经形态计算的有前途的模型。为了训练不可微分的SNN模型，通过代理梯度（SG）的时间反向传播（BPTT）方法已经取得了很高的性能。然而，这种方法在训练过程中存在较大的内存开销和训练时间。本文提出了一种空间学习通过时间（SLTT）方法，可以在与BPTT相比大大提高训练效率的同时实现高性能。首先，我们表明SNN的反向传播通过时间域对最终计算的梯度的贡献很小。因此，我们提出在反向传播过程中忽略计算图中的不重要路径。所提出的方法减少了标量乘法的数量，并实现了独立于总时间步长的小内存占用。此外，我们还提出了SLTT的一种变体SLTT-K，该方法只允许反向传播在特定时间步长上进行。",
    "tldr": "本文提出了一种空间学习通过时间（SLTT）方法以提高训练脉冲神经网络的效率。该方法忽略了不重要的路径，减少了内存开销和训练时间。"
}