{
    "title": "Data efficiency and extrapolation trends in neural network interatomic potentials. (arXiv:2302.05823v2 [cs.LG] UPDATED)",
    "abstract": "Over the last few years, key architectural advances have been proposed for neural network interatomic potentials (NNIPs), such as incorporating message-passing networks, equivariance, or many-body expansion terms. Although modern NNIP models exhibit small differences in energy/forces errors, improvements in accuracy are still considered the main target when developing new NNIP architectures. In this work, we show how architectural and optimization choices influence the generalization of NNIPs, revealing trends in molecular dynamics (MD) stability, data efficiency, and loss landscapes. Using the 3BPA dataset, we show that test errors in NNIP follow a scaling relation and can be robust to noise, but cannot predict MD stability in the high-accuracy regime. To circumvent this problem, we propose the use of loss landscape visualizations and a metric of loss entropy for predicting the generalization power of NNIPs. With a large-scale study on NequIP and MACE, we show that the loss entropy pr",
    "link": "http://arxiv.org/abs/2302.05823",
    "context": "Title: Data efficiency and extrapolation trends in neural network interatomic potentials. (arXiv:2302.05823v2 [cs.LG] UPDATED)\nAbstract: Over the last few years, key architectural advances have been proposed for neural network interatomic potentials (NNIPs), such as incorporating message-passing networks, equivariance, or many-body expansion terms. Although modern NNIP models exhibit small differences in energy/forces errors, improvements in accuracy are still considered the main target when developing new NNIP architectures. In this work, we show how architectural and optimization choices influence the generalization of NNIPs, revealing trends in molecular dynamics (MD) stability, data efficiency, and loss landscapes. Using the 3BPA dataset, we show that test errors in NNIP follow a scaling relation and can be robust to noise, but cannot predict MD stability in the high-accuracy regime. To circumvent this problem, we propose the use of loss landscape visualizations and a metric of loss entropy for predicting the generalization power of NNIPs. With a large-scale study on NequIP and MACE, we show that the loss entropy pr",
    "path": "papers/23/02/2302.05823.json",
    "total_tokens": 950,
    "translated_title": "神经网络原子势的数据效率和外推趋势",
    "translated_abstract": "近年来，神经网络原子势（NNIP）的关键架构进步包括了消息传递网络、等变性或多体展开项。尽管现代NNIP模型在能量/力误差方面存在微小差异，但在开发新的NNIP体系结构时，提高准确性仍然被视为主要目标。在本研究中，我们展示了体系结构和优化选择如何影响NNIP的普遍化，揭示了分子动力学（MD）稳定性、数据效率和损失空间的趋势。利用3BPA数据集，我们表明NNIP的测试误差遵循比例关系，并且可以耐受噪声，但不能预测高精度区域的MD稳定性。为了解决这个问题，我们提出了使用损失空间可视化和损失熵的度量来预测NNIP的泛化能力。通过对NequIP和MACE进行大规模研究，我们展示了损失熵的优势。",
    "tldr": "本文研究了神经网络原子势模型在体系架构和优化选择方面的影响，揭示了分子动力学稳定性、数据效率和损失空间等趋势。研究结果表明，使用损失空间可视化和损失熵的度量可以预测NNIP的泛化能力。"
}