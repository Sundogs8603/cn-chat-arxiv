{
    "title": "Adversarial Calibrated Regression for Online Decision Making. (arXiv:2302.12196v2 [cs.LG] UPDATED)",
    "abstract": "Accurately estimating uncertainty is an essential component of decision-making and forecasting in machine learning. However, existing uncertainty estimation methods may fail when data no longer follows the distribution seen during training. Here, we introduce online uncertainty estimation algorithms that are guaranteed to be reliable on arbitrary streams of data points, including data chosen by an adversary. Specifically, our algorithms perform post-hoc recalibration of a black-box regression model and produce outputs that are provably calibrated -- i.e., an 80% confidence interval will contain the true outcome 80% of the time -- and that have low regret relative to the learning objective of the base model. We apply our algorithms in the context of Bayesian optimization, an online model-based decision-making task in which the data distribution shifts over time, and observe accelerated convergence to improved optima. Our results suggest that robust uncertainty quantification has the pot",
    "link": "http://arxiv.org/abs/2302.12196",
    "context": "Title: Adversarial Calibrated Regression for Online Decision Making. (arXiv:2302.12196v2 [cs.LG] UPDATED)\nAbstract: Accurately estimating uncertainty is an essential component of decision-making and forecasting in machine learning. However, existing uncertainty estimation methods may fail when data no longer follows the distribution seen during training. Here, we introduce online uncertainty estimation algorithms that are guaranteed to be reliable on arbitrary streams of data points, including data chosen by an adversary. Specifically, our algorithms perform post-hoc recalibration of a black-box regression model and produce outputs that are provably calibrated -- i.e., an 80% confidence interval will contain the true outcome 80% of the time -- and that have low regret relative to the learning objective of the base model. We apply our algorithms in the context of Bayesian optimization, an online model-based decision-making task in which the data distribution shifts over time, and observe accelerated convergence to improved optima. Our results suggest that robust uncertainty quantification has the pot",
    "path": "papers/23/02/2302.12196.json",
    "total_tokens": 959,
    "translated_title": "对在线决策制定的对抗校准回归",
    "translated_abstract": "在机器学习中，精确估计不确定性是决策制定和预测的基本组成部分。但是，现有的不确定性估计方法可能会失败，当数据不再遵循训练时所见的分布时。本文提出了在线不确定性估计算法，这些算法能够在任意数据流上保证可靠性，包括被对手选择的数据。具体而言，我们的算法对黑盒回归模型进行事后校准，并产生可证明校准的输出，即80％置信区间将在80％的时间内包含真实结果，并且相对于基本模型的学习目标具有较低的遗憾。我们将我们的算法应用于贝叶斯优化的情境中，这是一种在线的基于模型的决策制定任务，在这种任务中，数据分布随时间变化，观察到加速收敛到改进的最优解。我们的结果表明，强大的不确定性量化有助于实现机器学习在关键领域如医疗保健和金融领域的安全使用。",
    "tldr": "本文提出在线的不确定性估计算法，这些算法可以在任意数据流上保证可靠性，并应用于贝叶斯优化，加速收敛到改进的最优解，有助于实现机器学习在关键领域如医疗保健和金融领域的安全使用。",
    "en_tdlr": "This paper proposes online uncertainty estimation algorithms that are reliable on arbitrary streams of data points, and applies them in the context of Bayesian optimization to accelerate convergence to improved optima, enabling safe use of machine learning in critical domains such as healthcare and finance."
}