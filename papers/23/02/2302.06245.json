{
    "title": "Calibrating a Deep Neural Network with Its Predecessors. (arXiv:2302.06245v2 [cs.LG] UPDATED)",
    "abstract": "Confidence calibration - the process to calibrate the output probability distribution of neural networks - is essential for safety-critical applications of such networks. Recent works verify the link between mis-calibration and overfitting. However, early stopping, as a well-known technique to mitigate overfitting, fails to calibrate networks. In this work, we study the limitions of early stopping and comprehensively analyze the overfitting problem of a network considering each individual block. We then propose a novel regularization method, predecessor combination search (PCS), to improve calibration by searching a combination of best-fitting block predecessors, where block predecessors are the corresponding network blocks with weight parameters from earlier training stages. PCS achieves the state-of-the-art calibration performance on multiple datasets and architectures. In addition, PCS improves model robustness under dataset distribution shift.",
    "link": "http://arxiv.org/abs/2302.06245",
    "context": "Title: Calibrating a Deep Neural Network with Its Predecessors. (arXiv:2302.06245v2 [cs.LG] UPDATED)\nAbstract: Confidence calibration - the process to calibrate the output probability distribution of neural networks - is essential for safety-critical applications of such networks. Recent works verify the link between mis-calibration and overfitting. However, early stopping, as a well-known technique to mitigate overfitting, fails to calibrate networks. In this work, we study the limitions of early stopping and comprehensively analyze the overfitting problem of a network considering each individual block. We then propose a novel regularization method, predecessor combination search (PCS), to improve calibration by searching a combination of best-fitting block predecessors, where block predecessors are the corresponding network blocks with weight parameters from earlier training stages. PCS achieves the state-of-the-art calibration performance on multiple datasets and architectures. In addition, PCS improves model robustness under dataset distribution shift.",
    "path": "papers/23/02/2302.06245.json",
    "total_tokens": 908,
    "translated_title": "用前身神经网络来校准深度神经网络",
    "translated_abstract": "对于神经网络的安全关键应用而言，置信度校准——即校准神经网络的输出概率分布——是至关重要的。最近的研究验证了校准不足与过拟合之间的联系。然而，作为一种防止过拟合的常用技术，早期停止不能够校准神经网络。在本工作中，我们研究了早期停止的局限性，并全面分析了网络的过拟合问题，考虑到每个单独的模块。然后，我们提出了一种新的正则化方法，“前身组合搜索”（PCS），通过寻找最佳适合块的前身组合来改善校准。块的前身是具有较早训练阶段的权重参数的相应网络块。PCS在多个数据集和架构上实现了最先进的校准性能。此外，PCS提高了模型在数据集分布转移下的稳健性。",
    "tldr": "这篇论文提出了一种利用前身神经网络校准深度神经网络的方法，该方法通过搜索最佳适合块的前身组合来改善校准。这种方法在多个数据集和架构上实现了最先进的校准性能，并提高了模型在数据集分布转移下的稳健性。",
    "en_tdlr": "This paper proposes a method of calibrating deep neural networks using their predecessors, which improves calibration by searching for a combination of best-fitting block predecessors. The method achieves state-of-the-art calibration performance on multiple datasets and architectures, and improves the model's robustness under dataset distribution shifts."
}