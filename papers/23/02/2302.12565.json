{
    "title": "Variational Linearized Laplace Approximation for Bayesian Deep Learning",
    "abstract": "The Linearized Laplace Approximation (LLA) has been recently used to perform uncertainty estimation on the predictions of pre-trained deep neural networks (DNNs). However, its widespread application is hindered by significant computational costs, particularly in scenarios with a large number of training points or DNN parameters. Consequently, additional approximations of LLA, such as Kronecker-factored or diagonal approximate GGN matrices, are utilized, potentially compromising the model's performance. To address these challenges, we propose a new method for approximating LLA using a variational sparse Gaussian Process (GP). Our method is based on the dual RKHS formulation of GPs and retains as the predictive mean the output of the original DNN. Furthermore, it allows for efficient stochastic optimization, which results in sub-linear training time in the size of the training dataset. Specifically, its training cost is independent of the number of training points. We compare our propose",
    "link": "https://rss.arxiv.org/abs/2302.12565",
    "context": "Title: Variational Linearized Laplace Approximation for Bayesian Deep Learning\nAbstract: The Linearized Laplace Approximation (LLA) has been recently used to perform uncertainty estimation on the predictions of pre-trained deep neural networks (DNNs). However, its widespread application is hindered by significant computational costs, particularly in scenarios with a large number of training points or DNN parameters. Consequently, additional approximations of LLA, such as Kronecker-factored or diagonal approximate GGN matrices, are utilized, potentially compromising the model's performance. To address these challenges, we propose a new method for approximating LLA using a variational sparse Gaussian Process (GP). Our method is based on the dual RKHS formulation of GPs and retains as the predictive mean the output of the original DNN. Furthermore, it allows for efficient stochastic optimization, which results in sub-linear training time in the size of the training dataset. Specifically, its training cost is independent of the number of training points. We compare our propose",
    "path": "papers/23/02/2302.12565.json",
    "total_tokens": 936,
    "translated_title": "变分线性化Laplace近似在贝叶斯深度学习中的应用",
    "translated_abstract": "最近，线性化Laplace近似（LLA）被用来对预训练深度神经网络（DNNs）的预测进行不确定性估计。然而，在训练点或DNN参数较多的情况下，其广泛应用受到了计算成本的限制。因此，其他LLA的近似方法，如Kronecker分解或对角线GGN矩阵的近似，被使用，可能会影响模型的性能。为了解决这些挑战，我们提出了一种基于变分稀疏高斯过程（GP）的LLA近似方法。我们的方法基于GP的对偶RKHS公式，并保留了原始DNN的预测均值。此外，它允许有效的随机优化，从而在训练数据集的大小中实现子线性训练时间。具体而言，其训练成本与训练点的数量无关。我们比较了我们的方法与其他近似方法的性能，并展示了其在准确性和计算效率方面的优势。",
    "tldr": "本论文提出了一种基于变分稀疏高斯过程的方法，用于近似线性化Laplace近似在贝叶斯深度学习中的应用。该方法保留了原始DNN的预测均值，并具有高效的随机优化，训练成本与训练点的数量无关。",
    "en_tdlr": "This paper proposes a method based on variational sparse Gaussian Process to approximate the Linearized Laplace Approximation in Bayesian deep learning. The method retains the predictive mean of the original DNN and allows for efficient stochastic optimization, with training cost independent of the number of training points."
}