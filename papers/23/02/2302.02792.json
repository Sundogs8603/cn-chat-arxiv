{
    "title": "Dealing With Non-stationarity in Decentralized Cooperative Multi-Agent Deep Reinforcement Learning via Multi-Timescale Learning. (arXiv:2302.02792v2 [cs.LG] UPDATED)",
    "abstract": "Decentralized cooperative multi-agent deep reinforcement learning (MARL) can be a versatile learning framework, particularly in scenarios where centralized training is either not possible or not practical. One of the critical challenges in decentralized deep MARL is the non-stationarity of the learning environment when multiple agents are learning concurrently. A commonly used and efficient scheme for decentralized MARL is independent learning in which agents concurrently update their policies independently of each other. We first show that independent learning does not always converge, while sequential learning where agents update their policies one after another in a sequence is guaranteed to converge to an agent-by-agent optimal solution. In sequential learning, when one agent updates its policy, all other agent's policies are kept fixed, alleviating the challenge of non-stationarity due to simultaneous updates in other agents' policies. However, it can be slow because only one agen",
    "link": "http://arxiv.org/abs/2302.02792",
    "context": "Title: Dealing With Non-stationarity in Decentralized Cooperative Multi-Agent Deep Reinforcement Learning via Multi-Timescale Learning. (arXiv:2302.02792v2 [cs.LG] UPDATED)\nAbstract: Decentralized cooperative multi-agent deep reinforcement learning (MARL) can be a versatile learning framework, particularly in scenarios where centralized training is either not possible or not practical. One of the critical challenges in decentralized deep MARL is the non-stationarity of the learning environment when multiple agents are learning concurrently. A commonly used and efficient scheme for decentralized MARL is independent learning in which agents concurrently update their policies independently of each other. We first show that independent learning does not always converge, while sequential learning where agents update their policies one after another in a sequence is guaranteed to converge to an agent-by-agent optimal solution. In sequential learning, when one agent updates its policy, all other agent's policies are kept fixed, alleviating the challenge of non-stationarity due to simultaneous updates in other agents' policies. However, it can be slow because only one agen",
    "path": "papers/23/02/2302.02792.json",
    "total_tokens": 929,
    "translated_title": "多时间尺度学习在处理分布式合作多智能体深度强化学习中的非平稳性问题中的应用",
    "translated_abstract": "分布式合作的多智能体深度强化学习（MARL）可以成为一种通用的学习框架，特别适用于无法进行集中式训练或不实际的场景。在分布式深度MARL中，当多个智能体同时学习时，学习环境的非平稳性是一个关键挑战。一个常用且高效的分布式MARL方案是独立学习，在这种方案中，智能体独立更新策略。我们首先证明了独立学习并非总能收敛，而顺序学习，即智能体依次更新策略，能够保证收敛到一个智能体最优解。在顺序学习中，当一个智能体更新策略时，其他智能体的策略保持不变，缓解了由于其他智能体策略的同时更新而导致的非平稳性挑战。然而，顺序学习速度较慢，因为只有一个智能体进行更新。",
    "tldr": "本文提出了一种多时间尺度学习的方法来解决分布式合作多智能体深度强化学习中的非平稳性问题，通过顺序学习的方式更新智能体的策略可以保证收敛性。",
    "en_tdlr": "This paper proposes a multi-timescale learning approach to address the non-stationarity challenge in decentralized cooperative multi-agent deep reinforcement learning. Sequential learning, where agents update their policies one by one, ensures convergence to agent-by-agent optimal solutions."
}