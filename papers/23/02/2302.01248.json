{
    "title": "Robust Markov Decision Processes without Model Estimation. (arXiv:2302.01248v2 [stat.ML] UPDATED)",
    "abstract": "Robust Markov Decision Processes (MDPs) are receiving much attention in learning a robust policy which is less sensitive to environment changes. There are an increasing number of works analyzing sample-efficiency of robust MDPs. However, there are two major barriers to applying robust MDPs in practice. First, most works study robust MDPs in a model-based regime, where the transition probability needs to be estimated and requires a large amount of memories $\\mathcal{O}(|\\mathcal{S}|^2|\\mathcal{A}|)$. Second, prior work typically assumes a strong oracle to obtain the optimal solution as an intermediate step to solve robust MDPs. However, in practice, such an oracle does not exist usually. To remove the oracle, we transform the original robust MDPs into an alternative form, which allows us to use stochastic gradient methods to solve the robust MDPs. Moreover, we prove the alternative form still plays a similar role as the original form. With this new formulation, we devise a sample-effici",
    "link": "http://arxiv.org/abs/2302.01248",
    "context": "Title: Robust Markov Decision Processes without Model Estimation. (arXiv:2302.01248v2 [stat.ML] UPDATED)\nAbstract: Robust Markov Decision Processes (MDPs) are receiving much attention in learning a robust policy which is less sensitive to environment changes. There are an increasing number of works analyzing sample-efficiency of robust MDPs. However, there are two major barriers to applying robust MDPs in practice. First, most works study robust MDPs in a model-based regime, where the transition probability needs to be estimated and requires a large amount of memories $\\mathcal{O}(|\\mathcal{S}|^2|\\mathcal{A}|)$. Second, prior work typically assumes a strong oracle to obtain the optimal solution as an intermediate step to solve robust MDPs. However, in practice, such an oracle does not exist usually. To remove the oracle, we transform the original robust MDPs into an alternative form, which allows us to use stochastic gradient methods to solve the robust MDPs. Moreover, we prove the alternative form still plays a similar role as the original form. With this new formulation, we devise a sample-effici",
    "path": "papers/23/02/2302.01248.json",
    "total_tokens": 976,
    "translated_title": "无需模型估计的鲁棒马尔科夫决策过程",
    "translated_abstract": "鲁棒马尔科夫决策过程（MDPs）在学习一个对环境变化不敏感的鲁棒策略方面受到了广泛关注。目前有越来越多的工作分析鲁棒MDPs的采样效率。然而，在实际应用中应用鲁棒MDPs存在两个主要障碍。首先，大多数工作都是在模型为基础的情况下研究鲁棒MDPs，其中转移概率需要进行估计，需要大量的记忆（O(|S|²|A|)）。其次，之前的工作通常假设存在一个强大的优化器来获得最优解，用作解决鲁棒MDPs的中间步骤。然而，在实践中，通常并不存在这样的优化器。为了去除优化器的依赖，我们将原始的鲁棒MDPs转化为另一种形式，使我们能够使用随机梯度方法来求解鲁棒MDPs。此外，我们证明了这种替代形式仍然具有类似的作用。通过这种新的公式，我们设计了一种采样有效的算法来解决鲁棒MDPs。",
    "tldr": "这篇论文提出了一种无需模型估计的鲁棒MDPs算法，通过将原始问题转化为另一种形式，并使用随机梯度方法求解，从而去除了对优化器的依赖。"
}