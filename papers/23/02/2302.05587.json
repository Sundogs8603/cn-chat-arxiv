{
    "title": "Hierarchical Optimization-Derived Learning. (arXiv:2302.05587v2 [cs.LG] UPDATED)",
    "abstract": "In recent years, by utilizing optimization techniques to formulate the propagation of deep model, a variety of so-called Optimization-Derived Learning (ODL) approaches have been proposed to address diverse learning and vision tasks. Although having achieved relatively satisfying practical performance, there still exist fundamental issues in existing ODL methods. In particular, current ODL methods tend to consider model construction and learning as two separate phases, and thus fail to formulate their underlying coupling and depending relationship. In this work, we first establish a new framework, named Hierarchical ODL (HODL), to simultaneously investigate the intrinsic behaviors of optimization-derived model construction and its corresponding learning process. Then we rigorously prove the joint convergence of these two sub-tasks, from the perspectives of both approximation quality and stationary analysis. To our best knowledge, this is the first theoretical guarantee for these two cou",
    "link": "http://arxiv.org/abs/2302.05587",
    "context": "Title: Hierarchical Optimization-Derived Learning. (arXiv:2302.05587v2 [cs.LG] UPDATED)\nAbstract: In recent years, by utilizing optimization techniques to formulate the propagation of deep model, a variety of so-called Optimization-Derived Learning (ODL) approaches have been proposed to address diverse learning and vision tasks. Although having achieved relatively satisfying practical performance, there still exist fundamental issues in existing ODL methods. In particular, current ODL methods tend to consider model construction and learning as two separate phases, and thus fail to formulate their underlying coupling and depending relationship. In this work, we first establish a new framework, named Hierarchical ODL (HODL), to simultaneously investigate the intrinsic behaviors of optimization-derived model construction and its corresponding learning process. Then we rigorously prove the joint convergence of these two sub-tasks, from the perspectives of both approximation quality and stationary analysis. To our best knowledge, this is the first theoretical guarantee for these two cou",
    "path": "papers/23/02/2302.05587.json",
    "total_tokens": 867,
    "translated_title": "分层优化导出学习",
    "translated_abstract": "近年来，通过利用优化技术来规划深度模型的传播，提出了各种所谓的优化导出学习（ODL）方法来解决不同的学习和视觉任务。尽管在实践中取得了令人满意的性能，但现有的ODL方法仍存在一些基本问题。特别地，当前的ODL方法倾向于将模型构建和学习视为两个独立的阶段，因此未能准确表达它们之间的相互关系。在这项研究中，我们首先建立了一个名为Hierarchical ODL（HODL）的新框架，同时研究了优化导出模型构建和相应学习过程的内在行为。然后，我们严格证明了这两个子任务的联合收敛性，从逼近质量和稳定性分析的角度展示了证明。据我们所知，这是对这两个相互关联的任务提供的首个理论保证。",
    "tldr": "该论文提出了一个名为HODL的新框架，用于同时研究优化导出模型构建和相应学习过程的内在行为，并证明了这两个子任务的联合收敛性。这是对这两个任务提供的首个理论保证。"
}