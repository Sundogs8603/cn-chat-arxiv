{
    "title": "Video Probabilistic Diffusion Models in Projected Latent Space. (arXiv:2302.07685v2 [cs.CV] UPDATED)",
    "abstract": "Despite the remarkable progress in deep generative models, synthesizing high-resolution and temporally coherent videos still remains a challenge due to their high-dimensionality and complex temporal dynamics along with large spatial variations. Recent works on diffusion models have shown their potential to solve this challenge, yet they suffer from severe computation- and memory-inefficiency that limit the scalability. To handle this issue, we propose a novel generative model for videos, coined projected latent video diffusion models (PVDM), a probabilistic diffusion model which learns a video distribution in a low-dimensional latent space and thus can be efficiently trained with high-resolution videos under limited resources. Specifically, PVDM is composed of two components: (a) an autoencoder that projects a given video as 2D-shaped latent vectors that factorize the complex cubic structure of video pixels and (b) a diffusion model architecture specialized for our new factorized laten",
    "link": "http://arxiv.org/abs/2302.07685",
    "context": "Title: Video Probabilistic Diffusion Models in Projected Latent Space. (arXiv:2302.07685v2 [cs.CV] UPDATED)\nAbstract: Despite the remarkable progress in deep generative models, synthesizing high-resolution and temporally coherent videos still remains a challenge due to their high-dimensionality and complex temporal dynamics along with large spatial variations. Recent works on diffusion models have shown their potential to solve this challenge, yet they suffer from severe computation- and memory-inefficiency that limit the scalability. To handle this issue, we propose a novel generative model for videos, coined projected latent video diffusion models (PVDM), a probabilistic diffusion model which learns a video distribution in a low-dimensional latent space and thus can be efficiently trained with high-resolution videos under limited resources. Specifically, PVDM is composed of two components: (a) an autoencoder that projects a given video as 2D-shaped latent vectors that factorize the complex cubic structure of video pixels and (b) a diffusion model architecture specialized for our new factorized laten",
    "path": "papers/23/02/2302.07685.json",
    "total_tokens": 773,
    "translated_title": "投影概率扩散模型中的视频生成",
    "translated_abstract": "尽管深层生成模型取得了显著进展，但由于视频的高维性、复杂的时间动态和大的空间变化，合成高分辨率和时间连贯的视频仍然是一个挑战。最近，扩散模型在解决这个问题方面展现了其潜力，但它们的计算和内存效率却受到严重的限制。为了解决这个问题，我们提出了一种新的视频生成模型，称为投影概率扩散模型(PVDM)，它是一种概率扩散模型，它在低维潜在空间中学习视频分布，因此可以在有限的资源下高效地训练高分辨率视频。",
    "tldr": "提出了投影概率扩散模型(PVDM)，在低维潜在空间中学习视频分布，从而可以在有限的资源下高效地训练高分辨率视频。",
    "en_tdlr": "Proposed the projected latent video diffusion model (PVDM), which learns video distribution in low-dimensional latent space, allowing efficient training of high-resolution videos with limited resources."
}