{
    "title": "DNArch: Learning Convolutional Neural Architectures by Backpropagation. (arXiv:2302.05400v2 [cs.LG] UPDATED)",
    "abstract": "We present Differentiable Neural Architectures (DNArch), a method that jointly learns the weights and the architecture of Convolutional Neural Networks (CNNs) by backpropagation. In particular, DNArch allows learning (i) the size of convolutional kernels at each layer, (ii) the number of channels at each layer, (iii) the position and values of downsampling layers, and (iv) the depth of the network. To this end, DNArch views neural architectures as continuous multidimensional entities, and uses learnable differentiable masks along each dimension to control their size. Unlike existing methods, DNArch is not limited to a predefined set of possible neural components, but instead it is able to discover entire CNN architectures across all feasible combinations of kernel sizes, widths, depths and downsampling. Empirically, DNArch finds performant CNN architectures for several classification and dense prediction tasks on sequential and image data. When combined with a loss term that controls t",
    "link": "http://arxiv.org/abs/2302.05400",
    "context": "Title: DNArch: Learning Convolutional Neural Architectures by Backpropagation. (arXiv:2302.05400v2 [cs.LG] UPDATED)\nAbstract: We present Differentiable Neural Architectures (DNArch), a method that jointly learns the weights and the architecture of Convolutional Neural Networks (CNNs) by backpropagation. In particular, DNArch allows learning (i) the size of convolutional kernels at each layer, (ii) the number of channels at each layer, (iii) the position and values of downsampling layers, and (iv) the depth of the network. To this end, DNArch views neural architectures as continuous multidimensional entities, and uses learnable differentiable masks along each dimension to control their size. Unlike existing methods, DNArch is not limited to a predefined set of possible neural components, but instead it is able to discover entire CNN architectures across all feasible combinations of kernel sizes, widths, depths and downsampling. Empirically, DNArch finds performant CNN architectures for several classification and dense prediction tasks on sequential and image data. When combined with a loss term that controls t",
    "path": "papers/23/02/2302.05400.json",
    "total_tokens": 945,
    "translated_title": "DNArch: 通过反向传播学习卷积神经网络的可学习架构",
    "translated_abstract": "我们提出了Differentiable Neural Architectures (DNArch)，一种通过反向传播同时学习卷积神经网络(CNNs)的权重和架构的方法。具体而言，DNArch允许学习(i)每一层的卷积核大小，(ii)每一层的通道数，(iii)下采样层的位置和值，以及(iv)网络的深度。为此，DNArch将神经架构视为连续的多维实体，并使用可学习的可微掩码来控制其大小。与现有方法不同，DNArch不限于预定义的可能神经组件集，而是能够发现在所有可行的核大小、宽度、深度和下采样组合中的整个CNN架构。实验证明，DNArch能够为顺序和图像数据的多个分类和密集预测任务找到有效的CNN架构。当与控制架构大小的损失项相结合时，DNArch还能在运行时间和模型性能之间实现有效的权衡。",
    "tldr": "DNArch是一种通过反向传播同时学习卷积神经网络的权重和架构的方法，它不仅可以学习每一层的卷积核大小和通道数，还可以学习网络的深度和下采样层的位置和值。与现有方法不同的是，DNArch不限于预定义的神经组件，能够发现各种核大小、宽度、深度和下采样组合中的整个CNN架构。在实验中，DNArch在多个分类和密集预测任务上找到了高性能的CNN架构。"
}