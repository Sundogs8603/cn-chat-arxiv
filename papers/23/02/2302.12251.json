{
    "title": "VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion. (arXiv:2302.12251v2 [cs.CV] UPDATED)",
    "abstract": "Humans can easily imagine the complete 3D geometry of occluded objects and scenes. This appealing ability is vital for recognition and understanding. To enable such capability in AI systems, we propose VoxFormer, a Transformer-based semantic scene completion framework that can output complete 3D volumetric semantics from only 2D images. Our framework adopts a two-stage design where we start from a sparse set of visible and occupied voxel queries from depth estimation, followed by a densification stage that generates dense 3D voxels from the sparse ones. A key idea of this design is that the visual features on 2D images correspond only to the visible scene structures rather than the occluded or empty spaces. Therefore, starting with the featurization and prediction of the visible structures is more reliable. Once we obtain the set of sparse queries, we apply a masked autoencoder design to propagate the information to all the voxels by self-attention. Experiments on SemanticKITTI show th",
    "link": "http://arxiv.org/abs/2302.12251",
    "context": "Title: VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion. (arXiv:2302.12251v2 [cs.CV] UPDATED)\nAbstract: Humans can easily imagine the complete 3D geometry of occluded objects and scenes. This appealing ability is vital for recognition and understanding. To enable such capability in AI systems, we propose VoxFormer, a Transformer-based semantic scene completion framework that can output complete 3D volumetric semantics from only 2D images. Our framework adopts a two-stage design where we start from a sparse set of visible and occupied voxel queries from depth estimation, followed by a densification stage that generates dense 3D voxels from the sparse ones. A key idea of this design is that the visual features on 2D images correspond only to the visible scene structures rather than the occluded or empty spaces. Therefore, starting with the featurization and prediction of the visible structures is more reliable. Once we obtain the set of sparse queries, we apply a masked autoencoder design to propagate the information to all the voxels by self-attention. Experiments on SemanticKITTI show th",
    "path": "papers/23/02/2302.12251.json",
    "total_tokens": 937,
    "translated_title": "VoxFormer： 基于稀疏体素变换的基于相机的三维语义场景补全",
    "translated_abstract": "人类很容易想象被遮挡物体和场景的完整三维几何形状，在识别和理解方面至关重要。为了使AI系统具备这种能力，我们提出了VoxFormer，这是一个基于变换器的语义场景补全框架，可以仅从2D图像输出完整的三维体积语义。我们的框架采用两阶段设计，从深度估计的稀疏可见和占用体素查询开始，随后进行生成稠密3D体素的稠密化阶段。这个设计的一个关键思想是，2D图像上的视觉特征仅对应于可见场景结构而不是遮挡或空间。因此，从可见结构的特征化和预测开始更加可靠。一旦我们获得了一组稀疏查询，我们就应用掩码自编码器设计通过自我注意将信息传播到所有体素。在SemanticKITTI上的实验表明了我们的方法的有效性。",
    "tldr": "VoxFormer是一个基于变换器的语义场景补全框架，可以仅从2D图像输出完整的三维体积语义。它采用两阶段设计，从可见的体素查询开始，并通过自我注意来传播信息，实现了有效的三维场景补全。",
    "en_tdlr": "VoxFormer is a Transformer-based semantic scene completion framework that can output complete 3D volumetric semantics from only 2D images. It adopts a two-stage design, starting from visible voxel queries and propagating information through self-attention to achieve effective 3D scene completion."
}