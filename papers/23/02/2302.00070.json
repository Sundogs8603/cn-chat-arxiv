{
    "title": "Debiasing Vision-Language Models via Biased Prompts. (arXiv:2302.00070v2 [cs.LG] UPDATED)",
    "abstract": "Machine learning models have been shown to inherit biases from their training datasets. This can be particularly problematic for vision-language foundation models trained on uncurated datasets scraped from the internet. The biases can be amplified and propagated to downstream applications like zero-shot classifiers and text-to-image generative models. In this study, we propose a general approach for debiasing vision-language foundation models by projecting out biased directions in the text embedding. In particular, we show that debiasing only the text embedding with a calibrated projection matrix suffices to yield robust classifiers and fair generative models. The proposed closed-form solution enables easy integration into large-scale pipelines, and empirical results demonstrate that our approach effectively reduces social bias and spurious correlation in both discriminative and generative vision-language models without the need for additional data or training.",
    "link": "http://arxiv.org/abs/2302.00070",
    "context": "Title: Debiasing Vision-Language Models via Biased Prompts. (arXiv:2302.00070v2 [cs.LG] UPDATED)\nAbstract: Machine learning models have been shown to inherit biases from their training datasets. This can be particularly problematic for vision-language foundation models trained on uncurated datasets scraped from the internet. The biases can be amplified and propagated to downstream applications like zero-shot classifiers and text-to-image generative models. In this study, we propose a general approach for debiasing vision-language foundation models by projecting out biased directions in the text embedding. In particular, we show that debiasing only the text embedding with a calibrated projection matrix suffices to yield robust classifiers and fair generative models. The proposed closed-form solution enables easy integration into large-scale pipelines, and empirical results demonstrate that our approach effectively reduces social bias and spurious correlation in both discriminative and generative vision-language models without the need for additional data or training.",
    "path": "papers/23/02/2302.00070.json",
    "total_tokens": 934,
    "translated_title": "通过偏向校准解决视觉语言模型中的偏差",
    "translated_abstract": "研究表明，机器学习模型会从它们的训练数据集中继承偏见。对于从互联网上爬取的未加筛选的数据集训练的视觉语言基础模型，这可能会特别具有问题。偏见可能会被放大并传播到其他应用程序，如零样本分类器和文本到图像生成模型中。在本研究中，我们提出了一种通用方法，通过在文本嵌入中投影出偏向方向来校准视觉语言基础模型的偏差。特别地，我们展示了仅使用经过校准的投影矩阵对文本嵌入进行去偏执就足以生成严谨的分类器和公正的生成模型。所提出的闭合形式解决方案使得易于在大规模管道中进行集成，实证结果表明，我们的方法有效地减少了区分性和生成性视觉语言模型的社交偏见和虚假相关，而不需要额外的数据或培训。",
    "tldr": "本研究提出了一种通用方法，通过在文本嵌入中投影出偏向方向来校准视觉语言基础模型的偏差，仅进行文本嵌入去偏执就足以生成严谨的分类器和公正的生成模型，有效减少了社交偏见和虚假相关，无需额外数据或培训。",
    "en_tdlr": "This study proposes a general approach for debiasing vision-language foundation models by projecting out biased directions in the text embedding, which can effectively reduce social bias and spurious correlation in both discriminative and generative vision-language models without the need for additional data or training."
}