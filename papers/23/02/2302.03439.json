{
    "title": "Ensemble Value Functions for Efficient Exploration in Multi-Agent Reinforcement Learning. (arXiv:2302.03439v5 [cs.MA] UPDATED)",
    "abstract": "Cooperative multi-agent reinforcement learning (MARL) requires agents to explore to learn to cooperate. Existing value-based MARL algorithms commonly rely on random exploration, such as $\\epsilon$-greedy, which is inefficient in discovering multi-agent cooperation. Additionally, the environment in MARL appears non-stationary to any individual agent due to the simultaneous training of other agents, leading to highly variant and thus unstable optimisation signals. In this work, we propose ensemble value functions for multi-agent exploration (EMAX), a general framework to extend any value-based MARL algorithm. EMAX trains ensembles of value functions for each agent to address the key challenges of exploration and non-stationarity: (1) The uncertainty of value estimates across the ensemble is used in a UCB policy to guide the exploration of agents to parts of the environment which require cooperation. (2) Average value estimates across the ensemble serve as target values. These targets exh",
    "link": "http://arxiv.org/abs/2302.03439",
    "context": "Title: Ensemble Value Functions for Efficient Exploration in Multi-Agent Reinforcement Learning. (arXiv:2302.03439v5 [cs.MA] UPDATED)\nAbstract: Cooperative multi-agent reinforcement learning (MARL) requires agents to explore to learn to cooperate. Existing value-based MARL algorithms commonly rely on random exploration, such as $\\epsilon$-greedy, which is inefficient in discovering multi-agent cooperation. Additionally, the environment in MARL appears non-stationary to any individual agent due to the simultaneous training of other agents, leading to highly variant and thus unstable optimisation signals. In this work, we propose ensemble value functions for multi-agent exploration (EMAX), a general framework to extend any value-based MARL algorithm. EMAX trains ensembles of value functions for each agent to address the key challenges of exploration and non-stationarity: (1) The uncertainty of value estimates across the ensemble is used in a UCB policy to guide the exploration of agents to parts of the environment which require cooperation. (2) Average value estimates across the ensemble serve as target values. These targets exh",
    "path": "papers/23/02/2302.03439.json",
    "total_tokens": 1140,
    "translated_title": "多智能体强化学习中的集成价值函数用于高效探索",
    "translated_abstract": "合作多智能体强化学习需要智能体进行探索以学习合作的方式。现有的基于价值的多智能体强化学习算法通常依赖于随机探索，如ε-贪心算法，这在发现多智能体合作方面效率低下。此外，由于其他智能体的同时训练，MARL中的环境对于任一单个智能体似乎是非静止的，导致高度不稳定的优化信号。在本文中，我们提出了面向多智能体探索的集成价值函数（EMAX），这是一个通用的框架，可扩展任何基于价值的多智能体强化学习算法。 EMAX为每个智能体训练价值函数集合，以解决探索和非静止性的关键挑战：（1）利用集合中价值估计的不确定性来引导智能体探索需要合作的环境部分的UCB策略。 （2）集合中平均值估计作为目标值。这些目标提供了低方差和稳定的更新，从而增强了训练效率。我们在两个合作任务上展示了EMAX的有效性，并表明在学习速度，最终性能和对非静止性的稳健性方面显着优于现有的探索策略。",
    "tldr": "本论文提出了集成价值函数框架EMAX，用于多智能体强化学习中的探索，使用UCB策略引导探索，并通过训练价值函数集合减少方差和不稳定性，展示了比其他策略更高的学习速度、最终性能和对非静止性的稳健性。",
    "en_tdlr": "This paper proposes a general framework, Ensemble Value Functions for multi-agent exploration (EMAX), to address the challenges of exploration and non-stationarity in cooperative multi-agent reinforcement learning, which uses UCB policy to guide exploration and reduces variance and instability by training value function ensembles, demonstrating higher learning speed, final performance, and robustness against non-stationarity compared to existing strategies."
}