{
    "title": "Cross-domain Compositing with Pretrained Diffusion Models. (arXiv:2302.10167v2 [cs.CV] UPDATED)",
    "abstract": "Diffusion models have enabled high-quality, conditional image editing capabilities. We propose to expand their arsenal, and demonstrate that off-the-shelf diffusion models can be used for a wide range of cross-domain compositing tasks. Among numerous others, these include image blending, object immersion, texture-replacement and even CG2Real translation or stylization. We employ a localized, iterative refinement scheme which infuses the injected objects with contextual information derived from the background scene, and enables control over the degree and types of changes the object may undergo. We conduct a range of qualitative and quantitative comparisons to prior work, and exhibit that our method produces higher quality and realistic results without requiring any annotations or training. Finally, we demonstrate how our method may be used for data augmentation of downstream tasks.",
    "link": "http://arxiv.org/abs/2302.10167",
    "context": "Title: Cross-domain Compositing with Pretrained Diffusion Models. (arXiv:2302.10167v2 [cs.CV] UPDATED)\nAbstract: Diffusion models have enabled high-quality, conditional image editing capabilities. We propose to expand their arsenal, and demonstrate that off-the-shelf diffusion models can be used for a wide range of cross-domain compositing tasks. Among numerous others, these include image blending, object immersion, texture-replacement and even CG2Real translation or stylization. We employ a localized, iterative refinement scheme which infuses the injected objects with contextual information derived from the background scene, and enables control over the degree and types of changes the object may undergo. We conduct a range of qualitative and quantitative comparisons to prior work, and exhibit that our method produces higher quality and realistic results without requiring any annotations or training. Finally, we demonstrate how our method may be used for data augmentation of downstream tasks.",
    "path": "papers/23/02/2302.10167.json",
    "total_tokens": 846,
    "translated_title": "利用预训练扩散模型的跨领域合成",
    "translated_abstract": "扩散模型已经实现了高质量的条件图像编辑功能。我们提出扩展它们的应用范围，并展示了现成的扩散模型可用于各种跨领域合成任务，包括图像融合，物体注入，纹理替换，甚至包括CG2Real的翻译或风格化。我们采用局部迭代细化方案，将注入的对象与背景场景的上下文信息相结合，并能够控制对象可能经历的程度和类型的变化。我们进行了一系列定性和定量比较，证明我们的方法产生的结果更高质量、更逼真，而不需要任何标注或训练。最后，我们展示了如何使用我们的方法来增强下游任务的数据。",
    "tldr": "本文介绍了一种跨领域合成的方法，利用预训练扩散模型进行局部迭代细化，可以实现高质量、逼真的图像编辑，包括图像融合、物体注入、纹理替换等多种任务，而不需要任何标注或训练，还可以用于数据增强。",
    "en_tdlr": "This paper proposes a method for cross-domain compositing using pretrained diffusion models with localized, iterative refinement to achieve high-quality and realistic image editing tasks, including blending, object immersion, texture-replacement, and more, without requiring annotations or training. The method can also be used for data augmentation of downstream tasks."
}