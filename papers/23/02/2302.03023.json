{
    "title": "V1T: large-scale mouse V1 response prediction using a Vision Transformer. (arXiv:2302.03023v3 [cs.CV] UPDATED)",
    "abstract": "Accurate predictive models of the visual cortex neural response to natural visual stimuli remain a challenge in computational neuroscience. In this work, we introduce V1T, a novel Vision Transformer based architecture that learns a shared visual and behavioral representation across animals. We evaluate our model on two large datasets recorded from mouse primary visual cortex and outperform previous convolution-based models by more than 12.7% in prediction performance. Moreover, we show that the self-attention weights learned by the Transformer correlate with the population receptive fields. Our model thus sets a new benchmark for neural response prediction and can be used jointly with behavioral and neural recordings to reveal meaningful characteristic features of the visual cortex.",
    "link": "http://arxiv.org/abs/2302.03023",
    "context": "Title: V1T: large-scale mouse V1 response prediction using a Vision Transformer. (arXiv:2302.03023v3 [cs.CV] UPDATED)\nAbstract: Accurate predictive models of the visual cortex neural response to natural visual stimuli remain a challenge in computational neuroscience. In this work, we introduce V1T, a novel Vision Transformer based architecture that learns a shared visual and behavioral representation across animals. We evaluate our model on two large datasets recorded from mouse primary visual cortex and outperform previous convolution-based models by more than 12.7% in prediction performance. Moreover, we show that the self-attention weights learned by the Transformer correlate with the population receptive fields. Our model thus sets a new benchmark for neural response prediction and can be used jointly with behavioral and neural recordings to reveal meaningful characteristic features of the visual cortex.",
    "path": "papers/23/02/2302.03023.json",
    "total_tokens": 865,
    "translated_title": "V1T：使用Vision Transformer进行大规模小鼠V1响应预测",
    "translated_abstract": "在计算神经科学中，对自然视觉刺激下的视觉皮层神经响应的精确预测模型仍然是一个挑战。本文介绍了V1T，一种基于Vision Transformer的新型架构，可以跨动物学习共享的视觉和行为表示。我们对记录于小鼠原始视觉皮层的两个大型数据集进行评估，并在预测性能上优于之前基于卷积的模型超过12.7％。此外，我们展示了Transformer学习的自我关注权重与群体感受野相关。因此，我们的模型为神经响应预测设立了新的基准，并可与行为和神经记录一起使用，以揭示视觉皮层的有意义的特征。",
    "tldr": "V1T是一种基于Vision Transformer的新型架构，可以跨动物学习共享的视觉和行为表示，对自然视觉刺激下的视觉皮层神经响应进行预测，并在预测性能上优于之前基于卷积的模型超过12.7％。同时，通过Transformer学习的自我关注权重还能够展示与群体感受野的相关性。",
    "en_tdlr": "V1T is a new architecture based on Vision Transformer that learns a shared visual and behavioral representation across animals, accurately predicts neural response of primary visual cortex to natural visual stimuli and outperforms previous convolution-based models by more than 12.7% in prediction performance. Additionally, the self-attention weights learned by Transformer correlate with the population receptive fields."
}