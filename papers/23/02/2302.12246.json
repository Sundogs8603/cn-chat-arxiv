{
    "title": "Active Prompting with Chain-of-Thought for Large Language Models. (arXiv:2302.12246v3 [cs.CL] UPDATED)",
    "abstract": "The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. It is known that the effective design of task-specific prompts is critical for LLMs' ability to produce high-quality answers. In particular, an effective approach for complex question-and-answer tasks is example-based prompting with chain-of-thought (CoT) reasoning, which significantly improves the performance of LLMs. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks. This paper proposes a new method, Active-Prompt, to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose, we propose a solution to the key problem of determining which questions are the most important and helpful ones to annotate from a pool of task-specific queries. By borrowi",
    "link": "http://arxiv.org/abs/2302.12246",
    "context": "Title: Active Prompting with Chain-of-Thought for Large Language Models. (arXiv:2302.12246v3 [cs.CL] UPDATED)\nAbstract: The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. It is known that the effective design of task-specific prompts is critical for LLMs' ability to produce high-quality answers. In particular, an effective approach for complex question-and-answer tasks is example-based prompting with chain-of-thought (CoT) reasoning, which significantly improves the performance of LLMs. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks. This paper proposes a new method, Active-Prompt, to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose, we propose a solution to the key problem of determining which questions are the most important and helpful ones to annotate from a pool of task-specific queries. By borrowi",
    "path": "papers/23/02/2302.12246.json",
    "total_tokens": 870,
    "translated_title": "大型语言模型的思维链主动提示",
    "translated_abstract": "大型语言模型的规模日益增大，为各种需要推理的复杂任务（如算术和常识推理）带来了新的能力。众所周知，任务特定提示的有效设计对LLMs产生高质量答案的能力至关重要。特别是，对于复杂的问答任务，一种有效的方法是基于示例的思维链（CoT）推导提示，它大大提高了LLMs的性能。但是，当前的CoT方法依赖于一组固定的人类注释示例，这些示例不一定是不同任务的最有效示例。本文提出了一种新方法，Active-Prompt，使用任务特定的示例提示（人为设计的CoT推理注释）来适应LLMs不同的任务。为此，我们提出了一个解决方案，确定哪些问题从任务特定查询池中注释最重要和有用。通过借鉴主动学习的方法，我们提出了一个主动提示(Acitve-Prompt)的方法，将最相关的问题作为任务特定提示添加给LLMs，从而改善其性能。",
    "tldr": "本论文提出了一种新的方法Active-Prompt，它使用任务特定的示例提示适应大型语言模型中的不同任务，提高模型性能与效率。",
    "en_tdlr": "This paper proposes a new method, Active-Prompt, which adapts large language models to different tasks with task-specific example prompts, improving the model performance and efficiency."
}