{
    "title": "The In-Sample Softmax for Offline Reinforcement Learning. (arXiv:2302.14372v2 [cs.LG] UPDATED)",
    "abstract": "Reinforcement learning (RL) agents can leverage batches of previously collected data to extract a reasonable control policy. An emerging issue in this offline RL setting, however, is that the bootstrapping update underlying many of our methods suffers from insufficient action-coverage: standard max operator may select a maximal action that has not been seen in the dataset. Bootstrapping from these inaccurate values can lead to overestimation and even divergence. There are a growing number of methods that attempt to approximate an \\emph{in-sample} max, that only uses actions well-covered by the dataset. We highlight a simple fact: it is more straightforward to approximate an in-sample \\emph{softmax} using only actions in the dataset. We show that policy iteration based on the in-sample softmax converges, and that for decreasing temperatures it approaches the in-sample max. We derive an In-Sample Actor-Critic (AC), using this in-sample softmax, and show that it is consistently better or ",
    "link": "http://arxiv.org/abs/2302.14372",
    "context": "Title: The In-Sample Softmax for Offline Reinforcement Learning. (arXiv:2302.14372v2 [cs.LG] UPDATED)\nAbstract: Reinforcement learning (RL) agents can leverage batches of previously collected data to extract a reasonable control policy. An emerging issue in this offline RL setting, however, is that the bootstrapping update underlying many of our methods suffers from insufficient action-coverage: standard max operator may select a maximal action that has not been seen in the dataset. Bootstrapping from these inaccurate values can lead to overestimation and even divergence. There are a growing number of methods that attempt to approximate an \\emph{in-sample} max, that only uses actions well-covered by the dataset. We highlight a simple fact: it is more straightforward to approximate an in-sample \\emph{softmax} using only actions in the dataset. We show that policy iteration based on the in-sample softmax converges, and that for decreasing temperatures it approaches the in-sample max. We derive an In-Sample Actor-Critic (AC), using this in-sample softmax, and show that it is consistently better or ",
    "path": "papers/23/02/2302.14372.json",
    "total_tokens": 900,
    "translated_title": "离线强化学习中的In-Sample Softmax",
    "translated_abstract": "强化学习RL代理可以利用以前收集的数据的批次来提取合理的控制策略。然而，在这种离线RL设置中，一个不断出现的问题是，许多方法下的bootstrapping更新受到行动覆盖不足的影响：标准max运算符可能会选择在数据集中没有出现过的最大动作。从这些不准确的值进行bootstrapping更新会导致高估甚至发散。有越来越多的方法尝试近似一个仅使用数据集中涵盖良好的操作的in-sample max。本文强调一个简单的事实：使用仅由数据集中的动作近似In-Sample softmax更加直观。我们展示了在In-Sample softmax基础上的策略迭代的收敛性，并且对于温度的下降，它会接近In-Sample max。我们使用这种In-Sample softmax推导出一个In-Sample Actor-Critic（AC），并且证明其在稳定性或性能上更好。",
    "tldr": "本文研究离线强化学习中的In-Sample Softmax，通过使用只由数据集中的操作组成的In-Sample softmax来解决操作覆盖不足问题，并且In-Sample Actor-Critic与该方法相比在稳定性或性能上表现更好。",
    "en_tdlr": "This paper proposes In-Sample Softmax to solve the problem of insufficient action coverage in offline reinforcement learning, and shows that In-Sample Actor-Critic based on this method outperforms other methods in stability and performance."
}