{
    "title": "Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery. (arXiv:2302.03668v2 [cs.LG] UPDATED)",
    "abstract": "The strength of modern generative models lies in their ability to be controlled through text-based prompts. Typical \"hard\" prompts are made from interpretable words and tokens, and must be hand-crafted by humans. There are also \"soft\" prompts, which consist of continuous feature vectors. These can be discovered using powerful optimization methods, but they cannot be easily interpreted, re-used across models, or plugged into a text-based interface.  We describe an approach to robustly optimize hard text prompts through efficient gradient-based optimization. Our approach automatically generates hard text-based prompts for both text-to-image and text-to-text applications. In the text-to-image setting, the method creates hard prompts for diffusion models, allowing API users to easily generate, discover, and mix and match image concepts without prior knowledge on how to prompt the model. In the text-to-text setting, we show that hard prompts can be automatically discovered that are effectiv",
    "link": "http://arxiv.org/abs/2302.03668",
    "context": "Title: Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery. (arXiv:2302.03668v2 [cs.LG] UPDATED)\nAbstract: The strength of modern generative models lies in their ability to be controlled through text-based prompts. Typical \"hard\" prompts are made from interpretable words and tokens, and must be hand-crafted by humans. There are also \"soft\" prompts, which consist of continuous feature vectors. These can be discovered using powerful optimization methods, but they cannot be easily interpreted, re-used across models, or plugged into a text-based interface.  We describe an approach to robustly optimize hard text prompts through efficient gradient-based optimization. Our approach automatically generates hard text-based prompts for both text-to-image and text-to-text applications. In the text-to-image setting, the method creates hard prompts for diffusion models, allowing API users to easily generate, discover, and mix and match image concepts without prior knowledge on how to prompt the model. In the text-to-text setting, we show that hard prompts can be automatically discovered that are effectiv",
    "path": "papers/23/02/2302.03668.json",
    "total_tokens": 953,
    "translated_title": "硬提示变简单：用基于梯度的离散优化方法进行提示调节和发现",
    "translated_abstract": "现代生成模型的优点在于可以通过基于文本的提示进行控制。传统的“硬”提示是由可解释的词汇和标记构成，必须由人手工制作。此外还有“软”提示，它们由连续的特征向量组成，可以通过强大的优化方法发现，但它们不能很容易地解释，不能在不同模型之间重复使用，也不能用于基于文本的接口。我们提出了一种基于高效梯度优化的方法，来稳健地优化硬文本提示。该方法自动为文本到图像和文本到文本应用生成硬文本提示。在文本到图像的设置中，该方法为扩散模型创建硬提示，使API用户可以轻松生成、发现、混合和匹配图像概念，而不需要事先了解如何提示模型。在文本到文本的设置中，我们展示了自动发现硬提示可以有效地优化模型性能。",
    "tldr": "该论文提出了一种基于梯度的离散优化方法，用于自动生成文本提示，进而控制现代生成模型的输出。该方法可以优化文本到图像和文本到文本的应用，为API用户提供了轻松生成、发现、混合和匹配图像概念的能力，同时自动发现硬提示可以有效地提高模型性能。",
    "en_tdlr": "This paper proposes a gradient-based discrete optimization method for automatically generating hard text prompts to control the output of modern generative models. The method can optimize text-to-image and text-to-text applications, providing API users with the ability to easily generate, discover, mix and match image concepts, while automatically discovering hard prompts can effectively improve model performance."
}