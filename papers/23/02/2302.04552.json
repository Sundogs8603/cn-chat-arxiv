{
    "title": "Optimistic Online Mirror Descent for Bridging Stochastic and Adversarial Online Convex Optimization. (arXiv:2302.04552v2 [cs.LG] UPDATED)",
    "abstract": "Stochastically Extended Adversarial (SEA) model is introduced by Sachs et al. [2022] as an interpolation between stochastic and adversarial online convex optimization. Under the smoothness condition, they demonstrate that the expected regret of optimistic follow-the-regularized-leader (FTRL) depends on the cumulative stochastic variance $\\sigma_{1:T}^2$ and the cumulative adversarial variation $\\Sigma_{1:T}^2$ for convex functions. They also provide a slightly weaker bound based on the maximal stochastic variance $\\sigma_{\\max}^2$ and the maximal adversarial variation $\\Sigma_{\\max}^2$ for strongly convex functions. Inspired by their work, we investigate the theoretical guarantees of optimistic online mirror descent (OMD) for the SEA model. For convex and smooth functions, we obtain the same $\\mathcal{O}(\\sqrt{\\sigma_{1:T}^2}+\\sqrt{\\Sigma_{1:T}^2})$ regret bound, without the convexity requirement of individual functions. For strongly convex and smooth functions, we establish an $\\mathc",
    "link": "http://arxiv.org/abs/2302.04552",
    "context": "Title: Optimistic Online Mirror Descent for Bridging Stochastic and Adversarial Online Convex Optimization. (arXiv:2302.04552v2 [cs.LG] UPDATED)\nAbstract: Stochastically Extended Adversarial (SEA) model is introduced by Sachs et al. [2022] as an interpolation between stochastic and adversarial online convex optimization. Under the smoothness condition, they demonstrate that the expected regret of optimistic follow-the-regularized-leader (FTRL) depends on the cumulative stochastic variance $\\sigma_{1:T}^2$ and the cumulative adversarial variation $\\Sigma_{1:T}^2$ for convex functions. They also provide a slightly weaker bound based on the maximal stochastic variance $\\sigma_{\\max}^2$ and the maximal adversarial variation $\\Sigma_{\\max}^2$ for strongly convex functions. Inspired by their work, we investigate the theoretical guarantees of optimistic online mirror descent (OMD) for the SEA model. For convex and smooth functions, we obtain the same $\\mathcal{O}(\\sqrt{\\sigma_{1:T}^2}+\\sqrt{\\Sigma_{1:T}^2})$ regret bound, without the convexity requirement of individual functions. For strongly convex and smooth functions, we establish an $\\mathc",
    "path": "papers/23/02/2302.04552.json",
    "total_tokens": 1055,
    "translated_title": "乐观的在线镜像下降算法用于连接随机性和对抗性在线凸优化",
    "translated_abstract": "Sachs等人介绍了Stochastically Extended Adversarial (SEA)模型，作为随机性和对抗性在线凸优化的插值方法。在光滑条件下，他们证明了乐观的Follow-the-Regularized-Leader (FTRL)算法的期望遗憾依赖于凸函数的累积随机方差和累积对抗变化。对于强凸函数，他们也给出了基于最大随机方差和最大对抗变化的稍弱界限。受到他们的工作的启发，我们研究了乐观的在线镜像下降算法在SEA模型中的理论保证。对于凸且平滑的函数，我们得到了相同的遗憾界限，即O(sqrt(σ_{1:T}^2) + sqrt(Σ_{1:T}^2))，而不需要个别函数的凸性要求。对于强凸且平滑的函数，我们建立了一个O(sqrt(σ_{\\max}^2) + sqrt(Σ_{\\max}^2))的界限。",
    "tldr": "本论文研究了乐观的在线镜像下降算法在Stochastically Extended Adversarial (SEA)模型中的理论保证，对于凸和平滑的函数，其遗憾界限为O(sqrt(σ_{1:T}^2) + sqrt(Σ_{1:T}^2))，对于强凸和平滑的函数，其界限为O(sqrt(σ_{\\max}^2) + sqrt(Σ_{\\max}^2))。"
}