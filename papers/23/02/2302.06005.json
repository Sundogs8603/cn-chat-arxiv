{
    "title": "Near-optimal learning with average H\\\"older smoothness. (arXiv:2302.06005v2 [cs.LG] UPDATED)",
    "abstract": "We generalize the notion of average Lipschitz smoothness proposed by Ashlagi et al. (COLT 2021) by extending it to H\\\"older smoothness. This measure of the \"effective smoothness\" of a function is sensitive to the underlying distribution and can be dramatically smaller than its classic \"worst-case H\\\"older constant. We consider both the realizable and the agnostic (noisy) regression settings, proving upper and lower risk bounds in terms of the average H\\\"older smoothness; these rates improve upon both previously known rates even in the special case of average Lipschitz smoothness. Moreover, our lower bound is tight in the realizable setting up to log factors, thus we establish the minimax rate. From an algorithmic perspective, since our notion of average smoothness is defined with respect to the unknown underlying distribution, the learner does not have an explicit representation of the function class, hence is unable to execute ERM. Nevertheless, we provide distinct learning algorithms",
    "link": "http://arxiv.org/abs/2302.06005",
    "context": "Title: Near-optimal learning with average H\\\"older smoothness. (arXiv:2302.06005v2 [cs.LG] UPDATED)\nAbstract: We generalize the notion of average Lipschitz smoothness proposed by Ashlagi et al. (COLT 2021) by extending it to H\\\"older smoothness. This measure of the \"effective smoothness\" of a function is sensitive to the underlying distribution and can be dramatically smaller than its classic \"worst-case H\\\"older constant. We consider both the realizable and the agnostic (noisy) regression settings, proving upper and lower risk bounds in terms of the average H\\\"older smoothness; these rates improve upon both previously known rates even in the special case of average Lipschitz smoothness. Moreover, our lower bound is tight in the realizable setting up to log factors, thus we establish the minimax rate. From an algorithmic perspective, since our notion of average smoothness is defined with respect to the unknown underlying distribution, the learner does not have an explicit representation of the function class, hence is unable to execute ERM. Nevertheless, we provide distinct learning algorithms",
    "path": "papers/23/02/2302.06005.json",
    "total_tokens": 870,
    "translated_title": "平均Hölder平滑度下的近似最优学习",
    "translated_abstract": "我们将Ashlagi等人（COLT 2021）提出的平均Lipschitz平滑性概念推广到Hölder平滑性，并证明了关于平均Hölder平滑性的上下风险界，这些界的速率甚至在平均Lipschitz平滑性的特殊情况下也优于之前已知界。此外，我们的下界在可实现情况下是最优的，最多差一个对数因子，从而建立了极小值率。从算法的角度来看，由于我们对平均平滑度的定义是针对未知的基础分布的，因此学习者没有函数类的显式表示，无法执行ERM。尽管如此，我们提供了独立的学习算法。",
    "tldr": "通过推广平均Lipschitz平滑性到Hölder平滑性，得到了关于平均Hölder平滑性的上下风险界，最优的下界对数因子最多差一个，提供了独立的学习算法。",
    "en_tdlr": "By generalizing the notion of average Lipschitz smoothness to H\\\"older smoothness, this paper provides upper and lower risk bounds in terms of the average H\\\"older smoothness, which are even better than previously known rates in the special case of average Lipschitz smoothness. The lower bound is tight in the realizable setting up to log factors, and distinct learning algorithms are provided."
}