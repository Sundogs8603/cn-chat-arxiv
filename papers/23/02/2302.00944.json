{
    "title": "TransFool: An Adversarial Attack against Neural Machine Translation Models. (arXiv:2302.00944v2 [cs.CL] UPDATED)",
    "abstract": "Deep neural networks have been shown to be vulnerable to small perturbations of their inputs, known as adversarial attacks. In this paper, we investigate the vulnerability of Neural Machine Translation (NMT) models to adversarial attacks and propose a new attack algorithm called TransFool. To fool NMT models, TransFool builds on a multi-term optimization problem and a gradient projection step. By integrating the embedding representation of a language model, we generate fluent adversarial examples in the source language that maintain a high level of semantic similarity with the clean samples. Experimental results demonstrate that, for different translation tasks and NMT architectures, our white-box attack can severely degrade the translation quality while the semantic similarity between the original and the adversarial sentences stays high. Moreover, we show that TransFool is transferable to unknown target models. Finally, based on automatic and human evaluations, TransFool leads to imp",
    "link": "http://arxiv.org/abs/2302.00944",
    "context": "Title: TransFool: An Adversarial Attack against Neural Machine Translation Models. (arXiv:2302.00944v2 [cs.CL] UPDATED)\nAbstract: Deep neural networks have been shown to be vulnerable to small perturbations of their inputs, known as adversarial attacks. In this paper, we investigate the vulnerability of Neural Machine Translation (NMT) models to adversarial attacks and propose a new attack algorithm called TransFool. To fool NMT models, TransFool builds on a multi-term optimization problem and a gradient projection step. By integrating the embedding representation of a language model, we generate fluent adversarial examples in the source language that maintain a high level of semantic similarity with the clean samples. Experimental results demonstrate that, for different translation tasks and NMT architectures, our white-box attack can severely degrade the translation quality while the semantic similarity between the original and the adversarial sentences stays high. Moreover, we show that TransFool is transferable to unknown target models. Finally, based on automatic and human evaluations, TransFool leads to imp",
    "path": "papers/23/02/2302.00944.json",
    "total_tokens": 1039,
    "translated_title": "TransFool: 面向神经机器翻译模型的对抗攻击",
    "translated_abstract": "深度神经网络已被证明会对输入的微小扰动产生易受攻击的漏洞。本文研究了神经机器翻译(NMT)模型受到对抗攻击的易感性，并提出了一种新的攻击算法称为TransFool。为了欺骗 NMT 模型，TransFool 建立在一个多项式优化问题和梯度投影步骤之上。通过集成语言模型的嵌入表征，我们在源语言中生成流畅的对抗性样本，其与原始样本有着高度的语义相似性。实验结果表明，对于不同的翻译任务和 NMT 架构，我们的白箱攻击可以严重削弱翻译质量，而原本与对抗性句子之间的语义相似性仍旧很高。此外，我们展示了 TransFool 可以转移至未知的目标模型。最后，在自动化和人为评估的基础上，TransFool 导致了重要的译文质量降低。",
    "tldr": "本文研究神经机器翻译(NMT)的对抗攻击易感性，并提出了一种新的攻击算法 TransFool。通过集成语言模型的嵌入表征，我们生成了与原始样本有着高度语义相似性的准确对抗性样本。实验结果表明，在不同翻译任务和 NMT 架构之下，TransFool 可以转移到未知的目标模型，导致严重降低译文质量。",
    "en_tdlr": "This paper investigates the vulnerability of Neural Machine Translation (NMT) models to adversarial attacks and proposes a new attack algorithm called TransFool. Through integrating the embedding representation of a language model, TransFool can generate fluent adversarial examples in the source language which maintain a high level of semantic similarity with the clean samples. The experimental results show that TransFool can be transferred to unknown target models and significantly reduce the translation quality."
}