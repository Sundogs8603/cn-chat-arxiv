{
    "title": "A low latency attention module for streaming self-supervised speech representation learning",
    "abstract": "arXiv:2302.13451v2 Announce Type: replace-cross  Abstract: The transformer is a fundamental building block in deep learning, and the attention mechanism is the transformer's core component. Self-supervised speech representation learning (SSRL) represents a popular use-case for the transformer architecture. Due to transformers' acausal behavior, the use of transformers for SSRL has been predominantly focused on acausal applications. However, several media processing problems, such as speech processing, require real-time solutions. In this paper, we present an implementation of the attention module that enables training of SSRL architectures with low compute and memory requirements, while allowing real-time inference with low and fixed latency. The attention module proposed in this paper includes two components, streaming attention (SA) and low-latency streaming attention (LLSA). The SA represents our proposal for an efficient streaming SSRL implementation, while the LLSA solves the late",
    "link": "https://arxiv.org/abs/2302.13451",
    "context": "Title: A low latency attention module for streaming self-supervised speech representation learning\nAbstract: arXiv:2302.13451v2 Announce Type: replace-cross  Abstract: The transformer is a fundamental building block in deep learning, and the attention mechanism is the transformer's core component. Self-supervised speech representation learning (SSRL) represents a popular use-case for the transformer architecture. Due to transformers' acausal behavior, the use of transformers for SSRL has been predominantly focused on acausal applications. However, several media processing problems, such as speech processing, require real-time solutions. In this paper, we present an implementation of the attention module that enables training of SSRL architectures with low compute and memory requirements, while allowing real-time inference with low and fixed latency. The attention module proposed in this paper includes two components, streaming attention (SA) and low-latency streaming attention (LLSA). The SA represents our proposal for an efficient streaming SSRL implementation, while the LLSA solves the late",
    "path": "papers/23/02/2302.13451.json",
    "total_tokens": 770,
    "translated_title": "用于流式自监督语音表示学习的低延迟注意力模块",
    "translated_abstract": "transformer是深度学习中的基本构建模块，注意力机制是transformer的核心组件。自监督语音表示学习（SSRL）是transformer架构的一个流行用例。由于transformer的非因果行为，对于SSRL的transformer的使用主要集中在非因果应用上。然而，一些媒体处理问题，如语音处理，需要实时解决方案。在本文中，我们提出了一个注意力模块的实现，该模块可以通过较低的计算和内存需求训练SSRL架构，并允许在低固定延迟下进行实时推断。本文提出的注意力模块包括两个组件，分别是流式注意力（SA）和低延迟流式注意力（LLSA）。",
    "tldr": "本文提出了用于流式自监督语音表示学习的低延迟注意力模块，实现了在低延迟的情况下进行实时推断。",
    "en_tdlr": "This paper introduces a low-latency attention module for streaming self-supervised speech representation learning, enabling real-time inference with low latency."
}