{
    "title": "Feature Affinity Assisted Knowledge Distillation and Quantization of Deep Neural Networks on Label-Free Data. (arXiv:2302.10899v3 [cs.LG] UPDATED)",
    "abstract": "In this paper, we propose a feature affinity (FA) assisted knowledge distillation (KD) method to improve quantization-aware training of deep neural networks (DNN). The FA loss on intermediate feature maps of DNNs plays the role of teaching middle steps of a solution to a student instead of only giving final answers in the conventional KD where the loss acts on the network logits at the output level. Combining logit loss and FA loss, we found that the quantized student network receives stronger supervision than from the labeled ground-truth data. The resulting FAQD is capable of compressing model on label-free data, which brings immediate practical benefits as pre-trained teacher models are readily available and unlabeled data are abundant. In contrast, data labeling is often laborious and expensive. Finally, we propose a fast feature affinity (FFA) loss that accurately approximates FA loss with a lower order of computational complexity, which helps speed up training for high resolution",
    "link": "http://arxiv.org/abs/2302.10899",
    "context": "Title: Feature Affinity Assisted Knowledge Distillation and Quantization of Deep Neural Networks on Label-Free Data. (arXiv:2302.10899v3 [cs.LG] UPDATED)\nAbstract: In this paper, we propose a feature affinity (FA) assisted knowledge distillation (KD) method to improve quantization-aware training of deep neural networks (DNN). The FA loss on intermediate feature maps of DNNs plays the role of teaching middle steps of a solution to a student instead of only giving final answers in the conventional KD where the loss acts on the network logits at the output level. Combining logit loss and FA loss, we found that the quantized student network receives stronger supervision than from the labeled ground-truth data. The resulting FAQD is capable of compressing model on label-free data, which brings immediate practical benefits as pre-trained teacher models are readily available and unlabeled data are abundant. In contrast, data labeling is often laborious and expensive. Finally, we propose a fast feature affinity (FFA) loss that accurately approximates FA loss with a lower order of computational complexity, which helps speed up training for high resolution",
    "path": "papers/23/02/2302.10899.json",
    "total_tokens": 940,
    "translated_title": "特征亲和力辅助的知识蒸馏和深度神经网络无标签数据的量化",
    "translated_abstract": "本文提出了一种特征亲和力（FA）辅助的知识蒸馏（KD）方法，以改进深度神经网络（DNN）的量化感知训练。DNN的中间特征图上的FA损失起到了将中间步骤的解决方案教给学生的作用，而不仅仅是在传统的KD中作用于网络输出级别的logits损失。将logit损失和FA损失结合起来，我们发现量化的学生网络得到的监督比来自标记地面真实数据的监督更强。所得到的FAQD能够在无标签数据上压缩模型，这带来了即时的实际效益，因为预先训练好的教师模型是随时可用的，而无标签数据又是丰富的。相反，数据标记通常是费时费力的。最后，我们提出了一种快速特征亲和力（FFA）损失，它以较低的计算复杂度准确近似FA损失，有助于加快高分辨率训练的速度。",
    "tldr": "本文提出了一种特征亲和力辅助的知识蒸馏方法，通过结合logit损失和特征亲和力损失，可以在无标签数据上压缩深度神经网络模型。"
}