{
    "title": "On the Training Instability of Shuffling SGD with Batch Normalization. (arXiv:2302.12444v2 [cs.LG] UPDATED)",
    "abstract": "We uncover how SGD interacts with batch normalization and can exhibit undesirable training dynamics such as divergence. More precisely, we study how Single Shuffle (SS) and Random Reshuffle (RR) -- two widely used variants of SGD -- interact surprisingly differently in the presence of batch normalization: RR leads to much more stable evolution of training loss than SS. As a concrete example, for regression using a linear network with batch normalization, we prove that SS and RR converge to distinct global optima that are \"distorted\" away from gradient descent. Thereafter, for classification we characterize conditions under which training divergence for SS and RR can, and cannot occur. We present explicit constructions to show how SS leads to distorted optima in regression and divergence for classification, whereas RR avoids both distortion and divergence. We validate our results by confirming them empirically in realistic settings, and conclude that the separation between SS and RR use",
    "link": "http://arxiv.org/abs/2302.12444",
    "context": "Title: On the Training Instability of Shuffling SGD with Batch Normalization. (arXiv:2302.12444v2 [cs.LG] UPDATED)\nAbstract: We uncover how SGD interacts with batch normalization and can exhibit undesirable training dynamics such as divergence. More precisely, we study how Single Shuffle (SS) and Random Reshuffle (RR) -- two widely used variants of SGD -- interact surprisingly differently in the presence of batch normalization: RR leads to much more stable evolution of training loss than SS. As a concrete example, for regression using a linear network with batch normalization, we prove that SS and RR converge to distinct global optima that are \"distorted\" away from gradient descent. Thereafter, for classification we characterize conditions under which training divergence for SS and RR can, and cannot occur. We present explicit constructions to show how SS leads to distorted optima in regression and divergence for classification, whereas RR avoids both distortion and divergence. We validate our results by confirming them empirically in realistic settings, and conclude that the separation between SS and RR use",
    "path": "papers/23/02/2302.12444.json",
    "total_tokens": 980,
    "translated_title": "关于带批量归一化的随机梯度下降训练不稳定性的研究",
    "translated_abstract": "本论文研究了随机梯度下降算法与批量归一化的相互作用，发现常用的单次重排和随机重排这两种 SGD 方式的训练稳定性差异非常大。实验证明，在使用批量归一化的线性网络回归问题中，单次重排和随机重排会分别收敛到扭曲的全局最优点，而在分类问题中，我们进一步探究了它们的训练是否会发散，并提出了实证验证结果，建议在使用 SGD 与批量归一化时，优先考虑随机重排。",
    "tldr": "本文研究了随机梯度下降算法与批量归一化的相互作用，在特定网络中表现为单次重排与随机重排收敛到不同的扭曲全局最优点，建议使用随机重排。",
    "en_tdlr": "This paper investigates the interaction between stochastic gradient descent and batch normalization, and shows that Single Shuffle and Random Reshuffle have significantly different training stability. Specifically, for linear network regression, they converge to distorted global optima, and for classification, training divergence can occur for Single Shuffle but not for Random Reshuffle. It is recommended to use Random Reshuffle when applying SGD with batch normalization."
}