{
    "title": "Gauss-Newton Temporal Difference Learning with Nonlinear Function Approximation",
    "abstract": "arXiv:2302.13087v2 Announce Type: replace-cross  Abstract: In this paper, a Gauss-Newton Temporal Difference (GNTD) learning method is proposed to solve the Q-learning problem with nonlinear function approximation. In each iteration, our method takes one Gauss-Newton (GN) step to optimize a variant of Mean-Squared Bellman Error (MSBE), where target networks are adopted to avoid double sampling. Inexact GN steps are analyzed so that one can safely and efficiently compute the GN updates by cheap matrix iterations. Under mild conditions, non-asymptotic finite-sample convergence to the globally optimal Q function is derived for various nonlinear function approximations. In particular, for neural network parameterization with relu activation, GNTD achieves an improved sample complexity of $\\tilde{\\mathcal{O}}(\\varepsilon^{-1})$, as opposed to the $\\mathcal{\\mathcal{O}}(\\varepsilon^{-2})$ sample complexity of the existing neural TD methods. An $\\tilde{\\mathcal{O}}(\\varepsilon^{-1.5})$ sample",
    "link": "https://arxiv.org/abs/2302.13087",
    "context": "Title: Gauss-Newton Temporal Difference Learning with Nonlinear Function Approximation\nAbstract: arXiv:2302.13087v2 Announce Type: replace-cross  Abstract: In this paper, a Gauss-Newton Temporal Difference (GNTD) learning method is proposed to solve the Q-learning problem with nonlinear function approximation. In each iteration, our method takes one Gauss-Newton (GN) step to optimize a variant of Mean-Squared Bellman Error (MSBE), where target networks are adopted to avoid double sampling. Inexact GN steps are analyzed so that one can safely and efficiently compute the GN updates by cheap matrix iterations. Under mild conditions, non-asymptotic finite-sample convergence to the globally optimal Q function is derived for various nonlinear function approximations. In particular, for neural network parameterization with relu activation, GNTD achieves an improved sample complexity of $\\tilde{\\mathcal{O}}(\\varepsilon^{-1})$, as opposed to the $\\mathcal{\\mathcal{O}}(\\varepsilon^{-2})$ sample complexity of the existing neural TD methods. An $\\tilde{\\mathcal{O}}(\\varepsilon^{-1.5})$ sample",
    "path": "papers/23/02/2302.13087.json",
    "total_tokens": 905,
    "translated_title": "非线性函数逼近下的高斯-牛顿时差学习",
    "translated_abstract": "在这篇论文中，提出了一种使用高斯-牛顿时差（GNTD）学习方法来解决具有非线性函数逼近的Q-learning问题。在每次迭代中，我们的方法采用高斯-牛顿（GN）步骤来优化一种变体的均方贝尔曼误差（MSBE），其中采用目标网络来避免双重采样。分析了不精确的GN步骤，因此可以通过廉价的矩阵迭代安全且高效地计算GN更新。在温和条件下，针对各种非线性函数逼近推导了有限样本非渐近收敛到全局最优Q函数。特别是对于具有relu激活的神经网络参数化，GNTD达到了$\\tilde{\\mathcal{O}}(\\varepsilon^{-1})$的改进样本复杂度，而现有神经网络TD方法的样本复杂度为$\\mathcal{\\mathcal{O}}(\\varepsilon^{-2})$。最近阶$\\tilde{\\mathcal{O}}(\\varepsilon^{-1.5})$的样本",
    "tldr": "提出了一种在非线性函数逼近下的高斯-牛顿时差学习方法，能够显著改进对全局最优Q函数的样本复杂度。",
    "en_tdlr": "Proposed a Gauss-Newton Temporal Difference learning method with nonlinear function approximation, significantly improving the sample complexity for the globally optimal Q function."
}