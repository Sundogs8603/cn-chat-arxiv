{
    "title": "Breaking the Curse of Multiagents in a Large State Space: RL in Markov Games with Independent Linear Function Approximation. (arXiv:2302.03673v3 [cs.LG] UPDATED)",
    "abstract": "We propose a new model, independent linear Markov game, for multi-agent reinforcement learning with a large state space and a large number of agents. This is a class of Markov games with independent linear function approximation, where each agent has its own function approximation for the state-action value functions that are marginalized by other players' policies. We design new algorithms for learning the Markov coarse correlated equilibria (CCE) and Markov correlated equilibria (CE) with sample complexity bounds that only scale polynomially with each agent's own function class complexity, thus breaking the curse of multiagents. In contrast, existing works for Markov games with function approximation have sample complexity bounds scale with the size of the \\emph{joint action space} when specialized to the canonical tabular Markov game setting, which is exponentially large in the number of agents. Our algorithms rely on two key technical innovations: (1) utilizing policy replay to tac",
    "link": "http://arxiv.org/abs/2302.03673",
    "context": "Title: Breaking the Curse of Multiagents in a Large State Space: RL in Markov Games with Independent Linear Function Approximation. (arXiv:2302.03673v3 [cs.LG] UPDATED)\nAbstract: We propose a new model, independent linear Markov game, for multi-agent reinforcement learning with a large state space and a large number of agents. This is a class of Markov games with independent linear function approximation, where each agent has its own function approximation for the state-action value functions that are marginalized by other players' policies. We design new algorithms for learning the Markov coarse correlated equilibria (CCE) and Markov correlated equilibria (CE) with sample complexity bounds that only scale polynomially with each agent's own function class complexity, thus breaking the curse of multiagents. In contrast, existing works for Markov games with function approximation have sample complexity bounds scale with the size of the \\emph{joint action space} when specialized to the canonical tabular Markov game setting, which is exponentially large in the number of agents. Our algorithms rely on two key technical innovations: (1) utilizing policy replay to tac",
    "path": "papers/23/02/2302.03673.json",
    "total_tokens": 1101,
    "translated_title": "在大状态空间中打破多智体的诅咒：带独立线性函数逼近的Markov博弈中的强化学习",
    "translated_abstract": "我们提出了一种新的模型——独立线性Markov博弈，用于具有大状态空间和大量代理的多智体强化学习。这是一类带有独立线性函数逼近的Markov博弈，每个代理都有自己的函数逼近，用于被其他玩家的策略边缘化的状态-动作值函数。我们设计了学习Markov粗略相关均衡和Markov相关均衡的新算法，并提供了样本复杂度界限，这些界限仅与每个代理自己的函数类复杂度成多项式比例，从而打破了多智体的诅咒。相比之下，现有的用于函数逼近的Markov博弈的研究，在特化于标准表格状况的Markov博弈设置时，其样本复杂度界限会随着\\emph{联合行动空间}的大小成指数级增长，而该联合行动空间在代理的数量上呈指数级增长。我们的算法依赖于两个关键的技术创新：(1) 利用策略重放来降低样本复杂度；(2) 利用独立线性函数逼近来获得计算上的有效性和统计上的高精度。",
    "tldr": "本研究提出了一种新的独立线性Markov博弈模型，针对多智体强化学习中的大状态空间和大量代理问题，设计了新算法以学习Markov粗略相关均衡和Markov相关均衡。相比于现有的Markov博弈函数逼近技术，我们的方法能够大大降低样本复杂度并取得更高的精度。",
    "en_tdlr": "This paper proposes a new independent linear Markov game model for multi-agent reinforcement learning in large state space with a large number of agents, and designs new algorithms to learn Markov coarse correlated equilibria and Markov correlated equilibria with significantly lower sample complexity than existing techniques based on Markov game function approximation."
}