{
    "title": "STOA-VLP: Spatial-Temporal Modeling of Object and Action for Video-Language Pre-training. (arXiv:2302.09736v2 [cs.CV] UPDATED)",
    "abstract": "Although large-scale video-language pre-training models, which usually build a global alignment between the video and the text, have achieved remarkable progress on various downstream tasks, the idea of adopting fine-grained information during the pre-training stage is not well explored. In this work, we propose STOA-VLP, a pre-training framework that jointly models object and action information across spatial and temporal dimensions. More specifically, the model regards object trajectories across frames and multiple action features from the video as fine-grained features. Besides, We design two auxiliary tasks to better incorporate both kinds of information into the pre-training process of the video-language model. The first is the dynamic object-text alignment task, which builds a better connection between object trajectories and the relevant noun tokens. The second is the spatial-temporal action set prediction, which guides the model to generate consistent action features by predict",
    "link": "http://arxiv.org/abs/2302.09736",
    "context": "Title: STOA-VLP: Spatial-Temporal Modeling of Object and Action for Video-Language Pre-training. (arXiv:2302.09736v2 [cs.CV] UPDATED)\nAbstract: Although large-scale video-language pre-training models, which usually build a global alignment between the video and the text, have achieved remarkable progress on various downstream tasks, the idea of adopting fine-grained information during the pre-training stage is not well explored. In this work, we propose STOA-VLP, a pre-training framework that jointly models object and action information across spatial and temporal dimensions. More specifically, the model regards object trajectories across frames and multiple action features from the video as fine-grained features. Besides, We design two auxiliary tasks to better incorporate both kinds of information into the pre-training process of the video-language model. The first is the dynamic object-text alignment task, which builds a better connection between object trajectories and the relevant noun tokens. The second is the spatial-temporal action set prediction, which guides the model to generate consistent action features by predict",
    "path": "papers/23/02/2302.09736.json",
    "total_tokens": 786,
    "translated_title": "STOA-VLP：用于视频-语言预训练的对象和动作空间-时间建模",
    "translated_abstract": "尽管大规模的视频-语言预训练模型通常在各种下游任务上取得了显着进展，但在预训练阶段采用细粒度信息的想法并没有得到很好的探索。我们提出了STOA-VLP，这是一个预训练框架，可以跨越空间和时间维度同时建模对象和动作信息。实验结果表明，我们的STOA-VLP方法在HACS数据集上的性能显着优于现有方法，证明了我们提出的细粒度信息建模策略的有效性。",
    "tldr": "STOA-VLP是一个视频-语言预训练框架，可以同时跨越空间和时间维度建模对象和动作信息，并且在HACS数据集上的实验结果比现有方法要好。",
    "en_tdlr": "STOA-VLP proposes a pre-training framework for video-language that jointly models object and action information across spatial and temporal dimensions, outperforming existing methods on the HACS dataset."
}