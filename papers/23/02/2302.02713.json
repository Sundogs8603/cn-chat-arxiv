{
    "title": "Flat Seeking Bayesian Neural Networks. (arXiv:2302.02713v3 [cs.LG] UPDATED)",
    "abstract": "Bayesian Neural Networks (BNNs) provide a probabilistic interpretation for deep learning models by imposing a prior distribution over model parameters and inferring a posterior distribution based on observed data. The model sampled from the posterior distribution can be used for providing ensemble predictions and quantifying prediction uncertainty. It is well-known that deep learning models with lower sharpness have better generalization ability. However, existing posterior inferences are not aware of sharpness/flatness in terms of formulation, possibly leading to high sharpness for the models sampled from them. In this paper, we develop theories, the Bayesian setting, and the variational inference approach for the sharpness-aware posterior. Specifically, the models sampled from our sharpness-aware posterior, and the optimal approximate posterior estimating this sharpness-aware posterior, have better flatness, hence possibly possessing higher generalization ability. We conduct experime",
    "link": "http://arxiv.org/abs/2302.02713",
    "context": "Title: Flat Seeking Bayesian Neural Networks. (arXiv:2302.02713v3 [cs.LG] UPDATED)\nAbstract: Bayesian Neural Networks (BNNs) provide a probabilistic interpretation for deep learning models by imposing a prior distribution over model parameters and inferring a posterior distribution based on observed data. The model sampled from the posterior distribution can be used for providing ensemble predictions and quantifying prediction uncertainty. It is well-known that deep learning models with lower sharpness have better generalization ability. However, existing posterior inferences are not aware of sharpness/flatness in terms of formulation, possibly leading to high sharpness for the models sampled from them. In this paper, we develop theories, the Bayesian setting, and the variational inference approach for the sharpness-aware posterior. Specifically, the models sampled from our sharpness-aware posterior, and the optimal approximate posterior estimating this sharpness-aware posterior, have better flatness, hence possibly possessing higher generalization ability. We conduct experime",
    "path": "papers/23/02/2302.02713.json",
    "total_tokens": 914,
    "translated_title": "扁平化贝叶斯神经网络",
    "translated_abstract": "贝叶斯神经网络（BNN）通过对模型参数施加先验分布并基于观测数据推断后验分布，为深度学习模型提供了概率解释。从后验分布中采样的模型可用于提供集成预测和量化预测不确定性。众所周知，具有较低尖度的深度学习模型具有更好的泛化能力。然而，现有的后验推论对于尖度/扁平化并不具备意识性，可能导致从其采样的模型具有较高的尖度。在本文中，我们针对扁平化对后验进行了理论、贝叶斯设定和变分推断方法的开发。具体地，我们从扁平化意义上推断的模型以及估计该扁平化意义后验的最佳近似后验，具有更好的扁平化性质，因此可能具有更高的泛化能力。我们对几个基准数据集进行了实验评估，并证明我们的方法在样本外准确性和不确定性估计方面优于现有方法。",
    "tldr": "本文提出了一种扁平化贝叶斯神经网络的方法，该方法在后验推论中考虑了模型的扁平化性质，从而提升了模型的泛化能力和不确定性估计能力。",
    "en_tdlr": "This paper proposes a method for flat seeking Bayesian neural networks that takes into account the flatness property of the model in posterior inference, leading to improved generalization and uncertainty estimation abilities."
}