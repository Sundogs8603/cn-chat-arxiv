{
    "title": "Generalization Bounds with Data-dependent Fractal Dimensions. (arXiv:2302.02766v2 [stat.ML] UPDATED)",
    "abstract": "Providing generalization guarantees for modern neural networks has been a crucial task in statistical learning. Recently, several studies have attempted to analyze the generalization error in such settings by using tools from fractal geometry. While these works have successfully introduced new mathematical tools to apprehend generalization, they heavily rely on a Lipschitz continuity assumption, which in general does not hold for neural networks and might make the bounds vacuous. In this work, we address this issue and prove fractal geometry-based generalization bounds without requiring any Lipschitz assumption. To achieve this goal, we build up on a classical covering argument in learning theory and introduce a data-dependent fractal dimension. Despite introducing a significant amount of technical complications, this new notion lets us control the generalization error (over either fixed or random hypothesis spaces) along with certain mutual information (MI) terms. To provide a clearer",
    "link": "http://arxiv.org/abs/2302.02766",
    "context": "Title: Generalization Bounds with Data-dependent Fractal Dimensions. (arXiv:2302.02766v2 [stat.ML] UPDATED)\nAbstract: Providing generalization guarantees for modern neural networks has been a crucial task in statistical learning. Recently, several studies have attempted to analyze the generalization error in such settings by using tools from fractal geometry. While these works have successfully introduced new mathematical tools to apprehend generalization, they heavily rely on a Lipschitz continuity assumption, which in general does not hold for neural networks and might make the bounds vacuous. In this work, we address this issue and prove fractal geometry-based generalization bounds without requiring any Lipschitz assumption. To achieve this goal, we build up on a classical covering argument in learning theory and introduce a data-dependent fractal dimension. Despite introducing a significant amount of technical complications, this new notion lets us control the generalization error (over either fixed or random hypothesis spaces) along with certain mutual information (MI) terms. To provide a clearer",
    "path": "papers/23/02/2302.02766.json",
    "total_tokens": 847,
    "translated_title": "数据依赖分形维度的泛化界限",
    "translated_abstract": "在统计学习中，为现代神经网络提供泛化保证是一项关键任务。最近，一些研究尝试使用分形几何的工具来分析这种情况下的泛化误差。尽管这些工作成功地引入了新的数学工具来理解泛化，但它们严重依赖于Lipschitz连续性假设，而这一假设通常不适用于神经网络，并且可能使界限变得无效。在这项工作中，我们解决了这个问题，并且证明了不需要任何Lipschitz假设的基于分形几何的泛化界限。为了实现这个目标，我们在学习理论中基于经典的覆盖论证，并引入了数据依赖的分形维度。尽管引入了大量的技术复杂性，但这个新概念使我们能够控制泛化误差（在固定或随机的假设空间上）以及特定的互信息（MI）项。",
    "tldr": "这项研究提出了基于数据依赖的分形维度的泛化界限，不需要Lipschitz假设，并能控制泛化误差和互信息项。",
    "en_tdlr": "This research introduces generalization bounds based on data-dependent fractal dimensions without requiring Lipschitz assumptions, allowing control over generalization error and mutual information terms."
}