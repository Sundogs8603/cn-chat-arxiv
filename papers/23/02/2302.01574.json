{
    "title": "An Operational Perspective to Fairness Interventions: Where and How to Intervene. (arXiv:2302.01574v2 [cs.LG] UPDATED)",
    "abstract": "As AI-based decision systems proliferate, their successful operationalization requires balancing multiple desiderata: predictive performance, disparity across groups, safeguarding sensitive group attributes (e.g., race), and engineering cost. We present a holistic framework for evaluating and contextualizing fairness interventions with respect to the above desiderata. The two key points of practical consideration are \\emph{where} (pre-, in-, post-processing) and \\emph{how} (in what way the sensitive group data is used) the intervention is introduced. We demonstrate our framework with a case study on predictive parity. In it, we first propose a novel method for achieving predictive parity fairness without using group data at inference time via distibutionally robust optimization. Then, we showcase the effectiveness of these methods in a benchmarking study of close to 400 variations across two major model types (XGBoost vs. Neural Net), ten datasets, and over twenty unique methodologies.",
    "link": "http://arxiv.org/abs/2302.01574",
    "context": "Title: An Operational Perspective to Fairness Interventions: Where and How to Intervene. (arXiv:2302.01574v2 [cs.LG] UPDATED)\nAbstract: As AI-based decision systems proliferate, their successful operationalization requires balancing multiple desiderata: predictive performance, disparity across groups, safeguarding sensitive group attributes (e.g., race), and engineering cost. We present a holistic framework for evaluating and contextualizing fairness interventions with respect to the above desiderata. The two key points of practical consideration are \\emph{where} (pre-, in-, post-processing) and \\emph{how} (in what way the sensitive group data is used) the intervention is introduced. We demonstrate our framework with a case study on predictive parity. In it, we first propose a novel method for achieving predictive parity fairness without using group data at inference time via distibutionally robust optimization. Then, we showcase the effectiveness of these methods in a benchmarking study of close to 400 variations across two major model types (XGBoost vs. Neural Net), ten datasets, and over twenty unique methodologies.",
    "path": "papers/23/02/2302.01574.json",
    "total_tokens": 937,
    "translated_title": "公平性干预的操作视角：何时和如何干预",
    "translated_abstract": "随着基于人工智能的决策系统的不断增加，它们的成功运营需要在预测性能、跨组差异、保护敏感群组属性（如种族）和工程成本之间取得平衡。我们提出了一个全面的框架来评估和将公平性干预与上述期望相结合。实践中需要考虑的两个关键点是干预的“何时”（预处理、处理过程中、后处理）和“如何”（敏感群组数据使用的方法）。我们通过对预测平等性的案例研究来展示我们的框架。在其中，我们首先提出了一种通过分布式鲁棒优化实现预测平等的新方法，而无需在推断时使用群组数据。然后，我们展示了这些方法在近400种不同变体的两个主要模型类型（XGBoost vs. Neural Net）、十个数据集和二十多种独特方法的基准研究中的有效性。",
    "tldr": "该论文提出了一个平衡预测性能、跨组差异、保护敏感群组属性和工程成本的全面框架，探讨了公平性干预的“何时”和“如何”，同时通过预测平等的案例研究，展示了一种新的方法。",
    "en_tdlr": "This paper proposes a comprehensive framework for balancing predictive performance, disparity across groups, safeguarding sensitive group attributes, and engineering cost in AI-based decision systems. It explores the \"where\" and \"how\" of fairness interventions and demonstrates a novel method for achieving predictive parity without using group data at inference time through distributionally robust optimization."
}