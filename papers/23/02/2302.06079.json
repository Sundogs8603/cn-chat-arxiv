{
    "title": "Byzantine-Robust Learning on Heterogeneous Data via Gradient Splitting. (arXiv:2302.06079v2 [cs.LG] UPDATED)",
    "abstract": "Federated learning has exhibited vulnerabilities to Byzantine attacks, where the Byzantine attackers can send arbitrary gradients to a central server to destroy the convergence and performance of the global model. A wealth of robust AGgregation Rules (AGRs) have been proposed to defend against Byzantine attacks. However, Byzantine clients can still circumvent robust AGRs when data is non-Identically and Independently Distributed (non-IID). In this paper, we first reveal the root causes of performance degradation of current robust AGRs in non-IID settings: the curse of dimensionality and gradient heterogeneity. In order to address this issue, we propose GAS, a \\shorten approach that can successfully adapt existing robust AGRs to non-IID settings. We also provide a detailed convergence analysis when the existing robust AGRs are combined with GAS. Experiments on various real-world datasets verify the efficacy of our proposed GAS. The implementation code is provided in https://github.com/Y",
    "link": "http://arxiv.org/abs/2302.06079",
    "context": "Title: Byzantine-Robust Learning on Heterogeneous Data via Gradient Splitting. (arXiv:2302.06079v2 [cs.LG] UPDATED)\nAbstract: Federated learning has exhibited vulnerabilities to Byzantine attacks, where the Byzantine attackers can send arbitrary gradients to a central server to destroy the convergence and performance of the global model. A wealth of robust AGgregation Rules (AGRs) have been proposed to defend against Byzantine attacks. However, Byzantine clients can still circumvent robust AGRs when data is non-Identically and Independently Distributed (non-IID). In this paper, we first reveal the root causes of performance degradation of current robust AGRs in non-IID settings: the curse of dimensionality and gradient heterogeneity. In order to address this issue, we propose GAS, a \\shorten approach that can successfully adapt existing robust AGRs to non-IID settings. We also provide a detailed convergence analysis when the existing robust AGRs are combined with GAS. Experiments on various real-world datasets verify the efficacy of our proposed GAS. The implementation code is provided in https://github.com/Y",
    "path": "papers/23/02/2302.06079.json",
    "total_tokens": 928,
    "translated_title": "通过梯度分割实现对异构数据的拜占庭容错学习",
    "translated_abstract": "联邦学习对拜占庭攻击具有 vulnerabilities，即攻击者可以向中央服务器发送任意梯度以破坏全局模型的收敛和性能。一些健壮的聚合规则（AGRs）已被提出来以抵御对抗拜占庭攻击。但是，当数据不服从独立同分布（non-IID）时，拜占庭客户端仍然可以规避健壮的 AGRs。本文首先揭示了当前健壮 AGRs 在非IID环境下表现下降的根本原因：维度灾难和梯度异质性。为了解决这个问题，我们提出了 GAS，一种缩短方法，可以成功地将现有的健壮 AGRs 适应于非IID环境。当现有健壮 AGRs 与 GAS 组合时，我们还提供了详细的收敛分析。各种真实数据集上的实验证明了我们提出的 GAS 的有效性。实现代码可在 https://github.com/Y 中找到。",
    "tldr": "这篇论文提出了一种缓解目前健壮算法在非IID环境下表现下降的方法，名为GAS，该方法能够成功将现有的健壮算法用于非IID的数据，并且在真实数据集上有效。"
}