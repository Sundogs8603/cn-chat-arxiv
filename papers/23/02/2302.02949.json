{
    "title": "Adaptive Parameterization of Deep Learning Models for Federated Learning. (arXiv:2302.02949v2 [cs.LG] UPDATED)",
    "abstract": "Federated Learning offers a way to train deep neural networks in a distributed fashion. While this addresses limitations related to distributed data, it incurs a communication overhead as the model parameters or gradients need to be exchanged regularly during training. This can be an issue with large scale distribution of learning tasks and negate the benefit of the respective resource distribution. In this paper, we we propose to utilise parallel Adapters for Federated Learning. Using various datasets, we show that Adapters can be incorporated to different Federated Learning techniques. We highlight that our approach can achieve similar inference performance compared to training the full model while reducing the communication overhead by roughly 90%. We further explore the applicability of Adapters in cross-silo and cross-device settings, as well as different non-IID data distributions.",
    "link": "http://arxiv.org/abs/2302.02949",
    "context": "Title: Adaptive Parameterization of Deep Learning Models for Federated Learning. (arXiv:2302.02949v2 [cs.LG] UPDATED)\nAbstract: Federated Learning offers a way to train deep neural networks in a distributed fashion. While this addresses limitations related to distributed data, it incurs a communication overhead as the model parameters or gradients need to be exchanged regularly during training. This can be an issue with large scale distribution of learning tasks and negate the benefit of the respective resource distribution. In this paper, we we propose to utilise parallel Adapters for Federated Learning. Using various datasets, we show that Adapters can be incorporated to different Federated Learning techniques. We highlight that our approach can achieve similar inference performance compared to training the full model while reducing the communication overhead by roughly 90%. We further explore the applicability of Adapters in cross-silo and cross-device settings, as well as different non-IID data distributions.",
    "path": "papers/23/02/2302.02949.json",
    "total_tokens": 738,
    "translated_title": "适应性参数化的联邦学习深度学习模型",
    "translated_abstract": "联邦学习提供了一种在分布式环境下训练深度神经网络的方法，但它需要在训练过程中定期交换模型参数或梯度，导致通信开销问题。本文提出利用并行适配器来优化联邦学习，并在多个数据集上验证了这一方法的可行性。实验结果表明，相比于传统方法，适配器可以降低约90%的通信开销，同时保持相近的预测性能。我们也探讨了适配器在不同环境下的应用，包括跨边界和跨设备场景及不同的非独立同分布数据分布情况。",
    "tldr": "本文提出了利用并行适配器来优化联邦学习，实验表明可以降低约90%的通信开销，同时保持相近的预测性能。",
    "en_tdlr": "This paper proposes to use parallel Adapters to optimize Federated Learning, and experiments show that it can reduce communication overhead by roughly 90% while maintaining similar prediction performance."
}