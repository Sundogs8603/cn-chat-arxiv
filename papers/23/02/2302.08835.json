{
    "title": "h-analysis and data-parallel physics-informed neural networks. (arXiv:2302.08835v2 [cs.CE] UPDATED)",
    "abstract": "We explore the data-parallel acceleration of physics-informed machine learning (PIML) schemes, with a focus on physics-informed neural networks (PINNs) for multiple graphics processing units (GPUs) architectures. In order to develop scale-robust and high-throughput PIML models for sophisticated applications which may require a large number of training points (e.g., involving complex and high-dimensional domains, non-linear operators or multi-physics), we detail a novel protocol based on $h$-analysis and data-parallel acceleration through the Horovod training framework. The protocol is backed by new convergence bounds for the generalization error and the train-test gap. We show that the acceleration is straightforward to implement, does not compromise training, and proves to be highly efficient and controllable, paving the way towards generic scale-robust PIML. Extensive numerical experiments with increasing complexity illustrate its robustness and consistency, offering a wide range of ",
    "link": "http://arxiv.org/abs/2302.08835",
    "context": "Title: h-analysis and data-parallel physics-informed neural networks. (arXiv:2302.08835v2 [cs.CE] UPDATED)\nAbstract: We explore the data-parallel acceleration of physics-informed machine learning (PIML) schemes, with a focus on physics-informed neural networks (PINNs) for multiple graphics processing units (GPUs) architectures. In order to develop scale-robust and high-throughput PIML models for sophisticated applications which may require a large number of training points (e.g., involving complex and high-dimensional domains, non-linear operators or multi-physics), we detail a novel protocol based on $h$-analysis and data-parallel acceleration through the Horovod training framework. The protocol is backed by new convergence bounds for the generalization error and the train-test gap. We show that the acceleration is straightforward to implement, does not compromise training, and proves to be highly efficient and controllable, paving the way towards generic scale-robust PIML. Extensive numerical experiments with increasing complexity illustrate its robustness and consistency, offering a wide range of ",
    "path": "papers/23/02/2302.08835.json",
    "total_tokens": 966,
    "translated_title": "h分析和数据并行的物理启发神经网络",
    "translated_abstract": "我们探索了物理启发机器学习（PIML）方案的数据并行加速，在多个图形处理单元（GPUs）体系结构下关注物理启发神经网络（PINNs）。为了开发适用于复杂应用的规模鲁棒和高吞吐量的PIML模型（例如，涉及复杂和高维领域、非线性操作符或多物理学），我们详细介绍了一种基于$h$-分析和通过Horovod训练框架进行数据并行加速的新协议。该协议基于对泛化误差和训练-测试差距的新收敛界限。我们表明加速实现简单，不会损害训练，并且证明是高效和可控的，为通用的规模鲁棒PIML铺平了道路。通过增加复杂性的大量数值实验证明了其稳健性和一致性，提供了广泛的",
    "tldr": "本论文提出了一种基于$h$-分析和数据并行加速的物理启发神经网络（PINNs）的新协议，可以实现具有规模鲁棒性和高吞吐量的PIML模型。实验证明该协议易于实现，不会影响训练，并且具有高效性和可控性，为实现通用的规模鲁棒PIML铺平了道路。"
}