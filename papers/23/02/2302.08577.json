{
    "title": "Keep it Neutral: Using Natural Language Inference to Improve Generation. (arXiv:2302.08577v2 [cs.CL] UPDATED)",
    "abstract": "We explore incorporating natural language inference (NLI) into the text generative pipeline by using a pre-trained NLI model to assess whether a generated sentence entails, contradicts, or is neutral to the prompt and preceding text. First, we show that the NLI task is predictive of generation errors made by GPT-3. We use these results to develop an NLI-informed generation procedure for GPT-J. Then, we evaluate these generations by obtaining human annotations on error types and overall quality. We find that an NLI strategy of maximizing entailment improves text generation when the nucleus sampling randomness parameter value is high, while one which maximizes contradiction is in fact productive when the parameter value is low. Overall, though, we demonstrate that an NLI strategy of maximizing the neutral class provides the highest quality of generated text (significantly better than the vanilla generations), regardless of parameter value.",
    "link": "http://arxiv.org/abs/2302.08577",
    "context": "Title: Keep it Neutral: Using Natural Language Inference to Improve Generation. (arXiv:2302.08577v2 [cs.CL] UPDATED)\nAbstract: We explore incorporating natural language inference (NLI) into the text generative pipeline by using a pre-trained NLI model to assess whether a generated sentence entails, contradicts, or is neutral to the prompt and preceding text. First, we show that the NLI task is predictive of generation errors made by GPT-3. We use these results to develop an NLI-informed generation procedure for GPT-J. Then, we evaluate these generations by obtaining human annotations on error types and overall quality. We find that an NLI strategy of maximizing entailment improves text generation when the nucleus sampling randomness parameter value is high, while one which maximizes contradiction is in fact productive when the parameter value is low. Overall, though, we demonstrate that an NLI strategy of maximizing the neutral class provides the highest quality of generated text (significantly better than the vanilla generations), regardless of parameter value.",
    "path": "papers/23/02/2302.08577.json",
    "total_tokens": 884,
    "translated_title": "保持中立：使用自然语言推理改进生成器",
    "translated_abstract": "本文研究将自然语言推理（NLI）引入文本生成过程中，通过使用预训练的NLI模型来评估生成的句子是否符合、与原始文本相矛盾或中立。首先，我们证明NLI任务能够预测GPT-3生成错误。我们利用这些结果为GPT-J开发了一种基于NLI的生成策略。然后，我们通过人工标注错误类型和整体质量来评估生成的结果。我们发现，在核心采样的随机参数值较高时，最大化蕴涵关系的NLI策略改善了文本生成，而在参数值较低时，最大化矛盾关系的策略实际上是有效的。总体而言，我们展示了最大化中立类别的NLI策略提供了最高质量的生成文本（显著优于普通生成器），无论参数取值如何。",
    "tldr": "本文将自然语言推理（NLI）引入文本生成过程中，通过预训练的NLI模型评估生成的句子是否符合、与原始文本相矛盾或中立。最大化中立类别的NLI策略提供了最高质量的生成文本，无论参数取值如何。",
    "en_tdlr": "This paper explores incorporating natural language inference (NLI) into the text generative pipeline to assess the generated sentences' entailment, contradiction, or neutrality. The NLI strategy of maximizing the neutral class provides the highest quality of generated text, regardless of parameter value."
}