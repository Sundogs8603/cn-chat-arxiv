{
    "title": "Efficient Parametric Approximations of Neural Network Function Space Distance. (arXiv:2302.03519v2 [cs.LG] UPDATED)",
    "abstract": "It is often useful to compactly summarize important properties of model parameters and training data so that they can be used later without storing and/or iterating over the entire dataset. As a specific case, we consider estimating the Function Space Distance (FSD) over a training set, i.e. the average discrepancy between the outputs of two neural networks. We propose a Linearized Activation Function TRick (LAFTR) and derive an efficient approximation to FSD for ReLU neural networks. The key idea is to approximate the architecture as a linear network with stochastic gating. Despite requiring only one parameter per unit of the network, our approach outcompetes other parametric approximations with larger memory requirements. Applied to continual learning, our parametric approximation is competitive with state-of-the-art nonparametric approximations, which require storing many training examples. Furthermore, we show its efficacy in estimating influence functions accurately and detecting ",
    "link": "http://arxiv.org/abs/2302.03519",
    "context": "Title: Efficient Parametric Approximations of Neural Network Function Space Distance. (arXiv:2302.03519v2 [cs.LG] UPDATED)\nAbstract: It is often useful to compactly summarize important properties of model parameters and training data so that they can be used later without storing and/or iterating over the entire dataset. As a specific case, we consider estimating the Function Space Distance (FSD) over a training set, i.e. the average discrepancy between the outputs of two neural networks. We propose a Linearized Activation Function TRick (LAFTR) and derive an efficient approximation to FSD for ReLU neural networks. The key idea is to approximate the architecture as a linear network with stochastic gating. Despite requiring only one parameter per unit of the network, our approach outcompetes other parametric approximations with larger memory requirements. Applied to continual learning, our parametric approximation is competitive with state-of-the-art nonparametric approximations, which require storing many training examples. Furthermore, we show its efficacy in estimating influence functions accurately and detecting ",
    "path": "papers/23/02/2302.03519.json",
    "total_tokens": 984,
    "tldr": "本文提出一种新的线性化激活函数技巧，针对ReLU神经网络导出了一种快速且高效的函数空间距离（FSD）逼近方法，本方法每个网络单元仅需一个参数，但在性能方面却超过了其他参数化逼近方法，同时也与最先进的非参数逼近方法竞争，且在估计影响函数和检测方面非常有效。",
    "en_tdlr": "This paper proposes a new Linearized Activation Function TRick (LAFTR) and an efficient approximation to Function Space Distance (FSD) for ReLU neural networks. With only one parameter per unit, the proposed method outperforms other parametric approximations with larger memory requirements, and competes with state-of-the-art nonparametric approximations. Moreover, its efficacy in accurately estimating influence functions and detecting is shown."
}