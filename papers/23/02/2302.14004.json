{
    "title": "Optimistic Planning by Regularized Dynamic Programming. (arXiv:2302.14004v3 [cs.LG] UPDATED)",
    "abstract": "We propose a new method for optimistic planning in infinite-horizon discounted Markov decision processes based on the idea of adding regularization to the updates of an otherwise standard approximate value iteration procedure. This technique allows us to avoid contraction and monotonicity arguments typically required by existing analyses of approximate dynamic programming methods, and in particular to use approximate transition functions estimated via least-squares procedures in MDPs with linear function approximation. We use our method to recover known guarantees in tabular MDPs and to provide a computationally efficient algorithm for learning near-optimal policies in discounted linear mixture MDPs from a single stream of experience, and show it achieves near-optimal statistical guarantees.",
    "link": "http://arxiv.org/abs/2302.14004",
    "context": "Title: Optimistic Planning by Regularized Dynamic Programming. (arXiv:2302.14004v3 [cs.LG] UPDATED)\nAbstract: We propose a new method for optimistic planning in infinite-horizon discounted Markov decision processes based on the idea of adding regularization to the updates of an otherwise standard approximate value iteration procedure. This technique allows us to avoid contraction and monotonicity arguments typically required by existing analyses of approximate dynamic programming methods, and in particular to use approximate transition functions estimated via least-squares procedures in MDPs with linear function approximation. We use our method to recover known guarantees in tabular MDPs and to provide a computationally efficient algorithm for learning near-optimal policies in discounted linear mixture MDPs from a single stream of experience, and show it achieves near-optimal statistical guarantees.",
    "path": "papers/23/02/2302.14004.json",
    "total_tokens": 750,
    "translated_title": "基于正则化动态规划的乐观规划方法",
    "translated_abstract": "我们提出了一种新的无限时段折扣马尔可夫决策过程中乐观规划的方法，基于在近似值迭代过程的更新中添加正则化的思想。此技术使我们能够避免萎缩和单调性论证，这通常是现有近似动态规划方法分析所要求的，特别是可以在具有线性函数逼近的MDPs中使用通过最小二乘法估计的近似转移函数。我们使用该方法恢复了表格MDPs中已知的保证，并提供了一种从单个流经验中学习折扣线性混合MDPs中接近最优策略的计算有效算法，并证明它实现了近乎最优的统计保证。",
    "tldr": "本文提出了一种基于正则化动态规划的乐观规划方法，可用于学习折扣线性混合MDPs中的最优策略，且具有近乎最优的统计保证",
    "en_tdlr": "This paper proposes a new optimistic planning method based on regularized dynamic programming, which can be used to learn optimal policies in discounted linear mixture MDPs with near-optimal statistical guarantees."
}