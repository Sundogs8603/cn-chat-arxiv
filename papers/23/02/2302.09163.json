{
    "title": "The Shrinkage-Delinkage Trade-off: An Analysis of Factorized Gaussian Approximations for Variational Inference. (arXiv:2302.09163v2 [stat.ML] UPDATED)",
    "abstract": "When factorized approximations are used for variational inference (VI), they tend to underestimate the uncertainty -- as measured in various ways -- of the distributions they are meant to approximate. We consider two popular ways to measure the uncertainty deficit of VI: (i) the degree to which it underestimates the componentwise variance, and (ii) the degree to which it underestimates the entropy. To better understand these effects, and the relationship between them, we examine an informative setting where they can be explicitly (and elegantly) analyzed: the approximation of a Gaussian,~$p$, with a dense covariance matrix, by a Gaussian,~$q$, with a diagonal covariance matrix. We prove that $q$ always underestimates both the componentwise variance and the entropy of $p$, \\textit{though not necessarily to the same degree}. Moreover we demonstrate that the entropy of $q$ is determined by the trade-off of two competing forces: it is decreased by the shrinkage of its componentwise varianc",
    "link": "http://arxiv.org/abs/2302.09163",
    "context": "Title: The Shrinkage-Delinkage Trade-off: An Analysis of Factorized Gaussian Approximations for Variational Inference. (arXiv:2302.09163v2 [stat.ML] UPDATED)\nAbstract: When factorized approximations are used for variational inference (VI), they tend to underestimate the uncertainty -- as measured in various ways -- of the distributions they are meant to approximate. We consider two popular ways to measure the uncertainty deficit of VI: (i) the degree to which it underestimates the componentwise variance, and (ii) the degree to which it underestimates the entropy. To better understand these effects, and the relationship between them, we examine an informative setting where they can be explicitly (and elegantly) analyzed: the approximation of a Gaussian,~$p$, with a dense covariance matrix, by a Gaussian,~$q$, with a diagonal covariance matrix. We prove that $q$ always underestimates both the componentwise variance and the entropy of $p$, \\textit{though not necessarily to the same degree}. Moreover we demonstrate that the entropy of $q$ is determined by the trade-off of two competing forces: it is decreased by the shrinkage of its componentwise varianc",
    "path": "papers/23/02/2302.09163.json",
    "total_tokens": 1014,
    "translated_title": "收缩-解耦平衡：分析因子化高斯逼近在变分推断中的应用",
    "translated_abstract": "当变分推断（VI）使用因子化逼近时，它们往往会低估它们用来逼近的分布的不确定性，如以各种方式测量。我们考虑两种衡量VI不确定性亏损的流行方法：（i）它低估分量方差的程度，（ii）它低估熵的程度。为了更好地理解这些影响以及它们之间的关系，我们考虑了一个信息丰富的设置，可以在其中明确（和优雅地）分析这些影响：使用对角协方差矩阵的高斯（$q$）逼近具有密集协方差矩阵的高斯（$p$）。我们证明了$q$总是低估了$p$的分量方差和熵，尽管不一定低估的程度相同。此外，我们证明$q$的熵由两个相互竞争的因素的平衡决定：它的分量方差收缩会降低它的熵。",
    "tldr": "研究分析因子化高斯逼近在变分推断中的应用，发现该方法低估所逼近分布的不确定性。特别地，当用对角协方差矩阵的高斯逼近具有密集协方差矩阵的高斯时，所推断的高斯总是低估了原始高斯的分量方差和熵。",
    "en_tdlr": "This paper analyzes the use of factorized Gaussian approximations in variational inference and finds that this method underestimates the uncertainty of the distributions being approximated. Particularly, when a Gaussian with a diagonal covariance matrix is used to approximate another Gaussian with a dense covariance matrix, the former always underestimates the variance and entropy of the latter."
}