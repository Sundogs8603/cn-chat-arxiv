{
    "title": "Optimization Methods in Deep Learning: A Comprehensive Overview. (arXiv:2302.09566v2 [cs.LG] UPDATED)",
    "abstract": "In recent years, deep learning has achieved remarkable success in various fields such as image recognition, natural language processing, and speech recognition. The effectiveness of deep learning largely depends on the optimization methods used to train deep neural networks. In this paper, we provide an overview of first-order optimization methods such as Stochastic Gradient Descent, Adagrad, Adadelta, and RMSprop, as well as recent momentum-based and adaptive gradient methods such as Nesterov accelerated gradient, Adam, Nadam, AdaMax, and AMSGrad. We also discuss the challenges associated with optimization in deep learning and explore techniques for addressing these challenges, including weight initialization, batch normalization, and layer normalization. Finally, we provide recommendations for selecting optimization methods for different deep learning tasks and datasets. This paper serves as a comprehensive guide to optimization methods in deep learning and can be used as a reference",
    "link": "http://arxiv.org/abs/2302.09566",
    "context": "Title: Optimization Methods in Deep Learning: A Comprehensive Overview. (arXiv:2302.09566v2 [cs.LG] UPDATED)\nAbstract: In recent years, deep learning has achieved remarkable success in various fields such as image recognition, natural language processing, and speech recognition. The effectiveness of deep learning largely depends on the optimization methods used to train deep neural networks. In this paper, we provide an overview of first-order optimization methods such as Stochastic Gradient Descent, Adagrad, Adadelta, and RMSprop, as well as recent momentum-based and adaptive gradient methods such as Nesterov accelerated gradient, Adam, Nadam, AdaMax, and AMSGrad. We also discuss the challenges associated with optimization in deep learning and explore techniques for addressing these challenges, including weight initialization, batch normalization, and layer normalization. Finally, we provide recommendations for selecting optimization methods for different deep learning tasks and datasets. This paper serves as a comprehensive guide to optimization methods in deep learning and can be used as a reference",
    "path": "papers/23/02/2302.09566.json",
    "total_tokens": 911,
    "translated_title": "深度学习中的优化方法：综述",
    "translated_abstract": "近年来，深度学习在图像识别、自然语言处理和语音识别等领域取得了显著的成功。深度学习的有效性在很大程度上取决于用于训练深度神经网络的优化方法。本文概述了一阶优化方法（如随机梯度下降、Adagrad、Adadelta和RMSprop）以及最近发展的基于动量和自适应梯度的方法（如Nesterov加速梯度、Adam、Nadam、AdaMax和AMSGrad）。我们还讨论了深度学习中优化面临的挑战，并探讨了解决这些挑战的技术，包括权重初始化、批归一化和层归一化等。最后，我们提供了针对不同深度学习任务和数据集选择优化方法的建议。本文是深度学习中优化方法的综合指南，可以作为参考资料。",
    "tldr": "本文概括了深度学习中用于训练深度神经网络的一阶优化方法和最近的基于动量和自适应梯度的方法，讨论了优化面临的挑战，并探讨了解决这些挑战的技术，并提供了针对不同深度学习任务和数据集选择优化方法的建议。",
    "en_tdlr": "This paper provides an overview of first-order and recent momentum-based and adaptive gradient methods used for training deep neural networks in deep learning, discusses the challenges associated with optimization, explores techniques for addressing these challenges, and provides recommendations for selecting optimization methods for different deep learning tasks and datasets."
}