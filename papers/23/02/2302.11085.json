{
    "title": "Learning to Generalize Provably in Learning to Optimize. (arXiv:2302.11085v2 [cs.LG] UPDATED)",
    "abstract": "Learning to optimize (L2O) has gained increasing popularity, which automates the design of optimizers by data-driven approaches. However, current L2O methods often suffer from poor generalization performance in at least two folds: (i) applying the L2O-learned optimizer to unseen optimizees, in terms of lowering their loss function values (optimizer generalization, or ``generalizable learning of optimizers\"); and (ii) the test performance of an optimizee (itself as a machine learning model), trained by the optimizer, in terms of the accuracy over unseen data (optimizee generalization, or ``learning to generalize\"). While the optimizer generalization has been recently studied, the optimizee generalization (or learning to generalize) has not been rigorously studied in the L2O context, which is the aim of this paper. We first theoretically establish an implicit connection between the local entropy and the Hessian, and hence unify their roles in the handcrafted design of generalizable optim",
    "link": "http://arxiv.org/abs/2302.11085",
    "context": "Title: Learning to Generalize Provably in Learning to Optimize. (arXiv:2302.11085v2 [cs.LG] UPDATED)\nAbstract: Learning to optimize (L2O) has gained increasing popularity, which automates the design of optimizers by data-driven approaches. However, current L2O methods often suffer from poor generalization performance in at least two folds: (i) applying the L2O-learned optimizer to unseen optimizees, in terms of lowering their loss function values (optimizer generalization, or ``generalizable learning of optimizers\"); and (ii) the test performance of an optimizee (itself as a machine learning model), trained by the optimizer, in terms of the accuracy over unseen data (optimizee generalization, or ``learning to generalize\"). While the optimizer generalization has been recently studied, the optimizee generalization (or learning to generalize) has not been rigorously studied in the L2O context, which is the aim of this paper. We first theoretically establish an implicit connection between the local entropy and the Hessian, and hence unify their roles in the handcrafted design of generalizable optim",
    "path": "papers/23/02/2302.11085.json",
    "total_tokens": 1252,
    "translated_title": "在「学习优化」中学习一般化的保证（Learning to Generalize Provably in Learning to Optimize）",
    "translated_abstract": "「学习优化」（Learning to optimize，L2O）已经变得越来越流行，通过数据驱动的方法自动化设计优化器。然而，当前的 L2O 方法在至少两个方面表现不佳：（i）将 L2O 设计的优化器应用于未见过的优化对象时降低其损失函数值（优化器一般化或“可泛化的优化器学习”）；以及（ii）由优化器训练的优化对象（本身为机器学习模型）在准确性上面对未见过的数据表现的测试性能（优化对象一般化或“学习一般化”）。虽然优化器一般化最近已被研究，但优化对象一般化在 L2O 上尚未得到严格研究，这是本文的目的。我们首先理论上建立了局部熵与 Hessian 之间的隐式联系，从而统一了它们在通用优化技术的手工设计中的作用。基于这种联系，我们提出了一个统一的 “数据增强框架\" 来学习 L2O 的一般化实现。我们的框架由两部分组成：一部分是 GO 模块，通过随机梯度下降方法最大化通用化目标学习一个通用化算法；另一部分是优化对象模块，通过可微分优化器和标准交叉熵损失函数学习机器学习模型。我们的框架可以轻松与现有的 L2O 方法相结合，并在优化器和优化对象的一般化性能方面优于现有的最优方法。",
    "tldr": "本文提出了一种统一的数据增强框架，用于学习到可以泛化地优化器和优化对象。该框架可以轻松地与现有的 L2O 方法相结合，并在优化器和优化对象的一般化性能方面优于现有的最优方法。",
    "en_tdlr": "This paper proposes a unified data-augmented framework for learning to generalize in learning to optimize (L2O), which can easily combine with existing L2O methods and outperforms the state-of-the-art methods in terms of generalization performance for both optimizer and optimizee."
}