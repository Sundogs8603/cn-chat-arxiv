{
    "title": "Eliciting User Preferences for Personalized Multi-Objective Decision Making through Comparative Feedback. (arXiv:2302.03805v2 [cs.LG] UPDATED)",
    "abstract": "In classic reinforcement learning (RL) and decision making problems, policies are evaluated with respect to a scalar reward function, and all optimal policies are the same with regards to their expected return. However, many real-world problems involve balancing multiple, sometimes conflicting, objectives whose relative priority will vary according to the preferences of each user. Consequently, a policy that is optimal for one user might be sub-optimal for another. In this work, we propose a multi-objective decision making framework that accommodates different user preferences over objectives, where preferences are learned via policy comparisons. Our model consists of a Markov decision process with a vector-valued reward function, with each user having an unknown preference vector that expresses the relative importance of each objective. The goal is to efficiently compute a near-optimal policy for a given user. We consider two user feedback models. We first address the case where a use",
    "link": "http://arxiv.org/abs/2302.03805",
    "context": "Title: Eliciting User Preferences for Personalized Multi-Objective Decision Making through Comparative Feedback. (arXiv:2302.03805v2 [cs.LG] UPDATED)\nAbstract: In classic reinforcement learning (RL) and decision making problems, policies are evaluated with respect to a scalar reward function, and all optimal policies are the same with regards to their expected return. However, many real-world problems involve balancing multiple, sometimes conflicting, objectives whose relative priority will vary according to the preferences of each user. Consequently, a policy that is optimal for one user might be sub-optimal for another. In this work, we propose a multi-objective decision making framework that accommodates different user preferences over objectives, where preferences are learned via policy comparisons. Our model consists of a Markov decision process with a vector-valued reward function, with each user having an unknown preference vector that expresses the relative importance of each objective. The goal is to efficiently compute a near-optimal policy for a given user. We consider two user feedback models. We first address the case where a use",
    "path": "papers/23/02/2302.03805.json",
    "total_tokens": 866,
    "translated_title": "通过比较反馈引导个性化多目标决策中的用户偏好",
    "translated_abstract": "在经典的强化学习和决策问题中，政策是根据标量奖励函数进行评估的，而所有最优政策在预期回报方面是相同的。然而，许多现实世界的问题涉及到平衡多个、有时是冲突的目标，这些目标的相对优先级会根据每个用户的偏好而变化。因此，对一个用户而言最优的政策可能对另一个用户而言是次优的。在这项工作中，我们提出了一个多目标决策框架，以适应不同用户对目标的偏好，其中偏好是通过政策比较来学习的。我们的模型由一个具有向量值奖励函数的马尔可夫决策过程组成，每个用户都有一个未知的偏好向量，表示每个目标的相对重要性。目标是高效地计算出给定用户的近似最优政策。我们考虑两种用户反馈模型。",
    "tldr": "这项研究提出了一个多目标决策框架，通过比较不同用户的政策来学习用户对目标的偏好，并根据偏好计算出近似最优的个性化政策。"
}