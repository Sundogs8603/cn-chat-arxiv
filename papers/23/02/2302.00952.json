{
    "title": "QR-CLIP: Introducing Explicit Open-World Knowledge for Location and Time Reasoning. (arXiv:2302.00952v2 [cs.CV] UPDATED)",
    "abstract": "Daily images may convey abstract meanings that require us to memorize and infer profound information from them. To encourage such human-like reasoning, in this work, we teach machines to predict where and when it was taken rather than performing basic tasks like traditional segmentation or classification. Inspired by Horn's QR theory, we designed a novel QR-CLIP model consisting of two components: 1) the Quantity module first retrospects more open-world knowledge as the candidate language inputs; 2) the Relevance module carefully estimates vision and language cues and infers the location and time. Experiments show our QR-CLIP's effectiveness, and it outperforms the previous SOTA on each task by an average of about 10% and 130% relative lift in terms of location and time reasoning. This study lays a technical foundation for location and time reasoning and suggests that effectively introducing open-world knowledge is one of the panaceas for the tasks.",
    "link": "http://arxiv.org/abs/2302.00952",
    "context": "Title: QR-CLIP: Introducing Explicit Open-World Knowledge for Location and Time Reasoning. (arXiv:2302.00952v2 [cs.CV] UPDATED)\nAbstract: Daily images may convey abstract meanings that require us to memorize and infer profound information from them. To encourage such human-like reasoning, in this work, we teach machines to predict where and when it was taken rather than performing basic tasks like traditional segmentation or classification. Inspired by Horn's QR theory, we designed a novel QR-CLIP model consisting of two components: 1) the Quantity module first retrospects more open-world knowledge as the candidate language inputs; 2) the Relevance module carefully estimates vision and language cues and infers the location and time. Experiments show our QR-CLIP's effectiveness, and it outperforms the previous SOTA on each task by an average of about 10% and 130% relative lift in terms of location and time reasoning. This study lays a technical foundation for location and time reasoning and suggests that effectively introducing open-world knowledge is one of the panaceas for the tasks.",
    "path": "papers/23/02/2302.00952.json",
    "total_tokens": 858,
    "translated_title": "QR-CLIP: 引入显式开放世界知识进行位置和时间推理",
    "translated_abstract": "每天的图像可能传达需要我们从中记忆和推断出深刻信息的抽象含义。在本文中，我们教会机器预测图片拍摄的地点和时间，而不是执行传统的分割或分类任务，以鼓励人类类似的推理。受到Horn的QR理论的启发，我们设计了一个由两个部分组成的新型QR-CLIP模型: 1) 数量模块首先回顾更多的开放世界知识作为候选的语言输入; 2) 相关性模块仔细估计视觉和语言线索，并推断出位置和时间。实验显示，我们的QR-CLIP十分有效，并且在每个任务上都比之前的最高水平表现平均提升了约10%和130%的相对提升。本研究为位置和时间推理奠定了技术基础，并表明有效引入开放世界知识是完成这些任务的方法之一。",
    "tldr": "本文提出QR-CLIP模型，通过引入开放世界知识进行位置和时间推理，在此任务上取得了约10%和130%的相对提升。",
    "en_tdlr": "This paper proposes the QR-CLIP model, which introduces open-world knowledge for location and time reasoning and achieves an average relative improvement of about 10% and 130% on each task."
}