{
    "title": "Computational Complexity of Learning Neural Networks: Smoothness and Degeneracy. (arXiv:2302.07426v2 [cs.LG] UPDATED)",
    "abstract": "Understanding when neural networks can be learned efficiently is a fundamental question in learning theory. Existing hardness results suggest that assumptions on both the input distribution and the network's weights are necessary for obtaining efficient algorithms. Moreover, it was previously shown that depth-$2$ networks can be efficiently learned under the assumptions that the input distribution is Gaussian, and the weight matrix is non-degenerate. In this work, we study whether such assumptions may suffice for learning deeper networks and prove negative results. We show that learning depth-$3$ ReLU networks under the Gaussian input distribution is hard even in the smoothed-analysis framework, where a random noise is added to the network's parameters. It implies that learning depth-$3$ ReLU networks under the Gaussian distribution is hard even if the weight matrices are non-degenerate. Moreover, we consider depth-$2$ networks, and show hardness of learning in the smoothed-analysis fr",
    "link": "http://arxiv.org/abs/2302.07426",
    "context": "Title: Computational Complexity of Learning Neural Networks: Smoothness and Degeneracy. (arXiv:2302.07426v2 [cs.LG] UPDATED)\nAbstract: Understanding when neural networks can be learned efficiently is a fundamental question in learning theory. Existing hardness results suggest that assumptions on both the input distribution and the network's weights are necessary for obtaining efficient algorithms. Moreover, it was previously shown that depth-$2$ networks can be efficiently learned under the assumptions that the input distribution is Gaussian, and the weight matrix is non-degenerate. In this work, we study whether such assumptions may suffice for learning deeper networks and prove negative results. We show that learning depth-$3$ ReLU networks under the Gaussian input distribution is hard even in the smoothed-analysis framework, where a random noise is added to the network's parameters. It implies that learning depth-$3$ ReLU networks under the Gaussian distribution is hard even if the weight matrices are non-degenerate. Moreover, we consider depth-$2$ networks, and show hardness of learning in the smoothed-analysis fr",
    "path": "papers/23/02/2302.07426.json",
    "total_tokens": 971,
    "translated_title": "学习神经网络的计算复杂度: 光滑性和退化性",
    "translated_abstract": "理解神经网络何时可以被有效学习是学习理论中的一个基本问题。已有的难度结果表明，对输入分布和网络权重都需要做出一定的假设才能得到有效的算法。此前的研究已经表明，假设输入分布为高斯分布且权重矩阵非退化时，可以有效地学习深度为2的网络。本文研究了这些假设是否适用于学习更深的网络，并给出了否定的结论。我们证明，在光滑分析框架下，即在网络参数中加入随机噪声的情况下，学习深度为3的ReLU网络在高斯输入分布下是困难的。这意味着，即使权重矩阵是非退化的，学习深度为3的ReLU网络在高斯分布下也是困难的。此外，我们还考虑了深度为2的网络，并展示了在光滑分析框架下学习的困难性。",
    "tldr": "本文研究了学习神经网络的计算复杂度，特别关注了输入分布和权重矩阵的假设对学习算法有效性的影响。结果表明，在高斯输入分布下，学习深度为3的ReLU网络是困难的，即使权重矩阵是非退化的。同时，学习深度为2的网络也面临困难。"
}