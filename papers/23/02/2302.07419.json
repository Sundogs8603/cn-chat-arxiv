{
    "title": "Spatially heterogeneous learning by a deep student machine. (arXiv:2302.07419v3 [cond-mat.dis-nn] UPDATED)",
    "abstract": "Despite the spectacular successes, deep neural networks (DNN) with a huge number of adjustable parameters remain largely black boxes. To shed light on the hidden layers of DNN, we study supervised learning by a DNN of width $N$ and depth $L$ consisting of perceptrons with $c$ inputs by a statistical mechanics approach called the teacher-student setting. We consider an ensemble of student machines that exactly reproduce $M$ sets of $N$ dimensional input/output relations provided by a teacher machine. We analyze the ensemble theoretically using a replica method (H. Yoshino (2020)) and numerically performing greedy Monte Carlo simulations. The replica theory which works on high dimensional data $N \\gg 1$ becomes exact in 'dense limit' $N \\gg c \\gg 1$ and $M \\gg 1$ with fixed $\\alpha=M/c$. Both the theory and the simulation suggest learning by the DNN is quite heterogeneous in the network space: configurations of the machines are more correlated within the layers closer to the input/output",
    "link": "http://arxiv.org/abs/2302.07419",
    "context": "Title: Spatially heterogeneous learning by a deep student machine. (arXiv:2302.07419v3 [cond-mat.dis-nn] UPDATED)\nAbstract: Despite the spectacular successes, deep neural networks (DNN) with a huge number of adjustable parameters remain largely black boxes. To shed light on the hidden layers of DNN, we study supervised learning by a DNN of width $N$ and depth $L$ consisting of perceptrons with $c$ inputs by a statistical mechanics approach called the teacher-student setting. We consider an ensemble of student machines that exactly reproduce $M$ sets of $N$ dimensional input/output relations provided by a teacher machine. We analyze the ensemble theoretically using a replica method (H. Yoshino (2020)) and numerically performing greedy Monte Carlo simulations. The replica theory which works on high dimensional data $N \\gg 1$ becomes exact in 'dense limit' $N \\gg c \\gg 1$ and $M \\gg 1$ with fixed $\\alpha=M/c$. Both the theory and the simulation suggest learning by the DNN is quite heterogeneous in the network space: configurations of the machines are more correlated within the layers closer to the input/output",
    "path": "papers/23/02/2302.07419.json",
    "total_tokens": 940,
    "translated_title": "通过深度学生机器实现空间异质性学习",
    "translated_abstract": "尽管深度神经网络（DNN）取得了非凡的成功，但由于具有大量可调参数，其仍然是黑匣子。为了研究DNN的隐藏层，本文通过一种统计力学方法称为教师-学生设置，研究了由宽度为N，深度为L，由具有c个输入的感知机组成的DNN的监督学习。我们考虑了一个学生机器的集合，该集合可以精确重现由教师机器提供的M组N维输入/输出关系。我们使用副本方法（H. Yoshino（2020））理论分析了集合，并进行了贪婪的Monte Carlo模拟。对于高维数据$N \\gg 1$，理论在'密集极限' $N \\gg c \\gg 1$ 和 $M \\gg 1$ 且固定$\\alpha=M/c$时变得精确。理论和模拟都表明，DNN的学习在网络空间中相当异质：机器的配置在靠近输入/输出的层内更加相关。",
    "tldr": "本论文研究了一种深度学生机器的教师-学生设置，通过学生机器的集合来研究由具有大量可调参数的DNN的监督学习。研究表明DNN的学习在网络空间中相当异质。",
    "en_tdlr": "This paper investigates supervised learning by a DNN with a huge number of adjustable parameters using a statistic mechanics approach called the teacher-student setting. Results show that the learning by the DNN is quite heterogeneous in the network space."
}