{
    "title": "Domain-Specific Pre-training Improves Confidence in Whole Slide Image Classification. (arXiv:2302.09833v2 [cs.CV] UPDATED)",
    "abstract": "Whole Slide Images (WSIs) or histopathology images are used in digital pathology. WSIs pose great challenges to deep learning models for clinical diagnosis, owing to their size and lack of pixel-level annotations. With the recent advancements in computational pathology, newer multiple-instance learning-based models have been proposed. Multiple-instance learning for WSIs necessitates creating patches and uses the encoding of these patches for diagnosis. These models use generic pre-trained models (ResNet-50 pre-trained on ImageNet) for patch encoding. The recently proposed KimiaNet, a DenseNet121 model pre-trained on TCGA slides, is a domain-specific pre-trained model. This paper shows the effect of domain-specific pre-training on WSI classification. To investigate the effect of domain-specific pre-training, we considered the current state-of-the-art multiple-instance learning models, 1) CLAM, an attention-based model, and 2) TransMIL, a self-attention-based model, and evaluated the mod",
    "link": "http://arxiv.org/abs/2302.09833",
    "context": "Title: Domain-Specific Pre-training Improves Confidence in Whole Slide Image Classification. (arXiv:2302.09833v2 [cs.CV] UPDATED)\nAbstract: Whole Slide Images (WSIs) or histopathology images are used in digital pathology. WSIs pose great challenges to deep learning models for clinical diagnosis, owing to their size and lack of pixel-level annotations. With the recent advancements in computational pathology, newer multiple-instance learning-based models have been proposed. Multiple-instance learning for WSIs necessitates creating patches and uses the encoding of these patches for diagnosis. These models use generic pre-trained models (ResNet-50 pre-trained on ImageNet) for patch encoding. The recently proposed KimiaNet, a DenseNet121 model pre-trained on TCGA slides, is a domain-specific pre-trained model. This paper shows the effect of domain-specific pre-training on WSI classification. To investigate the effect of domain-specific pre-training, we considered the current state-of-the-art multiple-instance learning models, 1) CLAM, an attention-based model, and 2) TransMIL, a self-attention-based model, and evaluated the mod",
    "path": "papers/23/02/2302.09833.json",
    "total_tokens": 940,
    "translated_title": "面向领域的预训练提高了整张切片图像分类的信心",
    "translated_abstract": "整张切片图像或组织病理学图像在数字病理学中被广泛使用。由于其大小和缺乏像素级注释，整张切片图像对于临床诊断的深度学习模型构成了巨大挑战。最近在计算病理学领域，提出了基于多实例学习的新模型。多实例学习需要创建补丁，并使用这些补丁的编码进行诊断。这些模型使用通用的预训练模型（在ImageNet上预训练的ResNet-50）进行补丁编码。最近提出的KimiaNet是一种基于DenseNet121的面向领域的预训练模型，该模型是在TCGA切片上进行预训练的。本文展示了面向领域的预训练对整张切片图像分类的影响。为了调查面向领域的预训练的效果，我们考虑了当前最先进的多实例学习模型：1）基于注意力的CLAM模型和2）自注意的TransMIL模型，并评估了模型的性能。",
    "tldr": "本文研究了面向领域的预训练对整张切片图像分类的作用，发现使用面向领域的预训练可以提高多实例学习模型在切片图像分类任务中的性能。"
}