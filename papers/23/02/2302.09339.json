{
    "title": "Efficient Exploration via Epistemic-Risk-Seeking Policy Optimization. (arXiv:2302.09339v2 [cs.LG] UPDATED)",
    "abstract": "Exploration remains a key challenge in deep reinforcement learning (RL). Optimism in the face of uncertainty is a well-known heuristic with theoretical guarantees in the tabular setting, but how best to translate the principle to deep reinforcement learning, which involves online stochastic gradients and deep network function approximators, is not fully understood. In this paper we propose a new, differentiable optimistic objective that when optimized yields a policy that provably explores efficiently, with guarantees even under function approximation. Our new objective is a zero-sum two-player game derived from endowing the agent with an epistemic-risk-seeking utility function, which converts uncertainty into value and encourages the agent to explore uncertain states. We show that the solution to this game minimizes an upper bound on the regret, with the 'players' each attempting to minimize one component of a particular regret decomposition. We derive a new model-free algorithm which",
    "link": "http://arxiv.org/abs/2302.09339",
    "context": "Title: Efficient Exploration via Epistemic-Risk-Seeking Policy Optimization. (arXiv:2302.09339v2 [cs.LG] UPDATED)\nAbstract: Exploration remains a key challenge in deep reinforcement learning (RL). Optimism in the face of uncertainty is a well-known heuristic with theoretical guarantees in the tabular setting, but how best to translate the principle to deep reinforcement learning, which involves online stochastic gradients and deep network function approximators, is not fully understood. In this paper we propose a new, differentiable optimistic objective that when optimized yields a policy that provably explores efficiently, with guarantees even under function approximation. Our new objective is a zero-sum two-player game derived from endowing the agent with an epistemic-risk-seeking utility function, which converts uncertainty into value and encourages the agent to explore uncertain states. We show that the solution to this game minimizes an upper bound on the regret, with the 'players' each attempting to minimize one component of a particular regret decomposition. We derive a new model-free algorithm which",
    "path": "papers/23/02/2302.09339.json",
    "total_tokens": 903,
    "translated_title": "通过认知风险导向策略优化的高效探索",
    "translated_abstract": "在深度强化学习中，探索仍然是一个关键的挑战。在表格设置中，乐观主义是一种众所周知的启发式方法，具有理论保证，但如何将该原则最好地转化到涉及在线随机梯度和深度网络函数逼近器的深度强化学习中，尚未充分理解。本文提出了一种新的可微乐观目标，当优化时，产生一种可证明有效探索的策略，即使在函数逼近下也具有保证。我们的新目标是一种零和二人博弈，源于赋予代理一个认知风险导向效用函数，将不确定性转化为价值，并鼓励代理人探索不确定状态。我们证明了这个游戏的解决方案最小化了悔恨的一个上界，其中“玩家”各自尝试最小化特定悔恨分解的一个组成部分。我们推导了一种新的无模型算法",
    "tldr": "本文提出了一种基于认知风险的新型目标函数，将不确定性转化为价值，鼓励智能体探索未知领域。该方法可以在深度强化学中实现高效探索，即使在函数逼近下也具有保证。",
    "en_tdlr": "This paper proposes a novel objective function based on epistemic risk, which converts uncertainty into value and encourages exploration of unknown states in deep reinforcement learning. The method provides efficient exploration in deep reinforcement learning, even under function approximation."
}