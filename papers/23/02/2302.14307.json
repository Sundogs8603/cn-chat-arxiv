{
    "title": "GradMA: A Gradient-Memory-based Accelerated Federated Learning with Alleviated Catastrophic Forgetting. (arXiv:2302.14307v3 [cs.CV] UPDATED)",
    "abstract": "Federated Learning (FL) has emerged as a de facto machine learning area and received rapid increasing research interests from the community. However, catastrophic forgetting caused by data heterogeneity and partial participation poses distinctive challenges for FL, which are detrimental to the performance. To tackle the problems, we propose a new FL approach (namely GradMA), which takes inspiration from continual learning to simultaneously correct the server-side and worker-side update directions as well as take full advantage of server's rich computing and memory resources. Furthermore, we elaborate a memory reduction strategy to enable GradMA to accommodate FL with a large scale of workers. We then analyze convergence of GradMA theoretically under the smooth non-convex setting and show that its convergence rate achieves a linear speed up w.r.t the increasing number of sampled active workers. At last, our extensive experiments on various image classification tasks show that GradMA ach",
    "link": "http://arxiv.org/abs/2302.14307",
    "context": "Title: GradMA: A Gradient-Memory-based Accelerated Federated Learning with Alleviated Catastrophic Forgetting. (arXiv:2302.14307v3 [cs.CV] UPDATED)\nAbstract: Federated Learning (FL) has emerged as a de facto machine learning area and received rapid increasing research interests from the community. However, catastrophic forgetting caused by data heterogeneity and partial participation poses distinctive challenges for FL, which are detrimental to the performance. To tackle the problems, we propose a new FL approach (namely GradMA), which takes inspiration from continual learning to simultaneously correct the server-side and worker-side update directions as well as take full advantage of server's rich computing and memory resources. Furthermore, we elaborate a memory reduction strategy to enable GradMA to accommodate FL with a large scale of workers. We then analyze convergence of GradMA theoretically under the smooth non-convex setting and show that its convergence rate achieves a linear speed up w.r.t the increasing number of sampled active workers. At last, our extensive experiments on various image classification tasks show that GradMA ach",
    "path": "papers/23/02/2302.14307.json",
    "total_tokens": 1008,
    "translated_title": "基于梯度内存的加速联邦学习方法GradMA并缓解灾难性遗忘问题",
    "translated_abstract": "联邦学习是一种新兴的机器学习领域，受到了社区的广泛关注。然而，因数据异质性和部分参与导致的灾难性遗忘给联邦学习带来了巨大的挑战，影响了性能。为了解决这些问题，我们提出了一种新的联邦学习方法（名为GradMA），它从持续学习中获得启示，同时纠正服务器端和工作端的更新方向，并充分利用服务器的丰富计算和内存资源。此外，我们还阐述了一种内存缩减策略，使GradMA能够适应具有大量工作者的联邦学习。然后，我们在光滑非凸环境下理论分析了GradMA的收敛性，并展示了其收敛速度随着采样活跃工作者数量的增加而实现线性加速。最后，我们在各种图像分类任务上进行了广泛的实验，结果显示，与几种最先进的联邦学习方法相比，GradMA在模型准确性、通信效率和灾难性遗忘缓解方面都取得了显著的改进。",
    "tldr": "提出了一种名为GradMA的方法，基于梯度内存加速联邦学习，并通过持续学习来解决灾难性遗忘问题，实验表明在模型准确性、通信效率和灾难性遗忘缓解方面都有显著提高。",
    "en_tdlr": "GradMA, a gradient-memory-based accelerated federated learning method, was proposed to address the catastrophic forgetting issue and achieved significant improvement in model accuracy, communication efficiency and catastrophic forgetting alleviation based on continual learning."
}