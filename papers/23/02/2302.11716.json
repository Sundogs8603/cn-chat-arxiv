{
    "title": "VRA: Variational Rectified Activation for Out-of-distribution Detection. (arXiv:2302.11716v4 [cs.LG] UPDATED)",
    "abstract": "Out-of-distribution (OOD) detection is critical to building reliable machine learning systems in the open world. Researchers have proposed various strategies to reduce model overconfidence on OOD data. Among them, ReAct is a typical and effective technique to deal with model overconfidence, which truncates high activations to increase the gap between in-distribution and OOD. Despite its promising results, is this technique the best choice for widening the gap? To answer this question, we leverage the variational method to find the optimal operation and verify the necessity of suppressing abnormally low and high activations and amplifying intermediate activations in OOD detection, rather than focusing only on high activations like ReAct. This motivates us to propose a novel technique called ``Variational Rectified Activation (VRA)'', which simulates these suppression and amplification operations using piecewise functions. Experimental results on multiple benchmark datasets demonstrate t",
    "link": "http://arxiv.org/abs/2302.11716",
    "context": "Title: VRA: Variational Rectified Activation for Out-of-distribution Detection. (arXiv:2302.11716v4 [cs.LG] UPDATED)\nAbstract: Out-of-distribution (OOD) detection is critical to building reliable machine learning systems in the open world. Researchers have proposed various strategies to reduce model overconfidence on OOD data. Among them, ReAct is a typical and effective technique to deal with model overconfidence, which truncates high activations to increase the gap between in-distribution and OOD. Despite its promising results, is this technique the best choice for widening the gap? To answer this question, we leverage the variational method to find the optimal operation and verify the necessity of suppressing abnormally low and high activations and amplifying intermediate activations in OOD detection, rather than focusing only on high activations like ReAct. This motivates us to propose a novel technique called ``Variational Rectified Activation (VRA)'', which simulates these suppression and amplification operations using piecewise functions. Experimental results on multiple benchmark datasets demonstrate t",
    "path": "papers/23/02/2302.11716.json",
    "total_tokens": 927,
    "translated_abstract": "越界检测在构建可靠的机器学习系统中非常重要。研究人员提出了各种策略来降低模型对越界数据的过度自信。其中，ReAct是一种典型且有效的处理模型过度自信的技术，它截断高激活值以增加内部数据和越界数据之间的差距。尽管该技术结果很好，但对于扩大差距，它是否是最佳选择呢？为回答这个问题，我们利用变分方法寻找最优操作并验证抑制异常低和高激活值以及放大中间激活值在越界检测中的必要性，而不仅仅像ReAct一样只关注高激活值。这启发我们提出了一种新的技术称为“变分修正激活函数 (VRA)”，它使用分段函数模拟这些抑制和放大操作。在多个基准数据集上的实验结果证明了VRA在越界检测中的有效性。",
    "tldr": "本论文提出了一种新的越界检测技术，称为“变分修正激活函数 (VRA)”，该方法不仅关注高激活值，还抑制异常低和高激活值以及放大中间激活值，实验结果证明其有效性。",
    "en_tdlr": "This paper proposes a new out-of-distribution detection technique, called \"Variational Rectified Activation (VRA)\", which not only focuses on high activations but also suppresses abnormally low and high activations and amplifies intermediate activations, and experimental results demonstrate its effectiveness on multiple benchmark datasets."
}