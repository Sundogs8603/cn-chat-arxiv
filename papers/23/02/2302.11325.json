{
    "title": "Video-SwinUNet: Spatio-temporal Deep Learning Framework for VFSS Instance Segmentation. (arXiv:2302.11325v2 [cs.CV] UPDATED)",
    "abstract": "This paper presents a deep learning framework for medical video segmentation. Convolution neural network (CNN) and transformer-based methods have achieved great milestones in medical image segmentation tasks due to their incredible semantic feature encoding and global information comprehension abilities. However, most existing approaches ignore a salient aspect of medical video data - the temporal dimension. Our proposed framework explicitly extracts features from neighbouring frames across the temporal dimension and incorporates them with a temporal feature blender, which then tokenises the high-level spatio-temporal feature to form a strong global feature encoded via a Swin Transformer. The final segmentation results are produced via a UNet-like encoder-decoder architecture. Our model outperforms other approaches by a significant margin and improves the segmentation benchmarks on the VFSS2022 dataset, achieving a dice coefficient of 0.8986 and 0.8186 for the two datasets tested. Our ",
    "link": "http://arxiv.org/abs/2302.11325",
    "context": "Title: Video-SwinUNet: Spatio-temporal Deep Learning Framework for VFSS Instance Segmentation. (arXiv:2302.11325v2 [cs.CV] UPDATED)\nAbstract: This paper presents a deep learning framework for medical video segmentation. Convolution neural network (CNN) and transformer-based methods have achieved great milestones in medical image segmentation tasks due to their incredible semantic feature encoding and global information comprehension abilities. However, most existing approaches ignore a salient aspect of medical video data - the temporal dimension. Our proposed framework explicitly extracts features from neighbouring frames across the temporal dimension and incorporates them with a temporal feature blender, which then tokenises the high-level spatio-temporal feature to form a strong global feature encoded via a Swin Transformer. The final segmentation results are produced via a UNet-like encoder-decoder architecture. Our model outperforms other approaches by a significant margin and improves the segmentation benchmarks on the VFSS2022 dataset, achieving a dice coefficient of 0.8986 and 0.8186 for the two datasets tested. Our ",
    "path": "papers/23/02/2302.11325.json",
    "total_tokens": 863,
    "translated_title": "Video-SwinUNet：VFSS 实例分割的时空深度学习框架",
    "translated_abstract": "本文提出了一种用于医学视频分割的深度学习框架。由于其具有令人难以置信的语义特征编码和全局信息理解能力，卷积神经网络（CNN）和基于Transformer的方法在医学图像分割任务中取得了巨大的里程碑。然而，大多数现有方法忽略了医学视频数据的一个显著方面 - 时间维度。我们提出的框架明确地从时间维度上的相邻帧中提取特征，并将其与一个时间特征混合器结合起来，然后使用Swin Transformer对高水平的时空特征进行标记。最终的分割结果通过一个类似UNet的编码器-解码器架构生成。我们的模型在VFSS2022数据集上表现出色，较其他方法提升显著，对于两个测试数据集的Dice系数分别达到了0.8986和0.8186。",
    "tldr": "本文提出了一个用于医学视频分割的深度学习框架，通过显式地利用时间维度并结合Swin Transformer的全局特征编码能力，改进了医学图像分割任务的性能。",
    "en_tdlr": "This paper presents a deep learning framework for medical video segmentation that improves the performance of medical image segmentation tasks by explicitly utilizing the temporal dimension and global feature encoding capability of Swin Transformer."
}