{
    "title": "Provably Bounding Neural Network Preimages",
    "abstract": "arXiv:2302.01404v4 Announce Type: replace-cross  Abstract: Most work on the formal verification of neural networks has focused on bounding the set of outputs that correspond to a given set of inputs (for example, bounded perturbations of a nominal input). However, many use cases of neural network verification require solving the inverse problem, or over-approximating the set of inputs that lead to certain outputs. We present the INVPROP algorithm for verifying properties over the preimage of a linearly constrained output set, which can be combined with branch-and-bound to increase precision. Contrary to other approaches, our efficient algorithm is GPU-accelerated and does not require a linear programming solver. We demonstrate our algorithm for identifying safe control regions for a dynamical system via backward reachability analysis, verifying adversarial robustness, and detecting out-of-distribution inputs to a neural network. Our results show that in certain settings, we find over-a",
    "link": "https://arxiv.org/abs/2302.01404",
    "context": "Title: Provably Bounding Neural Network Preimages\nAbstract: arXiv:2302.01404v4 Announce Type: replace-cross  Abstract: Most work on the formal verification of neural networks has focused on bounding the set of outputs that correspond to a given set of inputs (for example, bounded perturbations of a nominal input). However, many use cases of neural network verification require solving the inverse problem, or over-approximating the set of inputs that lead to certain outputs. We present the INVPROP algorithm for verifying properties over the preimage of a linearly constrained output set, which can be combined with branch-and-bound to increase precision. Contrary to other approaches, our efficient algorithm is GPU-accelerated and does not require a linear programming solver. We demonstrate our algorithm for identifying safe control regions for a dynamical system via backward reachability analysis, verifying adversarial robustness, and detecting out-of-distribution inputs to a neural network. Our results show that in certain settings, we find over-a",
    "path": "papers/23/02/2302.01404.json",
    "total_tokens": 794,
    "translated_title": "可证明边界神经网络前像",
    "translated_abstract": "大部分关于神经网络的形式验证工作侧重于限定给定输入集对应的输出集（例如，标准输入的有界扰动）。但是，神经网络验证的许多应用情景需要解决逆问题，或者对导致特定输出的输入集进行过度近似。我们提出了INVPROP算法，用于验证在线性约束输出集的前像上的属性，可以与分支界限结合以增加精度。与其他方法相反，我们的高效算法是GPU加速的，并且不需要线性规划求解器。我们展示了我们的算法用于通过后向可达性分析识别动态系统的安全控制区域，验证对抗鲁棒性，并检测神经网络的超出分布的输入。我们的结果表明，在某些情况下，我们找到了过渡",
    "tldr": "提出了INVPROP算法用于验证神经网络输出集的前像上的属性，结合分支界限以增加精度，并且实现了GPU加速，避免了线性规划求解器的需求。",
    "en_tdlr": "Proposed INVPROP algorithm for verifying properties over the preimage of neural network output set, combined with branch-and-bound for increased precision, GPU-accelerated and avoiding the need for a linear programming solver."
}