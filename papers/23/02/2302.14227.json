{
    "title": "A unified scalable framework for causal sweeping strategies for Physics-Informed Neural Networks (PINNs) and their temporal decompositions. (arXiv:2302.14227v2 [physics.comp-ph] UPDATED)",
    "abstract": "Physics-informed neural networks (PINNs) as a means of solving partial differential equations (PDE) have garnered much attention in the Computational Science and Engineering (CS&E) world. However, a recent topic of interest is exploring various training (i.e., optimization) challenges - in particular, arriving at poor local minima in the optimization landscape results in a PINN approximation giving an inferior, and sometimes trivial, solution when solving forward time-dependent PDEs with no data. This problem is also found in, and in some sense more difficult, with domain decomposition strategies such as temporal decomposition using XPINNs. We furnish examples and explanations for different training challenges, their cause, and how they relate to information propagation and temporal decomposition. We then propose a new stacked-decomposition method that bridges the gap between time-marching PINNs and XPINNs. We also introduce significant computational speed-ups by using transfer learnin",
    "link": "http://arxiv.org/abs/2302.14227",
    "context": "Title: A unified scalable framework for causal sweeping strategies for Physics-Informed Neural Networks (PINNs) and their temporal decompositions. (arXiv:2302.14227v2 [physics.comp-ph] UPDATED)\nAbstract: Physics-informed neural networks (PINNs) as a means of solving partial differential equations (PDE) have garnered much attention in the Computational Science and Engineering (CS&E) world. However, a recent topic of interest is exploring various training (i.e., optimization) challenges - in particular, arriving at poor local minima in the optimization landscape results in a PINN approximation giving an inferior, and sometimes trivial, solution when solving forward time-dependent PDEs with no data. This problem is also found in, and in some sense more difficult, with domain decomposition strategies such as temporal decomposition using XPINNs. We furnish examples and explanations for different training challenges, their cause, and how they relate to information propagation and temporal decomposition. We then propose a new stacked-decomposition method that bridges the gap between time-marching PINNs and XPINNs. We also introduce significant computational speed-ups by using transfer learnin",
    "path": "papers/23/02/2302.14227.json",
    "total_tokens": 917,
    "translated_title": "物理信息神经网络（PINNs）和它们的时间分解的因果扫描策略的统一可扩展框架",
    "translated_abstract": "物理信息神经网络（PINNs）作为求解偏微分方程（PDE）的一种方法，在计算科学和工程（CS&E）领域引起了广泛的关注。然而，近期一个有趣的话题是探索各种训练（即优化）挑战 - 特别是在优化景观中陷入了差的局部最小点时，导致了一个PDE近似给出了一个较差甚至是微不足道的解，而且不需要数据以前进的时间相关PDE的求解时，这个问题还存在于一些领域分解策略中，如使用XPINNs的时间分解。我们提供了不同训练挑战的示例和解释，以及它们的原因，以及它们如何与信息传播和时间分解有关。然后，我们提出了一种新的堆叠分解方法，以弥合时间步进PINNs和XPINNs之间的差距。我们还通过使用迁移学习引入了显著的计算速度提升。",
    "tldr": "这篇论文提出了一个统一可扩展框架，用于解决物理信息神经网络（PINNs）在优化过程中出现的挑战，并介绍了一种新的堆叠分解方法来加快计算速度。"
}