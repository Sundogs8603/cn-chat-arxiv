{
    "title": "Gaussian Process-Gated Hierarchical Mixtures of Experts",
    "abstract": "arXiv:2302.04947v2 Announce Type: replace  Abstract: In this paper, we propose novel Gaussian process-gated hierarchical mixtures of experts (GPHMEs). Unlike other mixtures of experts with gating models linear in the input, our model employs gating functions built with Gaussian processes (GPs). These processes are based on random features that are non-linear functions of the inputs. Furthermore, the experts in our model are also constructed with GPs. The optimization of the GPHMEs is performed by variational inference. The proposed GPHMEs have several advantages. They outperform tree-based HME benchmarks that partition the data in the input space, and they achieve good performance with reduced complexity. Another advantage is the interpretability they provide for deep GPs, and more generally, for deep Bayesian neural networks. Our GPHMEs demonstrate excellent performance for large-scale data sets, even with quite modest sizes.",
    "link": "https://arxiv.org/abs/2302.04947",
    "context": "Title: Gaussian Process-Gated Hierarchical Mixtures of Experts\nAbstract: arXiv:2302.04947v2 Announce Type: replace  Abstract: In this paper, we propose novel Gaussian process-gated hierarchical mixtures of experts (GPHMEs). Unlike other mixtures of experts with gating models linear in the input, our model employs gating functions built with Gaussian processes (GPs). These processes are based on random features that are non-linear functions of the inputs. Furthermore, the experts in our model are also constructed with GPs. The optimization of the GPHMEs is performed by variational inference. The proposed GPHMEs have several advantages. They outperform tree-based HME benchmarks that partition the data in the input space, and they achieve good performance with reduced complexity. Another advantage is the interpretability they provide for deep GPs, and more generally, for deep Bayesian neural networks. Our GPHMEs demonstrate excellent performance for large-scale data sets, even with quite modest sizes.",
    "path": "papers/23/02/2302.04947.json",
    "total_tokens": 929,
    "translated_title": "高斯过程门控的分层专家混合模型",
    "translated_abstract": "在这篇论文中，我们提出了一种新颖的高斯过程门控的分层专家混合模型（Gaussian Process-Gated Hierarchical Mixtures of Experts，GPHMEs）。与其他采用输入线性门控模型的专家混合模型不同，我们的模型采用了基于高斯过程（GPs）构建的门控函数。这些过程基于输入的非线性函数的随机特征。此外，我们模型中的专家也是用GPs构建的。GPHMEs的优化通过变分推断来实现。所提出的GPHMEs具有几个优点。它们优于在输入空间中对数据进行分区的基于树的HME基准，并且能够在减少复杂性的同时实现良好的性能。另一个优点是它们为深层GPs以及更一般的深度贝叶斯神经网络提供的可解释性。我们的GPHMEs在大规模数据集上展现了出色的性能，即使数据规模相当适中也是如此。",
    "tldr": "该论文提出了一种新颖的高斯过程门控的分层专家混合模型，通过使用GPs构建门控函数和专家，优于传统基于树的模型，同时在复杂性较低的情况下表现出良好性能，还提供了深层GPs和深度贝叶斯神经网络的可解释性。",
    "en_tdlr": "The paper proposes a novel Gaussian process-gated hierarchical mixtures of experts (GPHMEs) model, which outperforms traditional tree-based models, achieves good performance with reduced complexity, and provides interpretability for deep GPs and deep Bayesian neural networks."
}