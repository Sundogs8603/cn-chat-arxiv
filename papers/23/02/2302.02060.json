{
    "title": "Representation Deficiency in Masked Language Modeling",
    "abstract": "arXiv:2302.02060v2 Announce Type: replace  Abstract: Masked Language Modeling (MLM) has been one of the most prominent approaches for pretraining bidirectional text encoders due to its simplicity and effectiveness. One notable concern about MLM is that the special $\\texttt{[MASK]}$ symbol causes a discrepancy between pretraining data and downstream data as it is present only in pretraining but not in fine-tuning. In this work, we offer a new perspective on the consequence of such a discrepancy: We demonstrate empirically and theoretically that MLM pretraining allocates some model dimensions exclusively for representing $\\texttt{[MASK]}$ tokens, resulting in a representation deficiency for real tokens and limiting the pretrained model's expressiveness when it is adapted to downstream data without $\\texttt{[MASK]}$ tokens. Motivated by the identified issue, we propose MAE-LM, which pretrains the Masked Autoencoder architecture with MLM where $\\texttt{[MASK]}$ tokens are excluded from the",
    "link": "https://arxiv.org/abs/2302.02060",
    "context": "Title: Representation Deficiency in Masked Language Modeling\nAbstract: arXiv:2302.02060v2 Announce Type: replace  Abstract: Masked Language Modeling (MLM) has been one of the most prominent approaches for pretraining bidirectional text encoders due to its simplicity and effectiveness. One notable concern about MLM is that the special $\\texttt{[MASK]}$ symbol causes a discrepancy between pretraining data and downstream data as it is present only in pretraining but not in fine-tuning. In this work, we offer a new perspective on the consequence of such a discrepancy: We demonstrate empirically and theoretically that MLM pretraining allocates some model dimensions exclusively for representing $\\texttt{[MASK]}$ tokens, resulting in a representation deficiency for real tokens and limiting the pretrained model's expressiveness when it is adapted to downstream data without $\\texttt{[MASK]}$ tokens. Motivated by the identified issue, we propose MAE-LM, which pretrains the Masked Autoencoder architecture with MLM where $\\texttt{[MASK]}$ tokens are excluded from the",
    "path": "papers/23/02/2302.02060.json",
    "total_tokens": 881,
    "translated_title": "掩码语言建模中的表示不足",
    "translated_abstract": "掩码语言建模（MLM）已经成为双向文本编码器预训练最突出的方法之一，因为它简单而有效。关于MLM的一个显著问题是特殊的 $\\texttt{[MASK]}$ 符号会导致预训练数据和下游数据之间存在差异，因为它只出现在预训练中而不出现在微调中。在这项工作中，我们提供了一个新的视角，探讨了这种差异的后果：我们在理论和实践上证明了MLM预训练专门分配了一些模型维度来表示 $\\texttt{[MASK]}$ 标记，导致真实标记的表示不足，并在没有 $\\texttt{[MASK]}$ 标记的情况下，限制了预训练模型在适应下游数据时的表达能力。受到识别问题的启发，我们提出了MAE-LM，该方法利用MLM对掩码自动编码器进行预训练，其中排除了 $\\texttt{[MASK]}$ 标记。",
    "tldr": "掩码语言建模中的 $\\texttt{[MASK]} $符号会导致模型维度过度分配，造成真实标记的表示不足，本文提出了MAE-LM来解决这一问题",
    "en_tdlr": "The $\\texttt{[MASK]}$ symbol in masked language modeling leads to an over-allocation of model dimensions, resulting in a representation deficiency for real tokens. This paper introduces MAE-LM to address this issue."
}