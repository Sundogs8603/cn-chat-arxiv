{
    "title": "Towards Minimax Optimality of Model-based Robust Reinforcement Learning. (arXiv:2302.05372v2 [cs.LG] UPDATED)",
    "abstract": "We study the sample complexity of obtaining an $\\epsilon$-optimal policy in \\emph{Robust} discounted Markov Decision Processes (RMDPs), given only access to a generative model of the nominal kernel. This problem is widely studied in the non-robust case, and it is known that any planning approach applied to an empirical MDP estimated with $\\tilde{\\mathcal{O}}(\\frac{H^3 \\mid S \\mid\\mid A \\mid}{\\epsilon^2})$ samples provides an $\\epsilon$-optimal policy, which is minimax optimal. Results in the robust case are much more scarce. For $sa$(resp $s$-)rectangular uncertainty sets, the best known sample complexity is $\\tilde{\\mathcal{O}}(\\frac{H^4 \\mid S \\mid^2\\mid A \\mid}{\\epsilon^2})$ (resp. $\\tilde{\\mathcal{O}}(\\frac{H^4 \\mid S \\mid^2\\mid A \\mid^2}{\\epsilon^2})$), for specific algorithms and when the uncertainty set is based on the total variation (TV), the KL or the Chi-square divergences. In this paper, we consider uncertainty sets defined with an $L_p$-ball (recovering the TV case), and",
    "link": "http://arxiv.org/abs/2302.05372",
    "context": "Title: Towards Minimax Optimality of Model-based Robust Reinforcement Learning. (arXiv:2302.05372v2 [cs.LG] UPDATED)\nAbstract: We study the sample complexity of obtaining an $\\epsilon$-optimal policy in \\emph{Robust} discounted Markov Decision Processes (RMDPs), given only access to a generative model of the nominal kernel. This problem is widely studied in the non-robust case, and it is known that any planning approach applied to an empirical MDP estimated with $\\tilde{\\mathcal{O}}(\\frac{H^3 \\mid S \\mid\\mid A \\mid}{\\epsilon^2})$ samples provides an $\\epsilon$-optimal policy, which is minimax optimal. Results in the robust case are much more scarce. For $sa$(resp $s$-)rectangular uncertainty sets, the best known sample complexity is $\\tilde{\\mathcal{O}}(\\frac{H^4 \\mid S \\mid^2\\mid A \\mid}{\\epsilon^2})$ (resp. $\\tilde{\\mathcal{O}}(\\frac{H^4 \\mid S \\mid^2\\mid A \\mid^2}{\\epsilon^2})$), for specific algorithms and when the uncertainty set is based on the total variation (TV), the KL or the Chi-square divergences. In this paper, we consider uncertainty sets defined with an $L_p$-ball (recovering the TV case), and",
    "path": "papers/23/02/2302.05372.json",
    "total_tokens": 1130,
    "translated_title": "迈向模型基础鲁棒强化学习的最小最大优化",
    "translated_abstract": "我们研究了在只有对正常核心的生成模型访问权限时，获得ε-最优策略的采样复杂度。这个问题在非鲁棒情况下已经得到了广泛研究，并且已知任何应用于经验MDP的规划方法，只需要用ε^2/（H^3 * |S| * |A|）个样本来估计，均可提供ε-最优策略，从而最小最大优化。鲁棒情况下的结果更加少见。对于sa（s-）矩形不确定集合，已知最佳样本复杂度为ε^2/（H^4 * |S|^2 * |A|）（响应为ε^2/（H^4 * |S|^2 * |A|^2）），对于特定算法和基于总变差（TV）、KL或卡方散度的不确定集合。在本文中，我们考虑用Lp球定义的不确定集合（回复到TV情况），并且...",
    "tldr": "本文研究了在鲁棒强化学习中，对于仅具有对正常核心的生成模型访问权限时，获得ε-最优策略的样本复杂度。对于sa（s-）矩形不确定集合，已知最佳样本复杂度为ε^2/（H^4 * |S|^2 * |A|）（响应为ε^2/（H^4 * |S|^2 * |A|^2）），对于特定算法和基于总变差（TV）、KL或卡方散度的不确定集合。",
    "en_tdlr": "This paper studies the sample complexity of obtaining an ε-optimal policy in robust reinforcement learning, given only access to a generative model of the nominal kernel. The best-known sample complexity for rectangular uncertainty sets is ε^2/(H^4 * |S|^2 * |A|) (or ε^2/(H^4 * |S|^2 * |A|^2)) for specific algorithms and when the uncertainty set is based on total variation, KL, or Chi-square divergences."
}