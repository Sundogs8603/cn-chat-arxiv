{
    "title": "Predicting the cardinality and maximum degree of a reduced Gr\\\"obner basis. (arXiv:2302.05364v2 [math.AC] UPDATED)",
    "abstract": "We construct neural network regression models to predict key metrics of complexity for Gr\\\"obner bases of binomial ideals. This work illustrates why predictions with neural networks from Gr\\\"obner computations are not a straightforward process. Using two probabilistic models for random binomial ideals, we generate and make available a large data set that is able to capture sufficient variability in Gr\\\"obner complexity. We use this data to train neural networks and predict the cardinality of a reduced Gr\\\"obner basis and the maximum total degree of its elements. While the cardinality prediction problem is unlike classical problems tackled by machine learning, our simulations show that neural networks, providing performance statistics such as $r^2 = 0.401$, outperform naive guess or multiple regression models with $r^2 = 0.180$.",
    "link": "http://arxiv.org/abs/2302.05364",
    "context": "Title: Predicting the cardinality and maximum degree of a reduced Gr\\\"obner basis. (arXiv:2302.05364v2 [math.AC] UPDATED)\nAbstract: We construct neural network regression models to predict key metrics of complexity for Gr\\\"obner bases of binomial ideals. This work illustrates why predictions with neural networks from Gr\\\"obner computations are not a straightforward process. Using two probabilistic models for random binomial ideals, we generate and make available a large data set that is able to capture sufficient variability in Gr\\\"obner complexity. We use this data to train neural networks and predict the cardinality of a reduced Gr\\\"obner basis and the maximum total degree of its elements. While the cardinality prediction problem is unlike classical problems tackled by machine learning, our simulations show that neural networks, providing performance statistics such as $r^2 = 0.401$, outperform naive guess or multiple regression models with $r^2 = 0.180$.",
    "path": "papers/23/02/2302.05364.json",
    "total_tokens": 828,
    "translated_title": "预测简化Gr\\\"obner基数和最大度数的神经网络回归模型",
    "translated_abstract": "我们构建了神经网络回归模型，用以预测二项理想的Gr\\\"obner基复杂度的关键指标。这项工作说明了为什么利用神经网络从Gr\\\"obner计算中进行预测并不是一个简单的过程。我们使用两个概率模型来生成和提供一个大规模的数据集，能够捕捉到Gr\\\"obner复杂度的足够变异性。我们利用这些数据来训练神经网络并预测简化Gr\\\"obner基的基数和其元素的最大总度数。虽然基数预测问题不同于经典的机器学习问题，但我们的模拟结果表明，神经网络具有更好的性能统计，如$r^2 = 0.401$，相比于朴素猜测或多元回归模型的$r^2 = 0.180$。",
    "tldr": "该论文利用神经网络回归模型预测了简化Gr\\\"obner基的基数和最大总度数，结果表明神经网络具有更好的性能统计，相比于朴素猜测或多元回归模型。",
    "en_tdlr": "This paper uses neural network regression models to predict the cardinality and maximum total degree of a reduced Gr\\\"obner basis, and the results show that neural networks outperform naive guess or multiple regression models."
}