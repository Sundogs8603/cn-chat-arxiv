{
    "title": "Deep Offline Reinforcement Learning for Real-world Treatment Optimization Applications. (arXiv:2302.07549v2 [cs.LG] UPDATED)",
    "abstract": "There is increasing interest in data-driven approaches for recommending optimal treatment strategies in many chronic disease management and critical care applications. Reinforcement learning methods are well-suited to this sequential decision-making problem, but must be trained and evaluated exclusively on retrospective medical record datasets as direct online exploration is unsafe and infeasible. Despite this requirement, the vast majority of treatment optimization studies use off-policy RL methods (e.g., Double Deep Q Networks (DDQN) or its variants) that are known to perform poorly in purely offline settings. Recent advances in offline RL, such as Conservative Q-Learning (CQL), offer a suitable alternative. But there remain challenges in adapting these approaches to real-world applications where suboptimal examples dominate the retrospective dataset and strict safety constraints need to be satisfied. In this work, we introduce a practical and theoretically grounded transition sampli",
    "link": "http://arxiv.org/abs/2302.07549",
    "context": "Title: Deep Offline Reinforcement Learning for Real-world Treatment Optimization Applications. (arXiv:2302.07549v2 [cs.LG] UPDATED)\nAbstract: There is increasing interest in data-driven approaches for recommending optimal treatment strategies in many chronic disease management and critical care applications. Reinforcement learning methods are well-suited to this sequential decision-making problem, but must be trained and evaluated exclusively on retrospective medical record datasets as direct online exploration is unsafe and infeasible. Despite this requirement, the vast majority of treatment optimization studies use off-policy RL methods (e.g., Double Deep Q Networks (DDQN) or its variants) that are known to perform poorly in purely offline settings. Recent advances in offline RL, such as Conservative Q-Learning (CQL), offer a suitable alternative. But there remain challenges in adapting these approaches to real-world applications where suboptimal examples dominate the retrospective dataset and strict safety constraints need to be satisfied. In this work, we introduce a practical and theoretically grounded transition sampli",
    "path": "papers/23/02/2302.07549.json",
    "total_tokens": 914,
    "translated_title": "面向现实世界的治疗优化应用的深度离线强化学习",
    "translated_abstract": "越来越多的人关注数据驱动的方法，以推荐治疗策略为慢性病管理和危重护理应用。强化学习方法非常适合这个顺序决策问题，但必须仅在回顾性医疗记录数据集上进行训练和评估，因为直接的在线探索是不安全和不可行的。尽管存在这个要求，但大多数治疗优化研究使用离线RL方法（例如，双深度Q网络（DDQN）或其变体），这些方法已知在纯离线环境中表现不佳。近期离线RL的进展，例如保守Q 学习（CQL），提供了一种合适的替代方案。但在适应这些方法到临床应用中仍然存在挑战，因为回顾性数据集主要是次优例子，需要满足严格的安全约束条件。本文介绍了一种实用且理论上有基础的转移取样方法，该方法使用CQL方法在现实世界疾病管理数据集上进行训练和评估。",
    "tldr": "本研究介绍了一种面向现实世界的治疗优化方法，通过使用保守Q学习法(CQL)和转移取样将离线强化学习应用于回顾性医疗记录数据集。",
    "en_tdlr": "This study introduces a real-world treatment optimization approach, applying offline reinforcement learning to retrospective medical record datasets using Conservative Q-learning (CQL) and transition sampling."
}