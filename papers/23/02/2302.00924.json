{
    "title": "LMC: Fast Training of GNNs via Subgraph Sampling with Provable Convergence",
    "abstract": "arXiv:2302.00924v3 Announce Type: replace  Abstract: The message passing-based graph neural networks (GNNs) have achieved great success in many real-world applications. However, training GNNs on large-scale graphs suffers from the well-known neighbor explosion problem, i.e., the exponentially increasing dependencies of nodes with the number of message passing layers. Subgraph-wise sampling methods -- a promising class of mini-batch training techniques -- discard messages outside the mini-batches in backward passes to avoid the neighbor explosion problem at the expense of gradient estimation accuracy. This poses significant challenges to their convergence analysis and convergence speeds, which seriously limits their reliable real-world applications. To address this challenge, we propose a novel subgraph-wise sampling method with a convergence guarantee, namely Local Message Compensation (LMC). To the best of our knowledge, LMC is the {\\it first} subgraph-wise sampling method with provab",
    "link": "https://arxiv.org/abs/2302.00924",
    "context": "Title: LMC: Fast Training of GNNs via Subgraph Sampling with Provable Convergence\nAbstract: arXiv:2302.00924v3 Announce Type: replace  Abstract: The message passing-based graph neural networks (GNNs) have achieved great success in many real-world applications. However, training GNNs on large-scale graphs suffers from the well-known neighbor explosion problem, i.e., the exponentially increasing dependencies of nodes with the number of message passing layers. Subgraph-wise sampling methods -- a promising class of mini-batch training techniques -- discard messages outside the mini-batches in backward passes to avoid the neighbor explosion problem at the expense of gradient estimation accuracy. This poses significant challenges to their convergence analysis and convergence speeds, which seriously limits their reliable real-world applications. To address this challenge, we propose a novel subgraph-wise sampling method with a convergence guarantee, namely Local Message Compensation (LMC). To the best of our knowledge, LMC is the {\\it first} subgraph-wise sampling method with provab",
    "path": "papers/23/02/2302.00924.json",
    "total_tokens": 779,
    "translated_title": "LMC: 基于子图抽样的GNN快速训练与收敛性保证",
    "translated_abstract": "基于消息传递的图神经网络（GNNs）在许多实际应用中取得了巨大成功。然而，在大规模图上训练GNNs存在着众所周知的邻居爆炸问题，即节点与消息传递层数的增加呈指数级增加的依赖关系。子图抽样方法是一类有前途的小批量训练技术，通过在反向传播中丢弃小批量之外的消息来避免邻居爆炸问题，但这会牺牲梯度估计的准确性。为解决这一挑战，我们提出了一种新颖的带有收敛性保证的子图抽样方法，即局部消息补偿（LMC）。",
    "tldr": "LMC是第一个带有收敛性保证的子图抽样方法，旨在解决邻居爆炸问题，提高训练收敛速度。",
    "en_tdlr": "LMC is the first subgraph-wise sampling method with provable convergence guarantee, aiming to address the neighbor explosion problem and improve training convergence speed."
}