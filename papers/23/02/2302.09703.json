{
    "title": "Reinforcement Learning with Function Approximation: From Linear to Nonlinear. (arXiv:2302.09703v2 [cs.LG] UPDATED)",
    "abstract": "Function approximation has been an indispensable component in modern reinforcement learning algorithms designed to tackle problems with large state spaces in high dimensions. This paper reviews recent results on error analysis for these reinforcement learning algorithms in linear or nonlinear approximation settings, emphasizing approximation error and estimation error/sample complexity. We discuss various properties related to approximation error and present concrete conditions on transition probability and reward function under which these properties hold true. Sample complexity analysis in reinforcement learning is more complicated than in supervised learning, primarily due to the distribution mismatch phenomenon. With assumptions on the linear structure of the problem, numerous algorithms in the literature achieve polynomial sample complexity with respect to the number of features, episode length, and accuracy, although the minimax rate has not been achieved yet. These results rely ",
    "link": "http://arxiv.org/abs/2302.09703",
    "context": "Title: Reinforcement Learning with Function Approximation: From Linear to Nonlinear. (arXiv:2302.09703v2 [cs.LG] UPDATED)\nAbstract: Function approximation has been an indispensable component in modern reinforcement learning algorithms designed to tackle problems with large state spaces in high dimensions. This paper reviews recent results on error analysis for these reinforcement learning algorithms in linear or nonlinear approximation settings, emphasizing approximation error and estimation error/sample complexity. We discuss various properties related to approximation error and present concrete conditions on transition probability and reward function under which these properties hold true. Sample complexity analysis in reinforcement learning is more complicated than in supervised learning, primarily due to the distribution mismatch phenomenon. With assumptions on the linear structure of the problem, numerous algorithms in the literature achieve polynomial sample complexity with respect to the number of features, episode length, and accuracy, although the minimax rate has not been achieved yet. These results rely ",
    "path": "papers/23/02/2302.09703.json",
    "total_tokens": 885,
    "translated_title": "基于函数逼近的强化学习：从线性到非线性",
    "translated_abstract": "函数逼近是现代强化学习算法中不可或缺的组成部分，其旨在解决高维度大状态空间问题。本文回顾了近年来在线性和非线性逼近环境下强化学习算法的错误分析，并强调了逼近误差和估计误差/样本复杂度。我们讨论了与逼近误差有关的各种性质，并给出了转移概率和奖励函数的具体条件，使得这些性质成立。在强化学习中，样本复杂度的分析比监督学习更为复杂，主要是由于分布不匹配现象。在线性问题结构的假设下，文献中的许多算法都可以实现多项式样本复杂度，与特征数量、剧集长度和准确性有关，尽管尚未实现最小最大速率。",
    "tldr": "本文回顾了近年来在线性和非线性逼近环境下强化学习算法的错误分析，并强调了逼近误差和估计误差/样本复杂度。在线性问题结构的假设下，近期的算法实现了多项式样本复杂度，然而尚未实现最小最大速率。"
}