{
    "title": "TAMUNA: Doubly Accelerated Federated Learning with Local Training, Compression, and Partial Participation. (arXiv:2302.09832v2 [cs.LG] UPDATED)",
    "abstract": "In federated learning, a large number of users collaborate to learn a global model. They alternate local computations and communication with a distant server. Communication, which can be slow and costly, is the main bottleneck in this setting. In addition to communication-efficiency, a robust algorithm should allow for partial participation, the desirable feature that not all clients need to participate to every round of the training process. To reduce the communication load and therefore accelerate distributed gradient descent, two strategies are popular: 1) communicate less frequently; that is, perform several iterations of local computations between the communication rounds; and 2) communicate compressed information instead of full-dimensional vectors. We propose TAMUNA, the first algorithm for distributed optimization and federated learning, which harnesses these two strategies jointly and allows for partial participation. TAMUNA converges linearly to an exact solution in the stron",
    "link": "http://arxiv.org/abs/2302.09832",
    "context": "Title: TAMUNA: Doubly Accelerated Federated Learning with Local Training, Compression, and Partial Participation. (arXiv:2302.09832v2 [cs.LG] UPDATED)\nAbstract: In federated learning, a large number of users collaborate to learn a global model. They alternate local computations and communication with a distant server. Communication, which can be slow and costly, is the main bottleneck in this setting. In addition to communication-efficiency, a robust algorithm should allow for partial participation, the desirable feature that not all clients need to participate to every round of the training process. To reduce the communication load and therefore accelerate distributed gradient descent, two strategies are popular: 1) communicate less frequently; that is, perform several iterations of local computations between the communication rounds; and 2) communicate compressed information instead of full-dimensional vectors. We propose TAMUNA, the first algorithm for distributed optimization and federated learning, which harnesses these two strategies jointly and allows for partial participation. TAMUNA converges linearly to an exact solution in the stron",
    "path": "papers/23/02/2302.09832.json",
    "total_tokens": 797,
    "translated_title": "TAMUNA: 带有局部训练、压缩和部分参与的双倍加速联邦学习",
    "translated_abstract": "在联邦学习中，大量用户合作学习全局模型。他们交替进行本地计算和与远程服务器的通信。通信是该设置中的主要瓶颈，它可以慢且昂贵。为了减少通信负载并加速分布式梯度下降，使用两种策略很受欢迎：1）更少地通信，即在通信轮之间执行几个本地计算的迭代；2）传输压缩信息而不是完整维度的矢量。我们提出了TAMUNA，这是第一个分布式优化和联邦学习算法，它联合利用这两种策略，同时允许部分参与。TAMUNA以线性速度收敛到精确解决方案。",
    "tldr": "TAMUNA是首个联合利用网络压缩和少量通信配合加速分布式梯度下降算法，并允许部分参与的算法。",
    "en_tdlr": "TAMUNA is the first algorithm that jointly uses network compression and less communication to accelerate distributed gradient descent, and allows for partial participation."
}