{
    "title": "Arbitrary Decisions are a Hidden Cost of Differentially Private Training. (arXiv:2302.14517v2 [cs.LG] UPDATED)",
    "abstract": "Mechanisms used in privacy-preserving machine learning often aim to guarantee differential privacy (DP) during model training. Practical DP-ensuring training methods use randomization when fitting model parameters to privacy-sensitive data (e.g., adding Gaussian noise to clipped gradients). We demonstrate that such randomization incurs predictive multiplicity: for a given input example, the output predicted by equally-private models depends on the randomness used in training. Thus, for a given input, the predicted output can vary drastically if a model is re-trained, even if the same training dataset is used. The predictive-multiplicity cost of DP training has not been studied, and is currently neither audited for nor communicated to model designers and stakeholders. We derive a bound on the number of re-trainings required to estimate predictive multiplicity reliably. We analyze--both theoretically and through extensive experiments--the predictive-multiplicity cost of three DP-ensuring",
    "link": "http://arxiv.org/abs/2302.14517",
    "context": "Title: Arbitrary Decisions are a Hidden Cost of Differentially Private Training. (arXiv:2302.14517v2 [cs.LG] UPDATED)\nAbstract: Mechanisms used in privacy-preserving machine learning often aim to guarantee differential privacy (DP) during model training. Practical DP-ensuring training methods use randomization when fitting model parameters to privacy-sensitive data (e.g., adding Gaussian noise to clipped gradients). We demonstrate that such randomization incurs predictive multiplicity: for a given input example, the output predicted by equally-private models depends on the randomness used in training. Thus, for a given input, the predicted output can vary drastically if a model is re-trained, even if the same training dataset is used. The predictive-multiplicity cost of DP training has not been studied, and is currently neither audited for nor communicated to model designers and stakeholders. We derive a bound on the number of re-trainings required to estimate predictive multiplicity reliably. We analyze--both theoretically and through extensive experiments--the predictive-multiplicity cost of three DP-ensuring",
    "path": "papers/23/02/2302.14517.json",
    "total_tokens": 1194,
    "translated_title": "差分隐私训练的任意决策是一个隐藏成本",
    "translated_abstract": "隐私保护的机器学习中使用的机制通常旨在在模型训练期间保证差分隐私(DP)。在将模型参数拟合到隐私敏感数据时，实践中使用随机化方法(例如，在截断的梯度上添加高斯噪声)以确保DP训练。我们证明了这种随机化会产生预测多样性：对于给定的输入示例，由同样DP-保证的模型预测的输出取决于训练中使用的随机性。因此，对于给定的输入，即使使用相同的训练数据集重新训练模型，预测输出也可能发生巨大变化。尚未研究由DP训练引起的多样性成本，并且目前还未经过审核或向模型设计者和利益相关者传达。我们得出了一种在可靠地估计预测多样性所需的重新训练次数的上限，并通过广泛的实验分析了三种DP-确保训练方法的预测多样性成本，包括理论和实验两个方面。",
    "tldr": "差分隐私训练会产生预测多样性，即使对于相同输入，使用不同的随机性也会得到不同的输出，这一成本不仅未被研究还未被审核或传达给模型设计者和利益相关者。",
    "en_tdlr": "Differential privacy training generates predictive multiplicity, which means even with the same input, different outputs would be generated because of the different randomness used in training. However, this cost has not been studied or audited, which is communicated neither to model designers nor stakeholders."
}