{
    "title": "Near-Minimax-Optimal Risk-Sensitive Reinforcement Learning with CVaR. (arXiv:2302.03201v2 [cs.LG] UPDATED)",
    "abstract": "In this paper, we study risk-sensitive Reinforcement Learning (RL), focusing on the objective of Conditional Value at Risk (CVaR) with risk tolerance $\\tau$. Starting with multi-arm bandits (MABs), we show the minimax CVaR regret rate is $\\Omega(\\sqrt{\\tau^{-1}AK})$, where $A$ is the number of actions and $K$ is the number of episodes, and that it is achieved by an Upper Confidence Bound algorithm with a novel Bernstein bonus. For online RL in tabular Markov Decision Processes (MDPs), we show a minimax regret lower bound of $\\Omega(\\sqrt{\\tau^{-1}SAK})$ (with normalized cumulative rewards), where $S$ is the number of states, and we propose a novel bonus-driven Value Iteration procedure. We show that our algorithm achieves the optimal regret of $\\widetilde O(\\sqrt{\\tau^{-1}SAK})$ under a continuity assumption and in general attains a near-optimal regret of $\\widetilde O(\\tau^{-1}\\sqrt{SAK})$, which is minimax-optimal for constant $\\tau$. This improves on the best available bounds. By di",
    "link": "http://arxiv.org/abs/2302.03201",
    "context": "Title: Near-Minimax-Optimal Risk-Sensitive Reinforcement Learning with CVaR. (arXiv:2302.03201v2 [cs.LG] UPDATED)\nAbstract: In this paper, we study risk-sensitive Reinforcement Learning (RL), focusing on the objective of Conditional Value at Risk (CVaR) with risk tolerance $\\tau$. Starting with multi-arm bandits (MABs), we show the minimax CVaR regret rate is $\\Omega(\\sqrt{\\tau^{-1}AK})$, where $A$ is the number of actions and $K$ is the number of episodes, and that it is achieved by an Upper Confidence Bound algorithm with a novel Bernstein bonus. For online RL in tabular Markov Decision Processes (MDPs), we show a minimax regret lower bound of $\\Omega(\\sqrt{\\tau^{-1}SAK})$ (with normalized cumulative rewards), where $S$ is the number of states, and we propose a novel bonus-driven Value Iteration procedure. We show that our algorithm achieves the optimal regret of $\\widetilde O(\\sqrt{\\tau^{-1}SAK})$ under a continuity assumption and in general attains a near-optimal regret of $\\widetilde O(\\tau^{-1}\\sqrt{SAK})$, which is minimax-optimal for constant $\\tau$. This improves on the best available bounds. By di",
    "path": "papers/23/02/2302.03201.json",
    "total_tokens": 874,
    "translated_title": "基于CVaR的风险敏感强化学习的近最小化风险算法",
    "translated_abstract": "本文研究了风险敏感强化学习(Reinforcement Learning)的目标条件风险价值(CVaR)，并针对多臂老虎机和标签化马尔可夫决策过程问题，提出了一种新的基于上置信界算法的伯恩斯坦奖励算法，以及一种基于价值迭代的算法。我们在连续性假设下证明了我们的算法达到了最优或者接近最优的风险。这些算法都是基于CVaR所实现的。",
    "tldr": "本文提出了基于CVaR的风险敏感强化学习算法，在针对多臂老虎机和标签化马尔可夫决策过程问题上，通过提出一种新的伯恩斯坦奖励算法和基于价值迭代的算法，实现了最优或接近最优的风险。",
    "en_tdlr": "This paper proposes a risk-sensitive Reinforcement Learning algorithm based on CVaR, which achieves optimal or near-optimal risks in multi-arm bandits and tabular Markov Decision Processes problems through a novel Bernstein bonus algorithm and a bonus-driven Value Iteration procedure."
}