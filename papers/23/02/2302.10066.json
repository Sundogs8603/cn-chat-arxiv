{
    "title": "Sharp analysis of EM for learning mixtures of pairwise differences. (arXiv:2302.10066v2 [math.ST] UPDATED)",
    "abstract": "We consider a symmetric mixture of linear regressions with random samples from the pairwise comparison design, which can be seen as a noisy version of a type of Euclidean distance geometry problem. We analyze the expectation-maximization (EM) algorithm locally around the ground truth and establish that the sequence converges linearly, providing an $\\ell_\\infty$-norm guarantee on the estimation error of the iterates. Furthermore, we show that the limit of the EM sequence achieves the sharp rate of estimation in the $\\ell_2$-norm, matching the information-theoretically optimal constant. We also argue through simulation that convergence from a random initialization is much more delicate in this setting, and does not appear to occur in general. Our results show that the EM algorithm can exhibit several unique behaviors when the covariate distribution is suitably structured.",
    "link": "http://arxiv.org/abs/2302.10066",
    "context": "Title: Sharp analysis of EM for learning mixtures of pairwise differences. (arXiv:2302.10066v2 [math.ST] UPDATED)\nAbstract: We consider a symmetric mixture of linear regressions with random samples from the pairwise comparison design, which can be seen as a noisy version of a type of Euclidean distance geometry problem. We analyze the expectation-maximization (EM) algorithm locally around the ground truth and establish that the sequence converges linearly, providing an $\\ell_\\infty$-norm guarantee on the estimation error of the iterates. Furthermore, we show that the limit of the EM sequence achieves the sharp rate of estimation in the $\\ell_2$-norm, matching the information-theoretically optimal constant. We also argue through simulation that convergence from a random initialization is much more delicate in this setting, and does not appear to occur in general. Our results show that the EM algorithm can exhibit several unique behaviors when the covariate distribution is suitably structured.",
    "path": "papers/23/02/2302.10066.json",
    "total_tokens": 894,
    "translated_title": "学习成对差分混合的EM算法的尖锐分析。",
    "translated_abstract": "我们考虑使用成对比较设计的随机样本的线性回归的对称混合，这可以看作是一种欧几里得距离几何问题的噪声版本。我们在真实值周围局部分析期望最大化（EM）算法，并建立起它的序列线性收敛性，从而为迭代估计误差提供一个$\\ell_\\infty$-范数保证。此外，我们表明，EM序列的极限实现了在$\\ell_2$-范数中的估计尖锐度，匹配信息理论上最优的常数。我们还通过模拟论证了在这种情况下从随机初始化收敛的问题更为微妙，通常不会发生。我们的结果表明，当协变量分布被适当地结构化时，EM算法可以表现出几个独特的行为。",
    "tldr": "该论文研究了使用成对比较设计的随机样本的线性回归的对称混合，通过分析EM算法的序列收敛性和极限值，得出了$\\ell_\\infty$范数和$\\ell_2$范数中的估计尖锐度。研究表明EM算法可以展现出多个独特的行为。",
    "en_tdlr": "This paper studies a symmetric mixture of linear regressions with random samples from the pairwise comparison design and analyzes the EM algorithm's sequence convergence and limit values to obtain estimates in $\\ell_\\infty$ and $\\ell_2$ norms. The research shows that the EM algorithm can exhibit several unique behaviors when the covariate distribution is suitably structured."
}