{
    "title": "Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems. (arXiv:2302.03735v2 [cs.IR] UPDATED)",
    "abstract": "The emergency of Pre-trained Language Models (PLMs) has achieved tremendous success in the field of Natural Language Processing (NLP) by learning universal representations on large corpora in a self-supervised manner. The pre-trained models and the learned representations can be beneficial to a series of downstream NLP tasks. This training paradigm has recently been adapted to the recommendation domain and is considered a promising approach by both academia and industry. In this paper, we systematically investigate how to extract and transfer knowledge from pre-trained models learned by different PLM-related training paradigms to improve recommendation performance from various perspectives, such as generality, sparsity, efficiency and effectiveness. Specifically, we propose an orthogonal taxonomy to divide existing PLM-based recommender systems w.r.t. their training strategies and objectives. Then, we analyze and summarize the connection between PLM-based training paradigms and differe",
    "link": "http://arxiv.org/abs/2302.03735",
    "context": "Title: Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems. (arXiv:2302.03735v2 [cs.IR] UPDATED)\nAbstract: The emergency of Pre-trained Language Models (PLMs) has achieved tremendous success in the field of Natural Language Processing (NLP) by learning universal representations on large corpora in a self-supervised manner. The pre-trained models and the learned representations can be beneficial to a series of downstream NLP tasks. This training paradigm has recently been adapted to the recommendation domain and is considered a promising approach by both academia and industry. In this paper, we systematically investigate how to extract and transfer knowledge from pre-trained models learned by different PLM-related training paradigms to improve recommendation performance from various perspectives, such as generality, sparsity, efficiency and effectiveness. Specifically, we propose an orthogonal taxonomy to divide existing PLM-based recommender systems w.r.t. their training strategies and objectives. Then, we analyze and summarize the connection between PLM-based training paradigms and differe",
    "path": "papers/23/02/2302.03735.json",
    "total_tokens": 880,
    "translated_title": "预训练、提示和推荐：语言模型范式在推荐系统中的综合调查",
    "translated_abstract": "预训练语言模型（PLM）的出现，通过自监督方式在大型语料库上学习通用表示，在自然语言处理（NLP）领域取得了巨大成功。预训练模型和学到的表示可受益于一系列下游NLP任务。这种培训范式最近被适用于推荐领域，并被学术界和工业界认为是一种有前途的方法。本文系统地研究了如何从不同PLM相关训练范式学习到的预训练模型中提取和转移知识，从多个角度（如通用性、稀疏性、效率和效果）提高推荐性能。具体而言，我们提出了一个正交分类法来划分现有的基于PLM的推荐系统，针对其培训策略和目标进行分析和总结。",
    "tldr": "本文系统地研究了如何从不同预训练语言模型中提取和转移知识，提高推荐系统性能。我们提出了一个分类法，分析和总结了基于预训练语言模型的推荐系统的培训策略和目标。",
    "en_tdlr": "This paper systematically investigates how to extract and transfer knowledge from different pre-trained language models to improve recommendation performance, and proposes a taxonomy to analyze and summarize PLM-based recommender systems' training strategies and objectives."
}