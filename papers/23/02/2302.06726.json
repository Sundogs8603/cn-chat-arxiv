{
    "title": "Swap Agnostic Learning, or Characterizing Omniprediction via Multicalibration. (arXiv:2302.06726v2 [cs.LG] UPDATED)",
    "abstract": "We introduce and study Swap Agnostic Learning. The problem can be phrased as a game between a predictor and an adversary: first, the predictor selects a hypothesis $h$; then, the adversary plays in response, and for each level set of the predictor $\\{x \\in \\mathcal{X} : h(x) = v\\}$ selects a (different) loss-minimizing hypothesis $c_v \\in \\mathcal{C}$; the predictor wins if $h$ competes with the adaptive adversary's loss. Despite the strength of the adversary, we demonstrate the feasibility Swap Agnostic Learning for any convex loss.  Somewhat surprisingly, the result follows through an investigation into the connections between Omniprediction and Multicalibration. Omniprediction is a new notion of optimality for predictors that strengthtens classical notions such as agnostic learning. It asks for loss minimization guarantees (relative to a hypothesis class) that apply not just for a specific loss function, but for any loss belonging to a rich family of losses. A recent line of work sh",
    "link": "http://arxiv.org/abs/2302.06726",
    "context": "Title: Swap Agnostic Learning, or Characterizing Omniprediction via Multicalibration. (arXiv:2302.06726v2 [cs.LG] UPDATED)\nAbstract: We introduce and study Swap Agnostic Learning. The problem can be phrased as a game between a predictor and an adversary: first, the predictor selects a hypothesis $h$; then, the adversary plays in response, and for each level set of the predictor $\\{x \\in \\mathcal{X} : h(x) = v\\}$ selects a (different) loss-minimizing hypothesis $c_v \\in \\mathcal{C}$; the predictor wins if $h$ competes with the adaptive adversary's loss. Despite the strength of the adversary, we demonstrate the feasibility Swap Agnostic Learning for any convex loss.  Somewhat surprisingly, the result follows through an investigation into the connections between Omniprediction and Multicalibration. Omniprediction is a new notion of optimality for predictors that strengthtens classical notions such as agnostic learning. It asks for loss minimization guarantees (relative to a hypothesis class) that apply not just for a specific loss function, but for any loss belonging to a rich family of losses. A recent line of work sh",
    "path": "papers/23/02/2302.06726.json",
    "total_tokens": 952,
    "translated_title": "无关交换学习，或通过多校准对全预测进行表征",
    "translated_abstract": "我们介绍并研究了无关交换学习。这个问题可以看作是预测者和对手之间的一种博弈：首先，预测者选择一个假设$h$；然后，对手以响应方式进行游戏，并且对于预测者的每个水平集$\\{x \\in \\mathcal{X} : h(x) = v\\}$选择一个（不同的）使损失最小化的假设$c_v \\in \\mathcal{C}$；如果$h$与对手的损失竞争，预测者获胜。尽管对手的强大，我们证明了无关交换学习在任何凸损失函数下是可行的。令人惊讶的是，这一结果通过研究全预测和多校准之间的联系得出。全预测是预测者的一种新的最优性概念，它加强了像无关学习这样的经典概念。它要求在适用于一个丰富的损失函数族的任何损失函数上，相对于一个假设类别的损失最小化保证。最近的一系列研究探索了这一领域。",
    "tldr": "本文介绍了无关交换学习的概念，并展示了其在任何凸损失函数下的可行性。此外，研究发现全预测和多校准之间存在联系，全预测是一种新的最优性概念，可以加强经典的无关学习。"
}