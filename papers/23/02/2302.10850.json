{
    "title": "Offline Reinforcement Learning for Mixture-of-Expert Dialogue Management. (arXiv:2302.10850v2 [cs.LG] UPDATED)",
    "abstract": "Reinforcement learning (RL) has shown great promise for developing dialogue management (DM) agents that are non-myopic, conduct rich conversations, and maximize overall user satisfaction. Despite recent developments in RL and language models (LMs), using RL to power conversational chatbots remains challenging, in part because RL requires online exploration to learn effectively, whereas collecting novel human-bot interactions can be expensive and unsafe. This issue is exacerbated by the combinatorial action spaces facing these algorithms, as most LM agents generate responses at the word level. We develop a variety of RL algorithms, specialized to dialogue planning, that leverage recent Mixture-of-Expert Language Models (MoE-LMs) -- models that capture diverse semantics, generate utterances reflecting different intents, and are amenable for multi-turn DM. By exploiting MoE-LM structure, our methods significantly reduce the size of the action space and improve the efficacy of RL-based DM.",
    "link": "http://arxiv.org/abs/2302.10850",
    "context": "Title: Offline Reinforcement Learning for Mixture-of-Expert Dialogue Management. (arXiv:2302.10850v2 [cs.LG] UPDATED)\nAbstract: Reinforcement learning (RL) has shown great promise for developing dialogue management (DM) agents that are non-myopic, conduct rich conversations, and maximize overall user satisfaction. Despite recent developments in RL and language models (LMs), using RL to power conversational chatbots remains challenging, in part because RL requires online exploration to learn effectively, whereas collecting novel human-bot interactions can be expensive and unsafe. This issue is exacerbated by the combinatorial action spaces facing these algorithms, as most LM agents generate responses at the word level. We develop a variety of RL algorithms, specialized to dialogue planning, that leverage recent Mixture-of-Expert Language Models (MoE-LMs) -- models that capture diverse semantics, generate utterances reflecting different intents, and are amenable for multi-turn DM. By exploiting MoE-LM structure, our methods significantly reduce the size of the action space and improve the efficacy of RL-based DM.",
    "path": "papers/23/02/2302.10850.json",
    "total_tokens": 909,
    "translated_title": "离线强化学习用于混合专家对话管理",
    "translated_abstract": "强化学习（RL）在开发对话管理（DM）代理，实现非目标导向，进行富有内容的对话，最大化用户满意度方面表现出了巨大的潜力。尽管强化学习和语言模型（LMs）最近取得了进展，但使用强化学习驱动的对话聊天机器人仍然具有挑战性，部分原因是强化学习需要在线探索以有效学习，而收集新颖的人机交互可能既昂贵又不安全。这个问题在面对这些算法的组合动作空间时变得更为严重，因为大多数语言模型代理以词级别生成响应。我们开发了多种针对对话规划的强化学习算法，利用最新的混合专家语言模型（MoE-LMs） - 一种捕捉多样语义，生成反映不同意图的话语的模型，适用于多轮对话管理。通过利用MoE-LM结构，我们的方法显著减少了行动空间的大小，并提高了基于强化学习的对话管理的有效性。",
    "tldr": "本论文研究了离线强化学习在混合专家对话管理中的应用。通过利用最新的混合专家语言模型（MoE-LMs），我们开发了针对对话规划的强化学习算法，显著减少了动作空间的大小，并提高了对话管理的有效性。"
}