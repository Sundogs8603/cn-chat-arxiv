{
    "title": "Bounding Training Data Reconstruction in DP-SGD. (arXiv:2302.07225v2 [cs.CR] UPDATED)",
    "abstract": "Differentially private training offers a protection which is usually interpreted as a guarantee against membership inference attacks. By proxy, this guarantee extends to other threats like reconstruction attacks attempting to extract complete training examples. Recent works provide evidence that if one does not need to protect against membership attacks but instead only wants to protect against training data reconstruction, then utility of private models can be improved because less noise is required to protect against these more ambitious attacks. We investigate this further in the context of DP-SGD, a standard algorithm for private deep learning, and provide an upper bound on the success of any reconstruction attack against DP-SGD together with an attack that empirically matches the predictions of our bound. Together, these two results open the door to fine-grained investigations on how to set the privacy parameters of DP-SGD in practice to protect against reconstruction attacks. Fin",
    "link": "http://arxiv.org/abs/2302.07225",
    "context": "Title: Bounding Training Data Reconstruction in DP-SGD. (arXiv:2302.07225v2 [cs.CR] UPDATED)\nAbstract: Differentially private training offers a protection which is usually interpreted as a guarantee against membership inference attacks. By proxy, this guarantee extends to other threats like reconstruction attacks attempting to extract complete training examples. Recent works provide evidence that if one does not need to protect against membership attacks but instead only wants to protect against training data reconstruction, then utility of private models can be improved because less noise is required to protect against these more ambitious attacks. We investigate this further in the context of DP-SGD, a standard algorithm for private deep learning, and provide an upper bound on the success of any reconstruction attack against DP-SGD together with an attack that empirically matches the predictions of our bound. Together, these two results open the door to fine-grained investigations on how to set the privacy parameters of DP-SGD in practice to protect against reconstruction attacks. Fin",
    "path": "papers/23/02/2302.07225.json",
    "total_tokens": 776,
    "translated_title": "DP-SGD中的训练数据重构界限研究",
    "translated_abstract": "差分隐私训练通常被解释为对抗成员推断攻击的保护。最近的研究发现，如果只需要保护免受训练数据重构攻击的威胁，那么私有模型的效用可以改善，因为保护免受这些更有野心的攻击需要更少的噪声。本文在DP-SGD的上下文中进一步研究了这一问题，并提供了针对DP-SGD的任何重建攻击成功的上限以及与我们边界预测相匹配的攻击。这两个结果为设置DP-SGD的隐私参数以保护免受重建攻击开辟了细致的研究方向。",
    "tldr": "本文在DP-SGD的上下文中研究了如何设置隐私参数以保护免受训练数据重构攻击，并提供了相关攻击的上限和匹配的攻击方式。",
    "en_tdlr": "This paper investigates how to set privacy parameters in DP-SGD to protect against training data reconstruction attacks, and provides an upper bound and matching attack to the success of any reconstruction attack against DP-SGD."
}