{
    "title": "What Makes a Language Easy to Deep-Learn?. (arXiv:2302.12239v2 [cs.CL] UPDATED)",
    "abstract": "Neural networks drive the success of natural language processing. A fundamental property of language is its compositional structure, allowing humans to produce forms for new meanings systematically. However, unlike humans, neural networks notoriously struggle with systematic generalization, and do not necessarily benefit from compositional structure in emergent communication simulations. This poses a problem for using neural networks to simulate human language learning and evolution, and suggests crucial differences in the biases of the different learning systems. Here, we directly test how neural networks compare to humans in learning and generalizing different input languages that vary in their degree of structure. We evaluate the memorization and generalization capabilities of a pre-trained language model GPT-3.5 (analagous to an adult second language learner) and recurrent neural networks trained from scratch (analaogous to a child first language learner). Our results show striking",
    "link": "http://arxiv.org/abs/2302.12239",
    "context": "Title: What Makes a Language Easy to Deep-Learn?. (arXiv:2302.12239v2 [cs.CL] UPDATED)\nAbstract: Neural networks drive the success of natural language processing. A fundamental property of language is its compositional structure, allowing humans to produce forms for new meanings systematically. However, unlike humans, neural networks notoriously struggle with systematic generalization, and do not necessarily benefit from compositional structure in emergent communication simulations. This poses a problem for using neural networks to simulate human language learning and evolution, and suggests crucial differences in the biases of the different learning systems. Here, we directly test how neural networks compare to humans in learning and generalizing different input languages that vary in their degree of structure. We evaluate the memorization and generalization capabilities of a pre-trained language model GPT-3.5 (analagous to an adult second language learner) and recurrent neural networks trained from scratch (analaogous to a child first language learner). Our results show striking",
    "path": "papers/23/02/2302.12239.json",
    "total_tokens": 873,
    "translated_title": "编程什么使一种语言易于深度学习？",
    "translated_abstract": "神经网络推动了自然语言处理的成功。语言的一个基本属性是其组成结构，使人类能够系统地产生新的意义形式。然而，与人类不同，神经网络在系统化概括方面一直存在困难，并且在新兴通信模拟中不一定受益于组成结构。这对于使用神经网络模拟人类语言学习和进化构成了一个问题，并且暗示了不同学习系统的偏见的关键差异。在这里，我们直接测试神经网络在学习和概括不同输入语言的能力，这些语言在其结构程度上有所不同。我们评估了一个预训练的语言模型GPT-3.5（类似于成年第二语言学习者）和从头开始训练的递归神经网络（类似于儿童第一语言学习者）的记忆和概括能力。我们的结果显示了令人震惊的",
    "tldr": "本研究通过测试神经网络和人类在学习和推广不同结构程度的语言方面的能力，发现神经网络在系统化概括方面存在困难，这对于模拟人类语言学习和进化构成了一个问题。",
    "en_tdlr": "This research tests the ability of neural networks and humans to learn and generalize different languages with varying degrees of structure, revealing that neural networks struggle with systematic generalization, posing a problem for simulating human language learning and evolution."
}