{
    "title": "Statistical Learning under Heterogenous Distribution Shift. (arXiv:2302.13934v2 [cs.LG] UPDATED)",
    "abstract": "This paper studies the prediction of a target $\\mathbf{z}$ from a pair of random variables $(\\mathbf{x},\\mathbf{y})$, where the ground-truth predictor is additive $\\mathbb{E}[\\mathbf{z} \\mid \\mathbf{x},\\mathbf{y}] = f_\\star(\\mathbf{x}) +g_{\\star}(\\mathbf{y})$. We study the performance of empirical risk minimization (ERM) over functions $f+g$, $f \\in F$ and $g \\in G$, fit on a given training distribution, but evaluated on a test distribution which exhibits covariate shift. We show that, when the class $F$ is \"simpler\" than $G$ (measured, e.g., in terms of its metric entropy), our predictor is more resilient to $\\textbf{heterogenous covariate shifts}$ in which the shift in $\\mathbf{x}$ is much greater than that in $\\mathbf{y}$. Our analysis proceeds by demonstrating that ERM behaves $\\textbf{qualitatively similarly to orthogonal machine learning}$: the rate at which ERM recovers the $f$-component of the predictor has only a lower-order dependence on the complexity of the class $G$, adjus",
    "link": "http://arxiv.org/abs/2302.13934",
    "context": "Title: Statistical Learning under Heterogenous Distribution Shift. (arXiv:2302.13934v2 [cs.LG] UPDATED)\nAbstract: This paper studies the prediction of a target $\\mathbf{z}$ from a pair of random variables $(\\mathbf{x},\\mathbf{y})$, where the ground-truth predictor is additive $\\mathbb{E}[\\mathbf{z} \\mid \\mathbf{x},\\mathbf{y}] = f_\\star(\\mathbf{x}) +g_{\\star}(\\mathbf{y})$. We study the performance of empirical risk minimization (ERM) over functions $f+g$, $f \\in F$ and $g \\in G$, fit on a given training distribution, but evaluated on a test distribution which exhibits covariate shift. We show that, when the class $F$ is \"simpler\" than $G$ (measured, e.g., in terms of its metric entropy), our predictor is more resilient to $\\textbf{heterogenous covariate shifts}$ in which the shift in $\\mathbf{x}$ is much greater than that in $\\mathbf{y}$. Our analysis proceeds by demonstrating that ERM behaves $\\textbf{qualitatively similarly to orthogonal machine learning}$: the rate at which ERM recovers the $f$-component of the predictor has only a lower-order dependence on the complexity of the class $G$, adjus",
    "path": "papers/23/02/2302.13934.json",
    "total_tokens": 1142,
    "translated_title": "异质分布偏移下的统计学习",
    "translated_abstract": "本文研究了从随机变量对$(\\mathbf{x},\\mathbf{y})$中预测目标$\\mathbf{z}$, 其中真实的预测器是加法的$\\mathbb{E}[\\mathbf{z} \\mid \\mathbf{x},\\mathbf{y}] = f_\\star(\\mathbf{x}) +g_{\\star}(\\mathbf{y})$。我们研究了在给定训练分布上拟合的函数$f+g$, $f \\in F$和$g \\in G$上的经验风险最小化(ERM)在表现上的差异，但在测试分布上得到评估时会显示出协变量偏移。我们的研究表明，当类别$F$比$G$更“简单”（例如，以度量熵为衡量标准）时，我们的预测器对于协变量偏移的抗干扰能力更强，其中$\\textbf{y}$的偏移要远小于$\\textbf{x}$的偏移。我们的分析表明，ERM的行为与正交机器学习$\\textbf{ qualitatively similarly}$：ERM恢复预测器中的$f$成分的速率仅对于类别$G$的复杂性具有较低阶的依赖性，调整后...",
    "tldr": "本文研究了异质分布偏移下的统计学习问题，通过研究经验风险最小化(ERM)在不同类别的复杂性下的表现，我们发现当类别$F$相比类别$G$更“简单”时，我们的预测器对于协变量偏移具有更强的鲁棒性，尤其在$\\textbf{y}$的偏移远小于$\\textbf{x}$的情况下。同时，我们发现ERM的行为与正交机器学习具有类似的特性。",
    "en_tdlr": "This paper examines the problem of statistical learning under heterogenous distribution shift, and shows that our predictor is more resilient to covariate shifts when the class F is \"simpler\" than the class G. The study also reveals that the behavior of empirical risk minimization (ERM) is qualitatively similar to orthogonal machine learning."
}