{
    "title": "Power Laws for Hyperparameter Optimization. (arXiv:2302.00441v2 [cs.LG] UPDATED)",
    "abstract": "Hyperparameter optimization is an important subfield of machine learning that focuses on tuning the hyperparameters of a chosen algorithm to achieve peak performance. Recently, there has been a stream of methods that tackle the issue of hyperparameter optimization, however, most of the methods do not exploit the scaling law property of learning curves. In this work, we propose Deep Power Laws (DPL), an ensemble of neural network models conditioned to yield predictions that follow a power-law scaling pattern. Our method dynamically decides which configurations to pause and train incrementally by making use of gray-box evaluations. We compare our method against 7 state-of-the-art competitors on 3 benchmarks related to tabular, image, and NLP datasets covering 59 diverse tasks. Our method achieves the best results across all benchmarks by obtaining the best any-time results compared to all competitors.",
    "link": "http://arxiv.org/abs/2302.00441",
    "context": "Title: Power Laws for Hyperparameter Optimization. (arXiv:2302.00441v2 [cs.LG] UPDATED)\nAbstract: Hyperparameter optimization is an important subfield of machine learning that focuses on tuning the hyperparameters of a chosen algorithm to achieve peak performance. Recently, there has been a stream of methods that tackle the issue of hyperparameter optimization, however, most of the methods do not exploit the scaling law property of learning curves. In this work, we propose Deep Power Laws (DPL), an ensemble of neural network models conditioned to yield predictions that follow a power-law scaling pattern. Our method dynamically decides which configurations to pause and train incrementally by making use of gray-box evaluations. We compare our method against 7 state-of-the-art competitors on 3 benchmarks related to tabular, image, and NLP datasets covering 59 diverse tasks. Our method achieves the best results across all benchmarks by obtaining the best any-time results compared to all competitors.",
    "path": "papers/23/02/2302.00441.json",
    "total_tokens": 820,
    "translated_title": "超参数优化的幂律法则",
    "translated_abstract": "超参数优化是机器学习中一个重要的子领域，它专注于调整所选算法的超参数以实现最佳性能。最近，有一系列方法解决了超参数优化的问题，然而，大多数方法没有利用学习曲线的缩放规律特性。在这项工作中，我们提出Deep Power Laws（DPL），一组神经网络模型，它们的预测遵循一个幂律缩放模式。我们的方法通过利用灰盒评估动态决定暂停和增量训练哪些配置。我们在与3个基准相关的表格，图像和NLP数据集上与7种最先进的竞争对手进行了比较，涵盖59项不同的任务。我们的方法在所有基准测试中都取得了最佳结果，并获得了比所有竞争对手更好的任何时候的结果。",
    "tldr": "该论文提出了一种基于幂律规律的Deep Power Laws（DPL）方法来解决超参数优化问题，并在表格、图像和NLP数据集上展示了最佳结果。",
    "en_tdlr": "This paper proposes a Deep Power Laws (DPL) method based on power-law scaling patterns to tackle the issue of hyperparameter optimization, achieving best results across all benchmarks on tabular, image, and NLP datasets covering 59 diverse tasks compared to 7 state-of-the-art competitors."
}