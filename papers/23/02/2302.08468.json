{
    "title": "LEVER: Learning to Verify Language-to-Code Generation with Execution. (arXiv:2302.08468v2 [cs.LG] UPDATED)",
    "abstract": "The advent of large language models trained on code (code LLMs) has led to significant progress in language-to-code generation. State-of-the-art approaches in this area combine LLM decoding with sample pruning and reranking using test cases or heuristics based on the execution results. However, it is challenging to obtain test cases for many real-world language-to-code applications, and heuristics cannot well capture the semantic features of the execution results, such as data type and value range, which often indicates the correctness of the program. In this work, we propose LEVER, a simple approach to improve language-to-code generation by learning to verify the generated programs with their execution results. Specifically, we train verifiers to determine whether a program sampled from the LLMs is correct or not based on the natural language input, the program itself and its execution results. The sampled programs are reranked by combining the verification score with the LLM generati",
    "link": "http://arxiv.org/abs/2302.08468",
    "context": "Title: LEVER: Learning to Verify Language-to-Code Generation with Execution. (arXiv:2302.08468v2 [cs.LG] UPDATED)\nAbstract: The advent of large language models trained on code (code LLMs) has led to significant progress in language-to-code generation. State-of-the-art approaches in this area combine LLM decoding with sample pruning and reranking using test cases or heuristics based on the execution results. However, it is challenging to obtain test cases for many real-world language-to-code applications, and heuristics cannot well capture the semantic features of the execution results, such as data type and value range, which often indicates the correctness of the program. In this work, we propose LEVER, a simple approach to improve language-to-code generation by learning to verify the generated programs with their execution results. Specifically, we train verifiers to determine whether a program sampled from the LLMs is correct or not based on the natural language input, the program itself and its execution results. The sampled programs are reranked by combining the verification score with the LLM generati",
    "path": "papers/23/02/2302.08468.json",
    "total_tokens": 868,
    "translated_title": "LEVER: 使用执行进行语言到代码生成的学习验证",
    "translated_abstract": "训练在代码上的大型语言模型（code LLMs）的出现，已经在语言到代码生成方面取得了显著进展。此领域的最新方法将LLM解码与使用测试用例或基于执行结果的启发式方法的样本修剪和重新排序相结合。然而，对于许多现实世界的语言到代码应用来说，获取测试用例是具有挑战性的，而启发式方法不能很好地捕捉执行结果的语义特征，比如数据类型和值范围，这往往表明程序的正确性。在这项工作中，我们提出了LEVER，一种通过学习使用执行结果来验证生成的程序，从而改进语言到代码生成的简单方法。具体地说，我们训练验证器根据自然语言输入、程序本身和执行结果来确定从LLM中抽样的程序是否正确。通过将验证分数与LLM生成分数相结合，对抽样的程序进行重新排序。",
    "tldr": "提出了一种使用执行结果来验证生成的程序的简单方法LEVER，通过训练验证器根据自然语言输入、程序本身和执行结果来确定程序的正确性，从而改进了语言到代码生成的过程。"
}