{
    "title": "Graph Neural Networks with Learnable and Optimal Polynomial Bases. (arXiv:2302.12432v2 [cs.LG] UPDATED)",
    "abstract": "Polynomial filters, a kind of Graph Neural Networks, typically use a predetermined polynomial basis and learn the coefficients from the training data. It has been observed that the effectiveness of the model is highly dependent on the property of the polynomial basis. Consequently, two natural and fundamental questions arise: Can we learn a suitable polynomial basis from the training data? Can we determine the optimal polynomial basis for a given graph and node features?  In this paper, we propose two spectral GNN models that provide positive answers to the questions posed above. First, inspired by Favard's Theorem, we propose the FavardGNN model, which learns a polynomial basis from the space of all possible orthonormal bases. Second, we examine the supposedly unsolvable definition of optimal polynomial basis from Wang & Zhang (2022) and propose a simple model, OptBasisGNN, which computes the optimal basis for a given graph structure and graph signal. Extensive experiments are conduct",
    "link": "http://arxiv.org/abs/2302.12432",
    "context": "Title: Graph Neural Networks with Learnable and Optimal Polynomial Bases. (arXiv:2302.12432v2 [cs.LG] UPDATED)\nAbstract: Polynomial filters, a kind of Graph Neural Networks, typically use a predetermined polynomial basis and learn the coefficients from the training data. It has been observed that the effectiveness of the model is highly dependent on the property of the polynomial basis. Consequently, two natural and fundamental questions arise: Can we learn a suitable polynomial basis from the training data? Can we determine the optimal polynomial basis for a given graph and node features?  In this paper, we propose two spectral GNN models that provide positive answers to the questions posed above. First, inspired by Favard's Theorem, we propose the FavardGNN model, which learns a polynomial basis from the space of all possible orthonormal bases. Second, we examine the supposedly unsolvable definition of optimal polynomial basis from Wang & Zhang (2022) and propose a simple model, OptBasisGNN, which computes the optimal basis for a given graph structure and graph signal. Extensive experiments are conduct",
    "path": "papers/23/02/2302.12432.json",
    "total_tokens": 823,
    "translated_title": "具有可学习和最优多项式基函数的图神经网络",
    "translated_abstract": "多项式滤波器是一种图神经网络，通常使用预定的多项式基函数，并从训练数据中学习系数。然而，模型的有效性很大程度上取决于多项式基函数的性质。因此，我们提出了两种谱图神经网络模型，它们能够肯定回答上述问题。首先，受到Favard定理的启发，我们提出了FavardGNN模型，该模型从所有可能的正交基函数空间中学习多项式基函数。其次，我们研究了Wang和Zhang（2022年）提出的所谓无法解决的最优多项式基函数的定义，并提出了一个简单模型OptBasisGNN，可计算给定图结构和图信号的最优基函数。进行了大量实验验证。",
    "tldr": "本文提出了两种具有可学习和最优多项式基函数的谱图神经网络模型，通过学习多项式基函数和计算最优基函数，解决了多项式滤波器在模型有效性方面的问题。",
    "en_tdlr": "This paper proposes two spectral graph neural network models with learnable and optimal polynomial bases, which address the issues regarding the effectiveness of polynomial filters by learning the bases and computing the optimal basis."
}