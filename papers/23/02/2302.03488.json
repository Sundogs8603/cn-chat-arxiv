{
    "title": "APAM: Adaptive Pre-training and Adaptive Meta Learning in Language Model for Noisy Labels and Long-tailed Learning. (arXiv:2302.03488v2 [cs.CL] UPDATED)",
    "abstract": "Practical natural language processing (NLP) tasks are commonly long-tailed with noisy labels. Those problems challenge the generalization and robustness of complex models such as Deep Neural Networks (DNNs). Some commonly used resampling techniques, such as oversampling or undersampling, could easily lead to overfitting. It is growing popular to learn the data weights leveraging a small amount of metadata. Besides, recent studies have shown the advantages of self-supervised pre-training, particularly to the under-represented data. In this work, we propose a general framework to handle the problem of both long-tail and noisy labels. The model is adapted to the domain of problems in a contrastive learning manner. The re-weighting module is a feed-forward network that learns explicit weighting functions and adapts weights according to metadata. The framework further adapts weights of terms in the loss function through a combination of the polynomial expansion of cross-entropy loss and foc",
    "link": "http://arxiv.org/abs/2302.03488",
    "context": "Title: APAM: Adaptive Pre-training and Adaptive Meta Learning in Language Model for Noisy Labels and Long-tailed Learning. (arXiv:2302.03488v2 [cs.CL] UPDATED)\nAbstract: Practical natural language processing (NLP) tasks are commonly long-tailed with noisy labels. Those problems challenge the generalization and robustness of complex models such as Deep Neural Networks (DNNs). Some commonly used resampling techniques, such as oversampling or undersampling, could easily lead to overfitting. It is growing popular to learn the data weights leveraging a small amount of metadata. Besides, recent studies have shown the advantages of self-supervised pre-training, particularly to the under-represented data. In this work, we propose a general framework to handle the problem of both long-tail and noisy labels. The model is adapted to the domain of problems in a contrastive learning manner. The re-weighting module is a feed-forward network that learns explicit weighting functions and adapts weights according to metadata. The framework further adapts weights of terms in the loss function through a combination of the polynomial expansion of cross-entropy loss and foc",
    "path": "papers/23/02/2302.03488.json",
    "total_tokens": 1073,
    "translated_title": "APAM：自适应预训练和自适应元学习在语言模型中用于处理噪声标签和长尾学习",
    "translated_abstract": "实际的自然语言处理 (NLP) 任务通常具有噪声标签和长尾分布的特点，这些问题会挑战复杂模型（如深度神经网络）的泛化和鲁棒性。常用的重抽样技术（如过采样或欠采样）容易导致过拟合。借助少量元数据学习数据权重正变得越来越流行。此外，最近的研究表明，自监督预训练对于弱表示数据的优点愈发明显。本文提出了一个通用的框架来处理长尾和噪声标签问题。该模型采用对比学习的方式适应问题域，并采用一种前馈神经网络的重新加权模块，该模块可以学习显式加权函数并根据元数据进行适应。我们在损失函数的项权重上进一步采用了交叉熵损失的多项式扩展和重点正则化的组合。此外，我们还将自适应元学习方法整合到预训练阶段中，以从弱表示类中学习更具可传递性的表示。实验表明，我们的方法在各种长尾和噪音标签分类任务中优于现有的最新方法。",
    "tldr": "本文提出了APAM框架，通过整合自适应预训练和元学习的方法，成功解决了长尾和噪声标签带来的挑战，实验证明该方法的效果优于现有最新方法。",
    "en_tdlr": "The APAM framework proposed in this paper successfully addresses the challenges brought about by long-tail and noisy labels by integrating the methods of adaptive pre-training and adaptive meta-learning, and experimental results show that the proposed method outperforms the state-of-the-art methods."
}