{
    "title": "A Simple Zero-shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models. (arXiv:2302.06235v2 [cs.LG] UPDATED)",
    "abstract": "Contrastively trained text-image models have the remarkable ability to perform zero-shot classification, that is, classifying previously unseen images into categories that the model has never been explicitly trained to identify. However, these zero-shot classifiers need prompt engineering to achieve high accuracy. Prompt engineering typically requires hand-crafting a set of prompts for individual downstream tasks. In this work, we aim to automate this prompt engineering and improve zero-shot accuracy through prompt ensembling. In particular, we ask \"Given a large pool of prompts, can we automatically score the prompts and ensemble those that are most suitable for a particular downstream dataset, without needing access to labeled validation data?\". We demonstrate that this is possible. In doing so, we identify several pathologies in a naive prompt scoring method where the score can be easily overconfident due to biases in pre-training and test data, and we propose a novel prompt scoring",
    "link": "http://arxiv.org/abs/2302.06235",
    "context": "Title: A Simple Zero-shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models. (arXiv:2302.06235v2 [cs.LG] UPDATED)\nAbstract: Contrastively trained text-image models have the remarkable ability to perform zero-shot classification, that is, classifying previously unseen images into categories that the model has never been explicitly trained to identify. However, these zero-shot classifiers need prompt engineering to achieve high accuracy. Prompt engineering typically requires hand-crafting a set of prompts for individual downstream tasks. In this work, we aim to automate this prompt engineering and improve zero-shot accuracy through prompt ensembling. In particular, we ask \"Given a large pool of prompts, can we automatically score the prompts and ensemble those that are most suitable for a particular downstream dataset, without needing access to labeled validation data?\". We demonstrate that this is possible. In doing so, we identify several pathologies in a naive prompt scoring method where the score can be easily overconfident due to biases in pre-training and test data, and we propose a novel prompt scoring",
    "path": "papers/23/02/2302.06235.json",
    "total_tokens": 847,
    "translated_title": "一种简单的零样本提示加权技术，以改善文本-图像模型中的提示集成",
    "translated_abstract": "对比训练的文本-图像模型具有显著的零样本分类能力，即将以前未见过的图像分类为模型从未明确训练过的类别。然而，这些零样本分类器需要提示工程来达到高准确性。提示工程通常需要手工创建一组用于个别下游任务的提示。在这项工作中，我们的目标是通过提示集成来自动化这个提示工程，并提高零样本准确性。具体而言，我们提出了一个问题：“给定大量的提示，我们是否可以自动评分提示并集成那些对特定下游数据集最合适的提示，而无需访问有标签的验证数据？”我们证明这是可能的。在这样做的过程中，我们确定了一个天真的提示评分方法中的几个病理问题，其中分数很容易因预训练和测试数据中的偏见而过于自信，我们提出了一种新颖的提示评分方法。",
    "tldr": "这项工作提出了一种简单的零样本提示加权技术，通过提示集成来自动化提示工程，从而提高文本-图像模型的零样本分类准确性。"
}