{
    "title": "mSAM: Micro-Batch-Averaged Sharpness-Aware Minimization. (arXiv:2302.09693v2 [stat.ML] UPDATED)",
    "abstract": "Modern deep learning models are over-parameterized, where different optima can result in widely varying generalization performance. The Sharpness-Aware Minimization (SAM) technique modifies the fundamental loss function that steers gradient descent methods toward flatter minima, which are believed to exhibit enhanced generalization prowess. Our study delves into a specific variant of SAM known as micro-batch SAM (mSAM). This variation involves aggregating updates derived from adversarial perturbations across multiple shards (micro-batches) of a mini-batch during training. We extend a recently developed and well-studied general framework for flatness analysis to theoretically show that SAM achieves flatter minima than SGD, and mSAM achieves even flatter minima than SAM. We provide a thorough empirical evaluation of various image classification and natural language processing tasks to substantiate this theoretical advancement. We also show that contrary to previous work, mSAM can be impl",
    "link": "http://arxiv.org/abs/2302.09693",
    "context": "Title: mSAM: Micro-Batch-Averaged Sharpness-Aware Minimization. (arXiv:2302.09693v2 [stat.ML] UPDATED)\nAbstract: Modern deep learning models are over-parameterized, where different optima can result in widely varying generalization performance. The Sharpness-Aware Minimization (SAM) technique modifies the fundamental loss function that steers gradient descent methods toward flatter minima, which are believed to exhibit enhanced generalization prowess. Our study delves into a specific variant of SAM known as micro-batch SAM (mSAM). This variation involves aggregating updates derived from adversarial perturbations across multiple shards (micro-batches) of a mini-batch during training. We extend a recently developed and well-studied general framework for flatness analysis to theoretically show that SAM achieves flatter minima than SGD, and mSAM achieves even flatter minima than SAM. We provide a thorough empirical evaluation of various image classification and natural language processing tasks to substantiate this theoretical advancement. We also show that contrary to previous work, mSAM can be impl",
    "path": "papers/23/02/2302.09693.json",
    "total_tokens": 923,
    "translated_title": "mSAM: 微批量平均锐度感知最小化",
    "translated_abstract": "现代深度学习模型是过参数化的，不同的极值可能导致广泛变化的泛化性能。锐度感知最小化（Sharpness-Aware Minimization，SAM）技术修改了基本的损失函数，使随机梯度下降方法朝着更平的极小值点前进，这被认为能够展现出增强的泛化能力。我们的研究深入探讨了一种特定的SAM变体，即微批量SAM（mSAM）。这种变体在训练过程中通过对来自多个分片（微批量）的对抗性扰动得到的更新进行聚合。我们将最近开发和研究的用于平坦性分析的通用框架扩展到理论上证明SAM实现了比随机梯度下降更平的极小值点，而mSAM比SAM实现了更加平坦的极小值点。我们对各种图像分类和自然语言处理任务进行了彻底的实证评估以验证这一理论进展。我们还表明，与以前的工作相反，mSAM可以被实现。",
    "tldr": "mSAM是一种深度学习优化方法，通过在训练过程中聚合对抗性扰动得到的更新，从理论上证明了比传统方法更平的极小值点，实验证实了其在各种任务上的优越性能。",
    "en_tdlr": "mSAM is a deep learning optimization method that aggregates updates from adversarial perturbations during training, theoretically demonstrating flatter minima than traditional methods and empirically showing superior performance in various tasks."
}