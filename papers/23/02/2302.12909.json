{
    "title": "Differentially Private Algorithms for the Stochastic Saddle Point Problem with Optimal Rates for the Strong Gap. (arXiv:2302.12909v2 [cs.LG] UPDATED)",
    "abstract": "We show that convex-concave Lipschitz stochastic saddle point problems (also known as stochastic minimax optimization) can be solved under the constraint of $(\\epsilon,\\delta)$-differential privacy with \\emph{strong (primal-dual) gap} rate of $\\tilde O\\big(\\frac{1}{\\sqrt{n}} + \\frac{\\sqrt{d}}{n\\epsilon}\\big)$, where $n$ is the dataset size and $d$ is the dimension of the problem. This rate is nearly optimal, based on existing lower bounds in differentially private stochastic optimization. Specifically, we prove a tight upper bound on the strong gap via novel implementation and analysis of the recursive regularization technique repurposed for saddle point problems. We show that this rate can be attained with $O\\big(\\min\\big\\{\\frac{n^2\\epsilon^{1.5}}{\\sqrt{d}}, n^{3/2}\\big\\}\\big)$ gradient complexity, and $\\tilde{O}(n)$ gradient complexity if the loss function is smooth. As a byproduct of our method, we develop a general algorithm that, given a black-box access to a subroutine satisfying",
    "link": "http://arxiv.org/abs/2302.12909",
    "context": "Title: Differentially Private Algorithms for the Stochastic Saddle Point Problem with Optimal Rates for the Strong Gap. (arXiv:2302.12909v2 [cs.LG] UPDATED)\nAbstract: We show that convex-concave Lipschitz stochastic saddle point problems (also known as stochastic minimax optimization) can be solved under the constraint of $(\\epsilon,\\delta)$-differential privacy with \\emph{strong (primal-dual) gap} rate of $\\tilde O\\big(\\frac{1}{\\sqrt{n}} + \\frac{\\sqrt{d}}{n\\epsilon}\\big)$, where $n$ is the dataset size and $d$ is the dimension of the problem. This rate is nearly optimal, based on existing lower bounds in differentially private stochastic optimization. Specifically, we prove a tight upper bound on the strong gap via novel implementation and analysis of the recursive regularization technique repurposed for saddle point problems. We show that this rate can be attained with $O\\big(\\min\\big\\{\\frac{n^2\\epsilon^{1.5}}{\\sqrt{d}}, n^{3/2}\\big\\}\\big)$ gradient complexity, and $\\tilde{O}(n)$ gradient complexity if the loss function is smooth. As a byproduct of our method, we develop a general algorithm that, given a black-box access to a subroutine satisfying",
    "path": "papers/23/02/2302.12909.json",
    "total_tokens": 964,
    "translated_title": "具有最佳速率的具有强间隙的差分隐私算法Saddle Point问题的研究",
    "translated_abstract": "我们展示了在$(\\epsilon,\\delta)$-差分隐私约束下，凸凹Lipschitz随机Saddle Point问题（也称为随机极小极大优化）可以被解决，其具有强（原始-对偶）间隙率为$\\tilde O\\big(\\frac{1}{\\sqrt{n}} + \\frac{\\sqrt{d}}{n\\epsilon}\\big)$，其中$n$为数据集大小，$d$为问题维度。根据现有的差分隐私随机优化的下界，该速率几乎是最优的。具体来说，我们通过重新设计并分析适用于Saddle Point问题的递归正则化技术，证明了强间隙的紧密上界。我们展示了该速率可以在$O\\big(\\min\\big\\{\\frac{n^2\\epsilon^{1.5}}{\\sqrt{d}}, n^{3/2}\\big\\}\\big)$的梯度复杂度以及在损失函数光滑的情况下，$\\tilde{O}(n)$的梯度复杂度下实现。作为我们方法的副产品，我们开发了一个通用算法，给定黑盒访问一个满足条件的子程序。",
    "tldr": "本研究提出了一种在差分隐私约束下解决convex-concave Lipschitz随机Saddle Point问题的方法，并证明了在满足条件的情况下，该方法具有最佳速率和梯度复杂度。"
}