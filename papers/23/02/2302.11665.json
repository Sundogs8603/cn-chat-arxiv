{
    "title": "AlpaServe: Statistical Multiplexing with Model Parallelism for Deep Learning Serving. (arXiv:2302.11665v2 [cs.LG] UPDATED)",
    "abstract": "Model parallelism is conventionally viewed as a method to scale a single large deep learning model beyond the memory limits of a single device. In this paper, we demonstrate that model parallelism can be additionally used for the statistical multiplexing of multiple devices when serving multiple models, even when a single model can fit into a single device. Our work reveals a fundamental trade-off between the overhead introduced by model parallelism and the opportunity to exploit statistical multiplexing to reduce serving latency in the presence of bursty workloads. We explore the new trade-off space and present a novel serving system, AlpaServe, that determines an efficient strategy for placing and parallelizing collections of large deep learning models across a distributed cluster. Evaluation results on production workloads show that AlpaServe can process requests at up to 10x higher rates or 6x more burstiness while staying within latency constraints for more than 99% of requests.",
    "link": "http://arxiv.org/abs/2302.11665",
    "context": "Title: AlpaServe: Statistical Multiplexing with Model Parallelism for Deep Learning Serving. (arXiv:2302.11665v2 [cs.LG] UPDATED)\nAbstract: Model parallelism is conventionally viewed as a method to scale a single large deep learning model beyond the memory limits of a single device. In this paper, we demonstrate that model parallelism can be additionally used for the statistical multiplexing of multiple devices when serving multiple models, even when a single model can fit into a single device. Our work reveals a fundamental trade-off between the overhead introduced by model parallelism and the opportunity to exploit statistical multiplexing to reduce serving latency in the presence of bursty workloads. We explore the new trade-off space and present a novel serving system, AlpaServe, that determines an efficient strategy for placing and parallelizing collections of large deep learning models across a distributed cluster. Evaluation results on production workloads show that AlpaServe can process requests at up to 10x higher rates or 6x more burstiness while staying within latency constraints for more than 99% of requests.",
    "path": "papers/23/02/2302.11665.json",
    "total_tokens": 800,
    "translated_title": "AlpaServe：用于深度学习服务的模型并行的统计复用",
    "translated_abstract": "传统上，模型并行被视为一种将单个大型深度学习模型扩展到单个设备内存限制之外的方法。本文证明了即使单个模型可以适应单个设备，模型并行还可以用于多个模型的统计复用，以降低提供服务时的延迟。我们探索了新的权衡空间，并提出了一种新颖的服务系统AlpaServe，它确定了在分布式集群中放置和并行处理大型深度学习模型集合的高效策略。生产工作负载上的评估结果显示，AlpaServe可以在满足超过99%请求的延迟约束的同时，以高达10倍的速率处理请求或者处理6倍以上的突发负载。",
    "tldr": "AlpaServe是一种新颖的服务系统，它利用模型并行和统计复用，在提供多个模型服务时降低延迟，提高处理速率和突发负载处理能力。",
    "en_tdlr": "AlpaServe is a novel serving system that utilizes model parallelism and statistical multiplexing to reduce latency, increase processing rates, and improve burst load handling when serving multiple models."
}