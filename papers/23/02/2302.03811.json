{
    "title": "On the Convergence of Modified Policy Iteration in Risk Sensitive Exponential Cost Markov Decision Processes",
    "abstract": "arXiv:2302.03811v2 Announce Type: replace-cross  Abstract: Modified policy iteration (MPI) is a dynamic programming algorithm that combines elements of policy iteration and value iteration. The convergence of MPI has been well studied in the context of discounted and average-cost MDPs. In this work, we consider the exponential cost risk-sensitive MDP formulation, which is known to provide some robustness to model parameters. Although policy iteration and value iteration have been well studied in the context of risk sensitive MDPs, MPI is unexplored. We provide the first proof that MPI also converges for the risk-sensitive problem in the case of finite state and action spaces. Since the exponential cost formulation deals with the multiplicative Bellman equation, our main contribution is a convergence proof which is quite different than existing results for discounted and risk-neutral average-cost problems as well as risk sensitive value and policy iteration approaches. We conclude our a",
    "link": "https://arxiv.org/abs/2302.03811",
    "context": "Title: On the Convergence of Modified Policy Iteration in Risk Sensitive Exponential Cost Markov Decision Processes\nAbstract: arXiv:2302.03811v2 Announce Type: replace-cross  Abstract: Modified policy iteration (MPI) is a dynamic programming algorithm that combines elements of policy iteration and value iteration. The convergence of MPI has been well studied in the context of discounted and average-cost MDPs. In this work, we consider the exponential cost risk-sensitive MDP formulation, which is known to provide some robustness to model parameters. Although policy iteration and value iteration have been well studied in the context of risk sensitive MDPs, MPI is unexplored. We provide the first proof that MPI also converges for the risk-sensitive problem in the case of finite state and action spaces. Since the exponential cost formulation deals with the multiplicative Bellman equation, our main contribution is a convergence proof which is quite different than existing results for discounted and risk-neutral average-cost problems as well as risk sensitive value and policy iteration approaches. We conclude our a",
    "path": "papers/23/02/2302.03811.json",
    "total_tokens": 891,
    "translated_title": "关于风险敏感指数成本马尔可夫决策过程中修改的策略迭代的收敛性的研究",
    "translated_abstract": "修改的策略迭代（MPI）是一种将策略迭代和值迭代相结合的动态规划算法。MPI的收敛性在折扣和平均成本MDP的背景下已经得到了广泛研究。本文研究了指数成本风险敏感MDP的形式，该形式对模型参数具有一定的鲁棒性。虽然针对风险敏感MDP已经对策略迭代和值迭代进行了深入研究，但MPI却未被探索。我们首次证明了在有限状态和动作空间的情况下，MPI也对风险敏感问题收敛。由于指数成本形式涉及乘法贝尔曼方程，我们的主要贡献是一种与折扣和风险中立平均成本问题以及风险敏感值和策略迭代方法不同的收敛证明。我们总结了我们的一个",
    "tldr": "这项研究证明了在有限状态和动作空间的情况下，修改的策略迭代算法（MPI）在风险敏感问题中的收敛性，并提供了与已有结果不同的证明方法。"
}