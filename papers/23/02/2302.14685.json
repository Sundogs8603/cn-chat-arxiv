{
    "title": "DART: Diversify-Aggregate-Repeat Training Improves Generalization of Neural Networks. (arXiv:2302.14685v2 [cs.LG] UPDATED)",
    "abstract": "Generalization of neural networks is crucial for deploying them safely in the real world. Common training strategies to improve generalization involve the use of data augmentations, ensembling and model averaging. In this work, we first establish a surprisingly simple but strong benchmark for generalization which utilizes diverse augmentations within a training minibatch, and show that this can learn a more balanced distribution of features. Further, we propose Diversify-Aggregate-Repeat Training (DART) strategy that first trains diverse models using different augmentations (or domains) to explore the loss basin, and further Aggregates their weights to combine their expertise and obtain improved generalization. We find that Repeating the step of Aggregation throughout training improves the overall optimization trajectory and also ensures that the individual models have a sufficiently low loss barrier to obtain improved generalization on combining them. We shed light on our approach by ",
    "link": "http://arxiv.org/abs/2302.14685",
    "context": "Title: DART: Diversify-Aggregate-Repeat Training Improves Generalization of Neural Networks. (arXiv:2302.14685v2 [cs.LG] UPDATED)\nAbstract: Generalization of neural networks is crucial for deploying them safely in the real world. Common training strategies to improve generalization involve the use of data augmentations, ensembling and model averaging. In this work, we first establish a surprisingly simple but strong benchmark for generalization which utilizes diverse augmentations within a training minibatch, and show that this can learn a more balanced distribution of features. Further, we propose Diversify-Aggregate-Repeat Training (DART) strategy that first trains diverse models using different augmentations (or domains) to explore the loss basin, and further Aggregates their weights to combine their expertise and obtain improved generalization. We find that Repeating the step of Aggregation throughout training improves the overall optimization trajectory and also ensures that the individual models have a sufficiently low loss barrier to obtain improved generalization on combining them. We shed light on our approach by ",
    "path": "papers/23/02/2302.14685.json",
    "total_tokens": 1130,
    "translated_title": "DART: 多样化聚合重复训练改进神经网络的泛化能力",
    "translated_abstract": "神经网络的泛化能力对于在现实世界中安全部署它们至关重要。改进泛化的常见训练策略包括使用数据增强、集成和模型平均化。在本文中，我们首先建立了一个惊人简单但强有力的泛化基准，它利用了训练小批量中的多种不同增强方法，并证明这可以学习到一个更平衡的特征分布。进一步地，我们提出了Diversify-Aggregate-Repeat Training (DART)策略，该策略首先使用不同的增强方法(或领域)训练不同的模型，以探索损失盆地，并进一步聚合它们的权重，结合它们的专业知识并获得改进的泛化能力。我们发现，在整个训练过程中重复聚合步骤可以提高整体优化轨迹，并确保单个模型具有足够低的损失障碍，在将它们组合时可以获得改进的泛化能力。我们解释了我们的方法是一种正则化形式，它迫使模型探索损失景观的多种模式。我们的实验表明，DART在CIFAR-10和CIFAR-100上实现了最先进的性能，并在ImageNet和鲁棒性基准测试中取得了有竞争力的结果。",
    "tldr": "本文中提出了DART策略，其中利用多样化的增强方法训练不同的模型，然后通过聚合这些模型的权重来结合其专业知识，并重复聚合步骤以实现更好的泛化能力。",
    "en_tdlr": "The Diversify-Aggregate-Repeat Training (DART) strategy is proposed in this paper which trains different models using diverse augmentations or domains and aggregates their weights to combine their expertise for improved generalization. The approach is interpreted as a form of regularization that forces the models to explore diverse modes of the loss landscape. DART achieved state-of-the-art performance on CIFAR-10 and CIFAR-100 and competitive results on ImageNet and robustness benchmarks."
}