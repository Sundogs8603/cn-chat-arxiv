{
    "title": "Utility-based Perturbed Gradient Descent: An Optimizer for Continual Learning. (arXiv:2302.03281v2 [cs.LG] UPDATED)",
    "abstract": "Modern representation learning methods often struggle to adapt quickly under non-stationarity because they suffer from catastrophic forgetting and decaying plasticity. Such problems prevent learners from fast adaptation since they may forget useful features or have difficulty learning new ones. Hence, these methods are rendered ineffective for continual learning. This paper proposes Utility-based Perturbed Gradient Descent (UPGD), an online learning algorithm well-suited for continual learning agents. UPGD protects useful weights or features from forgetting and perturbs less useful ones based on their utilities. Our empirical results show that UPGD helps reduce forgetting and maintain plasticity, enabling modern representation learning methods to work effectively in continual learning.",
    "link": "http://arxiv.org/abs/2302.03281",
    "context": "Title: Utility-based Perturbed Gradient Descent: An Optimizer for Continual Learning. (arXiv:2302.03281v2 [cs.LG] UPDATED)\nAbstract: Modern representation learning methods often struggle to adapt quickly under non-stationarity because they suffer from catastrophic forgetting and decaying plasticity. Such problems prevent learners from fast adaptation since they may forget useful features or have difficulty learning new ones. Hence, these methods are rendered ineffective for continual learning. This paper proposes Utility-based Perturbed Gradient Descent (UPGD), an online learning algorithm well-suited for continual learning agents. UPGD protects useful weights or features from forgetting and perturbs less useful ones based on their utilities. Our empirical results show that UPGD helps reduce forgetting and maintain plasticity, enabling modern representation learning methods to work effectively in continual learning.",
    "path": "papers/23/02/2302.03281.json",
    "total_tokens": 895,
    "translated_title": "基于效用的扰动梯度下降：一种连续学习优化器。",
    "translated_abstract": "现代表示学习方法在面对非稳态问题时往往难以快速适应，因为它们遭受灾难性遗忘和衰减的可塑性。这些问题阻碍了学习者的快速适应，因为他们可能会遗忘有用的特征或难以学习新的特征。因此，这些方法在连续学习中变得无效。本文提出了一种在线学习算法——基于效用的扰动梯度下降（UPGD），这种算法非常适合连续学习代理。UPGD保护有用的权重或特征不被遗忘，并基于它们的效用扰动不太有用的权重或特征。我们的实证结果表明，UPGD有助于减少遗忘和保持可塑性，使现代表示学习方法在连续学习中有效地工作。",
    "tldr": "本文提出了一种在线学习算法——基于效用的扰动梯度下降（UPGD），该算法可保护有用的权重或特征，并基于它们的效用扰动不太有用的权重或特征。实验证明，UPGD有助于减少遗忘和保持可塑性，在连续学习中大有用处。",
    "en_tdlr": "This paper proposes Utility-based Perturbed Gradient Descent (UPGD), an online learning algorithm well-suited for continual learning agents. UPGD protects useful weights or features from forgetting and perturbs less useful ones based on their utilities. Empirical results show that UPGD helps reduce forgetting and maintain plasticity, enabling modern representation learning methods to work effectively in continual learning."
}