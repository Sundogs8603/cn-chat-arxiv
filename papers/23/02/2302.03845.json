{
    "title": "Two-step hyperparameter optimization method: Accelerating hyperparameter search by using a fraction of a training dataset. (arXiv:2302.03845v2 [cs.LG] UPDATED)",
    "abstract": "Hyperparameter optimization (HPO) is an important step in machine learning (ML) model development, but common practices are archaic -- primarily relying on manual or grid searches. This is partly because adopting advanced HPO algorithms introduces added complexity to the workflow, leading to longer computation times. This poses a notable challenge to ML applications, as suboptimal hyperparameter selections curtail the potential of ML model performance, ultimately obstructing the full exploitation of ML techniques. In this article, we present a two-step HPO method as a strategic solution to curbing computational demands and wait times, gleaned from practical experiences in applied ML parameterization work. The initial phase involves a preliminary evaluation of hyperparameters on a small subset of the training dataset, followed by a re-evaluation of the top-performing candidate models post-retraining with the entire training dataset. This two-step HPO method is universally applicable acr",
    "link": "http://arxiv.org/abs/2302.03845",
    "context": "Title: Two-step hyperparameter optimization method: Accelerating hyperparameter search by using a fraction of a training dataset. (arXiv:2302.03845v2 [cs.LG] UPDATED)\nAbstract: Hyperparameter optimization (HPO) is an important step in machine learning (ML) model development, but common practices are archaic -- primarily relying on manual or grid searches. This is partly because adopting advanced HPO algorithms introduces added complexity to the workflow, leading to longer computation times. This poses a notable challenge to ML applications, as suboptimal hyperparameter selections curtail the potential of ML model performance, ultimately obstructing the full exploitation of ML techniques. In this article, we present a two-step HPO method as a strategic solution to curbing computational demands and wait times, gleaned from practical experiences in applied ML parameterization work. The initial phase involves a preliminary evaluation of hyperparameters on a small subset of the training dataset, followed by a re-evaluation of the top-performing candidate models post-retraining with the entire training dataset. This two-step HPO method is universally applicable acr",
    "path": "papers/23/02/2302.03845.json",
    "total_tokens": 872,
    "translated_title": "使用训练数据集的一部分加速超参数搜索的两步超参数优化方法",
    "translated_abstract": "超参数优化(HPO)是机器学习(ML)模型开发中的重要步骤，但常见做法过时，主要依赖于手动或网格搜索。这部分是因为采用先进的HPO算法会引入额外的复杂性，导致计算时间更长。这对ML应用构成了显著挑战，因为次优的超参数选择限制了ML模型性能的潜力，最终阻碍了对ML技术的充分利用。在本文中，我们提出了一种两步HPO方法作为解决计算需求和等待时间的战略性方案，这是从应用ML参数化工作的实践经验中得到的。初始阶段涉及对训练数据集的一个小子集上的超参数进行初步评估，然后在整个训练数据集上重新训练并重新评估最佳候选模型。这种两步HPO方法是普适的",
    "tldr": "这项研究提出了一种两步HPO方法，通过在训练数据集的一部分上进行预评估，然后在整个训练数据集上重新训练和评估，实现了加速超参数搜索的目的",
    "en_tdlr": "This research proposes a two-step HPO method that accelerates hyperparameter search by conducting preliminary evaluation on a subset of the training dataset, followed by retraining and evaluation on the entire dataset."
}