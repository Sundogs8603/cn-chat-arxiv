{
    "title": "In-Context Retrieval-Augmented Language Models. (arXiv:2302.00083v2 [cs.CL] UPDATED)",
    "abstract": "Retrieval-Augmented Language Modeling (RALM) methods, which condition a language model (LM) on relevant documents from a grounding corpus during generation, were shown to significantly improve language modeling performance. In addition, they can mitigate the problem of factually inaccurate text generation and provide natural source attribution mechanism. Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment. This paper considers a simple alternative, which we dub In-Context RALM: leaving the LM architecture unchanged and prepending grounding documents to the input, without any further training of the LM. We show that In-Context RALM that builds on off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse corpora. We also demonstrate that the document retrieval and ranking mechanism can be specialized to the RALM setting to ",
    "link": "http://arxiv.org/abs/2302.00083",
    "context": "Title: In-Context Retrieval-Augmented Language Models. (arXiv:2302.00083v2 [cs.CL] UPDATED)\nAbstract: Retrieval-Augmented Language Modeling (RALM) methods, which condition a language model (LM) on relevant documents from a grounding corpus during generation, were shown to significantly improve language modeling performance. In addition, they can mitigate the problem of factually inaccurate text generation and provide natural source attribution mechanism. Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment. This paper considers a simple alternative, which we dub In-Context RALM: leaving the LM architecture unchanged and prepending grounding documents to the input, without any further training of the LM. We show that In-Context RALM that builds on off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse corpora. We also demonstrate that the document retrieval and ranking mechanism can be specialized to the RALM setting to ",
    "path": "papers/23/02/2302.00083.json",
    "total_tokens": 902,
    "translated_title": "上下文检索增强的语言模型",
    "translated_abstract": "检索增强的语言模型(RALM)方法在生成过程中，通过将相关文件从语料库中检索出来与语言模型(LM)进行协同，已被证明可以显著提高语言建模性能。此外，它们还可以缓解事实不准确的文本生成问题，并提供自然的源归因机制。现有的RALM方法着重于修改LM架构以便于整合外部信息，从而大大增加了部署的复杂性。本文提出了一种简单的替代方法，称为上下文RALM：保持LM架构不变，并在输入中添加检索到的文件，无需对LM进行任何进一步的训练。我们展示了基于现成的通用检索器的上下文RALM在模型大小和不同语料库中能够提供出人意料的大幅度的LM增益。我们还证明，文件检索和排名机制可以针对RALM设置进行专门优化。",
    "tldr": "本研究提出了一种上下文检索增强的语言模型（In-Context RALM）方法，通过将相关文件作为输入的一部分，无需对语言模型进行进一步的训练即可显著提高语言建模性能和源归因能力，并且相对于现有的RALM方法，它具有更简单的部署过程。"
}