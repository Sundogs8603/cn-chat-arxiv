{
    "title": "Reducing SO(3) Convolutions to SO(2) for Efficient Equivariant GNNs. (arXiv:2302.03655v2 [cs.LG] UPDATED)",
    "abstract": "Graph neural networks that model 3D data, such as point clouds or atoms, are typically desired to be $SO(3)$ equivariant, i.e., equivariant to 3D rotations. Unfortunately equivariant convolutions, which are a fundamental operation for equivariant networks, increase significantly in computational complexity as higher-order tensors are used. In this paper, we address this issue by reducing the $SO(3)$ convolutions or tensor products to mathematically equivalent convolutions in $SO(2)$ . This is accomplished by aligning the node embeddings' primary axis with the edge vectors, which sparsifies the tensor product and reduces the computational complexity from $O(L^6)$ to $O(L^3)$, where $L$ is the degree of the representation. We demonstrate the potential implications of this improvement by proposing the Equivariant Spherical Channel Network (eSCN), a graph neural network utilizing our novel approach to equivariant convolutions, which achieves state-of-the-art results on the large-scale OC-2",
    "link": "http://arxiv.org/abs/2302.03655",
    "context": "Title: Reducing SO(3) Convolutions to SO(2) for Efficient Equivariant GNNs. (arXiv:2302.03655v2 [cs.LG] UPDATED)\nAbstract: Graph neural networks that model 3D data, such as point clouds or atoms, are typically desired to be $SO(3)$ equivariant, i.e., equivariant to 3D rotations. Unfortunately equivariant convolutions, which are a fundamental operation for equivariant networks, increase significantly in computational complexity as higher-order tensors are used. In this paper, we address this issue by reducing the $SO(3)$ convolutions or tensor products to mathematically equivalent convolutions in $SO(2)$ . This is accomplished by aligning the node embeddings' primary axis with the edge vectors, which sparsifies the tensor product and reduces the computational complexity from $O(L^6)$ to $O(L^3)$, where $L$ is the degree of the representation. We demonstrate the potential implications of this improvement by proposing the Equivariant Spherical Channel Network (eSCN), a graph neural network utilizing our novel approach to equivariant convolutions, which achieves state-of-the-art results on the large-scale OC-2",
    "path": "papers/23/02/2302.03655.json",
    "total_tokens": 924,
    "translated_title": "将SO(3)卷积降维至SO(2)以实现高效等变GNN",
    "translated_abstract": "模拟点云或原子等3D数据的图神经网络通常需要是SO(3)等变的，即对3D旋转等变。然而，等变卷积（是等变网络的基本操作）随着更高阶张量的使用，在计算复杂度上显著增加。本文通过将SO(3)卷积或张量积降维至SO(2)，从而将节点嵌入的主轴与边向量对齐，从而稀疏化张量积并将计算复杂度从O(L^6)降至O(L^3)，其中L为表示的度。通过提出利用我们新的等变卷积方法实现等变球形通道网络（eSCN）的图神经网络且在大规模OC-2数据集上获得最先进的结果，我们展示了这一改进的潜在影响。",
    "tldr": "本文将SO(3)卷积降维至SO(2)，以减少等变卷积在高阶张量上的计算复杂度，并通过提出的等变球形通道网络（eSCN）在大规模OC-2数据集上获得最先进的结果。",
    "en_tdlr": "This paper reduces the computational complexity of equivariant convolutions on higher-order tensors by reducing SO(3) convolutions or tensor products to mathematically equivalent convolutions in SO(2) through aligning node embeddings' primary axis with edge vectors. The proposed Equivariant Spherical Channel Network (eSCN) achieves state-of-the-art results on large-scale OC-2 dataset."
}