{
    "title": "Beyond the Universal Law of Robustness: Sharper Laws for Random Features and Neural Tangent Kernels. (arXiv:2302.01629v2 [stat.ML] UPDATED)",
    "abstract": "Machine learning models are vulnerable to adversarial perturbations, and a thought-provoking paper by Bubeck and Sellke has analyzed this phenomenon through the lens of over-parameterization: interpolating smoothly the data requires significantly more parameters than simply memorizing it. However, this \"universal\" law provides only a necessary condition for robustness, and it is unable to discriminate between models. In this paper, we address these gaps by focusing on empirical risk minimization in two prototypical settings, namely, random features and the neural tangent kernel (NTK). We prove that, for random features, the model is not robust for any degree of over-parameterization, even when the necessary condition coming from the universal law of robustness is satisfied. In contrast, for even activations, the NTK model meets the universal lower bound, and it is robust as soon as the necessary condition on over-parameterization is fulfilled. This also addresses a conjecture in prior ",
    "link": "http://arxiv.org/abs/2302.01629",
    "context": "Title: Beyond the Universal Law of Robustness: Sharper Laws for Random Features and Neural Tangent Kernels. (arXiv:2302.01629v2 [stat.ML] UPDATED)\nAbstract: Machine learning models are vulnerable to adversarial perturbations, and a thought-provoking paper by Bubeck and Sellke has analyzed this phenomenon through the lens of over-parameterization: interpolating smoothly the data requires significantly more parameters than simply memorizing it. However, this \"universal\" law provides only a necessary condition for robustness, and it is unable to discriminate between models. In this paper, we address these gaps by focusing on empirical risk minimization in two prototypical settings, namely, random features and the neural tangent kernel (NTK). We prove that, for random features, the model is not robust for any degree of over-parameterization, even when the necessary condition coming from the universal law of robustness is satisfied. In contrast, for even activations, the NTK model meets the universal lower bound, and it is robust as soon as the necessary condition on over-parameterization is fulfilled. This also addresses a conjecture in prior ",
    "path": "papers/23/02/2302.01629.json",
    "total_tokens": 1134,
    "translated_title": "超越稳健性的普适定律：随机特征和神经切向核的更尖锐法则",
    "translated_abstract": "机器学习模型容易受到对抗性干扰，Bubeck和Sellke的一个有思想启示的文章通过过度参数化的视角分析了这一现象：平滑地插值数据需要的参数显著多于简单地记忆数据。然而，“普适”的法则仅为稳健性提供了必要条件，无法区分模型。本文通过专注于随机特征和神经切向核（NTK）的两个典型设置中的经验风险最小化来解决这些差距。我们证明，在随机特征中，即使满足稳健性的通用定律所需的必要条件，模型也不具有任何过度参数化程度的稳健性。相反，对于偶激活情况，NTK模型满足普遍下限，只要满足过参数条件就能稳健。这也解决了先前在NTK架构的最优性上的猜想。我们的结果为机器学习中的稳健性提供了更尖锐的法则，超越了先前建立的普适定律。",
    "tldr": "本文通过研究随机特征和神经切向核（NTK）的经验风险最小化，证明了在随机特征中，即使满足稳健性的通用定律所需的必要条件，模型也不具有任何过度参数化程度的稳健性。相对地，对于偶激活情况，NTK模型满足普遍下限，只要满足过参数条件就能稳健。这为机器学习中的稳健性提供了更尖锐的法则，超越了先前建立的普适定律。",
    "en_tdlr": "This paper provides sharper laws for robustness in machine learning beyond the universal law by studying empirical risk minimization in random features and the neural tangent kernel (NTK). The results show that, in contrast to NTK models, even when the necessary condition from the universal law is satisfied, models in random features are not robust at any degree of over-parameterization."
}