{
    "title": "One-Shot Labeling for Automatic Relevance Estimation. (arXiv:2302.11266v2 [cs.IR] UPDATED)",
    "abstract": "Dealing with unjudged documents (\"holes\") in relevance assessments is a perennial problem when evaluating search systems with offline experiments. Holes can reduce the apparent effectiveness of retrieval systems during evaluation and introduce biases in models trained with incomplete data. In this work, we explore whether large language models can help us fill such holes to improve offline evaluations. We examine an extreme, albeit common, evaluation setting wherein only a single known relevant document per query is available for evaluation. We then explore various approaches for predicting the relevance of unjudged documents with respect to a query and the known relevant document, including nearest neighbor, supervised, and prompting techniques. We find that although the predictions of these One-Shot Labelers (1SL) frequently disagree with human assessments, the labels they produce yield a far more reliable ranking of systems than the single labels do alone. Specifically, the stronges",
    "link": "http://arxiv.org/abs/2302.11266",
    "context": "Title: One-Shot Labeling for Automatic Relevance Estimation. (arXiv:2302.11266v2 [cs.IR] UPDATED)\nAbstract: Dealing with unjudged documents (\"holes\") in relevance assessments is a perennial problem when evaluating search systems with offline experiments. Holes can reduce the apparent effectiveness of retrieval systems during evaluation and introduce biases in models trained with incomplete data. In this work, we explore whether large language models can help us fill such holes to improve offline evaluations. We examine an extreme, albeit common, evaluation setting wherein only a single known relevant document per query is available for evaluation. We then explore various approaches for predicting the relevance of unjudged documents with respect to a query and the known relevant document, including nearest neighbor, supervised, and prompting techniques. We find that although the predictions of these One-Shot Labelers (1SL) frequently disagree with human assessments, the labels they produce yield a far more reliable ranking of systems than the single labels do alone. Specifically, the stronges",
    "path": "papers/23/02/2302.11266.json",
    "total_tokens": 909,
    "translated_title": "自动相关性估计中的一次性标注",
    "translated_abstract": "在离线实验中评估搜索系统时，处理未经评定的文档（“空洞”）是一个长期存在的问题。空洞可能会降低评估中检索系统的表现效果，并在使用不完整数据进行训练的模型中引入偏差。在本研究中，我们探讨了是否可以利用大型语言模型来填补这些空洞，以提高离线评估的效果。我们研究了一种极端但常见的评估设置，即每个查询只有一个已知相关文档可用于评估。然后，我们探讨了各种方法来预测未经评定文档与查询和已知相关文档的相关性，包括最近邻、监督和提示技术。我们发现，尽管这些一次性标注器（1SL）的预测经常与人工评定不一致，但它们产生的标签比单独的标签更可靠地对系统进行排序。具体地说，最强的一次性标注器可以显著提高系统的排序效果。",
    "tldr": "本研究探索了利用大型语言模型来填补搜索系统离线评估过程中未评定文档的问题。研究结果表明，尽管预测结果与人工评定存在差异，但使用一次性标注器能够提供更可靠的系统排序效果。"
}