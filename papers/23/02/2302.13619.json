{
    "title": "Orca: A Few-shot Benchmark for Chinese Conversational Machine Reading Comprehension. (arXiv:2302.13619v2 [cs.CL] UPDATED)",
    "abstract": "The conversational machine reading comprehension (CMRC) task aims to answer questions in conversations, which has been a hot research topic in recent years because of its wide applications. However, existing CMRC benchmarks in which each conversation is assigned a static passage are inconsistent with real scenarios. Thus, model's comprehension ability towards real scenarios are hard to evaluate reasonably. To this end, we propose the first Chinese CMRC benchmark Orca and further provide zero-shot/few-shot settings to evaluate model's generalization ability towards diverse domains. We collect 831 hot-topic driven conversations with 4,742 turns in total. Each turn of a conversation is assigned with a response-related passage, aiming to evaluate model's comprehension ability more reasonably. The topics of conversations are collected from social media platform and cover 33 domains, trying to be consistent with real scenarios. Importantly, answers in Orca are all well-annotated natural resp",
    "link": "http://arxiv.org/abs/2302.13619",
    "context": "Title: Orca: A Few-shot Benchmark for Chinese Conversational Machine Reading Comprehension. (arXiv:2302.13619v2 [cs.CL] UPDATED)\nAbstract: The conversational machine reading comprehension (CMRC) task aims to answer questions in conversations, which has been a hot research topic in recent years because of its wide applications. However, existing CMRC benchmarks in which each conversation is assigned a static passage are inconsistent with real scenarios. Thus, model's comprehension ability towards real scenarios are hard to evaluate reasonably. To this end, we propose the first Chinese CMRC benchmark Orca and further provide zero-shot/few-shot settings to evaluate model's generalization ability towards diverse domains. We collect 831 hot-topic driven conversations with 4,742 turns in total. Each turn of a conversation is assigned with a response-related passage, aiming to evaluate model's comprehension ability more reasonably. The topics of conversations are collected from social media platform and cover 33 domains, trying to be consistent with real scenarios. Importantly, answers in Orca are all well-annotated natural resp",
    "path": "papers/23/02/2302.13619.json",
    "total_tokens": 915,
    "translated_title": "Orca: 一种用于中文对话机器阅读理解的少样本测试基准",
    "translated_abstract": "对话机器阅读理解（CMRC）任务旨在回答对话中的问题，由于其广泛应用，近年来已成为热门研究课题。然而，现有的CMRC基准在每个对话中分配一个静态段落，与真实场景不一致。因此，很难合理评估模型对真实场景的理解能力。为此，我们提出了第一个中文CMRC基准Orca，并进一步提供了零样本/少样本设置，以评估模型对多样领域的泛化能力。我们收集了831个热门话题驱动的对话，共计4,742轮。每个对话的每个轮次都会分配一个与回答有关的段落，旨在更合理地评估模型的理解能力。对话的主题来自社交媒体平台，涵盖33个领域，力争与真实场景保持一致。重要的是，Orca中的答案都是经过良好注释的自然回答。",
    "tldr": "Orca是中文对话机器阅读理解的第一个基准，提供了零样本/少样本设置来评估模型对多样领域的泛化能力，并通过提供与回答相关的段落来更合理地评估模型的理解能力。",
    "en_tdlr": "Orca is the first benchmark for Chinese conversational machine reading comprehension, providing zero-shot/few-shot settings to evaluate model's generalization ability across diverse domains, and offering response-related passages to more reasonably evaluate model's comprehension capability."
}