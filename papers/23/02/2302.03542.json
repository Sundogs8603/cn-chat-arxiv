{
    "title": "Two Losses Are Better Than One: Faster Optimization Using a Cheaper Proxy. (arXiv:2302.03542v2 [cs.LG] UPDATED)",
    "abstract": "We present an algorithm for minimizing an objective with hard-to-compute gradients by using a related, easier-to-access function as a proxy. Our algorithm is based on approximate proximal point iterations on the proxy combined with relatively few stochastic gradients from the objective. When the difference between the objective and the proxy is $\\delta$-smooth, our algorithm guarantees convergence at a rate matching stochastic gradient descent on a $\\delta$-smooth objective, which can lead to substantially better sample efficiency. Our algorithm has many potential applications in machine learning, and provides a principled means of leveraging synthetic data, physics simulators, mixed public and private data, and more.",
    "link": "http://arxiv.org/abs/2302.03542",
    "context": "Title: Two Losses Are Better Than One: Faster Optimization Using a Cheaper Proxy. (arXiv:2302.03542v2 [cs.LG] UPDATED)\nAbstract: We present an algorithm for minimizing an objective with hard-to-compute gradients by using a related, easier-to-access function as a proxy. Our algorithm is based on approximate proximal point iterations on the proxy combined with relatively few stochastic gradients from the objective. When the difference between the objective and the proxy is $\\delta$-smooth, our algorithm guarantees convergence at a rate matching stochastic gradient descent on a $\\delta$-smooth objective, which can lead to substantially better sample efficiency. Our algorithm has many potential applications in machine learning, and provides a principled means of leveraging synthetic data, physics simulators, mixed public and private data, and more.",
    "path": "papers/23/02/2302.03542.json",
    "total_tokens": 785,
    "translated_title": "两种损失比一种更好：使用更便宜的代理加快优化",
    "translated_abstract": "我们提出了一种算法，通过使用相关的、易于访问的函数作为代理，来最小化一个难以计算梯度的目标函数。我们的算法基于代理的近似近端点迭代，结合来自目标函数的相对较少的随机梯度。当目标函数与代理之间的差异是$\\delta$-平滑时，我们的算法保证以与$\\delta$-平滑目标函数上的随机梯度下降相匹配的速率收敛，这可以显著提高样本效率。我们的算法在机器学习中有许多潜在应用，并提供了一种利用合成数据、物理模拟器、混合公共和私人数据等的原则性方法。",
    "tldr": "该论文提出了一种代理算法，通过使用一个易于访问的函数作为代理，可以以与原函数梯度下降相匹配的速度收敛，从而显著提高样本效率，并在机器学习中具有许多潜在应用。",
    "en_tdlr": "This paper proposes a proxy algorithm that uses an easier-to-access function as a proxy to minimize a hard-to-compute objective function, and guarantees convergence at a rate matching stochastic gradient descent on the original function, leading to improved sample efficiency, with potential applications in machine learning."
}