{
    "title": "On Statistical Properties of Sharpness-Aware Minimization: Provable Guarantees. (arXiv:2302.11836v3 [stat.ML] UPDATED)",
    "abstract": "Sharpness-Aware Minimization (SAM) is a recent optimization framework aiming to improve the deep neural network generalization, through obtaining flatter (i.e. less sharp) solutions. As SAM has been numerically successful, recent papers have studied the theoretical aspects of the framework and have shown SAM solutions are indeed flat. However, there has been limited theoretical exploration regarding statistical properties of SAM. In this work, we directly study the statistical performance of SAM, and present a new theoretical explanation of why SAM generalizes well. To this end, we study two statistical problems, neural networks with a hidden layer and kernel regression, and prove under certain conditions, SAM has smaller prediction error over Gradient Descent (GD). Our results concern both convex and non-convex settings, and show that SAM is particularly well-suited for non-convex problems. Additionally, we prove that in our setup, SAM solutions are less sharp as well, showing our res",
    "link": "http://arxiv.org/abs/2302.11836",
    "context": "Title: On Statistical Properties of Sharpness-Aware Minimization: Provable Guarantees. (arXiv:2302.11836v3 [stat.ML] UPDATED)\nAbstract: Sharpness-Aware Minimization (SAM) is a recent optimization framework aiming to improve the deep neural network generalization, through obtaining flatter (i.e. less sharp) solutions. As SAM has been numerically successful, recent papers have studied the theoretical aspects of the framework and have shown SAM solutions are indeed flat. However, there has been limited theoretical exploration regarding statistical properties of SAM. In this work, we directly study the statistical performance of SAM, and present a new theoretical explanation of why SAM generalizes well. To this end, we study two statistical problems, neural networks with a hidden layer and kernel regression, and prove under certain conditions, SAM has smaller prediction error over Gradient Descent (GD). Our results concern both convex and non-convex settings, and show that SAM is particularly well-suited for non-convex problems. Additionally, we prove that in our setup, SAM solutions are less sharp as well, showing our res",
    "path": "papers/23/02/2302.11836.json",
    "total_tokens": 988,
    "translated_title": "关于锐度感知最小化的统计性质：可证明的保证",
    "translated_abstract": "锐度感知最小化 (SAM) 是一种旨在通过获得更平坦（即更不锐利）的解来改善深度神经网络泛化能力的最新优化框架。由于SAM在数值上十分成功，因此最近的论文研究了该框架的理论方面，并表明SAM的解确实是平坦的。然而，在SAM的统计性质方面，理论探索有限。本文直接研究SAM的统计性能，并提出了一个新的理论解释，解释了为什么SAM能够进行良好的泛化。为此，我们研究了两个统计问题，包括具有隐藏层的神经网络和核回归，并证明在某些条件下，SAM对于梯度下降(GD)相比有更小的预测误差。我们的结果涉及凸和非凸设置，并表明SAM特别适用于非凸问题。此外，我们还证明，在我们的设置中，SAM的解也更不锐利，证明了我们的结论。",
    "tldr": "SAM是一种优化框架，旨在通过获得更平坦（即更不锐利）的解来改善深度神经网络的泛化能力。我们研究两个统计问题，在某些条件下，证明了SAM在预测误差方面比梯度下降有更小的误差，并适用于非凸问题。此外，我们的设置表明，SAM的解更不锐利，证明了我们的结论。"
}