{
    "title": "Generalization and Stability of Interpolating Neural Networks with Minimal Width. (arXiv:2302.09235v2 [stat.ML] UPDATED)",
    "abstract": "We investigate the generalization and optimization properties of shallow neural-network classifiers trained by gradient descent in the interpolating regime. Specifically, in a realizable scenario where model weights can achieve arbitrarily small training error $\\epsilon$ and their distance from initialization is $g(\\epsilon)$, we demonstrate that gradient descent with $n$ training data achieves training error $O(g(1/T)^2 /T)$ and generalization error $O(g(1/T)^2 /n)$ at iteration $T$, provided there are at least $m=\\Omega(g(1/T)^4)$ hidden neurons. We then show that our realizable setting encompasses a special case where data are separable by the model's neural tangent kernel. For this and logistic-loss minimization, we prove the training loss decays at a rate of $\\tilde O(1/ T)$ given polylogarithmic number of neurons $m=\\Omega(\\log^4 (T))$. Moreover, with $m=\\Omega(\\log^{4} (n))$ neurons and $T\\approx n$ iterations, we bound the test loss by $\\tilde{O}(1/n)$. Our results differ from ",
    "link": "http://arxiv.org/abs/2302.09235",
    "context": "Title: Generalization and Stability of Interpolating Neural Networks with Minimal Width. (arXiv:2302.09235v2 [stat.ML] UPDATED)\nAbstract: We investigate the generalization and optimization properties of shallow neural-network classifiers trained by gradient descent in the interpolating regime. Specifically, in a realizable scenario where model weights can achieve arbitrarily small training error $\\epsilon$ and their distance from initialization is $g(\\epsilon)$, we demonstrate that gradient descent with $n$ training data achieves training error $O(g(1/T)^2 /T)$ and generalization error $O(g(1/T)^2 /n)$ at iteration $T$, provided there are at least $m=\\Omega(g(1/T)^4)$ hidden neurons. We then show that our realizable setting encompasses a special case where data are separable by the model's neural tangent kernel. For this and logistic-loss minimization, we prove the training loss decays at a rate of $\\tilde O(1/ T)$ given polylogarithmic number of neurons $m=\\Omega(\\log^4 (T))$. Moreover, with $m=\\Omega(\\log^{4} (n))$ neurons and $T\\approx n$ iterations, we bound the test loss by $\\tilde{O}(1/n)$. Our results differ from ",
    "path": "papers/23/02/2302.09235.json",
    "total_tokens": 1241,
    "tldr": "本文研究了在插值区间内通过梯度下降训练的浅层神经网络分类器的泛化和优化性能，结论表明，需要至少$m=\\Omega(g(1/T)^4)$个隐藏神经元才能保证训练误差和泛化误差最小，对于特定情况，训练误差以$\\tilde O(1/T)$的速率衰减，测试误差被限制在$\\tilde{O}(1/n)$。",
    "en_tdlr": "This paper investigates the generalization and optimization properties of shallow neural-network classifiers trained by gradient descent in the interpolating regime, and concludes that at least $m=\\Omega(g(1/T)^4)$ hidden neurons are needed to ensure minimum training and generalization errors. For specific scenarios, the training error decays at a rate of $\\tilde O(1/T)$ with a polylogarithmic number of neurons $m=\\Omega(\\log^4(T))$, and the test error is bounded by $\\tilde{O}(1/n)$ with $m=\\Omega(\\log^{4}(n))$ neurons and $T\\approx n$ iterations."
}