{
    "title": "Target-based Surrogates for Stochastic Optimization. (arXiv:2302.02607v2 [cs.LG] UPDATED)",
    "abstract": "We consider minimizing functions for which it is expensive to compute the (possibly stochastic) gradient. Such functions are prevalent in reinforcement learning, imitation learning and adversarial training. Our target optimization framework uses the (expensive) gradient computation to construct surrogate functions in a \\emph{target space} (e.g. the logits output by a linear model for classification) that can be minimized efficiently. This allows for multiple parameter updates to the model, amortizing the cost of gradient computation. In the full-batch setting, we prove that our surrogate is a global upper-bound on the loss, and can be (locally) minimized using a black-box optimization algorithm. We prove that the resulting majorization-minimization algorithm ensures convergence to a stationary point of the loss. Next, we instantiate our framework in the stochastic setting and propose the $SSO$ algorithm, which can be viewed as projected stochastic gradient descent in the target space. ",
    "link": "http://arxiv.org/abs/2302.02607",
    "context": "Title: Target-based Surrogates for Stochastic Optimization. (arXiv:2302.02607v2 [cs.LG] UPDATED)\nAbstract: We consider minimizing functions for which it is expensive to compute the (possibly stochastic) gradient. Such functions are prevalent in reinforcement learning, imitation learning and adversarial training. Our target optimization framework uses the (expensive) gradient computation to construct surrogate functions in a \\emph{target space} (e.g. the logits output by a linear model for classification) that can be minimized efficiently. This allows for multiple parameter updates to the model, amortizing the cost of gradient computation. In the full-batch setting, we prove that our surrogate is a global upper-bound on the loss, and can be (locally) minimized using a black-box optimization algorithm. We prove that the resulting majorization-minimization algorithm ensures convergence to a stationary point of the loss. Next, we instantiate our framework in the stochastic setting and propose the $SSO$ algorithm, which can be viewed as projected stochastic gradient descent in the target space. ",
    "path": "papers/23/02/2302.02607.json",
    "total_tokens": 942,
    "translated_title": "基于目标的随机优化代理",
    "translated_abstract": "我们考虑最小化函数，这些函数计算（可能是随机的）梯度是昂贵的。这种函数在强化学习、模仿学习和对抗性训练中都很普遍。我们的目标优化框架使用（昂贵的）梯度计算来在\\emph{目标空间}（例如用于分类的线性模型输出的logits）中构造代理函数，可以有效地进行最小化。这允许对模型进行多个参数更新，摊销梯度计算的成本。在完全批处理的设置中，我们证明我们的代理是损失的全局上界，并且可以使用黑盒优化算法（局部地）最小化。我们证明，最终的主导最小化算法确保收敛到损失的稳定点。接下来，我们在随机设置中实例化我们的框架，并提出SSO算法，可以视为目标空间中的投影随机梯度下降。",
    "tldr": "本文提出了一个基于目标的优化框架，可以构造出有效的代理函数来代替计算昂贵的梯度，实现在目标空间的最优化，摊销梯度计算的成本，进而提出了SSO算法进行投影随机梯度下降，以达到损失的稳定点。"
}