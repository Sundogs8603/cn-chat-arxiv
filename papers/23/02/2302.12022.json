{
    "title": "DoG is SGD's Best Friend: A Parameter-Free Dynamic Step Size Schedule. (arXiv:2302.12022v2 [cs.LG] UPDATED)",
    "abstract": "We propose a tuning-free dynamic SGD step size formula, which we call Distance over Gradients (DoG). The DoG step sizes depend on simple empirical quantities (distance from the initial point and norms of gradients) and have no ``learning rate'' parameter. Theoretically, we show that a slight variation of the DoG formula enjoys strong parameter-free convergence guarantees for stochastic convex optimization assuming only \\emph{locally bounded} stochastic gradients. Empirically, we consider a broad range of vision and language transfer learning tasks, and show that DoG's performance is close to that of SGD with tuned learning rate. We also propose a per-layer variant of DoG that generally outperforms tuned SGD, approaching the performance of tuned Adam. A PyTorch implementation is available at https://github.com/formll/dog",
    "link": "http://arxiv.org/abs/2302.12022",
    "context": "Title: DoG is SGD's Best Friend: A Parameter-Free Dynamic Step Size Schedule. (arXiv:2302.12022v2 [cs.LG] UPDATED)\nAbstract: We propose a tuning-free dynamic SGD step size formula, which we call Distance over Gradients (DoG). The DoG step sizes depend on simple empirical quantities (distance from the initial point and norms of gradients) and have no ``learning rate'' parameter. Theoretically, we show that a slight variation of the DoG formula enjoys strong parameter-free convergence guarantees for stochastic convex optimization assuming only \\emph{locally bounded} stochastic gradients. Empirically, we consider a broad range of vision and language transfer learning tasks, and show that DoG's performance is close to that of SGD with tuned learning rate. We also propose a per-layer variant of DoG that generally outperforms tuned SGD, approaching the performance of tuned Adam. A PyTorch implementation is available at https://github.com/formll/dog",
    "path": "papers/23/02/2302.12022.json",
    "total_tokens": 916,
    "translated_title": "DoG是SGD最好的朋友：一个无需参数调整的动态步长大小计划",
    "translated_abstract": "我们提出了一个无需参数调整的动态SGD步长公式，称为梯度距离公式（DoG）。DoG步长依赖于简单的经验量（初始点距离和梯度范数），并且没有“学习率”参数。在理论上，我们证明了DoG公式的一个略微变化可以保证具有强大的无参数收敛性，假定只有局部有界的随机梯度优化。在实践中，我们考虑了广泛的视觉和语言转移学习任务，并显示DoG的性能接近具有调整的学习率的SGD。我们还提出了一种逐层变量的DoG变体，通常优于调整的SGD，并接近调整的Adam的性能。PyTorch实现可在https://github.com/formll/dog获取。",
    "tldr": "我们提出了一个参数-free 的动态 SGD 步长公式，称为梯度距离公式（DoG）， 它没有“学习率”参数，但是在局部有界的随机梯度优化中拥有强大的无参数收敛性，并在广泛的视觉和语言转移学习任务中的表现与有调整学习率的 SGD 相当接近。",
    "en_tdlr": "We propose a parameter-free dynamic SGD step size formula, called Distance over Gradients (DoG), which has no \"learning rate\" parameter, but enjoys strong parameter-free convergence guarantees for locally bounded stochastic gradients. Empirically, DoG's performance is close to that of SGD with tuned learning rates on a broad range of vision and language transfer learning tasks. Furthermore, a per-layer variant of DoG generally outperforms tuned SGD and approaches the performance of tuned Adam."
}