{
    "title": "The Sample Complexity of Approximate Rejection Sampling with Applications to Smoothed Online Learning. (arXiv:2302.04658v2 [stat.ML] UPDATED)",
    "abstract": "Suppose we are given access to $n$ independent samples from distribution $\\mu$ and we wish to output one of them with the goal of making the output distributed as close as possible to a target distribution $\\nu$. In this work we show that the optimal total variation distance as a function of $n$ is given by $\\tilde\\Theta(\\frac{D}{f'(n)})$ over the class of all pairs $\\nu,\\mu$ with a bounded $f$-divergence $D_f(\\nu\\|\\mu)\\leq D$. Previously, this question was studied only for the case when the Radon-Nikodym derivative of $\\nu$ with respect to $\\mu$ is uniformly bounded. We then consider an application in the seemingly very different field of smoothed online learning, where we show that recent results on the minimax regret and the regret of oracle-efficient algorithms still hold even under relaxed constraints on the adversary (to have bounded $f$-divergence, as opposed to bounded Radon-Nikodym derivative). Finally, we also study efficacy of importance sampling for mean estimates uniform o",
    "link": "http://arxiv.org/abs/2302.04658",
    "context": "Title: The Sample Complexity of Approximate Rejection Sampling with Applications to Smoothed Online Learning. (arXiv:2302.04658v2 [stat.ML] UPDATED)\nAbstract: Suppose we are given access to $n$ independent samples from distribution $\\mu$ and we wish to output one of them with the goal of making the output distributed as close as possible to a target distribution $\\nu$. In this work we show that the optimal total variation distance as a function of $n$ is given by $\\tilde\\Theta(\\frac{D}{f'(n)})$ over the class of all pairs $\\nu,\\mu$ with a bounded $f$-divergence $D_f(\\nu\\|\\mu)\\leq D$. Previously, this question was studied only for the case when the Radon-Nikodym derivative of $\\nu$ with respect to $\\mu$ is uniformly bounded. We then consider an application in the seemingly very different field of smoothed online learning, where we show that recent results on the minimax regret and the regret of oracle-efficient algorithms still hold even under relaxed constraints on the adversary (to have bounded $f$-divergence, as opposed to bounded Radon-Nikodym derivative). Finally, we also study efficacy of importance sampling for mean estimates uniform o",
    "path": "papers/23/02/2302.04658.json",
    "total_tokens": 951,
    "translated_title": "近似拒绝采样的样本复杂度及其在平滑在线学习中的应用",
    "translated_abstract": "假设我们可以访问来自分布μ的n个独立样本，并且我们希望输出其中一个样本，使得输出的分布尽可能接近目标分布ν。在这项工作中，我们展示了在所有具有有界f-散度Df(ν|μ)≤D的ν,μ对中，关于n的最优总变差距离由Θ(~(D/f'(n)))给出。之前，这个问题只研究了ν相对于μ的Radon-Nikodym导数一致有界的情况。我们还考虑了似乎非常不同的平滑在线学习领域的一个应用，我们展示了最小化遗憾和具有oracle效率的算法的遗憾即使在对手有边界f-散度（而不是有界Radon-Nikodym导数）的松弛约束下，仍然成立。最后，我们还研究了在均匀估计中用于平均估计的重要性采样的效果。",
    "tldr": "本研究展示了在有界f-散度约束下，近似拒绝采样的样本复杂度可以通过Θ(~(D/f'(n)))函数来表示，并且应用于平滑在线学习中的相关算法的性能依然成立。"
}