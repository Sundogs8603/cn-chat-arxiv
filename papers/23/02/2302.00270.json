{
    "title": "Internally Rewarded Reinforcement Learning. (arXiv:2302.00270v2 [cs.LG] UPDATED)",
    "abstract": "We study a class of reinforcement learning problems where the reward signals for policy learning are generated by a discriminator that is dependent on and jointly optimized with the policy. This interdependence between the policy and the discriminator leads to an unstable learning process because reward signals from an immature discriminator are noisy and impede policy learning, and conversely, an under-optimized policy impedes discriminator learning. We call this learning setting \\textit{Internally Rewarded Reinforcement Learning} (IRRL) as the reward is not provided directly by the environment but \\textit{internally} by the discriminator. In this paper, we formally formulate IRRL and present a class of problems that belong to IRRL. We theoretically derive and empirically analyze the effect of the reward function in IRRL and based on these analyses propose the clipped linear reward function. Experimental results show that the proposed reward function can consistently stabilize the tra",
    "link": "http://arxiv.org/abs/2302.00270",
    "context": "Title: Internally Rewarded Reinforcement Learning. (arXiv:2302.00270v2 [cs.LG] UPDATED)\nAbstract: We study a class of reinforcement learning problems where the reward signals for policy learning are generated by a discriminator that is dependent on and jointly optimized with the policy. This interdependence between the policy and the discriminator leads to an unstable learning process because reward signals from an immature discriminator are noisy and impede policy learning, and conversely, an under-optimized policy impedes discriminator learning. We call this learning setting \\textit{Internally Rewarded Reinforcement Learning} (IRRL) as the reward is not provided directly by the environment but \\textit{internally} by the discriminator. In this paper, we formally formulate IRRL and present a class of problems that belong to IRRL. We theoretically derive and empirically analyze the effect of the reward function in IRRL and based on these analyses propose the clipped linear reward function. Experimental results show that the proposed reward function can consistently stabilize the tra",
    "path": "papers/23/02/2302.00270.json",
    "total_tokens": 922,
    "translated_title": "内部奖励的强化学习",
    "translated_abstract": "我们研究了一类强化学习问题，其中用于策略学习的奖励信号由一个与策略相关且与策略同时优化的判别器生成。策略和判别器之间的相互依赖导致了不稳定的学习过程，因为来自不成熟判别器的奖励信号是嘈杂的，阻碍了策略的学习；反过来，未经优化的策略也会阻碍判别器的学习。我们将这种学习设置称为“内部奖励的强化学习”（IRRL），因为奖励不是直接来自环境，而是由判别器“内部”提供的。在本文中，我们正式地表述了IRRL，并提出了一类属于IRRL的问题。我们从理论上推导并经验性地分析了IRRL中奖励函数的影响，并基于这些分析提出了修剪线性奖励函数。实验结果表明，所提出的奖励函数可以持续稳定训练过程。",
    "tldr": "这项研究探讨了一类强化学习问题，其中策略的奖励信号由与之相关且同时优化的判别器生成，导致学习过程不稳定。实验结果表明，修剪线性奖励函数可以稳定训练过程。",
    "en_tdlr": "This study investigates a class of reinforcement learning problems where the policy's reward signals are generated by a discriminator that is optimized together with the policy, leading to an unstable learning process. The proposed clipped linear reward function effectively stabilizes the training process."
}