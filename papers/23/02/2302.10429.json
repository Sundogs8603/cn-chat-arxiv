{
    "title": "FedSpeed: Larger Local Interval, Less Communication Round, and Higher Generalization Accuracy. (arXiv:2302.10429v2 [cs.LG] UPDATED)",
    "abstract": "Federated learning is an emerging distributed machine learning framework which jointly trains a global model via a large number of local devices with data privacy protections. Its performance suffers from the non-vanishing biases introduced by the local inconsistent optimal and the rugged client-drifts by the local over-fitting. In this paper, we propose a novel and practical method, FedSpeed, to alleviate the negative impacts posed by these problems. Concretely, FedSpeed applies the prox-correction term on the current local updates to efficiently reduce the biases introduced by the prox-term, a necessary regularizer to maintain the strong local consistency. Furthermore, FedSpeed merges the vanilla stochastic gradient with a perturbation computed from an extra gradient ascent step in the neighborhood, thereby alleviating the issue of local over-fitting. Our theoretical analysis indicates that the convergence rate is related to both the communication rounds $T$ and local intervals $K$ w",
    "link": "http://arxiv.org/abs/2302.10429",
    "context": "Title: FedSpeed: Larger Local Interval, Less Communication Round, and Higher Generalization Accuracy. (arXiv:2302.10429v2 [cs.LG] UPDATED)\nAbstract: Federated learning is an emerging distributed machine learning framework which jointly trains a global model via a large number of local devices with data privacy protections. Its performance suffers from the non-vanishing biases introduced by the local inconsistent optimal and the rugged client-drifts by the local over-fitting. In this paper, we propose a novel and practical method, FedSpeed, to alleviate the negative impacts posed by these problems. Concretely, FedSpeed applies the prox-correction term on the current local updates to efficiently reduce the biases introduced by the prox-term, a necessary regularizer to maintain the strong local consistency. Furthermore, FedSpeed merges the vanilla stochastic gradient with a perturbation computed from an extra gradient ascent step in the neighborhood, thereby alleviating the issue of local over-fitting. Our theoretical analysis indicates that the convergence rate is related to both the communication rounds $T$ and local intervals $K$ w",
    "path": "papers/23/02/2302.10429.json",
    "total_tokens": 893,
    "translated_title": "FedSpeed: 更大的本地间隔，更少的通信轮次，更高的泛化准确性。",
    "translated_abstract": "联邦学习是一种新兴的分布式机器学习框架，通过大量具有数据隐私保护的本地设备联合训练全局模型。其性能受到本地不一致最优引入的非消失偏差和本地过拟合引起的不稳定客户漂移的影响。本文提出了一种新颖而实用的方法FedSpeed，以减轻这些问题带来的负面影响。具体地说，FedSpeed在当前本地更新上应用了prox校正项，以有效减少prox项引入的偏差，prox项是一种必要的正则化器，用于保持强烈的本地一致性。此外，FedSpeed将纯随机梯度与邻域中额外梯度上升步骤计算的扰动相结合，从而减轻本地过拟合的问题。我们的理论分析表明，收敛速度与通信轮次$T$和本地间隔$K$有关",
    "tldr": "本文提出了FedSpeed方法，通过应用prox校正项和合并额外梯度上升步骤的扰动，从而减少偏差和过拟合的影响，提高了联邦学习的性能。"
}