{
    "title": "To the Noise and Back: Diffusion for Shared Autonomy. (arXiv:2302.12244v3 [cs.RO] UPDATED)",
    "abstract": "Shared autonomy is an operational concept in which a user and an autonomous agent collaboratively control a robotic system. It provides a number of advantages over the extremes of full-teleoperation and full-autonomy in many settings. Traditional approaches to shared autonomy rely on knowledge of the environment dynamics, a discrete space of user goals that is known a priori, or knowledge of the user's policy -- assumptions that are unrealistic in many domains. Recent works relax some of these assumptions by formulating shared autonomy with model-free deep reinforcement learning (RL). In particular, they no longer need knowledge of the goal space (e.g., that the goals are discrete or constrained) or environment dynamics. However, they need knowledge of a task-specific reward function to train the policy. Unfortunately, such reward specification can be a difficult and brittle process. On top of that, the formulations inherently rely on human-in-the-loop training, and that necessitates t",
    "link": "http://arxiv.org/abs/2302.12244",
    "context": "Title: To the Noise and Back: Diffusion for Shared Autonomy. (arXiv:2302.12244v3 [cs.RO] UPDATED)\nAbstract: Shared autonomy is an operational concept in which a user and an autonomous agent collaboratively control a robotic system. It provides a number of advantages over the extremes of full-teleoperation and full-autonomy in many settings. Traditional approaches to shared autonomy rely on knowledge of the environment dynamics, a discrete space of user goals that is known a priori, or knowledge of the user's policy -- assumptions that are unrealistic in many domains. Recent works relax some of these assumptions by formulating shared autonomy with model-free deep reinforcement learning (RL). In particular, they no longer need knowledge of the goal space (e.g., that the goals are discrete or constrained) or environment dynamics. However, they need knowledge of a task-specific reward function to train the policy. Unfortunately, such reward specification can be a difficult and brittle process. On top of that, the formulations inherently rely on human-in-the-loop training, and that necessitates t",
    "path": "papers/23/02/2302.12244.json",
    "total_tokens": 922,
    "translated_abstract": "共享自治是一种操作概念，其中用户和自主代理协作控制机器人系统。它在许多场景下提供了比全电传操作和完全自治更多的优势。传统的共享自治方法依赖于对环境动态、已知的用户目标的离散空间或用户策略的了解-这些假设在许多领域都是不现实的。最近的作品通过使用无模型深度强化学习(RL)来松弛这些假设，从而制定共享自治。特别地，他们不再需要目标空间的知识(例如，目标是否是离散的或约束的)或环境动态。然而，他们需要了解一个特定任务的奖励函数来训练策略。不幸的是，这种奖励规定可能是一个困难而脆弱的过程。此外，这些公式本质上依赖于人在循环训练，并且这必须尽可能的减少。",
    "tldr": "该论文研究了基于扩散的共享自治技术，通过模型自由的深度强化学习(RL)代替了传统的依赖环境动态和离散目标空间的方法，同时避免了困难脆弱的训练过程中的人工干预。",
    "en_tdlr": "This paper studies diffusion for shared autonomy, which uses model-free deep reinforcement learning (RL) to replace traditional approaches that rely on knowledge of environment dynamics and discrete goal spaces, while avoiding difficult and brittle human-in-the-loop training process."
}