{
    "title": "Linear-scaling kernels for protein sequences and small molecules outperform deep learning while providing uncertainty quantitation and improved interpretability. (arXiv:2302.03294v2 [cs.LG] UPDATED)",
    "abstract": "Gaussian process (GP) is a Bayesian model which provides several advantages for regression tasks in machine learning such as reliable quantitation of uncertainty and improved interpretability. Their adoption has been precluded by their excessive computational cost and by the difficulty in adapting them for analyzing sequences (e.g. amino acid and nucleotide sequences) and graphs (e.g. ones representing small molecules). In this study, we develop efficient and scalable approaches for fitting GP models as well as fast convolution kernels which scale linearly with graph or sequence size. We implement these improvements by building an open-source Python library called xGPR. We compare the performance of xGPR with the reported performance of various deep learning models on 20 benchmarks, including small molecule, protein sequence and tabular data. We show that xGRP achieves highly competitive performance with much shorter training time. Furthermore, we also develop new kernels for sequence ",
    "link": "http://arxiv.org/abs/2302.03294",
    "context": "Title: Linear-scaling kernels for protein sequences and small molecules outperform deep learning while providing uncertainty quantitation and improved interpretability. (arXiv:2302.03294v2 [cs.LG] UPDATED)\nAbstract: Gaussian process (GP) is a Bayesian model which provides several advantages for regression tasks in machine learning such as reliable quantitation of uncertainty and improved interpretability. Their adoption has been precluded by their excessive computational cost and by the difficulty in adapting them for analyzing sequences (e.g. amino acid and nucleotide sequences) and graphs (e.g. ones representing small molecules). In this study, we develop efficient and scalable approaches for fitting GP models as well as fast convolution kernels which scale linearly with graph or sequence size. We implement these improvements by building an open-source Python library called xGPR. We compare the performance of xGPR with the reported performance of various deep learning models on 20 benchmarks, including small molecule, protein sequence and tabular data. We show that xGRP achieves highly competitive performance with much shorter training time. Furthermore, we also develop new kernels for sequence ",
    "path": "papers/23/02/2302.03294.json",
    "total_tokens": 893,
    "translated_title": "线性缩放核对蛋白质序列和小分子具有更好的性能，提供不确定性量化和解释性优化，胜过深度学习",
    "translated_abstract": "高斯过程（GP）是一种贝叶斯模型，可以在机器学习中用于回归任务，具有可靠的不确定性量化和改进的解释性等优点，并已被应用于分析序列（如氨基酸和核苷酸序列）和图（如表示小分子的图）等。本文开发了高效可扩展的适用于GP模型的方法和可线性扩展的卷积核，构建了名为xGPR的开源Python库。我们对比了xGPR和各种深度学习模型在20个基准测试中（包括小分子、蛋白质序列和表格数据）的性能表现，结果表明xGPR训练时间更短，性能也非常具有竞争力。此外，我们还为序列和图添加了新的核函数。",
    "tldr": "本文提出了可线性扩展的卷积核以用于处理序列和图，利用高斯过程建立了高效可扩展的模型，比起各种深度学习模型，更快且性能更好，并提供了可靠的不确定性量化和解释性优化。",
    "en_tdlr": "This paper proposes linearly scalable convolution kernels for sequences and graphs, develops efficient and scalable models using Gaussian processes, achieves better performance with faster speed compared to various deep learning models, and provides reliable uncertainty quantification and improved interpretability."
}