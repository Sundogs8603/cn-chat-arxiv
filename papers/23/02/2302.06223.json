{
    "title": "Variational Mixture of HyperGenerators for Learning Distributions Over Functions. (arXiv:2302.06223v2 [cs.LG] UPDATED)",
    "abstract": "Recent approaches build on implicit neural representations (INRs) to propose generative models over function spaces. However, they are computationally costly when dealing with inference tasks, such as missing data imputation, or directly cannot tackle them. In this work, we propose a novel deep generative model, named VAMoH. VAMoH combines the capabilities of modeling continuous functions using INRs and the inference capabilities of Variational Autoencoders (VAEs). In addition, VAMoH relies on a normalizing flow to define the prior, and a mixture of hypernetworks to parametrize the data log-likelihood. This gives VAMoH a high expressive capability and interpretability. Through experiments on a diverse range of data types, such as images, voxels, and climate data, we show that VAMoH can effectively learn rich distributions over continuous functions. Furthermore, it can perform inference-related tasks, such as conditional super-resolution generation and in-painting, as well or better tha",
    "link": "http://arxiv.org/abs/2302.06223",
    "context": "Title: Variational Mixture of HyperGenerators for Learning Distributions Over Functions. (arXiv:2302.06223v2 [cs.LG] UPDATED)\nAbstract: Recent approaches build on implicit neural representations (INRs) to propose generative models over function spaces. However, they are computationally costly when dealing with inference tasks, such as missing data imputation, or directly cannot tackle them. In this work, we propose a novel deep generative model, named VAMoH. VAMoH combines the capabilities of modeling continuous functions using INRs and the inference capabilities of Variational Autoencoders (VAEs). In addition, VAMoH relies on a normalizing flow to define the prior, and a mixture of hypernetworks to parametrize the data log-likelihood. This gives VAMoH a high expressive capability and interpretability. Through experiments on a diverse range of data types, such as images, voxels, and climate data, we show that VAMoH can effectively learn rich distributions over continuous functions. Furthermore, it can perform inference-related tasks, such as conditional super-resolution generation and in-painting, as well or better tha",
    "path": "papers/23/02/2302.06223.json",
    "total_tokens": 952,
    "translated_title": "变分混合超生成器用于学习函数分布的方法",
    "translated_abstract": "最近的一些方法基于隐式神经表示（INRs）提出了函数空间上的生成模型。然而，处理推断任务（如缺失数据插值）时，它们在计算上代价高，或者根本不能处理这些问题。在本文中，我们提出了一种新的深度生成模型，称为VAMoH。VAMoH结合了使用INRs对连续函数进行建模的能力和变分自编码器（VAEs）的推断能力。此外，VAMoH依赖于一个归一化流来定义先验，以及一个超网络混合来参数化数据对数似然。这使得VAMoH具有高度表达能力和可解释性。通过在各种数据类型（如图像、体素和气候数据）上进行实验证明，VAMoH可以有效地学习连续函数的丰富分布。此外，它可以执行与推断相关的任务，如条件超分辨率生成和修复，效果优于或不亚于其他方法。",
    "tldr": "本文提出了一种新的深度生成模型VAMoH，结合了INRs对连续函数进行建模的能力和VAEs的推断能力，以及归一化流和超网络混合方法。在不同类型的数据上进行实验证明，VAMoH可以有效地学习连续函数的分布，并可以执行与推断相关的任务。",
    "en_tdlr": "This paper proposes a new deep generative model, VAMoH, which combines the ability to model continuous functions using INRs and the inferential capabilities of VAEs. VAMoH also relies on a normalizing flow and a mixture of hypernetworks. Experiments show that VAMoH can effectively learn distributions over continuous functions and perform inference-related tasks."
}