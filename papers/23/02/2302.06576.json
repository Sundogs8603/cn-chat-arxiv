{
    "title": "GFlowNet-EM for learning compositional latent variable models. (arXiv:2302.06576v2 [cs.LG] UPDATED)",
    "abstract": "Latent variable models (LVMs) with discrete compositional latents are an important but challenging setting due to a combinatorially large number of possible configurations of the latents. A key tradeoff in modeling the posteriors over latents is between expressivity and tractable optimization. For algorithms based on expectation-maximization (EM), the E-step is often intractable without restrictive approximations to the posterior. We propose the use of GFlowNets, algorithms for sampling from an unnormalized density by learning a stochastic policy for sequential construction of samples, for this intractable E-step. By training GFlowNets to sample from the posterior over latents, we take advantage of their strengths as amortized variational inference algorithms for complex distributions over discrete structures. Our approach, GFlowNet-EM, enables the training of expressive LVMs with discrete compositional latents, as shown by experiments on non-context-free grammar induction and on image",
    "link": "http://arxiv.org/abs/2302.06576",
    "context": "Title: GFlowNet-EM for learning compositional latent variable models. (arXiv:2302.06576v2 [cs.LG] UPDATED)\nAbstract: Latent variable models (LVMs) with discrete compositional latents are an important but challenging setting due to a combinatorially large number of possible configurations of the latents. A key tradeoff in modeling the posteriors over latents is between expressivity and tractable optimization. For algorithms based on expectation-maximization (EM), the E-step is often intractable without restrictive approximations to the posterior. We propose the use of GFlowNets, algorithms for sampling from an unnormalized density by learning a stochastic policy for sequential construction of samples, for this intractable E-step. By training GFlowNets to sample from the posterior over latents, we take advantage of their strengths as amortized variational inference algorithms for complex distributions over discrete structures. Our approach, GFlowNet-EM, enables the training of expressive LVMs with discrete compositional latents, as shown by experiments on non-context-free grammar induction and on image",
    "path": "papers/23/02/2302.06576.json",
    "total_tokens": 930,
    "translated_title": "GFlowNet-EM用于学习组合隐变量模型",
    "translated_abstract": "具有离散组合潜变量的潜变量模型（LVM）是一个重要但具有挑战性的领域，由于潜变量的可能组合数量组合很大。在建模潜变量的后验概率时，表现和可跟踪的优化之间具有关键的权衡。对于基于期望最大化（EM）的算法，E-步骤往往在没有对后验进行限制的近似的情况下是不可跟踪的。我们提出使用GFlowNets，一种学习从未规范化的密度中采样的随机策略以进行顺序样本构建的算法，来处理这个不可跟踪的E-步骤。通过训练GFlowNets从潜变量后验中采样，我们利用了它们作为离散结构复杂分布的变分推理算法的优势。我们的方法，GFlowNet-EM，可以实现对具有离散组合潜变量的表现力强的LVM进行训练，如在非上下文无关文法归纳和图像翻译实验证明。",
    "tldr": "GFlowNet-EM采用GFlowNets算法作为建模潜变量后验概率的E步骤，从而实现了对具有离散组合潜变量的表现力强的LVM进行训练。",
    "en_tdlr": "GFlowNet-EM uses GFlowNets algorithm as the E-step in modeling the posterior over latent variables, enabling the training of expressive LVMs with discrete compositional latents."
}