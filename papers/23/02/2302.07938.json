{
    "title": "Scalable Multi-Agent Reinforcement Learning with General Utilities. (arXiv:2302.07938v2 [cs.LG] UPDATED)",
    "abstract": "We study the scalable multi-agent reinforcement learning (MARL) with general utilities, defined as nonlinear functions of the team's long-term state-action occupancy measure. The objective is to find a localized policy that maximizes the average of the team's local utility functions without the full observability of each agent in the team. By exploiting the spatial correlation decay property of the network structure, we propose a scalable distributed policy gradient algorithm with shadow reward and localized policy that consists of three steps: (1) shadow reward estimation, (2) truncated shadow Q-function estimation, and (3) truncated policy gradient estimation and policy update. Our algorithm converges, with high probability, to $\\epsilon$-stationarity with $\\widetilde{\\mathcal{O}}(\\epsilon^{-2})$ samples up to some approximation error that decreases exponentially in the communication radius. This is the first result in the literature on multi-agent RL with general utilities that does",
    "link": "http://arxiv.org/abs/2302.07938",
    "context": "Title: Scalable Multi-Agent Reinforcement Learning with General Utilities. (arXiv:2302.07938v2 [cs.LG] UPDATED)\nAbstract: We study the scalable multi-agent reinforcement learning (MARL) with general utilities, defined as nonlinear functions of the team's long-term state-action occupancy measure. The objective is to find a localized policy that maximizes the average of the team's local utility functions without the full observability of each agent in the team. By exploiting the spatial correlation decay property of the network structure, we propose a scalable distributed policy gradient algorithm with shadow reward and localized policy that consists of three steps: (1) shadow reward estimation, (2) truncated shadow Q-function estimation, and (3) truncated policy gradient estimation and policy update. Our algorithm converges, with high probability, to $\\epsilon$-stationarity with $\\widetilde{\\mathcal{O}}(\\epsilon^{-2})$ samples up to some approximation error that decreases exponentially in the communication radius. This is the first result in the literature on multi-agent RL with general utilities that does",
    "path": "papers/23/02/2302.07938.json",
    "total_tokens": 928,
    "translated_title": "具有通用效用的可扩展多智能体强化学习系统",
    "translated_abstract": "我们研究了具有通用效用的可扩展多智能体强化学习（MARL），其中通用效用被定义为团队长期状态-动作占有率测度的非线性函数。我们的目标是找到一个局部策略，最大化团队局部效用函数的平均值，而不需要完全观测团队中的每个智能体。通过利用网络结构的空间相关衰减性质，我们提出了一种可扩展的分布式策略梯度算法，其中包括三个步骤：（1）阴影奖励估计，（2）截断阴影Q函数估计，以及（3）截断策略梯度估计和策略更新。我们的算法收敛于$\\epsilon$-稳定性，高概率下需要$\\widetilde{\\mathcal{O}}(\\epsilon^{-2})$个样本，直到一定程度上的近似误差以指数速度减小到通信半径内。这是关于具有通用效用的多智能体强化学习的文献中的首个结果。",
    "tldr": "本论文研究了具有通用效用的可扩展多智能体强化学习，并提出了一种基于分布式策略梯度算法的解决方案，通过利用网络结构的空间相关衰减性质实现了算法的收敛性。",
    "en_tdlr": "This paper studies scalable multi-agent reinforcement learning with general utilities and proposes a distributed policy gradient algorithm that utilizes the spatial correlation decay property of the network structure to achieve convergence."
}