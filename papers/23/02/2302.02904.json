{
    "title": "Rethinking Gauss-Newton for learning over-parameterized models. (arXiv:2302.02904v2 [cs.LG] UPDATED)",
    "abstract": "This work studies the global convergence and generalization properties of Gauss Newton's (GN) when optimizing one-hidden layer networks in the over-parameterized regime. We first establish a global convergence result for GN in the continuous-time limit exhibiting a faster convergence rate compared to GD due to improved conditioning. We then perform an empirical study on a synthetic regression task to investigate the implicit bias of GN's method. We find that, while GN is consistently faster than GD in finding a global optimum, the performance of the learned model on a test dataset is heavily influenced by both the learning rate and the variance of the randomly initialized network's weights. Specifically, we find that initializing with a smaller variance results in a better generalization, a behavior also observed for GD. However, in contrast to GD where larger learning rates lead to the best generalization, we find that GN achieves an improved generalization when using smaller learning",
    "link": "http://arxiv.org/abs/2302.02904",
    "context": "Title: Rethinking Gauss-Newton for learning over-parameterized models. (arXiv:2302.02904v2 [cs.LG] UPDATED)\nAbstract: This work studies the global convergence and generalization properties of Gauss Newton's (GN) when optimizing one-hidden layer networks in the over-parameterized regime. We first establish a global convergence result for GN in the continuous-time limit exhibiting a faster convergence rate compared to GD due to improved conditioning. We then perform an empirical study on a synthetic regression task to investigate the implicit bias of GN's method. We find that, while GN is consistently faster than GD in finding a global optimum, the performance of the learned model on a test dataset is heavily influenced by both the learning rate and the variance of the randomly initialized network's weights. Specifically, we find that initializing with a smaller variance results in a better generalization, a behavior also observed for GD. However, in contrast to GD where larger learning rates lead to the best generalization, we find that GN achieves an improved generalization when using smaller learning",
    "path": "papers/23/02/2302.02904.json",
    "total_tokens": 988,
    "translated_title": "重新思考高斯-牛顿方法在过参数模型学习中的应用",
    "translated_abstract": "本研究探讨了在过参数化模型中，使用高斯-牛顿法（GN）对一层隐藏层网络进行优化时的全局收敛和泛化特性。我们首先在连续时间极限下确定了GN的全局收敛结果，由于改善了条件，其收敛速度比梯度下降（GD）更快。然后，我们在合成回归任务中进行了实证研究，以调查GN方法的隐式偏差。我们发现，虽然GN始终比GD更快地找到全局最优解，但学习模型在测试数据集上的表现受到学习率和随机初始化网络权重方差的影响。具体而言，我们发现使用更小的方差初始化结果会获得更好的泛化，这也是GD的一种行为。然而，与GD不同的是，我们发现使用更小的学习率可以使GN在实现更好的泛化方面取得成效。",
    "tldr": "本研究重新思考了在过参数模型中使用高斯-牛顿法的应用，通过实证研究发现，虽然GN在找到全局最优解方面比GD更快，但学习率和随机初始化网络权重方差对模型泛化性能影响很大，更小的方差初始化能够获得更好的泛化性能，而与GD不同的是，GN在实现更好的泛化方面使用更小的学习率能够取得成效。",
    "en_tdlr": "This paper rethinks the application of Gauss-Newton method in over-parameterized models, and an empirical study reveals that while GN is faster in finding a global optimum than GD, the variance of randomly initialized network weights and learning rate strongly affect the generalization performance of the learned model. Smaller variance initialization leads to better generalization, and contrary to GD, GN achieves better generalization with smaller learning rates."
}