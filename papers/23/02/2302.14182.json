{
    "title": "Taylor TD-learning. (arXiv:2302.14182v2 [cs.LG] UPDATED)",
    "abstract": "Many reinforcement learning approaches rely on temporal-difference (TD) learning to learn a critic. However, TD-learning updates can be high variance. Here, we introduce a model-based RL framework, Taylor TD, which reduces this variance in continuous state-action settings. Taylor TD uses a first-order Taylor series expansion of TD updates. This expansion allows Taylor TD to analytically integrate over stochasticity in the action-choice, and some stochasticity in the state distribution for the initial state and action of each TD update. We include theoretical and empirical evidence that Taylor TD updates are indeed lower variance than standard TD updates. Additionally, we show Taylor TD has the same stable learning guarantees as standard TD-learning with linear function approximation under a reasonable assumption. Next, we combine Taylor TD with the TD3 algorithm, forming TaTD3. We show TaTD3 performs as well, if not better, than several state-of-the art model-free and model-based basel",
    "link": "http://arxiv.org/abs/2302.14182",
    "context": "Title: Taylor TD-learning. (arXiv:2302.14182v2 [cs.LG] UPDATED)\nAbstract: Many reinforcement learning approaches rely on temporal-difference (TD) learning to learn a critic. However, TD-learning updates can be high variance. Here, we introduce a model-based RL framework, Taylor TD, which reduces this variance in continuous state-action settings. Taylor TD uses a first-order Taylor series expansion of TD updates. This expansion allows Taylor TD to analytically integrate over stochasticity in the action-choice, and some stochasticity in the state distribution for the initial state and action of each TD update. We include theoretical and empirical evidence that Taylor TD updates are indeed lower variance than standard TD updates. Additionally, we show Taylor TD has the same stable learning guarantees as standard TD-learning with linear function approximation under a reasonable assumption. Next, we combine Taylor TD with the TD3 algorithm, forming TaTD3. We show TaTD3 performs as well, if not better, than several state-of-the art model-free and model-based basel",
    "path": "papers/23/02/2302.14182.json",
    "total_tokens": 948,
    "translated_title": "Taylor TD学习",
    "translated_abstract": "许多强化学习方法依赖于时间差分（TD）学习来学习一个评论家。然而，TD学习的更新可能具有较高的方差。在这里，我们引入了一个基于模型的RL框架，即Taylor TD，它减少了连续状态-动作设置中的方差。Taylor TD使用TD更新的一阶泰勒级数展开。该展开允许Taylor TD在行动选择的随机性和每个TD更新的初始状态和动作的状态分布的一些随机性上进行分析积分。我们提供理论和实证证据，证明Taylor TD的更新确实比标准的TD更新具有较低的方差。此外，我们还展示了在合理的假设下，Taylor TD具有与线性函数逼近下的标准TD学习相同的稳定学习保证。接下来，我们将Taylor TD与TD3算法相结合，形成TaTD3。我们展示TaTD3的表现与几种最先进的无模型和基于模型的基准相当，甚至更好。",
    "tldr": "Taylor TD是一个基于模型的RL框架，通过使用TD更新的泰勒级数展开，减少了连续状态-动作设置中的方差，并具有与标准TD学习相同的稳定学习保证。TaTD3是Taylor TD与TD3算法相结合所形成的方法，其表现优于一些最先进的无模型和基于模型的基准。",
    "en_tdlr": "Taylor TD is a model-based RL framework that reduces variance in continuous state-action settings by using a first-order Taylor series expansion of TD updates and has the same stable learning guarantees as standard TD learning. TaTD3, formed by combining Taylor TD with the TD3 algorithm, performs as well, if not better, than several state-of-the-art model-free and model-based baselines."
}