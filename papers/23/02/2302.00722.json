{
    "title": "A Survey of Deep Learning: From Activations to Transformers. (arXiv:2302.00722v2 [cs.LG] UPDATED)",
    "abstract": "The past decade has witnessed remarkable advancements in deep learning, owing to the emergence of various architectures, layers, objectives, and optimization techniques. These consist of a multitude of variations of attention, normalization, skip connections, transformer, and self-supervised learning methods, among others. Our goal is to furnish a comprehensive survey of significant recent contributions in these domains to individuals with a fundamental grasp of deep learning. Our aspiration is that an integrated and comprehensive approach of influential recent works will facilitate the formation of new connections between different areas of deep learning. In our discussion, we discuss multiple patterns that summarize the key strategies for many of the successful innovations over the last decade. We also include a discussion on recent commercially built, closed-source models such as OpenAI's GPT-4 and Google's PaLM 2.",
    "link": "http://arxiv.org/abs/2302.00722",
    "context": "Title: A Survey of Deep Learning: From Activations to Transformers. (arXiv:2302.00722v2 [cs.LG] UPDATED)\nAbstract: The past decade has witnessed remarkable advancements in deep learning, owing to the emergence of various architectures, layers, objectives, and optimization techniques. These consist of a multitude of variations of attention, normalization, skip connections, transformer, and self-supervised learning methods, among others. Our goal is to furnish a comprehensive survey of significant recent contributions in these domains to individuals with a fundamental grasp of deep learning. Our aspiration is that an integrated and comprehensive approach of influential recent works will facilitate the formation of new connections between different areas of deep learning. In our discussion, we discuss multiple patterns that summarize the key strategies for many of the successful innovations over the last decade. We also include a discussion on recent commercially built, closed-source models such as OpenAI's GPT-4 and Google's PaLM 2.",
    "path": "papers/23/02/2302.00722.json",
    "total_tokens": 872,
    "translated_title": "深度学习综述：从激活函数到Transformer",
    "translated_abstract": "过去十年中，深度学习取得了显著的进展，得益于各种架构、层、目标和优化技术的涌现。这些包括关注机制、归一化、跳跃连接、Transformer和自监督学习等多种变体方法。我们的目标是向具有深度学习基本理解的人提供对这些领域中最新重要贡献的全面调查。我们的期望是通过对重要最新作品的综合和全面的探讨，促进不同深度学习领域之间形成新的联系。在我们的讨论中，我们总结了过去十年中许多成功创新的关键策略。我们还对最近一些商业闭源模型进行了讨论，例如OpenAI的GPT-4和Google的PaLM 2。",
    "tldr": "这篇综述调查了深度学习领域的重要进展，包括各种架构、层、目标和优化技术的发展，以及关注机制、归一化、跳跃连接、Transformer和自监督学习等方法的变体。总结了成功创新的关键策略，并讨论了最近的商业闭源模型。",
    "en_tdlr": "This survey explores significant advancements in the field of deep learning, including the development of various architectures, layers, objectives, and optimization techniques, as well as variations of attention mechanisms, normalization, skip connections, Transformer, and self-supervised learning methods. It summarizes key strategies for successful innovations and discusses recent commercially built, closed-source models."
}