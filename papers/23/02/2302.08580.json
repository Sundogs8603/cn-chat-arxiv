{
    "title": "Online Learning Guided Curvature Approximation: A Quasi-Newton Method with Global Non-Asymptotic Superlinear Convergence. (arXiv:2302.08580v2 [math.OC] UPDATED)",
    "abstract": "Quasi-Newton algorithms are among the most popular iterative methods for solving unconstrained minimization problems, largely due to their favorable superlinear convergence property. However, existing results for these algorithms are limited as they provide either (i) a global convergence guarantee with an asymptotic superlinear convergence rate, or (ii) a local non-asymptotic superlinear rate for the case that the initial point and the initial Hessian approximation are chosen properly. In particular, no current analysis for quasi-Newton methods guarantees global convergence with an explicit superlinear convergence rate. In this paper, we close this gap and present the first globally convergent quasi-Newton method with an explicit non-asymptotic superlinear convergence rate. Unlike classical quasi-Newton methods, we build our algorithm upon the hybrid proximal extragradient method and propose a novel online learning framework for updating the Hessian approximation matrices. Specificall",
    "link": "http://arxiv.org/abs/2302.08580",
    "context": "Title: Online Learning Guided Curvature Approximation: A Quasi-Newton Method with Global Non-Asymptotic Superlinear Convergence. (arXiv:2302.08580v2 [math.OC] UPDATED)\nAbstract: Quasi-Newton algorithms are among the most popular iterative methods for solving unconstrained minimization problems, largely due to their favorable superlinear convergence property. However, existing results for these algorithms are limited as they provide either (i) a global convergence guarantee with an asymptotic superlinear convergence rate, or (ii) a local non-asymptotic superlinear rate for the case that the initial point and the initial Hessian approximation are chosen properly. In particular, no current analysis for quasi-Newton methods guarantees global convergence with an explicit superlinear convergence rate. In this paper, we close this gap and present the first globally convergent quasi-Newton method with an explicit non-asymptotic superlinear convergence rate. Unlike classical quasi-Newton methods, we build our algorithm upon the hybrid proximal extragradient method and propose a novel online learning framework for updating the Hessian approximation matrices. Specificall",
    "path": "papers/23/02/2302.08580.json",
    "total_tokens": 921,
    "translated_title": "在线学习引导的曲率逼近：具有全局非渐近超线性收敛的拟牛顿法。",
    "translated_abstract": "拟牛顿算法是解决无约束最小化问题中最受欢迎的迭代方法之一，这主要归功于其良好的超线性收敛性质。然而，现有的算法结果存在限制，要么提供了具有渐近超线性收敛速度的全局收敛保证，要么仅在初始点和初始Hessian逼近选择适当的情况下提供了局部非渐近超线性速率。特别地，目前没有拟牛顿方法的分析保证了具有明确超线性收敛速率的全局收敛性。在本文中，我们填补了这一空白，并提出了第一个具有明确非渐近超线性收敛速率的全局收敛拟牛顿方法。与传统的拟牛顿方法不同，我们基于混合近端外梯度法构建了我们的算法，并提出了一种新颖的在线学习框架来更新Hessian逼近矩阵。",
    "tldr": "本文提出了第一个具有明确非渐近超线性收敛速率的全局收敛拟牛顿方法，并采用混合近端外梯度法结构和在线学习框架来更新Hessian逼近矩阵。",
    "en_tdlr": "This paper presents the first globally convergent quasi-Newton method with an explicit non-asymptotic superlinear convergence rate. The algorithm is built upon the hybrid proximal extragradient method and introduces a novel online learning framework for updating the Hessian approximation matrices."
}