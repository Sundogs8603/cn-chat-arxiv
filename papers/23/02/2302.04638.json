{
    "title": "Better Diffusion Models Further Improve Adversarial Training. (arXiv:2302.04638v2 [cs.CV] UPDATED)",
    "abstract": "It has been recognized that the data generated by the denoising diffusion probabilistic model (DDPM) improves adversarial training. After two years of rapid development in diffusion models, a question naturally arises: can better diffusion models further improve adversarial training? This paper gives an affirmative answer by employing the most recent diffusion model which has higher efficiency ($\\sim 20$ sampling steps) and image quality (lower FID score) compared with DDPM. Our adversarially trained models achieve state-of-the-art performance on RobustBench using only generated data (no external datasets). Under the $\\ell_\\infty$-norm threat model with $\\epsilon=8/255$, our models achieve $70.69\\%$ and $42.67\\%$ robust accuracy on CIFAR-10 and CIFAR-100, respectively, i.e. improving upon previous state-of-the-art models by $+4.58\\%$ and $+8.03\\%$. Under the $\\ell_2$-norm threat model with $\\epsilon=128/255$, our models achieve $84.86\\%$ on CIFAR-10 ($+4.44\\%$). These results also beat",
    "link": "http://arxiv.org/abs/2302.04638",
    "context": "Title: Better Diffusion Models Further Improve Adversarial Training. (arXiv:2302.04638v2 [cs.CV] UPDATED)\nAbstract: It has been recognized that the data generated by the denoising diffusion probabilistic model (DDPM) improves adversarial training. After two years of rapid development in diffusion models, a question naturally arises: can better diffusion models further improve adversarial training? This paper gives an affirmative answer by employing the most recent diffusion model which has higher efficiency ($\\sim 20$ sampling steps) and image quality (lower FID score) compared with DDPM. Our adversarially trained models achieve state-of-the-art performance on RobustBench using only generated data (no external datasets). Under the $\\ell_\\infty$-norm threat model with $\\epsilon=8/255$, our models achieve $70.69\\%$ and $42.67\\%$ robust accuracy on CIFAR-10 and CIFAR-100, respectively, i.e. improving upon previous state-of-the-art models by $+4.58\\%$ and $+8.03\\%$. Under the $\\ell_2$-norm threat model with $\\epsilon=128/255$, our models achieve $84.86\\%$ on CIFAR-10 ($+4.44\\%$). These results also beat",
    "path": "papers/23/02/2302.04638.json",
    "total_tokens": 1127,
    "translated_title": "更好的扩散模型进一步改进对抗训练",
    "translated_abstract": "众所周知，由去噪扩散概率模型（DDPM）产生的数据可以提高对抗性训练。随着扩散模型的迅速发展，一个自然的问题出现了：更好的扩散模型是否能够进一步改进对抗训练？本文采用了最新的扩散模型，与DDPM相比，该模型具有更高的效率（约$\\sim20$个采样步骤）和更低的图像质量（更低的FID分数），证明了更好的扩散模型可以进一步提高对抗训练。我们训练的模型仅使用生成的数据（没有外部数据集）在RobustBench上实现了最佳性能。在$\\ell_\\infty$-norm威胁模型下，当$\\epsilon=8/255$时，我们的模型在CIFAR-10和CIFAR-100上分别达到$70.69\\%$和$42.67\\%$的鲁棒准确度，即分别比以前的最先进模型提高了$+4.58\\%$和$+8.03\\%$。在$\\ell_2$-norm威胁模型下，当$\\epsilon=128/255$时，我们的模型在CIFAR-10上可以达到$84.86\\%$的准确率，提高了$+4.44\\%$。",
    "tldr": "本文证明了更好的扩散模型可以进一步提高对抗训练的性能，通过采用最新的扩散模型，我们训练的模型仅使用生成的数据就在RobustBench上实现了最佳性能，并在CIFAR-10和CIFAR-100数据集上分别提高了$+4.58\\%$和$+8.03\\%$的性能。",
    "en_tdlr": "This paper demonstrates that better diffusion models can further improve the performance of adversarial training. By employing the most recent and efficient diffusion model with high image quality, the adversarial models achieved state-of-the-art performance on RobustBench using only generated data and improved performance on CIFAR-10 and CIFAR-100 datasets by $+4.58\\%$ and $+8.03\\%$, respectively."
}