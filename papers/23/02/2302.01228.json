{
    "title": "Dual Propagation: Accelerating Contrastive Hebbian Learning with Dyadic Neurons. (arXiv:2302.01228v3 [cs.LG] UPDATED)",
    "abstract": "Activity difference based learning algorithms-such as contrastive Hebbian learning and equilibrium propagation-have been proposed as biologically plausible alternatives to error back-propagation. However, on traditional digital chips these algorithms suffer from having to solve a costly inference problem twice, making these approaches more than two orders of magnitude slower than back-propagation. In the analog realm equilibrium propagation may be promising for fast and energy efficient learning, but states still need to be inferred and stored twice. Inspired by lifted neural networks and compartmental neuron models we propose a simple energy based compartmental neuron model, termed dual propagation, in which each neuron is a dyad with two intrinsic states. At inference time these intrinsic states encode the error/activity duality through their difference and their mean respectively. The advantage of this method is that only a single inference phase is needed and that inference can be ",
    "link": "http://arxiv.org/abs/2302.01228",
    "context": "Title: Dual Propagation: Accelerating Contrastive Hebbian Learning with Dyadic Neurons. (arXiv:2302.01228v3 [cs.LG] UPDATED)\nAbstract: Activity difference based learning algorithms-such as contrastive Hebbian learning and equilibrium propagation-have been proposed as biologically plausible alternatives to error back-propagation. However, on traditional digital chips these algorithms suffer from having to solve a costly inference problem twice, making these approaches more than two orders of magnitude slower than back-propagation. In the analog realm equilibrium propagation may be promising for fast and energy efficient learning, but states still need to be inferred and stored twice. Inspired by lifted neural networks and compartmental neuron models we propose a simple energy based compartmental neuron model, termed dual propagation, in which each neuron is a dyad with two intrinsic states. At inference time these intrinsic states encode the error/activity duality through their difference and their mean respectively. The advantage of this method is that only a single inference phase is needed and that inference can be ",
    "path": "papers/23/02/2302.01228.json",
    "total_tokens": 910,
    "translated_title": "双向传播：用二元神经元加速对比海比学习",
    "translated_abstract": "基于活动差异的学习算法，如对比海比学习和平衡传播，已被提出作为生物合理的替代方案，用于替代误差反向传播。然而，在传统的数字芯片上，这些算法需要解决一个昂贵的推断问题两次，使这些方法比反向传播慢两个数量级以上。在模拟情况下，平衡传播可能是快速和能量高效的学习方法，但是状态仍然需要推断和存储两次。受提升的神经网络和分室神经元模型的启发，我们提出了一种简单的基于能量的分室神经元模型，称为双向传播，其中每个神经元是一个由两个内在状态组成的二元组。在推断时，这些内在状态通过它们的差异和平均值编码误差/活动的二元性。这种方法的优点是只需要单个推断阶段，并且推断可以由神经元本身进行，从而减少计算时间和能量消耗。",
    "tldr": "本论文提出了一种双向传播的方法，使用具有两个内在状态的二元组神经元模型来加速对比海比学习，并且只需要单个推断阶段，可降低计算时间和能量消耗。"
}