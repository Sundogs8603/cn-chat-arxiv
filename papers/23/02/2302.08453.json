{
    "title": "T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models. (arXiv:2302.08453v2 [cs.CV] UPDATED)",
    "abstract": "The incredible generative ability of large-scale text-to-image (T2I) models has demonstrated strong power of learning complex structures and meaningful semantics. However, relying solely on text prompts cannot fully take advantage of the knowledge learned by the model, especially when flexible and accurate controlling (e.g., color and structure) is needed. In this paper, we aim to ``dig out\" the capabilities that T2I models have implicitly learned, and then explicitly use them to control the generation more granularly. Specifically, we propose to learn simple and lightweight T2I-Adapters to align internal knowledge in T2I models with external control signals, while freezing the original large T2I models. In this way, we can train various adapters according to different conditions, achieving rich control and editing effects in the color and structure of the generation results. Further, the proposed T2I-Adapters have attractive properties of practical value, such as composability and gen",
    "link": "http://arxiv.org/abs/2302.08453",
    "context": "Title: T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models. (arXiv:2302.08453v2 [cs.CV] UPDATED)\nAbstract: The incredible generative ability of large-scale text-to-image (T2I) models has demonstrated strong power of learning complex structures and meaningful semantics. However, relying solely on text prompts cannot fully take advantage of the knowledge learned by the model, especially when flexible and accurate controlling (e.g., color and structure) is needed. In this paper, we aim to ``dig out\" the capabilities that T2I models have implicitly learned, and then explicitly use them to control the generation more granularly. Specifically, we propose to learn simple and lightweight T2I-Adapters to align internal knowledge in T2I models with external control signals, while freezing the original large T2I models. In this way, we can train various adapters according to different conditions, achieving rich control and editing effects in the color and structure of the generation results. Further, the proposed T2I-Adapters have attractive properties of practical value, such as composability and gen",
    "path": "papers/23/02/2302.08453.json",
    "total_tokens": 981,
    "translated_title": "T2I-Adapter：学习适配器以挖掘文本到图像扩散模型更多可控制能力",
    "translated_abstract": "大规模文本到图像（T2I）模型的惊人生成能力已经展示了学习复杂结构和有意义语义的强大能力。然而，仅仅依靠文本提示无法充分利用模型所学的知识，特别是当需要灵活精确的控制（例如颜色和结构）时。本文旨在“挖掘”T2I模型隐含学习的能力，然后明确地使用它们来更加精确地控制生成结果。具体而言，我们提出学习简单轻量级的T2I适配器，将T2I模型内部知识与外部控制信号对齐，同时保持原始大型T2I模型不变。通过这种方式，我们可以根据不同的条件训练各种适配器，实现在生成结果的颜色和结构方面丰富的控制和编辑效果。此外，所提出的T2I适配器具有组合性和生成效率等实际价值的吸引力。",
    "tldr": "该论文提出利用T2I模型隐含学习的知识来更加精确地控制生成结果。通过训练简单轻量级的T2I适配器来对齐内部知识与外部控制信号，实现在生成结果的颜色和结构方面丰富的控制和编辑效果。",
    "en_tdlr": "This paper aims to utilize the implicit knowledge learned by large-scale text-to-image (T2I) models to achieve more precise control over generation results. By training simple and lightweight T2I-Adapters to align internal knowledge with external control signals, rich control and editing effects in the color and structure of the generation results are achieved."
}