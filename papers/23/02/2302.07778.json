{
    "title": "Measuring the Instability of Fine-Tuning. (arXiv:2302.07778v2 [cs.CL] UPDATED)",
    "abstract": "Fine-tuning pre-trained language models on downstream tasks with varying random seeds has been shown to be unstable, especially on small datasets. Many previous studies have investigated this instability and proposed methods to mitigate it. However, most studies only used the standard deviation of performance scores (SD) as their measure, which is a narrow characterization of instability. In this paper, we analyze SD and six other measures quantifying instability at different levels of granularity. Moreover, we propose a systematic framework to evaluate the validity of these measures. Finally, we analyze the consistency and difference between different measures by reassessing existing instability mitigation methods. We hope our results will inform the development of better measurements of fine-tuning instability.",
    "link": "http://arxiv.org/abs/2302.07778",
    "context": "Title: Measuring the Instability of Fine-Tuning. (arXiv:2302.07778v2 [cs.CL] UPDATED)\nAbstract: Fine-tuning pre-trained language models on downstream tasks with varying random seeds has been shown to be unstable, especially on small datasets. Many previous studies have investigated this instability and proposed methods to mitigate it. However, most studies only used the standard deviation of performance scores (SD) as their measure, which is a narrow characterization of instability. In this paper, we analyze SD and six other measures quantifying instability at different levels of granularity. Moreover, we propose a systematic framework to evaluate the validity of these measures. Finally, we analyze the consistency and difference between different measures by reassessing existing instability mitigation methods. We hope our results will inform the development of better measurements of fine-tuning instability.",
    "path": "papers/23/02/2302.07778.json",
    "total_tokens": 779,
    "translated_title": "测量微调不稳定性",
    "translated_abstract": "在小数据集上，使用不同的随机种子对预训练语言模型进行微调已被证明是不稳定的。许多先前的研究已经调查了这种不稳定性并提出了减轻的方法。然而，大多数研究只使用性能得分的标准差（SD）作为其衡量指标，这是对不稳定性的狭义刻画。在本文中，我们分析了SD和其他六个不同粒度的衡量不稳定性的指标。此外，我们提出了一个系统的框架来评估这些指标的有效性。最后，通过重新评估现有的减轻不稳定性的方法，我们分析了不同指标之间的一致性和差异。我们希望我们的结果可以为改进微调不稳定性的测量方法提供指导。",
    "tldr": "本文分析了微调不稳定性的七个指标，提出了一个评估框架，重新评估了减轻不稳定性的方法，并希望为改进微调不稳定性的测量方法提供指导。",
    "en_tdlr": "This paper analyzes seven metrics for measuring fine-tuning instability, proposes an evaluation framework, reassesses methods for mitigating instability, and aims to provide guidance for improving the measurement of fine-tuning instability."
}