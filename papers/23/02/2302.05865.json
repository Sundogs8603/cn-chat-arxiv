{
    "title": "Flag Aggregator: Scalable Distributed Training under Failures and Augmented Losses using Convex Optimization. (arXiv:2302.05865v2 [cs.LG] UPDATED)",
    "abstract": "Modern ML applications increasingly rely on complex deep learning models and large datasets. There has been an exponential growth in the amount of computation needed to train the largest models. Therefore, to scale computation and data, these models are inevitably trained in a distributed manner in clusters of nodes, and their updates are aggregated before being applied to the model. However, a distributed setup is prone to Byzantine failures of individual nodes, components, and software. With data augmentation added to these settings, there is a critical need for robust and efficient aggregation systems. We define the quality of workers as reconstruction ratios $\\in (0,1]$, and formulate aggregation as a Maximum Likelihood Estimation procedure using Beta densities. We show that the Regularized form of log-likelihood wrt subspace can be approximately solved using iterative least squares solver, and provide convergence guarantees using recent Convex Optimization landscape results. Our e",
    "link": "http://arxiv.org/abs/2302.05865",
    "context": "Title: Flag Aggregator: Scalable Distributed Training under Failures and Augmented Losses using Convex Optimization. (arXiv:2302.05865v2 [cs.LG] UPDATED)\nAbstract: Modern ML applications increasingly rely on complex deep learning models and large datasets. There has been an exponential growth in the amount of computation needed to train the largest models. Therefore, to scale computation and data, these models are inevitably trained in a distributed manner in clusters of nodes, and their updates are aggregated before being applied to the model. However, a distributed setup is prone to Byzantine failures of individual nodes, components, and software. With data augmentation added to these settings, there is a critical need for robust and efficient aggregation systems. We define the quality of workers as reconstruction ratios $\\in (0,1]$, and formulate aggregation as a Maximum Likelihood Estimation procedure using Beta densities. We show that the Regularized form of log-likelihood wrt subspace can be approximately solved using iterative least squares solver, and provide convergence guarantees using recent Convex Optimization landscape results. Our e",
    "path": "papers/23/02/2302.05865.json",
    "total_tokens": 955,
    "translated_title": "Flag Aggregator: 可扩展的分布式训练在故障和增量损失下使用凸优化",
    "translated_abstract": "现代机器学习应用越来越依赖于复杂的深度学习模型和大规模数据集。训练最大模型所需的计算量呈指数增长。因此，为了扩展计算和数据，这些模型不可避免地以集群节点的分布式方式进行训练，并在应用于模型之前进行更新的聚合。然而，分布式设置容易受到个别节点、组件和软件的拜占庭故障的影响。在这些场景中加入数据增强后，对于稳健且高效的聚合系统有着重要的需求。我们将工作质量定义为重建比率（介于(0,1]之间），并将聚合定义为使用Beta密度的最大似然估计过程。我们展示了正则化对子空间似然估计的近似求解方法，并利用最近的凸优化景观结果提供收敛保证。",
    "tldr": "本论文提出了一种名为Flag Aggregator的系统，可以在故障和增量损失的情况下，通过使用凸优化来实现可扩展的分布式训练。该系统利用最大似然估计过程和Beta密度来进行工作节点的聚合，并通过迭代最小二乘求解器近似解决了正则化子空间似然估计问题，同时提供了收敛保证。",
    "en_tdlr": "This paper proposes a system called Flag Aggregator, which enables scalable distributed training under failures and augmented losses using convex optimization. The system utilizes maximum likelihood estimation and Beta densities for worker aggregation, and approximates the regularized subspace likelihood estimation problem using an iterative least squares solver, while providing convergence guarantees."
}