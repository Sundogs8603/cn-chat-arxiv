{
    "title": "LazyGNN: Large-Scale Graph Neural Networks via Lazy Propagation. (arXiv:2302.01503v2 [cs.LG] UPDATED)",
    "abstract": "Recent works have demonstrated the benefits of capturing long-distance dependency in graphs by deeper graph neural networks (GNNs). But deeper GNNs suffer from the long-lasting scalability challenge due to the neighborhood explosion problem in large-scale graphs. In this work, we propose to capture long-distance dependency in graphs by shallower models instead of deeper models, which leads to a much more efficient model, LazyGNN, for graph representation learning. Moreover, we demonstrate that LazyGNN is compatible with existing scalable approaches (such as sampling methods) for further accelerations through the development of mini-batch LazyGNN. Comprehensive experiments demonstrate its superior prediction performance and scalability on large-scale benchmarks. The implementation of LazyGNN is available at https://github.com/RXPHD/Lazy_GNN.",
    "link": "http://arxiv.org/abs/2302.01503",
    "context": "Title: LazyGNN: Large-Scale Graph Neural Networks via Lazy Propagation. (arXiv:2302.01503v2 [cs.LG] UPDATED)\nAbstract: Recent works have demonstrated the benefits of capturing long-distance dependency in graphs by deeper graph neural networks (GNNs). But deeper GNNs suffer from the long-lasting scalability challenge due to the neighborhood explosion problem in large-scale graphs. In this work, we propose to capture long-distance dependency in graphs by shallower models instead of deeper models, which leads to a much more efficient model, LazyGNN, for graph representation learning. Moreover, we demonstrate that LazyGNN is compatible with existing scalable approaches (such as sampling methods) for further accelerations through the development of mini-batch LazyGNN. Comprehensive experiments demonstrate its superior prediction performance and scalability on large-scale benchmarks. The implementation of LazyGNN is available at https://github.com/RXPHD/Lazy_GNN.",
    "path": "papers/23/02/2302.01503.json",
    "total_tokens": 798,
    "translated_title": "通过惰性传播实现大规模图神经网络的 LazyGNN",
    "translated_abstract": "近期的研究表明，通过更深的图神经网络（GNN）捕捉图中的远程依赖性带来了好处。但是，在大规模图中，由于邻域爆炸问题，更深的GNN会受到长期的扩展挑战。在本文中，我们提出通过更浅的模型来捕获图中的远程依赖性，从而得到一种更高效的模型——LazyGNN，用于图表示学习。此外，我们证明，LazyGNN 与现有的可扩展方法（如采样方法）兼容，可以通过开发混合批量的LazyGNN来进一步加速。全面的实验显示了它在大规模基准测试上优异的预测性能和可扩展性。LazyGNN 的实现可在 https://github.com/RXPHD/Lazy_GNN 进行查看。",
    "tldr": "本文提出了一种基于浅层模型和惰性传播的图神经网络模型 LazyGNN，用于大规模图的表示学习，具有很高的效率和可扩展性。",
    "en_tdlr": "This paper proposes LazyGNN, a graph neural network model based on shallower models and lazy propagation for efficient and scalable graph representation learning on large-scale graphs."
}