{
    "title": "Transformers Meet Directed Graphs. (arXiv:2302.00049v2 [cs.LG] UPDATED)",
    "abstract": "Transformers were originally proposed as a sequence-to-sequence model for text but have become vital for a wide range of modalities, including images, audio, video, and undirected graphs. However, transformers for directed graphs are a surprisingly underexplored topic, despite their applicability to ubiquitous domains, including source code and logic circuits. In this work, we propose two direction- and structure-aware positional encodings for directed graphs: (1) the eigenvectors of the Magnetic Laplacian - a direction-aware generalization of the combinatorial Laplacian; (2) directional random walk encodings. Empirically, we show that the extra directionality information is useful in various downstream tasks, including correctness testing of sorting networks and source code understanding. Together with a data-flow-centric graph construction, our model outperforms the prior state of the art on the Open Graph Benchmark Code2 relatively by 14.7%.",
    "link": "http://arxiv.org/abs/2302.00049",
    "context": "Title: Transformers Meet Directed Graphs. (arXiv:2302.00049v2 [cs.LG] UPDATED)\nAbstract: Transformers were originally proposed as a sequence-to-sequence model for text but have become vital for a wide range of modalities, including images, audio, video, and undirected graphs. However, transformers for directed graphs are a surprisingly underexplored topic, despite their applicability to ubiquitous domains, including source code and logic circuits. In this work, we propose two direction- and structure-aware positional encodings for directed graphs: (1) the eigenvectors of the Magnetic Laplacian - a direction-aware generalization of the combinatorial Laplacian; (2) directional random walk encodings. Empirically, we show that the extra directionality information is useful in various downstream tasks, including correctness testing of sorting networks and source code understanding. Together with a data-flow-centric graph construction, our model outperforms the prior state of the art on the Open Graph Benchmark Code2 relatively by 14.7%.",
    "path": "papers/23/02/2302.00049.json",
    "total_tokens": 852,
    "translated_title": "Transformers遇见有向图",
    "translated_abstract": "Transformers最初被提出作为文本的序列到序列模型，但现在已广泛应用于包括图像、音频、视频和无向图等多种模态。然而，有向图的transformers却是一个意外未被充分开发的主题，尽管它们在包括源代码和逻辑电路在内的普遍领域中具有适用性。在这项工作中，我们提出了两种有向图的方向和结构感知的位置编码：（1）磁场拉普拉斯算子的特征向量 - 是组合拉普拉斯算子的方向感知推广；（2）方向随机游走编码。在实证上，我们展示了附加的方向信息在包括排序网络的正确性测试和源代码理解等各种下游任务中的有效性。结合数据流为中心的图构建，我们的模型在Open Graph Benchmark Code2上相对于之前的最新技术提升了14.7%。",
    "tldr": "这项工作提出了两种有向图的方向和结构感知的位置编码，通过应用于排序网络的正确性测试和源代码理解等任务中，该模型相对于之前的最新技术提升了14.7%。",
    "en_tdlr": "This work proposes two direction- and structure-aware positional encodings for directed graphs, and shows that the model outperforms the prior state of the art by 14.7% in tasks such as correctness testing of sorting networks and source code understanding."
}