{
    "title": "AccelTran: A Sparsity-Aware Accelerator for Dynamic Inference with Transformers. (arXiv:2302.14705v2 [cs.AR] UPDATED)",
    "abstract": "Self-attention-based transformer models have achieved tremendous success in the domain of natural language processing. Despite their efficacy, accelerating the transformer is challenging due to its quadratic computational complexity and large activation sizes. Existing transformer accelerators attempt to prune its tokens to reduce memory access, albeit with high compute overheads. Moreover, previous works directly operate on large matrices involved in the attention operation, which limits hardware utilization. In order to address these challenges, this work proposes a novel dynamic inference scheme, DynaTran, which prunes activations at runtime with low overhead, substantially reducing the number of ineffectual operations. This improves the throughput of transformer inference. We further propose tiling the matrices in transformer operations along with diverse dataflows to improve data reuse, thus enabling higher energy efficiency. To effectively implement these methods, we propose Acce",
    "link": "http://arxiv.org/abs/2302.14705",
    "context": "Title: AccelTran: A Sparsity-Aware Accelerator for Dynamic Inference with Transformers. (arXiv:2302.14705v2 [cs.AR] UPDATED)\nAbstract: Self-attention-based transformer models have achieved tremendous success in the domain of natural language processing. Despite their efficacy, accelerating the transformer is challenging due to its quadratic computational complexity and large activation sizes. Existing transformer accelerators attempt to prune its tokens to reduce memory access, albeit with high compute overheads. Moreover, previous works directly operate on large matrices involved in the attention operation, which limits hardware utilization. In order to address these challenges, this work proposes a novel dynamic inference scheme, DynaTran, which prunes activations at runtime with low overhead, substantially reducing the number of ineffectual operations. This improves the throughput of transformer inference. We further propose tiling the matrices in transformer operations along with diverse dataflows to improve data reuse, thus enabling higher energy efficiency. To effectively implement these methods, we propose Acce",
    "path": "papers/23/02/2302.14705.json",
    "total_tokens": 1168,
    "translated_title": "AccelTran：一种稀疏感知加速器，用于动态推理中的Transformer",
    "translated_abstract": "基于Self-attention的Transformer模型在自然语言处理领域取得了巨大的成功。尽管它们高效，但由于它的二次计算复杂度和大的激活大小，加速Transformer是具有挑战性的。现有的Transformer加速器试图修剪其令牌以减少内存访问，但计算开销很高。此外，之前的工作直接操作参与注意力操作的大型矩阵，这限制了硬件利用率。为了应对这些挑战，本文提出了一种新的动态推理方案DynaTran，在运行时修剪激活，开销较低，大大减少了无效操作的数量，提高了Transformer推理的吞吐量。我们进一步建议使用沿着Transformer操作平铺矩阵的各种数据流来提高数据重用率，从而实现更高的能量效率。为了有效实现这些方法，我们提出了AccelTran，一种稀疏感知加速器，利用平铺感知硬件并行和一种新的修剪方案。与现有技术最先进的加速器相比，AccelTran在模型尺寸和内存带宽需求较低的情况下实现了平均加速比为1.9倍的竞争性推理精度，同时保持了大型和小型Transformer模型的竞争性推理精度。",
    "tldr": "本论文提出了一种稀疏感知加速器AccelTran，用于动态推理中的Transformer，通过运行时修剪激活并优化数据流实现了高性能和能量效率的平衡，相较于其他现有加速器，AccelTran在模型尺寸和内存带宽需求较低的情况下实现了平均1.9倍的加速比，同时保持了推理精度。",
    "en_tdlr": "This paper proposes a sparsity-aware accelerator, AccelTran, for dynamic inference with Transformers. By pruning activations at runtime and optimizing dataflows, AccelTran achieves a balance between high performance and energy efficiency. Compared to other existing accelerators, AccelTran achieves an average speedup of 1.9x with lower model sizes and memory bandwidth requirements while maintaining competitive accuracy for both large and small Transformer models."
}