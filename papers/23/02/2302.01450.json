{
    "title": "Performance Bounds for Policy-Based Average Reward Reinforcement Learning Algorithms. (arXiv:2302.01450v3 [cs.LG] UPDATED)",
    "abstract": "Many policy-based reinforcement learning (RL) algorithms can be viewed as instantiations of approximate policy iteration (PI), i.e., where policy improvement and policy evaluation are both performed approximately. In applications where the average reward objective is the meaningful performance metric, discounted reward formulations are often used with the discount factor being close to $1,$ which is equivalent to making the expected horizon very large. However, the corresponding theoretical bounds for error performance scale with the square of the horizon. Thus, even after dividing the total reward by the length of the horizon, the corresponding performance bounds for average reward problems go to infinity. Therefore, an open problem has been to obtain meaningful performance bounds for approximate PI and RL algorithms for the average-reward setting. In this paper, we solve this open problem by obtaining the first finite-time error bounds for average-reward MDPs, and show that the asymp",
    "link": "http://arxiv.org/abs/2302.01450",
    "context": "Title: Performance Bounds for Policy-Based Average Reward Reinforcement Learning Algorithms. (arXiv:2302.01450v3 [cs.LG] UPDATED)\nAbstract: Many policy-based reinforcement learning (RL) algorithms can be viewed as instantiations of approximate policy iteration (PI), i.e., where policy improvement and policy evaluation are both performed approximately. In applications where the average reward objective is the meaningful performance metric, discounted reward formulations are often used with the discount factor being close to $1,$ which is equivalent to making the expected horizon very large. However, the corresponding theoretical bounds for error performance scale with the square of the horizon. Thus, even after dividing the total reward by the length of the horizon, the corresponding performance bounds for average reward problems go to infinity. Therefore, an open problem has been to obtain meaningful performance bounds for approximate PI and RL algorithms for the average-reward setting. In this paper, we solve this open problem by obtaining the first finite-time error bounds for average-reward MDPs, and show that the asymp",
    "path": "papers/23/02/2302.01450.json",
    "total_tokens": 852,
    "translated_title": "基于策略的平均回报强化学习算法的性能界限",
    "translated_abstract": "许多基于策略的强化学习算法可以看作是近似策略迭代的实例，即策略改进和策略评估都是近似进行的。在平均回报目标是有意义的性能度量的应用中，通常使用折扣回报公式，并使折扣因子接近1，这相当于使预期的时间长的无限大。然而，对于误差性能，相应的理论界限与时间长的平方成比例。因此，即使将总回报除以时间长，平均回报问题的相应性能界限仍趋于无穷大。因此，获得近似策略迭代和强化学习算法在平均回报设置中的有意义的性能界限一直是一个未解决的问题。在本文中，我们通过获得平均回报MDPs的有限时间误差界限来解决这个未解决的问题，并证明了渐近情况下，",
    "tldr": "本文通过获得平均回报MDPs的有限时间误差界限解决了近似策略迭代和强化学习算法在平均回报设置中的性能界限问题",
    "en_tdlr": "This paper solves the problem of obtaining meaningful performance bounds for approximate policy iteration and RL algorithms in the average-reward setting by obtaining finite-time error bounds for average-reward MDPs."
}