{
    "title": "Hiding Data Helps: On the Benefits of Masking for Sparse Coding. (arXiv:2302.12715v2 [cs.LG] UPDATED)",
    "abstract": "Sparse coding, which refers to modeling a signal as sparse linear combinations of the elements of a learned dictionary, has proven to be a successful (and interpretable) approach in applications such as signal processing, computer vision, and medical imaging. While this success has spurred much work on provable guarantees for dictionary recovery when the learned dictionary is the same size as the ground-truth dictionary, work on the setting where the learned dictionary is larger (or over-realized) with respect to the ground truth is comparatively nascent. Existing theoretical results in this setting have been constrained to the case of noise-less data. We show in this work that, in the presence of noise, minimizing the standard dictionary learning objective can fail to recover the elements of the ground-truth dictionary in the over-realized regime, regardless of the magnitude of the signal in the data-generating process. Furthermore, drawing from the growing body of work on self-superv",
    "link": "http://arxiv.org/abs/2302.12715",
    "context": "Title: Hiding Data Helps: On the Benefits of Masking for Sparse Coding. (arXiv:2302.12715v2 [cs.LG] UPDATED)\nAbstract: Sparse coding, which refers to modeling a signal as sparse linear combinations of the elements of a learned dictionary, has proven to be a successful (and interpretable) approach in applications such as signal processing, computer vision, and medical imaging. While this success has spurred much work on provable guarantees for dictionary recovery when the learned dictionary is the same size as the ground-truth dictionary, work on the setting where the learned dictionary is larger (or over-realized) with respect to the ground truth is comparatively nascent. Existing theoretical results in this setting have been constrained to the case of noise-less data. We show in this work that, in the presence of noise, minimizing the standard dictionary learning objective can fail to recover the elements of the ground-truth dictionary in the over-realized regime, regardless of the magnitude of the signal in the data-generating process. Furthermore, drawing from the growing body of work on self-superv",
    "path": "papers/23/02/2302.12715.json",
    "total_tokens": 940,
    "translated_title": "遮盖数据有帮助：关于稀疏编码中遮盖的好处",
    "translated_abstract": "稀疏编码被用在信号处理、计算机视觉以及医学成像等领域，在这些应用中，信号被建模为用学习到的字典中的元素的稀疏线性组合。然而，目前大多数研究关注学习到的字典与真实字典大小相同时的情况，并且仅研究了没有噪声的情景。本文中，我们提出了一种简单而有效的数据遮盖方法，能够保证在遮盖的情况下获得可靠的字典恢复结果。我们在多个数据集和信号模态下对我们的遮盖方法进行了实验验证，得到了优于现有方法的结果。",
    "tldr": "本文研究了稀疏编码中学习得到的大于实际字典的情况下，噪声会导致标准的字典学习目标函数无法恢复出实际字典的问题，提出了通过遮盖数据的方法进行可靠的字典恢复适用于多种信号模态。",
    "en_tdlr": "This paper explores the issue of not being able to recover the ground-truth dictionary in over-realized sparse coding when noise is present, proposes a method of masking data to ensure reliable recovery in various signal modalities, and shows its superiority over existing methods on multiple datasets."
}