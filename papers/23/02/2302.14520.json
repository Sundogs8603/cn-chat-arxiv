{
    "title": "Large Language Models Are State-of-the-Art Evaluators of Translation Quality. (arXiv:2302.14520v2 [cs.CL] UPDATED)",
    "abstract": "We describe GEMBA, a GPT-based metric for assessment of translation quality, which works both with a reference translation and without. In our evaluation, we focus on zero-shot prompting, comparing four prompt variants in two modes, based on the availability of the reference. We investigate nine versions of GPT models, including ChatGPT and GPT-4. We show that our method for translation quality assessment only works with GPT~3.5 and larger models. Comparing to results from WMT22's Metrics shared task, our method achieves state-of-the-art accuracy in both modes when compared to MQM-based human labels. Our results are valid on the system level for all three WMT22 Metrics shared task language pairs, namely English into German, English into Russian, and Chinese into English. This provides a first glimpse into the usefulness of pre-trained, generative large language models for quality assessment of translations. We publicly release all our code and prompt templates used for the experiments ",
    "link": "http://arxiv.org/abs/2302.14520",
    "context": "Title: Large Language Models Are State-of-the-Art Evaluators of Translation Quality. (arXiv:2302.14520v2 [cs.CL] UPDATED)\nAbstract: We describe GEMBA, a GPT-based metric for assessment of translation quality, which works both with a reference translation and without. In our evaluation, we focus on zero-shot prompting, comparing four prompt variants in two modes, based on the availability of the reference. We investigate nine versions of GPT models, including ChatGPT and GPT-4. We show that our method for translation quality assessment only works with GPT~3.5 and larger models. Comparing to results from WMT22's Metrics shared task, our method achieves state-of-the-art accuracy in both modes when compared to MQM-based human labels. Our results are valid on the system level for all three WMT22 Metrics shared task language pairs, namely English into German, English into Russian, and Chinese into English. This provides a first glimpse into the usefulness of pre-trained, generative large language models for quality assessment of translations. We publicly release all our code and prompt templates used for the experiments ",
    "path": "papers/23/02/2302.14520.json",
    "total_tokens": 1028,
    "translated_title": "大型语言模型是翻译质量评估的最佳方法",
    "translated_abstract": "我们介绍了一种基于GPT的GEMBA度量方法，用于评估翻译质量，可以在有或没有参考翻译的情况下进行。在评估中，我们针对零-shot提示进行了比较，比较了两种基于参考翻译可用性的模式下的四种提示变量。我们研究了9个GPT模型版本，包括ChatGPT和GPT-4。我们证明了，我们的翻译质量评估方法只适用于GPT 3.5及更大的模型。与WMT22度量共享任务的结果相比，我们的方法在两种模式下与MQM人类标签相比，实现了最先进的准确性。我们的结果对所有三种WMT22度量共享任务的语言对（即英语到德语、英语到俄语和中文到英语）都适用。这为预训练、生成式大型语言模型在翻译质量评估中的实用性提供了首次的展示。我们公开发布了所有代码和用于实验的提示模板。",
    "tldr": "该论文介绍了一种基于GPT模型的翻译质量评估方法GEMBA，并展示了它在零-shot提示、多种模式下与WMT22度量共享任务的MQM人类标签相比取得了最先进的准确性，这为大型语言模型在翻译质量评估中的实用性提供了首次的证明。",
    "en_tdlr": "This paper presents a GPT-based metric called GEMBA for evaluating translation quality which works with or without reference translations. The authors show that their method achieves state-of-the-art accuracy in comparison to human labels in the MQM-based evaluation of the WMT22 Metrics shared task. They also demonstrate that their method only works with GPT 3.5 and larger models, providing a glimpse into the usefulness of pre-trained, generative large language models for translation quality assessment. Code and prompt templates used in the experiments are made publicly available."
}