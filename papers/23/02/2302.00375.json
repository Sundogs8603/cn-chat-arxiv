{
    "title": "Bayes-optimal Learning of Deep Random Networks of Extensive-width. (arXiv:2302.00375v2 [stat.ML] UPDATED)",
    "abstract": "We consider the problem of learning a target function corresponding to a deep, extensive-width, non-linear neural network with random Gaussian weights. We consider the asymptotic limit where the number of samples, the input dimension and the network width are proportionally large. We propose a closed-form expression for the Bayes-optimal test error, for regression and classification tasks. We further compute closed-form expressions for the test errors of ridge regression, kernel and random features regression. We find, in particular, that optimally regularized ridge regression, as well as kernel regression, achieve Bayes-optimal performances, while the logistic loss yields a near-optimal test error for classification. We further show numerically that when the number of samples grows faster than the dimension, ridge and kernel methods become suboptimal, while neural networks achieve test error close to zero from quadratically many samples.",
    "link": "http://arxiv.org/abs/2302.00375",
    "context": "Title: Bayes-optimal Learning of Deep Random Networks of Extensive-width. (arXiv:2302.00375v2 [stat.ML] UPDATED)\nAbstract: We consider the problem of learning a target function corresponding to a deep, extensive-width, non-linear neural network with random Gaussian weights. We consider the asymptotic limit where the number of samples, the input dimension and the network width are proportionally large. We propose a closed-form expression for the Bayes-optimal test error, for regression and classification tasks. We further compute closed-form expressions for the test errors of ridge regression, kernel and random features regression. We find, in particular, that optimally regularized ridge regression, as well as kernel regression, achieve Bayes-optimal performances, while the logistic loss yields a near-optimal test error for classification. We further show numerically that when the number of samples grows faster than the dimension, ridge and kernel methods become suboptimal, while neural networks achieve test error close to zero from quadratically many samples.",
    "path": "papers/23/02/2302.00375.json",
    "total_tokens": 854,
    "translated_title": "深度随机网络的 Bayes 最优学习",
    "translated_abstract": "本文考虑学习一个深度广度非线性神经网络的目标函数，其具有随机高斯权重。我们研究了样本数量、输入维数和网络宽度成比例增加时的渐近情况，并为回归和分类任务提出了 Bayes 最优测试误差的闭式表达式。此外，我们还计算了岭回归、核函数和随机特征回归的测试误差的闭式表达式。我们发现，最优正则化的岭回归以及核回归可以达到 Bayes 最优表现，而逻辑损失函数对于分类问题几乎能达到最优的测试误差。我们通过数字实验证明，当样本数量增长速度快于维数时，岭回归和核方法变得次优，而神经网络可以从平方级的样本数量中获得接近于零的测试误差。",
    "tldr": "本文研究了深度随机网络的学习问题，提出了 Bayes 最优测试误差的闭式表达式。岭回归和核回归能够达到最优表现，而神经网络的测试误差也可以从平方级的样本数量中获得接近于零的结果。",
    "en_tdlr": "This paper studies the learning problem of deep random networks and proposes a closed-form expression for the Bayes-optimal test error. Ridge and kernel regression achieve optimal performance, while neural networks can approach zero test error with quadratically many samples."
}