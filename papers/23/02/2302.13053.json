{
    "title": "Scalable Neural Network Training over Distributed Graphs",
    "abstract": "Graph neural networks (GNNs) fuel diverse machine learning tasks involving graph-structured data, ranging from predicting protein structures to serving personalized recommendations. Real-world graph data must often be stored distributed across many machines not just because of capacity constraints, but because of compliance with data residency or privacy laws. In such setups, network communication is costly and becomes the main bottleneck to train GNNs. Optimizations for distributed GNN training have targeted data-level improvements so far -- via caching, network-aware partitioning, and sub-sampling -- that work for data center-like setups where graph data is accessible to a single entity and data transfer costs are ignored.   We present RETEXO, the first framework which eliminates the severe communication bottleneck in distributed GNN training while respecting any given data partitioning configuration. The key is a new training procedure, lazy message passing, that reorders the sequen",
    "link": "https://arxiv.org/abs/2302.13053",
    "context": "Title: Scalable Neural Network Training over Distributed Graphs\nAbstract: Graph neural networks (GNNs) fuel diverse machine learning tasks involving graph-structured data, ranging from predicting protein structures to serving personalized recommendations. Real-world graph data must often be stored distributed across many machines not just because of capacity constraints, but because of compliance with data residency or privacy laws. In such setups, network communication is costly and becomes the main bottleneck to train GNNs. Optimizations for distributed GNN training have targeted data-level improvements so far -- via caching, network-aware partitioning, and sub-sampling -- that work for data center-like setups where graph data is accessible to a single entity and data transfer costs are ignored.   We present RETEXO, the first framework which eliminates the severe communication bottleneck in distributed GNN training while respecting any given data partitioning configuration. The key is a new training procedure, lazy message passing, that reorders the sequen",
    "path": "papers/23/02/2302.13053.json",
    "total_tokens": 870,
    "translated_title": "可扩展的分布式图上神经网络训练",
    "translated_abstract": "图神经网络（GNN）在涉及图结构数据的各种机器学习任务中发挥着重要作用，包括预测蛋白质结构和提供个性化推荐等。实际世界中的图数据往往需要分布式存储在许多计算机上，原因不仅是因为容量限制，还有数据所在地或隐私法律的要求。在这种设置中，网络通信成本很高，成为训练GNN的主要瓶颈。迄今为止，分布式GNN训练的优化主要针对数据级别的改进，例如缓存、网络感知划分和子采样等，这些方法适用于数据中心类似的设置，其中图数据对单个实体可访问且数据传输成本被忽略。我们提出了RETEXO，这是一种可以消除分布式GNN训练中严重通信瓶颈的首个框架，同时尊重任何给定的数据分区配置。关键是通过一种新的训练过程，即懒消息传递，重新排序了消息传递的顺序。",
    "tldr": "RETEXO是第一个消除分布式图神经网络训练中通信瓶颈的框架，通过新的训练过程懒消息传递来改善网络通信效率。"
}