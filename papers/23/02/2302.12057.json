{
    "title": "ProsAudit, a prosodic benchmark for self-supervised speech models. (arXiv:2302.12057v3 [cs.CL] UPDATED)",
    "abstract": "We present ProsAudit, a benchmark in English to assess structural prosodic knowledge in self-supervised learning (SSL) speech models. It consists of two subtasks, their corresponding metrics, and an evaluation dataset. In the protosyntax task, the model must correctly identify strong versus weak prosodic boundaries. In the lexical task, the model needs to correctly distinguish between pauses inserted between words and within words. We also provide human evaluation scores on this benchmark. We evaluated a series of SSL models and found that they were all able to perform above chance on both tasks, even when evaluated on an unseen language. However, non-native models performed significantly worse than native ones on the lexical task, highlighting the importance of lexical knowledge in this task. We also found a clear effect of size with models trained on more data performing better in the two subtasks.",
    "link": "http://arxiv.org/abs/2302.12057",
    "context": "Title: ProsAudit, a prosodic benchmark for self-supervised speech models. (arXiv:2302.12057v3 [cs.CL] UPDATED)\nAbstract: We present ProsAudit, a benchmark in English to assess structural prosodic knowledge in self-supervised learning (SSL) speech models. It consists of two subtasks, their corresponding metrics, and an evaluation dataset. In the protosyntax task, the model must correctly identify strong versus weak prosodic boundaries. In the lexical task, the model needs to correctly distinguish between pauses inserted between words and within words. We also provide human evaluation scores on this benchmark. We evaluated a series of SSL models and found that they were all able to perform above chance on both tasks, even when evaluated on an unseen language. However, non-native models performed significantly worse than native ones on the lexical task, highlighting the importance of lexical knowledge in this task. We also found a clear effect of size with models trained on more data performing better in the two subtasks.",
    "path": "papers/23/02/2302.12057.json",
    "total_tokens": 891,
    "translated_title": "ProsAudit：自监督语音模型韵律基准测试",
    "translated_abstract": "我们提出 ProsAudit，一个用于评估自监督学习（SSL）语音模型中结构韵律知识的英语基准测试。它由两个子任务、相应的指标和一个评估数据集组成。在原型句法任务中，模型必须正确识别强调和弱调的韵律边界。在词汇任务中，模型需要正确区分插入单词和内部的停顿。我们还提供了对这个基准测试的人为评估分数。我们评估了一系列 SSL 模型，并发现它们在两个任务上都能够在未见过的语言上进行评估时表现良好。然而，非母语模型在词汇任务上表现明显较差，突显了该任务中词汇知识的重要性。我们还发现，模型的大小对表现有明显的影响，训练数据更多的模型在这两个子任务中表现更好。",
    "tldr": "ProsAudit是一个自监督语音模型韵律基准测试，由原型句法任务和词汇任务两部分组成。研究发现模型大小对表现有明显影响，词汇任务中母语模型表现优异，但非母语模型表现较差。",
    "en_tdlr": "ProsAudit is a benchmark for assessing structural prosodic knowledge in self-supervised learning speech models, consisting of two subtasks. The study found that the size of the model has a clear impact on performance, and native models perform better than non-native models in the lexical task."
}