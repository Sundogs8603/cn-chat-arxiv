{
    "title": "Ten Lessons We Have Learned in the New \"Sparseland\": A Short Handbook for Sparse Neural Network Researchers. (arXiv:2302.02596v3 [cs.LG] UPDATED)",
    "abstract": "This article does not propose any novel algorithm or new hardware for sparsity. Instead, it aims to serve the \"common good\" for the increasingly prosperous Sparse Neural Network (SNN) research community. We attempt to summarize some most common confusions in SNNs, that one may come across in various scenarios such as paper review/rebuttal and talks - many drawn from the authors' own bittersweet experiences! We feel that doing so is meaningful and timely, since the focus of SNN research is notably shifting from traditional pruning to more diverse and profound forms of sparsity before, during, and after training. The intricate relationships between their scopes, assumptions, and approaches lead to misunderstandings, for non-experts or even experts in SNNs. In response, we summarize ten Q\\&As of SNNs from many key aspects, including dense vs. sparse, unstructured sparse vs. structured sparse, pruning vs. sparse training, dense-to-sparse training vs. sparse-to-sparse training, static spars",
    "link": "http://arxiv.org/abs/2302.02596",
    "context": "Title: Ten Lessons We Have Learned in the New \"Sparseland\": A Short Handbook for Sparse Neural Network Researchers. (arXiv:2302.02596v3 [cs.LG] UPDATED)\nAbstract: This article does not propose any novel algorithm or new hardware for sparsity. Instead, it aims to serve the \"common good\" for the increasingly prosperous Sparse Neural Network (SNN) research community. We attempt to summarize some most common confusions in SNNs, that one may come across in various scenarios such as paper review/rebuttal and talks - many drawn from the authors' own bittersweet experiences! We feel that doing so is meaningful and timely, since the focus of SNN research is notably shifting from traditional pruning to more diverse and profound forms of sparsity before, during, and after training. The intricate relationships between their scopes, assumptions, and approaches lead to misunderstandings, for non-experts or even experts in SNNs. In response, we summarize ten Q\\&As of SNNs from many key aspects, including dense vs. sparse, unstructured sparse vs. structured sparse, pruning vs. sparse training, dense-to-sparse training vs. sparse-to-sparse training, static spars",
    "path": "papers/23/02/2302.02596.json",
    "total_tokens": 810,
    "translated_title": "新“Sparseland”中我们学到的十个教训: 稀疏神经网络研究者的简短手册",
    "translated_abstract": "本文旨在为不断发展的稀疏神经网络(SNN)研究群体服务。我们尝试总结了SNN中最常见的一些困惑，并从许多关键方面总结了十个问题和答案，包括密集与稀疏，无结构稀疏与结构稀疏，修剪与稀疏训练，稠密到稀疏训练与稀疏到稀疏训练，静态稀疏和动态稀疏等。",
    "tldr": "稀疏神经网络研究群体需要解决的十个核心问题与解决方案被总结在本文中。",
    "en_tdlr": "This article provides a brief handbook for researchers in sparse neural networks (SNNs) by summarizing ten common confusions in the SNN research community and providing solutions, including dense vs. sparse, unstructured sparse vs. structured sparse, pruning vs. sparse training, dense-to-sparse training vs. sparse-to-sparse training, static sparsity vs. dynamic sparsity, etc."
}