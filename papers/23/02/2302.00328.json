{
    "title": "Learning Functional Transduction. (arXiv:2302.00328v2 [cs.LG] UPDATED)",
    "abstract": "Research in machine learning has polarized into two general approaches for regression tasks: Transductive methods construct estimates directly from available data but are usually problem unspecific. Inductive methods can be much more specific but generally require compute-intensive solution searches. In this work, we propose a hybrid approach and show that transductive regression principles can be meta-learned through gradient descent to form efficient in-context neural approximators by leveraging the theory of vector-valued Reproducing Kernel Banach Spaces (RKBS). We apply this approach to function spaces defined over finite and infinite-dimensional spaces (function-valued operators) and show that once trained, the Transducer can almost instantaneously capture an infinity of functional relationships given a few pairs of input and output examples and return new image estimates. We demonstrate the benefit of our meta-learned transductive approach to model complex physical systems influe",
    "link": "http://arxiv.org/abs/2302.00328",
    "context": "Title: Learning Functional Transduction. (arXiv:2302.00328v2 [cs.LG] UPDATED)\nAbstract: Research in machine learning has polarized into two general approaches for regression tasks: Transductive methods construct estimates directly from available data but are usually problem unspecific. Inductive methods can be much more specific but generally require compute-intensive solution searches. In this work, we propose a hybrid approach and show that transductive regression principles can be meta-learned through gradient descent to form efficient in-context neural approximators by leveraging the theory of vector-valued Reproducing Kernel Banach Spaces (RKBS). We apply this approach to function spaces defined over finite and infinite-dimensional spaces (function-valued operators) and show that once trained, the Transducer can almost instantaneously capture an infinity of functional relationships given a few pairs of input and output examples and return new image estimates. We demonstrate the benefit of our meta-learned transductive approach to model complex physical systems influe",
    "path": "papers/23/02/2302.00328.json",
    "total_tokens": 853,
    "translated_title": "学习函数转导",
    "translated_abstract": "机器学习中的研究分为两种方法来进行回归任务：转导方法直接从可用的数据中构建估计，但通常问题不具体。归纳方法可以更具体，但通常需要计算密集的解决方案搜索。在这项工作中，我们提出了一种混合方法，并展示了通过梯度下降元学习转导回归原则，通过利用向量值再生核Banach空间（RKBS）理论形成高效的上下文神经逼近器，我们将此方法应用于有限和无限维空间（函数值算子）上的函数空间，并展示了一旦训练完成，转导器几乎可以即时地捕捉到无穷多的功能关系，给定少量输入和输出示例，并返回新的图像估计。我们演示了我们元学的转导方法对于模拟复杂物理系统的模型的好处。",
    "tldr": "提出了一种利用元学习方法和向量值再生核Banach空间理论形成高效的上下文神经逼近器的混合方法，将其应用于函数空间，可以在给定少量输入和输出示例的情况下帮助快速捕捉到无穷多的功能关系。",
    "en_tdlr": "Proposed a hybrid approach that leverages meta-learning and the theory of vector-valued Reproducing Kernel Banach Spaces to form efficient in-context neural approximators. It is applied to function spaces, allowing for the quick capture of an infinity of functional relationships given just a few input and output examples."
}