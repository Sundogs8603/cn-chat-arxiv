{
    "title": "The Expressive Power of Tuning Only the Normalization Layers. (arXiv:2302.07937v2 [cs.LG] UPDATED)",
    "abstract": "Feature normalization transforms such as Batch and Layer-Normalization have become indispensable ingredients of state-of-the-art deep neural networks. Recent studies on fine-tuning large pretrained models indicate that just tuning the parameters of these affine transforms can achieve high accuracy for downstream tasks. These findings open the questions about the expressive power of tuning the normalization layers of frozen networks. In this work, we take the first step towards this question and show that for random ReLU networks, fine-tuning only its normalization layers can reconstruct any target network that is $O(\\sqrt{\\text{width}})$ times smaller. We show that this holds even for randomly sparsified networks, under sufficient overparameterization, in agreement with prior empirical work.",
    "link": "http://arxiv.org/abs/2302.07937",
    "context": "Title: The Expressive Power of Tuning Only the Normalization Layers. (arXiv:2302.07937v2 [cs.LG] UPDATED)\nAbstract: Feature normalization transforms such as Batch and Layer-Normalization have become indispensable ingredients of state-of-the-art deep neural networks. Recent studies on fine-tuning large pretrained models indicate that just tuning the parameters of these affine transforms can achieve high accuracy for downstream tasks. These findings open the questions about the expressive power of tuning the normalization layers of frozen networks. In this work, we take the first step towards this question and show that for random ReLU networks, fine-tuning only its normalization layers can reconstruct any target network that is $O(\\sqrt{\\text{width}})$ times smaller. We show that this holds even for randomly sparsified networks, under sufficient overparameterization, in agreement with prior empirical work.",
    "path": "papers/23/02/2302.07937.json",
    "total_tokens": 765,
    "translated_title": "仅调整归一化层的表达能力",
    "translated_abstract": "特征归一化转换，如批量归一化和层归一化，已成为当今先进深度神经网络不可或缺的组成部分。关于微调大型预训练模型的最近研究表明，仅调整这些仿射变换的参数就可以在下游任务中获得高准确性。这些研究结果引发了对调整冻结网络的归一化层的表达能力的问题。本文首次探讨这个问题，并显示对于随机ReLU网络，仅微调其归一化层可以重建任何大小为O(根号宽度)倍小的目标网络。我们证明，即使在随机稀疏网络中，在足够超参数化的情况下，这个结论也成立，与先前的实证工作一致。",
    "tldr": "本研究发现，仅调整神经网络的归一化层参数就可以达到高准确性，甚至可以重建比原网络小O(根号宽度)倍的目标网络。",
    "en_tdlr": "This study finds that fine-tuning only the parameters of the normalization layers in neural networks can achieve high accuracy, and even reconstruct target networks that are O(sqrt(width)) times smaller."
}