{
    "title": "Is This Loss Informative? Faster Text-to-Image Customization by Tracking Objective Dynamics. (arXiv:2302.04841v2 [cs.CV] UPDATED)",
    "abstract": "Text-to-image generation models represent the next step of evolution in image synthesis, offering a natural way to achieve flexible yet fine-grained control over the result. One emerging area of research is the fast adaptation of large text-to-image models to smaller datasets or new visual concepts. However, many efficient methods of adaptation have a long training time, which limits their practical applications, slows down research experiments, and spends excessive GPU resources. In this work, we study the training dynamics of popular text-to-image personalization methods (such as Textual Inversion or DreamBooth), aiming to speed them up. We observe that most concepts are learned at early stages and do not improve in quality later, but standard model convergence metrics fail to indicate that. Instead, we propose a simple drop-in early stopping criterion that only requires computing the regular training objective on a fixed set of inputs for all training iterations. Our experiments on ",
    "link": "http://arxiv.org/abs/2302.04841",
    "context": "Title: Is This Loss Informative? Faster Text-to-Image Customization by Tracking Objective Dynamics. (arXiv:2302.04841v2 [cs.CV] UPDATED)\nAbstract: Text-to-image generation models represent the next step of evolution in image synthesis, offering a natural way to achieve flexible yet fine-grained control over the result. One emerging area of research is the fast adaptation of large text-to-image models to smaller datasets or new visual concepts. However, many efficient methods of adaptation have a long training time, which limits their practical applications, slows down research experiments, and spends excessive GPU resources. In this work, we study the training dynamics of popular text-to-image personalization methods (such as Textual Inversion or DreamBooth), aiming to speed them up. We observe that most concepts are learned at early stages and do not improve in quality later, but standard model convergence metrics fail to indicate that. Instead, we propose a simple drop-in early stopping criterion that only requires computing the regular training objective on a fixed set of inputs for all training iterations. Our experiments on ",
    "path": "papers/23/02/2302.04841.json",
    "total_tokens": 915,
    "translated_title": "这是一篇关于通过跟踪目标动态来实现更快的文本到图像定制的论文",
    "translated_abstract": "文本到图像生成模型代表了图像合成的下一个发展阶段，为实现灵活但精细的控制结果提供了一种自然的方式。研究的一个新兴领域是将大型文本到图像模型快速适应到较小的数据集或新的视觉概念。然而，许多高效的适应方法需要长时间的训练，这限制了它们的实际应用，降低了研究实验的速度，并消耗了过多的GPU资源。在这项工作中，我们研究了流行的文本到图像个性化方法（如文本倒转或梦幻小屋）的训练动态，旨在加速它们。我们观察到大多数概念在早期阶段就已经学习到了，并且质量在后期没有得到改善，但是标准的模型收敛指标未能指示这一点。相反，我们提出了一种简单的即插即用的早停准则，该准则只需要在所有训练迭代中对一组固定输入计算常规训练目标。我们对...进行了实验",
    "tldr": "本文研究了文本到图像个性化方法的训练动态，并提出了一种简单的早停准则来加快训练速度",
    "en_tdlr": "This paper investigates the training dynamics of text-to-image personalization methods and proposes a simple early stopping criterion to speed up the training process."
}