{
    "title": "Learning to Optimize for Reinforcement Learning. (arXiv:2302.01470v2 [cs.LG] UPDATED)",
    "abstract": "In recent years, by leveraging more data, computation, and diverse tasks, learned optimizers have achieved remarkable success in supervised learning, outperforming classical hand-designed optimizers. Reinforcement learning (RL) is essentially different from supervised learning and in practice these learned optimizers do not work well even in simple RL tasks. We investigate this phenomenon and identity three issues. First, the gradients of an RL agent vary across a wide range in logarithms while their absolute values are in a small range, making neural networks hard to obtain accurate parameter updates. Second, the agent-gradient distribution is non-independent and identically distributed, leading to inefficient meta-training. Finally, due to highly stochastic agent-environment interactions, the agent-gradients have high bias and variance, which increase the difficulty of learning an optimizer for RL. We propose gradient processing, pipeline training, and a novel optimizer structure wit",
    "link": "http://arxiv.org/abs/2302.01470",
    "context": "Title: Learning to Optimize for Reinforcement Learning. (arXiv:2302.01470v2 [cs.LG] UPDATED)\nAbstract: In recent years, by leveraging more data, computation, and diverse tasks, learned optimizers have achieved remarkable success in supervised learning, outperforming classical hand-designed optimizers. Reinforcement learning (RL) is essentially different from supervised learning and in practice these learned optimizers do not work well even in simple RL tasks. We investigate this phenomenon and identity three issues. First, the gradients of an RL agent vary across a wide range in logarithms while their absolute values are in a small range, making neural networks hard to obtain accurate parameter updates. Second, the agent-gradient distribution is non-independent and identically distributed, leading to inefficient meta-training. Finally, due to highly stochastic agent-environment interactions, the agent-gradients have high bias and variance, which increase the difficulty of learning an optimizer for RL. We propose gradient processing, pipeline training, and a novel optimizer structure wit",
    "path": "papers/23/02/2302.01470.json",
    "total_tokens": 929,
    "translated_title": "学习优化增强学习",
    "translated_abstract": "近年来，通过利用更多的数据、计算和不同的任务，学习优化器在监督学习中取得了显著的成功，超过了传统手动设计的优化器。然而，强化学习与监督学习本质上不同，这些学习优化器在简单的强化学习任务中效果不佳。我们调查了这一现象，发现了三个问题。首先，强化学习代理的梯度在对数上变化范围很大，而在绝对值上范围较小，这使得神经网络难以获得准确的参数更新。其次，代理梯度分布非独立且不同，导致元训练效率低下。最后，由于代理与环境之间的高度随机交互，代理梯度存在较高的偏差和方差，增加了强化学习优化器的学习难度。我们提出了梯度处理、管道训练和一种新颖的优化器结构。",
    "tldr": "学习优化器在监督学习中取得了显著的成功，但在强化学习中面临梯度范围变化大、梯度分布非独立且不同、高方差偏差等问题。本文提出了梯度处理、管道训练和一种新颖的优化器结构来解决这些问题。"
}