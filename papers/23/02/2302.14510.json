{
    "title": "Bayesian Kernelized Tensor Factorization as Surrogate for Bayesian Optimization. (arXiv:2302.14510v2 [stat.ML] UPDATED)",
    "abstract": "Bayesian optimization (BO) primarily uses Gaussian processes (GP) as the key surrogate model, mostly with a simple stationary and separable kernel function such as the squared-exponential kernel with automatic relevance determination (SE-ARD). However, such simple kernel specifications are deficient in learning functions with complex features, such as being nonstationary, nonseparable, and multimodal. Approximating such functions using a local GP, even in a low-dimensional space, requires a large number of samples, not to mention in a high-dimensional setting. In this paper, we propose to use Bayesian Kernelized Tensor Factorization (BKTF) -- as a new surrogate model -- for BO in a $D$-dimensional Cartesian product space. Our key idea is to approximate the underlying $D$-dimensional solid with a fully Bayesian low-rank tensor CP decomposition, in which we place GP priors on the latent basis functions for each dimension to encode local consistency and smoothness. With this formulation, ",
    "link": "http://arxiv.org/abs/2302.14510",
    "context": "Title: Bayesian Kernelized Tensor Factorization as Surrogate for Bayesian Optimization. (arXiv:2302.14510v2 [stat.ML] UPDATED)\nAbstract: Bayesian optimization (BO) primarily uses Gaussian processes (GP) as the key surrogate model, mostly with a simple stationary and separable kernel function such as the squared-exponential kernel with automatic relevance determination (SE-ARD). However, such simple kernel specifications are deficient in learning functions with complex features, such as being nonstationary, nonseparable, and multimodal. Approximating such functions using a local GP, even in a low-dimensional space, requires a large number of samples, not to mention in a high-dimensional setting. In this paper, we propose to use Bayesian Kernelized Tensor Factorization (BKTF) -- as a new surrogate model -- for BO in a $D$-dimensional Cartesian product space. Our key idea is to approximate the underlying $D$-dimensional solid with a fully Bayesian low-rank tensor CP decomposition, in which we place GP priors on the latent basis functions for each dimension to encode local consistency and smoothness. With this formulation, ",
    "path": "papers/23/02/2302.14510.json",
    "total_tokens": 875,
    "translated_title": "贝叶斯内核张量分解作为贝叶斯优化的替代模型",
    "translated_abstract": "贝叶斯优化（BO）在很大程度上使用高斯过程（GP）作为主要的代理模型，大多使用简单的固定和可分离的内核函数，例如具有自动相关决定（SE-ARD）的平方指数内核。然而，这样的简单内核规格说明不足以学习具有复杂特征的函数，例如非定常，非可分离和多峰。即使在低维空间中，使用局部GP逼近这样的函数也需要大量样本，更不用说在高维环境中了。在本文中，我们提出使用贝叶斯内核张量分解（BKTF）作为$ D $维笛卡尔乘积空间中 BO 的新代理模型。我们的关键思想是使用全贝叶斯低秩张量 CP 分解近似基础的 $ D $ 维实体，在其中我们为每个维度的潜在基础函数放置 GP 先验，以编码局部一致性和平滑性。",
    "tldr": "本文提出了在贝叶斯优化中使用贝叶斯内核张量分解作为代理模型的方法，以学习具有复杂特征的函数。",
    "en_tdlr": "This paper proposes using Bayesian Kernelized Tensor Factorization (BKTF) as a surrogate model for Bayesian optimization to learn functions with complex features."
}