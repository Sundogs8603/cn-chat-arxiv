{
    "title": "Vector Quantized Wasserstein Auto-Encoder. (arXiv:2302.05917v2 [cs.LG] UPDATED)",
    "abstract": "Learning deep discrete latent presentations offers a promise of better symbolic and summarized abstractions that are more useful to subsequent downstream tasks. Inspired by the seminal Vector Quantized Variational Auto-Encoder (VQ-VAE), most of work in learning deep discrete representations has mainly focused on improving the original VQ-VAE form and none of them has studied learning deep discrete representations from the generative viewpoint. In this work, we study learning deep discrete representations from the generative viewpoint. Specifically, we endow discrete distributions over sequences of codewords and learn a deterministic decoder that transports the distribution over the sequences of codewords to the data distribution via minimizing a WS distance between them. We develop further theories to connect it with the clustering viewpoint of WS distance, allowing us to have a better and more controllable clustering solution. Finally, we empirically evaluate our method on several wel",
    "link": "http://arxiv.org/abs/2302.05917",
    "context": "Title: Vector Quantized Wasserstein Auto-Encoder. (arXiv:2302.05917v2 [cs.LG] UPDATED)\nAbstract: Learning deep discrete latent presentations offers a promise of better symbolic and summarized abstractions that are more useful to subsequent downstream tasks. Inspired by the seminal Vector Quantized Variational Auto-Encoder (VQ-VAE), most of work in learning deep discrete representations has mainly focused on improving the original VQ-VAE form and none of them has studied learning deep discrete representations from the generative viewpoint. In this work, we study learning deep discrete representations from the generative viewpoint. Specifically, we endow discrete distributions over sequences of codewords and learn a deterministic decoder that transports the distribution over the sequences of codewords to the data distribution via minimizing a WS distance between them. We develop further theories to connect it with the clustering viewpoint of WS distance, allowing us to have a better and more controllable clustering solution. Finally, we empirically evaluate our method on several wel",
    "path": "papers/23/02/2302.05917.json",
    "total_tokens": 848,
    "translated_title": "向量量化Wasserstein自编码器",
    "translated_abstract": "学习深度离散潜在表示提供了更好的符号和总结的抽象，更有用于后续的下游任务。受到 Vector Quantized Variational Auto-Encoder (VQ-VAE) 的启发，学习深度离散表示的工作主要集中在改进原始的VQ-VAE形式，没有一个讨论从生成的角度学习深度离散表示。在这项工作中，我们从生成的角度研究了学习深度离散表示。具体而言，我们赋予离散分布在码字序列上，并学习一个确定性解码器，通过最小化它们之间的WS距离将码字序列的分布传递到数据分布。我们进一步发展了理论，将其与WS距离的聚类视图连接起来，这使我们可以有更好的、可控的聚类解决方案。最后，我们在几个常见测试集上进行了实证评估。",
    "tldr": "本文提出了一种向量量化Wasserstein自编码器，可通过最小化WS距离将离散潜在表示传递到数据分布，实现更好的、可控的聚类解决方案。"
}