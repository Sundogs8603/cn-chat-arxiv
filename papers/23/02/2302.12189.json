{
    "title": "HL Dataset: Visually-grounded Description of Scenes, Actions and Rationales. (arXiv:2302.12189v2 [cs.CL] UPDATED)",
    "abstract": "Current captioning datasets focus on object-centric captions, describing the visible objects in the image, e.g. \"people eating food in a park\". Although these datasets are useful to evaluate the ability of Vision & Language models to recognize and describe visual content, they do not support controlled experiments involving model testing or fine-tuning, with more high-level captions, which humans find easy and natural to produce. For example, people often describe images based on the type of scene they depict ('people at a holiday resort') and the actions they perform ('people having a picnic'). Such descriptions draw on personal experience and commonsense assumptions. We present the High-Level Dataset a dataset extending 14997 images from the COCO dataset, aligned with a new set of 134,973 human-annotated (high-level) captions collected along three axes: scenes, actions, and rationales. We further extend this dataset with confidence scores collected from an independent set of readers,",
    "link": "http://arxiv.org/abs/2302.12189",
    "context": "Title: HL Dataset: Visually-grounded Description of Scenes, Actions and Rationales. (arXiv:2302.12189v2 [cs.CL] UPDATED)\nAbstract: Current captioning datasets focus on object-centric captions, describing the visible objects in the image, e.g. \"people eating food in a park\". Although these datasets are useful to evaluate the ability of Vision & Language models to recognize and describe visual content, they do not support controlled experiments involving model testing or fine-tuning, with more high-level captions, which humans find easy and natural to produce. For example, people often describe images based on the type of scene they depict ('people at a holiday resort') and the actions they perform ('people having a picnic'). Such descriptions draw on personal experience and commonsense assumptions. We present the High-Level Dataset a dataset extending 14997 images from the COCO dataset, aligned with a new set of 134,973 human-annotated (high-level) captions collected along three axes: scenes, actions, and rationales. We further extend this dataset with confidence scores collected from an independent set of readers,",
    "path": "papers/23/02/2302.12189.json",
    "total_tokens": 927,
    "translated_title": "HL数据集: 基于视觉的场景、动作和理由的描述",
    "translated_abstract": "当前的描述数据集侧重于以物体为中心的描述，描述图像中可见的物体，例如“人们在公园里吃东西”。虽然这些数据集对于评估视觉和语言模型识别和描述视觉内容的能力很有用，但它们不支持涉及模型测试或微调的受控实验，使用更高级的描述，人们发现很容易和自然地产生。例如，人们通常根据图像所描绘的场景类型（“人们在度假胜地”）和他们进行的动作（“人们正在野餐”）来描述图像。这些描述基于个人经验和常识性的假设。我们提供了高级别数据集，该数据集扩展了来自COCO数据集的14997个图像，并与一组新的134,973个人工注释（高级别）描述对齐，这些描述从场景、动作和理由三个方面进行了收集。我们进一步使用从独立阅读者组收集的信心得分扩展了该数据集。",
    "tldr": "这个论文介绍了HL数据集，该数据集扩展了COCO数据集，包含14997个图像和134,973个人工注释的高级别描述，涉及场景、动作和理由，可以用于对视觉和语言模型进行更高级别的测试和微调。"
}