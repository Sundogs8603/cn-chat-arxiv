{
    "title": "BrainCLIP: Bridging Brain and Visual-Linguistic Representation via CLIP for Generic Natural Visual Stimulus Decoding from fMRI. (arXiv:2302.12971v2 [cs.CV] UPDATED)",
    "abstract": "Reconstructing perceived natural images or decoding their categories from fMRI signals are challenging tasks with great scientific significance. Due to the lack of paired samples, most existing methods fail to generate semantically recognizable reconstruction and are difficult to generalize to novel classes. In this work, we propose, for the first time, a task-agnostic brain decoding model by unifying the visual stimulus classification and reconstruction tasks in a semantic space. We denote it as BrainCLIP, which leverages CLIP's cross-modal generalization ability to bridge the modality gap between brain activities, images, and texts. Specifically, BrainCLIP is a VAE-based architecture that transforms fMRI patterns into the CLIP embedding space by combining visual and textual supervision. Note that previous works rarely use multi-modal supervision for visual stimulus decoding. Our experiments demonstrate that textual supervision can significantly boost the performance of decoding model",
    "link": "http://arxiv.org/abs/2302.12971",
    "context": "Title: BrainCLIP: Bridging Brain and Visual-Linguistic Representation via CLIP for Generic Natural Visual Stimulus Decoding from fMRI. (arXiv:2302.12971v2 [cs.CV] UPDATED)\nAbstract: Reconstructing perceived natural images or decoding their categories from fMRI signals are challenging tasks with great scientific significance. Due to the lack of paired samples, most existing methods fail to generate semantically recognizable reconstruction and are difficult to generalize to novel classes. In this work, we propose, for the first time, a task-agnostic brain decoding model by unifying the visual stimulus classification and reconstruction tasks in a semantic space. We denote it as BrainCLIP, which leverages CLIP's cross-modal generalization ability to bridge the modality gap between brain activities, images, and texts. Specifically, BrainCLIP is a VAE-based architecture that transforms fMRI patterns into the CLIP embedding space by combining visual and textual supervision. Note that previous works rarely use multi-modal supervision for visual stimulus decoding. Our experiments demonstrate that textual supervision can significantly boost the performance of decoding model",
    "path": "papers/23/02/2302.12971.json",
    "total_tokens": 918,
    "translated_title": "BrainCLIP：通过CLIP框架实现从fMRI中获取自然视觉信息的通用解码方法",
    "translated_abstract": "从fMRI信号重构感知到的自然图像或解码它们的类别是具有科学意义的挑战性任务。由于缺乏成对样本，大多数现有方法无法生成语义可识别的重构，并且难以推广到新颖类别。在这项工作中，我们首次提出了一种任务不可知的大脑解码模型，通过在语义空间中统一视觉刺激分类和重构任务来实现。我们将它命名为BrainCLIP，利用了CLIP跨模态泛化能力，以填补大脑活动，图像和文本之间的模态差距。具体而言，BrainCLIP是一种基于VAE的体系结构，通过结合视觉和文本监督，将fMRI模式转换为CLIP嵌入空间。注意，以前的研究很少使用多模态监督进行视觉刺激解码。我们的实验表明，文本监督可以显着提高解码模型的性能。",
    "tldr": "BrainCLIP是一种从fMRI中获取自然图像信息的解码方法，它利用了CLIP跨模态泛化能力并在语义空间中统一视觉刺激分类和重构任务，同时文本监督也能提高解码模型的性能。",
    "en_tdlr": "BrainCLIP is a decoding method for obtaining natural visual information from fMRI, which utilizes the cross-modal generalization ability of CLIP and unifies the visual stimulus classification and reconstruction tasks in a semantic space. Textual supervision also significantly improves the performance of the decoding model."
}