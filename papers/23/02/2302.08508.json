{
    "title": "Sanity checks and improvements for patch visualisation in prototype-based image classification. (arXiv:2302.08508v2 [cs.CV] UPDATED)",
    "abstract": "In this work, we perform an in-depth analysis of the visualisation methods implemented in two popular self-explaining models for visual classification based on prototypes - ProtoPNet and ProtoTree. Using two fine-grained datasets (CUB-200-2011 and Stanford Cars), we first show that such methods do not correctly identify the regions of interest inside of the images, and therefore do not reflect the model behaviour. Secondly, using a deletion metric, we demonstrate quantitatively that saliency methods such as Smoothgrads or PRP provide more faithful image patches. We also propose a new relevance metric based on the segmentation of the object provided in some datasets (e.g. CUB-200-2011) and show that the imprecise patch visualisations generated by ProtoPNet and ProtoTree can create a false sense of bias that can be mitigated by the use of more faithful methods. Finally, we discuss the implications of our findings for other prototype-based models sharing the same visualisation method.",
    "link": "http://arxiv.org/abs/2302.08508",
    "context": "Title: Sanity checks and improvements for patch visualisation in prototype-based image classification. (arXiv:2302.08508v2 [cs.CV] UPDATED)\nAbstract: In this work, we perform an in-depth analysis of the visualisation methods implemented in two popular self-explaining models for visual classification based on prototypes - ProtoPNet and ProtoTree. Using two fine-grained datasets (CUB-200-2011 and Stanford Cars), we first show that such methods do not correctly identify the regions of interest inside of the images, and therefore do not reflect the model behaviour. Secondly, using a deletion metric, we demonstrate quantitatively that saliency methods such as Smoothgrads or PRP provide more faithful image patches. We also propose a new relevance metric based on the segmentation of the object provided in some datasets (e.g. CUB-200-2011) and show that the imprecise patch visualisations generated by ProtoPNet and ProtoTree can create a false sense of bias that can be mitigated by the use of more faithful methods. Finally, we discuss the implications of our findings for other prototype-based models sharing the same visualisation method.",
    "path": "papers/23/02/2302.08508.json",
    "total_tokens": 831,
    "translated_title": "原型图像分类中补丁可视化的合理性检查和改进",
    "translated_abstract": "本文对基于原型的视觉分类中实施的可视化方法进行了深入分析，使用两个精细的数据集（CUB-200-2011 和 Stanford Cars）首先表明这种方法不能正确地识别图像中的感兴趣区域，因此不能反映出模型的行为。其次，使用删除度量，我们定量地证明了 Smoothgrads 或 PRP 等显著性方法提供了更忠实的图像补丁。我们还提出了一种基于某些数据集（例如 CUB-200-2011）中提供的对象分割的相关性度量，并展示了 ProtoPNet 和 ProtoTree 产生的不精确的补丁可视化可能会产生错误的偏见感，可以通过使用更忠实的方法来减轻。最后，我们讨论了我们的发现对其他使用相同可视化方法的基于原型的模型的影响。",
    "tldr": "本文通过精细的数据集，发现了基于原型的视觉分类中可视化方法的局限性，并提出了使用更忠实方法的必要性。",
    "en_tdlr": "This paper identifies the limitations of visualisation methods in prototype-based image classification using fine-grained datasets and proposes the necessity of using more faithful methods."
}