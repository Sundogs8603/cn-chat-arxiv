{
    "title": "Revisiting Weighted Aggregation in Federated Learning with Neural Networks. (arXiv:2302.10911v2 [cs.LG] UPDATED)",
    "abstract": "In federated learning (FL), weighted aggregation of local models is conducted to generate a global model, and the aggregation weights are normalized (the sum of weights is 1) and proportional to the local data sizes. In this paper, we revisit the weighted aggregation process and gain new insights into the training dynamics of FL. First, we find that the sum of weights can be smaller than 1, causing global weight shrinking effect (analogous to weight decay) and improving generalization. We explore how the optimal shrinking factor is affected by clients' data heterogeneity and local epochs. Second, we dive into the relative aggregation weights among clients to depict the clients' importance. We develop client coherence to study the learning dynamics and find a critical point that exists. Before entering the critical point, more coherent clients play more essential roles in generalization. Based on the above insights, we propose an effective method for Federated Learning with Learnable Ag",
    "link": "http://arxiv.org/abs/2302.10911",
    "context": "Title: Revisiting Weighted Aggregation in Federated Learning with Neural Networks. (arXiv:2302.10911v2 [cs.LG] UPDATED)\nAbstract: In federated learning (FL), weighted aggregation of local models is conducted to generate a global model, and the aggregation weights are normalized (the sum of weights is 1) and proportional to the local data sizes. In this paper, we revisit the weighted aggregation process and gain new insights into the training dynamics of FL. First, we find that the sum of weights can be smaller than 1, causing global weight shrinking effect (analogous to weight decay) and improving generalization. We explore how the optimal shrinking factor is affected by clients' data heterogeneity and local epochs. Second, we dive into the relative aggregation weights among clients to depict the clients' importance. We develop client coherence to study the learning dynamics and find a critical point that exists. Before entering the critical point, more coherent clients play more essential roles in generalization. Based on the above insights, we propose an effective method for Federated Learning with Learnable Ag",
    "path": "papers/23/02/2302.10911.json",
    "total_tokens": 1168,
    "translated_title": "重新审视联邦学习中的加权聚合方法",
    "translated_abstract": "在联邦学习（FL）中，对本地模型进行加权聚合以生成全局模型，聚合权重被标准化（权重和为1）并与本地数据大小成比例。本文重新审视了加权聚合过程，并深入探讨了FL的训练动力学。首先，我们发现权重总和可能小于1，导致全局权重缩小效应（类似于权重衰减）并改善了泛化性能。我们探讨了最优缩小因子如何受到客户端数据异质性和本地周期的影响。其次，我们深入研究了客户端之间的相对聚合权重以描绘客户端的重要性。我们开发了客户端相干性来研究学习动态，并发现存在一个关键点。在进入临界点之前，相干性更高的客户端在泛化中发挥了更重要的作用。基于上述洞见，我们提出了一种有效的联邦学习方法——具有可学习聚合权重的联邦学习（FLLAW），它允许全局权重缩小效应和可学习聚合权重。在各种基准测试中的实验结果表明，FLLAW在更快的收敛速度、更高的准确性和更好的抗数据异质性方面具有很好的效果。",
    "tldr": "本文重新审视了联邦学习中的加权聚合方法。作者发现权重总和可能小于1，从而改善了泛化性能。作者探索了最优缩小因子如何受到客户端数据异质性和本地周期的影响，并使用客户端相干性研究了客户端之间的相对聚合权重以描绘客户端的重要性。作者提出了一种有效的联邦学习方法（FLLAW），该方法具有可学习聚合权重和全局权重缩小效应。",
    "en_tdlr": "This paper revisits the weighted aggregation in federated learning, finding that the sum of weights can be smaller than 1, which improves generalization. The authors explore the optimal shrinking factor affected by data heterogeneity and local epochs, and propose an effective method called FLLAW with learnable aggregation weights and global weight shrinking effect."
}