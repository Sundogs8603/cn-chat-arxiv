{
    "title": "Implicit Regularization Leads to Benign Overfitting for Sparse Linear Regression. (arXiv:2302.00257v2 [cs.LG] UPDATED)",
    "abstract": "In deep learning, often the training process finds an interpolator (a solution with 0 training loss), but the test loss is still low. This phenomenon, known as benign overfitting, is a major mystery that received a lot of recent attention. One common mechanism for benign overfitting is implicit regularization, where the training process leads to additional properties for the interpolator, often characterized by minimizing certain norms. However, even for a simple sparse linear regression problem $y = \\beta^{*\\top} x +\\xi$ with sparse $\\beta^*$, neither minimum $\\ell_1$ or $\\ell_2$ norm interpolator gives the optimal test loss. In this work, we give a different parametrization of the model which leads to a new implicit regularization effect that combines the benefit of $\\ell_1$ and $\\ell_2$ interpolators. We show that training our new model via gradient descent leads to an interpolator with near-optimal test loss. Our result is based on careful analysis of the training dynamics and prov",
    "link": "http://arxiv.org/abs/2302.00257",
    "context": "Title: Implicit Regularization Leads to Benign Overfitting for Sparse Linear Regression. (arXiv:2302.00257v2 [cs.LG] UPDATED)\nAbstract: In deep learning, often the training process finds an interpolator (a solution with 0 training loss), but the test loss is still low. This phenomenon, known as benign overfitting, is a major mystery that received a lot of recent attention. One common mechanism for benign overfitting is implicit regularization, where the training process leads to additional properties for the interpolator, often characterized by minimizing certain norms. However, even for a simple sparse linear regression problem $y = \\beta^{*\\top} x +\\xi$ with sparse $\\beta^*$, neither minimum $\\ell_1$ or $\\ell_2$ norm interpolator gives the optimal test loss. In this work, we give a different parametrization of the model which leads to a new implicit regularization effect that combines the benefit of $\\ell_1$ and $\\ell_2$ interpolators. We show that training our new model via gradient descent leads to an interpolator with near-optimal test loss. Our result is based on careful analysis of the training dynamics and prov",
    "path": "papers/23/02/2302.00257.json",
    "total_tokens": 1053,
    "translated_title": "隐式正则化对于稀疏线性回归的良性过拟合现象具有促进作用",
    "translated_abstract": "在深度学习中，训练过程经常会找到一个内插器（一个0训练损失的解），但测试损失仍然很低。这种被称为良性过拟合的现象，是一个备受关注的重要谜团。良性过拟合的一个常见机制是隐式正则化，训练过程会导致内插器具有额外的性质，常被描述为最小化某些范数。然而，即使对于一个简单的稀疏线性回归问题$y=\\beta^{*\\top}x+\\xi$，最小的$\\ell_1$或$\\ell_2$范数内插器也不能给出最优的测试损失。在这项工作中，我们给出了一个模型的不同参数化形式，它导致了一种新的隐式正则化效应，结合了$\\ell_1$和$\\ell_2$内插器的优点。我们证明，通过梯度下降训练我们的新模型，可以得到一个具有接近最优测试损失的内插器。我们的结果基于对训练动力学的细致分析，并提供了一个新的理解隐式正则化的框架。",
    "tldr": "本文研究发现，隐式正则化可对于稀疏线性回归的良性过拟合现象具有促进作用，并给出了一个模型参数化形式，结合了$\\ell_1$和$\\ell_2$内插器的优点，通过梯度下降训练可得到一个接近最优测试损失的内插器。",
    "en_tdlr": "This paper finds that implicit regularization can promote benign overfitting for sparse linear regression and proposes a new model parameterization combining the benefits of $\\ell_1$ and $\\ell_2$ interpolators, which can be trained via gradient descent to obtain an interpolator with near-optimal test loss."
}