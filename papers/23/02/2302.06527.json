{
    "title": "An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation. (arXiv:2302.06527v3 [cs.SE] UPDATED)",
    "abstract": "Unit tests play a key role in ensuring the correctness of software. However, manually creating unit tests is a laborious task, motivating the need for automation. Large Language Models (LLMs) have recently been applied to this problem, utilizing additional training or few-shot learning on examples of existing tests. This paper presents a large-scale empirical evaluation on the effectiveness of LLMs for automated unit test generation without additional training or manual effort, providing the LLM with the signature and implementation of the function under test, along with usage examples extracted from documentation. We also attempt to repair failed generated tests by re-prompting the model with the failing test and error message. We implement our approach in TestPilot, a test generation tool for JavaScript that automatically generates unit tests for all API functions in an npm package. We evaluate TestPilot using OpenAI's gpt3.5-turbo LLM on 25 npm packages with a total of 1,684 API fun",
    "link": "http://arxiv.org/abs/2302.06527",
    "context": "Title: An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation. (arXiv:2302.06527v3 [cs.SE] UPDATED)\nAbstract: Unit tests play a key role in ensuring the correctness of software. However, manually creating unit tests is a laborious task, motivating the need for automation. Large Language Models (LLMs) have recently been applied to this problem, utilizing additional training or few-shot learning on examples of existing tests. This paper presents a large-scale empirical evaluation on the effectiveness of LLMs for automated unit test generation without additional training or manual effort, providing the LLM with the signature and implementation of the function under test, along with usage examples extracted from documentation. We also attempt to repair failed generated tests by re-prompting the model with the failing test and error message. We implement our approach in TestPilot, a test generation tool for JavaScript that automatically generates unit tests for all API functions in an npm package. We evaluate TestPilot using OpenAI's gpt3.5-turbo LLM on 25 npm packages with a total of 1,684 API fun",
    "path": "papers/23/02/2302.06527.json",
    "total_tokens": 926,
    "translated_title": "使用大型语言模型进行自动化单元测试生成的实证评估",
    "translated_abstract": "单元测试在确保软件正确性方面起着关键作用。然而，手动创建单元测试是一项费力的任务，这促使自动化的需求。最近，大型语言模型（LLMs）已被应用于解决这个问题，利用对现有测试样例的额外训练或少样本学习。本文对LLMs在无需额外训练或手动努力的情况下自动化生成单元测试的有效性进行了大规模实证评估，为LLM提供被测试函数的签名和实现以及从文档中提取的使用示例。我们还尝试通过重新提示模型使用失败的测试和错误消息来修复生成失败的测试。我们在JavaScript中实现了我们的方法，使用TestPilot作为一个自动为npm软件包中的所有API函数生成单元测试的测试生成工具，并使用OpenAI的gpt3.5-turbo LLM在25个npm软件包上进行了评估，共计1,684个API函数。",
    "tldr": "本研究通过大规模实证评估证明，无需额外训练或手动努力，利用大型语言模型（LLMs）自动化生成单元测试是有效的。实验结果表明LLMs结合函数的签名、实现和文档中的使用示例可以成功生成单元测试，并通过重新提示模型来修复生成失败的测试。",
    "en_tdlr": "This study presents a large-scale empirical evaluation on the effectiveness of using large language models (LLMs) to automate unit test generation without additional training or manual effort. The results show that incorporating LLMs with function signatures, implementations, and usage examples from documentation successfully generates unit tests and can repair failed tests by re-prompting the model."
}