{
    "title": "GNOT: A General Neural Operator Transformer for Operator Learning. (arXiv:2302.14376v2 [cs.LG] UPDATED)",
    "abstract": "Learning partial differential equations' (PDEs) solution operators is an essential problem in machine learning. However, there are several challenges for learning operators in practical applications like the irregular mesh, multiple input functions, and complexity of the PDEs' solution. To address these challenges, we propose a general neural operator transformer (GNOT), a scalable and effective transformer-based framework for learning operators. By designing a novel heterogeneous normalized attention layer, our model is highly flexible to handle multiple input functions and irregular meshes. Besides, we introduce a geometric gating mechanism which could be viewed as a soft domain decomposition to solve the multi-scale problems. The large model capacity of the transformer architecture grants our model the possibility to scale to large datasets and practical problems. We conduct extensive experiments on multiple challenging datasets from different domains and achieve a remarkable improv",
    "link": "http://arxiv.org/abs/2302.14376",
    "context": "Title: GNOT: A General Neural Operator Transformer for Operator Learning. (arXiv:2302.14376v2 [cs.LG] UPDATED)\nAbstract: Learning partial differential equations' (PDEs) solution operators is an essential problem in machine learning. However, there are several challenges for learning operators in practical applications like the irregular mesh, multiple input functions, and complexity of the PDEs' solution. To address these challenges, we propose a general neural operator transformer (GNOT), a scalable and effective transformer-based framework for learning operators. By designing a novel heterogeneous normalized attention layer, our model is highly flexible to handle multiple input functions and irregular meshes. Besides, we introduce a geometric gating mechanism which could be viewed as a soft domain decomposition to solve the multi-scale problems. The large model capacity of the transformer architecture grants our model the possibility to scale to large datasets and practical problems. We conduct extensive experiments on multiple challenging datasets from different domains and achieve a remarkable improv",
    "path": "papers/23/02/2302.14376.json",
    "total_tokens": 933,
    "translated_title": "GNOT: 一种用于运算符学习的通用神经运算符Transformer",
    "translated_abstract": "在机器学习中，学习偏微分方程的解算子是一个重要的问题。然而，在实际应用中，学习算子存在一些挑战，例如不规则的网格、多个输入函数和解决PDE解的复杂性。为了解决这些挑战，我们提出了GNOT，一种可扩展且有效的基于Transformer的算子学习框架。通过设计新颖的异构归一化注意力层，我们的模型高度灵活，能够处理多个输入函数和不规则网格。此外，我们引入了一种几何门控机制，可以看作是软域分解以解决多尺度问题。Transformer体系结构的大模型容量使我们的模型能够应用于大型数据集和实际问题。我们在不同领域的多个具有挑战性的数据集上进行了广泛的实验，并取得了显着的改进。",
    "tldr": "提出了一种通用神经运算符Transformer——GNOT，用于解决机器学习中学习偏微分方程的解算子的问题，并通过设计新颖的异构归一化注意力层和引入几何门控机制来增强模型的灵活性和解决多尺度问题。在多个领域的具有挑战性的数据集上进行广泛实验，取得了显着的改进。",
    "en_tdlr": "GNOT is proposed as a scalable and effective transformer-based framework for learning operators in machine learning, which can handle the challenges of irregular mesh, multiple input functions and complexity of PDEs' solution through the design of a novel heterogeneous normalized attention layer and introduction of a geometric gating mechanism for solving multi-scale problems. The experiments on challenging datasets from various domains show remarkable improvement."
}