{
    "title": "Online Recommendations for Agents with Discounted Adaptive Preferences",
    "abstract": "We consider a bandit recommendations problem in which an agent's preferences (representing selection probabilities over recommended items) evolve as a function of past selections, according to an unknown $\\textit{preference model}$. In each round, we show a menu of $k$ items (out of $n$ total) to the agent, who then chooses a single item, and we aim to minimize regret with respect to some $\\textit{target set}$ (a subset of the item simplex) for adversarial losses over the agent's choices. Extending the setting from Agarwal and Brown (2022), where uniform-memory agents were considered, here we allow for non-uniform memory in which a discount factor is applied to the agent's memory vector at each subsequent round. In the \"long-term memory\" regime (when the effective memory horizon scales with $T$ sublinearly), we show that efficient sublinear regret is obtainable with respect to the set of $\\textit{everywhere instantaneously realizable distributions}$ (the \"EIRD set\", as formulated in pr",
    "link": "https://arxiv.org/abs/2302.06014",
    "context": "Title: Online Recommendations for Agents with Discounted Adaptive Preferences\nAbstract: We consider a bandit recommendations problem in which an agent's preferences (representing selection probabilities over recommended items) evolve as a function of past selections, according to an unknown $\\textit{preference model}$. In each round, we show a menu of $k$ items (out of $n$ total) to the agent, who then chooses a single item, and we aim to minimize regret with respect to some $\\textit{target set}$ (a subset of the item simplex) for adversarial losses over the agent's choices. Extending the setting from Agarwal and Brown (2022), where uniform-memory agents were considered, here we allow for non-uniform memory in which a discount factor is applied to the agent's memory vector at each subsequent round. In the \"long-term memory\" regime (when the effective memory horizon scales with $T$ sublinearly), we show that efficient sublinear regret is obtainable with respect to the set of $\\textit{everywhere instantaneously realizable distributions}$ (the \"EIRD set\", as formulated in pr",
    "path": "papers/23/02/2302.06014.json",
    "total_tokens": 964,
    "translated_title": "在具有折扣自适应偏好的代理中的在线推荐问题",
    "translated_abstract": "我们考虑了一个Bandit推荐问题，其中代理的偏好（代表对推荐物品的选择概率）根据过去的选择作为未知\"偏好模型\"的函数而演变。在每一轮中，我们向代理展示$k$个物品（共$n$个），代理选择一个物品，我们的目标是在代理的选择上对于某个\"目标集合\"（物品简单形式的子集）的对抗损失最小化后悔。我们扩展了Agarwal和Brown（2022）的设定，他们考虑了一种均匀记忆的代理，而在这里，我们允许非均匀记忆，在每一轮中对代理的记忆向量应用一个折扣因子。在\"长期记忆\"的情况下（当有效的记忆时程与$T$的次线性变化），我们证明了可以实现对于\"能够在任何时刻实现的分布集合\"（即\"即时实现分布集合\"）的高效次线性后悔。",
    "tldr": "本文研究了具有折扣自适应偏好的代理的在线推荐问题，通过在每一轮中展示一系列物品并考虑代理的偏好演变，以实现对于目标集合的最小化后悔。在长期记忆的情况下，可以实现对于能够在任何时刻实现的分布集合的高效次线性后悔。",
    "en_tdlr": "This paper investigates the problem of online recommendations for agents with discounted adaptive preferences. By considering the evolution of agent preferences and minimizing regret with respect to a target set, efficient sublinear regret is achievable for everywhere instantaneously realizable distributions in the long-term memory regime."
}