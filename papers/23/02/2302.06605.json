{
    "title": "UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling. (arXiv:2302.06605v2 [cs.CV] UPDATED)",
    "abstract": "Large-scale vision-language pre-trained models have shown promising transferability to various downstream tasks. As the size of these foundation models and the number of downstream tasks grow, the standard full fine-tuning paradigm becomes unsustainable due to heavy computational and storage costs. This paper proposes UniAdapter, which unifies unimodal and multimodal adapters for parameter-efficient cross-modal adaptation on pre-trained vision-language models. Specifically, adapters are distributed to different modalities and their interactions, with the total number of tunable parameters reduced by partial weight sharing. The unified and knowledge-sharing design enables powerful cross-modal representations that can benefit various downstream tasks, requiring only 1.0%-2.0% tunable parameters of the pre-trained model. Extensive experiments on 6 cross-modal downstream benchmarks (including video-text retrieval, image-text retrieval, VideoQA, and VQA) show that in most cases, UniAdapter ",
    "link": "http://arxiv.org/abs/2302.06605",
    "context": "Title: UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling. (arXiv:2302.06605v2 [cs.CV] UPDATED)\nAbstract: Large-scale vision-language pre-trained models have shown promising transferability to various downstream tasks. As the size of these foundation models and the number of downstream tasks grow, the standard full fine-tuning paradigm becomes unsustainable due to heavy computational and storage costs. This paper proposes UniAdapter, which unifies unimodal and multimodal adapters for parameter-efficient cross-modal adaptation on pre-trained vision-language models. Specifically, adapters are distributed to different modalities and their interactions, with the total number of tunable parameters reduced by partial weight sharing. The unified and knowledge-sharing design enables powerful cross-modal representations that can benefit various downstream tasks, requiring only 1.0%-2.0% tunable parameters of the pre-trained model. Extensive experiments on 6 cross-modal downstream benchmarks (including video-text retrieval, image-text retrieval, VideoQA, and VQA) show that in most cases, UniAdapter ",
    "path": "papers/23/02/2302.06605.json",
    "total_tokens": 930,
    "translated_title": "UniAdapter: 用于跨模态建模的统一参数高效传递学习",
    "translated_abstract": "大规模的视觉-语言预训练模型已经证明在各种下游任务中具有良好的可转移性。随着这些基础模型的规模和下游任务的数量增加，由于计算和存储成本的增加，标准的完全微调范式变得不可行。本文提出了 UniAdapter，它将单模态和多模态适配器统一起来，对预训练的视觉-语言模型进行参数高效的跨模态适应。具体而言，适配器分布在不同的模态及其交互中，并通过部分权重共享来减少可调参数的总数。统一和知识共享的设计实现了强大的跨模态表示，可以帮助各种下游任务，只需要预训练模型的1.0％-2.0％可调参数。在包括视频-文本检索、图像-文本检索、VideoQA和VQA在内的6个跨模态下游基准上进行了大量实验，结果显示，在大多数情况下，UniAdapter都能带来更好的性能和更低的成本。",
    "tldr": "本论文提出了 UniAdapter，它是一种新颖的跨模态适配器，可以高效跨模态适应预训练的视觉-语言模型，能够用较少的调参成本提升下游任务的性能。",
    "en_tdlr": "This paper proposes UniAdapter, a novel cross-modal adapter that efficiently adapts pre-trained vision-language models with fewer tunable parameters, enabling powerful cross-modal representations that benefit downstream tasks at lower costs. Extensive experiments on 6 cross-modal downstream benchmarks show improved performance in most cases."
}