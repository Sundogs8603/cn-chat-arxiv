{
    "title": "Guided Deep Kernel Learning. (arXiv:2302.09574v2 [cs.LG] UPDATED)",
    "abstract": "Combining Gaussian processes with the expressive power of deep neural networks is commonly done nowadays through deep kernel learning (DKL). Unfortunately, due to the kernel optimization process, this often results in losing their Bayesian benefits. In this study, we present a novel approach for learning deep kernels by utilizing infinite-width neural networks. We propose to use the Neural Network Gaussian Process (NNGP) model as a guide to the DKL model in the optimization process. Our approach harnesses the reliable uncertainty estimation of the NNGPs to adapt the DKL target confidence when it encounters novel data points. As a result, we get the best of both worlds, we leverage the Bayesian behavior of the NNGP, namely its robustness to overfitting, and accurate uncertainty estimation, while maintaining the generalization abilities, scalability, and flexibility of deep kernels. Empirically, we show on multiple benchmark datasets of varying sizes and dimensionality, that our method i",
    "link": "http://arxiv.org/abs/2302.09574",
    "context": "Title: Guided Deep Kernel Learning. (arXiv:2302.09574v2 [cs.LG] UPDATED)\nAbstract: Combining Gaussian processes with the expressive power of deep neural networks is commonly done nowadays through deep kernel learning (DKL). Unfortunately, due to the kernel optimization process, this often results in losing their Bayesian benefits. In this study, we present a novel approach for learning deep kernels by utilizing infinite-width neural networks. We propose to use the Neural Network Gaussian Process (NNGP) model as a guide to the DKL model in the optimization process. Our approach harnesses the reliable uncertainty estimation of the NNGPs to adapt the DKL target confidence when it encounters novel data points. As a result, we get the best of both worlds, we leverage the Bayesian behavior of the NNGP, namely its robustness to overfitting, and accurate uncertainty estimation, while maintaining the generalization abilities, scalability, and flexibility of deep kernels. Empirically, we show on multiple benchmark datasets of varying sizes and dimensionality, that our method i",
    "path": "papers/23/02/2302.09574.json",
    "total_tokens": 973,
    "translated_title": "引导深度核学习",
    "translated_abstract": "现在通常通过深度核学习 (DKL) 将高斯过程与深度神经网络的表达能力结合起来。不幸的是，由于核优化过程，这种方法常常会失去它们的贝叶斯优势。在本研究中，我们提出了一种利用无限宽度神经网络学习深度核的新方法。我们提出使用神经网络高斯过程 (NNGP) 模型作为 DKl 模型在优化过程中的指导。我们的方法利用 NNGP 的可靠不确定性估计，以使 DKL 在遇到新数据点时能够适应目标置信度。因此，我们既利用了 NNGP 的贝叶斯行为 (即其抗过拟合性和准确的不确定性估计)，又保持了深度核的泛化能力，可扩展性和灵活性。在多个基准数据集上进行的实证分析表明，我们的方法在不同大小和维度的数据集上都取得了很好的效果。",
    "tldr": "本文提出了一种引导深度核学习的方法，利用无限宽度神经网络学习深度核，通过神经网络高斯过程模型在优化中指导深度核学习模型，在遇到新数据点时能够适应目标置信度，既利用了贝叶斯行为，又保持了深度核的泛化能力、可扩展性和灵活性。",
    "en_tdlr": "This paper proposes a guided deep kernel learning method, which uses infinite-width neural networks to learn deep kernels and utilizes the Neural Network Gaussian Process (NNGP) model to guide the optimization process of the deep kernel learning model, adapt the target confidence when encountering novel data points, and thus leverage the Bayesian behavior while maintaining the generalization abilities, scalability, and flexibility of deep kernels."
}