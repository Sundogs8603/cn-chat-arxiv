{
    "title": "Listen2Scene: Interactive material-aware binaural soundbpropagation for reconstructed 3D scenes. (arXiv:2302.02809v2 [eess.AS] UPDATED)",
    "abstract": "We present an end-to-end binaural audio rendering approach (Listen2Scene) for virtual reality (VR) and augmented reality (AR) applications. We propose a novel neural-network-based binaural sound propagation method to generate acoustic effects for 3D models of real environments. Any clean audio or dry audio can be convolved with the generated acoustic effects to render audio corresponding to the real environment. We propose a graph neural network that uses both the material and the topology information of the 3D scenes and generates a scene latent vector. Moreover, we use a conditional generative adversarial network (CGAN) to generate acoustic effects from the scene latent vector. Our network is able to handle holes or other artifacts in the reconstructed 3D mesh model. We present an efficient cost function to the generator network to incorporate spatial audio effects. Given the source and the listener position, our learning-based binaural sound propagation approach can generate an acou",
    "link": "http://arxiv.org/abs/2302.02809",
    "context": "Title: Listen2Scene: Interactive material-aware binaural soundbpropagation for reconstructed 3D scenes. (arXiv:2302.02809v2 [eess.AS] UPDATED)\nAbstract: We present an end-to-end binaural audio rendering approach (Listen2Scene) for virtual reality (VR) and augmented reality (AR) applications. We propose a novel neural-network-based binaural sound propagation method to generate acoustic effects for 3D models of real environments. Any clean audio or dry audio can be convolved with the generated acoustic effects to render audio corresponding to the real environment. We propose a graph neural network that uses both the material and the topology information of the 3D scenes and generates a scene latent vector. Moreover, we use a conditional generative adversarial network (CGAN) to generate acoustic effects from the scene latent vector. Our network is able to handle holes or other artifacts in the reconstructed 3D mesh model. We present an efficient cost function to the generator network to incorporate spatial audio effects. Given the source and the listener position, our learning-based binaural sound propagation approach can generate an acou",
    "path": "papers/23/02/2302.02809.json",
    "total_tokens": 971,
    "translated_title": "Listen2Scene：交互式物质感知双耳音频传播重构三维场景",
    "translated_abstract": "本文提出了一种用于虚拟现实（VR）和增强现实（AR）应用的端到端双耳音频渲染方法（Listen2Scene）。我们提出了一种新颖的基于神经网络的双耳声学传播方法，以生成真实环境的3D模型的声学效果。任何清洁音频或干音频都可以与生成的声学效果卷积，以渲染与真实环境相对应的音频。我们提出了一个图神经网络，利用3D场景的材料和拓扑信息生成场景潜在向量。此外，我们使用条件生成对抗网络（CGAN）从场景潜在向量生成声学效果。我们的网络能够处理重构的三维网格模型中的孔洞或其他伪像。我们提出了一种高效的成本函数，用于生成器网络以整合空间音频效果。给定源和听者位置，我们的基于学习的双耳声音传播方法可以生成与真实环境精度相符的声学输出。",
    "tldr": "本文提出了一种交互式的物质感知双耳音频传播方法，能够生成真实环境下的渲染音频，利用图神经网络和条件生成对抗网络，处理重构三维模型中的缺陷，并且能够精确生成与真实环境相符的声学输出。",
    "en_tdlr": "This paper proposes an interactive material-aware binaural sound propagation method that is capable of generating rendered audio corresponding to a real environment. It uses a graph neural network and a conditional generative adversarial network to handle defects in the reconstructed 3D scene and can accurately generate acoustic output that matches the real environment."
}