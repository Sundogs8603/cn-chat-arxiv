{
    "title": "High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance. (arXiv:2302.00999v2 [math.OC] UPDATED)",
    "abstract": "During recent years the interest of optimization and machine learning communities in high-probability convergence of stochastic optimization methods has been growing. One of the main reasons for this is that high-probability complexity bounds are more accurate and less studied than in-expectation ones. However, SOTA high-probability non-asymptotic convergence results are derived under strong assumptions such as the boundedness of the gradient noise variance or of the objective's gradient itself. In this paper, we propose several algorithms with high-probability convergence results under less restrictive assumptions. In particular, we derive new high-probability convergence results under the assumption that the gradient/operator noise has bounded central $\\alpha$-th moment for $\\alpha \\in (1,2]$ in the following setups: (i) smooth non-convex / Polyak-Lojasiewicz / convex / strongly convex / quasi-strongly convex minimization problems, (ii) Lipschitz / star-cocoercive and monotone / quas",
    "link": "http://arxiv.org/abs/2302.00999",
    "context": "Title: High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance. (arXiv:2302.00999v2 [math.OC] UPDATED)\nAbstract: During recent years the interest of optimization and machine learning communities in high-probability convergence of stochastic optimization methods has been growing. One of the main reasons for this is that high-probability complexity bounds are more accurate and less studied than in-expectation ones. However, SOTA high-probability non-asymptotic convergence results are derived under strong assumptions such as the boundedness of the gradient noise variance or of the objective's gradient itself. In this paper, we propose several algorithms with high-probability convergence results under less restrictive assumptions. In particular, we derive new high-probability convergence results under the assumption that the gradient/operator noise has bounded central $\\alpha$-th moment for $\\alpha \\in (1,2]$ in the following setups: (i) smooth non-convex / Polyak-Lojasiewicz / convex / strongly convex / quasi-strongly convex minimization problems, (ii) Lipschitz / star-cocoercive and monotone / quas",
    "path": "papers/23/02/2302.00999.json",
    "total_tokens": 1029,
    "translated_title": "高概率界限的随机优化和变分不等式：无界方差情况下的讨论",
    "translated_abstract": "在最近几年，优化和机器学习界对随机优化方法的高概率收敛性越来越感兴趣。其中一个主要原因是高概率复杂度界限比期望复杂度界限更准确且需求较少。然而，现有的高概率非渐近收敛结果都是在假设梯度噪声的方差或目标的梯度本身是有界的情况下得到的。本文提出了几种算法，它们在较宽松的假设下具备高概率收敛性。特别地，我们在以下情况下推导了梯度/算子噪声具有有界中心的α阶矩（其中α∈(1,2]）的情况下的高概率收敛性：（i）光滑非凸/Polyak-Lojasiewicz/凸/强凸/拟强凸最小化问题，（ii）Lipschitz/星形强协同且单调/拟协同的问题。",
    "tldr": "本文提出了几种算法，它们在梯度/算子噪声具有有界中心的 α 阶矩的宽松假设下具备高概率收敛性。这些算法适用于光滑非凸、Polyak-Lojasiewicz、凸、强凸、拟强凸最小化问题以及Lipschitz、星形强协同且单调、拟协同的问题。",
    "en_tdlr": "This paper proposes several algorithms that achieve high-probability convergence under the assumption of bounded central α-th moment of the gradient/operator noise. These algorithms are applicable to various optimization problems, including smooth non-convex, Polyak-Lojasiewicz, convex, strongly convex, quasi-strongly convex minimization, as well as Lipschitz, star-cocoercive and monotone, and quas"
}