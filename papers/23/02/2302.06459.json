{
    "title": "Encoding Sentence Position in Context-Aware Neural Machine Translation with Concatenation. (arXiv:2302.06459v2 [cs.CL] UPDATED)",
    "abstract": "Context-aware translation can be achieved by processing a concatenation of consecutive sentences with the standard Transformer architecture. This paper investigates the intuitive idea of providing the model with explicit information about the position of the sentences contained in the concatenation window. We compare various methods to encode sentence positions into token representations, including novel methods. Our results show that the Transformer benefits from certain sentence position encoding methods on English to Russian translation if trained with a context-discounted loss (Lupo et al., 2022). However, the same benefits are not observed in English to German. Further empirical efforts are necessary to define the conditions under which the proposed approach is beneficial.",
    "link": "http://arxiv.org/abs/2302.06459",
    "context": "Title: Encoding Sentence Position in Context-Aware Neural Machine Translation with Concatenation. (arXiv:2302.06459v2 [cs.CL] UPDATED)\nAbstract: Context-aware translation can be achieved by processing a concatenation of consecutive sentences with the standard Transformer architecture. This paper investigates the intuitive idea of providing the model with explicit information about the position of the sentences contained in the concatenation window. We compare various methods to encode sentence positions into token representations, including novel methods. Our results show that the Transformer benefits from certain sentence position encoding methods on English to Russian translation if trained with a context-discounted loss (Lupo et al., 2022). However, the same benefits are not observed in English to German. Further empirical efforts are necessary to define the conditions under which the proposed approach is beneficial.",
    "path": "papers/23/02/2302.06459.json",
    "total_tokens": 801,
    "translated_title": "利用拼接将句子位置编码引入上下文感知神经机器翻译",
    "translated_abstract": "使用标准Transformer架构处理连续句子拼接可以实现上下文感知翻译。本文研究了给模型提供关于拼接窗口中句子位置显式信息的直觉想法。我们比较了各种编码句子位置的方法，包括新方法。我们的结果表明，如果以context-discounted loss（Lupo et al.，2022）进行训练，则Transformer架构在英语到俄语翻译中受益于某些句子位置编码方法。然而，在英语到德语翻译中并没有观察到同样的收益。进一步的实证研究需要定义此方法有益的条件。",
    "tldr": "本文探究了在上下文感知神经机器翻译中将句子位置信息引入模型的想法，比较了不同的编码方法，结果表明在使用context-discounted loss对英语到俄语翻译时有益，但在英语到德语翻译中无显著收益。",
    "en_tdlr": "This paper investigates the idea of introducing explicit information about sentence positions into context-aware neural machine translation models through various encoding methods, and shows that certain encoding methods are beneficial for English to Russian translation when trained with a discounted loss, but not for English to German translation. Further empirical studies are needed to determine the conditions under which this approach is effective."
}