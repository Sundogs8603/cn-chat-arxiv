{
    "title": "Learning to Play Text-based Adventure Games with Maximum Entropy Reinforcement Learning. (arXiv:2302.10720v2 [cs.LG] UPDATED)",
    "abstract": "Text-based games are a popular testbed for language-based reinforcement learning (RL). In previous work, deep Q-learning is commonly used as the learning agent. Q-learning algorithms are challenging to apply to complex real-world domains due to, for example, their instability in training. Therefore, in this paper, we adapt the soft-actor-critic (SAC) algorithm to the text-based environment. To deal with sparse extrinsic rewards from the environment, we combine it with a potential-based reward shaping technique to provide more informative (dense) reward signals to the RL agent. We apply our method to play difficult text-based games. The SAC method achieves higher scores than the Q-learning methods on many games with only half the number of training steps. This shows that it is well-suited for text-based games. Moreover, we show that the reward shaping technique helps the agent to learn the policy faster and achieve higher scores. In particular, we consider a dynamically learned value fu",
    "link": "http://arxiv.org/abs/2302.10720",
    "context": "Title: Learning to Play Text-based Adventure Games with Maximum Entropy Reinforcement Learning. (arXiv:2302.10720v2 [cs.LG] UPDATED)\nAbstract: Text-based games are a popular testbed for language-based reinforcement learning (RL). In previous work, deep Q-learning is commonly used as the learning agent. Q-learning algorithms are challenging to apply to complex real-world domains due to, for example, their instability in training. Therefore, in this paper, we adapt the soft-actor-critic (SAC) algorithm to the text-based environment. To deal with sparse extrinsic rewards from the environment, we combine it with a potential-based reward shaping technique to provide more informative (dense) reward signals to the RL agent. We apply our method to play difficult text-based games. The SAC method achieves higher scores than the Q-learning methods on many games with only half the number of training steps. This shows that it is well-suited for text-based games. Moreover, we show that the reward shaping technique helps the agent to learn the policy faster and achieve higher scores. In particular, we consider a dynamically learned value fu",
    "path": "papers/23/02/2302.10720.json",
    "total_tokens": 815,
    "translated_title": "使用最大熵强化学习玩文本基础冒险游戏",
    "translated_abstract": "文本基础游戏是语言强化学习的流行测试平台。本文中，我们将软演员-评论家算法(SAC)适应到文本环境中，并结合基于潜力的奖励塑形技术，以提供更多信息密集的奖励信号给强化学习机器人。我们应用该方法来玩具有挑战性的文本游戏。SAC方法在许多游戏中的得分比Q学习方法高，并且只需一半的训练步骤。这表明该方法非常适用于文本游戏。此外，我们还展示了奖励塑形技术可以帮助机器人更快地学习策略并获得更高的分数。",
    "tldr": "本文提出了一种使用最大熵强化学习玩文本基础冒险游戏的方法，通过结合奖励塑形技术，提供更多信息密集的奖励信号给强化学习机器人，从而使其在许多游戏中达到更高的得分，且只需一半的训练步骤。"
}