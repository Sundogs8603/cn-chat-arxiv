{
    "title": "Generating High-Precision Feedback for Programming Syntax Errors using Large Language Models. (arXiv:2302.04662v2 [cs.PL] UPDATED)",
    "abstract": "Large language models (LLMs), such as Codex, hold great promise in enhancing programming education by automatically generating feedback for students. We investigate using LLMs to generate feedback for fixing syntax errors in Python programs, a key scenario in introductory programming. More concretely, given a student's buggy program, our goal is to generate feedback comprising a fixed program along with a natural language explanation describing the errors/fixes, inspired by how a human tutor would give feedback. While using LLMs is promising, the critical challenge is to ensure high precision in the generated feedback, which is imperative before deploying such technology in classrooms. The main research question we study is: Can we develop LLMs-based feedback generation techniques with a tunable precision parameter, giving educators quality control over the feedback that students receive? To this end, we introduce PyFiXV, our technique to generate high-precision feedback powered by Cod",
    "link": "http://arxiv.org/abs/2302.04662",
    "context": "Title: Generating High-Precision Feedback for Programming Syntax Errors using Large Language Models. (arXiv:2302.04662v2 [cs.PL] UPDATED)\nAbstract: Large language models (LLMs), such as Codex, hold great promise in enhancing programming education by automatically generating feedback for students. We investigate using LLMs to generate feedback for fixing syntax errors in Python programs, a key scenario in introductory programming. More concretely, given a student's buggy program, our goal is to generate feedback comprising a fixed program along with a natural language explanation describing the errors/fixes, inspired by how a human tutor would give feedback. While using LLMs is promising, the critical challenge is to ensure high precision in the generated feedback, which is imperative before deploying such technology in classrooms. The main research question we study is: Can we develop LLMs-based feedback generation techniques with a tunable precision parameter, giving educators quality control over the feedback that students receive? To this end, we introduce PyFiXV, our technique to generate high-precision feedback powered by Cod",
    "path": "papers/23/02/2302.04662.json",
    "total_tokens": 1004,
    "translated_title": "利用大型语言模型为编程语法错误生成高精度反馈",
    "translated_abstract": "大型语言模型（LLM）比如Codex有望通过自动生成反馈为编程教育提供帮助。本文研究使用LLM为Python程序中的语法错误生成反馈，这是入门编程中的关键场景之一。具体来说，目标是生成包含固定程序和自然语言解释的反馈，以描述错误修复，就像人类导师一样。虽然使用LLM很有前途，但关键挑战在于确保反馈的高精度，这在将此类技术应用于教室之前是至关重要的。我们研究的主要问题是：我们是否可以开发基于LLM的反馈生成技术，具有可调节的精度参数，以便教育工作者控制学生接收到的反馈质量？为此，我们介绍了PyFiXV，我们的技术，它由Codex提供支持，用于生成高精度反馈。",
    "tldr": "本文介绍了使用LLMs生成高精度反馈的技术，可用于固定Python程序中的语法错误。使用PyFiXV生成的反馈准确性高达92％，错误覆盖率高达72％，在编程教育中有潜在的应用价值。",
    "en_tdlr": "This paper investigates the use of large language models (LLMs) to generate high-precision feedback for fixing syntax errors in Python programs, and introduces PyFiXV, a technique powered by Codex. The generated feedback shows high precision (up to 92% agreement with human annotators) and coverage (up to 72% of the errors), suggesting the potential value of LLMs in programming education."
}