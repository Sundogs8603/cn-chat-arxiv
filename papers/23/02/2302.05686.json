{
    "title": "A High-dimensional Convergence Theorem for U-statistics with Applications to Kernel-based Testing. (arXiv:2302.05686v3 [math.ST] UPDATED)",
    "abstract": "We prove a convergence theorem for U-statistics of degree two, where the data dimension $d$ is allowed to scale with sample size $n$. We find that the limiting distribution of a U-statistic undergoes a phase transition from the non-degenerate Gaussian limit to the degenerate limit, regardless of its degeneracy and depending only on a moment ratio. A surprising consequence is that a non-degenerate U-statistic in high dimensions can have a non-Gaussian limit with a larger variance and asymmetric distribution. Our bounds are valid for any finite $n$ and $d$, independent of individual eigenvalues of the underlying function, and dimension-independent under a mild assumption. As an application, we apply our theory to two popular kernel-based distribution tests, MMD and KSD, whose high-dimensional performance has been challenging to study. In a simple empirical setting, our results correctly predict how the test power at a fixed threshold scales with $d$ and the bandwidth.",
    "link": "http://arxiv.org/abs/2302.05686",
    "context": "Title: A High-dimensional Convergence Theorem for U-statistics with Applications to Kernel-based Testing. (arXiv:2302.05686v3 [math.ST] UPDATED)\nAbstract: We prove a convergence theorem for U-statistics of degree two, where the data dimension $d$ is allowed to scale with sample size $n$. We find that the limiting distribution of a U-statistic undergoes a phase transition from the non-degenerate Gaussian limit to the degenerate limit, regardless of its degeneracy and depending only on a moment ratio. A surprising consequence is that a non-degenerate U-statistic in high dimensions can have a non-Gaussian limit with a larger variance and asymmetric distribution. Our bounds are valid for any finite $n$ and $d$, independent of individual eigenvalues of the underlying function, and dimension-independent under a mild assumption. As an application, we apply our theory to two popular kernel-based distribution tests, MMD and KSD, whose high-dimensional performance has been challenging to study. In a simple empirical setting, our results correctly predict how the test power at a fixed threshold scales with $d$ and the bandwidth.",
    "path": "papers/23/02/2302.05686.json",
    "total_tokens": 1179,
    "translated_title": "一种适用于核心测试的U统计的高维收敛定理",
    "translated_abstract": "我们证明了一个U统计的二次收敛定理，其中数据维度$d$可以随样本大小$n$的变化而变化。我们发现，一个U统计的极限分布会经历从非退化高斯极限到退化极限的相变，不论其退化性如何，只取决于一个矩比率。一个令人惊讶的结果是，在高维情况下，一个非退化的U统计可能具有一个具有较大方差和不对称分布的非高斯极限。我们的界限对任何有限的$n$和$d$都是有效的，与底层函数的个别特征值无关，并且在一个适度的假设下与维度无关。作为应用，我们将我们的理论应用到两个流行的基于核心的分布测试，MMD和KSD上，这些测试在高维性能的研究一直是有挑战性的。在一个简单的经验设置中，我们的结果正确地预测了在固定阈值下测试功率如何随着$d$和带宽的缩放。",
    "tldr": "本论文证明了一个适用于核心测试的高维U统计的收敛定理，并发现U统计的极限分布会经历从非退化高斯极限到退化极限的相变。这一现象对于高维情况下的非退化U统计具有较大方差和不对称分布的非高斯极限具有重要意义。此外，我们提出的界限适用于任何有限数量和维度的样本，与底层函数的特征值无关，并且在某些假设下与维度无关。我们还将我们的理论应用到两个常用的基于核函数的分布测试方法，MMD和KSD，来研究它们的高维性能。我们的结果能够准确预测测试功率如何与维度和带宽的关系。"
}