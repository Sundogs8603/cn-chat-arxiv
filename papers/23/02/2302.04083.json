{
    "title": "Improving the Model Consistency of Decentralized Federated Learning. (arXiv:2302.04083v2 [cs.LG] UPDATED)",
    "abstract": "To mitigate the privacy leakages and communication burdens of Federated Learning (FL), decentralized FL (DFL) discards the central server and each client only communicates with its neighbors in a decentralized communication network. However, existing DFL suffers from high inconsistency among local clients, which results in severe distribution shift and inferior performance compared with centralized FL (CFL), especially on heterogeneous data or sparse communication topology. To alleviate this issue, we propose two DFL algorithms named DFedSAM and DFedSAM-MGS to improve the performance of DFL. Specifically, DFedSAM leverages gradient perturbation to generate local flat models via Sharpness Aware Minimization (SAM), which searches for models with uniformly low loss values. DFedSAM-MGS further boosts DFedSAM by adopting Multiple Gossip Steps (MGS) for better model consistency, which accelerates the aggregation of local flat models and better balances communication complexity and generaliza",
    "link": "http://arxiv.org/abs/2302.04083",
    "context": "Title: Improving the Model Consistency of Decentralized Federated Learning. (arXiv:2302.04083v2 [cs.LG] UPDATED)\nAbstract: To mitigate the privacy leakages and communication burdens of Federated Learning (FL), decentralized FL (DFL) discards the central server and each client only communicates with its neighbors in a decentralized communication network. However, existing DFL suffers from high inconsistency among local clients, which results in severe distribution shift and inferior performance compared with centralized FL (CFL), especially on heterogeneous data or sparse communication topology. To alleviate this issue, we propose two DFL algorithms named DFedSAM and DFedSAM-MGS to improve the performance of DFL. Specifically, DFedSAM leverages gradient perturbation to generate local flat models via Sharpness Aware Minimization (SAM), which searches for models with uniformly low loss values. DFedSAM-MGS further boosts DFedSAM by adopting Multiple Gossip Steps (MGS) for better model consistency, which accelerates the aggregation of local flat models and better balances communication complexity and generaliza",
    "path": "papers/23/02/2302.04083.json",
    "total_tokens": 931,
    "translated_title": "提高分散式联邦学习的模型一致性",
    "translated_abstract": "为了减轻联邦学习（FL）的隐私泄漏和通信负担，分散式联邦学习（DFL）放弃了中央服务器，每个客户端只与其在分散式通信网络中的邻居通信。然而，现有的DFL在本地客户端之间存在高度不一致性，这导致了严重的分布转移和较低的性能，尤其是在异构数据或稀疏通信拓扑上。为了缓解这个问题，我们提出了两种名为DFedSAM和DFedSAM-MGS的DFL算法来改善DFL的性能。具体而言，DFedSAM利用梯度扰动通过Sharpness Aware Minimization（SAM）来生成本地平坦模型，SAM搜索具有统一低损失值的模型。DFedSAM-MGS通过采用多次八卦步骤（MGS）来进一步提高DFedSAM，以实现更好的模型一致性，加速本地平坦模型的聚合，并更好地平衡通信复杂度和泛化性能。",
    "tldr": "为了解决分散式联邦学习中的模型不一致性问题，我们提出了两种算法，DFedSAM 和 DFedSAM-MGS，分别通过采用梯度扰动和多次八卦步骤来生成本地平坦模型，从而提高了 DFL 的性能。"
}