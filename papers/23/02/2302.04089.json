{
    "title": "ZipLM: Inference-Aware Structured Pruning of Language Models. (arXiv:2302.04089v2 [cs.LG] UPDATED)",
    "abstract": "The breakthrough performance of large language models (LLMs) comes with major computational footprints and high deployment costs. In this paper, we progress towards resolving this problem by proposing a novel structured compression approach for LLMs, called ZipLM. ZipLM achieves state-of-the-art accuracy-vs-speedup, while matching a set of desired target runtime speedups in any given inference environment. Specifically, given a model, a dataset, an inference environment, as well as a set of speedup targets, ZipLM iteratively identifies and removes components with the worst loss-runtime trade-off. Unlike prior methods that specialize in either the post-training/one-shot or the gradual compression setting, and only for specific families of models such as BERT (encoder) or GPT (decoder), ZipLM produces state-of-the-art compressed models across all these settings. Furthermore, ZipLM achieves superior results for a fraction of the computational cost relative to prior distillation and prunin",
    "link": "http://arxiv.org/abs/2302.04089",
    "context": "Title: ZipLM: Inference-Aware Structured Pruning of Language Models. (arXiv:2302.04089v2 [cs.LG] UPDATED)\nAbstract: The breakthrough performance of large language models (LLMs) comes with major computational footprints and high deployment costs. In this paper, we progress towards resolving this problem by proposing a novel structured compression approach for LLMs, called ZipLM. ZipLM achieves state-of-the-art accuracy-vs-speedup, while matching a set of desired target runtime speedups in any given inference environment. Specifically, given a model, a dataset, an inference environment, as well as a set of speedup targets, ZipLM iteratively identifies and removes components with the worst loss-runtime trade-off. Unlike prior methods that specialize in either the post-training/one-shot or the gradual compression setting, and only for specific families of models such as BERT (encoder) or GPT (decoder), ZipLM produces state-of-the-art compressed models across all these settings. Furthermore, ZipLM achieves superior results for a fraction of the computational cost relative to prior distillation and prunin",
    "path": "papers/23/02/2302.04089.json",
    "total_tokens": 932,
    "translated_title": "ZipLM: 语言模型的推理感知结构化剪枝",
    "translated_abstract": "大规模语言模型（LLM）的突破性性能给计算和部署成本带来了重大负担。本文通过提出一种名为ZipLM的新型结构化压缩方法，向解决这个问题迈进。ZipLM在达到一组目标推理时速度优于现有的方法，并在任何给定的推理环境中匹配一组目标运行时加速度。具体而言，给定模型、数据集、推理环境以及一组加速度目标，ZipLM迭代地识别并删除损失时长权衡最差的组件。与先前专门用于后训练/一次性或逐渐压缩设置，并且仅适用于特定的模型家族（如BERT（编码器）或GPT（解码器））的方法不同，ZipLM在所有这些设置中生成了最先进的压缩模型。此外，与先前的蒸馏和修剪方法相比，ZipLM以更低的计算成本获得了更好的结果。",
    "tldr": "ZipLM是一种新型的语言模型压缩方法，能够在任何给定的推理环境中实现与目标运行速度相匹配的最先进压缩模型。与现有方法相比，ZipLM在速度和准确性之间取得了最佳的权衡，并且以更低的计算成本获得了更好的结果。",
    "en_tdlr": "ZipLM is a novel compression approach for language models that achieves state-of-the-art compressed models with matched runtime speedups in any given inference environment. It outperforms existing methods in terms of accuracy-vs-speed trade-off and computational cost."
}