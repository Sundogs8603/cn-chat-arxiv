{
    "title": "OPORP: One Permutation + One Random Projection. (arXiv:2302.03505v2 [stat.ML] UPDATED)",
    "abstract": "Consider two $D$-dimensional data vectors (e.g., embeddings): $u, v$. In many embedding-based retrieval (EBR) applications where the vectors are generated from trained models, $D=256\\sim 1024$ are common. In this paper, OPORP (one permutation + one random projection) uses a variant of the ``count-sketch'' type of data structures for achieving data reduction/compression. With OPORP, we first apply a permutation on the data vectors. A random vector $r$ is generated i.i.d. with moments: $E(r_i) = 0, E(r_i^2)=1, E(r_i^3) =0, E(r_i^4)=s$. We multiply (as dot product) $r$ with all permuted data vectors. Then we break the $D$ columns into $k$ equal-length bins and aggregate (i.e., sum) the values in each bin to obtain $k$ samples from each data vector. One crucial step is to normalize the $k$ samples to the unit $l_2$ norm. We show that the estimation variance is essentially: $(s-1)A + \\frac{D-k}{D-1}\\frac{1}{k}\\left[ (1-\\rho^2)^2 -2A\\right]$, where $A\\geq 0$ is a function of the data ($u,v$)",
    "link": "http://arxiv.org/abs/2302.03505",
    "context": "Title: OPORP: One Permutation + One Random Projection. (arXiv:2302.03505v2 [stat.ML] UPDATED)\nAbstract: Consider two $D$-dimensional data vectors (e.g., embeddings): $u, v$. In many embedding-based retrieval (EBR) applications where the vectors are generated from trained models, $D=256\\sim 1024$ are common. In this paper, OPORP (one permutation + one random projection) uses a variant of the ``count-sketch'' type of data structures for achieving data reduction/compression. With OPORP, we first apply a permutation on the data vectors. A random vector $r$ is generated i.i.d. with moments: $E(r_i) = 0, E(r_i^2)=1, E(r_i^3) =0, E(r_i^4)=s$. We multiply (as dot product) $r$ with all permuted data vectors. Then we break the $D$ columns into $k$ equal-length bins and aggregate (i.e., sum) the values in each bin to obtain $k$ samples from each data vector. One crucial step is to normalize the $k$ samples to the unit $l_2$ norm. We show that the estimation variance is essentially: $(s-1)A + \\frac{D-k}{D-1}\\frac{1}{k}\\left[ (1-\\rho^2)^2 -2A\\right]$, where $A\\geq 0$ is a function of the data ($u,v$)",
    "path": "papers/23/02/2302.03505.json",
    "total_tokens": 1069,
    "translated_title": "OPORP：一次置换+一次随机投影",
    "translated_abstract": "考虑两个$D$维数据向量（例如嵌入）：$u, v$。在许多基于嵌入的检索（EBR）应用程序中，$D=256\\sim 1024$很常见。在本文中，OPORP（一次置换+一次随机投影）使用一种“计数草图”类型的数据结构变体进行数据降维/压缩。使用OPORP，我们首先对数据向量进行置换。生成随机向量$r$，i.i.d. ，满足：$E（r_i）=0，E（r_i^2）=1，E（r_i^3）=0，E（r_i^4）=s$。我们将$r$与所有置换数据向量相乘（作为点积）。然后，我们将$D$列分成$k$个相等长度的箱（bin），并汇总（即求和）每个箱中的值以从每个数据向量中获取$k$个样本。一个关键的步骤是将$k$个样本标准化为单位$l_2$范数。我们表明，估计方差本质上是：$(s-1)A + \\frac{D-k}{D-1}\\frac{1}{k}\\left[ (1-\\rho^2)^2 -2A\\right]$，其中$A\\geq 0$是数据（$u,v$）的函数",
    "tldr": "OPORP使用一种\"计数草图\"类型的数据降维/压缩方法，可以用于嵌入式检索，在保证较少的信息损失的前提下，显著降低了计算和存储的成本"
}