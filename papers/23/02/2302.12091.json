{
    "title": "Random Teachers are Good Teachers. (arXiv:2302.12091v2 [cs.LG] UPDATED)",
    "abstract": "In this work, we investigate the implicit regularization induced by teacher-student learning dynamics in self-distillation. To isolate its effect, we describe a simple experiment where we consider teachers at random initialization instead of trained teachers. Surprisingly, when distilling a student into such a random teacher, we observe that the resulting model and its representations already possess very interesting characteristics; (1) we observe a strong improvement of the distilled student over its teacher in terms of probing accuracy. (2) The learned representations are data-dependent and transferable between different tasks but deteriorate strongly if trained on random inputs. (3) The student checkpoint contains sparse subnetworks, so-called lottery tickets, and lies on the border of linear basins in the supervised loss landscape. These observations have interesting consequences for several important areas in machine learning: (1) Self-distillation can work solely based on the im",
    "link": "http://arxiv.org/abs/2302.12091",
    "context": "Title: Random Teachers are Good Teachers. (arXiv:2302.12091v2 [cs.LG] UPDATED)\nAbstract: In this work, we investigate the implicit regularization induced by teacher-student learning dynamics in self-distillation. To isolate its effect, we describe a simple experiment where we consider teachers at random initialization instead of trained teachers. Surprisingly, when distilling a student into such a random teacher, we observe that the resulting model and its representations already possess very interesting characteristics; (1) we observe a strong improvement of the distilled student over its teacher in terms of probing accuracy. (2) The learned representations are data-dependent and transferable between different tasks but deteriorate strongly if trained on random inputs. (3) The student checkpoint contains sparse subnetworks, so-called lottery tickets, and lies on the border of linear basins in the supervised loss landscape. These observations have interesting consequences for several important areas in machine learning: (1) Self-distillation can work solely based on the im",
    "path": "papers/23/02/2302.12091.json",
    "total_tokens": 838,
    "translated_title": "随机教师是好的教师",
    "translated_abstract": "本文研究了自身蒸馏中由师生学习动态引发的隐式规则化。为了隔离其影响，作者进行了一个简单的实验，使用随机初始化的教师代替了训练有素的教师。惊奇地发现，当将学生蒸馏到这样一位随机教师时，生成的模型及其表征已经拥有非常有趣的特点；(1)在探究准确性方面，与其教师相比，我们观察到蒸馏出的学生明显提高。(2)所学习的表征是数据相关的，在不同的任务之间是可传递的，但如果在随机输入上进行训练，会严重恶化。(3)学生检查点含有稀疏子网络，称为抽奖票，位于受监督的损失景观的线性盆地边缘。这些观察结果对机器学习中的几个重要领域有着有趣的影响：(1)自身蒸馏可以仅基于随机初始化的教师工作。",
    "tldr": "在自身蒸馏中，使用随机初始化的教师可以使用，可以提高模型并产生有趣的特性，但要注意表征是数据相关的。"
}