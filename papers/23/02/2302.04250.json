{
    "title": "Learning How to Infer Partial MDPs for In-Context Adaptation and Exploration. (arXiv:2302.04250v2 [cs.LG] UPDATED)",
    "abstract": "To generalize across tasks, an agent should acquire knowledge from past tasks that facilitate adaptation and exploration in future tasks. We focus on the problem of in-context adaptation and exploration, where an agent only relies on context, i.e., history of states, actions and/or rewards, rather than gradient-based updates. Posterior sampling (extension of Thompson sampling) is a promising approach, but it requires Bayesian inference and dynamic programming, which often involve unknowns (e.g., a prior) and costly computations. To address these difficulties, we use a transformer to learn an inference process from training tasks and consider a hypothesis space of partial models, represented as small Markov decision processes that are cheap for dynamic programming. In our version of the Symbolic Alchemy benchmark, our method's adaptation speed and exploration-exploitation balance approach those of an exact posterior sampling oracle. We also show that even though partial models exclude r",
    "link": "http://arxiv.org/abs/2302.04250",
    "context": "Title: Learning How to Infer Partial MDPs for In-Context Adaptation and Exploration. (arXiv:2302.04250v2 [cs.LG] UPDATED)\nAbstract: To generalize across tasks, an agent should acquire knowledge from past tasks that facilitate adaptation and exploration in future tasks. We focus on the problem of in-context adaptation and exploration, where an agent only relies on context, i.e., history of states, actions and/or rewards, rather than gradient-based updates. Posterior sampling (extension of Thompson sampling) is a promising approach, but it requires Bayesian inference and dynamic programming, which often involve unknowns (e.g., a prior) and costly computations. To address these difficulties, we use a transformer to learn an inference process from training tasks and consider a hypothesis space of partial models, represented as small Markov decision processes that are cheap for dynamic programming. In our version of the Symbolic Alchemy benchmark, our method's adaptation speed and exploration-exploitation balance approach those of an exact posterior sampling oracle. We also show that even though partial models exclude r",
    "path": "papers/23/02/2302.04250.json",
    "total_tokens": 1011,
    "translated_title": "学习如何推断部分马尔可夫决策过程进行上下文适应和探索",
    "translated_abstract": "为了在任务间进行泛化，智能体应该从过去的任务中获取知识，以促进未来任务中的适应和探索。我们关注上下文适应和探索问题，其中一个智能体只依赖于上下文，即状态、动作和/或奖励的历史记录，而不是梯度更新。后验抽样是一种有前途的方法，但它需要贝叶斯推理和动态规划，通常涉及未知量（例如，先验）和昂贵的计算。为了解决这些困难，我们使用一个变压器来从训练任务中学习推理过程，并考虑一个假设空间的部分模型，表示为小的马尔可夫决策过程，这对于动态规划来说是廉价的。在我们版本的Symbolic Alchemy基准测试中，我们的方法的适应速度和探索利用平衡接近于精确的后验抽样神谕。我们还展示了即使部分模型排除了r",
    "tldr": "本论文介绍了一种通过学习部分马尔可夫决策过程来进行上下文适应和探索的新方法，其使用变压器进行推理过程学习，考虑了模型假设空间，假设表示为小的马尔可夫决策过程，可以在性价比高的情况下进行动态规划。该方法在Symbolic Alchemy基准测试中表现出与精确后验抽样相近的适应速度和探索利用平衡。"
}