{
    "title": "Pretraining Language Models with Human Preferences. (arXiv:2302.08582v2 [cs.CL] UPDATED)",
    "abstract": "Language models (LMs) are pretrained to imitate internet text, including content that would violate human preferences if generated by an LM: falsehoods, offensive comments, personally identifiable information, low-quality or buggy code, and more. Here, we explore alternative objectives for pretraining LMs in a way that also guides them to generate text aligned with human preferences. We benchmark five objectives for pretraining with human feedback across three tasks and study how they affect the trade-off between alignment and capabilities of pretrained LMs. We find a Pareto-optimal and simple approach among those we explored: conditional training, or learning distribution over tokens conditional on their human preference scores given by a reward model. Conditional training reduces the rate of undesirable content by up to an order of magnitude, both when generating without a prompt and with an adversarially-chosen prompt. Moreover, conditional training maintains the downstream task per",
    "link": "http://arxiv.org/abs/2302.08582",
    "context": "Title: Pretraining Language Models with Human Preferences. (arXiv:2302.08582v2 [cs.CL] UPDATED)\nAbstract: Language models (LMs) are pretrained to imitate internet text, including content that would violate human preferences if generated by an LM: falsehoods, offensive comments, personally identifiable information, low-quality or buggy code, and more. Here, we explore alternative objectives for pretraining LMs in a way that also guides them to generate text aligned with human preferences. We benchmark five objectives for pretraining with human feedback across three tasks and study how they affect the trade-off between alignment and capabilities of pretrained LMs. We find a Pareto-optimal and simple approach among those we explored: conditional training, or learning distribution over tokens conditional on their human preference scores given by a reward model. Conditional training reduces the rate of undesirable content by up to an order of magnitude, both when generating without a prompt and with an adversarially-chosen prompt. Moreover, conditional training maintains the downstream task per",
    "path": "papers/23/02/2302.08582.json",
    "total_tokens": 991,
    "translated_title": "用人类偏好预训练语言模型",
    "translated_abstract": "语言模型（LMs）的预训练是为了模仿互联网文本，其中包括如果由LMs生成而违反人类偏好的内容:虚假信息，冒犯性评论，个人可识别信息，质量较低或有缺陷的代码等。在这里，我们探讨了预训练LMs的备选目标，以引导它们生成与人类偏好一致的文本。我们在三项任务中针对人类反馈对五个预训练目标进行基准测试，并研究它们如何影响预训练LMs的一致性和能力之间的权衡。我们发现在我们探索的方法中有一种帕累托最优且简单的方法：条件训练，或学习在奖励模型给出的人类偏好得分条件下的令牌分布。条件训练将不良内容的生成速率降低了一个数量级，无论是在没有提示的情况下生成还是在对抗选择的提示下生成，都如此。此外，条件训练保持了LMs的下游任务性能，表明它是预训练LMs生成与人类偏好一致的文本的可行方法。",
    "tldr": "本论文探索了用人类反馈替代传统互联网文本来预训练语言模型的方法，其中条件训练是最优和简单的方法，可将不良内容的生成速率降低一个数量级，同时保持语言模型在下游任务上的性能。"
}