{
    "title": "FedCLIP: Fast Generalization and Personalization for CLIP in Federated Learning. (arXiv:2302.13485v2 [cs.LG] UPDATED)",
    "abstract": "Federated learning (FL) has emerged as a new paradigm for privacy-preserving computation in recent years. Unfortunately, FL faces two critical challenges that hinder its actual performance: data distribution heterogeneity and high resource costs brought by large foundation models. Specifically, the non-IID data in different clients make existing FL algorithms hard to converge while the high resource costs, including computational and communication costs that increase the deployment difficulty in real-world scenarios. In this paper, we propose an effective yet simple method, named FedCLIP, to achieve fast generalization and personalization for CLIP in federated learning. Concretely, we design an attention-based adapter for the large model, CLIP, and the rest operations merely depend on adapters. Lightweight adapters can make the most use of pretrained model information and ensure models be adaptive for clients in specific tasks. Simultaneously, small-scale operations can mitigate the co",
    "link": "http://arxiv.org/abs/2302.13485",
    "context": "Title: FedCLIP: Fast Generalization and Personalization for CLIP in Federated Learning. (arXiv:2302.13485v2 [cs.LG] UPDATED)\nAbstract: Federated learning (FL) has emerged as a new paradigm for privacy-preserving computation in recent years. Unfortunately, FL faces two critical challenges that hinder its actual performance: data distribution heterogeneity and high resource costs brought by large foundation models. Specifically, the non-IID data in different clients make existing FL algorithms hard to converge while the high resource costs, including computational and communication costs that increase the deployment difficulty in real-world scenarios. In this paper, we propose an effective yet simple method, named FedCLIP, to achieve fast generalization and personalization for CLIP in federated learning. Concretely, we design an attention-based adapter for the large model, CLIP, and the rest operations merely depend on adapters. Lightweight adapters can make the most use of pretrained model information and ensure models be adaptive for clients in specific tasks. Simultaneously, small-scale operations can mitigate the co",
    "path": "papers/23/02/2302.13485.json",
    "total_tokens": 962,
    "translated_title": "FedCLIP：联邦学习中用于CLIP的快速泛化和个性化方法",
    "translated_abstract": "近年来，联邦学习（FL）作为一种保护隐私计算的新范式已经出现。然而，FL面临两个关键挑战，限制了其实际性能：数据分布异质性和大型基础模型带来的高资源成本。具体而言，不同客户端中的非独立同分布数据使得现有的FL算法难以收敛，而高资源成本（包括计算和通信成本）增加了在实际场景中的部署难度。本文提出了一种有效而简单的方法，名为FedCLIP，用于实现联邦学习中CLIP的快速泛化和个性化。具体而言，我们设计了一个基于注意力的适配器来适应大型模型CLIP，其余操作仅依赖于适配器。轻量级适配器可以充分利用预训练模型信息，并确保模型在特定任务的客户端中具有自适应性。同时，小规模操作可以缓解计算负担和通信压力，提高模型训练的效率和性能。",
    "tldr": "本文介绍了一种名为FedCLIP的方法，用于在联邦学习中实现CLIP的快速泛化和个性化。该方法通过设计基于注意力的适配器，充分利用预训练模型信息，并确保模型适应特定任务的客户端。这种方法可以提高模型训练的效率和性能。",
    "en_tdlr": "This paper proposes a method called FedCLIP for achieving fast generalization and personalization for CLIP in federated learning. By designing attention-based adapters, the method utilizes pretrained model information and ensures adaptability for specific tasks on client side, improving training efficiency and performance."
}