{
    "title": "Best Arm Identification for Stochastic Rising Bandits. (arXiv:2302.07510v2 [cs.LG] UPDATED)",
    "abstract": "Stochastic Rising Bandits (SRBs) model sequential decision-making problems in which the expected rewards of the available options increase every time they are selected. This setting captures a wide range of scenarios in which the available options are learning entities whose performance improves (in expectation) over time. While previous works addressed the regret minimization problem, this paper, focuses on the fixed-budget Best Arm Identification (BAI) problem for SRBs. In this scenario, given a fixed budget of rounds, we are asked to provide a recommendation about the best option at the end of the identification process. We propose two algorithms to tackle the above-mentioned setting, namely R-UCBE, which resorts to a UCB-like approach, and R-SR, which employs a successive reject procedure. Then, we prove that, with a sufficiently large budget, they provide guarantees on the probability of properly identifying the optimal option at the end of the learning process. Furthermore, we de",
    "link": "http://arxiv.org/abs/2302.07510",
    "context": "Title: Best Arm Identification for Stochastic Rising Bandits. (arXiv:2302.07510v2 [cs.LG] UPDATED)\nAbstract: Stochastic Rising Bandits (SRBs) model sequential decision-making problems in which the expected rewards of the available options increase every time they are selected. This setting captures a wide range of scenarios in which the available options are learning entities whose performance improves (in expectation) over time. While previous works addressed the regret minimization problem, this paper, focuses on the fixed-budget Best Arm Identification (BAI) problem for SRBs. In this scenario, given a fixed budget of rounds, we are asked to provide a recommendation about the best option at the end of the identification process. We propose two algorithms to tackle the above-mentioned setting, namely R-UCBE, which resorts to a UCB-like approach, and R-SR, which employs a successive reject procedure. Then, we prove that, with a sufficiently large budget, they provide guarantees on the probability of properly identifying the optimal option at the end of the learning process. Furthermore, we de",
    "path": "papers/23/02/2302.07510.json",
    "total_tokens": 924,
    "translated_title": "针对随机递增赌博机的最佳臂识别问题",
    "translated_abstract": "随机递增赌博机 (SRB) 模型描述了顺序决策问题，其中可选选项的预期奖励每次选择都会增加。这个设置涵盖了许多场景，其中可用选项是学习实体，其表现（期望）随时间改善。虽然以前的工作解决了遗憾最小化问题，但本文专注于 SRB 的固定预算最佳臂识别 (BAI) 问题。在这种情况下，给定一定轮数的固定预算，我们要在识别过程结束时提供关于最佳选项的推荐。我们提出了两个算法来解决上述问题，即 R-UCBE，它采用类似于 UCB 的方法，和 R-SR，它采用逐步拒绝程序。然后，我们证明了在足够大的预算下，它们可以保证能够正确识别最优选项的概率。此外，我们对 SRB 的 BAI 问题的随机复杂度进行了分析，并们证明了 R-UCBE 的上界和 R-SR 的下界。",
    "tldr": "本文提出了两种算法解决了固定预算下，针对随机递增赌博机最佳臂识别的问题，并且在足够大的预算下，这两个算法都能正确识别最优选项。",
    "en_tdlr": "This paper proposes two algorithms to solve the Best Arm Identification problem for Stochastic Rising Bandits (SRBs) with a fixed budget and proves that they can properly identify the optimal option with a sufficiently large budget."
}