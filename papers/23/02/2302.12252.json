{
    "title": "Boosting Adversarial Transferability using Dynamic Cues. (arXiv:2302.12252v2 [cs.CV] UPDATED)",
    "abstract": "The transferability of adversarial perturbations between image models has been extensively studied. In this case, an attack is generated from a known surrogate \\eg, the ImageNet trained model, and transferred to change the decision of an unknown (black-box) model trained on an image dataset. However, attacks generated from image models do not capture the dynamic nature of a moving object or a changing scene due to a lack of temporal cues within image models. This leads to reduced transferability of adversarial attacks from representation-enriched \\emph{image} models such as Supervised Vision Transformers (ViTs), Self-supervised ViTs (\\eg, DINO), and Vision-language models (\\eg, CLIP) to black-box \\emph{video} models. In this work, we induce dynamic cues within the image models without sacrificing their original performance on images. To this end, we optimize \\emph{temporal prompts} through frozen image models to capture motion dynamics. Our temporal prompts are the result of a learnabl",
    "link": "http://arxiv.org/abs/2302.12252",
    "context": "Title: Boosting Adversarial Transferability using Dynamic Cues. (arXiv:2302.12252v2 [cs.CV] UPDATED)\nAbstract: The transferability of adversarial perturbations between image models has been extensively studied. In this case, an attack is generated from a known surrogate \\eg, the ImageNet trained model, and transferred to change the decision of an unknown (black-box) model trained on an image dataset. However, attacks generated from image models do not capture the dynamic nature of a moving object or a changing scene due to a lack of temporal cues within image models. This leads to reduced transferability of adversarial attacks from representation-enriched \\emph{image} models such as Supervised Vision Transformers (ViTs), Self-supervised ViTs (\\eg, DINO), and Vision-language models (\\eg, CLIP) to black-box \\emph{video} models. In this work, we induce dynamic cues within the image models without sacrificing their original performance on images. To this end, we optimize \\emph{temporal prompts} through frozen image models to capture motion dynamics. Our temporal prompts are the result of a learnabl",
    "path": "papers/23/02/2302.12252.json",
    "total_tokens": 788,
    "translated_title": "动态提示提高对抗性转移性能",
    "translated_abstract": "对抗性攻击在图像模型之间的转移性能已经得到广泛的研究。然而，从图像模型生成的攻击不能捕捉到移动物体或变化场景的动态特征，因此对于表示丰富的图像模型（如Supervised Vision Transformers、Self-supervised ViTs和Vision-language模型）到黑盒视频模型的对抗攻击的转移性能降低。本研究为图像模型引入了动态提示，通过冻结图像模型来优化时间提示，从而捕捉动态特征，提高了对抗性攻击的转移性能。",
    "tldr": "本研究通过优化时间提示来引入动态特征，提高了图像模型到视频模型的对抗攻击的转移性能。",
    "en_tdlr": "This paper improves the transferability of adversarial attacks from image models to video models by introducing dynamic cues through optimizing temporal prompts. The proposed method captures motion dynamics without sacrificing the performance of image models."
}