{
    "title": "ERNIE-Music: Text-to-Waveform Music Generation with Diffusion Models. (arXiv:2302.04456v2 [cs.SD] UPDATED)",
    "abstract": "In recent years, the burgeoning interest in diffusion models has led to significant advances in image and speech generation. Nevertheless, the direct synthesis of music waveforms from unrestricted textual prompts remains a relatively underexplored domain. In response to this lacuna, this paper introduces a pioneering contribution in the form of a text-to-waveform music generation model, underpinned by the utilization of diffusion models. Our methodology hinges on the innovative incorporation of free-form textual prompts as conditional factors to guide the waveform generation process within the diffusion model framework. Addressing the challenge of limited text-music parallel data, we undertake the creation of a dataset by harnessing web resources, a task facilitated by weak supervision techniques. Furthermore, a rigorous empirical inquiry is undertaken to contrast the efficacy of two distinct prompt formats for text conditioning, namely, music tags and unconstrained textual description",
    "link": "http://arxiv.org/abs/2302.04456",
    "context": "Title: ERNIE-Music: Text-to-Waveform Music Generation with Diffusion Models. (arXiv:2302.04456v2 [cs.SD] UPDATED)\nAbstract: In recent years, the burgeoning interest in diffusion models has led to significant advances in image and speech generation. Nevertheless, the direct synthesis of music waveforms from unrestricted textual prompts remains a relatively underexplored domain. In response to this lacuna, this paper introduces a pioneering contribution in the form of a text-to-waveform music generation model, underpinned by the utilization of diffusion models. Our methodology hinges on the innovative incorporation of free-form textual prompts as conditional factors to guide the waveform generation process within the diffusion model framework. Addressing the challenge of limited text-music parallel data, we undertake the creation of a dataset by harnessing web resources, a task facilitated by weak supervision techniques. Furthermore, a rigorous empirical inquiry is undertaken to contrast the efficacy of two distinct prompt formats for text conditioning, namely, music tags and unconstrained textual description",
    "path": "papers/23/02/2302.04456.json",
    "total_tokens": 1003,
    "translated_title": "ERNIE-Music: 使用扩散模型的文本到波形音乐生成",
    "translated_abstract": "近年来，对扩散模型的兴趣日益增加，这导致了图像和语音生成方面的重大进展。然而，从无限制的文本提示直接合成音乐波形仍然是一个相对未被充分探索的领域。为了填补这一空白，本文介绍了一种创新性贡献，即以扩散模型为基础的文本到波形音乐生成模型。我们的方法依赖于将自由形式的文本提示作为有条件的因素，以指导扩散模型框架内的波形生成过程。为了解决有限的文本-音乐平行数据的挑战，我们通过利用网络资源来创建一个数据集，这一任务得到了弱监督技术的帮助。此外，我们进行了严格的实证调查，对比了两种不同的文本条件格式的有效性，即音乐标签和无约束的文本描述。",
    "tldr": "本文提出了ERNIE-Music，一种基于扩散模型的文本到波形音乐生成模型。通过创新地利用自由形式的文本提示作为条件因素，我们成功实现了从文本到音乐波形的生成。通过利用网络资源构建数据集并采用弱监督技术，我们解决了有限的文本-音乐平行数据的挑战。我们还对比了两种不同的文本条件格式的有效性，为该领域的研究提供了实证结果。",
    "en_tdlr": "This paper presents ERNIE-Music, a text-to-waveform music generation model based on diffusion models. By innovatively utilizing free-form textual prompts as conditional factors, the paper successfully generates music waveforms from text. The challenge of limited text-music parallel data is addressed through the creation of a dataset using web resources and weak supervision techniques. The efficacy of two different text conditioning formats, music tags and unconstrained textual description, is rigorously compared, providing empirical results for the field."
}