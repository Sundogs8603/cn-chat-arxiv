{
    "title": "Fast Attention Requires Bounded Entries. (arXiv:2302.13214v2 [cs.LG] UPDATED)",
    "abstract": "In modern machine learning, inner product attention computation is a fundamental task for training large language models such as Transformer, GPT-1, BERT, GPT-2, GPT-3 and ChatGPT. Formally, in this problem, one is given as input three matrices $Q, K, V \\in [-B,B]^{n \\times d}$, and the goal is to construct the matrix $\\mathrm{Att}(Q,K,V) := \\mathrm{diag}(A {\\bf 1}_n)^{-1} A V \\in \\mathbb{R}^{n \\times d}$, where $A = \\exp(QK^\\top/d)$ is the `attention matrix', and $\\exp$ is applied entry-wise. Straightforward methods for this problem explicitly compute the $n \\times n$ attention matrix $A$, and hence require time $\\Omega(n^2)$ even when $d = n^{o(1)}$ is small.  In this paper, we investigate whether faster algorithms are possible by implicitly making use of the matrix $A$. We present two results, showing that there is a sharp transition at $B = \\Theta(\\sqrt{\\log n})$.  $\\bullet$ If $d = O(\\log n)$ and $B = o(\\sqrt{\\log n})$, there is an $n^{1+o(1)}$ time algorithm to approximate $\\math",
    "link": "http://arxiv.org/abs/2302.13214",
    "context": "Title: Fast Attention Requires Bounded Entries. (arXiv:2302.13214v2 [cs.LG] UPDATED)\nAbstract: In modern machine learning, inner product attention computation is a fundamental task for training large language models such as Transformer, GPT-1, BERT, GPT-2, GPT-3 and ChatGPT. Formally, in this problem, one is given as input three matrices $Q, K, V \\in [-B,B]^{n \\times d}$, and the goal is to construct the matrix $\\mathrm{Att}(Q,K,V) := \\mathrm{diag}(A {\\bf 1}_n)^{-1} A V \\in \\mathbb{R}^{n \\times d}$, where $A = \\exp(QK^\\top/d)$ is the `attention matrix', and $\\exp$ is applied entry-wise. Straightforward methods for this problem explicitly compute the $n \\times n$ attention matrix $A$, and hence require time $\\Omega(n^2)$ even when $d = n^{o(1)}$ is small.  In this paper, we investigate whether faster algorithms are possible by implicitly making use of the matrix $A$. We present two results, showing that there is a sharp transition at $B = \\Theta(\\sqrt{\\log n})$.  $\\bullet$ If $d = O(\\log n)$ and $B = o(\\sqrt{\\log n})$, there is an $n^{1+o(1)}$ time algorithm to approximate $\\math",
    "path": "papers/23/02/2302.13214.json",
    "total_tokens": 984,
    "translated_title": "快速注意力需要有界条目",
    "translated_abstract": "在现代机器学习中，内积关注计算是训练大型语言模型（如Transformer，GPT-1，BERT，GPT-2，GPT-3和ChatGPT）的基本任务。形式上，在这个问题中，输入三个矩阵$Q，K，V \\in [-B，B]^{n \\times d}$，目标是构造矩阵$\\mathrm{Att}(Q,K,V) := \\mathrm{diag}(A {\\bf 1}_n)^{-1} A V \\in \\mathbb{R}^{n \\times d}$，其中 $A = \\exp(QK^\\top/d)$ 是“注意力矩阵”，$\\exp$是分量应用。本文研究是否可以通过隐式利用矩阵 $A$ 来实现更快的算法。我们提出了两个结果，证明了在 $B = \\Theta(\\sqrt{\\log n})$处存在一个尖锐的转换。$\\bullet$ 如果 $d = O(\\log n)$，$B = o(\\sqrt{\\log n})$，则存在一个$n^{1+o(1)}$时间算法来近似$\\mathbb{R}^{n \\times d}$中的 $\\mathrm{Att}(Q,K,V)$。",
    "tldr": "本文研究了内积关注计算的快速算法问题，提出了两个结果，证明了在B = O(sqrt(log n))时存在一种n ^（1 + O（1））时间算法。"
}