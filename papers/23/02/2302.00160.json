{
    "title": "Local transfer learning from one data space to another. (arXiv:2302.00160v2 [cs.LG] UPDATED)",
    "abstract": "A fundamental problem in manifold learning is to approximate a functional relationship in a data chosen randomly from a probability distribution supported on a low dimensional sub-manifold of a high dimensional ambient Euclidean space. The manifold is essentially defined by the data set itself and, typically, designed so that the data is dense on the manifold in some sense. The notion of a data space is an abstraction of a manifold encapsulating the essential properties that allow for function approximation. The problem of transfer learning (meta-learning) is to use the learning of a function on one data set to learn a similar function on a new data set. In terms of function approximation, this means lifting a function on one data space (the base data space) to another (the target data space). This viewpoint enables us to connect some inverse problems in applied mathematics (such as inverse Radon transform) with transfer learning. In this paper we examine the question of such lifting w",
    "link": "http://arxiv.org/abs/2302.00160",
    "context": "Title: Local transfer learning from one data space to another. (arXiv:2302.00160v2 [cs.LG] UPDATED)\nAbstract: A fundamental problem in manifold learning is to approximate a functional relationship in a data chosen randomly from a probability distribution supported on a low dimensional sub-manifold of a high dimensional ambient Euclidean space. The manifold is essentially defined by the data set itself and, typically, designed so that the data is dense on the manifold in some sense. The notion of a data space is an abstraction of a manifold encapsulating the essential properties that allow for function approximation. The problem of transfer learning (meta-learning) is to use the learning of a function on one data set to learn a similar function on a new data set. In terms of function approximation, this means lifting a function on one data space (the base data space) to another (the target data space). This viewpoint enables us to connect some inverse problems in applied mathematics (such as inverse Radon transform) with transfer learning. In this paper we examine the question of such lifting w",
    "path": "papers/23/02/2302.00160.json",
    "total_tokens": 774,
    "translated_title": "从一个数据空间到另一个数据空间的本地迁移学习",
    "translated_abstract": "流形学习中的一个基本问题是在从支持在高维欧几里得空间中的低维子流形上随机选择的数据上近似一个函数关系。流形本质上由数据集本身定义，并且通常设计为数据在某种意义上在流形上稠密。数据空间的概念是一个抽象的流形，封装了允许进行函数逼近的基本属性。迁移学习（元学习）问题是利用在一个数据集上学习一个函数来学习在另一个数据集上的类似函数。在函数逼近方面，这意味着将一个数据空间上的函数（基本数据空间）提升到另一个数据空间（目标数据空间）。这个观点使我们能够将应用数学中的一些逆问题（如逆Radon变换）与迁移学习联系起来。本文探讨了这种提升问题。",
    "tldr": ""
}