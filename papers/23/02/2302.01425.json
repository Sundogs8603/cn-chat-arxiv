{
    "title": "Fast, Differentiable and Sparse Top-k: a Convex Analysis Perspective. (arXiv:2302.01425v3 [cs.LG] UPDATED)",
    "abstract": "The top-k operator returns a sparse vector, where the non-zero values correspond to the k largest values of the input. Unfortunately, because it is a discontinuous function, it is difficult to incorporate in neural networks trained end-to-end with backpropagation. Recent works have considered differentiable relaxations, based either on regularization or perturbation techniques. However, to date, no approach is fully differentiable and sparse. In this paper, we propose new differentiable and sparse top-k operators. We view the top-k operator as a linear program over the permutahedron, the convex hull of permutations. We then introduce a p-norm regularization term to smooth out the operator, and show that its computation can be reduced to isotonic optimization. Our framework is significantly more general than the existing one and allows for example to express top-k operators that select values in magnitude. On the algorithmic side, in addition to pool adjacent violator (PAV) algorithms, ",
    "link": "http://arxiv.org/abs/2302.01425",
    "context": "Title: Fast, Differentiable and Sparse Top-k: a Convex Analysis Perspective. (arXiv:2302.01425v3 [cs.LG] UPDATED)\nAbstract: The top-k operator returns a sparse vector, where the non-zero values correspond to the k largest values of the input. Unfortunately, because it is a discontinuous function, it is difficult to incorporate in neural networks trained end-to-end with backpropagation. Recent works have considered differentiable relaxations, based either on regularization or perturbation techniques. However, to date, no approach is fully differentiable and sparse. In this paper, we propose new differentiable and sparse top-k operators. We view the top-k operator as a linear program over the permutahedron, the convex hull of permutations. We then introduce a p-norm regularization term to smooth out the operator, and show that its computation can be reduced to isotonic optimization. Our framework is significantly more general than the existing one and allows for example to express top-k operators that select values in magnitude. On the algorithmic side, in addition to pool adjacent violator (PAV) algorithms, ",
    "path": "papers/23/02/2302.01425.json",
    "total_tokens": 983,
    "translated_title": "快速，可微分和稀疏的Top-k: 凸分析视角",
    "translated_abstract": "Top-k运算符返回一个稀疏向量，其中非零值对应于输入k个最大值。然而，由于它是一个不连续的函数，所以很难将其纳入使用反向传播进行端到端训练的神经网络中。本文提出新的可微分和稀疏的Top-k运算符。我们将Top-k运算符视为排列凸包上的线性规划，并引入p-范数正则化项以平滑运算符，证明其计算可以减少到同向优化。我们的算法框架显著比现有框架更通用，可以表示选择大小值的Top-k运算符。在算法方面，除了池相邻违规算法（PAV）外，我们提出了一种新的近端算子，允许有效的Top-k运算，并可轻松集成到神经网络中。在图像分类和序列标记等多个任务上的实验验证了我们方法的有效性。",
    "tldr": "本研究提出了一种新的可微分和稀疏的Top-k运算符，将其视为排列凸包上的线性规划，并引入p-范数正则化项以平滑运算符。算法方面，提出了一种新的近端算子，可以有效地进行Top-k运算。实验结果表明该方法在多个任务中都很有效。",
    "en_tdlr": "This paper proposes a new differentiable and sparse top-k operator by viewing it as a linear program over the permutahedron and introducing a p-norm regularization term. A novel proximal operator is also proposed that allows efficient top-k operations, which can be easily integrated into neural networks, and experiments show its effectiveness in multiple tasks."
}