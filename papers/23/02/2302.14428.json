{
    "title": "Stochastic Gradient Descent under Markovian Sampling Schemes. (arXiv:2302.14428v2 [math.OC] UPDATED)",
    "abstract": "We study a variation of vanilla stochastic gradient descent where the optimizer only has access to a Markovian sampling scheme. These schemes encompass applications that range from decentralized optimization with a random walker (token algorithms), to RL and online system identification problems. We focus on obtaining rates of convergence under the least restrictive assumptions possible on the underlying Markov chain and on the functions optimized. We first unveil the theoretical lower bound for methods that sample stochastic gradients along the path of a Markov chain, making appear a dependency in the hitting time of the underlying Markov chain. We then study Markov chain SGD (MC-SGD) under much milder regularity assumptions than prior works. We finally introduce MC-SAG, an alternative to MC-SGD with variance reduction, that only depends on the hitting time of the Markov chain, therefore obtaining a communication-efficient token algorithm.",
    "link": "http://arxiv.org/abs/2302.14428",
    "context": "Title: Stochastic Gradient Descent under Markovian Sampling Schemes. (arXiv:2302.14428v2 [math.OC] UPDATED)\nAbstract: We study a variation of vanilla stochastic gradient descent where the optimizer only has access to a Markovian sampling scheme. These schemes encompass applications that range from decentralized optimization with a random walker (token algorithms), to RL and online system identification problems. We focus on obtaining rates of convergence under the least restrictive assumptions possible on the underlying Markov chain and on the functions optimized. We first unveil the theoretical lower bound for methods that sample stochastic gradients along the path of a Markov chain, making appear a dependency in the hitting time of the underlying Markov chain. We then study Markov chain SGD (MC-SGD) under much milder regularity assumptions than prior works. We finally introduce MC-SAG, an alternative to MC-SGD with variance reduction, that only depends on the hitting time of the Markov chain, therefore obtaining a communication-efficient token algorithm.",
    "path": "papers/23/02/2302.14428.json",
    "total_tokens": 881,
    "translated_title": "基于马尔科夫抽样方案的随机梯度下降算法研究",
    "translated_abstract": "本文研究了一种变形的随机梯度下降算法，其中优化器只能访问马尔科夫抽样方案。这些方案涵盖从具有随机行走者（token算法）的分散优化到RL和在线系统识别问题的应用。我们专注于在对基础马尔科夫链和优化函数施加最不限制性的假设的情况下获得收敛速度。我们首先揭示了样本随机梯度沿着马尔可夫链路径抽样的方法的理论下界，使出现了对基础马尔可夫链的命中时间的依赖性。然后，我们研究了比之前作品更温和的规律性假设下的Markov链SGD（MC-SGD）。最后，我们介绍了MC-SAG，这是MC-SGD的一种带有方差缩减的替代方案，仅取决于马尔可夫链的碰撞时间，因此获得了通信效率高的token 算法。",
    "tldr": "本文研究了基于马尔科夫抽样方案的随机梯度下降算法，提出了MC-SAG算法实现了用于分布式算法的通信效率高的token 算法。",
    "en_tdlr": "This paper studied stochastic gradient descent algorithms based on Markovian sampling schemes, and proposed MC-SAG algorithm for efficient communication in decentralized optimization problems."
}