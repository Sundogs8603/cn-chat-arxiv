{
    "title": "Stochastic Approximation Beyond Gradient for Signal Processing and Machine Learning. (arXiv:2302.11147v2 [math.OC] UPDATED)",
    "abstract": "Stochastic Approximation (SA) is a classical algorithm that has had since the early days a huge impact on signal processing, and nowadays on machine learning, due to the necessity to deal with a large amount of data observed with uncertainties. An exemplar special case of SA pertains to the popular stochastic (sub)gradient algorithm which is the working horse behind many important applications. A lesser-known fact is that the SA scheme also extends to non-stochastic-gradient algorithms such as compressed stochastic gradient, stochastic expectation-maximization, and a number of reinforcement learning algorithms. The aim of this article is to overview and introduce the non-stochastic-gradient perspectives of SA to the signal processing and machine learning audiences through presenting a design guideline of SA algorithms backed by theories. Our central theme is to propose a general framework that unifies existing theories of SA, including its non-asymptotic and asymptotic convergence resu",
    "link": "http://arxiv.org/abs/2302.11147",
    "context": "Title: Stochastic Approximation Beyond Gradient for Signal Processing and Machine Learning. (arXiv:2302.11147v2 [math.OC] UPDATED)\nAbstract: Stochastic Approximation (SA) is a classical algorithm that has had since the early days a huge impact on signal processing, and nowadays on machine learning, due to the necessity to deal with a large amount of data observed with uncertainties. An exemplar special case of SA pertains to the popular stochastic (sub)gradient algorithm which is the working horse behind many important applications. A lesser-known fact is that the SA scheme also extends to non-stochastic-gradient algorithms such as compressed stochastic gradient, stochastic expectation-maximization, and a number of reinforcement learning algorithms. The aim of this article is to overview and introduce the non-stochastic-gradient perspectives of SA to the signal processing and machine learning audiences through presenting a design guideline of SA algorithms backed by theories. Our central theme is to propose a general framework that unifies existing theories of SA, including its non-asymptotic and asymptotic convergence resu",
    "path": "papers/23/02/2302.11147.json",
    "total_tokens": 868,
    "translated_title": "信号处理和机器学习中超越梯度的随机逼近",
    "translated_abstract": "随机逼近（SA）是一个经典的算法，在信号处理方面从早期就产生了巨大的影响，现在在机器学习中也因处理大量带有不确定性的数据而变得重要。一个典型的SA特例是流行的随机（子）梯度算法，它是许多重要应用的关键。一个较少人知道的事实是，SA方案也适用于非随机梯度算法，如压缩随机梯度、随机期望最大化和一些强化学习算法。本文的目的是通过提供支持理论的SA算法设计指南，概述和介绍SA的非随机梯度视角，以便吸引信号处理和机器学习研究者的注意。我们的核心主题是提出一个统一现有SA理论的通用框架，包括其非渐近和渐近收敛结果。",
    "tldr": "本文介绍了随机逼近算法在信号处理和机器学习中的非随机梯度视角，提出了一个通用框架来统一现有的SA理论，包括非渐近和渐近收敛结果。",
    "en_tdlr": "This paper presents the non-stochastic gradient perspective of Stochastic Approximation (SA) in signal processing and machine learning, proposing a general framework to unify existing theories of SA, including non-asymptotic and asymptotic convergence results."
}