{
    "title": "Universal Soldier: Using Universal Adversarial Perturbations for Detecting Backdoor Attacks. (arXiv:2302.00747v3 [cs.LG] UPDATED)",
    "abstract": "Deep learning models achieve excellent performance in numerous machine learning tasks. Yet, they suffer from security-related issues such as adversarial examples and poisoning (backdoor) attacks. A deep learning model may be poisoned by training with backdoored data or by modifying inner network parameters. Then, a backdoored model performs as expected when receiving a clean input, but it misclassifies when receiving a backdoored input stamped with a pre-designed pattern called \"trigger\". Unfortunately, it is difficult to distinguish between clean and backdoored models without prior knowledge of the trigger. This paper proposes a backdoor detection method by utilizing a special type of adversarial attack, universal adversarial perturbation (UAP), and its similarities with a backdoor trigger. We observe an intuitive phenomenon: UAPs generated from backdoored models need fewer perturbations to mislead the model than UAPs from clean models. UAPs of backdoored models tend to exploit the sh",
    "link": "http://arxiv.org/abs/2302.00747",
    "context": "Title: Universal Soldier: Using Universal Adversarial Perturbations for Detecting Backdoor Attacks. (arXiv:2302.00747v3 [cs.LG] UPDATED)\nAbstract: Deep learning models achieve excellent performance in numerous machine learning tasks. Yet, they suffer from security-related issues such as adversarial examples and poisoning (backdoor) attacks. A deep learning model may be poisoned by training with backdoored data or by modifying inner network parameters. Then, a backdoored model performs as expected when receiving a clean input, but it misclassifies when receiving a backdoored input stamped with a pre-designed pattern called \"trigger\". Unfortunately, it is difficult to distinguish between clean and backdoored models without prior knowledge of the trigger. This paper proposes a backdoor detection method by utilizing a special type of adversarial attack, universal adversarial perturbation (UAP), and its similarities with a backdoor trigger. We observe an intuitive phenomenon: UAPs generated from backdoored models need fewer perturbations to mislead the model than UAPs from clean models. UAPs of backdoored models tend to exploit the sh",
    "path": "papers/23/02/2302.00747.json",
    "total_tokens": 976,
    "translated_title": "通用士兵：利用通用对抗扰动来检测后门攻击",
    "translated_abstract": "深度学习模型在许多机器学习任务中表现出色，但是它们面临着诸如对抗示例和中毒（后门）攻击等与安全相关的问题。深度学习模型可能通过使用带有后门数据的训练或修改内部网络参数来中毒。然后，当接收到干净输入时，后门模型表现如预期，但是当接收到带有预先设计的\"触发器\"的后门输入时，它会错误分类。不幸的是，如果没有事先了解触发器，很难区分干净模型和后门模型。本文提出了一种后门检测方法，利用一种特殊类型的对抗攻击——通用对抗扰动（UAP）及其与后门触发器的相似之处。我们观察到一个直观的现象：从后门模型生成的UAP比从干净模型生成的UAP需要更少的扰动来引导模型误导。后门模型的UAP倾向于利用网络漏洞。",
    "tldr": "本论文提出了一种利用通用对抗扰动（UAP）检测后门攻击的方法。通过观察UAPs生成的方式，我们发现后门模型只需要较少的扰动即可欺骗模型，而干净模型需要更多扰动。这一发现可以用来区分干净模型和后门模型。"
}