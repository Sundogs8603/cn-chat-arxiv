{
    "title": "Achieving acceleration despite very noisy gradients. (arXiv:2302.05515v2 [stat.ML] UPDATED)",
    "abstract": "We present a generalization of Nesterov's accelerated gradient descent algorithm. Our algorithm (AGNES) provably achieves acceleration for smooth convex minimization tasks with noisy gradient estimates if the noise intensity is proportional to the magnitude of the gradient. Nesterov's accelerated gradient descent does not converge under this noise model if the constant of proportionality exceeds one. AGNES fixes this deficiency and provably achieves an accelerated convergence rate no matter how small the signal to noise ratio in the gradient estimate. Empirically, we demonstrate that this is an appropriate model for mini-batch gradients in overparameterized deep learning. Finally, we show that AGNES outperforms stochastic gradient descent with momentum and Nesterov's method in the training of CNNs.",
    "link": "http://arxiv.org/abs/2302.05515",
    "context": "Title: Achieving acceleration despite very noisy gradients. (arXiv:2302.05515v2 [stat.ML] UPDATED)\nAbstract: We present a generalization of Nesterov's accelerated gradient descent algorithm. Our algorithm (AGNES) provably achieves acceleration for smooth convex minimization tasks with noisy gradient estimates if the noise intensity is proportional to the magnitude of the gradient. Nesterov's accelerated gradient descent does not converge under this noise model if the constant of proportionality exceeds one. AGNES fixes this deficiency and provably achieves an accelerated convergence rate no matter how small the signal to noise ratio in the gradient estimate. Empirically, we demonstrate that this is an appropriate model for mini-batch gradients in overparameterized deep learning. Finally, we show that AGNES outperforms stochastic gradient descent with momentum and Nesterov's method in the training of CNNs.",
    "path": "papers/23/02/2302.05515.json",
    "total_tokens": 804,
    "translated_title": "实现加速尽管梯度非常嘈杂。",
    "translated_abstract": "我们提出了Nesterov加速梯度下降算法的一般化。如果噪声的强度与梯度的大小成比例，我们的算法（AGNES）可以证明在具有嘈杂梯度估计的平滑凸优化任务中实现加速。如果常数比例超过一，Nesterov加速梯度下降在这种噪声模型下不会收敛。AGNES能修复这种不足，并且可以证明它的收敛速度加快，无论梯度估计的信噪比有多小。实验证明，这是用于超参数过多的深度学习小批量梯度的适当模型。最后，我们证明AGNES在CNN训练中的性能优于动量随机梯度下降和Nesterov的方法。",
    "tldr": "AGNES是一种能在平滑凸优化任务中实现加速的算法，即使梯度估计的信噪比很小，它也能表现出优异的性能，在深度学习中的应用效果显著优于动量随机梯度下降和Nesterov方法。",
    "en_tdlr": "AGNES is an algorithm that achieves acceleration for smooth convex minimization tasks with noisy gradient estimates and outperforms stochastic gradient descent with momentum and Nesterov's method in training CNNs."
}