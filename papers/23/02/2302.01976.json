{
    "title": "SPARLING: Learning Latent Representations with Extremely Sparse Activations. (arXiv:2302.01976v2 [cs.LG] UPDATED)",
    "abstract": "Real-world processes often contain intermediate state that can be modeled as an extremely sparse tensor. We introduce Sparling, a technique that allows you to learn models with intermediate layers that match this state from only end-to-end labeled examples (i.e., no supervision on the intermediate state). Sparling uses a new kind of informational bottleneck that enforces levels of activation sparsity unachievable using other techniques. We find that extreme sparsity is necessary to achieve good intermediate state modeling. On our synthetic DigitCircle domain as well as the LaTeX-OCR and Audio-MNIST-Sequence domains, we are able to precisely localize the intermediate states up to feature permutation with > 90% accuracy, even though we only train end-to-end.",
    "link": "http://arxiv.org/abs/2302.01976",
    "context": "Title: SPARLING: Learning Latent Representations with Extremely Sparse Activations. (arXiv:2302.01976v2 [cs.LG] UPDATED)\nAbstract: Real-world processes often contain intermediate state that can be modeled as an extremely sparse tensor. We introduce Sparling, a technique that allows you to learn models with intermediate layers that match this state from only end-to-end labeled examples (i.e., no supervision on the intermediate state). Sparling uses a new kind of informational bottleneck that enforces levels of activation sparsity unachievable using other techniques. We find that extreme sparsity is necessary to achieve good intermediate state modeling. On our synthetic DigitCircle domain as well as the LaTeX-OCR and Audio-MNIST-Sequence domains, we are able to precisely localize the intermediate states up to feature permutation with > 90% accuracy, even though we only train end-to-end.",
    "path": "papers/23/02/2302.01976.json",
    "total_tokens": 817,
    "translated_title": "SPARLING：使用极度稀疏激活进行学习潜在表示",
    "translated_abstract": "现实世界的过程常常包含可以被建模为极度稀疏张量的中间状态。我们引入了Sparling，一种允许您从仅具有端到端标记示例（即无中间状态的监督）中学习与该状态相匹配的模型的技术。Sparling使用一种新型的信息瓶颈，通过强制激活稀疏程度来实现其他技术无法达到的水平。我们发现，极度稀疏性是实现良好的中间状态建模所必需的。在我们的合成DigitCircle领域以及LaTeX-OCR和Audio-MNIST-Sequence领域中，即使我们仅进行端到端训练，我们也能以超过90％的准确性精确定位中间状态，即使存在特征置换的情况。",
    "tldr": "本论文介绍了一种名为Sparling的技术，通过使用极度稀疏激活，在没有中间状态监督的情况下，从端到端标记示例中学习模型。该技术利用了一种新型的信息瓶颈来实现极度稀疏激活，达到了良好的中间状态建模精度，并且在不同领域的实验中取得了较高的准确性。"
}