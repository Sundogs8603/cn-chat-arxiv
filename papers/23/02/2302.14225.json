{
    "title": "Weighted Sampling for Masked Language Modeling. (arXiv:2302.14225v2 [cs.CL] UPDATED)",
    "abstract": "Masked Language Modeling (MLM) is widely used to pretrain language models. The standard random masking strategy in MLM causes the pre-trained language models (PLMs) to be biased toward high-frequency tokens. Representation learning of rare tokens is poor and PLMs have limited performance on downstream tasks. To alleviate this frequency bias issue, we propose two simple and effective Weighted Sampling strategies for masking tokens based on the token frequency and training loss. We apply these two strategies to BERT and obtain Weighted-Sampled BERT (WSBERT). Experiments on the Semantic Textual Similarity benchmark (STS) show that WSBERT significantly improves sentence embeddings over BERT. Combining WSBERT with calibration methods and prompt learning further improves sentence embeddings. We also investigate fine-tuning WSBERT on the GLUE benchmark and show that Weighted Sampling also improves the transfer learning capability of the backbone PLM. We further analyze and provide insights in",
    "link": "http://arxiv.org/abs/2302.14225",
    "context": "Title: Weighted Sampling for Masked Language Modeling. (arXiv:2302.14225v2 [cs.CL] UPDATED)\nAbstract: Masked Language Modeling (MLM) is widely used to pretrain language models. The standard random masking strategy in MLM causes the pre-trained language models (PLMs) to be biased toward high-frequency tokens. Representation learning of rare tokens is poor and PLMs have limited performance on downstream tasks. To alleviate this frequency bias issue, we propose two simple and effective Weighted Sampling strategies for masking tokens based on the token frequency and training loss. We apply these two strategies to BERT and obtain Weighted-Sampled BERT (WSBERT). Experiments on the Semantic Textual Similarity benchmark (STS) show that WSBERT significantly improves sentence embeddings over BERT. Combining WSBERT with calibration methods and prompt learning further improves sentence embeddings. We also investigate fine-tuning WSBERT on the GLUE benchmark and show that Weighted Sampling also improves the transfer learning capability of the backbone PLM. We further analyze and provide insights in",
    "path": "papers/23/02/2302.14225.json",
    "total_tokens": 906,
    "translated_title": "基于加权采样的遮蔽语言模型。",
    "translated_abstract": "遮蔽语言建模（MLM）被广泛用于预训练语言模型。MLM中标准的随机遮蔽策略会导致预训练的语言模型（PLMs）偏向于高频率的词汇。罕见词汇的表示学习效果较差，PLMs在下游任务中的性能有限。为了解决这个频率偏差问题，我们提出了两种简单而有效的基于词频和训练损失的加权遮蔽策略。我们将这两种策略应用于BERT，并获得了加权采样BERT（WSBERT）。在语义文本相似性基准（STS）上的实验表明，WSBERT显着改进了BERT的句子嵌入。将WSBERT与校准方法和提示学习相结合，进一步改善了句子嵌入。我们还调查了在GLUE基准上微调WSBERT，并表明加权采样也改善了骨干PLM的迁移学习能力。我们进一步分析并提供了见解。",
    "tldr": "本文提出了两种有效的加权采样策略用于遮蔽语言建模，可以有效解决高频率词汇导致的偏差问题，提高了语言模型的性能和迁移学习能力。",
    "en_tdlr": "This paper proposes two effective weighted sampling strategies for masked language modeling, which can effectively solve the bias problem caused by high-frequency tokens, and improve the performance and transfer learning capability of language models."
}