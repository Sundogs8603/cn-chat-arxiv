{
    "title": "Generalization Error Bounds for Noisy, Iterative Algorithms via Maximal Leakage. (arXiv:2302.14518v2 [cs.LG] UPDATED)",
    "abstract": "We adopt an information-theoretic framework to analyze the generalization behavior of the class of iterative, noisy learning algorithms. This class is particularly suitable for study under information-theoretic metrics as the algorithms are inherently randomized, and it includes commonly used algorithms such as Stochastic Gradient Langevin Dynamics (SGLD). Herein, we use the maximal leakage (equivalently, the Sibson mutual information of order infinity) metric, as it is simple to analyze, and it implies both bounds on the probability of having a large generalization error and on its expected value. We show that, if the update function (e.g., gradient) is bounded in $L_2$-norm and the additive noise is isotropic Gaussian noise, then one can obtain an upper-bound on maximal leakage in semi-closed form. Furthermore, we demonstrate how the assumptions on the update function affect the optimal (in the sense of minimizing the induced maximal leakage) choice of the noise. Finally, we compute ",
    "link": "http://arxiv.org/abs/2302.14518",
    "context": "Title: Generalization Error Bounds for Noisy, Iterative Algorithms via Maximal Leakage. (arXiv:2302.14518v2 [cs.LG] UPDATED)\nAbstract: We adopt an information-theoretic framework to analyze the generalization behavior of the class of iterative, noisy learning algorithms. This class is particularly suitable for study under information-theoretic metrics as the algorithms are inherently randomized, and it includes commonly used algorithms such as Stochastic Gradient Langevin Dynamics (SGLD). Herein, we use the maximal leakage (equivalently, the Sibson mutual information of order infinity) metric, as it is simple to analyze, and it implies both bounds on the probability of having a large generalization error and on its expected value. We show that, if the update function (e.g., gradient) is bounded in $L_2$-norm and the additive noise is isotropic Gaussian noise, then one can obtain an upper-bound on maximal leakage in semi-closed form. Furthermore, we demonstrate how the assumptions on the update function affect the optimal (in the sense of minimizing the induced maximal leakage) choice of the noise. Finally, we compute ",
    "path": "papers/23/02/2302.14518.json",
    "total_tokens": 971,
    "translated_title": "通过最大泄露分析噪声迭代算法的泛化误差界限",
    "translated_abstract": "我们采用信息论框架来分析一类迭代式、带有噪声的学习算法的泛化行为。由于这类算法具有随机性，并且包含常用的算法（如随机梯度 Langevin 动力学），所以在信息论度量下研究它们尤为合适。在本文中，我们使用最大泄露（等价于无穷阶 Sibson 互信息）度量，因其易于分析且可以同时获得泛化误差大概率上界和期望值上界。我们证明了如果更新函数（如梯度）在L2-范数下有界，且加性噪声为各向同性高斯噪声，则可以得到一个半封闭形式下的最大泄露上界。另外，我们还展示了更新函数的假设如何影响噪声的最优选择（即最小化产生的最大泄露）。最后，我们计算了...",
    "tldr": "通过最大泄露分析噪声迭代算法的泛化误差界限，证明了如果更新函数在L2-范数下有界且加性噪声为各向同性高斯噪声，则可以得到一个半封闭形式下的最大泄露上界，同时展示了更新函数的假设如何影响噪声的最优选择。",
    "en_tdlr": "This paper analyzes the generalization error bounds for noisy, iterative learning algorithms using the framework of maximal leakage. It demonstrates that if the update function is bounded in the L2-norm and the additive noise is isotropic Gaussian, an upper-bound on maximal leakage can be obtained. Moreover, it explores how the assumptions on the update function impact the optimal choice of noise."
}