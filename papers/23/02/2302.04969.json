{
    "title": "Communication-Efficient Federated Hypergradient Computation via Aggregated Iterative Differentiation. (arXiv:2302.04969v3 [cs.LG] UPDATED)",
    "abstract": "Federated bilevel optimization has attracted increasing attention due to emerging machine learning and communication applications. The biggest challenge lies in computing the gradient of the upper-level objective function (i.e., hypergradient) in the federated setting due to the nonlinear and distributed construction of a series of global Hessian matrices. In this paper, we propose a novel communication-efficient federated hypergradient estimator via aggregated iterative differentiation (AggITD). AggITD is simple to implement and significantly reduces the communication cost by conducting the federated hypergradient estimation and the lower-level optimization simultaneously. We show that the proposed AggITD-based algorithm achieves the same sample complexity as existing approximate implicit differentiation (AID)-based approaches with much fewer communication rounds in the presence of data heterogeneity. Our results also shed light on the great advantage of ITD over AID in the federated/",
    "link": "http://arxiv.org/abs/2302.04969",
    "context": "Title: Communication-Efficient Federated Hypergradient Computation via Aggregated Iterative Differentiation. (arXiv:2302.04969v3 [cs.LG] UPDATED)\nAbstract: Federated bilevel optimization has attracted increasing attention due to emerging machine learning and communication applications. The biggest challenge lies in computing the gradient of the upper-level objective function (i.e., hypergradient) in the federated setting due to the nonlinear and distributed construction of a series of global Hessian matrices. In this paper, we propose a novel communication-efficient federated hypergradient estimator via aggregated iterative differentiation (AggITD). AggITD is simple to implement and significantly reduces the communication cost by conducting the federated hypergradient estimation and the lower-level optimization simultaneously. We show that the proposed AggITD-based algorithm achieves the same sample complexity as existing approximate implicit differentiation (AID)-based approaches with much fewer communication rounds in the presence of data heterogeneity. Our results also shed light on the great advantage of ITD over AID in the federated/",
    "path": "papers/23/02/2302.04969.json",
    "total_tokens": 960,
    "translated_abstract": "随着机器学习和通信应用的出现，联邦双层优化引起了越来越多的关注。最大的挑战在于在联邦设置中计算上层目标函数的梯度（即超梯度），因为它需要非线性和分布构造一系列全局黑塞矩阵。本文提出了一种基于聚合迭代微分（AggITD）的新型通信高效的联邦超梯度估计器。AggITD易于实现，并通过同时进行联邦超梯度估计和下层优化显著降低通信成本。我们证明，所提出的基于AggITD的算法在数据异构性存在的情况下具有与现有近似隐式微分（AID）方法相同的样本复杂度，并且通信轮次明显较少。我们的结果还揭示了ITD在联邦/非联邦下备受欢迎的优势。",
    "tldr": "本文提出了一种基于聚合迭代微分（AggITD）的新型通信高效的联邦超梯度估计器，可以同时进行联邦超梯度估计和下层优化，从而显著降低通信成本，并具有与现有方法相同的样本复杂度，揭示了ITD在联邦/非联邦下的优势。",
    "en_tdlr": "This paper proposes a novel communication-efficient federated hypergradient estimator via aggregated iterative differentiation (AggITD) that significantly reduces communication costs by conducting both the federated hypergradient estimation and the lower-level optimization simultaneously. The proposed algorithm achieves the same sample complexity as existing methods with much fewer communication rounds and highlights the advantages of ITD in both federated and non-federated settings."
}