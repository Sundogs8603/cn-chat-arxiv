{
    "title": "Stochastic Gradient Descent-Induced Drift of Representation in a Two-Layer Neural Network. (arXiv:2302.02563v2 [cond-mat.dis-nn] UPDATED)",
    "abstract": "Representational drift refers to over-time changes in neural activation accompanied by a stable task performance. Despite being observed in the brain and in artificial networks, the mechanisms of drift and its implications are not fully understood. Motivated by recent experimental findings of stimulus-dependent drift in the piriform cortex, we use theory and simulations to study this phenomenon in a two-layer linear feedforward network. Specifically, in a continual online learning scenario, we study the drift induced by the noise inherent in the Stochastic Gradient Descent (SGD). By decomposing the learning dynamics into the normal and tangent spaces of the minimum-loss manifold, we show the former corresponds to a finite variance fluctuation, while the latter could be considered as an effective diffusion process on the manifold. We analytically compute the fluctuation and the diffusion coefficients for the stimuli representations in the hidden layer as functions of network parameters ",
    "link": "http://arxiv.org/abs/2302.02563",
    "context": "Title: Stochastic Gradient Descent-Induced Drift of Representation in a Two-Layer Neural Network. (arXiv:2302.02563v2 [cond-mat.dis-nn] UPDATED)\nAbstract: Representational drift refers to over-time changes in neural activation accompanied by a stable task performance. Despite being observed in the brain and in artificial networks, the mechanisms of drift and its implications are not fully understood. Motivated by recent experimental findings of stimulus-dependent drift in the piriform cortex, we use theory and simulations to study this phenomenon in a two-layer linear feedforward network. Specifically, in a continual online learning scenario, we study the drift induced by the noise inherent in the Stochastic Gradient Descent (SGD). By decomposing the learning dynamics into the normal and tangent spaces of the minimum-loss manifold, we show the former corresponds to a finite variance fluctuation, while the latter could be considered as an effective diffusion process on the manifold. We analytically compute the fluctuation and the diffusion coefficients for the stimuli representations in the hidden layer as functions of network parameters ",
    "path": "papers/23/02/2302.02563.json",
    "total_tokens": 911,
    "translated_title": "一种两层神经网络中随机梯度下降引起的表示漂移问题",
    "translated_abstract": "表示漂移是指神经元激活随时间变化而任务表现保持稳定。尽管在大脑和人工网络中观察到，但漂移的机制及其影响仍不完全了解。受最近在海马皮质中发现的刺激依赖性漂移的实验结果的启发，我们使用理论和模拟研究了一个两层线性前馈网络中的这种现象。具体而言，在连续在线学习场景中，我们研究了随机梯度下降（SGD）固有噪声引起的漂移。通过将学习动态分解为最小损失流形的正切空间和切空间，我们发现前者对应于有限方差波动，而后者可以被视为流形上的有效扩散过程。我们分析地计算了隐藏层刺激表示的波动和扩散系数作为网络参数的函数。",
    "tldr": "本文研究了一个两层神经网络中因随机梯度下降而引起的表示漂移问题，发现其前者对应于有限方差波动，而后者可以被视为流形上的有效扩散过程。",
    "en_tdlr": "This paper studies the problem of representational drift induced by stochastic gradient descent in a two-layer neural network. The authors decompose the learning dynamics into the normal and tangent spaces of the minimum-loss manifold and show that the former corresponds to a finite variance fluctuation while the latter could be considered as an effective diffusion process on the manifold."
}