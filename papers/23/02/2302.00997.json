{
    "title": "Constrained Online Two-stage Stochastic Optimization: Near Optimal Algorithms via Adversarial Learning. (arXiv:2302.00997v2 [cs.LG] UPDATED)",
    "abstract": "We consider an online two-stage stochastic optimization with long-term constraints over a finite horizon of $T$ periods. At each period, we take the first-stage action, observe a model parameter realization and then take the second-stage action from a feasible set that depends both on the first-stage decision and the model parameter. We aim to minimize the cumulative objective value while guaranteeing that the long-term average second-stage decision belongs to a set. We develop online algorithms for the online two-stage problem from adversarial learning algorithms. Also, the regret bound of our algorithm cam be reduced to the regret bound of embedded adversarial learning algorithms. Based on our framework, we obtain new results under various settings. When the model parameter at each period is drawn from identical distributions, we derive state-of-art $O(\\sqrt{T})$ regret that improves previous bounds under special cases. Our algorithm is also robust to adversarial corruptions of model",
    "link": "http://arxiv.org/abs/2302.00997",
    "context": "Title: Constrained Online Two-stage Stochastic Optimization: Near Optimal Algorithms via Adversarial Learning. (arXiv:2302.00997v2 [cs.LG] UPDATED)\nAbstract: We consider an online two-stage stochastic optimization with long-term constraints over a finite horizon of $T$ periods. At each period, we take the first-stage action, observe a model parameter realization and then take the second-stage action from a feasible set that depends both on the first-stage decision and the model parameter. We aim to minimize the cumulative objective value while guaranteeing that the long-term average second-stage decision belongs to a set. We develop online algorithms for the online two-stage problem from adversarial learning algorithms. Also, the regret bound of our algorithm cam be reduced to the regret bound of embedded adversarial learning algorithms. Based on our framework, we obtain new results under various settings. When the model parameter at each period is drawn from identical distributions, we derive state-of-art $O(\\sqrt{T})$ regret that improves previous bounds under special cases. Our algorithm is also robust to adversarial corruptions of model",
    "path": "papers/23/02/2302.00997.json",
    "total_tokens": 1027,
    "translated_title": "受限在线两阶段随机优化：通过对抗学习获得近似最优算法",
    "translated_abstract": "我们考虑一个在线两阶段随机优化问题，其具有有限的$T$期紧约束条件。在每个时间段，我们先作出第一阶段决策，然后观察模型参数的实现，最后从取决于第一阶段决策和模型参数的可行集中做出第二阶段决策。我们旨在最小化累计目标值，同时保证长期平均的第二阶段决策属于一个集合。我们利用对抗性学习算法从在线两阶段问题中开发在线算法。此外，我们算法的遗憾界可以降至嵌入对抗性学习算法的遗憾界。基于我们的框架，在各种设置下我们都获得了新的结果。当每个时间段的模型参数都是从相同的分布中抽取的时候，我们得到了最先进的$O（\\sqrt{T}）$遗憾界，这比之前的特殊情况下的界有所提升。我们的算法还可以抵抗模型的敌对性扰动。",
    "tldr": "在线两阶段随机优化算法的累计目标值最小化，同时保证长期平均第二阶段决策结果属于一个集合。采用对抗性学习算法从在线两阶段问题中开发在线算法，其遗憾界可以降至嵌入对抗性学习算法的遗憾界，并在各种设置下获得了新的结果。",
    "en_tdlr": "This paper proposes an online algorithm for the two-stage stochastic optimization problem under long-term constraints, which ensures the minimization of cumulative objective value and the guarantee of the long-term average second-stage decision to a set. The algorithm is developed from adversarial learning algorithms, with the regret bound reduced to the regret bound of embedded adversarial learning algorithms. It achieves state-of-the-art $O(\\sqrt{T})$ regret under identical distribution setting and robust to adversarial corruptions of model."
}