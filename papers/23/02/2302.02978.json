{
    "title": "MuG: A Multimodal Classification Benchmark on Game Data with Tabular, Textual, and Visual Fields. (arXiv:2302.02978v2 [cs.LG] UPDATED)",
    "abstract": "Previous research has demonstrated the advantages of integrating data from multiple sources over traditional unimodal data, leading to the emergence of numerous novel multimodal applications. We propose a multimodal classification benchmark MuG with eight datasets that allows researchers to evaluate and improve their models. These datasets are collected from four various genres of games that cover tabular, textual, and visual modalities. We conduct multi-aspect data analysis to provide insights into the benchmark, including label balance ratios, percentages of missing features, distributions of data within each modality, and the correlations between labels and input modalities. We further present experimental results obtained by several state-of-the-art unimodal classifiers and multimodal classifiers, which demonstrate the challenging and multimodal-dependent properties of the benchmark. MuG is released at https://github.com/lujiaying/MUG-Bench with the data, tutorials, and implemented",
    "link": "http://arxiv.org/abs/2302.02978",
    "context": "Title: MuG: A Multimodal Classification Benchmark on Game Data with Tabular, Textual, and Visual Fields. (arXiv:2302.02978v2 [cs.LG] UPDATED)\nAbstract: Previous research has demonstrated the advantages of integrating data from multiple sources over traditional unimodal data, leading to the emergence of numerous novel multimodal applications. We propose a multimodal classification benchmark MuG with eight datasets that allows researchers to evaluate and improve their models. These datasets are collected from four various genres of games that cover tabular, textual, and visual modalities. We conduct multi-aspect data analysis to provide insights into the benchmark, including label balance ratios, percentages of missing features, distributions of data within each modality, and the correlations between labels and input modalities. We further present experimental results obtained by several state-of-the-art unimodal classifiers and multimodal classifiers, which demonstrate the challenging and multimodal-dependent properties of the benchmark. MuG is released at https://github.com/lujiaying/MUG-Bench with the data, tutorials, and implemented",
    "path": "papers/23/02/2302.02978.json",
    "total_tokens": 906,
    "translated_title": "MuG: 一个多模态分类基准，用于带有表格、文本和视觉字段的游戏数据",
    "translated_abstract": "先前的研究已经证明了整合多个数据源的优势，相对于传统的单一模态数据，引发了许多新的多模态应用的出现。我们提出了一个多模态分类基准MuG，其中包含了八个数据集，可以让研究人员评估和改进自己的模型。这些数据集来自四种不同类型的游戏，涵盖了表格、文本和视觉模态。我们进行了多方面的数据分析，提供了基准的洞见，包括标签平衡比、缺失特征的百分比、每个模态中数据的分布，以及标签和输入模态之间的相关性。我们还展示了几个最先进的单一模态分类器和多模态分类器的实验结果，这些结果显示了基准的具有挑战性和多模态依赖性的特点。MuG已经在https://github.com/lujiaying/MUG-Bench上发布，其中包括数据、教程和实现。",
    "tldr": "提出了一个多模态分类基准MuG，该基准包括八个来自不同类型游戏的数据集，涵盖了表格、文本和视觉模态。通过实验结果表明，该基准具有挑战性和多模态依赖性的特点。"
}