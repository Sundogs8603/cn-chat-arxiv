{
    "title": "THC: Accelerating Distributed Deep Learning Using Tensor Homomorphic Compression",
    "abstract": "arXiv:2302.08545v2 Announce Type: replace-cross  Abstract: Deep neural networks (DNNs) are the de facto standard for essential use cases, such as image classification, computer vision, and natural language processing. As DNNs and datasets get larger, they require distributed training on increasingly larger clusters. A main bottleneck is the resulting communication overhead where workers exchange model updates (i.e., gradients) on a per-round basis. To address this bottleneck and accelerate training, a widely-deployed approach is compression. However, previous deployments often apply bi-directional compression schemes by simply using a uni-directional gradient compression scheme in each direction. This results in significant computational overheads at the parameter server and increased compression error, leading to longer training and lower accuracy. We introduce Tensor Homomorphic Compression (THC), a novel bi-directional compression framework that enables the direct aggregation of com",
    "link": "https://arxiv.org/abs/2302.08545",
    "context": "Title: THC: Accelerating Distributed Deep Learning Using Tensor Homomorphic Compression\nAbstract: arXiv:2302.08545v2 Announce Type: replace-cross  Abstract: Deep neural networks (DNNs) are the de facto standard for essential use cases, such as image classification, computer vision, and natural language processing. As DNNs and datasets get larger, they require distributed training on increasingly larger clusters. A main bottleneck is the resulting communication overhead where workers exchange model updates (i.e., gradients) on a per-round basis. To address this bottleneck and accelerate training, a widely-deployed approach is compression. However, previous deployments often apply bi-directional compression schemes by simply using a uni-directional gradient compression scheme in each direction. This results in significant computational overheads at the parameter server and increased compression error, leading to longer training and lower accuracy. We introduce Tensor Homomorphic Compression (THC), a novel bi-directional compression framework that enables the direct aggregation of com",
    "path": "papers/23/02/2302.08545.json",
    "total_tokens": 837,
    "translated_title": "THC：使用张量同态压缩加速分布式深度学习",
    "translated_abstract": "深度神经网络（DNNs）已经成为必要用例（如图像分类、计算机视觉和自然语言处理）的事实标准。随着DNNs和数据集变得越来越大，它们需要在越来越大的集群上进行分布式训练。 主要瓶颈是由工作者在每轮基础上交换模型更新（即梯度）产生的通信开销。 为了解决这一瓶颈并加速训练，一个广泛部署的方法是压缩。 但是，先前的部署通常只是在每个方向上使用单方向梯度压缩方案来应用双向压缩方案。 这导致参数服务器上的显着计算开销和压缩误差增加，从而导致训练时间更长和准确性更低。 我们介绍了张量同态压缩（THC），这是一种新颖的双向压缩框架，能够直接聚合",
    "tldr": "引入了Tensor Homomorphic Compression (THC)，一种新颖的双向压缩框架，可以加速分布式深度学习中的模型训练"
}