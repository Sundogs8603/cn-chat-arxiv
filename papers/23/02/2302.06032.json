{
    "title": "Near-Optimal Non-Convex Stochastic Optimization under Generalized Smoothness. (arXiv:2302.06032v2 [cs.LG] UPDATED)",
    "abstract": "The generalized smooth condition, $(L_{0},L_{1})$-smoothness, has triggered people's interest since it is more realistic in many optimization problems shown by both empirical and theoretical evidence. Two recent works established the $O(\\epsilon^{-3})$ sample complexity to obtain an $O(\\epsilon)$-stationary point. However, both require a large batch size on the order of $\\mathrm{ploy}(\\epsilon^{-1})$, which is not only computationally burdensome but also unsuitable for streaming applications. Additionally, these existing convergence bounds are established only for the expected rate, which is inadequate as they do not supply a useful performance guarantee on a single run. In this work, we solve the prior two problems simultaneously by revisiting a simple variant of the STORM algorithm. Specifically, under the $(L_{0},L_{1})$-smoothness and affine-type noises, we establish the first near-optimal $O(\\log(1/(\\delta\\epsilon))\\epsilon^{-3})$ high-probability sample complexity where $\\delta\\i",
    "link": "http://arxiv.org/abs/2302.06032",
    "context": "Title: Near-Optimal Non-Convex Stochastic Optimization under Generalized Smoothness. (arXiv:2302.06032v2 [cs.LG] UPDATED)\nAbstract: The generalized smooth condition, $(L_{0},L_{1})$-smoothness, has triggered people's interest since it is more realistic in many optimization problems shown by both empirical and theoretical evidence. Two recent works established the $O(\\epsilon^{-3})$ sample complexity to obtain an $O(\\epsilon)$-stationary point. However, both require a large batch size on the order of $\\mathrm{ploy}(\\epsilon^{-1})$, which is not only computationally burdensome but also unsuitable for streaming applications. Additionally, these existing convergence bounds are established only for the expected rate, which is inadequate as they do not supply a useful performance guarantee on a single run. In this work, we solve the prior two problems simultaneously by revisiting a simple variant of the STORM algorithm. Specifically, under the $(L_{0},L_{1})$-smoothness and affine-type noises, we establish the first near-optimal $O(\\log(1/(\\delta\\epsilon))\\epsilon^{-3})$ high-probability sample complexity where $\\delta\\i",
    "path": "papers/23/02/2302.06032.json",
    "total_tokens": 934,
    "translated_title": "近似最优的非凸随机优化在广义光滑性下",
    "translated_abstract": "广义光滑条件，$(L_{0},L_{1})$-光滑性，在许多优化问题中都更加现实，这通过经验和理论证据都得到了证明。最近的两项工作建立了$O(\\epsilon^{-3})$的样本复杂度，以获得$O(\\epsilon)$-稳定点。然而，这两者都需要一个大批量的大小，其数量级是$\\mathrm{poly}(\\epsilon^{-1})$，这不仅在计算上很负担，也不适用于流式应用。此外，这些现有的收敛界限只对预期速率进行了建立，这是不足的，因为它们在单次运行时没有提供有用的性能保证。在这项工作中，我们通过重新审视STORM算法的一个简单变体来同时解决前两个问题。具体来说，在$(L_{0},L_{1})$-光滑性和仿射型噪声下，我们建立了第一个近似最优的高概率样本复杂度，为$O(\\log(1/(\\delta\\epsilon))\\epsilon^{-3})$，其中$\\delta\\i",
    "tldr": "本文提出了一种在广义光滑性条件下的近似最优非凸随机优化算法解决了大批量大小和只基于预期速率的收敛界限两个问题。",
    "en_tdlr": "This paper proposes a near-optimal non-convex stochastic optimization algorithm under the condition of generalized smoothness, which solves the problems of large batch size and convergence bounds based only on the expected rate."
}