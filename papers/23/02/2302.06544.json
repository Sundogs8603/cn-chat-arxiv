{
    "title": "Probabilistic Circuits That Know What They Don't Know. (arXiv:2302.06544v3 [cs.LG] UPDATED)",
    "abstract": "Probabilistic circuits (PCs) are models that allow exact and tractable probabilistic inference. In contrast to neural networks, they are often assumed to be well-calibrated and robust to out-of-distribution (OOD) data. In this paper, we show that PCs are in fact not robust to OOD data, i.e., they don't know what they don't know. We then show how this challenge can be overcome by model uncertainty quantification. To this end, we propose tractable dropout inference (TDI), an inference procedure to estimate uncertainty by deriving an analytical solution to Monte Carlo dropout (MCD) through variance propagation. Unlike MCD in neural networks, which comes at the cost of multiple network evaluations, TDI provides tractable sampling-free uncertainty estimates in a single forward pass. TDI improves the robustness of PCs to distribution shift and OOD data, demonstrated through a series of experiments evaluating the classification confidence and uncertainty estimates on real-world data.",
    "link": "http://arxiv.org/abs/2302.06544",
    "context": "Title: Probabilistic Circuits That Know What They Don't Know. (arXiv:2302.06544v3 [cs.LG] UPDATED)\nAbstract: Probabilistic circuits (PCs) are models that allow exact and tractable probabilistic inference. In contrast to neural networks, they are often assumed to be well-calibrated and robust to out-of-distribution (OOD) data. In this paper, we show that PCs are in fact not robust to OOD data, i.e., they don't know what they don't know. We then show how this challenge can be overcome by model uncertainty quantification. To this end, we propose tractable dropout inference (TDI), an inference procedure to estimate uncertainty by deriving an analytical solution to Monte Carlo dropout (MCD) through variance propagation. Unlike MCD in neural networks, which comes at the cost of multiple network evaluations, TDI provides tractable sampling-free uncertainty estimates in a single forward pass. TDI improves the robustness of PCs to distribution shift and OOD data, demonstrated through a series of experiments evaluating the classification confidence and uncertainty estimates on real-world data.",
    "path": "papers/23/02/2302.06544.json",
    "total_tokens": 956,
    "translated_title": "知道自己不知道的概率电路",
    "translated_abstract": "概率电路（PC）是一种允许准确和可处理的概率推断的模型。与神经网络相比，它们通常被认为是良好校准的，并且对于超出分布（OOD）数据具有鲁棒性。本文表明 PC 实际上不具有对OOD数据的鲁棒性，进而展示了如何通过模型不确定性量化来克服这一挑战。为此，我们提出了可处理的随机失活推断（TDI）——一种推断程序，通过方差传播导出蒙特卡洛失活（MCD）的解析解来估计不确定性。与神经网络中的 MCD 不同，TDI不需要进行多次网络评估就可以提供可处理的无采样不确定性估计。通过一系列实验评估在真实数据上的分类置信度和不确定性估计，TDI改善了PC对分布漂移和OOD数据的鲁棒性。",
    "tldr": "本文指出概率电路（PC）对超出分布（OOD）数据不具备鲁棒性；通过模型不确定性量化，我们提出了可处理的随机失活推断（TDI）来克服这一挑战，并且这种方法可以在单个正向传递中提供可处理的无采样不确定性估计，从而改善了PC对分布漂移和OOD数据的鲁棒性。",
    "en_tdlr": "This paper shows that probabilistic circuits (PCs) are not robust to out-of-distribution (OOD) data and proposes tractable dropout inference (TDI) to overcome this challenge by quantifying model uncertainty. TDI provides tractable sampling-free uncertainty estimates in a single forward pass and improves the robustness of PCs to distribution shift and OOD data."
}