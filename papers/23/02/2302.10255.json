{
    "title": "NeuralStagger: Accelerating Physics-constrained Neural PDE Solver with Spatial-temporal Decomposition. (arXiv:2302.10255v2 [cs.LG] UPDATED)",
    "abstract": "Neural networks have shown great potential in accelerating the solution of partial differential equations (PDEs). Recently, there has been a growing interest in introducing physics constraints into training neural PDE solvers to reduce the use of costly data and improve the generalization ability. However, these physics constraints, based on certain finite dimensional approximations over the function space, must resolve the smallest scaled physics to ensure the accuracy and stability of the simulation, resulting in high computational costs from large input, output, and neural networks. This paper proposes a general acceleration methodology called NeuralStagger by spatially and temporally decomposing the original learning tasks into several coarser-resolution subtasks. We define a coarse-resolution neural solver for each subtask, which requires fewer computational resources, and jointly train them with the vanilla physics-constrained loss by simply arranging their outputs to reconstruct",
    "link": "http://arxiv.org/abs/2302.10255",
    "context": "Title: NeuralStagger: Accelerating Physics-constrained Neural PDE Solver with Spatial-temporal Decomposition. (arXiv:2302.10255v2 [cs.LG] UPDATED)\nAbstract: Neural networks have shown great potential in accelerating the solution of partial differential equations (PDEs). Recently, there has been a growing interest in introducing physics constraints into training neural PDE solvers to reduce the use of costly data and improve the generalization ability. However, these physics constraints, based on certain finite dimensional approximations over the function space, must resolve the smallest scaled physics to ensure the accuracy and stability of the simulation, resulting in high computational costs from large input, output, and neural networks. This paper proposes a general acceleration methodology called NeuralStagger by spatially and temporally decomposing the original learning tasks into several coarser-resolution subtasks. We define a coarse-resolution neural solver for each subtask, which requires fewer computational resources, and jointly train them with the vanilla physics-constrained loss by simply arranging their outputs to reconstruct",
    "path": "papers/23/02/2302.10255.json",
    "total_tokens": 925,
    "translated_title": "NeuralStagger: 利用时空分解加速物理约束下的神经偏微分方程解法器",
    "translated_abstract": "神经网络在加速偏微分方程的求解方面表现出了很大的潜力。最近，越来越多的研究关注于在训练神经偏微分方程解法器中引入物理约束来减少数据使用和提高模型的泛化能力。然而，这些基于有限维度近似的物理约束为了保证模拟的准确性和稳定性，必须解决最小尺度的物理问题，从而导致大量的计算资源使用，包括输入、输出和神经网络。本文提出了一种名为NeuralStagger的通用加速方法，通过将原始的学习任务空间和时间上进行分解，形成几个较粗分辨率的子任务。我们为每个子任务定义了一个粗分辨率的神经求解器，其所需的计算资源更少，并使用具有物理约束的标准损失联合训练它们，只需简单地排列其输出以重构完整的模拟结果。",
    "tldr": "本文提出了一种新的名为NeuralStagger的通用加速方法，通过将物理约束下的神经偏微分方程解法器进行时空分解得到多个粗分辨率的子任务，并使用物理约束的标准损失联合训练，从而极大地减少了计算资源使用。",
    "en_tdlr": "This paper proposes a new general acceleration method called NeuralStagger, which decomposes the neural PDE solver with physics constraints into several coarser-resolution subtasks through spatial and temporal decomposition, and trains them with the physics-constrained loss jointly, thus greatly reducing computational resources."
}