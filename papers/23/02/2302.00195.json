{
    "title": "Weight Prediction Boosts the Convergence of AdamW. (arXiv:2302.00195v2 [cs.LG] UPDATED)",
    "abstract": "In this paper, we introduce weight prediction into the AdamW optimizer to boost its convergence when training the deep neural network (DNN) models. In particular, ahead of each mini-batch training, we predict the future weights according to the update rule of AdamW and then apply the predicted future weights to do both forward pass and backward propagation. In this way, the AdamW optimizer always utilizes the gradients w.r.t. the future weights instead of current weights to update the DNN parameters, making the AdamW optimizer achieve better convergence. Our proposal is simple and straightforward to implement but effective in boosting the convergence of DNN training. We performed extensive experimental evaluations on image classification and language modeling tasks to verify the effectiveness of our proposal. The experimental results validate that our proposal can boost the convergence of AdamW and achieve better accuracy than AdamW when training the DNN models.",
    "link": "http://arxiv.org/abs/2302.00195",
    "context": "Title: Weight Prediction Boosts the Convergence of AdamW. (arXiv:2302.00195v2 [cs.LG] UPDATED)\nAbstract: In this paper, we introduce weight prediction into the AdamW optimizer to boost its convergence when training the deep neural network (DNN) models. In particular, ahead of each mini-batch training, we predict the future weights according to the update rule of AdamW and then apply the predicted future weights to do both forward pass and backward propagation. In this way, the AdamW optimizer always utilizes the gradients w.r.t. the future weights instead of current weights to update the DNN parameters, making the AdamW optimizer achieve better convergence. Our proposal is simple and straightforward to implement but effective in boosting the convergence of DNN training. We performed extensive experimental evaluations on image classification and language modeling tasks to verify the effectiveness of our proposal. The experimental results validate that our proposal can boost the convergence of AdamW and achieve better accuracy than AdamW when training the DNN models.",
    "path": "papers/23/02/2302.00195.json",
    "total_tokens": 831,
    "translated_title": "权重预测提升AdamW的收敛性",
    "translated_abstract": "本文将权重预测引入AdamW优化器，以加速深度神经网络（DNN）模型的收敛过程。在每个小批量训练之前，我们根据AdamW的更新规则预测未来的权重，并将预测的未来权重应用于前向传播和反向传播。通过这种方式，AdamW优化器始终利用与未来权重相关的梯度而不是当前权重来更新DNN参数，使得AdamW优化器能够实现更好的收敛性。我们的提议简单直观，易于实现，但在加速DNN训练的收敛性方面非常有效。我们在图像分类和语言建模任务上进行了大量的实验评估，结果验证了我们的提议可以提升AdamW的收敛性，并在训练DNN模型时实现更高的准确性。",
    "tldr": "本文通过将权重预测技术引入AdamW优化器，加速了深度神经网络的收敛过程，实验结果表明，该方法提升了AdamW的收敛性并在训练DNN模型时获得更高的准确性。",
    "en_tdlr": "This paper introduces weight prediction into the AdamW optimizer to boost its convergence when training deep neural network (DNN) models. Experimental results validate that this method improves the convergence of AdamW and achieves higher accuracy when training DNN models."
}