{
    "title": "Better Training of GFlowNets with Local Credit and Incomplete Trajectories. (arXiv:2302.01687v2 [cs.LG] UPDATED)",
    "abstract": "Generative Flow Networks or GFlowNets are related to Monte-Carlo Markov chain methods (as they sample from a distribution specified by an energy function), reinforcement learning (as they learn a policy to sample composed objects through a sequence of steps), generative models (as they learn to represent and sample from a distribution) and amortized variational methods (as they can be used to learn to approximate and sample from an otherwise intractable posterior, given a prior and a likelihood). They are trained to generate an object $x$ through a sequence of steps with probability proportional to some reward function $R(x)$ (or $\\exp(-\\mathcal{E}(x))$ with $\\mathcal{E}(x)$ denoting the energy function), given at the end of the generative trajectory. Like for other RL settings where the reward is only given at the end, the efficiency of training and credit assignment may suffer when those trajectories are longer. With previous GFlowNet work, no learning was possible from incomplete tr",
    "link": "http://arxiv.org/abs/2302.01687",
    "context": "Title: Better Training of GFlowNets with Local Credit and Incomplete Trajectories. (arXiv:2302.01687v2 [cs.LG] UPDATED)\nAbstract: Generative Flow Networks or GFlowNets are related to Monte-Carlo Markov chain methods (as they sample from a distribution specified by an energy function), reinforcement learning (as they learn a policy to sample composed objects through a sequence of steps), generative models (as they learn to represent and sample from a distribution) and amortized variational methods (as they can be used to learn to approximate and sample from an otherwise intractable posterior, given a prior and a likelihood). They are trained to generate an object $x$ through a sequence of steps with probability proportional to some reward function $R(x)$ (or $\\exp(-\\mathcal{E}(x))$ with $\\mathcal{E}(x)$ denoting the energy function), given at the end of the generative trajectory. Like for other RL settings where the reward is only given at the end, the efficiency of training and credit assignment may suffer when those trajectories are longer. With previous GFlowNet work, no learning was possible from incomplete tr",
    "path": "papers/23/02/2302.01687.json",
    "total_tokens": 1184,
    "translated_title": "基于局部信用和不完整轨迹的GFlowNets更好的训练方法",
    "translated_abstract": "生成流网络或GFlowNets与Markov链蒙特卡罗方法相关(因为他们从由能量函数确定的分布中取样),与强化学习(因为他们学习通过一系列步骤取样组合的对象的策略),生成模型(因为他们学习表示和从分布中取样),以及分摊变分方法(因为它们可以用来学习近似并从一个否则难以处理的后验分布中取样,给定先验分布和似然函数)。它们被训练来生成一个对象$x$，通过一系列步骤，其概率与一些奖励函数$R(x)$(或$\\exp(-\\mathcal{E}(x))$，其中$\\mathcal{E}(x)$表示能量函数)成正比，给出了生成轨迹的末端。与其他RL设置一样，在奖励仅在结束时给出时，当这些轨迹较长时，训练效率和信用分配可能受到影响。在以前的GFlowNet工作中，不能从不完整的过渡中进行学习。在本文中，作者提出了一种新的信用分配策略，允许使用不完整的轨迹更好地训练GFlowNets。该方法涉及到为轨迹的每个步骤分配部分奖励，基于局部奖励估计器。这导致了对可用数据的更有效利用和GFlowNets性能的改善。在基准数据集上的实验结果证明了所提出方法的有效性。",
    "tldr": "本文提出了一种基于局部信用和不完整轨迹的GFlowNets更好的训练方法，通过为轨迹的每个步骤分配部分奖励，基于局部奖励估计器，实现更有效利用数据，提高了GFlowNets性能。",
    "en_tdlr": "This paper proposes a better training method for GFlowNets based on local credit and incomplete trajectories. The method assigns partial rewards to each step of the trajectory, based on a local reward estimator, resulting in more efficient use of available data and improved performance of GFlowNets."
}