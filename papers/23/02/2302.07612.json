{
    "title": "Towards Optimal Compression: Joint Pruning and Quantization. (arXiv:2302.07612v2 [cs.LG] UPDATED)",
    "abstract": "Model compression is instrumental in optimizing deep neural network inference on resource-constrained hardware. The prevailing methods for network compression, namely quantization and pruning, have been shown to enhance efficiency at the cost of performance. Determining the most effective quantization and pruning strategies for individual layers and parameters remains a challenging problem, often requiring computationally expensive and ad hoc numerical optimization techniques. This paper introduces FITCompress, a novel method integrating layer-wise mixed-precision quantization and unstructured pruning using a unified heuristic approach. By leveraging the Fisher Information Metric and path planning through compression space, FITCompress optimally selects a combination of pruning mask and mixed-precision quantization configuration for a given pre-trained model and compression constraint. Experiments on computer vision and natural language processing benchmarks demonstrate that our propos",
    "link": "http://arxiv.org/abs/2302.07612",
    "context": "Title: Towards Optimal Compression: Joint Pruning and Quantization. (arXiv:2302.07612v2 [cs.LG] UPDATED)\nAbstract: Model compression is instrumental in optimizing deep neural network inference on resource-constrained hardware. The prevailing methods for network compression, namely quantization and pruning, have been shown to enhance efficiency at the cost of performance. Determining the most effective quantization and pruning strategies for individual layers and parameters remains a challenging problem, often requiring computationally expensive and ad hoc numerical optimization techniques. This paper introduces FITCompress, a novel method integrating layer-wise mixed-precision quantization and unstructured pruning using a unified heuristic approach. By leveraging the Fisher Information Metric and path planning through compression space, FITCompress optimally selects a combination of pruning mask and mixed-precision quantization configuration for a given pre-trained model and compression constraint. Experiments on computer vision and natural language processing benchmarks demonstrate that our propos",
    "path": "papers/23/02/2302.07612.json",
    "total_tokens": 988,
    "translated_title": "追求最优压缩：联合剪枝和量化",
    "translated_abstract": "模型压缩对于在资源受限的硬件上优化深度神经网络推理至关重要。目前网络压缩的主要方法，即量化和剪枝，已被证明可以提高效率，但会付出性能的代价。确定适用于单个层和参数的最有效量化和剪枝策略仍然是一项具有挑战性的问题，通常需要计算量昂贵且特别的数值优化技术。本文介绍了FITCompress，一种新颖的方法，它使用统一的启发式方法集成了逐层混合精度量化和非结构化剪枝。通过利用Fisher信息度量和压缩空间中的路径规划，FITCompress可以为给定的预训练模型和压缩约束优化选择剪枝掩码和混合精度量化配置的组合。在计算机视觉和自然语言处理基准测试上的实验表明，我们提出的方法实现了最先进的压缩率，同时保持或甚至提高了模型的初始性能。",
    "tldr": "本论文介绍了一种名为FITCompress的方法，它可以联合使用剪枝和混合精度量化来对预训练模型进行优化选择，以实现最优压缩。该方法基于Fisher信息度量和压缩空间中的路径规划，可在保持模型原始性能的情况下实现最新颖的压缩。"
}