{
    "title": "Improving Differentiable Architecture Search via Self-Distillation. (arXiv:2302.05629v2 [cs.CV] UPDATED)",
    "abstract": "Differentiable Architecture Search (DARTS) is a simple yet efficient Neural Architecture Search (NAS) method. During the search stage, DARTS trains a supernet by jointly optimizing architecture parameters and network parameters. During the evaluation stage, DARTS discretizes the supernet to derive the optimal architecture based on architecture parameters. However, recent research has shown that during the training process, the supernet tends to converge towards sharp minima rather than flat minima. This is evidenced by the higher sharpness of the loss landscape of the supernet, which ultimately leads to a performance gap between the supernet and the optimal architecture. In this paper, we propose Self-Distillation Differentiable Neural Architecture Search (SD-DARTS) to alleviate the discretization gap. We utilize self-distillation to distill knowledge from previous steps of the supernet to guide its training in the current step, effectively reducing the sharpness of the supernet's loss",
    "link": "http://arxiv.org/abs/2302.05629",
    "context": "Title: Improving Differentiable Architecture Search via Self-Distillation. (arXiv:2302.05629v2 [cs.CV] UPDATED)\nAbstract: Differentiable Architecture Search (DARTS) is a simple yet efficient Neural Architecture Search (NAS) method. During the search stage, DARTS trains a supernet by jointly optimizing architecture parameters and network parameters. During the evaluation stage, DARTS discretizes the supernet to derive the optimal architecture based on architecture parameters. However, recent research has shown that during the training process, the supernet tends to converge towards sharp minima rather than flat minima. This is evidenced by the higher sharpness of the loss landscape of the supernet, which ultimately leads to a performance gap between the supernet and the optimal architecture. In this paper, we propose Self-Distillation Differentiable Neural Architecture Search (SD-DARTS) to alleviate the discretization gap. We utilize self-distillation to distill knowledge from previous steps of the supernet to guide its training in the current step, effectively reducing the sharpness of the supernet's loss",
    "path": "papers/23/02/2302.05629.json",
    "total_tokens": 925,
    "translated_title": "通过自我蒸馏改进可微分架构搜索方法",
    "translated_abstract": "可微分架构搜索（DARTS）是一种简单且高效的神经架构搜索（NAS）方法。DARTS在搜索阶段通过联合优化架构参数和网络参数来训练超网。在评估阶段，DARTS将超网离散化，从而得到基于架构参数的最优架构。然而，最近的研究表明，在训练过程中，超网往往会收敛到尖锐的极小值点而不是平坦的极小值点。这体现在超网损失曲面的尖锐程度较高，最终导致超网与最优架构之间存在性能差距。本文提出了自我蒸馏可微分神经架构搜索（SD-DARTS）来缓解离散化差距。我们利用自我蒸馏从超网的先前步骤中蒸馏知识，引导其在当前步骤中的训练，有效降低了超网损失的尖锐度。",
    "tldr": "本文提出了自我蒸馏可微分神经架构搜索（SD-DARTS）方法，通过从超网的先前步骤中蒸馏知识来指导其训练，有效降低了超网损失的尖锐度，从而缓解了离散化差距。",
    "en_tdlr": "This paper proposes a self-distillation differentiable neural architecture search method (SD-DARTS) to alleviate the discretization gap by distilling knowledge from previous steps of the supernet to reduce the sharpness of its loss, thereby improving the performance."
}