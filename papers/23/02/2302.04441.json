{
    "title": "Multi-task Representation Learning for Pure Exploration in Linear Bandits. (arXiv:2302.04441v2 [cs.LG] UPDATED)",
    "abstract": "Despite the recent success of representation learning in sequential decision making, the study of the pure exploration scenario (i.e., identify the best option and minimize the sample complexity) is still limited. In this paper, we study multi-task representation learning for best arm identification in linear bandits (RepBAI-LB) and best policy identification in contextual linear bandits (RepBPI-CLB), two popular pure exploration settings with wide applications, e.g., clinical trials and web content optimization. In these two problems, all tasks share a common low-dimensional linear representation, and our goal is to leverage this feature to accelerate the best arm (policy) identification process for all tasks. For these problems, we design computationally and sample efficient algorithms DouExpDes and C-DouExpDes, which perform double experimental designs to plan optimal sample allocations for learning the global representation. We show that by learning the common representation among ",
    "link": "http://arxiv.org/abs/2302.04441",
    "context": "Title: Multi-task Representation Learning for Pure Exploration in Linear Bandits. (arXiv:2302.04441v2 [cs.LG] UPDATED)\nAbstract: Despite the recent success of representation learning in sequential decision making, the study of the pure exploration scenario (i.e., identify the best option and minimize the sample complexity) is still limited. In this paper, we study multi-task representation learning for best arm identification in linear bandits (RepBAI-LB) and best policy identification in contextual linear bandits (RepBPI-CLB), two popular pure exploration settings with wide applications, e.g., clinical trials and web content optimization. In these two problems, all tasks share a common low-dimensional linear representation, and our goal is to leverage this feature to accelerate the best arm (policy) identification process for all tasks. For these problems, we design computationally and sample efficient algorithms DouExpDes and C-DouExpDes, which perform double experimental designs to plan optimal sample allocations for learning the global representation. We show that by learning the common representation among ",
    "path": "papers/23/02/2302.04441.json",
    "total_tokens": 1009,
    "translated_title": "线性赌臂算法中的多任务表示学习",
    "translated_abstract": "尽管序列决策中表示学习的最近成功, 但在纯探索场景 (即, 找到最佳选项和最小化样本复杂度) 的研究仍然有限. 在本文中, 我们研究了纯探索设置中的多任务表示学习, 包括线性赌臂中的最佳臂识别 (RepBAI-LB) 和上下文线性赌臂中的最佳策略识别 (RepBPI-CLB). 在这两个问题中, 所有任务共享相同的低维线性表示, 我们的目标是利用这个特性加速所有任务的最佳臂 (策略) 识别过程. 我们设计了计算和样本高效的算法 DouExpDes and C-DouExpDes 来解决这些问题, 它们执行双重实验设计来规划学习全局表示的最佳样本分配。我们表明，通过学习各个任务之间的公共表示，我们的算法可以实现与现有最先进算法相比样本复杂度的显著改进。在合成和现实数据集上的实验结果证明了我们提出方法的有效性。",
    "tldr": "本文研究了纯探索设置下的多任务表示学习，通过学习所有任务之间的公共低维线性表示，设计了计算和样本高效的算法来加速最佳臂或策略识别过程，在合成和现实数据集上实验结果证明了算法的有效性。",
    "en_tdlr": "This paper studies multi-task representation learning in pure exploration settings, and designs computationally and sample efficient algorithms by leveraging the common low-dimensional linear representation among all tasks, achieving significant improvements in terms of sample complexity for best arm or policy identification in linear bandits, as demonstrated by experimental results on both synthetic and real-world datasets."
}