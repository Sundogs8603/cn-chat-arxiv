{
    "title": "Is Multimodal Vision Supervision Beneficial to Language?. (arXiv:2302.05016v2 [cs.CV] UPDATED)",
    "abstract": "Vision (image and video) - Language (VL) pre-training is the recent popular paradigm that achieved state-of-the-art results on multi-modal tasks like image-retrieval, video-retrieval, visual question answering etc. These models are trained in an unsupervised way and greatly benefit from the complementary modality supervision. In this paper, we explore if the language representations trained using vision supervision perform better than vanilla language representations on Natural Language Understanding and commonsense reasoning benchmarks. We experiment with a diverse set of image-text models such as ALBEF, BLIP, METER and video-text models like ALPRO, Frozen-in-Time (FiT), VIOLET. We compare the performance of language representations of stand-alone text encoders of these models to the language representations of text encoders learnt through vision supervision. Our experiments suggest that vanilla language representations show superior performance on most of the tasks. These results she",
    "link": "http://arxiv.org/abs/2302.05016",
    "context": "Title: Is Multimodal Vision Supervision Beneficial to Language?. (arXiv:2302.05016v2 [cs.CV] UPDATED)\nAbstract: Vision (image and video) - Language (VL) pre-training is the recent popular paradigm that achieved state-of-the-art results on multi-modal tasks like image-retrieval, video-retrieval, visual question answering etc. These models are trained in an unsupervised way and greatly benefit from the complementary modality supervision. In this paper, we explore if the language representations trained using vision supervision perform better than vanilla language representations on Natural Language Understanding and commonsense reasoning benchmarks. We experiment with a diverse set of image-text models such as ALBEF, BLIP, METER and video-text models like ALPRO, Frozen-in-Time (FiT), VIOLET. We compare the performance of language representations of stand-alone text encoders of these models to the language representations of text encoders learnt through vision supervision. Our experiments suggest that vanilla language representations show superior performance on most of the tasks. These results she",
    "path": "papers/23/02/2302.05016.json",
    "total_tokens": 873,
    "translated_title": "多模态视觉监督对语言有益吗？",
    "translated_abstract": "视觉（图像和视频）-语言（VL）预训练是最近流行的模式，它在多模态任务如图像检索、视频检索、视觉问题回答等方面取得了最先进的结果。这些模型以无监督的方式进行训练，并且非常受益于补充模态监督。在本文中，我们探讨了使用视觉监督训练的语言表示是否比普通语言表示在自然语言理解和常识推理基准测试方面表现更好。我们将试验不同的图像-文本模型，如ALBEF、BLIP、METER等，以及视频-文本模型，如ALPRO、Frozen-in-Time（FiT）、VIOLET等。我们将这些模型的独立文本编码器的语言表示与通过视觉监督学习的文本编码器的语言表示进行比较。我们的实验表明，大多数任务中，普通的语言表示表现出更好的性能。",
    "tldr": "本文探讨了使用视觉监督训练的语言表示是否比普通语言表示在自然语言理解和常识推理基准测试方面表现更好。结果表明，大多数任务中，普通的语言表示表现出更好的性能。",
    "en_tdlr": "This paper explores whether language representations trained using vision supervision perform better than vanilla language representations on Natural Language Understanding and commonsense reasoning benchmarks. The experiments suggest that vanilla language representations show superior performance on most tasks."
}