{
    "title": "On Modeling Long-Term User Engagement from Stochastic Feedback. (arXiv:2302.06101v2 [cs.IR] UPDATED)",
    "abstract": "An ultimate goal of recommender systems (RS) is to improve user engagement. Reinforcement learning (RL) is a promising paradigm for this goal, as it directly optimizes overall performance of sequential recommendation. However, many existing RL-based approaches induce huge computational overhead, because they require not only the recommended items but also all other candidate items to be stored. This paper proposes an efficient alternative that does not require the candidate items. The idea is to model the correlation between user engagement and items directly from data. Moreover, the proposed approach consider randomness in user feedback and termination behavior, which are ubiquitous for RS but rarely discussed in RL-based prior work. With online A/B experiments on real-world RS, we confirm the efficacy of the proposed approach and the importance of modeling the two types of randomness.",
    "link": "http://arxiv.org/abs/2302.06101",
    "context": "Title: On Modeling Long-Term User Engagement from Stochastic Feedback. (arXiv:2302.06101v2 [cs.IR] UPDATED)\nAbstract: An ultimate goal of recommender systems (RS) is to improve user engagement. Reinforcement learning (RL) is a promising paradigm for this goal, as it directly optimizes overall performance of sequential recommendation. However, many existing RL-based approaches induce huge computational overhead, because they require not only the recommended items but also all other candidate items to be stored. This paper proposes an efficient alternative that does not require the candidate items. The idea is to model the correlation between user engagement and items directly from data. Moreover, the proposed approach consider randomness in user feedback and termination behavior, which are ubiquitous for RS but rarely discussed in RL-based prior work. With online A/B experiments on real-world RS, we confirm the efficacy of the proposed approach and the importance of modeling the two types of randomness.",
    "path": "papers/23/02/2302.06101.json",
    "total_tokens": 809,
    "translated_title": "论建立基于随机反馈的长期用户参与度模型",
    "translated_abstract": "推荐系统的终极目标是提高用户参与度。强化学习是实现此目标的一种有前途的范例，因为它直接优化了序贯推荐的整体表现。然而，现有的基于强化学习的方法需要保存推荐的物品以及其他候选物品，这会导致巨大的计算开销。本文提出了一种高效的替代方法，不需要候选项，而是直接从数据中建立用户参与度与物品之间的相关性。此外，所提出的方法考虑了用户反馈和终止行为的随机性，在推荐系统中具有普适性但在以前的基于强化学习的工作中很少被讨论。在真实推荐系统的在线 A/B 实验中，我们证实了所提出的方法的有效性和建立两种类型随机模型的重要性。",
    "tldr": "本文提出了一种高效的基于数据的用户参与度与物品相关性建模方法，特别考虑了推荐系统中用户反馈和终止行为的随机性。",
    "en_tdlr": "This paper proposes an efficient method to model the correlation between user engagement and items directly from data, which considers randomness in user feedback and termination behavior, and confirms its efficacy with online A/B experiments on real-world recommender systems."
}