{
    "title": "Lightweight Parameter Pruning for Energy-Efficient Deep Learning: A Binarized Gating Module Approach. (arXiv:2302.10798v2 [cs.LG] UPDATED)",
    "abstract": "The subject of green AI has been gaining attention within the deep learning community given the recent trend of ever larger and more complex neural network models. Existing solutions for reducing the computational load of training at inference time usually involve pruning the network parameters. Pruning schemes often create extra overhead either by iterative training and fine-tuning for static pruning or repeated computation of a dynamic pruning graph. We propose a new parameter pruning strategy for learning a lighter-weight sub-network that minimizes the energy cost while maintaining comparable performance to the fully parameterised network on given downstream tasks. Our proposed pruning scheme is green-oriented, as it only requires a one-off training to discover the optimal static sub-networks by dynamic pruning methods. The pruning scheme consists of a binary gating module and a novel loss function to uncover sub-networks with user-defined sparsity. Our method enables pruning and tr",
    "link": "http://arxiv.org/abs/2302.10798",
    "context": "Title: Lightweight Parameter Pruning for Energy-Efficient Deep Learning: A Binarized Gating Module Approach. (arXiv:2302.10798v2 [cs.LG] UPDATED)\nAbstract: The subject of green AI has been gaining attention within the deep learning community given the recent trend of ever larger and more complex neural network models. Existing solutions for reducing the computational load of training at inference time usually involve pruning the network parameters. Pruning schemes often create extra overhead either by iterative training and fine-tuning for static pruning or repeated computation of a dynamic pruning graph. We propose a new parameter pruning strategy for learning a lighter-weight sub-network that minimizes the energy cost while maintaining comparable performance to the fully parameterised network on given downstream tasks. Our proposed pruning scheme is green-oriented, as it only requires a one-off training to discover the optimal static sub-networks by dynamic pruning methods. The pruning scheme consists of a binary gating module and a novel loss function to uncover sub-networks with user-defined sparsity. Our method enables pruning and tr",
    "path": "papers/23/02/2302.10798.json",
    "total_tokens": 1043,
    "translated_title": "轻量化参数裁剪以实现节能深度学习: 一种二值门控模块方法",
    "translated_abstract": "随着神经网络越来越大且更加复杂，绿色AI已经引起了深度学习社区的关注。现有的解决方案通常采用对网络参数进行裁剪，以减少训练推断时的计算负荷。然而，裁剪方案通常会导致额外的开销，因为需要进行迭代训练和微调或重复计算动态裁剪图。我们提出了一种新的参数裁剪策略，以学习轻量级子网络，既能最小化能耗，又能在给定的下游任务上与完全参数化的网络保持相似的性能。我们的裁剪方案以绿色为导向，因为它仅需要动态裁剪方法进行一次训练来发现最佳的静态子网络。该方案由一个二进制门控模块和一个新颖的损失函数组成，以发现具有用户定义稀疏度的子网络。我们的方法可以对卷积神经网络（CNN）和循环神经网络（RNN）等通用神经网络进行裁剪和转换，大大减少了计算复杂度和能量消耗，同时在已有的基准测试上取得了竞争性的结果。",
    "tldr": "本论文提出了一种轻量化参数裁剪策略来实现节能深度学习，可以发现最佳的静态子网络，包含二值门控模块和新颖的损失函数，适用于各种神经网络，同时在已有的基准测试中取得了竞争性结果。",
    "en_tdlr": "This paper proposes a lightweight parameter pruning strategy for energy-efficient deep learning, which can discover optimal static sub-networks with a binary gating module and novel loss function, applicable to various neural networks, and achieving competitive results on established benchmarks."
}