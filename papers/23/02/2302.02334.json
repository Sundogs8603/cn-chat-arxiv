{
    "title": "Revisiting Discriminative vs. Generative Classifiers: Theory and Implications. (arXiv:2302.02334v2 [cs.LG] UPDATED)",
    "abstract": "A large-scale deep model pre-trained on massive labeled or unlabeled data transfers well to downstream tasks. Linear evaluation freezes parameters in the pre-trained model and trains a linear classifier separately, which is efficient and attractive for transfer. However, little work has investigated the classifier in linear evaluation except for the default logistic regression. Inspired by the statistical efficiency of naive Bayes, the paper revisits the classical topic on discriminative vs. generative classifiers. Theoretically, the paper considers the surrogate loss instead of the zero-one loss in analyses and generalizes the classical results from binary cases to multiclass ones. We show that, under mild assumptions, multiclass naive Bayes requires $O(\\log n)$ samples to approach its asymptotic error while the corresponding multiclass logistic regression requires $O(n)$ samples, where $n$ is the feature dimension. To establish it, we present a multiclass $\\mathcal{H}$-consistency bo",
    "link": "http://arxiv.org/abs/2302.02334",
    "context": "Title: Revisiting Discriminative vs. Generative Classifiers: Theory and Implications. (arXiv:2302.02334v2 [cs.LG] UPDATED)\nAbstract: A large-scale deep model pre-trained on massive labeled or unlabeled data transfers well to downstream tasks. Linear evaluation freezes parameters in the pre-trained model and trains a linear classifier separately, which is efficient and attractive for transfer. However, little work has investigated the classifier in linear evaluation except for the default logistic regression. Inspired by the statistical efficiency of naive Bayes, the paper revisits the classical topic on discriminative vs. generative classifiers. Theoretically, the paper considers the surrogate loss instead of the zero-one loss in analyses and generalizes the classical results from binary cases to multiclass ones. We show that, under mild assumptions, multiclass naive Bayes requires $O(\\log n)$ samples to approach its asymptotic error while the corresponding multiclass logistic regression requires $O(n)$ samples, where $n$ is the feature dimension. To establish it, we present a multiclass $\\mathcal{H}$-consistency bo",
    "path": "papers/23/02/2302.02334.json",
    "total_tokens": 962,
    "translated_title": "重新审视判别式分类器与生成式分类器：理论与应用",
    "translated_abstract": "大规模深度模型预先在大规模标记或未标记数据上进行训练，可以有效地转移到下游任务。线性评估将预先训练的模型中的参数冻结，并单独训练一个线性分类器，这是一种有效且有吸引力的转移方法。然而，目前很少有研究线性评估中的分类器，除了默认的逻辑回归分类器。本文受到朴素贝叶斯的统计效率启发，重新审视了关于判别式与生成式分类器的经典主题。理论上，本文考虑使用代理损失而不是0-1损失进行分析，并将经典结果从二元情况推广到多类情况。我们表明，在温和的假设下，多类朴素贝叶斯需要$O(\\log n)$个样本来接近其渐近误差，而相应的多类逻辑回归需要$O(n)$个样本，其中$n$是特征维度。为了证明这一点，我们提出了一个多类$\\mathcal{H}$-一致性下界。",
    "tldr": "本文重新审视关于判别式与生成式分类器的经典主题，利用多类$\\mathcal{H}$-一致性下界，证明了在温和的假设下，多类朴素贝叶斯分类器的样本要求比逻辑回归分类器多了$O(\\log n)$。"
}