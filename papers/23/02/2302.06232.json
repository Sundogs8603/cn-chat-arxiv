{
    "title": "Understanding Multimodal Contrastive Learning and Incorporating Unpaired Data. (arXiv:2302.06232v3 [cs.LG] UPDATED)",
    "abstract": "Language-supervised vision models have recently attracted great attention in computer vision. A common approach to build such models is to use contrastive learning on paired data across the two modalities, as exemplified by Contrastive Language-Image Pre-Training (CLIP). In this paper, under linear representation settings, (i) we initiate the investigation of a general class of nonlinear loss functions for multimodal contrastive learning (MMCL) including CLIP loss and show its connection to singular value decomposition (SVD). Namely, we show that each step of loss minimization by gradient descent can be seen as performing SVD on a contrastive cross-covariance matrix. Based on this insight, (ii) we analyze the performance of MMCL. We quantitatively show that the feature learning ability of MMCL can be better than that of unimodal contrastive learning applied to each modality even under the presence of wrongly matched pairs. This characterizes the robustness of MMCL to noisy data. Furthe",
    "link": "http://arxiv.org/abs/2302.06232",
    "context": "Title: Understanding Multimodal Contrastive Learning and Incorporating Unpaired Data. (arXiv:2302.06232v3 [cs.LG] UPDATED)\nAbstract: Language-supervised vision models have recently attracted great attention in computer vision. A common approach to build such models is to use contrastive learning on paired data across the two modalities, as exemplified by Contrastive Language-Image Pre-Training (CLIP). In this paper, under linear representation settings, (i) we initiate the investigation of a general class of nonlinear loss functions for multimodal contrastive learning (MMCL) including CLIP loss and show its connection to singular value decomposition (SVD). Namely, we show that each step of loss minimization by gradient descent can be seen as performing SVD on a contrastive cross-covariance matrix. Based on this insight, (ii) we analyze the performance of MMCL. We quantitatively show that the feature learning ability of MMCL can be better than that of unimodal contrastive learning applied to each modality even under the presence of wrongly matched pairs. This characterizes the robustness of MMCL to noisy data. Furthe",
    "path": "papers/23/02/2302.06232.json",
    "total_tokens": 961,
    "translated_title": "理解多模式对比学习及整合非配对数据",
    "translated_abstract": "最近，语言监督的视觉模型在计算机视觉领域引起了广泛关注。构建这种模型的常见方法是使用对比学习方法在两种模态之间对配对数据进行学习，例如对比语言 - 图像预训练（CLIP）。在这篇论文中，我们在线性表示设置下，（i）启动了一个关于多模式对比学习（MMCL）的一般非线性Loss函数的调查，包括CLIP Loss，并展示了它与奇异值分解（SVD）之间的联系。即，我们展示了梯度下降每一步Loss最小化可以被视为对一个对比性协方差矩阵进行SVD。基于这个洞察，（ii）我们分析了MMCL的性能。我们定量地表明，在错误匹配的情况下，MMCL的特征学习能力可以比单模式对比学习应用于每种模式更好。这表征了MMCL对嘈杂数据的鲁棒性。",
    "tldr": "本论文研究了一般类的非线性损失函数进行多模式对比学习，揭示了其与奇异值分解的联系。并证明在错误匹配的情况下，多模式对比学习可以比单模式对比学习表现更佳，表现了其对嘈杂数据的鲁棒性。",
    "en_tdlr": "This paper investigates a general class of nonlinear loss functions for multimodal contrastive learning and shows its connection to singular value decomposition. It proves that the feature learning ability of multimodal contrastive learning can be better than that of unimodal contrastive learning in the presence of wrongly matched pairs, which characterizes its robustness to noisy data."
}