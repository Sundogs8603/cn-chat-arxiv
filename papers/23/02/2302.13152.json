{
    "title": "On Bellman's principle of optimality and Reinforcement learning for safety-constrained Markov decision process. (arXiv:2302.13152v3 [eess.SY] UPDATED)",
    "abstract": "We study optimality for the safety-constrained Markov decision process which is the underlying framework for safe reinforcement learning. Specifically, we consider a constrained Markov decision process (with finite states and finite actions) where the goal of the decision maker is to reach a target set while avoiding an unsafe set(s) with certain probabilistic guarantees. Therefore the underlying Markov chain for any control policy will be multichain since by definition there exists a target set and an unsafe set. The decision maker also has to be optimal (with respect to a cost function) while navigating to the target set. This gives rise to a multi-objective optimization problem. We highlight the fact that Bellman's principle of optimality may not hold for constrained Markov decision problems with an underlying multichain structure (as shown by the counterexample due to Haviv. We resolve the counterexample by formulating the aforementioned multi-objective optimization problem as a ze",
    "link": "http://arxiv.org/abs/2302.13152",
    "context": "Title: On Bellman's principle of optimality and Reinforcement learning for safety-constrained Markov decision process. (arXiv:2302.13152v3 [eess.SY] UPDATED)\nAbstract: We study optimality for the safety-constrained Markov decision process which is the underlying framework for safe reinforcement learning. Specifically, we consider a constrained Markov decision process (with finite states and finite actions) where the goal of the decision maker is to reach a target set while avoiding an unsafe set(s) with certain probabilistic guarantees. Therefore the underlying Markov chain for any control policy will be multichain since by definition there exists a target set and an unsafe set. The decision maker also has to be optimal (with respect to a cost function) while navigating to the target set. This gives rise to a multi-objective optimization problem. We highlight the fact that Bellman's principle of optimality may not hold for constrained Markov decision problems with an underlying multichain structure (as shown by the counterexample due to Haviv. We resolve the counterexample by formulating the aforementioned multi-objective optimization problem as a ze",
    "path": "papers/23/02/2302.13152.json",
    "total_tokens": 957,
    "translated_title": "关于贝尔曼最优性原理和安全限制马尔可夫决策过程的强化学习研究",
    "translated_abstract": "本文研究了安全限制马尔可夫决策过程，这是安全强化学习的基本框架。具体而言，我们考虑了一个有限状态和有限动作的受限马尔可夫决策过程，在这个过程中，决策者的目标是在一定的概率保证下到达目标集合，同时避免进入一个不安全的集合。因此，任何控制策略下的马尔可夫链都会表现为多链结构，因为根据定义，存在一个目标集合和一个不安全集合。决策者在导航到目标集合时还必须是最优的（基于一个成本函数）。这导致了一个多目标优化问题。我们强调贝尔曼最优性原理在具有多链结构的受限马尔可夫决策问题中可能不成立（正如Haviv的反例所示）。我们通过将上述多目标优化问题表示为一个新的优化框架，解决了这个反例。",
    "tldr": "本文研究了安全限制马尔可夫决策过程，强调了贝尔曼最优性原理在具有多链结构的受限马尔可夫决策问题中可能不成立。通过将多目标优化问题表示为新的优化框架，解决了这个问题。",
    "en_tdlr": "This paper investigates the safety-constrained Markov decision process and highlights the potential violation of Bellman's principle of optimality in such problems with a multichain structure. The paper resolves this issue by formulating the multi-objective optimization problem in a new framework."
}