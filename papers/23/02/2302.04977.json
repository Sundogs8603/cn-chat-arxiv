{
    "title": "Mithridates: Boosting Natural Resistance to Backdoor Learning. (arXiv:2302.04977v2 [cs.CR] UPDATED)",
    "abstract": "Machine learning (ML) models trained on data from potentially untrusted sources are vulnerable to poisoning. A small, maliciously crafted subset of the training inputs can cause the model to learn a \"backdoor\" task (e.g., misclassify inputs with a certain feature) in addition to its main task. While backdoor attacks remain largely a hypothetical threat, state-of-the-art defenses require massive changes to the existing ML pipelines and are too complex for practical deployment.  In this paper, we take a pragmatic view and investigate natural resistance of ML pipelines to backdoor attacks, i.e., resistance that can be achieved without changes to how models are trained. We design, implement, and evaluate Mithridates, a new method that helps practitioners answer two actionable questions: (1) how well does my model resist backdoor poisoning attacks?, and (2) how can I increase its resistance without changing the training pipeline? Mithridates leverages hyperparameter search $\\unicode{x2013}$",
    "link": "http://arxiv.org/abs/2302.04977",
    "context": "Title: Mithridates: Boosting Natural Resistance to Backdoor Learning. (arXiv:2302.04977v2 [cs.CR] UPDATED)\nAbstract: Machine learning (ML) models trained on data from potentially untrusted sources are vulnerable to poisoning. A small, maliciously crafted subset of the training inputs can cause the model to learn a \"backdoor\" task (e.g., misclassify inputs with a certain feature) in addition to its main task. While backdoor attacks remain largely a hypothetical threat, state-of-the-art defenses require massive changes to the existing ML pipelines and are too complex for practical deployment.  In this paper, we take a pragmatic view and investigate natural resistance of ML pipelines to backdoor attacks, i.e., resistance that can be achieved without changes to how models are trained. We design, implement, and evaluate Mithridates, a new method that helps practitioners answer two actionable questions: (1) how well does my model resist backdoor poisoning attacks?, and (2) how can I increase its resistance without changing the training pipeline? Mithridates leverages hyperparameter search $\\unicode{x2013}$",
    "path": "papers/23/02/2302.04977.json",
    "total_tokens": 969,
    "translated_title": "Mithridates：增强对后门学习的自然抵抗",
    "translated_abstract": "在潜在不可信来源的数据上训练的机器学习模型容易受到中毒攻击。训练输入的一个小的恶意制作的子集可以导致模型学习一个“后门”任务（例如，错误分类具有特定特征的输入），除了其主任务。虽然后门攻击仍然主要是一种假设的威胁，但最先进的防御需要对现有的机器学习管道进行巨大的改变，而且对于实际部署来说太复杂了。在本文中，我们采取了一种实用的视角，研究了机器学习管道对后门攻击的自然抵抗力，即在不改变模型训练方式的情况下实现的抵抗力。我们设计、实现和评估了 Mithridates，一种新的方法，帮助从业人员回答两个可行的问题：（1）我的模型在抵御后门中毒攻击方面表现如何？（2）如何提高它的抵抗力而不改变训练管道？Mithridates 利用超参数搜索",
    "tldr": "Mithridates 是一种新方法，通过在不改变训练管道的情况下增强机器学习模型对后门攻击的自然抵抗力，让从业人员能够回答如何评估模型对后门中毒攻击的抵抗力以及如何提高抵抗力的可行问题。",
    "en_tdlr": "Mithridates is a new method that enhances the natural resistance of machine learning models to backdoor attacks without changing the training pipeline, enabling practitioners to evaluate their model's resistance to backdoor poisoning attacks and increase its resistance."
}