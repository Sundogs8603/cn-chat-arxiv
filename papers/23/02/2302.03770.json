{
    "title": "Provably Efficient Offline Goal-Conditioned Reinforcement Learning with General Function Approximation and Single-Policy Concentrability. (arXiv:2302.03770v2 [cs.LG] UPDATED)",
    "abstract": "Goal-conditioned reinforcement learning (GCRL) refers to learning general-purpose skills that aim to reach diverse goals. In particular, offline GCRL only requires purely pre-collected datasets to perform training tasks without additional interactions with the environment. Although offline GCRL has become increasingly prevalent and many previous works have demonstrated its empirical success, the theoretical understanding of efficient offline GCRL algorithms is not well established, especially when the state space is huge and the offline dataset only covers the policy we aim to learn. In this paper, we provide a rigorous theoretical analysis of an existing empirically successful offline GCRL algorithm. We prove that under slight modification, this algorithm enjoys an $\\widetilde{O}(\\text{poly}(1/\\epsilon))$ sample complexity (where $\\epsilon$ is the desired suboptimality of the learned policy) with general function approximation thanks to the property of (semi-)strong convexity of the o",
    "link": "http://arxiv.org/abs/2302.03770",
    "context": "Title: Provably Efficient Offline Goal-Conditioned Reinforcement Learning with General Function Approximation and Single-Policy Concentrability. (arXiv:2302.03770v2 [cs.LG] UPDATED)\nAbstract: Goal-conditioned reinforcement learning (GCRL) refers to learning general-purpose skills that aim to reach diverse goals. In particular, offline GCRL only requires purely pre-collected datasets to perform training tasks without additional interactions with the environment. Although offline GCRL has become increasingly prevalent and many previous works have demonstrated its empirical success, the theoretical understanding of efficient offline GCRL algorithms is not well established, especially when the state space is huge and the offline dataset only covers the policy we aim to learn. In this paper, we provide a rigorous theoretical analysis of an existing empirically successful offline GCRL algorithm. We prove that under slight modification, this algorithm enjoys an $\\widetilde{O}(\\text{poly}(1/\\epsilon))$ sample complexity (where $\\epsilon$ is the desired suboptimality of the learned policy) with general function approximation thanks to the property of (semi-)strong convexity of the o",
    "path": "papers/23/02/2302.03770.json",
    "total_tokens": 889,
    "translated_title": "可证明高效的离线目标条件强化学习与通用函数逼近和单一策略集中性研究",
    "translated_abstract": "目标条件强化学习（GCRL）是指学习通用技能，以达到不同的目标。特别是，离线GCRL只需要纯预先收集的数据集来执行训练任务，而无需与环境进行额外的交互。尽管离线GCRL越来越普遍，并且许多之前的工作已经证明了其实证成功，但对于大状态空间且离线数据集仅覆盖我们希望学习的策略的高效离线GCRL算法的理论理解尚未建立起来。本文对一种现有经验成功的离线GCRL算法进行了严格的理论分析。我们证明，通过轻微修改，该算法在具有通用函数逼近的情况下，具有$\\widetilde{O}(\\text{poly}(1/\\epsilon))$样本复杂度（其中$\\epsilon$是学习策略的期望非最优性）。",
    "tldr": "本文提供了关于离线目标条件强化学习算法的理论分析，证明了经过轻微修改后，该算法在满足通用函数逼近时具有 O(poly(1/ε)) 的样本复杂度。",
    "en_tdlr": "This paper provides a theoretical analysis of an offline goal-conditioned reinforcement learning algorithm and proves that with slight modifications, the algorithm achieves a sample complexity of O(poly(1/ε)) under the condition of general function approximation."
}