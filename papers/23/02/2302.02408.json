{
    "title": "Multi-View Masked World Models for Visual Robotic Manipulation. (arXiv:2302.02408v2 [cs.RO] UPDATED)",
    "abstract": "Visual robotic manipulation research and applications often use multiple cameras, or views, to better perceive the world. How else can we utilize the richness of multi-view data? In this paper, we investigate how to learn good representations with multi-view data and utilize them for visual robotic manipulation. Specifically, we train a multi-view masked autoencoder which reconstructs pixels of randomly masked viewpoints and then learn a world model operating on the representations from the autoencoder. We demonstrate the effectiveness of our method in a range of scenarios, including multi-view control and single-view control with auxiliary cameras for representation learning. We also show that the multi-view masked autoencoder trained with multiple randomized viewpoints enables training a policy with strong viewpoint randomization and transferring the policy to solve real-robot tasks without camera calibration and an adaptation procedure. Video demonstrations are available at: https:/",
    "link": "http://arxiv.org/abs/2302.02408",
    "context": "Title: Multi-View Masked World Models for Visual Robotic Manipulation. (arXiv:2302.02408v2 [cs.RO] UPDATED)\nAbstract: Visual robotic manipulation research and applications often use multiple cameras, or views, to better perceive the world. How else can we utilize the richness of multi-view data? In this paper, we investigate how to learn good representations with multi-view data and utilize them for visual robotic manipulation. Specifically, we train a multi-view masked autoencoder which reconstructs pixels of randomly masked viewpoints and then learn a world model operating on the representations from the autoencoder. We demonstrate the effectiveness of our method in a range of scenarios, including multi-view control and single-view control with auxiliary cameras for representation learning. We also show that the multi-view masked autoencoder trained with multiple randomized viewpoints enables training a policy with strong viewpoint randomization and transferring the policy to solve real-robot tasks without camera calibration and an adaptation procedure. Video demonstrations are available at: https:/",
    "path": "papers/23/02/2302.02408.json",
    "total_tokens": 988,
    "translated_title": "多视图遮蔽世界模型用于视觉机器人操作",
    "translated_abstract": "视觉机器人操作研究和应用通常使用多个摄像头或视角来更好地感知世界。那么我们如何利用多视图数据的丰富性呢？本文研究如何利用多视图数据学习良好的表示，并将其用于视觉机器人操作。具体而言，我们训练一个多视图遮蔽自编码器，该自编码器重构出随机遮蔽视点的像素，然后学习一个基于自编码器表示的世界模型。我们在一系列场景中展示了我们方法的有效性，包括多视图控制、为表示学习使用辅助摄像头的单视图控制。我们还表明，使用多个随机视点训练的多视图遮蔽自编码器能够训练具有强视点随机化的策略，并将该策略转移到解决实际机器人任务中，而无需相机校准和适应程序。视频演示可在以下网址查看：https:/ / 2302.02408v2 [cs.RO] UPDATED",
    "tldr": "本文介绍了一个多视图遮蔽自编码器，用于重构出随机遮蔽视点的像素，并学习基于自编码器表示的世界模型。该方法在多视图控制和单视图控制场景中展示了其有效性，能够训练出具有强视点随机化的策略，并应用于实际机器人任务中。",
    "en_tdlr": "This paper presents a multi-view masked autoencoder for reconstructing pixels of randomly masked viewpoints and learning a world model based on the representations from the autoencoder. The method is effective in multi-view control and single-view control scenarios, can train a policy with strong viewpoint randomization, and can be applied to real robot tasks without camera calibration and adaptation procedures."
}