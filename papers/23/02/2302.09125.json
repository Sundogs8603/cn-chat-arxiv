{
    "title": "JANA: Jointly Amortized Neural Approximation of Complex Bayesian Models. (arXiv:2302.09125v2 [cs.LG] UPDATED)",
    "abstract": "This work proposes ''jointly amortized neural approximation'' (JANA) of intractable likelihood functions and posterior densities arising in Bayesian surrogate modeling and simulation-based inference. We train three complementary networks in an end-to-end fashion: 1) a summary network to compress individual data points, sets, or time series into informative embedding vectors; 2) a posterior network to learn an amortized approximate posterior; and 3) a likelihood network to learn an amortized approximate likelihood. Their interaction opens a new route to amortized marginal likelihood and posterior predictive estimation -- two important ingredients of Bayesian workflows that are often too expensive for standard methods. We benchmark the fidelity of JANA on a variety of simulation models against state-of-the-art Bayesian methods and propose a powerful and interpretable diagnostic for joint calibration. In addition, we investigate the ability of recurrent likelihood networks to emulate comp",
    "link": "http://arxiv.org/abs/2302.09125",
    "context": "Title: JANA: Jointly Amortized Neural Approximation of Complex Bayesian Models. (arXiv:2302.09125v2 [cs.LG] UPDATED)\nAbstract: This work proposes ''jointly amortized neural approximation'' (JANA) of intractable likelihood functions and posterior densities arising in Bayesian surrogate modeling and simulation-based inference. We train three complementary networks in an end-to-end fashion: 1) a summary network to compress individual data points, sets, or time series into informative embedding vectors; 2) a posterior network to learn an amortized approximate posterior; and 3) a likelihood network to learn an amortized approximate likelihood. Their interaction opens a new route to amortized marginal likelihood and posterior predictive estimation -- two important ingredients of Bayesian workflows that are often too expensive for standard methods. We benchmark the fidelity of JANA on a variety of simulation models against state-of-the-art Bayesian methods and propose a powerful and interpretable diagnostic for joint calibration. In addition, we investigate the ability of recurrent likelihood networks to emulate comp",
    "path": "papers/23/02/2302.09125.json",
    "total_tokens": 990,
    "translated_title": "JANA：复杂贝叶斯模型的联合分摊近似神经网络",
    "translated_abstract": "本文提出了“联合分摊神经网络近似”（JANA）方法，用于处理贝叶斯代理建模和基于模拟的推理中出现的难以计算的似然函数和后验密度。我们以端到端的方式训练三个相互补充的神经网络：1）一个总结网络，将个别数据点、集合或时间序列压缩成信息嵌入向量；2）一个后验网络，学习分摊的近似后验；3）一个似然网络，学习分摊的近似似然。它们的交互为分摊边缘似然和后验预测估计提供了新的途径，这是贝叶斯工作流程的两个重要组成部分，常常对于标准方法来说太昂贵了。我们在各种模拟模型中对JANA的保真度进行了基准测试，与最先进的贝叶斯方法进行了比较，并提出了一种强大而可解释的联合校准诊断方法。此外，我们研究了循环似然网络模拟复杂模型的能力。",
    "tldr": "本文提出了 JANA 方法，用于处理复杂贝叶斯模型的近似计算。通过端到端训练三个神经网络来实现分摊的近似后验和似然，为贝叶斯工作流程提供了一种新的途径。此方法在多种模拟模型中进行了基准测试，并提出了一种联合校准诊断方法。",
    "en_tdlr": "This paper proposes JANA method for approximating complex Bayesian models. It trains three complementary neural networks in an end-to-end fashion to achieve amortized approximate posterior and likelihood, providing a new route for Bayesian workflows. JANA is benchmarked against state-of-the-art Bayesian methods and a joint calibration diagnostic is proposed."
}