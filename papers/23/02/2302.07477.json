{
    "title": "Optimal Sample Complexity of Reinforcement Learning for Mixing Discounted Markov Decision Processes. (arXiv:2302.07477v3 [cs.LG] UPDATED)",
    "abstract": "We consider the optimal sample complexity theory of tabular reinforcement learning (RL) for maximizing the infinite horizon discounted reward in a Markov decision process (MDP). Optimal worst-case complexity results have been developed for tabular RL problems in this setting, leading to a sample complexity dependence on $\\gamma$ and $\\epsilon$ of the form $\\tilde \\Theta((1-\\gamma)^{-3}\\epsilon^{-2})$, where $\\gamma$ denotes the discount factor and $\\epsilon$ is the solution error tolerance. However, in many applications of interest, the optimal policy (or all policies) induces mixing. We establish that in such settings, the optimal sample complexity dependence is $\\tilde \\Theta(t_{\\text{mix}}(1-\\gamma)^{-2}\\epsilon^{-2})$, where $t_{\\text{mix}}$ is the total variation mixing time. Our analysis is grounded in regeneration-type ideas, which we believe are of independent interest, as they can be used to study RL problems for general state space MDPs.",
    "link": "http://arxiv.org/abs/2302.07477",
    "context": "Title: Optimal Sample Complexity of Reinforcement Learning for Mixing Discounted Markov Decision Processes. (arXiv:2302.07477v3 [cs.LG] UPDATED)\nAbstract: We consider the optimal sample complexity theory of tabular reinforcement learning (RL) for maximizing the infinite horizon discounted reward in a Markov decision process (MDP). Optimal worst-case complexity results have been developed for tabular RL problems in this setting, leading to a sample complexity dependence on $\\gamma$ and $\\epsilon$ of the form $\\tilde \\Theta((1-\\gamma)^{-3}\\epsilon^{-2})$, where $\\gamma$ denotes the discount factor and $\\epsilon$ is the solution error tolerance. However, in many applications of interest, the optimal policy (or all policies) induces mixing. We establish that in such settings, the optimal sample complexity dependence is $\\tilde \\Theta(t_{\\text{mix}}(1-\\gamma)^{-2}\\epsilon^{-2})$, where $t_{\\text{mix}}$ is the total variation mixing time. Our analysis is grounded in regeneration-type ideas, which we believe are of independent interest, as they can be used to study RL problems for general state space MDPs.",
    "path": "papers/23/02/2302.07477.json",
    "total_tokens": 1009,
    "translated_title": "对于混合折扣马尔可夫决策过程的强化学习的最优样本复杂度研究",
    "translated_abstract": "我们考虑了表格型强化学习（RL）对于在马尔可夫决策过程（MDP）中最大化无穷时间折扣奖励的最优样本复杂度理论。在这种设定下，已经为表格型问题开发了最优最坏情况复杂度结果，导致样本复杂度依赖于折扣系数$\\gamma$和解误差容忍度$\\epsilon$的形式为$\\tilde \\Theta((1-\\gamma)^{-3}\\epsilon^{-2})$，其中$\\gamma$表示折扣因子，$\\epsilon$为解误差容忍度。然而，在许多感兴趣的应用中，最优策略（或所有策略）会产生混合。我们确定，在这种情况下，最优样本复杂度的依赖关系为$\\tilde \\Theta(t_{\\text{mix}}(1-\\gamma)^{-2}\\epsilon^{-2})$，其中$t_{\\text{mix}}$是总变异混合时间。我们的分析基于再生型思想，我们认为这些思想对于研究一般状态空间MDPs的RL问题具有独立的兴趣。",
    "tldr": "这篇论文研究了对于混合折扣马尔可夫决策过程的强化学习的最优样本复杂度理论。作者发现，在混合的情况下，最优样本复杂度依赖于总变异混合时间、折扣因子和解误差容忍度。"
}