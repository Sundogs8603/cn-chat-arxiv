{
    "title": "Efficient Exploration Using Extra Safety Budget in Constrained Policy Optimization. (arXiv:2302.14339v2 [cs.RO] UPDATED)",
    "abstract": "Reinforcement learning (RL) has achieved promising results on most robotic control tasks. Safety of learning-based controllers is an essential notion of ensuring the effectiveness of the controllers. Current methods adopt whole consistency constraints during the training, thus resulting in inefficient exploration in the early stage. In this paper, we propose an algorithm named Constrained Policy Optimization with Extra Safety Budget (ESB-CPO) to strike a balance between the exploration efficiency and the constraints satisfaction. In the early stage, our method loosens the practical constraints of unsafe transitions (adding extra safety budget) with the aid of a new metric we propose. With the training process, the constraints in our optimization problem become tighter. Meanwhile, theoretical analysis and practical experiments demonstrate that our method gradually meets the cost limit's demand in the final training stage. When evaluated on Safety-Gym and Bullet-Safety-Gym benchmarks, ou",
    "link": "http://arxiv.org/abs/2302.14339",
    "context": "Title: Efficient Exploration Using Extra Safety Budget in Constrained Policy Optimization. (arXiv:2302.14339v2 [cs.RO] UPDATED)\nAbstract: Reinforcement learning (RL) has achieved promising results on most robotic control tasks. Safety of learning-based controllers is an essential notion of ensuring the effectiveness of the controllers. Current methods adopt whole consistency constraints during the training, thus resulting in inefficient exploration in the early stage. In this paper, we propose an algorithm named Constrained Policy Optimization with Extra Safety Budget (ESB-CPO) to strike a balance between the exploration efficiency and the constraints satisfaction. In the early stage, our method loosens the practical constraints of unsafe transitions (adding extra safety budget) with the aid of a new metric we propose. With the training process, the constraints in our optimization problem become tighter. Meanwhile, theoretical analysis and practical experiments demonstrate that our method gradually meets the cost limit's demand in the final training stage. When evaluated on Safety-Gym and Bullet-Safety-Gym benchmarks, ou",
    "path": "papers/23/02/2302.14339.json",
    "total_tokens": 901,
    "translated_title": "利用额外安全预算在受限策略优化中高效探索",
    "translated_abstract": "强化学习（RL）在大多数机器人控制任务上取得了有希望的结果。学习驱动控制器的安全性是确保控制器有效性的重要概念。当前方法在训练过程中采用整体一致性约束，导致早期探索效率低下。在本文中，我们提出了一种名为额外安全预算的受限策略优化算法（ESB-CPO），以在探索效率和约束满足之间取得平衡。在早期阶段，我们的方法通过引入新的度量标准来放宽不安全转换的实际约束（添加额外安全预算）。随着训练过程的进行，我们优化问题中的约束逐渐变得更紧。同时，理论分析和实验结果表明，我们的方法能够逐步满足最终训练阶段的成本限制要求。在Safety-Gym和Bullet-Safety-Gym基准测试中的评估中，我们的方法表现优秀。",
    "tldr": "本文提出了一种名为ESB-CPO的算法，通过在早期阶段放宽约束并引入额外安全预算，在探索效率和约束满足之间取得平衡，从而提高了强化学习中受限策略优化的效果。"
}