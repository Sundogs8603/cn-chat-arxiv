{
    "title": "Neural Algorithmic Reasoning with Causal Regularisation. (arXiv:2302.10258v2 [cs.LG] UPDATED)",
    "abstract": "Recent work on neural algorithmic reasoning has investigated the reasoning capabilities of neural networks, effectively demonstrating they can learn to execute classical algorithms on unseen data coming from the train distribution. However, the performance of existing neural reasoners significantly degrades on out-of-distribution (OOD) test data, where inputs have larger sizes. In this work, we make an important observation: there are many different inputs for which an algorithm will perform certain intermediate computations identically. This insight allows us to develop data augmentation procedures that, given an algorithm's intermediate trajectory, produce inputs for which the target algorithm would have exactly the same next trajectory step. We ensure invariance in the next-step prediction across such inputs, by employing a self-supervised objective derived by our observation, formalised in a causal graph. We prove that the resulting method, which we call Hint-ReLIC, improves the OO",
    "link": "http://arxiv.org/abs/2302.10258",
    "context": "Title: Neural Algorithmic Reasoning with Causal Regularisation. (arXiv:2302.10258v2 [cs.LG] UPDATED)\nAbstract: Recent work on neural algorithmic reasoning has investigated the reasoning capabilities of neural networks, effectively demonstrating they can learn to execute classical algorithms on unseen data coming from the train distribution. However, the performance of existing neural reasoners significantly degrades on out-of-distribution (OOD) test data, where inputs have larger sizes. In this work, we make an important observation: there are many different inputs for which an algorithm will perform certain intermediate computations identically. This insight allows us to develop data augmentation procedures that, given an algorithm's intermediate trajectory, produce inputs for which the target algorithm would have exactly the same next trajectory step. We ensure invariance in the next-step prediction across such inputs, by employing a self-supervised objective derived by our observation, formalised in a causal graph. We prove that the resulting method, which we call Hint-ReLIC, improves the OO",
    "path": "papers/23/02/2302.10258.json",
    "total_tokens": 929,
    "translated_title": "具有因果正则化的神经算法推理",
    "translated_abstract": "最近关于神经算法推理的研究探究了神经网络的推理能力，有效地证明了它们可以学习在训练分布中未见过的数据上执行经典算法。然而，现有神经推理器在分布外（OOD）的测试数据上性能显著下降，因为输入的规模更大。在这项工作中，我们做出了重要观察：对于算法来说，有很多不同的输入会表现出完全相同的中间计算。这种洞察力使我们能够开发数据增强的程序，根据算法的中间轨迹生成能够使目标算法有完全相同下一轨迹步骤的输入。我们通过使用由我们的观察导出并在因果图中形式化的自监督目标来确保这些输入下的下一步预测的不变性。我们证明了由此产生的方法，我们称之为Hint-ReLIC，改善了OOD测试数据上的性能。",
    "tldr": "提出了一种具有因果正则化的神经算法推理方法，通过观察到对于某些中间计算来说存在许多不同的输入，可以开发数据增强程序，生成能够使目标算法有完全相同下一轨迹步骤的输入，从而提高在分布外测试数据上的性能。",
    "en_tdlr": "A neural algorithmic reasoning method with causal regularization, which utilizes the insight that there are many different inputs for which an algorithm will perform identically in intermediate computations, is proposed. This method generates inputs that have the same next trajectory step as the target algorithm by developing data augmentation procedures based on the algorithm's intermediate trajectory. The method, called Hint-ReLIC, improves the performance on out-of-distribution test data."
}