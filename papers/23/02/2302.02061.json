{
    "title": "Reinforcement Learning with History-Dependent Dynamic Contexts. (arXiv:2302.02061v2 [cs.LG] UPDATED)",
    "abstract": "We introduce Dynamic Contextual Markov Decision Processes (DCMDPs), a novel reinforcement learning framework for history-dependent environments that generalizes the contextual MDP framework to handle non-Markov environments, where contexts change over time. We consider special cases of the model, with a focus on logistic DCMDPs, which break the exponential dependence on history length by leveraging aggregation functions to determine context transitions. This special structure allows us to derive an upper-confidence-bound style algorithm for which we establish regret bounds. Motivated by our theoretical results, we introduce a practical model-based algorithm for logistic DCMDPs that plans in a latent space and uses optimism over history-dependent features. We demonstrate the efficacy of our approach on a recommendation task (using MovieLens data) where user behavior dynamics evolve in response to recommendations.",
    "link": "http://arxiv.org/abs/2302.02061",
    "context": "Title: Reinforcement Learning with History-Dependent Dynamic Contexts. (arXiv:2302.02061v2 [cs.LG] UPDATED)\nAbstract: We introduce Dynamic Contextual Markov Decision Processes (DCMDPs), a novel reinforcement learning framework for history-dependent environments that generalizes the contextual MDP framework to handle non-Markov environments, where contexts change over time. We consider special cases of the model, with a focus on logistic DCMDPs, which break the exponential dependence on history length by leveraging aggregation functions to determine context transitions. This special structure allows us to derive an upper-confidence-bound style algorithm for which we establish regret bounds. Motivated by our theoretical results, we introduce a practical model-based algorithm for logistic DCMDPs that plans in a latent space and uses optimism over history-dependent features. We demonstrate the efficacy of our approach on a recommendation task (using MovieLens data) where user behavior dynamics evolve in response to recommendations.",
    "path": "papers/23/02/2302.02061.json",
    "total_tokens": 945,
    "translated_title": "历史依赖动态环境下的强化学习",
    "translated_abstract": "我们引入了动态上下文马尔可夫决策过程（DCMDPs），这是一种新的强化学习框架，用于处理依赖历史环境的情况。它推广了上下文MDP框架，以处理非马尔可夫环境，其中上下文随时间变化。我们考虑了这个模型的特殊情况，着重于逻辑DCMDPs，它通过利用聚合函数确定上下文转换来打破对历史长度的指数依赖。这种特殊结构使我们能够推导出一种类似于上限置信界算法的算法，并建立了遗憾界。受我们的理论结果的启发，我们引入了一种实用的基于模型的算法，用于逻辑DCMDPs，这个算法在一个潜在空间中进行规划，并使用历史依赖特征上的乐观主义。我们在一个推荐任务上展示了我们方法的有效性（使用MovieLens数据集），其中用户行为动态地随着推荐的变化而演变。",
    "tldr": "介绍了一种称为DCMDPs的新型强化学习框架，用于处理依赖历史环境的情况。其中的逻辑DCMDPs通过利用聚合函数确定上下文转换，打破了对历史长度的指数依赖，并引入了一种实用的基于模型的算法。在推荐任务中展示了该方法的有效性。",
    "en_tdlr": "Introduced a new reinforcement learning framework called DCMDPs, which handles history-dependent environments. Novel logistic DCMDPs break the exponential dependence on history length using aggregation functions to determine context transitions. A practical model-based algorithm is introduced and demonstrated in a recommendation task where user behavior dynamics evolve in response to recommendations."
}