{
    "title": "Constrained Decision Transformer for Offline Safe Reinforcement Learning. (arXiv:2302.07351v2 [cs.LG] UPDATED)",
    "abstract": "Safe reinforcement learning (RL) trains a constraint satisfaction policy by interacting with the environment. We aim to tackle a more challenging problem: learning a safe policy from an offline dataset. We study the offline safe RL problem from a novel multi-objective optimization perspective and propose the $\\epsilon$-reducible concept to characterize problem difficulties. The inherent trade-offs between safety and task performance inspire us to propose the constrained decision transformer (CDT) approach, which can dynamically adjust the trade-offs during deployment. Extensive experiments show the advantages of the proposed method in learning an adaptive, safe, robust, and high-reward policy. CDT outperforms its variants and strong offline safe RL baselines by a large margin with the same hyperparameters across all tasks, while keeping the zero-shot adaptation capability to different constraint thresholds, making our approach more suitable for real-world RL under constraints. The code",
    "link": "http://arxiv.org/abs/2302.07351",
    "context": "Title: Constrained Decision Transformer for Offline Safe Reinforcement Learning. (arXiv:2302.07351v2 [cs.LG] UPDATED)\nAbstract: Safe reinforcement learning (RL) trains a constraint satisfaction policy by interacting with the environment. We aim to tackle a more challenging problem: learning a safe policy from an offline dataset. We study the offline safe RL problem from a novel multi-objective optimization perspective and propose the $\\epsilon$-reducible concept to characterize problem difficulties. The inherent trade-offs between safety and task performance inspire us to propose the constrained decision transformer (CDT) approach, which can dynamically adjust the trade-offs during deployment. Extensive experiments show the advantages of the proposed method in learning an adaptive, safe, robust, and high-reward policy. CDT outperforms its variants and strong offline safe RL baselines by a large margin with the same hyperparameters across all tasks, while keeping the zero-shot adaptation capability to different constraint thresholds, making our approach more suitable for real-world RL under constraints. The code",
    "path": "papers/23/02/2302.07351.json",
    "total_tokens": 922,
    "translated_title": "离线安全强化学习中的约束决策Transformer",
    "translated_abstract": "安全强化学习通过与环境交互训练约束满足策略。本文致力于解决更具挑战性的问题：从离线数据集中学习安全策略。我们从多目标优化的角度研究了离线安全强化学习问题，并提出了可ε-可减概念来表征问题难度。安全和任务绩效之间的固有权衡激发了我们提出约束决策Transformer（CDT）方法，该方法可以在部署过程中动态调整权衡。大量实验证明了所提出的方法在学习自适应、安全、鲁棒且高报酬的策略方面的优势。与所有任务相同的超参数下，CDT在保持零-shot适应能力的同时，以大幅超过其变体和强离线安全RL基线的性能表现，使我们的方法更适用于约束条件下的现实RL。",
    "tldr": "本文研究了离线安全强化学习问题，提出了约束决策Transformer方法。该方法可以在部署过程中动态调整权衡，具有学习自适应、安全、鲁棒且高报酬的优势表现。",
    "en_tdlr": "This paper proposes a constrained decision transformer approach for learning a safe policy from an offline dataset in safe reinforcement learning. The approach dynamically adjusts the trade-offs between safety and task performance, resulting in adaptive, safe, robust, and high-reward policy. It outperforms its variants and strong offline safe RL baselines by a large margin with the same hyperparameters across all tasks, making the approach suitable for real-world RL under constraints."
}