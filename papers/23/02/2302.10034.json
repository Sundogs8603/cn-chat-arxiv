{
    "title": "Over-Parameterization Exponentially Slows Down Gradient Descent for Learning a Single Neuron. (arXiv:2302.10034v2 [cs.LG] UPDATED)",
    "abstract": "We revisit the problem of learning a single neuron with ReLU activation under Gaussian input with square loss. We particularly focus on the over-parameterization setting where the student network has $n\\ge 2$ neurons. We prove the global convergence of randomly initialized gradient descent with a $O\\left(T^{-3}\\right)$ rate. This is the first global convergence result for this problem beyond the exact-parameterization setting ($n=1$) in which the gradient descent enjoys an $\\exp(-\\Omega(T))$ rate. Perhaps surprisingly, we further present an $\\Omega\\left(T^{-3}\\right)$ lower bound for randomly initialized gradient flow in the over-parameterization setting. These two bounds jointly give an exact characterization of the convergence rate and imply, for the first time, that over-parameterization can exponentially slow down the convergence rate. To prove the global convergence, we need to tackle the interactions among student neurons in the gradient descent dynamics, which are not present in",
    "link": "http://arxiv.org/abs/2302.10034",
    "context": "Title: Over-Parameterization Exponentially Slows Down Gradient Descent for Learning a Single Neuron. (arXiv:2302.10034v2 [cs.LG] UPDATED)\nAbstract: We revisit the problem of learning a single neuron with ReLU activation under Gaussian input with square loss. We particularly focus on the over-parameterization setting where the student network has $n\\ge 2$ neurons. We prove the global convergence of randomly initialized gradient descent with a $O\\left(T^{-3}\\right)$ rate. This is the first global convergence result for this problem beyond the exact-parameterization setting ($n=1$) in which the gradient descent enjoys an $\\exp(-\\Omega(T))$ rate. Perhaps surprisingly, we further present an $\\Omega\\left(T^{-3}\\right)$ lower bound for randomly initialized gradient flow in the over-parameterization setting. These two bounds jointly give an exact characterization of the convergence rate and imply, for the first time, that over-parameterization can exponentially slow down the convergence rate. To prove the global convergence, we need to tackle the interactions among student neurons in the gradient descent dynamics, which are not present in",
    "path": "papers/23/02/2302.10034.json",
    "total_tokens": 988,
    "translated_title": "过度参数化会导致梯度下降在学习单个神经元时的收敛速度指数级减慢",
    "translated_abstract": "我们重新审视了在具有ReLU激活函数和方形损失的高斯输入下学习单个神经元的问题。我们特别关注学生网络具有n≥2个神经元的过度参数化设置。我们证明了随机初始化的梯度下降算法以O(T^-3)的速度全局收敛。这是对于该问题的第一个超过精确参数化设置(n=1)的全局收敛结果，其中梯度下降算法呈现出exp(-Ω(T))的速度。令人惊讶的是，我们进一步证明了在过度参数化设置中，随机初始化的梯度流算法的下界是Ω(T^-3)。这两个下界共同给出了收敛速度的精确刻画，并首次暗示了过度参数化会使收敛速度指数级减慢。为了证明全局收敛，我们需要处理梯度下降动力学中学生神经元之间的相互作用，这在精确参数化设置中不存在。",
    "tldr": "通过研究学习单个神经元的过度参数化设置，本研究发现梯度下降算法的收敛速度会以指数级减慢，是首个给出该问题全局收敛结果的研究。通过证明上下界，我们精确刻画出了收敛速度，并指出了过度参数化对于收敛速度的影响。",
    "en_tdlr": "By studying the over-parameterization setting in learning a single neuron, this research discovers that the convergence rate of gradient descent algorithm exponentially slows down, providing the first global convergence result for this problem. Through proving upper and lower bounds, we accurately characterize the convergence rate and demonstrate the impact of over-parameterization on the convergence rate."
}