{
    "title": "MultiRobustBench: Benchmarking Robustness Against Multiple Attacks. (arXiv:2302.10980v2 [cs.LG] UPDATED)",
    "abstract": "The bulk of existing research in defending against adversarial examples focuses on defending against a single (typically bounded Lp-norm) attack, but for a practical setting, machine learning (ML) models should be robust to a wide variety of attacks. In this paper, we present the first unified framework for considering multiple attacks against ML models. Our framework is able to model different levels of learner's knowledge about the test-time adversary, allowing us to model robustness against unforeseen attacks and robustness against unions of attacks. Using our framework, we present the first leaderboard, MultiRobustBench, for benchmarking multiattack evaluation which captures performance across attack types and attack strengths. We evaluate the performance of 16 defended models for robustness against a set of 9 different attack types, including Lp-based threat models, spatial transformations, and color changes, at 20 different attack strengths (180 attacks total). Additionally, we a",
    "link": "http://arxiv.org/abs/2302.10980",
    "context": "Title: MultiRobustBench: Benchmarking Robustness Against Multiple Attacks. (arXiv:2302.10980v2 [cs.LG] UPDATED)\nAbstract: The bulk of existing research in defending against adversarial examples focuses on defending against a single (typically bounded Lp-norm) attack, but for a practical setting, machine learning (ML) models should be robust to a wide variety of attacks. In this paper, we present the first unified framework for considering multiple attacks against ML models. Our framework is able to model different levels of learner's knowledge about the test-time adversary, allowing us to model robustness against unforeseen attacks and robustness against unions of attacks. Using our framework, we present the first leaderboard, MultiRobustBench, for benchmarking multiattack evaluation which captures performance across attack types and attack strengths. We evaluate the performance of 16 defended models for robustness against a set of 9 different attack types, including Lp-based threat models, spatial transformations, and color changes, at 20 different attack strengths (180 attacks total). Additionally, we a",
    "path": "papers/23/02/2302.10980.json",
    "total_tokens": 1048,
    "translated_title": "MultiRobustBench: 对抗多种攻击的鲁棒性基准测试",
    "translated_abstract": "对抗性示例防御领域的很大一部分现有研究都专注于防御单一（通常是有界的Lp范数）攻击，但在实际应用中，机器学习模型需要对各种攻击具有鲁棒性。在本文中，我们提出了一种考虑多种攻击对机器学习模型鲁棒性的统一框架。我们的框架能够模拟学习器对测试时攻击者的不同了解水平，从而使我们能够对未知攻击和攻击集的鲁棒性进行建模。使用我们的框架，我们提出了第一个针对多攻击评估的排行榜 MultiRobustBench，该排行榜能够捕捉攻击类型和攻击强度之间的表现差异。我们对16个防御模型进行了评估，针对9种不同的攻击类型，包括Lp范数威胁模型、空间转换和颜色改变等，在20种不同的攻击强度下进行了测试（总共180次攻击）。此外，我们还针对现实世界中的图像数据集进行了实证评估，评估结果表明我们的多攻击框架的有效性和实用性。",
    "tldr": "本文提出了一个针对对抗性攻击的多个层面的鲁棒性统一框架，通过第一个多攻击评估排行榜 MultiRobustBench，评估了16个防御模型针对9种不同攻击类型和20种不同攻击强度的鲁棒性表现。",
    "en_tdlr": "This paper proposes a unified framework for evaluating robustness against multiple attacks in defending against adversarial examples, and presents the first multiattack evaluation leaderboard called MultiRobustBench. This framework models different levels of the learner's knowledge about the test-time adversary and captures performance across attack types and strengths, evaluated 16 defended models against 9 different attack types at 20 different attack strengths."
}