{
    "title": "On The Coherence of Quantitative Evaluation of Visual Explanations. (arXiv:2302.10764v3 [cs.CV] UPDATED)",
    "abstract": "Recent years have shown an increased development of methods for justifying the predictions of neural networks through visual explanations. These explanations usually take the form of heatmaps which assign a saliency (or relevance) value to each pixel of the input image that expresses how relevant the pixel is for the prediction of a label.  Complementing this development, evaluation methods have been proposed to assess the \"goodness\" of such explanations. On the one hand, some of these methods rely on synthetic datasets. However, this introduces the weakness of having limited guarantees regarding their applicability on more realistic settings. On the other hand, some methods rely on metrics for objective evaluation. However the level to which some of these evaluation methods perform with respect to each other is uncertain.  Taking this into account, we conduct a comprehensive study on a subset of the ImageNet-1k validation set where we evaluate a number of different commonly-used expla",
    "link": "http://arxiv.org/abs/2302.10764",
    "context": "Title: On The Coherence of Quantitative Evaluation of Visual Explanations. (arXiv:2302.10764v3 [cs.CV] UPDATED)\nAbstract: Recent years have shown an increased development of methods for justifying the predictions of neural networks through visual explanations. These explanations usually take the form of heatmaps which assign a saliency (or relevance) value to each pixel of the input image that expresses how relevant the pixel is for the prediction of a label.  Complementing this development, evaluation methods have been proposed to assess the \"goodness\" of such explanations. On the one hand, some of these methods rely on synthetic datasets. However, this introduces the weakness of having limited guarantees regarding their applicability on more realistic settings. On the other hand, some methods rely on metrics for objective evaluation. However the level to which some of these evaluation methods perform with respect to each other is uncertain.  Taking this into account, we conduct a comprehensive study on a subset of the ImageNet-1k validation set where we evaluate a number of different commonly-used expla",
    "path": "papers/23/02/2302.10764.json",
    "total_tokens": 997,
    "translated_title": "关于视觉解释定量评估的一致性",
    "translated_abstract": "近年来，通过视觉解释来证明神经网络预测的方法得到了增强发展。这些解释通常采用热图的形式，为输入图像的每个像素分配一个显著性值，表示像素对标签预测的相关性。为了评估这种解释的质量，已经提出了评估方法。一些这样的评估方法依赖于合成数据集，但这样会引入在更现实的情景下适用性的有限保证。另一些方法依赖于客观评估的度量。但是有关这些评估方法的执行水平的不确定性很大。因此，我们对ImageNet-1k验证集的一个子集进行了全面研究，使用多个评估度量来评估不同的常用神经网络解释方法。我们的研究旨在确定不同的评估设置下各个方法的表现如何，以及不同的评估方法之间的比较如何。我们发现，在所使用的评估度量上，这些方法的表现经常是不一致的，而且在观察的表现中，选择评估度量是至关重要的。",
    "tldr": "本研究针对常用神经网络解释方法，探究不同评估度量下的表现以及评估方法之间的比较，发现方法的表现经常不一致且选择评估度量至关重要。",
    "en_tdlr": "This study explores the performance of commonly used neural network explanation methods under different evaluation metrics and compares different evaluation methods. Results show that the methods' performance is often inconsistent across evaluation metrics and the choice of evaluation metric is critical."
}