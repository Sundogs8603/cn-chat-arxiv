{
    "title": "Task-Specific Skill Localization in Fine-tuned Language Models. (arXiv:2302.06600v2 [cs.CL] UPDATED)",
    "abstract": "Pre-trained language models can be fine-tuned to solve diverse NLP tasks, including in few-shot settings. Thus fine-tuning allows the model to quickly pick up task-specific ``skills,'' but there has been limited study of where these newly-learnt skills reside inside the massive model. This paper introduces the term skill localization for this problem and proposes a solution. Given the downstream task and a model fine-tuned on that task, a simple optimization is used to identify a very small subset of parameters ($\\sim0.01$% of model parameters) responsible for ($>95$%) of the model's performance, in the sense that grafting the fine-tuned values for just this tiny subset onto the pre-trained model gives performance almost as well as the fine-tuned model. While reminiscent of recent works on parameter-efficient fine-tuning, the novel aspects here are that: (i) No further re-training is needed on the subset (unlike, say, with lottery tickets). (ii) Notable improvements are seen over vanil",
    "link": "http://arxiv.org/abs/2302.06600",
    "context": "Title: Task-Specific Skill Localization in Fine-tuned Language Models. (arXiv:2302.06600v2 [cs.CL] UPDATED)\nAbstract: Pre-trained language models can be fine-tuned to solve diverse NLP tasks, including in few-shot settings. Thus fine-tuning allows the model to quickly pick up task-specific ``skills,'' but there has been limited study of where these newly-learnt skills reside inside the massive model. This paper introduces the term skill localization for this problem and proposes a solution. Given the downstream task and a model fine-tuned on that task, a simple optimization is used to identify a very small subset of parameters ($\\sim0.01$% of model parameters) responsible for ($>95$%) of the model's performance, in the sense that grafting the fine-tuned values for just this tiny subset onto the pre-trained model gives performance almost as well as the fine-tuned model. While reminiscent of recent works on parameter-efficient fine-tuning, the novel aspects here are that: (i) No further re-training is needed on the subset (unlike, say, with lottery tickets). (ii) Notable improvements are seen over vanil",
    "path": "papers/23/02/2302.06600.json",
    "total_tokens": 1016,
    "translated_title": "针对任务的技能定位在Fine-tuned语言模型中的应用",
    "translated_abstract": "预训练的语言模型可以通过Fine-tuned来解决各种NLP任务，包括少样本情况下的任务。因此，Fine-tuning使得模型能够快速掌握任务特定的“技能”，但是关于这些新学到的技能在庞大模型中的位置的研究还很有限。本文引入了技能定位的概念，并提出了一种解决方案。给定下游任务和在该任务上进行Fine-tuned的模型，利用简单的优化方法可以识别出负责模型性能的非常小的参数子集（占模型参数的约0.01%），这个子集对模型性能的贡献占比超过95%。换句话说，仅将Fine-tuned的值嫁接到预训练模型的这个小子集上，就可以获得几乎和Fine-tuned模型一样好的性能。虽然与最近关于参数高效Fine-tuning的工作相似，但这里的两个新颖之处是：（i）不需要在子集上进行进一步的重新训练（不像“lottery tickets”那样）。（ii）相对于传统的Fine-tuned模型，可以看到显著的改进。",
    "tldr": "本文提出了针对Fine-tuned语言模型中任务特定技能定位的问题，并提出了一种解决方案，通过优化可以识别出贡献模型性能的非常小的参数子集，使得将Fine-tuned的值嫁接到这个子集上可以获得几乎和Fine-tuned模型一样好的性能。",
    "en_tdlr": "This paper introduces the concept of skill localization in fine-tuned language models and proposes a solution. By optimizing, a very small subset of parameters responsible for the model's performance can be identified, and grafting the fine-tuned values onto this subset achieves performance almost as well as the fine-tuned model."
}