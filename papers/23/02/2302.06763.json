{
    "title": "Breaking the Lower Bound with (Little) Structure: Acceleration in Non-Convex Stochastic Optimization with Heavy-Tailed Noise. (arXiv:2302.06763v2 [cs.LG] UPDATED)",
    "abstract": "We consider the stochastic optimization problem with smooth but not necessarily convex objectives in the heavy-tailed noise regime, where the stochastic gradient's noise is assumed to have bounded $p$th moment ($p\\in(1,2]$). Zhang et al. (2020) is the first to prove the $\\Omega(T^{\\frac{1-p}{3p-2}})$ lower bound for convergence (in expectation) and provides a simple clipping algorithm that matches this optimal rate. Cutkosky and Mehta (2021) proposes another algorithm, which is shown to achieve the nearly optimal high-probability convergence guarantee $O(\\log(T/\\delta)T^{\\frac{1-p}{3p-2}})$, where $\\delta$ is the probability of failure. However, this desirable guarantee is only established under the additional assumption that the stochastic gradient itself is bounded in $p$th moment, which fails to hold even for quadratic objectives and centered Gaussian noise.  In this work, we first improve the analysis of the algorithm in Cutkosky and Mehta (2021) to obtain the same nearly optimal h",
    "link": "http://arxiv.org/abs/2302.06763",
    "context": "Title: Breaking the Lower Bound with (Little) Structure: Acceleration in Non-Convex Stochastic Optimization with Heavy-Tailed Noise. (arXiv:2302.06763v2 [cs.LG] UPDATED)\nAbstract: We consider the stochastic optimization problem with smooth but not necessarily convex objectives in the heavy-tailed noise regime, where the stochastic gradient's noise is assumed to have bounded $p$th moment ($p\\in(1,2]$). Zhang et al. (2020) is the first to prove the $\\Omega(T^{\\frac{1-p}{3p-2}})$ lower bound for convergence (in expectation) and provides a simple clipping algorithm that matches this optimal rate. Cutkosky and Mehta (2021) proposes another algorithm, which is shown to achieve the nearly optimal high-probability convergence guarantee $O(\\log(T/\\delta)T^{\\frac{1-p}{3p-2}})$, where $\\delta$ is the probability of failure. However, this desirable guarantee is only established under the additional assumption that the stochastic gradient itself is bounded in $p$th moment, which fails to hold even for quadratic objectives and centered Gaussian noise.  In this work, we first improve the analysis of the algorithm in Cutkosky and Mehta (2021) to obtain the same nearly optimal h",
    "path": "papers/23/02/2302.06763.json",
    "total_tokens": 1023,
    "translated_title": "以(小)结构突破下界：具有重尾噪声的非凸随机优化中的加速。(arXiv:2302.06763v2 [cs.LG] 更新)",
    "translated_abstract": "在重尾噪声区域中，我们考虑具有平滑但不一定是凸目标的随机优化问题，其中假设随机梯度的噪声具有有界的$p$阶矩($p\\in(1,2]$)。Zhang等人(2020)首次证明了收敛的$\\Omega(T^{\\frac{1-p}{3p-2}})$下界，并提供了一个简单的剪切算法，以匹配这个最优速率。Cutkosky和Mehta(2021)提出了另一种算法，该算法被证明能够实现近乎最优的高概率收敛保证$O(\\log(T/\\delta)T^{\\frac{1-p}{3p-2}})$，其中$\\delta$是失败的概率。然而，这个理想的保证只在附加的假设下成立，即随机梯度本身在$p$阶矩上有界，而即使对于二次目标和中心化的高斯噪声，这个假设也不成立。在这项工作中，我们首先改进了Cutkosky和Mehta(2021)中算法的分析，以获得相同的近乎最优结果。",
    "tldr": "本论文在具有重尾噪声的非凸随机优化问题中，改进了Cutkosky和Mehta的算法，并提供了近乎最优的收敛保证，而无需对随机梯度的矩条件进行额外的假设。"
}