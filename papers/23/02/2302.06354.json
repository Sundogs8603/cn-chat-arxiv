{
    "title": "Less is More: Selective Layer Finetuning with SubTuning. (arXiv:2302.06354v3 [cs.LG] UPDATED)",
    "abstract": "Finetuning a pretrained model has become a standard approach for training neural networks on novel tasks, resulting in fast convergence and improved performance. In this work, we study an alternative finetuning method, where instead of finetuning all the weights of the network, we only train a carefully chosen subset of layers, keeping the rest of the weights frozen at their initial (pretrained) values. We demonstrate that \\emph{subset finetuning} (or SubTuning) often achieves accuracy comparable to full finetuning of the model, and even surpasses the performance of full finetuning when training data is scarce. Therefore, SubTuning allows deploying new tasks at minimal computational cost, while enjoying the benefits of finetuning the entire model. This yields a simple and effective method for multi-task learning, where different tasks do not interfere with one another, and yet share most of the resources at inference time. We demonstrate the efficiency of SubTuning across multiple task",
    "link": "http://arxiv.org/abs/2302.06354",
    "context": "Title: Less is More: Selective Layer Finetuning with SubTuning. (arXiv:2302.06354v3 [cs.LG] UPDATED)\nAbstract: Finetuning a pretrained model has become a standard approach for training neural networks on novel tasks, resulting in fast convergence and improved performance. In this work, we study an alternative finetuning method, where instead of finetuning all the weights of the network, we only train a carefully chosen subset of layers, keeping the rest of the weights frozen at their initial (pretrained) values. We demonstrate that \\emph{subset finetuning} (or SubTuning) often achieves accuracy comparable to full finetuning of the model, and even surpasses the performance of full finetuning when training data is scarce. Therefore, SubTuning allows deploying new tasks at minimal computational cost, while enjoying the benefits of finetuning the entire model. This yields a simple and effective method for multi-task learning, where different tasks do not interfere with one another, and yet share most of the resources at inference time. We demonstrate the efficiency of SubTuning across multiple task",
    "path": "papers/23/02/2302.06354.json",
    "total_tokens": 974,
    "translated_title": "少即是多：选择性层微调与子微调",
    "translated_abstract": "对预训练模型进行微调已成为在新任务上训练神经网络的标准方法，导致了快速收敛和改善的性能。在这项工作中，我们研究了一种替代的微调方法，即不对网络的所有权重进行微调，而是只训练一组精心选择的层，使其余的权重保持在其初始（预训练）值上。我们证明了\\emph{子微调}（SubTuning）经常能够达到与对模型进行全微调相当的准确性，并且在训练数据稀缺时甚至超过了全微调的性能。因此，SubTuning允许以最小的计算成本部署新任务，同时享受整个模型微调的好处。这为多任务学习提供了一种简单有效的方法，在推理时不同任务之间不干扰，而在大部分资源上共享。我们展示了SubTuning在多个任务上的效率。",
    "tldr": "本研究提出了一种选择性层微调与子微调的方法，通过仅对精心选择的层进行微调，而将其余权重保持在预训练值上。该方法在准确性上能够与全模型微调相媲美，并在训练数据稀缺时表现更好。这一简单而有效的方法适用于多任务学习，并能够在推理过程中实现任务间的资源共享。",
    "en_tdlr": "This work introduces a method called selective layer finetuning with SubTuning, where only a carefully chosen subset of layers is finetuned while the rest of the weights are frozen. It achieves comparable accuracy to full finetuning and performs even better when training data is scarce. This simple and effective method is suitable for multi-task learning and allows resource sharing among tasks during inference."
}