{
    "title": "Greedy Ordering of Layer Weight Matrices in Transformers Improves Translation. (arXiv:2302.02123v3 [cs.CL] UPDATED)",
    "abstract": "Prior work has attempted to understand the internal structures and functionalities of Transformer-based encoder-decoder architectures on the level of multi-head attention and feed-forward sublayers. Interpretations have focused on the encoder and decoder, along with the combinatorial possibilities of the self-attention, cross-attention, and feed-forward sublayers. However, without examining the low-level structures, one gains limited understanding of the motivation behind sublayer reordering. Could we dive into the sublayer abstraction and permute layer weight matrices to improve the quality of translation? We propose AEIUOrder to greedily reorder layer weight matrices in the encoder by their well-trainedness, as measured by Heavy-Tailed Self-Regularization (HT-SR) metrics, and order the decoder matrices correspondingly. Our results suggest that greedily reordering layer weight matrices to maximize Total well-trainedness facilitates the model to learn representations and generate trans",
    "link": "http://arxiv.org/abs/2302.02123",
    "context": "Title: Greedy Ordering of Layer Weight Matrices in Transformers Improves Translation. (arXiv:2302.02123v3 [cs.CL] UPDATED)\nAbstract: Prior work has attempted to understand the internal structures and functionalities of Transformer-based encoder-decoder architectures on the level of multi-head attention and feed-forward sublayers. Interpretations have focused on the encoder and decoder, along with the combinatorial possibilities of the self-attention, cross-attention, and feed-forward sublayers. However, without examining the low-level structures, one gains limited understanding of the motivation behind sublayer reordering. Could we dive into the sublayer abstraction and permute layer weight matrices to improve the quality of translation? We propose AEIUOrder to greedily reorder layer weight matrices in the encoder by their well-trainedness, as measured by Heavy-Tailed Self-Regularization (HT-SR) metrics, and order the decoder matrices correspondingly. Our results suggest that greedily reordering layer weight matrices to maximize Total well-trainedness facilitates the model to learn representations and generate trans",
    "path": "papers/23/02/2302.02123.json",
    "total_tokens": 882,
    "translated_title": "基于Transformers的贪婪排序优化翻译性能",
    "translated_abstract": "先前的工作试图在多头注意力和前馈子图层的层次上理解基于Transformer的编码器-解码器架构的内部结构和功能。但是，如果不检查低层次的结构，就不能深入理解子层重排背后的动机。本文通过AEIUOrder算法，通过衡量经过充分训练的层的Heavy-Tailed Self-Regularization（HT-SR）指标，贪婪地对编码器中的层重量矩阵进行重新排序，然后相应地排序解码器矩阵。我们的结果表明，通过最大化总的经过充分训练的层所贡献的\"well-trainedness\"指标进行重排，可以更好地学习表示，并生成更高质量的翻译输出，在各种翻译任务上达到了最好的性能。",
    "tldr": "本论文提出了一种基于贪婪重排权重矩阵的算法AEIUOrder，能够最大化总的经过充分训练的层所贡献的\"well-trainedness\"指标进行优化，从而提高翻译质量并在各种翻译任务上达到最佳性能。",
    "en_tdlr": "This paper proposes the AEIUOrder algorithm which greedily reorders layer weight matrices in the encoder by their well-trainedness, to maximize Total well-trainedness, thus improving translation quality and achieving state-of-the-art performance on various translation tasks."
}