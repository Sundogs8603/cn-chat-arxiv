{
    "title": "Equivariant Polynomials for Graph Neural Networks. (arXiv:2302.11556v2 [cs.LG] UPDATED)",
    "abstract": "Graph Neural Networks (GNN) are inherently limited in their expressive power. Recent seminal works (Xu et al., 2019; Morris et al., 2019b) introduced the Weisfeiler-Lehman (WL) hierarchy as a measure of expressive power. Although this hierarchy has propelled significant advances in GNN analysis and architecture developments, it suffers from several significant limitations. These include a complex definition that lacks direct guidance for model improvement and a WL hierarchy that is too coarse to study current GNNs. This paper introduces an alternative expressive power hierarchy based on the ability of GNNs to calculate equivariant polynomials of a certain degree. As a first step, we provide a full characterization of all equivariant graph polynomials by introducing a concrete basis, significantly generalizing previous results. Each basis element corresponds to a specific multi-graph, and its computation over some graph data input corresponds to a tensor contraction problem. Second, we ",
    "link": "http://arxiv.org/abs/2302.11556",
    "context": "Title: Equivariant Polynomials for Graph Neural Networks. (arXiv:2302.11556v2 [cs.LG] UPDATED)\nAbstract: Graph Neural Networks (GNN) are inherently limited in their expressive power. Recent seminal works (Xu et al., 2019; Morris et al., 2019b) introduced the Weisfeiler-Lehman (WL) hierarchy as a measure of expressive power. Although this hierarchy has propelled significant advances in GNN analysis and architecture developments, it suffers from several significant limitations. These include a complex definition that lacks direct guidance for model improvement and a WL hierarchy that is too coarse to study current GNNs. This paper introduces an alternative expressive power hierarchy based on the ability of GNNs to calculate equivariant polynomials of a certain degree. As a first step, we provide a full characterization of all equivariant graph polynomials by introducing a concrete basis, significantly generalizing previous results. Each basis element corresponds to a specific multi-graph, and its computation over some graph data input corresponds to a tensor contraction problem. Second, we ",
    "path": "papers/23/02/2302.11556.json",
    "total_tokens": 1130,
    "translated_title": "图神经网络的等变多项式",
    "translated_abstract": "图神经网络(GNN)在其表达能力上存在一定限制。最近的重要工作(Xu等，2019；Morris等，2019b)引入了Weisfeiler-Lehman(WL)层次结构作为表达能力的度量标准。虽然这个层次结构推动了GNN分析和架构发展上的显著进展，但它存在着一些显著的限制。其中包括一个复杂的定义，缺乏指导模型改进的直接指导以及一个过于粗糙无法研究当前GNN的WL层次结构。本文介绍了一种基于GNN能够计算特定次数的等变多项式的表达能力层次结构的替代方法。首先，我们通过引入一个具体的基底，显著推广了以前的结果，提供了所有等变图多项式的全面刻画。每个基底元素对应于一个特定的多图，其在某些图数据输入上的计算对应于一个张量收缩问题。其次，我们利用这些等变多项式来定义新的表达能力度量标准，扩展了WL层次结构。我们的度量标准更易于计算，并提供了更精细的信息，可以指导模型改进。最后，我们通过设计和分析新的GNN架构来证明我们方法的有用性，在多个基准数据集上超越了现有的最先进模型。",
    "tldr": "本文提出了一种基于等变多项式的表达能力层次结构，可以更好的指导GNN的模型改进。通过定义一个具体的基底，全面刻画了所有等变图多项式。此外，我们设计和分析了新的GNN架构，在多个基准数据集上超越了现有的最先进的模型。",
    "en_tdlr": "This paper introduces a novel expressive power hierarchy for Graph Neural Networks based on equivariant polynomials. It provides a full characterization of all equivariant graph polynomials by introducing a concrete basis and extends the Weisfeiler-Lehman hierarchy. The approach is demonstrated to be useful through the improved performance of new GNN architectures on multiple benchmark datasets."
}