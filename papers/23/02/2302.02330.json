{
    "title": "CIPER: Combining Invariant and Equivariant Representations Using Contrastive and Predictive Learning. (arXiv:2302.02330v2 [cs.CV] UPDATED)",
    "abstract": "Self-supervised representation learning (SSRL) methods have shown great success in computer vision. In recent studies, augmentation-based contrastive learning methods have been proposed for learning representations that are invariant or equivariant to pre-defined data augmentation operations. However, invariant or equivariant features favor only specific downstream tasks depending on the augmentations chosen. They may result in poor performance when the learned representation does not match task requirements. Here, we consider an active observer that can manipulate views of an object and has knowledge of the action(s) that generated each view. We introduce Contrastive Invariant and Predictive Equivariant Representation learning (CIPER). CIPER comprises both invariant and equivariant learning objectives using one shared encoder and two different output heads on top of the encoder. One output head is a projection head with a state-of-the-art contrastive objective to encourage invariance ",
    "link": "http://arxiv.org/abs/2302.02330",
    "context": "Title: CIPER: Combining Invariant and Equivariant Representations Using Contrastive and Predictive Learning. (arXiv:2302.02330v2 [cs.CV] UPDATED)\nAbstract: Self-supervised representation learning (SSRL) methods have shown great success in computer vision. In recent studies, augmentation-based contrastive learning methods have been proposed for learning representations that are invariant or equivariant to pre-defined data augmentation operations. However, invariant or equivariant features favor only specific downstream tasks depending on the augmentations chosen. They may result in poor performance when the learned representation does not match task requirements. Here, we consider an active observer that can manipulate views of an object and has knowledge of the action(s) that generated each view. We introduce Contrastive Invariant and Predictive Equivariant Representation learning (CIPER). CIPER comprises both invariant and equivariant learning objectives using one shared encoder and two different output heads on top of the encoder. One output head is a projection head with a state-of-the-art contrastive objective to encourage invariance ",
    "path": "papers/23/02/2302.02330.json",
    "total_tokens": 833,
    "translated_title": "CIPER: 使用对比学习和预测学习结合不变和等变表示",
    "translated_abstract": "自监督表示学习方法在计算机视觉领域取得了巨大的成功。最近的研究中，基于增强对比学习方法已经被提出用于学习对预定义的数据增强操作具有不变性或等变性的表示。然而，具有不变或等变特征仅适用于特定的下游任务，取决于所选择的增强方式。当学到的表示不符合任务要求时，可能会导致性能较差。在这里，我们考虑一个能够操作对象视图并知道生成每个视图的动作的主动观察者。我们引入了对比不变和预测等变表示学习（CIPER）。CIPER包括使用一个共享编码器和两个不同的输出头部的不变和等变学习目标。一个输出头部是一个具有最先进对比目标的投影头部，以鼓励不变性",
    "tldr": "CIPER是一个自监督表示学习方法，通过结合不变和等变学习目标来提高性能，适用于具有特定数据增强要求的计算机视觉任务",
    "en_tdlr": "CIPER is a self-supervised representation learning method that improves performance by combining invariant and equivariant learning objectives, suitable for computer vision tasks with specific data augmentation requirements."
}