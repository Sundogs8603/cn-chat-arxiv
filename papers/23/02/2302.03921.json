{
    "title": "Predictable MDP Abstraction for Unsupervised Model-Based RL. (arXiv:2302.03921v2 [cs.LG] UPDATED)",
    "abstract": "A key component of model-based reinforcement learning (RL) is a dynamics model that predicts the outcomes of actions. Errors in this predictive model can degrade the performance of model-based controllers, and complex Markov decision processes (MDPs) can present exceptionally difficult prediction problems. To mitigate this issue, we propose predictable MDP abstraction (PMA): instead of training a predictive model on the original MDP, we train a model on a transformed MDP with a learned action space that only permits predictable, easy-to-model actions, while covering the original state-action space as much as possible. As a result, model learning becomes easier and more accurate, which allows robust, stable model-based planning or model-based RL. This transformation is learned in an unsupervised manner, before any task is specified by the user. Downstream tasks can then be solved with model-based control in a zero-shot fashion, without additional environment interactions. We theoretical",
    "link": "http://arxiv.org/abs/2302.03921",
    "context": "Title: Predictable MDP Abstraction for Unsupervised Model-Based RL. (arXiv:2302.03921v2 [cs.LG] UPDATED)\nAbstract: A key component of model-based reinforcement learning (RL) is a dynamics model that predicts the outcomes of actions. Errors in this predictive model can degrade the performance of model-based controllers, and complex Markov decision processes (MDPs) can present exceptionally difficult prediction problems. To mitigate this issue, we propose predictable MDP abstraction (PMA): instead of training a predictive model on the original MDP, we train a model on a transformed MDP with a learned action space that only permits predictable, easy-to-model actions, while covering the original state-action space as much as possible. As a result, model learning becomes easier and more accurate, which allows robust, stable model-based planning or model-based RL. This transformation is learned in an unsupervised manner, before any task is specified by the user. Downstream tasks can then be solved with model-based control in a zero-shot fashion, without additional environment interactions. We theoretical",
    "path": "papers/23/02/2302.03921.json",
    "total_tokens": 951,
    "translated_title": "可预测的MDP抽象用于无监督的基于模型的强化学习",
    "translated_abstract": "模型化强化学习（RL）的一个关键组件是一个能预测行动结果的动态模型。预测模型中的错误会降低模型化控制器的性能，复杂的马尔可夫决策过程（MDPs）可能会带来极其困难的预测问题。为了缓解这个问题，我们提出了可预测的MDP抽象（PMA）：不是在原始MDP上训练预测模型，而是在一个转换后的具有学习行动空间的MDP上训练模型，该行动空间只允许可预测、易建模的行动，同时尽可能地覆盖原始状态-行动空间。结果是，模型学习变得更加容易和准确，这允许鲁棒、稳定的基于模型的规划或基于模型的RL。这种转换是以无监督的方式学习的，在用户指定任何任务之前。随后，下游任务可以以零-shot方式通过模型化控制解决，而无需额外的环境交互。我们从理论上证明了方法的有效性，并在多个任务上进行了实验验证。",
    "tldr": "该论文提出了可预测的MDP抽象方法，通过无监督学习将原始MDP转换为学习行动空间，使模型学习变得更加准确和稳定，在多项任务上得到了验证。",
    "en_tdlr": "This paper proposes the predictable MDP abstraction method, which transforms the original MDP into a learned action space through unsupervised learning to make the model learning more accurate and stable, and the effectiveness of the approach is theoretically proved and experimentally verified on multiple tasks."
}