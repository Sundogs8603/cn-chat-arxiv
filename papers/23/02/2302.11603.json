{
    "title": "Some Might Say All You Need Is Sum. (arXiv:2302.11603v2 [cs.LG] UPDATED)",
    "abstract": "The expressivity of Graph Neural Networks (GNNs) is dependent on the aggregation functions they employ. Theoretical works have pointed towards Sum aggregation GNNs subsuming every other GNNs, while certain practical works have observed a clear advantage to using Mean and Max. An examination of the theoretical guarantee identifies two caveats. First, it is size-restricted, that is, the power of every specific GNN is limited to graphs of a specific size. Successfully processing larger graphs may require an other GNN, and so on. Second, it concerns the power to distinguish non-isomorphic graphs, not the power to approximate general functions on graphs, and the former does not necessarily imply the latter.  It is desired that a GNN's usability will not be limited to graphs of any specific size. Therefore, we explore the realm of unrestricted-size expressivity. We prove that basic functions, which can be computed exactly by Mean or Max GNNs, are inapproximable by any Sum GNN. We prove that ",
    "link": "http://arxiv.org/abs/2302.11603",
    "context": "Title: Some Might Say All You Need Is Sum. (arXiv:2302.11603v2 [cs.LG] UPDATED)\nAbstract: The expressivity of Graph Neural Networks (GNNs) is dependent on the aggregation functions they employ. Theoretical works have pointed towards Sum aggregation GNNs subsuming every other GNNs, while certain practical works have observed a clear advantage to using Mean and Max. An examination of the theoretical guarantee identifies two caveats. First, it is size-restricted, that is, the power of every specific GNN is limited to graphs of a specific size. Successfully processing larger graphs may require an other GNN, and so on. Second, it concerns the power to distinguish non-isomorphic graphs, not the power to approximate general functions on graphs, and the former does not necessarily imply the latter.  It is desired that a GNN's usability will not be limited to graphs of any specific size. Therefore, we explore the realm of unrestricted-size expressivity. We prove that basic functions, which can be computed exactly by Mean or Max GNNs, are inapproximable by any Sum GNN. We prove that ",
    "path": "papers/23/02/2302.11603.json",
    "total_tokens": 881,
    "tldr": "本文研究了图神经网络的表现取决于聚合函数，证明了使用Sum GNN无法逼近基本函数，探索了无限制大小表达能力的领域。",
    "en_tdlr": "This paper explores the expressivity of Graph Neural Networks (GNNs) and the limitations of using Sum aggregation GNNs, proving that basic functions cannot be approximated by them and exploring the realm of unrestricted-size expressivity."
}