{
    "title": "Large language models predict human sensory judgments across six modalities. (arXiv:2302.01308v2 [cs.CL] UPDATED)",
    "abstract": "Determining the extent to which the perceptual world can be recovered from language is a longstanding problem in philosophy and cognitive science. We show that state-of-the-art large language models can unlock new insights into this problem by providing a lower bound on the amount of perceptual information that can be extracted from language. Specifically, we elicit pairwise similarity judgments from GPT models across six psychophysical datasets. We show that the judgments are significantly correlated with human data across all domains, recovering well-known representations like the color wheel and pitch spiral. Surprisingly, we find that a model (GPT-4) co-trained on vision and language does not necessarily lead to improvements specific to the visual modality. To study the influence of specific languages on perception, we also apply the models to a multilingual color-naming task. We find that GPT-4 replicates cross-linguistic variation in English and Russian illuminating the interacti",
    "link": "http://arxiv.org/abs/2302.01308",
    "context": "Title: Large language models predict human sensory judgments across six modalities. (arXiv:2302.01308v2 [cs.CL] UPDATED)\nAbstract: Determining the extent to which the perceptual world can be recovered from language is a longstanding problem in philosophy and cognitive science. We show that state-of-the-art large language models can unlock new insights into this problem by providing a lower bound on the amount of perceptual information that can be extracted from language. Specifically, we elicit pairwise similarity judgments from GPT models across six psychophysical datasets. We show that the judgments are significantly correlated with human data across all domains, recovering well-known representations like the color wheel and pitch spiral. Surprisingly, we find that a model (GPT-4) co-trained on vision and language does not necessarily lead to improvements specific to the visual modality. To study the influence of specific languages on perception, we also apply the models to a multilingual color-naming task. We find that GPT-4 replicates cross-linguistic variation in English and Russian illuminating the interacti",
    "path": "papers/23/02/2302.01308.json",
    "total_tokens": 895,
    "translated_title": "大型语言模型可以预测人类在六个感官模态下的感知评判",
    "translated_abstract": "确定从语言中可以恢复感知世界的程度是哲学和认知科学中长期存在的问题。本研究展示了，最先进的大型语言模型通过提供从语言中提取感知信息的下限，可以为解决这个问题提供新的见解。具体而言，我们从GPT模型中引出了六个心理物理数据集的成对相似度评估结果。我们发现这些评估结果在所有领域中均与人类数据显著相关，回复了众所周知的表现，如颜色环和音高螺旋。令人惊讶的是，我们发现一个在视觉和语言上共同训练的模型（GPT-4）并不一定会导致对视觉模态的特定改进。为了研究特定语言对感知的影响，我们还将这些模型应用于多语言颜色命名任务。我们发现，GPT-4在英语和俄语中复制了跨语言差异，阐明了它们之间的相互作用。",
    "tldr": "本研究表明，最先进的大型语言模型能预测人类在六个感官模态下的感知评判，并能提供从语言中提取感知信息的下限。",
    "en_tdlr": "This study shows that state-of-the-art large language models can predict human sensory judgments across six modalities, providing a lower bound on the amount of perceptual information that can be extracted from language."
}