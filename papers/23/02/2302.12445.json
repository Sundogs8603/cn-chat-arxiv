{
    "title": "DeAR: Accelerating Distributed Deep Learning with Fine-Grained All-Reduce Pipelining. (arXiv:2302.12445v2 [cs.LG] UPDATED)",
    "abstract": "Communication scheduling has been shown to be effective in accelerating distributed training, which enables all-reduce communications to be overlapped with backpropagation computations. This has been commonly adopted in popular distributed deep learning frameworks. However, there exist two fundamental problems: (1) excessive startup latency proportional to the number of workers for each all-reduce operation; (2) it only achieves sub-optimal training performance due to the dependency and synchronization requirement of the feed-forward computation in the next iteration. We propose a novel scheduling algorithm, DeAR, that decouples the all-reduce primitive into two continuous operations, which overlaps with both backpropagation and feed-forward computations without extra communications. We further design a practical tensor fusion algorithm to improve the training performance. Experimental results with five popular models show that DeAR achieves up to 83% and 15% training speedup over the ",
    "link": "http://arxiv.org/abs/2302.12445",
    "context": "Title: DeAR: Accelerating Distributed Deep Learning with Fine-Grained All-Reduce Pipelining. (arXiv:2302.12445v2 [cs.LG] UPDATED)\nAbstract: Communication scheduling has been shown to be effective in accelerating distributed training, which enables all-reduce communications to be overlapped with backpropagation computations. This has been commonly adopted in popular distributed deep learning frameworks. However, there exist two fundamental problems: (1) excessive startup latency proportional to the number of workers for each all-reduce operation; (2) it only achieves sub-optimal training performance due to the dependency and synchronization requirement of the feed-forward computation in the next iteration. We propose a novel scheduling algorithm, DeAR, that decouples the all-reduce primitive into two continuous operations, which overlaps with both backpropagation and feed-forward computations without extra communications. We further design a practical tensor fusion algorithm to improve the training performance. Experimental results with five popular models show that DeAR achieves up to 83% and 15% training speedup over the ",
    "path": "papers/23/02/2302.12445.json",
    "total_tokens": 914,
    "translated_title": "DeAR：细粒度All-Reduce管道加速分布式深度学习",
    "translated_abstract": "通信调度在加速分布式训练中已被证明是有效的，它使得All-Reduce通信与反向传播计算重叠。这在流行的分布式深度学习框架中已被广泛采用。然而，存在两个基本问题：（1）针对每个All-Reduce操作，启动延迟与工作节点数成正比；（2）由于下一次迭代中前向计算的依赖和同步要求，它仅能实现次优的训练性能。我们提出了一种新的调度算法DeAR，将All-Reduce原语分解成两个连续操作，它们与反向传播和前向计算同时重叠而无需额外通信。我们还设计了一个实用的张量融合算法来提高训练性能。使用五种流行的模型进行的实验结果表明，DeAR在训练速度上实现了多达83%和15%的加速。",
    "tldr": "DeAR提出了一种新的调度算法，将All-Reduce原语分解成两个连续操作，与反向传播和前向计算同时重叠。使用实用的张量融合算法可以提高训练性能，实验结果表明DeAR在训练速度上可达到83%和15%的加速。",
    "en_tdlr": "DeAR proposes a novel scheduling algorithm that decomposes the All-Reduce primitive into two continuous operations, overlapping them with both backpropagation and feed-forward computations without extra communications. A practical tensor fusion algorithm is also designed to improve training performance, which achieves up to 83% and 15% training speedup according to experimental results with five popular models."
}