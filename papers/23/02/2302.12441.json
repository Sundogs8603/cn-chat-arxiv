{
    "title": "MUX-PLMs: Data Multiplexing for High-throughput Language Models. (arXiv:2302.12441v2 [cs.LG] UPDATED)",
    "abstract": "The widespread adoption of large language models such as ChatGPT and Bard has led to unprecedented demand for these technologies. The burgeoning cost of inference for ever-increasing model sizes coupled with hardware shortages has limited affordable access and poses a pressing need for efficiency approaches geared towards high throughput and performance. Multi-input multi-output (MIMO) algorithms such as data multiplexing, offer a promising solution with a many-fold increase in throughput by performing inference for multiple inputs at the cost of a single input. Yet these approaches are not currently performant enough to be deployed in modern systems. We change that by developing MUX-PLMs, a class of high throughput pre-trained language models (PLMs) trained with data multiplexing, that can be fine-tuned for any downstream task to yield high-throughput high-performance. Our novel multiplexing and demultiplexing modules proficiently entangle and disentangle inputs, and enable high-perfo",
    "link": "http://arxiv.org/abs/2302.12441",
    "context": "Title: MUX-PLMs: Data Multiplexing for High-throughput Language Models. (arXiv:2302.12441v2 [cs.LG] UPDATED)\nAbstract: The widespread adoption of large language models such as ChatGPT and Bard has led to unprecedented demand for these technologies. The burgeoning cost of inference for ever-increasing model sizes coupled with hardware shortages has limited affordable access and poses a pressing need for efficiency approaches geared towards high throughput and performance. Multi-input multi-output (MIMO) algorithms such as data multiplexing, offer a promising solution with a many-fold increase in throughput by performing inference for multiple inputs at the cost of a single input. Yet these approaches are not currently performant enough to be deployed in modern systems. We change that by developing MUX-PLMs, a class of high throughput pre-trained language models (PLMs) trained with data multiplexing, that can be fine-tuned for any downstream task to yield high-throughput high-performance. Our novel multiplexing and demultiplexing modules proficiently entangle and disentangle inputs, and enable high-perfo",
    "path": "papers/23/02/2302.12441.json",
    "total_tokens": 879,
    "translated_title": "MUX-PLMs：高吞吐量语言模型的数据复用",
    "translated_abstract": "大型语言模型（如ChatGPT和Bard）的广泛采用带来了前所未有的需求。越来越大的模型尺寸所需的推断成本以及硬件短缺，限制了经济实惠的访问，并提出了针对高吞吐量和高性能的效率方法的迫切需求。多输入多输出（MIMO）算法（例如数据复用）通过对多个输入执行推断，以单个输入的成本提供了多重吞吐量的有前途的解决方案。然而，这些方法目前的表现还不足以部署在现代系统中。我们通过开发MUX-PLMs，一种使用数据复用训练的高吞吐量预训练语言模型（PLMs），可以微调任何下游任务以产生高吞吐量和高性能。我们的新型复用和解复用模块能够有效地纠缠和解缠输入，并实现高性能的MIMO样式语言模型推断。",
    "tldr": "该论文开发了一种名为MUX-PLMs的高吞吐量预训练语言模型，使用数据复用训练，可用于高性能的MIMO样式语言模型推断。",
    "en_tdlr": "The paper develops high throughput pre-trained language models called MUX-PLMs that are trained with data multiplexing, and can be used for high-performance MIMO-style language model inference."
}