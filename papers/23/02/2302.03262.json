{
    "title": "Membership Inference Attacks against Diffusion Models. (arXiv:2302.03262v2 [cs.CR] UPDATED)",
    "abstract": "Diffusion models have attracted attention in recent years as innovative generative models. In this paper, we investigate whether a diffusion model is resistant to a membership inference attack, which evaluates the privacy leakage of a machine learning model. We primarily discuss the diffusion model from the standpoints of comparison with a generative adversarial network (GAN) as conventional models and hyperparameters unique to the diffusion model, i.e., time steps, sampling steps, and sampling variances. We conduct extensive experiments with DDIM as a diffusion model and DCGAN as a GAN on the CelebA and CIFAR-10 datasets in both white-box and black-box settings and then confirm if the diffusion model is comparably resistant to a membership inference attack as GAN. Next, we demonstrate that the impact of time steps is significant and intermediate steps in a noise schedule are the most vulnerable to the attack. We also found two key insights through further analysis. First, we identify ",
    "link": "http://arxiv.org/abs/2302.03262",
    "context": "Title: Membership Inference Attacks against Diffusion Models. (arXiv:2302.03262v2 [cs.CR] UPDATED)\nAbstract: Diffusion models have attracted attention in recent years as innovative generative models. In this paper, we investigate whether a diffusion model is resistant to a membership inference attack, which evaluates the privacy leakage of a machine learning model. We primarily discuss the diffusion model from the standpoints of comparison with a generative adversarial network (GAN) as conventional models and hyperparameters unique to the diffusion model, i.e., time steps, sampling steps, and sampling variances. We conduct extensive experiments with DDIM as a diffusion model and DCGAN as a GAN on the CelebA and CIFAR-10 datasets in both white-box and black-box settings and then confirm if the diffusion model is comparably resistant to a membership inference attack as GAN. Next, we demonstrate that the impact of time steps is significant and intermediate steps in a noise schedule are the most vulnerable to the attack. We also found two key insights through further analysis. First, we identify ",
    "path": "papers/23/02/2302.03262.json",
    "total_tokens": 943,
    "translated_title": "扩散模型的成员推断攻击",
    "translated_abstract": "近年来，扩散模型作为创新的生成模型引起了人们的关注。本文研究了扩散模型是否能够抵抗成员推断攻击，以评估机器学习模型的隐私泄露。我们主要从与传统模型生成对抗网络（GAN）的比较和扩散模型独特的超参数，即时间步长、采样步长和采样方差的角度讨论了扩散模型。我们在CelebA和CIFAR-10数据集上使用DDIM作为扩散模型、DCGAN作为GAN进行了广泛的白盒和黑盒实验，然后确认扩散模型是否能够像GAN一样具有抵抗成员推断攻击的能力。接下来，我们证明时间步长的影响显著，并且在噪声计划中的中间步骤最容易受到攻击。通过进一步的分析，我们还发现了两个关键见解。首先，我们确定了攻击的位置。其次，在攻击中加入噪声会显著降低攻击的成功率。",
    "tldr": "本文研究了扩散模型是否能够抵抗成员推断攻击，并在多个数据集上进行了实验证明其易受攻击影响，尤其是在时间步骤的中间步骤。通过引入噪声，攻击的成功率可以显著降低。"
}