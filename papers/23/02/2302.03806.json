{
    "title": "SLaM: Student-Label Mixing for Distillation with Unlabeled Examples. (arXiv:2302.03806v2 [cs.LG] UPDATED)",
    "abstract": "Knowledge distillation with unlabeled examples is a powerful training paradigm for generating compact and lightweight student models in applications where the amount of labeled data is limited but one has access to a large pool of unlabeled data. In this setting, a large teacher model generates ``soft'' pseudo-labels for the unlabeled dataset which are then used for training the student model. Despite its success in a wide variety of applications, a shortcoming of this approach is that the teacher's pseudo-labels are often noisy, leading to impaired student performance. In this paper, we present a principled method for knowledge distillation with unlabeled examples that we call Student-Label Mixing (SLaM) and we show that it consistently improves over prior approaches by evaluating it on several standard benchmarks. Finally, we show that SLaM comes with theoretical guarantees; along the way we give an algorithm improving the best-known sample complexity for learning halfspaces with mar",
    "link": "http://arxiv.org/abs/2302.03806",
    "context": "Title: SLaM: Student-Label Mixing for Distillation with Unlabeled Examples. (arXiv:2302.03806v2 [cs.LG] UPDATED)\nAbstract: Knowledge distillation with unlabeled examples is a powerful training paradigm for generating compact and lightweight student models in applications where the amount of labeled data is limited but one has access to a large pool of unlabeled data. In this setting, a large teacher model generates ``soft'' pseudo-labels for the unlabeled dataset which are then used for training the student model. Despite its success in a wide variety of applications, a shortcoming of this approach is that the teacher's pseudo-labels are often noisy, leading to impaired student performance. In this paper, we present a principled method for knowledge distillation with unlabeled examples that we call Student-Label Mixing (SLaM) and we show that it consistently improves over prior approaches by evaluating it on several standard benchmarks. Finally, we show that SLaM comes with theoretical guarantees; along the way we give an algorithm improving the best-known sample complexity for learning halfspaces with mar",
    "path": "papers/23/02/2302.03806.json",
    "total_tokens": 938,
    "translated_title": "SLaM: 用未标注样本的学生-标签混合进行蒸馏",
    "translated_abstract": "未标注样本的知识蒸馏是一种强大的训练范式，用于在标记数据有限但可以访问大量未标注数据的应用中生成紧凑、轻量级的学生模型。在这种设置中，一个大的教师模型为未标记的数据集生成“软”伪标签，然后用于训练学生模型。尽管在各种应用中都取得了成功，但这种方法的一个缺点是教师的伪标签经常是带有噪声的，从而导致学生性能受到影响。在本文中，我们提出了一种名为学生-标签混合（SLaM）的未标注示例知识蒸馏的原则方法，并展示了它在评估几个标准基准时始终优于先前的方法。最后，我们展示SLaM具有理论保证；在此过程中，我们给出了一种算法，改进了半空间学习的最佳已知样本复杂度。",
    "tldr": "这篇论文介绍了一种名为SLaM的方法，用于解决知识蒸馏中教师伪标签噪声带来的问题。SLaM使用学生-标签混合的方式来生成更准确的标签，并在多个标准基准上取得了更好的性能。",
    "en_tdlr": "This paper presents a principled method called SLaM for knowledge distillation with unlabeled examples, which improves upon previous approaches by addressing the issue of noisy teacher pseudo-labels. SLaM uses student-label mixing to generate more accurate labels and achieves better performance on multiple standard benchmarks."
}