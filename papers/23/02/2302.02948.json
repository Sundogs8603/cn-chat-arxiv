{
    "title": "Efficient Online Reinforcement Learning with Offline Data. (arXiv:2302.02948v4 [cs.LG] UPDATED)",
    "abstract": "Sample efficiency and exploration remain major challenges in online reinforcement learning (RL). A powerful approach that can be applied to address these issues is the inclusion of offline data, such as prior trajectories from a human expert or a sub-optimal exploration policy. Previous methods have relied on extensive modifications and additional complexity to ensure the effective use of this data. Instead, we ask: can we simply apply existing off-policy methods to leverage offline data when learning online? In this work, we demonstrate that the answer is yes; however, a set of minimal but important changes to existing off-policy RL algorithms are required to achieve reliable performance. We extensively ablate these design choices, demonstrating the key factors that most affect performance, and arrive at a set of recommendations that practitioners can readily apply, whether their data comprise a small number of expert demonstrations or large volumes of sub-optimal trajectories. We see",
    "link": "http://arxiv.org/abs/2302.02948",
    "context": "Title: Efficient Online Reinforcement Learning with Offline Data. (arXiv:2302.02948v4 [cs.LG] UPDATED)\nAbstract: Sample efficiency and exploration remain major challenges in online reinforcement learning (RL). A powerful approach that can be applied to address these issues is the inclusion of offline data, such as prior trajectories from a human expert or a sub-optimal exploration policy. Previous methods have relied on extensive modifications and additional complexity to ensure the effective use of this data. Instead, we ask: can we simply apply existing off-policy methods to leverage offline data when learning online? In this work, we demonstrate that the answer is yes; however, a set of minimal but important changes to existing off-policy RL algorithms are required to achieve reliable performance. We extensively ablate these design choices, demonstrating the key factors that most affect performance, and arrive at a set of recommendations that practitioners can readily apply, whether their data comprise a small number of expert demonstrations or large volumes of sub-optimal trajectories. We see",
    "path": "papers/23/02/2302.02948.json",
    "total_tokens": 868,
    "translated_title": "利用离线数据进行高效在线强化学习",
    "translated_abstract": "样本效率和探索仍然是在线强化学习中的主要挑战。一种强有力的方法是包括离线数据，如来自人类专家或次优探索策略的先前轨迹。以前的方法依赖于广泛的修改和额外的复杂性来确保有效使用这些数据。相反，我们想问：我们是否可以简单地应用现有的离线策略方法来利用离线数据进行在线学习？在这项工作中，我们证明了答案是肯定的。但是，为了实现可靠的性能，需要对现有的离线策略强化学习算法进行一些最少但重要的更改。我们广泛地测试了这些设计选择，并展示了对性能影响最大的关键因素，得出了一组实践者可以轻松应用的建议，无论其数据包括少量专家演示或大量次优轨迹。",
    "tldr": "本文研究了利用离线数据进行高效在线强化学习的方法，证明了现有的离线策略方法能够应用于在线学习，提出了一些最少但重要的更改，来实现可靠的性能并提供了实践中可应用的建议。"
}