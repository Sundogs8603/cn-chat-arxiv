{
    "title": "Convolutional Neural Operators for robust and accurate learning of PDEs. (arXiv:2302.01178v2 [cs.LG] UPDATED)",
    "abstract": "Although very successfully used in conventional machine learning, convolution based neural network architectures -- believed to be inconsistent in function space -- have been largely ignored in the context of learning solution operators of PDEs. Here, we present novel adaptations for convolutional neural networks to demonstrate that they are indeed able to process functions as inputs and outputs. The resulting architecture, termed as convolutional neural operators (CNOs), is designed specifically to preserve its underlying continuous nature, even when implemented in a discretized form on a computer. We prove a universality theorem to show that CNOs can approximate operators arising in PDEs to desired accuracy. CNOs are tested on a novel suite of benchmarks, encompassing a diverse set of PDEs with possibly multi-scale solutions and are observed to significantly outperform baselines, paving the way for an alternative framework for robust and accurate operator learning.",
    "link": "http://arxiv.org/abs/2302.01178",
    "context": "Title: Convolutional Neural Operators for robust and accurate learning of PDEs. (arXiv:2302.01178v2 [cs.LG] UPDATED)\nAbstract: Although very successfully used in conventional machine learning, convolution based neural network architectures -- believed to be inconsistent in function space -- have been largely ignored in the context of learning solution operators of PDEs. Here, we present novel adaptations for convolutional neural networks to demonstrate that they are indeed able to process functions as inputs and outputs. The resulting architecture, termed as convolutional neural operators (CNOs), is designed specifically to preserve its underlying continuous nature, even when implemented in a discretized form on a computer. We prove a universality theorem to show that CNOs can approximate operators arising in PDEs to desired accuracy. CNOs are tested on a novel suite of benchmarks, encompassing a diverse set of PDEs with possibly multi-scale solutions and are observed to significantly outperform baselines, paving the way for an alternative framework for robust and accurate operator learning.",
    "path": "papers/23/02/2302.01178.json",
    "total_tokens": 960,
    "translated_title": "卷积神经算子用于偏微分方程的鲁棒准确学习",
    "translated_abstract": "尽管在传统机器学习中非常成功，基于卷积的神经网络架构在函数空间中存在不一致性，因此在学习偏微分方程的解算子的上下文中被广泛忽视。在这里，我们提出了卷积神经网络的新适应方法，以证明它们确实能够处理输入和输出的函数。结果产生的架构称为卷积神经算子（CNO），专门设计为在计算机上以离散形式实现时保持其基本的连续性质。我们证明了一个普适定理，以展示CNOs可以近似偏微分方程中出现的算子到所需的精度。 CNOs在一个包括各种具有可能具有多尺度解的偏微分方程的新套件上进行测试，并被观察到显着优于基线，为鲁棒准确操作学习的另一种框架铺平了道路。",
    "tldr": "该论文提出了卷积神经算子（CNO），它能够在学习偏微分方程的上下文中处理函数输入和输出，并且证明了 CNOs 可以近似偏微分方程中出现的算子到所需的精度。在测试中，CNOs 显着优于基线，这为鲁棒准确操作学习的另一种框架铺平了道路。",
    "en_tdlr": "This paper proposes Convolutional Neural Operators (CNOs) that can process function inputs and outputs to learn operators of partial differential equations (PDEs) accurately and robustly. CNOs are shown to approximate PDE operators to the desired accuracy and outperform baselines on a diverse set of PDE benchmarks with potentially multi-scale solutions."
}