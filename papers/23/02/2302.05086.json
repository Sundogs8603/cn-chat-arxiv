{
    "title": "Making Substitute Models More Bayesian Can Enhance Transferability of Adversarial Examples. (arXiv:2302.05086v3 [cs.LG] UPDATED)",
    "abstract": "The transferability of adversarial examples across deep neural networks (DNNs) is the crux of many black-box attacks. Many prior efforts have been devoted to improving the transferability via increasing the diversity in inputs of some substitute models. In this paper, by contrast, we opt for the diversity in substitute models and advocate to attack a Bayesian model for achieving desirable transferability. Deriving from the Bayesian formulation, we develop a principled strategy for possible finetuning, which can be combined with many off-the-shelf Gaussian posterior approximations over DNN parameters. Extensive experiments have been conducted to verify the effectiveness of our method, on common benchmark datasets, and the results demonstrate that our method outperforms recent state-of-the-arts by large margins (roughly 19% absolute increase in average attack success rate on ImageNet), and, by combining with these recent methods, further performance gain can be obtained. Our code: https:",
    "link": "http://arxiv.org/abs/2302.05086",
    "context": "Title: Making Substitute Models More Bayesian Can Enhance Transferability of Adversarial Examples. (arXiv:2302.05086v3 [cs.LG] UPDATED)\nAbstract: The transferability of adversarial examples across deep neural networks (DNNs) is the crux of many black-box attacks. Many prior efforts have been devoted to improving the transferability via increasing the diversity in inputs of some substitute models. In this paper, by contrast, we opt for the diversity in substitute models and advocate to attack a Bayesian model for achieving desirable transferability. Deriving from the Bayesian formulation, we develop a principled strategy for possible finetuning, which can be combined with many off-the-shelf Gaussian posterior approximations over DNN parameters. Extensive experiments have been conducted to verify the effectiveness of our method, on common benchmark datasets, and the results demonstrate that our method outperforms recent state-of-the-arts by large margins (roughly 19% absolute increase in average attack success rate on ImageNet), and, by combining with these recent methods, further performance gain can be obtained. Our code: https:",
    "path": "papers/23/02/2302.05086.json",
    "total_tokens": 888,
    "translated_title": "通过使替代模型更贝叶斯化，可以增强对抗性样本的可迁移性",
    "translated_abstract": "对抗性样本在深度神经网络之间的可迁移性是许多黑盒攻击的关键。以往的努力主要集中在提高一些替代模型输入的多样性上以改善可迁移性。相比之下，本文选择了替代模型的多样性，并提出攻击贝叶斯模型以实现理想的可迁移性。基于贝叶斯公式，我们提出了一种可行的精调策略，该策略可以与基于深度神经网络参数的许多常见高斯后验逼近方法相结合。通过大量实验证明了我们方法的有效性，这些实验证明我们方法在常见的基准数据集上优于最新的现有方法（在ImageNet上攻击成功率的平均绝对增加率约为19%），并且与这些最新方法的结合可以进一步提高性能。我们的代码: https:",
    "tldr": "本文通过使替代模型更贝叶斯化，提出了一种攻击贝叶斯模型以实现理想的对抗性样本可迁移性的方法，通过实验证明了该方法的有效性，并在常见基准数据集上优于最新方法。"
}