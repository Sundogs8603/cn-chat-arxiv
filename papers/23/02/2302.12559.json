{
    "title": "From Noisy Fixed-Point Iterations to Private ADMM for Centralized and Federated Learning. (arXiv:2302.12559v2 [cs.LG] UPDATED)",
    "abstract": "We study differentially private (DP) machine learning algorithms as instances of noisy fixed-point iterations, in order to derive privacy and utility results from this well-studied framework. We show that this new perspective recovers popular private gradient-based methods like DP-SGD and provides a principled way to design and analyze new private optimization algorithms in a flexible manner. Focusing on the widely-used Alternating Directions Method of Multipliers (ADMM) method, we use our general framework to derive novel private ADMM algorithms for centralized, federated and fully decentralized learning. For these three algorithms, we establish strong privacy guarantees leveraging privacy amplification by iteration and by subsampling. Finally, we provide utility guarantees using a unified analysis that exploits a recent linear convergence result for noisy fixed-point iterations.",
    "link": "http://arxiv.org/abs/2302.12559",
    "context": "Title: From Noisy Fixed-Point Iterations to Private ADMM for Centralized and Federated Learning. (arXiv:2302.12559v2 [cs.LG] UPDATED)\nAbstract: We study differentially private (DP) machine learning algorithms as instances of noisy fixed-point iterations, in order to derive privacy and utility results from this well-studied framework. We show that this new perspective recovers popular private gradient-based methods like DP-SGD and provides a principled way to design and analyze new private optimization algorithms in a flexible manner. Focusing on the widely-used Alternating Directions Method of Multipliers (ADMM) method, we use our general framework to derive novel private ADMM algorithms for centralized, federated and fully decentralized learning. For these three algorithms, we establish strong privacy guarantees leveraging privacy amplification by iteration and by subsampling. Finally, we provide utility guarantees using a unified analysis that exploits a recent linear convergence result for noisy fixed-point iterations.",
    "path": "papers/23/02/2302.12559.json",
    "total_tokens": 1050,
    "translated_title": "从有噪声的不动点迭代到私有的ADMM算法，用于集中式和联合学习",
    "translated_abstract": "我们研究差分隐私（DP）机器学习算法作为噪声固定点迭代的实例，以便从这个经过充分研究的框架中得出隐私和效用结果。我们展示了这一新的视角恢复了流行的私有梯度下降方法，如DP-SGD，并为设计和分析新的私有优化算法提供了一种有原则的方式。专注于广泛使用的交替方向乘子法（ADMM）方法，我们利用我们的通用框架推导出用于集中式、联合和完全去中心化学习的新颖私有ADMM算法。对于这三个算法，我们利用迭代和子采样的隐私放大建立了强隐私保证。最后，我们利用最近的有噪声固定点迭代的线性收敛结果进行统一分析，提供效用保证。",
    "tldr": "本论文研究了将差分隐私机器学习算法视为有噪声的不动点迭代，从而从这一框架中推导出隐私和效用结果。通过利用这个新的视角，我们恢复了流行的私有梯度下降方法并提供了一种有原则的方法来设计和分析新的私有优化算法。我们通过通用框架推导出了用于集中式、联合和完全去中心化学习的新颖私有ADMM算法，并通过迭代和子采样的隐私放大建立了强隐私保证。最后，我们利用最近的有噪声固定点迭代的线性收敛结果提供效用保证。",
    "en_tdlr": "This paper studies differentially private machine learning algorithms as noisy fixed-point iterations, deriving privacy and utility results from this well-established framework. By recovering popular private gradient-based methods and providing a principled approach to design and analyze new private optimization algorithms, the authors introduce novel private ADMM algorithms for centralized, federated, and decentralized learning. Strong privacy guarantees are established using privacy amplification by iteration and subsampling, while utility guarantees are based on a unified analysis leveraging recent linear convergence results for noisy fixed-point iterations."
}