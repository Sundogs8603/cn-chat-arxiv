{
    "title": "Fixing Overconfidence in Dynamic Neural Networks. (arXiv:2302.06359v3 [cs.LG] UPDATED)",
    "abstract": "Dynamic neural networks are a recent technique that promises a remedy for the increasing size of modern deep learning models by dynamically adapting their computational cost to the difficulty of the inputs. In this way, the model can adjust to a limited computational budget. However, the poor quality of uncertainty estimates in deep learning models makes it difficult to distinguish between hard and easy samples. To address this challenge, we present a computationally efficient approach for post-hoc uncertainty quantification in dynamic neural networks. We show that adequately quantifying and accounting for both aleatoric and epistemic uncertainty through a probabilistic treatment of the last layers improves the predictive performance and aids decision-making when determining the computational budget. In the experiments, we show improvements on CIFAR-100, ImageNet, and Caltech-256 in terms of accuracy, capturing uncertainty, and calibration error.",
    "link": "http://arxiv.org/abs/2302.06359",
    "context": "Title: Fixing Overconfidence in Dynamic Neural Networks. (arXiv:2302.06359v3 [cs.LG] UPDATED)\nAbstract: Dynamic neural networks are a recent technique that promises a remedy for the increasing size of modern deep learning models by dynamically adapting their computational cost to the difficulty of the inputs. In this way, the model can adjust to a limited computational budget. However, the poor quality of uncertainty estimates in deep learning models makes it difficult to distinguish between hard and easy samples. To address this challenge, we present a computationally efficient approach for post-hoc uncertainty quantification in dynamic neural networks. We show that adequately quantifying and accounting for both aleatoric and epistemic uncertainty through a probabilistic treatment of the last layers improves the predictive performance and aids decision-making when determining the computational budget. In the experiments, we show improvements on CIFAR-100, ImageNet, and Caltech-256 in terms of accuracy, capturing uncertainty, and calibration error.",
    "path": "papers/23/02/2302.06359.json",
    "total_tokens": 808,
    "translated_title": "修复动态神经网络中的过度自信问题",
    "translated_abstract": "动态神经网络是一种最近的技术，通过根据输入难度动态调整计算代价，承诺缓解现代深度学习模型越来越大的问题。然而，深度学习模型中uncertainty estimates的质量较差，很难区分hard和easy的样本。为了解决这个挑战，我们提出了一种在动态神经网络中进行后处理不确定性量化的计算有效方法。我们展示了通过对最后几层进行概率化处理，充分量化和纳入aleatoric和epistemic uncertainty，可以提高预测性能，并在确定计算预算时有助于决策。在实验中，我们在CIFAR-100、ImageNet和Caltech-256方面展示了准确性、捕获不确定性和校准误差的改进。",
    "tldr": "该论文提出了一种修复动态神经网络中过度自信问题的方法，通过对最后几层进行概率化处理，量化和纳入不确定性并有助于决定计算预算的确定。",
    "en_tdlr": "This paper proposes a method to fix overconfidence in dynamic neural networks. By probabilistically treating the last layers and adequately quantifying both aleatoric and epistemic uncertainty, the method improves predictive performance and decision-making for determining the computational budget."
}