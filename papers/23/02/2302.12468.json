{
    "title": "Adapting Prompt for Few-shot Table-to-Text Generation. (arXiv:2302.12468v2 [cs.CL] UPDATED)",
    "abstract": "Pretrained language models (PLMs) have made remarkable progress in table-to-text generation tasks. However, the lack of domain-specific knowledge makes it challenging to bridge the topological gap between tabular data and text, especially in real-world applications with limited resources. To mitigate the limitation of insufficient labeled data, we propose a novel framework: Adapt-Prompt-to-Generate (AdaPTGen). The core insight of AdaPTGen is to adapt prompt templates of domain-specific knowledge into the model, which brings at least three benefits: (1) it injects representation of normal table-related descriptions to bridge the topological gap between tabular data and texts; (2) it enables us to use large amounts of unlabeled domain-specific knowledge fully, which can alleviate the PLMs' inherent shortcomings of lacking domain knowledge; (3) it allows us to design various tasks to explore the domain-specific knowledge. Extensive experiments and analyses are conducted on three open-doma",
    "link": "http://arxiv.org/abs/2302.12468",
    "context": "Title: Adapting Prompt for Few-shot Table-to-Text Generation. (arXiv:2302.12468v2 [cs.CL] UPDATED)\nAbstract: Pretrained language models (PLMs) have made remarkable progress in table-to-text generation tasks. However, the lack of domain-specific knowledge makes it challenging to bridge the topological gap between tabular data and text, especially in real-world applications with limited resources. To mitigate the limitation of insufficient labeled data, we propose a novel framework: Adapt-Prompt-to-Generate (AdaPTGen). The core insight of AdaPTGen is to adapt prompt templates of domain-specific knowledge into the model, which brings at least three benefits: (1) it injects representation of normal table-related descriptions to bridge the topological gap between tabular data and texts; (2) it enables us to use large amounts of unlabeled domain-specific knowledge fully, which can alleviate the PLMs' inherent shortcomings of lacking domain knowledge; (3) it allows us to design various tasks to explore the domain-specific knowledge. Extensive experiments and analyses are conducted on three open-doma",
    "path": "papers/23/02/2302.12468.json",
    "total_tokens": 971,
    "translated_title": "为少数据样本的表格生成自适应提示",
    "translated_abstract": "预训练语言模型（PLM）在表格生成任务中取得了显著进展。然而，缺乏领域特定知识很难弥合表格数据和文本之间的拓扑差距，尤其是在具有有限资源的实际应用中。为了解决标注数据不足的限制，我们提出了一种新的框架：自适应生成提示（AdaPTGen）。AdaPTGen的核心是将领域特定知识的提示模板调整为模型所需，带来了至少三个好处：（1）它注入了常规表格相关描述的表示，以弥合表格数据和文本之间的拓扑差距；（2）它使我们能够充分利用大量未标记的领域特定知识，从而减轻了PLMs缺乏领域知识的固有缺点；（3）它允许我们设计各种任务来探索领域特定知识。在三个开放领域的实验和分析中进行了广泛实验。",
    "tldr": "这项研究提出了一个新的框架AdaPTGen，通过将领域特定知识的提示模板调整为模型所需，来解决缺乏标注数据的限制。该框架注入了常规表格相关描述的表示，充分利用未标记的领域特定知识，并允许设计各种任务来探索领域特定知识。",
    "en_tdlr": "This study proposes a novel framework, AdaPTGen, to address the limitation of insufficient labeled data by adapting prompt templates of domain-specific knowledge into the model. The framework injects representation of normal table-related descriptions, utilizes unlabeled domain-specific knowledge, and allows the design of various tasks to explore domain-specific knowledge."
}